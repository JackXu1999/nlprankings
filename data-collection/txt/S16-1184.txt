



















































CLIP@UMD at SemEval-2016 Task 8: Parser for Abstract Meaning Representation using Learning to Search


Proceedings of SemEval-2016, pages 1190–1196,
San Diego, California, June 16-17, 2016. c©2016 Association for Computational Linguistics

CLIP@UMD at SemEval-2016 Task 8: Parser for Abstract Meaning
Representation using Learning to Search

Sudha Rao1,3∗, Yogarshi Vyas1,3∗, Hal Daumé III1,3, Philip Resnik2,3
1Computer Science, 2Linguistics, 3UMIACS

University of Maryland
{raosudha,yogarshi,hal}@cs.umd.edu, resnik@umd.edu

Abstract

In this paper we describe our approach to
the Abstract Meaning Representation (AMR)
parsing shared task as part of SemEval 2016.
We develop a novel technique to parse En-
glish sentences into AMR using Learning to
Search. We decompose the AMR parsing task
into three subtasks - that of predicting the con-
cepts, the relations, and the root. Each of these
subtasks are treated as a sequence of predic-
tions. Using Learning to Search, we add past
predictions as features for future predictions,
and define a combined loss over the entire
AMR structure.

1 Introduction

This paper describes our submission to the Abstract
Meaning Representation (AMR) Parsing Shared
Task at SemEval 2016. The goal of the task is to
generate AMRs automatically for English sentences.
We develop a novel technique for AMR parsing that
uses Learning to Search (L2S) (Ross et al., 2011;
Daumé III et al., 2009; Collins and Roark, 2004).

L2S is a family of approaches that solves struc-
tured prediction problems. These algorithms have
proven to be highly effective for problems in NLP
like part-of-speech tagging, named entity recogni-
tion (Daumé III et al., 2014), coreference resolution
(Ma et al., 2014), and dependency parsing (He et
al., 2013). Briefly, L2S attempts to do structured
prediction by (1) decomposing the production of the
structured output in terms of an explicit search space
(states, actions, etc.); and (2) learning hypotheses

∗The first two authors contributed equally to this work.

read-01

i book

name forest

Stories

from

Nature

ARG0
ARG1

name

topic

op1
op2

op3

Figure 1: AMR graph for the sentence “I read a
book, called Stories from Nature, about the forest.”

that control a policy that takes actions in this search
space. AMR (Banarescu et al., 2013), in turn, is a
structured semantic representation which is a rooted,
directed, acyclic graph. The nodes of this graph rep-
resent concepts in the given sentence and the edges
represent relations between these concepts. As such,
the task of predicting AMRs can be naturally placed
in the L2S framework. This allows us to model the
learning of concepts and relations in a unified set-
ting which aims to minimize the loss over the entire
predicted structure.

In the next section, we briefly review DAGGER
and explain its various components with respect to
our AMR parsing task. Section 3 describes our main
algorithm along with the strategies we use to deal
with the large search space of the search problem.

1190



I read a book called Stories from Nature

i read-01 NULL book NULL

call-01

called

Stories

story

NULL

from

NULL

Nature

NULL

(a) Concept prediction stage: Shaded nodes indicate predicted concepts (Current state). The middle row represents the
oracle action. Other rows represents other possible actions.

r

i b

ARG0

(b) Sample current state for
relation prediction

i

r

b i

r

b i

r

b

ARG0

ARG1

ARG0

mod

ARG0

(c) Three possible actions given the current state for relation prediction, the last one being
the true relation i.e. no edge

Figure 2: Using DAGGER for AMR parsing

We then describe our experiments and results (Sec-
tion 4).

2 Using L2S for AMR Parsing

L2S works on the notion of a policy which can be de-
fined as “what is the best next action (yi) to take” in
a search space given the current state. It starts with
an initial policy on a trajectory (called rollin policy),
takes a one-step deviation and completes the trajec-
tory with another policy (called the rollout policy).
The different variations of L2S are defined based
on what kind of policies it uses during rollin and
rollout. For example DAGGER uses rollin=learned
policy and rollout=reference policy, SEARN uses
rollin=rollout=stochastic mixture of reference and
learned policy, LOLS uses rollin=learned policy and
rollout=stochastic mixture of reference and learned
policy.

Next, we describe how we use L2S for AMR
parsing. We decompose the full AMR parsing task
into three subtasks - that of predicting the con-
cepts, predicting the root, and predicting the rela-
tions between the predicted concepts (explained in
more detail under section 3). The search space for

concept and relation prediction consists of a state
s = (x1, x2, ..., xm, y1, y2, .., yi−1), where the input
(x1, x2, ..., xm) are the m words of a sentence.

• During concept prediction, the labels
(y1, y2, .., yi−1) are the concepts predicted
up to the index (i − 1) and the next action yi
is the concept for word xi from a k-best list
of concepts. In Figure 2a, the current state
corresponds to the concepts {‘i’, ‘read-01’,
‘book’} and the next action is assigning one
of {‘call-01’, ‘called’, NULL} to the word
‘called’.

• During relation prediction, the labels are the re-
lations predicted for pairs of concepts obtained
during the concept prediction stage. In Figure
2c, the current state corresponds to the rela-
tion ‘ARG0’ predicted between ‘r’ and ‘i’ and
the next action is assigning one of {‘ARG1’,
‘mod’, NO-EDGE} to the pair of concept ‘b’
and ‘i’.

For the root prediction subtask we train a multi-
class classifier which makes a single prediction - that

1191



of choosing the root concept from all the predicted
concepts.

Next, we need to define how we learn a policy for
AMR parsing. When the reference policy is optimal,
it has been shown that it is effective to rollout using
the reference policy (Ross and Bagnell, 2014; Chang
et al., 2015). This is true for our task and hence
we use DAGGER. Below, we explain how we use
DAGGER, with a running example in Figure 2.

At training time, DAGGER operates in an iterative
fashion. It starts with an initial policy and given an
input x, makes a prediction y = y1, y2, ..., ym using
the current policy. For each prediction yi it gener-
ates a set of multi-class classification examples each
of which correspond to a possible action the algo-
rithm can take given the current state. Each exam-
ple can be defined using local features and features
that depend on previous predictions. We use a one-
against-all classifier to make the multi-class classifi-
cation decisions.

During training, DAGGER has access to the ref-
erence policy. The reference policy during concept
prediction is to predict the concept that was aligned
to a given span using the JAMR aligner (explained
in Section 3.4) For e.g. in Figure 2a, the reference
policy is to predict NULL since the span “called” was
aligned to NULL by the aligner. The reference pol-
icy during relation prediction is to predict the gold
edge between two concepts. For e.g. in Figure 2c,
the reference policy is to predict NO-EDGE.

DAGGER then calculates the loss between the
predicted action and the best action using a pre-
specified loss function. It then computes a new pol-
icy based on this loss and interpolates it with the cur-
rent policy to get an updated policy, before moving
on to the next iteration. At test time, predictions are
made greedily using the policy learned during train-
ing.

3 Methodology

3.1 Learning technique

We use DAGGER as described in Section 2 to learn a
model that can successfully predict the AMR y for
a sentence x. The sentence x is composed of a se-
quence of spans (s1, s2, ..., sn) each of which can be
a single word or a span of words (We describe how
we go from a raw sentence to a sequence of spans

Algorithm 1
1: for each span si do
2: ci = predict concept(si)
3: end for
4: croot = predict root([c1, ..., cn])
5: for each concept ci do
6: for each j < i do
7: r(i,j) = predict relation(ci, cj)
8: r(j,i) = predict relation(cj , ci)
9: end for

10: end for

in Section 3.4). Given that our input has n spans,
we first decompose the structure into a sequence of
n2 + 1 predictions D = (C,ROOT,R), where

C = c1, c2, ..., cn - where ci is the concept pre-
dicted for span si

ROOT is the decision of choosing one of the pre-
dicted concepts as the root (croot) of the AMR

R = r2,∗, r∗,2, r3,∗, r∗,3, ..., rn,∗, r∗,n - where ri,∗
are the predictions for the directed relations from ci
to cj ∀j < i, and r∗,i are the predictions for the
directed relations from cj to ci ∀j < i. We constrain
our algorithm to not predict any incoming relations
to croot.

During training time, the possible set of actions
for each prediction is given by the k-best list (Sec-
tion 3.2). We use Hamming Loss as our loss func-
tion. Under Hamming Loss, the reference policy is
simply choosing the right action for each prediction
i.e. choosing the correct concept (relation) during
the concept (relation) prediction phase. This loss is
defined on the entire predicted output, and hence the
model learns to minimize the loss for concepts and
relations jointly.

Algorithm 1 describes the sequence of predic-
tions to be made in our problem. We learn
three different policies corresponding to each of
the functions predict concept, predict root and
predict relation. The learner in each stage uses
features that depend on predictions made in the pre-
vious stages. Tables 1, 2 and 3 describe the set of
features we use for the concept prediction, relation
prediction and root prediction stages respectively.

1192



Feature label Description
wi−2, wi−i, wi, wi+1, wi+2 Words in si and context
pi−2, pi−i, pi, pi+1, pi+2 POS tags of words in si and context

NEi Named entity tags for words in si
si Binary feature indicating whether wi is(are) stopword(s)

depi All dependency edges originating from words in wi
bc Binary feature indicating whether c is the most frequently aligned

concept with si or not
ci−2, ci−1 Predicted concepts for two previous spans

c Concept label and its conjunction with all previous features
framei and sensei If the label is a PropBank frame (e.g. ‘see-01’, use the frame

(‘see’) and the sense(‘01’) as additional features.

Table 1: Concept prediction features for span si and concept label ci

Feature label Description
ci, cj , ci ∧ cj The two concepts and their conjunction

wi, wj , wi ∧ wj Words in the corresponding spans and their conjunction
pi, pj , pi ∧ pj POS tags of words in spans and their conjunction

depij All dependency edges with tail in wi and head in wj
dir Binary feature which is true iff i < j
r Relation label and its conjunction with all other features

Table 2: Relation prediction features for concepts ci and cj and relation label r

Feature label Description
ci Concept label. If the label is a PropBank frame (e.g. ‘see-01’, use

the frame (‘see’) and the sense(‘01’) as additional features.
wi Words in si, i.e. the span corresponding to ci
pi POS tags of words in si

is dep rooti Binary feature indicating whether one of the words in si is the
root in the dependency tree of the sentence

Table 3: Root prediction features for concept ci

3.2 Selecting k-best lists

For predicting the concepts and relations using DAG-
GER, we need a candidate-list (possible set of ac-
tions) to make predictions from.

Concept candidates: For a span si, the
candidate-list of concepts, CL-CONsi is the set of all
concepts that were aligned to si in the entire training
data. If si has not been seen in the training data, CL-
CONsi consists of the lemmatized span, PropBank
frames (for verbs) obtained using the Unified Verb
Index (Schuler, 2005) and the NULL concept.

Relation candidates: The candidate list of rela-
tions for a relation from concept ci to concept cj ,
CL-RELij , is the union of the following three sets:

• pairwisei,j - All directed relations from ci to
cj when ci and cj occurred in the same AMR,
• outgoingi - All outgoing relations from ci, and
• incomingj - All incoming relations into cj .
In the case when both ci and cj have not been

seen in the training data, CL-RELij consists of all
relations seen in the training data. In both cases,
we also provide an option NO-EDGE which indicates
that there is no relation between ci and cj .

3.3 Pruning the search space
To prune the search space of our learning task, and
to improve the quality of predictions, we use two ob-
servations about the nature of the edges of the AMR

1193



of a sentence, and its dependency tree (obtained us-
ing the Stanford dependency parser (De Marneffe et
al., 2006)), within our algorithm.

First, we observe that a large fraction of the
edges in the AMR for a sentence are between con-
cepts whose underlying spans (more specifically,
the words in these underlying spans) are within
two edges of each other in the dependency tree of
the sentence. Thus, we refrain from calling the
predict relation function in Algorithm 1 between
concepts ci and cj if each word in wi is three or more
edges away from all words in wj in the dependency
tree of the sentence under consideration, and vice
versa. This implies that there will be no relation rij
in the predicted AMR of that sentence. This doesn’t
affect the number of calls to predict relation in the
worst case (n2−n, for a sentence with n spans), but
practically, the number of calls are far fewer. Also,
to make sure that this method does not filter out too
many AMR edges, we calculated the percentage of
AMR edges that are more than two edges away in
dependency tree. We found this number to be only
about 5% across all our datasets.

Secondly, and conversely, we observe that for a
large fraction of words which have a dependency
edge between them, there is an edge in the AMR
between the concepts corresponding to those two
words. Thus, when we observe two concepts ci
and cj which satisfy this property, we force our
predict relation function to assign a relation rij
that is not NULL.

3.4 Preprocessing

JAMR Aligner: The training data for AMR pars-
ing consists of sentences paired with corresponding
AMRs. To convert a raw sentence into a sequence
of spans (as required by our algorithm), we ob-
tain alignments between words in the sentence and
concepts in the AMR using the automatic aligner
of JAMR. The alignments obtained can be of three
types (Examples refer to Figure 1):

• A single word aligned to a single concept: E.g.,
word ‘read’ aligned to concept ‘read-01’.
• Span of words aligned to a graph fragment:

E.g., span ‘Stories from Nature’ aligned to the
graph fragment rooted at ’name’. This usually
happens for named entities and multiword ex-

pressions such as those related to date and time.
• A word aligned to NULL concept: Most func-

tion words like ‘about’, ‘a’, ‘the’, etc are not
aligned to any particular concept. These are
considered to be aligned to the NULL concept.

Forced alignments: The JAMR aligner does not
align all concepts in a given AMR to a span in the
sentence. We use a heuristic to forcibly align these
leftover concepts and improve the quality of align-
ments. For every unaligned concept, we count the
number of times an unaligned word occurs in the
same sentence with the unaligned concept across all
training examples. We then align every leftover con-
cept in every sentence with the unaligned word in the
sentence with which it has maximally coocurred.

Span identification: During training time, the
aligner takes in a sentence and its AMR graph and
splits each sentence into spans that can be aligned to
the concepts in the AMR. However, during test time,
we do not have access to the AMR graph. Hence,
given a test sentence, we need to split the sentence
into spans, on which we can predict concepts. We
consider each word as a single span except for two
cases. First, we detect possible multiword spans cor-
responding to named entities, using a named entity
recognizer (Lafferty et al., 2001). Second, we use
some basic regular expressions to identify time and
date expressions in sentences.

3.5 Connectivity

Algorithm 1 does not place explicit constraints on
the structure of the AMR. Hence, the predicted out-
put can have disconnected components. Since we
want the predicted AMR to be connected, we con-
nect the disconnected components (if any) using the
following heuristic. For each component, we find
its roots (i.e. concepts with no incoming relations).
We then connect the components together by sim-
ply adding an edge from our predicted root croot to
each of the component roots. To decide what edge
to use between our predicted root croot and the root
of a component, we get the k-best list (as described
in section 3.2) between them and choose the most
frequent edge from it.

1194



Dataset Training Dev Test
BOLT DF MT 1061 133 133

Broadcast conversation 214 0 0
Weblog and WSJ 0 100 100
BOLT DF English 6455 210 229
Guidelines AMRs 689 0 0

2009 Open MT 204 0 0
Proxy reports 6603 826 823

Weblog 866 0 0
Xinhua MT 741 99 86

Table 4: Dataset statistics. All figures represent
number of sentences.

3.6 Acyclicity
The post-processing step described in the previous
section ensures that the predicted AMRs are rooted,
connected, graphs. However, an AMR, by defini-
tion, is also acyclic. We do not model this constraint
explicitly within our learning framework. Despite
this, we observe that only a very small number of
AMRs predicted using our fully automatic approach
have cycles in them. Out of the total AMRs pre-
dicted in all test sets, less than 5% have cycles in
them. Besides, almost all cycles that are predicted
consist of only two nodes, i.e. both rij and rji have
non-NO-EDGE values for concepts ci and cj . To get
an acyclic graph, we can greedily select one of rij
or rji, without any loss in parser performance.

4 Experiments and Results

The primary corpus for this shared task is the AMR
Annotation Release 1.0 (LDC2015E86). This cor-
pus consists of datasets from varied domains such as
online discussion forums, blogs, and newswire, with
about 19,000 sentence-AMR pairs. All datasets have
a pre-specified training, dev and test split (Table 4).

We trained three systems. The first was trained
on all available training data. The two other sys-
tems were trained using data from a single do-
main. Specifically, we chose BOLT DF English and
Proxy reports since these are the two largest train-
ing datasets individually. The system trained on the
Proxy reports dataset performed the best when eval-
uated on the dev set. Hence we used this system as
our primary system for the task. Additionally, we
use DAGGER as implemented in the Vowpal Wab-

bit machine learning library (Langford et al., 2007;
Daumé III et al., 2014).

The evaluation of predicted AMRs is done us-
ing Smatch (Cai and Knight, 2013) 1, which com-
pares two AMRs using precision, recall and F1. Our
system obtained a Smatch F1 score of 0.46 with a
Precision of 0.51 and a Recall of 0.43 on the test
set in the Shared Task (We made a tokenization er-
ror during the actual semeval submission and so re-
ported an F1 score of 0.44 instead). The mean F1
score of all systems submitted to the shared task was
0.55 and the standard deviation was 0.06.

5 Conclusion and Future work

We have presented a novel technique for parsing En-
glish sentences into AMR using DAGGER, a Learn-
ing to Search algorithm. We decompose the AMR
parsing task into subtasks of concept prediction, root
prediction and relation prediction. Using Learning
to Search allows us to use past predictions as fea-
tures for future predictions and also define a com-
bined loss over the entire AMR structure. Addition-
ally, we use a k-best candidate list constructed from
the training data to make predictions from. To prune
the large search space, we incorporate useful heuris-
tics based on the dependency parse of the sentence.
Our system is available for download 2.

Currently we ensure various properties of AMR,
such as connectedness and acyclicity using heuris-
tics. In the future, we plan to incorporate these as
constraints in our learning technique.

References
Laura Banarescu, Claire Bonial, Shu Cai, Madalina

Georgescu, Kira Griffitt, Ulf Hermjakob, Kevin
Knight, Philipp Koehn, Martha Palmer, and Nathan
Schneider. 2013. Abstract meaning representation for
sembanking.

Shu Cai and Kevin Knight. 2013. Smatch: an evalu-
ation metric for semantic feature structures. In Pro-
ceedings of the 51st Annual Meeting of the Association
for Computational Linguistics, ACL 2013, 4-9 August
2013, Sofia, Bulgaria, Volume 2: Short Papers, pages
748–752.
1http://amr.isi.edu/download/smatch-v2.

0.tar.gz
2http://cs.umd.edu/˜yogarshi/amr_parser.

zip

1195



Kai-Wei Chang, Akshay Krishnamurthy, Alekh Agarwal,
Hal Daumé III, and John Langford. 2015. Learning
to search better than your teacher. In Proceedings of
the 32nd International Conference on Machine Learn-
ing, ICML 2015, Lille, France, 6-11 July 2015, pages
2058–2066.

Michael Collins and Brian Roark. 2004. Incremental
parsing with the perceptron algorithm. In Proceed-
ings of the 42nd Annual Meeting on Association for
Computational Linguistics, page 111. Association for
Computational Linguistics.

Hal Daumé III, John Langford, and Daniel Marcu. 2009.
Search-based structured prediction. Machine learn-
ing, 75(3):297–325.

Hal Daumé III, John Langford, and Stephane Ross. 2014.
Efficient programmable learning to search. arXiv
preprint arXiv:1406.1837.

Marie-Catherine De Marneffe, Bill MacCartney, Christo-
pher D Manning, et al. 2006. Generating typed de-
pendency parses from phrase structure parses. In Pro-
ceedings of LREC, volume 6, pages 449–454.

He He, Hal Daumé III, and Jason Eisner. 2013. Dynamic
feature selection for dependency parsing. In Proceed-
ings of the 2013 Conference on Empirical Methods
in Natural Language Processing, EMNLP 2013, 18-
21 October 2013, Grand Hyatt Seattle, Seattle, Wash-
ington, USA, A meeting of SIGDAT, a Special Interest
Group of the ACL, pages 1455–1464.

John Lafferty, Andrew McCallum, and Fernando CN
Pereira. 2001. Conditional random fields: Probabilis-
tic models for segmenting and labeling sequence data.

John Langford, Lihong Li, and Alexander Strehl. 2007.
Vowpal wabbit online learning project.

Chao Ma, Janardhan Rao Doppa, J Walker Orr, Prashanth
Mannem, Xiaoli Fern, Tom Dietterich, and Prasad
Tadepalli. 2014. Prune-and-score: Learning for
greedy coreference resolution. In Proceedings of Con-
ference on Empirical Methods in Natural Language
Processing (EMNLP).

Stephane Ross and J Andrew Bagnell. 2014. Reinforce-
ment and imitation learning via interactive no-regret
learning. arXiv preprint arXiv:1406.5979.

Stéphane Ross, Geoff J. Gordon, and J. Andrew Bagnell.
2011. A reduction of imitation learning and structured
prediction to no-regret online learning. In Proceedings
of the Workshop on Artificial Intelligence and Statis-
tics (AIStats).

Karin Kipper Schuler. 2005. Verbnet: A broad-coverage,
comprehensive verb lexicon.

1196


