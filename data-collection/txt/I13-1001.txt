










































Semi-Supervised Answer Extraction from Discussion Forums


International Joint Conference on Natural Language Processing, pages 1–9,
Nagoya, Japan, 14-18 October 2013.

Semi-Supervised Answer Extraction from Discussion Forums

Rose Catherine, Rashmi Gangadharaiah, Karthik Visweswariah, Dinesh Raghu
IBM Research

Bangalore, India
{rosecatherinek, rashgang, v-karthik, diraghu1} @in.ibm.com

Abstract

Mining online discussions to extract an-
swers is an important research problem.
Methods proposed in the past used super-
vised classifiers trained on labeled data.
But, collecting training data for each target
forum is labor intensive and time consum-
ing, thus limiting their deployment. A re-
cent approach had proposed to extract an-
swers in an unsupervised manner, by tak-
ing cues from their repetitions. This as-
sumption however, does not hold true in
many cases. In this paper, we propose
two semi-supervised methods for extract-
ing answers from discussions, which uti-
lize the large amount of unlabeled data
available, alongside a very small training
set to obtain improved accuracies. We
show that it is possible to boost the perfor-
mance by introducing a related, but paral-
lel task of identifying acknowledgments to
the answers. The accuracy achieved by our
approaches surpass the baselines by a wide
margin, as shown by our experiments.

1 Introduction
Online discussion forums, also known as com-

munity question answering (CQA) sites, are inter-
net sites that provide a medium for users to dis-
cuss and share information on a wide range of top-
ics. Due to their vast popularity, gradually, they
have aggregated a massive collection of discussion
data. Mining such forums have numerous appli-
cations such as improving question–answer (QA)
retrieval (Cong et al., 2008), learning important
insights like features of products that are draw-
ing negative reviews (Lakkaraju et al., 2011) or
discovering longstanding unresolved severe tech-
nical issues (Gangadharaiah and Catherine, 2012)
etc. For this reason, substantial research effort
has been directed at mining discussions, in recent

times. In this paper, we focus on the specific prob-
lem of extracting answers from these discussions.

In forums, typically a user starts a discussion
by posting a question to which multiple mem-
bers of the forum suggest answers. The discus-
sion evolves into a complex multi-party conversa-
tion as the question gets refined, with additional
details specified, clarifications sought, multiple
answers provided, frequent digressions, and oc-
casional follow-up discussions and acknowledg-
ments, altogether spanning several pages. An-
swers easily get buried deep within this and locat-
ing them automatically is far from straightforward.

In this paper, we propose two semi-supervised
approaches that require only a very small amount
of training data (only 3 manually tagged discus-
sion threads ) and achieve high accuracy levels by
using the available unlabeled data. With this, we
eliminate the need to collect vast amounts of train-
ing data, thus aiding faster deployment for new do-
mains. Specifically, our contributions are:
• A semi-supervised answer extraction method for

discussions: This paper makes the first attempt
at extracting answers from discussions in a semi-
supervised manner. We show how existing fea-
tures can be engineered into a co-training frame-
work to accomplish this.

• A parallel co-training method to leverage ac-
knowledgments for improved answer extraction
accuracy: We motivate and demonstrate that it
is possible to improve the performance tremen-
dously by introducing a related task of identify-
ing acknowledgments in the discussions, which
we run as a parallel task alongside the main an-
swer extraction task (Section 5).

• We demonstrate that with a very small training
data and by using the available unlabeled data, it
is possible to extract answers from forums with
an accuracy that is substantially better than ex-
tracting them in an unsupervised manner or in a
fully supervised setting.

1



The rest of the paper is organized as follows:
Section 2 discusses related work. Section 3 sets
the terminology and introduces the co-training
framework, which is used throughout this paper.
Section 4 details how the co-training framework
can be applied to the answer extraction task. Sec-
tion 5 introduces the acknowledgment extraction
task in a parallel co-training framework. Experi-
ments and results are discussed in Section 6 fol-
lowed by conclusions in Section 7.

2 Related work
Research in the area of extracting question and

answers from online forums, has grown consider-
ably. Almost all approaches proposed so far for
this task are supervised learning methods. Ding
et al. (2008), Kim et al. (2010), Raghavan et al.
(2010) and Kim et al. (2012) employed Condi-
tional Random Fields, Hong and Davison (2009),
Huang et al. (2007) and Catherine et al. (2012)
used Support Vector Machines (SVM), Shrestha
and McKeown (2004) learnt rules using Ripper,
and Yang et al. (2009) used Struct SVMs for ex-
tracting answers. The obvious downside to these
methods is that for any new domain or forum, sub-
stantial amounts of manually labeled training ex-
amples have to be collected. This is usually time
consuming and costly. Gandhe et al. (2012) pro-
posed an approach for adapting an answer extrac-
tor trained on one domain to another, by separating
out the lexical characteristics of an answer from its
domain relevance. However, learning the lexical
characteristics still required a training set.

A recent work by Cong et al. (2008) proposed
an unsupervised method using PageRank-style
random walks on a graph representation of the dis-
cussion, with the hypothesis that inter-candidate
similarities can improve accuracy of the answer
extraction task. The intuition is that posts that bear
more resemblance to other posts in the thread have
higher chances of being answers. However, in a lot
of discussion forums, especially those related to
troubleshooting and problem resolution, we found
that this assumption usually does not hold. An an-
swer that was suggested earlier in the discussion
is not usually suggested again – only new ones or
a modification of the same would appear. A gen-
eral observation here was that posts that had simi-
lar content as other posts were found to be others
complaining about the same issue. This was also
noted by Gandhe et al. (2012) and Catherine et al.
(2012). Nevertheless, (Cong et al., 2008) is the

only work so far, that sought to extract answers
without supervision.

One of the methods proposed in this paper
that uses a parallel acknowledgment classification
task, belongs to the family of Multi-Task Learn-
ing (MTL) (Caruana, 1997) since what is learned
for each task is used to improve the other task.
However, to the best of our knowledge, this is
the first work that proposes a MTL-type answer
classifier for forums in a semi-supervised setting.
Cross-Training (Sarawagi et al., 2003) is a related
methodology which improves classification per-
formance on one taxonomy by accessing labels
from another taxonomy for the same document.
Our method differs because, the answer and ac-
knowledgment labels are on different posts.

Some other closely related works are listed be-
low; however, their focus is different from the
task proposed in this paper. Jijkoun and de Rijke
(2005) proposed a method to automatically extract
question–answer pairs from FAQ pages using for-
matting cues. Since it is known that the entry fol-
lowing the question is definitely an answer, they
did not have to classify the entries. Sarencheh et
al. (2010) proposed a semi-automatic wrapper in-
duction method for extracting different structural
components of a discussion, like the time of post-
ing, author name, content of the post etc. Answer
retrieval is another closely related task where the
emphasis is on retrieving the most relevant post
(Xue et al., 2008). The scope of our paper, how-
ever, is limited to tagging posts in forum discus-
sions as answers or not.
3 Preliminaries
3.1 Terminology and Scope

A discussion in an online forum is created when
a user posts a question. Other members of the fo-
rum reply to this post or to other replies, thereby
evolving the discussion. A sample discussion with
7 posts including 2 answers and 2 acknowledg-
ments, is shown in Figure 1. In this paper, we
use the terms discussion and thread (as in,
a thread of discussion) interchangeably.

An answer is typically spread over multiple sen-
tences within the same post, especially in the case
of non-factoid answers. It would have been ideal,
if the system extracted answers at the granularity
of a sentence. However, the inter-annotator agree-
ment1 for answer sentences in our dataset (Section
6) was a mere 0.19. Hence, we extract answers at

1http://en.wikipedia.org/wiki/Cohen’s kappa

2



I am having the same problem when charging the phone from the USB 
port charger. I just plugged it into a wall outlet and it is now charging fine. 
Maybe it's not recognizing the new lightning cable? Try using an AC outlet

Post: 0
Author: A
Level: 1 
Points: 50

Post: 1
Author: B
Level: 3
Points: 150

My iPhone 5 will not charge has anyone else got this problem?
When I plug the lightning cable in, its as if the phone registers this 
but it will not charge.

Post: 2
Author: A
Level: 1 
Points: 50

Thanks for the reply, the AC outlet has already been tried but it won't charge 
at all.

Question

Answer Suggestion

Acknowledgment - Negative

Exactly the same problem here. Tried USB on computer and wall using the 
plug provided. It detects the cable, but zero charge!  Bad times for a new 
product!

Post: 3
Author: C
Level: 1
Points: 0

I'm having the same issue, and it's not recognizing it at all. There's always 
a problem!

Post: 4
Author: D
Level: 1
Points: 5

Sounds like “Lightning Gate”...  I have the same issue. The phone won't 
charge via the wall plug or connect to the computer. Sounds like bad cable 
or connection. Why don't you get another cable?

Post: 5
Author: E
Level: 5
Points: 270

Answer Suggestion

Post: 6
Author: A
Level: 1 
Points: 50

I have managed to use another cable and the phone now charges with no 
problems. Thanks to everyone who has contributed to this discussion, it 
really is appreciated!  

Acknowledgment - Positive

Figure 1: A Sample Discussion

a post level – a post is classified as an answer if
any sentence within it suggests a solution.

Digressions are very common in such commu-
nity question answering systems. We do not at-
tempt to find these questions or separate out the
sub-discussions. Question detection as well as dis-
entangling multi-party discussions, is a well re-
searched area (Cong et al., 2008; Elsner and Char-
niak, 2010), and is outside the scope of this paper.
For the purposes of this paper, the first post is the
question and we attempt to find answers to only
this question. Answers to other questions within
the discussion are negative examples.
3.2 Co-Training Methodology

Co-Training introduced by Blum and Mitchell
(1998), is a general framework for semi-
supervised classification, where the features for
classifying each data point can be partitioned into
two distinct sets or views. The views are such that
either of them is sufficient to classify any data-
point, had there been enough training examples.

The algorithm proceeds in two half-steps: in it-
eration i, the current set of labeled examples Li

(initially, a very small set) is used to train a classi-
fier C1 that uses only one view v1 of each train-
ing instance and another classifier C2 that uses
only view v2. C1 and C2 are then used to clas-
sify the unlabeled points, and the most confident
m predictions are moved from the unlabeled pool
U to the set of labeled examples, which are used

Φ1(x) Φ2(x)

C1 C2

Most confident predictions on U – Li 

Li 

Iteration i

Figure 2: Co-Training Framework

for training in the i + 1th iteration. Essentially,
each classifier teaches the other by providing ex-
amples which the other would have misclassified.
Figure 2 shows this workflow, where Φ1 and Φ2
are the feature vectors of the input corresponding
to the two views v1 and v2. The paper showed
that when the two views are independent given
the label of the data point (conditional indepen-
dence), any initial weak predictor can be boosted
to a high accuracy using unlabeled examples by
co-training. This was empirically evaluated for a
webpage classification task where v1 was the set of
words in the webpage and v2, the anchor texts of
all links pointing to that page. Co-training frame-
work is widely used in many text mining tasks
like parsing (Sarkar, 2001), machine translation
(Callison-Burch, 2002) and for creating parallel
corpora (Callison-Burch and Osborne, 2003).

4 Answer Extraction by Co-Training:
ANS CT Model

To apply the co-training framework to the task
of answer extraction, we need two independent
views of the data. Prior work in supervised an-
swer extraction from forum discussions have re-
ported good accuracies when using features con-
structed from the structure of the thread (Ding et
al., 2008; Hong and Davison, 2009; Kim et al.,
2010; Catherine et al., 2012). This provides us
with one of the views, which we refer to as the
STRUCT view.

Cong et al. (2008) had previously used pattern
mining for the related task of question sentence ex-
traction. Similarly, Jindal and Liu (2006) had used
pattern mining for identifying comparative sen-
tences in a supervised learning setting. We mine
patterns on the sentences of the posts and employ
it as the second view, which we refer to as the PAT-
TERN view. The exact set of features are explained
in the subsections below.

3



Note that we have used only a modest set of
features in both STRUCT and PATTERN views, to
highlight the effect of co-training in improving the
answer extraction accuracy.
4.1 STRUCT view

Compared to a general text document, discus-
sion threads have a structure, which can be used
to construct features for classification. The fea-
tures that we use, referred to as STRUCT features
henceforth, are listed in Table 1. All the features
are eventually converted to binary attributes for
the experiments, where numerical attributes are
grouped into a suitable number of buckets. Each
binary value corresponds to a dimension in the
STRUCT view, Φistruct, which is 1 if that attribute-
value was present in the post; else, is set to 0.

STRUCT Feature Description

Author
Rating

A forum specific value measuring the
expertise of the author. Could be nu-
merical (e.g. 50 points) or categorical
(e.g. Expert).

Relative
Post
Position

The position of the post with respect
to the thread. It is grouped into
Beginning, Middle and End.

Post Rating A measure of how informative the
post is. Could be numerical
(e.g. 50 votes) or categorical (e.g.
Helpful).

Table 1: STRUCT features for a post
4.2 PATTERN view

Consider the below snippets from different dis-
cussion threads. Some words have been intention-
ally masked to illustrate that it is possible to iden-
tify to a considerable extent, that these are answer
suggestions from the structure of the sentence and
without regard to the context or the question.

... You can see if X will solve it ...

... Try resetting your X with the Y turned off and then turn
it back on after the X is fully booted back up ...
... Go to A -> B -> C and toggle the D mode ...
... X is no longer supported by Y ...

The PATTERN view uses a pattern mining mod-
ule, which mines the answer posts in Li to dis-
cover the most frequent sequential patterns, FP i

for iteration i. Each such discovered pattern p ∈
FP i corresponds to a dimension in the PATTERN
view, Φipattern, which is 1 if p matches (is sub-
sequence of) any sentence of the post.

We implemented the PrefixSpan (Pei et al.,
2001) algorithm for mining sequential patterns,
but with the following modifications to contain the
blow up in the number of patterns:

• Variable Minimum Support: the number of
items in which a pattern appears is called its
support, and minimum support, min sup is
an input parameter that determines whether a
pattern is frequent enough. We set min sup
to max(min sup0, frac×numItemsi), where
frac is a preset fraction, set to 0.03 in our ex-
periments, numItemsi is the number of items
being mined in iteration i, and min sup0 is a de-
fault minimum, set to 3 in our experiments.

• Pattern Length: only patterns of length at least
min len, set to 3 in our experiments, are accept-
able.

• Item Gap: the items of a frequent pattern are
sequential, but not consecutive, thus allowing
PrefixSpan to pick items that are arbitrary num-
ber of items apart (gap). We constrain the gap
between items of a pattern to a maximum of
max gap, set to again 3 in our experiments.

Posts are mined at a sentence level, for which we
use OpenNLP2 sentence detector.
4.2.1 Text Pre-Processing

We found that using the exact words limited
the number of frequent patterns that could be
found. To minimize this problem, we used Part-
Of-Speech (POS) tags of the words to:
• Replace all nouns with their POS tags.
• Replace all verbs with its root/stemmed (us-

ing Porter stemmer (Porter, 1980)) form and
its POS tag. For example, restarting be-
comes restart VBG. We let PrefixSpan pick
the verb-stem and/or the POS tag according to
their support.
All words are lowercased. A discussion on the

set of patterns that were detected is in Section 6.3.

5 Leveraging Acknowledgment Signals:
ANS-ACK PCT Model

In this section, we motivate and introduce a re-
lated task of extracting acknowledgments in forum
discussions and inducing signals from them to im-
prove the accuracy of the answer extraction task.

Merriam-Webster3 defines an acknowledgment
as a recognition or favorable notice of an act or
achievement. Acknowledgment is an inevitable
component of any conversation, especially, when
it evolves around seeking assistance. And so they
find their place in forum discussions too. Consider
the below snippets taken from replies by question

2http://opennlp.apache.org
3http://www.merriam-webster.com

4



authors. They are grouped according to their po-
larity – Positive, Negative and Neutral.

Positive: author reports that the suggestion solved
the issue.

... Great! That solved it! Thanks a bunch ...

... Thanks for your help. Finally got it working ...

... Switching on X did the trick. Now I can Y without any
problem ...
... Thanks a lot guys. X solved my woes. I must have Y-ed
it by mistake at some point ...

Negative: the suggestion did not solve the issue.
... That didn’t help. Any other suggestions? ...
... I tried that. It is still showing X ...
... Getting the same X. Thanks anyway ...
... Thanks for your advice. Unfortunately, it didn’t help!

Neutral: it is not clear if the issue was solved, but
the statement is an acknowledgment nevertheless

... Thanks for the reply ...

... I will try that ...

... Thanks for the helpful advice. Hope resetting X prop-
erly will fix my problem ...
... I’m reinstalling X. Will keep you posted ...

Similar to the case of answer sentences in Sec-
tion 4, the above examples can be easily identi-
fied as acknowledgments and it is fairly clear that
the posts to which the above sentences are replies,
are answer suggestions. Note that this can be de-
termined without knowing the contents of the lat-
ter, if we can assume that the reply-to relation of
the posts is known. This however is not always
the case. In a small study conducted, we found
that only 75% of forums displayed or had the re-
quired information in the html of the webpage for
constructing the reply-to relation of the posts, out
of 12 technical forums that we inspected. In the
absence of this information, (Wang et al., 2011;
Seo et al., 2009; Wang and Rosé, 2010) propose
techniques to automatically recover the structure.
For the purposes of this paper, we assume that the
reply-to structure of the discussion thread is given.

ANS-ACK PCT (ANSwer ACKnowledgment
Parallel Co-Training) aims to leverage signals
from acknowledgment posts, to better identify an-
swer posts. We cast this as another instance of
semi-supervised learning task (another co-training
instance) which runs in parallel to the main an-
swer extraction instance of co-training. Hence the
name, PARALLEL co-training.

It is worth listing down some of the design de-
cisions for this choice of approach:
(i) There is no public dataset available to train an
acknowledgment classifier. So, it is important to
note here that the task of detecting acknowledg-
ments cannot be fully supervised where a large

amount of training data is collected for the spe-
cific domain; this will defeat the entire purpose of
semi-supervised answer extraction.
(ii) For the initial small training set required for
the semi-supervised approach, we do not label ad-
ditional threads. Instead, we create a training set
from the initial training set of the answer extrac-
tion task by marking replies from the question au-
thor to posts that are answers, as positive exam-
ples. Other replies from the question author be-
come negative examples. To avoid getting influ-
enced by digressions, we do not consider replies
from other authors.
(iii) The reader might suggest using acknowledg-
ment as one of the views within the co-training
instance of answer detection, instead of two par-
allel co-training instances. i.e. to mark all posts
that have an acknowledgment as an answer in that
view. Here, we would like to point out that ac-
knowledgment is a strong indicator only when it
is available. In other words, even if we learn to
classify acknowledgment posts perfectly, it cannot
classify all answer posts perfectly since not all an-
swers are acknowledged. In our test set (Section
6), there were 559 answers, but only 173 of them
had any reply from the question author (30.9%), of
which only 72% were actually acknowledgments,
as found through manual inspection (the rest had
to do with refining the question, requesting clarifi-
cation on the answer, etc.). So, the hope is to learn
how to use the signal when it is available, and not
rely on it exclusively by using it as one of the two
views of answer co-training.

The acknowledgment extraction uses the same
two views – STRUCT and PATTERN – for its co-
training instance, similar to the ANS CT model of
Section 4, to generate the views, Ψack istruct and
Ψack ipattern, respectively. Except that here, positive
examples are the posts that are acknowledgments,
as obtained by Point (ii) above.

5.1 Parallel Co-Training for Answer
Extraction

Parallel Co-Training is a method for semi-
supervised learning where there are two (or more)
co-training instances corresponding to different,
but related learning tasks running side by side,
where in iteration i, each task can induce features
based on the current state of the system. i.e. using
the outcome of iteration i − 1 of other tasks. Fig-
ure 3 depicts Parallel Co-Training for the specific
case of answer extraction, where:

5



Most confident Answer 
predictions on U –    

 

Iteration i

( Φstructians ( x) , Φstructiack (x ))

S struct
ians

Answer  Co-training

Lians

Lians
Most confident Ack 
predictions on U –    

 

Iteration i – 1

( Φpatternians ( x) , Φ patterniack (x))

S pattern
ians

Acknowledgment  Co-training

Li−1ack

Li−1ack

H struct
i−1ack

Ψ struct
i−1ack ( x) Ψ pattern

i−1ack (x )

H pattern
i−1ack

Figure 3: Parallel Co-Training Methodology

• Two tasks run in parallel:
1. Answer Co-training: the main task which

learns to classify each post as ans or ans.
2. Acknowledgment Co-training: the auxil-

iary task which learns to classify each post
as ack or ack.

• In iteration i, Answer Co-training uses the Ac-
knowledgment classifiers, Hack i−1· of the i −
1th iteration, to induce more features (detailed
in Section 5.2), Φack i· which is then concate-
nated to its original feature vector, Φans i· , to
get the new feature vector

(
Φans i· , Φ

ack i
·

)
(we

use
(

~A, ~B
)

to represent concatenation of two
vectors, where the length of the new vector is∣∣∣ ~A∣∣∣ + ∣∣∣ ~B∣∣∣). The concatenated feature vector is
then used to train the answer classifiers Sans i· of
the ith iteration.

• The STRUCT view of Answer Co-training uses
predictions from the acknowledgment classifier
that was trained on the STRUCT view of Ac-
knowledgment Co-training, and similarly for the
PATTERN view, so that the concatenated features
vectors still remain independent.

5.2 Inducing Features from
Acknowledgments

Given a thread and acknowledgment tags on
some of the posts, the most obvious feature that
can be induced on an answer post is a hasAck
feature which is True if any child of this post is
marked as an acknowledgment; else False. All
features that we generated are listed in Table 2.

ACK Feature Description

Has Ack True if this post has a reply that
is tagged as an acknowledgment; else
False.

Ack
Distance

The number of posts, in the chronolog-
ical order, between this post and its ac-
knowledgment.

Last Ack
Distance

The number of posts, in the chronologi-
cal order, between this post and the last
acknowledgment post in the thread.

Table 2: Features induced from Acknowledgments

6 Experiments
We crawled about 140K threads from Apple

Discussions4. From these, after discarding those
with no replies, 303 threads were randomly cho-
sen, and manually tagged. The inter-annotator
agreement5 between 3 annotators for this task was
0.71. For the experiments, the training set had 3 of
the tagged threads and the remaining 300 formed
the test set, the statistics of which are in Table 3.

Statistics Training Set Test Set

No. of Threads 3 300
Avg. Length of Threads 6.3 5.8
Avg. Answers per Thread 1.9 1.8
Fraction of Answers with
Question Author’s reply6

47.4% 30.9%

Table 3: Statistics of the Training and Test Sets

We used Support Vector Machines (Vapnik,
1995) (implementation from the LibSVM7 li-
brary) for all the individual classifiers, Sans i· and

4https://discussions.apple.com
5http://en.wikipedia.org/wiki/Cohen’s kappa
7http://www.csie.ntu.edu.tw/∼cjlin/libsvm/

6



Hack i· , used in the different views of the co-training
instances of ANS CT and ANS-ACK PCT models.
6.1 Study of Improvement in Answer

Extraction Accuracy

STRUCT PATTERN COMBINED

SVM 1.1% 2.5% 28.6%
ANS CT 55.6% 55.6% 55.6%
ANS-ACK PCT 63.7% 63.6% 67.8%

Table 4: F Scores for the Answer Extraction task

To demonstrate the benefits of co-training, we
first trained a supervised classifier (SVM) on the
training set for answer extraction, separately on
the two views – STRUCT and PATTERN. With
such a little amount of training data, the classi-
fiers gave unimpressive F scores (van Rijsbergen,
1979), shown in the first row of Table 4. The
COMBINED classifier is a combination of the in-
dividual STRUCT and PATTERN classifiers, com-
puted as: (P (ans|Scombined) ∝ P (ans|Sstruct)×
P (ans|Spattern)); and similarly for ans. The
post is tagged as ans if P (ans|Scombined) ≥
P (ans|Scombined). Else, it is ans.

Next, we performed 40 iterations of co-training
and in each step, 5 threads with the most confi-
dent predictions were added by each view from
the unlabeled pool to the training set. If more
than one thread had the same confidence, any
one thread was chosen randomly. The accuracies
achieved by ANS CT after the final iteration (av-
eraged over 3 runs) is listed in Table 4. Clearly,
both STRUCT and PATTERN classifiers drastically
improved their F scores and the COMBINED clas-
sifier showed a substantial 94% improvement over
the SVM baseline. The growth of F score of the
two sub-classifiers as the co-training proceeds, is
plotted in Figure 4. It can be seen that both the
classifiers reached their best within 10 iterations
and did not improve any further.

 0

 0.2

 0.4

 0.6

 0.8

 1

 0  5  10  15  20  25  30  35  40

F
 S

co
re

Co-Training Iterations

Struct
Pattern

Figure 4: ANS CT: F-scores of the STRUCT and
PATTERN sub-classifiers after each iteration

 0

 0.2

 0.4

 0.6

 0.8

 1

 0  5  10  15  20  25  30  35  40

F
 S

co
re

Co-Training Iterations

Struct
Pattern

Figure 5: ANS-ACK PCT: F scores of the
STRUCT and PATTERN sub-classifiers of the An-
swer Classifier after each iteration

 0

 0.2

 0.4

 0.6

 0.8

 1

 0  5  10  15  20  25  30  35  40

F
 S

co
re

Co-Training Iterations

Struct
Pattern

Figure 6: ANS-ACK PCT: F scores of theSTRUCT
and PATTERN sub-classifiers of the Acknowledg-
ment Classifier after each iteration

For ANS-ACK PCT, we performed 40 co-
training iterations similar to ANS CT; the num-
ber of threads chosen after each iteration was sim-
ilarly set to 5, for both answer and acknowledg-
ment instances. The answer classification accu-
racies achieved by ANS-ACK PCT after the fi-
nal iteration (averaged over 3 runs) is also listed
in Table 4. Similar to ANS CT, both STRUCT
and PATTERN classifiers improved their F scores
significantly. The COMBINED classifier showed
a substantial 137% improvement over the SVM
baseline and 22% improvement over ANS CT.
The F score growth of the answer and the ac-
knowledgment classifiers as the co-training pro-
ceeds, is plotted in Figure 5 and Figure 6 respec-
tively. Unlike Figure 4, the answer classifiers in
Figure 5 continue to improve even after 10 itera-
tions, since the acknowledgment instance is sup-
plying it with more signals. In Figure 6, the PAT-
TERN sub-classifier of the acknowledgment clas-
sifier constantly improved throughout the 40 iter-
ations even though its STRUCT counterpart stabi-
lized in about 10 iterations. The F scores of the ac-
knowledgment classifiers at the end of the final it-
eration was 82.8%, 52.9% and 81.9% respectively
for STRUCT, PATTERN and COMBINED.

7



6.2 Comparison between approaches

Precision Recall F score

SVM - STRUCT 75.0% 0.5% 1.1%
SVM - PATTERN 25.9% 1.3% 2.5%

ANS CT 40.6% 88.0% 55.6%
ANS-ACK PCT 56.8% 84.1% 67.8%

CONG 29.7% 55.6% 38.7%

Table 5: Comparison of accuracy measures of dif-
ferent methods for the answer classification task

Graph Propagation based Answer Extraction by
Cong et al. (Cong et al., 2008) is an unsuper-
vised method for extracting answers from discus-
sion forums. It is based on the premise that a
correct answer will be repeated often within the
discussion and thus, the similarity of the post to
other posts can be used as a measure of their
“answer-ness”. We used our own implementation
of this algorithm, referred to as CONG in the ex-
periments . The similarity between posts is com-
puted using Kullback-Leibler divergence (Kull-
back, 1997), which was reported by the authors to
have given the best performance. The Precision,
Recall and F scores of CONG are compared with
those of the proposed methods in Table 5.

Table 5 shows that both proposed methods per-
form substantially better than CONG: ANS CT ex-
ceeds it by 43.6% and ANS-ACK PCT surpasses
it by 75.1% (F score). Consequently, we can con-
clude that with very small training data, it is pos-
sible to achieve high accuracies compared to ex-
tracting them in an unsupervised manner.
6.3 Discussion: Answer and

Acknowledgment Patterns
This section studies the patterns that were

mined from answers and acknowledgments, which
reveal the types of sentence structures that fre-
quently appear in them. Some of the interesting
answer patterns are listed in Table 6. They are
grouped into Imperative, Factual, Conditional and
Questions, based on manual inspection. Similarly,
the acknowledgment sentences also showed inter-
esting patterns, manually grouped into Action and
Others, listed in Table 7. From inspecting the an-
swer and acknowledgment patterns, we conjecture
that it should be possible to build classifiers based
on rules defined over the structure of the sentence,
without requiring access to a training set.
7 Conclusion and Future Work

In this paper, we proposed two semi-supervised
methods for extracting answers from discussion

Pattern Type Examples

Imperative Sentence 1. go - to - NNS - NN - on
2. you - can - VB - NN
3. VBG - your - NN - NN
4. VB - to - NNS - NN
5. VBG - your - NN
6. check - NN - NN

Fact 7. is - VBZ - not - NN

Conditional State-
ment

8. if - you - VBP - NN

Questions 9. have - VB - you - tri - VBN -
VBG - NN
10. have - you - VBN - VBG - NN

Table 6: Mined Answer Patterns
Pattern Type Examples

Action 1. i - VBP - to - VB
2. i - VBP - NN - it
3. i - not - VB - NN
4. i - have - VBP - NN,
5. i - am - VBP - VBG - NN

Others 6. but - i - VBP
7. i - am - VBP - sure

Table 7: Mined Acknowledgment Patterns

threads. We showed how the structural features
and sentence construction patterns could be en-
gineered into a co-training setting such that by
using a very small training set, and the large
amount of unlabeled data available, answers could
be extracted with high accuracy, substantially sur-
passing that attained by an unsupervised method.
To demonstrate the benefits of our method, we
also showed that completely supervised methods
would fail to train a decent model with the very
little training data that we used.

In one of our methods, we motivated and intro-
duced a related task of identifying acknowledg-
ments to the answers, which was cast in a paral-
lel co-training setting. We proposed new features
which the answer labeling instance could induce
from the acknowledgment instance. Our experi-
ments showed that having access to this view of
the discussion thread substantially improved the
answer extraction accuracy.

Our work opens up new directions of research.
In the parallel co-training setting, other than in-
ducing features, the co-training instances are es-
sentially independent. In future, we plan to ex-
tend it such that the two instances would collab-
oratively label new threads; this should lead to
higher gains since the instances would now strive
to achieve higher coherence between their labels.
Also, extending the method to extract answers at a
lower granularity like a snippet or a sentence, in-
stead of at a post level would be advantageous for
domains that have more factoid type answers.

8



References
A. Blum and T. Mitchell. 1998. Combining labeled and un-

labeled data with co-training. In Proc. eleventh annual
conference on Computational learning theory, COLT’ 98,
pages 92–100.

C. Callison-Burch and M. Osborne. 2003. Bootstrapping
parallel corpora. In Proc. HLT-NAACL 2003 Workshop
on Building and using parallel texts: data driven ma-
chine translation and beyond - Volume 3, HLT-NAACL-
PARALLEL ’03, pages 44–49. Association for Computa-
tional Linguistics.

C. Callison-Burch. 2002. Co-training for statistical machine
translation. In Proc. of the 6th Annual CLUK Research
Colloquium.

R. Caruana. 1997. Multitask learning. Machine Learning,
28(1):41–75, July.

R. Catherine, A. Singh, R. Gangadharaiah, D. Raghu, and
K. Visweswariah. 2012. Does similarity matter? the
case of answer extraction from technical discussion fo-
rums. In Proc. 24th International Conference on Com-
putational Linguistics, COLING, pages 175–184.

G. Cong, L. Wang, C. Lin, Y. Song, and Y. Sun. 2008. Find-
ing question-answer pairs from online forums. In The 31st
Annual International ACM SIGIR Conference, pages 467–
474.

S. Ding, G. Cong, C. Y. Lin, and X. Zhu. 2008. Using con-
ditional random fields to extract contexts and answers of
questions from online forums. In Meeting of the Associa-
tion for Computational Linguistics (ACL).

M. Elsner and E. Charniak. 2010. Disentangling chat. Com-
putational Linguistics, 36:389–409.

A. Gandhe, D. Raghu, and R. Catherine. 2012. Domain
adaptive answer extraction for discussion boards. In Proc.
21st international conference companion on World Wide
Web, WWW ’12 Companion, pages 501–502. ACM.

R. Gangadharaiah and R. Catherine. 2012. Prism: discov-
ering and prioritizing severe technical issues from prod-
uct discussion forums. In Proc. 21st ACM International
Conference on Information and Knowledge Management,
CIKM ’12, pages 1627–1631.

L. Hong and B. D. Davison. 2009. A classification-based
approach to question answering in discussion boards. In
Proc. 32nd Annual Intl ACM SIGIR Conf. on Research and
Dev. in Information Retrieval.

J. Huang, M. Zhou, and D. Yang. 2007. Extracting chatbot
knowledge from online discussion forums. In Proc. 20th
international joint conference on Artifical intelligence, IJ-
CAI’07, pages 423–428.

V. Jijkoun and M. de Rijke. 2005. Retrieving answers from
frequently asked questions pages on the web. In Proc.
14th ACM international conference on Information and
knowledge management, CIKM ’05, pages 76–83.

Nitin Jindal and Bing Liu. 2006. Identifying comparative
sentences in text documents. In In Proc. 29th SIGIR.

S. N. Kim, L. Wang, and T. Baldwin. 2010. Tagging and
linking web forum posts. In Proc. Fourteenth Conference
on Computational Natural Language Learning, CoNLL
’10, pages 192–202. Association for Computational Lin-
guistics.

S. N. Kim, L. Cavedon, and T. Baldwin. 2012. Classifying
dialogue acts in multi-party live chats. In Proc. 26th Pacic
Asia Conference on Language, Information and Compu-
tation, pages 463–472.

S. Kullback. 1997. Information Theory and Statistics. Dover
Publications.

H. Lakkaraju, C. Bhattacharyya, I. Bhattacharya, and
S. Merugu. 2011. Exploiting coherence for the simul-
taneous discovery of latent facets and associated senti-
ments. In Proc. 11th SIAM International Conference on
Data Mining, SDM ’11, pages 498–509.

J. Pei, J. Han, B. Mortazavi-asl, H. Pinto, Q. Chen, U. Dayal,
and M. Hsu. 2001. Prefixspan: Mining sequential pat-
terns efficiently by prefix-projected pattern growth. In
Proc. 17th International Conference on Data Engineer-
ing. IEEE Computer Society.

M.F. Porter. 1980. An algorithm for suffix stripping. In
Program.

P. Raghavan, R. Catherine, S. Ikbal, N. Kambhatla, and
D. Majumdar. 2010. Extracting problem and resolution
information from online discussion forums. In Proc. 16th
International Conference on Management of Data, CO-
MAD.

S. Sarawagi, S. Chakrabarti, and S. Godbole. 2003. Cross-
training: learning probabilistic mappings between topics.
In Proc. ninth ACM SIGKDD international conference on
Knowledge discovery and data mining, KDD ’03, pages
177–186.

S. Sarencheh, V. Potdar, E. Yeganeh, and N. Firoozeh.
2010. Semi-automatic information extraction from dis-
cussion boards with applications for anti-spam technol-
ogy. In Computational Science and Its Applications -
ICCSA 2010, volume 6017 of Lecture Notes in Computer
Science, pages 370–382. Springer Berlin Heidelberg.

A. Sarkar. 2001. Applying co-training methods to statistical
parsing. In Proc. second meeting of the North American
Chapter of the Association for Computational Linguistics
on Language technologies, NAACL ’01.

J. Seo, B. Croft, and D. A. Smith. 2009. Online community
search using thread structure. In Proceeding of the 18th
ACM conference on Information and knowledge manage-
ment, CIKM ’09, pages 1907–1910. ACM.

L. Shrestha and K. McKeown. 2004. Detection of
question-answer pairs in email conversation. In Proc.
20th International Conference on Computational LIN-
Guistic(COLING).

C. J. van Rijsbergen. 1979. Information Retrieval (2nd ed.).
Butterworth.

V. N. Vapnik. 1995. The Nature of Statistical Learning The-
ory. Springer.

Y. Wang and C. P. Rosé. 2010. Making conversational struc-
ture explicit: identification of initiation-response pairs
within online discussions. In Human Language Technolo-
gies: The 2010 Annual Conference of the North American
Chapter of the Association for Computational Linguistics,
HLT ’10, pages 673–676. Association for Computational
Linguistics.

H. Wang, C. Wang, C. Zhai, and J. Han. 2011. Learning
online discussion structures by conditional random fields.
In Proc. 34th international ACM SIGIR conference on Re-
search and development in Information Retrieval, SIGIR
’11, pages 435–444.

X. Xue, J. Jeon, and W. B. Croft. 2008. Retrieval models for
question and answer archives. In Proc. 31st annual inter-
national ACM SIGIR conference on Research and devel-
opment in information retrieval, SIGIR ’08, pages 475–
482. ACM.

W. Yang, Y. Cao, and C. Lin. 2009. A structural support vec-
tor method for extracting contexts and answers of ques-
tions from online forums. In Proc. 2009 Conference on
Empirical Methods in Natural Language Processing: Vol-
ume 2 - Volume 2, EMNLP ’09, pages 514–523.

9


