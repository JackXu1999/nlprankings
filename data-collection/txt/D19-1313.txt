



















































Exploring Diverse Expressions for Paraphrase Generation


Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing
and the 9th International Joint Conference on Natural Language Processing, pages 3173–3182,
Hong Kong, China, November 3–7, 2019. c©2019 Association for Computational Linguistics

3173

Exploring Diverse Expressions for Paraphrase Generation

Lihua Qian1, Lin Qiu1, Weinan Zhang1, Xin Jiang2, Yong Yu1
1Shanghai Jiao Tong University

{qianlihua, lqiu, wnzhang, yyu}@apex.sjtu.edu.cn
2Noah’s Ark Lab, Huawei Technologies

jiang.xin@huawei.com

Abstract

Paraphrasing plays an important role in vari-
ous natural language processing (NLP) tasks,
such as question answering, information re-
trieval and sentence simplification. Recently,
neural generative models have shown promis-
ing results in paraphrase generation. Howev-
er, prior work mainly focused on single para-
phrase generation, while ignoring the fact that
diversity is essential for enhancing generaliza-
tion capability and robustness of downstream
applications. Few works have been done to
solve diverse paraphrase generation. In this
paper, we propose a novel approach with t-
wo discriminators and multiple generators to
generate a variety of different paraphrases. A
reinforcement learning algorithm is applied to
train our model. Our experiments on two real-
world datasets demonstrate that our model not
only gains a significant increase in diversity
but also improves generation quality over sev-
eral state-of-the-art baselines.

1 Introduction

Paraphrases refer to texts with identical meaning
but expressed in different ways. Paraphrase gen-
eration has a wide range of applications. For ex-
ample, paraphrases can be used to extend the ref-
erence texts for the automatic evaluation of text
generation tasks, which make the evaluation more
robust and accurate (Madnani and Dorr, 2010). In
addition to increasing reference texts, paraphrases
can also augment training data for text classifica-
tion problems, especially for tasks lack of training
data. Besides, paraphrases can be used as varia-
tions of queries for information retrieval and ques-
tion answering systems. Query paraphrasing can
narrow the gap between system comprehension
and user questions (Fader et al., 2014; Yin et al.,
2015). And systems can obtain better results from
multiple feedbacks for different query variants.

Traditional paraphrase generation approaches
leverage manual-crafted rules (McKeown, 1983;
Hassan et al., 2007). In recent years, sequence to
sequence learning techniques have achieved im-
provements in a variety of NLP tasks, such as
machine translation (Bahdanau et al., 2014; Cho
et al., 2014), text summarization (Nallapati et al.,
2016) and dialogue systems (Serban et al., 2017).
Specifically, the paraphrase generation can also
be formulated as a sequence to sequence prob-
lem. Existing neural paraphrase generation works
(Prakash et al., 2016; Gupta et al., 2018; Li et al.,
2018) also demonstrate the effectiveness of deep
learning method.

Although prior paraphrase generation method-
s mainly consider only one single output for each
input, the nature of paraphrasing indicates that we
can paraphrase one sentence into several different
sentences. Therefore, we hope to generate a vari-
ety of paraphrases while ensuring quality. How-
ever, most of the previous work dedicated to im-
proving the quality of top generated sentence, but
did not conduct in-depth research on the diversi-
ty of paraphrase generation. In fact, diversity is
very important in many applications. More diverse
paraphrases will be beneficial to robustness and
accuracy of automatic text evaluation, text clas-
sification, and can avoid the blandness caused by
repetitive patterns.

A straightforward way to generate multiple d-
ifferent sentences is using beam search to select
the top k sentences. However, beam search gener-
ates suboptimal sentences, and the resulting sen-
tences are very similar to each other. Gupta et al.
(2018) employ a variational auto-encoder frame-
work to produce multiple sentences according to
different noise input. Xu et al. (2018b) learn a
shared decoder with different decoder embeddings
for various decoding pattern. Although we can get
multiple outputs with different noise inputs or de-



3174

coder embeddings, different outputs may have s-
light changes or even no difference.

In order to address the problem, in this pa-
per, we utilize multiple generators to generate di-
verse paraphrases without sacrificing quality. Re-
inforcement learning makes it possible for every
generator to explore different generation pattern,
breaking the restriction of learning the same tar-
get sentences given by data in supervised learning.
Thus, we design a reinforcement learning algorith-
m to guide the models learning procedure. The
proposed framework contains two discriminators.
One discriminator is to examine whether two sen-
tences convey the same meaning, the other one is
to distinguish the sentence is generated by which
generator. The higher the confidence of one text be
judged as generated by a certain generator is, in-
dicating the more obvious the differences between
that text and texts generated by other generators
are. We combine the scores of the two discrim-
inators as guiding signals to the generators using
reinforcement learning. Therefore, generators can
learn to synthesize texts with apparent differences
and good quality.

Our approach can automatically discover the so-
lution to diverse paraphrase generation through the
jointly learning of the discriminators and gener-
ators. And the model uses the same data pairs
as in learning mono-paraphrase generation with-
out extra annotations. Experiments on two real-
world datasets show that our model improves both
in quality and diversity. There are mainly two con-
tributions in our work: the first one is proposing a
novel framework as well as a corresponding learn-
ing procedure to generate various paraphrases, the
second one is performing experiments demonstrat-
ing that our method achieves improvement regard-
ing both generation diversity and quality.

2 Related Work

Paraphrase generation In the early years, vari-
ous traditional techniques have been developed to
solve the paraphrase generation problem. McKe-
own (1983) makes use of manually defined rules.
Hassan et al. (2007) use thesaurus-based method
for lexical substitution. And statistical machine
translation (SMT) has also been used. Quirk
et al. (2004) train SMT tools on a large number
of sentence pairs collected from newspapers.
Wubben et al. (2010) propose a phrase-based
SMT model trained on aligned news headlines.

Zhao et al. (2008) utilize multiple resources to
strengthen a log-linear SMT model. Recently,
deep neural models have also been applied to
paraphrase generation due to their great success
on natural language processing tasks. Prakash
et al. (2016) design a deep stacked network with
residual connections, Gupta et al. (2018) propose
a conditional variational auto-encoder which
can produce multiple paraphrases, Iyyer et al.
(2018) learn a model to generate syntactically
controlled paraphrase, Li et al. (2018) propose
a generator-evaluator architecture coupled with
deep reinforcement learning. Although being
similar to the architecture proposed in (Li et al.,
2018), our framework targets a different goal.
Our work focuses on generating multiple diverse
paraphrases, while theirs dedicates to improving
the quality of the top generated paraphrase. The
main difference is that our approach utilizes a
generator discriminator to encourage more diverse
paraphrases, which is essential for diversity.
Diverse text generation A few works have
explored to produce diverse generation by chang-
ing decoding schemes or introducing random
noise. Methods that change decoding schemes
are orthogonal and complementary to our work.
Li et al. (2016) modify the score function in
decoding to encourage usage of novel words and
penalize words after the same partially generated
sentence. Dai et al. (2017) utilize conditional
generative adversarial network (GAN) to generate
diverse image caption according to the input
noise. Gupta et al. (2018) employ a variational
auto-encoder framework with both encoder and
decoder conditioned on source input. Shi et al.
(2018) employ inverse reinforcement learning for
unconditional diverse text generation. Xu et al.
(2018a) propose a modified GAN to generate di-
verse and informative outputs for different inputs.
Xu et al. (2018b) train a generator with different
embeddings to generate multiple paraphrases,
only the decoder embedding with lowest cross
entropy can get updated.

3 Methods

3.1 Problem Definition

Given a source sentence X , we aim to generate
multiple target sentences S = {Y1, Y2, ..., Yk}.
Specifically, our goal is to learn a model which
can generate S such that every Yi (i=1..k) has the



3175

same meaning as X and obvious differences be-
tween each other.

3.2 Models
Our diverse paraphrase generation framework
consists of three parts: one paraphrase discrim-
inator, one generator discriminator and multiple
generators. Paraphrase discriminator determines
whether two sentences have the same meaning,
and the generator discriminator determines the
sentence is generated by which generator. The
goal of multiple paraphrase generators is to gener-
ate sentences that are judged as paraphrases of the
input sentence, and the differences between sen-
tences should be as large as possible. The intuition
is that sentences with quite different expressions
can be easily distinguished by the generator dis-
criminator and will be judged as generated by the
corresponding generator with high confidence.

3.2.1 Paraphrase generator
We frame paraphrase generation as a sequence-
to-sequence (Seq2Seq) problem and adopt the
encoder-decoder framework. Multiple generators
share the same encoder yet with their own decoder.
The generation posterior of the i-th generator is:

Gi(Y |X) =
T∏
t=1

P (yt|y1:t−1, X; θi),

where y1:t−1 indicates the subsequence from
timestep 1 to timestep t− 1, T denotes the length
of output sentence Y , θi represents the parame-
ter of the i-th generator, we omit θi in the follow-
ing illustration for conciseness. We use pointer-
generator (See et al., 2017) for our generators to
alleviate the out-of-vocabulary problem and en-
hance the performance. In pointer-generator, the
decoder can generate not only words from the
chosen vocabulary as standard decoders but also
words from the input sentences by copying them
from the input. The probability of copying words
can be represented as

pcopy(yt|y1:t−1, X) = ati,

ati =
exp(f(st−1, hi))∑
i exp(f(st−1, hi))

,

where st−1 is the decoder state at timestep t − 1,
hi is the encoder hidden state at timestep i. pcopy
is actually ati, which is the attention distribution at
timestep t. We use attention in (Bahdanau et al.,

Figure 1: The overall framework

2014), thus f is a linear function. The probability
of extracting the next word from vocabulary is:

pvocab(yt|y1:t−1, X) = g(st−1, yt−1, ct),

ct =
∑
i

ati ∗ hi,

where yt−1 is the word generated last step, ct
means the context vector at timestep t, g is a linear
function with softmax output. The overall proba-
bility of the next word is

p(yt|y1:t−1, X) = pgen∗pvocab+(1−pgen)∗pcopy,

pgen = m(st, yt−1, ct)

where m is a linear transformation with sigmoid
activation output. pgen acts as a gate to control
whether the next word is generated from vocabu-
lary or is copied from the input.

3.2.2 Paraphrase discriminator
We follow (Li et al., 2018) to cast paraphrase
recognition as a sentence matching problem. In
our work, we use a model with two recurrent neu-
ral networks (RNN). Assume giving a pair of sen-
tences (X,Y ), paraphrase discriminator will score
a value to evaluate the semantic similarity between
X and Y .

α = RNNα(X), β = RNNβ(Y ),

γ = tanh(Wα ∗ α+ bα) ∗ tanh(Wβ ∗ β + bβ),

Dp = softmax(Wp ∗ fc(γ) + bp),

sentence X and sentence Y are encoded using
RNN-based encoder. The encoded representations
α and β are integrated into γ. Then we input γ
into a fully connected network fc and finally out-
put a softmax classification probability, which rep-
resents the degree of meaning similarity between



3176

X and Y . We optimize the cross entropy loss for
paraphrase discriminator.

L = − log(Dp(X,Y+))− log(1−Dp(X,Y−)),

where(X,Y−) means sentence Y− is not a para-
phrase of X and (X,Y+) means sentence Y+ is
a paraphrase of X . Paraphrase discriminator sep-
arate the negative sentence pairs from the positive
ones.

3.2.3 Generator discriminator
In order to distinguish sentences generated by
different generators, we learn a sentence dis-
criminator. Suppose there is a set of sentences
{S1, S2, ..., Sk}, where Si denotes sentences gen-
erated by the i-th generator. We hope our gen-
erator discriminator can separate sentences in Si
(i = 1..k) from sentences generated by other gen-
erators. If there are obvious differences between
sentences in Si and those generated by other gen-
erators, generator discriminator can determine the
sentence is generated by the i-th generator with
high confidence. Otherwise, if sentences in Si are
very similar to those generated by other genera-
tors, generator discriminator will get confused and
misclassify the sentences into another generator,
thus confidence will be low as well. For simplici-
ty, we use a RNN to extract feature from sentences
and use a softmax output as activation function.

e = RNN(Y ), Dg(y|Y ) = softmax(e)

where e indicates the sentence feature and y means
which generator generates Y . The generator dis-
criminator is pretrained using data generated by
the pretrained generators. During the reinforce-
ment learning phase, generator discriminator up-
dates iteratively using new samples produced by
all generators.

3.3 Learning
3.3.1 Maximum likelihood estimation
The most typical technique to train a Seq2Seq
model is teacher forcing (Williams and Zipser,
1989), which is also referred to as maximum like-
lihood estimation (MLE).

L =

T∑
t=1

log p(y∗t |y∗1:t−1, X) (1)

where y∗t denotes the t-th word in the target sen-
tence given by training sample. We pretrain all

Algorithm 1: Training Procedure of our
framework

Input : A data with paraphrase pairs {(X,Y )} and
non-paraphrase pairs {(X,Y−)}

Output : Generators {Gi}i=ki=1
1 Random initialize all generators {Gi}i=ki=1 ;
2 Pre-train all generators Gi with {(X,Y )};
3 Generate sentences {Si}i=ki=1 , Si = {Ŷ }i using Gi;
4 Train paraphrase discriminator with {(X,Y )} and
{(X,Y−)};

5 Pre-train generator discriminator with {Si}i=ki=1 ;
6 while not converge do
7 Sample sentences {X} from the paraphrase corpus;
8 for i = 1 To k do
9 Using Gi to generate sentences {Ŷ }i by

sampling and {Ȳ }i by greedy search;
10 Compute the rewards using Eq(3);
11 Update generators by minimizing Eq(4);
12 end
13 Generate sentences {Si}i=ki=1 using {Gi}i=ki=1 ;
14 Update generator discriminator with {Si}i=ki=1 by

minimizing Eq(7);
15 end
16 Return {Gi}i=ki=1

generators using teacher forcing. However, teach-
er forcing suffers from exposure bias (Ranzato
et al., 2015). During training, the next word to
generate is conditioned on the correctly annotat-
ed word subsequence. But in the inference stage,
the model predicts the next word conditioned on
words generated by itself, which causes inconsis-
tency between training and inference. As model
may generate wrong words, future generation con-
ditioned on the generated subsequence will also be
affected, leading to accumulated errors. One so-
lution to this problem is finetuning generators via
reinforcement learning after pretraining.

3.3.2 Reinforcement learning
Regarding a generator as an agent interacting with
the environment (i.e. the word input and context
vector). The objective function of the generator
can be written as

L =
∑
Ŷ

R(Ŷ ), Ŷ ∼ πθ (2)

Ŷ indicates the word sequence sampled from the
generator. R is the reward function, and a batch of
sentences is sampled to approximate the objective
function since it is intractable to use all sentences.
The optimization objective is to minimize the neg-
ative expected rewards of generated texts, in other
words, maximize the expected rewards.



3177

The reward of each sentence is determined by
the paraphrase discriminator and the generator dis-
criminator together. The reward of the i-th gener-
ator is

Ri = Dp(X,Yi)�Dg(y = i|Yi), (3)

where Yi indicates the sentence generated by the
i-th generator. The i-th generator can get a high
reward only when sentence Yi is recognized as a
paraphrase of X by the paraphrase discriminator
and the differences between Yi and sentences gen-
erated by other generators is so large that generator
discriminator takes Yi as a sentence generated by
the i-th generator.

Specifically, we use the self-critical algorithm
(Rennie et al., 2017) as our reinforcement learning
method. In order to reduce the variance of policy
gradient method, a typical technique is subtract-
ing baseline values from the original rewards. In
the self-critical algorithm, the baseline is the re-
ward of sentences generated in inference. At each
training iteration, the generator generates two sen-
tences. One sentence is sampled from the out-
put probability distribution, ŷt ∼ p(yt|ŷ1:1−t, X).
The other sentence is generated by greedy search,
ȳt = arg maxyt P (yt|ȳ1:t−1, X), the word with
the highest probability is selected at each step. The
reward of the sentence generated by greedy search
is used as a baseline. We use rewards subtracted
by baseline values to update the model

L =
T∑
t=1

(R(ŷ)−R(ȳ)) log p(yt|y1:t−1, X), (4)

which means that we compare the rewards of the
sampled sentences ŷ and those of the sentences ȳ
which are produced by greedy search. Only the
generated sentences that outperform the sentence
ȳ are given positive signals. As a result, Reward-
s will be increased as the generators increase the
generation probability of better sequences while
decreasing the chances of worse sequence genera-
tion.

3.3.3 The overall objective
Our goal is to generate multiple diverse paraphras-
es of high quality. We illustrate the reason why our
model can achieve this goal from the aspect of the
whole optimization objective. Given input X , we
hope to generate multiple high-quality sentences
Y . We make the assumption that y is only depen-
dent on Y , that is, Dg(y|Y,X) = Dg(y|Y ). The

weaker condition forces the generator discrimina-
tor to only focus on the generated sentences and
encourages stronger diversity. The overall objec-
tive can be written as:∑

Y

R(X,Y )P (Y |X)

=
∑
Y

∑
y

R(X,Y )P (Y, y|X)

=
∑
Y

k∑
i=1

R(X,Y )Dg(y = i|Y )Gi(Y |X)

(5)

where R(X,Y ) is the reward evaluating output Y
for input sentence X . Multiple generators Gi gen-
erate their own sequences for each input and Dg
learns to separate sentences generated by differen-
t generators. We can define different rewards for
corresponding applications and we use Dp(X,Y )
for our paraphrase task. We define θd as the pa-
rameter of generator discriminator and θi as the
parameter of the i-th generator. The derivative of
J(θ) is computed as follow:

∇θJ(θ)

=

k∑
i=1

∑
Y

[R(X,Y )Gi(Y |X)∇θdDg(y = i|Y )

+R(X,Y )Dg(y = i|Y )∇θiGi(Y |X)]

=
k∑
i=1

EGi(Y |X)[R(X,Y )∇θdDg(y = i|Y )

+R(X,Y )Dg(y = i|Y )∇θi log(Gi(Y |X))]
(6)

Since the gradient can not be propagated back-
ward to Gi and Dg by gradient descent, we use
reinforcement learning to update both Dg and Gi.
And directly optimizing the above objective will
reduce the reward of a good output Y ∗ to zero for
all generators except the one with the highest gen-
eration probability for this output Y ∗. We should
lower the reward of that good output for other gen-
erators, but reducing the reward of Y ∗ to zero will
affect the stability of other generators. To address
this issue, we can regularizeDg by adding entropy
or we propose to scale the gradient ofDg by 1/Dg.
The derived cost function of generator discrimina-
tor is:

L = −
k∑
i=1

EGi(Y |X)R(X,Y ) log(Dg(y = i|Y ))

(7)



3178

The outputs generated by Gi are not equal-
ly contributed to Dg(y = i|Y ), outputs that
have better quality are more encouraged by Dg.
High-quality sentences Y generated by Gi max-
imize Dg(y = i|Y ) while minimizing Dg(y =
j|Y ) (j 6= i) to force other generators to generate
different sentences. The i-th generator maximizes
R(X,Y )Dg(y = i|Y ) so that sentences generated
by the i-th generator satisfy both the quality and
diversity requirement. The complete learning pro-
cedure is shown in algorithm 1.

4 Experiment

4.1 Dataset

Generator Paraphrase discriminator

Dataset #Train #Validation #Test #Positive #Negative

Quora 100K 3K 30K 100K 160K
Twitter 110K 1K 5K 10K 40K

Table 1: Statistics of datasets.

To validate our model’s capability to gener-
ate diverse paraphrases, we conduct experiments
on two real-world datasets: Quora question para-
phrase dataset 1 and Twitter URL paraphrasing
dataset (Lan et al., 2017).

Quora dataset contains a large corpus of ques-
tion pairs collected from Quora. The Quora
dataset totally contains about 400k data pairs. Al-
l the data are manually labeled whether two sen-
tences convey the identical meaning. We random-
ly select 100k pairs for training, 3k for validation
and 30k for test. The twitter dataset contains sen-
tence pairs collected from Twitter. It also marks
whether the sentence pairs are paraphrases. But
the difference is that some of them are manually
labeled and some are automatically labeled by the
algorithm. For generators, we split test set and val-
idation set from the manually labeled subset. The
training set contains the rest data from the manual-
ly labeled subset and data from the automatically
labeled paraphrase pairs. In order to avoid bias in-
duced by the labelling algorithm, we only use the
manually labeled data for the paraphrase discrim-
inator. A summary of dataset statistics is given in
Table 1.

4.2 Evaluation metric and baseline
We evaluate our model’s performance for both
quality and diversity. For sentence quality, we
1 https://www.kaggle.com/c/

quora-question-pairs

use two commonly used metrics: BLEU (Pap-
ineni et al., 2002) and METEOR (Lavie and A-
garwal, 2007). BLEU measures word overlap-
ping between generated sentences and reference
texts. METEOR introduces stemming and syn-
onym matching to alleviate problems caused by
only exact matching in BLEU. BLEU and ME-
TEOR scores are averaged for multiple generat-
ed sentences. To evaluate sentence quality more
reliably, we also conduct human evaluations to
compare the quality of sentences generated by d-
ifferent models. For diversity, we use self-BLEU
(Zhu et al., 2018). To compute self-BLEU of a set
of outputs, every output is picked once as candi-
date and others are used as references to compute
BLEU score. And the average of all the BLEU s-
cores computed for this set is self-BLEU. A lower
self-BLEU score indicates better diversity.

Several basic models and existing neural-based
methods are used as baseline models for compari-
son. For both datasets, we report results of atten-
tive Seq2Seq (Bahdanau et al., 2014), variational
auto-encoder (VAE-SVG-eq) (Gupta et al., 2018),
conditional GAN (Dai et al., 2017) with the gen-
erator replaced by Seq2Seq, diversity-promoting
GAN (DP-GAN) (Xu et al., 2018a), the gen-
erator with inverse reinforcement learning (IRL)
(Shi et al., 2018), the generator with multiple de-
coder embeddings (D-PAGE) (Xu et al., 2018b)
and pointer-generator (See et al., 2017). We use
beam search for attentive Seq2Seq (Seq2Seq-BS),
and pointer-generator (PG-BS) to generate multi-
ple outputs. Top three outputs of each method are
obtained for evaluation.

4.3 Implementation

We use the following experimental setting for our
model. For generators, we randomly initialize al-
l generators and train them by teacher forcing in
the same way. We use three generators and utilize
single-layer LSTM (Hochreiter and Schmidhuber,
1997) for both encoder and decoders. The hidden
dimension of encoder and decoders is set to 256,
the word embedding dimension is 128. We use
Adam optimizer (Duchi et al., 2011) with a learn-
ing rate of 1e-3 in MLE pretrain. In reinforcement
learning, the learning rate decreases to 2e-5 and
the batch size is fixed at 32. We also use ground-
truth with reward of Dg to stabilize the training.
We strengthen the performance of D-PAGE by re-
placing the basic Seq2Seq with pointer-generator.

https://www.kaggle.com/c/quora-question-pairs
https://www.kaggle.com/c/quora-question-pairs


3179

Quora (Quality) Quora (Diversity)

Models BLEU2↑ BLEU3↑ BLEU4↑ METEOR↑ self-BLEU2↓ self-BLEU3↓ self-BLEU4↓

Seq2Seq-BS 37.03 27.53 21.11 27.78 73.89 66.51 59.33
VAE-SVG-eq 34.78 25.96 19.97 26.45 89.82 87.53 85.43
GAN 36.22 27.27 21.12 26.63 92.85 91.22 89.76
DP-GAN 36.60 26.92 20.42 28.17 73.76 66.37 59.12
IRL 35.53 26.19 19.94 27.29 76.76 70.22 63.59
D-PAGE 38.70 29.23 22.66 28.54 90.10 87.66 85.41
PG-BS 38.91 29.26 22.54 28.88 75.60 68.72 61.89

ours 36.84 27.46 20.98 29.28 56.38 47.76 40.55

Table 2: Performances on Quora dataset.

Twitter (Quality) Twitter (Diversity)

Models BLEU2↑ BLEU3↑ BLEU4↑ METEOR↑ self-BLEU2↓ self-BLEU3↓ self-BLEU4↓

Seq2Seq-BS 32.69 28.25 24.99 22.51 82.79 80.29 77.98
VAE-SVG-eq 26.43 23.04 20.57 18.34 91.46 90.51 89.67
GAN 23.10 20.45 18.47 15.33 94.35 93.75 93.22
DP-GAN 33.07 28.68 25.49 22.84 82.52 80.05 77.63
IRL 32.96 28.60 25.39 22.72 83.53 81.22 78.95
D-PAGE 32.95 28.82 25.88 22.59 88.35 86.45 84.76
PG-BS 33.86 29.42 26.15 23.52 82.57 79.99 77.48

ours 34.23 29.66 26.38 24.29 65.83 61.17 57.45

Table 3: Performances on Twitter dataset.

Quora Twitter

Models Fluency Consistency Fluency Consistency

D-PAGE 4.21 3.44 3.66 3.08
PG-BS 4.20 3.34 3.85 3.17
DP-GAN 4.27 3.49 4.09 3.30

ours 4.57 3.82 4.24 3.59

Table 4: Human evaluation results.

For paraphrase discriminator and generator dis-
criminator, the sentence encoders are all built with
one-layer LSTM with 256-dimensional hidden u-
nits and the dimension of word embeddings is set
to 256. For each of the two discriminators, Adam
optimizer with a learning rate of 1e-4 is used to
optimize the loss function. The batch size is kep-
t at 64 for paraphrase discriminator. We employ
a 3-layer fully connected network with ReLU ac-
tivation in paraphrase discriminator and set the
dropout to 0.2.

5 Results

Results of automatic evaluation on Quora is shown
in Table 2. Our model achieves the best METEOR
score among all the models. As BLEU only calcu-
late the exact overlapping between generated sen-
tences and reference sentences, lower BLEU score
does not necessarily indicate bad quality. In addi-
tion, every test case only has one reference text,
which makes automatic evaluation more difficult

to measure the real quality of multiple generated
sentences. Since METEOR compares the generat-
ed sentences with the reference text using not only
exact word matching but also stemming and syn-
onym matching, we believe it measures the gener-
ation quality more accurately.

For human evaluation, we randomly select 100
input sentences from the test set of each dataset
and get the generated results of different models
for these inputs. We follow the human evaluation
guideline in (Li et al., 2018). The sentence pairs
are scored for two aspects of the generated result-
s: fluency (whether the generated sentence is flu-
ent and grammatical) and consistency (the mean-
ing similarity between the input sentence and the
generated sentence). Each output is given two rat-
ings, scaling from 1 to 5, on fluency and consis-
tency separately. An input and outputs generat-
ed for the input from different systems form a test
group. The outputs from different systems in the
test group are shuffled. The test groups are ran-
domly assigned to annotators. Every test group is
evaluated by two annotators and scores for each
output are averaged. The agreement between an-
notators is moderate (kappa=0.54). The final score
for each system is the average score for all outputs.
Table 4 shows our model achieves better scores in
both meaning similarity and fluency than baseline
models.



3180

Source How do you know you ’re in love again ?

PG-BS
How do you know you are in love with someone ?
How do you know if you ’re in love with someone ?
How do you know if you are in love with someone ?

ours
How do you know if you ’re in love with someone ?
What are some ways to know that you are in love with someone ?
How can you know if you ’re in love ?

Source How does this demonetisation of 500 & 1000 rupee notes help to reduce the price of real estate ?

PG-BS
What will be the effect of banning 500 and 1000 notes on the Indian economy ?
What will be the effect of banning 500 and 1000 notes on the stock markets ?
What will be the effect of banning 500 and 1000 notes on the real estate sector ?

ours
What will be the effect of banning 500 and 1000 notes on real estate sector in India ?
How does the declaration that Rs 500 and Rs 1000 notes would not be accepted as valid transactions
affect real estate in India ?
How can the ban of 500 and 1000 rupee notes help in curbing the price of real estate ?

Source I want to write a book - where should I start ?

PG-BS
I want to write a book , where should I start ?
I want to write a book , how should I start ?
I want to write a book , where can I start ?

ours
How do I write a book ?
What should I do to write a book - where should I start ?
I want to write a book , where should I start ?

Table 5: Example paraphrases on Quora

In terms of diversity, our approach outperform-
s other methods by a large margin. VAE-SVG-
eq, GAN and D-PAGE do not show better result-
s than beam search. As discussed in (Xu et al.,
2018b), beam search guarantees that multiple gen-
erated sentences are different from each other by at
least one word, which yields better diversity. GAN
methods often suffer from mode collapse problem-
s and generate similar outputs. For VAE-SVG-eq,
different random noise sampled from a Gaussian
prior is used identically in training, so different
noise inputs may produce the same sentences. The
generator with IRL and DP-GAN have no obvious
improvement in diversity in our task. Although
the generator with IRL and DP-GAN also claim to
improve diversity, their goal is essentially differ-
ent from ours. The generator with IRL was pro-
posed to tackle the unconditional text generation
problem, where texts are generated without any
input or condition. DP-GAN generates a more in-
formative output for each input to avoid repetitive
response for different inputs. In the experimen-
t, we notice that VAE-SVG-eq and GAN generate
similar or even identical sentences according to d-
ifferent random noise, which is consistent with the
results posted in (Xu et al., 2018b). D-PAGE may
able to find different patterns for very short sen-
tences or phrases, but results in our experiments
show that D-PAGE does not separate different pat-
terns in longer sentences and the generated sen-
tences lack diversity.

Table 3 shows scores on Twitter dataset. Our

model achieves the best BLEU score and outper-
forms all baselines for METEOR. The human e-
valuation results presented in table 4 also show
that our model produces better quality paraphras-
es. For diversity, our model again performs con-
siderably better than other models. Since part
of sentence pairs in Twitter dataset may be mis-
labelled by the labeling algorithm, it is harder
to generate high-quality paraphrases on Twitter.
Paraphrases generated by our model have not on-
ly more diverse expression but also better quality
compared to those generated by other models.

Table 5 demonstrates some examples generat-
ed by pointer-generator using beam search and our
model. Although pointer-generator indeed gener-
ates different sentences, it only yields little diver-
sity. Sentences produced by pointer-generator can
be very similar, many of them only differ from oth-
ers by one word. Compared to sentences generated
by pointer-generator, those produced by our mod-
el has obvious differences between each other. In
the first example, our model uses different types of
interrogative. In contrast, pointer-generator gen-
erates sentences that are almost the same. In the
second example, pointer-generator generates sim-
ilar sentences and yields obvious meaning changes
from the input sentence. The sentences generated
by our model are in different expressions and more
relevant to the input sentence. In the third exam-
ple, the sentences generated by pointer-generator
show minor variations, while those generated by
our model present richer expressions.



3181

6 Conclusion

In this paper, we present a novel approach to gen-
erate diverse paraphrases. Multiple generators are
pretrained in the same way, then learn different
generation pattern and enhance generation quality
via reinforcement learning. Paraphrase discrimi-
nator evaluates the meaning similarity between t-
wo sentences, generator discriminator distinguish-
es sentences generated from different generators.
Experiments show that our model has better re-
sults concerning both diversity and quality. In the
future, we would like to apply our framework to
increase reference texts for automatic evaluation
or augment training data for text classification.

Acknowledgments

The work is sponsored by Huawei Innovation
Research Program. The corresponding authors
Weinan Zhang and Yong Yu are also supported
by NSFC (61702327, 61772333, 61632017). We
thank the anonymous reviewers for their valuable
comments.

References
Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben-

gio. 2014. Neural machine translation by jointly
learning to align and translate. arXiv preprint arX-
iv:1409.0473.

Kyunghyun Cho, Bart van Merrienboer, Caglar Gul-
cehre, Dzmitry Bahdanau, Fethi Bougares, Holger
Schwenk, and Yoshua Bengio. 2014. Learning
phrase representations using rnn encoder–decoder
for statistical machine translation. In Proceedings of
the 2014 Conference on Empirical Methods in Nat-
ural Language Processing (EMNLP), pages 1724–
1734.

Bo Dai, Sanja Fidler, Raquel Urtasun, and Dahua Lin.
2017. Towards diverse and natural image descrip-
tions via a conditional gan. In Proceedings of the
IEEE International Conference on Computer Vision,
pages 2970–2979.

John Duchi, Elad Hazan, and Yoram Singer. 2011.
Adaptive subgradient methods for online learning
and stochastic optimization. Journal of Machine
Learning Research, 12(Jul):2121–2159.

Anthony Fader, Luke Zettlemoyer, and Oren Etzion-
i. 2014. Open question answering over curated
and extracted knowledge bases. In Proceedings of
the 20th ACM SIGKDD international conference on
Knowledge discovery and data mining, pages 1156–
1165. ACM.

Ankush Gupta, Arvind Agarwal, Prawaan Singh, and
Piyush Rai. 2018. A deep generative framework for
paraphrase generation. In Thirty-Second AAAI Con-
ference on Artificial Intelligence.

Samer Hassan, Andras Csomai, Carmen Banea, Ravi
Sinha, and Rada Mihalcea. 2007. Unt: Subfinder:
Combining knowledge sources for automatic lexical
substitution. In Proceedings of the 4th International
Workshop on Semantic Evaluations, pages 410–413.
Association for Computational Linguistics.

Sepp Hochreiter and Jürgen Schmidhuber. 1997.
Long short-term memory. Neural computation,
9(8):1735–1780.

Mohit Iyyer, John Wieting, Kevin Gimpel, and Luke
Zettlemoyer. 2018. Adversarial example generation
with syntactically controlled paraphrase networks.
In Proceedings of the 2018 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics: Human Language Technologies,
Volume 1 (Long Papers), pages 1875–1885.

Wuwei Lan, Siyu Qiu, Hua He, and Wei Xu. 2017.
A continuously growing dataset of sentential para-
phrases. In Proceedings of the 2017 Conference on
Empirical Methods in Natural Language Process-
ing, pages 1224–1234.

Alon Lavie and Abhaya Agarwal. 2007. Meteor: An
automatic metric for mt evaluation with high levels
of correlation with human judgments. In Proceed-
ings of the Second Workshop on Statistical Machine
Translation, pages 228–231. Association for Com-
putational Linguistics.

Jiwei Li, Will Monroe, and Dan Jurafsky. 2016. A sim-
ple, fast diverse decoding algorithm for neural gen-
eration. arXiv preprint arXiv:1611.08562.

Zichao Li, Xin Jiang, Lifeng Shang, and Hang Li.
2018. Paraphrase generation with deep reinforce-
ment learning. In Proceedings of the 2018 Con-
ference on Empirical Methods in Natural Language
Processing, pages 3865–3878.

Nitin Madnani and Bonnie J Dorr. 2010. Generat-
ing phrasal and sentential paraphrases: A survey
of data-driven methods. Computational Linguistics,
36(3):341–387.

Kathleen R McKeown. 1983. Paraphrasing questions
using given and new information. Computational
Linguistics, 9(1):1–10.

Ramesh Nallapati, Bowen Zhou, Cicero dos Santos,
Ça glar Gulçehre, and Bing Xiang. 2016. Abstrac-
tive text summarization using sequence-to-sequence
rnns and beyond. CoNLL 2016, page 280.

Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic e-
valuation of machine translation. In Proceedings of
the 40th annual meeting on association for compu-
tational linguistics, pages 311–318. Association for
Computational Linguistics.



3182

Aaditya Prakash, Sadid A Hasan, Kathy Lee, Vivek
Datla, Ashequl Qadir, Joey Liu, and Oladimeji Far-
ri. 2016. Neural paraphrase generation with s-
tacked residual lstm networks. arXiv preprint arX-
iv:1610.03098.

Chris Quirk, Chris Brockett, and Bill Dolan. 2004.
Monolingual machine translation for paraphrase
generation.

Marc’Aurelio Ranzato, Sumit Chopra, Michael Auli,
and Wojciech Zaremba. 2015. Sequence level train-
ing with recurrent neural networks. arXiv preprint
arXiv:1511.06732.

Steven J Rennie, Etienne Marcheret, Youssef Mroueh,
Jarret Ross, and Vaibhava Goel. 2017. Self-critical
sequence training for image captioning. In CVPR,
volume 1, page 3.

Abigail See, Peter J Liu, and Christopher D Manning.
2017. Get to the point: Summarization with pointer-
generator networks. In Proceedings of the 55th An-
nual Meeting of the Association for Computation-
al Linguistics (Volume 1: Long Papers), volume 1,
pages 1073–1083.

Iulian Vlad Serban, Tim Klinger, Gerald Tesauro, Kar-
tik Talamadupula, Bowen Zhou, Yoshua Bengio,
and Aaron C Courville. 2017. Multiresolution re-
current neural networks: An application to dialogue
response generation. In AAAI, pages 3288–3294.

Zhan Shi, Xinchi Chen, Xipeng Qiu, and Xuanjing
Huang. 2018. Towards diverse text generation with
inverse reinforcement learning. arXiv preprint arX-
iv:1804.11258.

Ronald J Williams and David Zipser. 1989. A learn-
ing algorithm for continually running fully recurren-
t neural networks. Neural computation, 1(2):270–
280.

Sander Wubben, Antal Van Den Bosch, and Emiel
Krahmer. 2010. Paraphrase generation as monolin-
gual translation: Data and evaluation. In Proceed-
ings of the 6th International Natural Language Gen-
eration Conference, pages 203–207. Association for
Computational Linguistics.

Jingjing Xu, Xuancheng Ren, Junyang Lin, and X-
u Sun. 2018a. Dp-gan: diversity-promoting gen-
erative adversarial network for generating infor-
mative and diversified text. arXiv preprint arX-
iv:1802.01345.

Qiongkai Xu, Juyan Zhang, Lizhen Qu, Lexing X-
ie, and Richard Nock. 2018b. D-page: Di-
verse paraphrase generation. arXiv preprint arX-
iv:1808.04364.

Pengcheng Yin, Nan Duan, Ben Kao, Junwei Bao, and
Ming Zhou. 2015. Answering questions with com-
plex semantic constraints on open knowledge bases.
In Proceedings of the 24th ACM International on
Conference on Information and Knowledge Man-
agement, pages 1301–1310. ACM.

Shiqi Zhao, Cheng Niu, Ming Zhou, Ting Liu, and
Sheng Li. 2008. Combining multiple resources to
improve smt-based paraphrasing model. Proceed-
ings of ACL-08: HLT, pages 1021–1029.

Yaoming Zhu, Sidi Lu, Lei Zheng, Jiaxian Guo,
Weinan Zhang, Jun Wang, and Yong Yu. 2018.
Texygen: A benchmarking platform for text genera-
tion models. In The 41st International ACM SIGIR
Conference on Research & Development in Informa-
tion Retrieval, pages 1097–1100. ACM.


