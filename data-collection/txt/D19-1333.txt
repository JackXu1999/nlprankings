











































QAInfomax: Learning Robust Question Answering System by Mutual Information Maximization


Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing
and the 9th International Joint Conference on Natural Language Processing, pages 3370–3375,
Hong Kong, China, November 3–7, 2019. c©2019 Association for Computational Linguistics

3370

QAInfomax: Learning Robust Question Answering System

by Mutual Information Maximization

Yi-Ting Yeh Yun-Nung Chen

Department of Computer Science and Information Engineering
National Taiwan University, Taipei, Taiwan

r07922064@csie.ntu.edu.tw y.v.chen@ieee.org

Abstract

Standard accuracy metrics indicate that mod-
ern reading comprehension systems have
achieved strong performance in many ques-
tion answering datasets. However, the ex-
tent these systems truly understand language
remains unknown, and existing systems are
not good at distinguishing distractor sentences,
which look related but do not actually answer
the question. To address this problem, we pro-
pose QAInfomax as a regularizer in reading
comprehension systems by maximizing mu-
tual information among passages, a question,
and its answer. QAInfomax helps regularize
the model to not simply learn the superficial
correlation for answering questions. The ex-
periments show that our proposed QAInfomax
achieves the state-of-the-art performance on
the benchmark Adversarial-SQuAD dataset1.

1 Introduction

Question answering tasks are widely used for
training and testing machine comprehension and
reasoning (Rajpurkar et al., 2016; Joshi et al.,
2017). However, high performance in standard au-
tomatic metrics has been achieved with only su-
perficial understanding, as models exploit simple
correlations in the data that happen to be predictive
on most test examples. Jia and Liang (2017) ad-
dressed this problem and proposed an adversarial
version of the SQuAD dataset, which was created
by adding a distractor sentence to each paragraph.
The distractor sentences challenge the model ro-
bustness, and the created Adversarial-SQuAD data
shows the inability of a model about distinguish-
ing a sentence that actually answers the question
from one that merely has words in common with
it, where almost all state-of-the-art machine com-
prehension systems are significantly degraded on

1The source code is publicly available at https://
github.com/MiuLab/QAInfomax.

adversarial examples.
Lewis and Fan (2018) argued that over-fitting

to superficial biases is partially caused by discrim-
inative loss functions, which saturate when sim-
ple correlations allow the question to be answered
confidently, leaving no incentive for further learn-
ing on the example. Therefore, they designed
generative QA models, which use a generative
loss function in question answering instead, and
showed the improvement on Adversarial-SQuAD.

Instead of regularizing models by generative
loss functions, we propose an alternative approach
named “QAInfomax” by maximizing mutual in-
formation (MI) among passages, questions, and
answers, aiming at helping models be not stuck
with superficial biases in the data during learning.
To efficiently estimate MI, QAInfomax incorpo-
rates the recently proposed deep infomax (DIM) in
the model (Hjelm et al., 2018), which was proved
effective in learning representations for image, au-
dio (Ravanelli and Bengio, 2018), and graph do-
mains (Veličković et al., 2018). In this work, the
proposed QAInfomax further extends DIM to the
text domain, and encourages the question answer-
ing model to generate answers carrying informa-
tion that can explain not only questions but also
itself, and thus be more sensitive to distractor sen-
tences. Our contributions are summarized:

• This paper first attempts at applying DIM-
based MI estimation as a regularizer for rep-
resentation learning in the NLP domain.

• The proposed QAInfomax achieves the state-
of-the-art performance on the Adversarial-
SQuAD dataset without additional training
data, demonstrating its better robustness.

2 Mutual Information (MI) Estimation

In this section, we introduce how scalable estima-
tion of mutual information is performed in terms

https://github.com/MiuLab/QAInfomax
https://github.com/MiuLab/QAInfomax


3371

of practical scenarios via mutual information neu-
ral estimation (MINE) (Belghazi et al., 2018) and
the deep infomax (DIM) (Hjelm et al., 2018) de-
scribed below.

The mutual information between two random
variable X and Y is defined as:

MI(X,Y ) = DKL(p(X,Y ) k p(X)p(Y )),

where DKL is the Kullback-Leibler (KL) diver-
gence between the joint distribution p(X,Y ) and
the product of marginals p(X)p(Y ).

MINE estimates mutual information by training
a classifier to distinguish between positive samples
(x, y) from the joint distribution and negative sam-
ples (x, ȳ) from the product of marginals. Mu-
tual information neural estimation (MINE) uses
Donsker-Varadhan representation (DV) (Donsker
and Varadhan, 1983) as a lower-bound to estimate
MI.

MI(X,Y ) � EP[g(x, y)]� log(EN[eg(x,ȳ)]),

where EP and EN denote the expectation over pos-
itive and negative samples respectively, and g is
the discriminator function that outputs a real num-
ber modeled by a neural network.

While the DV representation is the strong bound
of mutual information shown in MINE, we are pri-
marily interested in maximizing MI but not focus-
ing on its precise value. Thus DIM proposes an al-
ternative estimation using Jensen-Shannon diver-
gence (JS), which can be efficiently implemented
using the cross-entropy (BCE) loss:

MI(X,Y ) � EP[log(g(x, y))] (1)
+ EN[log(1� g(x, ȳ))].

While two representations should behave simi-
larly, considering that both act like classifiers with
objectives maximizing the expected log-ratio of
the joint over the product of marginals, it is found
that the BCE loss empirically works better than the
DV-based objective (Hjelm et al., 2018; Ravanelli
and Bengio, 2018; Veličković et al., 2018). The
reason may be that the BCE loss is bounded (i.e.,
its maximum is zero), making the convergence of
the network more numerically stable. In our ex-
periments, we primarily use the JS representation
to estimate mutual information.

Recently, Tian et al. (2019) showed strong em-
pirical performance through the improved mul-
tiview CPC training (Oord et al., 2018), which

shares many common ideas as mutual information
maximization. Inspired by their work, we mod-
ify (1) by first switching the role of x and y and
summing them up:

MI(X,Y ) � EP[log(g(x, y))] (2)

+
1

2
EN[log(1� g(x, ȳ))]

+
1

2
EN[log(1� g(x̄, y))],

where (x̄, y) is also the negative sample sampled
from the product of marginals.

We empirically find that (2) gives the best per-
formance, and more exploration about parameter-
ization of MI is left as our future work.

3 Methodology

In the extractive question answering dataset like
SQuAD, the answer A = {a1, . . . , aM} to the
question Q = {q1, . . . , qK} is guaranteed to be
the span {pm, . . . , pm+M} in the paragraph P =
{p1, . . . , pN}. Given Q and P , the encoded repre-
sentations from the QA system M can be formu-
lated as:

{rq, rp} = {rq1, . . . , r
q
K , r

p
1, . . . , r

p
N} = M(Q,P ),

where rq and rp are representations of the ques-
tion and the passage respectively after the reason-
ing process in the QA system M .

Most models then feed the passage representa-
tion rp to a single-layer neural network, obtain the
span start and end probabilities for each passage
word, and compute the loss Lspan, which is the
negative sum of log probabilities of the predicted
distributions indexed by true start and end indices.

Our QAInfomax aims at regularizing the QA
system M to not simply exploit the superficial bi-
ases in the dataset for answering questions. There-
fore, two constraints are introduced in order to
guide the model learning.

1. Local Constraint (LC): each answer word
representation rpi in the answer representa-
tion ra = {rpm, . . . , rpm+M} should contain
information about what the remaining answer
words and its surrounding context are.

2. Global Constraint (GC): the summarized
answer representation s = S(ra) should
maximize the averaged mutual information to
all other question representations in rq and
passage representations in rp, where S is a
summarization function described below.



3372

Question, Passage
(from another example)

Question, Passage

Answer, Context 
(from another example )

!
Fake

RealAnswer, Context

Answer

Fake

Real

Global Constraint 

Local Constraint 

Figure 1: Illustration of the LC and GC.

Intuitively, the model is expected to choose the an-
swer span after fully considering the entire ques-
tion and paragraph. However, traditional QA
models suffered the overstability problem, and
tended to be fooled by distractor answers, such as
the one containing an unrelated human name. As
Lewis and Fan (2018) argued, we also believe that
the main reason is that QA models are only trained
to predict start and end positions of answer spans.
Correlation in the dataset allows QA models to
find shortcuts and ignore what the answer span
looks like. A learned behavior of traditional QA
models can be viewed as a simple pattern match-
ing, such as choosing the 5-length span after the
word “river” if a question is about a river and the
context talks about countries in European.

Following the intuition, two constraints LC and
GC are introduced to guide models to learn the de-
sired behaviors. To prevent the model from only
learning to match some specific word patterns to
find the answer, LC forces the model to gener-
ate answer span representations while maximizing
mutual information among words in the span and
the context words surrounding the span. By maxi-
mizing the mutual information between an answer
word and all of its context words, models need
to incorporate the entire context into its decision
process while choosing answers, and thus can be
more robust to the adversarial sentences. Then we
further require models to maximize mutual infor-
mation among answer words, so models can no
longer ignore any word in the chosen answer span.

On the other hand, different from LC , which
only focuses on the answer span and its context,

GC pushes the model to prefer answer representa-
tions carrying information that is globally shared
across the whole input conditions Q and P , be-
cause shortcuts do not necessarily appear near to
the answer. If the model only learns to leverage
the correlation specific to the partial input, the MI
of any input word without such relationship would
not increased.

The overview about two proposed constraints
is illustrated in Figure 1. The detail of two con-
straints and our QAInfomax regularizer is de-
scribed below.

3.1 Local Constraint

As shown in Section 2, the maximization of
MI needs positive samples and negative samples
drawn from joint distribution and the product of
marginal distribution respectively.

In LC , because all answer word representations
are expected to carry the information of each other
and their contexts, we choose to maximize aver-
aged MI between the sampled answer word rep-
resentations and the whole answer sequence with
its context words. Specifically, a positive sam-
ple is obtained by pairing the sampled answer
word representation x 2 ra = {rpm, . . . , rpm+M}
to all other answer and context words rc =
{rpm�C , . . . r

p
m+M+C} \ {x}, where C is the hy-

perparameter defining how many context words
for consideration. Negative samples, on the other
hand, are obtained by randomly sampling answer
representation r̄a = {r̄pl , . . . , r̄

p
l+L} and the cor-

responding r̄c from other training examples. Fol-
lowing (2), the objective for sampled x, rc, x̄ 2 r̄a
and r̄c is formulated.

LC(x, rc, x̄, r̄c) =
1

|rc|
X

rci2rc
log(g(x, rci )) (3)

+
1

2|r̄c|
X

r̄cj2r̄c
log(1� g(x, r̄cj))

+
1

2|rc|
X

rci2rc
log(1� g(x̄, rci )).

3.2 Global Constraint

Different from LC described above, GC forces the
learned answer representations ra to have infor-
mation shared with all other question and passage
representations. Here, we maximize the mutual in-
formation between the summarized answer vector
s = S(ra) and rl 2 r = {rq, rp} \ {ra} pairs. In



3373

Model Original ADDSENT ADDONESENT

BiDAF-S (Seo et al., 2016) 75.5 34.3 45.7
ReasoNet-S (Shen et al., 2017) 78.2 39.4 50.3
Reinforced Mnemonic Reader-S (Hu et al., 2017) 78.5 46.6 56.0
QANet-S (Yu et al., 2018) 83.8 45.2 55.7
GQA-S (Lewis and Fan, 2018) 83.7 47.3 57.8
FusionNet-E (Huang et al., 2017) 83.6 51.4 60.7
BERT-S (Devlin et al., 2018) 88.5 51.0 63.4
BERT-S + QAInfomax 88.6 54.5 † 64.9 †

Table 1: F-measure on ADVERSARIALSQUAD (S: single, E: ensemble). † indicates the significant improvement
over baselines with p-value < 0.05.

the experiments, we use S(ra) = �( 1M
P

rai ) as
our summarization function, where � is the logis-
tic sigmoid nonlinearity.

Specifically, a positive sample here is the pair of
a answer summary vector s = S(ra) and all other
word representations in r. Negative samples are
provided by sampling question, passage and an-
swer representations {r̄q, r̄p, r̄a} from an alterna-
tive training example. Then we pair the summary
s with r̄ = {r̄q, r̄p} \ {r̄a}, and s̄ = S(r̄a) with r.

Similar to (3), the objective for the sampled s,
r, s̄ and r̄ is:

GC(s, r, s̄, r̄) =
1

|r|
X

ri2r
log(g(s, ri)) (4)

+
1

2|r̄|
X

r̄j2r̄
log((1� g(s, r̄j)))

+
1

2|r|
X

ri2r
log((1� g(s̄, ri))).

3.3 QAInfomax

In our proposed model, we combine two
objectives and formulate the model as the
complete QAInfomax regularizer. For each
training batch consisting of training examples
{{Q1, P1, A1}, . . . {QB, PB, AB}}, we pass the
batch into the model M and obtain representa-
tions {{rq1, r

p
1, r

a
1}, . . . , {r

q
B, r

p
B, r

a
B}}. Note that

we abuse the subscripts to denote the example in-
dex in the batch for simplicity.

Then we shuffle the whole batch to obtain neg-
ative examples {{r̄q1, r̄

p
1, r̄

a
1}, . . . , {r̄

q
B, r̄

p
B, r̄

a
B}}.

The complete objective Linfo for QAInfomax be-
comes:

� 1
B

BX

i=1

(↵LC(xi, r
c
i , x̄i, r̄

c
i ) + �GC(si, ri, r̄i)),

where xi and x̄i are the representation sampled
from rai and r̄

a
i , r

c
i and r̄

c
i are r

a
i and r̄

a
i expanded

with its context words respectively, si and s̄i are
the summary vectors of rai and r̄

a
i , and ↵ and �

are hyperparameters.
Combined with QAInfomax as a regularizer, the

final objective of the model becomes

L = Lspan + �Linfo, (5)

where Lspan is the answer span prediction loss and
� is the regularize strength. The objective can be
optimized through the simple gradient descent.

4 Experiments

To evaluate the effectiveness of the proposed
QAInfomax, we conduct the experiments on a
challenging dataset, Adversarial-SQuAD.

4.1 Setup

BERT-base (Devlin et al., 2018) is employed as
our QA system M in the experiments, where we
set the same hyperparameters as one released in
SQuAD training2.

We set C, ↵, � and � to be 5, 1, 0.5, 0.3 respec-
tively in all experiments, and add the proposed
QAInfomax into the BERT model as described
above. The discriminator function g is the bilin-
ear function similar to the scoring used by Oord
et al. (2018):

g(x, y) = xTWy, (6)

where W is a learnable scoring matrix.
We train the BERT model with the proposed

QAInfomax on the orignal SQuAD dataset, and
2We use PyTorch (Paszke et al., 2017) reimple-

mentation for experiments: https://github.com/
huggingface/pytorch-pretrained-BERT.

https://github.com/huggingface/pytorch-pretrained-BERT
https://github.com/huggingface/pytorch-pretrained-BERT


3374

Model Adversary F1 Speed (iter/s)

BERT 51.0 / 63.4 3.80
+ LC 53.6 / 64.2 3.51
+ GC 52.2 / 63.7 2.75
+ LC + GC 54.5 / 64.9 2.72

Table 2: Ablation study with F1 scores on ADDSENT
/ ADDONESENT. The speed is measured on RTX
2080Ti.

use Adversarial-SQuAD to test the robustness of
the augmented model. Only ADDSENT and AD-
DONESENT metrics are reported for the compari-
son with previous models, because most previous
models did not report their ADDANY and AD-
DCOMMON scores. Briefly, for each example,
ADDSENT runs the model M on every human-
approved adversarial sentence, picks the one that
makes the model give the worst answer and returns
that score. ADDONESENT, on the other hand,
only picks a random human-approved adversarial
sentence. The numbers reported in all experiments
are the best number across at least three runs.

4.2 Results

Table 1 reports model performance on
Adversarial-SQuAD. It can be found that
QAInfomax yields substantial improvement
over the vanilla BERT model, and achieves the
state-of-the-art performance on both ADDSENT
and ADDONESENT metrics.3 QAInfomax obtains
larger improvement on the ADDSENT, which
picks the worst scores of the model. It shows
the effectiveness of our QAInfomax in terms of
forcing the model to ignore simple correlation in
the data and learn the more human-like reasoning
processes. It is worth to note that while QAIn-
fomax mitigates the overstability problem and
improves the robustness to adversarial examples,
it does not hurt the original performance of the
QA system, demonstrating the benefit for the
practical usage. Some example results from the
Adversarial-SQuAD dataset can be found in the
Appendix, where adversarial distracting sentences
are shown in italic blue fonts.

Table 2 shows the ablation study of our pro-
posed QAInfomax, where two proposed con-

3Note that Wang and Bansal modified distractor para-
graphs and added them into training data, so we do not com-
pare with them, because we only use the original SQuAD
training data.

Function ADDSENT ADDONESENT

Mean 52.2 63.7
Max 52.0 63.3
Sample 52.2 63.0

Table 3: Different summarization functions for GC .

straints are both important for achieving such re-
sults. We also show the training speed of the pro-
posed method and its limitation, where the GC ob-
jective degrades the training speed by 28%. The
reason is that GC measures the averaged MI over
the whole question and passage representations,
which may include a long sequence of vectors.

Considering that the summarization function S
plays an important role in GC, we explore its dif-
ferent variants in Table 3:

• Mean: �( 1M
P

rai )

• Max: �(maxpool(ra))

• Sample: randomly sample one rai 2 ra

According to the experimental results, Mean per-
forms the best while Max and Sample has the com-
petitive performance, showing the great robust-
ness of the proposed methods to different archi-
tecture choices.

5 Conclusion

This paper presents a novel regularizer based
on MI maximization for question answering sys-
tems named QAInfomax, which helps models
be not stuck with superficial correlation in the
data and improves its robustness. The proposed
QAInfomax is flexible to apply to different ma-
chine comprehension models. The experiments on
Adversirial-SQuAD demonstrate the effectiveness
of our model, and the augmented model achieves
the state-of-the-art results. In the future, we will
investigate more methods for reducing the limita-
tions of QAInfomax and improving the capability
of generalization in QA systems.

Acknowledgements

We would like to thank reviewers for their insight-
ful comments on the paper. This work was finan-
cially supported from the Young Scholar Fellow-
ship Program by Ministry of Science and Technol-
ogy (MOST) in Taiwan, under Grant 108-2636-E-
002-003.



3375

References

Mohamed Ishmael Belghazi, Aristide Baratin, Sai
Rajeswar, Sherjil Ozair, Yoshua Bengio, Aaron
Courville, and R Devon Hjelm. 2018. Mine: mu-
tual information neural estimation. arXiv preprint
arXiv:1801.04062.

Jacob Devlin, Ming-Wei Chang, Kenton Lee, and
Kristina Toutanova. 2018. Bert: Pre-training of deep
bidirectional transformers for language understand-
ing. arXiv preprint arXiv:1810.04805.

Monroe D Donsker and SR Srinivasa Varadhan. 1983.
Asymptotic evaluation of certain markov process ex-
pectations for large time. iv. Communications on
Pure and Applied Mathematics, 36(2):183–212.

R Devon Hjelm, Alex Fedorov, Samuel Lavoie-
Marchildon, Karan Grewal, Adam Trischler, and
Yoshua Bengio. 2018. Learning deep representa-
tions by mutual information estimation and maxi-
mization. arXiv preprint arXiv:1808.06670.

Minghao Hu, Yuxing Peng, Zhen Huang, Xipeng Qiu,
Furu Wei, and Ming Zhou. 2017. Reinforced
mnemonic reader for machine reading comprehen-
sion. arXiv preprint arXiv:1705.02798.

Hsin-Yuan Huang, Chenguang Zhu, Yelong Shen, and
Weizhu Chen. 2017. Fusionnet: Fusing via fully-
aware attention with application to machine compre-
hension. arXiv preprint arXiv:1711.07341.

Robin Jia and Percy Liang. 2017. Adversarial exam-
ples for evaluating reading comprehension systems.
arXiv preprint arXiv:1707.07328.

Mandar Joshi, Eunsol Choi, Daniel S Weld, and Luke
Zettlemoyer. 2017. Triviaqa: A large scale distantly
supervised challenge dataset for reading comprehen-
sion. arXiv preprint arXiv:1705.03551.

Mike Lewis and Angela Fan. 2018. Generative ques-
tion answering: Learning to answer the whole ques-
tion.

Aaron van den Oord, Yazhe Li, and Oriol Vinyals.
2018. Representation learning with contrastive pre-
dictive coding. arXiv preprint arXiv:1807.03748.

Adam Paszke, Sam Gross, Soumith Chintala, Gre-
gory Chanan, Edward Yang, Zachary DeVito, Zem-
ing Lin, Alban Desmaison, Luca Antiga, and Adam
Lerer. 2017. Automatic differentiation in pytorch.

Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and
Percy Liang. 2016. Squad: 100,000+ questions
for machine comprehension of text. arXiv preprint
arXiv:1606.05250.

Mirco Ravanelli and Yoshua Bengio. 2018. Learn-
ing speaker representations with mutual informa-
tion. arXiv preprint arXiv:1812.00271.

Min Joon Seo, Aniruddha Kembhavi, Ali Farhadi,
and Hannaneh Hajishirzi. 2016. Bidirectional at-
tention flow for machine comprehension. CoRR,
abs/1611.01603.

Yelong Shen, Po-Sen Huang, Jianfeng Gao, and
Weizhu Chen. 2017. Reasonet: Learning to stop
reading in machine comprehension. In Proceedings
of the 23rd ACM SIGKDD International Conference
on Knowledge Discovery and Data Mining, pages
1047–1055. ACM.

Yonglong Tian, Dilip Krishnan, and Phillip Isola.
2019. Contrastive multiview coding. arXiv preprint
arXiv:1906.05849.

Petar Veličković, William Fedus, William L Hamil-
ton, Pietro Liò, Yoshua Bengio, and R Devon
Hjelm. 2018. Deep graph infomax. arXiv preprint
arXiv:1809.10341.

Yicheng Wang and Mohit Bansal. 2018. Robust ma-
chine comprehension models via adversarial train-
ing. arXiv preprint arXiv:1804.06473.

Adams Wei Yu, David Dohan, Minh-Thang Luong, Rui
Zhao, Kai Chen, Mohammad Norouzi, and Quoc V
Le. 2018. Qanet: Combining local convolution
with global self-attention for reading comprehen-
sion. arXiv preprint arXiv:1804.09541.

http://arxiv.org/abs/1611.01603
http://arxiv.org/abs/1611.01603

