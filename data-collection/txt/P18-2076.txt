



















































Linear-time Constituency Parsing with RNNs and Dynamic Programming


Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 477–483
Melbourne, Australia, July 15 - 20, 2018. c©2018 Association for Computational Linguistics

477

Linear-Time Constituency Parsing with RNNs
and Dynamic Programming

Juneki Hong 1
1School of EECS

Oregon State University, Corvallis, OR
{juneki.hong, liang.huang.sh}@gmail.com

Liang Huang 1,2
2Silicon Valley AI Lab

Baidu Research, Sunnyvale, CA

Abstract

Recently, span-based constituency parsing
has achieved competitive accuracies with
extremely simple models by using bidirec-
tional RNNs to model “spans”. However,
the minimal span parser of Stern et al.
(2017a) which holds the current state of
the art accuracy is a chart parser running in
cubic time, O(n3), which is too slow for
longer sentences and for applications be-
yond sentence boundaries such as end-to-
end discourse parsing and joint sentence
boundary detection and parsing. We pro-
pose a linear-time constituency parser with
RNNs and dynamic programming using
graph-structured stack and beam search,
which runs in time O(nb2) where b is
the beam size. We further speed this up
to O(nb log b) by integrating cube prun-
ing. Compared with chart parsing base-
lines, this linear-time parser is substan-
tially faster for long sentences on the Penn
Treebank and orders of magnitude faster
for discourse parsing, and achieves the
highest F1 accuracy on the Penn Treebank
among single model end-to-end systems.

1 Introduction

Span-based neural constituency parsing (Cross
and Huang, 2016; Stern et al., 2017a) has attracted
attention due to its high accuracy and extreme
simplicity. Compared with other recent neural
constituency parsers (Dyer et al., 2016; Liu and
Zhang, 2016; Durrett and Klein, 2015) which use
neural networks to model tree structures, the span-
based framework is considerably simpler, only us-
ing bidirectional RNNs to model the input se-
quence and not the output tree. Because of this
factorization, the output space is decomposable

which enables efficient dynamic programming al-
gorithm such as CKY. But existing span-based
parsers suffer from a crucial limitation in terms
of search: on the one hand, a greedy span parser
(Cross and Huang, 2016) is fast (linear-time) but
only explores one single path in the exponentially
large search space, and on the other hand, a chart-
based span parser (Stern et al., 2017a) performs
exact search and achieves state-of-the-art accu-
racy, but in cubic time, which is too slow for
longer sentences and for applications that go be-
yond sentence boundaries such as end-to-end dis-
course parsing (Hernault et al., 2010; Zhao and
Huang, 2017) and integrated sentence boundary
detection and parsing (Björkelund et al., 2016).

We propose to combine the merits of both
greedy and chart-based approaches and design a
linear-time span-based neural parser that searches
over exponentially large space. Following Huang
and Sagae (2010), we perform left-to-right dy-
namic programming in an action-synchronous
style, with (2n − 1) actions (i.e., steps) for a sen-
tence of nwords. While previous non-neural work
in this area requires sophisticated features (Huang
and Sagae, 2010; Mi and Huang, 2015) and thus
high time complexity such as O(n11), our states
are as simple as ` : (i, j) where ` is the step in-
dex and (i, j) is the span, modeled using bidirec-
tional RNNs without any syntactic features. This
gives a running time ofO(n4), with the extraO(n)
for step index. We further employ beam search
to have a practical runtime of O(nb2) at the cost
of exact search where b is the beam size. How-
ever, on the Penn Treebank, most sentences are
less than 40 words (n < 40), and even with a small
beam size of b = 10, the observed complexity of
an O(nb2) parser is not exactly linear in n (see
Experiments). To solve this problem, we apply
cube pruning (Chiang, 2007; Huang and Chiang,
2007) to improve the runtime toO(nb log b) which



478

renders an observed complexity that is linear in n
(with minor extra inexactness).

We make the following contributions:

• We design the first neural parser that is both
linear time and capable of searching over ex-
ponentially large space.1

• We are the first to apply cube pruning to in-
cremental parsing, and achieves, for the first
time, the complexity of O(nb log b), i.e., lin-
ear in sentence length and (almost) linear in
beam size. This leads to an observed com-
plexity strictly linear in sentence length n.

• We devise a novel loss function which penal-
izes wrong spans that cross gold-tree spans,
and employ max-violation update (Huang
et al., 2012) to train this parser with struc-
tured SVM and beam search.

• Compared with chart parsing baselines, our
parser is substantially faster for long sen-
tences on the Penn Treebank, and orders
of magnitude faster for end-to-end discourse
parsing. It also achieves the highest F1 score
on the Penn Treebank among single model
end-to-end systems.

• We devise a new formulation of graph-
structured stack (Tomita, 1991) which re-
quires no extra bookkeeping, proving a new
theorem that gives deep insight into GSS.

2 Preliminaries

2.1 Span-Based Shift-Reduce Parsing

A span-based shift-reduce constituency parser
(Cross and Huang, 2016) maintains a stack of
spans (i, j), and progressively adds a new span
each time it takes a shift or reduce action. With
(i, j) on top of the stack, the parser can either shift
to push the next singleton span (j, j + 1) on the
stack, or it can reduce to combine the top two
spans, (k, i) and (i, j), forming the larger span
(k, j). After each shift/reduce action, the top-most
span is labeled as either a constituent or with a null
label ∅, which means that the subsequence is not a
subtree in the final decoded parse. Parsing initial-
izes with an empty stack and continues until (0, n)
is formed, representing the entire sentence.

1
https://github.com/junekihong/beam-span-parser

input w0 . . . wn−1

state ` : 〈i, j〉 : (c, v)

init 0 : 〈0, 0〉 : (0, 0)

goal 2n− 1 : 〈0, n〉 : (c, c)

shift
` : 〈 , j〉 : (c, )

`+ 1 : 〈j, j + 1〉 : (c+ ξ, ξ)
j < n

reduce
`′ : 〈k, i〉 : (c′, v′) ` : 〈i, j〉 : ( , v)

`+ 1 : 〈k, j〉 : (c′ + v + σ, v′ + v + σ)

Figure 1: Our shift-reduce deductive system. Here
` is the step index, c and v are prefix and inside
scores. Unlike Huang and Sagae (2010) and Cross
and Huang (2016), ξ and σ are not shift/reduce
scores; instead, they are the (best) label scores of
the resulting span: ξ = maxX s(j, j+1, X) and
σ = maxX s(k, j,X) where X is a nonterminal
symbol (could be ∅). Here `′ = `− 2(j − i) + 1.

2.2 Bi-LSTM features
To get the feature representation of a span (i, j),
we use the output sequence of a bi-directional
LSTM (Cross and Huang, 2016; Stern et al.,
2017a). The LSTM produces f0, ..., fn forwards
and bn, ...,b0 backwards outputs, which we con-
catenate the differences of (fj−fi) and (bi−bj) as
the representation for span (i, j). This eliminates
the need for complex feature engineering, and can
be stored for efficient querying during decoding.

3 Dynamic Programming

3.1 Score Decomposition
Like Stern et al. (2017a), we also decompose the
score of a tree t to be the sum of the span scores:

s(t) =
∑

(i,j,X)∈t

s(i, j,X) (1)

=
∑

(i,j)∈t

max
X

s((fj − fi;bi − bj), X) (2)

Note that X is a nonterminal label, a unary chain
(e.g., S-VP), or null label ∅.2 In a shift-reduce
setting, there are 2n − 1 steps (n shifts and n − 1
reduces) and after each step we take the best label
for the resulting span; therefore there are exactly

2The actual code base of Stern et al. (2017b) forces
s(i, j,∅) to be 0, which simplifies their CKY parser and
slightly improves their parsing accuracy. However, in our
incremental parser, this change favors shift over reduce and
degrades accuracy, so our parser keeps a learned score for ∅.

https://github.com/junekihong/beam-span-parser


479

2n−1 such (labeled) spans (i, j,X) in tree t. Also
note that the choice of the label for any span (i, j)
is only dependent on (i, j) itself (and not depend-
ing on any subtree information), thus the max over
label X is independent of other spans, which is
a nice property of span-based parsing (Cross and
Huang, 2016; Stern et al., 2017a).

3.2 Graph-Struct. Stack w/o Bookkeeping

We now reformulate this DP parser in the above
section as a shift-reduce parser. We maintain a step
index ` in order to perform action-synchronous
beam search (see below). Figure 1 shows how
to represent a parsing stack using only the top
span (i, j). If the top span (i, j) shifts, it pro-
duces (j, j + 1), but if it reduces, it needs to know
the second last span on the stack, (k, i), which is
not represented in the current state. This problem
can be solved by graph-structure stack (Tomita,
1991; Huang and Sagae, 2010), which maintains,
for each state p, a set of predecessor states π(p)
that p can combine with on the left.

This is the way our actual code works (π(p) is
implemented as a list of pointers, or “left point-
ers”), but here for simplicity of presentation we
devise a novel but easier-to-understand formula-
tion in Fig. 1, where we explicitly represent the set
of predecessor states that state ` : (i, j) can com-
bine with as `′ : (k, i) where `′ = `−2(j− i) + 1,
i.e., (i, j) at step ` can combine with any (k, i) for
any k at step `′. The rationale behind this new for-
mulation is the following theorem:

Theorem 1 The predecessor states π(` : (i, j))
are all in the same step `′ = `− 2(j − i) + 1.
Proof. By induction.

This Theorem bring new and deep insights and
suggests an alternative implementation that does
not require any extra bookkeeping. The time com-
plexity of this algorithm is O(n4) with the extra
O(n) due to step index.3

3.3 Action-Synchronous Beam Search

The incremental nature of our parser allows us to
further lower the runtime complexity at the cost of
inexact search. At each time step, we maintain the
top b parsing states, pruning off the rest. Thus, a
candidate parse that made it to the end of decod-
ing had to survive within the top b at every step.

3The word-synchronous alternative does not need the step
index ` and enjoys a cubic time complexity, being almost
identical to CKY. However, beam search becomes very tricky.

With O(n) parsing actions our time complexity
becomes linear in the length of the sentence.

3.4 Cube Pruning
However, Theorem 1 suggests that a parsing state
p can have up to b predecessor states (“left point-
ers”), i.e., |π(p)| ≤ b because π(p) are all in the
same step, a reduce action can produce up to b
subsequent new reduced states. With b items on
a beam and O(n) actions to take, this gives us an
overall complexity of O(nb2). Even though b2 is
a constant, even modest values of b can make b2

dominate the length of the sentence. 4

To improve this at the cost of additional inex-
actness, we introduce cube pruning to our beam
search, where we put candidate actions into a heap
and retrieve the top b states to be considered in
the next time-step. We heapify the top b shift-
merged states and the top b reduced states. To
avoid inserting all b2 reduced states from the pre-
vious beam, we only consider each state’s high-
est scoring left pointer,5 and whenever we pop a
reduced state from the heap, we iterate down its
left pointers to insert the next non-duplicate re-
duced state back into the heap. This process fin-
ishes when we pop b items from the heap. The
initialization of the heap takes O(b) and popping
b items takes O(b log b), giving us an overall im-
proved runtime of O(nb log b).

4 Training

We use a Structured SVM approach for training
(Stern et al., 2017a; Shi et al., 2017). We want
the model to score the gold tree t∗ higher than any
other tree t by at least a margin ∆(t, t∗):

∀t, s(t∗)− s(t) ≥ ∆(t, t∗).

Note that ∆(t, t) = 0 for any t and ∆(t, t∗) > 0
for any t 6= t∗. At training time we perform loss-
augmented decoding:

t̂ = arg max
t
s∆(t) = arg max

t
s(t) + ∆(t, t∗).

4The average length of a sentence in the Penn Treebank
training set is about 24. Even with a beam size of 10, we al-
ready have b2 = 100, which would be a significant factor in
our runtime. In practice, each parsing state will rarely have
the maximum b left pointers so this ends up being a loose
upper-bound. Nevertheless, the beam search should be per-
formed with the input length in mind, or else as b increases
we risk losing a linear runtime.

5If each previous beam is sorted, and if the beam search
is conducted by going top-to-bottom, then each state’s left
pointers will implicitly be kept in sorted order.



480

20 40 60 80 100 120 140

Sentence Length

0.0

0.5

1.0

1.5

2.0

T
im

e
 (

se
c)

Chart Parsing: O(n2. 26)

Beam 20 No Cube-Pruning: O(n1. 26)

Beam 20 Cube Pruned: O(n1. 08)

Beam 5 Cube Pruned: O(n0. 97)

Figure 2: Runtime plots of decoding on the train-
ing set of the Penn Treebank. The differences be-
tween the different algorithms become evident af-
ter sentences of length 40. The regression curves
have been empirically fitted.

where s∆(·) is the loss-augmented score. If t̂ =
t∗, then all constraints are satisfied (which implies
arg maxt s(t) = t

∗), otherwise we perform an up-
date by backpropagating from s∆(t̂)− s(t∗).

4.1 Cross-Span Loss
The baseline loss function from Stern et al.
(2017a) counts the incorrect labels (i, j,X) in the
predicted tree:

∆base(t, t
∗) =

∑
(i,j,X)∈t

1
(
X 6= t∗(i,j)

)
.

Note that X can be null ∅, and t∗(i,j) denotes
the gold label for span (i, j), which could also
be ∅.6 However, there are two cases where
t∗(i,j) = ∅: a subspan (i, j) due to binarization
(e.g., a span combining the first two subtrees in
a ternary branching node), or an invalid span in
t that crosses a gold span in t∗. In the baseline
function above, these two cases are treated equiv-
alently; for example, a span (3, 5,∅) ∈ t is not pe-
nalized even if there is a gold span (4, 6,VP) ∈ t∗.
So we revise our loss function as:

∆new(t, t
∗) =

∑
(i,j,X)∈t

1
(
X 6= t∗(i,j)

∨ cross(i, j, t∗)
)

6Note that the predicted tree t has exactly 2n − 1 spans
but t∗ has much fewer spans (only labeled spans without ∅).

500 1000 1500 2000

Discourse Length (words)

0

500

1000

1500

2000

2500

3000

3500

Ti
m

e 
(s

ec
) C

h
a
rt

 P
a
rs

in
g

This Work Beam 10

102 103

Discourse Length (words)

10-1

100

101

102

103

Ti
m

e 
(s

ec
)

Th
is 

W
or

k B
ea

m 
10

C
h
ar

t 
P
ar

si
n
g

Figure 3: Runtime plot of decoding the discourse
treebank training set. The log-log plot on the right
shows the cubic complexity of baseline chart pars-
ing. Whereas beam search decoding maintains lin-
ear time even for sequences of thousands of words.

where cross(i, j, t∗) = ∃ (k, l) ∈ t∗, and i < k <
j < l or k < i < l < j.

4.2 Max Violation Updates
Given that we maintain loss-augmented scores
even for partial trees, we can perform a training
update on a given example sentence by choos-
ing to take the loss where it is the greatest along
the parse trajectory. At each parsing time-step `,
the violation is the difference between the high-
est augmented-scoring parse trajectory up to that
point and the gold trajectory (Huang et al., 2012;
Yu et al., 2013). Note that computing the viola-
tion gives us the max-margin loss described above.
Taking the largest violation from all time-steps
gives us the max-violation loss.

5 Experiments

We present experiments on the Penn Treebank
(Marcus et al., 1993) and the PTB-RST discourse
treebank (Zhao and Huang, 2017). In both cases,
the training set is shuffled before each epoch, and
dropout (Hinton et al., 2012) is employed with
probability 0.4 to the recurrent outputs for regu-
larization. Updates with minibatches of size 10
and 1 are used for PTB and the PTB-RST respec-
tively. We use Adam (Kingma and Ba, 2014) with
default settings to schedule learning rates for all
the weights. To address unknown words during
training, we adopt the strategy described by Kiper-
wasser and Goldberg (Kiperwasser and Goldberg,
2016); words in the training set are replaced with
the unknown word symbol UNK with probability
punk =

1
1+f(w) , with f(w) being the number of



481

Development Set 22 (F1)
Baseline +cross-span Time

This Work Beam 1 89.41 89.93 0.042
This Work Beam 5 91.27 91.91 0.050
This Work Beam 10 91.56 92.09 0.058
This Work Beam 15 91.74 92.16 0.062
This Work Beam 20 91.65 92.20 0.066
Chart 92.02 92.21 0.076

Table 1: Comparison of PTB development set
results, with the time measured in seconds-per-
sentence. The baseline chart parser is from Stern
et al. (2017b), with null-label scores unconstrained
to be nonzero, replicating their paper.

Test Set 23
End-to-End & Single Model LR LP F1
Socher et al. (2013) 90.4
Durrett and Klein (2015) 91.1
Cross and Huang (2016) 90.5 92.1 91.3
Liu and Zhang (2016) 91.3 92.1 91.7
Dyer et al. (2016) (discrim.) 91.7
Stern et al. (2017a) 90.63 92.98 91.79
Stern et al. (2017a) +cross-span 91.67 91.94 91.81
Stern et al. (2017b) 91.35 92.38 91.86
This Work Beam 10 91.44 91.91 91.67
This Work Beam 15 91.64 92.04 91.84
This Work Beam 20 91.49 92.45 91.97
Reranking/Ensemble/Separate Decoding
Vinyals et al. (2015) (ensem) 90.5
Dyer et al. (2016) (gen., rerank) 93.3
Choe and Charniak (2016) (rerank) 93.8
Stern et al. (2017c) (sep. decoding) 92.57 92.56 92.56
Fried et al. (2017) (ensem/rerank) 94.25

Table 2: Final PTB Test Results. We compare our
models with other (neural) single-model end-to-
end trained systems.

occurrences of word w in the training corpus. Our
system is implemented in Python using the DyNet
neural network library (Neubig et al., 2017).

5.1 Penn Treebank

We use the Wall Street Journal portion of the Penn
Treebank, with the standard split of sections 2-21
for training, 22 for development, and 23 for test-
ing. Tags are provided using the Stanford tagger
with 10-way jackknifing.

Table 1 shows our development results and
overall speeds, while Table 2 compares our test re-
sults. We show that a beam size of 20 can be fast
while still achieving state-of-the-art performances.

5.2 Discourse Parsing

To measure the tractability of parsing on longer
sequences, we also consider experiments on the

LR LP F1
Zhao and Huang (2017) 81.6 83.5 82.5
This Work Beam 10 80.47 80.61 80.54
This Work Beam 20 80.86 80.73 80.79
This Work Beam 200 81.51 80.84 81.18
This Work Beam 500? 81.50 80.81 81.16
This Work Beam 1000? 81.55 80.85 81.20

Table 3: Overall test accuracies for PTB-RST dis-
course treebank. Starred? rows indicate a run that
was decoded from the beam 200 model.

segment structure +nuclearity +relation
Bach et al. (2012) 95.1 - - -
Hernault et al. (2010) 94.0 72.3 59.1 47.3
Zhao and Huang (2017) 95.4 78.8 65.0 52.2
This Work Beam 200 91.20 73.36 58.87 46.38
This Work Beam 500? 93.52 74.93 60.16 47.03
This Work Beam 1000? 94.06 75.60 60.61 47.37

Table 4: F1 scores comparing discourse systems.
Results correspond to the accuracies in Table 3,
broken down to focus on the discourse labels.

PTB-RST discourse Treebank, a joint discourse
and constituency dataset with a combined rep-
resentation, allowing for parsing at either level
(Zhao and Huang, 2017). We compare our run-
times out-of-the-box in Figure 3. Without any
pre-processing, and by treating discourse exam-
ples as constituency trees with thousands of words,
our trained models represent end-to-end discourse
parsing systems.

For our overall constituency results in Table 3,
and for discourse results in Table 4, we adapt the
split-point feature described in (Zhao and Huang,
2017) in addition to the base parser. We find that
larger beamsizes are required to achieve good dis-
course scores.

6 Conclusions
We have developed a new neural parser that main-
tains linear time, while still searching over an ex-
ponentially large space. We also use cube prun-
ing to further improve the runtime to O(nb log b).
For training, we introduce a new loss function,
and achieve state-of-the-art results among single-
model end-to-end systems.

Acknowledgments
We thank Dezhong Deng who contributed greatly
to Secs. 3.2 and 4 (he deserves co-authorship), and
Mitchell Stern for releasing his code and and sug-
gestions. This work was supported in part by NSF
IIS-1656051 and DARPA N66001-17-2-4030.



482

References
Ngo Xuan Bach, Nguyen Le Minh, and Akira Shimazu.

2012. A reranking model for discourse segmenta-
tion using subtree features. In Proceedings of the
13th Annual Meeting of the Special Interest Group
on Discourse and Dialogue. Association for Com-
putational Linguistics, pages 160–168.

Anders Björkelund, Agnieszka Faleńska, Wolfgang
Seeker, and Jonas Kuhn. 2016. How to train depen-
dency parsers with inexact search for joint sentence
boundary detection and parsing of entire documents.
In Proceedings of the 54th Annual Meeting of the As-
sociation for Computational Linguistics (Volume 1:
Long Papers). volume 1, pages 1924–1934.

David Chiang. 2007. Hierarchical phrase-based trans-
lation. Computational Linguistics 33(2):201–208.

Do Kook Choe and Eugene Charniak. 2016. Parsing
as language modeling. In Proceedings of the Con-
ference on Empirical Methods in Natural Language
Processing. pages 2331–2336.

James Cross and Liang Huang. 2016. Span-based
constituency parsing with a structure-label system
and provably optimal dynamic oracles. In Proceed-
ings of the 2016 Conference on Empirical Meth-
ods in Natural Language Processing. Association
for Computational Linguistics, Austin, Texas, pages
1–11. https://aclweb.org/anthology/D16-1001.

Greg Durrett and Dan Klein. 2015. Neural CRF pars-
ing. arXiv preprint arXiv:1507.03641 .

Chris Dyer, Adhiguna Kuncoro, Miguel Ballesteros,
and Noah A Smith. 2016. Recurrent neural network
grammars. arXiv preprint arXiv:1602.07776 .

Daniel Fried, Mitchell Stern, and Dan Klein. 2017. Im-
proving neural parsing by disentangling model com-
bination and reranking effects. In Proceedings of the
Association for Computational Linguistics.

Hugo Hernault, Helmut Prendinger, David A DuVerle,
Mitsuru Ishizuka, and Tim Paek. 2010. Hilda: a dis-
course parser using support vector machine classifi-
cation. Dialogue and Discourse 1(3):1–33.

Geoffrey E Hinton, Nitish Srivastava, Alex Krizhevsky,
Ilya Sutskever, and Ruslan R Salakhutdinov. 2012.
Improving neural networks by preventing co-
adaptation of feature detectors. arXiv preprint
arXiv:1207.0580 .

Liang Huang and David Chiang. 2007. Forest rescor-
ing: Fast decoding with integrated language models.
In Proceedings of ACL 2007. Prague, Czech Rep.

Liang Huang, Suphan Fayong, and Yang Guo.
2012. Structured perceptron with inex-
act search. In Proceedings of NAACL.
http://www.isi.edu/ lhuang/perc-inexact.pdf.

Liang Huang and Kenji Sagae. 2010. Dynamic pro-
gramming for linear-time incremental parsing. In
Proceedings of ACL 2010. Uppsala, Sweden.

Diederik P Kingma and Jimmy Ba. 2014. Adam: A
method for stochastic optimization. arXiv preprint
arXiv:1412.6980 .

Eliyahu Kiperwasser and Yoav Goldberg. 2016. Sim-
ple and accurate dependency parsing using bidi-
rectional LSTM feature representations. CoRR
abs/1603.04351. http://arxiv.org/abs/1603.04351.

Jiangming Liu and Yue Zhang. 2016. Shift-reduce
constituent parsing with neural lookahead features.
arXiv preprint arXiv:1612.00567 .

Mitchell P Marcus, Mary Ann Marcinkiewicz, and
Beatrice Santorini. 1993. Building a large annotated
corpus of english: The penn treebank. Computa-
tional linguistics 19(2):313–330.

Haitao Mi and Liang Huang. 2015. Shift-reduce con-
stituency parsing with dynamic programming and
pos tag lattice. In Proceedings of NAACL 2015.

Graham Neubig, Chris Dyer, Yoav Goldberg, Austin
Matthews, Waleed Ammar, Antonios Anastasopou-
los, Miguel Ballesteros, David Chiang, Daniel
Clothiaux, Trevor Cohn, Kevin Duh, Manaal
Faruqui, Cynthia Gan, Dan Garrette, Yangfeng Ji,
Lingpeng Kong, Adhiguna Kuncoro, Gaurav Ku-
mar, Chaitanya Malaviya, Paul Michel, Yusuke
Oda, Matthew Richardson, Naomi Saphra, Swabha
Swayamdipta, and Pengcheng Yin. 2017. Dynet:
The dynamic neural network toolkit. arXiv preprint
arXiv:1701.03980 .

Tianze Shi, Liang Huang, and Lillian Lee. 2017.
Fast(er) exact decoding and global training for
transition-based dependency parsing via a minimal
feature set. In Proceedings of EMNLP 2017 (to ap-
pear).

Richard Socher, John Bauer, Christopher D Manning,
and Andrew Y Ng. 2013. Parsing with composi-
tional vector grammars. In Proceedings of the As-
sociation for Computational Linguistics. Associa-
tion for Computational Linguistics, volume 1, pages
455–465.

Mitchell Stern, Jacob Andreas, and Dan Klein. 2017a.
A minimal span-based neural constituency parser.
In Proceedings of the Association for Computational
Linguistics.

Mitchell Stern, Jacob Andreas, and Dan Klein.
2017b. A minimal span-based neural constituency
parser (code base). https://github.com/
mitchellstern/minimal-span-parser.

Mitchell Stern, Daniel Fried, and Dan Klein. 2017c.
Effective inference for generative neural parsing. In
Proceedings of Empirical Methods in Natural Lan-
guage Processing. pages 1695–1700.

https://aclweb.org/anthology/D16-1001
https://aclweb.org/anthology/D16-1001
https://aclweb.org/anthology/D16-1001
https://aclweb.org/anthology/D16-1001
http://www.isi.edu/~lhuang/perc-inexact.pdf
http://www.isi.edu/~lhuang/perc-inexact.pdf
http://www.isi.edu/~lhuang/perc-inexact.pdf
http://arxiv.org/abs/1603.04351
http://arxiv.org/abs/1603.04351
http://arxiv.org/abs/1603.04351
http://arxiv.org/abs/1603.04351
https://github.com/mitchellstern/minimal-span-parser
https://github.com/mitchellstern/minimal-span-parser


483

Masaru Tomita, editor. 1991. Generalized LR Parsing.
Kluwer Academic Publishers.

Oriol Vinyals, Łukasz Kaiser, Terry Koo, Slav Petrov,
Ilya Sutskever, and Geoffrey Hinton. 2015. Gram-
mar as a foreign language. In Advances in Neural
Information Processing Systems. pages 2773–2781.

Heng Yu, Liang Huang, Haitao Mi, and Kai Zhao.
2013. Max-violation perceptron and forced decod-
ing for scalable mt training. In Proceedings of
EMNLP 2013.

Kai Zhao and Liang Huang. 2017. Joint syntacto-
discourse parsing and the syntacto-discourse tree-
bank. In Proceedings of the 2017 Conference on
Empirical Methods in Natural Language Process-
ing. pages 2117–2123.


