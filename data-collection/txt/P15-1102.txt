



















































Co-training for Semi-supervised Sentiment Classification Based on Dual-view Bags-of-words Representation


Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics
and the 7th International Joint Conference on Natural Language Processing, pages 1054–1063,

Beijing, China, July 26-31, 2015. c©2015 Association for Computational Linguistics

Co-training for Semi-supervised Sentiment Classification Based on
Dual-view Bags-of-words Representation

Rui Xia1,2, Cheng Wang1, Xinyu Dai2, and Tao Li3,4
1School of Computer Science, Nanjing University of Science & Technology, China
2State Key Laboratory for Novel Software Technology, Nanjing University, China

3School of Computer Science, Florida International University, USA
4School of Computer Science, Nanjing University of Posts & Telecommunications, China

rxia@njust.edu.cn, wangcheng1022@gmail.com,
daixinyu@nju.edu.cn, taoli@cs.fiu.edu

Abstract

A review text is normally represented as
a bag-of-words (BOW) in sentiment clas-
sification. Such a simplified BOW model
has fundamental deficiencies in modeling
some complex linguistic phenomena such
as negation. In this work, we propose a
dual-view co-training algorithm based on
dual-view BOW representation for semi-
supervised sentiment classification. In
dual-view BOW, we automatically con-
struct antonymous reviews and model a
review text by a pair of bags-of-words
with opposite views. We make use of the
original and antonymous views in pairs,
in the training, bootstrapping and test-
ing process, all based on a joint observa-
tion of two views. The experimental re-
sults demonstrate the advantages of our ap-
proach, in meeting the two co-training re-
quirements, addressing the negation prob-
lem, and enhancing the semi-supervised
sentiment classification efficiency.

1 Introduction

In the past decade, there has been an explosion
of user-generated subjective texts on the Internet
in forms of online reviews, blogs and microblogs.
With the need of automatically identifying senti-
ments and opinions from those online texts, senti-
ment classification has attracted much attention in
the field of natural language processing.

Lots of previous research focused on the task
of supervised sentiment classification. However,
in some domains, it is hard to obtain a sufficient
amount of labeled training data. Manual annota-
tion is also very expensive and time-consuming.
To address this problem, semi-supervised learning

approaches were employed in sentiment classifica-
tion, to reduce the need for labeled reviews by tak-
ing advantage of unlabeled reviews.

The dominating text representation method in
both supervised and semi-supervised sentiment
classification is known as the bag-of-words (BOW)
model, which is difficult to meet the requirements
for understanding the review text and dealing with
complex linguistic structures such as negation. For
example, the BOW representations of two opposite
reviews “It works well” and “It doesn’t work well”
are considered to be very similar by most statistical
learning algorithms.

In supervised sentiment classification, many ap-
proaches have been proposed in addressing the
negation problem (Pang et al., 2002; Na et al.,
2004; Polanyi and Zaenen , 2004; Kennedy and
Inkpen, 2006; Ikeda et al., 2008; Li et al., 2010b;
Orimaye et al., 2012; Xia et al., 2013). Nev-
ertheless, in semi-supervised sentiment classifica-
tion, most of the current approaches directly ap-
ply standard semi-supervised learning algorithms,
without paying attention to appropriate representa-
tion for review texts. For example, Aue and Ga-
mon (2005) applied the naı̈ve Bayes EM algorithm
(Nigam et al., 2000). Goldberg and Zhu (2006) ap-
plied a graph-based semi-supervised learning algo-
rithm by (Zhu et al., 2003). Wan (2009) employed
a co-training approach for cross-language senti-
ment classification. Li et al. (2010a) employed co-
training with personal and impersonal views. Ren
et al. (2011) explored the use of label propagation
(Zhu and Ghahramani, 2002).

As pointed by (Goldberg and Zhu, 2006): it is
necessary to investigate better review text represen-
tations and similarity measures based on linguis-
tic knowledge, as well as reviews’ sentiment pat-
terns. However, to the best knowledge, such inves-
tigations are very scarce in the research of semi-

1054



supervised sentiment classification.
In (Xia et al., 2013), we have developed a

dual sentiment analysis approach, which creates
antonymous reviews and makes use of original and
antonymous reviews together for supervised sen-
timent classification. In this work, we propose
a dual-view co-training approach based on dual-
view BOW representation for semi-supervised sen-
timent classification. Specifically, we model both
the original and antonymous reviews by a pair of
bags-of-words with opposite views. Based on such
a dual-view representation, we design a dual-view
co-training approach. The training, bootstrapping
and testing processes are all performed by observ-
ing two opposite sides of one review. That is, we
consider not only how positive/negative the orig-
inal review is, but also how negative/positive the
antonymous review is.

In comparison with traditional methods, our
dual-view co-training approach has the following
advantages:

• Effectively address the negation problem;
• Automatically learn the associations among

antonyms;
• Better meet the two co-training requirements

in (Blum and Mitchell, 1998).

2 Related Work

The mainstream of the research in sentiment clas-
sification focused on supervised and unsupervised
learning tasks. In comparison, semi-supervised
sentiment classification has much less related stud-
ies. In this section, we focus on reviewing the work
of semi-supervised sentiment classification.

Aue and Gamon (2005) combined a small
amount of labeled data with a large amount of
unlabeled data in target domain for cross-domain
sentiment classification based on the EM algo-
rithm. Goldberg and Zhu (2006) presented a graph-
based semi-supervised learning algorithm (Zhu et
al., 2003) for the sentiment analysis task of rat-
ing inference. Dasgupta and Ng (2009) proposed
a semi-supervised approach to mine the unambigu-
ous reviews at first and then exploiting them to
classify the ambiguous reviews, via a combination
of active learning, transductive learning and en-
semble learning. Ren et al. (2011) explored the
use of label propagation (LP) (Zhu and Ghahra-
mani, 2002) in building a semi-supervised senti-
ment classifier, and compared their results with
Transductive SVMs(T-SVM). LP and T-SVM are

transductive learning methods where the test data
should participate in the training process.

Zhou et al. (2010) proposed a deep learning
approach called active deep networks to address
semi-supervised sentiment classification with ac-
tive learning. Socher et al. (2012) introduced a
deep learning framework called semi-supervised
recursive autoencoders for predicting sentence-
level sentiment distributions. The limitation of
deep learning approaches might be their depen-
dence on a considerable amount of unlabeled data
to learn the representations and the inability to ex-
plicitly model the negation problem.

One line of semi-supervised learning research
is to bootstrap class labels using techniques like
self-training, co-training and their variations. Wan
(2009) proposed a co-training approach to address
the cross-lingual sentiment classification problem.
They made use of the machine translation service
to produce two views (a English view and a Chi-
nese view) for co-training a Chinese review senti-
ment classifier, based on English corpus and unla-
beled Chinese corpus. Li et al. (2010a) proposed
an unsupervised method at first to automatically
separate the review text into a personal view and an
impersonal view, based on which the standard co-
training algorithm is then applied to build a semi-
supervised sentiment classifier. Li et al. (2011)
further studied semi-supervised learning for imbal-
anced sentiment classification by using a dynamic
co-training approach. Su et al. (2012) proposed
a multi-view learning approach to semi-supervised
sentiment classification with both feature partition
and language translation strategies (Wan , 2009).
Following (Li et al., 2010a), Li (2013) proposed
a co-training approach which exploits subjective
and objective views for semi-supervised sentiment
classification. Our approach can also be viewed as
a variation of co-training. The innovation of our
approach is the dual-view construction technique
by incorporating antonymous reviews and the boot-
strapping mechanism by observing two opposite
sides of one review.

3 The Proposed Approach

3.1 Dual-view BOW Representation for
Review Texts

Every coin has two sizes. In this work, we are mo-
tivated to automatically construct the antonymous
reviews, consider the original and antonymous re-
views as two opposite sides of one review, and rep-

1055



1

1

1

1

1

1

1

0

1

1

0

1

1

0

1

1

app

phone

didn't

work

well

disappointing

recommend

satisfactory

Feature 
Space

Original
View

Antonymous
View

Figure 1: An illustration of the dual-view BOW
representation. The feature vector with black font
color and grey background denotes the original
view; while the one with white font color and
black background denotes the reversed antony-
mous view.

resent them in pairs by a dual-view BOW model.
Look at the following example:

Original Review: “The app doesn’t
work well on my phone. Disappointing.
Don’t recommend it.”

Antonymous Review: “The app works
well on my phone. Satisfactory. Recom-
mend it.”

Given an original review, its antonymous review
is automatically created as follows1: 1) We first de-
tect the negations in each subsentence of the review
text; 2) If there is a negation, we remove negators
in that subsentence; 3) Otherwise, we reverse all
the sentiment words in the subsentence into their
antonyms, according to a pre-defined antonym dic-
tionary2.

We subsequently use a dual-view BOW model to
represent such a pair of reviews, as shown in Fig-
ure 1. The original and antonymous reviews will
be used in pairs in our dual-view semi-supervised
learning approach. As we determine the sentiment
of one review, we could observe not only the orig-
inal view, but also the antonymous view.

1It is worth noting that our emphasis here is not to generate
natural-language-like review texts. Since either the original or
the created antonymous review will be represented as a vector
of independent words in the BOW model, the grammatical
requirement is not as strict as that in human languages.

2In our experiments, we extract the antonym dic-
tionary from the WordNet lexicon http://wordnet.
princeton.edu/.

Antonymous ViewAntonymous ViewOriginal View

Antonymous
Sentiment
Classifier

Original
Sentiment
Classifier

Labeled
Original
Reviews

Labeled
Antonymous
Reviews

Unlabeled
Original
Reviews

Unlabeled
Antonymous
Reviews

Review
Reversion

Bootstrapping

Review
Reversion

Dual-view
Sentiment 
Consensus

Dual
Sentiment
Classifier

Figure 2: The process of dual-view co-training.
Again, the white font color and black background
are used to denote the antonymous view.

It is important to notice that the antony-
mous view removes all negations and incorporates
antonymous features. On this basis, we design a
dual-view co-training approach. We will introduce
our approach in detail in Section 3.2, and analyze
its potential advantages in Section 3.3.

3.2 The Dual-view Co-training Approach

Since the original and antonymous views form two
different views of one review text, it is natural to
employ the co-training algorithm, which requires
two views for semi-supervised classification.

Co-training is a typical bootstrapping algorithm
that first learns a separate classifier for each view
using the labeled data. The most confident predic-
tions of each classifier on the unlabeled data are
then used to construct additional labeled training
data iteratively. Co-training has been extensively
used in NLP, including statistical parsing (Sarkar ,
2001), reference resolution (Ng and Cardie, 2003),
part-of-speech tagging (Clark et al., 2003), word
sense disambiguation (Mihalcea, 2004), and senti-
ment classification (Wan , 2009; Li et al., 2010a).

But it should be noted that the dual views in
our approach are different from traditional views.
One important property of our approach is that
two views are opposite and therefore associated
with opposite class labels. Figure 2 illustrates the
process of dual-view co-training.

1056



(1) Dual-view training

For each instance in the initial labeled set, we con-
struct the dual-view representations. Let xlo and
xla denote the bags of words in the original view
and the antonymous view, respectively. Note that
the class labels in two views are kept opposite:
yla = 1 − ylo (y ∈ {0, 1}). That is, we reverse
the class label in the original view (i.e., positive to
negative, or vice versa), as the class label of the
created antonymous view.

Suppose L is the labeled set, with Lo and La
denoting the original-view and antonymous-view
labeled sets, respectively. We train two distinct
classifiers: the original-view classifier ho and the
antonymous-view classifier ha , based on Lo and
La, respectively. We further train a joint classifier
by using Lo and La together as the training data,
and refer to it as hd.

(2) Dual-view bootstrapping

In standard co-training, we allow each classifier to
examine the unlabeled set U and select the most
confidently predicted examples in each category.
The selected examples are then added into L ,
along with the predicted class labels.

In this work, we design a dual-view co-training
algorithm to bootstrap the class labels by a joint ob-
servation of two sides of one review. Specifically,
we propose a new bootstrapping mechanism, based
on a principle called dual-view sentiment consen-
sus. Given an unlabeled instance {xuo , xua}, dual
view sentiment consensus requires that, the orig-
inal prediction yuo and the antonymous prediction
should be opposite: yua = 1− yuo . In other words,
we only select the instances of which the original
prediction is positive/negative, and the same time
the antonymous prediction is negative/positive. To
increase the degree of sentiment consensus, we fur-
ther require that the predition yud of hd should be
the same as yuo .

We sort all unlabeled instances according to the
dual-view predictions in each class, filter the list
according to the dual-view sentiment consensus
principle, and add the top-ranked s instances in
each class to the labeled set. For each selected un-
labeled instance, its original view xuo is added into
Lo with class label yuo ; and the antonymous view
xua is added into La, with an opposite class label
yua = 1− yuo . When Lo and La receive the supple-
mental labeled instances, we update ho and ha.

Our bootstrapping mechanism differs from the
traditional methods in two major aspects: First, in
traditional co-training, given the same instance,
the class labels in two views are the same. But in
our approach, the class labels in two views need
to be opposite. Second, in traditional co-training,
the most confidently predicted examples in each
view are selected to extend the amount of labeled
data. It is dangerous to believe the confident but
incorrect predictions. While in our approach, the
candidates are further filtered by the principle of
dual-view sentiment consensus. In this way, the
labeling accuracy and learning efficiency can be
improved.

(3) Dual-view testing
Finally, in the testing stage, standard co-training
uses a joint set of features in two views to train the
classifier. In dual-view testing, we use ho and ha
to predict the test example in two views, and make
the final prediction by considering both sizes of the
review.

Given a test example xte with its original view
denoted by xteo and antonymous view denoted
by xtea , let po(·|xteo ) be the posterior probability
predicted by the original-view classifier ho, and
pa(·|xtea ) be the posterior probability predicted by
ha. The dual-view testing process can be formu-
lated as follows:

p(+|xte) = p(+|xteo , xtea ) =
po(+|xteo ) + pa(−|xtea )

2
;

p(−|xte) = p(−|xteo , xtea ) =
po(−|xteo ) + pa(+|xtea )

2
.

That is, the final positive score is assigned by
measuring not only how positive the original re-
view is, but also how negative the antonymous one
is; the negative score is assigned by measuring not
only how positive the original review is, but also
how negative the antonymous one is.

3.3 Advantages of Dual-view Co-training
Our proposed dual-view co-training approach has
the following three advantages.

(1) Effectively address the negation issue
We use the antonymous review as a view to effec-
tively address the negation issue. Let us revisit the
example in Section 3.1 and assume that the orig-
inal review (i.e., “The app doesn’t work well on
my phone. Disappointing. Do not recommend it.”)
is an unlabeled sample. Because the traditional

1057



BOW model cannot well represent negative struc-
tures, the review is likely to be incorrectly labeled
as positive and then added into the labeled set.

In our proposed approach, the antonymous re-
view (i.e., “The app works well on my phone. Sat-
isfactory. Recommend it.”) removed all the neg-
ative structures, and is thus more suited for the
BOW representation. In this example, the antony-
mous review is also likely to be marked as positive.
Hence, in this case, both the original review and
its antonymous review will be labeled as positive,
which violates the principle of dual-view sentiment
consensus as mentioned in Section 3.2. As a result,
the unlabeled instance will not be added into the
labeled set.

Therefore, our approach can overcome the limi-
tations of the conventional methods in addressing
the negation issue and reduce the labeling error
rate (caused by the negative structures) during the
bootstrapping process.

(2) Automatically learn the associations among
antonyms

In semi-supervised sentiment classification, only
limited association information between the words
and categories can be obtained from a small num-
ber of initial labeled data.

For instance, in the above example “disappoint-
ing” and “satisfactory” are a pair of antonyms.
From the initial labeled data, we may only learn
that “disappointing” is derogatory, but we cannot
infer that “satisfactory” is commendatory.

During the bootstrapping process in our
approach, when constructing the dual view rep-
resentation, the original view and its antonymous
view are required to have opposite class labels.
Hence we can automatically infer the relationship
between “satisfactory” and “disappointing” (e.g.,
one is positive and one is negative), thereby
improving the learning efficiency of the system.

(3) Better meet two co-training requirements

Compared with traditional methods, our dual-view
co-training can better meet the two co-training re-
quirements: 1) sufficient condition (i.e., each view
is sufficient for classification); 2) complementary
condition (i.e., the two views are conditionally in-
dependent).

First, for the sufficient condition, we use a dif-
ferent view construction method. Most traditional
methods construct the two views by feature parti-
tioning (i.e., dividing the original feature set into

two subsets), while we use data expansion by gen-
erating antonymous reviews. We will demonstrate
in the experimental section (Section 4.6), that our
data expansion method can construct better views
than the feature partition method in terms of pre-
dicting the class labels from individual views.

Second, as we know, every coin has two sides
and the two sides are often complementary. In
our proposed approach, the original review and its
antonymous review (i.e., two sides of one review)
are used as two views for co-training and they can
better meet the complementary condition. We will
illustrate this point in Section 4.6 by calculating the
KL divergence between the two views.

4 Experimental Study

4.1 Datasets and Experimental Settings
We conduct the experiments on the multi-domain
sentiment datasets, which were introduced in
(Blitzer et al., 2007) and have been widely used in
sentiment classification. It consists of four domains
(Book, DVD, Electronics, and Kitchen) of reviews
extracted from Amazon.com. Each of the four
datasets contains 1,000 positive and 1,000 negative
reviews. Following the experimental settings used
in (Li et al., 2010a), we randomly separate all the
reviews in each class into a labeled data set, a un-
labeled data set, and a test set, with a proportion of
10%, 70% and 20%, respectively. We report the av-
eraged results of 10-fold cross-validation in terms
of classification accuracy.

Note that our approach is a general framework
that allows different classification algorithms. Due
to the space limitation, we only report the results by
using logistic regression3. Note the similar conclu-
sions can be obtained by using the other algorithms
such as SVMs and naı̈ve Bayes. The LibLinear
toolkit4 is utilized, with a dual L2-regularized fac-
tor, and a default tradeoff parameter c. Similar to
(Wan , 2009; Li et al., 2010a), we carry out the ex-
periments with the unigram features without fea-
ture selection. Presence is used as the term weight-
ing scheme as it was reported in (Pang et al., 2002)
that it performed better than TF and TF-IDF. Fi-
nally, the paired t-test (Yang and Liu , 1999) is per-
formed to test the significance of the difference be-

3Logistic regression is quite similar to Maximum Entropy,
and has been proved to be more efficient in sentiment clas-
sification than some other classification algorithms including
naı̈ve Bayes and SVMs (Pang et al., 2002).

4http://www.csie.ntu.edu.tw/˜cjlin/
liblinear/

1058



BOOK DVD ELEC KITC Avg.
Baseline 0.680 0.691 0.726 0.740 0.709

LP 0.681 0.676 0.697 0.722 0.694
T-SVM 0.671 0.677 0.716 0.729 0.698

EM 0.702 0.706 0.758 0.744 0.728
Self-Training 0.689 0.705 0.736 0.751 0.720
Self-Reserved 0.690 0.708 0.735 0.754 0.722

Co-Static 0.696 0.714 0.745 0.762 0.729
Co-Dynamic 0.701 0.725 0.756 0.767 0.737

Co-PI 0.702 0.716 0.746 0.769 0.733
Our approach 0.721 0.738 0.769 0.780 0.752

Table 1: The semi-supervised classification accu-
racy of ten systems.

tween two systems, with a default significant level
of 0.05.

4.2 Compared Systems

We implement the following nine systems and
compare them with our approach:

• Baseline, the supervised baseline trained with
the initial labeled data only;
• Expectation Maximization (EM), with the

naı̈ve Bayes model proposed by Nigam et al.
(2000);
• Label Propagation (LP), a graph-based

semi-supervised learning method proposed by
Zhu and Ghahramani (2002);
• Transductive SVM (T-SVM), an extension

of SVM so that it can exploit unlabeled data in
semi-supervised learning ( Joachims, 1999);
• Self-Training, a bootstrapping model that

first trains a classifier, uses it to classify the
unlabeled data, and adds the most confident
data to the labeled set;
• Self-Reserved, a variation of self-training

proposed in (Liu et al., 2013),with a reserved
procedure to incorporate some less confident
examples;
• Co-Static, the co-training algorithm by using

two static partitions of feature set as two views
(Blum and Mitchell, 1998);
• Co-Dynamic, a variation of co-training that

uses dynamic feature space in each loop. It
was reported in (Li et al., 2011) that the Co-
Dynamic significantly outperforms Co-Static
significantly;
• Co-PI, another variation of co-training pro-

posed by (Li et al., 2010a), by using personal

0 100 200 300 400 500 600 700 800 900
Number of new labeled data bootstrapped from unlabeled set

0.69

0.70

0.71

0.72

0.73

0.74

0.75

A
cc

u
ra

cy

Performance across four datasets

Self-training
Co-static
Co-PI
Co-dynamic
Our approach

Figure 3: Comparsion of different boostrapping
methods.

and impersonal views for co-training.

4.3 Performance Comparison
In table 1, we report the semi-supervised classifica-
tion accuracy of ten evaluated systems. We report
the results with 200 labeled, 1400 unlabeled and
400 test reviews. Note that the similar conclusions
can be obtained when the size of the initial labeled
data changes. We will discuss its influence later.

As can be seen, trained with only 200 labeled
data, the supervised baseline yields an average ac-
curacy of 0.709. Self-training gains an improve-
ment of 1.1%. Self-reserved does not show sig-
nificant priority against Self-training. Three co-
training systems (Co-static, Co-dynamic and Co-
PI) get significant improvements. They increase
the supervised baseline by 2.0%, 2.8% and 2.4%,
respectively.

It is somehow surprising that T-SVM and LP do
not outperform the supervised baseline, probably
because the supervised baseline is obtained by lo-
gistic regression, which was reported to be more ef-
fective than SVMs in sentiment classification (the
supervised result of SVMs is 0.695).

Our proposed approach significantly outper-
forms all the other methods. It gains the improve-
ment over the supervised baseline, Self-training,
Co-static, Co-dynamic and Co-PI by 4.3%, 3.2%,
2.3%, 1.5% and 1.9%, respectively. All of the im-
provements are significant according to the paired
t-test.

4.4 Comparison of Bootstrapping Methods
In Figure 3, we further compare five bootstrap-
ping methods by drawing the accuracy curve dur-

1059



0.55

0.6

0.65

0.7

0.75

0.8

20 50 100 150 200 300 400

Supervised Co-Dynamic Co-PI Our Approach

Figure 4: Influence of the size of initial labeled
data.

ing the bootstrapping process. The x-axis denotes
the number of new labeled data bootstrapped from
the unlabeled data.

We can roughly rank five bootstrapping methods
as follows: Our approach � Co-dynamic > Co-
PI > Co-static� Self-training. Self-training gives
the worst performance. Co-static works better but
the effect is limited. Co-PI and Co-dynamic are
significantly better. Our proposed approach outper-
forms the other systems robustly, along with the in-
creased number of the new labeled data. It suggests
that our approach is very efficient in bootstrapping
the class labels from the unlabeled data.

4.5 Influence of the Size of the Initial Labeled
Set

The above results are obtained with 200 labeled,
1400 unlabeled and 400 test reviews. We now tune
the size of the initial labeled set (from 20 to 400),
and report its influence in Figure 4. For all the set-
tings, we fix the size of test set as 400. The x-axis
denotes the number of initial labeled set. For ex-
ample, “20” denotes the setting of 20 labeled and
1580 unlabeled data.

We can observe that our all methods improve as
the initial size increases. But the improvements be-
come limited when the size becomes larger. When
the initial size is 400, the semi-supervised perfor-
mance is close to the golden result obtained by the
supervised classifier trained with all 1600 labeled
data.

Our approach performs consistently the best
across different sizes of the initial sizes. The
smaller the initial size is, the more improvements
our approach can gain, in comparison with the
other methods. This confirms our analysis in Sec-
tion 3.3 that the technique of dual-view construc-
tion is very effective to boost the semi-supervised

0 200 400 600 800 1000
Number of new labeled data bootstrapped from unlabeled set

0.60

0.62

0.64

0.66

0.68

0.70

0.72

0.74

A
cc

u
ra

cy

DVD

Co-PI-view1
Co-PI-view2
Co-PI
Original view
Antonymous view
Dual-view testing

0 200 400 600 800 1000
Number of new labeled data bootstrapped from unlabeled set

0.64

0.66

0.68

0.70

0.72

0.74

0.76

0.78

A
cc

u
ra

cy

ELEC

Co-PI-view1
Co-PI-view2
Co-PI
Original view
Antonymous view
Dual-view testing

Figure 5: Comparison of different views on the
DVD and Electronics datasets.

classification performance, especially when the
size of the initial labeled set is small.

4.6 Discussion on the Two Co-training
Requirements

Ideally, co-training requires that each view is
sufficient for classification (sufficient condition)
and two views provide complementary informa-
tion of the instance,(complementary condition).In
this section, we answer the following question
empirically: whether our approach could meet the
two requirements?

(1) Sufficient condition

In Figure 5, we report the classification perfor-
mance obtained by the classifiers trained with dis-
tinct views and compared them with the two views
in Co-PI, on the DVD and Electronics datasets.
The observation in Book is similar to that in Elec-
tronics; the observation in DVD is similar to that in
Kitchen.

Seen from Figure 5, the classification perfor-
mance of both the original-view and antonymous-
view classifiers are satisfactory. It shows that in

1060



our approach, each individual view is sufficient to
predict the sentiment. In comparison with the two
views in Co-PI (i.e., the personal and impersonal
views), two views in our approach perform signifi-
cantly better.

As has been mentioned in Section 3.3, in tradi-
tional methods, such as Co-PI and Co-dynamic,
two views are created by data partition (or feature
partition). In comparison, the two views in our
approach are constructed in a manner of data ex-
pansion. By creating a new antonymous view, our
approach can provide more sufficient information
of the reviews than traditional methods.

(2) Complementary condition
Since we have not found a direct measure of the
complementarity of two views, we instead calcu-
late the Kullback-Leibler (KL) divergence between
them, based on an assumption that two views with
higher KL divergence can provide more comple-
mentary information of the instance.

KL divergence is a widely used metric of statis-
tical distance. We assume that distribution of the
review text is multinomial, and calculate the K-L
divergence between two views as follows:

DKL(p||q) =
V∑

i=1

pi log
(

pi
qi

)
where pi and qi are the probabilities of word ap-
pearing in two views, respectively. In our ex-
periments, we use information gain (IG) to select
a set of discriminative words with the dimension
V = 2000.

In Table 2, we report the results of three differ-
ent methods: 1) dataset random partition; 2) per-
sonal and impersonal views in Co-PI; 3) original
and antonymous views in our approach. We can
observe from Table 2 that, random partition has
the lowest KL divergence. It shows that the dis-
tributional distance between two randomly parti-
tioned views is very small. Co-PI is a higher value,
but it still does not have significant difference in
two views. By contrast, the KL divergence be-
tween the original view and the antonymous view
is much higher than both random partition and Co-
PI. It demonstrates that the distributions of two
views in our approach are significantly different.
We thereby infer that the two views constructed in
our approach can provide more complementary in-
formation than traditional methods. It is reason-
able since the antonymous view incorporates the

KL divergence
Random Partition 2.43

Co-PI 4.59
Our approach 12.33

Table 2: The average KL divergence between two
views across four datasets.

antonyms that might have not appeared in the origi-
nal view (e.g., “satisfactory” in the example in Sec-
tion 3.2). These features might provide new infor-
mation about the instance.

4.7 The Effect of Dual-view Testing

In Figure 5, we can further observe the effect of
dual-view testing. On the Electronics dataset, the
antonymous view performs better than the orig-
inal view. This suggests the advantage of the
antonymous view, as it removes the negations and
thus is more suitable for the BOW representa-
tion. On the DVD dataset, the original view is
slightly better. This is also reasonablel, because the
antonymous review is automatically created and its
quality might be limited in some cases. By tak-
ing two opposite views into a joint consideration,
our dual-view testing technique guarantees a satis-
factory classification performance across different
datasets.

Note that in the current version, the original-
view and antonymous-view classifiers have the
same predicting weight. We believe that by learn-
ing the tradeoff between two views in different set-
tings may further improve our approach’s perfor-
mance. For example, if the original view on the
Electronics dataset gets a relatively larger weight,
dual-view testing might gain more improvements.

5 Conclusions

In this work, a review text is represented by a
pair of bags-of-words with opposite views (i.e., the
original and antonymous views). By making use
of two views in pairs, a dual-view co-training al-
gorithm is proposed for semi-supervised sentiment
classification. The dual-view representation is in a
good accordance with the two co-training require-
ments (i.e., sufficient condition and complemen-
tary condition). The experimental results demon-
strate the effect of our approach, in addressing the
negation problem and enhancing the bootstrapping
efficiency for semi-supervised sentiment classifica-
tion.

1061



Acknowledgement

The work is supported by the Natural Science
Foundation of China (61305090), and the Jiangsu
Provincial Natural Science Foundation of China
(BK2012396).

References
A. Aue and M. Gamon. 2005. Customizing Sentiment

Classifiers to New Domains: A Case Study. In Pro-
ceedings of Recent Advances in Natural Language
Processing.

J. Blitzer, M. Dredze, and F. Pereira. 2007. Biogra-
phies, Bollywood, Boom-boxes and Blenders: Do-
main Adaptation for Sentiment Classification. In
Proceedings of ACL .

A. Blum and T. Mitchell. 1998. Combining labeled
and unlabeled data with co-training. In Proceedings
of COLT.

S. Clark, J. R. Curran, and M. Osborne. 2003. Boot-
strapping POS taggers using unlabelled data. In Pro-
ceedings of CoNLL.

S. Dasgupta and V. Ng. 2009. Mine the Easy and Clas-
sify the Hard: Experiments with Automatic Senti-
ment Classification. In Proceedings of ACL-IJCNLP.

A. Goldberg and X. Zhu. 2006. Seeing stars when
there aren’t many stars: graph-based semi-supervised
learning for sentiment categorization. In Proceed-
ings of the Workshop on TextGraphs at HLT-NAACL.

M. Hu and B. Liu. 2004. Mining opinion features in
customer reviews. In Proceedings of the National
Conference on Artificial Intelligence (AAAI).

D. Ikeda, H. Takamura, L. Ratinov, and M. Okumura.
2008. Learning to Shift the Polarity of Words for
Sentiment Classification. In Proceedings of IJCNLP.

T. Joachims. 1999. Transductive Inference for Text
Classification using Support Vector Machines. In
Proceedings of ICML.

A. Kennedy and D. Inkpen. 2006. Sentiment classi-
fication of movie reviews using contextual valence
shifters. Computational Intelligence, 22:110–125.

LaTeX Error: File ‘url.sty’ not found. V. Ng and C.
Cardie. 2003. Weakly supervised natural language
learning without redundant views. In Proceedings of
HLT-NAACL.

K. Nigam, A. McCallum, S. Thrun, and T. Mitchell.
2000. Text classification from labeled and unlabeled
documents using EM. Machine Learning, 39(2/3):
103–134.

S. Li, C. Huang, G. Zhou, and S. Y. M. Lee. 2010a.
Employing personal/impersonal views in supervised
and semi-supervised sentiment classification. In Pro-
ceedings of the Annual Meeting of the Association for
Computational Linguistics (ACL) .

S. Li, S. Lee, Y. Chen, C. Huang, and G. Zhou. 2010b.
Sentiment Classification and Polarity Shifting. In
Proceeding of the International Conference on Com-
putational Linguistics (COLING).

S. Li, Z. Wang, G. Zhou, and S. Lee. 2011. Semi-
Supervised Learning for Imbalanced Sentiment Clas-
sification. In Proceedings of IJCAI.

S. Li. 2013. Sentiment classification using subjective
and objective views. International Journal of Com-
puter Applications, 80(7): 30–34.

Z. Liu, X. Dong, Y. Guan, and J. Yang. 2013. Reserved
Self-training: A Semi-supervised Sentiment Classifi-
cation Method for Chinese Microblogs. In Proceed-
ings of IJCNLP.

R. Mihalcea. 2004. Co-training and self-training
for word sense disambiguation. In Proceedings of
CoNLL.

J. Na, H. Sui, C. Khoo, S. Chan, and Y. Zhou. 2004.
Effectiveness of simple linguistic processing in au-
tomatic sentiment classification of product reviews.
In Proceeding of the Conference of the International
Society for Knowledge Organization.

S. Orimaye, S. Alhashmi, and E. Siew. 2012. Buy it -
don’t buy it: sentiment classification on Amazon re-
views using sentence polarity shift. In Proceedings
of the Pacific Rim International Conferences on Arti-
ficial Intelligence (PRICAI).

B. Pang, L. Lee, and S. Vaithyanathan. 2002. Thumbs
up?: sentiment classification using machine learning
techniques. In Proceedings of EMNLP.

L. Polanyi and A. Zaenen. 2004. Contextual lexical
valence shifters. In Proceedings of the AAAI Spring
Symposium on Exploring Attitude and Affect in Text.

Y. Ren, N. Kaji, N. Yoshinaga, M. Toyoda, and M.
Kitsuregawa. 2011. Sentiment Classification in
Resource-Scarce Languages by using Label Prop-
agation. In Proceedings of the Pacific Asia Con-
ference on Language, Information and Computation
(PACLIC).

A. Sarkar. 2001. Applying cotraining methods to sta-
tistical parsing. In Proceedings of NAACL.

R. Socher, J. Pennington, E. H. Huang, and A. Y. 2012.
Semi-Supervised Recursive Autoencoders for Pre-
dicting Sentiment Distributions. In Proceedings of
EMNLP.

Y. Su, S. Li, S. Ju, G. Zhou and J. Li. 2012. 2012.
Multi-view Learning for Semi-supervised Sentiment
Classification. In Proceedings of the International
Conference on Asian Language Processing.

1062



X. Wan. 2009. Co-Training for Cross-Lingual Senti-
ment Classification. In Proceedings of ACL-IJCNLP.

R. Xia, T. Wang, X. Hu, S. Li, and C. Zong. 2013. Dual
Training and Dual Prediction for Polarity Classifica-
tion. In Proceedings of ACL.

Y. Yang and X. Liu. 1999. A re-examination of text
categorization methods. In Proceedings SIGIR.

S. Zhou, Q. Chen, and X. Wang. 2010. Active Deep
Networks for Semi-Supervised Sentiment Classifica-
tion. In Proceedings of COLING.

X. Zhu and Z. Ghahramani. 2002. Learning
from labeled and unlabeled data with label prop-
agation. Technical Report CMU-CALD-02-107,
Carnegie Mellon University.

X. Zhu, Z. Ghahramani, and J. Lafferty. 2003. Semi-
supervised learning using Gaussian fields and har-
monic functions. In Proceddings of the International
Conference on Machine Learning (ICML).

1063


