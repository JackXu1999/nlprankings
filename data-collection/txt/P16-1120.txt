



















































Bilingual Segmented Topic Model


Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 1266–1276,
Berlin, Germany, August 7-12, 2016. c©2016 Association for Computational Linguistics

Bilingual Segmented Topic Model

Akihiro Tamura and Eiichiro Sumita
National Institute of Information and Communications Technology

3-5 Hikaridai, Seika-cho, Soraku-gun, Kyoto, JAPAN
{akihiro.tamura, eiichiro.sumita}@nict.go.jp

Abstract

This study proposes the bilingual seg-
mented topic model (BiSTM), which hi-
erarchically models documents by treat-
ing each document as a set of segments,
e.g., sections. While previous bilingual
topic models, such as bilingual latent
Dirichlet allocation (BiLDA) (Mimno et
al., 2009; Ni et al., 2009), consider only
cross-lingual alignments between entire
documents, the proposed model consid-
ers cross-lingual alignments between seg-
ments in addition to document-level align-
ments and assigns the same topic distri-
bution to aligned segments. This study
also presents a method for simultane-
ously inferring latent topics and segmen-
tation boundaries, incorporating unsuper-
vised topic segmentation (Du et al., 2013)
into BiSTM. Experimental results show
that the proposed model significantly out-
performs BiLDA in terms of perplexity
and demonstrates improved performance
in translation pair extraction (up to +0.083
extraction accuracy).

1 Introduction

Probabilistic topic models, such as probabilis-
tic latent semantic analysis (PLSA) (Hofmann,
1999) and latent Dirichlet allocation (LDA) (Blei
et al., 2003), are generative models for documents
that have been used as unsupervised frameworks
to discover latent topics in document collections
without prior knowledge. These topic models
were originally applied to monolingual data; how-
ever, various recent studies have proposed the use
of probabilistic topic models in multilingual set-

��������	


���
�


�������

�����

��

���������������

�����������	
�������
�����������	
�������	����	

��������	�����	��	
�������	

��	���� �� ��		�	�����	������	

������ �� �
	������	�������	

����				�� ������� �����	�	

������	��	�											�	�������	��	

����								�	��										���������	

���� ��	��	���	�������

��������	


���	����	����������

��������	

������

������������������

��� (football)

������: soccer��	


��� �����1��

�11��2 ������

������ �� �!

"#�$ �%&'()�

( *�+�ciatiall ; ,-

./�+�� �0�1�2

�&'()�( 34./�+

1 ��

2 �� (name)

3 �� (game)

4 �� (history)

5 	
����
� �

Figure 1: Wikipedia Article Example

tings1, where latent topics are shared across multi-
ple languages. These models have improved sev-
eral multilingual tasks, such as translation pair ex-
traction and cross-lingual text classification (see
the survey paper by Vulić et al. (2015) for details).

Most multilingual topic models, including bilin-
gual LDA (BiLDA) (Mimno et al., 2009; Ni et
al., 2009), model a document-aligned comparable
corpus, such as a collection of Wikipedia articles,
where aligned documents are topically similar but
are not direct translations2. In particular, these
models assume that the documents in each tuple
share the same topic distribution and that each
cross-lingual topic has a language-specific word
distribution.

Existing multilingual topic models consider
only document-level alignments. However, most
documents are hierarchically structured, i.e., a
document comprises segments (e.g., sections and
paragraphs) that can be aligned across languages.
Figure 1 shows a Wikipedia article example,
which contains a set of sections. Sections 1, 2,
and 3 in the English article correspond topically to
sections 4, 2, and 3 in the Japanese counterpart, re-

1In this work, we deal with a bilingual setting, but our
approach can be extended straightforwardly to apply to more
than two languages.

2In this study, we focus on models for a document-aligned
comparable corpus. We describe other types of multilingual
topic models and their limitations in Section 7.

1266



spectively. To date, such segment-level alignments
have been ignored; however, we consider that such
corresponding segments must share the same topic
distribution.

Du et al. (2010) have shown that segment-level
topics and their dependencies can improve model-
ing accuracy in a monolingual setting. Based on
that research, we expect that segment-level topics
can also be useful for modeling multilingual data.

This study proposes a bilingual segmented topic
model (BiSTM) that extends BiLDA to capture
segment-level alignments through a hierarchical
structure. In particular, BiSTM considers each
document as a set of segments and models a docu-
ment as a document-segment-word structure. The
topic distribution of each segment (per-segment
topic distribution) is generated using a Pitman–
Yor process (PYP) (Pitman and Yor, 1997), in
which the base measure is the topic distribution of
the related document (per-document topic distri-
bution). In addition, BiSTM introduces a binary
variable that indicates whether two segments in
different languages are aligned. If two segments
are aligned, their per-segment topic distributions
are shared; if they are not aligned, they are inde-
pendently generated.

BiSTM leverages existing segments from a
given segmentation. However, a segmentation is
not always given, and a given segmentation might
not be optimal for statistical modeling. Therefore,
this study also presents a model, BiSTM+TS, that
incorporates unsupervised topic segmentation into
BiSTM. BiSTM+TS integrates point-wise bound-
ary sampling into BiSTM in a manner similar to
that proposed by Du et al. (2013) and infers seg-
mentation boundaries and latent topics jointly.

Experiments using an English–Japanese and
English–French Wikipedia corpus show that the
proposed models (BiSTM and BiSTM+TS) sig-
nificantly outperform the standard bilingual topic
model (BiLDA) in terms of perplexity, and that
they improve performance in translation extrac-
tion (up to +0.083 top 1 accuracy). The exper-
iments also reveal that BiSTM+TS is comparable
to BiSTM, which uses manually provided segmen-
tation, i.e., section boundaries in Wikipedia arti-
cles.

2 Bilingual LDA

This section describes the BiLDA model (Mimno
et al., 2009; Ni et al., 2009), which we take as

� �

� � � �� �

� � � �� �

�
�

�
�

��

�

�

�

�

����������

����������

Figure 2: Graphical Model of BiLDA

Algorithm 1 Generative Process of BiLDA
1: for each topic k ∈ {1, ..., K} do
2: for each language l ∈ {e, f} do
3: choose ϕlk ∼ Dirichlet(βl)
4: end for
5: end for
6: for each document pair di (i ∈ {1, ..., D}) do
7: choose θi ∼ Dirichlet(α)
8: for each language l ∈ {e, f} do
9: for each word wlim (m ∈ {1, ..., N li}) do

10: choose zlim ∼ Multinomial(θi)
11: choose wlim ∼ p(wlim|zlim,ϕl)
12: end for
13: end for
14: end for

our baseline. BiLDA is a bilingual extension of
basic monolingual LDA (Blei et al., 2003) for
a document-aligned comparable corpus. While
monolingual LDA assumes that each document
has its own topic distribution, BiLDA assumes that
aligned documents share the same topic distribu-
tion and discovers latent cross-lingual topics.

Algorithm 1 and Figure 2 show the genera-
tive process and graphical model, respectively, of
BiLDA. BiLDA models a document-aligned com-
parable corpus, i.e., a set of D document pairs
in two languages, e and f . Each document pair
di (i ∈ {1, ..., D}) comprises aligned documents
in the language e and f : di=(dei , d

f
i ). BiLDA as-

sumes that each topic k ∈ {1, ..., K} comprises
the set of a discrete distribution over words for
each language. Each language-specific per-topic
word distribution ϕlk (l ∈ {e, f}) is drawn from
a Dirichlet distribution with the prior βl (Steps
1-5). To generate a document pair di, the per-
document topic distribution θi is first drawn from
a Dirichlet distribution with the prior α (Step 7).
Thus, aligned documents dei and d

f
i share the same

topic distribution. Then, for each word at m ∈
{1, ..., N li} in document dli in language l, a latent
topic assignment zlim is drawn from a multinomial

1267



� �

� � � �� �

� � � �� �

�
�

�
�

��

�

�

�

�

����������

����������

�

�

�
�

�
�

�

�

�

Figure 3: Graphical Model of BiSTM

Algorithm 2 Generative Process of BiSTM
1: for each topic k ∈ {1, ..., K} do
2: for each language l ∈ {e, f} do
3: choose ϕlk ∼ Dirichlet(βl)
4: end for
5: end for
6: for each document pair di (i ∈ {1, ..., D}) do
7: choose θi ∼ Dirichlet(α)
8: if yi are not given then
9: choose γi ∼ Beta(η0, η1)

10: choose yi ∼ Bernoulli(γi)
11: end if
12: generate aligned segment sets ASi = genAS(yi)
13: for each set ASig (g ∈ {1, ..., |ASi|}) do
14: choose νig ∼ PYP(a, b, θi)
15: end for
16: for each language l ∈ {e, f} do
17: for each segment slij (j ∈ {1, ..., Sli}) do
18: get index of slij in ASi: g =get idx(ASi,s

l
ij)

19: for each word wlijm (m ∈ {1, ..., N lij}) do
20: choose zlijm ∼ Multinomial(νig)
21: choose wlijm ∼ p(wlijm|zlijm, ϕl)
22: end for
23: end for
24: end for
25: end for

distribution with the prior θi (Step 10). Later, a
word wlim is drawn from a probability distribution
p(wlim|zlim, ϕl) given the topic zlim (Step 11).

3 Bilingual Segmented Topic Model

Here, we describe BiSTM, which extends BiLDA
to capture segment-level alignments. Algorithm
2 and Figure 3 show the generative process and
graphical model, respectively, of BiSTM. As can
be seen in Figure 3, BiSTM introduces a segment-
level layer between the document- and word-level
layers in both languages. In other words, per-
segment topic distributions for each language, νe

and νf , are introduced between per-document
topic distributions θ and topic assignments for

words, ze and zf . In addition, BiSTM incorpo-
rates binary variables y to represent segment-level
alignments.

Each document dli in a pair of aligned doc-
uments di is divided into Sli segments: d

l
i =∪Sli

j=1 s
l
ij . BiSTM makes the same assumption for

per-topic word distributions as BiLDA, i.e., ϕlk are
language-specific and drawn from Dirichlet distri-
butions (Steps 1-5).

In the generative process for a document pair
di, the per-document topic distribution θi is first
drawn in the same way as in BiLDA (Step 7).
Thus, in BiSTM, each document pair shares the
same topic distribution.

Then, if segment-level alignments are not given,
yi are generated (Steps 8-11). We assume that
each document pair di has a probability γi that
indicates comparability between segments across
languages. γi is drawn from a Beta distribution
with the priors η0 and η1 (Step 9). Then, each of
yi is drawn from a Bernoulli distribution with the
prior γi (Step 10). Here, yijj′ = 1 if and only if
seij and s

f
ij′ are aligned; otherwise, yijj′ = 0. Note

that if segment-level alignments are observed, then
Steps 8-11 are skipped. Later, a set of aligned
segment sets ASi is generated based on yi (Step
12). For example, given dei = {sei1, sei2}, dfi =
{sfi1, sfi2, sfi3}, yi11 and yi12 are 1, and the other y’s
are 0, ASi = {ASi1 = {sei1, sfi1, sfi2}, ASi2 =
{sei2}, ASi3 = {sfi3}} is generated in Step 12.
Then, for each aligned segment set ASig (g ∈
{1, ..., |ASi|}), the per-segment topic distribution
νig is obtained from a Pitman–Yor process with
the base measure θi, the concentration parame-
ter a, and the discount parameter b (Step 14).
Through Steps 12-15, aligned segments indicated
by y share the same per-segment topic distribu-
tion. For instance, sei1, s

f
i1, and s

f
i2 have the same

topic distribution νi1 ∼ PYP(a, b, θi) in the above
example.

Then, for each word at m ∈ {1, ..., N lij} in
segment slij in document d

l
i in language l, a la-

tent topic assignment zlijm is drawn from a multi-
nomial distribution with the prior νig (Step 20),
where g denotes the index of the element set of
ASi that includes the segment slij , e.g., g for s

f
i2

is 1. Subsequently, a word wlijm is drawn based on
the assigned topic zlijm and the language-specific
per-topic word distribution ϕl in the same manner
as in BiLDA (Step 21).

1268



tigk Table count of topic k in the CRP for ali-
gned segment set g in document pair i.

tig K-dimensional vector, where k-th value
is tigk.

tig· Total table count in aligned segment set
g in document pair i, i.e.,

∑
k tigk.

nigk Total number of words with topic k in al-
igned segment set g in document pair i.

nig· Total number of words in aligned segme-
nt set g in document pair i, i.e.,

∑
k nigk.

M lkw Total number of word w with topic k in
language l.

M lk |W l|-dimensional vector, where w-th
value is M lkw.

Table 1: Statistics used in our Inference

3.1 Inference for BiSTM

In inference, we find the set of latent variables
θ, ν, z, and ϕ that maximizes their posterior
probability given the model parameters α, β and
observations w, y, i.e., p(θ, ν, z, ϕ|α, β, w,y).
Here, a language-dependent variable without a su-
perscript denotes both of the variable in language
e and that in f , e.g., z = {ze, zf}. Unfortu-
nately, as in other probabilistic topic models, such
as LDA and BiLDA, we cannot compute this pos-
terior using an exact inference method. This sec-
tion presents an approximation method for BiSTM
based on blocked Gibbs sampling, inspired by Du
et al. (2013).

In our inference, the hierarchy in BiSTM, i.e.,
the generation of ν and z, is explained by the
Chinese restaurant process (CRP), through which
the parameters θ, ν, and ϕ are integrated out,
and the statistics on table counts in the CRP, t,
are introduced. Table 1 lists all statistics used in
our inference, where W l denotes a vocabulary
set in language l. Moreover, to accelerate con-
vergence, we introduce an auxiliary binary vari-
able δlijm for w

l
ijm, indicating whether w

l
ijm is

the first customer on a table (δlijm = 1) or not
(δlijm = 0), and tigk is computed based on δ
in the same manner as in Chen et al. (2011):

tigk =
∑

slij∈ASig

N lij∑
m=1

δlijmI(z
l
ijm = k), where I(x)

is a function that returns 1 if the condition x is true
and 0 otherwise.

Our inference groups zlijm and δ
l
ijm (each group

is called a “block”) and jointly samples them.

Moreover, if y is not observed, our inference al-
ternates two different kinds of blocks, (zlijm, δ

l
ijm)

and yijj′ . In each sampling, individual variables
are resampled, conditioned on all other variables.
In the following, we describe each sampling stage.

Sampling (z, δ):
The joint posterior distribution of z, w, and δ is
induced in a manner similar to that in Du et al.
(2010; 2013): p(z, w, δ|α, β, a, b, y)

=
D∏

i=1

(
BetaK(α +

∑
ASi

tig)
BetaK(α)∏

ASi

(
(b|a)tig·
(b)nig·

K∏
k=1

S
(
nigk, tigk, a

)(nigk
tigk

)−1))
K∏

k=1

(
BetaW e(βe + Mek )

BetaW e(βe)
BetaW f (βf + M

f
k )

BetaW f (βf )

)
,

where BetaK(·) and BetaW l(·) are K- and |W l|-
dimensional beta functions, respectively, (b|a)n is
the Pochhammer symbol3, and (b)n is given by
(b|1)n. S(n,m, a) is a generalized Stirling num-
ber of the second kind (Hsu and Shiue, 1998),
which is given by the linear recursion S(n +
1,m, a) = S(n,m− 1, a)+ (n−ma)S(n, m, a).
To reduce computational cost, the Stirling num-
bers are preliminarily calculated in a logarithm
format (Buntine and Hutter, 2012). Then, the
cached values are used in our sampling.

The joint conditional distributions of
zlijm and δ

l
ijm are obtained from the

above joint distribution using Bayes’ rule:
p(zlijm = k, δ

l
ijm = 1|z−z

l
ijm , w, δ−δ

l
ijm , α, β, a, b, y)

=
βl

wlijm
+ M l

kwlijm∑
w∈W l(βlw + M

l
kw)

αk +
∑

ASi
tigk∑K

k=1(αk +
∑

ASi
tigk)

b + atig′·
b + nig′·

S(nig′k + 1, tig′k + 1, a)
S(nig′k, tig′k, a)

tig′k + 1
nig′k + 1

,

p(zlijm = k, δ
l
ijm = 0|z−z

l
ijm , w, δ−δ

l
ijm , α, β, a, b, y)

=
βl

wlijm
+ M l

kwlijm∑
w∈W l(βlw + M

l
kw)

1
b + nig′·

S(nig′k + 1, tig′k, a)
S(nig′k, tig′k, a)

nig′k + 1− tig′k
nig′k + 1

,

where slij is included in ASig′ .

Sampling y:
In our inference, each aligned segment set cor-
responds to a restaurant in the CRP. We regard
the sampling of yijj′ as the choice of splitting or
merging restaurant(s) in a manner similar to that

3(b|a)n = ∏n−1t=0 (b + ta).
1269



in the sampling of segmentation boundaries in Du
et al. (2013). In particular, if yijj′ = 0, then
one aligned segment set ASm is split into two
aligned segment sets ASl and ASr, where ASl,
ASr, and ASm include seij , s

f
ij′ , and both, re-

spectively. If yijj′ = 1, then ASl and ASr are
merged to ASm. For simplicity, our inference
specifies ASl and ASr based on the current y as
follows: if ASi(seij) = ASi(s

f
ij′), then ASl =

{seij} ∪ ASfi (seij) \ {sfij′} and ASr = {sfij′} ∪
ASei (s

f
ij′)\{seij}; otherwise, ASl = ASi(seij) and

ASr = ASi(s
f
ij′). Here, ASi(j) is the element set

of ASi that includes the segment j, and ASli(j)
is the set of segments in language l included in
ASi(j). For example, in the example in Section 3,
ASi(s

f
i1) = ASi1 = {sei1, sfi1, sfi2}, ASei (sfi1) =

{sei1}, and ASfi (sfi1) = {sfi1, sfi2}. In addition, if
yi11 = 0, then ASm = {sei1, sfi1, sfi2} is split into
ASl = {sei1} ∪ ASfi (sei1) \ {sfi1} = {sei1, sfi2}
and ASr = {sfi1} ∪ ASei (sfi1) \ {sei1} = {sfi1}.
If yi23 = 1, then ASl = ASi(sei2) = {sei2}
and ASr = ASi(s

f
i3) = {sfi3} are merged to

ASm = {sei2, sfi3}.
The conditional distributions of yijj′ are as

follows:
p(yijj′ = 0|y−yijj′ , z,w, δ, α, a, b, η0, η1)
∝ η0 + ci0

η0 + η1 + ci0 + ci1
BetaK

(
α +

∑
ASi

tig

)
∏

g∈{ASl,ASr}

(b|a)tig·
(b)nig·

K∏
k=1

S(nigk, tigk, a),

p(yijj′ = 1|y−yijj′ , z,w, t \ T, α, a, b, η0, η1)
∝
∑

T

(
η1 + ci1

η0 + η1 + ci0 + ci1
BetaK

(
α +

∑
ASi

tig

)
(b|a)ti,ASm,·
(b)ni,ASm,·

K∏
k=1

S(ni,ASm,k, ti,ASm,k, a)

)
,

where T is the set of tigk such that for either or
both of ASl and ASr, tigk = 1. ci0 and ci1 are
the total number of yi’s whose values are 0 and
that of yi’s whose values are 1, respectively. Note
that we change yi’s that relate to the selected
action (merging or splitting), in addition to yijj′ to
maintain consistency between y and the aligned
segment sets.

Inference of θ, ν, ϕ:
Although our inference does not directly estimate
θ, ν, and ϕ, these variables can be inferred
from the following posterior expected values via

Algorithm 3 Generative Process for Segments
1: for each document dli (i ∈ {1, ..., D}) do
2: choose πli ∼ Beta(λ0,λ1)
3: for each passage ulih (h ∈ {1, ..., U li}) do
4: choose ρlih ∼ Bernoulli(πli)
5: end for
6: sli = concatenate(u

l
i, ρ

l
i)

7: end for

sampling:

θ̂ik = Ezi,ti|wi,α,β,a,b,y

[
αk +

∑
ASi

tigk∑K
k=1(αk +

∑
ASi

tigk)

]
,

ν̂igk = Ezi,ti|wi,α,β,a,b,y

[
nigk − atigk

b + nig·
+ θik

atig· + b
b + nig·

]
,

ϕ̂lkw = Ez,t|w,α,β,a,b,y

[
βlw + M

l
kw∑

w′∈W l(β
l
w′ + M

l
kw′)

]
.

4 Integration of Topic Segmentation into
BiSTM (BiSTM+TS)

To infer segmentation boundaries simultaneously
with cross-lingual topics, we integrate the unsu-
pervised Bayesian topic segmentation method pro-
posed by Du et al. (2013) into the proposed
BiSTM (BiSTM+TS).

We assume that each segment is a sequence of
topically-related passages. In particular, we con-
sider a sentence as a passage. Our segmenta-
tion model defines a segment in document dli by
a boundary indicator variable ρlih for each pas-
sage ulih (h ∈ {1, ..., U li}); ρlih is 1 if there is a
boundary after passage ulih (otherwise 0). For ex-
ample, ρli = (0, 1, 0, 0, 1) indicates that the doc-
ument dli comprises the two segments {uli1, uli2}
and {uli3, uli4, uli5}.

Algorithm 3 shows the generative process for
segments. The generative process of BiSTM+TS
inserts Algorithm 3 between Steps 7 and 8 of Al-
gorithm 2. Note that two documents (dei , d

f
i ) ∈

di are segmented independently. BiSTM+TS as-
sumes that each document dli has its own topic
shift probability πli. For each document d

l
i, π

l
i

is first drawn from a Beta distribution with the
priors λ0 and λ1 (Step 2). Then, for each pas-
sage ulih (h ∈ {1, ..., U li}), ρlih is drawn from a
Bernoulli distribution with the prior πli (Step 4).
Finally, segments sli are generated by concatenat-
ing passages based on ρli (Step 6).

1270



4.1 Inference for BiSTM+TS

Our inference for BiSTM+TS alternates three dif-
ferent kinds of blocks, sampling of ρ and sam-
plings for BiSTM ((z, δ) and y). The conditional
distribution of ρ comprises the Gibbs probability
for splitting one segment sm into two segments sr
and sl by placing the boundary after ulih (ρ

l
ih = 1)

and that for merging sr and sl to sm by removing
the boundary after ulih (ρ

l
ih = 0).

These probabilities are estimated in the same
manner as the conditional probabilities of yijj′ ,
where y (yijj′ = 0, 1), ASl, ASr, ASm, η0, and
η1 are replaced with ρ (ρlih = 1, 0), sl, sr, sm, λ1,
and λ0, respectively, and the statistics t and n are
summed for every segment rather than for every
aligned segment set (see Equation (6) and (9) in
Du et al. (2013)).

Our inference assumes that sampling ρ does
not depend on aligned segments in the other lan-
guage, i.e., y4. After splitting or merging, we
set the y’s of sm, sl, and sr as follows: if sm is
split into sl and sr, then AS(sl) = AS(sm) and
AS(sr) = AS(sm); if sl and sr are merged to sm,
then AS(sm) = AS(sl) ∪AS(sr).

5 Experiment

We evaluated the proposed models in terms of
perplexity and performance in translation pair
extraction, which is a well-known application
that uses a bilingual topic model. We used a
document-aligned comparable corpus comprising
3,995 document pairs, each of which is a Japanese
Wikipedia article in the Kyoto Wiki Corpus5 and
its corresponding English Wikipedia article6. Note
that the English articles were collected from the
English Wikipedia database dump (2 June 2015)7

based on inter-language links, even though the
original Kyoto Wiki corpus is a parallel corpus,
in which each sentence in the Japanese articles is
manually translated into English. Thus, our ex-
perimental data is not a parallel corpus. We ex-
tracted texts from the collected English articles
using an open-source script8. All Japanese and

4We leave a bilingual extension of the topic segmentation,
i.e., incorporation of y, for future work.

5http://alaginrc.nict.go.jp/
WikiCorpus/index_E.html

6We filtered out the Japanese articles that do not have cor-
responding English articles.

7http://dumps.wikimedia.org/enwiki/
8https://github.com/attardi/

wikiextractor/

English texts were segmented using MeCab9 and
TreeTagger10 (Schmid, 1994), respectively. Then,
function words were removed, and the remaining
words were lemmatized to reduce data sparsity.

For translation extraction experiments, we au-
tomatically created a gold-standard translation set
according to Liu et al. (2013). We first com-
puted p(we|wf ) and p(wf |we) by running IBM
Model 4 on the original Kyoto Wiki corpus,
which is a parallel corpus, using GIZA++ (Och
and Ney, 2003), and then extracted word pairs
(ŵe, ŵf ) that satisfy both of the following con-
ditions: ŵe = argmaxwep(w

e|wf = ŵf ) and
ŵf = argmaxwf p(w

f |we = ŵe). Finally, we
eliminated word pairs that do not appear in the
document pairs in the document-aligned compa-
rable corpus. We used all 7,930 Japanese words
in the resulting gold-standard set as the evaluation
input.

5.1 Competing Methods

We compared the proposed models (BiSTM
and BiSTM+TS) with a standard bilingual topic
model (BiLDA). BiSTM considers each section in
Wikipedia articles as a segment. Note that align-
ments between sections are not given in our exper-
imental data. Thus, y is inferred in both BiSTM
and BiSTM+TS.

As in the proposed models, BiLDA was trained
using Gibbs sampling (Mimno et al., 2009; Ni
et al., 2009; Vulić et al., 2015). In the training
of each model, each variable was first initialized.
Here, zlijm is randomly initialized to an integer be-
tween 1 and K, and each of δlijm, yijj′ , and ρ

l
ih is

randomly initialized to 0 or 1. We then performed
10,000 Gibbs iterations. We used the symmetric
prior αk = 50/K and βlw = 0.01 over θ and
ϕl, respectively, in accordance with Vulić et al.
(2011). The hyperparameters a, b, λ0, and λ1 were
set to 0.2, 10, 0.1, and 0.1, respectively, in accor-
dance with Du et al. (2010; 2013). Both η0 and
η1 were set to 0.2 as a result of preliminary exper-
iments. We used several values of K to measure
the impact of topic size: we used K = 100 and
K = 400 in accordance with Liu et al. (2013)
in addition to the suggested value K = 2, 000 in
Vulić et al. (2011).

In the translation extraction experiments,
9http://taku910.github.io/mecab/

10http://www.cis.uni-muenchen.de/
˜schmid/tools/TreeTagger/

1271



Model K=100 K=400 K=2,000
BiLDA 693.6 530.7 479.9
BiSTM 520.1 429.3 394.6

BiSTM+TS 537.5 445.3 411.8

Table 2: Test Set Perplexity

we used two translation extraction methods,
i.e., Cue (Vulić et al., 2011) and Liu (Liu et
al., 2013). Both methods first infer cross-
lingual topics for words using a bilingual
topic model (BiLDA/BiSTM/BiSTM+TS)
and then extract word pairs (we, wf ) with a
high value of the probability p(we|wf ) de-
fined by the inferred topics. Cue calculates
p(we|wf ) = ∑Kk=1 p(we|k)p(k|wf ), where
p(k|w) ∝ p(w|k)∑K

k=1 p(w|k)
and p(w|k) = ϕkw.

Liu first converts a document-aligned com-
parable corpus into a topic-aligned parallel
corpus according to the topics of words and
computes p(we|wf , k) by running IBM Model
1 on the parallel corpus. Liu then calcu-
lates p(we|wf ) = ∑Kk=1 p(we|wf , k)p(k|wf ).
Hereafter, a bilingual topic model used in an
extraction method is shown in parentheses, e.g.,
Cue(BiLDA) denotes Cue with BiLDA.

5.2 Experimental Results
We evaluated the predictive performance of each
model by computing the test set perplexity based
on 5-fold cross validation. A lower perplexity in-
dicates better generalization performance. Table
2 shows the perplexity of each model. As can
be seen, BiSTM and BiSTM+TS are better than
BiLDA in terms of perplexity.

We measured the performance of translation ex-
traction with top N accuracy (ACCN ), the number
of test words whose top N translation candidates
contain a correct translation over the total num-
ber of test words (7,930). Table 3 summarizes
ACC1 and ACC10 for each model. As can be
seen, Cue/Liu(BiSTM) and Cue/Liu(BiSTM+TS)
significantly outperform Cue/Liu(BiLDA) (p <
0.01 in the sign test). This indicates that BiSTM
and BiSTM+TS improve the performance of trans-
lation extraction for both the Cue and Liu methods
by assigning more suitable topics.

Both experiments prove that capturing segment-
level alignments is effective for modeling bilin-
gual data. In addition, these experiments show that
BiSTM+TS is comparable with BiSTM, indicat-

ACC1
Method K=100 K=400 K=2,000

Cue(BiLDA) 0.024 0.056 0.101
Cue(BiSTM) 0.055 0.112 0.184

Cue(BiSTM+TS) 0.052 0.107 0.176
Liu(BiLDA) 0.206 0.345 0.426
Liu(BiSTM) 0.287 0.414 0.479

Liu(BiSTM+TS) 0.283 0.406 0.467
ACC10

Method K=100 K=400 K=2,000
Cue(BiLDA) 0.093 0.170 0.281
Cue(BiSTM) 0.218 0.286 0.410

Cue(BiSTM+TS) 0.196 0.274 0.398
Liu(BiLDA) 0.463 0.550 0.603
Liu(BiSTM) 0.531 0.625 0.671

Liu(BiSTM+TS) 0.536 0.612 0.667

Table 3: Performance of Translation Extraction

Reference y = 1 Reference y = 0

Inference y = 1 195 174
Inference y = 0 43 1132

Table 4: Distribution of Segment-level Align-
ments

ing that the proposed model could yield a signifi-
cant benefit even if the boundaries of segments are
unknown.

Tables 2 and 3 show that a larger topic size
yields better performance for each model. Fur-
thermore, Liu outperforms Cue regardless of the
choice of bilingual topic models, which is con-
sistent with previously reported results (Liu et al.,
2013). The results of our experiments demonstrate
that the proposed models have the same tendencies
as BiLDA.

6 Discussion

6.1 Inferred Segment-level Alignments
We created a reference set to evaluate segment-
level alignments y inferred by BiSTM (K=2,000).
We randomly selected 100 document pairs from
the comparable corpus and then manually iden-
tified cross-lingual alignments between sections.
Table 4 shows the distribution of inferred y values
and that of y values in the reference set. As can be
seen, the accuracy of y is 0.859 (1,327/1,544).

The majority of false negatives (121/174) are
sections that are not parallel but correspond par-
tially. An example is the alignment between the

1272



Model Japanese article English article
BiSTM 4.8 2.9

BiSTM+TS 10.6 4.1

Table 5: Average Number of Segments

Japanese section “history” and the English sec-
tion “Bujutsu (old type of Budo)” in the “Budo (a
Japanese martial art)” article pair, where a part of
the English section “Bujutsu” is described in the
Japanese section “history.” Such errors might not
necessarily have a negative effect, because partial
alignments can be useful.

6.2 Inferred Segmentation Boundaries

This section compares segment boundaries in-
ferred by BiSTM+TS (K=2,000) with section
boundaries in the original articles, which have
been referred to by BiSTM. The recall of
BiSTM+TS for the original section boundaries
is 0.727. This indicates that the unsupervised
segmentation in BiSTM+TS finds drastic topical
changes, i.e., section boundaries, with high recall.

Table 5 shows the average number of seg-
ments per article for each model. As can be
seen, BiSTM+TS divides an article into segments
smaller than the original sections. This seems to
be reasonable, because some original sections in-
clude multiple topics. However, Tables 2 and 3
show that inferred boundaries do not work better
than section boundaries. One reason for that is
that some errors are caused by a sparseness prob-
lem, when BiSTM+TS separates an article into ex-
tremely fine-grained segments. In addition, Table
5 reveals that BiSTM+TS increases the gap be-
tween languages. Thus, segmentation with a com-
parable granularity between languages might be
favorable for the proposed models.

6.3 Effectiveness for an English–French
Wikipedia Corpus

We evaluated BiLDA, BiSTM, and BiSTM+TS in
terms of perplexity and performance in translation
extraction on an English–French Wikipedia corpus
to verify the effectiveness of the proposed models
for language pairs other than English–Japanese.
The settings, e.g., parameters, for each model are
the same as in Section 5. Note that we report only
the performances of each model with K = 2, 000,
because all models achieved the best performances
when K = 2, 000.

Model Test Set Perplexity
BiLDA 439.1
BiSTM 379.4

BiSTM+TS 396.6
Model ACC1 ACC10

Cue(BiLDA) 0.219 0.556
Cue(BiSTM) 0.275 0.580

Cue(BiSTM+TS) 0.257 0.582
Liu(BiLDA) 0.715 0.838
Liu(BiSTM) 0.742 0.859

Liu(BiSTM+TS) 0.732 0.852

Table 6: Performance on an English–French
Wikipedia Corpus (K = 2, 000)

We collected French articles that correspond to
the English articles used in the experiments in
Section 5, from the French Wikipedia database
dump (2 June 2015) based on inter-language links.
As a result, our English–French corpus comprises
3,159 document pairs. The French articles were
preprocessed in the same manner as the English ar-
ticles: text extraction using the open-source script,
segmentation using TreeTagger, removal of func-
tion words, and lemmatization.

We created a gold-standard translation set for
translation extraction experiments using Google
Translate service11 in a manner similar to that in
Gouws et al. (2015) and Coulmance et al. (2015),
translating the French words in our corpus us-
ing Google Translate, and then eliminating word
pairs that do not appear in the document pairs in
our corpus. We used the top 1,000 most frequent
French words in the resulting gold-standard set as
the evaluation input.

Table 6 summarizes ACC1 , ACC10 , and per-
plexity. It shows that the proposed models are ef-
fective also for the English–French Wikipedia cor-
pus. BiSTM and BiSTM+TS outperform BiLDA
in terms of perplexity and performance of transla-
tion extraction, and BiSTM+TS works well even
if the boundaries of segments are unknown.

7 Related Work

Multilingual topic models other than BiLDA (Sec-
tion 2) have been proposed for document-aligned
comparable corpora. Fukumasu et al. (2012) ap-
plied SwitchLDA (Newman et al., 2006) and Cor-
respondence LDA (Blei and Jordan, 2003), which

11http://translate.google.com/

1273



were originally intended to work with multimodal
data, such as annotated image data, to modeling
multilingual text data. They also proposed a sym-
metric version of Correspondence LDA. Platt et
al. (2010) projected monolingual models based
on PLSA or Principal Component Analysis into a
shared multilingual space with the constraint that
document pairs must map to similar locations. Hu
et al. (2014) proposed a multilingual tree-based
topic model that uses a hierarchical bilingual dic-
tionary in addition to document alignments. Note
that these models do not consider segment-level
alignments.

There are several multilingual topic models tai-
lored for data other than a document-aligned com-
parable corpus, including bilingual topic mod-
els for word alignment and machine translation
on parallel sentence pairs (Zhao and Xing, 2006;
Zhao and Xing, 2008). Some models have
mined multilingual topics from unaligned text
data by bridging the gap between different lan-
guages using a bilingual dictionary (Jagarlamudi
and Daumé III, 2010; Zhang et al., 2010; Negi,
2011). Boyd-Graber and Blei (2009) used parallel
sentences in combination with a bilingual dictio-
nary. However, these models have the drawback
that they require a parallel corpus or a bilingual
dictionary in advance, which cannot be obtained
for some language pairs or domains.

In a monolingual setting, some topic models
that consider segment-level topics have been pro-
posed. Du et al. (2010) considered a document as
a set of segments and generated each per-segment
topic distribution from the topic distribution of the
related document through a Pitman–Yor process.
Others have considered a document as a sequence
of segments. Cheng et al. (2009) reflected the un-
derlying sequences of segments’ topics by posit-
ing a permutation distribution over a document.
Wang et al. (2011) modeled topical sequences in
documents with a latent first-order Markov chain,
and Du et al. (2012) generated each per-segment
topic distribution from the topic distribution of its
document and that of its previous segment. Note
that none of these models have been extended to a
multilingual setting.

8 Conclusions

In this paper, we proposed BiSTM, which models
a document hierarchically and deals with segment-
level alignments. BiSTM assigns the same topic

distribution to both aligned documents and aligned
segments. We also presented an extended model,
BiSTM+TS, that infers segmentation boundaries
in addition to latent topics by incorporating unsu-
pervised topic segmentation (Du et al., 2013). Our
experimental results show that capturing segment-
level alignments improves perplexity and transla-
tion extraction performance, and that BiSTM+TS
yields a significant benefit even if the boundaries
of segments are not given.

This paper presented an extension to BiLDA,
but hierarchical structures can also be incorporated
into other bilingual topic models (Section 7). As
future work, we would like to verify the effec-
tiveness of the proposed models for other datasets
or other cross-lingual tasks, such as cross-lingual
document classification (Ni et al., 2009; Platt et
al., 2010; Ni et al., 2011; Smet et al., 2011) and
cross-lingual information retrieval (Vulić et al.,
2013).

Acknowledgments

We thank Atsushi Fujita for valuable comments on
earlier versions of this manuscript.

References
David M. Blei and Michael I. Jordan. 2003. Model-

ing Annotated Data. In Proceedings of the 26th An-
nual International ACM SIGIR Conference on Re-
search and Development in Informaion Retrieval,
pages 127–134.

David M. Blei, Andrew Y. Ng, and Michael I. Jordan.
2003. Latent Dirichlet Allocation. Journal of Ma-
chine Learning Research, 3:993–1022.

Jordan Boyd-Graber and David M. Blei. 2009. Mul-
tilingual Topic Models for Unaligned Text. In Pro-
ceedings of the Twenty-Fifth Conference on Uncer-
tainty in Artificial Intelligence, pages 75–82.

Wray Buntine and Marcus Hutter. 2012. A Bayesian
View of the Poisson-Dirichlet Process. http://
arxiv.org/pdf/1007.0296.pdf.

Harr Chen, S.R.K. Branavan, Regina Barzilay, and
David R. Karger. 2009. Global Models of Docu-
ment Structure using Latent Permutations. In Pro-
ceedings of Human Language Technologies: The
2009 Annual Conference of the North American
Chapter of the Association for Computational Lin-
guistics, pages 371–379.

Changyou Chen, Lan Du, and Wray Buntine. 2011.
Sampling Table Configurations for the Hierarchi-
cal Poisson-Dirichlet Process. In Proceedings of
the European Conference on Machine Learning and

1274



Principles and Practice of Knowledge Discovery in
Databases 2011, pages 296–311.

Jocelyn Coulmance, Jean-Marc Marty, Guillaume
Wenzek, and Amine Benhalloum. 2015. Trans-
gram, Fast Cross-lingual Word-embeddings. In Pro-
ceedings of the 2015 Conference on Empirical Meth-
ods in Natural Language Processing, pages 1109–
1113.

Lan Du, Wray Buntine, and Huidong Jin. 2010.
A Segmented Topic Model Based on the Two-
parameter Poisson-Dirichlet Process. Machine
Learning, 81(1):5–19.

Lan Du, Wray Buntine, and Huidong Jin. 2012.
Modelling Sequential Text with an Adaptive Topic
Model. In Proceedings of the 2012 Joint Confer-
ence on Empirical Methods in Natural Language
Processing and Computational Natural Language
Learning, pages 535–545.

Lan Du, Wray Buntine, and Mark Johnson. 2013.
Topic Segmentation with a Structured Topic Model.
In Proceedings of the 2013 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics: Human Language Technologies,
pages 190–200.

Kosuke Fukumasu, Koji Eguchi, and Eric P. Xing.
2012. Symmetric Correspondence Topic Models for
Multilingual Text Analysis. In Advances in Neu-
ral Information Processing Systems 25, pages 1286–
1294.

Stephan Gouws, Yoshua Bengio, and Greg Corrado.
2015. BilBOWA: Fast Bilingual Distributed Rep-
resentations without Word Alignments. In Proceed-
ings of the 32nd International Conference on Ma-
chine Learning, pages 748–756.

Thomas Hofmann. 1999. Probabilistic Latent Seman-
tic Indexing. In Proceedings of the 22nd Annual
International ACM SIGIR Conference on Research
and Development in Information Retrieval, pages
50–57.

Leetsch C. Hsu and Peter Jau-Shyong Shiue. 1998. A
Unified Approach to Generalized Stirling Numbers.
Advances in Applied Mathematics, 20(3):366–384.

Yuening Hu, Ke Zhai, Vladimir Eidelman, and Jordan
Boyd-Graber. 2014. Polylingual Tree-Based Topic
Models for Translation Domain Adaptation. In Pro-
ceedings of the 52nd Annual Meeting of the Asso-
ciation for Computational Linguistics, pages 1166–
1176.

Jagadeesh Jagarlamudi and Hal Daumé III. 2010. Ex-
tracting Multilingual Topics from Unaligned Com-
parable Corpora. In Proceedings of the 32nd Eu-
ropean Conference on Advances in Information Re-
trieval, pages 444–456.

Xiaodong Liu, Kevin Duh, and Yuji Matsumoto. 2013.
Topic Models + Word Alignment = A Flexible
Framework for Extracting Bilingual Dictionary from
Comparable Corpus. In Proceedings of the Seven-
teenth Conference on Computational Natural Lan-
guage Learning, pages 212–221.

David Mimno, Hanna M. Wallach, Jason Naradowsky,
David A. Smith, and Andrew McCallum. 2009.
Polylingual Topic Models. In Proceedings of the
2009 Conference on Empirical Methods in Natural
Language Processing, pages 880–889.

Sumit Negi. 2011. Mining Bilingual Topic Hierarchies
from Unaligned Text. In Proceedings of 5th Interna-
tional Joint Conference on Natural Language Pro-
cessing, pages 992–1000.

David Newman, Chaitanya Chemudugunta, Padhraic
Smyth, and Mark Steyvers. 2006. Statistical Entity-
topic Models. In Proceedings of the 12th ACM
SIGKDD International Conference on Knowledge
Discovery and Data Mining, pages 680–686.

Xiaochuan Ni, Jian-Tao Sun, Jian Hu, and Zheng Chen.
2009. Mining Multilingual Topics from Wikipedia.
In Proceedings of the 18th International World Wide
Web Conference, pages 1155–1156.

Xiaochuan Ni, Jian-Tao Sun, Jian Hu, and Zheng Chen.
2011. Cross Lingual Text Classification by Mining
Multilingual Topics from Wikipedia. In Proceed-
ings of the Fourth ACM International Conference on
Web Search and Data Mining, pages 375–384.

Franz Josef Och and Hermann Ney. 2003. A System-
atic Comparison of Various Statistical Alignment
Models. Computational Linguistics, 29:19–51.

Jim Pitman and Marc Yor. 1997. The Two-Parameter
Poisson-Dirichlet Distribution Derived from a Sta-
ble Subordinator. The Annals of Probability,
25(2):855–900.

John Platt, Kristina Toutanova, and Wen tau Yih. 2010.
Translingual Document Representations from Dis-
criminative Projections. In Proceedings of the 2010
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 251–261.

Helmut Schmid. 1994. Probabilistic Part-of-Speech
Tagging Using Decision Trees. In Proceedings of
the International Conference on New Methods in
Language Processing, pages 44–49.

Wim De Smet, Jie Tang, and Marie-Francine Moens.
2011. Knowledge Transfer Across Multilingual
Corpora via Latent Topics. In Proceedings of
the 15th Pacific-Asia Conference on Advances in
Knowledge Discovery and Data Mining, pages 549–
560.

Ivan Vulić, Wim De Smet, and Marie-Francine Moens.
2011. Identifying Word Translations from Compa-
rable Corpora Using Latent Topic Models. In Pro-
ceedings of the 49th Annual Meeting of the Associ-

1275



ation for Computational Linguistics: Human Lan-
guage Technologies, pages 479–484.

Ivan Vulić, Wim De Smet, and Marie-Francine Moens.
2013. Cross-Language Information Retrieval Mod-
els Based on Latent Topic Models Trained with
Document-Aligned Comparable Corpora. Informa-
tion Retrieval, 16(3):331–368.

Ivan Vulić, Wim De Smet, Jie Tang, and Marie-
Francine Moens. 2015. Probabilistic Topic Mod-
eling in Multilingual Settings: An Overview of Its
Methodology and Applications. Information Pro-
cessing & Management, 51(1):111–147.

Hongning Wang, Duo Zhang, and ChengXiang Zhai.
2011. Structural Topic Model for Latent Topical
Structure Analysis. In Proceedings of the 49th An-
nual Meeting of the Association for Computational
Linguistics: Human Language Technologies, pages
1526–1535.

Duo Zhang, Qiaozhu Mei, and ChengXiang Zhai.
2010. Cross-Lingual Latent Topic Extraction. In
Proceedings of the 48th Annual Meeting of the Asso-
ciation for Computational Linguistics, pages 1128–
1137.

Bing Zhao and Eric P. Xing. 2006. BiTAM: Bilingual
Topic AdMixture Models for Word Alignment. In
Proceedings of the 21st International Conference on
Computational Linguistics and 44th Annual Meet-
ing of the Association for Computational Linguis-
tics, pages 969–976.

Bing Zhao and Eric P. Xing. 2008. HM-BiTAM:
Bilingual Topic Exploration, Word Alignment, and
Translation. In Advances in Neural Information
Processing Systems 20, pages 1689–1696.

1276


