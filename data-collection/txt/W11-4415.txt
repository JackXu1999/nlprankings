



















































Measuring the Confusability of Pronunciations in Speech Recognition


Proceedings of the 9th International Workshop on Finite State Methods and Natural Language Processing, pages 107–115,
Blois (France), July 12-15, 2011. c©2011 Association for Computational Linguistics

Measuring the confusability of pronunciations in speech recognition

Panagiota Karanasou
LIMSI/CNRS

Université Paris-Sud
pkaran@limsi.fr

François Yvon
LIMSI/CNRS

Université Paris-Sud
yvon@limsi.fr

Lori Lamel
LIMSI/CNRS

lamel@limsi.fr

Abstract

In this work, we define a measure aimed at
assessing how well a pronunciation model
will function when used as a component of
a speech recognition system. This measure,
pronunciation entropy, fuses information from
both the pronunciation model and the lan-
guage model. We show how to compute this
score by effectively composing the output of a
phoneme recognizer with a pronunciation dic-
tionary and a language model, and investigate
its role as predictor of pronunciation model
performance. We present results of this mea-
sure for different dictionaries with and without
pronunciation variants and counts.

1 Introduction

As explained in (Strik & Cucchiarini, 1999), pro-
nunciation variations can be incorporated at differ-
ent levels in ASR systems: the lexicon, the acous-
tic model, the language model. At the acoustic
level, context dependent phone modeling captures
the phone variations within particular contexts. At
the lexicon level, a lexicon with alternative pronun-
ciations is used. At the language model (LM) level,
the inter-word pronunciation variations are handled
with grammar network, statistical LMs or multi-
word models.

The growing interest in automatic transcription of
Conversational Speech (CTS) increases the need for
modeling pronunciation variation. Indeed, there is
a large number of possible pronunciation variants
occurring in spontaneous speech; these variants of-
ten extend beyond single speech sounds (modeled

by the acoustic model) and reach up to whole words
or word tuples. Not even context-dependent acoustic
models for sub-word units (like phonemes) are able
to cover pronunciation variants of this kind (Kipp et
al, 1997). Thus, pronunciation variation is usually
modeled by enumerating appropriate pronunciations
for each word in the vocabulary using a pronuncia-
tion lexicon.

However, when adding alternative pronunciations
to a lexicon, there is always the potential of introduc-
ing a detrimental amount of confusability. The ho-
mophone (words that sound the same but are written
differently) rate increases, which means that these
additional variants may not be helpful to the recog-
nition performance (Tsai et al, 2001). A typical ex-
ample in English is the word you: the received pro-
nunciation is /yu/ and is chosen when one single
variant is used; modeling some variation requires
to consider the pronunciations /yu/ and /yc/, which
both occur in our multiple pronunciation dictionary.
The latter pronunciation (/yc/), in the phrase you are
is easily confused with /ycr/, the pronunciation of
your. Such confusions, in particular when they in-
volve frequent words, can cause a degradation of the
ASR system as more alternatives are added.

A lot of work has been carried out on the genera-
tion of pronunciation and pronunciation variants in-
dependently of the speech (g2p conversion, p2p con-
version) or in a task specific framework using sur-
face pronunciations generated from a phoneme rec-
ognizer or including acoustic and language model
information. However, most works lack a sense
of how added alternative pronunciations will affect
the overall decoding process. For example, some

107



of the confusability introduced by the pronunciation
model is compensated by the LM. Thus, a method
for quantifying the confusion inherent in a combined
acoustic-lexical system is needed. A confusabil-
ity measure traditionally used to measure the uncer-
tainty residual to a system is entropy. Specifically in
an ASR system, lexical entropy measures the con-
fusability introduced by an LM. In some previous
works, lexical entropy not only takes the LM scores
into account, but also integrate the scores of the
acoustic and pronunciation models (Printz & Olsen,
2000). In (Wolff et al, 2002), the authors consider
as a measure of the pronunciation confusability the
entropy of the variant distribution, but they do not
take into account the language model. Our aim is to
integrate pronunciation model and language model
information into a single framework for describing
the confusability. Especially incorporating language
model information would provide a more accurate
reflection of the decoding process, and hence a more
accurate picture of the possible lexical/acoustic con-
fusions (Fosler-Lussier et al, 2002). The idea is
then to introduce a measure inspired by the pro-
posed formulation in (Printz & Olsen, 2000) but in a
somewhat reverse fashion. Instead of measuring the
“true” disambiguation capacity of the LM by taking
acoustic similarities into account, we aim at measur-
ing the actual confusability introduced in the system
by the pronunciation model, taking also into account
the LM. We call this measure pronunciation entropy.

To compute this measure, we will decompose the
decoding process in two separate parts: the acous-
tic decoding on the one hand, the linguistic de-
coding on the other hand. Given an input signal,
a phoneme recognizer is first used to obtain a se-
quence of phonemes; the rest of the decoding pro-
cess is realized using a set of Finite State Machines
(FSMs) modeling the various linguistic resources in-
volved in the process. Doing so allows us to measure
the confusability incurred by the acoustic decoder
for fixed linguistic models; or, conversely, to assess
the impact of adding more pronunciations, for fixed
acoustic and language models. This latter scenario
is especially appealing, as these measurements do
not require to redecode the speech signal: it thus be-
come possible to try to iteratively optimize the pro-
nunciation lexicon at a moderate computational cost.
Experiments are carried out to measure the confus-

ability introduced by single and multiple pronuncia-
tion dictionaries in an ASR system, using the newly
introduced pronunciation entropy.

The remainder of the paper is organized as fol-
lows. Section 2 describes the necessary Finite State
Tranducers (FSTs) background. Section 3 presents
the FST decoding and details the new confusability
measure. Sections 4 and 5 present the recognition
experiments and the pronunciation entropy results.
The paper concludes with a discussion of the results
and of some future work in Section 6.

2 Background

2.1 Generalities

In the last decade, FSTs have been shown to be
useful for a number of applications in speech and
language processing (Mohri et al, 1997). FST op-
erations such as composition, determinization, and
minimization make manipulating FSTs both effec-
tive and efficient.

Weighted transducers (resp. automata) are finite-
state transducers (resp. automata) in which each
transition carries some weight in addition to the in-
put and output (resp. input) labels. The interpreta-
tion of the weights depends on the algebraic struc-
ture of the semiring in which they are defined.

A semiring is a system (K,⊕,⊗, 0̄, 1̄) containing
the weights K and the operators ⊕ and ⊗, such that:
(K,⊕, 0̄) is a commutative monoid with 0̄ as the
identity element for ⊕; (K,⊗, 1̄) is a monoid with
1̄ as the identity element for ⊗; ⊗ distributes over
⊕: for all a, b, c in K: (a⊕b)⊗c = (a⊗c)⊕(b⊗c)
and c⊗ (a⊕ b) = (c⊗ a)⊕ (c⊗ b), and 0̄ is an an-
nihilator for ⊗ : ∀a ∈ K, a⊗ 0̄ = 0̄⊗ a = 0̄. When
manipulating weighted tranducers, the ⊗ and ⊕ op-
erators are used to combine weights in a serial and
parallel fashion, respectively. A semiring is idempo-
tent if for all a ∈ K, a ⊕ a = a. It is commutative
when ⊗ is commutative.

The real semiring (R,+,×, 0, 1) is used when
the weights represent probabilities. The semirings
used in this work are the log semiring, the entropy
semiring, as well as a new, defined for computa-
tional reasons, log-entropy semiring. The log semir-
ing is defined as (R ∪ [−∞,∞],− log(exp(−x) +
exp(−y)),+,∞, 0). It is isomorphic to the real
semiring via the negative-log mapping and is used

108



in practice for numerical stability.
A weighted finite-state transducer T over a semir-

ing K is an 8-tuple T = (Σ,∆, Q, I, F,E, λ, ρ)
where: Σ is the finite input alphabet of the trans-
ducer; ∆ is the finite output alphabet; Q is a finite
set of states; I ⊆ Q the set of initial states; F ⊆ Q
the set of final states; E ⊆ Q × (Σ ∪ {�}) × (∆ ∪
{�}) × K ×Q a finite set of transitions; λ : I → K
the initial weight function; and ρ : F → K the final
weight function mapping F to K.

A weighted automaton A = (Σ, Q, I, F,E, λ, ρ)
is defined in a similar way simply by omitting the
output labels. The weighted transducers and au-
tomata considered in this paper are assumed to be
trimmed, i.e. all their states are both accessible and
co-accessible. Omitting the input (resp. output) la-
bels of a weighted transducer T results in a weighted
automaton which is said to be the output (resp. in-
put) projection of T .

Using the notations of (Cortes et al, 2006), if
e = (q, a, b, q′) is a transition in E, p(e) = q
(resp. n(e) = q′) denotes its origin (resp. destina-
tion) state, i(e) = a its input label, o[e] = b its
output label and w(e) = E(e) its weight. These
notations extend to paths: if π is a path in T , p(π)
(resp. n(π)) is its initial (resp. ending) state and i(π)
is the label along the path. We denote by P (q, q′) the
set of paths from q to q′ and by P (q, x, y, q′) the set
of paths for q to q′ with input label x ∈ Σ∗ and out-
put label y ∈ Σ∗. The path from an initial to a final
state is a successful path. The output weight associ-
ated by a weighted transducer T to a pair of strings
(x, y) ∈ Σ∗ × Σ∗ is denoted by T (x, y) and is ob-
tained by ⊗-summing the weights of all successful
paths with input label x and output label y:

T (x, y) =
⊕

π∈P (I,x,y,F )
λ(p[π])⊗ w[π]⊗ ρ(n[π])

(1)
T (x, y) = 0̄ when P (I, x, y, F ) = ∅.
The composition of two weighted tranducers T1

and T2 with matching input and output alphabet Σ,
is a weighted transducer denoted by (T1 ◦ T2) when
the semiring is commutative and when the sum:

(T1 ◦ T2)(x, y) =
∑
z∈Σ∗

T1(x, z)⊗ T2(z, y) (2)

is well-defined and in K for all x, y.

2.2 Entropy Semiring
The entropy H(p) of a probability mass function p
defined over a discrete set X is defined as (Cover &
Thomas, 1991):

H(p) = −
∑
x∈X

p(x)logp(x), (3)

where, by convention, 0log0 = 0. This definition
can be extended to probabilistic automata which
define distributions over sets of strings. We call an
automaton probabilistic if for any state q ∈ Q, the
sum of the weights of all cycles at q is well-defined
and in K and

∑
x∈Σ∗ A(x) = 1. A probabilistic

automaton such that at each state the weights of
the outgoing transitions and the final weight sum to
one, is a stochastic automaton. The entropy of A
can be written as:

H(A) = −
∑
x

A(x)logA(x), (4)

where A(x) is the output weight associated by an
automaton A to an input string x ∈ Σ∗.

The expectation (or entropy) semiring is defined
in (Eisner, 2001) as (K,⊕,⊗, (0, 0), (1, 0)), where
K denotes (R ∪ [−∞,∞]) × (R ∪ [−∞,∞]). For
weight pairs (a1, b1) and (a2, b2) in K, the ⊕ and ⊗
operations are defined as follows:

(a1, b1)⊕ (a2, b2) = (a1 + a2, b1 + b2) (5)
(a1, b1)⊗ (a2, b2) = (a1a2, a1b2 + a2b1) (6)

The entropy of A defined in equation (4) can be
seen as a single-source shortest distance for an au-
tomaton defined over the entropy semiring (Cortes et
al, 2006) with weights (w,−wlogw) where w ∈ R.
If the sum of the weights of all paths from any state
p ∈ Q to any state q ∈ Q is well-defined, the short-
est distance from p to q is:

d[p, q] =
⊕

π∈P (p,q)
w[π]. (7)

Thus, the shortest distance from the initial states
to the final states for the probabilistic automaton A
with weights (w,−wlogw) in K will be:

d[I, F ] = (
∑
x

A(x),−
∑
x

A(x)logA(x)) (8)

= (1, H(A)). (9)

109



3 A new confusability measure

3.1 ASR decoding with FSTs
The recognition process can be modeled with
a sequence of weighted finite-state transducers
(WFSTs) (Pereira & Riley, 1996). An abstract
representation of the Viterbi decoding process of
the present work can be given as:

Ŵ = bestpath(A ◦ P ◦ L ◦G), (10)
where Ŵ is the sequence of words corresponding to
the best recognition hypothesis. A is the phoneme
hypothesis lattice generated by the phoneme recog-
nizer, P is an FST that contains a mapping from
phonemes to the phonemic lexical representation of
each word, L is the pronunciation model FST, con-
taining a mapping from each phonemic lexical rep-
resentation to the corresponding word, G is the lan-
guage model finite state automaton (FSA), which
contains n-gram statistics, and ◦ is the composition
operator. Constraining the model by the pronuncia-
tion and the language models means that only words
that are part of complete paths in the decoding will
be counted as confusions. In this work, the FSTs
and FSAs will be manipulated using the open-source
toolkit OpenFst (Allauzen et al, 2007).

3.2 Decomposing the acoustic and linguistic
modeling

In a first place, a phoneme recognizer generates the
phoneme hypothesis lattice A from the speech sig-
nal. These phonemes are the input in the following
process of consecutive compositions. The phoneme
lattices are generated by the ASR system without
taking into account the pronunciation nor the lan-
guage model during decoding. The aim is to decom-
pose the decoding parts in order to better evaluate
the influence of the pronunciation model in the de-
coding process. The acoustic scores are considered
stable and independent of the linguistic (pronuncia-
tion and language) confusability and thus are omit-
ted. No time information is kept. The pronuncia-
tion model will automatically segment the phoneme
sequences in pronunciations, and consequently in
words.

The FST P representing the set of valid pronun-
ciations in our lexicon (see Section 4) is then con-
structed; it takes as input a sequence of phonemes

0

rho:eps

1

a:eps

3

b:eps

b:ab

phi:eps 2

b:eps

c:bc

phi:eps

a:ba

a:aba

phi:eps

Figure 1: Expansion of the topology of the P FST with
phi matchers that consume the phonemes inserted be-
tween valid pronunciations

and returns the sequence of corresponding phonemic
lexical representation (pronunciation). P is com-
posed with each phoneme lattice. In order to account
for insertions of phonemes between valid pronunci-
ations, the topology of P is slightly expanded. This
expansion simulates a simple error recovery strategy
consisting in deleting superfluous phonemes in a left
to right fashion. Fig. 1 illustrates this expansion
on a simple example, with the use of failure transi-
tions implemented with the so-called phi-matchers
and rho-matchers. Each state in P corresponds to
the prefix of an actual pronunciation: whenever we
reach a state from which no continuation is possi-
ble, a phi-transition allows to reach the state corre-
sponding to a trimmed prefix, from which the first
phoneme has been deleted. This simple error re-
covery strategy is applied recursively. A rho-loop
is finally used in the initial state, which is also the
final state, in case the first or last phonemes of a se-
quence do not permit to complete a known pronun-
ciation. Assume, for instance, that we reach state 2
in P (see Fig. 1), and that the following symbol is
’c’, for which no transition exists. The phi-transition
will then allow to move to state 3, and continue the
prefix ’bc’.

Next, the FST L representing the pronunciation
dictionary with pronunciations as inputs and words
as outputs is constructed. Its weights are the condi-
tional probabilities of a pronunciation given a word.

110



When no pronunciation probabilities are available,
a uniform distribution over the probabilities of pro-
nunciations of each word is applied. This FST is
composed with each phoneme-pronunciation FSTs
A ◦ P resulting from the previous composition.

A final composition is made with the FSA G
that models the backoff language model, with
word probabilities as weights. G is constructed
as described in (Riccardi et al, 1996; Allauzen et
al, 2003). This results in FSTs with phonemes as
input and words as output, which are projected to
the output and determinized. Then, the arc weights
of each FST are normalized per state, i.e. scaled
such that the probability of arcs leading out of a
state (plus the probability of state finality) sums
to 1 for each state. A general weight-pushing
algorithm in the log semiring (Mohri et al, 1997)
is applied for the normalization and the weights in
the new stochastic FSA are converted to the desired
posterior probabilities given the pronunciations.
What is calculated is the conditional probability
p(w | a) of all the word sequences that can be
transcribed as a and, thus, are competitors:

p(w | a) = p(a | w)p(w)∑
w∈W p(a | w)p(w)

. (11)

3.3 Definition of pronunciation entropy

In order to have a measure of the confusability
of the pronunciation lexicon, the entropy of the
posterior probability p(w | a) that combines the
pronunciation model and the language model infor-
mation is computed. As described in Section 2.2,
calculating appropriately the shortest distance
on the entropy semiring can result in the desired
entropy. However, the entropy semiring must
have components in the real semiring in order to
calculate the entropy correctly, but even real num-
bers of double precision are not stable enough for
large lattices. Thus, an expectation semiring with
components on the log semiring is needed. That is
why we define a new semiring, the log-expectation
(or log-entropy) semiring, changing the ⊕ and ⊗
operators as well as the identities of the semiring.
In this new semiring (K,⊕,⊗, (∞, 0), (0, 0)), K
denotes (R ∪ [−∞,∞]) × (R ∪ [−∞,∞]) and the
operations ⊕ and ⊗ on weight pairs (a1, b1) and
(a2, b2) in K, are defined as:

(a1, b1)⊕(a2, b2) =
(−log(exp(−a1) + exp(−a2)), b1 + b2)

(12)

(a1, b1)⊗(a2, b2) =
(a1 + a2, exp(−a1)b2 + exp(−a2)b1)

(13)

When working on the log-entropy semiring, each
negative log arc weight w is replaced by the new
weight (w,w ∗ exp(−w)). Then, the shortest dis-
tance from the initial to the final state is calcu-
lated as explained in Section 2.2. Some experiments
were realised on small exemplar lattices with real
arc weights and the entropy was calculated directly
with the entropy semiring, already defined in Open-
Fst. However, for larger lattices the use of the log
and the log-entropy semirings was required in order
to keep the numerical stability.

4 Phoneme Recognition Configuration

The phoneme recognizer used in these experiments
makes use of continuous density HMMs with Gaus-
sian mixtures for acoustic modeling. The acoustic
models are gender-dependent, speaker-adapted, and
Maximum Likelihood trained on about 500 hours
of audio data. They cover about 30k phone con-
texts with 11600 tied states. Unsupervised acous-
tic model adaptation is performed for each segment
cluster using the CMLLR and MLLR techniques
prior to decoding. It suffices to say that the phone
labels that are produced at that stage are determin-
istically mapped to the corresponding phonemes,
which constitute the actual labels in the phoneme
lattice. The recognition dictionary is a simple lex-
icon made up of the same list of phonemes used
to represent pronunciations in the word lexicon. A
unigram phoneme-based language model was also
constructed to respond to the demands of the sys-
tem for language modeling, but its weight was set to
zero during the decoding phase. A phoneme lattice
is thus generated after a single decoding pass, with
no pronunciation model nor language model infor-
mation included. The lattices are pruned so as to
limit them to a reasonable size. To circumvent the
fact that a lattice does not always finish with an end-
of-phrase symbol, which can be the case because of

111



segmentation based on time, an end-of-phrase sym-
bol is added before the final state of each lattice.

The FST approach described in Section 3 is ap-
plied for word decoding. A 4-gram word LM is
used, trained on a corpus of 1.2 billion words of
texts from various LDC corpora (English Gigaword,
Broadcast News (BN) transcriptions, commercial
transcripts), news articles downloaded from the web,
and assorted audio transcriptions. The recognition
word list contains 78k words, selected by interpola-
tion of unigram LMs trained on different text subsets
as to minimize the out-of-vocabulary (OOV) rate on
set of development texts. The recognition dictionary
used as a baseline is the LIMSI American English
recognition dictionary with 78k word entries with
1.2 pronunciations per word. The pronunciations are
represented using a set of 45 phonemes (Lamel &
Adda, 1996). This dictionary is constructed with ex-
tensive manual supervision to be well-suited to the
needs of an ASR system. Other dictionaries with
and without counts and variants were also tested, as
described in the next section.

A part of the Quaero (www.quaero.org) 2010 de-
velopment data was used in the recognition experi-
ments. This data set covers a range of styles, from
broadcast news (BN) to talk shows. Roughly 50%
of the data can be classed as BN and 50% broad-
cast conversation (BC). These data are considerably
more difficult than pure BN data. The part of the
Quaero data that was used resulted in 285 lattices
generated by the phoneme recognizer. This is a suf-
ficient number of lattices to have statistically sig-
nificant results that can be generalized. The FST-
based decoding is applied to these lattices. Table 1
summarizes some of the characteristics of the lat-
tices and FSTs used in the composition process: the
average number of states and arcs of the lattices A
and of the phonemes-to-pronunciations FST P , of
the pronunciations-to-words FST L and of the 4-
gram word LM FSA G. Their size indicates that we
are working in a real ASR framework with FSMs of
large size. Thus, there is an important computational
gain by the fact that the FST approach permits to
change a part of the decoding without repeating the
whole process.

Table 1: Average number of states and arcs in the lattices
Num. A P L G
States 303 180,833 2 83,367,599
Arcs 353 503,234 171,272 200,322,203

5 Pronunciation Entropy Results

The presented pronunciation entropy is an average
of the entropy calculated on the FSAs of the word
sequences generated after the application of the FST
decoding on the output lattices of the phoneme rec-
ognizer. The pronunciation entropy is calculated for
the baseline dictionary with and without frequency
of occurrence counts, as well as for the “longest”
baseline (keeping only the longest pronunciation per
word in the original recognition dictionary) and the
“most frequent” baseline (keeping only the most fre-
quent pronunciation per word in the original recog-
nition dictionary based on counts collected on the
training data). The higher order language model
(LM) used in the decoding of the word recognition
experiments is a 4-gram.

In Table 2, the pronunciation entropy is presented
when 2-, 3- and 4-gram LMs are used for the FST-
based decoding. As expected, as the order of the
LM diminishes, the entropy increases. The results
when the order of the LM diminishes warrant some
more thoughts. The difference in entropy even be-
tween the use of 4-gram and 2-gram is smaller than
expected. The decoding is actually restricted by
the given lexicon, that does not permit pronuncia-
tions, and thus words, to be correctly recognized if
there is an error in the phoneme sequence. Dele-
tions, insertions and substitutions of phonemes are
ignored, with the exception of insertions between
valid pronunciations. By manual observation of the
best word hypotheses and their comparison with the
corresponding references, it was thus noticed that
not many long sequences of words are correctly rec-
ognized and, consequently, the impact of using a
longer n-gram is relatively limited.

It is worth staying a bit longer in Table 2 to
compare the pronunciation entropy of the baselines
which contain one or more pronunciations per word
(upper part of the table) and the single-pronunciation
baselines (lower part of the table). The entropy is
lower in the single pronunciation baselines, and its
lowest score is observed when the “longest” base-
line is used. The fact that its entropy is lower even

112



Table 2: Pronunciation entropy on baselines
4g LM 3g LM 2g LM

Baseline with uniform probabilities
4.003 4.025 4.025

Baseline with counts
4.065 4.083 4.108

Baseline longest with uniform probs
3.013 3.024 3.022

Baseline mostfreq with uniform probs
3.669 3.689 3.756

compared with the “most frequent” baseline, which
is also a single-pronunciation baseline, may be ex-
plained by the fact that the most frequent pronun-
ciations represent better the spoken terms that can
be often easily confused. Especially in spontaneous
speech, some function words are often pronounced
similar to other function words and may not be eas-
ily distinguished by the LM. This is particularly a
problem for frequent words that are easy to insert,
delete or substitute.

A final observation from Table 2, comparing
“Baseline with uniform probabilities” and “Baseline
with counts”, can be that pronunciations with counts
do not reduce confusability. It is normal not to see
a lot of changes because in any case the majority of
words has only one pronunciation and thus probabil-
ity 1 which do not change when counts are added. In
addition, counts are not available for all words, but
only for those observed in the training data. When
no counts are available, uniform probabilities are ap-
plied. Thus, finally there are no great differences be-
tween the dictionary with counts and the dictionary
without counts. Moreover, it could be that counts
only for a few words create an inconsistency that
explains the light deterioration of the pronunciation
entropy.

The increase in entropy is much greater when
more pronunciations are added in the dictionary as
can be seen in Tables 3 and 4. The n-best pronun-
ciations are added in the “longest” and the “most
frequent” baselines. The M1, M2 and M5 in these
tables correspond to the 1-, 2- and 5-best pronuncia-
tions generated automatically using Moses (Koehn
et al, 2007) as a g2p converter, being trained on
the baseline dictionary (with 1.2 pronunciations per

Table 3: Pronunciation entropy with the 4-gram LM af-
ter adding n-best pronunciations, produced by a Moses-
based g2p converter, to the “longest” baseline
Training condition M1 M2 M5
Multiple pronunciations 4.523 6.578 10.005
Baseline longest 3.013

Table 4: Pronunciation entropy with the 4-gram LM after
adding Moses’ n-best pronunciations to the “most fre-
quent” baseline
Training condition M1 M2 M5
Multiple pronunciations 5.185 6.914 10.077
Baseline most frequent 3.669

word). Moses has been successfully used as a g2p
converter for several languages, and for English it
gives state-of-the-art results (Karanasou & Lamel,
2011). The results in Tables 3 and 4 are calculated
with the 4-gram LM.

These results suggest that there can be a large in-
fluence of the pronunciation dictionary in the con-
fusability of an ASR system, not sufficiently com-
pensated by the language model. However, when
adding as much alternative pronunciations some
non-uniform probabilities should be used to mod-
erate confusability. If not, the uniform probability
contributed to each variant of a word with multiple
pronunciations is lower. Thus, for highly probable
words, since the system will have the tendency to
choose them, the confusability will increase. But
if the pronunciation probabilities are also taken into
account, this confusability can be moderated, be-
cause a pronunciation of a word with lower proba-
bility and lower confusability (higher pronunciation
probability) can be prefered from a pronunciation of
a word with higher probability but lower pronunci-
ation probability. We have started working on this
direction and plan to see if our measure will actually
improve when pronunciation probabilities are added
to the decoding.

More confusability is observed when adding vari-
ants to the “most frequent” than to the “longest”
baseline. This is consistent with the explanation
given above supporting that the most frequent base-
line presented more confusability than the longest
baseline because it is closer to the real spoken terms

113



that are often difficult to distinguish.

6 Conclusion and Discussion

A new measure of the confusability of the pronun-
ciation model during the decoding phase in an ASR
system, that integrates also language model infor-
mation, was presented and results were reported us-
ing baseline dictionaries with one or more pronunci-
ations per word, with and without counts, as well as
on dictionaries extended with variants generated by
a state-of-art data-driven method.

It is not straightforward to find a correlation be-
tween this work and ASR performance. The follow-
up of this work will be to examine this correlation
and propose a combined measure of confusability
and accuracy for the selection of pronunciation vari-
ants and for the training of weights for the existing
ones. What makes this procedure particularly com-
plicated is the fact that confusable words are a non-
negligible phenomenon of natural speech and ignor-
ing them severely reduces the completeness of the
dictionary, meaning that a consistent set of pronun-
ciations is not necessarily connected with a pronun-
ciation network of low perplexity.

A drawback of the work so far is that sequences
obtained from the phoneme recognizer contain many
errors. To avoid the problems caused by the
low performance of the phoneme recognizer, some
phoneme substitutions should be permitted. For this,
a confusion matrix and a consensus representation
could be useful.

Lastly, the pronunciation entropy introduced in
this work is a measure of the confusability at the
sentence level. It would be interesting to try and
measure confusability at a sub-sentence level, such
as at a word level using confusion networks, as the
error rate of an ASR system is also calculated at a
word level. At a more general sub-sentence level,
some word-context could be taken into account to
better model the cross-word confusability, because
when adding many variants to an ASR system what
is more important than the homophone rate in the
dictionary, is to measure this rate in the data. In
fact the homophone rate in the original recognition
dictionary (baseline) is 1.16, while in the baseline
“longest” is 1.10 and in the baseline “most frequent”
is 1.15. When adding up to 5 pronunciation variants

to the baseline “longest” or the baseline “most fre-
quent”, the homophone rate becomes 1.24 in both
cases. All these rates are close to one another, so
it seems that what mostly influences confusability
are some frequent homophone words or word se-
quences.

Acknowledgments

This work is partly realized as part of the Quaero
Programme, funded by OSEO, the French State
agency for innovation and by the ANR EdyLex
project. The authors wish to thank T. Lavergne for
sharing his expertise on the OpenFst library.

References
Allauzen,C., Mohri, M. and Roark, B.. 2003. Gener-

alized algorithms for constructing statistical language
models. In Proceedings of ACL, volume 1, pages 40-
47.

Allauzen, C., Riley, M., Schalkwyk, J., Skut, W. and
Mohri, M.. 2007. OpenFst: a general and efficient
weighted finite-state transducer library. In Proceed-
ings of CIAA, pages 11-23.

Cortes, C., Mohri, M., Rastogi, A. and Riley, M. D..
2006. Efficient Computation of the Relative Entropy
of Probabilistic Automata. In Proceedings of LATIN,
volume 3887, pages 323-336.

Cover, T.M. and Thomas, J. A.. 1991. Elements of Infor-
mation Theory. John Wiley & Sons, inc., New York.

Eisner, J.. 2001. Expectation Semiring: Flexible EM for
Learning Finite-State Transducers. In Proceedings of
FSMNLP.

Fosler-Lussier, E., Amdal, I. and Kuo, H.-K. J. 2002. On
the road to improved lexical confusability metrics. In
Proceedings of PMLA, pages 53-58.

Karanasou, P. and Lamel, L.. 2011. Pronunciation Vari-
ants Generation Using SMT-inspired Approaches. In
Proceedings of ICASSP.

Kipp, A., Weswnick, M.-B. and Schiel, F.. 1997. Pro-
nunciation modeling applied to automatic segmenta-
tion of spontaneous speech. In Proceedings of Eu-
rospeech, pages 1023-1026.

Koehn, P., Hoang H., Birch, A., Callison-Burch, C., Fed-
erico, M., Bertoldi, N., Cowan B., Shen, W., Moran,
C., Zens, R., Dyer, C., Bojar, O., Constantin, A. and
Herbst, E. 2007. Moses:Open source toolkit for sta-
tistical machine translation. In Proceedings of ACL,
pages 177-180.

Lamel, L. and Adda, G.. 1996. On designing pronunci-
ation lexicons for large vocabulary continuous speech
recognition. In Proceedings of ICSLP.

114



Mohri, M., Riley, M. and Sproat, R.. 1997. Finite-state
transducers in language and speech processing. In
Computational Linguistics, volume 23-2, pages 269-
311.

Pereira, F.C.N. and Riley, M.D.. 1996. Speech recog-
nition by composition of weighted finite automata. In
Finite-State Language Processing, pages 431-453.

Printz, H. and Olsen, P.. 2000. Theory and practice of
acoustic confusability. In Proceedings of ISCA ITRW
ASR, pages 77-84.

Riccardi, G., Pieraccini, R. and Bocchieri, E.. 1996.
Stochastic automata for language modeling. In Com-
puter Speech and Language, volume 10, pages 265-
293.

Strik, H. and Cucchiarini, C.. 1999. Modeling pronunci-
ation variation for ASR: A survey of the literature. In
Speech Communication, volume 29, pages 225-246.

Tsai, M., Chou, F. and Lee, L.. 2001. Improved
pronunciation modeling by inverse word frequency
and pronunciation entropy. In Proceedings of ASRU,
pages 53-56.

Wolff, M., Eichner, M. and Hoffmann, R.. 2002. Mea-
suring the quality of pronunciation dictionaries. In
Proceedings of PMLA, pages 117-122.

115


