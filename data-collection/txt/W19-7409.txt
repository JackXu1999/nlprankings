

















































Automatic Text Tagging of Arabic News Articles Using Ensemble Deep
Learning Models

Ashraf Elnagar, Omar Einea and Ridhwan Al-Debsi
Machine Learning and Arabic Language Processing Research Group

Dept. Computer Science
University of Sharjah

Sharjah, UAE
{ashraf,oeinea,raldebsi}@sharjah.ac.ae

Abstract

Automatic document categorization gains
more importance in view of the plethora
of textual documents added constantly on
the web. Text categorization or classifi-
cation is the process of automatically tag-
ging a textual document with most rele-
vant label. Text categorization for Ara-
bic language become more challenging in
the absence of large and free datasets. We
propose new, rich and unbiased dataset for
the single-label (SANAD) text classifica-
tion, which is made freely available to the
research community on Arabic computa-
tional linguistics. In contrast to the major-
ity of the available categorization systems
of Arabic text, we offer several deep learn-
ing classifiers. With deep learning, we
eliminate the heavy pre-processing phase
usually used to on the data. Our experi-
mental results showed solid performance
on SANAD corpus with a minimum ac-
curacy of 93.43%, achieved by CGRU,
and top performance of 95.81%, achieved
by HANGRU. In pursuit of superior per-
formance, we implemented an ensemble
model to combine best deep learning mod-
els together in a majority-voting paradigm.

1 Introduction

As a result of the rise of the Internet and Web
2.0, unimaginable amount of data is constantly
on the rise, which is produced by several sources
including social media users. The presence of
such unstructured data makes a great resource for
data processing and management in order to ex-
tract useful information. One important task is text
classification and clustering, which is a field of re-
search that gained much momentum in the last few

years. The recent advances in machine learning
paved the road for proposing successful text cate-
gorization systems.

The terms text categorization and text classi-
fication are used interchangeably to indicate the
process of predicting predefined categories or do-
mains to a given document. The automated cat-
egorization process may report the most relevant
single category or multiple close ones (Figure 1).
For the huge amount of available documents (or
text) on the internet, manual classification by do-
main experts becomes ineffective and unfeasible.
Therefore, automated classifiers had become not
only an alternative but a necessity utilizing ma-
chine learning algorithms. However, the unstruc-
tured nature of the textual documents necessitates
the need of machine learning algorithms to rep-
resent the data in a compatible format such as
using numeric vectors. Text categorization is a
key prerequisite to several evolving applications
in different areas such as language (and dialects)
identification (Lulu and Elnagar, 2018), sentiment
analysis (Elnagar and Einea, 2016; Elnagar et al.,
2018b,a), genre classification (Onan, 2018), and
spam filtering (Li et al., 2018) to list few.

Text categorization is well studied in several
languages and in particular the English language.
Despite of the importance of Arabic language
being the fourth used language on the Internet
and 6th official language reported by United Na-
tions ((Eldos, 2003)), few research attempts are
reported on the Arabic language text classifica-
tion as detailed in the next section. According to
Wikipedia, as of 2018, there are 25 independent
nations where Arabic is an official language and
the number of Arabic speakers reach 380 million.
With the rise of Arabic data on the internet, the
need for an effective and robust automated classi-
fication system becomes a must. The research at-
tempts at addressing this problem for Arabic text



are limited to using shallow deep learning clas-
sifiers and were conducted on small and mostly
unavailable datasets. As a result, we report the
construction of a dataset for Arabic categorization
tasks collected from news sources. The dataset is
made free to use for the research community. In
addition and unlike previous research works, we
utilize deep learning models for investigating both
single-label Arabic text categorization and provide
comparative results of the different models.

We constructed a new corpus for the Ara-
bic classification tasks, namely, SANAD (Single-
label Arabic News Articles Dataset), (Einea et al.,
2019). This corpus consists of more than one
dataset. It is made available on Mendely1. It is
our objective to make the dataset accessible for the
research community.

Figure 1: Single-label text classifier.

Several reported works proposed robust text
classifiers but mostly designed for English text.
As for Arabic, reported works are conducted on
small datasets. Besides, the reported accuracies of
such solutions have a big room for improvement.
We implement nine robust deep neural network
based classifiers that are tested on large datasets
and yield high accuracy on single-label categoriza-
tion tasks.

The remainder of this paper is organized as fol-
lows. In Section 2, we describe previous research
work on Arabic text categorization. Next, we de-
scribe the datasets in detail in Section 3. In Section
4, we list the deep learning models implemented
for the Arabic categorization task. In Section 5, we
demonstrate the performance and improvement of
our models over existing systems on SANAD as
well as a recently reported benchmark dataset. Fi-
nally, we conclude our research in Section 6.

1http://dx.doi.org/10.17632/57zpx667y9.1

2 Literature Review

Numerous papers addressed the problem of auto-
matic text categorization proposing different tech-
niques and solutions. This is mainly true for the
English language. Comprehensive surveys already
exist and provide a thorough coverage of text cat-
egorization classifiers (Sebastiani, 2002; Aggar-
wal and Zhai, 2012; Korde and Mahender, 2012;
Joachims, 2002). A relatively recent good sur-
vey on Arabic text categorization is available in
(Hmeidi et al., 2014).

As our emphasis, in this work, is Arabic lan-
guage, we pay more attention to research work
on Arabic text categorization. The early work of
Saad, (Saad, 2010), used several shallow learn-
ing supervised classifiers including Decision Tree,
KNN, SVM, and Naı̈ve Bayes. He studied the
impact of pre-processing on text categorization
results. For this purpose, he used the widely
spread, but relatively small, BBC and CNN Ara-
bic news datasets. Similarly, the effect of pre-
processing of Arabic text in order to reduce the
feature spaces are reported in (Duwairi et al.,
2009; Al-Kabi et al., 2011; Yaseen and Hmeidi,
2014) in which the authors investigated the im-
pact of stemming, light stemming, and synonyms-
clustering on the features space reduction and ac-
curacy. For the same purpose, Feature Subset Se-
lection (FSS) metrics, (Mesleh, 2011), were used
with SVM classifier to categorize text. Although
the training time is reduced, accuracy deteriorates
as well.

Furthermore, Maximum Entropy (ME) is used
to classify news articles, (Sawaf et al., 2001). The
work concluded that the Dice measures with N-
gram produce better results than the Manhattan
distance. Combining both ME and pre-processing
is reported in (A, 2007). The author showed that
the use of normalization and stop-words removal
has enhanced F1-measure.

The use of Neural Networks (NN) for Arabic
text categorization was first reported in (Umer and
Khiyal, 2007) using Learning Vector Quantiza-
tion (LVQ) classifier and self-organization Maps
(SOM). Good accuracy results were reported
while using a relatively small dataset. Similarly,
the authors of (Harrag et al., 2011) showed that
NN outperforms SVM after reducing the features
space.

The majority of reported research on Arabic
text classification used classical supervised ma-



chine learning classifiers such as NB (El Kourdi
et al., 2004; Mesleh, 2007; Hadi et al., 2008;
Joachims, 1998; Alsaleem, 2011; Khorsheed
and Al-thubaity, 2013), SVM (Mesleh, 2007;
Joachims, 1998; Alsaleem, 2011; Khorsheed and
Al-thubaity, 2013), Rocchio (Joachims, 1998),
KNN (Mesleh, 2007; Hadi et al., 2008; Joachims,
1998), and decision trees (Joachims, 1998; Khor-
sheed and Al-thubaity, 2013; Harrag et al., 2009).
The results mostly conclude that SVM is reported
as the top classifier for categorizing Arabic texts
followed by NB and decision trees.

Different from the previous research works,
El-Mahdaouy et al (El Mahdaouy et al., 2017)
performed Arabic document classification using
Word and document Embedding rather than rely-
ing on text pre-processing and word counting rep-
resentation. It was shown that document Embed-
ding outperformed text pre-processing techniques
either by learning them using Doc2Vec or aver-
aging word vectors. The results are in line with
the conclusions reported by Baroni et al. (Baroni
et al., 2014) which evaluated the use of word em-
bedding against classical approaches that rely on
pre-processing or word counting on an array of
applications such as concept categorization on the
English language. Besides, it has been shown
that neural network based models are more robust
when it comes for sensitivity to parameters set-
tings.

In our work, we introduce new benchmark
datasets for both single-label and multi-label Ara-
bic text categorization. However, the datasets may
serve the research community on Arabic compu-
tational linguistics working on other supervised
learning tasks. Therefore, the datasets are publicly
available. Moreover, we investigate the use of nine
deep learning models to solve the single-label as
well as the multi-label Arabic text categorization
problem.

3 Dataset

We use three different datasets that we collected
using web scraping (Python Selenium, Requests

Source Categories Train Test
Alarabiya.net 5 22203 4075
Khaleej.ae 7 42000 3500
Akhbarona.com 7 42000 4900

Table 1: Number of articles in SANAD.

and BeautifulSoup or PowerShell), from three
popular news websites (alarabiya.net, alkhaleej.ae
and akhbarona.com). All datasets have the cate-
gories [Finance, Medical, Politics, Sports, Tech,
Culture and Religion] except alarabiya.net; it does
not have the last 2 categories. As these datasets
were collected from news portals, the articles are
expressed in modern standard Arabic, so there are
no dialects involved. Since all datasets are tagged
with single labels, we grouped them in one corpus
called SANAD. We partitioned the datasets into
training and testing sets, Table 1 details the num-
ber of articles and categories in each one of them.

The scraped articles are cleaned by removing
Latin alphabet and punctuation marks. In the se-
quel, we describe each one of the 3 datasets that
make SANAD:

3.0.1 alarabiya.net
All scraped articles were initially grouped into 7
categories. However, 2 of the categories did not
have much data (i.e., ’Culture’ and ’Iran News’)
when compared with the rest of the categories. We
merged ’Iran News’ with the ’Politics’ category
and dropped the ’Culture’ set. The articles col-
lected are until early 2018. Figure 2 shows the
distribution of the five resulting categories of this
dataset.

Figure 2: Distribution of categories in the proposed
single-label datasets.

3.0.2 alkhaleej.ae
We collected around 1.2M ( 4GB) articles since
2008 until 2018. However, the tagging in the news
portal was incomplete and vague. Therefore, we
had to manually tag a reasonable amount of arti-
cles in each one of the aforementioned seven cate-



gories. This is a balanced dataset in which each
category has 6.5k articles. the total size of the
dataset is 45.5k articles. Figure 2 shows the bal-
anced distribution of the 7 categories.

3.0.3 akhbarona.com
We scraped a large number of articles in the 7 cate-
gories. However, the ’Religion’ category had half
as much as other categories did. In order to in-
crease the number, we scraped the remaining half
of this category from a similar newspaper portal,
which is Alanba.com. Figure 2 shows the re-
sulting distribution of the seven categories of this
dataset (Table 1).

4 Deep Leaning Models

• CNN The hierarchy of our CNN model con-
sists of a dropout layer, followed by 3 CNN
layers with kernel size of 5, and 128 filters,
followed by global max-pooling with default
values, and another dropout layer.

• RNN We used both GRU and LSTM mod-
els. The GRU model consists of 2 GRU lay-
ers. While our LSTM model consists of 1
LSTM layer. This selection has been deter-
mined by trying out different methods until
we obtained the best accuracy. Both RNN
layers are an improvement on the basic RNN
layer to involve memory capabilities, where
GRU has a memory, but LSTM was intro-
duced to solve the Vanishing Gradient Prob-
lem (Hochreiter et al., 2001).

• BiRNN Both RNN models mentioned above
were also wrapped around with a Bidirec-
tional wrapper, giving us 2 more models; Bi-
GRU and BiLSTM. Both models are com-
posed of 1 BiRNN layer. The reason for im-
plementing the bidirectional strategy is be-
cause of the nature of text, where each word
is defined by the preceding and the proceed-
ing words. Bidirectional wrappers allow the
layers to go over the data in both directions,
resulting in a vector that is 2 times as big as a
uni-directional layer.

• Attention The attention mechanism was
added only to the RNN models, as it was
noted in (Raffel and Ellis, 2015) that it will
solve the long term memory issues, hence it
was applied to GRU and LSTM only. The at-
tention models simply have an attention layer

after the RNN model producing more mod-
els.

• CNN+RNN For our final 2 models, we used a
combination of CNN and RNN layers to pro-
duce CRNNs (Convolution Recurrent Neu-
ral Networks). The hierarchy of the network
consists of a dropout layer, followed by one
CNN layer, one RNN layer, global max pool-
ing, and another dropout layer.

5 Experimental Results and Discussion

5.1 Setup and Pre-processing

Our objective is to explore the success of using
DNN models to classify Arabic news categories.
We conducted several experiments involving cate-
gorization of Arabic text on different datasets. Our
experiments involve single-label classification on
our own three constructed datasets (Arabiya,
Khaleej, and Akhbarona.

We split all datasets into 80% for training, 10%
for cross-validation, and 10% for testing. We re-
port the accuracy on testing datasets for each of the
nine implemented deep learning models. It should
be noted that embeddings are initialized at random
for the input layer in all experiments. We chose
Tensorflow and Keras frameworks for the imple-
mentation of all DNN models.

Simple text pre-processing is used to clean the
dataset by filtering out non-Arabic content. This
is particularly important when dealing with data
collected from the web. Although Arabic charac-
ter set is somehow unique, it is easy to eliminate
non-Arabic characters. We further eliminate all di-
acritics, elongation (i.e., ”É����J
Ôg. ” is reduced to

”ÉJ
Ôg. ”, punctuation marks, extra spaces, etc. An-
other widely adopted practice is to apply normal-
ization on some Arabic characters. This involves
replacing the letters ” @


” , ”


@” , and ”

�
@” with let-

ter ” @” , letter ” �è” with ” è” , and letter ”ø



” with

”ø” . In contrast with the majority of research
works on Arabic computational linguistics, we ar-
gue that the normalization step is not required; we
believe it can affect the contextual meaning for
some words such as ”P


A
	
¯” and ”PA 	¯” or ” �èQ»” and

” èQ»” . This is clear when producing word embed-
ding models. As a result, we did not normalize the
Arabic text.



5.2 Single-label Text Classification

We implemented 9 DNN models. Namely, 4 RNN
models (GRU, BiGRU, LSTM, BiLSTM), 2 at-
tention models (HANGRU, HANLSTM), and 3
CNN based models (CNN, CGRU, CLSTM). We
trained the 9 models on each of the 3 training
datasets of SANAD. Then, we tested the resulting
trained model on each of the 3 datasets. For ex-
ample, we trained the BIGRU model on Arabiya
training dataset and tested it on Arabiya testing
dataset, Khaleej testing dataset, and Akhbarona
testing dataset. The resulting accuracy scores of
this comprehensive testing is depicted in Figures 4
and 3 for each of the 9 DNN models.

Figure 3: Performance evaluation of the 9 models on
the ’Arabiya’ datasets.

Figure 3 summarizes the accuracy results on
our three constructed datasets. In the first experi-
ment, we trained the nine DNN models on Arabiya
dataset and then tested the models on Arabiya,
Khaleej, and Akhbarona datasets on five cate-
gories. When testing on Arabiya dataset, six mod-
els out of the nine produced close results between
95.63% and 96.05% (CNN), one model (HANL-
STM) reported 95.63, which is around average,
and two models, CLSTM and CGRU, performed
below average with accuracy scores of 94.67%
and 94.87%, respectively. We further tested the
Arabiya-trained model on totally different testing
data from Khaleej and Akhbarona datasets. On
Khaleej testing dataset, the best and worst results
are reported by BiLSTM model with accuracy of
92.40% and GRU model with accuracy of 86.64%.
As for Akhbarona test dataset, the best and worst
results are reported by GRU model with accuracy
of 92.00% and CLSTM model with accuracy of
89.05%. In the second experiment (Figure 4), we
trained the nine DNN models on Khaleej dataset
and then tested the models on Arabiya, Khaleej,
and Akhbarona datasets on seven categories. The

Table 2: Performance of the 9 DNN models on the
datasets (AR-5, KH-7, and AB-7. Best and worst per-
forming DL model is shown in Bold font for each
dataset.

AR-5 KH-7 AB-7
BIGRU 95.78% 95.00% 92.94%
BILSTM 95.75% 93.91% 93.53%
CGRU 94.87% 94.23% 91.18%
CLSTM 94.67% 94.57% 92.55%
CNN 96.05% 95.89% 93.94%
GRU 95.63% 93.86% 93.37%
HANGRU 95.85% 96.94% 94.63%
HANLSTM 95.36% 95.49% 94.08%
LSTM 95.95% 95.23% 93.26%

results on Khaleej test dataset ranged between
93.85% and 96.94%. Whereas the results on the
other two test datasets ranged between 75.04%
and 87.12% for Arabiya and 66.38% and 76.40%
for Akhbarona. In the third experiment (Figure 4),
we trained the nine DNN models on Akhbarona
dataset and tested against the three datasets. The
results ranged between 78.43% and 89.79% for
Arabiya; and 70.14% and 80.46% for Khaleej; and
91.18% and 94.63% for Akhbarona. The results
of these experiments show that Arabiya-training
model is the best one to use for single-label classi-
fication of Arabic news articles.

The performance of the DNN models vary in
the above set of experiments. While some DNN
models produce above average results, few oth-
ers are trailing behind. Figure 5 reflects the
level of performance of each model in the experi-
ments. For example, BiLSTM model yielded ac-
curacy above average in eight of the nine exper-
iments. Similarly, both HANGRU and BiGRU
models were successfully producing solid results
around or above average. However, GRU per-
formed poorly compared to the rest.

Table 2 depicts the results on SANAD datasets.
namely, Arabiya with 5 categories (AR), Khaleej
with 7 categories (KH-7), and Akhbarona with 7
categories (AB-7).

5.3 Ensemble Models

To further enhance the accuracy results of the deep
learning models, we employed the ensemble con-
cept to produce better classifiers. Ensemble mod-
eling is the process of combining more than one
model together while producing a single accuracy



Figure 4: Performance evaluation of the 9 models on the datasets: Khaleej and Akhbarona.

Figure 5: Top and average performing deep learning
models in all experiments.

score. We use the majority voting principle to
compute such score. By combining different mod-
els, we anticipate eliminating drawbacks of some
models such as biases and high variability of data.

We performed a greedy ensemble on all com-
binations of DNN models. We solicited models
that produce higher accuracy than the best sin-
gle model reported above. As expected, a com-
bination of two or more models outperformed the
top individual model’s accuracy. Although the
number of generated ensemble models reached
459 models in some cases, the improvement in
accuracy scores did not exceed 2.1%. This is
was achieved when testing Khaleej models against
Akhbarona dataset. On the other hand, no single
ensemble model beat the top single model of test-
ing Khaleej models on Khaleej dataset. It is worth
noting that the impact is little because the reported
accuracy scores of individual models are already
high. Figure 6 compares top ensemble models

with top individual model for all nine tests.

Figure 6: Top ensemble model vs. top individual
model.

We observed that some of the DNN models had
more contribution in the successful selected en-
semble models than others. The model that had the
most contribution is HANGRU, which appeared in
7 experiments out of the 9 ones; major contribu-
tor to the top ensemble models. CNN appeared
6 times. However, the BiGRU model is the least
contributor (only once). Figure 7 shows the con-
tribution percentages of each model in the top en-
semble models.

6 Conclusion

In this work, we described a new large corpus for
single-label Arabic text categorization tasks as a
contribution to the research community on Arabic
computational linguistics. SANAD is collected



Figure 7: Contributions of DNN models to the top en-
semble models.

from annotated Arabic news articles, and consists
of 3 datasets; 2 (Arabiya and Akhbarona) are im-
balanced while Khaleej dataset is a balanced one.
The total number of Arabic articles amount to
200k, which makes it the largest freely available
benchmark. The articles are classified into a max-
imum of seven categories.

We further implemented a variety of deep learn-
ing Arabic text classifiers and tested them thor-
oughly on SANAD corpus. Our treatment is dif-
ferent from existing Arabic single-label text sys-
tems that adopt standard machine learning classi-
fiers with heavy pre-processing phase to prepare
the data. Besides, we eliminated the heavy pre-
processing requirements. Our experimental results
showed that DNN models performed very well
on SANAD corpus with a minimum accuracy of
93.43%, achieved by CGRU, and top performance
of 95.81%, achieved by HANGRU. Furthermore,
we introduced ensemble modeling to boost the
performance, which resulted in enhancing the re-
sults in 8 experiments out of the 9 ones.

References
El-Halees A. 2007. Arabic text classification using

maximum entropy. The Islamic University Jour-
nal (Series of Natural Studies and Engineering),
15:167–167.

Charu C. Aggarwal and Cheng Xiang Zhai. 2012. Min-
ing Text Data. Springer Publishing Company, Incor-
porated.

Mohammed N. Al-Kabi, Qasem A. Al-Radaideh, and
Khalid W. Akkawi. 2011. Benchmarking and as-
sessing the performance of arabic stemmers. Jour-
nal of Information Science, 37(2):111–119.

Saleh Alsaleem. 2011. Automated arabic text catego-
rization using svm and nb. International Arab Jour-
nal of eTechnology.

Marco Baroni, Georgiana Dinu, and Germán
Kruszewski. 2014. Don’t count, predict! a
systematic comparison of context-counting vs.
context-predicting semantic vectors. In Proceedings
of the 52nd Annual Meeting of the Association
for Computational Linguistics (Volume 1: Long
Papers), pages 238–247. Association for Computa-
tional Linguistics.

Rehab Duwairi, Mohammad Nayef Al-Refai, and
Natheer Khasawneh. 2009. Feature reduction tech-
niques for arabic text categorization. Journal of the
American Society for Information Science and Tech-
nology, 60(11):2347–2352.

Omar Einea, Ashraf Elnagar, and Ridhwan Al Debsi.
2019. Sanad: Single-label arabic news articles
dataset for automatic text categorization. Data in
Brief, page 104076.

Mohamed El Kourdi, Amine Bensaid, and Tajje-eddine
Rachidi. 2004. Automatic arabic document cate-
gorization based on the naÏve bayes algorithm. In
Proceedings of the Workshop on Computational Ap-
proaches to Arabic Script-based Languages, Semitic
’04, pages 51–58, Stroudsburg, PA, USA. Associa-
tion for Computational Linguistics.

Abdelkader El Mahdaouy, Eric Gaussier, and
Saı̈d Ouatik El Alaoui. 2017. Arabic text classifi-
cation based on word and document embeddings.
In Proceedings of the International Conference
on Advanced Intelligent Systems and Informatics
2016, pages 32–41, Cham. Springer International
Publishing.

T.M. Eldos. 2003. Arabic text data mining: a root-
based hierarchical indexing model. International
Journal of Modelling and Simulation, 23(3):158–
166.

A. Elnagar and O. Einea. 2016. Brad 1.0: Book re-
views in arabic dataset. In 2016 IEEE/ACS 13th
International Conference of Computer Systems and
Applications (AICCSA), pages 1–8.

Ashraf Elnagar, Yasmin S. Khalifa, and Anas Einea.
2018a. Hotel Arabic-Reviews Dataset Construction
for Sentiment Analysis Applications, pages 35–52.
Springer International Publishing.

Ashraf Elnagar, Leena Lulu, and Omar Einea. 2018b.
An annotated huge dataset for standard and collo-
quial arabic reviews for subjective sentiment anal-
ysis. Procedia Computer Science, 142:182 – 189.
Arabic Computational Linguistics.

Wael Hadi, S Al Hawari, Fadi Thabtah, and Jafar Abab-
neh. 2008. Naive bayesian and k-nearest neigh-
bour to categorize arabic text data. In The 22nd
annual European Simulation and Modelling Confer-
ence (ESM’2008), pages 196–200.

https://doi.org/10.1177/0165551510392305
https://doi.org/10.1177/0165551510392305
https://doi.org/10.3115/v1/P14-1023
https://doi.org/10.3115/v1/P14-1023
https://doi.org/10.3115/v1/P14-1023
https://doi.org/10.1002/asi.21173
https://doi.org/10.1002/asi.21173
https://doi.org/https://doi.org/10.1016/j.dib.2019.104076
https://doi.org/https://doi.org/10.1016/j.dib.2019.104076
http://dl.acm.org/citation.cfm?id=1621804.1621819
http://dl.acm.org/citation.cfm?id=1621804.1621819
https://doi.org/10.1080/02286203.2003.11442267
https://doi.org/10.1080/02286203.2003.11442267
https://doi.org/10.1109/AICCSA.2016.7945800
https://doi.org/10.1109/AICCSA.2016.7945800
https://doi.org/10.1007/978-3-319-67056-0_3
https://doi.org/10.1007/978-3-319-67056-0_3
https://doi.org/https://doi.org/10.1016/j.procs.2018.10.474
https://doi.org/https://doi.org/10.1016/j.procs.2018.10.474
https://doi.org/https://doi.org/10.1016/j.procs.2018.10.474


F. Harrag, E. El-Qawasmeh, and P. Pichappan. 2009.
Improving arabic text categorization using decision
trees. In 2009 First International Conference on
Networked Digital Technologies, pages 110–115.

Fouzi Harrag, Eyas El-Qawasmah, and Abdul Malik S.
Al-Salman. 2011. Stemming as a feature reduction
technique for arabic text categorization. 2011 10th
International Symposium on Programming and Sys-
tems, pages 128–133.

Ismail Hmeidi, Mahmoud Al-Ayyoub, Nawaf Abdulla,
Abdalrahman Almodawar, Raddad Abooraig, and
Nizar A. Ahmed. 2014. Automatic arabic text cat-
egorization: A comprehensive comparative study.
Journal of Information Science, 41:114–124.

Sepp Hochreiter, Yoshua Bengio, Paolo Frasconi,
Jürgen Schmidhuber, et al. 2001. Gradient flow in
recurrent nets: the difficulty of learning long-term
dependencies.

Thorsten Joachims. 1998. Text categorization with
support vector machines: Learning with many rel-
evant features. In Machine Learning: ECML-98,
pages 137–142, Berlin, Heidelberg. Springer Berlin
Heidelberg.

Thorsten Joachims. 2002. Learning to Classify Text
Using Support Vector Machines: Methods, Theory
and Algorithms. Kluwer Academic Publishers, Nor-
well, MA, USA.

Mohammad Khorsheed and Abdulmohsen Al-thubaity.
2013. Comparative evaluation of text classification
techniques using a large diverse arabic dataset. Lan-
guage Resources and Evaluation, 47:513–538.

SVandana Korde and C. Namrata Mahender. 2012.
Text classification and classifiers: A survey. IJAIA
Journal, 3(2):85–99.

Yuancheng Li, Xiangqian Nie, and Rong Huang. 2018.
Web spam classification method based on deep be-
lief networks. Expert Systems with Applications,
96:261 – 270.

Leena Lulu and Ashraf Elnagar. 2018. Automatic ara-
bic dialect classification using deep learning models.
Procedia Computer Science, 142:262 – 269. Arabic
Computational Linguistics.

Abdelwadood Moh’d Mesleh. 2011. Feature sub-set
selection metrics for arabic text classification. Pat-
tern Recogn. Lett., 32(14):1922–1929.

Abdelwadood Moh’d A Mesleh. 2007. Chi square
feature extraction based svms arabic language
text categorization system. Journal of Com-
puter Science, 3(6):430–435. Exported from
https://app.dimensions.ai on 2019/02/03.

Aytuğ Onan. 2018. An ensemble scheme based on lan-
guage function analysis and feature engineering for
text genre classification. Journal of Information Sci-
ence, 44(1):28–47.

Colin Raffel and Daniel PW Ellis. 2015. Feed-
forward networks with attention can solve some
long-term memory problems. arXiv preprint
arXiv:1512.08756.

Motaz Saad. 2010. The Impact of Text Preprocess-
ing and Term Weighting on Arabic Text Classifica-
tion. Master’s thesis, Computer Engineering Dept.,
Islamic University of Gaza, Palestine.

Hassan Sawaf, Jörg Zaplo, and Hermann Ney. 2001.
Statistical classification methods for arabic news ar-
ticles. In Arabic Natural Language Processing in
ACL2001, pages 1–6.

Fabrizio Sebastiani. 2002. Machine learning in au-
tomated text categorization. ACM Comput. Surv.,
34(1):1–47.

Muhammad Fahad Umer and M. Sikander Hayat
Khiyal. 2007. Classification of textual documents
using learning vector quantization. Information
Technology Journal, 6:154–159.

Qussai Yaseen and Ismail Hmeidi. 2014. Extracting
the roots of arabic words without removing affixes.
Journal of Information Science, 40(3):376–385.

https://doi.org/10.1109/NDT.2009.5272214
https://doi.org/10.1109/NDT.2009.5272214
https://doi.org/10.1177/0165551514558172
https://doi.org/10.1177/0165551514558172
https://doi.org/10.1007/s10579-013-9221-8
https://doi.org/10.1007/s10579-013-9221-8
https://doi.org/10.5121/ijaia.2012.3208
https://doi.org/https://doi.org/10.1016/j.eswa.2017.12.016
https://doi.org/https://doi.org/10.1016/j.eswa.2017.12.016
https://doi.org/https://doi.org/10.1016/j.procs.2018.10.489
https://doi.org/https://doi.org/10.1016/j.procs.2018.10.489
https://doi.org/10.1016/j.patrec.2011.07.010
https://doi.org/10.1016/j.patrec.2011.07.010
https://doi.org/10.3844/jcssp.2007.430.435
https://doi.org/10.3844/jcssp.2007.430.435
https://doi.org/10.3844/jcssp.2007.430.435
https://doi.org/10.1177/0165551516677911
https://doi.org/10.1177/0165551516677911
https://doi.org/10.1177/0165551516677911
https://doi.org/10.1145/505282.505283
https://doi.org/10.1145/505282.505283
https://doi.org/https://10.3923/itj.2007.154.159
https://doi.org/https://10.3923/itj.2007.154.159
https://doi.org/10.1177/0165551514526348
https://doi.org/10.1177/0165551514526348

