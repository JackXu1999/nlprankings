



















































Improved Sentiment Detection via Label Transfer from Monolingual to Synthetic Code-Switched Text


Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 3528–3537
Florence, Italy, July 28 - August 2, 2019. c©2019 Association for Computational Linguistics

3528

Improved Sentiment Detection via Label Transfer
from Monolingual to Synthetic Code-Switched Text

Bidisha Samanta
IIT Kharagpur
bidisha@

iitkgp.ac.in

Niloy Ganguly
IIT Kharagpur
niloy@

cse.iitkgp.ac.in

Soumen Chakrabarti
IIT Bombay
soumen@

cse.iitb.ac.in

Abstract
Multilingual writers and speakers often alter-
nate between two languages in a single dis-
course, a practice called “code-switching”.
Existing sentiment detection methods are usu-
ally trained on sentiment-labeled monolingual
text. Manually labeled code-switched text, es-
pecially involving minority languages, is ex-
tremely rare. Consequently, the best mono-
lingual methods perform relatively poorly
on code-switched text. We present an ef-
fective technique for synthesizing labeled
code-switched text from labeled monolingual
text, which is more readily available. The
idea is to replace carefully selected subtrees
of constituency parses of sentences in the
resource-rich language with suitable token
spans selected from automatic translations to
the resource-poor language. By augment-
ing scarce human-labeled code-switched text
with plentiful synthetic code-switched text, we
achieve significant improvements in sentiment
labeling accuracy (1.5%, 5.11%, 7.20%) for
three different language pairs (English-Hindi,
English-Spanish and English-Bengali). We
also get significant gains for hate speech de-
tection: 4% improvement using only synthetic
text and 6% if augmented with real text.

1 Introduction

Sentiment analysis on social media is critical for
commerce and governance. Multilingual social
media users often use code-switching, particularly
to express emotion (Rudra et al., 2016). However,
a basic requirement to train any sentiment analysis
(SA) system is the availability of large sentiment-
labeled corpora. These are extremely challenging
to obtain (Chittaranjan et al., 2014; Vyas et al.,
2014; Barman et al., 2014), requiring volunteers
fluent in multiple languages.

We present CSGen, a system which provides
supervised SA algorithms with synthesized unlim-
ited sentiment-tagged code-switched text, without

involving human labelers of code-switched text,
or any linguistic theory or grammar for code-
switching. These texts can then train state-of-
the-art SA algorithms which, until now, primarily
worked with monolingual text.

A common scenario in code-switching is that
a resource-rich source language is mixed with a
resource-poor target language. Given a sentiment-
labeled source corpus, we first create a parallel
corpus by translating to the target language, us-
ing a standard translator. Although existing neural
machine translators (NMTs) can translate a com-
plete source sentence to a target sentence with
good quality, it is difficult to translate only des-
ignated source segments in isolation because of
missing context and lack of coherent semantics.

Among our key contributions is a suite of
approaches to automatic segment conversion.
Broadly, given a source segment selected for code-
switching, we propose intuitive ways to select a
corresponding segment from the target sentence,
based on maximum similarity or minimum dis-
similarity with the source segment, so that the
segment blends naturally in the outer source con-
text. Finally, the generated synthetic sentence is
tagged with the same sentiment label as the source
sentence. The source segment to replace is care-
fully chosen based on an observation that, apart
from natural switching points dictated by syn-
tax, there is a propensity to code-switch between
highly opinionated segments.

Extensive experiments show that augmenting
scarce natural labeled code-switched text with
plentiful synthetic text associated with ‘borrowed’
source labels enriches the feature space, enhances
its coverage, and improves sentiment detection ac-
curacy, compared to using only natural text. On
four natural corpora having gold sentiment tags,
we demonstrate that adding synthetic text can
improve accuracy by 5.11% in English-Spanish,



3529

7.20% in English-Bengali and (1.5%, 0.97%) in
English-Hindi (Twitter, Facebook). The synthetic
code-switch text, even when used by itself to train
SA, performs almost as well as natural text in sev-
eral cases. Hate speech is an extreme emotion ex-
pressed often on social media. On an English-
Hindi gold-tagged hate speech benchmark, we
achieve 6% absolute F1 improvement with data
augmentation, partly because synthetic text miti-
gates label imbalance present in scarce real text.

2 Related Work

Recent SA systems are trained on labeled text
(Sharma et al., 2015; Vilares et al., 2015; Joshi
et al., 2016). For European and Indian code-
switched sentiment analysis, several shared tasks
have been initiated (Barman et al., 2014; Rosen-
thal et al., 2017; Patra et al., 2018; Sequiera et al.,
2015; Solorio et al., 2014). Some of these involve
human annotations on code-switched text. Vilares
et al. (2015) have annotated the data set released
for POS tagging by Solorio and Liu (2008). Joshi
et al. (2016) had Hindi-English code-switched
Facebook text manually annotated and developed
a deep model for supervised prediction.

In a different direction, synthetic monolingual
text has been created by Generative Adversar-
ial Networks (GAN) (Kannan and Vinyals, 2017;
Zhang et al., 2016, 2017; Maqsud, 2015), or Vari-
ational Auto Encoders (VAE) (Bowman et al.,
2015). Some of these models can be used to gen-
erate sentiment-tagged synthetic text. However,
most of them are not directly suitable for generat-
ing bilingual code-mixed text, due to the unavail-
ability of sufficient volume of gold-tagged code-
mixed text. Samanta et al. (2019) proposed a
generative method using a handful of gold-tagged
data; but they cannot produce sentence level tags.
Recently, Pratapa et al. (2018) used linguistic
constraints arising from Equivalence Constraint
Theory to design a code-switching grammar that
guides text synthesis. Earlier, Bhat et al. (2016)
presented similar ideas, but without empirical re-
sults. In contrast, CSGen uses a data-driven com-
bination of word alignment weights, similarity of
word embeddings between source and target, and
attention (Bahdanau et al., 2015).

3 Generation of code-switched text

CSGen takes a sentiment-labeled source sentence
s and translates it into a target language sentence t.

Then it generates text with language switches on
particular constituent boundaries. This involves
two sub-steps: select a segment in s (§3.1), and
then select text from t that can replace it (§3.2–
§3.3). This generation process is sketched in Al-
gorithm 1.

3.1 Sentiment-oriented source segment
selection

In this step, our goal is to select a contiguous seg-
ment from the source sentence that could poten-
tially be replaced by some segment in the target
sentence. (Allowing non-contiguous target seg-
ments usually led to unnatural sentences.) Code
switching tends to occur at constituent boundaries
(Sankoff and Poplack, 1981), an observation that
holds even for social media texts (Begum et al.,
2016). Therefore, we apply a constituency parser
to the source sentence. Specifically, we use the
Stanford CoreNLP shift-reduce parser (Zhu et al.,
2013) to generate a parse tree1. Then we select
segments under non-terminals, i.e., subtrees, hav-
ing certain properties, chosen using heuristics in-
formed by patterns observed in real code-switched
text.

NP and VP: We allow as candidates all sub-
trees rooted at NP (noun phrase) and VP (verb
phrase) nonterminals, which may cover multiple
words. Translating single-word spans is more
likely to result in ungrammatical output (Sankoff
and Poplack, 1981).

SBAR: Bilingual writers often use a clause to
provide a sentiment-neutral part and then switch
to another language in another sentence-piece to
express an opinion or vice-versa. An example is
“Ramdhanu ended with tears kintu sesh ta besh
onho rokom etar” (Ramdhanu ended with tears
but the ending was quite different). Here the con-
stituent “but the ending was quite different” comes
under the subtree of SBAR.

Highly opinionated segments: We also include
segments which have a strong opinion polarity,
as detected by a (monolingual) sentiment ana-
lyzer (Gilbert, 2014). E.g., the tweet “asimit
khusi prasangsakako ke beech . . . as India won the
world cup after 28 years” translates to “Unlimited
happiness among fans . . . as India won the world
cup after 28 years”.

1http://stanfordnlp.github.io/CoreNLP/

http://stanfordnlp.github.io/CoreNLP/


3530

Algorithm 1 CSGen overview.
1: Input: Sentiment-labeled source sentences S =
{(sn, yn)}

2: Output: Synthetic code-switched sentences C =
{(c, y)}

3: tn ← Translate(sn) ∀sn ∈ S /* Make parallel
corpus */

4: C ← ∅
5: for each parallel sentence pair s, t do
6: /*Collect word alignment signals*/
7: a← AttentionScore(s, t), g ← GizaScore(s, t)
8: /* Source segment selection */
9: P ← SentimentOrientedSegmentSelection(s)

10: for each segment ps ∈ P to replace do
11: /* Target segment selection */
12: q̂1, q̂2 ← MaxSimTargetSeg(s, ps, t, a, g)
13: q̂3, q̂4←MinDissimTargetSeg(s, ps, t, 1−a, 1−g)
14: /*Code-switched text generation*/
15: Ck ← Project(s, t, ps, q̂k) where k ∈ {1, . . . , 4}
16: C ← C ∪ SelectBest({Ck : k ∈ {1, . . . , 4}})
17: end for
18: end for
19: C ← Threshold(C) /* Retain only best

replacements */

An example sentence, its parse tree, and its can-
didate replacement segments are shown in Fig-
ure 1. In Algorithm 1, ps ∈ P denotes the set
of candidate replacement subtrees, which corre-
spond to segments. For each candidate segment,
we generate a code-switched version of the source
sentence, as described next.

3.2 Target segment selection

Given a source sentence s, corresponding tar-
get t, and one (contiguous) source segment
ps = {wis · · ·wi+xs }, the goal is to identify the
best possible a contiguous target segment qt =
{wjt · · ·w

j+y
t } that could be used to replace ps

to create a realistic code-switched sentence. We
adopt two approaches to achieve this goal: (a) se-
lecting a target segment that has maximum simi-
larity with ps, and (b) selecting a target segment
having minimum dissimilarity with ps, for various
definitions of similarity and dissimilarity. Below,
we describe methods that achieve this goal after
describing several alignment scores which will be
used in these methods. Overall, these lead to target
segments q̂1t , q̂

2
t , . . . shown in Algorithm 1, with t

removed for clarity.

3.2.1 Word alignment signals
Signals based on word alignment methods are part
of the recipe in choosing the best possible qt given
the sentence pair and ps.

GIZA score: The standard machine translation
word alignment tool Giza++ (Och and Ney, 2003)

A  coalition  with  the  Lib  Dems     is  what  the  electorate  want.

DT      NN    IN    DT   NNP   NNPS   VBZ   WP  DT      NN     VBP
.

NP NP NP VP

S

WHNP

SBAR
VP

PP

NP

S

A  coalition  with  the  Lib  Dems     is matadaata chaahata hai.

lib dems ke saath ek gathabandhan matadaata chaahata hai.

EN:

HI:
CS:

Figure 1: A phrase-structure tree for a sample syn-
thesis. Dotted-boxes around constituents indicate that
they are candidates for replacement on the source
side (§3.1). EN: English source sentence, HI: Hindi
target sentence, CS: code-switched sentence. The ital-
icized segment is the target segment to replace the
source segment under the non-terminal SBAR.

uses IBM statistical word alignment models 1–
5 (Fernández, 2008; Schoenemann, 2010; Brown
et al., 1993; Riley and Gildea, 2012). This tool in-
corporates principled probabilistic formulations of
the IBM models and gives a correspondence score
G[wit, w

j
s] between target and source words for a

given sentence pair. This word-pair score is used
as a signal to find the best q̂t.

NMT attention score: Given an attention-
guided trained sequence-to-sequence neural ma-
chine translation (NMT) model (Bahdanau et al.,
2015; Luong et al., 2015) and sentence pair s, t,
we use the attention score matrix A[wit, w

j
s] as an

alignment signal.

Inverse document frequency (rarity): The in-
verse document frequency (IDF) of a word in
a corpus signifies its importance in the sen-
tence (van Rijsbergen, 1979). We define I(w) =
σ(a IDF(w) − b) as a shifted, squashed IDF that
normalizes the raw corpus-level score. Here σ is
the sigmoid function and parameters a and b are
empirically tuned. This IDF-based signal is op-
tionally incorporated while choosing q̂t.

3.2.2 Target segment with maximum
similarity

Given word-pair scores derived from either
Giza++ or NMT attention described in §3.2.1, we
formulate two methods for identifying target seg-
ments. First, we identify the best target segment



3531

given Giza++ scores, G[·, ·], as follows:
q̂1t ← argmax

qt

∏
wt∈qt

∑
ws∈ps

G[wt, ws] (1)

For each word in qt, we compute the total attention
score concentrated in ps and then multiply them as
if they are independent.

Second, we use the attention score learned by
the NMT system of Luong et al. (2017) (a bidirec-
tional LSTM model with attention). Essentially,
given the attention score A[·, ·] between target and
source words, we intend to select the target seg-
ment qt whose maximum attention is concentrated
in the given ps.

Initial exploration of the above method revealed
that the attention of a target word may spread out
over several related but less appropriate source
words, and accrue better overall similarity than a
single more appropriate word. Here IDF can come
to the rescue, the intuition being that wordswit and
wjs with very different IDFs are less likely to align,
because (barring polysemy and synonymy) rare
(common) words in one language tend to translate
to rare (common) words in another. This intuition
is embodied in the improved formulation:
q̂2t ← argmax

qt

∏
wt∈qt

I(wt)
∑

ws∈ps
I(ws)A[wt, ws]

(2)
Informally, if a source segment contains many rare
words, the target segment should also have a sim-
ilar number of rare words from the target domain,
and vice-versa.

3.2.3 Target segment with minimum
dissimilarity

We examine an alternative method for identifying
target segments that leverage the Earth Mover’s
Distance (EMD) (Vaseršteı̆n, 1969). Kusner et al.
(2015) extended EMD to the Word Mover Dis-
tance to measure the dissimilarity between doc-
uments by ‘transporting’ word vectors from one
document to the vectors of the other. In the same
spirit, we define a dissimilarity measure between
ps and candidate target segments using EMD. We
present here EMD as a minimization over frac-
tional transportation matrix F ∈ R|qt|×|ps| as be-
low:

EMD(qt, ps) = min
F

|qt|∑
i=1

|ps|∑
j=1

Fi,jdi,j (3)

where
∑

i Fi,j =
1
|qt| and

∑
j Fi,j =

1
|ps| and di,j

is a distance metric between a target and a source
word pair, given suitable representations. Finally,

we choose the target segment which is least dis-
similar to a given source segment defined by the
EMD. We compute di,j in two ways, described be-
low.

Attention-based distance: Here the distance
between the embeddings is defined as:

dAi,j = 1−A[wit, wjs] (4)

Giza-based distance: Similarly we can com-
pute the distance using Giza score as:

dGi,j = 1−G[wit, wjs] (5)
Given the two types of distances in Eq. (4)–(5) and
the definition of EMD in Eq. (3) we can formulate
two methods for identifying target segments:

q̂kt ← argmin
qt

min
F

|qt|∑
i=1

|ps|∑
j=1

Fi,jd
k
i,j (6)

where k ∈ {3, 4} and d3i,j ≡ dAi,j and d4i,j ≡ dGi,j .
We can also use Euclidean distance as di,j .

However, this method requires multilingual word
embeddings for every word to calculate the dis-
tance. The volume of labeled source text we can
use is usually smaller than the vocabulary size,
making it difficult to learn reliable word embed-
dings. Also, if these corpora contain informal so-
cial media text like the ones described in §4.1, then
publicly available pretrained word embeddings ex-
clude a significant percentage of them.

3.3 Projecting target segments
Given a source sentence s with designated seg-
ment ps to replace, and target sentence t, we have
by now identified four possible target segments
q̂kt where k ∈ {1, . . . , 4} as described in §3.2.2–
§3.2.3. We now project the target segment to the
source sentence, meaning, (a) replace the source
segment with the target segment and (b) translit-
erate the replacement using the Google Translit-
eration API to the source script2. This creates
four possible synthetic code-switched sentences
for each instance of (s, ps, t). Finally, we trans-
fer the labels of the original monolingual corpora
to the generated synthetic text corpora.

3.4 Best candidate via reverse translation
From these four code-switched sentences
c1, . . . , c4, we wish to retain the one that retains
most of the syntactic structures of the source
sentence. Each code-switched sentence ck has an
associated score as defined in §3.2. We use two

2http://www.google.com/transliterate?
langpair=hi|en&text=<text>



3532

empirically tuned thresholds: a lower cut-off for
the similarity score of c1, c2 and an upper cut-off
for the dissimilarity score of c3, c4, to improve
the quality of candidates retained. These scores
are not normalized and cannot be compared
across different methods. Therefore, we perform
a reverse translation of each candidate back to
the source language using the Google translation
API to obtain s̃. We retain the candidate whose
retranslated version s̃ has the highest BLEU score
(Papineni et al., 2002) wrt s. In case of a tie, we
select the candidate with maximum word overlap
with s.

3.5 Thresholding and stratified sampling

In addition to retaining only the best among code-
switched candidates c1,...,4, we discard the win-
ner if its BLEU score is below a tuned threshold.
Further, we sample source sentences such that the
surviving populations of sentiment labels of the
code-switched sentences match the populations in
the low-resource evaluation corpus. Another tuned
system parameter is the amount of synthetic text to
generate to supplement the gold text.

We do not depend on any domain coherence
between the source corpus used to synthesize
text and the gold ‘payload’ corpus — this is the
more realistic situation. Our expectation, there-
fore, is that adding some amount of synthetic text
should improve sentiment prediction, but exces-
sive amounts of off-domain synthetic text may
hurt it. In our experiments we grid search the syn-
thetic:gold ratio between 1/4 and 2 using 3-fold
cross validation.

4 Experiments

We demonstrate the effectiveness of augment-
ing gold code-switched text with synthetic code-
switched text. We also measure the usefulness of
synthetic text without gold text. In this section, we
will first describe the data sets used to generate the
synthetic text and then the resource-poor labeled
code-switched text used for evaluation. Next, we
will present the method used for sentiment detec-
tion, baseline performance, and finally our perfor-
mance, along with a detailed comparative analysis.

4.1 Source corpora for text synthesis

We use publicly available monolingual sentiment-
tagged (positive, negative or neutral) gold corpora
in the source language.

ACL: Dong et al. (2014) released about 6000
manually labeled English tweets.

Election: Wang et al. (2017) published about
5000 human-labeled English tweets.

Mukherjee: This data set contains about 8000
human-labeled English tweets (Mukherjee and
Bhattacharyya, 2012; Mukherjee et al., 2012).

Semeval shared task: This provides about 10000
human-labeled English tweets (Rosenthal et al.,
2017).

Union: This is the union of above mentioned dif-
ferent data sets.

Hatespeech: We collected 15K tagged English
tweets from (Founta et al., 2018) which con-
sists of 4.7K abusive, 1.7K hateful and 4K nor-
mal tweets.

We picked Spanish, which is homologous to
English, and Hindi and Bengali, which are com-
paratively dissimilar to English, for our experi-
ments. We translated these monolingual tweets to
Spanish, Hindi and Bengali using Google Trans-
lation API3 and used as parallel corpus to train
attention-based NMT models and statistical MT
model (GIZA) to learn the word alignment signals
as described in §3.2.1.

4.2 Preliminary qualitative analysis

Analysis of texts synthesized by various mecha-
nisms proposed in §3.2 shows that similarity based
methods contribute 82–85% of the best candidates
and the rest come from dissimilarity based meth-
ods. Similarity-based methods using NMT atten-
tion and Giza perform well because the segments
selected for replacement often constitute nouns
and entity mentions, which have a very strong
alignment in the corresponding target segment.
NMT attention and Giza-EMD perform well when
segments contract or expand in translation.

4.3 Low-resource evaluation corpora

To evaluate the usefulness of the generated syn-
thetic tagged sentences as a training set for senti-
ment analysis, we have used three different code-
switched language pair data sets. Each data set
below was divided into 70% training, 10% valida-
tion and 20% testing folds. The training fold was
(or was not) augmented with synthetic labeled text

3https://translation.googleapis.com

https://translation.googleapis.com


3533

to train sentiment classifiers, which were then ap-
plied on the test fold judge the quality of synthesis.

HI-EN, FB (Hindi-English, Facebook): Joshi
et al. (2016) released around 4000 labeled code-
switched sentences from the Facebook timeline
of Narendra Modi (Indian Prime Minister) and
Salman Khan (Bollywood actor).

HI-EN, TW (Hindi-English, Twitter): This is a
shared task from ICON 2017 (Patra et al., 2018)
with 15575 instances.

ES-EN (English-Spanish): We collected 2883
labeled tweets specified by Vilares et al. (2015).

BN-EN (Bengali-English): This is another
shared task from ICON 2017 (Patra et al., 2018)
with 2499 instances.

HI-EN, Hatespeech: Bohra et al. (2018) pub-
lished 4000 manually-labeled code-switched
Hindi-English tweets: 1500 exhibiting hate
speech and 2500 normal. We also found a sig-
nificant number of abusive tweets marked hate
speech. For uniformity, we merged hate speech
tweets and abusive tweets.

4.4 Sentiment classifier
We adopt the sub-word-LSTM system of Joshi
et al. (2016). We prefer this over feature-based
methods because (a) feature extraction for code-
switched text is very difficult, and varies widely
across language pairs, and (b) the vocabulary is
large and informal, with many tokens outside
standard (full-) word embedding vocabularies and
(c) sub-word-LSTM captures semantic features
via convolution and pooling.

Loss functions: If the sentiment labels
{−1, 0,+1} are regarded as categorical, cross-
entropy loss is standard. However, prediction
errors between the extreme polarities {−1,+1}
need to be penalized more than errors between
{-1,0} or {0,+1}. Hence, we use ordinal cross-
entropy loss (Niu et al., 2016), introducing a
weight factor proportional to the order of intended
penalty multiplied with the cross entropy loss. On
the test fold, we report 0/1 accuracy and per class
micro-averaged F1 score.

Baseline and prior art: Our baseline scenario is
a self-contained train-dev-test split of the gold cor-
pus. The primary prior art is the work of Pratapa
et al. (2018).

Feature space coherence: Our source corpora
are quite unrelated to the gold corpora. Table 1
shows that the average Euclidean distance be-
tween feature space of gold training and testing
texts is much lower than that between gold and
synthetic texts. While this may be inescapable in a
low-resource situation, the gold baseline does not
pay for such decoherence, which can lead to mis-
leading conclusions.

HI EN,TW HI EN,FB ES EN BN EN
ACL 2.21 (2.13) 3.72 (2.24) 2.09 (1.73) 4.11 (2.67)
Election 2.40 (2.12) 6.27 (2.67) 1.58 (1.49) 5.23 (2.43)
Mukherjee 2.47 (2.33) 3.82 (2.26) 1.64 (1.64) 5.18 (2.50)
Semeval 2.23 (2.11) 4.04 (2.26) 1.69 (1.67) 3.63 (2.59)
Union 2.55 (2.15) 3.80(2.65) 1.65 (1.53) 5.48 (2.56)
Gold 2.05 1.87 1.64 1.83

Table 1: Average pairwise Euclidean distance between
training data and test data features. Rows correspond to
standalone (respectively, augmented) text for training.
Gray: reference distance of gold test from gold train.
Red: largest distance observed.

Training regimes: Absence of coherent tagged
gold text may lead to substantial performance loss.
Hence, along with demonstrating the usefulness
of augmenting natural with synthetic text, we also
measure the efficacy of synthetic text on its own.
We train the SA classifier with three labeled cor-
pora: (a) limited gold code-switched text, (b) gold
code-switched text augmented with synthetic text
and (c) only synthetic text. Then we evaluate the
resulting models on labeled gold code-switched
test fold.

4.5 Sentiment detection accuracy

Table 2 shows the benefits of augmenting natural
with synthetic text. Test accuracy increases fur-
ther (shown in brackets) if thresholding and strat-
ified sampling are used. Gains for HI EN,TW,
HI EN,FB, ES EN and BN EN are 1.5% (2.43%),
0.23% (1.43%), 4.76% (6.24%), and 2.8% (4.8%)
respectively. Categorical cross-entropy loss was
used here. Similar improvements in accuracy of
1.45% (1.5%), 0.59% (0.97%), 2.16% (5.11%),
3% (7.20%) are observed after training with or-
dinal loss function. Our conclusion is that care-
ful augmentation with synthetic data can lead to
useful gain in accuracy. Moreover, by selecting
synthetic text which is syntactically more natural,
even larger gains can be achieved. Notably, the
distance between training and test features (Ta-
ble 1) is negatively correlated with accuracy gain
(Pearson correlation coefficient of −0.48).



3534

Train
Test HI EN

(TW)
HI EN
(FB)

ES EN BN EN
HI EN
(TW)

HI EN
(FB)

ES EN BN EN

Categorical cross entropy training loss Ordinal cross entropy training loss
ACL 51.80 (52.59) 62.59 (65.33) 44.80 (48.84) 52.59 (59.81) 52.68 (53.76) 62.72 (64.22) 45.69 (50.31) 50.08 (51.60)
Election 52.84 (54.59) 65.59 (66.80) 43.07 (43.07) 57.99 (59.00) 52.89 (54.64) 64.88 (65.26) 45.21 (45.80) 53.40 (55.60)
Mukherjee 53.76 (54.69) 64.82 (65.85) 47.06 (46.36) 49.79 (57.40) 53.79 (53.53) 59.66 (64.57) 44.85 (43.07) 51.00 (49.40)
Semeval 52.99 (54.19) 65.33 (65.46) 47.36 (44.69) 53.40 (59.99) 52.99 (53.83) 63.26 (64.66) 47.36 (44.64) 53.14 (57.59)
Union 53.28 (54.64) 65.50 (65.99) 44.24 (45.32) 57.23 (59.89) 53.65 (53.69) 64.30 (67.65) 44.04 (46.00) 53.40 (57.40)
MSR 54.50 65.58 48.14 59.79 53.69 62.80 47.50 52.8
Gold 52.26 65.37 42.6 55.19 52.34 64.29 45.20 50.39

Table 2: Accuracy (%) on 20% test data after training with augmented and only gold text. Rows correspond to
sources of augmentation. In most cells we show (A) no thresholding or stratification and (B) with thresholding
and stratification (within brackets). Gray: reference accuracy with only gold training. Blue: A or B or MSR
outperforms gold. Green: B performs best. Row ‘MSR’ uses text synthesized by Pratapa et al. (2018).

Comparison with Pratapa et al. (2018): They
depend on finding correspondences between con-
stituency parses of the source and target sentences.
However, the common case is that a constituency
parser is unavailable or ineffective for the target
language, particularly for informal social media.
They are thus restricted to synthesizing text from
only a subset of monolingual data. Training SA
with natural text augmented with their synthesized
text leads to poorer accuracy, albeit by a small
amount, than using CSGen. The performance is
worse for target languages that are more resource-
poor.

Ordinal vs. categorical loss: Table 2 shows that
ordinal loss helps when the neutral label domi-
nates. However, neither is a clear winner and the
gains are small. Therefore, we use categorical loss
henceforth.

Choice of monolingual corpus: Across all
monolingual corpora, Election performs consis-
tently well. Best test performance on HI EN,TW
was obtained by synthesizing from the Mukherjee
corpus. Text synthesized from Election provides
the best results for HI EN,FB for both setups. The
performance of Union is also good but not the
best. This is because although a larger and diverse
amount of data is available which ensures its qual-
ity, the Euclidean distance between test data and
some individual corpora is still large.

4.6 Sentiment detection F1 score

Beyond 0/1 accuracy, Table 3 shows F1 score
gains. Election yields consistently good results.
We have reported the F1 score gain for different
sentiment classes only for Election in Table 3 for
brevity. Augmenting synthetic data with gold data
yields better F1 score than training only with gold
tagged data. Also, it is interesting to observe that

Categorical Cross
Entropy training

Ordinal Cross
Entropy training

Pos Neu Neg Pos Neu Neg
HI EN,TW

CSGen 0.52 0.62 0.38 0.55 0.63 0.34
Gold 0.48 0.63 0.24 0.50 0.62 0.35

HI EN,FB
CSGen 0.59 0.73 0.56 0.62 0.71 0.55
Gold 0.60 0.74 0.54 0.60 0.71 0.44

ES EN
CSGen 0.38 0.53 0.37 0.48 0.50 0.42
Gold 0.47 0.44 0.41 0.40 0.53 0.43

BN EN
CSGen 0.63 0.49 0.58 0.55 0.47 0.59
Gold 0.55 0.51 0.65 0.37 0.49 0.61

Table 3: F1 score for each class prediction. Blue: CS-
Gen is better than Gold.

there is a sharp drop of F1 score for HI EN,FB
and BN EN data sets for Gold data while training
with ordinal cross entropy function across all the
sentiment labels. As described in §4.5, this is due
to non-discriminative features. However, mixing
them with synthetic data helps in achieving better
results.

Train
Test HI EN

(TW)
HI EN
(FB) ES EN BN EN

ACL 40.33 49.96 38.40 47.81
Election 47.22 48.78 31.20 42.44
Mukherjee 46.22 48.98 39.76 44.42
Semeval 45.80 48.38 39.18 45.99
Union 43.50 49.80 41.90 41.20
Gold 52.26 65.37 42.60 55.19

Table 4: Percent accuracy on 20% test data after train-
ing on only synthetic and only gold text. Each row
corresponds to a source. Grey: Accuracy achieved
with only gold training. Blue: The closest accuracy
achieved to best.

4.7 Performance of standalone synthetic data

The accuracy of using only synthetic data as train-
ing is reported in Table 4. We can see that for
EN HI,TW and EN ES the synthetic data is very
close to the gold data performance (lagging by



3535

Category of failure Example sentence Gold Predicted
Keywords with different
polarity

manana voy conquistar la will forever be an amazing song not because me
la dedicaron but because my momma always jams to it

Positive Negative

“tomorrow I will conquer the will” forever be an amazing song not because
they dedicated it to me but because my momma always jams to it.
twin brothers lost in fair reunited in adulthood amidst dramatic circumstances
ei themer movie akhon ar viewers der attract kore na

Neutral Positive

twin brothers lost in fair reunited in adulthood amidst dramatic circumstances,
this theme does not yet attract viewers.

Ambiguous overall
meaning

elizaibq ellen quiere entrevistar julianna margulies clooney says she is tough
cookie she is hard one to crack

Negative Neutral

elizaibq ellen wants to interview julianna margulies clooney says she is tough
cookie she is hard one to crack
hum kam se kam fight ker haaray lekin tum loog zillat ki maut maaray gaye Positive Negative
We lost at least after a fight, but you died a terrible death.

Table 5: Examples cases of failure in prediction. Red: Negative Polarity words. Green: Positive polarity words.
Blue represents the English translation of the code-switched sentence.

5.04% and 2.84%). However, it performed poorly
for HI EN,FB and BN EN dataset. This is be-
cause there is heavy mismatch between the syn-
thetic text set generated and the test data distribu-
tion (Table 1) in these two datasets. The Pearson
rank correlation coefficient between the distance
(between test and training set) measures and rela-
tive accuracy gain is highly negative, −0.66.

To further establish the importance of domain
coherence, we report on an experiment performed
with HI EN,FB gold dataset. This dataset has texts
corresponding to two different entities namely
Narendra Modi and Salman Khan. Training SA
with natural text corresponding to one entity and
testing on the rest leads to a steep accuracy drop
from 65.37% to 52.32%.

4.8 Error analysis

We found two dominant error modes where syn-
thetic augmentation confuses the system. Table 5
shows a few examples. The first error mode can be
triggered by the presence of words of different po-
larities, one polarity more common than the other,
and the gold label being the minority polarity. The
second error mode is prevalent when the emotion
is weak or mixed. Either there is no strong opin-
ion, or there are two agents, one regarded posi-
tively and the other negatively.

4.9 Hate speech detection results

Table 6 shows hate speech detection results. Train-
ing with only synthetic text after thresholding and
stratified sampling outperforms training with only
gold-tagged text by 4% F1, and using both gold
and synthetic text gives a F1 boost of 6% beyond
using gold alone. Remarkably, synthetic text alone

outperforms gold text, because gold text has high
class imbalance, leading to poorer prediction. Be-
cause we can create arbitrary amounts of synthetic
text, we can balance the labels to achieve better
prediction.

Prec Recall F-score
Only synthetic 0.58 (0.63) 0.60 (0.63) 0.51 (0.52)
Synthetic +Gold 0.59 (0.60) 0.63 (0.63) 0.53 (0.54)
Gold 0.40 0.62 0.48

Table 6: Hate speech results (3-fold cross val.). In
most cells we show performance without thresholding
and stratification (within bracket with thresholding and
stratification). Green: Best performance in each col-
umn.

5 Conclusion

Code-mixing is an important and rapidly evolv-
ing mechanism of expression among multilingual
populations on social media. Monolingual senti-
ment analysis techniques perform poorly on code-
mixed text, partly because code-mixed text often
involves resource-poor languages. Starting from
sentiment-labeled text in resource-rich source lan-
guages, we propose an effective method to syn-
thesize labeled code-mixed text without designing
switching grammars. Augmenting scarce natural
text with synthetic text improves sentiment detec-
tion accuracy.

Acknowledgments

Bidisha Samanta was supported by Google India
Ph.D. Fellowship. We would like to thank Dipan-
jan Das and Dan Garrette for their valuable in-
puts. Soumen Chakrabarti was partly supported
by IBM.



3536

References

Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben-
gio. 2015. Neural machine translation by jointly
learning to align and translate. In Proceedings of
ICLR.

Utsab Barman, Amitava Das, Joachim Wagner, and
Jennifer Foster. 2014. Code mixing: A challenge
for language identification in the language of social
media. In Proceedings of the first workshop on com-
putational approaches to code switching, pages 13–
23.

Rafiya Begum, Kalika Bali, Monojit Choudhury, Kous-
tav Rudra, and Niloy Ganguly. 2016. Functions of
code-switching in Tweets: An annotation scheme
and some initial experiments. Proceedings of LREC.

Gayatri Bhat, Monojit Choudhury, and Kalika Bali.
2016. Grammatical constraints on intra-sentential
code-switching: From theories to working models.
arXiv preprint arXiv:1612.04538.

Aditya Bohra, Deepanshu Vijay, Vinay Singh,
Syed Sarfaraz Akhtar, and Manish Shrivastava.
2018. A dataset of hindi-english code-mixed social
media text for hate speech detection. In Proceedings
of the Second Workshop on Computational Model-
ing of People’s Opinions, Personality, and Emotions
in Social Media, pages 36–41.

Samuel R Bowman, Luke Vilnis, Oriol Vinyals, and
Dai. 2015. Generating sentences from a continuous
space. arXiv preprint arXiv:1511.06349.

Peter F Brown, Vincent J Della Pietra, Stephen A Della
Pietra, and Robert L Mercer. 1993. The mathemat-
ics of statistical machine translation: Parameter esti-
mation. Computational Linguistics, 19(2):263–311.

Gokul Chittaranjan, Yogarshi Vyas, Kalika Bali, and
Monojit Choudhury. 2014. Word-level language
identification using CRF: Code-switching shared
task report of MSR India system. In Proceedings of
The First Workshop on Computational Approaches
to Code Switching, pages 73–79.

Li Dong, Furu Wei, Chuanqi Tan, Duyu Tang, Ming
Zhou, and Ke Xu. 2014. Adaptive recursive neural
network for target-dependent twitter sentiment clas-
sification. In Proceedings of the 52nd Annual Meet-
ing of the Association for Computational Linguistics
(Volume 2: Short Papers), volume 2, pages 49–54.

Pablo Malvar Fernández. 2008. Improving Word-to-
word Alignments Using Morphological Information.
Ph.D. thesis, San Diego State University.

Antigoni-Maria Founta, Constantinos Djouvas, De-
spoina Chatzakou, Ilias Leontiadis, Jeremy Black-
burn, Gianluca Stringhini, Athena Vakali, Michael
Sirivianos, and Nicolas Kourtellis. 2018. Large
scale crowdsourcing and characterization of twitter
abusive behavior. arXiv preprint arXiv:1802.00393.

CJ Hutto Eric Gilbert. 2014. Vader: A parsimo-
nious rule-based model for sentiment analysis of so-
cial media text. In Eighth International Confer-
ence on Weblogs and Social Media (ICWSM-14).
Available at (20/04/16) http://comp. social. gatech.
edu/papers/icwsm14. vader. hutto. pdf.

Aditya Joshi, Ameya Prabhu, Manish Shrivastava, and
Vasudeva Varma. 2016. Towards sub-word level
compositions for sentiment analysis of hindi-english
code mixed text. In Proceedings of COLING 2016,
the 26th International Conference on Computational
Linguistics: Technical Papers, pages 2482–2491.

Anjuli Kannan and Oriol Vinyals. 2017. Adversar-
ial evaluation of dialogue models. arXiv preprint
arXiv:1701.08198.

Matt Kusner, Yu Sun, Nicholas Kolkin, and Kilian
Weinberger. 2015. From word embeddings to docu-
ment distances. In International Conference on Ma-
chine Learning, pages 957–966.

Minh-Thang Luong, Eugene Brevdo, and Rui Zhao.
2017. Neural machine translation (seq2seq) tutorial.
https://github.com/tensorflow/nmt.

Minh-Thang Luong, Hieu Pham, and Christopher D
Manning. 2015. Effective approaches to attention-
based neural machine translation. In Proceedings of
EMNLP.

Umar Maqsud. 2015. Synthetic text generation for sen-
timent analysis. In Workshop on Computational Ap-
proaches to Subjectivity, Sentiment and Social Me-
dia Analysis.

Subhabrata Mukherjee and Pushpak Bhattacharyya.
2012. Sentiment analysis in twitter with lightweight
discourse analysis. Proceedings of COLING 2012,
pages 1847–1864.

Subhabrata Mukherjee, Akshat Malu, Balamurali AR,
and Pushpak Bhattacharyya. 2012. Twisent: a mul-
tistage system for analyzing sentiment in twitter. In
Proceedings of the 21st ACM international confer-
ence on Information and knowledge management,
pages 2531–2534. ACM.

Zhenxing Niu, Mo Zhou, Le Wang, Xinbo Gao, and
Gang Hua. 2016. Ordinal regression with multiple
output cnn for age estimation. In Proceedings of
the IEEE conference on computer vision and pattern
recognition, pages 4920–4928.

Franz Josef Och and Hermann Ney. 2003. A systematic
comparison of various statistical alignment models.
Computational Linguistics, 29(1):19–51.

Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: a method for automatic
evaluation of machine translation. In Proceedings
of ACL.



3537

Braja Gopal Patra, Dipankar Das, and Amitava Das.
2018. Sentiment analysis of code-mixed indian
languages: An overview of sail code-mixed shared
task@ icon-2017. arXiv preprint arXiv:1803.06745.

Adithya Pratapa, Gayatri Bhat, Monojit Choudhury,
Sunayana Sitaram, Sandipan Dandapat, and Kalika
Bali. 2018. Language modeling for code-mixing:
The role of linguistic theory based synthetic data. In
Proceedings of ACL.

C J van Rijsbergen. 1979. Information Retrieval. But-
terworths, London. Online at http://www.dcs.
gla.ac.uk/Keith/Preface.html.

Darcey Riley and Daniel Gildea. 2012. Improving the
IBM alignment models using variational Bayes. In
Proceedings of ACL.

Sara Rosenthal, Noura Farra, and Preslav Nakov. 2017.
SemEval-2017 task 4: Sentiment analysis in Twitter.
In Proceedings of the 11th International Workshop
on Semantic Evaluation, SemEval ’17, Vancouver,
Canada. Association for Computational Linguistics.

Koustav Rudra, Shruti Rijhwani, Rafiya Begum, Ka-
lika Bali, Monojit Choudhury, and Niloy Ganguly.
2016. Understanding language preference for ex-
pression of opinion and sentiment: What do hindi-
english speakers do on twitter? In Proceedings of
the 2016 Conference on Empirical Methods in Nat-
ural Language Processing, pages 1131–1141.

Bidisha Samanta, Sharmila Reddi, Hussain Jagirdar,
Niloy Ganguly, and Soumen Chakrabarti. 2019. A
deep generative model for code-switched text. In
Proceedings of IJCAI.

David Sankoff and Shana Poplack. 1981. A formal
grammar for code-switching. Research on Lan-
guage & Social Interaction, 14(1):3–45.

Thomas Schoenemann. 2010. Computing optimal
alignments for the IBM-3 translation model. In Pro-
ceedings of CoNLL.

Royal Sequiera, Monojit Choudhury, and Kalika Bali.
2015. POS tagging of Hindi-English code mixed
text from social media: Some machine learning ex-
periments. In Proceedings of International Confer-
ence on NLP.

Shashank Sharma, PYKL Srinivas, and Rakesh Chan-
dra Balabantaray. 2015. Text normalization of code
mix and sentiment analysis. In Advances in Com-
puting, Communications and Informatics (ICACCI),
2015 International Conference on, pages 1468–
1473. IEEE.

Thamar Solorio, Elizabeth Blair, Suraj Mahar-
jan, Steven Bethard, Mona Diab, Mahmoud
Ghoneim, Abdelati Hawwari, Fahad AlGhamdi, Ju-
lia Hirschberg, Alison Chang, et al. 2014. Overview
for the first shared task on language identification
in code-switched data. In Proceedings of the First
Workshop on Computational Approaches to Code
Switching, pages 62–72.

Thamar Solorio and Yang Liu. 2008. Part-of-speech
tagging for English-Spanish code-switched text. In
Proceedings of EMNLP.

Leonid Nisonovich Vaseršteı̆n. 1969. Markov pro-
cesses over denumerable products of spaces describ-
ing large systems of automata. Problems of Informa-
tion Transmission, 5(3):47–52.

David Vilares, Miguel A Alonso, and Carlos Gómez-
Rodrı́guez. 2015. Sentiment analysis on monolin-
gual, multilingual and code-switching twitter cor-
pora. In Proceedings of the 6th Workshop on Com-
putational Approaches to Subjectivity, Sentiment
and Social Media Analysis, pages 2–8.

Yogarshi Vyas, Spandana Gella, Jatin Sharma, Kalika
Bali, and Monojit Choudhury. 2014. POS tagging of
English-Hindi code-mixed social media content. In
Proceedings of EMNLP.

Bo Wang, Maria Liakata, Arkaitz Zubiaga, and Rob
Procter. 2017. Tdparse: Multi-target-specific sen-
timent recognition on twitter. In Proceedings of the
15th Conference of the European Chapter of the As-
sociation for Computational Linguistics: Volume 1,
Long Papers, volume 1, pages 483–493.

Yizhe Zhang, Zhe Gan, and Lawrence Carin. 2016.
Generating text via adversarial training. In NIPS
workshop on Adversarial Training.

Yizhe Zhang, Zhe Gan, Kai Fan, Zhi Chen, Ricardo
Henao, Dinghan Shen, and Lawrence Carin. 2017.
Adversarial feature matching for text generation.
arXiv preprint arXiv:1706.03850.

Muhua Zhu, Yue Zhang, Wenliang Chen, Min Zhang,
and Jingbo Zhu. 2013. Fast and accurate shift-
reduce constituent parsing. In Proceedings of ACL.

http://www.dcs.gla.ac.uk/Keith/Preface.html
http://www.dcs.gla.ac.uk/Keith/Preface.html

