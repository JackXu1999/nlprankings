




















































Modeling Online Discourse with Coupled Distributed Topics


Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 4673–4682
Brussels, Belgium, October 31 - November 4, 2018. c©2018 Association for Computational Linguistics

4673

Modeling Online Discourse with Coupled Distributed Topics

Akshay Srivatsan† Zachary Wojtowicz‡ Taylor Berg-Kirkpatrick†

†Language Technologies Institute, Carnegie Mellon University, {asrivats,tberg}@cs.cmu.edu
‡Social and Decision Sciences, Carnegie Mellon University, zdw@andrew.cmu.edu

Abstract

In this paper, we propose a deep, globally nor-

malized topic model that incorporates struc-

tural relationships connecting documents in so-

cially generated corpora, such as online fo-

rums. Our model (1) captures discursive in-
teractions along observed reply links in addi-

tion to traditional topic information, and (2)
incorporates latent distributed representations

arranged in a deep architecture, which enables

a GPU-based mean-field inference procedure

that scales efficiently to large data. We ap-

ply our model to a new social media dataset

consisting of 13M comments mined from the
popular internet forum Reddit, a domain that

poses significant challenges to models that do

not account for relationships connecting user

comments. We evaluate against existing meth-

ods across multiple metrics including perplex-

ity and metadata prediction, and qualitatively

analyze the learned interaction patterns.

1 Introduction

Topic models have become one of the most com-

mon unsupervised methods for uncovering latent

semantic information in natural language data, and

have found a wide variety of applications across

the sciences. However, many common models -

such as Latent Dirichlet Allocation (Ng and Jordan,

2003) - make an explicit exchangeability assump-

tion that treats documents as independent samples

from a generative prior, thereby ignoring important

aspects of text corpora which are generated by non-

ergodic, interconnected social systems. While the

direct application of such models to datasets such

as transcripts of The French Revolution (Barron

et al., 2017) and discussions on Twitter (Zhao et al.,

2011) have yielded sensible topics and exciting in-

sights, their exclusion of document-to-document

interactions imposes limitations on the scope of

their applicability and the analyses they support.

For instance, on many social media platforms, com-

ments are short (the average Reddit comment is 10
words long), making them difficult to treat as full

documents, yet they do cohere as a collection, sug-

gesting that contextual relationships should be con-

sidered. Moreover, analysis of social data is often

principally concerned with understanding relation-

ships between documents (such as question-asking

and -answering), so a model able to capture such

features is of direct scientific relevance.

To address these issues, we propose a design that

models representations of comments jointly along

observed reply links. Specifically, we attach a vec-

tor of latent binary variables to each comment in

a collection of social data, which in turn connect

to each other according to the observed reply-link

structure of the dataset. The inferred representa-

tions can provide information about the rhetorical

moves and linguistic elements that characterize an

evolving discourse. An added benefit is that while

previous work such as Sequential LDA (Du et al.,

2012) has focused on modeling a linear progres-

sion, the model we present applies to a more gen-

eral class of acyclic graphs such as tree-structured

comment threads ubiquitous on the web.

Online data can be massive, which presents a

scalability issue for traditional methods. Our ap-

proach uses latent binary variables similar to a Re-

stricted Boltzmann Machine (RBM); related mod-

els such as Replicated Softmax (RS) (Salakhutdi-

nov and Hinton, 2009) have previously seen suc-

cess in capturing latent properties of language, and

found substantial speedups over previous methods

due to their GPU amenable training procedure. RS

was also shown to deal well with documents of

significantly different length, another key charac-

teristic of online data. While RBMs permit exact in-

ference, the additional coupling potentials present

in our model make inference intractable. How-

ever, the choice of bilinear potentials and latent



4674

ℎ1
�1 ℎ2 ℎ3 ℎ4

ℎ
� log−bilinear factor

unobserved topics

observed word counts

�2 �3
�4

ℎ0 Thread-Level  
Representation

Comment-Level

Representations

�3
�4 He took 300,000  photos so it’s a 1  

in 300,00 shot!

Getting that  

lightning strike is a

1 in a trillion shot…

FYI this won The�2 National Geographic
photo contest last year…

<URL> For 14 years, Sergio�1 Tapiro took 300,000+ photos“of which only 100 are good”…
Thread

Comment

Reply Link

�

Figure 1: DDTM factor graph

for an example thread. Each

comment is modeled as an ob-

served bag-of-words x with top-

ics represented by a latent bi-

nary vector h. Log-bilinear fac-

tors connect the latent and ob-

served variables of each com-

ment, and the latent variables

of parent-child comment pairs

along observed reply links. Bi-

ases are omitted for clarity.

features admits a mean-field inference procedure

which takes the form of a series of dense matrix

multiplications followed by nonlinearities, which

is particularly amenable to GPU computation and

lets us scale efficiently to large data.

Our model outperforms LDA and RS baselines

on perplexity and downstream tasks including meta-

data prediction and document retrieval when evalu-

ated on a new dataset mined from Reddit. We also

qualitatively analyze the learned topics and discuss

the social phenomena uncovered.

2 Model

We now present an overview of our model. Specifi-

cally, it will take the probabilistic form of an undi-

rected graphical model whose architecture mirrors

the tree structure of the threads in our data.

2.1 Motivating Dataset

We evaluate on a corpus mined from Reddit, an

internet forum which ranks as the fourth most traf-

ficked site in the US (Alexa, 2018) and sees mil-

lions of daily comments (Reddit, 2015). Discourse

on Reddit follows a branching pattern, shown in

Figure 1. The largest unit of discourse is a thread,

beginning with a link to external content or a natu-

ral language prompt, posted to a relevant subreddit

based on its subject matter. Users comment in re-

sponse to the original post (OP), or to any other

comment. The result is a structure which splits

at many points into more specific or tangential

discussions that while locally coherent may dif-

fer substantially from each other. The data reflect

features of the underlying memory and network

structure of the generating process; comments are

serially correlated and highly cross-referential. We

treat individual comments as “documents” under

the standard topic modeling paradigm, but use ob-

served reply structure to induce a tree of documents

for every thread.

2.2 Description of Discursive Distributed

Topic Model

We now introduce the Discursive Distributed Topic

Model (DDTM) (illustrated in Figure 1). For each

comment in the thread, DDTM assigns a latent

vector of binary random variables (or bits) that col-

lectively form a distributed embedding of the topi-

cal content of that comment; for instance, one bit

might represent sarcastic language while another

might track usage of specific acronyms - a given

comment could have any combination of those fea-

tures. These representations are tied to those of

parent and child comments via coupling potentials

(see Section 2.3), which allow them to learn dis-

cursive properties by inducing a deep undirected

network over the thread. In order to encourage

the model to use these comment-level representa-

tions to learn discursive and stylistic patterns as

opposed to simply topics of discussion, we incor-

porate a single additional latent vector for the entire

thread that interacts with each comment, explain-

ing word choices that are mainly topical rather than

discursive or stylistic. As we demonstrate in our

experiments (see Section 6) the thread-level embed-

ding learns distributions more reminiscent of what

a traditional topic model would uncover, while the

comment-level embeddings model styles of speak-



4675

ing and mannerisms that do not directly indicate

specific subjects of conversation. The joint proba-

bility is defined in terms of an energy function that

scores latent embeddings and observed word counts

across the tree of comments within a thread using

log-bilinear potentials, and is globally normalized

over all word count and embedding combinations.

2.3 Probability Model

More formally, consider a thread containing N

comments each of size Dn with a vocabulary of

size K. As depicted in Figure 1, each comment

is viewed as a bag-of-words, densely connected

via a log-bilinear potential to a latent embedding

of size F . Let each comment be represented as as

an integer vector xn ∈ Z
K where xnk is number

of times word k was observed in comment n, and

let hn = {0, 1}
F be the topic embedding for each

comment, and let h0 = {0, 1}
F be the embedding

for the entire thread. To model topic transitions,

we score the embeddings of parent-child pairs with

a separate coupling potential as shown in Figure 1

(comments with no parents or children receive ad-

ditional start/stop biases respectively). Let replies

be represented with sets R, PN , and CN where

(n,m) ∈ R and n ∈ Pm and m ∈ Cn if comment
m is a reply to comment n. DDTM assigns prob-

ability to a specific configuration of x, h with an

energy function scored by the emission (πe) and

coupling (πc) potentials.

E(x, h; θ) =

N∑

n=1

πe(h, x, n)

︸ ︷︷ ︸

Emission Potentials

+
∑

(n,m)∈R

πc(h, n,m)

︸ ︷︷ ︸

Coupling Potentials

πe(h, x, n) = h
⊺

nUxn + x
⊺

na+Dnh
⊺

nb

+ h⊺0V xn +Dnh
⊺

0c

πc(h, n,m) = h
⊺

nWhm

(1)

Note that the bias on embeddings is scaled by the

number of words in the comment, which controls

for their highly variable length. The joint probabil-

ity is computed by exponentiating the energy and

dividing by a normalizing constant.

p(x, h; θ) =
exp(E(x, h; θ))

Z(θ)

Z(θ) =
∑

x′,h′

exp(E(x′, h′; θ))
(2)

This architecture encourages the model to learn

discursive maneuvers via the coupling potentials

while separating within-thread variance and across-

thread variance through the comment-level and

thread-level embeddings respectively. The cou-

pling of latent variables makes factored inference

impossible, meaning that even the exact computa-

tion of the partition function is no longer tractable.

This necessitates approximating the gradients for

learning which we will now address.

3 Learning and Inference

Inference in this model class in intractable, so as

has been done in previous work on topic modeling

(Ng and Jordan, 2003) we rely on variational meth-

ods to approximate the gradients needed during

training as well as the posteriors over the topic bit

vectors. Specifically, we will need the gradients of

the normalizer and the sum of the energy function

over the hidden variables

E(x; θ) = log
∑

h

exp(E(x, h; θ)) (3)

which we refer to as the marginal energy. Follow-

ing the approach described for undirected models

by Eisner (2011), we approximate these quantities

and their gradients with respect to the model pa-

rameters θ as we will now describe (thread-level

embeddings are omitted in this section for clarity).

3.1 Normalizer Approximation

We aim to train our model to maximize the

marginal likelihood of the observed comment word

counts, conditioned on the reply links. To do this

we must compute the gradient of the normalizer

Z(θ). However, this quantity is computationally
intractable, as it contains a summation over all

exponential choices for every word in the thread.

Therefore, we must approximate Z(θ). Observe
that under Jensen’s Inequality, we can form the

following lower bound on the normalizer using an

approximate joint distribution q(Z).

logZ(θ) = log
∑

x,h

exp(E(x, h; θ))

≥ Eq(Z) [E(x, h; θ)]− Eq(Z) [log q
(Z)(x, h;φ, γ)]

(4)

We now define q(Z) as depicted in Figure 2

as a mean-field approximation that treats all vari-

ables as independent. We parameterize q(Z) with



4676

ℎ�
� �, ℎ; �, �

ℎ�
��

�
��

�, ℎ; �
��, ��, ��, ��,

�� ��

ℎ�
� ℎ; �

��, ��, ��, ��,

Joint Probability Variational Approx. to Joint Variational Approx. to Post.

Figure 2: Factor

graph of full joint

compared to mean-

field approximations

to joint and posterior.

φnf ∈ [0, 1], independent Bernoulli parameters
representing the probability of hnf being equal to

1, and γnk replicated softmaxes representing the
probability of a word in xn taking the value k. Note

that all words in xn are modeled as samples from

this single distribution. The approximation then

factors as follows:

q(Z)(x, h;φ, γ) = q(Z)(x; γ) · q(Z)(h;φ)

q(Z)(x; γ) =

N∏

n=1

K∏

k=1

(γnk)
xnk

q(Z)(h;φ) =

N∏

n=1

F∏

f=1

(

(φnf )
hnf (1− φnf )

(1−hnf )
)

(5)

We optimize the parameters of q(Z) to maximize

its variational lower bound, via iterative mean-field

updates, which allow us to perform coordinate as-

cent over the parameters of q(Z). Maximizing the

lower bound with respect to particular φnf and γnk
while holding all other parameters frozen, yields

the following mean-field update equations (biases

omitted for clarity):

φn· = σ

(

Uγn +
∑

m∈Cn

Wφm + φ
⊺

Pn
W

)

γn· = σ (φ
⊺

nU)

(6)

We iterate over the parameters of q(Z) in an

“upward-downward” manner; first updating φ for

all comments with no children, then all comments

whose children have been updated, and so on up to

the root of the thread. Then we perform the same

updates in reverse order. After updating all φ, we

then update γ simultaneously (the components of

γ are independent conditioned on φ). We iterate

these upward-downward passes until convergence.

3.2 Marginal Energy Approximation

We can now approximate the normalizer, but still

need the marginal data likelihood in order to take

gradient steps on it and train our model. In order

to recover the marginal likelihood, we must next

approximate the marginal energy E(x; θ) as it too
is intractable. This is due to the coupling potentials,

which make the topics across comments dependent

even when conditioned on the word counts. To do

this, we form an additional variational approxima-

tion (see Figure 2) to the marginal energy, which

we optimize similarly.

E(x; θ) = log
∑

h

exp(E(x, h; θ))

≥ Eq(E) [E(x, h; θ)]− Eq(E) [log q
(E)(h;ψ)]

(7)

Since q(E)(h;ψ) need only model the hidden
units h, we can parameterize it in the same man-

ner as q(Z)(h;φ). Note that while these distribu-
tions factor similarly, they do not share parame-

ters, although we find that in practice, initializing

φ← ψ improves our approximation. We optimize
the lower bound on E(x; θ) via a similar coordi-
nate ascent strategy, where the mean-field updates

take the following form (biases omitted for clarity):

ψn· = σ

(

Uhn +
∑

m∈Cn

Wψm + ψ
⊺

Pn
W

)

(8)

We can use q(E) to perform inference at test time

in our model, as its parameters ψ directly corre-

spond to the expected values of the hidden topic

embeddings under our approximation.

3.3 Learning via Gradient Ascent

We train the parameters of our true model p(x, h; θ)
via stochastic updates wherein we optimize both ap-



4677

proximations on a single datum (i.e. thread) to com-

pute the approximate gradient of its log-likelihood,

and take a single gradient step on the model param-

eters (repeating on all training instances until con-

vergence). That gradient is given by the difference

in feature expectations under the approximations

(entropy terms from the lower bounds are dropped

as they do not depend on θ).

∇ log p(x; θ) ≈ Eq(E)(h;ψ) [∇E(x, h; θ)]

− Eq(Z)(x′,h;ψ)
[
∇E(x′, h; θ)

] (9)

In summary, we use two separate mean-field

approximations to compute lower bounds on the

marginal energy E(x, h; θ), and its normalizer
Z(θ), which lets us approximate the marginal like-
lihood p(x; θ). Note that as our estimate on the
marginal likelihood is the difference between two

lower bounds, it is not a lower bound itself, al-

though in practice it works well for training.

3.4 Scalability and GPU Implementation

Given the magnitude of our dataset, it is essen-

tial to be able to train efficiently at scale. Many

commonly used topic models such as LDA (Ng

and Jordan, 2003) have difficulty scaling, partic-

ularly if trained via MCMC methods. Improve-

ments have been shown from online training (Hoff-

man et al., 2010), but extending such techniques

to model comment-to-comment connections and

leverage GPU compute is nontrivial.

In contrast, our proposed model and mean-field

procedure can be scaled efficiently to large data

because they are amenable to GPU implementation.

Specifically, the described inference procedure can

be viewed as the output of a neural network. This is

because DDTM is globally normalized with edges

parameterized as log-bilinear weights, which re-

sults in the mean-field updates taking the form

of matrix operations followed by nonlinearities.

Therefore, a single iteration of mean-field is equiv-

alent to a forward pass through a recursive neu-

ral network, whose architecture is defined by the

tree structure of the thread. Multiple iterations are

equivalent to feeding the output of the network

back into itself in a recurrent manner, and optimiz-

ing for T iterations is achieved by unrolling the

network over T timesteps. This property makes

DDTM highly amenable to efficient training on

a GPU, and allowed us to scale experiments to a

dataset of over 13M total Reddit comments.

4 Experimental Setup

4.1 Data

We mined a corpus of Reddit threads pulled

through the platform’s API. Focusing on the

twenty most popular subreddits (gifs, today-

ilearned, CFB, funny, aww, AskReddit, Black-

PeopleTwitter, videos, pics, politics, The_Donald,

soccer, leagueoflegends, nba, nfl, worldnews,

movies, mildlyinteresting, news, gaming) over a

one month period yielded 200, 000 threads consist-
ing of 13, 276, 455 comments total. The data was
preprocessed by removing special characters, re-

placing URLs with a domain-specific token, stem-

ming English words using a Snowball English

Stemmer (Porter, 2001), removing stopwords, and

truncating the vocabulary to only include the top

10, 000 most common words. OPs are modeled as
a comment at the root of each thread to which all

top-level comments respond. This dataset will be

made available for public use after publication.

4.2 Baselines and Comparisons

We compare to baselines of Replicated Softmax

(RS) (Salakhutdinov and Hinton, 2009) and Latent

Dirichlet Allocation (LDA) (Ng and Jordan, 2003).

RS is a distributed topic model similar to our own,

albeit without any coupling potentials. LDA is a lo-

cally normalized topic model which defines topics

as non-overlapping distributions over words. To en-

sure that DDTM does not gain an unfair advantage

purely by having a larger embedding space, we

divide the dimensions equally between comment-

and thread-level. Unless specified 64 bits/topics
were used. We experiment with RS and LDA treat-

ing either comments or full threads as documents.

4.3 Training and Initialization

SGD was performed using the Adam opti-

mizer (Kingma and Ba, 2015). When running in-

ference, we found convergence was reached in an

average of 2 iterations of updates. Using a sin-
gle NVIDIA Titan X (Pascal) card, we were able

to train our model to convergence on the training

set of 10M comments in less than 30 hours. It is
worth noting that we found DDTM to be fairly sen-

sitive to initialization. We found best results from

Gaussian noise, with comment-level emissions at

variance of 0.01, thread-level emissions at 0.0001,
and transitions at 0. We initialized all biases to 0
except for the bias on word counts, which we set

to the unigram log-probabilities from the train set.



4678

Perplexity (nats)

Bits 32 64 96 128

RS (thr) 2240 2234 2233 2257

RS (cmt) 1675 1894 2245 2518

DDTM (-cpl) 2027 1704 1766 1953

DDTM 1624 1590 1719 713

Table 1: Perplexity of DDTM with and without

coupling potentials (-cpl) vs. baselines trained at

comment (cmt) or thread (thr) level across vari-

ous numbers of topics and bits. For reference, a

unigram model achieves 2644.

5 Results

5.1 Evaluating Perplexity

We compare models by perplexity on a held-out

test set, a standard evaluation for generative and

latent variables models.

Setup: Due to the use of mean-field approxima-

tions for both the marginal energy and normalizer

we lose any guarantees regarding the accuracy of

our likelihood estimate (both approximations are

lower bounds, and therefore their difference is nei-

ther a strict lower bound nor guaranteed to be unbi-

ased). To evaluate perplexity in a more principled

way, we use Annealed Importance Sampling (AIS)

to estimate the ratio between our model’s normal-

izer and the tractable normalizer of a base model

from which we can draw true independent samples

as described by Salakhutdinov and Murray (2008).

Note that since the marginal energy is intractable

in our model, unlike a standard RBM, we must

sample the joint - and not the marginal - intermedi-

ate distributions. This yields an unbiased estimate

of the normalizer. The marginal energy must still

be approximated via a lower bound, but given that

AIS is unbiased and empirically low in variance,

we can treat the overall estimate as a lower bound

on likelihood for evaluation. Using 2000 interme-
diate distributions, and averaging over 20 runs, we
evaluated per-word perplexity over a set of 50 un-
seen threads. Results are shown in Table 1.

Results: DDTM achieves the lowest perplexity at

all dimensionalities. Note our ablation with the

coupling potentials removed (-cpl), increases per-

plexity noticeably, indicating that modeling replies

helps beyond simply modeling threads and com-

ments jointly, particularly at larger embeddings.

For reference, a unigram model achieves 2644.

Task Upvote Regr. Deletion Pred.

(MSE) (% acc.)

LDA (thr) 1.952 68.35

LDA (cmt) 2.047 59.26

RS (thr) 2.024 69.92

RS (cmt) 2.007 66.45

DDTM 1.933 70.39

Table 2: Performance of DDTM vs. Replicated

Softmax (RS) and Latent Dirichlet Allocation

(LDA) at predicting upvotes and child deletion.

We find that LDA’s approximate perplexity is even

worse, likely due to slackness in its lower bound.

5.2 Upvote Regression

To measure how well embeddings capture

comment-level characteristics, we feed them into a

linear regression model that predicts the number of

upvotes the comment received. Upvotes provide a

loose human-annotated measure of likability. We

expect that context matters in determining how well

received a comment is; the same comment posted

in response to different parents may receive a very

different number of upvotes. Hence, we expect

comment-level embeddings to be more informa-

tive for this task when connected via our model’s

coupling potentials.

Setup: We trained a standard linear regressor for

each model. The regressor was trained using or-

dinary least squares on the entire training set of

comments using the model’s computed topic em-

beddings as input, and the number of upvotes on

the comment as the output to predict. As a pre-

processing step, we took the log of the absolute

number of votes before training. We compared

models by mean squared error (MSE) on our test

set. Results are shown in Table 2.

Results: DDTM achieves lowest MSE. To assess

statistical significance, we performed a 500 sample
bootstrap of our training set. The standard errors

of these replications are small, and a two-sample

t-test rejects the null hypothesis that DDTM has an

average MSE equal to that of the next best method

(p < .001). Note that our model outperforms both
comment- and thread-level embeddings, suggesting

that modeling these jointly, and modeling the ef-

fect of neighboring representations in the comment

graph, more accurately learns information relevant

to a comment’s social impact.



4679

Figure 3: Precision vs. recall for document retrieval

based on subreddit comparing various models for

1000 randomly selected held-out query comments.

5.3 Deletion Prediction

Comments that are excessively provocative or in

violation of site rules are often deleted, either by

the author or a moderator. We can measure whether

DDTM captures discursive interactions that lead

to such intervention by training a logistic classifier

that predicts whether any of a given comment’s

children have been deleted.

Setup: For each model, a logistic regression classi-

fier was trained stochastically with the Adam opti-

mizer on the entire training set of comments using

the model’s computed topic embeddings as input,

and a binary label for whether the comment had any

deleted children as the output to predict. We com-

pared models by accuracy on our test set. Results

are shown in Table 2.

Results: DDTM gets the highest accuracy. In-

terestingly, thread-level models do better than

comment-level ones, which suggests that certain

topics or even subreddits may correlate with com-

ments being deleted. This makes sense given

that subreddits vary in severity of moderation.

DDTM’s performance also demonstrates that mod-

eling comment-to-comment interaction patterns is

helpful in predicting when a comment will spawn

a deleted future response, which strongly matches

our intuition.

5.4 Document Retrieval

Finally, while DDTM is not designed to better cap-

ture topical structure, we evaluate the extent to

which it can still capture this information by per-

forming document retrieval, a standard evaluation,

for which we treat the subreddit to which a thread

Figure 4: t-SNE visualization of a random sample

of DDTM thread-level embeddings colored by sub-

reddit (not observed in training)

Figure 5: t-SNE visualization of a random sample

of DDTM comment-level embeddings colored by

log of comment length (darker is longer).

was posted as a label for relevance. Note that every

comment within the same thread belongs to the

same subreddit, which gives thread-level models

an inherent advantage at this task. We include this

task purely for the purpose of demonstrating that

by capturing discursive patterns, DDTM does not

lose the ability to model thread-level topics as well.

Setup: Given a query comment from our held-out

test set, we rank the training set by the Dice simi-

larity of the hidden embeddings computed by the

model. We consider a retrieved comment relevant

to the query if they both originate from the same

subreddit, which loosely categorizes the seman-

tic content. Tuning the number of documents we

return allows us to form precision recall curves,

which we show in Figure 3.

Results: DDTM outperforms both comment-level

baselines and is competitive with thread-level mod-

els, even beating LDA at high levels of recall. This

indicates that despite using half of its dimensions to

model comment-to-comment interactions DDTM

can still do almost as good a job of modeling thread-

level semantics as a model using its entire capacity



4680

Bit # Associated Word Stems by Emission Weight (Higher Score→ Lower Score)

Comment-Level

Bit 1 faq tldrs pms 165 til keyword questions feedback chat pm

2 irl riamverysmart legend omfg riski aboard favr madman skillset tunnel

3 lotta brah ouch spici oof bummer buildup viewership hd uncanni

4 funniest mah tfw teleport fav hoo plz bah whyd dumbest

5 handsom hipster texan hottest whore norwegian shittier scandinavian jealousi douch

Thread-Level

Bit 1 btc gameplay tutori cyclist dev currenc kitti bitcoin rpg crypto

2 url_youtu url_leagueoflegends url_businessinsider url_twitter url_redd url_snopes

3 comey pede macron pg13 maga globalist ucf committe cuck distributor

4 maduro venezuelan ballot puerto catalonia rican quak skateboard venezuela quebec

5 nra scotus opioid cheney nevada metallica marijuana vermont colorado xanax

Table 3: Words with the highest emission weight for various comment-level and thread-level bits.

to do so. The gap between comment-level RS and

LDA is also consistent with LDA’s known issues

dealing with sparse data (Sridhar, 2015), and lends

credence to our theory that distributed topic repre-

sentations are better suited to such domains.

6 Qualitative Analysis of Topics

We now offer qualitative analysis of the topic em-

beddings learned by our model. Note that since we

use distributed embeddings, our bits are more akin

to filters than complete distributions over words,

and we typically observe as many as half of them

active for a single comment. In a sense, we have

an exponential number of topics, whose parame-

terization simply factors over the bits. Therefore,

it can be difficult to interpret them as one would

interpret topics learned by a model such as LDA.

Furthermore, we find that in practice this effect

is correlated with the topic embedding size; the

more bits our model has, the less sparse and con-

sequently less individually meaningful the bits be-

come. Therefore for this analysis, we specifically

focus on DDTM trained with 64 bits total.

6.1 Bits in Isolation

Directly inspecting the emission parameters, re-

veals that the comment-level and thread-level

halves of our embeddings capture substantially

different aspects of the data (shown in Table 3)

akin to vertical, within-thread, and horizontal,

across-thread sources of variance respectively. The

comment-level topic bits tend to reflect styles of

speaking, lingo, and memes that are not unique to

a particular subject of discourse or even subreddit.

For example, comment-level Bit 2 captures many

words typical of taunting Reddit comments; reply-

ing with “/r/iamverysmart” (a subreddit dedicated

to mocking people who make grandiose claims

about their intellect) is a common way of jokingly

implying that the author of the parent comment

takes themselves too seriously — and thus corre-

sponds to a certain kind of rhetorical move. Further,

it is grouped with other words that indicate related

rhetorical moves; calling a user “risky” or a “mad-

man” is a common means of suggesting that they

are engaging in a pointless act of rebellion. They

also cluster at the coarsest level by length (see Fig-

ure 5) which we find to correlate with writing style.

By contrast, the thread-level bits are more in-

dicative of specific topics of discussion, and unsur-

prisingly they cluster by subreddit (see Figure 4).

For example, thread-level Bit 3 captures lexicon

used almost exclusively by alt-right Donald Trump

supporters as well as the names of various politi-

cal figures. Bit 4 highlights words related to civil

unrest in Spanish speaking parts of the world.

6.2 Bits in Combination

While these distributions over words (particularly

for comment-level bits) can seem vague, when mul-

tiple bits are active, their effects compound to pro-

duce much more specific topics. One can think of

the bits as defining soft filters over the space of

words, that when stacked together carve out pat-

terns not apparent in any of them individually. We

now analyze a few sample topic embeddings. To

do this, we perform inference as described on a

held-out thread, and pass the comment-level topic

embedding for a single sampled comment through

our emission matrix and inspect the words with the

highest corresponding weight (shown in Table 4).

In generative terminology, these can be thought of

as reconstructions of comments.

These topic embeddings capture more specific

conversational and rhetorical moves. For example,



4681

Sample # Associated Word Stems by Emission Weight (Higher Score→ Lower Score)

Comment-Level

Sample 1 grade grader math age 5th 9th 10th till mayb 7th

2 repost damn dope bamboozl shitload imagin cutest sad legendari awhil

3 heh dawg hmm spooki buddi aye m8 aww fam woah

4 hug merci bless tfw prayer pleas dear bear banana satan

5 chuckl cutest funniest yall bummer oooh mustv coolest ok oop

6 cutest heard coolest funniest havent seen ive craziest stupidest weirdest

7 reev keanu christoph murphi walken vincent chris til wick roger

8 moron douchebag stupid dipshit snitch jackass dickhead idioci hypocrit riddanc

9 technic actual realiz happen escal werent citat practic memo cba

10 reddit shill question background user subreddit answer relev discord guild

Table 4: Words with the highest emission weight for sample held-out comment reconstructions.

Sample 6 displays supportive and interested reac-

tionary language, which one might expect to see

used in response to a post or comment linking to

media or describing something intriguing. This is

of note given that one of the primary aims of includ-

ing coupling potentials was to encourage DDTM

to learn “topics” that correspond to responses and

interactive behavior, something existing methods

are largely not designed for. By contrast, Sample 9

captures a variety of hostile language and insults,

which unlike those discussed previously do not de-

note membership in a particular online community.

As patterns of toxic and hateful behavior on Red-

dit are more well-studied (Chandrasekharan et al.,

2017), it could be useful to have a tool to analyze

precipitous contexts and parent comments, some-

thing which we hope systems based on coupling

of comment embeddings have the capacity to pro-

vide. Sample 10 is of particular interest as it con-

sists largely of Reddit terminology. Conversations

about the meta of the site can manifest for example

in users accusing each other of being “shills” (i.e.

accounts paid to astroturf on behalf of external in-

terests) or requesting/responding to “guilding”, a

feature which lets users purchase premium access

for each other often in response to a particularly

well made comment.

7 Related Work

Many topic models such as LDA (Ng and Jordan,

2003) treat documents as independent mixtures, yet

this approach fails to model how comments inter-

act with one another throughout a larger discourse

if such connections exist in the data. Other work

has considered modeling hierarchy in topics (Grif-

fiths et al., 2004). These models form hierarchical

representations of topics themselves, but still treat

documents as independent. While this approach

can succeed in learning topics of various granulari-

ties, it does not explicitly track how topics interact

in the context of a nested conversation.

Some approaches such as Pairwise-Link-LDA

and Link-PSLA-LDA (Nallapati et al., 2008) at-

tempt to model interactions among documents in

an arbitrary graph, albeit with important drawbacks.

The former models every possible pairwise link

between comments, and the latter models links

as a bipartite graph, limiting its ability to scale

to large tree-structured threads. Similar work on

Topic-Link LDA (Liu et al., 2009) models link

probabilities conditioned on both topic similar-

ity and an authorship model, yet this approach is

poorly suited to high volume, semi-anonymous on-

line domains. Other studies have leveraged reply-

structures on Reddit in the context of predicting

persuasion (Hidey and McKeown), but DDTM dif-

fers in its generative, unsupervised approach.

DDTM’s emission potentials are similar to those

of Replicated Softmax (Salakhutdinov and Hinton,

2009), an undirected model based on a Restricted

Boltzmann Machine. Unlike LDA-style models,

RS does not assign a topic to each word, but instead

builds a distributed representation. In this setting, a

single word can be likely under two different topics,

both of which are present, and lend probability

mass to that word. LDA-style models by contrast

would require the topics to compete for the word.

8 Conclusion

In this paper we introduce a novel way to learn

topic interactions in observed discourse trees, and

describe GPU-amenable learning techniques to

train on large-scale data mined from Reddit. We

demonstrate improvements over previous models

on perplexity and downstream tasks, and offer qual-

itative analysis of learned discursive patterns. The

dichotomy between the two levels of embeddings

hints at applications in style-transfer.



4682

References

Alexa. 2018. Reddit.com traffic, demographics
and competitors. https://www.alexa.com/
siteinfo/reddit.com. Accessed: 2018-02-
22.

Alexander TJ Barron, Jenny Huang, Rebecca L Spang,
and Simon DeDeo. 2017. Individuals, institutions,
and innovation in the debates of the french revolu-
tion. arXiv preprint arXiv:1710.06867.

Eshwar Chandrasekharan, Umashanthi Pavalanathan,
Anirudh Srinivasan, Adam Glynn, Jacob Eisenstein,
and Eric Gilbert. 2017. You can’t stay here: The
efficacy of reddit’s 2015 ban examined through
hate speech. Proceedings of the ACM on Human-
Computer Interaction, 1(CSCW):31.

Lan Du, Wray Buntine, Huidong Jin, and Changyou
Chen. 2012. Sequential latent dirichlet allocation.
Knowledge and information systems, 31(3):475–
503.

Jason Eisner. 2011. High-level explanation of varia-
tional inference. https://www.cs.jhu.edu/
~jason/tutorials/variational.html.
Accessed: 2018-02-22.

Thomas L Griffiths, Michael I Jordan, Joshua B Tenen-
baum, and David M Blei. 2004. Hierarchical topic
models and the nested chinese restaurant process. In
Advances in neural information processing systems,
pages 17–24.

Christopher Hidey and Kathleen McKeown. Persua-
sive influence detection: The role of argument se-
quencing. In Association for the Advancement of Ar-
tificial Intelligence.

M. Hoffman, D. Blei, and F. Bach. 2010. Online learn-
ing for latent dirichlet allocation. Neural Informa-
tion Processing Systems.

Diederik P. Kingma and Jimmy Ba. 2015. Adam: A
method for stochastic optimization. 3rd Interna-
tional Conference for Learning Representations.

Yan Liu, Alexandru Niculescu-Mizil, and Wojciech
Gryc. 2009. Topic-link lda: joint models of topic
and author community. In proceedings of the 26th
annual international conference on machine learn-
ing, pages 665–672. ACM.

Ramesh M Nallapati, Amr Ahmed, Eric P Xing, and
William W Cohen. 2008. Joint latent topic models
for text and citations. In Proceedings of the 14th
ACM SIGKDD international conference on Knowl-
edge discovery and data mining, pages 542–550.
ACM.

D. Blei A. Ng and M. Jordan. 2003. Latent dirichlet
allocation. Journal of Machine Learning Research,
3:993–1022.

Martin Porter. 2001. Snowball: A language for
stem-ming algorithms. http://snowball.
tartarus.org/texts. Accessed: 2018-02-22.

Reddit. 2015. Reddit in 2015. https:
//redditblog.com/2015/12/31/

reddit-in-2015/. Accessed: 2018-02-22.

Ruslan Salakhutdinov and Geoffrey Hinton. 2009.
Replicated softmax: An undirected topic model. Ad-
vances in Neural Information Processing Systems,
22:1607–1614.

Ruslan Salakhutdinov and Iain Murray. 2008. On the
quantitative analysis of deep belief networks. Pro-
ceedings of the 25th International Conference on
Machine Learning.

Vivek Kumar Rangarajan Sridhar. 2015. Unsupervised
topic modeling for short texts using distributed repre-
sentations of words. In Proceedings of the 1st Work-
shop on Vector Space Modeling for Natural Lan-
guage Processing, pages 192–200.

Wayne Xin Zhao, Jing Jiang, Jianshu Weng, Jing He,
Ee-Peng Lim, Hongfei Yan, and Xiaoming Li. 2011.
Comparing twitter and traditional media using topic
models. In European Conference on Information Re-
trieval, pages 338–349. Springer.

https://www.alexa.com/siteinfo/reddit.com
https://www.alexa.com/siteinfo/reddit.com
https://www.cs.jhu.edu/~jason/tutorials/variational.html
https://www.cs.jhu.edu/~jason/tutorials/variational.html
http://snowball.tartarus.org/texts
http://snowball.tartarus.org/texts
https://redditblog.com/2015/12/31/reddit-in-2015/
https://redditblog.com/2015/12/31/reddit-in-2015/
https://redditblog.com/2015/12/31/reddit-in-2015/

