Modelling the informativeness and timing of non-verbal cues

in parent–child interaction

Kristina Nilsson Bj¨orkenstam1, Mats Wir´en1 and Robert ¨Ostling2

{kristina.nilsson, mats.wiren}@ling.su.se, robert.ostling@helsinki.fi

1Department of Linguistics

Stockholm University

SE-106 91 Stockholm, Sweden

Abstract

How do infants learn the meanings of their
ﬁrst words? This study investigates the in-
formativeness and temporal dynamics of
non-verbal cues that signal the speaker’s
referent in a model of early word–referent
mapping. To measure the information pro-
vided by such cues, a supervised clas-
siﬁer is trained on information extracted
from a multimodally annotated corpus of
18 videos of parent–child interaction with
three children aged 7 to 33 months. Con-
tradicting previous research, we ﬁnd that
gaze is the single most informative cue,
and we show that this ﬁnding can be at-
tributed to our ﬁne-grained temporal an-
notation. We also ﬁnd that offsetting the
timing of the non-verbal cues reduces ac-
curacy, especially if the offset is negative.
This is in line with previous research, and
suggests that synchrony between verbal
and non-verbal cues is important if they
are to be perceived as causally related.

1 Background and introduction

There is a growing literature on how infants use
non-verbal input such as parents’ hand manipu-
lations of salient objects to infer the meanings
of their ﬁrst words. Meaning seems to arise as
a probabilistic process where recurrent acoustic
patterns gain referential value as they are linked
to time-synchronous recurrent patterns in other
modalities (Trueswell et al., 2016; Gogate et
al., 2006; Matatyaho and Gogate, 2008; Lac-
erda, 2009). The details of this process, such
as the informativeness and temporal dynamics

2Department of Modern Languages

University of Helsinki
PL 24 (Unionsgatan 40)
00014 Helsinki, Finland

of different cues in word–referent mapping, are
still contested, though.
In the social-pragmatic
approach,
joint attention and understanding of
speakers’ communicative intentions are the cen-
tral vehicle for investigating the mapping, but
the mechanisms typically appear to be determin-
istic (Tomasello, 2000).
In contrast, the asso-
ciative learning approach emphasises how cross-
situational co-occurrences of words and referents
increase the salience of objects, including multiple
objects in ambiguous learning contexts.

A frequently used methodology for studying
word–referent mapping is the Human Simula-
tion Paradigm (HSP), originally devised by Gleit-
man and colleagues (Gillette et al., 1999; Pic-
cin and Waxman, 2007; Medina et al., 2011).
Here, observers try to estimate referential trans-
parency by reconstructing intended referents from
non-verbal cues as they watch a muted video of
parent–child interaction. Another methodology,
which is used in this paper, is to try to model the
word–referent mapping directly. Such a model
is based on coding of the referential events in
a video, typically as perceived by an ideal ob-
server (Geisler, 2011); in other words, someone
assumed to optimally handle the perceptual task
given by the learning environment as a whole, as
recorded by the video. An example of this line of
work is Yu and Ballard (2007). They combined
social cues (in the form of prosodic affect and
joint attention) with statistical learning of cross-
situational co-occurrence into a uniﬁed model of
word learning, showing that this model performed
better than a purely statistical approach. Further-
more, Frank et al. (2009) showed that a uniﬁed
model of cross-situational co-occurrence and in-
terpretation of speakers’ referential intention out-

performed other models of cross-situational word
learning, including the model of Yu and Ballard
(2007).

In a subsequent study which is the closest paral-
lel to the problems dealt with in this paper, Frank
et al. (2012) attempted to quantify the informative-
ness of eye gaze, hand positions and hand point-
ing (social cues), as well as referents of previous
utterances (discourse continuity), using an ideal
observer scenario. For each utterance, the toys
present in the ﬁeld of view of the child at the time
of the utterance were coded.
(To determine the
timing, coders were listening to the audio.) The
union of the sets of such objects associated with
all the utterances of a video thus formed the set of
possible referents. There were between 3 and 21
different objects per dyad, but the number of ob-
jects in the child’s view (the ambiguity) for each
utterance was on average between 1.18 and 2.93
per dyad. Then the object(s) in the context that
were being looked at, held or pointed to by the par-
ent (the social cues) were coded. In addition, the
object(s) that were being looked at or held by the
child (referred to as attentional cues) were coded.
Finally, the parent’s intended referent for each ut-
terance — those that contained the name of an
object or pronoun referring to it — were coded
(“look at the doggie”, “look at his eyes and ears”).
The result, based on regarding each cue as a
predictor for the object reference, was that point-
ing was a powerful predictor with a precision of
0.78. However, pointing was not frequently used;
in other words, it had low recall in the sense that
it was seldom used when an object was referred to
(and instead other means were used). Eye gaze
and hand position, on the other hand, had low
prediction accuracies, with F -scores around 0.45.
The result was that the social cues appeared to be
noisy and that, generally speaking, no such cue
on its own would allow an observer to resolve the
referential ambiguities. Simulations with a super-
vised classiﬁer indicated that the prediction accu-
racy could be somewhat improved by combining
information from any two different cues, but that
the third did not add anything.

As discussed by Frank et al. (2012), however,
it is possible that some discriminatory power was
lost because of the coarse temporal granularity of
the model, where any temporal coordination be-
low the utterance level was invisible. For example,
if the parent was looking ﬁrst at one object and

later at another object during the same utterance,
the coding did not capture the timing and ordering
of these events. More generally, if there is a sys-
tematic timing relation between verbal and non-
verbal cues that can support the learner’s choice of
referent, then we would want to distinguish it. A
second limitation of the model was that all kinds
of hand movements and gestures were coded as
either of two discrete cues, namely, hand position
and hand pointing.

This paper attempts to provide answers to two
research questions arising out of this line of work:
First, is it possible to obtain a more precise mea-
sure of the relative informativeness of the differ-
ent social cues by adopting a more ﬁne-grained
model? Secondly, can we see any effects on in-
formativeness in this model if we offset the tim-
ing of the non-verbal cues? In other words, is the
timing actually used by the parents in some sense
optimal with respect to the synchrony of verbal
and non-verbal cues, or is the informativeness ro-
bust to (small) displacements of the cues forward
or backward in time? To measure the information
provided by social and attentional cues, we use a
supervised classiﬁcation method, and different as-
sumptions about the length of short-term memory.

2 Data

This section describes our corpus and the annota-
tion used to code the parents’ and children’s refer-
ential behaviour.

2.1 Corpus
Our primary data consist of audio and video
recordings (using two cameras) from parent–child
interaction in a recording studio at the Phonet-
ics Laboratory at Stockholm University (Lacerda,
2009). The corpus consists of 18 parent–child
dyads, totalling 7:29 hours, with three children
each participating longitudinally in six dyads be-
tween the ages of seven and 33 months. The mean
duration of a dyad is 24:58 minutes. The scenario
was free play where the set of toys varied over
time, but where two of them (the target objects)
were present in all dyads.

2.2 Coding
All annotation of the corpus was made with the
ELAN tool (Wittenburg et al., 2006) according to
the guideline of Bj¨orkenstam and Wir´en (2014),
producing annotation cells on tiers time-aligned

Figure 1: Screencap of ELAN annotation.

with the audio and video ﬁles (see Figure 1). The
basic approach was to code each type of verbal
and non-verbal referential event as well as the par-
ent and child in separate tiers, thereby allowing for
analysis separately and in different combinations.
First, for each dyad, the discourse segments in
which a target object was in focus were coded
by creating cells that spanned the corresponding
timelines in a designated tier, annotated with the
name of the focused object.1 “Focus” here means
that at least one of the participants’ attention was
directed to a target object,2 and that, in the course
of the segment, at least one verbal reference to the
object was made by the parent. Such a segment
was considered to end when the focus was shifted
permanently to another (target or non-target) ob-
ject.

These segments were then coded for verbal and
non-verbal referential cues, involving speech, eye
gaze, manual gesture, and manipulation of an ob-
ject by (one or two) hands. The coding used cells
spanning the timelines corresponding to the re-
spective events in a separate tier for each type, and
with separate tiers for the parent and child, thus re-

1In some segments, both of the target objects were in fo-

cus and were then annotated with both names.

2Thus, there is not necessarily joint attention to the target

object in the whole of such a segment.

sulting in eight ELAN tiers overall. We took care
in trying to recover information from each cue as
objectively as possible. Accordingly, an impor-
tant methodological consideration was that each
tier was coded independently of the others in such
a way that all the other tiers were hidden for the
annotator.

The coding of speech involved all references to
objects and persons present in the room by means
of a name, deﬁnite description or pronoun. Each
such reference was coded in an annotation cell
spanning the timeline corresponding to the dura-
tion of the expression, with addition of its ortho-
graphic transcription and the speaker’s intended
referent. There were altogether 45 types of ob-
jects referred to verbally in the videos, but the
distribution of these events was heavily skewed,
mostly because of the prominent role of the two
target objects. The most frequently referred ob-
jects are shown in the two leftmost columns of
Table 1. As seen in the table, only three objects
were referred to more than 30 times: target object
1 (called Siffu), target object 2 (Kucka) and
child.

As for non-verbal references, the coding of gaze
similarly consisted of a cell spanning the timeline
of the act, with a speciﬁcation of the object looked
at. If two objects were joined together in the ﬁeld

Table 1: Number of occurrences of ten most frequent objects referred to verbally, ditto hand manipula-
tions of objects, and ditto objects referred to non-verbally (using gaze or hand), all in decreasing order.
P = parent, C = child, Siffu = target object 1, Kucka = target object 2.

Objects referred Occur. Hand mani- Occur. Objects referred Occur.
to verbally
Siffu
Kucka
C
subS
subK
P
dress-white
bib
car
wire

to non-verbally
Siffu
Kucka
C
bag-lid
bag
P
dress-white
bottle
dress-pink
brush

pulation
hold
reach
move
show
touch
grab
pick-up
explore
enact
shake

1229
1103
184
173
146
66
61
55
42
36

377
275
166
29
24
22
14
11
11
8

797
539
321
262
217
165
143
120
114
95

Table 2: Values of Cohen’s Kappa (required over-
lap 0.6)

Annotation tier
Parent’s object manipulation
Child’s object manipulation
Parent’s eye gaze
Child’s eye gaze

Kappa
0.71
0.75
0.60
0.69

of view of an agent, the object looked at was coded
as the larger of them. For example, if the parent
was looking at the child holding a car, we would
code this as the child being the subject of the gaze.
In the coding of manual object manipulation,
we wanted to capture the large variation in how
the parent and child were handling the objects. We
thus distinguished 79 types of object manipulation
acts, which again turned out to occur in a skewed
distribution as shown in the two middle columns
in Table 1. Altogether, there were 85 different ob-
jects referred to non-verbally (using gaze or hand),
of which the most frequent ones are shown in the
two rightmost columns in Table 1. Manual gesture
occurred very infrequently (and only for the pur-
pose of deictic pointing), and was not used in the
subsequent analysis.

The use of timelines in ELAN allows for a high
temporal resolution, permitting us to track the in-
formation from the cues very precisely. The high
resolution also brings technical challenges, how-
ever; while Frank et al. (2012) could assume a
discrete-time setting and simply use a model pre-
dicting referents from all the events observed dur-

Table 3: Tuples extracted from coding of gaze. P =
parent, C = child, Siffu = target object 1, Kucka
= target object 2

Element
Values
Predicate gaze
Agent
Patient

P, C
Siffu, Kucka, C, bag-lid,
bag, P, . . .

ing an entire utterance, we need a continuous-time
model to fully exploit the information from our
coding.

The reliability of the coding scheme was evalu-
ated by comparing the output by two annotators on
two representative dyads, using the built-in ELAN
function for calculating Cohen’s Kappa (see Ta-
ble 2). Reliability was high for children’s eye gaze
as well as object manipulation by parent and child
(around 0.7), but slightly lower for parent eye gaze
(0.6).

3 Method

While the child has access to a vast amount of in-
formation from different senses (including touch,
taste, smell, etc.), as well as memories from be-
fore the recording session, the goal of our simu-
lated learner is to predict which object is being re-
ferred to given nothing but the information from
the different cues. We assume, however, that our
learner knows how to segment continuous speech
into utterances and words, that it can perceive and

Table 4: Tuples extracted from coding of hand ma-
nipulation of object. P = parent, C = child, Siffu
= target object 1, Kucka = target object 2

Values

Element
Predicate hold, reach, move, show, . . .
Agent
Patient

P, C
Siffu, Kucka, C, bag-lid,
bag, P, . . .

represent objects in the physical context, and that
it is sensitive to the interlocutor’s gaze. We fur-
thermore assume that the learner simulates the be-
ginnings of lexical acquisition in the sense that the
only information provided by the speech is that
some object in the context is being referred to ver-
bally, but nothing related to the meaning of the
words.

To provide a measure of the information inher-
ent in the cues, we use a supervised classiﬁcation
method. Following Frank et al. (2012), we thus
use classiﬁcation accuracy as a proxy for the vari-
able we are really interested in, namely, the in-
formativeness of different cues. Highly informa-
tive cues provide relatively unambiguous informa-
tion about the referent, and a reasonable classiﬁer
should then be able to identify the referent with a
high level of accuracy.

It would also be possible to use the perplexity
or, equivalently, likelihood of the test data in order
to compare different models. This would capture
the (un)certainty of each model, rather than just its
ability to predict the correct referent. While intu-
itively appealing, this would increase the inﬂuence
of uninteresting model parameters (such as regu-
larization strength) on the result, so for this reason
we stick to the more easily interpretable measure
of plain classiﬁcation accuracy.

As features for the classiﬁer, we extracted in-
formation from the coding which we represent as
tuples. Thus, for gaze, we extract triples con-
sisting of (cid:104)gaze, agent, patient(cid:105), as shown in Ta-
ble 3. For object manipulation we extracted triples
in the format (cid:104)predicate, agent, patient(cid:105), for exam-
ple, (cid:104)pick-up, C, car(cid:105). As mentioned in Sec-
tion 2.2, there were 79 different values for predi-
cate and 85 different values for patient; the most
frequent ones of these are shown in Table 4.3 We

3Sometimes one predicate was associated with several
patients, for example, (cid:104)gaze, C,(cid:104)car, Siffu(cid:105)(cid:105).
In this
case, two features were generated with the same timestamps:

also keep track of the timing information for each
mention and each gaze- or hand-related cue.

The particular task that our model solves is
a multinomial classiﬁcation between the possible
referents at time t, which we choose to coincide
with the start of a mention by the parent. For this,
we use a multinomial logistic regression (Maxi-
mum Entropy) model with predictors that depend
on the type of event as well as the time passed
since the event ﬁnished.
Each combination of values in a tuple that en-
codes a non-verbal event, such as (cid:104)gaze, P, car(cid:105)
or (cid:104)pick-up, C, car(cid:105), corresponds to a feature
in the model. To compute the value of this feature
at time t, we use an exponential decay function to
simulate short-term memory. The memory equa-
tion has the form f (t) = e−kt, where k is a con-
stant that determines the length (half-life) of the
memory, and t is deﬁned by

t = tstart

mention − tend

event

mention is the time at which the mention
where tstart
event is the time at which the non-verbal
starts and tend
event ends, or t = 0 in case these two overlap.
Ongoing non-verbal events are deﬁned to have a
value of 1, but as soon as the non-verbal event
ends, the decay begins.
In case the non-verbal
event and mention overlap, the event will have a
value of 1, according to the memory equation. Fu-
ture events (that is, events that have not yet oc-
curred) are deﬁned to have a value of 0.4

As mentioned in Section 2.2, the distributions
of predicates and objects ware skewed. To avoid
having a lot of unusual features in the model, we
therefore used one threshold for inclusion of ver-
bal mentions, which we set to 100, and one thresh-
old for the use by the classiﬁer of unique triples
representing object manipulations, which we set
to 10. The rationale for the lower threshold is that
the classiﬁer is robust to some noise, but only if
there is a sufﬁcient number of instances for the
predicting variable (verbal mentions), hence the
higher threshold in that case. Consequently, only
the three most frequently mentioned objects were
used in the classiﬁcation.

We train and evaluate the model using a leave-
one-out strategy on the recording session level, so
(cid:104)gaze, C, car(cid:105) and (cid:104)gaze, C, Siffu(cid:105).

4If we would like to put more emphasis on changes of
state, it is possible to include decay during an event as well to
down-weigh the information from this event once the novelty
wears off.

Table 5: Results of experiment 1. Accuracy (in
percent) of model prediction given type of cue.
Columns show from which agents information is
incorporated into the model (P = parent, C = child,
P + C = both). The upper half shows results from
our model as described, the lower half uses the
same data but only utterance-level binary features,
thus emulating the model of Frank et al. (2012).

C

P

Type of cue used
P + C
Fine-grained temporal information
Hand
82.5
84.2
Gaze
Hand + gaze
88.7
Utterance-level temporal information
Hand
66.6
Gaze
62.3
Hand + gaze
69.5

61.5
61.4
64.4

64.1
59.8
65.0

72.9
75.8
81.7

71.8
80.8
83.6

that we ﬁt as many models as there are recording
sessions (18). Each model is ﬁtted using data from
all but one session, then used to predict the refer-
ents of the remaining session. This method allows
us to use as much as possible of the available data,
while at the same time avoiding session-speciﬁc
context to inﬂuence the model.

4 Experiments

This section describes how we used our model in
three experiments to try to measure the informa-
tiveness and timing of non-verbal cues.

Experiment 1: Informativeness of non-verbal
cues
First, we were interested in obtaining measures of
the informativeness of the non-verbal cues from
both the parent and child as seen from a third-
person observer (in effect, looking at their joint
interaction), as well as from the agents as seen sep-
arately. To this end, we trained classiﬁers on cues
including gaze and hand manipulation for the in-
put from each agent as well as from both of them.
For this experiment, we used the two target ob-
jects as referents. We did not include the child, be-
cause the objective here was to use external infor-
mation sources as seen from the parent and child,
and we did not include any other objects for lack
of data. The half-life of the short-term memory
decay used here was 3 seconds. The baseline is
given by the most frequently referred one, target

Table 6: Results of experiment 2. Accuracy (in
percent) of model prediction per referent.

Precision Recall F -score

C
Kucka
Siffu

31.0
69.0
73.6

13.3
74.5
87.8

18.6
71.7
80.0

object 1 (Siffu), which was used in 58% of the
cases. An uninformed model could thus achieve
an accuracy of 58% by always predicting Siffu.
Table 5 shows the accuracy of the model’s pre-
dictions given different cue combinations and in-
formation sources (agents). Overall, the differ-
ences in predictive accuracy between the various
cue combinations are fairly small, but we can note
some things. First, gaze turns out to be more in-
formative than hand manipulation of objects. Sec-
ondly, a comparison of the P and C columns shows
that roughly the same amount of information is
provided by both agents, indicating a high degree
of convergence in their interaction.

For comparison, we also include at the end of
table 5 the corresponding accuracies obtained us-
ing the paradigm of Frank et al. (2012), that is, dis-
carding our ﬁne-grained temporal information and
using only utterance-level binary features. The re-
sult is a sharp decline in prediction accuracy. It is
noteworthy that gaze comes out as less informa-
tive than hand manipulation under these circum-
stances, which is consistent with the results re-
ported by Frank et al. The relative importance of
cues thus seems to depend strongly on the resolu-
tion of the temporal information available to the
model.

Finally, we can see that the prediction accu-
racy is higher when the information sources are
combined, as we would expect. The P + C col-
umn shows that the prediction accuracy of a third
person view classiﬁer (trained on both parent and
child input) is consistently higher than the accu-
racy of the classiﬁers trained on input from P and
C, respectively.

Experiment 2: Informativeness of non-verbal
cues to known referents
In the second experiment, we were interested in
determining if there were differences in informa-
tivess of non-verbal cues that depended on the
object referred to. This question may bear upon
problems related to givenness and accessibility in

the domain. In each dyad, the child is a second-
person referent, and the target objects are third-
person referents. For example, according to Ariel
(1999), second-person referents are consistently
highly accessible, whereas third-person referents
are highly accessible only when they constitute
the discourse topic. Our model thus permits us
to investigate whether there are differences in the
informativity of non-verbal cues with respect to
second- and third-person referents. Since the num-
ber of references to the child was exceeded only by
the target objects, we therefore included this as a
third object.

For this experiment, we thus trained classiﬁers
on cues including gaze and hand manipulation for
the input from both agents combined. Table 6
shows that predicting the child is much more difﬁ-
cult than the external (target) objects. Using gaze
and action information from both participants, we
achieve F -scores of 71.6% and 80.0% for the two
toys, but only 18.6% for the child.

Figure 2: Results of experiment 3. Classiﬁcation
accuracy (y-axis) as a function of verbal mention,
offset whole seconds from actual word occurrence
in parent speech up/down to ±4 seconds (x-axis),
given a short-term memory of 1, 3, and 10 sec-
onds, respectively. Time = 0 coincides with the
start of the mentions by the parent.

Experiment 3: Timing of non-verbal cues
Our ﬁnal experiment concerned the timing of non-
verbal cues. Previous research has highlighted the
time-synchronicity of non-verbal cues with ver-
bal utterances (Matatyaho and Gogate, 2008; Lac-
erda, 2009). Furthermore, there has been work in
the HSP paradigm on determining the effects to
referential transparency by displacing these cues
(Trueswell et al., 2016). Using our ﬁne-grained

representation of time, we wanted to investigate
the effects in our model to see if would arrive at
similar effects as Trueswell et al.

Our hypothesis was that non-verbal cues are
synchronised with speech, and that displacing the
verbal mention from its actual temporal position
in the input would lead to a drop in classiﬁer per-
formance. We tested this by training a classiﬁer on
input where the timing of the predictions relative
to the onset of speech had been moved by whole
seconds up/down to ±4 seconds. This is compa-
rable to displacing the speech relative to the non-
verbal event with the same amount of time. We
also explored how short-term memory decay inﬂu-
enced classiﬁcation accuracy by comparing three
classiﬁers with a memory half-life of 1, 3 and 10
seconds, respectively.

The effects of the timing displacement on ac-
curacy appear in Figure 2. The 0 second verbal
mention offset is the baseline, with an accuracy of
about 86% for the 1 second memory model, and
around 88% for the 3 and 10 second memory mod-
els. Accuracy dropped when verbal mention offset
was displaced. Moving the verbal mention offset
ahead in time by as little as two seconds resulted
in accuracy scores of 82% for the 1 second model,
and 84% for the 3 and 10 second memory mod-
els. Delaying the verbal mention by 2 seconds had
a less detrimental effect, in particular for the 10
second model.

5 Discussion

The goal of this study was to develop a model for
ﬁne-grained measuring of the informativeness and
effects of displaced timing of non-verbal cues in
parent–child interaction. To this end, we used a
corpus of videos of child-directed interaction in
a free-play setting involving several objects, but
where most of the interaction was centred on two
target objects. We coded the segments of the inter-
action that were focused on these objects with ver-
bal and non-verbal references, using speech, gaze
and hand manipulation of objects for this study.
To obtain a measure of the informativeness of dif-
ferent cues, we used classiﬁcation accuracy of the
different referents.

The main difference with respect to the model
of Frank et al. (2012) concerns the representation
of time. Frank et al. use a discrete-time setting in
which a referent is predicted from all the events
observed during an entire utterance. In contrast,

0.95

y
c
a
r
u
c
c
a
 
n
o
i
t
a
c
i
f
i
s
s
a
C

l

0.90

0.85

0.80

0.75

4000

3000

2000

1 second
3 seconds
10 seconds

2000

3000

4000

1000

0

1000

Time delay (ms)

our model uses a continuous-time representation
working off the coding along ELAN timelines. A
further difference is that our model includes a sim-
ulation of short-term memory decay, where the
value of a feature is 1 if it occurs at the time of
the mention (the noun phrase), and then decreases
exponentially.

Another kind of difference concerns the way in
which we represent non-verbal cues. Frank et al.
also investigated cues associated with speech, gaze
and hand, but for the latter they only used binary
features consisting of one discrete cue for hand po-
sition and hand pointing, respectively. Our cod-
ing is more feature-rich, distinguishing 79 types
of hand manipulation.

On the other hand, Frank et al. have a broader
perspective in the sense that they also model dis-
course continuity; in other words, the fact that in
the absence of contradicting information, it is most
likely that what is being talked about now is the
same thing as what was talked about a moment
ago. We also do not take prosody into account, as
is done by Yu and Ballard (2007).

Our ﬁrst experiment concerned the relative
informativeness of non-verbal cues for word-
referent mapping. We found that gaze is the most
informative cue, which is inconsistent with the
study of Frank et al. In particular, child gaze was
highly informative. We interpret this as evidence
of the parent’s ability to recognise the focus of the
child’s attention, and to create and maintain joint
attention. Additional support for our hypothesis is
given by the fact that non-verbal cues, and gaze
in particular, became much less informative when
we emulated Frank et al.’s experimental setup by
discarding temporal information for our classiﬁer.
The third person view classiﬁer, trained on both
parent and child input, achieved the highest accu-
racy. Although we do not have any direct coding
of joint attention, it seems that to some degree the
third person view classiﬁer captured instances of
joint attention through the coding of gaze and ob-
ject manipulation.

In our second experiment, we compared the in-
formativeness of non-verbal cues to mentions of a
second person referent (the child) with mentions
of third person referents (the target objects). We
found that this task is more complex than classiﬁ-
cation of mentions of third person referents. These
results raise the question whether non-verbal cues
are used less when the speaker assumes that the

referent of a word is known to the listener.
In
this case, the parent knows that the child already
knows his/her name, and thus references to the
child may be used mainly as means of getting the
attention of the child.

In our third experiment, we tested the hypoth-
esis that non-verbal cues are synchronous with
speech by displacing the verbal mention from its
temporal position in the input. We expected a
drop in classiﬁer performance, and found that es-
pecially negative offsets resulted in lower accu-
racy. We found an assymmetry in the effect of tim-
ing that is similar to experimental results on tim-
ing by Trueswell et al. (2016, p. 128), who note
that “the greatest changes in cues to referential in-
tent occur just before, rather than after, word onset
[. . . ]; moving the beep [that is, word onset] early
effectively causes these events to happen too late
to be perceived as causally related to the linguistic
event”.

6 Conclusions

Our ﬁndings show that gaze is the single most
important non-verbal cue for predicting external
object referents, thereby contradicting the study
of Frank et al. (2012). We attribute the differ-
ence to our addition of ﬁne-grained temporal in-
formation, as we can compare our results to those
of Frank et al. by simulating their time resolu-
tion. Another result is that that non-verbal cues
seem much more informative for predicting third-
person than second-person references. Finally, we
have demonstrated the importance of synchrony
by showing that displacing the verbal mention
in time degrades prediction accuracy, particularly
when the offset is negative. This is consistent with
the ﬁndings of Trueswell et al. (2016, Figure 2,
and compare our Figure 2) who instead of a sta-
tistical classiﬁer working off the annotation used
human observers of the video.

Acknowledgements

This research is part of the project “Modelling the
emergence of linguistic structures in early child-
hood”, funded by the Swedish Research Council
as 2011-675-86010-31. We would like to thank (in
chronological order) Anna Ericsson, Joel Peters-
son Ivre, Johan Sjons, Lisa Tengstrand, and An-
nika Schwittek for annotation work, and the three
anonymous reviewers for valuable comments.

J.C. Trueswell, Y. Lin, B. Armstrong III, E.A. Cart-
mill, S. Goldin-Meadow, and L.R. Gleitman. 2016.
Perceiving referential intent: Dynamics of refer-
ence in natural parent-child interactions. Cognition,
148:117–135.

P. Wittenburg, H. Brugman, A. Russel, A. Klassmann,
2006. ELAN: A Professional
and H. Sloetjes.
In Pro-
Framework for Multimodality Research.
ceedings of LREC 2006, Fifth International Con-
ference on Language Resources and Evaluation.
ELRA.

C. Yu and D.H. Ballard. 2007. A uniﬁed model of
early world learning: Integrating statistical and so-
cial cues. Neurocomputing, 70:2149–2165.

References
Mira Ariel. 1999. The development of person agree-
ment markers: From pronouns to higher accessibil-
ity markers. In M. Barlow and S. Kemmer, editors,
Usage-based Models of Language, pages 197–260.
Stanford, California: CSLI Publications.

K.N. Bj¨orkenstam and M. Wir´en. 2014. Multimodal
annotation of synchrony in longitudinal parent–child
interaction. In J. Edlund, D. Heylen, and P. Paggio,
editors, MMC 2014 Multimodal Corpora: Combin-
ing applied and basic research targets: Workshop
at The 9th edition of the Language Resources and
Evaluation Conference. ELRA.

M.C. Frank, N.D. Goodman, and J.B. Tenenbaum.
2009. Using speakers’ referential
intentions to
model early cross-situational word learning. Psy-
chological Science, 20(5):578–585.

M.C. Frank, J.B. Tenenbaum, and A. Fernald. 2012.
Social and discourse contributions to the determina-
tion of reference in cross-situational learning. Lan-
guage Learning and Development, pages 1–24.

Wilson S. Geisler. 2011. Contributions of ideal ob-
server theory to vision research. Vision Research,
51(7):771–781. Vision Research 50th Anniversary
Issue: Part 1.

J. Gillette, H. Gleitman, L. Gleitman, and A. Lederer.
1999. Human simulations of vocabulary learning.
Cognition, 73:135––176.

L.J. Gogate, L.H. Bolzani, and E.A. Betancourt. 2006.
Attention to maternal multimodal naming by 6- to
8-month-old infants and learning of word-object re-
lations. Infancy, 9:259–288.

Francisco Lacerda. 2009. On the emergence of early
linguistic functions: A biologic and interactional
perspective. In Brain Talk: Discourse with and in
the brain, number 1 in Birgit Rausing Language
Program Conference in Linguistics, pages 207–230.
Media-Tryck.

D.J. Matatyaho and L.J. Gogate. 2008. Type of mater-
nal object motion during synchronous naming pre-
dicts preverbal infants’ learning of word-object rela-
tions. Infancy, 13:172–184.

T.N. Medina, J. Snedeker, J.C. Trueswell, and L. Gleit-
man. 2011. How words can and cannot be learned
by observation. PNAS, 108(22):9014–9019.

T.B. Piccin and S.R. Waxman. 2007. Why nouns
trump verbs in word learning: New evidence
from children and adults in the human simulation
paradigm. Language Learning and Development,
3(4):295–323.

M. Tomasello. 2000. The social-pragmatic theory of

word learning. Pragmatics, 10(4):401–413.

