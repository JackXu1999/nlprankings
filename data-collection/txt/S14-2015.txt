



















































AUEB: Two Stage Sentiment Analysis of Social Network Messages


Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 114–118,
Dublin, Ireland, August 23-24, 2014.

AUEB: Two Stage Sentiment Analysis of Social Network Messages

Rafael Michael Karampatsis, John Pavlopoulos and Prodromos Malakasiotis
mpatsis13@gmail.com, annis@aueb.gr, rulller@aueb.gr

Department of Informatics
Athens University of Economics and Business

Patission 76, GR-104 34 Athens, Greece

Abstract

This paper describes the system submit-
ted for the Sentiment Analysis in Twitter
Task of SEMEVAL 2014 and specifically
the Message Polarity Classification sub-
task. We used a 2–stage pipeline approach
employing a linear SVM classifier at each
stage and several features including mor-
phological features, POS tags based fea-
tures and lexicon based features.

1 Introduction

Recently, Twitter has gained significant popularity
among the social network services. Lots of users
often use Twitter to express feelings or opinions
about a variety of subjects. Analysing this kind of
content can lead to useful information for fields,
such as personalized marketing or social profiling.
However such a task is not trivial, because the lan-
guage used in Twitter is often informal presenting
new challenges to text analysis.

In this paper we focus on sentiment analysis,
the field of study that analyzes people’s sentiment
and opinions from written language (Liu, 2012).
Given some text (e.g., tweet), sentiment analysis
systems return a sentiment label, which most often
is positive, negative, or neutral. This classification
can be performed directly or in two stages; in the
first stage the system examines whether the text
carries sentiment and in the second stage, the sys-
tem decides for the sentiment’s polarity (i.e., posi-
tive or negative).1 This decomposition is based on
the assumption that subjectivity detection and sen-
timent polarity detection are different problems.

This work is licensed under a Creative Commons At-
tribution 4.0 International Licence. Page numbers and pro-
ceedings footer are added by the organisers. Licence details:
http://creativecommons.org/licenses/by/4.0/

1For instance a 2–stage approach is better suited to sys-
tems that focus on subjectivity detection; e.g., aspect based
sentiment analysis systems which extract aspect terms only
from evaluative texts.

We choose to follow the 2–stage approach, be-
cause it allows us to focus on each of the two prob-
lems separately (e.g., features, tuning, etc.). In the
following we will describe the system with which
we participated in the Message Polarity Classi-
fication subtask of Sentiment Analysis in Twit-
ter (Task 9) of SEMEVAL 2014 (Rosenthal et al.,
2014). Specifically Section 2 describes the data
provided by the organizers of the task. Sections 3
and 4 present our system and its performance re-
spectively. Finally, Section 5 concludes and pro-
vides hints for future work.

2 Data

At first, we describe the data used for this year’s
task. For system tuning the organizers released the
training and development data of SEMEVAL 2013
Task 2 (Wilson et al., 2013). Both these sets are
allowed to be used for training. The organizers
also provided the test data of the same Task to be
used for development only. As argued in (Malaka-
siotis et al., 2013) these data suffer from class im-
balance. Concerning the test data, they contained
8987 messages broken down in the following 5
datasets:

– LJ14: 2000 sentences from LIVEJOURNAL.

– SMS13: SMS test data from last year.

– TW13: Twitter test data from last year.

– TW14: 2000 new tweets.

– TWSARC14: 100 tweets containing sarcasm.

The details of the test data were made available to
the participants only after the end of the Task. Re-
call that SMS13 and TW13 were also provided as
development data. In this way the organizers were
able to check, i) the progress of the systems since
last year’s task, and ii) the generalization capabil-
ity of the participating systems.

114



3 System Overview

The main objective of our system is to detect
whether a message M expresses positive, negative
or no sentiment. To achieve that we follow a 2–
stage approach. During the first stage we detect
whether M expresses sentiment (“subjective”) or
not; this process is called subjectivity detection.
In the second stage we classify the “subjective”
messages of the first stage as “positive” or “neg-
ative”. Both stages utilize a Support Vector Ma-
chine (SVM (Vapnik, 1998)) classifier with lin-
ear kernel.2 Similar approaches have also been
proposed in (Pang and Lee, 2004; Wilson et al.,
2005; Barbosa and Feng, 2010; Malakasiotis et al.,
2013). Finally, we note that the 2–stage approach,
in datasets such the one here (Malakasiotis et al.,
2013), alleviates the class imbalance problem.

3.1 Data preprocessing

A very essential part of our system is data pre-
processing. At first, each message M is passed
through a twitter specific tokenizer and part-of-
speech (POS) tagger (Owoputi et al., 2013) to ob-
tain the tokens and the corresponding POS tags,
which are necessary for some sets of features.3

We then use a dictionary to replace any slang with
the actual text.4 We also normalize the text of
each message by combining a trie data structure
(De La Briandais, 1959) with an English dictio-
nary.5 In more detail, we replace every token of M
not in the dictionary with the most similar word of
the dictionary. Finally, we obtain POS tags of all
the new tokens.

3.2 Sentiment lexicons

Another key attribute of our system is the use of
sentiment lexicons. We have used the following:

– HL (Hu and Liu, 2004).

– SENTIWORDNET (Baccianella et al., 2010).

– SENTIWORDNET lexicon with POS tags
(Baccianella et al., 2010).

– AFINN (Nielsen, 2011).

– MPQA (Wilson et al., 2005).
2We used the LIBLINEAR distribution (Fan et al., 2008)
3Tokens could be words, emoticons, hashtags, etc. No

lemmatization or stemming has been applied
4See http://www.noslang.com/dictionary/.
5We used the OPENOFFICE dictionary

– NRC Emotion lexicon (Mohammad and Tur-
ney, 2013).

– NRC S140 lexicon (Mohammad et al.,
2013).

– NRC Hashtag lexicon (Mohammad et al.,
2013).

– The three lexicons created from the training
data in (Malakasiotis et al., 2013).

Note that concerning the MPQA Lexicon we
applied preprocessing similar to Malakasiotis et al.
(2013) to obtain the following sub–lexicons:

S+ : Contains strong subjective expressions with
positive prior polarity.

S− : Contains strong subjective expressions with
negative prior polarity.

S± : Contains strong subjective expressions with
either positive or negative prior polarity.

S0 : Contains strong subjective expressions with
neutral prior polarity.

W+ : Contains weak subjective expressions with
positive prior polarity.

W− : Contains weak subjective expressions with
negative prior polarity.

W± : Contains weak subjective expressions with
either positive or negative prior polarity.

W0 : Contains weak subjective expressions with
neutral prior polarity.

3.3 Feature engineering

Our system employs several types of features
based on morphological attributes of the mes-
sages, POS tags, and lexicons of section 3.2.6

3.3.1 Morphological features
– The existence of elongated tokens (e.g.,

“baaad”).

– The number of elongated tokens.

– The existence of date references.

– The existence of time references.
6All the features are normalized to [−1, 1]

115



– The number of tokens that contain only upper
case letters.

– The number of tokens that contain both upper
and lower case letters.

– The number of tokens that start with an upper
case letter.

– The number of exclamation marks.

– The number of question marks.

– The sum of exclamation and question marks.

– The number of tokens containing only excla-
mation marks.

– The number of tokens containing only ques-
tion marks.

– The number of tokens containing only excla-
mation or question marks.

– The number of tokens containing only ellip-
sis (...).

– The existence of a subjective (i.e., positive or
negative) emoticon at the message’s end.

– The existence of an ellipsis and a link at the
message’s end.

– The existence of an exclamation mark at the
message’s end.

– The existence of a question mark at the mes-
sage’s end.

– The existence of a question or an exclamation
mark at the message’s end.

– The existence of slang.

3.3.2 POS based features
– The number of adjectives.

– The number of adverbs.

– The number of interjections.

– The number of verbs.

– The number of nouns.

– The number of proper nouns.

– The number of urls.

– The number of subjective emoticons.7

– The number of positive emoticons.8

– The number of negative emoticons.9

– The average, maximum and minimum F1
scores of the message’s POS bigrams for the
subjective and the neutral classes.10

– The average, maximum and minimum F1
scores of the message’s POS bigrams for the
positive and the negative classes.11

For a bigram b and a class c, F1 is calculated as:

F1(b, c) =
2 · Pre(b, c) · Rec(b, c)
Pre(b, c) + Rec(b, c)

(1)

where:

Pre(b, c) =
#messages of c containing b

#messages containing b
(2)

Rec(b, c) =
#messages of c containing b

#messages of c
(3)

3.3.3 Sentiment lexicon based features
For each lexicon we use seven different features
based on the scores provided by the lexicon for
each word present in the message.12

– Sum of scores.

– Maximum of scores.

– Minimum of scores.

– Average of scores.

– The count of words with scores.

– The score of the last word of the message that
appears in the lexicon.

– The score of the last word of the message.

7This feature is used only for subjectivity detection.
8This feature is used only for polarity detection.
9This feature is used only for polarity detection.

10This feature is used only for subjectivity detection.
11This feature is used only for polarity detection.
12If a word does not appear in the lexicon it is assigned

with a score of 0 and it is not considered in the calculation of
the average, maximum, minimum and count scores. Also, we
have removed from SENTIWORDNET any instances having
positive and negative scores that sum to zero. Moreover, the
MPQA lexicon does not provide scores, so, for each word in
the lexicon we assume a score equal to 1.

116



We also created features based on the Pre and
F1 scores of MPQA and the train data generated
lexicons in a similar manner to that described in
(Malakasiotis et al., 2013), with the difference that
the features are stage dependent. Thus, for subjec-
tivity detection we use the subjective and neutral
classes and for polarity detection we use the posi-
tive and negative classes to compute the scores.

3.3.4 Miscellaneous features

Negation. Negation not only is a good subjec-
tivity indicator but it also may change the
polarity of a message. We therefore add 7
more features, one indicating the existence
of negation, and the remaining six indicat-
ing the existence of negation that precedes
words from lexicons S±, S+, S−, W±, W+
and W−.13 Each feature is used in the appro-
priate stage.14 We have not implement this
type of feature for other lexicons but it might
be a good addition to the system.

Carnegie Mellon University’s Twitter clusters.
Owoputi et al. (2013) released a dataset of
938 clusters containing words coming from
tweets. Words of the same clusters share
similar attributes. We try to exploit this
observation by adding 938 features, each of
which indicates if a message’s token appears
or not in the corresponding attributes.

3.4 Feature Selection

To allow our model to better scale on unseen data
we have performed feature selection. More specif-
ically, we first merged training and development
data of SEMEVAL 2013 Task 2. Then, we ranked
the features with respect to their information gain
(Quinlan, 1986) on this dataset. To obtain the best
set of features we started with a set containing the
top 50 features and we kept adding batches of 50
features until we have added all of them. At each
step we evaluated the corresponding feature set on
the TW13 and SMS13 datasets and chose the fea-
ture set with the best performance. This resulted in
a system which used the top 900 features for Stage
1 and the top 1150 features for Stage 2.

13We use a list of words with negation. We assume that a
token precedes a word if it is in a distance of at most 5 tokens.

14The features concerning S± and W± are used in subjec-
tivity detection and the remaining four in polarity detection.

Test Set AUEB Median Best
LJ14 70.75 65.48 74.84
SMS13 64.32 57.53 70.28
TW13 63.92 62.88 72.12
TW14 66.38 63.03 70.96
TWSARC14 56.16 45.77 58.16
AVGall 64.31 56.56 68.78
AVG14 64.43 57.97 67.62

Table 1: F1(±) scores per dataset.

Test Set Ranking
LJ14 9/50
SMS13 8/50
TW13 21/50
TW14 14/50
TWSARC14 4/50
AVGall 6/50
AVG14 5/50

Table 2: Rankings of our system.

4 Experimental Results

The official measure of the Task is the average F1
score of the positive and negative classes (F1(±)).
Table 1 illustrates the F1(±) score per evaluation
dataset achieved by our system along with the me-
dian and best F1(±). In the same table AVGall
corresponds to the average F1(±) across the five
datasets while AVG14 corresponds to the average
F1(±) across LJ14, TW14 and TWSARC14. We
observe that in all cases our results are above the
median. Table 2 illustrates the ranking of our sys-
tem according to F1(±). Our system ranked 6th
according to AVGall and 5th according to AVG14
among the 50 participating systems. Note that our
best results were achieved on the new test sets
(LJ14, TW14, TWSARC14) meaning that our sys-
tem has a good generalization ability.

5 Conclusion and future work

In this paper we presented our approach for the
Message Polarity Classification subtask of the
Sentiment Analysis in Twitter Task of SEMEVAL
2014. We proposed a 2–stage pipeline approach,
which first detects sentiment and then decides
about its polarity. The results indicate that our sys-
tem handles well the class imbalance problem and
has a good generalization ability. A possible ex-
planation is that we do not use bag-of-words fea-

117



tures which often suffer from over–fitting. Never-
theless, there is still some room for improvement.
A promising direction would be to improve the
1st stage (subjectivity detection) either by adding
more data or by adding more features, mostly be-
cause the performance of stage 1 greatly affects
that of stage 2. Finally, the addition of more data
for the negative class on stage 2 might be a good
improvement because it would further reduce the
class imbalance of the training data for this stage.

References
Stefano Baccianella, Andrea Esuli, and Fabrizio Se-

bastiani. 2010. Sentiwordnet 3.0: An enhanced
lexical resource for sentiment analysis and opin-
ion mining. In Nicoletta Calzolari (Conference
Chair), Khalid Choukri, Bente Maegaard, Joseph
Mariani, Jan Odijk, Stelios Piperidis, Mike Ros-
ner, and Daniel Tapias, editors, Proceedings of the
Seventh International Conference on Language Re-
sources and Evaluation (LREC’10), Valletta, Malta,
may.

Luciano Barbosa and Junlan Feng. 2010. Robust sen-
timent detection on twitter from biased and noisy
data. In Proceedings of the 23rd International
Conference on Computational Linguistics: Posters,
COLING ’10, pages 36–44, Beijing, China.

Rene De La Briandais. 1959. File searching using
variable length keys. In Papers Presented at the the
March 3-5, 1959, Western Joint Computer Confer-
ence, IRE-AIEE-ACM ’59 (Western), pages 295–
298, New York, NY, USA.

Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-
Rui Wang, and Chih-Jen Lin. 2008. Liblinear: A
library for large linear classification. The Journal of
Machine Learning Research, 9:1871–1874.

Minqing Hu and Bing Liu. 2004. Mining and summa-
rizing customer reviews. In Proceedings of the Tenth
ACM SIGKDD International Conference on Knowl-
edge Discovery and Data Mining, KDD ’04, pages
168–177, New York, NY, USA.

Bing Liu. 2012. Sentiment analysis and opinion min-
ing. Synthesis Lectures on Human Language Tech-
nologies, 5(1):1–167.

Prodromos Malakasiotis, Rafael Michael Karampat-
sis, Konstantina Makrynioti, and John Pavlopoulos.
2013. nlp.cs.aueb.gr: Two stage sentiment analysis.
In Second Joint Conference on Lexical and Com-
putational Semantics (*SEM), Volume 2: Proceed-
ings of the Seventh International Workshop on Se-
mantic Evaluation (SemEval 2013), pages 562–567,
Atlanta, Georgia, June.

Saif Mohammad and Peter Turney. 2013. Crowdsourc-
ing a word-emotion association lexicon. 29(3):436–
465.

Saif Mohammad, Svetlana Kiritchenko, and Xiaodan
Zhu. 2013. Nrc-canada: Building the state-of-the-
art in sentiment analysis of tweets. In Proceedings
of the Seventh International Workshop on Semantic
Evaluation (SemEval 2013), Atlanta, Georgia, USA,
June.

Finn Årup Nielsen. 2011. A new anew: evaluation of
a word list for sentiment analysis in microblogs. In
Matthew Rowe, Milan Stankovic, Aba-Sah Dadzie,
and Mariann Hardey, editors, Proceedings of the
ESWC2011 Workshop on ’Making Sense of Micro-
posts’: Big things come in small packages, volume
718 of CEUR Workshop Proceedings, pages 93–98,
May.

Olutobi Owoputi, Brendan OConnor, Chris Dyer,
Kevin Gimpel, Nathan Schneider, and Noah A
Smith. 2013. Improved part-of-speech tagging for
online conversational text with word clusters. In
Proceedings of NAACL.

Bo Pang and Lillian Lee. 2004. A sentimental educa-
tion: sentiment analysis using subjectivity summa-
rization based on minimum cuts. In Proceedings of
the 42nd Annual Meeting on Association for Com-
putational Linguistics, ACL ’04, Barcelona, Spain.

Ross Quinlan. 1986. Induction of decision trees.
Mach. Learn., 1(1):81–106, March.

Sara Rosenthal, Preslav Nakov, Alan Ritter, and
Veselin Stoyanov. 2014. SemEval-2014 Task 9:
Sentiment Analysis in Twitter. In Preslav Nakov and
Torsten Zesch, editors, Proceedings of the 8th In-
ternational Workshop on Semantic Evaluation, Se-
mEval ’14, Dublin, Ireland.

Vladimir Vapnik. 1998. Statistical learning theory.

Theresa Wilson, Janyce Wiebe, and Paul Hoffmann.
2005. Recognizing contextual polarity in phrase-
level sentiment analysis. In Proceedings of the con-
ference on Human Language Technology and Em-
pirical Methods in Natural Language Processing,
pages 347–354.

Theresa Wilson, Zornitsa Kozareva, Preslav Nakov,
Sara Rosenthal, Veselin Stoyanov, and Alan Ritter.
2013. SemEval-2013 task 2: Sentiment analysis in
twitter. In Proceedings of the International Work-
shop on Semantic Evaluation, SemEval ’13, June.

118


