



















































A Practical Perspective on Latent Structured Prediction for Coreference Resolution


Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 2, Short Papers, pages 143–149,
Valencia, Spain, April 3-7, 2017. c©2017 Association for Computational Linguistics

A Practical Perspective on Latent Structured Prediction for
Coreference Resolution

Iryna Haponchyk∗ and Alessandro Moschitti

∗DISI, University of Trento, 38123 Povo (TN), Italy
Qatar Computing Research Institute, HBKU, 34110, Doha, Qatar
{gaponchik.irina,amoschitti}@gmail.com

Abstract

Latent structured prediction theory pro-
poses powerful methods such as Latent
Structural SVM (LSSVM), which can po-
tentially be very appealing for coreference
resolution (CR). In contrast, only small
work is available, mainly targeting the la-
tent structured perceptron (LSP). In this
paper, we carried out a practical study
comparing for the first time online learn-
ing with LSSVM. We analyze the intrica-
cies that may have made initial attempts
to use LSSVM fail, i.e., a huge training
time and much lower accuracy produced
by Kruskal’s spanning tree algorithm. In
this respect, we also propose a new effec-
tive feature selection approach for improv-
ing system efficiency. The results show
that LSP, if correctly parameterized, pro-
duces the same performance as LSSVM,
being at the same time much more effi-
cient.

1 Introduction

Recent research on CR has shown effective ap-
plications of structured prediction, e.g., the latent
structured perceptron (LSP) by Fernandes et al.
(2014) obtained the top rank in the CoNLL-2012
Shared Task (Pradhan et al., 2012). There has been
an exploration of LSP variants (Chang et al., 2011;
Björkelund and Kuhn, 2014; Lassalle and De-
nis, 2015), and also of SGD-like methods (Chang
et al., 2013; Peng et al., 2015; Kummerfeld et al.,
2015). Surprisingly, no study was devoted to
LSSVM by Yu and Joachims (2009), which of-
fers theoretical guarantees on reducing the error
upper-bound. The major advantage of such a the-
ory is the possibility to stop the optimization pro-
cess, carried out using the Concave-Convex Pro-
cedure (CCCP) by Yuille and Rangarajan (2003),

when the approximation to the optimum is close
as much as we want. In contrast, the gradient de-
scent operated by perceptron-like algorithms does
not allow us to estimate how much our solution is
far away from the optimum. In other words, we
do not know at which epoch our algorithm should
stop. Thus, LSSVM holds an important advantage
over online methods.

In this paper, we empirically compare LSSVM
with two online learning algorithms, LSP and
LSPA (a structured passive-aggressive (PA) algo-
rithm (Crammer et al., 2006) that we extended
with latent variables) using the exact setting of the
CoNLL-2012 dataset. This preserves comparabil-
ity with the work in CR. For example, we use the
latest version of the MELA scorer1.

It should be noted that implementing a sound
comparison was rather complex as it required test-
ing all the algorithms in the same conditions and
optimally setting their parameters. In particu-
lar, LSSVM and LSP adopt different graph mod-
els and use different methods to extract spanning
trees from a document graph, namely, Kruskal’s
(Kruskal, 1956) and Edmonds’ (Chu and Liu,
1965; Edmonds, 1967). Although both extract op-
timal spanning trees, they provide different solu-
tions, which critically impact on accuracy and ef-
ficiency. The latter is problematic as LSSVM re-
quires too long time for convergence on the large
CoNLL dataset.

To tackle this issue, we applied two kinds of ef-
ficiency boost: feature and mention pair selection.
Feature selection was rather challenging as the CR
feature space is different from a standard text cat-
egorization setting. We could not apply a filtering
threshold on simple and effective statistics such as
document frequency since almost all the features
appear in many documents. For solving this prob-
lem, we explored the use of efficient binary SVMs
for computing feature weights, which we used for

1conll.cemantix.org/2012/software.html

143



our selection. Additionally, we also provided a
parallelized version of LSSVM to afford the com-
putation requirement of the full CoNLL dataset.

The results of our study show that LSSVM can
be trained on large data and achieve the state of
the art of online methods. However, the latter us-
ing optimal parameters can even surpass its accu-
racy and outperform the current state of the art of
LSP by 2 points. Finally, our feature selection al-
gorithm is rather efficient and effective.

2 Related Work

The first work of structured prediction for CR is
an SVMcluster approach by Finley and Joachims
(2005), who couple the structural SVM (Tsochan-
taridis et al., 2004) with approximate clustering
inference. They maximize the clustering objec-
tive by either (i) a simple greedy approach or (ii) a
relaxation of the correlation clustering technique.
Both methods resulted computationally very ex-
pensive. To overcome such inefficiency, Yu and
Joachims (2009) proposed LSSVM performing in-
ference on undirected (latent) graphs built on doc-
ument mentions using Kruskal’s spanning algo-
rithm.

Fernandes et al. (2014) specialized the la-
tent structured perceptron proposed by Sun et al.
(2009) for solving CR tasks (LSP). This is based
on (i) the Minimum Spanning Tree algorithm on
the directed mention graph and (ii) the structured
perceptron, updated on a per-document basis.

The same approach, referred to as antecedent
trees, is included in the generalized latent structure
framework of Martschat and Strube (2015). The
authors report that the mention-ranking approach,
which uses the LSP inference and mention-based
updates2, produces slightly better results.

It should be noted that the LSP inference is
equivalent to the best-left-link inference of Chang
et al. (2013), who coupled it with SGD updates
on a per-mention basis. Chang et al. (2011, 2012,
2013); Peng et al. (2015) reformulated the best-
left-link in terms of Integer Linear Programming
inference.

Björkelund and Kuhn (2014) experimented with
updates both on a per-mention and document basis
to enable inference with non-local features. Las-
salle and Denis (2015) experimented with a sim-
ilar inference procedure by also jointly modeling

2A perceptron update is performed after selecting the best
antecedent for a mention.

Model Parameters
LSSVMK C = 100.0 r = 0.5
LSSVME C = 100.0 r = 1.0
LSPK C = 1000.0 r = 0.1
LSPE C = 1000.0 r = 1.0

Table 1: Best parameter combinations.

anaphoricity and mention coreference.

In summary, although many models have been
tested, LSSVM has never been trained on a re-
alistic CR dataset. Chang et al. (2013) tested it
on the CoNLL-2012 dataset but they could not
use CCCP, exactly for efficiency reasons, and thus
they applied an SGD approach.

2.1 Algorithm Equivalence

LSSVM, LSP, LSPA can reach the same accuracy
subject to different convergence rates and bounds.
Indeed, LSSVM solves an optimization problem
using a CCCP iteration, the cost of the latter is
nearly a cost of one SVMstruct problem, which in
turn is polynomial.

LSP and LSPA require linear times, however, in
contrast to LSSVM, they do not have stopping cri-
teria - the number of epochs T has to be set. The
CCCP procedure is guaranteed to converge to a lo-
cal minimum or a saddle point. LSP and LSPA,
in essence, perform an update, which is equiva-
lent, up to some constant, to an SGD update of the
LSSVM objective, with a gradient taken w.r.t. a
document variable.

They can approach the local minimum as close
as possible, which is supported by our experi-
ments, reflecting the results compatible among the
three algorithms. For LSP and LSPA though, we
do not know a priori when to stop training. While,
for LSPA, there are error bounds derived by Cram-
mer et al. (2006), there are no bounds for LSP at
all.

However, for CR, as it can be seen from our
experiments, values of T for LSP and LSPA
can be reliably selected on a validation set for
a fixed training data size and a choice of fea-
tures/instances. Since the algorithms optimize a
surrogate objective, it is often the case that accu-
rately tuned LSP and LSPA result in higher perfor-
mance than LSSVM, not mentioning an excessive
complexity of the latter.

144



0 10 20 30 40 50

52

54

56

58

60

number of epochs, T

M
E

L
A

LSPE LSPAE LSPO (T = 5)

LSPK LSPAK

Figure 1: LSP learning curves, with 100 random
documents used for training (all the features, all
the edges), tested on all the dev. documents.

3 Experiments

3.1 Setup

Data We performed our experiments on the En-
glish part of the corpus from CoNLL 2012-Shared
Task3, containing 2,802, 343 and 348 documents
for training, development and test sets, respec-
tively.

Evaluation measure We report our coreference
results in terms of the MELA score (Pradhan et al.,
2012) computed using the version 8 of the official
CoNLL scorer.

Models and software As baselines, we used
(i) the original implementation of the Latent
SVMstruct 4 (denoted as LSSVMK) performing
inference on undirected graphs using Kruskal’s
spanning algorithm, (ii) LSPE – our implemen-
tation of the LSP algorithm with a tree mod-
eling of Fernandes et al. (2014) and Edmonds’
spanning tree algorithm, (iii) cort – coreference
toolkit by Martschat and Strube (2015), precisely
its antecedent tree approach, encoding, as well as
LSPE , the modeling of Fernandes et al. (denoted
as LSPO, where ”O” stands for Original).

In LSPE , the candidate graph, by construction,
does not contain cycles, and the inference by Ed-
monds’ algorithm is reduced to selecting for each
node an incoming edge with a maximum weight,
in other words, the best antecedent or no an-
tecedent for each mention. Thus, the difference
between our LSPE and cort is only due to a differ-
ent implementation.

3conll.cemantix.org/2012/data.html
4www.cs.cornell.edu/˜cnyu/latentssvm/

Model Dev. Test Tbest Time, h
LSSVMK 61.03 59.89 – 1164.09
LSSVME 62.91 61.88 – 210.01
LSPK 61.08 60.00 10 27.77
LSPE 64.01 63.04 43 32.55
LSPAK 61.15 60.16 6 47.73
LSPAE 64.14 62.81 8 37.33
LSPO 62.92 62.00 5 –
∗LSPO 62.31 61.24 5 5 –

Table 2: Main results for the systems evaluated on
CoNLL-2012 English development and test sets,
using all the training documents for training. Tbest
is evaluated on the development set and used on
the test set. ∗LSPO is the result published in
Martschat and Strube (2015).

Along with the baselines, we consider the fol-
lowing models: (i) LSSVME , i.e., LSSVM with
the latent trees and Edmonds’, (ii) LSPK , i.e., LSP
using Kruskal’s on undirected graphs, and (iii) two
structured versions of the PA online learning algo-
rithms, LSPAE and LSPAK .

We employed the cort toolkit both to preprocess
the CoNLL data and to extract candidate mentions
and features (the basic cort feature set).

As emphasized by Fernandes et al., averaging
the perceptron weights renders the learning curve
rather smooth. We applied weight averaging in all
the LSP and LSPA variants.

Parametrization All the models require tuning
of a regularization parameter C and of a specific
loss parameter r. In LSSVMK and LSPK , r is a
penalty for adding an incorrect edge; in LSSVME

and LSPE , r is a penalty for selecting an incor-
rect root arc. We selected the parameters on the
entire development set by training on 100 random
documents from the training set. We picked a C
from {1.0, 100.0, 1000.0, 2000.0}, the r values for
LSSVMK and LSPK from {0.05, 0.1, 0.5}, and
the r values for LSSVME and LSPE from the in-
terval [0.5, 2.5] with step 0.5. The values reported
in Table 1 were used for all our experiments.

3.2 Selecting the epoch number

A standard previous work setting for the number
of epochs T of the online learning algorithms is
5 (Martschat and Strube, 2015). Fernandes et al.

5This result is obtained using a concatenation of the train-
ing and the development set.

145



104 105 106 107

5

10

15

20

25

30

35

N

H
ou

rs
training time, T = 50

104 105 106 107

30

40

50

N

#
of

ep
oc

hs

Tbest on development

104 105 106 107

45

50

55

60

65

N

M
E

L
A

performance on development

104 105 106 107

45

50

55

60

65

N

M
E

L
A

performance on test

Tbest on development T = 5 T = 50

Figure 2: LSPE training time and accuracy with
respect to the number of features N , selected ac-
cording to the binary classifier weights.

(2014) noted that T = 50 was sufficient for con-
vergence. Figure 1 shows that setting T is cru-
cial for achieving a high accuracy. We also note
that the dataset size and the selected sets of fea-
tures and/or instances highly affect the best epoch
number, thus, for each particular experiment, we
selected the best T from 1 to 50 on the dev. set.

3.3 Model Comparison

Table 2 reports the results of the models trained on
the entire training set, and the numbers of epochs
Tbest for LSP and LSPA, tuned on the develop-
ment set. LSPO denotes the result of our run of the
original cort software. We note that (i) LSP and
LSPA perform on a par in both the settings; (ii) the
latent trees used with Edmonds’ algorithm outper-
form the undirected graphs used with Kruskal’s;
(iii) LSSVME is around one point less than LSPE

and LSPAE ; (iv) the training time of LSSVME is
one order of magnitude longer than that of LSPE ;
and (v) LSSVMK took more than 1.5 months to
converge.

3.4 Feature Selection

The number of distinct features extracted from
cort and used for training in the above experiments
is around 16.8 millions. Training systems with
such a large model size is nearly prohibitive, this
especially concerns SVMs, which may require a
substantial number of iterations for convergence.

8 10 15 20 all

5

10

15

20

25

30

35

d

H
ou

rs

training time, T = 50

8 10 15 20 all
20

30

40

d

#
of

ep
oc

hs

Tbest on development

8 10 15 20 all

60

61

62

63

64

d

M
E

L
A

performance on development

8 10 15 20 all

60

61

62

63

d

M
E

L
A

performance on test

Tbest on development T = 5 T = 50

Figure 3: LSPE training time and accuracy with
respect to d (max number of candidate antecedent
edges for each mention).

We tried to filter out less relevant features re-
moving those that appear in a fewer number of
documents but these were too few, e.g., less than
1% of all features have document frequency ≤ 3.

Thus, we proposed a feature selection technique
consisting in (i) training a binary classification
model, ~w, on all mention-pair feature vectors and
(ii) removing features with lower absolute weights
in ~w. Figure 2 plots the accuracy of CR models,
using different numbers of features selected as de-
scribed above. Interestingly, only retaining 5% of
the features (N = 106) results in a small loss.

3.5 Candidate edge selection

Using all the candidate edges in the CR graph is
another cause of computational burden, which is
overcome by the best CR systems by exploiting
heuristic linguistic filters.

In cort, filtering is not implemented and all the
candidate edges are used for training. We simply
adopted one of the filters, the so-called sieves, of
Fernandes et al. (2014) to reduce the number of
candidate links. Such a sieve retains links between
two mentions only if their distance is lower than
or equal to d, i.e., we consider only links (mi, mj)
with |j − i| ≤ d. Fernandes et al. use d = 8.

Figure 3 shows that, although the training time
is reduced considerably, the accuracy suffers. In
our experiments, we used d = 20, which causes
a loss smaller than 0.5 in MELA. It should be

146



noted that we also had to enable the LSSVM
implementation to operate on non-complete can-
didate graphs as it was originally designed for
making inference on fully-connected graphs only
(Haponchyk and Moschitti, 2014).

3.6 Results on Filtered Data
Table 3 reports the results using filtering corre-
sponding to the setting N = 106, d = 20. We note
that (i) the training time is reduced by more than
10 times; (ii) LSSVMK is outperformed by LSPK

(2 points) and performs worse than LSSVME ; (iii)
LSPAK seems to generalize better on filtered data
than LSPK ; and (iv) w.r.t. no filtering, LSSVME

faces a lower drop in performance than LSPE

does, approaching nearer to the latter.

3.7 Discussion
The results of our study are the following:

(i) for the first time, we show that LSSVM
can be applied to a realistic CR dataset and
achieve the same state of the art of the online
methods;

(ii) although the optimum found by CCCP pro-
duces better results than online learning algo-
rithms, the latter, when parameterized, pro-
vide similar accuracy, while at the same time
being much more efficient;

(iii) in this respect, we studied the optimal model
parameterization and found that LSP can be
highly improved, almost 2 points (63.04 vs.
61.24) over the previous best LSP result, by
accurately selecting the number of epochs on
a validation set;

(iv) the results of all the approaches using
an undirected graph model coupled with
Kruskal’s are 3 − 7 absolute percent points
lower than their results obtained with a di-
rected tree model coupled with Edmonds’.
Our outcome is supported by Chang et al.
(2013) who employed a fast SGD approach
with the best-left-link inference, which is
equivalent to Edmonds’ algorithm applied to
the directed latent trees. They compared the
previous inference approach with the span-
ning graph algorithm by Kruskal on undi-
rected graphs. They explain that the better
accuracy of the first method is due to the fact
that the latent tree structure considers the or-
der of the mentions in the document. Apart

Model Dev. Test Tbest Time, h
LSSVMK 56.16 54.50 – 23.06
LSSVME 62.82 61.75 – 24.09
LSPK 57.98 56.81 6 1.82
LSPE 63.11 61.98 49 1.62
LSPAK 58.69 57.38 3 3.50
LSPAE 63.28 62.11 6 1.98

Table 3: Main results for the systems evaluated on
CoNLL-2012 English development and test sets,
using all training documents with filtered features
(N=106) and edges (d=20).

from that, by using an artificial root, it implic-
itly models the cluster initial elements (i.e.,
discourse-new mentions).

(v) The use of direct trees in Edmonds’ method
delivers comparable results among all the al-
gorithms; and

(vi) our new approach to feature selection based
on binary SVMs turned out to be efficient and
effective and, together with mention pair in-
stance filtering, sped up training by 88% only
losing 0.15 of a point in accuracy.

4 Conclusions

This work provides a comparative analysis of on-
line and batch methods for structured prediction in
CR. Although LSSVM can reliably select a stop-
ping point of its learning, LSP and LSPA, when
well parameterized, can achieve the same accu-
racy. This empirically demonstrates that all these
methods, inherently optimizing the same objec-
tive, are able to achieve the same optimum.

Additionally, we show a very positive impact of
our new feature selection method for CR, based on
a pairwise classifier, which we can efficiently train
thanks to linear SVMs.

Finally, we also demonstrate that a noticeable
benefit to all online methods comes from accu-
rately parameterizing the epoch number. The lat-
ter is rather stable between development and test
sets but must be parametrized when using differ-
ent training data, feature or instance sets.

Acknowledgements

This work has been supported by the EC project
CogNet, 671625 (H2020-ICT-2014-2, Research
and Innovation action). Many thanks to the anony-
mous reviewers for their valuable suggestions.

147



References

Anders Björkelund and Jonas Kuhn. 2014. Learn-
ing structured perceptrons for coreference reso-
lution with latent antecedents and non-local fea-
tures. In Proceedings of the 52nd Annual Meet-
ing of the Association for Computational Lin-
guistics (Volume 1: Long Papers). Association
for Computational Linguistics, pages 47–57.

Kai-Wei Chang, Rajhans Samdani, and Dan Roth.
2013. A constrained latent variable model
for coreference resolution. In Proceedings of
the 2013 Conference on Empirical Methods in
Natural Language Processing. Association for
Computational Linguistics, pages 601–612.

Kai-Wei Chang, Rajhans Samdani, Alla Ro-
zovskaya, Nick Rizzolo, Mark Sammons, and
Dan Roth. 2011. Inference Protocols for Coref-
erence Resolution, Association for Computa-
tional Linguistics, chapter Proceedings of the
Fifteenth Conference on Computational Natural
Language Learning: Shared Task, pages 40–44.

Kai-Wei Chang, Rajhans Samdani, Alla Ro-
zovskaya, Mark Sammons, and Dan Roth.
2012. Joint Conference on EMNLP and CoNLL
- Shared Task, Association for Computational
Linguistics, chapter Illinois-Coref: The UI Sys-
tem in the CoNLL-2012 Shared Task, pages
113–117.

Y. J. Chu and T. H. Liu. 1965. On the shortest
arborescence of a directed graph. Science Sinica
14:1396–1400.

Koby Crammer, Ofer Dekel, Joseph Keshet, Shai
Shalev-Shwartz, and Yoram Singer. 2006. On-
line passive-aggressive algorithms. Journal of
Machine Learning Research 7:551–585.

Jack Edmonds. 1967. Optimum branchings. Jour-
nal of research of National Bureau of standards
pages 233–240.

Rezende Eraldo Fernandes, Nogueira Cı́cero dos
Santos, and Luiz Ruy Milidiú. 2014. Latent
trees for coreference resolution. Computational
Linguistics 40(4):801–835.

Thomas Finley and Thorsten Joachims. 2005. Su-
pervised clustering with support vector ma-
chines. In ICML ’05: Proceedings of the 22nd
international conference on Machine learning.
ACM, New York, NY, USA, pages 217–224.

Iryna Haponchyk and Alessandro Moschitti. 2014.
Making Latent SVMstruct practical for coref-

erence resolution. In Proceedings of the First
Italian Conference on Computational Linguis-
tics (CLiC-it 2014) & the Fourth International
Workshop EVALITA 2014. Pisa, Italy, pages
203–207.

Joseph Bernard Kruskal. 1956. On the Shortest
Spanning Subtree of a Graph and the Traveling
Salesman Problem. In Proceedings of the Amer-
ican Mathematical Society, 7.

K. Jonathan Kummerfeld, Taylor Berg-
Kirkpatrick, and Dan Klein. 2015. An empirical
analysis of optimization for max-margin nlp. In
Proceedings of the 2015 Conference on Empir-
ical Methods in Natural Language Processing.
Association for Computational Linguistics,
pages 273–279.

Emmanuel Lassalle and Pascal Denis. 2015. Joint
anaphoricity detection and coreference reso-
lution with constrained latent structures. In
Proceedings of the Twenty-Ninth AAAI Con-
ference on Artificial Intelligence. AAAI Press,
AAAI’15, pages 2274–2280.

Sebastian Martschat and Michael Strube. 2015.
Latent structures for coreference resolution.
Transactions of the Association of Computa-
tional Linguistics 3:405–418.

Haoruo Peng, Kai-Wei Chang, and Dan Roth.
2015. A joint framework for coreference reso-
lution and mention head detection. In Proceed-
ings of the Nineteenth Conference on Computa-
tional Natural Language Learning. Association
for Computational Linguistics, pages 12–21.

Sameer Pradhan, Alessandro Moschitti, Nianwen
Xue, Olga Uryupina, and Yuchen Zhang. 2012.
Joint Conference on EMNLP and CoNLL -
Shared Task, Association for Computational
Linguistics, chapter CoNLL-2012 Shared Task:
Modeling Multilingual Unrestricted Corefer-
ence in OntoNotes, pages 1–40.

Xu Sun, Takuya Matsuzaki, Daisuke Okanohara,
and Jun’ichi Tsujii. 2009. Latent variable per-
ceptron algorithm for structured classification.
In Proceedings of the 21st International Jont
Conference on Artifical Intelligence. Morgan
Kaufmann Publishers Inc., San Francisco, CA,
USA, IJCAI’09, pages 1236–1242.

Ioannis Tsochantaridis, Thomas Hofmann,
Thorsten Joachims, and Yasemin Altun. 2004.
Support vector machine learning for inter-
dependent and structured output spaces. In

148



Proceedings of the Twenty-first International
Conference on Machine Learning. ACM, New
York, NY, USA, ICML ’04, pages 104–.

Chun-Nam John Yu and Thorsten Joachims. 2009.
Learning structural svms with latent variables.
In Proceedings of the 26th Annual International
Conference on Machine Learning. ACM, New
York, NY, USA, ICML ’09, pages 1169–1176.

Alan Yuille and Anand Rangarajan. 2003. The
concave-convex procedure (CCCP). Neural
Computation 15:915–936.

149


