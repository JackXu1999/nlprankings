



















































LSTM Neural Reordering Feature for Statistical Machine Translation


Proceedings of NAACL-HLT 2016, pages 977–982,
San Diego, California, June 12-17, 2016. c©2016 Association for Computational Linguistics

LSTM Neural Reordering Feature for Statistical Machine Translation

Yiming Cui, Shijin Wang and Jianfeng Li
iFLYTEK Research, Beijing, China

{ymcui,sjwang3,jfli3}@iflytek.com

Abstract

Artificial neural networks are powerful mod-
els, which have been widely applied into
many aspects of machine translation, such
as language modeling and translation mod-
eling. Though notable improvements have
been made in these areas, the reordering prob-
lem still remains a challenge in statistical ma-
chine translations. In this paper, we present
a novel neural reordering model that directly
models word pairs and their alignment. Fur-
ther by utilizing LSTM recurrent neural net-
works, much longer context could be learned
for reordering prediction. Experimental re-
sults on NIST OpenMT12 Arabic-English
and Chinese-English 1000-best rescoring task
show that our LSTM neural reordering feature
is robust, and achieves significant improve-
ments over various baseline systems.

1 Introduction

In statistical machine translation, the language
model, translation model, and reordering model are
the three most important components. Among these
models, the reordering model plays an important
role in phrase-based machine translation (Koehn et
al., 2004), and it still remains a major challenge in
current study.

In recent years, various phrase reordering meth-
ods have been proposed for phrase-based SMT sys-
tems, which can be classified into two broad cate-
gories:

(1) Distance-based RM: Penalize phrase displace-
ments with respect to the degree of non-
monotonicity (Koehn et al., 2004).

(2) Lexicalized RM: Conditions reordering proba-
bilities on current phrase pairs. According to the
orientation determinants, lexicalized reordering
model can further be classified into word-based
RM (Tillman, 2004), phrase-based RM (Koehn
et al., 2007), and hierarchical phrase-based RM
(Galley and Manning, 2008).

Furthermore, some researchers proposed a re-
ordering model that conditions both current and
previous phrase pairs by utilizing recursive auto-
encoders (Li et al., 2014).

In this paper, we propose a novel neural reorder-
ing feature by including longer context for pre-
dicting orientations. We utilize a long short-term
memory recurrent neural network (LSTM-RNN)
(Graves, 1997), and directly models word pairs to
predict its most probable orientation. Experimen-
tal results on NIST OpenMT12 Arabic-English and
Chinese-English translation show that our neural re-
ordering model achieves significant improvements
over various baselines in 1000-best rescoring task.

2 Related Work

Recently, various neural network models have been
applied into machine translation.

Feed-forward neural language model was first
proposed by Bengio et al. (2003), which was a
breakthrough in language modeling. Mikolov et
al. (2011) proposed to use recurrent neural net-
work in language modeling, which can include
much longer context history for predicting next
word. Experimental results show that RNN-based
language model significantly outperform standard
feed-forward language model.

977



Devlin et al. (2014) proposed a neural network
joint model (NNJM) by conditioning both source
and target language context for target word predict-
ing. Though the network architecture is a simple
feed-forward neural network, the results have shown
significant improvements over state-of-the-art base-
lines.

Sundermeyer et al. (2014) also put forward
a neural translation model, by utilizing LSTM-
based RNN and Bidirectional RNN. In bidirectional
RNNs, the target word is conditioned on not only the
history but also future source context, which forms
a full source sentence for predicting target words.

Li et al. (2013) proposed to use a recursive auto-
encoder (RAE) to map each phrase pairs into contin-
uous vectors, and handle reordering problems with
a classifier. Also, they suggested that by both in-
cluding current and previous phrase pairs to deter-
mine the phrase orientations could achieve further
improvements in accuracy (Li et al., 2014).

By far, we have noticed that this is the first time to
use LSTM-RNN in reordering model. We could in-
clude much longer context information to determine
phrase orientations using RNN architecture. Fur-
thermore, by utilizing the LSTM layer, the network
is able to capture much longer range dependencies
than standard RNNs.

Because we need to record fixed length of history
information in SMT decoding step, we only utilize
our LSTM-RNN reordering model as a feature in
1000-best rescoring step. As word alignments are
known after generating n-best list, it is possible to
use LSTM-RNN reordering model to score each hy-
pothesis.

3 Lexicalized Reordering Model

In traditional statistical machine translation, lexical-
ized reordering models (Koehn et al., 2007) have
been widely used. It considers alignments of current
and previous phrase pairs to determine the orienta-
tion.

Formally, when given source language sentence
f = {f1, ..., fn}, target language sentence e =
{e1, ..., en}, and phrase alignment a = {a1, ..., an},
the lexicalized reordering model can be illustrated
in Equation 1, which only conditions on ai−1 and

ai, i.e. previous and current alignment.

p(o|e, f) =
n∏

i=1

p(oi|ei, fai , ai−1, ai) (1)

In Equation 1, the oi represents the set of phrase
orientations. For example, in the most commonly
used MSD-based orientation type, oi takes three val-
ues: M stands for monotone, S for swap, and D for
discontinuous. The definition of MSD-based orien-
tation is shown in Equation 2.

oi =


M, ai − ai−1 = 1
S, ai − ai−1 = −1
D, |ai − ai−1| 6= 1

(2)

For other orientation types, such as LR and MSLR
are also widely used, whose definition can be found
on Moses official website 1.

Recent studies on reordering model suggest that
by also conditioning previous phrase pairs can im-
prove context sensitivity and reduce reordering am-
biguity.

4 LSTM Neural Reordering Model

In order to include more context information for de-
termining reordering, we propose to use a recurrent
neural network, which has been shown to perform
considerably better than standard feed-forward ar-
chitectures in sequence prediction (Mikolov et al.,
2011). However, RNN with conventional back-
propagation training suffers from gradient vanishing
issues (Bengio et al., 1994) .

Later, the long short-term memory was proposed
for solving gradient vanishing problem, and it could
catch longer context than standard RNNs with sig-
moid activation functions. In this paper, we adopt
LSTM architecture for training neural reordering
model.

4.1 Training Data Processing
For reducing model complexity and easy implemen-
tation, our neural reordering model is purely lexical-
ized and trained on word-level.

We will take LR orientation for explanations,
while other orientation types (MSD, MSLR) can
be induced similarly. Given a sentence pair and

1http://www.statmt.org/moses/

978



its alignment information, we can induce the word-
based reordering information by following steps.
Note that, we always evaluate the model in the or-
der of target sentence.

(1) If current target word is one-to-one alignment,
then we can directly induce its orientations, i.e.
〈left〉 or 〈right〉.

(2) If current source/target word is one-to-many
alignment, then we judge its orientation by con-
sidering its first aligned target/source word, and
the other aligned target/source words are anno-
tated as 〈follow〉 reordering type, which means
these word pairs inherent the orientation of pre-
vious word pair.

(3) If current source/target word is not aligned to
any target/source words, we introduce a 〈null〉
token in its opposite side, and annotate this word
pair as 〈follow〉 reordering type.

Figure 1 shows an example of data processing.

wait for approval of the government

等到 政府的 批准

dengdao zhengfu de    pizhun

(a)
R               R                  L

for approval of

等到 政府 批准

dengdao zhengfu de    pizhun

(b)
R               F R L L F

的

wait the government

...... ......

...... ......

...... ......

...... ......

Figure 1: Illustration of data processing. (a) Original reorder-
ing (omit alignment inside each phrase); (b) processed reorder-

ing, all alignments are regularized to word level, R-right, L-left,

F-follow.

4.2 LSTM Network Architecture

After processing the training data, we can directly
utilize the word pairs and its orientation to train a
neural reordering model.

Given a word pair and its orientation, a neural re-
ordering model can be illustrated by Equation 3.

p(o|e, f) =
n∏

i=1

p(oi|ei1, fai1 , ai−1, ai) (3)

Where ei1 = {e1, ..., ei}, fai1 = {f1, ..., fai}. In-
clusion of history word pairs is done with recurrent
neural network, which is known for its capability of
learning history information.

The architecture of LSTM-RNN reordering
model is depicted in Figure 2, and corresponding
equations are shown in Equation 4 to 6.

yi = W1 ∗ fai + W2 ∗ ei (4)
zi = LSTM(yi, W3, yi−11 ) (5)

p(oi|ei1, fai1 , ai−1, ai) = softmax(W4 ∗ zi) (6)
The input layer consists both source and target

language word, which is in one-hot representation.
Then we perform a linear transformation of input
layer to a projection layer, which is also called em-
bedding layer. We adopt extended-LSTM as our hid-
den layer implementation, which consists of three
gating units, i.e. input, forget and output gates. We
omit rather extensive LSTM equations here, which
can be found in (Graves and Schmidhuber, 2005).
The output layer is composed by orientation types.
For example, in LR condition, the output layer con-
tains two units: 〈left〉 and 〈right〉 orientation. Fi-
nally, we apply softmax function to obtain normal-
ized probabilities of each orientation.

Output Layer

LSTM Layer

Projection Layer

Input Layer

𝑃(𝑜$|𝑒'(, 𝑓'
+,, 𝑎(.', 𝑎()

𝑓+, 𝑒(

Figure 2: Architecture of LSTM neural reordering model.

5 Experiments

5.1 Setups
We mainly tested our approach on Arabic-English
and Chinese-English translation. The training
corpus contains 7M words for Arabic, and 4M
words for Chinese, which is selected from NIST

979



System Dev Test1 Test2

Ar-En
MT04-05-06 MT08 MT09

(3795) (1360) (1313)

Zh-En
MT05-08 MT08.prog MT12.rd

(2439) (1370) (820)
Table 1: Statistics of development and test set. The number of
segments are indicated in brackets.

OpenMT12 parallel dataset. We use the SAMA to-
kenizer2 for Arabic word tokenization, and in-house
segmenter for Chinese words. The English part of
parallel data is tokenized and lowercased. All de-
velopment and test sets have 4 references for each
segment. The statistics of development and test sets
are shown in Table 1.

The baseline systems are built with the open-
source phrase-based SMT toolkit Moses (Koehn et
al., 2007). Word alignment and phrase extrac-
tion are done by GIZA++ (Och and Ney, 2000)
with L0-normalization (Vaswani et al., 2012), and
grow-diag-final refinement rule (Koehn et al., 2004).
Monolingual part of training data is used to train
a 5-gram language model using SRILM (Stolcke,
2002). Parameter tuning is done by K-best MIRA
(Cherry and Foster, 2012). For guarantee of re-
sult stability, we tune every system 5 times inde-
pendently, and take the average BLEU score (Clark
et al., 2011). The translation quality is evaluated
by case-insensitive BLEU-4 metric (Papineni et al.,
2002). The statistical significance test is also car-
ried out with paired bootstrap resampling method
with p < 0.001 intervals (Koehn, 2004). Our mod-
els are evaluated in a 1000-best rescoring step, and
all features in 1000-best list as well as LSTM-RNN
reordering feature are retuned via K-best MIRA al-
gorithm.

For neural network training, we use all parallel
text in the baseline training. As a trade-off be-
tween computational cost and performance, the pro-
jection layer and hidden layer are set to 100, which
is enough for our task (We have not seen signifi-
cant gains when increasing dimensions greater than
100). We use an initial learning rate of 0.01 with
standard SGD optimization without momentum. We
trained model for a total of 10 epochs with cross-
entropy criterion. Input and output vocabulary are

2https://catalog.ldc.upenn.edu/LDC2010L01

set to 100K and 50K respectively, and all out-of-
vocabulary words are mapped to a 〈unk〉 token.
5.2 Results on Different Orientation Types
At first, we test our neural reordering model (NRM)
on the baseline that contains word-based reordering
model with LR orientation. The results are shown in
Table 2 and 3.

As we can see that, among various orienta-
tion types (LR, MSD, MSLR), our model could
give consistent improvements over baseline system.
The overall BLEU improvements range from 0.42
to 0.79 for Arabic-English, and 0.31 to 0.72 for
Chinese-English systems. All neural results are sig-
nificantly better than baselines (p < 0.001 level).

In the meantime, we also find that “Left-Right”
based orientation methods, such as LR and MSLR,
consistently outperform MSD-based orientations.
The may caused by non-separability problem, which
means that MSD-based methods are vulnerable to
the change of context, and weak in resolving re-
ordering ambiguities. Similar conclusion can be
found in Li et al. (2014) .

Ar-En System Dev Test1 Test2
Baseline 43.87 39.84 42.05
+NRM LR 44.43 40.53 42.84
+NRM MSD 44.29 40.41 42.62
+NRM MSLR 44.52 40.59 42.78

Table 2: LSTM reordering model with different orientation
types for Arabic-English system.

Zh-En System Dev Test1 Test2
Baseline 27.18 26.17 24.04
+NRM LR 27.90 26.58 24.70
+NRM MSD 27.49 26.51 24.39
+NRM MSLR 27.82 26.78 24.53

Table 3: LSTM reordering model with different orientation
types for Chinese-English system.

5.3 Results on Different Reordering Baselines
We also test our approach on various baselines,
which either contains word-based, phrase-based, or
hierarchical phrase-based reordering model. We
only show the results of MSLR orientation, which
is relatively superior than others according to the re-
sults in Section 5.2.

980



Ar-En System Dev Test1 Test2
Baseline wbe 43.87 39.84 42.05
+NRM MSLR 44.52 40.59 42.78
Baseline phr 44.11 40.09 42.21
+NRM MSLR 44.52 40.73 42.89
Baseline hier 44.30 40.23 42.38
+NRM MSLR 44.61 40.82 42.86
Zh-En System Dev Test1 Test2
Baseline wbe 27.18 26.17 24.04
+NRM MSLR 27.90 26.58 24.70
Baseline phr 27.33 26.05 24.13
+NRM MSLR 27.86 26.46 24.73
Baseline hier 27.56 26.29 24.38
+NRM MSLR 28.02 26.49 24.67

Table 4: Results on various baselines for Arabic-English and
Chinese-English system. “wbe”: word-based; “phr”: phrase-

based; “hier”: hierarchical phrase-based reordering model. All

NRM results are significantly better than baselines (p < 0.001

level).

In Table 4 and 5, we can see that though we add
a strong hierarchical phrase-based reordering model
in the baseline, our model can still bring a maximum
gain of 0.59 BLEU score, which suggest that our
model is applicable and robust in various circum-
stances. However, we have noticed that the gains
in Arabic-English system is relatively greater than
that in Chinese-English system. This is probably be-
cause hierarchical reordering features tend to work
better for Chinese words, and thus our model will
bring little remedy to its baseline.

6 Conclusions

We present a novel work that build a reordering
model using LSTM-RNN, which is much sensitive
to the change of context and introduce rich con-
text information for reordering prediction. Further-
more, the proposed model is purely lexicalized and
straightforward, which is easy to realize. Experi-
mental results on 1000-best rescoring show that our
neural reordering feature is robust, and could give
consistent improvements over various baseline sys-
tems.

In future, we are planning to extend our word-
based LSTM reordering model to phrase-based re-
ordering model, in order to dissolve much more am-
biguities and improve reordering accuracy. Further-

more, we are also going to integrate our neural re-
ordering model into neural machine translation sys-
tems.

Acknowledgments

We sincerely thank the anonymous reviewers for
their thoughtful comments on our work.

References
Y. Bengio, P. Simard, and P. Frasconi. 1994. Learn-

ing long-term dependencies with gradient descent is
difficult. IEEE Transactions on Neural Networks,
5(2):157–166.

Yoshua Bengio, Holger Schwenk, Jean Sbastien Sencal,
Frderic Morin, and Jean Luc Gauvain. 2003. A neu-
ral probabilistic language model. Journal of Machine
Learning Research, 3(6):1137–1155.

Colin Cherry and George Foster. 2012. Batch tuning
strategies for statistical machine translation. In Pro-
ceedings of the 2012 Conference of the North Ameri-
can Chapter of the Association for Computational Lin-
guistics: Human Language Technologies, pages 427–
436, Montréal, Canada, June. Association for Compu-
tational Linguistics.

Jonathan H. Clark, Chris Dyer, Alon Lavie, and Noah A.
Smith. 2011. Better hypothesis testing for statisti-
cal machine translation: Controlling for optimizer in-
stability. In Proceedings of the 49th Annual Meet-
ing of the Association for Computational Linguistics:
Human Language Technologies, pages 176–181, Port-
land, Oregon, USA, June. Association for Computa-
tional Linguistics.

Jacob Devlin, Rabih Zbib, Zhongqiang Huang, Thomas
Lamar, Richard Schwartz, and John Makhoul. 2014.
Fast and robust neural network joint models for sta-
tistical machine translation. In Proceedings of the
52nd Annual Meeting of the Association for Compu-
tational Linguistics (Volume 1: Long Papers), pages
1370–1380, Baltimore, Maryland, June. Association
for Computational Linguistics.

Michel Galley and Christopher D. Manning. 2008. A
simple and effective hierarchical phrase reordering
model. In Proceedings of the 2008 Conference on
Empirical Methods in Natural Language Processing,
pages 848–856, Honolulu, Hawaii, October. Associa-
tion for Computational Linguistics.

A. Graves and J. Schmidhuber. 2005. Framewise
phoneme classification with bidirectional lstm net-
works. In Proceedings in 2005 IEEE International
Joint Conference on Neural Networks, pages 2047–
2052 vol. 4.

981



Alex Graves. 1997. Long short-term memory. Neural
Computation, 9(8):1735–1780.

Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2004. Statistical phrase-based translation. In Con-
ference of the North American Chapter of the Asso-
ciation for Computational Linguistics on Human Lan-
guage Technology-volume, pages 127–133.

Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran, Richard
Zens, Chris Dyer, Ondrej Bojar, Alexandra Con-
stantin, and Evan Herbst. 2007. Moses: Open source
toolkit for statistical machine translation. In Proceed-
ings of the 45th Annual Meeting of the Association for
Computational Linguistics Companion Volume Pro-
ceedings of the Demo and Poster Sessions, pages 177–
180, Prague, Czech Republic, June. Association for
Computational Linguistics.

Philipp Koehn. 2004. Statistical significance tests for
machine translation evaluation. In Dekang Lin and
Dekai Wu, editors, Proceedings of EMNLP 2004,
pages 388–395, Barcelona, Spain, July. Association
for Computational Linguistics.

Peng Li, Yang Liu, and Maosong Sun. 2013. Recursive
autoencoders for ITG-based translation. In Proceed-
ings of the 2013 Conference on Empirical Methods in
Natural Language Processing, pages 567–577, Seat-
tle, Washington, USA, October. Association for Com-
putational Linguistics.

Peng Li, Yang Liu, Maosong Sun, Tatsuya Izuha, and
Dakun Zhang. 2014. A neural reordering model for
phrase-based translation. In Proceedings of COLING
2014, the 25th International Conference on Compu-
tational Linguistics: Technical Papers, pages 1897–
1907, Dublin, Ireland, August. Dublin City University
and Association for Computational Linguistics.

T. Mikolov, S. Kombrink, L. Burget, and J. H. Cernocky.
2011. Extensions of recurrent neural network lan-
guage model. In IEEE International Conference on
Acoustics, Speech Signal Processing, pages 5528–
5531.

Franz Josef Och and Hermann Ney. 2000. A compari-
son of alignment models for statistical machine trans-
lation. In Proceedings of the 18th conference on Com-
putational linguistics - Volume 2, pages 1086–1090.

Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic eval-
uation of machine translation. In Proceedings of 40th
Annual Meeting of the Association for Computational
Linguistics, pages 311–318, Philadelphia, Pennsylva-
nia, USA, July. Association for Computational Lin-
guistics.

Andreas Stolcke. 2002. Srilm — an extensible language
modeling toolkit. In Proceedings of the 7th Inter-

national Conference on Spoken Language Processing
(ICSLP 2002), pages 901–904.

Martin Sundermeyer, Tamer Alkhouli, Joern Wuebker,
and Hermann Ney. 2014. Translation modeling with
bidirectional recurrent neural networks. In Proceed-
ings of the 2014 Conference on Empirical Methods in
Natural Language Processing (EMNLP), pages 14–25,
Doha, Qatar, October. Association for Computational
Linguistics.

Christoph Tillman. 2004. A unigram orientation model
for statistical machine translation. In Daniel Marcu
Susan Dumais and Salim Roukos, editors, HLT-
NAACL 2004: Short Papers, pages 101–104, Boston,
Massachusetts, USA, May 2 - May 7. Association for
Computational Linguistics.

Ashish Vaswani, Liang Huang, and David Chiang. 2012.
Smaller alignment models for better translations: Un-
supervised word alignment with the l0-norm. In Pro-
ceedings of the 50th Annual Meeting of the Associa-
tion for Computational Linguistics (Volume 1: Long
Papers), pages 311–319, Jeju Island, Korea, July. As-
sociation for Computational Linguistics.

982


