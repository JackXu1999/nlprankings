



















































Learning When Not to Answer: a Ternary Reward Structure for Reinforcement Learning Based Question Answering


Proceedings of NAACL-HLT 2019, pages 122–129
Minneapolis, Minnesota, June 2 - June 7, 2019. c©2019 Association for Computational Linguistics

122

Learning When Not to Answer:
A Ternary Reward Structure for Reinforcement

Learning based Question Answering

Fréderic Godin
Ghent University
Ghent, Belgium

frederic.godin@ugent.be

Anjishnu Kumar
Amazon Research

Cambridge, United Kingdom
anjikum@amazon.com

Arpit Mittal
Amazon Research

Cambridge, United Kingdom
mitarpit@amazon.co.uk

Abstract

In this paper, we investigate the challenges
of using reinforcement learning agents for
question-answering over knowledge graphs
for real-world applications. We examine the
performance metrics used by state-of-the-art
systems and determine that they are inade-
quate for such settings. More specifically, they
do not evaluate the systems correctly for situ-
ations when there is no answer available and
thus agents optimized for these metrics are
poor at modeling confidence. We introduce a
simple new performance metric for evaluating
question-answering agents that is more rep-
resentative of practical usage conditions, and
optimize for this metric by extending the bi-
nary reward structure used in prior work to a
ternary reward structure which also rewards
an agent for not answering a question rather
than giving an incorrect answer. We show
that this can drastically improve the precision
of answered questions while only not answer-
ing a limited number of previously correctly
answered questions. Employing a supervised
learning strategy using depth-first-search paths
to bootstrap the reinforcement learning algo-
rithm further improves performance.

1 Introduction

A number of approaches for question answering
have been proposed recently that use reinforce-
ment learning to reason over a knowledge graph
(Das et al., 2018; Lin et al., 2018; Chen et al.,
2018; Zhang et al., 2018). In these methods the in-
put question is first parsed into a constituent ques-
tion entity and relation. The answer entity is then
identified by sequentially taking a number of steps
(or ‘hops’) over the knowledge graph (KG) start-
ing from the question entity. The agent receives
a positive reward if it arrives at the correct an-
swer entity and a negative reward for an incor-
rect answer entity. For example, for the question

France

. . .

Paris

Macron

. . .

Capital of

President of-1 Lives in

Figure 1: Fictional graph for the the question “What’s
the capital of France?”. The relation (Capital of) does
not exist in the graph and thus an alternative path needs
to be used that leads to the correct answer.

“What is the capital of France?”, the question en-
tity is (France) and the goal is to find a path in
the KG which connects it to (Paris). The rela-
tion between the answer entity and question entity
in this example is (Capital of) which is missing
from the KG and has to be inferred via alternative
paths. This is illustrated in Figure 1. A possible
two-hop path to find the answer is to use the fact
that (Macron) is the president of (France) and
that he lives in (Paris). However, there are many
paths that lead to the entity (Paris) but also to
other entities which makes finding the correct an-
swer a non-trivial task.

The standard evaluation metrics used for these
systems are metrics developed for web search such
as Mean Reciprocal Rank (MRR) and hits@k,
where k ranges from 1 to 20. We argue that this is
not a correct evaluation mechanism for a practical
question-answering system (such as Alexa, Cor-
tana, Siri, etc.) where the goal is to return a single
answer for each question. Moreover it is assumed
that there is always an answer entity that could be



123

reached from the question entity in limited num-
ber of steps. However this cannot be guaranteed in
a large-scale commercial setting and for all KGs.
For example, in our proprietary dataset used for
the experimentation, for 15.60% of questions the
answer entity cannot be reached within the limit
of number of steps used by the agent. Hence, we
propose a new evaluation criterion, allowing sys-
tems to return ‘no answer’ as a response when no
answer is available.

We demonstrate that existing state-of-the-art
methods are not suited for a practical question-
answering setting and perform poorly in our eval-
uation setup. The root-cause of poor performance
is the reward structure which does not provide any
incentive to learn not to answer. The modified re-
ward structure we present allows agents to learn
not to answer in a principled way. Rather than hav-
ing only two rewards, a positive and a negative re-
ward, we introduce a ternary reward structure that
also rewards agents for not answering a question.
A higher reward is given to the agent for correctly
answering a question compared to not answering
a question. In this setup the agent learns to make
a trade-off between these three possibilities to ob-
tain the highest total reward over all questions.

Additionally, because the search space of possi-
ble paths exponentially grows with the number of
hops, we also investigate using Depth-First-Search
(DFS) algorithm to collect paths that lead to the
correct answer. We use these paths as a supervised
signal for training the neural network before the
reinforcement learning algorithm is applied. We
show that this improves overall performance.

2 Related work

The closest works to ours are the works by Lin
et al. (2018), Zhang et al. (2018) and Das et al.
(2018), which consider the question answering
task in a reinforcement learning setting in which
the agent always chooses to answer.1 Other ap-
proaches consider this as a link prediction problem
in which multi-hop reasoning can be used to learn
relational paths that link two entities. One line
of work focuses on composing embeddings (Nee-
lakantan et al., 2015; Guu et al., 2015; Toutanova
et al., 2016) initially introduced for link predic-
tion, e.g., TransE (Bordes et al., 2013), ComplexE

1An initial version of this paper has been presented at
the Relational Representation Learning Workshop at NeurIPS
2018 as Godin et al. (2018).

(Trouillon et al., 2016) or ConvE (Dettmers et al.,
2018). Another line of work focuses on logical
rule learning such as neural logical programming
(Yang et al., 2017) and neural theorem proving
(Rocktäschel and Riedel, 2017). Here, we focus
on question answering rather than link prediction
or rule mining and use reinforcement learning to
circumvent that we do not have ground truth paths
leading to the answer entity.

Recently, popular textual QA datasets have been
extended with not-answerable questions (Trischler
et al., 2017; Rajpurkar et al., 2018). Questions that
cannot be answered are labeled with ‘no answer’
option which allows for supervised training. This
is different from our setup in which there are no
ground truth ‘no answer’ labels.

3 Background: Reinforcement learning

We base our work on the recent reinforcement
learning approaches introduced in Das et al.
(2018) and Lin et al. (2018). We denote the knowl-
edge graph as G, the set of entities as E , the set
of relations as R and the set of directed edges L
between entities of the form l = (e1, r, e2) with
e1, e2 ∈ E and r ∈ R. The goal is to find an an-
swer entity ea given a question entity eq and the
question relation rq, when (eq, rq, ea) is not part
of graph G.

We formulate this problem as a Markov De-
cision Problem (MDP) (Sutton and Barto, 1998)
with the following states, actions, transition func-
tion and rewards:

States. At every timestep t, the state st is defined
by the current entity et, the question entity eq and
relation rq, for which et, eq ∈ E and rq ∈ R. More
formally, st = (et, eq, rq).

Actions. For a given entity et, the set of possible
actions is defined by the outgoing edges from et.
Thus At = {(r′, e′)|(et, r′, e′) ∈ G}.

Transition function. The transition function δ
maps st to a new state st+1 based on the ac-
tion taken by the agent. Consequently, st+1 =
δ(st, At) = δ(et, eq, rq, At).

Rewards. The agent is rewarded based on the fi-
nal state. For example, in Das et al. (2018) and Lin
et al. (2018) the agent obtains a reward of 1 if the
correct answer entity is reached as the final state
and 0 otherwise (i.e., R(sT ) = I{eT = ea}).



124

LSTM . . . LSTM . . . LSTM
h1 ht−1 ht hT−1

r0, eq rt, et rT , eT

(a) History LSTM

FFNN (f )

rq, et,ht

rt+1,1 et+1,1
. . .

rt+1,|At| et+1,|At|

⊗
π

softmax

(b) Policy network at timestep t

Figure 2: Figure 2a illustrates the LSTM which encodes history of the path taken. The output at timestep t is used
as input to the policy network, illustrated in Figure 2b, to determine which action to take next.

3.1 Training
We train a policy network π using the REIN-
FORCE algorithm of Williams (1992) which max-
imizes the expected reward:

J(θ) = E(eq ,rq ,ea)∈GEa1,...,aT∼π[R(sT |eq, rq)]
(1)

in which at is the action selected at timestep t fol-
lowing the policy π, and θ are the parameters of
the network.

The policy network consists of two parts:
a Long Short-Term Memory (LSTM) network
which encodes the history of the traversed path,
and a feed-forward neural network to select an
action (at) out of all possible actions. Each
entity and relation have a corresponding vector
et, rt ∈ Rd. The action at ∈ At is represented
by the vectors of the relation and entity as at =
[rt+1; et+1] ∈ R2d. The LSTM encodes the his-
tory of the traversed path and updates its hidden
state each timestep, based on the selected action:

ht = LSTM(ht−1,at−1) (2)

This is illustrated in Figure 2a.
Finally, the feed-forward neural network (f )

combines the history ht, the current entity repre-
sentation et and the query relation rq. Using soft-
max, we compute the probability for each action
by calculating the dot product between the output
of f and each action vector at:

π(at|st) = softmax(Atf(ht, et, rq)) (3)

in which At ∈ R|At|×2d is a matrix consisting of
rows of action vectors at. This is illustrated in Fig-
ure 2b. During training, we sample over this prob-
ability distribution to select the action at, whereas
during inference, we use beam search to select the
most probable path.

4 Evaluation

User-facing question answering systems inher-
ently face a trade-off between presenting an an-
swer to a user that could potentially be incorrect,
and choosing not to answer. However, prior work
in knowledge graph question-answering (QA)
only considers cases in which the answering agent
always produces an answer. This setup originates
from the link prediction and knowledge base com-
pletion tasks in which the evaluation criteria are
hits@k and Mean Reciprocal Rank (MRR), where
k ranges from 1 to 20. However, these met-
rics are not an accurate representation of practical
question-answering systems in which the goal is to
return a single correct answer or not answer at all.
Moreover, using these metrics result in the prob-
lem of the model learning ‘spurious’ paths since
the metrics encourage the models to make wild
guesses even if the path is unlikely to lead to the
correct answer.

We therefore propose to measure the fraction of
questions the system answers (Answer Rate) and
the number of correct answers out of all answers
(Precision) to measure the system performance.
We combine these two metrics by taking the har-
monic mean and call this the QA Score. This can
be viewed as a variant of the popular F-Score met-
ric, with answer rate used as an analogue to recall
in the original metric.

5 Proposed method

In this section, we will first introduce the super-
vised learning technique we used to pretrain the
neural network before applying the reinforcement
learning algorithm. Next we will describe the
ternary reward structure.



125

5.1 Supervised learning

Typically in reinforcement learning, the search
space of possible actions and paths grows expo-
nentially with the path length. Our problem is no
exception to this. Hence an imitation learning ap-
proach could be beneficial here where we provide
a number of expert paths to the learning algorithm
to bootstrap the learning process. This idea has
been explored previously in the context of link
and fact prediction in knowledge graphs where
Xiong et al. (2017) proposed to use a Breadth-
First-Search (BFS) between the entity pairs to se-
lect a set of plausible paths. However BFS favours
identification of shorter paths which could bias
the learner. We therefore use Depth-First-Search
(DFS) to identify paths between question and an-
swer entities and sample up to 100 paths to be used
for the supervised training. If no path can be found
between the entity pair we return a ‘no answer’ la-
bel. Following this, we train the network using
reinforcement learning algorithm which refines it
further. Note that it is not guaranteed that the set of
paths found using DFS are all most efficient. How-
ever as we show in our experiments, bootstrapping
with these paths provide good initialization for the
reinforcement learning algorithm.

5.2 Ternary reward structure

As mentioned previously, we encounter situations
when the answer entity cannot be reached in the
limited number of steps taken by an agent. In
such cases, the system should return a special an-
swer ‘no answer’ as the response. We can achieve
this by adding a synthetic ‘no answer’ action that
leads to a special entity eNOANSWER. This is il-
lustrated in Figure 3. In the framework of Das
et al. (2018) a binary reward is used which rewards
the learner for the answer being wrong or cor-
rect. Following a similar protocol, we could award
a score of 1 to return ‘no answer’ when there is
no answer available in the KG. However, we can-
not achieve reasonable training with such reward
structure. This is because there is no specific pat-
tern for ‘no answer’ that could be directly learned.
Hence, if we reward a system equally for correct or
no answer, it learns to always predict ‘no answer’.
We therefore propose a ternary reward structure in
which a positive reward is given to a correct an-
swer, a neutral reward when eNOANSWER is se-
lected as an answer, and a negative reward for an

France

. . .

No
Answer

Paris

No
Answer

Macron

. . .

No
Answer

Capital of

No answer No answer

President of-1 Lives in

No answer

Figure 3: Fictional graph for the the question “What’s
the capital of France?”. The relation (Capital of) does
not exist in the graph and thus an alternative path needs
to be used that leads to the correct answer. To avoid that
the agent returns an incorrect answer when not finding
the correct answer, a ‘no answer’ relation is added be-
tween every entity node and a special ‘no answer’ node,
to be able to return ‘no answer’.

incorrect answer. More formally:

R(sT ) =


rpos if eT = ea,
0 if eT = eNOANSWER,
rneg if eT 6∈ {ea, eNOANSWER}

(4)
with rpos>0 and rneg<0. The idea is that the agent
receives a larger reward for a correct answer com-
pared to not answering the question, and a neg-
ative reward for incorrectly answering a question
compared to not answering the question. In the ex-
perimental section, we show that this mechanism
provides better performance.

6 Experimental setup

We evaluate our proposed approach on a pub-
licly available dataset, FB15k-237 (Toutanova
and Chen, 2015) which is based on the Free-
base knowledge graph and a proprietary dataset
Alexa69k-378 which is a sample of Alexa’s pro-
prietary knowledge graph. Both the public dataset
and the proprietary dataset are good examples of



126

Model Hits@1 Hits@10 MRR Precision Answer Rate QA Score

(Das et al., 2018) 0.217 0.456 0.293 0.217 1 0.357
(Lin et al., 2018) 0.329 0.544 0.393 0.329 1 0.495

RL 0.2475 0.4032 0.2983 0.2475 1 0.3968
Supervised 0.2474 0.4929 0.3276 0.2474 1 0.3967

Supervised + RL 0.2736 0.5015 0.3469 0.2736 1 0.4296
No Answer RL 0.2345 0.3845 0.2831 0.4011 0.5847 0.4758

All 0.2738 0.4412 0.3286 0.4835 0.5663 0.5216

Table 1: Results on FB15k-237 dataset.

Model Hits@1 Hits@10 MRR Precision Answer Rate QA Score

(Das et al., 2018) 0.1790 0.2772 0.2123 0.1790 1 0.3036
(Lin et al., 2018) 0.1915 0.3184 0.2358 0.1915 1 0.3214

RL 0.1677 0.2716 0.2031 0.1677 1 0.2872
Supervised 0.1471 0.3142 0.203 0.1471 1 0.2565

Supervised + RL 0.1937 0.3045 0.2312 0.1937 1 0.3245
No Answer RL 0.1564 0.2442 0.1858 0.3892 0.4019 0.3955

All 0.1865 0.294 0.2229 0.3454 0.5401 0.4213

Table 2: Results on Alexa69k-378 dataset.

#queries
#ent #rel #facts valid test

FB15k-237

14,505 237 272,115 17,535 20,466

Alexa69k-378

69,098 378 442,591 55,186 55,474

Table 3: Statistics of the datasets.

real-world general-purpose knowledge graphs that
can be used for question answering. FB15k-237
contains 14,505 different entities and 237 different
relations resulting in 272,115 facts. Alexa69k-378
contains 69,098 different entities and 378 differ-
ent relations resulting in 442,591 facts. We fol-
low the setup of Das et al. (2018), using the same
train/val/test splits for FB15k-237. For Alexa69k-
378 we use 10% of the full dataset for validation
and test. For both datasets, we add the reverse re-
lations of all relations in the training set in order
to facilitate backward navigation following the ap-
proach of previous work. Similarly, a ‘no op’ rela-
tion is added for each entity between the entity and
itself, which allows the agent to loop/reason mul-

tiple consecutive steps over the same entity. An
overview of both datasets can be found in Table 3.

We extend the publicly available implementa-
tion of Das et al. (2018) for our experimentation.
We set the size of the entity and relation repre-
sentations d at 100 and the hidden state at 200.
We use a single layer LSTM and train models
with path length 3 (tuned using hyper-parameter
search). We optimize the neural network using
Adam (Kingma and Ba, 2015) with learning rate
0.001, mini-batches of size 256 with 20 rollouts
per example. During the test time, we use beam
search with the beam size of 100. Unlike Das et al.
(2018), we also train entity embeddings after ini-
tializing them with random values. Reward values
are set as rpos = 10 and rneg = −0.1 after per-
forming a coarse grid search for various reward
values on the validation set. For all experiments,
we selected the best model with the highest QA
Score on the corresponding validation set.

7 Results

The results of our experiments for FB15k-237 and
Alexa69k-378 are given in Table 1 and Table 2 re-
spectively.



127

Supervised learning For FB15k-237, we see
that the model trained using reinforcement learn-
ing (RL) scores as well as the model trained using
supervised learning. This makes supervised learn-
ing using DFS a strong baseline system for ques-
tion answering over knowledge graphs, and for
FB15k-237 in particular. On Alexa69k-378, mod-
els trained using supervised learning score lower
on all metrics compared to RL. When combining
supervised learning with RL overall performance
increases.

No answer When we train RL system with our
ternary reward structure (No Answer RL), the
precision and QA score increase significantly on
both datasets. For FB15k-237, our No Answer
RL model decided not to answer over 40% of
the questions, with an absolute hits@1 reduc-
tion of only 1.3% over standard RL. Moreover,
of all the answered questions, 40.11% were an-
swered correctly compared to 24.75% of the orig-
inal question-answering system: an absolute im-
provement of over 15%. This resulted in the final
QA Score of 47.58%, around 8% higher than stan-
dard RL and 12% higher than Das et al. (2018).

Similarly, 60% of the questions did not get an-
swered on Alexa69k-378. This resulted in hits@1
decrease of roughly 1% but compared to stan-
dard RL, the precision increased from 16.77% to
38.92%: an absolute increase of more than 20%.
The final QA Score also increased from 28.72%
to 39.55%, and also significantly improved over
Das et al. (2018) and Lin et al. (2018). The re-
sults indicate that using our method allows us to
improve the precision of the question-answering
system by choosing the right questions to be an-
swered by not answering many questions that were
previously answered incorrectly. This comes at the
expense of not answering some questions that pre-
viously could be answered correctly.

All Finally, all methods were combined in a sin-
gle method. First the model was pretrained in a su-
pervised way. Then the model was retrained using
RL algorithm with ternary reward structure. This
jointly trained model obtained better QA scores
than any individually trained model. On FB15k-
237, a QA score of 52.16% is obtained which is
an absolute improvement of 4.58% over the best
individual model and 2.66% over Lin et al. (2018).
Similarly, on Alexa69k-378, an absolute improve-
ment of 2.57% over the best individual result is

0 5 10 15
0%

25%

50%

75%

100%

Positive Reward

Pe
rc

en
ta

ge

Precision
Answer Rate

QA score

Figure 4: Influence of changing the positive reward for
FB15k-237. The negative reward is fixed at rneg =
−0.1 and the neutral reward is fixed at rneutral = 0.

−0.5 −0.4 −0.3 −0.2 −0.1 00%

25%

50%

75%

100%

Negative Reward

Pe
rc

en
ta

ge

Precision
Answer Rate

QA score

Figure 5: Influence of changing the negative reward for
FB15k-237. The positive reward is fixed at rpos = 10
and the neutral reward is fixed at rneutral = 0.

obtained, almost 10% absolute improvement over
Lin et al. (2018). Sample results from our method
are given in Table 4 and Table 5.

Reward tuning An important part of increasing
the QA score is to select the right combination of
rewards. Therefore, we ran additional experiments
where we varied either the positive or negative re-
ward, keeping the other rewards fixed. In Figure 4,
the precision, answer rate and QA score are shown
when varying the positive reward and keeping the
neutral and negative rewards fixed. When, the pos-
itive reward is very small (rpos = 0.625), almost
no question is answered. When the positive re-
ward rpos is 1.25, roughly 20% of the questions
are answered with a 50% precision. After that,
the precision starts declining and the answer rate



128

Question: eq = Bruce Broughton, rq = Profession. Answer: ea = Music Composer

Bruce Broughton Oscar Best Music Nino Rota Music Composer
Award Nominee Award Winner Profession

Question: eq = Washington nationals, rq = Sports Team Sport. Answer: ea = Baseball

Washington Nationals National League Milwaukee Braves Baseball
Sports League-1 Sports League Sports Team Sport

Table 4: Example paths of correctly answered questions on FB15k-237. Note that the fact (eq, rq, ea) is not part
of the KG.

Question: eq = Sherlock holmes (movie), rq = Story By. Answer: ea = Conan Doyle

Sherlock holmes (movie) Wardrobe Supervisor Wardrobe Sup. Wardrobe Sup.
Film Crew Role No op No op

Sherlock holmes (movie) Wardrobe Supervisor No answer No answer
Film Crew Role No answer No answer

Table 5: Example question from FB15k-237, incorrectly answered by (Das et al., 2018) and not answered by our
system. Note that the fact (eq, rq, ea) is not part of the KG.

starts increasing, resulting in an overall increase in
QA score. The QA score plateaus between 5 and
10 and then starts decreasing slowly. In Figure 5,
the precision, answer rate and QA score are shown
when varying the negative reward and keeping the
neutral and positive rewards fixed. In this case,
the highest QA score is achieved when the nega-
tive reward is between -0.25 and -0.1. As long as
the negative reward is lower than zero, a wrong an-
swer gets penalized and the QA score stays high.

8 Conclusions

In this paper, we addressed the limitations of
current approaches for question answering over
a knowledge graph that use reinforcement learn-
ing. Rather than only returning a correct or in-
correct answer, we allowed the model to not an-
swer a question when it is not sure about it. Our
ternary reward structure gives different rewards
for correctly answered, incorrectly answered and
not answered questions. We also introduced a
new evaluation metric which takes these three op-
tions into account. We showed that we can signif-
icantly improve the precision of answered ques-
tions compared to previous approaches, making
this a promising direction for the practical usage
in knowledge graph-based QA systems.

References
Antoine Bordes, Nicolas Usunier, Alberto Garcia-

Duran, Jason Weston, and Oksana Yakhnenko.
2013. Translating embeddings for modeling multi-
relational data. In Advances in Neural Information
Processing Systems (NIPS).

Wenhu Chen, Wenhan Xiong, Xifeng Yan, and
William Yang Wang. 2018. Variational knowledge
graph reasoning. In Proceedings of the Conference
of the North American Chapter of the Association
for Computational Linguistics: Human Language
Technologies (NAACL-HTL).

Rajarshi Das, Shehzaad Dhuliawala, Manzil Zaheer,
Luke Vilnis, Ishan Durugkar, Akshay Krishna-
murthy, Alexander J. Smola, and Andrew McCal-
lum. 2018. Go for a walk and arrive at the answer:
Reasoning over paths in knowledge bases using re-
inforcement learning. International Conference on
Learning Representations (ICLR).

Tim Dettmers, Minervini Pasquale, Stenetorp Pon-
tus, and Sebastian Riedel. 2018. Convolutional 2d
knowledge graph embeddings. In Proceedings of
the 32th AAAI Conference on Artificial Intelligence
(AAAI).

Fréderic Godin, Anjishnu Kumar, and Arpit Mittal.
2018. Using ternary rewards to reason over knowl-
edge graphs with deep reinforcement learning. In
Workshop on Relational Representation Learning
(NeurIPS).

Kelvin Guu, John Miller, and Percy Liang. 2015.
Traversing knowledge graphs in vector space. In
Proceedings of the Conference on Empirical Meth-
ods in Natural Language Processing (EMNLP).



129

Diederik P. Kingma and Jimmy Ba. 2015. Adam: A
method for stochastic optimization. In Proceedings
of the International Conference on Representation
Learning (ICLR).

Xi Victoria Lin, Richard Socher, and Caiming Xiong.
2018. Multi-hop knowledge graph reasoning with
reward shaping. In Proceedings of the 2018 Con-
ference on Empirical Methods in Natural Language
Processing (EMNLP).

Arvind Neelakantan, Benjamin Roth, and Andrew Mc-
Callum. 2015. Compositional vector space mod-
els for knowledge base completion. In Proceedings
of the Annual Meeting of the Association for Com-
putational Linguistics and the International Joint
Conference on Natural Language Processing (ACL-
IJCNLP).

Pranav Rajpurkar, Robin Jia, and Percy Liang. 2018.
Know what you don’t know: Unanswerable ques-
tions for squad. In Proceedings of the Annual Meet-
ing of the Association for Computational Linguistics
(ACL).

Tim Rocktäschel and Sebastian Riedel. 2017. End-to-
end differentiable proving. In Advances in Neural
Information Processing Systems (NIPS).

Richard S. Sutton and Andrew G. Barto. 1998. In-
troduction to Reinforcement Learning, 1st edition.
MIT Press, Cambridge, MA, USA.

Kristina Toutanova and Danqi Chen. 2015. Observed
versus latent features for knowledge base and text
inference. In 3rd Workshop on Continuous Vector
Space Models and Their Compositionality (ACL).

Kristina Toutanova, Victoria Lin, Wen-tau Yih, Hoi-
fung Poon, and Chris Quirk. 2016. Compositional
learning of embeddings for relation paths in knowl-
edge base and text. In Proceedings of the Annual
Meeting of the Association for Computational Lin-
guistics (ACL).

Adam Trischler, Tong Wang, Xingdi Yuan, Justin Har-
ris, Alessandro Sordoni, Philip Bachman, and Ka-
heer Suleman. 2017. Newsqa: A machine compre-
hension dataset. In Proceedings of the 2nd Work-
shop on Representation Learning for NLP (ACL).

Théo Trouillon, Johannes Welbl, Sebastian Riedel, Éric
Gaussier, and Guillaume Bouchard. 2016. Complex
embeddings for simple link prediction. In Proceed-
ings of the International Conference on Machine
Learning (ICML).

Ronald J. Williams. 1992. Simple statistical gradient-
following algorithms for connectionist reinforce-
ment learning. Machine Learning.

Wenhan Xiong, Thien Hoang, and William Yang
Wang. 2017. Deeppath: A reinforcement learning
method for knowledge graph reasoning. In Proceed-
ings of the 2017 Conference on Empirical Methods
in Natural Language Processing (EMNLP).

Fan Yang, Zhilin Yang, and William W. Cohen. 2017.
Differentiable learning of logical rules for knowl-
edge base reasoning. In Advances in Neural Infor-
mation Processing Systems (NIPS).

Yuyu Zhang, Hanjun Dai, Zornitsa Kozareva, Alexan-
der J. Smola, and Le Song. 2018. Variational
reasoning for question answering with knowledge
graph. In Proceedings of the AAAI Conference on
Artificial Intelligence (AAAI).


