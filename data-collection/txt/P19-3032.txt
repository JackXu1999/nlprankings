



















































MoNoise: A Multi-lingual and Easy-to-use Lexical Normalization Tool


Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics: System Demonstrations, pages 201–206
Florence, Italy, July 28 - August 2, 2019. c©2019 Association for Computational Linguistics

201

MoNoise: A Multi-lingual and Easy-to-use Lexical Normalization Tool

Rob van der Goot
Center for Language and Cognition

University of Groningen
r.van.der.goot@rug.nl

Abstract

In this paper, we introduce and demonstrate
the online demo as well as the command line
interface of a lexical normalization system
(MoNoise) for a variety of languages. We
further improve this model by using features
from the original word for every normalization
candidate. For comparison with future work,
we propose the bundling of seven datasets
in six languages to form a new benchmark,
together with a novel evaluation metric which
is particularly suitable for cross-dataset com-
parisons. MoNoise reaches a new state-of-art
performance for six out of seven of these
datasets. Furthermore, we allow the user
to tune the ‘aggressiveness’ of the normal-
ization, and show how the model can be
made more efficient with only a small loss in
performance. The online demo can be found
on: http://www.robvandergoot.
com/monoise and the corresponding
code on: https://bitbucket.org/
robvanderg/monoise/

1 Lexical Normalization

Because many natural language processing (NLP)
systems are designed with standard texts in mind,
they suffer performance drops when applied to
texts from other domains. In recent years, social
media has become a major source of information.
Due to their hasty and spontaneous nature, texts
on social media are particularly non-standard.

One solution to adapt NLP systems, is to ‘trans-
late’ these non-standard texts to their standard
equivalent. This task is also called lexical nor-
malization, see Figure 1 for the normalization of
“most social pple r troublesome”. Previous work
on normalization is fragmented; a variety of ap-
proaches is evaluated on a variety of benchmarks,
using a variety of evaluation metrics and assump-
tions. Furthermore, most normalization systems
are not opensource or publicly available.

most social pple r troublesome
most social people are troublesome

Figure 1: Example normalization of “most social pple
r troublesome”

In this paper, we present MoNoise, an easy-to-
use normalization system, consisting of an online
demo as well a more elaborate command line in-
terface. We include benchmarks, results and pre-
trained models for a variety of languages.

2 Multi-lingual Normalization
Benchmark

The manually annotated datasets on which we will
evaluate MoNoise are summarized in Table 1. In
all datasets, gold tokenization was assumed. When
1-N is indicated in the table, this means that split-
ting of words was included in the annotation, and
in some rare cases also merging. Since capital-
ization is usually not corrected (merely kept) in
almost all of these datasets, we will lowercase ev-
erything in our evaluation. For datasets with ex-
isting splits, those are used, in other cases the
data is split 80%-10%-10% (train-dev-test). For
LexNorm1.2 we use LiLiu as training and devel-
opment data, in line with previous work (Li and
Liu, 2014, 2015).1.

3 MoNoise

In this section, we will give a summary of the nor-
malization model MoNoise (van der Goot and van
Noord, 2017). Additionally, we added one group
of features which improves the performance of
this model.

1This benchmark can be obtained by running
./scripts/0.getNormData.sh from the reposi-
tory

http://www.robvandergoot.com/monoise
http://www.robvandergoot.com/monoise
https://bitbucket.org/robvanderg/monoise/
https://bitbucket.org/robvanderg/monoise/


202

Corpus Words Lang. %normed 1-N Caps
Source

GhentNorm 12,901 NL 4.8 + +-
De Clercq et al. (2014)

TweetNorm 13,542 ES 6.3 + +-
Alegria et al. (2013)

LexNorm1.2 10,576 EN 11.6 - -
Yang and Eisenstein (2013)

LiLiu 40,560 EN 10.5 - +-
Li and Liu (2014)

LexNorm2015 73,806 EN 9.1 + -
Baldwin et al. (2015)

IWT 38,918 TR 8.5 + +
Eryiǧit and Torunoǧ-Selamet (2017)

Janes-Norm 75,276 SL 15.0 - +-
Erjavec et al. (2017)

ReLDI-hr 89,052 HR 9.0 - +-
Ljubešić et al. (2017a)

ReLDI-sr 91,738 SR 8.0 - +-
Ljubešić et al. (2017b)

Table 1: Comparison of the normalization corpora used
in this work. %normed indicates the percentage of
words which is normalized. The ‘1-N’ column indi-
cates whether words are split/merged in the annota-
tion, the ‘caps’ column indicates whether everything
was lowercased (-), capitalization was transferred to the
normalization (+-), or corrected (+).

3.1 The Architecture

MoNoise splits the normalization task in two sub-
tasks; candidate generation and candidate rank-
ing. In contrast to most other systems, no error
detection is performed beforehand, the decision
whether to normalize is made during ranking. Be-
cause the normalization task consists of a variety
of replacement types (van der Goot et al., 2018),
MoNoise is developed in a modular way. Some of
the modules are based on raw, external data. This
dependency allows the model to be transferred to
new domains and timespans more easily. For both
sub-tasks a variety of modules is designed, which
are described in the following two paragraphs.

Important modules for candidate generation are
Aspell2, a dictionary learned from the training
data and word embeddings (Mikolov et al., 2013)
trained on non-standard data (where the 40 clos-
ests words using cosine distance are used). Be-
cause no normalization detection is done, the orig-
inal word is also included as a candidate.

For the ranking of candidates, features from the

2http://aspell.net/

generation are complemented with additional fea-
tures. The additional features are: N-gram proba-
bilities over non-standard text as well as standard
texts, a feature which indicates whether a word
contains alphanumeric characters or is a domain-
specific token (hashtags, usernames and URLs)
and the length of the original word and the candi-
date. All these features are combined in a random
forest classifier, and the probability that a candi-
date belongs to the ‘correct’ class is used to rank
the candidates.

3.2 Re-use Features of Original Word

A word should only be normalized when a sub-
stantially better candidate is found, this was not
taken into account in the original model. To incor-
porate this intuition, we copy the features from the
orginal word to all the other normalization candi-
dates as additional features.

3.3 Models

For reproducability and reusability of the system,
we provide pre-trained models for all the lan-
guages available in our multi-lingual benchmark
(Section 2). These models are all trained using the
default settings of MoNoise. These models exploit
raw data from the source (non-standard) as well as
the target (standard) domain, as n-gram probabili-
ties and word embeddings are derived from these.
As target domain data we use Wikipedia dumps
from 01-01-20193. For the non-standard data, we
use raw data based on an in-house twitter collec-
tion4. In contrast to van der Goot and van Noord
(2017), we do not not use language specific collec-
tions, but collect random tweets provided by the
Twitter API during 2012 and 2018, and filter these
by language based on the FastText language iden-
tifier (Joulin et al., 2016). Furthermore, we train
embeddings with only 100 dimensions as opposed
to van der Goot and van Noord (2017), who used
400. Because of these new embeddings, MoNoise
uses 2-3 times less RAM, while experiencing only
a very minor performance loss.

The pre-trained models can be found on:
http://www.robvandergoot.com/
data/monoise

3cleaned with https://github.com/attardi/
wikiextractor

4https://developer.twitter.com/

http://aspell.net/
http://www.robvandergoot.com/data/monoise
http://www.robvandergoot.com/data/monoise
https://github.com/attardi/wikiextractor
https://github.com/attardi/wikiextractor
https://developer.twitter.com/


203

Corpus Lang ERR Precision Recall Prev. SOTA Metric Prev. MoNoise

GhentNorm NL 44.62 89.19 50.77 Schulz et al. (2016) WER 3.2 1.365

TweetNorm ES 38.73 94.37 41.19 Porta and Sancho (2013) OOV-Precision 63.4 70.40
LexNorm1.2 EN 59.21 80.87 77.56 Li and Liu (2015) OOV Accuracy 87.58 87.63
LexNorm2015 EN 77.09 95.49 80.91 Jin (2015) F1 84.21 86.58
IWT TR 28.94 96.24 30.12 Eryiǧit et al. (2017) OOV Accuracy 67.37 48.99
Janes-Norm SL 31.67 85.19 0.3833 Ljubešic et al. (2016) L1 CER 0.38 0.53
Janes-Norm SL 63.90 95.66 0.6694 Ljubešic et al. (2016) L3 CER 1.58 2.24
ReLDI-hr HR 51.65 95.66 0.541
ReLDI-sr SR 64.61 94.70 68.43

Table 2: Results of MoNoise on the test data and a comparison with previous benchmarks. For WER and CER
lower scores are better. Words normalized to the wrong candidate are classified as false positive (recall).

4 Evaluation

In this section, we first discuss existing evaluation
metrics and their shortcomings and then introduce
our novel metric. Secondly, we evaluate MoNoise
on the test-splits of the multi-lingual benchmark,
this is done twofold: using our preferred metric as
well as a comparison to previous work.

4.1 Evaluation Metrics
Evaluation beyond word-level Some previous
work used evaluation metrics which allow for
evaluation beyond the word level. However, most
of the normalization corpora do not include anno-
tation beyond the word-level, except for the work
of Zhang et al. (2013). Sometimes, BLEU score is
used, whereas others use word error rate (WER) or
character error rate (CER). We consider these met-
rics to be overly complex, since the word-order is
not altered during annotation.

F1 In the shared task on normalization hosted
at WNUT (Baldwin et al., 2015), F1 score was
used as main evaluation. During development
of MoNoise, we found multiple reasons why this
might not be the preferable metric:

• Hard to interpret (how much of the problem
is solved with an F1 score of 0.35?)

• It is unclear what to do with a word which
should be normalized, but is normalized in-
correctly, does this harm recall (false posi-
tive), precision (false negative) or both? 6

• Because of the previous point, reproducibil-
ity and comparison with previous work can
be difficult.

5Results are not directly comparable as different splits and
tokenization is used

6For a more extensive discussion on this, we refer
to van der Goot (2019). In the WNUT share task, they are
FP and FN, in this paper they are considered only FP (recall).

Accuracy Early work on normalization often
used accuracy over the words in need of nor-
malization as main evaluation (Han and Baldwin,
2011; Liu et al., 2012) (OOV accuracy in Table 2).
However, in this setting, the detection of which
words need to be normalized is not taken into ac-
count. To include the full task of normalization,
accuracy over all the words could be used. Accu-
racy is much easier interpretable compared to F1
score. However, accuracy does not allow for easy
comparison across corpora, as different percent-
ages of words might be in need of normalization.
This is the main motivation for a novel evaluation
metric, discussed in the next paragraph.

Error Reduction Rate Because previous met-
rics are overly complicated, hard to interpret or
do not allow for an easy comparison between dif-
ferent datasets, we introduce the Error Reduction
Rate (ERR). Error reduction rate can be inter-
preted as accuracy normalized for the number of
words that are normalized in the gold standard.
This allows for a direct comparison with a baseline
which always copies the original word, the accu-
racy of such a baseline is equal to the percentage
of words which need to be normalized. The for-
mula for ERR is:

ERR =
Accuracysystem −Accuracybaseline

1.0−Accuracybaseline
(1)

The ERR will usually have a value between 0.0
and 1.0. A negative ERR indicates that the sys-
tem normalizes makes more erroneous than cor-
rect normalizations. A baseline which always
keeps the original word scores exactly 0.0, and a
perfect system will score 1.0. We will use ERR
as main evaluation metric and additionally report
precision and recall, to gain more insights into the
strengths and weaknesses of the system. A more



204

0.0 0.2 0.4 0.6 0.8 1.0 1.2 1.4 1.6

Weight

0.0

0.1

0.2

0.3

0.4

0.5

0.6

0.7
E
R

R
GhentNorm

TweetNorm

LexNorm1.2

LexNorm2015

IWT

Janes-Norm

ReLDI-hr

ReLDI-sr

Figure 2: The effect of tuning the weight of the original
candidate. A low weight indicates a more aggressive
system.

detailed discussion on the motivation behind ERR
can be found in (van der Goot, 2019).

4.2 Results
We present the results on all corpora of the multi-
lingual benchmark in Table 2. We report the ERR,
precision, recall, as well as the standard metric for
each dataset. During development, no tuning was
done on the test data, as described in more detail
in (van der Goot and van Noord, 2017).

As mentioned in Section 4.1, it is disputable
whether words which are normalized to the wrong
normalization candidate should be counted as
false positive or false negative. We choose to cat-
egorize these as false negatives, and thus be ac-
counted for in the recall.

The ERR differs quite substantially across the
datasets, this is due to different sizes of train-
ing data as well as differences in annotation. In
the LexNorm2015 dataset, for example, phrasal
abbreviations are expanded (‘lol’7→‘laughing out
loud’), leading to a lot of very common replace-
ments, which can easily be learned from the train-
ing data. On all datasets, the precision is higher
than the recall. In other words, the model is con-
servative. This is arguably a desirable result, as it
is important to avoid over-normalization.

When we look at the comparison to the previous
state-of-the-art systems, we see that MoNoise per-
forms highly competitive. Only on the Slovenian
and Turkish datasets, the previous state-of-the-art
is not surpassed.

5 Aggressiveness

To gain more control over the output normaliza-
tion, we introduce a parameter which controls for
the aggressiveness of the model. This is done by
weighing the confidence score estimated for the

original word (see also Section 3.1). If this is given
a high weight, the original word is more likely to
rank high.

The effect of tuning this parameter is plotted
in Figure 2. It becomes apparent that the default
weight of 1.0 leads to rather stable performance.
For some datasets, minor gains can be achieved
by a more aggressive setting. Furthermore, we
can see that the parameter only becomes effective
with extreme values, which is because the classi-
fier is relatively certain about its predictions, and
often gives very high scores to a certain candi-
date (>0.99). Although this parameter only leads
to minor gains, it can still be useful to adapt the
model to other domains, or to inspect whether the
model was close to the correct normalization.

6 Efficiency

The main bottleneck for efficiency in the model is
the searching of the 40 closest words in the word-
embeddings. In the original word2vec (Mikolov
et al., 2013) code, this is done by a for-loop which
iterates through the whole vocabulary, and calcu-
lates the cosine distance for every word. There
are more efficient techniques to calculate these
distance, like the one used by Gensim (Řehůřek
and Sojka, 2010). However, this still takes half a
second per word on a modern pc for our English
embeddings with a vocabulary size of 4,500,000.
Therefore, we cache the 40 closest candidates in
the embeddings for each word. We included the
cached embeddings with each of our models. 7

The next bottleneck of the model is the random
forest classifier. So, further gains in efficiency can
be gained by limiting the number of allowed can-
didates, as previously done by (Jin, 2015). This
can be done by only considering words which oc-
cur in the corrected data, or a larger list by also
including the Aspell dictionary.

The effect of filtering the candidates on the
LiLiu dataset is shown in Table 3. Results on
the other datasets showed similar trends. Filter-
ing candidates based on only the training data re-
sults in a huge speedup, however also substan-
tially harms performance. However, if we add
the Aspell dictionary to the list of allowed candi-
dates, performance remains relatively close, and a
speedup of factor 2 is achieved.

7The code to cache embeddings (including a python
wrapper) is available at: https://bitbucket.org/
robvanderg/cacheembeds.

https://bitbucket.org/robvanderg/cacheembeds
https://bitbucket.org/robvanderg/cacheembeds


205

Restrictions ERR avg. cands words/ trainTime
sec (seconds)

None 61.83 84 29 2,171
Train 51.64 12 137 280
Train + Aspell 61.12 43 62 1,104

Table 3: ERR when filtering candidates before ranking,
and speed of the model when predicting and training.
All reported results are the average of five runs on the
LiLiu development set.

7 Interface

We provide two interfaces to use MoNoise; a com-
mand line application and a demo website.

7.1 Command Line
The only requirement to install MoNoise is a
somewhat recent c++ compiler (c++11 or newer).
In this section, we will highlight the most useful
commands, for the full list of options we refer to
the repository.
--cands N Outputs at most N candidates

for each word and their probability. These proba-
bilities are obtained by normalizing the confidence
scores of the classifier so that they sum to 1.0.
--caps Do not lowercase everything, can

be used during training as well as testing/running.
This parameter is enabled for the online demo.
--feats Lets you provide a bytestring with

which specific modules can be disabled, the mod-
ules are explained in van der Goot and van Noord
(2017) and listed in utils/feats.txt.
--known N Only allow normalization can-

didates which occur in the normalized version of
the training data(N=1), or allow candidates which
occur in the training data or the Aspell dictio-
nary(N=2).
--tokenize Employ a conservative to-

kenizer, which splits sequences of punctuation
(∗.”?!(){} :; /, \˜&) from the beginning and end
of a word. Here, we assume that the normalization
model will take care of other irregularities.
--weight N Weigh the confidence of the

original word with N, thereby tuning the aggres-
siveness (Section 5).

7.2 Online
In the online interface (Figure 3), the user can
type a sentence and get the predicted normaliza-
tion. The dropdown menu includes all languages
for which a pre-trained model is available. The
aggressiveness (Section 5) can be tuned with a

Figure 3: The layout of the online demo on a low reso-
lution screen.

slider which converts this agressiveness factor to
a weight for the original word. This allows the
user to inspect beyond the top-1 predicted nor-
malization sequence. Additionally, some exam-
ple social media posts are displayed for the user,
which are not shown in Figure 3 due to privacy is-
sues. The online demo can be used on: http:
//www.robvandergoot.com/monoise

8 Conclusion

In this paper, we have demonstrated the online in-
terface and command line interface of MoNoise.
For this system, we release models for 6 differ-
ent languages, of which a new state-of-the-art is
reached for multiple datasets. The system is easy
to install and use on Unix-based systems and has
many useful extra options. For even easier usage,
the online demo can be used from any device with
internet access and a browser. We discussed multi-
ple practical issues, like evaluation, efficiency, and
extra tuning parameters.

Acknowledgements

I would like to thank Kevin Humphreys for his
help with integrating Aspell into MoNoise, Ian
Matroos for providing the python wrapper for the
cached embeddings, Gertjan van Noord for his
suggestions during the development of MoNoise,
and Hessel Haagsma for the suggesting the name
‘error reduction rate’. This system is developed
in the ‘Parsing Algorithms for Uncertain Input’
project, funded by the Nuance foundation.

http://www.robvandergoot.com/monoise
http://www.robvandergoot.com/monoise


206

References
Inaki Alegria, Nora Aranberri, Vı́ctor Fresno, Pablo

Gamallo, Lluis Padró, Inaki San Vicente, Jordi
Turmo, and Arkaitz Zubiaga. 2013. Introducción
a la tarea compartida Tweet-Norm 2013: Normal-
ización léxica de tuits en español. In Tweet-Norm@
SEPLN, pages 1–9.

Timothy Baldwin, Marie-Catherine de Marneffe,
Bo Han, Young-Bum Kim, Alan Ritter, and Wei
Xu. 2015. Shared tasks of the 2015 workshop on
noisy user-generated text: Twitter lexical normal-
ization and named entity recognition. In Proceed-
ings of the Workshop on Noisy User-generated Text,
pages 126–135, Beijing, China.

Orphée De Clercq, Sarah Schulz, Bart Desmet, and
Véronique Hoste. 2014. Towards shared datasets for
normalization research. In Proceedings of the Ninth
International Conference on Language Resources
and Evaluation (LREC’14), Reykjavik, Iceland. Eu-
ropean Language Resources Association (ELRA).

Tomaž Erjavec, Darja Fišer, Jaka Čibej, Špela
Arhar Holdt, Nikola Ljubešić, and Katja Zupan.
2017. CMC training corpus janes-tag 2.0.

Gülşen Eryiǧit and Dilara Torunoǧ-Selamet. 2017. So-
cial media text normalization for turkish. Natural
Language Engineering, 23(6):835–875.

Rob van der Goot. 2019. Normalization and Parsing
Algorithms for Uncertain Input. Ph.D. thesis, Uni-
versity of Groningen.

Rob van der Goot and Gertjan van Noord. 2017.
MoNoise: Modeling noise using a modular normal-
ization system. Computational Linguistics in the
Netherlands Journal, 7:129–144.

Rob van der Goot, Rik van Noord, and Gertjan van
Noord. 2018. A taxonomy for in-depth evaluation
of normalization for user generated content. In
Proceedings of the Eleventh International Confer-
ence on Language Resources and Evaluation (LREC
2018), Miyazaki, Japan. European Language Re-
sources Association (ELRA).

Bo Han and Timothy Baldwin. 2011. Lexical normal-
isation of short text messages: Makn sens a #twit-
ter. In Proceedings of the 49th Annual Meeting of
the Association for Computational Linguistics: Hu-
man Language Technologies, pages 368–378, Port-
land, Oregon, USA.

Ning Jin. 2015. NCSU-SAS-Ning: Candidate gener-
ation and feature engineering for supervised lexical
normalization. In Proceedings of the Workshop on
Noisy User-generated Text, pages 87–92, Beijing,
China.

Armand Joulin, Edouard Grave, Piotr Bojanowski, and
Tomas Mikolov. 2016. Bag of tricks for efficient text
classification. arXiv preprint arXiv:1607.01759.

Chen Li and Yang Liu. 2014. Improving text normal-
ization via unsupervised model and discriminative
reranking. In Proceedings of the ACL 2014 Student
Research Workshop, pages 86–93, Baltimore, Mary-
land, USA.

Chen Li and Yang Liu. 2015. Joint POS tagging and
text normalization for informal text. In Proceedings
of IJCAI 2015, Buenos Aires, Argentina, July 25-31,
2015, pages 1263–1269.

Fei Liu, Fuliang Weng, and Xiao Jiang. 2012. A broad-
coverage normalization system for social media lan-
guage. In Proceedings of the 50th Annual Meet-
ing of the Association for Computational Linguistics
(Volume 1: Long Papers), pages 1035–1044, Jeju Is-
land, Korea.

Nikola Ljubešić, Tomaž Erjavec, Maja Miličević, and
Tanja Samardžić. 2017a. Croatian Twitter training
corpus ReLDI-NormTagNER-hr 2.0.

Nikola Ljubešić, Tomaž Erjavec, Maja Miličević, and
Tanja Samardžić. 2017b. Serbian Twitter training
corpus ReLDI-NormTagNER-sr 2.0.

Nikola Ljubešic, Katja Zupan, Darja Fišer, and Tomaz
Erjavec. 2016. Normalising slovene data: historical
texts vs. user-generated content. Bochumer Linguis-
tische Arbeitsberichte.

Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey
Dean. 2013. Efficient estimation of word represen-
tations in vector space. Proceedings of Workshop at
ICLR.

Jordi Porta and José-Luis Sancho. 2013. Word nor-
malization in Twitter using finite-state transducers.
Tweet-Norm@ SEPLN, 1086:49–53.

Radim Řehůřek and Petr Sojka. 2010. Software Frame-
work for Topic Modelling with Large Corpora. In
Proceedings of the LREC 2010 Workshop on New
Challenges for NLP Frameworks, pages 45–50, Val-
letta, Malta. ELRA.

Sarah Schulz, Guy De Pauw, Orphée De Clercq, Bart
Desmet, Véronique Hoste, Walter Daelemans, and
Lieve Macken. 2016. Multimodular text normaliza-
tion of Dutch user-generated content. ACM Trans-
actions on Intelligent Systems Technology, 7(4):1–
22.

Yi Yang and Jacob Eisenstein. 2013. A log-linear
model for unsupervised text normalization. In Pro-
ceedings of the 2013 Conference on Empirical Meth-
ods in Natural Language Processing, pages 61–72,
Seattle, Washington, USA.

Congle Zhang, Tyler Baldwin, Howard Ho, Benny
Kimelfeld, and Yunyao Li. 2013. Adaptive parser-
centric text normalization. In Proceedings of the
51st Annual Meeting of the Association for Compu-
tational Linguistics (Volume 1: Long Papers), pages
1159–1168, Sofia, Bulgaria.

http://www.aclweb.org/anthology/W15-4319
http://www.aclweb.org/anthology/W15-4319
http://www.aclweb.org/anthology/W15-4319
http://hdl.handle.net/11356/1123
http://www.robvandergoot.com/doc/thesis.pdf
http://www.robvandergoot.com/doc/thesis.pdf
https://www.aclweb.org/anthology/P11-1038
https://www.aclweb.org/anthology/P11-1038
https://www.aclweb.org/anthology/P11-1038
http://www.aclweb.org/anthology/W15-4313
http://www.aclweb.org/anthology/W15-4313
http://www.aclweb.org/anthology/W15-4313
http://www.aclweb.org/anthology/P14-3012
http://www.aclweb.org/anthology/P14-3012
http://www.aclweb.org/anthology/P14-3012
http://ijcai.org/Proceedings/15/Papers/182.pdf
http://ijcai.org/Proceedings/15/Papers/182.pdf
http://www.aclweb.org/anthology/P12-1109
http://www.aclweb.org/anthology/P12-1109
http://www.aclweb.org/anthology/P12-1109
http://hdl.handle.net/11356/1170
http://hdl.handle.net/11356/1170
http://hdl.handle.net/11356/1171
http://hdl.handle.net/11356/1171
http://doi.acm.org/10.1145/2850422
http://doi.acm.org/10.1145/2850422
http://www.aclweb.org/anthology/D13-1007
http://www.aclweb.org/anthology/D13-1007
http://www.aclweb.org/anthology/P13-1114
http://www.aclweb.org/anthology/P13-1114

