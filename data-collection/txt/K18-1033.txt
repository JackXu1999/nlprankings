



















































Learning to Actively Learn Neural Machine Translation


Proceedings of the 22nd Conference on Computational Natural Language Learning (CoNLL 2018), pages 334–344
Brussels, Belgium, October 31 - November 1, 2018. c©2018 Association for Computational Linguistics

334

Learning to Actively Learn Neural Machine Translation

Ming Liu Wray Buntine
Faculty of Information Technology, Monash University

{ming.m.liu, wray.buntine, gholamreza.haffari} @ monash.edu

Gholamreza Haffari

Abstract

Traditional active learning (AL) methods for
machine translation (MT) rely on heuristics.
However, these heuristics are limited when the
characteristics of the MT problem change due
to e.g. the language pair or the amount of the
initial bitext. In this paper, we present a frame-
work to learn sentence selection strategies for
neural MT. We train the AL query strategy us-
ing a high-resource language-pair based on AL
simulations, and then transfer it to the low-
resource language-pair of interest. The learned
query strategy capitalizes on the shared char-
acteristics between the language pairs to make
an effective use of the AL budget. Our ex-
periments on three language-pairs confirms
that our method is more effective than strong
heuristic-based methods in various conditions,
including cold-start and warm-start as well as
small and extremely small data conditions.

1 Introduction

Parallel training bitext plays a key role in the
quality neural machine translation (NMT). Learn-
ing high-quality NMT models in bilingually low-
resource scenarios is one of the key challenges, as
NMT’s quality degrades severely in such setting
(Koehn and Knowles, 2017).

Recently, the importance of learning NMT
models in scarce parallel bitext scenarios has
gained attention. Unsupervised approaches try
to learn NMT models without the need for par-
allel bitext (Artetxe et al., 2017; Lample et al.,
2017a). Dual learning/backtranslation tries to start
off from a small amount of bilingual text, and
leverage monolingual text in the source and tar-
get language (Sennrich et al., 2015a; He et al.,
2016). Zero/few shot approach attempts to trans-
fer NMT learned from rich bilingual settings to
low-resource settings (Johnson et al., 2016; Gu
et al., 2018).

In this paper, we approach this problem from
the active learning (AL) perspective. Assuming
the availability of an annotation budget and a pool
of monolingual source text as well as a small
training bitext, the goal is to select the most use-
ful source sentences and query their translation
from an oracle up to the annotation budget. The
queried sentences need to be selected carefully to
get the value for the budget, i.e. get the highest
improvements in the translation quality of the re-
trained model. The AL approach is orthogonal to
the aforementioned approaches to bilingually low-
resource NMT, and can be potentially combined
with them.

We present a framework to learn the sentence
selection policy most suitable and effective for the
NMT task at hand. This is in contrast to the
majority of work in AL-MT where hard-coded
heuristics are used for query selection (Haffari
and Sarkar, 2009; Bloodgood and Callison-Burch,
2010). More concretely, we learn the query pol-
icy based on a high-resource language-pair shar-
ing similar characteristics with the low-resource
language-pair of interest. After trained, the policy
is applied to the language-pair of interest capital-
ising on the learned signals for effective query se-
lection. We make use of imitation learning (IL) to
train the query policy. Previous work has shown
that the IL approach leads to more effective pol-
icy learning (Liu et al., 2018), compared to rein-
forcement learning (RL) (Fang et al., 2017) . Our
proposed method effectively trains AL policies for
batch queries needed for NMT, as opposed to the
previous work on single query selection.

We conduct experiments on three language
pairs Finnish-English, German-English, and
Czech-English. Simulating low resource sce-
narios, we consider various settings, including
cold-start and warm-start as well as small and
extremely small data conditions. The experiments



335

show the effectiveness and superiority of our
policy query compared to strong baselines.

2 Learning to Actively Learn MT

Active learning is an iterative process: Firstly, a
model is built using some initially available data.
Then, the most worthwhile data points are selected
from the unlabelled set for annotation by the ora-
cle. The underlying model is then re-trained using
the expanded labeled data. This process is then
repeated until the budget is exhausted. The main
challenge is how to identify and select the most
beneficial unlabelled data points during the AL it-
erations.

The AL strategy can be learned by attempting
to actively learn on tasks sampled from a distri-
bution over the tasks (Bachman et al., 2017). We
simulate the AL scenario on instances of a low-
resource MT problem created using the bitext of
the resource-rich language pair, where the transla-
tion of some part of the bitext is kept hidden. This
allows to have an automatic oracle to reveal the
translations of the queried sentences, resulting in
an efficient way to quickly evaluate an AL strat-
egy. Once the AL strategy is learned on simula-
tions, it is then applied to real AL scenarios. The
more related are the low-resource language-pair in
the real scenario to those used to train the AL strat-
egy, the more effective the AL strategy would be.

We are interested to train a translation model
mφφφ which maps an input sentence from a source
language xxx ∈ X to its translation yyy ∈ Yxxx in a
target language, where Yxxx is the set of candidate
translations for the input xxx and φφφ is the parameter
vector of the translation model. Let D = {(xxx,yyy)}
be a support set of parallel corpus, which is ran-
domly partitioned into parallel bitext Dlab, mono-
lingual text Dunl, and evaluation Devl datasets.
Repeated random partitioning creates multiple in-
stances of the AL problem.

3 Hierarchical MDP Formulation

A crucial difference of our setting to the previous
work (Fang et al., 2017; Liu et al., 2018) is that
the AL agent receives the reward from the oracle
only after taking a sequence of actions, i.e. se-
lection of an AL batch which may correspond to
multiple training minibatches for the underlying
NMT model. This fulfils the requirements for ef-
fective training of NMT, as minibatch updates are
more effective than those of single sentence pairs.

Furthermore, it is presumably more efficient and
practical to query the translation of an untranslated
batch from a human translator, rather than one sen-
tence in each AL round.

At each time step t of an AL problem, the al-
gorithm interacts with the oracle and queries the
labels of a batch selected from the pool Dunlt to
form bbbt. As the result of this sequence of ac-
tions to select sentences in bbbt, the AL algorithm
receives a reward BLEU(mφφφ, Devl) which is the
BLEU score on Devl based on the retrained NMT
model using the batch mbbbtφφφ .

Formally, this results in a hierarchical Markov
decision process (HMDP) for batch sentence se-
lection in AL. A state ssst := 〈Dlabt , Dunlt , bbbt,φφφt〉 of
the HMDP in the time step t consist of the bitext
Dlabt , the monotext D

unl
t , the current text batch

bbbt, and the parameters of the currently trained
NMT model φφφt. The high-level MDP consists of
a goal set G := {retrain, haltHI}, where set-
ting a goal gt ∈ G corresponds to either halt-
ing the AL process, or giving the execution to
the low-level MDP to collect a new batch of bi-
text bbbt, re-training the underlying NMT model to
get the update parameters φφφt+1, receiving the re-
ward RHI(ssst, at, ssst+1) := BLEU(mφφφt+1 , D

evl),
and updating the new state as ssst+1 = 〈Dlabt ∪
bbbt, D

unl
t , ∅,φφφt+1〉. The haltHI goal is set in case

the full AL annotation budget is exhausted, other-
wise the re-train goal is set in the next time step.

The low-level MDP consists of primitive ac-
tions at ∈ Dunlt ∪ {haltLO} corresponding to
either selecting of the monolingual sentences in
Dunlt , or halting the low-level policy and giving
the execution back to the high-level MDP. The halt
action is performed in case the maximum amount
of source text is chosen for the current AL round,
when the oracle is asked for the translation of the
source sentences in the monolingual batch, which
is then replaced by the resulting bitext. The sen-
tence selection action, on the other hand, forms the
next state by adding the chosen monolingual sen-
tence to the batch and removing it from the pool
of monolingual sentences. The underlying NMT
model is not trained as a result of taking an action
in the low-level policy, and the reward function is
constant zero.

A trajectory in our HMDP consists of σ :=
(sss1, g1, τ1, r1, sss2, ..., sssH , gH , rH , sssH+1) which is
the concatenation of interleaved high-level trajec-
tory τHI := (sss1, g1, r1, sss2, .., sssH+1) and low-level



336

score

x
y

cccglobal

ccclocal

Figure 1: The policy network.

trajectories τ := (sss1, a1, sss2, a2, ..., sssT , aT , sssT+1).
Clearly, the intermediate goals set by the top-
level MDP into the σ are retrain, and only the
last goal gH is haltHI, where H is determined
by checking whether the total AL budget BHI is
exhausted. Likewise, the intermediate actions in
τh are sentence selection, and only the last action
aT is haltLO, where T is determined by checking
whether the round-AL budget BLO is exhausted.

We aim to find the optimal AL policy prescrib-
ing which datapoint needs to be queried in a given
state to get the most benefit. The optimal policy is
found by maximising the expected long-term re-
ward, where the expectation is over the choice of
the synthesised AL problems and other sources of
randomness, i.e. partioing of D into Dlab, Dunl,
and Devl. Following Bachman et al. (2017), we
maximise the sum of the rewards after each AL
round to encourage the anytime behaviour, i.e. the
model should perform well after each batch query.

4 Deep Imitation learning for AL-NMT

The question remains of how to train the policy
network to maximize the reward, i.e. the generali-
sation performance of the underlying NMT model.
As the policy for the high-level MDP is fixed, we
only need to learn the optimal policy for the low-
level MDP. We formulate learning the AL pol-
icy as an imitation learning problem. More con-
cretely, the policy is trained using an algorithmic
expert, which can generate a reasonable AL tra-
jectories (batches) for each AL state in the high-
level MDP. The algorithmic expert’s trajectories,
i.e. sequences of AL states paired with the expert’s
actions in the low-level MDP, are then used to train
the policy network. As such, the policy network is
a classifier, conditioned on a context summarising
both global and local histories, to choose the best
sentence (action) among the candidates. After the

AL policy is trained based on AL simulations, it is
then transferred to the real AL scenario.

For simplicity of presentation, the training algo-
rithms are presented using a fixed number of AL
iterations for the high-level and low-level MDPs.
This corresponds to AL with the sentence-based
budget. However, extending them for AL with
token-based budget is straightforward, and we ex-
periment with both versions in §5.

Policy Network’s Architecture The policy
scoring network is a fully-connected network with
two hidden layers (see Figure 1). The input in-
volves the representation for three elements: (i)
global context which includes all previous AL
batches, (ii) local context which summarises the
previous sentences selected for the current AL
batch, and (iii) the candidate sentence paired with
its translation generated by the currently trained
NMT model.

For each source sentencexxx paired with its trans-
lation yyy, we denote the representation by rep(xxx,yyy).
We construct it by simply concatenating the rep-
resentations of the source and target sentences,
each of which is built by summing the embed-
dings of its words. We found this simple method to
work well, compared to more complicated meth-
ods, e.g. taking the last hidden state of the decoder
in the underlying NMT model. The global context
(cccglobal) and local contexts (ccclocal) are constructed
by summing the representation of the previously
selected batches and sentence-pairs, respectively.

IL-based Training Algorithm The IL-based
training method is presented in Algorithm 1. The
policy network is initialised randomly, and trained
based T simulated AL problems (lines 3–20), by
portioning the available large bilingual corpus into
three sets: (i) Dlab as the growing training bi-
tex, (ii) Dunl as the pool of untranslated sentences
where we pretend the translations are not given,
and (iii) Devl as the evaluation set used by our al-
gorithmic expert.

For each simulated AL problem, Algorithm 1
executes THI iterations (lines 7–19) to collect AL
batches for training the underlying NMT model
and the policy network. An AL batch is obtained
either from the policy network (line 15) or from
the algorithmic expert (lines 10-13), depending on
tossing a coin (line 9). The latter also includes
adding the selected batch, the candidate batches,
and the relevant state information to the replay



337

Algorithm 1 Learning AL-NMT Policy
Input: Parallel corpus D, Iwidth the width of the constructed

search lattices, the coin parameter α, the number of sam-
pled AL batches K

Output: policy π
1: M ← ∅ . Replay Memory
2: Initialise π with a random policy
3: for T training iterations do
4: Dlab, Devl, Dunl ← randomPartition(D)
5: φφφ← trainModel(Dlab)
6: cccglobal ← 000
7: for t← 1 to THI do . MDPHI
8: S ← searchLattice(Dunl, Iwidth)
9: if coinToss(α) = Head then

10: BBB ← {samplePath(S,φφφ,cccglobal, π, β)}K1
11: BBB ← BBB + samplePath(S,φφφ,cccglobal, π, 0)
12: bbb← argmax

bbb′∈BBB
BLEU(mbbb

′
φφφ , D

evl) . expert

13: M ←M ∪ {(cccglobal,φφφ,BBB,bbb)}
14: else
15: bbb← samplePath(S,φφφ,cccglobal, π, 0) . policy
16: Dlab ← Dlab + bbb
17: Dunl ← Dunl − {xxx s.t. (xxx,yyy) ∈ bbb}
18: φφφ← retrainModel(φφφ,Dlab)
19: cccglobal ← cccglobal ⊕ rep(bbb)
20: π ← updatePolicy(π,M,φφφ)
21: return π

Algorithm 2 samplePath (selecting an AL batch)
Input: Search lattice S, global context cccglobal, policy π,

perturbation probability β
Output: Selected AL batch bbb

1: bbb← ∅
2: ccclocal ← 000
3: for t← 1 to TLO do . MDPLO
4: if coinToss(β) = Head then
5: xxxt ← π0(S[t]) . perturbation policy
6: else
7: xxxt ← argmaxxxx∈S[t] π(cccglobal, ccclocal,xxx)
8: yyyt ← oracle(xxxt) . getting the gold translation
9: ccclocal ← ccclocal ⊕ rep(xxxt, yyyt)

10: bbb← bbb+ (xxxt, yyyt)
11: return bbb

memory M , based on which the policy will be re-
trained. The selected batch is then used to retrain
the underlying NMT model, update the training
bilingual corpus and pool of monotext, and update
the global context vector (lines 16–19).

The mixture of the policy network and algo-
rithmic expert in batch collection on simulated
AL problems is inspired by Dataset Aggregation
DAGGER (Ross and Bagnell, 2014). This makes
sure that the collected states-actions pairs in the
replay memory include situations encountered be-
yond executing only the algorithmic expert. This
informs the trained AL policy how to act reason-
ably in new situations encountered in the test time,
where only the network policy is in charge and the
expert does not exist.

⇡
⇡

S[1] S[TLO]....

⇡0

⇡0

Figure 2: The search lattice and the selection of a batch
based on the perturbed policy. Each circle denotes a
sentence from the pool of monotext. The number of
sentences at each time step, denoted by S[t], is Iwidth.
The black sentences are selected in this AL batch.

Algorithmic Expert At a given AL state, the al-
gorithmic expert selects a reasonable batch from
the pool, Dunl via:

argmax
bbb∈BBB

BLEU(mbbbφφφ, D
evl) (1)

where mbbbφφφ denotes the underlying NMT model φ
further retrained by incorporating the batch bbb, and
BBB denotes the possible batches from Dunl (Liu
et al., 2018). However, the number of possible
batches is exponential in the size Dunl, hence the
above optimisation procedure would be very slow
even for a moderately-sized pool.

We construct a search lattice S from which the
candidate batches in BBB are sampled (see Figure
2). The search lattice is constructed by sampling
a fixed number of candidate sentences Iwidth from
Dunl for each position in a batch, whose size is
TLO. A candidate AL batch is then be selected us-
ing Algorithm 2. It executes a mixture of the cur-
rent AL policy π and a perturbation policy π0 (e.g.
random sentence selection or any other heuristic)
in the lower-level MDP to sample a batch. After
several such batches are sampled to form BBB, the
best one is selected according to eqn 1.

We have carefully designed the search space to
be able to incorporate the current policy’s recom-
mended batch and sampled deviations from it in
BBB. This is inspired by the LOLS (Locally Opti-
mal Learning to Search) algorithm (Chang et al.,
2015), to invest efforts in the neighbourhood of
the current policy and improve it. Moreover, hav-
ing to deal with only Iwidth number of sentences
at each selection stage makes the batch formation
algorithm based on the policy fast and efficient.

Re-training the Policy Network To train our
policy network, we turn sentence preference
scores to probabilities over the candidate batches,



338

Algorithm 3 Policy Transfer
Input: Bitext Dlab, monotext Dunl, pre-trained policy π
Output: NMT model φφφ
1: Initialise φφφ with Dlab

2: cccglobal ← 000
3: for t← 1 to THI do
4: S ← searchLattice(Dunl, Iwidth)
5: bbb← samplePath(S,φφφ,cccglobal, π, 0)
6: Dlab ← Dlab + bbb
7: Dunl ← Dunl − {xxx s.t. (xxx,yyy) ∈ bbb}
8: φφφ← retrainModel(φφφ,Dlab)
9: cccglobal ← cccglobal ⊕ rep(bbb)

10: return φφφ

and optimise the parameters to maximise the log-
likelihood objective (Liu et al., 2018).

More specifically, let (cccglobal,BBB,bbb) be a training
tuple in the replay memory. We define the proba-
bility of the correct action/batch as

Pr(bbb|BBB,cccglobal) :=
score(bbb, cccglobal)∑

bbb′∈BBB score(bbb′, cccglobal)
.

The preference score for a batch is the sum of its
sentences’ preference scores,

score(bbb, cccglobal) :=
|bbb|∑
t=1

π(cccglobal, ccclocal<t, rep(xxxt, yyyt))

where ccclocal<t denotes the local context up to the
sentence t in the batch.

To form the log-likelihood, we use recent tuples
and randomly sample several older ones from the
replay memory. We then use stochastic gradient
descent (SGD) to maximise the training objective,
where the gradient of the network parameters are
calculated using the backpropagation algorithm.

Transfering the Policy We now apply the pol-
icy learnt on the source language pair to AL in the
target task (see Algorithm 3). To enable transfer-
ring the policy to a new language pair, we make
use of pre-trained multilingual word embeddings.
In our experiments, we either use the pre-trained
word embeddings from Ammar et al. (2016) or
build it based on the available bitext and monotext
in the source and target language (c.f. §5.2). To
retrain our NMT model, we make parameter up-
dates based on the mini-batches from the AL batch
as well as sampled mini-batches from the previous
iterations.

5 Experiment

Datasets Our experiments use the following
language pairs in the news domain based on

WMT2018: English-Czech (EN-CS), English-
German (EN-DE), English-Finnish (EN-FI). For
AL evaluation, we randomly sample 500K sen-
tence pairs from the parallel corpora in WMT2018
for each of the three language pairs, and take 100K
as the initially available bitext and the rest of 400K
as the pool of untranslated sentences, pretending
the translation is not available. During the AL it-
erations, the translation is revealed for the queried
source sentences in order to retrain the underlying
NMT model.

For pre-processing the text, we normalise the
punctuations and tokenise using moses1 scripts.
The trained models are evaluated using BLEU on
tokenised and cased sensitive test data from the
newstest 2017.

NMT Model Our baseline model consists of a
2-layer bi-directional LSTM encoder with an em-
beddings size of 512 and a hidden size of 512. The
1-layer LSTM decoder with 512 hidden units uses
an attention network with 128 hidden units. We
use a multiplicative-style attention attention archi-
tecture (Luong et al., 2015). The model is opti-
mized using Adam (Kingma and Ba, 2014) with
a learning rate of 0.0001, where the dropout rate
is set to 0.3. We set the mini-batch size to 200
and the maximum sentence length to 50. We train
the base NMT models for 5 epochs on the initially
available bitext, as the perplexity on the dev set do
not improve beyond more training epochs. After
getting new translated text in each AL iteration,
we further sample ×5 more bilingual sentences
from the previously available bitext, and make one
pass over this data to re-train the underlying NMT
model. For decoding, we use beam-search with
the beam size of 3.

Selection Strategies We compare our policy-
based sentence selection for NMT-AL with the
following heuristics:

• Random We randomly select monolingual
sentences up to the AL budget.

• Length-based We use shortest/longest
monolingual sentences up to the AL budget.

• Total Token Entropy (TTE) We sort
monolingual sentences based on their TTE
which has been shown to be a strong AL
heuristic (Settles and Craven, 2008) for

1http://www.statmt.org/moses



339

System EN→DE EN→FI EN→DE EN→CS EN→CS EN→FI
Base NMT (100K) 13.2 10.3 13.2 8.1 8.1 10.3

AL with 135K token budget
Random 13.9 11.2 13.9 8.3 8.3 11.2
Shortest 14.5 11.5 14.5 8.6 8.6 11.5
Longest 14.1 11.3 14.1 8.2 8.2 11.3
TTE 14.2 11.3 14.2 8.5 8.5 11.3
Token Policy 15.5 12.8 14.8 8.5 9.0 12.5

AL with 677K token budget
Random 15.9 13.5 15.9 9.2 9.2 13.5
Shortest 15.8 13.7 15.8 8.9 8.9 13.7
Longest 15.6 13.5 15.6 8.5 8.5 13.5
TTE 15.6 13.7 15.6 8.6 8.6 13.7
Token Policy 16.6 14.1 16.3 9.2 10.3 13.9
FULL bitext (500K) 20.5 18.3 20.5 12.1 12.1 18.3

Table 1: BLEU scores on tests sets with different selection strategies, the budget is at token level with annotation
for 135.45k tokens and 677.25k tokens respectively.

sequence-prediction tasks. Given a mono-
lingual sentence xxx, we compute the TTE as∑|ŷyy|

i=1 Entropy[Pi(.|ŷyy<i,xxx,φφφ)] where ŷyy is the
decoded translation based on the current un-
derlying NMT model φφφ, and Pi(.|ŷyy<i,xxx,φφφ)
is the distribution over the vocabulary words
for the position i of the translation given the
source sentence and the previously generated
words. We also experimented with the nor-
malised version of this measure, i.e. dividing
TTE by |ŷyy|, and found that their difference is
negligible. So we only report TTE results.

5.1 Translating from English

Setting We train the AL policy on a language-
pair treating it as high-resource, and apply it to
another language-pair treated as low-resource. To
transfer the policies across languages, we make
use of pre-trained multilingual word embeddings
learned from monolingual text and bilingual dic-
tionaries (Ammar et al., 2016). Furthermore, we
use these cross-lingual word embeddings to ini-
tialise the embedding table of the NMT in the low-
resource language-pair. The source and target vo-
cabularies for the NMT model in the low-resource
scenario are constructed using the initially avail-
able 100K bitext, and are expanded during the AL
iterations as more translated text becomes avail-
able.

Results Table 1 shows the results. The exper-
iments are performed with two limits on token
annotation budget: 135k and 677k corresponding
to select roughly 10K and 50K sentences in to-

System EN→DE EN→FI
Base NMT (100K) 13.2 10.3

AL with 10K sentence budget
Random 14.2 11.3
Shortest 13.9 11.0
Longest 14.7 11.8
TTE 14.3 11.2
Sent πEN→CS 14.5 11.5
Token πEN→CS 15.3 11.8

AL with 50K sentence budget
Random 16.1 13.5
Shortest 15.5 12.9
Longest 17.2 14.2
TTE 16.6 13.5
Sent πEN→CS 16.3 14.0
Token πEN→CS 17.0 14.1

Table 2: BLEU scores on tests sets for different lan-
guage pairs with different selection strategies, the bud-
get is at sentence level with annotation for 10k sen-
tences and 50k sentences respectively.

tal in AL2, respectively. The number of AL it-
erations is 50, hence the token annotation budget
for each round is 2.7K and 13.5K. As we can see
our policy-based AL method is very effective, and
outperforms the strong AL baselines in all cases
except, when transferring the policy trained on
EN → FI to EN → CS where it is on-par with
the best baseline.

Sentence vs Token Budget In our results in Ta-
ble 1, we have taken the number of tokens in the
selected sentences as a proxy for the annotation
cost. Another option to measure the annotation
cost is the number of selected sentences, which
admittedly is not the best proxy. Nonetheless, one

2These token limit budgets are calculated using random
selection of 10K and 50K sentences multiple times, and tak-
ing the average of the tokens across the sampled sets.



340

100K initial bitext 10K initial bitext
AL method cold-start warm-start cold-start warm-start
Base NMT 10.6/11.8 13.9/14.7 2.3/2.5 5.4/5.8
Random 12.9/13.3 15.1/16.2 5.5/5.6 9.3/9.6
Shortest 13.0/13.5 15.9/16.4 5.9/6.1 9.1/9.3
Longest 12.5/12.9 15.3/15.8 5.7/5.9 9.8/10.2
TTE 12.8/13.2 15.8/16.1 5.9/6.2 9.8/10.1
πCS→EN 13.9/14.2 16.8/17.3 6.3/6.5 10.5/10.9
πFI→EN 13.5/14.0 16.5/16.9 6.1/6.4 10.2/10.3
πEN→CS 13.3/13.6 16.4/16.5 5.1/5.7 10.3/10.5
πEN→FI 13.2/13.5 15.9/16.3 5.1/5.6 9.8/10.2
Ensemble
πCS,FI→EN 14.1/14.3 16.8/17.5 6.3/6.5 10.5/10.9
πEN→CS,FI 13.6/13.8 16.5/16.9 5.8/5.9 10.3/10.5

Full Model (500K) 20.5/20.6 22.3/22.5 - -

Table 3: BLEU scores on tests sets using different selection strategies. The token level annotation budget is 677K.

may be interested to see how different AL meth-
ods compare against each other based on this cost
measure.

Table 2 show the results based on the sentence-
based annotation cost. We train a policy on EN→
CS, and apply it to EN → DE and EN → FI
translation tasks. In addition to the token-based
AL policy from Table 1, we train another policy
based on the sentence budget. The token-based
policy is competitive in EN → DE, where the
longest sentence heuristic achieves the best perfor-
mance, presumably due to the enormous training
signal obtained by translation of long sentences.
The token-based policy is on par with longest sen-
tence heuristic in EN→ FI for both 10K and 100K
AL budgets to outperform the other methods.

5.2 Translating into English

Setting We investigate the performance of the
AL methods on DE → EN based on the policies
trained on the other language pairs. In addition to
100K training data condition, we assess the effec-
tiveness of the AL methods in an extremely low-
resource condition consisting of only 10K bilin-
gual sentences as the initial bitext.

In addition to the source word embedding ta-
ble that we initialised in the previous section’s
experiments using the cross lingual word embed-
dings, we are able further to initialise all of the
other NMT parameters for DE → EN transla-
tion. This includes the target word embedding ta-
ble and the decoder softmax, as the target language
is the same (EN) in the language-pairs used for

both policy training and policy testing. We refer to
this setting as warm-start, as opposed to cold-start
in which we only initialised the source embed-
ding table with the cross-lingual embeddings. For
the warm-start experiments, we transfer the NMT
trained on 500K CS-EN bitext, based on which
the policy is trained. We use byte-pair encoding
(BPE) (Sennrich et al., 2015b) with 30K opera-
tions to bpe the EN side. For the source side, we
use words in order to use the cross-lingual word
embeddings. All parameters of the transferred
NMT are frozen, except the ones corresponding
to the bidirectional RNN encoder and the source
word embedding table.

To make this experimental condition as realis-
tic as possible, we learn the cross-lingual word
embedding for DE using large amounts of mono-
lingual text and the initially available bitext, as-
suming a multilingual word embedding already
exists for the languages used in the policy train-
ing phase. More concretely, we sample 5M DE
text from WMT2018 data3, and train monolingual
word embeddings as part of a skip-gram language
model using fastText.4 We then create a bilin-
gual EN-DE word dictionary based on the initially
available bitext (either 100K or 10K) using word
alignments generated by fast align.5 The
bilingual dictionary is used to project the mono-
lingual DE word embedding space into that of EN,

3We make sure that it does not include the DE sentences
in the 400K pool used in the AL experiments.

4https://github.com/facebookresearch/fastText
5https://github.com/clab/fast align



341

hence aligning the spaces through the following
orthogonal projection:

argmax
QQQ

m∑
i=1

eee[yi]
T ·QQQ · eee[xi] s.t. QQQT ·QQQ = III

where {(yi, xi)}mi=1 is the bilingual dictionary
consisting of pairs of DE-EN words6, eee[yi] and
eee[xi] are the embeddings of the DE and EN words,
and QQQ is the orthogonal transformation matrix
aligning the two embedding spaces. We solve
the above optimisation problem using SVD as in
Smith et al. (2017). The cross-lingual word em-
bedding for a DE word y is then eee[y]T · QQQ. We
build two such cross-lingual embeddings based on
the two bilingual dictionaries constructed from the
10K and 100K bitext, in order to use in their cor-
responding experiments.

Results Table 3 presents the results, on two con-
ditions of 100K and 10K initial bilingual sen-
tences. For each of these data conditions, we ex-
periments with both cold-start and warm-start set-
tings using the pre-trained multilingual word em-
beddings from Ammar et al. (2016) or those we
have trained with the available bitext plus addi-
tional monotext. Firstly, the warm start strategy
to transfer the NMT system from CS → EN to
DE → EN has been very effective, particularly
on extremely low bilingual condition of 10K sen-
tence pairs. It is worth noting that our multilingual
word embeddings are very effective, even-though
they are trained using small bitext. Secondly, our
policy-based AL methods are more effective than
the baseline methods and lead to up to +1 BLEU
score improvements.

We further take the ensemble of multiple trained
policies to build a new AL query strategy. In the
ensemble, we rank sentences based on each of the
policies. Then we produce a final ranking by com-
bining these rankings. Specifically, we sum the
ranking of each sentence according to each pol-
icy to get a rank score, and re-rank the sentences
according to their rank score. Table 3 shows that
ensembling is helpful, but does not produce signif-
icant improvements compared to the best policy.

5.3 Analysis

Distribution of word frequency TTE is a com-
petitive heuristic-based strategy, as shown in the

6One can incorporate human curated bilingual lexicons to
the automatically curated dictionaries as well.

Figure 3: On the task of DE→EN, the plot shows the
log fraction of words vs the log frequency from the se-
lected data returned by different strategies, in which we
have a 677K token budget and do warm start with 100K
initial bitext. The AL policy here is πCS→EN .

above experiments. We compare the word fre-
quency distributions of the selected source text re-
turned by Random, TTE against our AL policy.
The policy we use here is πCS→EN and applied
on the task of DE→EN, which is conducted in the
warm-start scenario with 100K initial bitext and
677K token budget.

Fig. 3 is the log-log plot of the fraction of vo-
cabulary words (y axis) having a particular fre-
quency (x axis). Our AL policy is less likely to
select high-frequency words than other two meth-
ods when it is given a fixed token budget.

Weighted combination of heuristics In order
to get the intuition of which of the heuristics
our AL policy resorts to, we again use policy
πCS→EN and apply on the task of DE→EN,
which is conducted in the warm-start scenario
with 100K initial bitext and 677K token budget.
Meanwhile, we get the preference scores for the
sentences from the monolingual set. Then, we
fit a linear regression model based on the sen-
tences and their scores, in which the response
variable is the preference score and the predic-
tor variables are extracted features or heuristics
based on the sentences. The extracted features
are (length, TTE, f0, f1, f2, f3+), where fi is the
fraction of words in the sentence that appear i
times in the bitext. Table 4 shows the the coeffients
of these heuristics, their standard errors (SE) and
t values. We can see that our AL policy consid-
ers length and TTE in parallel as they have a close
range of coefficients, the policy also prefers low
frequency than high frequency words.



342

heuristics coefficient SE t value
length 0.0328 0.0105 3.1238
TTE 0.0357 0.0147 2.4285
f0 0.0223 0.0132 1.6893
f1 0.0105 0.0092 1.1413
f2 0.0052 0.0023 2.260
f3+ -0.0043 0.0006 -7.1667

Table 4: The table gives an estimation of the resorted
heuristics.

6 Related Work

For statistical MT (SMT), active learning is well
explored, e.g. see Haffari and Sarkar (2009);
Haffari et al. (2009), where several heuristics for
query sentence selection have been proposed, in-
cluding the entropy over the potential translations
(uncertainty sampling), query by committee, and a
similarity-based sentence selection method. How-
ever, active learning is largely under-explored for
NMT. The goal of this paper is to provide an
approach to learn an active learning strategy for
NMT based on a Hierarchical Markov Decision
Process (HMDP) formulation of the pool-based
AL (Bachman et al., 2017; Liu et al., 2018).

Expoliting monolingual data for nmt Mono-
lingual data play a key role in neural machine
translation systems, previous work have consid-
ered training a seperate language model on the tar-
get side (Jean et al., 2014; Gulcehre et al., 2015;
Domhan and Hieber, 2017). Rather than using ex-
plicit language model, Cheng et al. (2016) intro-
duced an auto-encoder-based approach, in which
the source-to-target and target-to-source transla-
tion models act as encoder and decoder respec-
tively. Moreover, back translation approaches
(Sennrich et al., 2015a; Zhang et al., 2018; Hoang
et al., 2018) show efficient use of monolingual
data to improve neural machine translation. Dual
learning (He et al., 2016) extends back transla-
tion by using a deep RL approach. More recently,
unsupervised approaches (Lample et al., 2017b;
Artetxe et al., 2017) and phrase-based NMT (Lam-
ple et al., 2018) learn how to translate when hav-
ing access to only a large amount of monolingual
corpora, these models also extend the use of back
translation and cross-lingual word embeddings are
provided as the latent semantic space for sentences
from monolingual corpora in different languages.

Meta-AL learning Several meta-AL ap-
proaches have been proposed to learn the AL
selection strategy automaticclay from data. These
methods rely on deep reinforcement learning
framework (Yue et al., 2012; Wirth et al., 2017) or
bandit algorithms (Nguyen et al., 2017). Bachman
et al. (2017) introduced a policy gradient based
method which jointly learns data representation,
selection heuristic as well as the model prediction
function. Fang et al. (2017) designed an active
learning algorithm based on a deep Q-network, in
which the action corresponds to binary annotation
decisions applied to a stream of data. Woodward
and Finn (2017) extended one shot learning
to active learning and combined reinforcement
learning with a deep recurrent model to make
labeling decisions. As far as we know, we are
the first one to develop the Meta-AL method to
make use of monolingual data for neural machine
translation, the method we proposed in this paper
can be applied at mini-batch level and conducted
in cross lingual settings.

7 Conclusion

We have introduced an effective approach for
learning active learning policies for NMT, where
the learner needs to make batch queries. We have
provides a hierarchical MDP formulation of the
problem, and proposed a policy network structure
capturing the context in both MDP levels. Our
policy training method uses imitation learning and
a search lattice to carefully collect AL trajecto-
ries for further improvement of the current policy.
We have provided experimental results on three
language pairs, where the policies are transferred
across languages using multilingual word embed-
dings. Our experiments confirms that our method
is more effective than strong heuristic-based meth-
ods in various conditions, including cold-start and
warm-start as well as small and extremely small
data conditions.

Acknowledgments

We would like to thank the feedback from anony-
mous reviewers. This work was supported by
computational resources from the Multi-modal
Australian ScienceS Imaging and Visualisation
Environment (MASSIVE) at Monash University,
and partly by an NVIDIA GPU grant.



343

References
Waleed Ammar, George Mulcaire, Yulia Tsvetkov,

Guillaume Lample, Chris Dyer, and Noah A Smith.
2016. Massively multilingual word embeddings.
arXiv preprint arXiv:1602.01925.

Mikel Artetxe, Gorka Labaka, Eneko Agirre, and
Kyunghyun Cho. 2017. Unsupervised neural ma-
chine translation. arXiv preprint arXiv:1710.11041.

Philip Bachman, Alessandro Sordoni, and Adam
Trischler. 2017. Learning algorithms for active
learning. In Proceedings of the 34th International
Conference on Machine Learning, volume 70 of
Proceedings of Machine Learning Research, pages
301–310, International Convention Centre, Sydney,
Australia. PMLR.

Michael Bloodgood and Chris Callison-Burch. 2010.
Bucking the trend: Large-scale cost-focused active
learning for statistical machine translation. In Pro-
ceedings of the 48th Annual Meeting of the Associa-
tion for Computational Linguistics, pages 854–864.

Kai-Wei Chang, Akshay Krishnamurthy, Alekh Agar-
wal, Hal Daumé, III, and John Langford. 2015.
Learning to search better than your teacher. In
Proceedings of the 32Nd International Conference
on International Conference on Machine Learning,
pages 2058–2066.

Yong Cheng, Wei Xu, Zhongjun He, Wei He, Hua
Wu, Maosong Sun, and Yang Liu. 2016. Semi-
supervised learning for neural machine translation.
arXiv preprint arXiv:1606.04596.

Tobias Domhan and Felix Hieber. 2017. Using target-
side monolingual data for neural machine translation
through multi-task learning. In Proceedings of the
2017 Conference on Empirical Methods in Natural
Language Processing, pages 1500–1505.

Meng Fang, Yuan Li, and Trevor Cohn. 2017. Learning
how to active learn: A deep reinforcement learning
approach. arXiv preprint arXiv:1708.02383.

Jiatao Gu, Hany Hassan, Jacob Devlin, and Victor OK
Li. 2018. Universal neural machine translation for
extremely low resource languages. arXiv preprint
arXiv:1802.05368.

Caglar Gulcehre, Orhan Firat, Kelvin Xu, Kyunghyun
Cho, Loic Barrault, Huei-Chi Lin, Fethi Bougares,
Holger Schwenk, and Yoshua Bengio. 2015. On us-
ing monolingual corpora in neural machine transla-
tion. arXiv preprint arXiv:1503.03535.

Gholamreza Haffari, Maxim Roy, and Anoop Sarkar.
2009. Active learning for statistical phrase-based
machine translation. In Proceedings of Human Lan-
guage Technologies: The 2009 Annual Conference
of the North American Chapter of the Association
for Computational Linguistics, pages 415–423. As-
sociation for Computational Linguistics.

Gholamreza Haffari and Anoop Sarkar. 2009. Active
learning for multilingual statistical machine transla-
tion. In Proceedings of the Joint Conference of the
47th Annual Meeting of the ACL, pages 181–189.

Di He, Yingce Xia, Tao Qin, Liwei Wang, Nenghai Yu,
Tieyan Liu, and Wei-Ying Ma. 2016. Dual learn-
ing for machine translation. In Advances in Neural
Information Processing Systems, pages 820–828.

Vu Cong Duy Hoang, Philipp Koehn, Gholamreza
Haffari, and Trevor Cohn. 2018. Iterative back-
translation for neural machine translation. In Pro-
ceedings of the 2nd Workshop on Neural Machine
Translation and Generation, pages 18–24.

Sébastien Jean, Kyunghyun Cho, Roland Memisevic,
and Yoshua Bengio. 2014. On using very large tar-
get vocabulary for neural machine translation. arXiv
preprint arXiv:1412.2007.

Melvin Johnson, Mike Schuster, Quoc V. Le, Maxim
Krikun, Yonghui Wu, Zhifeng Chen, Nikhil Thorat,
Fernanda Vigas, Martin Wattenberg, G.s Corrado,
Macduff Hughes, and Jeffrey Dean. 2016. Google’s
multilingual neural machine translation system: En-
abling zero-shot translation.

Diederik P Kingma and Jimmy Ba. 2014. Adam: A
method for stochastic optimization. arXiv preprint
arXiv:1412.6980.

Philipp Koehn and Rebecca Knowles. 2017. Six chal-
lenges for neural machine translation. In Pro-
ceedings of the First Workshop on Neural Machine
Translation, pages 28–39. Association for Compu-
tational Linguistics.

Guillaume Lample, Alexis Conneau, Ludovic Denoyer,
and Marc’Aurelio Ranzato. 2017a. Unsupervised
machine translation using monolingual corpora only.
arXiv preprint arXiv:1711.00043.

Guillaume Lample, Ludovic Denoyer, and
Marc’Aurelio Ranzato. 2017b. Unsupervised
machine translation using monolingual corpora
only. arXiv preprint arXiv:1711.00043.

Guillaume Lample, Myle Ott, Alexis Conneau, Lu-
dovic Denoyer, and Marc’Aurelio Ranzato. 2018.
Phrase-based & neural unsupervised machine trans-
lation. arXiv preprint arXiv:1804.07755.

Ming Liu, Wray Buntine, and Gholamreza Haffari.
2018. Learning how to actively learn: A deep im-
itation learning approach. In Proceedings of the An-
nual Meeting of the Association for Computational
Linguistics.

Minh-Thang Luong, Hieu Pham, and Christopher D
Manning. 2015. Effective approaches to attention-
based neural machine translation. arXiv preprint
arXiv:1508.04025.



344

Khanh Nguyen, Hal Daumé III, and Jordan Boyd-
Graber. 2017. Reinforcement learning for bandit
neural machine translation with simulated human
feedback. arXiv preprint arXiv:1707.07402.

Stephane Ross and J Andrew Bagnell. 2014. Rein-
forcement and imitation learning via interactive no-
regret learning. arXiv preprint arXiv:1406.5979.

Rico Sennrich, Barry Haddow, and Alexandra Birch.
2015a. Improving neural machine translation
models with monolingual data. arXiv preprint
arXiv:1511.06709.

Rico Sennrich, Barry Haddow, and Alexandra Birch.
2015b. Neural machine translation of rare
words with subword units. arXiv preprint
arXiv:1508.07909.

Burr Settles and Mark Craven. 2008. An analysis of ac-
tive learning strategies for sequence labeling tasks.
In Proceedings of the conference on empirical meth-
ods in natural language processing, pages 1070–
1079. Association for Computational Linguistics.

Samuel L. Smith, David H. P. Turban, Steven Hamblin,
and Nils Y. Hammerla. 2017. Offline bilingual word
vectors, orthogonal transformations and the inverted
softmax. arXiv preprint arXiv:1702.03859.

Christian Wirth, Riad Akrour, Gerhard Neumann, and
Johannes Fürnkranz. 2017. A survey of preference-
based reinforcement learning methods. Journal of
Machine Learning Research, 18(136):1–46.

Mark Woodward and Chelsea Finn. 2017. Active one-
shot learning. arXiv preprint arXiv:1702.06559.

Yisong Yue, Josef Broder, Robert Kleinberg, and
Thorsten Joachims. 2012. The k-armed dueling ban-
dits problem. J. Comput. Syst. Sci., 78(5):1538–
1556.

Zhirui Zhang, Shujie Liu, Mu Li, Ming Zhou, and En-
hong Chen. 2018. Joint training for neural machine
translation models with monolingual data. arXiv
preprint arXiv:1803.00353.


