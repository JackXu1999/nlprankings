



















































Dependency Parsing with Dilated Iterated Graph CNNs


Proceedings of the 2nd Workshop on Structured Prediction for Natural Language Processing, pages 1–6
Copenhagen, Denmark, September 7–11, 2017. c©2017 Association for Computational Linguistics

Dependency Parsing with Dilated Iterated Graph CNNs

Emma Strubell Andrew McCallum
College of Information and Computer Sciences

University of Massachusetts Amherst
{strubell, mccallum}@cs.umass.edu

Abstract

Dependency parses are an effective way
to inject linguistic knowledge into many
downstream tasks, and many practitioners
wish to efficiently parse sentences at scale.
Recent advances in GPU hardware have
enabled neural networks to achieve signif-
icant gains over the previous best models,
these models still fail to leverage GPUs’
capability for massive parallelism due to
their requirement of sequential process-
ing of the sentence. In response, we pro-
pose Dilated Iterated Graph Convolutional
Neural Networks (DIG-CNNs) for graph-
based dependency parsing, a graph con-
volutional architecture that allows for ef-
ficient end-to-end GPU parsing. In ex-
periments on the English Penn TreeBank
benchmark, we show that DIG-CNNs per-
form on par with some of the best neural
network parsers.

1 Introduction

By vastly accelerating and parallelizing the core
numeric operations for performing inference and
computing gradients in neural networks, recent de-
velopments in GPU hardware have facilitated the
emergence of deep neural networks as state-of-
the-art models for many NLP tasks, such as syn-
tactic dependency parsing. The best neural de-
pendency parsers generally consist of two stages:
First, they employ a recurrent neural network such
as a bidirectional LSTM to encode each token in
context; next, they compose these token represen-
tations into a parse tree. Transition based depen-
dency parsers (Nivre, 2009; Chen and Manning,
2014; Andor et al., 2016) produce a well-formed
tree by predicting and executing a series of shift-
reduce actions, whereas graph-based parsers (Mc-

[ro
ot]

My dog als
o

like
s

eat
ing

sau
sag

e

.

Heads

My

dog

als
o

like
s

eat
ing

sau
sag

e

.

D
e
p
e
n
d
e
n
ts

nmod

nsubj

advmod

root

xcomp

dobj

punct

Figure 1: Receptive field for predicting the head-
dependent relationship between likes and eating.
Darker cell indicates more layers include that
cell’s representation. Heads and labels corre-
sponding to gold tree are indicated.

Donald et al., 2005; Kiperwasser and Goldberg,
2016; Dozat and Manning, 2017) generally em-
ploy attention to produce marginals over each pos-
sible edge in the graph, followed by a dynamic
programming algorithm to find the most likely tree
given those marginals.

Because of their dependency on sequential pro-
cessing of the sentence, none of these architectures
fully exploit the massive parallel processing ca-
pability that GPUs possess. If we wish to max-
imize GPU resources, graph-based dependency
parsers are more desirable than their transition-
based counterparts since attention over the edge-
factored graph can be parallelized across the entire
sentence, unlike the transition-based parser which
must sequentially predict and perform each transi-
tion. By encoding token-level representations with

1



an Iterated Dilated CNN (ID-CNN) (Strubell et al.,
2017), we can also remove the sequential depen-
dencies of the RNN layers. Unlike Strubell et al.
(2017) who use 1-dimensional convolutions over
the sentence to produce token representations, our
network employs 2-dimensional convolutions over
the adjacency matrix of the sentence’s parse tree,
modeling attention from the bottom up. By train-
ing with an objective that encourages our model
to predict trees using only simple matrix opera-
tions, we additionally remove the additional com-
putational cost of dynamic programming infer-
ence. Combining all of these ideas, we present
Dilated Iterated Graph CNNs (DIG-CNNs): a
combined convolutional neural network architec-
ture and training objective for efficient, end-to-end
GPU graph-based dependency parsing.

We demonstrate the efficacy of these models in
experiments on English Penn TreeBank, in which
our models perform similarly to the state-of-the-
art.

2 Dilated Convolutions

Though common in other areas such as computer
vision, 2-dimensional convolutions are rarely used
in NLP since it is usually unclear how to pro-
cess text as a 2-dimensional grid. However, 2-
dimensional convolutional layers are a natural
model for embedding the adjacency matrix of a
sentence’s parse.

A 2-dimensional convolutional neural network
layer transforms each input element, in our case
an edge in the dependency graph, as a linear func-
tion of the width rw and height rh window of sur-
rounding input elements (other possible edges in
the dependency graph). In this work we assume
square convolutional windows: rh = rw.

Dilated convolutions perform the same opera-
tion, except rather than transforming directly adja-
cent inputs, the convolution is defined over a wider
input window by skipping over δ inputs at a time,
where δ is the dilation width. A dilated convo-
lution of width 1 is equivalent to a simple con-
volution. Using the same number of parameters
as a simple convolution with the same radius, the
δ > 1 dilated convolution incorporates broader
context into the representation of a token than a
simple convolution.

2.1 Iterated Dilated CNNs
Stacking many dilated CNN layers can easily in-
corporate information from a whole sentence. For
example, with a radius of 1 and 4 layers of dilated
convolutions, the effective input window size for
each token is width 31, which exceeds the average
sentence length (23) in the Penn TreeBank corpus.
However, simply increasing the depth of the CNN
can cause considerable over-fitting when data is
sparse relative to the growth in model parameters.
To address this, we employ Iterated Dilated CNNs
(ID-CNNs) (Strubell et al., 2017), which instead
apply the same small stack of dilated convolutions
repeatedly, each time taking the result of the last
stack as input to the current iteration. Applying
the parameters recurrently in this way increases
the size of the window of context incorporated
into each token representation while allowing the
model to generalize well. Their training objec-
tive additionally computes a loss for the output of
each application, encouraging parameters that al-
low subsequent stacks to resolve dependency vio-
lations from their predecessors.

3 Dilated Iterated Graph CNNs

We describe how to extend ID-CNNs (Strubell
et al., 2017) to 2-dimensional convolutions over
the adjacency matrix of a sentence’s parse tree,
allowing us to model the parse tree through
the whole network, incorporating evidence about
nearby head-dependent relationships in every
layer of the network, rather than modeling at
the token level followed by a single layer of at-
tention to produce head-dependent compatibilities
between tokens. ID-CNNs allow us to efficiently
incorporate evidence from the entire tree without
sacrificing generalizability.

3.1 Model architecture
Let x = [x1, . . . , xT ] be our input text1 Let
y = [y1, . . . , yT ] be labels with domain size D for
the edge between each token xi and its head xj .
We predict the most likely y, given a conditional
model P (y|x) where the tags are conditionally in-
dependent given some features for x:

P (y|x) =
T∏
t=1

P (yt|F (x)), (1)

1In practice, we include a dummy root token at the begin-
ning of the sentence which serves as the head of the root. We
do not predict a head for this dummy token.

2



The local conditional distributions of Eqn. (1)
come from a straightforward extension of ID-
CNNs (Strubell et al., 2017) to 2-dimensional con-
volutions. This network takes as input a sequence
of T vectors xt, and outputs a T × T matrix of
per-class scores hij for each pair of tokens in the
sentence.

We denote the kth dilated convolutional layer of
dilation width δ as D(k)δ . The first layer in the net-
work transforms the input to a graph by concate-
nating all pairs of vectors in the sequence xi,xj
and applying a 2-dimensional dilation-1 convolu-
tion D(0)1 to form an initial edge representation
c(0)ij for each token pair:

cij(0) = D
(0)
1 [xi;xj] (2)

We denote vector concatenation with [·; ·]. Next,
Lc layers of dilated convolutions of exponentially
increasing dilation width are applied to cij(0),
folding in increasingly broader context into the
embedded representation of eij at each layer. Let
r() denote the ReLU activation function (Glorot
et al., 2011). Beginning with ct(0) = it we define
the stack of layers with the following recurrence:

cij(k) = r
(
D

(k−1)
2Lc−1ct

(k−1)
)

(3)

and add a final dilation-1 layer to the stack:

cij(Lc+1) = r
(
D

(Lc)
1 ct

(Lc)
)

(4)

We refer to this stack of dilated convolutions as a
block B(·), which has output resolution equal to
its input resolution. To incorporate even broader
context without over-fitting, we avoid making B
deeper, and instead iteratively apply B Lb times,
introducing no extra parameters. Starting with
bt(1) = B (it), we define the output of block m:

bij(m) = B
(
bt(m−1)

)
(5)

We apply a simple affine transformationWo to this
final representation to obtain label scores for each
edge eij:

hij(Lb) = Wobt(Lb) (6)

We can obtain the most likely head (and its la-
bel) for each dependent by computing the argmax
over all labels for all heads for each dependent:

ht = arg max
j

hij(Lb) (7)

3.2 Training
Our main focus is to apply the DIG-CNN as fea-
ture extraction for the conditional model described
in Sec. 3.1, where tags are conditionally indepen-
dent given deep features, since this will enable
prediction that is parallelizable across all possi-
ble edges. Here, maximum likelihood training is
straightforward because the likelihood decouples
into the sum of the likelihoods of independent lo-
gistic regression problems for every edge, with
natural parameters given by Eqn. (6):

1
T

T∑
t=1

logP (yt | ht) (8)

We could also use the DIG-CNN as input fea-
tures for an MST parser, where the partition func-
tion and its gradient are computed using Kirch-
hoffs Matrix-Tree Theorem (Tutte, 1984), but
our aim is to approximate inference in a tree-
structured graphical model using greedy inference
and expressive features over the input in order to
perform inference as efficiently as possible on a
GPU.

To help bridge the gap between these two tech-
niques, we use the training technique described in
(Strubell et al., 2017). The tree-structured graph-
ical model has preferable sample complexity and
accuracy since prediction directly reasons in the
space of structured outputs. Instead, we com-
pile some of this reasoning in output space into
DIG-CNN feature extraction. Instead of explicit
reasoning over output labels during inference, we
train the network such that each block is predictive
of output labels. Subsequent blocks learn to cor-
rect dependency violations of their predecessors,
refining the final sequence prediction.

To do so, we first define predictions of the
model after each of the Lb applications of the
block. Let ht(m) be the result of applying the ma-
trix Wo from (6) to bt(m), the output of block m.
We minimize the average of the losses for each ap-
plication of the block:

1
Lb

Lb∑
k=1

1
T

T∑
t=1

logP (yt | ht(m)). (9)

By rewarding accurate predictions after each
application of the block, we learn a model where
later blocks are used to refine initial predictions.
The loss also helps reduce the vanishing gradient
problem (Hochreiter, 1998) for deep architectures.

3



We apply dropout (Srivastava et al., 2014) to the
raw inputs xij and to each block’s output bt(m) to
help prevent overfitting.

4 Related work

Currently, the most accurate parser in terms of
labeled and unlabeled attachment scores is the
neural network graph-based dependency parser of
Dozat and Manning (2017). Their parser builds
token representations with a bidirectional LSTM
over word embeddings, followed by head and de-
pendent MLPs. Compatibility between heads and
dependents is then scored using a biaffine model,
and the highest scoring head for each dependent is
selected.

Previously, (Chen and Manning, 2014) pio-
neered neural network paring with a transition-
based dependency parser which used features from
a fast feed-forward neural network over word, to-
ken and label embeddings. Many improved upon
this work by increasing the size of the network
and using a structured training objective (Weiss
et al., 2015; Andor et al., 2016). (Kiperwasser
and Goldberg, 2016) were the first to present a
graph-based neural network parser, employing an
MLP with bidirectional LSTM inputs to score
arcs and labels. (Cheng et al., 2016) propose a
similar network, except with additional forward
and backward encoders to allow for conditioning
on previous predictions. (Kuncoro et al., 2016)
take a different approach, distilling a consensus of
many LSTM-based transition-based parsers into
one graph-based parser. (Ma and Hovy, 2017) em-
ploy a similar model, but add a CNN over char-
acters as an additional word representation and
perform structured training using the Matrix-Tree
Theorem. Hashimoto et al. (2017) train a large
network which performs many NLP tasks includ-
ing part-of-speech tagging, chunking, graph-based
parsing, and entailment, observing benefits from
multitasking with these tasks.

Despite their success in the area of computer
vision, in NLP convolutional neural networks
have mainly been relegated to tasks such as sen-
tence classification, where each input sequence
is mapped to a single label (rather than a la-
bel for each token) Kim (2014); Kalchbrenner
et al. (2014); Zhang et al. (2015); Toutanova et al.
(2015). As described above, CNNs have also
been used to encode token representations from
embeddings of their characters, which similarly

perform a pooling operation over characters. Lei
et al. (2015) present a CNN variant where convo-
lutions adaptively skip neighboring words. While
the flexibility of this model is powerful, its adap-
tive behavior is not well-suited to GPU accelera-
tion.

More recently, inspired by the success of deep
dilated CNNs for image segmentation in com-
puter vision (Yu and Koltun, 2016; Chen et al.,
2015), convolutional neural networks have been
employed as fast models for tagging, speech gen-
eration and machine translation. (van den Oord
et al., 2016) use dilated CNNs to efficiently gen-
erate speech, and Kalchbrenner et al. (2016) de-
scribes an encoder-decoder model for machine
translation which uses dilated CNNs over bytes
in both the encoder and decoder. Strubell et al.
(2017) first described the one-dimensional ID-
CNN architecture which is the basis for this work,
demonstrating its success as a fast and accurate
NER tagger. Gehring et al. (2017) report state-of-
the-art results and much faster training from using
many CNN layers with gated activations as en-
coders and decoders for a sequence-to-sequence
model. While our architecture is similar to the
encoder architecture of these models, ours is dif-
ferentiated by (1) being tailored to smaller-data
regimes such as parsing via our iterated architec-
ture and loss, and (2) employing two-dimensional
convolutions to model the adjacency matrix of the
parse tree. We are the first to our knowledge to
use dilated convolutions for parsing, or to use two-
dimensional dilated convolutions for NLP.

5 Experimental Results

5.1 Data and Evaluation

We train our parser on the English Penn Tree-
Bank on the typical data split: training on sec-
tions 2–21, testing on section 23 and using sec-
tion 22 for development. We convert constituency
trees to dependencies using the Stanford depen-
dency framework v3.5 (de Marneffe and Man-
ning, 2008), and use part-of-speech tags from the
Stanford left3words part-of-speech tagger. As is
the norm for this dataset, our evaluation excludes
punctuation. Hyperparameters that resulted in the
best performance on the validation set were se-
lected via grid search. A more detailed descrip-
tion of optimization and data pre-processing can
be found in the Appendix.

4



Model UAS LAS
Kiperwasser and Goldberg (2016) 93.9 91.9
Cheng et al. (2016) 94.10 91.49
Kuncoro et al. (2016) 94.3 92.1
Hashimoto et al. (2017) 94.67 92.90
Ma and Hovy (2017) 94.9 93.0
Dozat and Manning (2017) 95.74 94.08
DIG-CNN 93.70 91.72
DIG-CNN + Eisner 94.03 92.00

Table 1: Labeled and unlabeled attachment scores
of our model compared to state-of-the-art graph-
based parsers

5.2 English PTB Results

We compare our models labeled and unlabeled at-
tachment scores to the neural network graph-based
dependency parsers described in Sec. 4. Without
enforcing trees at test time, our model performs
just under the LSTM-based parser of Kiperwasser
and Goldberg (2016), and a few points lower than
the state-of-the-art. When we post-process our
model’s outputs into trees, like all the other mod-
els in our table, our results increase to perform
slightly above Kiperwasser and Goldberg (2016).

We believe our model’s relatively poor perfor-
mance compared to existing models is due to its
limited incorporation of context from the entire
sentence. While each bidirectional LSTM token
representation observes all tokens in the sentence,
our reported model observes a relatively small
window, only 9 tokens. We hypothesize that this
window is not sufficient for producing accurate
parses. Still, we believe this is a promising archi-
tecture for graph-based parsing, and with further
experimentation could meet or exceed the state-
of-the-art while running faster by better leveraging
GPU architecture.

6 Conclusion

We present DIG-CNNs, a fast, end-to-end convo-
lutional architecture for graph-based dependency
parsing. Future work will experiment with deeper
CNN architectures which incorporate broader sen-
tence context in order to increase accuracy without
sacrificing speed.

Acknowledgments

We thank Patrick Verga and David Belanger for
helpful discussions. This work was supported in

part by the Center for Intelligent Information Re-
trieval, in part by DARPA under agreement num-
ber FA8750-13-2-0020, in part by Defense Ad-
vanced Research Agency (DARPA) contract num-
ber HR0011-15-2-0036, in part by the National
Science Foundation (NSF) grant number DMR-
1534431, and in part by the National Science
Foundation (NSF) grant number IIS-1514053.
The U.S. Government is authorized to reproduce
and distribute reprints for Governmental purposes
notwithstanding any copyright notation thereon.
Any opinions, findings and conclusions or recom-
mendations expressed in this material are those of
the authors and do not necessarily reflect those of
the sponsor.

References
Daniel Andor, Chris Alberti, David Weiss, Aliaksei

Severyn, Alessandro Presta, Kuzman Ganchev, Slav
Petrov, and Michael Collins. 2016. Globally nor-
malized transition-based neural networks. In Pro-
ceedings of the 54th Annual Meeting of the Associa-
tion for Computational Linguistics.

Danqi Chen and Christopher D. Manning. 2014. A fast
and accurate dependency parser using neural net-
works. In EMNLP.

Liang-Chieh Chen, George Papandreou, Iasonas
Kokkinos, Kevin Murphy, and Alan L. Yuille. 2015.
Semantic image segmentation with deep convolu-
tional nets and fully connected crfs. In ICLR.

Hao Cheng, Hao Fang, Xiaodong He, Jianfeng Gao,
and Li Deng. 2016. Bi-directional attention with
agreement for dependency parsing. In EMNLP.

Marie-Catherine de Marneffe and Christopher D. Man-
ning. 2008. The stanford typed dependencies rep-
resentation. In COLING 2008 Workshop on Cross-
framework and Cross-domain Parser Evaluation.

Timothy Dozat and Christopher D. Manning. 2017.
Deep biaffine attention for neural dependency pars-
ing. In ICLR.

Jonas Gehring, Michael Auli, David Grangier, Denis
Yarats, and Yann N. Dauphin. 2017. Convolutional
sequence to sequence learning. arXiv preprint:
arXiv:1705.03122 .

Xavier Glorot, Antoine Bordes, and Yoshua Bengio.
2011. Deep sparse rectifier neural networks. In AIS-
TATS.

Kazuma Hashimoto, Caiming Xiong, Yoshimasa Tsu-
ruoka, and Richard Socher. 2017. A joint many-task
model: Growing a neural network for multiple nlp
tasks. arXiv preprint: arXiv:1611.01587 .

5



Sepp Hochreiter. 1998. The vanishing gradient prob-
lem during learning recurrent neural nets and prob-
lem solutions. International Journal of Uncer-
tainty, Fuzziness and Knowledge-Based Systems
6(02):107–116.

Nal Kalchbrenner, Lasse Espeholt, Karen Simonyan,
Aaron van den Oord, Alex Graves, and Koray
Kavukcuoglu. 2016. Neural machine translation in
linear time. arXiv preprint arXiv:1610.10099 .

Nal Kalchbrenner, Edward Grefenstette, and Phil Blun-
som. 2014. A convolutional neural network for
modelling sentences. In Proceedings of the 52nd
Annual Meeting of the Association for Computa-
tional Linguistics.

Yoon Kim. 2014. Convolutional neural networks for
sentence classification. In EMNLP.

Eliyahu Kiperwasser and Yoav Goldberg. 2016. Sim-
ple and accurate dependency parsing using bidirec-
tional lstm feature representations. Transactions
of the Association for Computational Linguistics
4:313–327.

Adhiguna Kuncoro, Miguel Ballesteros, Lingpeng
Kong, Chris Dyer, and Noah A. Smith. 2016. Dis-
tilling an ensemble of greedy dependency parsers
into one mst parser. In EMNLP.

Tao Lei, Regina Barzilay, and Tommi Jaakkola. 2015.
Molding cnns for text: non-linear, non-consecutive
convolutions. Empirical Methods in Natural Lan-
guage Processing .

Xuezhe Ma and Eduard Hovy. 2017. Neural proba-
bilistic model for non-projective mst parsing. arXiv
preprint: 1701.00874 .

Ryan McDonald, Fernando Pereira, Kiril Ribarov,
and Jan Hajic. 2005. Non-projective depen-
dency parsing using spanning tree algorithms. In
Proc. Human Language Technology Conf. and
Conf. Empirical Methods Natural Language Pro-
cess. (HLT/EMNLP). pages 523–530.

Joakim Nivre. 2009. Non-projective dependency pars-
ing in expected linear time. In Proceedings of the
47th Annual Meeting of the ACL and the 4th IJC-
NLP of the AFNLP.

Nitish Srivastava, Geoffrey E Hinton, Alex Krizhevsky,
Ilya Sutskever, and Ruslan Salakhutdinov. 2014.
Dropout: a simple way to prevent neural networks
from overfitting. Journal of Machine Learning Re-
search 15(1):1929–1958.

Emma Strubell, Patrick Verga, David Belanger, and
Andrew McCallum. 2017. Fast and accurate se-
quence labeling with iterated dilated convolutions.
arXiv preprint: arXiv:1702.02098 .

Kristina Toutanova, Danqi Chen, Patrick Pantel, Hoi-
fung Poon, Pallavi Choudhury, and Michael Gamon.
2015. Representing text for joint embedding of text

and knowledge bases. In Proceedings of the 2015
Conference on Empirical Methods in Natural Lan-
guage Processing. Association for Computational
Linguistics, pages 1499–1509.

William Thomas Tutte. 1984. Graph theory, vol-
ume 11. Addison-Wesley Menlo Park.

Aaron van den Oord, Sander Dieleman, Heiga Zen,
Karen Simonyan, Oriol Vinyals, Alex Graves,
Nal Kalchbrenner, Andrew Senior, and Koray
Kavukcuoglu. 2016. Wavenet: A generative model
for raw audio. arXiv preprint arXiv:1609.03499 .

David Weiss, Chris Alberti, Michael Collins, and Slav
Petrov. 2015. Structured training for neural network
transition-based parsing. In Annual Meeting of the
Association for Computational Linguistics.

Fisher Yu and Vladlen Koltun. 2016. Multi-scale con-
text aggregation by dilated convolutions. In Inter-
national Conference on Learning Representations
(ICLR).

Xiang Zhang, Junbo Zhao, and Yann LeCun. 2015.
Character-level convolutional networks for text clas-
sification. In Advances in Neural Information Pro-
cessing Systems 28 (NIPS).

6


