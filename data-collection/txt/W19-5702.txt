

















































Sensing Tree Automata as a Model of Syntactic Dependencies

Thomas Graf
Department of Linguistics
Stony Brook University
Stony Brook, NY, USA

mail@thomasgraf.net

Aniello De Santo
Department of Linguistics
Stony Brook University
Stony Brook, NY, USA

aniello.desanto@stonybrook.edu

Abstract
Various aspects of syntax have recently been
characterized in subregular terms. However,
these characterizations operate over very dif-
ferent representations, including string encod-
ings of c-command relations as well as tiers
projected from derivation trees. We present
a way to unify these approaches via sensing
tree automata over Minimalist grammar de-
pendency trees. Sensing tree automata are de-
terministic top-down tree automata that may
inspect the labels of all daughter nodes be-
fore assigning them specific states. It is al-
ready known that these automata cannot cor-
rectly enforce all movement dependencies in
Minimalist grammars, but we show that this
result no longer holds if one takes into account
several well-established empirical restrictions
on movement. Sensing tree automata thus fur-
nish a strong yet uniform upper bound on the
complexity of syntactic dependencies.

1 Introduction

This paper proposes a novel, unified upper bound
on the subregular complexity of syntactic depen-
dencies. Since the merit of this result might be
opaque to a reader who is not intimately familiar
with the most recent developments in subregular
complexity, we start out with a very detailed back-
ground discussion.

The subregular program seeks to identify for-
mal machinery that provides a tighter characteri-
zation of natural language than the familiar classes
of the Chomsky hierarchy. Subregular phonology
took as its vantage point the well-known result that
phonology is regular (Johnson, 1972; Kaplan and
Kay, 1994) and then identified proper subclasses
that are still powerful enough for specific types
of phonological dependencies. Among these sub-
classes are SL, SP (Rogers et al., 2010), IBSP
(Graf, 2017, 2018a), TSL (Heinz et al., 2011; Mc-
Mullin, 2016), and several extensions of the latter

(Baek, 2018; Graf and Mayer, 2018). The highly
restrictive nature of these classes has allowed for
new learning algorithms (Heinz et al., 2012; Jar-
dine and McMullin, 2017) and also furnishes com-
putational explanations for typological gaps.

Syntax cannot be subregular in this strict sense
by virtue of being at least mildly context-sensitive
(Huybregts, 1984; Shieber, 1985; Michaelis and
Kracht, 1997; Kobele, 2006, a.o.). However, lin-
guists formulate syntactic dependencies over trees,
not strings. This shift in perspective has also taken
place within formal grammar, first with model-
theoretic syntax (Blackburn et al. 1993; Back-
ofen et al. 1995; Cornell and Rogers 1998; Rogers
1998, 2003, a.o.) and then the two-step approach
(see Morawietz 2003, Mönnich 2006, and ref-
erences therein). The two-step approach high-
lighted how the mildly context-sensitive nature of
syntax arises from the interaction of two finite-
state components: a regular tree language that en-
codes a kind of “deep structure” and a finite-state
tree transduction to the intended “surface struc-
ture”. This perspective has proven particularly
fruitful in Minimalist grammars (MGs; Stabler,
1997, 2011a), where derivation trees provide the
regular tree language and the transduction is a for-
mal analogue to Chomsky’s notion of movement
(Kobele et al., 2007; Graf, 2012b). With the regu-
lar nature of syntax properly identified, a subregu-
lar characterization of syntax is suddenly feasible.

Graf (2012a) provided the first subregular
(un)definability results for MG derivation tree lan-
guages, but these results were expanded on only
recently. Notably, all follow-up work strived to re-
main closer to the classes that enjoy prominence in
subregular phonology. This, however, also led to
a marked divergence in approaches. Graf (2018b)
operates directly over MG derivation trees. Fol-
lowing the tradition of model-theoretic syntax,
Graf equates the MG operations Merge and Move



with constraints on MG derivation tree languages
and shows that they belong to the tree analogue
of the subregular string class TSL. This view is
also adopted by Vu (2018) and Vu et al. (2019) in
the analysis of negative-polarity items and case li-
censing, respectively. Graf and Shafiei (2019) and
Shafiei and Graf (2019), on the other hand, pursue
a purely string-based perspective of syntactic de-
pendencies. For each node they identify its string
of c-commanders, the shape of which must fol-
low the constraints imposed by, say, Principle A
or NPI-licensing. When construed as such string
constraints, syntactic licensing conditions not only
turn out to be subregular, they also fit into classes
that have been proposed for subregular phonology.
The work so far thus has unearthed two distinct
subregularity results: MG operations are subreg-
ular over MG derivation trees, and licensing con-
ditions are subregular over a specific string repre-
sentation grounded in c-command (cf. Frank and
Vijay-Shanker, 2001).

Even though each perspective is worthwhile and
has proven very fruitful, their apparent incom-
mensurability raises the question how these two
notions of subregularity can be brought to bear
on each other. The central contribution of our
paper is a uniform upper bound on syntax that
encompasses both MG operations and licensing
conditions. This upper bound takes the form of
sensing tree automata (STAs) operating over de-
pendency tree representations of MG derivations
(these dependency trees are distinct from the MG
dependency trees of Boston et al. 2010). STAs
provide a minimal amount of look-ahead to de-
terministic top-down tree automata: the automa-
ton may inspect the labels of all daughter nodes
before assigning them specific states. Far more
than just a mathematical curiosity, limiting syntax
to STA-recognizable constraints over MG depen-
dency trees is very natural in several respects:

1. MG dependency trees are a natural encoding
of head-argument relations.

2. STAs explain why licensing conditions are
mediated by c-command instead of the many
alternative command relations one could
imagine (Barker and Pullum, 1990).

3. In order for movement to be regulated by
STAs, it must obey additional restrictions be-
yond those of the standard MG formalism.
These restrictions coincide with well-known

empirical phenomena such as the Specifier
Island Constraint and the Coordinate Struc-
ture Constraint.

4. As a single STA can handle movement and li-
censing conditions at the same time, it is un-
surprising that the two occasionally interact,
e.g. when movement induces a licensing con-
figuration or violates one.

5. Since STAs are deterministic top-down au-
tomata with a minimal amount of look-ahead,
they are a natural match for top-down pars-
ing, which has been argued to play a central
role in human sentence processing (Stabler,
2013; Kobele et al., 2013; Graf et al., 2017).

Hence our contribution does not merely unify
the two existing approaches to subregular syntax,
it also accounts for empirical aspects of syntax that
the latter leave unexplained. At the same time, we
do not intend for our perspective to supplant the
existing ones. Each one provides useful insights
and can now be safely pursued in the knowledge
that there is a principled formal connection to the
other approaches.

The paper is laid out as follows: Section 2 in-
troduces our mathematical notation for trees and
tree languages (§2.1), which form the basis for our
definition of STAs (§2.2). We then define MGs and
their dependency trees in §3. In order to simplify
some of the subsequent proofs, we do not follow
the standard definitions and instead adopt a for-
mat that is partly inspired by Kobele et al. (2007).
Sections 4 and 5 cover the two core results of this
paper: dependency trees of MGs with the Spec-
ifier Island Constraint are STA-recognizable, and
so are licensing conditions based on c-command.
The last section briefly sketches some extensions
of these results, including interactions of move-
ment and c-command (§6.1), subcommand (§6.2),
recognizability of MG derivation trees (§6.3), ad-
junct islands and the coordinate structure con-
straint (§6.4, §6.5), and connections to top-down
parsing (§6.6).

2 Preliminaries

2.1 Trees and Tree Languages

A ranked alphabet Σ is a finite set of symbols,
each one of which has a rank or arity assigned by
the function r : Σ → N. We write Σ(n) to de-
note {σ ∈ Σ | r(σ) = n}, and σ(n) indicates that



σ has rank n. Given a ranked alphabet Σ, the set
T (Σ) of Σ-trees contains all σ(0) and all terms
σ(n)(t1, . . . , tn) (n ≥ 0) such that t1, . . . , tn ∈
T (Σ).

Example 1. Given Σ :=
{
a(0), b(0), c(2), d(2)

}
,

T (Σ) is an infinite sets that contains, among oth-
ers, the term d(c(b, b), d(b, a)). This term corre-
sponds to the tree below:

d

c

b b

d

b a

If Σ consisted only of c(2) and d(2), then T (Σ)
would be empty. y

Given a term m(n)(s1, . . . , sn) where each si is
a subtree with root di, we call m the mother of
the daughters d1, . . . , dn (1 ≤ i ≤ n). If two
distinct nodes have the same mother, they are sib-
lings. We use the term proper dominance for the
transitive closure of the mother-of relation, and re-
flexive dominance for the reflexive, transitive clo-
sure. A node a is an ancestor of node n iff a prop-
erly dominates n.

Every node u in a Σ-tree has a unique address
g(u) as defined in Gorn (1967). The subtree of Σ-
tree t rooted in Gorn address g(u) is denoted by
t/g(u). Given two Σ-trees s and t and node u in
t, t[g(u)← s] is the result of replacing t/g(u) in t
by s. Throughout the paper, we write u instead of
g(u) so that t/u and t[u ← s] are shorthands for
t/g(u) and t[g(u)← s], respectively.

2.2 Sensing Tree Automata
A sensing tree automaton (STA; Martens et al.,
2008) is a deterministic top-down tree automaton
that may also take the labels of a node’s daugh-
ters into account before assigning states to them.
In other words, these automata have a finite look-
ahead of 1. This intuition is reflected in our defini-
tion of sensing rules below. Following the format
of Comon et al. (2008, p. 38) for top-down tree
automata, we deliberately define sensing rules in
a tree transducer format. This is a marked devia-
tion from Martens et al. (2008), but it should make
it easier for future work to extend our perspective
from constraints to transformations.

Let Σ be some ranked alphabet and Q a set of
symbols of rank 1 disjoint from Σ. Members of
Q are called states. In addition, X is a countably
infinite set of variable symbols (X ∩ Σ = X ∩

Q = ∅). For the rest of this section, we use ~σi as
a shorthand for σi(xi1 , . . . , xin), with σi ∈ Σ(n)
and xi1 , . . . , xin ∈ X .

Definition 1 (Sensing rule). A sensing rule
over alphabet Σ and state set Q is an ex-
pression of the form q(σ(n)( ~σ1, . . . , ~σn)) →
σ(n)(q1( ~σ1), . . . , qn( ~σn)) such that
σ, σ1, . . . , σn ∈ Σ and q, q1, . . . , qn ∈ Q. y

Note that for σ(0), sensing rules are of the form
q(σ(0))→ σ(0). Such sensing rules effectively re-
move states from leaf nodes, whereas sensing rules
for σ(≥1) remove the state of σ and instead add a
state on top of each daughter of σ.

Definition 2 (Sensing relation). Let t and t′ be in
T (Σ∪Q), and suppose δ is some sensing rule over
Σ and Q that has the form q(σ(n)( ~σ1, . . . , ~σn))→
σ(n)(q1( ~σ1), . . . , qn( ~σn)). Then the relation →δ
holds between t and t′ (t →δ t′) iff t con-
tains a subtree s := q(σ(n)(s1, . . . , sn)) such
that each si (1 ≤ i ≤ n) is a Σ-tree with root
σi, and t′ is the result of replacing s in t with
σ(n)(q1(s1), . . . , qn(sn)).

Given a set ∆ of sensing rules, we write t →∆
t′ iff t →δ t′ for some δ ∈ ∆. The reflexive,
transitive closure of→∆ is denoted by→∗∆. y

Intuitively, t →∗∆ t′ iff t′ can be obtained from t
by a sequence of sensing rules.

Definition 3 (Sensing tree automaton). A sens-
ing tree automaton (STA) over Σ is a 4-tuple
A := 〈Q,Σ, qI ,∆〉 such that Q is a finite set of
states disjoint from alphabet Σ, qI ∈ Q is the ini-
tial state, and ∆ is a finite set of sensing rules over
Σ and Q. The tree language recognized by A is
L(A) := {t ∈ T (Σ) | qI(t)→∗∆ t}. The class of
all tree languages that are recognized by at least
one STA is called STA. y

An STA thus recognizes a tree t iff there is a se-
quence of sensing rules that starts from the initial
state qI and passes states through t until they are
all removed again at the leaves of t. If the STA
ever gets stuck because there is no suitable sens-
ing rule, t is rejected.

Example 2. Suppose Σ :=
{
a(0), b(0), c(2), d(2)

}
,

and consider the STA with Q := {0, 1}, where 0
is the initial state. The automaton uses all sensing
rules of the following form:

• 1(σ( ~σ1, ~σ2))→ σ(1( ~σ1), 1( ~σ2)),
for σ ∈ {c, d} and σ1, σ2 ∈ {a, b, c, d},



• 0(σ( ~σ1, ~σ2))→ σ(0( ~σ1), 0( ~σ2)),
for σ ∈ {c, d} and σ1, σ2 ∈ {a, b, d},

• 0(σ( ~σ1, ~σ2))→ σ(q0( ~σ1), q1( ~σ2)),
for σ ∈ {c, d}, σ1, σ2 ∈ {a, b, c, d}, and
qi := 1 iff σ2−i := c (where i ∈ {0, 1}),

• q(b)→ b,
for q ∈ {0, 1},

• 1(a)→ a.

The STA only accepts those Σ-trees where each a
is c-commanded by some c (that is to say, c must
be the sibling of an ancestor of a). It thus emulates
a simplified version of Principle A. y

It will also be convenient in some proofs to em-
ploy a substitution-based characterization of the
tree languages that are recognized by STAs. The
characterization capitalizes on the fact that every
STA only has a look-ahead of 1. Consequently, the
state it assigns to a node n depends only on three
components: I) the label of n and its siblings, II)
the states assigned to n’s ancestors, and III) the
states assigned to the siblings of each ancestor. If
all of those are kept constant, nwill always receive
the same state no matter what the rest of the tree
looks like.

Definition 4 (Spine closure). Given a node u of
some Σ-tree t, lsibt(u) is the string consisting of
the label of u’s left siblings (if they exist) followed
by the label of u. Analogously, rsibt(u) is the
string consisting of the label of u and the label
of its right siblings (if they exist). Also, 5 and
⇓ are two distinguished symbols not in Σ.1 Let
u1, . . . , un be the shortest path of nodes extending
from the root of t to u. That is to say, each ui is
the mother of ui+1 (1 ≤ i < n), u1 is the root, and
un = u. By spinet(u) we denote the string re-
cursively defined by spinet(u1) = u1 5 u1 and
spinet(u1, . . . , un) = spine

t(u1, . . . , un−1) ⇓
lsibt(un) 5 rsibt(un) A regular tree language L
is spine-closed iff it holds for all trees s, t ∈ L and
nodes u and v belonging to s and t, respectively,
that spines(u) = spinet(v) implies s[u ← t/
v] ∈ L. y

Theorem 1 (Martens 2006). A regular tree lan-
guage L belongs to the class STA iff L is spine-
closed.

1Martens et al. (2008) use # instead of ⇓. We prefer the
latter as it emphasizes visually that this symbol marks the
start of a string at the next lower level in the tree.

Example 3. Consider the left tree l and right tree
r in Fig. 1. Let l0 and r0 be the left daugh-
ter of the root in l and r, respectively. Then
spinel(l0) = spiner(r0) = merge 5 merge ⇓
merge 5 merge merge. Hence a tree language
that contains both l and r is an STA language iff it
also contains r[r0← r/l0]. y

In linguistic terms, spine-closure tells us that
two subtrees s/u and t/v with identical root la-
bels can be freely exchanged whenever they have
the same ancestors and the same c-commanders.
This will be of great importance throughout this
paper.

3 Minimalist Grammars

Minimalist grammars (MGs; Stabler, 1997) are
a formalization of Minimalist syntax (Chomsky,
1995). Readers who are unfamiliar with the for-
malism should consult Stabler (2011a) for a more
accessible introduction.

Every MG consists of a finite set of feature an-
notated lexical items. Each lexical item is a pair
of a phonetic exponent and a finite, non-empty
string of features. There are four types of features,
whose job it is to trigger the structure-building
operations Merge and Move. Merge establishes
head-argument relations and is triggered when a
selector feature F+ on a head finds a matching
category feature F− on an argument. For exam-
ple, the noun guest carries a feature N−, for which
we also write guest :: N−. It can be merged with
the :: N+D− to yield a DP, thanks to the match-
ing category and selector features. Move displaces
a subtree from its current position to a higher po-
sition in the syntactic structure. Move takes place
when a licensor features f+ on a head that pro-
vides a landing site can be checked by a corre-
sponding licensee feature f− on a mover. The or-
der of features on a lexical item determines the or-
der in which the corresponding operations are trig-
gered. Hence the determiner which :: N+D−wh−

would first select a noun phrase, get merged with
a head looking for a DP, and then undergo wh-
movement.

The sequence of Merge and Move steps is com-
monly represented as a derivation tree, e.g. in
Fig. 2. However, we will use a dependency tree
representation instead. Our dependency trees are
merely a more compact encoding of MG deriva-
tion trees and have no connection to the MG de-
pendency trees of Boston et al. (2010). We will



merge

merge

a :: A− a :: A+A−

merge

a :: A− a :: A+A+A−

merge

merge

b :: B− b :: B+B−

merge

b :: B− b :: B+B+B−

Figure 1: MG derivation tree languages are not recognizable by STAs because they are not spine-closed.

directly define MGs as sets of well-formed depen-
dency trees, mirroring earlier definitions in terms
of derivation trees (primarily Kobele et al. 2007).

We pick a ranked alphabet Σ such that Σ(n) con-
tains all lexical items of MG G, and only those,
that carry exactly n selector features. For simplic-
ity, we use G to also refer to this alphabet. Not
every G-tree is a well-formed dependency tree,
though, due to the constraints of the MG feature
calculus. The calculus is illustrated in Fig. 2,
where each node in the dependency tree is anno-
tated with the feature configuration corresponding
to its subtree. We formalize this calculus via a re-
cursive function feat that computes these values
based on more primitive functions for the feature
checking steps that trigger Merge and Move.

Definition 5 (Dependency tree language). If G
is an MG with n distinct licensee features, then the
set dep(G) of well-formed dependency trees of G
is {t ∈ T (G) | feat(t) = 〈C−, ε1, . . . , εn〉}. y

The remainder of this section defines feat in
terms of feature checking operators M and ⊗ for
Move and Merge, respectively. All operations ma-
nipulate one or more sequences of feature strings.
The Shortest Move Constraint (SMC) will be used
to filter out illicit sequences.

Definition 6 (SMC). Let G be an MG and Lce its
set of n licensee features. Given a sequence s :=
s1, . . . , sm (m ≥ 0) of strings in Lce∗, SMC :
(Lce∗)∗ → (Lce∗)∗ is undefined for s if there are
si and sj (1 ≤ i 6= j ≤ m) that start with the
same licensee feature. Otherwise, SMC maps s to
s itself. y

The SMC ensures that Move is unambiguous in
the sense that there can never be more than one
active mover of a specific type (wh, topicalization,
and so on). It is an integral part of MGs, and
removing it would greatly alter their expressivity
(Salvati, 2011).

Next we add a helper function sort that orders
SMC-approved sequences based on the first li-
censee feature of each feature string.

Definition 7 (sort). Let G and Lce be as be-
fore. Now fix some bijection b between Lce and
{1, . . . , n}. Then sort maps s := s1, . . . , sm ∈
Lce∗ to the sequence s′1, . . . , s

′
n such that s

′
i := sj

if sj starts with f and b(f) = i; otherwise, s′i := ε
(1 ≤ i ≤ n, 1 ≤ j ≤ m). y

Now we can finally define M for Move, which
is also a crucial part of the Merge operator ⊗.
Throughout we use γ as a shorthand for any string
of features, and δ for a (possibly empty) string of
licensee features.

Definition 8 (Move). Suppose that expression e is
〈fγ, δ1, . . . , f−δi, . . . , δn〉 for some licensee fea-
ture f− and 1 ≤ i ≤ n. Then

M(e) := M(〈γ, sort(SMC(δ1, . . . , δi, . . . , δn))〉)

if f is the licensor feature f+, and e otherwise. y

Definition 9 (Merge). Given two ex-
pressions e := 〈fγ, δ1, . . . , δm〉 and
e′ := 〈f ′δ, δm+1, . . . , δz〉, e⊗ e′ is

M(〈γ, sort(SMC(δ, δ1, . . . , δm, δm+1, . . . , δz))〉)

if f is some selector feature F+ and f ′ the match-
ing category feature F−. In all other cases, ⊗ is
undefined. y

Note how Merge is always followed by an applica-
tion of the Move operator, but this does not trigger
any feature checking unless γ starts with a licen-
sor feature. Merge steps are thus interleaved with
movement checks, not all of which may actually
result in movement.

The operatorsM and⊗ on their own do not nar-
row down the set of G-trees. They are invoked as
part of a recursive function feat over G-trees that
computes the feature values of subtrees, as already
expressed in Def. 5.

Definition 10 (feat). The partial function feat re-
cursively maps MG dependency trees to fea-
ture expressions. For lexical items, feat(σ ::
φ) := 〈φ, ε1, . . . , εn〉. If t := σ(σ1, . . . , σz),
then feat(t) := ((feat(σ) ⊗ feat(σz)) ⊗ · · · ) ⊗
feat(σ1). y



CP

C TP

Johni T′

T VP

ti V′

wonders CP

DPj

which guest

C′

C TP

tj T′

T VP

tj left

merge

ε :: T+C− move

merge

ε :: V+nom+T− merge

John :: D−nom− merge

wonders :: C+D+V− move

merge

ε :: T+wh+C− move

merge

ε :: V+nom+T− merge

merge

which :: N+D−nom−wh− guest :: N−

left :: D+V−

ε :: T+C−

ε :: V+nom+T−

wonders :: C+D+V−

John :: D−nom− ε :: T+wh+C−

ε :: V+nom+T−

left :: D+V−

which :: N+D−nom−wh−

guest :: N−

〈C−, ε, ε〉

〈T−, ε, ε〉

〈V−,nom−, ε〉

〈C−, ε, ε〉

〈T−, ε,wh−〉

〈V−,nom−wh−, ε〉

〈D−nom−wh−, ε, ε〉

〈N−, ε, ε〉

〈D−nom−, ε, ε〉

Figure 2: X′-bar tree, corresponding MG derivation tree, and (feat-annotated) MG dependency tree

Two important lemmata follow immediately
from the preceding definitions.

Lemma 1. Let G be an MG and s a G-tree. Then
it holds for every t ∈ dep(G) with node u that
t[u← s] ∈ dep(G) iff feat(s) = feat(t/u).

Lemma 2. Let G be an MG with n distinct li-
censee features and s a subtree of some t ∈
dep(G). Then feat(s) must be of the form
〈F−δ, δ1, . . . , δn〉 for some category feature F−.

Lemma 2 is apparent from the derivation tree ex-
ample in Fig. 2. It is a minor extension of the
well-known fact that a lexical item may occur in
a well-formed MG derivation iff its feature string
is of the form φFδ, where φ is either ε or a se-
lector feature followed by 0 or more selector and
licensor features, F is a category feature, and δ is
a (possibly empty) string of licensee features.

4 Merge and Move via STAs

With all the preliminaries in place, we can fi-
nally turn to the core results regarding the STA-
recognizability of MGs with respect to Merge and
Move. The next sections then extend this to licens-
ing conditions and some other special cases.

Graf (2012a) uses the argument from exam-
ple 3 in §2.2 to prove that MG derivation tree
languages are not STA languages. However, this
proof does not carry over to MG dependency trees.
Adopting the terminology of Graf (2012a), we use
MDEP[merge,move] for the full class of MG de-

pendency tree languages and MDEP[merge] for the
subclass of movement-free MGs (no lexical item
carries any licensee features).

Theorem 2. MDEP[merge] ( STA

This is just a corollary of a more fundamen-
tal property of MGs. For any arbitrary L ∈
MDEP[merge] and nodes u and v of s, t ∈ L,
spines(u) = spinet(v) necessarily entails that
feat(s/u) = feat(t/v), so that Theorem 2 im-
mediately follows from Lemma 1. We omit a full
proof here as Lemma 3 will cover a more complex
case that subsumes this one.

Even with the dependency tree format, though,
STAs are too weak for standard MGs with both
Merge and Move.

Theorem 3. MDEP[merge,move] and STA are in-
comparable.

Proof. Consider the dependency trees l and r in
Fig. 3. The respective instances of the :: N+D−

have different values under feat (〈D−, ε〉 and
〈D−,wh−〉, respectively). By Lemma 1, then,
their subtrees are not interchangeable even though
spinel(the :: N+D−) = spiner(the :: N+D−). 2

The example in Fig. 3 is peculiar, though. The
left dependency tree encodes the derivation for
Who does a teacher of like the father of John,
which is severely degraded. It has been ar-
gued that such cases of left-branch subextraction
are generally forbidden. MGs can be equipped
with the Specifier Island Constraint (SpIC) to



ε :: T+wh+C−

does :: V+T−

like :: D+D+V−

a :: N+D−

teacher :: P+N−

of :: D+P−

who :: D−wh−

the :: N+D−

father :: P+N−

of :: D+P−

John :: D−

ε :: T+wh+C−

does :: V+T−

like :: D+D+V−

a :: N+D−

teacher :: P+N−

of :: D+P−

John :: D−

the :: N+D−

father :: P+N−

of :: D+P−

who :: D−wh−

Figure 3: Even though the boxed nodes have the same spine, their subtrees cannot be exchanged.

rule out movement from within a specifier. This
takes the form of an additional restriction on feat
that only allows complements to properly contain
unchecked licensee features. Since the comple-
ment of a head is its rightmost daughter in the de-
pendency tree (rather than the leftmost one), the
SpIC amounts to a restriction on all daughters ex-
cept the last one.

Definition 11 (SpIC). Suppose s1, . . . , sm areG-
trees and σ ∈ G(m). Then feat(σ(s1, . . . , sm)) is
undefined if there is an i < m and 1 ≤ j ≤ n such
that feat(si) is of the form 〈γ, δ1, . . . , δj , . . . , δn〉
and δj 6= ε.

Note that any licensee features on the head of a
specifier are part of γ, not δj , so specifiers can
still move without violating the SpIC. Only ex-
traction of a proper subtree from within a speci-
fier is not allowed. Also note that our version of
the SpIC only bans extraction from base specifiers,
but not from specifiers that are derived via move-
ment. This is why it can be easily stated over de-
pendency trees. Even so, this limited version of
the SpIC greatly limits the weak generative capac-
ity of MGs with the SMC, while still keeping them
mildly context-sensitive (Michaelis, 2004, 2009;
Kobele and Michaelis, 2011). For our purposes,
though, the major contribution of the SpIC is that
it also lowers the complexity of MG dependency
trees into STA.

Lemma 3. Given an MG G that obeys the SpIC,
pick arbitrary nodes u and v of s, t ∈ dep(G), re-
spectively. If spines(u) = spinet(v), then feat(s/
u) = feat(t/v).

Proof. Since s and t are well-formed, both
feat(s/u) and feat(t/v) must be defined. By

Lemma 2 we may assume w.l.o.g. that feat(s/
u) := 〈F−δ, δ1, . . . , δn〉 and feat(t/v) :=
〈F′−δ′, δ′1, . . . , δ′n〉. As u and v have identical
spines, u and v themselves must be identical.
Therefore both F− = F′− and δ = δ′ hold. Now
suppose that δi 6= δ′i for some 1 ≤ i ≤ n. Then
either s/u or t/v is missing a licensee feature f−

that is present in the other. Suppose it is s/u that
is missing a feature present in t/v. Note that this
immediately entails by the SpIC that t/v is a com-
plement, wherefore s/u is also a complement be-
cause u and v have identical spines. Since both s
and t are well-formed, whatever feature is missing
in s/u must occur somewhere else in s to match
some f+ in the spine of u. But by the SpIC, f−

cannot occur properly inside any specifier. We al-
ready know that s/u is a complement, so f− can
only occur on an ancestor of u or on one of its left
siblings. But then it would occur on a node in the
spine of u, which contradicts our initial assump-
tion that spines(u) = spinet(v). Hence δi = δ′i
after all, wherefore feat(s/u) = feat(t/v). 2

Theorem 4. For every MGG that obeys the SpIC,
it holds that dep(G) ∈ STA.
Proof. Lemma 1 and 3 jointly imply that dep(G)
is spine-closed, which guarantees that it can be
recognized by an STA (Thm. 1). 2

Intuitively, Theorem 4 holds because the SpIC
creates a unique “elsewhere case” for missing
movers. Suppose that an STA is at node n in a G-
tree t. Since it has processed t top-down, it knows
exactly which movers it has to look for by virtue of
the licensor features it has come across. With its
look-ahead of 1, the STA can scan the daughters
of n to see if any of them carry some of the de-
sired licensee features. Any licensee features that



are not among them must be embedded deeper in
the tree. Due to the SpIC, though, they can only
reside in the complement, i.e. the subtree rooted
in the rightmost daughter of n.

The SpIC is just one way of creating such an
elsewhere case. STA-recognizability would also
hold if movers could only escape from the left-
most argument, or if the label of the selecting
head decides which one of its arguments can be
extracted from. Extraction from arguments could
also be parameterized for each feature so that wh-
movers may only leave complements whereas top-
icalization is only allowed from the last but one
argument, if it exists. Or the STA could switch
between these four constraints depending on the
number of ancestors of the current node modulo
4. STA-recognizability holds as long as distribut-
ing the head’s δi across its arguments is fully de-
terministic based on the information available to a
sensing rule (current state, label of selecting head,
labels of selected heads). Hence the class of STA-
recognizable MG dependency tree languages is
larger than what is allowed by the SpiC.

This does not change the fact, though, that the
initial finding of Graf (2012a) regarding the in-
sufficiency of STAs is incomplete. In the case
of Merge, the insufficiency disappears with the
more compact representation format of MG de-
pendency trees. Alternatively, one could also
keep the derivation tree format while increasing
the STA look-ahead beyond just one level —
this is a point we will revisit soon in §6.2 and
§6.3, for very different reasons. With respect
to Move, the choice of representation format is
immaterial. Neither derivation trees nor depen-
dency trees make movement as defined in MGs
STA-recognizable. However, the SpIC does make
movement STA-recognizable because specifiers
do not need to be probed deeper than their head.
For MG dependency trees, this coincides with the
1-level look-ahead of STAs, whereas derivation
trees once again require a more generous look-
ahead window. The choice of representation thus
has an impact on the amount of required look-
ahead, but the essence of our STA-recognizability
result for MGs rests on the SpIC, not the tree for-
mat.

5 STAs and C-Command Conditions

The previous section has successfully established
that the central operations of MGs can be han-

dled by STAs, assuming that I) they are con-
strued as constraints on MG dependency trees,
and II) movement is subject to the SpIC. But the
structure-building operations Merge and Move are
just one part of syntax. Licensing conditions also
play a major role, in particular those rooted in c-
command. This section shows that these condi-
tions are also captured by STAs.

Licensing conditions based on c-command are
ubiquitous in the syntactic literature. They were
recently studied from a subregular perspective by
Graf and Shafiei (2019) and Shafiei and Graf
(2019). Both papers use similar ideas, but define
them very differently. We adopt the formalism of
Graf and Shafiei (2019) because it defines all es-
sential concepts directly in terms of dependency
trees.
Definition 12 (C-string). Let t be some MG de-
pendency tree. For every node n of t in config-
uration m(d1, . . . , di, n, di+1, . . . , dj) , its imme-
diate c[ommand]-string is ics(n) = d1 · · · di n.
The augmented c[ommand]-string acs(n) of n is
recursively defined as shown below, where ↑ is a
distinguished symbol:

acs(n) :=

{
ics(n) if n is the root of t
acs(m) ↑ ics(n) if m is n’s mother

y

Example 4. The c-string of the :: N+D− in Fig. 3
is ε :: T+wh+C− ↑ does :: V+T− ↑ like ::
D+D+V− ↑ a :: N+D− the :: N+D− y

Licensing conditions are then formalized as
constraints on the shape of permissible c-strings.
This is comparable to restricting a tree via its path
language, except that paths are now replaced by
c-strings.
Definition 13 (C-string constraints). A c-string
constraint C is some string language L over Σ ∪
{↑}. A Σ-tree t is well-formed with respect to C
iff acs(n) ∈ L for every node n of t. y
Which subregular class provides the most appro-
priate fit for syntactic licensing conditions is still
a matter of debate. Graf and Shafiei (2019) pro-
pose IO-TSL as a generous upper bound. IO-TSL
is a subregular string language recently defined in
(Graf and Mayer, 2018) in their analysis of San-
skrit n-retroflxion. Shafiei and Graf (2019), on the
other hand, argue that at least island constraints are
best captured by IBSP, another class from subreg-
ular work on phonology (Graf, 2017, 2018a). Nei-
ther class is particularly well-understood at this



point. Intuitively, IO-TSL treats local dependen-
cies as primitive and reduces non-local constraints
to local ones over enriched representations. IBSP,
on the other hand, takes all constraints to be non-
local and then uses locality domain to prune down
their reach. Either way there is ample evidence
that all attested conditions that can be correctly
stated over c-strings are at most regular. That’s
all we need to show that they can be enforced by
an STA.

Before we proceed, the reader should take note
that c-strings as defined in Graf and Shafiei (2019)
do not quite capture the standard notion of c-
command over phrase structure trees. First of
all, movement is factored out, so that c-strings
only capture the c-command relations between the
base positions where arguments enter the deriva-
tion. Graf and Shafiei (2019) point out several
options for adding movement, but they do not ex-
plore them in depth. We will provide our own STA
account for movement interactions later on (§6.1).
Another, less important deviation from standard c-
command pertains to the status of heads and spec-
ifiers. If one interprets linear precedence in com-
mand strings as c-command, then specifiers do
not c-command their selecting head even though
they c-command the head’s object. At the same
time, the head c-commands the specifier. While
there seem to be no cases in the syntactic liter-
ature where this difference to c-command mat-
ters, it nonetheless highlights that c-strings only
approximate the standard notion of c-command.

This approximation of c-command is very con-
venient for our purposes, though. The construc-
tion of a node’s c-string closely mirrors the def-
inition of a node’s spine, so it is perhaps unsur-
prising that every regular c-string constraint can be
enforced by an STA. The construction of an STA
automaton for this purpose is remarkably straight-
forward.

First, we simplify c-string constraints by con-
sidering only constraints that generate prefix-
closed sets of c-strings. The lemma below estab-
lishes that this assumption is innocuous for deter-
mining tree well-formedness.

Lemma 4. Let L be some regular language of
well-formed c-strings, and let Lp be the largest
subset of L such that u /∈ Lp entails uv /∈ Lp
for all u ∈ (Σ∪ {↑})+ and v ∈ (Σ∪ {↑})∗. Then
a Σ-tree is well-formed with respect to L iff it is
well-formed with respect to Lp.

Proof. We only consider c-strings that do not start
or end with ↑ as these never occur in any trees to
begin with. Since Lp ⊆ L and, by Def. 13, a tree
is well-formed with respect to c-string set C iff all
its c-strings are members of C, two entailments
follow immediately: I) if t is well-formed with re-
spect to Lp, it is well-formed with respect to L,
and II) if t is ill-formed with respect to L, it is ill-
formed with respect to Lp.

Next, suppose t is well-formed with respect to
L. Since it contains no illicit c-string, t could
only be ill-formed with respect to Lp if it contains
some licit c-string uv such that u /∈ Lp, where-
fore uv /∈ Lp. But if uv is a c-string of t, then so
is every non-empty prefix of uv that does not end
in ↑. By our initial assumption, t is well-formed,
and hence every non-empty prefix of uv is a mem-
ber of L. It then must also be a member of Lp
because, by definition, Lp must be largest among
the prefix-closed subsets of L. It follows that t is
well-formed with respect to Lp, too.

Finally, consider the case where t is ill-formed
with respect to Lp. Then there is some c-string u
of t such that u /∈ Lp but every non-empty pre-
fix of u (that does not end in ↑) is a member of
Lp. In this case it must also hold that u /∈ L, for
otherwise Lp is either not largest or violates pre-
fix closure. Consequently, t is also ill-formed with
respect to L. 2

Intuitively, Lemma 4 capitalizes on the fact that
whenever a tree contains at least one unlicensed
node, the status of other nodes no longer matters
because the tree is already ill-formed. Hence we
may freely assume that the c-string of node n is
illicit as soon as a prefix of that c-string is illicit,
even in cases where n itself would be licensed.

Now let D := 〈Σ ∪ {↑} , Q, qI , F, δ〉 be
the complete, deterministic finite-state string
automaton that recognizes Lp as defined in
Lemma 4. As D is deterministic, it has a
unique initial state qI . Its transition relation
δ : (Q × Σ) × Q is a function. We ex-
pand δ to strings such that δ(q, σ1σ2 · · ·σn) :=
δ(· · · δ(δ(q, σ1), σ2) · · · , σn). Since Lp is prefix-
closed, it also holds that Q = F ∪ {s}, where
s /∈ F is some sink state from which no other state
can be reached except s itself. Prefix closure also
entails that qI ∈ F , so that empty c-strings are
always allowed (since by definition c-strings are
never empty, this is innocuous).

We then construct the corresponding STA



AD := 〈Σ, Q, qI ,∆〉. For n ≥ 1, each sensing
rule in ∆ is of the form

q(σ(n)( ~σ1, ~σ2, . . . , ~σn))→
σ(δ(q, σ ↑)( ~σ1),
δ(q, σ ↑ σ1)( ~σ2),
. . . ,

δ(q, σ ↑ σ1σ2 · · ·σn−1)( ~σn)))

where q ∈ F and, as previously defined at the be-
ginning of §2.2, the use of ~σi with some symbol
σ

(n)
i is a shorthand for σi(xi1 , . . . , xin). For leaf

nodes, we require q(σ(0)) → σ(0) ∈ ∆ iff both
q ∈ F and δ(q, σ) ∈ F .

This construction effectively simulates runs of
the string automaton D over c-strings. The only
complication is that the process of assigning a
state to σi does not consider σi itself. But σi
does affect the states of its right siblings and all
its daughters. And if σi is a leaf, it indirectly de-
termines whether the state can be removed so that
the tree may be accepted.

Theorem 5. Let L be a regular language of well-
formed c-strings. Then there is some STA A such
that L(A) is the set of all Σ-trees that are well-
formed with respect to L.

Proof. Following Lemma 4, we replace L by Lp
and consider the complete, deterministic automa-
ton D that generates Lp. We construct A from D
in the manner described above. We then give a
proof by induction on the depth of Σ-trees.

Pick some Σ-tree t and suppose t ∈ Σ(0). The
only c-string of t is t. Then A recognizes t iff
qI ∈ F and δ(qI , t) ∈ F . The former holds by
definition, so t ∈ L(A) iff t ∈ L(D). This estab-
lishes the base case.

Next, consider any arbitrary configuration
q(σ(n)(σ1, . . . , σn)). By our induction assump-
tion, q = δ(qI , u) with acs(σ) = uσ. Sup-
pose acs(σi) /∈ Lp for some 1 ≤ i ≤ n. Then
δ(qI , acs(σi)) = δ(q, σ ↑ σ1 · · ·σi) /∈ F . If
i < n, A will assign some non-final state to σi+1.
If i = n and σi is not a leaf, A assigns the non-
final state to the leftmost daughter of σi. In both
cases, t now contains some node with a non-final
state. But there is no sensing rule with a non-
final state on its left-hand side, so that A cor-
rectly rejects t. If i = n and σi is a leaf, then
δ(δ(q, σ ↑ σ1 · · ·σi−1), σi) is not final and conse-
quently there is no suitable leaf rewrite rule. As

this covers all possible configurations for σi, we
conclude thatA rejects every tree that is ill-formed
with respect to Lp.

In the other direction, suppose A rejects
t. Then there must be some configuration
σ(q1(σ1), . . . , qi(σi), . . . , qn(σn)) such that qi is
not a final state. But then acs(σi−1) /∈ Lp, as de-
sired. 2

Theorem 5 establishes that the same subreg-
ular machinery of STAs can be used for both
the structure-building operations Merge and Move
and the c-command licensing conditions that
deeply permeate syntax. The STA perspective
also provides a new answer as to why c-command
should play such an important role in syntax.
How an STA treats a given node depends solely
on the spine of that node. The spine contains
the node itself, all its ancestors, and the sib-
lings of all the nodes in the spine. This imme-
diately precludes generalized notion of command
like the S-command relation of Barker and Pul-
lum (1990), which in modern terminology would
be CP-command: x CP-commands y iff x does not
reflexively dominate y (or the other way round)
and every CP that properly dominates x properly
dominates y. As x and y can be arbitrarily deep
within distinct subtrees while S-commanding each
other, this is not an STA-recognizable command
relation. C-command, on the other hand, stays
within the narrow confines of STAs.

Admittedly STAs could also selectively ignore
some c-commanders, operate with “inverse c-
command” where a complement c-commands into
its specifiers, or switch between different notions
of c-command based on some modulo counting
condition. So just as with the SpIC, the power
of STAs goes quite a bit beyond what is desir-
able for c-command. Still, it is striking that c-
command is a very natural relation from STA per-
spective, whereas more global notions of com-
mand are correctly ruled out. As long as one is
willing to accept dependency structures as a natu-
ral representation that arises from head-argument
relations, c-command is a natural companion of
STA-recognizability.

A lot of work remains to be done, though. As
just discussed, STAs allow for some very unnat-
ural command relations. Even more troubling is
that any arbitrary regular constraint over c-strings
can be enforced by an STA. Seeing how syntactic
constraints are very limited in the shape of depen-



dencies they enforce, STAs are overly powerful.
Hence STAs can only act as an upper bound.

At the same time, our current STA approach
is too limited. In cases where licensing condi-
tions interact with movement, the licensing ele-
ment might not be part of the c-string of a node.
Our STA construction, which is based purely on
c-strings, will necessarily fail in these cases. In
addition, some cases of licensing involve general-
ized notions of c-command that go beyond what
STAs can handle. We are confident, though, that
these issues can be addressed in future work. The
next section briefly sketches the solutions we have
in mind.

6 Expanding the Core Results

6.1 Interactions of Movement and Licensing

Linguists have identified many cases where move-
ment obfuscates licensing configurations by dis-
placing the licensed element. A simple example
would be [which book about himself]i does John
like ti, where the reflexive is not c-commanded
by its antecedent John in the corresponding phrase
structure tree. These cases are entirely unproblem-
atic for STAs since the MG dependency trees fully
factor out movement, so which book about him-
self remains in the object position where it is c-
commanded by the subject John. The problematic
cases are much rarer, to such a degree that con-
vincing examples are hard to come by: I) move-
ment of a licensed element bleeding licensing, and
II) movement of a licensing element feeding li-
censing.

The first case covers configurations where a li-
censed element — or a subtree containing it —
moves to a position above the element’s licensor
and where the subsequent change in c-command
relations does make licensing impossible (in con-
trast to the binding example above, where licens-
ing holds nonetheless). In MGs with the SpIC this
can be captured by an STA. The states of the STA
would be n-tuples similar to the output of the feat-
function we defined for MGs. The first compo-
nent records c-string states in the usual fashion,
whereas each other component i records the most
recent head hwith an f+i -feature, plus the state that
was assigned to h. When an fi-mover m is found,
the information in the i-th component is used to
compute the state for m as if m were a left daugh-
ter of h. In this case, m is also excluded from the
state computation of its siblings in the dependency

tree. The details remain to be worked out, but
movement of a licensee should be easy enough to
handle because the STA can separately keep track
of the c-command configurations for each position
that is targeted by a mover.

The second option is more complicated. Here
some phrase must move into a higher position
from where it can c-command the element that
must be licensed. In combination with the SpIC,
this means that the licensing of a node can be con-
tingent on the nodes contained by its rightmost
sibling. An STA can still handle this, but it re-
quires a very different construction from the one
described in this paper. As in the bleeding case, a
state consists of multiple components, each one of
which corresponds to a movement feature. Com-
ponent i records all the types of licensing condi-
tions that still must be met for nodes that are c-
commanded from the most recent head with licen-
sor feature f+i . The states also keep track of the c-
command relations between the heads hosting the
relevant licensor features. When a mover with f−i
is encountered, the STA checks if the mover can
satisfy any of the licensing requirements in com-
ponent i. If so, those are removed from compo-
nent i and all other components whose heads are
c-commanded by the head for component i. At
the end, no state may contain any non-empty com-
ponents. This strategy reimplements licensing as
a mechanism where the STA accrues “licensing
debt” while moving through the tree. This debt has
to be paid off by movers at a later point. The strat-
egy works because the SpIC allows us to correctly
synchronize the licensing debt across the states of
all daughter nodes.

While each strategy is relatively simple on its
own, integrating them is more difficult. An ele-
ment may move to a higher position p from where
it is only licensed by another element that moves
to an even higher position. This requires keeping
track of potential licensing debts for p which are
then narrowed down to the actual licensing debt
once it is known what actually moves to p, and
then this debt must be paid off by whatever moves
to a position above p. Further complicating the
picture, some licensing requirements only need to
be satisfied once (e.g. Principle A in the exam-
ple above), whereas others hold throughout the
derivation and are enforced after each movement
step. The individual components of the automa-
ton states thus must be synchronized in just the



right way, which complicates the construction of
the STA even more.

6.2 Subcommand

It has been argued that some cases of long-
distance binding involve subcommand instead of
c-command (see Tang 1989 and Huang and Liu
2001, a.o.). A node x subcommands y iff x c-
commands y or x is a specifier of some z that c-
commands y. From the perspective of c-strings, x
c-commands y iff there is some z in the c-string
of y such that x = z or x is the left sibling of
a daughter of z. For instance, if Principle A in
English allowed for subcommand instead of just
c-command, then John’s picture pleases himself
would be well-formed as John is a specifier of the
subject DP and thus subcommands himself.

Subcommand is beyond the reach of STAs be-
cause it makes the status of a node n dependent on
the daughters of n’s left siblings. Similar to the
case of movement interactions, there are two pos-
sible replies to this. One could point out the rarity
of subcommand, and that in the few cases where
it arises, it serves as a means to furnish additional
antecedents for reflexives beyond those that are al-
ready provided via c-command. Hence subcom-
mand might be limited to the syntax-semantics in-
terface and may not directly factor into licensing.
Alternatively, one could simply increase the look-
ahead of STAs from 1 to 2 so that the daughters
of daughters are also taken into account. Sub-
command then is just a more demanding case of
c-command.

6.3 Extension to MG Derivation Trees

Once one equips STAs with a more powerful look-
ahead mechanism to handle subcommand, MG
derivation trees once again become a viable alter-
native to MG dependency trees. All the results in
this paper extend from dependency tree to deriva-
tion trees if one generalizes STAs to determinis-
tic top-down tree automata with finite look-ahead.
That is not surprising because dependency trees
can be converted to derivation trees by a linear tree
transduction. This shows that derivation trees are
the result of separating mothers and daughters in
a dependency tree by a finitely bounded amount
of material (Merge and Move nodes). Finite look-
ahead is a means to reconstruct the relations of the
dependency tree that have been obfuscated by this
additional material.

It remains to be seen which one of the two rep-
resentation formats ultimately provides for more
insightful characerizations. One issue that deriva-
tion trees might shed some light on is the mono-
tonic nature of c-command. With an STA over de-
pendency trees, the daughters could be evaluated
in any order to determine the states for a c-string
constraint. While our current model proceeds left-
to-right, we could have just as well gone right-to-
left, inside out, or switched between those options
based on the label of the mother. Yet only the first
option corresponds to c-command as we know it.
In a derivation tree, all information is conveyed
via dominance. A specifier is not a sibling of
the complement but rather resides in a structurally
higher position (cf. the positions of John and the
CP-complement in Fig. 2). Among all the com-
mand variants we just described, the empirically
attested one in the form of c-command is the only
one that is monotonic with respect to dominance.

6.4 Adjuncts
Another problem of dependency trees is the sta-
tus of adjuncts. In order to obtain the correct c-
command relations, an adjunct of node n would
have to be treated as a left sibling of all the speci-
fiers of n. But since adjunction is unbounded, this
would mean that a node can have arbitrarily many
daughters, whereas STAs are usually defined for
trees with a finitely bounded arity. Adjuncts in
derivation trees, on the other hand, do not create
such issues because each adjunct grows the tree
vertically rather than horizontally.

Either way, adjuncts must be subject to the same
restriction as specifiers to preserve recognizability
by STAs or a suitably generalized variant: even
though an adjunct may move on its own, nothing
may move out of an adjunct. This is of course
a well-established property of adjuncts, known as
the Adjunct Island Constraint. Once again, then,
our specific subregular perspective derives a well-
known limitation of movement.

The Adjunct Island Constraint has some prin-
cipled exceptions such as [which car]i did John
drive Mary crazy while trying to fix ti (Truswell,
2007). It remains to be seen how these exceptions
can be reconciled with our approach.

6.5 Across-the-Board Movement
Another well-known island constraint is the Coor-
dinate Structure Constraint (CSC), which forbids
extraction from a conjunct. The CSC itself is easy



enough to enforce with an STA. It is the excep-
tion to the CSC that is of interest here: extraction
from a conjunct is permitted if extraction takes
place from all conjuncts in the same coordination.
Hence which beer did Ed buy and Greg drink is
well-formed even though which beer did Ed buy
wine and Greg drink is illicit. This is known as
across-the-board movement (ATB).

Curiously, the ATB-exception is very natural
from the STA perspective. STAs fail if one can-
not clearly tell from the local configuration which
one of several subtrees a move feature should
be passed into. The SpIC and the Adjunct Is-
land Constraint address this by excluding speci-
fiers and adjuncts from this equation, leaving the
complement as the only subtree that might con-
tain additional movers. ATB-movement consti-
tutes the opposite solution where the feature is in-
stead passed into every subtree. This, too, is a
fully deterministic process and thus within the lim-
its of STAs. If even one conjunct did not need
to contain a mover, then STAs would be faced
with a non-deterministic choice that they cannot
handle. While this has to be explored in greater
detail based on a proper formalization of ATB-
movement (e.g. Torr and Stabler 2016), it is re-
markable that the abilities of STAs closely line up
with the attested movement configurations.

6.6 Connection to Parsing

The preceding discussion shows that STAs not
only furnish a tighter upper bound on syntactic
complexity, they also explain core aspects of syn-
tax: the importance of island constraints, the avail-
ability of ATB-movement, and the central role of
c-command, which merely co-opts mechanisms
that are independently needed for movement. But
this in turn raises the fundamental question why
STA-recognizability should be a relevant concept
in syntax. We conjecture that STAs exhibit two
properties that are attractive for parsing: top-down
recognition, and determinism.

The highly predictive nature of human sentence
processing suggests that some top-down strategy
is employed, either directly in the form of recur-
sive descent parser, or as a top-down filter of a left-
corner parser. Given a choice between bottom-up
or top-down recognition, the latter is a much more
natural match for such parsing algorithms. Pre-
compiling a top-down filter into the parse schema
of an MG parser like the one in Stabler (2011b)

or Stanojević and Stabler (2018) is not trivial, but
feasible. Determinism ensures that this precom-
pilation does not explode the parse space, which
would slow down the parser. Hence STAs are a
good match for current MG models of human sen-
tence processing (Kobele et al., 2013; Gerth, 2015;
Graf et al., 2017).

Of course a deterministic top-down automaton
would also exhibit these properties, but these au-
tomata cannot even handle Merge, let alone Move.
STAs are a minimal extension of deterministic top-
down automata while also furnishing plenty of
power for syntactic dependencies. They require
some restrictions on movement, but each one of
them also improves parsing performance by expo-
nentially reducing the search space (see the discus-
sion of the SpIC’s impact on parsing performance
in Stabler 2013).

If one assumes that the grammar is but a high-
level description of the parser, the restriction to
STAs may be an abstract counterpart to various
parser constraints that are meant to improve effi-
ciency. Subregular complexity in syntax may thus
be closely connected to parsing.

7 Conclusion

We have shown that all current results on the
subregular complexity of syntax are insightfully
subsumed by sensing tree automata operating
over MG dependency trees. The limits of
STA-recognizability line up closely with well-
established restrictions on movement and syntac-
tic licensing conditions. A lot of issues remain to
be formally worked out, but we are confident that
the perspective developed in this paper will greatly
expand our understanding of subregular syntax.

Acknowledgments

The work reported in this paper was supported
by the National Science Foundation under Grant
No. BCS-1845344. We thank the participants of
the 2019 workshop on subregular complexity at
Stony Brook University for their feedback. We
are also grateful to the three anonymous review-
ers, whose suggestions allowed us to streamline
a lot of the formal definitions. In particular Re-
viewer 3 went above and beyond the call of duty.



References
Rolf Backofen, James Rogers, and K. Vijay-Shanker.

1995. A first-order axiomatization of the theory of
finite trees. Journal of Logic, Language and Infor-
mation, 4:5–39.

Hyunah Baek. 2018. Computational representation of
unbounded stress: Tiers with structural features. In
Proceedings of CLS 53, pages 13–24.

Chris Barker and Geoffrey K. Pullum. 1990. A theory
of command relations. Linguistics and Philosophy,
13:1–34.

Patrick Blackburn, Claire Gardent, and Wilfried
Meyer-Viol. 1993. Talking about trees. In Proceed-
ings of the Sixth Conference of the European Chap-
ter of the Association for Computational Linguistics,
pages 30–36.

Marisa Ferrara Boston, John T. Hale, and Marco
Kuhlmann. 2010. Dependency structures derived
from Minimalist grammars. In Christian Ebert, Ger-
hard Jäger, and Jens Michaelis, editors, MOL 10/11,
volume 6149 of Lecture Notes in Computer Science,
pages 1–12. Springer, Berlin.

Noam Chomsky. 1995. The Minimalist Program. MIT
Press, Cambridge, MA.

H. Comon, M. Dauchet, R. Gilleron, C. Löding,
F. Jacquemard, D. Lugiez, S. Tison, and M. Tom-
masiK. 2008. Tree automata: Techniques and
applications. Published online: http://www.
grappa.univ-lille3.fr/tata. Release
from November 18, 2008.

Thomas Cornell and James Rogers. 1998. Model the-
oretic syntax. In The Glot International State of
the Article Book, volume 1 of Studies in Generative
Grammar 48, pages 101–125. Mouton de Gruyter.

Robert Frank and K Vijay-Shanker. 2001. Primitive c-
command. Syntax, 4(3):164–204.

Sabrina Gerth. 2015. Memory Limitations in Sentence
Comprehension. A Structure-Based Complexity Met-
ric of Processing Difficulty. Ph.D. thesis, University
of Potdsam.

Saul Gorn. 1967. Explicit definitions and linguis-
tic dominoes. In Systems and Computer Science,
Proceedings of the Conference held at University
of Western Ontario, 1965, Toronto. University of
Toronto Press.

Thomas Graf. 2012a. Locality and the complexity of
Minimalist derivation tree languages. In Formal
Grammar 2010/2011, volume 7395 of Lecture Notes
in Computer Science, pages 208–227, Heidelberg.
Springer.

Thomas Graf. 2012b. Movement-generalized Mini-
malist grammars. In LACL 2012, volume 7351 of
Lecture Notes in Computer Science, pages 58–73.

Thomas Graf. 2017. The power of locality domains in
phonology. Phonology, 34:385–405.

Thomas Graf. 2018a. Locality domains and phonolog-
ical c-command over strings. In NELS 48: Proceed-
ings of the Forty-Eighth Annual Meeting of the North
East Linguistic Society, volume 1, pages 257–270,
Amherst, MA. GLSA.

Thomas Graf. 2018b. Why movement comes for free
once you have adjunction. In Proceedings of CLS
53, pages 117–136.

Thomas Graf and Connor Mayer. 2018. Sanskrit n-
retroflexion is input-output tier-based strictly local.
In Proceedings of SIGMORPHON 2018, pages 151–
160.

Thomas Graf, James Monette, and Chong Zhang. 2017.
Relative clauses as a benchmark for Minimalist pars-
ing. Journal of Language Modelling, 5:57–106.

Thomas Graf and Nazila Shafiei. 2019. C-command
dependencies as TSL string constraints. In Proceed-
ings of the Society for Computation in Linguistics
(SCiL) 2019, pages 205–215.

Jeffrey Heinz, Anna Kasprzik, and Timo Kötzing.
2012. Learning in the limit with lattice-structured
hypothesis spaces. Theoretical Computer Science,
457:111–127.

Jeffrey Heinz, Chetan Rawal, and Herbert G. Tanner.
2011. Tier-based strictly local constraints in phonol-
ogy. In Proceedings of the 49th Annual Meeting
of the Association for Computational Linguistics,
pages 58–64.

Cheng-Teh James Huang and Cheng-Sheng Luther Liu.
2001. Logophoricity, attitudes and ziji at the inter-
face. In Peter Cole, Gabrielle Herman, and Cheng-
Tea James Huang, editors, Long Distance Reflexives,
volume 33 of Syntax and Semantics, pages 141–195.
Academic Press, New York.

M. A. C. Huybregts. 1984. The weak adequacy of
context-free phrase structure grammar. In Ger J.
de Haan, Mieke Trommelen, and Wim Zonneveld,
editors, Van Periferie naar Kern, pages 81–99. Foris,
Dordrecht.

Adam Jardine and Kevin McMullin. 2017. Efficient
learning of tier-based strictly k-local languages. In
Proceedings of Language and Automata Theory and
Applications, Lecture Notes in Computer Science,
pages 64–76, Berlin. Springer.

C. Douglas Johnson. 1972. Formal Aspects of Phono-
logical Description. Mouton, The Hague.

Ronald M. Kaplan and Martin Kay. 1994. Regular
models of phonological rule systems. Computa-
tional Linguistics, 20(3):331–378.

Gregory M. Kobele. 2006. Generating Copies: An In-
vestigation into Structural Identity in Language and
Grammar. Ph.D. thesis, UCLA.

https://doi.org/10.1007/BF01048403
https://doi.org/10.1007/BF01048403
https://doi.org/10.3115/976744.976748
https://doi.org/10.1007/978-3-642-14322-9_1
https://doi.org/10.1007/978-3-642-14322-9_1
https://doi.org/10.7551/mitpress/9780262527347.003.0003
http://www.grappa.univ-lille3.fr/tata
http://www.grappa.univ-lille3.fr/tata
http://www.grappa.univ-lille3.fr/tata
http://www.grappa.univ-lille3.fr/tata
https://doi.org/10.1515/9783110822861.171
https://doi.org/10.1515/9783110822861.171
https://doi.org/10.1007/978-3-642-32024-8_14
https://doi.org/10.1007/978-3-642-32024-8_14
https://doi.org/10.1007/978-3-642-31262-5_4
https://doi.org/10.1007/978-3-642-31262-5_4
https://doi.org/10.1017/S0952675717000197
https://doi.org/10.1017/S0952675717000197
http://ling.auf.net/lingbuzz/004080
http://ling.auf.net/lingbuzz/004080
https://doi.org/10.15398/jlm.v5i1.157
https://doi.org/10.15398/jlm.v5i1.157
https://doi.org/10.1016/j.tcs.2012.07.017
https://doi.org/10.1016/j.tcs.2012.07.017
http://www.aclweb.org/anthology/P11-2011
http://www.aclweb.org/anthology/P11-2011
https://doi.org/10.1007/978-3-319-53733-7_4
https://doi.org/10.1007/978-3-319-53733-7_4
http://www.aclweb.org/anthology/J94-3001.pdf
http://www.aclweb.org/anthology/J94-3001.pdf
http://home.uchicago.edu/~gkobele/files/Kobele06GeneratingCopies.pdf
http://home.uchicago.edu/~gkobele/files/Kobele06GeneratingCopies.pdf
http://home.uchicago.edu/~gkobele/files/Kobele06GeneratingCopies.pdf


Gregory M. Kobele, Sabrina Gerth, and John T. Hale.
2013. Memory resource allocation in top-down
Minimalist parsing. In Formal Grammar: 17th and
18th International Conferences, FG 2012, Opole,
Poland, August 2012, Revised Selected Papers, FG
2013, Düsseldorf, Germany, August 2013, pages 32–
51, Berlin, Heidelberg. Springer.

Gregory M. Kobele and Jens Michaelis. 2011. Dis-
entangling notions of specifier impenetrability. In
The Mathematics of Language, volume 6878 of Lec-
ture Notes in Artificial Intelligence, pages 126–142,
Berlin, Heidelberg. Springer.

Gregory M. Kobele, Christian Retoré, and Sylvain Sal-
vati. 2007. An automata-theoretic approach to Min-
imalism. In Model Theoretic Syntax at 10, pages
71–80.

Wim Martens. 2006. Static Analysis of XML
Transformation- and Schema Languages. Ph.D. the-
sis, Hasselt University.

Wim Martens, Frank Neven, and Thomas Schwentick.
2008. Deterministic top-down tree automata: Past,
present, and future. In Proceedings of Logic and
Automata 2008, pages 505–530.

Kevin McMullin. 2016. Tier-Based Locality in Long-
Distance Phonotactics: Learnability and Typology.
Ph.D. thesis, University of British Columbia.

Jens Michaelis. 2004. Observations on strict deriva-
tional minimalism. Electronic Notes in Theoretical
Computer Science, 53:192–209.

Jens Michaelis. 2009. An additional observation on
strict derivational minimalism. In FG-MOL 2005,
pages 101–111.

Jens Michaelis and Marcus Kracht. 1997. Semilin-
earity as a syntactic invariant. In Logical Aspects
of Computational Linguistics, volume 1328 of Lec-
ture Notes in Artifical Intelligence, pages 329–345.
Springer.

Uwe Mönnich. 2006. Grammar morphisms. Ms. Uni-
versity of Tübingen.

Frank Morawietz. 2003. Two-Step Approaches to Nat-
ural Language Formalisms. Walter de Gruyter,
Berlin.

James Rogers. 1998. A Descriptive Approach to
Language-Theoretic Complexity. CSLI, Stanford.

James Rogers. 2003. Syntactic structures as multi-
dimensional trees. Research on Language and Com-
putation, 1(1):265–305.

James Rogers, Jeffrey Heinz, Gil Bailey, Matt Edlef-
sen, Molly Vischer, David Wellcome, and Sean
Wibel. 2010. On languages piecewise testable in the
strict sense. In Christan Ebert, Gerhard Jäger, and
Jens Michaelis, editors, The Mathematics of Lan-
guage, volume 6149 of Lecture Notes in Artificial
Intelligence, pages 255–265. Springer, Heidelberg.

Sylvain Salvati. 2011. Minimalist grammars in the
light of logic. In Sylvain Pogodalla, Myriam Qua-
trini, and Christian Retoré, editors, Logic and Gram-
mar — Essays Dedicated to Alain Lecomte on the
Occasion of His 60th Birthday, number 6700 in
Lecture Notes in Computer Science, pages 81–117.
Springer, Berlin.

Nazila Shafiei and Thomas Graf. 2019. The subregular
complexity of syntactic islands. Ms., Stony Brook
University.

Stuart M. Shieber. 1985. Evidence against the context-
freeness of natural language. Linguistics and Phi-
losophy, 8(3):333–345.

Edward P. Stabler. 1997. Derivational Minimalism. In
Christian Retoré, editor, Logical Aspects of Compu-
tational Linguistics, volume 1328 of Lecture Notes
in Computer Science, pages 68–95. Springer, Berlin.

Edward P. Stabler. 2011a. Computational perspectives
on Minimalism. In Cedric Boeckx, editor, Oxford
Handbook of Linguistic Minimalism, pages 617–
643. Oxford University Press, Oxford.

Edward P. Stabler. 2011b. Top-down recognizers for
MCFGs and MGs. In Proceedings of the 2nd Work-
shop on Cognitive Modeling and Computational
Linguistics, pages 39–48.

Edward P. Stabler. 2013. Two models of minimalist,
incremental syntactic analysis. Topics in Cognitive
Science, 5:611–633.

Miloš Stanojević and Edward Stabler. 2018. A
sound and complete left-corner parser for Minimal-
ist grammars. In Proceedings of the 8th Workshop
on Cognitive Aspects of Computational Language
Learning and Processing, pages 65–74.

Chih-Chen Jane Tang. 1989. Chinese reflexives. Natu-
ral Language and Linguistic Theory, 7:93–121.

John Torr and Edward P. Stabler. 2016. Coordination in
Minimalist grammars: Excorporation and across the
board (head) movement. In Proceedings of the 12th
International Workshop on Tree Adjoining Gram-
mars and Related Formalisms (TAG+12), pages 1–
17, Düsseldorf, Germany.

Robert Truswell. 2007. Extraction from adjuncts and
the structure of events. Lingua, 117:1355–1377.

Mai Ha Vu. 2018. Towards a formal description of
NPI-licensing patterns. In Proceedings of the Soci-
ety for Computation in Linguistics (SCiL) 2018, vol-
ume 1, pages 154–163. Article 17.

Mai Ha Vu, Nazila Shafiei, and Thomas Graf. 2019.
Case assignment in TSL syntax: A case study. In
Proceedings of the Society for Computation in Lin-
guistics (SCiL) 2019, pages 267–276.

https://doi.org/10.1007/978-3-642-39998-5_3
https://doi.org/10.1007/978-3-642-39998-5_3
https://doi.org/10.1007/978-3-642-23211-4_8
https://doi.org/10.1007/978-3-642-23211-4_8
https://doi.org/10.1007/BFb0052165
https://doi.org/10.1007/BFb0052165
https://doi.org/10.1515/9783110197259
https://doi.org/10.1515/9783110197259
https://doi.org/10.1007/978-3-642-14322-9_19
https://doi.org/10.1007/978-3-642-14322-9_19
https://doi.org/10.1007/978-3-642-21490-5_5
https://doi.org/10.1007/978-3-642-21490-5_5
https://doi.org/10.1007/BF00630917
https://doi.org/10.1007/BF00630917
https://doi.org/10.1007/BFb0052152
https://doi.org/10.1093/oxfordhb/9780199549368.013.0027
https://doi.org/10.1093/oxfordhb/9780199549368.013.0027
https://doi.org/10.1111/tops.12031
https://doi.org/10.1111/tops.12031
https://www.aclweb.org/anthology/W16-3301
https://www.aclweb.org/anthology/W16-3301
https://www.aclweb.org/anthology/W16-3301
https://doi.org/10.7275/R5DF6PDP
https://doi.org/10.7275/R5DF6PDP
https://doi.org/10.7275/sywz-xw23

