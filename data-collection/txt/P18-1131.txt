



















































Twitter Universal Dependency Parsing for African-American and Mainstream American English


Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers), pages 1415–1425
Melbourne, Australia, July 15 - 20, 2018. c©2018 Association for Computational Linguistics

1415

Twitter Universal Dependency Parsing for African-American and
Mainstream American English

Su Lin Blodgett Johnny Tian-Zheng Wei Brendan O’Connor
College of Information and Computer Sciences

University of Massachusetts Amherst
blodgett@cs.umass.edu jwei@umass.edu brenocon@cs.umass.edu

Abstract

Due to the presence of both Twitter-
specific conventions and non-standard and
dialectal language, Twitter presents a sig-
nificant parsing challenge to current de-
pendency parsing tools. We broaden En-
glish dependency parsing to handle social
media English, particularly social media
African-American English (AAE), by de-
veloping and annotating a new dataset of
500 tweets, 250 of which are in AAE,
within the Universal Dependencies 2.0
framework. We describe our standards
for handling Twitter- and AAE-specific
features and evaluate a variety of cross-
domain strategies for improving parsing
with no, or very little, in-domain labeled
data, including a new data synthesis ap-
proach. We analyze these methods’ impact
on performance disparities between AAE
and Mainstream American English tweets,
and assess parsing accuracy for specific
AAE lexical and syntactic features. Our
annotated data and a parsing model are
available at: http://slanglab.cs.umass.edu/
TwitterAAE/.

1 Introduction

Language on Twitter diverges from well-edited
Mainstream American English (MAE, also called
Standard American English) in a number of ways,
presenting significant challenges to current NLP
tools. It contains, among other phenomena, non-
standard spelling, punctuation, capitalization, and
syntax, as well as Twitter-specific conventions
such as hashtags, usernames, and retweet to-
kens (Eisenstein, 2013). Additionally, it con-
tains an abundance of dialectal language, includ-

ing African-American English (AAE), a dialect of
American English spoken by millions of individu-
als, which contains lexical, phonological, and syn-
tactic features not present in MAE (Green, 2002;
Stewart, 2014; Jones, 2015).

Since standard English NLP tools are typically
trained on well-edited MAE text, their perfor-
mance is degraded on Twitter, and even more
so for AAE tweets compared to MAE tweets—
gaps exist for part-of-speech tagging (Jørgensen
et al., 2016), language identification, and depen-
dency parsing (Blodgett et al., 2016; Blodgett and
O’Connor, 2017). Expanding the linguistic cov-
erage of NLP tools to include minority and col-
loquial dialects would help support equitable lan-
guage analysis across sociolinguistic communi-
ties, which could help information retrieval, trans-
lation, or opinion analysis applications (Jurgens
et al., 2017). For example, sentiment analysis sys-
tems ought to count the opinions of all types of
people, whether they use standard dialects or not.

In this work, we broaden Universal Dependen-
cies (Nivre et al., 2016) parsing1 to better han-
dle social media English, in particular social me-
dia AAE. First, we develop standards to handle
Twitter-specific and AAE-specific features within
Universal Dependencies 2.0 (§3), by selecting and
annotating a new dataset of 500 tweets, 250 of
which are in AAE.

Second, we evaluate several state-of-the-art de-
pendency parsers, finding that, as expected, they
perform poorly on our dataset relative to the UD
English Treebank (§4). Third, since the UD En-
glish Treebank contains substantial amounts of
traditional MAE data for training, we investigate
cross-domain training methods to improve Twitter
AAE dependency parsing with no, or very little,

1http://universaldependencies.org/

http://slanglab.cs.umass.edu/TwitterAAE/
http://slanglab.cs.umass.edu/TwitterAAE/
http://universaldependencies.org/


1416

in-domain labeled data, by using Twitter-specific
taggers, embeddings, and a novel heuristic train-
ing data synthesis procedure. This helps close
some of the gap between MAE and AAE perfor-
mance. Finally, we provide an error analysis of the
parsers’ performance on AAE lexical and syntac-
tic constructions in our dataset (§5.4).2

2 Related Work

2.1 Parsing for Twitter

Parsing for noisy social media data presents in-
teresting and significant challenges. Foster et al.
(2011) develop a dataset of 519 constituency-
annotated English tweets, which were converted
to Stanford dependencies. Their analysis found a
substantial drop in performance of an off-the-shelf
dependency parser on the new dataset compared to
a WSJ test set. Sanguinetti et al. (2017) annotated
a dataset of 6,738 Italian tweets according to UD
2.0 and examined the performance of two parsers
on the dataset, finding that they lagged consid-
erably relative to performance on the Italian UD
Treebank.

Kong et al. (2014) develop an English depen-
dency parser designed for Twitter, annotating a
dataset of 929 tweets (TWEEBANK V1) accord-
ing to the unlabeled FUDG dependency formalism
(Schneider et al., 2013). It has substantially differ-
ent structure than UD (for example, prepositions
head PPs, and auxiliaries govern main verbs).

More recently, Liu et al. (2018) developed
TWEEBANK V2, fully annotating TWEEBANK V1
according to UD 2.0 and annotating addition-
ally sampled tweets, for a total of 3,550 tweets.
They found that creating consistent annotations
was challenging, due to frequent ambiguities in in-
terpreting tweets; nevertheless, they were able to
train a pipeline for tokenizing, tagging, and pars-
ing the tweets, and develop ensemble and distil-
lation models to improve parsing accuracy. Our
work encounters similar challenges; in our ap-
proach, we intentionally oversample AAE-heavy
messages for annotation, detail specific annotation
decisions for AAE-specific phenomena (§3.2), and
analyze parser performance between dialects and
for particular constructions (§5.3–5.4). Future
work may be able to combine these annotations for
effective multi-dialect Twitter UD parsers, which

2Our annotated dataset and trained dependency parser are
available at http://slanglab.cs.umass.edu/TwitterAAE/ and
annotations are available in the public Universal Dependen-
cies repository.

may allow for the use of pre-existing downstream
tools like semantic relation extractors (e.g. White
et al. (2016)).

One line of work for parsing noisy social media
data, including Khan et al. (2013) and Nasr et al.
(2016), examines the effects of the domain mis-
matches between traditional sources of training
data and social media data, finding that matching
the data as closely as possible aids performance.
Other work focuses on normalization, including
Daiber and van der Goot (2016) and van der Goot
and van Noord (2017), which develop a dataset
of 500 manually normalized and annotated tweets,
and uses normalization within a parser. Separately,
Zhang et al. (2013) created a domain-adaptable,
parser-focused system by directly linking parser
performance to normalization performance.

2.2 Parsing for Dialects

For Arabic dialects, Chiang et al. (2006) parse
Levantine Arabic by projecting parses from Mod-
ern Standard Arabic translations, while Green and
Manning (2010) conduct extensive error analysis
of Arabic constituency parsers and the Penn Ara-
bic Treebank. Scherrer (2011) parse Swiss Ger-
man dialects by transforming Standard German
phrase structures. We continue in this line of
work in our examination of AAE-specific syntac-
tic structures and generation of synthetic data with
such structures (§4.2.1).

Less work has examined parsing dialectal lan-
guage on social media. Recently, Wang et al.
(2017) annotate 1,200 Singlish (Singaporean En-
glish) sentences from a Singaporean talk fo-
rum, selecting sentences containing uniquely Sin-
gaporean vocabulary items. Like other work,
they observe a drop in performance on dialectal
Singlish text, but increase performance through a
stacking-based domain adaptation method.

3 Dataset and Annotation

3.1 Dataset

Our dataset contains 500 tweets, with a total of
5,951 non-punctuation edges, sampled from the
publicly available TwitterAAE corpus.3 Each
tweet in that corpus is accompanied by a model’s
demographically-aligned topic model probabili-
ties jointly inferred from Census demographics
and word likelihood by Blodgett et al. (2016), in-
cluding the African-American and White topics.

3http://slanglab.cs.umass.edu/TwitterAAE/

http://slanglab.cs.umass.edu/TwitterAAE/
http://slanglab.cs.umass.edu/TwitterAAE/


1417

We create a balanced sample to get a range of di-
alectal language, sampling 250 tweets from those
where the African-American topic has at least
80% probability, and 250 from those where the
White topic has at least 80% probability. We refer
to these two subcorpora as AA and WH; Blodgett
et al. (2016) showed the former exhibits linguistic
features typical of AAE.

The 250 AA tweets include many alternate
spellings of common words that correspond to
well-known phonological phenomena—including
da, tha (the), dat, dhat (that), dis, dhis (this), ion,
iont (I don’t), ova (over), yo (your), dere, der
(there), den, dhen (then), ova (over), and nall,
null (no, nah)—where each of the mentioned ital-
icized AAE terms appears in the AAE data, but
never in the MAE data. We examine these lexi-
cal variants more closely in §5.4. Across the AA
tweets, 18.0% of tokens were not in a standard En-
glish dictionary, while the WH tweets’ OOV rate
was 10.7%.4 We further observe a variety of AAE
syntactic phenomena in our AA tweets, several of
which are described in §3.2 and §5.4.

3.2 Annotation

To effectively measure parsing quality and develop
better future models, we first focus on developing
high-quality annotations for our dataset, for which
we faced a variety of challenges. We detail our
annotation principles using Universal Dependency
2.0 relations (Nivre et al., 2016).

All tweets were initially annotated by two an-
notators, and disagreements resolved by one of
the annotators. Annotation decisions for several
dozen tweets were discussed in a group of three
annotators early in the annotation process.

Our annotation principles are in alignment with
those proposed by Liu et al. (2018), with the ex-
ception of contraction handling, which we discuss
briefly in §3.2.2.

3.2.1 Null Copulas

The AAE dialect is prominently characterized by
the drop of copulas, which can occur when the
copula is present tense, not first person, not ac-
cented, not negative, and expressing neither the
habitual nor the remote present perfect tenses
(Green, 2002). We frequently observed null copu-
las, as in:

4The dictionary of 123,377 words with American
spellings was generated using http://wordlist.aspell.net/.

If u wit me den u pose to RESPECT ME

nsubjnsubj

“If you (are) with me, then you (are)
supposed to respect me”

The first dropped are is a null copula; UD2.0
would analyze the MAE version as you nsubj←−− me
cop−→ are, which we naturally extend to analyze the
null copula by simply omitting cop (which is now
over a null element, so cannot exist in a depen-
dency graph). The second are is a null auxiliary
(in MAE, you nsubj←−− supposed aux−→ are), a tightly
related phenomenon (for example, Green et al.
(2007) studies both null copulas and null auxiliary
be in infant AAE), which we analyze similarly by
simply omitting the aux edge.

3.2.2 AAE Verbal Auxiliaries
We observed AAE verbal auxiliaries, e.g.,

fees be looking upside my head

aux

Now we gone get fucked up

aux

damnnn I done let alot of time pass by

aux

including habitual be (“Continually, over and over,
fees are looking at me...”), future gone (“we are
going to get...”), and completive done (“I did let
time pass by,” emphasizing the speaker completed
a time-wasting action).

We attach the auxiliary to the main verb with
the aux relation, as UD2.0 analyzes other English
auxiliaries (e.g. would or will).

3.2.3 Verbs: Auxiliaries vs. Main Verbs
We observed many instances of quasi-auxiliary, “-
to” shortened verbs such as wanna, gotta, finna,
bouta, tryna, gonna, which can be glossed as want
to, got to, fixing to, about to, etc. They control
modality, mood and tense—for example, finna and
bouta denote an immediate future tense; Green
(2002, ch. 2) describes finna as a preverbal marker.
From UD’s perspective, it is difficult to decide
if they should be subordinate auxiliaries or main
verbs. In accordance with the UD Treebank’s han-
dling of MAE want to X and going to X as main
verbs (want xcomp−−→ X), we analyzed them similarly,
e.g.

Lol he bouta piss me off  “He is about to piss me off”

xcomp

http://wordlist.aspell.net/


1418

This is an instance of a general principle that, if
there is a shortening of an MAE multiword phrase
into a single word, the annotations on that word
should mirror the edges in and out of the original
phrase’s subgraph (as in Schneider et al. (2013)’s
fudge expressions).

However, in contrast to the UD Treebank, we
did not attempt to split up these words into their
component words (e.g. wanna → want to), since
to do this well, it would require a more involved
segmentation model over the dozens or even hun-
dreds of alternate spellings each of the above can
take;5 we instead rely on Owoputi et al. (2013);
O’Connor et al. (2010)’s rule-based tokenizer that
never attempts to segment within such shorten-
ings. This annotation principle is in contrast to
that of Liu et al. (2018), which follows UD tok-
enization for contractions.

3.2.4 Non-AAE Twitter issues
We also encountered many issues general to Twit-
ter but not AAE; these are still important to deal
with since AAE tweets include more non-standard
linguistic phenomena overall. When possible,
we adapted Kong et al. (2014)’s annotation con-
ventions into the Universal Dependencies con-
text, which are the only published conventions we
know of for Twitter dependencies (for the FUDG
dependency formalism). Issues include:

• @-mentions, which require different treat-
ment when they are terms of address, versus
nominal elements within a sentence.

• Hashtags, which in their tag-like usage are
utterances by themselves (#tweetliketheoppo-
sitegender Oh damn .). or sometimes can
be words with standard syntactic relations
within the sentence (#She’s A Savage, having
#She’s nsubj←−− Savage). Both hashtag and @-
mention ambiguities are handled by Owoputi
et al. (2013)’s POS tagger.

• Multiple utterances, since we do not attempt
sentence segmentation, and in many cases
sentential utterances are not separated by ex-
plicit punctuation. FUDG allows for multiple
roots for a text, but UD does not; instead we
follow UD’s convention of the parataxis re-
lation for what they describe as “side-by-side
run-on sentences.”

5For example, Owoputi et al. (2013)’s Twitter word clus-
ter 0011000 has 36 forms of gonna alone: http://www.cs.
cmu.edu/∼ark/TweetNLP/cluster viewer.html

• Emoticons and emoji, which we attach as dis-
course relations to the utterance root, follow-
ing UD’s treatment of interjections.

• Collapsed phrases, like omw for “on my
way.” When possible, we used the principle
of annotating according to the root of the sub-
tree of the original phrase. For example, UD
2.0 prescribes way xcomp−−→ get for the sentence
On my way to get...; therefore we use omw
xcomp−−→ get for omw to get.

• Separated words, like uh round for “around,”
which we analyze as multiword phrases (flat
or compound).

We discuss details for these and other cases in the
online appendix.

4 Experiments

4.1 Models

Our experiments use the following two parsers.
UDPipe (Straka et al., 2016) is a neural pipeline

containing a tokenizer, morphological analyzer,
tagger, and transition-based parser intended to be
easily retrainable. The parser attains 80.2% LAS
(labeled attachment score) on the UD English tree-
bank with automatically generated POS tags, and
was a baseline system used in the CoNLL 2017
Shared Task (Zeman et al., 2017).6

Deep Biaffine (Dozat et al., 2017; Dozat and
Manning, 2016) is a graph-based parser incorpo-
rating neural attention and biaffine classifiers for
arcs and labels. We used the version of the parser
in the Stanford CoNLL 2017 Shared Task submis-
sion, which attained 82.2% LAS on the UD En-
glish treebank with automatically generated tags,
and achieved the best performance in the task. The
model requires pre-trained word embeddings. 7

4.2 Experimental Setup

We considered a series of experiments within
both a cross-domain scenario (§4.2.1), where we
trained only on UD Treebank data, and an in-
domain scenario (§4.2.2) using small amounts of
our labeled data. We use the parsing systems’
default hyperparameters (e.g. minibatch size and
learning rate) and the default training/development
split of the treebank (both systems perform early
stopping based on development set performance).

6https://github.com/ufal/udpipe
7https://github.com/tdozat/UnstableParser/

http://www.cs.cmu.edu/~ark/TweetNLP/cluster_viewer.html
http://www.cs.cmu.edu/~ark/TweetNLP/cluster_viewer.html
https://github.com/ufal/udpipe
https://github.com/tdozat/UnstableParser/


1419

4.2.1 Cross-Domain Settings
Morpho-Tagger vs. ARK POS tags: The UD
Treebank contains extensive fine-grained POS and
morphological information, on which UDPipe’s
morphological analyzer and tagging system is
originally trained. This rich information should be
useful for parsing, but the analyzers may be highly
error-prone on out-of-domain, dialectal Twitter
data, and contribute to poor parsing performance.
We hypothesize that higher quality, even if coarser,
POS information should improve parsing.

To test this, we retrain UDPipe in two differ-
ent settings. We first retrain the parser compo-
nent with fine-grained PTB-style POS tags and
morphological information provided by the tagger
component;8 we call this the Morpho-Tagger set-
ting. Second, we retrain the parser with morpho-
logical information stripped and its tags predicted
from the ARK Twitter POS tagger (Owoputi et al.,
2013), which is both tailored for Twitter and dis-
plays a smaller AAE vs MAE performance gap
than traditional taggers (Jørgensen et al., 2016);
we call this the ARK Tagger setting.9 The ARK
Tagger’s linguistic representation is impoverished
compared to Morpho-Tagger: its coarse-grained
POS tag system does not include tense or number
information, for example.10

Synthetic Data: Given our knowledge of
Twitter- and AAE-specific phenomena that do not
occur in the UD Treebank, we implemented a rule-
based method to help teach the machine-learned
parser these phenomena; we generated synthetic
data for three Internet-specific conventions and
one set of AAE syntactic features. (This is in-
spired by Scherrer (2011)’s rule transforms be-
tween Standard and Swiss German.) We per-
formed each of the following transformations sep-
arately on a copy of the UD Treebank data and
concatenated the transformed files together for the
final training and development files, so that each
final file contained several transformed copies of
the original UD Treebank data.

1. @-mentions, emojis, emoticons, expressions,
and hashtags: For each sentence in the UD Tree-
bank we inserted at least one @-mention, emoji,
emoticon, expression (Internet-specific words and

8We also retrained this component, to maintain consis-
tency of training and development split. We also remove the
universal (coarse) POS tags it produces, replacing them with
the same PTB tags.

9We strip lemmas from training and development files for
both settings.

10Derczynski et al. (2013)’s English Twitter tagger, which
outputs PTB-style tags, may be of interest for future work.

abbreviations such as lol, kmsl, and xoxo), or hash-
tag, annotated with the correct relation, at the be-
ginning of the sentence. An item of the same
type was repeated with 50% probability, and a sec-
ond item was inserted with 50% probability. @-
mentions were inserted using the ATMENTION to-
ken and emojis using the EMOJI token. Emoti-
cons were inserted from a list of 20 common
emoticons, expressions were inserted from a list of
16 common expressions, and hashtags were sam-
pled for insertion according to their frequency in
a list of all hashtags observed in the TwitterAAE
corpus.

2. Syntactically participating @-mentions: To
replicate occurrences of syntactically participating
@-mentions, for each sentence in the UD Tree-
bank with at least one token annotated with an
nsubj or obj relation and an NNP POS tag, we re-
placed one at random with the ATMENTION to-
ken.

3. Multiple utterances: To replicate occur-
rences of multiple utterances, we randomly col-
lapsed pairs of two short sentences (< 15 tokens)
together, attaching the root of the second to the
root of the first with the parataxis relation.

4. AAE preverbal markers and auxiliaries:
We introduced instances of verbal constructions
present in AAE that are infrequent or non-existent
in the UD Treebank data. First, constructions
such as going to, about to, and want to are fre-
quently collapsed to gonna, bouta, and wanna,
respectively (see §3.2.2); for each sentence with
at least one of these constructions, we randomly
chose one to collapse. Second, we randomly re-
placed instances of going to with finna, a prever-
bal marker occurring in AAE and in the Ameri-
can South (Green, 2002). Third, we introduced
the auxiliaries gone and done, which denote fu-
ture tense and past tense, respectively; for the for-
mer, for each sentence containing at least one aux-
iliary will, we replace it with gone, and for the lat-
ter, for each sentence containing at least one non-
auxiliary, non-passive, past-tense verb, we choose
one and insert done before it. Finally, for each sen-
tence containing at least one copula, we delete one
at random.

Word Embeddings: Finally, since a tremen-
dous variety of Twitter lexical items are not
present in the UD Treebank, we use 200-
dimensional word embeddings that we trained
with word2vec11 (Mikolov et al., 2013) on the

11https://github.com/dav/word2vec

https://github.com/dav/word2vec


1420

TwitterAAE corpus, which contains 60.8 million
tweets. Before training, we processed the cor-
pus by replacing @-mentions with ATMENTION,
replacing emojis with EMOJI, and replacing se-
quences of more than two repeated letters with two
repeated letters (e.g. partyyyyy → partyy). This
resulted in embeddings for 487,450 words.

We retrain and compare UDPipe on each of
the Morpho-Tagger and ARK Tagger settings with
synthetic data and pre-trained embeddings, and
without. We additionally retrain Deep Biaffine
with and without synthetic data and embeddings.12

4.2.2 In-domain Training
We additionally investigate the effects of small
amounts of in-domain training data from our
dataset. We perform 2-fold cross-validation, ran-
domly partitioning our dataset into two sets of 250
tweets. We compare two different settings (all us-
ing the UDPipe ARK Tagger setting):

Twitter-only: To explore the effect of training
with Twitter data alone, for each set of 250 we
trained on that set alone, along with our Twitter
embeddings, and tested on the remaining 250.

UDT+Twitter: To explore the additional signal
provided by the UD Treebank, for each set of 250
we trained on the UD Treebank concatenated with
that set (with the tweets upweighted to approxi-
mately match the size of the UD Treebank, in or-
der to use similar hyperparameters) and tested on
the remaining 250.

5 Results and Analysis

In our evaluation, we ignored punctuation tokens
(labeled with punct) in our LAS calculation.

5.1 Effects of Cross-Domain Settings

Morpho-Tagger vs. ARK Tagger: As hypothe-
sized, UDPipe’s ARK Tagger setting outperformed
the Morpho-Tagger across all settings, ranging
from a 2.8% LAS improvement when trained only
on the UD Treebank with no pre-trained word em-
beddings, to 4.7% and 5.4% improvements when
trained with Twitter embeddings and both Twitter
embeddings and synthetic data, respectively. The
latter improvements suggest that the ARK Tagger
setup is able to take better advantage of Twitter-
specific lexical information from the embeddings

12As the existing implementation of Deep Biaffine requires
pre-trained word embeddings, for the Deep Biaffine base-
line experiments we use the CoNLL 2017 Shared Task 100-
dimensional embeddings that were pretrained on the English
UD Treebank.

Model LAS
(1) UDPipe, Morpho-Tagger, UDT 50.5
(2) + Twitter embeddings 53.9
(3) + synthetic, Twitter embeddings 58.9
(4) UDPipe, ARK Tagger, UDT 53.3
(5) + Twitter embeddings 58.6
(6) + synthetic, Twitter embeddings 64.3
Deep Biaffine, UDT
(7) + CoNLL MAE embeddings 62.3
(8) + Twitter embeddings 63.7
(9) + synthetic, Twitter embeddings 65.0

Table 1: Results from cross-domain training set-
tings (see §4.2.1).

Model LAS
(10) UDPipe, Twitter embeddings 62.2
(11) + UDT 70.3

Table 2: Results from in-domain training settings
(with the ARK Tagger setting, see §4.2.2).

and syntactic patterns from the synthetic data. Ta-
ble 1 shows the LAS for our various settings.

After observing the better performance of the
ARK Tagger setting, we opted not to retrain the
Deep Biaffine parser in any Morpho-Tagger set-
tings due to the model’s significantly longer train-
ing time; all our Deep Biaffine results are reported
for models trained with an ARK Tagger setting.

Synthetic data and embeddings: We observed
that synthetic data and Twitter-trained embeddings
were independently helpful; embeddings provided
a 1.4–5.3% boost across the UDPipe and Deep Bi-
affine models, while synthetic data provided a 1.3–
5.7% additional boost (Table 1).

UDPipe vs. Deep Biaffine: While the base-
line models for UDPipe and Deep Biaffine are not
directly comparable (since the latter required pre-
trained embeddings), in the Twitter embeddings
setting Deep Biaffine outperformed UDPipe by
5.1%. However, given access to both synthetic
data and Twitter embeddings, UDPipe’s perfor-
mance approached that of Deep Biaffine.

5.2 Effects of In-Domain Training
Perhaps surprisingly, training with even limited
amounts of in-domain training data aided in pars-
ing performance; training with just in-domain data
produced an LAS comparable to that of the base-
line Deep Biaffine model, and adding UD Tree-
bank data further increased LAS by 8.1%, indicat-



1421

Model AA LAS WH LAS Gap
(1) UDPipe, Morpho-Tagger 43.0 57.0 14.0
(2) + Twitter embeddings 45.5 61.2 15.7
(3) + synthetic, Twitter embeddings 50.7 66.2 15.5
(4) UDPipe, ARK Tagger 50.2 56.1 5.9
(5) + Twitter embeddings 54.1 62.5 8.4
(6) + synthetic, Twitter embeddings 59.9 68.1 8.2
Deep Biaffine, ARK Tagger
(7) + CoNLL MAE embeddings 56.1 67.7 11.6
(8) + Twitter embeddings 58.7 66.7 8.0
(9) + synthetic, Twitter embeddings 59.9 70.8 10.9

Table 3: AA and WH tweets’ labeled attachment scores for UD Treebank-trained models (see §5.3 for
discussion); Gap is the WH− AA difference in LAS.

ing that they independently provide critical signal.

5.3 AAE/MAE Performance Disparity
For each model in each of the cross-domain set-
tings, we calculated the LAS on the 250 tweets
drawn from highly African-American tweets and
the 250 from highly White tweets (see §3 for de-
tails); we will refer to these as the AA and WH
tweets, respectively. We observed clear dispari-
ties in performance between the two sets of tweets,
ranging from 5.9% to 15.7% (Table 3). Addition-
ally, across settings, we observed several patterns.

First, the UDPipe ARK Tagger settings pro-
duced significantly smaller gaps (5.9–8.4%) than
the corresponding Morpho-Tagger settings (14.0–
15.7%). Indeed, most of the performance im-
provement of the ARK Tagger setting comes from
the AA tweets; the LAS on the AA tweets jumps
7.2–9.2% from each Morpho-Tagger setting to the
corresponding ARK Tagger setting, compared to
differences of −0.9–1.9% for the WH tweets.

Second, the Deep Biaffine ARK Tagger settings
produced larger gaps (8.0–11.6%) than the UD-
Pipe ARK Tagger settings, with the exception of
the embeddings-only setting.

Finally, we observed the surprising result that
adding Twitter-trained embeddings and synthetic
data, which contains both Twitter-specific and
AAE-specific features, increases the performance
gap across both UDPipe settings. We hypothesize
that while UDPipe is able to effectively make use
of both Twitter-specific lexical items and annota-
tion conventions within MAE-like syntactic struc-
tures, it continues to be stymied by AAE-like syn-
tactic structures, and is therefore unable to make
use of the additional information.

We further calculated recall for each relation

type across the AA tweets and WH tweets, and
the resulting performance gap, under the UDPipe
Morpho-Tagger and ARK Tagger models trained
with synthetic data and embeddings. Table 4
shows these calculations for the 15 relation types
for which the performance gap was highest and
which had at least 15 instances in each of the AA
and WH tweet sets, along with the correspond-
ing calculation under the ARK Tagger model. The
amount by which the performance gap is reduced
from the first setting to the second setting is also
reported. Of the 15 relations shown, the gap was
reduced for 14, and 7 saw a reduction of at least
10%.

5.4 Lexical and Syntactic Analysis of AAE

In this section, we discuss AAE lexical and syn-
tactic variations observed in our dataset, with the
aim of providing insight into decreased AA pars-
ing accuracy, and the impact of various parser set-
tings on their parsing accuracy.

AAE contains a variety of phonological features
which present themselves on Twitter through a
number of lexical variations (Green, 2002; Jones,
2015), many of which are listed in §3.1, instances
of which occur a total of 80 times in the AA
tweets; notably, none occur in the WH tweets.

We investigated the accuracy of various cross-
domain parser settings on these lexical variants;
for each of the baseline Morpho-Tagger, baseline
ARK Tagger, ARK Tagger with embeddings, and
ARK Tagger with synthetic data and embeddings
models, we counted the number of instances of
lexical variants from §3.1 for which the model
gave the correct head with the correct label.

While the lexical variants challenged all four
models, switching from the Morpho-Tagger set-



1422

Morpho-Tagger ARK Tagger
Relation AA

Recall
WH

Recall
Gap (WH - AA) AA

Recall
WH

Recall
Gap (WH - AA) Reduction

compound 36.4 71.2 34.8 42.4 72.9 30.5 4.4
obl:tmod 25.0 51.7 26.7 43.8 55.2 11.4 15.3

nmod 28.6 54.4 25.8 45.7 51.5 5.8 20.1
cop 56.5 82.1 25.6 65.2 79.1 13.9 11.7
obl 41.4 65.4 24.0 56.8 62.5 5.7 18.3
cc 56.9 79.0 22.1 78.5 82.7 4.3 17.8

ccomp 33.3 54.2 20.8 40.5 54.2 13.7 7.1
obj 61.3 81.5 20.2 72.8 83.5 10.7 9.5
case 60.5 79.8 19.3 75.2 83.4 8.2 11.1
det 73.1 90.7 17.5 83.4 92.2 8.8 8.7

advmod 53.8 71.2 17.3 62.9 72.1 9.1 8.2
advcl 31.5 46.8 15.3 25.9 46.8 20.9 -5.6
root 56.4 71.6 15.2 62.8 74.0 11.2 4.0

xcomp 40.0 54.9 14.9 51.2 50.0 1.2 13.7
discourse 30.7 44.9 14.2 46.0 51.4 5.4 8.8

Table 4: Recall by relation type under UDPipe’s Morpho-Tagger and ARK Tagger settings (+syn-
thetic+embeddings; (3) and (6) from Table 3; §5.3). Reduction is the reduction in performance gap
from the Morpho-Tagger setting to the ARK Tagger setting; bolded numbers indicate a gap reduction of
≥ 10.0.

Feature AA Count WH Count Example
Dropped copula 44 0 MY bestfrienddd mad at me tho

Habitual be, describing
repeated actions

10 0 fees be looking upside my head likee ion kno
wat be goingg on .
I kno that clown, u don’t be around tho

Dropped possessive marker 5 0 ATMENTION on Tv...tawkn bout dat man gf
Twink rude lol can’t be calling ppl ugly that’s
somebody child lol...

Dropped 3rd person singular 5 0 When a female owe you sex you don’t even
wanna have a conversation with her

Future gone 4 0 she gone dance without da bands lol
it is instead of there is 2 1 It was too much goin on in dat mofo .

Completive done 1 0 damnnn I done let alot of time pass by . .

Table 5: Examples of AAE syntactic phenomena and occurrence counts in the 250 AA and 250 WH
tweet sets.

ting to the ARK Tagger settings produced signif-
icant accuracy increases (Table 6). We observed
that the greatest improvement came from using
the ARK Tagger setting with Twitter-trained em-
beddings; the Twitter-specific lexical information
provided by the embeddings was critical to rec-
ognizing the variants. Surprisingly, adding syn-
thetic data decreased the model’s ability to parse
the variants.

We next investigated the presence of AAE syn-
tactic phenomena in our dataset. Table 5 shows ex-
amples of seven well-documented AAE morpho-
logical and syntactic features and counts of their
occurrences in our AA and WH tweet sets; again,
while several of the phenomena, such as dropped

copulas and habitual be, occur frequently in our
AA tweets, there is only one instance of any of
these features occurring in the WH tweet set.

We measured the parsing accuracy for the two
most frequent syntactic features, dropped copulas
and habitual be, across the four models; accura-
cies are given in Table 6. For dropped copulas,
we measured parsing correctness by checking if
the parser correctly attached the subject to the cor-
rect predicate word via the nsubj relation; for the
first example in Table 5, for example, we consid-
ered the parser correct if it attached bestfrienddd
to mad via the nsubj relation. For habitual be, we
checked for correct attachment via the aux or cop
relations as in the first and second examples in Ta-



1423

AAE Feature Morpho-Tagger
Baseline

ARK Tagger
Baseline

ARK Tagger
with

Embeddings

ARK Tagger
with Synthetic,

Embeddings
Lexical Variants

(§3.1)
16.3 (13/80) 61.3 (49/80) 63.8 (51/80) 57.5 (46/80)

Dropped copula 54.5 (24/44) 70.5 (31/44) 61.4 (27/44) 68.2 (30/44)
Habitual be 50.0 (5/10) 80.0 (8/10) 90.0 (9/10) 90.0 (9/10)

Table 6: Parsing accuracies of syntactic and lexical variations across four UDPipe models (see §5.4).

ble 5, respectively.
As before, we observed significant increases in

accuracy moving from the Morpho-Tagger to the
ARK Tagger settings. However, neither adding
embeddings nor synthetic data appeared to signif-
icantly increase accuracy for these features. From
manual inspection, most of the dropped copu-
las errors appear to arise either from challenging
questions (e.g. ATMENTION what yo number ?)
or from mis-identification of the word to which to
attach the subject (e.g. He claim he in love llh,
where he was attached to llh rather than to love).

6 Conclusion

While current neural dependency parsers are
highly accurate on MAE, our analyses suggest that
AAE text presents considerable challenges due to
lexical and syntactic features which diverge sys-
tematically from MAE. While the cross-domain
strategies we presented can greatly increase ac-
curate parsing of these features, narrowing the
performance gap between AAE- and MAE-like
tweets, much work remains to be done for accurate
parsing of even linguistically well-documented
features.

It remains an open question whether it is bet-
ter to use a model with a smaller accuracy dis-
parity (e.g. UDPipe), or a model with higher av-
erage accuracy, but a worse disparity (e.g. Deep
Biaffine). The emerging literature on fairness in
algorithms suggests interesting further challenges;
for example, Kleinberg et al. (2017) and Corbett-
Davies et al. (2017) argue that as various com-
monly applied notions of fairness are mutually
incompatible, algorithm designers must grapple
with such trade-offs. Regardless, the modeling de-
cision should be made in light of the application
of interest; for example, applications like opin-
ion analysis and information retrieval may bene-
fit from equal (and possibly weaker) performance
between groups, so that concepts or opinions in-

ferred from groups of authors (e.g. AAE speak-
ers) are not under-counted or under-represented in
results returned to a user or analyst.

Acknowledgments

We thank the anonymous reviewers for their help-
ful comments. This work was supported by a
Google Faculty Research Award, and a National
Science Foundation Graduate Research Fellow-
ship (No. 1451512).

References
Su Lin Blodgett, Lisa Green, and Brendan O’Connor.

2016. Demographic dialectal variation in social me-
dia: A case study of African-American English.
Proceedings of EMNLP.

Su Lin Blodgett and Brendan O’Connor. 2017. Racial
disparity in natural language processing: A case
study of social media African-American English.
arXiv preprint arXiv:1707.00061; presented at Fair-
ness, Accountability, and Transparency in Machine
Learning (FAT/ML) workshop at KDD 2017.

David Chiang, Mona Diab, Nizar Habash, Owen Ram-
bow, and Safiullah Shareef. 2006. Parsing Arabic
dialects. In Proceedings of EACL.

Sam Corbett-Davies, Emma Pierson, Avi Feller, Sharad
Goel, and Aziz Huq. 2017. Algorithmic decision
making and the cost of fairness. In Proceedings of
the KDD. ACM.

Joachim Daiber and Rob van der Goot. 2016. The de-
noised web treebank: Evaluating dependency pars-
ing under noisy input conditions. In Proceedings of
LREC.

Leon Derczynski, Alan Ritter, Sam Clark, and Kalina
Bontcheva. 2013. Twitter part-of-speech tagging for
all: Overcoming sparse and noisy data. In Recent
Advances in Natural Language Processing, pages
198–206.

Timothy Dozat and Christopher D Manning. 2016.
Deep biaffine attention for neural dependency pars-
ing. Proceedings of ICLR.



1424

Timothy Dozat, Peng Qi, and Christopher D. Manning.
2017. Stanford’s graph-based neural dependency
parser at the CoNLL 2017 shared task. In Proceed-
ings of the CoNLL 2017 Shared Task: Multilingual
Parsing from Raw Text to Universal Dependencies.

Jacob Eisenstein. 2013. What to do about bad language
on the internet. In HLT-NAACL, pages 359–369.

Jennifer Foster, Özlem Çetinoglu, Joachim Wagner,
Joseph Le Roux, Stephen Hogan, Joakim Nivre,
Deirdre Hogan, and Josef Van Genabith. 2011. #
hardtoparse: Pos tagging and parsing the twitter-
verse. In AAAI 2011 Workshop on Analyzing Mi-
crotext.

Rob van der Goot and Gertjan van Noord. 2017. Parser
adaptation for social media by integrating normal-
ization. In Proceedings of ACL.

Lisa Green, Toya A Wyatt, and Qiuana Lopez. 2007.
Event arguments and ‘be’ in child African American
English. University of Pennsylvania Working Papers
in Linguistics, 13(2):8.

Lisa J Green. 2002. African American English: A lin-
guistic introduction. Cambridge University Press.

Spence Green and Christopher D Manning. 2010. Bet-
ter arabic parsing: Baselines, evaluations, and anal-
ysis. In Proceedings of COLING. ACL.

Taylor Jones. 2015. Toward a description of African
American Vernacular English dialect regions using
“Black Twitter”. American Speech, 90(4).

Anna Jørgensen, Dirk Hovy, and Anders Søgaard.
2016. Learning a pos tagger for aave-like language.
In Proceedings of NAACL. Association for Compu-
tational Linguistics.

David Jurgens, Yulia Tsvetkov, and Dan Jurafsky.
2017. Incorporating dialectal variability for socially
equitable language identification. In Proceedings of
ACL.

Mohammad Khan, Markus Dickinson, and Sandra
Kübler. 2013. Towards domain adaptation for pars-
ing web data. In RANLP.

Jon Kleinberg, Sendhil Mullainathan, and Manish
Raghavan. 2017. Inherent trade-offs in the fair de-
termination of risk scores. Proceedings of ITCS.

Lingpeng Kong, Nathan Schneider, Swabha
Swayamdipta, Archna Bhatia, Chris Dyer, and
Noah A. Smith. 2014. A dependency parser for
tweets. In Proceedings of the 2014 Conference
on Empirical Methods in Natural Language Pro-
cessing (EMNLP), pages 1001–1012, Doha, Qatar.
Association for Computational Linguistics.

Yijia Liu, Yi Zhu, Wanxiang Che, Bing Qin, Nathan
Schneider, and Noah A Smith. 2018. Parsing
tweets into universal dependencies. Proceedings of
NAACL.

Tomas Mikolov, Kai Chen, Greg Corrado, and Jef-
frey Dean. 2013. Efficient estimation of word
representations in vector space. arXiv preprint
arXiv:1301.3781.

Alexis Nasr, Geraldine Damnati, Aleksandra Guerraz,
and Frederic Bechet. 2016. Syntactic parsing of
chat language in contact center conversation corpus.
In Annual SIGdial Meeting on Discourse and Dia-
logue.

Joakim Nivre, Marie-Catherine de Marneffe, Filip Gin-
ter, Yoav Goldberg, Jan Hajic, Christopher D Man-
ning, Ryan McDonald, Slav Petrov, Sampo Pyysalo,
Natalia Silveira, Reut Tsarfaty, and Daniel Zeman.
2016. Universal Dependencies v1: A multilingual
treebank collection. In Proceedings of the 10th In-
ternational Conference on Language Resources and
Evaluation (LREC 2016).

Brendan O’Connor, Michel Krieger, and David Ahn.
2010. TweetMotif: Exploratory search and topic
summarization for Twitter. In Proceedings of the In-
ternational AAAI Conference on Weblogs and Social
Media.

Olutobi Owoputi, Brendan O’Connor, Chris Dyer,
Kevin Gimpel, Nathan Schneider, and Noah A.
Smith. 2013. Improved part-of-speech tagging for
online conversational text with word clusters. In
Proceedings of NAACL.

Manuela Sanguinetti, Cristina Bosco, Alessandro
Mazzei, Alberto Lavelli, and Fabio Tamburini. 2017.
Annotating Italian social media texts in universal de-
pendencies. In Proceedings of Depling 2017.

Yves Scherrer. 2011. Syntactic transformations for
Swiss German dialects. In Proceedings of the First
Workshop on Algorithms and Resources for Mod-
elling of Dialects and Language Varieties. ACL.

Nathan Schneider, Brendan O’Connor, Naomi Saphra,
David Bamman, Manaal Faruqui, Noah A. Smith,
Chris Dyer, and Jason Baldridge. 2013. A frame-
work for (under)specifying dependency syntax with-
out overloading annotators. In Proceedings of the
7th Linguistic Annotation Workshop and Interoper-
ability with Discourse, pages 51–60, Sofia, Bulgaria.
Association for Computational Linguistics.

Ian Stewart. 2014. Now we stronger than ever:
African-american english syntax in twitter. In Pro-
ceedings of the Student Research Workshop at the
14th Conference of the European Chapter of the As-
sociation for Computational Linguistics, pages 31–
37, Gothenburg, Sweden. Association for Computa-
tional Linguistics.

Milan Straka, Jan Hajic, and Jana Straková. 2016.
Udpipe: Trainable pipeline for processing conll-u
files performing tokenization, morphological anal-
ysis, pos tagging and parsing. In Proceedings of
LREC.

http://www.aclweb.org/anthology/D14-1108
http://www.aclweb.org/anthology/D14-1108
http://www.aclweb.org/anthology/W13-2307
http://www.aclweb.org/anthology/W13-2307
http://www.aclweb.org/anthology/W13-2307
http://www.aclweb.org/anthology/E14-3004
http://www.aclweb.org/anthology/E14-3004


1425

Hongmin Wang, Yue Zhang, GuangYong Leonard
Chan, Jie Yang, and Hai Leong Chieu. 2017. Uni-
versal dependencies parsing for colloquial Singa-
porean english. Proceedings of ACL.

Aaron Steven White, Drew Reisinger, Keisuke Sak-
aguchi, Tim Vieira, Sheng Zhang, Rachel Rudinger,
Kyle Rawlins, and Benjamin Van Durme. 2016.
Universal decompositional semantics on universal
dependencies. In Proceedings of the 2016 Confer-
ence on Empirical Methods in Natural Language
Processing, pages 1713–1723, Austin, Texas. Asso-
ciation for Computational Linguistics.

Daniel Zeman, Martin Popel, Milan Straka, Jan Ha-
jic, Joakim Nivre, Filip Ginter, Juhani Luotolahti,
Sampo Pyysalo, Slav Petrov, Martin Potthast, et al.
2017. CoNLL 2017 shared task: Multilingual pars-
ing from raw text to universal dependencies. Pro-
ceedings of the CoNLL 2017 Shared Task: Multilin-
gual Parsing from Raw Text to Universal Dependen-
cies.

Congle Zhang, Tyler Baldwin, Howard Ho, Benny
Kimelfeld, and Yunyao Li. 2013. Adaptive parser-
centric text normalization. In Proceedings of ACL.

https://aclweb.org/anthology/D16-1177
https://aclweb.org/anthology/D16-1177

