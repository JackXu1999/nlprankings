



















































Gromov-Wasserstein Alignment of Word Embedding Spaces


Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 1881–1890
Brussels, Belgium, October 31 - November 4, 2018. c©2018 Association for Computational Linguistics

1881

Gromov-Wasserstein Alignment of Word Embedding Spaces

David Alvarez-Melis
CSAIL, MIT

dalvmel@mit.edu

Tommi S. Jaakkola
CSAIL, MIT

tommi@mit.edu

Abstract
Cross-lingual or cross-domain correspon-
dences play key roles in tasks ranging from
machine translation to transfer learning. Re-
cently, purely unsupervised methods operating
on monolingual embeddings have become ef-
fective alignment tools. Current state-of-the-
art methods, however, involve multiple steps,
including heuristic post-hoc refinement strate-
gies. In this paper, we cast the correspon-
dence problem directly as an optimal trans-
port (OT) problem, building on the idea that
word embeddings arise from metric recovery
algorithms. Indeed, we exploit the Gromov-
Wasserstein distance that measures how sim-
ilarities between pairs of words relate across
languages. We show that our OT objective
can be estimated efficiently, requires little or
no tuning, and results in performance compa-
rable with the state-of-the-art in various unsu-
pervised word translation tasks.

1 Introduction

Many key linguistic tasks, within and across lan-
guages or domains, including machine translation,
rely on learning cross-lingual correspondences be-
tween words or other semantic units. While the as-
sociated alignment problem could be solved with
access to large amounts of parallel data, broader
applicability relies on the ability to do so with
largely mono-lingual data, from Part-of-Speech
(POS) tagging (Zhang et al., 2016), dependency
parsing (Guo et al., 2015), to machine translation
(Lample et al., 2018). The key subtask of bilingual
lexical induction, for example, while long stand-
ing as a problem (Fung, 1995; Rapp, 1995, 1999),
has been actively pursued recently (Artetxe et al.,
2016; Zhang et al., 2017a; Conneau et al., 2018).

Current methods for learning cross-domain cor-
respondences at the word level rely on distributed
representations of words, building on the observa-
tion that mono-lingual word embeddings exhibit

similar geometric properties across languages
Mikolov et al. (2013). While most early work
assumed some, albeit minimal, amount of paral-
lel data (Mikolov et al., 2013; Dinu et al., 2014;
Zhang et al., 2016), recently fully-unsupervised
methods have been shown to perform on par
with their supervised counterparts (Conneau et al.,
2018; Artetxe et al., 2018). While successful, the
mappings arise from multiple steps of process-
ing, requiring either careful initial guesses or post-
mapping refinements, including mitigating the ef-
fect of frequent words on neighborhoods. The as-
sociated adversarial training schemes can also be
challenging to tune properly (Artetxe et al., 2018).

In this paper, we propose a direct optimization
approach to solving correspondences based on re-
cent generalizations of optimal transport (OT). OT
is a general mathematical toolbox used to evalu-
ate correspondence-based distances and establish
mappings between probability distributions, in-
cluding discrete distributions such as point-sets.
However, the nature of mono-lingual word embed-
dings renders the classic formulation of OT inap-
plicable to our setting. Indeed, word embeddings
are estimated primarily in a relational manner to
the extent that the algorithms are naturally in-
terpreted as metric recovery methods (Hashimoto
et al., 2016). In such settings, previous work
has sought to bypass this lack of registration by
jointly optimizing over a matching and an or-
thogonal mapping (Rangarajan et al., 1997; Zhang
et al., 2017b). Due to the focus on distances rather
than points, we instead adopt a relational OT for-
mulation based on the Gromov-Wasserstein dis-
tance that measures how distances between pairs
of words are mapped across languages. We show
that the resulting mapping admits an efficient so-
lution and requires little or no tuning.

In summary, we make the following contribu-
tions:



1882

• We propose the use of the Gromov-
Wasserstein distance to learn correspon-
dences between word embedding spaces
in a fully-unsupervised manner, leading to
a theoretically-motivated optimization prob-
lem that can be solved efficiently, robustly, in
a single step, and requires no post-processing
or heuristic adjustments.

• To scale up to large vocabularies we realize
an extended mapping to words not part of the
original optimization problem.

• We show that the proposed approach per-
forms on par with state-of-the-art neural net-
work based methods on benchmark word
translation tasks, while requiring a frac-
tion of the computational cost and/or hyper-
parameter tuning.

2 Problem Formulation

In the unsupervised bilingual lexical induction
problem we consider two languages with vocabu-
laries Vx and Vy, represented by word embeddings
X = {x(i)}ni=1 and Y = {y(j)}mj=1, respectively,
where x(i) ∈ X ⊂ Rdx corresponds to wxi ∈ Vx
and y(j) ∈ Y ⊂ Rdy to wyj ∈ Vy. For simplicity,
we let m = n and dx = dy, although our meth-
ods carry over to the general case with little or no
modifications. Our goal is to learn an alignment
between these two sets of words without any par-
allel data, i.e., we learn to relate x(i) ↔ y(j) with
the implication that wxi translates to w

y
j .

As background, we begin by discussing the
problem of learning an explicit map between em-
beddings in the supervised scenario. The associ-
ated training procedure will later be used for ex-
tending unsupervised alignments (Section 3.2).

2.1 Supervised Maps: Procrustes
In the supervised setting, we learn a map T : X →
Y such that T (x(i)) ≈ y(j) whenever wyj is a
translation of wxi . Let X and Y be the matrices
whose columns are vectors x(i) and y(j), respec-
tively. Then we can find T by solving

min
T∈F
‖X− T (Y)‖2F (1)

where ‖ · ‖F is the Frobenius norm ‖A‖F =√∑
i,j |aij |2. Naturally, both the difficulty of

finding T and the quality of the resulting align-
ment depend on the choice of space F . A classic

approach constrains T to be orthonormal matrices,
i.e., rotations and reflections, resulting in the or-
thogonal Procrustes problem

min
P∈O(n)

‖X−PY‖2F (2)

where O(n) = {P ∈ Rn×n | P>P = I}.
One key advantage of this formulation is that
it has a closed-form solution in terms of a sin-
gular value decomposition (SVD), whereas for
most other choices of constraint set F it does
not. Given an SVD decomposition UΣV> of
XY>, the solution to problem (2) is P∗ = UV>

(Schönemann, 1966). Besides obvious compu-
tational advantage, constraining the mapping be-
tween spaces to be orthonormal is justified in the
context of word embedding alignment because
orthogonal maps preserve angles (and thus dis-
tances), which is often the only information used
by downstream tasks (e.g., for nearest neighbor
search) that rely on word embeddings. (Smith
et al., 2017) further show that orthogonality is re-
quired for self-consistency of linear transforma-
tions between vector spaces.

Clearly, the Procrustes approach only solves the
supervised version of the problem as it requires a
known correspondence between the columns of X
and Y. Steps beyond this constraint include using
small amounts of parallel data (Zhang et al., 2016)
or an unsupervised technique as the initial step
to generate pseudo-parallel data (Conneau et al.,
2018) before solving for P.

2.2 Unsupervised Maps: Optimal Transport
Optimal transport formalizes the problem of find-
ing a minimum cost mapping between two point
sets, viewed as discrete distributions. Specifically,
we assume two empirical distributions over em-
beddings, e.g.,

µ =
n∑

i=1

piδx(i) , ν =
m∑
j=1

qjδy(i) (3)

where p and q are vectors of probability weights
associated with each point set. In our case, we
usually consider uniform weights, e.g., pi = 1/n
and qj = 1/m, although if additional information
were provided (such as in the form of word fre-
quencies), those could be naturally incorporated
via p and q (see discussion at the end of Section
3). We find a transportation map T realizing

inf
T

{∫
X
c(x, T (x))dµ(x) | T#µ = ν

}
, (4)



1883

where the cost c(x, T (x)) is typically just ‖x −
T (x)‖ and T#µ = ν implies that the source points
must exactly map to the targets. However, such a
map need not exist in general and we instead fol-
low a relaxed Kantorovich’s formulation. In this
case, the set of transportation plans is a polytope:

Π(p,q) = {Γ ∈ Rn×m+ | Γ1n = p, Γ>1n = q}.

The cost function is given as a matrix C ∈ Rn×m,
e.g., Cij = ‖x(i) − y(j)‖. The total cost incurred
by Γ is 〈Γ, C〉 :=

∑
ij ΓijCij . Thus, the discrete

optimal transport (DOT) problem consists of find-
ing a plan Γ that solves

min
Γ∈Π(p,q)

〈Γ,C〉. (5)

Problem (5) is a linear program, and thus can be
solved exactly in O(n3 log n) with interior point
methods. However, regularizing the objective
leads to more efficient optimization and often bet-
ter empirical results. The most common such reg-
ularization, popularized by Cuturi (2013), involves
adding an entropy penalization:

min
Γ∈Π(p,q)

〈Γ,C〉 − λH(Γ). (6)

The solution of this strictly convex optimization
problem has the form Γ∗ = diag (a)Kdiag (b),
with K = e−

C
λ (element-wise), and can be ob-

tained efficiently via the Sinkhorn-Knopp algo-
rithm, a matrix-scaling procedure which itera-
tively computes:

a← p�Kb and b← q�K>a, (7)

where � denotes entry-wise division. The deriva-
tion of these updates is immediate from the form
of Γ∗ above, combined with the marginal con-
straints Γ1n = p, Γ>1n = q (Peyré and Cuturi,
2018).

Although simple, efficient and theoretically-
motivated, a direct application of discrete OT for
unsupervised word translation is not appropriate.
One reason is that the mono-lingual embeddings
are estimated in a relative manner, leaving, e.g.,
an overall rotation unspecified. Such degrees of
freedom can dramatically change the entries of the
cost matrix Cij = ‖x(i) − y(j)‖ and the resulting
transport map. One possible solution is to simulta-
neously learn an optimal coupling and an orthog-
onal transformation (Zhang et al., 2017b). The
transport problem is then solved iteratively, using

Cij = ‖x(i) − Py(j)‖, where P is in turn cho-
sen to minimize the transport cost (via Procrustes).
While promising, the resulting iterative approach
is sensitive to initialization, perhaps explaining
why Zhang et al. (2017b) used an adversarially
learned mapping as the initial step. The compu-
tational cost can also be prohibitive (Artetxe et al.,
2018) though could be remedied with additional
development.

We adopt a theoretically well-founded gener-
alization of optimal transport for pairs of points
(their distances), thus in line with how the embed-
dings are estimated in the first place. We explain
the approach in detail in the next Section.

3 Transporting across unaligned spaces

In this section we introduce the Gromov-
Wasserstein distance, describe an optimization al-
gorithm for it, and discuss how to extend the ap-
proach to out-of-sample vectors.

3.1 The Gromov Wasserstein Distance

The classic optimal transport requires a distance
between vectors across the two domains. Such
a metric may not be available, for example,
when the sample sets to be matched do not be-
long to the same metric space (e.g., different
dimension). The Gromov-Wasserstein distance
(Mémoli, 2011) generalizes optimal transport by
comparing the metric spaces directly instead of
samples across the spaces. In other words, this
framework operates on distances between pairs of
points calculated within each domain and mea-
sures how these distances compare to those in the
other domain. Thus, it requires a weaker but easy
to define notion of distance between distances, and
operates on pairs of points, turning the problem
from a linear to a quadratic one.

Formally, in its discrete version, this framework
considers two measure spaces expressed in terms
of within-domain similarity matrices (C,p) and
(C′,q) and a loss function defined between simi-
larity pairs: L : R × R → R, where L(Cik, C ′jl)
measures the discrepancy between the distances
d(x(i),x(k)) and d′(y(j),y(l)). Typical choices for
L are L(a, b) = 12(a − b)

2 or L(a, b) = KL(a|b).
In this framework, L(Cik, C ′jl) can also be under-
stood as the cost of “matching” i to j and k to l.

All the relevant values of L(·, ·) can be put in
a 4-th order tensor L ∈ RN1×N1×N2×N2 , where
Lijkl = L(Cik, C

′
jl). As before, we seek a cou-



1884

in
mo

st
yo

rk
sta

tio
n
me

dia
ne

ve
r

we
nt

use
rs

me
an

s ye
t

EN

in

most

york

station

media

never

went

users

means

yet

EN
Intra-Lang. Similarities (EN)

)
alle olt

re

alle
na

tor
e

scr
itto

re

pre
sen

za
con

tea lat
o

da
llo

dif
esa

IT

)

alle

oltre

allenatore

scrittore

presenza

contea

lato

dallo

difesa

IT

Intra-Lang. Similarities (IT)

)
alle olt

re

alle
na

tor
e

scr
itto

re

pre
sen

za
con

tea lat
o

da
llo

dif
esa

IT

in

most

york

station

media

never

went

users

means

yet

EN

Optimal GW Coupling

on
e

tw
o

th
re

e
ja

nu
ar

y
fe

br
ua

ry

one
two

three
january

february

un
o

du
e tre

ge
nn

ai
o

fe
bb

ra
io

uno
due
tre

gennaio
febbraio

un
o

du
e tre

ge
nn

ai
o

fe
bb

ra
io

one
two

three
january

february

Figure 1: The Gromov-Wasserstein distance is well suited for the task of cross-lingual alignment be-
cause it relies on relational rather than positional similarities to infer correspondences across domains.
Computing it requires two intra-domain similarity (or equivalently cost) matrices (left & center), and it
produces an optimal coupling of source and target points with minimal discrepancy cost (right).

pling Γ specifying how much mass to transfer be-
tween each pair of points from the two spaces. The
Gromov-Wasserstein problem is then defined as
solving

GW(C,C′,p,q) = min
Γ∈Π(p,q)

∑
i,j,k,l

LijklΓijΓkl (8)

Compared to problem (5), this version is sub-
stantially harder since the objective is now not
only non-linear, but non-convex too.1 In addi-
tion, it requires operating on a fourth-order ten-
sor, which would be prohibitive in most settings.
Surprisingly, this problem can be optimized ef-
ficiently with first-order methods, whereby each
iteration involves solving a traditional optimal
transport problem (Peyré et al., 2016). Fur-
thermore, for suitable choices of loss function
L, Peyré et al. (2016) show that instead of the
O(N21N

2
2 ) complexity implied by naive fourth-

order tensor product, this computation reduces to
O(N21N2 + N1N

2
2 ) cost. Their approach con-

sists of solving (5) by projected gradient descent,
which yields iterations that involve projecting onto
Π(p,q) a pseudo-cost matrix of the form

ĈΓ(C,C
′,Γ) = Cxy − h1(C)Γh2(C′)> (9)

where

Cxy = f1(C)p1
>
m + 1nq

>f2(C
′)>

and f1, f2, h2, h2 are functions that depend on the
loss L. We provide an explicit algorithm for the
case L = L2 at the end of this section.

1In fact, the discrete (Monge-type) formulation of the
problem is essentially an instance of the well-known (and
NP-hard) quadratic assignment problem (QAP).

Once we have solved (8), the optimal trans-
port coupling Γ∗ provides an explicit (soft) match-
ing between source and target samples, which for
the problem of interest can be interpreted as a
probabilistic translation: for every pair of words
(w

(i)
src, w

(j)
trg), Γ

∗
ij provides a likelihood that these

two words are translations of each other. This
itself is enough to translate, and we show in
the experiments section that Γ∗ by itself, with-
out any further post-processing, provides high-
quality translations. This stands in sharp con-
trast to mapping-based methods, which rely on
nearest-neighbor computation to infer transla-
tions, and thus become prone to hub-word effects
which have to be mitigated with heuristic post-
processing techniques such as Inverted Softmax
(Smith et al., 2017) and Cross-Domain Similarity
Scaling (CSLS) (Conneau et al., 2018). The trans-
portation coupling Γ, being normalized by con-
struction, requires no such artifacts.

The Gromov-Wasserstein problem (8) pos-
sesses various desirable theoretical properties, in-
cluding the fact that for a suitable choice of the
loss function it is indeed a distance:

Theorem 3.1 (Mémoli 2011). With the choice
L = L2, GW

1
2 is a distance on the space of metric

measure spaces.

Solving problem (8) therefore yields a fas-
cinating accompanying notion: the Gromov-
Wasserstein distance between languages, a mea-
sure of semantic discrepancy purely based on the
relational characterization of their word embed-
dings. Owing to Theorem 3.1, such values can be



1885

interpreted as distances, so that, e.g., the triangle
inequality holds among them. In Section 4.4 we
compare various languages in terms of their GW-
distance.

Finally, we note that whenever word frequency
counts are available, those would be used for p
and q. If they are not, but words are sorted ac-
cording to occurrence (as they often are in popu-
lar off-the-shelf embedding formats), one can es-
timate rank-probabilities such as Zipf power laws,
which are known to accurately model multiple lan-
guages (Piantadosi, 2014). In order to provide a
fair comparison to previous work, throughout our
experiments we use uniform distributions so as
to avoid providing our method with additional in-
formation not available to others.

3.2 Scaling Up
While the pure Gromov-Wasserstein approach
leads to high quality solutions, it is best suited
to small-to-moderate vocabulary sizes,2 since its
optimization becomes prohibitive for very large
problems. For such settings, we propose a two-
step approach in which we first match a subset
of the vocabulary via the optimal coupling, after
which we learn an orthogonal mapping through a
modified Procrustes problem. Formally, suppose
we solve problem (8) for a reduced matrices X1:k
and Yi:k consisting of the first columns k of X
and Y, respectively, and let Γ∗ be the optimal
coupling. We seek an orthogonal matrix that best
recovers the barycentric mapping implied by Γ∗.
Namely, we seek to find P which solves:

min
P∈O(n)

‖XΓ∗ −PY‖22 (10)

Just as problem (2), it is easy to show that this
Procrustes-type problem has a closed form solu-
tion in terms of a singular value decomposition.
Namely, the solution to (10) is P∗ = UV>, where
UΣV∗ = X1:mΓ

∗Y>1:m. After obtaining this pro-
jection, we can immediately map the rest of the
embeddings via ŷ(j) = P∗y(j).

We point out that this two-step procedure re-
sembles that of Conneau et al. (2018). Both ul-
timately produce an orthogonal mapping obtained
by solving a Procrustes problems, but they differ
in the way they produce pseudo-matches to allow
for such second-step: while their approach relies

2As shown in the experimental section, we are able to run
problems of size in the order of |Vs| ≈ 105 ≈ |Vt| on a single
machine without relying on GPU computation.

Algorithm 1 Gromov-Wasserstein Computation
for Word Embedding Alignment

Input: Source and target embeddings X, Y.
Regularization λ. Probability vectors p,q.
// Compute intra-language similarities
Cs ← cos(X,X), Ct ← cos(Y,Y)
Cst ← C2sp1>m + 1nq(C2t )>
while not converged do

// Compute pseudo-cost matrix (Eq. (9))
ĈΓ ← Cst − 2CsΓC>t
// Sinkhorn iterations (Eq. (7))
a← 1, K← exp{−ĈΓ/λ}
while not converged do
a← p�Kb, b← q�K>a

end while
Γ← diag (a)Kdiag (b)

end while
// Optional step: Learn explicit projection
U,Σ,V> ← SVD(XΓY>)
P = UV>

return Γ,P

on an adversarially-learned transformation, we use
an explicit optimization problem.

We end this section by discussing parameter and
configuration choices. To leverage the fast algo-
rithm of Peyré et al. (2016), we always use the L2
distance as the loss function L between cost ma-
trices. On the other hand, we observed throughout
our experiments that the choice of cosine distance
as the metric in both spaces consistently leads to
better results, which agrees with common wis-
dom on computing distances between word em-
beddings. This leaves us with a single hyper-
parameter to control: the entropy regularization
term λ. By applying any sensible normalization
on the cost matrices (e.g., dividing by the mean
or median value), we are able to almost entirely
eliminate sensitivity to that parameter. In prac-
tice, we use a simple scheme in all experiments:
we first try the same fixed value (λ = 5× 10−5),
and if the regularization proves too small (by lead-
ing to floating point errors), we instead use λ =
1× 10−4. We never had to go beyond these two
values in all our experiments. We emphasize that
at no point we use train (let alone test) supervision
available with many datasets—model selection is
done solely in terms of the unsupervised objective.
Pseudocode for the full method (with L = L2 and
cosine similarity) is shown here as Algorithm 1.



1886

0 10 20 30 40 50
Iteration

0

20

40

60

80
Ac

cu
ra

cy
 (%

)

3.0

3.5

4.0

4.5

5.0

5.5

6.0

Di
st

an
ce

 (O
pt

. O
bj

ec
tiv

e)

1e 3

(a) EN→FR, 15K words, λ = 5 · 10−4

0 10 20 30 40 50
Iteration

0

20

40

60

80

Ac
cu

ra
cy

 (%
)

Acc@1
Acc@5
Acc@10

3

4

5

6

Di
st

an
ce

 (O
pt

. O
bj

ec
tiv

e)

1e 3

(b) EN→FR, 15K words, λ = 10−4

0 10 20 30 40 50
Iteration

0

5

10

15

20

25

30

35

40

Ac
cu

ra
cy

 (%
)

3.5

4.0

4.5

5.0

5.5

6.0

6.5

Di
st

an
ce

 (O
pt

. O
bj

ec
tiv

e)

1e 3

(c) EN→RU, 15K words, λ = 10−4

Figure 2: Training dynamics for the Gromov-Wasserstein alignment problem. The algorithm prov-
ably makes progress in each iteration, and the objective (red dashed line) closely follows the metric of
interest (translation accuracy, not available during training). More related languages (e.g., EN →FR in
2b,2a) lead to faster optimization, while more distant pairs yield slower learning curves (EN→RU, 2c).

4 Experiments

Through this experimental evaluation we seek
to: (i) understand the optimization dynamics of
the proposed approach (§4.2), evaluate its perfor-
mance on benchmark cross-lingual word embed-
ding tasks (§4.3), and (iii) qualitatively investi-
gate the notion of distance-between-languages it
computes (§4.4). Rather than focusing solely on
prediction accuracy, we seek to demonstrate that
the proposed approach offers a fast, principled,
and robust alternative to state-of-the-art multi-step
methods, delivering comparable performance.

4.1 Evaluation Tasks and Methods
Datasets We evaluate our method on two stan-
dard benchmark tasks for cross-lingual embed-
dings. First, we consider the dataset of Conneau
et al. (2018), which consists of word embeddings
trained with FASTTEXT (Bojanowski et al., 2017)
on Wikipedia and parallel dictionaries for 110 lan-
guage pairs. Here, we focus on the language
pairs for which they report results: English (EN)
from/to Spanish (ES), French (FR), German (DE),
Russian (RU) and simplified Chinese (ZH). We do
not report results on Esperanto (EO) as dictionar-
ies for that language were not provided with the
original dataset release.

For our second set of experiments, we con-
sider the—substantially harder3—dataset of (Dinu
et al., 2014), which has been extensively compared
against in previous work. It consists of embed-
dings and dictionaries in four pairs of languages;
EN from/to ES, IT, DE, and FI (Finnish).

3We discuss the difference in hardness of these two bench-
mark datasets in Section 4.3.

Methods To see how our fully-unsupervised
method compares with methods that require
(some) cross-lingual supervision, we follow (Con-
neau et al., 2018) and consider a simple but strong
baseline consisting of solving a procrustes prob-
lem directly using the available cross-lingual em-
bedding pairs. We refer to this method sim-
ply as PROCRUSTES. In addition, we compare
against the fully-unsupervised methods of Zhang
et al. (2017a), Artetxe et al. (2018) and Conneau
et al. (2018).4 As proposed by the latter, we
use CSLS whenever nearest neighbor search is re-
quired, which has been shown to improve upon
naive nearest-neighbor retrieval in multiple work.

4.2 Training Dynamics of G-W
As previously mentioned, our approach involves
only two optimization choices, one of which is
required only for very large settings. When run-
ning Algorithm 1 for the full set of embeddings is
infeasible (due to memory limitations), one must
decide what fraction of the embeddings to use
during optimization. In our experiments, we use
the largest possible size allowed by memory con-
straints, which was found to be K = 20, 000 for
the personal computer we used.

The other—more interesting—optimization
choice involves the entropy regularization pa-
rameter λ used within the Sinkhorn iterations.
Large regularization values lead to denser optimal
coupling Γ∗, while less regularization leads to
sparser solutions,5 at the cost of a harder (more

4Despite its relevance, we do not include the OT-based
method of Zhang et al. (2017b) in the comparison because
their implementation required use of proprietary software.

5In the limit λ→ 0, when n = m, the solution converges



1887

EN-ES EN-FR EN-DE EN-IT EN-RU

Supervision Time → ← → ← → ← → ← → ←

PROCRUSTES 5K words 3 77.6 77.2 74.9 75.9 68.4 67.7 73.9 73.8 47.2 58.2
PROCRUSTES + CSLS 5K words 3 81.2 82.3 81.2 82.2 73.6 71.9 76.3 75.5 51.7 63.7
(Conneau et al., 2018) None 957 81.7 83.3 82.3 82.1 74.0 72.2 77.4 76.1 52.4 61.4

G-W (λ = 10−4) None 70 78.3 79.5 79.3 78.3 69.6 66.9 75.3 74.1 26.1 35.4
G-W (λ = 10−5) None 37 81.7 80.4 81.3 78.9 71.9 72.8 78.9 75.2 45.1 43.7

Table 1: Performance (P@1) of unsupervised and minimally-supervised methods on the dataset of Con-
neau et al. (2018). The time columns shows the average runtime in minutes of an instance (i.e., one
language pair) of the method in this task on the same quad-core CPU machine.

non-convex) optimization problem.
In Figure 2 we show the training dynamics of

our method when learning correspondences be-
tween word embeddings from the dataset of Con-
neau et al. (2018). As expected, larger values
of λ lead to smoother improvements with faster
runtime-per-iteration, at a price of some drop in
performance. In addition, we found that comput-
ing GW distances between closer languages (such
as EN and FR) leads to faster convergence than for
more distant ones (such as EN and RU, in Fig. 2c).

Worth emphasizing are three desirable opti-
mization properties that set apart the Gromov-
Wasserstein distance from other unsupervised
alignment approaches, particularly adversarial-
training ones: (i) the objective decreases mono-
tonically (ii) its value closely follows the true
metric of interest (translation, which naturally is
not available during training) and (iii) there is no
risk of degradation due to overtraining, as is the
case for adversarial-based methods trained with
stochastic gradient descent (Conneau et al., 2018).

4.3 Benchmark Results

We report the results on the dataset of Conneau
et al. (2018) in Table 1. The strikingly high per-
formance of all methods on this task belies the
hardness of the general problem of unsupervised
cross-lingual alignment. Indeed, as pointed out
by Artetxe et al. (2018), the FASTTEXT embed-
dings provided in this task are trained on very
large and highly comparable—across languages—
corpora (Wikipedia), and focuses on closely re-
lated pairs of languages. Nevertheless, we carry
out experiments here to have a broad evaluation of
our approach in both easier and harder settings.

Next, we present results on the more challeng-

to a permutation matrix, which gives a hard-matching solu-
tion to the transportation problem (Peyré and Cuturi, 2018).

0 2000 4000 6000 8000 10000

0

2000

4000

6000

8000

10000

Ground Cost Source

0 2000 4000 6000 8000 10000

0

2000

4000

6000

8000

10000

Ground Cost Target

0 2000 4000 6000 8000 10000

0

2000

4000

6000

8000

10000

Gamma t=0

0.0

0.2

0.4

0.6

0.8

1.0

1.2

0.0

0.2

0.4

0.6

0.8

1.0

1.2

1.4

0.900

0.925

0.950

0.975

1.000

1.025

1.050

1.075

1.1001e 8

0 2000 4000 6000 8000 10000

0

2000

4000

6000

8000

10000

Ground Cost Source

0 2000 4000 6000 8000 10000

0

2000

4000

6000

8000

10000

Ground Cost Target

0 2000 4000 6000 8000 10000

0

2000

4000

6000

8000

10000

Gamma t=0

0.0

0.2

0.4

0.6

0.8

1.0

0.0

0.2

0.4

0.6

0.8

1.0

0.900

0.925

0.950

0.975

1.000

1.025

1.050

1.075

1.1001e 8

Figure 3: Top: Word embeddings trained on non-
comparable corpora can lead to uneven distribu-
tions of pairwise distances as shown here for the
EN-FI pair of (Dinu et al., 2014). Bottom: Nor-
malizing the cost matrices leads to better optimiza-
tion and improved performance.

ing dataset of (Dinu et al., 2014) in Table 2. Here,
we rely on the results reported by (Artetxe et al.,
2018) since by the time of writing the present work
their implementation was not available yet.

Part of what makes this dataset hard is the wide
discrepancy between word distance across lan-
guages, which translates into uneven distance ma-
trices (Figure 3), and in turn leads to poor results
for G-W. To account for this, previous work has
relied on an initial whitening step on the embed-
dings. In our case, it suffices to normalize the
pairwise similarity matrices to the same range to
obtain substantially better results. While we have
observed that careful choice of the regularization
parameter λ can obviate the need for this step, we
opt for the normalization approach since it allows
us to optimize without having to tune λ.

We compare our method (with and without nor-



1888

EN-IT EN-DE EN-FI EN-ES

P@1 Time P@1 Time P@1 Time P@1 Time

(Zhang et al., 2017a)† 0 46.6 0 46.0 0.07 44.9 0.07 43.0
(Conneau et al., 2018)† 45.40 46.1 47.27 45.4 1.62 44.4 36.20 45.3
(Artetxe et al., 2018)† 48.53 8.9 48.47 7.3 33.50 12.9 37.60 9.1

G-W 44.4 35.2 37.83 36.7 6.8 15.6 12.5 18.4
G-W + NORMALIZE 49.21 36 46.5 33.2 18.3 42.1 37.60 38.2

Table 2: Results of unsupervised methods on the dataset of Dinu et al. (2014) with runtimes in min-
utes. Those marked with † are from (Artetxe et al., 2018). Note that their runtimes correspond to GPU
computation, while ours are CPU-minutes, so the numbers are not directly comparable.

malization) against alternative approaches in Ta-
ble 2. Note that we report the runtimes of Artetxe
et al. (2018) as-is, which are obtained by running
on a Titan XP GPU, while our runtimes are, as be-
fore, obtained purely by CPU computation.

4.4 Qualitative Results
As mentioned earlier, Theorem 3.1 implies that the
optimal value of the Gromov-Wasserstein problem
can be legitimately interpreted as a distance be-
tween languages, or more explicitly, between their
word embedding spaces. This distributional no-
tion of distance is completely determined by pair-
wise geometric relations between these vectors. In
Figure 4 we show the values GW(Cs,Ct,p,q)
computed on the FASTTEXT word embeddings of
Conneau et al. (2018) corresponding to the most
frequent 2000 words in each language.

Overall, these distances conform to our intu-
itions: the cluster of romance languages exhibits
some of the shortest distances, while classical Chi-
nese (ZH) has the overall largest discrepancy with
all other languages. But somewhat surprisingly,
Russian is relatively close to the romance lan-
guages in this metric. We conjecture that this
could be due to Russian’s rich morphology (a trait
shared by romance languages but not English).
Furthermore, both Russian and Spanish are pro-
drop languages (Haspelmath, 2001) and share syn-
tactic phenomena, such as dative subjects (Moore
and Perlmutter, 2000; Melis et al., 2013) and dif-
ferential object marking (Bossong, 1991), which
might explain why ES is closest to RU overall.

On the other hand, English appears remarkably
isolated from all languages, equally distant from
its germanic (DE) and romance (FR) cousins. In-
deed, other aspects of the data (such as corpus
size) might be underlying these observations.

DE EN ES FR IT RU ZH

D
E

E
N

E
S

F
R

IT
R

U
Z

H

0 2.3 2.9 2.3 2.2 2.2 7.3

2.3 0 2.4 2.5 2.4 2.4 8.2

2.9 2.4 0 1.7 1.6 1.5 6.2

2.3 2.5 1.7 0 1.7 1.7 6.7

2.2 2.4 1.6 1.7 0 1.8 6.5

2.2 2.4 1.5 1.7 1.8 0 7.4

7.3 8.2 6.2 6.7 6.5 7.4 0

Gromov-Wasserstein Distances

0

2

4

6

8

10

Figure 4: Pairwise language Gromov-Wasserstein
distances obtained as the minimal transportation
cost (8) between word embedding similarity ma-
trices. Values scaled by 102 for easy visualization.

5 Related Work

Study of the problem of bilingual lexical induction
goes back to Rapp (1995) and Fung (1995). While
the literature on this topic is extensive, we focus
here on recent fully-unsupervised and minimally-
supervised approaches, and refer the reader to one
of various existing surveys for a broader panorama
(Upadhyay et al., 2016; Ruder et al., 2017).

Methods with coarse or limited parallel data.
Most of these fall in one of two categories: meth-
ods that learn a mapping from one space to
the other, e.g., as a least-squares objective (e.g.,
(Mikolov et al., 2013)) or via orthogonal transfor-
mations Zhang et al. (2016); Smith et al. (2017);
Artetxe et al. (2016), and methods that find a com-



1889

mon space on which to project both sets of embed-
dings (Faruqui and Dyer, 2014; Lu et al., 2015).

Fully Unsupervised methods. Conneau et al.
(2018) and Zhang et al. (2017a) rely on adversarial
training to produce an initial alignment between
the spaces. The former use pseudo-matches de-
rived from this initial alignment to solve a Pro-
crustes (2) alignment problem. Our Gromov-
Wasserstein framework can be thought of as pro-
viding an alternative to these adversarial training
steps, albeit with a concise optimization formula-
tion and producing explicit matches (via the op-
timal coupling) instead of depending on nearest
neighbor search, as the adversarially-learnt map-
pings do.

Zhang et al. (2017b) also leverage optimal
transport distances for the cross-lingual embed-
ding task. However, to address the issue of non-
alignment of embedding spaces, their approach
follows the joint optimization of the transportation
and procrustes problem as outlined in Section 2.2.
This formulation makes an explicit modeling as-
sumption (invariance to unitary transformations),
and requires repeated solution of Procrustes prob-
lems during alternating minimization. Gromov-
Wasserstein, on the other hand, is more flexible
and makes no such assumption, since it directly
deals with similarities rather than vectors. In the
case where it is required, such an orthogonal map-
ping can be obtained by solving a single procrustes
problem, as discussed in Section 3.2.

6 Discussion and future work

In this work we provided a direct optimization
approach to cross-lingual word alignment. The
Gromov-Wasserstein distance is well-suited for
this task as it performs a relational comparison of
word-vectors across languages rather than word-
vectors directly. The resulting objective is concise,
and can be optimized efficiently. The experimen-
tal results show that the resulting alignment frame-
work is fast, stable and robust, yielding near state-
of-the-art performance at a computational cost or-
ders of magnitude lower than that of alternative
fully unsupervised methods.

While directly solving Gromov-Wasserstein
problems of reasonable size is feasible, scaling up
to large vocabularies made it necessary to learn an
explicit mapping via Procrustes. GPU computa-
tions or stochastic optimization could help avoid
this secondary step.

Acknowledgments

The authors would like to thank the anonymous re-
viewers for helpful feedback. The work was par-
tially supported by MIT-IBM grant “Adversarial
learning of multimodal and structured data”, and
Graduate Fellowships from Hewlett Packard and
CONACYT.

References
Mikel Artetxe, Gorka Labaka, and Eneko Agirre. 2016.

Learning principled bilingual mappings of word em-
beddings while preserving monolingual invariance.
In Proceedings of the 2016 Conference on Empiri-
cal Methods in Natural Language Processing, pages
2289–2294.

Mikel Artetxe, Gorka Labaka, and Eneko Agirre. 2018.
A robust self-learning method for fully unsuper-
vised cross-lingual mappings of word embeddings.
In Proceedings of the 56th Annual Meeting of the
Association for Computational Linguistics (Volume
1: Long Papers), pages 789—-798. Association for
Computational Linguistics.

Piotr Bojanowski, Edouard Grave, Armand Joulin, and
Tomas Mikolov. 2017. Enriching Word Vectors with
Subword Information. Transactions of the Associa-
tion for Computational Linguistics, 5:135–146.

Georg Bossong. 1991. Differential object marking in
Romance and beyond. New analyses in Romance
linguistics, pages 143–170.

Alexis Conneau, Guillaume Lample, Marc’Aurelio
Ranzato, Ludovic Denoyer, and Hervé Jégou. 2018.
Word Translation Without Parallel Data. In Interna-
tional Conference on Learning Representations.

Marco Cuturi. 2013. Sinkhorn distances: Lightspeed
computation of optimal transport. In Advances
in Neural Information Processing Systems, pages
2292—-2300.

Georgiana Dinu, Angeliki Lazaridou, and Marco Ba-
roni. 2014. Improving zero-shot learning by
mitigating the hubness problem. arXiv preprint
arXiv:1412.6568.

Manaal Faruqui and Chris Dyer. 2014. Improving vec-
tor space word representations using multilingual
correlation. In Proceedings of the 14th Conference
of the European Chapter of the Association for Com-
putational Linguistics, pages 462–471.

Pascale Fung. 1995. Compiling bilingual lexicon en-
tries from a non-parallel English-Chinese corpus. In
Third Workshop on Very Large Corpora.

Jiang Guo, Wanxiang Che, David Yarowsky, Haifeng
Wang, and Ting Liu. 2015. Cross-lingual depen-
dency parsing based on distributed representations.



1890

In Proceedings of the 53rd Annual Meeting of the
Association for Computational Linguistics and the
7th International Joint Conference on Natural Lan-
guage Processing (Volume 1: Long Papers), vol-
ume 1, pages 1234–1244.

Tatsunori B Hashimoto, David Alvarez-Melis, and
Tommi S Jaakkola. 2016. Word Embeddings as
Metric Recovery in Semantic Spaces. Transactions
of the Association for Computational Linguistics,
4:273–286.

Martin Haspelmath. 2001. The European linguistic
area: Standard Average European. In Language ty-
pology and language universals: An international
handbook, volume 2, pages 1492–1510. de Gruyter.

Guillaume Lample, Ludovic Denoyer, and
Marc’Aurelio Ranzato. 2018. Unsupervised
Machine Translation Using Monolingual Corpora
Only. International Conference on Learning
Representations.

Ang Lu, Weiran Wang, Mohit Bansal, Kevin Gimpel,
and Karen Livescu. 2015. Deep multilingual cor-
relation for improved word embeddings. In Pro-
ceedings of the 2015 Conference of the North Amer-
ican Chapter of the Association for Computational
Linguistics: Human Language Technologies, pages
250–256.

Chantal Melis, Marcela Flores, and A Holvoet.
2013. On the historical expansion of non-
canonically marked ‘subjects’ in Spanish. The di-
achronic Typology of Non-Canonical Subjects, Am-
sterdam/Philadelphia, Benjamins, pages 163–184.

Facundo Mémoli. 2011. Gromov–Wasserstein dis-
tances and the metric approach to object match-
ing. Foundations of computational mathematics,
11(4):417–487.

Tomas Mikolov, Quoc V Le, and Ilya Sutskever.
2013. Exploiting Similarities among Lan-
guages for Machine Translation. arXiv preprint
arXiv:1309.4168v1, pages 1–10.

John Moore and David M. Perlmutter. 2000. What
does it take to be a dative subject? Natural Lan-
guage and Linguistic Theory, 18(2):373–416.

Gabriel Peyré and Marco Cuturi. 2018. Computational
Optimal Transport. Technical report.

Gabriel Peyré, Marco Cuturi, and Justin Solomon.
2016. Gromov-Wasserstein averaging of kernel and
distance matrices. In International Conference on
Machine Learning, pages 2664–2672.

Steven T Piantadosi. 2014. Zipf’s word frequency
law in natural language: A critical review and fu-
ture directions. Psychonomic Bulletin & Review,
21(5):1112–1130.

Anand Rangarajan, Haili Chui, and Fred L Book-
stein. 1997. The Softassign Procrustes Matching
Algorithm. Lecture Notes in Computer Science,
1230:29–42.

Reinhard Rapp. 1995. Identifying word translations in
non-parallel texts. In Proceedings of the 33rd an-
nual meeting on Association for Computational Lin-
guistics, pages 320–322. Association for Computa-
tional Linguistics.

Reinhard Rapp. 1999. Automatic identification of
word translations from unrelated English and Ger-
man corpora. In Proceedings of the 37th annual
meeting of the Association for Computational Lin-
guistics on Computational Linguistics, pages 519–
526. Association for Computational Linguistics.

Sebastian Ruder, Ivan Vulić, and Anders Søgaard.
2017. A survey of cross-lingual embedding models.
arXiv preprint arXiv:1706.04902.

Peter H. Schönemann. 1966. A generalized solution of
the orthogonal procrustes problem. Psychometrika,
31(1):1–10.

Samuel L Smith, David H P Turban, Steven Ham-
blin, and Nils Y Hammerla. 2017. Offline bilingual
word vectors, orthogonal transformations and the in-
verted softmax. International Conference on Learn-
ing Representations.

Shyam Upadhyay, Manaal Faruqui, Chris Dyer, and
Dan Roth. 2016. Cross-lingual models of word em-
beddings: An empirical comparison. arXiv preprint
arXiv:1604.00425.

Meng Zhang, Yang Liu, Huanbo Luan, and Maosong
Sun. 2017a. Adversarial training for unsupervised
bilingual lexicon induction. In Proceedings of the
55th Annual Meeting of the Association for Compu-
tational Linguistics (Volume 1: Long Papers), vol-
ume 1, pages 1959–1970.

Meng Zhang, Yang Liu, Huanbo Luan, and Maosong
Sun. 2017b. Earth Mover’s Distance Minimiza-
tion for Unsupervised Bilingual Lexicon Induction.
In Proceedings of the 2017 Conference on Empiri-
cal Methods in Natural Language Processing, pages
1934–1945.

Yuan Zhang, David Gaddy, Regina Barzilay, and
Tommi Jaakkola. 2016. Ten Pairs to Tag – Multilin-
gual POS Tagging via Coarse Mapping between Em-
beddings. In Proceedings of the 2016 Conference of
the North American Chapter of the Association for
Computational Linguistics: Human Language Tech-
nologies, pages 1307–1317, San Diego, California.
Association for Computational Linguistics.


