Proceedings of the 2nd Workshop on Predicting and Improving Text Readability for Target Reader Populations, pages 49–58,

Soﬁa, Bulgaria, August 4-9 2013. c(cid:13)2013 Association for Computational Linguistics

49

Modeling Comma Placement in Chinese Text for Better Readability using

Linguistic Features and Gaze Information

∗
Tadayoshi Hara1 Chen Chen2
1National Institute of Informatics, Japan

Yoshinobu Kano3,1 Akiko Aizawa1
2The University of Tokyo, Japan

3PRESTO, Japan Science and Technology Agency
{harasan, kano, aizawa}@nii.ac.jp

Abstract

Comma placements in Chinese text are
relatively arbitrary although there are
some syntactic guidelines for them. In this
research, we attempt to improve the read-
ability of text by optimizing comma place-
ments through integration of linguistic fea-
tures of text and gaze features of readers.
We design a comma predictor for gen-
eral Chinese text based on conditional ran-
dom ﬁeld models with linguistic features.
After that, we build a rule-based ﬁlter for
categorizing commas in text according to
their contribution to readability based on
the analysis of gazes of people reading text
with and without commas.

The experimental results show that our
predictor reproduces the comma distribu-
tion in the Penn Chinese Treebank with
78.41 in F1-score and commas chosen by
our ﬁlter smoothen certain gaze behaviors.

1 Introduction

Chinese is an ideographic language, with no natu-
ral apparent word boundaries, little morphology,
and no case markers. Moreover, most Chinese
sentences are quite long. These features make it
especially difﬁcult for Chinese learners to identify
composition of a word or a clause in a sentence.

Punctuation marks, especially commas, are al-
lowed to be placed relatively arbitrarily to serve as
important segmentation cues (Yue, 2006) for pro-
viding syntactic and prosodic boundaries in text;
commas indicate not only phrase or clause bound-
aries but also sentence segmentations, and they
capture some of the major aspects of a writer’s
prosodic intent (Chafe, 1988). The combination
of both aspects promotes cognition when reading
text (Ren and Yang, 2010; Walker et al., 2001).

∗

The Japan Research Institute, Ltd. (from April, 2013)

Figure 1: Our approach

However, although there are guidelines and re-
search on the syntactic aspects of comma place-
ment, prosodic aspects have not been explored,
since they are more related with cognition. It is
as yet unclear how comma placement should be
optimized for reading, and it has thus far been up
to the writer (Huang and Chen, 2011).

In this research, we attempt to optimize comma
placements by integrating the linguistic features of
text and the gaze features of readers. Figure 1 il-
lustrates our approach. First, we design a comma
predictor for general Chinese text based on con-
ditional random ﬁeld (CRF) models with various
linguistic features. Second, we build a rule-based
ﬁlter for classifying commas in text into ones fa-
cilitating or obstructing readability, by comparing
the gaze features of persons reading text with and
without commas. These two steps are connected
by applying our rule-based ﬁlter to commas pre-
dicted by our comma predictor. The experimental
results for each step validate our approach.

Related work is described in Section 2. The
functions of Chinese commas are described in
Section 3. Our CRF model-based comma predic-
tor is examined in Section 4, and our rule-based
comma ﬁlter is constructed and examined in Sec-
tion 5 and 6. Section 7 contains a summary and
outlines future directions of this research.

Input (Comma-less) Text

CRF model-based Comma Predictor
CRF model

Linguistic Features

+

Comma Distribution in General Text

Rule-based Comma Filter

Human Annotation

Gaze Features

+

Comma Distribution for Readability

Parse Tree

Treebank

Text with/without 

Commas

50

(,) means the original or comparative position of the comma in Chinese text.)

∗
[Case 1] When a pause between a subject and a predicate is needed. (
e.g. 我们看得见的星星，绝大多数是离地球非常远的恒星。(The stars we can see (,)
[Case 2] When a pause between an inner predicate and an object of a sentence is needed.
e.g. 应该看到，科学需要一个人贡献出毕生的精力。(We should see that (,) science needs a person to devote all his/her life to it.)
[Case 3] When a pause after an inner (adverbial, prepositional, etc.) modiﬁer of a sentence is needed.
e.g. 对于这个城市，他并不陌生。(He is no stranger (,) to this city.) (The order of the modiﬁer and the main clause is opposite in the English translation.)
[Case 4] When a pause between clauses in a complex sentence is needed, besides the use of semicolon (；).
e.g. 据说苏州园林有一百多处，我到过的不过十多处。(It is said that there are more than 100 Suzhou traditional gardens, (,) no more than 10 of which I
have been to.)
[Case 5] When a pause between phrases of the same syntactic type is needed.
e.g. 学生比较喜欢年轻，有活力的教师 (The students prefer young (,) and energetic teachers.)

are mostly ﬁxed stars that are far away from the earth.)

∗

Table 1: Five main usages of commas in Chinese text

(a) Screenshot of a material

(b) Scene of the experiment

(c) Window around a gaze point

Figure 3: Settings for eye-tracking experiments

WS
POS
DIP
STAG
OIC
WL
LOD

Word surface
POS tag
Depth of a word in the parse tree
Syntactic tag
Order of the clause in a sentence that a word belongs to
Word length
Length of fragment with speciﬁc depth in a parsing tree

Table 2: Features used in our CRF model

2 Related Work

Previous work on Chinese punctuation prediction
mostly focuses on sentence segmentation in au-
tomatic speech recognition (Shriberg et al., 2000;
Huang and Zweig, 2002; Peitz et al., 2011).

Jin et al. (2002) classiﬁed commas for sentence
segmentation and succeeded in improving pars-
ing performance. Lu and Ng (2010) proposed
an approach built on a dynamic CRF for predict-
ing punctuations, sentence boundaries, and sen-
tence types of speech utterances without prosodic
cues. Zhang et al. (2006) suggested that a cascade
CRF-based approach can deal with ancient Chi-
nese prose punctuation better than a single CRF.
Guo et al. (2010) implemented a three-tier max-
imum entropy model incorporating linguistically
motivated features for generating commonly used
Chinese punctuation marks in unpunctuated sen-
tences output by a surface realizer.

(a)

(b)

Figure 2: Example of a parse tree (a) and its cor-
responding training data (b) with the features

3 Functions of Chinese Commas

There are ﬁve main uses of commas in Chinese
text, as shown in Table 1. Cases 1 to 4 are from
ZDIC.NET (2005), and Case 5 obviously exists in
Chinese text. The ﬁrst three serve the function of
emphasis, while the latter two indicate coordinat-
ing or subordinating clauses or phrases.

In Cases 1 and 2, a comma is inserted as a
kind of pause between a short subject and a long
predicate, or between a short remainder predicate,
such as 看到 (see/know), 説明/表明 (indicate), 発

Host PC Monitor

Display PC Monitor

Subject

Eye Tracker

WS|POS|STAG|DIP|OIC|WL|LOD|IOB-tag

51

F1 (P/R)

Feature
WS

WL

DIP

OIC

POS

LOD

+WL

+DIP

+OIC

STAG

+LOD

+STAG

59.32 (72.67/50.12)
32.51 (69.06/21.26)
34.14 (68.65/22.72)
22.44 (64.00/13.60)
9.27 (66.56/ 4.98)
10.70 (75.24/ 5.76)
35.32 (59.20/25.17)
63.75 (79.93/53.01)
WS+POS
70.06 (83.27/60.47)
WS
57.42 (81.94/44.19)
WS
60.35 (77.98/49.22)
WS
60.90 (76.39/50.63)
WS
70.85 (78.87/64.31)
WS
73.41 (84.62/64.82)
WS+POS+DIP
74.58 (83.66/67.27)
WS+POS+DIP+STAG
76.87 (84.29/70.65)
WS+POS+DIP
70.18 (83.33/60.62)
WS+POS+DIP
76.61 (82.61/71.43)
WS+POS+DIP
76.62 (84.48/70.09)
WS+POS+DIP+STAG+OIC
74.12 (84.00/66.33)
WS+POS+DIP+STAG
77.64 (85.11/71.38)
WS+POS+DIP+STAG
75.43 (84.76/67.95)
WS+POS+DIP
78.23 (84.23/73.03)
WS+POS+DIP
74.01 (85.80/65.06)
WS+POS+DIP
77.25 (83.97/71.53)
WS+POS+DIP+STAG+OIC+WL
77.31 (86.36/69.97)
+LOD
WS+POS+DIP+STAG+OIC
76.55 (85.24/69.46)
+WL+LOD
WS+POS+DIP+STAG
77.60 (84.30/71.89)
WS+POS+DIP
+OIC+WL+LOD
WS+POS+DIP+STAG+OIC+WL+LOD
78.41 (83.97/73.54)
F1: F1-Score, P: precision (%), R: recall (%), A: accuracy (%)

+LOD
+WL+LOD

+OIC+WL
+OIC

+LOD

+LOD

+OIC

+WL

+WL

A

95.45
94.08
94.13
93.67
93.42
93.52
93.81
96.03
96.61
95.67
95.73
95.71
96.53
96.93
97.01
97.23
96.63
97.16
97.21
96.98
97.33
97.11
97.36
97.02
97.26
97.33
97.23
97.30
97.36

Table 3: Performance of the comma predictor

Article

ID
6
7
10
12
14
18
79
82
121
294
401
406
413
423
438

(A) #Characters,
(B) #Punctuations,
(C) #Commas

692
335
346
221
572
471
655
471
629
608
567
558
552
580
674

49
30
18
18
33
36
53
30
41
50
43
39
52
49
46

28
15
7
7
14
13
28
13
19
24
21
18
22
26
28

Average

528.73 39.13 18.87

(C) / (A)

(C) / (B)

Subjects

4.04%
4.48%
2.02%
3.17%
2.45%
2.76%
4.27%
2.76%
3.02%
3.95%
3.70%
3.23%
3.99%
4.48%
4.15%
3.57%

57.14%
50.00%
38.89%
38.89%
42.42%
36.11%
52.83%
43.33%
46.34%
48.00%
48.84%
46.15%
42.31%
53.06%
60.87%
48.22%

L, T, C
L, T, C
L, T, C, Z
L, T, C
L, T, C
C, Z

Z
Z
Z
Z

L, T, C

Z

T, C, Z
L, C, Z

Z
-

Table 4: Materials assigned to each subject

Figure 4: Obtained eye-movement trace map

Figure 5: Total viewing time

that Chinese sentence segmentation can be viewed
as detecting loosely coordinated clauses separated
by commas.

4 CRF Model-based Comma Predictor

We ﬁrst predict comma placements in existing
text. The prediction is formalized as a task to an-
notate each word in a word sequence with an IOB-
style tag such as I-Comma (following a comma),
B-Comma (preceding a comma) or O (neither I-
Comma nor B-Comma). We utilize a CRF model
for this sequential labeling (Lafferty et al., 2001).

4.1 CRF Model for Comma Prediction

A conditional probability assigned to a label se-
quence Y for a particular sequence of words X in
a ﬁrst-order linear-chain CRF is given by:

∑

∑

Pλ(Y |X) =

exp(

n
w

k

i λifi(Yw−1, Yw, X, w))

Z0(X)

見 (ﬁnd) etc., and following long clause-style ob-
jects. English commas, on the other hand, sel-
dom have such usages (Zeng, 2006). In Cases 3
and 4, commas instead of conjunctions sometimes
connect two clauses in a relation of either coordi-
nation or subordination. English commas, on the
other hand, are only required between independent
clauses connected by conjunctions (Zeng, 2006).
Liu et al. (2010) proved that Chinese commas
can change the syntactic structures of sentences
by playing lexical or syntactic roles. Ren and
Yang (2010) claimed that inserting commas as
clause boundaries shortens the ﬁxation time in
post-comma regions. Meanwhile, in computa-
tional linguistics, Xue and Yang (2011) showed

where w is a word position in X, fi is a binary
function describing a feature for Yw−1, Yw, X, and
w, λi is a weight for that feature, and Z0 is a nor-
malization factor over all possible label sequences.
The weight λi for each fi is learned on training
data. For fi, the linguistic features shown in Ta-
ble 2 are derived from a syntactic parse of a sen-
tence1. The ﬁrst three were used initially; the rest
were added after we got feedback from construc-
tion of our rule-based ﬁlters (see Section 5). Fig-
ure 2 shows an example of a parsing tree and its
corresponding training data.

1Some other features or tag formats which worked well in
the previous research, such as bi-/tri-gram, a preceding word
(L-1) or its POS (POS-1), and IO-style tag (Leaman and Gon-
zalez, 2008) were also examined, but they did not work that
well, probably because of the difference in task settings.

 

i

)
.

200,000
200
g
n
w
100,000
100
e
v

i

0

0

c
e
s
(
 
e
m

i
t

 
l

a
t
o
T

With commas
With Comma

Without commas
No Comma

1
L

2
L

4
L

3
L

L1 – L7

5
L

6
L

7
L

1
T

2
T

4
T

3
T

T1 – T7

5
T

6
T

8
T

1
C

2
C

4
C

3
C

C1 – C8

5
C

6
C

7
C

8
C

1
Z

2
Z

Trials (“Subject” + “Trial No.”)

3
Z

4
Z

5
Z

Z1 – Z10

6
Z

7
Z

8
Z

9
Z

0
1
Z

52

Figure 6: Fixation time per comma

Figure 8: Saccade length (1) per comma

Figure 7: Number of regressions per comma

Figure 9: Saccade length (2) per comma

4.2 Experimental Settings
The Penn Chinese Treebank (CTB) 7.0 (Nai-
wen Xue and Palmer, 2005) consists of 2,448
articles in ﬁve genres.
It contains 1,196,329
words, and all sentences are annotated with parse
trees. We selected four genres for written Chi-
nese (newswire, news magazine, broadcast news
and newsgroups/weblogs) from this corpus as our
dataset. These were randomly divided into train-
ing (90%) and test data (10%). We also corrected
errors in tagging and inconsistencies in the dataset,
mainly by solving problems around strange char-
acters tagged as PU (punctuation). The commas
and characters after this preprocessing numbered
63,571 and 1,533,928 in the training data and
4,116 and 111,172 in the test data.

MALLET (McCallum, 2002) and its applica-
tion ABNER (Settles, 2005) were used to train the
CRF model. We evaluated the results in terms
of precision (P = tp/(tp + f p)), recall (R =
tp/(tp+f n)), F1-score (F1 = 2PR/(P+R)), and
accuracy (A = (tp + tn)/(tp + tn + f p + f n)),
where tp, tn, f p and f n are respectively the num-
ber of true positives, true negatives, false positives
and false negatives, based on whether the model
and the corpus provided commas at each location.

4.3 Performance of the CRF Model
Table 3 shows the performance of our CRF
model2. We can see that WS contributed much
more to the performance than other features, prob-
ably because a word surface itself has a lot of
information on both prosodic and syntactic func-
tions. Combining WS with other features greatly
improved performance, and as a result, with all

2Precision, recall, F1-score, and accuracy with WS + POS
+ DIP + L-1 + POS-1 were 82.96%, 65.04%, 72.91 and
96.84%, respectively (lower than those with WS+POS+DIP).

features (WS + POS + STAG + DIP + OIC + LOD
+ WL), precision, recall, F1-score and accuracy
were 83.97%, 73.54%, 78.41 and 97.36%.

We also found that a large number of false pos-
itives seemed helpful according to native speakers
(see the description of the subjects in Section 5 and
6). Although these commas do not appear in the
CTB text, they might smoothen the reading expe-
rience. We constructed a rule-based ﬁlter in order
to pick out such commas.

5 Rule-based Comma Filter

We constructed a rule-based comma ﬁlter for clas-
sifying commas in text into ones facilitating (pos-
itive) or obstructing (negative) the reading process
as follows:
[Step 1]: Collect gaze data from persons reading
text with or without commas (Section 5.1).
[Step 2]: Compare gaze features around commas
to ﬁnd those features that reﬂect the effect of
comma placement. (Section 5.2).
[Step 3]: Annotate commas with categories based
on the obtained features (Section 5.3), and devise
rules to explain the annotation (Section 5.4).

5.1 Collecting Human Eye-movement Data
Eye-movements during reading contain rich infor-
mation on how the document is being read, what
the reader is interested in, where difﬁculties hap-
pen, etc. The movements are characterized by ﬁx-
ations (short periods of steadiness), saccades (fast
movements), and regressions (backward saccades)
(Rayner, 1998). In order to analyze the effect of
commas on reading through the features, we col-
lected gaze data from subjects reading text in the
following settings.
[Subjects and Materials] Four native Man-

/
 

e
m

i
t
 
n
o
i
t
a
x
F

i

)
.

2,000
2.0
c
e
s
1,000
1.0
(
 
a
m
0
0.0
m
o
c

1
L

2
L

4
L

3
L

L1 – L7

5
L

6
L

With commas Without commas
With Comma

No Comma

7
L

1
T

2
T

4
T

3
T

T1 – T7

5
T

6
T

8
T

1
C

2
C

4
C

3
C

C1 – C8

5
C

6
C

7
C

8
C

1
Z

2
Z

Trials (“Subject” + “Trial No.”)

3
Z

4
Z

5
Z

Z1 – Z10

6
Z

7
Z

8
Z

9
Z

0
1
Z

3.0
3
2.0
2
1.0
1
0.0
0

a
m
m
o
c

1
L

/
 

i

s
n
o
s
s
e
r
g
e
r
#

2
L

4
L

3
L

L1 – L7

5
L

6
L

With commas Without commas
With Comma

No Comma

7
L

1
T

2
T

4
T

3
T

T1 – T7

5
T

6
T

8
T

1
C

2
C

4
C

3
C

C1 – C8

5
C

6
C

7
C

8
C

1
Z

2
Z

Trials (“Subject” + “Trial No.”)

3
Z

4
Z

5
Z

Z1 – Z10

6
Z

7
Z

8
Z

9
Z

0
1
Z

a
m
m
o
c

160
160
120
120
80
80
40
40

 
/
 
)
1
(

h
t
g
n
e

l
 

e
d
a
c
c
a
S

(pixel)

With commas Without commas
With Comma

No Comma

1
L

2
L

4
L

3
L

L1 – L7

5
L

6
L

7
L

1
T

2
T

4
T

3
T

T1 – T7

5
T

6
T

8
T

1
C

2
C

4
C

3
C

C1 – C8

5
C

6
C

7
C

8
C

1
Z

2
Z

Trials (“Subject” + “Trial No.”)

3
Z

4
Z

5
Z

6
Z

Z1 – Z10

7
Z

8
Z

9
Z

0
1
Z

90
90

60
60

30
30

a
m
m
o
c

 
/
 
)
2
(

h
t
g
n
e

l
 

e
d
a
c
c
a
S

(pixel)

With commas Without commas
With Comma

No Comma

1
L

2
L

4
L

3
L

L1 – L7

5
L

6
L

7
L

1
T

2
T

4
T

3
T

T1 – T7

5
T

6
T

8
T

1
C

2
C

4
C

3
C

C1 – C8

5
C

6
C

7
C

8
C

1
Z

2
Z

Trials (“Subject” + “Trial No.”)

3
Z

4
Z

5
Z

Z1 – Z10

6
Z

7
Z

8
Z

9
Z

0
1
Z

53

Effect on readability
Can improve readability.

Categories
Positive (⃝)
Semi-positive (△) Might be necessary for readability, but the importance is not as obvious as a positive comma.
Semi-negative (2) Might be negative, but its severity is not as obvious as a negative comma.
Negative (×)
GF+/GF-: values of eye-tracking features that represent good/poor readability

Thought to reduce a document’s readability.

Outward manifestation
Presence would cause GF+.
Absence might cause GF-.
Absence might cause GF+.
Presence would cause GF-.

Table 5: Comma categories

Subject

Positive (⃝)
>800
∆FT
>900
∆FT
>600
∆FT
∆FT
>650

Adjustment formula
′
′
′
′
∆FT = [ ﬁxation time (without commas) [ms]] − [ ﬁxation time (with commas) [ms]]
∆RT = [ #regressions (without commas) ] − [ #regressions (with commas) ]

Semi-negative (2)
′≤500
-100<∆FT
′≤600
-200<∆FT
′≤300
-300<∆FT
′≤350
-250<∆FT

<-100 ∆FT
<-200 ∆FT
<-300 ∆FT
<-250 ∆FT

= ∆FT ＋ ∆RT × 200
= ∆FT ＋ ∆RT × 275
= ∆FT ＋ ∆RT × 250
= ∆FT ＋ ∆RT × 250

Semi-positive (△)
′≤800
500<∆FT
′≤900
600<∆FT
′≤600
300<∆FT
′≤650
350<∆FT

Negative (×)
∆FT
∆FT
∆FT
∆FT

L
C
T
Z

′
′
′
′

′
′
′
′

Table 6: Estimation formula for judging the contribution of commas to readability

ID ⃝ △ 2 ×
5
6
0
7
10
1
0
12
1
14
3
18
4
79
82
0

13
8
5
1
4
5
11
5

4
1
1
2
5
4
9
2

6
6
0
4
4
1
4
6

ID
121
294
401
406
413
423
438
Total

⃝ △ 2
6
11
4
9
10
2
5
5
6
8
7
11
6
6
112
64

2
9
7
6
5
4
16
80

×
0
1
2
2
3
4
0
26

Table 7: Categories of annotated commas

darin Chinese speakers (graduate students and re-
searchers) read 15 newswire articles selected from
CTB 7.0 (included in the test data in Section 4.2).
Table 4 and Figure 3(a) show the materials as-
signed to each subject and a screenshot of one ma-
terial. Each article was presented in 12-15 points
of bold-faced Fang-Song font occupying 13×13,
14×15, 15×16 or 16×16 pixels along with a line
spacing of 5-10 pixels3.
[Apparatus] Figure 3(b) shows a scene of the
experiment. An EyeLink 1000 eye tracker (SR
Research Ltd., Toronto, Canada) with a desktop
mount monitored the movements of a right eye at
1,000 Hz. The subject’s head was supported at the
chin and forehead. The distance between the eyes
and the monitor was around 55 cm, and each Chi-
◦
nese character subtended a visual angle 1
. Text
was presented on a 19” monitor at a resolution
of 800×600 pixels, with the brightness adjusted
to a comfortable level. The displayed article was
masked except for the area around a gaze point
(see Figure 3(c)) in order to conﬁrm that the gaze
point was correctly detected and make the subject
concentrate on the area (adjusted for him/her).
[Procedure] Each article was presented twice
(once with/once without commas) to each subject.

3These values, as well as the screen position of the article,

were adjusted for each subject.

The one without commas was presented ﬁrst4 (not
necessarily in a row). We did not give any compre-
hension test after reading; we just asked the sub-
jects to read carefully and silently at their normal
or lower speed, in order to minimize the effect of
the ﬁrst reading on the second. The subjects were
informed of the presence or absence of commas
beforehand. The apparatus was calibrated before
the experiment and between trials. The experi-
ment lasted around two hours for each subject.
[Alignment of eye-tracking data to text] Figure 4
shows an example of the obtained eye-movement
trace map, where circles and lines respectively
mean ﬁxation points and saccades, and color depth
shows their duration. The alignment of the data to
the text is a critical task, and although automatic
approaches have been proposed (Mart´ınez-G´omez
et al., 2012a; Mart´ınez-G´omez et al., 2012b), they
do not seem robust enough for our purpose. Ac-
cordingly, we here just compared the entire layout
of the gaze point distribution and that of the actual
text, and adjusted them to have relatively coherent
positions on the x-axis; i.e., the beginning and end
of the gaze point sequence in a line were made as
close as possible to those of the line in the text.

5.2 Analysis of Eye-movement Data
The gaze data were analyzed by focusing on re-
gions around each comma or where each one
should be (three characters left and right to the
comma5).

4If we had used the reversed order, the subject would have
knowledge about original comma distribution, and this would
cause abnormally quick reading of the text without commas.
With the order we set, conﬂicts between false segmentations
(made in ﬁrst reading) and correct ones might bother the sub-
ject, which is trade-off (though minor) in the second reading.
5When a comma appeared at the beginning of a line, two
characters to the left and right of the comma and one charac-

54

1. If L Seg and R Seg are both very long, a comma must be put between them.
2. If two △ appear serially, one is necessary whereas the other might be optional or judged negative, but it still depends on the lengths of the siblings.
3. If two neighboring commas appear very close to each other, one of them is judged as negative whereas judgment on the other one is reserved.
4. If several (more than 2) ×s appear continually, one or more ×s might be reserved in consideration of the global condition.
5. A comma is always needed after a long sentence or clause without any syntactically signiﬁcant punctuation with the function of segmentation.
6. If a △ appears near a ⃝, it might be judged as negative with a high probability. However, the judgment process is always from the bottom up, which
means × → 2 → △ → ⃝. For example, if a 2 appears near a △, we judge 2 ﬁrst (to be positive or negative), then judge the △ in the condition
with or without the comma of 2.

Table 8: General rules for reference

Figure 5, 6 and 7 respectively show the total
viewing time, ﬁxation time (duration for all ﬁx-
ations and saccades in a target region) per comma,
and number of regressions per comma6 for each
trial. We can see a general trend wherein the for-
mer two were shorter and the latter was smaller for
the articles with commas than without. The diver-
sity of the subjects was also observed in Figure 6.
Figure 8 and 9 show the saccade length per
comma for different measures. The former (lat-
ter) ﬁgure considers a saccade in which at least
one edge (both edges) was in the region. We can-
not see any global trend, probably because of the
difference in global layout of materials brought by
the presence or absence of commas.

5.3 Categorization of Commas
Using the features shown to be effective to repre-
sent the effect of comma placement, we analyzed
the statistics for each comma in order to manu-
ally construct an estimation formula for judging
the contribution of each comma to readability. The
contribution was classiﬁed into four categories
(Table 5), and the formula is described in Table 67.
The adjustment formula was based on our obser-
vation that the number of regressions could only
be regarded as an aid. For example, for subject
=−350,
C, if ∆FT=200ms and ∆RT =−2, ∆FT
′
and therefore, the comma is annotated as negative.
All parameters were decided empirically and man-
ually checked twice (self-judgment and feedback
from the subjects).

On the basis of this estimation formula, all arti-
cles in Table 4 were manually annotated. Table 7
shows the distribution of the assigned categories8.
ter to the left and right of the ﬁnal character of the last line
were analyzed.

6Calculated by counting the instances where the x-
position of [a ﬁxation / end point of a saccade ] was ahead
of [the former ﬁxation / its start point]. Although the counts
of these two types were almost the same, by counting both of
them, we expected to cover any possible regression.

7One or two features are used to judge the category of a

comma. We will explore more features in the future.

8In the case of severe contradictions, the annotators dis-

cussed them and resolved them by voting.

5.4 Implementation of Rule-based Filter
The annotated commas were classiﬁed into Cases
1 to 5 in Table 1, based on the types of left and
right segment conjuncts (L Seg and R Seg, which
were obtained from the parse trees in CTB). For
each of the ﬁve cases, the reason for the assign-
ment of a category (⃝, △, 2 or ×) to each
comma was explained by a manually constructed
rule which utilized information about L Seg and
R Seg. The rules were constructed so that they
would cover as many instances as possible. Ta-
ble 8 shows the general rules utilized as a refer-
ence, and Table 9 shows the ﬁnally obtained rules.
The rightmost column in this table shows the num-
ber of commas matching each rule. These rules
were then implemented as a ﬁlter for classifying
commas in a given text.
For several rules (⃝10, 28, 210, 211 and
212), there were only single instances. In addi-
tion, although our rules were built carefully, a few
exceptions to the detailed threshold were found.
Collecting and investigating more gaze data would
help to make our rules more sophisticated.

6 Performance of the Rule-based Filter

We assumed that our comma predictor provides a
CTB text with the same distribution as the origi-
nal one in CTB (see Figure 1). Accordingly, we
examined the quality of the comma categorization
by our rule-based ﬁlter through gaze experiments.

6.1 Experimental Settings
Another ﬁve native Mandarin Chinese speakers
were invited as test subjects. The CTB articles as-
signed to the subjects are listed in Table 10. These
articles were selected from the test data in Sec-
tion 4.2 in such a way that 520<#characters<700,
#commas/#punctuations>38%,
#commas>17,
and
since we
needed articles of appropriate length with a fair
number of commas. After that, we manually
chose articles that seemed to attract the subjects’
interest from those that satisﬁed the conditions.

#commas/#characters>3.1%,

55

6
4
9

2
7
2

#commas

#commas

#commas

13
4
1
6
9
4
2
5
8
12
6
1
1
8
1

L IP-SBJ + R VP (length both<14 (In Seg Len))
L IP-SBJ/NP-SBJ (Org Len>13, Ttl Len>15)
L NP-SBJ/IP-SBJ (<14) + R VP (≥25)
Long frontings (Modiﬁer/Subject, >7) + short L predicate (VV/VRD/VSB· · · , ≤3) + Longer R object (IP-OBJ, >28)
Short frontings (<5) + short L predicate (<3) + moderate-length R object (IP-SBJ, <20)
Short frontings (<6) + short L predicate (≤3) + long R object (IP-SBJ, >23)
Short frequently used L modiﬁer (2-3, 经…, 据…, etc.) + moderate-length/long R SPO (≥w18p10)
Short L (PP/LCP)-TMP (5, 6) + long R NP (≥10)
Long L CP-CND (e.g., 若…, >18) + moderate-length R Seg (SPO, IP, etc. <18)
Long L modiﬁer (PP(-XXX, P+Long NP/IP), IP-ADV, ≥17)
Moderate-length/short L modiﬁer (PP(-XXX, P+IP, There is IP inside, >6<15, cf. 26 (NP))
Long L (PP/LCP)-TMP (Ttl Len≥10), short R Seg (NP/ADVP, <3)
Short L (LCP/PP)-LOC (<8)
Long L LOC (or there is LCP inside PP, >10)
Very short frequently used L ADVP/ADV (2)
Short L (PP/LCP/NP)-TMP (4;5-6, when R Seg is short (<10))
Moderate-length PP(-XXX, P+NP, >8 ≤13) + R Seg (SPO, IP, VO, MSPO, etc.)
Short L IP-CND (<8)
Long L PP-DIR (>20) + short R VO (≤10)
Very short L (QP/NP/LCP)-TMP (≤3)
Short frequently used L modiﬁer (as in ⃝3, ≤3) + short/moderate-length R Seg (SPO etc., <c20w9)
L c & R c are both long (In Seg Len≥15; or one>13, the other near 20)
L c is the summary of R c
Moderate-length L c + R c (both ≥10≤15; or one≥17, the other≤12)
Moderate-length clause (>10), but connected with familiar CC or ADVP
Three or more consecutive moderate-length clauses (all<15, and at least one ≤10)
Very short L c + R c (both <5), something like slogan)

Case 1: L Subject + R Predicate
⃝6
△7
×6
Case 2: L Predicate + R Object
⃝9
△8
26
Case 3: L Modiﬁer
⃝3
⃝7
⃝10
△1
△4
△9
△10
22
23
25
24
28
211
×2
×5
Case 4: L c + R c
⃝2
⃝8
△2
△3
△5
×7
Case 5: L p + R p
⃝1
⃝4
⃝5
⃝11
(△3
△6
21
27
29
210
212
×1
· L x/R x: the left/right segment of a target comma which is x.
(x can be “p” (phrase) / “c” (clause), syntactic tags (with function tags) such as “VP” and “IP-SBJ”, or general functions such as “Subject” and “Predicate”.)
· Org Len: the number of characters in a segment (including other commas or punctuation inside).
· In Seg Len/Ttl Len: the number of characters between the comma and nearest punctuation (inside a long/outside a short target segment).
· SPO: subject + predicate + object, belonging to the outermost sentence. The length is deﬁned in the similar way as In Seg Len.
· MSPO: modiﬁer + subject + predicate + object. The length is deﬁned in the similar way as In Seg Len.
· -XX or -XXX: arbitrary type of possible functional tag (or without any functional tag) connected with the former syntactic tag.
· ≤ciwj: #characters≤i and #words≤j.
· In some cases (in Case 3, 4 and 5), the length is calculated after negative (or judged negative) commas are eliminated.
· The rules related with TMP are applied faster than ones related with LCP (in Case 3).
· △3 appears in both Case 4 (clause) and Case 5 (phrase). The number of commas is given by the sum of those in both cases.

Short coordinate modiﬁers (Both side <5)
Short L p+R p (both<c15w5, and at least one <10), but pre-L p (e.g., SBJ) is too long (>18)
Between two moderate-length/long phrases (both ≥15; or L p≥17, R p=10-14; Or L p=10-14, R p>20)
Long pre-L p (SBJ /ADV, etc. >16) + short L p (≤5) + long R p (≥18)
Moderate-length phrase (>10), but connected with familiar CC or ADVP)
Three or more consecutive short/moderate-length phrases (both<15, at least one<8)
Between short phrases (both ≤c13w5), and pre-L p (SBJ/ADV, etc.) is short/moderate-length (<11)
Coordinate VPs, and L VP is a moderate-length VP (PP-MNR VP)
Phrasal coordination between a long (≥18) and a short (<10) phrase
Moderate-length coordinate VPs (>10<15), and R VP has the structure like VP (MSP VP)
Between two short/moderate-length NP phrases (both ≤15, e.g., L NP-TPC+R NP-SBJ)
Moderate-length/short phrase ((i) c:one>10<18, The other >5≤10, w:one≤5, the other>5≤10; (ii) c:both≥10<15,
w:both>5≤7), and pre-L p (SBJ/ADV, etc.) is short (≤5)

4
2
39
2
(6)
5
13
4
3
1
1
13

39
2
25
6
12
1

#commas

#commas

Table 9: Entire classiﬁcation of rules based on traditional comma categories

Article

ID
6
11
15
16
56
73
79
99

(A) #Characters,
(B) #Punctuations,
(C) #Commas

692
672
674
547
524
595
655
671

49
48
67
43
43
46
53
55

28
21
26
22
18
28
28
24

Average

628.75 50.50 24.38

(C) / (A)

(C) / (B)

Subjects

4.04%
3.13%
3.86%
4.02%
3.44%
4.71%
4.27%
3.58%
3.88%

57.14%
43.75%
38.81%
51.16%
41.86%
60.87%
52.83%
43.64%
48.27%

L, S, H
L, S, F
L, S, H
L, S, F
L, H, M
S, H, F, M
H, F, M

F, M

-

Table 10: Materials assigned to each subject

Our rule-based ﬁlter was applied to the commas
of each article9, and the commas were classiﬁed

Figure 10: Total viewing time for two distributions

into two distributions: a positive one (positive +
semi-positive commas) and a negative one (nega-
tive + semi-negative commas). Two types of ma-
terials were thus generated by leaving the commas
in one distribution and removing the others.

9Instances of incoherence among the applied rules were

manually checked and corrected.

120
120,000
80
)
80,000
.
c
e
40
40,000
s
(
 
0
0
e
m

i
t

i

g
n
w
e
v

i

 
l

a
t
o
T

Positive distribution Negative distribution
Distribution(G)

Distribution(B)

9
7
F

1
1
F

6
1
F

3
7
F

9
9
F

9
7
H

5
1
H

6
0
H

3
3
7
7
H
M
Trials (“Subject” + “Article ID”)

9
9
M

9
7
M

6
5
H

6
0
L

1
1
L

5
1
L

6
1
L

6
5
L

6
5
M

6
0
S

1
1
S

5
1
S

6
1
S

3
7
S

56

Figure 11: EMF F T for two distributions

Figure 13: EMRT for two distributions

Figure 12: EMF T for two distributions

Figure 14: EMSLO for two distributions

The apparatus and procedure were almost the
same as those in Section 5.1, whereas, on the ba-
sis of the feedback from the previous experiments,
the font size, number of characters in a line, and
line spacing were ﬁxed to single optimized values,
respectively, 14-point Fang-Song font occupying
15×16 pixels, 33 characters and 7 pixels.

6.2 Evaluation Metrics

We examined whether our positive/negative distri-
butions really facilitated/obstructed the subjects’
reading process by using the following metrics:
CN·TT

10, EMF T = FT

11,

TT, EMF F T = FFT
FT
EMRT = RT
2·CN

12, EMSLO = SLO
2·TT,

where TT, FT, RT and CN are total viewing time,
ﬁxation time, number of regressions, and num-
ber of commas respectively, as described in Sec-
tion 5.2. FFT and SLO are additionally introduced
metrics respectively for the “total duration for all
ﬁrst-pass ﬁxations in a target region that exclude
any regressions” and for the “length of saccades
from inside a target region to the outside”13. All of
the areas around commas appearing in the original
article were considered target areas for the metrics.
The other settings were the same as in Section 5.

6.3 Contribution of Categorized Commas

Figure 10, 11, 12, 13 and 14 respectively show TT,
EMF F T , EMF T , EMRT and EMSLO for two types
of comma distributions in each trial.

10Ratio to the total ﬁxation time in the target areas (FT).
11Normalized by the total viewing time (TT).
12Two types of RT count (see Section 5.2) were averaged.
13Respectively to reﬂect “the early-stage processing of the
region” and “the information processed for a ﬁxation and a
decision of the next ﬁxation point” (Hirotani et al., 2006).

For TT, we cannot see any general trend, mainly
because this time, the reading order of the text
was random, which spread out the second reading
effect evenly between the two distributions. For
EMF F T , we cannot reach a conclusion either. In
contrast, in more than half of the trials, EMF F T
was larger for positive distributions, which would
imply that the positive commas helped to prevent
the reader’s gaze from revisiting the target regions.
For most trials, except for subject S whose cal-
ibration was poor and reading process was poor
in M56, EMF T and EMRT decreased and EMSLO
increased for positive distributions, which implies
that the positive commas smoothed the reading
process around the target regions.

7 Conclusion

We proposed an approach for modeling comma
placement in Chinese text for smoothing reading.
In our approach, commas are added to the text on
the basis of a CRF model-based comma predic-
tor trained on the treebank, and a rule-based ﬁlter
then classiﬁes the commas into ones facilitating or
obstructing reading. The experimental results on
each part of this approach were encouraging.

In our future work, we would like see how com-
mas affect reading by using much more material,
and thereby reﬁne our framework in order to bring
a better reading experience to readers.

Acknowledgments

This research was partially supported by Kakenhi,
MEXT Japan [23650076] and JST PRESTO.

80
60
40
20

)
0
0
1

(

T
F
F

M
E

Positive distribution Negative distribution
Distribution(G)

Distribution(B)

9
7
F

1
1
F

6
1
F

3
7
F

9
9
F

3
7
H

6
0
L

6
5
H

9
7
H

5
1
H

6
6
5
0
H
M
Trials (“Subject” + “Article ID”)

9
9
M

9
7
M

3
7
M

1
1
L

5
1
L

6
1
L

6
5
L

6
0
S

1
1
S

5
1
S

6
1
S

3
7
S

10
8
6
4

 
)
0
0
8

(

T
F

M
E

Positive distribution Negative distribution
Distribution(G)

Distribution(B)

9
7
F

1
1
F

6
1
F

3
7
F

9
9
F

3
7
H

6
5
H

9
7
H

5
1
H

6
3
6
0
7
5
H
M
M
Trials (“Subject” + “Article ID”)

9
9
M

9
7
M

6
0
L

1
1
L

5
1
L

6
1
L

6
5
L

6
0
S

1
1
S

5
1
S

6
1
S

3
7
S

Positive distribution Negative distribution
Distribution(G)

Distribution(B)

10
5
0

)
0
1

(

T
R
M
E

9
7
F

1
1
F

6
1
F

3
7
F

9
9
F

3
7
H

6
5
H

9
7
H

5
1
H

6
3
6
0
7
5
H
M
M
Trials (“Subject” + “Article ID”)

9
9
M

9
7
M

6
0
L

1
1
L

5
1
L

6
1
L

6
5
L

6
0
S

1
1
S

5
1
S

6
1
S

3
7
S

10

5

0

)
0
0
1

(
O
L
S
M
E

Positive distribution Negative distribution
Distribution(G)

Distribution(B)

9
7
F

1
1
F

6
1
F

3
7
F

9
9
F

9
7
H

5
1
H

6
3
6
5
7
0
H
H
M
Trials (“Subject” + “Article ID”)

9
9
M

9
7
M

3
7
M

6
5
H

6
0
L

1
1
L

5
1
L

6
1
L

6
5
L

6
0
S

1
1
S

5
1
S

6
1
S

3
7
S

57

References
Wallace Chafe. 1988. Punctuation and the prosody of
written language. Written Communication, 5:396–
426.

Yuqing Guo, Haifeng Wang, and Josef van Genabith.
2010. A linguistically inspired statistical model
for Chinese punctuation generation. ACM Trans-
actions on Asian Language Information Processing,
9(2):6:1–6:27, June.

Masako Hirotani, Lyn Frazier, and Keith Rayner. 2006.
Punctuation and intonation effects on clause and
sentence wrap-up: Evidence from eye movements.
Journal of Memory and Language, 54(3):425–443.

Hen-Hsen Huang and Hsin-Hsi Chen. 2011. Pause and
stop labeling for Chinese sentence boundary detec-
tion. In Proceedings of Recent Advances in Natural
Language Processing, pages 146–153.

Jing Huang and Geoffrey Zweig. 2002. Maximum en-
tropy model for punctuation annotation from speech.
In Proceedings of the International Conference on
Spoken Language Processing, pages 917–920.

Mei xun Jin, Mi-Young Kim, Dongil Kim, and Jong-
Segmentation of Chinese
Hyeok Lee.
In Proceedings of
long sentences using commas.
the Third SIGHAN Workshop on Chinese Language
Processing, pages 1–8.

2002.

John D. Lafferty, Andrew McCallum, and Fernando
C. N. Pereira. 2001. Conditional random ﬁelds:
Probabilistic models for segmenting and labeling se-
quence data. In Proceedings of the Eighteenth Inter-
national Conference on Machine Learning, ICML
’01, pages 282–289, San Francisco, CA, USA. Mor-
gan Kaufmann Publishers Inc.

Robert Leaman and Graciela Gonzalez. 2008. BAN-
NER: An executable survery of advances in biomed-
ical named entity recognition. In Paciﬁc Symposium
on Biocomputing (PSB’08), pages 652–663.

Baolin Liu, Zhongning Wang, and Zhixing Jin. 2010.
The effects of punctuations in Chinese sentence
comprehension: An erp study. Journal of Neurolin-
guistics, 23(1):66–68.

Wei Lu and Hwee Tou Ng. 2010. Better punctuation
prediction with dynamic conditional random ﬁelds.
In Proceedings of the 2010 Conference on Empirical
Methods in Natural Language Processing (EMNLP
’10), pages 177–186.

Pascual Mart´ınez-G´omez, Chen Chen, Tadayoshi Hara,
Yoshinobu Kano, and Akiko Aizawa. 2012a. Image
registration for text-gaze alignment. In Proceedings
of the 2012 ACM international conference on Intel-
ligent User Interfaces (IUI ’12), pages 257–260.

Pascual Mart´ınez-G´omez, Tadayoshi Hara, Chen
Chen, Kyohei Tomita, Yoshinobu Kano, and Akiko

Aizawa. 2012b. Synthesizing image representa-
tions of linguistic and topological features for pre-
dicting areas of attention. In Patricia Anthony, Mit-
suru Ishizuka, and Dickson Lukose, editors, PRICAI
2012: Trends in Artiﬁcial Intelligence, pages 312–
323. Springer.

Andrew Kachites McCallum. 2002. MALLET: A ma-

chine learning for language toolkit.

Fu-dong Chiou Naiwen Xue, Fei Xia and Marta
Palmer. 2005. The Penn Chinese TreeBank: Phrase
structure annotation of a large corpus. Natural Lan-
guage Engineering, 11(2):207–238.

Stephan Peitz, Markus Freitag, Arne Mauser, and Her-
mann Ney. 2011. Modeling punctuation prediction
as machine translation. In Proceedings of Interna-
tional Workshop on Spoken Language Translation,
pages 238–245.

Keith Rayner. 1998. Eye movements in reading and
information processing: 20 years of research. Psy-
chological Bulletin, 124(3):372–422.

Gui-Qin Ren and Yufang Yang.

Syntac-
tic boundaries and comma placement during silent
reading of Chinese text: evidence from eye move-
ments. Journal of Research in Reading, 33(2):168–
177.

2010.

Burr Settles. 2005. ABNER: an open source tool
for automatically tagging genes, proteins, and other
entity names in text. Bioinformatics, 21(14):3191–
3192.

Elizabeth Shriberg, Andreas Stolcke, Dilek Hakkani-
T¨ur, and G¨okhan T¨ur. 2000. Prosody-based au-
tomatic segmentation of speech into sentences and
topics. Speech Communication, 32(1-2):127–154.

Judy Perkins Walker, Kirk Fongemie, and Tracy
Daigle. 2001. Prosodic facilitation in the resolu-
tion of syntactic ambiguities in subjects with left
and right hemisphere damage. Brain and Language,
78(2):169–196.

Nianwen Xue and Yaqin Yang. 2011. Chinese sen-
tence segmentation as comma classiﬁcation. In Pro-
ceedings of the 49th Annual Meeting of the Asso-
ciation for Computational Linguistics:shortpapers,
pages 631–635.

Ming Yue. 2006. Discursive usage of six Chinese
punctuation marks.
In Proceedings of the COL-
ING/ACL 2006 Student Research Workshop, pages
43–48.

ZDIC.NET. 2005. Commonly used Chinese punctua-
tion usage short list. Long Wiki, Retrieved Dec 10,
2012,
from http://www.zdic.net/appendix/f3.htm.
(in Chinese).

X. Y. Zeng.

2006. The comparison and the use
of English and Chinese comma. College English,
3(2):62–65. (in Chinese).

58

Kaixu Zhang, Yunqing Xia, and Hang Yu.

2006.
CRF-based approach to sentence segmentation and
punctuation for ancient Chinese prose.
Jour-
nal of Tsinghua Univ (Science and Technology),
49(10):1733–1736. (in Chinese).

