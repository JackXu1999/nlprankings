



















































Misleading Failures of Partial-input Baselines


Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 5533–5538
Florence, Italy, July 28 - August 2, 2019. c©2019 Association for Computational Linguistics

5533

Misleading Failures of Partial-input Baselines

Shi Feng
Computer Science

University of Maryland
shifeng@umiacs.umd.edu

Eric Wallace
Allen Institute for

Artificial Intelligence
ericw@allenai.org

Jordan Boyd-Graber
Computer Science, iSchool,

UMIACS, and LSC
University of Maryland

jbg@umiacs.umd.edu

Abstract

Recent work establishes dataset difficulty and
removes annotation artifacts via partial-input
baselines (e.g., hypothesis-only models for
SNLI or question-only models for VQA). When
a partial-input baseline gets high accuracy, a
dataset is cheatable. However, the converse
is not necessarily true: the failure of a partial-
input baseline does not mean a dataset is free
of artifacts. To illustrate this, we first design ar-
tificial datasets which contain trivial patterns
in the full input that are undetectable by any
partial-input model. Next, we identify such ar-
tifacts in the SNLI dataset—a hypothesis-only
model augmented with trivial patterns in the
premise can solve 15% of the examples that
are previously considered “hard”. Our work
provides a caveat for the use of partial-input
baselines for dataset verification and creation.

1 Dataset Artifacts Hurt Generalizability

Dataset quality is crucial for the development and
evaluation of machine learning models. Large-
scale natural language processing (NLP) datasets
often use human annotations on web-crawled data,
which can introduce artifacts. For example, crowd-
workers might use specific words to contradict a
given premise (Gururangan et al., 2018). These ar-
tifacts corrupt the intention of the datasets to train
and evaluate models for natural language under-
standing. Importantly, a human inspection of indi-
vidual examples cannot catch artifacts because they
are only visible in aggregate on the dataset level.
However, machine learning algorithms, which de-
tect and exploit recurring patterns in large datasets
by design, can just as easily use artifacts as real lin-
guistic clues. As a result, models trained on these
datasets can achieve high test accuracy by exploit-
ing artifacts but fail to generalize, e.g., they fail
under adversarial evaluation (Jia and Liang, 2017;
Ribeiro et al., 2018).

The identification of dataset artifacts has
changed model evaluation and dataset construc-
tion (Chen et al., 2016; Jia and Liang, 2017; Goyal
et al., 2017). One key method is to use partial-
input baselines, i.e., models that intentionally ig-
nore portions of the input. Example use cases in-
clude hypothesis-only models for natural language
inference (Gururangan et al., 2018), question-only
models for visual question answering (Goyal et al.,
2017), and paragraph-only models for reading com-
prehension (Kaushik and Lipton, 2018). A success-
ful partial-input baseline indicates that a dataset
contains artifacts which make it easier than ex-
pected. On the other hand, examples where this
baseline fails are “hard” (Gururangan et al., 2018),
and the failure of partial-input baselines is consid-
ered a verdict of a dataset’s difficulty (Zellers et al.,
2018; Kaushik and Lipton, 2018).

These partial-input analyses are valuable and in-
deed reveal dataset issues; however, they do not tell
the whole story. Just as being free of one ailment is
not the same as a clean bill of health, a baseline’s
failure only indicates that a dataset is not broken in
one specific way. There is no reason that artifacts
only infect part of the input—models can exploit
patterns that are only visible in the full input.

After reviewing partial-input baselines (Sec-
tion 2), we construct variants of a natural language
inference dataset to highlight the potential pitfalls
of partial-input dataset validation (Section 3). Sec-
tion 4 shows that real datasets have artifacts that
evade partial-input baselines; we use a hypothesis-
plus-one-word model to solve 15% of the “hard”
examples from SNLI (Bowman et al., 2015; Guru-
rangan et al., 2018) where hypothesis-only models
fail. Furthermore, we highlight some of the arti-
facts learned by this model using k-nearest neigh-
bors in representation space. Section 5 discusses
how partial-input baselines should be used in future
dataset creation and analysis.



5534

2 What are Partial-input Baselines?

A long-term goal of NLP is to solve tasks that we
believe require a human-level understanding of lan-
guage. The NLP community typically defines tasks
with datasets: reproduce these answers given these
inputs, and you have solved the underlying task.
This task-dataset equivalence is only valid when
the dataset accurately represents the task. Unfor-
tunately, verifying this equivalence via humans is
fundamentally insufficient: humans reason about
examples one by one, while models can discover
recurring patterns. Patterns that are not part of the
underlying task, or artifacts of the data collection
process, can lead to models that “cheat”—ones that
achieve high test accuracy using patterns that do
not generalize.

One frequent type of artifact, especially in classi-
fication datasets where each input contains multiple
parts (e.g., a question and an image), is a strong
correlation between a part of the input and the label.
For example, a model can answer many VQA ques-
tions without looking at the image (Goyal et al.,
2017). These artifacts can be detected using partial-
input baselines: models that are restricted to using
only part of the input. Validating a dataset with a
partial-input baseline has the following steps:

1. Decide which part of the input to use.
2. Reduce all examples in the training set and

the test set.
3. Train a new model from scratch on the partial-

input training set.
4. Test the model on the partial-input test set.

High accuracy from a partial-input model im-
plies the original dataset is solvable (to some ex-
tent) in the wrong ways, i.e., using unintended pat-
terns. Partial-input baselines have identified ar-
tifacts in many datasets, e.g., SNLI (Gururangan
et al., 2018; Poliak et al., 2018), VQA (Goyal et al.,
2017), EmbodiedQA (Anand et al., 2018), visual
dialogue (Massiceti et al., 2018), and visual navi-
gation (Thomason et al., 2019).

3 How Partial-input Baselines Fail

If a partial-input baseline fails, e.g., it gets close to
chance accuracy, one might conclude that a dataset
is difficult. For example, partial-input baselines are
used to identify the “hard” examples in SNLI (Gu-
rurangan et al., 2018), verify that SQuAD is well
constructed (Kaushik and Lipton, 2018), and that
SWAG is challenging (Zellers et al., 2018).

Reasonable as it might seem, this kind of argu-
ment can be misleading—it is important to under-
stand what exactly these results do and do not imply.
A low accuracy from a partial-input baseline only
means that the model failed to confirm a specific
exploitable pattern in the part of the input that the
model can see. This does not mean, however, that
the dataset is free of artifacts—the full input might
still contain very trivial patterns.

To illustrate how the failures of partial-input
baselines might shadow more trivial patterns that
are only visible in the full input, we construct two
variants of the SNLI dataset (Bowman et al., 2015).
The datasets are constructed to contain trivial pat-
terns that partial-input baselines cannot exploit, i.e.,
the patterns are only visible in the full input. As
a result, a full-input can achieve perfect accuracy
whereas partial-input models fail.

3.1 Label as Premise
In SNLI, each example consists of a pair of sen-
tences: a premise and a hypothesis. The goal
is to classify the semantic relationship between
the premise and the hypothesis—either entailment,
neutral, or contradiction.

Our first SNLI variant is an extreme example of
artifacts that cannot be detected by a hypothesis-
only baseline. Each SNLI example (training and
testing) is copied three times, and the copies are
assigned the labels Entailment, Neutral, and Con-
tradiction, respectively. We then set each example’s
premise to be the literal word of the associated la-
bel: “Entailment”, “Neutral”, or “Contradiction”
(Table 1). From the perspective of a hypothesis-
only model, the three copies have identical inputs
but conflicting labels. Thus, the best accuracy from
any hypothesis-only model is chance—the model
fails due to high Bayes error. However, a full-input
model can see the label in the premise and achieve
perfect accuracy.

This serves as an extreme example of a dataset
that passes a partial-input baseline test but still con-
tains artifacts. Obviously, a premise-only baseline
can detect these artifacts; we address this in the
next dataset variant.

3.2 Label Hidden in Premise and Hypothesis
The artifact we introduce in the previous dataset
can be easily detected by a premise-only baseline.
In this variant, we “encrypt” the label such that it is
only visible if we combine the premise and the hy-
pothesis, i.e., neither premise-only nor hypothesis-



5535

Old Premise Animals are running
New Premise Entailment
Hypothesis Animals are outdoors

Label Entailment

Table 1: Each example in this dataset has the ground-
truth label set as the premise. Every hypothesis occurs
three times in the dataset, each time with a unique la-
bel and premise combination (not shown in this table).
Therefore, a hypothesis-only baseline will only achieve
chance accuracy, but a full-input model can trivially
solve the dataset.

Label Combinations

Entailment A+B C+D E+F
Contradiction A+F C+B E+D
Neutral A+D C+F E+B

Table 2: We “encrypt” the labels to mimic an artifact
that requires both parts of the input. Each capital let-
ter is a code word, and each label is derived from the
combination of two code words. Each combination
uniquely identifies a label, e.g., A in the premise and
B in the hypothesis equals Entailment. However, a sin-
gle code word cannot identify the label.

only baselines can detect the artifact. Each label
is represented by the concatenation of two “code
words”, and this mapping is one-to-many: each la-
bel has three combinations of code words, and each
combination uniquely identifies a label. Table 2
shows our code word configuration. The design
of the code words ensures that a single code word
cannot uniquely identify a label—you need both.

We put one code word in the premise and the
other in the hypothesis. These encrypted labels
mimic an artifact that requires both parts of the
input. Table 3 shows an SNLI example modified
accordingly. A full-input model can exploit the
artifact and trivially achieve perfect accuracy, but a
partial-input model cannot.

A more extreme version of this modified dataset
has exactly the nine combinations in Table 2 as both
the training set and the test set. Since a single code
word cannot identify the label, neither hypothesis-
only nor premise-only baselines can achieve more
than chance accuracy. However, a full-input model
can perfectly extract the label by combining the
premise and the hypothesis.

Premise A Animals are running
Hypothesis B Animals are outdoors

Label Entailment

Table 3: Each example in this dataset has a code word
added to both the premise and the hypothesis. Follow-
ing the configuration of Table 2, A in the premise com-
bined with B in the hypothesis indicates the label is En-
tailment. A full-input model can easily exploit this arti-
fact but partial-input models cannot.

4 Artifacts Evade Partial-input Baselines

Our synthetic dataset variants contain trivial arti-
facts that partial-input baselines fail to detect. Do
real datasets such as SNLI have artifacts that are not
detected by partial-input baselines?

We investigate this by providing additional in-
formation about the premise to a hypothesis-only
model. In particular, we provide the last noun of the
premise, i.e., we form a hypothesis-plus-one-word
model. Since this additional information appears
useless to humans (examples below), it is an artifact
rather than a generalizable pattern.

We use a BERT-based (Devlin et al., 2019) clas-
sifier that gets 88.28% accuracy with the regular,
full input. The hypothesis-only version reaches
70.10% accuracy.1 With the hypothesis-plus-one-
word model, the accuracy improves to 74.6%, i.e.,
the model solves 15% of the “hard” examples that
are unsolvable by the hypothesis-only model.2

Table 4 shows examples that are only solvable
with the one additional word from the premise. For
both the hypothesis-only and hypothesis-plus-one-
word models, we follow Papernot and McDaniel
(2018) and Wallace et al. (2018) and retrieve train-
ing examples using nearest neighbor search in the
final BERT representation space. In the first ex-
ample, humans would not consider the hypothesis
“The young boy is crying” as a contradiction to the
premise “camera”. In this case, the hypothesis-only
model incorrectly predicts Entailment, however,
the hypothesis-plus-one-word model correctly pre-
dicts Contradiction. This pattern—including one
premise word—is an artifact that regular partial-
input baselines cannot detect but can be exploited
by a full-input model.

1Gururangan et al. (2018) report 67.0% using a simpler
hypothesis-only model.

2We create the easy-hard split of the dataset using our
model, not using the model from Gururangan et al. (2018).



5536

Label Premise Hypothesis
Contradiction A young boy hanging on a pole smiling at the camera. The young boy is crying.
Contradiction A boy smiles tentatively at the camera. a boy is crying.
Contradiction A happy child smiles at the camera. The child is crying at the playground.
Contradiction A girl shows a small child her camera. A boy crying.
Entailment A little boy with a baseball on his shirt is crying. A boy is crying.
Entailment Young boy crying in a stroller. A boy is crying.
Entailment A baby boy in overalls is crying. A boy is crying.

Entailment Little boy playing with his toy train. A boy is playing with toys.
Entailment A little boy is looking at a toy train. A boy is looking at a toy.
Entailment Little redheaded boy looking at a toy train. A little boy is watching a toy train.
Entailment A young girl in goggles riding on a toy train. A girl rides a toy train.
Contradiction A little girl is playing with tinker toys. A little boy is playing with toys.
Contradiction A toddler shovels a snowy driveway with a shovel. A young child is playing with toys.
Contradiction A boy playing with toys in a bedroom. A boy is playing with toys at the park.

Table 4: We create a hypothesis-plus-one-word model that sees the hypothesis alongside the last noun in the
premise. We show two SNLI test examples (highlighted) that are answered correctly using this model but are an-
swered incorrectly using a hypothesis-only model. For each test example, we also show the training examples that
are nearest neighbors in BERT’s representation space. When using the hypothesis and the last noun in the premise
(underlined), training examples with the correct label are retrieved; when using only the hypothesis, examples with
the incorrect label are retrieved.

5 Discussion and Related Work

Partial-input baselines are valuable sanity checks
for datasets, but as we illustrate, their implications
should be understood carefully. This section dis-
cusses methods for validating and creating datasets
in light of possible artifacts from the annotation
process, as well as empirical results that corrobo-
rate the potential pitfalls highlighted in this paper.
Furthermore, we discuss alternative approaches for
developing robust NLP models.

Hypothesis Testing Validating datasets with
partial-input baselines is a form of hypothesis-
testing: one hypothesizes trivial solutions to the
dataset (i.e., a spurious correlation between labels
and a part of the input) and verifies if these hypothe-
ses are true. While it is tempting to hypothesize
other ways a model can cheat, it is infeasible to enu-
merate over all of them. In other words, if we could
write down all the necessary tests for test-driven
development (Beck, 2002) of a machine learning
model, we would already have a rule-based system
that can solve our task.

Adversarial Annotation Rather than using
partial-input baselines as post-hoc tests, a natural
idea is to incorporate them into the data genera-
tion process to reject bad examples. For example,
the SWAG (Zellers et al., 2018) dataset consists of
multiple-choice answers that are selected adver-
sarially against an ensemble of partial-input and
heuristic classifiers. However, since these classi-

fiers can be easily fooled if they rely on superficial
patterns, the resulting dataset may still contain arti-
facts. In particular, a much stronger model (BERT)
that sees the full-input easily solves the dataset.
This demonstrates that using partial-input baselines
as adversaries may lead to datasets that are just dif-
ficult enough to fool the baselines but not difficult
enough to ensure that no model can cheat.

Adversarial Evaluation Instead of validating a
dataset, one can alternatively probe the model di-
rectly. For example, models can be stress tested
using adversarial examples (Jia and Liang, 2017;
Wallace et al., 2019) and challenge sets (Glock-
ner et al., 2018; Naik et al., 2018). These tests
can reveal strikingly simple model limitations, e.g.,
basic paraphrases can fool textual entailment and
visual question answering systems (Iyyer et al.,
2018; Ribeiro et al., 2018), while common typos
drastically degrade neural machine translation qual-
ity (Belinkov and Bisk, 2018).

Interpretations Another technique for probing
models is to use interpretation methods. Inter-
pretations, however, have a problem of faithful-
ness (Rudin, 2018): they approximate (often lo-
cally) a complex model with a simpler, inter-
pretable model (often a linear model). Since in-
terpretations are inherently an approximation, they
can never be completely faithful—there are cases
where the original model and the simple model
behave differently (Ghorbani et al., 2019). These



5537

cases might also be especially important as they
usually reflect the counter-intuitive brittleness of
the complex models (e.g., in adversarial examples).

Certifiable Robustness Finally, an alternative
approach for creating models that are free of ar-
tifacts is to alter the training process. In particular,
model robustness research in computer vision has
begun to transition from an empirical arms race be-
tween attackers and defenders to more theoretically
sound robustness methods. For instance, convex re-
laxations can train models that are provably robust
to adversarial examples (Raghunathan et al., 2018;
Wong and Kolter, 2018). Despite these method’s
impressive (and rapidly developing) results, they
largely focus on adversarial perturbations bounded
to an L∞ ball. This is due to the difficulties in
formalizing attacks and defenses for more complex
threat models, of which the discrete nature of NLP
is included. Future work can look to generalize
these methods to other classes of model vulnerabil-
ities and artifacts.

6 Conclusion

Partial-input baselines are valuable sanity checks
for dataset difficulty, but their implications should
be analyzed carefully. We illustrate in both syn-
thetic and real datasets how partial-input baselines
can overshadow trivial, exploitable patterns that
are only visible in the full input. Our work pro-
vides an alternative view on the use of partial-input
baselines in future dataset creation.

Acknowledgments

This work was supported by NSF Grant IIS-
1822494. Boyd-Graber and Feng are also sup-
ported by DARPA award HR0011-15-C-0113 un-
der subcontract to Raytheon BBN Technologies.
Any opinions, findings, conclusions, or recommen-
dations expressed here are those of the authors and
do not necessarily reflect the view of the sponsor.

References
Ankesh Anand, Eugene Belilovsky, Kyle Kastner,

Hugo Larochelle, and Aaron Courville. 2018. Blind-
fold baselines for embodied QA. In NeurIPS
Visually-Grounded Interaction and Language Work-
shop.

Kent Beck. 2002. Test-Driven Development by Exam-
ple. Addison-Wesley.

Yonatan Belinkov and Yonatan Bisk. 2018. Synthetic
and natural noise both break neural machine transla-
tion. In Proceedings of the International Conference
on Learning Representations.

Samuel R. Bowman, Gabor Angeli, Christopher Potts,
and Christopher D. Manning. 2015. A large an-
notated corpus for learning natural language infer-
ence. In Proceedings of Empirical Methods in Natu-
ral Language Processing.

Danqi Chen, Jason Bolton, and Christopher D. Man-
ning. 2016. A thorough examination of the
CNN/Daily Mail reading comprehension task. In
Proceedings of the Association for Computational
Linguistics.

Jacob Devlin, Ming-Wei Chang, Kenton Lee, and
Kristina Toutanova. 2019. BERT: Pre-training of
deep bidirectional transformers for language under-
standing. In Conference of the North American
Chapter of the Association for Computational Lin-
guistics.

Amirata Ghorbani, Abubakar Abid, and James Y. Zou.
2019. Interpretation of neural networks is fragile. In
Association for the Advancement of Artificial Intelli-
gence.

Max Glockner, Vered Shwartz, and Yoav Goldberg.
2018. Breaking NLI systems with sentences that re-
quire simple lexical inferences. In Proceedings of
the Association for Computational Linguistics.

Yash Goyal, Tejas Khot, Douglas Summers-Stay,
Dhruv Batra, and Devi Parikh. 2017. Making the
V in VQA matter: Elevating the role of image under-
standing in visual question answering. In Computer
Vision and Pattern Recognition.

Suchin Gururangan, Swabha Swayamdipta, Omer
Levy, Roy Schwartz, Samuel R. Bowman, and
Noah A. Smith. 2018. Annotation artifacts in nat-
ural language inference data. In Conference of the
North American Chapter of the Association for Com-
putational Linguistics.

Mohit Iyyer, John Wieting, Kevin Gimpel, and Luke S.
Zettlemoyer. 2018. Adversarial example generation
with syntactically controlled paraphrase networks.
In Conference of the North American Chapter of the
Association for Computational Linguistics.

Robin Jia and Percy Liang. 2017. Adversarial exam-
ples for evaluating reading comprehension systems.



5538

In Proceedings of Empirical Methods in Natural
Language Processing.

Divyansh Kaushik and Zachary C. Lipton. 2018. How
much reading does reading comprehension require?
a critical investigation of popular benchmarks. In
Proceedings of Empirical Methods in Natural Lan-
guage Processing.

Daniela Massiceti, Puneet K. Dokania, N. Siddharth,
and Philip H.S. Torr. 2018. Visual dialogue without
vision or dialogue. In NeurIPS Workshop on Cri-
tiquing and Correcting Trends in Machine Learning.

Aakanksha Naik, Abhilasha Ravichander, Norman
Sadeh, Carolyn Rose, and Graham Neubig. 2018.
Stress test evaluation for natural language inference.
In Proceedings of International Conference on Com-
putational Linguistics.

Nicolas Papernot and Patrick D. McDaniel. 2018.
Deep k-nearest neighbors: Towards confident, inter-
pretable and robust deep learning. arXiv preprint
arXiv: 1803.04765.

Adam Poliak, Jason Naradowsky, Aparajita Haldar,
Rachel Rudinger, and Benjamin Van Durme. 2018.
Hypothesis only baselines in natural language infer-
ence. In 7th Joint Conference on Lexical and Com-
putational Semantics (*SEM).

Aditi Raghunathan, Jacob Steinhardt, and Percy Liang.
2018. Certified defenses against adversarial exam-
ples. In Proceedings of the International Confer-
ence on Learning Representations.

Marco Tulio Ribeiro, Sameer Singh, and Carlos
Guestrin. 2018. Semantically equivalent adversarial
rules for debugging NLP models. In Proceedings of
the Association for Computational Linguistics.

Cynthia Rudin. 2018. Please stop explaining black box
models for high stakes decisions. In NeurIPS Work-
shop on Critiquing and Correcting Trends in Ma-
chine Learning.

Jesse Thomason, Daniel Gordan, and Yonatan Bisk.
2019. Shifting the baseline: Single modality perfor-
mance on visual navigation & QA. In Conference of
the North American Chapter of the Association for
Computational Linguistics.

Eric Wallace, Shi Feng, and Jordan Boyd-Graber. 2018.
Interpreting neural networks with nearest neighbors.
In EMNLP Workshop BlackboxNLP: Analyzing and
Interpreting Neural Networks for NLP.

Eric Wallace, Pedro Rodriguez, Shi Feng, Ikuya Ya-
mada, and Jordan Boyd-Graber. 2019. Trick me if
you can: Human-in-the-loop generation of adversar-
ial examples for question answering. In Transac-
tions of the Association for Computational Linguis-
tics.

Eric Wong and J. Zico Kolter. 2018. Provable defenses
against adversarial examples via the convex outer ad-
versarial polytope. In Proceedings of the Interna-
tional Conference of Machine Learning.

Rowan Zellers, Yonatan Bisk, Roy Schwartz, and
Yejin Choi. 2018. SWAG: A large-scale adversar-
ial dataset for grounded commonsense inference. In
Proceedings of Empirical Methods in Natural Lan-
guage Processing.


