



















































Towards Surface Realization with CCGs Induced from Dependencies


Proceedings of the 8th International Natural Language Generation Conference, pages 147–151,
Philadelphia, Pennsylvania, 19-21 June 2014. c©2014 Association for Computational Linguistics

Towards Surface Realization with CCGs Induced from Dependencies

Michael White
Department of Linguistics
The Ohio State University

Columbus, OH 43210, USA
mwhite@ling.osu.edu

Abstract

We present a novel algorithm for inducing
Combinatory Categorial Grammars from
dependency treebanks, along with initial
experiments showing that it can be used
to achieve competitive realization results
using an enhanced version of the surface
realization shared task data.

1 Introduction

In the first surface realization shared task (Belz
et al., 2011), no grammar-based systems achieved
competitive results, as input conversion turned
out to be more difficult than anticipated. Since
then, Narayan & Gardent (2012) have shown that
grammar-based systems can be substantially im-
proved with error mining techniques. In this pa-
per, inspired by recent work on converting depen-
dency treebanks (Ambati et al., 2013) and seman-
tic parsing (Kwiatkowksi et al., 2010; Artzi and
Zettlemoyer, 2013) with Combinatory Categorial
Grammar (CCG), we pursue the alternative strat-
egy of inducing a CCG from an enhanced version
of the shared task dependencies, with initial exper-
iments showing even better results.

A silver lining of the failure of grammar-based
systems in the shared task is that it revealed some
problems with the data. In particular, it became
evident that in cases where a constituent is an-
notated with multiple roles in the Penn Treebank
(PTB), the partial nature of Propbank annotation
and the restriction to syntactic dependency trees
meant that information was lost between the sur-
face and deep representations, leading grammar-
based systems to fail for good reason. For ex-
ample, Figure 1 shows that with free object rel-
atives, only one of the two roles played by how
much manufacturing strength is captured in the
deep representation, making it difficult to linearize
this phrase correctly. By contrast, Figure 2 (top)

!"#$%&'(#)*'+,-./' 0#12%'+,-./'

3%-%,&%,45'6$78'!"#
/7'$"%'89))9,:'6$78'
)"#$%&'/#)*'9,-./;''
<=,&'6$78'&'''/7'$"%'
9,',#12%'9,-./>?'

Figure 1: Shared Task Input for Economists are
divided as to [how much manufacturing strength]i
they expect to see ti in September reports on indus-
trial production and capacity utilization , also due
tomorrow (wsj 2400.6, “deep” representation)

shows an experimental version of the shallow rep-
resentation intended to capture all the syntactic de-
pendencies in the PTB, including the additional
object role played by this phrase here.1 Includ-
ing all PTB syntactic dependencies in the shallow
representation makes it feasible to define a com-
patible CCG; at the bottom of the figure, a cor-
responding CCG derivation for these dependen-
cies is shown. In the next section, we present an
algorithm for inducing such derivations. In con-
trast to Ambati et al.’s (2013) approach, the algo-
rithm integrates the proposal of candidate lexical
categories with the derivational process, making it
possible to derive categories involving unsaturated
arguments, such as se,dcl\npx/(se′,to\npx ); it also
makes greater use of unary type-changing rules,
as with Artzi & Zettlemoyer’s (2013) approach.

1Kudos to Richard Johansson for making these enhance-
ments available.

147



Unlike their approach though, it works in a broad
coverage setting, and makes use of all the combi-
nators standardly used with CCG, including ones
for type-raising.

2 Inducing CCGs from Dependencies

Pseudocode for the induction algorithm is given
in Figure 3. The algorithm takes as input a set
of training sentences with their gold standard de-
pendencies. We pre-processed the dependencies
to make coordinating conjunctions the head, and
to include features for zero-determiners. The algo-
rithm also makes use of a seed lexicon that spec-
ifies category projection by part of speech as well
as a handful of categories for function words. For
example, (1) shows how a tensed verb projects to
a finite clause category, while (2) shows the usual
CCG category for a determiner, which here intro-
duces a 〈NMOD〉 dependency.2

(1) expect ` se,dcl : @e(expect ∧ 〈TENSE〉pres)
(2) the ` npx/nx : @x(〈NMOD〉(d ∧ the))

The algorithm begins by instantiating the lexical
categories and type-changing rules that match the
input dependency graph, tracking the categories
in a map (edges) from nodes to edges (i.e., signs
with a coverage vector). It then recursively vis-
its each node in the primary dependency tree bot-
tom up (combineEdges), using a local chart (do-
Combos) at each step to combine categories for
adjacent phrases in all possible ways. Along the
way, it creates new categories (extendCats and co-
ordCats) and unary rules (applyNewUnary). For
example, when processing the node for expect in
Figure 2, the nodes for they and to are recursively
processed first, deriving the categories npw9 and
sw11 ,to\npw9 /npw8 for they and to see . . . , respec-
tively. The initial category for expect is then ex-
tended as shown in (3), which allows for com-
position with to see . . . (as well as with a cate-
gory for simple application). When there are co-
ordination relations for a coordinating conjunction
(or coordinating punctuation mark), the appropri-
ate category for combining like types is instead
constructed, as in (4). Additionally, for modifiers,
unary rules are instantiated and applied, e.g. the
rule for noun-noun compounds in (5).

2In the experiments reported here, we made use of only
six (non-trivial) hand-specified categories and two type-
changing rules; though we anticipate adding more initial cat-
egories to handle some currently problematic cases, the vast
majority of the categories in the resulting grammar can be
induced automatically.

Inputs Training set of sentences with dependencies. Initial
lexicon and rules. Argument and modifier relations. Deriva-
tion scoring metric. Maximum agenda size.

Definitions edges is a map from dependency graph nodes
to their edges, where an edge is a CCG sign together with a
coverage bitset; agenda is a priority queue of edges sorted
by the scoring metric; chart manages equivalence classes of
edges; see text for descriptions of auxiliary functions such as
extendCats and coordCats below.

Algorithm
bestDerivs, lexcats, unaryRules← ∅
For each item in training set:

1. edges[node] ← instCats(node), ruleInsts[node] ← in-
stRules(node), for node in input graph

2. combineEdges(root), with root of input graph

3. bestEdge ← unpack (edges[root]); bestDerivs +←
bestEdge.sign; lexcats +← abstractedCats(bestEdge),
unaryRules +← abstractedRules(bestEdge), if best-
Edge complete

def combineEdges(node):

1. combineEdges(child) for child in node.kids

2. edges[node] +← coordCats(node) if node has co-
ord relations, otherwise edges[node] ← extend-
Cats(node,rels) for argument rels

3. agenda ← edges[node]; agenda +← edges[child] for
child in node.kids; chart← ∅

4. While agenda not empty:

(a) next← agenda.pop
(b) chart +← next
(c) doCombos(next), unless next packed into an ex-

isting chart item

5. edges[node]← chart edges for node filtered for maxi-
mal input coverage

def doCombos(next):

1. agenda +← applyUnary(next), if next is for node

2. For item in chart:

(a) agenda +← applyBinary(next,item), if next is ad-
jacent to item

(b) agenda +← applyNewUnary(next,item), if next
connected to item by a modifier relation

Outputs bestDerivs, lexcats, unaryRules

Figure 3: CCG Induction Algorithm

148



economists are divided as to how much manufacturing strength they expect to see in . . . .

root

subj vc

obj

adv
pmod

pmod
nmod

amod
nmod sub

obj

subj

sbj

oprd im loc pmod

p

. . . September reports on industrial production and capacity utilization , also due tomorrow

pmod

nmod
nmod

pmod

nmod
coord1

coord2

nmod

p

appo

amod

tmp

. . . to how much manuf. strength they expect to see . . .

pp/np wh adj sng n np sdcl\np/(sto\np) sto\np/(sb\np) sb\np/np
>T >B

wh\wh n/n s/(s\np) sto\np/np
< > >B

wh n sdcl\np/np
>B

np/(s/np)/n sdcl/np
>

np/(s/np)
>np
>pp

. . . Sept. reports on industrial production and capacity utilization . . .

np n pp/np adj n np\⇤np/⇤np n n
n/n n/n n/n

> > >n n n
np np

>
np\⇤np

<np
>pp

n\n
<n

np

Figure 1: exampleFigure 2: Augmented Syntactic Dependencies with Corresponding CCG Derivation (dashed dependen-
cies indicate relations from additional parents beyond those in the primary tree structure)

149



(3) expect ` sw10 ,dcl\npw9 /(sw11 ,to\npw9 ) :
@w10(expect ∧ 〈TENSE〉pres ∧

〈SUBJ〉w9 ∧ 〈OPRED〉w11)

(4) and ` npw19 \∗npw18 /∗npw21 :
@w19(and ∧ 〈COORD1〉w18 ∧ 〈COORD2〉w21)

(5) nw20 ⇒ nw21 /nw21 : @w21(〈NMOD〉w20)

At the end of the recursion, the lexical cate-
gories and type-changing rules are extracted from
the highest-scoring derivation and added to the
output sets, after first replacing indices such as w10
with variables.

3 Experiments and Future Work

We ran the induction algorithm over the stan-
dard PTB training sections (02–21), recovering
complete derivations more than 90% of the time
for most sections. Robust treatment of coordina-
tion, including argument cluster coordination and
gapping, remains a known issue; other causes of
derivation failures remain to be investigated. To
select preferred derivations, we used a complex-
ity metric that simply counts the number of steps
and the number of slashes in the categories. We
then trained a generative syntactic model (Hock-
enmaier and Steedman, 2002) and used it along
with a composite language model to generate n-
best realizations for reranking (White and Rajku-
mar, 2012), additionally using a large-scale (giga-
word) language model. Development and test re-
sults appear in Table 1. Perhaps because of the
expanded use of type-changing rules with sim-
ple lexical categories, the generative model and
hypertagger (Espinosa et al., 2008) performed
worse than expected. Combining the generative
syntactic model and composite language model
(GEN) with equal weight yielded a devtest BLEU
score of only 0.4513, while discriminatively train-
ing the generative component models (GLOBAL)
increased the score to 0.7679. Using all fea-
tures increased the score to 0.8083, while dou-
bling the beam size (ALL+) pushed the score to
0.8210, indicating that search errors may be an
issue. Ablation results show that leaving out
the large-scale language model (NO-BIGLM) and
dependency-ordering features (NO-DEPORD) sub-
stantially drops the score.3 Focusing only on
the 80.5% of the sentences for which a complete
derivation was found (COMPLETE) yielded a score
of 0.8668. By comparison, realization with the

3All differences were statistically significant at p < 0.01
with paired bootstrap resampling (Koehn, 2004).

Model Exact Complete BLEU
Sect 00
GEN 2.4 79.5 0.4513
GLOBAL 29.7 79.0 0.7679
NO-BIGLM 29.1 78.2 0.7757
NO-DEPORD 34.3 77.9 0.7956
ALL 35.8 78.4 0.8083
ALL+ 36.4 80.5 0.8210
COMPLETE 44.4 - 0.8668
NATIVE 48.0 88.7 0.8793

Sect 23
GEN 2.8 80.3 0.4560
GLOBAL 31.3 78.5 0.7675
ALL 37.6 77.2 0.8083
ALL+ 38.1 80.4 0.8260
COMPLETE 47.0 - 0.8743
NATIVE 46.4 86.4 0.8694

Table 1: Development set (Section 00) & test set
(Section 23) results, including exact match and
complete derivation percentages and BLEU scores

native OpenCCG inputs (and the large-scale LM)
on all sentences (NATIVE) yields a score more
than five BLEU points higher, despite using in-
puts with more semantically-oriented relations and
leaving out many function words, indicating that
there is likely substantial room for improvement
in the pre-processing and grammar induction pro-
cess. Towards that end, we tried selecting the best
derivations using several rounds of Viterbi EM
with the generative syntactic model, but doing so
did not improve realization quality.

A similar pattern is seen in the Section 23 re-
sults, with a competitive BLEU score of 0.8260
with the expanded beam, much higher than
Narayan & Gardent’s (2012) score of 0.675 with
38.8% coverage, the best previous score with a
grammar-based system. This score still trails the
shared task scores of the top statistical dependency
realizers by several points (STUMABA-S at 0.8911
and DCU at 0.8575), though it exceeds the score of
a purpose-built system using no external resources
(ATT at 0.6701). In future work, we hope to close
the gap with the top systems by integrating an im-
proved ranking model into the induction process
and resolving the remaining representational is-
sues with problematic constructions.

Acknowledgments

Thanks to the anonymous reviewers, Richard Johansson and

the University of Sydney Schwa Lab for helpful comments

and discussion. This work was supported in part by NSF

grants IIS-1143635 and IIS-1319318.

150



References
Bharat Ram Ambati, Tejaswini Deoskar, and Mark

Steedman. 2013. Using CCG categories to improve
Hindi dependency parsing. In Proc. ACL.

Yoav Artzi and Luke Zettlemoyer. 2013. Weakly su-
pervised learning of semantic parsers for mapping
instructions to actions. TACL, 1:49–62.

Anja Belz, Michael White, Dominic Espinosa, Eric
Kow, Deirdre Hogan, and Amanda Stent. 2011. The
first surface realisation shared task: Overview and
evaluation results. In Proc. ENLG.

Dominic Espinosa, Michael White, and Dennis Mehay.
2008. Hypertagging: Supertagging for surface real-
ization with CCG. In Proc. ACL.

Julia Hockenmaier and Mark Steedman. 2002. Gen-
erative models for statistical parsing with Combina-
tory Categorial Grammar. In Proc. ACL.

Philipp Koehn. 2004. Statistical significance tests for
machine translation evaluation. In Proc. EMNLP.

Tom Kwiatkowksi, Luke Zettlemoyer, Sharon Goldwa-
ter, and Mark Steedman. 2010. Inducing probabilis-
tic CCG grammars from logical form with higher-
order unification. In Proc. EMNLP.

Shashi Narayan and Claire Gardent. 2012. Error min-
ing with suspicion trees: Seeing the forest for the
trees. In Proc. COLING.

Michael White and Rajakrishnan Rajkumar. 2012.
Minimal dependency length in realization ranking.
In Proc. EMNLP.

151


