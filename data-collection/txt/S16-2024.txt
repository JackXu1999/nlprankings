



















































Random Positive-Only Projections: PPMI-Enabled Incremental Semantic Space Construction


Proceedings of the Fifth Joint Conference on Lexical and Computational Semantics (*SEM 2016), pages 189–198,
Berlin, Germany, August 11-12, 2016.

Random Positive-Only Projections:
PPMI-Enabled Incremental Semantic Space Construction

Behrang QasemiZadeh
DFG SFB 991

Heinrich-Heine-Universität Düsseldorf
Düsseldorf, Germany

zadeh@phil.hhu.de

Laura Kallmeyer
DFG SFB 991

Heinrich-Heine-Universität Düsseldorf
Düsseldorf, Germany

kallmeyer@phil.hhu.de

Abstract

We introduce positive-only projection
(PoP), a new algorithm for constructing
semantic spaces and word embeddings.
The PoP method employs random projec-
tions. Hence, it is highly scalable and com-
putationally efficient. In contrast to pre-
vious methods that use random projec-
tion matrices R with the expected value
of 0 (i.e., E(R) = 0), the proposed
method uses R with E(R) > 0. We use
Kendall’s τb correlation to compute vector
similarities in the resulting non-Gaussian
spaces. Most importantly, since E(R) >
0, weighting methods such as positive
pointwise mutual information (PPMI) can
be applied to PoP-constructed spaces after
their construction for efficiently transfer-
ring PoP embeddings onto spaces that are
discriminative for semantic similarity as-
sessments. Our PoP-constructed models,
combined with PPMI, achieve an average
score of 0.75 in the MEN relatedness test,
which is comparable to results obtained by
state-of-the-art algorithms.

1 Introduction

The development of data-driven methods of nat-
ural language processing starts with an educated
guess, a distributional hypothesis: We assume
that some properties of linguistic entities can be
modelled by ‘some statistical’ observations in lan-
guage data. In the second step, this statistical in-
formation (which is determined by the hypothe-
sis) is collected and represented in a mathemati-
cal framework. In the third step, tools provided
by the chosen mathematical framework are used
to implement a similarity-based logic to iden-
tify linguistic structures, and/or to verify the pro-

posed hypothesis. Harris’s distributional hypothe-
sis (Harris, 1954) is a well-known example of step
one that states that meanings of words correlate
with the environment in which the words appear.
Vector space models and η-normed-based similar-
ity measures are notable examples of steps two
and three, respectively (i.e., word space models or
word embeddings).

However, as pointed out for instance by Ba-
roni et al. (2014), the count-based models resulting
from the steps two and three are not discrimina-
tive enough to achieve satisfactory results; instead,
predictive models are required. To this end, an ad-
ditional transformation step is often added. Tur-
ney and Pantel (2010) describe this extra step as
a combination of weighting and dimensionality re-
duction.1 This transformation from count-based to
predictive models can be implemented simply via
a collection of rules of thumb (such as frequency
threshold to filter out highly frequent and/or rare
context elements), and/or it can involve more so-
phisticated mathematical transformations, such as
converting raw counts to probabilities and using
matrix factorization techniques. Likewise, by ex-
ploiting the large amounts of computational power
available nowadays, this transformation can be
achieved via neural word embedding techniques
(Mikolov et al., 2013; Levy and Goldberg, 2014).

To a large extent, the need for such transfor-
mations arises from the heavy-tailed distributions
that we often find in statistical natural language
models (such as the Zipfian distribution of words
in contexts when building word spaces). Conse-
quently, count-based models are sparse and high-
dimensional and therefore both computationally
expensive to manipulate (due of the high dimen-
sionality of models) and nondiscriminatory (due to
the combination of the high-dimensionality of the

1Similar to topics of feature weighting, selection, and en-
gineering in statistical machine learning.

189



models and the sparseness of observations—see
Minsky and Papert (1969, chap. 12)).2

On the one hand, although neural networks
are often the top performers for addressing this
problem, their usage is costly: they need to be
trained, which is often very time-consuming,3 and
their performance can vary from one task to an-
other depending on their objective function.4 On
the other hand, although methods based on ran-
dom projections efficiently address the problem
of reducing the dimensionality of vectors—such
as random indexing (RI) (Kanerva et al., 2000),
reflective random indexing (RRI), (Cohen et al.,
2010), ISA (Baroni et al., 2007) and random Man-
hattan indexing (RMI) (Zadeh and Handschuh,
2014)—in effect they retain distances between en-
tities in the original space.5 Moreover, since these
methods use asymptotic Gaussian or Cauchy ran-
dom projection matrices R with E(R) = 0, their
resulting vectors cannot be adjusted and trans-
formed using weighting techniques such as PPMI.
Consequently, these methods often do not out-
perform neural embeddings and combinations of
PPMI weighting of count-based models followed
by matrix factorization—such as the truncation of
weighted vectors using singular value decomposi-
tion (SVD).

To overcome these problems, we propose a
new method called positive-only projection (PoP).
PoP is an incremental semantic space construc-
tion method which employs random projections.
Hence, building models using PoP does not re-
quire training but simply generating random vec-
tors. However, in contrast to RI (and previous
methods), the PoP-constructed spaces can undergo
weighting transformations such as PPMI, after
their construction and at a reduced dimensional-
ity. This is due to the fact that PoP uses random
vectors that contain only positive integer values.
Because the PoP method employs random pro-
jections, models can be built incrementally and
efficiently. Since the vectors in PoP-constructed
models are small (i.e., with a dimensionality of a
few hundred), applying weighting methods such

2That is, the well known curse of dimensionality problem.
3Baroni et al. (2014) state that it took Ronan Collobert

two months to train a set of embeddings from a Wikipedia
dump. Even using GPU-accelerated computing, the required
computation and training time for inducing neural word em-
beddings is high.

4Ibid, see results reported in supplemental materials.
5For η-normed space that they are designed for, i.e., η = 2

for RI, RRI, and ISA and η = 1 for RMI.

as PPMI to these models is incredibly faster than
applying them to classical count-based models.
Combined with a suitable weighting method such
as PPMI, the PoP algorithm yields competitive
results concerning accuracy in semantic similar-
ity assessments, compared for instance to neural
net-based approaches and combinations of count-
based models with weighting and matrix factoriza-
tion. These results, however, are achieved without
the need for heavy computations. Thus, instead of
hours, models can be built in a matter of a few sec-
onds or minutes. Note that even without weighting
transformation, PoP-constructed models display a
better performance than RI on tasks of semantic
similarity assessments.

We describe the PoP method in § 2. In order to
evaluate our models, in § 3, we report the perfor-
mance of PoP in the MEN relatedness test. Finally,
§ 4 concludes with a discussion.

2 Method

2.1 Construction of PoP Models
A transformation of a count-based model to a pre-
dictive one can be expressed using a matrix nota-
tion such as:

Cp×n ×Tn×m = Pp×m. (1)

In Equation 1, C denotes the count-based model
consisting of p vectors and n context elements
(i.e., n dimensions). T is the transformation ma-
trix that maps the p n-dimensional vectors in C to
an m-dimensional space (often, but not necessar-
ily,m 6= n andm� n). Finally, P is the resulting
m-dimensional predictive model. Note that T can
be a composition of several transformations, e.g.,
a weighting transformation W followed by a pro-
jection onto a space of lower dimensionality R,
i.e., Tn×m = Wn×n ×Rn×m.

In the proposed PoP technique, the transforma-
tion Tn×m (for m � n, e.g., 100 ≤ m ≤ 7000)
is simply a randomly generated matrix. The el-
ements tij of Tn×m have the following distribu-
tion:

tij =

{
0 with probability 1− s
b 1Uα c with probability s

, (2)

in which U is an independent uniform random
variable in (0, 1], and s is an extremely small num-
ber (e.g., s = 0.01) such that each row vector
of T has at least one element that is not 0 (i.e.,

190



∑m
i=1 tji 6= 0 for each row vector tj ∈ T). For

α, we choose α = 0.5. Given Equations 1 and 2
and using the distributive property of multiplica-
tion over addition in matrices,6 the desired seman-
tic space (i.e., P in Equation 1) can be constructed
using the two-step procedure of incremental word
space construction (such as used in RI, RRI, and
RMI):

Step 1. Each context element is mapped to one
m-dimensional index vector ~r. ~r is randomly gen-
erated such that most elements in ~r are 0 and only
a few are positive integers (i.e., the elements of ~r
have the distribution given in Equation 2).

Step 2. Each target entity that is being analysed
in the model is represented by a context vector ~v
in which all the elements are initially set to 0. For
each encountered occurrence of this target entity
together with a context element (e.g., through a se-
quential scan of a corpus), we update ~v by adding
the index vector ~r of the context element to it.

This process results in a model built directly at
the reduced dimensionality m (i.e., P in Equa-
tion 1). The first step corresponds to the construc-
tion of the randomly generated transformation ma-
trix T: Each index vector is a row of the transfor-
mation matrix T. The second step is an implemen-
tation of the matrix multiplication in Equation 1
which is distributed over addition: Each context
vector is a row of P, which is computed in an iter-
ative process.

2.2 Measuring Similarity
Once P is constructed, if desirable, similarities be-
tween entities can be computed by their Kendall’s
τb (−1 ≤ τb ≤ 1) correlation (Kendall, 1938). To
compute τb, we adopt an implementation of the al-
gorithm proposed by Knight (1966), which has a
computational complexity of O(n log n).7

In order to compute τb, we need to define a num-
ber of values. Given vectors ~x and ~y of the same
dimension, we call a pair of observations (xj , yj)
and (xj+1, yj+1) in ~x and ~y concordant if (xj <
xj+1 ∧ yj < yj+1) ∨ (xj > xj+1 ∧ yj > yj+1).
The pair is called discordant if (xj < xj+1∧ yj >
yj+1) ∨ (xj > xj+1 ∧ yj < yj+1). Finally, the
pair is called tied if xj = xj+1 ∨ yj = yj+1. Note
that a tied pair is neither concordant nor discor-
dant. We define n1 and n2 as the number of pairs

6That is (A + B)×C = A×C + B×C.
7In our evaluation, we use the implementation of Knight’s

algorithm in the Apache Commons Mathematics Library.

with tied values in ~x and ~y, respectively. We use
nc and nd to denote the number of concordant and
discordant pairs, respectively. If m is the dimen-
sion of the two vectors, then n0 is defined as the
total number of observation pairs: n0 =

m(m−1)
2 .

Given these definitions, Kendall’s τb is given by

τb =
nc − nd√

(n0 − n1)(n0 − n2)
.

The choice of τb can be motivated by generalis-
ing the role that cosine plays for computing sim-
ilarities between vectors that are derived from a
standard Gaussian random projection. In random
projections with R of (asymptotic) N (0, 1) dis-
tribution, despite the common interpretation of the
cosine similarity as the angle between two vectors,
cosine can be seen as a measure of the product-
moment correlation coefficient between the two
vectors. Since R and thus the obtained projected
spaces have zero expectation, Pearson’s correla-
tion and the cosine measure have the same defi-
nition in these spaces (see also Jones and Furnas
(1987) for a similar claim and on the relationships
between correlation and the inner product and co-
sine). Subsequently, one can propose that in Gaus-
sian random projections, Pearson’s correlation is
used to compute similarities between vectors.

However, the use of projections proposed in this
paper (i.e., T with a distribution set in Equation 2)
will result in vectors that have a non-Gaussian
distribution. In this case, τb becomes a reason-
able candidate for measuring similarities (i.e., cor-
relations between vectors) since it is a nonpara-
metric correlation coefficient measure that does
not assume a Gaussian distribution (see Chen and
Popovich (2002)) of projected spaces. However,
we do not exclude the use of other similarity mea-
sures and may employ them in future work. In
particular, we envisage additional transformations
of PoP-constructed spaces to induce vectors with
Gaussian distributions (see for instance the log-
based PPMI transformation used in the next sec-
tion). If a transformation to a Gaussian-like distri-
bution is performed, then it is expected that the use
of Pearson’s correlation, which works under the
assumption of Gaussian distribution, yields better
results than Kendall’s correlation (as confirmed by
our experiments).

2.3 Some Delineation of the PoP Method
The PoP method is a randomized algorithm. In
this class of algorithms, at the expense of a tolera-

191



ble loss in accuracy of the outcome of the com-
putations (of course, with a certain acceptable
amount of probability) and by the help of ran-
dom decisions, the computational complexity of
algorithms for solving a problem is reduced (see,
e.g., Karp (1991), for an introduction to random-
ized algorithms).8 For instance, using Gaussian-
based sparse random projections in RI, the com-
putation of eigenvectors (often of the complexity
of O(n2 logm)) is replaced by a much simpler
process of random matrix construction (of an es-
timated complexity of O(n))—see Bingham and
Mannila (2001). In return, randomized algorithms
such as the PoP and RI methods give different re-
sults even for the same input.

Assume the difference between the optimum re-
sult and the result from a randomized algorithm
is given by δ (i.e., the error caused by replacing
deterministic decisions with random ones). Much
research in theoretical computer science and ap-
plied statistics focuses on specifying bounds for δ,
which is often expressed as a function of the prob-
ability � of encountered errors. For instance, δ and
� in Gaussian random projections are often derived
from the lemma proposed by Johnson and Linden-
strauss (1984) and its variations. Similar studies
for random projections in `1-normed spaces and
deep neural networks are Indyk (2000) and Arora
et al. (2014), respectively.

At this moment, unfortunately, we are not able
to provide a detailed mathematical account for
specifying δ and � for the results obtained by the
PoP method (nor are we able to pinpoint a theo-
retical discussion about PoP’s underlying random
projection). Instead, we rely on the outcome of our
simulations and the performance of the method in
an NLP task. Note that this is not an unusual sit-
uation. For instance, Kanerva et al. (2000) pro-
posed RI with no mathematical justification. In
fact, it was only a few years later that Li et al.
(2006) proposed mathematical lemmas for justify-
ing very sparse Gaussian random projections such
as RI (QasemiZadeh, 2015). At any rate, projec-
tions onto manifolds is a vibrant research both in
theoretical computer science and in mathematical
statistics. Our research will benefit from this in the
near future. If δ refers to the amount of distortion
in pairwise `2 norm correlation measures in a PoP
space,9 it can be shown that δ and its variance σ2δ

8Such as many classic search algorithms that are proposed
for solving NP-complete problems in artificial intelligence.

9As opposed to pairwise correlations in the original high-

are functions of the dimension m of the projected
space, that is: σ2δ ≈ 1m , based on similar mathe-
matical principles proposed by Kaski (1998) (and
of Hecht-Nielsen (1994)) for the random mapping.

Our empirical research and observations on
language data show that projections using the
PoP method exhibit similar behavioural patterns
as other sparse random projections in α-normed
spaces. The dimensionm of random index vectors
can be seen as the capacity of the method to mem-
orize and distinguish entities. Form up to a certain
number (100 ≤ m ≤ 6000) in our experiments, as
was expected, a PoP-constructed model for a large
m shows a better performance and smaller δ than
a model for a small m. Since observations in se-
mantic spaces have a very-long-tailed distribution,
choosing different values of non-zero elements for
index vectors does not effect the performance (as
mentioned, in most cases 1, 2 or 3 non-zero ele-
ments are sufficient). Furthermore, changes in the
adopted distribution of tij only slightly affect the
performance of the method.

In the next section, using empirical investiga-
tions we show the advantages of the PoP model
and support the claims from this section.

3 Evaluation & Empirical Investigations

3.1 Comparing PoP and RI

For evaluation purposes, we use the MEN re-
latedness test set (Bruni et al., 2014) and the
UKWaC corpus (Baroni et al., 2009). The dataset
consists of 3000 pairs of words (from 751 dis-
tinct tagged lemmas). Similar to other ‘related-
ness tests’, Spearman’s rank correlation ρ score
from the comparison of human-based ranking and
system-induced rankings is the figure of merit. We
use these resources for evaluation since they are
in public domain, both the dataset and corpus are
large, and they have been used for evaluating sev-
eral word space models—for example, see Levy
et al. (2015), Tsvetkov et al. (2015), Baroni et al.
(2014), Kiela and Clark (2014). In this section,
unless otherwise stated, we use cosine for similar-
ity measurements.

Figure 1 shows the performance of the simple
count-based word space model for lemmatized-
context-windows that extend symmetrically
around lemmas from MEN.10 As expected, up to

dimensional space.
10We use the tokenized preprocessed UKWaC. However,

except for using part-of-speech tags for locating lemmas

192



a certain context-window size, the performance
using count-based methods increases with an
extension of the window.11 For context-windows
larger than 25+25 the performance gradually
declines. More importantly, in all cases, we have
ρ < 0.50.

We performed the same experiments using the
RI technique. For each context window size, we
performed 10 runs of the RI model construction.
Figure 1 reports for each context-window size the
average of the observed performances for the 10
RI models. In this experiment, we used index
vectors of dimensionality 1000 containing 4 non-
zero elements. As shown in Figure 1, the aver-
age performance of the RI is almost identical to
the performance of the count-based model. This
is an expected result since RI’s objective is to re-
tain Euclidean distances between vectors (thus co-
sine) but in spaces of lowered dimensionality. In
this sense, RI is successful and achieves its goal
of lowering the dimensionality while keeping Eu-
clidean distances between vectors. However, us-
ing RI+cosine does not yield any improvements in
the similarity assessment task.

We then performed similar experiments using
PoP-constructed models, with the same context
window sizes and the same dimensions as in the
RI experiments, averaging again over 10 runs for
each context window size. The performance is
also reported in Figure 1. For the PoP method,
however, instead of using the cosine measure we
use Kendall’s τb for measuring similarities. The
PoP-constructed models converge faster than RI
and count-based methods and for smaller context-
windows they outperform the count-based and RI
methods with a large margin. However, as the
sizes of the windows grow, performances of these
methods become more similar (but PoP still out-
performs the others). In any case, the performance
of PoP remains above 0.50 (i.e., ρ > 0.50). Note
that in RI-constructed models, using Kendall’s τb
also yield better performance than using cosine.

3.2 PPMI Transformation of PoP Vectors

Although PoP outperforms RI and count-based
models, compared to the state-of-the-art methods,

listed in MEN, we do not use any additional information or
processes (i.e., no frequency cut-off for context selection, no
syntactic information, etc.).

11After all, in models for relatedness tests, relationships of
topical nature play a more important role than other relation-
ships such as synonymy.

1+
1
4+
4

25
+2
5

50
+5
0

0.3

0.4

0.5

0.55

Context Window Size

Sp
ea

rm
an

’s
C

or
re

la
tio

n
ρ

Count-based+Cos
RI+Cos

PoP+Kendall

Figure 1: Performance of the classic count-based
a-word-per-dimension model vs. RI vs. Pop in the
MEN relatedness test. Note that count-based and
RI models show almost an identical performance
in this task.

its performance is still not satisfying. Transfor-
mations based on association measures such as
PPMI have been proposed to improve the discrim-
inatory power of context vectors and thus the per-
formance of models in semantic similarity assess-
ment tasks (see Church and Hanks (1990), Turney
(2001), Turney (2008), and Levy et al. (2015)).
For a given set of vectors, pointwise mutual infor-
mation (PMI) is interpreted as a measure of infor-
mation overlap between vectors. As put by Bouma
(2009), PMI is a mathematical tool for measuring
how much the actual probability of a particular
co-occurrence (e.g., two words in a word space)
deviate from the expected probability of their in-
dividual occurrences (e.g., the probability of oc-
currences of each word in a words space) under
the assumption of independence (i.e., the occur-
rence of one word does not affect the occurrences
of other words).

In Figure 2, we show the performance of PMI-
transformed spaces. Count-based PMI+Cosine
models outperform other techniques including the
introduced PoP method. The performance of
PMI models can be further enhanced by their
normalization, often discarding negative values12

and using PPMI. Also, SVD truncation of PPMI-
weighted spaces can improve the performance
slightly (see the above mentioned references)
requiring, however, expensive computations of
eigenvectors.13 For a p × n matrix with elements
vxy, 1 ≤ x ≤ p and 1 ≤ y ≤ n, we compute the

12See Bouma (2009) for a mathematical delineation. Juraf-
sky and Martin (2015) also provide an intuitive description.

13In our experiments, applying SVD truncation to models
results in negligible improvements between 0.01 and 0.001.

193



1+
1

4+
4

9+
9

17
+1
7

0.65

0.7

0.75

Context Window Size

Sp
ea

rm
an

’s
C

or
re

la
tio

n
ρ

PMI-Dense+Cos
PPMI-Dense+Cos

Our PoP+PPMI+Kendall
Our PoP+PPMI+Pearson

Figure 2: Performances of (P)PMI-transformed
models for various sizes of context-windows.
From context size 4+4, the performance re-
mains almost intact (0.72 for PMI and 0.75 for
PPMI). We also report the average performance
for PoP-constructed models constructed at the
dimensionality m = 1000 and s = 0.002.
PoP+PPMI+Pearson exhibits a performance sim-
ilar as dense PPMI-weighted models, however,
much faster and using far less amount of computa-
tional resources. Note that reported PoP+PMI per-
formances can be enhanced by using m > 1000.

PPMI weight for a component vxy as follows:

ppmi(vxy) = max(0, log
vxy×

∑p
i=1

∑n
j=1 vij∑p

i=1 viy×
∑n
j=1 vxj

). (3)

The most important benefit of the PoP method
is that PoP-constructed models, in contrast to pre-
viously suggested random projection-based mod-
els, can be still weighted using PPMI (or any other
weighting techniques applicable to the original
count-based models). In an RI-constructed model,
the sum of values of row and column vectors of the
model are always 0 (i.e.,

∑p
i=1 viy and

∑n
j=1 vxj

in Equation 3 are always 0). As mentioned ear-
lier, this is due to the fact that a random projec-
tion matrix in RI has an asymptotic standard Gaus-
sian distribution (i.e., transformation matrix R has
E(R) = 0). As a result, PPMI weights for the RI-
induced vector elements are undefined. In contrast
to RI, the sum of values of vector elements in the
PoP-constructed models is always greater than 0
(because the transformation is carried out by a pro-
jection matrix R of E(R) > 0). Also, depending
on the structure of data in the underlying count-
based model, by choosing a suitably large value of
s, it can be guaranteed that the sum of column vec-
tors is always a non-zero value. Hence, vectors in
PoP models can undergo the PPMI transformation
defined in Equation 3. Moreover, the PPMI trans-

formation in PoP models is much faster, compared
to the one performed on count-based models, due
to the low dimensionality of vectors in the PoP-
constructed model. Therefore, the PoP method
makes it possible to benefit both from the high ef-
ficiency of randomized techniques as well as from
the high accuracy of PPMI transformation in se-
mantic similarity tasks.

If we put aside the information-theoretic inter-
pretation of PPMI weighting (i.e., distilling sta-
tistical information that matters), the logarithmic
transformation of probabilities in the PPMI def-
inition plays the role of a power transformation
process for converting long-tailed distributions in
the original high-dimensional count-based models
to Gaussian-like distributions in the transformed
models. From a statistical perspective, any varia-
tion of PMI transformation can be seen as an at-
tempt to stabilize the variance of vector coordi-
nates and therefore to make the observations more
similar/fit to Gaussian distribution (a practice with
a long history in research, particularly in the bio-
logical and psychological sciences).

To exemplify this phenomenon, in Figure 3, we
show histograms of the distributions of the as-
signed weights to the vector that represents the
lemmatized form of the verb ‘abandon’ in vari-
ous models. As shown, the raw collected fre-
quencies in the original high-dimensional count-
based model have a long tail distribution (see Fig-
ure 3a). Applying the log transformation to this
vector yields a vector of weights with a Gaus-
sian distribution (Figure 3b). Weights in the
RI-constructed vector (Figure 3c) have a perfect
Gaussian distribution but with an expected value
of 0 (i.e., N (0, 1)). The PoP method, however,
largely preserves the long tail distribution of coor-
dinates from the original space (Figure 3d), which
in turn can be weighted using PPMI and thereby
transformed into a Gaussian-like distribution.

Given that models after the PPMI transforma-
tion have bell-shaped Gaussian distributions, we
expect that a correlation measure such as Pear-
son’s r, which takes advantage of the prior knowl-
edge about the distribution of data, outperforms
the non-parametric Kendall’s τb for computing
similarities in PPMI-transformed spaces.14 This
is indeed the case (see Figure 2).

14Note that using correlation measures such as Pearson’s r
and Kendall’s τb in count-based model may excel measures
such as cosine. However, their application is limited due to
the high-dimensionality of count-based methods.

194



100 10,000
1

10

100

1,000

log(Raw Frequencies)

log(Frequency)

(a) Count-based

0 50

100

200

300

400

PMI Weights

Frequency

(b) PMI

−1,700 0 1,700

20

40

RI Weights

Frequency

(c) RI

10 1,000 100,000

10

log(POP Weights)

log(Frequency)

(d) POP

−50 0 50 100

100

200

300

400

POP PMI Weights

Frequency

(e) POP+PMI

Figure 3: A histogram of the distribution of frequencies of weights (i.e., the value of the coordinates) in
various models built from 1+1 context-windows for the lemmatized form of the verb ‘abandon’ in the
UKWaC corpus.

3.3 PoP’s Parameters, its Random Behavior
and Performance

As discussed in § 2.3, PoP is a randomized al-
gorithm and its performance is influenced by a
number of parameters. In this section, we study
the PoP method’s behavior by reporting its perfor-
mance in the MEN relatedness test under different
parameter settings. To keep evaluations and re-
ports to a manageable size, we focus on models
built using context-windows of size 4+4.

Figure 4 shows the method’s performance when
the dimensionm of the projected index vectors in-
creases. In these experiments, index vectors are
built using 4 non-zero elements; thus, as m in-
creases, s in Equation 2 decreases. For each m,
100 ≤ m ≤ 5000, the models are built 10 times
and the average as well as the maximum and the
minimum observed performances in these exper-
iments are reported. For PPMI transformed PoP
spaces, with increasing dimensions, the perfor-
mance boosts and, furthermore, the variance in
performance (i.e., the shaded areas)15 gets smaller.

However, for the count-based PoP method with-
out PPMI transformation (shown by the dash-
dotted lines) and with the number of non-zero ele-
ments fixed to 4, increasingm over 2000 decreases
the performance. This is unexpected since an in-
crease in dimensionality is usually assumed to en-
tail an increase in performance. This behavior,
however, can be the result of using a very small
s; simply put, the number of non-zero elements
are not sufficient to build projected spaces with
adequate distribution. To investigate this matter,
we study the performance of the method with the
dimension m fixed to 3000 but with index vec-

15Evidently, the probability of worst and best perfor-
mances can be inferred from the reported average results.

10
0

50
0

10
00

15
00

20
00

30
00

40
00

50
00

0.5

0.55

0.6

0.7

0.75

Models’ Dimensionality (m)

Sp
ea

rm
an

’s
C

or
re

la
tio

n
ρ

PoP+PPMI+Pearson
PoP+PPMI+ Kendall

PoP+Kendall

Figure 4: Changes in PoP’s performance when
the dimensionality of models increases. The av-
erage performance in each set-up is shown by the
marked lines. The margins around these lines
show the minimum and maximum performance
observed in 10 independent executions.

tors built using different numbers of non-zero ele-
ments, i.e., different values of s.

Figure 5 shows the observed performances. For
PPMI-weighted spaces, increasing the number of
non-zero elements clearly deteriorates the perfor-
mance. For unweighted PoP models, an increase
in s up to the limit that does not result in non-
orthogonal index vectors enhances performances.
As shown in Figure 6, when the dimensionality
of the index vectors is fixed and s increases, the
chances of having non-orthogonal vectors in index
vectors are boosted. Hence, the chance of distor-
tions in similarities increases. These distortions
can enhance the result if they are controlled (e.g.,
using a training procedure such as the one used
in neural net embedding). However, when left to
chance, they can often lower the performance. Ev-
idently, this is an oversimplified justification: in
fact, s plays the role of a switch that controls the
resemblance between the distribution of data in

195



1 2 4 8 16 32
0.5

0.55

0.6

0.7

0.75

1

Number of Non-Zero Elements in Index Vectors

Sp
ea

rm
an

’s
C

or
re

la
tio

n
ρ

PoP+PPMI+Pearson
PoP+PPMI+ Kendall

PoP+Kendall

Figure 5: Changes in PoP’s performances when
the dimensionality of models are fixed to m =
3000 and the number of non-zero elements in in-
dex vectors (i.e., s) increases. The average perfor-
mances in each set-up are shown by marked lines.
The margins around these lines show the minimum
and maximum performance observed in 10 inde-
pendent executions.

the original space and the projected/transformed
spaces. It seems that the sparsity of vectors in
the original matrix plays a role in finding the
optimal value for s. If PoP-constructed models
are used directly (together with τb) for comput-
ing similarities, then we propose 0.002 < s. If
PoP-constructed models are subject to an addi-
tional weighting process for stabilizing vector dis-
tributions into Gaussian-like distributions such as
PPMI, we propose using only 1 or 2 non-zero ele-
ments.

Last but not least, we confirm that by care-
fully selecting context elements (i.e., removing
stop words and using lower and upper bound fre-
quency cut-offs for context selection) and fine tun-
ing PoP+PPMI+Pearson (i.e., increasing the di-
mension of models and scaling PMI weights as
in Levy et al. (2015)) we achieve an even higher
score in the MEN test (i.e., an average of 0.78 with
the max of 0.787). Moreover, although improve-
ments from applying SVD truncation are negligi-
ble, we can employ it for reducing the dimension-
ality of PoP vectors (e.g., from 6000 to 200).

4 Conclusion

We introduced a new technique called PoP for the
incremental construction of semantic spaces. PoP
can be seen as a dimensionality reduction method,
which is based on a newly devised random pro-
jection matrix that contains only positive integer
values. The major benefit of PoP is that it trans-
fers vectors onto spaces of lower dimensionality
without changing their distribution to a Gaussian

4 6 8 10 12 14 16 18 20
0

0.2

0.4

0.6

0.8

1

non-zero elements

P
6⊥

#index vectors n = 104

1 4

·104n

#non-zero elements = 8

m = 100
m = 1000
m = 2000

Figure 6: The proportion of non-orthogonal pairs
of index vectors (P 6⊥) obtained in a simulation for
various dimensionality and number of non-zero el-
ements. The left figure shows the changes of P6⊥
for a fixed number of index vectors n = 104 when
the number of non-zero elements increases. The
right figure shows P6⊥ when the number of non-
zero elements is fixed to 8 but the number of index
vectors n increases. As shown, P 6⊥ is determined
by the number of non-zero elements and the di-
mensionality of index vectors and independently
of n.

shape with zero expectation. The obtained trans-
formed spaces using PoP can, therefore, be manip-
ulated similarly to the original high-dimensional
spaces, only much faster and consequently requir-
ing a considerably lower amount of computational
resources.

PPMI weighting can be easily applied to
PoP-constructed models. In our experiments, we
observe that PoP+PPMI+Pearson can be used
to build models that achieve a high perfor-
mance in semantic relatedness tests. More con-
cretely, for index vector dimensions m ≥ 3000,
PoP+PPMI+Pearson achieves an average score of
0.75 in the MEN relatedness test, which is compa-
rable to many neural embedding techniques (e.g.,
see scores reported in Chen and de Melo (2015)
and Tsvetkov et al. (2015)). However, in contrast
to these approaches, PoP+PPMI+Pearson achieves
this competitive performance without the need for
time-consuming training of neural nets. Moreover,
the processes involved are all done on vectors of
low dimensionality. Hence, the PoP method can
dramatically enhance the performance in tasks in-
volving distributional analysis of natural language.

Acknowledgments

The work described in this paper is funded by the
Deutsche Forschungsgemeinschaft (DFG) through
the ’Collaborative Research Centre 991 (CRC
991): The Structure of Representations in Lan-
guage, Cognition, and Science”.

196



References
Sanjeev Arora, Aditya Bhaskara, Rong Ge, and Tengyu

Ma. 2014. Provable bounds for learning some deep
representations. In Proceedings of the 31st Inter-
national Conference on Machine Learning, ICML
2014, Beijing, China, 21-26 June 2014, pages 584–
592.

Marco Baroni, Alessandro Lenci, and Luca Onnis.
2007. ISA meets Lara: An incremental word space
model for cognitively plausible simulations of se-
mantic learning. In Proceedings of the Workshop
on Cognitive Aspects of Computational Language
Acquisition, pages 49–56, Prague, Czech Republic,
June. Association for Computational Linguistics.

Marco Baroni, Silvia Bernardini, Adriano Ferraresi,
and Eros Zanchetta. 2009. The wacky wide web:
a collection of very large linguistically processed
web-crawled corpora. Language Resources and
Evaluation, 43(3):209–226.

Marco Baroni, Georgiana Dinu, and Germán
Kruszewski. 2014. Don’t count, predict! a
systematic comparison of context-counting vs.
context-predicting semantic vectors. In Proceedings
of the 52nd Annual Meeting of the Association
for Computational Linguistics (Volume 1: Long
Papers), pages 238–247, Baltimore, Maryland,
June. Association for Computational Linguistics.

Ella Bingham and Heikki Mannila. 2001. Random
projection in dimensionality reduction: applications
to image and text data. In Proceedings of the Sev-
enth ACM SIGKDD International Conference on
Knowledge discovery and data mining, KDD ’01,
pages 245–250, New York, NY, USA. ACM.

Gerlof Bouma. 2009. Normalized (pointwise) mutual
information in collocation extraction. In Proceed-
ings of the Biennial GSCL Conference.

Elia Bruni, Nam Khanh Tran, and Marco Baroni. 2014.
Multimodal distributional semantics. J. Artif. Int.
Res., 49(1):1–47, January.

Jiaqiang Chen and Gerard de Melo. 2015. Semantic
information extraction for improved word embed-
dings. In Proceedings of the 1st Workshop on Vector
Space Modeling for Natural Language Processing,
pages 168–175, Denver, Colorado, June. Associa-
tion for Computational Linguistics.

Peter Y. Chen and Paula M. Popovich. 2002. Cor-
relation: Parametric and Nonparametric Measures.
Quantitative Applications in the Social Sciences.
Sage Publications.

Kenneth Ward Church and Patrick Hanks. 1990. Word
association norms, mutual information, and lexicog-
raphy. Comput. Linguist., 16(1):22–29, March.

Trevor Cohen, Roger Schvaneveldt, and Dominic Wid-
dows. 2010. Reflective random indexing and indi-
rect inference: A scalable method for discovery of

implicit connections. Journal of Biomedical Infor-
matics, 43(2):240 – 256.

Zellig Harris. 1954. Distributional structure. Word,
10(2-3):146–162.

Robert Hecht-Nielsen. 1994. Context vectors: gen-
eral purpose approximate meaning representations
self-organized from raw data. Computational Intel-
ligence: Imitating Life, pages 43–56.

Piotr Indyk. 2000. Stable distributions, pseudorandom
generators, embeddings and data stream computa-
tion. In 41st Annual Symposium on Foundations of
Computer Science, pages 189–197.

William Johnson and Joram Lindenstrauss. 1984. Ex-
tensions of Lipschitz mappings into a Hilbert space.
In Conference in modern analysis and probabil-
ity (New Haven, Conn., 1982), volume 26 of Con-
temporary Mathematics, pages 189–206. American
Mathematical Society.

William P. Jones and George W. Furnas. 1987. Pic-
tures of relevance: A geometric analysis of similar-
ity measures. Journal of the American Society for
Information Science, 38(6):420–442.

Daniel Jurafsky and James H. Martin, 2015. Speech
and Language Processing, chapter Chapter 19: Vec-
tor Semantics. Prentice Hall, 3rd edition. Draft of
August 24, 2015.

Pentti Kanerva, Jan Kristoferson, and Anders Holst.
2000. Random indexing of text samples for latent
semantic analysis. In Proceedings of the 22nd An-
nual Conference of the Cognitive Science Society,
pages 103–6. Erlbaum.

Richard M. Karp. 1991. An introduction to ran-
domized algorithms. Discrete Applied Mathematics,
34(13):165 – 201.

Samuel Kaski. 1998. Dimensionality reduction by ran-
dom mapping: fast similarity computation for clus-
tering. In The 1998 IEEE International Joint Con-
ference on Neural Networks, volume 1, pages 413–
418.

Maurice G. Kendall. 1938. A new measure of rank
correlation. Biometrika, 30(1-2):81–93.

Douwe Kiela and Stephen Clark. 2014. A systematic
study of semantic vector space model parameters.
In Proceedings of the 2nd Workshop on Continu-
ous Vector Space Models and their Compositionality
(CVSC), pages 21–30, Gothenburg, Sweden, April.
Association for Computational Linguistics.

William R. Knight. 1966. A computer method
for calculating kendall’s tau with ungrouped data.
Journal of the American Statistical Association,
61(314):436–439.

197



Omer Levy and Yoav Goldberg. 2014. Neural
word embedding as implicit matrix factorization.
In Z. Ghahramani, M. Welling, C. Cortes, N. D.
Lawrence, and K. Q. Weinberger, editors, Advances
in Neural Information Processing Systems 27, pages
2177–2185. Curran Associates, Inc.

Omer Levy, Yoav Goldberg, and Ido Dagan. 2015. Im-
proving distributional similarity with lessons learned
from word embeddings. Transactions of the Associ-
ation for Computational Linguistics, 3:211–225.

Ping Li, Trevor J. Hastie, and Kenneth W. Church.
2006. Very sparse random projections. In Proceed-
ings of the 12th ACM SIGKDD International Con-
ference on Knowledge Discovery and Data Mining,
KDD ’06, pages 287–296, New York, NY, USA.
ACM.

Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Cor-
rado, and Jeff Dean. 2013. Distributed representa-
tions of words and phrases and their composition-
ality. In C. J. C. Burges, L. Bottou, M. Welling,
Z. Ghahramani, and K. Q. Weinberger, editors, Ad-
vances in Neural Information Processing Systems
26, pages 3111–3119. Curran Associates, Inc.

Marvin Lee Minsky and Seymour Papert. 1969. Per-
ceptrons: An Introduction to Computational Geom-
etry. MIT Press.

Behrang QasemiZadeh. 2015. Random indexing revis-
ited. In 20th International Conference on Applica-
tions of Natural Language to Information Systems,
NLDB, pages 437–442. Springer.

The Apache Commons Mathematics Li-
brary [Computer Software]. 2016.
KendallsCorrelation Class. Retrieved from
https://commons.apache.org/proper/commons-
math/javadocs/api-3.6.1/index.html.

Yulia Tsvetkov, Manaal Faruqui, Wang Ling, Guil-
laume Lample, and Chris Dyer. 2015. Evaluation of
word vector representations by subspace alignment.
In Proceedings of the 2015 Conference on Empiri-
cal Methods in Natural Language Processing, pages
2049–2054, Lisbon, Portugal, September. Associa-
tion for Computational Linguistics.

Peter D. Turney and Patrick Pantel. 2010. From fre-
quency to meaning: Vector space models of seman-
tics. J. Artif. Int. Res., 37(1):141–188, January.

Peter D. Turney. 2001. Mining the web for synonyms:
PMI-IR versus LSA on TOEFL. In Proceedings of
the 12th European Conference on Machine Learn-
ing, EMCL ’01, pages 491–502, London, UK, UK.
Springer-Verlag.

Peter D. Turney. 2008. The latent relation mapping
engine: Algorithm and experiments. J. Artif. Int.
Res., 33(1):615–655, December.

Behrang Q. Zadeh and Siegfried Handschuh. 2014.
Random Manhattan integer indexing: Incremental
L1 normed vector space construction. In Proceed-
ings of the 2014 Conference on Empirical Methods
in Natural Language Processing, EMNLP 2014, Oc-
tober 25-29, 2014, Doha, Qatar, A meeting of SIG-
DAT, a Special Interest Group of the ACL, pages
1713–1723.

A Supplemental Material

Codes and resulting embeddings from ex-
periments are available from https:
//user.phil-fak.uni-duesseldorf.
de/˜zadeh/material/pop-vectors.

198


