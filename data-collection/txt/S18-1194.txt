



















































TRANSRW at SemEval-2018 Task 12: Transforming Semantic Representations for Argument Reasoning Comprehension


Proceedings of the 12th International Workshop on Semantic Evaluation (SemEval-2018), pages 1142–1145
New Orleans, Louisiana, June 5–6, 2018. ©2018 Association for Computational Linguistics

TRANSRW at SemEval-2018 Task 12: Transforming Semantic
Representations for Argument Reasoning Comprehension

Zhimin Chen, Wei Song∗, Lizhen Liu
Information Engineering College

Capital Normal University
Beijing 100048, China

{zmchen, wsong, liz liu7480}@cnu.edu.cn

Abstract

This paper describes our system in SemEval-
2018 task 12: Argument Reasoning Compre-
hension. The task is to select the correct war-
rant that explains reasoning of a particular ar-
gument consisting of a claim and a reason. The
main idea of our methods is based on the as-
sumption that the semantic composition of the
reason and the warrant should be close to the
semantic representation of the corresponding
claim. We propose two neural network mod-
els. The first one considers two warrant can-
didates simultaneously, while the second one
processes each candidate separately and then
chooses the best one. We also incorporate
sentiment polarity by assuming that there are
kinds of sentiment associations between the
reason, the warrant and the claim. The exper-
iments show that the first framework is more
effective and sentiment polarity is useful.

1 Introduction

Argument reasoning is a key step in the process
of argumentation mining and is a very challeng-
ing task in natural language processing and artifi-
cial intelligence. Maccartney and Manning (2008)
suggested that the key factor in the study of natu-
ral language understanding is the mastery of nat-
ural language reasoning. When we argue for an
argument, it is necessary to reconstruct the implic-
it reasoning (Newman and Marshall, 1992; Haber-
nal et al., 2017) under the relevant assumption and
premise to get a simple and concise explanation of
the whole reasoning process.

The Argument Reasoning Comprehension task
is defined as following:

Given an argument consisting of a claim C and
a reason R, the goal is to select the correct war-
rant that explains reasoning of this particular ar-

∗*corresponding author

gument. There are only two options W0 and W1
given and only one answer is correct.

Our solution is based on the assumption that the
semantic composition of the reason and the true
warrant should be close to the semantic represen-
tation of the claim. We propose two frameworks.
First one is dependent on the task settings that two
warrant candidates are considered simultaneously
to make a decision. The second one is more gener-
al that the task is simplified as determine whether
a warrant candidate can explain argument reason-
ing. We found that the first one performed better.

In addition, we attempt to incorporate sentimen-
t polarity to capture the sentiment association be-
tween the reason, the warrant and the claim. The
experimental results demonstrate that adding sen-
timent polarity can improve the performance.

The final result is produced by an ensemble ap-
proach that combines the outputs of multiple s-
ingle models. Our system achieves an accuracy
of 0.67 on development dataset and 0.57 on test
dataset.

2 System Description

2.1 Model1: Competitive Model

The first proposed model is designed depending
on the specific task setting. The architecture of
Modell is shown in Figure 1. We first get the rep-
resentations of the claim C, the reason R and the
two warrant candidates W0 and W1. Then we use
a transformer to get the representation of a pseudo
claim, by compositing the reason R and a warran-
t candidate W . Finally, the model predicts which
warrant candidate is the correct one by consider-
ing the claim C and two pseudo claims.

2.1.1 Sentence Representation
Figure 2 illustrates the architecture for getting the
sentence representation.

1142



Softmax

composition

Concatenation

Sentence Representation

claim               warrant0                       reason                       warrant1  

composition

Figure 1: The architecture of Model1: Competitive
Model.

word[1]              ……               word[80]

word embedding 
+ sentiment polarity

sentence

Convolutional 
neural 

network

Sentence Representation

Representation layer

word embedding 
+ sentiment polarity 

……         

Figure 2: The representation of the sentence obtained
by the convolutional neural network.

Word Embeddings. We first map each word to a
word embedding, which is a dense distributed vec-
tor. Since the dataset of this task is relatively smal-
l, we hope the word embeddings can improve gen-
eralization. The word embeddings we used were
pre-trained and released by Huang et al. (2012).
Sentiment Polarity of Words. We expect that
there are relations between the claim, the reason
and the warrant. For example, they may have the
same sentiment polarity, while there may be polar-
ity conflicts when involving a false warrant.

Therefore, we use the sentiment polarity of
words as a kind of common sense knowledge. The
negative words and positive words come from the
dictionary provided by Hu and Liu (2004). The
polarity representation of each word is a two di-
mensional vector. A positive word is represent-
ed as [1, 0] and a negative word is represented
as [0, 1]. The representation of out-of dictionary
words is [0, 0].

We concatenate the word embedding and senti-
ment polarity representation together as the final
representation of a word.
Convolutional Neural Networks. The word em-

beddings are feed into a convolutional neural net-
work (CNN) to get the representation of a sen-
tence. We mainly follow the architecture of Kim
(2014), which reports excellent performance for
several sentence classification tasks. The dimen-
sion of word embeddings is k and the sentence
length is fixed. A sentence s consisting of n word-
s can be represented as the concatenation of the
embeddings of the n words:

s = ~e1 ⊕ ~e2 ⊕ · · · ⊕ ~en, (1)
where ~ei is the k-dimensional word vector of the
ith word. A convolution operation involves a fil-
ter w ∈ Rh,k, which is applied to a window of h
words to produce a new feature ai:

ai = f(w · si:i+h−1 + b), (2)
where f is a non-linear activation function, which
is set to Relu (Nair and Hinton, 2010) and b is a
bias term. The feature map a can be represented
as

a = [a1, a2, · · · , an−h+1]. (3)
Then a max-pooling operation is applied to the re-
sulted feature map to get the sentence representa-
tion.

In experiments, k = 52, h = 3, and we used
64 filters. A dropout layer is added after the word
embedding layer with a probability 0.25. The rep-
resentations of the claim, reason and warrant can-
didates are all learned in this way.

2.1.2 Pseudo Claim Representation
We assume that the claim is a semantic composi-
tion of a reason and a warrant. Therefore, we use
a composition operator to combine the representa-
tions of the reason and a warrant candidate to get
the representation of a pseudo claim, noted as C0
and C1 respectively.

We have tried four composition operators: AD-
D, INNERPRODUCT, CONCATENATION and FUL-
LYCONNECTEDNETWORK. In experiments, ADD
performs best.

2.1.3 Prediction
Finally, we connect the representations of C, C0
and C1 to fully connect layers and concatenate
them into one representation. And we connect the
representation to the output layer through a non-
linear transformation layer (Relu). The output is
expected to be 1 if W1 is right and expected to be
0 if W0 is right. In experiments, we permuted the
order of W0 and W1 to enlarge the training dataset.

1143



2.2 Model2: Isolation Model

We consider a more general setting: Given the
claim and reason of argument, determine whether
a warrant candidate can support the argumenta-
tion. As shown in Figure 3, Model2 is just a
simplification of Model1. It processes one war-
rant candidate individually. The output indicates
whether the given warrant candidate is right.

claim reason warrant 

Softmax

composition

Concatenation

Sentence Representation

Figure 3: The architecture of Model2: Isolation Model.

For W0 and W1, we can get their corresponding
probability p(1|W0) and p(1|W1). We choose the
one with a higher probability as the predicted right
warrant for the task.

2.3 Ensemble Model

We have already described the two proposed mod-
els. For each model, we trained several times
with different initialization parameters. Finally,
we chose 3 models from Model1 and 3 models
from Model2, which performed well on the de-
velopment dataset. We used the prediction prob-
abilities of the 6 models as features and trained a
random forest classifier for ensemble (Pal, 2005).

3 Evaluation

We conducted experiments on the official datasets
of SemEval-2018 Task 12. The model parameter-
s are trained using the training dataset and tuned
based on the performance of development dataset.
We will report the results on both development
dataset and test dataset. Accuracy is the official
evaluation metric. We also report precision, recall
and F1 score.

We are interested in two research questions:

• RQ1: Which proposed model is more effec-
tive?

• RQ2: Whether incorporating sentiment po-
larity can benefit this task?

3.1 Results on Development Dataset

Table 1 shows the results on the developmen-
t dataset. The accuracy of the random baseline is
0.503. The proposed models significantly outper-
form the baseline.

By adding sentiment polarity representations,
Model1 and Model2 both improve a lot. The ac-
curacy of Model1 increases 3.16%, while the ac-
curacy of Model2 increases 2.43%. The precision,
recall and F1 score all have the same trend. With
the sentiment polarity added, the Model1 performs
better than Model2. Without the sentiment polari-
ty, their performance is very close.

3.2 Results on Test Dataset

Table 2 shows the results on test dataset. The
random baseline submitted by task organizer is
0.527. The accuracy of the ensemble model is
0.57, which outperforms the random baseline by
4.3%. After the task organizer released the gold
test dataset, we predicted it again using the ensem-
ble model and the accuracy is 0.5811.

Similar to the results on development dataset,
with the sentiment polarity added, both Model1
and Model2 achieve a better performance. We can
see that on test dataset, Model1 outperforms Mod-
el2 no matter using or removing sentiment polari-
ty representations. When using sentiment polarity,
the performance difference is larger.

3.3 Discussion

From the experimental results on the development
dataset and the test dataset, we can see that sen-
timent polarity is always useful for distinguishing
the correct warrant from the false one. Model1
performs slightly better than Model2. It is rea-
sonable since Model1 considers richer information
than Model2. But Model2 actually is a more gen-
eral model. With sentiment polarity added, the ad-
vantage of Model1 is amplified. This also indi-
cates the usefulness of sentiment polarity of word-
s.

The model performance is better on the devel-
opment dataset than on the test dataset. The pro-
posed models may still suffer the overfitting prob-
lem, since the training dataset is not very large.

4 Conclusion

In this paper we presented our system that partic-
ipated in the SemEval-2018 Task 12: Argumen-
t Reasoning Comprehension. Our assumption is

1144



Model Precision Recall F1 score Accuracy
Random Baseline - - - 0.503

Model1 0.6380± 0.018 0.6276± 0.010 0.6216± 0.010 0.6276± 0.010
w/o polarity 0.5982± 0.007 0.5960± 0.006 0.5932± 0.005 0.5960± 0.006

Model2 0.6244± 0.013 0.6203± 0.016 0.6157± 0.019 0.6203± 0.016
w/o polarity 0.5968± 0.018 0.5960± 0.017 0.5946± 0.017 0.5960± 0.017

Table 1: Results on the development dataset and w/o polarity means sentiment polarity representations of words
are removed.

Model Precision Recall F1 score Accuracy
Random Baseline - - - 0.527

Model1 0.5457± 0.013 0.5420± 0.013 0.5347± 0.007 0.5420± 0.013
w/o polarity 0.5381± 0.009 0.5338± 0.011 0.5277± 0.010 0.5338± 0.011

Model2 0.5392± 0.019 0.5338± 0.021 0.5263± 0.028 0.5338± 0.021
w/o polarity 0.5340± 0.016 0.5285± 0.018 0.5227± 0.022 0.5285± 0.018
Ensemble 0.5806 0.5811 0.5805 0.5811

Table 2: Results on the test dataset and w/o polarity means sentiment polarity representations of words are
removed.

that the semantic composition of the reason and
the warrant should be close to the semantic rep-
resentation of the corresponding claim. We pro-
posed two neural networks based models: a com-
petitive model that knows two warrant candidates
and an isolation model that only considers one
candidate for classification. In particular, we in-
corporated sentiment polarity of words into the
models. The experimental results demonstrate that
incorporating sentiment polarity of words always
improves the performance. The competitive mod-
el is slightly better than the isolation model. All
proposed models outperform the random baseline
by a large margin.

Acknowledgements

The research work is funded by the National Nat-
ural Science Foundation of China (No.61402304),
Beijing Municipal Education Commission (K-
M201610028015, Connotation Development) and
Beijing Advanced Innovation Center for Imaging
Technology.

References
Ivan Habernal, Henning Wachsmuth, Iryna Gurevych,

and Benno Stein. 2017. The argument reasoning
comprehension task.

Minqing Hu and Bing Liu. 2004. Mining and summa-
rizing customer reviews. In Tenth ACM SIGKDD
International Conference on Knowledge Discovery

and Data Mining, Seattle, Washington, Usa, August,
pages 168–177.

Eric H. Huang, Richard Socher, Christopher D. Man-
ning, and Andrew Y. Ng. 2012. Improving word
representations via global context and multiple word
prototypes. In Meeting of the Association for Com-
putational Linguistics: Long Papers, pages 873–
882.

Yoon Kim. 2014. Convolutional neural networks for
sentence classification. In Proceedings of the 2014
Conference on Empirical Methods in Natural Lan-
guage Processing (EMNLP), pages 1746–1751.

Bill Maccartney and Christopher D. Manning. 2008.
Modeling semantic containment and exclusion in
natural language inference. In International Confer-
ence on Computational Linguistics, pages 521–528.

Vinod Nair and Geoffrey E. Hinton. 2010. Rectified
linear units improve restricted boltzmann machines.
In International Conference on International Con-
ference on Machine Learning, pages 807–814.

Susan E. Newman and Catherine C. Marshall. 1992.
Pushing toulmin too far: Learning from an argument
representation scheme. Xerox Parc Tech Rpt Ssl.

M. Pal. 2005. Random forest classifier for remote sens-
ing classification. International Journal of Remote
Sensing, 26(1):217–222.

1145


