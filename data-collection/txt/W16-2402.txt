



















































Adaptive Importance Sampling from Finite State Automata


Proceedings of the ACL Workshop on Statistical NLP and Weighted Automata, pages 11–20,
Berlin, Germany, August 12, 2016. c©2016 Association for Computational Linguistics

Adaptive Importance Sampling from Probabilistic Tree Automata

Christoph Teichmann
University of Potsdam

Kasimir Wansing
Leipzig University

{chriteich|akoller}@uni-potsdam.de
kasimir.wansing@uni-leipzig.de

Alexander Koller
University of Potsdam

Abstract

We present a general importance sampling
technique for approximating expected val-
ues based on samples from probabilistic fi-
nite tree automata. The algorithm uses the
samples it produces to adapt rule probabil-
ities for the automaton in order to improve
sample quality.

1 Introduction

Natural language processing (NLP) often re-
quires the computation of expected values
EG(T ) [f(T )] =

∑
t∈LA G(T = t)f(t) where

the random variable T takes values from the lan-
guage LA of a probabilistic regular tree automaton
(pRTA)A, f measures a quantity of interest andG
is a probability distribution on LA. A tree automa-
ton provides a natural generalization of acyclic hy-
pergraphs and latent variable grammars that are
often used in natural language processing to ex-
press e.g. a parse chart or all the ways a word
could be decomposed into morphemes. For dif-
ferent choices of G and f we obtain e.g. (Li and
Eisner, 2009):

Feature Expectations for conditional random
fields.

Kullback-Leibler Divergence to compare the
predictions of different probability models.

Expected Loss for minimum risk decoding.

Log-Likelihood for predicting the next word in
language models.

Gradients of those quantities for optimization.

Exact computation of these values is feasible
if LA is small or additional assumptions can be
made, e.g. if the expected value is defined via

semiring operations on the automaton defining LA
(Li and Eisner, 2009).

Li and Eisner (2009) give an exact semiring so-
lution if two key assumptions can be made. First
the definition of the probability G(T ) must de-
compose into smaller derivation steps along the
rules of A. Second the number of rules of A can-
not be too large, as they must all be visited. The
first assumption is violated when e.g. non-local
features (Huang, 2008) are used to define prob-
abilities or when probabilities are defined by re-
current neural nets that use hidden states derived
from whole subtrees (Socher et al., 2013; Dyer
et al., 2016). The second assumption is violated
when e.g. tree automata are used to represent parse
charts for combinatorially complex objects like in
graph parsing (Groschwitz et al., 2015).

When semiring techniques are not applicable, it
is necessary to use approximation techniques. One
popular technique is the use of Monte Carlo meth-
ods, i.e. sampling. It is often based on Markov
Chain Monte Carlo (Gamerman and Lopes, 2006)
or Particle Monte Carlo (Cappé et al., 2007) ap-
proaches and requires minimal knowledge about
the expected value being approximated. In this
work we develop an importance sampler based on
pRTAs which can be used to approximate expected
values in settings where exact solutions are infea-
sible. One can efficiently sample from a pRTA
making it a suitable tool for generating proposals
for importance sampling, as we show in Section
3.2.

Good performance of importance sampling re-
quires choosing rule probabilities for the pRTA
to closely approximate the target distribution.
One can attempt to derive rule probabilities that
achieve this by analyzing the target distribution a
priori or by using a proposal that is known to be
a good fit (Dyer et al., 2016). We present a tech-
nique for self-adaption in Section 4 that allows the

11



sampler to learn a good proposal distribution on
its own. Following recent advances in adaptive
importance sampling (Ryu and Boyd, 2014; Douc
et al., 2007) our technique picks the best possible
rule probabilities for the pRTA according to an ap-
propriate quality measure.

Both the generation of proposals from a pRTA
and the adaption procedure we propose allow for a
lazy implementation that only needs to visit states
seen when drawing a sample. Therefore our algo-
rithm can deal with very large automata.

2 Tree Automata

A signature Σ is a set of functors, for which any
l ∈ Σ is assigned a positive integer as an arity
denoted by rank (l). A well formed tree over Σ
has the form l(t1, . . . , trank(l)) where l ∈ Σ and
each ti is also a well formed tree.

2.1 Regular Tree Automata
A probabilistic regular tree automaton (pRTA)
(Comon et al., 2007) is a tuple A =
〈Σ, Q,R, qS , θ〉 with Σ a signature, Q a finite
set of states and qS ∈ Q the start state. R
is a finite set of rules that have the form q0 →
l(q1, . . . , qrank(l)) with each qi taken from Q and
l from Σ.

The rules in R are mapped one to one to inte-
gers in [1, |R|] so that we can say, e.g. ri is the
i-th rule. θ ∈ R|R| is a vector of parameters for
the rules and θi is the parameter for ri. We call the
values of θ parameters because we use them in-
directly to derive probabilities. By using a vector
θ and defining probabilities indirectly, we deviate
from standard notation for pRTA. We do this in or-
der to more easily express the optimization prob-
lem we will later have to solve during adaption.
For a rule r = q → l(. . . ) we call lhs(r) = q
the left hand side of r and use sis (r) to denote the
set of rules with the same left hand side as r. For
pRTAA with parameters θ we define the probabil-
ity P qA(R = ri) of using a rule ri given a state q as
0 if lhs(ri) 6= q and otherwise as:

eθi∑
rj∈sis(r) e

θj
(1)

The probabilities sum to 1 for q if there is at
least one rule r with lhs(r) = q and therefore θ
can take any value in R|R|. We shorten P (X =
x1, . . . |Y1 = y1, . . . ) to P (x1, . . . |y1, . . . ) if ran-
dom variables are clear from the context.

As an example for a pRTA we will use Aex =
〈{l, l′}, {q0}, {r1 = q0 → l(q0, q0), r2 = q0 →
l′()}, q0, 〈1, 2〉〉. By our definition P q0Aex(r1) =
e1

e1+e2
.

2.2 Derivations

At the core of our paper are derivations for a
pRTA A, which are easily sampled. They are
well formed trees for a signature ΣR that is de-
rived by using the rules of A as functors with
rank (q0 → l(q1, . . . )) = rank (l). A derivation
d for A must have a rule r with lhs(r) = qS as
its root functor and any node in d with a functor
of the form q0 → l(q1, . . . , qn), must have an i-th
child with a root functor of the form qi → l′(. . . ).
For Aex one derivation is dex = r1(r2, r2).

We say a derivation maps to a tree t, denoted by
m(d) = t, if replacing all the rules in d by their
functor produces t. For Aex we have m(dex) =
l(l′, l′). The set of derivations from a pRTA A that
map to a tree t is denoted as ∆tA.

The language of A is LA = {t|∆tA 6= ∅}.
If |∆tA| ≤ 1 for every t then A is said to be
unambiguous. Assuming unambiguous automata
simplifies most of our discussion and we do so
throughout. Therefore we can denote the single
derivation for a tree t as dt. Many tree automata
used in NLP, e.g. parse charts, are unambiguous.
An extension to the ambiguous case is straightfor-
ward but laborious and is left for future work.

We denote by R(d) the multi-set of all the rules
that occur in the derivation d, e.g. R(dex) =
{r1, r2, r2}. For A = 〈Σ, Q,R, qS , θ〉 we define
the joint probability PA(T = t,D = d) of a
derivation d and a tree t as 0 if m(d) 6= t and
otherwise as:

∏
r∈R(d)

P
lhs(r)
A (r) (2)

Lack of ambiguity implies PA(t) = PA(t, dt).

2.3 Sampling Derivations

For almost all Monte Carlo techniques it is of great
importance that one can efficiently generate pro-
posals and compute their probability. This is very
easy for pRTAs which is why we use them here.
One can draw a sample 〈t, d〉 from PA(T,D) if
it is possible to sample rules given their left hand
state. Begin with a tree consisting of only the
start state qS . Until we have a derivation we re-
place the leftmost state ql in our tree with a rule

12



ql → l(q1, . . . , qn) drawn according to P qlA (R)
and make q1, . . . , qn its children. This produces
a derivation d and tree t given by m(d). Note
that the probability of generating any derivation-
tree pair 〈t, d〉 this way is given by the multipli-
cation of rule probabilities matching the definition
of PA(T,D). For our case this means that we can
also sample PA(t).

The sampling of derivations can fail in three
ways. First, there may be states for which there are
no rules to expand them. The sampling procedure
we have outlined is undefined when such a state
is reached. Secondly, it is possible that a pRTA is
deficient1 (Nederhof and Satta, 2006). This means
that there is a non-zero probability that the sam-
pling procedure we have outlined keeps expand-
ing without ever stopping. Finally the number of
rules for a single state might be so large that it is
impossible to iterate over them efficiently to select
one for a sample. The techniques for solving these
problems are outside the scope of this paper and
we therefore make three additional assumptions in
addition to our condition that a pRTA A is unam-
biguous:

1. For every state q inA there is at least one rule
r with lhs(r) = q.

2. A is cycle free, i.e. no state can be expanded
to reach the same state. This restricts us to
finite tree languages and removes the possi-
bility of deficiency.

3. For each state q inAwe can efficiently list all
the rules r with lhs(r).

3 Importance Sampling

3.1 Problem Setting
The formulation for the problem we want to solve
is as follows. We have access to some pRTAA, the
language LA of which is the domain over which
we are trying to compute an expected value. We
are also given a probability function G(T ) defined
over LA and some function f , which measures
some quantity of interest, e.g. the number of times
a certain functor occurs as a leaf in a tree, whether
a constituent appears in a parse tree or the count
of a feature used for training a model. We want
to compute the value of EG(T ) [f(T )]. Note that
many problem domains in NLP can be expressed
as the language of a pRTA as they can express

1Also called inconsistent.

charts that occur e.g. in context free parsing and
also graph parsing (Groschwitz et al., 2015).

Li and Eisner (2009) give an algorithm for this
problem that computes the expected frequency of
seeing every rule in LA under G and then exactly
computes EG(T ) [f(T )] rule by rule. The algo-
rithm they present is not applicable if G does not
decompose according to the rules of the A or if A
is too large to compute expectations efficiently. It
can also be the case that we cannot compute G(T )
and instead have to use a proportional function
g(t) with G(t) = g(t)Z where Z is some normal-
ization constant. This is the case if we are using
e.g. a discriminate model that defines probabilities
based on global normalization such as Conditional
Random Fields (Lafferty et al., 2001). It also hap-
pens whenever we are only interested in the prob-
ability of a tree given an observation, e.g. when
we want to know the probability of a parse tree
given an observed sentence for EM training. If g
is defined using non-local features (Huang, 2008)
or using e.g. recurrent neural network structures
(Dyer et al., 2016; Vinyals et al., 2014), then it is
often infeasible to compute either Z or G.

The algorithm by Li and Eisner (2009) also re-
quires f to decompose with the rules of A, which
may not be the case if we are computing e.g. com-
plex losses in loss aware training (Gimpel and
Smith, 2010).

There are therefore situations in which we need
a sampling algorithm that uses A to generate an
approximation for E g(T )

Z

[f(T )] using only evalu-
ations of g and f on single trees. This algorithm
should work without computing Z or visiting all
rules of A.

3.2 Solution with Importance Sampling

Importance sampling (Robert and Casella, 2004)
is a well known technique based on the following
fundamental equalities that hold for any set L and
auxiliary probability distribution P (T ):2

EG(T ) [f(T )] =
∑
t∈L

G(t)f(t)
P (t)

P (t) (3)

= EP (T )
[
G(T )f(T )
P (T )

]
(4)

On this basis we can approximate any expected
value we are interested in by generating indepen-
dent samples s = t1, . . . , tn from P , defining:

2We assume P (t) 6= 0 for all t ∈ L

13



S〈t1,...,tn〉 [h(ti)] =
1
n

n∑
i=1

h(ti) (5)

and using Ss
[
G(ti)f(ti)
P (ti)

]
as our approximation.

Under mild conditions the law of large numbers
tells us that this approximation will almost surely
become arbitrarily close to the correct value as the
number of samples increases.

As stated before, we might be unable to com-
pute the normalizer Z to derive G from g. We can
also estimate Z from samples. Note that:

Z =
∑
t∈L

g(t)
P (t)

P (t) (6)

= EP (T )
[
g(T )
P (t)

]
(7)

and we can estimate Z in a similar way to how
we estimate EG(T ) [f(T )]. In practice Z is esti-
mated from the same sample s used to compute
Ss
[
G(ti)f(ti)
P (ti)

]
. For a sample s = t1, . . . , tn from

P we therefore approximate EG(T ) [f(T )] through
the self normalized importance sampling estimate
(Cappé et al., 2004) defined by :

Snorms
[
g(ti)f(ti)
P (ti)

]
=

n∑
i=1

f(ti)g(ti)
∑n

j=0
P (tj)
g(tj)

P (t)

(8)

If Pn is the probability of generating a sample
s = t1, . . . , tn through independent draws from
P then EPn(S)

[
SnormS

[
g(ti)f(ti)
PA(ti)

]]
may not equal

EG(T ) [f(T )] as nested estimates can create sys-
tematic bias. But the law of large numbers ensures
that Z converges to its true value with increasing
n and the self normalized importance sampling es-
timate converges on EG(T ) [f(T )].

If we want to use importance sampling over a
set of trees then we need a way to specify P that
allows us easy sampling and evaluation of P (t)
for any tree t. As we saw in Section 2 pRTAs meet
these requirements. Therefore we will usePA with
respect to some unambiguous pRTA A as our pro-
posal distribution. It must be true that L ⊆ LA and
such an automaton is usually given by the problem
setting, e.g. one would directly use a parse chart
when computing expected values over parses. We
draw samples from PA and compute an approxi-
mation according to equation 8. Note that this al-
gorithm does not require us to know much about

f or g, we only have be be able to evaluate them
both on the trees that occur in our sample, which
will usually cover a small fraction of LA. Most
importantly g and f do not need to decompose for
any sub-parts of t, as is often required in dynamic
programming based techniques.

While the technique we have outline works no
matter how we set rule probabilities for A, the
term:

G(ti)f(ti)
PA(ti)

(9)

may be become comparatively small when
PA(ti) >> G(ti) so ti contributes almost nothing
to the estimate. It may also become relatively large
if PA(ti) << G(ti) which can lead to a single ti
“dominating” the sample, again allowing other tj
almost no contribution. In both cases we will have
to draw much more samples to obtain a good es-
timate (Robert and Casella, 2004). Therefore we
would prefer PA to not be too different from G.
Choosing rule weights to set PA to be close to G
through analysis of G may be difficult and would
have to be solved for every new problem instance.
Therefore we develop an algorithm that automati-
cally adapts PA to improve sample quality.

4 Proposal Optimization

In this section we will discuss how to learn the
auxiliary distribution PA. We will use one out of a
familyAθ of pFTAs with the form 〈Σ, Q,R, qS , θ〉
for which Σ, Q, R and qS are fixed and we only
adjust θ. We write θ(i) with i ∈ N to denote a
fixed choice for the parameters. Aθ(i) denotes the
member of the family with θ = θ(i). As in other
recent research on adaptive importance sampling
(Lian, 2011; Douc et al., 2007; Gu et al., 2015)
we try to find a θ(i) that minimizes a measure for
how bad the fit between PA

θ(i)
and G is. A natural

measure from information theory is the Kullback-
Leibler Divergence between PAθ and G:

DKL(G||PAθ) = EG(T )
[

log
(
G(T )
PAθ(T )

)]
(10)

If we subtract G(t)log (G(t)) – which does not
vary with θ – and replace G(t) = g(t)Z with just
g(t) we do not change the values of θ for which
minima are obtained. A simpler function with the
same minima would therefore be:

14



o (θ) =
∑
t∈LA

−g(t)
 ∑
ri∈R(dt)

θi − log
 ∑
rj∈sis(ri)

eθj

+ ‖θ‖2
2λ

(11)

∂o
(
θ(n)

)
∂θk

=
∑
t∈LA

−g(t)
 ∑
ri∈R(dt)

1(i = k)− P lhs(ri)A
θ(n)

(rk)

+ θ(n)k
λ

(12)

o〈θ(n),k〉(t) = −g(t)
 ∑
ri∈R(dt)

1(i = k)− P lhs(ri)A
θ(n)

(rk)

 (13)

∑
t∈LA

−g(t) (log (PAθ(t))) (14)

Note that this function is no longer dependent
on the complicated normalization constant Z and
we can therefore ignore Z for our optimization al-
gorithm. We will attempt to find the optimal θ it-
eratively. To ensure that any estimate PA

θ(i)
as-

signs nonzero weight to any tree in LA we add a
regularization term to our objective. As a result
we are going to attempt to find parameters θ that
minimize the objective function given by (11). λ
is a configuration parameter used to trade off be-
tween fitting G and spreading out rule probabili-
ties evenly. Note that unambiguous automata are
important to derive this convex objective function.

4.1 Optimization

Like Ryu and Boyd (2014) we use stochastic gra-
dient descent (SGD) (Bottou, 1998). In SGD we
attempt to find a minimum for o (θ) by generat-
ing a series θ(1), . . . , θ(m) of approximations to a
stationary point according to:

θ(n+1) = θ(n) −
(
α(n) �∇o (θ(n))) (15)

Here ∇o (θ(n)) is an approximation of the gra-
dient of o (θ) at the point θ(n), α(n) is a vector of
pre-set learning rates and � denotes element-wise
multiplication of the two arguments to the oper-
ator. If we can efficiently obtain an estimate for
∇o (θ(n)) then we can run SGD for a number of it-
erations and optimize θ. Standard derivative rules
show that the kth dimension of ∇o (θ(n)) takes
the form given by (12), where 1(x) is 1 if x is
true and 0 otherwise. Note that, if we ignore the
contribution of the regularization term, the gradi-
ent becomes smaller the more the probability of
a rule given its left hand side matches the same

Given G,f , Aθ and population size ps

1. set θ(1) = 0

2. for n ∈ [1,m] do:
(a) generate a sample of trees

s(n) = tn1 , . . . , t
n
ps from PAθ(n)

(b) estimate gradient∇o (θ(n))
k

as

Ss(n)

[
o〈θ(n),k〉(ti)
PA

θ(n)
(ti)

]
+
θ
(n)
k

λ

(c) generate learning rates α(n)

(d) set θ(n+1) according to (15)

3. compute an importance estimate for
EG(T ) [f(T )].

Algorithm 1: The adaptive importance sampling
algorithm for pRTAs.

value according to G(T ). Intuitively this means
that the rule weights adapt to match the expected
frequency of a rule given its parent which is rem-
iniscent of the use of expected rule counts in the
technique of (Li and Eisner, 2009).

The gradient is a sum over a fairly simple func-
tion of trees and we can denote the “contribution”
from a single tree t to a gradient dimension k by
o〈θ(n),k〉(t) given by (13). We therefore use the it-
erative procedure given in Figure 1 for to chose a
good Aθ(n) and produce an estimate.

For the last step in our algorithm we can gen-
erate an additional final sample from Aθ(m) . A
more efficient use of computation resources is to
use all the samples s(n) to create m importance
sampling estimates for EG(T ) [f(T )] and then take
a weighted average. The use of a weighed aver-
age is due to the fact that early values for θ(n) are

15



likely to be of poorer quality than the final ones
and there might be less merit to their contribution.
The number of iteration steps m, the number ps
of samples drawn in each iteration and the learn-
ing rates α(n) must be specified externally. The
parameters that need to be configured are usually
much easier to set than values for θ, e.g. ps and m
should simply be set as large as computationally
feasible for optimal performance.

At first it may seem as if Step 2d requires us to
adjust all parameters of the automaton at the same
time. This would make it expensive to apply our
algorithm to large automata. However we can add
the gradient contribution of the regularization in a
lazy fashion as in Carpenter (2008). This means
that we only update rules seen in the sample. The
regularization also forces parameters that have not
been updated for a while towards 0, allowing us
to remove these from explicit storage. As a result
we can sample with very large pRTAs as proposal
distributions.

4.2 Convergence

Before we give evaluation results we should make
some statements on the convergence behavior of
our algorithm. By the law of large numbers we
can state:

Theorem 1. In Algorithm 1 we almost surely

have for each n: lim
ps→∞ S

norm
s(n)

[
g(ti)f(ti)
PA

θ(n)

]
=

EG(T ) [f(T )].
Estimates will converge to EG(T ) [f(T )] as

stated by Theorem 1 even if SGD did not improve
the fit between PAθ and G. But it would be reas-
suring if we could show that the θ(n) tend towards
a minimum for o (θ) as n increases towards infin-
ity. This can be shown by standard convergence
conditions for SGD. The objective function given
by equation 11 is actually strictly convex. This is
the case because the square norm is strictly convex
and a logarithm over a sum of exponentials is con-
vex. All other terms in the formula are either con-
stant or linear in θ and therefore also convex. The
following conditions ensure convergence to an op-
timal setting of θ (for details see Bottou (1998)):

1. For every dimension k and sample size ps:

EP ps
Aθ

(n)
(S)

[
Ss(n)

[
o〈θ(n),k〉(ti)
PA

θ(n)
(ti)

]]

equals ∇o (θ(n))
k

2. o (θ) is bounded from below.

3. For every dimension k it is true that α(n)k >
0 for all n and

∑∞
n=0(α

(n)
k )

2 < ∞,∑∞
n=0 α

(n)
k =∞.

4. The second moment of our estimate for
∇o (θ(n)) is bounded by a linear function of
‖θ(n)‖2.

Condition 3 depends only on the α which may
be chosen by the user to be i.e. 1n to meet the re-
quirement. Condition 2 is true as the Kullback-
Leibler Divergence is never smaller than 0 and the
same is true for the Euclidean norm. Condition 4
can be verified by inspecting the gradient and not-
ing that LA is finite by the assumptions we made
in Section 2 and that all the terms in the sum are
bounded by either 1,-1 or θ(n). Condition 1 can be
verified by simple algebra from our exposition of
importance sampling. As a result we can state:

Theorem 2. For Algorithm 1 we almost surely
have:

lim
n→∞ θ

(n) = argmin
θ

o (θ)

This means that the rule probabilities converge
towards values that are optimal as measured by the
Kullback-Leibler Divergence.

5 Evaluation

We created an implementation of our ap-
proach in order to investigate its be-
havior. It, along with all evaluation
data and documentation, is available via
https://bitbucket.org/tclup/alto/
wiki/AdaptiveImportanceSampling
as part of Alto, an Interpreted Regular Tree
Grammar (Koller and Kuhlmann, 2011) toolkit.
We encourage readers to use the sampler in their
own experiments. In this section we will evaluate
the ability of our algorithm to learn good proposal
weights in an artificial data experiment that
specifically focuses on this aspect. This will show
whether the theoretical guarantees correspond to
practical benefits. In future work we will evaluate
the algorithm in End-to-End NLP experiments to
see how it interacts with a larger tool chain.

5.1 Evaluation Problem

We tested our approach by computing a constant
expected value EG(T ) [1]. The ratio

1∗G(T )
PAθ (T )

be-

16



γ=0.5
 l=20 
 k=500

E
st

im
at

ed
 V

al
ue

0

0,5

1

1,5

2

Estimation Round
0 20 40 60 80 100

γ=0.1
 l=20 
 k=500

E
st

im
at

ed
 V

al
ue

0,25

0,5

1

1,5

1,75

2

Estimation Round
0 20 40 60 80 100

γ=0.5
 l=30 
 k=500

E
st

im
at

ed
 V

al
ue

0

0,5

1

1,5

2

Estimation Round
0 20 40 60 80 100

γ=0.5
 l=20 
 k=2000

E
st

im
at

ed
 V

al
ue

0

0,5

1

1,5

2

Estimation Round
0 20 40 60 80 100

γ=0.1
 l=20 
 k=2000

E
st

im
at

ed
 V

al
ue

0

0,5

1

1,5

2

Estimation Round
0 20 40 60 80 100

γ=0.5
 l=30 
 k=2000

E
st

im
at

ed
 V

al
ue

0

0,5

1

1,5

2

Estimation Round
0 20 40 60 80 100

Figure 1: Convergence plots for different problem settings.

comes 1 if PAθ perfectly matches G and a sin-
gle sample would suffice for an exact estimate.
Therefore any error in the estimate is directly due
to a mismatch between G and PAθ . We defined
G through a pRTA with the same structure as the
automaton we are sampling from. Therefore it is
possible for PAθ to exactly match G. If we were
told the correct parameters for θ then we would
obtain a perfect sample in the first step and a good
sample should be generated if the SGD steps suc-
cessfully move θ from 0 to the values defining G.

The parameters for G were randomly chosen
so that the resulting probabilities for rules given
their left hand sides where distributed according
to a symmetric Dirichlet Distribution. This is a
good model for probability distributions that de-
scribe natural language processes (Goldwater et
al., 2011). The symmetric Dirichlet Distribution
is parametrized by a concentration parameter γ.
The rule weights become more likely to be con-
centrated on a few of the rules for each left hand
side as γ goes toward 0. Therefore many trees will
be improbable according to G.

We obtain a complete evaluation problem by
giving the structure of the automata used. As
stated, we use the same automaton to specify G
and Aθ save for θ which we initialize to be 0

for all entries. We chose a structure similar to
a CKY parse chart for the underlying rules and
states (Younger, 1967). Given a length parameter
l ∈ N we added to the automaton all states of the
form 〈i, j〉with 0 ≤ i < j ≤ l. The automaton has
all rules of the form 〈i, i + 1〉 → i() and all rules
〈i, j〉 → ∗(〈i, h〉, 〈h, j〉) with 0 ≤ i < h < j ≤ l
and ∗ an arbitrary functor. Therefore the parame-
ters for our evaluation problems are l and γ.

A central concern in making SGD based algo-
rithms efficient is the choice of α(n). We use an
adaptive strategy for configuring the learning rates
(Duchi et al., 2010; Schaul et al., 2012). These
schemes usually have convergence proofs that re-
quire much stricter conditions than “vanilla” SGD
and we therefore cannot claim that θ(n) will con-
verge with these approaches, but in practice they
often perform well. Concretely, we use the tech-
nique for setting learning rates that was introduced
by Duchi et al. (2010) which uses α′ and divides
it by the sum of all the gradient estimates seen
so far to obtain the learning rate for each dimen-
sion. α′ is fixed ahead of time – we chose 0.5 and
we found that values between 1.0 and 0.1 could
be used interchangeably to obtain the best perfor-
mance. We set the regularization parameter λ in
our objective o (θ) to 100 for comparatively weak

17



regularization.

5.2 Evaluation Results

Figure 1 plots the convergence behavior of the al-
gorithm for different problem settings with fixed
parameters l and γ. The upper row shows experi-
ments with 500 samples and the lower uses 2000
samples. Each plot gives results for a single ex-
ample automaton. Experiments with different au-
tomata showed the same trends. Each graph shows
100 repetitions of the experiment as box plots.
Different random number seed were used for the
repetitions. The box plots show how well the
expected value was approximated in each of the
m = 100 rounds of adaption. The value in each
round/repetition n is computed only for the sam-
ple s(n) drawn in that round/repetition. Whiskers
indicate the 9th and 91th percentile value.

Note the tendency towards underestimation in
all experiments. This indicates that the algorithm
proposes many trees with low probability under G
and has to adapt in order to find more likely trees.

For l = 20 and γ = 0.5 and k = 500 the
sampler converges in 40 iterations. Note that per-
formance after four steps with k = 500 is better
than with one step of k = 2000. This shows that
adapting parameters provides a benefit over sim-
ply increasing the sample size. With γ = 0.1,
l = 20 and k = 500 the samples improve much
more slowly. This is to be expected as there are
more than 1 billion trees in LA and only very few
of them will be assigned large probabilities by the
more peaky rule probabilities. Therefore the algo-
rithm has to randomly find these few trees to pro-
duce a good estimate both for the evaluated value
and for the gradient used in adaptation. When
k = 2000 there are faster improvements as the
algorithm has a better gradient estimate.

Convergence is also slower when l is increased
to 30 as the number of trees to consider rises and
the amount of parameters in θ grows in the order
ofO(l3). Convergence speed again increases if we
set the sample size k = 2000.

Overall we can see that the adaption steps im-
prove the quality of our importance sampler and
lead to a simple, yet versatile algorithm for ap-
proximating expected values.

6 Related Work

Sampling in NLP is most often implemented via
Markov Chain Monte Carlo methods that either

have to move through the relevant domain with
small steps (Chung et al., 2013) or use a good pro-
posal distribution in order to generate new trees
(Johnson et al., 2007). Because it is difficult
to adapt Markov Chain Monte Carlo algorithms
(Liang et al., 2010) the proposal distribution for
generating new trees needs to be specified by the
user in advance. Particle Monte Carlo Methods
(Cappé et al., 2007; Börschinger et al., 2012) are
related to importance sampling and would allow
for more adaptive proposals, but have not been
used this way for the structured outputs used in
natural language processing. The idea of using
adaptive versions of importance sampling has be-
come much more prevalent in the last years (Douc
et al., 2007; Lian, 2011; Ryu and Boyd, 2014).
Ryu and Boyd (2014) discussed the use of SGD to
optimize a convex function in order to improve an
importance sampler. They discussed applications
where the proposal distribution is from the expo-
nential family of distributions and used the vari-
ance of their sampler as the optimization objective.
Douc et al. (2007) and Lian (2011) used an ob-
jective based on Kullback-Leibler divergence, but
they used techniques other than SGD to optimize
the objective. It is also possible to use more com-
plex models to generate proposals (Gu et al., 2015)
at the price of less efficient training.

7 Conclusion

We have presented an adaptive importance sam-
pler that can be used to approximate expected val-
ues taken over the languages of probabilistic reg-
ular tree automata. These values play a central
role in many natural language processing appli-
cations and cannot always be computed analyti-
cally. Our sampler adapts itself for improved per-
formance and only requires the ability to evaluate
all involved functions on single trees. To achieve
adaptiveness, we have introduced a convex objec-
tive function which does not depend on a complex
normalization term. We hope that this simple tech-
nique will allow researchers to use more complex
models in their research.

Acknowledgments. We thank the anonymous
reviewers for their comments. We thank the work-
shop organizers for their patients in waiting for our
final version. We received valuable feedback from
Jonas Groschwitz and Martn Villalba. This work
was supported by the DFG grant KO 2916/2-1.

18



References

Benjamin Börschinger, Katherine Demuth, and Mark
Johnson. 2012. Studying the effect of input size
for Bayesian Word Segmentation on the Providence
Corpus. In Proceedings of COLING 2012, pages
325–340.

Léon Bottou. 1998. Online Algorithms and stochas-
tic approximations. In David Saad, editor, Online
Learning in Neural Networks, pages 9–13. Cam-
bridge University Press. revised, oct 2012.

Olivier Cappé, Arnaud Guillin, Jean-Michel Marin,
and Christian Robert. 2004. Population Monte
Carlo. Journal of Computational and Graphical
Statistics, 13(4):907–929.

Olivier Cappé, Simon J. Godsill, and Eric Moulines.
2007. An overview of existing methods and recent
advances in Sequential Monte Carlo. In Proceed-
ings of the IEEE, volume 95, pages 899–924.

Bob Carpenter. 2008. Lazy sparse Stochastic Gradient
Descent for Regularized Multinomial Logistic Re-
gression. Technical report, Alias-i, Inc.

Tagyoung Chung, Licheng Fang, Daniel Gildea, and
Daniel Štefankovič. 2013. Sampling tree frag-
ments from forests. Computational Linguistics,
40(1):203–229.

Hubert Comon, Max Dauchet, Rémi Gilleron, Flo-
rent Jacquemard, Denis Lugiez, Sophie Tison, Marc
Tommasi, and Christof Löding. 2007. Tree Au-
tomata techniques and applications. published on-
line - http://tata.gforge.inria.fr/.

Randal Douc, Arnaud Guillin, Jean-Michel Marin, and
Christian Robert. 2007. Convergence of adaptive
mixtures of Importance Sampling schemes. The An-
nals of Statistics, 35(1):420–448.

John Duchi, Elad Hazan, and Yoram Singer. 2010.
Adaptive Subgradient methods for Online Learning
and stochastic optimization. In COLT 2010 - The
23rd Conference on Learning Theory, pages 257–
269.

Chris Dyer, Adhiguna Kuncoro, Miguel Ballesteros,
and Noah A. Smith. 2016. Recurrent Neural Net-
work grammars. In Proceedings of the 2016 Con-
ference of the North American Chapter of the Asso-
ciation for Computational Linguistics: Human Lan-
guage Technologies, pages 199–209.

Dani Gamerman and Hedibert F. Lopes. 2006. Markov
Chain Monte Carlo - Stochastic Simulation for
Bayesian Inference. Chapman & Hall/CRC.

Kevin Gimpel and Noah A. Smith. 2010. Softmax-
Margin Training for structured Log-Linear Models.
Technical report, Carnegie Mellon University.

Sharon Goldwater, Thomas L. Griffiths, and Mark
Johnson. 2011. Producing Power-Law Distribu-
tions and damping Word Frequencies with two-stage
language models. Journal of Machine Learning Re-
search, 12:2335–2382.

Jonas Groschwitz, Alexander Koller, and Christoph Te-
ichmann. 2015. Graph parsing with S-graph Gram-
mars. In Proceedings of the 53rd Annual Meeting of
the Association for Computational Linguistics and
the 7th International Joint Conference on Natural
Language Processing, pages 1481–1490.

Shixiang Gu, Zoubin Ghahramani, and Richard E.
Turner. 2015. Neural adaptive Sequential Monte
Carlo. In Advances in Neural Information Process-
ing Systems 28, pages 2629–2637.

Liang Huang. 2008. Forest reranking: Discriminative
parsing with non-local features. In Proceedings of
ACL-08: HLT, pages 586–594.

Mark Johnson, Thomas L. Griffiths, and Sharon Gold-
water. 2007. Bayesian Inference for PCFGs
via Markov Chain Monte Carlo. In Human Lan-
guage Technology Conference of the North Amer-
ican Chapter of the Association of Computational
Linguistics, pages 139–146. The Association for
Computational Linguistics.

Alexander Koller and Marco Kuhlmann. 2011. A gen-
eralized view on parsing and translation. In Pro-
ceedings of the 12th International Conference on
Parsing Technologies, pages 2–13.

John D. Lafferty, Andrew McCallum, and Fernando
C. N. Pereira. 2001. Conditional Random Fields:
Probabilistic models for segmenting and labeling se-
quence data. In Proceedings of the Eighteenth In-
ternational Conference on Machine Learning, pages
282–289.

Zhifei Li and Jason Eisner. 2009. First- and second-
order Expectation Semirings with applications to
Minimum-Risk training on Translation Forests. In
Proceedings of the 2009 Conference on Empirical
Methods in Natural Language Processing, pages
40–51.

Heng Lian. 2011. Stochastic adaptation of Importance
Sampler. Statistics, 46(6):777–785.

Faming Liang, Chuanhai Liu, and Raymond Carroll.
2010. Advanced Markov Chain Monte Carlo Meth-
ods. John Wiley and Sons Ltd.

Mark-Jan Nederhof and Giorgio Satta. 2006. Estima-
tion of consistent Probabilistic Context-free Gram-
mars. In Proceedings of the Human Language Tech-
nology Conference of the North American Chapter
of the ACL, pages 343–350.

Christian P. Robert and George Casella. 2004. Monte
Carlo statistical methods. Springer, 2 edition.

19



Ernest K. Ryu and Stephen P. Boyd. 2014. Adaptive
Importance Sampling via stochastic Convex Pro-
gramming. CoRR, abs/1412.4845.

Tom Schaul, Sixin Zhang, and Yann LeCun. 2012. No
more pesky learning rates. CoRR, abs/1206.1106.

Richard Socher, John Bauer, Christopher D. Manning,
and Andrew Y. Ng. 2013. Parsing with Compo-
sitional Vector Grammars. In Proceedings of the
51st Annual Meeting of the Association for Compu-
tational Linguistics, pages 455–465.

Oriol Vinyals, Lukasz Kaiser, Terry Koo, Slav
Petrov, Ilya Sutskever, and Geoffrey E. Hinton.
2014. Grammar as a foreign language. CoRR,
abs/1412.7449.

Daniel H. Younger. 1967. Recognition and parsing
of Context-Free Languages in time n3. Information
and Control, 10:189–208.

20


