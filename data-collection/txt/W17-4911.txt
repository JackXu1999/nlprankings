



















































Stylistic Variation in Television Dialogue for Natural Language Generation


Proceedings of the Workshop on Stylistic Variation, pages 85–93
Copenhagen, Denmark, September 7–11, 2017. c©2017 Association for Computational Linguistics

Stylistic Variation in Television Dialogue for
Natural Language Generation

Grace I. Lin and Marilyn A. Walker

Natural Language and Dialogue Systems Lab
University of California, Santa Cruz
{glin5,mawwalker}@ucsc.edu

Abstract

Conversation is a critical component of
storytelling, where key information is of-
ten revealed by what/how a character says
it. We focus on the issue of character voice
and build stylistic models with linguistic
features related to natural language gen-
eration decisions. Using a dialogue cor-
pus of the television series, The Big Bang
Theory, we apply content analysis to ex-
tract relevant linguistic features to build
character-based stylistic models, and we
test the model-fit through an user percep-
tual experiment with Amazon’s Mechani-
cal Turk. The results are encouraging in
that human subjects tend to perceive the
generated utterances as being more simi-
lar to the character they are modeled on,
than to another random character.

1 Introduction

Conversation is an essential component of social
behavior, one of the primary means by which hu-
mans express emotions, moods, attitudes, and per-
sonality. Conversation is also critical to story-
telling, where key information is often revealed by
what a character says, how s/he says it, and how
s/he reacts to what other characters say. Here we
focus on the issue of character voice. One way to
produce believable, dramatic dialogue is to build
stylistic models with linguistic features related
to natural language generation (NLG) decisions.
Television dialogue are exemplars of many differ-
ent linguistic styles that were designed to express
dramatic characters. Thus we construct a corpus of
television character dialogue from The Big Bang
Theory (BBT) and apply content analysis and lan-
guage modeling techniques to extract relevant lin-
guistic features to build character-based stylistic

models. We test the model-fit of character models
through a generation experiment to test user per-
ceptions of characters.

Our work can be applied to storytelling appli-
cations such as video games, interactive narra-
tive, chatbots, or education systems where dia-
logue with personalities may improve user expe-
rience.

2 Related Work

Research from corpus linguistics include
Bednarek’s work on using Gilmore Girls to
compare the genre dramedy to other types
(Bednarek, 2011a), and Quaglio’s work on using
Friends with unscripted conversations (Quaglio,
2009). Other related research focuses on charac-
terization through dialogue. For example, Bubel
explored the friendship among characters in the
Sex and the City (Bubel, 2005), and Bednarek
analyzed linguistic stylistics shifts from characters
from the Gilmore Girls (Bednarek, 2011b) and
The Big Bang Theory (Bednarek, 2012).

Research from computational stylistics (or sty-
lometry) focuses on the use of quantitative meth-
ods to study writing styles to characterize authors,
which can be applied to many applications such
as classical literary text, modern forensic text, and
online reviews, just to name a few (Stamatatos,
2009). Principal component analysis is used to
analyze the variations in words, focusing on the
challenge of relating features and meanings in
text, which is not fixed depending on the context
(Schreibman et al., 2008).

There is an extensive amount of research in
story generation (narrative content), which tends
to focus on plots and character development to
achieve narrative goals. One source of creating
stories comes from crowd participants writing de-
tailed descriptions for events, going into details

85



with characters’ intentions, facial expressions, and
actions (Li et al., 2014). In addition, they used
the Google N-Gram Corpus and Project Guten-
berg to help select different types of sentences
(most/least probable, most fictional, most interest-
ing details) and different sentiments (most posi-
tive/negative). Our work is also related to charac-
ter modeling from film dialogue for NLG (Lin and
Walker, 2011; Walker et al., 2011), except that we
focused on TV series because they offered more
dialogue.

Despite overlaps, our work differs in that we:
1) extract linguistic stylistic features based on per-
sonality studies from psychology; 2) focus on fea-
tures that can be generated given our current sys-
tem; 3) find significant features and use them as
building blocks to 4) create models using tech-
niques such as standard scores and classification;
and 5) apply the models to applications such as
natural language generation.

Natural Language Generation Engine

PYPER (Bowden et al., 2016) is a spin-off imple-
mentation of PERSONAGE (Mairesse and Walker,
2007) in Python that provides new controls for
expressive NLG. It is currently part of the M2D
Monolog-to-Dialogue generation (Bowden et al.,
2016) framework, which we briefly describe the
architecture below (Figure 1).

The EST framework (Rishes et al., 2013)
produces a story annotated by SCHEHERAZADE
(Elson and McKeown, 2009) as a list of sen-
tences represented as Deep Syntactic Structures
(DsyntS). DSyntS is therefore a dependency-tree
structure with nodes containing lexical informa-
tion about words. This is the input format for
the surface realizer RealPro (). M2D converts the
story (list of DsyntS) into two-speaker dialogue by
accepting input parameters that control the alloca-
tion of content, pragmatic markers, etc.

Scheherazade
ES-Translator

M2D: Monolog 
to Dialog RealPro

Corpus of Personal Narratives/Stories

Monolog 
Input

Dialog 
Output

DsyntS Altered DsyntS

Figure 1: M2D Monolog-to-Dialogue Generation
(Bowden et al., 2016)

3 Corpus

We parsed fan-transcribed BBT scripts, seasons 1-
4 and partial season 5, to obtain scenes, speakers,
and utterances. The series centers around 5 char-
acters, 4 of them (all male) are scientists/engineers
working at Caltech, and 1 (Penny) is a waitress.
The comedy’s theme focuses on the contrast be-
tween the geekiness of the male characters and
Penny’s social skills. Two additional female char-
acters, both scientists, were introduced as love in-
terests to two main male characters, and have since
became main characters themselves.

4 Stylistic Features Extraction

After extracting dialogic utterances from tran-
scripts, we extract features reflecting particular
linguistic behaviors for each character. Table 1
describes major feature sets, which include sen-
timent polarity, dialogue act, passive voice, word
categories from LIWC (Pennebaker et al., 2001),
tag questions, etc.

Character Stylistic Models
We calculate a standard score (z-value) for each
feature to measure the differences between main
characters: Leonard, Sheldon, Penny, Howard,
Raj, Bernadette, and Amy. A better measurement
could be used due to the small population and nor-
mal assumption, however we reviewed the results
and they seem to capture enough relative differ-
ences among characters. Character models are
composed of significant features with |z| ≥ 1.
While using features with |z| ≥ 2 might be a bet-
ter choice, our NLG engine can manipulate many
features under |z| ≥ 1.

The number and examples of significant fea-
tures for each character are shown in Table 2. We
see that for |z| ≥ 1, Sheldon, Penny, Bernadette
and Amy have over 200 significant features. Shel-
don, more specifically, has close to 400 significant
features. When we narrow them down to z ≥ 2,
significant features for Bernadette and Amy de-
creased by over 85%, Leonard, Penny, Howard,
and Raj decreased by 70%, and Sheldon decreased
by 54%.

5 Generating Expressive Utterances

The workflow for generation is to 1) annotate
stories using SCHEHERAZADE; 2) use EST to
automatically translate annotated stories to deep
syntactic structures (DSyntS); 3) PYPER reads

86



Table 1: Automatically Annotated Linguistic Features for TV Dialogue
Feature Set and Description
1. Basic. Tokens per sentence, tokens per utterance, etc., plus words from different types of emotion and other psycho-
logical categories from the Nodebox English Linguistics library.
2. Sentiment Polarity. Overall polarity, polarity of sentences, etc., using SENTIWORDNET1 to calculate positive,
negative, and neutral score.
3. Dialogue Act. Train Naive Bayes classifier with NPS Chat Corpus’ 15 dialogue act types using simple features. We
also determine “First Dialogue Act”, where we look at the dialogue act of the first sentence of each turn.
4. Merge Ratio. Use regular expression to detect the merging of subject and verb of two propositions.
5. Passive Voice. Using a third party software (see text) to detect passive sentences.
6. Concession Polarity. Look for concession cues, then calculate polarity of concession portion.
7. LIWC Categories. Word categories from the Linguistic Inquiry and Word Count (LIWC) text analysis software.
8. Markers - PERSONAGE. collect words used in PERSONAGE for generation, which where selected based on psycho-
logical studies to identify pragmatic markers of personality that affect the utterance.
9. Tag Questions. Use regular expression to capture tag questions.
10. Verb Strength. Averaged sentiment values of verbs.
11. Content Words Length. Find the average length of content words.
12. Markers - Others. Inspired by PERSONAGE words. Extended set.
13. Hedges. Collect words from a list of pre-defined hedges and their categories. LACKOFF hedges.
14. Repeating Verbs. Find verbs that are repeated used in a turn.
15. BIGRAMs. Top 10 bigrams.
16. Part-of-Speech BIGRAMs. Top 10 POS bigrams.

Table 2: Number and Examples of Significant Features for The Big Bang Theory Characters
Speaker |z| ≥ 1 |z| ≥ 2 Example Features for z ≥ 1 (i.e., positive z-values only)
Leonard 172 54 words:[even if, nevertheless, whereas, even though], Dialogue Act–{Greet, Bye},

LIWC-{Causation, Impersonal Pronouns}, hedges per sentence, connect words,
concept words

Sheldon 394 180 words: [all the same, although, despite, however, nevertheless, on the other
hand, whereas, more or less, though, all, yet], passive-ratio, important words
per utt/sent, LIWC–{Inhibition, Prepositions, Number, Quantifiers}

Penny 232 68 words:[nevertheless, even if, while, even though, on the other hand, yet], connect
words, emotional words, Dialogue Act–{Greet, Bye}, swear/near swear words,
LIWC–{Adverbs, Present Tense, Dictionary Words}

Howard 133 41 words:[although, even if, whereas], LIWC–{Hear, See, Third Person Singular},
concept words, in-group words, hedges-per-sent

Raj 179 51 words:[on the other hand, however, despite, though, also, even though, but], in-
group words, LIWC–{Conjunctions, Third Person Plural, See}, hedges per sen-
tence

Bernadette 283 43 persuasive words, emotional words, conceptual words, words:[even though, yet,
while], Dialogue Act–emphasis, LIWC–{Personal Pronouns, Second person,
Auxiliary Verbs, Function Words, Past Tense}

Amy 246 43 LIWC–{Quantifiers, UniqueWords, FutureTense, Causation}, RID Emotion
words, Dialogue Act - Continuer, opinion words, words:[though, but]

and manipulates DSyntS to add expressive ele-
ments, and 4) send “expressive” DSyntS to Real-
Pro (Lavoie and Rambow, 1997) (a sentence re-
alizer) for generation. We focus on operation 3
where we use our learned character stylistic mod-
els to add expressive elements to generic sen-
tences.

5.1 Mapping Stylistic Features to NLG
Decisions

The re-written and better-controlled PYPER al-
lows for more useful mapping of character models
for NLG. For example, hedge insertion patterns
are kept in a library where new additions can be

easily added. As an example, a partial mapping
for LIWC categories are shown in Table 3. For
multiple features mapped to the same PYPER pa-
rameter, we calculate a weighted average of the
features.

5.2 Narrative Content

Our narrative content comes from fables and sto-
ries: 1 fable (The Fox and the Crow) and 6 blog
stories about garden, protest, squirrel, bug, em-
ployer, and storm (Gordon et al., 2007). We use
The Fox and the Crow fable as an example to de-
scribe our process shown in Figure 2.

Some phrases are highlighted to show how they

87



Table 3: Partial Mapping of LIWC Categories to Expressive NLG Parameters
PYPER Parameter LIWC Category PYPER Parameter LIWC Category
near-expletives swear, anger low-expletives swear, anger
emph-actually certain emph-exclamation excl
emph-really certain emph-great assent
emph-you-know filler emph-particularly certain
emph-technically certain emph-literally certain
emph-quintessential certain emph-essentially certain, i
emph-somewhat tentat emph-very certain
emph-especially certain emph-roughly tentat
in-group-marker family, friends, we, incl init-reject tentat

were annotated and translated. Many complicated
sentences have been broken down into shorter
ones. Note that some additional descriptions (ad-
jectives) were added in order to provide enough
search space for PYPER to exercise enough ex-
pressive parameters, so that characters’ personal-
ities will come through in different variations of
the story.

The final, expressive version of the story shows
different stylistic features such as converting a
statement to a question and adding character di-
alogue inspired expressions such as Typical.

6 Evaluation with User Perceptual
Experiment

We used Mechanical Turk to get user feedback
on the generated dialogue. The PYPER generated
output dialogue were post-processed to get rid of
typos and minor grammatical issues. Referring to
the MTurk survey (one HIT) in Figure 3, we first
show some information about the character in in-
terest (Sheldon, in this case), followed by two sets
of dialogue: one by Sheldon and the other by a
different random character. The worker does not
know which one was modeled by Sheldon. S/he
was asked to pick the dialogue that sounded most
similar to Sheldon, along with providing reasons.

Referring to results in Table 5, we used three
participants per pair of characters comparison per
story. The character on the left-most column in-
dicates the modeled character, and the compared-
to character was the “other random character” in
the survey. Each circle (empty or filled) indicates
a worker’s choice. A filled circle (•) means the
worker picked the “matched” generated dialogue
to the intended character, otherwise an empty cir-
cle (◦) is shown.

The probability that at least two out of three par-
ticipants agree on the right character is > 50%
(Table 4), while all three participants agree on
the right character is 31.3%, which is higher than

chance (12.5%).

Table 4: Participants Agreement
Choose the right character # HITs (out of 294) %
• • • (3 out of 3) 92 31.3
• • ◦ (2 out of 3) 57 20.4
• ◦ ◦ (1 out of 3) 122 41.5
◦ ◦ ◦ (0 out of 3) 23 7.8

Overall the 7 characters over all 7 stories were
recognized about 65.5% of the time (out of 882
ratings). Per character-wise over all 7 stories,
Penny was recognized the most with 82.5% of the
time, followed by Leonard (78.6%), Bernadette
(66.7%), Amy and Sheldon (both 61.9%), Howard
(57.9%), and finally Raj, who was recognized the
least with 49.2% of the time.

Certain character pairs were easier to dis-
tinguish than others. For example, Leonard-
Penny and vise-versa (95.2%), Sheldon-Penny and
vise-versa (85.7%, 90.5%), and Amy-Bernadette
and vise-versa (85.7%). On the other hand,
these were among the pairs harder to distinguish:
Amy-Leonard and vise-versa (47.6%, 57.1%),
Bernadette-Penny and vise-versa (33.3%, 57.1%),
and Sheldon-Howard and vise-versa (47.6%,
57.1%).

7 Character Analysis

In this section we provide some insights to the re-
sults, including with participants’ comments about
their perception of characters through the gener-
ated dialogue.

We found the following pairs of characters
most distinguishable from each other (Table 6):
Amy vs. Bernadette, Bernadette vs. Howard,
Howard, Leonard, Sheldon vs. Penny, Penny vs.
Leonard, and Raj vs. Sheldon. On the other
hand, we found the most similar pairs of characters
to be Amy-Leonard, Bernadette-Penny, Howard-
Leonard, Leonard-amy, Penny-Bernadette, Raj-

88



The Fox and the Crow Partial (Original)

A Crow was sitting on a branch of a tree with a piece of cheese in her beak when a Fox observed her and set his wits to
work to discover some way of getting the cheese.

SCHEHERAZADE & EST

The crow sat on the tall tree’s branch. The crow has elegant talons. The crow has a good beak. The crow has ugly wings.
The crow has bad eyes. The delicious cheese was in the crow’s beak. The hungry fox observed the crow. The fox has
good claws. The fox has bad teeth. The fox tried to discover for the fox to get the cheese.

PYPER: Monologue to Dialogue

Speaker 1: The crow sat on the tall tree’s branch. The crow has elegant talons, and a good beak. The crow has ugly
wings, and bad eyes.
Speaker 2: The delicious cheese was in the crow’s beak. The hungry fox observed the crow. The fox has good claws, but
bad teeth.
Speaker 1: The fox tried to discover for the fox to get the cheese. The fox came and stood under the tall tree. The fox
looked toward the crow.
Speaker 2: The fox said the fox saw the crow. The fox said the crow’s beauty was incomparable. The fox said the crow’s
plumage’s hue was exquisite.

PYPER: M2D + Stylistic Parameters

Speaker 1: She sat on the tall tree’s branch. She has elegant talons, and a good beak. She has ugly wings, and bad eyes.
Speaker 2: He has good claws, but bad teeth.
Speaker 1: What was in her beak?
Speaker 2: The delicious cheese was, i see, in her beak. Typical. Really ok, I do not remember what happened next? Do
you want to know more about that?

Figure 2: The Fox and the Crow Story Transformation

Leonard, and Sheldon-Howard/Leonard. Note that
the comparison is not symmetrical because in the
survey we gave a “known” (reference) character,
which is the first column in the table.

It is not surprising to see Penny being differ-
ent from most of the male characters, as it is the
premise of BBT. Raj is an exception, mainly due
to his lack of (expressive) dialogue, though he is
definitely different from Sheldon. It is also believ-
able that Leonard is similar to many characters, as
he is the most “normal” character out of the group.

We further explore a few characters below.

7.1 Penny

7.1.1 Perception of Penny in comparison to
Leonard (most distinguishable)

Penny is one of the best expressed character in
the experiment, missing only by one selection in
comparison to Leonard (95.2%), and missing by
two in comparison to Sheldon (90.5%). Here we
take a look at the comparison with Leonard, where
20 (out of 21) Penny-modeled generated dialogue
were rated more similar to Penny, and only 1 (out
of 21) Leonard-modeled generated dialogue were
rated more similar to Penny.

Overall, participants’ perception of Penny-
modeled generated dialogue seem to agree with
Penny’s personality, capturing her “bubbly, cheer-
fulness”, as mentioned by one worker. Some no-

table descriptions include:

- talkative, randomness, random pauses, better wording, more personality
- seek feedback from others, lots of questions, not always sure of what
she’s saying, hesitation
- good mix of colloquialisms and Penny-like filler, some brief, fairly sim-
ple statements
- stand-out word choices: magic, huh?, mhmm, let’s see, that..., the crow
needed what?, oh gosh, I mean, damn yeah

Participants perceived Leonard-modeled gen-
erated dialogue as not suitable for Penny, mostly
because of his bland language. Here are some no-
table descriptions:

- too simple, monotone, boring, direct, bare, straightforward, matter-of-
fact, boxy, bland, not enough questioning for Penny
- too much adverb usage on precision or intellect for Penny
- not like Penny to use complex words and phrases
- not like Penny to use: technically, darn
- too rude for her to use, since she wants people to like her: everybody
knows that, obviously

The MTurk worker of the one missed selection
cited Penny being a very simple speaker, implying
that her dialogue would contain brief and simple
statements. While this is true, she also uses quite
a bit of fillers and questions around her “simple”
dialogue to sound chatty.

7.1.2 Perception of Penny in comparison to
Bernadette (least distinguishable)

It is not surprising to see Penny being the least dis-
tinguishable with Bernadette (57.1%). Bernadette
was introduced in the series as Penny’s friend and
coworker working as a waitress. Her role on the
show seemed to be more similar to Penny (friendly

89



Figure 3: Amazon Mechanical Turk Survey (One HIT) Example

and sociable) than everyone else (nerdy and so-
cially awkward), despite that she became a scien-
tist eventually.

While the Bernadette-model contain chatty
word choices (similar to Penny’s), it also con-
tains “intellect” word choices. However due to
the randomness of the generated dialogue, where
not all features are expressed/activated, some di-
alogue/story might not show enough of her nerdy
side. For example, precise adverbs such as essen-
tially, particularly are more likely to be used by a

scientist/engineer (Bernadette) but not by Penny.
In terms of stories, Bug and Garden did the

best at distinguished the character pair, while Em-
ployer and Storm did the worst (none of the Penny-
modeled dialogue sounded like Penny).

7.2 Sheldon

7.2.1 Perception of Sheldon in comparison to
Penny (most distinguishable)

With Sheldon differs the most with Penny
(85.7%), we focus on comments by participants

90



Table 5: Characters and Stories MTurk Results by HITs
Each HIT had 3 participants, each indicated by a circle (◦).

A solid circle (•) indicates the worker picked the “matched” generated dialogue to the original character.
Characters are listed in alphabetical order; circles are sorted by • then ◦

blue: best result; red: worst result
Character Story #/% similar

compared-to Bug Employer FoxCrow Garden Protest Squirrel Storm (out of 21)

A
m

y

Bernadette • • • • • • • • ◦ • • • • • • • • • • ◦ ◦ 18 / 85.7
Howard • ◦ ◦ • • ◦ • • ◦ • • ◦ • • ◦ • • ◦ • ◦ ◦ 12 / 57.1
Leonard ◦ ◦ ◦ • • ◦ ◦ ◦ ◦ • • • • • ◦ • • ◦ • ◦ ◦ 10 / 47.6

Penny • ◦ ◦ • • ◦ • • ◦ • • ◦ • • ◦ • • ◦ • • ◦ 13 / 61.9
Raj • ◦ ◦ • • • ◦ ◦ ◦ • • ◦ • • ◦ • • ◦ • • ◦ 12 / 57.1

Sheldon • • ◦ • • ◦ • • • • • • • ◦ ◦ • • ◦ ◦ ◦ ◦ 13 / 61.9
# / % similar 8 / 44.4 14 / 77.8 9 / 50.0 15 / 83.3 12 / 66.7 13 / 72.2 7 / 38.9 78 / 61.9

(out of 18) (out of 126)

B
er

na
de

tt
e

Amy • • • • • • • • • • • ◦ • • ◦ • • ◦ • • • 18 / 85.7
Howard • • • • • • • • ◦ • • • • • ◦ • • ◦ • • • 18 / 85.7
Leonard • • ◦ • • ◦ • ◦ ◦ • • • • ◦ ◦ • • ◦ • • ◦ 13 / 61.9

Penny • • ◦ • ◦ ◦ ◦ ◦ ◦ • ◦ ◦ • • • ◦ ◦ ◦ ◦ ◦ ◦ 7 / 33.3
Raj • • ◦ • • • • • • ◦ ◦ ◦ • • ◦ • ◦ ◦ • • • 14 / 66.7

Sheldon • • ◦ • • ◦ • ◦ ◦ • • ◦ • • ◦ • • ◦ • • • 14 / 66.7
# / % similar 14 / 77.8 14 / 77.8 10 / 55.6 11 / 61.1 12 / 66.7 9 / 50.0 14 / 77.8 84 / 66.7

H
ow

ar
d

Amy • • ◦ • • ◦ • ◦ ◦ • • ◦ • ◦ ◦ • • ◦ • • ◦ 12 / 57.1
Bernadette • • ◦ • • ◦ ◦ ◦ ◦ • • ◦ • ◦ ◦ • • ◦ • • ◦ 11 / 52.4

Leonard • • ◦ • • ◦ • ◦ ◦ ◦ ◦ ◦ • ◦ ◦ • • ◦ • • ◦ 10 / 47.6
Penny • • • • ◦ ◦ • • ◦ • ◦ ◦ • • • • • ◦ • • • 15 / 71.4

Raj • • ◦ • • • • ◦ ◦ • ◦ ◦ • • ◦ • • • • ◦ ◦ 13 / 61.9
Sheldon • • ◦ • • ◦ • • • • ◦ ◦ • ◦ ◦ • • ◦ • ◦ ◦ 12 / 57.1

# / % similar 13 / 72.2 12 / 66.7 8 / 44.4 7 / 38.9 9 / 50.0 13 / 72.2 11 / 61.1 73 / 57.9

L
eo

na
rd

Amy • • ◦ • • ◦ • • ◦ • • ◦ • ◦ ◦ ◦ ◦ ◦ • • • 12 / 57.1
Bernadette • ◦ ◦ • • ◦ • • • • • • • ◦ ◦ • • ◦ • • • 15 / 71.4

Howard • • ◦ • • • • • • • • • • • • • • ◦ • • • 19 / 90.5
Penny • • ◦ • • • • • • • • • • • • • • • • • • 20 / 95.2

Raj • ◦ ◦ • • • • • ◦ • • • • • ◦ • • • • • ◦ 16 / 76.2
Sheldon • • • • • • • • ◦ • • ◦ • • ◦ • • ◦ • • • 17 / 81.0

# / % similar 11 / 61.1 16 / 88.9 15 / 83.3 16 / 88.9 12 / 66.7 12 / 66.7 17 / 94.4 99 / 78.6

Pe
nn

y

Amy • • • • • • • • • • • • • • • • • • ◦ ◦ ◦ 18 / 85.7
Bernadette • • • ◦ ◦ ◦ • • ◦ • • • • • ◦ • • ◦ ◦ ◦ ◦ 12 / 57.1

Howard • • • • • • • • ◦ • • • • • • • ◦ ◦ • • ◦ 17 / 81.0
Leonard • • • • • • • • ◦ • • • • • • • • • • • • 20 / 95.2

Raj • • • • • • • • ◦ • • • • • • • • ◦ • • ◦ 18 / 85.7
Sheldon • • • • • • • • ◦ • • • • • • • • • • • ◦ 19 / 90.5

# / % similar 18 / 100 15 / 83.3 13 / 72.2 18 / 100 17 / 94.4 14 / 77.8 9 / 50.0 104 / 82.5

R
aj

Amy • • ◦ • ◦ ◦ • • ◦ • ◦ ◦ • ◦ ◦ • ◦ ◦ • ◦ ◦ 9 / 42.9
Bernadette • • ◦ • ◦ ◦ • • ◦ • ◦ ◦ • ◦ ◦ • ◦ ◦ • • ◦ 10 / 47.6

Howard • ◦ ◦ • ◦ ◦ • • ◦ • • • • • • • ◦ ◦ • • ◦ 13 / 61.9
Leonard ◦ ◦ ◦ ◦ ◦ ◦ • • ◦ • • ◦ • ◦ ◦ ◦ ◦ ◦ • ◦ ◦ 6 / 28.6

Penny • ◦ ◦ • • ◦ • ◦ ◦ ◦ ◦ ◦ • • ◦ • ◦ ◦ • • • 10 / 47.6
Sheldon • • • • • ◦ • • ◦ • • • • ◦ ◦ • • ◦ • ◦ ◦ 14 / 66.7

# / % similar 9 / 50.0 7 / 38.9 11 / 61.1 10 / 55.6 9 / 50.0 6 / 33.3 10 / 55.6 62 / 49.2

Sh
el

do
n

Amy • • • • ◦ ◦ ◦ ◦ ◦ • • • • ◦ ◦ • • ◦ • • • 13 / 61.9
Bernadette • • ◦ • • ◦ • • ◦ • • • • • • • • ◦ • • ◦ 16 / 76.2

Howard • • ◦ • ◦ ◦ • • ◦ • • • • ◦ ◦ • ◦ ◦ ◦ ◦ ◦ 10 / 47.6
Leonard • • ◦ • ◦ ◦ • ◦ ◦ • • • • • ◦ • ◦ ◦ ◦ ◦ ◦ 10 / 47.6

Penny • • ◦ • • ◦ • • • • • • • • • • • ◦ • • • 18 / 85.7
Raj • • • • • ◦ • • ◦ • • ◦ ◦ ◦ ◦ • • ◦ ◦ ◦ ◦ 11 / 52.4

# / % similar 14 / 77.8 9 / 50.0 10 / 55.6 17 / 94.4 10 / 55.6 10 / 55.6 8 / 44.4 78 / 61.9
# / % similar 87 / 69.0 87 / 69.0 76 / 60.3 94 / 74.6 81 / 64.3 77 / 61.1 76 / 60.3 578 / 65.5

(out of 126) (out of 882)

Table 6: Most/Least Distinguishable Characters
Ref. Char Most distinguishable with Least distinguishable with
Amy Bernadette (85.7%) Leonard (47.6%)
Bernadette Amy, Howard (85.7%) Penny (33.3%)
Howard Penny (71.4%) Leonard (47.6%)
Leonard Penny (95.2%) Amy (57.1%)
Penny Leonard (95.2%) Bernadette (57.1%)
Raj Sheldon (66.7%) Leonard (28.6%)
Sheldon Penny (85.7%) Howard, Leonard (47.6%)

who confused the two characters. It turns out that
certain phrases intended for Penny were perceived
as “arrogant” when spoken by Sheldon. Here are
the actual comments by participants:

- “mmhm...” I can picture coming from Sheldon in an irritated manner.
“...you are kidding, right” would be said by Sheldon in an arrogant and
condescending manner.
- “You might be interested in knowing...” sounds like an arrogant Sheldon
line, followed by the “Oh God...” I can actually picture Sheldon saying
this line.
- “You might be interested in knowing...” is used twice in Dialogue 2,
and would be something Sheldon might say to make another person feel
inferior.

91



7.2.2 Perception of Sheldon in comparison to
Leonard (least distinguishable)

As roommates and colleague at work, their sim-
ilarity is understandable. Here are summarized
comments by participants describing the dialogue:

- matter-of-fact, straightforward
- clear, unhesitant
- shorter, more direct sentences; to the point
- use “technically”
- do not use a long string of adjectives

7.3 Leonard
For Leonard, Penny is considered the most distin-
guishable. Even though Leonard is considered less
nerdy than other male characters, his language is
still very different from Penny’s.

Amy being the least distinguishable for Leonard
is also believable. Amy, despite her language
closely resembles Sheldon, is also interested in re-
lationships and friendship (e.g., with Penny and
Bernadette).

Here are some participants’ comments on per-
ceiving Amy’s dialogue as Leonard’s dialogue:

- intelligently spoken but also have a natural tone to them
- quick and to the point without over complicating things
- “I mean...” sounds like Leonard in his somewhat whiny manner
- Leonard sometimes smooths things over for Sheldon so he doesn’t get
upset. I think he would soften some things he says when he uses “I
think” or “I mean”
- intelligent yet normal way of speaking
- both dialogue work okay really

7.4 Other Observations
Leonard and Penny represent the opposite-attracts
couple. The biggest differentiating factor is that
Penny’s dialogue are perceived as being more
emotional than Leonard’s.

A general theme for Leonard’s dialogue is that
his speech pattern is “normal”, implying that ev-
eryone else has a more stylized dialogue. This is
an interesting observation because Leonard is not
“normal” relative to the general population; he is
being characterized as a typical nerd. Yet he is
“normal” relative to his friends and therefore eas-
ier to identify on many cases.

According to (Brooks and Hébert, 2006), in-
dividual’s social identities are largely shaped by
the popular media: what it means to be white,
black, male, female, heterosexual, homosexual,
etc. Since characters are expressed through lan-
guage and therefore connected to characters’ iden-
tity as an individual and as part of a community
(Hurst, 2011), the media such as television often
provides the first (and sometimes the only) impres-
sion of certain groups of people.

In the context of BBT and the significant fea-
tures we used to represent characters, it seems that

Penny’s language represents the typical female as
identified by Lackoff (Lakoff, 1973): hedging,
emotional emphasis, adjectives, etc. This is in
contrast with the male characters as scientists, who
tend to be more matter-of-fact.

Do scientists talk differently from the general
population? Our results answer with a “yes” in
that Penny’s language is mostly in contrast with
male scientists’ language. Such contrast is also re-
flected in the real world (e.g., % of scientists ver-
sus. U.S. population believe in climate change).

What makes the show interesting is the ”in-
between” characters: female scientists Amy and
Bernadette. The perception of the dialogue
showed that the Penny-Bernadette, and Leonard-
Amy pairs shared some similar language. With
the right intention and scripts, the media can help
narrow the perception and narrative gap between
scientists and the general public.

8 Conclusion and Future Work

We explored character voice from the TV show
BBT by building stylistic models relating char-
acter dialogue’s linguistic features to natural lan-
guage generation decisions. These models are
then used to manipulate an expressive NLG to
transform regular sentences into an expressive ver-
sion. The generated, expressive dialogue are then
used in a perceptual experiment to see how users
perceive expressed personalities. Our results were
encouraging in that people were able to perceive
differences among characters, though some better
than others. For the ones that were hard to distin-
guish, participants’ comments provided great in-
sight into how to better express the extracted fea-
tures through NLG.

One possible future work is to use people’s
blogs as a source to create speaker-specific mod-
els. Another possible future work is to use char-
acter models to drive the monologue-to-dialogue
process that created the stories used in our experi-
ment. For example, if the character sounds mostly
negative, the process can try to allocate all nega-
tive sentences to a story character’s dialogue.

We believe our work can be applied to story-
telling applications, such as video games, inter-
active narrative, chatbots, or education systems
where dialogue with personalities may improve
user experience, in a more controllable way (than
using a neural network for generation, for exam-
ple).

92



References
Monika Bednarek. 2011a. The language of fictional

television: a case study of the dramedy gilmore girls.
English Text Construction 4(1):54–83.

Monika Bednarek. 2011b. The stability of the televi-
sual character: A corpus stylistic case study. R. Pi-
azza, M .

Monika Bednarek. 2012. Constructing ”nerdiness”:
Characterisation in the big bang theory. Multilingua
31(2-3):199–229.

Kevin K Bowden, Grace I Lin, Lena I Reed, Jean E
Fox Tree, and Marilyn A Walker. 2016. M2d:
Monolog to dialog generation for conversational
story telling. In Interactive Storytelling: 9th In-
ternational Conference on Interactive Digital Story-
telling, ICIDS 2016, Los Angeles, CA, USA, Novem-
ber 15–18, 2016, Proceedings 9. Springer, pages
12–24.

Dwight E Brooks and Lisa P Hébert. 2006. Gender,
race, and media representation. Handbook of gender
and communication 16:297–317.

Claudia Bubel. 2005. The linguistic construction of
character relations in TV drama: Doing friendship
in Sex and the City. Ph.D. thesis, Universität des
Saarlandes.

D.K. Elson and K.R. McKeown. 2009. A tool for deep
semantic encoding of narrative texts. In Proceedings
of the ACL-IJCNLP 2009 Software Demonstrations.
Association for Computational Linguistics, pages 9–
12.

Andrew S Gordon, Qun Cao, and Reid Swanson. 2007.
Automated story capture from internet weblogs. In
Proceedings of the 4th international conference on
Knowledge capture. ACM, pages 167–168.

M Hurst. 2011. Language, Gender, and Community
in Late Twentieth-century Fiction: American Voices
and American Identities. Springer.

Robin Lakoff. 1973. Language and woman’s place.
Language in society 2(1):45–79.

Benoit Lavoie and Owen Rambow. 1997. A fast and
portable realizer for text generation systems. In Pro-
ceedings of the fifth conference on Applied natu-
ral language processing. Association for Computa-
tional Linguistics, pages 265–268.

Boyang Li, Mohini Thakkar, Yijie Wang, and Mark O
Riedl. 2014. Data-driven alibi story telling for so-
cial believability. In Proceedings of the FDG 2014
Social Believability in Games Workshop. Citeseer.

G.I. Lin and M.A. Walker. 2011. All the world’s a
stage: Learning character models from film. In Pro-
ceedings of the Seventh AI and Interactive Digital
Entertainment Conference, AIIDE. volume 11.

François Mairesse and Marilyn Walker. 2007. Person-
age: Personality generation for dialogue. In An-
nual Meeting-Association For Computational Lin-
guistics. volume 45, page 496.

James W Pennebaker, Martha E Francis, and Roger J
Booth. 2001. Linguistic inquiry and word count:
Liwc 2001. Mahway: Lawrence Erlbaum Asso-
ciates 71:2001.

Paulo Quaglio. 2009. Television Dialogue: The sitcom
Friends vs. natural conversation.. John Benjamins
Publishing Company.

Elena Rishes, Stephanie M Lukin, David K Elson, and
Marilyn A Walker. 2013. Generating different story
tellings from semantic representations of narrative.
In Interactive Storytelling, Springer, pages 192–204.

Susan Schreibman, Ray Siemens, and John Unsworth.
2008. A companion to digital humanities. John Wi-
ley & Sons.

Efstathios Stamatatos. 2009. A survey of modern au-
thorship attribution methods. Journal of the Ameri-
can Society for information Science and Technology
60(3):538–556.

M.A. Walker, R. Grant, J. Sawyer, G.I. Lin,
N. Wardrip-Fruin, and M. Buell. 2011. Perceived or
not perceived: Film character models for expressive
nlg. International Conference on Interactive Digital
Storytelling .

93


