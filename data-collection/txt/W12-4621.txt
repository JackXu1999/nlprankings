



















































State-Split for Hypergraphs with an Application to Tree Adjoining Grammars


Proceedings of the 11th International Workshop on Tree Adjoining Grammars and Related Formalisms (TAG+11), pages 180–188,
Paris, September 2012.

State-Split for Hypergraphs with an Application to Tree-Adjoining
Grammars

Johannes Osterholzer and Torsten Stüber
Department of Computer Science
Technische Universität Dresden

D-01062 Dresden
{johannes.osterholzer,torsten.stueber}@tu-dresden.de

Abstract

In this work, we present a generalization of
the state-split method to probabilistic hyper-
graphs. We show how to represent the deriva-
tional stucture of probabilistic tree-adjoining
grammars by hypergraphs and detail how
the generalized state-split procedure can be
applied to such representations, yielding a
state-split procedure for tree-adjoining gram-
mars.

1 Introduction

The state-split method (Petrov et al., 2006) allows
the successive refinement of a probabilistic context-
free grammar (PCFG) for the purpose of natural
language processing (NLP). It employs automatic
subcategorization of nonterminal symbols: in an
iteration of a split-merge cycle, every nontermi-
nal of the PCFG is split into two, along with the
corresponding grammar rules, whose probabilities
are distributed uniformly to the split rules. The re-
sulting PCFG’s rule probabilities are then trained
on an underlying treebank using the Expectation-
Maximization algorithm (Dempster et al., 1977)
for maximum-likelihood estimation on incomplete
data. Finally, split nonterminal symbols which do
not contribute to a significant increase in likelihood
are merged back together. This counteracts an ex-
ponential blowup in the number of nonterminals
and prevents, to some degree, the phenomenon of
overfitting.

The in-house developed statistical machine trans-
lation toolkit Vanda (Büchse et al., 2012) offers
state-splitting for the refinement of its language
models. Vanda’s internal representation of the
various weighted tree grammar, automaton and
transducer formalisms utilized for translation is
by means of probabilistic hypergraphs, i.e., graphs

consisting of vertices and hyperedges, where each
of the latter connects a (possibly empty) sequence
of tail vertices to a head vertex and is assigned a
probability. Hence, our implementation of state-
split operates on such hypergraphs as underlying
data structures. The connection between parsing
and hypergraphs is well-known in the field of NLP
(Klein and Manning, 2004), where a hypergraph
represents the derivation forest of a certain word. In
our system, however, we use such a probabilistic hy-
pergraph to represent the whole derivational struc-
ture of a grammar, with a one-to-one correspon-
dence between hyperpaths and grammar deriva-
tions. We apply well-known product constructions
from the theory of weighted automata, going back
to Bar-Hillel et al. (1961) and generalized to the
case of weighted tree automata by Maletti and Satta
(2009), to restrict them to the derivations of a given
word.

The mildly context-sensitive generative capacity
of tree-adjoining grammars (TAG) is well-suited to
the purpose of NLP (Joshi and Schabes, 1991). tree-
adjoining grammars allow two basic operations
to rewrite and derive trees: substitution, where a
tree’s leaf node is replaced with another tree, and
adjoining, which can be seen as the second-order
substitution of a context (called an auxiliary tree)
into another tree.

Probabilistic tree-adjoining grammars (PTAG)
(Schabes, 1992; Resnik, 1992) assign to every de-
rived tree an associated probability. They can also
be incorporated into Vanda by an appropriate rep-
resentation with hypergraphs. Such PTAGs can
be extracted from treebank corpora, as detailed by
Chen et al. (2006). The idea to refine these gram-
mars furthermore by executing the already imple-
mented state-split procedure on their hypergraph
representations arises naturally. The main contribu-
tion of the work at hand is a formalization of this

180



idea. However, one should note that our proposed
method does not just apply to PTAG, but should
carry over to many grammar formalisms that can be
represented by hypergraphs. In our formalization,
we have to deal with a complication, introduced by
the nature of adjunction in TAG. For this purpose,
we introduce split relations.

An alternative to our approach to state-splitting
PTAG is the one taken by Shindo et al. (2012),
where a method for symbol refinement of proba-
bilistic tree substitution grammars is presented.
Since tree substitution grammars are just tree-
adjoining grammars without adjoining sites (and
indeed, adjoining can be simulated by the combina-
tion of a tree substitution grammar which encodes
adjoining explicitly, and a yield function which
performs these encoded operations, cf. (Maletti,
2010)), it is possible that their technique can be
adapted to the more general setting. This would
have the additional advantage that smoothing by
backoff to simpler context-free grammar rules is in-
corporated in their system, increasing performance
in the case of sparse data, while we do not cover
smoothing in the work at hand.

Note that, although most of the preliminaries
may be safely skimmed, we want to point out that
our definitions of first- and second-order substitu-
tion are slightly non-standard (but they enable a
concise formalization of TAG).

2 Preliminaries

In the following, we will denote the set of non-
negative integers by N. The set {1, . . . , k} shall be
abbreviated by [k]. The set of the non-negative real
numbers will be denoted by R≥0, and the closed
interval between two reals a and b by [a, b].

The set of finite words over a set A is written as
A∗, ε ∈ A∗ is the empty word and A+ = A∗ \ {ε}.
The reflexive-transitive closure of a binary relation
R shall be denoted by R∗.

2.1 Unranked Trees
We call a finite set Σ of symbols an alphabet. Given
a finite set A and alphabet Σ, let ΣA denote the
alphabet of all σa with σ ∈ Σ, a ∈ A. Note that
the new symbol σa is merely a syntactic construct
and should be identified neither with σ nor a.

Presuming an alphabet Σ and set A, the set
UΣ(A) of unranked trees over Σ indexed by A
is the smallest set U such that A ⊆ U and for
every n ∈ N, t1, . . . , tn ∈ U , and σ ∈ Σ, also

σ(t1, . . . , tn) ∈ U . For a tree σ() we will just
write σ and UΣ(∅) will be denoted by UΣ. The
set of positions pos(t) of a tree t ∈ UΣ(A) is de-
fined by pos(σ(t1, . . . , tn)) = {ε} ∪ {iw | i ∈
[n], w ∈ pos(ti)} and t(w) denotes the label of t
at position w.

Throughout the rest of the paper, let X =
{x1, x2, . . .} denote an infinite set of variables
and, for n ∈ N, let Xn = {x1, . . . , xn}. Simi-
larly, let Y = {y1, y2, . . .}, let Yn = {y1, . . . , yn}
for n ∈ N, and Z = {z}. Given k ∈ N, we call
a tree t ∈ UΣ(A ∪ ∆X) (resp. t ∈ UΣ∪∆Y (A))
proper in Xk (resp. Yk) if for every i > k, there
is no appearance of δxi (resp. of δyi) in t, and for
every i ∈ [k], there is exactly one position in t
labeled with δxi (resp. δyj ), for some δ ∈ ∆. This
unique δ ∈ ∆ will be denoted by lbt(xi) (resp.
lbt(yj)).

For an alphabet Σ and k ∈ N, we denote first-
order substitution of the trees s1, . . . , sk ∈ UΣ into
t ∈ UΣ(ΣXk) by t[x1/s1, . . . , xk/sk] ∈ UΣ, ab-
breviated by t[x/s]. The tree t[x/s] is the result of
replacing every node in t which is labeled by σxi ,
for i ∈ [k] and some σ ∈ Σ, by the tree si. Sim-
ilarly, for s ∈ UΣ(A) and t ∈ UΣ(Z), let t[z/s]
denote the tree obtained from t by replacing every
node that is labelled with z by s.

For every alphabet Σ and k ∈ N, second-
order substitution of the trees s1, . . . , sk ∈
UΣ(Z) into the tree t ∈ UΣ∪ΣYk will be de-
noted by tJy1/s1, . . . , yk/skK ∈ UΣ, or shorter,
by tJy/sK. For every symbol σ ∈ Σ and vari-
able yi ∈ Yk, we define σyi(t1, . . . , tn)Jy/sK =
si[z/σ(t1Jy/sK, . . . , tnJy/sK)], and we define
σ(t1, . . . , tn)Jy/sK = σ(t1Jy/sK, . . . , tnJy/sK)
for σ ∈ Σ.

2.2 Hypergraphs

A hypergraph is a tuple (V,E, µ, g) where the
set V contains the graph’s vertices and E the hy-
peredges (or just edges). Edge connectivity is de-
noted by the function µ : E → V + and g ∈ V
is the graph’s goal vertex. For a hyperedge e
with µ(e) = a0a1 · · · an, we define its head as
hd(e) = a0, its tail as tl(e) = a1 · · · an and its
arity as ar(e) = n. The set of a-hyperpaths HaG
of a hypergraph G = (V,E, µ, g), a ∈ V , is the
largest set of trees H ⊆ UE such that for every
d ∈ H , w ∈ pos(d), we have hd(d(ε)) = a,
|{v ∈ pos(d) | v = wi, i ∈ N}| = | tl(e)|,
and tl(d(w)) = hd(d(w1)) · · · hd(d(wn)), where

181



A

By1

Cx2c

Cx1
C

c

B

zb

Figure 1: PTAG G with trees α1, α2 and β

n = | tl(e)|. The hyperpaths of G are its g-
hyperpaths: HG = H

g
G.

A tuple (V,E, µ, g, p) is called a probabilistic
hypergraph (phg) if (V,E, µ, g) is a hypergraph
and p : E → [0, 1] assigns a probability to every
hyperedge. We will denote the class of all proba-
bilistic hypergraphs byH and assume definitions
for unweighted hypergraphs to carry over to the
probabilistic case. The probability of a derivation
d ∈ HG is defined to be

P (d | G) =
∏

w∈pos(d)
p(d(w)).

In the following, we will fix the phg G
(resp. G′, G′′) to be of the form
(V,E, µ, g, p) (resp. (V ′, E′, µ′, g′, p′),
etc.).

2.3 Tree-Adjoining Grammars

In our definition of tree-adjoining grammars, the
variable xi (resp. yj) in a symbol σxi (resp. σyj )
will be used to tag σ with the information that it
labels the ith substitution (resp. jth adjoining) site
of the respective tree. The variable z in an auxiliary
tree denotes the tree’s foot node. This formalization
allows us, among others, to give a straightforward
definition of derived trees.

A tree-adjoining grammar (TAG) is a tuple
G = (Σ, S,S,A) where Σ is an alphabet, S ∈ Σ
is called the start symbol, S ⊆ UΓ(∆) the set of
initial trees, and A ⊆ UΓ(∆ ∪ Z) the set of aux-
iliary trees, with Γ = Σ ∪ ΣY and ∆ = ΣX . We
demand for every t ∈ S ∪ A that t(ε) ∈ Σ, and
that there are n, m ∈ N such that t is proper in Xn
and in Ym. In the following, we will denote these
unique n and m by rk1(t), resp. rk2(t). Moreover,
we require that for every t ∈ A, z appears exactly
once in t.

A probabilistic tree-adjoining grammar (PTAG)
is then a tuple G = (Σ, S,S,A, P,Q) such
that (Σ, S,S,A) is a TAG, P : S ∪ A → [0, 1]
maps trees to their probabilities and Q =
(Qt : Yrk2(t) → [0, 1])t∈S∪A assigns to adjoining

A

s(
α

1
)/
0
.9

C

S(α1, y1)

n
(α

1
,y

1
)/
0
.3

y(α
1 ,y

1 )/
0
.7

B?
a(β)/0.6s(α2)/0.5

Figure 2: Hypergraph representation of G

sites the probability of their activation. We will
denote the class of all PTAG by T .

In the following, the PTAG G
(resp. G′) will be assumed to be
of the form (Σ, S,S,A, P,Q) (resp.
(Σ′, S′,S ′,A′, P ′, Q′)).

Let us examine an example PTAG G, with Σ =
{A,B,C, c, b}, S = A, initial trees S = {α1, α2},
and an auxiliary tree A = {β}. Refer to Fig. 1,
which depicts the tree α1 with two substitution
sites labeled, resp., with symbols A and C, as well
as one adjoining site, labeled with B. To the right
are the initial tree α2 and the auxiliary tree β, both
with no substitution or adjoining sites. The foot
node of β is indicated by z.

We denote the conditional probability that an
elementary tree t with root symbol A is used to
rewrite substitution or adjoining sites that are la-
beled withA by P (t). For G, e.g., let P (α1) = 0.9,
P (α2) = 0.5, and P (β) = 0.6. The probabil-
ity that the adjoining site in α1, tagged with y1,
is activated during a derivation, is denoted by
Qα1(y1). In the case of this running example, let
Qα1(y1) = 0.7.

3 Hypergraph Representations

For every PTAG G, we can construct a hypergraph
G = hg(G), whose hyperpaths stand in a one-to-
one correspondence to the grammar’s derivations.
Hence, we will call G a hypergraph representation
of G.

In our following definition, such a hypergraph
hg(G) can contain three different types of vertices.
The vertices of the first type model the derivation
of initial trees. These vertices are just copies of the
symbols of G: for every symbol A ∈ Σ, a vertex
A is introduced. Refer to Fig. 2, which depicts the
hypergraph hg(G) that represents the derivational
structure of the PTAG G from our running example.
Its A- and C-hyperpaths model derivation of initial

182



trees with respective root symbols. Other, irrelevant,
vertices of this form are omitted from the figure.

The second vertex type helps in the derivation
of auxiliary trees. For each A ∈ Σ, the hypergraph
contains a corresponding vertex A?. In Fig. 2, the
only relevant vertex of this form is B?, whose hy-
perpaths represent derivations of auxiliary trees
with root symbol B.

Lastly, for every adjoining site that appears in an
elementary tree t and is tagged with yi, we include
a vertex S(t, yi). This vertex is used to explicitly
model the decision to activate or not to activate the
corresponding adjoining site. In the figure, there is
only one such vertex, S(α1, y1).

The hyperedges of hg(G) can also be classified
by their intended meaning: a hyperedge of the form
s(α) signifies the substitution of the initial tree
α during a derivation. Its head vertex is its root
symbol. Its tail vertices are, in order, the labels of
α’s substitution sites (indicating that the derivation
must continue with derivations of trees with the
respective symbols at their root) and the vertices
S(α, yi) modelling its adjoining sites (these indi-
cate the necessary decision on activating the sites).
In Fig. 2, we see, among others, s(α1), whose head
vertex A corresponds to α1’s root symbol, while
its two tail vertices C stand for its respective sub-
stitution sites, and S(α1, y1) for its adjoining site.

Hyperedges of the form a(β), with β an elemen-
tary auxiliary tree, possess essentially the same
structure as the former, but signify derivation of
auxiliary trees. In the figure, the only edge of this
form is a(β). Its head vertex is B? because β’s root
symbol is B, and it has no tail vertices, since there
are no sites in β.

Finally, the hyperedge y(t, yi) (resp. n(t, yi))
encodes the information that an adjoining site in
t was activated (deactivated). Both have S(t, yi)
as head, and while n(t, yi) has no tail, the tail of
y(t, yi) signifies that the derivation should continue
with the adjoining of an auxiliary tree. In Fig. 2,
e.g., y(α1, y1) models the activation of the site
that corresponds to its head vertex S(α1, y1), and
has the tail vertex B?, because that site is labeled
with B.

The probabilities of these hyperedges are taken
over from P and Q in the obvious way. Formally,
we define hg(G) = G with
V ={A,A? | A ∈ Σ}
∪ {S(t, yi) | t ∈ S ∪ A, i ∈ [rk2(t)]},

E ={s(t) | t ∈ S} ∪ {a(t) | t ∈ A}
∪ {y(t, yi), n(t, yi) | t ∈ S ∪A, i ∈ [rk2(t)]},

goal vertex g = S, and

µ(s(t)) = AB1 · · ·Bn S(t, y1) · · · S(t, ym),
µ(a(t)) = A?B1 · · ·Bn S(t, y1) · · · S(t, ym),

µ(y(t, yj)) = S(t, yj)C
?
j ,

µ(n(t, yj)) = S(t, yj),

where n = rk1(t), m = rk2(t), A = t(ε), Bi =
lbt(xi) for i ∈ [n], and Cj = lbt(yj) for j ∈ [m],
while

p(s(t)) = P (t), p(y(t, yj)) = Qt(yj),

p(a(t)) = P (t), p(n(t, yj)) = 1−Qt(yj).
Note that for our simple running example from

Fig. 1, there are only two possible derivations of
trees with root symbol A: in the first one, we sub-
stitute two instances of α2 into α1, and, after ac-
tivation of the adjoining site in α1, adjoin β into
this site. The corresponding A-hyperpath in Fig. 2
is d1 = s(α1)

(
s(α2), s(α2), y(α1, y1)

(
a(β)

))
.

Alternatively, we can ignore the adjoining site,
and arrive at the corresponding derivation d2 =
s(α1)

(
s(α2), s(α2), n(α1, y1)

)
.

Given such a hyperpath that represents a PTAG
derivation, we can compute the derivation’s de-
rived tree in a bottom-up manner with the function
yd: Hhg(G) → UΣ(X ∪ Z) defined by

yd(s(t)(d1, . . . , dn, d
′
1, . . . , d

′
m))

= t[x/ yd(d)]Jy/ yd(d′)K
yd(a(t)(d1, . . . , dn, d

′
1, . . . , d

′
m))

= t[x/ yd(d)]Jy/ yd(d′)K
yd(y(t, yj)(d)) = yd(d)

yd(n(t, yj)) = z,

where again n = rk1(t), m = rk2(t), and yd(d)
denotes the element-wise application of yd to all
d1, . . . , dn, analogously for yd(d′).

For example, for d1 from above, we can compute
its derived tree as

yd(d1) = α1[x1/α2, x2/α2]Jy1/βK
= A

(
C(c),By1(c,C(c))

)
Jy1/βK

= A
(
C(c), β

[
z/B(c,C(c))

])

= A
(
C(c),B

(
b,B(c,C(c))

))
.

4 State-Split Hypergraphs

In this section, we will detail how to generalize
the state-split method presented by Petrov et al. to
hypergraphs that represent PTAGs.

183



S(α1, y1)〈1〉
y(α1, y1)〈21〉

S(α1, y1)〈2〉

y(α1, y1)〈12〉
B?〈1〉 B?〈2〉

y(
α

1
,y

1
)〈
1
1
〉 y(α

1 ,y
1 )〈2

2〉

Figure 3: Crossing hyperedges

As explained in the introduction, the state-split
algorithm proceeds in three distinct phases: first of
all, it splits every vertex into two, and then it trains
the probabilities of the resulting hyperedges on a
given treebank corpus, using the EM algorithm. In a
third step, vertices which do not increase likelihood
are merged back together. However, due to the
nature of adjunction in tree-adjoining grammar, the
splits and merges cannot be quite as liberal as in
the case of context-free grammars.

The reason for this will be shown immediately,
but first and foremost, let us define how to represent
split vertices. Given a hypergraphG, for any a ∈ V ,
and b ∈ {0, 1, 2}, define

a〈b〉 =
{

(a, b) if b ∈ {1, 2}
a if b = 0.

For a vertex a, the so-defined a〈1〉 and a〈2〉 will
denote the two vertices which result from splitting
a, while a〈0〉 is just a notation for the merged-back-
together vertex a. We will also have to annotate
split hyperedges in this way: for a hyperedge e ∈ E
with ar(e) = n, as well as a tuple b ∈ {0, 1, 2}n+1,
we introduce the syntax e〈b〉, which stands for
(e, b).

Given this notation, we can examine the men-
tioned complication of the state-split procedure,
which arises when splitting hyperedges of the form
y(t, yj). Let us assume that the vertices S(α1, y1),
B?, as well as the hyperedge y(α1, y1) from Fig. 2
are to undergo subcategorization.

If we split the vertices and hyperedges indiscrim-
inately, we arrive at the situation displayed in Fig. 3:
each of the vertices has been split into two copies,
and four new y-hyperedges were introduced. What
is the meaning of these hyperedges? In a hyperpath
representing a PTAG derivation, the appearance
of the hyperedge y(α1, y1)〈11〉 signifies that the
adjoining site in α1, labeled with the split symbol

B〈1〉, is activated, and next, an auxiliary tree with
equal root symbol must be adjoined into it.

However, there is a problem with the hyperedge
y(α1, y1)〈21〉 which crosses the shaded ellipses in
the figure (and, analogously, with y(α1, y1)〈12〉).
This hyperedge can be interpreted as activation of
the mentioned adjoining site, labeled with B〈1〉,
and preparation of adjoining an auxiliary tree with
root symbol B〈2〉. This stands in conflict to the
concept of adjoining, where the label of the node
to be replaced must be identical to the symbol at
the root of the auxiliary tree.

There are several distinct possibilities to handle
this complication. First of all, one could just do
away with the above condition regarding adjoining.
Actually, such a relaxation of the formalism was
already proposed by Rogers (2003), resulting in
non-strict tree-adjoining grammars.

As a second option, the formalism of TAG could
be extended by introducing states (cf. (Büchse et
al., 2011) for synchronous TAG). In this modifica-
tion, the derivational structure of the grammar is no
longer dependent on the labels of adjoining and sub-
stitution sites, this information is instead encoded
into the states, which only appear as intermediate
symbols in a derivation. Performing the splits and
merges on such states, instead of symbols, could
also remedy the problem.

However, for this work, we chose to stick to a
conceptually simpler solution, thus staying close to
the established notion of tree-adjoining grammar:
we just disallow the creation of such crossing hyper-
edges during the split-merge cycle; or, to put it dif-
ferently, we only introduce a hyperedge y(t, y)〈i j〉
into the split hypergraph if i = j. Hence, in
Fig. 3, only the two hyperedges y(α1, y1)〈11〉 and
y(α1, y1)〈22〉, which are both “within” the two
shaded ellipses, would be generated.

To formalize this idea, we augment the state-split
method with what we call split relations. Given a
hypergraph G which is to undergo a split-merge
cycle, a split relation on G is a symmetric relation
R ⊆ V ×V on the graph’s vertices. If a hyperedge
e from G connects, among others, two vertices a1
and a2 such that a1Ra2, then the idea is to split
e only into such hyperedges which connect either
only a1〈1〉 and a2〈1〉, or only a1〈2〉 and a2〈2〉, but
not, e.g., a1〈1〉 and a2〈2〉.

This invariant must also be heeded when we
merge back together parts of the hypergraph: for
example, if a1Ra2, we cannot merge back together

184



a1〈1〉 and a1〈2〉 but leave a2〈1〉 and a2〈2〉 split at
the same time. Hence, what we must consider is
merging all elements which are (in-)directly related
by R simultaneously together. More succinctly, the
objects considered to be merged must be the equiv-
alence classes of the reflexive-transitive closure R∗

of R, which we also call split classes.

4.1 Splitting
When we split the vertices of a hypergraph, we
must take care that the created hyperedges respect
the supplied split relation R, as explained above.

This is achieved by the following function
splitR, which splits every node and introduces
new hyperedges respecting R. Given a hyper-
graph G and split relation R ⊆ V × V , we let
splitR(G) = G

′ with

V ′ = {a〈b〉 | a ∈ V, b ∈ {1, 2}},
E′ = {e〈b0, . . . , bk〉 | e ∈ E,µ(e) = a0 · · · ak,

b0, . . . , bk ∈ {1, 2},
aiRaj implies bi = bj},

µ′(e〈b0, . . . , bk〉) = a0〈b0〉 · · · ak〈bk〉,
where µ(e) = a0 · · · ak,

g′ = g〈1〉, and

p′(e〈b0, . . . , bk〉) =
p(e)

2c

where c = |{e〈b′〉 ∈ E′ | b′ = b0 · · · bk}|.
Note that the probabilities of hyperedges are dis-
tributed uniformly to their split copies in G′. In an
implementation, these should be slightly random-
ized to give starting values for the EM algorithm,
as mentioned by Petrov et al. (2006).

The split hypergraph’s number of hyperedges is
exponential in their arity. We can try to mitigate the
problem of this exponential blowup by binarizing
the trees of the TAG which was initially extracted
from a corpus, following the description of Lang
(1994).

4.2 Merging
As explained above, we have to merge all elements
of a split class back together simultaneously. This
will be denoted with the following function. Let
G′ = splitR(G) be a hypergraph resulting from a
split, R ⊆ V × V a split relation, and C ∈ V/R∗
one of its split classes. The hypergraph which re-
sults from merging C back together is denoted by
G′′ = mergeCR(G

′) with

V ′′ = V ′ \ {a〈b〉 | a ∈ C, b ∈ {1, 2}} ∪ C,

E′′ = {e〈b′0, . . . , b′k〉 | e〈b0, . . . , bk〉 ∈ E′,
µ(e) = a0 · · · ak,
ai ∈ C implies b′i = 0,
ai 6∈ C implies b′i = bi},

where we write o(e〈b′0, . . . , b′k〉) = e〈b0, . . . , bk〉
for the relation of e〈b′0, . . . , b′k〉 to e〈b0, . . . , bk〉,
µ′′(e〈b0, . . . , bk〉) = a0〈b0〉 · · · ak〈bk〉,

where µ(e) = a0 · · · ak,

g′′ =

{
g if g ∈ C
g′ otherwise, and

p′′(e′′) =





∑

e′∈o−1(e′′)
p′(e′)/2 if hd(e) ∈ C

∑

e∈o−1(e′′)
p′(e′) otherwise

Note that the probabilities of hyperedges which are
merged back together are summed up. In the case
that a hyperedge’s head vertex is merged, we have
to normalize the resulting probability.

4.3 Treebank Corpora and Likelihood

During the split-merge procedure, we want to train
the split hypergraph representations on a supplied
treebank. Following the presentation of Prescher
(2005), we will abstract away from the concrete
data structures which might be used to represent
a collection of trees, and just define a treebank
corpus as a function K : UΣ → R≥0, assigning to
every tree over an alphabet Σ a certain frequency,
such that the set of trees with non-zero frequency
is finite.

Let us assume that G0 = hg(G) is the hyper-
graph representation of a PTAG G and Gn is the
result of n split-merge cycles on G0. Then we can
define the likelihood of a corpus K : UΣ → R≥0
on Gn as

L(K | Gn) =
∏

t∈UΣ

(∑

d∈HGn
yd(un(d))=t

P (d | Gn)
)K(t)

=
∏

t∈UΣ

( ∑

d′∈HG0
yd(d′)=t

( ∑

d∈HGn
un(d)=d′

P (d | Gn)
))K(t)

. (1)

Hereby, the function un removes n levels of an-
notation from an annotated derivation, for every
n ∈ N. It is defined as the homomorphic exten-
sion of the function ũn on hyperedges to unranked
trees, where ũ is defined for every hyperedge e by
ũ0(e) = e, and for n > 1, ũn(e〈b〉) = ũn−1(e).

185



Algorithm 1 The state-split algorithm
Require: phg G, n ∈ N, corpus K : UΣ → R≥0

1: function STATESPLIT(G,n,K)
2: G0 ← G
3: compute split relation R0
4: for all i ∈ [n] do
5: Gi ← SPLITMERGE(Gi−1, Ri−1,K)
6: update split relation Ri−1 to Ri−1
7: end for
8: return Gn
9: end function

Note that in (1), for every t ∈ UΣ with non-zero
frequency, there are only finitely many derivations
d′ ∈ HG0 , which can be determined by employing
a TAG parser at the beginning of the state-split
process. For each of these d′ we can compute the
value of the innermost sum in a bottom-up manner,
similarly to the computation of inside probabilities
as described by Petrov et al. (2006).

In the following, given a hypergraph Gn, n > 0,
we will identify yd(d) with yd(un(d)), for every
d ∈ HGn .

4.4 Overview of the Algorithm
Now we can denote the state-split algorithm for
hypergraphs in pseudocode, cf. Alg. 1. After com-
puting the initial split relation, the algorithm’s outer
loop (ll. 4–7) executes n split-merge cycles on G
using the treebank corpus K for training. Addition-
ally, in each step it updates the split relation to the
newly generated hypergraph.

Note that the concrete computation of split
classes depends strongly on the grammar formal-
ism represented by the hypergraph, hence we leave
it abstract in the general formulation of the algo-
rithm. In our case, where we use tree-adjoining
grammars as underlying formalism, we can instanti-
ate it as follows:R0 is defined as the finest symmet-
ric relationR such that, for every vertex of the form
S(t, yi) in G0, we have (S(t, yi), lbt(yi)?) ∈ R.
Similarly, given a split relation Ri−1, the updated
split relation Ri is the finest symmetric relation R
such that, for every b ∈ {0, 1, 2} and pair a1〈b〉,
a2〈b〉, if a1〈b〉 and a2〈b〉 are vertices in Gi such
that a1Ri−1 a2, then a1〈b〉Ra2〈b〉.

The split-merge cycle (cf. Alg. 2) can be consid-
ered as the core of the state-split algorithm. Sup-
plied with a phg G, split relation R on G, and
treebank corpus K, SPLITMERGE first of all splits
the nodes of G, as defined in section 4.1. Then it

Algorithm 2 A split-merge cycle
Require: phg G, R ⊆ V × V , corpus

K : UΣ → R≥0
1: function SPLITMERGE(G,R,K)
2: G′ ← splitR(G)
3: G′ ← EMTRAIN(G′,K)
4: for all C ∈ V/R∗ do
5: G′′ ← mergeCR(G′)
6: if L(G′′ | K)/L(G′ | K) ≥ λ then
7: G′ ← G′′
8: end if
9: end for

10: return G′
11: end function

uses the EM algorithm for maximum likelihood es-
timation on incomplete data to train the newly-split
phg on the treebankK. Finally, in ll. 4–9, each split
class is tentatively merged back together, and if the
concomitant loss in likelihood does not fall below
a certain factor λ ∈ [0, 1], this merge is taken over
permanently. As noted by Petrov et al., this com-
bats the exponential blow-up of the hypergraph, as
well as the phenomenon of overfitting.

For the sake of completeness, let us give a rough
sketch of the EM algorithm for training state-split
hypergraphs on treebank corpora, following the
exposition of Prescher (2005). Given a phg G
and a treebank corpus K, the EM algorithm al-
ternatingly repeats two computations, called the
Expectation step (E-step) and the Maximization
step (M-step), until the increase in likelihood of the
newly-computed hypergraph on the corpus K falls
beneath a certain threshold δ ∈ R.

Note that the EM algorithm is not guaranteed
to find the actual global maximum of the likeli-
hood, however, as already shown by Dempster et al.
(1977), the likelihoods of the respective grammars
are monotonically non-decreasing, and so at least a
local maximum can be approximated.

In the algorithm’s E-step, a complete-data cor-
pus C : HG → R≥0 on the hyperpaths of G is
generated by distributing the frequencies of every
derived tree t ∈ UΣ in the corpus K to the hyper-
paths representing derivations of t, weighted by
their conditional probability given t. The M-step
then uses this complete-data corpus to compute
hyperedge probabilities by relative frequency es-
timation. Thereby, cte(d) denotes the number of
appearances of the hyperedge e in the derivation d,

186



Algorithm 3 EM for (state-split) hypergraphs
Require: phg G, corpus K : UΣ → R≥0

1: function EMTRAIN(G,K)
2: repeat
3: G′ ← G
4: E-Step: define C : HG → R by:
5: C(d)=K(yd(d))·P

(
d | yd(d), G

)

6: M-step: set new probabilities:
7: for all e ∈ E do
8: p(e)←

∑
d∈HG C(d)·cte(d)∑

d∈HG C(d)·cthd(e)(d)
9: end for

10: until L(G|K)− L(G′|K) < δ
11: return G
12: end function

and cthd(e)(d) the number of appearing hyperedges
with the same head vertex as e.

One might remark that this concise, but formu-
laic presentation of EM is not immediately suitable
for implementation, but, using the derivation em-
ployed by Gupta and Chen (2011), it is straightfor-
ward to bring it into the form of the well-known
Inside-Outside algorithm, which has been adapted
to PTAG by Schabes (1992).

5 State-Split preserves Hypergraph
Representations

After all these definitions, we might ask ourselves
the following question: given a hypergraph repre-
sentation G of some PTAG G, does the hypergraph
which results from an application of the split-merge
cycle to G still represent a PTAG? This question
indeed arises quite naturally, after all it is an im-
portant requirement for the correctness of the state-
split procedure for PTAGs with hypergraphs.

If we denote the result of a split-merge cy-
cle on a phg G by sm(G), and the class of all
hypergraph representations of probabilistic tree-
adjoining grammars byHT , then this question can
essentially be answered by proving the inclusion

sm(HT ) ⊆ HT . (2)
But the validity of this inclusion does certainly

depend on the formal definition ofHT . Indeed, for
the definition which comes to mind first, in which
we just fixHT to be the set {hg(G) | G ∈ T }, i.e.
the image of the class of all PTAGs under hg, the
inclusion is not valid! This is due to the annotation
of vertices and hyperedges in a split-merge cycle:
hypergraphs that contain, for example, a vertex

G G

G′ G′′ G′
sm

hg

hg ∼
Φ

∃?

Figure 4: Proof idea

A〈2〉, which was generated by splitting a vertex A,
are arguably not in the image of hg!

However, one can show that the structure of a
split-and-merged hypergraph still corresponds to
the image of a PTAG G under hg. We will capture
this structural identity by means of hypergraph
isomorphisms.

A hypergraph isomorphism Φ: G ∼−→ G′
is a tuple (Φ1,Φ2), where Φ1 : V → V ′ and
Φ2 : E → E′ are bijections such that Φ1(g) = g′
and µ′(Φ2(e)) = Φ1(a0) · · ·Φ1(an) if µ(e) =
a0 · · · an. We write G ∼= G′ if there is an isomor-
phism Φ: G ∼−→ G′.

We define the set of all possible hypergraph rep-
resentations of PTAG as

HT = {G ∈ H | ∃G ∈ T .hg(G) ∼= G}.
Obviously, for every PTAG G, hg(G) ∈ HT , i.e.,
it is indeed a hypergraph representation accord-
ing to this definition. As visualized in Fig. 4, the
core of the proof of (2) is then as follows: Given
G ∈ T , G ∈ H, and G′ = sm(G), we have to
construct a PTAG G′ and hypergraph isomorphism
Φ: hg(G′) ∼−→ G′.

We construct G′ so that the trees of G′ are relabel-
ings of those in G, generated by incorporating the
annotations to the hyperedges in G to substitution
and adjoining sites:

S ′ = {t〈b〉 | s(t)〈b〉 ∈ E′},
A′ = {t〈b〉 | a(t)〈b〉 ∈ E′},

where t〈a b1 · · · brk1(t) c1 · · · crk2(t)〉 is the result of
replacing the root symbol A of t by A〈a〉, every
substitution site Axi in t by A〈bi〉xi , and every ad-
joining site Ayj by A〈cj〉yj , for i ∈ [rk1(t)] and
j ∈ [rk2(t)]. The probabilities P (t) of elemen-
tary trees t, as well as the activation probabilities
Qt(yi) are just read off from p(s(t)), p(a(t)), resp.
p(y(t, yi)).

The hypergraph isomorphism Φ then just re-
verses this relabeling. Given nodes or hyperedges
from hg(G′) with annotations in them, it removes
them from the contained symbols resp. elementary

187



trees and moves them “to the back”, i.e.,

Φ1(A〈b〉) = A〈b〉, Φ1
(
(A〈b〉)?

)
= A?〈b〉,

Φ1
(
S(t〈b〉, yi)

)
= S(t, yi)〈b(i)〉

and

Φ2
(
s(t〈b〉)

)
= s(t)〈b〉,

Φ2
(
s(t〈b〉)

)
= s(t)〈b〉,

Φ2
(
y(t〈b〉, yi)

)
= y(t, yi)〈b(i)b(i)〉,

Φ2
(
n(t〈b〉, yi)

)
= n(t, yi)〈b(i)〉,

where, in all three cases, b(i) = b(rk1(t) + i).
Now, Φ can indeed be proven to be a hypergraph

isomorphism between G′ and sm(G), but for rea-
sons of space, we will omit these details from this
work. Note that the construction of G′ and Φ can
alternatively be interpreted as the definition of a
read-off procedure, which allows our system Vanda
to convert back its internal state-split hypergraph
representation into a probabilistic tree-adjoining
grammar, e.g. to display the resulting grammars for
means of debugging. Thus, it also has a hands-on
relevance for implementation.

Acknowledgements We thank the anonymous
reviewers for their valuable comments, and our
colleagues Toni Dietze and Matthias Büchse for
fruitful discussions.

References
Yehoshua Bar-Hillel, Micha A Perles, and Eli Shamir.

1961. On formal properties of simple phrase struc-
ture grammars. Zeitschrift für Phonetik, Sprachwis-
senschaft und Kommunikationsforschung, 14:143–
172.

Matthias Büchse, Mark-Jan Nederhof, and Heiko Vogler.
2011. Tree Parsing with Synchronous Tree-
Adjoining Grammars. In Proceedings of the 12th
International Conference on Parsing Technologies,
pages 14–25, Dublin, Ireland, October. Association
for Computational Linguistics.

Matthias Büchse, Toni Dietze, Johannes Osterholzer,
Anja Fischer, and Linda Leuschner. 2012. Vanda –
A Statistical Machine Translation Toolkit. In Pro-
ceedings of the 6th International Workshop Weighted
Automata: Theory and Applications, pages 36–37.

John Chen, Srinivas Bangalore, and K Vijay-Shanker.
2006. Automated Extraction of Tree-Adjoining
Grammars from Treebanks. Natural Language Engi-
neering, 12(3):251.

Arthur P Dempster, Nan McKenzie Laird, and Donald B
Rubin. 1977. Maximum Likelihood from Incom-
plete Data via the EM Algorithm. Journal of the
Royal Statistical Society, B(39):1–38.

Maya R Gupta and Yihua Chen. 2011. Theory and Use
of the EM Algorithm. Foundations and Trends in
Signal Processing, 4(3):223–296.

Aravind K Joshi and Yves Schabes. 1991. Tree-
Adjoining Grammars and Lexicalized Grammars. In
Maurice Nivat and Andreas Podelski, editors, Defin-
ability and Recognizability of Sets of Trees, pages
409–431. Elsevier.

Dan Klein and Christopher D Manning. 2004. Parsing
and Hypergraphs. In Harry Bunt, John Carroll, and
Giorgio Satta, editors, New Developments in Parsing
Technology, volume 23 of Text, Speech and Language
Technology, pages 351–372. Springer Netherlands.

Bernard Lang. 1994. Recognition can be Harder than
Parsing. Computational Intelligence, 10(4):486–494.

Andreas Maletti and Giorgio Satta. 2009. Parsing Algo-
rithms based on Tree Automata. In Harry Bunt, edi-
tor, Proceedings of the 11th International Conference
on Parsing Technologies, pages 1–12. Association
for Computational Linguistics.

Andreas Maletti. 2010. A Tree Transducer Model for
Synchronous Tree-Adjoining Grammars. In Proceed-
ings of the 48th Annual Meeting of the Association
for Computational Linguistics, pages 1067–1076. As-
sociation for Computational Linguistics.

Slav Petrov, Leon Barrett, Romain Thibaux, and Dan
Klein. 2006. Learning Accurate, Compact, and Inter-
pretable Tree Annotation. In Proceedings of the 21st
International Conference on Computational Linguis-
tics and 44th Annual Meeting of the Association for
Computational Linguistics, pages 433–440, Sydney,
Australia. Association for Computational Linguistics.

Detlef Prescher. 2005. A Tutorial on the Expectation-
Maximization Algorithm Including Maximum-
Likelihood Estimation and EM Training of Prob-
abilistic Context-Free Grammars. Technical report,
15th European Summer School in Logic, Language,
and Information.

Philip Resnik. 1992. Probabilistic Tree-Adjoining
Grammar As A Framework For Statistical Natural
Language Processing. In Proceedings of the 14th
Conference on Computational Linguistics, pages 418–
424.

James Rogers. 2003. wMSO theories as grammar for-
malisms. Theoretical Computer Science, 293(2):291–
320, February.

Yves Schabes. 1992. Stochastic Tree-Adjoining Gram-
mars. In Proceedings of the Workshop on Speech
and Natural Language, HLT ’91, pages 140–145,
Stroudsburg, PA, USA. Association for Computa-
tional Linguistics.

Hiroyuki Shindo, Yusuke Miyao, Akinori Fujino, and
Masaaki Nagata. 2012. Bayesian Symbol-Refined
Tree Substitution Grammars for Syntactic Parsing. In
Proceedings of the 50th Annual Meeting of the Associ-
ation for Computational Linguistics, pages 440–448.

188


