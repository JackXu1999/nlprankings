



















































Multi-Task Learning with Language Modeling for Question Generation


Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing
and the 9th International Joint Conference on Natural Language Processing, pages 3394–3399,
Hong Kong, China, November 3–7, 2019. c©2019 Association for Computational Linguistics

3394

Multi-Task Learning with Language Modeling for Question Generation

Wenjie Zhou, Minghua Zhang, Yunfang Wu∗
Key Laboratory of Computational Linguistics, Ministry of Education

School of Electronics Engineering and Computer Science, Peking University, Beijing, China
{wjzhou013,zhangmh,wuyf}@pku.edu.cn

Abstract

This paper explores the task of answer-aware
questions generation. Based on the attention-
based pointer generator model, we propose
to incorporate an auxiliary task of language
modeling to help question generation in a hi-
erarchical multi-task learning structure. Our
joint-learning model enables the encoder to
learn a better representation of the input se-
quence, which will guide the decoder to gen-
erate more coherent and fluent questions. On
both SQuAD and MARCO datasets, our multi-
task learning model boosts the performance,
achieving state-of-the-art results. Moreover,
human evaluation further proves the high qual-
ity of our generated questions.

1 Introduction

Question generation (QG) receives increasing in-
terests in recent years due to its benefits to several
real applications: (1) QG can aid in the develop-
ment of annotated questions to boost the question
answering systems (Duan et al., 2017; Tang et al.,
2017); (2) QG enables the dialogue systems to ask
questions which make it more proactive (Shum
et al., 2018; Colby, 1975); (3) QG can help to gen-
erate questions for reading comprehension texts in
the education field. In this paper, we focus on
answer-aware QG. Giving a sentence and an an-
swer span as input, we want to generate a question
whose response is the answer.

Previous work on QG was mainly tackled by
two approaches: the rule-based approach and
neural-based approach. The neural-based ap-
proach receives a booming development due to
the release of large-scale reading comprehension
datasets like SQuAD (Rajpurkar et al., 2016) and
MARCO (Nguyen et al., 2016). Most of the neu-
ral approaches on QG employ the encoder-decoder

∗Corresponding author.

framework, which incorporate attention mecha-
nism to pay more attention to the informative part
and copy mode to copy some tokens from the input
text (Du et al., 2017; Zhou et al., 2017; Song et al.,
2018; Subramanian et al., 2018; Zhao et al., 2018;
Sun et al., 2018). To make better use of answer
information, Song et al. (2018) leverage multi-
perspective matching, and Sun et al. (2018) pro-
pose a position-aware model that aims at putting
more emphasis on the answer-surrounded context
words. Zhao et al. (2018) aggregate paragraph-
level information to help QG. Another line of work
is to deal with question answering and question
generation as dual tasks (Tang et al., 2017; Duan
et al., 2017). Some other works try to generate
questions from a text without answers as input
(Subramanian et al., 2018; Du and Cardie, 2017).
Although some progress has been made, there is
still much room for improvement for QG.

Multi-task learning is an effective way to im-
prove model expressiveness via related tasks by
introducing more data and fruitful semantic in-
formation to the model (Caruana, 1998). Many
works in NLP have adopted multi-task learning
and prove its effectiveness on textual entailment
(Hashimoto et al., 2017), keyphrase generation
(Ye and Wang, 2018) and document summariza-
tion (Guo et al., 2018). To the best of our
knowledge, no work attempts to employ multi-
task learning for question generation. Although
language modeling has been applied to multi-task
learning for classification tasks, they are different
from our generation task.

In this work, we propose to incorporate lan-
guage modeling as an auxiliary task to help QG
via multi-task learning. We adopt the pointer-
generator (See et al., 2017) reinforced with fea-
tures as the baseline model, which yields state-
of-the-art result (Sun et al., 2018). The language
modeling task is to predict the next word and the



3395

People follow courage

hlm1 hlm2 h
lm3

hlm1 hlm2 hlm3

w1hlm1w1
follow

LM hidden states
<s>

w2hlm2w2
courage

w3hlm3w3
</s>follow

Language  
Modeling

h11 h12 h13

h21 h22 h23

Attention

y1

y2

y3

What

do

people

y4 follow

People

Answer Position

Word Embedding

Lexical Features

Pcopy
Pvocab

pg

Answer Position

Word Embedding

Lexical Features

y4 ?

Figure 1: Overall structure of our joint-learning model.

previous word whose input is a plain text without
relying on any annotation. The two tasks are then
combined with a hierarchical structure, where the
low-level language modeling encourages our rep-
resentation to learn richer language features that
will help the high-level network to generate better
expressive questions.

We conduct extensive experiments on two
reading comprehension datasets: SQuAD and
MARCO. We experiment with different settings
to prove the efficacy of our multi-task learn-
ing model: with/without language modeling and
with/without features. Experimental results show
that the language modeling consistently yields ob-
vious performance gain over baselines, for all
evaluation metrics, including BLEU, perplexity
and distinct. Our full model outperforms the
existing state-of-the-art results on both datasets,
achieving a high BLEU-4 score of 16.23 on
SQuAD and 20.88 on MARCO, respectively. We
also conduct human evaluation, and our generated
questions get higher scores on all three metrics,
including matching, fluency and relevance.

2 Model Description

The baseline model is an attention-based seq2seq
pointer-generator reinforced by lexical features,
like the work of Sun et al. (2018). In our proposed
model, we employ multi-task learning with lan-
guage modeling as an auxiliary task for QG. The
whole structure of our model is shown in Figure 1.

2.1 Feature-enriched Pointer Generator
The feature-rich encoder is a bidirectional LSTM
used to produce a sequence of hidden states hLt .
The encoder takes a sequence of word-and-feature
vectors as input ((x1, ..., xT )), which concatenates
the word embedding et, answer position embed-
ding at and lexical feature embedding lt (xt =
[et; at; lt]).The lexical feature is composed of POS
tags, NER tags, and word case.

The attention-based decoder is another unidi-
rectional LSTM, which is conditioned on the pre-
vious decoder state si−1, decoded word wi−1, and
context vector ci−1 which is generated via atten-
tion mechanism (Bahdanau et al., 2014):

si = LSTM([wi−1; ci−1], si−1) (1)

Further, a two-layer feed-forward network is used
to produce the vocabulary distribution Pvocab.

The pointer generator (See et al., 2017) incorpo-
rates a copy mode Pcopy(w), which allows copy-
ing words from the source text via pointing. The
final probability distribution is to combine both
modes with a generation probability pg∈[0, 1]:

P (w) = pgPvocab(w) + (1− pg)Pcopy(w) (2)

The model is trained to minimize the negative
log-likelihood of the target sequence. We denote
this loss as E.

2.2 Language Modeling
The language model is to predict the next word
and previous word in the sequence with a forward
LSTM and a backward LSTM, respectively. First,
we feed the input sequence into a bidirectional
LSTM to get the hidden representations hlmt .

Then, these states are fed into a softmax layer
to predict the next and the previous word:

P lm(wt+1|w<t+1) = softmax(Wf
−→
hlmt ) (3)

P lm(wt−1|w>t−1) = softmax(Wb
←−
hlmt ) (4)

The training objective is to minimize the loss
function which is defined as the average of the
negative log-likelihood of the next word and the
previous word in the sequence:

Elm = − 1
T − 1

T−1∑
t=1

log(P lm(wt+1|w<t+1))

− 1
T − 1

T∑
t=2

log(P lm(wt−1|w>t−1)) (5)



3396

Dataset SQuAD MARCO
Model BLEU-1 BLEU-2 BLEU-3 BLEU-4 BLEU-1 BLEU-2 BLEU-3 BLEU-4

NQG++ (Zhou et al., 2017) - - - 13.29 - - - -
matching strategy (Song et al., 2018) - - - 13.91 - - - -
Maxout Pointer (sentence) (Zhao et al., 2018) 44.51 29.07 21.06 15.82 - - - 16.02

answer-focused model (Sun et al., 2018) 42.10 27.52 20.14 15.36 46.59 33.46 24.57 18.73
position-aware model (Sun et al., 2018) 42.16 27.37 20.00 15.23 47.16 34.20 24.40 18.19
hybrid model (Sun et al., 2018) 43.02 28.14 20.51 15.64 48.24 35.95 25.79 19.45

Our Model

pointer generator with features (baseline) 41.25 26.76 19.53 14.89 54.04 36.68 26.62 20.15
w/ features + language modeling 42.80 28.43 21.08 16.23 54.47 37.30 27.31 20.88
w/o features + language modeling 42.72 27.73 20.26 15.43 54.62 37.37 27.18 20.71
w/ features + 1-layer encoder 42.12 27.48 20.12 15.33 53.51 36.42 26.49 20.11

Table 1: Experimental results of our model in different settings comparing with previous methods on two datasets.

2.3 Multi-task Learning
Instead of sharing representations between two
tasks (Rei, 2017) or encoding two tasks at the same
level (Liu et al., 2018; Chen et al., 2018; Kendall
et al., 2018), we adopt a hierarchical structure to
combine the two tasks, by treating language mod-
eling as a low-level task and pointer generator net-
work as high-level, because language modeling
is fundamental and its semantic information will
benefit question generation. In details, we first
feed the input sequence into the language model-
ing layer to get a sequence of hidden states. Then
we concatenate them with the input sequence to
obtain the input of the feature-rich encoder.

Finally, the loss of LM is added to the main loss
to form a combined training objective:

Etotal = E + βElm (6)

where β is a hyper-parameter, which is used to
control the relative importance of two tasks.

3 Experiments

3.1 Dataset
We conduct experiments on two reading compre-
hension datasets: SQuAD and MARCO, using
the data shared by Zhou et al. (2017) and Sun
et al. (2018), where the lexical features are ex-
tracted with Stanford CoreNLP. In details, there
are 86,635, 8,965 and 8,964 sentence-answer-
question triples in the training, development and
test set for SQuAD, and 74,097, 4,539 and 4,539
sentence-answer-question triples in the training,
development and test set for MARCO.

3.2 Experiment Settings
Our vocabulary contains the most frequent 20,000
words in each training set. Word embeddings are

initialized with the pre-trained 300-dimensional
Glove vectors, and are allowed to be fine-tuned
during training. The representations of answer po-
sition, POS tags, NER tags and word cases are ran-
domly initialized as 32-dimensional vectors, re-
spectively. The encoder of our baseline model
consists of 2 BiLSTM layers, and the hidden size
of both the encoder and decoder is set to 512.

In our joint model, grid search is used to deter-
mine β and results are shown in Figure 2. Conse-
quently, we set the value of β to 0.6.

Figure 2: The impact of β on BLEU-4

We search the best-trained checkpoint base on
the dev-set. In order to mitigate the fluctuation of
the training procedure, we then average the nearest
5 checkpoints to obtain a single averaged model.
Beam search is used with a beam size of 12.

3.3 Automatic Evaluation
Results on BLEU The experimental results on
BLEU (Papineni et al., 2002) are illustrated in
Table 1. Our full model (w/ features + lan-
guage modeling) significantly outperforms previ-
ous models and achieves state-of-the-art results
on both datasets, with 16.23 BLEU-4 score on
SQuAD and 20.88 on MARCO respectively.
Results without Features To investigate the ro-
bustness of our model, we conduct an experiment
whose input sequence only takes word embed-
dings and answer position, but without lexical fea-



3397

Dataset SQuAD MARCO
Model perplexity distinct-1 distinct-2 perplexity distinct-1 distinct-2

pointer generator with features (baseline) 41.24 9.67 39.46 17.69 16.84 45.61
w/ features + language modeling 34.09 9.80 40.94 15.17 17.31 47.51
w/o features + language modeling 38.70 9.73 40.89 14.07 17.13 46.98
w/ features + 1-layer encoder 38.40 9.56 39.71 17.61 17.10 46.87

Table 2: Perplexity and distinct of different setting models on two datasets

Dataset SQuAD MARCO
Model Matching Fluency Relevance Matching Fluency Relevance

pointer generator with features (baseline) 0.983 1.573 1.540 1.133 1.667 1.593
+ language modeling 1.147 1.690 1.600 1.160 1.720 1.603

kendall correlation coefficient 0.820 0.814 0.796 0.852 0.792 0.824

Table 3: Human evaluation results on two datasets.

tures (w/o features + language modeling). We
can see that the auxiliary task of language mod-
eling boosts model performance on both datasets,
demonstrating that our model guarantees higher
stability because it does not depend on the quality
of lexical features. Therefore, our model can ap-
ply to low-resource languages where there is not
adequate data for training a well-performed model
for lexical features extraction.
Results with a 3-layer Encoder To validate that
we gain the improvement not due to a deeper net-
work, we replace the language modeling module
with one encoder layer, that is to say, we adopt
a 3-layer encoder. Comparing this model (w/
features+1-layer encoder) with the full model (w/
features+language modeling), we can see that our
joint-learning model performs better than simply
adding an extra encoding layer. The results on
MARCO also clearly show that a deeper network
does not guarantee better performance.
Perplexity and Diversity Since BLEU only mea-
sures a hard matching between references and gen-
erated text, we further adopt perplexity and distinct
(Li et al., 2016) to judge the quality of generated
questions. The results in Table 2 indicate that the
language modeling task helps the model to gener-
ate more fluent and readable questions. Besides,
the generated questions have better diversity.

3.4 Human Evaluation

For a better study on the quality of generations,
we perform human evaluation. Three annotators
are asked to grade the generated questions in three
aspects: matching indicates whether a question
can be answered with the given answer; fluency

indicates whether a question is fluent and gram-
matical; relevance indicates whether a question
can be answered according to the given context.
The rating score ranges from 0 to 2. We randomly
sample 100 cases from each dataset for evalua-
tion. Results are displayed in Table 3. The co-
efficient between human judges is high, validating
a high quality of our annotation. The results show
that by incorporating language modeling, the gen-
erated questions receive higher scores across all
three metrics.

Context: Prior to the early 1960s, access to the forest’s
interior was highly restricted, and the forest remained ba-
sically intact.
Answer: The early 1960s
Reference: Accessing the Amazon rainforest was re-
stricted before what era?
Baseline: When did access to the forest’s interior?
Joint-model: When did access to the forest’s interior be-
come restricted?

Context: This teaching by Luther was clearly expressed
in his 1525 publication on the bondage of the will, which
was written in response to on free will by Desiderius Eras-
mus (1524).
Answer: 1525
Reference: When did Luther publish on the bondage of
the will?
Baseline: In what year was the bondage of the will on the
bondage of the will?
Joint-model: When was the bondage of the will pub-
lished?

Table 4: Examples of generated questions by different
models.

3.5 Case Study

Further, Table 4 gives two examples of the gen-
erated questions on SQuAD dataset, by the base-



3398

line model and our joint model respectively. It is
obvious that questions generated by our proposed
model are more complete and grammatical.

4 Conclusion

This paper proves that equipped with language
modeling as an auxiliary task, the neural model
for QG can learn better representations that help
the decoder to generate more accurate and fluent
questions. In future work, we will adopt the aux-
iliary language modeling task to other neural gen-
eration systems to test its generalization ability.

Acknowledgments

We thank Weikang Li and Xin Jia for their valu-
able comments and suggestions. This work is sup-
ported by the National Natural Science Founda-
tion of China (61773026, 61572245).

References
Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua

Bengio. 2014. Neural machine translation by
jointly learning to align and translate. CoRR,
abs/1409.0473.

Rich Caruana. 1998. Learning to learn. chapter Mul-
titask Learning, pages 95–133. Kluwer Academic
Publishers, Norwell, MA, USA.

Ying Chen, Wenjun Hou, Xiyao Cheng, and Shoushan
Li. 2018. Joint learning for emotion classification
and emotion cause detection. In Proceedings of the
2018 Conference on Empirical Methods in Natural
Language Processing, Brussels, Belgium, October
31 - November 4, 2018, pages 646–651.

Kenneth Mark Colby. 1975. Artificial Paranoia. Else-
vier Science Inc., New York, NY, USA.

Xinya Du and Claire Cardie. 2017. Identifying where
to focus in reading comprehension for neural ques-
tion generation. In Proceedings of the 2017 Con-
ference on Empirical Methods in Natural Language
Processing, EMNLP 2017, Copenhagen, Denmark,
September 9-11, 2017, pages 2067–2073.

Xinya Du, Junru Shao, and Claire Cardie. 2017. Learn-
ing to ask: Neural question generation for reading
comprehension. In Proceedings of the 55th Annual
Meeting of the Association for Computational Lin-
guistics, ACL 2017, Vancouver, Canada, July 30 -
August 4, Volume 1: Long Papers, pages 1342–1352.

Nan Duan, Duyu Tang, Peng Chen, and Ming Zhou.
2017. Question generation for question answer-
ing. In Proceedings of the 2017 Conference on Em-
pirical Methods in Natural Language Processing,
EMNLP 2017, Copenhagen, Denmark, September
9-11, 2017, pages 866–874.

Han Guo, Ramakanth Pasunuru, and Mohit Bansal.
2018. Soft layer-specific multi-task summarization
with entailment and question generation. In Pro-
ceedings of the 56th Annual Meeting of the Associa-
tion for Computational Linguistics, ACL 2018, Mel-
bourne, Australia, July 15-20, 2018, Volume 1: Long
Papers, pages 687–697.

Kazuma Hashimoto, Caiming Xiong, Yoshimasa Tsu-
ruoka, and Richard Socher. 2017. A joint many-task
model: Growing a neural network for multiple NLP
tasks. In Proceedings of the 2017 Conference on
Empirical Methods in Natural Language Process-
ing, EMNLP 2017, Copenhagen, Denmark, Septem-
ber 9-11, 2017, pages 1923–1933.

Alex Kendall, Yarin Gal, and Roberto Cipolla. 2018.
Multi-task learning using uncertainty to weigh
losses for scene geometry and semantics. In 2018
IEEE Conference on Computer Vision and Pattern
Recognition, CVPR 2018, Salt Lake City, UT, USA,
June 18-22, 2018, pages 7482–7491.

Jiwei Li, Michel Galley, Chris Brockett, Jianfeng Gao,
and Bill Dolan. 2016. A diversity-promoting ob-
jective function for neural conversation models. In
NAACL HLT 2016, The 2016 Conference of the
North American Chapter of the Association for
Computational Linguistics: Human Language Tech-
nologies, San Diego California, USA, June 12-17,
2016, pages 110–119.

Lizhen Liu, Xiao Hu, Wei Song, Ruiji Fu, Ting Liu,
and Guoping Hu. 2018. Neural multitask learning
for simile recognition. In Proceedings of the 2018
Conference on Empirical Methods in Natural Lan-
guage Processing, Brussels, Belgium, October 31 -
November 4, 2018, pages 1543–1553.

Tri Nguyen, Mir Rosenberg, Xia Song, Jianfeng Gao,
Saurabh Tiwary, Rangan Majumder, and Li Deng.
2016. MS MARCO: A human generated machine
reading comprehension dataset. In Proceedings
of the Workshop on Cognitive Computation: Inte-
grating neural and symbolic approaches 2016 co-
located with the 30th Annual Conference on Neu-
ral Information Processing Systems (NIPS 2016),
Barcelona, Spain, December 9, 2016.

Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic eval-
uation of machine translation. In Proceedings of the
40th Annual Meeting of the Association for Compu-
tational Linguistics, July 6-12, 2002, Philadelphia,
PA, USA., pages 311–318.

Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and
Percy Liang. 2016. Squad: 100, 000+ questions for
machine comprehension of text. In Proceedings of
the 2016 Conference on Empirical Methods in Nat-
ural Language Processing, EMNLP 2016, Austin,
Texas, USA, November 1-4, 2016, pages 2383–2392.

Marek Rei. 2017. Semi-supervised multitask learn-
ing for sequence labeling. In Proceedings of the

http://arxiv.org/abs/1409.0473
http://arxiv.org/abs/1409.0473
http://dl.acm.org/citation.cfm?id=296635.296645
https://aclanthology.info/papers/D18-1066/d18-1066
https://aclanthology.info/papers/D18-1066/d18-1066
https://aclanthology.info/papers/D17-1219/d17-1219
https://aclanthology.info/papers/D17-1219/d17-1219
https://aclanthology.info/papers/D17-1219/d17-1219
https://doi.org/10.18653/v1/P17-1123
https://doi.org/10.18653/v1/P17-1123
https://doi.org/10.18653/v1/P17-1123
https://aclanthology.info/papers/D17-1090/d17-1090
https://aclanthology.info/papers/D17-1090/d17-1090
https://aclanthology.info/papers/P18-1064/p18-1064
https://aclanthology.info/papers/P18-1064/p18-1064
https://aclanthology.info/papers/D17-1206/d17-1206
https://aclanthology.info/papers/D17-1206/d17-1206
https://aclanthology.info/papers/D17-1206/d17-1206
https://doi.org/10.1109/CVPR.2018.00781
https://doi.org/10.1109/CVPR.2018.00781
http://aclweb.org/anthology/N/N16/N16-1014.pdf
http://aclweb.org/anthology/N/N16/N16-1014.pdf
https://aclanthology.info/papers/D18-1183/d18-1183
https://aclanthology.info/papers/D18-1183/d18-1183
http://ceur-ws.org/Vol-1773/CoCoNIPS_2016_paper9.pdf
http://ceur-ws.org/Vol-1773/CoCoNIPS_2016_paper9.pdf
http://www.aclweb.org/anthology/P02-1040.pdf
http://www.aclweb.org/anthology/P02-1040.pdf
http://aclweb.org/anthology/D/D16/D16-1264.pdf
http://aclweb.org/anthology/D/D16/D16-1264.pdf
https://doi.org/10.18653/v1/P17-1194
https://doi.org/10.18653/v1/P17-1194


3399

55th Annual Meeting of the Association for Compu-
tational Linguistics, ACL 2017, Vancouver, Canada,
July 30 - August 4, Volume 1: Long Papers, pages
2121–2130.

Abigail See, Peter J. Liu, and Christopher D. Manning.
2017. Get to the point: Summarization with pointer-
generator networks. In Proceedings of the 55th An-
nual Meeting of the Association for Computational
Linguistics, ACL 2017, Vancouver, Canada, July 30
- August 4, Volume 1: Long Papers, pages 1073–
1083.

Heung-Yeung Shum, Xiaodong He, and Di Li. 2018.
From eliza to xiaoice: challenges and opportuni-
ties with social chatbots. Frontiers of IT & EE,
19(1):10–26.

Linfeng Song, Zhiguo Wang, Wael Hamza, Yue Zhang,
and Daniel Gildea. 2018. Leveraging context infor-
mation for natural question generation. In Proceed-
ings of the 2018 Conference of the North American
Chapter of the Association for Computational Lin-
guistics: Human Language Technologies, NAACL-
HLT, New Orleans, Louisiana, USA, June 1-6, 2018,
Volume 2 (Short Papers), pages 569–574.

Sandeep Subramanian, Tong Wang, Xingdi Yuan,
Saizheng Zhang, Adam Trischler, and Yoshua Ben-
gio. 2018. Neural models for key phrase extrac-
tion and question generation. In Proceedings of
the Workshop on Machine Reading for Question An-
swering@ACL 2018, Melbourne, Australia, July 19,
2018, pages 78–88.

Xingwu Sun, Jing Liu, Yajuan Lyu, Wei He, Yan-
jun Ma, and Shi Wang. 2018. Answer-focused and
position-aware neural question generation. In Pro-
ceedings of the 2018 Conference on Empirical Meth-
ods in Natural Language Processing, Brussels, Bel-
gium, October 31 - November 4, 2018, pages 3930–
3939.

Duyu Tang, Nan Duan, Tao Qin, and Ming Zhou. 2017.
Question answering and question generation as dual
tasks. CoRR, abs/1706.02027.

Hai Ye and Lu Wang. 2018. Semi-supervised learning
for neural keyphrase generation. In Proceedings of
the 2018 Conference on Empirical Methods in Natu-
ral Language Processing, Brussels, Belgium, Octo-
ber 31 - November 4, 2018, pages 4142–4153.

Yao Zhao, Xiaochuan Ni, Yuanyuan Ding, and Qifa
Ke. 2018. Paragraph-level neural question gener-
ation with maxout pointer and gated self-attention
networks. In Proceedings of the 2018 Conference on
Empirical Methods in Natural Language Process-
ing, Brussels, Belgium, October 31 - November 4,
2018, pages 3901–3910.

Qingyu Zhou, Nan Yang, Furu Wei, Chuanqi Tan,
Hangbo Bao, and Ming Zhou. 2017. Neural ques-
tion generation from text: A preliminary study. In
Natural Language Processing and Chinese Comput-
ing - 6th CCF International Conference, NLPCC

2017, Dalian, China, November 8-12, 2017, Pro-
ceedings, pages 662–671.

https://doi.org/10.18653/v1/P17-1099
https://doi.org/10.18653/v1/P17-1099
https://doi.org/10.1631/FITEE.1700826
https://doi.org/10.1631/FITEE.1700826
https://aclanthology.info/papers/N18-2090/n18-2090
https://aclanthology.info/papers/N18-2090/n18-2090
https://aclanthology.info/papers/W18-2609/w18-2609
https://aclanthology.info/papers/W18-2609/w18-2609
https://aclanthology.info/papers/D18-1427/d18-1427
https://aclanthology.info/papers/D18-1427/d18-1427
http://arxiv.org/abs/1706.02027
http://arxiv.org/abs/1706.02027
https://aclanthology.info/papers/D18-1447/d18-1447
https://aclanthology.info/papers/D18-1447/d18-1447
https://aclanthology.info/papers/D18-1424/d18-1424
https://aclanthology.info/papers/D18-1424/d18-1424
https://aclanthology.info/papers/D18-1424/d18-1424
https://doi.org/10.1007/978-3-319-73618-1_56
https://doi.org/10.1007/978-3-319-73618-1_56

