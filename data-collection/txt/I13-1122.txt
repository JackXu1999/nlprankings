










































Automated Grammar Correction Using Hierarchical Phrase-Based Statistical Machine Translation


International Joint Conference on Natural Language Processing, pages 937–941,
Nagoya, Japan, 14-18 October 2013.

Automated Grammar Correction Using Hierarchical Phrase-Based
Statistical Machine Translation

Bibek Behera, Pushpak Bhattacharyya
Dept. of Computer Science and Engineering

IIT Bombay, Mumbai, India
{bibek,pb}@cse.iitb.ac.in

Abstract

We introduce a novel technique that uses
hierarchical phrase-based statistical ma-
chine translation (SMT) for grammar cor-
rection. SMT systems provide a uni-
form platform for any sequence transfor-
mation task. Thus grammar correction can
be considered a translation problem from
incorrect text to correct text. Over the
years, grammar correction data in the elec-
tronic form (i.e., parallel corpora of incor-
rect and correct sentences) has increased
manifolds in quality and quantity, mak-
ing SMT systems feasible for grammar
correction. Firstly, sophisticated transla-
tion models like hierarchical phrase-based
SMT can handle errors as complicated as
reordering or insertion, which were diffi-
cult to deal with previously throuh the me-
diation of rule based systems. Secondly,
this SMT based correction technique is
similar in spirit to human correction, be-
cause the system extracts grammar rules
from the corpus and later uses these rules
to translate incorrect sentences to correct
sentences. We describe how to use Joshua,
a hierarchical phrase-based SMT system
for grammar correction. An accuracy of
0.77 (BLEU score) establishes the efficacy
of our approach.

1 Introduction

We consider grammar correction as a translation
problem - translation from an incorrect sentence
to a correct sentence. The correcting system is
trained using a parallel corpus of incorrect and
their corresponding correct sentences. The system
learns SCFG (synchronous context free grammar)
rules (Chiang, 2005) during translation. SCFG
rules look like this:-

• X → X1 of X2, X1 for X2

The above rule implies that phrases X1 and X2
in source language are translated to phrases in tar-
get language, while of is replaced with for. The
position of both phrases w.r.t. of remains same in
the target language, which means there is no re-
ordering of phrases.

After generating such grammar rules, it con-
verts the erroneous sentence to a tree using the
rules of grammar, i.e., the left hand side of the
SCFG rules. It then applies correction rules, i.e.,
the right hand side of the SCFG rules, to convert
the tree as explained later in section 3. The yield
of the tree generates the corrected sentence.

Here are various types of errors that one en-
counters in grammar correction:

Article choice errors:- a Himalayas is the
longest mountain range in the world. The correct
translation is ‘the Himalayas is the longest moun-
tain range in the world’.

Preposition errors:- Helicopter crashed at cen-
tral London. The correct translation is ‘Helicopter
crashed in central London’.

Word form errors:- The rain mays fall in July
should be changed to ‘The rain may fall in July’.

Word insertion errors:- The court deemed nec-
essary that she respond to the summons should be
changed to ‘The court deemed it necessary that
she respond to the summons’.

Reordering errors:- never we miss deadlines
should be corrected to ‘we never miss deadlines’.

Article choice errors and preposition errors have
been tackled by rule based techniques. But rules
are customized, so to say, for each error, which is a
time consuming and fragile process. SMT, on the
other hand, treats all errors uniformly, consider-
ing error correction as a translation problem. Sec-
ondly, problems such as reordering or word inser-
tion are well known in machine translation.

937



The roadmap of the paper is as follows. In sec-
tion 2, we discuss previous work. In section 3,
we elaborate on how hierarchical machine trans-
lation system can do automatic grammar correc-
tion. Section 4 states the grammar rules that are
extracted by the system automatically. In section
5, we present our experiments followed by the re-
sults in section 6. We conclude in section 7 with
pointers to future work.

2 Background

Initially the work that has been done in grammar
correction is based on identifying grammar errors.
Chodorow and Leacock (2000) used an ngram
model for error detection by comparing correct
ngrams with ngrams to be tested. Later, classifi-
cation techniques like Maximum entropy models
have been proposed (Izumi et al., 2003; Tetreault
and Chodorow, 2008; Tetreault and Chodorow,
2008). These classifiers not only identify errors,
but also correct them. These methods do not make
use of erroneous words thus making error correc-
tion similar to the task of filling empty blanks.
While in editing sentences, humans often require
the information in the erroneous words for gram-
mar correction.

In other works, machine translation has been
previously used for grammar correction. Brock-
ett (2006) used phrasal based MT for noun correc-
tion of ESL students. Désilets and Hermet (2009)
translate from native language L1 to L2 and back
to L1 to correct grammar in their native languages.
Mizumoto (2012) also used phrase-based SMT
for error correction. He used large-scale learner
corpus to train his system. These translation tech-
niques suffered from lack of good quality parallel
corpora and also good translation systems.

If high quality parallel corpus can be obtained,
the task of grammar correction becomes easy us-
ing a powerful translation model like hierarchical
phrase based machine translation.

3 Automatic grammar correction using
hierarchical phrase-based SMT

In this section we discuss the working and the im-
plementation of the grammar correction system.

3.1 Working

Grammar correction can be seen as a process of
translation of incorrect sentences to correct ones.
Basically the translation system needs a parallel

corpus of incorrect and correct sentences. The
system starts with an alignment to obtain word to
word translation probabilities. The second stage is
grammar extraction using the hiero style of gram-
mar (Chiang, 2005). Non-terminals are general-
ized form of phrases, i.e., all possible phrases al-
lowed in the framework of Chiang (2005) are rep-
resented by the symbol X. There is another symbol
S to start the parse tree. These rules are in the form
of SCFG rules. If the incorrect sentence is, ‘few
has arrived’ and the correct sentence is, ‘few have
arrived’, the grammar rules extracted are :-

• X → few has X1, few have X1

• X → arrived, arrived

The first rule means that few has followed by a
phrase may be translated to few have followed by
translation of that phrase. Second rule suggests
that any phrase that yields arrived can be trans-
lated to arrived.

After the grammar extraction is done, the left
sides of the grammar rules are stripped and used
to generate the parse tree of the sentence few has
arrived.
Here are the left side rules:-

• X→ few has X1

• X→ arrived

Also, there is a “glue rule” to combine two trees
or just derive a non terminal from the start symbol
S.

• S→ S1 X | X

The glue rule is used to start the parsing process.
It generates a sub-tree for the string few has and a
non-terminal for arrived. Then the right side rules
are used to convert few has to few have as shown
in Figure 1, while arrived is translated as arrived.

S

S

X

few has

X

arrived

⇒ S

S

X

few have

X

arrived

Figure 1: Parse tree for transformation from incor-
rect to correct sentences.

1Here S means start of the tree

938



The yield of the tree generates few have ar-
rived, which is the required correction. This is the
essence of decoding in hierarchical machine trans-
lation.

3.2 Implementation

The translation system being used is the Joshua
Machine translation system (Li et al., 2010). The
SMT based correction pipeline is a six step pro-
cess in conformity with the Joshua decoder (Gan-
itkevitch et al., 2012). First we create the dataset
in a input folder with six files such as:-

1. train.incorrect- Incorrect sentences in our
training corpus

2. train.correct- Correct sentences in our train-
ing corpus

3. tune.correct- Incorrect sentences in our de-
velopment set

4. tune.incorrect- Correct sentences in our de-
velopment set

5. test.correct- Incorrect sentences in our testing
set

6. test.incorrect- Correct sentences in our test-
ing set

The pipeline starts with preprocessing the cor-
pus, i.e., tokenisation and lowercasing followed by
word alignment. The result of word alignment is
stored in training.align file. Then a file, “gram-
mar.gz” is created by joshua that stores SCFG
rules using information from the training.align file
and the training corpus. This process is called
grammar generation and is followed by the build-
ing of the language model.

For developing the language model, the Joshua
MT system uses KenLM (Heafield, 2011) toolkit
or BerkeleyLM. This is the end of the training pro-
cess. The steps that follow in the pipeline are tun-
ing and testing. Tuning iterates over the develop-
ment set to obtain the best parameters for the trans-
lation model. At the end of tuning, the system
obtains the optimized parameters that can be de-
ployed into the translation model for testing. The
testing phase translates sentences from test set to
evaluate the overall BLEU score (Papineni et al.,
2002).

4 Analysis of grammar rules extracted

In this section we look at how various grammar
corrections have been handled. The various types
of errors handled are article choice errors, prepo-
sition errors, word-form choice errors and word
insertion errors as mentioned in Park and Levy
(2011). Apart from these errors, we also discuss
errors due to reordering and errors due to unseen
verbs which have not been implemented in previ-
ous models.

4.1 Article choice errors

The article a has been replaced by the before
proper nouns like a amazon and a himalayas. The
grammar rules are:-

• X → a himalayas X1, the himalayas X1

• X → a amazon X1, the amazon X1

The rules suggest that a himalayas succeeded by
a phrase X1 can be replaced by the himalayas fol-
lowed by the same phrase.

4.2 Preposition errors

Preposition at has been replaced by in before a
place like at central London. The grammar rule
is:-

• X → X1 at central London, X1 in central
London

4.3 Unknown Verb correction

Lets say the training data has these sentences

• He like milk→ He likes milk

• They hate the pollution → They hate pollu-
tion

This system will not be able to correct He hate
milk, because hate needs to be corrected to hates
and its grammar has no rule for hate→ hates. But
it has a rule for like→ likes. From these two rules,
the grammar extractor wont be able to derive hate
→ hates. This can be solved by splitting likes to
like s

• He like milk→ He like s milk

Now extractor will have a rule for this training sen-
tence.

• X → He X1 milk, He X1 s milk

939



• X → hate, hate

Using these two rules it generates He hate s milk
from He hate milk. Later we combine all the split
verbs to get He hate s milk.

4.4 Word insertion errors
As the name suggests these errors are due to miss-
ing words, e.g.,

• The court deemed necessary that she respond
to the summons. → The court deemed it nec-
essary that she respond to the summons.

For this example the grammar rule extracted is :-

• X → X1 deemed X2, X1 deemed it X2
4.5 Reordering errors
Reordering errors arise due to incorrect ordering
of the subject object verb, e.g.,

• Given Hindi sentence:- s��V~ l l�Xn m� EgrA
h�Elko=Vr

• Transliteration of Hindi sentence is:- sentrala
landana me giraa helicoptera

• The correct translation of this sentence is:-
helicopter crash in central London

• Output translation from Hindi-English trans-
lation system of this sentence:- central down
in London helicopter.

If the output translation and correct translation is
added to the training corpus of grammar correction
system such as,

• central down in London helicopter → heli-
copter down in central London.

we can obtain the correct translation.

5 Experiments

Now we present the data set and evaluation tech-
niques for our experiment.

5.1 Data set
We ran the grammar correction system on the NU-
CLE (NUS Corpus of Learner English) corpus
(Dahlmeier et al., 2013). The dataset has been de-
veloped at NUS in collaboration with the Centre
for English Language Communication (CELC).
This is a parallel corpus of 50000 incorrect and
correct sentences, all aligned. We took a subset
of 4000 line training corpus, 3000 for training and
1000 for testing.

5.2 Cleaning training corpus

This is a preprocessing step before training the
grammar correction system. This was primarily
due to the presence of noisy data like:-

1. HYPERLINK- http://en.wikipedia.org/wiki/

2. Bracketed information:- (DoD) {Common
Access Card}

3. Citations:- (Ben, 2008)

4. Presence of sentence pairs without any
changes.

6 Results

We present the results of SMT based grammar cor-
rection in table 1. The results show improvement
in BLEU score with increase in the size of training
corpus. The baseline is the system which passes
incorrect sentences as such i.e., performs no cor-
rection. We wanted to check what the bleu score
would be when no correction is incorporated.

Size of
training
corpus
(sen-
tences)

Size of
tuning
corpus
(sen-
tences)

Size of
testing
corpus
(sen-
tences)

BLEU
score

Baseline 0.7551
1000 1000 1000 0.7668
2000 1000 1000 0.7679
3000 1000 1000 0.7744

Table 1: Variation of accuracy with variation of
training size

7 Conclusion

We have shown how a hierarchical phrase-based
MT system like Joshua could be used as a gram-
mar correction system. We observed that increas-
ing training data definitely increases accuracy be-
cause patterns in grammar correction keep repeat-
ing even if test data is completely different from
training set. In future work, we would like to con-
centrate on “unknown word handling”.

940



References
Chris Brockett, William B. Dolan and Michael Gamon.

2006. Correcting ESL errors using phrasal SMT
techniques. ACL ’06, Sydney, Australia.

Daniel Dahlmeier, Hwee Tou Ng and Siew Mei Wu.
2013. Building a Large Annotated Corpus of
Learner English: The NUS Corpus of Learner En-
glish.. Proceedings of the 8th Workshop on Innova-
tive Use of NLP for Building Educational Applica-
tions. BEA 2013, Atlanta, Georgia, USA.

David Chiang. 2005. A Hierarchical Phrase-Based
Model for Statistical Machine Translation. Proceed-
ings of the 43rd Annual Meeting on Association for
Computational Linguistics. ACL ’05, Ann Arbor,
Michigan.

Emi Izumi, Kiyotaka Uchimoto, Toyomi Saiga, Thep-
chai Supnithi and Hitoshi Isahara. 2003. Automatic
error detection in the Japanese learners’ English
spoken data. Proceedings of the 41st Annual Meet-
ing on Association for Computational Linguistics-
Volume 2:145–148. ACL ’03, Sapporo, Japan.

Joel R. Tetreault and Martin Chodorow . 2008. The
ups and downs of preposition error detection in ESL
writing. COLING ’08, Manchester, United King-
dom.

Juri Ganitkevitch, Yuan Cao, Jonathan Weese, Matt
Post and Chris Callison-Burch. 2012. Joshua 4.0:
packing, PRO, and paraphrases. Proceedings of the
Seventh Workshop on Statistical Machine Transla-
tion. WMT ’12, Montreal, Canada.

Kenneth Heafield. 2011. KenLM: faster and
smaller language model queries. Proceedings of the
Sixth Workshop on Statistical Machine Translation.
WMT ’11, Edinburgh, Scotland.

Kishore Papineni, Salim Roukos and Todd Ward and
Wei-Jing Zhu. 2002. BLEU: a method for auto-
matic evaluation of machine translation. Proceed-
ings of the 40th Annual Meeting on Association for
Computational Linguistics. ACL ’02, Philadelphia,
Pennsylvania.

Martin Chodorow and Claudia Leacock. 2000. An Un-
supervised Method for Detecting Grammatical Er-
rors. NAACL 2000, Seattle, Washington.

Matthieu Hermet and Alain Désilets. 2009 Using first
and second language models to correct preposition
errors in second language authoring. EdAppsNLP
’09, Boulder, Colorado.

Omar F. Zaidan. 2009. Z-MERT: A Fully Configurable
Open Source Tool for Minimum Error Rate Training
of Machine Translation Systems. The Prague Bul-
letin of Mathematical Linguistics, 91:79–88.

Percy Liang, Ben Taskar and Dan Klein. . 2006. Align-
ment by agreement. Proceedings of the main confer-
ence on Human Language Technology Conference

of the North American Chapter of the Association of
Computational Linguistics. HLT-NAACL ’06, New
York.

Peter F. Brown, Vincent J. Della Pietra, Stephen A.
Della Pietra and Robert L. Mercer. . 1993. The
mathematics of statistical machine translation: pa-
rameter estimation. Computational Linguistics,
1993.

Rachele De Felice and Stephen G Pulman. 2008. A
classifier-based approach to preposition and deter-
miner error correction in L2 English. COLING ’08,
Manchester, United Kingdom.

Tomoya Mizumoto, Yuta Hayashibe, Mamoru Ko-
machi, Masaaki Nagata and Yuji Matsumoto. 2012.
The Effect of Learner Corpus Size in Grammatical
Error Correction of ESL Writings. COLING ’12,
Mumbai, India.

Y. Albert Park and Roger Levy. 2011. Automated
whole sentence grammar correction using a noisy
channel model. HLT ’11, Portland, Oregon.

Zhifei Li, Chris Callison-Burch, Chris Dyer, Juri Gan-
itkevitch, Ann Irvine, Sanjeev Khudanpur, Lane
Schwartz, Wren N. G. Thornton, Ziyuan Wang and
Jonathan Weese, and Omar F. Zaidan. 2010. Joshua
2.0: a toolkit for parsing-based machine translation
with syntax, semirings, discriminative training and
other goodies. Proceedings of the Joint Fifth Work-
shop on Statistical Machine Translation and Metrics
MATR. WMT ’10, Uppsala, Sweden.

941


