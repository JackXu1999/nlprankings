



















































'Lighter' Can Still Be Dark: Modeling Comparative Color Descriptions


Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 790–795
Melbourne, Australia, July 15 - 20, 2018. c©2018 Association for Computational Linguistics

790

‘Lighter’ Can Still Be Dark:
Modeling Comparative Color Descriptions

Olivia Winn
Computer Science Department

Columbia University
olivia@cs.columbia.edu

Smaranda Muresan
Data Science Institute
Columbia University

smara@columbia.edu

Abstract

We propose a novel paradigm of ground-
ing comparative adjectives within the
realm of color descriptions. Given a ref-
erence RGB color and a comparative term
(e.g., ‘lighter’, ‘darker’), our model
learns to ground the comparative as a di-
rection in the RGB space such that the col-
ors along the vector, rooted at the refer-
ence color, satisfy the comparison. Our
model generates grounded representations
of comparative adjectives with an average
accuracy of 0.65 cosine similarity to the
desired direction of change. These vec-
tors approach colors with Delta-E scores
of under 7 compared to the target colors,
indicating the differences are very small
with respect to human perception. Our
approach makes use of a newly created
dataset for this task derived from existing
labeled color data.

1 Introduction

Multimodal approaches to object recognition have
achieved a degree of success by grounding adjec-
tives and nouns from descriptive text in image fea-
tures (Farhadi et al., 2009; Lampert et al., 2009;
Russakovsky and Fei-Fei, 2010; Lazaridou et al.,
2015). One limitation of this approach, particu-
larly for fine-grained object recognition, is when
objects are differentiated not by having unique sets
of attributes but by a difference in the strengths
of their shared attributes (Wang et al., 2009; Duan
et al., 2012; Maji et al., 2013; Vedaldi et al., 2014).
In text, this difference is described using compar-
ative adjectives. For example, the sexual dimor-
phism of the American black duck is described
with the phrase “females tend to be slightly paler

(a) The grounding of ‘darker’ trained on teal data, applied to a teal sample

(b) The grounding of ‘darker’ trained on pink data, applied to a teal sample

Figure 1: Grounding ‘darker’

than males, with duller olive bills”.1

In a recent study of pragmatic referring expres-
sion interpretation in the context of color selection,
Monroe et al. (2017) found that speakers almost
always used comparative adjectives when the tar-
get color was very similar to a distractor, rather
than using multiple positive form adjectives to cre-
ate a highly specific description of the color in-
dependent of its surroundings. Though color has
been studied in terms of its contextual dependence
and vagueness in grounding (Egré et al., 2013;
McMahan and Stone, 2015; Monroe et al., 2016,
2017), no approaches have focused explicitly on
learning to ground comparative adjective; in this
work we focus on comparative color descriptions.

The presence of distractors in the Monroe et al.
(2017) study is important - comparatives describe
a change in a feature with respect to a reference
point. While the description light blue can be un-
derstood to represent a particular subset of col-
ors in RGB, for example, neither ‘lighter’ nor
‘lighter blue’ have explicit representations; it
is only with a reference that we can image what
color either might refer to. If the reference color is
a deep navy blue, then we imagine the target to be
much closer to navy than, for example, a sky blue.

We propose a new paradigm of learning to
ground comparative adjectives within the realm of
color descriptions: given a reference RGB color
and a comparative term (e.g. ‘lighter’, ‘paler’),

1https://www.allaboutbirds.org/guide/
American_Black_Duck/id

https://www.allaboutbirds.org/guide/American_Black_Duck/id
https://www.allaboutbirds.org/guide/American_Black_Duck/id


791

our deep learning model learns to ground the com-
parative as a direction in RB space such that the
colors along the vector, rooted at the reference
color, satisfy the comparison (Section 3). The ref-
erence color does more than quantify the specific
RGB values to apply the comparative to: it also
affects the grounding of the comparative. For ex-
ample, ‘darker’ might seem like a simple change
- simply reduce the values of all color channels
equally towards 0. But as Fig 1 shows, ‘darker’
refers to a different direction in RGB space de-
pending on the reference color, and thus we need
a reference-dependent approach.

Our approach makes use of a newly created
dataset for this task derived from an existing la-
beled color dataset (McMahan and Stone, 2015)
(Section 2). Our results in Section 5 show that
our model generates grounded representations of
comparative adjectives with an average accuracy
of 0.65 cosine similarity to the desired direction
of change. These learned vectors approach colors
with Delta-E scores of under 7 compared to the
target colors, indicating the differences are very
small with respect to human perception.

2 Data

We utilize the labeled RGB color data originally
collected by Munroe (2010), through an online
survey asking participants to provide free-form la-
bels to various RGB samples. This data was then
cleaned by McMahan and Stone (2015)2. The
cleaned data contains 821 color labels, averaging
600 RGB datapoints per label. These labels do
not contain comparative adjectives, but many start
with adjectives in the positive form (e.g., dusty,
bright). As Lassiter and Goodman (2017) write,
“Vague terms . . . are generally thought in linguis-
tic semantics to rely on a free threshold vari-
able: ‘heavy’ is interpreted as ‘heaver than θ’.”
Coming back to the example of light blue, im-
plicit in the term is the assumption that there is
a reference blue, such that light blue is under-
stood as ‘lighter’ than this reference. By rep-
resenting this referential blue with the blue RGB
samples from the data, we can assume the light
blue RGB samples are ‘lighter’ than these ref-
erences, giving us a quantitative θ in which to
ground ‘lighter’. Applying this process to the

2A few of the labels (such as ‘horrible’) were manually
discarded, as the corresponding set of colors were too widely
spread across RGB space for the label to be considered as
describing a distinct color.

rest of the labels, we convert the original dataset
into 415 tuples (reference color label, comparative
adjective, and target color label), such as ( blue,
‘lighter’, light blue), where each color label
is a set of RGB datapoints as in McMahan and
Stone (2015). Note that not all labels containing
quantifiers could be utilized in this manner; one
does not consider cobalt blue to be ‘more cobalt’
than the average blue. The new dataset of 415 tu-
ples contains 79 unique reference color labels and
81 unique comparatives and is made available on-
line.3

While it is reasonable to believe that the com-
parative adjective describes the relationship be-
tween the colors in general, individual pairs of col-
ors from the data may not display the appropriate
θ. Thus, we make the assumption that the compar-
ison holds true for the average of the target light
blue samples, and use the average as our ground
truth given the blue reference colors and the com-
parative adjective ‘lighter’.

3 Method

We have chosen to represent comparative adjec-
tives in RGB space as directions, such that given
ain input RGB reference color rc and a compar-
ative adjective w our model outputs a vector ~wg
pointing from rc in the direction of change in
RGB, which in training is measured against the di-
rection towars a target color tc. Fig 1 is a good
indication for why this representation is appro-
priate; our output ~wg corresponds to the rate of
change across the color bar, indicating the direc-
tion along with the degree of the compared prop-
erty increases. All points along this line are repre-
sentations of w in respect to rc.

The network architecture consists of two fully
connected layers, shown in Fig 2. The com-
parative is represented as a bi-gram to account
for comparatives which necessitate using ‘more’
(e.g. “more electric”); single-word compara-
tives are preceded by the zero vector. We used the
Google pre-trained word embeddings4 with d=300
(Mikolov et al., 2013a,b). As these vectors are two
orders of magnitude larger than the reference RGB
color, we input the reference directly into both lay-
ers of the network, helping to mitigate the loss of

3https://bitbucket.org/o_winn/
comparative_colors

4https://code.google.com/archive/p/
word2vec/

https://bitbucket.org/o_winn/comparative_colors
https://bitbucket.org/o_winn/comparative_colors
https://code.google.com/archive/p/word2vec/
https://code.google.com/archive/p/word2vec/


792

Figure 2: Network Architecture

Data # Tuples # Dtpts
Training 271 15.3M
Test (Seen Pairings) 271 2.4M
Test (Unseen Pairings) 29 0.29M
Test (Unseen Ref. Color) 63 2.4M
Test(Unseen Comparative) 41 0.38M
Test (Fully Unseen) 11 58k

Table 1: Data Split

information this dichotomy in size would other-
wise produce (an empirical study of various input
configurations determined inputting the color into
only one of the layers to be insufficient). The out-
put of the first hidden layer has d=30; each layer
reduces the dimension of the output by an order of
magnitude.

The loss function of the model has two metrics.
The first is the cosine similarity between ~wg and
the vector from rc to tc. To restrain the length
of ~wg, the second metric is the distance between
tc and the result of ~wg + rc. Training the length
of ~wg to roughly match the distance between rc
and tc helps it to capture that the difference should
be small enough to warrant a comparison rather
than separate descriptors, while still representing
enough of a difference to be comparable.

4 Experimental Setup

Table 1 shows the data split between training and
testing both in terms of tuples (#Tuples column)
and in terms of the actual datapoint instances
(#Dtpts column) for our experiments. To properly
measure the accuracy of our model, our test set
covers five input conditions:

• Seen Pairings. The reference color label, the

comparative adjective and their pairing have
been seen in the training data.

• Unseen Pairings. The reference color label
and the comparative adjective have been seen
in the training data, but not their pairing.

• Unseen Ref. Color. The reference color la-
bel, and thus all the corresponding RGB color
datapoints, have not be seen in training, while
the comparative has been seen in the training
data.

• Unseen Comparative. The comparative ad-
jective has not been seen in training, but the
reference color label has been seen.

• Fully Unseen. Neither the comparative adjec-
tive nor the reference color have been seen in
the training.

For the conditions where the reference color la-
bel has been seen in training, the actual RGB
reference color datapoints associated with the la-
bels were different from the ones used in training:
15% of the datapoints from each training reference
color label were set aside for testing, providing
RGB values close but not equivalent to those seen
in training. 10% of the reference color labels were
set aside for testing, as were 10% of the compar-
ative words; this amounted to 8 reference colors
and 8 comparatives. The number of tuples and ac-
tual datapoints instances for each test condition is
given in Table 1.

The network was trained at a 0.001 learning rate
for 800 epochs, with the output of the first layer of
dimension d=30.



793

Figure 3: Examples of learned comparatives for each test condition

5 Results

Figure 3 shows examples of learned groundings of
comparatives for each of the five test conditions
(Test Type column). It shows the reference RGB
color datapoint r

′
c (always unseen), the compara-

tive word w, the learned grounding vector ~wg, the
target color tc, and two scores: cosine similarity
and Delta-E. The upper sample for each test type
is an example of a highly accurate result, while the
lower sample exemplifies failure.

Delta-E is a metric for understanding how the
human eye perceives color differences (Table 2).
This is a useful metric as distances in RGB space
are not perceived linearly. Figure 4 shows two
example pairs of colors which are spaced equally
in terms of distance in RGB, but in terms of the
Delta-E metric the green colors are closer together.

As seen in Figure 3, grounding comparatives in
directional vectors over RGB allows them to cap-
ture a full range of modification of the reference
color. Even for some of the error cases the re-
sulting outputs tend to capture directions which
are reasonable illustrations of the color the com-
parative described. Though the ‘darker’ ground-
ing example from unseen pairings is incorrectly
de-saturating the reference color, it is also in fact
making the color darker. Most impressive is the
‘paler’ example at the bottom, which is able to
capture the direction of the comparative almost
perfectly. Regarding failures, we see that they tend

Delta-E Perception
≤ 1.0 Imperceptible
1 - 2 Requires close observation
2 - 10 Perceivable
11 - 49 More similar than opposite
100 Exact opposites

Table 2: Delta-E Ranges

to be of comparatives words that relate to a differ-
ent color, such as ‘more greenish’ and ‘bluer’,
rather than comparatives such as ‘lighter’.

Table 3 provides quantitative results in terms
of average cosine similarity and average Delta-E.
Overall, the average cosine similarity is 0.65, with
an average Delta-E of 6.8. Separating the perfor-
mance by test condition, we see that the conditions
where the reference and comparatives were both
seen perform the best (independent of whether the
pairing was seen in training); again ‘seen refer-
ence’ refers only to the label being seen and not the
reference color datapoint itself. The fully unseen
case performs the worst by far with respect to co-
sine similarity, though it is not as deviant in Delta-
E. It is again apparent that the performance of the
model drops when given comparatives which refer
to another color.

Figure 5 shows the comparative ‘electric’ ap-
plied to colors outside of our dataset. With no
known tcs we cannot quantitatively measure the
accuracy, but we can qualitatively assess the re-



794

Figure 4: Same RGB distance, different Delta-E

Test Condition Avg Cos Avg Delta-E
Seen Pairings 0.68 6.1
Unseen Pairings 0.68 7.9
Unseen Ref. Color 0.40 11.4
Unseen Comparative 0.41 10.5
Fully Unseen -0.21 15.9
Overall 0.65 6.8

Table 3: Results

sults as plausible.
We also examined whether the model could

generate plausible comparative terms given a rc
and tc. All of the comparatives in the model’s vo-
cabulary were applied to rc, and the correspond-
ing ~wg were sorted by cosine similarity to given
reference-target direction. When given a green
reference and a dark green target (both sampled
from the test data), the model outputs ‘truer’,
‘deeper’, and ‘darker’ as the closest compara-
tives.

In Figure 6, given a reference sampled from
‘purple’ and a target sampled from ‘soft
purple’, the model outputs the 5 most plausible
comparatives - ‘softer’ was the 9th closest. They
are presented in descending order by distance be-
tween the target color and its projection on the
modifying vector. We see that the comparatives
the model returns are semantically very similar, as
are their corresponding ~wg vectors.

6 Related Work

Though color has been studied in terms of its con-
textual dependence and vagueness in grounding

Figure 5: Groundings for ‘more electric’

Figure 6: Top comparatives generated by the
model

(Egré et al., 2013; McMahan and Stone, 2015;
Monroe et al., 2016, 2017), no approaches have
focused explicitly on learning to ground compar-
atives. Related to this work is that of image
ranking, which is inherently a form of compar-
ison (Parikh and Grauman, 2011; Yu and Grau-
man, 2014). However, ranking methods do not
ground the comparatives themselves in image fea-
tures. Besides the fact that no ranked color data
exists, ranking methods are not flexible enough to
handle the high dependence of color comparatives
on the individual reference color.

7 Conclusion

We propose a new paradigm of grounding com-
parative adjectives describing colors as directions
in RGB space such that the colors along the vector,
rooted at the reference color, satisfy the compari-
son. We introduce a new methodology for trans-
forming labeled color data into comparative color
data, and propose a simple but effective learning
model that is able to accurately modify unseen col-
ors and comparatives. With respect to the desired
output, the representations have an average accu-
racy of 0.65 cosine similarity, and average Delta-
E scores of under 7. Our model can also pro-
vide plausible descriptions of the difference be-
tween a given reference and target pair, as well as
the grounded representations of the comparatives
generated, providing an explanation for the model
decision. This model is the first step towards
fine-grained object recognition through compar-
ative descriptions, providing a way to utilize re-
lational descriptive text. This approach could be
extended to other properties such as size, tex-
ture, or curvature. It could also be used to aid
in zero-shot learning from text sources, generat-
ing human-understandable explanations for cate-
gorization of similar objects, or providing descrip-
tions of new, unknown objects with respect to
known ones.



795

References
Kun Duan, Devi Parikh, David Crandall, and Kristen

Grauman. 2012. Discovering localized attributes
for fine-grained recognition. In Computer Vision
and Pattern Recognition (CVPR), 2012 IEEE Con-
ference on, pages 3474–3481. IEEE.

Paul Egré, Vincent De Gardelle, and David Ripley.
2013. Vagueness and order effects in color catego-
rization. Journal of Logic, Language and Informa-
tion, 22(4):391–420.

Ali Farhadi, Ian Endres, Derek Hoiem, and David
Forsyth. 2009. Describing objects by their at-
tributes. In Computer Vision and Pattern Recogni-
tion, 2009. CVPR 2009. IEEE Conference on, pages
1778–1785. IEEE.

Christoph H Lampert, Hannes Nickisch, and Stefan
Harmeling. 2009. Learning to detect unseen object
classes by between-class attribute transfer. In Com-
puter Vision and Pattern Recognition, 2009. CVPR
2009. IEEE Conference on, pages 951–958. IEEE.

Daniel Lassiter and Noah D Goodman. 2017. Adjecti-
val vagueness in a Bayesian model of interpretation.
Synthese, 194(10):3801–3836.

Angeliki Lazaridou, Georgiana Dinu, Adam Liska, and
Marco Baroni. 2015. From Visual Attributes to Ad-
jectives through Decompositional Distributional Se-
mantics. Transactions of the Association for Com-
putational Linguistics, 3(0):183–196.

Subhransu Maji, Esa Rahtu, Juho Kannala, Matthew
Blaschko, and Andrea Vedaldi. 2013. Fine-grained
visual classification of aircraft. arXiv preprint
arXiv:1306.5151.

Brian McMahan and Matthew Stone. 2015. A bayesian
model of grounded color semantics. Transactions
of the Association of Computational Linguistics,
3(1):103–115.

Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey
Dean. 2013a. Efficient estimation of word represen-
tations in vector space. In Proceedings of Workshop
at ICLR.

Tomas Mikolov, Wen-tau Yih, and Geoffrey Zweig.
2013b. Linguistic regularities in continuous space
word representations. In Proceedings of the 2013
Conference of the North American Chapter of the
Association for Computational Linguistics: Human
Language Technologies, pages 746–751.

Will Monroe, Noah D Goodman, and Christopher
Potts. 2016. Learning to generate compositional
color descriptions. In Proceedings of the 2016 Con-
ference on Empirical Methods in Natural Language
Processing.

Will Monroe, Robert XD Hawkins, Noah D Goodman,
and Christopher Potts. 2017. Colors in context: A

pragmatic neural model for grounded language un-
derstanding. Transactions of the Association for
Computational Linguistics, 5:325–338.

Randall Munroe. 2010. Color survey.

Devi Parikh and Kristen Grauman. 2011. Relative At-
tributes. ICCV, pages 503–510.

Olga Russakovsky and Li Fei-Fei. 2010. Attribute
learning in large-scale datasets. In European Con-
ference on Computer Vision, pages 1–14. Springer.

Andrea Vedaldi, Siddharth Mahendran, Stavros
Tsogkas, Subhransu Maji, Ross Girshick, Juho
Kannala, Esa Rahtu, Iasonas Kokkinos, Matthew B
Blaschko, David Weiss, et al. 2014. Understanding
objects in detail with fine-grained attributes. In
Proceedings of the IEEE conference on computer
vision and pattern recognition, pages 3622–3629.

Josiah Wang, Katja Markert, and Mark Everingham.
2009. Learning models for object recognition
from natural language descriptions. In Proceed-
ings of the 20th British Machine Vision Conference
(BMVC2009).

Aron Yu and Kristen Grauman. 2014. Fine-Grained
Visual Comparisons with Local Learning. In Pro-
ceedings of the IEEE Conference on Computer Vi-
sion and Pattern Recognition (CVPR).

https://doi.org/10.1007/s11229-015-0786-1
https://doi.org/10.1007/s11229-015-0786-1
http://arxiv.org/abs/arXiv:1501.02714v1
http://arxiv.org/abs/arXiv:1501.02714v1
http://arxiv.org/abs/arXiv:1501.02714v1
https://blog.xkcd.com/2010/05/03/color-survey-results/

