



















































Relevant Emotion Ranking from Text Constrained with Emotion Relationships


Proceedings of NAACL-HLT 2018, pages 561–571
New Orleans, Louisiana, June 1 - 6, 2018. c©2018 Association for Computational Linguistics

Relevant Emotion Ranking from Text Constrained with Emotion
Relationships

Deyu Zhou† Yang Yang† Yulan He§
† School of Computer Science and Engineering, Key Laboratory of Computer Network

and Information Integration, Ministry of Education, Southeast University, China
§ School of Engineering and Applied Science, Aston University, UK
{d.zhou,yyang}@seu.edu.cn, y.he@cantab.net

Abstract

Text might contain or invoke multiple emo-
tions with varying intensities. As such, emo-
tion detection, to predict multiple emotions as-
sociated with a given text, can be cast into a
multi-label classification problem. We would
like to go one step further so that a ranked list
of relevant emotions are generated where top
ranked emotions are more intensely associat-
ed with text compared to lower ranked emo-
tions, whereas the rankings of irrelevant emo-
tions are not important. A novel framework of
relevant emotion ranking is proposed to tackle
the problem. In the framework, the objective
loss function is designed elaborately so that
both emotion prediction and rankings of on-
ly relevant emotions can be achieved. More-
over, we observe that some emotions co-occur
more often while other emotions rarely co-
exist. Such information is incorporated into
the framework as constraints to improve the
accuracy of emotion detection. Experimental
results on two real-world corpora show that
the proposed framework can effectively deal
with emotion detection and performs remark-
ably better than the state-of-the-art emotion
detection approaches and multi-label learning
methods.

1 Introduction

With the growing prosperity of Web 2.0, people
tend to share their feelings, attitudes and opin-
ions through the social platforms such as online
news sites, blogs. Detecting emotions from tex-
t can enhance the understanding of users’ emo-
tional states, which is useful in many downstream
applications, such as human-computer interaction
and personalized recommendation. Therefore, it is
crucial to analyze and predict emotions from text
accurately (Picard and Picard, 1997).

Research on emotion detection can be rough-
ly categorized into two types: lexicon-based and

learning-based approaches. Lexicon-based ap-
proaches usually rely on emotion lexicons (Lei
et al., 2014; Rao et al., 2012). They cannot deal
with text when words can’t be found in emo-
tion lexicons. Learning-based approaches can be
furthered classified into unsupervised and super-
vised learning methods. Unsupervised approach-
es do not require annotated data for training. For
example, by adding an emotion layer into tradi-
tional topic models, emotion-topic models were
constructed to detect users’ emotions (Bao et al.,
2012, 2009). Supervised learning approaches con-
sider each emotion category as a class label and
emotion detection is cast as a classification prob-
lem. If only choosing the strongest emotion as the
emotion label for a given text, emotion detection
is essentially a single-label classification prob-
lem (Lin et al., 2008; Quan et al., 2015). To predic-
t multiple emotions simultaneously, emotion de-
tection can be solved in the multi-label classifica-
tion framework (Bhowmick, 2009). Moreover, to
predict both multiple emotions and their intensi-
ties, some approaches have been proposed using
emotion distribution learning (Zhou et al., 2016).
Some lexicon-based approaches such as (Wang
and Pal, 2015) can also output multiple emotions
with intensities using non-negative matrix factor-
ization.

In this paper, we are interested in exploring e-
motion ranking from either readers’ perspective
or writers’ perspective in two different real-world
corpora. In both cases, a given text is associated
with multiple emotions. For example, Figure 1 il-
lustrates an online news article crawled from Sina
News Society Channel together with readers’ emo-
tion votes. It can be observed that when read-
ing the news article, readers expressed differen-
t emotions with the majority showed “Sadness”
and “Anger”. We notice that some emotions such
as “Touching”, “Curiosity” and “Amusement” on-

561



2-year-old baby found abandoned in garbage heap by his runaway mother and drug-

taking father

Recently, a netizen seek help for a 2-year-old baby who is alone at home unattended and

starving because of his runaway mother and drug-taking father. According to the

published pictures, the baby lives in a messy home with garbage everywhere. ……

妈妈出走爸爸吸毒 2岁娃无人管活在恶臭垃圾堆
近日网友发求助称因母亲离家出走父亲长期吸毒精神不正常，留下2岁的小“臭蛋”独自在家
无人照料甚至连吃的都没有。在发布的图片中,小“臭蛋”居住的家里凌乱不堪垃圾地。……

Figure 1: An example of an online news article from
Sina Society Channel with voted emotions.

ly received 1 to 3 votes. In comparison to the to-
tal number of votes received, these votes could be
considered as outliers or irrelevances. Also, the
extremely low emotion votes might be due to read-
ers’ clicking errors. Taking into account such e-
motions during the learning process could intro-
duce bias. Therefore, we aim to differentiate rele-
vant emotions from irrelevant ones and only learn
the rankings of relevant emotions while neglecting
the irrelevant ones.

Our work makes the following contributions:

• We propose a novel framework based on rel-
evant emotion ranking to identify multiple e-
motions and produce the rankings of relevant
emotions from text. In the framework, the
objective emotion loss function is designed
elaborately so that both emotion prediction
and rankings for only relevant emotions are
achieved without being affected by irrelevan-
t ones. To the best of our knowledge, it is
the first attempt to perform emotion detec-
tion and relevant emotion ranking at the same
time.

• As some emotions co-occur more often while
others rarely co-exist, the prior knowledge of
emotion relationships is incorporated into the
framework as a constraint. Such emotion re-
lationship can provide important cues for e-
motion detection.

• Experimental results on two real-world cor-
pora show that the proposed framework can
effectively deal with the emotion detection
problem and performs better than the state-
of-the-art emotion detection methods and
multi-label learning methods.

2 Related work

Emotion detection is one of the subfields of sen-
timent analysis where emotions are more fine-
grained and expressive. In general, emotion detec-
tion approaches can be categorized into two types:
lexicon-based and learning-based approaches.

Lexicon-based approaches usually rely on emo-
tion lexicons consisting of words and their cor-
responding emotion labels. For example, Aman
and Szpakowicz (2007) classified emotional and
non-emotional sentences with a predefined emo-
tion lexicon. Emotional dictionaries could also be
constructed from training corpora of news articles
and be used to predict the readers’ emotion of a
new articles (Lei et al., 2014; Rao et al., 2012).
Agrawal and An (2012) proposed a context-based
approach to detect emotions from text at sentence
level. An emotion vector for each potential affec-
t bearing word based on the semantic relation be-
tween emotion concepts and words was generated.
The emotion vector was then tuned based on the
syntactic dependencies within a sentence struc-
ture. Other lexicon-based approach such as (Wang
and Pal, 2015) can also output multiple emotions
with intensities using non-negative matrix factor-
ization with constraints derived based on an emo-
tion lexicon.

Learning-based approaches can be further cat-
egoried as unsupervised and supervised learning
methods. Unsupervised learning approaches do
not require labelled data for training. For exam-
ple, the emotion-topic models (Bao et al., 2012,
2009) were proposed by adding an extra emotion
layer into traditional topic models such as Latent
Dirichlet Allocation (Blei et al., 2003), thus cap-
turing the generation of both emotion and text at
the same time.

Supervised learning approaches typically cast
emotion detection as a classification problem by
considering each emotion category as a class label.
If only choosing the strongest emotion as the label
for a given text, emotion detection is essentially
a single-label classification problem. Lin, Yang
and Chen (2008) studied the classification of news
articles into different categories based on reader-
s’ emotions with various combinations of feature
sets. Strapparava and Mihalcea (2008) proposed
several knowledge-based and corpus-based meth-
ods for emotion classification. Quan et al. (2015)
proposed a logistic regression model with emotion
dependency for emotion detection. Latent vari-

562



ables were introduced to model the latent structure
of input text. To predict multiple emotions simul-
taneously, emotion detection can be solved using
multi-label classification. Bhowmick (2009) pre-
sented a method for classifying news sentences in-
to multiple emotion categories using an ensemble
based multi-label classification technique. Zhou
et al. (2016) proposed a novel approach based on
emotion distribution learning to predict multiple
emotions with different intensities in a single sen-
tence.

3 Methodology

Assuming a set of T emotions E = {e1, e2, ...eT }
and a set of n instances X = {x1, x2, x3, ..., xn},
each instance xi ∈ Rd is associated with a ranked
list of its relevant emotions Ri ⊆ E and also a
list of irrelevant emotions Ri = E − Ri. Rele-
vant emotion ranking aims to learn a score func-
tion g(xi) = [g1(xi), ..., gT (xi)] assigning a s-
core gt(xi) to each emotion et, (t ∈ {1, ..., T}).
As mentioned before, it is unnecessary to consid-
er the rankings of irrelevant emotions since they
might introduce errors into the model during the
learning process. In order to differentiate relevant
emotions from irrelevant ones, we need to define a
threshold gΘ(x) which could be simply set to 0 or
learned from data (Fürnkranz et al., 2008). Those
emotions with scores lower than the threshold will
be considered as irrelevant and hence discarded.
The identification of relevant emotions and their
ranking can be obtained simultaneously according
to their scores assigned by the ranking function g.
Here, the predicted relevant emotions of instance
xi are denoted as R̂i = {et ∈ E|gt(xi)>gΘ(xi)}.

3.1 Emotion Loss Function

The goal of relevant emotion ranking is to learn the
parameter of the ranking function g. Without loss
of generality, we assume that g are linear models,
i.e., gt(xi) = w

ᵀ
t · xi, t ∈ {1, 2, 3, ..., T} ∪ {Θ},

where Θ denotes the threshold. Relevant emo-
tion ranking can be regarded as a special case of
multi-label learning. Several evaluation criteria
typically used in multi-label learning can also be
used to measure the ranking function’s ability of
distinguishing relevant emotions from irrelevan-
t ones, such as hamming loss, one error, coverage,
ranking loss, and average precision as suggested
in (Zhang and Zhou, 2014). However, these multi-
label criteria cannot meet our requirement exactly

as none of them considers the ranking among emo-
tions which are considered relevant. Therefore, by
incorporating PRO loss (Xu et al., 2013), the loss
function for the instance xi is defined as follows:

L(xi, Ri,≺,g) =
∑

et∈Ri∪{Θ}

∑

es∈≺(et)

1

normt,s
lt,s

(1)
where et refers to the emotion belonging to rele-

vant emotion set Ri or the threshold Θ of instance
xi while es refers to the emotion which is less rel-
evant than et denoted as ≺. Thus, (et, es) repre-
sents four types of emotion pairs: i.e., (relevant,
relevant), (relevant, irrelevant), (relevant, thresh-
old), and (threshold, irrelevant). The normaliza-
tion term normt,s is used to balance those four
types of emotion pairs to avoid dominated terms
by their respective set sizes. The set sizes of the
four different types of emotion pairs mentioned
above are |Ri| × (|Ri| − 1)/2, |Ri| × |Ri|, |Ri|,
and |Ri|, respectively. Here, lt,s refers to a modi-
fied 0-1 error. Specifically,

lt,s =





1, gt(xi) < gs(xi)
1
2 , gt(xi) = gs(xi)

0, otherwise

Note that lt,s is non-convex and difficult to op-
timize. Thus, a large margin surrogate convex
loss (Vapnik and Vapnik, 1998) implemented in
hinge form is used instead as follows:

L̂(xi,Ri,≺,g) =
∑

et∈Ri∪{Θ}

∑

es∈≺(et)
1

normt,s
(1 + gs(xi)− gt(xi))+

(2)

where (u)+ = max{0, u}.
However, Eq. 2 ignores the relationships be-

tween different emotions. As mentioned in In-
troduction section, some emotions often co-occur
such as “joy” and “love” while some rarely co-
exist such as “joy” and “anger”. Such relation-
ship information among emotions can provide im-
portant clues for emotion ranking. Therefore, we
incorporate this information into the emotion loss
function as constraints. The objective function

563



L̂(xi, Ri,≺,g) can be redefined as:

L̂ω(xi, Ri,≺,g) =
∑

et∈Ri∪{Θ}

∑

es∈≺(et)

1

normt,s
×

(1 + gs(xi)− gt(xi) + ωts(wt − ws))+
(3)

where the weight ωts models the relationship
between the t-th emotion and the s-th emotion in
the emotion set and can be calculated in multiple
ways. Since the Pearson correlation coefficient
(Nicewander, 1988) is the most familiar measure
of relationship between two variables, we use it
to measure the relationship of two emotions using
their original emotion scores across each corpus.

From the above, it can be observed that the
goal of relevant emotion ranking can be achieved
through predicting an accurate relevant emotion
set as well as the ranking of relevant emotions.

3.2 Relevant Emotion Ranking
After defining an appropriate loss function, we
need to define a way to minimize the empirical er-
ror measured by the appropriate loss function and
at the same time to control the complexity of the
resulting model. It can be done by introducing
a maximum margin strategy and regularization to
deal with emotion ranking data, where a set of lin-
ear classifiers are optimized to minimize the emo-
tion loss function mentioned before while having
a large margin. We could potentially use an ap-
proach based on a label ranking method (Elisseeff
and Weston, 2001). It is worth mentioning that the
margin of the (relevant, relevant) label pair needs
to be dealt with carefully, which is not considered
in (Elisseeff and Weston, 2001).

The learning procedure of relevant emotion
ranking (RER) is illustrated in Figure 2. The big
rectangular dash line boxes denoted by x1 to xn
represent n instances in the training set. In each
small box, ei, i ∈ {1, ...T} ∪ {Θ} represents an
emotion of the instance where the shaded smal-
l boxes represent the relevant emotions while the
dashed small boxes represent irrelevant ones and
the last one eΘ is the threshold. Each emotion’s
corresponding weight vector is wi. We use mt,s
to represents the margin between label et and es.
There are four types of emotion pairs’ margins in
total, i.e., (relevant, relevant), (relevant, irrelevan-
t), (relevant, threshold), and (threshold, irrelevan-
t). Different types of emotion pairs’ margins are

denoted using different text/line colors. For each
training instance xi, margin(xi) represents the
margin of instance xi which can be obtained by
taking the minimum margin of all its possible la-
bel pairs mt,s. Similarly, the margin of the learn-
ing system margin(learningsystem) can be ob-
tained by taking the minimum margin of all the
training instances. By maximizing the margin of
the learning system, the weight vector of each e-
motion can be derived from which the predicted
emotion set and the ranking of relevant emotions
can be obtained.

The learning system is composed of T + 1 lin-
ear classifiers [w1; ...;wT ;wΘ] with one classifier
for each emotion label and the threshold, where
wt, t ∈ {1, ...T} ∪ {Θ} is the weight vector for
the t-th classifier of emotion et. For a training in-
stance xi and its corresponding emotion label set
Ei, the learning system’s margin on instance xi is
defined as follows by considering its ranking abil-
ity on xi’s four types of emotion pairs, i.e., (rel-
evant, relevant), (relevant, irrelevant), (relevant,
threshold), and (threshold, irrelevant):

min
et∈Ri∪{Θ},es∈≺(et)

〈wt − ws, xi〉
||wt − ws||

(4)

Here, 〈u, v〉 returns the inner product u>v.
For each emotion pair (et, es), its discrimination
boundary corresponds to the hyperplane 〈wt −
ws, xi〉 = 0. Therefore, Eq. 4 returns the mini-
mum value as the margin on instance xi. The mar-
gin on the whole training set G can be calculated
as follows:

min
xi∈G

min
et∈Ri∪{Θ},es∈≺(et)

〈wt − ws, xi〉
||wt − ws||

(5)

If the learning algorithm is capable of properly
ranking the four types of label pairs for each train-
ing instance, Eq. 5 will return a positive margin.
In this ideal case, the final goal is to maximize the
margin in Eq. 5:

max
wj

min
xi∈G

min
et∈Ri∪{Θ},es∈≺(et)

1

||wt − ws||
s.t.〈wt − ws, xi〉 ≥ 1, 1 ≤ i ≤ n, 1 ≤ j ≤ T + 1

(6)

564



x1

e1

e2

e4

e3

w1

w2

w4

w3

g1(x1)

g2(x1)

g3(x1)

g4(x1)

xn

e1

e2

e4

e3

w1

w2

w4

w3

g1(xn)

g2(xn)

g3(xn)

g4(xn)

margin(x1)=min(m12,m13,m14,m23,m24,m1 ,m2  ,m 3 ,m 4) margin(xn)=min(m12,m13,m14,m1 ,m 2 ,m 3 ,m 4)

margin(learning system)=min(margin(x1),…,margin(xn))

max(margin(learning system))
(relevant, relevant) (relevant, irrelevant) (relevant, threshold) (threshold, irrelevant)

e  w    g  (x1) e   w   g  (xn)

Figure 2: The overall framework of our proposed Relevant Emotion Ranking (RER) method.

Suppose we have sufficient training examples
such that for each label pair (et, es), there ex-
ists xi ∈ Gsatisfying et ∈ Ri ∪ {Θ}, es ∈≺ (et).
Thus, the objective in Eq.6 becomes equivalent to
maxwj min1≤s<t≤T+1

1
||wt−ws|| and can be rewrit-

ten as minwj max1≤s<t≤T+1 ||wt − ws||.
Moreover, to overcome the complexity brought

in by the max operator, the objective of the op-
timization problem can be re-written by approx-
imating the max operator with the sum operator.
Thus, the objective of Eq. 6 can be transformed
as:

min
wj

T+1∑

t=1

||wt||2

s.t. 〈wt − ws, xi〉 ≥ 1, 1 ≤ i ≤ n, (7)
1 ≤ j ≤ T + 1, et ∈ Ri ∪ {Θ}, es ∈ ≺ (et)

To accommodate real-world scenarios where
constraints in Eq. 7 can not be fully satisfied, s-
lack variables can be incorporated into the objec-
tive function:

min
wj ,ξits

T+1∑

t=1

||wt||2 + λ
n∑

i=1

∑

et∈Ri∪{Θ}

∑

es∈≺(et)

1

normt,s
ξits

s.t.〈wt − ws, xi〉 ≥ 1− ξits, 1 ≤ j ≤ T + 1, ξits ≥ 0
(8)

Since ξits does not need to be optimized since
it can be easily determined by wt, ws. The final
objective function can be reformulated as:

min
wt,L̂

T+1∑

t=1

||wt||2 + λ
n∑

i=1

L̂(xi, Ri,≺,g) (9)

As can be seen, Eq.9 consists of two parts bal-
anced by the trade-off parameter λ. Specifically,
the first part corresponds to the maximum margin
of the learning system and it can also represent the
complexity of the learning system, while the sec-
ond part corresponds to the emotion loss function
of the learning system implemented in hinge form.

3.3 Parameter Estimation
Let w = [w1; ...;wT ;wΘ], Eq. 9 is cast into a gen-
eral form in SVM-type:

min
w,ξ

1

2
||w||2 + λC>ξ

s.t. Aw ≥ 1p − ξ, ξ ≥ 0p
(10)

where p is the total number of label pairs, cal-
culated by

∑n
i=1

∑
et∈Ri∪{Θ}

∑
es∈≺(et) normt,s

and 1p(0p) is the p × 1 all one (zero) vector. The
entries in vector C correspond to the weights of
hinge losses, i.e., the normalization term to bal-
ance the four kinds of label pairs. The matrix A
corresponds to the constraints for instances which
reflects the emotion relationships and the margin
of the label pairs.
ξ does not need to be optimized since it can be

easily determined by w. Hence the objective func-
tion can be reformulated into the following form
without ξ:

min
w

F (w, G) =
1

2
||w||2 + λC>(1p −Aw)+

(11)

565



Through minimizing the objective function
F (w, G), we can finally obtain parameter w and
the ranking function g. Eq. 11 involves a large
scale optimization. To address Eq. 11, we con-
sider an efficient Alternating Direction Method of
Multipliers (ADMM) solution (Bertsekas and T-
sitsiklis, 1989). The basic idea of ADMM is to
take the decomposition-coordinate procedure such
that the solution of subproblems can be coordi-
nated to find the solution to the original problem.
We decompose G into M disjoint subsets, i.e.,
{G1, G2, ..., GM} and then Eq. 11 is converted in-
to the following form:

min
w0,w1,wm

M∑

m=1

F (wm, Gm),

s.t. wm = w0,∀m = 1, ...,M
(12)

The surrogate augmented Lagrangian Function
(LF) was introduced into Eq. 12 and it was cast
into the following form:

LF ({w0,w1, ...,wm}, {αm}Mm=1, β) =
M∑

m=1

F (wm, Gm)

+

M∑

m=1

(αm)>(wm −w0) + β
2

M∑

m=1

||wm −w0||2

(13)

where α, β are the Lagrange multiplies. The up-
dating process of Eq. 13 is shown in Algorithm 1.

Algorithm 1 Parameter updating process.
1: Decompose data setG intoM disjoint subsets

i.e., {G1, G2, ..., GM}. Set iteration i = 0.
2: Initialize {w00,w10, ...,wM0 , α10, ..., αM0 } as

zeros.
3: while not converged do
4: Set i = i+ 1
5: Update w0i , {wmi , αmi }Mm=1 as:

{wmi }Mm=1 =
argmin
w1..wm

LF (w0i−1, {wmi−1, αmi−1}Mm=1, β)

w0i = argmin
w0

LF (w0, {wmi−1, αmi−1}Mm=1, β

αmi = α
m
i−1 + β(w

m
i − w0i )>,∀m =

1, 2, ...,M
6: end while

Output: Final w0

4 Experiments

4.1 Setup

We evaluate the proposed approach on two real-
world corpora, one is document level and the other
is sentence level:
Sina Social News (News) was collected from
the Sina news Society channel where readers can
choose one of the six emotions such as Amusemen-
t, Touching, Anger, Sadness, Curiosity, and Shock
after reading a news article. As Sina is one of the
largest online news sites in China, it is sensible to
carry out experiments to explore the readers’ emo-
tion (social emotion). News articles with less than
20 votes were discarded since few votes can not
be considered as proper representation of social
emotion. In total, 5,586 news articles published
from January 2014 to July 2016 were kept, togeth-
er with the readers’ emotion votes.
Ren-CECps corpus (Blogs) (Quan and Ren,
2010) contains 34,719 sentences selected from
blogs in Chinese. Each sentence is annotated with
eight basic emotions from writer’s perspective, in-
cluding anger, anxiety, expect, hate, joy, love,
sorrow and surprise, together with their emotion
scores indicating the level of emotion intensity
which range from 0 to 1. Higher scores represents
higher emotion intensity.

The statistics of the two corpora are shown in
Table 1.

Sina Social News Ren-CECps Corpus
Category #Votes Category #Scores
Touching 694,006 Joy 1,349.6
Shock 572,651 Hate 6,103.9
Amusement 869,464 Love 2,911.1
Sadness 837,431 Sorrow 2,042.5
Curiosity 212,559 Surprise 3,873.9
Anger 1,109,315 Anger 7,832.1

Anxiety 5,006.4
Expect 610.4

All 4,295,426 All 29,729.9

Table 1: Statistics for the two corpora used in our ex-
periments.

The two corpora were preprocessed by using
word segmentation and filtering. The python jie-
ba segmenter is used for the segmentation and a
removal of stop words is performed based on a
stop word thesaurus. Words appeared only once
or appeared in less than two documents were re-

566



moved to alleviate data sparsity. We used the s-
ingle layer long short-term memory (LSTM) net-
works (Hochreiter and Schmidhuber, 1997) to ex-
tract the features of each text. LSTM is one kind
of recurrent neural networks, which can capture
sequence information from text and can represen-
t meanings of inputs in the reduced dimensional
space. It treats text as a sequence of word embed-
dings and outputs a state vector over each word,
which contains the information of the previous
words. The final state vector can be used as the
representation of the text. In our experiments, we
set the dimension of each text representation to
100. During LSTM model training, we optimized
the hyper parameters using a development dataset
which is built using external data. We train LST-
M using a learning rate of 0.001, a dropout rate of
0.3 and categorical cross-entropy as the loss func-
tion. The mini batch (Cotter et al., 2011) size is
set to 32. After that, the learned text representa-
tions are fed into the proposed system for relevant
emotion ranking as has been previously presented
in the Methodology section.

4.2 Comparison with Baselines
There are only few baselines which address multi-
ple emotions learning from text. We first compare
the proposed framework with two baselines which
have previously achieved the state-of-the-art per-
formances on multi-emotion detection.

• Emotion Distribution Learning (EDL)
(Zhou et al., 2016): It learns a mapping func-
tion from texts to their emotion distribution-
s describing multiple emotions and their re-
spective intensities based on label distribu-
tion learning. Moreover, the relationships of
emotions are captured based on the Plutchik’s
wheel of emotions which are subsequently
incorporated into the learning algorithm in
order to improve the accuracy of emotion de-
tection.

• EmoDetect (Wang and Pal, 2015): It out-
puts the emotion distribution based on a di-
mensionality reduction method using non-
negative matrix factorization which com-
bines several constraints such as emotions
bindings, topic correlations and emotion lex-
icons in a constraint optimization framework.

For each method, 10-fold cross validation is
conducted using the same feature construction

Name Definition

PRO Loss 1n
∑n

i=1

∑
et∈Ri∪{Θ}

∑
es∈≺(et)

1
normt,s

lt,s

lt,s is a modified 0-1 error;normt,sis the set size of label pair(t, s)

Hamming Loss 1nT
∑n

i=1 |R̂i4Ri|
Ranking Loss 1n

∑n
i=1(

∑
(et,es)∈Ri×Ri δ[gt(xi) < gs(xi)])/(|Ri| × |Ri|)

where δ is the indicator function.

One Error 1n
∑n

i=1 δ[argmax
et

gt(xi) /∈ Ri]

Average Precision 1n
∑n

i=1
1
|Ri|×

(
∑

t:et∈Ri
|{es ∈ Ri|gs(xi) > gt(xi)}|)/(|{es|gs(xi) > gt(xi)}|)

Coverage 1n
∑n

i=1 maxt:et∈Ri |{es|gs(xi) > gt(xi)}|
Subset Accuracy 1n

∑n
i=1 δ[R̂i = Ri]

F1exam
1
n

∑n
i=1 2|Ri ∩ R̂i|/(|Ri|+ |R̂i|)

MicroF1 F1(
T∑
t=1

TPt,
T∑
t=1

FPt,
T∑
t=1

TNt,
T∑
t=1

FNt)

MacroF1 1T
T∑
t=1

F1(TPt, FPt, TNt, FNt)

Table 2: Evaluation criteria for the Multi-Label Learn-
ing (MLL) methods. TPt, FPt, TNt, FNt represent
the number of true positive, false positive, true nega-
tive, and false negative test examples with respect to
emotion t respectively. F1(TPt, FPt, TNt, FNt) rep-
resent specific binary classification metric F1 (Man-
ning et al., 2008).

method to get the final performance. Supposing
n test instances and T emotion categories, sever-
al evaluation criteria as presented in Table 2 typ-
ically used in multi-label learning can be used to
measure the efficiency of the proposed framework
and the baseline approaches. PRO Loss concern-
ing the prediction on all labels as well as the rank-
ings of only relevant labels. Hamming loss eval-
uates how many times an emotion label is mis-
classified. Ranking loss evaluates the fraction of
reversely ordered emotion pairs. One-error eval-
uates the fraction of sentences whose top-ranked
emotion is not in the relevant emotion set. Aver-
age precision evaluates the average fraction of the
relevant emotions ranked higher than a particular
emotion. Coverage evaluates how many steps are
needed to move down the ranked emotion list so as
to cover all the relevant emotions of the example.
Subset Accuracy evaluates the fraction of correct-
ly classified examples, i.e. the predicted label set
is identical to the ground-truth label set. F1exam
evaluates the averaged F1 (Manning et al., 2008)
over instances. MicroF1 pools each document de-
cisions across categories, and then computes an
effectiveness measure on the pooled contingency
table. MacroF1 take the average of F1 for all cat-
egories. Note that the threshold Θ is removed be-
fore evaluation. It should be pointed out that met-
rics from PRO Loss to F1exam work by evaluating
the learning systems performance on each test ex-

567



ample separately, and then returning the mean val-
ue across the test set. MicroF1 and MacroF1 work
by evaluating the learning systems performance on
each emotion category separately, and then return-
ing the macro/micro-averaged value across all e-
motion categories.

The evaluation results using 10 different eval-
uation criteria are shown in Table 3. It can be
observed that our proposed method Relevant E-
motion Ranking(RER) outperforms baseline ap-
proaches on all 10 evaluation metrics on both
datasets.

Datasets Evaluation Criterion
Methods

RER RERc EDL EmoDetect

News

PRO loss(↓) 0.1992 0.1913 0.2596 0.2465
Hamming Loss(↓) 0.2318 0.2277 0.2671 0.2696
Ranking Loss(↓) 0.1477 0.1405 0.1689 0.1769
One-error(↓) 0.1579 0.1562 0.2115 0.1903
Average Precision(↑) 0.8775 0.8789 0.8028 0.7865
Coverage(↓) 2.1398 2.1316 2.1595 2.2348
Subset Accuracy(↓) 0.1899 0.1822 0.2026 0.2243
F1exam(↑) 0.7062 0.7143 0.6503 0.6469
MicroF1(↑) 0.7086 0.7171 0.6346 0.6375
MacroF1(↑) 0.6244 0.6291 0.5641 0.5767

Blogs

PRO loss(↓) 0.2354 0.2321 0.2739 0.2912
Hamming Loss(↓) 0.2054 0.2014 0.2102 0.2202
Ranking Loss(↓) 0.2137 0.2102 0.2589 0.2781
One-error(↓) 0.4556 0.4550 0.5227 0.5352
Average Precision(↑) 0.6749 0.6803 0.6411 0.5663
Coverage(↓) 2.1269 2.1268 2.1699 2.8956
Subset Accuracy(↓) 0.1663 0.1663 0.2116 0.2321
F1exam(↑) 0.5080 0.5114 0.4606 0.4650
MicroF1(↑) 0.5093 0.5116 0.4620 0.4552
MacroF1(↑) 0.4102 0.4161 0.3923 0.3622

Table 3: Comparison with emotion detection baselines.
“↓” indicates “the smaller the better”, while “↑” indi-
cates “the larger the better”. The best performance on
each evaluation measure is highlighted by boldface.

We have also extended RER by incorporating
emotion relationships as constraints into the learn-
ing framework, denoted as RERc in Table 3. The
correlation of every pair of emotions is calculat-
ed based on their respective votes from news ar-
ticles or scores from blogs. It can be observed
from Table 3 that in almost all cases, incorporating
the constraints gives better performance. It should
be pointed out that the results of the baseline ap-
proach EDL are slightly different from those re-
ported in (Zhou et al., 2016) since we employ L-
STM for feature construction instead of recursive
autoencoders.

Since relevant emotion ranking can be seen as
an extension of multi-label learning, the proposed
framework is also compared with 8 widely used
multi-label learning methods using the threshold

Θ which is initialized as 0.15 after normalization,
such as ML-KNN (Zhang and Zhou, 2007), LIFT
(Zhang, 2011), CLR (Fürnkranz et al., 2008),
Rank-SVM (Zhang and Zhou, 2014) , MLLOC
(Huang and Zhou, 2012), BP-MLL (Zhang and
Zhou, 2006), ECC (Read et al., 2009) and ML-
RBF (Zhang, 2009). ML-KNN is based on the
traditional k-nearest neighbor (KNN) algorithm.
LIFT constructs features specific to each emotion
by conducting clustering analysis on its positive
or negative instances. CLR transforms MLL into
a label ranking problem by pairwise comparison
which considers each label pairs and rank all the
labels without recognizing that only the rankings
of relevant ones are crucial. Rank-SVM focus-
es on distinguishing relevant from irrelevant while
neglecting the rankings of relevant ones. MLLOC
tries to exploit emotion correlations in the expres-
sion data locally. The global discrimination fitting
and local correlation sensitivity are incorporated
into a unified framework. BP-MLL is derived
from the back propagation algorithm through em-
ploying a novel error function capturing the char-
acteristics of multi-label learning. ECC applies
classifier chains in an ensemble framework. ML-
RBF gets the multi-label neural networks adopted
from the traditional Radial Basis Function (RBF)
neural networks.

Anger Anxiety Expect Hate
)í(angry) ³ù(fear) 64(blessing) ?�(hate)
²ä(rage) �(lose) 34(happy) J(hypocrisy)
��(complain) �Õ(lonely) {Ð(fine) (hype)
1µ(criticize) Øå(pressure) (dream) Ã·(shameless)
|Ã(interest) y¢(reality) gd(freedom) Ãã(means)
ÜÀ(discriminate) ))(strange) "(long for) yY(silly)
(stop) %((heart) F"(hope) L¤(waste)
I(accuse) Û£(pain) ÆS(learn¤ ��(behind)
dN(annoy) (imagine) &g(faith) Z9(dirty)
Ã·(shameless) ú³(hurt) [p(home) Ð¢(lie)

Joy Love Sorrow Surprised
¯W(happy) {w(beautiful) �Õ(lonely) ÐÛ(curious)
p,(joyful ) O(love) úb(tears) ¯ç(surprise)
*l(friend) *l(friend) O(love) �¯(shock)
aÄ(touching) 34(happiness) N((solitude) ¯Û(wonder)
%(mood) ¯f(child) Û£(pain) ¯<(amazing)
§æ(warm) )·(life) a(feeling) ¿	(accident)
É(enjoy) �1(sunshine) ú³(hurt) ¯h(fright)
,¯(excited) §æ(warmth) �(lose) ¯�(scream)
Â¼(harvest) gg(miss) gg(miss) Ø²¿(accidently)
�(smile) O(lovely) )¹(life) �É(amazed)

Figure 3: The top 10 words for each emotion label from
Blogs dataset.

For the MLL methods, the value of k is set to
8 in ML-KNN, ratio is 0.02 and µ is 2 in ML-
RBF. Linear kernel is used in LIFT. Rank-SVM
uses the RBF kernel with the width σ equals to 1.
The CR4.5 is used as the base classifier for CLR
and ECC. The evaluation results of the proposed

568



Datasets Evaluation Criterion Methods
RERc ML-KNN LIFT CLR Rank-SVM MLLOC BP-MLL ECC ML-RBF

News

PRO loss(↓) 0.1913 0.2551 0.2426 0.2487 0.2670 0.3429 0.2603 0.2823 0.2658
Hamming Loss(↓) 0.2277 0.2876 0.3118 0.3023 0.3127 0.3241 0.3040 0.3079 0.3599
Ranking Loss(↓) 0.1405 0.1898 0.1987 0.2142 0.2271 0.3234 0.1897 0.2563 0.1949
One-error(↓) 0.1562 0.2366 0.1881 0.2242 0.2258 0.2025 0.2043 0.2151 0.2240
Average Precision(↑) 0.8789 0.8095 0.7945 0.7916 0.8001 0.7545 0.8044 0.6245 0.8106
Coverage(↓) 2.1316 2.3602 2.4641 2.3453 2.6093 3.1272 2.4032 2.4122 2.4390
Subset Accuracy(↓) 0.1822 0.1916 0.1857 0.2386 0.1839 0.2107 0.2765 0.2222 0.2609
F1exam(↑) 0.7143 0.6215 0.6262 0.6032 0.6244 0.5193 0.5879 0.5108 0.6147
MicroF1(↑) 0.7171 0.6280 0.6131 0.6177 0.6268 0.5389 0.6231 0.5699 0.6160
MacroF1(↑) 0.6291 0.5587 0.5593 0.5658 0.5613 0.4913 0.5563 0.4573 0.5543

Blogs

PRO loss(↓) 0.2321 0.3036 0.2912 0.3041 0.2869 0.3523 0.3429 0.2867 0.2922
Hamming Loss(↓) 0.2014 0.2409 0.2242 0.2162 0.2585 0.2156 0.2241 0.2301 0.2204
Ranking Loss(↓) 0.2102 0.2928 0.2881 0.2947 0.3024 0.4532 0.3234 0.3345 0.2364
One-error(↓) 0.4550 0.5543 0.5152 0.5229 0.5606 0.6143 0.4625 0.6635 0.4679
Average Precision(↑) 0.6803 0.5897 0.5963 0.6370 0.5832 0.4532 0.5545 0.5256 0.6412
Coverage(↓) 2.1268 2.4448 2.4356 2.2671 2.5962 3.5634 3.1272 2.7756 2.5067
Subset Accuracy(↓) 0.1663 0.1978 0.2116 0.1938 0.2321 0.2251 0.2107 0.2236 0.1803
F1exam(↑) 0.5114 0.4616 0.4620 0.4509 0.4832 0.4931 0.5093 0.4986 0.4997
MicroF1(↑) 0.5116 0.4720 0.4552 0.4859 0.4962 0.4902 0.4889 0.5003 0.5051
MacroF1(↑) 0.4161 0.3632 0.3656 0.4056 0.3965 0.3853 0.3813 0.3957 0.4086

Table 4: Comparison with Multi-Label Learning (MLL) Methods.“↓” indicates “the smaller the better”, while “↑”
indicates “the larger the better”. The best performance on each evaluation measure is highlighted by boldface.

approach in comparison to all MLL baselines are
presented in Table 4. RERc performs the best on
all evaluation measures. It verifies the advantage
of RERc due to its consideration of varying inten-
sities of the emotion labels and the ignorance of
irrelevant ones during the learning of the relevant
emotion ranking model. We also observe that, in
most cases, the performance on the News dataset
is better than that in the Blogs dataset. This may
due to different types of text observed in these two
platforms. News articles are more formal while
bogs typically contain informal language and are
more colloquial.

4.3 Result Analysis

To fully understand the emotion detection results,
we generate the top 10 most frequent words in the
test set for each emotion label from Blogs dataset
presented in Figure 3. Intuitively, we can find that
most top words are quite expressive of their asso-
ciated emotions. For example, the word “happy”
delivers the emotion of Joy and the word “tears”
tells Sorrow, etc. Moreover, we also observe that
there are some common words across multiple e-
motion categories. For instance, “friend” appears
in both Joy and Love. The results demonstrate that
the proposed framework can learn emotions from
text precisely.

5 Conclusions

In this paper, we have proposed a novel frame-
work to detect multiple emotions from text based
on relevant emotion ranking. Moreover, the rela-
tionships between emotions are incorporated in-
to the learning framework as constraints. Exper-
imental results on both News and Blogs datasets
show that the proposed framework is able to de-
tect multiple emotions as well as generating rank-
ings of relevant emotions. It performs remarkably
better than the state-of-the-art baselines on multi-
emotion detection and also outperforms several d-
ifferent methods used for multi-label learning. In
the future, we will explore the possibility of ex-
tending the current framework by detecting emo-
tions at more fine-grained level, for example, e-
motions associated with specific events reported
in text.

Acknowledgments

The work was supported by the National Key
R&D Program of China (No. 2017YFB1002801),
the National Natural Science Foundation of Chi-
na (61772132), the Natural Science Foundation of
Jiangsu Province of China (BK20161430) and In-
novate UK (103652).

569



References
Ameeta Agrawal and Aijun An. 2012. Unsupervised

emotion detection from text using semantic and syn-
tactic relations. In Ieee/wic/acm International Con-
ferences on Web Intelligence and Intelligent Agent
Technology. pages 346–353.

Saima Aman and Stan Szpakowicz. 2007. Identify-
ing expressions of emotion in text. In Text, Speech
and Dialogue, International Conference, Tsd 2007,
Pilsen, Czech Republic, September 3-7, 2007, Pro-
ceedings. pages 196–205.

Shenghua Bao, Shengliang Xu, Li Zhang, Rong Yan,
Zhong Su, Dingyi Han, and Yong Yu. 2009. Join-
t emotion-topic modeling for social affective tex-
t mining. In 2009 Ninth IEEE International Con-
ference on Data Mining. IEEE, pages 699–704.

Shenghua Bao, Shengliang Xu, Li Zhang, Rong Yan,
Zhong Su, Dingyi Han, and Yong Yu. 2012. Min-
ing social emotions from affective text. IEEE
transactions on knowledge and data engineering
24(9):1658–1670.

Dimitri P Bertsekas and John N Tsitsiklis. 1989. Paral-
lel and distributed computation: numerical method-
s. Reprint of the 1989 edition published by Prentice-
Hall.. Prentice Hall.

Plaban Kumar Bhowmick. 2009. Reader perspective
emotion analysis in text through ensemble based
multi-label classification framework. Computer and
Information Science 2(4):64.

David M Blei, Andrew Y Ng, and Michael I Jordan.
2003. Latent dirichlet allocation. Journal of ma-
chine Learning research 3(Jan):993–1022.

Andrew Cotter, Ohad Shamir, Nathan Srebro, and
Karthik Sridharan. 2011. Better mini-batch algo-
rithms via accelerated gradient methods. Advances
in Neural Information Processing Systems pages
1647–1655.

Andr Elisseeff and Jason Weston. 2001. A kernel
method for multi-labelled classification. In Interna-
tional Conference on Neural Information Processing
Systems: Natural and Synthetic. pages 681–687.

Johannes Fürnkranz, Eyke Hüllermeier, Eneldo Loza
Mencı́a, and Klaus Brinker. 2008. Multilabel classi-
fication via calibrated label ranking. Machine learn-
ing 73(2):133–153.

Sepp Hochreiter and Jrgen Schmidhuber. 1997.
Long short-term memory. Neural Computation
9(8):1735–1780.

Sheng Jun Huang and Zhi Hua Zhou. 2012. Multi-label
learning by exploiting label correlations locally. In
Twenty-Sixth AAAI Conference on Artificial Intelli-
gence. pages 949–955.

Jingsheng Lei, Yanghui Rao, Qing Li, Xiaojun Quan,
and Liu Wenyin. 2014. Towards building a social
emotion detection system for online news. Future
Generation Computer Systems 37:438–448.

Kevin Hsin-Yih Lin, Changhua Yang, and Hsin-Hsi
Chen. 2008. Emotion classification of online news
articles from the reader’s perspective. In Proceed-
ings of the 2008 IEEE/WIC/ACM International Con-
ference on Web Intelligence and Intelligent Agen-
t Technology-Volume 01. IEEE Computer Society,
pages 220–226.

Christopher D. Manning, Prabhakar Raghavan, and
Hinrich Schtze. 2008. An introduction to infor-
mation retrieval. Journal of the American Society
for Information Science and Technology 43(3):824–
825.

W. Alan Nicewander. 1988. Thirteen ways to look
at the correlation coefficient. American Statistician
42(1):59–66.

Rosalind W Picard and Roalind Picard. 1997. Affective
computing, volume 252. MIT press Cambridge.

Changqin Quan and Fuji Ren. 2010. Sentence emotion
analysis and recognition based on emotion words us-
ing ren-cecps. International Journal of Advanced
Intelligence Paradigms 2(1):105–117.

Xiaojun Quan, Qifan Wang, Ying Zhang, Luo Si, and
Liu Wenyin. 2015. Latent discriminative models
for social emotion detection with emotional depen-
dency. ACM Transactions on Information Systems
(TOIS) 34(1):2.

Yanghui Rao, Xiaojun Quan, Liu Wenyin, Qing Li,
and Mingliang Chen. 2012. Building word-emotion
mapping dictionary for online news. In SDAD 2012
The 1st International Workshop on Sentiment Dis-
covery from Affective Data. page 28.

Jesse Read, Bernhard Pfahringer, Geoff Holmes, and
Eibe Frank. 2009. Classifier chains for multi-
label classification. In Joint European Conference
on Machine Learning and Knowledge Discovery in
Databases. pages 254–269.

Carlo Strapparava and Rada Mihalcea. 2008. Learning
to identify emotions in text. In Proceedings of the
2008 ACM symposium on Applied computing. ACM,
pages 1556–1560.

Vladimir Naumovich Vapnik and Vlamimir Vapnik.
1998. Statistical learning theory, volume 1. Wiley
New York.

Yichen Wang and Aditya Pal. 2015. Detecting e-
motions in social media: A constrained optimiza-
tion approach. In Proceedings of the Twenty-Fourth
International Joint Conference on Artificial Intelli-
gence (IJCAI 2015). pages 996–1002.

Miao Xu, Yu Feng Li, and Zhi Hua Zhou. 2013. Multi-
label learning with pro loss. In Twenty-Seventh
AAAI Conference on Artificial Intelligence.

570



Min Ling Zhang. 2009. M l-rbf : Rbf neural networks
for multi-label learning. Neural Processing Letters
29(2):61–74.

Min Ling Zhang. 2011. Lift: multi-label learning with
label-specific features. In International Joint Con-
ference on Artificial Intelligence. pages 1609–1614.

Min Ling Zhang and Zhi Hua Zhou. 2006. Multil-
abel neural networks with applications to function-
al genomics and text categorization. IEEE Transac-
tions on Knowledge Data Engineering 18(10):1338–
1351.

Min Ling Zhang and Zhi Hua Zhou. 2007. Ml-knn: A
lazy learning approach to multi-label learning. Pat-
tern Recognition 40(7):2038–2048.

Min-Ling Zhang and Zhi-Hua Zhou. 2014. A re-
view on multi-label learning algorithms. IEEE
transactions on knowledge and data engineering
26(8):1819–1837.

Deyu Zhou, Xuan Zhang, Yin Zhou, Quan Zhao, and
Xin Geng. 2016. Emotion distribution learning from
texts. In Conference on Empirical Methods in Natu-
ral Language Processing. pages 638–647.

571


