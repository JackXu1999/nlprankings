



















































Effective Greedy Inference for Graph-based Non-Projective Dependency Parsing


Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 711–720,
Austin, Texas, November 1-5, 2016. c©2016 Association for Computational Linguistics

Effective Greedy Inference for Graph-based
Non-Projective Dependency Parsing

Ilan Tchernowitz Liron Yedidsion
Faculty of Industrial Engineering and Management, Technion, IIT

{ilantc@campus|lirony@ie|roiri@ie}.technion.ac.il

Roi Reichart

Abstract

Exact inference in high-order graph-based
non-projective dependency parsing is in-
tractable. Hence, sophisticated approximation
techniques based on algorithms such as be-
lief propagation and dual decomposition have
been employed. In contrast, we propose a sim-
ple greedy search approximation for this prob-
lem which is very intuitive and easy to im-
plement. We implement the algorithm within
the second-order TurboParser and experiment
with the datasets of the CoNLL 2006 and 2007
shared task on multilingual dependency pars-
ing. Our algorithm improves the run time of
the parser by a factor of 1.43 while losing 1%
in UAS on average across languages. More-
over, an ensemble method exploiting the joint
power of the parsers, achieves an average UAS
0.27% higher than the TurboParser.

1 Introduction

Dependency parsing is instrumental in NLP appli-
cations, with recent examples in information extrac-
tion (Wu and Weld, 2010), word embeddings (Levy
and Goldberg, 2014), and opinion mining (Almeida
et al., 2015). The two main approaches for this task
are graph based (McDonald et al., 2005) and transi-
tion based (Nivre et al., 2007).

The graph based approach aims to optimize a
global objective function. While exact polyno-
mial inference algorithms exist for projective pars-
ing (Eisner, 1996; McDonald et al., 2005; Carreras,
2007; Koo and Collins, 2010, inter alia), high order
non-projective parsing is NP-hard (McDonald and
Pereira, 2006). The current remedy for this comes in

the form of advanced optimization techniques such
as dual decomposition (Martins et al., 2013), LP re-
laxations (Riedel et al., 2012), belief propagation
(Smith and Eisner, 2008; Gormley et al., 2015) and
sampling (Zhang et al., 2014b; Zhang et al., 2014a).

The transition based approach (Zhang and Nivre,
2011; Bohnet and Nivre, 2012; Honnibal et al.,
2013; Choi and McCallum, 2013a, inter alia), and
the easy first approach (Goldberg and Elhadad,
2010) which extends it by training non-directional
parsers that consider structural information from
both sides of their decision points, lack a global ob-
jective function. Yet, their sequential greedy solvers
are fast and accurate in practice.

We propose a greedy search algorithm for high-
order, non-projective graph-based dependency pars-
ing. Our algorithm is a simple iterative graph-based
method that does not rely on advanced optimization
techniques. Moreover, we factorize the graph-based
objective into a sum of terms and show that our basic
greedy algorithm relaxes the global objective by se-
quentially optimizing these terms instead of globally
optimizing their sum.

Unlike previous greedy approaches to depen-
dency parsing, transition based and non-directional,
our algorithm does not require a specialized feature
set or a training method that specializes in local deci-
sions. In contrast, it supports global parameter train-
ing based on the comparison between an induced
tree and the gold tree. Hence, it can be integrated
into any graph-based parser.

We first present a basic greedy algorithm that re-
laxes the global graph-based objective (Section 3).
However, as this simple algorithm does not provide a

711



realistic estimation of the impact of an arc selection
on uncompleted high-order structures in the partial
parse forest, it is not competitive with state of the
art approximations. We hence present an advanced
version of our algorithm with an improved arc score
formulation and show that this simple algorithm pro-
vides high quality solutions to the graph-based infer-
ence problem (Section 4).

Particularly, we implement the algorithm within
the TurboParser (Martins et al., 2013) and exper-
iment (Sections 8 and 9) with the datasets of the
CoNLL 2006-2007 shared tasks on multilingual de-
pendency parsing (Buchholz and Marsi, 2006; Nils-
son et al., 2007). On average across languages our
parser achieves UAS scores of 87.78% and 89.25%
for first and second order parsing respectively, com-
pared to respective UAS of 87.98% and 90.26%
achieved by the original TurboParser.

We further implement (Section 6) an ensemble
method that integrates information from the output
tree of the original TurboParser and the arc weights
learned by our variant of the parser into our search
algorithm to generate a new tree. This yields an im-
provement: average UAS of 88.03% and 90.53% for
first and second parsing, respectively.

Despite being greedy, the theoretical runtime
complexity of our advanced algorithm is not better
than the best previously proposed approximations
for our problem (O(nk+1), for nword sentences and
k order parsing, Section 5). In experiments, our al-
gorithms improve the runtime of the TurboParser by
a factor of up to 2.41.

The main contribution of this paper is hence in
providing a simple, intuitive and easy to implement
solution for a long standing problem that has been
addressed in past with advanced optimization tech-
niques. Besides the intellectual contribution, we
believe this will make high-order graph-based de-
pendency parsing accessible to a much broader re-
search and engineering community as it substan-
tially relaxes the coding and algorithmic proficiency
required for the implementation and understanding
of parsing algorithms.

2 Problem Formulation

We start with a brief definition of the high order
graph-based dependency parsing problem. Given an

n word input sentence, an input graph G = (V,E)
is defined. The set of vertices is V = {0, ..., n},
with the {1, . . . , n} vertices representing the words
of the sentence, in their order of appearance, and the
0 vertex is a specialized root vertex. The set of arcs
is E = {(u, v) : u ∈ {0, ..., n}, v ∈ {1, ..., n}, u 6=
v}, that is, the root vertex has no incoming arcs.

We further define a part of order k to be a subset
of E of size k, and denote the set of all parts with
parts. For the special case of k = 1 a part is an
arc. Different works employed different parts sets
(e.g. (Martins et al., 2013; McDonald et al., 2005;
Koo and Collins, 2010)). Generally, most parts sets
consist of arcs connecting vertices either vertically
(e.g. {(u, v), (v, z)} for k = 2) or horizontally (e.g.
{(u, v), (u, z)}, for k = 2). In this paper we focus
on the parts employed by (Martins et al., 2013), a
state-of-the-art parser, but our algorithms are gener-
ally applicable for any parts set consistent with this
general definition.1

In graph-based dependency parsing, each part p
is given a score Wp ∈ R. A Dependency Tree
(DT) T is a subset of arcs for which the following
conditions hold: (1) Every vertex, except for the
root, has an incoming arc: ∀v ∈ V \ {0} : ∃u ∈
V s.t.(u, v) ∈ T ; (2) No vertex has multiple incom-
ing arcs: ∀(u, u′, v) ∈ V, (u, v) ∈ T → (u′, v) /∈ T ;
and (3) There are no cycles in T . The score of a DT
T is finally defined by:

score(T ) =
∑

part⊆T
Wpart

The inference problem in this model is to find the
highest scoring DT in the input weighted graph.

3 Basic Greedy Inference

We start with a basic greedy algorithm (Algorithm
1), analyze the approximation it provides for the
graph-based objective and its inherent limitations.

1More generally, a part is defined by two arc subsets, A and
B, such that a part p belongs to a tree T if ∀e ∈ A : e ∈
T and ∀e ∈ B : e /∈ T . In this paper we assume B = φ.
Hence, we cannot experiment with the third order TurboParser
as in all its third order parts B 6= φ. Also, when we integrate
our algorithms into the second order TurboParser we omit the
nextSibling part for whichB 6= φ. For the original TurboParser
to which we compare our results, we do not omit this part as it
improves the parser’s performance.

712



Algorithm 1 maintains a partial tree data struc-
ture, T i, to which it iteratively adds arcs from the
input graph G, one in each iteration, until a depen-
dency tree Tn is completed. For this end, in every
iteration, i, a value, vie, composed of loss

i
e and gain

i
e

terms, is computed for every arc e ∈ E and the arc
with the lowest vie value is added to T

i−1 to create
the extended partial tree T i.

Due to the aforementioned conditions on the in-
duced dependency tree, every arc that is added to
T i−1 yields a set of lostArcs and lostParts that can-
not be added to the partial tree in subsequent itera-
tions. The loss value is defined to be:

lossie :=
∑

part∈lostParts
Wpart

That is, every part that contains one or more arcs
that violate the dependency tree conditions for a tree
that extends the partial tree T i−1∪{e} is considered
a lost part as it will not be included in any tree ex-
tending T i−1∪{e}. The loss value sums the weights
of these parts.

Likewise, the gain value is the sum of the weights
of all the parts that are added to T i−1 when we add
e to it. Denote this set of parts with Pe := {part :
part ⊆ T i−1 ∪ {e}, part /∈ T i−1}, then:

gainie :=
∑

part∈Pe
Wpart

Finally, vie is given by:

vie = loss
i
e − gainie

After the arc with the minimal value vie is added to
T i−1, the arcs that violate the structural constraints
on dependency trees are removed from G.

An example of an update iteration of the algo-
rithm (lines 3-16) is given in Figure 1. In this ex-
ample we consider two types of parts: first-order,
arc, parts (ARC) and second-order grandparent parts
(GP), consisting of arc pairs, {(g, u), (u, v)}. The
upper graph shows the partial tree T 2 (solid arcs) as
well as the rest of the graph G (dashed arcs). The
parts included in T 2 are ARC(0,2), ARC(2,1) and
GP[(0,2),(2,1)]. The table contains the weights of
the parts and the values computed for the arcs dur-
ing the third iteration. The arc that is chosen is (2,3),
as it has the minimal v3e value. Thus, in the bottom

root John walked home
0 1 2 3

part weight loss3e gain3e v3e
ARC(0,3) 2 3.5 2 1.5
ARC(1,3) 1 5.5 0 5.5
ARC(2,3) 1.5 2 3.5 -1.5

GP[(0,2)(2,3)] 2 – – –
GP[(2,1)(1,3)] -1 – – –

root John walked home
0 1 2 3

Figure 1: An example of an iteration of Algorithm 1
(lines 3-16)). See description in text.

graph that corresponds to T 3 all other incoming arcs
to vertex 3 are removed. (in this instance there are
no cycle forming arcs).
Analysis We now turn to an analysis of the
relaxation that Algorithm 1 provides for the global
graph-based objective. Recall that our objective in
iteration i is: vi

ei
= min{vie}. For the inferred tree

Tn it holds that:
∑

ei∈Tn
viei −

∑

part⊆G
Wpart =

∑

part*Tn
Wpart −

∑

part⊆Tn
Wpart −

∑

part⊆G
Wpart =

− 2×
∑

part⊆Tn
Wpart +

∑

part*Tn
Wpart −

∑

part*Tn
Wpart =

− 2×
∑

part⊆Tn
Wpart

The first equation holds since
∑

ei∈Tn v
i
ei

is the
sum of all lost parts (parts that are not in Tn) minus
all the gained parts (parts in Tn). Each of these parts
was counted exactly once: when the part was added
to the partial tree or when one of its arcs was re-
moved from G. The second equation splits the term
of

∑
part⊆GWpart to two sums, one over parts in T

n

and the other over the rest. Since
∑

part⊆GWpart
and 2 are constants, we get:

argmin
Tn

(−
∑

part⊆Tn
Wpart) = argmin

Tn

∑

ei∈Tn
viei

From this argument it follows that our inference
algorithm performs sequential greedy optimization
over the presented factorization of the graph-based

713



objective instead of optimizing the sum of terms,
and hence the objective, globally.

The main limitation of Algorithm 1 is that it does
not take into account high order parts contribution
until the part is actually added to T . For exam-
ple, in Figure 1, when the arc (2, 1) is added, the
part GP[(2,1),(1,3)] is getting closer to completion.
Yet, this is not taken into account when considering
whether (2, 1) should be added to the tree or not. In-
cluding this information in the gain and loss values
of an arc can improve the accuracy of the algorithm,
especially in high-order parsing.

Algorithm 1 Basic Greedy Inference
1: T 0 = {}
2: for i ∈ 1..n do
3: for e = (u, v) ∈ E do
4: Pe := {part ∈ parts : part ⊆ T i−1 ∪
{e}, part * T i−1}

5: gainie :=
∑

part∈Pe Wpart
6: incomingSet := {(u′, v) ∈ E : u′ 6= u}
7: cycleSet := {(u′, v′) ∈ E : T i−1 ∪ {e} ∪

(u′, v′) contains a cycle}
8: lostArcs = (incomingSet ∪ cycleSet)
9: lostParts = {part : ∃e ∈ lostArcs ∩ part}

10: lossie :=
∑

part∈lostPartsWpart
11: vie := lossie − gainie
12: end for
13: ei = (ui, vi) = argmine′{vie′}
14: T i = T i−1 ∪ {ei}
15: remove from G all incoming arcs to vi

16: remove from G all cycle forming arcs w.r.t T i

17: end for

4 Greedy Inference with Partial Part
Predictions

In order for the algorithm to account for information
about partial high order parts, we estimate the prob-
ability that such parts would be eventually included
in Tn. Our way to do this (Algorithm 2) is by es-
timating these probabilities for arcs and from these
derive parts probabilities.

Particularly, for the set of incoming arcs of a ver-
tex v, Ev = {e = (u, v) : e ∈ E}, a probability
measure pe is computed according to:

pe=(u,v) =
expα×We∑

e′=(u′,v) exp
α×We′

Where α is a hyper parameter of the model. For
α = 0 we get a uniform distribution over all possible

Algorithm 2 Greedy Inference with Partial Part Pre-
dictions

1: T 0 = {}
2: for i ∈ 1..n do
3: for e = (u, v) ∈ E do
4: Pe := {part ∈ parts : e ∈ part}
5: gainie :=

∑
part∈Pe Wpart× ppart|(T

i−1 ∪{e})
6: incomingSet := {(u′, v) ∈ E : u′ 6= u}
7: cycleSet := {(u′, v′) ∈ E : T i−1 ∪ {e} ∪

(u′, v′) contains a cycle}
8: lostArcs = (incomingSet ∪ cycleSet)
9: lostParts = {part : ∃e ∈ lostArcs ∩ part}

10: lossie :=
∑

part∈lostPartsWpart × ppart|T i−1
11: vie := β × lossie − (1− β)× gainie
12: end for
13: ei = (ui, vi) = argmine′{vie′}
14: T i = T i−1 ∪ {ei}
15: remove from G all incoming arcs to vi

16: remove from G all cycle forming arcs w.r.t T i

17: end for

heads of a vertex v, and for large α values arcs with
larger weights get higher probabilities.

The intuition behind this measure is that arcs
mostly compete with other arcs that have the same
target vertex and hence their weight should be nor-
malized accordingly. Using this measure, we define
the arc-factored probability of a part to be:

ppart =
∏

e∈part
pe

And the residual probability of a part given an exist-
ing partial tree T :

ppart|T =
ppart∏

e∈(part⋂T ) pe

These probability measures are used in both the
gain and the loss computations (lines 5 and 10 in
Algorithm 2) as follows:

gainie :=
∑

part:e∈part
Wpart × ppart|(T i−1 ∪ {e})

lossie :=
∑

part∈lostParts
Wpart × ppart|T i−1

Finally, as adding an arc to the dependency sub-
tree results in an exclusion of several arcs, the num-
ber of lost parts is also likely to be much higher
than the number of gained parts. In order to com-
pensate for this effect, we introduce a balancing

714



hyper-parameter, β ∈ [0, 1], and change the com-
putation of vie (line 10 in Algorithm 2) to be: v

i
e :=

β × lossie − (1− β)× gainie.

5 Runtime Complexity Analysis

In this section we provide a sketch of the runtime
complexity analysis of the algorithm. Full details
are in appendix A. In what follows, we denote the
maximal indegree of a vertex with nin.

Algorithm 1 Algorithm 1 consists of two nested
loops (lines 2-3) and hence lines 4-11 are repeated
O (n× |E|) = O(n×n×nin) times. At each repe-
tition, loss (lines 6-10) and gain (lines 4-5) values
are computed. Afterwards the graph’s data struc-
tures are updated (lines 13-16). We define data
structures (DSs) that keep our computations effi-
cient. With these DSs the total runtime of lines 4-11
is O(nin + min{nk−1in , n2in}). The DSs are initial-
ized in O(|parts| × k) time and their total update
time is O(k × |parts|) = O(nk+1in ). Thus algo-
rithm 1 runs in O(|parts| × k + n2 × nin × (nin +
min{nk−1in , n2in})) time.
Algorithm 2 Algorithm 2 is similar in structure to
Algorithm 1. The enhanced loss and gain computa-
tions take O(nin +min{nk−1in , n2in}) time. The ini-
tialization of the DSs takes O(|parts| × k) time and
their update time is O(nkin × k2). The total runtime
of Algorithm 2 isO(nk+1in ×k+n×(n×nin×(nin+
min{n2in, nk−1in })+nkin×k2)). For unpruned graphs
and k ≥ 2 this is equivalent to O(nk+1), the theo-
retical runtime of the TurboParser’s dual decompo-
sition inference algorithm.

6 Error Propagation

Unlike modern approximation algorithms for our
problem, our algorithm is greedy and determinis-
tic. That is, in each iteration it selects an arc to
be included in its final dependency tree and this de-
cision cannot be changed in subsequent iterations.
Hence, our algorithm is likely to suffer from error-
propagation. We propose two solutions to this prob-
lem described within Algorithm 2.

Beam search In each iteration (lines 3-16) the al-
gorithm outputs its |B| best solutions to be subse-
quently considered in the next iteration. That is,
lines 4-10 are performed |B| times for each edge

e ∈ E, one for each of the |B| partial solutions in
the beam, bj ∈ B. For each such solution, we de-
note its weight, as calculated by the previous itera-
tion of the algorithm with beamV albj . When evalu-
ating vie for an arc e with respect to b

j (line 11), we
set vi,je = beamV albj+β×lossie−(1−β)×gainie.

Post-search improvements After Algorithm 2 is
executed, we perform s iterations of local greedy arc
swaps. That is, for every vertex v, s.t. (u, v) ∈ Tn,
we try to switch the arc (u, v) with the arc (u′, v) as
follows. Let Tnv be the sub tree that is rooted at v,
we distinguish between two cases:
(1) If u′ /∈ Tnv then Tn = Tn \ {(u, v)} ∪ {(u′, v)}.
(2) If u′ ∈ Tnv then let w be the first vertex on the
path from v to u′ (if (v, u′) ∈ T then w = u′):
Tn = Tn \ {(u, v), (v, w)} ∪ {(u′, v), (u,w)}.

After inspecting all possible substitutions, we
choose the one that yields the best increase in the
tree score (if such a substitution exists) and perform
the substitution.

7 Parser Combination

In our experiments (see below), we implemented our
algorithms within the TurboParser so that each of
them, in turn, serves as its inference algorithm. In
development data experiments with Algorithm 2 we
found that for first order parsing, both our algorithm
and the TurboParser predict on average over all lan-
guages around 1% of the gold arcs that are not in-
cluded in the output of the other algorithm. For sec-
ond order parsing, the corresponding numbers are
1.75% (for gold arcs in the output of our algorithm
but not of the original TurboParser) and 4.3% (for
the other way around). This suggests that an ensem-
ble method may improve upon both parsers.

We hence introduce a variation of Algorithm 2
that accepts a dependency tree To as an input, and
biases its output towards that tree. As different
parsers usually generate weights on different scales,
we do not directly integrate part weights. Instead,
we change the weight of each part part ⊆ To of
order j, to be Wpart = Wpart + γj , where γj is
an hyperparameter reflecting our belief in the pre-
diction of the other parser on parts of order j. The
change is applied only at test time, thus integrating
two pre-trained parsers.

715



8 Experimental Setup

We implemented our algorithms within the Tur-
boParser (Martins et al., 2013)2. That is, every other
aspect of the parser – feature set, pruning algorithm,
cost-augmented MIRA training (Crammer et al.,
2006) etc., is kept fixed but our algorithms replace
the inference algorithms: Chu-Liu-Edmonds ((Ed-
monds, 1967), first order) and dual-decomposition
(higher order). We implemented two variants, for
algorithm 1 and 2 respectively, and compare their
results to those of the original TurboParser.

We experiment with the datasets of the CoNLL
2006 and 2007 shared task on multilingual depen-
dency parsing (Buchholz and Marsi, 2006; Nilsson
et al., 2007), for a total of 17 languages. When a
language is represented in both sets, we used the
2006 version. We followed the standard train/test
split of these datasets and, for the 8 languages with
a training set of at least 10000 sentences, we ran-
domly sampled 1000 sentences from the training set
to serve as a development set. For these languages,
we first trained the parser on the training set and then
used the development set for hyperparameter tuning
(|B|, s, α, β, and γ1, . . . , γk for k order parsing).34

We employ four evaluation measures, where ev-
ery measure is computed per language, and we re-
port the average across all languages: (1) Unlabeled
Attachment Score (UAS); (2) Undirected UAS (U-
UAS) - for error analysis purposes; (3) Shared arcs
(SARC) - the percentage of arcs shared by the pre-
dictions of each of our algorithms and of the origi-
nal TurboParser; and (4) Tokens per second (TPS)
- for ensemble models this measure includes the
TurboParser’s inference time.5 We also report a
gold(x,y) = (a,b) measure: where a is the percentage
of gold standard arcs included in trees produced by
algorithm x but not by y, and b is the corresponding
number for y and x. We consider two setups.

2https://github.com/andre-martins/TurboParser
3|B| = 3, s = 5, α ∈ [0, 2.5], β ∈ [0.2, 0.5], γ1 ∈

[0.5, 1.5], γ2 ∈ [0.2, 0.3]. Our first order part weights are in
[−9, 4], and second order part weights in [−3, 13].

4The original TurboParser is trained on the training set of
each language and tested on its test set, without any further di-
vision of the training data to training and development sets.

5Run times where computed on an Intel(R) Xeon(R) CPU
E5-2697 v3@2.60GHz machine with 20GB RAM memory.

Fully Supervised Training In this setup we only
consider the 8 languages with a development set.
For each language, the parser is trained on the train-
ing set and then the hyperparameters are tuned. First
we set the beam size (|B|) and number of improve-
ment iterations (s) to 0, and tune the other hyperpa-
rameters on the language-specific development set.
Then, we tune |B| and s, using the optimal parame-
ters of the first step, on the English dev. set.

Minimally Supervised Training Here we con-
sider all 17 languages. For each language we ran-
domly sampled 20 training sets of 500 sentences
from the original training set, trained a parser on
each set and tested on the original test set. Results
for each language were calculated as the average
over these 20 folds. The hyper parameters for all
languages were tuned once on the English develop-
ment set to the values that yielded the best average
results across the 20 training samples.

9 Results

Fully Supervised Training Average results for
this setup are presented in table 1 (top). Unsur-
prisingly, UAS for second order parsing with basic
greedy inference (Algorithm 1, BGI) is very low, as
this model does not take information about partial
high order parts into account in its edge scores. We
hence do not report more results for this algorithm.

The table further reflects the accuracy/runtime
tradeoff provided by Algorithm 2 (basic greedy in-
ference with partial part predictions, BGI-PP): a
UAS degradation of 0.34% and 2.58% for first and
second order parsing respectively, with a runtime
improvement by factors of 1.01 and 2.4, respec-
tively. Employing beam search and post search im-
provements (BGI-PP+i+b) to compensate for error
propagation improves UAS but harms the runtime
gain: for example, the UAS gap in second order
parsing is 1.01% while the speedup factor is 1.43.

As discussed in footnote 1 and Section 11, our
algorithm does not support the third-order parts of
the TurboParser. However, the average UAS of
the third-order TurboParser is 90.62% (only 0.36%
above second order TurboParser) and its TPS is
72.12 (almost 5 times slower).

The accuracy gaps according to UAS and undi-
rected UAS are similar, indicating that the source

716



Fully supervised Individual Models Ensemble ModelsUAS TPS SARC U-UAS UAS TPS SARC U-UAS

TurboParser order1 87.98 5621.30 – 88.82 – – – –order2 90.26 356.63 – 90.98 – – – –

BGI order1 83.78 5981.91 90.87 90.87 – – – –order2 27.54 715.41 27.76 27.77 – – – –

BGI-PP order1 87.64 5680.60 97.15 88.53 88.03 2876.03 99.59 88.84order2 87.68 858.25 92.66 88.73 90.50 249.40 99.54 91.20

BGI-PP + i order1 87.76 4648.4 98.10 88.64 87.96 2557.00 99.47 88.80order2 88.98 639.97 94.40 89.81 90.50 297.10 99.43 91.19

BGI-PP + i + b order1 87.78 3253.80 98.29 88.73 87.91 2053.00 99.07 88.82order2 89.25 511.47 94.79 90.02 90.53 212.40 99.40 91.21

(a) The fully supervised setup.

Minimally supervised Individual Models Ensemble ModelsUAS TPS SARC U-UAS UAS TPS SARC U-UAS

TurboParser order1 78.99 13097.00 – 80.38 – – –order2 80.52 830.05 – 81.84 – – –

BGI-PP order1 78.76 13848.00 85.36 80.15 79.14 6499.00 87.36 80.50order2 78.80 3089.40 84.59 80.27 80.60 636.30 95.57 81.88

BGI-PP + i order1 78.87 11673.00 85.54 80.25 79.24 6516.00 87.55 80.59order2 79.36 2414.00 84.81 80.76 80.67 621.50 95.41 82.16

BGI-PP + i + b order1 78.91 4212.50 85.58 80.29 79.29 4349.00 87.61 80.62order2 79.45 1372.70 84.89 80.84 80.69 518.10 95.44 81.96

(b) The minimally supervised setup.

Table 1: Results for the fully supervised (top table) and minimally supervised (bottom table) setups. The left column
section of each table is for individual models while the right column section is for ensemble models (Section 7). BGI-
PP is the basic greedy inference algorithm with partial part predictions, +i indicates post-search improvements and
+b indicates beam search (Section 6). The Tokens per Second (TPS) measure for the ensemble models reports the
additional inference time over the TurboParser inference. All scores are averaged across individual languages.

of differences between the parsers is not arc direc-
tionality. The percentage of arcs shared between
the parsers increases with model complexity but is
still as low as 94.79% for BGI-PP+i+b in second or-
der parsing. In this setup, gold(BGI-PP+i+b, Tur-
boParser) = (1.6%,2.6%) which supports the devel-
opment data pattern reported in Section 6 and further
justifies an ensemble approach.

The right column section of the table indeed
shows consistent improvements of the ensemble
models over the TurboParser for second order pars-
ing: the ensemble models achieve UAS of 90.5-
90.53% compared to 90.26% of the TurboParser.
Naturally, running the TurboParser alone is faster by
a factor of 1.67. Like for the individual inference
algorithms, the undirected UAS measure indicates
that the gain does not come from arc directionality
improvements. The ensemble methods share almost
all of their arcs with the TurboParser, but in cases of
disagreement ensembles tend to be more accurate.

Table 2 complements our results, providing UAS
values for each of the 8 languages participat-
ing in this setup. The UAS difference between

BGI+PP+i+b and the TurboParser are (+0.24)-(-
0.71) in first order parsing and (+0.18)-(-2.46) in
second order parsing. In the latter case, combining
these two models (BGI+PP+i+b+e) yields improve-
ments over the TurboParser in 6 out of 8 languages.

Minimally Supervised Training Results for this
setup are in table 1 (bottom). While result pat-
terns are very similar to the fully supervised case,
two observations are worth mentioning. First,
the percentage of arcs shared by our algorithms
and the original parser is much lower than in
the fully supervised case. This is true also for
shared gold arcs: gold(BGI-PP+b+i,TurboParser) =
(4.86%,5.92%) for second order parsing. This sug-
gests that more sophisticated ensemble techniques
may be useful in this setup.

Second, ensemble modeling improves UAS over
the TurboParser also for first order parsing, lead-
ing to a gain of 0.3% in UAS for the BGI+i+b
ensemble (79.29% vs. 78.99%). As the percent-
age of shared arcs between the ensemble mod-
els and the TurboParser is particularly low in first
order parsing, as well as the shared gold arcs

717



language First Order Second Order
TurboParser BGI-PP BGI-PP BGI-PP TurboParser BGI-PP BGI-PP BGI-PP

+ i + b + i + b + e + i + b + i + b + e
swedish 87.12 86.35 86.93 87.12 88.65 86.14 87.85 89.29
bulgarian 90.66 90.22 90.42 90.66 92.43 89.73 91.50 92.58
chinese 84.88 83.89 84.17 84.17 86.53 81.33 85.18 86.59
czech 83.53 83.46 83.44 83.44 86.35 84.91 86.26 87.50
dutch 88.48 88.56 88.43 88.43 91.30 89.64 90.49 91.34
japanese 93.03 93.18 93.27 93.27 93.83 93.78 94.01 94.01
catalan 88.94 88.50 88.67 88.93 92.25 89.3 90.46 92.24
english 87.18 86.94 86.84 87.18 90.70 86.52 88.24 90.66

Table 2: Per language UAS for the fully supervised setup. Model names are as in Table 1, ‘e’ stands for ensemble.
Best results for each language and parsing model order are highlighted in bold.

(gold(BGI+i+b,TurboParser) = (4.98%,5.5%)), im-
proving the ensemble techniques is a promising fu-
ture research direction.

10 Related Work

Our work brings together ideas that have been con-
sidered in past, although in different forms.

Greedy Inference Goldberg and Elhadad (2010)
introduced an easy-first, greedy, approach to depen-
dency parsing. Their algorithm adds at each iteration
the best candidate arc, in contrast to the left to right
ordering of standard transition based parsers. This
work is extended at (Tratz and Hovy, 2011; Gold-
berg and Nivre, ; Goldberg and Nivre, 2013).

The easy-first parser consists of a feature set and
a specialized variant of the structured perceptron
training algorithm, both dedicated to greedy infer-
ence. In contrast, we show that a variant of the Tur-
boParser that employs Algorithm 2 for inference and
is trained with its standard global training algorithm,
performs very similarly to the same parser that em-
ploys dual decomposition inference.

Error Propagation in Deterministic Parsing
Since deterministic algorithms are standard in
transition-based parsing, the error-propagation prob-
lem has been dealt with in that context. Various
methods were employed, with beam search being a
prominent idea (Sagae and Lavie, 2006; Titov and
Henderson, 2007; Zhang and Clark, 2008; Huang et
al., 2009; Zhang and Nivre, 2011; Bohnet and Nivre,
2012; Choi and McCallum, 2013b, inter alia).

Post Search Improvements Several previous
works employed post-search improvements tech-
niques. Like in our case, these techniques improve

the tree induced by an initial, possibly more princi-
pled, search technique through local, greedy steps.

McDonald and Pereira (2006) proposed to ap-
proximate high-order graph-based non-projective
parsing, by arc-swap iterations over a previously in-
duced projective tree. Levi et al. (2016) proposed
a post-search improvements method, different than
ours, to compensate for errors of their graph-based,
undirected inference algorithm. Finally, Zhang et
al. (2014a) demonstrated that multiple random ini-
tialization followed by local improvements with re-
spect to a high-order parsing objective result in ex-
cellent parsing performance. Their algorithm, how-
ever, shouldbhhb employ hundreds of random ini-
tializations in order to provide state-of-the-art re-
sults.

Ensemble Approaches Finally, several previous
works combined dependency parsers. These include
Nivre and McDonald (2008) who used the output
of one parser to provide features for another, Zhang
and Clark (2008) that proposed a beam-search based
parser that combines two parsers into a single sys-
tem for training and inference, and Martins et al.
(2008) that employed stacked learning, in which a
second predictor is trained to improve the perfor-
mance of the first. Our work complements these
works by integrating information from a pre-trained
TurboParser in our algorithm at test time only.

11 Discussion

We presented a greedy inference approach for
graph-based, high-order, non-projective dependency
parsing. Our experiments with 17 languages show
that our simple and easy to implement algorithm is a
decent alternative for dual-decomposition inference.

A major limitation of our algorithm is in-

718



cluding information from parts that require a
given set of arcs not to be included in the de-
pendency tree (footnote 1). For example, the
nextSibling((1, 2), (1, 5)) part of the TurboParser
would fire iff the tree includes the arcs (1, 2) and
(1, 5) but not the arcs (1, 3) and (1, 4).

In order to account for such parts, we should de-
cide how to compute their probabilities and, addi-
tionally, at which point they are considered part of
the tree. We explored several approaches, but failed
to improve our results. Hence, we did not experi-
ment with the third-order TurboParser as all of its
third-order parts contain ”non-included” arcs. This
is left for future work.

A Runtime Complexity Analysis

Here we analyze the complexity of our algorithms,
denoting the maximal indegree of a vertex with nin.

Algorithm 1 Algorithm 1 consists of two nested
loops (lines 2-k3) and hence lines 4-11 are repeated
O (n× |E|) = O(n×n×nin) times. At each repe-
tition, loss (lines 6-10) and gain (lines 4-5) values
are computed. Afterwards the graph’s data struc-
tures are updated (lines 13-16).

For every arc that we examine (line 3), there are
O(nin) lost arcs, as there are O(nin) incoming arcs
(set 1) and O(nin) cycles to break (set 2). Since
every lost arc translates to a set of lost parts, we
can avoid repeating computations by storing the par-
tial loss of every arc in a data structure (DS): e →∑

part:e∈partwpart. Now, instead of summing all
the lost parts, (every edge participates in O(nk−1in )
parts,6 thus there are O(nkin) lost parts per added
arc), we can sum only O(nin) partial loss values.
However, since some lost parts may contain an arc
from set 1 and an arc from set 2, we need to sub-
tract the values that were summed twice, this can be
done in O(min{nk−1in , n2in}) time by holding a sec-
ond DS: e1 × e2 →

∑
part:e1∈part∧e2∈partwpart.

7

In order to efficiently compute the gain values, we
hold a mapping from arcs to the sum of weights of
parts that can be completed in the current iteration
by adding the arc to the tree. With this DS, gain val-

6Assuming that a part is a connected component.
7For first order parsing this is not needed; for second order

parsing it is done in O(nin) time.

ues can be computed in constant time. In total, the
runtime of lines 4-11 is O(nin +min{nk−1in , n2in}).

The DSs are initialized in O(|parts| × k) time.
Since every part is deleted at most once, and gets
updated (its arcs are added to the tree) at most k
times, the total DS update time is O(k × |parts|) =
O(nk+1in ). Thus algorithm 1 runs in O(|parts|×k+
n2 × nin × (nin +min{nk−1in , n2in})) time.
Algorithm 2 Algorithm 2 is similar in structure
to Algorithm 1 but the loss and gain computations
are more complex. To facilitate efficiency, we hold
two DSs: (a) a mapping from arcs to the sum of
lost parts values, which are now wpart × Ppart for
part ∈ parts; and (b) a mapping from arc pairs
to the sum of part values for parts that contain both
arcs. The loss and gain values can be computed, as
above, in O(nin +min{nk−1in , n2in}) time.

The initialization of the DSs takes O(|parts|×k)
time. In the i-th iteration we add e = (u, v) to
T i, and remove the lostArcs from E. Every lost
arc participates in O(nk−1in ) parts, and we need to
update O(k) entries for each lost part in DS(a) (as
the value of the other arcs of that part should no
longer account for that part’s weight) and O(k2) en-
tries in DS (b). Thus, the total update time of the
DSs is O(nkin × k2) and the total runtime of Algo-
rithm 2 is O(nk+1in × k + n × (n × nin × (nin +
min{n2in, nk−1in })+nkin×k2)). For unpruned graphs
and k ≥ 2 this is equivalent to O(nk+1), the theo-
retical runtime of the TurboParser’s dual decompo-
sition inference algorithm.

Acknowledgments

The third author was partly supported by a research
grant from the Microsoft/Technion research center
for electronic commerce: Context Sensitive Sen-
tence Understanding for Natural Language Process-
ing.

References
Mariana SC Almeida, Cláudia Pinto, Helena Figueira,

Pedro Mendes, and André FT Martins. 2015. Align-
ing opinions: Cross-lingual opinion mining with de-
pendencies. In ACL.

Bernd Bohnet and Joakim Nivre. 2012. A transition-
based system for joint part-of-speech tagging and la-
beled non-projective dependency parsing. In Pro-

719



ceedings of EMNLP-CoNLL. Association for Compu-
tational Linguistics.

Sabine Buchholz and Erwin Marsi. 2006. Conll-x shared
task on multilingual dependency parsing. In CoNLL.

Xavier Carreras. 2007. Experiments with a higher-order
projective dependency parser. In EMNLP-CoNLL.

Jinho D Choi and Andrew McCallum. 2013a.
Transition-based dependency parsing with selectional
branching. In ACL.

Jinho D Choi and Andrew McCallum. 2013b.
Transition-based dependency parsing with selectional
branching. In ACL.

Koby Crammer, Ofer Dekel, Joseph Keshet, Shai Shalev-
Shwartz, and Yoram Singer. 2006. Online passive-
aggressive algorithms. The Journal of Machine Learn-
ing Research, 7:551–585.

J. Edmonds. 1967. Optimum branchings. Journal of Re-
search of the National Bureau of Standards, 71B:233–
240.

Jason Eisner. 1996. Efficient normal-form parsing for
combinatory categorial grammar. In ACL.

Yoav Goldberg and Michael Elhadad. 2010. An effi-
cient algorithm for easy-first non-directional depen-
dency parsing. In NAACL-HLT.

Yoav Goldberg and Joakim Nivre. A dynamic oracle for
arc-eager dependency parsing. In COLING.

Yoav Goldberg and Joakim Nivre. 2013. Training
deterministic parsers with non-deterministic oracles.
Transactions of the Association for Computational
Linguistics, 1(Oct):403–414.

Matthew Gormley, Mark Dredze, and Jason Eisner. 2015.
Approximation-aware dependency parsing by belief
propagation. Transactions of the Association for Com-
putational Linguistics, 3:489–501.

Matthew Honnibal, Yoav Goldberg, and Mark Johnson.
2013. A non-monotonic arc-eager transition system
for dependency parsing. In CoNLL.

Liang Huang, Wenbin Jiang, and Qun Liu. 2009.
Bilingually-constrained (monolingual) shift-reduce
parsing. In EMNLP.

Terry Koo and Michael Collins. 2010. Efficient third-
order dependency parsers. In ACL.

Effi Levi, Roi Reichart, and Ari Rappoport. 2016. Edge-
linear first-order dependency parsing with undirected
minimum spanning tree inference. In ACL.

Omer Levy and Yoav Goldberg. 2014. Neural word em-
bedding as implicit matrix factorization. In NIPS.

André FT Martins, Dipanjan Das, Noah A Smith, and
Eric P Xing. 2008. Stacking dependency parsers. In
EMNLP.

A. Martins, M. Almeida, and N. A. Smith. 2013. Turn-
ing on the turbo: Fast third-order non-projective turbo
parsers. In ACL.

Ryan McDonald and Fernando Pereira. 2006. On-
line learning of approximate dependency parsing al-
gorithms. In EACL.

Ryan McDonald, Fernando Pereira, Kiril Ribarov, and
Jan Hajic. 2005. Non-projective dependency parsing
using spanning tree algorithms. In HLT-EMNLP.

Jens Nilsson, Sebastian Riedel, and Deniz Yuret. 2007.
The conll 2007 shared task on dependency parsing.
In Proceedings of the CoNLL shared task session of
EMNLP-CoNLL.

Joakim Nivre and Ryan McDonald. 2008. Integrating
graph-based and transition-based dependency parsers.
In ACL-08: HLT, June.

Joakim Nivre, Johan Hall, Jens Nilsson, Atanas Chanev,
Gülsen Eryigit, Sandra Kübler, Svetoslav Marinov,
and Erwin Marsi. 2007. Maltparser: A language-
independent system for data-driven dependency pars-
ing. Natural Language Engineering, 13(02):95–135.

Sebastian Riedel, David Smith, and Andrew McCallum.
2012. Parse, price and cut – delayed column and
row generation for graph based parsers. In EMNLP-
CoNLL.

Kenji Sagae and Alon Lavie. 2006. A best-first prob-
abilistic shift-reduce parser. In Proc. of the COL-
ING/ACL on Main conference poster sessions.

David Smith and Jason Eisner. 2008. Dependency pars-
ing by belief propagation. In EMNLP.

Ivan Titov and James Henderson. 2007. Fast and robust
multilingual dependency parsing with a generative la-
tent variable model. In EMNLP-CoNLL.

Stephen Tratz and Eduard Hovy. 2011. A fast, accu-
rate, non-projective, semantically-enriched parser. In
EMNLP.

Fei Wu and Daniel S Weld. 2010. Open information
extraction using wikipedia. In ACL.

Yue Zhang and Stephen Clark. 2008. A tale of
two parsers: Investigating and combining graph-based
and transition-based dependency parsing using beam-
search. In EMNLP.

Yue Zhang and Joakim Nivre. 2011. Transition-based
dependency parsing with rich non-local features. In
ACL.

Yuan Zhang, Tao Lei, Regina Barzilay, and Tommi
Jaakkola. 2014a. Greed is good if randomized: New
inference for dependency parsing. In EMNLP.

Yuan Zhang, Tao Lei, Regina Barzilay, Tommi Jaakkola,
and Amir Globerson. 2014b. Steps to excellence:
Simple inference with refined scoring of dependency
trees. In ACL.

720


