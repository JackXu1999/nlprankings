



















































Efficient techniques for parsing with tree automata


Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 2042–2051,
Berlin, Germany, August 7-12, 2016. c©2016 Association for Computational Linguistics

Efficient techniques for parsing with tree automata

Jonas Groschwitz and Alexander Koller
Department of Linguistics

University of Potsdam
Germany

groschwi|akoller@uni-potsdam.de

Mark Johnson
Department of Computing

Macquarie University
Australia

mark.johnson@mq.edu.au

Abstract

Parsing for a wide variety of grammar for-
malisms can be performed by intersecting
finite tree automata. However, naive im-
plementations of parsing by intersection
are very inefficient. We present techniques
that speed up tree-automata-based pars-
ing, to the point that it becomes practically
feasible on realistic data when applied to
context-free, TAG, and graph parsing. For
graph parsing, we obtain the best runtimes
in the literature.

1 Introduction

Grammar formalisms that go beyond context-free
grammars have recently enjoyed renewed atten-
tion throughout computational linguistics. Clas-
sical grammar formalisms such as TAG (Joshi and
Schabes, 1997) and CCG (Steedman, 2001) have
been equipped with expressive statistical mod-
els, and high-performance parsers have become
available (Clark and Curran, 2007; Lewis and
Steedman, 2014; Kallmeyer and Maier, 2013).
Synchronous grammar formalisms such as syn-
chronous context-free grammars (Chiang, 2007)
and tree-to-string transducers (Galley et al., 2004;
Graehl et al., 2008; Seemann et al., 2015) are
being used as models that incorporate syntac-
tic information in statistical machine translation.
Synchronous string-to-tree (Wong and Mooney,
2006) and string-to-graph grammars (Chiang et
al., 2013) have been applied to semantic parsing;
and so forth.

Each of these grammar formalisms requires its
users to develop new algorithms for parsing and
training. This comes with challenges that are both
practical and theoretical. From a theoretical per-
spective, many of these algorithms are basically
the same, in that they rest upon a CKY-style pars-

ing algorithm which recursively explores substruc-
tures of the input object and assigns them non-
terminal symbols, but their exact relationship is
rarely made explicit. On the practical side, this
parsing algorithm and its extensions (e.g. to EM
training) have to be implemented and optimized
from scratch for each new grammar formalism.
Thus, development time is spent on reinventing
wheels that are slightly different from previous
ones, and the resulting implementations still tend
to underperform.

Koller and Kuhlmann (2011) introduced Inter-
preted Regular Tree Grammars (IRTGs) in order
to address this situation. An IRTG represents
a language by describing a regular language of
derivation trees, each of which is mapped to a term
over some algebra and evaluated there. Gram-
mars from a wide range of monolingual and syn-
chronous formalisms can be mapped into IRTGs
by using different algebras: Context-free and tree-
adjoining grammars use string algebras of differ-
ent kinds, graph grammars can be captured by us-
ing graph algebras, and so on. In addition, IRTGs
come with a universal parsing algorithm based on
closure results for tree automata. Implementing
and optimizing this parsing algorithm once, one
could apply it to all grammar formalisms that can
be mapped to IRTG. However, while Koller and
Kuhlmann show that asymptotically optimal pars-
ing is possible in theory, it is non-trivial to imple-
ment their algorithm optimally.

In this paper, we introduce practical algorithms
for the two key operations underlying IRTG pars-
ing: computing the intersection of two tree au-
tomata and applying an inverse tree homomor-
phism to a tree automaton. After defining IRTGs
(Section 2), we will first illustrate that a naive
bottom-up implementation of the intersection al-
gorithm yields asymptotic parsing complexities
that are too high (Section 3). We will then

2042



show how the parsing complexity can be im-
proved by combining algebra-specific index data
structures with a generic parsing algorithm (Sec-
tion 4), and by replacing bottom-up with top-down
queries (Section 5). In contrast to the naive al-
gorithm, both of these methods achieve the ex-
pected asymptotic complexities, e.g. O(n3) for
context-free parsing, O(n6) for TAG parsing, etc.
Furthermore, an evaluation with realistic gram-
mars shows that our algorithms improve practi-
cal parsing times with IRTG grammars encoding
context-free grammars, tree-adjoining grammars,
and graph grammars by orders of magnitude (Sec-
tion 6). Thus our algorithms make IRTG pars-
ing practically feasible for the first time; for graph
parsing, we obtain the fastest reported runtimes.

2 Interpreted Regular Tree Grammars

We will first define IRTGs and explain how the
universal parsing algorithm for IRTGs works.

2.1 Formal foundations

First, we introduce some fundamental theoretical
concepts and notation.

A signature Σ is a finite set of symbols r, f, . . .,
each of which has an arity ar(r) ≥ 0. A tree t over
the signature Σ is a term of the form r(t1, . . . , tn),
where the ti are trees and r ∈ Σ has arity n. We
identify the nodes of t by their Gorn addresses, i.e.
paths π ∈ N∗ from the root to the node, and write
t(π) for the label of π. We write TΣ for the set of
all trees over Σ, and TΣ(Xk) for the trees in which
each node either has a label from Σ, or is a leaf
labeled with one of the variables {x1, . . . , xk}.

A (linear, nondeleting) tree homomorphism h
from a signature Σ to a signature ∆ is a mapping
h : TΣ → T∆. It is defined by specifying, for each
symbol r ∈ Σ of arity k, a term h(r) ∈ T∆(Xk)
in which each variable occurs exactly once. This
symbol-wise mapping is lifted to entire trees by
letting h(r(t1, . . . , tk)) = h(r)[h(t1), . . . , h(tk)],
i.e. by replacing the variable xi in h(r) by the re-
cursively computed value h(ti).

Let ∆ be a signature. A ∆-algebra A con-
sists of a nonempty set A, called the domain, and
for each symbol f ∈ ∆ with arity k, a function
fA : Ak → A, the operation associated with
f . We can evaluate any term t ∈ T∆ to a value
tA ∈ A, by evaluating the operation symbols
bottom-up. In this paper, we will be particularly
interested in the string algebra E∗ over the finite

automaton rule homomorphism
S→ r1(NP,VP) ∗(x1, x2)
NP→ r2 John
VP→ r3 walks
VP→ r4(VP,NP) ∗(x1, ∗(on, x2))
NP→ r5 Mars

Figure 1: An example IRTG.

alphabet E. Its domain is the set of all strings over
E. For each symbol a ∈ E, it has a nullary oper-
ation symbol a with aE

∗
= a. It also has a single

binary operation symbol ∗, such that ∗E∗(w1, w2)
is the concatenation of the stringsw1 andw2. Thus
the term ∗(John, ∗(walks, ∗(on,Mars))) in Fig. 2b
evaluates to the string “John walks on Mars”.

A finite tree automaton M over the signature Σ
is a structure M = (Σ, Q,R,XF ), where Q is a
finite set of states and XF ∈ Q is a final state.
R is a finite set of transition rules of the form
X → r(X1, . . . , Xk), where the terminal symbol
r ∈ Σ is of arity k and X,X1, . . . , Xk ∈ Q. A
tree automaton can run non-deterministically on
a tree t ∈ TΣ by assigning states to the nodes
of t bottom-up. If we have t = r(t1, . . . , tn)
and M can assign the state Xi to each ti, written
Xi →∗ ti, then we also have X →∗ t. We say that
M accepts t if XF →∗ t, and define the language
L(M) ⊆ TΣ of M as the (possibly infinite) set
of all trees that M accepts. An example of a tree
automaton (with states S, NP, etc.) is shown in
the “automaton rule” column of Fig. 1. It accepts,
among others, the tree τ1 in Fig. 2a.

Tree automata can be defined top-down or
bottom-up, and are equivalent to regular tree
grammars. The languages that can be accepted by
finite tree automata are called the regular tree lan-
guages. See e.g. Comon et al. (2008) for details.

2.2 Interpreted regular tree grammars

We can combine tree automata, homomorphisms,
and algebras into grammars that can describe lan-
guages of arbitrary objects, as well as relations be-
tween such objects – in a way that inherits many
technical properties from context-free grammars,
while extending the expressive capacity.

An interpreted regular tree grammar
(IRTG, Koller and Kuhlmann (2011))
G = (M, (h1,A1), . . . , (hn,An)) consists of
a tree automaton M over some signature Σ,
together with an arbitrary number n of inter-

2043



r1

r2 r4

r3 r5

(a) Tree τ1.

h−→

∗

John ∗

walks ∗
on Mars

(b) Term h (τ1).

evaluate−−−−→ “John walks on Mars”

(c) h (τ1) evaluated in E∗.

Figure 2: The tree τ1, evaluated by the homomorphism h and the algebra E∗

pretations (hi,Ai), where each Ai is an algebra
over some signature ∆i and each hi is a tree
homomorphism from Σ to ∆i. The automaton
M describes a language L(M) of derivation
trees which represent abstract syntactic structures.
Each derivation tree τ is then interpreted n ways:
we map it to a term hi(τ) ∈ T∆i , and then we
evaluate hi(τ) to a value ai = hi(τ)Ai ∈ Ai of the
algebra Ai. Thus, the IRTG G defines a language
L(G) = {(h1(τ)A1 , . . . , hn(τ)An) | τ ∈ L(M)},
which is an n-place relation between the domains
of the algebras.

Consider the IRTG G shown in Fig. 1. The “au-
tomaton rule” column indicates the five rules of
M ; the final state is S. We already saw the deriva-
tion tree τ1 ∈ L(M). G has a single interpre-
tation, into a string algebra E∗, and with a ho-
momorphism that is specified by the “homomor-
phism” column; for instance, h(r1) = ∗(x1, x2)
and h(r2) = John. Applying this homomorphism
to τ1, we obtain the term h(τ1) in Fig. 2b. As we
saw earlier, this term evaluates in the string alge-
bra to the string “John walks on Mars” (Fig. 2c).
Thus this string is an element of L(G).

We assume that no two rules of M use the same
terminal symbol; this is generally not required
in tree automata, but every IRTG can be brought
into this convenient form. Furthermore, we focus
(but only for simplicity of presentation) on IRTGs
that use a single string-algebra interpretation, as in
Fig. 1. Such grammars capture context-free gram-
mars. However, IRTGs can capture a wide vari-
ety of grammar formalisms by using different al-
gebras. For instance, an interpretation that uses
a TAG string algebra (or TAG derived-tree alge-
bra) models a tree-adjoining grammar (Koller and
Kuhlmann, 2012), and an interpretation into an
s-graph algebra models a hyperedge replacement
graph grammar (HRG, Groschwitz et al. (2015)).
By using multiple algebras, IRTGs can also repre-

sent synchronous grammars and (bottom-up) tree-
to-tree and tree-to-string transducers. In general,
any grammar formalism whose grammars describe
derivations in terms of a finite set of states can typ-
ically be converted into IRTG.

2.3 Parsing IRTGs

Koller and Kuhlmann (2011) present a uniform
parsing algorithm for IRTGs based on tree au-
tomata. The (monolingual) parsing problem of
IRTG consists in determining, for an IRTG G and
an input object a ∈ A, a representation of the set
parses(a) = {τ ∈ L(M) | h(τ)A = a}, i.e. of the
derivation trees that are grammatically correct and
are mapped to a by the interpretation. In the ex-
ample, we have parses(“John walks on Mars”) =
{τ1}, where τ1 is as above. In general, parses(a)
may be infinite, and thus we aim to represent
it using a tree automaton Cha with L(Cha) =
parses(a), the parse chart of a.

We can compute Cha as follows. First, ob-
serve that parses(a) = L(M) ∩ h−1(terms(a)),
where h−1(L) = {τ ∈ TΣ | h(τ) ∈ L} (the in-
verse homomorphic image, or invhom, of L) and
terms(a) = {t ∈ T∆ | tA = a}, i.e. the set
of all terms that evaluate to a. Now assume that
the algebra A is regularly decomposable, which
means that every a ∈ A has a decomposition au-
tomatonDa, i.e. there is a tree automatonDa such
that L(Da) = terms(a). Because regular tree lan-
guages are closed under invhom and intersection,
we can then compute a tree automaton Cha by in-
tersecting M with the invhom of Da.

To illustrate the IRTG parsing algorithm, let us
compute a chart for the sentence s = “John walks
on Mars” with the example grammar G of Fig. 1.
The states of the decomposition automaton Ds are
spans [i, k] of s; the final state is XF = [1, 5]. The
automaton contains fourteen rules, including the
ones shown in Fig. 3a.

2044



[1, 5]→ ∗([1, 2], [2, 5])
[2, 5]→ ∗([2, 3], [3, 5])
[3, 5]→ ∗([3, 4], [4, 5])
[3, 4]→ on
[4, 5]→ Mars

(a) Some rules of Ds.

[1, 5]→ r1([1, 2], [2, 5])
[2, 5]→ r4([2, 3], [4, 5])
[2, 4]→ r1([2, 3], [3, 4])
[1, 2]→ r2
[2, 3]→ r3

(b) Some rules of I = h−1 (Ds).

S[1, 5]→ r1(NP[1, 2],VP[2, 5])
VP[2, 5]→ r4(VP[2, 3],NP[4, 5])
NP[1, 2]→ r2
VP[2, 3]→ r3
NP[4, 5]→ r5

(c) The parse chart Chs.

Figure 3: Example rules for the sentence s = “John walks on Mars”

Algorithm 1 Naive bottom-up intersection
1: initialize agenda with state pairs for constants
2: initialize P as empty
3: while agenda is not empty do
4: T ′X ′ ← pop(agenda)
5: add T ′X ′ to P
6: for T ′′X ′′ ∈ P do
7: for {T1X1, T2X2} = {T ′X ′, T ′′X ′′} do
8: for T → r(T1, T2) in ML do
9: for X → r(X1, X2) in MR do

10: store TX → r(T1X1, T2X2)
11: add TX to agenda if new

We can then compute the invhom automaton I ,
such that L(I) = h−1(L(Ds)). I uses the same
states as Ds, but uses terminal symbols from Σ in-
stead of ∆. Some rules of the invhom automaton I
in the example are shown in Fig. 3b. Notice that I
also contains rules that are not consistent with M ,
i.e. that would not occur in a grammatical parse
of the sentence, such as [2, 4] → r1([2, 3], [3, 4]).
Finally, the chart Chs is computed by intersecting
M with I (see Fig. 3c). The states of Chs are pairs
of states from M and states from I . It accepts τ1,
because τ1 ∈ parses(s). Observe the similarity to
a traditional context-free parse chart.

3 Bottom-up intersection

Both the practical efficiency of this algorithm
and its asymptotic complexity depend crucially on
how we compute intersection and invhom. We il-
lustrate this using an overly naive intersection al-
gorithm as a strawman, and then analyze the prob-
lem to lay the foundations for the improved algo-
rithms in Sections 4 and 5.

Let’s say that we want to compute a tree au-
tomaton C for the intersection of a “left” automa-
ton ML and a “right” automaton MR both over
the same signature Σ. In the application to IRTG
parsing, ML is typically the derivation tree au-

tomaton (called M above) and MR is the inv-
hom of a decomposition automaton. As in the
product construction for finite string automata, the
states of C will be pairs TX of states T of ML
and states X of MR, and the rules of C will
all have the form TX → r(T1X1, . . . , TnXn),
where T → r(T1, . . . , Tn) is a rule in ML, and
X → r(X1, . . . , Xn) is a rule in MR.
3.1 Naive intersection

A naive bottom-up algorithm is shown in Alg. 1.1

This algorithm maintains an agenda of state pairs
that have been discovered, but not explored as
children of bottom-up rule applications; and a
chart-like set P of all state pairs that have ever
been popped off the agenda. The algorithm main-
tains the invariant that if TX is on the agenda or
in P , then T and X are partners (written T ≈ X),
i.e. there is a tree t ∈ TΣ such that T →∗ t in ML
and X →∗ t in MR.

The agenda is initialized with all state pairs
TX , for which ML has a rule T → r and MR
has a rule X → r for some nullary symbol r ∈ Σ.
Then, while there are state pairs left on the agenda,
Alg. 1 pops a state pair T ′X ′ off the agenda and
adds it to P ; iterates over all state pairs T ′′X ′′ in
P ; and queriesML andMR bottom-up for rules in
which these states appear as children.2 The itera-
tion in line 7 allows T ′ and X ′ to be either left or
right children in these rules. For each pair of left
and right rules, the rules are combined into a rule
of C, and the pair of the parent states T and X is
added to the agenda.

This naive intersection algorithm yields an
asymptotic complexity for IRTG parsing that is
higher than expected. Assume, for example,

1We assume binary symbols for simplicity; all algorithms
generalize to arbitrary arities.

2For the invhom automaton this can be done by substi-
tuting the variables in the homomorphic image h(r) with the
corresponding states X ′ and X ′′, and running the decompo-
sition automaton on the resulting tree.

2045



Algorithm 2 Bottom-up intersection with BU
1: initialize agenda with state pairs for constants
2: generate new Sr = S(MR, r) for every r ∈ Σ
3: while agenda is not empty do
4: T ′X ′ ← pop(agenda)
5: for T → r(T1, T2) in ML s.t. Ti = T ′ do
6: for X → r(X1, X2) ∈ BU(Sr, i,X ′) do
7: store rule TX → r(T1X1, T2X2)
8: add TX to agenda if new

that we are parsing with an IRTG encoding of a
context-free grammar, i.e. with a string algebra
(as in Fig. 1). Then the states of MR are spans
[i, k], i.e. MR has O(n2) states. Once line 4 has
picked a span X ′ = [i, j], line 6 iterates over all
spans X ′′ = [k, l] that have been discovered so
far – including ones in which j 6= k and i 6= l.
Thus the bottom-up lookup in line 9 is executed
O(n4) times, most of which will yield no rules.
The overall runtime of Alg. 1 is therefore higher
than the asymptotic runtime ofO(n3) expected for
context-free parsing. Similar problems arise for
other algebras; for instance, the runtime of Alg. 1
for TAG parsing is O(n8) rather than O(n6).

3.2 Indexing
In context-free parsing algorithms, such as CKY
or Earley, this issue is addressed through appro-
priate index datastructures, which organize P such
that the lookup in line 5 only returns state pairs
where X ′′ is of the form [j, k] or [k, i]. This re-
duces the runtime to cubic.

The idea of obtaining optimal asymptotic com-
plexities in IRTG parsing through appropriate in-
dexing was already mentioned from a theoreti-
cal perspective by Koller and Kuhlmann (2011).
However, they assumed an optimal indexing data
structure as given. In practice, indexing requires
algebra-specific knowledge about X ′′: A CKY-
style index only works if we assume that the states
of the decomposition automaton are spans (this is
not the case in other algebras), and that the only
binary operation in the string algebra is ∗, which
composes spans in a certain way. Furthermore, in
IRTG parsing the rules of the invhom automaton
do not directly correspond to algebra operations,
but to terms of operations, which further compli-
cates indexing.

In this paper, we incorporate indexing into the
intersection algorithm through sibling-finders. A
sibling-finder S = S(M, r) for an automaton M

and a label r in M ’s signature is a data structure
that supports a single operation, BU(S, i,X ′). We
require that a call to BU(S, i,X ′) returns the set
of rules X → r(X1, . . . , Xn) of M such that X ′
is the i-th child state, and for every j 6= i, Xj
must be a state for which we previously called
BU(S, j,Xj). Thus a sibling-finder performs a
bottom-up rule lookup, changing its state after
each call by caching the state and position.

Assume that we have sibling-finders for MR.
Then we can modify the naive Alg. 1 to the closely
related algorithm shown as Alg. 2. This algorithm
maintains the same agenda as Alg. 1, but instead
of iterating over all explored partner states T ′′X ′′,
it first iterates over all rules in ML that have T ′ as
a child (line 5). In line 6, Alg. 2 then queries MR
sibling-finders – we maintain one for each rule la-
bel – for right rules with matching rule label and
child positions. Note that because there is only one
rule with label r inML, the sibling-finders implic-
itly keep track of the partners of T2 we have seen
so far. Thus they play the role of a more structured
variant of P .

There are a number of ways in which sibling-
finders can be implemented. First, they could
simply maintain sets chi(Sr, i) where a call to
BU(Sr, i,X ′) first adds X ′ to chi(Sr, i). The
query can then iterate over the set chi(Sr, 3−i), to
check for each stateX ′′ in that set whetherMR ac-
tually contains a rule with terminal symbol r and
children X ′ and X ′′ (in the right order). This es-
sentially reimplements the behavior of Alg. 1, and
comes with the same complexity issues.

Second, we could theoretically iterate over all
rules of MR to implement the sibling finders via
a bottom-up index (e.g., a trie) that supports effi-
cient BU queries. However, in IRTG parsing MR
is the invhom of a decomposition automaton. Be-
cause the decomposition automaton represents all
the ways in which the input object can be built re-
cursively out of smaller structures, including ones
which will later be rejected by the grammar, such
automata can be very large in practice. Thus we
would like to work with a lazy representation of
MR and avoid iterating over all rules.

4 Efficient bottom-up lookup

Finally, we can exploit the fact that in IRTG
parsing, MR is the invhom of a decomposition
automaton. Below, we first show how to de-
fine algebra-specific sibling-finders for decompo-

2046



Algorithm 3 passUpwards(Y, π, i, r)
1: rules← BU(Sr,π, i, Y )
2: if π = π′k 6= � then
3: for X → f(X1, . . . , Xn) ∈ rules do
4: passUpwards(X,π′, k, r)

sition automata. Then we develop an algebra-
independent way to generate invhom sibling-
finders out of those for the decomposition au-
tomata. These can be plugged into Alg. 2 to
achieve the expected parsing complexity.

4.1 Sibling-finders for decomposition
automata

First, consider the special case of sibling-finders
for a decomposition automaton D. The terminal
symbols f of D are the operation symbols of an
algebra. If we have information about the opera-
tions of this algebra, and how they operate on the
states of D, a sibling-finder S = S(D, f) can use
indexing specific to the operation f to look up po-
tential siblings, and only for them query D to an-
swer BU(S, i,X)

For instance, a sibling-finder for the ‘∗’ op-
eration of the string algebra may store all states
[k, l] for i = 1 under the index l. Thus a lookup
BU(S, 2, [l,m]) can directly retrieve siblings from
the l-bin, just as a traditional parse chart would.
Spans which do not end at l are never consid-
ered. Different algebras require different index
structures. For instance, sibling-finders for the
string-wrapping operation in the TAG string alge-
bra might retrieve all pairs of substrings [k, l,m, o]
that wrap around [l,m] instead. Analogous data
structures can be defined for the s-graph algebra.

4.2 Sibling-finders for invhom
We can build upon the D-sibling-finders to con-
struct sibling-finders for the invhom I of D.
The basic idea is as follows. Consider the term
h (r1) = ∗(x1, x2) from Fig. 1. It contains a sin-
gle operation symbol ∗ (plus variables); the homo-
morphism only replaces one symbol with another.
Thus a sibling-finder S(D, ∗) from the decompo-
sition automaton can directly serve as a sibling-
finder S(I, r1). We only need to replace the ∗ label
on the returned rules with r1.

In general, the situation is more complicated,
because t = h(r) may be a complex term con-
sisting of many algebra operations. In such a
case, we construct a separate sibling-finder Sr,π =

new S(D, t(π)) for each node π with at least two
children. For instance, consider the term t =
h (r4) in Fig. 1. It contains three nodes which
are labeled by algebra operations, two of which
are the concatenation. We decorate these with the
sibling-finders Sr4,� and Sr4,1. Each of these is a
sibling-finder for the algebra’s concatenation op-
eration; but they may have different state because
they received different queries.

We can then construct an invhom sibling-
finder Sr = S(I, r), which answers a query
BU(Sr, i,X ′) in two steps. First, we substitute the
variable xi by the state X ′ and percolate it upward
through t using the D-sibling-finders on the path
from xi to the root. If π = π′k is the path to xi,
we do this by calling passUpwards(X ′, π′, k, r),
as defined in Alg. 3. If the local sibling-finder re-
turns rules and we are not at the root yet, we recur-
sively call passUpwards at the parent node π′ with
each parent state of these rules.

As we do this, we let each sibling-finder main-
tain the set of rules it found, indexed by their
parent state. This allows us to perform the sec-
ond step: we traverse t top-down from the root
to extract the rules of the invhom automaton that
answer the BU query. Recall that BU(Sr, i,X ′)
should return only rules X → r(X1, X2) where
BU(Sr, 3− i,X3−i) was called before. Here, this
is guaranteed by having distinct D-sibling-finders
Sπ,r for every node π at every tree h(r). A final
detail is that before the first query to r, we initial-
ize the sibling-finders by calling passUpwards for
all the leaves that are labeled by constants.

This process is illustrated in Fig. 4, on the
sibling-finder S = S(I, r4) and the input string
“John walks on a hill on Mars”, parsed with a
suitable extension of the IRTG in Fig. 1. The de-
composition automaton can accept the word “on”
from states [3, 4] and [6, 7], which during initial-
ization are entered into position 1 of the lower
D-sibling-finder Sr4,2, indexed by their end po-
sitions (a). Alg. 2 may then generate the query
BU(S, 2, [4, 6]). This enters [4, 6] into the lower
D-sibling-finder, and because there is a state with
end position 4 on the left side of this sibling-
finder, BU(Sr4,2, 2, [4, 6]) returns a rule with par-
ent [3, 6]. The parent is subsequently entered into
the upper sibling-finder (b). Finally, the query
BU(S, 1, [2, 3]) enters [2, 3] into the upper D-
sibling-finder and discovers its sibling [3, 6] (c).
This yields a state X = [2, 6] for the whole phrase

2047



∗[ ∅ ∅ ]
x1 ∗[

4 : [3,4] ∅ ]
7 : [6,7]

on x2

(a) After initialization.

∗[ ∅ 3 : [3,6] ]
x1 ∗[

4 : [3, 4] 4 : [4,6]
]

7 : [6, 7]

on x2

(b) After BU(S, 2, [4, 6]).

∗[
3 : [2,3] 3 : [3, 6]

]
x1 ∗[

4 : [3, 4] 4 : [4, 6]
]

7 : [6, 7]

on x2

(c) After BU(S, 1, [2, 3]).

Figure 4: Three stages of BU on S(I, r4) for the sentence “John walks on a hill on Mars”.

Algorithm 4 Top-down intersection
1: function expand(X):
2: if X /∈ visited then
3: add X to visited
4: for X → r(X1, X2) in MR do
5: call expand(Xi) for i = 1, 2
6: for T→r(T1, T2) s.t. Ti ∈ prt(Xi) do
7: store rule TX→r(T1X1, T2X2)
8: add T to prt(X)

“walks on a hill”. The top-down traversal of the
sibling-finders reveals that this state is reached by
combining x1 = [2, 3], for which this BU query
asked, with x2 = [4, 6], and thus the BU query
yields the rule [2, 6] → r4([2, 3], [4, 6]). A sub-
sequent query for BU(S, 2, [4, 8]) would yield the
rule [2, 8]→ r4([2, 3], [4, 8]), and so on.

The overall construction allows us to answer
BU queries on invhom automata while making use
of algebra-specific index structures. Given suit-
able index structures, the asymptotic complexity
drops down to the expected levels, e.g. O(n3) for
IRTGs using the string algebra,O(n6) for the TAG
string algebra, and so on. This yields a practical
algorithm that can be flexibly adapted to new al-
gebras by implementing their sibling-finders.

5 Top-down intersection

Instead of investing into efficient bottom-up
queries, we can also explore the use of top-down
queries instead. These ask for all rules with parent
stateX and terminal symbol r. Such queries com-
pletely avoid the problem of finding siblings in
MR. An invhom automaton can answer top-down
queries for r efficiently by running the decomposi-
tion automaton top-down on h(r), collecting child
states at the variable nodes. For instance, if we
query I from Section 2 top-down for rules with

the parent [1, 5] and symbol r1, it will enumer-
ate the rules [1, 5] → r1([1, 2], [2, 5]), [1, 5] →
r1([1, 3], [3, 5]), and [1, 5] → r1([1, 4], [4, 5]),
without ever considering any other combination of
child states.

This is the idea underlying the intersection al-
gorithm in Alg. 4. It recursively visits states X of
MR, collecting for each X a set prt(X) of states
T ofML such that T ≈ X . Line 5 ensures that the
prt sets have been computed for both child states
of the rule X → r(X1, X2). Line 6 then does a
bottom-up lookup of ML rules with the terminal
symbol r and with child states that are partners
of X1 and X2. Applied to our running example,
Alg. 4 parses “John walks on Mars” by recursive
calls on expand([1, 5]) and expand([2, 5]), fol-
lowing the rules of I top-down. Recursive calls
for [2, 3] and [4, 5] establish VP ∈ prt([2, 3]) and
NP ∈ prt([4, 5]), which enables the recursive call
for [2, 5] to apply r4 in line 6 and consequently add
VP to prt([2, 5]) in line 8.

The algorithm mixes top-down queries to MR
with bottom-up queries toML. Line 6 implements
the core idea of the CKY parser, in that it performs
bottom-up queries on sets of nonterminals that are
partners of adjacent spans – but generalized to ar-
bitrary IRTGs instead of just the string algebra.
The top-down query to MR in line 4 is bounded
by the number of rules that actually exist in MR,
which isO(n3) for the string algebra,O(n6) in the
TAG string algebra, and O

(
ns · 3dsds) for graphs

of degree d and treewidth s − 1 in the graph al-
gebra. Thus Alg. 4 achieves the same asymptotic
complexity as native parsing algorithms.

Condensed top-down intersection. One weak-
ness of Alg. 4 is that it iterates over all rules
X → r(X1, X2) of MR individually. This can be
extremely wasteful when MR is the invhom of a
decomposition automaton, because it may contain

2048



0 10 20 30 40 50

sentenceLength

101

102

103

104

105

106

107

ru
n
ti

m
e
 [

m
s]

bottom-up

top-down

top-down cond.

sibling-finder

Figure 5: Runtimes for context-free parsing.

a great number of rules that have the same states
and only differ in the terminal symbol r. For in-
stance, when we encode a context-free grammar as
an IRTG, for every rule r of the form A→ B C we
have h(r) = ∗(x1, x2). The rules of the invhom
automaton are the same for all terminal symbols r
with the same term h(r). But Alg. 4 iterates over
rules r and not over different terms h(r), repeating
the exact same computation for every binary rule
of the context-free grammar.

To solve this, we define condensed tree au-
tomata, which have rules of the form X →
ρ(X1, . . . , Xn), where ρ ⊆ Σ is a nonempty set
of symbols with arity n. A condensed automaton
represents the tree automaton which for all con-
densed rules X → ρ(X1, . . . , Xn) and all r ∈ ρ
has the rule X → r(X1, . . . , Xn). It is straight-
forward to represent an invhom automaton as a
condensed condensed automaton, by determining
for each distinct homomorphic image t the set
ρt = {r1, . . . , rk} of symbols with h(ri) = t.

We can modify Alg. 4 to iterate over condensed
rules in line 4, and to iterate in line 6 over the rules
T → r(T1, T2) for which Ti ∈ prt(Xi) and r ∈ ρ.
This bottom-up query to ML can be answered ef-
ficiently from an appropriate index on the rules of
ML. Altogether, this condensed intersection algo-
rithm can be dramatically faster than the original
version, if the grammar contains many symbols
with the same homomorphic image.

6 Evaluation

We compare the runtime performance of the pro-
posed algorithms on practical grammars and in-
puts, from three very different grammar for-
malisms: context-free grammars, TAG, and HRG
graph grammars. In each setting, we measure the

0 10 20 30 40 50

sentenceLength

102

103

104

105

106

107

ru
n
ti

m
e
 [

m
s]

bottom-up

top-down

top-down cond.

sibling-finder

Figure 6: Runtimes for TAG parsing.

1 2 3 4 5 6 7 8 9 10

nodeCount

101

102

103

104

105

106

107

ru
n
ti

m
e
 [

m
s]

bottom-up

top-down

top-down cond.

sibling-finder

GKT 15

Figure 7: Runtimes for graph parsing.

runtime of four algorithms: the naive bottom-up
baseline of Section 3; the sibling-finder algorithm
from Section 4; and the non-condensed and the
condensed version of the top-down algorithm from
Section 5. The results are shown in Figures 5, 6
and 7. We measure the runtimes for computing
the complete chart, and plot the geometric mean
of runtimes for each input size on a log scale.

We measured all runtimes on an Intel Xeon E7-
8857 CPU at 3 GHz using Java 8. The JVM was
warmed up before the measurements. The parser
filtered each grammar automatically, removing all
rules whose homomorphic image contained a con-
stant that could not be used for a given input (e.g.,
a word that did not occur in the sentence).

PCFG. We extracted a binarized context-free
grammar with 6929 rules from Section 00 of the
Penn Treebank, and parsed the sentences of Sec-
tion 00 with it. The homorphism in the corre-
sponding IRTG assigns every terminal symbol a
constant or the term ∗(x1, x2), as in Fig. 1. As a
consequence, the condensed automaton optimiza-
tion from Section 5 outperforms all other algo-

2049



rithms, achieving a 100x speedup over the naive
bottom-up algorithm when it was cancelled.

TAG. We also extracted a tree-adjoining gram-
mar from Section 00 of the PTB as described by
Chen and Vijay-Shanker (2000), converted it to
an IRTG as described by Koller and Kuhlmann
(2012), and binarized it, yielding an IRTG with
26652 rules. Each term h(r) in this grammar
represents an entire TAG elementary tree, which
means the terms are much more complex than
for the PCFG and there are much fewer terminal
symbols with the same homomorphic term. As a
consequence, condensing the invhom is much less
helpful. However, the sibling-finder algorithm ex-
cels at maintaining state information within each
elementary tree, yielding a 1000x speedup over the
naive bottom-up algorithm when it was cancelled.

Graphs. Finally, we parsed a corpus of graphs
instead of strings, using the 13681-rule graph
grammar of Groschwitz et al. (2015) to parse the
1258 graphs with up to 10 nodes from the “Little
Prince” AMR-Bank (Banarescu et al., 2013). The
top-down algorithms are slow in this experiment,
confirming Groschwitz et al.’s findings. Again,
the sibling-finder algorithm outperforms all other
algorithms. Note that Groschwitz et al.’s parser
(“GKT 15” in Fig. 7) shares much code with our
system. It uses the same decomposition automata,
but a less mature version of the sibling-finder
method which fully computes the invhom automa-
ton. Our new system achieves a 9x speedup for
parsing the whole corpus, compared to GKT 15.

7 Related Work

Describing parsing algorithms at a high level of
abstraction has a long tradition in computational
linguistics, e.g. in deductive parsing with parsing
schemata (Shieber et al., 1995). A key challenge
under this view is to index chart entries so they
can be retrieved efficiently, which parallels the
situation in automata intersection discussed here.
Gómez-Rodrı́guez et al. (2009) present an algo-
rithm that automatically establishes index struc-
tures that guarantee optimal asymptotic runtime,
but also requires algebra-specific extensions for
grammar formalisms that go beyond context-free
string grammars.

Efficient parsing has also been studied in other
generalized grammar formalisms beyond IRTG.
Kanazawa (to appear) shows how the parsing
problem of Abstract Categorial Grammars (de

Groote, 2001) can be translated into Datalog,
which enables the use of generic indexing strate-
gies for Datalog to achieve optimal asymptotic
complexity. Ranta (2004) discusses parsing for his
Grammatical Framework formalism in terms of
partial evaluation techniques from functional pro-
gramming, which are related to the step-by-step
evaluation of sibling-finders in Figure 4. Like the
approach of Gomez-Rodriguez et al., these meth-
ods have not been evaluated for large-scale gram-
mars and realistic evaluation data, which makes it
hard to judge their relative practical merits.

Most work in the tree automata community has
a theoretical slant, and there is less research on the
efficient implementation of algorithms for tree au-
tomata than one would expect; Cleophas (2009)
and Lengal et al. (2012) are notable exceptions.
Even these tend to be motivated by applications
such as specification and verification, where the
tree automata are much smaller and much less am-
biguous than in computational linguistics. This
makes these systems hard to apply directly.

8 Conclusion

We have presented novel algorithms for comput-
ing the intersection and the inverse homomorphic
image of finite tree automata. These can be used
to implement a generic algorithm for IRTG pars-
ing, and apply directly to any grammar formalism
that can be represented as an IRTG. An evaluation
on practical data from three different grammar for-
malisms shows consistent speed improvements of
several orders of magnitude, and our graph parser
has the fastest published runtimes.

A Java implementation of our algorithms is
available as part of the Alto parser, http://
bitbucket.org/tclup/alto.

We focused here purely on symbolic parsing,
and on computing complete parse charts. In the
presence of a probability model (e.g. for IRTG en-
codings of PCFGs), our algorithms could be made
faster through the use of appropriate pruning tech-
niques. It would also be interesting to combine
the strengths of the condensed and sibling-finder
algorithms for further efficiency gains.

Acknowledgments. We thank the anonymous
reviewers for their comments. We are grateful
to Johannes Gontrum for an early implementation
of Alg. 4, and to Christoph Teichmann for many
fruitful discussions. This work was supported by
the DFG grant KO 2916/2-1.

2050



References
Laura Banarescu, Claire Bonial, Shu Cai, Madalina

Georgescu, Kira Griffitt, Ulf Hermjakob, Kevin
Knight, Philipp Koehn, Martha Palmer, and Nathan
Schneider. 2013. Abstract meaning representation
for sembanking. In Proceedings of the Linguistic
Annotation Workshop (LAW VII-ID).

John Chen and K. Vijay-Shanker. 2000. Automated
extraction of TAGs from the Penn Treebank. In Pro-
ceedings of IWPT.

David Chiang, Jacob Andreas, Daniel Bauer,
Karl Moritz Hermann, Bevan Jones, and Kevin
Knight. 2013. Parsing graphs with hyperedge
replacement grammars. In Proceedings of the 51st
Annual Meeting of the Association for Computa-
tional Linguistics (ACL).

David Chiang. 2007. Hierarchical phrase-based trans-
lation. Computational Linguistics, 33(2):201–228.

Stephen Clark and James Curran. 2007. Wide-
coverage efficient statistical parsing with CCG
and log-linear models. Computational Linguistics,
33(4):493–552.

Loek Cleophas. 2009. Forest FIRE and FIRE Wood:
Tools for tree automata and tree algorithms. In Pro-
ceedings of the Conference on Finite-State Methods
and Natural Language Processing (FSMNLP).

Hubert Comon, Max Dauchet, Rémi Gilleron, Flo-
rent Jacquemard, Denis Lugiez, Christof Löding,
Sophie Tison, and Marc Tommasi. 2008. Tree
automata techniques and applications. http://
tata.gforge.inria.fr/.

Philippe de Groote. 2001. Towards abstract catego-
rial grammars. In Proceedings of the 39th ACL/10th
EACL.

Michel Galley, Mark Hopkins, Kevin Knight, and
Daniel Marcu. 2004. What’s in a translation rule?
In Proceedings of HLT/NAACL.

Carlos Gómez-Rodrı́guez, Jesús Vilares, and Miguel A.
Alonso. 2009. A compiler for parsing schemata.
Software: Practice and Experience, 39(5):441–470.

Jonathan Graehl, Kevin Knight, and Jonathan May.
2008. Training tree transducers. Computational
Linguistics, 34(3).

Jonas Groschwitz, Alexander Koller, and Christoph Te-
ichmann. 2015. Graph parsing with s-graph gram-
mars. In Proceedings of the 53rd ACL and 7th IJC-
NLP.

Aravind K. Joshi and Yves Schabes. 1997. Tree-
Adjoining Grammars. In G. Rozenberg and A. Salo-
maa, editors, Handbook of Formal Languages, vol-
ume 3, pages 69–123. Springer-Verlag, Berlin.

Laura Kallmeyer and Wolfgang Maier. 2013. Data-
driven parsing using probabilistic linear context-
free rewriting systems. Computational Linguistics,
39(1):87–119.

Makoto Kanazawa. to appear. Parsing and genera-
tion as datalog query evaluation. IfCoLog Journal
of Logics and Their Applications.

Alexander Koller and Marco Kuhlmann. 2011. A gen-
eralized view on parsing and translation. In Pro-
ceedings of the 12th International Conference on
Parsing Technologies (IWPT).

Alexander Koller and Marco Kuhlmann. 2012. De-
composing TAG algorithms using simple alge-
braizations. In Proceedings of the 11th TAG+ Work-
shop.

Ondrej Lengal, Jiri Simacek, and Tomas Vojnar. 2012.
Vata: A library for efficient manipulation of non-
deterministic tree automata. In C. Flanagan and
B. König, editors, Tools and Algorithms for the Con-
struction and Analysis of Systems: 18th Interna-
tional Conference, TACAS 2012. Springer.

Mike Lewis and Mark Steedman. 2014. A* CCG pars-
ing with a supertag-factored model. In Proceedings
of EMNLP.

Aarne Ranta. 2004. Grammatical framework: A type-
theoretical grammar formalism. Journal of Func-
tional Programming, 14(2):145–189.

Nina Seemann, Fabienne Braune, and Andreas Maletti.
2015. String-to-tree multi bottom-up tree transduc-
ers. In Proceedings of the 53rd ACL and 7th IJC-
NLP.

Stuart M Shieber, Yves Schabes, and Fernando CN
Pereira. 1995. Principles and implementation of de-
ductive parsing. The Journal of logic programming,
24(1):3–36.

Mark Steedman. 2001. The Syntactic Process. MIT
Press, Cambridge, MA.

Yuk Wah Wong and Raymond J. Mooney. 2006.
Learning for semantic parsing with statistical ma-
chine translation. In Proceedings of the Human Lan-
guage Technology Conference of the North Ameri-
can Chapter of the Association for Computational
Linguistics (HLT/NAACL-2006).

2051


