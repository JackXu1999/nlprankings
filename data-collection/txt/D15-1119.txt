



















































Improving fastalign by Reordering


Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 1034–1039,
Lisbon, Portugal, 17-21 September 2015. c©2015 Association for Computational Linguistics.

Improving fast align by Reordering

Chenchen Ding, Masao Utiyama, Eiichiro Sumita
Multilingual Translation Laboratory

National Institute of Information and Communications Technology
3-5 Hikaridai, Seikacho, Sorakugun, Kyoto, 619-0289, Japan

{chenchen.ding, mutiyama, eiichiro.sumita}@nict.go.jp

Abstract

fast align is a simple, fast, and effi-
cient approach for word alignment based
on the IBM model 2. fast align per-
forms well for language pairs with rel-
atively similar word orders; however, it
does not perform well for language pairs
with drastically different word orders. We
propose a segmenting-reversing reorder-
ing process to solve this problem by al-
ternately applying fast align and re-
ordering source sentences during train-
ing. Experimental results with Japanese-
English translation demonstrate that the
proposed approach improves the per-
formance of fast align significantly
without the loss of efficiency. Experiments
using other languages are also reported.

1 Introduction

Aligning words in a parallel corpus is a basic task
for almost all state-of-the-art statistical machine
translation (SMT) systems. Word alignment is
used to extract translation rules in various way,
such as the phrase pairs used in a phrase-based
(PB) SMT system (Koehn et al., 2003), the hier-
archical rules used in a HIERO system (Chiang,
2007), and the sophisticated translation templates
used in tree-based SMT systems (Liu et al., 2006).

Among different approaches, GIZA++1 (Och
and Ney, 2003), which is based on the IBM
translation models, is the most widely used word
alignment tool. Other well-known tools are
the BerkeleyAligner2, Nile3 (Riesa et al.,
2011), and pialign4 (Neubig et al., 2011).

1http://www.statmt.org/moses/giza/
GIZA++.html

2https://code.google.com/p/
berkeleyaligner/

3http://jasonriesa.github.io/nile/
4http://www.phontron.com/pialign/

fast align5 (Dyer et al., 2013) is a recently
proposed word alignment approach based on the
reparameterization of the IBM model 2, which
is usually referred to as a zero-order alignment
model (Och and Ney, 2003). Taking advantage of
the simplicity of the IBM model 2, fast align
introduces a “tension” parameter to model the
overall accordance of word orders and an effi-
cient parameter re-estimation algorithm is devised.
It has been reported that the fast align ap-
proach is more than 10 times faster than baseline
GIZA++, with comparable results in end-to-end
French-, Chinese-, and Arabic-to-English transla-
tion experiments.

However, the simplicity of the IBM model 2
also leads to a limitation. As demonstrated in
this study, fast align does not perform well
when applied to language pairs with drastically
different word orders, e.g., Japanese and English.
The problem is because of the IBM model 2’s in-
trinsic inability to handle complex distortions. In
this study, we propose a simple and efficient re-
ordering approach to improve the fast align’s
performance in such situations, referred to as
segmenting-reversing (seg rev). Our motivation
is to apply a rough but robust reordering to make
the source and target sentences have more simi-
lar word orders, where fast align can show
its power. Specifically, seg rev first segments
a source-target sentence pair into a sequence of
minimal monotone chunk pairs6 based on the au-
tomatically generated word alignment. Within the
chunk pairs, source word sequences are exam-
ined to determine whether they should be com-
pletely reversed or the original order should be
retained. The objective of this step is to con-
vert the source sentence to a roughly target-like
word order. The seg rev process is applied re-
cursively but not deeply (only twice in our ex-

5https://github.com/clab/fast_align
6same as the “tuple” used in Mariño et al. (2006)

1034



th
e

cu
tt
in
g

u
n
it

8 is ac
ti
va
te
d

b
y

th
is

si
gn
al

.

この

信号

により

切断

部

８

が

作動

する  reverse 

。

si
gn
al

th
is

b
y

ac
ti
va
te
d

is 8 u
n
it

cu
tt
in
g

th
e

.

 

rev.

 reverse 

th
is

si
gn
al

b
y

cu
tt
in
g

u
n
it

8 is ac
ti
va
te
d

th
e

.

この

信号

により

切断

部

８

が

作動

する

。

Figure 1: Example of seg rev applied to a word-aligned English-Japanese sentence pair. Based on the
word alignment, the source sentence is reordered in a target-like order after applying seg rev twice.

periments) for each source sentence in the train-
ing data. Consequently, the seg rev process is
lightweight and shallow. Local word sequences,
except those at chunk boundaries, are not scram-
bled, while global word orders are re-arranged if
there are large chunks.

Our primary experimental results for Japanese-
English translation show that applying seg rev
significantly improves fast align’s perfor-
mance to a level comparable to GIZA++. The
training time becomes 2–4 times that of a baseline
fast align, which is still at least 2 – 4 times
faster than the training time required by base-
line GIZA++. Results for German-, French-, and
Chinese-English translations are also reported.

2 Segmenting-Reversing Reordering

The seg rev is inspired by the “REV preorder”
(Katz-Brown and Collins, 2008), which is a sim-
ple pre-reordering approach originally designed
for the Japanese-to-English translation task. More
efficient pre-reordering approaches usually require
trained parsers and sophisticated machine learning
frameworks (de Gispert et al., 2015; Hoshino et
al., 2015). We adopt the REV method in Katz-
Brown and Collins (2008) considering it is the
simplest and lightest pre-reordering approach (to
our knowledge), which may bring a minimal ef-
fect on the efficiency of fast align.

An example seg rev process, where the word
alignment is generated by fast align, is illus-
trated in Fig. 1. The example we selected has rel-
atively correct word alignment and seg rev per-
forms well. In general cases, the alignment has
significant noise and the reordering is rougher .

Algorithm 1 describes the repeated (δ times)
application of the seg rev process, and Algo-
rithm 2 describes a single application. Specifi-

cally, Algorithm 1 applies Algorithm 2 δ times.
For each application of Algorithm 2, source sen-
tence S and source indices in alignment A are
reordered, and the overall permutation RI is up-
dated and recorded. In Fig. 1, the original En-
glish sentence had 10 words (including the pe-
riod), being indexed as [0, 1, 2, 3, 4, 5, 6, 7, 8, 9].
After the first application of seg rev, RI was
[8,7,6,5,4,3,2,1,0 , 9], and after the second appli-
cation, RI was [7,8 , 6, 1,2,3,4,5 , 0, 9] (reversed
parts are boxed).

In Algorithm 2, the main for loop (line 3) scans
the source sentence from the beginning to the end
to obtain monotone segmentation. The foreach
(line 5) and if (line 11) are general phrase pair ex-
traction process. The if (line 13) guarantees that
the chunk is monotone on the target side. The
rev function (line 16), which is described in Al-
gorithm 3, determines whether the sub-sequence
from sstart to send should be reversed by exam-
ining the related alignment Asub. For example,
in the first application shown in Fig.1, two sub-
sequences [0 : 8] and [9 : 9] are processed by rev
and [0 : 8] is reversed. Four sub-sequences [0 : 1],
[2 : 2], [3 : 7], and [9 : 9] are processed in the sec-
ond application and [0 :1] and [3 :7] are reversed.7

Finally, source sentence S and source indices in
alignment A are reordered (lines 19 – 20).8

Algorithm 3 performs the reversal. We count
the concordant and discordant pairs9 and reverse

7The sub-sequences are based on the input, not the origi-
nal sentence, e.g., sub-sequence[0 :1]contains the 8th and 7th
word of the original source sentence in the 2nd application.

8Unaligned words between chunks on the source side are
problematic. They are not touched by line 18. Although they
can be attached to preceding or succeeding chunks, we do
not use further heuristics to handle them. An example is the
drifting “the” in the English sentence in Fig. 1, which our
approach cannot handle properly.

9As used in Kendall’s τ or Goodman and Kruskal’s γ.

1035



Algorithm 1: seg revδ
input : same as Algorithm 2 and a depth δ;
output : same as Algorithm 2 except RA;

1 RI ← [0, · · · , I]; RS ← S; RA ← A;
2 for i← 1 to δ do
3 RS , RA, RI′ ← seg rev (RS , T,RA);
4 permute RI with RI′ ;

5 return RS , RI ;

Algorithm 2: seg rev
input : source sentence S = s0, · · · , sI ;

target sentence T = t0 · · · , tJ ;
word alignment A = {(i, j)|
si, tj are aligned, i ∈ [0, I], j ∈ [0, J ]};

output : reordered source sentence RS ;
source index reordered alignmentRA;
reordered indices RI = π(0, · · · , I),

which is a permutation from 0 to I;
1 RI ← [0, · · · , I];
2 sstart ← 0; tpre end ← −1;
3 for send ← sstart to I do
4 tstart ← J + 1; tend ← −1;
5 foreach (i, j) ∈ A do
6 if i ∈ [sstart, send] then
7 if j < tstart then
8 tstart ← j;
9 if j > tend then

10 tend ← j;

11 if ∃(i, j) ∈ A :
j ∈ [tstart, tend] ∧ i /∈ [sstart, send] then

12 continue;
13 if ∃(i, j) ∈ A : j ∈ (tpre end, tstart) then
14 continue;
15 Asub ← {(i, j)|(i, j) ∈ A,

i ∈ [sstart, send] ∧ j ∈ [tstart, tend]};
16 rev (RI [sstart : send], Asub);
17 tpre end ← tend;
18 sstart ← min({i|i>send ∧ ∃(i, j) ∈ A});
19 RS ← permute S according to RI ;
20 RA ← permute i in A according to RI ;
21 return RS , RA, RI ;

the sub-sequence if and only if there are more dis-
cordant pairs than the concordant pairs. In Fig.1,
the sub-sequence [0 : 8] in the first application has
C28 = 28 pairs of aligned word pair (i.e., 28 gray
block pairs for eight gray blocks); however, only
11 pairs are concordant (C25 =10 pairs in [1 :5] and
one pair in [7 :8]), Consequently, the sub-sequence
[0 : 8] is reversed because there are more discor-
dant pairs (17 = 28−11). The two reversed sub-
sequences in the second application are obvious.

Algorithm 4 describes the training frame-
work, where fast align and seg rev are ap-
plied alternately. To generate word alignment,
fast align is run bi-directionally and sym-
metrization heuristics are applied to reduce noise
(line 11). In each iteration, the source sen-
tences for seg rev are the original sentences,

Algorithm 3: rev
input : index sequence Isub; word alignment Asub;

1 con← 0; dis← 0;
2 foreach unordered tuple ((i0, j0), (i1, j1)) :

(i0, j0) ∈ Asub ∧ (i1, j1) ∈ Asub do
3 x← (i1 − i0)× (j1 − j0);
4 if x > 0 then
5 con← con+ 1;
6 else if x < 0 then
7 dis← dis+ 1;
8 else
9 continue;

10 if dis > con then
11 Isub ← reverse Isub;

Algorithm 4: fast align with seg rev
input : parallel corpus C with N sentence pairs

C = {(S1, T 1) · · · , (SN , TN )};
maximum iteration M ; depth δ for seg rev;

output : word alignment A = {A1, · · · , AN};
1 A ← ∅;
2 for iter ← 1 to M do
3 I ← ∅; C′ ← ∅;
4 if A 6= ∅ then
5 for n← 1 to N do
6 RS ,RI←seg revδ(Sn, Tn, An);
7 append (RS , Tn) to C′;
8 append RI to I;
9 else

10 C′ ← C;
11 A ← sym ◦ fast align (C′);
12 if I 6= ∅ then
13 for n← 1 to N do
14 recover An by I [n];

15 return A;

and fast align uses the reordered sentences
with the exception of the first iteration. The word
alignment generated is thus based on the reordered
source sentences; consequently, the recorded per-
mutation (line 14) is used to recover word align-
ment before the next iteration. The permutation is
a one-to-one mapping; therefore, recovering is re-
alized by the inverse mapping of the permutation,
which transfers the source-side word alignment in-
dices to match the original source sentences.

The time complexity of Algorithm 3 is O(l2),
where l is the size of Asub that is related to the
chunk size. If the average chunk size is a constant
C depending on languages pairs or data sets, then
the time complexity of Algorithm 2 is O(C · I2)
assuming J and the size of A are both linear
against I . The average chunk size will be reduced
when seg rev is applied successively; there-
fore, the time required for subsequent seg rev
processes will decrease. In practice, compared
with the training time required by fast align,

1036



seg rev processing time is negligible. Note that
seg rev processes are accelerated easily by par-
allel processing.

3 Experiments and Discussion

We applied the proposed approach to Japanese-
English translation, a language pair with dramat-
ically different word orders. In addition, we ap-
plied the approach to German-English translation,
a language pair with relatively different word or-
ders among European languages.

For Japanese-English translation, we used
NTCIR-7 PAT-MT data (Fujii et al., 2008). For
German-English translation, we used the Eu-
roparl v7 corpus10 (Koehn, 2005) for training, the
WMT 0811 / WMT 0912 test sets for development
/ testing, respectively. Default settings for the PB
SMT in MOSES13 (Koehn et al., 2007) were used,
except for Japanese-English translations where the
distortion-limit was set to 12 to reach a recently
reported baseline (Isozaki et al., 2012). MERT
(Och, 2003) was used to tune development set pa-
rameter weights and BLEU (Papineni et al., 2002)
was used on test sets to evaluate the translation
performance. Bootstrap sampling (Koehn, 2004)
was employed to test statistical significance using
bleu kit14.

We compared GIZA++ and fast align with
default settings. GIZA++ was used as a mod-
ule of MOSES. The bi-directional outputs of
fast align were symmetrized by atools in
cdec15 (Dyer et al., 2010), and further training
steps were conducted using MOSES. grow-diag-
final-and symmetrization was used consistently in
the experiments. For the the proposed approach,
we set δ=2 and M =4 in Algorithm 4. Note that
δ can be set to a larger value and seg rev could
be applied repeatedly until no additional reorder-
ing is possible. As mentioned, the word alignment
is noisy and our intention is a robust and rough
process; therefore, we restricted seg rev to two
applications and did not consider the difference
in sentence lengths or different languages during
training. Within each iteration, fast align was
run with default settings, except initial diagonal-

10http://www.statmt.org/europarl/
11http://www.statmt.org/wmt08/
12http://www.statmt.org/wmt09/
13http://www.statmt.org/moses/
14http://www.nlp.mibel.cs.tsukuba.ac.

jp/bleu_kit/
15http://www.cdec-decoder.org/

ja-en en-ja de-en en-de
GIZA++ 28.8 30.8 18.2 12.9

FAλini=4.0 28.1‡ 29.5‡ 18.0† 12.7†

FAλini=0.1 28.0‡ 29.8‡ 17.5‡ 12.5‡

iteration 2 28.3† 30.9 17.9‡ 12.8
iteration 3 28.4† 30.1‡ 18.1 12.7†

iteration 4 28.8 30.7 18.1 12.7†

Table 1: Test set BLEU scores for Japanese-
English and German-English translations. (‡, sta-
tistical significance at p<0.01; †, at p<0.05; bold-
face, no significance; all compared with GIZA++)

tension (λini) was set to 0.1 in the first iteration,
to avoid overly strong monotone preference at the
beginning of training.

Experimental results for Japanese-English and
German-English translations in both directions
are listed in Table 1. The first two rows show
the baseline performance. fast align (using
a default λini = 4.0) performance was statisti-
cally significantly lower than GIZA++, particu-
larly for Japanese-English translation. The fol-
lowing four rows show the results of the pro-
posed approach. For the first iteration, λini was
set to 0.1, and the performance did not change
significantly. The translations from English im-
proved (equal to GIZA++) at the second itera-
tion. However, translations to English improved
more slowly. We attribute the difference in im-
provement rates between translation to and from
English to the relatively fixed word order of En-
glish, whereby the reordering process is easier
and more consistent. Note that once transla-
tions from English improved in the second iter-
ation, performance decreased in the following it-
erations. The results in Table 1 were obtained us-
ing predictable-seed for tuning, which generated
determinate results. Another attempt using ran-
dom seeds to tune returned test set BLEU scores
of 30.5, 30.4 on en-ja and 12.8, 12.8 on en-
de, for iterations 3 and 4, respectively. These
four scores had no statistical significance against
GIZA++. The instability is largely due to the
alignment of function words, which affects trans-
lation performance (Riesa et al., 2011). The align-
ment does not change significantly after the sec-
ond iteration; however, it is unstable around func-
tion words,16 because seg rev does not process

16Specifically, to, articles, and prepositions in English-
Japanese; of, have, and relative pronouns in English-German.

1037



fr-en zh-en
GIZA++ 23.3 31.4

FAλini=4.0 23.1 31.7
iteration 2 23.1 31.7

Table 2: Test set BLEU scores for French-to-
English and Chinese-to-English translations. For
fr-en, the data sets were the same as for de-en.
For zh-en, NIST 2006 OpenMT data were used
for training and test; test data from 2002 to 2005
OpenMT were used for tuning.

unaligned function words between chunks. Our
approach is too rough to handle function words
precisely. We plan to address this in future.

We also tested our approach on French- and
Chinese-to-English translations. The results are
listed in Table 2. GIZA++ and fast align
showed no statistically significant difference in
performance, which is consistent with Dyer et al.
(2013). The proposed approach did not affect
performance for French- and Chinese-to-English
translations. These results are expected as these
language pairs have similar word orders.

With regard to processing time, a naı̈ve, single-
thread implementation of seg rev in C++ took
approximately 60s / 40s in the first / second ap-
plication on the entire Japanese-English corpus17.
The recover process took less than 30s in each it-
eration. In contrast, fast align, although very
fast, took approximately one hour for one round
of training (using five iterations for its log-linear
model) on the same corpus. Therefore, the addi-
tional time required in our approach is quite small
and can be ignored compared with the training
time of fast align.18

4 Conclusion and Future Work

We have proposed a simple and efficient approach
to improve the performance of fast align on
language pairs with drastically different word or-
ders. With the proposed approach, fast align
obtained results comparable with GIZA++, and its
efficiency is retained. We are investigating further
properties of seg rev and plan to extend it to
achieve greater stability and efficiency.19

171.8 M sentence pairs with an average length of 35 words.
18GIZA++ actually took around 18 hours to align the

Japanese-English corpus (parallel processes for two direc-
tions, including mkcls, five iterations for the model 1 and
the HMM model, three iteration for the model 3 and 4).

19Ongoing experiments using seg rev with GIZA++ re-
turned negative results. BLEU decreases by approx. 1 point.

Acknowledgments

We thank Dr. Atsushi Fujita for his helpful discus-
sions on this work.

References
David Chiang. 2007. Hierarchical phrase-based trans-

lation. Computational Linguistics, 33(2):201–228.

Adrià de Gispert, Gonzalo Iglesias, and Bill Byrne.
2015. Fast and accurate preordering for SMT us-
ing neural networks. In Proc. of NAACL-HLT, pages
1012–1017.

Chris Dyer, Adam Lopez, Juri Ganitkevitch, Johnathan
Weese, Ferhan Ture, Phil Blunsom, Hendra Seti-
awan, Vladimir Eidelman, and Philip Resnik. 2010.
cdec: A decoder, alignment, and learning framework
for finite-state and context-free translation models.
In Proc. of ACL (System Demonstrations), pages 7–
12.

Chris Dyer, Victor Chahuneau, and Noah A Smith.
2013. A simple, fast, and effective reparameteri-
zation of IBM model 2. In Proc. of NAACL-HLT,
pages 644–648.

Atsushi Fujii, Masao Utiyama, Mikio Yamamoto,
Takehito Utsuro, Terumasa Ehara, Hiroshi Echizen-
ya, and Sayori Shimohata. 2008. Overview of the
patent translation task at the NTCIR-7 workshop. In
Proc. of NTCIR, pages 389–400.

Sho Hoshino, Yusuke Miyao, Katsuhito Sudoh, Kat-
suhiko Hayashi, and Masaaki Nagata. 2015. Dis-
criminative preordering meets Kendall’s Tau max-
imization. In Proc. of ACL (Short Papers), pages
139–144.

Hideki Isozaki, Katsuhito Sudoh, Hajime Tsukada, and
Kevin Duh. 2012. HPSG-based preprocessing
for English-to-Japanese translation. ACM Transac-
tions on Asian Language Information Processing,
11(3):8.

Jason Katz-Brown and Michael Collins. 2008. Syn-
tactic reordering in preprocessing for Japanese →
English translation: MIT system description for
NTCIR-7 patent translation task. In Proc. of NTCIR,
pages 409–414.

Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In Proc.
of HLT-NAACL, pages 48–54.

Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ondrej Bojar, Alexan-
dra Constantin, and Evan Herbst. 2007. Moses:
Open source toolkit for statistical machine transla-
tion. In Proc. of ACL (Demo and Poster Sessions),
pages 177–180.

1038



Philipp Koehn. 2004. Statistical significance tests for
machine translation evaluation. In Proc. of EMNLP,
pages 388–395.

Philipp Koehn. 2005. Europarl: A parallel corpus for
statistical machine translation. In Proc. of MT sum-
mit, pages 79–86.

Yang Liu, Qun Liu, and Shouxun Lin. 2006. Tree-
to-string alignment template for statistical machine
translation. In Proc. of ACL, pages 609–616.

José B. Mariño, Rafael E. Banchs, Josep M. Crego,
Adrià de Gispert, Patrik Lambert, José A. R. Fonol-
losa, and Marta R. Costa-Jussà. 2006. N-gram-
based machine translation. Computational Linguis-
tics, 32(4):527–549.

Graham Neubig, Taro Watanabe, Eiichiro Sumita,
Shinsuke Mori, and Tatsuya Kawahara. 2011. An
unsupervised model for joint phrase alignment and
extraction. In Proc. of ACL-HLT, pages 632–641.

Franz Josef Och and Hermann Ney. 2003. A sys-
tematic comparison of various statistical alignment
models. Computational Linguistics, 29(1):19–51.

Franz Josef Och. 2003. Minimum error rate training
in statistical machine translation. In Proc. of ACL,
pages 160–167.

Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: a method for automatic
evaluation of machine translation. In Proc. of ACL,
pages 311–318.

Jason Riesa, Ann Irvine, and Daniel Marcu. 2011.
Feature-rich language-independent syntax-based
alignment for statistical machine translation. In
Proc. of EMNLP, pages 497–507.

1039


