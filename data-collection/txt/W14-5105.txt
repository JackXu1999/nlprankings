



















































Proceedings of the...


D S Sharma, R Sangal and J D Pawar. Proc. of the 11th Intl. Conference on Natural Language Processing, pages 30–38,
Goa, India. December 2014. c©2014 NLP Association of India (NLPAI)

Supertag Based Pre-ordering in Machine Translation
Rajen Chatterjee, Anoop Kunchukuttan, Pushpak Bhattacharyya

Department of Computer Science and Engineering
Indian Institute of Technology, Bombay
{rajen,anoopk,pb}@cse.iitb.ac.in

Abstract
This paper presents a novel approach
to integrate mildly context sensitive
grammar in the context of pre-ordering
for machine translation. We discuss
the linguistic insights available in this
grammar formalism and use it to de-
velop a pre-ordering system. We show
that mildly context sensitive gram-
mar proves to be beneficial over con-
text free grammar, which facilitates
better reordering rules. For English
to Hindi, we see significant improve-
ment of 1.8 BLEU and error reduc-
tion of 4.46 TER score over CFG based
pre-ordering system, on the WMT14
data set. We also show that our ap-
proach improves translation quality for
English to Indian languages machine
translation over standard phrase based
systems.

1 Introduction
India is a multilingual country with 22 offi-
cial languages, spanning four language fam-
ilies (Indo-Aryan,Dravidian, Tibeto-Burman
and Austro-Asiatic)1. In such a liguistically
diverse country there is always a great need for
translation services to serve government, busi-
ness and overall social communication needs.
Hindi and English act as link languages across
the country and languages of official commu-
nication for the Union Government. Thus, the
importance of English to Hindi translation is
obvious.

Languages can be differentiated in terms of
structural divergences and morphological us-
age. English is structurally classified as a
Subject-Verb-Object (SVO) language as op-
posed to Hindi which follows Subject-Object-
Verb (SOV) word order. Largely, these di-
vergences are responsible for the difficulties

1http://en.wikipedia.org/wiki/Languages_of_
India

in standard statistical machine translation
(SMT) systems.

Our objective is to reduce the structural di-
vergence by reordering words in the source
language (English) to conform to the target
language (Hindi) word order and then provide
this data to train a pharse based SMT system.
This approach is known as pre-ordering in
SMT research. The novelty of our work is the
inclusion of linguistic context obtained from
higher level of grammar formalism known as
mildly context sensitive grammar. To the best
of our knowledge, this is the first approach to
bring such a formalism for pre-ordering in ma-
chine translation.

To begin with, Section 2 discusses work re-
lated to pre-ordering. We then provide an in-
troduction to TAG and Supertag, in Section
3. Section 4 describes our approach for pre-
ordering. In Section 5, we provide the method-
ology for pre-ordering. Experimental setup is
explained in Section 6, while corresponding re-
sults are shown in Section 7. We conclude our
work in Section 8 providing acknowledgement
in Section 9.

2 Related Work

Word order has direct impact on the flu-
ency of transaltion obtained using SMT sys-
tems. There are basically two paradigms
for generating correct word order. The first
paradigm deals with developing a reordering
model which is used during decoding. Dif-
ferent solutions such as syntax-based models
(Chiang, 2005), lexicalized reordering (Och et
al., 2004), and tree-to-string methods (Zhang
et al., 2006) have been proposed. Most of these
approaches use statistical models to learn re-
ordering rules, but all of them have differ-
ent methods to solve the reordering problem.
The next paradigm deals with developing a re-
ordering model which is used as pre-processing
step (also known as pre-ordering) in SMT sys-
tems. In pre-ordering, the objective is to re-30



order source language words to conform to the
target language word order.

Xia and McCord (2004) describe an ap-
proach for translation from French to En-
glish, where reordering rules are acquired au-
tomatically using source and target parses
and word alignment. The reordering rules in
their approach operate at the level of context-
free rules in the parse tree. Collins et al.
(2005), describe clause restructuring for Ger-
man to English machine translation. They use
six transformations that are applied on Ger-
man parsed text to reorder it before train-
ing with a phrase based system. Popovic
and Ney (2006) use hand-made rules to re-
order the source side based on POS infor-
mation. Zhang et al. (2007) propose chunk
level reordering, where reordering rules are au-
tomatically learned from source-side chunks
and word alignments. They allow all possi-
ble reorderings to be used to create a lattice
that is input to the decoder. Genzel (2010),
shows automatic rule extraction for 8 language
pairs. They first extract a dependency tree
and then converts it to a shallow constituent
tree. The trees are annotated by both POS
tags and by Stanford dependency types, then
they learn reordering rules given a set of fea-
tures. This paper discusses about creating
manual reordering rules with the help of Tree
Adjoining Grammar (TAG), a mildly context
sensitive formalism as discussed in Section 3.

3 Introduction to TAG/Supertag

Tree Adjoining Grammar (TAG) was intro-
duced by Joshi et al. (1975). Tree Adjoin-
ing Languages (TALs) generate some strictly
context-sensitive languages and fall in the class
of the so-called mildly context-sensitive lan-
guages. Lexicalized Tree Adjoining Grammar
(LTAG) (Joshi and Schabes, 1991) is the TAG
formalism where each lexical item is associated
with atleast one elementary structure. This
elementary structure of LTAG is known as
Supertag. The concept of Supertag was first
proposed by Joshi and Srinivas (1994). Su-
pertag localize dependencies, including long-
distance dependencies, by requiring that all
and only the dependent elements be present
within the same structure. They provide syn-
tactic as well as dependency information at the

word level by imposing complex constraints in
a local context. Supertags provide informa-
tion like POS tag, subcategorization and other
syntactic constraints at the level of agreement
feature. Supertag can also be viewed as frag-
ments of parse tree associated with each lex-
ical item. An example of supertag is shown
in Figure 1. This supertag is used for tran-
sitive verbs. Subcategorization information is

S

NP0↓ VP

V♢

ate

NP1↓

Figure 1: Supertag for transitive verb ate

clearly visible in the verb, which takes a sub-
ject to its left and an object to its right.

3.1 Structural description of supertag
Each supertag, at its frontier, has exactly one
anchor node marked by ♢, to which a lexical
item gets anchored. Apart from anchor node
at its frontier, it may have an optional substi-
tution node marked by ↓ or an adjunction node
marked by ∗. Substitution and adjunction are
those nodes which can be replaced by another
supertag. For an adjunction node, it is nec-
essary for its label to match the label of root
node of supertag. A supertag can have atmost
one adjunction node but can have more than
one substitution node.

3.2 Supertagging
Supertagging refers to tagging each word of a
sentence with a supertag. An example of a
supertagged sentence is shown in figure 2.

NP

G♢

I

S

NP0↓ VP

V♢

ate

NP1↓

NP

A♢

dark

NP∗
NP

N♢

chocolate

Figure 2: Supertagged sentence “I ate dark
chocolate”

We use MICA Parser (Bangalore et al.,31



2009)2 to obtain rich linguistic information
like POS tag, supertag, voice, the presence
of empty subjects (PRO), wh-movement, deep
syntactic argument (deep subject, deep direct
object, deepindirect object), whether a verb
heads a relative clause and also dependency
relation, for each word. From this rich set
of features, for each word, we extract word
ID (essentially word position in a sentence),
supertag, dependency relation and deep syn-
tactic argument. Given a dependency rela-
tion, these supertags can be assembled using
composition operation of TAG to form a con-
stituent tree. Composition operation includes
substitution and adjunction:

• Substitution: It deals with substituting
a non-terminal node at the frontier of su-
pertag with another supertag. An exam-
ple of substitution operation is shown in
Figure 3. When substitution occurs at a
node, the node is replaced by the supertag
to be substituted.

• Adjunction: It deals with inserting a su-
pertag within another supertag satisfying
adjunction constraints (Joshi, 1987). An
example of adjoining operation is shown
in Figure 4. One of the key contraint is
the root label of supertag to be adjoined
should match label of the node where ad-
junction occurs.

NP

G♢

I

S

+ NP0↓ VP

V♢

ate

NP1↓

S

= NP

G♢

I

VP

V♢

ate

NP1↓

Figure 3: Example illustraing Substitution op-
eration

4 Supertag Based Reordering
Each supertag encapsulates rich linguistic con-
text, based on which, we reorder supertag
nodes to preserve same context in target lan-
guage. We first highlight linguistic relevance
of supertag followed by showing why supertag
facilitates reordering and finally illustrate re-
ordering with supertag.

2http://mica.lif.univ-mrs.fr/

NP

A♢

dark

NP∗ +

NP

N♢

chocolate

NP

= A♢

dark

NP

N♢

chocolate

Figure 4: Example illustraing Adjunction op-
eration

4.1 Linguistic Relevance
In this section, we show the linguistic relevance
of supertag. Each Supertag falls in a specific
syntactic environment, a few examples of su-
pertag along with their syntactic environment
are shown in Table 1. For each supertag ID,
Table 1 gives the linguistic context (syntac-
tic environment) in which a supertag appears
along with an example sentence. The lexical
item attached to the anchor node of supertag
is written in italic, in the example sentence.
Table 2 shows supertag ID and their corre-
sponding tree structure.

Supertag
ID

Linguistic
Context

Example

t3 Noun Phrase the man
t87 Topicalization Those dogs, I

am terrified of
t68 Imperative Please tidy

your room.
t36 Adjectival Modi-

fier
red hat

t27 Transitive Verb I brought dark
chocolates.

Table 1: Example showing syntactic environ-
ment of different supertag

Syntactic environment of a supertag gives
us the liberty to control reordering at a finer
granularity. Thus, supertags of tree adjoining
grammar prove to be advantageous over con-
text free grammar (CFG) for reordering.

4.2 Why Supertag facilitates
Reordering

In this section, we discuss key properties of
LTAG which facilitates reordering, such as lex-
icalization, followed by Extended Domain of
Locality (EDL) and finally Factoring of Recur-
sion from the Domain of dependency (FRD)32



Supertag ID Supertag Structure
t3 NP

N♢
t87 NP

NP∗ S

NP1

-NONE-

S

NP0↓ VP

V♢ NP

-NONE-
t68 S

NP0

-NONE-

VP

V♢ NP1↓

t36 NP

A♢ NP∗

t27 S

NP0↓ VP

V♢ NP1↓

Table 2: Figure showing supertag structure for
each supertag ID

• Lexicalization:
Lexicalization ensures that each supertag
is anchored by a lexical item. This prop-
erty prove to be linguistically crucial since
it establishes a direct connection between
the lexicon and the syntactic structures
defined in the grammar. Reordering syn-
tactic structure at word level for every
word in a sentence helps us to obtain word
order that conforms to target language
word order.

• Extended Domain of Locality
(EDL):
This property of LTAG states that for
each lexical item, the grammar must
contain an elementary structure for each
syntactic environment, the lexical item
might appear in. This means that for
each lexical item there can be multiple
supertags representing different syntactic
environment. Another part of EDL
states that every supertag must contain

all and only the argument of anchor
in the same structure. This allows the
anchor to impose syntactic and semantic
constraints on its arguments directly
since they appear in the same elementary
structure that it anchors. This ensures,
to reorder we deal only with primary
features of lexical item on which the
reordering primarily depends on.

• Factoring of Recursion from the Do-
main of dependency (FRD): This
property ensures that supertags are min-
imal in nature; i.e. there is no recur-
sion present within the given elementary
structure of supertag. Recursive con-
structs are carried out with the help of
adjunction operation. Auxiliary supertag,
by adjunction to other supertag, accounts
for long distance behavior of these depen-
dencies. Due to this property the fea-
ture space of supertag is simplified with-
out any recursive feature involved. This
helps to build simple but strong reorder-
ing rules by just considering few but most
important features of the supertag.

4.3 Detecting Linguistic Patterns for
Reordering

In this section, we show that linguistic pat-
terns can be detected with the help of su-
pertag. English is structurally classified as a
Subject-Verb-Object (SVO) language whereas
Hindi is Subject-Object-Verb (SOV) language.
However, this is not true for all verbs, specially
in case of raising verb. We compare control
verb v/s raising verb, where the former re-
quires reordering as opposed to the later. An
example has been shown in Table 3. It can
be seen that supertag provides finer granular-
ity to control reordering based on linguistic
features. Studying supertag in-depth and as-
sociating it with linguistic context will help
to discover various pattern useful for building
better reordering system.

5 Methodology for Reordering
Listed below are the sequence of modules of
our reordering system:

• Reorder Supertags
We begin with reordering supertags, for33



Control Verb Raising Verb
Sentence Supertag Sentence Supertag
E: I told him. S

NP0↓ VP

V♢ NP1↓

It seems you are busy. S

NP0↓ VP

V♢ S1↓

H: मैंने उसे बोला | लगता है िक तुम व्यस्त हो |
T: mainne use bola lagta hai ki tum vyast ho
G: I him told seems is that you busy are

Table 3: Table showing comparision of control verb v/s raising verb
E:English; H:Hindi; T:Transliteration; G:Gloss

which we are provided with 4725 su-
pertags 3 in all, but seldom all of them
are seen in action. Therefore, we supertag
50,000 sentences from health and tourism
domain of ILCI corpus (Jha, 2010), from
which we filter out spurious supertags
which occur less than 10 times, leaving
600 supertags with us, for analysis. Our
manual analysis shows that out of 600 su-
pertags, 200 required reordering transfor-
mations to be applied.

Example of supertag, representing transi-
tive verb, before and after reordering in
shown in Table 4

Before Reordering After Reordering
S

NP0↓ VP

V♢ NP1↓

S

NP0↓ VP

NP1↓ V♢

Table 4: Example showing transitive verb be-
fore and after reordering

An example of reordering prepositional
phrase which gets adjunct to a noun
phrase is shown in Table 5.

Before Reordering After Reordering
NP

NP∗ PP

IN♢ NP1↓

NP

PP

NP1↓ IN♢

NP∗

Table 5: Example of prepositional phrase be-
fore and after reordering

3http://mica.lif.univ-mrs.fr/

• Extract Linguistic Information
As stated in Section 3.2, we use MICA
Parser to obtain linguistic informa-
tion along with superatg at word
level. For example, consider the input
sentence “I told him”, for which we
extract essential information and ap-
pend it with respective words as shown
“I|1|2|PRP|t29|0 told|2|2|VBD|t27|.
him|3|2|PRP|t29|1”. Here, each
word contains “lexical-item|ID|parent-
ID|POStag|Supertag|deep-argument-
position”. We carry this example
sentence in further modules of reorder-
ing.

• Construct Derivation Tree
Once linguistic information is extracted,
we proceed with construction of deriva-
tion tree. The process of combining the
elementary trees (supertags) to yield a
parse of the sentence is represented by the
derivation tree. Derivation tree for the ex-
ample sentence is shown in Figure 5.

told|2|2|VBD|t27|.

I|1|2|PRP|t29|0 him|3|2|PRP|t29|1

Figure 5: Example of Derivation Tree

• Construct Derived Tree
We have implemented the composition
operation (adjunction and substitution),
which, given a derivation tree would build
a derived/parse tree. Derived tree for the
example sentence is shown in Figure 6. To
build this derived tree we use reordered
version of supertag (for eg. t27 for tran-
sitive verb from Table 4).34



S

NP

G♢

I

VP

NP

G♢

him

V♢

told

Figure 6: Example of Derived Tree

• Extract leaf nodes Finally, we extract
leaf nodes from the derived tree, which
gives us reordered English sentence. In
our case, the input sentence is “I told
him” for which, we get “I him told ”
as output. We now use these reordered
English sentences along with their paral-
lel Hindi translation to train English to
Hindi machine translation system.

6 Experiment Setup

In this section, we discuss different data set
used and various experiments that we have
performed.

6.1 Data Sets
We use the shared task data set provided in
Workshop on Statistical Machine Translation,
2014 (WMT14)4 to train English to Hindi
translation system. WMT14 training set con-
tains data from general domain, obtained from
multiple sources, whereas test set belongs to
news domain. The training set consist of
275K parallel segments, while test set contains
2.5K. We train a 5 gram language model us-
ing SRILM5 on 1.5M monolingual Hindi cor-
pus provided in WMT14 shared task.

For translation from English to multiple In-
dian languages we use Indian Language Cor-
pora Initiative (ILCI) (Jha, 2010) corpus.
ILCI data belongs to the health and tourism
domain. The training set consist of 40K paral-
lel sentences while test set contains 1.1K. We
train a 5 gram language model using SRILM
on a 50K monolingual Hindi corpus from the
same domain.

4WMT14 resources:-http://ufallab.ms.mff.
cuni.cz/~bojar/hindencorp/

5http://www.speech.sri.com/projects/srilm

The English data was tokenized using the
Stanford tokenizer and then true-cased using
truecase.perl provided in MOSES toolkit. We
normalize Hindi corpus using NLP Indic Li-
brary (Kunchukuttan et. al.,2014)6. Normal-
ization is followed by tokenization, wherein we
make use of the trivtokenizer.pl provided with
WMT14 shared task.

6.2 List of Experiments

We use the MOSES toolkit (Koehn et al.,
2007) to train various statistical machine
translation systems. For English to Hindi we
train three systems, on each data set (WMT14
and ILCI), as follows:

• Phrase Based Systems: We train
phrase based model without pre-ordering.

• Context Free Grammar based pre-
orderding: In this model, we reorder
both train and test set using Patel et
al. (2013)’s source side reordering rules
which is refinement of Ramanathan et al.
(2008)’s rule-based reordering system.

• Supertag based pre-ordering: In this
model, we reorder both train and test
set using our supertag based pre-ordering
method as discussed in Section 5.

We provide systematic comparision among
these systems in Section 7. As the reordering
rules are developed to conform Hindi word or-
der, we were interested to see how does it affect
other Indian languages which have the same
word order as Hindi. So, we developed various
translation systems from English to other In-
dian languages using ILCI corpus and compare
it with standard pharse based systems.

7 Results

In this section, we also provide quantitative
results for various systems and provide sys-
tematic comparision among them. We also
provide couple of translations from English
to Hindi, showing improvement in translation
quality with our approach.

6https://bitbucket.org/anoopk/indic_nlp_
library35



Data Set Phrase Based CFG based reordering Supertag based reorderingBLEU TER BLEU TER BLEU TER
WMT14 8.00 84.00 8.6 86.66 10.40 82.20
ILCI 23.77 58.36 26.45 56.24 25.75 56.14

Table 6: Result of English to Hindi SMT System
Higher BLEU Score and Lower TER Score indicate better system

Language Pair Baseline Phrase Based Supertag based reordering Improvement overBaseline (BLEU %)BLEU TER BLEU TER
English-Punjabi 20.71 62.94 22.60 60.64 +9.12
English-Urdu 16.20 69.12 17.43 67.38 +7.59
English-Konkani 10.18 79.35 11.18 78.83 +9.82
English-Telugu 4.8 95.41 5.44 95.46 +13.33
English-Gujarati 14.76 70.42 15.40 69.34 +4.33
English-Bengali 12.73 75.49 13.26 74.48 +4.16
English-Malayalam 3.49 100.23 3.78 100.39 +8.31

Table 7: Result of English to Indian Language SMT System

7.1 Quantitative Evaluation

We use two standard evaluation metrics BLEU
(Papineni et al., 2002) and TER (Snover et
al., 2006), for comparing translation quality
of various systems. Table 6 compares phrase
based, CFG based reordering and supertag
based reordering systems built using WMT14
and ILCI data set, for English to Hindi lan-
guage pair. We see TER score of our system
is best for both data sets. For WMT14 data
set, our approach shows improvement of 1.8
BLEU score and error reduction of 4.46 TER
score over CFG based reordering system.

For the ILCI data set, our approach shows
significant improvement over both evaluation
metrics over phrase based SMT sytem but
shows slight degradation in BLEU score when
compared with CFG based reordering system.
However, TER score remains almost same. On
an average, over both data sets, we see that
TAG based reordering performs better than
PB and CFG based systems.

Also from Table 7, we see our approch of re-
ordering English data set proves to give signif-
icant improvement in translation quality over
phrase based, for most of the English to Indian
language machine translation systems. The re-
sult is statistically significant at the p <= 0.07
level.

7.2 Qualitative Evaluation

We provide two source (English) sentences
(Example 1 and Example 2) along with their
actual translation (AT) and machine transla-
tions obtained from phrase based system (PB),
CFG based reordered system (CFG) and TAG
based reordered system (TAG), as shown in
Table 8. Transliteration of all Hindi sen-
tences have been shown in corresponding col-
umn. We see that translations of TAG is much
closer to the actual translation. Example 1
shows improvement in reordering which is the
main focus of our work, whereas, Example 2
shows morphological improvement inherently
achieved with reordering.

8 Conclusion

We presented a novel method of using mildly
context sensitive grammar formalism in the
context of pre-ordering in SMT systems. We
show that the rich linguistic information em-
bedded in supertags provides finer granular-
ity for framing reordering rules over Context
Free Grammar (CFG). We also showed that
supertag can be used for detecting linguis-
tic pattern based on which specific reorder-
ing rules can be written. We also discuss
the linguistic relevance of Supertag and bridg-
ing it with machine translation. Finally our
approach has shown significant improvement
over CFG based reordered systems. In future36



Example 1 One may wear clothes in many folds on body .
AT शरीर पर कई तह में कपड़े पहनें । sharir par kai taha mein kapade pa-

hane .
PB सकती है पर कई तह में कपड़े पहनें । sakti hai par kai taha mein kapade

pahane .
CFG एक कई तह में कपड़े शरीर पर पहन सकते हैं । ek kai taha mein kapade sharir par

pahan sakte hain .
STAG एक शरीर पर कई तह में कपड़े पहनें । ek sharir par kai taha mein kapde

pahane .
Example 2 The symptoms of which are as follows :

AT जसके लक्षण इस पर्कार हैं : jisake lakshan is prakar hain :
PB जो के लक्षण इस पर्कार हैं : jo ke lakshan is prakar hain :

CFG जो के लक्षण इस पर्कार हैं : jo ke lakshan is prakar hain :
STAG जसके लक्षण इस पर्कार हैं : jisake lakshan is prakar hain :

Table 8: Comparision of translation among various systems for English to Hindi

work, we would like to classify supertag based
on their linguistic relevance and try to gener-
alize reordering rules for each class.

9 Acknowledgement

We thank Owen Rambow and Srinivas Ban-
galore to help us in understanding supertags
and also making the grammar file of supertags
available to us. We would like to thank
the Technology Development for Indian Lan-
guages (TDIL) Programme and the Depart-
ment of Electronics Information Technology,
Govt. of India for providing the ILCI cor-
pus. Also we would like to thank all members
of the Centre for Indian Language Technology
(CFILT) for their valuable feedback and com-
ments.

References
Srinivas Bangalore, Pierre Boulllier, Alexis Nasr,

Owen Rambow, and Benoît Sagot. 2009. Mica:
a probabilistic dependency parser based on tree
insertion grammars application note. In Pro-
ceedings of HLT-NAACL, pages 185–188.

David Chiang. 2005. A hierarchical phrase-based
model for statistical machine translation. In
Proceedings of the 43rd Annual Meeting on ACL,
pages 263–270.

Michael Collins, Philipp Koehn, and Ivona
Kučerová. 2005. Clause restructuring for sta-
tistical machine translation. In Proceedings of
the ACL, pages 531–540.

Dmitriy Genzel. 2010. Automatically learning
source-side reordering rules for large scale ma-
chine translation. In Proceedings of the COL-
ING, pages 376–384.

Girish Nath Jha. 2010. The tdil program and the
indian language corpora initiative (ilci). In Pro-
ceedings of the LREC.

Aravind K Joshi and Yves Schabes. 1991. Tree-
adjoining grammars and lexicalized grammars.

Aravind K Joshi and Bangalore Srinivas. 1994.
Disambiguation of super parts of speech (or su-
pertags): Almost parsing. In Proceedings of the
COLING, pages 154–160.

Aravind K Joshi, Leon S Levy, and Masako Taka-
hashi. 1975. Tree adjunct grammars. Journal
of computer and system sciences, 10(1):136–163.

Aravind K Joshi. 1987. An introduction to tree
adjoining grammars. Mathematics of language,
1:87–115.

Philipp Koehn, Hieu Hoang, Alexandra Birch,
Chris Callison-Burch, Marcello Federico, Nicola
Bertoldi, Brooke Cowan, Wade Shen, Christine
Moran, Richard Zens, et al. 2007. Moses: Open
source toolkit for statistical machine translation.
In Proceedings of the ACL, pages 177–180.

Franz Josef Och, Daniel Gildea, Sanjeev Khu-
danpur, Anoop Sarkar, Kenji Yamada, Alexan-
der Fraser, Shankar Kumar, Libin Shen, David
Smith, Katherine Eng, et al. 2004. A smorgas-
bord of features for statistical machine transla-
tion. In Proceedings of the HLT-NAACL, pages
161–168.

Kishore Papineni, Salim Roukos, Todd Ward, and
Wei-Jing Zhu. 2002. Bleu: a method for au-
tomatic evaluation of machine translation. In
Proceedings of the ACL, pages 311–318.37



Raj Patel, Rohit Gupta, Prakash Pimpale, and
M. Sasikumar. 2013. Reordering rules for
English-Hindi SMT. In Proceedings of the Sec-
ond Workshop on Hybrid Approaches to Trans-
lation.

Maja Popovic and Hermann Ney. 2006. Pos-based
word reorderings for statistical machine transla-
tion. In Proceedings of the LREC, pages 1278–
1283.

Ananthakrishnan Ramanathan, Jayprasad Hegde,
Ritesh M Shah, Pushpak Bhattacharyya, and
M Sasikumar. 2008. Simple syntactic and mor-
phological processing can help english-hindi sta-
tistical machine translation. In Proceedings of
the IJCNLP, pages 513–520.

Matthew Snover, Bonnie Dorr, Richard Schwartz,
Linnea Micciulla, and John Makhoul. 2006. A
study of translation edit rate with targeted hu-
man annotation. In Proceedings of the AMTA,
pages 223–231.

Fei Xia and Michael McCord. 2004. Improving a
statistical mt system with automatically learned
rewrite patterns. In Proceedings of the COL-
ING, page 508.

Hao Zhang, Liang Huang, Daniel Gildea, and
Kevin Knight. 2006. Synchronous binarization
for machine translation. In Proceedings of the
HLT-NAACL, pages 256–263.

Yuqi Zhang, Richard Zens, and Hermann Ney.
2007. Chunk-level reordering of source language
sentences with automatically learned rules for
statistical machine translation. In Proceedings
of the HLT-NAACL, pages 1–8.

38


