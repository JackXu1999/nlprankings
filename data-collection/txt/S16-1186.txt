



















































CMU at SemEval-2016 Task 8: Graph-based AMR Parsing with Infinite Ramp Loss


Proceedings of SemEval-2016, pages 1202–1206,
San Diego, California, June 16-17, 2016. c©2016 Association for Computational Linguistics

CMU at SemEval-2016 Task 8:
Graph-based AMR Parsing with Infinite Ramp Loss

Jeffrey Flanigan♠ Chris Dyer♠ Noah A. Smith♥ Jaime Carbonell♠
♠School of Computer Science, Carnegie Mellon University, Pittsburgh, PA, USA
♥Computer Science & Engineering, University of Washington, Seattle, WA, USA

{jflanigan,cdyer,jgc}@cs.cmu.edu, nasmith@cs.washington.edu

Abstract

We present improvements to the JAMR parser
as part of the SemEval 2016 Shared Task 8 on
AMR parsing. The major contributions are:
improved concept coverage using external re-
sources and features, an improved aligner, and
a novel loss function for structured prediction
called infinite ramp, which is a generalization
of the structured SVM to problems with un-
reachable training instances.

1 Introduction

Our entry to the SemEval 2016 Shared Task 8 is a set
of improvements to the system presented in Flanigan
et al. (2014). The improvements are: a novel train-
ing loss function for structured prediction, which we
call “infinite ramp,” new sources for concepts, im-
proved features, and improvements to the rule-based
aligner in Flanigan et al. (2014). The overall archi-
tecture of the system and the decoding algorithms
for concept identification and relation identification
are unchanged from Flanigan et al. (2014), and we
refer readers seeking a complete understanding of
the system to that paper.

2 New Concept Fragment Sources and
Features

The concept identification stage relies on a function
called clex in Section 3 of Flanigan et al. (2014)
to provide candidate concept fragments. In that
work, clex has three sources of concept fragments:
a lexicon extracted from the training data, rules for
named entities identified by the named entity tagger,

and rules for time expressions. We augment these
sources with five additional sources:

• Frame file lookup: for every word in the input
sentence, if the lemma matches the name of a
frame in the AMR frame files (with sense tag
removed), we add the lemma concatenated with
“-01” as a candidate concept fragment.

• Lemma: for every word in the input sentence,
we add the lemma of the word as a candidate
concept fragment.

• Verb pass-through: for every word in the in-
put sentence, if the word is a verb, we add the
lemma concatenated with “-00” as a candidate
concept fragment.

• Named entity pass-through: for every span of
words of length 1 until 7 in the input, we add
the concept fragment “(thing :name (name :op1
word1 . . . :opn wordn)” as a candidate concept
fragment, where n is the length of the span,
and “word1” and “wordn” are the first and last
words in the fragment.

We use the following features for concept identi-
fication:

• Fragment given words: Relative frequency
estimates of the probability of a concept frag-
ment given the sequence of words in the span.

• Length of the matching span (number of to-
kens).

• Bias: 1 for any concept graph fragment.

1202



• First match: 1 if this is the first place in the
sentence that matches the span.

• Number: 1 if the span is length 1 and matches
the regular expression “[0-9]+”.

• Short concept: 1 if the length of the concept
fragment string is less than 3 and contains only
upper or lowercase letters.

• Sentence match: 1 if the span matches the en-
tire input sentence.

• ; list: 1 if the span consists of the single word
“;” and the input sentence is a “;” separated list.

• POS: the sequence of POS tags in the span.

• POS and event: same as above but with an in-
dicator if the concept fragment is an event con-
cept (matches the regex “.*-[0-9][0-9]”).

• Span: the sequence of words in the span if the
words have occurred more than 10 times in the
training data as a phrase with no gaps.

• Span and concept: same as above concate-
nated with the concept fragment in PENMAN
notation.

• Span and concept with POS: same as above
concatenated with the sequence of POS tags in
the span.

• Concept fragment source: indicator for the
source of the concept fragment (corpus, NER
tagger, date expression, frame files, lemma,
verb-pass through, or NE pass-through).

• No match from corpus: 1 if there is no match-
ing concept fragment for this span in the rules
extracted from the corpus.

The new sources of concepts complicate concept
identification training. The new sources improve
concept coverage on held-out data but they do not
improve coverage on the training data since one of
the concept sources is a lexicon extracted from the
training data. Thus correctly balancing use of the
training data lexicon versus the additional sources to
prevent overfitting is a challenge.

To balance the training data lexicon with the other
sources, we use a variant of cross-validation. Dur-
ing training, when processing a training example in
the training data, we exclude concept fragments ex-
tracted from the same section of the training data.
This is accomplished by keeping track of the training
instances each phrase-concept fragment pair was ex-
tracted from, and excluding all phrase-concept frag-
ment pairs within a window of the current training
instance. In our submission the window is set to 20.

While excluding phrase-concept fragment pairs
allows the learning algorithm to balance the use of
the training data lexicon versus the other concept
sources, it creates another problem: some of the
gold standard training instances may be unreachable
(cannot be produced), because of the phrase-concept
pair need to produce the example has been excluded.
This can cause problems during learning. To han-
dle this, we use a generalization of structured SVMs
which we call “infinite ramp.” We discuss this in
the general framework of structured prediction in the
next section.

3 Infinite Ramp Loss

The infinite ramp is a new loss function for struc-
tured prediction problems. It is useful when the
training data contains outputs that the decoder can-
not produce given their inputs (we refer to these as
“unreachable examples”). It is a direct generaliza-
tion of the SVM loss and latent SVM loss.

Let x be the input, Y(x) be the space of all possi-
ble outputs given the input x, and ŷ be the predicted
output. Let f(x, y′) denote the feature vector for the
output y′ with the input x, which is the sum of the lo-
cal features. (In concept identification, the local fea-
tures are the features computed for each span, and
f is the sum of the features for each span.) Let w
be the parameters of a linear model, used to make
predictions as follows:

ŷ = argmax
y′∈Y(x)

w · f(x, y′)

To train the model parameters w, a function of the
training data is minimized with respect to w. This
function is a sum of individual training examples’
losses L, plus a regularizer:

L(D;w) =
∑

(xi,yi)∈D
L(xi, yi;w) + λ‖w‖2

1203



L(xi, yi;w) = −


 lim
α→∞

max
y∈Y(xi)


w · f(xi, y) + α ·




C(xi,yi)︷ ︸︸ ︷
min

y′′∈Y(xi)
cost(yi, y

′′)−cost(yi, y)










+ max
y′∈Y(xi)

(
w · f(xi, y′) + cost(yi, y′)

)
(1)

Figure 1: Infinite ramp loss.

Typical loss functions are the structured percep-
tron loss (Collins, 2002):

L(xi, yi;w) = −w · f(xi, yi) + max
y∈Y(xi)

w · f(xi, y)
(2)

and the structured SVM loss (Taskar et al., 2003;
Tsochantaridis et al., 2004), which incorporates
margin using a cost function:1

L(xi, yi;w) = −w · f(xi, yi)
+ max

y∈Y(xi)

(
w · f(xi, y) + cost(yi, y)

)
(3)

Both (2) and (3) are problematic if example i is
unreachable, i.e., yi /∈ Y(xi), due to imperfect data
or an imperfect definition of Y . In this case, the
model is trying to learn an output it cannot produce.
In some applications, the features f(xi, yi) cannot
even be computed for these examples. This prob-
lem is well known in machine translation: some ex-
amples cannot be produced by the phrase-table or
grammar. It also occurs in AMR parsing.

To handle unreachable training examples, we
modify (3), introducing the infinite ramp loss,
shown in Eq. 1 in Fig. 1. The term labeled C(xi, yi)
is present only to make the limit well-defined in case
miny∈Y(xi) cost(yi, y) 6= 0. In practice, we set α to
be a very large number (1012) instead of taking a
proper limit, and set C(xi, yi) = 0.

The intuition behind Eq. 1 is the following: for
very large α, the first max picks a y that minimizes
cost(yi, y), using the model score w · f(xi, y) to
break any ties. This is what the model updates to-
wards in subgradient descent-style updates, called
the “hope derivation” by Chiang (2012). The second
max is the usual cost augmented decoding that gives

1cost(yi, y) returns the cost of mistaking y for correct out-
put yi.

a margin in the SVM loss, and is what the model up-
dates away from in subgradient descent, called the
“fear derivation” by Chiang (2012).

Eq. 1 generalizes the structured SVM loss. If yi
is reachable and the minimum over y ∈ Y(xi) of
cost(y, yi) occurs when y = yi, then the first max
in Eq. 1 picks out y = yi and Eq. 1 reduces to the
structured SVM loss.

The infinite ramp is also a generalization of the
latent structured SVM (Yu and Joachims, 2009),
which is a generalization of the structured SVM for
hidden variables. This loss can be used when the
output can be written yi = (ỹi, hi), where ỹi is ob-
served output and hi is latent (even at training time).
Let Ỹ(xi) be the space of all possible observed out-
puts and H(xi) be the hidden space for the example
xi. Let c̃ be the cost function for the observed out-
put. The latent structured SVM loss is:

L(xi, yi;w) = − max
h∈H(xi)

(
w · f(xi, ỹi, h)

)

+ max
ỹ∈Ỹ(xi)

max
h′∈H(xi)

(
w · f(xi, ỹ, h) + c̃(ỹi, ỹ)

)
(4)

If we set cost(yi, y) = c̃(ỹi, ỹ) in Eq. 1, and the
minimum of c̃(ỹi, ỹ) occurs when ỹ = ỹi, then min-
imizing Eq. 1 is equivalent to minimizing Eq. 4.

Eq. 1 is related to ramp loss (Collobert et al.,
2006; Chapelle et al., 2009; Keshet and McAllester,
2011):

L(xi, yi;w) =

− max
y∈Y(xi)

(
w · f(xi, y)− α · cost(yi, y)

)

+ max
y′∈Y(xi)

(
w · f(xi, y′) + cost(yi, y′)

)
(5)

The parameter α is often set to zero, and controls
the “height” of the ramp, which is α + 1. Taking

1204



α→∞ in Eq. 5 corresponds roughly to Eq. 1, hence
the name “infinite ramp loss”. However, Eq. 1 also
includes C(xi, yi) term to make the limit well de-
fined even when miny∈Y(xi) cost(yi, y) 6= 0. Like
infinite ramp loss, ramp loss also handles unreach-
able training examples (Gimpel and Smith, 2012),
but we have found ramp loss to be more diffi-
cult to optimize than infinite ramp loss in practice
due to local minima. Both loss functions are non-
convex. However, infinite ramp loss is convex if
argminy∈Y(xi) cost(yi, y) is unique.

4 Training

We train the concept identification stage using in-
finite ramp loss (1) with AdaGrad (Duchi et al.,
2011). We process examples in the training data
((x1, y1), . . . , (xN , yN )) one at a time. At time t,
we decode with the current parameters and the cost
function as an additional local factor to get the two
outputs:

ht = argmax
y′∈Y(xt)

(
wt · f(xt, y′)−α · cost(yi, y)

)
(6)

f t = argmax
y′∈Y(xt)

(
wt · f(xt, y′) + cost(yi, y)

)
(7)

and compute the subgradient:

st = f(xt, h
t)− f(xt, f t)− 2λwt

We then update the parameters and go to the next
example. Each component i of the parameters gets
updated as:

wt+1i = w
t
i −

η√∑t
t′=1 s

t′
i

sti

5 Experiments

We evaluate using Smatch (Cai and Knight, 2013).
Following the recommended train/dev./test split
of LDC2015E86, our parser achieves 70% pre-
cision, 65% recall, and 67% F1 Smatch on the
LDC2015E86 test set. The JAMR baseline on this
same dataset is 55% F1 Smatch, so the improve-
ments are quite substantial. On the SemEval 2016
Task 8 test set, our improved parser achieves 56%
F1 Smatch.

We hypothesize that the lower performance of the
parser on the SemEval Task 8 test set is due to drift in

the AMR annotation scheme between the production
of the LDC2015E86 training data and the SemEval
test set. During that time, there were changes to the
concept senses and the concept frame files. Because
the improvements in our parser were due to boosting
recall in concept identification (and using the frame
files to our advantage), our approach does not show
as large improvements on the SemEval test set as on
the LDC2015E86 test set.

6 Conclusion

We have presented improvements to the JAMR
parser as part of the SemEval 2016 Shared Task
on AMR parsing, showing substantial improvements
over the baseline JAMR parser. As part of these im-
provements, we introduced infinite ramp loss, which
generalizes the structured SVM to handle training
data with unreachable training examples. We hope
this loss function will be useful in other application
areas as well.

Acknowledgments

This work is supported by the U.S. Army Re-
search Office under grant number W911NF-10-1-
0533. Any opinion, findings, and conclusions or rec-
ommendations expressed in this material are those
of the author(s) and do not necessarily reflect the
view of the U.S. Army Research Office or the United
States Government.

References

Shu Cai and Kevin Knight. 2013. Smatch: an evalua-
tion metric for semantic feature structures. In Proc. of
ACL.

Olivier Chapelle, Chuong B. Do, Choon H Teo, Quoc V.
Le, and Alex J. Smola. 2009. Tighter bounds for
structured estimation. In NIPS.

David Chiang. 2012. Hope and fear for discrimina-
tive training of statistical translation models. JMLR,
13(1):1159–1187.

Michael Collins. 2002. Discriminative training methods
for hidden Markov models: Theory and experiments
with perceptron algorithms. In Proc. of EMNLP.

Ronan Collobert, Fabian Sinz, Jason Weston, and Léon
Bottou. 2006. Trading convexity for scalability. In
Proc. of ICML.

1205



John Duchi, Elad Hazan, and Yoram Singer. 2011.
Adaptive subgradient methods for online learning and
stochastic optimization. JMLR, 12:2121–2159.

Jeffrey Flanigan, Sam Thomson, Jaime Carbonell, Chris
Dyer, and Noah A. Smith. 2014. A discriminative
graph-based parser for the abstract meaning represen-
tation. In Proc. of ACL.

Kevin Gimpel and Noah A. Smith. 2012. Structured
ramp loss minimization for machine translation. In
Proc. of NAACL.

Joseph Keshet and David A McAllester. 2011. Gener-
alization bounds and consistency for latent structural
probit and ramp loss. In Advances in Neural Informa-
tion Processing Systems.

Ben Taskar, Carlos Guestrin, and Daphne Koller. 2003.
Max-margin markov networks. In NIPS.

Ioannis Tsochantaridis, Thomas Hofmann, Thorsten
Joachims, and Yasemin Altun. 2004. Support vec-
tor machine learning for interdependent and structured
output spaces. In Proc. of ICML.

Chun-Nam John Yu and Thorsten Joachims. 2009.
Learning structural SVMs with latent variables. In
Proc. of ICML.

1206


