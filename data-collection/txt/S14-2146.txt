



















































UWM: Applying an Existing Trainable Semantic Parser to Parse Robotic Spatial Commands


Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 823–827,
Dublin, Ireland, August 23-24, 2014.

UWM: Applying an Existing Trainable Semantic Parser to Parse Robotic
Spatial Commands

Rohit J. Kate
University of Wisconsin-Milwaukee

Milwaukee, WI
katerj@uwm.edu

Abstract

This paper describes Team UWM’s sys-
tem for the Task 6 of SemEval 2014
for doing supervised semantic parsing of
robotic spatial commands. An existing
semantic parser, KRISP, was trained us-
ing the provided training data of natural
language robotic spatial commands paired
with their meaning representations in the
formal robot command language. The en-
tire process required very little manual ef-
fort. Without using the additional annota-
tions of word-aligned semantic trees, the
trained parser was able to exactly parse
new commands into their meaning repre-
sentations with 51.18% best F-measure at
72.67% precision and 39.49% recall. Re-
sults show that the parser was particularly
accurate for short sentences.

1 Introduction

Semantic parsing is the task of converting natu-
ral language utterances into their complete formal
meaning representations which are executable for
some application. Example applications of seman-
tic parsing include giving natural language com-
mands to robots and querying databases in natu-
ral language. Some old semantic parsers were de-
veloped manually to work for specific applications
(Woods, 1977; Warren and Pereira, 1982). How-
ever, such semantic parsers were generally brittle
and building them required a lot of manual effort.
In addition, these parsers could not be ported to
any other application without again putting signif-
icant manual effort.

More recently, several semantic parsers have
been developed using machine learning (Zelle and

This work is licensed under a Creative Commons At-
tribution 4.0 International Licence. Page numbers and pro-
ceedings footer are added by the organisers. Licence details:
http://creativecommons.org/licenses/by/4.0/

Mooney, 1996; Ge and Mooney, 2005; Zettle-
moyer and Collins, 2005; Wong and Mooney,
2006; Kate and Mooney, 2006; Lu et al., 2008;
Kwiatkowski et al., 2011). In this approach, train-
ing data is first created for the domain of inter-
est. Then using one of the many machine learn-
ing methods and semantic parsing frameworks, a
semantic parser is automatically learned from the
training data (Mooney, 2007). The trained seman-
tic parser is then capable of parsing new natu-
ral language utterances into their meaning repre-
sentations. Semantic parsers built using machine
learning tend to be more robust and can be easily
ported to other application domains with appropri-
ate domain-specific training data.

The Task 6 of SemEval 2014 provided a new ap-
plication domain for semantic parsing along with
training and test data. The domain involved giv-
ing natural language commands to a robotic arm
which would then move blocks on a board (Dukes,
2013). The domain was inspired from the classic
AI system SHRDLU (Winograd, 1972). The train-
ing data contained 2500 examples of sentences
paired with their meaning representations in the
Robot Command Language (RCL) which was de-
signed for this domain (Dukes, 2013). The test
data contained 909 such example pairs.

We trained an existing and freely available1 se-
mantic parser KRISP (Kate and Mooney, 2006)
using the training data for this domain. Besides
changing the format of the data for running KRISP
and writing a context-free grammar for the mean-
ing representation language RCL, the entire pro-
cess required minimal manual effort. The author
spent less than a week’s time for participating in
the Task 6, and most of it was spent in running
the experiments. This demonstrates that train-
able semantic parsers like KRISP can be rapidly
adopted to new domains. In the Results section
we show different precisions and recalls it ob-

1http://www.cs.utexas.edu/users/ml/krisp/

823



tained at different confidence levels in the form of
a precision-recall curve. The results also show that
the parser was particularly accurate on shorter sen-
tences. Two major reasons that prevented KRISP
from performing better on this domain were - its
high computational demand for memory which
prevented it from being trained beyond 1500 train-
ing examples, and some variability in the mean-
ing representation language RCL that negatively
affected training as well as evaluation.

2 Background: KRISP Semantic Parser

KRISP (Kernel-based Robust Interpretation for Se-
mantic Parsing) is a trainable semantic parser
(Kate and Mooney, 2006) that uses Support Vector
Machines (SVMs) (Cristianini and Shawe-Taylor,
2000) as the machine learning method with string-
subsequence kernel (Lodhi et al., 2002). It takes
natural language utterances and their correspond-
ing formal meaning representation as the training
data along with the context-free grammar of the
meaning representation language (MRL). The key
idea in KRISP is that every production of the MRL
is treated as a semantic concept. For every MRL
production, an SVM classifier is trained so that it
can give for any input natural language substring
of words the probability that it expresses the corre-
sponding semantic concept. Once these classifiers
are trained, parsing a sentence reduces to finding
the most probable semantic derivation of the sen-
tence in which different productions cover differ-
ent parts of the sentence and together form a com-
plete meaning representation. Figure 1 shows an
example semantic derivation of a robotic spatial
command. Productions of RCL grammar (Table 1)
are shown at tree nodes depicting different parts of
the sentence they cover.

Since the training data is not in the form of such
semantic derivations, an EM-like iterative algo-
rithm is used to collect appropriate positive and
negative examples in order to train the classifiers
(Kate and Mooney, 2006). Positive examples are
collected from correct semantic derivations de-
rived by the parser learned in the previous itera-
tion, and negative examples are collected from the
incorrect semantic derivations.

KRISP was shown to work well on the US geog-
raphy database query domain (Tang and Mooney,
2001) as well as on the RoboCup Coach Lan-
guage (CLang) domain (Kate et al., 2005). It was
also shown to be particularly robust to noise in

Figure 1: Semantic derivation of the robotic spatial com-
mand “pick up the turquoise pyramid” obtained by KRISP
during testing which gives the correct RCL representation
(event: (action: take) (entity: (color: cyan) (type: prism))).

the natural language utterances (Kate and Mooney,
2006). KRISP was later extended to do semi-
supervised semantic parsing (Kate and Mooney,
2007b), to learn from ambiguous supervision in
which multiple sentences could be paired with a
single meaning representation in the training data
(Kate and Mooney, 2007a), and to transform the
MRL grammar to improve semantic parsing (Kate,
2008).

3 Methods
In order to apply KRISP to the Task 6 of SemEval
2014, the format of the provided data was first
changed to the XML-type format that KRISP ac-
cepts. The data contained several instances of
co-references which was also part of RCL, but
KRISP was not designed to handle co-references
and expects them to be pre-resolved. We ob-
served that almost all co-references in the mean-
ing representations, indicated by “reference-id”
token, resolved to the first occurrence of an “en-
tity” element in the meaning representation. This
was found to be true for more than 99% of the
cases. We used this observation to resolve co-
references during semantic parsing in the follow-
ing way. As a pre-processing step, we first remove
from the meaning representations all the “id:” to-
kens (these resolve the references) but keep the
“reference-id:” tokens (these encode presence of
co-references). The natural language sentences
are not modified in any way and the parser learns
from the training data to relate words like “it”
and “one” to the RCL token “reference-id”. After
KRISP generates a meaning representation during
testing, as a post-processing step, “id: 1” is added
to the first “entity” element in the meaning repre-
sentation if it contains the “reference-id:” token.

The context-free grammar for RCL was not pro-
vided by the Task organizers. There are multi-

824



ple ways to write a context-free grammar for a
meaning representation language and those that
conform better to natural language work better
for semantic parsing (Kate, 2008). We manu-
ally wrote grammar for RCL which mostly fol-
lowed the structure of the meaning representa-
tions as they already conformed highly to natural
language commands and hence writing the gram-
mar was straightforward. KRISP runs faster if
there are fewer non-terminals on the right-hand-
side (RHS) of the grammar because that makes
the search for the most probable semantic deriva-
tion faster. Hence we kept non-terminals on RHS
as few as possible while writing the grammar.
Table 1 shows the entire grammar for RCL that
we wrote which was given to KRISP. The non-
terminals are indicated with a “*” in their front.
We point out that KRISP needs grammar only for
the meaning representation language (an applica-
tion will need it anyway if the statements are to be
executed) and not for the natural language.

KRISP’s training algorithm could be aided by
providing it with information about which natu-
ral language words are usually used to express the
concept of a production. For example, word “red”
usually expresses “*color: → ( color: red )”. The
data provided with the Task 6 came with the word-
aligned semantic trees which indicated which nat-
ural language words corresponded to which mean-
ing representation components. This information
could have been used to aid KRISP, however, we
found many inconsistencies and errors in the pro-
vided word-aligned semantic trees and chose not
to use them. In addition, KRISP seemed to learn
most of that information on its own anyway.

The Task 6 also included integrating semantic
parsing with spatial planning. This meant that if
the semantic parser generates an RCL representa-
tion that does not make sense for the given block
configuration on the board, then it could be dis-
missed and the next best RCL representation could
be considered. Besides generating the best mean-
ing representation for a natural language utterance,
KRISP is also capable of generating multiple pos-
sible meaning representations sorted by their prob-
abilities. We could have used this capability to
output only the best RCL representation that is
valid for the given board configuration. Unfortu-
nately, unfamiliarity with the provided Java API
for the spatial planner and lack of time prevented
us from doing this.

*action:→ ( action: move )
*action:→ ( action: drop )
*action:→ ( action: take )
*cardinal:→ ( cardinal: 1 )
*cardinal:→ ( cardinal: 2 )
*cardinal:→ ( cardinal: 3 )
*cardinal:→ ( cardinal: 4 )
*color:→ ( color: magenta )
*color:→ ( color: red )
*color:→ ( color: white )
*color:→ ( color: cyan )
*color:→ ( color: green )
*color:→ ( color: yellow )
*color:→ ( color: blue )
*color:→ ( color: gray )
*indicator:→ ( indicator: rightmost )
*indicator:→ ( indicator: back )
*indicator:→ ( indicator: center )
*indicator:→ ( indicator: right )
*indicator:→ ( indicator: leftmost )
*indicator:→ ( indicator: individual )
*indicator:→ ( indicator: nearest )
*indicator:→ ( indicator: front )
*indicator:→ ( indicator: left )
*reference-id:→ ( reference-id: 1 )
*relation:→ ( relation: right )
*relation:→ ( relation: forward )
*relation:→ ( relation: within )
*relation:→ ( relation: above )
*relation:→ ( relation: nearest )
*relation:→ ( relation: adjacent )
*relation:→ ( relation: front )
*relation:→ ( relation: left )
*relation:→ ( relation: backward )
*type:→ ( type: type-reference-group )
*type:→ ( type: board )
*type:→ ( type: prism )
*type:→ ( type: cube )
*type:→ ( type: type-reference )
*type:→ ( type: cube-group )
*type:→ ( type: corner )
*type:→ ( type: robot )
*type:→ ( type: stack )
*type:→ ( type: edge )
*type:→ ( type: region )
*type:→ ( type: tile )
*type:→ ( type: reference )
*indicator:→ *indicator: *indicator:
*color:→ *color: *color:
*ct:→ *color: *type:
*ict:→ *indicator: *ct:
*ctr:→ *ct: *reference-id:
*cct:→ *cardinal: *ct:
*ed:→ *entity: ( destination: *spatial-relation: )
*entity:→ ( entity: *type: )
*entity:→ ( entity: *type: *reference-id: )
*entity:→ ( entity: *type: *spatial-relation: )
*entity:→ ( entity: *ct: )
*entity:→ ( entity: *indicator: *type: )
*entity:→ ( entity: *ict: )
*entity:→ ( entity: *ict: *spatial-relation: )
*entity:→ ( entity: *cardinal: *type: )
*entity:→ ( entity: *cct: )
*entity:→ ( entity: *cct: *spatial-relation: )
*entity:→ ( entity: *ctr: )
*entity:→ ( entity: *ct: *spatial-relation: )
*entity:→ ( entity: *ctr: *spatial-relation: )
*measure:→ ( measure: *entity: )
*mr:→ *measure: *relation:
*spatial-relation:→ ( spatial-relation: *relation: *entity: )
*spatial-relation:→ ( spatial-relation: *mr: )
*spatial-relation:→ ( spatial-relation: *mr: *entity: )
*S→ ( sequence: *S *S )
*S→ ( event: *action: *ed: )
*S→ ( event: *action: *entity: )

Table 1: Grammar for the Robot Command Lan-
guage (RCL) given to KRISP for semantic parsing.
The non-terminals are indicated with a “*” in their
front. The start symbol is *S.

825



 0

 10

 20

 30

 40

 50

 60

 70

 80

 90

 100

 0  5  10  15  20  25  30  35  40  45  50

P
re

ci
si

on
 (%

)

Recall (%)

Figure 2: Precision-recall curve for the semantic
parsing output on test sentences.

4 Results
We found that KRISP could not be trained beyond
1500 examples in this domain because the num-
ber of negative examples that are generated during
the training process would become too large for
the available memory size. This is something that
could be fixed in the future by suitably sampling
negative examples. Using the first 1500 train-
ing examples, we evaluated KRISP’s performance
on the provided 909 test examples. A generated
RCL representation is considered correct only if
it exactly matches the correct answer; no partial
credit is given. In order to avoid generating incor-
rect meaning representations when it is not confi-
dent, KRISP uses a threshold and if the confidence
(probability) of the best semantic derivation is be-
low this threshold, it does not generate any mean-
ing representation. This threshold was set to 0.05
as was previously done for other domains.

Performance was measured in terms of preci-
sion (the percentage of generated meaning repre-
sentations that were correct) and recall (the per-
centage of all sentences for which correct meaning
representations were obtained). Given that KRISP
also gives confidences with its output meaning
representations, we can compute precisions and
recalls at various confidence levels. Figure 2
shows the entire precision-recall curve thus ob-
tained. The best F-measure (harmonic mean of
precision and recall) on this curve is 51.18% pdf
at 72.67% precision and 39.49% recall. The pre-
cision at the highest recall was 45.98% which we
had reported as our official evaluation result for
the SemEval Task 6.

We further analyzed the results according to the
lengths of the sentences and found that KRISP was

Sentence length Accuracy (Correct/Total)
1-3 100.00% (15/15)
4-7 71.20% (136/191)
8-11 51.76% (147/284)

12-15 41.80% (79/189)
16-19 22.22% (28/126)
20-23 15.71% (11/70)
24-27 3.23% (1/31)
28-31 33.33% (1/3)

All 45.98% (418/909)

Table 2: Accuracy of semantic parsing across test
sentences of varying lengths.

very accurate with shorter sentences and became
progressively less accurate as the lengths of the
sentences increase. Table 2 shows these results.
This could be simply because the longer the sen-
tence, the more the likelihood of making an error,
and since no partial credit is given, the entire out-
put meaning representation is deemed incorrect.

On further error analysis we observed that there
was some variability in the meaning representa-
tions. The “move” and “drop” actions seemed
to mean the same thing and were used alterna-
tively. For example in the training data, the ut-
terance “place the red block on single blue block”
had “(action: drop)” in the corresponding mean-
ing representation, while “place red cube on grey
cube” had “(action: move)”, but there is no ap-
parent difference between the two cases. There
were many such instances. This was confusing
KRISP’s training algorithm because it would col-
lect the same phrase sometimes as a positive ex-
ample and sometimes as a negative example. This
also affected the evaluation, because KRISP would
generate “move” which won’t match “drop”, or
vice-versa, and the evaluator will call it an error.

5 Conclusions
We participated in the SemEval 2014 Task 6 of su-
pervised semantic parsing of robotic spatial com-
mands. We used an existing semantic parser
learner, KRISP, and trained it on this domain
which required minimum time and effort from our
side. The trained parser was able to map natu-
ral language robotic spatial commands into their
formal robotic command language representations
with good accuracy, particularly for shorter sen-
tences.

826



References
Nello Cristianini and John Shawe-Taylor. 2000. An

Introduction to Support Vector Machines and Other
Kernel-based Learning Methods. Cambridge Uni-
versity Press.

Kais Dukes. 2013. Semantic annotation of robotic
spatial commands. In Proceedings of the Language
and Technology Conference (LTC-2013), Poznan,
Poland.

Ruifang Ge and Raymond J. Mooney. 2005. A sta-
tistical semantic parser that integrates syntax and
semantics. In Proceedings of the Ninth Confer-
ence on Computational Natural Language Learning
(CoNLL-2005), pages 9–16, Ann Arbor, MI, July.

Rohit J. Kate and Raymond J. Mooney. 2006. Us-
ing string-kernels for learning semantic parsers. In
Proceedings of the 21st International Conference on
Computational Linguistics and 44th Annual Meet-
ing of the Association for Computational Linguistics
(COLING/ACL-06), pages 913–920, Sydney, Aus-
tralia, July.

Rohit J. Kate and Raymond J. Mooney. 2007a. Learn-
ing language semantics from ambiguous supervi-
sion. In Proceedings of the Twenty-Second Con-
ference on Artificial Intelligence (AAAI-07), pages
895–900, Vancouver, Canada, July.

Rohit J. Kate and Raymond J. Mooney. 2007b.
Semi-supervised learning for semantic parsing us-
ing support vector machines. In Proceedings of
Human Language Technologies: The Conference of
the North American Chapter of the Association for
Computational Linguistics (NAACL-HLT-07), pages
81–84, Rochester, NY, April.

Rohit J. Kate, Yuk Wah Wong, and Raymond J.
Mooney. 2005. Learning to transform natural to for-
mal languages. In Proceedings of the Twentieth Na-
tional Conference on Artificial Intelligence (AAAI-
05), pages 1062–1068, Pittsburgh, PA, July.

Rohit J. Kate. 2008. Transforming meaning represen-
tation grammars to improve semantic parsing. In
Proceedings of the Twelfth Conference on Computa-
tional Natural Language Learning (CoNLL-2008),
pages 33–40. Association for Computational Lin-
guistics.

Tom Kwiatkowski, Luke Zettlemoyer, Sharon Goldwa-
ter, and Mark Steedman. 2011. Lexical generaliza-
tion in CCG grammar induction for semantic pars-
ing. In Proceedings of the Conference on Empirical
Methods in Natural Language Processing (EMNLP-
2011), pages 1512–1523. Association for Computa-
tional Linguistics.

Huma Lodhi, Craig Saunders, John Shawe-Taylor,
Nello Cristianini, and Chris Watkins. 2002. Text
classification using string kernels. 2:419–444.

Wei Lu, Hwee Tou Ng, Wee Sun Lee, and Luke S.
Zettlemoyer. 2008. A generative model for pars-
ing natural language to meaning representations. In
Proceedings of the 2008 Conference on Empirical
Methods in Natural Language Processing (EMNLP-
08), Honolulu, HI, October.

Raymond J. Mooney. 2007. Learning for semantic
parsing. In A. Gelbukh, editor, Computational Lin-
guistics and Intelligent Text Processing: Proceed-
ings of the 8th International Conference (CICLing-
2007), Mexico City, pages 311–324. Springer Ver-
lag, Berlin.

Lappoon R. Tang and Raymond J. Mooney. 2001. Us-
ing multiple clause constructors in inductive logic
programming for semantic parsing. In Proceedings
of the 12th European Conference on Machine Learn-
ing (ECML-2001), pages 466–477, Freiburg, Ger-
many.

David H. D. Warren and Fernando C. N. Pereira. 1982.
An efficient easily adaptable system for interpret-
ing natural language queries. American Journal of
Computational Linguistics, 8(3-4):110–122.

Terry Winograd. 1972. Understanding Natural Lan-
guage. Academic Press, Orlando, FL.

Yuk Wah Wong and Raymond J. Mooney. 2006.
Learning for semantic parsing with statistical ma-
chine translation. In Proceedings of Human Lan-
guage Technology Conference / North American
Chapter of the Association for Computational Lin-
guistics Annual Meeting (HLT-NAACL-06), pages
439–446, New York City, NY.

William A. Woods. 1977. Lunar rocks in natural
English: Explorations in natural language question
answering. In Antonio Zampoli, editor, Linguistic
Structures Processing. Elsevier North-Holland, New
York.

John M. Zelle and Raymond J. Mooney. 1996. Learn-
ing to parse database queries using inductive logic
programming. In Proceedings of the Thirteenth Na-
tional Conference on Artificial Intelligence (AAAI-
96), pages 1050–1055, Portland, OR, August.

Luke S. Zettlemoyer and Michael Collins. 2005.
Learning to map sentences to logical form: Struc-
tured classification with probabilistic categorial
grammars. In Proceedings of 21st Conference on
Uncertainty in Artificial Intelligence (UAI-2005),
Edinburgh, Scotland, July.

827


