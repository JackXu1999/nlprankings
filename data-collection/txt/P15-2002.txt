



















































On metric embedding for boosting semantic similarity computations


Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics
and the 7th International Joint Conference on Natural Language Processing (Short Papers), pages 8–14,

Beijing, China, July 26-31, 2015. c©2015 Association for Computational Linguistics

On metric embedding for boosting semantic similarity computations

Julien Subercaze, Christophe Gravier, Frederique Laforest
Université de Lyon, F-42023, Saint-Etienne, France,

CNRS, UMR5516, Laboratoire Hubert Curien, F-42000, Saint-Etienne, France,
Université de Saint-Etienne, Jean Monnet, F-42000, Saint-Etienne, France.

firstname.lastname@univ-st-etienne.fr

Abstract

Computing pairwise word semantic simi-
larity is widely used and serves as a build-
ing block in many tasks in NLP. In this
paper, we explore the embedding of the
shortest-path metrics from a knowledge
base (Wordnet) into the Hamming hyper-
cube, in order to enhance the computa-
tion performance. We show that, although
an isometric embedding is untractable, it
is possible to achieve good non-isometric
embeddings. We report a speedup of
three orders of magnitude for the task of
computing Leacock and Chodorow (LCH)
similarity while keeping strong correla-
tions (r = .819, ρ = .826).

1 Introduction

Among semantic relatedness measures, seman-
tic similarity encodes the conceptual distance be-
tween two units of language – this goes beyond
lexical ressemblance. When words are the speech
units, semantic similarity is at the very core of
many NLP problems. It has proven to be essen-
tial for word sense disambiguation (Mavroeidis et
al., 2005; Basile et al., 2014), open domain ques-
tion answering (Yih et al., 2014), and informa-
tion retrieval on the Web (Varelas et al., 2005),
to name a few. Two established strategies to es-
timate pairwise word semantic similarity includes
knowledge-based and distributional semantics.

Knowledge-based approaches exploit the struc-
ture of the taxonomy ((Leacock and Chodorow,
1998; Hirst and St-Onge, 1998; Wu and Palmer,
1994)), its content ((Banerjee and Pedersen,
2002)), or both (Resnik, 1995; Lin, 1998). In
the earliest applications, Wordnet-based semantic
similarity played a predominant role so that se-
mantic similarity measures reckon with informa-
tion from the lexical hierarchy. It therefore ignores

contextual information on word occurrences and
relies on humans to encode such hierarchies – a
tedious task in practice. In contrast, well-known
distributional semantics strategies encode seman-
tic similarity using the correlation of statistical ob-
servations on the occurrences of words in a textual
corpora (Lin, 1998).

While providing a significant impact on a
broad range of applications, (Herbelot and Gane-
salingam, 2013; Lazaridou et al., 2013; Beltagy
et al., 2014; Bernardi et al., 2013; Goyal et al.,
2013; Lebret et al., 2013), distributional semantics
– similarly to knowledge-based strategies – strug-
gle to process the ever-increasing size of textual
corpora in a reasonable amount of time. As an an-
swer, embedding high-dimensional distributional
semantics models for words into low-dimensional
spaces (henceforth word embedding (Collobert
and Weston, 2008)) has emerged as a popular
method. Word embedding utilizes deep learn-
ing to learn a real-valued vector representation of
words so that any vector distance – usually the
cosine similarity – encodes the word-to-word se-
mantic similarity. Although word embedding was
successfully applied for several NLP tasks (Her-
mann et al., 2014; Andreas and Klein, 2014; Clin-
chant and Perronnin, 2013; Xu et al., 2014; Li
and Liu, 2014; Goyal et al., 2013), it implies a
slow training phase – measured in days (Collobert
and Weston, 2008; Mnih and Kavukcuoglu, 2013;
Mikolov et al., 2013), though re-embedding words
seems promising (Labutov and Lipson, 2013).
There is another usually under-considered issue:
the tractability of the pairwise similarity computa-
tion in the vector space for large volume of data.
Despite these limitations, the current enthusiasm
for word embedding certainly echoes the need
for lightning fast word-to-word semantic similar-
ity computation.

In this context, it is surprising that embedding
semantic similarity of words in low dimensional

8



spaces for knowledge-based approaches is under-
studied. This oversight may well condemn the
word-to-word semantic similarity task to remain
corpus-dependant – i.e. ignoring the background
knowledge provided by a lexical hierarchy.

In this paper, we propose an embedding of
knowledge base semantic similarity based on the
shortest path metric (Leacock and Chodorow,
1998), into the Hamming hypercube of size n (the
size of targeted binary codes). The Leacock and
Chodorow semantic similarity is one of the most
meaningful measure. It yields the second rank
for highest correlation with the data collected by
(Miller and Charles, 1991), and the first one within
edge centric approaches, as shown by (Seco et al.,
2004). This method is only surpassed by the infor-
mation theoretic based similarity from (Jiang and
Conrath, 1997). A second study present similar
result (Budanitsky and Hirst, 2006), while a third
one ranks this similarity measure at the first rank
for precision in paraphrase identification (Mihal-
cea et al., 2006).

The hypercube embedding technique benefits
from the execution of Hamming distance within a
few cycles on modern CPUs. This allows the com-
putation of several millions distances per second.
Multi-index techniques allows the very fast com-
putation of top-k queries (Norouzi et al., 2012) on
the Hamming space. However, the dimension of
the hypercube (i.e. the number of bits used to
represent an element) should obey the threshold
of few CPU words (64, 128 . . . , bits) to maintain
such efficiency (Heo et al., 2012).

An isometric embedding requires a excessively
high number of dimensions to be feasible. How-
ever, in this paper we show that practical em-
beddings exist and present a method to construct
them. The best embedding presents very strong
correlations (r = .819, ρ = .829) with the Lea-
cock & Chodorow similarity measure (LCH in the
rest of this paper). Our experiments against the
state-of-the art implementation including caching
techniques show that performance is increased by
up to three orders of magnitude.

2 Shortest path metric embedding

Let us first introduce few notations. We denoteHn2
as an n-dimensional hypercube whose nodes are
labeled by the 2n binary n-tuples. The nodes are
adjacent if and only if their corresponding n-tuples
differ in exactly one position, i.e. their Hamming

distance (`1) is equal to one. In what follows, Qn

denotes the metric space composed ofHn2 with `1.
We tackle the following problem: We aim

at defining a function f that maps every node
w of the taxonomy (Wordnet for Leacock &
Chodorow) intoQn so that for every pair of nodes:
∀(wi, wj), d(wi, wj) = λ · `1(f(wi), f(wj)),
where λ is a scalar. For practical purposes, the
construction of the mapping should also be rea-
sonable in terms of time complexity.

Theoretical limitations Wordnet with its hyper-
nym relation forms a partially ordered set (poset).
The first approach is to perform an isometric em-
bedding from the poset with shortest path distance
into the Hamming hypercube. Such a mapping
would exactly preserve the original distance in the
embedding. As proven by (Deza and Laurent,
1997), poset lattices, with their shortest path met-
ric, can be isometrically embedded into the hyper-
cube, but the embedding requires 2n dimensions.
The resulting embedding would not fit in the mem-
ory of any existing computer, for a lattice having
more than 60 nodes. Using Wordnet, with tens of
thousands synsets, this embedding is untractable.
The bound given by Deza et al. is not tight, how-
ever it would require a more than severe improve-
ment to be of any practical interest.

Tree embedding To reduce the dimensionality,
we weaken the lattice into a tree. We build a
tree from the Wordnet’s Hyponyms/Hypernyms
poset by cutting 1,300 links, which correspond to
roughly one percent of the edges in the original lat-
tice. The nature of the cut to be performed can be
subject to discussion. In this preliminary research,
we used a simple approach. Since hypernyms are
ordered, we decided to preserve only the first hy-
pernym – semantically more relevant, or at least
statistically – and to cut edges to other hypernyms.

00000

0010010000 01000

01010 01001

A

B C F

D E

000

001100 010

A

B C F

D E

A

B C F

D E

Figure 1: Construction of isometric embedding on
a sample tree. For this six nodes tree, the embed-
ding requires five bits.

9



Our experiments in Table 1 shows that using the
obtained tree instead of the lattice keeps a high
correlation (r = .919, ρ = .931) with the origi-
nal LCH distance, thus validating the approach.

(Wilkeit, 1990) showed that any k-ary tree of
size n can be embedded into Qn−1. We give an
isometric embedding algorithm, which is linear
in time and space, exhibiting a much better time
complexity than Winkler’s generic approach for
graphs, running in O(n5) (Winkler, 1984). Start-
ing with an empty binary signature, the algorithm
is the following: at each step of a depth-first pre-
order traversal: if the node has k children, we set
the signature for the i-th child by appending k ze-
roes to the parent’s signature and by setting the i-th
of the k bits to one. An example is given in Figure
1. However, when using real-world datasets such
as Wordnet, the embedding still requires several
thousands of bits to represent a node. This dimen-
sion reduction to tens of kilobits per node remains
far from our goal of several CPU words, and calls
for a task-specific approach.

Looking at the construction of the isometric em-
bedding, the large dimension results from the ap-
pending of bits to all nodes in the tree. This results
in a large number of bits that are rarely set to one.
At the opposite, the optimal embedding in terms
of dimension is given by the approach of (Chen
and Stallmann, 1995) that assigns gray codes to
each node. However, the embedding is not isomet-
ric and introduces a very large error. As shown in
Table 1, this approach gives the most compact em-
bedding with dlog2(87,000)e = 17 bits, but leads
to poor correlations (r = .235 and ρ = .186).

An exhaustive search is also out of reach: for
a fixed dimension n and r nodes in the tree, the
number of combinations C is given by:

C =
(2n)!

(n− r)!
Even with the smallest value of n = 17 and r =
87,000, we have C > 1010,000. With n = 64, to
align to a CPU word, C > 10100,000.

3 Non-isometric Embedding

Our approach is a trade-off between the isomet-
ric embedding and the pre-order gray code solu-
tion. When designing our algorithm, we had to
decide which tree distance we will preserve, either
between parent and children, or among siblings.

Therefore, we take into account the nature of the
tree that we aim to embed into the hypercube. Let

B C D E F

A

00000

00001 00010 00100 01000 10000

B C D E F

A

000

001 011 010 110 101

B C D E F

A
0000

0001 0010 0100 1000 1001

Isometric Pre-order Gray Code 
RMSE=.66, r=-0.07,  ρ=-0.12

Additional bit and sorting 
RMSE=.33, r=.55,  ρ=-.57

B C D E F

A

000

001 010 100 101 011
Value sorting

RMSE=.6, r=.19,  ρ=.16

Figure 2: Approaches to reduce the tree embed-
ding dimensions.

first analyse the characteristics of the tree obtained
from the cut. The tree has an average branching
factor of 4.9, with a standard deviation of 14 and
96% of the nodes have a branching factor lesser
than 20. At the opposite, the depth is very stable
with an average of 8.5, a standard deviation of 2,
and a maximum of 18. Consequently, we decide
to preserve the parent-children distance over the
very unstable siblings distance. To lower the di-
mensions, we aim at allocating less than k bits for
a node with k children, thus avoiding the signature
extension taking place for every node in the iso-
metric approach. Our approach uses the following
principles.

Branch inheritance: each node inherits the
signature from its father, but contrary to isometric
embedding, the signature extension does not ap-
ply to all the nodes in the tree. This guarantees the
compactness of the structure.

Parentship preservation: when allocating less
bits than required for the isometric embedding,
we introduce an error. Our allocation favours as
much as possible the parentship distance at the
expense of the sibling distance. As a first allo-
cation, for a node with k children, we allocate
dlog2(k + 1)e bits for the signatures, in order to
guarantee the unicity of the signature. Each child
node is assigned a signature extension using a
gray code generation on the dlog2(k + 1)e bits.
The parent node simply extends its signature with
dlog2(k + 1)e zeroes, which is much more com-
pact than the k bits from the isometric embedding
algorithm.

Word alignment: The two previous techniques
give a compact embedding for low-depth trees,
which is the case of Wordnet. The dimension D

10



of the embedding is not necessarily aligned to
a CPU word size W : kW ≤ D ≤ (k + 1)W .
We want to exploit the potential (k + 1)W − D
bits that are unused but still processed by the
CPU. For this purpose we rank the nodes along
a value v(i), i ∈ N to decide which nodes are
allowed to use extra bits. Since our approach
favours parent/child distance, we want to allow
additional bits for nodes that are both close to
the root and the head of a large branch. To bal-
ance the two values, we use the following formula:
v(i) = (maxdepth − depth(i)) · log(sizebranch(i))
We therefore enable our approach to take full
advantage of the otherwise unused bits.

In order to enhance the quality of the embed-
ding, we also introduce two potential optimiza-
tions:

The first is called Children-sorting: we allocate
a better preserving signature to children having
larger descents. A better signature is among the
available the 2dlog2(k+1)e available, the one that re-
duces the error with the parent node. We rank the
children by the size of their descent and assign the
signatures accordingly.

The second optimization is named Value-
sorting and is depicted in Figure 2. Among the
2dlog2(k+1)e available signatures, only k + 1 will
be assigned (one for the parent and k for the chil-
dren). For instance in the case of 5 children as
depicted in Figure 2, we allocate 3 bits for 6 signa-
tures. We favor the parentship distance by select-
ing first the signatures where one bit differs from
the parent’s one.

4 Experiments

In this section, we run two experiments to eval-
uate both the soundness and the performance of
our approach. In the first experiment, we test the
quality of our embedding against the tree distance
and the LCH similarity. The goal is to assess the
soundness of our approach and to measure the cor-
relation between the approximate embedding and
the original LCH similarity.

In the second experiment we compare the com-
putational performance of our approach against an
optimized in-memory library that implements the
LCH similarity.

Our algorithm called FSE for Fast Similarity
Embedding, is implemented in Java and avail-
able publicly1. Our testbed is an Intel Xeon E3

1Source code, binaries and instructions to reproduce

80 90 100 110 120 130

0.75

0.8

0.85

0.9

Embedding dimension

Pe
ar

so
n’

s
r

Combined

Base + value sorting

Base + children sorting

FSE-Base

Figure 3: FSE: influence of optimizations and di-
mensions on the correlation over the tree distance
on Wordnet.

1246v3 with 16GB of memory, a 256Go PCI Ex-
press SSD. The system runs a 64-bit Linux 3.13.0
kernel with Oracle’s JDK 7u67.

The FSE algorithm is implemented in various
flavours. FSE-Base denotes the basic algorithm,
containing none of the optimizations detailed in
the previous section. FSE-Base can be aug-
mented with either or both of the optimizations.
This latter version is denoted FSE-Best.

4.1 Embedding

We first measure the correlation of the embedded
distance with the original tree distance, to validate
the approach and to determine the gain induced by
the optimizations. Figure 3 shows the influence
of dimensions and optimizations on the Pearson’s
product moment correlation r. The base version
reaches r = .77 for an embedding of dimension
128. Regarding the optimizations, children sort-
ing is more efficient than value sorting, excepted
for dimensions under 90. Finally, combined opti-
mizations (FSE-Best) exhibit a higher correlation
(r = .89) than the other versions.

We then measure the correlation with the Lea-
cock & Chodorow similarity measure. We com-
pare our approach to the gray codes embedding
from (Chen and Stallmann, 1995) as well as the
isometric embedding. We compute the correlation
on 5 millions distances from the Wordnet-Core
noun pairs2 (Table 1). As expected, the embed-

the experiments are available at http://demo-satin.
telecom-st-etienne.fr/FSE/

2https://wordnet.princeton.edu/
wordnet/download/standoff/

11



Embedding Bits Pearson’s r Spearman’s ρ

Chen et al. 17 .235 .186
FSE-Base 84 .699 .707
FSE-Best 128 .819 .829
Isometric 84K .919 .931

Table 1: Correlations between LCH, isometric em-
bedding, and FSE for all distances on all Wordnet-
Core noun pairs (p-values ≤ 10−14).

Algorithm Measure
Amount of pairs (n)

103 104 105 106 107

WS4J 103· ms 0.156 1.196 11.32 123.89 1,129.3
FSE-Best ms 0.04 0.59 14.15 150.58 1,482

speedup ×3900 ×2027 ×800 ×822 ×762

Table 2: Running time in milliseconds for pairwise
similarity computations.

ding obtained using gray codes present a very low
correlation with the original distance.

Similarly to the results obtained on the tree dis-
tance correlation, FSE-Best exhibits the highest
scores with r = .819 and ρ = .829, not far from
the theoretical bound of r = .919 and ρ = .931
for the isometric embedding of the same tree. Our
approach requires 650 times less bits than the iso-
metric one, while keeping strong guarantees on the
correlation with the original LCH distance.

4.2 Speedup

Table 4.2 presents the computation time of the
LCH similarity. This is computed using WS4J3, an
efficient library that enables in-memory caching.

Because of the respective computational com-
plexities of the Hamming distance and the shortest
path algorithms, FSE unsurprisingly boosts LCH
similarity computation by orders of magnitudes.
When the similarity is computed on a small num-
ber of pairs (a situation of the utmost practical in-
terest), the factor of improvement is three orders
of magnitude. This factor decreases to an amount
of 800 times for very large scale applications. The
reason of the decrease is that WS4J caching mech-
anism becomes more efficient for larger numbers
of comparisons. As the caching system stores
shortest path between nodes, these computed val-
ues are more likely to be a subpath of another
query when the number of queries grows.

3https://code.google.com/p/ws4j/

5 Conclusion

We proposed in this paper a novel approach based
on metric embedding to boost the computation of
shortest-path based similarity measures such as
the one of Leacock & Chodorow. We showed that
an isometric embedding of the Wordnet’s hyper-
nym/hyponym lattice does not lead to a practical
solution. To tackle this issue, we weaken the lat-
tice structure into a tree by cutting less relevant
edges. We then devised an algorithm and several
optimizations to embed the tree shortest-path dis-
tance in a word-aligned number of bits. Such an
embedding can be used to boost NLP core algo-
rithms – this was demonstrated here on the com-
putation of LCH for which our approach offers a
factor of improvement of three orders of magni-
tude, with a very strong correlation.

Acknowledgements

This work is supported by the OpenCloudware
project. OpenCloudware is funded by the French
FSN (Fond national pour la Société Numérique),
and is supported by Pôles Minalogic, Systematic
and SCS.

References
Jacob Andreas and Dan Klein. 2014. How much do

word embeddings encode about syntax. In Associa-
tion for Computational Linguistics (ACL).

Satanjeev Banerjee and Ted Pedersen. 2002. An
adapted lesk algorithm for word sense disambigua-
tion using wordnet. In Computational linguis-
tics and intelligent text processing, pages 136–145.
Springer.

Pierpaolo Basile, Annalina Caputo, and Giovanni Se-
meraro. 2014. An enhanced lesk word sense dis-
ambiguation algorithm through a distributional se-
mantic model. In Proceedings of COLING, pages
1591–1600.

Islam Beltagy, Katrin Erk, and Raymond Mooney.
2014. Semantic parsing using distributional seman-
tics and probabilistic logic. Association for Compu-
tational Linguistics (ACL), page 7.

Raffaella Bernardi, Georgiana Dinu, Marco Marelli,
and Marco Baroni. 2013. A relatedness benchmark
to test the role of determiners in compositional dis-
tributional semantics. In Association for Computa-
tional Linguistics (ACL), pages 53–57.

Alexander Budanitsky and Graeme Hirst. 2006. Eval-
uating wordnet-based measures of lexical semantic
relatedness. Computational Linguistics, 32(1):13–
47.

12



Woei-Kae Chen and Matthias FM Stallmann. 1995.
On embedding binary trees into hypercubes.
Journal of Parallel and Distributed Computing,
24(2):132–138.

Stéphane Clinchant and Florent Perronnin. 2013. Ag-
gregating continuous word embeddings for informa-
tion retrieval. Association for Computational Lin-
guistics (ACL).

Ronan Collobert and Jason Weston. 2008. A unified
architecture for natural language processing: Deep
neural networks with multitask learning. In Pro-
ceedings of the 25th international conference on
Machine learning, pages 160–167. ACM.

M. Deza and M. Laurent. 1997. Geometry of Cuts and
Metrics. Springer, 588 pages.

Kartik Goyal, Sujay Kumar Jauhar, Huiying Li, Mrin-
maya Sachan, Shashank Srivastava, and Eduard H
Hovy. 2013. A structured distributional semantic
model for event co-reference. In Association for
Computational Linguistics (ACL), pages 467–473.

Jae-Pil Heo, Youngwoon Lee, Junfeng He, Shih-Fu
Chang, and Sung-Eui Yoon. 2012. Spherical hash-
ing. In Computer Vision and Pattern Recognition
(CVPR), 2012 IEEE Conference on, pages 2957–
2964. IEEE.

Aurélie Herbelot and Mohan Ganesalingam. 2013.
Measuring semantic content in distributional vec-
tors. In Association for Computational Linguistics
(ACL), pages 440–445.

Karl Moritz Hermann, Dipanjan Das, Jason Weston,
and Kuzman Ganchev. 2014. Semantic frame iden-
tification with distributed word representations. In
Association for Computational Linguistics (ACL).

Graeme Hirst and David St-Onge. 1998. Lexical
chains as representations of context for the detec-
tion and correction of malapropisms. WordNet: An
electronic lexical database, 305:305–332.

Jay J Jiang and David W Conrath. 1997. Semantic
similarity based on corpus statistics and lexical tax-
onomy. Proceedings of the 10th Research on Com-
putational Linguistics International Conference.

Igor Labutov and Hod Lipson. 2013. Re-embedding
words. In ACL (2), pages 489–493.

Angeliki Lazaridou, Marco Marelli, Roberto Zampar-
elli, and Marco Baroni. 2013. Compositional-ly
derived representations of morphologically complex
words in distributional semantics. In Association
for Computational Linguistics (ACL), pages 1517–
1526.

Claudia Leacock and Martin Chodorow. 1998. Com-
bining local context and wordnet similarity for word
sense identification. WordNet: An electronic lexical
database, 49(2):265–283.

Rémi Lebret, Joël Legrand, and Ronan Collobert.
2013. Is Deep Learning Really Necessary for Word
Embeddings? Technical report, Idiap.

Chen Li and Yang Liu. 2014. Improving text normal-
ization via unsupervised model and discriminative
reranking. Association for Computational Linguis-
tics (ACL), page 86.

Dekang Lin. 1998. An information-theoretic defini-
tion of similarity. In ICML, volume 98, pages 296–
304.

Dimitrios Mavroeidis, George Tsatsaronis, Michalis
Vazirgiannis, Martin Theobald, and Gerhard
Weikum. 2005. Word sense disambiguation for
exploiting hierarchical thesauri in text classification.
In Knowledge Discovery in Databases: PKDD
2005, pages 181–192. Springer.

Rada Mihalcea, Courtney Corley, and Carlo Strappa-
rava. 2006. Corpus-based and knowledge-based
measures of text semantic similarity. In AAAI, vol-
ume 6, pages 775–780.

Tomas Mikolov, Kai Chenand, Greg Corradoand, and
Jeffrey Dean. 2013. Efficient estimation of word
representations in vector space. In Proceedings of
the International Conference on Learning Represen-
tations.

George A Miller and Walter G Charles. 1991. Contex-
tual correlates of semantic similarity. Language and
cognitive processes, 6(1):1–28.

Andriy Mnih and Koray Kavukcuoglu. 2013. Learning
word embeddings efficiently with noise-contrastive
estimation. In C.J.C. Burges, L. Bottou, M. Welling,
Z. Ghahramani, and K.Q. Weinberger, editors, Ad-
vances in Neural Information Processing Systems
26, pages 2265–2273. Curran Associates, Inc.

Mohammad Norouzi, Ali Punjani, and David J Fleet.
2012. Fast search in hamming space with multi-
index hashing. In Computer Vision and Pattern
Recognition (CVPR), 2012 IEEE Conference on,
pages 3108–3115. IEEE.

Philip Resnik. 1995. Using information content to
evaluate semantic similarity in a taxonomy. In
Proceedings of the 14th International Joint Confer-
ence on Artificial Intelligence - Volume 1, IJCAI’95,
pages 448–453, San Francisco, CA, USA. Morgan
Kaufmann Publishers Inc.

Nuno Seco, Tony Veale, and Jer Hayes. 2004. An in-
trinsic information content metric for semantic sim-
ilarity in wordnet. In ECAI, volume 16, page 1089.

Giannis Varelas, Epimenidis Voutsakis, Paraskevi
Raftopoulou, Euripides GM Petrakis, and Evange-
los E Milios. 2005. Semantic similarity methods
in wordnet and their application to information re-
trieval on the web. In Proceedings of the 7th an-
nual ACM international workshop on Web informa-
tion and data management, pages 10–16. ACM.

13



Elke Wilkeit. 1990. Isometric embeddings in ham-
ming graphs. Journal of Combinatorial Theory, Se-
ries B, 50(2):179–197.

Peter M Winkler. 1984. Isometric embedding in prod-
ucts of complete graphs. Discrete Applied Mathe-
matics, 7(2):221–225.

Zhibiao Wu and Martha Palmer. 1994. Verbs seman-
tics and lexical selection. In Proceedings of the 32nd
annual meeting on Association for Computational
Linguistics, pages 133–138. Association for Com-
putational Linguistics.

Liheng Xu, Kang Liu, Siwei Lai, and Jun Zhao. 2014.
Product feature mining: Semantic clues versus syn-
tactic constituents. In Association for Computa-
tional Linguistics (ACL), pages 336–346.

Wen-tau Yih, Xiaodong He, and Christopher Meek.
2014. Semantic parsing for single-relation question
answering. In Proceedings of ACL.

14


