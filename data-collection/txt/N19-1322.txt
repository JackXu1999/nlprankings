




































Customizing Grapheme-to-Phoneme System for Non-Trivial Transcription Problems in Bangla Language


Proceedings of NAACL-HLT 2019, pages 3191–3200
Minneapolis, Minnesota, June 2 - June 7, 2019. c©2019 Association for Computational Linguistics

3191

Customizing Grapheme-to-Phoneme System for Non-Trivial
Transcription Problems in Bangla Language

Sudipta Saha Shubha1, Nafis Sadeq1, Shafayat Ahmed1,Md. Nahidul
Islam1,Muhammad Abdullah Adnan1,Md. Yasin Ali Khan2, and

Mohammad Zuberul Islam2

1Bangladesh University of Engineering and Technology (BUET)
2Samsung R&D Institute, Bangladesh

{sudipta,nafis,shafayat,nahid.rimon}@ra.cse.buet.ac.bd
adnan@cse.buet.ac.bd,{yasin.ali,m.zuberul}@samsung.com

Abstract

Grapheme to phoneme (G2P) conversion is an
integral part in various text and speech process-
ing systems, such as: Text to Speech system,
Speech Recognition system, etc. The existing
methodologies for G2P conversion in Bangla
language are mostly rule-based. However,
data-driven approaches have proved their su-
periority over rule-based approaches for large-
scale G2P conversion in other languages, such
as: English, German, etc. As the performance
of data-driven approaches for G2P conversion
depend largely on pronunciation lexicon on
which the system is trained, in this paper, we
investigate on developing an improved train-
ing lexicon by identifying and categorizing the
critical cases in Bangla language and include
those critical cases in training lexicon for de-
veloping a robust G2P conversion system in
Bangla language. Additionally, we have incor-
porated nasal vowels in our proposed phoneme
list. Our methodology outperforms other state-
of-the-art approaches for G2P conversion in
Bangla language.

1 Introduction

Grapheme to phoneme (G2P) conversion provides
a mapping between a word and its pronunciation.
Such mapping provides opportunity for a non-
native person to learn the correct pronunciation of
words of a foreign language. Moreover, in mod-
ern Text to Speech (TTS) and Automatic Speech
Recognition (ASR) systems, G2P conversion is an
integral task.
The task of G2P conversion is generally lan-

guage specific due to language specific conven-
tions, rules, pronunciation constraints, etc. In this
paper, we focus on Modern Standard Bangla. An
example of G2P conversion in Bangla language:
phonetic transcription of অনুশীলন (practice) is /o
n u sh i l O n/. (Please refer to Table 1 for our
phoneme symbols.)

The simplest means of G2P conversion is to
build up a lexicon or dictionary containing the
mapping from words to their corresponding pro-
nunciations. However, it fails to provide pronun-
ciations for unknownwords and inclusion of newer
words increases memory requirement. In another
approach (Mosaddeque et al., 2006), there are pre-
defined rules for the conversion of a word to its
pronunciation. Though such rule-based approach
can work for any word, the system becomes com-
plex when it tries to formulate rules for incorpo-
rating all irregularities of pronunciation in a lan-
guage.
Clearly, these approaches are not feasible for

large-scale G2P conversion which is necessary in
any modern TTS or ASR system. Data-driven ma-
chine learning approaches have great potential in
such large-scale G2P conversion (Rao et al., 2015).
In such an approach, a machine learning model
predicts the phoneme conversion of a grapheme,
being trained on a lexicon. A predominant work
following such approach in Bangla language is by
Google (Gutkin et al., 2016), where they train their
system using 37K words and achieve word-level
accuracy of 81.5%. However, a system trained
on their lexicon will face several shortcomings,
such as: কাদা(mud) andকাঁদা(to cry) are pronounced
differently but will have same phoneme represen-
tation in their system as: /k a d a/. Similarly,
পরী(fairy) and পি (to read) are pronounced differ-
ently but will have same phoneme representation
in their system as: /p o r i/. Moreover, G2P sys-
tem trained on their lexicon performs poorly on
our identified critical cases from the most frequent
100K words (Table 3).
Being motivated to increase the accuracy of

grapheme to phoneme conversion in Bangla lan-
guage, which will also perform well for critical in-
puts, we have developed a customized and robust
G2P system for Bangla language.



3192

Our major contributions are as follows:

(i) We identify and categorize the critical cases
for grapheme to phoneme (G2P) conversion
in Bangla language by analyzing the most fre-
quent 100K words.

(ii) We enrich the training lexicon for develop-
ing a robust G2P conversion system in Bangla
language that performs much better for crit-
ical cases compared to other state-of-the-art
G2P systems.

(iii) We perform phonetic transcriptions consider-
ing nasal vowels as separate phonemes.

(iv) We perform extensive simulations on large-
scale dataset and show that our methodology
outperforms other state-of-the-art approaches
for G2P conversion in Bangla language by
providing word-level accuracy of 90.2%.

The rest of the paper is organized as follows:
we discuss the previous works in Section 2, our
phoneme list in Section 3, identification of critical
cases and categorization of errors in Section 4, de-
velopment of our system in Section 5, experimen-
tal results in Section 6, and conclusion and future
works in Section 7.

2 Previous Works

The research works for G2P in English are quite
extensive. Chen (2003) investigate machine learn-
ing based systems for G2P in English. They exper-
iment with joint maximum entropy n-gram model,
conditional maximum entropymodel, etc. Yao and
Zweig (2015) utilize bi-directional LSTM (Long
Short Term Memory) recurrent neural network for
G2P and achieve 5.45% PER on CMU dictionary
(air, 2015). Thu et al. (2016) show comparisons
among various machine learning algorithms for
G2P in Burmese language. Joint sequence n-gram
models aim to discover joint vocabulary consist-
ing of graphemes and phonemes through the align-
ment of graphemes and phonemes. Bisani and
Ney (2008) develop a joint-sequence model for
G2P. Novak (2012), Novak et al. (2013) are other
prominent works working on this model. Neu-
ral sequence to sequence models are popular for
G2P conversion. Some prominent works on such
models are: Caruana (1997), Jiampojamarn et al.
(2007), Sutskever et al. (2014), Yao and Zweig
(2015), Jiampojamarn et al. (2007), Rao et al.

(2015), Yao and Zweig (2015), Schnober et al.
(2016), Tsvetkov et al. (2016), He et al. (2016),
Wu et al. (2016), Johnson et al. (2016), Toshniwal
and Livescu (2016), and Vaswani et al. (2017).
Again, another line of research deals with G2P

conversion for more than one language. Such
works include: Mana et al. (2001), Kim and Sny-
der (2012), Deri and Knight (2016), and Milde
et al. (2017).
Most of the works related to G2P conversion

that are focused on Bangla language, follow rule-
based approach. Rule-based approach of Mosad-
deque et al. (2006) provides accuracy of 97.01%
on a previously seen corpus containing 736words,
but the system’s accuracy is 81.95% on an previ-
ously unobserved corpus containing 8399 words.
This work was extended by Alam et al. (2011) de-
scribing 3880 rules with an accuracy of 89.48% on
another corpus. Basu et al. (2009) discuss a rule-
based approach considering several information:
parts-of-speech, subsequent context, etc. Their
work describes only 21 rules and provides an accu-
racy of 91.48% on a corpus of 9294 words. Ghosh
et al. (2010) provide a heuristic for G2P that takes
into account parts-of-speech, orthographic, and
contextual information. Their work provides 70%
accuracy on a corpus containing of 755 words. A
prominent work for data driven G2P in Bangla lan-
guage is by Google (Gutkin et al. (2016)). They
develop a lexicon and achieve word-level accu-
racy of 81.5%. Chowdhury et al. (2017) use condi-
tional random field for G2P in Bangla. They report
14.88% phoneme error rate on Google lexicon.

3 Phoneme List

Our Phoneme symbols are provided in Table 1.
This table is a good reference for the 47 phoneme
symbols that we have followed in this paper and
their corresponding International Phonetic Alpha-
bet (IPA) symbols. Throughout the paper, we use
these 47 phoneme symbols, not the IPA symbols.
There is disagreement between linguists whether
nasal vowels should be considered as separate
phonemes (Barman, 2009). We added nasal vow-
els in our phoneme list to differentiate between a
word with its nasalized counterpart, such as the
word কাঁদা(to cry) and কাদা(mud). Here, /a/, /e/, /u/,
/i/, /o/, /O/, /E/, /an/, /en/, /un/, /in/, /on/, /On/, /En/
are normal vowels, /ew/, /ow/, /uw/, /iw/ are weak
vowels, and the rest are consonants.



3193

Ph
on
em

e

IP
A

Ph
on
em

e

IP
A

Ph
on
em

e

IP
A

Ph
on
em

e

IP
A

i i On O ̃ D ã m m
u u an ã Dh ãH r r
e e k k t t R ó
o o kh kh th th l l
E E g g d d h H
O O gh gH dh dH s s
a a c Ù p p sh S
in ĩ ch Ùh ph ph iw i

ˆun ũ j dZ b b ew e
ˆen ẽ jh dZH bh bH ow o
ˆon õ T ú N ŋ uw u
ˆEn æ̃ Th úh n n

Table 1: Our Phoneme Symbols with Their Corre-
sponding IPA Symbols

4 Identification and Categorization of
Non-Trivial Cases for Transcription

We envision of developing a robust G2P system
that will perform reasonably well on any word in
Bangla language. A G2P system that performs
well on the most frequent words, should also do
well on other words. With this motivation, we fo-
cus on increasing accuracy on the most frequent
words. Especially, we are concerned about those
words that are among the most frequent words
but non-trivial or critical for phonetic transcrip-
tion, i.e., current state-of-the-art methodologies
perform poorly on these critical words. We inves-
tigate on identifying and categorizing such non-
trivial or critical cases so that future researchworks
can give special focus on developing methods for
improving phonetic transcriptions of these critical
words.

4.1 Identifying the Most Frequent Words
To get a hold of contemporary usage of Bangla
language, we do extensive crawling. We crawled
42 websites of various Bangla newspapers, blogs,
e-book libraries, wikipedia, etc. covering vari-
ous domains such as: politics, economics, sports,
drama, novel, story, education, entertainment,
general knowledge, history, etc. After data clean-
ing and data normalization, we had about 10M
sentences. We counted how many times each of
the unique words appeared in those sentences. We
then consider the most frequent 100K words and

aim to identify the critical cases for phonetic tran-
scription among these most frequent words.

4.2 Identifying the Critical Cases for
Transcription

After changing the Google lexicon (of size 60K
(around)) according to our phoneme symbols (Ta-
ble 1), we prepare 4 versions of Google’s lexi-
con of size 12K, 24K, 40K, and 60K respec-
tively for identifying the critical cases for phonetic
transcription. Algorithm 1 shows prefix compar-
ing algorithm that we use for compressing a pho-
netic lexicon or dictionary of grapheme sequence
to phoneme sequence. The algorithm matches the
prefix of consecutive words (grapheme sequence)
of a sorted dictionary (sorted according to ascend-
ing order of grapheme sequence of a word) and
keeps a word (with its corresponding phoneme se-
quence) only if it does not share its prefix with any
other words. We run the algorithm successively 3
times, i.e., we use the destination dictionary of one
iteration as the source dictionary of next iteration.
Each iteration produces a minimized version of the
basic lexicon (Google lexicon). After 3 iterations,
the dictionary does not get any more compressed.
We find the phonetic transcriptions of each of the
100K most frequent words using models trained
on each of the 4 versions of Google’s lexicon (ba-
sic + 3minimized). So, from 4models (eachmodel
trained on a version of the basic Google lexicon),
we get 4 sets of transcriptions for the most frequent
100K words. For most of the words (around 70K
words), we observe that the phonetic transcriptions
are exactly same in each of the 4 set. However, for
the remaining 30K words (29105 words to be ex-
act), we observe that at least one set provides dif-
ferent transcription. We take these 30K words to
be the critical cases. Our intuition is that if two
G2P systems: one trained on a smaller version of
the basic lexicon, and another trained on a larger
version of the basic lexicon provide the same tran-
scription for a word, then the word is a trivial case
for phonetic transcription. We then manually ver-
ify the phonetic transcriptions of these 30K words
taking help from 3 linguists and following Chowd-
hury (2016), and consider these 30K words as crit-
ical cases for phonetic transcription.

4.3 Categorizing the Critical Cases
We categorize the critical cases into 7 categories
and observe the distribution of the critical tran-
scriptions into these 7 categories. These 7 cate-



3194

Algorithm 1Algorithm for Compressing a Dictio-
nary or Lexicon
1: sd← sorted sourceDictionary
2: dd← sorted destinationDictionary
3: a.grs← grapheme sequence of lexicon
4: entry a
5: add sd[0] to dd
6: i = 1
7: while i ̸= length(sd) do
8: pw = sd[i− 1]
9: cw = sd[i]
10: if length(pw.grs) ≥ 3 & pw.grs is

prefix of cw.grs then
11: continue
12: else
13: add cw to dd
14: i← i+ 1

gories capture most of the errors. The categories
are:

• Open Close Vowel Confusion: G2P sys-
tem provides pronunciation as close vowel
that should be pronounced as open vowel ide-
ally, and vice-versa. For example, correct
phoneme of ব াঙ (frog) is /b E n g/, but if G2P
system provides output /b e n g/, then it is
an error under this category as in the place of
open vowel (here, /E/), G2P system is giving
close vowel (here, /e/).

• Inherent Vowel Confusion: G2P system
does not provide inherent vowel as output
where there should be an inherent vowel ide-
ally. For example, correct phoneme of সকাল
(morning) is /sh O k a l/, but if G2P system
provides output /sh k a l/, then it is an error
under this category as the output of G2P does
not give inherent vowel (here, /O/).

• Diphthong Confusion: G2P system does
not provide falling diphthong in output where
there should be a falling diphthong ideally.
Or, system does not provide rising diphthong
in output where there should be a rising diph-
thong ideally. For example, correct phoneme
of সই (friend) is /sh o iw/, but if G2P system
provides output /sh o i/, then it is an error un-
der this category as the output of G2P does not
capture the falling diphthong (here, /o iw/).

• s or sh Confusion: G2P system provides /s/
in phonetic transcription, where there should

be /sh/, and vice-versa. For example, correct
phoneme sequence of সংগঠন (organization) is
/sh O N g O Th o n/, but if G2P system pro-
vides output /s O N g O Th o n/, then it is an
error under this category as the output of G2P
gives /s/ in place of /sh/.

• s or ch Confusion: G2P system provides /s/
in phonetic transcription, where there should
be /ch/, and vice-versa. For example, correct
phoneme sequence of
ছাতা (umbrella) is /ch a t a/, but if G2P system
provides output /s a t a/, then it is an error un-
der this category as the output of G2P gives
/s/ in place of /ch/.

• Nasal Confusion: G2P system does not pro-
vide any nasal vowel where there should be
a nasal vowel, and vice-versa. For example,
correct phoneme sequence of চাঁদ (moon) is /c
an d/, but if G2P system provides output /c a
d/, then it is an error under this category as the
output of G2P gives /a/ in place of /an/.

• Other Vowel Confusion: G2P system pro-
vides completely different vowel than the cor-
responding vowel that should ideally be in
that position of the phoneme sequence. Note
that, in the other error categories, for each po-
sition in the phoneme sequence, the gener-
ated and ideal phonemes were somehow re-
lated. But in this category, at a specific posi-
tion of the phoneme sequence, the generated
and ideal phonemes are completely different.
For example, correct phoneme sequence of
অধ বসায় (perseverance) is /o d dh o b O sh
a ew/, but if G2P system provides output /o
d dh a b O sh a ew/, then it is an error under
this category as the output of G2P gives /a/ in
place of /o/ (fourth phoneme).

Algorithm 2 compares amachine-generated lex-
icon with a reference lexicon (manually verified),
where both the lexicons have same grapheme
sequences, but the corresponding phoneme se-
quences may be different. This algorithm counts
how many errors of each category are there in the
machine generated lexicon. The algorithm takes
each entry of the generated lexicon and increases
the count of the corresponding error category (if an
error is present there).
We train attention mechanism based Trans-

former model on each of the 4 lexicons and get



3195

Algorithm 2 Comparing a Generated Lexicon (gl)
with Reference Lexicon (rl)
1: N ← total number of entries in each lexicon
2: A,B,C,D,E, F,G are Open Close Vowel, s

or sh, s or ch, Nasal, Dipthong, Other Vowel,
and Inherent confusions, respectively, all ini-
tially zero

3: H denotes other errors not captured by the 7
categories, initially zero

4: vl and wl are lists of vowels and weak vowels
respectively

5: a.phs← phoneme sequence of lexicon
6: entry a
7: i← 0
8: while i ̸= N do
9: g = gl[i].phs
10: r = rl[i].phs
11: M = min(length(g), length(r))
12: j ← 0
13: while j ̸= M do
14: x = g[j]
15: y = r[j]
16: if x = y then
17: continue
18: (x, y)← sorted(x, y)
19: Total_error = Total_error + 1
20: if ocConfusion(x, y) then
21: A← A+ 1
22: else if (x, y) = (‘‘s”, ‘‘sh”) then
23: B ← B + 1
24: else if (x, y) = (‘‘ch”, ‘‘s”) then
25: C ← C + 1
26: else if x+ ‘‘n” = y then
27: D ← D + 1
28: else if x in wl or y in wl then
29: E ← E + 1
30: else if x in vl and y in vl then
31: F ← F + 1
32: else if removeV owel(g) =

removeV owel(r) then
33: G← G+ 1
34: else
35: H ← H + 1
36: j ← j + 1
37: i← i+ 1

4 G2P models. We find the phoneme representa-
tion of 30K critical cases using each of the 4 G2P
models. Using Algorithm 2, we count the errors of
each category for each of the 4 models. We report

Algorithm 3 Procedure: ocConfusion (x, y)
(Checks if open close vowel confusion)
1: ocSet← [(‘‘O”, ‘‘o”), (‘‘E”, ‘‘e”),
2: (‘‘On”, ‘‘on”), (‘‘En”, ‘‘en”)]
3: if (x, y) in ocSet then
4: return True
5: else
6: return False

Algorithm 4 Procedure: removeVowel
(phoneme_sequence)
1: return phoneme_sequence removing all vow-

els from it

the results in Table 2 and Figure 1. Here, other er-
ror denotes the errors that are not captured by these
7 categories. We see from these results that most
of the errors are under Open Close vowel, s or sh,
Diphthong, and Inherent Vowel confusions.

5 Developing an Improved G2P System
for Bangla Language

We develop an improved lexicon and use two ma-
chine learning based models trained on our lexicon
to develop an improved G2P system for Bangla
language.

5.1 Developing an Improved Lexicon

For developing an improved lexicon, we include
with Google’s lexicon the manually verified 30K
non-trivial or critical entries. Also, we include the
70K entries in which all of the 4 models (trained
on each of the 4 versions of Google lexicon) unan-
imously agreed. After removing the repeated en-
tries with same grapheme sequence, our lexicon
consists of around 90K entries. In case of repeated
entries having same grapheme sequence, we care-
fully keep only the entry that has been manually
verified.

5.2 G2P Models

We use Neural Sequence to Sequence models. In
these models, a conditional distribution of a se-
quence (here, phoneme sequence) is learned con-
ditioned on another sequence (here, grapheme se-
quence). We train two following Sequence to Se-
quence models on our lexicon for G2P conversion:
LSTM-RNN: This is a plain Sequence to Se-

quence model that incorporates an encoder and
decoder mechanism. Recurrent Neural Network



3196

Figure 1: Categorization of errors in critical cases, here each of the 60K, 40K, 24K, and 12K denotes the model
trained on that particular lexicon.

(RNN) is usually utilized in encoder and decoder
design. For addressing vanishing gradient prob-
lem in RNN, Long-Short Term Memory (LSTM)
(Hochreiter and Schmidhuber, 1997) is used. We
follow Yao and Zweig (2015) for implementation.
Transformer Model: Transformer Model uses

attention mechanism. Attention mechanism pro-
vides improvement upon plain Sequence to Se-
quence by easing the flow of information from
source sequence to destination sequence. We fol-
low Vaswani et al. (2017) for implementation.
We show the performance of both of these

models in Section 6. We observe that Trans-
former Model provides higher token-level accu-
racy (lower Word Error Rate) than LSTM-RNN.

6 Experimental Results

We run extensive simulations and use two mea-
sures for evaluating the performances of the sys-
tems:
Word Error Rate (WER): For calculating

Word Error Rate (WER), we use the following for-
mula:

WER =
E

T

where E denotes the number of words that have
disagreement in their generated phoneme sequence
and reference phoneme sequence, and T denotes

the total number of words.
Phoneme Error Rate (PER): For calculating

Phoneme Error Rate (PER), we use the following
formula:

PER =
I + S +D

T

where I , S, D denote respectively the total num-
ber of insertion, substitution, and deletion opera-
tions needed for all thewords to align the generated
phoneme sequence with the reference phoneme se-
quence for each word. T denotes the total number
of phonemes present in all the words.
Our best performing model is Transformer

Model. We use batch size of 4096. Our neural
network has 3 hidden layers, each containing 256
nodes. We use a computer having 8GB RAM, In-
tel Core i7 CPU, and Nvidia Geforce 1050 GPU
for running all of the simulations. For each model,
we run the simulations for around 110K iterations
taking around 5 hours.

6.1 Performance on Critical Cases
We report the experiment results of Google’s lex-
icon on critical cases in Table 3. We do not re-
port our lexicon here as critical cases are already
included in our lexicon.

6.2 Performance Comparison In General
For comparing the performances of models trained
on our lexicon and Google’s lexicon, we randomly



3197

Error Type 60K 40K 24K 12K
total error 6415 7222 8087 10400
Open Close
Confusion (%)

32.0 30.7 34.8 23.6

Inherent Vowel
Confusion (%)

28.4 36.7 32.0 40.1

s or sh confusion
(%)

14.6 15.3 14.2 12.8

Dipthong confu-
sion (%)

11.6 9.8 7.6 9.4

Other Vowel
Confusion (%)

2.1 1.3 4.2 3.1

s or ch confusion
(%)

0.8 0.7 0.2 0.5

Nasal Confusion
(%)

0.2 0.1 0 0

Other Error (%) 10.4 5.4 7.1 10.6

Table 2: Error classification of 30K critical cases, here
each of the four rightmost columns denotes the model
trained on that particular lexicon.

Lexicon Model WER
(%)

PER
(%)

Google LSTM-RNN 25.7 3.26Transformer
Model

23.6 2.71

Table 3: Performance on Critical Cases

take 9000 entries from our manually verified 30K
critical cases as test set. We use this test set for
evaluating all the models. Though our actual lex-
icon contains these 9000 entries, we do not keep
them in our lexicon while doing the experiments
to fairly evaluate the performances of the lexicons.
For both lexicons, we keep 90% of the lexicon in
train set and remaining 10% in validation set. Ta-
ble 4 shows the result. Models trained on our lexi-
con outperforms those trained on Google’s lexicon
by a significant margin. Moreover, Transformer
Model performs better than LSTM-RNN.
Figure 2 and Table 5 categorize the errors of

systems trained on 3 types of lexicons (Roman-
ized version of our lexicon is discussed in section
6.3) by using Algorithm 2. Here, we report the
results of Transformer Model only as it has been
better performing that LSTM-RNN in our experi-
ments. We observe most of the errors are related to
Open Close vowel, s or sh, Diphthong, and Inher-
ent Vowel confusions - this finding also conforms
to Figure 1 and Table 2, which were error catego-

Lexicon Model WER
(%)

PER
(%)

Google LSTM-RNN 17.1 2.32Transformer
Model

14.8 1.88

Our Lexicon LSTM-RNN 10.5 1.42Transformer
Model

9.8 1.33

Table 4: Performance Comparison In General

Error Type Our
Lexi-
con

Roma-
nized
Lexi-
con

Google
Lexi-
con

Total Error 1337 1406 2961
Inherent Vowel
Confusion (%)

35.6 34.5 33.8

Open Close Con-
fusion (%)

30.1 30.4 27.9

s or sh confusion
(%)

13.3 13.0 12.1

Diphthong
confusion (%)

10.7 9.8 13.1

Other Vowel
Confusion (%)

1.9 4.3 2.7

s or ch confusion
(%)

0.3 0.2 0.2

Nasal Confusion
(%)

0.2 0.43 0.6

Other Error (%) 7.9 7.4 9.7

Table 5: Performance comparison on different error
categories. Here each of the three rightmost columns
denotes the model trained on that particular lexicon.

rization of critical cases.

6.3 Effect of Romanization
For doing experiments on the effect of roman-
ization on G2P conversion, we romanized all
grapheme sequences in our lexicon to prepare ro-
manized counterpart of our lexicon. During ro-
manization, each grapheme symbol in Bangla is
replaced with a single English letter except that if
a consonant grapheme is not followed by a vowel
grapheme, “O” was added after the romanized
symbol of that consonant as the roman symbol for
``অ'', which is usually inherently pronounced in
such cases. All the symbols used for romanization
were completely disjoint to avoid any ambiguity
in the lexicon. Figure 3 shows the WER and PER



3198

Figure 2: Performance Comparison on Different Error Categories

Figure 3: Comparison in terms of WER and PER

of systems trained on 3 types of lexicons. Here,
we report the results of Transformer Model only
as it has been better performing than LSTM-RNN
in our experiments. Both versions of our lexicon
perform better (lower WER and lower PER) than
Google’s lexicon. Also romanization does not sig-
nificantly increase or decrease the performance.
Figures 4 and 5 show respectively the Word

Recognition Accuracy (1−WER) and Phoneme
Recognition Accuracy (1−PER) with respect to
number of iterations run during simulation. Both
versions of our lexicon perform better (higher
Word Recognition Accuracy and higher Phoneme
Recognition Accuracy) than Google’s lexicon.
Figure 6 shows Negative Log Perplexity vs num-
ber of iterations. Both versions of our lexicon pro-
vide higher negative log perplexity than Google’s
lexicon.

Figure 4: Word Recognition Accuracy vs Iteration

6.4 Effectiveness of Our Identified Critical
Cases

In this section, we want to establish that the im-
proved performance of our lexicon comes not only
from the increase in number of training samples,
but also due to the fact that the critical cases identi-
fied by our novel methodology have been added as
training samples. For this, we prepare a new train-
ing lexicon by combining a portion of the Google
lexicon with a portion of our identified critical
cases. As we have kept 9K entries from the crit-
ical cases as our test set, we take the remaining
21K critical cases and combine them with the ran-
domly taken 39K entries from Google lexicon to
prepare a new lexicon of size 60K. While taking
entries from Google lexicon, we ensure that we do
not take any repeated entry that has already been
in the critical cases and added to the new lexicon.



3199

Figure 5: Phoneme Recognition Accuracy vs Iteration

Figure 6: Negative Log Perplexity vs Iteration

Lexicon Model WER
(%)

PER
(%)

Google LSTM-RNN 17.1 2.32Transformer
Model

14.8 1.88

New Lexicon LSTM-RNN 12.6 1.54Transformer
Model

11.2 1.49

Table 6: Effectiveness of critical cases. Both lexicons
are of size 60K. New Lexicon consists of 21K critical
cases and 39K entries from Google lexicon.

We then compare the performance of this new lex-
icon with the Google lexicon, both of which are of
same size (60K), on our test set. The results are in
Table 6. The results clearly show that even in the
case of same sized lexicons, our identified critical
cases can significantly improve the performance as
evidenced by the lower WER and lower PER than
those for the Google lexicon.

7 Conclusion and Future Works

In this paper, we have identified the critical cases,
categorized the errors, and increased the current
state-of-the-art accuracy in G2P conversion for
Bangla language by developing an improved lex-
icon. In future, we will do classification of errors

using unsupervised clustering approaches. Tables
2 and 5 both show that most of the errors occur in
Open Close Vowel, s or sh, Diphthong, and Inher-
ent Vowel confusion categories. In future, we will
focus on each category to devise novel methodol-
ogy for mitigating the errors of that category and
in turn further increase the accuracy of G2P system
in Bangla.

Acknowledgements

This research work is conducted at the De-
partment of Computer Science and Engineering,
Bangladesh University of Engineering and Tech-
nology (BUET) and is supported by Samsung Re-
search.

References
air. 2015. https://svn.code.sf.net/p/
cmusphinx/code/branches/cmudict/
cmudict-0.7b.

Firoj Alam, SM Murtoza Habib, and Mumit Khan.
2011. Bangla text to speech using festival. In Con-
ference on Human Language Technology for Devel-
opment, pages 154–161.

Binoy Barman. 2009. A contrastive analysis of english
and bangla phonemics. Dhaka University Journal of
Linguistics, 2(4):19–42.

Joyanta Basu, Tulika Basu, Mridusmita Mitra, and
Shyamal Kr Das Mandal. 2009. Grapheme to
phoneme (g2p) conversion for bangla. In Speech
Database and Assessments, 2009 Oriental CO-
COSDA International Conference on, pages 66–71.
IEEE.

Maximilian Bisani and Hermann Ney. 2008. Joint-
sequence models for grapheme-to-phoneme conver-
sion. Speech communication, 50(5):434–451.

Rich Caruana. 1997. Multitask learning. Machine
learning, 28(1):41–75.

Stanley F Chen. 2003. Conditional and joint models
for grapheme-to-phoneme conversion. InEighth Eu-
ropean Conference on Speech Communication and
Technology.

Jamil Chowdhury, editor. 2016. Adhunik Bangla Ovid-
han. Bangla Academy, Bangla Academy, Dhaka -
1000.

Shammur Absar Chowdhury, Firoj Alam, Naira Khan,
and Sheak RH Noori. 2017. Bangla grapheme
to phoneme conversion using conditional random
fields. In Computer and Information Technology
(ICCIT), 2017 20th International Conference of,
pages 1–6. IEEE.



3200

Aliya Deri and Kevin Knight. 2016. Grapheme-to-
phoneme models for (almost) any language. In Pro-
ceedings of the 54th Annual Meeting of the Associa-
tion for Computational Linguistics (Volume 1: Long
Papers), volume 1, pages 399–408.

Krishnendu Ghosh, Ramu V Reddy, NP Naren-
dra, S Maity, SG Koolagudi, and KS Rao. 2010.
Grapheme to phoneme conversion in bengali for
festival based tts framework. In 8th international
conference on natural language processing (ICON).
Macmillan Publishers.

Alexander Gutkin, Linne Ha, Martin Jansche, Knot Pi-
patsrisawat, and Richard Sproat. 2016. Tts for low
resource languages: A bangla synthesizer. In LREC.

Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian
Sun. 2016. Deep residual learning for image recog-
nition. In Proceedings of the IEEE conference on
computer vision and pattern recognition, pages 770–
778.

Sepp Hochreiter and Jürgen Schmidhuber. 1997.
Long short-term memory. Neural computation,
9(8):1735–1780.

Sittichai Jiampojamarn, Grzegorz Kondrak, and Tarek
Sherif. 2007. Applying many-to-many alignments
and hiddenmarkovmodels to letter-to-phoneme con-
version. In Human Language Technologies 2007:
The Conference of the North American Chapter of
the Association for Computational Linguistics; Pro-
ceedings of the Main Conference, pages 372–379.

Melvin Johnson, Mike Schuster, Quoc V Le, Maxim
Krikun, Yonghui Wu, Zhifeng Chen, Nikhil Thorat,
Fernanda Viégas, Martin Wattenberg, Greg Corrado,
et al. 2016. Google’s multilingual neural machine
translation system: enabling zero-shot translation.
arXiv preprint arXiv:1611.04558.

Young-Bum Kim and Benjamin Snyder. 2012. Uni-
versal grapheme-to-phoneme prediction over latin
alphabets. In Proceedings of the 2012 Joint Con-
ference on Empirical Methods in Natural Language
Processing and Computational Natural Language
Learning, pages 332–343. Association for Compu-
tational Linguistics.

Franco Mana, Paolo Massimino, and Alberto Pac-
chiotti. 2001. Usingmachine learning techniques for
grapheme to phoneme transcription. In Seventh Eu-
ropean Conference on Speech Communication and
Technology.

Benjamin Milde, Christoph Schmidt, and Joachim
Köhler. 2017. Multitask sequence-to-sequence
models for grapheme-to-phoneme conversion. Proc.
Interspeech 2017, pages 2536–2540.

Ayesha Binte Mosaddeque, Naushad UzZaman, and
Mumit Khan. 2006. Rule based automated pronun-
ciation generator.

J. R. Novak. 2012. https://github.com/
AdolfVonKleist/Phonetisaurus.

Josef R Novak, Nobuaki Minematsu, and Keikichi Hi-
rose. 2013. Failure transitions for joint n-gram mod-
els and g2p conversion. In INTERSPEECH, pages
1821–1825.

Kanishka Rao, Fuchun Peng, Haşim Sak, and Françoise
Beaufays. 2015. Grapheme-to-phoneme conversion
using long short-term memory recurrent neural net-
works. In Acoustics, Speech and Signal Processing
(ICASSP), 2015 IEEE International Conference on,
pages 4225–4229. IEEE.

Carsten Schnober, Steffen Eger, Erik-Lân Do Dinh, and
Iryna Gurevych. 2016. Still not there? comparing
traditional sequence-to-sequence models to encoder-
decoder neural networks on monotone string transla-
tion tasks. arXiv preprint arXiv:1610.07796.

Ilya Sutskever, Oriol Vinyals, and Quoc V Le. 2014.
Sequence to sequence learningwith neural networks.
In Advances in neural information processing sys-
tems, pages 3104–3112.

Ye Kyaw Thu, Win Pa Pa, Yoshinori Sagisaka, and
Naoto Iwahashi. 2016. Comparison of grapheme-
to-phoneme conversion methods on a myanmar pro-
nunciation dictionary. In Proceedings of the 6th
Workshop on South and Southeast Asian Natural
Language Processing (WSSANLP2016), pages 11–
22.

Shubham Toshniwal and Karen Livescu. 2016.
Jointly learning to align and convert graphemes to
phonemes with neural attention models. In Spoken
Language Technology Workshop (SLT), 2016 IEEE,
pages 76–82. IEEE.

Yulia Tsvetkov, Sunayana Sitaram, Manaal Faruqui,
Guillaume Lample, Patrick Littell, David
Mortensen, Alan W Black, Lori Levin, and
Chris Dyer. 2016. Polyglot neural language models:
A case study in cross-lingual phonetic representation
learning. arXiv preprint arXiv:1605.03832.

Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob
Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz
Kaiser, and Illia Polosukhin. 2017. Attention is all
you need. In Advances in Neural Information Pro-
cessing Systems, pages 5998–6008.

Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V
Le, Mohammad Norouzi, Wolfgang Macherey,
Maxim Krikun, Yuan Cao, Qin Gao, Klaus
Macherey, et al. 2016. Google’s neural machine
translation system: Bridging the gap between
human and machine translation. arXiv preprint
arXiv:1609.08144.

Kaisheng Yao and Geoffrey Zweig. 2015. Sequence-
to-sequence neural net models for grapheme-
to-phoneme conversion. arXiv preprint

arXiv:1506.00196.


