



















































Proceedings of the...


D S Sharma, R Sangal and E Sherly. Proc. of the 12th Intl. Conference on Natural Language Processing, pages 59–64,
Trivandrum, India. December 2015. c©2015 NLP Association of India (NLPAI)

Using Word Embeddings for Bilingual Unsupervised WSD

Sudha Bhingardive Dhirendra Singh Rudramurthy V Pushpak Bhattacharyya
Department of Computer Science and Engineering,

Indian Institute of Technology Bombay.
{sudha,dhirendra,rudra,pb}@cse.iitb.ac.in

Abstract

Unsupervised Word Sense Disambigua-
tion (WSD) is one of the challenging prob-
lems in natural language processing. Re-
cently, an unsupervised bilingual WSD ap-
proach has been proposed. This approach
uses context aware EM formulation for es-
timating the sense distribution by using
the co-occurrence counts of cross-linked
words in comparable corpora. WordNet-
based similarity measures are used for ap-
proximating the co-occurrence counts. In
this paper, we explore the feasibility of
the use of Word Embeddings for approx-
imating these counts, which is an exten-
sion to the existing approach. We evalu-
ated our approach for Hindi-Marathi lan-
guage pair, on Health domain. On us-
ing the combination of Word Embeddings
and WordNet-based similarity measures,
we observed 8.5% and 2.5% improvement
in the F-score of verbs and adjectives re-
spectively for Marathi and 7% improve-
ment in the F-score of adjectives for Hindi.
The experiments show that the combina-
tion of Word Embeddings and WordNet-
based similarity measures is a reasonable
approximation for the bilingual WSD.

1 Introduction

One of the well known research area in the field of
Natural Language Processing (NLP) is the word
sense disambiguation. Over the years, various
WSD algorithms are proposed. These algorithms
come under two broad categories, viz., Knowledge
based and Machine Learning based. Knowledge
based approaches rely on various lexical knowl-
edge resources like machine readable dictionaries,
ontologies, WordNets etc. Machine learning based
approaches are further classified as supervised,

semi-supervised and unsupervised. Supervised
WSD approaches (Lee et al., 2004; Ng and Lee,
1996) always perform better because of the avail-
ability of the sense-annotated data. However, the
cost of creation of the sense-annotated data limits
their applicability to only a few resource rich lan-
guages. On the other hand, semi-supervised ap-
proaches (Yarowsky, 1995; Khapra et al., 2010)
provide a fine balance in terms of resource re-
quirements and accuracy, but they still rely on
some amount of sense-annotated data. There-
fore, despite of the less accuracy, much focus
is given for unsupervised WSD algorithms (Diab
and Resnik, 2002; Kaji and Morimoto, 2002; Mi-
halcea et al., 2004; Jean, 2004; Khapra et al.,
2011). These algorithms do not need any sense-
annotated data for the disambiguation. Moreover,
they make use of lexical knowledge resources or
comparable/parallel corpora for training the al-
gorithm (Kaji and Morimoto, 2002; Diab and
Resnik, 2002; Specia et al., 2005; Lefever and
Hoste, 2010; Khapra et al., 2011).

Khapra et al. (2011) have shown that how two
resource deprived languages can help each other
in WSD without using any sense-annotated data
in either of the languages. Here, the intuition is
that, the sense distribution remains same across
languages when the comparable corpora is pro-
vided. They used the Expectation Maximization
(EM) based formulation for estimating the sense
distribution of words. Further, Bhingardive et al.
(2013) extended this approach and hypothesized
that, the co-occurrence sense distribution also re-
mains same across languages, given the compara-
ble corpora. Since, the co-occurrence counts re-
quire a large corpora, they approximate the co-
occurrence counts using WordNet-based similar-
ity measures. An improvement of 17% - 35% in
the F-Score of verbs was observed while the F-
Score was comparable for other POS categories.

In this paper, we propose to explore the use of59



distributional similarity measures as an approxi-
mation to the co-occurrence counts in Bhingardive
et al. (2013) approach. We used the cosine dis-
tance between the word embeddings of the words
as a similarity measure. These word embeddings
are obtained from a large monolingual corpus.

The roadmap of the paper is as follows. Section
2 covers the background work on Bilingual EM
Our extension of the Bilingual EM using distribu-
tional similarity is explained in Section 3. Section
4 gives detail about the experimental setup. Re-
sults are presented in section 5. Section 6 covers
discussion on the results. Related work is given
in section 7. Conclusion and future work are pre-
sented in section 8.

2 Bilingual EM using WordNet-based
Similarity

Bhingardive et al. (2013) extended the bilingual
EM approach (Khapra et al., 2011) and observed
that adding contextual information further helps
in the disambiguation process. Original bilingual
EM approach estimates the sense distribution
in one language by using the raw counts of the
cross-linked words from the other language using
EM algorithm. Bhingardive et al. (2013) modified
this approach by replacing the raw counts of the
words with the co-occurrence counts of the target
word and the context words. They approximated
the co-occurrence counts by using WordNet based
similarity measures to avoid the data sparsity.
The modified EM formulation with context
information is as follows:

E-Step:

P (SL1k |u, a) =

∑

v,b

P (πL2 (S
L1
k )|v, b) · simi(v, b)

∑

S
L1
i

∑

x,b

P (πL2 (S
L1
i )|x, b) · simi(x, b)

where, SL1i , S
L1
k ∈ synsetsL1 (u)
a ∈ context(u)
v ∈ crosslinksL2 (u, S

L1
k )

b ∈ crosslinksL2 (a)
x ∈ crosslinksL2 (u, S

L1
i )

Here, u is the target word to be disambiguated, a
is the context word, πL2 (S

L1
k ) means the linked

synset of the sense SL1k in L2. simi(x, b) is the
WordNet based similarity over all senses of words

x and b. crosslinksL2 (u, S
L1
j ) is the set of possi-

ble translations of the word ‘u’ from language L1
to L2 in the sense SL1j . crosslinksL2 (a) is the set
of all possible translations of the word ‘a’ from
L1 to L2 in all its senses.

M-Step:

P (SL2j |v, b) =

∑

u,a

P (πL1 (S
L2
j )|u, a) · simi(u, a)

∑

S
L2
i

∑

y,b

P (πL1 (S
L2
i )|y, a) · simi(y, a)

where, SL2i , S
L2
j ∈ synsetsL2 (v)
b ∈ context(v)
u ∈ crosslinksL1 (v, S

L2
j )

a ∈ crosslinksL1(b)
y ∈ crosslinksL1 (v, S

L2
i )

where, simi(y, a) is the WordNet based similarity
over all senses of words y and a.

Here, given a target word and its context in one
language, the probability of various senses of the
target word is calculated in that particular con-
text using their cross-links information from other
language. Synset aligned multilingual dictionary
(Mohanty et al., 2008) is used to find the cross-
links of the target word and its context words in
other language. The probability of the sense of
the target word given its context is estimated by
the modified EM formulation as mentioned ear-
lier. The maximum similarity over all senses of
the target word is chosen as the sense of the target
word. In this way, given the bilingual compara-
ble corpora and the synset aligned dictionary, con-
text aware EM formulation is used to estimate the
sense distributions in both the languages.

3 Our approach: Bilingual EM using
Distributional Similarity

Continuous word embeddings have recently
gained popularity in various NLP tasks like POS
Tagging, Named Entity Recognition, Semantic
Role Labeling, Sentiment Analysis, etc. (Col-
lobert et al., 2011; Tang et al., 2014). Word em-
beddings have shown to capture the syntactic and
semantic information about a word. In our ap-
proach, we look forward to use these word em-
beddings for the bilingual WSD and compare the
results with the existing approaches.60



WSD Algorithm HIN-HEALTH
NOUN ADV ADJ VERB Overall

EM-C-DistSimi+WnSimi 59.32 68.98 63.18 60.02 60.94
EM-C-DistSimi 59.59 69.20 63.87 55.73 61.09
EM-C-WnSimi 59.82 67.80 56.66 60.38 59.63
EM 60.68 67.48 55.54 25.29 58.16
WFS 53.49 73.24 55.16 38.64 54.46
RB 32.52 45.08 35.42 17.93 33.31

Table 1: Comparison(F-Score) of our approach (EM-C-DistSimi-WnSimi and EM-C-DistSimi) with
other WSD algorithms on Hindi-Health domain

WSD Algorithm MAR-HEALTH
NOUN ADV ADJ VERB Overall

EM-C-DistSimi+WnSimi 62.75 61.19 56.22 60.99 61.30
EM-C-DistSimi 63.09 61.82 55.60 43.69 58.92
EM-C-WnSimi 62.90 62.54 53.63 52.49 59.77
EM 63.88 58.88 55.71 35.60 58.03
WFS 59.35 67.32 38.12 34.91 52.57
RB 33.83 38.76 37.68 18.49 32.45

Table 2: Comparison(F-Score) of our approach (EM-C-DistSimi-WnSimi and EM-C-DistSimi) with
other WSD algorithms on Marathi-Health domain

Our formulation is based on Bhingardive et al.
(2013) formulation, where we use distributional
similarity measures along with WordNet based
similarity measures for finding the sense distribu-
tion. As shown previously, in E-step and M-step,
simi(v, b), simi(x, b), simi(u, a) and simi(y, a)
are computed as the distributional similarities (co-
sine distance) calculated from the large untagged
text.

4 Experimental setup

In this section, we describe various datasets used
in our experiments. We discuss how we obtained
word embeddings and evaluated their quality.

4.1 Datasets used for WSD
In our experiments, we used the same corpus as
used by Khapra et al. (2011) . This corpus is pub-
licly available1 for Health domain.

4.2 Training Word Embeddings
The word embeddings were obtained using
word2vec2 tool (Mikolov et al., 2013). This tool
provides two broad techniques for creating word
embeddings : Continuous Bag of Words (CBOW)

1http://www.cfilt.iitb.ac.in/wsd/annotated corpus/
2https://code.google.com/p/word2vec/

and Skip-gram models. CBOW model predicts
the current word based on the surrounding con-
text whereas, the Skip-gram model tries to max-
imize the probability of seeing the context word
given the word under consideration (Mikolov et
al., 2013).

We have used the most widely used hyperpa-
rameter settings for training word embeddings.
The Skip-gram model is used with 300 dimensions
along with the window size equal to 5 (i.e. w = 5).

Word Embeddings for Hindi
For obtaining the word embeddings for Hindi, we
used Bojar et al. (2014) corpus. This corpus con-
tains around 812.6 million words along with POS
and lemma information. We have trained the word
embeddings using the lemmatized version of the
corpus.

Word Embeddings for Marathi
Marathi corpus was collected from various re-
sources like Web & Wikipedia dumps3, Leipzig
corpus4 , Newspaper corpus from Maharashtra
Times 5 & e-Sakal, 6 etc. This corpus contains

3http://ufal.mff.cuni.cz/ majlis/w2c/download.html
4http://corpora.uni-leipzig.de/
5http://maharashtratimes.indiatimes.com/
6http://online4.esakal.com/61



approximately 26.3 million words. Marathi word
embeddings are trained on this corpus using the
same parameters as used for Hindi.

4.3 Evaluating the quality of Word
Embeddings

For evaluating the quality of both Hindi and
Marathi word embeddings, we have created a sim-
ilarity word pair dataset by translating the stan-
dard similarity word pair dataset (Finkelstein et
al., 2001) available for English. Three annotators
were instructed to give the score for each word-
pair based on the semantic similarity and relat-
edness. The scale was chosen between 0 - 10.
The least similar word-pair was given a score of
0, while the most similar word-pair was given
a score of 10. We calculated the average inter-
annotator agreement using Spearman’s correlation
coefficient. The embeddings giving the best Pear-
son’s correlation coefficient was used in our exper-
iments.

4.4 WSD Experiments

We performed WSD experiments on all content
words. The entire sentence was considered as the
context for the word to be disambiguated. Experi-
ments are performed on Hindi-Marathi health do-
main corpus.

The F-Score of the following WSD algorithms
are reported.

Random Baseline [RB]
This algorithm assigns the senses randomly to the
words to be disambiguated.

Wordnet First Sense [WFS]
WFS baseline assigns the first listed sense in the
WordNet to the word irrespective of its context.

Basic EM [EM]
This is basic EM approach by Khapra et al., (2011)
which estimates the sense probability of a word in
one language by using the raw counts of its cross-
linked words in another language.

EM-C-WnSimi
This is an extended EM approach where Bhingar-
dive et al. (2013) modified the basic EM formula-
tion by adding the contextual information and us-
ing the WordNet based similarity for approximat-
ing the co-occurrence counts.

EM-C-DistSimi
This is our approach where we modify the for-
mulation of Bhingardive et al. (2013) using the
distributional similarity measure for estimating the
sense distributions.

EM-C-DistSimi-WnSimi
This is also our approach where we use combina-
tion of distributional and WordNet similarity for
estimating the sense distributions.

5 Results

In this section, we discuss our results and compare
it with other WSD approaches. Table 1 and Table 2
show the results of our approach on Hindi-Health
and Marathi-Health domain respectively. Results
are given in terms of F-score. EM-C-DitSimi-
WnSimi and EM-C-DistSimi achieves better re-
sult as compared to EM-C-WnSimi and EM. Us-
ing EM-C-DistSimi-WnSimi approach, verb accu-
racy increases at the level 8.5% for Marathi and for
Hindi, it is very close to the existing approaches.
The adjective accuracy also improved by 7% for
Hindi and 2.5% for Marathi. Results for noun and
adverb are observed very close to the existing ap-
proaches. The overall F-Score obtained is compa-
rable. The results show that word embeddings can
be used as an approximation along with WordNet-
Based similarity measures for bilingual WSD.

6 Discussion

6.1 Poor performance for verbs using Word
Embeddings

It has been observed that if we use only distribu-
tional measure (EM-C-DistSimi) as an approxima-
tion then we get significant performance except for
verbs. We believe the reason that the word em-
beddings of verbs fail to capture the semantic in-
formation resulting in poor performance. There-
fore, the word embeddings of verb fails in finding
out relevant context words and choose its correct
sense. But if we use the combination of distri-
butional similarity and wordnet similarity then we
get better results for the same.

6.2 Misleading contexts

In our approach, we consider the entire sentence
as the context for performing WSD. As a result,
we end up choosing many context words which
causes topic drift. The approach needs to be care-62



ful while selecting the context words for the dis-
ambiguation task.

7 Related work

Recently, several unsupervised WSD algorithms
have been proposed. McCarthy et. al (2004)
used distributional methods for finding the con-
text clues for unsupervised most frequent sense
detection. They have shown that MFS can be de-
tected without the need of any sense tagged cor-
pora. Only untagged text is used for finding the
predominant senses of words. Parallel or com-
parable corpora have also been explored for un-
superwised WSD (Diab and Resnik, 2002; Kaji
and Morimoto, 2002; Mihalcea et al., 2004; Jean,
2004; Khapra et al., 2011). Chen et. al (2014)
have presented a unified model which focused on
creating sense representations using word embed-
dings and used the same for the disambiguation
purpose.

8 Conclusion and Future Work

We explored the usefulness of word embeddings
from a bilingual WSD perspective. We used the
distributional similarity measure as an approxi-
mation to the co-occurrence counts in bilingual
EM Context based WSD. We found that the word
embeddings along with wordnet similarity mea-
sure are a reasonable approximation to the simple
co-occurrence counts. We also observed that the
word embeddings for verbs fail to capture the rel-
evant semantic information. Much focus is needed
on getting the good quality word embeddings for
verbs. We would also like to explore the strategies
for choosing the most informative context words
for disambiguation depending on the POS cate-
gory of the word.

References
Sudha Bhingardive, Samiulla Shaikh, and Pushpak

Bhattacharyya. 2013. Neighbors help: Bilingual
unsupervised wsd using context. In ACL (2), pages
538–542. The Association for Computer Linguis-
tics.

Ondrej Bojar, Vojtěch Diatka, Pavel Rychlý, Pavel
Stranak, Vit Suchomel, Aleš Tamchyna, and Daniel
Zeman. 2014. Hindencorp - hindi-english and
hindi-only corpus for machine translation. In Nico-
letta Calzolari (Conference Chair), Khalid Choukri,
Thierry Declerck, Hrafn Loftsson, Bente Maegaard,
Joseph Mariani, Asuncion Moreno, Jan Odijk, and
Stelios Piperidis, editors, Proceedings of the Ninth

International Conference on Language Resources
and Evaluation (LREC’14), Reykjavik, Iceland,
may. European Language Resources Association
(ELRA).

Xinxiong Chen, Zhiyuan Liu, and Maosong Sun. 2014.
A unified model for word sense representation and
disambiguation. In Proceedings of the 2014 Con-
ference on Empirical Methods in Natural Language
Processing (EMNLP), pages 1025–1035. Associa-
tion for Computational Linguistics.

Ronan Collobert, Jason Weston, Léon Bottou, Michael
Karlen, Koray Kavukcuoglu, and Pavel Kuksa.
2011. Natural language processing (almost) from
scratch. J. Mach. Learn. Res., 12:2493–2537,
November.

Mona Diab and Philip Resnik. 2002. An unsupervised
method for word sense tagging using parallel cor-
pora. In Proceedings of the 40th Annual Meeting
on Association for Computational Linguistics, ACL
’02, pages 255–262, Morristown, NJ, USA. Associ-
ation for Computational Linguistics.

Lev Finkelstein, Evgeniy Gabrilovich, Yossi Matias,
Ehud Rivlin, Zach Solan, Gadi Wolfman, and Ey-
tan Ruppin. 2001. Placing search in context: The
concept revisited. ACM Trans. Inf. Syst.

Véronis Jean. 2004. Hyperlex: Lexical cartography
for information retrieval. In Computer Speech and
Language, pages 18(3):223–252.

Hiroyuki Kaji and Yasutsugu Morimoto. 2002. Unsu-
pervised word sense disambiguation using bilingual
comparable corpora. In Proceedings of the 19th in-
ternational conference on Computational linguistics
- Volume 1, COLING ’02, pages 1–7, Stroudsburg,
PA, USA. Association for Computational Linguis-
tics.

Mitesh M. Khapra, Anup Kulkarni, Saurabh Sohoney,
and Pushpak Bhattacharyya. 2010. All words do-
main adapted wsd: Finding a middle ground be-
tween supervision and unsupervision. In Jan Ha-
jic, Sandra Carberry, and Stephen Clark, editors,
ACL, pages 1532–1541. The Association for Com-
puter Linguistics.

Mitesh M Khapra, Salil Joshi, and Pushpak Bhat-
tacharyya. 2011. It takes two to tango: A bilingual
unsupervised approach for estimating sense distribu-
tions using expectation maximization. In Proceed-
ings of 5th International Joint Conference on Nat-
ural Language Processing, pages 695–704, Chiang
Mai, Thailand, November. Asian Federation of Nat-
ural Language Processing.

K. Yoong Lee, Hwee T. Ng, and Tee K. Chia. 2004.
Supervised word sense disambiguation with support
vector machines and multiple knowledge sources.
In Proceedings of Senseval-3: Third International
Workshop on the Evaluation of Systems for the Se-
mantic Analysis of Text, pages 137–140.63



Els Lefever and Veronique Hoste. 2010. Semeval-
2010 task 3: cross-lingual word sense disambigua-
tion. In Katrin Erk and Carlo Strapparava, editors,
SemEval 2010 : 5th International workshop on Se-
mantic Evaluation : proceedings of the workshop,
pages 15–20. ACL.

Diana Mccarthy, Rob Koeling, Julie Weeds, and John
Carroll. 2004. Finding predominant word senses in
untagged text. In In Proceedings of the 42nd An-
nual Meeting of the Association for Computational
Linguistics, pages 280–287.

Rada Mihalcea, Paul Tarau, and Elizabeth Figa. 2004.
Pagerank on semantic networks, with application to
word sense disambiguation. In COLING.

Tomas Mikolov, Kai Chen, Greg Corrado, and Jef-
frey Dean. 2013. Efficient estimation of word
representations in vector space. arXiv preprint
arXiv:1301.3781.

Rajat Mohanty, Pushpak Bhattacharyya, Prabhakar
Pande, Shraddha Kalele, Mitesh Khapra, and Aditya
Sharma. 2008. Synset based multilingual dic-
tionary: Insights, applications and challenges. In
Global Wordnet Conference.

Hwee Tou Ng and Hian Beng Lee. 1996. Integrating
multiple knowledge sources to disambiguate word
sense: an exemplar-based approach. In Proceedings
of the 34th annual meeting on Association for Com-
putational Linguistics, pages 40–47, Morristown,
NJ, USA. ACL.

Lucia Specia, Maria Das Graças, Volpe Nunes, and
Mark Stevenson. 2005. Exploiting parallel texts to
produce a multilingual sense tagged corpus for word
sense disambiguation. In In Proceedings of RANLP-
05, Borovets, pages 525–531.

Duyu Tang, Furu Wei, Nan Yang, Ming Zhou, Ting
Liu, and Bing Qin. 2014. Learning sentiment-
specific word embedding for twitter sentiment clas-
sification. In Proceedings of the 52nd Annual Meet-
ing of the Association for Computational Linguistics
(Volume 1: Long Papers), pages 1555–1565. Asso-
ciation for Computational Linguistics.

David Yarowsky. 1995. Unsupervised word sense dis-
ambiguation rivaling supervised methods. In Pro-
ceedings of the 33rd Annual Meeting on Association
for Computational Linguistics, ACL ’95, pages 189–
196, Stroudsburg, PA, USA. Association for Com-
putational Linguistics.

64


