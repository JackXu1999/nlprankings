



















































Joint Learning with Global Inference for Comment Classification in Community Question Answering


Proceedings of NAACL-HLT 2016, pages 703–713,
San Diego, California, June 12-17, 2016. c©2016 Association for Computational Linguistics

Joint Learning with Global Inference
for Comment Classification in Community Question Answering

Shafiq Joty, Lluı́s Màrquez and Preslav Nakov
ALT Research Group

Qatar Computing Research Institute — HBKU, Qatar Foundation
{sjoty,lmarquez,pnakov}@qf.org.qa

Abstract

This paper addresses the problem of comment
classification in community Question Answer-
ing. Following the state of the art, we ap-
proach the task with a global inference pro-
cess to exploit the information of all com-
ments in the answer-thread in the form of a
fully connected graph. Our contribution com-
prises two novel joint learning models that are
on-line and integrate inference within learn-
ing. The first one jointly learns two node- and
edge-level MaxEnt classifiers with stochastic
gradient descent and integrates the inference
step with loopy belief propagation. The sec-
ond model is an instance of fully connected
pairwise CRFs (FCCRF). The FCCRF model
significantly outperforms all other approaches
and yields the best results on the task to date.
Crucial elements for its success are the global
normalization and an Ising-like edge potential.

1 Introduction

Online community fora have been gaining a lot of
popularity in recent years. Many of them, such as
Stack Exchange1, are quite open, allowing anybody
to ask and anybody to answer a question, which
makes them very valuable sources of information.
Yet, this same democratic nature resulted in some
questions accumulating a large number of answers,
many of which are of low quality. While nowa-
days online fora are typically searched using stan-
dard search engines that index entire threads, this is
not optimal, as it can be very time-consuming for a
user to go through and make sense of a long thread.

1http://stackexchange.com/

Q: hello guys and gals..could anyone of u knows where
to buy a good and originals RC helicopters and toy
guns here in qatar..im longin for this toys but its
nowhere to find.. thanks

A1 Go to Doha city center you may get it at 4 floor.
Local: Good, Human: Good

A2 “Hobby Shop” in City center has these toys with orig-
inal motors. They are super cool.. U will love that
shop..and will definetly buy one :) Have fun :)
Local: Good, Human: Good

A3 IM selling all my rc nitro helicopters. call me at
5285113.. (1)TREX 600 new/ (1) TREX500 (1)
SHUTTLERG (1) FUTABA ... [truncated]
Local: Good, Human: Bad

A4 Hobby Shop- City Centre
Local: Bad, Human: Good

A5 OMG!! :— Guns and helicopters??!!
Local: Good, Human: Bad

A6 Speed Marine- Salwa Road I think these guys r the
best in town...
Local: Good, Human: Good

A7 City center, i’ve seen wonderful collection.. Its some
wer besides the kids fun place..
Local: Bad, Human: Good

A8 try the shop in city center. they have many RC toys
for sale there. and for the toy guns, in your talking
baout airsoft i think its prohibited here. good luck
Local: Good, Human: Good

Figure 1: Example answer-thread with human an-
notations and automatic predictions by a local clas-
sifier at the comment level.

703



Thus, the creation of automatic systems for Com-
munity Question Answering (cQA), which could
provide efficient and effective ways to find good an-
swers in a forum, has received a lot of research atten-
tion recently (Duan et al., 2008; Li and Manandhar,
2011; dos Santos et al., 2015; Zhou et al., 2015a;
Wang and Ittycheriah, 2015; Tan et al., 2015; Feng
et al., 2015; Nicosia et al., 2015; Barrón-Cedeño et
al., 2015; Joty et al., 2015). There have been also re-
lated shared tasks at SemEval-20152 and SemEval-
20163 (Nakov et al., 2015; Nakov et al., 2016).

In this paper, we focus on the particular problem
of classifying comments in the answer-thread of a
given question as good or bad answers. Figure 1
presents an excerpt of a real example from the Qatar-
Living dataset from SemEval-2015 Task 3. There is
a question on top (Q) followed by eight comments
(A1, A2, · · · , A8). According to the human annota-
tions (‘Human’), all comments but 3 and 5 are good
answers to Q. The comments also contain the predic-
tions of a good-vs-bad binary classifier trained with
state-of-the-art features on this dataset (Nicosia et
al., 2015); its errors are highlighted in red. Many
comments are short, making it difficult for the clas-
sifier to make the right decisions, but some errors
could be corrected using information in the other
comments. For instance, comments 4 and 7 are sim-
ilar to each other, but also to comments 2 and 8
(‘Hobby shop’, ‘City Center’, etc.). It seems rea-
sonable to think that similar comments should have
the same labels, so comments 2, 4, 7 and 8 should
all be labeled consistently as good comments.

Indeed, recent work has shown the benefit of us-
ing varied thread-level information for answer clas-
sification, either by developing features modeling
the thread structure and dialogue (Barrón-Cedeño et
al., 2015), or by applying global inference mecha-
nisms at the thread level using the predictions of lo-
cal classifiers (Joty et al., 2015). We follow the sec-
ond approach, assuming a graph representation of
the answer-thread, where nodes are comments and
edges represent pairwise (similarity) relations be-
tween them. Classification decisions are at the level
of nodes and edges, and global inference is used to
get the best label assignment to all comments.

2http://alt.qcri.org/semeval2015/task3/
3http://alt.qcri.org/semeval2016/task3/

Our main contribution is to propose online mod-
els for learning the decisions jointly, incorporat-
ing the inference inside the joint learning algo-
rithm. Building on the ideas from papers coupling
learning and inference for NLP structure prediction
problems (Punyakanok et al., 2005; Carreras et al.,
2005), we propose joint learning of two MaxEnt
classifiers with stochastic gradient descent, integrat-
ing global inference based on loopy belief propaga-
tion. We also propose a joint model with global nor-
malization, that is an instance of Fully Connected
Conditional Random Fields (Murphy, 2012). We
compare our joint models with the previous state of
the art for the comment classification problem. We
find that the coupled learning-and-inference model
is not competitive, probably due to the label bias
problem. On the contrary, the fully connected CRF
model improves results significantly over all rivaling
models, yielding the best results on the task to date.

In the remainder of this paper, after discussing re-
lated work in Section 2, we introduce our joint mod-
els in Section 3. We then describe our experimental
settings in Section 4. The experiments and analy-
sis of results are presented in Section 5. Finally, we
summarize our contributions with future directions
in Section 6.

2 Related Work

The idea of using global inference based on lo-
cally learned classifiers has been tried in various
settings. In the family of graph-based inference,
Pang and Lee (2004) used local classification scores
with proximity information as edge weights in a
graph-cut inference to collectively identify subjec-
tive sentences in a review. Thomas et al. (2006) used
the same framework to classify congressional tran-
scribed speeches. They applied a classifier to pro-
vide edge weights that reflect the degree of agree-
ment between speakers. Burfoot et al. (2011) ex-
tended the framework by including other inference
algorithms such as loopy belief propagation and
mean-field.

“Learning and inference under structural and lin-
guistic constraints” (Roth and Yih, 2004) is a frame-
work to combine the predictions of local classifiers
in a global decision process solved by Integer Linear
Programming.

704



The framework has been applied to many NLP
structure prediction problems, including shallow
parsing (Punyakanok and Roth, 2000), semantic role
labeling (Punyakanok et al., 2004), and joint learn-
ing of entities and relations (Roth and Yih, 2004).
Further work explored the possibility of coupling
learning and inference in the previous setting. For
instance, Carreras et al. (2005) presented a model
for parsing that jointly trains several local decisions
with a perceptron-like algorithm that gets feedback
after inference. Punyakanok et al. (2005) studied
empirically and theoretically the cases in which this
inference-based learning strategy is superior to the
decoupled approach.

On the particular problem of comment classifica-
tion in cQA, we find some work exploiting thread-
level information. Hou et al. (2015) used features
about the position of the comment in the thread.
Barrón-Cedeño et al. (2015) developed more elab-
orated global features to model thread structure and
the interaction among users. Other work exploited
global inference algorithms at the thread-level. For
instance, (Zhou et al., 2015c; Zhou et al., 2015b;
Barrón-Cedeño et al., 2015) treated the task as se-
quential classification, using a variety of machine
learning algorithms to label the sequence of time-
sorted comments: LSTMs, CRFs, SVMhmm, etc.
Finally, Joty et al. (2015) showed that exploiting the
pairwise relations between comments (at any dis-
tance) is more effective than the sequential informa-
tion. Their results are the best on this task to date.
In this paper, we assume the same setting (cf. Sec-
tion 3) and we experiment with new models to do
learning jointly with inference in the same manner
as in (Punyakanok et al., 2005), and also using fully
connected pairwise CRFs.

3 Our Model

Given a forum question Q and a thread of answers
T = {A1, A2, · · · , An}, the task is to classify each
answer Ai in the thread into one of K possible
classes based on its relevance to the question. We
represent each thread as a fully-connected graph,
where each node represents an answer in the thread.

Given this setting, there exist at least three funda-
mentally different approaches to learn classification
functions.

First, the traditional approach of learning a local
classifier ignoring the structure in the output and us-
ing it to predict the label of each node Ai separately.
This approach only considers correlations between
the label of Ai and features extracted from Ai.

The second approach, adopted by Joty et al.
(2015), is to first learn two local classifiers sepa-
rately: (i) a node-level classifier to predict the label
for each individual node, and (ii) an edge-level clas-
sifier to predict whether the two nodes connected by
an edge should have the same label or not (assuming
a fully connected graph). The predictions of the lo-
cal classifiers are then used in a global inference al-
gorithm (e.g., graph-cut) to perform collective clas-
sification by maintaining structural constraints in the
output. There are two issues with this model: (i) the
local classifiers are trained separately; (ii) by decou-
pling learning from inference, this approach can lead
to suboptimal solutions, as Punyakanok et al. (2005)
pointed out.

The third approach, which we adopt in this pa-
per, is to model the dependencies between the out-
put variables while learning the classification func-
tions jointly by optimizing a global performance cri-
terion. The dependencies are captured using node-
level and edge-level factors defined over a fully con-
nected graph. The idea is that incorporating struc-
tural constraints in the form of all-pair relations dur-
ing training can yield a better solution that directly
optimizes an objective function for the target task.

Before we present our models in subsections 3.1
and 3.2, let us first introduce the notation that we
will use. Each thread T = {A1, A2, · · · , An} is
represented by a complete graph G = (V,E). Each
node i ∈ V in the graph is associated with an input
vector xi, which represents the features of an an-
swer Ai, and an output variable yi ∈ {1, 2, · · · ,K},
representing the class label. Similarly, each edge
(i, j) ∈ E is associated with an input feature vector
φ(xi,xj), derived from the node-level features, and
an output variable yi,j ∈ {1, 2, · · · , L}, representing
the labels for the pair of nodes. We use ψn(yi|xi,v)
and ψe(yi,j |φ(xi,xj),w) to denote the node-level
and the edge-level classification functions, respec-
tively. We call ψn and ψe factors, which can be ei-
ther normalized (e.g., probabilities) or unnormalized
quantities. The model parameters θ = [v,w] are to
be learned during training.

705



3.1 Joint Learning of Two Classifiers with
Global Thread-Level Inference

Our aim is to train the local classifiers so that they
produce correct global classification. To this end, in
our first model we train the node- and the edge-level
classifiers jointly based on global feedback provided
by a global inference algorithm. The global feed-
back determines how much to adjust the local classi-
fiers so that the classifiers and the inference together
produce the desired result. We use log-linear models
(aka maximum entropy) for both classifiers:

ψn(yi = k|xi,v) = exp(v
T
k xi)∑K

k′=1 exp(v
T
k xi)

(1)

ψe(yi,j = l|φ(xi,xj),w)= exp(w
T
l φ(xi,xj))∑L

l′=1 exp(w
T
l′φ(xi,xj))

(2)

The log likelihood (LL) for one data point (x,y)
(i.e., a thread) can be written as follows:

f(θ) =
∑
i∈V

K∑
k=1

yki
[
vTk xi − logZ(v,xi)

]
+

∑
(i,j)∈E

L∑
l=1

yli,j
[
wTl φ(xi,xj)− logZ(w,xi,xj)

]
(3)

where yki and y
l
i,j are the gold labels for i-th node

and (i, j)-th edge expressed in 1-of-K (or 1-of-L)
encoding, respectively, and Z(·) terms are the local
normalization constants.

We give a pseudocode in Algorithm 1 that trains
this model in an online fashion using feedback from
the loopy belief propagation (LBP) inference algo-
rithm (to be described later in Section 3.1.1). Specif-
ically, the marginals from the LBP are used in a
stochastic gradient descent (SGD) algorithm, which
has the following (minibatch) update rule:

θt+1 = θt − ηt 1
N
f ′(θt) (4)

where θt and ηt are the model parameters and the
learning rate at step t, respectively, and 1N f

′(θt) is
the mean gradient for the minibatch (a thread). For
our maximum entropy models, the gradients become

f ′(v) =
∑
i∈V

[βn(yi)− yi] .xi (5)

f ′(w) =
∑

(i,j)∈E
[βe(yi,j)− yi,j ] .φ(xi,xj) (6)

Algorithm 1: Joint learning of local classifiers
with global thread-level inference

1. Initialize the model parameters v and w;
2. repeat

for each thread G = (V,E) do
a. Compute node and edge probabilities
ψn(yi|xi,v) and ψe(yi,j |φ(xi,xj),w);
b. Infer node and edge marginals βn(yi)
and βe(yi,j) using sum-product LBP;
c. Update: v = v − η|V |f ′(v);
d. Update: w = w − η|E|f ′(w);

end
until convergence;

In the above equations, β and y are the marginals
and the gold labels, respectively.

Note that when applying the model to the test
threads, we need to perform the same global infer-
ence to get the best label assignments.

3.1.1 Inference Using Belief Propagation
Belief Propagation or BP (Pearl, 1988) is a mes-

sage passing algorithm for inference in probabilis-
tic graphical models. It supports (i) sum-product,
to compute the marginal distribution for each un-
observed variable, i.e., p(yi|x, θ); and (ii) max-
product, to compute the most likely label configu-
ration, i.e., argmaxy p(y|x, θ). We describe here
the variant that operates on undirected graphs (aka
Markov random fields) with pairwise factors, which
uses the following equations:

µi→j(yj) =
∑
yi

ψn(yi)ψe(yi,j)
∏

k∈N(i)\j
µk→i(yi) (7)

βn(yi) ≈ ψn(yi)
∏

j∈N(i)
µj→i(yi) (8)

where µi→j is a message from node i to node j,
N(i) are the nodes neighbouring i, and ψn(yi) and
ψe(yi,j) are the node and the edge factors.

The algorithm proceeds by sending messages on
each edge until the node beliefs βn(yi) stabilize.
The edge beliefs can be written as follows:

βe(yi,j) ≈ ψe(yi,j)× µi→j(yi)× µj→i(yj) (9)

706



y2 

y3 

y1 

x1 

x3 

x2 

(a) Locally normalized joint model

y2 

y3 

y1 

x1 

x3 

x2 

(b) Globally normalized joint model

Figure 2: Graphical representation of our two joint models: (a) a joint model with locally normalized factors;
(b) a joint model with global normalization, i.e., a fully connected conditional random field.

The node and the edge marginals are then com-
puted by normalizing the node and the edge beliefs,
respectively. By replacing the summation with a
max operation in Equation 7, we can get the most
likely label configuration (i.e., argmax over labels).

BP is guaranteed to converge to an exact solution
if the graph is a tree. However, exact inference is in-
tractable for general graphs, i.e., graphs with loops.
Despite this, it has been advocated by Pearl (1988) to
use BP in loopy graphs as an approximation scheme;
see also (Murphy, 2012), page 768. The algorithm is
then called “loopy” BP, or LBP. Although LBP gives
approximate solutions for general graphs, it often
works well in practice (Murphy et al., 1999), outper-
forming other methods such as mean field (Weiss,
2001) and graph-cut (Burfoot et al., 2011).

It is important to mention that the approach
presented above (i.e., subsection 3.1) is similar
in spirit to the approach of Collins (2002), Car-
reras and Màrquez (2003) and Punyakanok et al.
(2005). The main difference is that they use a
Perceptron-like online algorithm, where the up-
dates are done based on the best label configuration
(i.e., argmaxy p(y|x, θ)) rather than the marginals.

One can use graph-cut (applicable only for binary
output variables) or max-product LBP for the decod-
ing task. However, this yields a discontinuous esti-
mate (even with averaged perceptron) for the gra-
dient (see Section 5). For the same reason, we use
sum-product LBP rather than max-product LBP.

3.2 A Joint Model with Global Normalization

Although the approach of updating the parameters
of the local classifiers based on the global inference
might seem like a natural extension to train the clas-
sifiers jointly, it suffers from at least two problems.
First, since the node and the edge scores are nor-
malized locally (see Equations 1 and 2), this ap-
proach leads to the so-called label bias problem, pre-
viously discussed by Lafferty et al. (2001). Namely,
due to the local normalization, local features at any
node do not influence states of other nodes in the
graph. Second, the two classifiers use their own fea-
ture sets. However, the same feature sets that give
optimal results locally (i.e., when trained on local
objectives), may not work well when the models are
trained jointly based on the global feedback. In or-
der to address these issues, below we propose a dif-
ferent model.

In our second approach, we seek to build a joint
model with global normalization. We define the fol-
lowing conditional joint distribution:

p(y|v,w,x) = 1
Z(v,w,x)

∏
i∈V

ψn(yi|x,v) ·∏
(i,j)∈E

ψe(yi,j |x,w) (10)

where ψn and ψe are the node and edge factors, and
Z(·) is the global normalization constant that en-
sures a valid probability distribution.

707



This model is essentially a fully connected condi-
tional random field or FCCRF (Murphy, 2012). Fig-
ure 2 shows the differences between the two models
with the standard graphical model representation.4

The global normalization allows CRFs to take long-
range interactions into account. Similar to our pre-
vious model, we use a log-linear representation for
the factors:

ψn(yi|x,v) = exp(vTφ(yi,x)) (11)
ψe(yi,j |x,w) = exp(wTφ(yi,j ,x)) (12)

where φ(·) is a feature vector derived from the inputs
and the labels. The LL for one data point becomes

f(θ) =
∑
i∈V

vTφ(yi,x) +
∑

(i,j)∈E
wTφ(yi,j ,x)

− logZ(v,w,x) (13)

This objective is convex, so we can use gradient-
based methods to find the global optimum. The gra-
dients have the following form:

f ′(v) =
∑
i∈V

φ(yi,x)− E[φ(yi,x)] (14)

f ′(w) =
∑

(i,j)∈E
φ(yi,j ,x)− E[φ(yi,j ,x)] (15)

where E[φ(.)] terms denote the expected feature vec-
tor. Traditionally, CRFs have been trained using of-
fline methods like limited-memory BFGS. Online
training of CRFs using SGD was proposed by Vish-
wanathan et al. (2006). To compare our two meth-
ods, we use SGD to train our CRF models. The
pseudocode is very similar to Algorithm 1.

3.2.1 Modeling Edge Factors
One crucial aspect in the joint models described

above is the modeling of edge factors. The tradi-
tional way is to define edge factors, where yi,j spans
over all possible state transitions, that is K2 differ-
ent transitions, each of which is associated with a
weight vector. This method has the advantage that
it models transitions in a fine-grained way, but, in
doing so, it also increases the number of model pa-
rameters, which may result in overfitting.

4Edge level features and output variables are not shown in
Figure 2 to avoid visual clutter.

Alternatively, one can define Ising-like edge fac-
tors, where we only distinguish between two transi-
tions: (i) same, when yi = yj and (ii) different, when
yi 6= yj . This modeling involves tying one set of pa-
rameters for all same transitions, and another set for
all different transitions.

4 Experimental Setting

In this section, we describe our experimental set-
ting. We first introduce the dataset we use, then we
present the features and the models that we compare.

4.1 Datasets and Evaluation

We experimented with the dataset from SemEval-
2015 Task 3 on Answer Selection for Community
Question Answering (Nakov et al., 2015). The
dataset contains question-answer threads from the
Qatar Living forum.5 Each thread consists of a ques-
tion followed by one or more (up to 143) comments.
The dataset is split into training, development and
test sets, with 2,600, 300, and 329 questions, and
16,541, 1,645, and 1,976 answers, respectively.

Each comment in the dataset is annotated with
one of the following labels, reflecting how well it an-
swers the question: Good, Potential, Bad, Dialogue,
Not English, and Other. At SemEval-2015 Task 3,
the latter four classes were merged into BAD at test-
ing time, and the evaluation measure uses a macro-
averaged F1 over the three classes: Good, Poten-
tial, and BAD. Unfortunately, the Potential class was
both the smallest (covering about 10% of the data),
and also the noisiest and the hardest to predict; yet,
its impact was magnified by the macro-averaged F1.
Thus, subsequent work has further merged Potential
under BAD (Barrón-Cedeño et al., 2015; Joty et al.,
2015), and has used for evaluation F1 with respect to
the Good category (or just accuracy). For our exper-
iments below, we also report F1 for the Good class
and the overall accuracy. We further perform sta-
tistical significance tests using an approximate ran-
domization test based on accuracy.6 We used SIGF
V.2 (Padó, 2006) with 10,000 iterations.

5http://www.qatarliving.com/forum
6Significance tests operate on individual instances rather

than individual classes; thus, they are not applicable for F1.

708



4.2 Features

For comparison, we use the features from our previ-
ous work (Joty et al., 2015) to implement all classi-
fiers in our models and baselines. There are two sets
of features, corresponding to the two main classifi-
cation problems in the models: node-level (i.e., clas-
sifying a comment as good or bad) and edge-level
(i.e., classifying a pair of comments as having the
same or different labels).

The features for node-level classification include
three types of information: (i) a variety of textual
similarity measures computed between the ques-
tion and the comment, (ii) several boolean features
capturing the presence of certain relevant words
or patterns, e.g., URLs, emails, positive/negative
words, acknowledgements, forum categories, pres-
ence of long words, etc., and (iii) a set of global
features modeling dialogue and user interactions in
the answer-thread. The features in the last two cate-
gories are manually engineered (Nicosia et al., 2015;
Barrón-Cedeño et al., 2015).

The features we use for edge-level classification
include (i) all features from the node classification
problem coded as the absolute value of the differ-
ence between the two comments, (ii) a variety of
text similarity features between the two comments,
(iii) the good/bad predictions of the node-level clas-
sifier on the two comments involved in the edge de-
cision. See (Joty et al., 2015) and (Barrón-Cedeño et
al., 2015) for a detailed description of the features.

4.3 Methods Compared

We experimentally compare our above-described
joint models to some baselines and to the state of the
art for this problem. We briefly describe all models
below, together with the names used in the tables of
results.

Independent Comment Classification (ICC)
These are binary classifiers to label thread com-
ments independently into good and bad categories.
The simplest baseline (Majority) classifies all
examples with the most frequent category. We also
train a MaxEnt classifier with stochastic gradient
descent (SGD) and a voted perceptron (ICCME and
ICCPerc, respectively).

Learning-and-Inference Models (LI) This is the
approach presented by Joty et al. (2015), who report
the best results on the task. The model is explained
in Section 3. We experiment with MaxEnt classifiers
trained on-line with SGD and two different inference
strategies, graph-cut and loopy BP (LIME−GC and
LIME−LBP , in our notation).

Joint Learning and Inference Models These are
our new models. First, we experiment with the
model for joint learning of two classifiers coupled
with thread-level inference (Section 3.1). We have
two versions, one using MaxEnt classifiers and the
other using averaged Perceptron. The inference al-
gorithm is loopy BP in both cases. We call these
methods JointME−LBP and JointPerc−LBP , respec-
tively. Second, we experiment with the joint model
with global normalization (cf. Section 3.2). We
call it FCCRF, for fully connected CRF. We use the
Ising-like edge factors defined in Section 3.2.1.

5 Evaluation

All results we report below are calculated in the test
set, using parameters tuned on the development set.

Our main results are shown in Table 1, where we
report accuracy (Acc) as well as precision (P), recall
(R) and F1-score (F1) for the good class.

The models are organized in four blocks. On top,
we see that the majority class baseline achieves ac-
curacy of 50.5%, as the dataset is very well balanced
between the classes.

In block II, we find the results for the local classi-
fiers, ICCME and ICCPerc, which achieve very sim-
ilar results. They are comparable to MaxEnt in Ta-
ble 2, where we report the best published results on
this dataset; yet, our classifiers are trained on-line.

Block III in the table reports results for models
that train two local MaxEnt classifiers and then per-
form thread-level inference using either graph-cut
(LIME−GC) or loopy BP (LIME−LBP ).7 This yields
improvements over the ICC models with the thread-
level inference in block II, which is consistent with
the findings of (Joty et al., 2015); however, the dif-
ference in terms of accuracy is not statistically sig-
nificant (p-value = 0.09).

7Given that MaxEnt and Perceptron perform comparably in
this setting, we have just used MaxEnt as it provides directly the
class probabilities needed for the thread-level inference.

709



Model Learning Inference P R F1 Acc
I. Majority – – 50.5 100.0 67.1 50.5
II. ICCME Local, SGD – 75.1 85.8 80.1 78.5

ICCPerc Local, Voted – 76.6 82.4 79.4 78.4
III. LIME−GC Local, SGD Graph-cut 77.4 83.6 80.4 79.4

LIME−LBP Local, SGD LBP 76.4 84.6 80.3 79.1
IV. JointME−LBP 2 classifiers, Joint, SGD LBP 76.1 84.4 80.0 78.7

JointPerc−LBP 2 classifiers, Joint, AVG LBP 77.1 74.5 75.8 76.0
FCCRF Joint, SGD LBP 77.3 86.2 81.5 80.5

Table 1: Results of all compared models on the test set. The best results are boldfaced.

Model P R F1 Acc
MaxEnt classifier 75.7 84.3 79.8 78.4
Linear CRF 74.9 83.5 78.9 77.5
MaxEnt+ILP 77.0 83.5 80.2 79.1
MaxEnt+GraphCut 78.3 82.9 80.6 79.8
Our method (FCCRF) 77.3 86.2 81.5 80.5

Table 2: Comparison to the best published results on
the same datasets, as reported in (Joty et al., 2015).

Comparing our LIME−GC to MaxEnt+GraphCut
in Table 2, we see that we are slightly worse: -0.2 in
F1-score, and -0.4 in accuracy. It turns out that this
is due to our on-line MaxEnt classifier for the pair-
wise classification being slightly worse (-0.4 accu-
racy points absolute), which could explain the lower
performance after the graph-cut inference.

Next, block IV shows that the fully connected
CRF model (FCCRF) improves over the models in
block III by more than one point absolute in both F1
and accuracy. The improvement is statistically sig-
nificant (p-value = 0.04); especially noticeable is the
increase in recall (+2.6 points). This result is also
an improvement over the state of the art, as Table 2
shows.

Again in block IV, we can see that the two mod-
els that perform joint training of two classifiers
and then integrate inference in the training loop,
JointME−LBP and JointPerc−LBP , do not work well
and fall below the learning and inference models
from block III. As we explained above, these models
have two major disadvantages compared to FCCRF:
(i) the local normalization of node and edge scores
is prone to label bias issues; (ii) each of the two clas-
sifiers uses its own feature set, which might not be
optimal when they are trained jointly based on the
global feedback.

Notice that the version using Perceptron,
JointPerc−LBP , works bad in this setting. Since
updates are done after each thread-level inference,
we could not use a voted perceptron, but an aver-
aged one (Collins, 2002). Moreover, it did not yield
probabilities but real-valued scores, which we had
to remap to the [0;1] interval using a sigmoid.

5.1 CRF Variants Analysis

Table 3 compares different variants of CRF. The first
two rows show the results for the commonly used
linear-chain CRF (LCCRF) of order 1 and 2. We
can see that these models fall two accuracy (and F1)
points below FCCRF, which indicates that the pair-
wise relations between non-consecutive comments
provide additional relevant information for the task.
The fourth row shows the results when we eliminate
the edge-level features and we consider state tran-
sitions using the bias features only: the decrease in
performance is tiny, which means that what matters
is to model the interaction in the first place; the par-
ticular features used are less important. More no-
ticeable is the effect of using Ising-like modeling of
the edge factors in our FCCRF model. If we use
finer-grained edge factors for each of the four com-
binations (Good-Good, Good-Bad, Bad-Good, and
Bad-Bad), the performance decreases significantly,
mostly due to a drop in recall (see ‘FCCRF (4C)’).

5.2 Error Analysis

Next, we get a closer look at the predictions made
by our best Local (ICCME), Inference (LIME−GC),
and Global (FCCRF) models. We focus on questions
for which there are at least two comments. There
were 280 such test questions (out of 329), with a
total of 1,927 comments.

710



Model P R F1 Acc
LCCRF (ord=1) 76.1 83.2 79.4 78.3
LCCRF (ord=2) 76.8 82.1 79.3 78.4
FCCRF 77.3 86.2 81.5 80.5
FCCRF-noFeatures 77.2 86.0 81.4 80.1
FCCRF (4C) 78.8 79.7 79.3 79.0

Table 3: Results for different variants of the joint
CRF model on the test set.

The Local, the Inference, and the Joint mod-
els made correct predictions for 78.7%, 79.1% and
80.4% of the comments, respectively. We can see
that the Inference model behaves more like Local,
and not so much like Joint. This is indeed further
confirmed when we look at the agreement between
each pair of models: Local vs. Inference has 6.0%
disagreement, for Local vs. Joint it is 9.9%, and for
Inference vs. Joint it is 8.8%.

Figure 3 compares the three models vs. the
gold human labels on a particular test question
(ID=Q2908; some long comments are truncated and
the four omitted answers were classified correctly by
all three classifiers). We can see that the Joint model
is more robust than the Local one: while Joint cor-
rects two of the three wrong classifications of Local,
Inference makes two further errors instead.

6 Conclusion

We have proposed two learning methods for com-
ment classification in community Question Answer-
ing. We depart from the state-of-the-art knowl-
edge that exploiting the interrelations between all
the comments in the answer-thread is beneficial for
the task. Thus, we take as our baseline the learn-
ing and inference model from Joty et al. (2015), in
which the answer-thread is modeled as a fully con-
nected graph. Our contribution consists of moving
the framework to on-line learning and proposing two
models for coupling learning with inference.

Our first model learns jointly the two MaxEnt
classifiers with SGD and incorporates the graph in-
ference at every step with loopy belief propagation.
This model, due to its local normalization, suffers
from the label bias problem. The alternative we pro-
posed is to use an instance of a Fully Connected CRF
that operates on the same graph and considers the
node and edge factors with a shared set of features.

Q: I have a female friend who is leaving for a teaching
job in Qatar in January. What would be a useful
portable gift to give her to take with her?

A1 A couple of good best-selling novels. [. . .]
Loc: Good, Inf: Good, Jnt: Good, Hum: Good

A5 A big box of decent tea.... like “Scottish blend” or
“Tetleys”.. [. . .]
Loc: Good, Inf: Good, Jnt: Good, Hum: Good

A6 Bacon. Nice bread, bacon, bacon, errmmm bacon
and a pork joint..
Loc: Good, Inf: Bad, Jnt: Good, Hum: Good

A8 Go to Tesco buy some good latest DVD.. [. . .]
Loc: Good, Inf: Good, Jnt: Good, Hum: Good

A9 Couple of good novels, All time favorite movies, ..
Loc: Good, Inf: Bad, Jnt: Good, Hum: Good

A10 Agree I do the same Indorachel..But some time you
get a good copy some time a bad one.. [. . .]
Loc: Good, Inf: Good, Jnt: Good, Hum: Bad

A11 Ditto on the books and dvd’s. Excedrin.
Loc: Bad, Inf: Bad, Jnt: Good, Hum: Good

A12 Ditto on the bacon, pork sausage, pork chops,
ham,..can you tell we miss pork! [. . .]
Loc: Bad, Inf: Bad, Jnt: Good, Hum: Good

Figure 3: Sample test question with a thread of com-
ments and, for each comment, decisions by the local
(Loc), the global inference (Inf), and the global joint
(Jnt) classifiers, as well as by the human annotators.

One of the main advantages is that the normalization
is global. We experimented with the SemEval-2015
Task 3 dataset and we confirmed the advantage of
the FCCRF model, which outperforms all baselines
and achieves better results than the state of the art.

In the near future, we plan to apply the FCCRF
model to the full cQA task, i.e., finding good an-
swers to newly-asked questions using previously-
asked questions and their answer threads. In this
setting, we want to experiment with (i) ranking com-
ments (instead of classifying them), (ii) exploiting
the similarities between the new question and the
questions in the database and also the relations be-
tween comments across different answer-threads.

Acknowledgments
This research was performed by the Arabic Lan-
guage Technologies (ALT) group at the Qatar Com-
puting Research Institute (QCRI), HBKU, part of
Qatar Foundation. It is part of the Interactive sYs-
tems for Answer Search (IYAS) project, which is
developed in collaboration with MIT-CSAIL.

711



References
Alberto Barrón-Cedeño, Simone Filice, Giovanni

Da San Martino, Shafiq Joty, Lluı́s Màrquez, Preslav
Nakov, and Alessandro Moschitti. 2015. Thread-level
information for comment classification in community
question answering. In Proceedings of the 53rd
Annual Meeting of the Association for Computational
Linguistics and the 7th International Joint Conference
on Natural Language Processing, ACL-IJCNLP ’15,
pages 687–693, Beijing, China.

Clinton Burfoot, Steven Bird, and Timothy Baldwin.
2011. Collective classification of congressional floor-
debate transcripts. In Proceedings of the 49th Annual
Meeting of the Association for Computational Linguis-
tics: Human Language Technologies - Volume 1, HLT
’11, pages 1506–1515, Portland, Oregon.

Xavier Carreras and Lluı́s Màrquez. 2003. Online learn-
ing via global feedback for phrase recognition. In
Proceedings of the 17th Annual Conference on Neural
Information Processing Systems, NIPS ’03, Whistler,
Canada. MIT Press.

Xavier Carreras, Lluı́s Màrquez, and Jorge Castro. 2005.
Filtering–ranking perceptron learning for partial pars-
ing. Machine Learning, 60:41–71.

Michael Collins. 2002. Discriminative training meth-
ods for hidden Markov models: Theory and experi-
ments with perceptron algorithms. In Proceedings of
the 2002 Conference on Empirical Methods in Nat-
ural Language Processing, EMNLP ’02, pages 1–8,
Philadelphia, Pennsylvania, USA.

Cicero dos Santos, Luciano Barbosa, Dasha Bogdanova,
and Bianca Zadrozny. 2015. Learning hybrid rep-
resentations to retrieve semantically equivalent ques-
tions. In Proceedings of the 53rd Annual Meeting of
the Association for Computational Linguistics and the
7th International Joint Conference on Natural Lan-
guage Processing (Volume 2: Short Papers), ACL-
IJCNLP ’15, pages 694–699, Beijing, China.

Huizhong Duan, Yunbo Cao, Chin-Yew Lin, and Yong
Yu. 2008. Searching questions by identifying ques-
tion topic and question focus. In Proceedings of the
46th Annual Meeting of the Association for Compu-
tational Linguistics and the Human Language Tech-
nology Conference, ACL-HLT ’08, pages 156–164,
Columbus, Ohio, USA.

Minwei Feng, Bing Xiang, Michael R Glass, Lidan
Wang, and Bowen Zhou. 2015. Applying deep learn-
ing to answer selection: A study and an open task. In
Proceedings of the 2015 IEEE Workshop on Automatic
Speech Recognition and Understanding, ASRU ’15,
pages 813–820, Scottsdale, Arizona, USA.

Yongshuai Hou, Cong Tan, Xiaolong Wang, Yaoyun
Zhang, Jun Xu, and Qingcai Chen. 2015. HITSZ-

ICRC: Exploiting classification approach for answer
selection in community question answering. In Pro-
ceedings of the 9th International Workshop on Seman-
tic Evaluation, SemEval ’15, pages 196–202, Denver,
Colorado, USA.

Shafiq Joty, Alberto Barrón-Cedeño, Giovanni
Da San Martino, Simone Filice, Lluı́s Màrquez,
Alessandro Moschitti, and Preslav Nakov. 2015.
Global thread-level inference for comment clas-
sification in community question answering. In
Proceedings of the 2015 Conference on Empir-
ical Methods in Natural Language Processing,
EMNLP ’15, pages 573–578, Lisbon, Portugal.

John Lafferty, Andrew McCallum, and Fernando Pereira.
2001. Conditional random fields: Probabilistic mod-
els for segmenting and labeling sequence data. In Pro-
ceedings of the Eighteenth International Conference
on Machine Learning, ICML ’01, pages 282–289, San
Francisco, California, USA.

Shuguang Li and Suresh Manandhar. 2011. Improv-
ing question recommendation by exploiting informa-
tion need. In Proceedings of the 49th Annual Meet-
ing of the Association for Computational Linguistics:
Human Language Technologies - Volume 1, ACL ’11,
pages 1425–1434, Portland, Oregon, USA.

Kevin P. Murphy, Yair Weiss, and Michael I. Jordan.
1999. Loopy belief propagation for approximate in-
ference: An empirical study. In Proceedings of the
Fifteenth Conference on Uncertainty in Artificial Intel-
ligence, UAI’99, pages 467–475, Stockholm, Sweden.

Kevin Murphy. 2012. Machine Learning A Probabilistic
Perspective. The MIT Press.

Preslav Nakov, Lluı́s Màrquez, Walid Magdy, Alessan-
dro Moschitti, Jim Glass, and Bilal Randeree. 2015.
SemEval-2015 task 3: Answer selection in commu-
nity question answering. In Proceedings of the 9th
International Workshop on Semantic Evaluation, Se-
mEval ’15, pages 269–281, Denver, Colorado, USA.

Preslav Nakov, Lluı́s Màrquez, Alessandro Moschitti,
Walid Magdy, Hamdy Mubarak, Abed Alhakim Frei-
hat, Jim Glass, and Bilal Randeree. 2016. SemEval-
2016 task 3: Community question answering. In Pro-
ceedings of the 10th International Workshop on Se-
mantic Evaluation, SemEval ’16, San Diego, Califor-
nia, USA.

Massimo Nicosia, Simone Filice, Alberto Barrón-
Cedeño, Iman Saleh, Hamdy Mubarak, Wei Gao,
Preslav Nakov, Giovanni Da San Martino, Alessandro
Moschitti, Kareem Darwish, Lluı́s Màrquez, Shafiq
Joty, and Walid Magdy. 2015. QCRI: Answer selec-
tion for community question answering - experiments
for Arabic and English. In Proceedings of the 9th
International Workshop on Semantic Evaluation, Se-
mEval ’15, pages 203–209, Denver, Colorado, USA.

712



Sebastian Padó, 2006. User’s guide to sigf: Signifi-
cance testing by approximate randomisation.

Bo Pang and Lillian Lee. 2004. A sentimental ed-
ucation: Sentiment analysis using subjectivity sum-
marization based on minimum cuts. In Proceed-
ings of the 42nd Annual Meeting on Association for
Computational Linguistics, ACL ’04, pages 271–278,
Barcelona, Spain.

Judea Pearl. 1988. Probabilistic Reasoning in Intelligent
Systems: Networks of Plausible Inference. Morgan
Kaufmann Publishers Inc., San Francisco, California,
USA.

Vasin Punyakanok and Dan Roth. 2000. Shallow parsing
by inferencing with classifiers. In Proceedings of the
2nd Workshop on Learning Language in Logic and the
4th Conference on Computational Natural Language
Learning - Volume 7, ConLL ’00, pages 107–110, Lis-
bon, Portugal.

Vasin Punyakanok, Dan Roth, Wen-tau Yih, and Dav Zi-
mak. 2004. Semantic role labeling via integer linear
programming inference. In Proceedings of the 20th In-
ternational Conference on Computational Linguistics,
COLING ’04, Geneva, Switzerland.

Vasin Punyakanok, Dan Roth, Wen-tau Yih, and Dav Zi-
mak. 2005. Learning and inference over constrained
output. In Proceedings of the 19th International Joint
Conference on Artificial Intelligence, IJCAI’ 05, pages
1124–1129, Edinburgh, Scotland.

Dan Roth and Wen-tau Yih. 2004. A linear programming
formulation for global inference in natural language
tasks. In Proceedings of the HLT-NAACL 2004 Work-
shop: Eighth Conference on Computational Natural
Language Learning, CoNLL ’04, pages 1–8, Boston,
Massachusetts, USA.

Ming Tan, Bing Xiang, and Bowen Zhou. 2015. LSTM-
based deep learning models for non-factoid answer se-
lection. arXiv preprint arXiv:1511.04108.

Matt Thomas, Bo Pang, and Lillian Lee. 2006. Get out
the vote: Determining support or opposition from con-
gressional floor-debate transcripts. In Proceedings of
the 2006 Conference on Empirical Methods in Natural
Language Processing, EMNLP ’06, pages 327–335,
Sydney, Australia.

S. V. N. Vishwanathan, Nicol N. Schraudolph, Mark W.
Schmidt, and Kevin P. Murphy. 2006. Accelerated
training of conditional random fields with stochastic
gradient methods. In Proceedings of the 23rd Interna-
tional Conference on Machine Learning, ICML ’06,
pages 969–976, Pittsburgh, Pennsylvania, USA.

Zhiguo Wang and Abraham Ittycheriah. 2015. Faq-
based question answering via word alignment. arXiv
preprint arXiv:1507.02628.

Yair Weiss. 2001. Comparing the mean field method and
belief propagation for approximate inference in MRFs.
Advanced Mean Field Methods.

Guangyou Zhou, Tingting He, Jun Zhao, and Po Hu.
2015a. Learning continuous word embedding with
metadata for question retrieval in community question
answering. In Proceedings of the 53rd Annual Meet-
ing of the Association for Computational Linguistics
and the 7th International Joint Conference on Natural
Language Processing (Volume 1: Long Papers), ACL-
IJCNLP ’15, pages 250–259, Beijing, China.

Xiaoqiang Zhou, Baotian Hu, Qingcai Chen, Buzhou
Tang, and Xiaolong Wang. 2015b. Answer sequence
learning with neural networks for answer selection in
community question answering. In Proceedings of the
53rd Annual Meeting of the Association for Computa-
tional Linguistics, pages 713–718, Beijing, China.

Xiaoqiang Zhou, Baotian Hu, Jiaxin Lin, Yang Xiang,
and Xiaolong Wang. 2015c. ICRC-HIT: A deep learn-
ing based comment sequence labeling system for an-
swer selection challenge. In Proceedings of the 9th
International Workshop on Semantic Evaluation, Se-
mEval ’15, pages 210–214, Denver, Colorado, USA.

713


