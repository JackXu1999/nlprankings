



















































Finding the Middle Ground - A Model for Planning Satisficing Answers


Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 547–557,
Berlin, Germany, August 7-12, 2016. c©2016 Association for Computational Linguistics

Finding the Middle Ground - A Model for Planning Satisficing Answers

Sabine Janzen
Saarland University

Saarbrücken, Germany
janzen@iss.uni-saarland.de

Wolfgang Maaß
Saarland University

Saarbrücken, Germany
maass@iss.uni-saarland.de

Tobias Kowatsch
University of St. Gallen
St. Gallen, Switzerland

tobias.kowatsch@unisg.ch

Abstract

To establish sophisticated dialogue sys-
tems, text planning needs to cope with
congruent as well as incongruent inter-
locutor interests as given in everyday di-
alogues. Little attention has been given to
this topic in text planning in contrast to di-
alogues that are fully aligned with antic-
ipated user interests. When considering
dialogues with congruent and incongru-
ent interlocutor interests, dialogue part-
ners are facing the constant challenge of
finding a balance between cooperation and
competition. We introduce the concept
of fairness that operationalize an equal
and adequate, i.e. equitable satisfaction
of all interlocutors’ interests. Focusing
on Question-Answering (QA) settings, we
describe an answer planning approach that
support fair dialogues under congruent and
incongruent interlocutor interests. Due
to the fact that fairness is subjective per
se, we present promising results from an
empirical study (N=107) in which human
subjects interacted with a QA system im-
plementing the proposed approach.

1 Introduction

For building dialogue systems that cope with con-
tradictions and individual interests of dialog part-
ners, text planning is required to process incon-
gruent and congruent interests of interlocutors. So
far, research on dialogue systems focusses on sup-
porting dialogues that are fully aligned with antic-
ipated user interests, e.g., (Hovy, 1991; Grosz and
Kraus, 1996; Moore and Paris, 1993; Lochbaum,
1998; Rich and Sidner, 1997), and, thus, max-
imizing cooperativeness (Bunt and Black, 2000,
191 p. 5). Few approaches exist that investi-

gate text planning with pure conflict, e.g., (Jame-
son et al., 1994; Hadjinikolis et al., 2013; Black
and Atkinson, 2011; Prakken, 2006). When con-
sidering dialogues with congruent as well as in-
congruent interlocutors interests, dialogue part-
ners are facing the constant challenge of finding
a balance between cooperation and competition
(Parikh, 2010). We introduce the concept of fair-
ness that operationalize an equal and adequate,
i.e. equitable satisfaction of all interlocutors’ in-
terests (Oxford Dictionaries, 2016). Focusing on
Question-Answering (QA) settings, we describe
an answer planning approach that support fair dia-
logues under congruent and incongruent interests
of interlocutors. Due to the fact that fairness is
subjective per se, we present results from an em-
pirical study in which human subjects interacted
with a QA system in various dialogue settings.
When determining appropriate answers in text
planning, approaches range from (1) wrong an-
swer avoidance concepts technically checking the
correctness of answers, e.g., Dong et al. (2011),
and (2) opponent models in persuasion dialogues
for choosing most suitable arguments, e.g., Had-
jinikolis et al. (2013), to (3) the prediction of emo-
tions of interlocutors to generate answers, e.g.,
Hasegawa et al. (2013). Here, related work is rel-
evant that focuses on the determination of appro-
priate answers by processing concepts like users’
intentions (e.g., Levelt (1993)), desires (e.g., Rao
& Georgeff (1995)), preferences (e.g., Li et al.
(2013)), objectives (e.g., Schelling (1960)) and
goals (e.g., Traum et al. (2008)) which we will
hereafter subsume under the term motives. Mo-
tives refer to objectives or situations that inter-
locutors would like to accomplish, e.g., to find
the best price when shopping. According to the
belief-desire-intention model, motives can be de-
scribed as desires in the sense of a motivational
state (Georgeff et al., 1998). Motives do not in-

547



volve the mandatory purpose of being recogniz-
able by other participants; so they are equivalent
with the concept of intentions in (Levelt, 1993).
Regarding the processing of congruent and incon-
gruent motives, existing approaches rather focus
on motives of single interlocutors or on joint mo-
tives, e.g., Paquette (2012), Li et al. (2013). In
the following, the aggregation of congruent and
incongruent interlocutor motives in dialogues will
be described as mixed motives.
In this work, we propose a model that formalizes
answer planning as psychological game (Bjorn-
dahl et al., 2013) embedded in text planning ap-
proaches (Mann and Thompson, 1986; Moore and
Paris, 1993) for creating dialogues perceived as
fair by all interlocutors. Since traditional formal-
ization of motives by means of utility functions
is not sufficient to handle complex interactions as
given in the considered dialogue setting (Bjorn-
dahl et al., 2013), psychological games enrich
classical game settings with user models, i.e. in
our case explicit representations of mixed motives.
One appeal of the model is the consideration of an-
swer planning as psychological game that lifts the
process of finding appropriate answers from the
short-term linguistic level to the long-term motive
level in contrast to other approaches (van Deemter,
2009; Stevens et al., 2015). Interlocutors do not
have preferences for answers, but try to satisfy mo-
tives. So, we assume that this approach enables a
more sophisticated simulation of human behavior
in mixed motive interactions as well as the estab-
lishment of “cooperativeness in response formu-
lation” (Bunt and Black, 2000, p. 5) for creat-
ing dialogues perceived as fair. By exemplifying
the model within a QA system as natural language
sales assistant for conducting sales dialogues, we
were able to evaluate the proposed approach in
an empirical user study (N=107) in terms of per-
ceived fairness of created dialogues with promis-
ing results.

2 Planning Answers given Mixed
Motives

Adopting a computational pragmatics perspective,
we intend to compute relevant linguistic aspects
of answers based on contextual aspects given by
mixed motives (Bunt and Black, 2000, p. 3).
When searching for answers that support an eq-
uitable satisfaction of mixed motives during dia-
logue, Ω represents the solution space with poten-

tial answers. An objective function f : Ω→ R as-
signs values to all answers x ∈ Ω for representing
their potential in satisfying motives of interlocutor
i ∈ I . Of course, all interlocutors I prefer an-
swers x that satisfy best their motives; xa � xb ⇔
f(xa) > f(xb). So, the goal would be to find an
answer x ∈ Ω with highest satisfaction of motives
f(x) of interlocutor i ∈ I in the sense of an opti-
mal solution x*; i.e. f(x*) = max{f(x)|x ∈ Ω}.
But, in order to achieve fair outcomes regarding
an equitable, i.e. equal and adequate satisfaction
of mixed motives, this definition is not sufficient.
First, decision making takes places in the context
of social dialogue interaction, i.e. answers have to
be selected based on multiple objective functions
since motives of all interlocutors i ∈ I shall be
satisfied; f i : Ω → R. For capturing the aspect of
equal motive satisfaction, the potential of answers
has to be represented absolutely and relatively. In
other words, the performance of an answer in sat-
isfying motives of an interlocutor i ∈ I is com-
bined with its performance in satisfying motives
of counterparts −i ∈ I; max{f i,-i(x)|x ∈ Ω}.
Second, the aforementioned conflict between co-
operation and competition needs to be solved ade-
quately. Since it is impossible to find an answer
satisfying all motives of all interlocutors at any
time in the dialogue, we search for a compromise
in form of a solution i.e. an answer x+ with a
minimum quality s so that f(x+) ≥ s. Adopt-
ing the concept of satisficing by Simon (1956), an
approach that attempts to find the best alternative
available in contrast to optimal decision making,
the goal is to find an answer x ∈ Ω with highest
sufficient satisfaction of motives f(x) ≥ s of all
interlocutors I in the sense of a satisficing solution
x+; i.e. f(x+) = max{f i,-i(x) ≥ s|x ∈ Ω}.

3 Model for Planning Satisficing
Answers

To capture these issues, we defined a model for
planning satisficing answers in dialogues with
mixed motives. In the considered setting, a user
with motives poses questions to a QA system that
takes the role of a proxy for indirect interlocu-
tors, e.g., retailers in online shopping scenarios.
The QA system adopts their motives and devel-
ops strategies to satisfy them. Adopted motives
as well as user motives that are anticipated by the
system represent mixed motives in the dialogue.
Task of the system is to process these mixed mo-

548



tives with the objective to create a dialogue that
is perceived as fair by all interlocutors after a fi-
nite number of question answer pairs. As full sat-
isfaction of all motives of all interlocutors at any
time in a mixed motive dialogue is not possible,
the QA system has to find a compromise, i.e. it
has to plan answers that satisfice mixed motives
during dialogue. Let us start by describing an ex-
ample dialogue between a customer and a retailer
in a shopping scenario:
Q: Is the range of this wifi router appropriate for
a house with 3 floors?
A: In case of 3 floors, I would recommend an ad-
ditional wifi repeater that got very good feedback
by other customers. You can buy both router and
repeater as a bundle with 15% discount.
In this dialogue snippet, the customer intends to
get comprehensive product information regarding
the wifi router; the retailer also wants to satisfy in-
formational needs of the customer to establish ex-
cellent services. Beyond these congruent motives,
the retailer wants to increase revenue and to raise
sales figures. A balance between mixed motives
is found by giving information regarding the wifi
router as well as preferences of other customers
followed by a discounted bundle offer.
In order to implement this kind of behavior into
dialogue systems, the model for planning satisfic-
ing answers separates linguistics from conceptual
non-linguistic aspects (Traum and Larsson, 2003;
Allen et al., 2001) and consists of three main mod-
ules: linguistic module, mapper and mixed motive
module (cf. Fig. 1). The linguistic module takes
care for handling user questions as input as well
for generating answers as output. Essential com-
ponents of the linguistic module are the linguistic
intention model and flexible text planning tech-
nologies. For the latter, we apply text plans ac-
cording to the Rhetorical Structure Theory (Mann
and Thompson, 1986) in form of plan operators
(Moore and Paris, 1993) for generating answers.
Each plan operator consists of a single compul-
sive part, called nucleus, that is related with di-
verse optional text segments, mentioned as satel-
lites. We assume that beside supporting the effect
of the nucleus, satellites represent an opportunity
to satisfice mixed motives during dialogue. Satel-
lites are linked with entities of the linguistic inten-
tion model, means linguistic intentions that cap-
ture the intended effects, i.e. functions of satel-
lites within answers (Grosz and Sidner, 1986). By

means of second module - the mapper - linguis-
tic intentions are mapped onto motives and vice
versa (cf. Fig. 1). Therefore domain-specific
knowledge about correlations between linguistic
intentions and mixed motives is required that is
induced by a domain configurator and has to be
derived empirically. Last, the mixed motive mod-
ule combines an explicit representation and sit-
uated processing of mixed motives (Cohen and
Levesque, 1990) with a game-theoretical equilib-
rium approach (Nash, 1951) to establish a psy-
chological game setting (Bjorndahl et al., 2013)
(cf. Fig. 1). Our approach operates by assum-
ing that interlocutors are rational. That means
they act strategically and purposively in pursuit of
their own motives that they try to maximally sat-
isfy. Therefore, we assume that game theory is
an adequate prospect to deliver the analytical tools
for planning answers in the context of mixed mo-
tives. In game theory literature, equilibrium con-
cepts are widely applied, e.g., Nash equilibrium
(Nash, 1951). A Nash equilibrium is an outcome
that holds because no involved actor has a ratio-
nal incentive to deviate from it, i.e., the final result
is “good enough” for all actors in the sense of a
happy medium. Adapted to this work, this refers
to a satisficing combination of motives at a partic-
ular time in the dialogue, that is good enough for
planning an answer that supports equitable satis-
faction of mixed motives.

3.1 Concepts
From a conceptual perspective, the model uses
several core entities. First, we have players p ∈
P that represent interlocutors I . Players have
domain-specific motives m for participating in the
dialogue. For each player p ∈ P , we assume
a MotiveSet that consists of individual motives,
IndM , as well as of motives the player, i.e. the
interlocutor anticipates from counterparts, AntM .

MotiveSetp = IndMp + AntM -p (1)

Mixed motives MM are represented by the non-
redundant aggregation of 1. . . n MotiveSet of
players p ∈ P in the dialogue.

MM = {MotiveSetp1 . . . MotiveSetpn} (2)

All motives m ∈ MM are operationalized by
means of real-valued weights for each player cov-
ered by a weight vector

−−−−−→
Weightm. Motives are

formed earlier and persist during dialogue, but

549



Figure 1: Model for planning satisficing answers in dialogues with mixed motives

players deliberate about weights of motives con-
tinuously (Bratman, 1987). The achievement and
thereby satisfaction of motives is supported by lin-
guistic intentions li ∈ LI that are satisfied by
satellites sat that are offered by plan operators and
integrated into an answer. That means motives are
achieved, if answers were given, that contributed
to satisfaction of these motives.

3.2 Algorithm and example
For introducing the proposed approach, we will
give an example course of satisficing answer plan-
ning starting with user question and ending with
system answer. The description of the process will
be supported by a model view marked with step
numbers in Fig. 1 as well as by an algorithmic
view in Alg. 1. In the example, we apply domain-
specific knowledge that was derived empirically in
the retailing domain. Although, in literature re-
view, customer and retailer motives in sales dia-
logues were specified. Combinations of these mo-
tives were analyzed in simulated sales conversa-
tions between real retailers (N=3) and subjects act-
ing as customers (N=12). Recorded as video files,
conversations and identified motives were vali-
dated in a web-based user study (N=120) regard-
ing their naturalness and relevance. Sales conver-
sations were transcribed, aggregated to a text cor-
pus and analyzed regarding question and answer
structures. So, the domain-specific knowledge
representation used in the example bases on re-
sults of this empirical analysis and covers all core
model concepts introduced before: a mixed motive
model with empirically derived default weights
consisting of 19 customer and 4 retailer motives
(cf. Tab. 1); 39 question and 33 answer schemata
(McKeown, 1985), 31 plan operators (Moore and

Motive m ∈MM Weight pa Weight pb
High level of reliability of prod-
uct (mR)

1.90 1.00

Fair price of product (mFP) 0.70 0.00
Exclusive design of product
(mED)

0.53 1.00

Comprehensive product infor-
mation (mCPI)

1.67 1.00

Improving customer relationship
(mICR)

0.00 4.00

Increase revenue (mIR) 4.00 4.00

Table 1: Extract of domain-specific mixed mo-
tive model with default weights for player (pa) and
player (pb) representing customer and retailer

Paris, 1993), 21 satellites with 18 linguistic in-
tentions (cf. Tab. 2) and 14 rhetorical relations
(Hobbs, 1978; Hovy, 1993; Mann and Thompson,
1986), and exemplary product information.
Imagine a sales conversation regarding consumer
electronics between customer and retailer repre-
sented by player (pa) and player (pb). Sets of mo-
tives by players are equal regarding the motives
included but differ in weights of individual and an-
ticipated motives by players (cf. Tab. 1).

MM = MotiveSetpa + MotiveSetpb (3)

MotiveSetpa = IndM pa + AntM pb

MotiveSetpb = IndM pb + AntM pa

The customer poses a question concerning
products with a specific feature: “How many
tablets offer the wifi features 802.11A, 802.11B,
802.11G, 802.11n?” Based on the identified ques-
tion schema as well as the determined communica-
tive function of the question, a dialogue system
that instantiates the proposed model selects an ap-
propriate plan operator (cf. Fig. 1, step 1 & 2).

550



Ling. Intention li ∈ LI supports m ∈
MM

Description

Advantages (liA) {mICR, mFP, mED,
mIPD, mR, mHLS,
mACB, mSCD, mI,
mHLP, mPB, mQ}

Integration of information
about advantages of prod-
uct(s) into answer

External
Review (liER)

{mSI} Presentation of customer
reviews

My Product (liMP) {mICR, mSP, mR,
mHLC}

Mentioning products that
could be interesting for
customer

Functionality (liF) {mEU} Extension of answer re-
garding product functions

Opinion (liO) {mHEM, mSP, mR,
mSI}

Integration of subjective
(retailer) opinion into an-
swer

Table 2: Extract of domain-specific linguistic in-
tentions li ∈ LI with supported motives and de-
scription

3.2.1 Definition of set S and determination of
SatisfactionSet

In our case, a plan operator named NUMBER OF
PRODUCTS is selected that offers an obligatory
nucleus and a set S of four optional satellites (cf.
Fig. 2):

S = {satAAS, satVER, satDF, satEUP} (4)

Overall objective is to determine set S+ out of set
S, that consists of satellites that - besides support-
ing the effect of the nucleus - contribute to satis-
ficing mixes motives of customer and retailer dur-
ing dialogue (cf. Alg. 1). According to (Grosz
and Sidner, 1986; Moore and Paris, 1993), satel-
lites are linked with linguistic intentions; i.e. they
fulfill certain functions regarding the overall dia-
logue. Set S is sent to the linguistic intention han-
dler that specifies the SatisfactionSet (cf. Fig.
1, step 3 and Alg. 1, line 1-4). This set covers lin-
guistic intentions that can be satisfied by satellites
of set S (cf. Tab. 2):

SatisfactionSet = {liA, liER, liF, liMP} (5)

Figure 2: Plan operator NUMBER OF PRODUCTS

3.2.2 Mapping linguistic intentions onto
mixed motives

Next, linguistic intentions have to be mapped onto
motives. The m:n correlation between linguistic
intentions and motives (Moore and Paris, 1993)
is domain-specific, has to be specified empirically
and is induced by the domain configurator (cf.
Fig. 1, step 5). Each motive is supported by a
set of linguistic intentions that contribute to the
achievement of this motive (cf. Fig. 3). On the
other hand, each linguistic intention can support
the achievement of several motives. By processing

Figure 3: Correlations between motives (M) and
linguistic intentions (LI)

the supports-relation between both concepts, the
mapper specifies the RelevanceSet based on the
SatisfactionSet. The resulting RelevanceSet
represents all mixed motives relevant for planning
the actual answer (cf. Alg. 1, line 5-8):

RelevanceSet = {mQ, mR, mIPD, mHCS, mACB, mICR,
mI, mSCD, mPB, mEU, mFP, mED,

mHLP, mSI, mSP, mHLC}
(6)

3.2.3 Satisficing mixed motives
Having identified the RelevanceSet, we now in-
tend to identify a satisficing combination of the in-
volved motives. Therefore, the mapper sends the
RelevanceSet to the mixed motive model handler
for specifying the SatisficingSet that consists
of motives that (1) are sufficiently interesting for
all interlocutors (i.e. weighted positively), and (2)
have preferably low conflict potential (i.e. small
differences in player weights) (cf. Fig. 1, step 6).
Satisficing mixed motives is considered as multi-
player non-zero-sum game that is played for in-
finitely many rounds, more precisely pairs of user
questions and system answers. In each round
of the game, it has to be decided which motives

551



Algorithm 1 Determining set S+ of satisficing satellites
Require: set of default satellites S = {sat1 . . . satn}; set of players P = {p1 . . . pn}; set of mixed motives MM =
{m1 . . . mn}; set of linguistic intentions LI = {li1 . . . lin}

Ensure: set of satisficing satellites S+ = {sat1 . . . satn}
1: Initialize SatisfactionSet = {li1 . . . lin ∈ LI|li.isSatisfiedBy(sat ∈ S)}
2: for ∀sat ∈ S do
3: SatisfactionSet⇐ SatisfactionSetsat ∪ SatisfactionSet
4: end for
5: Initialize RelevanceSet = {m1 . . . mn ∈MM |m.isSupportedBy(li ∈ SatisfactionSet)}
6: for ∀li ∈ SatisfactionSet do
7: RelevanceSet⇐ RelevanceSetli ∪RelevanceSet
8: end for
9: Determine StrategySet⇐ P(RelevanceSet)

10: Initialize StrategyProfiles = {−→s 1 . . .−→s n}
11: for ∀s ∈ StrategySet; ∀p ∈ P do
12: Calculate LocalPayout(s)
13: Define −→s = {s1 . . . sn ∈ StrategySet|LocalPayout(sp*|s-p) ≥ LocalPayout(sp|s-p)}
14: StrategyProfiles.add(−→s )
15: end for
16: for ∀−→s ∈ StrategyProfiles do
17: if LocalPayout(sp*|s*-p) ≥ LocalPayout(sp|s*-p) then
18: −→s * ⇐ −→s
19: end if
20: end for
21: Determine SatisficingSet = {m1 . . . mn ∈ s ∈ −→s *}
22: if SatisficingSet 6= ∅ then
23: Initialize SupportSet = {li1 . . . lin ∈ LI|li.supports(m ∈ SatisficingSet)}
24: for ∀m ∈ SatisficingSet do
25: SupportSet⇐ SupportSetm ∪ SupportSet
26: end for
27: Return S+ = {sat1 . . . satn ∈ S|sat.satisfies(li ∈ SupportSet ∩ SatisfactionSet)}
28: else
29: Return S+ = {∅}
30: end if

of the RelevanceSet are selected as trigger for
planning an answer that supports the creation of
dialogues perceived as fair by all interlocutors.
The equilibrium identifier specifies strategy sets
Sp = {s1 . . . sn} for all players P by generating
the power set of the RelevanceSet (cf. Fig. 1,
step 7 and Alg. 1, line 9). Each of the 137 result-
ing strategies s = {m1 . . . mn} represents a pos-
sible combination of motives or the empty set and
is measured by a normalized local payout for each
player based on weights of involved motives.

Spa = Spb = {s1 . . . s137}; s18 = {mQ, mR} (7)
LocalPayoutpa ,s18 = 0.1280; LocalPayoutpb ,s18 = 0.0090

Strategy sets of players are identical regarding
types of covered strategies, but differ in local pay-
outs that can be expected by players when play-
ing this strategy as shown in eq. (7). As players
prefer those strategies that provide high local pay-
outs, the equilibrium identifier identifies strategies
s* ∈ Sp for each player that represent best answers
regarding the behavior of counterparts −→s-p:

LocalPayout(s*,−→s-p) ≥ LocalPayout(s,−→s-p), ∀s ∈ Sp
(8)

Best answers of players in the sense of highest
local payouts are aggregated to 17 strategy pro-
files, each a vector consisting of two strategies
one for each player (cf. Alg. 1, line 10-15):−→s = {sx, sy}; sx ∈ Spa , sy ∈ Spb .
Next, strategy profiles are selected that meet the
Nash equilibrium condition, i.e. those strategy
profiles exclusively cover strategies that represent
mutual best answers of players (cf. Alg. 1, line
16-20):

LocalPayout(s*,−→s-p*) ≥ LocalPayout(s,−→s-p*) (9)

∀s ∈ −→s1 . . .−→sn

In our example, we find two Nash equilibria.
Those two strategy profiles represent best answers
for the player p as well as the whole group of play-
ers P in the sense of a solution with minimum
quality. No player has an incentive to deviate from
those strategy profile because then its local pay-
out would decrease. With −→s = {s36, s36}, we
select the non-pareto-dominant option for finding
the strategy profile with the lowest difference in
local payouts following the idea of the model to

552



create a balance between mixed motives. With
each answer planning, players generate local pay-
outs that are added during the course of dialogue
to global payouts. Instead of gaining high global
payouts, the objective of the model is to balance
payouts of players during dialogue or to approx-
imate them in case of drifting apart. We assume
that similar global payouts of players can be re-
garded as evidence for satisficed mixed motives.
Based on the selected strategy profile, involved
motives are aggregated to the SatisficingSet =
{mR, mICR} that represents a combination of
mixed motives that is satisficing for all players in
this time in the dialogue (cf. Alg. 1, line 21).

3.2.4 Mapping mixed motives onto linguistic
intentions

The resulting SatisficingSet is forwarded to the
mapper for mapping back motives onto linguistic
intentions (cf. Fig. 1, step 8 & 9). In case, the
SatisficingSet covers zero motives, no mapping
takes place, the process ends and none of the satel-
lites in set S, cf. eq (4), will be considered in the
actual answer planning. Otherwise, the mapper
determines the set of supporting linguistic inten-
tions by processing the inverse is-supported-by-
relation between motives and linguistic intentions
(cf. Alg. 1, line 22-30) (cf. Fig. 3). Comparing
this set with the SatisfactionSet (cf. eq. (5)),
an intersection called SupportSet is created that
represents the set of linguistic intentions that will
be satisfied in current answer planning:

SupportSet = {liA, liMP} (10)

3.2.5 Determination of set S+ and generation
of answer

The linguistic intention handler determines the fi-
nal set of satellites S+ by analyzing 1:1 relations
between linguistic intentions of the SupportSet
and satellites of the set S (cf. Fig. 1, step 10
and Alg. 1, line 27). The resulting set S+ =
{satAAS, satEUP} consists of two satellites: Al-
ternatives Advantages Survey (satAAS) and Emo-
tion User Preferences (satEUP). The text plan lib
handler adjusts the final text plan regarding the se-
lected satellites before sending it to the answer
generator (cf. Fig. 1, step 11 & 12). Last,
the text plan provided by the plan operator NUM-
BER OF PRODUCTS is transformed into an answer.
Thereby, answer schemata referenced by nucleus
as well as satellites of set S+ are instantiated (cf.
Fig. 1, step 13):

Q: “How many tablets offer the wifi features
802.11A, 802.11B, 802.11G, 802.11n?”
A: “[nuc The following tablets offer this feature:
Sony SGPT122 Xperia.] [satAAS Due to its features,
e.g., storage capacity: 32GB, Sony SGPT122 Xpe-
ria has some advantages compared to other prod-
ucts in this category.] [satEUP How about having a
look at Sony SGPT122 Xperia by Sony?]

3.2.6 Summary

In summary, satisficing answer planning is con-
sidered as a game consisting of four components
〈P, S, F, A〉: the set of players P = {pa, pb},
strategies of players S = {Spa , Spb}, objective
functions of players F = {fpa , fpb}, and a state
space A = {a1 . . . at} that represents the rounds
of the game, i.e. answers planned in the dialogue.
The game starts in an initial state a1. At a partic-
ular time t in the dialogue, the equilibrium iden-
tifier observes the state at characterized by P, S,
and F and identifies best answers for all players;
st ∈ Sp;∀p ∈ P . Consequential, a strategy pro-
file meeting the Nash equilibrium condition,−→s t =
{stpa , stpb}, is specified and resulting payouts are
observed: f(at,−→s t) → LocalPayout → R. The
calculation of local payouts by means of objec-
tive functions f ∈ F in state at does not de-
pend solely on the selected strategy profile, but
on results of former states in A, i.e. all answers
planned in the dialogue until at. That means, infi-
nite playing of the described non-zero-sum game
a1,−→s 1, . . . , at,−→s t, . . . generates a stream of pay-
outs f1, f2, . . . , f t = f(at,−→s t). Besides relevant
motives of the RelevanceSet, answer planning in
state at+1 is directly influenced by local payouts
f t(at,−→s t) in at leading to a continuous delibera-
tion of the mixed motive model during dialogue.

4 Implementation and Evaluation

Based on the proposed model (cf. Fig. 1), we
implemented a German text-based QA system in
form of a online shopping assistant (cf. Fig.
4)1. Users are able to construct questions term-
by-term. Having tapped the last term of a ques-
tion, the answer is given. The QA system uses the
domain-specific knowledge representation men-
tioned in section 3.2 formalized in RDF2.

1QA system was implemented as web application:
http://redqueen.iss.uni-saarland.de/satin

2Resource Description Framework

553



Figure 4: Web-based QA system with posed ques-
tion and given answer in German

4.1 Setting
To evaluate our approach, we conducted a user
study with the implemented prototype in German
that was set up as lab experiment. Goal of this
study was to assess the perceived fairness and
naturalness of the dialogue with the QA system
as well as the extent of motive satisfaction of
participants. For that purpose, four randomized
groups were formed. Each group was character-
ized by a combination of motives by users (fair
price of product (mFP) or exclusive design of prod-
uct (mED)) and the QA system representing the re-
tailer (increasing revenue (mIR) or improving cus-
tomer relationship (mICR)) (cf. Tab. 3). These

Table 3: Groups and mixed motive combinations
of user study

mixed motives were combined systematically by
means of scenarios given to users and a manipu-
lated mixed motive model of the QA system. Be-
fore interacting with the QA system that was em-
bedded into a web-based questionnaire, partici-
pants had to opportunity to get to know the QA
system and interacting with it for the first time
(cf. Fig. 8). Participants were then asked to pose
questions to the QA system and to evaluate gener-
ated answers against the background of their mo-
tive (e.g., mFP) and the related scenario, e.g.:
“You are searching for a new tablet that shall be

functional regarding standby and storage capac-
ity. A fair price is important; no need for the lat-
est innovation. You do not want to spend a lot
of money for the new tablet. You are price con-
scious.”
Participants were told to interact with the QA sys-
tem as long as it needed to gain the information
that was required by the scenario. Finally, seven-
point Likert scales ranging from strongly disagree
(1), neither (4) to strongly agree (7) were used to
assess the perceived fairness of the dialogue, the
naturalness of the dialogue and the motive satis-
faction. Tab. 4 lists the questionnaire items for
each of these constructs.

4.2 Results

In summary, 120 subjects participated in the ex-
periment. A complete dataset from 107 partici-
pants (58,3% female) with an average age of 24.3
(SD=6.9) was considered for analysis. On av-
erage, interactions between participants (N=107)
and the QA system covered 5.19 question answer
pairs (cf. example dialogue in appendix A). 556
questions were posed by subjects; 35.07% of them
were propositional questions (e.g., “Is product A
up-to-date?”), 62.41% set questions (e.g., “Where
is the difference between product A and product
B?”) and 2.52% choice questions (e.g., “Which
product is better than product A?”), cf. Bunt et
al. (2010).
Due to the fact that Cronbachs alpha values for all
three multi-item constructs lie clearly above the
recommended threshold of .70 (Nunnally, 1967),

Figure 5: Subject during interaction with QA sys-
tem in user study

which indicates a good to excellent reliability of
the scales, we calculated aggregated mean scores
for each construct. The descriptive statistics of
the three core constructs are presented in Tab. 4.

554



Table 4: Descriptive statistics and results of one-sample t-tests for the empirical core constructs (N=107)

Additionally, results of one-sample t-tests are pro-
vided to evaluate whether the aggregated scores
lie significantly above or below the neutral scale
value of 4. Results indicate that the participants
were undecided with respect to the “Perceived
Naturalness of Dialogue” with the QA system.
We assume that this is owed to the restricted QA
setting since there were no significant differences
among the four groups (F(3,104) = 2.06, p = .11)
(cf. Tab. 3). However, the data support the conclu-
sion that participants perceived the dialogue as fair
and that they were able to sufficiently satisfy their
motives. Assuming rather conflicting motives of
subject and QA system as given for instance in
group #4 in Tab. 3, it could be assumed that per-
ceived fairness and motive satisfaction should be
smaller than in rather congruent motive combi-
nations as shown in group #1. Nonetheless, the
mean value of the construct “Perceived Fairness
of Dialogue” was 5.17 across all groups (signifi-
cant above mean value 4) and there were no signif-
icant differences between the randomized groups
(F(3,104) = 1.59, p = .20). Furthermore, “Motive
Satisfaction” was rated with a mean value of 5.16
across all groups (significant above mean value 4)
and again, there was not a significant effect of the
group on motive satisfaction at the .05 level of sig-
nificance (F(3,104) = 2.33, p = .08).
Overall, this indicates a positive evaluation of the
QA system regarding its ability to generate satis-
ficing answers despite of mixed motives of inter-
locutors.

5 Conclusion

We considered dialogues with congruent as well as
incongruent interlocutor motives, where dialogue
partners are facing the constant challenge of find-
ing a balance between cooperation and competi-
tion. Despite of the overall presence of dialogues
with such mixed motives in everyday life, little at-
tention has been given to this topic in text planning
in contrast to scrutinized dialogue systems that
support dialogues fully aligned with anticipated
user interests. Focusing on Question-Answering
(QA) settings, we introduced a model that formal-
izes answer planning as psychological game em-
bedded in text planning approaches for supporting
fair dialogues under mixed motives. The model
was exemplified within a QA sales assistant with
domain-specific world knowledge for conducting
sales dialogues. Due to the fact that fairness is sub-
jective per se, we presented results from an empir-
ical study (N=107) in which human subjects inter-
acted with the QA system in various mixed motive
settings. Results indicate a positive evaluation of
the systems performance in planning answers that
support fair dialogues despite of mixed motives of
interlocutors.

Acknowledgments

This work was partially funded by the Ger-
man Federal Ministry for Education and Research
(BMBF) under the contract 01IS12030.

555



References
James F. Allen, Donna K Byron, Myroslava Dzikovska,

George Ferguson, Lucian Galescu, and Amanda
Stent. 2001. Toward conversational human-
computer interaction. AI magazine, 22(4):27.

Adam Bjorndahl, Joseph Y Halpern, and Rafael Pass.
2013. Language-based games. In Proc. of the
23rd Int. Joint Conf. on Artificial Intelligence, pages
2967–2971.

Elizabeth Black and Katie Atkinson. 2011. Choos-
ing persuasive arguments for action. In 10th Int.
Conf. on Autonomous Agents and Multiagent Sys-
tems, pages 905–912.

Michael Bratman. 1987. Intention, Plans, and Practi-
cal Reason. Center for the Study of Language and
Information.

Harry Bunt and William Black. 2000. The abc of com-
putational pragmatics. Abduction, Belief and Con-
text in Dialogue: Studies in Computational Prag-
matics, pages 1–46.

Harry Bunt, Jan Alexandersson, Jean Carletta, Jae-
Woong Choe, Alex Chengyu Fang, Koiti Hasida,
Kiyong Lee, Volha Petukhova, Andrei Popescu-
Belis, Laurent Romary, et al. 2010. Towards an
iso standard for dialogue act annotation. In Seventh
conference on International Language Resources
and Evaluation (LREC’10).

Philip R. Cohen and Hector J. Levesque. 1990. Inten-
tion is choice with commitment. Artif. Intell., 42(2-
3):213–261.

Tiansi Dong, Ulrich Furbach, Ingo Glöckner, and Björn
Pelzer. 2011. A natural language question answer-
ing system as a participant in human q&a portals. In
Proc. of the 22nd Int. Joint Conf. on Artificial Intel-
ligence, pages 2430–2435.

Michael P. Georgeff, Barney Pell, Martha E. Pollack,
Milind Tambe, and Michael Wooldridge. 1998. The
belief-desire-intention model of agency. In Proc. of
the 5th Int. Workshop on Intelligent Agents V, Agent
Theories, Architectures, and Languages, pages 1–
10.

Barbara J. Grosz and Sarit Kraus. 1996. Collaborative
plans for complex group action. Artificial Intelli-
gence, 86(2):269 – 357.

Barbara J. Grosz and Candace L. Sidner. 1986. Atten-
tion, intentions, and the structure of discourse. Com-
put. Linguist., 12(3):175–204.

Christos Hadjinikolis, Yiannis Siantos, Sanjay Modgil,
Elizabeth Black, and Peter McBurney. 2013. Op-
ponent modelling in persuasion dialogues. In Proc.
of the 23rd Int. Joint Conf. on Artificial Intelligence,
pages 164–170.

Takayuki Hasegawa, Nobuhiro Kaji, Naoki Yoshinaga,
and Masashi Toyoda. 2013. Predicting and eliciting
addressee’s emotion in online dialogue. In Proc. of
the 51st Annual Meeting of the Association for Com-
putational Linguistics, pages 964–972.

Jerry R. Hobbs. 1978. Why is Discourse Coherent?:
Technical Note 176. Stanford Research Inst., Menlo
Park.

Eduard H. Hovy, 1991. Approaches to the planning of
coherent text, volume 119 of The Kluwer Interna-
tional Series in Engineering and Computer Science,
pages 83–102. Springer.

Eduard H. Hovy, 1993. Automated discourse genera-
tion using discourse structure relations, pages 341–
385. MIT Press.

Anthony Jameson, Bernhard Kipper, Alassane Ndi-
aye, Ralph Schäfer, Joep Simons, Thomas Weis, and
Detlev Zimmermann. 1994. Cooperating to be non-
cooperative: The dialog system pracma. In Proc. of
KI 1994. Springer.

Willem JM Levelt. 1993. Speaking: From intention to
articulation, volume 1. MIT press.

Fangtao Li, Yang Gao, Shuchang Zhou, Xiance Si, and
Decheng Dai. 2013. Deceptive answer prediction
with user preference graph. In ACL (1), pages 1723–
1732. Citeseer.

Karen E Lochbaum. 1998. A collaborative planning
model of intentional structure. Comput. Linguist.,
24(4):525–572.

William C. Mann and Sandra A. Thompson. 1986. As-
sertions from discourse structure. In Proc. of Work-
shop on Strategic comp. natural language, pages
257–270.

Kathleen R. McKeown. 1985. Discourse strategies for
generating natural-language text. Artificial Intelli-
gence, 27(1):1–41.

Johanna D. Moore and Cécile L. Paris. 1993. Plan-
ning text for advisory dialogues: capturing inten-
tional and rhetorical information. Comput. Lin-
guist., 19(4):651–694.

John Nash. 1951. Non-cooperative games. Annals of
Mathematics, 54(2):286–295.

Jum C. Nunnally. 1967. Psychometric Theory.
McGraw-Hill, New York.

Oxford Dictionaries. 2016. ”fair”. Oxford University
Press (http://oxforddictionaries.com).

Michel A. Paquette. 2012. Speech acts, dialogues and
the common ground. In FLAIRS Conference.

Prashant Parikh. 2010. Language and Equilibrium.
MIT Press.

556



Henry Prakken. 2006. Formal systems for persua-
sion dialogue. The Knowledge Engineering Review,
21(02):163–188.

Anand S. Rao and Michael P. Georgeff. 1995. Bdi
agents: From theory to practice. In Proc. of the 1st
Intl. Conf. on Multiagent Systems (ICMAS), pages
312–319. The MIT Press.

Charles Rich and Candace L. Sidner. 1997. Col-
lagen: when agents collaborate with people. In
Proceedings of the first international conference on
Autonomous agents, AGENTS ’97, pages 284–291,
New York, NY, USA. ACM.

Thomas C Schelling. 1960. The strategy of conflict.
Harvard university press.

Herbert A. Simon. 1956. Rational choice and the
structure of the environment. Psychological review,
63(2):129.

Jon Scott Stevens, Anton Benz, Sebastian Reue, and
Ralf Klabunde. 2015. A strategic reasoning model
for generating alternative answers. In ACL (1),
pages 534–542. The Association for Computer Lin-
guistics.

David Traum and Staffan Larsson, 2003. The informa-
tion state approach to dialogue management, pages
325–353. Springer.

David Traum, Stacy C Marsella, Jonathan Gratch, Jina
Lee, and Arno Hartholt. 2008. Multi-party, multi-
issue, multi-strategy negotiation for multi-modal
virtual agents. In Intelligent Virtual Agents, pages
117–130. Springer.

Kees van Deemter. 2009. What game theory can do for
nlg: the case of vague language. In Proceedings of
the 12th European Workshop on Natural Language
Generation, pages 154–161.

A Example dialogue between user and
QA system

In the following, an example dialogue between a
user and the QA system consisting of four ques-
tion answer pairs is given. Questions (Q) are
segmentally constructed by the user. Answers
(A) consisting of text, images, links to webpages
and/or external product offers are generated by the
QA system.

(1) Q: Which tablets are available in lower
price range?

(2) A: In this price range, the following prod-
ucts are available: Lenovo Yoga. Com-
pared to other products in this category,
Lenovo Yoga offers advantages regarding
its price of 198.99e. Currently, a lot of

customers ask for Lenovo Yoga. Regard-
ing the price-performance ratio, the prod-
uct is absolutely recommended.

(3) Q: Is there a discount for Lenovo Yoga?

(4) A: For Lenovo Yoga, there is a discount of
1%. The total comes to 197.00e. Lenovo
Yoga goes with Apple iPod Nano, Sony
NWZ-E585B Walkman, Apple iPhone 5S
and Apple iPhone 4S.

(5) Q: How many tablets offer a standby time
of at least 18h?

(6) A: The following tablets offer this feature:
Lenovo Yoga. How about having a look at
Lenovo Yoga by Lenovo?

(7) Q: Are there any offers for Lenovo Yoga?

(8) A: You can order this product online, as
you can see here:

557


