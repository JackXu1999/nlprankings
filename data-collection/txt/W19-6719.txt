




































iADAATPA Project: Pangeanic use cases

Mercedes Garcı́a-Martı́nez, Amando Estela,
Laurent Bié, Alexander Helle, Manuel Herranz

m.garcia/a.estela/l.bie/a.helle/m.herranz@pangeanic.com

Abstract

The iADAATPA1 project coded as N◦

2016-EU-IA-0132 that ended in February
2019 is made for building of customized,
domain-specific engines for public admin-
istrations from EU Member States. The
consortium of the project decided to use
neural machine translation at the beginning
of the project. This represented a challenge
for all involved, and the positive aspect is
that all public administrations engaged in
the iADAATPA project were able to try,
test and use state-of-the-art neural technol-
ogy with a high level of satisfaction.

One of the main challenges faced by all
partners was data availability. Although
all public administrations had some data
available, it was clearly insufficient for
high-level customization. In some cases,
we had merely a few hundred words or sev-
eral tens of thousand words. Each domain
(field) has its own unique word distribution
and neural machine translation systems are
known to suffer a decrease in performance
when data is out-of-domain.

Pangeanic is a language service provider
(LSP) specialised in natural language pro-
cessing and machine translation. It pro-
vides solutions to cognitive companies,
institutions, translation professionals, and
corporations. The problem faced by
the iADAATPA project at Pangeanic was
twofold:

c© 2019 The authors. This article is licensed under a Creative
Commons 4.0 licence, no derivative works, attribution, CC-
BY-ND.
1http://iadaatpa.com/

1. Availability of training data in some
language combinations.

2. How to successfully train a transla-
tion model on multi-domain data.

Language pairs and domains
Pangeanic’s use cases are for 2 Span-
ish public administrations: (1) Generalitat
Valenciana (regional administration)
translating from Spanish into and out of
English, French, Catalan/Valencian, Ger-
man, Italian, Russian and (2) SEGITTUR2

(tourism administration) translating from
Spanish into and out of English, French,
German, Italian, Portuguese.

Data acquisition For translation from
Spanish to Russian there was no available
in-domain data. Therefore, 2 translators
were contracted as part of the project to
create 30,000 segments of in-domain data,
translating public administrations web-
sites. They also cleaned United Nations
material and post-edited general-domain
data that was previously filtered as in-
domain following the “invitation model”
(Hoang and Sima’an, 2014). For the
other language pairs, the input material
was 30,000 post-edited segments. The
main part of the training corpora (approxi-
mately 75%) was part of Pangeanic’s own
repository harvested through web crawl-
ing and also OpenSubtitles (Tiedemann,
2012). The rest of the corpus was auto-
matically validated synthetic material us-
ing general data from Leipzig (Goldhahn
et al., 2012).

2https://www.segittur.es/es/inicio/index.html

Proceedings of MT Summit XVII, volume 2 Dublin, Aug. 19-23, 2019 | p. 114



Engine customization The data was
cleaned using the Bicleaner tool (Sánchez-
Cartagena et al., 2018). The data was low-
ercased and extra embeddings were added
in order to keep the case information. The
tokenization used was the one provided
by OpenNMT3 and words were divided in
subwords according to the BPE (Sennrich
et al., 2016) approach. The models were
trained with multi-domain data and we im-
proved performance following a domain-
mixing approach (Britz et al., 2017). The
domain information was prepended with
special tokens for each target sequence.
The domain prediction was based only on
the source as the extra token was added at
target-side and there was no need for a-
priori domain information. This approach
allowed the model to improve the quality
for each domain.

Acknowledgements The work reported
in this paper was conducted during the
iADAATPA project, which was funded by
INEA through grant N◦ 2016-EU-IA-0132
as part of the EU’s CEF Telecom Pro-
gramme.

References
Britz, Denny, Quoc Le, and Reid Pryzant. 2017. Effec-

tive domain mixing for neural machine translation.
In Proceedings of the Second Conference on Ma-
chine Translation, pages 118–126. Association for
Computational Linguistics.

Goldhahn, Dirk, Thomas Eckart, and Uwe Quasthoff.
2012. Building large monolingual dictionaries at
the leipzig corpora collection: From 100 to 200 lan-
guages. In Proceedings of the Eighth International
Conference on Language Resources and Evaluation
(LREC12), pages 759–765.

Hoang, Cuong and Khalil Sima’an. 2014. La-
tent domain translation models in mix-of-domains
haystack. In Proceedings of COLING 2014, the 25th
International Conference on Computational Linguis-
tics: Technical Papers, pages 1928–1939. Dublin
City University and Association for Computational
Linguistics.

Sánchez-Cartagena, Vı́ctor M., Marta Bañón, Sergio
Ortiz-Rojas, and Gema Ramı́rez-Sánchez. 2018.
Prompsit’s submission to wmt 2018 parallel cor-
pus filtering shared task. In Proceedings of the

3http://opennmt.net/

Third Conference on Machine Translation, Volume
2: Shared Task Papers, pages 95–103, Brussels, Bel-
gium, October. Association for Computational Lin-
guistics.

Sennrich, Rico, Barry Haddow, and Alexandra Birch.
2016. Neural machine translation of rare words with
subword units. In Proceedings of the 54th Annual
Meeting of the Association for Computational Lin-
guistics (Volume 1: Long Papers), pages 1715–1725,
Berlin, Germany, August. Association for Computa-
tional Linguistics.

Tiedemann, Jörg. 2012. Parallel data, tools and in-
terfaces in OPUS. In Proceedings of the Eighth In-
ternational Conference on Language Resources and
Evaluation (LREC-2012), pages 2214–2218, Istan-
bul, Turkey, May. European Languages Resources
Association (ELRA).

Proceedings of MT Summit XVII, volume 2 Dublin, Aug. 19-23, 2019 | p. 115


