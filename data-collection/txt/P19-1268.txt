



















































Aiming beyond the Obvious: Identifying Non-Obvious Cases in Semantic Similarity Datasets


Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 2792–2798
Florence, Italy, July 28 - August 2, 2019. c©2019 Association for Computational Linguistics

2792

Aiming beyond the Obvious:
Identifying Non-Obvious Cases in Semantic Similarity Datasets

Nicole Peinelt1,2 and Maria Liakata1,2 and Dong Nguyen1,3
1The Alan Turing Institute, London, UK
2University of Warwick, Coventry, UK

3Utrecht University, Utrecht, The Netherlands
{n.peinelt, m.liakata}@warwick.ac.uk, dnguyen@turing.ac.uk

Abstract

Existing datasets for scoring text pairs in terms
of semantic similarity contain instances whose
resolution differs according to the degree of
difficulty. This paper proposes to distinguish
obvious from non-obvious text pairs based
on superficial lexical overlap and ground-truth
labels. We characterise existing datasets in
terms of containing difficult cases and find that
recently proposed models struggle to capture
the non-obvious cases of semantic similarity.
We describe metrics that emphasise cases of
similarity which require more complex infer-
ence and propose that these are used for eval-
uating systems for semantic similarity.

1 Introduction

Modelling semantic similarity between a pair of
texts is a fundamental task in NLP with a wide
range of applications (Baudiš et al., 2016). One
area of active research is Community Question
Answering (CQA) (Nakov et al., 2017; Bonadi-
man et al., 2017), which is concerned with the au-
tomatic answering of questions based on user gen-
erated content from Q&A websites (e.g. StackEx-
change) and requires modelling the semantic simi-
larity between question and answer pairs. Another
well-studied task is paraphrase detection (Socher
et al., 2011; He et al., 2015; Tomar et al., 2017),
which models the semantic equivalence between a
pair of sentences.

Evaluation for such tasks has primarily fo-
cused on metrics, such as mean average preci-
sion (MAP), F1 or accuracy, which give equal
weights to all examples, regardless of their diffi-
culty. However, as illustrated by the examples in
Table 1, not all items within text pair similarity
datasets are equally difficult to resolve.

Recent work has shown the need to better un-
derstand limitations of current models and datasets
in natural language understanding (Wadhwa et al.,

id case documents

160174 Po
what‘s the origin of the word o‘clock?
what is the origin of the word o‘clock?

115695 Pn
which is the best way to learn coding?
how do you learn to program?

193190 No
what are the range of careers in
biotechnology in indonesia?
how do you tenderize beef stew meat?

268368 Nn
what is meant by ‘e‘ in mathematics?
what is meant by mathematics?

Table 1: Examples for difficulty cases from the devel-
opment set of the Quora dataset. o=obvious, n=non-
obvious, N=negative label, P=positive label

2018a; Rajpurkar et al., 2018). For example,
Kaushik and Lipton (2018) showed that models
sometimes exploit dataset properties to achieve
high performance even when crucial task infor-
mation is withheld, and Gururangan et al. (2018)
demonstrated that model performance is inflated
by annotation artefacts in natural language infer-
ence tasks.

In this paper, we analyse current datasets and re-
cently proposed models by focusing on item diffi-
culty based on shallow lexical overlap. Rodrigues
et al. (2018) found declarative CQA sentence pairs
to be more difficult to resolve than interrogative
pairs as the latter contain more cases of superfi-
cial overlap. In addition, Wadhwa et al. (2018b)
showed that competitive neural reading compre-
hension models are susceptible to shallow patterns
(e.g. lexical overlap). Our study digs deeper into
these findings to investigate the properties of cur-
rent text pair similarity datasets with respect to
different levels of difficulty and evaluates models
based on how well they can resolve difficult cases.

We make the following contributions:

1. We propose a criterion to distinguish between
obvious and non-obvious examples in text



2793

pair similarity datasets (section 4).

2. We characterise current datasets in terms of
the extent to which they contain obvious vs.
non-obvious items (section 4).

3. We propose alternative evaluation metrics
based on example difficulty (section 5)
and provide a reference implementation at
https://github.com/wuningxi/LexSim.

2 Datasets and Tasks

We selected well-known benchmark datasets dif-
fering in size (small vs. large), document length
(single sentence vs. multi-sentence), document
types (declarative vs. interrogative) and tasks (an-
swer ranking vs. paraphrase detection vs. similar-
ity scoring), see Table 2.

SemEval The SemEval Community Question
Answering (CQA) dataset (Nakov et al., 2015,
2016, 2017) contains posts from the online forum
Qatar Living. The task is to rank relevant posts
above non-relevant ones. Each subtask involves
an initial post and 10 possibly relevant posts with
binary annotations. Task A contains questions and
comments from the same thread, task B involves
question paraphrases, and task C is similar to A
but contains comments from an external thread.

MSRP The Microsoft Research Paraphrase cor-
pus (MSRP) is a popular paraphrase detection
dataset, consisting of pairs of sentences with bi-
nary judgments (Dolan and Brockett, 2005).

Name Task Type Size

SemEval (A) answer ranking rank 26K
(B) paraphrase ranking rank 4K
(C) answer ranking rank 47K

Quora paraphrase detection class 404K
MSRP paraphrase detection class 5K
STS similarity scoring regr 8K

Table 2: Selected text pair similarity data sets.
Size as number of text pairs. rank=ranking task,
class=classification task, regr=regression task.

Quora The Quora duplicate questions dataset
contains a large number of question pairs with bi-
nary labels1. The task is to predict whether two
questions are paraphrases, similar to Task B of Se-
mEval, but it is framed as a classification rather
than a ranking problem. We use the same train-
ing / development / test set partition as Wang et al.
(2017).

STS The Semantic Textual Similarity Bench-
mark (STS) dataset (Cer et al., 2017) consists
of a selection of STS SemEval shared tasks
(2012-2017). It contains sentence pairs annotated
with continuous semantic relatedness scores on a
scale from 0 (low similarity) to 5 (high similarity).

In this paper, we focus on predicting the seman-
tic similarity between two text snippets in a binary
classification scenario, as the ranking scenario is
only applicable to some of the datasets. Binary
labels are already provided for all tasks except for

1https://engineering.quora.com/Semantic-Question-
Matching-with-Deep-Learning

0.0 0.2 0.4 0.6 0.8 1.0
JSD

0
250
500
750

1000
1250
1500
1750
2000
2250

Nu
m

be
r o

f t
ex

t p
ai

rs

Semeval A
Negative
Positive

0.0 0.2 0.4 0.6 0.8 1.0
JSD

0

100

200

300

400

500

Nu
m

be
r o

f t
ex

t p
ai

rs

Semeval B
Negative
Positive

0.0 0.2 0.4 0.6 0.8 1.0
JSD

0

1000

2000

3000

4000

5000

6000

Nu
m

be
r o

f t
ex

t p
ai

rs

Semeval C
Negative
Positive

0.0 0.2 0.4 0.6 0.8 1.0
JSD

0

2500

5000

7500

10000

12500

15000

17500

Nu
m

be
r o

f t
ex

t p
ai

rs

Quora
Negative
Positive

0.0 0.2 0.4 0.6 0.8 1.0
JSD

0

100

200

300

400

500

Nu
m

be
r o

f t
ex

t p
ai

rs

MSRP
Negative
Positive

0.0 0.2 0.4 0.6 0.8 1.0
JSD

0

100

200

300

400

500

600

Nu
m

be
r o

f t
ex

t p
ai

rs

STS
Negative
Positive

Figure 1: Lexical divergence distribution by labels across datasets. JSD=Jensen-Shannon divergence.



2794

STS. In the case of STS, we convert the scores into
binary labels. Based on the description of the re-
latedness scores in Cer et al. (2017), we assign a
positive label if relatedness≥ 4 and a negative one
otherwise to use a similar criterion as in the other
datasets.

3 Lexical divergence in current datasets

To characterise the datasets, we represent the text
pairs as two distributions over words and measure
their lexical divergence using Jensen-Shannon di-
vergence (JSD) (Lin, 1991).2 Figure 1 shows the
entire JSD distribution by label for each dataset.

The datasets differ with respect to the degree
of lexical divergence they contain: The three Se-
mEval CQA datasets show a high degree of lexical
divergence (majority > 0.5), especially in the ex-
ternal QA scenario (task C). Text pairs in MSRP
tend to have low-medium JSD scores (majority
< 0.6), while items in Quora and STS show the
widest range of lexical divergence (see also Ap-
pendix A). Overall, pairs with negative labels tend
to have higher JSD scores than pairs with positive
labels. Especially in Quora, MSRP and STS, dis-
tinct distributions emerge for positive vs. negative
labels, providing direct clues for label assignment.

4 Distinguishing between obvious and
non-obvious examples

As shown, pairs with high lexical divergence tend
to have a negative label in the above datasets (e.g.
No in Table 1), while low lexical divergence is as-
sociated with a positive label (e.g. Po in Table 1).
Intuitively, these are cases which should be rela-
tively easy to identify. More difficult are text pairs
with a positive label but high lexical divergence
(e.g. Pn in Table 1), or a negative label despite low
lexical divergence (e.g. Nn in Table 1). We use Ta-
ble 3 to categorise cases in terms of their difficulty
level.

positive label negative label

low div obvious pos (Po) non-obvious neg (Nn)
high div non-obvious pos (Pn) obvious neg (No)

Table 3: Defining obvious and non-obvious similarity
cases based on labels and lexical overlap.

2We also calculated set-based similarity metrics (Jaccard
Index and Dice Coefficient) and found consistent results with
JSD, but give preference to the distribution-based metric
which is more natural for text. Due to space restrictions, we
only report JSD in this paper.

Fleiss’ Kappa Avg. time per pair Instances

Po 0.6429 11.58s 35
Pn 0.0878 11.68s 15
No 0.3886 12.50s 34
Nn 0.0892 13.83s 16

total 0.6267 12.27s 100

Table 4: Statistics for manual annotation on Quora.
o=obvious, n=non-obvious, N=negative, P=positive

SemEval Quora MSRP STS
A B C

Po 5893 1162 2492 107612 2398 1597
Pn 4428 531 1590 41691 1502 409
No 8842 1843 22155 160410 1398 3900
Nn 7377 1213 21253 94632 503 2719

o 56 63 52 66 65 64

m 0.80 0.79 0.82 0.53 0.52 0.52

Table 5: Difficulty case splits across datasets (train, dev
and test combined). o=obvious, m=median JSD.

Pairs are categorised into high and low lexical
divergence categories by comparing their JSD
score to the median of the entire JSD distribu-
tion in order to account for differences between
datasets (>median: high div, ≤median: low div).
To verify if this automatic difficulty distinction
corresponds with real-world difficulty, the authors
of the study annotated the semantic relatedness of
100 random pairs from the Quora development set
and measured inter-annotator agreement based on
Fleiss’ Kappa. The agreement for non-obvious
cases (Pn and Nn) is significantly lower (p-value<
0.01 with permutation test) than for obvious cases
(Po and No) and the average annotation time per
item is longer for non-obvious cases (Table 4),
confirming the validity of this distinction.

Table 5 shows the number of instances in the
four cases across datasets. In all of the analysed
datasets, there are more obvious positives (Po)
than non-obvious positives (Pn) and more obvious
negatives (No) than non-obvious negatives (Nn).
All obvious cases combined (Po+No) make up
more than 50% of pairs across all datasets.

5 Evaluating model predictions based on
difficulty

We now use this categorisation for the purpose of
model evaluation (Tables 6-8).3 We calculate the

3Due to the lack of openly available model pre-
diction files, we only present our analysis for the Se-



2795

KeLP BeihangMSRA
IIT

UHH ECNU bunji EICA
Swiss
Alps

FuRong
Wang FA3L

Snow
Man

ran-
dom

TPRo 0.652 1.000 0.800 0.790 0.681 0.328 0.333 0.562 0.691 0.677 0.501
TPRn 0.496 1.000 0.676 0.636 0.575 0.269 0.223 0.399 0.478 0.469 0.499
TNRo 0.909 0.000 0.731 0.877 0.894 0.959 0.984 0.913 0.787 0.900 0.515
TNRn 0.908 0.000 0.676 0.820 0.851 0.953 0.950 0.892 0.751 0.757 0.536

F1o 0.751 0.682 0.781 0.829 0.765 0.480 0.494 0.684 0.731 0.765 0.513
F1n 0.628 0.686 0.686 0.707 0.672 0.410 0.352 0.533 0.560 0.555 0.519

F1 0.698 0.684 0.739 0.777 0.725 0.450 0.433 0.621 0.659 0.673 0.516
MAP 0.884 0.882 0.869 0.867 0.866 0.865 0.862 0.843 0.834 0.818 0.623

Table 6: Proposed evaluation metrics for top 10 primary submissions on SemEval Task A. The systems are ordered
in columns according to their MAP ranking. Bold indicates the highest value for each metric. We indicate the 2nd

and 3rd systems based on F1n and F1.

Sim
Bow

LearningTo
Question KeLP Talla

Beihang
MSRA

NLM
NIH

Uin-
suska

TiTech

IIT
UHH

SCIR
QA FA3L

ran-
dom

TPRo 0.976 1.000 0.920 0.760 1.000 0.880 0.752 0.704 0.912 0.448 0.552
TPRn 0.842 1.000 0.632 0.763 1.000 0.500 0.421 0.737 0.842 0.263 0.395
TNRo 0.609 0.000 0.831 0.684 0.000 0.841 0.858 0.682 0.709 0.861 0.495
TNRn 0.197 0.000 0.432 0.467 0.000 0.397 0.552 0.403 0.352 0.756 0.521

F1o 0.604 0.383 0.746 0.548 0.383 0.736 0.681 0.516 0.641 0.473 0.348
F1n 0.198 0.195 0.199 0.247 0.195 0.154 0.164 0.221 0.234 0.160 0.147

F1 0.424 0.312 0.506 0.426 0.312 0.473 0.467 0.390 0.464 0.365 0.280
MAP 0.472 0.469 0.467 0.457 0.448 0.446 0.434 0.431 0.427 0.422 0.298

Table 7: Proposed evaluation metrics for top 10 primary submissions on SemEval Task B.

true positive rate TPR (for Po and Pn) and true neg-
ative rate TNR (for No and Nn) to analyse model
performance within each difficulty category. In the
three SemEval 2017 CQA tasks, all systems per-
form worse on the hard cases compared to the ob-
vious cases (TPRn < TPRo and TNRn < TNRo),
while there are only minor changes in the ran-
dom baseline which predicts all classes with equal
probability. To compare how well models do on

IIT
UHH bunji KeLP EICA

ran-
dom

TPRo 0.570 0.246 0.911 0.006 0.520
TPRn 0.358 0.045 0.836 0.000 0.433
TNRo 0.898 0.991 0.720 0.998 0.502
TNRn 0.779 0.965 0.538 0.999 0.502

F1o 0.283 0.339 0.209 0.011 0.076
F1n 0.047 0.028 0.054 0.000 0.027

F1 0.144 0.197 0.121 0.008 0.053
MAP 0.155 0.147 0.144 0.135 0.058

Table 8: Proposed evaluation metrics for top 4 primary
submissions on SemEval Task C.

mEval CQA Tasks based on prediction files obtained from
http://alt.qcri.org/semeval2017/task3/index.php?id=results.

obvious vs. non-obvious cases overall, we com-
pute F1 scores for obvious cases (Po and No) as
F1o and non-obvious cases (Pn and Nn) as F1n sep-
arately. This is necessary as the high percentage of
obvious cases (observed in section 4) can inflate
the overall F1 score. F1n scores are consistently
lower than the F1o scores. This difference is espe-
cially pronounced in Task B, which contained the
highest proportion of obvious cases (62%) of the
SemEval tasks. Using the non-obvious F1 scores
results in a different ranking compared to the of-
ficial SemEval evaluation metrics (F1 or MAP),
even resulting in a change in the highest ranked
system in Task B (Talla instead of KeLP or Sim-
Bow) and C (KeLP instead of bunji or IIT-UHH).

6 Conclusion

We present an automated criterion for automat-
ically distinguishing between easy and difficult
items in text pair similarity prediction tasks. We
find that more than 50% of cases in current
datasets are relatively obvious. Recently pro-
posed models perform significantly worse on non-
obvious cases compared to obvious cases. In or-



2796

der to encourage the development of models that
perform well on difficult items, we propose to
use non-obvious F1 scores (F1n) as a complemen-
tary ranking metric for model evaluation. We also
recommend publishing prediction files along with
models to facilitate error analysis.

Acknowledgments

This work was supported by The Alan Turing In-
stitute under the EPSRC grant EP/N510129/1.

References

Petr Baudiš, Jan Pichl, Tomáš Vyskočil, and Jan
Šedivý. 2016. Sentence Pair Scoring: Towards Uni-
fied Framework for Text Comprehension. arXiv
preprint arXiv:1603.06127.

Daniele Bonadiman, Antonio Uva, and Alessandro
Moschitti. 2017. Effective Shared Representations
with Multitask Learning for Community Question
Answering. In Proceedings of the Conference of the
European Chapter of the Association for Computa-
tional Linguistics (EACL), pages 726–732, Valencia,
Spain. Association for Computational Linguistics.

Daniel Cer, Mona Diab, Eneko Agirre, Inigo Lopez-
Gazpio, and Lucia Specia. 2017. SemEval-2017
Task 1: Semantic Textual Similarity Multilingual
and Crosslingual Focused Evaluation. In Proceed-
ings of the 11th International Workshop on Seman-
tic Evaluation (SemEval-2017), pages 1–14, Van-
couver, Canada. Association for Computational Lin-
guistics.

William B. Dolan and Chris Brockett. 2005. Auto-
matically Constructing a Corpus of Sentential Para-
phrases. In Proceedings of the Third International
Workshop on Paraphrasing (IWP@IJCNLP), pages
9–16, Jeju Island, Korea. Asian Federation of Natu-
ral Language Processing.

Suchin Gururangan, Swabha Swayamdipta, Omer
Levy, Roy Schwartz, Samuel Bowman, and Noah A.
Smith. 2018. Annotation Artifacts in Natural Lan-
guage Inference Data. In Proceedings of the 2018
Conference of the North American Chapter of the
Association for Computational Linguistics: Human
Language Technologies, Volume 2 (Short Papers),
pages 107–112, New Orleans, Louisiana. Associa-
tion for Computational Linguistics.

Hua He, Kevin Gimpel, and Jimmy Lin. 2015. Multi-
Perspective Sentence Similarity Modeling with Con-
volutional Neural Networks. In Proceedings of the
2015 Conference on Empirical Methods in Natu-
ral Language Processing (EMNLP), pages 1576–
1586, Lisbon, Portugal. Association for Computa-
tional Linguistics.

Divyansh Kaushik and Zachary C Lipton. 2018. How
Much Reading Does Reading Comprehension Re-
quire? A Critical Investigation of Popular Bench-
marks. In Proceedings of the 2018 Conference on
Empirical Methods in Natural Language Process-
ing, pages 5010–5015, Brussels, Belgium. Associ-
ation for Computational Linguistics.

Jianhua Lin. 1991. Divergence Measures based on the
Shannon Entropy. IEEE Transactions on Informa-
tion theory, 37(1):145–151.

Preslav Nakov, Doris Hoogeveen, Llúis Màrquez,
Alessandro Moschitti, Hamdy Mubarak, Timothy
Baldwin, and Karin Verspoor. 2017. SemEval-
2017 Task 3: Community Question Answering. In
Proceedings of the 11th International Workshop on
Semantic Evaluation (SemEval@ACL 2017), pages
27–48, Vancouver, Canada. Association for Compu-
tational Linguistics.

Preslav Nakov, Lluis Marquez, Alessandro Mos-
chitti, Walid Magdy, Hamdy Mubarak, Abed Al-
hakim Freihat, James Glass, and Bilal Randeree.
2016. SemEval-2016 Task 3: Community Ques-
tion Answering. In Proceedings of the 10th
International Workshop on Semantic Evaluation
(SemEval@NAACL-HLT 2016), pages 525–545,
San Diego, California. Association for Computa-
tional Linguistics.

Preslav Nakov, Lluis Marquez, Magdy Walid, Alessan-
dro Moschitti, James Glass, and Bilal Randeree.
2015. SemEval-2015 task 3: Answer Selection in
Community Question Answering. In Proceedings of
the 9th International Workshop on Semantic Eval-
uation (SemEval@NAACL-HLT 2015), pages 269–
281, Denver, Colorado. Association for Computa-
tional Linguistics.

Pranav Rajpurkar, Robin Jia, and Percy Liang. 2018.
Know What You Don’t Know: Unanswerable Ques-
tions for SQuAD. In Proceedings of the 56th Annual
Meeting of the Association for Computational Lin-
guistics (Short Papers), pages 784–789, Melbourne,
Australia. Association for Computational Linguis-
tics.

Joao Rodrigues, Chakaveh Saedi, Antonio Branco, and
Joao Silva. 2018. Semantic Equivalence Detec-
tion: Are Interrogatives Harder than Declaratives?
In Proceedings of the 11th International Confer-
ence on Language Resources and Evaluation, pages
3248–3253, Miyazaki, Japan. European Language
Resources Association.

Richard Socher, Eric H Huang, Jeffrey Pennin, Christo-
pher D Manning, and Andrew Y Ng. 2011. Dy-
namic Pooling and Unfolding Recursive Autoen-
coders for Paraphrase Detection. In Advances in
Neural Information Processing Systems 24: 25th
Annual Conference on Neural Information Pro-
cessing Systems (NIPS), pages 801–809, Granada,
Spain.

https://doi.org/10.18653/v1/S17-2001
https://doi.org/10.18653/v1/S17-2001
https://doi.org/10.18653/v1/S17-2001
https://doi.org/10.18653/v1/N18-2017
https://doi.org/10.18653/v1/N18-2017
https://doi.org/10.18653/v1/D15-1181
https://doi.org/10.18653/v1/D15-1181
https://doi.org/10.18653/v1/D15-1181


2797

Gaurav Singh Tomar, Thyago Duque, Oscar
Täckström, Jakob Uszkoreit, and Dipanjan Das.
2017. Neural Paraphrase Identification of Questions
with Noisy Pretraining. In Proceedings of the First
Workshop on Subword and Character Level Models
in NLP, pages 142–147, Copenhagen, Denmark.
Association for Computational Linguistics.

Soumya Wadhwa, Khyathi Raghavi Chandu, and Eric
Nyberg. 2018a. Comparative Analysis of Neural
QA models on SQuAD. In Proceedings of the Work-
shop on Machine Reading for Question Answering,
pages 89–97, Melbourne, Australia. Association for
Computational Linguistics.

Soumya Wadhwa, Varsha Embar, Matthias Grabmair,
and Eric Nyberg. 2018b. Towards Inference-
Oriented Reading Comprehension: ParallelQA. In
Proceedings of the Workshop on Generalization in
the Age of Deep Learning, pages 1–7, New Orleans,
Louisiana. Association for Computational Linguis-
tics.

Zhiguo Wang, Wael Hamza, and Radu Florian. 2017.
Bilateral Multi-Perspective Matching for Natural
Language Sentences. In Proceedings of the Twenty-
Sixth International Joint Conference on Artificial In-
telligence (IJCAI), pages 4144–4150, Melbourne,
Australia.

https://doi.org/10.18653/v1/W17-4121
https://doi.org/10.18653/v1/W17-4121
http://arxiv.org/abs/1806.06972
http://arxiv.org/abs/1806.06972
https://doi.org/10.18653/v1/W18-1001
https://doi.org/10.18653/v1/W18-1001


2798

A Appendix

0.0 0.2 0.4 0.6 0.8 1.0
JSD

0

500

1000

1500

2000

2500

Nu
m

be
r o

f t
ex

t p
ai

rs

Semeval A
train_large
test2016
test2017

0.0 0.2 0.4 0.6 0.8 1.0
JSD

0

100

200

300

400

500

Nu
m

be
r o

f t
ex

t p
ai

rs

Semeval B
train_large
test2016
test2017

0.0 0.2 0.4 0.6 0.8 1.0
JSD

0

1000

2000

3000

4000

Nu
m

be
r o

f t
ex

t p
ai

rs

Semeval C
train_large
test2016
test2017

0.0 0.2 0.4 0.6 0.8 1.0
JSD

0

5000

10000

15000

20000

Nu
m

be
r o

f t
ex

t p
ai

rs

Quora
train
dev
test

0.0 0.2 0.4 0.6 0.8 1.0
JSD

0
50

100
150
200
250
300
350
400

Nu
m

be
r o

f t
ex

t p
ai

rs

MSRP
train
dev
test

0.0 0.2 0.4 0.6 0.8 1.0
JSD

0

100

200

300

400

500

Nu
m

be
r o

f t
ex

t p
ai

rs

STS
train
dev
test

Figure 2: Lexical divergence distribution by training, development and test set across different semantic similarity
datasets. JSD=Jensen-Shannon divergence.


