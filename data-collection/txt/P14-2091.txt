



















































A joint inference of deep case analysis and zero subject generation for Japanese-to-English statistical machine translation


Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 557–562,
Baltimore, Maryland, USA, June 23-25 2014. c©2014 Association for Computational Linguistics

A joint inference of deep case analysis and zero subject generation for
Japanese-to-English statistical machine translation

Taku Kudo, Hiroshi Ichikawa, Hideto Kazawa
Google Japan

{taku,ichikawa,kazawa}@google.com

Abstract

We present a simple joint inference of
deep case analysis and zero subject gener-
ation for the pre-ordering in Japanese-to-
English machine translation. The detec-
tion of subjects and objects from Japanese
sentences is more difficult than that from
English, while it is the key process to gen-
erate correct English word orders. In addi-
tion, subjects are often omitted in Japanese
when they are inferable from the context.
We propose a new Japanese deep syntac-
tic parser that consists of pointwise proba-
bilistic models and a global inference with
linguistic constraints. We applied our new
deep parser to pre-ordering in Japanese-to-
English SMT system and show substantial
improvements in automatic evaluations.

1 Introduction

Japanese to English translation is known to be one
of the most difficult language pair for statistical
machine translation (SMT). It has been widely be-
lieved for years that the difference of word or-
ders, i.e., Japanese is an SOV language, while En-
glish is an SVO language, makes the English-to-
Japanese and Japanese-to-English translation dif-
ficult. However, simple, yet powerful pre-ordering
techniques have made this argument a thing of the
past (Isozaki et al., 2010b; Komachi et al., 2006;
Fei and Michael, 2004; Lerner and Petrov, 2013;
Wu et al., 2011; Katz-Brown and Collins, 2008;
Neubig et al., 2012; Hoshino et al., 2013). Pre-
ordering processes the source sentence in such a
way that word orders appear closer to their final
positions on the target side.

While many successes of English-to-Japanese
translation have been reported recently, the quality
improvement of Japanese-to-English translation is
still small even with the help of pre-ordering (Goto

et al., 2013). We found that there are two ma-
jor issues that make Japanese-to-English transla-
tion difficult. One is that Japanese subject and ob-
ject cannot easily be identified compared to En-
glish, while their detections are the key process
to generate correct English word orders. Japanese
surface syntactic structures are not always corre-
sponding to their deep structures, i.e., semantic
roles. The other is that Japanese is a pro-drop lan-
guage in which certain classes of pronouns may
be omitted when they are pragmatically inferable.
In Japanese-to-English translation, these omitted
pronouns have to be generated properly.

There are several researches that focused on the
pre-ordering with Japanese deep syntactic analysis
(Komachi et al., 2006; Hoshino et al., 2013) and
zero pronoun generation (Taira et al., 2012) for
Japanese-to-English translation. However, these
two issues have been considered independently,
while they heavily rely on one another.

In this paper, we propose a simple joint infer-
ence which handles both Japanese deep structure
analysis and zero pronoun generation. To the best
of our knowledge, this is the first study that ad-
dresses these two issues at the same time.

This paper is organized as follows. First, we de-
scribe why Japanese-to-English translation is dif-
ficult. Second, we show the basic idea of this
work and its implementation based on pointwise
probabilistic models and a global inference with
an integer linear programming (ILP). Several ex-
periments are employed to confirm that our new
model can improve the Japanese to English trans-
lation quality.

2 What makes Japanese-to-English
translation difficult?

Japanese syntactic relations between arguments
and predicates are usually specified by particles.
There are several types of particles, but we focus
on が (ga), を (wo) and は (wa) for the sake of

557



Table 1: An example of difficult sentence for pars-
ing

Sentence: 今日 は お酒 が 飲める.
Gloss: today wa TOP liquor ga NOM can drink.
Translation: (I) can drink liquor today.

simplicity 1.

• ga is usually a subject marker. However, it
becomes an object marker if the predicate has
a potential voice type, which is usually trans-
lated into can, be able to, want to, or would
like to.

• wo is an object marker.
• wa is a topic case marker. The topic can be

anything that a speaker wants to talk about. It
can be subject, object, location, time or any
other grammatical elements.

We cannot always identify Japanese subject and
object only by seeing the surface case markers ga,
wo and wa. Especially the topic case marker is
problematic, since there is no concept of topic in
English. It is necessary to get a deep interpretation
of topic case markers in order to develop accurate
Japanese-to-English SMT systems.

Another big issue is that Japanese subject (or
even an object) can be omitted when they can
pragmatically be inferable from the context. Such
a pronoun-dropping is not a unique phenomenon
in Japanese actually. For instance, Spanish also
allows to omit pronouns. However, the inflec-
tional suffix of Spanish verbs include a hint of the
person of the subject. On the other hand, infer-
ring Japanese subjects is more difficult than Span-
ish, since Japanese verbs usually do not have any
grammatical cues to tell the subject type.

Table 1 shows an example Japanese sentence
which cannot be parsed only with the surface
structure. The second token wa specifies the rela-
tion between今日 (today) and飲める (can drink).
Human can easily tell that the relation of them is
not a subject but an adverb (time). The topic case
marker wa implies that the time when the speaker
drinks liquor is the focus of this sentence. The
4th token ga indicates the relation between お酒
(liquor) and 飲める (can drink). Since the predi-
cate has a potential voice (can drink), the ga par-
ticle should be interpreted as an object here. In

1Other case markers are less frequent than these three
markers

this sentence, the subject is omitted. In general, it
is unknown who speaks this sentence, but the first
person is a natural interpretation in this context.

Another tricky phenomenon is that detecting
voice type is not always deterministic. There
are several ways to generate a potential voice in
Japanese, but we usually put the suffix wordれる
(reru) or られる (rareru) after predicates. How-
ever, these suffix words are also used for a passive
voice.

In summary, we can see that the following
four factors are the potential causes that make the
Japanese parsing difficult.

• Japanese voice type detection is not straight-
forward. reru or rareru are used either for
passive or potential voice.

• surface case ga changes its interpretation
from subject to object when the predicate has
a potential voice.

• topic case marker wa is used as a topic case
marker which doesn’t exist in English. Topic
is either subject, object or any grammatical
elements depending on the context.

• Japanese subject is often omitted when it is
inferable from the context. There is no cue to
tell the subject person in verb suffix (inflec-
tion) like in Spanish verbs

We should note that they are not always inde-
pendent issues. For instance, the deep case detec-
tion helps to tell the voice type, and vice versa.

Another note is that they are unique issues
observed only in Japanese-to-English translation.
In English-to-Japanese translation, it is accept-
able to generate Japanese sentences that do not
use Japanese topic markers wa. Also, generating
Japanese pronoun from English pronoun is accept-
able, although it sounds redundant and unnatural
for native speakers.

3 A joint inference of deep case analysis
and zero subject generation

3.1 Probabilistic model over
predicate-argument structures

Our deep parser runs on the top of a dependency
parse tree. First, it extracts all predicates and their
arguments from a dependency tree by using man-
ual rules over POS tags. Since our pre-ordering
system generates the final word orders from a
labeled dependency tree, we formalize our deep

558



parsing task as a simple labeling problem over de-
pendency links, where the label indicates the deep
syntactic roles between head and modifier.

We here define a joint probability over a predi-
cate and its arguments as follows:

P (p, z, v, A, S,D) (1)

where

• p: a predicate
• z: a zero subject candidate for p. z ∈ Z =
{I, you, we, it, he/she, imperative, already exists}

• v: voice type of the predicate p. v ∈ V =
{active, passive, potential}

• ak ∈ A: k-th argument which modifies or is
modified by the predicate2.

• dk ∈ D: deep case label which represents a
deep relation between ak and p. d ∈ { sub-
ject, object, other }, where other means that
deep case is neither subject nor object.

• sk ∈ S: surface relation (surface case
marker) between ak and p.

We assume that a predicate p is independent
from other predicates in a sentence. This assump-
tion allows us to estimate the deep structures of p
separately, with no regard to which decisions are
made in other predicates.

An optimal zero subject label z, deep cases D,
and voice type v for a given predicate p can be
solved as the following optimization problem.

〈ẑ, v̂, D̂〉 = argmax
z,v,D

P (p, z, v, A, S, D)

Since the inference of this joint probability is diffi-
cult, we decompose P (p, z, v, A, S, D) into small
independent sub models:

P (p, z, v, A, S, D) ≈
Pz(z|p,A, S)Pv(v|p,A, S)
Pd(D|p, v, A, S)P (p,A, S) (2)

We do not take the last term P (p, A, S) into con-
sideration, since it is constant for the optimization.
In the next sections, we describe how these proba-
bilities Pz , Pd, and Pv are computed.

2Generally, an argument modifies a predicate, but in rela-
tive clauses, a predicate modifies an argument

3.1.1 Zero subject model: Pz(z|p,A, S)
This model estimates the syntactic zero subject 3

of the predicate p. For instance, z= I means that the
subject of p is omitted and its type is first person.
z=imperative means that we do not need to aug-
ment a subject because the predicate is imperative.
z=already exists means that a subject already ap-
pears in the sentence. A maximum entropy classi-
fier is used in our zero subject model, which takes
the contextual features extracted from p, A, and S.

3.1.2 Voice type model: Pv(v|p, A, S)
This model estimates the voice type of a predicate.
We also use a maximum entropy classifier for this
model. This classifier is used only when the predi-
cate has the ambiguous suffix reru or rareru. If the
predicate does not have any ambiguous suffix, this
model returns pre-defined voice types with with
very high probabilities.

3.1.3 Deep case model: Pd(D|p, v,A, S)
This model estimates the deep syntactic role be-
tween a predicate p and its arguments A. This
model helps to resolve the deep cases when their
surface cases are topic. We define Pd as follows
after introducing an independent assumption over
predicate-argument structures:

P (D|p, v, A, S) ≈∏
i

[max(p(di|ai, p) − m(si, di, v), δ)].

p(d|a, p) models the deep relation between p and
a. We use a maximum likelihood estimation for
p(d|a, p):

p(d = subj|a, p) = freq(s = ga, a, active form of p)
freq(a, active form of p)

p(d = obj|a, p) = freq(s = wo, a, active form of p)
freq(a, active form of p)

,

where freq(s = ga, a, active form of p) is the
frequency of how often an argument a and p ap-
pears with the surface case ga. The frequencies
are aggregated only when the predicate appear in
active voice. If the voice type is active, we can
safely assume that the surface cases ga and wo
correspond to subject and object respectively. We
compute the frequencies from a large amount of
auto-parsed data.

m(s, d, v) is a non-negative penalty variable de-
scribing how the deep case d generates the sur-
face case s depending on the voice type v. Since

3Here syntactic subject means the subject which takes the
voice type into account.

559



the number of possible surface cases, deep cases,
and voice types are small, we define this penalty
manually by referring to the Japanese grammar
book (descriptive grammar research group, 2009).
We use these manually defined penalties in order
to put more importance on syntactic preferences
rather than those of semantics. Even if a predicate-
augment structure is semantically irrelevant, we
take this structure as long as it is syntactically cor-
rect in order to avoid SMT from generating liberal
translations.

δ is a very small positive constant to avoid zero
probability.

3.2 Joint inference with linguistic constraints
Our initial model (2) assumes that zero subjects
and deep cases are generated independently. How-
ever, this assumption does not always capture
real linguistic phenomena. English is a subject-
prominent language in which almost all sentences
(or predicates) must have a subject. This implies
that it is more reasonable to introduce strong lin-
guistic constraints to the final solution for pre-
ordering, which are described as follows:

• Subject is a mandatory role. A subject must
be inferred either by zero subject or deep case
model 4. When the voice type is passive, an
object role in D is considered as a syntactic
subject.

• A predicate can not have multiple subjects
and objects respectively.

These two constraints avoid the model from in-
ferring syntactically irrelevant solutions.

In order to find the result with the constraints
above, we formalize our model as an integer lin-
ear programming, ILP. Let {x1, , ..., xn} be bi-
nary variables, i.e., xi ∈ {0, 1}. xi corresponds
to the binary decisions in our model, e.g., xk =
1 if di = subj and v = active. Let {p1, ..., pn} be
probability vector corresponding to the binary de-
cisions. ILP can be formalized as a mathematical
problem, in which the objective function and the
constraints are linear:

{x̂1, ..., x̂n} = argmax
{x1,...,xn}∈{0,1}n

n∑
i=1

log(pi)xi

s.t. linear constraints over {x1, .., xn}.
After taking the log of (2), our optimization model
can be converted into an ILP. Also, the constraints

4imperative is also handled as an invisible subject

described above can be represented as linear equa-
tions over binary variables X . We leave the details
of the representations to (Punyakanok et al., 2004;
Iida and Poesio, 2011).

3.3 Japanese pre-ordering with deep parser

We use a simple rule-based approach to make pre-
ordered Japanese sentences from our deep parse
trees, which is similar to the algorithms described
in (Komachi et al., 2006; Katz-Brown and Collins,
2008; Hoshino et al., 2013). First, we naively re-
verse all the bunsetsu-chunks 5. Then, we move
a subject chunk just before its predicate. This
process converts SOV to SVO. When the subject
is omitted, we generate a subject with our deep
parser and insert it to a subject position in the
source sentence. There are three different ways
to generate a subject.

1. Generate real Japanese words (Insert 私 (I),
あなた (you).. etc)

2. Generate virtual seed Japanese words (Insert
1st person, 2nd person..., which are not in
the Japanese lexicon.)

3. Generate only a single virtual seed Japanese
word regardless of the subject type. (Insert
zero subject)

1) is the most aggressive method, but it causes
completely incorrect translations if the detection
of subject type fails. 2) and 3) is rather conser-
vative, since they leave SMT to generate English
pronouns.

We decided to use the following hybrid ap-
proach, since it shows the best performance in our
preliminary experiments.

• In the training of SMT, use 3).
• In decoding, use 1) if the input sentence only

has one predicate. Otherwise, use 3).

3.4 Examples of parsing results

Table 2 shows examples of our deep parser output.
It can be seen that our parser can correctly identify
the deep case of topic case markers wa.

5bunsetsu is a basic Japanese grammatical unit consisting
of one content word and functional words.

560



Table 2: Examples of deep parser output
今日は (today wa) {d=other} 酒が (liquor ga) {d=obj} 飲める (can drink) {v=potential, z=I}
ニュースが (news ga) {d=subj} 伝えられた (was broadcast) {v=passive, z=already exist}
パスタは (pasta wa) {d=obj} 食べましたか (ate+question) {v=active, z=you}
あなたは (you wa) {d=subj} 食べましたか (ate+question) {v=active, z=already exist}

4 Experiments

4.1 Experimental settings

We carried out all our experiments using a state-
of-the-art phrase-based statistical Japanese-to-
English machine translation system (Och, 2003)
with pre-ordering. During the decoding, we
use the reordering window (distortion limit) to 4
words. For parallel training data, we use an in-
house collection of parallel sentences. These come
from various sources with a substantial portion
coming from the web. We trained our system on
about 300M source words. Our test set contains
about 10,000 sentences randomly sampled from
the web.

The dependency parser we apply is an imple-
mentation of a shift-reduce dependency parser
which uses a bunsetsu-chunk as a basic unit for
parsing (Kudo and Matsumoto, 2002).

The zero subject and voice type models were
trained with about 20,000 and 5,000 manually an-
notated web sentences respectively. In order to
simplify the rating tasks for our annotators, we ex-
tracted only one candidate predicate from a sen-
tence for annotations.

We tested the following six systems.

• baseline: no pre-ordering.
• surface reordering : pre-ordering only with

surface dependency relations.

• independent deep reordering: pre-ordering
using deep parser without global linguistic
constraints.

• independent deep reordering + zero sub-
ject: pre-ordering using deep parser and zero
subject generation without global linguistic
constraints.

• joint deep reordering: pre-ordering using
our new deep parser with global linguistic
constraints.

• joint deep reordering + zero-subject: pre-
ordering using deep parser and zero subject
generation with global linguistic constraints.

Table 3: Results for different reordering methods
System BLEU RIBES
baseline (no reordering) 16.15 52.67
surface reordering 19.39 60.30
independent deep reordering 19.68 61.27
independent deep reordering + zero subj. 19.81 61.67
joint deep reordering 19.76 61.43
joint deep reordering + zero subj. 19.90 61.89

As translation metrics, we used BLEU (Pap-
ineni et al., 2002), as well as RIBES (Isozaki et
al., 2010a), which is designed for measuring the
quality of distant language pairs in terms of word
orders.

4.2 Results

Table 3 shows the experimental results for six pre-
reordering systems. It can be seen that the pro-
posed method with deep parser outperforms base-
line and naive reordering with surface syntactic
trees. The zero subject generation can also im-
prove both BLEU and RIBES scores, but the im-
provements are smaller than those with reordering.
Also, joint inference with global linguistics con-
straints outperforms the model which solves deep
syntactic analysis and zero subject generation in-
dependently.

5 Conclusions

In this paper, we proposed a simple joint inference
of deep case analysis and zero subject generation
for Japanese-to-English SMT. Our parser consists
of pointwise probabilistic models and a global in-
ference with linguistic constraints. We applied our
new deep parser to pre-ordering in Japanese-to-
English SMT system and showed substantial im-
provements in automatic evaluations.

Our future work is to enhance our deep parser so
that it can handle other linguistic phenomena, in-
cluding causative voice, coordinations, and object
ellipsis. Also, the current system is built on the
top of a dependency parser. The final output of our
deep parser is highly influenced by the parsing er-
rors. It would be interesting to develop a full joint
inference of dependency parsing and deep syntac-
tic analysis.

561



References
Japan descriptive grammar research group. 2009. Con-

temporary Japanese grammar book 2. Part 3. Case
and Syntax, Part 4. Voice. Kuroshio Publishers.

Xia Fei and McCord Michael. 2004. Improving a sta-
tistical mt system with automatically learned rewrite
patterns. In Proc. of ACL.

Isao Goto, Bin Lu, Ka Po Chow, Eiichiro Sumita, and
Benjamin K Tsou. 2013. Overview of the patent
machine translation task at the ntcir-10 workshop.
In Proc. of NTCIR.

Sho Hoshino, Yusuke Miyao, Katsuhito Sudoh, and
Masaaki Nagata. 2013. Two-stage pre-ordering
for japanese-to-english statistical machine transla-
tion. In Proc. IJCNLP.

Ryu Iida and Massimo Poesio. 2011. A cross-lingual
ilp solution to zero anaphora resolution. In Proc. of
ACL.

Hideki Isozaki, Tsutomu Hirao, Kevin Duh, Katsuhito
Sudoh, and Hajime Tsukada. 2010a. Automatic
evaluation of translation quality for distant language
pairs. In Proc. of EMNLP. Association for Compu-
tational Linguistics.

Hideki Isozaki, Katsuhito Sudoh, Hajime Tsukada, and
Kevin Duh. 2010b. Head finalization: A simple re-
ordering rule for sov languages. In Proc. of the Joint
Fifth Workshop on Statistical Machine Translation
and MetricsMATR.

Jason Katz-Brown and Michael Collins. 2008. Syntac-
tic reordering in preprocessing for japanese → en-
glish translation: Mit system description for ntcir-
7 patent translation task. In Proc. of the NTCIR-7
Workshop Meeting.

Mamoru Komachi, Masaaki Nagata, and Yuji Mat-
sumoto. 2006. Phrase reordering for statistical ma-
chine translation based on predicate-argument struc-
ture. In Proc. of the International Workshop on Spo-
ken Language Translation.

Taku Kudo and Yuji Matsumoto. 2002. Japanese
dependency analysis using cascaded chunking. In
Proc. of CoNLL.

Uri Lerner and Slav Petrov. 2013. Source-side classi-
fier preordering for machine translation. In Proc. of
EMNLP.

Graham Neubig, Taro Watanabe, and Shinsuke Mori.
2012. Inducing a discriminative parser to optimize
machine translation reordering. In Proc. of EMNLP.

Franz Josef Och. 2003. Minimum error rate training in
statistical machine translation. In Proc. of ACL.

Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic eval-
uation of machine translation. In Proc. of ACL.

Vasin Punyakanok, Dan Roth, Wen-tau Yih, and Dav
Zimak. 2004. Semantic role labeling via integer
linear programming inference. In Proc. of ACL.

Hirotoshi Taira, Katsuhito Sudoh, and Masaaki Na-
gata. 2012. Zero pronoun resolution can improve
the quality of je translation. In Proc. of Workshop on
Syntax, Semantics and Structure in Statistical Trans-
lation.

Xianchao Wu, Katsuhito Sudoh, Kevin Duh, Hajime
Tsukada, and Masaaki Nagata. 2011. Extracting
pre-ordering rules from predicate-argument struc-
tures. In Proc. of IJCNLP.

562


