



















































Feature Selection for Short Text Classification using Wavelet Packet Transform


Proceedings of the 19th Conference on Computational Language Learning, pages 321–326,
Beijing, China, July 30-31, 2015. c©2015 Association for Computational Linguistics

Feature Selection for Short Text Classification using Wavelet Packet
Transform

Anuj Mahajan
Indian Institute of Technology, Delhi

Hauz Khas, New Delhi 110016
India

anujmahajan.iitd@gmail.com

Sharmistha, Shourya Roy
Xerox Research Centre India

Bangalore - 560103
India

{sharmistha.jat,shourya.roy}@xerox.com

Abstract

Text classification tasks suffer from curse of
dimensionality due to large feature space.
Short text data further exacerbates the prob-
lem due to their sparse and noisy nature. Fea-
ture selection thus becomes an important step
in improving the classification performance.
In this paper, we propose a novel feature se-
lection method using Wavelet Packet Trans-
form. Wavelet Packet Transform (WPT) has
been used widely in various fields due to its
efficiency in encoding transient signals. We
demonstrate how short text classification task
can be benefited by feature selection using
WPT due to their sparse nature. Our technique
chooses the most discriminating features by
computing inter-class distances in the trans-
formed space. We experimented extensively
with several short text datasets. Compared to
well known techniques our approach reduces
the feature space size and improves the overall
classification performance significantly in all
the datasets.

1 Introduction
Text classification task consists of assigning a docu-
ment to one or more classes. This can be done us-
ing machine learning techniques by training a model
with labelled documents. Documents are usually repre-
sented as vectors with a variety of techniques like bag-
of-words(unigram, bigram), TFIDF representation, etc.
Typically, text corpora have very high dimensional
document representation equal to the size of vocabu-
lary. This leads to curse of dimensionality1 in machine
learning models, thereby degrading the performance.

Short text corpora, like SMS, tweets, etc., in partic-
ular suffer from sparse high dimensional feature space,
due to large vocabulary and short document length. To
give an idea as to how these factors affect the size
of the feature space we compare Reuters with Twitter
data corpus. In Reuters-21578 corpus there are approx-
imately 2.5 Million words in total and 14506 unique
vocabulary entries after standard preprocessing steps

1https://en.wikipedia.org/wiki/Curse_
of_dimensionality

(which is the dimensionality of the feature space).
However, Twitter 1 corpus, we used for experiments
has approximately 15,000 words in total and feature
space size of 7423 words. Additionally, the average
length of an English tweet is 10-12 words whereas the
average length of a document in Reuters-21578 news
classification corpus is 200 words. Therefore, the di-
mensionality is extremely high even for small corpora
with short texts. In addition, the average number of
words in a document is significantly less in short text
data leading to higher sparsity of feature space repre-
sentation of documents.

Owing to this high dimensionality problem, one of
the important steps in text classification workflows is
feature selection. Feature selection techniques for tradi-
tional documents have been aplenty and a few seminal
survey articles have been written on this topic (Blitzer,
2008). In contrast, for short text there is much less
work on statistical feature selection but more focus has
gone to feature engineering towards word normaliza-
tion, canonicalization etc. (Han and Baldwin, 2011).

In this paper, we propose a dimensionality reduc-
tion technique for short text using Wavelet packet trans-
form called Improvised Adaptive Discriminant Wavelet
Packet Transform (IADWPT). IAWDPT does dimen-
sionality reduction by selecting discriminative features
(wavelet coefficients) from the Wavelet Packet Trans-
form (WPT) representation. Short text data resembles
transient signals in vector representation and WPT en-
codes transient signals (signals lasting for very short
duration) well (Learned and Willsky, 1995), using very
few coefficients. This leads to considerable decrease in
the dimensionality of the feature space along with in-
crease in classification accuracy. Additionally, we op-
timise the procedure to select the most discriminative
features from WPT representation. To the best of our
knowledge this is the first attempt to apply an algorithm
based on wavelet packet transform to the feature selec-
tion in short text classification.

2 Related Work

Feature selection has been widely adopted for dimen-
sionality reduction of text datasets in the past. Yim-
ing Yang et al. (Yang and Pedersen, 1997) performed
a comparative study of some of these methods includ-
ing, document frequency, information gain(IG), mutual

321



information(MI), χ2-test(CHI) and term strength(TS).
They concluded that IG and CHI are the most effective
in aggressive dimensionality reduction. The mRMR
technique proposed by (Peng et al., 2005) selects the
best feature subset by increasing relevancy of feature
with target class and reducing redundancy between
chosen features.

Wavelet transform provides time-frequency repre-
sentation of a given signal. The time-frequency rep-
resentation is useful for describing signals with time
varying frequency content. Detailed explanation of
wavelet transform theory is beyond the scope of
this paper. For detailed theory, refer to Daubechies
(Daubechies, 2006; Daubechies, 1992; Coifman and
Wickerhauser, 2006) and Robi Polikar (Polikar, ). First
use of wavelet transform for compression was proposed
by Ronald R Coifman et al. (Coifman et al., 1994).
Hammad Qureshi et al. (Qureshi et al., 2008) pro-
posed an adaptive discriminant wavelet packet trans-
form(ADWPT) for feature reduction.

In past wavelet transform has been applied to natural
language processing tasks. A survey on wavelet appli-
cations in data mining (Li et al., 2002), discusses the
basics and properties of wavelets which make it a very
effective technique in Data Mining. CC Aggarwal (Ag-
garwal, 2002) uses wavelets for strings classification.
He notes that wavelet technique creates a hierarchical
decomposition of the data which can capture trends at
varying levels of granularity and thus helps classifica-
tion task with the new representation. Geraldo Xexeo
et al. (Xexeo et al., 2008) used wavelet transform to
represent documents for classification.

3 Wavelet Packet Transform for
Short-text Dimensionality Reduction

Feature selection performs compression of feature
space to preserve maximum discriminative power of
features for classification. We use this analogy to do
compression of document feature space using Wavelet
Packet Transform. Vector format(e.g. dictionary en-
coded vector) representation of a document is equiv-
alent to a digital representation. This vector format
can then be processed using wavelet transform to get
a compressed representation of the document in terms
of wavelet coefficients. Document features are trans-
formed into wavelet coefficients. Wavelet coefficients
are ranked and selected based on their discrimination
power between classes. Classification model is trained
on these highly informative coefficients. Results show
a considerable improvement in model accuracy using
our dimensionality reduction technique.

Typically, vector representation of short text will
have very few non-zero entries due to short length of
the documents. If we plot count of each word in the dic-
tionary on y-axis v/s distinct words on x-axis. Just like
transient signals, the resulting graph will have very few
spikes. Transient signals last for a very little time in the
whole duration of the observation. (Learned and Will-

sky, 1995) show the efficacy of wavelet packet trans-
form in representing transient signal. This motivates
our use of Wavelet Packet Transform to encode short
text.

Wavelet transform is a popular choice for feature
representation in image processing. Our approach is in-
spired by a related work by (Qureshi et al., 2008). They
propose Adaptive Discriminant Wavelet Packet Trans-
form (ADWPT) based representation for meningioma
subtype classfication. ADWPT obtains a wavelet based
representation by optimising the discrimination power
of the various features. Proposed technique IADWPT
differs from ADWPT in the way discriminative fea-
ture are selected. Next section provides details about
the proposed approach IADWPT.

4 IADWPT - Improvised Adaptive
Discriminant Wavelet Packet
Transform

This section presents the proposed short text feature
selection technique IADWPT. IADWPT uses wavelet
packet transform of the data to extract useful discrimi-
native features from the sub-bands at various depths.

Natural language processing tasks usually represent
their documents in dictionary encoded bag-of-words
representation. This numerical vector representation of
a document is equivalent to signal representation. In or-
der to get IADWPT representation of the document fol-
lowing steps should be computed:

1) Compute full wavelet packet transform of the doc-
ument vector representation.

2) Compute the discrimination power of each coeffi-
cient in wavelet packet transform representation.

3) Select the most discriminative coefficients to rep-
resent all the documents in the corpus.

Once the 1-D wavelet transform is computed at a de-
sired level l, wavelet packet transform (WPT) produces
2l different sets of coefficients (nodes in WPT tree).
These coefficients represent the magnitude of various
frequencies present in the signal at a given time. We
select the most discriminative coefficients to represent
all the documents in the corpus by calculating the dis-
criminative power of each coefficient.

The classification task consists of c classes with d
documents. 1-D Wavelet Packet Transform of the dthk
document yields l levels with f sub bands consisting
of m coefficients in each sub band. xm,f,l represent
the coefficients of Wavelet Packet Transform. Follow-
ing terms are defined for Algorithm 1.

• probability density estimates (Sm,f,l) of a partic-
ular sub-band in a level l a training sample docu-
ment dik of a given Class ci is given by:

Skm,f,l =
(xkm,f,l)

2∑
j
(xk

j,f,l
)2

Here, xm,f,l is the mth coefficient in f th sub-
band of lth level of document dk. Where, j varies

322



Algorithm 1 IADWPT Algorithm for best discriminative feature selection
1: for all classes C do
2: Calculate Wavelet Packet Transform for all the documents dk in class ci
3: for all Documents dk do
4: Calculate probability density estimates Skm,f,l
5: end for
6: for all Levels of WPT l and their sub bands f do
7: for all Wavelet Packet Transform Coefficients m in subband f do
8: Calculate average probability density Acim,f,l
9: end for

10: end for
11: end for
12: for all Class Pairs ca, cb do
13: Calculate discriminative power Da,bm,f,l
14: end for
15: Select top m′ coefficients for representing documents in corpus

over the length of sub-band. Wavelet supports vary
with the bands, Normalization ensures that the
feature selection done gives uniform weightage to
the features from different bands. This step cal-
culates the normalised value of coefficients in a
sub-band.

• Average probability density (Acim,f,l) estimates are
derived using all the training samples dkin a given
class ci.

Acim,f,l =
∑

k
Skm,f,l
d

for, coefficient m in sub-band f for class ci and
d is the total number of documents in the class. k
varies over the number of documents in the class.
It measures the average value of a coefficient in all
the documents belonging to a class.

• Discriminative power (Da,bm,f,l) of each coefficient
in lth level’s f th sub band’s mth coefficient, be-
tween classes a and b is defined as follows:

Da,bm,f,l = |
√
Aam,f,l −

√
Abm,f,l|

Discriminative power is the hellinger distance be-
tween the average probability density estimates
of a coefficient for the two classes. It quantifies
the difference in the average value of a coeffi-
cient between a pair of classes. More the differ-
ence, better the discriminative power of the coef-
ficient. Thus discriminative features tend to have
a higher average probability density in one of the
classes whereas redundant features cancel out in
taking the difference in computing the distance.
(Rajpoot, 2003) have shown efficacy of Hellinger
distance applied to ADWPT.

Selecting coefficients with greater discriminative
power helps the classifier perform well. Full algorithm
is mentioned in algorithm 1.

Multi class classification can then be handled in this
framework using one-vs-one classification. We select

the top m′ features from the wavelet representation for
representing the data in the classification task. Time
complexity of the algorithm is polynomial. The method
is based on adaptive discriminant wavelet packet trans-
form (ADWPT) (Qureshi et al., 2008). Therefore, we
name it as improvised adaptive discriminant wavelet
packet transform (IADWPT). ADWPT uses best basis
for classification which is a union of the various sub-
bands selected that can span the transformed space, so
noise is still retained in the signal whereas IADWPT
selects coefficients from the sub-band having maximal
discriminative power thus improving the classification
results. As opposed to ADWPT, IADWPT is a one way
transform, original signal cannot be recovered from the
transform domain. Experimental results confirm that
IADWPT performs better than ADWPT in short text
datasets.

4.1 IADWPT Example
Figure 1 Gives intuition of the workings of IAD-
WPT transform. Uppermost graph displays the Aver-
age probability density in positive class samples. Mid-
dle graph in the Figure 1 shows the Average probabil-
ity density in negative class samples. These two energy
values are then subtracted, resulting values are shown
in the bottom most component of Figure 1. Peak posi-
tive and negative values in the bottommost graph rep-
resent the most discriminant features. Absolute value
of the discriminative power can then be used to select
the most discriminative features to represent each doc-
ument in corpus.

5 Experiments and Results
We used multiple short text datasets to prove efficacy of
proposed algorithm against state of the art algorithms
for feature selection.

1) Twitter 1: This dataset is a part of the SemEval
2013 task B dataset (Nakov et al., ) for two class senti-
ment classification. We gathered 624 examples in pos-
itive and negative class each for our experiments.

323



0 500 1000 1500 2000 2500
0

0.2

0.4

0 500 1000 1500 2000 2500
0

0.2

0.4

0 500 1000 1500 2000 2500
−0.5

0

0.5

Wavelet Features

E
n
e
rg

y

Calculation of Discriminative features in Wavelet Packet Transform Representation

Figure 1: x-axis represents the wavelet packet transform coefficients, y-axis represents amplitude. From top to
bottom, 1) Average probability density Aam,f,lvalue of coefficients in positive class a), 2) Average probability
density Abm,f,lvalue of coefficients in negative class (b), 3) Difference between A

a
m,f,l and A

b
m,f,l

Table 1: CLASSIFICATION RESULTS - SUPPORT VECTOR MACHINE (SVM) AND LOGISTIC REGRESSION (LR)
Dataset Baseline MI-avg χ2 PCA ADWPT IADWPT
Classification accuracy - SVM
Twitter 1 47.04 47.57 45.62 59.28 46 63.44
SMS Spam 1 - HAM Accuracy 99.82 99.71 99.77 99.87 99.63 99.94
SMS Spam 1 - SPAM Accuracy 83.10 83.32 82.96 83.81 83.57 83.92
SMS Spam 2 - HAM Accuracy 55.2 56.31 55.62 81.12 54.1 87.7
SMS Spam 2 - SPAM Accuracy 46.6 46.7 46.49 92.39 47.3 99.42
Total dimensions in best classification accuracy result - SVM
Twitter 1 7423 2065 515 540 7423 23
SMS Spam 1 9394 3540 550 750 9394 815
SMS Spam 2 10681 2985 490 855 1068 250
Classification accuracy - Logistic Regression
Twitter 1 75.8 74.97 75.21 76.28 68.2 76.72
SMS Spam 1 - HAM Accuracy 97.91 94.67 95.28 98.71 98.03 99.61
SMS Spam 1 - SPAM Accuracy 95.48 85.34 86.37 91.37 82.2 87.54
SMS Spam 2 - HAM Accuracy 96.02 89.54 92.76 71.21 95.09 98.5
SMS Spam 2 - SPAM Accuracy 91.2 88.37 91.38 89.15 92.2 94.51
Total dimensions in best classification accuracy result - Logistic Regression
Twitter 1 7423 5600 3250 3575 7423 2749
SMS Spam 1 9394 7545 1755 2350 9394 1680
SMS Spam 2 10681 6550 3000 3050 10681 9981

2) SMS Spam 1: UCI spam dataset (Almeida, )
consists of 5,574 instances of SMS classified into
SPAM and HAM classes. SPAM class is defined as
messages which are not useful and HAM is the class
of useful messages. We compare our results with the
results they published in their paper (Almeida et al.,
2013). Therefore, we followed the same experiment
procedure as cited in the paper. First 30% samples were
used in train and the rest in test set as reported in the
paper.

3) SMS Spam 2: The dataset was published by Ya-
dav et al. (Yadav et al., 2011). It consists of 2000 SMS,
1000 SPAM and 1000 HAM messages. Experiment set-
tings are same as that of dataset SMS Spam 1.

The goal of our experiments is to examine the ef-
fectiveness of the proposed algorithm in feature selec-
tion for short text datasets. We measure the effective-
ness of the feature selection technique with respect to
the increase in accuracy in the final machine learning
task. Our method does not depend on a specific classi-
fier used in the final classification. Therefore, we used

50 100 150 200 250 300 350 400 450 500
0

0.1

0.2

0.3

0.4

0.5

0.6

0.7

0.8

0.9

1

Comparision of different feature selection methods: Spam Accuracy

Number of dimensions

S
p
a
m

 A
c
c
u
ra

c
y

 

 

MRMR

IADWPT

PCA

Chi

MI

Figure 2: Spam Classification Accuracy comparison
across various feature selection algorithm for SMS
Spam 2 dataset

324



50 100 150 200 250 300 350 400 450 500
0

0.1

0.2

0.3

0.4

0.5

0.6

0.7

0.8

0.9

1

Number of Dimensions

H
a
m

 A
c
c
u
ra

c
y

Comparision of different feature selection methods: Ham Accuracy

 

 

MRMR

IADWPT

PCA

Chi

MI

Figure 3: Ham Classification Accuracy comparison
across various feature selection algorithm for SMS
Spam 2 dataset

popular classifiers like Support Vector Machine (RBF
kernel with grid search) (Cortes and Vapnik, 1995) and
Logistic Regression with and without dimensionality
reduction for unigram representation to benchmark the
performance. All the experiments were done with 10
fold cross validation and grid search on C parameter.
We report results with respect to classification accuracy
which is measured as #correctly classified datapoints#total datapoints .

We conducted detailed experiments comparing IAD-
WPT using Coiflets of order 2 with other feature selec-
tion techniques such as PCA, Mutual Information, χ2,
mRMR (Peng et al., 2005) and ADWPT (Qureshi et
al., 2008). Results are reported in Table 1. The table
reports best accuracy values and respective feature set
size selected by the technique. It can be observed that
IADWPT gives best accuracy in most of the cases with
very few features.

We compared performance of our algorithm with
mRMR. Results for SMS Spam 2 dataset are shown
in Figure 2 and Figure 3. The plots prove efficacy of
our algorithm versus state of the art mRMR algorithm.
mRMR technique could not finish execution for the rest
of the datasets. It can also be observed from results
in Table 1 and Figure 1,2 that performance of feature
selection algorithms follow consistent pattern in short
text. Following is observed order of performance of al-
gorithms in decreasing order, IADWPT, mRMR, PCA,
Chi Square, MI. Further, it is observed that IADWPT
performs well at feature selection without losing dis-
criminative information, even when the dimensional-
ity of feature space is reduced to as far as 1/40th of
original feature space and steadily maintains the ac-
curacy as dimensionality is reduced, which makes it a
suitable technique for aggressive dimensionality reduc-
tion. This also helps in learning ML (machine learn-
ing) models faster due to reduced dimensionality. We
plotted the discrimination power of coefficients in each
dataset. Plot suggested that very few coefficients con-
tained most of the discriminative power. And, therefore
just working with these coefficients can help in getting
good accuracies resulting in aggressive dimensionality
reduction. Results establish the effectiveness of IAD-
WPT for applicability in compressing short text feature
representation and reducing noise to improve classifi-

Figure 4: Plot of Discriminative Power (Da,bm,f,l) ar-
ranged in descending order

cation accuracy.

5.1 IADWPT Effectiveness
Short text data is noisy and consists of many features
which are irrelevant to the task of classification of data.
IADWPT effectively gets rid of the noise in approxima-
tions(as Signal strength is greater than the noise), the
feature selection step at the sub-band level as described
in Algorithm 1, it enforces selection of good discrim-
inative features and thus improves classifier accuracy,
reducing feature space dimensionality at the same time.
Features from sub-bands of the signal are chosen based
on their discriminative power, therefore, the original
signal information is lost and the transform is not re-
versible.

IADWPT gives good compression of data and with-
out losing discriminative information, even when the
dimensionality of space is reduced to as far as 1/40th
of original feature space and is thus steadily maintain-
ing the accuracy as dimensionality is reduced, which
makes it a suitable technique for dimensionality re-
duction. This also helps learning machine learning
models faster due to reduced dimensionality. Figure
4 shows the plot of Discriminative Power Da,bm,f,l val-
ues for coefficients arranged in descending order for
SMS Spam 2 dataset. Other datasets displayed similar
graph for Discriminative Power. From the figure it can
be observed that few coefficients hold most of the dis-
criminative power, and thus aggressive dimensionality
reduction is possible with IADWPT algorithm. Results
establish the effectiveness of IADWPT for applicabil-
ity in compressing short text feature representation and
reducing noise.

6 Conclusion and Future Work
In this paper, we have proposed IADWPT algorithm
for effective dimensionality reduction for short text cor-
pus. The algorithm can be used in a number of scenar-
ios where high dimensionality and sparsity pose chal-
lenge. Experiments prove efficacy of IADWPT based
dimensionality reduction for short text data. This tech-
nique can prove useful to a number of social media data
analysis applications. In future, we would like to ex-
plore theoretical bounds on best number of dimensions
to choose from wavelet representation.

325



References
Charu C. Aggarwal. 2002. On effective classification

of strings with wavelets.

Tiago A Almeida. Sms spam collection v.1.
http://www.dt.fee.unicamp.br/
˜tiago/smsspamcollection/,[Online;
accessed 10-September-2014].

T. Almeida, J. M. G. Hidalgo, and T. P. Silva. 2013.
Towards sms spam filtering: Results under a new
dataset. International Journal of Information Secu-
rity Science.

John Blitzer. 2008. A survey of dimensional-
ity reduction techniques for natural language.
http://john.blitzer.com/papers/
wpe2.pdf,[Online; accessed 10-September-2014].

R. R. Coifman and M. V. Wickerhauser. 2006.
Entropy-based algorithms for best basis selection.
IEEE Trans. Inf. Theor., 38(2):713–718, September.

RonaldR. Coifman, Yves Meyer, Steven Quake, and
M.Victor Wickerhauser. 1994. Signal process-
ing and compression with wavelet packets. In J.S.
Byrnes, JenniferL. Byrnes, KathrynA. Hargreaves,
and Karl Berry, editors, Wavelets and Their Applica-
tions, volume 442 of NATO ASI Series, pages 363–
379. Springer Netherlands.

Corinna Cortes and Vladimir Vapnik. 1995. Support-
vector networks. Mach. Learn., 20(3):273–297,
September.

Ingrid Daubechies. 1992. Ten Lectures on Wavelets.
Society for Industrial and Applied Mathematics,
Philadelphia, PA, USA.

I. Daubechies. 2006. The wavelet transform, time-
frequency localization and signal analysis. IEEE
Trans. Inf. Theor., 36(5):961–1005, September.

Bo Han and Timothy Baldwin. 2011. Lexical normal-
isation of short text messages. In Proceedings of the
49th Annual Meeting of the Association for Compu-
tational Linguistics: Human Language Technologies
- Volume 1, HLT ’11, pages 368–378, Stroudsburg,
PA, USA. Association for Computational Linguis-
tics.

Rachel E. Learned and Alan S. Willsky. 1995. A
wavelet packet approach to transient signal classifi-
cation. Applied and Computational Harmonic Anal-
ysis, 2(3):265 – 278.

Tao Li, Qi Li, Shenghuo Zhu, and Mitsunori Ogihara.
2002. A survey on wavelet applications in data min-
ing. SIGKDD Explor. Newsl., 4(2):49–68, Decem-
ber.

Preslav Nakov, Zornitsa Kozareva, Alan Ritter, Sara
Rosenthal, Veselin Stoyanov, and Theresa Wilson.
Semeval-2013 task 2: Sentiment analysis in twitter.

Hanchuan Peng, Fuhui Long, and Chris Ding. 2005.
Feature selection based on mutual information: cri-
teria of max-dependency, max-relevance, and min-
redundancy. IEEE Transactions on Pattern Analysis
and Machine Intelligence, 27:1226–1238.

Robi Polikar. Wavelet Tutorial. http:
//users.rowan.edu/˜polikar/
wavelets/wttutorial.html,[Online;
accessed 10-January-2015].

Hammad Qureshi, Olcay Sertel, Nasir Rajpoot, Roland
Wilson, and Metin Gurcan. 2008. Adaptive dis-
criminant wavelet packet transform and local binary
patterns for meningioma subtype classification. In
Dimitris N. Metaxas, Leon Axel, Gabor Fichtinger,
and Gbor Szkely, editors, MICCAI (2), volume 5242
of Lecture Notes in Computer Science, pages 196–
204. Springer.

NM Rajpoot. 2003. Local discriminant wavelet packet
basis for texture classification. In SPIE Wavelets X,
pages 774–783. SPIE.

Geraldo Xexeo, Jano de Souza, Patricia F. Castro, and
Wallace A. Pinheiro. 2008. Using wavelets to
classify documents. Web Intelligence and Intel-
ligent Agent Technology, IEEE/WIC/ACM Interna-
tional Conference on, 1:272–278.

Kuldeep Yadav, Ponnurangam Kumaraguru, Atul
Goyal, Ashish Gupta, and Vinayak Naik. 2011. Sm-
sassassin: Crowdsourcing driven mobile-based sys-
tem for sms spam filtering. In Proceedings of the
12th Workshop on Mobile Computing Systems and
Applications, HotMobile ’11, pages 1–6, New York,
NY, USA. ACM.

Yiming Yang and Jan O. Pedersen. 1997. A compar-
ative study on feature selection in text categoriza-
tion. In Proceedings of the Fourteenth International
Conference on Machine Learning, ICML ’97, pages
412–420, San Francisco, CA, USA. Morgan Kauf-
mann Publishers Inc.

326


