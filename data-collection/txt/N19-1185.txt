



















































Tweet Stance Detection Using an Attention based Neural Ensemble Model


Proceedings of NAACL-HLT 2019, pages 1868‚Äì1873
Minneapolis, Minnesota, June 2 - June 7, 2019. c¬©2019 Association for Computational Linguistics

1868

Tweet Stance Detection Using an Attention based Neural Ensemble Model

Umme Aymun Siddiqua, Abu Nowshed Chy, and Masaki Aono
Department of Computer Science and Engineering

Toyohashi University of Technology, Toyohashi, Aichi, Japan
{aymun,nowshed}@kde.cs.tut.ac.jp, and aono@tut.jp

Abstract

Stance detection in twitter aims at mining
user stances expressed in a tweet towards a
single or multiple target entities. To tackle
this problem, most of the prior studies have
been explored the traditional deep learning
models, e.g., LSTM and GRU. However, in
compared to these traditional approaches, re-
cently proposed densely connected Bi-LSTM
and nested LSTMs architectures effectively
address the vanishing-gradient and overfitting
problems as well as dealing with long-term de-
pendencies. In this paper, we propose a neu-
ral ensemble model that adopts the strengths
of these two LSTM variants to learn better
long-term dependencies, where each module
coupled with an attention mechanism that am-
plifies the contribution of important elements
in the final representation. We also employ
a multi-kernel convolution on top of them
to extract the higher-level tweet representa-
tions. Results of extensive experiments on sin-
gle and multi-target stance detection datasets
show that our proposed method achieves sub-
stantial improvement over the current state-of-
the-art deep learning based methods.

1 Introduction

Tweet stance detection is the task of automati-
cally determining the stance of a tweet whether the
tweet is in favor of, against, or none towards a tar-
get (Mohammad et al., 2017). We can consider it
as a sub-domain of sentiment analysis. However,
the goal of sentiment analysis is to classify the po-
larity of a tweet sentiment based on its contents,
whereas identification of stance is dependent on
the specific target. For example, Figure 1 depicts
the stance of a tweet towards different targets.

Stance detection in twitter poses unique chal-
lenges to the research community since tweets
are short and informal user-generated text, which
usually tend not to follow the grammatical rules.

Sample Tweet:
@realDonaldTrump Crazy or blind, death and mute?

You liberal troll. #TRUMP IS GOING TO BEAT #Hillary

LANDSLIDE. #TrumpTsunami

Target II: Donald Trump

Stance II: Favor

Target I: Hillary Clinton

Stance I: Against

Figure 1: Example of target-specific stance detection.

Moreover, tweets contain plenty of idiosyncratic
abbreviations as well as other twitter specific syn-
taxes such as #hashtags and emoticons. To ad-
dress the challenges of stance detection in twit-
ter, Mohammad et al. (2016) presented a tweet
stance detection task that focused on a single tar-
get in SemEval-2016. Top performing systems
in this task proposed several deep learning based
approaches by using CNN (Wei et al., 2016),
RNN (Zarrella and Marsh, 2016), and so on.

Later, (Du et al., 2017) utilized the target-
augmented embeddings in an attention based neu-
ral network, whereas (Zhou et al., 2017) proposed
an attention mechanism at the semantic level in
the bidirectional GRU-CNN structure to perform
target-specific stance detection on tweets. More
recently, (Dey et al., 2018) proposed a two-phase
LSTM based model with attention and (Wei et al.,
2018b) proposed an end-to-end neural memory
model via target and tweet interactions.

By considering the dependency of related tar-
gets, Sobhani et al. (2017) introduced a multi-
target stance detection (MTSD) task and proposed
an attentive encoder-decoder network to capture
the dependencies among stance labels regarding
multiple targets. Later, (Wei et al., 2018a) pro-
posed a dynamic memory-augmented network that
utilized a shared external memory to capture and
store multi-targets stance indicative clues.

However, most of the related work of tweet
stance detection explored the traditional deep
learning models in their methods. In this paper,



1869

we propose a neural ensemble method that com-
bines the attention based state-of-the-art densely
connected Bi-LSTM and nested LSTMs models
with the multi-kernel convolution in a unified ar-
chitecture. Experimental results on both the sin-
gle and multi-target benchmark stance detection
datasets demonstrate the efficacy of our method
over the state-of-the-art deep learning based meth-
ods discussed above.

The rest of the paper is structured as follows:
In Section 2, we introduce our proposed stance
detection framework. Section 3 includes experi-
ments and evaluations as well as the comparisons
with the state-of-the-arts to show the effectiveness
of our proposed method. Some concluded remarks
and future directions of our work are described in
Section 4.

2 Proposed Stance Detection Framework

In this section, we describe the details of our pro-
posed neural ensemble model (PNEM) for twitter
stance detection. Figure 2 depicts an overview of
our proposed framework.

ùõºùëõ

Attention

Mechanism

Feature Vector

ùõºùëõ

Attention

Mechanism

Densely Connected 

Bi-LSTM
Nested LSTMs

Dense Layer

Feature Vector

Concatenate Layer

Final Stance Label

Activation Layer

Kernel Size =[2,3,4,5]

Target Appended Tweet
Embeddings

L √ó D
L = Target Length + Tweet Length

D = Word Vectors Dimension

Figure 2: Proposed stance detection framework.

At first, we utilize the multi-kernel convo-
lution filters to extract higher-level feature se-
quences from the target appended tweet embed-
dings. These feature sequences are fed into the
attention based densely connected Bi-LSTM and
nested LSTMs to learn long-term dependencies.
Final representations of these modules are con-
catenated and pass to the stance prediction module
to determine the stance label. Next, we describe
each component elaborately.

Embedding Layer: Prior works already estab-
lished the significance of target information for
stance detection. To integrate the target informa-
tion, we generate a unified word vector matrix by
concatenating the vector representations of the tar-
get and tweet. The dimensionality of the matrix
will be L √ó D, where length L is the sum of the
target length and tweet length, and D denotes the
word-vector dimension. We utilize a pre-trained
word embedding model for obtaining the vector
representation of words.

Multi-Kernel Convolution: In our multi-
kernel convolution, we adopt the idea proposed
by (Kim, 2014) to extract the higher-level features.
The input of this module is the target appended
tweet matrix generated in the embedding layer.
We then perform the convolution on it by using
a filter. We apply multiple convolutions based on
four different kernel sizes, i.e., the size of the con-
volution filters: 2, 3, 4, and 5. After performing
convolutions, each filter generates the correspond-
ing feature maps and a max pooling function is
then applied to generate a univariate feature vec-
tor. Finally, the feature vectors generated from
each kernel are concatenated to form a single high-
level feature vector.

Densely Connected Bi-LSTM: With the
emerging trend of deep learning, LSTM based
models are the most popular for sequential
tasks. Recently, the densely connected structure
of LSTM models gets attention among the re-
searchers (Li et al., 2018; Wu et al., 2017) that en-
able the effective connection from lower to upper
layers features without any loss of information on
lower-layer features thus alleviate the vanishing-
gradient and overfitting problems effectively.

In our proposed framework, we utilize the
densely connected Bi-LSTM (Ding et al., 2018)
(DC-Bi-LSTM) model. A DC-Bi-LSTM model
consists of multiple Bi-LSTM layers, where repre-
sentation of each layer is estimated by concatenat-



1870

ing its hidden states and all the preceding layers‚Äô
hidden states. Hence, for the first Bi-LSTM layer,
the input is a higher-level features sequences gen-
erated from multi-kernel convolution (MKC) and
the output is {h1 = h11, h12, ..., h1s}. For the sec-
ond Bi-LSTM layer, the input is the concatenation
of higher-level feature sequences from MKC and
the output from first Bi-LSTM layer to generate
the corresponding output. Rest of the layers are
processed accordingly. We can define the above
process as follows:

hlt = [
‚àí‚Üí
hlt ;
‚Üê‚àí
hlt ], h

0
t = MKC feature sequence,

‚àí‚Üí
hlt = lstm(

‚àí‚àí‚Üí
hlt‚àí1,M

l‚àí1
t ),

‚Üê‚àí
hlt = lstm(

‚Üê‚àí‚àí
hlt+1,M

l‚àí1
t ),

M l‚àí1t = [h
0
t ;h

1
t ; ...;h

l‚àí1
t ].

Therefore, from a L layer DC-Bi-LSTM model,
the output is {hL = hL1 , hL2 , ..., hLs }.

Nested LSTMs: The nested LSTMs (NL-
STMs) architecture (Moniz and Krueger, 2017)
creates a temporal hierarchy of memories that
achieved significant improvement over the single-
layer or stacked LSTM architectures to learn
longer-term dependencies.

In NLSTMs, the LSTM memory cells have ac-
cess to their inner memory, where they can selec-
tively read and write relevant long-term informa-
tion. While the value of the outer memory cell in
the LSTM is estimated as coutert = ft ÔøΩ ct‚àí1 +
it ÔøΩ gt, memory cells of the NLSTMs use the
concatenation (ft ÔøΩ ct‚àí1, it ÔøΩ gt) as input to an
inner LSTM (or NLSTM) memory cell, and set
coutert = h

inner
t . Therefore, in compared to the

LSTM and stacked LSTM, the inner memories of
NLSTMs operate on longer time-scales and effec-
tively capture the context information from the in-
put texts.

Feed-Forward Attention: Recently, the atten-
tion mechanism has been introduced in the neural
network models for effectively modeling the long-
term dependencies by enabling the model to learn
what to attend based on the input text (Vaswani
et al., 2017). In order to amplify the contribu-
tion of important elements in the final representa-
tion of both the DC-Bi-LSTM and NLSTMs mod-
ule, we employ a feed-forward attention mecha-
nism (Raffel and Ellis, 2015) to aggregate all the
hidden states according to their relative impor-
tance weight.

Stance Prediction and Model Training: We
concatenate the final tweet representation from the
attention based DC-Bi-LSTM and NLSTMs mod-
ule and pass it to a fully connected softmax layer
for stance detection. We consider cross-entropy as
the loss function and train the model by minimiz-
ing the error, which is defined as:

E(x(i), y(i)) =
k‚àë

j=1

1{y(i) = j} log(y‚àº(i)j )

where x(i) is the training sample with its true label
y(i). y‚àº(i)j is the estimated probability in [0, 1] for
each label j. 1{condition} is an indicator which
is 1 if true and 0 otherwise. We use the stochastic
gradient descent (SGD) to learn the model param-
eter and adopt the Adam optimizer (Kingma and
Ba, 2014).

3 Experiments and Evaluations

3.1 Model Configuration
In the following, we describe the set of parameters
that we have used in our proposed neural ensemble
model (PNEM) during experiments. We used the
300-dimensional fastText embedding model pre-
trained on Wikipedia with skip-gram (Bojanowski
et al., 2017) to initialize the word embeddings in
the embedding layer. For the multi-kernel convo-
lution, we employed 4 kernel sizes (2,3,4,5), and
the number of filters was set to 600. In our model,
DC-Bi-LSTM module contains 5 layers and NL-
STMs module contains 2 layers. We trained all
models for max 45 epochs with a batch size of
32 and an initial learning rate 0.001 by Adam op-
timizer. L2 regularization with a factor of 0.01
was applied to the weights in the softmax layer. In
this paper, we reported the results based on these
settings. Unless otherwise stated, default settings
were used for the other parameters.

To preprocess the data, we removed the stop
words based on NLTK‚Äôs standard stoplist, special
characters removal, and performed hashtag seg-
mentation according to (Baziotis et al., 2017).

3.2 Single Target Stance Detection
Dataset and Setup: To validate the effectiveness
of our proposed method for the single target stance
detection, we made use of a widely popular bench-
mark twitter dataset used in the SemEval-2016
Task 6-A (Mohammad et al., 2016). The training
set consists of 2914 tweets and the test set consists



1871

of 1249 tweets relevant to 5 targets. Each tweet
was annotated as Favor, Against or None towards
the specific target.

Following the SemEval-2016 Task 6-A bench-
mark, we employed the macro-average of F1-
score for the Favor and Against stance classes as
the evaluation measure. To estimate it, we used
the evaluation script provided by the organizer.

Results and Analysis: We divided the whole
dataset across targets and trained the model ac-
cordingly. We used 5% of the training samples
as the validation set. The summarized experimen-
tal results of our proposed neural ensemble model
(PNEM) on single target stance detection are pre-
sented in Table 1.

At first, we report the results based on the
baseline, which is the combination of CNN and
LSTM (Zhou et al., 2015) and obtained compet-
itive performances on several text classification
tasks. Next, we report the results of our pro-
posed PNEM model. It showed that our PNEM
method outperformed the baseline by a large mar-
gin. In order to estimate the effect of each com-
ponent of our model, we performed the compo-
nent ablation study on our proposed model. In
this regard, we removed one component each
time and repeated the experiment. From the re-
sults, it can be observed that when removing tar-
get embedding, multi-kernel convolution (MKC),
attention mechanism (ATT), NLSTMs with cor-
responding attention (NLSTMs+ATT), and DC-
Bi-LSTM with corresponding attention (DC-Bi-
LSTM+ATT), the results decreased by 0.99%,
4.98%, 1.68%, 4.14%, and 1.11%, respectively.
Thus deduced the importance of each of the com-
ponent in our model.

Method Ffavor Fagainst Favg

CNN+LSTM 59.36 74.93 67.15

PNEM 66.56 77.66 72.11

‚àí Target Embedding 64.87 77.36 71.12
‚àí MKC 60.49 73.78 67.13
‚àí ATT 63.94 76.92 70.43
‚àí (NLSTMs+ATT) 61.51 74.43 67.97
‚àí (DC-Bi-LSTM+ATT) 64.47 77.52 71.00

Table 1: Comparative performance with different ex-
perimental settings for single target stance detection.
The best results are highlighted in boldface.

Moreover, the comparative performance of our
proposed model, PNEM with the state-of-the-art
methods are presented in Table 2. It showed

that our model gained 3.13% and 4.29% im-
provement while compared with the SemEval-
2016 baseline (SVM-ngrams) and the best per-
forming system MITRE, respectively. Further-
more, in comparison with the related deep learn-
ing based methods, PNEM achieved at least 1.07%
and at best 3.44% improvement. Overall, our pro-
posed PNEM method greatly surpassed the previ-
ous works. The results show that attentive neural
ensemble model could benefit stance detection.

3.3 Multi-Target Stance Detection

Dataset and Setup: In order to assess our method
for the multi-target stance detection, we made use
of a benchmark twitter dataset (Sobhani et al.,
2017), where each tweet is annotated with two
stance labels towards two targets of a pair. The
tweets in this dataset are related to the 2016 US
election. Overall, the dataset contains 4455 tweets
for the three target-pairs including Hillary Clinton
- Bernie Sanders, Hillary Clinton - Donald Trump,
and Ted Cruz - Donald Trump. The train, devel-
opment, and test set contains 3119, 446, and 890
tweets, respectively.

As the evaluation measure, we used a simi-
lar kind of approach reported in the dataset pa-
per (Sobhani et al., 2017). Following this, the
Favg of each target is estimated according to the
SemEval-2016 Task-A benchmark (Mohammad
et al., 2016). Then, the Favg of the two targets
of a target-pair is averaged to estimate the average
score of a target-pair. Finally, the average scores
of all the target-pairs are averaged to estimate the
overall score.

Results and Analysis: We divided the whole
dataset based on each target-pairs. The dataset is
then separated into two parts for each target in the
target-pair. We trained and evaluated the model
for each target in the target-pair and combined the
results to estimate the overall performance.

We used a similar kind of baseline
(CNN+LSTM) for comparison that we used
in the single target stance detection as well as
compared with the state-of-the-art deep learning
based methods. The comparative results of our
proposed PNEM model on multi-target stance
dataset are presented in Table 3. It showed that our
method surpassed the baseline by a large margin
and gained 3.91% and 1.99% improvement over
the state-of-the-art methods Seq2Seq and DMAN,
respectively.



1872

Method
Overall Target

Ffavor Fagainst Favg Atheism Change
Climate

Movement
Feminist

Clinton
Hillary

Abortion
Legal. of

PNEM 66.56 77.66 72.11 67.73 44.27 66.76 60.28 64.23

SemEval-2016 Baseline and Related Deep Learning based Methods

SVM-ngrams 62.98 74.98 68.98 65.19 42.35 57.46 58.63 66.42
MITRE 59.32 76.33 67.82 61.47 41.63 62.09 57.67 57.28
n-grams+embeddings - - 70.30 68.30 43.80 58.40 57.80 66.90
TGMN-CR 65.52 76.55 71.04 64.60 43.02 59.35 66.21 66.21
T-PAN - - 68.84 61.19 66.27 58.45 57.48 60.21
AS-biGRU-CNN - - 69.42 66.76 43.40 58.83 57.12 65.45
TAN - - 68.79 59.33 53.59 55.77 65.38 63.72

Table 2: (Single target) Comparative performance of our model against the SemEval-2016 official baseline (SVM-
ngrams) (Mohammad et al., 2016) and related deep learning based methods including MITRE (SemEval-2016 best
performing system) (Zarrella and Marsh, 2016), n-grams+embeddings (Mohammad et al., 2017), TGMN-CR (Wei
et al., 2018b), T-PAN (Dey et al., 2018), AS-biGRU-CNN (Zhou et al., 2017), and TAN (Du et al., 2017). The best
results are highlighted in boldface.

Method Overall
Target-Pair

Clinton
Sanders

Clinton
Trump

Cruz
Trump

CNN+LSTM 46.03 43.02 47.35 47.71

Seq2Seq 54.81 54.72 56.60 53.12
DMAN 56.73 56.25 60.30 53.64
PNEM 58.72 57.74 60.05 58.36

Table 3: (Multi-target) Comparative performance of
our model against the baseline (CNN+LSTM) and
state-of-the-art deep learning based methods including
Seq2Seq (Sobhani et al., 2017) and DMAN (Wei et al.,
2018a). The best results are highlighted in boldface.

4 Conclusion

In this paper, we proposed an attention based neu-
ral ensemble model for the target-specific tweet
stance detection. The main contribution of our
unified model is to learn the contextual informa-
tion effectively which in turns improved the stance
detection performance and outperformed the state-
of-the-art deep learning based methods for both
the single and multi-target stance detection bench-
mark datasets.

In the future, we have a plan to leverage external
knowledge and generalize our model for target-
independent stance detection in the same domain.

Acknowledgments

The part of this research is supported by MEXT
KAKENHI, Grant-in-Aid for Scientific Research
(B), Grant Number 17H01746 and by grants from
the KDDI Foundation.

References
Christos Baziotis, Nikos Pelekis, and Christos Doulk-

eridis. 2017. Datastories at SemEval-2017 task
4: Deep lstm with attention for message-level and
topic-based sentiment analysis. In Proceedings
of the 11th International Workshop on Semantic
Evaluation (SemEval), pages 747‚Äì754, Vancouver,
Canada. ACL.

Piotr Bojanowski, Edouard Grave, Armand Joulin, and
Tomas Mikolov. 2017. Enriching word vectors with
subword information. Transactions of the Associa-
tion for Computational Linguistics (TACL), 5:135‚Äì
146.

Kuntal Dey, Ritvik Shrivastava, and Saroj Kaushik.
2018. Topical stance detection for twitter: A two-
phase lstm model using attention. In Proceedings of
the 40th European Conference on Information Re-
trieval (ECIR), pages 529‚Äì536. Springer.

Zixiang Ding, Rui Xia, Jianfei Yu, Xiang Li, and Jian
Yang. 2018. Densely connected bidirectional lstm
with applications to sentence classification. In CCF
International Conference on Natural Language Pro-
cessing and Chinese Computing, pages 278‚Äì287.
Springer.

Jiachen Du, Ruifeng Xu, Yulan He, and Lin Gui. 2017.
Stance classification with target-specific neural at-
tention networks. In Proceedings of the 26th Inter-
national Joint Conference on Artificial Intelligence
(IJCAI), pages 3988‚Äì3994.

Yoon Kim. 2014. Convolutional neural networks
for sentence classification. In Proceedings of the
19th Conference on Empirical Methods in Natural
Language Processing (EMNLP), pages 1746‚Äì1751.
ACL.

Diederik Kingma and Jimmy Ba. 2014. Adam: A
method for stochastic optimization. arXiv preprint
arXiv:1412.6980.



1873

Dexu Li, Yimin Chen, Mingke Gao, Siyu Jiang, and
Chen Huang. 2018. Multimodal gesture recogni-
tion using densely connected convolution and blstm.
In Proceedings of the 24th International Conference
on Pattern Recognition (ICPR), pages 3365‚Äì3370.
IEEE.

Saif Mohammad, Svetlana Kiritchenko, Parinaz Sob-
hani, Xiaodan Zhu, and Colin Cherry. 2016.
SemEval-2016 task 6: Detecting stance in tweets.
In Proceedings of the 10th International Workshop
on Semantic Evaluation (SemEval), pages 31‚Äì41.

Saif M Mohammad, Parinaz Sobhani, and Svetlana
Kiritchenko. 2017. Stance and sentiment in tweets.
ACM Transactions on Internet Technology (TOIT),
17(3):26.

Joel Ruben Antony Moniz and David Krueger. 2017.
Nested lstms. In Proceedings of the 9th Asian Con-
ference on Machine Learning (ACML), pages 530‚Äì
544. Springer.

Colin Raffel and Daniel PW Ellis. 2015. Feed-
forward networks with attention can solve some
long-term memory problems. arXiv preprint
arXiv:1512.08756.

Parinaz Sobhani, Diana Inkpen, and Xiaodan Zhu.
2017. A dataset for multi-target stance detection.
In Proceedings of the 15th Conference of the Euro-
pean Chapter of the Association for Computational
Linguistics (EACL): Volume 2, Short Papers, pages
551‚Äì557. ACL.

Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob
Uszkoreit, Llion Jones, Aidan N Gomez, ≈Åukasz
Kaiser, and Illia Polosukhin. 2017. Attention is all
you need. In Advances in Neural Information Pro-
cessing Systems (NIPS), pages 5998‚Äì6008.

Penghui Wei, Junjie Lin, and Wenji Mao. 2018a.
Multi-target stance detection via a dynamic
memory-augmented network. In Proceedings of
the 41st International ACM SIGIR Conference on
Research & Development in Information Retrieval,
pages 1229‚Äì1232. ACM.

Penghui Wei, Wenji Mao, and Daniel Zeng. 2018b. A
target-guided neural memory model for stance de-
tection in twitter. In Proceedings of the 2018 In-
ternational Joint Conference on Neural Networks
(IJCNN), pages 1‚Äì8. IEEE.

Wan Wei, Xiao Zhang, Xuqin Liu, Wei Chen, and
Tengjiao Wang. 2016. Pkudblab at SemEval-2016
task 6: A specific convolutional neural network sys-
tem for effective stance detection. In Proceedings of
the 10th International Workshop on Semantic Eval-
uation (SemEval), NAACL-HLT, pages 384‚Äì388.

Chuhan Wu, Fangzhao Wu, Yongfeng Huang, Sixing
Wu, and Zhigang Yuan. 2017. Thu ngn at IJCNLP-
2017 task 2: Dimensional sentiment analysis for
Chinese phrases with deep lstm. Proceedings of the

8th International Joint Conference on Natural Lan-
guage Processing (IJCNLP), Shared Tasks, pages
47‚Äì52.

Guido Zarrella and Amy Marsh. 2016. Mitre at
SemEval-2016 task 6: Transfer learning for stance
detection. In Proceedings of the 10th International
Workshop on Semantic Evaluation (SemEval-2016),
pages 458‚Äì463.

Chunting Zhou, Chonglin Sun, Zhiyuan Liu, and Fran-
cis C. M. Lau. 2015. A c-lstm neural network for
text classification. CoRR, abs/1511.08630.

Yiwei Zhou, Alexandra I Cristea, and Lei Shi. 2017.
Connecting targets to tweets: Semantic attention-
based model for target-specific stance detection. In
Proceedings of the 18th International Conference
on Web Information Systems Engineering (WISE),
pages 18‚Äì32. Springer.


