




































Recovering Missing Characters in Old Hawaiian Writing


Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 4929–4934
Brussels, Belgium, October 31 - November 4, 2018. c©2018 Association for Computational Linguistics

4929

Recovering Missing Characters in Old Hawaiian Writing

Brendan Shillingford∗1,2 and Oiwi Parker Jones∗1
University of Oxford1, DeepMind2

brendan.shillingford@cs.ox.ac.uk,
oiwi.parkerjones@wolfson.ox.ac.uk

Abstract

In contrast to the older writing system of the
19th century, modern Hawaiian orthography
employs characters for long vowels and glottal
stops. These extra characters account for about
one-third of the phonemes in Hawaiian, so in-
cluding them makes a big difference to read-
ing comprehension and pronunciation. How-
ever, transliterating between older and newer
texts is a laborious task when performed man-
ually. We introduce two related methods to
help solve this transliteration problem automat-
ically. One approach is implemented, end-
to-end, using finite state transducers (FSTs).
The other is a hybrid deep learning approach,
which approximately composes an FST with a
recurrent neural network language model.

1 Introduction

From 1834 to 1948, more than 125,000 newspa-
per pages were published in the Hawaiian lan-
guage (Nogelmeier, 2010). Yet by 1981, many
expected this once flourishing language to die
(Benton, 1981). Hawaiian has since defied ex-
pectations and experienced the beginnings of a
remarkable recovery (Warner, 2001; Wilson and
Kamanā, 2001). However much of the literary in-
heritance that is contained in the newspapers has
become difficult for modern Hawaiians to read,
since the newspapers were written in an orthogra-
phy that failed to represent about one-third of the
language’s phonemes. This orthography, which
we will refer to as the missionary orthography, ex-
cluded Hawaiian phonemes that did not have equiv-
alents in American English (see Schütz, 1994), in-
cluding Hawaiian’s long vowels /i: e: a: o: u:/
and glottal stop /P/. By contrast, the modern
Hawaiian orthography, an innovation of Pukui
and Elbert’s Hawaiian dictionary (Pukui and El-
bert, 1957), presents a nearly perfect, one-to-one

∗Authors contributed equally.

mapping between graphemes and phonemes (see
Appendix A.1). The process of manual translit-
eration from missionary to modern Hawaiian or-
thography is extremely labor intensive. Yet the
cultural benefits are so great that hundreds of pages
of newspaper-serials have already been transliter-
ated by hand, such as Nogelmeier’s new edition of
the epic tale of Hi‘iakaikapoliopele, the volcano
goddess’s sister (Ho‘oulumāhiehie, 2007). Criti-
cally important as such efforts are to the continued
revitalization of this endangered language, they are
still only an introduction to the material that could
be translated for a modern Hawaiian audience.

In this paper, we propose to automate, or semi-
automate, the transliteration of old Hawaiian texts
into the modern orthography. Following a brief
review of related work (Section 2), we begin by de-
scribing a dataset of modern Hawaiian (Section 3).
In Section 4, we present two methods for recover-
ing missing graphemes (and hence phonemes) from
the missionary orthography. The first composes a
series of weighted FSTs; the second approximately
composes a FST with a recurrent neural network
language model (RNNLM) using a beam search
procedure. Both approaches require only modern
Hawaiian texts for training, which are much more
plentiful than parallel corpora. Section 5 reports
the results of our transliteration experiments us-
ing a simulated parallel corpus, as well as two
19th century newspaper articles for which we also
have modern Hawaiian transcriptions. Being based
on FSTs, both approaches are modular and exten-
sible. We observe useful and promising results
for both of our methods, with the best results ob-
tained by the hybrid FST-RNNLM. These results
showcase the strength of combining established
hand-engineering methods with deep learning in a
smaller data regime, with practical applications for
an endangered language.



4930

2 Related work

Many of the themes that we address relate to ex-
isting literature. For example, Hajič et al. (2000)
and Scannell (2014) have written on machine trans-
lation (MT) for closely related languages and on
multilingual text normalization. Though language-
relatedness makes MT easier (Kolovratnı́k et al.,
2010), state-of-the-art techniques such as neural
machine translation (NMT) have not performed
well for languages with little data (Östling and
Tiedemann, 2017). So while the Hawaiian translit-
eration problem could be cast as an instance of MT
or of NMT, we chose to sidestep the scarcity of
parallel data by not considering such approaches.

Hybrid approaches that combine expert knowl-
edge for well-understood structures with deep
learning for data-plentiful subproblems offer rich
opportunities for data-efficient modelling. Prior
work has combined FSTs with RNNs, although not
using the approximate FST-to-RNN composition
algorithm that we introduce here (in Appendix A.4).
For example, Sproat and Jaitly (2016) used an FST
to restrict the search space when decoding from an
RNN and Rastogi et al. (2016) incorporated RNN
information into an FST.

3 Data

3.1 Missionary & modern orthography

The primary difference between the missionary and
modern Hawaiian orthographies is that the mission-
ary orthography does not encode long vowels or
the glottal stop (see Appendix A.1). For example,
the following Hawaiian phrases were recorded by
a 19th-century German traveller in the missionary
orthography: Ua oia au, E ue ae oe ia Ii, E ao
ae oe ia ia (Chamisso, 1837, p. 7). In the mod-
ern orthography these become: Ua ‘ō ‘ia au ‘I am
speared’, E uē a‘e ‘oe iā ‘Ī‘ı̄ ‘You must weep for
‘Ī‘ı̄ (a person)’, and E a‘o a‘e ‘oe iā ia ‘You teach
him’ (Elbert and Pukui, 1979, p. 3).

We can convert text in the modern Hawaiian or-
thography backward chronologically to an approx-
imate missionary orthography by mapping each
glottal stop 〈‘〉 to the empty string ε, and each long
vowel, e.g. 〈ā ē ı̄ ō ū〉, to its corresponding short
vowel, 〈a e i o u〉. As a first approximation, we may
treat mappings from the modern-to-missionary or-
thographies as unambiguously many-to-one; thus
there is information loss. We will return to sec-
ondary differences between the orthographies in

Source Chars Words

Ulukau(160 texts) 6,518,451 1,334,451
Hi‘iakaikapoliopele 1,272,935 259,947
Wikipedia 577,794 10,221

Total 8,369,180 1,604,619

Figure 1: Modern data sources and their sizes.

Section 6. To illustrate, the following four words
in the modern orthography all map to the same
missionary string aa: a‘a (root), ‘a‘a (brave), ‘a‘ā
(crumbly lava rock), and ‘ā‘ā (stutter).

The forward mapping from missionary-to-
modern orthographies is one-to-many. Thus the
missionary string aa could map to a‘a, ‘a‘a, ‘a‘ā,
or ‘ā‘ā. The transliteration problem we address
here seeks to discover how we can use context to
recover the information not present in the mission-
ary orthography that modern Hawaiian orthography
retains.

3.2 Data sources

We draw on three sources for modern Hawai-
ian text: the main text of Hi‘iakaikapoliopele
(Ho‘oulumāhiehie, 2007), 160 short texts from
Ulukau: The Hawaiian Electronic Library, and
the full Hawaiian Wikipedia (see Figure 1).1

For evaluation, we simulate a missionary-era ver-
sion of the modern texts using the backward map-
ping described above. In addition, we evaluated
our models on a couple of 19th century newspaper
samples for which we have parallel missionary-era
and modern text. Both simulated and real parallel
corpora will be described in Section 5.

4 Models

We can frame the task of transliterating from
missionary-to-modern Hawaiian orthographies as a
sequence transduction problem. Many deep learn-
ing approaches (e.g. Sutskever et al., 2014; Graves,
2012) are not easily applicable to this task since
we do not have a sufficiently large dataset of paral-
lel texts. Instead, we focus on approaches that
mix hand-designed finite state transducers with
trained language models, including deep learning
approaches like RNNLMs (Mikolov et al., 2010).

1Ulukau: The Hawaiian Electronic Library: http:
//ulukau.org/, Hawaiian Wikipedia: https://haw.
wikipedia.org/. Both accessed 19 May 2018.



4931

4.1 Finite state transducers

Our initial approach represents the mapping from
missionary to modern orthography using a compo-
sition of (weighted) FSTs. For a thorough review
of FSTs, see Mohri (1997).

First, we construct a finite state acceptor, I , from
the input text. Here we construct a trivial chain-
shaped acceptor that accepts only the input text.
Each symbol in the input text is represented by a
state which emits this symbol on a single transition
that moves to the next state. The transition emit-
ting the final symbol in the string leads to the sole
accepting state.

Second, we construct a missionary-to-modern or-
thography conversion FST which we call C, which
models potential orthography changes that can oc-
cur when transliterating from the missionary to
modern Hawaiian orthography. For example, two
non-deterministic transitions introduce an optional
long-vowel map (a : ā) and (a : a). Another transi-
tion inserts glottal stops: (� : ‘). By capturing the
orthographic changes we know to occur, the com-
position I ◦C produces a large set of candidates to
be narrowed using the language model.

Third, we use the modern Hawaiian text from
Section 3.2 to construct and evaluate a number of
character-level n-gram language models, of vari-
ous combinations of order and Katz backoff and
Kneser-Ney (KN) smoothing (Katz, 1987; Kneser
and Ney, 1995); see Appendix A.5 for details.
N-gram language models can be expressed as
weighted FSTs. We denote the n-gram or weighted
FST language model as G. Character-level mod-
els are used as we wanted to generalize to out-
of-vocabulary words, which we expected to occur
frequently in a small corpus like the one we have
for Hawaiian.

Finally, we use this model to infer modern or-
thography given a piece of text in missionary or-
thography as input, then compose the FSTs to form
the search graph FST: S = I◦C◦G. The minimum
cost path through S gives the predicted modern or-
thography. Of these n-gram-based approached, we
found the Kneser-Ney-based models to perform
best; these approaches are called FST-C-NGRAM-
KN and FST-Cwb-NGRAM-KN.

We circumvent the lack of a large, non-simulated
parallel corpus by training the language model ex-
clusively on text in the modern Hawaiian orthogra-
phy. In turn, the orthographic transliteration FST
C produces candidates which are disambiguated by

the language model. The result is finally evaluated
against the ground-truth modern text.

Although the orthographic transliteration model
is an approximation, and thus not exhaustive, it
embodies an explicit and interpretable represen-
tation that can be easily extended independently
of the rest of the model. To illustrate how the ap-
proach can be extended, we constructed a variant
Cwb (wherewb stands for word boundary). Cwb op-
tionally inserts a space after each vowel using an ad-
ditional arc that maps (� : space), as diagrammed
in Appendix A.2. This variant is able to model
some changes in Hawaiian’s word-boundary con-
ventions (Wilson, 1976), such as alaila becoming a
laila which demarcates the preposition a ‘until’ and
noun laila ‘then’. We employ this variant to predict
modern equivalents from 19th century newspaper
samples in Section 5. Pseudocode summarizing
this method is shown in Appendix A.3. Example
predictions can be found in Appendix A.6.

4.2 FSTs with LSTM language models

As an alternative approach, we combined the
FST C in the previous section with an RNNLM
(Mikolov et al., 2010). RNNLMs often generalize
better than n-gram models.

An RNN is a neural network that models tem-
poral or sequential data, by iterating a function
mapping a state and input to a new state and out-
put. These can be stacked to form a deep RNN.
For language modelling, each step of the final
RNN layer models a word or character sequence
via p(w1, . . . ,wn) =

∏n
i=1 p(wi|w1:i−1) and can

be trained by maximum likelihood. Recent lan-
guage modeling work has typically used the long
short-term memory (LSTM) unit (Hochreiter and
Schmidhuber, 1997) for its favorable gradient prop-
agation properties. All RNNs in this paper are
LSTMs.

Our goal is to replace the n-gram language model
in the end-to-end FST approach with an RNNLM.
While the minimum cost path through an FST can
be computed exactly as done in the previous sec-
tion, it is not straightforward to compose the re-
lation defined by an FST with an arbitrary one
like that defined by an RNNLM. A minimum cost
path through the composition of the FST and the
RNNLM can be defined as a path (i.e. label se-
quence) that minimizes the sum of the FST path
cost and the RNNLM cost.



4932

We can approximately find a minimum cost path
of the composition of the two models by a breadth-
first search over the FST graph, or using a beam
search, as follows. At any particular iteration, con-
sider a single beam element. The beam element
holds the current FST and RNN states, and the path
taken through the FST so far. We follow each possi-
ble arc from the current FST state, each producing
a new child beam element, and feed the output
symbol into the RNN (unless it is �). There may be
duplicate beam elements due the nondeterminicity
of the FST; in this case, the lower cost edge wins.
We sort by the sum of the FST and RNN costs,
keep the lowest-cost K, and then proceed to the
next iteration. If a beam element is on an accepting
state of the FST, it is kept as-is between iterations.
Detailed pseudocode is provided in Appendix A.4.

In the following we will refer to the hybrid mod-
els as FST-RNNLM—or as FST-RNNLM-C and
FST-RNNLM-Cwb if we want to distinguish be-
tween which FST we used. Similarly, the FST-
only models will be referred to as FST-C and FST-
Cwb, with suffixes denoting what kind of n-gram
and smoothing were used. For example, FST-C-
7GRAM-KN denotes a FST-only model with an
7-gram language model and Kneser-Ney smooth-
ing. Details of the language models trained can be
found in Appendix A.5.

5 Results

Evaluation. Since we were unable to find a suf-
ficiently large corpus of parallel texts in the mis-
sionary and modern Hawaiian orthographies, we
instead used a corpus of modern Hawaiian texts
(ground-truth) as summarized in Section 3.2 and
Figure 1. Note that training the n-gram and RNN
language models required only this modern corpus.

To evaluate the accuracy of our approaches, we
derived a synthetic parallel corpus from these mod-
ern Hawaiian texts. We also used a small but real
parallel corpus, based on two 19th century newspa-
per texts and their hand-edited modern equivalents.

Simulated parallel corpus. To produce a simu-
lated parallel corpus (input-missionary), we sys-
tematically reduced the orthography in the mod-
ern texts using the backward mapping described in
Section 3.1. We then applied the two approaches
described in Section 4, with the aim of recovering
the information lost.

We evaluated the predicted modern text (predic-
tions) by computing

CERR =
d(prediction, ground-truth)

d(input-missionary, ground-truth)
,

where d denotes character-level edit distance. This
is a modification of character error rate, normalized
by the distance of the input and target rather than
by the length of the target. We note that CERR may
be high even when the predictions are very accu-
rate as d(input-missionary, ground-truth) is small
when the text is similar in both orthographies.

Table 1 reports the results of the approaches we
described in Section 4. Out of the Kneser-Ney
n-gram models, we found that the FST-C-9GRAM-
KN and the version modelling word boundaries
(FST-Cwb-9GRAM-KN) to perform best on the
synthetic parallel corpus and newspapers, respec-
tively. Cwb was not applied to the synthetic parallel
corpus as we did not model word splitting. The
hybrid models (FST-RNNLM) outperformed all
FST-only approaches.

Real parallel corpus (newspaper texts). Not
content to evaluate the model on simulated mis-
sionary orthography, we also evaluated it on two
newspaper texts, using selections originally pub-
lished in 1867 and 1894 for which we had 19th
century and manually-edited modern equivalents.
The newspaper selections discuss Kahahana, one
of the last kings of O‘ahu (Kamakau and Perreira,
2002), and Uluhaimalama, a garden party and se-
cret political gathering, held after the deposition of
Hawai‘i’s last queen (Pukui et al., 2006). Unlike
the synthetic missionary corpus evaluation where
we did not model word splitting, we found that
replacing C with Cwb on the newspaper texts sig-
nificantly improved the output, especially on the
FST-RNNLM model. Thus, we found the word-
splitting hybrid model (FST-RNNLM-Cwb) to be
the best performing model overall (Table 1).

6 Conclusions and future work

With this paper we introduced a new translitera-
tion problem to the field, that of mapping between
old and new Hawaiian orthographies—where the
modern Hawaiian orthography represents linguistic
information that is missing from older missionary-
era texts. One difficulty of this problem is that
there is a limited amount of Hawaiian data, making
data-hungry solutions like end-to-end deep learning



4933

LM perplexity Transliteration performance (%CERR)
Transliteration model Valid. Test Corpus Newspaper 1 Newspaper 2

FST-(C/Cwb)-7GRAM-KN 3.07 3.13 27.3% 50.1% / 38.7% 52.0% / 47.5%
FST-(C/Cwb)-9GRAM-KN 2.95 3.02 26.6% 50.7% / 39.3% 52.5% / 47.2%

FST-(C/Cwb)-11GRAM-KN 2.94 3.02 27.8% 53.9% / 41.3% 54.1% / 48.7%

FST-RNNLM-(C/Cwb) 2.65 2.69 16.3% 47.2% / 34.3% 49.8% / 41.2%

Table 1: Performance (%CERR). Slash-separated pairs denote FSTs incapable/capable of inserting word
boundaries, respectively; see Section 4. The -KN suffix denotes Kneser-Ney smoothing. The data from
Section 3.2 is used for evaluating the modern-orthography language model perplexity, and “Corpus”
evaluates test-set transliteration performance from the synthetic missionary text back to the original
modern text.

Input Ua lawe ola ia o Keawehano imua o Kahekili, a ua hai aku o Kapohu...
Prediction Ua lawe ola ‘ia ‘o Keawehano i mua o Kahekili, a ua ha‘i aku ‘o Kapohu...

Ground-truth Ua lawe ola ‘ia ‘o Keawehano i mua o Kahekili, a ua ha‘i aku ‘o Kapohū...

Figure 2: An example of (missionary input, predicted modern text, ground-truth), from each newspaper.
Note the correctly split word in the second example. Incorrect characters, which are quite rare, are shown
as red and underlined. More sample predictions can be found in Appendix A.6.

unlikely to work. To solve the transliteration prob-
lem, we therefore proposed two models: the first
was implemented end-to-end using weighted FSTs;
the second was a hybrid deep learning approach
that combined an FST and an RNNLM. Both mod-
els gave promising results, but the hybrid approach,
which allowed us to use a more powerful recurrent
neural network-based language model despite our
dataset’s small size, performed best. Factoring a
problem like ours into one part that can be mod-
elled exactly using expert domain knowledge and
into another part that can be learned directly from
data using deep learning is not novel; however it
is a promising research direction for data-efficient
modelling. To our knowledge, this paper is the first
to describe a procedure to compose an FST with an
RNN by approximately performing beam search
over the FST.

While the role of the RNNLM part of the hy-
brid approach may be obvious, the FST compo-
nent plays an important role too. For example, the
hand-designed FST component can be replaced
without needing to retrain the RNNLM. We tried
to showcase this modularity by constructing two
FSTs which we referred to as C and Cwb, where
only the latter allowed the insertion of spaces. Fu-
ture work could extend the FST to model ortho-
graphic changes suggested by an error analysis of
the current model’s predictions (see Appendix A.6).
These errors motivate new mappings for consonant

substitutions like (r : l) and (s : k) observed in
loanword adaptations (e.g. rose⇒ loke). The error
analysis also motivates mappings to delete spaces
( : �) and to handle contractions, like na’lii⇒ nā
ali‘i. We could further incorporate linguistic knowl-
edge of Hawaiian into the FST, which tells us, for
example, that a consonant is typically followed by
a vowel (Parker Jones, 2010). Additional improve-
ments to the hybrid model might be obtained by in-
creasing the amount of modern Hawaiian text used
to train the RNNLM. One way to do this would
be to accelerate the rate at which missionary-era
Hawaiian texts are modernized. To this end, we
hope that the present models will be used within
the Hawaiian community to semi-automate, and
thereby accelerate, the modernization of old Hawai-
ian texts.

Acknowledgments

We are grateful to M. Puakea Nogelmeier for pro-
viding an electronic copy of Hi‘iakaikapoliopele
(Ho‘oulumāhiehie, 2007).

References

Richard A Benton. 1981. The flight of the Amokura:
Oceanic languages and formal education in the
South Pacific. New Zealand Council for Educational
Research, Wellington.



4934

Adelbert von Chamisso. 1837. Über die Hawaiische
Sprache, Vorgelegt der Königlichen Academie der
Wissenschaften zu Berlin am 12, Januar, 1837. Wei-
dmann, Leipzig.

Samuel H. Elbert and Mary Kawena Pukui. 1979.
Hawaiian Grammar. University of Hawai‘i Press,
Honolulu.

Alex Graves. 2012. Sequence transduction with
recurrent neural networks. arXiv preprint
arXiv:1211.3711.

Jan Hajič, Jan Hric, and Vladislav Kuboň. 2000. Ma-
chine translation of very close languages. In ANLC

’00 Proceedings of the sixth conference on Applied
natural language processing, pages 7–12.

Sepp Hochreiter and Jürgen Schmidhuber. 1997.
Long short-term memory. Neural computation,
9(8):1735–1780.

Ho‘oulumāhiehie. 2007. Ka Mo‘olelo o
Hi‘iakaikapoliopele. Awaiaulu Press, Honolulu.
Edited by M. Puakea Nogelmeier.

Samuel Manaiakalani Kamakau and Hiapo Perreira.
2002. Ka mo‘olelo o Kahahana, māhele 1. Ka
Ho‘oilina, 1(1):102–121.

Slava Katz. 1987. Estimation of probabilities from
sparse data for the language model component of a
speech recognizer. IEEE transactions on acoustics,
speech, and signal processing, 35(3):400–401.

Reinhard Kneser and Hermann Ney. 1995. Improved
backing-off for m-gram language modeling. In
Acoustics, Speech, and Signal Processing, 1995.
ICASSP-95., 1995 International Conference on, vol-
ume 1, pages 181–184. IEEE.

David Kolovratnı́k, Natalia Klyueva, and Ondřej Bo-
jar. 2010. Statistical machine translation between
related and unrelated languages. In Proceedings of
the Conference on Theory and Practice of Informa-
tion Technologies (ITAT-09), pages 31–36.

Tomas Mikolov, Martin Karafiát, Lukás Burget, Jan
Černocký, and Sanjeev Khudanpur. 2010. Recurrent
neural network based language model. In Proceed-
ings of Interspeech, pages 1045–1048.

Mehryar Mohri. 1997. Finite-state transducers in lan-
guage and speech processing. Computational lin-
guistics, 23(2):269–311.

M. Puakea Nogelmeier. 2010. Mai Pa‘a i ka Leo: His-
torical Voice in Hawaiian Primary Materials: Look-
ing Forward and Listening Back. Bishop Museum
Press, Honolulu.

Robert Östling and Jörg Tiedemann. 2017. Neu-
ral machine translation for low-resource languages.
Computing Research Repository, arXiv:1708.05729.
Version 1.

Oiwi Parker Jones. 2010. A computational phonology
and morphology of Hawaiian. Ph.D. thesis, Univer-
sity of Oxford.

Oiwi Parker Jones. 2018. Illustrations of the IPA:
Hawaiian. Journal of the International Phonetic As-
sociation, 48:103–115.

Mary Kawena Pukui and Samuel H. Elbert. 1957.
Hawaiian-English Dictionary. University of
Hawai‘i Press, Honolulu.

Mary Kawena Pukui, Holo Ho‘opai, Oiwi Parker Jones,
and Keao NeSmith. 2006. No ka mahi‘ai ‘ana,
māhele 6. Ka Ho‘oilina, 5(1):2–23.

Pushpendre Rastogi, Ryan Cotterell, and Jason Eisner.
2016. Weighted finite-state transductions with neu-
ral context. In Proceedings of the 2016 Conference
of the North American Chapter of the Association
for Computational Linguistics: Human Language
Technologies, pages 623–633.

Kevin Scannell. 2014. Statistical models for text nor-
malization and machine translation. In Proceedings
of the First Celtic Language Technology Workshop,
pages 33–40.

Albert J. Schütz. 1994. The Voices of Eden: A his-
tory of Hawaiian language studies. University of
Hawai‘i Press, Honolulu.

Richard Sproat and Navdeep Jaitly. 2016. RNN
approaches to text normalization: A challenge.
Computing Research Repository, arXiv:1611.00068.
Version 2.

Ilya Sutskever, Oriol Vinyals, and Quoc V Le. 2014.
Sequence to sequence learning with neural networks.
In Advances in neural information processing sys-
tems, pages 3104–3112.

Sam L. No‘eau Warner. 2001. The movement to re-
vitalize Hawaiian language and culture. In Leanne
Hinton and Kenneth Hale, editors, The Green Book
of Language Revitalization in Practice, pages –144.
Academic Press, San Diego, CA.

William H. Wilson. 1976. Standardized Hawaiian or-
thography. Manuscript, University of Hawai‘i.

William H. Wilson and Kauanoe Kamanā. 2001. “Mai
loko mai o ka ‘i‘ini: Proceeding from a dream”: The
‘Aha Pūnana Leo connection in Hawaiian language
revitalization. In Leanne Hinton and Kenneth Hale,
editors, The Green Book of Language Revitalization
in Practice, pages 147–176. Academic Press, San
Diego, CA.


