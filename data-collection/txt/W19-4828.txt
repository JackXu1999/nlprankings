



















































What Does BERT Look at? An Analysis of BERT's Attention


Proceedings of the Second BlackboxNLP Workshop on Analyzing and Interpreting Neural Networks for NLP, pages 276–286
Florence, Italy, August 1, 2019. c©2019 Association for Computational Linguistics

276

What does BERT look at?
An Analysis of BERT’s Attention

Kevin Clark† Urvashi Khandelwal† Omer Levy‡ Christopher D. Manning†
†Computer Science Department, Stanford University

‡Facebook AI Research
{kevclark,urvashik,manning}@cs.stanford.edu

omerlevy@fb.com

Abstract

Large pre-trained neural networks such as
BERT have had great recent success in NLP,
motivating a growing body of research investi-
gating what aspects of language they are able
to learn from unlabeled data. Most recent anal-
ysis has focused on model outputs (e.g., lan-
guage model surprisal) or internal vector rep-
resentations (e.g., probing classifiers). Com-
plementary to these works, we propose meth-
ods for analyzing the attention mechanisms of
pre-trained models and apply them to BERT.
BERT’s attention heads exhibit patterns such
as attending to delimiter tokens, specific po-
sitional offsets, or broadly attending over the
whole sentence, with heads in the same layer
often exhibiting similar behaviors. We further
show that certain attention heads correspond
well to linguistic notions of syntax and coref-
erence. For example, we find heads that at-
tend to the direct objects of verbs, determiners
of nouns, objects of prepositions, and corefer-
ent mentions with remarkably high accuracy.
Lastly, we propose an attention-based probing
classifier and use it to further demonstrate that
substantial syntactic information is captured in
BERT’s attention.

1 Introduction

Large pre-trained language models achieve very
high accuracy when fine-tuned on supervised tasks
(Dai and Le, 2015; Peters et al., 2018; Radford
et al., 2018), but it is not fully understood why.
The strong results suggest pre-training teaches the
models about the structure of language, but what
specific linguistic features do they learn?

Recent work has investigated this question by
examining the outputs of language models on
carefully chosen input sentences (Linzen et al.,
2016) or examining the internal vector representa-
tions of the model through methods such as prob-
ing classifiers (Adi et al., 2017; Belinkov et al.,
2017). Complementary to these approaches, we

study1 the attention maps of a pre-trained model.
Attention (Bahdanau et al., 2015) has been a
highly successful neural network component. It is
naturally interpretable because an attention weight
has a clear meaning: how much a particular word
will be weighted when computing the next repre-
sentation for the current word. Our analysis fo-
cuses on the 144 attention heads in BERT (De-
vlin et al., 2019), a large pre-trained Transformer
(Vaswani et al., 2017) network that has demon-
strated excellent performance on many tasks.

We first explore generally how the attention
heads behave. We find that there are common pat-
terns in their behavior, such as attending to fixed
positional offsets or attending broadly over the
whole sentence. A surprisingly large amount of
BERT’s attention focuses on the deliminator to-
ken [SEP], which we argue is used by the model
as a sort of no-op. Generally we find that attention
heads in the same layer tend to behave similarly.

We next probe each attention head for linguis-
tic phenomena. In particular, we treat each atten-
tion head as a simple no-training-required classi-
fier that, given a word as input, outputs the most-
attended-to other word. We then evaluate the abil-
ity of the heads to classify various syntactic rela-
tions. While no single head performs well at many
relations, we find that particular heads correspond
remarkably well to particular relations. For exam-
ple, we find heads that find direct objects of verbs,
determiners of nouns, objects of prepositions, and
objects of possesive pronouns with >75% accu-
racy. We perform a similar analysis for corefer-
ence resolution, also finding a BERT head that per-
forms quite well. These results are intriguing be-
cause the behavior of the attention heads emerges
purely from self-supervised training on unlabeled
data, without explicit supervision for syntax or
coreference.

1Code will be released at https://github.com/
clarkkev/attention-analysis.

https://github.com/clarkkev/attention-analysis
https://github.com/clarkkev/attention-analysis


277

 Head 1-1 
 

Attends broadly 
 
 

Head 3-1 
 

Attends to next token 
 
 

Head 8-7 
 

Attends to [SEP] 
 
 

Head 11-6 
 

Attends to periods 
 
 

Figure 1: Examples of heads exhibiting the patterns discussed in Section 3. The darkness of a line indicates the
strength of the attention weight (some attention weights are so low they are invisible).

Our findings show that particular heads special-
ize to specific aspects of syntax. To get a more
overall measure of the attention heads’ syntac-
tic ability, we propose an attention-based probing
classifier that takes attention maps as input. The
classifier achieves 77 UAS at dependency pars-
ing, showing BERT’s attention captures a substan-
tial amount about syntax. Several recent works
have proposed incorporating syntactic information
to improve attention (Eriguchi et al., 2016; Chen
et al., 2018; Strubell et al., 2018). Our work sug-
gests that to an extent this kind of syntax-aware
attention already exists in BERT, which may be
one of the reason for its success.

2 Background: Transformers and BERT

Although our analysis methods are applicable
to any model that uses an attention mechanism,
in this paper we analyze BERT (Devlin et al.,
2019), a large Transformer (Vaswani et al., 2017)
network. Transformers consist of multiple lay-
ers where each layer contains multiple attention
heads. An attention head takes as input a sequence
of vectors h = [h1, ..., hn] corresponding to the
n tokens of the input sentence. Each vector hi
is transformed into query, key, and value vectors
qi, ki, vi through separate linear transformations.
The head computes attention weights α between
all pairs of words as softmax-normalized dot prod-
ucts between the query and key vectors. The out-
put o of the attention head is a weighted sum of the
value vectors.

αij =
exp (qTi kj)∑n
l=1 exp (q

T
i kl)

oi =

n∑

j=1

αijvj

Attention weights can be viewed as governing how
“important” every other token is when producing
the next representation for the current token.

BERT is pre-trained on 3.3 billion tokens of un-
labeled text to perform two tasks. In the “masked
language modeling” task, the model predicts the
identities of words that have been masked-out of
the input text. In the “next sentence prediction”
task, the model predicts whether the second half
of the input follows the first half of the input in
the corpus, or is a completely separate random text
segment. Further training the model on supervised
data results in impressive performance across a va-
riety of tasks ranging from sentiment analysis to
question answering. An important detail of BERT
is the preprocessing used for the input text. A spe-
cial token [CLS] is added to the beginning of the
text and another token [SEP] is added to the end. If
the input consists of multiple separate texts (e.g., a
reading comprehension example consists of a sep-
arate question and context), [SEP] tokens are also
used to separate them. As we show in the next sec-
tion, these special tokens play an important role in
BERT’s attention. We use the “base” sized BERT
model, which has 12 layers containing 12 attention
heads each. We use <layer>-<head number> to
denote a particular attention head.

3 Surface-Level Patterns in Attention

Before looking at specific linguistic phenomena,
we first perform an analysis of surface-level pat-
terns in how BERT’s attention heads behave. Ex-
amples of heads exhibiting these patterns are
shown in Figure 1.



278

2 4 6 8 10 12
Layer

0.0

0.2

0.4

0.6

0.8
Av

g.
 A

tte
nt

io
n

[CLS]
[SEP]
. or ,

2 4 6 8 10 12
Layer

0.0

0.2

0.4

0.6

0.8

1.0

Av
g.

 A
tte

nt
io

n

[SEP] -> [SEP]
other -> [SEP]

Figure 2: Each point corresponds to the average atten-
tion a particular BERT attention head puts toward a to-
ken type. Above: heads often attend to “special” to-
kens. Early heads attend to [CLS], middle heads attend
to [SEP], and deep heads attend to periods and com-
mas. Often more than half of a head’s total attention is
to these tokens. Below: heads attend to [SEP] tokens
even more when the current token is [SEP] itself.

Setup. We extract the attention maps from
BERT-base over 1000 Wikipedia segments. We
follow the setup used for pre-training BERT
where each segment consists of at most 128
tokens corresponding to two consecutive para-
graphs of Wikipedia. The input presented to the
model is [CLS]<paragraph-1>[SEP]<paragraph-
2>[SEP].

3.1 Relative Position

First, we compute how often BERT attention
heads attend to the current token, the previous to-
ken, or the next token. We find that most heads
put little attention on the current token. However,
there are heads that specialize to attending heavily
on the next or previous token, especially in ear-
lier layers of the network. In particular four atten-
tion heads (in layers 2, 4, 7, and 8) on average put
>50% of their attention on the previous token and
five attention heads (in layers 1, 2, 2, 3, and 6) put
>50% of their attention on the next token.

2 4 6 8 10 12

Layer

0.0

0.5

1.0

1.5

2.0

2.5

3.0

A
ve

ra
ge
∣ ∣∂
L
∂
α

∣ ∣

All unmasked tokens

[SEP]

. or ,

Figure 3: Gradient-based feature importance estimates
for attention to [SEP], periods/commas, and other to-
kens.

2 4 6 8 10 12
Layer

0

2

4
Av

g.
 A

tte
nt

io
n 

En
tro

py
 (n

at
s)

uniform attention

BERT Heads

Figure 4: Entropies of attention distributions. In the
first layer there are particularly high-entropy heads that
produce bag-of-vector-like representations.

3.2 Attending to Separator Tokens

Interestingly, we found that a substantial amount
of BERT’s attention focuses on a few tokens (see
Figure 2). For example, over half of BERT’s at-
tention in layers 6-10 focuses on [SEP]. To put
this in context, since most of our segments are 128
tokens long, the average attention for a token oc-
curring twice in a segments like [SEP] would nor-
mally be 1/64. [SEP] and [CLS] are guaranteed
to be present and are never masked out, while pe-
riods and commas are the most common tokens
in the data excluding “the,” which might be why
the model treats these tokens differently. A sim-
ilar pattern occurs for the uncased BERT model,
suggesting there is a systematic reason for the at-
tention to special tokens rather than it being an ar-
tifact of stochastic training.

One possible explanation is that [SEP] is used
to aggregate segment-level information which can
then be read by other heads. However, further
analysis makes us doubtful this is the case. If this



279

hypothesis were true, we would expect attention
heads processing [SEP] to attend broadly over the
whole segment to build up these representations.
However, they instead almost entirely (more than
90%; see bottom of Figure 2) attend to themselves
and the other [SEP] token. Furthermore, qualita-
tive analysis (see Figure 5) shows that heads with
specific functions attend to [SEP] when the func-
tion is not called for. For example, in head 8-10
direct objects attend to their verbs. For this head,
non-nouns mostly attend to [SEP]. Therefore, we
speculate that attention over these special tokens
might be used as a sort of “no-op” when the atten-
tion head’s function is not applicable.

To further investigate this hypothesis, we ap-
ply gradient-based measures of feature importance
(Sundararajan et al., 2017). In particular, we com-
pute the magnitude of the gradient of the loss from
BERT’s masked language modeling task with re-
spect to each attention weight. Intuitively, this
value measures how much changing the attention
to a token will change BERT’s outputs. Results are
shown in Figure 3. We find that starting in layer 5
– the same layer where attention to [SEP] becomes
high – the gradients for attention to [SEP] become
small. This indicates that attending more or less to
[SEP] does not substantially change BERT’s out-
puts, supporting the theory that attention to [SEP]
is used as a no-op for attention heads.

3.3 Focused vs Broad Attention

Lastly, we measure whether attention heads fo-
cus on a few words or attend broadly over many
words. To do this, we compute the average en-
tropy of each head’s attention distribution (see
Figure 4). We find that some attention heads, es-
pecially in lower layers, have very broad atten-
tion. These high-entropy attention heads typically
spend at most 10% of their attention mass on any
single word. The output of these heads is roughly
a bag-of-vectors representation of the sentence.

We also measured entropies for all attention
heads from only the [CLS] token. While the av-
erage entropies from [CLS] for most layers are
very close to the ones shown in Figure 4, the
last layer has a high entropy from [CLS] of 3.89
nats, indicating very broad attention. This find-
ing makes sense given that the representation for
the [CLS] token is used as input for the “next sen-
tence prediction” task during pre-training, so it at-
tends broadly to aggregate a representation for the

whole input in the last layer.

4 Probing Individual Attention Heads

Next, we investigate individual attention heads to
probe what aspects of language they have learned.
In particular, we evaluate attention heads on la-
beled datasets for tasks like dependency parsing.
An overview of our results is shown in Figure 5.

4.1 Method
We wish to evaluate attention heads at word-level
tasks, but BERT uses byte-pair tokenization (Sen-
nrich et al., 2016), which means some words
(∼8% in our data) are split up into multiple to-
kens. We therefore convert token-token attention
maps to word-word attention maps. For attention
to a split-up word, we sum up the attention weights
over its tokens. For attention from a split-up word,
we take the mean of the attention weights over its
tokens. These transformations preserve the prop-
erty that the attention from each word sums to
1. For a given attention head and word, we take
whichever other word receives the most attention
weight as that model’s prediction2

4.2 Dependency Syntax
Setup. We extract attention maps from BERT on
the Wall Street Journal portion of the Penn Tree-
bank (Marcus et al., 1993) annotated with Stanford
Dependencies. We evaluate both “directions” of
prediction for each attention head: the head word
attending to the dependent and the dependent at-
tending to the head word. Some dependency rela-
tions are simpler to predict than others: for exam-
ple a noun’s determiner is often the immediately
preceding word. Therefore as a point of compar-
ison, we show predictions from a simple fixed-
offset baseline. For example, a fixed offset of -2
means the word two positions to the left of the de-
pendent is always considered to be the head.

Results. Table 1 shows that there is no single at-
tention head that does well at syntax “overall”; the
best head gets 34.5 UAS, which is not much better
than the right-branching baseline, which gets 26.3
UAS. This finding is similar to the one reported by
Raganato and Tiedemann (2018), who also evalu-
ate individual attention heads for syntax.

However, we do find that certain attention heads
specialize to specific dependency relations, some-

2We ignore [SEP] and [CLS], although in practice this
does not significantly change the accuracies for most heads.



280

 

Head 9-6 
 

- Prepositions attend to their objects 
 

- 76.3% accuracy at the pobj relation 

Head 8-11 
 

- Noun modifiers (e.g., determiners) attend 
  to their noun 
 

- 94.3% accuracy at the det relation 

Head 8-10 
 

- Direct objects attend to their verbs 
 

- 86.8% accuracy at the dobj relation 

Head 7-6 
 

- Possessive pronouns and apostrophes 
  attend to the head of the corresponding NP 
 

- 80.5% accuracy at the poss relation 

Head 4-10 
 

- Passive auxiliary verbs attend to the 
  verb they modify 
 

- 82.5% accuracy at the auxpass relation 

Head 5-4 
 

- Coreferent mentions attend to their antecedents 
 

- 65.1% accuracy at linking the head of a  
  coreferent mention to the head of an antecedent 

Figure 5: BERT attention heads that correspond to linguistic phenomena. In the example attention maps, the
darkness of a line indicates the strength of the attention weight. All attention to/from red words is colored red;
these colors are there to highlight certain parts of the attention heads’ behaviors. For Head 9-6, we don’t show
attention to [SEP] for clarity. Despite not being explicitly trained on these tasks, BERT’s attention heads perform
remarkably well, illustrating how syntax-sensitive behavior can emerge from self-supervised training alone.



281

Relation Head Accuracy Baseline

All 7-6 34.5 26.3 (1)
prep 7-4 66.7 61.8 (-1)
pobj 9-6 76.3 34.6 (-2)
det 8-11 94.3 51.7 (1)
nn 4-10 70.4 70.2 (1)
nsubj 8-2 58.5 45.5 (1)
amod 4-10 75.6 68.3 (1)
dobj 8-10 86.8 40.0 (-2)
advmod 7-6 48.8 40.2 (1)
aux 4-10 81.1 71.5 (1)

poss 7-6 80.5 47.7 (1)
auxpass 4-10 82.5 40.5 (1)
ccomp 8-1 48.8 12.4 (-2)
mark 8-2 50.7 14.5 (2)
prt 6-7 99.1 91.4 (-1)

Table 1: The best performing attentions heads of
BERT on WSJ dependency parsing by dependency
type. Numbers after baseline accuracies show the best
offset found (e.g., (1) means the word to the right is
predicted as the head). We show the 10 most common
relations as well as 5 other ones attention heads did well
on. Bold highlights particularly effective heads.

times achieving high accuracy and substantially
outperforming the fixed-offset baseline. We find
that for all relations in Table 1 except pobj, the
dependent attends to the head word rather than the
other way around, likely because each dependent
has exactly one head but heads have multiple de-
pendents. We also note heads can disagree with
standard annotation conventions while still per-
forming syntactic behavior. For example, head 7-
6 marks ’s as the dependent for the poss relation,
while gold-standard labels mark the complement
of an ’s as the dependent (the accuracy in Table 1
counts ’s as correct). Such disagreements high-
light how these syntactic behaviors in BERT are
learned as a by-product of self-supervised train-
ing, not by copying a human design.

Figure 5 shows some examples of the attention
behavior. While the similarity between learned at-
tention weights and human-defined syntactic re-
lations are striking, we note these are relations
for which attention heads do particularly well on.
There are many relations for which BERT only
slightly improves over the simple baseline, so we
would not say individual attention heads capture
dependency structure as a whole.

4.3 Coreference Resolution

Having shown BERT attention heads reflect cer-
tain aspects of syntax, we now explore using at-
tention heads for the more challenging semantic
task of coreference resolution. Coreference links
are usually longer than syntactic dependencies and
state-of-the-art systems generally perform much
worse at coreference compared to parsing.

Setup. We evaluate the attention heads on coref-
erence resolution using the CoNLL-2012 dataset3

(Pradhan et al., 2012). In particular, we compute
antecedent selection accuracy: what percent of the
time does the head word of a coreferent mention
most attend to the head of one of that mention’s
antecedents. We compare against three baselines
for selecting an antecedent:

• Picking the nearest other mention.
• Picking the nearest other mention with the

same head word as the current mention.

• A simple rule-based system inspired by Lee
et al. (2011). It proceeds through 4 sieves: (1)
full string match, (2) head word match, (3)
number/gender/person match, (4) all other
mentions. The nearest mention satisfying the
earliest sieve is returned.

We also show the performance of a recent neural
coreference system from (Wiseman et al., 2015).

Results. Results are shown in Table 2. We find
that one of BERT’s attention heads achieves de-
cent coreference resolution performance, improv-
ing by over 10 accuracy points on the string-
matching baseline and performing close to the
rule-based system. It is particularly good with
nominal mentions, perhaps because it is capable
of fuzzy matching between synonyms as seen in
the bottom right of Figure 5.

5 Probing Attention Head Combinations

Since individual attention heads specialize to par-
ticular aspects of syntax, the model’s overall
“knowledge” about syntax is distributed across
multiple attention heads. We now measure this
overall ability by proposing a novel family of
attention-based probing classifiers and applying
them to dependency parsing. For these classifiers
we treat the BERT attention outputs as fixed, i.e.,

3We truncate documents to 128 tokens long to keep mem-
ory usage manageable.



282

Model All Pronoun Proper Nominal

Nearest 27 29 29 19
Head-word
match

52 47 67 40

Rule-based 69 70 77 60
Neural coref 83* – – –

Head 5-4 65 64 73 58

*Only roughly comparable because on non-truncated docu-
ments and with different mention detection.

Table 2: Accuracies (%) for different mention types of
systems selecting a correct antecedent given a corefer-
ent mention in the CoNLL-2012 data. One of BERT’s
attention heads performs fairly well at coreference.

we do not back-propagate into BERT and only
train a small number of parameters.

The probing classifiers are basically graph-
based dependency parsers. Given an input word,
the classifier produces a probability distribution
over other words in the sentence indicating how
likely each other word is to be the syntactic head
of the current one.

Attention-Only Probe. Our first probe learns a
simple linear combination of attention weights.

p(i|j) ∝ exp
( n∑

k=1

wkα
k
ij + ukα

k
ji

)

where p(i|j) is the probability of word i being
word j’s syntactic head, αkij is the attention weight
from word i to word j produced by head k, and n
is the number of attention heads. We include both
directions of attention: candidate head to depen-
dent as well as dependent to candidate head. The
weight vectors w and u are trained using standard
supervised learning on the train set.

Attention-and-Words Probe. Given our finding
that heads specialize to particular syntactic rela-
tions, we believe probing classifiers should benefit
from having information about the input words. In
particular, we build a model that sets the weights
of the attention heads based on the GloVe (Pen-
nington et al., 2014) embeddings for the input
words. Intuitively, if the dependent and candi-
date head are “the” and “cat,” the probing classi-
fier should learn to assign most of the weight to
the head 8-11, which achieves excellent perfor-
mance at the determiner relation. The attention-
and-words probing classifier assigns the probabil-

ity of word i being word j’s head as

p(i|j) ∝ exp
( n∑

k=1

Wk,:(vi ⊕ vj)αkij+

Uk,:(vi ⊕ vj)αkji
)

Where v denotes GloVe embeddings and ⊕ de-
notes concatenation. The GloVe embeddings are
held fixed in training, so only the two weight ma-
trices W and U are learned. The dot product
Wk,:(vi⊕vj) produces a word-sensitive weight for
the particular attention head.

Results. We evaluate our methods on the Penn
Treebank dev set annotated with Stanford depen-
dencies. We compare against three baselines:

• A right-branching baseline that always pre-
dicts the head is to the dependent’s right.

• A simple one-hidden-layer network that takes
as input the GloVe embeddings for the depen-
dent and candidate head as well as a set of
features indicating the distances between the
two words.4

• Our attention-and-words probe, but with at-
tention maps from a BERT network with pre-
trained word/positional embeddings but ran-
domly initialized other weights. This kind of
baseline is surprisingly strong at other prob-
ing tasks (Conneau et al., 2018).

Results are shown in Table 3. We find the Attn
+ GloVe probing classifier substantially outper-
forms our baselines and achieves a decent UAS
of 77, suggesting BERT’s attention maps have a
fairly thorough representation of English syntax.
As a rough comparison, we also report results
from the structural probe from Hewitt and Man-
ning (2019), which builds a probing classifier on
top of BERT’s vector representations rather than
attention. The scores are not directly compara-
ble because the structural probe only uses a sin-
gle layer of BERT, produces undirected rather than
directed parse trees, and is trained to produce the
syntactic distance between words rather than di-
rectly predicting the tree structure. Nevertheless,
the similarity in score to our Attn + Glove probing
classifier suggests there is not much more syntac-
tic information in BERT’s vector representations
compared to its attention maps.

4indicator features for short distances as well as continu-
ous distance features, with distance ahead/behind treated sep-
arately to capture word order



283

Model UAS

Right-branching 26
Distances + GloVe 58
Random Init Attn + GloVe 30
Attn 61
Attn + GloVe 77
Structural probe (Hewitt and
Manning, 2019)

80 UUAS*

Table 3: Results of attention-based probing tasks on
dependency parsing. A simple model taking BERT at-
tention maps and GloVe word embeddings as input per-
forms quite well at dependency parsing. *Not directly
comparable to our numbers; see text.

Overall, our results from probing both individ-
ual and combinations of attention heads suggest
that BERT learns some aspects syntax purely as a
by-product of self-supervised training. Other work
has drawn a similar conclusions from examin-
ing BERT’s predictions on agreement tasks (Gold-
berg, 2019) or internal vector representations (He-
witt and Manning, 2019; Liu et al., 2019). Tra-
ditionally, syntax-aware models have been devel-
oped through architecture design (e.g., recursive
neural networks) or from direct supervision from
human-curated treebanks. Our findings are part of
a growing body of work indicating that indirect
supervision from rich pre-training tasks like lan-
guage modeling can also produce models sensitive
to language’s hierarchical structure.

6 Clustering Attention Heads

Are attention heads in the same layer similar to
each other or different? In general, can attention
heads be clearly grouped by behavior? We inves-
tigate these questions by computing the distances
between all pairs of attention heads. Formally, we
measure the distance between two heads Hi and
Hj as:

∑

token∈data
JS(Hi(token),Hj(token))

Where JS is the Jensen-Shannon Divergence be-
tween attention distributions. Using these dis-
tances, we visualize the attention heads by apply-
ing multidimensional scaling (Kruskal, 1964) to
embed each head in two dimensions such that the
euclidean distance between embeddings reflects
the Jensen-Shannon distance between the corre-
sponding heads as closely as possible.

Figure 6: BERT attention heads embedded in two-
dimensional space. Distance between points approx-
imately matches the average Jensen-Shannon diver-
gences between the outputs of the corresponding heads.
Heads in the same layer tend to be close together. At-
tention head “behavior” was found through the analysis
methods discussed throughout this paper.

Results are shown in Figure 6. We find that
there are several clear clusters of heads that be-
have similarly, often corresponding to behaviors
we have already discussed in this paper. Heads
within the same layer are often fairly close to each
other, meaning that heads within the layer have
similar attention distributions. This finding is a bit
surprising given that Tu et al. (2018) show that en-
couraging attention heads to have different behav-
iors can improve Transformer performance at ma-



284

chine translation. One possibility for the apparent
redundancy in BERT’s attention heads is the use
of attention dropout, which causes some attention
weights to be zeroed-out during training.

7 Related Work

There has been substantial recent work perform-
ing analysis to better understand what neural net-
works learn, especially from language model pre-
training. One line of research examines the out-
puts of language models on carefully chosen in-
put sentences (Linzen et al., 2016; Khandelwal
et al., 2018; Gulordava et al., 2018; Marvin and
Linzen, 2018). For example, the model’s perfor-
mance at subject-verb agreement (generating the
correct number of a verb far away from its sub-
ject) provides a measure of the model’s syntactic
ability, although it does not reveal how that ability
is captured by the network.

Another line of work investigates the internal
vector representations of the model (Adi et al.,
2017; Giulianelli et al., 2018; Zhang and Bow-
man, 2018), often using probing classifiers. Prob-
ing classifiers are simple neural networks that take
the internal vector representations of a pre-trained
model as input. They are trained to do a supervised
task (e.g., part-of-speech tagging). If a probing
classifier achieves high accuracy, it suggests that
the vector representations reflect the correspond-
ing aspect of language (e.g., low-level syntax).
Like our work, some of these studies have also
demonstrated neural networks capturing aspects of
syntactic structures (Shi et al., 2016; Blevins et al.,
2018) or coreference (Tenney et al., 2018, 2019)
without explicitly being trained for the tasks.

With regards to analyzing attention, Vig (2019)
builds a visualization tool for the BERT’s attention
and reports observations about some of the heads’
behaviors, but does not perform any quantitative
analysis. Burns et al. (2018) analyze the attention
of memory networks to understand model perfor-
mance on a question answering dataset; we instead
aim to understand linguistic information captured
in pre-trained models. There has also been some
initial work in correlating attention with syntax.
Raganato and Tiedemann (2018) evaluate the at-
tention heads of a machine translation model on
dependency parsing, but only report overall UAS
scores instead of investigating heads for specific
syntactic relations or using probing classifiers.
Marecek and Rosa (2018) propose heuristic ways

of converting attention scores to syntactic trees,
but do not quantitatively evaluate their approach.

Concurrently with our work Voita et al. (2019)
identify syntactic, positional, and rare-word-
sensitive attention heads in machine translation
models. They also demonstrate that many atten-
tion heads can be pruned away without substan-
tially hurting model performance. Interestingly,
the important attention heads that remain after
pruning tend to be ones with identified behaviors.
Michel et al. (2019) similarly show that many of
BERT’s attention heads can be pruned. Although
our analysis in this paper only found interpretable
behaviors in a subset of BERT’s attention heads,
these recent works suggest that there might not be
much to explain for some attention heads because
they have little effect on model perfomance.

Jain and Wallace (2019) argue that attention of-
ten does not “explain” model predictions. They
show that attention weights frequently do not cor-
relate with other measures of feature importance.
Furthermore, attention weights can often be sub-
stantially changed without altering model predic-
tions. However, our motivation for looking at at-
tention is different: rather than explaining model
predictions, we are seeking to understand infor-
mation learned by the models. For example, if
a particular attention head learns a syntactic rela-
tion, we consider that an important finding from
an analysis perspective even if that head is not
always used when making predictions for some
downstream task.

8 Conclusion

We have proposed a series of analysis methods for
understanding the attention mechanisms of mod-
els and applied them to BERT. While most recent
work on model analysis for NLP has focused on
probing vector representations or model outputs,
we have shown that a substantial amount of lin-
guistic knowledge can be found not only in the
hidden states, but also in the attention maps. We
think probing attention maps complements these
other model analysis techniques, and should be
part of the toolkit used by researchers to under-
stand what neural networks learn about language.

Acknowledgements

We thank the anonymous reviews for their
thoughtful comments and suggestions. Kevin is
supported by a Google PhD Fellowship.



285

References
Yossi Adi, Einat Kermany, Yonatan Belinkov, Ofer

Lavi, and Yoav Goldberg. 2017. Fine-grained anal-
ysis of sentence embeddings using auxiliary predic-
tion tasks. In ICLR.

Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben-
gio. 2015. Neural machine translation by jointly
learning to align and translate. In ICLR.

Yonatan Belinkov, Nadir Durrani, Fahim Dalvi, Has-
san Sajjad, and James R. Glass. 2017. What do neu-
ral machine translation models learn about morphol-
ogy? In ACL.

Terra Blevins, Omer Levy, and Luke S. Zettlemoyer.
2018. Deep rnns encode soft hierarchical syntax. In
ACL.

Kaylee Burns, Aida Nematzadeh, Alison Gopnik, and
Thomas L. Griffiths. 2018. Exploiting attention to
reveal shortcomings in memory models. In Black-
boxNLP@EMNLP.

Kehai Chen, Rui Wang, Masao Utiyama, Eiichiro
Sumita, and Tiejun Zhao. 2018. Syntax-directed at-
tention for neural machine translation. In AAAI.

Alexis Conneau, Germán Kruszewski, Guillaume
Lample, Loı̈c Barrault, and Marco Baroni. 2018.
What you can cram into a single $&!#* vector:
Probing sentence embeddings for linguistic proper-
ties. In ACL.

Andrew M Dai and Quoc V Le. 2015. Semi-supervised
sequence learning. In NIPS.

Jacob Devlin, Ming-Wei Chang, Kenton Lee, and
Kristina Toutanova. 2019. BERT: Pre-training of
deep bidirectional transformers for language under-
standing. In NAACL-HLT.

Akiko Eriguchi, Kazuma Hashimoto, and Yoshimasa
Tsuruoka. 2016. Tree-to-sequence attentional neu-
ral machine translation. In ACL.

Mario Giulianelli, Jack Harding, Florian Mohnert,
Dieuwke Hupkes, and Willem H. Zuidema. 2018.
Under the hood: Using diagnostic classifiers to in-
vestigate and improve how language models track
agreement information. In BlackboxNLP@EMNLP.

Yoav Goldberg. 2019. Assessing BERT’s syntactic
abilities. arXiv preprint arXiv:1901.05287.

Kristina Gulordava, Piotr Bojanowski, Edouard Grave,
Tal Linzen, and Marco Baroni. 2018. Colorless
green recurrent networks dream hierarchically. In
NAACL-HLT.

John Hewitt and Christopher D. Manning. 2019. Find-
ing syntax with structural probes. In NAACL-HLT.

Sarthak Jain and Byron C. Wallace. 2019. Attention is
not explanation. arXiv preprint arXiv:1902.10186.

Urvashi Khandelwal, He He, Peng Qi, and Daniel Ju-
rafsky. 2018. Sharp nearby, fuzzy far away: How
neural language models use context. In ACL.

Joseph B Kruskal. 1964. Multidimensional scaling by
optimizing goodness of fit to a nonmetric hypothe-
sis. Psychometrika, 29(1):1–27.

Heeyoung Lee, Yves Peirsman, Angel Chang,
Nathanael Chambers, Mihai Surdeanu, and Dan Ju-
rafsky. 2011. Stanford’s multi-pass sieve corefer-
ence resolution system at the conll-2011 shared task.
In CoNLL.

Tal Linzen, Emmanuel Dupoux, and Yoav Goldberg.
2016. Assessing the ability of lstms to learn syntax-
sensitive dependencies. TACL.

Nelson F. Liu, Matt Gardner, Yonatan Belinkov, M. Pe-
ters, and Noah A. Smith. 2019. Linguistic knowl-
edge and transferability of contextual representa-
tions. In NAACL-HLT.

Mitchell P Marcus, Mary Ann Marcinkiewicz, and
Beatrice Santorini. 1993. Building a large annotated
corpus of english: The Penn treebank. Computa-
tional linguistics, 19(2):313–330.

David Marecek and Rudolf Rosa. 2018. Extract-
ing syntactic trees from transformer encoder self-
attentions. In BlackboxNLP@EMNLP.

Rebecca Marvin and Tal Linzen. 2018. Targeted syn-
tactic evaluation of language models. In EMNLP.

Paul Michel, Omer Levy, and Graham Neubig. 2019.
Are sixteen heads really better than one? arXiv
preprint arXiv:1905.10650.

Jeffrey Pennington, Richard Socher, and Christopher
Manning. 2014. Glove: Global vectors for word
representation. In EMNLP.

Matthew E Peters, Mark Neumann, Mohit Iyyer, Matt
Gardner, Christopher Clark, Kenton Lee, and Luke
Zettlemoyer. 2018. Deep contextualized word rep-
resentations. In NAACL-HLT.

Sameer Pradhan, Alessandro Moschitti, Nianwen Xue,
Olga Uryupina, and Yuchen Zhang. 2012. Conll-
2012 shared task: Modeling multilingual unre-
stricted coreference in ontonotes. In Joint Confer-
ence on EMNLP and CoNLL-Shared Task.

Alec Radford, Karthik Narasimhan, Tim Salimans,
and Ilya Sutskever. 2018. Improving lan-
guage understanding by generative pre-training.
https://blog.openai.com/language-unsupervised.

Alessandro Raganato and Jörg Tiedemann. 2018.
An analysis of encoder representations in
transformer-based machine translation. In Black-
boxNLP@EMNLP.

Rico Sennrich, Barry Haddow, and Alexandra Birch.
2016. Neural machine translation of rare words with
subword units. In ACL.



286

Xing Shi, Inkit Padhi, and Kevin Knight. 2016. Does
string-based neural mt learn source syntax? In
EMNLP.

Emma Strubell, Patrick Verga, Daniel Andor,
David I Weiss, and Andrew McCallum. 2018.
Linguistically-informed self-attention for semantic
role labeling. In EMNLP.

Mukund Sundararajan, Ankur Taly, and Qiqi Yan.
2017. Axiomatic attribution for deep networks. In
ICML.

Ian Tenney, Dipanjan Das, and Ellie Pavlick. 2019.
Bert rediscovers the classical nlp pipeline. arXiv
preprint arXiv:1905.05950.

Ian Tenney, Patrick Xia, Berlin Chen, Alex Wang,
Adam Poliak, R Thomas McCoy, Najoung Kim,
Benjamin Van Durme, Samuel R Bowman, Dipan-
jan Das, et al. 2018. What do you learn from con-
text? probing for sentence structure in contextual-
ized word representations. In ICLR.

Zhaopeng Tu, Baosong Yang, Michael R. Lyu, and
Tong Zhang. 2018. Multi-head attention with dis-
agreement regularization. In EMNLP.

Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob
Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz
Kaiser, and Illia Polosukhin. 2017. Attention is all
you need. In NIPS.

Jesse Vig. 2019. Visualizing attention in transformer-
based language models. arXiv preprint
arXiv:1904.02679.

Elena Voita, David Talbot, Fedor Moiseev, Rico Sen-
nrich, and Ivan Titov. 2019. Analyzing multi-
head self-attention: Specialized heads do the heavy
lifting, the rest can be pruned. arXiv preprint
arXiv:1905.09418.

Sam Joshua Wiseman, Alexander Matthew Rush, Stu-
art Merrill Shieber, and Jason Weston. 2015. Learn-
ing anaphoricity and antecedent ranking features for
coreference resolution. In ACL.

Kelly W. Zhang and Samuel R. Bowman. 2018. Lan-
guage modeling teaches you more syntax than trans-
lation does: Lessons learned through auxiliary task
analysis. In BlackboxNLP@EMNLP.


