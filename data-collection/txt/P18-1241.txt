




































Attacking Visual Language Grounding with Adversarial Examples: A Case Study on Neural Image Captioning


Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers), pages 2587–2597
Melbourne, Australia, July 15 - 20, 2018. c©2018 Association for Computational Linguistics

2587

Attacking Visual Language Grounding with Adversarial Examples:
A Case Study on Neural Image Captioning

Hongge Chen1*, Huan Zhang23*, Pin-Yu Chen3, Jinfeng Yi4, and Cho-Jui Hsieh2
1MIT, Cambridge, MA 02139, USA
2UC Davis, Davis, CA 95616, USA

3IBM Research, NY 10598, USA
4JD AI Research, Beijing, China

chenhg@mit.edu, ecezhang@ucdavis.edu

pin-yu.chen@ibm.com, yijinfeng@jd.com, chohsieh@ucdavis.edu
*Hongge Chen and Huan Zhang contribute equally to this work

Abstract
Visual language grounding is widely stud-
ied in modern neural image caption-
ing systems, which typically adopts an
encoder-decoder framework consisting of
two principal components: a convolu-
tional neural network (CNN) for image
feature extraction and a recurrent neural
network (RNN) for language caption gen-
eration. To study the robustness of lan-
guage grounding to adversarial perturba-
tions in machine vision and perception,
we propose Show-and-Fool, a novel al-
gorithm for crafting adversarial examples
in neural image captioning. The pro-
posed algorithm provides two evaluation
approaches, which check whether neural
image captioning systems can be mislead
to output some randomly chosen captions
or keywords. Our extensive experiments
show that our algorithm can successfully
craft visually-similar adversarial examples
with randomly targeted captions or key-
words, and the adversarial examples can
be made highly transferable to other image
captioning systems. Consequently, our ap-
proach leads to new robustness implica-
tions of neural image captioning and novel
insights in visual language grounding.

1 Introduction

In recent years, language understanding grounded
in machine vision and perception has made re-
markable progress in natural language processing
(NLP) and artificial intelligence (AI), such as im-
age captioning and visual question answering. Im-
age captioning is a multimodal learning task and
has been used to study the interaction between lan-
guage and vision models (Shekhar et al., 2017). It

takes an image as an input and generates a lan-
guage caption that best describes its visual con-
tents, and has many important applications such
as developing image search engines with complex
natural language queries, building AI agents that
can see and talk, and promoting equal web ac-
cess for people who are blind or visually impaired.
Modern image captioning systems typically adopt
an encoder-decoder framework composed of two
principal modules: a convolutional neural network
(CNN) as an encoder for image feature extraction
and a recurrent neural network (RNN) as a decoder
for caption generation. This CNN+RNN archi-
tecture includes popular image captioning mod-
els such as Show-and-Tell (Vinyals et al., 2015),
Show-Attend-and-Tell (Xu et al., 2015) and Neu-
ralTalk (Karpathy and Li, 2015).

Recent studies have highlighted the vulnerabil-
ity of CNN-based image classifiers to adversarial
examples: adversarial perturbations to benign im-
ages can be easily crafted to mislead a well-trained
classifier, leading to visually indistinguishable ad-
versarial examples to human (Szegedy et al., 2014;
Goodfellow et al., 2015). In this study, we in-
vestigate a more challenging problem in visual
language grounding domain that evaluates the ro-
bustness of multimodal RNN in the form of a
CNN+RNN architecture, and use neural image
captioning as a case study. Note that crafting ad-
versarial examples in image captioning tasks is
strictly harder than in well-studied image classifi-
cation tasks, due to the following reasons: (i) class
attack v.s. caption attack: unlike classification
tasks where the class labels are well defined, the
output of image captioning is a set of top-ranked
captions. Simply treating different captions as dis-
tinct classes will result in an enormous number
of classes that can even precede the number of
training images. In addition, semantically similar



2588

Figure 1: Adversarial examples crafted by Show-
and-Fool using the targeted caption method. The
target captioning model is Show-and-Tell (Vinyals
et al., 2015), the original images are selected from
the MSCOCO validation set, and the targeted cap-
tions are randomly selected from the top-1 inferred
caption of other validation images.

captions can be expressed in different ways and
hence should not be viewed as different classes;
and (ii) CNN v.s. CNN+RNN: attacking RNN
models is significantly less well-studied than at-
tacking CNN models. The CNN+RNN architec-
ture is unique and beyond the scope of adversarial
examples in CNN-based image classifiers.

In this paper, we tackle the aforementioned
challenges by proposing a novel algorithm called
Show-and-Fool. We formulate the process of
crafting adversarial examples in neural image cap-
tioning systems as optimization problems with
novel objective functions designed to adopt the
CNN+RNN architecture. Specifically, our objec-
tive function is a linear combination of the dis-
tortion between benign and adversarial examples
as well as some carefully designed loss functions.
The proposed Show-and-Fool algorithm provides
two approaches to craft adversarial examples in
neural image captioning under different scenarios:

1. Targeted caption method: Given a targeted
caption, craft adversarial perturbations to any
image such that its generated caption matches
the targeted caption.

2. Targeted keyword method: Given a set of
keywords, craft adversarial perturbations to
any image such that its generated caption
contains the specified keywords. The cap-
tioning model has the freedom to make sen-
tences with target keywords in any order.

As an illustration, Figure 1 shows an adversarial
example crafted by Show-and-Fool using the tar-
geted caption method. The adversarial perturba-
tions are visually imperceptible while can success-
fully mislead Show-and-Tell to generate the tar-
geted captions. Interestingly and perhaps surpris-
ingly, our results pinpoint the Achilles heel of the
language and vision models used in the tested im-
age captioning systems. Moreover, the adversar-
ial examples in neural image captioning highlight
the inconsistency in visual language grounding be-
tween humans and machines, suggesting a possi-
ble weakness of current machine vision and per-
ception machinery. Below we highlight our major
contributions:

• We propose Show-and-Fool, a novel optimiza-
tion based approach to crafting adversarial ex-
amples in image captioning. We provide two
types of adversarial examples, targeted caption
and targeted keyword, to analyze the robustness
of neural image captioners. To the best of our
knowledge, this is the very first work on craft-
ing adversarial examples for image captioning.
• We propose powerful and generic loss functions

that can craft adversarial examples and evaluate
the robustness of the encoder-decoder pipelines
in the form of a CNN+RNN architecture. In par-
ticular, our loss designed for targeted keyword
attack only requires the adversarial caption to
contain a few specified keywords; and we al-
low the neural network to make meaningful sen-
tences with these keywords on its own.
• We conduct extensive experiments on the

MSCOCO dataset. Experimental results show
that our targeted caption method attains a 95.8%
attack success rate when crafting adversarial ex-
amples with randomly assigned captions. In ad-
dition, our targeted keyword attack yields an
even higher success rate. We also show that
attacking CNN+RNN models is inherently dif-
ferent and more challenging than only attacking



2589

CNN models.
• We also show that Show-and-Fool can produce

highly transferable adversarial examples: an
adversarial image generated for fooling Show-
and-Tell can also fool other image captioning
models, leading to new robustness implications
of neural image captioning systems.

2 Related Work

In this section, we review the existing work on vi-
sual language grounding, with a focus on neural
image captioning. We also review related work
on adversarial attacks on CNN-based image clas-
sifiers. Due to space limitations, we defer the sec-
ond part to the supplementary material.

Visual language grounding represents a fam-
ily of multimodal tasks that bridge visual and
natural language understanding. Typical exam-
ples include image and video captioning (Karpa-
thy and Li, 2015; Vinyals et al., 2015; Donahue
et al., 2015b; Pasunuru and Bansal, 2017; Venu-
gopalan et al., 2015), visual dialog (Das et al.,
2017; De Vries et al., 2017), visual question an-
swering (Antol et al., 2015; Fukui et al., 2016;
Lu et al., 2016; Zhu et al., 2017), visual story-
telling (Huang et al., 2016), natural question gen-
eration (Mostafazadeh et al., 2017, 2016), and im-
age generation from captions (Mansimov et al.,
2016; Reed et al., 2016). In this paper, we focus on
studying the robustness of neural image captioning
models, and believe that the proposed method also
sheds lights on robustness evaluation for other vi-
sual language grounding tasks using a similar mul-
timodal RNN architecture.

Many image captioning methods based on deep
neural networks (DNNs) adopt a multimodal RNN
framework that first uses a CNN model as the
encoder to extract a visual feature vector, fol-
lowed by a RNN model as the decoder for cap-
tion generation. Representative works under this
framework include (Chen and Zitnick, 2015; De-
vlin et al., 2015; Donahue et al., 2015a; Karpa-
thy and Li, 2015; Mao et al., 2015; Vinyals et al.,
2015; Xu et al., 2015; Yang et al., 2016; Liu et al.,
2017a,b), which are mainly differed by the under-
lying CNN and RNN architectures, and whether
or not the attention mechanisms are considered.
Other lines of research generate image captions
using semantic information or via a compositional
approach (Fang et al., 2015; Gan et al., 2017; Tran
et al., 2016; Jia et al., 2015; Wu et al., 2016; You

et al., 2016).
The recent work in (Shekhar et al., 2017)

touched upon the robustness of neural image cap-
tioning for language grounding by showing its in-
sensitivity to one-word (foil word) changes in the
language caption, which corresponds to the untar-
geted attack category in adversarial examples. In
this paper, we focus on the more challenging tar-
geted attack setting that requires to fool the cap-
tioning models and enforce them to generate pre-
specified captions or keywords.

3 Methodology of Show-and-Fool

3.1 Overview of the Objective Functions

We now formally introduce our approaches to
crafting adversarial examples for neural image
captioning. The problem of finding an adversar-
ial example for a given image I can be cast as the
following optimization problem:

min
δ

c · loss(I + δ) + ‖δ‖22

s.t. I + δ ∈ [−1, 1]n. (1)

Here δ denotes the adversarial perturbation to I .
‖δ‖22 = ‖(I + δ) − I‖22 is an `2 distance metric
between the original image and the adversarial im-
age. loss(·) is an attack loss function which takes
different forms in different attacking settings. We
will provide the explicit expressions in Sections
3.2 and 3.3. The term c > 0 is a pre-specified reg-
ularization constant. Intuitively, with larger c, the
attack is more likely to succeed but at the price of
higher distortion on δ. In our algorithm, we use
a binary search strategy to select c. The box con-
straint on the image I ∈ [−1, 1]n ensures that the
adversarial example I + δ ∈ [−1, 1]n lies within a
valid image space.

For the purpose of efficient optimization, we
convert the constrained minimization problem in
(1) into an unconstrained minimization problem
by introducing two new variables y ∈ Rn and
w ∈ Rn such that

y = arctanh(I) and w = arctanh(I + δ)− y,

where arctanh denotes the inverse hyperbolic tan-
gent function and is applied element-wisely. Since
tanh(yi + wi) ∈ [−1, 1], the transformation will
automatically satisfy the box constraint. Conse-
quently, the constrained optimization problem in



2590

(1) is equivalent to

minw∈Rn c · loss(tanh(w + y)) (2)
+‖ tanh(w + y)− tanh(y)‖22.

In the following sections, we present our designed
loss functions for different attack settings.

3.2 Targeted Caption Method
Note that a targeted caption is denoted by

S = (S1, S2, ..., St, ..., SN ),

where St indicates the index of the t-th word in
the vocabulary list V , S1 is a start symbol and SN
indicates the end symbol. N is the length of cap-
tion S, which is not fixed but does not exceed a
predefined maximum caption length. To encour-
age the neural image captioning system to output
the targeted caption S, one needs to ensure the log
probability of the caption S conditioned on the im-
age I + δ attains the maximum value among all
possible captions, that is,

logP (S|I + δ) = max
S′∈Ω

logP (S′|I + δ), (3)

where Ω is the set of all possible captions. It is
also common to apply the chain rule to the joint
probability and we have

logP (S′|I+δ) =
N∑
t=2

logP (S′t|I+δ, S′1, ..., S′t−1).

In neural image captioning networks,
p(S′t|I + δ, S′1, ..., S′t−1) is usually computed
by a RNN/LSTM cell f , with its hidden state ht−1
and input S′t−1:

zt = f(ht−1, S
′
t−1) and pt = softmax(zt), (4)

where zt := [z
(1)
t , z

(2)
t , ..., z

(|V|)
t ] ∈ R|V| is a vec-

tor of the logits (unnormalized probabilities) for
each possible word in the vocabulary. The vector
pt represents a probability distribution on V with
each coordinate p(i)t defined as:

p
(i)
t := P (S

′
t = i|I + δ, S′1, ..., S′t−1).

Following the definition of softmax function:

P (S′t|I+δ, S′1, ..., S′t−1) = exp(z
(S′t)
t )/

∑
i∈V

exp(z
(i)
t ).

Intuitively, to maximize the targeted caption’s
probability, we can directly use its negative log

probability (5) as a loss function. The inputs of
the RNN are the first N − 1 words of the targeted
caption (S1, S2, ..., SN−1).

lossS,log-prob(I + δ) = − logP (S|I + δ)

= −
N∑
t=2

logP (St|I + δ, S1, ..., St−1).
(5)

Applying (5) to (2), the formulation of targeted
caption method given a targeted caption S is:

min
w∈Rn

c · lossS,log prob(tanh(w + y))

+ ‖ tanh(w + y)− tanh(y)‖22.

Alternatively, using the definition of the soft-
max function,

logP (S′|I + δ) =
N∑
t=2

[z
(S′t)
t − log(

∑
i∈V

exp(z
(i)
t ))]

=
N∑
t=2

z
(S′t)
t − constant, (6)

(3) can be simplified as

logP (S|I + δ) ∝
N∑
t=2

z
(St)
t = max

S′∈Ω

N∑
t=2

z
(S′t)
t .

Instead of making each z(St)t as large as possi-
ble, it is sufficient to require the target word St
to attain the largest (top-1) logit (or probability)
among all the words in the vocabulary at position
t. In other words, we aim to minimize the differ-
ence between the maximum logit except St, de-
noted by maxk∈V,k 6=St{z

(k)
t }, and the logit of St,

denoted by z(St)t . We also propose a ramp function
on top of this difference as the final loss function:

lossS,logits(I+δ) =
N−1∑
t=2

max{−�,max
k 6=St
{z(k)t }−z

(St)
t },

(7)
where � > 0 is a confidence level accounting for
the gap between maxk 6=St{z

(k)
t } and z

(St)
t . When

z
(St)
t > maxk 6=St{z

(k)
t } + �, the corresponding

term in the summation will be kept at−� and does
not contribute to the gradient of the loss function,
encouraging the optimizer to focus on minimizing
other terms where z(St)t is not large enough.

Applying the loss (7) to (1), the final formula-
tion of targeted caption method given a targeted



2591

caption S is

min
w∈Rn

c ·
N−1∑
t=2

max{−�,max
k 6=St
{z(k)t } − z

(St)
t }

+ ‖ tanh(w + y)− tanh(y)‖22.

We note that (Carlini and Wagner, 2017) has re-
ported that in CNN-based image classification, us-
ing logits in the attack loss function can produce
better adversarial examples than using probabili-
ties, especially when the target network deploys
some gradient masking schemes such as defensive
distillation (Papernot et al., 2016b). Therefore, we
provide both logit-based and probability-based at-
tack loss functions for neural image captioning.

3.3 Targeted Keyword Method

In addition to generating an exact targeted cap-
tion by perturbing the input image, we offer an
intermediate option that aims at generating cap-
tions with specific keywords, denoted by K :=
{K1, · · · ,KM} ⊂ V . Intuitively, finding an ad-
versarial image generating a caption with specific
keywords might be easier than generating an exact
caption, as we allow more degree of freedom in
caption generation. However, as we need to ensure
a valid and meaningful inferred caption, finding an
adversarial example with specific keywords in its
caption is difficult in an optimization perspective.
Our target keyword method can be used to investi-
gate the generalization capability of a neural cap-
tioning system given only a few keywords.

In our method, we do not require a target key-
word Kj , j ∈ [M ] to appear at a particular po-
sition. Instead, we want a loss function that al-
lows Kj to become the top-1 prediction (plus a
confidence margin �) at any position. Therefore,
we propose to use the minimum of the hinge-like
loss terms over all t ∈ [N ] as an indication of Kj
appearing at any position as the top-1 prediction,
leading to the following loss function:

lossK,logits =
M∑
j=1

min
t∈[N ]
{max{−�,max

k 6=Kj
{z(k)t }−z

(Kj)
t }}.

(8)
We note that the loss functions in (4) and (5)

require an input S′t−1 to predict zt for each t ∈
{2, . . . , N}. For the targeted caption method, we
use the targeted caption S as the input of RNN.
In contrast, for the targeted keyword method we
no longer know the exact targeted sentence, but

only require the presence of specified keywords in
the final caption. To bridge the gap, we use the
originally inferred caption S0 = (S01 , · · · , S0N )
from the benign image as the initial input to RNN.
Specifically, after minimizing (8) for T iterations,
we run inference on I + δ and set the RNN’s input
S1 as its current top-1 prediction, and continue this
process. With this iterative optimization process,
the desired keywords are expected to gradually ap-
pear in top-1 prediction.

Another challenge arises in targeted keyword
method is the problem of “keyword collision”.
When the number of keywords M ≥ 2, more
than one keywords may have large values of
maxk 6=Kj{z

(k)
t } − z

(Kj)
t at a same position t. For

example, if dog and cat are top-2 predictions for
the second word in a caption, the caption can ei-
ther start with “A dog ...” or “A cat ...”. In this
case, despite the loss (8) being very small, a cap-
tion with both dog and cat can hardly be gener-
ated, since only one word is allowed to appear at
the same position. To alleviate this problem, we
define a gate function gt,j(x) which masks off all
the other keywords when a keyword becomes top-
1 at position t:

gt,j(x) =

{
A, if arg maxi∈V z

(i)
t ∈ K \ {Kj}

x, otherwise,

where A is a predefined value that is significantly
larger than common logits values. Then (8) be-
comes:

M∑
j=1

min
t∈[N ]
{gt,j(max{−�, max

k 6=Kj
{z(k)t } − z

(Kj)
t })}.

(9)

The log-prob loss for targeted keyword method is
discussed in the Supplementary Material.

4 Experiments

4.1 Experimental Setup and Algorithms
We performed extensive experiments to test the ef-
fectiveness of our Show-and-Fool algorithm and
study the robustness of image captioning systems
under different problem settings. In our experi-
ments1, we use the pre-trained TensorFlow imple-
mentation2 of Show-and-Tell (Vinyals et al., 2015)

1Our source code is available at: https://github.com/
huanzhang12/ImageCaptioningAttack

2https://github.com/tensorflow/models/tree/master/
research/im2txt



2592

with Inception-v3 as the CNN for visual feature
extraction. Our testbed is Microsoft COCO (Lin
et al., 2014) (MSCOCO) data set. Although some
more recent neural image captioning systems can
achieve better performance than Show-and-Tell,
they share a similar framework that uses CNN
for feature extraction and RNN for caption gen-
eration, and Show-and-Tell is the vanilla version
of this CNN+RNN architecture. Indeed, we find
that the adversarial examples on Show-and-Tell
are transferable to other image captioning mod-
els such as Show-Attend-and-Tell (Xu et al., 2015)
and NeuralTalk23, suggesting that the attention
mechanism and the choice of CNN and RNN ar-
chitectures do not significantly affect the robust-
ness. We also note that since Show-and-Fool is
the first work on crafting adversarial examples for
neural image captioning, to the best of our knowl-
edge, there is no other method for comparison.

We use ADAM to minimize our loss functions
and set the learning rate to 0.005. The number of
iterations is set to 1, 000. All the experiments are
performed on a single Nvidia GTX 1080 Ti GPU.
For targeted caption and targeted keyword meth-
ods, we perform a binary search for 5 times to find
the best c: initially c = 1, and c will be increased
by 10 times until a successful adversarial example
is found. Then, we choose a new c to be the aver-
age of the largest c where an adversarial example
can be found and the smallest cwhere an adversar-
ial example cannot be found. We fix � = 1 except
for transferability experiments. For each experi-
ment, we randomly select 1,000 images from the
MSCOCO validation set. We use BLEU-1 (Pa-
pineni et al., 2002), BLEU-2, BLEU-3, BLEU-
4, ROUGE (Lin, 2004) and METEOR (Lavie and
Agarwal, 2005) scores to evaluate the correlations
between the inferred captions and the targeted cap-
tions. These scores are widely used in NLP com-
munity and are adopted by image captioning sys-
tems for quality assessment. Throughout this sec-
tion, we use the logits loss (7)(9). The results of
using the log-prob loss (5) are similar and are re-
ported in the supplementary material.

4.2 Targeted Caption Results

Unlike the image classification task where all pos-
sible labels are predefined, the space of possible
captions in a captioning system is almost infinite.
However, the captioning system is only able to

3https://github.com/karpathy/neuraltalk2

Table 1: Summary of targeted caption method
(Section 3.2) and targeted keyword method (Sec-
tion 3.3) using logits loss. The `2 distortion of
adversarial noise ‖δ‖2 is averaged over success-
ful adversarial examples. For comparison, we also
include CNN based attack methods (Section 4.5).

Experiments Success Rate Avg. ‖δ‖2
targeted caption 95.8% 2.213

1-keyword 97.1% 1.589
2-keyword 97.5% 2.363
3-keyword 96.0% 2.626

C&W on CNN 22.4% 2.870
I-FGSM on CNN 34.5% 15.596

Table 2: Statistics of the 4.2% failed adversarial
examples using the targeted caption method and
logits loss (7). All correlation scores are computed
using the top-5 inferred captions of an adversar-
ial image and the targeted caption (higher score
means better targeted attack performance).

c 1 10 102 103 104

`2 Distortion 1.726 3.400 7.690 16.03 23.31
BLEU-1 .567 .725 .679 .701 .723
BLEU-2 .420 .614 .559 .585 .616
BLEU-3 .320 .509 .445 .484 .514
BLEU-4 .252 .415 .361 .402 .417
ROUGE .502 .664 .629 .638 .672

METEOR .258 .407 .375 .403 .399

output relevant captions learned from the train-
ing set. For instance, the captioning model can-
not generate a passive-voice sentence if the model
was never trained on such sentences. Therefore,
we need to ensure that the targeted caption lies in
the space where the captioning system can pos-
sibly generate. To address this issue, we use the
generated caption of a randomly selected image
(other than the image under investigation) from
MSCOCO validation set as the targeted caption S.
The use of a generated caption as the targeted cap-
tion excludes the effect of out-of-domain caption-
ing, and ensures that the target caption is within
the output space of the captioning network.

Here we use the logits loss (7) plus a `2 distor-
tion term (as in (2)) as our objective function. A
successful adversarial example is found if the in-
ferred caption after adding the adversarial pertur-
bation δ is exactly the same as the targeted caption.
In our setting, 1,000 ADAM iterations take about
38 seconds for one image. The overall success
rate and average distortion of adversarial perturba-
tion δ are shown in Table 1. Among all the tested
images, our method attains 95.8% attack success



2593

rate. Moreover, our adversarial examples have
small `2 distortions and are visually identical to
the original images, as displayed in Figure 1. We
also examine the failed adversarial examples and
summarize their statistics in Table 2. We find that
their generated captions, albeit not entirely identi-
cal to the targeted caption, are in fact highly corre-
lated to the desired one. Overall, the high success
rate and low `2 distortion of adversarial examples
clearly show that Show-and-Tell is not robust to
targeted adversarial perturbations.

4.3 Targeted Keyword Results
In this task, we use (9) as our loss function, and
choose the number of keywords M = {1, 2, 3}.
We run an inference step on I + δ every T = 5
iterations, and use the top-1 caption as the input
of RNN/LSTMs. Similar to Section 4.2, for each
image the targeted keywords are selected from the
caption generated by a randomly selected valida-
tion set image. To exclude common words like
“a”, “the”, “and”, we look up each word in the
targeted sentence and only select nouns, verbs, ad-
jectives or adverbs. We say an adversarial image is
successful when its caption contains all specified
keywords. The overall success rate and average
distortion are shown in Table 1. When compared
to the targeted caption method, targeted keyword
method achieves an even higher success rate (at
least 96% for 3-keyword case and at least 97%
for 1-keyword and 2-keyword cases). Figure 2
shows an adversarial example crafted from our
targeted keyword method with three keywords -
“dog”, “cat” and “frisbee”. Using Show-and-Fool,
the top-1 caption of a cake image becomes “A dog
and a cat are playing with a frisbee” while the ad-
versarial image remains visually indistinguishable
to the original one. WhenM = 2 and 3, even if we
cannot find an adversarial image yielding all spec-
ified keywords, we might end up with a caption
that contains some of the keywords (partial suc-
cess). For example, when M = 3, Table 3 shows
the number of keywords appeared in the captions
(M ′) for those failed examples (not all 3 targeted
keywords are found). These results clearly show
that the 4% failed examples are still partially suc-
cessful: the generated captions contain about 1.5
targeted keywords on average.

4.4 Transferability of Adversarial Examples
It has been shown that in image classification
tasks, adversarial examples found for one machine

Figure 2: An adversarial example (‖δ‖2 = 1.284)
of an cake image crafted by the Show-and-Fool
targeted keyword method with three keywords -
“dog”, “cat” and “frisbee”.

Table 3: Percentage of partial success with differ-
ent c in the 4.0% failed images that do not contain
all the 3 targeted keywords.

c Avg. ‖δ‖2 M ′ ≥ 1 M ′ = 2 Avg. M ′
1 2.49 72.4% 34.5% 1.07
10 5.40 82.7% 37.9% 1.21
102 12.95 93.1% 58.6% 1.52
103 24.77 96.5% 51.7% 1.48
104 29.37 100.0% 58.6% 1.59

learning model may also be effective against an-
other model, even if the two models have dif-
ferent architectures (Papernot et al., 2016a; Liu
et al., 2017c). However, unlike image classifica-
tion where correct labels are made explicit, two
different image captioning systems may generate
quite different, yet semantically similar, captions
for the same benign image. In image caption-
ing, we say an adversarial example is transfer-
able when the adversarial image found on model
A with a target sentence SA can generate a similar
(rather than exact) sentence SB on model B.

In our setting, model A is Show-and-Tell, and
we choose Show-Attend-and-Tell (Xu et al., 2015)
as model B. The major differences between
Show-and-Tell and Show-Attend-and-Tell are the
addition of attention units in LSTM network for
caption generation, and the use of last convolu-
tional layer (rather than the last fully-connected
layer) feature maps for feature extraction. We
use Inception-v3 as the CNN architecture for both
models and train them on the MSCOCO 2014 data
set. However, their CNN parameters are different
due to the fine-tuning process.



2594

Table 4: Transferability of adversarial examples from Show-and-Tell to Show-Attend-and-Tell, using
different � and c. ori indicates the scores between the generated captions of the original images and the
transferred adversarial images on Show-Attend-and-Tell. tgt indicates the scores between the targeted
captions on Show-and-Tell and the generated captions of transferred adversarial images on Show-Attend-
and-Tell. A smaller ori or a larger tgt value indicates better transferability. mis measures the differences
between captions generated by the two models given the same benign image (model mismatch). When
C = 1000, � = 10, tgt is close to mis, indicating the discrepancy between adversarial captions on the two
models is mostly bounded by model mismatch, and the adversarial perturbation is highly transferable.

� = 1 � = 5 � = 10
C=10 C=100 C=1000 C=10 C=100 C=1000 C=10 C=100 C=1000

ori tgt ori tgt ori tgt ori tgt ori tgt ori tgt ori tgt ori tgt ori tgt mis
BLEU-1 .474 .395 .384 .462 .347 .484 .441 .429 .368 .488 .337 .527 .431 .421 .360 .485 .339 .534 .649
BLEU-2 .337 .236 .230 .331 .186 .342 .300 .271 .212 .343 .175 .389 .287 .266 .204 .342 .174 .398 .521
BLEU-3 .256 .154 .151 .224 .114 .254 .220 .184 .135 .254 .103 .299 .210 .185 .131 .254 .102 .307 .424
BLEU-4 .203 .109 .107 .172 .077 .198 .170 .134 .093 .197 .068 .240 .162 .138 .094 .197 .066 .245 .352
ROUGE .463 .371 .374 .438 .336 .465 .429 .402 .359 .464 .329 .502 .421 .398 .351 .463 .328 .507 .604
METEOR .201 .138 .139 .180 .118 .201 .177 .157 .131 .199 .110 .228 .172 .157 .127 .202 .110 .232 .300
‖δ‖2 3.268 4.299 4.474 7.756 10.487 10.952 15.757 21.696 21.778

Figure 3: A highly transferable adversarial exam-
ple (‖δ‖2 = 15.226) crafted by Show-and-Tell tar-
geted caption method, transfers to Show-Attend-
and-Tell, yielding similar adversarial captions.

To investigate the transferability of adversarial
examples in image captioning, we first use the tar-
geted caption method to find adversarial examples
for 1,000 images in modelAwith different c and �,
and then transfer successful adversarial examples
(which generate the exact target captions on model
A) to model B. The generated captions by model
B are recorded for transferability analysis. The
transferability of adversarial examples depends on
two factors: the intrinsic difference between two
models even when the same benign image is used
as the input, i.e., model mismatch, and the trans-
ferability of adversarial perturbations.

To measure the mismatch between Show-and-
Tell and Show-Attend-and-Tell, we generate cap-
tions of the same set of 1,000 original images
from both models, and report their mutual BLEU,

ROUGE and METEOR scores in Table 4 under
the mis column. To evaluate the effectiveness of
transferred adversarial examples, we measure the
scores for two set of captions: (i) the captions of
original images and the captions of transferred ad-
versarial images, both generated by Show-Attend-
and-Tell (shown under column ori in Table 4); and
(ii) the targeted captions for generating adversarial
examples on Show-and-Tell, and the captions of
the transferred adversarial image on Show-Attend-
and-Tell (shown under column tgt in Table 4).
Small values of ori suggest that the adversarial
images on Show-Attend-and-Tell generate signif-
icantly different captions from original images’
captions. Large values of tgt suggest that the ad-
versarial images on Show-Attend-and-Tell gener-
ate similar adversarial captions as on the Show-
and-Tell model. We find that increasing c or �
helps to enhance transferability at the cost of larger
(but still acceptable) distortion. When C = 1, 000
and � = 10, Show-and-Fool achieves the best
transferability results: tgt is close to mis, indicat-
ing that the discrepancy between adversarial cap-
tions on the two models is mostly bounded by the
intrinsic model mismatch rather than the transfer-
ability of adversarial perturbations, and implying
that the adversarial perturbations are easily trans-
ferable. In addition, the adversarial examples gen-
erated by our method can also fool NeuralTalk2.
When c = 104, � = 10, the average `2 distortion,
BLEU-4 and METEOR scores between the origi-
nal and transferred adversarial captions are 38.01,
0.440 and 0.473, respectively. The high transfer-
ability of adversarial examples crafted by Show-



2595

and-Fool also indicates the problem of common
robustness leakage between different neural image
captioning models.

4.5 Attacking Image Captioning v.s.
Attacking Image Classification

In this section we show that attacking image cap-
tioning models is inherently more challenging
than attacking image classification models. In the
classification task, a targeted attack usually be-
comes harder when the number of labels increases,
since an attack method needs to change the classi-
fication prediction to a specific label over all the
possible labels. In the targeted attack on image
captioning, if we treat each caption as a label,
we need to change the original label to a specific
one over an almost infinite number of possible la-
bels, corresponding to a nearly zero volume in the
search space. This constraint forces us to develop
non-trivial methods that are significantly different
from the ones designed for attacking image classi-
fication models.

To verify that the two tasks are inherently dif-
ferent, we conducted additional experiments on
attacking only the CNN module using two state-
of-the-art image classification attacks on Ima-
geNet dataset. Our experiment setup is as fol-
lows. Each selected ImageNet image has a la-
bel corresponding to a WordNet synset ID. We
randomly selected 800 images from ImageNet
dataset such that their synsets have at least one
word in common with Show-and-Tell’s vocabu-
lary, while ensuring the Inception-v3 CNN (Show-
and-Tell’s CNN) classify them correctly. Then,
we perform Iterative Fast Gradient Sign Method
(I-FGSM) (Kurakin et al., 2017) and Carlini and
Wagner’s (C&W) attack (Carlini and Wagner,
2017) on these images. The attack target la-
bels are randomly chosen and their synsets also
have at least one word in common with Show-
and-Tell’s vocabulary. Both I-FGSM and C&W
achieve 100% targeted attack success rate on the
Inception-v3 CNN. These adversarial examples
were further employed to attack Show-and-Tell
model. An attack is considered successful if any
word in the targeted label’s synset or its hyper-
nyms up to 5 levels is presented in the resulting
caption. For example, for the chain of hypernyms
‘broccoli’⇒‘cruciferous vegetable’⇒‘vegetable,
veggie, veg’⇒‘produce, green goods, green gro-
ceries, garden truck’⇒‘food, solid food’, we in-

clude ‘broccoli’,‘cruciferous’,‘vegetable’,‘veggie’
and all other following words. Note that this cri-
terion of success is much weaker than the crite-
rion we use in the targeted caption method, since a
caption with the targeted image’s hypernyms does
not necessarily leads to similar meaning of the tar-
geted image’s captions. To achieve higher attack
success rates, we allow relatively larger distortions
and set �∞ = 0.3 (maximum `∞ distortion) in I-
FGSM and κ = 10, C = 100 in C&W. How-
ever, as shown in Table 1, the attack success rates
are only 34.5% for I-FGSM and 22.4% for C&W,
respectively, which are much lower than the suc-
cess rates of our methods despite larger distor-
tions. This result further confirms that perform-
ing targeted attacks on neural image captioning re-
quires a careful design (as proposed in this paper),
and attacking image captioning systems is not a
trivial extension to attacking image classifiers.

5 Conclusion

In this paper, we proposed a novel algorithm,
Show-and-Fool, for crafting adversarial examples
and providing robustness evaluation of neural im-
age captioning. Our extensive experiments show
that the proposed targeted caption and keyword
methods yield high attack success rates while the
adversarial perturbations are still imperceptible to
human eyes. We further demonstrate that Show-
and-Fool can generate highly transferable adver-
sarial examples. The high-quality and transferable
adversarial examples in neural image captioning
crafted by Show-and-Fool highlight the inconsis-
tency in visual language grounding between hu-
mans and machines, suggesting a possible weak-
ness of current machine vision and perception ma-
chinery. We also show that attacking neural image
captioning systems are inherently different from
attacking CNN-based image classifiers.

Our method stands out from the well-studied
adversarial learning on image classifiers and CNN
models. To the best of our knowledge, this is the
very first work on crafting adversarial examples
for neural image captioning systems. Indeed, our
Show-and-Fool algorithm1 can be easily extended
to other applications with RNN or CNN+RNN ar-
chitectures. We believe this paper provides poten-
tial means to evaluate and possibly improve the ro-
bustness (for example, by adversarial training or
data augmentation) of a wide range of visual lan-
guage grounding and other NLP models.



2596

References
Stanislaw Antol, Aishwarya Agrawal, Jiasen Lu, Mar-

garet Mitchell, Dhruv Batra, C Lawrence Zitnick,
and Devi Parikh. 2015. VQA: Visual question an-
swering. In Proceedings of the IEEE International
Conference on Computer Vision, pages 2425–2433.

Nicholas Carlini and David Wagner. 2017. Towards
evaluating the robustness of neural networks. In
IEEE Symposium on Security and Privacy, pages
39–57.

Pin-Yu Chen, Yash Sharma, Huan Zhang, Jinfeng Yi,
and Cho-Jui Hsieh. 2018. EAD: elastic-net attacks
to deep neural networks via adversarial examples.
AAAI.

Pin-Yu Chen, Huan Zhang, Yash Sharma, Jinfeng Yi,
and Cho-Jui Hsieh. 2017. ZOO: zeroth order opti-
mization based black-box attacks to deep neural net-
works without training substitute models. In Pro-
ceedings of the 10th ACM Workshop on Artificial In-
telligence and Security, AISec@CCS, pages 15–26.

Xinlei Chen and C. Lawrence Zitnick. 2015. Mind’s
eye: A recurrent visual representation for image cap-
tion generation. In CVPR, pages 2422–2431.

Abhishek Das, Satwik Kottur, Khushi Gupta, Avi
Singh, Deshraj Yadav, José MF Moura, Devi Parikh,
and Dhruv Batra. 2017. Visual dialog. In Proceed-
ings of the IEEE Conference on Computer Vision
and Pattern Recognition (CVPR), volume 2.

Harm De Vries, Florian Strub, Sarath Chandar, Olivier
Pietquin, Hugo Larochelle, and Aaron Courville.
2017. Guesswhat?! visual object discovery through
multi-modal dialogue. In CVPR.

Jacob Devlin, Hao Cheng, Hao Fang, Saurabh Gupta,
Li Deng, Xiaodong He, Geoffrey Zweig, and Mar-
garet Mitchell. 2015. Language models for image
captioning: The quirks and what works. In Proceed-
ings of the 53rd Annual Meeting of the Association
for Computational Linguistics and the 7th Interna-
tional Joint Conference on Natural Language Pro-
cessing (Volume 2: Short Papers), volume 2, pages
100–105.

Jeff Donahue, Lisa Anne Hendricks, Sergio Guadar-
rama, Marcus Rohrbach, Subhashini Venugopalan,
Trevor Darrell, and Kate Saenko. 2015a. Long-term
recurrent convolutional networks for visual recogni-
tion and description. In CVPR, pages 2625–2634.

Jeffrey Donahue, Lisa Anne Hendricks, Sergio Guadar-
rama, Marcus Rohrbach, Subhashini Venugopalan,
Kate Saenko, and Trevor Darrell. 2015b. Long-term
recurrent convolutional networks for visual recogni-
tion and description. In CVPR, pages 2625–2634.

Hao Fang, Saurabh Gupta, Forrest Iandola, Rupesh K
Srivastava, Li Deng, Piotr Dollár, Jianfeng Gao, Xi-
aodong He, Margaret Mitchell, John C Platt, et al.
2015. From captions to visual concepts and back.
In CVPR, pages 1473–1482.

Akira Fukui, Dong Huk Park, Daylen Yang, Anna
Rohrbach, Trevor Darrell, and Marcus Rohrbach.
2016. Multimodal compact bilinear pooling for
visual question answering and visual grounding.
In Proceedings of the 2016 Conference on Em-
pirical Methods in Natural Language Processing
(EMNLP), pages 457–468.

Zhe Gan, Chuang Gan, Xiaodong He, Yunchen Pu,
Kenneth Tran, Jianfeng Gao, Lawrence Carin, and
Li Deng. 2017. Semantic compositional networks
for visual captioning. In Proceedings of the IEEE
Conference on Computer Vision and Pattern Recog-
nition, pages 5630–5639.

Ian J Goodfellow, Jonathon Shlens, and Chris-
tian Szegedy. 2015. Explaining and harness-
ing adversarial examples. ICLR; arXiv preprint
arXiv:1412.6572.

Ting-Hao Kenneth Huang, Francis Ferraro, Nasrin
Mostafazadeh, Ishan Misra, Aishwarya Agrawal, Ja-
cob Devlin, Ross Girshick, Xiaodong He, Pushmeet
Kohli, Dhruv Batra, et al. 2016. Visual storytelling.
In Proceedings of the 2016 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics: Human Language Technologies
(NAACL-HLT), pages 1233–1239.

Xu Jia, Efstratios Gavves, Basura Fernando, and Tinne
Tuytelaars. 2015. Guiding the long-short term mem-
ory model for image caption generation. In Com-
puter Vision (ICCV), 2015 IEEE International Con-
ference on, pages 2407–2415. IEEE.

Andrej Karpathy and Fei-Fei Li. 2015. Deep visual-
semantic alignments for generating image descrip-
tions. In CVPR, pages 3128–3137.

Alexey Kurakin, Ian Goodfellow, and Samy Bengio.
2017. Adversarial machine learning at scale. ICLR;
arXiv preprint arXiv:1611.01236.

Alon Lavie and Abhaya Agarwal. 2005. Meteor: An
automatic metric for mt evaluation with improved
correlation with human judgments. In Proceedings
of the EMNLP 2011 Workshop on Statistical Ma-
chine Translation, pages 65–72.

Chin-Yew Lin. 2004. Rouge: A package for auto-
matic evaluation of summaries. In Text summariza-
tion branches out: Proceedings of the ACL-04 work-
shop, volume 8. Barcelona, Spain.

Tsung-Yi Lin, Michael Maire, Serge Belongie, James
Hays, Pietro Perona, Deva Ramanan, Piotr Dollár,
and C Lawrence Zitnick. 2014. Microsoft coco:
Common objects in context. In European confer-
ence on computer vision, pages 740–755. Springer.

Chenxi Liu, Junhua Mao, Fei Sha, and Alan L Yuille.
2017a. Attention correctness in neural image cap-
tioning. In AAAI, pages 4176–4182.

Feng Liu, Tao Xiang, Timothy M Hospedales, Wankou
Yang, and Changyin Sun. 2017b. Semantic regular-
isation for recurrent image annotation. CVPR.



2597

Yanpei Liu, Xinyun Chen, Chang Liu, and Dawn Song.
2017c. Delving into transferable adversarial exam-
ples and black-box attacks. ICLR; arXiv preprint
arXiv:1611.02770.

Jiasen Lu, Jianwei Yang, Dhruv Batra, and Devi
Parikh. 2016. Hierarchical question-image co-
attention for visual question answering. In Advances
In Neural Information Processing Systems (NIPS),
pages 289–297.

Elman Mansimov, Emilio Parisotto, Jimmy Lei Ba, and
Ruslan Salakhutdinov. 2016. Generating images
from captions with attention. ICLR; arXiv preprint
arXiv:1511.02793.

Junhua Mao, Wei Xu, Yi Yang, Jiang Wang, and
Alan L. Yuille. 2015. Deep captioning with mul-
timodal recurrent neural networks (m-rnn). ICLR;
arXiv preprint arXiv:1412.6632.

Seyed-Mohsen Moosavi-Dezfooli, Alhussein Fawzi,
Omar Fawzi, and Pascal Frossard. 2017. Universal
adversarial perturbations. In CVPR.

Nasrin Mostafazadeh, Chris Brockett, Bill Dolan,
Michel Galley, Jianfeng Gao, Georgios Sp-
ithourakis, and Lucy Vanderwende. 2017. Image-
grounded conversations: Multimodal context for
natural question and response generation. In Pro-
ceedings of the Eighth International Joint Confer-
ence on Natural Language Processing (Volume 1:
Long Papers), volume 1, pages 462–472.

Nasrin Mostafazadeh, Ishan Misra, Jacob Devlin, Mar-
garet Mitchell, Xiaodong He, and Lucy Vander-
wende. 2016. Generating natural questions about
an image. In Proceedings of the 54th Annual Meet-
ing of the Association for Computational Linguistics
(Volume 1: Long Papers), volume 1, pages 1802–
1813.

Nicolas Papernot, Patrick McDaniel, and Ian Good-
fellow. 2016a. Transferability in machine learning:
from phenomena to black-box attacks using adver-
sarial samples. arXiv preprint arXiv:1605.07277.

Nicolas Papernot, Patrick McDaniel, Xi Wu, Somesh
Jha, and Ananthram Swami. 2016b. Distillation as
a defense to adversarial perturbations against deep
neural networks. In Security and Privacy (SP), 2016
IEEE Symposium on, pages 582–597. IEEE.

Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic eval-
uation of machine translation. In annual meeting
on association for computational linguistics (ACL),
pages 311–318.

Ramakanth Pasunuru and Mohit Bansal. 2017. Multi-
task video captioning with video and entailment
generation. In Annual Meeting of the Association
for Computational Linguistics (ACL), pages 1273–
1283.

Scott Reed, Zeynep Akata, Xinchen Yan, Lajanu-
gen Logeswaran, Bernt Schiele, and Honglak Lee.
2016. Generative adversarial text to image synthe-
sis. In International Conference on Machine Learn-
ing, pages 1060–1069.

Ravi Shekhar, Sandro Pezzelle, Yauhen Klimovich,
Aurelie Herbelot, Moin Nabi, Enver Sangineto, Raf-
faella Bernardi, et al. 2017. Foil it! Find one mis-
match between image and language caption. In An-
nual Meeting of the Association for Computational
Linguistics (ACL).

Christian Szegedy, Wojciech Zaremba, Ilya Sutskever,
Joan Bruna, Dumitru Erhan, Ian Goodfellow, and
Rob Fergus. 2014. Intriguing properties of neural
networks. ICLR;arXiv preprint arXiv:1312.6199.

Kenneth Tran, Xiaodong He, Lei Zhang, and Jian Sun.
2016. Rich image captioning in the wild. In Com-
puter Vision and Pattern Recognition Workshops
(CVPRW), 2016 IEEE Conference on, pages 434–
441. IEEE.

Subhashini Venugopalan, Huijuan Xu, Jeff Donahue,
Marcus Rohrbach, Raymond J. Mooney, and Kate
Saenko. 2015. Translating videos to natural lan-
guage using deep recurrent neural networks. In
NAACL-HLT, pages 1494–1504.

Oriol Vinyals, Alexander Toshev, Samy Bengio, and
Dumitru Erhan. 2015. Show and tell: A neural im-
age caption generator. In CVPR, pages 3156–3164.

Qi Wu, Chunhua Shen, Lingqiao Liu, Anthony Dick,
and Anton van den Hengel. 2016. What value do ex-
plicit high level concepts have in vision to language
problems? In CVPR, pages 203–212.

Kelvin Xu, Jimmy Ba, Ryan Kiros, Kyunghyun
Cho, Aaron C. Courville, Ruslan Salakhutdinov,
Richard S. Zemel, and Yoshua Bengio. 2015. Show,
attend and tell: Neural image caption generation
with visual attention. In ICML, pages 2048–2057.

Zhilin Yang, Ye Yuan, Yuexin Wu, William W. Cohen,
and Ruslan Salakhutdinov. 2016. Review networks
for caption generation. In NIPS, pages 2361–2369.

Quanzeng You, Hailin Jin, Zhaowen Wang, Chen Fang,
and Jiebo Luo. 2016. Image captioning with seman-
tic attention. In CVPR, pages 4651–4659.

Linchao Zhu, Zhongwen Xu, Yi Yang, and Alexan-
der G Hauptmann. 2017. Uncovering the temporal
context for video question answering. International
Journal of Computer Vision, 124(3):409–421.


