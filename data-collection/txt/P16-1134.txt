



















































Segment-Level Sequence Modeling using Gated Recursive Semi-Markov Conditional Random Fields


Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 1413–1423,
Berlin, Germany, August 7-12, 2016. c©2016 Association for Computational Linguistics

Segment-Level Sequence Modeling using Gated Recursive
Semi-Markov Conditional Random Fields

Jingwei Zhuo1,2 ∗, Yong Cao2, Jun Zhu1 †, Bo Zhang1, Zaiqing Nie2
1Dept. of Comp. Sci. & Tech., State Key Lab of Intell. Tech. & Sys., TNList Lab,

Tsinghua University, Beijing, 100084, China
2Microsoft Research, Beijing, 100084, China

{zjw15@mails, dcszj@mail, dcszb@mail}.tsinghua.edu.cn; {yongc, znie}@microsoft.com

Abstract

Most of the sequence tagging tasks in nat-
ural language processing require to recog-
nize segments with certain syntactic role
or semantic meaning in a sentence. They
are usually tackled with Conditional Ran-
dom Fields (CRFs), which do indirect
word-level modeling over word-level fea-
tures and thus cannot make full use of
segment-level information. Semi-Markov
Conditional Random Fields (Semi-CRFs)
model segments directly but extracting
segment-level features for Semi-CRFs is
still a very challenging problem. This pa-
per presents Gated Recursive Semi-CRFs
(grSemi-CRFs), which model segments
directly and automatically learn segment-
level features through a gated recursive
convolutional neural network. Our exper-
iments on text chunking and named en-
tity recognition (NER) demonstrate that
grSemi-CRFs generally outperform other
neural models.

1 Introduction

Most of the sequence tagging tasks in natural lan-
guage processing (NLP) are segment-level tasks,
such as text chunking and named entity recog-
nition (NER), which require to recognize seg-
ments (i.e., a set of continuous words) with cer-
tain syntactic role or semantic meaning in a sen-
tence. These tasks are usually tackled with Con-
ditional Random Fields (CRFs) (Lafferty et al.,
2001), which do word-level modeling as putting
each word a tag, by using some predefined tag-
ging schemes, e.g., the “IOB” scheme (Ramshaw

∗This work was done when J.W.Z was on an internship
with Microsoft Research.

† J.Z is the corresponding author.

and Marcus, 1995). Such tagging schemes are
lossy transformations of original segment tags:
They do indicate the boundary of adjacent seg-
ments but lose the length information of segments
to some extent. Besides, CRFs can only employ
word-level features, which are either hand-crafted
or extracted with deep neural networks, such as
window-based neural networks (Collobert et al.,
2011) and bidirectional Long Short-Term Mem-
ory networks (BI-LSTMs) (Huang et al., 2015).
Therefore, CRFs cannot make full use of segment-
level information, such as inner properties of seg-
ments, which cannot be fully encoded in word-
level features.

Semi-Markov Conditional Random Fields
(Semi-CRFs) (Sarawagi and Cohen, 2004) are
proposed to model segments directly and thus
readily utilize segment-level features that encode
useful segment information. Existing work has
shown that Semi-CRFs outperform CRFs on
segment-level tagging tasks such as sequence
segmentation (Andrew, 2006), NER (Sarawagi
and Cohen, 2004; Okanohara et al., 2006), web
data extraction (Zhu et al., 2007) and opinion
extraction (Yang and Cardie, 2012). However,
Semi-CRFs need many more features compared
to CRFs as they need to model segments with
different lengths. As manually designing the
features is tedious and often incomplete, how to
automatically extract good features becomes a
very important problem for Semi-CRFs. A naive
solution that builds multiple feature extractors,
each of which extracts features for segments with
a specific length, is apparently time-consuming.
Moreover, some of these separate extractors may
underfit as the segments with specific length may
be very rare in the training data. By far, Semi-
CRFs are lacking of an automatic segment-level
feature extractor.

In this paper, we fill the research void by

1413



proposing Gated Recursive Semi-Markov Condi-
tional Random Fields (grSemi-CRFs), which can
automatically learn features for segment-level se-
quence tagging tasks. Unlike previous approaches
which usually use a neural-based feature extrac-
tor with a CRF layer, a grSemi-CRF consists of a
gated recursive convolutional neural network (gr-
Conv) (Cho et al., 2014) with a Semi-CRF layer.
The grConv is a variant of recursive neural net-
works. It builds a pyramid-like structure to ex-
tract segment-level features in a hierarchical way.
This feature hierarchy well matches the intuition
that long segments are combinations of their short
sub-segments. This idea was first explored in Cho
et al. (2014) to build an encoder in neural machine
translation and then extended to solve other prob-
lems, such as sentence-level classification (Zhao et
al., 2015) and Chinese word segmentation (Chen
et al., 2015).

The advantages of grSemi-CRFs are two folds.
First, thanks to the pyramid architecture of gr-
Convs, grSemi-CRFs can extract all the segment-
level features using one single feature extractor,
and there is no underfitting problem as all param-
eters of the feature extractor are shared globally.
Besides, unlike recurrent neural network (RNN)
models, the training and inference of grSemi-
CRFs are very fast as there is no time dependency
and all the computations can be done in parallel.
Second, thanks to the semi-Markov structure of
Semi-CRFs, grSemi-CRFs can model segments in
sentences directly without the need to introduce
extra tagging schemes, which solves the problem
that segment length information cannot be fully
encoded in tags. Besides, grSemi-CRFs can also
utilize segment-level features which can flexibly
encode segment-level information such as inner
properties of segments, compared to word-level
features as used in CRFs. By combining grConvs
with Semi-CRFs, we propose a new way to auto-
matically extract segment-level features for Semi-
CRFs.

Our major contributions can be summarized as:

(1) We propose grSemi-CRFs, which solve both
the automatic feature extraction problem for
Semi-CRFs and the indirect word-level mod-
eling problem in CRFs. As a result, grSemi-
CRFs can do segment-level modeling directly
and make full use of segment-level features;

(2) We evaluate grSemi-CRFs on two segment-
level sequence tagging tasks, text chunking

and NER. Experimental results show the ef-
fectiveness of our model.

2 Preliminary

In sequence tagging tasks, given a word sequence,
the goal is to assign each word (e.g., in POS Tag-
ging) or each segment (e.g., in text chunking and
NER) a tag. By leveraging a tagging scheme like
“IOB”, all the tasks can be regarded as word-
level tagging. More formally, let X denote the
set of words and Y denote the set of tags. A
word sentence with length T can be denoted by
x = (x1, ..., xT ) and its corresponding tags can be
denoted as y = (y1, ..., yT ). A CRF (Lafferty et
al., 2001) defines a conditional distribution

p(y|x) = 1
Z(x)

exp

(
T∑

t=1

F (yt,x) +A(yt−1, yt)

)
, (1)

where F (yt,x) is the tag score (or potential) for
tag yt at position t, A(yt−1, yt) is the transi-
tion score between yt−1 and yt to measure the
tag dependencies of adjacent words and Z(x) =∑

y′ exp
(∑T

t=1 F (y
′
t,x) +A(yt−1, y′t)

)
is the

normalization factor. For the common log-linear
models, F (yt,x) can be computed by

F (yt,x) = v
T
yt f(yt,x) + byt , (2)

where V = (v1, ...,v|Y|)T ∈ R|Y|×D, bV =
(b1, ..., b|Y|)T ∈ R|Y|, f(yt,x) ∈ RD are the
word-level features for yt over the sentence x and
D is the number of features. f(yt,x) can be
manually designed or automatically extracted us-
ing neural networks, such as window-based neural
networks (Collobert et al., 2011).

If we consider segment-level tagging directly1,
we get a segmentation of the previous tag sen-
tence. With a little abuse of notation, we denote a
segmentation by s = (s1, ..., s|s|) in which the jth
segment sj = 〈hj , dj , yj〉 consists of a start posi-
tion hj , a length dj < L where L is a predefined
upperbound and a tag yj . Conceptually, sj means
a tag yj is given to words (xhj , ..., xhj+dj−1). A
Semi-CRF (Sarawagi and Cohen, 2004) defines a
conditional distribution

p(s|x) = 1
Z(x)

exp

 |s|∑
j=1

F (sj ,x) +A(yj−1, yj)

, (3)
1Word-level tagging can be regarded as segment-level tag-

ging over length-1 segments.

1414



Figure 1: An overview of grSemi-CRFs. For simplicity, we set the segment length upperbound L = 4,
and the sentence length T = 6. The left side is the feature extractor, in which each node denotes a
vector of segment-level features (e.g., z(d)k for the kth node in the dth layer). Embeddings of word-level
input features are used as length-1 segment-level features, and the length-d feature is extracted from two
adjacent length-(d − 1) features. The right side is the Semi-CRF. Tag score vectors are computed as
linear transformations of segment-level features and the number of them equals the number of nodes in
the same layer. For clarity, we use triangle, square, pentagon and hexagon to denote the tag score vectors
for length-1, 2, 3, 4 segments and directed links to denote the tag transformations of adjacent segments.

whereF (sj ,x) is the potential or tag score for seg-
ment sj ,A(yj−1, yj) is the transition score to mea-
sure tag dependencies of adjacent segments and
Z(x) =

∑
s′ exp

(∑|s′|
j=1 F (s

′
j ,x) +Ay′j−1,y′j

)
is

the normalization factor. For the common log-
linear models, F (sj ,x) can be computed by

F (sj ,x) = v
T
yj f(sj ,x) + byj , (4)

where V = (v1, ...,v|Y|)T ∈ R|Y|×D, bV =
(b1, ..., b|Y|)T ∈ R|Y| and f(sj ,x) ∈ RD are the
segment-level features for sj over the sentence x.

As Eq. (1) and Eq. (3) show, CRFs can be re-
garded as a special case of Semi-CRFs when L =
1. CRFs need features for only length-1 segments
(i.e., words), while Semi-CRFs need features for
length-` segments (1 ≤ ` ≤ L). Therefore, to
model the same sentence, Semi-CRFs generally
need many more features than CRFs, especially
when L is large. Besides, unlike word-level fea-
tures used in CRFs, the sources of segment-level
features are often quite limited. In existing work,
the sources of f(sj ,x) can be roughly divided
into two parts: (1) Concatenations of word-level
features (Sarawagi and Cohen, 2004; Okanohara
et al., 2006); and (2) Hand-crafted segment-level
features, including task-insensitive features, like
the length of segments, and task-specific features,

like the verb phrase patterns in opinion extraction
(Yang and Cardie, 2012). As manually design-
ing features is time-consuming and often hard to
capture rich statistics underlying the data, how to
automatically extract features for Semi-CRFs re-
mains a challenge.

3 Gated Recursive Semi-Markov CRFs

In this section, we present Gated Recursive Semi-
CRFs (grSemi-CRFs), which inherit the advan-
tages of Semi-CRFs in segment-level modeling,
and also solve the feature extraction problem of
Semi-CRFs by introducing a gated recursive con-
volutional neural network (grConv) as the fea-
ture extractor. Instead of building multiple feature
extractors at different scales of segment lengths,
grSemi-CRFs can extract features with any length
by using a single grConv, and learn the parameters
effectively via sharing statistics.

3.1 Architecture

The architecture of grSemi-CRFs is illustrated in
Figure 1. A grSemi-CRF can be divided into two
parts, a feature extractor (i.e., grConv) and a Semi-
CRF. Below, we explain each part in turn.

As is shown, the feature extractor is a pyramid-
like directed acyclic graph (DAG), in which nodes

1415



Figure 2: The the building block of the feature
extractor (i.e., grConv), in which parameters are
shared among the pyramid structure. We omit the
dependency of θL,θR,θM on GL,GR.

are stacked layer by layer and information is prop-
agated from adjacent nodes in the same layer to
their co-descendants in the higher layer through
directed links. Recall that L denotes the upper-
bound of segment length (i.e., the height of the
grConv), we regard the bottom level as the 1st
level and the top level as the Lth level. Then,
for a length-T sentence, the dth level will have
T − d+ 1 nodes, which correspond to features for
T − d+ 1 length-d segments. The kth node in the
dth layer corresponds to the segment-level latent
features z(d)k ∈ RD, which denote the meaning of
the segment, e.g., the syntactic role (i.e., for text
chunking) or semantic meaning (i.e., for NER).

Like CRFs, grSemi-CRFs allow word-level cat-
egorical inputs (i.e., xk) which are transformed
into continuous vectors (i.e., embeddings) accord-
ing to look-up tables and then used as length-1
segment-level features (i.e., z(1)k ). To be clear,
we call these inputs as input features and those
extracted segment-level features (i.e., z(d)k ) as
segment-level latent features. Besides, grSemi-
CRFs also allow segment-level input features
(e.g., gazetteers) directly as shown in Eq. (12).
We will discuss more details in section 4.3.

The building block of the feature extractor is
shown in Figure 2, where an intermediate node
ẑ(d) ∈ RD is introduced to represent the inter-
actions of two length-(d − 1) segments. To cap-
ture such complex interactions, ẑ(d)k is computed
through a non-linear transformation, i.e.,

ẑ
(d)
k = g(α

(d)
k ) = g(WLz

(d−1)
k + WRz

(d−1)
k+1 + bW), (5)

where WL,WR ∈ RD×D and bW ∈ RD are
shared globally, and g(·) is a non-linear activation
function2.

2We use a modified version of the sigmoid function, i.e.,
g(x) = 4

(
1

1+e−x − 12
)

.

Then, the length-d segment-level latent features
z(d)k can be computed as

z
(d)
k = θLz

(d−1)
k + θRz

(d−1)
k+1 + θM ẑ

(d)
k , (6)

where θL, θM and θR ∈ R are the gating coeffi-
cients which satisfy the condition θL, θR, θM ≥ 0
and θL + θR + θM = 1. Here, we make a lit-
tle modification of grConvs by making the gating
coefficients as vectors instead of scalars, i.e.,

z
(d)
k = θL ◦ z(d−1)k + θR ◦ z(d−1)k+1 + θM ◦ ẑ(d)k , (7)

where ◦ denotes the element-wise product and
θL, θR and θM ∈ RD are vectorial gat-
ing coefficients3 which satisfy the condition that
θL,i, θR,i, θM,i ≥ 0 and θL,i + θR,i + θM,i = 1 for
1 ≤ i ≤ D. There are two reasons for this modifi-
cation: (1) Theoretically, the element-wise combi-
nation makes a detailed modeling as each feature
in z(d)k may have its own combining; and (2) Ex-
perimentally, this setting makes our grSemi-CRF4

more flexible, which increases its generalizability
and leads to better performance in experiments as
shown in Table 4.

We can regard Eq. (7) as a soft gate function
to control the propagation flows. Besides, all the
parameters (i.e., WL,WR,bW,GL,GR,bG) are
shared globally and recursively applied to the in-
put sentence in a bottom-up manner. All of these
account for the name gated recursive convolu-
tional neural networks (grConvs).

Eq. (5) and Eq. (7) build the information prop-
agation criteria in a grConv. The basic assumption
behind Eq. (5) and Eq. (7) is that the meaning of
one segment can be represented as a linear com-
bination of three parts: (1) the meaning of its pre-
fix segment, (2) the meaning of its suffix segment
and (3) the joint meaning of both (i.e., the com-
plex interaction). This process matches our intu-
ition about the hierarchical structure in the com-
position of a sentence. For example, the meaning
of the United States depends on the suffix segment
United States, whose meaning is not only from its
prefix United or suffix States, but the interaction
of both.

The vectorial gating coefficients θL, θR and θM

3We omit the dependency of θL,θR and θM on d and k
for notation simplicity.

4Unless otherwise stated, we regard “grSemi-CRF” as
grSemi-CRF with vectorial gating coefficients in default.

1416



are computed adaptively, i.e.,θLθR
θM

 =
 1/Z1/Z

1/Z

◦exp(GLz(d−1)k + GRz(d−1)k+1 + bG) ,
(8)

where GL,GR ∈ R3D×D and bG ∈ R3D are
shared globally. Z ∈ Rd is normalization coef-
ficients and the ith element of Z is computed via

Zi =

3∑
j=1

[
exp

(
GLz

(d−1)
k + GRz

(d−1)
k+1 + bG

)]
D×(j−1)+i

.

(9)

After the forward propagation of the feature ex-
tractor is over, the tag scores (i.e., the potential
functions for Semi-CRFs) are computed through
a linear transformation. For segment sj =
〈hj , dj , yj〉, its latent feature is f(sj ,x) = z(dj)hj
and corresponding potential/tag score is

F (sj ;x) = f(〈hj , dj , yj〉;x) =
[
V

(dj)

0 z
(dj)

hj
+ b

(dj)

V

]
yj

,

(10)

where V(dj)0 ∈ R|Y|×D and b(dj) ∈ R|Y| are pa-
rameters for length-dj segments. To encode con-
textual information, we can assume that the tag of
a segment depends not only on itself but also its
neighbouring segments with the same length, i.e.,

F (sj ;x) =

[
H∑

i=−H
V

(dj)

i z
(dj)

hj+i
+ b

(dj)

V

]
yj

, (11)

where V(dj)−H , ...,V
(dj)
0 , ...,V

(dj)
H ∈ R|Y|×D and H

is the window width for neighbouring segments.
Apart from the automatically extracted segment-
level latent features z(d)k , grSemi-CRFs also allow
segment-level input features (e.g., gazetteers), i.e.,

F (sj ;x) =

[
H∑

i=−H
V

(dj)

i z
(dj)

hj+i
+ b

(dj)

V + U
(dj)c

(dj)

hj

]
yj

,

(12)

where U(dj) ∈ R|Y|×D′ and c(dj)hj ∈ RD
′

is a vec-
tor of segment-level input features.

Then, we can use Eq. (3) for inference by
using a Semi-CRF version of Viterbi algorithms
(Sarawagi and Cohen, 2004).

3.2 Learning of Parameters
To learn grSemi-CRFs, we maximize the log like-
lihood L = log p(s|x) over all the parameters.
Here, for notation simplity, we consider the sim-
pliest case, i.e., using Eq. (10) to compute tag
scores. More details can be found in the supple-
mentary note.

Gradients of Semi-CRF-based parameters (i.e.,
A and V0) and tag scores F (sj ,x) can be com-
puted based on the marginal probability of neigh-
bouring segments via a Semi-CRF version of
forward-backward algorithms (Sarawagi and Co-
hen, 2004). As for the grConv-based parameters,
we can compute their gradients by back propaga-
tion. For example, gradients for WL and GL are5

∂L
∂WL

=

L∑
d=1

T−d+1∑
k=1

∂L
∂z

(d)
k

∂z
(d)
k

∂WL
,
∂L
∂GL

=

L∑
d=1

T−d+1∑
k=1

∂L
∂z

(d)
k

∂z
(d)
k

∂GL
,

(13)

where ∂z
(d)
k

∂WL
and ∂z

(d)
k

∂GL
can be derived from Eq. (5),

Eq. (7) and Eq. (8). For ∂L
∂z

(d)
k

, thanks to the recur-

sive structure, it can be computed as

∂L
∂z

(d)
k

=
∂z

(d+1)
k

∂z
(d)
k

∂L
∂z

(d+1)
k

+
∂z

(d+1)
k−1
∂z

(d)
k

∂L
∂z

(d+1)
k−1

+ V
(d)
0

T ∂L
∂F (s

(d)
k ,x)

,

(14)

where s(d)k = 〈k, d,Y〉 is a length-|Y| vector
which denotes segments with all possible tags for
z(d)k ,

∂L
∂F (s

(d)
k ,x)

is the gradient for F (s(d)k ,x) and

∂z
(d+1)
k

∂z
(d)
k

= diag(θL) + diag(θM ◦ g′(α(d+1)k ))WL, (15)

where diag(θL) denotes the diagonal matrix

spanned by vector θL, and
∂z

(d+1)
k−1

∂z
(d)
k

has a simi-

lar form. As Eq. (14) shows, for each node in
the feature extractor of grSemi-CRFs, its gradi-
ent consists of two parts: (1) the gradients back
propagated from high layer nodes (i.e., longer seg-
ments); and (2) the supervising signals from Semi-
CRFs. In other words, the supervision in the ob-
jective function is added to each node in grSemi-
CRFs. This is a nice property compared to other
neural-based feature extractors used in CRFs, in
which only the nodes of several layers on the top
receive supervision. Besides, the term diag(θL)

in Eq. (15) prevents ∂z
(d+1)
k

∂z
(d)
k

from being too small

when g′(α(d)k ) and WL are small, which acts as
the linear unit recurrent connection in the mem-
ory block of LSTM (Hochreiter and Schmidhu-
ber, 1997; Zhao et al., 2015). All of these help
in avoiding gradient vanishing problems in train-
ing grSemi-CRFs.

5Gradients for WR,bW,GR,bG can be computed in
similar ways.

1417



4 Experiments

We evaluate grSemi-CRFs on two segment-level
sequence tagging NLP tasks: text chunking and
named entity recognition (NER).

4.1 Datasets

For text chunking, we use the CONLL 2000 text
chunking shared dataset6 (Tjong Kim Sang and
Buchholz, 2000), in which the objective is to di-
vide the whole sentence into different segments
according to their syntactic roles, such as noun
phrases (“NP”), verb phrases (“VP”) and adjec-
tive phrases (“ADJP”). We call it a “segment-rich”
tasks as the number of phrases are much higher
than that of non-phrases which is tagged with oth-
ers (“O”). We evaluate performance over all the
chunks instead of only noun pharse (NP) chunks.

For NER, we use the CONLL 2003 named en-
tity recognition shared dataset7 (Tjong Kim Sang
and De Meulder, 2003), in which segments
are tagged with one of four entity types: per-
son (“PER”), location (“LOC”), organization
(“ORG”) and miscellaneous(“MISC”), or others
(“O”) which is used to denote non-entities. We
call it a “segment-sparse” task as entities are rare
while non-entities are common.

4.2 Input Features

For each word, we use multiple input features,
including the word itself, its length-3 prefix and
length-4 suffix, its capitalization pattern, its POS
tag, the length-4,8,12,20 prefixs of its Brown clus-
ters (Brown et al., 1992) and gazetteers8. All of
them are used as word-level input features except
gazetteers, which are used as segment-level fea-
tures directly. All the embeddings for word-level
inputs are randomly initialized except word em-
beddings, which can be initialized randomly or
by pretraining over unlabeled data, which is exter-
nal information compared to the dataset. Besides
word embeddings, Brown clusters and gazetteers
are also based on external information, as summa-
rized below:

• Word embeddings. We use Senna embed-
dings9 (Collobert et al., 2011), which are 50-
dimensional and have been commonly used

6Available at: http://www.cnts.ua.ac.be/conll2000/chunking/
7Available at: http://www.cnts.ua.ac.be/conll2003/ner/
8Among them, POS tags are provided in the dataset.
9Available at http://ronan.collobert.com/senna/

in sequence tagging tasks (Collobert et al.,
2011; Turian et al., 2010; Huang et al., 2015);

• Brown clusters. We train two types of
Brown clusters using the implementation
from Liang (2005): (1) We follow the se-
tups of Ratinov and Roth (2009), Turian et
al. (2010) and Collobert et al. (2011) to gen-
erate 1000 Brown clusters on Reuters RCV1
dataset (Lewis et al., 2004); (2) We gener-
ate 1000 Brown clusters on New York Times
(NYT) corpus (Sandhaus, 2008);

• Gazetteers. We build our gazetteers based
on the gazetteers used in Senna (Collobert et
al., 2011) and Wikipedia entries, mainly the
locations and organizations. We also denoise
our gazetteers by removing overlapped enti-
ties and using BBN Pronoun Coreference and
Entity Type Corpus (Weischedel and Brun-
stein, 2005) as filters10.

4.3 Implementation Details
To learn grSemi-CRFs, we employ Adagrad
(Duchi et al., 2011), an adaptive stochastic gra-
dient descent method which has been proved suc-
cessful in similar tasks (Chen et al., 2015; Zhao et
al., 2015). To avoid overfitting, we use the dropout
strategy (Srivastava et al., 2014) and apply it on the
first layer (i.e., z(0)k ). We also use the strategy of
ensemble classifiers, which is proved an effective
way to improve generalization performance (Col-
lobert et al., 2011). All results are obtained by de-
coding over an average Semi-CRF after 10 train-
ing runs with randomly initialized parameters.

For the CONLL 2003 dataset, we use the F1
scores on the development set to help choose
the best-performed model in each run. For the
CONLL 2000 dataset, as there is no development
set provided, we use cross validation as Turian et
al. (2010) to choose hyperparameters. After that,
we retrain model according to the hyperparame-
ters and choose the final model in each run.

Our hyperparameter settings for these two tasks
are shown in Table 1. The segment length is set ac-
cording to the maximum segment length in train-
ing set. We set the minibatch size to 10, which
means that we process 10 sentences in a batch.
The window width defines the parameter H in Eq.
(12) when producing tag score vectors.

10We apply gazetteers on BBN corpus, collect lists of false
positive entities and clean our gazetteers according to these
lists.

1418



Hyperparameters CONLL 2000 CONLL 2003
Segment length 15 10
Dropout 0.3 0.3
Learning rate 0.3 0.3
Epochs 15 20
Minibatches 10 10
Window width 2 2

Table 1: Hyperparameter settings for our model.

4.4 Results and Analysis

Table 2 shows the results of our grSemi-CRFs
and other models11. We divide other models into
two categories, i.e., neural models and non-neural
models, according to whether neural networks are
used as automatic feature extractors. For neural
models, Senna (Collobert et al., 2011) consists of
a window-based neural network for feature extrac-
tion and a CRF for word-level modeling while BI-
LSTM-CRF (Huang et al., 2015) uses a bidirec-
tional Long Short-Term Memory network for fea-
ture extraction and a CRF for word-level model-
ing.

For non-neural models, JESS-CM (Suzuki and
Isozaki, 2008) is a semi-supervised model which
combines Hidden Markov Models (HMMs) with
CRFs and uses 1 billion unlabelled words in train-
ing. Lin and Wu (2009) cluster 20 million phrases
over corpus with around 700 billion tokens, and
use the resulting clusters as features in CRFs. Pas-
sos et al. (2014) propose a novel word embed-
ding method which incorporates gazetteers as su-
pervising signals in pretraining and builds a log-
linear CRF over them. Ratinov and Roth (2009)
use CRFs based on many non-local features and
30 gazetteers extracted from Wikipedia and other
websites with more than 1.5 million entities.

As Table 2 shows, grSemi-CRFs outperform
other neural models, in both text chunking and
named entity recognition (NER) tasks. BI-LSTM-
CRFs use many more input features than ours,
which accounts for the phenomenon that the per-
formance of our grSemi-CRFs is rather mediocre
(i.e., 93.92% versus 94.13% and 84.66% versus
84.26%) without external information. However,
once using Senna embeddings, our grSemi-CRFs
perform much better than BI-LSTM-CRFs.

For non-neural models, one similarity of them
is that they use a lot of hand-crafted features, and
many of them are even task-specific. Unlike them,

11Because of the space limit, we only compare our model
with other models which follow similar settings and achieve
high performance.

Input Features CONLL 2000 CONLL 2003
None 93.92 84.66

Brown(NYT) 94.18 86.57
Brown(RCV1) 94.05 88.22

Emb 94.73 88.12
Gaz – 87.94

Emb + Brown(NYT) 95.01 88.86
Emb + Brown(RCV1) 94.87 89.44

Emb + Gaz – 89.88
Brown(NYT) + Gaz – 88.69

Brown(RCV1) + Gaz – 89.82
All(NYT) – 90.00

All(RCV1) – 90.87

Table 3: Results of grSemi-CRF with external
information, measured in F1 score. None = no
external information, Emb = Senna embeddings,
Brown = Brown clusters, Gaz = gazetteers and
All = Emb + Brown + Gaz. NYT and RCV1 in
the parenthesis denote the corpus used to generate
Brown clusters. “–” means no results. Notice that
gazetteers are only applied to NER.

grSemi-CRFs use much fewer input features and
most of them are task-insensitive13. However,
grSemi-CRFs achieve almost the same perfor-
mance, sometimes even better. For text chunking,
grSemi-CRF outperforms all reported supervised
models, except JESS-CM (Suzuki and Isozaki,
2008), a semi-supervised model using giga-word
scale unlabeled data in training14. However, the
performance of our grSemi-CRF (95.01%) is very
close to that of JESS-CM (95.15%). For NER, the
performance of grSemi-CRFs are also very closed
to state-of-the-art results (90.87% versus 90.90%).

4.4.1 Impact of External Information
As Table 3 shows, external information improve
the performance of grSemi-CRFs for both tasks.
Compared to text chunking, we can find out that
external information plays an extremely important
role in NER, which coincides with the general idea
that NER is a knowledge-intensive task (Ratinov
and Roth, 2009). Another interesting thing is that,
Brown clusters generated from NYT corpus per-
forms better on the CONLL 2000 task while those
generated from Reuters RCV1 dataset performs
better on the CONLL 2003 task. The reason is

13E.g.: for NER, JESS-CM uses 79 different features; Lin
and Wu (2009) use 48 baseline and phrase cluster features;
while we only use 11. Besides, grSemi-CRFs use almost the
same features for chunking and NER (except gazetteers).

14Being semi-supervised, JESS-CM can learn from inter-
actions between labelled and unlabelled data during training
but the training is slow compared to supervised models.

1419



Models CONLL 2000 CONLL 2003

Ours
grSemi-CRF (Random embeddings) 93.92 84.66
grSemi-CRF (Senna embeddings) 95.01 89.44 (90.87)

Neural Models

Senna (Random embeddings) 90.33 81.47
Senna (Senna embeddings) 94.32 88.67 (89.59)
BI-LSTM-CRF (Random) 94.13 84.26

BI-LSTM-CRF (Senna embeddings) 94.46 88.83 (90.10)

Non-Neural Models

JESS-CM (Suzuki and Isozaki, 2008), 15M 94.67 89.36
JESS-CM (Suzuki and Isozaki, 2008), 1B 95.15 89.92

Ratinov and Roth (2009)12 – 90.57
Lin and Wu (2009) – 90.90
Passos et al. (2014) – 90.90

Table 2: Experimental results over the CONLL-2000 and CONLL-2003 shared datasets, measured in F1
score. Numbers in parentheses are the F1 score when using gazetteers. JESS-CM (Suzuki and Isozaki,
2008) is a semi-supervised model, in which 15M or 1B denotes the number of unlabeled words it uses
for training.

Gating Coefficients CONLL 2000 CONLL 2003
Scalars 94.47 89.27(90.54)
Vectors 95.01 89.44(90.87)

Table 4: F1 scores of grSemi-CRF with scalar or
vectorial gating coefficients. Numbers in paren-
theses are the F1 score when using gazetteers.

that the CONLL 2000 dataset is the subset of Wall
Street Journal (WSJ) part of the Penn Treebank II
Corpus (Marcus et al., 1993) while the CONLL
2003 dataset is a subset of Reuters RCV1 dataset.
Maybe the writing styles between NYT and WSJ
are more similar than those between RCV1 and
WSJ.

4.4.2 Impact of Vectorial Gating Coefficients

As Table 4 shows, a grSemi-CRF using vectorial
gating coefficients (i.e., Eq. (7)) performs bet-
ter than that using scalar gating coefficients (i.e.,
Eq. (6)), which provides evidences for the the-
oretical intuition that vectorial gating coefficients
can make a detailed modeling of the combinations
of segment-level latent features and thus performs
better than scalar gating coefficients.

4.4.3 Visualization of Learnt Segment-Level
Features

To demonstrate the quality of learnt segment-
level features, we use an indirect way as widely
adopted in previous work, e.g., Collobert et al.
(2011). More specifically, we show 10 nearest
neighbours for some selected queried segments
according to Euclidean metric of corresponding

features15. To fully demonstrate the power of
grSemi-CRF in learning segment-level features,
we use the Emb+Brown(RCV1) model in Table
3, which uses no gazetteers. We train the model
on the CONLL 2003 training set and find nearest
neighbours in the CONLL 2003 test set. We make
no restrictions on segments, i.e., all possible seg-
ments with different lengths in the CONLL 2003
test set are candidates.

As Table 5 shown, most of the nearest segments
are meaningful and semantically related. For ex-
ample, the nearest segments for “Filippo Inzaghi”
are not only tagged with person, but also names of
famous football players as “Filippo Inzaghi”.

There also exist some imperfect results. E.g.,
for “Central African Republic”, nearest segments,
which contain the same queried segment, are se-
mantically related but not syntactically similar.
The major reason may be that the CONLL 2003
dataset is a small corpus (if compared to the
vast unlabelled data used to train Senna embed-
dings), which restricts the range for candidate seg-
ments and the quality of learnt segment-level fea-
tures. Another reason is that labels in the CONLL
2003 dataset mainly encodes semantic information
(e.g., named entities) instead of syntactic informa-
tion (e.g., chunks).

Besides, as we make no restriction on the for-
mulation of candidate segments, sometimes only
a part of the whole phrase will be retrieved, e.g.,
“FC Hansa”, which is the prefix of “FC Hansa
Rostock”. Exploring better way of utilizing unla-

15Using the cosine similarity generates similar results.
However, as Collobert et al. (2011) use Euclidean metric, we
follow their settings.

1420



Queried
Filippo Inzaghi AC Milan Central African Republic Asian Cup

Segments
Pierluigi Casiraghi FC Hansa From Central African Republic Scottish Cup
Fabrizio Ravanelli SC Freiburg Southeast Asian Nations European Cup

Bogdan Stelea FC Cologne In Central African Republic African Cup
Nearest Francesco Totti Aston Villa The Central African Republic World Cup

Neighbour Predrag Mijatovic Red Cross South African Breweries UEFA Cup
Results Fausto Pizzi Yasuto Honda Of Southeast Asian Nations Europoean Cup

Pierre Laigle NAC Breda New South Wales Asian Games
Pavel Nedved La Plagne Central African Republic . Europa Cup

Anghel Iordanescu Sporting Gijon Papua New Guinea National League
Zeljko Petrovic NEC Nijmegen Central Africa F.A. Cup

Table 5: Visualization of segment-level features learnt on the CONLL 2003 dataset. For each column
the queried segment is followed by its 10 nearest neighbors (measured by the cosine similarity of their
feature vectors). Corresponding tags for these four queried segments are (from left to right): person,
organization, location and miscellaneous.

belled data to improve learning segment-level fea-
tures is part of the future work.

5 Discussions and Related Work

Cho et al. (2014) first propose grConvs to learn
fix-length representations of the whole source sen-
tence in neural machine translation. Zhao et al.
(2015) use grConvs to learn hierarchical represen-
tations (i.e., multiple fix-length representations) of
the whole sentence for sentence-level classifica-
tion problem. Both of them focus on sentence-
level classification problems while grSemi-CRFs
are solving segment-level classification (sequence
tagging) problems, which is fine-grained. Chen et
al. (2015) propose Gated Recursive Neural Net-
works (GRNNs), a variant of grConvs, to solve
Chinese word segmentation problem. GRNNs
still do word-level modeling by using CRFs while
grSemi-CRFs do segment-level modeling directly
by using semi-CRFs and makes full use of the re-
cursive structure of grConvs.

We believe that, the recursive neural network
(e.g., grConv) is a natural feature extractor for
Semi-CRFs, as it extracts features for every possi-
ble segments by one propagation over one trained
model, which is fast-computing and efficient. In
this sense, grSemi-CRFs provide a promising di-
rection to explore.

6 Conclusions

In this paper, we propose Gated Recursive Semi-
Markov Conditional Random Fields (grSemi-
CRFs) for segment-level sequence tagging tasks.
Unlike word-level models such as CRFs, grSemi-
CRFs model segments directly without the need

of using extra tagging schemes and also readily
utilize segment-level features, both hand-crafted
and automatically extracted by a grConv. Exper-
imental evaluations demonstrate the effectiveness
of grSemi-CRFs on both text chunking and NER
tasks.

In future work, we are interested in exploring
better ways of utilizing vast unlabelled data to im-
prove grSemi-CRFs, e.g., to learn phrase embed-
dings from unlabelled data or designing a semi-
supervised version of grSemi-CRFs.

Acknowledgments

J.W.Z, J.Z and B.Z are supported by the National
Basic Research Program (973 Program) of China
(No. 2013CB329403), National NSF of China
(Nos. 61322308, 61332007), the Youngth Top-
notch Talent Support Program, Tsinghua TNList
Lab Big Data Initiative, and Tsinghua Initiative
Scientific Research Program (No. 20141080934).

References
Galen Andrew. 2006. A hybrid markov/semi-markov

conditional random field for sequence segmentation.
In Proceedings of the 2006 Conference on Empiri-
cal Methods in Natural Language Processing, pages
465–472.

Peter F Brown, Peter V Desouza, Robert L Mercer,
Vincent J Della Pietra, and Jenifer C Lai. 1992.
Class-based n-gram models of natural language.
Computational linguistics, 18(4):467–479.

Xinchi Chen, Xipeng Qiu, Chenxi Zhu, and Xuanjing
Huang. 2015. Gated recursive neural network for
chinese word segmentation. In Proceedings of An-
nual Meeting of the Association for Computational
Linguistics.

1421



Kyunghyun Cho, Bart van Merriënboer, Dzmitry Bah-
danau, and Yoshua Bengio. 2014. On the properties
of neural machine translation: Encoder–decoder ap-
proaches. Eighth Workshop on Syntax, Semantics
and Structure in Statistical Translation, page 103.

Ronan Collobert, Jason Weston, Léon Bottou, Michael
Karlen, Koray Kavukcuoglu, and Pavel Kuksa.
2011. Natural language processing (almost) from
scratch. The Journal of Machine Learning Re-
search, 12:2493–2537.

John Duchi, Elad Hazan, and Yoram Singer. 2011.
Adaptive subgradient methods for online learning
and stochastic optimization. The Journal of Ma-
chine Learning Research, 12:2121–2159.

Sepp Hochreiter and Jürgen Schmidhuber. 1997.
Long short-term memory. Neural computation,
9(8):1735–1780.

Zhiheng Huang, Wei Xu, and Kai Yu. 2015. Bidi-
rectional LSTM-CRF models for sequence tagging.
arXiv preprint arXiv:1508.01991.

John D. Lafferty, Andrew McCallum, and Fernando
C. N. Pereira. 2001. Conditional random fields:
Probabilistic models for segmenting and labeling se-
quence data. In Proceedings of the Eighteenth In-
ternational Conference on Machine Learning, pages
282–289.

David D Lewis, Yiming Yang, Tony G Rose, and Fan
Li. 2004. Rcv1: A new benchmark collection for
text categorization research. The Journal of Ma-
chine Learning Research, 5:361–397.

Percy Liang. 2005. Semi-Supervised Learning for Nat-
ural Language. Ph.D. thesis, Massachusetts Insti-
tute of Technology.

Dekang Lin and Xiaoyun Wu. 2009. Phrase clustering
for discriminative learning. In Proceedings of An-
nual Meeting of the Association for Computational
Linguistics, pages 1030–1038.

Mitchell P Marcus, Mary Ann Marcinkiewicz, and
Beatrice Santorini. 1993. Building a large anno-
tated corpus of english: The penn treebank. Compu-
tational linguistics, 19(2):313–330.

Daisuke Okanohara, Yusuke Miyao, Yoshimasa Tsu-
ruoka, and Jun’ichi Tsujii. 2006. Improving
the scalability of semi-markov conditional random
fields for named entity recognition. In Proceedings
of Annual Meeting of the Association for Computa-
tional Linguistics, pages 465–472.

Alexandre Passos, Vineet Kumar, and Andrew McCal-
lum. 2014. Lexicon infused phrase embeddings for
named entity resolution. Proceedings of the Eigh-
teenth Conference on Computational Natural Lan-
guage Learning, pages 78–86.

Lance A. Ramshaw and Mitchell P. Marcus. 1995.
Text chunking using transformation-based learning.
In Proceedings of the Third Workshop on Very Large
Corpora.

Lev Ratinov and Dan Roth. 2009. Design challenges
and misconceptions in named entity recognition. In
Proceedings of the Thirteenth Conference on Com-
putational Natural Language Learning, pages 147–
155.

Evan Sandhaus. 2008. The new york times annotated
corpus. Linguistic Data Consortium, Philadelphia,
6.

Sunita Sarawagi and William W Cohen. 2004. Semi-
markov conditional random fields for information
extraction. In Advances in Neural Information Pro-
cessing Systems, pages 1185–1192.

Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky,
Ilya Sutskever, and Ruslan Salakhutdinov. 2014.
Dropout: A simple way to prevent neural networks
from overfitting. The Journal of Machine Learning
Research, 15(1):1929–1958.

Jun Suzuki and Hideki Isozaki. 2008. Semi-supervised
sequential labeling and segmentation using giga-
word scale unlabeled data. In Proceedings of An-
nual Meeting of the Association for Computational
Linguistics, pages 665–673.

Erik F. Tjong Kim Sang and Sabine Buchholz. 2000.
Introduction to the conll-2000 shared task: Chunk-
ing. In Proceedings of the Forth Conference on
Computational Natural Language Learning, pages
127–132.

Erik F. Tjong Kim Sang and Fien De Meulder.
2003. Introduction to the conll-2003 shared task:
Language-independent named entity recognition. In
Proceedings of the Seventh Conference on Computa-
tional Natural Language Learning, pages 142–147.

Joseph Turian, Lev Ratinov, and Yoshua Bengio. 2010.
Word representations: a simple and general method
for semi-supervised learning. In Proceedings of An-
nual Meeting of the Association for Computational
Linguistics, pages 384–394.

Ralph Weischedel and Ada Brunstein. 2005. Bbn pro-
noun coreference and entity type corpus. Linguistic
Data Consortium, Philadelphia, 112.

Bishan Yang and Claire Cardie. 2012. Extracting opin-
ion expressions with semi-markov conditional ran-
dom fields. In Proceedings of the 2012 Joint Con-
ference on Empirical Methods in Natural Language
Processing and Computational Natural Language
Learning, pages 1335–1345. Association for Com-
putational Linguistics.

Han Zhao, Zhengdong Lu, and Pascal Poupart. 2015.
Self-adaptive hierarchical sentence model. In Pro-
ceedings of the 24th International Joint Conference
on Artificial Intelligence, pages 4069–4076.

1422



Jun Zhu, Zaiqing Nie, Ji-Rong Wen, Bo Zhang, and
Hsiao-Wuen Hon. 2007. Webpage understanding:
an integrated approach. In Proceedings of SIGKDD
Conference on Knowledge Discovery and Data Min-
ing.

1423


