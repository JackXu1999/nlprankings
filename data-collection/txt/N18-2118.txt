



















































Slot-Gated Modeling for Joint Slot Filling and Intent Prediction


Proceedings of NAACL-HLT 2018, pages 753–757
New Orleans, Louisiana, June 1 - 6, 2018. c©2018 Association for Computational Linguistics

Slot-Gated Modeling for Joint Slot Filling and Intent Prediction

Chih-Wen Goo† Guang Gao† Yun-Kai Hsu? Chih-Li Huo?
Tsung-Chieh Chen? Keng-Wei Hsu? Yun-Nung Chen†

†National Taiwan University
?Institute for Information Industry

r05944049@ntu.edu.tw y.v.chen@ieee.org

Abstract

Attention-based recurrent neural network
models for joint intent detection and slot
filling have achieved the state-of-the-art
performance, while they have independent
attention weights. Considering that slot and
intent have the strong relationship, this paper
proposes a slot gate that focuses on learning
the relationship between intent and slot atten-
tion vectors in order to obtain better semantic
frame results by the global optimization. The
experiments show that our proposed model
significantly improves sentence-level semantic
frame accuracy with 4.2% and 1.9% relative
improvement compared to the attentional
model on benchmark ATIS and Snips datasets
respectively1.

1 Introduction

Spoken language understanding (SLU) is a criti-
cal component in spoken dialogue systems. SLU
is aiming to form a semantic frame that captures
the semantics of user utterances or queries. It typ-
ically involves two tasks: intent detection and slot
filling (Tur and De Mori, 2011). These two tasks
focus on predicting speakers intent and extract-
ing semantic concepts as constraints for the nat-
ural language. Take a movie-related utterance as
an example, ”find comedies by James Cameron”,
as shown in Figure 1. There are different slot la-
bels for each word in the utterance, and a specific
intent for the whole utterance.

Slot filling can be treated as a sequence label-
ing task that maps an input word sequence x =
(x1, · · · , xT ) to the corresponding slot label se-
quence yS = (yS1 , · · · , yST ), and intent detection
can be seen as a classification problem to decide
the intent label yI . Popular approaches for slot fill-
ing include conditional random fields (CRF) (Ray-

1The code is available at: https://github.com/
MiuLab/SlotGated-SLU.

W find comedies by james cameron
↓ ↓ ↓ ↓ ↓

S O B-genre O B-dir I-dir
I find movie

Figure 1: An example utterance with annotations of se-
mantic slots in IOB format (S) and intent (I), B-dir and
I-dir denote the director name.

mond and Riccardi, 2007) and recurrent neural
network (RNN) (Yao et al., 2014), and different
classification methods, such as support vector ma-
chine (SVM) and RNN, have been applied to in-
tent prediction.

Considering that pipelined approaches usually
suffer from error propagation due to their inde-
pendent models, the joint model for slot filling
and intent detection has been proposed to improve
sentence-level semantics via mutual enhancement
between two tasks (Guo et al., 2014; Hakkani-Tür
et al., 2016; Chen et al., 2016). In addition, the
attention mechanism (Bahdanau et al., 2014) was
introduced and leveraged into the model in order
to provide the precise focus, which allows the net-
work to learn where to pay attention in the in-
put sequence for each output label (Liu and Lane,
2015, 2016). The attentional model proposed by
Liu and Lane (2016) achieved the state-of-the-art
performance for joint slot filling and intent predic-
tion, where the parameters for slot filling and in-
tent prediction are learned in a single model with
a shared objective. However, the prior work did
not “explicitly” model the relationships between
the intent and slots; instead, it applied a joint loss
function to “implicitly” consider both cues. Be-
cause the slots often highly depend on the in-
tent, this work focuses on how to model the ex-
plicit relationships between slots and intent vec-
tors by introducing a slot-gated mechanism. The
contributions are three-fold: 1) the proposed slot-

753



𝑥1 𝑥2 𝑥3 𝑥4

Intent     Attention

𝑦1
𝑆 𝑦2

𝑆 𝑦3
𝑆 𝑦4

𝑆

𝑦𝐼

Slot 
Gate

BLSTM

Word 
Sequence

Slot 
Sequence

Slot       Attention

ℎ1 ℎ2 ℎ3 ℎ4
Intent

𝑥1 𝑥2 𝑥3 𝑥4

Intent     Attention

𝑦1
𝑆 𝑦2

𝑆 𝑦3
𝑆 𝑦4

𝑆

𝑦𝐼

Slot 
Gate

BLSTM

Word 
Sequence

Slot 
Sequence

ℎ1 ℎ2 ℎ3 ℎ4
Intent

(a) Slot-Gated Model with Full Attention (b) Slot-Gated Model with Intent Attention

Figure 2: The architecture of the proposed slot-gated models.

gated approach achieves better performance than
the attention-based models; 2) the experiments on
two SLU datasets show the generalization and the
effectiveness of the proposed slot gate; 3) the gat-
ing results help us analyze the slot-intent relations.

2 Proposed Approach

This section first explains our attention-based
RNN model and then introduces the proposed slot
gate mechanism for joint slot filling and intent pre-
diction. The model architecture is illustrated in
Figure 2, where there are two different model. (a)
is one with both slot attention and intent attention
and (b) is another with only intent attention.

2.1 Attention-Based RNN Model
The bidirectional long short-term memory
(BLSTM) model (Mesnil et al., 2015) takes
a word sequence x = (x1, . . . , xT ) as input,
and then generates forward hidden state

−→
hi and

backward hidden state
←−
hi . The final hidden state

hi at time step i is a concatenation of
−→
hi and

←−
hi ,

i.e. hi = [
−→
hi ,
←−
hi ].

Slot Filling For slot filling, x is mapping
to its corresponding slot label sequence y =
(yS1 , . . . , y

S
T ). For each hidden state hi, we com-

pute the slot context vector cSi as the weighted
sum of LSTM’s hidden states, h1, ..., hT , by the
learned attention weights αSi,j :

cSi =
T∑

j=1

αSi,jhj , (1)

where the slot attention weights are computed as
below.

αSi,j =
exp(ei,j)∑T

k=1 exp(ei,k)
, (2)

ei,k = σ(W
S
hehk), (3)

where σ is the activation function, and WShe is the
weight matrix of a feed-forward neural network.
Then the hidden state and the slot context vector
are utilized for slot filling:

ySi = softmax(W
S
hy(hi + c

S
i )), (4)

where ySi is the slot label of the i-th word in the in-
put, and WShy is the weight matrix. The slot atten-
tion is shown as the blue component in Figure 2(a).

Intent Prediction The intent context vector cI
can also be computed in the same manner as cS ,
but the intent detection part only takes the last hid-
den state of BLSTM. The intent prediction is mod-
eled similarly:

yI = softmax(W Ihy(hT + c
I)). (5)

2.2 Slot-Gated Mechanism
This section describes the proposed slot-gated
mechanism illustrated in the red part of Figure 2.
The proposed slot-gated model introduces an ad-
ditional gate that leverages intent context vector
for modeling slot-intent relationships in order to
improve slot filling performance. First, slot con-
text vector cSi and intent context vector c

I are com-
bined (cI broadcasts in time dimension to have the

754



𝑊

𝑐𝐼

𝑣

tanh

𝑔

𝑐𝑖
𝑆

Figure 3: Illustration of the slot gate.

same shape with cSi ) to pass through a slot gate
illustrated in Figure 3:

g =
∑

v · tanh(cSi +W · cI) (6)

where v and W are trainable vector and matrix re-
spectively. The summation is done over elements
in one time step. g can be seen as a weighted fea-
ture of the joint context vector (cSi and c

I ). We use
g to weight between hi and cSi to derive y

S
i and

replace (4) as below:

ySi = softmax(W
S
hy(hi + c

S
i · g)). (7)

A larger g indicates that the slot context vector and
the intent context vector pay attention to the same
part of the input sequence, which also infers that
the correlation between the slot and the intent is
stronger and the context vector is more “reliable”
for contributing the prediction results.

To compare the power of the slot gate with at-
tention mechanism, we also propose a slot-gated
model with only intent attention in which (6) and
(7) are reformed as (8) and (9) respectively (shown
in Figure 2(b)):

g =
∑

v · tanh(hi +W · cI) (8)

ySi = softmax(W
S
hy(hi + hi · g)) (9)

This version allows the slots and intent to share the
attention mechanism.

2.3 Joint Optimization
To obtain both slot filling and intent prediction
jointly, the objective is formulated as

p(yS , yI | x) (10)

ATIS Snips
Vocabulary Size 722 11,241
#Slots 120 72
#Intents 21 7
Training Set Size 4,478 13,084
Development Set Size 500 700
Testing Set Size 893 700

Table 1: Statistics of ATIS and Snips datasets.

= p(yI | x)
T∏

t=1

p(ySt | x)

= p(yI | x1, · · · , xT )
T∏

t=1

p(ySt | x1, · · · , xT ),

where p(yS , yI | x) is the conditional probability
of the understanding result (slot filling and intent
prediction) given the input word sequence and is
maximized for SLU.

3 Experiment

To evaluate the proposed model, we conduct ex-
periments on the benchmark datasets, ATIS (Air-
line Travel Information System) and Snips. The
statistics are shown in Table 1.

3.1 Setup

The ATIS (Airline Travel Information Systems)
dataset (Tur et al., 2010) is widely used in SLU
research. The dataset contains audio recordings of
people making flight reservations. The training set
contains 4,478 utterances and the test set contains
893 utterances. We use another 500 utterances for
development set. There are 120 slot labels and 21
intent types in the training set.

To justify the generalization of the proposed
model, we use another NLU dataset custom-
intent-engines2 collected by Snips for model eval-
uation. This dataset is collected from the Snips
personal voice assistant, where the number of
samples for each intent is approximately the same.
The training set contains 13,084 utterances and the
test set contains 700 utterances. We use another
700 utterances as the development set. There are
72 slot labels and 7 intent types.

Compared to single-domain ATIS dataset, Snips
is more complicated mainly due to the intent diver-

2https://github.com/snipsco/
nlu-benchmark/tree/master/
2017-06-custom-intent-engines

755



Intent Utterance Example
SearchCreativeWork Find me the I, Robot television show
GetWeather Is it windy in Boston, MA right now?
BookRestaurant I want to book a highly rated restaurant tomorrow night
PlayMusic Play the last track from Beyonc off Spotify
AddToPlaylist Add Diamonds to my roadtrip playlist
RateBook Give 6 stars to Of Mice and Men
SearchScreeningEvent Check the showtimes for Wonder Woman in Paris

Table 2: Intents and examples in Snips dataset.

Model
ATIS Dataset Snips Dataset

Slot Intent Sentence Slot Intent Sentence
(F1) (Acc) (Acc) (F1) (Acc) (Acc)

Joint Seq. (Hakkani-Tür et al., 2016) 94.3 92.6 80.7 87.3 96.9 73.2
Atten.-Based (Liu and Lane, 2016) 94.2 91.1 78.9 87.8 96.7 74.1

Proposed
Slot-Gated (Full Atten.) 94.8† 93.6† 82.2† 88.8† 97.0 75.5†
Slot-Gated (Intent Atten.) 95.2† 94.1† 82.6† 88.3 96.8 74.6

Table 3: SLU performance on ATIS and Snips datasets (%). † indicates the significant improvement over all
baselines (p < 0.05).

sity and large vocabulary. Table 2 shows the in-
tents and associated utterance examples. Regard-
ing the intent diversity, for example, GetWeather
and BookRestaurant in Snips are from different
topics, resulting larger vocabulary. In the other
hand, intents in ATIS are all about flight informa-
tion with similar vocabularies across them. More-
over, intents in ATIS are highly unbalanced, where
atis flight accounts for about 74% of training data
while atis cheapest appears only once. The com-
parison between two datasets can be found in Ta-
ble 1.

In all experiments, we set the size of hidden
vectors to 64, the optimizer is adam, the reported
numbers are averaged over 20 runs, and the maxi-
mum epoch is set to 10 and 20 on ATIS and Snips
respectively with an early-stop strategy.

3.2 Results and Analysis

We evaluate the SLU performance about slot fill-
ing using F1 score, intent prediction using ac-
curacy, and sentence-level semantic frame pars-
ing using whole frame accuracy. The experimen-
tal results are shown in Table 3, where the com-
pared baselines for joint slot filling and intent pre-
diction include the state-of-the-art sequence-based
joint model using bidirectional LSTM (Hakkani-
Tür et al., 2016) and attention-based model (Liu
and Lane, 2016). We validate the performance
improvement with statistical significance test for

all experiments, where single-tailed t-test is per-
formed to measure whether the results from the
proposed model are significant better than ones
from baselines. The numbers with star markers
indicate that the improvement is significant with
p < 0.05.

Table 3 shows that the proposed slot-gated
mechanism with full attention significantly outper-
forms the baselines for both datasets, where al-
most all tasks (slot filling, intent prediction, and
semantic frame) obtain the improvement, demon-
strating that explicitly modeling strong relation-
ships between slots and intent can benefit SLU ef-
fectively. In ATIS dataset, the proposed slot-gated
model with only intent attention achieves slightly
better performance with fewer parameters (from
284K to 251K). However, it does not achieve bet-
ter results in Snips dataset. Considering different
complexity of these datasets, the probable reason
is that a simpler SLU task, such as ATIS, does not
require additional slot attention to achieve good
results, and the slot gate is capable of providing
enough cues for slot filling. On the other hand,
Snips is more complex, so that the slot attention is
needed in order to model slot filling better (as well
as the semantic frame results).

It is obvious that our proposed model performs
better especially on sentence-level semantic frame
results, where the relative improvement is around
4.1% and 1.9% for ATIS and Snips respectively.

756



It may credit to the proposed slot gate that learns
the slot-intent relations to provide helpful infor-
mation for global optimization of the joint model.
In sum, for joint slot filling and intent prediction,
the experiments show that leveraging explicit slot-
intent relations controlled by the slot-gated mech-
anism can effectively achieve better sentence-level
semantic frame performance due to global consid-
eration.

4 Conclusion

This paper focuses on learning the explicit slot-
intent relations by introducing a slot-gated mech-
anism into the state-of-the-art attention model,
which allows the slot filling can be conditioned on
the learned intent result in order to achieve better
SLU (joint slot filling and intent detection). The
experiments show that the proposed approach out-
performs the baselines and can be generalized to
different datasets. Also, the slot-gated model is
more useful for a simple understanding task, be-
cause the slot-intent relations are stronger and eas-
ily modeled, and this paper provides the guidance
of model design for future SLU work.

Acknowledgements

We would like to thank reviewers for their insight-
ful comments on the paper. The authors are sup-
ported by the Institute for Information Industry,
Ministry of Science and Technology of Taiwan,
Google Research, Microsoft Research, and Medi-
aTek Inc..

References
Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben-

gio. 2014. Neural machine translation by jointly
learning to align and translate. arXiv preprint
arXiv:1409.0473.

Yun-Nung Chen, Dilek Hakanni-Tür, Gokhan Tur,
Asli Celikyilmaz, Jianfeng Guo, and Li Deng.
2016. Syntax or semantics? knowledge-guided
joint semantic frame parsing. In Proceedings of
2016 IEEE Spoken Language Technology Workshop,
pages 348–355. IEEE.

Daniel Guo, Gokhan Tur, Wen-tau Yih, and Geoffrey
Zweig. 2014. Joint semantic utterance classification
and slot filling with recursive neural networks. In
Proceedings of 2014 IEEE Spoken Language Tech-
nology Workshop, pages 554–559. IEEE.

Dilek Hakkani-Tür, Gökhan Tür, Asli Celikyilmaz,
Yun-Nung Chen, Jianfeng Gao, Li Deng, and Ye-
Yi Wang. 2016. Multi-domain joint semantic frame

parsing using bi-directional rnn-lstm. In Proceed-
ings of INTERSPEECH, pages 715–719.

Bing Liu and Ian Lane. 2015. Recurrent neural net-
work structured output prediction for spoken lan-
guage understanding. In Proc. NIPS Workshop
on Machine Learning for Spoken Language Under-
standing and Interactions.

Bing Liu and Ian Lane. 2016. Attention-based recur-
rent neural network models for joint intent detection
and slot filling. In Proceedings of INTERSPEECH.

Grégoire Mesnil, Yann Dauphin, Kaisheng Yao,
Yoshua Bengio, Li Deng, Dilek Hakkani-Tur, Xi-
aodong He, Larry Heck, Gokhan Tur, Dong Yu, et al.
2015. Using recurrent neural networks for slot fill-
ing in spoken language understanding. IEEE/ACM
Transactions on Audio, Speech and Language Pro-
cessing (TASLP), 23(3):530–539.

Christian Raymond and Giuseppe Riccardi. 2007.
Generative and discriminative algorithms for spoken
language understanding. In Eighth Annual Confer-
ence of the International Speech Communication As-
sociation.

Gokhan Tur and Renato De Mori. 2011. Spoken lan-
guage understanding: Systems for extracting seman-
tic information from speech. John Wiley & Sons.

Gokhan Tur, Dilek Hakkani-Tür, and Larry Heck.
2010. What is left to be understood in ATIS? In
Proceedings of 2010 IEEE Spoken Language Tech-
nology Workshop (SLT), pages 19–24. IEEE.

Kaisheng Yao, Baolin Peng, Yu Zhang, Dong Yu, Ge-
offrey Zweig, and Yangyang Shi. 2014. Spoken lan-
guage understanding using long short-term memory
neural networks. In Proceedings of 2014 IEEE Spo-
ken Language Technology Workshop, pages 189–
194. IEEE.

757


