



















































Topic-Specific Sentiment Analysis Can Help Identify Political Ideology


Proceedings of the 9th Workshop on Computational Approaches to Subjectivity, Sentiment and Social Media Analysis, pages 79–84
Brussels, Belgium, October 31, 2018. c©2018 Association for Computational Linguistics

https://doi.org/10.18653/v1/P17

79

Topic-Specific Sentiment Analysis Can Help Identify Political Ideology

Sumit Bhatia
IBM Research AI
New Delhi, India

sumitbhatia@in.ibm.com

Deepak P
Queen’s University Belfast

Belfast, UK
deepaksp@acm.org

Abstract

Ideological leanings of an individual can of-
ten be gauged by the sentiment one expresses
about different issues. We propose a sim-
ple framework that represents a political ide-
ology as a distribution of sentiment polarities
towards a set of topics. This representation
can then be used to detect ideological leanings
of documents (speeches, news articles, etc.)
based on the sentiments expressed towards dif-
ferent topics. Experiments performed using a
widely used dataset show the promise of our
proposed approach that achieves comparable
performance to other methods despite being
much simpler and more interpretable.

1 Introduction

The ideological leanings of a person within the
left-right political spectrum are often reflected by
how one feels about different topics and by means
of preferences among various choices on partic-
ular issues. For example, a left-leaning person
would prefer nationalization and state control of
public services (such as healthcare) where privati-
zation would be often preferred by people that lean
towards the right. Likewise, a left-leaning person
would often be supportive of immigration and will
often talk about immigration in a positive man-
ner citing examples of benefits of immigration on
a country’s economy. A right-leaning person, on
the other hand, will often have a negative opinion
about immigration.

Most of the existing works on political ideol-
ogy detection from text have focused on utilizing
bag-of-words and other syntactic features to cap-
ture variations in language use (Sim et al., 2013;
Biessmann, 2016; Iyyer et al., 2014). We pro-
pose an alternative mechanism for political ide-
ology detection based on sentiment analysis. We
posit that adherents of a political ideology gener-
ally have similar sentiment toward specific topics
(for example, right wing followers are often posi-

tive towards free markets, lower tax rates, etc.) and
thus, a political ideology can be represented by a
characteristic sentiment distribution over different
topics (Section 3). This topic-specific sentiment
representation of a political ideology can then be
used for automatic ideology detection by compar-
ing the topic-specific sentiments as expressed by
the content in a document (news article, magazine
article, collection of social media posts by a user,
utterances in a conversation, etc.).

In order to validate our hypothesis, we con-
sider exploiting the sentiment information towards
topics from archives of political debates to build
a model for identifying political orientation of
speakers as one of right or left leaning, which
corresponds to republicans and democrats respec-
tively, within the context of US politics. This is
inspired by our observation that the political lean-
ings of debators are often expressed in debates
by way of speakers’ sentiments towards particu-
lar topics. Parliamentary or Senate debates of-
ten bring the ideological differences to the cen-
tre stage, though somewhat indirectly. Heated de-
bates in such forums tend to focus on the choices
proposed by the executive that are in sharp con-
flict with the preference structure of the opposition
members. Due to this inherent tendency of par-
liamentary debates to focus on topics of disagree-
ment, the sentiments exposited in debates hold
valuable cues to identify the political orientation
of the participants.

We develop a simple classification model
that uses a topic-specific sentiment summariza-
tion for republican and democrat speeches sep-
arately. Initial results of experiments conducted
using a widely used dataset of US Congress de-
bates (Thomas et al., 2006) are encouraging and
show that this simple model compares well with
classification models that employ state-of-the-art
distributional text representations (Section 4).



80

2 Related Work

2.1 Political Ideology Detection

Political ideology detection has been a relatively
new field of research within the NLP community.
Most of the previous efforts have focused on cap-
turing the variations in language use in text rep-
resenting content of different ideologies. Beiss-
mann et al. (2016) employ bag-of-word features
for ideology detection in different domains such as
speeches in German parliament, party manifestos,
and facebook posts. Sim et al. (2013) use a labeled
corpus of political writings to infer lexicons of
cues strongly associated with different ideologies.
These “ideology lexicons” are then used to ana-
lyze political speeches and identify their ideologi-
cal leanings. Iyyer at al. (2014) recently adopted a
recursive neural network architecture to detect ide-
ological bias of single sentences. In addition, topic
models have also been used for ideology detec-
tion by identifying latent topic distributions across
different ideologies (Lin et al., 2008; Ahmed and
Xing, 2010). Gerrish and Blei (2011) connected
text of the legislations to voting patterns of legis-
lators from different parties.

2.2 Sentiment Analysis for Controversy
Detection

Sentiment analysis has proved to be a useful tool
in detecting controversial topics as it can help
identify topics that evoke different feelings among
people on opposite side of the arguments. Mejova
et al. (2014) analyzed language use in controver-
sial news articles and found that a writer may
choose to highlight the negative aspects of the op-
posing view rather than emphasizing the positive
aspects of ones view. Lourentzou et al. (2015) uti-
lize the sentiments expressed in social media com-
ments to identify controversial portions of news
articles. Given a news article and its associated
comments on social media, the paper links com-
ments with each sentence of the article (by using
a sentence as a query and retrieving comments us-
ing BM25 score). For all the comments associated
with a sentence, a sentiment score is then com-
puted, and sentences with large variations in posi-
tive and negative comments are identified as con-
troversial sentences. Choi et al. (2010) go one step
further and identify controversial topics and their
sub-topics in news articles.

3 Using Topic Sentiments for Ideology
Detection

Let D = {. . . , d, . . .} be a corpus of political doc-
uments such as speeches or social media postings.
Let L = {. . . , l, . . .} be the set of ideology class
labels. Typical scenarios would just have two class
labels (i.e., |L| = 2), but we will outline our for-
mulation for a general case. For document d ∈ D,
ld ∈ L denotes the class label for that document.
Our method relies on the usage of topics, each of
which are most commonly represented by a prob-
ability distribution over the vocabulary. The set of
topics overD, which we will denote using T , may
be identified using a topic modeling method such
as LDA (Blei et al., 2003) unless a pre-defined set
of handcrafted topics is available.

Given a document d and a topic t, our method
relies on identifying the sentiment as expressed
by content in d towards the topic t. The sen-
timent could be estimated in the form of a cat-
egorical label such as one of positive, negative
and neutral (Haney, 2013). Within our mod-
elling, however, we adopt a more fine-grained
sentiment labelling, whereby the sentiment for a
topic-document pair is a probability distribution
over a plurality of ordinal polarity classes rang-
ing from strongly positive to strongly negative.
Let sdt represent the topic-sentiment polarity vec-
tor of d towards t such that sdt(x) represents the
probability of the polarity class x. Combining
the topic-sentiment vectors for all topics yields a
document-specific topic-sentiment matrix (TSM)
as follows:

Sd,T =

. . . sdt1(x) . . .. . . sdt2(x) . . .
...

...
...

 (1)
Each row in the matrix corresponds to a topic

within T , with each element quantifying the prob-
ability associated with the sentiment polarity class
x for the topic t within document d. The topic-
sentiment matrix above may be regarded as a sen-
timent signature for the document over the topic
set T .

3.1 Determining Topic-specific Sentiments

In constructing TSMs, we make use of topic-
specific sentiment estimations as outlined above.
Typical sentiment analysis methods (e.g., NLTK



81

Sentiment Analysis1) are designed to determine
the overall sentiment for a text segment. Using
such sentiment analysis methods in order to de-
termine topic-specific sentiments is not necessar-
ily straightforward. We adopt a simple keyword
based approach for the task. For every document-
topic pair (t, d), we extract the sentences from d
that contain at least one of the top-k keywords as-
sociated with the topic t. We then collate the sen-
tences in the order in which they appear in d and
form a mini-document dt. This document dt is
then passed on to a conventional sentiment ana-
lyzer that would then estimate the sentiment po-
larity as a probability distribution over sentiment
polarity classes, which then forms the sdt(.) vec-
tor. We use k = 5 and the RNN based sentiment
analyzer (Socher et al., 2013) in our method.

3.2 Nearest TSM Classification

We now outline a simple classification model that
uses summaries of TSMs. Given a labeled training
set of documents, we would like to find the proto-
typical TSM corresponding to each label. This can
be done by identifying the matrix that minimizes
the cumulative deviation from those correspond-
ing to the documents with the label.

Sl,T = argmin
X

∑
d∈D∧ld=l

||X − Sd,T ||2F (2)

where ||M ||F denotes the Frobenius norm. It
turns out that such a label-specific signature ma-
trix is simply the mean of the topic-sentiment ma-
trices corresponding to documents that bear the re-
spective label, which may be computed using the
below equation.

Sl,T =
1

|{d|d ∈ D ∧ ld = l}|
∑

d∈D∧ld=l
Sd,T

(3)
For an unseen (test) document d′, we first com-

pute the TSM Sd′,T , and assign it the label corre-
sponding to the label whose TSM is most proximal
to Sd′,T .

ld′ = argmin
l
||Sd′,T − Sl,T ||2F (4)

1http://text-processing.com/demo/
sentiment/

3.3 Logistic Regression Classification

In two class scenarios with label such as
{left, right} or {democrat, republican} as we
have in our dataset, TSMs can be flattened into
a vector and fed into a logistic regression clas-
sifier that learns weights - i.e., co-efficients for
each topic + sentiment polarity class combination.
These weights can then be used to estimate the la-
bel by applying it to the new document’s TSM.

4 Experiments

4.1 Dataset

We used the publicly available Convote
dataset2 (Thomas et al., 2006) for our exper-
iments. The dataset provides transcripts of
debates in the House of Representatives of the
U.S Congress for the year 2005. Each file in the
dataset corresponds to a single, uninterrupted
utterance by a speaker in a given debate. We
combine all the utterances of a speaker in a
given debate in a single file to capture different
opinions/view points of the speaker about the
debate topic. We call this document the view
point document (VPD) representing the speaker’s
opinion about different aspects of the issue being
debated. The dataset also provides political
affiliations of all the speakers – Republican (R),
Democrat (D), and Independent (I). With there
being only six documents for the independent
class (four in training, two in test), we excluded
them from our evaluation. Table 1 summarizes
the statistics about the dataset and distribution of
different classes. We obtained 50 topics using
LDA from Mallet3 run over the training dataset.
The topic-sentiment matrix was obtained using
the Stanford CoreNLP sentiment API4 (Man-
ning et al., 2014) which provides probability
distributions over a set of five sentiment polarity
classes.

4.2 Methods

In order to evaluate our proposed TSM-based
methods - viz., nearest class (NC) and logistic re-
gression (LR) - we use the following methods in
our empirical evaluation.

2http://www.cs.cornell.edu/home/llee/
data/convote.html

3http://mallet.cs.umass.edu/
4https://nlp.stanford.edu/sentiment/

code.html

http://text-processing.com/demo/sentiment/
http://text-processing.com/demo/sentiment/
http://www.cs.cornell.edu/home/llee/data/convote.html
http://www.cs.cornell.edu/home/llee/data/convote.html
http://mallet.cs.umass.edu/
https://nlp.stanford.edu/sentiment/code.html
https://nlp.stanford.edu/sentiment/code.html


82

Training Set Test Set

Republican (R) 530 194
Democrat (D) 641 215

Total 1175 411

Table 1: Distribution of different classes in the Con-
Vote dataset.

Method R D Total

GloVe d2v 0.6391 0.6465 0.6430
TSM-NC 0.6907 0.4558 0.5672
TSM-LR 0.5258 0.7628 0.6504
GloVe-d2v + TSM 0.5051 0.7023 0.6088

Table 2: Results achieved by different methods on the
ideology classification task.

1. GloVe-d2v: We use pre-trained GloVe (Pen-
nington et al., 2014) word embeddings to
compute vector representation of each VPD
by averaging the GloVe vectors for all words
in the document. A logistic regression clas-
sifier is then trained on the vector representa-
tions thus obtained.

2. GloVe-d2v+TSM: A logistic regression clas-
sifier trained on the GloVe features as well as
TSM features.

4.3 Results

Table 2 reports the classification results for dif-
ferent methods described above. TSM-NC, the
method that uses the TSMvectors and performs
simple nearest class classification achieves an
overall accuracy of 57%. Next, training a logis-
tic regression classifier trained on TSMvectors as
features, TSM-LR, achieves significant improve-
ment with an overall accuracy of 65.04%. The
word embedding based baseline, the GloVe-d2v
method, achieves slightly lower performance with
an overall accuracy of 64.30%. However, we do
note that the per-class performance of GloVe-d2v
method is more balanced with about 64% accu-
racy for both classes. The TSM-LR method on
the other hand achieves about 76% for R class and
only 52% for the D class. The results obtained are
promising and lend weight to out hypothesis that
ideological leanings of a person can be identified
by using the fine-grained sentiment analysis of the
viewpoint a person has towards different underly-
ing topics.

4.4 Discussion
Towards analyzing the significance of the results,
we would like to start with drawing attention to
the format of the data used in the TSM methods.
The document-specific TSM matrices do not con-
tain any information about the topics themselves,
but only about the sentiment in the document to-
wards each topic; one may recollect that sdt(.) is a
quantification of the strength of the sentiment in d
towards topic t. Thus, in contrast to distributional
embeddings such as doc2vec, TSMs contain only
the information that directly relates to sentiment
towards specific topics that are learnt from across
the corpus. The results indicate that TSM meth-
ods are able to achieve comparable performance
to doc2vec-based methods despite usage of only a
small slice of informatiom. This points to the im-
portance of sentiment information in determining
the political leanings from text. We believe that
leveraging TSMs along with distributional embed-
dings in a manner that can combine the best of
both views would improve the state-of-the-art of
political ideology detection.

Next, we also studied if there are topics that are
more polarizing than others and how different top-
ics impact classification performance. We identi-
fied polarizing topics, i.e, topics that invoke oppo-
site sentiments across two classes (ideologies) by
using the following equation.

dist(t, R,D) = ||sR,t − sR,t||F (5)

Here, sR,t and sD,t represent the sentiment vec-
tors for topic t for republican and democrat
classes. Note that these sentiment vectors are the
rows corresponding to topic t in TSMs for the two
classes, respectively.

Table 3 lists the top five topics with most dis-
tance, i.e., most polarizing topics (top) and five
topics with least distance, i.e.,least polarizing top-
ics (bottom) as computed by equation 5. Note
that the topics are represented using the top key-
words that they contain according to the proba-
bility distribution of the topic. We observe that
the most polarizing topics include topics related
to healthcare (H3, H4), military programs (H5),
and topics related to administration processes (H1
and H2). The least polarizing topics include topics
related to worker safety (L3) and energy projects
(L2). One counter-intuitive observation is topic
related to gun control (L4) that is amongst the
least polarizing topics. This anomaly could be at-



83

Most polarizing topics

H1: republican congress majority administration leadership n’t vote
party republicans special
H2: administration process vote work included find n’t true fix carriers
H3: health programs education funding million program cuts care billion
year
H4: health insurance small care coverage businesses plans ahps
employees state
H5: military center n’t students recruiters policy houston men
universities colleges

Least polarizing topics

L1: enter director march years response found letter criminal paid
general
L2: corps nuclear year energy projects committee project million
funding funds
L3: osha safety workers commission health h.r employers occupational
bills workplace
L4: gun police industry lawsuits firearms dept chief manufacturers
dealers guns
L5: medal gold medals individuals reagan history legislation ronald king
limiting

Table 3: List of most polarizing (top) and least polarizing (bottom) topics as computed using equation 5.

tributed to only a few speeches related to this issue
in the training set (only 23 out of 1175 speeches
mention gun) that prevents a reliable estimate of
the probability distributions. We observed simi-
lar low occurrences of other lower distance top-
ics too indicating the potential for improvements
in computation of topic-specific sentiment repre-
sentations with more data. In fact, performing
the nearest neighbor classification (TSM −NC)
with only top-10 most polarizing topics led to im-
provements in classification accuracy from 57%
to 61% suggesting that with more data, better
TSM representations could be learned that are
better at discriminating between different ideolo-
gies.

5 Conclusions

We proposed to exploit topic-specific sentiment
analysis for the task of automatic ideology de-
tection from text. We described a simple frame-
work for representing political ideologies and doc-
uments as a matrix capturing sentiment distri-
butions over topics and used this representation
for classifying documents based on their topic-
sentiment signatures. Empirical evaluation over a
widely used dataset of US Congressional speeches
showed that the proposed approach performs on a
par with classifiers using distributional text repre-

sentations. In addition, the proposed approach of-
fers simplicity and easy interpretability of results
making it a promising technique for ideology de-
tection. Our immediate future work will focus
on further solidifying our observations by using
a larger dataset to learn better TSMs for different
ideologies. Further, the framework easily lends it-
self to be used for detecting ideological leanings of
authors, social media users, news websites, maga-
zines, etc. by computing their TSMs and compar-
ing against the TSMs of different ideologies.

Acknowledgments

We would like to thank the anonymous reviewers
for their valuable comments and suggestions that
helped us improve the quality of this work.

References
Amr Ahmed and Eric P. Xing. 2010. Staying informed:

Supervised and semi-supervised multi-view topical
analysis of ideological perspective. In EMNLP,
pages 1140–1150. ACL.

Felix Biessmann. 2016. Automating political bias pre-
diction. arXiv preprint arXiv:1608.02195.

David M Blei, Andrew Y Ng, and Michael I Jordan.
2003. Latent dirichlet allocation. Journal of ma-
chine Learning research, 3(Jan):993–1022.



84

Yoonjung Choi, Yuchul Jung, and Sung-Hyon Myaeng.
2010. Identifying controversial issues and their sub-
topics in news articles. In Intelligence and Security
Informatics, Pacific Asia Workshop, PAISI 2010, Hy-
derabad, India, June 21, 2010. Proceedings, volume
6122 of Lecture Notes in Computer Science, pages
140–153. Springer.

Sean Gerrish and David M. Blei. 2011. Predicting leg-
islative roll calls from text. In Proceedings of the
28th International Conference on Machine Learn-
ing, ICML 2011, Bellevue, Washington, USA, June
28 - July 2, 2011, pages 489–496. Omnipress.

Carol Haney. 2013. Sentiment analysis: Providing cat-
egorical insight into unstructured textual data. So-
cial Media, Sociality, and Survey Research, pages
35–59.

Mohit Iyyer, Peter Enns, Jordan L. Boyd-Graber, and
Philip Resnik. 2014. Political ideology detection us-
ing recursive neural networks. In ACL (1), pages
1113–1122. The Association for Computer Linguis-
tics.

Wei-Hao Lin, Eric P. Xing, and Alexander G. Haupt-
mann. 2008. A joint topic and perspective model
for ideological discourse. In Machine Learning
and Knowledge Discovery in Databases, European
Conference, ECML/PKDD 2008, Antwerp, Belgium,
September 15-19, 2008, Proceedings, Part II, vol-
ume 5212 of Lecture Notes in Computer Science,
pages 17–32. Springer.

Ismini Lourentzou, Graham Dyer, Abhishek Sharma,
and ChengXiang Zhai. 2015. Hotspots of news ar-
ticles: Joint mining of news text & social media to
discover controversial points in news. In Big Data,
pages 2948–2950. IEEE.

Christopher D. Manning, Mihai Surdeanu, John Bauer,
Jenny Rose Finkel, Steven Bethard, and David Mc-
Closky. 2014. The stanford corenlp natural lan-
guage processing toolkit. In ACL (System Demon-
strations), pages 55–60. The Association for Com-
puter Linguistics.

Yelena Mejova, Amy X. Zhang, Nicholas Diakopoulos,
and Carlos Castillo. 2014. Controversy and senti-
ment in online news. CoRR, abs/1409.8152.

Jeffrey Pennington, Richard Socher, and Christo-
pher D. Manning. 2014. Glove: Global vectors for
word representation. In Proceedings of the 2014
Conference on Empirical Methods in Natural Lan-
guage Processing, EMNLP 2014, October 25-29,
2014, Doha, Qatar, A meeting of SIGDAT, a Special
Interest Group of the ACL, pages 1532–1543. ACL.

Yanchuan Sim, Brice D. L. Acree, Justin H. Gross, and
Noah A. Smith. 2013. Measuring ideological pro-
portions in political speeches. In Proceedings of
the 2013 Conference on Empirical Methods in Nat-
ural Language Processing, pages 91–101. Associa-
tion for Computational Linguistics.

Richard Socher, Alex Perelygin, Jean Wu, Jason
Chuang, Christopher D. Manning, Andrew Ng, and
Christopher Potts. 2013. Recursive deep models
for semantic compositionality over a sentiment tree-
bank. In Proceedings of the 2013 Conference on
Empirical Methods in Natural Language Process-
ing, pages 1631–1642, Seattle, Washington, USA.
Association for Computational Linguistics.

Matt Thomas, Bo Pang, and Lillian Lee. 2006. Get out
the vote: Determining support or opposition from
congressional floor-debate transcripts. In EMNLP
2007, Proceedings of the 2006 Conference on Em-
pirical Methods in Natural Language Processing,
22-23 July 2006, Sydney, Australia, pages 327–335.
ACL.


