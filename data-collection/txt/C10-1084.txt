Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 743–751,

Beijing, August 2010

743

Enhancing Morphological Alignment for Translating

Highly Inﬂected Languages ∗

Minh-Thang Luong
School of Computing

National University of Singapore
luongmin@comp.nus.edu.sg

Abstract

We propose an unsupervised approach uti-
lizing only raw corpora to enhance mor-
phological alignment involving highly in-
ﬂected languages. Our method focuses on
closed-class morphemes, modeling their
inﬂuence on nearby words. Our language-
independent model
recovers important
links missing in the IBM Model 4 align-
ment and demonstrates improved end-to-
end translations for English-Finnish and
English-Hungarian.

Min-Yen Kan

School of Computing

National University of Singapore
kanmy@comp.nus.edu.sg

target words are diffused over many morpholog-
ical forms. This problem has a direct impact on
end translation quality.

this

Our work addresses

shortcoming by
proposing a morphologically sensitive approach
to word alignment for language pairs involving
a highly inﬂected language.
In particular, our
method focuses on a set of closed-class mor-
phemes (CCMs), modeling their inﬂuence on
nearby words. With the model, we correct er-
roneous alignments in the initial IBM Model 4
runs and add new alignments, which results in im-
proved translation quality.

1 Introduction
Modern statistical machine translation (SMT)
systems, regardless of whether they are word-,
phrase- or syntax-based, typically use the word as
the atomic unit of translation. While this approach
works when translating between languages with
limited morphology such as English and French,
it has been found inadequate for morphologically-
rich languages like Arabic, Czech and Finnish
(Lee, 2004; Goldwater and McClosky, 2005;
Yang and Kirchhoff, 2006). As a result, a line
of SMT research has worked to incorporate mor-
phological analysis to gain access to information
encoded within individual words.

In a typical MT process, word aligned data is
fed as training data to create a translation model.
In cases where a highly inﬂected language is
involved, the current word-based alignment ap-
proaches produce low-quality alignment, as the
statistical correspondences between source and
∗This work was supported by a National Research Foun-
dation grant “Interactive Media Search” (grant # R-252-000-
325-279)

After reviewing related work, we give a case
study for morpheme alignment in Section 3. Sec-
tion 4 presents our four-step approach to construct
and incorporate our CCM alignment model into
the grow-diag process. Section 5 describes exper-
iments, while Section 6 analyzes the system mer-
its. We conclude with suggestions for future work.

2 Related Work

MT alignment has been an active research area.
One can categorize previous approaches into those
that use language-speciﬁc syntactic information
and those that do not.
Syntactic parse trees
have been used to enhance alignment (Zhang and
Gildea, 2005; Cherry and Lin, 2007; DeNero
and Klein, 2007; Zhang et al., 2008; Haghighi et
al., 2009). With syntactic knowledge, modeling
long distance reordering is possible as the search
space is conﬁned to plausible syntactic variants.
However, they generally require language-speciﬁc
tools and annotated data, making such approaches
infeasible for many languages. Works that follow
non-syntactic approaches, such as (Matusov et al.,

744

Figure 1: Sample English-Finnish IBM Model 4 alignments: (a) word-level and (b) morpheme-level. Solid lines indicate
intersection alignments, while the exhaustive asymmetric alignments are listed below. In (a), translation glosses for Finnish
are given; the dash-dot line is the incorrect alignment. In (b), bolded texts are closed-class morphemes (CCM), while bolded
indices indicate alignments involving CCMs. The dotted lines are correct CCM alignments not found by IBM Model 4.

2004; Liang et al., 2006; Ganchev et al., 2008),
which aim to achieve symmetric word alignment
during training, though good in many cases, are
not designed to tackle highly inﬂected languages.
Our work differs from these by taking a middle
road.
Instead of modifying the alignment algo-
rithm directly, we preprocess asymmetric align-
ments to improve the input to the symmetrizing
process later. Also, our approach does not make
use of speciﬁc language resources, relying only on
unsupervised morphological analysis.

3 A Case for Morpheme Alignment
The notion that morpheme based alignment would
be useful in highly inﬂected languages is intu-
itive. Morphological inﬂections might indicate
tense, gender or number that manifest as separate
words in largely uninﬂected languages. Capturing
these subword alignments can yield better word
alignments that otherwise would be missed.

Let us make this idea concrete with a case study
of the beneﬁts of morpheme based alignment. We
show the intersecting alignments of an actual En-
glish (source) → Finnish (target) sentence pair in
Figure 1, where (a) word-level and (b) morpheme-
level alignments are shown. The morpheme-
level alignment is produced by automatically seg-
menting words into morphemes and running IBM
Model 4 on the resulting token stream.

Intersection links (i.e., common to both direct
and inverse alignments) play an important role in
creating the ﬁnal alignment (Och and Ney, 2004).
While there are several heuristics used in the sym-
metrizing process, the grow-diag(onal) process is

common and prevalent in many SMT systems,
such as Moses (Koehn et al., 2007). In the grow-
diag process, intersection links are used as seeds
to ﬁnd other new alignments within their neigh-
borhood. The process continues iteratively, until
no further links can be added.

In our example, the morpheme-level intersec-
tion alignment is better as it has no misalignments
and adds new alignments. However it misses
some key links. In particular, the alignments of
closed-class morphemes (CCMs;
later formally
deﬁned) as indicated by the dotted lines in (b) are
overlooked in the IBM Model 4 alignment. This
difﬁculty in aligning CCMs is due to:

1. Occurrences of garbage-collector words
(Moore, 2004) that attract CCMs to align to
them. Examples of such links in (b) are 1–23
or 11–21 with the occurrences of rare words
adjourn+11 and avatuksi23. We further
characterize such errors in Section 6.1.

2. Ambiguity among CCMs of the same surface
that causes incorrect matchings. In (b), we
observe multiple occurrence of the and n
on the source and target sides respectively.
While the link 8–6 is correct, 5–4 is not as i1
should be aligned to n4 instead. To resolve
such ambiguity, context information should
be considered as detailed in Section 4.3.

The fact that rare words and multiple afﬁxes
often occur in highly inﬂected languages exacer-
bates this problem, motivating our focus on im-
proving CCM alignment. Furthermore, having ac-
cess to the correct CCM alignments as illustrated

i1 declare2 resumed3 the4 session5 of6 the7 european8 parliament9 adjourned10 on11 1312 december13 199614

(a)

-1 julistan2 euroopan3 parlamentin4 perjantaina5 136 joulukuuta7 19968 keskeytyneen9 istuntokauden10 uudelleen11 avatuksi12
Gloss: -1 declare2 european3 parliament 4 on-friday5 136 december7 19968 adjourned9 session10 resumed11,12

Direct: 1-2 2-2 3-9 4-3 5-10 6-10 7-3 8-12 9-12 10-12 11-5 12-6 13-7 14-8
Inverse: 1-1 2-2 8-3 9-4 10-5 12-6 13-7 14-8 10-9 10-10 10-11 10-12

i1 declare2 resume+3

d4 the5 session6

of7 the8 european9 parliament10 adjourn+11

ed12 on13 1314 december15 199616

(b)

- julist+ a+ n euroopa+ n parlament+ in perjantai+ n+ a 13 joulukuu+ ta 1996 keskeyty+ neen istunto+ kauden uude+ lle+ en avatuksi
1   2       3   4        5          6          7           8        9          10 11 12      13        14   15         16            17        18            19        20       21  22       23

Direct: 1-23 2-23 3-23 4-23 5-22 6-23 7-22 8-6 9-5 10-7 11-16 12-16 13-9 14-12 15-13 16-15
Inverse: 1-1 2-2 2-3 5-4 9-5 8-6 10-7 10-8 11-9 0-10 7-11 14-12 15-13 15-14 16-15 11-16 11-17 11-18 11-19 11-20 11-21 0-22 11-23

745

in Figure 1 guides the grow-diag process in ﬁnd-
ing the remaining correct alignments. For exam-
ple, the addition of CCM links i1–n4 and d4–
lle21 helps to identify declare2–julist2
and resume3–avatuksi23 as admissible align-
ments, which would otherwise be missed.

4 Methodology

Our idea is to enrich the standard IBM Model 4
alignment by modeling closed-class morphemes
(CCMs) more carefully using global statistics and
context. We realize our idea by proposing a four-
step method. First, we take the input parallel cor-
pus and convert it into morphemes before training
the IBM Model 4 morpheme alignment. Second,
from the morpheme alignment, we induce auto-
matically bilingual CCM pairs. The core of our
approach is in the third and fourth steps. In Step 3,
we construct a CCM alignment model, and apply
it on the segmented input corpus to obtain an au-
tomatic CCM alignment. Finally, in Step 4, we in-
corporate the CCM alignment into the symmetriz-
ing process via our modiﬁed grow-diag process.

4.1 Step 1: Morphological Analysis
The ﬁrst step presupposes morphologically seg-
mented input to compute the IBM Model 4 mor-
pheme alignment.
Following Virpioja et al.
(2007), we use Morfessor, an unsupervised an-
alyzer which learns morphological segmentation
from raw tokenized text (Creutz and Lagus, 2007).
The tool segments input words into labeled
morphemes: PRE (preﬁx), STM (stem), and SUF
(sufﬁx). Multiple afﬁxes can be proposed for
each word; word compounding is allowed as well,
e.g., uncarefully is analyzed as un/PRE+
care/STM+ ful/SUF+ ly/SUF. We append a
“+” sign to each nonﬁnal tag to distinguish word-
internal morphemes from word-ﬁnal ones, e.g.,
“x/STM” and “x/STM+” are considered different
tokens. The “+” annotation enables the restoration
of the original words, a key point to enforce word
boundary constraints in our work later.

4.2 Step 2: Bilingual CCM Pairs
We observe that low and highly inﬂected lan-
guages, while intrinsically different, share more

en
the1
-s2
to3
to3
of4
of4
of4
and5
and5
and5

en
me166
me166
why168
view172
still181

ﬁ
-ssa‡15
on‡2
ett¨a‡7
ettei‡283
-mme‡10

ﬁ
-n†1
-t‡9
-¨a6
maan91
-a4
-en†5
-sta†19
ja‡3
emme123
sek¨a‡122 we10 meill¨a†231
eik¨a203

en
in6
is7
that8
that8
we10
we10 meid¨an†52 where183
we10 me‡113
same186
he187
we10
good189
over-408

. . .

. . .

ﬁ
-ni‡60
minun†282
siksi‡187
mielt¨a†162
viel¨a‡108
jossa‡209
samaa‡334
h¨an‡184
hyv¨a‡321
yli-‡391

Table 1: English(en)-Finnish(ﬁ) Bilingual CCM pairs
(N=128). Shown are the top 19 and last 10 of 168 bilingual
CCM pairs extracted. Subscript i indicates the ith most fre-
quent morpheme in each language. ‡ marks exact correspon-
dence linguistically, whereas † suggests rough correspon-
dence w.r.t http://en.wiktionary.org/wiki/.
in common at the morpheme level. The many-
to-one relationships among words on both sides
is often captured better by one-to-one correspon-
dences among morphemes. We wish to model
such bilingual correspondence in terms of closed-
class morphemes (CCM), similar to Nguyen and
Vogel (2008)’s work that removes nonaligned af-
ﬁxes during the alignment process. Let us now
formally deﬁne CCM and an associative measure
to gauge such correspondence.

Deﬁnition 1. Closed-class Morphemes (CCM)
are a ﬁxed set of stems and afﬁxes that ex-
hibit grammatical functions just like closed-class
words. In highly inﬂected languages, we observe
that grammatical meanings may be encoded in
morphological stems and afﬁxes, rather than sep-
arate words. While we cannot formally identify
valid CCMs in a language-independent way (as
by deﬁnition they manifest language-dependent
grammatical functions), we can devise a good ap-
proximation. Following Setiawan et al. (2007),
we induce the set of CCMs for a language as the
top N frequent stems together with all afﬁxes1.

Deﬁnition 2.

Bilingual Normalized PMI
(biPMI) is the averaged normalized PMI com-
puted on the asymmetric morpheme alignments.
Here, normalized PMI (Bouma, 2009), known to
be less biased towards low-frequency data, is de-
ﬁned as: nPMI(x, y) = ln p(x,y)
p(x)p(y) )/- ln p(x, y),
where p(x), p(y), and p(x, y) follow deﬁnitions
in the standard PMI formula. In our case, we only
1Note that we employ length and vowel sequence heuris-

tics to ﬁlter out corpus-speciﬁc morphemes.

746

compute the scores for x, y being morphemes fre-
quently aligned in both asymmetric alignments.

Given these deﬁnitions, we now consider a pair
of source and target CCMs related and termed a
bilingual CCM pair (CCM pair, for short) if they
exhibit positive correlation in their occurrences
(i.e., positive nPMI2 and frequent cooccurrences).
We should note that relying on a hard thresh-
old of N as in (Setiawan et al., 2007) is brittle
as the CCM set varies in sizes across languages.
Our method is superior in the use of N as a start-
ing point only; the bilingual correspondence of the
two languages will ascertain the ﬁnal CCM sets.
Take for example the en and ﬁ CCM sets with
154 and 214 morphemes initially (each consist-
ing of N=128 stems). As morphemes not having
their counterparts in the other language are spu-
rious, we remove them by retaining only those in
the CCM pairs. This effectively reduces the re-
spective sizes to 91 and 114. At the same time,
these ﬁnal CCMs cover a much larger range of top
frequent morphemes than N, up to 408 en and 391
ﬁ morphemes, as evidenced in Table 1.

4.3 Step 3: The CCM Alignment Model
The goal of this model is to predict when appear-
ances of a CCM pair should be deemed as linking.
With an identiﬁed set of CCM pairs, we know
when source and target morphemes correspond.
However, in a sentence pair there can be many in-
stances of both the source and target morphemes.
In our example, the the–n pair corresponds to
deﬁnite nouns; there are two the and three -n in-
stances, yielding 2 × 3=6 possible links.
Deciding which instances are aligned is a deci-
sion problem. To solve this, we inspect the IBM
Model 4 morpheme alignment to construct a CCM
alignment model. The CCM model labels whether
an instance of a CCM pair is deemed semantically
related (linked). We cast the modeling problem as
supervised learning, where we choose a maximum
entropy (ME) formulation (Berger et al., 1996).

We ﬁrst discuss sample selection from the IBM
Model 4 morpheme alignment, and then give de-
tails on the features extracted. The processes de-
scribed below are done per sentence pair with f m
1 ,
2nPMI has a bounded range of [−1, 1] with values 1 and
0 indicating perfect positive and no correlation, respectively.

1 and U denoting the source, target sentences and
en
the union alignments, respectively.

Class labels. We base this on the initial IBM
Model 4 alignment to label each CCM pair in-
stance as a positive or negative example: Positive
examples are simply CCM pairs in U. To be pre-
cise, links j–i in U are positive examples if fj–ei
is a CCM pair. To ﬁnd negative examples, we in-
ventory other potential links that share the same
lexical items with a positive one. That is, a link
j0–i0 not in U is a negative example, if a positive
link j–i such that fj = f0j and ei = e0i exists.

We stress that our collection of positive exam-
ples contains high-precision but low-recall IBM
Model 4 links, which connect the reliable CCM
pairs identiﬁed before. The model then general-
izes from these samples to detect incorrect CCM
links and to recover the correct ones, enhancing
recall. We later detail this process in §4.4.
Feature Set. Given a CCM pair instance, we
construct three feature types:
lexical, monolin-
gual, and bilingual (See Table 2). These features
capture the global statistics and contexts of CCM
pairs to decide if they are true alignment links.

• Lexical features reﬂect the tendency of the
CCM pair being aligned to themselves. We use
biPMI, which aggregates the global alignment
statistics, to determine how likely source and tar-
get CCMs are associated with each other.

• Monolingual context features measure the
association among tokens of the same language,
capturing what other stems and afﬁxes co-occur
with the source/target CCM:

1. within the same word (intra). The aim is to
disambiguate afﬁxes as necessary in highly
inﬂected languages where same stems could
generate different roles or meanings.

2. outside the CCM’s word boundary (inter).
This potentially capture cues such as tense,
or number agreement. For example, in En-
glish, the 3sg agreement marker on verbs -s
often co-occurs with nearby pronouns e.g.,
he, she, it; whereas the same marker on
nouns (-s), often appears with plural deter-
miners e.g., these, those, many.

To accomplish this, we compute two monolin-
gual nPMI scores in the same spirit as biPMI, but
using the morphologically segmented input from

747

Feature Description

Lexical — biPMI: None [−1, 0], Low (0, 1/3], Medium (1/3, 2/3], High (2/3, 1]
Monolingual Context — Capture morpheme cooccurrence with the src/tgt CCM
Intra – Within the same word
Inter – To the Left & Right, in different words
Bilingual context — Capture neighbor links’ cooccurrence with the CCM pair link
bi0 – Most descriptive, capturing in terms of surface forms only → maybe sparse
bi1 – Generalizes morphemes into relative locations (Left, Within, Right)
bi2 – Most general, coupling token types (Close, Open) /w relative positions

Examples

pmid−lle=Low
srcWd−lle=resume, tgtWd−lle=en, tgtWd−lle=uude
srcLd−lle=i, srcRd−lle=the, tgtRd−lle=avatuksi
bi0d−lle=resume–avatuksi
bi1d−lle=W–avatuksi, bi1d−lle=resume–R
bi2d−lle=O–WR

Table 2: Maximum entropy feature set. Shown are feature types, descriptions and examples. Most examples are given for
the alignment d4–lle+21 of the same running example in §3. Note that we only partially list the bilingual context features.
to determine which links are kept and which are
each language separately. Two morphemes are
“linked” if within a context window of wc words.
dropped to satisfy our assumption.
In our case,
• Bilingual context features model cross-
we pick the most conﬁdent link, d4–lle21 with
probability 0.99. This precludes the incorrect link,
lingual reordering, capturing the relationships be-
tween the CCM pair link and its neighbor3 links.
ed12–lle21, but admits the other correct one
ed12–neen17, probability 0.93. As a result, this
Consider a simple translation between an English
resolution successfully removes the incorrect link.
phrase of the form we hverbi and the Finnish
Modifying grow-diag. We incorporate the set
one hverbi -mme, where -mme is the 1pl verb
marker. We aim to capture movements such as
of conﬂict-resolved CCM links into the grow-diag
“the open-class morphemes on the right of we and
process. This step modiﬁes the input alignments
on the left of -mme are often aligned”. These will
as well as the growing process. U and I denote the
function as evidence for the ME learner to align
IBM Model 4 union and intersection alignments.
the CCM pair (we, -mme). We encode the bilin-
In our view, the resolved CCM links can serve
gual context at three different granularities, from
as a quality mark to “upgrade” links before input
most speciﬁc to most general ones (cf Table 2).
into the grow-diag process. We upgrade resolved
CCM links: (a) those ∈ U → part of I, treating
them as alignment seeds; (b) those /∈ U → part
of U, using them for exploration and growing. To
reduce spurious alignments, we discarded links in
U that conﬂict with the resolved CCM links.

4.4 Step 4: Incorporate CCM Alignment
At test time, we apply the trained CCM alignment
model to all CCM pairs occurring in each sentence
pair to ﬁnd CCM links. On our running exam-
ple in Figure 1, the CCM classiﬁer tests 17 CCM
pairs, identifying 6 positive CCM links of which
4 are true positives (dotted lines in (b)).

Though mostly correct, we note that some of
the predicted links conﬂict:
(d4–lle21, ed12–
neen17 and ed12–lle21) share alignment end-
points. Such sharing in CCM alignments is rare
and we believe should be disallowed. This moti-
vates us to resolve all CCM link conﬂicts before
incorporating them into the symmetrizing process.
Resolving link conﬂicts. As CCM pairs are
classiﬁed independently, they possess classiﬁca-
tion probabilities which we use as evidence to re-
solve the conﬂicts. In our example, the classiﬁca-
tion probabilities for (d4–lle21, ed12–neen17,
ed12–lle21) are (0.99, 0.93, 0.79) respectively.
We use a simple, “best-ﬁrst” greedy approach

3Within a context window of wc words as in monolingual.

In the usual grow-diag, links immediately adja-
cent to a seed link l are considered candidates to
be appended into the alignment seeds. While suit-
able for word-based alignment, we believe it is too
small a context when the input are morphemes.

For morpheme alignment, the candidate context
makes more sense in terms of word units. We thus
enforce word boundaries in our modiﬁed grow-
diag. We derive word boundaries for end points in
l using the morphological tags and the “+” word-
end marker mentioned in §4.1. Using such bound-
aries, we can then extend the grow-diag to con-
sider candidate links within a neighborhood of wg
words; hence, enhancing the candidate coverage.

5 Experiments

We use English-Finnish and English-Hungarian
data from past shared tasks (WPT05 and WMT09)

748

to validate our approach. Both Finnish and Hun-
garian are highly inﬂected languages, with numer-
ous verbal and nominal cases, exhibiting agree-
ment. Dataset statistics are given in Table 3.

en-ﬁ

#

en-hu

Europarl-v1

Train
714K
LM Europarl-v1-ﬁ 714K
2000
Dev
Test
2000

wpt05-dev
wpt05-test

Europarl-v4

News-hu

news-dev2009
news-test2009

#

1,510K
4,209K
2051
3027

Table 3: Dataset Statistics: the numbers of parallel sen-
tences for training, LM training, development and test sets.
We use the Moses SMT framework for our
work, creating both our CCM-based systems and
the baselines.
In all systems built, we obtain
the IBM Model 4 alignment via GIZA++ (Och
and Ney, 2003). Results are reported using case-
insensitive BLEU (Papineni et al., 2001).

Baselines. We build two SMT baselines:
w-system: This is a standard phrase-based
SMT, which operates at the word level. The sys-
tem extracts phrases of maximum length 7 words,
and uses a 4-gram word-based LM.

wm-system: This baseline works at the word
level just like the w-system, but differs at the
alignment stage. Speciﬁcally, input to the IBM
Model 4 training is the morpheme-level corpus,
segmented by Morfessor and augmented with “+”
to provide word-boundary information (§4.1). Us-
ing such information, we constrain the alignment
symmetrization to extract phrase pairs of 7 words
or less in length. The morpheme-based phrase ta-
ble is then mapped back to word forms. The pro-
cess continues identically as in the w-system.

CCM-based systems. Our CCM-based sys-
tems are similar in spirit to the wm system: train at
the morpheme, but decode at the word level. We
further enhance the wm-system at the alignment
stage. First, we train our CCM model based on
the initial IBM Model 4 morpheme alignment, and
apply it to the morpheme corpus to obtain CCM
alignment, which are input to our modiﬁed grow-
diag process. The CCM approach deﬁnes the set-
ting of three parameters: hN, wc, wgi (Section 4).
Due to our resource constraints, we set N=128,
similar to (Setiawan et al., 2007), and wc=1 ex-
perimentally. We only focus on the choice of wg,
testing wg={1, 2} to explore the effect of enforc-
ing word boundaries in the grow-diag process.

5.1 English-Finnish results
We test the translation quality of both directions
(en-ﬁ) and (ﬁ-en). We present results in Table 4 for
7 systems, including: our baselines, three CCM-
based systems with word-boundary knowledge
wg={0, 1, 2} and two wm-systems wg={1, 2}.
Results in Table 4 show that our CCM approach
effectively improves the performance. Compared
to the wm-system, it chalks up a gain of 0.46
BLEU points for en-ﬁ, and a larger improvement
of 0.93 points for the easier, reverse direction.

Further using word boundary knowledge in our
modiﬁed grow-diag process demonstrates that the
additional ﬂexibility consistently enhances BLEU
for wg = 1, 2. We achieve the best performance
at wg = 2 with improvements of 0.67 and 1.22
BLEU points for en-ﬁ and ﬁ-en, respectively.

w-system
wm-system
wm-system + CCM
wm-system + CCM + wg = 1
wm-system + CCM + wg = 2
wm-system + wg = 1
wm-system + wg = 2
(Ganchev, 2008) - Base
(Ganchev, 2008) - Postcat
(Yang, 2006) - Base
(Yang, 2006) - Backoff

en-ﬁ
14.58
14.47
14.93+0.46
15.01
15.14+0.67
14.44
14.28
14.72
14.74
N/A
N/A

ﬁ-en
23.56
22.89
23.82+0.93
23.95
24.11+1.22
22.92
23.01
22.78
23.43+0.65
22.0
22.3+0.3

Table 4: English/Finnish results. Shown are BLEU
scores (in %) with subscripts indicating absolute improve-
ments with respect to the wm-system baseline.

Interestingly, employing the word boundary
heuristic alone in the original grow-diag does not
yield any improvement for en-ﬁ, and even worsens
as wg is enlarged (as seen in Rows 6–7). There
are only slight improvements for ﬁ-en with larger
wg.This attests to the importance of combining the
CCM model and the modiﬁed grow-diag process.
Our best system outperforms the w-system
baseline by 0.56 BLEU points for en-ﬁ, and yields
an improvement of 0.55 points for ﬁ-en.

Compared to works experimenting en/ﬁ trans-
lation, we note the two prominent ones by Yang
and Kirchhoff (2006) and recently by Ganchev
et al. (2008). The former uses a simple back-off
method experimenting only ﬁ-en, yielding an im-
provement of 0.3 BLEU points. Work in the op-

749

posite direction (en-ﬁ) is rare, with the latter pa-
per extending the EM algorithm using posterior
constraints, but showing no improvement; for ﬁ-
en, they demonstrate a gain of 0.65 points. Our
CCM method compares favorably against both ap-
proaches, which use the same datasets as ours.

5.2 English-Hungarian results
To validate our CCM method as language-
independent, we also perform preliminary exper-
iments on en-hu. Table 5 shows the results using
the same CCM setting and experimental schemes
as in en/ﬁ. An improvement of 0.35 BLEU points
is shown using the CCM model. We further im-
prove by 0.44 points with word boundary wg=1,
but performance degrades for the larger window.
Due to time constraints, we leave experiments
for the reverse, easier direction as future work.
Though modest, the best improvement for en-hu
is statistical signiﬁcant at p<0.01 according to
Collins’ sign test (Collins et al., 2005).

System
w-system
wm-system
wm-system + CCM
wm-system + CCM + wg = 1
wm-system + CCM + wg = 2

BLEU
9.63
9.47
9.82 +0.35
9.91 +0.44
9.87

Table 5: English/Hungarian results. Subscripts indicate
absolute improvements with respect to the wm-system.

We note that MT experiments for en/hu 4 are
very limited, especially for the en to hu direction.
Nov´ak (2009) obtained an improvement of 0.22
BLEU with no distortion penalty; whereas Koehn
and Haddow (2009) enhanced by 0.5 points us-
ing monotone-at-punctuation reordering, mini-
mum Bayes risk and larger beam size decoding.

While not directly comparable in the exact set-
tings, these systems share the same data source
and splits similar to ours. In view of these com-
munity results, we conclude that our CCM model
does perform competitively in the en-hu task, and
indeed seems to be language independent.

6 Detailed Analysis
The macroscopic evaluation validates our ap-
proach as improving BLEU over both baselines,
4Hungarian was used in the ACL shared task 2008, 2009.

but how do the various components contribute?
We ﬁrst analyze the effects of Step 4 in produc-
ing the CCM alignment, and then step backward
to examine the contribution of the different feature
classes in Step 3 towards the ME model.

6.1 Quality of CCM alignment
To evaluate the quality of the predicted CCM
alignment, we address the following questions:

Q1: What is the portion of CCM pairs being

misaligned in the IBM Model 4 alignment?

Q2: How does the CCM alignment differ from

the IBM Model 4 alignment?

Q3: To what extent do the new links introduced

by our CCM model address Q1?

Given that we do not have linguistic expertise in
Finnish or Hungarian, it is not possible to exhaus-
tively list all misaligned CCM pairs in the IBM
Model 4 alignment. As such, we need to ﬁnd other
form of approximation in order to address Q1.

We observe that correct links that do not exist
in the original alignment could be entirely miss-
ing, or mistakenly aligned to neighboring words.
With morpheme input, we can also classify mis-
takes with respect to intra- or inter-word errors.
Figure 2 characterizes errors T1, T2 and T3, each
being a more severe error class than the previous.
Focusing on ei in the ﬁgure, links connecting ei
to fj0 or fj00 are deemed T1 errors (misalignments
happen on one side). A T2 error aligns f00j within
the same word, while a T3 error aligns it outside
the current word but still within its neighborhood.
This characterization is automatic, cheap and has
the advantage of being language-independent.

Figure 2: Categorization of CCM missing links. Given
that a CCM pair link (fj–ei) is not present in the IBM Model
4, occurrences of any nearby link of the types T[1−3] can be
construed as evidence of a potential misalignment.

Statistics in Table 6(ii) answers Q1, suggest-
ing a fairly large number of missing CCM links:
3, 418K for en/ﬁ and 6, 216K for en/hu, about
12.35% and 12.06% of the IBM Model 4 union
alignment respectively. We see that T1 errors con-

1 word

1 word

fj

ei

fj'

ei'

fj’’

ei’’

T1
T2
T3

750

stitute the majority, a reasonable reﬂection of the
garbage- collector5 effect discussed in Section 3.

General (i)

en/ﬁ

en/hu

Direct
Inverse
D ∩ I
D ∪ I

17,632K 34,312K T1
18,681K 34,676K T2
8,643K 17,441K T3
27,670K 51,547K Total

Missing CCM links (ii)
en/ﬁ
en/hu
2,215K 3,487K
690K
358K
845K
2,039K
3,418K 6,216K

Table 6: IBM Model 4 alignment statistics. (i) General
statistics. (ii) Potentially missing CCM links.

Q2 is addressed by the last column in Ta-
ble 7. Our CCM model produces about 11.98%
(1,035K/8,643K) new CCM links as compared to
the size of the IBM Model 4 intersection align-
ment for en/ﬁ, and similarly, 9.52% for en/hu.

Orig.
5,299K
9,425K

Resolved
3,433K
6,558K

en/ﬁ
en/hu

I

U\I

New
1065K 1,332K 1,035K
2,752K 2,146K 1,660K

Table 7: CCM vs IBM Model 4 alignments. Orig. and
Resolved give # CCM links predicted in Step 4 before and
after resolving conﬂicts. Also shown are the number of re-
solved links present in the Intersection, Union excluding I
(U\I) of the IBM Model 4 alignment and New CCM links.
Lastly, ﬁgures in Table 8 answer Q3, revealing
that for en/ﬁ, 91.11% (943K/1,035K) of the new
CCM links effectively cover the missing CCM
alignments, recovering 27.59% (943K/3,418K) of
all missing CCM links. Our modiﬁed grow-diag
realizes a majority 76.56% (722K/943K) of these
links in the ﬁnal alignment.

We obtain similar results in the en/hu pair for
link recovery, but a smaller percentage 22.59%
(330K/1,461K) are realized through the modiﬁed
symmetrization. This partially explains why im-
provements are modest for en/hu.

New CCM Links (i) Modiﬁed grow-diag (ii)
en/ﬁ
707K
108K
128K
943K

en/hu
1,002K
146K
313K
1,461K

en/hu
228K
22K
80K
330K

en/ﬁ
547K
79K
96K
722K

T1
T2
T3
Total

Table 8: Quality of the newly introduced CCM links.
Shown are # new CCM links addressing the three error types
before (i) and after (ii) the modiﬁed grow-diag process.
6.2 Contributions of ME Feature Classes
We also evaluate the effectiveness the ME features
individually through ablation tests. For brevity,

5E.g., ei prefers f0j or f00j (garbage collectors) over fj.

we only examine the more difﬁcult translation di-
rection, en to ﬁ. Results in Table 9 suggest that
all our features are effective, and that removing
any feature class degrades performance. Balanc-
ing speciﬁcity and generality, bi1 is the most
inﬂuential feature in the bilingual context group.
For monolingual context, inter, which captures
larger monolingual context, outperforms intra.
The most important feature overall is pmi, which
captures global alignment preferences. As feature
groups, bilingual and monolingual context fea-
tures are important sources of information, as re-
moving them drastically decreases system perfor-
mance by 0.23 and 0.16 BLEU, respectively.

System
all (wm-system+CCM)
−bi2
−bi1
−bi0
−inter

14.90
−intra
14.84∗−0.09 −pmi
−bi{2/1/0}
14.89
−in{ter/tra}
14.85

BLEU
14.93
14.89
14.81∗−0.12
14.70∗−0.23
14.77∗−0.16

Table 9: ME feature ablation tests for English-Finnish
experiments. ∗ mark results statistically signiﬁcant at p <
0.05, differences are subscripted.

7 Conclusion and Future Work

In this work, we have proposed a language-
independent model
that addresses morpheme
alignment problems involving highly inﬂected
languages. Our method is unsupervised, requiring
no language speciﬁc information or resources, yet
its improvement on BLEU is comparable to much
semantically richer, language-speciﬁc work. As
our approach deals only with input word align-
ment, any downstream modiﬁcations of the trans-
lation model also beneﬁt.

As alignment is a central focus in this work, we
plan to extend our work over different and mul-
tiple input alignments. We also feel that better
methods for the incorporation of CCM alignments
is an area for improvement. In the en/hu pair, a
large proportion of discovered CCM links are dis-
carded, in favor of spurious links from the union
alignment. Automatic estimation of the correct-
ness of our CCM alignments may improve end
translation quality over our heuristic method.

751

References
Berger, Adam L., Stephen D. Della Pietra, and Vin-
cent J. D. Della Pietra. 1996. A maximum entropy
approach to natural language processing. Computa-
tional Linguistics, 22(1):39–71.

Bouma, Gerlof. 2009. Normalized (pointwise) mutual
information in collocation extraction. In Proceed-
ings of the Biennial GSCL Conference, T¨ubingen,
Gunter Narr Verlag.

Cherry, Colin and Dekang Lin. 2007. Inversion trans-
duction grammar for joint phrasal translation mod-
eling. In SSST.

Collins, Michael, Philipp Koehn, and Ivona Kuˇcerov´a.
2005. Clause restructuring for statistical machine
translation. In ACL.

Creutz, Mathias and Krista Lagus. 2007. Unsuper-
vised models for morpheme segmentation and mor-
phology learning. ACM Trans. Speech Lang. Pro-
cess., 4(1):3.

DeNero, John and Dan Klein. 2007. Tailoring word
In

alignments to syntactic machine translation.
ACL.

Ganchev, Kuzman, Jo˜ao V. Grac¸a, and Ben Taskar.
2008. Better alignments = better translations? In
ACL-HLT.

Goldwater, Sharon and David McClosky. 2005. Im-
proving statistical mt through morphological analy-
sis. In HLT.

Moore, Robert C.

2004.

Improving IBM word-

alignment model 1. In ACL.

Nguyen, Thuy Linh and Stephan Vogel.

2008.
Context-based Arabic morphological analysis for
machine translation. In CoNLL.

Nov´ak, Attila. 2009. MorphoLogic’s submission for

the WMT 2009 shared task. In EACL.

Och, Franz Josef and Hermann Ney. 2003. A sys-
tematic comparison of various statistical alignment
models. Computational Linguistics, 29(1):19–51.

Och, Franz Josef and Hermann Ney. 2004. The align-
ment template approach to statistical machine trans-
lation. Computational Linguistics, 30(4):417–449.

Papineni, Kishore, Salim Roukos, Todd Ward, and
Wei-Jing Zhu. 2001. Bleu: a method for automatic
evaluation of machine translation. In ACL ’02: Pro-
ceedings of the 40th Annual Meeting on Associa-
tion for Computational Linguistics, pages 311–318,
Morristown, NJ, USA. Association for Computa-
tional Linguistics.

Setiawan, Hendra, Min-Yen Kan, and Haizhou Li.
In

2007. Ordering phrases with function words.
ACL.

Virpioja, Sami, Jaakko J. Vyrynen, Mathias Creutz,
and Markus Sadeniemi. 2007. Morphology-aware
statistical machine translation based on morphs in-
duced in an unsupervised manner.
In MT Summit
XI.

Haghighi, Aria, John Blitzer, John DeNero, and Dan
Klein. 2009. Better word alignments with super-
vised itg models. In ACL.

Yang, Mei and Katrin Kirchhoff. 2006. Phrase-based
backoff models for machine translation of highly in-
ﬂected languages. In EACL.

Zhang, Hao and Daniel Gildea. 2005. Stochastic lex-
icalized inversion transduction grammar for align-
ment. In ACL.

Zhang, Hao, Chris Quirk, Robert C. Moore, and
Daniel Gildea. 2008. Bayesian learning of non-
compositional phrases with synchronous parsing. In
ACL-HLT.

Koehn, Philipp and Barry Haddow. 2009. Edinburgh’s
submission to all tracks of the WMT2009 shared
task with reordering and speed improvements to
Moses. In EACL.

Koehn, Philipp, Hieu Hoang, Alexandra Birch Mayne,
Christopher Callison-Burch, Marcello Federico,
Nicola Bertoldi, Brooke Cowan, Wade Shen, Chris-
tine Moran, Richard Zens, Chris Dyer, Ondrej Bo-
jar, Alexandra Constantin, and Evan Herbst. 2007.
Moses: Open source toolkit for statistical machine
translation. In ACL, Demonstration Session.

Lee, Young-Suk. 2004. Morphological analysis for

statistical machine translation. In HLT-NAACL.

Liang, Percy, Ben Taskar, and Dan Klein.
Alignment by agreement. In HLT-NAACL.

2006.

Matusov, Evgeny, Richard Zens, and Hermann Ney.
2004. Symmetric word alignments for statistical
machine translation. In COLING.

