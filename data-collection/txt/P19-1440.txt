
























































Complex_Question_Decomposition_for_Semantic_Parsing__ACL_19.pdf


Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 4477–4486
Florence, Italy, July 28 - August 2, 2019. c©2019 Association for Computational Linguistics

4477

Complex Question Decomposition for Semantic Parsing

Haoyu Zhang†, Jingjing Cai‡, Jianjun Xu†, Ji Wang†
†HPCL, College of Computer, National University of Defense Technology, China

‡Meituan-Dianping Group, China
{zhanghaoyu10, jjxu, wj}@nudt.edu.cn

caijingjing02@meituan.com

Abstract

In this work, we focus on complex question
semantic parsing and propose a novel Hierar-
chical Semantic Parsing (HSP) method, which
utilizes the decompositionality of complex
questions for semantic parsing. Our model
is designed within a three-stage parsing archi-
tecture based on the idea of decomposition-
integration. In the first stage, we propose
a question decomposer which decomposes a
complex question into a sequence of sub-
questions. In the second stage, we design
an information extractor to derive the type
and predicate information of these questions.
In the last stage, we integrate the generated
information from previous stages and gener-
ate a logical form for the complex question.
We conduct experiments on COMPLEXWE-
BQUESTIONS which is a large scale com-
plex question semantic parsing dataset, results
show that our model achieves significant im-
provement compared to state-of-the-art meth-
ods.

1 Introduction

Semantic parsing is a task which maps natural
language utterances into logical forms such as
SQL queries that can be executed based on re-
lational databases or knowledge bases directly.
Semantic parsing is a long-standing and difficult
problem in natural language processing. In re-
cent studies, researchers usually treat natural lan-
guage descriptions/questions as input and use dif-
ferent sequence-to-sequence frameworks to gener-
ate logical forms (Xu et al., 2017; Dong and Lap-
ata, 2016). However, these methods ignore the de-
compositionality of a complex question which is
usually composed of a set of sub-questions, the un-
derstanding of each sub-question could contribute
to the semantic parsing of the original complex
question.

Figure 1 gives an example of a complex ques-
tion and its logical form. The related sub-
questions in stage-1 and the corresponding pred-
icate (relation) information of each sub-question
in stage-2 could help to obtain the logical form of
the complex question in stage-3.

Stage 1 :

Stage 2 :

Stage 3 :

When was Obama’s daughter born?

Who is Obama’s daughter? When was #entity# born?

people.person.children people.person.date_of_birth

Question:

QD

IE

SP

Figure 1: Example of question decomposition(QD), in-
formation extraction(IE) and semantic parsing(SP).

Question decomposition is important and many
previous work utilize the decompositionality of
complex questions to help question understand-
ing. Kalyanpur et al. (2012) propose to use a suite
of decomposition rules for question decomposi-
tion. The drawback of rule-based methods is that
it needs experts to design rules and the rules are
usually with low coverage and hard to be extended
to other domains and tasks. Talmor and Berant
(2018) propose a neural question decomposition
approach to answer complex questions. They use
the pointer network (Vinyals et al., 2015) to gen-
erate splitting points in the complex question and
separate the complex question into a sequence of
simple questions. This neural-based method alle-
viates the cost of manually designed rules or fea-
tures. However, sometimes decomposing a com-
plex question by splitting points may not find best
sub-questions, and thus lose some information.
For example, the sub-question “Who is Obama’s
daughter?’ can not be generated by the splitting
points of the complex question in Figure 1. To
address the above problem, we propose to use a
more flexible neural generative question decom-



4478

poser to directly generate complete and natural
sub-questions based on an input complex question,
without word order and content restrictions.

To parse a complex question into its corre-
sponding logical form, we propose a hierarchical
semantic parsing (HSP) model which is designed
as a hierarchical neural sequence-to-sequence ar-
chitecture. The underline idea of our HSP model
is decomposition and integration. Specifically, as
shown in Figure 1, our HSP model first decom-
poses a complex question into sub-question se-
quence with a question decomposer (QD), and
then extracts key semantic information based on
the generated sub-questions and the original com-
plex question with an information extractor (IE).
Finally, HSP model integrates the previously gen-
erated auxiliary information and generates the log-
ical form of the complex question. Our HSP
model can be seen as a multi-stage reasoning pro-
cess, with each stage focusing on different level of
information and reducing the search space of log-
ical forms step-by-step by integrating previously
generated information.

The main contributions of this paper are three-
fold:

1. We propose an effective and flexible question
decomposition method;

2. We propose a hierarchical semantic pars-
ing model based on a sequence-to-sequence
paradigm which incorporates a question de-
composer and an information extractor;

3. Experimental results demonstrate that the
proposed model achieves a significant im-
provement in semantic parsing performance.

2 Related Work

2.1 Semantic Parsing

Typically, traditional semantic parsing models
(Zettlemoyer and Collins, 2005; Mooney, 2007;
Liang et al., 2011; Cai and Yates, 2013; Artzi et al.,
2013; Kwiatkowski et al., 2013; Berant et al.,
2014; Yih et al., 2015; Yao, 2015) are learned
based on carefully designed features. For instance,
Kwiatkowski et al. (2011) propose a combinatory
categorical grammar induction technique for se-
mantic parsing with different levels of features.
Liang et al. (2011); Reddy et al. (2014) build se-
mantic parsers without relying on logical form an-
notations but through distant supervision. Xiao

et al. (2016); Yin and Neubig (2017) use syntax
information to improve semantic parsing models.
Fan et al. (2017) apply a transfer learning method
in semantic parsing. To alleviate the cost of feature
engineering, neural semantic parsing approaches
have attracted significant attention (Jia and Liang,
2016; Dong and Lapata, 2016; Herzig and Berant,
2017; Gardner et al., 2018; Goldman et al., 2018;
Chen et al., 2018; Dong et al., 2018; Dong and
Lapata, 2018). For example, Jia and Liang (2016)
propose a framework to introduce data recombi-
nation and train a sequence-to-sequence model for
semantic parsing. Dong and Lapata (2018) pro-
pose to firstly parse a question to a coarse logi-
cal form then a fine-grained one based on a neu-
ral architecture. However, these approaches miss
the opportunity to utilize question decomposition
information for complex question semantic pars-
ing. In this work, we leverage a sequence-to-
sequence architecture and design a neural hierar-
chical sequence-to-sequence model to capture the
syntactic structure, e.g., question decomposition
information of complex questions.

2.2 Question Decomposition

Question decomposition has been successfully
used in complex question answering (Kalyanpur
et al., 2012; Iyyer et al., 2016; Talmor and Be-
rant, 2018; Song et al., 2018). Kalyanpur et al.
(2012) propose a framework using decomposi-
tion rules to identify facts in complex questions
based on lexicon-syntactic features. The model
then leverages the identified facts alone with a
question rewriting component and a candidate re-
ranker to generate final ranked answer list. Their
work rely on feature engineering and manually de-
signed rules which is difficult to be adapted to ap-
plications in other domains. Iyyer et al. (2016)
propose a method for complex question answer-
ing based on tables. To answer complex ques-
tions, they split each complex question into several
inter-related simple questions by crowd-sourcing,
and design an end-to-end neural model to predict
the answer based on the simple questions. Talmor
and Berant (2018) propose a splitting-based ques-
tion decomposition model to find splitting points
in the original complex question and decompose
it into a sequence of sub-questions. They then
use a machine reading comprehension method to
get the answers of each sub-question and com-
pose the answers to obtain answer of the complex



4479

Question When was Obama’s daughter born?
DR Who is Obama’s daughter? # When was #entity# born?
SR composition # people.person.children # people.person.date of birth

Table 1: Example of intermediate representations, #entity# in one sub-question represents a placeholder, and the
real word is filled in by the answer of another sub-question.

question. Sub-questions obtained by this method
are usually incomplete. In this work, we pro-
pose a neural generative question decomposition
approach to directly generate complete and natu-
ral sub-questions, which also improves the perfor-
mance of complex question semantic parsing.

3 Model

In this section, we introduce the architecture of
our hierarchical semantic parsing model. The
model receives complex question inputs and gen-
erates logical forms. It combines the sequence-
to-sequence paradigm with a hierarchical parsing
mechanism in a differentiable way and can be
trained end-to-end.

3.1 Model Overview

Our model treats complex questions and logical
forms as sequences, learns to generate logical
forms for questions. We denote a complex ques-
tion as x = {x1, · · · , x|x|}, and logical form as
y = {y1, · · · , y|y|}.

To better model and generate logical forms,
our model utilizes two types of intermediate rep-
resentations: the decomposed representation(DR)
and the semantic representation(SR). DR consists
of decomposed simple questions and SR contains
key information of the original complex ques-
tion including question type and all predicates
in the question. An example of the two inter-
mediate representations format is shown in Ta-
ble 1. Decomposed representation is denoted as
z = {z1, · · · , z|z|} and semantic representation is
denoted as w = {w1, · · · , w|w|}. Each training
sample is a < x, y, z, w > quad.

3.2 Basic Architecture

First we illustrate the basic structure of our model:
a parsing unit. A parsing unit consists of an
encoder network and a decoder network, based
on the multi-head attention encoder/decoder of
Transformer (Vaswani et al., 2017). Its input has
two parts: the input sequence and additional in-
formation, and the output is the parsed target se-

quence. The input sequence and target sequence
are text utterances, and additional information is
a sequence of vectors representing encoding for
some kind of auxiliary information.

In this subsection, we represent the in-
put sequence of the paring unit as a =
{a1, · · · , a|a|}, input additional information as e =
{e1, · · · , e|e|}, ei ∈ Rn, and output sequence as
o = {o1, · · · , o|o|}.
3.2.1 Encoder
On the encoder side, the parsing unit encodes the
input sequence a to context aware representation
h = {h1, · · · , h|a|}, hi ∈ Rm. We introduce the
Transformer encoder (Vaswani et al., 2017) here.

The encoder first maps the sequence to word
representations and then generates the output us-
ing a L layer Transformer encoder. The total pro-
cess is denoted by:

h = fenc(a) = f
proc
enc (f

emb
enc (a)) (1)

3.2.2 Decoder
The decoder receives encoder output h and input
additional information e, first fuses the two en-
coded representations by concatenating them to
get fused representation [h, e].

At decoder time step t, with fused represen-
tation [h, e] and previous decoded output o<t =
{o1, · · · , ot−1}, decoder calculates conditional
probability P (ot|o<t, [h, e]).

First decoder embedding function f embdec maps
previous decoder outputs o<t to word embeddings
and add positional encoding to get decoder word
representations. Like the encoder, decoder also
stacks L identical layer and the word representa-
tions are then fed to these layers along with fused
representation [h, e]. If we represent the l-th layer
output vector of position j as klj and represent l-th
layer previous output as kl≤j = {kl1, · · · , klj}, the
decoder layer output is klj = Layer(k

l−1
≤j , [h, e]).

Given the last layer output kLj , the probability
of current word P jvocab(w) on target vocabulary



4480

Stage 1 Stage 2 Stage 3

Figure 2: Model Overview, L1, L2, L3 represent length of the corresponding decoder’s output, and N repre-
sents the decoder layer number. Yellow rectangles denote input and output sequence, orange rectangles denote
intermediate output utterances, and gray ones are encoded representations. DR and SR represent decomposed
representation and semantic representation respectively.

V = {w1, · · · , w|v|} is calculated as the following
equation, where Wo, bo is parameters:

P jvocab(w) = Softmax(Wo · aLj + bo) (2)
The decode process is triggered with the start

of sequence token “[BOS]” and terminated on the
end of sequence token “[EOS]”.

3.2.3 Copy Mechanism
To tackle out-of-vocabulary words, we incorporate
copy mechanism (Gu et al., 2016) in the decoder.

At decode time step t, first we calculate the at-
tention distribution over source sequence a using
the bilinear dot product of last layer decoder out-
put kLt and encoder output h, as Eq. 3 4 shows.

uit = k
L
t Wqhi (3)

αt = Softmax(ut) (4)

Then we calculate copy probability P tcopy ∈
[0, 1] as following equation. Wq,Wg, bg are learn-
able parameters:

P tcopy = σ(Wg · [kLt , h, e] + bg) (5)
Using P tcopy we calculate the weighted sum of

copy probability and generation probability to get
the final predicted probability of extended vocab-
ulary V + X , where X is set of out of vocabulary
words in source sequence a:

Pt(w) = (1− P tcopy)Pvocab(w) + P tcopy
∑

i:wi=w

αit

(6)
The decoding process is formulated by Eq. 7.

Note here that we use f tdec to represent one time
step of the decoder with the copy mechanism pro-
cess. For brevity we roll all time steps of the de-
coder, using Eq. 8 to denote P (b|[h, e]).

ot = f
t
dec(f

emb
dec (o<t), [h, e]) (7)

o = fdec([h, e]) (8)

Following is the loss function of the basic archi-
tecture with parameters θ, b∗t is the target word in
time step t:

L(θ) = 1
T

T∑

t=1

− logP (ot = o∗t |a, e, o<t) (9)

3.3 Hierarchical Semantic Parsing
We now introduce HSP based on the above basic
architecture, HSP is a bottom up multi-stage pars-
ing process. Figure 2 illustrates the typical three
stage HSP structure of our model, each stage pro-
cess is similar to the basic architecture we elabo-
rate above.

As illustrated in the model overview subsec-
tion, we denote the input question as x and the



4481

output logical form as y. The train objective of
the basic architecture in Eq. 9 directly minimizes
the cross entropy between conditional probabil-
ity P (y|x) and true probability of target sequence
P (y∗). HSP mechanism turns the process into a
multi-stage process by splitting the objective to
several conditional probabilities’ products. For
our three stage HSP model shown in Figure 2,
the objective is P (y|x, z, w)P (w|x, z)P (z|x), in
which z and w represent decomposed representa-
tion and semantic representation respectively.

3.3.1 Question Decomposer
On the first stage of HSP, we design a question
decomposer to decompose the complex question
to simple question sequences. The input of the
question decomposer is the complex question x,
and the output is the decomposed representation z.
The model first maps the input x to context aware
representations h using the question encoder h =
fenc1(x), at this stage no additional information
is given, so fused representation is identical to
h. Then with a decomposed decoder, the decom-
posed representation is predicted: z = fdec1(h).
In Figure 2 the decoding process is unrolled to
time steps and surrounded by a dotted frame, at
each time step previous outputs are shifted right
and fed into the decoder. The beginning of the blue
line pointing to the decoder is fused representation
used by the decoder, for question decomposer it is
the question embedding.

3.3.2 Information Extractor
The second stage of HSP extracts key information
of complex questions, from the complex question
itself and the decomposed simple questions. The
input sequence of the information extractor is de-
composed representation, additional information
is question embedding, and the target output se-
quence is semantic representation. The encoder
process encodes decomposed representation z us-
ing sub-question encoder: hz = fenc2(z). The
fused representation [h, hz] is then fed into the se-
mantic decoder to decode semantic representation:
w = fdec2([h, h

z]). In Figure 2, the ⊕ notation on
the top denotes the representation fusing process.

3.3.3 Semantic Parser
The final stage of the HSP model is a seman-
tic parser. It receives the context aware embed-
ding of complex question and decomposed repre-
sentation, and semantic representation sequence.

It encodes the semantic representation hw =
fenc3(w), concatenates the three part of represen-
tation [h, hw, hz], and logical form are predicted
conditioned upon the fusing representation: y =
fdec3([h, h

w, hz]).
While the loss function of the basic architec-

ture is as shown in Eq. 9, the training objective
of HSP model is to minimize following loss func-
tions as Eq. 10, where L1 = − logP (z|x), L2 =
− logP (w|x, z) and L3 = − logP (y|x, z, w) de-
notes losses of three stages. λ1, λ2 in the equation
are two hyperparameters.

LHSP (θ) = λ1 · L1 + λ2 · L2 + L3 (10)

During inference, the model uses a three stage
inference process, first getting the prediction of
decomposed representation ẑ = argmaxzP (z|x),
and then predicting semantic representation ŵ =
argmaxwP (w|x, z), finally predicting logical
form ŷ = argmaxyP (y|x, z, w). Each sequence
is obtained using a greedy search method like
beam search.

From a cognitive view, HSP can be seen as an-
other form of attention mechanism, it helps the
model concentrate on the most important seman-
tic part first, and fills other skeletons step by step.
From the point of modeling, HSP simplifies the
generation by splitting the semantic part with log-
ical form grammars, which simplifies the model-
ing task of each process. The HSP mechanism can
also be regarded as a kind of information flow, the
information parsed on the previous stages can pro-
vide a soft constraint for the generation process at
a later stage.

Note that we just introduce one particular form
of HSP for semantic parsing in this section. HSP
is actually a mechanism that is highly flexible;
its structure can be applied to any sequence-to-
sequence framework and used in many structured
sequence generation tasks.

4 Experiment

4.1 Settings

During our experiments, we build a vocabulary
for complex questions, all intermediate represen-
tations and logical forms. The vocabulary con-
tains up to 30K words, constructed from all words
with more than 4 occurrences in the corpus. All
out-of-vocabulary words are represented by UNK.



4482

Our model uses pre-trained 6B tokens 300 di-
mensional Glove word embeddings (Pennington
et al., 2014), for vocabulary words which do not
have pre-trained embeddings(including three spe-
cial words: UNK, BOS and EOS), we assign them
uniform randomized values. During training, we
update all word embeddings.

Our model also uses a pre-trained Stanford-
CoreNLP POS model (Manning et al., 2014) in
the encoder embedding process. We use categor-
ical POS annotations and map them to POS em-
bedding vectors of dimension 30, the POS embed-
ding vectors are initialized from uniform distri-
bution U(−0.1, 0.1) and updated during training.
The POS embeddings are concatenated with word
embeddings to generate word representations.

We fix hidden size of all encoder and decoder
units to 300. The encoder and decoder of all HSP
models are stacked by 6 identical layers. We train
the model using Adam optimizer with β1 = 0.9,
β2 = 0.98 and � = 10−9 and use dynamic learn-
ing rate during training process. For regulariza-
tion, we use dropout (Srivastava et al., 2014) and
label smoothing (Szegedy et al., 2016) in our mod-
els and set the dropout rate to 0.2, set the label
smoothing value to 0.1.

During training, we train our models using
minibatches of 128 samples, all models are trained
for at most 20,000 steps, selecting the best model
based on development set performance. After one
model is trained, we use beam search of beam size
16 to generate logical form sequences.

The implementations of our model would be re-
leased for further study1.

4.2 Dataset

To evaluate the performance of our model on se-
mantic parsing, we conduct experiments on Com-
plexWebQuestions(v1.0) dataset (Talmor and Be-
rant, 2018) released here2, which is built on the
WebQuestions dataset (Berant et al., 2014) and
consists of samples of complex question, decom-
posed question sequence and sparql format logi-
cal form. ComplexWebQuestions is a large scale
semantic parsing dataset and contains 27734 train-
ing samples, 3480 development and 3475 test sam-
ples.

The dataset has four types of complex
questions: composition (46.7%), conjunctions

1https://github.com/cairohy/hsp
2https://www.tau-nlp.org/compwebq

(42.4%), superlatives (5.3%) and comparatives
(5.6%). Each question is either the combination
of two simple questions, or an extension of a sim-
ple question. We identify entities in logical forms
and replace them with placeholders during train-
ing and inference.

4.3 Results

We measure model performance by calculating the
accuracy of generated logical forms, and compare
performance of our approach(HSP) with various
competitive baselines. In table 2, SP Unit de-
note for the semantic parsing unit, it uses the ba-
sic structure of HSP model with no intermediate
representations, cooperates POS embedding, copy
mechanism and Glove word embedding together
with the Transformer.

Table 2 presents all models’ accuracy on de-
velopment and test set. Note that we treat SP
Unit as the performance baseline and calculate
other models’ accuracy gain or decline compared
to it, recorded in parentheses in the table. SP
Unit gets 59.91% accuracy on test set, 8.91%
higher than Pointer-Generator which matches 51%
golden sparql queries. We also observe that the
performances of SEQ2SEQ and SEQ2TREE are
lower than Pointer-Generator, the two models get
47.3% and 49.68% accuracy on test set. We think
the reason is that Pointer-Generator’s copy mech-
anism helps logical form generation. Transformer
achieves 53.41% on the test set which is also
6.5% lower than SP Unit but higher than Pointer-
Generator. This group of experiment proves
that semantic parsing on ComplexWebQuestions
is difficult for traditional sequence-to-sequence
models, and SP Unit is more effective than some
previous systems. The reason is that by combining
self-attention with copy mechanism, POS embed-
ding and other modules, SP Unit has good model-
ing ability for logical forms of complex questions.

Coarse2Fine obtains 53.52% accuracy on the
test set which is 1.84% lower than SP Unit. Our
HSP model outperforms SP Unit by 6.27% ac-
curacy which is a wide margin (with SP Unit as
a baseline, the relative improvement of HSP is
10.5%). It proves the effectiveness of HSP mecha-
nism. Compared to other neural semantic parsing
models, HSP achieves significant improvement,
proving that incorporate sub-questions and key in-
formation together boost logical form generation
effectively. We think the key reason is that ques-



4483

Model Dev (%) Relative perf Test (%) Relative perf
SEQ2SEQ (Dong and Lapata, 2016) 50.22 -11.47 47.30 -12.61
SEQ2TREE (Dong and Lapata, 2016) 51.87 -9.82 49.68 -10.23
PointerGenerator (See et al., 2017) 53.10 -8.59 51.00 -8.91
Transformer (Vaswani et al., 2017) 56.78 -4.91 53.41 -6.50
Coarse2Fine (Dong and Lapata, 2018) 58.59 -3.10 58.07 -1.84
SP Unit 61.69 / 59.91 /
HSP w/o DR 66.09 +4.40 63.16 +3.25
HSP w/o SR 67.32 +5.63 64.48 +4.57
HSP(Switch) 68.13 +6.44 65.29 +5.38
HSP 68.79 +7.10 66.18 +6.27

Table 2: Logical form accuracy on development and test set of ComplexWebQuestions dataset. The second column
and forth column of the table show relative performance compared to the SP Unit on dev set and test set separately.

tion decomposition turns the complex question
into simple questions and then solves simple ques-
tions in a divide-and-conquer manner, which sim-
plify representation learning process of the model
in each stage.

4.3.1 Ablation Analysis

As the above part of Table 2 shows, we conduct an
ablation study on HSP to analyze the importance
of each component in HSP. We use four HSP ab-
lation models for experiments. HSP(Switch): A
three stage model which switches the order of in-
termediate parsing targets, first parsing semantic
representation and then decomposing the original
question; HSP w/o SR: HSP without semantic rep-
resentation, HSP degrades to a two stage model;
HSP w/o DR: Remove decomposed representa-
tion from HSP, it degrades to a two stage model;
SP Unit: Remove all intermediate representations,
HSP degrades to SP Unit.

First, we compare the HSP model with
HSP(Switch), we observe that HSP outperforms
HSP(Switch) by 0.89%, suggesting that parsing
the intermediate representations in a bottom up
way is more effective. Then we analyze the effect
of different intermediate representations by re-
moving semantic representation or sub-questions
from HSP, resulting in performance degradation
of 1.7% and 3.02% separately. Results prove that
the question decomposition stage is most critical
in HSP process. Finally, without any intermediate
representations(the model degrades to a SP unit
model), performance drops by 6.27% compared to
our full HSP model.

4.4 Discussion and Analysis

4.4.1 Performance on Different Question
Types

To evaluate the impact of question types on model
performances, we calculate logical form accuracy
on each type of questions of test set. Results are
shown in Figure 3. Note that the test set consists
of roughly 45% composition questions, 45% con-
junction questions, 5% comparative questions and
5% superlative questions.

0

10

20

30

40

50

60

70

80

Composition Conjunction Comparative Superlative

Ac
cu

ra
cy

HSP Transformer Coarse2Fine

Figure 3: Model performances on different question
types.

As Figure 3 shows, HSP has highest accuracy
on the four type of question samples among the
three models. Moreover, the accuracy of Trans-
former on composition and conjunction questions
is comparable to that of Coarse2Fine and lower
than HSP, showing that the HSP mechanism helps
improve modeling capability. Finally, compared
to Transformer, the accuracy of Coarse2Fine and
HSP in comparative and superlative questions has
been significantly improved, because these two
models utilize additional information to enhance
the robustness of the model, thus obtaining better
results on types with much fewer training samples.



4484

35

40

45

50

55

60

65

70

20 40 60 80 100

Ac
cu

ra
cy

 (%
)

Training data size (%)

HSP
Coarse2Fine
Transformer

Figure 4: Model performances with different amount
of training data.

4.4.2 Performance on Different Training
Data Volumes

In Figure 4, we depict the trends of test set accu-
racy with different portions of training data. The
results of this experiment demonstrate that the per-
formance of the HSP exceeds the other two base-
lines, regardless of the amount of training data.
Moreover, as training data volume increases, the
performance improvement that HSP can achieve
is higher than the other two models. We think the
reason is that as the training resources increase,
HSP learns better question decomposer and infor-
mation extractor and generates more accurate sub-
questions and key information, which help HSP
semantic parser to obtain better logical form re-
sults.

4.5 Question Decomposition Results

To further evaluate the effectiveness and gener-
alization ability of our HSP model, we conduct
question decomposition experiment with an HSP
model variant and compare its performance to sev-
eral neural models. We use case-insensitive Bleu-
4 (Papineni et al., 2002) and Rouge-L (Lin, 2004)
as evaluation metrics for question decomposition.
For all models, the input is the complex question,
and the output is decomposed sub-question se-
quence with the same format as decomposed rep-
resentation.

Table 3 shows the question decomposition re-
sults of different models. PointerNetwork refers
to the model (Talmor and Berant, 2018) on split-
ting the complex question into sub-questions us-
ing splitting points predicted by a pointer network
model (Vinyals et al., 2015). HSP(SR) refers to
a two-stage HSP model for which we use seman-
tic representation as intermediate representation.
We observe that compared to PointerNetwork, the
other two models obtain much better results, prov-

Model Dev Test
PointerNetwork 31.3 / 55.2 31.9 / 55.7
PointerGenerator 55.5 / 69.3 55.0 / 69.0
HSP(SR) 81.2 / 90.6 78.9 / 88.7
w/o SR 78.9 / 88.5 76.3 / 86.8
w/o POS 78.3 / 88.1 75.8 / 86.3
w/o Glove 77.2 / 87.4 75.4 / 85.6
w/o Copy 73.7 / 86.5 71.3 / 84.6

Table 3: Bleu-4/Rouge-L scores on ComplexWe-
bQuestions dataset, question decomposition task.

ing that compared to decomposing complex ques-
tion by finding splitting points in the question, our
neural generative question decomposer is more ef-
fective. The Pointer-Generator follows the set-
tings in semantic parsing experiments, and it ob-
tains 55.0 Bleu-4 and 69.0 Rouge-L scores, which
are lower than HSP. With the help of semantic
representation and HSP model, HSP(SR) achieves
81.2/90.6 Bleu-4/Rouge-L scores on the test set,
much higher than the two baselines.

We also perform ablation experiments on ques-
tion decomposition to measure the impact of dif-
ferent modules, the results are also shown in Ta-
ble 3. We examine four main modules in the
HSP model: semantic representation(SR), POS
embedding(POS), pre-trained Glove word embed-
ding(Glove) and copy mechanism(Copy), and in-
crementally remove these modules from HSP(SR).
Results show that without semantic representation
in HSP, the model’s Bleu-4 score decreases 2.6
points and the Rouge-L score decreases 1.9 points.
The decrease of Bleu-4 score by removing HSP is
only lower than removing the copy mechanism(4.1
points), and Rouge degradation is highest among
the four ablation models. It indicates that HSP
mechanism is vital for the model.

5 Conclusion

In this work, we propose a novel hierarchical se-
mantic parsing (HSP) model based on sequence-
to-sequence paradigm. Experiments show that
compared to several previous systems, HSP ef-
fectively improves performance. We also design
a neural generative question decomposer which
achieves much higher performance than splitting-
based question decomposition approach. Further
experiments also prove that the proposed neural
generative question decomposer also benefits from
the HSP mechanism.



4485

Acknowledgments

This research was supported by the National Key
Research and Development Program of China
(2018YFB1004502) and the National Natural Sci-
ence Foundation of China (61690203, 61532001).
We thank the anonymous reviewers for their help-
ful comments.

References
Yoav Artzi, Nicholas FitzGerald, and Luke S. Zettle-

moyer. 2013. Semantic parsing with combinatory
categorial grammars. In ACL 2013, page 2.

J. Berant, A. Chou, R. Frostig, and P. Liang. 2014.
Semantic parsing on freebase from question-answer
pairs. In EMNLP 2014.

Qingqing Cai and Alexander Yates. 2013. Large-scale
semantic parsing via schema matching and lexicon
extension. In ACL 2013, pages 423–433.

Bo Chen, Le Sun, and Xianpei Han. 2018. Sequence-
to-action: End-to-end semantic graph generation for
semantic parsing. In ACL 2018, pages 766–777. As-
sociation for Computational Linguistics.

Li Dong and Mirella Lapata. 2016. Language to logical
form with neural attention. In ACL 2016, pages 33–
43.

Li Dong and Mirella Lapata. 2018. Coarse-to-fine de-
coding for neural semantic parsing. In ACL 2018,
pages 731–742.

Li Dong, Chris Quirk, and Mirella Lapata. 2018. Con-
fidence modeling for neural semantic parsing. In
ACL 2018, pages 743–753.

Xing Fan, Emilio Monti, Lambert Mathias, and Markus
Dreyer. 2017. Transfer learning for neural semantic
parsing. arXiv preprint arXiv:1706.04326.

Matt Gardner, Pradeep Dasigi, Srinivasan Iyer, Alane
Suhr, and Luke Zettlemoyer. 2018. Neural semantic
parsing. In ACL 2018, pages 17–18.

Omer Goldman, Veronica Latcinnik, Ehud Nave, Amir
Globerson, and Jonathan Berant. 2018. Weakly su-
pervised semantic parsing with abstract examples.
In ACL 2018, pages 1809–1819.

Jiatao Gu, Zhengdong Lu, Hang Li, and Victor O. K.
Li. 2016. Incorporating Copying Mechanism in
Sequence-to-Sequence Learning. In ACL 2016.

Jonathan Herzig and Jonathan Berant. 2017. Neural
semantic parsing over multiple knowledge-bases. In
ACL 2017, pages 623–628. Association for Compu-
tational Linguistics.

Mohit Iyyer, Wen-tau Yih, and Ming-Wei Chang. 2016.
Answering complicated question intents expressed
in decomposed question sequences. arXiv preprint
arXiv:1611.01242.

Robin Jia and Percy Liang. 2016. Data recombination
for neural semantic parsing. In ACL 2016, pages
12–22. Association for Computational Linguistics.

Aditya Kalyanpur, Siddharth Patwardhan, BK Bogu-
raev, Adam Lally, and Jennifer Chu-Carroll. 2012.
Fact-based question decomposition in deepqa. IBM
Journal of Research and Development, 56(3.4):13–
1.

Tom Kwiatkowski, Eunsol Choi, Yoav Artzi, and
Luke S. Zettlemoyer. 2013. Scaling semantic
parsers with on-the-fly ontology matching. In
EMNLP 2013, pages 1545–1556.

Tom Kwiatkowski, Luke Zettlemoyer, Sharon Goldwa-
ter, and Mark Steedman. 2011. Lexical generaliza-
tion in ccg grammar induction for semantic parsing.
In EMNLP 2011, pages 1512–1523.

Percy Liang, Michael I. Jordan, and Dan Klein. 2011.
Learning dependency-based compositional seman-
tics. In ACL 2011, pages 590–599.

Chin-Yew Lin. 2004. Rouge: A package for auto-
matic evaluation of summaries. Text Summarization
Branches Out.

Christopher Manning, Mihai Surdeanu, John Bauer,
Jenny Finkel, Steven Bethard, and David McClosky.
2014. The Stanford CoreNLP Natural Language
Processing Toolkit. In ACL 2014, pages 55–60.

Raymond J. Mooney. 2007. Learning for semantic
parsing. In CICLing 2007, pages 311–324.

Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic eval-
uation of machine translation. In ACL 2002, pages
311–318.

Jeffrey Pennington, Richard Socher, and Christopher D
Manning. 2014. GloVe: Global Vectors for Word
Representation. In EMNLP 2014, pages 1532–1543.

Siva Reddy, Mirella Lapata, and Mark Steedman. 2014.
Large-scale semantic parsing without question-
answer pairs. Transactions of the ACL, 2(1):377–
392.

Abigail See, Peter J. Liu, and Christopher D. Man-
ning. 2017. Get to the point: Summarization with
pointer-generator networks. In ACL 2017, pages
1073–1083. Association for Computational Linguis-
tics.

Linfeng Song, Zhiguo Wang, Mo Yu, Yue Zhang,
Radu Florian, and Daniel Gildea. 2018. Exploring
graph-structured passage representation for multi-
hop reading comprehension with graph neural net-
works. arXiv preprint arXiv:1809.02040.



4486

Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky,
Ilya Sutskever, and Ruslan Salakhutdinov. 2014.
Dropout: a simple way to prevent neural networks
from overfitting. Journal of Machine Learning Re-
search, 15(1):1929–1958.

Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe,
Jonathon Shlens, and Zbigniew Wojna. 2016. Re-
thinking the inception architecture for computer vi-
sion. In CVPR 2016, pages 2818–2826.

Alon Talmor and Jonathan Berant. 2018. The web as
a knowledge-base for answering complex questions.
arXiv preprint arXiv:1803.06643.

Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob
Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz
Kaiser, and Illia Polosukhin. 2017. Attention is all
you need. In NIPS 2017.

Oriol Vinyals, Meire Fortunato, and Navdeep Jaitly.
2015. Pointer networks. In NIPS 2015, pages 2692–
2700.

Chunyang Xiao, Marc Dymetman, and Claire Gardent.
2016. Sequence-based structured prediction for se-
mantic parsing. In ACL 2016.

Xiaojun Xu, Chang Liu, and Dawn Song. 2017. Sql-
net: Generating structured queries from natural
language without reinforcement learning. CoRR,
abs/1711.04436.

Xuchen Yao. 2015. Lean question answering over free-
base from scratch. In NAACL-HLT 2015, pages 66–
70.

Wen-tau Yih, Ming-Wei Chang, Xiaodong He, and
Jianfeng Gao. 2015. Semantic parsing via staged
query graph generation: Question answering with
knowledge base. In ACL 2015, pages 1321–1331.
Association for Computational Linguistics.

Pengcheng Yin and Graham Neubig. 2017. A syntactic
neural model for general-purpose code generation.
arXiv preprint arXiv:1704.01696.

Luke S. Zettlemoyer and Michael Collins. 2005.
Learning to map sentences to logical form: Struc-
tured classification with probabilistic categorial
grammars. In UAI 2005, pages 658–666.


