



















































Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics


Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, pages 688–697
Vancouver, Canada, July 30 - August 4, 2017. c©2017 Association for Computational Linguistics

https://doi.org/10.18653/v1/P17-1064

Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, pages 688–697
Vancouver, Canada, July 30 - August 4, 2017. c©2017 Association for Computational Linguistics

https://doi.org/10.18653/v1/P17-1064

Modeling Source Syntax for Neural Machine Translation

Junhui Li† Deyi Xiong† Zhaopeng Tu‡∗
Muhua Zhu‡ Min Zhang† Guodong Zhou†

†School of Computer Science and Technology,
Soochow University, Suzhou, China

{lijunhui, dyxiong, minzhang, gdzhou}@suda.edu.cn
‡Tencent AI Lab, Shenzhen, China

tuzhaopeng@gmail.com, muhuazhu@tencent.com

Abstract

Even though a linguistics-free sequence to
sequence model in neural machine trans-
lation (NMT) has certain capability of im-
plicitly learning syntactic information of
source sentences, this paper shows that
source syntax can be explicitly incorpo-
rated into NMT effectively to provide fur-
ther improvements. Specifically, we lin-
earize parse trees of source sentences to
obtain structural label sequences. On the
basis, we propose three different sorts of
encoders to incorporate source syntax into
NMT: 1) Parallel RNN encoder that learns
word and label annotation vectors paral-
lelly; 2) Hierarchical RNN encoder that
learns word and label annotation vectors in
a two-level hierarchy; and 3) Mixed RNN
encoder that stitchingly learns word and
label annotation vectors over sequences
where words and labels are mixed. Exper-
imentation on Chinese-to-English transla-
tion demonstrates that all the three pro-
posed syntactic encoders are able to im-
prove translation accuracy. It is interesting
to note that the simplest RNN encoder, i.e.,
Mixed RNN encoder yields the best perfor-
mance with an significant improvement of
1.4 BLEU points. Moreover, an in-depth
analysis from several perspectives is pro-
vided to reveal how source syntax benefits
NMT.

1 Introduction

Recently the sequence to sequence model
(seq2seq) in neural machine translation (NMT)
has achieved certain success over the state-of-
the-art of statistical machine translation (SMT)

∗Work done at Huawei Noah’s Ark Lab, HongKong.

              

NP2NP1 VV

tokoyo stock exchange approves new listing bank 

input:

output:

reference: tokyo exchange approves shinsei bank 's application for listing

(a). An example of discontinuous translation

          ,              

NP

they came from six families with two girls and two girls .

they came from six families and two girls are without parents .

(b). An example of over translation

input:

output:

reference:

Figure 1: Examples of NMT translation that fail to
respect source syntax.

on various language pairs (Bahdanau et al., 2015;
Jean et al., 2015; Luong et al., 2015; Luong and
Manning, 2015). However, Shi et al. (2016)
show that the seq2seq model still fails to capture
a lot of deep structural details, even though it is
capable of learning certain implicit source syntax
from sentence-aligned parallel corpus. Moreover,
it requires an additional parsing-task-specific
training mechanism to recover the hidden syntax
in NMT. As a result, in the absence of explicit
linguistic knowledge, the seq2seq model in NMT
tends to produce translations that fail to well
respect syntax. In this paper, we show that syntax
can be well exploited in NMT explicitly by taking
advantage of source-side syntax to improve the
translation accuracy.

In principle, syntax is a promising avenue for
translation modeling. This has been verified
by tremendous encouraging studies on syntax-
based SMT that substantially improves translation
by integrating various kinds of syntactic knowl-
edge (Liu et al., 2006; Marton and Resnik, 2008;

688

https://doi.org/10.18653/v1/P17-1064
https://doi.org/10.18653/v1/P17-1064


Shen et al., 2008; Li et al., 2013). While it is yet to
be seen how syntax can benefit NMT effectively,
we find that translations of NMT sometimes fail
to well respect source syntax. Figure 1 (a) shows a
Chinese-to-English translation example of NMT.
In this example, the NMT seq2seq model incor-
rectly translates the Chinese noun phrase (i.e., 新
生/xinsheng 银行/yinhang) into a discontinuous
phrase in English (i.e., new ... bank) due to the
failure of capturing the internal syntactic structure
in the input Chinese sentence. Statistics on our de-
velopment set show that one forth of Chinese noun
phrases are translated into discontinuous phrases
in English, indicating the substantial disrespect of
syntax in NMT translation.1 Figure 1 (b) shows
another example with over translation, where the
noun phrase 两/liang 个/ge 女孩/nvhai is trans-
lated twice in English. Similar to discontinuous
translation, over translation usually happens along
with the disrespect of syntax which results in the
repeated translation of the same source words in
multiple positions of the target sentence.

In this paper we are not aiming at solving any
particular issue, either the discontinuous transla-
tion or the over translation. Alternatively, we ad-
dress how to incorporate explicitly the source syn-
tax to improve the NMT translation accuracy with
the expectation of alleviating the issues above in
general. Specifically, rather than directly assign-
ing each source word with manually designed syn-
tactic labels, as Sennrich and Haddow (2016) do,
we linearize a phrase parse tree into a structural
label sequence and let the model automatically
learn useful syntactic information. On the basis,
we systematically propose and compare several
different approaches to incorporating the label se-
quence into the seq2seq NMT model. Experimen-
tation on Chinese-to-English translation demon-
strates that all proposed approaches are able to im-
prove the translation accuracy.

2 Attention-based NMT

As a background and a baseline, in this section,
we briefly describe the NMT model with an atten-
tion mechanism by Bahdanau et al. (2015), which
mainly consists of an encoder and a decoder, as
shown in Figure 2.

Encoder The encoding of a source sentence is for-
1Manually examining 200 random such discontinuously

translated noun phrases, we find that 90% of them should be
continuously translated according to the reference translation.

h1 
h1

h1 
h1

hm 
hm

x1     x2 …..   xm

h

Attenh
si-1

ci

RNN

MLP
yi

yi-1

si

(a) encoder (b) decoder

Figure 2: Attention-based NMT model.

mulated using a pair of neural networks, i.e., two
recurrent neural networks (denoted bi-RNN): one
reads an input sequence x = (x1, ..., xm) from left
to right and outputs a forward sequence of hid-
den states (

−→
h1, ...,

−→
hm), while the other operates

from right to left and outputs a backward sequence
(
←−
h1, ...,

←−
hm). Each source word xj is represented

as hj (also referred to as word annotation vector):
the concatenation of hidden states

−→
hj and

←−
hj . Such

bi-RNN encodes not only the word itself but also
its left and right context, which can provide impor-
tant evidence for its translation.

Decoder The decoder is also an RNN that pre-
dicts a target sequence y = (y1, ..., yn). Each tar-
get word yi is predicted via a multi-layer percep-
tron (MLP) component which is based on a recur-
rent hidden state si, the previous predicted word
yi−1, and a source-side context vector ci. Here,
ci is calculated as a weighted sum over source an-
notation vectors (h1, ..., hm). The weight vector
αi ∈ Rm over source annotation vectors is ob-
tained by an attention model, which captures the
correspondences between the source and the target
languages. The attention weight αij is computed
based on the previous recurrent hidden state si−1
and source annotation vector hj .

3 NMT with Source Syntax

The conventional NMT models treat a sentence as
a sequence of words and ignore external knowl-
edge, failing to effectively capture various kinds
of inherent structure of the sentence. To lever-
age external knowledge, specifically the syntax in
the source side, we focus on the parse tree of a
sentence and propose three different NMT mod-
els that explicitly consider the syntactic structure
into encoding. Our purpose is to inform the NMT
model the structural context of each word in its
corresponding parse tree with the goal that the
learned annotation vectors (h1, ..., hm) encode not

689



I         love        dogs

w1   w2       w3
(a) word sequence

S

NP

PRN

VP

VBP NP

NNSI love

dogs
(b) phrase parse tree

S  NP  PRN  VP VBP NP NNS

l1 l2  l3    l4  l5   l6  l7
(c) structural label sequence

Figure 3: An example of an input sentence (a), its
parse tree (b), and the parse tree’s sequential form
(c).

only the information of words and their surround-
ings, but also structural context in the parse tree. In
the rest of this section, we use English sentences
as examples to explain our methods.

3.1 Syntax Representation

To obtain the structural context of a word in its
parse tree, ideally the model should not only cap-
ture and remember the whole parse tree structure,
but also discriminate the contexts of any two dif-
ferent words. However, considering the lack of
efficient way to directly model structural informa-
tion, an alternative way is to linearize the phrase
parse tree into a sequence of structural labels and
learn the structural context through the sequence.
For example, Figure 3(c) shows the structural la-
bel sequence of Figure 3(b) in a simple way fol-
lowing a depth-first traversal order. Note that lin-
earizing a parse tree in a depth-first traversal or-
der into a sequence of structural labels has also
been widely adopted in recent advances in neural
syntactic parsing (Vinyals et al., 2015; Choe and
Charniak, 2016), suggesting that the linearized se-
quence can be viewed as an alternative to its tree
structure.2

2We have also tried to include the ending brackets in the
structural label sequence, as what (Vinyals et al., 2015; Choe

hw1 
  

hw1
hw2 

  
hw2

hw3 
  

hw3
 I      love  dogs S   NP  PRN  VP  VBP  NP  NNS

hl1 
  

hl1

… 
  

…

hl7 
  

hl7
… 

  
…

… 
  

…

… 
  

…

… 
  

…

(a) Parallel RNN encoder

+ + +

word RNN structural label RNN

hw1 
  

hw1 

hl3 

hl3

hw2 
  

hw2 

hl5 

hl5

hw3 
  

hw3 

hl7 

hl7

word RNN

S   NP  PRN  VP  VBP  NP  NNS

hl1 
  

hl1

… 
  

…

hl7 
  

hl7
… 

  
…

… 
  

…

… 
  

…

… 
  

…

hw1 
  

hw1
hw2 

  
hw2

hw3 
  

hw3

ew1 
  

+
ew2 

  
+

ew3 
  
+

I Iove dogs

structural label  
RNN

(b) Hierarchical RNN encoder

Figure 4: The graphical illustration of the Parallel
RNN encoder (a) and the Hierarchical RNN en-
coder (b). Here,

−−→
hwj and

←−−
hwj are the forward and

backward hidden states for word wj ,
−→
hli and

←−
hli

are for structural label li, ewj is the word embed-
ding for word wj , and

⊕
is for concatenation op-

erator.

There is no doubt that the structural label se-
quence is much longer than its word sequence.
In order to obtain the structural label annotation
vector for wi in word sequence, we simply look
for wi’s part-of-speech (POS) tag in the label se-
quence and view the tag’s annotation vector as
wi’s label annotation vector. This is because wi’s
POS tag location can also represent wi’s location
in the parse tree. For example, in Figure 3, word
w1 in (a) maps to l3 in (c) since l3 is the POS tag
of w1. Likewise, w2 maps to l5 and w3 to l7. That
is to say, we use l3’s learned annotation vector as
w1’s label annotation vector.

and Charniak, 2016) do. However, the performance gap is
very small by adding the ending brackets or not.

690



3.2 RNN Encoders with Source Syntax
In the next, we first propose two different encoders
to augment word annotation vector with its corre-
sponding label annotation vector, each of which
consists of two RNNs 3: in one encoder, the two
RNNs work independently (i.e., Parallel RNN En-
coder) while in another encoder the two RNNs
work in a hierarchical way (i.e., Hierarchical RNN
Encoder). The difference between the two en-
coders lies in how the two RNNs interact. Then,
we propose the third encoder with a single RNN,
which learns word and label annotation vectors
stitchingly (i.e., Mixed RNN Encoder). Since any
of the above three approaches focuses only on the
encoder as to generate source annotation vectors
along with structural information, we keep the rest
part of the NMT models unchanged.

Parallel RNN Encoder Figure 4 (a) illustrates
our Parallel RNN encoder, which includes two
parallel RNNs: i.e., a word RNN and a structural
label RNN. On the one hand, the word RNN, as in
conventional NMT models, takes a word sequence
as input and output a word annotation vector
for each word. On the other hand, the structural
label RNN takes the structural label sequence of
the word sequence as input and obtains a label
annotation vector for each label. Besides, we
concatenate each word’s word annotation vector
and its POS tag’s label annotation vector as the
final annotation vector for the word. For example,
the final annotation vector for word love in
Figure 4 (a) is [

−−→
hw2;

←−−
hw2;

−→
hl5;
←−
hl5], where the first

two subitems [
−−→
hw2;

←−−
hw2] are the word annotation

vector and the rest two subitems [
−→
hl5;
←−
hl5] are its

POS tag VBP’s label annotation vector.

Hierarchical RNN Encoder Partially inspired
by the model architecture of GNMT (Wu et al.,
2016) which consists of multiple layers of LSTM
RNNs, we propose a two-layer model architec-
ture in which the lower layer is the structural label
RNN while the upper layer is the word RNN, as
shown in Figure 4 (b). We put the word RNN in
the upper layer because each item in the word se-
quence can map into an item in the structural label
sequence, while this does not hold if the order of
the two RNNs is reversed. As shown in Figure 4
(b), for example, the POS tag VBP’s label anno-
tation vector [

−→
hl5,
←−
hl5] is concatenated with word

3Hereafter, we simplify bi-RNN as RNN.

S    NP  PRN    I    VP  VBP  love  NP  NNS dogs

h1 
  

h1
h2 

  
h2

h3 
  

h3
h4 

  
h4

h5 
  

h5
h6 

  
h6

h7 
  

h7
h8 

  
h8

h9 
  

h9
h10 

  
h10

Figure 5: The graphical illustration of the Mixed
RNN encoder. Here,

−→
hj and

←−
hj are the forward and

backward hidden annotation vectors for j-th item,
which can be either a word or a structural label.

love’s word embedding ew2 to feed as the input to
the word RNN.

Mixed RNN Encoder Figure 5 presents our
Mixed RNN encoder. Similarly, the sequence of
input is the linearization of its parse tree (as in
Figure 3 (b)) following a depth-first traversal or-
der, but being mixed with both words and struc-
tural labels in a stitching way. It shows that the
RNN learns annotation vectors for both the words
and the structural labels, though only the annota-
tion vectors of words are further fed to decoding
(e.g., ([

−→
h4,
←−
h4], [

−→
h7,
←−
h7], [

−→
h10,
←−
h10])). Even though

the annotation vectors of structural labels are not
directly fed forward for decoding, the error signal
is back propagated along the word sequence and
allows the annotation vectors of structural labels
being updated accordingly.

3.3 Comparison of RNN Encoders with
Source Syntax

Though all the three encoders model both word
sequence and structural label sequence, the dif-
ferences lie in their respective model architecture
with respect to the degree of coupling the two se-
quences:

• In the Parallel RNN encoder, the word RNN
and structural label RNN work in a parallel
way. That is to say, the error signal back
propagated from the word sequence would
not affect the structural label RNN, and vice
versa. In contrast, in the Hierarchical RNN
encoder, the error signal back propagated
from the word sequence has a direct impact
on the structural label annotation vectors, and
thus on the structural label embeddings. Fi-
nally, the Mixed RNN encoder ties the struc-
tural label sequence and word sequence to-
gether in the closest way. Therefore, the
degrees of coupling the word and structural

691



label sequences in these three encoders are
like this: Mixed RNN encoder > Hierarchi-
cal RNN encoder > Parallel RNN encoder.

• Figure 4 and Figure 5 suggest that the Mixed
RNN encoder is the simplest. Moreover,
comparing to conventional NMT encoders,
the difference lies only in the length of the in-
put sequence. Statistics on our training data
reveal that the Mixed RNN encoder approxi-
mately triples the input sequence length com-
pared to conventional NMT encoders.

4 Experimentation

We have presented our approaches to incorporat-
ing the source syntax into NMT encoders. In
this section, we evaluate their effectiveness on
Chinese-to-English translation.

4.1 Experimental Settings

Our training data for the translation task consists
of 1.25M sentence pairs extracted from LDC cor-
pora, with 27.9M Chinese words and 34.5M En-
glish words respectively.4 We choose NIST MT
06 dataset (1664 sentence pairs) as our develop-
ment set, and NIST MT 02, 03, 04, and 05 datasets
(878, 919, 1788 and 1082 sentence pairs, respec-
tively) as our test sets.5 To get the source syn-
tax for sentences on the source-side, we parse the
Chinese sentences with Berkeley Parser 6 (Petrov
and Klein, 2007) trained on Chinese TreeBank
7.0 (Xue et al., 2005). We use the case insensitive
4-gram NIST BLEU score (Papineni et al., 2002)
for the translation task.

For efficient training of neural networks, we
limit the maximum sentence length on both source
and target sides to 50. We also limit both the
source and target vocabularies to the most frequent
16K words in Chinese and English, covering ap-
proximately 95.8% and 98.2% of the two corpora
respectively. All the out-of-vocabulary words are
mapped to a special token UNK. Besides, the word
embedding dimension is 620 and the size of a hid-
den layer is 1000. All the other settings are the
same as in Bahdanau et al.(2015).

4The corpora include LDC2002E18, LDC2003E07,
LDC2003E14, Hansards portion of LDC2004T07,
LDC2004T08 and LDC2005T06.

5http://www.itl.nist.gov/iad/mig/
tests/mt/

6https://github.com/slavpetrov/
berkeleyparser

The inventory of structural labels includes 16
phrase labels and 32 POS tags. In both our Paral-
lel RNN encoder and Hierarchical RNN encoder,
we set the embedding dimension of these labels as
100 and the size of a hidden layer as 100. Besides,
the maximum structural label sequence length is
set to 100. In our Mixed RNN encoder, since we
only have one input sequence, we equally treat the
structural labels and words (i.e., a structural label
is also initialized with 620 dimension embedding).
Compared to the baseline NMT model, the only
different setting is that we increase the maximum
sentence length on source-side from 50 to 150.

We compare our method with two state-of-the-
art models of SMT and NMT:

• cdec (Dyer et al., 2010): an open source hi-
erarchical phrase-based SMT system (Chi-
ang, 2007) with default configuration and a
4-gram language model trained on the target
portion of the training data.7

• RNNSearch: a re-implementation of the at-
tentional NMT system (Bahdanau et al.,
2015) with slight changes taken from dl4mt
tutorial.8 For the activation function f of an
RNN, RNNSearch uses the gated recurrent
unit (GRU) recently proposed by (Cho et al.,
2014b). It incorporates dropout (Hinton
et al., 2012) on the output layer and improves
the attention model by feeding the lastly gen-
erated word. We use AdaDelta (Zeiler, 2012)
to optimize model parameters in training with
the mini-batch size of 80. For translation, a
beam search with size 10 is employed.

4.2 Experiment Results

Table 1 shows the translation performances mea-
sured in BLEU score. Clearly, all the proposed
NMT models with source syntax improve the
translation accuracy over all test sets, although
there exist considerable differences among differ-
ent variants.

Parameters The three proposed models introduce
new parameters in different ways. As a baseline
model, RNNSearch has 60.6M parameters. Due to
the infrastructure similarity, the Parallel RNN sys-
tem and the Hierarchical RNN system introduce

7https://github.com/redpony/cdec
8https://github.com/nyu-dl/

dl4mt-tutorial

692



# System #Params Time MT06 MT02 MT03 MT04 MT05 All
1 cdec - - 33.4 34.8 33.0 35.7 32.1 34.2
2 RNNSearch 60.6M 153m 34.0 36.9 33.7 37.0 34.1 35.6
3 Parallel RNN +1.1M +9m 34.8† 37.8‡ 34.2 38.3‡ 34.6 36.6‡
4 Hierarchical RNN +1.2M +9m 35.2‡ 37.2 34.7† 38.7‡ 34.7† 36.7‡
5 Mixed RNN +0 +40m 35.6‡ 37.7† 34.9‡ 38.6‡ 35.5‡ 37.0‡

Table 1: Evaluation of the translation performance. † and ‡: significant over RNNSearch at 0.05/0.01,
tested by bootstrap resampling (Koehn, 2004). “+” is the additional number of parameters or training
time on the top of the baseline system RNNSearch. Column Time indicates the training time in minutes
per epoch for different NMT models

the similar size of additional parameters, result-
ing from the RNN model for structural label se-
quences (about 0.1M parameters) and catering ei-
ther the augmented annotation vectors (as shown
in Figure 4 (a)) or the augmented word embed-
dings (as shown in Figure 4 (b)) (the remain pa-
rameters). It is not surprising that the Mixed RNN
system does not require any additional parameters
since though the input sequence becomes longer,
we keep the vocabulary size unchanged, resulting
in no additional parameters.

Speed Introducing the source syntax slightly
slows down the training speed. When running on
a single GPU GeForce GTX 1080, the baseline
model speeds 153 minutes per epoch with 14K
updates while the proposed structural label RNNs
in both Parallel RNN and Hierarchical RNN sys-
tems only increases the training time by about 6%
(thanks to the small size of structural label embed-
dings and annotation vectors), and the Mixed RNN
system spends 26% more training time to cater the
triple sized input sequence.

Comparison with the baseline NMT model
(RNNSearch) While all the three proposed NMT
models outperform RNNSearch, the Parallel RNN
system and the Hierarchical RNN system achieve
similar accuracy (e.g., 36.6 v.s. 36.7). Besides,
the Mixed RNN system achieves the best accu-
racy overall test sets with the only exception of
NIST MT 02. Over all test sets, it outperforms
RNNSearch by 1.4 BLEU points and outperforms
the other two improved NMT models by 0.3∼0.4
BLEU points, suggesting the benefits of high de-
gree of coupling the word sequence and the struc-
tural label sequence. This is very encouraging
since the Mixed RNN encoder is the simplest,
without introducing new parameters and with only
slight additional training time.

Figure 6: Performance of the generated transla-
tions with respect to the lengths of the input sen-
tences.

Comparison with the SMT model (cdec) Ta-
ble 1 also shows that all NMT systems outper-
form the SMT system. This is very consistent
with other studies on Chinese-to-English transla-
tion (Mi et al., 2016; Tu et al., 2017b; Wang et al.,
2017).

5 Analysis

As the proposed Mixed RNN system achieves
the best performance, we further look at the
RNNSearch system and the Mixed RNN system to
explore more on how syntactic information helps
in translation.

5.1 Effects on Long Sentences

Following Bahdanau et al. (2015), we group sen-
tences of similar lengths together and compute
BLEU scores. Figure 6 presents the BLEU scores
over different lengths of input sentences. It shows
that Mixed RNN system outperforms RNNSearch
over sentences with all different lengths. It also
shows that the performance drops substantially

693



System AER
RNNSearch 50.1
Mixed RNN 47.9

Table 2: Evaluation of alignment quality. The
lower the score, the better the alignment quality.

when the length of input sentences increases. This
performance trend over the length is consistent
with the findings in (Cho et al., 2014a; Tu et al.,
2016, 2017a). We also observe that the NMT sys-
tems perform surprisingly bad on sentences over
50 in length, especially compared to the perfor-
mance of SMT system (i.e., cdec). We think that
the bad behavior of NMT systems towards long
sentences (e.g., length of 50) is due to the fol-
lowing two reasons: (1) the maximum source sen-
tence length limit is set as 50 in training, 9 making
the learned models not ready to translate sentences
over the maximum length limit; (2) NMT systems
tend to stop early for long input sentences.

5.2 Analysis on Word Alignment

Due to the capability of carrying syntactic infor-
mation in source annotation vectors, we conjec-
ture that our model with source syntax is also
beneficial for alignment. To test this hypothe-
sis, we carry out experiments of the word align-
ment task on the evaluation dataset from Liu and
Sun (2015), which contains 900 manually aligned
Chinese-English sentence pairs. We force the de-
coder to output reference translations, as to get au-
tomatic alignments between input sentences and
their reference translations. To evaluate alignment
performance, we report the alignment error rate
(AER) (Och and Ney, 2003) in Table 2.

Table 2 shows that source syntax information
improves the attention model as expected by main-
taining an annotation vector summarizing struc-
tural information on each source word.

5.3 Analysis on Phrase Alignment

The above subsection examines the alignment per-
formance at the word level. In this subsection, we
turn to phrase alignment analysis by moving from
word unit to phrase unit. Given a source phrase
XP, we use word alignments to examine if the
phrase is translated continuously (Cont.), or dis-

9Though the maximum source length limit in Mixed RNN
system is set to 150, it approximately contains 50 words in
maximum.

System XP Cont. Dis. Un.

RNNSearch

PP 57.3 33.6 9.1
NP 59.8 25.5 14.7
CP 47.3 44.6 8.1
QP 54.0 22.2 23.8

ALL 58.1 27.1 14.8

Mixed RNN

PP 63.3 27.5 9.2
NP 63.1 23.1 13.8
CP 54.5 36.6 8.9
QP 56.2 19.7 24.1

ALL 60.4 25.0 14.6

Table 3: Percentages (%) of syntactic phrases in
our test sets being translated continuously, discon-
tinuously, or not being translated. Here PP is for
prepositional phrase, NP for noun phrase, CP for
clause headed by a complementizer, QP for quain-
ter phrase.

continuously (Dis.), or if it is not translated at all
(Un.).

There are some phrases, such as noun phrases
(NPs), prepositional phrases (PPs) that we usu-
ally expect to have a continuous translation. With
respect to several such types of phrases, Table 3
shows how these phrases are translated. From
the table, we see that translations of RNNSearch
system do not respect source syntax very well.
For example, in RNNSearch translations, 57.3%,
33.6%, and 9.1% of PPs are translated continu-
ously, discontinuously, and untranslated, respec-
tively. Fortunately, our Mixed RNN system is
able to have more continuous translation for those
phrases. Table 3 also suggests that there is still
much room for NMT to show more respect to syn-
tax.

5.4 Analysis on Over Translation
To estimate the over translation generated by
NMT, we propose ratio of over translation (ROT):

ROT =

∑
wi

t(wi)

|w| (1)

where |w| is the number of words in consider-
ation, t(wi) is the times of over translation for
word wi. Given a word w and its translation
e = e1e2 . . . en, we have:

t(w) = |e| − |uniq(e)| (2)

where |e| is the number of words in w’s transla-
tion e, while |uniq(e)| is the number of unique
words in e. For example, if a source word 香

694



System POS ROT (%)

RNNSearch

NR 15.7
CD 7.4
DT 4.9
NN 8.0

ALL 5.5

Mixed RNN

NR 12.3
CD 5.1
DT 2.4
NN 6.8

ALL 4.5

Table 4: Ratio of over translation (ROT) on test
sets. Here NR is for proper noun, CD for cardi-
nal number, DT for determiner, and NN for nouns
except proper nouns and temporal nouns.

港/xiangkang is translated as hong kong hong
kong, we say it being over translated 2 times.

Table 4 presents ROT grouped by some typical
POS tags. It is not surprising that RNNSearch sys-
tem has high ROT with respect to POS tags of NR
(proper noun) and CD (cardinal number): this is
due to the fact that the two POS tags include high
percentage of unknown words which tend to be
translated multiple times in translation. Words of
DT (determiner) are another source of over trans-
lation since they are usually translated to multiple
the in English. It also shows that by introducing
source syntax, Mixed RNN system alleviates the
over translation issue by 18%: ROT drops from
5.5% to 4.5%.

5.5 Analysis on Rare Word Translation

We analyze the translation of source-side rare
words that are mapped to a special token UNK.
Given a rare word w, we examine if it is translated
into a non-UNK word (non-UNK), UNK (UNK),
or if it is not translated at all (Un.).

Table 5 shows how source-side rare words are
translated. The four POS tags listed in the table
account for about 90% of all rare words in the test
sets. It shows that in Mixed RNN system is more
likely to translate source-side rare words into UNK
on the target side. This is reasonable since the
source side rare words tends to be translated into
rare words in the target side. Moreover, it is hard
to obtain its correct non-UNK translation when a
source-side rare word is replaced as UNK.

Note that our approach is compatible with with
approaches of open vocabulary. Taking the sub-

System POS non-UNK UNK Un.

RNNSearch

NN 27.2 40.4 32.4
NR 22.9 58.5 18.6
VV 34.5 32.9 32.7
CD 10.7 63.4 25.9

ALL 27.2 40.4 32.4

Mixed RNN

NN 24.8 41.4 33.8
NR 17.0 64.5 18.6
VV 33.6 34.0 32.3
CD 9.6 68.7 21.7

ALL 23.9 47.5 28.7

Table 5: Percentages (%) of rare words in our test
sets being translated into a non-UNK word (non-
UNK), UNK (UNK), or if it is not translated at all
(Un.).

word approach (Sennrich et al., 2016) as an exam-
ple, for a word on the source side which is divided
into several subword units, we can synthesize sub-
POS nodes that cover these units. For example, if
misunderstand/VB is divided into units of mis and
understand, we construct substructure (VB (VB-F
mis) (VB-I understand)).

6 Related Work

While there has been substantial work on lin-
guistically motivated SMT, approaches that lever-
age syntax for NMT start to shed light very re-
cently. Generally speaking, NMT can provide a
flexible mechanism for adding linguistic knowl-
edge, thanks to its strong capability of automati-
cally learning feature representations.

Eriguchi et al. (2016) propose a tree-to-
sequence model that learns annotation vectors not
only for terminal words, but also for non-terminal
nodes. They also allow the attention model to
align target words to non-terminal nodes. Our ap-
proach is similar to theirs by using source-side
phrase parse tree. However, our Mixed RNN sys-
tem, for example, incorporates syntax informa-
tion by learning annotation vectors of syntactic la-
bels and words stitchingly, but is still a sequence-
to-sequence model, with no extra parameters and
with less increased training time.

Sennrich and Haddow (2016) define a few lin-
guistically motivated features that are attached to
each individual words. Their features include lem-
mas, subword tags, POS tags, dependency labels,
etc. They concatenate feature embeddings with
word embeddings and feed the concatenated em-

695



beddings into the NMT encoder. On the contrast,
we do not specify any feature, but let the model
implicitly learn useful information from the struc-
tural label sequence.

Shi et al. (2016) design a few experiments to in-
vestigate if the NMT system without external lin-
guistic input is capable of learning syntactic infor-
mation on the source-side as a by-product of train-
ing. However, their work is not focusing on im-
proving NMT with linguistic input. Moreover, we
analyze what syntax is disrespected in translation
from several new perspectives.

Garcı́a-Martı́nez et al. (2016) generalize NMT
outputs as lemmas and morphological factors in
order to alleviate the issues of large vocabulary
and out-of-vocabulary word translation. The lem-
mas and corresponding factors are then used to
generate final words in target language. Though
they use linguistic input on the target side, they are
limited to the word level features. Phrase level, or
even sentence level linguistic features are harder
to obtain for a generation task such as machine
translation, since this would require incremental
parsing of the hypotheses at test time.

7 Conclusion

In this paper, we have investigated whether and
how source syntax can explicitly help NMT to im-
prove its translation accuracy.

To obtain syntactic knowledge, we linearize a
parse tree into a structural label sequence and
let the model automatically learn useful infor-
mation through it. Specifically, we have de-
scribed three different models to capture the syn-
tax knowledge, i.e., Parallel RNN, Hierarchi-
cal RNN, and Mixed RNN. Experimentation on
Chinese-to-English translation shows that all pro-
posed models yield improvements over a state-of-
the-art baseline NMT system. It is also interesting
to note that the simplest model (i.e., Mixed RNN)
achieves the best performance, resulting in obtain-
ing significant improvements of 1.4 BLEU points
on NIST MT 02 to 05.

In this paper, we have also analyzed the transla-
tion behavior of our improved system against the
state-of-the-art NMT baseline system from several
perspectives. Our analysis shows that there is still
much room for NMT translation to be consistent
with source syntax. In our future work, we expect
several developments that will shed more light on
utilizing source syntax, e.g., designing novel syn-

tactic features (e.g., features showing the syntactic
role that a word is playing) for NMT, and employ-
ing the source syntax to constrain and guild the
attention models.

Acknowledgments

The authors would like to thank three anony-
mous reviewers for providing helpful comments,
and also acknowledge Xing Wang, Xiangyu Duan,
Zhengxian Gong for useful discussions. This work
was supported by National Natural Science Foun-
dation of China (Grant No. 61525205, 61331011,
61401295).

References
Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben-

gio. 2015. Neural machine translation by jointly
learning to align and translate. In Proceedings of
ICLR 2015.

David Chiang. 2007. Hierarchical phrase-based trans-
lation. Computational Linguistics 33(2):201–228.

Kyunghyun Cho, Bart van Merrienboer, Dzmitry Bah-
danau, and Yoshua Bengio. 2014a. On the proper-
ties of neural machinetranslation: Encoder-decoder
approaches. In Proceedings of SSST 2014. pages
103–111.

Kyunghyun Cho, Bart van Merrienboer, Caglar Gul-
cehre, Dzmitry Bahdanau, Fethi Bougares, Holger
Schwenk, and Yoshua Bengio. 2014b. Learning
phrase representations using rnn encoder-decoder
for statistical machine translation. In Proceedings
of EMNLP 2014. pages 1724–1734.

Do Kook Choe and Eugene Charniak. 2016. Parsing
as language modeling. In Proceedings of EMNLP
2016. pages 2331–2336.

Chris Dyer, Adam Lopez, Juri Ganitkevitch, Jonathan
Weese, Ferhan Ture, Phil Blunsom, Hendra Seti-
awan, Vladimir Eidelman, and Philip Resnik. 2010.
cdec: A decoder, alignment, and learning framework
for finite-state and context-free translation models.
In Proceedings of ACL 2010 System Demonstra-
tions. pages 7–12.

Akiko Eriguchi, Kazuma Hashimoto, and Yoshimasa
Tsuruoka. 2016. Tree-to-sequence attentional neu-
ral machine translation. In Proceedings of ACL
2016. pages 823–833.

Mercedes Garcı́a-Martı́nez, Loic Barrault, and Fethi
Bougares. 2016. Factored neural machine transla-
tion. In arXiv:1609.04621.

Geoffrey E. Hinton, Nitish Srivastava, Alex
Krizhevsky, Ilya Sutskever, and Ruslan R. Salakhut-
dinov. 2012. Improving neural networks by

696



preventing co-adaptation of feature detectors. In
arXiv:1207.0580.

Sébastien Jean, Orhan Firat, Kyunghyun Cho, Roland
Memisevic, and Yoshua Bengio. 2015. Montreal
neural machine translation systems for wmt’15. In
Proceedings of WMT 2015. pages 134–140.

Philipp Koehn. 2004. Statistical significance tests for
machine translation evaluation. In Proceedings of
EMNLP 2004. pages 388–395.

Junhui Li, Philip Resnik, and Hal Daumé III. 2013.
Modeling syntactic and semantic structures in hier-
archical phrase-based translation. In Proceedings of
HLT-NAACL 2013. pages 540–549.

Yang Liu, Qun Liu, and Shouxun Lin. 2006. Tree-
to-string alignment template for statistical machine
translation. In Proceedings of ACL-COLING 2006.
pages 609–616.

Yang Liu and Maosong Sun. 2015. Contrastive unsu-
pervised word alignment with non-local features. In
Proceedings of AAAI 2015. pages 857–868.

Minh-Thang Luong and Christopher D. Manning.
2015. Stanford neural machine translation systems
for spoken language domains. In Proceedings of
IWSLT 2015. pages 76–79.

Minh-Thang Luong, Hieu Pham, and Christopher D.
Manning. 2015. Effective approaches to attention-
based neural machine translation. In Proceedings of
EMNLP 2015. pages 1412–1421.

Yuval Marton and Philip Resnik. 2008. Soft syntac-
tic constraints for hierarchical phrased-based trans-
lation. In Proceedings of ACL-HLT 2008. pages
1003–1011.

Haitao Mi, Zhiguo Wang, and Abe Ittycheriah. 2016.
Supervised attentions for neural machine translation.
In Proceedings of EMNLP 2016. pages 2283–2288.

Franz J. Och and Hermann Ney. 2003. A systematic
comparison of various statistical alignment models.
Computational Linguistics 29(1):19–51.

Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: A method for automatic
evaluation of machine translation. In Proceedings
of ACL 2002. pages 311–318.

Slav Petrov and Dan Klein. 2007. Improved inference
for unlexicalized parsing. In Proceedings of HLT-
NAACL 2007. pages 404–411.

Rico Sennrich and Barry Haddow. 2016. Linguistic
input features improve neural machine translation.
In Proceedings of the First Conference on Machine
Translation. pages 83–91.

Rico Sennrich, Barry Haddow, and Alexandra Birch.
2016. Neural machine translation of rare words with
subword units. In Proceedings of ACL 2016. pages
1715–1725.

Libin Shen, Jinxi Xu, and Ralph Weischedel. 2008. A
new string-to-dependency machine translation algo-
rithm with a target dependency language model. In
Proceedings of ACL-HLT 2008. pages 577–585.

Xing Shi, Inkit Padhi, and Kevin Knight. 2016. Does
string-based neural MT learn source syntax? In Pro-
ceedings of EMNLP 2016. pages 1526–1534.

Zhaopeng Tu, Yang Liu, Zhengdong Lu, Xiaohua Liu,
and Hang Li. 2017a. Context gates for neural ma-
chine translation. Transactions of the Association
for Computational Linguistics 5:87–99.

Zhaopeng Tu, Yang Liu, Lifeng Shang, Xiaohua Liu,
and Hang Li. 2017b. Neural machine translation
with reconstruction. In Proceedings of AAAI 2017.
pages 3097–3103.

Zhaopeng Tu, Zhengdong Lu, Yang Liu, Xiaohua Liu,
and Hang Li. 2016. Modeling coverage for neural
machine translation. In Proceedings of ACL 2016.
pages 76–85.

Oriol Vinyals, Lukasz Kaiser, Terry Koo, Slav Petrov,
Ilya Sutskever, and Geoffrey Hinton. 2015. Gram-
mar as a foreign language. In Proceedings of NIPS
2015.

Xing Wang, Zhengdong Lu, Zhaopeng Tu, Hang Li,
Deyi Xiong, and Min Zhang. 2017. Neural machine
translation advised by statistical machine transla-
tion. In Proceedings of AAAI 2017. pages 3330–
3336.

Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V.
Le, Mohammad Norouzi, Wolfgang Macherey,
Maxim Krikun, Yuan Cao, Qin Gao, Klaus
Macherey, Jeff Klingner, Apurva Shah, Melvin
Johnson, Xiaobing Liu, Lukasz Kaiser, Stephan
Gouws, Yoshikiyo Kato, Taku Kudo, Hideto
Kazawa, Keith Stevens, George Kurian, Nishant
Patil, Wei Wang, Cliff Young, Jason Smith, Jason
Riesa, Alex Rudnick, Oriol Vinyals, Greg Corrado,
Macduff Hughes, and Jeffrey Dean. 2016. Google’s
neural machine translation system: Bridging the gap
between human and machine translation. In arXiv
preprint arXiv:1609.08144.

Nianwen Xue, Fei Xia, Fu-Dong Chiou, and Martha
Palmer. 2005. The Penn Chinese Treebank: Phrase
structure annotation of a large corpus. Natural Lan-
guage Engineering 11(2):207–238.

Matthew D. Zeiler. 2012. ADADELTA: An adaptive
learning rate method. In arXiv:1212.5701.

697


	Modeling Source Syntax for Neural Machine Translation

