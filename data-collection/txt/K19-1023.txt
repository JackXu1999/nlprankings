



















































Representation Learning and Dynamic Programming for Arc-Hybrid Parsing


Proceedings of the 23rd Conference on Computational Natural Language Learning, pages 238–248
Hong Kong, China, November 3-4, 2019. c©2019 Association for Computational Linguistics

238

Representation Learning and Dynamic Programming for Arc-Hybrid
Parsing

Joseph Le Roux Antoine Rozenknop Mathieu Lacroix
Laboratoire d’Informatique de Paris Nord,

Université Paris 13 – SPC, CNRS UMR 7030,
F-93430, Villetaneuse, France

{leroux,rozenknop,lacroix}@lipn.fr

Abstract

We present a new method for transition-based
parsing where a solution is a pair made of a
dependency tree and a derivation graph de-
scribing the construction of the former. From
this representation we are able to derive an ef-
ficient parsing algorithm and design a neural
network that learns vertex representations and
arc scores. Experimentally, although we only
train via local classifiers, our approach im-
proves over previous arc-hybrid systems and
reach state-of-the-art parsing accuracy.

1 Introduction

While transition-based dependency parsing is usu-
ally implemented as a beam-search procedure,
e.g. (Kiperwasser and Goldberg, 2016), some re-
cent work such as (Shi et al., 2017) showed that
global inference can be performed efficiently with
dynamic programming. To this end, the stack rep-
resenting pending subparses and the buffer repre-
senting the unconsumed input must both be ab-
stracted into equivalence classes, while remaining
rich enough to help with accurate predictions.

In this paper we first explicitly consider that a
solution in transition-based parsing is represented
as a pair made of a derivation graph and a derived
dependency tree allowing the scoring function to
be expressed naturally as a sum over these 2 struc-
tures. While we restrict our presentation to arc-
hybrid systems, our method can be applied quite
directly to other transition rule systems.

Secondly we show that this representation leads
to an exact O(n4) parsing algorithm using dy-
namic programming. This algorithm can be seen
as an extension of the minimal feature set arc-
hybrid parsing algorithm presented in (Shi et al.,
2017) where the contribution of the dependency
arcs can be explicitly added to the scoring function
as in the Eisner parsing algorithm (Eisner, 1996).

We then propose an alternative approach to
global inference where derivation steps are repre-
sented as dense vectors based on the number and
type of steps in a derivation. With this abstrac-
tion we design a neural architecture based on non-
local networks (Wang et al., 2017) related to self-
attention mechanism (Vaswani et al., 2017; Gu
et al., 2018) to learn these representations while
maintaining the possibility for exact decoding.

Our contribution can be summarized as follows:
(i) a representation of arc-hybrid parsing as max-
imum subgraphs selection where a solution con-
tains dependencies and derivation information; (ii)
a polynomial dynamic programming algorithm to
solve this problem exactly; (iii) a neural architec-
ture able to learn representations for the subgraph
vertices and compute arc scores without explicit
stack and buffer representations.

These contributions are validated empirically
by experimental results on the Penn Treebank
where our system reaches state-of-the-art accu-
racy (94.8% UAS) for arc-hybrid parsing with net-
works of comparable size.

We first review arc-hybrid dependency parsing
(§2) then present a deductive scheme to solve it
(§3). The neural architecture is presented in §4
and experiments reported in §5. Finally we discuss
some related work in §6.

2 Arc-Hybrid Dependency Parsing

2.1 Arc-Hybrid Derivations

Intuitively, the arc-hybrid parsing strat-
egy (Gómez-Rodrı́guez et al., 2008; Kuhlmann
et al., 2011) builds dependency parses incremen-
tally by reading the sentence from left to right.
The pending words are words which have been
given all their left modifiers but may have not
been given all their right dependents yet. The
pending words are stored in the stack which



239

initially contains a dummy root word. The words
which have not been read yet are stored in order
in the buffer. The first word in the buffer is called
the frontier word.

The algorithm proceeds by reducing the most
recent pending word (the top of the stack) which
means assigning it a governor. This governor is
either the current frontier word, creating a left arc,
or the previously most recent pending word (be-
low in the stack), creating a right arc. Once given a
governor, the most recent pending word is popped
out. When the frontier word has been given all
his left dependents, it is shifted which means it
is pushed in the stack, becomes the most recent
pending word and the next word in the sentence
becomes the frontier word.

More formally, the arc-hybrid parsing algorithm
is defined using configurations. Following stan-
dard definition, a configuration is a triplet [σ, β,A]
where σ is a stack of indexes of words which have
not been given a governor yet, β a list (buffer) of
indexes of words still to be read1, and A the set of
dependency arcs that have been constructed so far.

Given sentence w = w1, ..., wn and dummy root
token w0, there are one initial configuration c1 =
[0, 1..., ∅] and many goal configurations of the form
[0, ∅, A]. There exist 3 transition rules to pass from
one configuration to another2:

shift [σ, b|β,A]→S [σ|b, β,A]

left [σ|d, h|β,A]→L [σ, h|β,A ∪ {(h, d)}]

right [σ|h|d, β,A]→R [σ|h, β,A ∪ {(h, d)}]

A derivation for w is a sequence γ =
c1, t1, c2, t2, . . . , t2n, c2n+1 of 2n transitions from
c1 leading to a goal configuration c2n+1.

The set of derivations for a sentence w is noted
Dw. It can be shown that derivations all gener-
ate projective dependency trees and that, for a sen-
tence with n words, they each contain n shift op-
erations and n left or right reductions3.

Shifts and reductions can be seen as forming a
well-parenthesized expression. Each word is as-
sociated with a kind of parenthesis and shifting
and reducing this word correspond to opening and
closing parentheses of this kind.

1A buffer containing (i, ..., n) will be denoted by “i...”.
2Shift and left transitions require a non-empty buffer and

the stack has to be of length at least 2 for left and right tran-
sitions since w0 cannot be the governor of a left arc.

3Note that derivation is not unique for a dependency tree.

For instance, in She wants to eat an apple, the
derivation (She)L(wants(to)L(eat(an)L(apple)R)R)R,
where shifting a word is represented by a sub-
scripted opening parenthesis and reductions are
closing parentheses typed either L or R, will gen-
erate the following dependency tree:

w0 She wants to eat an apple

Arc-hybrid parsing amounts to finding the
highest-scoring derivation for a sentence:

γ̂ = argmax
γ∈Dw

s(γ)

If we assume s decomposes over transition scores
s`(c`, t`), we retrieve the well-studied cumulative
sum of its transition scores.

s(γ) =
∑

1≤`≤2n
s`(c`, t`) (1)

2.2 General Formulation for Dynamic
Programming

We present a dynamic programming (DP) algo-
rithm for arc-hybrid parsing with cumulative tran-
sition scores as in Eq. 1. This algorithm cannot be
used as such since states references complete stack
contents, of which there is an exponential number,
leading to an intractable complexity. To use this
algorithm in practice we would need to resort to
beam search in order to approximate solutions, see
for instance (Dyer et al., 2015).

However, this constitutes a general framework
from which efficient algorithms can be derived
by considering various equivalence classes over
states and independence assumptions in the scor-
ing function. For instance we can retrieve the Min-
imal Feature Set algorithm of Shi et al. (2017), or
the new algorithm presented in Section 3.

In our algorithm, an item σ,A〈i, j〉B represents
the following set of subsequences of a derivation:

σ,A〈i, j〉B : [σ, i..., A] ∗−→ [σ|i, j..., B]

i.e. subsequences which start with shifting wi and
end withwi on top of the stack andwj on top of the
buffer4. As a special case, we will note ∅,∅〈0, j〉B
the set of subderivations starting from the initial
configuration [0, 1..., ∅] and leading to [0, j..., B].

4such a subderivation is only possible if i < j ≤ n+1
and A ⊂ B.



240

Goal: Goal items have the form:
∅,∅〈0, n+1〉A : [∅, 0..., ∅] ∗−→ [0, ∅, A].

Axioms: The algorithm starts with the set
of items corresponding to possible shifts5:
σ,A〈i, i+1〉A : [σ, i..., A] → [σ|i, i+1..., A]. The
first axiom6 ∅,∅〈0,1〉∅ pictures a dummy sub-
derivation that would put w0 on top of the stack
and lead to the initial configuration [0, 1..., ∅].

DP Steps: A DP step consists in building
an item σ,A〈i, j〉B by composing σ,A〈i, k〉C ,
σ|i,C〈k, j〉D, and a reduce operation on word wk:

σ,A〈i, k〉C : [σ, i..., A] ∗−→ [σ|i, k..., C]
σ|i,C〈k, j〉D : [σ|i, k..., C] ∗−→ [σ|i|k, j..., D]

reduce : [σ|i|k, j..., D] → [σ|i, j..., B]

We thus have a right reduction rule:

σ,A〈i, k〉C σ|i,C〈k, j〉D
σ,A〈i, j〉D∪{(i,k)}

[(i→ k)]

and a left reduction rule:

σ,A〈i, k〉C σ|i,C〈k, j〉D
σ,A〈i, j〉D∪{(k,j)}

[(k ← j)]

Scoring Items: We trivially score an axiom
σ,A〈i, i+1〉A with the score of a shift transition
occurring in configuration [σ, i..., A].

We compute the score of a DP step as the sum
of the scores of the combined items and the score
of reducing wk from configuration [σ|i|k, j..., D].
When several DP steps produce the same item, it
is assigned the highest score.

2.3 Minimal Feature Set Algorithm
For this algorithm (Shi et al., 2017), the score of
a transition t` does not depend on the whole con-
figuration but only on the index on top of the stack
and the first index of the buffer. In other words, lo-
cal scores s` only depend on word indexes (i,j,k).
This assumption is crude but it allows for quite
large items equivalence classes. We can retrieve
this algorithm from the one above by removing
unnecessary information in the items. Items of the
form σ,A〈i, j〉B will simply reduce to 〈i, j〉, there
will be O(n2) such items, and the DP complexity
will be O(n3). More concretely, we have the fol-
lowing schemata:

5There is one axiom for each possible configuration
[σ, i..., A] with 0 ≤ i < n, σ a valid stack and A the set
of corresponding arcs.

6Other axioms can be generated lazily from stack and arcs
set pairs of items created by DP steps.

Goal: 〈0, n+1〉.

Axioms: 〈i, i+1〉.

DP Steps:

〈i, k〉 〈k, j〉
〈i, j〉 [(i→ k)]

and
〈i, k〉 〈k, j〉

〈i, j〉 [(k ← j)]

One of the issues with this parsing scheme is
the difficulty to interpret item scores consistently.
In the case of a left reduction (producing k ← j
above) the score of 〈k, j〉 can be interpreted as the
score of word j being the governor of word k and
the score of 〈i, k〉 as the score of shifting word k
in the context of i being the most recent pending
word.

On the contrary, in the case of a right reduction
(producing i → k above) if the score of 〈i, k〉
may well be interpreted analogously as the score
of having i as a governor of k, we cannot interpret
〈k, j〉 as the score of shifting k, and it may be
difficult to interpret this item score in terms of a
transition operation.

3 Parsing with Derivations and
Dependencies

3.1 A New Score for Derivations
We depart form previous work and make the de-
pendency arc contribution explicit in the score
function:

γ̂, τ̂ = argmax
γ,τ∈Sw

s(γ) + s(τ) (2)

where Sw is the set of pairs (γ, τ) with γ ∈ Dw
and τ the dependency tree corresponding to γ. We
use an arc-factored model for τ from now on and
discuss scores for γ.

We define an equivalence class (i, q) contain-
ing all configurations c` such that the first index i`
of β` equals i and the size of stack |σ`| equals q.
Thus, we rewrite cumulative transition scores as:

s(u)(γ) =
∑

1≤`≤2n
su(i`, |σ`|, t`). (3)

We also consider a score function based on the
nestedness property of shift/reduce derivations. A
score is given to each pair consisting of a shift and
its corresponding (left or right) reduction. In other



241

words, we exploit the perfect matching induced
by a derivation between shift and reduce transi-
tions: if t` is a shift transition and t`′ its matching
reduction, we consider a score depending on the
equivalence classes of c` and c`′+1. Note that the
nestedness property implies that the stacks σ` and
σ`′+1 are equal. Moreover, i`′+1 = i`′ as t`′ is
a reduction, an operation which does not modify
the buffer. Denoting by M the set of matching
shift/reduce pairs (t`, t`′) in γ, this gives:

s(m)(γ) =
∑

(t`,t`′ )∈M

sm(i`, |σ`|, i`′). (4)

Note that sm(i`, |σ`|, i`′) can be seen as the
score of performing a reduction when i` is on the
top of the stack of size |σ`| and i`′ is the first word
position of the buffer. Hence, this gives a score
similar to the reduction score used in the minimal
feature set algorithm. The difference is that the
score takes into account the size of the stack but
not the type (left or right) of reduction that is per-
formed. The direction information will be given
by the dependency arc score.

Finally, the derivation score is:

s(γ) = s(u)(γ) + s(m)(γ) (5)

3.2 Graph Representation of a Derivation
In this section we represent the derivation of the
arc-hybrid parsing using graphs.

A derivation is a sequence of n shifts and n re-
ductions such that no more reductions than shifts
are performed at each step. Such a sequence is
a Dyck word and can then be represented as a
path in an (n + 1) × (n + 1) grid starting at the
lower left corner, ending at the lower right corner,
using only up-right diagonal arcs and downward
arcs (Roman, 2015). Such representation is the
starting point of our derivation graph.

Define the derivation graph G = (V,A) as fol-
lows. V = {vqi |1 ≤ q ≤ i ≤ n + 1} represents
the set of equivalence classes. A vertex vqi corre-
sponds to the class (i, q) of configurations, where
wi is the first word in the buffer7 and the stack is
of size q. The arc set A is given by A = T ∪ E
where T represents transitions between states and
E matches between shifts and reductions. We note
T = TS ∪ TL ∪ TR:

• TS = {(vqi , v
q+1
i+1 )|1 ≤ q ≤ i ≤ n},

7The value n+ 1 for i indicates that the buffer is empty.

• TL = {(vqi , v
q−1
i )

L|2 ≤ q ≤ i ≤ n},

• TR = {(vqi , v
q−1
i )

R|2 ≤ q ≤ i ≤ n+ 1}.

TS , TL and TR represent shifts and tagged reduc-
tions respectively. We set E = {(vqi , v

q
j )|1 ≤ q ≤

i < j ≤ n+ 1} because a shift can be matched to
a reduction only if the size of the stack is the same
before the shift and after the reduce. Note that for
a sentence with n words, G will have O(n2) ver-
tices and O(n3) arcs.

A derivation γ obtained by the arc-hybrid pars-
ing will be represented by a pair (P,M) where
P ⊆ T is a path representing the derivation γ and
M ⊆ E is the set of arcs matching the shifts with
their associated reductions in the derivation.

More formally, (P,M) is a solution if and only
if we have the following. P is a path from v11 to
v1n+1 in G using only arcs of T . By construction,
it contains n arcs of TS and n arcs of TL ∪ TR.
It then corresponds to the sequence of transitions
t1, . . . , t2n. Note that any arc (v

q
i , v

q+1
i+1 ) in P con-

sists in pushing wordwi on the top of the stack and
any arc (vqi , v

q−1
i ) in P consists in popping the top

of the stack. One can retrieve the sequence of con-
figurations c1, . . . , c2n+1 thanks to the transitions.

Remark that each vertex vqi covered by P cor-
responds to configuration c2i−q since i − 1 shifts
and i − q reduces have been performed. An arc
(vqi , v

q
j ) ofE belongs toM if the transition t2i−q is

a shift, transition t2j−q−1 is a reduction and these
shift and reduce operations are matched together.
Hence, (vqi , v

q
j ) ∈ M implies that (v

q
i , v

q+1
i+1 ) and

(vq+1j , v
q
j ) belong to P .

One can associate a score su from Eq. (3) with
each arc of T and a score sm from Eq. (4) with
each arc of E. In this case, s(γ) corresponds to

s(γ) =
∑
a∈P

su(a) +
∑
a∈M

sm(a) (6)

As an illustration, the derivation presented in
the introduction can be represented by the set of
black arcs P and red arcs M in Figure 1.

3.3 Dynamic Programming Algorithm

From the graph representation above we can de-
rive a DP algorithm which computes the score of
the optimal solution subgraph. This algorithm can
be seen as a specialization of the general frame-
work presented in Section 2.2 for our scoring func-
tions.



242

L

L

L R

R

R

w0 She wants to eat an apple

Figure 1: A Derivation for She wants to eat an apple.
Shifts (resp. reductions) are represented in black diag-
onal (resp. vertical) arcs. Red curved edges represent
matchings. Vertices are configuration classes vqi .

Since score functions su and sm take the size of
the stack into account, items with different (initial)
stack sizes cannot be equivalent. Hence, items of
the form q〈i, j〉 are needed, where q = |σ| rep-
resents the size of the stack before shifting wi.
Leaving out the resulting dependency arcs, both
reduction rules can be written with these equiva-
lent items:

q〈i, k〉 q+1〈k, j〉
q〈i, j〉

There areO(n3) equivalent items and the DP com-
plexity will be O(n4). Such items will be scored
in the following way :

• the score of an axiom q〈i, i+1〉 is
su(i, q, shift). Axioms appear as black
diagonal arcs (vqi , v

q+1
i+1 ) in Figure 1.

• In a DP step, the score of the reduce transition
is su(j, q+2, reduce). The transition is repre-
sented as a black vertical arc (vq+2j , v

q+1
j ) in

Figure 1.

• Finally the matching score in a DP step is
sm(k, q+1, j). This score corresponds to red
curved arcs (vq+1k , v

q+1
j ) in Figure 1.

Figure 2 depicts both reduction rules. An item
is represented as an arrow for the initial shift, and a
triangle for the well-nested part of the subderiva-
tion. A reduction builds a new item by extend-
ing the well-nested part of the left antecedent with
a new matching arc obtained from the right an-
tecedent and the new reduction arc.

A dependency arc is added to assign a head to
k, the midpoint of the reduction rule, depending
on the direction of the reduction.

vqi

vq+1i+1

q〈i, k〉

vq+1k

vq+1k

vq+2k+1

q+1〈k, j〉

vq+2j

q〈i, j〉

vqi

vq+1i+1

vq+1k

vq+2k+1 vq+2j

vq+1j

L/R

wi wk wj
LR

Figure 2: Illustration of reduction rules. Items are rep-
resented by a diagonal arc (first shift) and an horizontal
edge (well-nested part). When combining two items in
a reduction rule, a (curved) matching edge and a (verti-
cal) reduction arc are added. The type of the reduction
leads to a new dependency arc for the modifier wk.

4 Learning Derivation Scores

We first present our network architecture inspired
by recent work on self attention (Vaswani et al.,
2017; Wang et al., 2017) which is able to learn rep-
resentations of arc-hybrid configuration classes,
that we call step representations. These represen-
tations are then used to compute derivation scores.

Our network is an encoder/decoder. The en-
coder computes word and step representations in
the specific context of the sentence to be parsed,
while the decoder computes arc scores.

We borrow from the transformer layer (Vaswani
et al., 2017) the idea of global attention but extend
it to the case where the size of the output is dif-
ferent from the size of the input. This has already
been explored in (Gu et al., 2018) in the context
of Machine Translation. However our problem is
simpler because the size of the output is always
twice the size of the input, in other words we do
not have to estimate the size of the output.

4.1 Notation

A feed-forward layer is a sequence of an affine
transformation, a ReLU filter and a linear trans-
formation, i.e. FF(x) = V (max(0, (Wx + b))),
with V,W, b trainable parameters.



243

We call interpolation layers functions like
I(x,y) = C(x)·x+(1−C(x))·y whereC is a lin-
ear transformation followed by a sigmoid squash-
ing, and · denotes the component-wise product.

Combining the previous two, we define high-
way layers (Greff et al., 2017) as functions
H(x) = I(x, FF (x)), i.e. an interpolation of in-
put x and a feed-forward transformation of x.

We also make use of biaffine functions follow-
ing (Dozat and Manning, 2017) that we define as
functions of the formB(e, f) = e>M f+V e, with
matrix M and vector V learnable parameters.

4.2 Layer Structure

Each layer Li is the composition of two sublay-
ers Ai, computing a generalized attention, and Bi,
performing a feed-forward transformation. As is
the case in previous approaches, each sublayer is
followed by a layer normalization (Ba et al., 2016)
and a residual connection to prevent underflows.

In more details, each layer takes as input a se-
quence of size n of dense vectors of size d packed
as a matrix X in Rn×d and a query vector sequence
of size o, either equal to n or 2n depending on the
layer (see infra) packed as a matrix Y in Ro×d.
When X and Y are the same, we recover the self-
attention mechanism of the transformer layer. The
layer forward value is given by the following equa-
tions, where LN is a layer normalization:

ai(X,Y) = X+Ai(X,Y)

bi(X) = X+Bi(X)

Li(X,Y) = LN(bi(LN(ai(X,Y)))).

The first sublayer Ai computes for each output
position in Y a multi-head scaled dot-product at-
tention over input query Y and key/value X , with
m attention heads.

Ai(X,Y) =
m∑
h=1

(A(QhiY,K
h
i X, V

h
i X))

Each attention head A takes a query as input
queries, keys and values, and computes for each
query vector a convex combination of value vec-
tors, the coefficients of which are given by an op-
eration between the query and the key vectors:

A(Q,V,K) = softmax(µQK>)V

where softmax is applied row-wise and µ is a
smoothing factor between 0 and 1, which is set to
d−0.5, where d is the size of a query vector, fol-
lowing previous implementations of dot-product
attention (Luong et al., 2015).

The second sublayer Bi applies the same feed-
forward transformation to each element of the se-
quence of vectors returned by Ai.

4.3 Word and Position Embeddings
Each word in the train set is associated with a real
vector stored in a lookup table E. In order to cope
with unseen words, we follow (Kiperwasser and
Goldberg, 2016) and at training time words are
randomly UNK-ed with a probability inversely pro-
portional to their frequency in the train set.

Contrarily to recurrent networks such as
LSTMs, attention networks do not have a built-
in notion of position so it must be provided exter-
nally. In our systems, we have two types of po-
sitions, namely word positions and step positions.
We use position embeddings stored in lookup ta-
bles called respectively T and S.

4.4 Word Encoder and Dependency Scores
Our encoder is the composition of e self-attention
layers starting from word and position vectors.

X0 = [E(w1) + T (1); . . . ;E(wn) + T (n)]

Xi = Li(Xi−1, Xi−1), 1 ≤ i ≤ e

After encoding, we getXe that we interpret as a
sequence of contextualized word vectors. We can
use these vectors to predict arc scores. In the fol-
lowing, we use a biaffine function Bdep to define
the raw score between head at position i and mod-
ifier at position j: swi→wj = Bdep(Xe[i], Xe[j]).

These raw scores are used in two ways.First and
most obviously they score dependency arcs of the
derived tree in this model. Second, for each modi-
fier we use incoming arc scores to weigh potential
heads and compute an expected head vector. We
use normalized scores via softmax to interpolate
head vectors. As a result for each vector word
Xe[i] we obtain an expected head vector Ye[i],
which will be used hereafter.

4.5 Step Encoder
For steps, we distinguish the first layer from the
others. For the first layer, input is the sequence re-
turned by the last word encoder noted Xe and ex-
pected heads Ye, and the query is initialized with



244

the increasing sequence of valid step position em-
beddings called P = [S(1); . . . ;S(2n)].

D0 = Xe + Ye, P1 = P, D1 = Le+1(D0, P1)

For k > 1, we set Dk = Le+k(Dk−1, Dk−1).
Given sentence w, we note h(w) the sequence

of vectors returned by the last decoder layer.

4.6 Decoders as Local Classifiers

In this section we present how the score function
can be decomposed as local probabilities. Given a
sentence w, we assume the probability of a deriva-
tion γ is conditioned upon its corresponding de-
rived tree τ . This condition prevents inconsisten-
cies between τ and γ but plays no other role in the
scoring function. The probability of a derived tree
decomposes as independent head predictions com-
puted by a logistic regression over head scores.

p(γ, τ |w) =p(γ|w, τ)× p(τ |w)

=p(γ|w, τ)×
∏

h→m∈τ
p(h|m,w)

The probability of a derivation in Dw is the
probability at each independent step ` of 2 events:
the transition t` and the step position r` of the cor-
responding reduction for a shift, or a dummy posi-
tion for a reduction. We consider these two events
to be independent but requiring the knowledge of
the difference q` between the number of shifts and
reductions already performed before the current
step. This difference can also be interpreted in the
context of the arc-hybrid algorithm as the depth of
the stack. The difference is then used as a param-
eter for the potentials. This gives:

p(γ|w) =
2|w|∏
`=1

p(t`, r`, q`|w, `)

=

2|w|∏
`=1

pd(q`|w, `)× p(t`, r`|w, `, q`)

=

2|w|∏
`=1

pd(q`|w, `)

× pu(t`|w, `, q`)× pm(r`|w, `, q`)

The condition on sentence and step index w, `
is implemented via functions taking the `th step
representation of w computed as described in the

previous section, denoted h(w)` or simply h` if
the sentence is clear from the context.

In practice we restrict the set of values for q` as
the set of natural numbers between one and nine,
and a special value for differences greater or equal
to ten8. We use a lookup table F to convert dis-
crete difference values to dense representations.

Note that although the 3 distributions are con-
ditioned on the same step representations intro-
duced in the previous section, these represen-
tations are first passed through highway layers,
{Hi}i=d,u,m, parameterized for each distribution.
This helps with the specialization of step repre-
sentations while keeping the possibility to share
information between tasks.

The first two distributions are categorical dis-
tributions of the exponential family. They are
computed as normalized potentials given by feed-
forward transformations of steps and differences.

pd(q|h`) ∝ expFFd(F (q) +Hd(h`)),
pu(t|q, h`) ∝ expFFt(F (q) +Hu(h`)).

The third distribution is computed with a bi-
affine function Bm followed by a softmax, as
in (Dozat and Manning, 2017). We reserve an ex-
tra value for the result random variable which en-
codes the absence of corresponding reduction.This
is used when s is not a shift step. Note the differ-
ence embedding is only used on the left side ofM .

pm(r|h`, q) ∝ expBm(F (q) +Hm(h`), h`+r)

Learning is performed by simply maximizing
the conditional log-likelihood of the 4 distribu-
tions over the correct derivations given a set of
sentences. Once parameterized, it is straightfor-
ward to see how these distributions can fit the
score model of Equation 5.

Conditional log-likelihood minimization re-
quires to compute values for each distribution
along gold solutions, which means it is a O(n2)
procedure, because of distributions on dependency
arcs and matching transitions, which both require
2 position parameters in order to be computed.

5 Experiments

Data We ran experiments on the Wall Street
sections of the English Penn Treebank (Marcus

8We found almost all train sentences could be parsed with
stack size below 10.



245

POS embedding size 28
Other embedding size 100
Encoder input/output size 256
Decoder input/output size 256
Hidden layer size in B subnetworks 512
attention heads 8
Maximum step numbers 200
Maximum difference D (stack height) 10
Number of word encoder layers 4
Number of step encode layers 4

Table 1: Network hyperparameters

et al., 1994) converted to Stanford Dependencies
(de Marneffe and Manning, 2008). Transition se-
quences were obtained from dependencies and in
case of ambiguity right reductions were always
performed before shift if possible. We followed
the standard split (02-21 for training, 23 for test-
ing and 22 for development purposes) and used
POS tags predicted by the Stanford MaxEnt tagger
trained using 10-way jackknifing (Toutanova et al.,
2003). We evaluate on unlabeled attachment with-
out punctuation, with CoNLL evaluation script.

Implementation Hyperparameters are given in
Table 1. In addition to word embeddings parame-
terized on the PTB, we use pretrained Glove vec-
tors (Pennington et al., 2014). Our prototype is
written in c++ with DYNET9 for neural compu-
tations (Adam optimizer and default values) and
UDPIPE10 for reading and writing data files. Mini-
batches contain around 1,000 tokens. We train
each model for 100 epochs and select our best
model according to its development UAS. We fol-
low previous works with transformers to set the
learning rate (Vaswani et al., 2017). For the first
8, 000 updates the learning increases linearly with
the number of steps, then it decreases proportion-
ally to the squared root of the number of steps. We
use the formula of Strubell et al. (2018).

Results Results on the PTB test set are presented
in Table 2 and comparisons with previous work
on arc-hybrid parsing with comparable network
sizes. Parsing results are obtained by averaging
5 models initialized with different random seeds
and standard deviation is also provided. Our sys-
tem reaches 94.8% UAS and parses the whole sec-
tion 23 in 1.13 seconds (DP only) on an Intel Xeon
2.10 GHz. Although our system is trained via local
classifiers, we can see that it improves over global

9https://github.com/clab/dynet
10http://ufal.mff.cuni.cz/udpipe

Setting UAS
Ours 94.82± 0.10
Ours, joint training, but decoding with

dependency scores only 94.80± 0.09
derivation scores only 93.95

Ours, training and decoding with
dependency scores only 94.73± 0.06
derivation scores only (no stack size) 84.81

(Shi et al., 2017) best local (4 features) 93.89
(Shi et al., 2017) global (2 features) 94.43
(Shi et al., 2017) global Eisner 94.50
(Kiperwasser and Goldberg, 2016) greedy 93.8

Table 2: Comparisons on PTB test set

systems trained without step encodings.
Ablations indicate that the major part of scoring

comes from dependencies. We may also conclude
that (i) derivation information is useful per se but
most importantly as an auxiliary task to improve
dependencies and (ii) the stack size is paramount
in this model since otherwise the network has no
indication of the stack content. If not considered,
accuracy drop considerably: step indexes alone
are too vague as they can correspond to many dif-
ferent stack and buffer contents.

6 Discussion and Related Work

Derivation Parsing Maximum subgraph selec-
tion has played a central role in dependency pars-
ing since the MST reduction by McDonald et al.
(2005) and can also be traced back to the parsing-
as-intersection tradition in phrase-based parsing –
see for instance (Billot and Lang, 1989) – where
the goal is to find, starting from a generic gram-
mar, a graph-structure (a shared forest) that recog-
nizes the input presented as a string or an automa-
ton. In dependency parsing, this approach has
since been extended to more complex dependen-
cies such as non-crossing and 1-endpoint-crossing
dependencies (Kuhlmann and Jonsson, 2015; Cao
et al., 2017).

There is a long line of research which solve the
different variants of transition-based dependency
parsing algorithms with dynamic programming.
Recent work showed that this can be performed ef-
ficiently, inO(n3), for arc-hybrid parsers (Gómez-
Rodrı́guez et al., 2008) and have since been ex-
tended with non-linear classifiers (Kiperwasser
and Goldberg, 2016; Shi et al., 2017) to reach
state-of-the-art parsing accuracy.

We depart from both in the following way. In
most works on maximum subgraph selection, the
class of valid subgraph is a class defined only by

https://github.com/clab/dynet
http://ufal.mff.cuni.cz/udpipe


246

properties on dependencies. Here we represent
derivations instead of derived structures. In that
respect, we are closer to approaches developed for
mildly-context sensitive formalisms such as Tree
Adjoining Grammars which work primarily on the
derivation tree (Corro et al., 2017) and consider
the derived tree, i.e. the parse structure, as a by-
product. Compared to other dynamic program-
ming approaches to arc-hybrid parsing, we there-
fore work on a richer model, and have more ex-
pressive power to take a representation of states
into account in the scoring scheme. This comes
at a cost since the time complexity of our pars-
ing algorithm is O(n4), an order of magnitude
higher. The stack information (size) is minimal
and is used to parameterize access to information
available from step embeddings.

Compared to joint parsing systems working on
both constituents and dependencies, our approach
doesn’t require external linguistic knowledge such
as head percolation rules. On the other hand, since
derivations don’t add new information, but merely
offer a new vision of the problem, the potential
accuracy gain is lower.

Machine Learning Aspects Self-attention net-
works have been used in parsing, see for instance
(Kitaev and Klein, 2018), whether based on de-
pendencies or syntagms. Curiously we found few
models of transition-based parsing based on these
networks, and bidirectional recurrent network are
still preferred in most architectures, where they
are believed to capture some information about the
sequential nature of transition-based algorithms.
Instead we present a non-sequential model of
transition-based parsing where representation vec-
tors are obtained via unrolled iterative estimation
(Greff et al., 2017).

Our encoder-decoder architecture together with
independence assumptions made in the probabilis-
tic model which decomposes a derivation score in
several subtasks can be seen as auxiliary tasks as
in (Coavoux et al., 2018).

The use of expected head vectors as input of
the step encoder is related to the syntactic head at-
tention of the SRL neural architecture in (Strubell
et al., 2018).

7 Conclusion

We presented the arc-hybrid parsing transition
rule system as a subgraph selection problem and
showed how this can be solved exactly by a dy-

namic programming algorithm. This theoretical
result is backed up by state-of-the-art results on
the PTB.

This new representation of the problem is the
basis of a novel neural architecture which learns
vertex representations (for derivation steps) and
edge scores (for derivation features).

From a parsing perspective, understanding why
derivation prediction is a good auxiliary task to
learn syntactic dependencies could prove insight-
ful to explain how transitions and dependencies
are related.

The derivation/derived pair is a very powerful
concept that remains to be fully exploited. In par-
ticular, there are two promising avenues for fu-
ture improvements. First, the learning framework
could be enriched in a setting where the derivation
graph is modeled as a latent variable and marginal-
ized over. It remains to be seen if this can be done
exactly or if sampling is required for efficiency.
Second, since the score of a solution is the sum of
the scores of elements in a pair, it should be pos-
sible to design an approximate solver based on la-
grangian decomposition more efficient in practice.

Acknowledgments

This work is partially supported by a public grant
overseen by the French National Research Agency
(ANR) as part of the program Investissements
d’Avenir (ANR-10-LABX-0083). It contributes
to the IdEx Université de Paris (ANR-18-IDEX-
0001). This work is partially supported by a pub-
lic grant overseen by the French ANR (ANR-16-
CE33-0021).

References
Lei Jimmy Ba, Ryan Kiros, and Geoffrey E. Hinton.

2016. Layer normalization. CoRR, abs/1607.06450.

Sylvie Billot and Bernard Lang. 1989. The structure of
shared forests in ambiguous parsing. In Proceedings
of the 27th annual meeting on Association for Com-
putational Linguistics, pages 143–151. Association
for Computational Linguistics.

Junjie Cao, Sheng Huang, Weiwei Sun, and Xiao-
jun Wan. 2017. Quasi-second-order parsing for 1-
endpoint-crossing, pagenumber-2 graphs. In Pro-
ceedings of the 2017 Conference on Empirical Meth-
ods in Natural Language Processing, pages 24–34.
Association for Computational Linguistics.

Maximin Coavoux, Shashi Narayan, and Shay B. Co-
hen. 2018. Privacy-preserving neural representa-

http://arxiv.org/abs/1607.06450
http://www.aclweb.org/anthology/P89-1018
http://www.aclweb.org/anthology/P89-1018
https://doi.org/10.18653/v1/D17-1003
https://doi.org/10.18653/v1/D17-1003
http://aclweb.org/anthology/D18-1001


247

tions of text. In Proceedings of the 2018 Confer-
ence on Empirical Methods in Natural Language
Processing, pages 1–10. Association for Computa-
tional Linguistics.

Caio Corro, Joseph Le Roux, and Mathieu Lacroix.
2017. Efficient discontinuous phrase-structure pars-
ing via the generalized maximum spanning arbores-
cence. In Proceedings of the 2017 Conference on
Empirical Methods in Natural Language Process-
ing, pages 1644–1654. Association for Computa-
tional Linguistics.

Timothy Dozat and Christopher D. Manning. 2017.
Deep biaffine attention for neural dependency pars-
ing. In International Conference on Learning Rep-
resentations.

Chris Dyer, Miguel Ballesteros, Wang Ling, Austin
Matthews, and Noah A. Smith. 2015. Transition-
based dependency parsing with stack long short-
term memory. In Proceedings of the 53rd Annual
Meeting of the Association for Computational Lin-
guistics and the 7th International Joint Conference
on Natural Language Processing (Volume 1: Long
Papers), pages 334–343, Beijing, China. Associa-
tion for Computational Linguistics.

Jason M. Eisner. 1996. Three new probabilistic models
for dependency parsing: An exploration. In COL-
ING 1996 Volume 1: The 16th International Confer-
ence on Computational Linguistics.

Carlos Gómez-Rodrı́guez, John Carroll, and David
Weir. 2008. A deductive approach to dependency
parsing. In Proceedings of ACL-08: HLT, pages
968–976, Columbus, Ohio. Association for Compu-
tational Linguistics.

Klaus Greff, Rupesh Kumar Srivastava, and Jürgen
Schmidhuber. 2017. Highway and residual net-
works learn unrolled iterative estimation. In Inter-
national Conference on Learning Representations.

Jiatao Gu, James Bradbury, Caiming Xiong, Victor OK
Li, and Richard Socher. 2018. Non-autoregressive
neural machine translation. In International Confer-
ence on Learning Representations.

Eliyahu Kiperwasser and Yoav Goldberg. 2016. Sim-
ple and accurate dependency parsing using bidirec-
tional lstm feature representations. Transactions
of the Association for Computational Linguistics,
4:313–327.

Nikita Kitaev and Dan Klein. 2018. Constituency
parsing with a self-attentive encoder. In Proceed-
ings of the 56th Annual Meeting of the Association
for Computational Linguistics (Volume 1: Long Pa-
pers), pages 2676–2686. Association for Computa-
tional Linguistics.

Marco Kuhlmann, Carlos Gómez-Rodrı́guez, and Gior-
gio Satta. 2011. Dynamic programming algorithms

for transition-based dependency parsers. In Pro-
ceedings of the 49th Annual Meeting of the Asso-
ciation for Computational Linguistics: Human Lan-
guage Technologies, pages 673–682, Portland, Ore-
gon, USA. Association for Computational Linguis-
tics.

Marco Kuhlmann and Peter Jonsson. 2015. Parsing to
noncrossing dependency graphs. Transactions of the
Association for Computational Linguistics, 3:559–
570.

Thang Luong, Hieu Pham, and Christopher D. Man-
ning. 2015. Effective approaches to attention-based
neural machine translation. In Proceedings of the
2015 Conference on Empirical Methods in Natural
Language Processing, pages 1412–1421. Associa-
tion for Computational Linguistics.

Mitchell Marcus, Grace Kim, Mary Ann
Marcinkiewicz, Robert MacIntyre, Ann Bies,
Mark Ferguson, Karen Katz, and Britta Schas-
berger. 1994. The penn treebank: annotating
predicate argument structure. In HLT’94: Pro-
ceedings of the workshop on Human Language
Technology, pages 114–119, Morristown, NJ, USA.
Association for Computational Linguistics.

Marie-Catherine de Marneffe and Christopher D. Man-
ning. 2008. Stanford typed dependencies manual.
Technical report, Stanford University.

Ryan McDonald, Fernando Pereira, Kiril Ribarov, and
Jan Hajic. 2005. Non-projective dependency pars-
ing using spanning tree algorithms. In Proceed-
ings of Human Language Technology Conference
and Conference on Empirical Methods in Natural
Language Processing, pages 523–530, Vancouver,
British Columbia, Canada. Association for Compu-
tational Linguistics.

Jeffrey Pennington, Richard Socher, and Christopher
Manning. 2014. Glove: Global vectors for word
representation. In Proceedings of the 2014 Con-
ference on Empirical Methods in Natural Language
Processing (EMNLP), pages 1532–1543. Associa-
tion for Computational Linguistics.

Steven Roman. 2015. An Introduction to Catalan Num-
bers, 1st edition. Birkhäuser Basel.

Tianze Shi, Liang Huang, and Lillian Lee. 2017.
Fast(er) exact decoding and global training for
transition-based dependency parsing via a minimal
feature set. In Proceedings of the 2017 Confer-
ence on Empirical Methods in Natural Language
Processing, pages 12–23. Association for Compu-
tational Linguistics.

Emma Strubell, Patrick Verga, Daniel Andor,
David Weiss, and Andrew McCallum. 2018.
Linguistically-informed self-attention for semantic
role labeling. In Proceedings of the 2018 Confer-
ence on Empirical Methods in Natural Language
Processing, pages 5027–5038. Association for
Computational Linguistics.

http://aclweb.org/anthology/D18-1001
https://doi.org/10.18653/v1/D17-1172
https://doi.org/10.18653/v1/D17-1172
https://doi.org/10.18653/v1/D17-1172
https://web.stanford.edu/~tdozat/files/TDozat-ICLR2017-Paper.pdf
https://web.stanford.edu/~tdozat/files/TDozat-ICLR2017-Paper.pdf
http://www.aclweb.org/anthology/P15-1033
http://www.aclweb.org/anthology/P15-1033
http://www.aclweb.org/anthology/P15-1033
https://www.aclweb.org/anthology/C96-1058
https://www.aclweb.org/anthology/C96-1058
http://www.aclweb.org/anthology/P/P08/P08-1110
http://www.aclweb.org/anthology/P/P08/P08-1110
http://arxiv.org/abs/1612.07771
http://arxiv.org/abs/1612.07771
http://aclweb.org/anthology/Q16-1023
http://aclweb.org/anthology/Q16-1023
http://aclweb.org/anthology/Q16-1023
http://aclweb.org/anthology/P18-1249
http://aclweb.org/anthology/P18-1249
http://www.aclweb.org/anthology/P11-1068
http://www.aclweb.org/anthology/P11-1068
https://transacl.org/ojs/index.php/tacl/article/view/709
https://transacl.org/ojs/index.php/tacl/article/view/709
https://doi.org/10.18653/v1/D15-1166
https://doi.org/10.18653/v1/D15-1166
http://aclweb.org/anthology/H/H94/H94-1020.pdf
http://aclweb.org/anthology/H/H94/H94-1020.pdf
http://www.aclweb.org/anthology/H/H05/H05-1066
http://www.aclweb.org/anthology/H/H05/H05-1066
https://doi.org/10.3115/v1/D14-1162
https://doi.org/10.3115/v1/D14-1162
http://aclweb.org/anthology/D17-1002
http://aclweb.org/anthology/D17-1002
http://aclweb.org/anthology/D17-1002
http://aclweb.org/anthology/D18-1548
http://aclweb.org/anthology/D18-1548


248

Kristina Toutanova, Dan Klein, Christopher D. Man-
ning, and Yoram Singer. 2003. Feature-rich part-of-
speech tagging with a cyclic dependency network.
In Proceedings of the 2003 Human Language Tech-
nology Conference of the North American Chapter
of the Association for Computational Linguistics.

Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob
Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz
Kaiser, and Illia Polosukhin. 2017. Attention is all
you need. In Advances in Neural Information Pro-
cessing Systems, pages 5998–6008.

Xiaolong Wang, Ross Girshick, Abhinav Gupta, and
Kaiming He. 2017. Non-local neural networks.
arXiv preprint arXiv:1711.07971, 10.

http://aclweb.org/anthology/N03-1033
http://aclweb.org/anthology/N03-1033

