



















































Recurrent Neural Network based Translation Quality Estimation


Proceedings of the First Conference on Machine Translation, Volume 2: Shared Task Papers, pages 787–792,
Berlin, Germany, August 11-12, 2016. c©2016 Association for Computational Linguistics

Recurrent Neural Network based Translation Quality Estimation

Hyun Kim
Creative IT Engineering,

Pohang University of Science and
Technology (POSTECH),

Pohang, Republic of Korea
hkim.postech@gmail.com

Jong-Hyeok Lee
Computer Science and Engineering,
Pohang University of Science and

Technology (POSTECH),
Pohang, Republic of Korea
jhlee@postech.ac.kr

Abstract

This paper describes the recurrent neural
network based model for translation qual-
ity estimation. Recurrent neural network
based quality estimation model consists of
two parts. The first part using two bidi-
rectional recurrent neural networks gener-
ates the quality information about whether
each word in translation is properly trans-
lated. The second part using another re-
current neural network predicts the final
quality of translation. We apply this model
to sentence, word and phrase level of
WMT16 Quality Estimation Shared Task.
Our results achieve the excellent perfor-
mance especially in sentence and phrase-
level QE.

1 Introduction

We introduce the recurrent neural network based
quality estimation (QE) model for predicting the
sentence, word and phrase-level translation quali-
ties, without relying on manual efforts to find QE
related features.

Existing QE researches have been usually fo-
cused on finding desirable QE related features
to use machine learning algorithms. Recently,
however, there have been efforts to apply neu-
ral networks to QE and these neural approaches
have shown potential for QE. Shah et al. (2015)
use continuous space language model features for
sentence-level QE and word embedding features
for word-level QE, in combination with other fea-
tures produced by QuEst++ (Specia et al., 2015).
Kreutzer et al. (2015) apply neural networks using
pre-trained alignments and word lookup-table to
word-level QE, which achieve the excellent per-
formance by using the combination of baseline

features at word level. However, these are not
‘pure’ neural approaches for QE.

Kim and Lee (2016) apply neural machine
translation (NMT) models, based on recurrent
neural network, to sentence-level QE. This is the
first try of using NMT models for the translation
quality estimation. This recurrent neural network
based quality estimation model is a pure neural ap-
proach for QE and achieves a competitive perfor-
mance in sentence-level QE (English-Spanish).

In this paper, we extend the recurrent neural
network based quality estimation model to word
and phrase level. Also, we apply this model to
sentence, word and phrase-level QE shared task
(English-German) of WMT16.

2 Recurrent Neural Network based
Quality Estimation Model

Recurrent neural network (RNN) based quality es-
timation model (Kim and Lee, 2016) consists of
two parts: two bidirectional RNNs on the source
and target sentences in the first part and another
RNN for predicting the quality in the second part.

In the first part (Figure 1), modified RNN-based
NMT model generates quality vectors, which in-
dicate a sequence of vectors about target words’
translation qualities. Each quality vector for each
target word has, as not a number unit but a vector
unit, the quality information about whether each
target word is properly translated from source sen-
tence. Each quality vector is generated by decom-
posing the probability of each target word from
the modified NMT model.1 Kim and Lee (2016)
modify the NMT model to 1) use source and target

1Existing NMT models (Cho et al., 2014; Bahdanau et al.,
2015) use RNNs on source and target sentences to predict the
probability of target word.

787



   POSTECH 

A Recurrent Neural Networks Approach 
for Estimating the Quality of Machine Translation Output 

𝒒𝒒𝒚𝒚𝒋𝒋 𝒒𝒒𝒚𝒚𝟏𝟏 𝒒𝒒𝒚𝒚𝟐𝟐 𝒒𝒒𝒚𝒚𝑻𝑻𝒚𝒚  

𝑏𝑏𝑜𝑜𝑏𝑏𝑜𝑜𝑜𝑜𝑏𝑏 

𝑞𝑞𝑜𝑜𝑏𝑏𝑏𝑏𝑏𝑏𝑏𝑏𝑦𝑦 𝑣𝑣𝑏𝑏𝑏𝑏𝑏𝑏𝑏𝑏𝑏𝑏𝑣𝑣 

𝑏𝑏𝑏𝑏𝑏𝑏𝑏𝑏𝑏𝑏𝑏𝑏𝑏𝑏𝑏𝑏𝑏𝑏𝑏𝑏𝑏𝑏𝑏𝑏𝑏𝑏 𝑅𝑅𝑅𝑅𝑅𝑅  
 ℎ𝑏𝑏𝑏𝑏𝑏𝑏𝑏𝑏𝑏𝑏 𝑣𝑣𝑏𝑏𝑏𝑏𝑏𝑏𝑏𝑏𝑣𝑣 
 

𝒉𝒉𝟏𝟏 𝒉𝒉𝟐𝟐 𝒉𝒉𝒋𝒋 𝒉𝒉𝑻𝑻𝒚𝒚  ⋯ ⋯ 

𝒒𝒒𝒚𝒚𝒋𝒋 𝒒𝒒𝒚𝒚𝟏𝟏 𝒒𝒒𝒚𝒚𝟐𝟐 𝒒𝒒𝒚𝒚𝑻𝑻𝒚𝒚  

𝑏𝑏𝑜𝑜𝑏𝑏𝑜𝑜𝑜𝑜𝑏𝑏 

𝑞𝑞𝑜𝑜𝑏𝑏𝑏𝑏𝑏𝑏𝑏𝑏𝑦𝑦 𝑣𝑣𝑏𝑏𝑏𝑏𝑏𝑏𝑏𝑏𝑏𝑏𝑣𝑣 

𝒔𝒔𝟏𝟏 𝒔𝒔𝟐𝟐 𝒔𝒔𝒋𝒋 𝒔𝒔𝑻𝑻𝒚𝒚  

⋯ 

⋯ 

⋯ 

⋯ 

𝒔𝒔𝟏𝟏 𝒔𝒔𝟐𝟐 𝒔𝒔𝒋𝒋 𝒔𝒔𝑻𝑻𝒚𝒚  

𝑸𝑸𝑸𝑸𝒘𝒘𝒋𝒋 𝑸𝑸𝑸𝑸𝒘𝒘𝟏𝟏  𝑸𝑸𝑸𝑸𝒘𝒘𝟐𝟐 𝑸𝑸𝑸𝑸𝒘𝒘𝑻𝑻𝒚𝒚  ⋯ ⋯ 𝑸𝑸𝑸𝑸𝒘𝒘𝒋𝒋 𝑸𝑸𝑸𝑸𝒘𝒘𝟏𝟏  𝑸𝑸𝑸𝑸𝒘𝒘𝟐𝟐 𝑸𝑸𝑸𝑸𝒘𝒘𝑻𝑻𝒚𝒚  ⋯ ⋯ 

ℎ𝑏𝑏𝑏𝑏𝑏𝑏𝑏𝑏𝑏𝑏 𝑣𝑣𝑏𝑏𝑏𝑏𝑏𝑏𝑏𝑏𝑣𝑣 

⋯ ⋯ ⋯ ⋯ 

𝒔𝒔𝒔𝒔𝒔𝒔𝒔𝒔𝒔𝒔𝒔𝒔 𝒔𝒔𝒔𝒔𝒔𝒔𝒔𝒔𝒔𝒔𝒔𝒔𝒔𝒔𝒔𝒔 (𝐱𝐱) 
𝑥𝑥1 𝑥𝑥2 𝑥𝑥𝑖𝑖 𝑥𝑥𝑇𝑇𝑥𝑥 ⋯ ⋯ 

𝒔𝒔𝒂𝒂𝒔𝒔𝒂𝒂𝒔𝒔𝒔𝒔 𝒔𝒔𝒔𝒔𝒔𝒔𝒔𝒔𝒔𝒔𝒔𝒔𝒔𝒔𝒔𝒔 (𝐲𝐲) 
𝑦𝑦1 𝑦𝑦2 𝑦𝑦𝑗𝑗 𝑦𝑦𝑇𝑇𝑦𝑦 ⋯ ⋯ 

 

𝑏𝑏𝑏𝑏𝑏𝑏𝑏𝑏𝑏𝑏𝑏𝑏𝑏𝑏𝑏𝑏𝑏𝑏𝑏𝑏𝑏𝑏𝑏𝑏𝑏𝑏 𝑅𝑅𝑅𝑅𝑅𝑅  
 

 

𝑏𝑏𝑏𝑏𝑏𝑏𝑏𝑏𝑏𝑏𝑏𝑏𝑏𝑏𝑏𝑏𝑏𝑏𝑏𝑏𝑏𝑏𝑏𝑏𝑏𝑏 𝑅𝑅𝑅𝑅𝑅𝑅  
 

𝒒𝒒𝒚𝒚𝒋𝒋 𝒒𝒒𝒚𝒚𝟏𝟏 𝒒𝒒𝒚𝒚𝟐𝟐 𝒒𝒒𝒚𝒚𝑻𝑻𝒚𝒚  ⋯ ⋯ 
𝒒𝒒𝒔𝒔𝒂𝒂𝒒𝒒𝒒𝒒𝒔𝒔𝒚𝒚 𝒗𝒗𝒔𝒔𝒔𝒔𝒔𝒔𝒔𝒔𝒔𝒔𝒔𝒔 

Figure 1: First part of recurrent neural network based quality estimation model for generating quality
vectors (Kim and Lee, 2016)

sentences as inputs,2 2) apply bidirectional RNNs
both on source and target sentences, which en-
able to fully utilize the bidirectional quality infor-
mation, and 3) generate quality vectors for target
words as outputs.

In the second part (Figure 2, 3 and 4), the final
quality of translation at various level (sentence-
level/word-level/phrase-level) is predicted by us-
ing the quality vectors as inputs. Kim and Lee
(2016) apply RNN based model to sentence-level
QE and we extend this model to word and phrase-
level QE. In subsection 2.1, 2.2 and 2.3, we de-
scribe the RNN based3 (second part) sentence,
word and phrase-level QE models.4

The cause of these separated parts of the
QE model comes from the insufficiency of QE
datasets to train the whole QE model. Thus, the
QE model is divided into two parts, and then
different training data are used to train each of
the separated parts: large-scale parallel corpora
such as Europarl for training the first part and QE
datasets, provided in Quality Estimation Shared
Task of WMT, for training the second part.

2.1 RNN based Sentence-level QE Model

In RNN based sentence-level QE model (Fig-
ure 2), HTER (human-targeted translation edit
rate) (Snover et al., 2006) in [0,1] for target sen-
tence is predicted by using a logistic sigmoid func-

2In MT/NMT, only source sentence is used as a input. In
QE, however, both source and target sentences can be used as
inputs.

3In all activation functions of RNN, the gated hidden unit
(Cho et al., 2014) is used to learn long-term dependencies.

4We, also, apply feedforward neural network (FNN) to
the second part of QE model (see Appendix A). However, to
reflect the dependencies between quality vectors and to fully
utilize QE related information from QE datasets, we focus on
the RNN based model.

POSTECH

A Recurrent Neural Networks Approach 
for Estimating the Quality of Machine Translation Output 

𝒔𝒔𝒔𝒔𝒔𝒔𝒔𝒔𝒔𝒔𝒔𝒔 𝒔𝒔𝒔𝒔𝒔𝒔𝒔𝒔𝒔𝒔𝒔𝒔𝒔𝒔𝒔𝒔 (𝐱𝐱)
𝑥𝑥1 𝑥𝑥2 𝑥𝑥𝑖𝑖 𝑥𝑥𝑇𝑇𝑥𝑥⋯ ⋯ 

𝒔𝒔𝒂𝒂𝒔𝒔𝒂𝒂𝒔𝒔𝒔𝒔 𝒔𝒔𝒔𝒔𝒔𝒔𝒔𝒔𝒔𝒔𝒔𝒔𝒔𝒔𝒔𝒔 (𝐲𝐲)
𝑦𝑦1 𝑦𝑦2 𝑦𝑦𝑗𝑗 𝑦𝑦𝑇𝑇𝑦𝑦⋯⋯ 

𝑏𝑏𝑏𝑏𝑏𝑏𝑏𝑏𝑏𝑏𝑏𝑏𝑏𝑏𝑏𝑏𝑏𝑏𝑏𝑏𝑏𝑏𝑏𝑏𝑏𝑏 𝑅𝑅𝑅𝑅𝑅𝑅 𝑏𝑏𝑏𝑏𝑏𝑏𝑏𝑏𝑏𝑏𝑏𝑏𝑏𝑏𝑏𝑏𝑏𝑏𝑏𝑏𝑏𝑏𝑏𝑏𝑏𝑏 𝑅𝑅𝑅𝑅𝑅𝑅

𝒉𝒉𝟏𝟏 𝒉𝒉𝟐𝟐 𝒉𝒉𝒋𝒋 𝒉𝒉𝑻𝑻𝒚𝒚  ⋯ ⋯ 

𝑸𝑸𝑸𝑸𝒔𝒔𝒔𝒔𝒔𝒔𝒔𝒔𝒔𝒔𝒔𝒔𝒔𝒔𝒔𝒔 𝑏𝑏𝑜𝑜𝑏𝑏𝑜𝑜𝑜𝑜𝑏𝑏 

(𝑅𝑅𝑅𝑅𝑅𝑅) 
ℎ𝑏𝑏𝑏𝑏𝑏𝑏𝑏𝑏𝑏𝑏 𝑠𝑠𝑏𝑏𝑏𝑏𝑏𝑏𝑏𝑏𝑠𝑠 

𝐬𝐬 𝑠𝑠𝑜𝑜𝑠𝑠𝑠𝑠𝑏𝑏𝑏𝑏𝑦𝑦 𝑜𝑜𝑏𝑏𝑏𝑏𝑏𝑏 

𝒒𝒒𝒚𝒚𝒋𝒋𝒒𝒒𝒚𝒚𝟏𝟏 𝒒𝒒𝒚𝒚𝟐𝟐 𝒒𝒒𝒚𝒚𝑻𝑻𝒚𝒚⋯ ⋯ 
𝒒𝒒𝒔𝒔𝒂𝒂𝒒𝒒𝒒𝒒𝒔𝒔𝒚𝒚 𝒗𝒗𝒔𝒔𝒔𝒔𝒔𝒔𝒔𝒔𝒔𝒔𝒔𝒔 

𝒒𝒒𝒚𝒚𝒋𝒋𝒒𝒒𝒚𝒚𝟏𝟏 𝒒𝒒𝒚𝒚𝟐𝟐 𝒒𝒒𝒚𝒚𝑻𝑻𝒚𝒚 𝑞𝑞𝑜𝑜𝑏𝑏𝑏𝑏𝑏𝑏𝑏𝑏𝑦𝑦 𝑣𝑣𝑏𝑏𝑏𝑏𝑏𝑏𝑏𝑏𝑏𝑏𝑠𝑠 ⋯ ⋯ 

Figure 2: Recurrent neural network based
sentence-level QE model (SENT/RNN) (Kim and
Lee, 2016)

tion such that

QEsentence( y, x )

=QE′sentence( qy1 , ... , qyT y )

= σ(Ws s) .

(1)

Ws is the weight matrix of sigmoid function5 at
sentence-level QE. s is a summary unit of the se-
quential quality vectors and is fixed to the last hid-
den state6 hT y of RNN. The hidden state hj is
computed by

hj = f(qyj , hj−1) (2)

where f is the activation function of RNN (Kim
and Lee, 2016).

2.2 RNN based Word-level QE Model

In RNN based word-level QE model (Figure 3),
we apply bidirectional RNN based binary classifi-
cation (OK/BAD) using quality vectors as inputs.
Through the bidirectional RNN, bidirectional hid-
den states {~hj ,

�
hj} for each target word yj are

5Bias terms are visually omitted in all equations.
6In RNN, the last hidden state is used as the summary of

inputs.

788



   POSTECH 

A Recurrent Neural Networks Approach 
for Estimating the Quality of Machine Translation Output 

𝒒𝒒𝒚𝒚𝒋𝒋 𝒒𝒒𝒚𝒚𝟏𝟏 𝒒𝒒𝒚𝒚𝟐𝟐 𝒒𝒒𝒚𝒚𝑻𝑻𝒚𝒚  

𝑏𝑏𝑜𝑜𝑏𝑏𝑜𝑜𝑜𝑜𝑏𝑏 

𝑞𝑞𝑜𝑜𝑏𝑏𝑏𝑏𝑏𝑏𝑏𝑏𝑦𝑦 𝑣𝑣𝑏𝑏𝑏𝑏𝑏𝑏𝑏𝑏𝑏𝑏𝑠𝑠 

𝑏𝑏𝑏𝑏𝑏𝑏𝑏𝑏𝑏𝑏𝑏𝑏𝑏𝑏𝑏𝑏𝑏𝑏𝑏𝑏𝑏𝑏𝑏𝑏𝑏𝑏 𝑅𝑅𝑅𝑅𝑅𝑅  
 ℎ𝑏𝑏𝑏𝑏𝑏𝑏𝑏𝑏𝑏𝑏 𝑠𝑠𝑏𝑏𝑏𝑏𝑏𝑏𝑏𝑏𝑠𝑠 
 

𝒉𝒉𝟏𝟏 𝒉𝒉𝟐𝟐 𝒉𝒉𝒋𝒋 𝒉𝒉𝑻𝑻𝒚𝒚  

⋯ 

⋯ 

⋯ 

⋯ 

𝒉𝒉𝟏𝟏 𝒉𝒉𝟐𝟐 𝒉𝒉𝒋𝒋 𝒉𝒉𝑻𝑻𝒚𝒚  

𝑸𝑸𝑸𝑸𝒘𝒘𝒋𝒋 𝑸𝑸𝑸𝑸𝒘𝒘𝟏𝟏  𝑸𝑸𝑸𝑸𝒘𝒘𝑻𝑻𝒚𝒚  ⋯ ⋯ 

⋯ ⋯ 

𝒔𝒔𝒔𝒔𝒔𝒔𝒔𝒔𝒔𝒔𝒔𝒔 𝒔𝒔𝒔𝒔𝒔𝒔𝒔𝒔𝒔𝒔𝒔𝒔𝒔𝒔𝒔𝒔 (𝐱𝐱) 
𝑥𝑥1 𝑥𝑥2 𝑥𝑥𝑖𝑖 𝑥𝑥𝑇𝑇𝑥𝑥 ⋯ ⋯ 

𝒔𝒔𝒂𝒂𝒔𝒔𝒂𝒂𝒔𝒔𝒔𝒔 𝒔𝒔𝒔𝒔𝒔𝒔𝒔𝒔𝒔𝒔𝒔𝒔𝒔𝒔𝒔𝒔 (𝐲𝐲) 
𝑦𝑦1 𝑦𝑦2 𝑦𝑦𝑗𝑗 𝑦𝑦𝑇𝑇𝑦𝑦 ⋯ ⋯ 

 

𝑏𝑏𝑏𝑏𝑏𝑏𝑏𝑏𝑏𝑏𝑏𝑏𝑏𝑏𝑏𝑏𝑏𝑏𝑏𝑏𝑏𝑏𝑏𝑏𝑏𝑏 𝑅𝑅𝑅𝑅𝑅𝑅  
 

 

𝑏𝑏𝑏𝑏𝑏𝑏𝑏𝑏𝑏𝑏𝑏𝑏𝑏𝑏𝑏𝑏𝑏𝑏𝑏𝑏𝑏𝑏𝑏𝑏𝑏𝑏 𝑅𝑅𝑅𝑅𝑅𝑅  
 

𝒒𝒒𝒚𝒚𝒋𝒋 𝒒𝒒𝒚𝒚𝟏𝟏 𝒒𝒒𝒚𝒚𝟐𝟐 𝒒𝒒𝒚𝒚𝑻𝑻𝒚𝒚  ⋯ ⋯ 
𝒒𝒒𝒔𝒔𝒂𝒂𝒒𝒒𝒒𝒒𝒔𝒔𝒚𝒚 𝒗𝒗𝒔𝒔𝒔𝒔𝒔𝒔𝒔𝒔𝒔𝒔𝒔𝒔 

𝑸𝑸𝑸𝑸𝒘𝒘𝟐𝟐 

Figure 3: Recurrent neural network based word-
level QE model (WORD/RNN)

   POSTECH 

A Recurrent Neural Networks Approach 
for Estimating the Quality of Machine Translation Output 

𝑏𝑏𝑜𝑜𝑏𝑏𝑜𝑜𝑜𝑜𝑏𝑏 𝑸𝑸𝑸𝑸𝒑𝒑𝒉𝒉𝟏𝟏  𝑸𝑸𝑸𝑸𝒑𝒑𝒉𝒉𝟐𝟐 𝑸𝑸𝑸𝑸𝒑𝒑𝒉𝒉𝑷𝑷𝒚𝒚 ⋯ 

𝑦𝑦1 𝑦𝑦2 𝑦𝑦𝑗𝑗 𝑦𝑦𝑇𝑇𝑦𝑦 

𝒑𝒑𝒉𝒉𝟏𝟏 𝒑𝒑𝒉𝒉𝟐𝟐 𝒑𝒑𝒉𝒉𝑷𝑷𝒚𝒚 ⋯ 

⋯ ⋯ 

𝑜𝑜ℎ𝑏𝑏𝑏𝑏𝑠𝑠𝑏𝑏𝑠𝑠 

𝑏𝑏𝑏𝑏𝑏𝑏𝑡𝑡𝑏𝑏𝑏𝑏 𝑤𝑤𝑏𝑏𝑏𝑏𝑏𝑏𝑠𝑠 

𝒔𝒔𝒔𝒔𝒔𝒔𝒔𝒔𝒔𝒔𝒔𝒔 𝒔𝒔𝒔𝒔𝒔𝒔𝒔𝒔𝒔𝒔𝒔𝒔𝒔𝒔𝒔𝒔 (𝐱𝐱) 
𝑥𝑥1 𝑥𝑥2 𝑥𝑥𝑖𝑖 𝑥𝑥𝑇𝑇𝑥𝑥 ⋯ ⋯ 

𝒔𝒔𝒂𝒂𝒔𝒔𝒂𝒂𝒔𝒔𝒔𝒔 𝒔𝒔𝒔𝒔𝒔𝒔𝒔𝒔𝒔𝒔𝒔𝒔𝒔𝒔𝒔𝒔 (𝐲𝐲) 
𝑦𝑦1 𝑦𝑦2 𝑦𝑦𝑗𝑗 𝑦𝑦𝑇𝑇𝑦𝑦 ⋯ ⋯ 

 

𝑏𝑏𝑏𝑏𝑏𝑏𝑏𝑏𝑏𝑏𝑏𝑏𝑏𝑏𝑏𝑏𝑏𝑏𝑏𝑏𝑏𝑏𝑏𝑏𝑏𝑏 𝑅𝑅𝑅𝑅𝑅𝑅  
 

 

𝑏𝑏𝑏𝑏𝑏𝑏𝑏𝑏𝑏𝑏𝑏𝑏𝑏𝑏𝑏𝑏𝑏𝑏𝑏𝑏𝑏𝑏𝑏𝑏𝑏𝑏 𝑅𝑅𝑅𝑅𝑅𝑅  
 

𝒒𝒒𝒚𝒚𝒋𝒋 𝒒𝒒𝒚𝒚𝟏𝟏 𝒒𝒒𝒚𝒚𝟐𝟐 𝒒𝒒𝒚𝒚𝑻𝑻𝒚𝒚  ⋯ ⋯ 
𝒒𝒒𝒔𝒔𝒂𝒂𝒒𝒒𝒒𝒒𝒔𝒔𝒚𝒚 𝒗𝒗𝒔𝒔𝒔𝒔𝒔𝒔𝒔𝒔𝒔𝒔𝒔𝒔 

𝑏𝑏𝑏𝑏𝑏𝑏𝑏𝑏𝑏𝑏𝑏𝑏𝑏𝑏𝑏𝑏𝑏𝑏𝑏𝑏𝑏𝑏𝑏𝑏𝑏𝑏 𝑅𝑅𝑅𝑅𝑅𝑅  
 ℎ𝑏𝑏𝑏𝑏𝑏𝑏𝑏𝑏𝑏𝑏 𝑠𝑠𝑏𝑏𝑏𝑏𝑏𝑏𝑏𝑏𝑠𝑠 
 

𝒉𝒉𝟏𝟏 𝒉𝒉𝟐𝟐 𝒉𝒉𝑷𝑷𝒚𝒚  

⋯ 

⋯ 

𝒉𝒉𝟏𝟏 𝒉𝒉𝟐𝟐 𝒉𝒉𝑷𝑷𝒚𝒚  

𝒒𝒒𝒚𝒚𝒋𝒋 𝒒𝒒𝒚𝒚𝟏𝟏 𝒒𝒒𝒚𝒚𝟐𝟐 𝒒𝒒𝒚𝒚𝑻𝑻𝒚𝒚  𝑞𝑞𝑜𝑜𝑏𝑏𝑏𝑏𝑏𝑏𝑏𝑏𝑦𝑦 𝑣𝑣𝑏𝑏𝑏𝑏𝑏𝑏𝑏𝑏𝑏𝑏𝑠𝑠 ⋯ ⋯ 

Figure 4: Recurrent neural network based phrase-
level QE model (PHR/RNN)

made such that7

~hj = f
′(qyj ,

~hj−1)
�
hj = g

′(qyj ,
�
hj+1) .

(3)

The forward hidden state ~hj indicates summary
information about the forward translation quality
of target word yj , reflecting qualities of preced-
ing target words {y1, ..., yj−1}. And the back-
ward hidden state

�
hj indicates summary informa-

tion about the backward translation quality of tar-
get word yj , reflecting qualities of following target
words8 {yj+1, ..., yTy}. We use the concatenated
hidden state hwj (= [~hj ;

�
hj ]) to predict the word-

level quality for target word yj such that

QEword( yj , x )

=QE′word( qyj )

=

{
OK , if σ(Ww hwj ) ≥ 0.5
BAD, if σ(Ww hwj ) < 0.5 .

(4)

Ww is the weight matrix of sigmoid function at
word-level QE.

7f ′(g′) is the activation function of the for-
ward(backward) RNN at word-level QE.

8Ty is the length of target sentence.

2.3 RNN based Phrase-level QE Model

RNN based phrase-level QE model is the extended
version of RNN based word-level QE model (in
subsection 2.2). In RNN based phrase-level QE
model (Figure 4), we also apply bidirectional
RNN based binary classification. We use the sim-
ply averaged quality vector qphj to predict the
phrase-level quality of the phrase9 phj , composed
of the corresponding target words {y

k
, y

k+1
, ...},

such that

QEphrase( phj , x )

=QE′phrase( qyk , qyk+1 , ... )

=QE′′phrase( qphj )

=

{
OK , if σ(Wph h

ph
j ) ≥ 0.5

BAD, if σ(Wph h
ph
j ) < 0.5 .

(5)

Wph is the weight matrix of sigmoid function at

phrase-level QE. hphj (= [~hj ;
�
hj ]) is the concate-

nated hidden state for phrase phj of bidirectional
RNN where10

~hj = f
′′(qphj ,

~hj−1)
�
hj = g

′′(qphj ,
�
hj+1) .

(6)

3 Results

RNN based QE models were evaluated on the
WMT16 Quality Estimation Shared Task11 at sen-
tence, word and phrase level of English-German.
Because whole QE models are separated into two
parts, each part of the QE models is trained sepa-
rately by using different training data. To train the
first part of the QE models, English-German par-
allel corpus of Europarl v7 (Koehn, 2005) were
used. To train the second part of the QE models,
WMT16 QE datasets of English-German (Specia
et al., 2016) were used.

To denote the each method, the following nam-
ing format is used: [level]/[model]-QV[num].
[level] is the QE granularity level: SENT (sen-
tence level), WORD (word level) and PHR (phrase
level). [model] is the type of model used in the
second part: RNN (of subsection 2.1, 2.2 and 2.3)

91 5 j 5 Py where Py is the number of phrases in target
sentence and Py ≤ Ty.

10f ′′(g′′) is the activation function of the for-
ward(backward) RNN at phrase-level QE.

11http://www.statmt.org/wmt16/quality-
estimation-task.html

789



Task 1. Test Pearson’s r ↑ MAE ↓ RMSE ↓ Rank
SENT/RNN-QV2 0.4600 0.1358 0.1860 2
SENT/RNN-QV3 0.4475 0.1352 0.1838 4
SENT/FNN-QV2 0.3588 0.1517 0.2001
SENT/FNN-QV3 0.3549 0.1529 0.2006

BASELINE 0.3510 0.1353 0.1839

Table 1: Results on test set for the scoring variant
of WMT16 sentence-level QE (Task 1).

Task 1. Test Spearman’s ρ ↑ DeltaAvg ↑ Rank
SENT/RNN-QV2 0.4826 0.0766 1
SENT/RNN-QV3 0.4660 0.0753 3
SENT/FNN-QV3 0.3910 0.0589
SENT/FNN-QV2 0.3905 0.0593

BASELINE 0.3900 0.0630

Table 2: Results on test set for the ranking variant
of WMT16 sentence-level QE (Task 1).

and FNN (of subsection A.1, A.2 and A.3). At
QV[num], [num] is the number of iterations while
the first part is trained by using large-scale parallel
corpora to make quality vectors (QV).

3.1 Results of Sentence-level QE (Task 1)
Pearson’s correlation (r), mean absolute error
(MAE), and root mean squared error (RMSE) are
used to evaluate the scoring variant of sentence-
level QE. And Spearman’s rank correlation (ρ) and
DeltaAvg are used to evaluate the ranking variant
of sentence-level QE.

Table 1 and 2 (Table B.1 and B.2) present
the results of the QE models on test (develop-
ment) set for the scoring and ranking variants
of the WMT16 sentence-level QE shared task
(Task 1). In all aspects of evaluation at sentence-
level QE, the RNN based QE model (SENT/RNN)
showed the better performance than the FNN
based QE model (SENT/FNN). Our two methods
(SENT/RNN-QV2 and SENT/RNN-QV3), partic-
ipated in WMT16 sentence-level QE shared task,
achieved top rank: each 2nd and 4th at the scoring
variant and each 1st and 3rd at the ranking variant.

3.2 Results of Word-level and Phrase-level
QE (Task 2)

The multiplication of F1-scores for the ‘OK’ and
‘BAD’ classes and F1-score for the ‘BAD’ class
are used to evaluate the word-level and phrase-
level QE.

Table 3 and 4 (Table B.3 and B.4) respectively
present the results on test (development) set of the
WMT16 word-level and phrase-level QE shared
task (Task 2). In all aspects of evaluation at word-
level and phrase-level QE, the RNN based QE

Task 2. Test Multiplication of F1- F1-
Word-level F1-OK and F1-BAD ↑ Bad ↑ OK ↑ Rank

WORD/RNN-QV3 0.3803 0.4475 0.8498 5
WORD/RNN-QV2 0.3759 0.4538 0.8284 6
WORD/FNN-QV3 0.3273 0.3800 0.8615
WORD/FNN-QV2 0.3241 0.3932 0.8242

BASELINE 0.3240 0.3682 0.8800

Table 3: Results on test set of WMT16 word-level
QE (Task 2).

Task 2. Test Multiplication of F1- F1-
Phase-level F1-OK and F1-BAD ↑ Bad ↑ OK ↑ Rank

PHR/RNN-QV3 0.3781 0.4950 0.7639 2
PHR/RNN-QV2 0.3693 0.4785 0.7718 3
PHR/FNN-QV3 0.3505 0.4722 0.7423
PHR/FNN-QV2 0.3353 0.4413 0.7599

BASELINE 0.3211 0.4014 0.8001

Table 4: Results on test set of WMT16 phrase-
level QE (Task 2).

models (WORD/RNN and PHR/RNN) showed the
better performance than the FNN based QE mod-
els (WORD/FNN and PHR/FNN). Our two meth-
ods (WORD/RNN-QV3 and WORD/RNN-QV2),
participated in WMT16 word-level QE shared
task, achieved each 5th and 6th rank. Our two
methods (PHR/RNN-QV3 and PHR/RNN-QV2),
participated in WMT16 phrase-level QE shared
task, achieved top rank: each 2nd and 3rd.

4 Conclusion

This paper described recurrent neural network
based quality estimation models of sentence, word
and phrase level. We extended the (existing
sentence-level) recurrent neural network based
quality estimation model to word and phrase level.
And we applied these models to sentence, word
and phrase-level QE shared task of WMT16.
These recurrent neural network based quality esti-
mation models are pure neural approaches for QE
and achieved excellent performance especially in
sentence and phrase-level QE.

Acknowledgments

This research was partly supported by the “ICT
R&D Program” of MSIP/IITP (R7119-16-1001,
Core technology development of the real-time si-
multaneous speech translation based on knowl-
edge enhancement) and “ICT Consilience Cre-
ative Program” of MSIP/IITP (R0346-16-1007)

790



References

Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben-
gio. 2015. Neural machine translation by jointly
learning to align and translate. In ICLR 2015.

Kyunghyun Cho, Bart van Merrienboer, Caglar Gul-
cehre, Dzmitry Bahdanau, Fethi Bougares, Holger
Schwenk, and Yoshua Bengio. 2014. Learning
phrase representations using rnn encoder–decoder
for statistical machine translation. In Proceedings of
the 2014 Conference on Empirical Methods in Nat-
ural Language Processing (EMNLP), pages 1724–
1734. Association for Computational Linguistics.

Hyun Kim and Jong-Hyeok Lee. 2016. A recurrent
neural networks approach for estimating the quality
of machine translation output. In Proceedings of the
2016 Conference of the North American Chapter of
the Association for Computational Linguistics: Hu-
man Language Technologies, pages 494–498. Asso-
ciation for Computational Linguistics.

Philipp Koehn. 2005. Europarl: A parallel corpus for
statistical machine translation. In MT summit, vol-
ume 5, pages 79–86. Citeseer.

Julia Kreutzer, Shigehiko Schamoni, and Stefan Rie-
zler. 2015. Quality estimation from scratch
(quetch): Deep learning for word-level translation
quality estimation. In Proceedings of the Tenth
Workshop on Statistical Machine Translation, pages
316–322. Association for Computational Linguis-
tics.

Kashif Shah, Varvara Logacheva, Gustavo Paetzold,
Frédéric Blain, Daniel Beck, Fethi Bougares, and
Lucia Specia. 2015. Shef-nn: Translation quality
estimation with neural networks. In Proceedings of
the Tenth Workshop on Statistical Machine Transla-
tion, pages 342–347. Association for Computational
Linguistics, September.

Matthew Snover, Bonnie Dorr, Richard Schwartz, Lin-
nea Micciulla, and John Makhoul. 2006. A study of
translation edit rate with targeted human annotation.
In Proceedings of association for machine transla-
tion in the Americas, pages 223–231.

Lucia Specia, Gustavo Paetzold, and Carolina Scar-
ton. 2015. Multi-level translation quality predic-
tion with quest++. In Proceedings of ACL-IJCNLP
2015 System Demonstrations, pages 115–120. Asso-
ciation for Computational Linguistics and The Asian
Federation of Natural Language Processing.

Lucia Specia, Varvara Logacheva, and Carolina Scar-
ton. 2016. WMT16 quality estimation shared task
training and development data. LINDAT/CLARIN
digital library at Institute of Formal and Applied
Linguistics, Charles University in Prague.

Appendix

A Feedforward Neural Network (FNN)
based Quality Estimation Model

In this section, we describe the FNN based (sec-
ond part) QE models of sentence, word and phrase
level, for comparison with RNN based (second
part) QE models. Quality vectors, generated from
the same RNN based first part QE model, are used
as inputs.

A.1 FNN based Sentence-level QE Model

In FNN based sentence-level QE model (Fig-
ure A.1), we also use a logistic sigmoid function
of (1). But in FNN based model we make each
hidden state hj by only using each quality vector
qyj for target word yj . And for s, which is a sum-
mary unit of the whole quality vectors, we simply
average all hidden states {h1, ..., hT y}.

   POSTECH 

A Recurrent Neural Networks Approach 
for Estimating the Quality of Machine Translation Output 

𝒔𝒔𝒔𝒔𝒔𝒔𝒔𝒔𝒔𝒔𝒔𝒔 𝒔𝒔𝒔𝒔𝒔𝒔𝒔𝒔𝒔𝒔𝒔𝒔𝒔𝒔𝒔𝒔 (𝐱𝐱) 
𝑥𝑥1 𝑥𝑥2 𝑥𝑥𝑖𝑖 𝑥𝑥𝑇𝑇𝑥𝑥 ⋯ ⋯ 

𝒔𝒔𝒂𝒂𝒔𝒔𝒂𝒂𝒔𝒔𝒔𝒔 𝒔𝒔𝒔𝒔𝒔𝒔𝒔𝒔𝒔𝒔𝒔𝒔𝒔𝒔𝒔𝒔 (𝐲𝐲) 
𝑦𝑦1 𝑦𝑦2 𝑦𝑦𝑗𝑗 𝑦𝑦𝑇𝑇𝑦𝑦 ⋯ ⋯ 

 

𝑏𝑏𝑏𝑏𝑏𝑏𝑏𝑏𝑏𝑏𝑏𝑏𝑏𝑏𝑏𝑏𝑏𝑏𝑏𝑏𝑏𝑏𝑏𝑏𝑏𝑏 𝑅𝑅𝑅𝑅𝑅𝑅  
 

 

𝑏𝑏𝑏𝑏𝑏𝑏𝑏𝑏𝑏𝑏𝑏𝑏𝑏𝑏𝑏𝑏𝑏𝑏𝑏𝑏𝑏𝑏𝑏𝑏𝑏𝑏 𝑅𝑅𝑅𝑅𝑅𝑅  
 

𝒒𝒒𝒚𝒚𝒋𝒋 𝒒𝒒𝒚𝒚𝟏𝟏 𝒒𝒒𝒚𝒚𝟐𝟐 𝒒𝒒𝒚𝒚𝑻𝑻𝒚𝒚  ⋯ ⋯ 
𝒒𝒒𝒔𝒔𝒂𝒂𝒒𝒒𝒒𝒒𝒔𝒔𝒚𝒚 𝒗𝒗𝒔𝒔𝒔𝒔𝒔𝒔𝒔𝒔𝒔𝒔𝒔𝒔 

𝒉𝒉𝟏𝟏 𝒉𝒉𝟐𝟐 𝒉𝒉𝒋𝒋 𝒉𝒉𝑻𝑻𝒚𝒚  ⋯ ⋯ 

𝑸𝑸𝑸𝑸𝒔𝒔𝒔𝒔𝒔𝒔𝒔𝒔𝒔𝒔𝒔𝒔𝒔𝒔𝒔𝒔 

𝒒𝒒𝒚𝒚𝒋𝒋 𝒒𝒒𝒚𝒚𝟏𝟏 𝒒𝒒𝒚𝒚𝟐𝟐 𝒒𝒒𝒚𝒚𝑻𝑻𝒚𝒚  ⋯ ⋯ 

ℎ𝑏𝑏𝑏𝑏𝑏𝑏𝑏𝑏𝑏𝑏 𝑠𝑠𝑏𝑏𝑏𝑏𝑏𝑏𝑏𝑏𝑠𝑠 

𝐬𝐬 

𝑏𝑏𝑜𝑜𝑏𝑏𝑜𝑜𝑜𝑜𝑏𝑏 

𝑞𝑞𝑜𝑜𝑏𝑏𝑏𝑏𝑏𝑏𝑏𝑏𝑦𝑦 𝑣𝑣𝑏𝑏𝑏𝑏𝑏𝑏𝑏𝑏𝑏𝑏𝑠𝑠 

𝑠𝑠𝑜𝑜𝑠𝑠𝑠𝑠𝑏𝑏𝑏𝑏𝑦𝑦 𝑜𝑜𝑏𝑏𝑏𝑏𝑏𝑏 

Figure A.1: Feedforward neural network based
sentence-level QE model (SENT/FNN)

A.2 FNN based Word-level QE Model

In FNN based word-level QE model (Fig-
ure A.2), we apply FNN based binary classifica-
tion (OK/BAD) using quality vectors as input. By
only using each quality vector qyj for target word
yj , each hidden state hj(= hwj ) is made. We pre-
dict the word-level QE by (4).

   POSTECH 

A Recurrent Neural Networks Approach 
for Estimating the Quality of Machine Translation Output 

𝒔𝒔𝒔𝒔𝒔𝒔𝒔𝒔𝒔𝒔𝒔𝒔 𝒔𝒔𝒔𝒔𝒔𝒔𝒔𝒔𝒔𝒔𝒔𝒔𝒔𝒔𝒔𝒔 (𝐱𝐱) 
𝑥𝑥1 𝑥𝑥2 𝑥𝑥𝑖𝑖 𝑥𝑥𝑇𝑇𝑥𝑥 ⋯ ⋯ 

𝒔𝒔𝒂𝒂𝒔𝒔𝒂𝒂𝒔𝒔𝒔𝒔 𝒔𝒔𝒔𝒔𝒔𝒔𝒔𝒔𝒔𝒔𝒔𝒔𝒔𝒔𝒔𝒔 (𝐲𝐲) 
𝑦𝑦1 𝑦𝑦2 𝑦𝑦𝑗𝑗 𝑦𝑦𝑇𝑇𝑦𝑦 ⋯ ⋯ 

 

𝑏𝑏𝑏𝑏𝑏𝑏𝑏𝑏𝑏𝑏𝑏𝑏𝑏𝑏𝑏𝑏𝑏𝑏𝑏𝑏𝑏𝑏𝑏𝑏𝑏𝑏 𝑅𝑅𝑅𝑅𝑅𝑅  
 

 

𝑏𝑏𝑏𝑏𝑏𝑏𝑏𝑏𝑏𝑏𝑏𝑏𝑏𝑏𝑏𝑏𝑏𝑏𝑏𝑏𝑏𝑏𝑏𝑏𝑏𝑏 𝑅𝑅𝑅𝑅𝑅𝑅  
 

𝒒𝒒𝒚𝒚𝒋𝒋 𝒒𝒒𝒚𝒚𝟏𝟏 𝒒𝒒𝒚𝒚𝟐𝟐 𝒒𝒒𝒚𝒚𝑻𝑻𝒚𝒚  ⋯ ⋯ 
𝒒𝒒𝒔𝒔𝒂𝒂𝒒𝒒𝒒𝒒𝒔𝒔𝒚𝒚 𝒗𝒗𝒔𝒔𝒔𝒔𝒔𝒔𝒔𝒔𝒔𝒔𝒔𝒔 

𝒉𝒉𝟏𝟏 𝒉𝒉𝟐𝟐 𝒉𝒉𝒋𝒋 𝒉𝒉𝑻𝑻𝒚𝒚  ⋯ ⋯ 

𝒒𝒒𝒚𝒚𝒋𝒋 𝒒𝒒𝒚𝒚𝟏𝟏 𝒒𝒒𝒚𝒚𝟐𝟐 𝒒𝒒𝒚𝒚𝑻𝑻𝒚𝒚  

𝑏𝑏𝑜𝑜𝑏𝑏𝑜𝑜𝑜𝑜𝑏𝑏 

𝑞𝑞𝑜𝑜𝑏𝑏𝑏𝑏𝑏𝑏𝑏𝑏𝑦𝑦 𝑣𝑣𝑏𝑏𝑏𝑏𝑏𝑏𝑏𝑏𝑏𝑏𝑠𝑠 

𝑸𝑸𝑸𝑸𝒘𝒘𝒋𝒋 𝑸𝑸𝑸𝑸𝒘𝒘𝟏𝟏  𝑸𝑸𝑸𝑸𝒘𝒘𝑻𝑻𝒚𝒚  ⋯ ⋯ 

ℎ𝑏𝑏𝑏𝑏𝑏𝑏𝑏𝑏𝑏𝑏 𝑠𝑠𝑏𝑏𝑏𝑏𝑏𝑏𝑏𝑏𝑠𝑠 

⋯ ⋯ 

𝑸𝑸𝑸𝑸𝒘𝒘𝟐𝟐 

Figure A.2: Feedforward neural network based
word-level QE model (WORD/FNN)

791



A.3 FNN based Phrase-level QE Model
In FNN based phrase-level QE model (Fig-
ure A.3), we also apply FNN based binary clas-
sification (OK/BAD). By only using the averaged
quality vector qphj for the phrase phj , composed
of the corresponding target words {y

k
, y

k+1
, ...},

the hidden state hj(= h
ph
j ) is made. We predict

the phrase-level QE by (5).

   POSTECH 

A Recurrent Neural Networks Approach 
for Estimating the Quality of Machine Translation Output 

𝒔𝒔𝒔𝒔𝒔𝒔𝒔𝒔𝒔𝒔𝒔𝒔 𝒔𝒔𝒔𝒔𝒔𝒔𝒔𝒔𝒔𝒔𝒔𝒔𝒔𝒔𝒔𝒔 (𝐱𝐱) 
𝑥𝑥1 𝑥𝑥2 𝑥𝑥𝑖𝑖 𝑥𝑥𝑇𝑇𝑥𝑥 ⋯ ⋯ 

𝒔𝒔𝒂𝒂𝒔𝒔𝒂𝒂𝒔𝒔𝒔𝒔 𝒔𝒔𝒔𝒔𝒔𝒔𝒔𝒔𝒔𝒔𝒔𝒔𝒔𝒔𝒔𝒔 (𝐲𝐲) 
𝑦𝑦1 𝑦𝑦2 𝑦𝑦𝑗𝑗 𝑦𝑦𝑇𝑇𝑦𝑦 ⋯ ⋯ 

 

𝑏𝑏𝑏𝑏𝑏𝑏𝑏𝑏𝑏𝑏𝑏𝑏𝑏𝑏𝑏𝑏𝑏𝑏𝑏𝑏𝑏𝑏𝑏𝑏𝑏𝑏 𝑅𝑅𝑅𝑅𝑅𝑅  
 

 

𝑏𝑏𝑏𝑏𝑏𝑏𝑏𝑏𝑏𝑏𝑏𝑏𝑏𝑏𝑏𝑏𝑏𝑏𝑏𝑏𝑏𝑏𝑏𝑏𝑏𝑏 𝑅𝑅𝑅𝑅𝑅𝑅  
 

𝒒𝒒𝒚𝒚𝒋𝒋 𝒒𝒒𝒚𝒚𝟏𝟏 𝒒𝒒𝒚𝒚𝟐𝟐 𝒒𝒒𝒚𝒚𝑻𝑻𝒚𝒚  ⋯ ⋯ 
𝒒𝒒𝒔𝒔𝒂𝒂𝒒𝒒𝒒𝒒𝒔𝒔𝒚𝒚 𝒗𝒗𝒔𝒔𝒔𝒔𝒔𝒔𝒔𝒔𝒔𝒔𝒔𝒔 

𝒉𝒉𝟏𝟏 𝒉𝒉𝟐𝟐 𝒉𝒉𝑻𝑻𝒚𝒚  ℎ𝑏𝑏𝑏𝑏𝑏𝑏𝑏𝑏𝑏𝑏 𝑠𝑠𝑏𝑏𝑏𝑏𝑏𝑏𝑏𝑏𝑠𝑠 

𝒒𝒒𝒚𝒚𝒋𝒋 𝒒𝒒𝒚𝒚𝟏𝟏 𝒒𝒒𝒚𝒚𝟐𝟐 𝒒𝒒𝒚𝒚𝑻𝑻𝒚𝒚  𝑞𝑞𝑜𝑜𝑏𝑏𝑏𝑏𝑏𝑏𝑏𝑏𝑦𝑦 𝑣𝑣𝑏𝑏𝑏𝑏𝑏𝑏𝑏𝑏𝑏𝑏𝑠𝑠 ⋯ ⋯ 

𝑦𝑦1 𝑦𝑦2 𝑦𝑦𝑗𝑗 𝑦𝑦𝑇𝑇𝑦𝑦 

𝒑𝒑𝒉𝒉𝟏𝟏 𝒑𝒑𝒉𝒉𝟐𝟐 𝒑𝒑𝒉𝒉𝑷𝑷𝒚𝒚 ⋯ 

⋯ ⋯ 

𝑜𝑜ℎ𝑏𝑏𝑏𝑏𝑠𝑠𝑏𝑏𝑠𝑠 

𝑏𝑏𝑏𝑏𝑏𝑏𝑡𝑡𝑏𝑏𝑏𝑏 𝑤𝑤𝑏𝑏𝑏𝑏𝑏𝑏𝑠𝑠 

𝑏𝑏𝑜𝑜𝑏𝑏𝑜𝑜𝑜𝑜𝑏𝑏 𝑸𝑸𝑸𝑸𝒑𝒑𝒉𝒉𝟏𝟏  𝑸𝑸𝑸𝑸𝒑𝒑𝒉𝒉𝟐𝟐 𝑸𝑸𝑸𝑸𝒑𝒑𝒉𝒉𝑷𝑷𝒚𝒚 ⋯ 

⋯ 

Figure A.3: Feedforward neural network based
phrase-level QE model (PHR/FNN)

B Results on Development Set of
WMT16 QE Shared Task

Task 1. Dev Pearson’s r ↑ MAE ↓ RMSE ↓
SENT/RNN-QV2 0.4661 0.1340 0.1921
SENT/RNN-QV3 0.4658 0.1341 0.1896
SENT/FNN-QV3 0.3915 0.1539 0.2007
SENT/FNN-QV2 0.3904 0.1560 0.2015

Table B.1: Results on development set for the
scoring variant of WMT16 sentence-level QE
(Task 1).

Task 1. Dev Spearman’s ρ ↑ DeltaAvg ↑
SENT/RNN-QV3 0.5222 0.0882
SENT/RNN-QV2 0.5154 0.0892
SENT/FNN-QV3 0.4370 0.0697
SENT/FNN-QV2 0.4227 0.0693

Table B.2: Results on development set for the
ranking variant of WMT16 sentence-level QE
(Task 1).

Task 2. Dev. Multiplication of F1- F1-
Word-level F1-OK and F1-BAD ↑ Bad ↑ OK ↑

WORD/RNN-QV3 0.3880 0.4567 0.8496
WORD/RNN-QV2 0.3838 0.4617 0.8313
WORD/FNN-QV3 0.3227 0.3812 0.8597
WORD/FNN-QV2 0.3171 0.3878 0.8178

Table B.3: Results on development set of
WMT16 word-level QE (Task 2).

Task 2. Dev. Multiplication of F1- F1-
Phase-level F1-OK and F1-BAD ↑ Bad ↑ OK ↑

PHR/RNN-QV2 0.3831 0.4955 0.7731
PHR/RNN-QV3 0.3770 0.4975 0.7578
PHR/FNN-QV3 0.3526 0.4755 0.7416
PHR/FNN-QV2 0.3391 0.4447 0.7626

Table B.4: Results on development set of
WMT16 phrase-level QE (Task 2).

792


