



















































Bag of Tricks for Efficient Text Classification


Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 2, Short Papers, pages 427–431,
Valencia, Spain, April 3-7, 2017. c©2017 Association for Computational Linguistics

Bag of Tricks for Efficient Text Classification

Armand Joulin Edouard Grave Piotr Bojanowski Tomas Mikolov
Facebook AI Research

{ajoulin,egrave,bojanowski,tmikolov}@fb.com

Abstract

This paper explores a simple and efficient
baseline for text classification. Our ex-
periments show that our fast text clas-
sifier fastText is often on par with
deep learning classifiers in terms of ac-
curacy, and many orders of magnitude
faster for training and evaluation. We can
train fastText on more than one bil-
lion words in less than ten minutes using a
standard multicore CPU, and classify half
a million sentences among 312K classes in
less than a minute.

1 Introduction

Text classification is an important task in Natu-
ral Language Processing with many applications,
such as web search, information retrieval, rank-
ing and document classification (Deerwester et
al., 1990; Pang and Lee, 2008). Recently, mod-
els based on neural networks have become in-
creasingly popular (Kim, 2014; Zhang and LeCun,
2015; Conneau et al., 2016). While these models
achieve very good performance in practice, they
tend to be relatively slow both at train and test
time, limiting their use on very large datasets.

Meanwhile, linear classifiers are often consid-
ered as strong baselines for text classification
problems (Joachims, 1998; McCallum and Nigam,
1998; Fan et al., 2008). Despite their simplicity,
they often obtain state-of-the-art performances if
the right features are used (Wang and Manning,
2012). They also have the potential to scale to very
large corpus (Agarwal et al., 2014).

In this work, we explore ways to scale these
baselines to very large corpus with a large output
space, in the context of text classification. Inspired
by the recent work in efficient word representation
learning (Mikolov et al., 2013; Levy et al., 2015),

we show that linear models with a rank constraint
and a fast loss approximation can train on a billion
words within ten minutes, while achieving perfor-
mance on par with the state-of-the-art. We eval-
uate the quality of our approach fastText1 on
two different tasks, namely tag prediction and sen-
timent analysis.

2 Model architecture

A simple and efficient baseline for sentence clas-
sification is to represent sentences as bag of
words (BoW) and train a linear classifier, e.g., a
logistic regression or an SVM (Joachims, 1998;
Fan et al., 2008). However, linear classifiers do
not share parameters among features and classes.
This possibly limits their generalization in the con-
text of large output space where some classes have
very few examples. Common solutions to this
problem are to factorize the linear classifier into
low rank matrices (Schütze, 1992; Mikolov et al.,
2013) or to use multilayer neural networks (Col-
lobert and Weston, 2008; Zhang et al., 2015).

Figure 1 shows a simple linear model with rank
constraint. The first weight matrix A is a look-up
table over the words. The word representations are
then averaged into a text representation, which is
in turn fed to a linear classifier. The text repre-
sentation is an hidden variable which can be po-
tentially be reused. This architecture is similar to
the cbow model of Mikolov et al. (2013), where
the middle word is replaced by a label. We use
the softmax function f to compute the probabil-
ity distribution over the predefined classes. For a
set of N documents, this leads to minimizing the
negative log-likelihood over the classes:

− 1
N

N∑
n=1

yn log(f(BAxn)),

1https://github.com/facebookresearch/
fastText

427



x1 x2 . . . xN−1 xN

hidden

output

Figure 1: Model architecture of fastText for a
sentence with N ngram features x1, . . . , xN . The
features are embedded and averaged to form the
hidden variable.

where xn is the normalized bag of features of
the n-th document, yn the label, A and B the
weight matrices. This model is trained asyn-
chronously on multiple CPUs using stochastic gra-
dient descent and a linearly decaying learning rate.

2.1 Hierarchical softmax

When the number of classes is large, comput-
ing the linear classifier is computationally expen-
sive. More precisely, the computational complex-
ity is O(kh) where k is the number of classes
and h the dimension of the text representation. In
order to improve our running time, we use a hi-
erarchical softmax (Goodman, 2001) based on the
Huffman coding tree (Mikolov et al., 2013).

During training, the computational complexity
drops to O(h log2(k)).

The hierarchical softmax is also advantageous
at test time when searching for the most likely
class. Each node is associated with a probability
that is the probability of the path from the root to
that node. If the node is at depth l + 1 with par-
ents n1, . . . , nl, its probability is

P (nl+1) =
l∏

i=1

P (ni).

This means that the probability of a node is always
lower than the one of its parent. Exploring the tree
with a depth first search and tracking the maxi-
mum probability among the leaves allows us to
discard any branch associated with a small prob-
ability. In practice, we observe a reduction of the
complexity to O(h log2(k)) at test time. This ap-
proach is further extended to compute the T -top
targets at the cost of O(log(T )), using a binary
heap.

2.2 N-gram features

Bag of words is invariant to word order but tak-
ing explicitly this order into account is often com-
putationally very expensive. Instead, we use a
bag of n-grams as additional features to capture
some partial information about the local word or-
der. This is very efficient in practice while achiev-
ing comparable results to methods that explicitly
use the order (Wang and Manning, 2012).

We maintain a fast and memory efficient
mapping of the n-grams by using the hashing
trick (Weinberger et al., 2009) with the same hash-
ing function as in Mikolov et al. (2011) and 10M
bins if we only used bigrams, and 100M other-
wise.

3 Experiments

We evaluate fastText on two different tasks.
First, we compare it to existing text classifers on
the problem of sentiment analysis. Then, we eval-
uate its capacity to scale to large output space on a
tag prediction dataset. Note that our model could
be implemented with the Vowpal Wabbit library,2

but we observe in practice, that our tailored imple-
mentation is at least 2-5× faster.

3.1 Sentiment analysis

Datasets and baselines. We employ the same 8
datasets and evaluation protocol of Zhang et al.
(2015). We report the n-grams and TFIDF
baselines from Zhang et al. (2015), as well as
the character level convolutional model (char-
CNN) of Zhang and LeCun (2015), the char-
acter based convolution recurrent network (char-
CRNN) of (Xiao and Cho, 2016) and the very
deep convolutional network (VDCNN) of Con-
neau et al. (2016). We also compare to Tang et
al. (2015) following their evaluation protocol. We
report their main baselines as well as their two
approaches based on recurrent networks (Conv-
GRNN and LSTM-GRNN).

Results. We present the results in Figure 1. We
use 10 hidden units and run fastText for 5
epochs with a learning rate selected on a valida-
tion set from {0.05, 0.1, 0.25, 0.5}. On this task,
adding bigram information improves the perfor-
mance by 1-4%. Overall our accuracy is slightly
better than char-CNN and char-CRNN and, a bit

2Using the options --nn, --ngrams and
--log multi

428



Model AG Sogou DBP Yelp P. Yelp F. Yah. A. Amz. F. Amz. P.

BoW (Zhang et al., 2015) 88.8 92.9 96.6 92.2 58.0 68.9 54.6 90.4
ngrams (Zhang et al., 2015) 92.0 97.1 98.6 95.6 56.3 68.5 54.3 92.0
ngrams TFIDF (Zhang et al., 2015) 92.4 97.2 98.7 95.4 54.8 68.5 52.4 91.5
char-CNN (Zhang and LeCun, 2015) 87.2 95.1 98.3 94.7 62.0 71.2 59.5 94.5
char-CRNN (Xiao and Cho, 2016) 91.4 95.2 98.6 94.5 61.8 71.7 59.2 94.1
VDCNN (Conneau et al., 2016) 91.3 96.8 98.7 95.7 64.7 73.4 63.0 95.7

fastText, h = 10 91.5 93.9 98.1 93.8 60.4 72.0 55.8 91.2
fastText, h = 10, bigram 92.5 96.8 98.6 95.7 63.9 72.3 60.2 94.6

Table 1: Test accuracy [%] on sentiment datasets. FastText has been run with the same parameters
for all the datasets. It has 10 hidden units and we evaluate it with and without bigrams. For char-CNN,
we show the best reported numbers without data augmentation.

Zhang and LeCun (2015) Conneau et al. (2016) fastText

small char-CNN big char-CNN depth=9 depth=17 depth=29 h = 10, bigram

AG 1h 3h 24m 37m 51m 1s
Sogou - - 25m 41m 56m 7s

DBpedia 2h 5h 27m 44m 1h 2s
Yelp P. - - 28m 43m 1h09 3s
Yelp F. - - 29m 45m 1h12 4s
Yah. A. 8h 1d 1h 1h33 2h 5s
Amz. F. 2d 5d 2h45 4h20 7h 9s
Amz. P. 2d 5d 2h45 4h25 7h 10s

Table 2: Training time for a single epoch on sentiment analysis datasets compared to char-CNN and
VDCNN.

worse than VDCNN. Note that we can increase
the accuracy slightly by using more n-grams, for
example with trigrams, the performance on Sogou
goes up to 97.1%. Finally, Figure 3 shows that
our method is competitive with the methods pre-
sented in Tang et al. (2015). We tune the hyper-
parameters on the validation set and observe that
using n-grams up to 5 leads to the best perfor-
mance. Unlike Tang et al. (2015), fastText
does not use pre-trained word embeddings, which
can be explained the 1% difference in accuracy.

Model Yelp’13 Yelp’14 Yelp’15 IMDB

SVM+TF 59.8 61.8 62.4 40.5
CNN 59.7 61.0 61.5 37.5
Conv-GRNN 63.7 65.5 66.0 42.5
LSTM-GRNN 65.1 67.1 67.6 45.3

fastText 64.2 66.2 66.6 45.2

Table 3: Comparision with Tang et al. (2015). The
hyper-parameters are chosen on the validation set.
We report the test accuracy.

Training time. Both char-CNN and VDCNN
are trained on a NVIDIA Tesla K40 GPU,
while our models are trained on a CPU us-
ing 20 threads. Table 2 shows that methods us-

ing convolutions are several orders of magnitude
slower than fastText. While it is possible
to have a 10× speed up for char-CNN by using
more recent CUDA implementations of convolu-
tions, fastText takes less than a minute to train
on these datasets. The GRNNs method of Tang et
al. (2015) takes around 12 hours per epoch on CPU
with a single thread. Our speed-up compared to
neural network based methods increases with the
size of the dataset, going up to at least a 15,000×
speed-up.

3.2 Tag prediction

Dataset and baselines. To test scalability of
our approach, further evaluation is carried on
the YFCC100M dataset (Thomee et al., 2016)
which consists of almost 100M images with cap-
tions, titles and tags. We focus on predicting the
tags according to the title and caption (we do not
use the images). We remove the words and tags
occurring less than 100 times and split the data
into a train, validation and test set. The train
set contains 91,188,648 examples (1.5B tokens).
The validation has 930,497 examples and the test
set 543,424. The vocabulary size is 297,141 and
there are 312,116 unique tags. We will release a
script that recreates this dataset so that our num-

429



Input Prediction Tags

taiyoucon 2011 digitals: individuals digital pho-
tos from the anime convention taiyoucon 2011 in
mesa, arizona. if you know the model and/or the
character, please comment.

#cosplay #24mm #anime #animeconvention
#arizona #canon #con #convention
#cos #cosplay #costume #mesa #play
#taiyou #taiyoucon

2012 twin cities pride 2012 twin cities pride pa-
rade

#minneapolis #2012twincitiesprideparade #min-
neapolis #mn #usa

beagle enjoys the snowfall #snow #2007 #beagle #hillsboro #january
#maddison #maddy #oregon #snow

christmas #christmas #cameraphone #mobile

euclid avenue #newyorkcity #cleveland #euclidavenue

Table 4: Examples from the validation set of YFCC100M dataset obtained with fastText with 200
hidden units and bigrams. We show a few correct and incorrect tag predictions.

Model prec@1
Running time

Train Test

Freq. baseline 2.2 - -
Tagspace, h = 50 30.1 3h8 6h
Tagspace, h = 200 35.6 5h32 15h

fastText, h = 50 31.2 6m40 48s
fastText, h = 50, bigram 36.7 7m47 50s
fastText, h = 200 41.1 10m34 1m29
fastText, h = 200, bigram 46.1 13m38 1m37

Table 5: Prec@1 on the test set for tag predic-
tion on YFCC100M. We also report the training
time and test time. Test time is reported for a sin-
gle thread, while training uses 20 threads for both
models.

bers could be reproduced. We report precision
at 1.

We consider a frequency-based baseline which
predicts the most frequent tag. We also compare
with Tagspace (Weston et al., 2014), which is a
tag prediction model similar to ours, but based
on the Wsabie model of Weston et al. (2011).
While the Tagspace model is described using con-
volutions, we consider the linear version, which
achieves comparable performance but is much
faster.

Results and training time. Table 5 presents
a comparison of fastText and the baselines.
We run fastText for 5 epochs and com-
pare it to Tagspace for two sizes of the hidden
layer, i.e., 50 and 200. Both models achieve a
similar performance with a small hidden layer, but
adding bigrams gives us a significant boost in ac-
curacy. At test time, Tagspace needs to compute
the scores for all the classes which makes it rel-
atively slow, while our fast inference gives a sig-

nificant speed-up when the number of classes is
large (more than 300K here). Overall, we are more
than an order of magnitude faster to obtain model
with a better quality. The speedup of the test phase
is even more significant (a 600× speedup). Table 4
shows some qualitative examples.

4 Discussion and conclusion

In this work, we propose a simple baseline
method for text classification. Unlike unsuper-
visedly trained word vectors from word2vec, our
word features can be averaged together to form
good sentence representations. In several tasks,
fastText obtains performance on par with re-
cently proposed methods inspired by deep learn-
ing, while being much faster. Although deep neu-
ral networks have in theory much higher represen-
tational power than shallow models, it is not clear
if simple text classification problems such as sen-
timent analysis are the right ones to evaluate them.
We will publish our code so that the research com-
munity can easily build on top of our work.

Acknowledgement. We thank Gabriel Syn-
naeve, Hervé Gégou, Jason Weston and Léon Bot-
tou for their help and comments. We also thank
Alexis Conneau, Duyu Tang and Zichao Zhang for
providing us with information about their meth-
ods.

References
Alekh Agarwal, Olivier Chapelle, Miroslav Dudı́k, and

John Langford. 2014. A reliable effective terascale
linear learning system. Journal of Machine Learn-
ing Research, 15(Mar):1111–1133.

Ronan Collobert and Jason Weston. 2008. A uni-
fied architecture for natural language processing:

430



Deep neural networks with multitask learning. In
Proceedings of the 25th International Conference
on Machine Learning, ICML ’08, pages 160–167,
Helsinki, Finland. ACM.

Alexis Conneau, Holger Schwenk, Loı̈c Barrault, and
Yann Lecun. 2016. Very deep convolutional
networks for natural language processing. arXiv
preprint arXiv:1606.01781.

Scott Deerwester, Susan T. Dumais, George W. Fur-
nas, Thomas K. Landauer, and Richard Harshman.
1990. Indexing by latent semantic analysis. Jour-
nal of the American Society for Information Science,
41(6):391–407.

Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-
Rui Wang, and Chih-Jen Lin. 2008. Liblinear: A
library for large linear classification. Journal of Ma-
chine Learning Research, 9(Aug):1871–1874.

Joshua Goodman. 2001. Classes for fast maximum en-
tropy training. In Proceedings of the International
Conference on Acoustics, Speech, and Signal Pro-
cessing, volume 1, pages 561–564, Salt Lake City,
USA. IEEE.

Thorsten Joachims. 1998. Text categorization with
support vector machines: Learning with many rel-
evant features. In Claire Nédellec and Céline Rou-
veirol, editors, 10th European Conference on Ma-
chine Learning, pages 137–142, Chemnitz, Ger-
many. Springer Berlin Heidelberg.

Yoon Kim. 2014. Convolutional neural networks
for sentence classification. In Proceedings of the
2014 Conference on Empirical Methods in Natu-
ral Language Processing (EMNLP), pages 1746–
1751, Doha, Qatar, October. Association for Com-
putational Linguistics.

Omer Levy, Yoav Goldberg, and Ido Dagan. 2015. Im-
proving distributional similarity with lessons learned
from word embeddings. Transactions of the Associ-
ation for Computational Linguistics, 3:211–225.

Andrew McCallum and Kamal Nigam. 1998. A com-
parison of event models for naive bayes text clas-
sification. In AAAI workshop on learning for text
categorization, pages 41–48, Madison, USA.

Tomáš Mikolov, Anoop Deoras, Daniel Povey, Lukáš
Burget, and Jan Černockỳ. 2011. Strategies for
training large scale neural network language models.
In Workshop on Automatic Speech Recognition Un-
derstanding, pages 196–201, Waikoloa, USA. IEEE.

Tomáš Mikolov, Kai Chen, Greg Corrado, and Jeffrey
Dean. 2013. Efficient estimation of word represen-
tations in vector space. In 1st International Con-
ference on Learning Representations (ICLR), Scotts-
dale, USA.

Bo Pang and Lillian Lee. 2008. Opinion mining and
sentiment analysis. Foundations and Trends in In-
formation Retrieval, 2(1-2):1–135, January.

H. Schütze. 1992. Dimensions of meaning. In Pro-
ceedings of the 1992 ACM/IEEE Conference on Su-
percomputing, Supercomputing ’92, pages 787–796,
Los Alamitos, CA, USA. IEEE Computer Society
Press.

Duyu Tang, Bing Qin, and Ting Liu. 2015. Document
modeling with gated recurrent neural network for
sentiment classification. In Proceedings of the 2015
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 1422–1432, Lisbon, Portu-
gal, September. Association for Computational Lin-
guistics.

Bart Thomee, David A. Shamma, Gerald Fried-
land, Benjamin Elizalde, Karl Ni, Douglas Poland,
Damian Borth, and Li-Jia Li. 2016. YFCC100M:
The new data in multimedia research. Communica-
tions of the ACM, 59(2):64–73.

Sida Wang and Christopher Manning. 2012. Baselines
and bigrams: Simple, good sentiment and topic clas-
sification. In Proceedings of the 50th Annual Meet-
ing of the Association for Computational Linguistics
(Volume 2: Short Papers), pages 90–94, Jeju Island,
Korea, July. Association for Computational Linguis-
tics.

Kilian Weinberger, Anirban Dasgupta, John Langford,
Alex Smola, and Josh Attenberg. 2009. Feature
hashing for large scale multitask learning. In Pro-
ceedings of the 26th Annual International Confer-
ence on Machine Learning, ICML ’09, pages 1113–
1120, New York, NY, USA. ACM.

Jason Weston, Samy Bengio, and Nicolas Usunier.
2011. Wsabie: Scaling up to large vocabulary image
annotation. In Proceedings of the Twenty-Second
International Joint Conference on Artificial Intel-
ligence - Volume Volume Three, IJCAI’11, pages
2764–2770. AAAI Press.

Jason Weston, Sumit Chopra, and Keith Adams. 2014.
#tagspace: Semantic embeddings from hashtags.
In Proceedings of the 2014 Conference on Em-
pirical Methods in Natural Language Processing
(EMNLP), pages 1822–1827, Doha, Qatar, October.
Association for Computational Linguistics.

Yijun Xiao and Kyunghyun Cho. 2016. Efficient
character-level document classification by combin-
ing convolution and recurrent layers. arXiv preprint
arXiv:1602.00367.

Xiang Zhang and Yann LeCun. 2015. Text understand-
ing from scratch. arXiv preprint arXiv:1502.01710.

Xiang Zhang, Junbo Zhao, and Yann LeCun. 2015.
Character-level convolutional networks for text clas-
sification. In Advances in Neural Information
Processing Systems 28, pages 649–657, Montreal,
Canada.

431


