



















































Improving Event Coreference Resolution by Learning Argument Compatibility from Unlabeled Data


Proceedings of NAACL-HLT 2019, pages 785–795
Minneapolis, Minnesota, June 2 - June 7, 2019. c©2019 Association for Computational Linguistics

785

Improving Event Coreference Resolution by Learning Argument
Compatibility from Unlabeled Data

Yin Jou Huang1, Jing Lu2, Sadao Kurohashi1, Vincent Ng2
1Graduate School of Informatics, Kyoto University

2Human Language Technology Research Institute, University of Texas at Dallas
huang@nlp.ist.i.kyoto-u.ac.jp, kuro@i.kyoto-u.ac.jp

{ljwinnie,vince}@hlt.utdallas.edu

Abstract

Argument compatibility is a linguistic condi-
tion that is frequently incorporated into mod-
ern event coreference resolution systems. If
two event mentions have incompatible argu-
ments in any of the argument roles, they can-
not be coreferent. On the other hand, if these
mentions have compatible arguments, then this
may be used as information toward deciding
their coreferent status. One of the key chal-
lenges in leveraging argument compatibility
lies in the paucity of labeled data. In this work,
we propose a transfer learning framework for
event coreference resolution that utilizes a
large amount of unlabeled data to learn the ar-
gument compatibility between two event men-
tions. In addition, we adopt an interactive in-
ference network based model to better capture
the (in)compatible relations between the con-
text words of two event mentions. Our exper-
iments on the KBP 2017 English dataset con-
firm the effectiveness of our model in learn-
ing argument compatibility, which in turn im-
proves the performance of the overall event
coreference model.

1 Introduction

Events are essential building blocks of all kinds of
natural language text. An event can be described
several times from different aspects in the same
document, resulting in multiple surface forms of
event mentions. The goal of event coreference
resolution is to identify event mentions that cor-
respond to the same real-world event. This task
is critical for natural language processing applica-
tions that require deep text understanding, such as
storyline extraction/generation, text summariza-
tion, question answering, and information extrac-
tion.

Figure 1 shows a document consisting of three
events described by six different event mentions.
Among these event mentions, m1, m2 and m4 are

Figure 1: A document with three events described in
six event mentions. Coreferent event mentions are
highlighted with the same color.

coreferent, since they all correspond to the event
of the KMT party electing a new party chief. Sim-
ilarly, m3 and m5 are also coreferent, while m6 is
not coreferent with any other event mentions.

An event mention consists of a trigger and zero
or more arguments. The trigger of an event men-
tion is the word/phrase that is considered the most
representative of the event, such as the word meet-
ing for m3 or the word elected for m6. Triggers of
coreferent event mentions must be related, that is,
they should describe the same type of events. For
example, m1 and m3 cannot be coreferent, since
their trigger words — elect and meeting — are
not related.

Arguments are the participants of an event, each
having its role. For example, KMT is the AGENT-
argument and new party chief is the PATIENT-
argument of m1. Argument compatibility is an
important linguistic condition for determining the
coreferent status between two event mentions.
Two arguments are incompatible if they do not
correspond to the same real-world entity when
they are expressed in the same level of specificity;



786

Figure 2: System overview.

otherwise, they are compatible. For example, a
pair of TIME-arguments — Wednesday and 2005
— which are expressed in different level of speci-
ficity, are considered compatible. If two event
mentions have incompatible arguments in some
specific argument roles, they cannot be coreferent.
For example, m2 and m6 are not coreferent since
their TIME-arguments — January 2012 and 2005
— and their PATIENT-arguments — a new chair-
person and Ma — are incompatible. On the other
hand, coreferent event mentions can only have
compatible arguments. For example, m3 and m5
both have Wednesday as TIME-arguments. In this
example, argument compatibility in the TIME ar-
gument role is a strong hint suggesting their coref-
erence.

Despite its importance, incorporating argument
compatibility into event coreference systems is
challenging due to the lack of sufficient labeled
data. Many existing works have relied on imple-
menting argument extractors as upstream compo-
nents and designing argument features that cap-
ture argument compatibility in event coreference
resolvers. However, the error introduced in each
of the steps propagates through these resolvers and
hinders their performance considerably.

In light of the aforementioned challenge, we
propose a framework for transferring argument
(in)compatibility knowledge to the event coref-
erence resolution system, specifically by adopt-
ing the interactive inference network (Gong et al.,
2018) as our model structure. The idea is as
follows. First, we train a network to determine
whether the corresponding arguments of an event
mention pair are compatible on automatically la-
beled training instances collected from a large
unlabeled news corpus. Second, to transfer the
knowledge of argument (in)compatibility to an

event coreference resolver, we employ the net-
work (pre)trained in the previous step as a starting
point and train it to determine whether two event
mentions are coreferent on manually labeled event
coreference corpora. Third, we iteratively repeat
the above two steps, where we use the learned
coreference model to relabel the argument com-
patibility instances, retrain the network to deter-
mine argument compatibility, and use the result-
ing pretrained network to learn an event corefer-
ence resolver. In essence, we mutually bootstrap
the argument (in)compatibility determination task
and the event coreference resolution task.

Our contributions are three-fold. First, we uti-
lize and leverage the argument (in)compatibility
knowledge acquired from a large unlabeled cor-
pus for event coreference resolution. Second, we
employ the interactive inference network as our
model structure to iteratively learn argument com-
patibility and event coreference resolution. Ini-
tially proposed for the task of natural language in-
ference, the interactive inference network is suit-
able for capturing the semantic relations between
word pairs. Experimental results on the KBP
coreference dataset show that this network archi-
tecture is also suitable for capturing the argument
compatibility between event mentions. Third, our
model achieves state-of-the-art results on the KBP
2017 English dataset (Ellis et al., 2015, 2016; Get-
man et al., 2017), which confirms the effectiveness
of our method.

2 Related Work

Ablation experiments conducted by Chen and Ng
(2013) provide empirical support for the useful-
ness of event arguments for event coreference res-
olution. Hence, it should not be surprising that,
with just a few exceptions (e.g., Sangeetha and



787

event mention DATE-compatibility with ma
ma The result of the election last October surprised everyone. -
m1 He was elected as president in 2005. no
m2 The presidential election took place on October 20th. yes
m3 The opposition party won the election. yes

Table 1: Examples of NER-based sample filtering. The phrases tagged as DATE are underlined, and the trigger
words are boldfaced.

Arock (2012); Araki and Mitamura (2015); Lu and
Ng (2017)), argument features have been exten-
sively exploited in event coreference systems to
capture the argument compatibility between two
event mentions. Basic features such as the num-
ber of overlapping arguments and the number of
unique arguments, and a binary feature encoding
whether arguments are conflicting have been pro-
posed (Chen et al., 2009; Chen and Ji, 2009; Chen
and Ng, 2016). More sophisticated features based
on different kinds of similarity measures have
also been considered, such as the surface similar-
ity based on Dice coefficient and the WuPalmer
WordNet similarity between argument heads (Mc-
Conky et al., 2012; Cybulska and Vossen, 2013;
Araki et al., 2014; Liu et al., 2014; Krause et al.,
2016). However, these features are computed us-
ing either the outputs of event argument extrac-
tors and entity coreference resolvers (Ahn, 2006;
Chen and Ng, 2014, 2015; Lu and Ng, 2016) or se-
mantic parsers (Bejan and Harabagiu, 2014; Yang
et al., 2015; Peng et al., 2016) and therefore suf-
fer from serious error propagation issues (see Lu
and Ng (2018)). Several previous works proposed
joint models to address this problem (Lee et al.,
2012; Lu et al., 2016), while others utilized iter-
ative methods to propagate argument information
(Liu et al., 2014; Choubey and Huang, 2017) in
order to alleviate this issue. However, all of these
methods still rely on argument extractors to iden-
tify arguments and their roles.

3 Method

Our proposed transfer learning framework con-
sists of two learning stages, the pretraining stage
of an argument compatibility classifier and the
fine-tuning stage of an event coreference resolver
(Figure 2). We provide the details of both stages
in sections 3.1 and 3.2, and describe the iterative
strategy combining the two training stages in sec-
tion 3.3. Details on the model structure are cov-
ered in section 3.4.

3.1 Argument Compatibility Learning

In the pretraining stage, we train the model as an
argument compatibility classifier with event men-
tions extracted from a large unlabeled news cor-
pus.

Task definition Given a pair of event mentions
(ma, mb) with related triggers, predict whether
their arguments are compatible or not.

Here, an event mention is represented by a trig-
ger word and the context words within an n-word
window around the trigger.

Related trigger extraction We analyze the
event coreference resolution corpus and extract
trigger pairs that are coreferent more than k times
in the training data. We define these trigger pairs
to be related triggers in our experiment. In this
work, we set k to 10. Table 2 shows some exam-
ples of related triggers with high counts.

trigger pair count
kill - death 86

shoot - shooting 35
retire - retire 34

demonstration - protest 30

Table 2: Examples of related triggers.

If the triggers of an event mention pair are re-
lated, their coreferent status cannot be determined
by looking at the triggers alone, and this is the
case in which argument compatibility affects the
coreferent status most directly. Thus, we focus on
the event mention pairs with related triggers in the
pretraining stage of argument compatibility learn-
ing.

Compatible samples extraction From each
document, we extract event mention pairs with
related triggers and check whether the following
conditions are satisfied:

1. DATE-compatibility (Table 1):
First, we perform named entity recognition



788

(NER) on the context words. If both event
mentions have phrases tagged as DATE in
the context, these two phrases must contain at
least one overlapping word. If there are mul-
tiple phrases tagged as DATE in the context,
only the phrase closest to the trigger word is
considered.

2. PERSON-compatibility: Similar to 1.

3. NUMBER-compatibility: Similar to 1.

4. LOCATION-compatibility: Similar to 1.

5. Apart from function words, the ratio of over-
lapping words in their contexts must be un-
der 0.3 for both event mentions. We add this
constraint in order to remove trivial samples
of nearly identical sentences.

Conditions 1–4 are heuristic filtering rules
based on NER tags, which aim to remove samples
with apparent incompatibilities. Here, we consider
four NER types — DATE, PERSON, NUMBER,
and LOCATION — because these types of words
are the most salient types of incompatibility that
can be observed between event mentions. Condi-
tion 5 aims to remove event mention pairs that are
“too similar”. We add this condition because we
do not want our model to base its decisions on the
number of overlapping words between the event
mentions.

We collect event mention pairs satisfying all the
above conditions as our initial set of compatible
samples.

Incompatible sample extraction From differ-
ent documents in the corpus, we extract event
mentions with related triggers and check whether
the following conditions are satisfied:

1. The creation date of the two documents must
be at least one month apart.

2. Apart from the trigger words and the function
words, the context of the event mentions must
contain at least one overlapping word.

In the unlabeled news corpus, articles describ-
ing similar news events are sometimes present.
Thus, we use condition 1 to roughly assure that the
event mention pairs extracted are not coreferent.
Mention pairs extracted from the same document
tend to contain overlapping content words, so to
prevent our model to make decisions based on the

existence of overlapping words, we add condition
2 as a constraint.

We collect event mention pairs satisfying all the
above conditions as our initial set of incompatible
samples.

Argument compatibility classifier With the
initial set of compatible and incompatible samples
acquired above, we train a binary classier to dis-
tinguish between samples of the two sets.

3.2 Event Coreference Learning

In the fine-tuning stage, we adapt the argument
compatibility classifier on the labeled event coref-
erence data to a mention-pair event coreference
model.

3.2.1 Event Mention Detection
Before proceeding to the task of event coreference
resolution, we have to identify the event mentions
in the documents. We train a separate event men-
tion detection model to identify event mentions
along with their subtypes.

We model event mention detection as a multi-
class classification problem. Given a candidate
word along with its context, we predict the subtype
of the event mention triggered by the word. If the
given candidate word is not a trigger, we label it as
NULL. We select the words that have appeared as
a trigger at least once in the training data as candi-
date trigger words. We do not consider multi-word
triggers in this work.

Given an input sentence, we first represent each
of its comprising words by the concatenation of
the word embedding and the character embedding
of the word. These representation vectors are fed
into a bidirectional LSTM (biLSTM) layer to ob-
tain the hidden representation of each word.

For each candidate word in the sentence, its hid-
den representation is fed into the inference layer to
predict the class label. Since the class distribution
is highly unbalanced, with the NULL label signif-
icantly outnumbering all the other labels, we use a
weighted softmax at the inference layer to obtain
the probability of each class. In this work, we set
the weight to 0.1 for the NULL class label and 1
for all the other class labels.

Intuitively, candidate triggers with the same sur-
face form in the same document tend to have the
same class label. However, it is difficult to model
this consistency since our model operates at the
sentence level. Thus, we account for this con-



789

sistency across sentences by the following post-
processing step: If a candidate word is assigned
the NULL label but more than half of the candi-
dates sharing the same surface form is detected
as triggers of a specific subtype, then we change
the label to this given subtype. Also, we disre-
gard event mentions with types contact, movement
and transaction in this post-processing step, since
the subtypes under these three types do not have a
good consistency across different sentences in the
same document.

3.2.2 Mention-Pair Event Coreference Model

With the argument compatibility classifier trained
in the previous stage, we use the labeled event
coreference corpus to fine-tune the model into an
event coreference resolver. We design the event
coreference resolver to be a mention-pair model
(Soon et al., 2001), which takes a pair of event
mentions as the input and outputs the likelihood
of them being coreferent.

With the pairwise event coreference predictions,
we further conduct best-first clustering (Ng and
Cardie, 2002) on the pairwise results to build
the event coreference clusters of each document.
Best-first clustering is an agglomerative clustering
algorithm that links each event mention to the an-
tecedent event mention with the highest corefer-
ence likelihood given the likelihood is above an
empirically determined threshold.

3.3 Iterative Relabeling Strategy

Previously, we collected a set of compatible event
mentions from the same document with simple
heuristic filtering. Despite this filtering step, the
initial compatible set is noisy. Here, we intro-
duce an iterative relabeling strategy to improve the
quality of the compatible set of event mentions.

First, we calculate the coreference likelihood of
the event mentions in the initial compatible set.
Mention pairs with a coreference likelihood above
threshold θM are added to the new compatible set.
On the other hand, mention pairs with a corefer-
ence likelihood below θm are added to the initial
incompatible set to form the new incompatible set.
With the new compatible and incompatible sets,
we can start another iteration of transfer learning
to train a coreference resolver with improved qual-
ity. In this work, we set θM to 0.8 and θm to 0.2.

3.4 Model Structure
We adopt an interactive inference network as the
model structure of our proposed method (Figure
3). A qualitative analysis of an interactive in-
ference network shows that it is good at captur-
ing word overlaps, antonyms and paraphrases be-
tween sentence pairs (Gong et al., 2018). Thus, we
believe this network is suitable for capturing the
argument compatibility between two event men-
tions. The model consists of the following com-
ponents:

Model inputs The input to the model is a pair
of event mentions (ma, mb), with ma being the
antecedent mention of mb:

ma = {w1a, w2a, ..., wNa }
mb = {w1b , w2b , ..., wNb }

(1)

Each event mention is represented by a sequence
of N tokens consisting of one trigger word and its
context. Here, we take the context to be the words
within an n-word window around the trigger. In
this work, n is set to 10.

Embedding layer We represent each input to-
ken by the concatenation of the following compo-
nents:

Word embedding The word representation of
the given token. We use pretrained word vectors to
initialize the word embedding layer.

Character embedding To identify
(in)compatibilities regarding person, orga-
nization or location names, the handling of
out-of-vocabulary (OOV) words is critical.

Adding character-level embeddings can allevi-
ate the OOV problem (Yang et al., 2017). Thus,
we apply a convolutional neural network over the
comprising characters of each token to acquire the
corresponding character embedding.

POS and NER one-hot vectors One-hot vec-
tors of the part-of-speech (POS) tag and NER tag.

Exact match A binary feature indicating
whether a given token appears in the context of
both event mentions. This feature is proved useful
for several NLP tasks operating on pairs of texts
(Chen et al., 2017; Gong et al., 2018; Pan et al.,
2018).

Trigger position We encode the position of
the trigger word by adding a binary feature to in-
dicate whether a given token is a trigger word.



790

Figure 3: Model structure.

Encoding layer We pass the sequence of em-
bedding vectors into a biLSTM layer (Hochreiter
and Schmidhuber, 1997), resulting in a sequence
of hidden vectors of size |h|:

hia = biLSTM(emb(w
i
a), h

i−1
a )

hib = biLSTM(emb(w
i
b), h

i−1
b )

(2)

where emb(w) is the embedding vector of token
w.

Interaction layer The interaction layer captures
the relations between two event mentions based on
the hidden vectors ha and hb. The interaction ten-
sor I , a 3-D tensor of shape (N , N , |h|), is calcu-
lated by taking the pairwise multiplication of the
corresponding hidden vectors:

Iij = h
i
a ◦ h

j
b (3)

Finally, we apply a multi-layer convolutional neu-
ral network to extract the event pair representation
vector fev.

Inference layer In the pretraining stage, we feed
fev to a fully-connected inference layer to make a
binary prediction of argument compatibility.

As for the fine-tuning stage, we concatenate an
auxiliary feature vector faux to fev before feeding
it into the inference layer. faux consists of two fea-
tures, a one-hot vector that encodes the sentence
distance between the two event mentions and the
difference of the word embedding vectors of the
two triggers.

4 Evaluation

4.1 Experimental Setup

4.1.1 Corpora
We use English Gigaword (Parker et al., 2009) as
the unlabeled corpus for argument compatibility
learning. This corpus consists of the news arti-
cles from five news sources, each annotated with
its creation date.

As for event coreference resolution, we use
the English portion of the KBP 2015 and 2016
datasets (Ellis et al., 2015, 2016) for training, and
the KBP 2017 dataset (Getman et al., 2017) for
evaluation. The KBP datasets comprise news arti-
cles and discussion forum threads. The KBP 2015,
2016, and 2017 corpora contain 648, 169, and 167
documents, respectively. Each document is anno-
tated with event mentions of 9 types and 18 sub-
types, along with the coreference clusters of these
event mentions.

4.1.2 Implementation Details
Preprocessing We use the Stanford CoreNLP
toolkit (Manning et al., 2014) to perform prepro-
cessing on the input data.

Network structure Each word embedding is
initialized with the 300-dimensional pretrained
GloVe embedding (Pennington et al., 2014). The
character embedding layer is a combination of an
8-dimensional embedding layer and three 1D con-
volution layers with a kernel size of 5 with 100
filters. The size of the biLSTM layer is 200.
The maximum length of a word is 16 characters;
shorter words are padded with zero and longer



791

MUC B3 CEAFe BLANC AVG-F
biLSTM (standard) 29.49 43.15 39.91 24.15 34.18
biLSTM (transfer) 33.84 42.91 38.39 26.59 35.43
Interact (standard) 31.12 42.84 39.01 24.99 34.49
Interact (transfer) 34.28 42.93 39.95 32.12 36.24
Interact (transfer, 2nd iter) 35.66 43.20 40.02 32.43 36.75
Interact (transfer, 3rd iter) 36.05 43.07 39.69 28.06 36.72
Jiang et al. (2017) 30.63 43.84 39.86 26.97 35.33

Table 3: Event coreference resolution results of our proposed system, compared with the biLSTM baseline model
and the current state-of-the-art system.

words are cropped. For the interaction layer, we
use convolution layers with a kernel size of 3 in
combination with max-pooling layers. The size of
the inference layer is 128. Sigmoid activation is
used for the inference layer, and all other layers
use ReLU as the activation function.

Event mention detection model For word em-
beddings, we use the concatenation of a 300-
dimensional pretrained GloVe embedding and the
50-dimensional embedding proposed by Turian
et al. (2010). The character embedding layer is a
combination of an 8-dimensional embedding layer
and three 1D convolution layers with kernel sizes
of 3, 4, 5 with 50 filters.

4.1.3 Evaluation Metrics
We follow the standard evaluation setup adopted
in the official evaluation of the KBP event nugget
detection and coreference task. This evaluation
setup is based on four distinct scoring measures —
MUC (Vilain et al., 1995) , B3 (Bagga and Bald-
win, 1998), CEAFe (Luo, 2005) and BLANC (Re-
casens and Hovy, 2011) — and the unweighted av-
erage of their F-scores (AVG-F). We use AVG-F
as the main evaluation measure when comparing
system performances.

4.2 Results

We present the experimental results on the KBP
2017 corpus in Table 3. In the following, we com-
pare the performance of methods with different
network architectures and experimental settings.

Comparison of network architectures We
compare the results of the interactive inference
network (Interact) with the biLSTM baseline
model (biLSTM).

The biLSTM baseline model does not have the
interaction layer. Instead, the last hidden vectors
of the biLSTM layer are concatenated and fed into
the inference layer directly.

When trained solely on the event coreference
corpus (standard), the model with the interactive
inference network performs slightly better than the
biLSTM baseline model, as shown in rows 1 and
3. However, with an additional pretraining step
of argument compatibility learning (transfer), the
interact inference network outperforms the biL-
STM baseline model by a considerable margin, as
shown in rows 2 and 4. We conclude that the in-
teractive inference network can better capture the
complex interactions between two event mentions,
accounting for the difference in performance.

Effect of transfer learning Regardless of the
network structure, we observe a considerable
improvement in performance by pretraining the
model as an argument compatibility classifier. The
biLSTM baseline model achieves an improvement
of 1.25 points in AVG-F by doing transfer learn-
ing, as can be seen in rows 1 and 2. As for the
interactive inference network, an improvement of
1.75 points in AVG-F is achieved, as can be seen
in rows 3 and 4. These results provide sugges-
tive evidence that our proposed transfer learning
framework, which utilizes a large unlabeled cor-
pus to perform argument compatibility learning, is
effective.

Effect of iterative relabeling We achieve an-
other boost in performance by using the trained
event coreference resolver to relabel the training
samples for argument compatibility learning. The
best result is achieved after two iterations (row 5)
with an improvement of 2.26 points in AVG-F
compared to the standard interactive inference net-
work (row 3). However, we are not able to ob-
tain further gains with more iterations of relabel-
ing (row 6). We speculate that the difference in
event coreference model predictions across differ-
ent iterations is not big enough to have a perceiv-
able impact, but additional experiments are needed
to determine the reason.



792

Type Event Mention Pair Gold System

Explicit
m1: ... the building where 13 people were killed will be razed, and a memorial ...
m2: In that case, George Hennard killed 23 people at a Luby ’s restaurant, ...

non-coref non-coref

Explicit
m1: Ten relatives of the victims arrived at the airport Sunday before traveling to the city of Jiangshan.
m2: On Monday , the victims’ relatives went to the Jiangshan Municipal Funeral Parlor.

non-coref non-coref

Implicit
m1: ... a young woman protester was brutally slapped while she was demonstrating ...
m2: ... explain why a women protester in her 60s was beaten up by policemen ...

non-coref coref

Implicit
m1: She died from a brain hemorrhage on July 10, 2003, ...
m2: ... has denied killing his second wife, whom he says died in a car accident.

non-coref non-coref

Implicit
m1: Nationwide demonstrations held in France to protest gay marriage.
m2: ... to protest against the country’s plan to legalize same-sex marriage.

coref coref

General
m1: ... Connecticut elementary school shooting has reignited the debate over gun control.
m2: Gun supporters hold that people, not guns, are to blame for the shootings.

non-coref coref

General
m1: Industrial accidents have injured and killed Foxconn workers, and the company also experienced ...
m2: ... explosion in May 2011 at Foxconn ’s Chengdu factory killed three workers ...

non-coref non-coref

Table 4: Examples of event pairs with related triggers. Trigger words are boldfaced, and words with
(in)compatibility information are colored in blue.

Comparison with the state of the art Com-
paring row 5 and 7, we can see that our method
outperforms the previous state-of-the-art model
(Jiang et al., 2017) by 1.42 points in AVG-F.

5 Discussion

In this section, we conduct a qualitative analysis
of the outputs of our best-performing system (the
Interact (transfer, 2nd iter) system in Table 3) on
the event coreference dataset and the unseen event
mention pairs extracted from the unlabeled corpus.

5.1 Compatibility Classification

We focus on the samples with related triggers hav-
ing either compatible or incompatible arguments
(Table 4). These samples can be roughly classified
into the following categories:

Explicit argument compatibility The existence
of identical/distinct time phrases, numbers, loca-
tion names or person names in the context is the
most explicit form of (in)compatibility.

For these event pairs, the existence of identi-
cal/distinct phrases with the same NER type is a
direct clue toward deciding their coreferent status.
Making use of this nature, we perform filtering on
the set of compatible samples acquired from the
unlabeled corpus in order to remove samples with
explicit incompatibility.

Our model can recognize this type of
(in)compatibility with a relatively high accu-
racy. Both examples shown in Table 4 are
predicted correctly.

Implicit argument compatibility Event pairs
with implicit (in)compatible arguments require ex-
ternal knowledge to resolve.

We present three examples in Table 4. In the
first example, the knowledge that a woman in her
60s is generally not referred to as being young is
required to determine the incompatibility. Simi-
larly, the knowledge that both brain hemorrhage
and car accident are causes of people’s death are
required to classify the second example correctly.

While the performance on samples with implicit
(in)compatibility is not as good as that on samples
with explicit (in)compatibility, our system is able
to capture implicit (in)compatibility to some ex-
tent. We believe that this type of (in)compatibility
is difficult to be captured with the argument fea-
tures that are designed based on the outputs of
argument extractors and entity coreference re-
solvers, and that the ability to resolve implicit
(in)compatibility contributes largely to our sys-
tem’s performance improvements.

General-specific incompatibilities Event men-
tions describing general events pose special chal-
lenges to the task of event coreference resolution.

In Table 4, we present two typical examples
of this category. In the first example, the second
event mention does not refer to any specific shoot-
ing event in the real world, in contrast to the first
event mention, which describes a specific school
shooting event. Similarly for the second exam-
ple, where the first event mention depicts a gen-
eral event and the second event mention depicts a
specific one.

General event mentions typically have few or
even no arguments and modifiers, making the
identification of non-coreference relations very
challenging. Since we cannot rely on argument
compatibility, a deeper understanding of the se-
mantics of the event mentions is needed. General



793

Event Mention Pair Type System

I

m1: What would have happened if Steve Jobs had never left Apple ... - -
ma2: ...in the state that is today if John hadn’t left. Explicit non-coref
mb2: ...in the state that is today if she hadn’t left. Implicit non-coref
mc2: ...in the state that is today if he hadn’t left. Implicit coref

II

m1: Police arrest 6 men for gangraping housewife in northern India. - -
ma2: Indian police have arrested six men for allegedly gangraping a 29-year-old housewife ... Explicit coref
mb2: Indian police have arrested six men for allegedly gangraping a woman ... Implicit coref
mc2: Indian police have arrested six men for allegedly gangraping a medical student ... Implicit non-coref

III
m1: Nationwide demonstrations in France to protest gay marriage. - -
ma2: ...took to the streets across the country to protest against the country’s plan to legalize same-sex marriage. Implicit coref
mb2: ...took to the streets across the country to protest against the contentious citizenship amendment bill. Implicit non-coref

Table 5: Case study on manually-generated event mention pairs. Trigger words are boldfaced, and the target
arguments are colored in blue.

event mentions account for a considerable fraction
of our system’s error, since they are quite perva-
sive in both news articles and discussion forum
threads.

5.2 Case Study
To better understand the behavior of our system,
we perform a case study on manually-generated
event pairs. Specifically, for a given pair of event
mentions, we first alter only one of the arguments
and keep the rest of the content fixed. We then
observe the behavior of the system across different
variations of the altered argument (Table 5).

Example I In this example, we pick the
AGENT-argument as the target and alter the
AGENT-argument of the second event mention.
The event pair (m1, ma2) is non-coreferent due
to the explicit incompatibility between Steve Jobs
and John, and the system’s prediction is also non-
coreferent. Further, we alter the target argument
to the pronoun she (mb2), resulting in an implicit
incompatibility in the AGENT argument since the
Steve Jobs is generally not considered a feminine
name. As expected, the system classifies the event
pair (m1,mb2) as non-coreferent. Finally, when we
alter the target argument to he (mc2), the system
correctly classifies the resulting pair as coreferent.

Example II In this example, we pick the
PATIENT-argument as the target and alter the
PATIENT-argument of the second event mention.
The system classifies the event pair (m1, ma2)
as coreferent, which is reasonable considering
the presence of the explicit compatible arguments
housewife and 29-year-old housewife. Further,
when we alter the target argument to woman (mb2),
the system output is still coreferent. This is con-
sistent with our prediction: the event mentions are
likely to be coreferent judging only from the con-

text of the two event mentions. However, when we
alter the target argument to medical student (mc2),
the event pair would become non-coreferent due to
the incompatibility between medical student and
housewife. The system classifies the event pair
correctly.

Example III In this example, we pick the
REASON-argument as the target and alter the
REASON-argument of the second event mention.
The event pair (m1, ma2) has a pair of implicit
compatible arguments in the REASON-argument
role and is likely to be coreferent. In contrast, al-
tering the target argument to contentious citizen-
ship amendment bill (mb2) would yield an pair of
implicit incompatible arguments, and the resulting
event pair would become non-coreferent. Our sys-
tem classifies both event pairs correctly.

6 Conclusion

We proposed an iterative transfer learning frame-
work for event coreference resolution. Our
method exploited a large unlabeled corpus to learn
a wide range of (in)compatibilities between argu-
ments, which contributes to the improvement in
performance on the event coreference resolution
task. We achieved state-of-the-art results on the
KBP 2017 English event coreference dataset, out-
performing the previous state-of-the-art system. In
addition, a qualitative analysis of the system out-
put confirmed the ability of our system to capture
(in)compatibilities between two event mentions.

Acknowledgments

We thank the three anonymous reviewers for their
detailed comments on an earlier draft of the paper.
This work was supported in part by NSF Grants
IIS-1219142 and IIS-1528037 and JST CREST
Grant Number JPMJCR1301, Japan.



794

References
David Ahn. 2006. The stages of event extraction.

In Proceedings of the COLING/ACL Workshop on
Annotating and Reasoning about Time and Events,
pages 1–8.

Jun Araki, Zhengzhong Liu, Eduard Hovy, and Teruko
Mitamura. 2014. Detecting subevent structure for
event coreference resolution. In Proceedings of
the 9th International Conference on Language Re-
sources and Evaluation, pages 4553 – 4558.

Jun Araki and Teruko Mitamura. 2015. Joint event trig-
ger identification and event coreference resolution
with structured perceptron. In Proceedings of the
2015 Conference on Empirical Methods in Natural
Language Processing, pages 2074–2080.

Amit Bagga and Breck Baldwin. 1998. Algorithms for
scoring coreference chains. In Proceedings of the
LREC Workshop on Linguistics Coreference, pages
563–566.

Cosmin Adrian Bejan and Sanda Harabagiu. 2014. Un-
supervised event coreference resolution. Computa-
tional Linguistics, 40(2):311–347.

Chen Chen and Vincent Ng. 2013. Chinese event
coreference resolution: Understanding the state of
the art. In Proceedings of the 6th International Joint
Conference on Natural Language Processing, pages
822–828.

Chen Chen and Vincent Ng. 2014. SinoCoreferencer:
An end-to-end Chinese event coreference resolver.
In Proceedings of the 9th International Confer-
ence on Language Resources and Evaluation, pages
4532–4538.

Chen Chen and Vincent Ng. 2015. Chinese event
coreference resolution: An unsupervised probabilis-
tic model rivaling supervised resolvers. In Proceed-
ings of Human Language Technologies: The 2015
Conference of the North American Chapter of the
Association for Computational Linguistics, pages
1097–1107.

Chen Chen and Vincent Ng. 2016. Joint inference over
a lightly supervised information extraction pipeline:
Towards event coreference resolution for resource-
scarce languages. In Proceedings of the 30th AAAI
Conference on Artificial Intelligence, pages 2913–
2920.

Danqi Chen, Adam Fisch, Jason Weston, and Antoine
Bordes. 2017. Reading Wikipedia to answer open-
domain questions. In Proceedings of the 55th An-
nual Meeting of the Association for Computational
Linguistics (Volume 1: Long Papers), pages 1870–
1879.

Zheng Chen and Heng Ji. 2009. Graph-based event
coreference resolution. In Proceedings of the
ACL-IJCNLP Workshop on Graph-based Methods
for Natural Language Processing (TextGraphs-4),
pages 54–57.

Zheng Chen, Heng Ji, and Robert Haralick. 2009. A
pairwise event coreference model, feature impact
and evaluation for event coreference resolution. In
Proceedings of the RANLP Workshop on Events in
Emerging Text Types, 3, pages 17–22.

Prafulla Kumar Choubey and Ruihong Huang. 2017.
Event coreference resolution by iteratively unfold-
ing inter-dependencies among events. In Proceed-
ings of the 2017 Conference on Empirical Methods
in Natural Language Processing, pages 2124–2133.

Agata Cybulska and Piek Vossen. 2013. Semantic
relations between events and their time, locations
and participants for event coreference resolution. In
Proceedings of the 9th International Conference on
Recent Advances in Natural Language Processing,
pages 156–163.

Joe Ellis, Jeremy Getman, Dana Fore, Neil Kuster,
Zhiyi Song, Ann Bies, and Stephanie Strassel. 2015.
Overview of linguistic resources for the TAC KBP
2015 evaluations: Methodologies and results. In
Proceedings of the TAC KBP 2015 Workshop.

Joe Ellis, Jeremy Getman, Neil Kuster, Zhiyi Song,
Ann Bies, and Stephanie Strassel. 2016. Overview
of linguistic resources for the TAC KBP 2016 eval-
uations: Methodologies and results. In Proceedings
of the TAC KBP 2016 Workshop.

Jeremy Getman, Joe Ellis, Zhiyi Song, Jennifer Tracey,
and Stephanie Strassel. 2017. Overview of linguis-
tic resources for the TAC KBP 2017 evaluations:
Methodologies and results. In Proceedings of the
TAC KBP 2017 Workshop.

Yichen Gong, Heng Luo, and Jian Zhang. 2018. Nat-
ural language inference over interaction space. In
Proceedings of the 6th International Conference on
Learning Representations.

Sepp Hochreiter and Jüngen Schmidhuber. 1997.
Long short-term memory. Neural Computation,
9(8):1735–1780.

Shanshan Jiang, Yihan Li, Tianyi Qin, Qian Meng, and
Bin Dong. 2017. SRCB entity discovery and linking
(EDL) and event nugget systems for TAC 2017. In
Proceedings of the TAC KBP 2017 Workshop.

Sebastian Krause, Feiyu Xu, Hans Uszkoreit, and Dirk
Weissenborn. 2016. Event Linking with Sentential
Features from Convolutional Neural Networks. In
Proceedings of the 20th Conference on Computa-
tional Natural Language Learning, pages 239–249.

Heeyoung Lee, Marta Recasens, Angel Chang, Mihai
Surdeanu, and Dan Jurafsky. 2012. Joint entity and
event coreference resolution across documents. In
Proceedings of the 2012 Joint Conference on Empir-
ical Methods in Natural Language Processing and
Computational Natural Language Learning, pages
489–500.

https://doi.org/10.3115/1629235.1629236
http://www.lrec-conf.org/proceedings/lrec2014/pdf/963_Paper.pdf
http://www.lrec-conf.org/proceedings/lrec2014/pdf/963_Paper.pdf
https://www.aclweb.org/anthology/D15-1247
https://www.aclweb.org/anthology/D15-1247
https://www.aclweb.org/anthology/D15-1247
https://pdfs.semanticscholar.org/ccda/cc60d9d68dfc1f94e7c68bd56646c000e4ab.pdf
https://pdfs.semanticscholar.org/ccda/cc60d9d68dfc1f94e7c68bd56646c000e4ab.pdf
https://www.aclweb.org/anthology/J14-2004
https://www.aclweb.org/anthology/J14-2004
https://www.aclweb.org/anthology/I13-1100
https://www.aclweb.org/anthology/I13-1100
https://www.aclweb.org/anthology/I13-1100
http://www.lrec-conf.org/proceedings/lrec2014/pdf/1144_Paper.pdf
http://www.lrec-conf.org/proceedings/lrec2014/pdf/1144_Paper.pdf
https://aclweb.org/anthology/N15-1116
https://aclweb.org/anthology/N15-1116
https://aclweb.org/anthology/N15-1116
https://www.aaai.org/ocs/index.php/AAAI/AAAI16/paper/view/12413
https://www.aaai.org/ocs/index.php/AAAI/AAAI16/paper/view/12413
https://www.aaai.org/ocs/index.php/AAAI/AAAI16/paper/view/12413
https://www.aaai.org/ocs/index.php/AAAI/AAAI16/paper/view/12413
https://www.aclweb.org/anthology/P17-1171
https://www.aclweb.org/anthology/P17-1171
http://www.aclweb.org/anthology/W/W09/W09-3208
http://www.aclweb.org/anthology/W/W09/W09-3208
https://www.aclweb.org/anthology/W09-4303
https://www.aclweb.org/anthology/W09-4303
https://www.aclweb.org/anthology/W09-4303
https://www.aclweb.org/anthology/D17-1226
https://www.aclweb.org/anthology/D17-1226
https://www.aclweb.org/anthology/R13-1021
https://www.aclweb.org/anthology/R13-1021
https://www.aclweb.org/anthology/R13-1021
https://www.ldc.upenn.edu/sites/www.ldc.upenn.edu/files/tackbp2016-linguistic-resources-tackbp-1.pdf
https://www.ldc.upenn.edu/sites/www.ldc.upenn.edu/files/tackbp2016-linguistic-resources-tackbp-1.pdf
https://www.ldc.upenn.edu/sites/www.ldc.upenn.edu/files/kbp-2016-overview-paper-V3.pdf
https://www.ldc.upenn.edu/sites/www.ldc.upenn.edu/files/kbp-2016-overview-paper-V3.pdf
https://www.ldc.upenn.edu/sites/www.ldc.upenn.edu/files/kbp-2016-overview-paper-V3.pdf
https://tac.nist.gov/publications/2017/additional.papers/TAC2017.KBP_resources_overview.proceedings.pdf
https://tac.nist.gov/publications/2017/additional.papers/TAC2017.KBP_resources_overview.proceedings.pdf
https://tac.nist.gov/publications/2017/additional.papers/TAC2017.KBP_resources_overview.proceedings.pdf
https://openreview.net/forum?id=r1dHXnH6-
https://openreview.net/forum?id=r1dHXnH6-
https://www.bioinf.jku.at/publications/older/2604.pdf
https://tac.nist.gov/publications/2017/participant.papers/TAC2017.srcb.proceedings.pdf
https://tac.nist.gov/publications/2017/participant.papers/TAC2017.srcb.proceedings.pdf
http://www.aclweb.org/anthology/K16-1024
http://www.aclweb.org/anthology/K16-1024
http://www.aclweb.org/anthology/D12-1045
http://www.aclweb.org/anthology/D12-1045


795

Zhengzhong Liu, Jun Araki, Eduard Hovy, and Teruko
Mitamura. 2014. Supervised within-document event
coreference using information propagation. In Pro-
ceedings of the 9th International Conference on
Language Resources and Evaluation, pages 4539–
4544.

Jing Lu and Vincent Ng. 2016. Event coreference res-
olution with multi-pass sieves. In Proceedings of
the 10th International Conference on Language Re-
sources and Evaluation, pages 3996–4003.

Jing Lu and Vincent Ng. 2017. Joint learning for event
coreference resolution. In Proceedings of 55th An-
nual Meeting of the Association for Computational
Linguistics (Volume 1: Long Papers), pages 90–101.

Jing Lu and Vincent Ng. 2018. Event coreference res-
olution: A survey of two decades of research. In
Proceedings of the 27th International Joint Confer-
ence on Artificial Intelligence, pages 5479–5486.

Jing Lu, Deepak Venugopal, Vibhav Gogate, and Vin-
cent Ng. 2016. Joint inference for event coreference
resolution. In Proceedings of the 26th International
Conference on Computational Linguistics: Techni-
cal Papers, pages 3264–3275.

Xiaoqiang Luo. 2005. On coreference resolution per-
formance metrics. In Proceedings of the Confer-
ence on Human Language Technology and Empiri-
cal Methods in Natural Language Processing, pages
25–32.

Christopher D. Manning, Mihai Surdeanu, John Bauer,
Jenny Finkel, Steven J. Bethard, and David Mc-
Closky. 2014. The Stanford CoreNLP natural lan-
guage processing toolkit. In Proceedings of the
52nd Annual Meeting of the Association for Compu-
tational Linguistics: System Demonstrations, pages
55–60.

Katie McConky, Rakesh Nagi, Moises Sudit, and
William Hughes. 2012. Improving event co-
reference by context extraction and dynamic feature
weighting. In Proceedings of the 2012 IEEE Inter-
national Multi-Disciplinary Conference on Cogni-
tive Methods in Situation Awareness and Decision
Support, pages 38–43.

Vincent Ng and Claire Cardie. 2002. Improving ma-
chine learning approaches to coreference resolution.
In Proceedings of the 40th Annual Meeting of the As-
sociation for Computational Linguistics, pages 104–
111.

Boyuan Pan, Yazheng Yang, Zhou Zhao, Yueting
Zhuang, Deng Cai, and Xiaofei He. 2018. Dis-
course marker augmented network with reinforce-
ment learning for natural language inference. In
Proceedings of the 56th Annual Meeting of the As-
sociation for Computational Linguistics (Volume 1:
Long Papers), pages 989–999.

Robert Parker, David Graff, Junbo Kong, Ke Chen,
and Kazuaki Maeda. 2009. English Gigaword fourth
edition. In Linguistic Data Consortium.

Haoruo Peng, Yangqi Song, and Dan Roth. 2016.
Event detection and co-reference with minimal su-
pervision. In Proceedings of the 2016 Conference
on Empirical Methods in Natural Language Pro-
cessing, pages 392–402.

Jeffrey Pennington, Richard Socher, and Christopher
Manning. 2014. GloVe: Global vectors for word
representation. In Proceedings of the 2014 Con-
ference on Empirical Methods in Natural Language
Processing, pages 1532–1543.

Marta Recasens and Eduard Hovy. 2011. BLANC: Im-
plementing the Rand Index for coreference evalu-
ation. Natural Language Engineering, 17(4):485–
510.

Satyan Sangeetha and Michael Arock. 2012. Event
coreference resolution using mincut based graph
clustering. In Proceedings of the 4th International
Workshop on Computer Networks & Communica-
tions, pages 253–260.

Wee Meng Soon, Hwee Tou Ng, and Daniel
Chung Yong Lim. 2001. A machine learning ap-
proach to coreference resolution of noun phrases.
Computational Linguistics, 27(4):521–544.

Joseph Turian, Lev Ratinov, and Yoshua Bengio. 2010.
Word representations: A simple and general method
for semi-supervised learning. In Proceedings of the
48th Annual Meeting of the Association for Compu-
tational Linguistics, pages 384–394.

Marc Vilain, John Burger, John Aberdeen, Dennis Con-
nolly, and Lynette Hirschman. 1995. A model-
theoretic coreference scoring scheme. In Proceed-
ings of the Sixth Message Understanding Confer-
ence, pages 45–52.

Bishan Yang, Claire Cardie, and Peter Frazier. 2015. A
hierarchical distance-dependent bayesian model for
event coreference resolution. Transactions of the
Association for Computational Linguistics, 3:517–
528.

Zhilin Yang, Bhuwan Dhingra, Ye Yuan, Junjie Hu,
William W. Cohen, and Ruslan Salakhutdinov. 2017.
Words or Characters? Fine-grained gating for read-
ing comprehension. In Proceedings of the 5th Inter-
national Conference on Learning Representations.

http://www.lrec-conf.org/proceedings/lrec2014/pdf/646_Paper.pdf
http://www.lrec-conf.org/proceedings/lrec2014/pdf/646_Paper.pdf
http://www.lrec-conf.org/proceedings/lrec2016/pdf/1005_Paper.pdf
http://www.lrec-conf.org/proceedings/lrec2016/pdf/1005_Paper.pdf
https://www.aclweb.org/anthology/P17-1009
https://www.aclweb.org/anthology/P17-1009
https://www.ijcai.org/proceedings/2018/0773.pdf
https://www.ijcai.org/proceedings/2018/0773.pdf
https://www.aclweb.org/anthology/C16-1308
https://www.aclweb.org/anthology/C16-1308
https://aclweb.org/anthology/papers/H/H05/H05-1004/
https://aclweb.org/anthology/papers/H/H05/H05-1004/
http://www.aclweb.org/anthology/P/P14/P14-5010
http://www.aclweb.org/anthology/P/P14/P14-5010
https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6188406
https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6188406
https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6188406
https://www.aclweb.org/anthology/P02-1014
https://www.aclweb.org/anthology/P02-1014
https://aclweb.org/anthology/P18-1091
https://aclweb.org/anthology/P18-1091
https://aclweb.org/anthology/P18-1091
https://catalog.ldc.upenn.edu/LDC2009T13
https://catalog.ldc.upenn.edu/LDC2009T13
https://aclweb.org/anthology/papers/D/D16/D16-1038
https://aclweb.org/anthology/papers/D/D16/D16-1038
http://www.aclweb.org/anthology/D14-1162
http://www.aclweb.org/anthology/D14-1162
http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.300.9229&rep=rep1&type=pdf
http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.300.9229&rep=rep1&type=pdf
http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.300.9229&rep=rep1&type=pdf
https://airccj.org/CSCP/vol2/csit2422.pdf
https://airccj.org/CSCP/vol2/csit2422.pdf
https://airccj.org/CSCP/vol2/csit2422.pdf
https://www.aclweb.org/anthology/J01-4004
https://www.aclweb.org/anthology/J01-4004
http://www.aclweb.org/anthology/P10-1040
http://www.aclweb.org/anthology/P10-1040
https://www.aclweb.org/anthology/M95-1005
https://www.aclweb.org/anthology/M95-1005
http://www.aclweb.org/anthology/Q15-1037
http://www.aclweb.org/anthology/Q15-1037
http://www.aclweb.org/anthology/Q15-1037
https://openreview.net/forum?id=B1hdzd5lg
https://openreview.net/forum?id=B1hdzd5lg

