



















































Improve Chinese Word Embeddings by Exploiting Internal Structure


Proceedings of NAACL-HLT 2016, pages 1041–1050,
San Diego, California, June 12-17, 2016. c©2016 Association for Computational Linguistics

Improve Chinese Word Embeddings by Exploiting Internal Structure

Jian Xu, Jiawei Liu, Liangang Zhang, Zhengyu Li, Huanhuan Chen∗
Department of Computer Science, University of Science and Technology of China, China
{jianxu1, ustcljw, liangang, lzy0503}@mail.ustc.edu.cn

hchen@ustc.edu.cn

Abstract

Recently, researchers have demonstrated that
both Chinese word and its component charac-
ters provide rich semantic information when
learning Chinese word embeddings. Howev-
er, they ignored the semantic similarity across
component characters in a word. In this paper,
we learn the semantic contribution of charac-
ters to a word by exploiting the similarity be-
tween a word and its component characters
with the semantic knowledge obtained from
other languages. We propose a similarity-
based method to learn Chinese word and char-
acter embeddings jointly. This method is also
capable of disambiguating Chinese character-
s and distinguishing non-compositional Chi-
nese words. Experiments on word similarity
and text classification demonstrate the effec-
tiveness of our method.

1 Introduction

Distributed representations of knowledge has re-
ceived wide attention in recent years. Researchers
have proposed various models to learn it at different
granularity levels. Distributed word representation-
s, also known as word embeddings, were learned in
(Rumelhart et al., 1988; Bengio et al., 2006; Mni-
h and Hinton, 2009; Mikolov et al., 2013a). Larger
granularity levels than words have also been inves-
tigated, including phrase level (Socher et al., 2010;
Zhang et al., 2014; Yu and Dredze, 2015), sentence
level (Le and Mikolov, 2014; Socher et al., 2013;
Kalchbrenner et al., 2014; Kiros et al., 2015), and

∗Corresponding author

document level (Le and Mikolov, 2014; Hermann
and Blunsom, 2014; Srivastava et al., 2013).

For language like Chinese, some smaller units
than word also provide rich semantic information.
For example, Chinese characters in word, Chinese
radicals in character. These internal structures have
been proved to be useful for Chinese word and char-
acter embeddings (Chen et al., 2015; Li et al., 2015).
Chen et al. (2015) took Chinese characters in a word
into account when modeling the semantic meaning
of the word. They proposed a character-enhanced
word embeddings model (CWE) by adding the em-
bedding of component characters in a word with
the same weight to the word embedding. However,
the internal characters in a Chinese word have dif-
ferent semantic contributions to its meaning. Take
Chinese word “�” (frog) as an example. The
character “” (blue or green) is to decorate char-
acter “�” (frog). It is obvious that the latter char-
acter contributes more than the former one to the
word meaning. In Li et al. (2015), they proposed
a component-enhanced Chinese character embed-
dings model based on the feature that most Chinese
characters are phono-semantic compounds. They
considered characters and bi-characters as the ba-
sic embedding units. However, some bi-characters
are meaningless, and may not form a Chinese word.
These bi-characters may undermine embeddings of
others.

This paper, motivated by Chen et al. (2015), ex-
ploits the internal structures of Chinese word, name-
ly the Chinese characters. We propose a method
to calculate the semantic contribution of character-
s to a word in a cross-lingual manner. The basic

1041



idea is that the semantic contribution of Chinese
characters in most Chinese words can be learned
from their translations in other languages. Such
as the word “�” we mentioned above. The
word embeddings of other languages are used to
calculate semantic contribution of characters to the
word they compose. Moreover, Chinese character-
s are more ambiguous than words. To tackle this
problem, multiple-prototype character embeddings
is proposed. Different meanings of characters will
be represented by different embeddings. Our contri-
butions can be summarized as follows:

1. We provide a method to calculate the semantic
contribution of Chinese characters to the word they
compose with English translation. Compared with
English, there are fewer human-made resources to
supervise the learning process of Chinese word and
character embeddings. While translation resources
are always easy to be accessed on the Internet.

2. We propose a novel way to disambiguate Chi-
nese characters with translating resources. There
are some limitations in existing cluster-based algo-
rithms (Huang et al., 2012; Neelakantan et al., 2015;
Chen et al., 2015). They either fixed the number of
clusters or proposed a nonparametric way to learn it
for each word. However, the number of clusters for
words varies a lot. For nonparametric method, dif-
ferent hyperparameters have to be tune to control the
number of clusters for different datasets.

3. We provide a method to distinguish whether
a Chinese word is semantically compositional auto-
matically. Not all Chinese words exhibit semantic
compositions from their component characters. For
example, entity names, transliterated words like “â
u” (sofa), single-morpheme multi-character words
like “þ}” (wander). In Chen et al. (2015), they
performed part-of-speech tagging to identify entity
names. The transliterated words are tagged manual-
ly, which requires human work and need to be up-
dated when new words are created.

The evaluations on word similarity, text classifica-
tion, Chinese characters disambiguation, and quali-
tative analysis of word embeddings demonstrate the
effectiveness of our method.

2 Related Work

2.1 Word2vec

Word2vec (Mikolov et al., 2013a) is an algorithm to
learn distributed word representations using a neu-
ral language model. Word2vec has two models, the
continuous bag-of-words model (CBOW) and the
skip-gram model. In this paper, we propose a new
model based on the CBOW, hence we focus atten-
tion on it. CBOW aims at predicting the target word
given context words in a slide window. Given a word
sequence D = {x1, x2, . . . , xT }, the objective of
CBOW is to maximize the average log probability

L =
1
T

T∑
i=1

log p(xi|xi+ji−j), (1)

where xi+ji−j is the context words centered at xi,
p(xi|xi+ji−j) is defined as:

exp(v
′
xi

T ∑
−j≤k≤j,k 6=0 vxi+k)∑W

x=1 exp(v
′
x
T ∑

−j≤k≤j,k 6=0 vxi+k)
, (2)

where vxi and v
′
xi are the input and output vector

representations of word xi. Since the size of En-
glish vocabulary W may be up to 106 scale, hier-
archical softmax and negative sampling (Mikolov et
al., 2013b) are applied during training to learn the
model efficiently. However, using CBOW to learn
Chinese word embeddings directly may have some
limitations. It fails to capture the internal struc-
ture of words. In (Botha and Blunsom, 2014; Lu-
ong et al., 2013; Trask et al., 2015; Chen et al.,
2015), they demonstrated the usefulness to exploit
the internal structure of words, and proposed some
morphological-based methods. For example, Chen
et al. (2015) exploit the internal structure in Chinese
words.

2.2 The CWE model

The basic idea of CWE is that both external context
words and internal component characters in words
provide rich information in modeling the semantic
meaning of the target word. In CWE, they learned
word embeddings with its component characters
embeddings. Let C denotes the Chinese characters
set, and the word xt in context x

i+j
i−j is composed by

1042



several characters in C, let xt = {c1, c2, . . . , cNt},
ck denotes the k-th character in xt,

v̂xt = vxt +
1
Nt

Nt∑
k=1

vck , (3)

where v̂xt is the modified word embedding, Nt de-
notes the number of Chinese characters in xt. To
address the issue of ambiguity in Chinese charac-
ters, they proposed several approaches for multiple-
prototype character embeddings: position-based,
cluster-based, nonparametric methods, and position-
cluster-based character embeddings. These meth-
ods are denoted as CWE+P, CWE+L, CWE+N,
CWE+LP respectively. However, this model has
some limitations. The internal characters are of the
same contribution to the semantic meaning of the
word in CWE, which is not the case for most Chi-
nese words.

3 Methodology

Our method can be described as three stages:

• Obtain translations of Chinese words and
characters
Chinese words segmentation tool is used to seg-
ment words in Chinese corpus. Then we use
an online English-Chinese translation tool to
translate all the Chinese characters and seg-
mented words.

• Perform Chinese character sense disam-
biguation
We train an English corpus with CBOW to
get English word embeddings. Then, we
merge some meanings of Chinese character-
s with small difference, and disambiguate the
meanings of characters in words by computing
the similarity between their English translation
words.

• Learn word and character embeddings with
our model
Based on the character sense disambiguation
process, we modify the objective of CWE to
learn Chinese word and character embeddings.
Then we analyse the complexity of our model
briefly.

3.1 Obtain translations of Chinese words and
characters

We use segmentation tools to segment words in Chi-
nese training corpus, and perform part-of-speech
tagging to recognize all the entity names. Since
entity name words do not exhibit semantic com-
positions, they are identified as non-compositional
words. We count the times of characters appearing
in different words. Words with Chinese character-
s rarely combined with other characters are classi-
fied as single-morpheme multi-character words and
identified as non-compositional.

Then programming interface of online translation
tool is used to translate Chinese words and char-
acters into English. For non-compositional Chi-
nese words, they are not included in the translation
list. Table 1 shows the English meanings of Chinese
word “ÑW”, “âu” and their component charac-
ters “Ñ” and “W”, “â” and “u”.

3.2 Perform Chinese character sense
disambiguation

We train an English corpus with CBOW to get En-
glish word embeddings. Then, the meanings of char-
acters with small difference are merged.

In Table 1, we observe that the difference between
some meanings of character “W” is very small,
some of them differ only in their part-of-speech. In
Chinese, the same characters and words are used
in different part-of-speech but express the same se-
mantic meaning. Hence these meanings are merged
as one semantic meaning. Let Sim(·) denotes the
function to calculate the similarity between mean-
ings of Chinese words and characters, we use cosine
distance as the distance metric. The i-th and j-th
meanings of Chinese character c are ci and cj . Their
similarity is defined as:

Sim(ci, cj) = max(cos(vxm , vxn)),

s.t. xm ∈ Trans(ci), xn ∈ Trans(cj),
xm, xn /∈ stop words(en),

(4)

where Trans(ci) denotes the English translation
words set of ci, stop words(en) denotes the stop
words in English, xm and xn are not in these
stop words. For example, the Chinese word “Ñ
W” in Table 1, c2 denotes the second character

1043



Word English Explanation

ÑW music;

Ñ ((Ñ) sound; (E) news, tidings; (Ñ) tone; (6¼) a surname;

W N. (ÑW) music; (6¼) a surname; (¯; ÷v) pleasure, enjoyment; JJ. (¯W)
happy,glad,joyful,cheerful; V. (U) enjoy, be glad to, love, find pleasure in; (�)
laugh, be amused; RB. (W¿) gladly, happily, willingly;

âu sofa, settee;

â N. (âf) sand; (,
¥âG� Ô) granulated, powdered; (6¼) a surname; JJ.
(ÓÑØy) (of voice) hoarse, husky;

u N. (Þu) hair; V. (xÑ;�G) send out, distribute, deliver; (u�) launch, discharge,
shoot, emit; (�), u)) produce, generate, come into existence; (L) express,
utter; (*, mÐ) expand, develop; (Ï�ãÔ,!) flourish; (Ñ, Ñm)
spread out, disperse, diffuse; etc.;

Table 1: English Translation of Chinese words and characters in ICIBA. V., N., JJ., RB. denote their verb, noun, adjective and
adverb meaning respectively. Different meanings of word and character are separated by semicolon.

“W” in the word. Trans(c32) is the third trans-
lation English words set of character “W”, which
is {pleasure, enjoyment}. Therefore xm can be
pleasure or enjoyment here.

If the Sim(ci, cj) is above a threshold δ, then they
are merged as one semantic meaning. For simplici-
ty, we use the union of English translation words set.
One character may be translated into several English
words. We may average all the translation word em-
beddings and then compute the similarity, or selec-
t the maximum value of the similarity between all
English word pairs. In our experiments, maximum
method works better.

Finally, we perform Chinese character sense dis-
ambiguation. In Chinese, characters may have mul-
tiple meanings, but for a certain word, their mean-
ings are determined. For exmaple, the word “ÑW”,
the English translation is music. For character “W”,
the first translation “music” matches the meaning of
the word. For character “Ñ”, the best match is the
first translation “sound”. For transliterated word like
“âu”, the English translations are sofa and settee,
neither sofa nor settee have high similarity with En-
glish translation words of character “â” and charac-
ter “u”. Formally, if max(Sim(xt, ck)) > λ, ck ∈
xt, then xt is identified as compositional word, and
belongs to the compositional set COMP. For com-
positional words, we build a set

F = {(xt, st, nt) | xt ∈ COMP}, (5)

where

st = {Sim(xt, ck) | ck ∈ xt},
nt = {max

i
Sim(xt, cik) | ck ∈ xt}

(6)

For example, the word “ÑW” is defined as
(“ÑW”, {Sim(“ÑW”, “Ñ”), Sim(“ÑW”, “
W”)},{1,1}) in F .

3.3 Learn word and character vectors with
SCWE

The internal characters in a word make differen-
t contributions to its semantic meaning. However,
in Chen et al. (2015), the contribution of compo-
nent characters to the semantic meaning of word are
treated equally. They add character embeddings to
the word embeddings with the same weight, which
may undermine the quality of word embeddings.
Based on this point, we propose a similarity-based
character-enhanced word embedding model, which
takes the contribution of characters into account. We
name it SCWE for ease of reference in the later part.
The architecture of CWE and SCWE are shown in
Fig. 1.

Similarity-Based Character-Enhanced word
Embedding In the character sense disambiguation
stage, we build a set F , which contains composi-
tional words, the similarity between words and its
component characters, and the meaning order num-
ber of characters in the word. Suppose xt in W is a

1044



Figure 1: Architecture of models. The left is CWE and right is SCWE. “� (frog)a? (jump into)³* (pond)” is the word
sequence. The word “�” is composed of characters “ (blue or green)” and “� (frog)”, and the word “³* (pond) is composed

of characters “³ (pond, pool)” and “* (pond)”.

compositional word, in SCWE,

v̂xt =
1
2
{
vxt +

1
Nt

Nt∑
k=1

Sim(xt, ck)vck
}

(7)

To deal with ambiguity problem of Chinese char-
acters, we propose multiple-prototype character em-
beddings and denote it as SCWE+M model. Since
the meaning of a character is determined in a given
word, we utilize the information provided by the last
element in set F , and use different character embed-
dings for different meanings of characters. Then, in
SCWE+M,

v̂xt =
1
2
{
vxt +

1
Nt

Nt∑
k=1

Sim(xt, ck)vcik
}

(8)

Complexity analysis We analyze the complexi-
ties of CBOW, CWE, SCWE and SCWE+M. Let S
denotes the size of corpus, |W | denotes the size of
vocabulary, |C| denotes the number of Chinese char-
acters in corpus. And d is the dimensions of Chi-
nese word and character embeddings, k is the con-
text window size, f is the time spend in computing
hierarchical softmax or negative sampling, n is the
average number of characters in a Chinese word, m
is the average meaning number of Chinese charac-
ters. The results are shown in Table 2.

In Chinese, most of words are composed by t-
wo Chinese characters, and the meaning number of
commonly used characters are usually less than five.

Moreover, according to CJK Unified Ideographs1,
the total number of Chinese characters is 20913,
the commonly used characters are less than 10000.
Therefore, our model is competitive to other meth-
ods in model parameters and computational com-
plexity.

Method Model parameters
Computational

complexity

CBOW |W |d 2kSf
CWE (|W |+ |C|)d 2kS(f + n)

SCWE (|W |+ |C|)d 2kS(f + n)
SCWE + M (|W |+m|C|)d 2kS(f+n+mn)

Table 2: Complexity analysis

4 Experiments and Analysis

4.1 Experiments Settings

We select English Wikipedia Dump2 to train English
word embeddings with CBOW, and set dimensions
to 200. For Chinese word embeddings, we selec-
t Chinese Wikipedia Dump3 to train character and
word embeddings. Before training, pure digits and
non-Chinese characters are removed. We use an
open-source Chinese segment tool called ANSJ4 to

1https://en.wikipedia.org/wiki/CJK_
Unified_Ideographs

2http://download.wikipedia.com/enwiki/
3http://download.wikipedia.com/zhwiki/
4https://github.com/NLPchina/ansj_seg

1045



segment words in corpus. ANSJ is a java implemen-
tation of ICTCLAS (Institute of Computing Tech-
nology, Chinese Lexical Analysis System). It can
process about one million words in a second, and
get up to 96 percent accuracy in segmentation task.
The part-of-speech tagging and name entity recog-
nition tasks are also done in this process. We select
ICIBA5 as English-Chinese translation tool, which
provides us with an application programming inter-
face. CBOW and CWE are used as baseline meth-
ods. Context window size is set as 5 and both Chi-
nese word and character embeddings are set as 100
dimension. After some cross validation steps, our
threshold δ and λ are set as 0.5 and 0.4 in character
disambiguation process. The influence of λ and δ is
report in the later part.

Model wordsim-240 wordsim-296

CBOW 51.78 60.82

CWE 52.57 60.36

SCWE 54.92 60.85

SCWE + M 55.10 62.86
Table 3: Evaluation on wordsim-240 and wordsim-296

4.2 Word Similarity

Word similarity is a task to compute semantic relat-
edness between given word pairs. The relatedness
between word pairs have been scored by human in
advance. The correlation between model results and
human judgement can be used to evaluate the per-
formance of models. In this paper, wordsim-240
and wordsim-296 (Jin and Wu, 2012) are used as e-
valuation datasets. The Spearman’s rank correlation
(Myers et al., 2010) is applied to compute the corre-
lation. The experimental results are summarized in
Table 3.

We observe that on wordsim-240, SCWE and
SCWE+M outperform the baseline methods, which
indicates the effectiveness of exploiting the inter-
nal structure. On dataset wordsim-296, we can
see that CBOW, CWE, SCWE perform similarly.
This may be explained by some highly ambiguous
Chinese characters in this dataset. In SCWE and
CWE, representing these ambiguous characters with
the same embeddings may undermine word embed-

5http://www.iciba.com/

Fudan-large Size
Environment 1218
Agriculture 1022
Economy 1601
Politics 1025
Sports 1254

Fudan-small Size
Education 59
Philosophy 44
Transport 58
Medical 52
Military 75

Table 4: 2 groups datasets of text classification, the first col-
umn denotes the category of documents and the second denotes

number of documents in each category.

dings. Therefore, SCWE+M achieves a better per-
formance by applying multiple-prototype character
embeddings.

4.3 Text Classification

In this experiment, we use Fudan Corpus6 as
datasets, which contains 20 categories of docu-
ments, including economy, politics, sports and etc..
The number of documents in each category ranges
from 27 to 1061. To avoid imbalance, we select 10
categories and organize them into 2 groups. One
group is named Fudan-large and each category in
this group contains more than 1000 documents. The
other is named Fudan-small and each category con-
tains less than 100 documents. In each category, 80
percent of documents are used as training set, the
rest are used as testing set to evaluate the perfor-
mance. The detailed information for two datasets
are reported in Table 4.

Similar to the way we deal with Chinese training
corpus, pure digits and non-Chinese characters are
removed and ANSJ is used to do word segmentation
on these datasets. The publish information of each
document is removed. We represent each documen-
t by averaging word embeddings in the document.
The classifiers are trained using LIBLINEAR pack-
age(Fan et al., 2008) with the embeddings obtained
from different methods. The performance of each
method is evaluated by predicting accuracy on test-
ing set. Experiment results are given in Table 5.

It is observed that our methods outperform the
baseline methods on both datasets. This can be ex-
plained that the semantic relatedness of a word with
the component characters which have more contri-
bution to its semantic meaning is strengthen in our
methods. Such as, in sports documents, the word

6http://www.datatang.com/data/44139

1046



Figure 2: Illustration of words and characters in two dimension plane.

Method Fudan-small Fudan-large

CBOW 84.75 91.42

CWE 88.14 91.84

SCWE 91.53 92.68

SCWE + M 93.22 92.89
Table 5: Evaluation accuracies (%) on text classification.

“¥” (ball) is used frequently. For Chinese word-
s like “;¥” (basketball) and “�¥” (tennis), the
character “¥” contributes more to their semantic
meaning than other characters. Therefore, they lie
closer to character “¥” in embedding space ob-
tained by our model than CBOW and CWE, and tend
to form a cluster in embedding space.

4.4 Multiple Prototype of Chinese Charaters

To tackle the ambiguity of Chinese characters, we
propose multiple-prototype character embeddings.
To evaluate the effectiveness of our method, we use
PCA to conduct dimensionality reduction on word
and character embeddings. The results are illustrat-
ed in Fig 2. We take 3 different meanings of Chinese
characters “�” and “1”, and 2 of their top-related
words as examples. The character followed by a dig-
it i denotes the i-th meanings of it.

We can observe that characters and words, which
have similar meanings are gathered together. For ex-
ample, “13”, “1r” and “ß1Ç” are all related
to the light. Thus, they get closer in the embedding
space.

We also develop a dataset to compare our method
with the disambiguation methods in Chen et al.
(2015). We select some ambiguous Chinese char-

Characters Words

�1 (say, speak)
`� (say)
¡� (speak)

�2 (Taoism, Taoist)
�² (Taoist scriptures)
��ä (Taoist)

�3 (road, path)

� (branch road)
¶� (royal road)

11 (scenery)
�1ìÚ (a landscape
of lakes and mountains)
² (bright and beautiful)

12 (time)
1 (time, year)
1Ò (time)

13 (light,ray)
1r (light intensity)
ß1Ç (light transmittance)

Table 6: English explanatory of characters and their nearest
words in vector space.

acters, and then use online Xinhua Dictionary7 as
our standard to disambiguate the words that contain
these ambiguous characters. Each word is assigned
a number according to their explanation in the dic-
tionary. We use KNN as classifier to evaluate all the
methods. The results are shown in Table 7. It is
observed that our method outperforms the methods
proposed in Chen et al. (2015).

Model Accuracy

CWE + P 84.9

CWE + L 81.0

CWE + LP 85.4

CWE + N 73.5

SCWE + M 91.1
Table 7: Evaluation accuracies (%) on ambiguous characters.

7http://xh.5156edu.com/

1047



Words CWE SCWE

�
(frog)

� (green snake)
� (blue crab)
þ (green pepper)
Ú�
(Rana catesbeiana)

Ú�
(Rana catesbeiana)
	k (fox)
�� (crab)
� (frog)

>{
(telephone)

>{�
(telephone network)
>e (Email)
>{k (phonecard)
å>{
(toll call)

>Õ (dispatch)
ÃÅ (cellphone)
ÏÕ
(communication)
á& (message)

Table 8: Nearest words example of Chinese words.

4.5 Qualitative analysis of word embeddings

In this part, we take two Chinese words as examples,
and list their nearest words to examine the quality
of word embeddings obtained by CWE and SCWE.
The results are shown in Table 8. We can observe
the most similar words return by CWE and SCWE
both tend to share common characters with the giv-
en word. In CWE, characters with little semantic
contribution to the word may undermine the quali-
ty of word embeddings. For example, the charac-
ter “” in word “�”. The semantic relatedness
of words with character “” to the given word are
overestimated in CWE. In our model, by calculat-
ing the semantic contribution of internal characters
to the word, we alleviate this misjudgement greatly,
which demonstrates the effectiveness of our model.

4.6 Parameter Analysis

In this part, the influence of parameters on our model
is investigated. The parameters include the compo-
sitional word similarity threshold λ, character dis-
ambiguation threshold δ.

Compositional word similarity To investigate
how λ influence the process of non-compositional
word detection, we build a word list of transliterated
words manually, which consists of 161 words. Then
161 of most frequent semantic compositional words
with more than one Chinese characters are added to
the list in the corpus. In Table 9, the performance of
our method in classifying transliterated words when
λ ranges from 0.25 to 0.55 are reported. From Ta-
ble 9, we can observe as λ increases, more composi-
tional words will be classified as non-compositional
words, while transliterated words are more likely to

be classified correctly. Our method achieves best F-
Score when λ = 0.4.

Character disambiguation threshold In Table
10, we show the performance of our model in disam-
biguating Chinese characters. We adopted the same
datasets in Section 4.4 with different δ. From Ta-
ble 1, we can observe some meanings of a character
are very close, therefore, a high δ are adopted in our
model. When δ = 0.5, our model gets the best result
in our dataset.

Parameter λ Precision Recall F-Score

0.25 97.0 60.9 74.8

0.30 96.5 68.9 80.4

0.35 94.6 75.8 84.2

0.40 92.0 78.9 85.0
0.45 88.9 80.1 84.3

0.50 84.0 84.5 84.2

0.55 82.5 85.1 83.8

Table 9: Precision, recall, F-score of transliterated words when
λ ranges from 0.25 to 0.55

Parameter δ Precision

0.35 87.5

0.40 89.0

0.45 89.5

0.50 91.1
0.55 89.9

0.60 89.5

0.65 88.5

Table 10: Evaluation accuracies (%) on ambiguous characters
when λ ranges from 0.35 to 0.65.

5 Conclusion

In this paper, we exploit the internal structure in
Chinese words by learning the semantic contribu-
tion of internal characters to the word. We pro-
pose a method to improve Chinese word and char-
acter embeddings with a similarity-based character-
enhanced word embeddings model. Ambiguity
problem of Chinese characters can also be tack-
led in our method. Moreover, we build a way to
classify whether a Chinese word is compositional

1048



automatically, which requires to be labelled man-
ually in CWE. We argue that our method may be
used to improve word embeddings of other language
whose internal structure is similar to Chinese. The
code and datasets we use is available at: https:
//github.com/JianXu123/SCWE.

Acknowledgement

This work is supported by NSFC grants 91546116
and 61511130083.

References
[Bengio et al.2006] Yoshua Bengio, Holger Schwenk,

Jean-Sébastien Senécal, Fréderic Morin, and Jean-Luc
Gauvain. 2006. Neural probabilistic language models.
In Innovations in Machine Learning, pages 137–186.
Springer.

[Botha and Blunsom2014] Jan A Botha and Phil Blun-
som. 2014. Compositional morphology for word rep-
resentations and language modelling. arXiv preprint
arXiv:1405.4273.

[Chen et al.2015] Xinxiong Chen, Lei Xu, Zhiyuan Liu,
Maosong Sun, and Huanbo Luan. 2015. Joint learning
of character and word embeddings. In Proceedings of
the 25th International Joint Conference on Artificial
Intelligence (IJCAI).

[Fan et al.2008] Rong-En Fan, Kai-Wei Chang, Cho-Jui
Hsieh, Xiang-Rui Wang, and Chih-Jen Lin. 2008. Li-
blinear: A library for large linear classification. The
Journal of Machine Learning Research, 9:1871–1874.

[Hermann and Blunsom2014] Karl Moritz Hermann and
Phil Blunsom. 2014. Multilingual models for com-
positional distributed semantics. arXiv preprint arX-
iv:1404.4641.

[Huang et al.2012] Eric H Huang, Richard Socher,
Christopher D Manning, and Andrew Y Ng. 2012.
Improving word representations via global context
and multiple word prototypes. In In Proceedings
of the 50th Annual Meeting of the Association for
Computational Linguistics: Long Papers-Volume
1, pages 873–882. Association for Computational
Linguistics.

[Jin and Wu2012] Peng Jin and Yunfang Wu. 2012.
Semeval-2012 task 4: evaluating chinese word similar-
ity. In In Proceedings of the Sixth International Work-
shop on Semantic Evaluation, pages 374–377. Associ-
ation for Computational Linguistics.

[Kalchbrenner et al.2014] Nal Kalchbrenner, Edward
Grefenstette, and Phil Blunsom. 2014. A convolu-
tional neural network for modelling sentences. arXiv
preprint arXiv:1404.2188.

[Kiros et al.2015] Ryan Kiros, Yukun Zhu, Ruslan R
Salakhutdinov, Richard Zemel, Raquel Urtasun, An-
tonio Torralba, and Sanja Fidler. 2015. Skip-thought
vectors. In Advances in Neural Information Process-
ing Systems, pages 3276–3284.

[Le and Mikolov2014] Quoc V Le and Tomas Mikolov.
2014. Distributed representations of sentences and
documents. arXiv preprint arXiv:1405.4053.

[Li et al.2015] Yanran Li, Wenjie Li, Fei Sun, and Su-
jian Li. 2015. Component-enhanced chinese character
embeddings. arXiv preprint arXiv:1508.06669.

[Luong et al.2013] Minh-Thang Luong, Richard Socher,
and Christopher D Manning. 2013. Better word rep-
resentations with recursive neural networks for mor-
phology. CoNLL-2013, 104.

[Mikolov et al.2013a] Tomas Mikolov, Kai Chen, Greg
Corrado, and Jeffrey Dean. 2013a. Efficient estima-
tion of word representations in vector space. arXiv
preprint arXiv:1301.3781.

[Mikolov et al.2013b] Tomas Mikolov, Ilya Sutskever,
Kai Chen, Greg S Corrado, and Jeff Dean. 2013b.
Distributed representations of words and phrases and
their compositionality. In Advances in neural infor-
mation processing systems, pages 3111–3119.

[Mnih and Hinton2009] Andriy Mnih and Geoffrey E
Hinton. 2009. A scalable hierarchical distributed lan-
guage model. In Advances in neural information pro-
cessing systems, pages 1081–1088.

[Myers et al.2010] Jerome L Myers, Arnold Well, and
Robert Frederick Lorch. 2010. Research design and
statistical analysis. Routledge.

[Neelakantan et al.2015] Arvind Neelakantan, Jeevan
Shankar, Alexandre Passos, and Andrew McCallum.
2015. Efficient non-parametric estimation of multiple
embeddings per word in vector space. arXiv preprint
arXiv:1504.06654.

[Rumelhart et al.1988] David E Rumelhart, Geoffrey E
Hinton, and Ronald J Williams. 1988. Learning repre-
sentations by back-propagating errors. Cognitive mod-
eling, 5:3.

[Socher et al.2010] Richard Socher, Christopher D Man-
ning, and Andrew Y Ng. 2010. Learning continu-
ous phrase representations and syntactic parsing with
recursive neural networks. In In Proceedings of the
NIPS-2010 Deep Learning and Unsupervised Feature
Learning Workshop, pages 1–9.

[Socher et al.2013] Richard Socher, Alex Perelygin,
Jean Y Wu, Jason Chuang, Christopher D Manning,
Andrew Y Ng, and Christopher Potts. 2013. Re-
cursive deep models for semantic compositionality
over a sentiment treebank. In In Proceedings of the
conference on empirical methods in natural language
processing (EMNLP), volume 1631, page 1642.
Citeseer.

1049



[Srivastava et al.2013] Nitish Srivastava, Ruslan R
Salakhutdinov, and Geoffrey E Hinton. 2013. Model-
ing documents with deep boltzmann machines. arXiv
preprint arXiv:1309.6865.

[Trask et al.2015] Andrew Trask, David Gilmore, and
Matthew Russell. 2015. Modeling order in neu-
ral word embeddings at scale. arXiv preprint arX-
iv:1506.02338.

[Yu and Dredze2015] Mo Yu and Mark Dredze. 2015.
Learning composition models for phrase embeddings.
Transactions of the Association for Computational
Linguistics, 3:227–242.

[Zhang et al.2014] Jiajun Zhang, Shujie Liu, Mu Li, Ming
Zhou, and Chengqing Zong. 2014. Bilingually-
constrained phrase embeddings for machine transla-
tion. In In Proceedings of the 52th Annual Meeting
on Association for Computational Linguistics. Associ-
ation for Computational Linguistics.

1050


