



















































Context-aware Interactive Attention for Multi-modal Sentiment and Emotion Analysis


Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing
and the 9th International Joint Conference on Natural Language Processing, pages 5647–5657,
Hong Kong, China, November 3–7, 2019. c©2019 Association for Computational Linguistics

5647

Context-aware Interactive Attention for Multi-modal Sentiment and
Emotion Analysis

Dushyant Singh Chauhan, Md Shad Akhtar, Asif Ekbal and Pushpak Bhattacharyya
Department of Computer Science & Engineering

Indian Institute of Technology Patna
Patna, Bihar, India-801106

{1821CS17, shad.pcs15, asif, pb}@iitp.ac.in

Abstract

In recent times, multi-modal analysis has been
an emerging and highly sought-after field at
the intersection of natural language process-
ing, computer vision, and speech processing.
The prime objective of such studies is to lever-
age the diversified information, (e.g., textual,
acoustic and visual), for learning a model. The
effective interaction among these modalities
often leads to a better system in terms of per-
formance. In this paper, we introduce a re-
current neural network based approach for the
multi-modal sentiment and emotion analysis.
The proposed model learns the inter-modal in-
teraction among the participating modalities
through an auto-encoder mechanism. We em-
ploy a context-aware attention module to ex-
ploit the correspondence among the neighbor-
ing utterances. We evaluate our proposed ap-
proach for five standard multi-modal affect
analysis datasets. Experimental results sug-
gest the efficacy of the proposed model for
both sentiment and emotion analysis over var-
ious existing state-of-the-art systems.

1 Introduction

In recent past, the world has witnessed tremen-
dous growth of various social media platforms,
e.g., YouTube, Instagram, Twitter, Facebook, etc.
People treat these platforms as a communication
medium and freely express themselves with the
help of a diverse set of input sources, e.g. videos,
images, audio, text etc. The amount of informa-
tion produced daily through these mediums are
enormous, and hence, the research on multi-modal
information processing has attracted attention to
the researchers and developers. A video is a multi-
modal input which provides visual, acoustic, and
textual information.

The motivation of multi-modal sentiment and
emotion analysis lies in fact to leverage the vari-
eties of (often distinct) information from multiple

sources for building more efficient systems. For
some cases, text can provide a better clue for the
prediction, whereas for the others, acoustic or vi-
sual sources can be more informative. Similarly, in
some situations, a combination of two or more in-
formation sources together ensures better and un-
ambiguous classification decision. For example,
only text ”shut up” can not decide the mood of
a person but acoustic (tone of a person) and vi-
sual (expression of a person) can reveal the ex-
act mood. Similarly, for some instances visual
features such as gesture, postures, facial expres-
sion etc. have important roles to play in deter-
mining the correctness of the system. However,
effectively combining this information is a non-
trivial task that researchers often have to face (Po-
ria et al., 2016; Ranganathan et al., 2016; Lee
et al., 2018).

Traditionally, ‘text’ has been the key factor in
any Natural Language Processing (NLP) tasks, in-
cluding sentiment and emotion analysis. However,
with the recent emergence of social media plat-
forms, an interdisciplinary study involving text, vi-
sual and acoustic features have drawn a great in-
terest among the research community. Express-
ing the feelings and emotions through a video is
much convenient than the text for a user, and it is
the best source to extract all multi-modal informa-
tion. Not only the visual, it also provides other
information such as acoustic and textual represen-
tation of spoken language. Additionally, a sin-
gle video can have multiple utterances based on a
speaker’s pause (speech bounded by breaths) with
different sentiments and emotions. The sentiments
and emotions of an utterance often have inter-
dependence on the other contextual utterances. In-
dependently classifying such an utterance poses
several challenges to the underlying problem. In
contrast, multi-modal sentiment and emotion anal-
ysis take inputs from more than one sources e.g.



5648

text, visual, acoustic for the analysis. Effectively
fusing this diverse information is non-trivial and
poses several challenges to the underlying prob-
lem.

In our current work, we propose an end-to-end
Context-aware Interactive Attention (CIA) based
recurrent neural network for sentiment and emo-
tion analysis. We aim to leverage the interaction
between the modalities to increase the confidence
of individual task in prediction. The main con-
tributions of our current research are as follows:
(1) We propose an Inter-modal Interactive Module
(IIM) that aims to learn the interaction among the
diverse and distinct features of the input modali-
ties, i.e., text, acoustic and visual; (2) We employ a
Context-aware Attention Module (CAM) that iden-
tifies and assigns the weights to the neighboring
utterances based on their contributing features.
It exploits the interactive representations of pair-
wise modalities to learn the attention weights, and
(3) We present new state-of-the-arts for five bench-
mark datasets for both sentiment and emotion pre-
dictions.

2 Related Work

Different reviews in (Arevalo et al., 2017; Poria
et al., 2016, 2017b; Ghosal et al., 2018; Morency
et al., 2011a; Zadeh et al., 2018a; Mihalcea, 2012;
Lee et al., 2018; Tsai et al., 2018) suggest that
multi-modal sentiment and emotion analysis are
relatively new areas as compared to uni-modal
analysis. Feature selection (fusion) is a challeng-
ing and important task for any multi-modal anal-
ysis. Poria et al. (2016) proposed a multi-kernel
learning based feature selection method for mul-
timodal sentiment and emotion recognition. A
convolutional deep belief network (CDBN) is pro-
posed in (Ranganathan et al., 2016) to learn salient
multi-modal features of low-intensity expressions
of emotions, whereas Lee et al. (2018) introduced
a convolutional attention network to learn multi-
modal feature representation between speech and
text data for multi-modal emotion recognition.

A feature level fusion vector was built, and
then a Support Vector Machine (SVM) classi-
fier was used to detect the emotional duality
and mixed emotional experience in (Patwardhan,
2017). Similar work on feature-level fusion based
on self- attention mechanism is reported in (Haz-
arika et al., 2018). Fu et al. (2017) introduced
an enhanced sparse local discriminative canoni-

cal correlation analysis approach (En-SLDCCA)
to learn the multi-modal shared feature represen-
tation. Tzirakis et al. (2017) introduced a Long
Short Term Memory (LSTM) based end-to-end
multi-modal emotion recognition system in which
convolutional neural network (CNN) and a deep
residual network are used to capture the emotional
content for various styles of speaking, robust fea-
tures.

Poria et al. (2017a) presented a literature survey
on various affect dimensions e.g., sentiment anal-
ysis, emotion analysis, etc., for the multi-modal
analysis. A multi-modal fusion-based approach
is proposed in (Blanchard et al., 2018) for senti-
ment classification. The author used exclusively
high-level fusion of visual and acoustic features
to classify the sentiment. Zadeh et al. (2016)
presented the multi-modal dictionary-based tech-
nique to capture the interaction between spoken
words and facial expression better when express-
ing the sentiment. In another work, Zadeh et al.
(2017) proposed a Tensor Fusion Network (TFN)
to capture the inter-modality and intra-modality
dynamics between the multi-modalities (i.e., text,
visual, and acoustic).

These works did not take contextual informa-
tion into account. Poria et al. (2017b) introduced
an Long Short Term Memory (LSTM) based
framework for sentiment classification which
uses contextual information to capture inter-
relationships between the utterances. In another
work, Poria et al. (2017c) proposed a user opinion
based framework to combine all the multi-modal
inputs (i.e., visual, acoustic, and textual) by apply-
ing a multi-kernel learning-based approach. Con-
textual inter-modal attention mechanism was not
explored in much details until recently. Zadeh
et al. (2018a) introduced a multi-attention blocks
based model for multi-modal sentiment classifica-
tion but did not account for contextual informa-
tion, whereas Ghosal et al. (2018) proposed a con-
textual inter-modal attention based framework for
multi-modal sentiment classification. Recently,
Zadeh et al. (2018c) introduced the largest multi-
modal dataset namely CMU-MOSEI for sentiment
and emotion analysis. Author effectively fused the
multi-modality inputs i.e., text, visual, and acous-
tic through a dynamic fusion graph and reported
competitive performance w.r.t. various state-of-
the-art systems for both sentiment and emotion
analysis. Very recently, Akhtar et al. (2019) in-



5649

troduced an attention based multi-task learning
framework for sentiment and emotion classifica-
tion on the CMU-MOSEI dataset.

In comparison to the existing systems, our pro-
posed approach aims to exploits the interaction
between the input modalities through an auto-
encoder based inter-modal interactive module.
The interactive module learns the joint represen-
tation for the participating modalities, which are
further utilized to capture the contributing contex-
tual utterances in a context-aware attention mod-
ule.

3 Context-aware Interactive Attention
(CIA) Affect Analysis

In this section, we describe our proposed ap-
proach for the effective fusion of multi-modal in-
put sources. We propose an end-to-end Context-
aware Interactive Attention (CIA) based recurrent
neural network for sentiment and emotion analy-
sis. As discussed earlier, one of the main chal-
lenges for multi-modal information analysis is to
exploit the interaction among the input modalities.
Therefore, we introduce an Inter-modal Interactive
Module (IIM) that aims to learn the interaction be-
tween any two modalities through an auto-encoder
like structure. For the text-acoustic pair of modal-
ities, we aim to decode the acoustic representation
through the encoded textual representation. Af-
ter training of IIM, we extract the encoded rep-
resentation for further processing. We argue that
the encoded representation learns the interaction
between the text and acoustic modalities. Sim-
ilarly, we compute the interaction among all the
other pairs (i.e., acoustic-text, text-visual, visual-
text, acoustic-visual, and visual-acoustic). Next,
we extract the sequential pattern of the utterances
through a Bi-directional Gated Recurrent Unit (Bi-
GRU) (Cho et al., 2014)). For each pair of modal-
ities, the two representations denoting the interac-
tions between them are combined through a mean
operation. For an instance, we compute the mean
of the text-acoustic and acoustic-text representa-
tions for text and acoustic modalities. The mean
operation ensures that the network utilizes the two
distinct representations by keeping the minimal di-
mension.

In our network, we, additionally, learn the in-
teraction among the modalities through a feed-
forward network. At first, all the three modalities
are passed through a separate Bi-GRU. Then, pair-

Algorithm 1 Inter-modal Interactive Module for
Multi-modal Sentiment and Emotion Recognition
(IIM-MMSE)

procedure IIM-MMSE(t, v, a)
for i ∈ 1, ...,K do . K = #modalities

for j ∈ 1, ...,K do
.∀x, y ∈ [T, V,A], x 6= y and i ≤ j
Cxiyj ← IIM(xi, yj)
Cxiyj ← biGRU(Cxiyj )
Cxi ← biGRU(xi)

for i, j ∈ 1, ...,K do
.∀x, y ∈ [T, V,A], and x 6= y
Mxi,yj ←Mean(Cxiyj , Cyixj )
catxi,yj ← Concatenate(Cxi , Cyj )
BIxi,yj ← FullyConnected(catxi,yj )
Axi,yj ← CAM(Mxi,yj , BIxi,yj )

Rep← [ATV , ATA, AAV ]
polarity ← Sent(Rep)/Emo(Rep)
return polarity

Algorithm 2 Inter-Modal Interactive Module
(IIM)

procedure IIM(X,Y )
CXY ← IIMEncoder(X,Y )
Ỹ ← IIMDecoder(CXY )
loss← cross entropy(Ỹ , Y )
Backpropagation to update the weights
return CXY

Algorithm 3 Context-aware Attention Module
(CAM)

procedure CAM(M,BI)
P ←M.BIT . Cross product
for i, j ∈ 1, ..., u do . u = #utterances

N(i, j)← eP (i,j)∑u
k=1 e

P (i,k)

O ← N.BI
A← O �M . Multiplicative gating.
return A

wise concatenation is performed over the output
of Bi-GRU and passed through a fully-connected
layer to extract the bi-modal interaction (BI). Fur-
ther, we employ a Context-aware Attention Mod-
ule (CAM) to exploit the correspondence among
the neighboring utterances. The inputs to the
CAM are the two representations for each pair
of modalities, e.g., mean representation MTA and
bi-modal interaction BITA for the text-acoustic



5650

Figure 1: Overall architecture of the proposed Context-aware Interactive Attention framework.

pair. The attention module assists the network
in attending the contributing features by putting
weights to the current and the neighboring utter-
ances in a video. In the end, the pair-wise (i.e.,
text-acoustic, text-visual, and acoustic-visual) at-
tended representations are concatenated and fed to
an output layer for the prediction.

We depict and summarize the proposed ap-
proach in Figure 1 and Algorithm 1, 2, and 3. The
source code is available at http://www.iitp.
ac.in/˜ai-nlp-ml/resources.html.

3.1 Context-aware Attention Module (CAM)

Since the utterances in a video are the split units of
the break/pause of the speech, their emotions (or
sentiments) often have relations with their neigh-
boring utterances. Therefore, knowledge of the
emotions (or, sentiments) of the neighboring ut-
terances is an important piece of information and
has the capability to derive the prediction of an ut-
terance, if the available inputs are insufficient for
the correct prediction.

Our proposed context-aware attention module
leverages the contextual information. For each

utterance in a video, we compute the attention
weights of all the neighboring utterances based on
their contributions in predicting the current utter-
ance. It ensures that the network properly utilizes
the local contextual information of an utterance
as well as the global contextual information of a
video together. The aim is to compute the interac-
tive attention weights utilizing a softmax activation
for each utterance in the video. Next, we apply
a multiplicative gating mechanism following the
work of Dhingra et al. (2016). The attentive rep-
resentation is, then, forwarded to the upper layers
for further processing. We summarize the process
of CAM in Algorithm3.

3.2 Inter-modal Interactive Module (IIM)

One of the key objectives of the multi-modal anal-
ysis is to fuse the available input modalities ef-
fectively. In general, different modalities repre-
sent distinct features despite serving a common
goal. For example, in multi-modal sentiment anal-
ysis all the three modalities, i.e., text, acoustic,
and visual, aim to predict the expressed polarity of
an utterance. The distinctive features in isolation

http://www.iitp.ac.in/~ai-nlp-ml/resources.html
http://www.iitp.ac.in/~ai-nlp-ml/resources.html


5651

might create an ambiguous scenario for a network
to learn effectively. Therefore, we introduce an
auto-encoder based inter-modal interactive mod-
ule whose objective is to learn the interaction be-
tween two distinct modalities to serve a common
goal. The IIM encodes the feature representation
of one modality (say, text), and aims to decode it
into the feature representation of another modality
(say, acoustic). Similar to an auto-encoder where
the input and output are conceptually the same (or
closely related), in our case the input and output
feature representations of two modalities also in-
tuitively serve a common goal. After training of
IIM, the encoded vector signifies a joint represen-
tation of the two modalities, which can be further
utilized in the network.

As the proposed architecture in Figure 1 de-
picts, our proposed model is an end-to-end system,
which takes multi-modal raw features for each ut-
terance in a video and predicts an output. We also
train our proposed IIM in the combined frame-
work. For any pair of modalities, e.g., text-visual,
the encoded vector in IIM receives two gradients
of errors, i.e., one error from the IIM output (vi-
sual) l1 and another from the task-specific label l2.
We aggregate the errors (l1 + l2) at the encoded
vector and backpropagate it to the input (text).
Thus, the weights in the encoder part will adjust
according to the desired task-specific label as well.
However, in contrast, the decoder part does not
have such information. Therefore, we employ an-
other IIM to capture the interaction between the
visual-text. This time, the visual features are aware
of the desired label during the interaction with tex-
tual features. A conceptual diagram, depicting the
gradient flow in IIM for the text and visual modal-
ities, is shown in Figure 2.

Figure 2: Difference between the interaction modules
of text-visual and visual-text pairs. l1 & l2 are the losses
from the IIM and output labels, respectively.

4 Datasets, Experiments and Analysis

In this section, we present our experimental results
along with necessary analysis. We also compare
our obtained results with several state-of-the-art
systems.

4.1 Datasets

For the evaluation of our proposed approach,
we employ five multi-modal benchmark datasets1

covering two affect analysis tasks, i.e., sentiment
and emotion.

1. YouTube (Morency et al., 2011b): The
YouTube opinion dataset contains 269 prod-
uct reviews utterances across 47 videos.
There are 169, 41, and 59 utterances in train-
ing, validation, and test set, respectively.

2. MOUD (Pérez-Rosas et al., 2013): The
dataset consists of 79 product review videos
in Spanish. Each video consists of multiple
utterances labeled with either positive, nega-
tive, or neutral sentiment. There are 243, 37,
106 utterances in training, validation, and test
set, respectively.

3. ICT-MMMO (Wöllmer et al., 2013): It is an
extension of YouTube opinion dataset that ex-
tends the number of videos from 47 to 340.
Each online social review video is annotated
for the sentiment. There are 220, 40, and 80
videos in training, validation, and test set, re-
spectively.

4. CMU-MOSI (Zadeh et al., 2016): It is a col-
lection of 2199 opinion utterances annotated
with the sentiment class. There are 1284 ut-
terances in the training set, 229 utterances in
the validation set, and 686 utterances in the
test set.

5. CMU-MOSEI (Zadeh et al., 2018c): CMU-
MOSEI is the largest dataset among all the
above. It has 3,229 videos which comprise
of approximately 23,000 utterances. Each
utterance is associated with one sentiment
value and six emotions (i.e., anger (4903
utterances), disgust (4028 utterance), fear
(1850 utterance), happy (12135 utterance),

1These datasets can be accessed through https://
github.com/A2Zadeh/CMU-MultimodalSDK.

https://github.com/A2Zadeh/CMU-MultimodalSDK
https://github.com/A2Zadeh/CMU-MultimodalSDK


5652

MOSEI MOSI ICT-MMMO YouTube MOUD
Emotion Sentiment Sentiment Sentiment Sentiment Sentiment

F1 W-Acc F1 A2 A5 A7 MAE r F1 A2 A7 MAE r F1 A2 F1 A3 F1 A2

T 77.66 60.13 77.22 79.45 48.50 48.87 0.695 0.572 77.43 77.69 37.75 0.996 0.658 71.80 77.50 48.27 49.15 72.24 73.58
A 76.14 57.68 74.64 77.27 45.02 44.97 0.782 0.433 59.30 59.32 23.17 1.423 0.165 77.90 78.34 43.12 44.06 45.46 60.37
V 75.03 57.59 69.13 75.04 42.22 42.51 0.804 0.317 50.48 51.31 23.03 1.465 0.122 78.28 78.75 49.79 50.84 62.47 62.76

T+V 78.01 57.10 77.31 79.51 48.98 49.84 0.685 0.592 77.70 78.13 38.19 0.946 0.673 80.10 81.25 53.11 53.91 78.40 79.24
T+A 77.53 60.69 77.80 80.06 48.66 49.08 0.694 0.579 78.98 79.15 39.35 0.952 0.671 78.99 80.01 48.82 49.37 72.93 74.52
A+V 76.60 59.47 74.76 77.38 45.32 45.87 0.775 0.437 49.30 53.49 24.19 1.464 0.189 79.12 81.25 51.93 52.54 64.41 65.09

T+A+V 79.02 62.97 78.23 80.37 49.15 50.14 0.683 0.594 79.54 79.88 38.92 0.914 0.689 81.47 82.75 55.13 55.93 82.07 82.41

Table 1: Results of sentiment and emotion analysis for the proposed approach. T: Text, V: Visual, A: Acoustic.
Weighted accuracy as a metric is chosen due to unbalanced samples across various emotions and it is also in line
with the other existing works (Zadeh et al., 2018c).

Multi-label No One Two Three Four Five Six

Count 3372 11050 5526 2084 553 84 8

Table 2: Statistics of multi-label emotions in CMU-MOSEI: Emotions-per-utterance.

sad (5856 utterance), and surprise (2262 ut-
terance). The emotion class happy is approx-
imately 52% of the total utterances, while
emotions fear and surprise are approximately
8-9% in the dataset. Further, many utterances
have more than one emotions representing
the case of multi-label classification problem.
The utterances for which all the emotions are
absent, we classify them into a no-emotion
class. We depict the statistics of multi-label
emotions in Table 2. The training, validation
and test set distributions are approximately
16K, 2K, and 4.6K, respectively.

4.2 Setups
The above datasets offer different dimension of
sentiment analysis. We define the following setups
for our experiments.

• Two-class (pos and neg) classification: MO-
SEI, MOSI, ICT-MMMO, and MOUD.

• Three-class (pos, neu, and neg) classifica-
tion: YouTube.

• Five-class (strong pos, weak pos, neu, weak
neg, and strong neg) classification: MOSEI.

• Seven-class (strong pos, moderate pos, weak
pos, neu, weak neg, moderate neg, and strong
neg) classification: MOSEI and MOSI.

• Intensity prediction: MOSEI and MOSI.

4.3 Experiments
We implement our proposed model on the Python-
based Keras deep learning library. As the evalua-
tion metric, we employ accuracy (weighted accu-

racy (Tong et al., 2017)) and F1-score for the clas-
sification problems, while for the intensity predic-
tion task, we compute Pearson correlation scores
and mean-absolute-error (MAE).

We evaluate our proposed CIA model on five
benchmark datasets i.e., MOUD, MOSI, YouTube,
ICT-MMMO, and MOSEI. For all the datasets, we
perform grid search to find the optimal hyper-
parameters (c.f. Table 4). Though we push for
a generic hyper-parameter configuration for all
datasets, in some cases, a different choice of the
parameter has a significant effect. Therefore, we
choose different parameters for different datasets
for our experiments. Details of hyper-parameters
for different datasets are depicted in Table 4.

We use different activation functions for the
various modules in our model. We use tanh as
the activation function for the inter-modal interac-
tive module (IIM), while we employ ReLu for the
context-aware attention module. For each dataset,
we use Adam as optimizer.

In this paper, we address three multi-modal af-
fective analysis problems, namely i.e., sentiment
classification (SC), sentiment intensity (SI ) and
emotion classification (EC). We use softmax as
a classifier for sentiment classification, while op-
timizing the categorical cross-entropy as a loss
function. In comparison, we use sigmoid for pre-
diction and binary cross-entropy as the loss func-
tion for the emotion classification. As the emo-
tions in the dataset are multi-labeled, we apply
a threshold over the predicted sigmoid outputs
for each emotion and consider all the emotions
as present whose respective values are above the
threshold. We cross-validate and optimize both



5653

MOSEI MOSI ICT-MMMO YouTube MOUD
Modality Emotion Sentiment Sentiment Sentiment Sentiment Sentiment

F1 W-Acc F1 A2 A5 A7 MAE r F1 A2 A7 MAE r F1 A2 F1 A3 F1 A2

CIA 79.02 62.97 78.23 80.37 49.15 50.14 0.683 0.594 79.54 79.88 38.92 0.914 0.689 81.47 82.75 55.13 55.93 82.07 82.41
CIA – IIM 77.81 61.86 77.69 79.53 48.02 49.16 0.714 0.566 39.32 55.24 34.40 0.941 0.652 80.10 81.25 50.02 52.54 78.14 78.30

Table 3: Ablation results for IIM module.

the evaluation metrics, i.e., F1-score and weighted
accuracy, and set the threshold as 0.4 and 0.18, re-
spectively.

Parameters MOUD MOSI YouTube MMMO MOSEI

Bi-GRU
50N 200N 100N

0.3D 0.5D

FC
50N 200N 100N

0.5D

Activations
ReLu as activation for our model

tanh as activation in IIM
Output Softmax (SC), tanh (SI ) & Sigmoid (EC)
Optimizer Adam (lr=0.001)
IIM Loss Mean Square Error (MSE)
Model Loss Cross-entropy (Classification) & MSE (Intensity)
Threshold 0.4 (F1) & 0.18 (W-Acc) for multi label in EC
Batch 16
Epochs 50

Table 4: Hyper-parameters for our experiments where
N, D, SC , SI and EC stands for #neurons, dropout,
sentiment classification, sentiment intensity and emo-
tion classification respectively.

We evaluate our proposed approach for all the
possible input combinations i.e., uni-modal (T,
A, V), bi-modal (T+V, T+A, A+V) and tri-modal
(T+V+A). We depict our obtained results in Ta-
ble 1. For MOSEI dataset, with tri-modal inputs,
our proposed system reports 79.02% F1-score and
62.97% weighted-accuracy for emotion classifi-
cation. For sentiment classification, we obtain
78.23%, 80.37%, 49.15% and 50.14% as F1-score
for two-class, five-class and seven-class, respec-
tively. For sentiment intensity prediction task, our
proposed system yields MAE and Pearson score
of 0.683 and 0.594, respectively. We also observe
that the proposed approach yields better perfor-
mance for the tri-modal inputs than the bi-modal
and uni-modal input combinations. This improve-
ment implies that our proposed CIA architecture
utilizes the interaction among the input modali-
ties very effectively. Furthermore, for the other
datasets, i.e., MOSI, ICT-MMMO, YouTube, and
MOUD, we also observe a similar phenomenon as
well (c.f. Table 1).

To show that our proposed IIM module, indeed,
learns the interaction among the distinct modali-
ties, we also perform an ablation study of the pro-
posed CIA architecture. Consequently, we omit

the IIM module from our architecture and compute
the self-attention on the pair-wise fully-connected
representations for the prediction. We observe
that, for all the datasets, the performance of this
modified architecture (i.e., CIA - IIM) is con-
stantly inferior (with 1% to 7% F-score points) to
the proposed CIA architecture. This performance
degradation suggests that the IIM module is, in-
deed, an important component of our proposed ar-
chitecture. In Table 3, we depict the evaluation
results for both- with and without IIM.

4.4 Comparative Analysis

In this section, we present our comparative stud-
ies against several existing and recent state-of-
the-art systems. For each dataset, we report
three best systems for the comparisons2. In
particular, we compare with the following sys-
tems: Bag of Feature - Multimodal Senti-
ment Analysis (BoF-MSA) (Blanchard et al.,
2018), Memory Fusion Network (MFN) (Zadeh
et al., 2018b), Deep Fusion - Deep Neural Net-
work (DF-DNN) (Nojavanasghari et al., 2016),
Multi View - LSTM (MV-LSTM) (Rajagopalan
et al., 2016), Early Fusion - LSTM (EF-LSTM)
(Zadeh et al., 2018c), Tensor Fusion Network
(TFN) (Zadeh et al., 2017), Random Forest
(RF) (Breiman, 2001), Support Vector Machine
(Zadeh et al., 2016), Multi-Attention Recur-
rent Network (MARN) (Zadeh et al., 2018a),
Dynamic Fusion Graph (DFG) (Zadeh et al.,
2018c), Multi Modal Multi Utterance-Bimodal
Attention (MMMU-BA) (Ghosal et al., 2018),
Bi-directional Contextual LSTM (BC-LSTM)
(Poria et al., 2017b) and Multimodal Factoriza-
tion Model (MFM) (Tsai et al., 2018).

We show the comparative results in Table 5a
and Table 5b for emotion and sentiment analy-
sis, respectively. We observe that the proposed
CIA framework yields better performance against
the state-of-the-art for all the cases. For emotion
classification, our proposed approach achieves
approximately 3 and 0.6 percentage higher F1-

2Please note that we report all the results, which are avail-
able for comparison



5654

MOSEI
Anger Disgust Fear Happy Sad Surprise Average

System F1 W-Acc F1 W-Acc F1 W-Acc F1 W-Acc F1 W-Acc F1 W-Acc F1 W-Acc
MFN? - - 71.4 65.2 89.9 - - - 60.8 - 85.4 53.3 - -
DF? 71.4 - - 67.0 - - - - - - - - - -
MV-LSTM? - 56.0 - - - - - - - - - - - -
EF-LSTM? - - - - - 56.7 - 57.8 - 59.2 - - - -
TFN? - 60.5 - - - - 66.6 66.5 - 58.9 - 52.2 - -
RF? 72.0 - 73.2 - 89.9 - - - 61.8 - 85.4 - - -
SVM? - - - - - 60.0 - - - - - - - -
MARN? - - - - - - 71.0 - - - - - - -
DFG 72.8 62.6 76.6 69.1 89.9 62.0 66.3 66.3 66.9 60.4 85.5 53.7 76.3 62.3
CIA 74.7 67.4 81.8 74.1 87.8 63.9 71.3 51.9 72.6 61.8 86.0 58.2 79.0 62.9
T-test - - - - - - - - - - - - 0.0002 0.0057

(a) Emotion Analysis

MOSEI MOSI ICT-MMMO YouTube MOUD
System F1 A2 A5 A7 MAE r F1 A2 A7 MAE r F1 A2 F1 A3 F1 A2

MARN† - - - - - - 77.0 77.1 34.7 0.968 0.625 - - - 48.3 81.2 81.1
MFN† 76.0 76.0 - - - - 77.3 77.4 34.1 0.965 0.632 73.1 73.8 51.6 51.7 80.4 81.1
TFN† - - - - - - - - - - - 72.6 72.5 - - - -
BC-LSTM† - - - - - - - - - - - - - 45.1 - - -
MFM - - - - - - 78.1 78.1 36.2 0.951 0.662 79.2 81.3 52.4 53.3 81.7 82.1
BoF-MSA? 63.2 60.0 - - 0.91 0.30 - - - - - - - - - - -
MV-LSTM? 76.4 76.4 - - - - - - - - - - - - - - -
DFG 77.0 76.9 45.1 45.0 0.71 0.54 - - - - - - - - - - -
MMMU-BA 77.6 79.8 - - - - - - - - - - - - - - -
CIA 78.2 80.4 49.2 50.1 0.68 0.59 79.54 79.88 38.92 0.914 0.689 81.47 82.75 55.13 55.93 82.07 82.41
T-test 0.038 0.038 0.00003 0.00001 0.0002 0.00001 0.0049 0.0022 0.0003 0.0018 0.0003 0.0005 0.0041 0.00006 0.00003 0.031 0.040

(b) Sentiment Analysis

Table 5: Comparative results. ?Values are taken from (Zadeh et al., 2018c). †Values are taken from (Tsai et al.,
2018). Significance T-test (< 0.05) signifies that the obtained results are statistically significant over the existing
systems with 95% confidence score.

score and weighted accuracy, respectively, than
the state-of-the-art DFG (Zadeh et al., 2018c) sys-
tem. Furthermore, we also see improvements for
most of the individual emotion classes as well.

In sentiment analysis (c.f. Table 5b), for all the
five datasets and different experimental setups, the
proposed CIA framework obtains the improved
accuracies for the classification tasks. For inten-
sity prediction, our proposed framework yields
lesser mean-absolute-error with high Pearson cor-
relation scores. On average, we observe 1 to 5%
improvement in accuracy values in comparison to
the next best systems. Similarly, for the intensity
prediction task, we report approximately 0.03 and
0.04 points improvement in mean-absolute-error
and Pearson score, respectively.

We perform statistical significance test (paired
T-test) on the obtained results and observe that
performance improvement in the proposed model
over the state-of-the-art is significant with 95%
confidence (i.e., p-value< 0.05).

4.5 Error Analysis

We analyze our proposed CIA model to under-
stand the importance of the baseline framework

CIA-IIM. We study the predictions of both the
models and observe that the proposed CIA frame-
work improves the predictions of the baseline
CIA–IIM model. It indicates that the CIA frame-
work, indeed, learns the interaction among the in-
put modalities, and the model effectively exploits
this interaction for better judgment. In Table 6, we
list the utterances of a CMU-MOSEI video along
with their correct and predicted labels for both the
proposed and baseline systems.

The video in Table 6 has 4 utterances, out of
which the correct sentiments of three utterances
(i.e., u1, u3, and u4) are positive, while one ut-
terance (i.e., u2) is negative. We observe that our
proposed CIA model predicts all the 4 utterances
correctly, while the CIA-IIM mis-classify the sen-
timents of the utterances, u2 and u3.

We also analyze the context-aware attention
module (CAM) with the help of heatmaps of the at-
tention weights. The heatmaps, as depicted in Fig-
ure 3, represent the contributing utterances in the
neighbourhood for the classification of each utter-
ance. Figures 3a, 3b and 3c show the heatmaps
of the pair-wise modality interaction of the pro-
posed model CIA. In Figure 3a, each cell(i,j) of



5655

Sentiment Emotion
Utterances Actual CIA−IIM CIA Actual CIA−IIM CIA

1 these critics argue that the welfare state breeds dependence
and incompetence among those who receive it they

Pos Pos Pos Happy, Sad Happy Happy, Sad

2 argue that it creates social pathologies such as single parent
families excess fertility and laziness

Neg Pos Neg Sad Happy Sad

3 some argue that people who receive welfare benefits cannot
spend their benefits rationally and

Pos Neg Pos Fear, Sad Sad Fear, Sad

4 and then lastly some people on the moral side argue that noth-
ing should be given to a person without requiring a reciprocal
obligation from that person so

Pos Pos Pos Sad Happy, Sad Sad

Table 6: Comparison between proposed CIA and CIA−IIM frameworks in MOSEI dataset. Few cases where
CIA framework performs better than the CIA−IIM framework. Red text signifies error in classification.

u1 u2 u3 u4

u 1
u 2

u 3
u 4

u1

u2

u3

u4
u1 u2 u3 u4

(a) SentimentTV

u1 u2 u3 u4

u 1
u 2

u 3
u 4

u1 u2 u3 u4

(b) SentimentTA

u1 u2 u3 u4

u 1
u 2

u 3
u 4

u1 u2 u3 u4

(c) SentimentAV

u1 u2 u3 u4

u 1
u 2

u 3
u 4

u1 u2 u3 u4

(d) EmotionTV

u1 u2 u3 u4

u 1
u 2

u 3
u 4

u1 u2 u3 u4

(e) EmotionTA

u1 u2 u3 u4

u 1
u 2

u 3
u 4

u1 u2 u3 u4

(f) EmotionAV

Figure 3: Heatmap analysis of MOSEI dataset where (a), (b) & (c) represents the Contextual Attention weights
for TV, TA and AV for sentiment and (d), (e) & (f) are contextual attention weights for TV, TA and AV for emotion.

the heatmap signifies the weights of utterance ‘j’
for the classification of utterance ‘i’. For the utter-
ance u4, the model puts more attention weights on
the u2 and u3 of the text-visual interactions, while
for the text-acoustic interaction the model assigns
higher weights to the u4 utterance itself. Similarly,
the model assigns the least weight to the u1 ut-
terance, whereas the utterance u3 gets the highest
weights. We argue that the proposed CAM module
captures the diversity in the input modalities of the
contextual utterances for the correct prediction.

For emotion prediction, the CIA model cap-
tures all the emotions correctly, while the CIA-IIM
framework fails to predict the correct emotions of
the utterances, u2 and u3. For the same video,
we also show the attention heatmaps for emotion
in Figure 3. For the utterance u2, our proposed
model (CIA) captures the emotion class ‘sad’ as
the CAM module assigns higher attention weights
on the utterances u2 and u3 in Figure 3d, u4 in Fig-
ure 3e, and u2 in Figure 3f. Since the system finds
the contributing neighbours as utterances u2, u3
and u4 for various combinations, we argue that it
utilizes the information of these utterances - which
all express the ‘sad’ emotion - for the correct pre-
diction of utterance u2 as ‘sad’.

5 Conclusion

In this paper, we have proposed a Context-aware
Interactive Attention framework that aims to cap-
ture the interaction between the input modalities
for the multi-modal sentiment and emotion predic-
tion. We employed a contextual attention module
to learn the contributing utterances in the neigh-
borhood by exploiting the interaction among the
input modalities. We evaluate our proposed ap-
proach on five standard multi-modal datasets. Ex-
periments suggest the effectiveness of the pro-
posed model over various existing systems, for
both sentiment and emotion analysis, as we ob-
tained new state-of-the-art for all five datasets.

In current work, we undertook the problem of
sentiment and emotion analysis for a single-party
utterances. In future, we would like to extend our
work towards the multi-party dialogue.

6 Acknowledgment

The research reported here is partially supported
by SkyMap Global India Private Limited. Asif
Ekbal acknowledges the Young Faculty Research
Fellowship (YFRF), supported by Visvesvaraya
PhD scheme for Electronics and IT, Ministry of
Electronics and Information Technology (MeitY),
Government of India, being implemented by Dig-
ital India Corporation (formerly Media Lab Asia).



5656

References
Md Shad Akhtar, Dushyant Chauhan, Deepanway

Ghosal, Soujanya Poria, Asif Ekbal, and Pushpak
Bhattacharyya. 2019. Multi-task learning for multi-
modal emotion recognition and sentiment analysis.
In Proceedings of the 2019 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics: Human Language Technologies,
Volume 1 (Long and Short Papers), pages 370–379,
Minneapolis, Minnesota. Association for Computa-
tional Linguistics.

John Arevalo, Thamar Solorio, Manuel Montes-y
Gómez, and Fabio A González. 2017. Gated mul-
timodal units for information fusion. arXiv preprint
arXiv:1702.01992.

Nathaniel Blanchard, Daniel Moreira, Aparna Bharati,
and Walter Scheirer. 2018. Getting the subtext with-
out the text: Scalable multimodal sentiment classifi-
cation from visual and acoustic modalities. In Pro-
ceedings of Grand Challenge and Workshop on Hu-
man Multimodal Language, pages 1–10.

Leo Breiman. 2001. Random forests. Machine Learn-
ing, 45(1):5–32.

KyungHyun Cho, Bart van Merrienboer, Dzmitry Bah-
danau, and Yoshua Bengio. 2014. On the properties
of neural machine translation: Encoder-decoder ap-
proaches. CoRR, abs/1409.1259.

Bhuwan Dhingra, Hanxiao Liu, Zhilin Yang,
William W Cohen, and Ruslan Salakhutdinov.
2016. Gated-attention readers for text comprehen-
sion. arXiv preprint arXiv:1606.01549.

Jiamin Fu, Qirong Mao, Juanjuan Tu, and Yongzhao
Zhan. 2017. Multimodal shared features learning
for emotion recognition by enhanced sparse local
discriminative canonical correlation analysis. Mul-
timedia Systems, pages 1–11.

Deepanway Ghosal, Md Shad Akhtar, Dushyant
Chauhan, Soujanya Poria, Asif Ekbal, and Pushpak
Bhattacharyya. 2018. Contextual inter-modal atten-
tion for multi-modal sentiment analysis. In Proceed-
ings of the 2018 Conference on Empirical Methods
in Natural Language Processing, pages 3454–3466,
Brussels, Belgium. Association for Computational
Linguistics.

Devamanyu Hazarika, Sruthi Gorantla, Soujanya Po-
ria, and Roger Zimmermann. 2018. Self-attentive
feature-level fusion for multimodal emotion detec-
tion. In IEEE 1st Conference on Multimedia Infor-
mation Processing and Retrieval, MIPR 2018, Mi-
ami, FL, USA, April 10-12, 2018, pages 196–201.

Chan Woo Lee, Kyu Ye Song, Jihoon Jeong, and
Woo Yong Choi. 2018. Convolutional atten-
tion networks for multimodal emotion recogni-
tion from speech and text data. arXiv preprint
arXiv:1805.06606.

Rada Mihalcea. 2012. Multimodal sentiment analy-
sis. In Proceedings of the 3rd Workshop in Com-
putational Approaches to Subjectivity and Sentiment
Analysis, WASSA@ACL 2012, July 12, 2012, Jeju
Island, Republic of Korea, page 1.

Louis-Philippe Morency, Rada Mihalcea, and Payal
Doshi. 2011a. Towards multimodal sentiment anal-
ysis: harvesting opinions from the web. In Pro-
ceedings of the 13th International Conference on
Multimodal Interfaces, ICMI 2011, Alicante, Spain,
November 14-18, 2011, pages 169–176.

Louis-Philippe Morency, Rada Mihalcea, and Payal
Doshi. 2011b. Towards multimodal sentiment anal-
ysis: Harvesting opinions from the web. In Proceed-
ings of the 13th international conference on multi-
modal interfaces, pages 169–176. ACM.

Behnaz Nojavanasghari, Deepak Gopinath, Jayanth
Koushik, Tadas Baltrušaitis, and Louis-Philippe
Morency. 2016. Deep multimodal fusion for per-
suasiveness prediction. In Proceedings of the 18th
ACM International Conference on Multimodal Inter-
action, ICMI 2016, pages 284–288, New York, NY,
USA. ACM.

Amol S Patwardhan. 2017. Multimodal mixed emotion
detection. In Communication and Electronics Sys-
tems (ICCES), 2017 2nd International Conference
on, pages 139–143. IEEE.

Verónica Pérez-Rosas, Rada Mihalcea, and Louis-
Philippe Morency. 2013. Utterance-level multi-
modal sentiment analysis. In Proceedings of the
51st Annual Meeting of the Association for Compu-
tational Linguistics (Volume 1: Long Papers), vol-
ume 1, pages 973–982.

Soujanya Poria, Erik Cambria, Rajiv Bajpai, and Amir
Hussain. 2017a. A review of affective computing:
From unimodal analysis to multimodal fusion. In-
formation Fusion, 37:98–125.

Soujanya Poria, Erik Cambria, Devamanyu Hazarika,
Navonil Majumder, Amir Zadeh, and Louis-Philippe
Morency. 2017b. Context-dependent sentiment
analysis in user-generated videos. In Proceedings of
the 55th Annual Meeting of the Association for Com-
putational Linguistics (Volume 1: Long Papers),
volume 1, pages 873–883.

Soujanya Poria, Iti Chaturvedi, Erik Cambria, and
Amir Hussain. 2016. Convolutional mkl based mul-
timodal emotion recognition and sentiment analysis.
In Data Mining (ICDM), 2016 IEEE 16th Interna-
tional Conference on, pages 439–448. IEEE.

Soujanya Poria, Haiyun Peng, Amir Hussain, Newton
Howard, and Erik Cambria. 2017c. Ensemble appli-
cation of convolutional neural networks and multiple
kernel learning for multimodal sentiment analysis.
Neurocomputing, 261:217–230.

https://doi.org/10.18653/v1/N19-1034
https://doi.org/10.18653/v1/N19-1034
http://aclweb.org/anthology/W18-3301
http://aclweb.org/anthology/W18-3301
http://aclweb.org/anthology/W18-3301
https://doi.org/10.1023/A:1010933404324
http://arxiv.org/abs/1409.1259
http://arxiv.org/abs/1409.1259
http://arxiv.org/abs/1409.1259
http://www.aclweb.org/anthology/D18-1382
http://www.aclweb.org/anthology/D18-1382
https://doi.org/10.1109/MIPR.2018.00043
https://doi.org/10.1109/MIPR.2018.00043
https://doi.org/10.1109/MIPR.2018.00043
https://doi.org/10.1145/2993148.2993176
https://doi.org/10.1145/2993148.2993176


5657

Shyam Sundar Rajagopalan, Louis-Philippe Morency,
Tadas Baltrusaitis, and Roland Goecke. 2016. Ex-
tending long short-term memory for multi-view
structured learning. In European Conference on
Computer Vision (ECCV-2016), pages 338–353.
Springer International Publishing.

Hiranmayi Ranganathan, Shayok Chakraborty, and
Sethuraman Panchanathan. 2016. Multimodal emo-
tion recognition using deep learning architectures.
In Applications of Computer Vision (WACV), 2016
IEEE Winter Conference on, pages 1–9. IEEE.

Edmund Tong, Amir Zadeh, Cara Jones, and Louis-
Philippe Morency. 2017. Combating Human Traf-
ficking with Multimodal Deep Models. In Proceed-
ings of the 55th Annual Meeting of the Association
for Computational Linguistics (Volume 1: Long Pa-
pers), pages 1547–1556. Association for Computa-
tional Linguistics.

Yao-Hung Hubert Tsai, Paul Pu Liang, Amir Zadeh,
Louis-Philippe Morency, and Ruslan Salakhutdinov.
2018. Learning factorized multimodal representa-
tions. arXiv preprint arXiv:1806.06176.

Panagiotis Tzirakis, George Trigeorgis, Mihalis A
Nicolaou, Björn W Schuller, and Stefanos Zafeiriou.
2017. End-to-end multimodal emotion recogni-
tion using deep neural networks. IEEE Journal of
Selected Topics in Signal Processing, 11(8):1301–
1309.

Martin Wöllmer, Felix Weninger, Tobias Knaup, Björn
Schuller, Congkai Sun, Kenji Sagae, and Louis-
Philippe Morency. 2013. Youtube movie reviews:
Sentiment analysis in an audio-visual context. IEEE
Intelligent Systems, 28(3):46–53.

A Zadeh, PP Liang, S Poria, P Vij, E Cambria, and
LP Morency. 2018a. Multi-attention recurrent net-
work for human communication comprehension. In
Thirty-Second AAAI Conference on Artificial Intel-
ligence (AAAI-2018), pages 5642 – 5649, New Or-
leans, USA.

A. Zadeh, R. Zellers, E. Pincus, and L. P. Morency.
2016. Multimodal Sentiment Intensity Analysis in
Videos: Facial Gestures and Verbal Messages. IEEE
Intelligent Systems, 31(6):82–88.

Amir Zadeh, Minghai Chen, Soujanya Poria, Erik
Cambria, and Louis-Philippe Morency. 2017. Ten-
sor fusion network for multimodal sentiment analy-
sis. In Proceedings of the 2017 Conference on Em-
pirical Methods in Natural Language Processing,
pages 1103–1114, Copenhagen, Denmark.

Amir Zadeh, Paul Pu Liang, Navonil Mazumder,
Soujanya Poria, Erik Cambria, and Louis-Philippe
Morency. 2018b. Memory fusion network for multi-
view sequential learning. In Proceedings of the
Thirty-Second AAAI Conference on Artificial Intelli-
gence, New Orleans, Louisiana, USA, February 2-7,
2018. AAAI Press.

Amir Zadeh, Paul Pu Liang, Soujanya Poria, Erik Cam-
bria, and Louis-Philippe Morency. 2018c. Multi-
modal language analysis in the wild: Cmu-mosei
dataset and interpretable dynamic fusion graph. In
Proceedings of the 56th Annual Meeting of the As-
sociation for Computational Linguistics (Volume 1:
Long Papers), pages 2236–2246. Association for
Computational Linguistics.

https://doi.org/10.18653/v1/P17-1142
https://doi.org/10.18653/v1/P17-1142
https://doi.org/10.1109/MIS.2016.94
https://doi.org/10.1109/MIS.2016.94
https://www.aclweb.org/anthology/D17-1115
https://www.aclweb.org/anthology/D17-1115
https://www.aclweb.org/anthology/D17-1115
https://www.aaai.org/ocs/index.php/AAAI/AAAI18/paper/view/17341
https://www.aaai.org/ocs/index.php/AAAI/AAAI18/paper/view/17341
http://aclweb.org/anthology/P18-1208
http://aclweb.org/anthology/P18-1208
http://aclweb.org/anthology/P18-1208

