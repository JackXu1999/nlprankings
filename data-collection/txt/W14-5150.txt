



















































Proceedings of the...


D S Sharma, R Sangal and J D Pawar. Proc. of the 11th Intl. Conference on Natural Language Processing, pages 362–367,
Goa, India. December 2014. c©2014 NLP Association of India (NLPAI)

Named Entity Based Answer Extraction form Hindi Text Corpus Using 
n-grams 

 
 

Lokesh Kumar Sharma Namita Mittal 
Dept. of Computer Science and Engineering Dept. of Computer Science and Engineering 
Malaviya National Institute of Technology Malaviya National Institute of Technology 

Jaipur, India Jaipur, India 
2013rcp9007@mnit.ac.in Nmittal.cse@mnit.ac.in 

 
 
 

Abstract 

Most existing systems, are constructed for the 
English language, such as state-of-art system 
Watson that win the Jeopardy challenge. 
While working with Indian languages (i.e. 
Hindi), a richer morphology, greater syntactic 
variability, and less number of standardized 
rules availability in the language are just some 
issues that complicate the construction of sys-
tems. It is also considered a resource-poor 
language since proper gazetteer lists and 
tagged corpora is not available for it. In this 
paper, Named Entity (NE) based n-gram ap-
proach is used for processing questions writ-
ten in Hindi language and extract the answer 
from Hindi documents. Combination of clas-
sical information retrieval term weighing 
model with a linguistic approach mainly based 
on syntactic analysis is used. We use a corpus 
of 420 questions and 300 documents contain-
ing around 20,000 words as a back-end for 
closed-domain (World History) Question 
Answering. A Named Entity Recognizer is 
employed to identify answer candidates which 
are then filtered according their usage. Results 
obtained using this technique outperforms the 
previously used techniques (e.g. Semantic 
Based Query Logic).   

1 Introduction 

With the advancement in technology, Question 
Answering has become a major area of research. 
Question Answering systems enable the user to ask 
questions in natural language instead of a query 
and retrieve one or many valid and accurate an-
swers in natural language. The explosion of infor-
mation on Internet, Natural language QA is 

recognized as a capability with great potential 
(Hirschman and Gaizauskas, 2001). Information 
retrieval systems allow us to locate full documents 
or best matching passages that might contain the 
pertinent information, but most of them leave it to 
the user to extract the useful information from a 
ranked list. Therefore, professionals from various 
areas are beginning to recognize the usefulness of 
other types of systems, such as QA systems, for 
quickly and effectively finding specialist informa-
tion. The QA technology takes both IR and IE a 
step further, and provides specific and brief an-
swers to the user’s questions formulated naturally. 
Hindi holds 5th position among top 100 spoken 
languages in the world, with no. of speakers being 
close to 200 million (Shachi et al., 2001) but com-
paring Indian languages with other languages, 
word segmentation is a key problem in Indian 
question answering. As per our knowledge not 
much work has been done in Hindi as compared to 
various other languages like English (Ittycheriah  
et  al., 2008), Chinese etc. This motivates for de-
veloping a Hindi question answering system (Vi-
shal and Jaspreet, 2013). Our dataset consists of 
420 questions and 300 documents containing 
around 20,000 words chosen from a specific do-
main (World History). Our model involves three 
general phases which are as follows. The first 
phase, Question Processing, involves analyzing 
and classifying the questions into different catego-
ries. This classification later helps in Answer type 
Detection. Further, in this module, a query is for-
mulated which is passed on to the next phase for 
searching the relevant documents which might 
contain the answer. In the second phase, Informa-
tion Retrieval, we have applied an algorithm called 
Term Frequency-Inverse-Document-Frequency 

362



 

(TF-IDF) (Ramos, 2003), which uses dot product 
and cosine similarity rule to find the probability of 
a given text in a given set of documents. This gives 
us the list of relevant documents. The next phase, 
Answer Extraction, uses bigram forming approach 
(Wang et al., 2005) to retrieve the answer from a 
given document. In this we have also used a pre-
built Hindi named entity recognition model which 
categorizes the given text into different categories. 

2 Related Work  

Specific research in the area of question answer-
ing has been prompted in the last few years in 
particular by the Question Answering track of 
the Text Retrieval Conference (TREC-QA) 
competitions (Satoshi and Ralph, 2003). Recent-
ly IBM Watson defeated two human winners and 
win the Jeopardy game show. Watson uses very 
complex algorithm to read any given clue. At the 
first stage in question analysis Watson does pars-
ing and semantic analysis using a deep Slot 
Grammar parser, a named entity recognizer, a 
co-reference resolution component, and a rela-
tion extraction component (Lilly et. al 2012). 
Our work uses similar approach by using named 
entity taggers and parsing. Research work has 
been done in Surprise Language Exercise (SLE) 
within the TIDES program where viability of a 
cross lingual question answering (CLQA) (Sha-
chi et al., 2001) has been shown by developing a 
basic system. It presents a model that answers 
English questions by finding answers in Hindi 
newswire documents and further translates the 
answer candidates into English along with the 
context surrounding each answer (Satoshi and 
Ralph, 2003). Another approach taken by some 
researchers (Praveen et al., 2003) presents a 
Hindi QA system based on a Hindi search en-
gine that works on locality-based similarity heu-
ristics to retrieve relevant passages from the 
corpus over agriculture and science domain. 
Some researchers (Sahu et al., 2012) discusses 
an implementation of a Hindi question answer-
ing system “PRASHNOTTAR”. It presents four 
classes of questions namely: “when”, “where”, 
“what time” and “how many” and their static 
dataset includes 15 questions of each type which 
gives an accuracy of 68%. In addition to the tra-
ditional difficulties with syntactic analysis, there 
remains many other problems to be solved, e.g., 

semantic interpretation, ambiguity resolution, 
discourse modeling, inference, common sense 
etc. 

3 Proposed Approach 

Question Processing is the first phase of our pro-
posed question answering model in which we ana-
lyze the question and create a proper IR query 
which is further used to retrieve some relevant 
documents which may contain the answer of the 
question. Another task is question classification to 
classify a question by the type of answer it re-
quires. The former task is called Question Classifi-
cation and the latter one is known as Query 
Formulation. Both these aspects are equally impor-
tant for Question Processing. 

3.1. Question Classification 

The goal of Question Classification is to accurately 
assign labels to questions based on expected an-
swer type. Hence, we detect the category of a giv-
en question.  

Question Phrase Answer Type (AT) 
या AT:Desc, Single type  can-

not  be decided 
कब Date 
कहाँ Location 
कतनी कतना 
कतने 

Number 

कौनसा कौनसी Answer  type  depends  on  
next following word 

कसका कसक  
कौन कसे कसने 

Person 

य  AT:Desc, Single type  can-
not  be decided 

कैसे AT:Method,  Single  type  
cannot be decided 

कस Answer  type  depends  on  
next following word 

Table 1. Possible Answer Type Based on Question 
Phrase 

In English there are 6 main categories namely 
LOCATION, PERSON, NUMERIC, ENTITY,  
ABBREVIATION and DESCRIPTION and but for 

363



 

Hindi we have taken only 4 categories for our ca-
tegorization process includes PERSON,  COUNT, 
DATE and LOCATION. We applied proposed al-
gorithm over the following answer types hig-
hlighted in table 1. The output file contains the 
previously mentioned category of question to 
which it belongs followed by the question itself 
and thus mapping from questions to answer types 
is done here. After categorization of the question, 
we store it in a file, so that it can be used later for 
answer extraction. Here is an example. Suppose we 
have the following question, लोक अदालत क  
शु आत राजःथान म सबसे पहले कहां हुई ? Then 
the output file will contain: LOCATION: लोक 
अदालत क  शु आत राजःथान म सबसे पहले कहां 
हुई। 

3.2. Query Formation 

Query Formation is a technique to make the ques-
tion format such that it can be passed on to a sys-
tem which takes the input in the form of a query 
and searches out the relevant documents i.e. the 
documents which have the maximum probability 
of containing the answer. For this purpose, we 
have formulated a query by extracting the main or 
focus words (Haung, 2008) of the question by re-
moving the stop words occurring in the question. 
For this, we have used a file containing a prebuilt 
list of stop words. Examples of some stop words 
are: (के, का, हुई, है, पर, इस, होता, , बनी, नह ं, तो, 
ह , या, एवं, दया, हो, इसका, था, ारा, हुआ, तक, 
साथ, करना, कुछ, सकते, कसी, हुई) After stop 
words removal, the text looks like this: लोक 
अदालत शु आत राजःथान पहले After removal of 
these less important words from the question, the 
resultant output can be used as a query for the in-
formation retrieval system which involves the next 
part of the model. 

3.3. Relevant Information Extraction 

The task of Information Retrieval phase is to query 
the IR Engine, find relevant documents and return 
candidate passages that are likely to contain the 
answer. In our model, our dataset is scattered over 
various documents, each containing question re-

lated text along with its answer. Then we per-
formed a search within these documents in order to 
find out such documents which may contain the 
answer. And for this purpose, we have applied an 
algorithm called TF-IDF; it gives as output the list 
of various documents which may contain any of 
the given words from the query. The term frequen-
cy (TF) for a given term ti within a particular doc-
ument dj is defined as the number of occurrences 
of that term in the djth document, which is equal to 
ni,j: the number of occurrences of the termti in the 
document dj. 

TFi,j = ni,j 

IDF(ti) = loge(Total number of documents / Num-
ber of documents with term t in it). 

IDFi = 
 | |

|    |
 

With |D|: total number of documents in the collec-
tion and |{d : ti  d}|: number of documents where 
the term ti appears. To avoid divide-by-zero, we 
can use 1 + |{d : ti  d}|. For a given corpus D, 
then the TF-IDF is then defined as:  

(TF-IDF )i,j = TF i,j × IDFi. 

The input of TF-IDF is the file which contains fo-
cus words of the question i.e. the output of query 
processing. When TF-IDF algorithm was applied 
on this file, it gave as output the relevant docu-
ments i.e. documents having maximum probability 
of containing the answer. TF-IDF numbers imply a 
strong relationship with the document they appear 
in, suggesting that if that word were to appear in a 
query, the document could be of interest to the user 
(Ramos, 2003). For our given example, the given 
method extracted the following: 

औधोिगक ववाद  के व रत िनपटारे के कए जयपुर 
ःथत राजःथान उ च- ययालय म 20 जुलाई को मेगा 
लोक अदालत का आयोजन कया जाएगा । लोक 
अदालत क  शु आत राजःथान म सबसे पहले कोटा म 
हुई । </146.txt> 
146 is the document number from which it extracts 
the passage. Then we take the mentioned docu-
ments and retrieve all its content in a separate file 
which is further used to find answers in the answer 
extraction phase. 

3.4. Answer Extraction 
364



 

The final task of a QA system is to process the 
relevant Passages (which we get after Information 
Retrieval phase) and extract a segment of word(s) 
that is likely to be the answer of the question. 
Question classification comes handy here. There 
are various techniques for answer extraction. We 
have used the following steps to extract answer.  
Step 1: Take the file containing the text and re-
move all of its stop words. 
Step 2: Take the file which contains the question 
and form its bigrams i.e. form words taking twice 
a time and stored it in a file. 
Step 3: Then take the output file and form the 
bigram of the text it contains and match it with 
the file which contains the question’s bigrams. 
Step 4: Save the number of bigrams matched for 
each line to the question’s bigrams. 
Step 5: Output the line which contains the maxi-
mum number of bigrams matched. 
We have the following output after removing stop 
words from the passage: 

औधोिगक ववाद  व रत िनपटारे जयपुर ःथत 
राजःथान उ च- ययालय 20 जुलाई मेगा लोक 
अदालत आयोजन जाएगा लोक अदालत शु आत 
राजःथान पहले कोटा 
After storing this output file as a target document. 
The questions are stored in a separate file of their 
bigrams i.e. taking two words together (Wang and 
McCallum, 2005). Storing the outcome in a file 
called QBigram-feature file. This gave us the fol-
lowing output, 

Q-Bi-gram(feature) = {(लोक अदालत)1, (अदालत 
शु आत)2, (शु आत राजःथान)3, (राजःथान पहले)4, 
(पहले हुई)5} 
The given passage will have following bigrams, 

P-Bi-gram(feature) = {(लोक अदालत)1, (अदालत 
शु आत)2, (शु आत राजःथान)3, (राजःथान पहले)4, 
(पहले कोटा)5, (कोटा हुई)6} 
Now these bigrams will be matched with the ques-
tion’s bigram as per our designed algorithm. The 
concept in this is, the line which contains the max-
imum number of two words same at a time will 

have maximum probability of containing the an-
swer. So when we do this we will get following 
line as output:  

लोक अदालत क  शु आत राजःथान म सबसे पहले 
कोटा म हुई । </146.txt> 

Now we pass the question containing file to the 
prebuilt Hindi Named Entity Recognition (NER) 
System (Maksim and Andrey, 2012) which will tag 
the given text into the aforementioned 5 categories. 
The NER gives output as following: 

लोक o            both 
अदालत o both 
श आत o both 
राजःथान LOCATION GAZETTEER   
पहले o both 

As we know the possible type of answer from the 
question classification method which we have ap-
plied earlier, we can remove those named entities 
which are present in both answer and the question, 
as they will not be the required answer. And hence, 
the remaining tagged entity will be our required 
answer. After removing the named entities which 
are tagged in the question, following words are left 
in the text: लोक अदालत शु आत पहले कोटा Now 
running the NER on the output line again, getting 
the tagged output: 

लोक o            both 
अदालत o both 
श आत o both 
पहले o both 
कोटा LOCATION GAZETTEER   

Through this output, we extract the entities which 
matches the Answer Type which we have detected 
earlier i.e. Answer Type Detection (Roberts and 
Hickl, 2008) is done on the output. For example in 
our case here the Answer Type is LOCATION, so 
we extract the entity which is tagged as location 
which is <कोटा>. 

365



 

 

Figure 1: Proposed Named Entity Based QA System Architecture 

Hence, this is our final answer. Overall system 
architecture is shown in figure 1. 

4. Experimental Setup and Analysis  

To evaluate the effectiveness of the proposed             
methods for answer extraction from Hindi cor-
pus, 300 standard documents datasets is used. 
The accuracy for the questions of category ‘कब’, 
‘कहाँ’, ‘ कतनी, कतना, कतने’, and ‘ कसका, 
कसक ’ is satisfactory in proposed approach 

shown in table 2. The accuracy of question type 
‘ कस समय’ is not considered by the proposed 
approach because the answer type of this ques-
tion has been not considered. The accuracy of 
the question type ‘कब’, ‘कहाँ’, and ‘ कसका 
कसक ’ is highly accurate. Some question has 

low syntactic information to reach the answer, 
and it is difficult for the system to answer.  For 
such a questions it may have multiple documents 
and multiple matches in these documents, an 
algorithm may not extract every answer in the 

dataset perfectly. For every question, first com-
pute its precision (P) and its recall (R) by taking 
the dataset as gold standard answers as the rele-
vant answer and the predicted answer at the re-
trieved set. Now, taking an average of P and R 
over all Topics. Now, calculating macro F1 us-
ing the harmonic mean of the average P and R, 

   

Where,  
 

   and   .  
 
Accuracy (F1-measure) is calculated which out-
performs existing Semantic Based Query Logic 
approach comparison results are highlighted in 
the Table 2. 
 

Type of 
Question 

(Total 420 
Question) 

Accuracy (macro F1) 

(Semantic 
Based Query 

Logic) 

(Proposed 
Approach- NE 
Based n-gram) 

366



 

कब 66.66% 74.33% 
कहाँ 53.00% 86.66% 
कतना 
कतने 
कतनी 

73.33% 72.50% 

कसका 
कसक  - 82.75% 

Total 64.33% 79.06% 

Table 2. Accuracy of the proposed approach 

The question set of 420 questions1 and supported 
answer documents used in this work are manual-
ly collected from web. The documents have an-
swer for every question still it is not easy to 
extract correct answers for all questions. 

5. Conclusion and Future Work 

In this paper, Question answering for Hindi 
language has been experimented on 420 natural 
language questions. Results outperforms the 
previously used semantic based logic query 
approach. Using this approach, we achieved 
state-of-art results for most of the question 
types namely Person, Location, Date and 
Count. But as this approach is syntactic, so us-
ing this approach we able to get answers for 
factoid questions. Text where usage of syn-
onyms or hyponyms of words is seen, accurate 
answers could not be extracted. Such issues can 
be dealt by introduction of the semantic ap-
proach. Results can be improved by adding fea-
tures like entailment, co-reference etc in the 
answer extraction phase. Improving the accura-
cy of Hindi NER will also help in improving 
the accuracy of the system. Also, as our model 
is domain based, one can extend its domain by 
using a searching algorithm over the Wikipedia 
or other online resources. 

References  

Hirschman L., Gaizauskas R., “Natural language  
question answering: the view from here”, Natural  
Language Engineering, v.7 n.4, p.275-300,  De-
cember, 2001.   

                                                           
1 https://code.google.com/p/hindiqset/ 

Ittycheriah  et  al.,  “IBM’s  Statistical  Question 
Answering System”, In Proceedings of the Ninth 
Text Retrieval Conference (TREC-9), 2000. Ro-
berts, K., & Hickl, A, “Scaling Answer Type De-
tection to Large Hierarchies”, In Proceedings of 
LREC, May 2008. 

Lally A., Prager J. M., McCord M. C., Boguraev B. 
K., S. Patwardhan, Fan J., Fodor P., and Chu-
Carroll J., “Question analysis: How Watson reads 
a clue”, IBM J. Res. Dev., vol. 56, no. 3/4, Paper 
2, pp. 2:1–2:14, May/Jul. 2012. 

Maksim Tkachenko, Andrey Simanovsky, “Named 
Entity Recognition: Exploring Features”. In Pro-
ceedings of KONVENS 2012, Vienna, September 
20, 2012.  

Praveen Kumar, Shrikant Kashyap and Ankush Mit-
tal, "A   Query   Answering System for   E-
Learning Hindi Documents", South Asian Lan-
guage  Review Vol. XIII, Nos. 1&2, January-
June,2003. 

Ramos J., “Using TF-IDF to determine word relev-
ance in document queries”. In Proceedings of the 
First Instructional Conference on Machine Learn-
ing, December 2003. 

Roberts K. and Hickl A, “Scaling Answer Type De-
tection to Large Hierarchies”, In Proceedings of 
LREC, May 2008. 

Sahu S., Vasnik N., and Roy D., “Prashnottar: A 
Hindi Question Answering System”, International 
Journal of Computer Science and Technology, Vol 
4, pp. 149-158, 2012.   

Satoshi Sekine and Ralph Grishman, “Hindi-English 
cross-lingual question-answering system”, ACM 
Transactions on Asian Language Information  
Processing (TALIP), v.2 n.3, p.181-192, Septem-
ber 2003. 

Shachi Dave, Pushpak Bhattacharya & Dietrich Kla-
kowya, “Knowledge Extraction from Hindi Text”, 
Journal of Institution of Electronic and telecom-
munication engineers, 18(4), 2001.  

Vishal G. and Jaspreet K., “Comparative Analysis of 
Question Answering System in Indian Languages”, 
International Journal of Advanced Research in 
Computer Science and Software Engineering” 3(7) 
pp.584-592, July 2013. 

Wang X. and McCallum A., “A note on topical n-
grams”, Massachusetts University Amherst Dept 
of Computer Science, 2005. 

367


