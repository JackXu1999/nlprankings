



















































Compositional Representation of Morphologically-Rich Input for Neural Machine Translation


Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 305–311
Melbourne, Australia, July 15 - 20, 2018. c©2018 Association for Computational Linguistics

305

Compositional Representation of Morphologically-Rich Input
for Neural Machine Translation

Duygu Ataman
FBK, Trento, Italy

University of Trento, Italy
ataman@fbk.eu

Marcello Federico
MMT Srl, Trento, Italy

FBK, Trento, Italy
federico@fbk.eu

Abstract

Neural machine translation (NMT) mod-
els are typically trained with fixed-size in-
put and output vocabularies, which creates
an important bottleneck on their accuracy
and generalization capability. As a solu-
tion, various studies proposed segmenting
words into sub-word units and performing
translation at the sub-lexical level. How-
ever, statistical word segmentation meth-
ods have recently shown to be prone to
morphological errors, which can lead to
inaccurate translations. In this paper, we
propose to overcome this problem by re-
placing the source-language embedding
layer of NMT with a bi-directional recur-
rent neural network that generates compo-
sitional representations of the input at any
desired level of granularity. We test our
approach in a low-resource setting with
five languages from different morpholog-
ical typologies, and under different com-
position assumptions. By training NMT to
compose word representations from char-
acter trigrams, our approach consistently
outperforms (from 1.71 to 2.48 BLEU
points) NMT learning embeddings of sta-
tistically generated sub-word units.

1 Introduction

An important problem in neural machine trans-
lation (NMT) is translating infrequent or unseen
words. The reasons are twofold: the necessity of
observing many examples of a word until its in-
put representation (embedding) becomes reliable,
and the computational requirement of limiting the
input and output vocabularies to few tens of thou-
sands of words. These requirements eventually
lead to coverage issues when dealing with low-

resource and/or morphologically-rich languages,
due to their high lexical sparseness. To cope
with this well-known problem, several approaches
have been proposed redefining the model vocabu-
lary in terms of interior orthographic units com-
pounding the words, ranging from character n-
grams (Ling et al., 2015b; Costa-jussà and Fonol-
losa, 2016; Lee et al., 2017; Luong and Manning,
2016) to statistically-learned sub-word units (Sen-
nrich et al., 2016; Wu et al., 2016; Ataman et al.,
2017). While the former provide an ideal open
vocabulary solution, they mostly failed to achieve
competitive results. This might be related to the
semantic ambiguity caused by solely relying on
input representations based on character n-grams
which are generally learned by disregarding any
morphological information. In fact, the second
approach is now prominent and has established a
pre-processing step for constructing a vocabulary
of sub-word units before training the NMT model.
However, several studies have shown that seg-
menting words into sub-word units without pre-
serving morpheme boundaries can lead to loss of
semantic and syntactic information and, thus, in-
accurate translations (Niehues et al., 2016; Ata-
man et al., 2017; Pinnis et al., 2017; Huck et al.,
2017; Tamchyna et al., 2017).

In this paper, we propose to improve the quality
of input (source language) representations of rare
words in NMT by augmenting its embedding layer
with a bi-directional recurrent neural network (bi-
RNN), which can learn compositional input repre-
sentations at different levels of granularity. Com-
positional word embeddings have recently been
applied in language modeling and obtained suc-
cessful results (Vania and Lopez, 2017). The ap-
parent advantage of our approach is that by feed-
ing NMT with simple character n-grams, our bi-
RNN can potentially learn the morphology neces-
sary to create word-level representations of the in-



306

put language directly at training time, thus, avoid-
ing the burden of a separate and sub-optimal word
segmentation step. We compare our approach
against conventional embedding-based represen-
tations learned from statistical word segmenta-
tion in a public evaluation benchmark, which pro-
vides low-resource training conditions by pair-
ing English with five morphologically-rich lan-
guages: Arabic, Czech, German, Italian and Turk-
ish, where each language represents a distinct
morphological typology and language family. The
experimental results show that our compositional
input representations lead to significantly and con-
sistently better translation quality in all language
directions.

2 Neural Machine Translation

In this paper, we use the NMT model of Bahdanau
et al. (2014). The model essentially estimates the
conditional probability of translating a source se-
quence x = (x1, x2, . . . xm) into a target sequence
y = (y1, y2, . . . yl), using the decomposition

p(y|x) =
l∏

i=1

p(yj |yi−1, .., y0, xm−1, .., x1) (1)

The model is trained by maximizing the log-
likelihood of a parallel training set via stochastic
gradient descent (Bottou, 2010) and the backprop-
agation through time (Werbos, 1990) algorithms.

The inputs of the network are one-hot vectors,
which are binary vectors with a single bit set to
1 to identify a specific word in the vocabulary.
Each one-hot vector is then mapped to an embed-
ding, a distributed representation of the word in
a lower dimension but a more dense continuous
space. From this input, a representation of the
whole input sequence is learned using a bi-RNN,
the encoder, which maps x into m dense sentence
vectors corresponding to its hidden states. Next,
another RNN, the decoder, predicts each target to-
ken yi by sampling from a distribution computed
from the previous target token yi−1, the previous
decoder hidden state, and the context vector. The
latter is a linear combination of the encoder hidden
states, whose weights are dynamically computed
by a feed-forward neural network called attention
model (Bahdanau et al., 2014). The probability of
generating each target word yj is normalized via a
softmax function.

Both the source and target vocabulary sizes play
an important role in terms of defining the complex-

ity of the model. In a standard architecture, like
ours, the source and target embedding matrices ac-
tually account for the vast majority of the network
parameters. The vocabulary size also plays an
important role when translating from and to low-
resource and morphologically-rich languages, due
to the sparseness of the lexical distribution. There-
fore, a conventional approach has now become
to compose both the source and target vocabular-
ies of sub-word units generated through statistical
segmentation methods (Sennrich et al., 2016; Wu
et al., 2016; Ataman et al., 2017), and perform-
ing NMT by directly learning embeddings of sub-
word units. A popular one of these is the Byte-Pair
Encoding (BPE) method (Gage, 1994; Sennrich
et al., 2016), which finds the optimal description
of a corpus vocabulary by iteratively merging the
most frequent character sequences. A more recent
approach is the Linguistically-Motivated Vocabu-
lary Reduction (LMVR) method (Ataman et al.,
2017), which similarly generates a new vocabu-
lary by segmenting words into sub-lexical units
based on their likeliness of being morphemes and
their morphological categories. A drawback of
these methods is that, as pre-processing steps to
NMT, they are not optimized for the translation
task. Moreover, they can suffer from morphologi-
cal errors at different levels, which can lead to loss
of semantic or syntactic information.

3 Learning Compositional Input
Representations via bi-RNNs

In this paper, we propose to perform NMT
from input representations learned by composing
smaller symbols, such as character n-grams (Ling
et al., 2015a), that can easily fit in the model vo-
cabulary. This composition is essentially a func-
tion which can establish a mapping between com-
binations of ortographic units and lexical meaning,
that is learned using the bilingual context so that it
can produce representations that are optimized for
machine translation.

In our model (Figure 1), the one-hot vectors, af-
ter being fed into the embedding layer, are pro-
cessed by an additional composition layer, which
computes the final input representations passed to
the encoder to generate translations. For learn-
ing the composition function, we employ a bi-
RNN. Hence, by encoding each interior unit in-
side the word, we hope to capture important cues
about their functional role, i.e. semantic or syn-



307

Figure 1: Translation of
the Italian sentence tor-
nai a casa (I came home)
with a word-level repre-
sentation composed from
character trigrams.

tactic contribution to the word. We implement
the network using gated recurrent units (GRUs)
(Cho et al., 2014), which have shown compara-
ble performance to long-short-term-memory units
(Hochreiter and Schmidhuber, 1997), whereas
they provide much faster computation. As a min-
imal set of input symbols required to cope with
contextual ambiguities, we opt to use intersecting
sequences of character trigrams, as recently sug-
gested by Vania and Lopez (2017).

Given a bi-RNN with a forward (f ) and back-
ward (b) layer, the input representation w of a to-
ken of t characters is computed from the hidden
states hft and h

0
b , i.e. the final outputs of the for-

ward and backward RNNs, as follows:

w = Wfh
t
f +Wbh

0
b + b (2)

where Wf and Wb are weight matrices asso-
ciated to each RNN and b is a bias vector (Ling
et al., 2015a). These parameters are jointly learned
together with the internal parameters of the GRUs
and the input token embedding matrix while train-
ing the NMT model. For an input of m tokens, our
implementation increases the computational com-
plexity of the network by O(Ktmaxm), where K
is the bi-RNN cost and tmax is the maximum num-
ber of symbols per word. However, since compu-
tation of each input representation is independent,
a parallelised implementation could cut the over-
head down to O(Ktmax).

4 Experiments

We test our approach along with statistical word
segmentation based open vocabulary NMT meth-
ods in an evaluation benchmark simulating a low-
resource translation setting pairing English (En)
with five languages from different language fam-
ilies and morphological typologies: Arabic (Ar),
Czech (Cs), German (De), Italian (It) and Turk-

ish (TR). The characteristics of each language are
given in Table 1, whereas Table 2 presents the sta-
tistical properties of the training data. We train our
NMT models using the TED Talks corpora (Cet-
tolo et al., 2012) and test them on the official data
sets of IWSLT1 (Mauro et al., 2017).

Language Morphological Morphological
Typology Complexity

Turkish Agglutinative High
Arabic Templatic High
Czech Fusional, High

Agglutinative
German Fusional Medium
Italian Fusional Low

Table 1: The languages evaluated in our study and
their morphological characteristics.

Language # tokens # types
Pair Src Tgt Src Tgt

Tr - En 2,7M 2,0M 171K 53K
Ar - En 3,9M 4,9M 220K 120K
Cs - En 2,0M 2,3M 118K 50K
De - En 4,0M 4,3M 144K 69K
It - En 3,5M 3,8M 95K 63K

Table 2: Sizes of the training sets and vocabularies
in the TED Talks benchmark. Development and
test sets are on average 50K to 100K tokens. (M:
Million, K: Thousand.)

The simple NMT model constitutes the baseline
in our study and performs translation directly at
the level of sub-word units, which can be of four
different types: characters, character trigrams,
BPE sub-word units, and LMVR sub-word units.

1The International Workshop on Spoken Language Trans-
lation with shared tasks organized between 2003-2017.



308

The compositional model, on the other hand, per-
forms NMT with input representations composed
from sub-lexical vocabulary units. In our study,
we evaluate representations composed from char-
acter trigrams, BPE, and LMVR units. In order
to choose the segmentation method to apply on
the English side (the output of NMT decoder), we
compare BPE and LMVR sub-word units by car-
rying out an evaluation on the official data sets
of Morpho Challenge 20102(Kurimo et al., 2010).
The results of this evaluation, as given in Table
3, suggest that LMVR seems to provide a seg-
mentation that is more consistent with morpheme
boundaries, which motivates us to use sub-word
tokens generated by LMVR for the target side.
This choice aids us in evaluating the morpholog-
ical knowledge contained in input representations
in terms of the translation accuracy in NMT.

The compositional bi-RNN layer is imple-
mented in Theano (Team et al., 2016) and inte-
grated into the Nematus NMT toolkit (Sennrich
et al., 2017). In our experiments, we use a compo-
sitional bi-RNN with 256 hidden units, an NMT
model with a one-layer bi-directional GRU en-
coder and one-layer GRU decoder of 512 hidden
units, and an embedding dimension of 256 for
both models. We use a highly restricted dictio-
nary size of 30,000 for both source and target lan-
guages, and train the segmentation models (BPE
and LMVR) to generate sub-word vocabularies of
the same size. We train the NMT models using
the Adagrad (Duchi et al., 2011) optimizer with a
mini-batch size of 50, a learning rate of 0.01, and a
dropout rate of 0.1 (in all layers and embeddings).
In order to prevent over-fitting, we stop training if
the perplexity on the validation does not decrease
for 5 epochs, and use the best model to translate
the test set. The model outputs are evaluated using
the (case-sensitive) BLEU (Papineni et al., 2002)
metric and the Multeval (Clark et al., 2011) sig-
nificance test.

2Shared Task on Unsupervised Morphological Analysis,
http://morpho.aalto.fi/events/morphochallenge.

Method Precision Recall F1 Score
BPE 52.87 24.44 33.43

LMVR 70.22 55.66 62.10

Table 3: The performance of different segmenta-
tion models trained on the English portion of our
benchmark in the Morpho Challenge shared task.

5 Results

The performance of NMT models in translating
each language using different vocabulary units and
encoder input representations can be seen in Ta-
ble 4. With the simple model, LMVR based units
achieve the best accuracy in translating all lan-
guages, with improvements over BPE by 0.85 to
1.09 BLEU points in languages with high morpho-
logical complexity (Arabic, Czech and Turkish)
and 0.32 to 0.53 BLEU points in languages with
low to medium complexity (Italian and German).
This confirms our previous results in (Ataman and
Federico, 2018). Moreover, simple models using
character trigrams as vocabulary units reach much
higher translation accuracy compared to models
using characters, indicating their superior perfor-
mance in handling contextual ambiguity. In the
Italian to English translation direction, the per-
formance of simple models using character tri-
grams and BPE sub-word units as input represen-
tations are almost comparable, showing that char-
acter trigrams can even be sufficient as the stand-
alone vocabulary units in languages with low lex-
ical sparseness. These findings suggest that each
type of sub-word unit used in the simple model is
specifically convenient for a given morphological
typology.

Using our compositional model improves the
quality of input representations for each type of
vocabulary unit, nevertheless, the best perfor-
mance is obtained by using character trigrams
as input symbols and words as input representa-
tions. The higher quality of these input repre-
sentations compared to those obtained from sub-
word units generated with LMVR suggest that our
compositional model can learn morphology better
than LMVR, which was found to provide compa-
rable performance to morphological analyzers in
Turkish to English NMT (Ataman et al., 2017).
Moreover, sample outputs from both models show
that the compositional model is also able to bet-
ter capture syntactic information of input sen-
tences. Figure 5 illustrates two example transla-
tions from Italian and Turkish. In Italian, the sim-
ple model fails to understand the common sub-
ject of different verbs in the sentence due to the
repetition of the same inflective suffix after seg-
mentation. In Turkish, the genitive case ”yer-
lerin fotoğraflarının” (the photographs of places)
and the complex predicate ”birleştirilmesiyle mey-
dana geldi” (is composed of ) are both incorrectly



309

Model Vocabulary Input BLEU
Units Representations Tr-En Ar-En Cs-En De-En It-En

Simple Characters Characters 12.29 8.95 13.42 21.32 22.88
Char Trigrams Char Trigrams 16.13 11.91 20.87 25.01 26.68

Subwords (BPE) Subwords (BPE) 16.79 11.14 21.99 26.61 27.02
Subwords (LMVR) Subwords (LMVR) 17.82 12.23 22.84 27.18 27.34

Composi- Char Trigrams Subwords (BPE) 15.40 11.50 21.67 27.05 27.80
tional Char Trigrams Subwords (LMVR) 16.63 13.29 23.07 26.86 26.84

Char Trigrams Words 19.53 14.22 25.16 29.09 29.82
Subwords (BPE) Words 12.64 11.51 23.13 27.10 27.96

Subwords (LMVR) Words 18.90 13.55 24.31 28.07 28.83

Table 4: Experiment results. Best scores for each translation direction are in bold font. All improvements
over the baseline (simple model with BPE) are statistically significant (p-value < 0.05).

Input e comunque, em@@ ig@@ riamo , circol@@ iamo e mescol@@ iamo cosı̀ tanto che
(Simple Model) non esiste più l’ isolamento necessario affinché avvenga un’ evoluzione .
NMT Output and anyway , we repair, and we mix so much that

(Simple Model) there ’s no longer the isolation that we need to happen to make an evolution .
Input e comunque, emigriamo, circoliamo e mescoliamo cosı̀ tanto che

(Compositional Model) non esiste più l’ isolamento necessario affinché avvenga un’ evoluzione.
NMT Output and anyway , we migrate , circle and mix so much that

(Compositional Model) there ’s no longer the isolation necessary to become evolutionary .
Reference and by the way , we immigrate and circulate and intermix so much that

you can ’t any longer have the isolation that is necessary for evolution to take place .

Input ama aslında bu resim tamamen , farklı yerlerin fotoğraf@@ larının
(Simple Model) birleştir@@ il@@ mesiyle meydana geldi .
NMT Output but in fact , this picture came up with a completely

(Simple Model) different place of photographs .
Input ama aslında bu resim tamamen , farklı yerlerin fotoğraflarının

(Compositional Model) birleştirilmesiyle meydana geldi .
NMT Output but in fact , this picture came from collecting pictures of

(Compositional Model) different places .
Reference but this image is actually entirely composed of photographs from different locations .

Table 5: Example translations with different approaches in Italian (above) and Turkish (below).

translated by the simple model. On the other
hand, the compositional model is able to cap-
ture the correct sentence semantics and syntax in
either case. These findings suggest that main-
taining translation at the lexical level apparently
aids the attention mechanism and provides more
semantically and syntactically consistent transla-
tions. The overall improvements obtained with
this model over the best performing simple model
are 1.99 BLEU points in Arabic, 2.32 BLEU
points in Czech, 1.91 BLEU points in German,
2.48 BLEU points in Italian and 1.71 BLEU points
in Turkish to English translation directions. As ev-
ident from the significant and consistent improve-
ments across all languages, our approach provides
a more promising and generic solution to the data
sparseness problem in NMT.

6 Conclusion

In this paper, we addressed the problem of trans-
lating infrequent words in NMT and proposed to
solve it by replacing the conventional sub-word
embeddings with input representations composi-
tionally learned from character n-grams using a bi-
RNN. Our approach showed significant and con-
sistent improvements over a variety of languages,
making it a competitive solution for NMT of low-
resource and morphologically-rich languages. In
the future, we plan to optimize our implementa-
tion and to test its scalability on larger data sets.

Acknowledgments

The authors would like to thank NVIDIA for their
computational support that aided this research.



310

References
Duygu Ataman and Marcello Federico. 2018. An Eval-

uation of Two Vocabulary Reduction Methods for
Neural Machine Translation. In Proceedings of the
13th Conference of The Association for Machine
Translation in the Americas, pages 97–110.

Duygu Ataman, Matteo Negri, Marco Turchi, and Mar-
cello Federico. 2017. Linguistically Motivated Vo-
cabulary Reduction for Neural Machine Translation
from Turkish to English. In The Prague Bulletin of
Mathematical Linguistics, volume 108, pages 331–
342.

Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben-
gio. 2014. Neural Machine Translation by Jointly
Learning to Align and Translate. In arXiv preprint
arXiv:1409.0473.

Léon Bottou. 2010. Large-Scale Machine Learning
with Stochastic Gradient Descent. In Proceedings
of 19th International Conference on Computational
Statistics (COMPSTAT), pages 177–186. Springer.

Mauro Cettolo, Christian Girardi, and Marcello Fed-
erico. 2012. Wit3: Web Inventory of Transcribed
and Translated Talks. In Proceedings of the 16th
Conference of the European Association for Ma-
chine Translation (EAMT), pages 261–268.

Kyunghyun Cho, Bart van Merrienboer, Dzmitry Bah-
danau, and Yoshua Bengio. 2014. On the properties
of neural machine translation: Encoder–decoder ap-
proaches. In Proceedings of 8th Workshop on Syn-
tax, Semantics and Structure in Statistical Transla-
tion (SSST), pages 103–111.

Jonathan H. Clark, Chris Dyer, Alon Lavie, and
Noah A. Smith. 2011. Better Hypothesis Testing for
Statistical Machine Translation: Controlling for Op-
timizer Instability. In Proceedings of the 49th An-
nual Meeting of the Association for Computational
Linguistics (ACL), pages 176–181.

Marta R Costa-jussà and José AR Fonollosa. 2016.
Character-based neural machine translation. In Pro-
ceedings of the 54th Annual Meeting of the Associa-
tion for Computational Linguistics (ACL), volume 2,
pages 357–361.

John Duchi, Elad Hazan, and Yoram Singer. 2011.
Adaptive Subgradient Methods for Online Learning
and Stochastic Optimization. In Journal of Machine
Learning Research, volume 12, pages 2121–2159.

Philip Gage. 1994. A New Algorithm for Data Com-
pression. In The C Users Journal, volume 12, pages
23–38.

Sepp Hochreiter and Jürgen Schmidhuber. 1997. Long
short-term memory. In Neural computation, vol-
ume 9, pages 1735–1780. MIT Press.

Matthias Huck, Simon Riess, and Alexander Fraser.
2017. Target-Side Word Segmentation Strategies

for Neural Machine Translation. In Proceedings of
the 2nd Conference on Machine Translation (WMT),
pages 56–67.

Mikko Kurimo, Sami Virpioja, Ville Turunen, and
Krista Lagus. 2010. Morpho challenge competition
2005–2010: evaluations and results. In Proceed-
ings of the 11th Meeting of the ACL Special Interest
Group on Computational Morphology and Phonol-
ogy, pages 87–95. Association for Computational
Linguistics.

Jason Lee, Kyunghyun Cho, and Thomas Hofmann.
2017. Fully character-Level Neural Machine Trans-
lation without Explicit Segmentation. In Transac-
tions of the Association for Computational Linguis-
tics (TACL), volume 5, pages 365–378.

Wang Ling, Chris Dyer, Alan W Black, Isabel Tran-
coso, Ramon Fermandez, Silvio Amir, Luis Marujo,
and Tiago Luis. 2015a. Finding function in form:
Compositional character models for open vocabu-
lary word representation. In Proceedings of the 2015
Conference on Empirical Methods in Natural Lan-
guage Processing (EMNLP), pages 1520–1530.

Wang Ling, Isabel Trancoso, Chris Dyer, and Alan W
Black. 2015b. Character-based Neural Machine
Translation. In arXiv preprint arXiv:1511.04586.

Minh-Thang Luong and Christopher D Manning. 2016.
Achieving Open Vocabulary Neural Machine Trans-
lation with Hybrid Word-Character Models. In Pro-
ceedings of the 54th Annual Meeting of the Asso-
ciation for Computational Linguistics (ACL), pages
1054–1063.

Cettolo Mauro, Federico Marcello, Bentivogli Luisa,
Niehues Jan, Stüker Sebastian, Sudoh Katsuitho,
Yoshino Koichiro, and Federmann Christian. 2017.
Overview of the iwslt 2017 evaluation campaign. In
International Workshop on Spoken Language Trans-
lation, pages 2–14.

Jan Niehues, Eunah Cho, Thanh-Le Ha, and Alex
Waibel. 2016. Pre-translation for Neural Machine
Translation. In Proceedings of The 26th Inter-
national Conference on Computational Linguistics
(COLING), pages 1828–1836.

Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: a Method for Automatic
Evaluation of Machine Translation. In Proceedings
of the 40th Annual Meeting of the Association for
Computational Linguistics (ACL), pages 311–318.

Mārcis Pinnis, Rihards Krišlauks, Daiga Deksne, and
Toms Miks. 2017. Neural Machine Translation
for Morphologically Rich Languages with Improved
Subword Units and Synthetic Data. In Proceedings
of the International Conference on Text, Speech, and
Dialogue (TSD), pages 237–245.

Rico Sennrich, Orhan Firat, Kyunghyun Cho, Alexan-
dra Birch, Barry Haddow, Julian Hitschler, Marcin



311

Junczys-Dowmunt, Samuel Läubli, Antonio Vale-
rio Miceli Barone, Jozef Mokry, et al. 2017. Ne-
matus: a toolkit for Neural Machine Translation. In
Proceedings of the 15th Annual Meeting of the Euro-
pean Chapter of the Association for Computational
Linguistics (EACL), pages 65–68.

Rico Sennrich, Barry Haddow, and Alexandra Birch.
2016. Neural Machine Translation of Rare Words
with Subword Units. In Proceedings of the 54th An-
nual Meeting of the Association for Computational
Linguistics (ACL), pages 1715–1725.

Aleš Tamchyna, Marion Weller-Di Marco, and Alexan-
der Fraser. 2017. Modeling Target-Side Inflection
in Neural Machine Translation. In Proceedings of
the 2nd Conference on Machine Translation (WMT),
pages 32–42.

The Theano Development Team, Rami Al-Rfou,
Guillaume Alain, Amjad Almahairi, Christof
Angermueller, Dzmitry Bahdanau, Nicolas Ballas,
Frédéric Bastien, Justin Bayer, Anatoly Belikov,
et al. 2016. Theano: A python framework for fast
computation of mathematical expressions. In arXiv
preprint arXiv:1605.02688.

Clara Vania and Adam Lopez. 2017. From characters
to words to in between: Do we capture morphology?
In Proceedings of the 55th Annual Meeting of the As-
sociation for Computational Linguistics (ACL), vol-
ume 1, pages 2016–2027.

Paul J Werbos. 1990. Backpropagation Through Time:
What it does and How to do it. In Proceedings of
the Institute of Electrical and Electronics Engineers
(IEEE), volume 78, pages 1550–1560.

Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V
Le, Mohammad Norouzi, Wolfgang Macherey,
Maxim Krikun, Yuan Cao, Qin Gao, Klaus
Macherey, et al. 2016. Googles neural machine
translation system: Bridging the gap between hu-
man and machine translation. In arXiv preprint
arXiv:1609.08144.


