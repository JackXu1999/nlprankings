



















































Multi-Granularity Hierarchical Attention Fusion Networks for Reading Comprehension and Question Answering


Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers), pages 1705–1714
Melbourne, Australia, July 15 - 20, 2018. c©2018 Association for Computational Linguistics

1705

Multi-Granularity Hierarchical Attention Fusion Networks
for Reading Comprehension and Question Answering

Wei Wang∗, Ming Yan∗, Chen Wu∗
Alibaba Group, 969 West Wenyi Road, Hangzhou 311121, China
{hebian.ww,ym119608,wuchen.wc}@alibaba-inc.com

Abstract

This paper describes a novel hierarchical
attention network for reading comprehen-
sion style question answering, which aims
to answer questions for a given narrative
paragraph. In the proposed method, atten-
tion and fusion are conducted horizontally
and vertically across layers at different
levels of granularity between question and
paragraph. Specifically, it first encode the
question and paragraph with fine-grained
language embeddings, to better capture
the respective representations at semantic
level. Then it proposes a multi-granularity
fusion approach to fully fuse information
from both global and attended representa-
tions. Finally, it introduces a hierarchical
attention network to focuses on the answer
span progressively with multi-level soft-
alignment. Extensive experiments on the
large-scale SQuAD and TriviaQA datasets
validate the effectiveness of the proposed
method. At the time of writing the pa-
per (Jan. 12th 2018), our model achieves
the first position on the SQuAD leader-
board for both single and ensemble mod-
els. We also achieves state-of-the-art re-
sults on TriviaQA, AddSent and AddOne-
Sent datasets.

1 Introduction

As a brand new field in question answering com-
munity, reading comprehension is one of the key
problems in artificial intelligence, which aims to
read and comprehend a given text, and then an-
swer questions based on it. This task is chal-
lenging which requires a comprehensive under-
standing of natural languages and the ability to
do further inference and reasoning. Restricted

by the limited volume of the annotated dataset,
early studies mainly rely on a pipeline of NLP
models to complete this task, such as seman-
tic parsing and linguistic annotation (Das et al.,
2014). Not until the release of large-scale cloze-
style dataset, such as Children’s Book Test (Hill
et al., 2015) and CNN/Daily Mail (Hermann et al.,
2015), some preliminary end-to-end deep learning
methods have begun to bloom and achieve supe-
rior results in reading comprehension task (Her-
mann et al., 2015; Chen et al., 2016; Cui et al.,
2016).

However, these cloze-style datasets still have
their limitations, where the goal is to predict the
single missing word (often a named entity) in a
passage. It requires less reasoning than previously
thought and no need to comprehend the whole pas-
sage (Chen et al., 2016). Therefore, Stanford pub-
lish a new large-scale dataset SQuAD (Rajpurkar
et al., 2016), in which all the question and an-
swers are manually created through crowdsourc-
ing. Different from cloze-style reading compre-
hension dataset, SQuAD constrains answers to all
possible text spans within the reference passage,
which requires more logical reasoning and content
understanding.

Benefiting from the availability of SQuAD
benchmark dataset, rapid progress has been made
these years. The work (Wang and Jiang, 2016) and
(Seo et al., 2016) are among the first to investigate
into this dataset, where Wang and Jiang propose
an end-to-end architecture based on match-LSTM
and pointer networks (Wang and Jiang, 2016),
and Seo et al. introduce the bi-directional atten-
tion flow network which captures the question-
document context at different levels of granular-
ity (Seo et al., 2016). Chen et al. devise a sim-
ple and effective document reader, by introducing
a bilinear match function and a few manual fea-
tures (Chen et al., 2017a). Wang et al. propose



1706

a gated attention-based recurrent network where
self-match attention mechanism is first incorpo-
rated (Wang et al., 2017). In (Liu et al., 2017b)
and (Shen et al., 2017), the multi-turn memory net-
works are designed to simulate multi-step reason-
ing in machine reading comprehension.

The idea of our approach derives from the nor-
mal human reading pattern. First, people scan
through the whole passage to catch a glimpse of
the main body of the passage. Then with the ques-
tion in mind, people make connection between
passage and question, and understand the main in-
tent of the question related with the passage theme.
A rough answer span is then located from the pas-
sage and the attention can be focused on to the lo-
cated context. Finally, to prevent from forgetting
the question, people come back to the question and
select a best answer according to the previously lo-
cated answer span.

Inspired by this, we propose a hierarchical at-
tention network which can gradually focus the at-
tention on the right part of the answer boundary,
while capturing the relation between the question
and passage at different levels of granularity, as il-
lustrated in Figure 1. Our model mainly consists
of three joint layers: 1) encoder layer where pre-
trained language models and recurrent neural net-
works are used to build representation for ques-
tions and passages separately; 2) attention layer
in which hierarchical attention networks are de-
signed to capture the relation between question
and passage at different levels of granularity; 3)
match layer where refined question and passage
are matched under a pointer-network (Vinyals
et al., 2015) answer boundary predictor.

In encoder layer, to better represent the ques-
tions and passages in multiple aspects, we com-
bine two different embeddings to give the funda-
mental word representations. In addition to the
typical glove word embeddings, we also utilize
the ELMo embeddings (Peters et al., 2018) de-
rived from a pre-trained language model, which
shows superior performance in a wide range of
NLP problems. Different from the original fusion
way for intermediate layer representations, we de-
sign a representation-aware fusion method to com-
pute the output ELMo embeddings and the context
information is also incorporated by further passing
through a bi-directional LSTM network.

The key in machine reading comprehension so-
lution lies in how to incorporate the question con-

text into the paragraph, in which attention mech-
anism is most widely used. Recently, many dif-
ferent attention functions and types have been de-
signed (Xiong et al., 2016; Seo et al., 2016; Wang
et al., 2017), which aims at properly aligning
the question and passage. In our attention layer,
we propose a hierarchical attention network by
leveraging both the co-attention and self-attention
mechanism, to gradually focus our attention on
the best answer span. Different from the previous
attention-based methods, we constantly comple-
ment the aligned representations with global infor-
mation from the previous layer, and an additional
fusion layer is used to further refine the represen-
tations. In this way, our model can make some
minor adjustment so that the attention will always
be on the right place.

Based on the refined question and passage rep-
resentation, a bilinear match layer is finally used
to identify the best answer span with respect to
the question. Following the work of (Wang and
Jiang, 2016), we predict the start and end bound-
ary within a pointer-network output layer.

The proposed method achieves state-of-the-art
results against strong baselines. Our single model
achieves 79.2% EM and 86.6% F1 score on the
hidden test set, while the ensemble model further
boosts the performance to 82.4% EM and 88.6%
F1 score. At the time of writing the paper (Jan.
12th 2018), our model SLQA+ (Semantic Learn-
ing for Question Answering) achieves the first po-
sition on the SQuAD leaderboard 1 for both single
and ensemble models. Besides, we are also among
the first to surpass human EM performance on this
golden benchmark dataset.

2 Related Work

2.1 Machine Reading Comprehension
Traditional reading comprehension style question
answering systems rely on a pipeline of NLP mod-
els, which make heavy use of linguistic annota-
tion, structured world knowledge, semantic pars-
ing and similar NLP pipeline outputs (Hermann
et al., 2015). Recently, the rapid progress of ma-
chine reading comprehension has largely bene-
fited from the availability of large-scale bench-
mark datasets and it is possible to train large
end-to-end neural network models. Among them,
CNN/Daily Mail (Hermann et al., 2015) and Chil-
dren’s Book Test (Hill et al., 2015) are the first

1 https://rajpurkar.github.io/SQuAD-explorer/



1707

large-scale datasets for reading comprehension
task. However, these datasets are in cloze-style,
in which the goal is to predict the missing word
(often a named entity) in a passage. Moreover,
Chen at al. have also shown that these cloze-
style datasets requires less reasoning than previ-
ously thought (Chen et al., 2016). Different from
the previous datasets, the SQuAD provides a more
challenging benchmark dataset, where the goal is
to extract an arbitrary answer span from the origi-
nal passage.

2.2 Attention-based Neural Networks

The key in MRC task lies in how to incorpo-
rate the question context into the paragraph, in
which attention mechanism is most widely used.
In spite of a variety of model structures and atten-
tion types (Cui et al., 2016; Xiong et al., 2016; Seo
et al., 2016; Wang et al., 2017; Clark and Gardner,
2017), a typical attention-based neural network
model for MRC first encodes the symbolic repre-
sentation of the question and passage in an embed-
ding space, then identify answers with particular
attention functions in that space. In terms of the
question and passage attention or matching strat-
egy, we roughly categorize these attention-based
models into two large groups: one-way attention
and two-way attention.

In one-way attention model, question is first
summarized into a single vector and then directly
matched with the passage. Most of the end-to-
end neural network methods on the cloze-style
datasets are based on this model (Hermann et al.,
2015; Kadlec et al., 2016; Chen et al., 2016; Dhin-
gra et al., 2016). Hermann et al. are the first
to apply the attention-based neural network meth-
ods to MRC task and introduce an attentive reader
and an impatient reader (Hermann et al., 2015),
by leveraging a two layer LSTM network. Chen
et al. (Chen et al., 2016) further design a bilin-
ear attention function based on the attentive reader,
which shows superior performance on CNN/Daily
Mail dataset. However, part of information may
be lost when summarizing the question and a fine-
grained attention on both the question and passage
words should be more reasonable.

Therefore, the two-way attention model un-
folds both the question and passage into respective
word embeddings, and compute the attention in a
two-dimensional matrix. Most of the top-ranking
methods on SQuAD leaderboard are based on this

attention mechanism (Wang et al., 2017; Huang
et al., 2017; Xiong et al., 2017; Liu et al., 2017b,a).
(Cui et al., 2016) and (Xiong et al., 2016) intro-
duce the co-attention mechanism to better couple
the representations of the question and document.
Seo et al. propose a bi-directional attention flow
network to capture the relevance at different lev-
els of granularity (Seo et al., 2016). (Wang et al.,
2017) further introduce the self-attention mecha-
nism to refine the representation by matching the
passage against itself, to better capture the global
passage information. Huang et al. introduce
a fully-aware attention mechanism with a novel
history-of-word concept (Huang et al., 2017).

We propose a hierarchical attention network
by leveraging both co-attention and self-attention
mechanisms in different layers, which can capture
the relevance between the question and passage at
different levels of granularity. Different from the
above methods, we further devise a fusion func-
tion to combine both the aligned representation
and the original representation from the previous
layer within each attention. In this way, the model
can always focus on the right part of the passage,
while keeping the global passage topic in mind.

3 Machine Comprehension Model

3.1 Task Description

Typical machine comprehension systems take an
evidence text and a question as input, and pre-
dict a span within the evidence that answers the
question. Based on this definition, given a pas-
sage and a question, the machine needs to first read
and understand the passage, and then finds the an-
swer to the question. The passage is described
as a sequence of word tokens P =

{
wPt

}n
t=1

and the question is described as Q =
{
wQt

}m
t=1

,
where n is the number of words in the passage,
and m is the number of words in the question. In
general, n � m. The answer can have differ-
ent types depending on the task. In the SQuAD
dataset (Rajpurkar et al., 2016), the answer A is
guaranteed to be a continuous span in the pas-
sage P. The object function for machine read-
ing comprehension is to learn a function f(q,p) =
argmaxa∈A(p) P(a|q, p). The training data is a
set of the question, passage and answer tuples
< Q,P,A >.



1708

Figure 1: Hierarchical Attention Fusion Network.

3.2 Encode-Interaction-Pointer Framework

We will now describe our framework from the bot-
tom up. As show in Figure 1, the proposed frame-
work consists of four typical layers to learn differ-
ent concepts of semantic representations:

• Encoder Layer as a language model, utilizes
contextual cues from surrounding words to re-
fine the embedding of the words. It converts the
passage and question from tokens to semantic
representation;
• Attention Layer attempts to capture relations

between question and passage. Besides the
aligned context, the contextual embeddings are
also merged by a fusion function. Moreover,
the multi-level of this operation forms a ”work-
ing memory”;
• Match Layer employs a bi-linear match func-

tion to compute the relevance between the
question and passage representation on a span
level;
• Output Layer uses a pointer network to search

the answer span of question.
The main contribution of this work is the atten-

tion layer, in order to capture the relationship be-
tween question and passage, a hierarchical strat-
egy is used to progressively make the answer
boundary clear with the refined attention mecha-
nism. A fine-grained fusion function is also in-
troduced to better align the contextual representa-
tions from different levels. The detailed descrip-

tion of the model is provided as follows.

3.3 Hierarchical Attention Fusion Network

Our design is based on a simple but natural in-
tuition: performing fine-grained mechanism re-
quires first to roughly see the potential answer do-
main and then progressively locate the most dis-
criminative parts of the domain.

The overall framework of our Hierarchical At-
tention Fusion Network is shown in Figure 1. It
consists of several parts: a basic co-attention layer
with shallow semantic fusion, a self-attention
layer with deep semantic fusion and a memory-
wise bilinear alignment function. The proposed
network has two distinctive characteristics: (i)
A fine-grained fusion approach to blend atten-
tion vectors for a better understanding of the re-
lationship between question and passage; (ii) A
multi-granularity attention mechanism applied at
the word and sentence-level, enabling it to prop-
erly attend to the most important content when
constructing the question and passage representa-
tion. Experiments conducted on SQuAD and ad-
versarial example datasets (Jia and Liang, 2017)
demonstrate that the proposed framework outper-
form previous methods by a large margin. Details
of different components will be described in the
following sections.

3.4 Language Model & Encoder Layer

Encoder layer of the model transform the dis-
crete word tokens of question and passage to a se-
quence of continuous vector representations. We
use a pre-trained word embedding model and a
char embedding model to lay the foundation for
our model. For the word embedding model, we
adopt the popular glove embeddings (Penning-
ton et al., 2014) which are widely used in deep
learning-based NLP domain. For the char em-
bedding model, the ELMo language model (Pe-
ters et al., 2018) is used due to its superior perfor-
mance in a wide range of NLP tasks. As a result,
we obtain two types of encoding vectors, i.e., word
embeddings

{
eQt

}m
t=1

,
{
ePt

}n
t=1

and char embed-

dings
{
cQt

}m
t=1

,
{
cPt

}n
t=1

.
To further utilize contextual cues from sur-

rounding words to refine the embedding of the
words, we then put a shared Bi-LSTM network
on top of the embeddings provided by the previ-
ous layers to model the temporal interactions be-
tween words. Before feeding into the Bi-LSTM



1709

contextual network, we concat the word embed-
dings and char embeddings for a full understand-
ing of each word. The final output of our encoder
layer is shown as below,

uQt =
[
BiLSTMQ([e

Q
t , c

Q
t ]), c

Q
t

]
(1)

uPt =
[
BiLSTMP([e

P
t , c

P
t ]), c

P
t

]
(2)

where we further concat the output of the con-
textual Bi-LSTM network with the pre-trained
char embeddings for its good performance (Peters
et al., 2018). This can be regarded as a residual
connection between word representations in dif-
ferent levels.

3.5 Hierarchical Attention & Fusion Layer
The attention layer is responsible for linking and
fusing information from the question and passage
representation, which is the most critical in most
MRC tasks. It aims to align the question and pas-
sage so that we can better locate on the most rele-
vant passage span with respect to the question. We
propose a hierarchical attention structure by com-
bining the co-attention and self-attention mecha-
nism in a multi-hop style. Besides, we think that
the original representation and the aligned repre-
sentation via attention can reflect the content se-
mantics in different granularities. Therefore, we
also apply a particular fusion function after each
attention function, so that different levels of se-
mantics can be better incorporated towards a better
understanding.

3.5.1 Co-attention & Fusion
Given the question and passage representation uQt
and uPt , a soft-alignment matrix S has been built to
calculate the shallow semantic similarity between
question and passage as follows:

Sij = Att(u
Q
t , u

P
t ) = ReLU(W

>
linu

Q
t )

> · ReLU(W>linuPt )
(3)

where Wlin is a trainable weight matrix.
This decomposition avoids the quadratic com-

plexity that is trivially parallelizable (Parikh et al.,
2016). Now we use the unnormalized attention
weights Sij to compute the attentions between
question and passage, which is further used to ob-
tain the attended vectors in passage to question and
question to passage direction, respectively.

P2Q Attention signifies which question words
are most relevant to each passage word, given as
below:

αj = softmax(S:j) (4)

where αj represents the attention weights on the
question words.

The aligned passage representation from ques-
tion Q =

{
uQt

}m
t=1

can thus be derived as,

Q̃:t =
∑
j

αtj ·Q:j,∀j ∈ [1, ...,m] (5)

Q2P Attention signifies which passage words
have the closest similarity to one of the question
words and are hence critical for answering the
question.

We utilize the same way to calculate this atten-
tion as in the passage to question attention (P2Q),
except for that in the opposite direction:

βi = softmax(Si:) (6)

P̃k: =
∑
i

βik · Pi:,∀i ∈ [1, ...,n] (7)

where P̃ indicates the weighted sum of the most
important words in the passage with respect to the
question.

With the aligned passage and question represen-
tations Q̃ and P̃ derived, a particular fusion unit
has been designed to combine the original contex-
tual representations and the corresponding atten-
tion vectors for question and passage separately:

P′ = Fuse(P, Q̃) (8)

Q′ = Fuse(Q, P̃) (9)

where Fuse(·, ·) is a typical fusion kernel.
The simplest way of fusion is a concatenation

or addition of the two representations, followed
by some linear or non-linear transformation. Re-
cently, a heuristic matching trick with difference
and element-wise product is found effective in
combining different representations (Mou et al.,
2016; Chen et al., 2017b):

m(P, Q̃) = tanh(Wf [P; Q̃; P ◦ Q̃; P− Q̃] + bf)
(10)

where ◦ denotes the element-wise product, and
Wf , bf are trainable parameters. The output di-
mension is projected back to the same size as the
original representation P or Q via the projected
matrix Wf .

Since we find that the original contextual repre-
sentations are important in reflecting the semantics
at a more global level, we also introduce differ-
ent levels of gating mechanism to incorporate the



1710

projected representations m(·, ·) with the original
contextual representations. As a result, the final
fused representations of passage and question can
be formulated as:

P′ = g(P, Q̃) ·m(P, Q̃)+(1−g(P, Q̃)) ·P (11)

Q′ = g(Q, P̃) ·m(Q, P̃)+(1−g(Q, P̃)) ·Q (12)

where g(·, ·) is a gating function. To capture
the relation between the representations in differ-
ent granularities, we also design a scalar-based,
a vector-based and a matrix-based sigmoid gating
function, which are compared in Section 4.5.

3.5.2 Self-attention & Fusion
Borrowing the idea from wide and deep net-
work (Cheng et al., 2016), manual features have
also been added to combine with the outputs of
previous layer for a more comprehensive repre-
sentation. In our model, these features are con-
catenated with the refined question-aware passage
representation as below:

D = BiLSTM([P′; featman]) (13)

where featman denotes the word-level manual pas-
sage features.

In this layer, we separately consider the se-
mantic representations of question and passage,
and further refine the obtained information from
the co-attention layer. Since fusing informa-
tion among context words allows contextual in-
formation to flow close to the correct answer,
the self-attention layer is used to further align
the question and passage representation against it-
self, so as to keep the global sequence informa-
tion in memory. Benefiting from the advantage
of self-alignment attention in addressing the long-
distance dependence (Wang et al., 2017), we adopt
a self-alignment fusion process in this level. To al-
low for more freedom of the aligning process, we
introduce a bilinear self-alignment attention func-
tion on the passage representation:

L = softmax(D ·Wl ·D>) (14)

D̃ = L ·D (15)

Another fusion function Fuse(·, ·) is again
adopted to combine the question-aware passage
representation D and self-aware representation D̃,
as below:

D′ = Fuse(D, D̃) (16)

Finally, a bidirectional LSTM is used to get the
final contextual passage representation:

D′′ = BiLSTM(D′) (17)

As for question side, since it is generally shorter
in length and could be adequately represented with
less information, we follow the question encoding
method used in (Chen et al., 2017a) and adopt a
linear transformation to encode the question rep-
resentation to a single vector.

First, another contextual bidirectional LSTM
network is applied on top of the fused question
representation: Q′′ = BiLSTM(Q′). Then we ag-
gregate the resulting hidden units into one single
question vector, with a linear self-alignment:

γ = softmax(w>q ·Q′′) (18)

q =
∑
j

γj ·Q′′:j,∀j ∈ [1, ...,m] (19)

where wq is a weight vector to learn, we self-align
the refined question representation to a single vec-
tor according to the question self-attention weight,
which can be further used to compute the match-
ing with the passage words.

3.6 Model & Output Layer
Instead of predicting the start and end positions
based only on D′′, a top-level bilinear match func-
tion is used to capture the semantic relation be-
tween question q and paragraph D′′ in a matching
style, which actually works as a multi-hop match-
ing mechanism.

Different from the co-attention layer that gen-
erates coarse candidate answers and the self-
attention layer that focus the relevant context of
passage to a certain intent of question, the top
model layer uses a bilinear matching function to
capture the interaction between outputs from pre-
vious layers and finally locate on the right answer
span.

The start and end distribution of the passage
words are calculated in a bilinear matching way
as below,

Pstart = softmax(q ·W>s ·D′′) (20)
Pend = softmax(q ·W>e ·D′′) (21)

where Ws and We are trainable matrices of the
bilinear match function.

The output layer is application-specific, in
MRC task, we use pointer networks to predict the



1711

start and end position of the answer, since it re-
quires the model to find the sub-phrase of the pas-
sage to answer the question.

In training process, with cross entropy as met-
ric, the loss for start and end position is the sum
of the negative log probabilities of the true start
and end indices by the predicted distributions, av-
eraged over all examples:

L(θ) = − 1
N

N∑
i

log ps(y
s
i ) + log pe(y

e
i ) (22)

where θ is the set of all trainable weights in the
model, and ps is the probability of start index, pe
is the probability of end index, respectively. ysi and
yei are the true start and end indices.

During prediction, we choose the answer span
with the maximum value of ps · pe under a con-
straint that s ≤ e ≤ s + 15, which is selected via a
dynamic programming algorithm in linear time.

4 Experiments

In this section, we first present the datasets used
for evaluation. Then we compare our end-to-end
Hierarchical Attention Fusion Networks with ex-
isting machine reading models. Finally, we con-
duct experiments to validate the effectiveness of
our proposed components. We evaluate our model
on the task of question answering using recently
released SQuAD and TriviaQA Wikipedia (Joshi
et al., 2017), which have gained a huge attention
over the past year. An adversarial evaluation for
the Stanford Question Answering SQuAD is also
used to demonstrate the robust of our model under
adversarial attacks (Jia and Liang, 2017).

4.1 Dataset

We focus on the SQuAD dataset to train and evalu-
ate our model. SQuAD is a popular machine com-
prehension dataset consisting of 100,000+ ques-
tions created by crowd workers on 536 Wikipedia
articles. Each context is a paragraph from an ar-
ticle and the answer to each question is guaran-
teed to be a span in the context. The answer to
each question is always a span in the context. The
model is given a credit if its answer matches one of
the human chosen answers. Two metrics are used
to evaluate the model performance: Exact Match
(EM) and a softer metric F1 score, which measures
the weighted average of the precision and recall
rate at a character level.

Table 1: The performance of our SLQA model
and competing approaches on SQuAD.

Dev Set Test Set
Single model EM / F1 EM / F1
LR Baseline (Rajpurkar et al., 2016) 40.0 / 51.0 40.4 / 51.0
Match-LSTM (Wang and Jiang, 2016) 64.1 / 73.9 64.7 / 73.7
DrQA (Chen et al., 2017a) - / - 70.7 / 79.4
DCN+ (Xiong et al., 2017) 74.5 / 83.1 75.1 / 83.1
Interactive AoA Reader+ (Cui et al., 2016) - / - 75.8 / 83.8
FusionNet (Huang et al., 2017) - / - 76.0 / 83.9
SAN (Liu et al., 2017b) 76.2 / 84.0 76.8 / 84.4
AttentionReader+ (unpublished) - / - 77.3 / 84.9
BiDAF + Self Attention + ELMo (Peters et al., 2018) - / - 78.6 / 85.8
r-net+ (Wang et al., 2017) - / - 79.9 / 86.5
SLQA+ 80.0 / 87.0 80.4 / 87.0
Ensemble model
FusionNet (Huang et al., 2017) - / - 78.8 / 85.9
DCN+ (Xiong et al., 2017) - / - 78.9 / 86.0
Interactive AoA Reader+ (Cui et al., 2016) - / - 79.0 / 86.4
SAN (Liu et al., 2017b) 78.6 / 85.9 79.6 / 86.5
BiDAF + Self Attention + ELMo (Peters et al., 2018) - / - 81.0 / 87.4
AttentionReader+ (unpublished) - / - 81.8 / 88.2
r-net+ (Wang et al., 2017) - / - 82.6 / 88.5
SLQA+ 82.0 / 88.4 82.4 / 88.6
Human Performance 80.3 / 90.5 82.3 / 91.2

TriviaQA is a newly available machine compre-
hension dataset consisting of over 650K context-
query-answer triples. The contexts are automat-
ically generated from either Wikipedia or Web
search results. The length of contexts in TriviaQA
(average 2895 words) is much more longer than
the one in SQuAD (average 122 words).

4.2 Training Details

We use the AdaMax optimizer, with a mini-batch
size of 32 and initial learning rate of 0.002. A
dropout rate of 0.4 is used for all LSTM layers. To
directly optimize our target against the evaluation
metrics, we further fine-tune the model with some
well-defined strategy. During fine-tuning, Focal
Loss (Lin et al., 2017) and Reinforce Loss which
take F1 score as reward are incorporated with
Cross Entropy Loss. The training process takes
roughly 20 hours on a single Nvidia Tesla M40
GPU. We also train an ensemble model consisting
of 15 training runs with the identical framework
and hyper-parameters. At test time, we choose the
answer with the highest sum of confidence scores
amongst the 15 runs for each question.

4.3 Main Results

The results of our model and competing ap-
proaches on the hidden test set are summarized in
Table 1. The proposed SLQA+ ensemble model
achieves an EM score of 82.4 and F1 score of 88.6,
outperforming all previous approaches, which val-
idates the effectiveness of our hierarchical atten-
tion and fusion network structure.

We also conduct experiments on the adversarial



1712

Table 2: The F1 scores of different models
on AddSent and AddOneSent datasets (S: Single
Model, E: Ensemble).

Model AddSent AddOneSent
Logistic (Rajpurkar et al., 2016) 23.2 30.4
Match-S (Wang and Jiang, 2016) 27.3 39.0
Match-E (Wang and Jiang, 2016) 29.4 41.8
BiDAF-S (Seo et al., 2016) 34.3 45.7
BiDAF-E (Seo et al., 2016) 34.2 46.9
ReasoNet-S (Shen et al., 2017) 39.4 50.3
ReasoNet-E (Shen et al., 2017) 39.4 49.8
Mnemonic-S (Hu et al., 2017) 46.6 56.0
Mnemonic-E (Hu et al., 2017) 46.2 55.3
QANet-S (Yu et al., 2018) 45.2 55.7
FusionNet-E (Huang et al., 2017) 51.4 60.7
SLQA-S (our) 52.1 62.7
SLQA-E (our) 54.8 64.2

SQuAD dataset (Jia and Liang, 2017) to study the
robustness of the proposed model. In the dataset,
one or more sentences are appended to the origi-
nal SQuAD context, aiming to mislead the trained
models. We use exactly the same model as in our
SQuAD dataset, the performance comparison re-
sult is shown in Table 2. It can be seen that the
proposed model can still get superior results than
all the other competing approaches.

4.4 Ablations

In order to evaluate the individual contribution of
each model component, we run an ablation study.
Table 3 shows the performance of our model and
its ablations on SQuAD dev set. The bi-linear
alignment plus fusion between passage and ques-
tion is most critical to the performance on both
metrics which results in a drop of nearly 15%.
The reason may be that in top-level attention layer,
the similar semantics between question and pas-
sage are strong evidence to locate the correct an-
swer span. The ELMo accounts for about 5% of
the performance degradation, which clearly shows
the effectiveness of language model. We conjec-
ture that language model layer efficiently encodes
different types of syntactic and semantic informa-
tion about words-in-context, and improves the task
performance. To evaluate the performance of hier-
archical architecture, we reduce the multi-hop fu-
sion with the standard LSTM network. The result
shows that multi-hop fusion outperforms the stan-
dard LSTM by nearly 5% on both metrics.

4.5 Fusion Functions

In this section, we experimentally demonstrate
how different choices of the fusion kernel impact
the performance of our model. The compared fu-
sion kernels are described as follows:

Simple Concat: a simple concatenation of two

Table 3: Ablation tests of SLQA single model on
the SQuAD dev set.

SLQA single model EM / F1
SLQA+ 80.0 / 87.0
-Manual Features 79.2 / 86.2
-Language Embedding (ELMo) 77.6 / 84.9
-Self Matching 79.5 / 86.4
-Multi-hop 79.1 / 86.1
-Bi-linear Match 65.4 / 72.0
-Fusion (simple concat) 78.8 / 85.8
-Fusion, -Multi-hop 77.5 / 84.8
-Fusion, -Bi-linear Match 63.1 / 69.6

Table 4: Comparison of different fusion kernels
on the SQuAD dev set.

Fusion Kernel EM / F1
Simple Concat 78.8 / 85.8
Add Full Projection (FPU) 79.1 / 86.1
Scalar-based Fusion (SFU) 79.5 / 86.5
Vector-based Fusion (VFU) 80.0 / 87.0
Matrix-based Fusion (MFU) 79.8 / 86.8

channel inputs.
Full Projection: the heuristic matching and

projecting function as in Equ. 10.
Scalar-based Fusion: the gating function is a

trainable scalar parameter (a coarse fusion level):

g(P, Q̃) = gp (23)

where gp is a trainable scalar parameter.
Vector-based Fusion: the gating function con-

tains a weight vector to learn, which acts as a one-
dimensional sigmoid gating,

g(P, Q̃) = σ(w>g ·[P; Q̃; P◦Q̃; P−Q̃]+bg) (24)

where wg is trainable weight vector, bg is trainable
bias, and σ is sigmoid function.

Matrix-based Fusion: the gating function con-
tains a weight matrix to learn, which acts as a two-
dimensional sigmoid gating,

g(P, Q̃) = σ(W>g ·[P; Q̃; P◦Q̃; P−Q̃]+bg) (25)

where Wg is a trainable weight matrix.
The comparison results of different fusion ker-

nels can be found in Table 4. We can see that
different fusion methods contribute differently to
the final performances, and the vector-based fu-
sion method performs best, with a moderate pa-
rameter size.

4.6 Attention Hierarchy and Function
In the proposed model, attention layer is the most
important part of the framework. At the bottom
of Table 5 we show the performances on SQuAD



1713

Table 5: Comparison of different attention styles
on the SQuAD dev set.

Attention Hierarchy EM / F1
1-layer attention (only qp co-attention) 61.9 / 68.4
2-layer attention (add self-attention) 65.4 / 71.7
3-layer attention (add bilinear match) 80.0 / 87.0
Attention Function EM / F1
dot product 62.9 / 69.3
linear attention 78.0 / 84.9
bilinear attention (linear + relu) 80.0 / 87.0
trilinear attention 78.9 / 85.8

Table 6: Published and unpublished results on the
TriviaQA wikipedia leaderboard.

Full Verified
Model EM / F1 EM / F1
BiDAF (Seo et al., 2016) 40.26 / 45.74 47.47 / 53.70
MEMEN (Pan et al., 2017) 43.16 / 46.90 49.28 / 55.83
M-Reader (Hu et al., 2017) 46.94 / 52.85 54.45 / 59.46
QANet (Yu et al., 2018) 51.10 / 56.60 53.30 / 59.20
document-qa (Clark and Gardner, 2017) 63.99 / 68.93 67.98 / 72.88
dirkweissenborn (unpublished) 64.60 / 69.90 72.77 / 77.44
SLQA-Single 66.56 / 71.39 74.83 / 78.74

for four common attention functions. Empirically,
we find bilinear attention which add ReLU after
linearly transforming does significantly better than
the others.

At the top of Table 5 we show the effect of vary-
ing the number of attention layers on the final per-
formance. We see a steep and steady rise in accu-
racy as the number of layers is increased from N =
1 to 3.

4.7 Experiments on TriviaQA

To further examine the robustness of the proposed
model, we also test the model performance on
TriviaQA dataset. The test performance of dif-
ferent methods on the leaderboard (on Jan. 12th
2018) is shown in Table 6. From the results, we
can see that the proposed model can also obtain
state-of-the-art performance in the more complex
TriviaQA dataset.

5 Conclusions

We introduce a novel hierarchical attention net-
work, a state-of-the-art reading comprehension
model which conducts attention and fusion hor-
izontally and vertically across layers at different
levels of granularity between question and para-
graph. We show that our proposed method is
very powerful and robust, which outperforms the
previous state-of-the-art methods in various large-
scale golden MRC datasets: SQuAD, TriviaQA,
AddSent and AddOneSent.

Figure 2: Learning curve of F1 / EM score on the
SQuAD dev set

Acknowledgments

We thank the Stanford NLP Group and the Univer-
sity of Washington NLP Group for evaluating our
results on the SQuAD and the TriviaQA test set.

References
Danqi Chen, Jason Bolton, and Christopher D Man-

ning. 2016. A thorough examination of the
cnn/daily mail reading comprehension task. arXiv
preprint arXiv:1606.02858.

Danqi Chen, Adam Fisch, Jason Weston, and An-
toine Bordes. 2017a. Reading wikipedia to an-
swer open-domain questions. arXiv preprint
arXiv:1704.00051.

Qian Chen, Xiaodan Zhu, Zhen-Hua Ling, and Di-
ana Inkpen. 2017b. Natural language infer-
ence with external knowledge. arXiv preprint
arXiv:1711.04289.

Heng-Tze Cheng, Levent Koc, Jeremiah Harmsen, Tal
Shaked, Tushar Chandra, Hrishi Aradhye, Glen An-
derson, Greg Corrado, Wei Chai, Mustafa Ispir, et al.
2016. Wide & deep learning for recommender sys-
tems. In Proceedings of the 1st Workshop on Deep
Learning for Recommender Systems, pages 7–10.
ACM.

Christopher Clark and Matt Gardner. 2017. Simple
and effective multi-paragraph reading comprehen-
sion. arXiv preprint arXiv:1710.10723.

Yiming Cui, Zhipeng Chen, Si Wei, Shijin Wang,
Ting Liu, and Guoping Hu. 2016. Attention-over-
attention neural networks for reading comprehen-
sion. arXiv preprint arXiv:1607.04423.

Dipanjan Das, Desai Chen, André FT Martins, Nathan
Schneider, and Noah A Smith. 2014. Frame-
semantic parsing. Computational linguistics,
40(1):9–56.



1714

Bhuwan Dhingra, Hanxiao Liu, Zhilin Yang,
William W Cohen, and Ruslan Salakhutdinov.
2016. Gated-attention readers for text comprehen-
sion. arXiv preprint arXiv:1606.01549.

Karl Moritz Hermann, Tomas Kocisky, Edward
Grefenstette, Lasse Espeholt, Will Kay, Mustafa Su-
leyman, and Phil Blunsom. 2015. Teaching ma-
chines to read and comprehend. In Advances in Neu-
ral Information Processing Systems, pages 1693–
1701.

Felix Hill, Antoine Bordes, Sumit Chopra, and Jason
Weston. 2015. The goldilocks principle: Reading
children’s books with explicit memory representa-
tions. arXiv preprint arXiv:1511.02301.

Minghao Hu, Yuxing Peng, and Xipeng Qiu. 2017. Re-
inforced mnemonic reader for machine comprehen-
sion. CoRR, abs/1705.02798.

Hsin-Yuan Huang, Chenguang Zhu, Yelong Shen, and
Weizhu Chen. 2017. Fusionnet: Fusing via fully-
aware attention with application to machine compre-
hension. arXiv preprint arXiv:1711.07341.

Robin Jia and Percy Liang. 2017. Adversarial exam-
ples for evaluating reading comprehension systems.
In Proceedings of the 2017 Conference on Empiri-
cal Methods in Natural Language Processing, pages
2021–2031.

Mandar Joshi, Eunsol Choi, Daniel S Weld, and Luke
Zettlemoyer. 2017. Triviaqa: A large scale distantly
supervised challenge dataset for reading comprehen-
sion. arXiv preprint arXiv:1705.03551.

Rudolf Kadlec, Martin Schmid, Ondrej Bajgar, and
Jan Kleindienst. 2016. Text understanding with
the attention sum reader network. arXiv preprint
arXiv:1603.01547.

Tsung-Yi Lin, Priya Goyal, Ross Girshick, Kaiming
He, and Piotr Dollár. 2017. Focal loss for dense ob-
ject detection. arXiv preprint arXiv:1708.02002.

Rui Liu, Wei Wei, Weiguang Mao, and Maria Chik-
ina. 2017a. Phase conductor on multi-layered at-
tentions for machine comprehension. arXiv preprint
arXiv:1710.10504.

Xiaodong Liu, Yelong Shen, Kevin Duh, and Jian-
feng Gao. 2017b. Stochastic answer networks for
machine reading comprehension. arXiv preprint
arXiv:1712.03556.

Lili Mou, Rui Men, Ge Li, Yan Xu, Lu Zhang, Rui
Yan, and Zhi Jin. 2016. Natural language inference
by tree-based convolution and heuristic matching.
In The 54th Annual Meeting of the Association for
Computational Linguistics, page 130.

Boyuan Pan, Hao Li, Zhou Zhao, Bin Cao, Deng Cai,
and Xiaofei He. 2017. Memen: Multi-layer embed-
ding with memory networks for machine compre-
hension. arXiv preprint arXiv:1707.09098.

Ankur Parikh, Oscar Täckström, Dipanjan Das, and
Jakob Uszkoreit. 2016. A decomposable attention
model for natural language inference. In Proceed-
ings of the 2016 Conference on Empirical Methods
in Natural Language Processing, pages 2249–2255.

Jeffrey Pennington, Richard Socher, and Christopher
Manning. 2014. Glove: Global vectors for word
representation. In Proceedings of the 2014 confer-
ence on empirical methods in natural language pro-
cessing (EMNLP), pages 1532–1543.

Matthew E Peters, Mark Neumann, Mohit Iyyer, Matt
Gardner, Christopher Clark, Kenton Lee, and Luke
Zettlemoyer. 2018. Deep contextualized word rep-
resentations. arXiv preprint arXiv:1802.05365.

Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and
Percy Liang. 2016. Squad: 100,000+ questions
for machine comprehension of text. arXiv preprint
arXiv:1606.05250.

Minjoon Seo, Aniruddha Kembhavi, Ali Farhadi, and
Hannaneh Hajishirzi. 2016. Bidirectional attention
flow for machine comprehension. arXiv preprint
arXiv:1611.01603.

Yelong Shen, Po-Sen Huang, Jianfeng Gao, and
Weizhu Chen. 2017. Reasonet: Learning to stop
reading in machine comprehension. In Proceedings
of the 23rd ACM SIGKDD International Conference
on Knowledge Discovery and Data Mining, pages
1047–1055. ACM.

Oriol Vinyals, Meire Fortunato, and Navdeep Jaitly.
2015. Pointer networks. In Advances in Neural In-
formation Processing Systems, pages 2692–2700.

Shuohang Wang and Jing Jiang. 2016. Machine com-
prehension using match-lstm and answer pointer.
arXiv preprint arXiv:1608.07905.

Wenhui Wang, Nan Yang, Furu Wei, Baobao Chang,
and Ming Zhou. 2017. Gated self-matching net-
works for reading comprehension and question an-
swering. In Proceedings of the 55th Annual Meet-
ing of the Association for Computational Linguistics
(Volume 1: Long Papers), volume 1, pages 189–198.

Caiming Xiong, Victor Zhong, and Richard Socher.
2016. Dynamic coattention networks for question
answering. arXiv preprint arXiv:1611.01604.

Caiming Xiong, Victor Zhong, and Richard Socher.
2017. Dcn+: Mixed objective and deep residual
coattention for question answering. arXiv preprint
arXiv:1711.00106.

Adams Wei Yu, David Dohan, Minh-Thang Luong, Rui
Zhao, Kai Chen, Mohammad Norouzi, and Quoc V
Le. 2018. Qanet: Combining local convolution
with global self-attention for reading comprehen-
sion. arXiv preprint arXiv:1804.09541.


