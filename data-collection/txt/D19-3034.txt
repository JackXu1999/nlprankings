



















































SEAGLE: A Platform for Comparative Evaluation of Semantic Encoders for Information Retrieval


Proceedings of the 2019 EMNLP and the 9th IJCNLP (System Demonstrations), pages 199–204
Hong Kong, China, November 3 – 7, 2019. c©2019 Association for Computational Linguistics

199

SEAGLE: A Platform for Comparative Evaluation of
Semantic Encoders for Information Retrieval

Fabian David Schmidt∗, Markus Dietsche∗, Simone Paolo Ponzetto and Goran Glavaš

Data and Web Science Group
University of Mannheim

fabian.david.schmidt@hotmail.de
dietsche.markus@googlemail.com

{simone, goran}@informatik.uni-mannheim.de

Abstract

We introduce SEAGLE,1 a platform for com-
parative evaluation of semantic text encod-
ing models on information retrieval (IR) tasks.
SEAGLE implements (1) word embedding ag-
gregators, which represent texts as algebraic
aggregations of pretrained word embeddings
and (2) pretrained semantic encoders, and al-
lows for their comparative evaluation on arbi-
trary (monolingual and cross-lingual) IR col-
lections. We benchmark SEAGLE’s models
on monolingual document retrieval and cross-
lingual sentence retrieval. SEAGLE function-
ality can be exploited via an easy-to-use web
interface and its modular backend (micro-
service architecture) can easily be extended
with additional semantic search models.

1 Introduction and Motivation

Traditional IR models operate on lexical overlap
and fail to identify relevance when documents and
queries differently lexicalize concepts. Semantic
search seeks to overcome such lexical mismatches
between document and user queries (Li and Xu,
2014). Early approaches to semantic search re-
lied on external resources like WordNet (Moldovan
and Mihalcea, 2000) and Wikipedia (Strube and
Ponzetto, 2006), suffering from the resource’s lim-
ited coverage. More recent semantic search sys-
tems (Vulić and Moens, 2015; Litschko et al., 2018;
Nogueira and Cho, 2019) remedy for those cover-
age issues by encoding text using distributional
word vectors (i.e., word embeddings) (Mikolov
et al., 2013; Bojanowski et al., 2017) and neural
text encoders (Devlin et al., 2018).

While there is a plethora of semantic text en-
coding models, there have been few attempts to
empirically compare them on IR tasks. In this

∗These authors contributed equally to this paper.
1SEAGLE is available on GitHub and demonstrated on

YouTube.

work, we aim to allow for such comparative eval-
uations on arbitrary IR test collections. We in-
troduce SEAGLE, a platform for concurrent ex-
ecution and comparative evaluation of semantic
search models. SEAGLE implements most recently
proposed (1) word embedding aggregation models
(Arora et al., 2017; Rücklé et al., 2018; Yang et al.,
2019; Zhelezniak et al., 2019) as well as (2) two
pretraining-based text encoders (Gysel et al., 2017;
Devlin et al., 2018) and allows users to evaluate
them on arbitrary IR collections. Coupled with
pretrained cross-lingual embedding spaces (Glavaš
et al., 2019), SEAGLE also supports cross-lingual
search. The platform’s modular architecture based
on micro-services makes it easy to extend it with
additional semantic encoding models. SEAGLE is
accessible via an easy-to-use web interface.

2 Semantic Representation Models

We first describe SEAGLE’s semantic encoders, be-
longing to two categories: word embedding aggre-
gators and pre-trained text encoders.

2.1 Word Embedding Aggregators

Word embedding aggregators encode the text by
aggregating pretrained d-dimensional embeddings
of its words. Formally, a document matrix Vd ∈
RN×d sequentially stacks embeddings ti ∈ Rd
corresponding to tokens ti (i ∈ {1, · · · , N}) of
document d from the collection D. A weight wi is
computed for every token ti and the contribution of
the embedding ti to the document representation
d ∈ Rd is scaled according to wi.
Continuos Bag-of-Words (CBOW) simply aver-
ages the rows of the document embedding matrix
Vd. Put differently, CBOW computes the simple
average of the word embeddings, i.e., it assigns
equal weights (wi=1/N ) to all embeddings.

Term Frequency-Inverse Document Frequency

http://bit.ly/SeagleEMNLP
https://youtu.be/8ncTgRqr8w4


200

(TF-IDF) aggregator computes the weight wi as
the product of term ti’s relative frequency within
the document d (TF) and the inverse of proportion
of documents in the collection containing ti (IDF).
The assumption is that (1) more frequent words
contribute more to the document’s meaning, as do
(2) the terms that are more specific to the document
(i.e., do not occur in many other documents).

Smooth Inverse Frequency (SIF) conflates bias-
adjusted weighted word embeddings to generate
document embeddings (Arora et al., 2017):

d =
1

N

∑
ti∈d

a

a+ p(ti|D)
ti (1)

Weight wi of a term ti is a smoothed inverse of the
probability of ti under the unigram language model
built from the whole collection D, with a being the
smoothing factor. In the next step, common compo-
nent removal (CCR) is applied to every document
vector d: let X ∈ Rd×|D| be the matrix obtained
by stacking vectors of all collection documents as
columns. Each document vector d is then replaced
with d − uu>d, where u is the left singular vec-
tor of X . CCR aims to eliminate the similarities
between document vectors that originate from syn-
tactic rather than semantic similarities.

Concatenated Power Means (CPM) generalizes
the aggregation of word vectors to their chosen
powers (Rücklé et al., 2018):

dp = (
tp1 + · · ·+ tpn

N
)
1
p (2)

Choices for p constitute a hyperparameter and
determine the dimensionality of the resulting docu-
ment embedding, as dp for different p are concate-
nated into a final document representation. Power
means can reduce a set of vectors to a geometric
mean (p = 0), arithmetic mean (p = 1), mini-
mum (p = −∞), and maximum (p = ∞). We
concatenate the last three to generate the final doc-
ument embedding, and then, following the original
work (Rücklé et al., 2018), perform element-wise
z-normalization over document vectors d.

Geometric Embedding (GEM) weighs word em-
beddings ti by summing their novelty, signifi-
cance, and uniqueness scores (Yang et al., 2019)
and correcting the resulting document vectors for
(document-dependent) principal components via
CCR. Let W i = [ti−m, ti−1, · · · , ti+m, ti] ∈
Rd×(2m+1) be the contextual window of the token
ti with m neighbours.

The novelty score of ti is computed as the nor-
malized minimal distance from from ti to the sub-
space spanned by the vectors of context words. The
significance score captures the semantic alignment
between the vectors ti and the context W i as the
similarity between ti and singular vectors of W i

obtained via SVD. Intuitively, a token is more sig-
nificant the more it is aligned with the context’s
principal components. Lastly, the uniqueness score
quantifies the alignment between the word vector
and the principal directions computed on the whole
collection. A token highly aligned with collection’s
principal components is an uninformative word and
should receive a low weight.2

DynaMax-Jaccard (DJ) is a non-parametric sim-
ilarity measure (Zhelezniak et al., 2019). The al-
gorithm projects stacked word embeddings of a
query q ∈ RM×d and stacked word embeddings
of a document d ∈ RN×d into the shared space
U ∈ R(M+N)×d. Features for q and d are then
max-pooled along the rows of projections, respec-
tively. Feature generation tests the degree of mem-
bership of q and d in U and represents an extension
of set theory to real-valued vectors. Accordingly,
the fuzzy Jaccard index measures the similarity be-
tween query and document representations and is
computed as follows: stacked features are min- and
max-pooled over rows and the sum of minima over
the sum of maxima yields the fuzzy Jaccard score.

2.2 Semantic Text Encoders

BERT (Devlin et al., 2018) is a general-purpose
unsupervised pretraining model based on the Trans-
former architecture that can dynamically pre-
dict contextualized token vectors. We integrate
bert-as-a-service (Xiao, 2018) to infer
document embeddings: we average-pool stacked
token representations in each of BERT’s Trans-
former’s layers (second to fourth from the last
layer) and concatenate the resulting averaged rep-
resentations of each layer to obtain a document
embedding d. We then element-wise z-normalize
d. Because BERT’s positional encoding limits the
maximal sequence length, we dissect collection
documents into 256-token segments (with 32 token
overlap between adjacent segments). The final doc-
ument embedding d is the average of 256-token
segments’ embeddings generated by BERT.

2For more details on GEM, we refer the reader to the
original work (Yang et al., 2019).



201

Neural Vector Space Model (NVSM) jointly
learns token and document representations (Gy-
sel et al., 2017). Specifically, during training (in-
ference), relevant queries are modeled as CBOW
embeddings of n-grams (query) sampled from the
source document and then mapped via a learned
transformation onto the document space. NVSM
then learns its parameters (word and document em-
beddings; mapping), such that the mapped n-grams
(queries) are most similar to the respective docu-
ment representation. The model accounts for the
lack of positive supervision (the number of rele-
vance judgments is typically rather limited) via
negative sampling and a contrastive maximum like-
lihood loss. For more details on NVSM, we refer
the reader to the original work (Gysel et al., 2017).

3 Benchmarking Semantic Models

We benchmark the above semantic search models
on two retrieval tasks: (1) monolingual document
retrieval on the LATimes test collection (112.082
documents, 114 queries, and 2.094 positive rel-
evance judgments) (2) cross-lingual sentence re-
trieval on the subset of Europarl’s sentence-aligned
corpora: we compile 5.000 aligned sentence trans-
lations between English (EN), Italian (IT), German
(DE), and Finnish (FI). Following the specifici-
ties of each collection, we measure performance
on LATimes collection in terms of NDCG@100,
MAP@1000, and Precision@10; and in terms of
MRR and Hits@1, 5, 10 for Europarl. We addition-
ally evaluate BM25, a robust probabilistic retrieval
model (Robertson et al., 2009) as a baseline for the
monolingual document retrieval. BM25 captures
only lexical overlaps between documents queries,
i.e., it cannot capture any semantic alignment not
originating from shared terms.

3.1 Experimental Setup

All required corpus statistics (e.g., IDF or com-
mon components) are computed offline on the
document collection as a preprocessing step. For
all aggregation based methods (see §2.1) we em-
ploy 300-dimensional fastText embeddings (Bo-
janowski et al., 2017), pretrained on Wikipedia.3

For cross-lingual sentence retrieval, we first in-
duce the shared bilingual word embedding spaces
by projecting the EN vectors to the monolin-
gual space of the target language (IT, DE, or

3Available at https://fasttext.cc/docs/en/
pretrained-vectors.html

FI). To this end, following (Glavaš et al., 2019),
we use 5K automatically obtained word trans-
lation pairs to induce the projection matrices
by solving the Procrustes problem. We in-
fer contextualized embeddings with pretrained
BERT models using Bert-Large, Uncased
(Whole Word Masking) and Bert-Base,
Multilingual Cased4 for LATimes and Eu-
roparl, respectively. Except for BERT, we lower-
case and tokenize text using BlingFire.5 For
NVSM, we trim the vocabulary to 60k most fre-
quent non-stop words and learn 512 dimensional
word and document embeddings with a tanh acti-
vation mapping on sampled n-grams of length 16
with 10 contrastive examples.

3.2 Results

Table 1 summarizes the monolingual document re-
trieval and cross-lingual sentence retrieval results.
NVSM yields the best retrieval performance on the
monolingual document retrieval task, suggesting
that reliable word and document representations
can be learned from regular-size retrieval collec-
tions. Among the embedding aggregation models,
SIF seems to display the best performance. The
fact that BM25, a semantically unaware baseline,
outperforms all semantic models on document re-
trieval is discouraging. However, this may merely
be an artifact of the LATimes dataset: out of 2.094
relevances, the document contains some (all) query
terms in 1.975 (647) cases.

The cross-lingual sentence retrieval shows that
all semantic search models outperform the simple
word embedding averaging. Overall, DynaMax-
Jaccard (DJ) yields the strongest performance, only
trailing SIF on the EN-FI evaluation. In EN-FI sce-
nario the common component removal (CCR) step
included in SIF strongly boosts the performance
(SIF weighting without CCR yields merely 41.1%
MRR). Our cross-lingual sentence retrieval based
on the pre-trained multilingual BERT model ex-
hibits strong performance across all three language
pairs – on EN-IT and EN-DE it lags behind DJ by
a small margin and substantially outperforms all
other aggregators; on EN-FI it outperforms DJ but
falls behind the surprisingly effective SIF (with
CCR). BERT’s and DJ’s effectiveness, however,
significantly drop in the monolingual document

4https://github.com/google-research/
bert

5https://github.com/Microsoft/
BlingFire

https://fasttext.cc/docs/en/pretrained-vectors.html
https://fasttext.cc/docs/en/pretrained-vectors.html
https://github.com/google-research/bert
https://github.com/google-research/bert
https://github.com/Microsoft/BlingFire
https://github.com/Microsoft/BlingFire


202

LATimes Europarl

EN-EN EN-IT EN-DE EN-FI

Model NDCG MAP P@10 MRR H@1 H@5 H@10 MRR H@1 H@5 H@10 MRR H@1 H@5 H@10

CBOW 28.4 18.8 14.6 59.4 52.9 66.4 71.1 55.4 48.7 62.7 67.5 38.3 30.7 46.6 52.8
TF-IDF 29.8 21.8 18.1 68.4 63.0 74.5 78.4 63.6 57.6 70.2 74.8 42.2 34.6 50.3 57.6

SIF 34.1 28.8 25.9 86.4 82.4 91.2 93.1 80.2 75.3 86.0 88.8 61.8 54.0 70.6 75.9
CPM 30.0 21.5 17.4 78.9 73.3 85.6 88.6 72.1 65.6 79.3 83.6 42.7 33.6 52.2 59.6
GEM 26.8 22.2 19.2 81.0 76.5 86.7 89.4 74.7 68.6 82.0 85.9 42.5 34.4 50.7 58.0

DJ 23.0 14.3 11.6 93.7 92.1 95.6 96.6 89.0 86.3 92.2 94.0 51.1 42.3 61.3 67.6

NVSM 31.8 34.9 29.7 – – – – – – – – – – – –
BERT 20.2 12.4 11.5 87.3 84.4 90.6 92.7 87.4 84.5 90.8 92.7 56.5 49.2 64.9 71.0

BM25 38.0 42.0 32.5 – – – – – – – – – – – –

Table 1: Results of the comparative evaluation of semantic search models on: (1) monolingual document retrieval
(LATimes); metrics: NDCG@100, MAP@1000 and Precision@10; and (2) cross-lingual sentence retrieval (Eu-
roparl, 5K sentence pairs, EN-DE, EN-IT, and EN-FI); metrics: MRR and Hits@{1, 5, 10}.

retrieval setup where the models need to encode
much longer documents.

Lastly, the complexity and runtime of the eval-
uated models should not be ignored. As a rule
of thumb, CBOW, TF-IDF, SIF and CPM, are
quite efficient and resource-light, whereas the re-
maining algorithms become increasingly resource-
demanding and decreasingly efficient – from GEM,
and DJ, which require embedding aggregation over
the whole collection, over predicting vectors with
BERT (one inference for each max. length to-
ken segment), to NVSM, for which any collection
change warrants model retraining.

4 Architecture & Interface

SEAGLE is implemented as an application based on
micro-services, consisting of a web client (see fig-
ure 1) for configuration and search and network
daemons (see figure 2), one for each semantic
search model. Such a modularized architecture
facilitates the implementation and addition of new
semantic search models (as new daemons).

4.1 SEAGLE’s Architecture

browser

Seagle web frontend daemon 1

daemon 2

daemon n

file system (documents)

init 
downloadshttp

TCP/IP

JSON

nginx

Python + django

PostgreSQL socket

Bootstrap

Figure 1: web front end components

Seagle is a Python application, based on the

Django6 framework for web development. It in-
dexes document collections (i.e., all document rep-
resentations required by semantic search models)
within a PostgreSQL database.7 It runs on a ng-
inx8 web-server and utilises Bootstrap9 to ensure
responsiveness.

Communication between the web application
and the network daemons implementing the seman-
tic search models is conducted through web sockets
via TCP/IP and a JSON API. TCP was chosen over
UDP so network daemons themselves can have a in-
creased degree of control about how many searches
and initializations are executed in parallel. This
becomes relevant in case of large document collec-
tions and computationally intensive algorithms.

Seagle
web frontend

Seagle network daemon 

socker
TCP/IP
JSON

Python

method container

 JSON
settings

Figure 2: network daemon layout

Network Daemon Server. On the server side
SEAGLE’s Python-based network daemons contain
the actual semantic search models. They are sup-
plied with the document collection via the front-
end and do not need to access the file system them-
selves. Each daemon comes with a template which
implements the API and loads its settings from a
local JSON file (see figure 2) containing setup in-
formation (e.g., the port on which it should run).
To implement new functionality the template needs

6https://www.djangoproject.com/
7https://www.postgresql.org/
8https://www.nginx.com/
9https://getbootstrap.com/

https://www.djangoproject.com/
https://www.postgresql.org/
https://www.nginx.com/
https://getbootstrap.com/


203

to be extended with an actual method, containing
the initialization function (i.e., a specification of
the document indexing procedure for a particular
retrieval model) and the search function (i.e., the
specification of the ranking function for a partic-
ular model). In case multiple network daemons
running on the same machine, they should all run
on different ports.

Backend Deployment. SEAGLE daemons com-
prise a modular backend in which all semantic em-
bedding models reuse I/O and evaluation functions,
allowing for easy integration of additional search
methods. Moreover, for efficiency, we implement
all encoders using matrix and vector operations
from Numpy.10 While SEAGLE offers an easy-to-
use web interface, semantic search models can be
executed and evaluated from the command line.

A major benefit of the micro service architecture
is its distributability on multiple machines. The
main reason for multi-machine deployment of SEA-
GLE’s backend is the computational complexity of
some search models. Considering that for some
models it might take hours or days to initialize (i.e.,
index) large document collections, the initialization
process can be significantly reduced by deploying
computation intensive models to different machines
and running them in parallel.

4.2 SEAGLE Interface

A. menu

1. collections 2. queries 3. methods

4. evaluation 5. results X. admin

Figure 3: Components of SEAGLE’s web interface

SEAGLE’s web interface allows users, with-
out programming and IR knowledge, to easily in-
dex document collections, select desired semantic
search models and conduct comparative retrieval
evaluations. Its core functionality is wrapped up in
a easy to navigate one page layout and bundles 5
components (see figure 3):

1. Collections: Allows selection of one or multi-
ple collections of documents on which meth-
ods are going to be evaluated.

10SEAGLE’s reliance on Numpy routines requires a care-
fully tuned Numpy installation linked to fast linear algebra
libraries.

Figure 4: SEAGLE’s queries component

2. Queries (see figure 4): 2.1. Manually add
and remove queries (and their respective doc-
ument relevance annotations); 2.2. Bundle
CSV import and export & download of arbi-
trary number of queries with relevance anno-
tations.

3. Methods: Select which semantic search meth-
ods to execute and evaluate.

4. Evaluation (see figure 5): 4.1. Change evalua-
tion parameters, e.g. include or exclude evalu-
ation metrics or specify the number of results
to be shown for each method. 4.2. Contains a
summary of collections, queries and methods,
which are going to be evaluated along with
the button triggering the evaluation process.

5. Results: 5.1. Bar charts of evaluation metrics,
(e.g. MAP) or execution time per method.
5.2. A query explorer, allowing real-time ex-
ploration of executed conducted queries and
top-ranked results by selected search models.
(see figure 6)

Figure 5: SEAGLE’s evaluation component

Figure 6: SEAGLE’s evaluation component



204

Additionally, there is a quick-link component for
administration (X. admin), enabling quick setup
and modifications of SEAGLE’s backend.

5 Conclusion

We presented SEAGLE, a platform implementing a
number of strong baselines for semantic IR and al-
lowing for their comparative evaluation on arbitrary
test collections. We benchmark the implemented
semantic search models on monolingual document
retrieval and cross-lingual sentence retrieval tasks,
offering insights into their comparative advantages
and shortcomings. Our benchmarking results indi-
cate that there is no single best-performing seman-
tic search model for all settings and that the users
must consider various factors when selecting the
best model for their retrieval task.

SEAGLE offers a satisfying out-of-the-box so-
lution for fast benchmarking of semantic retrieval
models on arbitrary collections. The platform’s
web interface allows the user to effortlessly index
document collections, select semantic search mod-
els and their hyperparameter setup, comparatively
evaluate selected models and finally visualize the
results for manual inspection. SEAGLE’s modular
architecture (based on network daemon templates)
allows for fast implementation of new search mod-
els and their inclusion into comparative evaluations
in a plug-and-play fashion.

Acknowledgments

We thank Leon Schüller and Siying Liu, who con-
tributed to the original student project on which
SEAGLE is based on. Additionally, we thank Hans-
Peter Zorn from inovex GmbH for his feedback
over the course of the same student project.

References

Sanjeev Arora, Yingyu Liang, and Tengyu Ma. 2017.
A simple but tough-to-beat baseline for sentence em-
beddings.

Piotr Bojanowski, Edouard Grave, Armand Joulin, and
Tomas Mikolov. 2017. Enriching word vectors with
subword information. Transactions of the Associa-
tion for Computational Linguistics, 5:135–146.

Jacob Devlin, Ming-Wei Chang, Kenton Lee, and
Kristina Toutanova. 2018. Bert: Pre-training of deep
bidirectional transformers for language understand-
ing. arXiv preprint arXiv:1810.04805.

Goran Glavaš, Robert Litschko, Sebastian Ruder, and
Ivan Vulić. 2019. How to (properly) evaluate cross-
lingual word embeddings: On strong baselines, com-
parative analyses, and some misconceptions. arXiv
preprint arXiv:1902.00508.

Christophe Van Gysel, Maarten de Rijke, and Evan-
gelos Kanoulas. 2017. Neural vector spaces
for unsupervised information retrieval. CoRR,
abs/1708.02702.

Hang Li and Jun Xu. 2014. Semantic matching in
search. Found. Trends Inf. Retr., 7(5):343–469.

Robert Litschko, Goran Glavaš, Simone Paolo
Ponzetto, and Ivan Vulić. 2018. Unsupervised cross-
lingual information retrieval using monolingual data
only. In SIGIR, pages 1253–1256.

Tomas Mikolov, Ilya Sutskever, Kai Chen, Gregory S.
Corrado, and Jeffrey Dean. 2013. Distributed rep-
resentations of words and phrases and their compo-
sitionality. In Proceedings of NIPS, pages 3111–
3119.

Dan I Moldovan and Rada Mihalcea. 2000. Using
wordnet and lexical operators to improve internet
searches. IEEE Internet Computing, 4(1):34–43.

Rodrigo Nogueira and Kyunghyun Cho. 2019. Pas-
sage re-ranking with bert. arXiv preprint
arXiv:1901.04085.

Stephen Robertson, Hugo Zaragoza, et al. 2009. The
probabilistic relevance framework: Bm25 and be-
yond. Foundations and Trends R© in Information Re-
trieval, 3(4):333–389.

Andreas Rücklé, Steffen Eger, Maxime Peyrard, and
Iryna Gurevych. 2018. Concatenated power mean
embeddings as universal cross-lingual sentence rep-
resentations. arXiv.

Michael Strube and Simone Paolo Ponzetto. 2006.
Wikirelate! computing semantic relatedness using
wikipedia. In AAAI, volume 6, pages 1419–1424.

Ivan Vulić and Marie-Francine Moens. 2015. Monolin-
gual and cross-lingual information retrieval models
based on (bilingual) word embeddings. In SIGIR,
pages 363–372.

Han Xiao. 2018. bert-as-service. https://
github.com/hanxiao/bert-as-service.

Ziyi Yang, Chenguang Zhu, and Weizhu Chen. 2019.
Zero-training sentence embedding via orthogonal
basis.

Vitalii Zhelezniak, Aleksandar Savkov, April Shen,
Francesco Moramarco, Jack Flann, and Nils Y. Ham-
merla. 2019. Don’t settle for average, go for the
max: Fuzzy sets and max-pooled word vectors. In
International Conference on Learning Representa-
tions.

http://arxiv.org/abs/1708.02702
http://arxiv.org/abs/1708.02702
https://doi.org/10.1561/1500000035
https://doi.org/10.1561/1500000035
https://arxiv.org/pdf/1310.4546
https://arxiv.org/pdf/1310.4546
https://arxiv.org/pdf/1310.4546
https://arxiv.org/abs/1803.01400
https://arxiv.org/abs/1803.01400
https://arxiv.org/abs/1803.01400
https://github.com/hanxiao/bert-as-service
https://github.com/hanxiao/bert-as-service
https://openreview.net/forum?id=rJedbn0ctQ
https://openreview.net/forum?id=rJedbn0ctQ
https://openreview.net/forum?id=SkxXg2C5FX
https://openreview.net/forum?id=SkxXg2C5FX

