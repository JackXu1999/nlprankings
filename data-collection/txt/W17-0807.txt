



















































Consistent Classification of Translation Revisions: A Case Study of English-Japanese Student Translations


Proceedings of the 11th Linguistic Annotation Workshop, pages 57–66,
Valencia, Spain, April 3, 2017. c©2017 Association for Computational Linguistics

Consistent Classification of Translation Revisions:
A Case Study of English-Japanese Student Translations

Atsushi Fujita
NICT

atsushi.fujita@nict.go.jp

Kikuko Tanabe
Kobe College

kikukotanabe@gmail.com

Chiho Toyoshima
Kansai Gaidai University

c.toyoshima1113@gmail.com

Mayuka Yamamoto
Honyaku Center Inc.
yamamoto.mayuka

@honyakuctr.co.jp

Kyo Kageura
University of Tokyo
kyo@p.u-tokyo.ac.jp

Anthony Hartley
Rikkyo University

A.Hartley@rikkyo.ac.jp

Abstract

Consistency is a crucial requirement in
text annotation. It is especially important
in educational applications, as lack of con-
sistency directly affects learners’ motiva-
tion and learning performance. This pa-
per presents a quality assessment scheme
for English-to-Japanese translations pro-
duced by learner translators at university.
We constructed a revision typology and
a decision tree manually through an ap-
plication of the OntoNotes method, i.e.,
an iteration of assessing learners’ transla-
tions and hypothesizing the conditions for
consistent decision making, as well as re-
organizing the typology. Intrinsic eval-
uation of the created scheme confirmed
its potential contribution to the consistent
classification of identified erroneous text
spans, achieving visibly higher Cohen’s κ
values, up to 0.831, than previous work.
This paper also describes an application
of our scheme to an English-to-Japanese
translation exercise course for undergrad-
uate students at a university in Japan.

1 Introduction

Assessing and assuring translation quality is one
of the main concerns for translation services, ma-
chine translation (MT) industries, and translation
teaching institutions.1 The assessment process for
a given pair of source document (SD) and its trans-
lation, i.e., target document (TD), consists of two
tasks. The first task is to identify erroneous text
spans in the TD. In professional settings, when as-
sessors consider a text span in a TD as erroneous,

1These include both private companies and translation-
related departments in colleges and universities.

they generally suggest a particular revision pro-
posal (Mossop, 2014). For instance, in example
(1), a transliteration error is corrected.

(1) SD: Mark Potok is a senior fellow at the
Southern Poverty Law Center.

TD: マーク・ポッドック (⇒ポトク)氏は
南部貧困法律センターの上級研究員だ。
(Podok ⇒ Potok)

Henceforth, we refer to a marked text span reflect-
ing the identification of a particular error or defi-
ciency as an issue. The second task is to classify
each identified issue into an abstract issue type,
such as “omission” or “misspelling.”

An inherent problem concerning translation
quality assessment is that it inevitably involves hu-
man judgments, and thus is subjective.2 The first
task, i.e., identifying issues in TDs, relies heav-
ily on assessors’ translation and linguistic compe-
tence, as may the subsequent step of making a re-
vision proposal for them, depending on the sub-
tlety of the issue. It therefore seems impractical
to create an annotation scheme that enables even
inexperienced translators to perform this task at a
comparable level to mature translators.

For regulating the second task, several typolo-
gies, such as those reviewed in Secarǎ (2005), the
Multilingual e-Learning in Language Engineer-
ing (MeLLANGE) error typology (Castagnoli et
al., 2006), and Multidimensional Quality Metrics
(MQM),3 have been proposed. Existing issue ty-
pologies show diversity in their granularity and
their organization of issue types, owing to the fact
that the scope and granularity of issues depend

2While automated metrics for MT quality evaluation are
often presented as objective, many, including BLEU (Pap-
ineni et al., 2002), rely on comparison with a one or more
human reference translations whose quality and subjectivity
are merely assumed and not independently validated.

3http://www.qt21.eu/launchpad/content/
multidimensional-quality-metrics

57



on the purpose of translations and the aim of hu-
man assessments (e.g., formative or summative).
However, the typology alone does not necessarily
guarantee consistent human assessments (Lommel
et al., 2015). For instance, while one may clas-
sify the issue in (1) as an “incorrect translation of
term,” it could also be regarded as a “misspelling.”

In this paper, we focus on the quality as-
sessment of learners’ translations. Motivated by
the increasing demand for translation, transla-
tion teaching institutions have been incorporat-
ing best practices of professionals into their cur-
ricula. When teaching the revision and review
processes in such institutions, the assessor’s re-
vision proposal is normally not provided, in or-
der to prevent learners believing that it is the only
correct solution (Klaudy, 1996). Thus, issue type
plays a crucial role in conveying the assessors’
intention to learners, and its consistency is espe-
cially important, since lack of consistency directly
affects learners’ motivation and learning perfor-
mance. Besides the consistency, the applicability
of an assessment tool to a wide range of transla-
tions is also important. To the best of our knowl-
edge, however, none of the existing typologies
have been validated for translations between lan-
guages whose structures are radically different,
such as English and Japanese. Neither have their
applicability to translations produced by less ad-
vanced learners, such as undergraduate students,
been fully examined.

Aiming at (i) a consistent human assessment,
(ii) of English-to-Japanese translations, (iii) pro-
duced by learner translators, we manually con-
structed a scheme for classifying identified issues.
We first collected English-to-Japanese translations
from learners in order to assure and validate the
applicability of our scheme (§3). We then manu-
ally created an issue typology and a decision tree
through an application of the OntoNotes method
(Hovy et al., 2006), i.e., an iteration of assess-
ing learners’ translations and updating the typol-
ogy and decision tree (§4). We adopted an existing
typology, that of MNH-TT (Babych et al., 2012),
as the starting point, because its origin (Castag-
noli et al., 2006) was tailored to assessing uni-
versity student learners’ translations and its ap-
plicability across several European languages had
been demonstrated. We evaluated our scheme with
inter-assessor agreement, employing four asses-
sors and an undergraduate learner translator (§5).

We also implemented our scheme in an English-
to-Japanese translation exercise course for under-
graduate students at a university in Japan, and ob-
served tendencies among absolute novices (§6).

2 Previous Work

To the best of our knowledge, the error typol-
ogy in the Multilingual e-Learning in Language
Engineering (MeLLANGE) project (Castagnoli et
al., 2006) was the first tool tailored to assess-
ing learners’ translations. It had been proved
applicable to learners’ translations across several
European languages, including English, German,
Spanish, French, and Italian. The MeLLANGE ty-
pology distinguished more than 30 types of issues,
grouped into Transfer (TR) issues, whose diagno-
sis requires reference to both SD and TD, and Lan-
guage (LA) issues, which relate to violations of
target language norms. This distinction underlies
the widespread distinction between adequacy and
fluency, the principal editing and revision strate-
gies advocated by Mossop (2014), and the differ-
entiation between (bilingual) revision and (mono-
lingual) reviewing specified in ISO/TC27 (2015).
Designed for offering formative assessment by ex-
perienced instructors to university learner transla-
tors, it provided a fine-grained discrimination seen
also in, for instance, the framework of the Amer-
ican Translators Association (ATA) with 23 cate-
gories.4

The MeLLANGE typology was simplified by
Babych et al. (2012), who conflated various sub-
categories and reduced the number of issue types
to 16 for their translation training environment,
MNH-TT, which differs from MeLLANGE in two
respects. First, it is designed for feedback from
peer learners acting as revisers and/or review-
ers, whose ability to make subtle distinctions is
reduced. Second, it is embedded in a project-
oriented translation scenario that simulates pro-
fessional practice and where more coarse-grained,
summative schemes prevail.5,6 In our pilot test,
however, we found that even the MNH-TT typol-
ogy did not necessarily guarantee consistent hu-
man assessments. When we identified 40 issues

4http://www.atanet.org/certification/
aboutexams_error.php

5SAE J2450, the standard for the automotive industry, has
only seven categories. http://standards.sae.org/
j2450_200508/

6The latest MQM (as of February 21, 2017) has eight top-
level issue types (dimensions) and more than 100 leaf nodes.
http://www.qt21.eu/mqm-definition/

58



Level 1: Incompleteness Translation is not finished.
Level 2: Semantic errors The contents of the SD are not properly transferred.
Level 3: TD linguistic issues The contents of the SD are transferred, but there are some linguistic issues in the TD.
Level 4: TD felicity issues The TD is meaning-preserving and has no linguistic issues, but have some flaws.
Level 5: TD register issues The TD is a good translation, but not suitable for the assumed text type.

Table 1: Priority of coarse-grained issue types for translation training for novices.

in an English-to-Japanese translation by a learner
and two of the authors separately classified them,
only 17 of them (43%) resulted in agreement on
the classification, achieving Cohen’s κ (Cohen,
1960) of 0.36. This highlighted the necessity of
a navigation tool, such as a decision tree, for con-
sistent human decision making, especially given
that the issue type serves as feedback to learners.

Multidimensional Quality Metrics (MQM) has
been widely used in the MT community and trans-
lation industries. However, the consistency of
classifying issues had not been guaranteed when
only its issue typology was used. Lommel et al.
(2014) measured the inter-assessor agreement of
identifying erroneous text spans in MT outputs
and classifying them, using the MQM typology
comprising 20 types. Having obtained low Co-
hen’s κ values, 0.18 to 0.36, and observed sev-
eral types of ambiguities, they pointed out the lack
of decision making tool. Motivated by this study,
MQM later established a decision tree (Burchardt
and Lommel, 2014). Nevertheless, MQM has not
been validated as applicable to learners’ transla-
tions, especially those between distant languages.

3 Collecting English-to-Japanese
Translations by Learners

Our study began with collecting English-to-
Japanese translations produced by learner transla-
tors. Assuming novice learner translators and the
very basic competences to teach, we selected jour-
nalistic articles as the text type. As the SDs in
English, 18 articles with similar conceptual and
linguistic difficulties were sampled from the col-
umn page of a news program “Democracy Now!”7

by a professional translator, who also had signif-
icant experience in teaching English-to-Japanese
translation at universities. The average number of
words in the SDs was 781. Then, 30 students (12
undergraduate and 18 graduate students) were em-
ployed to translate one of the SDs into Japanese.8

7http://www.democracynow.org/blog/
category/weekly_column/

8Some of the SDs were separately translated by more than
one student.

All these participants were native Japanese speak-
ers and had attended translation classes at a uni-
versity in Japan. They were asked to produce a
TD that served as a Japanese version of the origi-
nal article.

The collected pairs of SD and TD were divided
into the following three partitions.

Development: Three randomly selected docu-
ment pairs were used to develop our scheme
(§4).

Validation 1: Another 17 sampled document
pairs were used to gauge the inter-assessor
agreement of the issue classification task,
given the identified issues (§5.1).

Validation 2: The remaining ten document pairs
were used to examine the stability of iden-
tifying erroneous text spans in the TDs, as
well as the inter-assessor agreement between
a learner and an experienced assessor (§5.2).

4 Development of an Issue Classification
Scheme

To alleviate potential inconsistencies, we struc-
turalized the issue types in the MNH-TT typology
(Babych et al., 2012), introducing a decision tree.
We chose a decision tree as a navigation tool for
human decision making, as in MQM (Burchardt
and Lommel, 2014), because the resulting issues
will be used not only by the instructors in order to
evaluate the translation quality but also by learn-
ers in order to understand the diagnoses. We also
considered that explicit explanation for decisions
is critical in such scenarios.

We first determined the priorities of issue types
through in-depth interviews with two professional
translators, who also had ample experience in
teaching English-to-Japanese translation at univer-
sities. These priorities were based on both the
work-flow of the professionals and the nature of
the issues they found in grading learners’ transla-
tions. Table 1 shows the coarse-grained figure re-
sulting from the two translators’ agreement. Ob-
vious incompleteness of translations are captured

59



Issue types in our typology MeLLANGE MQM
Level 1: Incompleteness
X4a Content-SD-intrusion-untranslated:

The TD contains elements of the SD left untranslated in error
TR-SI-UT Untranslated

X6 Content-indecision:
The TD contains alternative choices left unresolved by the translator

TR-IN n/a

Level 2: Semantic errors
X7 Lexis-incorrect-term: Item is a non-term, incorrect, inconsistent

with the glossary or inconsistent within the TD
LA-TL-
{IN,NT,IG,IT}

Terminology

X1 Content-omission:
Content present in the SD is wrongly omitted in the TD

TR-OM Omission

X2 Content-addition:
Content not present in the SD is wrongly added to the TD

TR-AD Addition

X3 Content-distortion:
Content present in the SD is misrepresented in the TD

TR-DI,
TR-TI-∗,
TR-TL-IN

Mistranslation

Level 3: TD linguistic issues
X8 Lexis-inappropriate-collocation: Item is not a usual collocate of a

neighbor it governs or is governed by
LA-TL-IC n/a

X10 Grammar-preposition/particle:
Incorrect preposition or (Japanese) particle

LA-PR Grammar

X11 Grammar-inflection: Incorrect inflection or agreement for tense, as-
pect, number, case, or gender

LA-IA-∗ Grammar

X12 Grammar-spelling: Incorrect spelling LA-HY-SP Spelling
X13 Grammar-punctuation: Incorrect punctuation LA-HY-PU Punctuation
X9 Grammar-others: Other grammatical and syntactic issues in the TD n/a Grammar
Level 4: TD felicity issues
X16 Text-incohesive: Inappropriate use or non-use of anaphoric expres-

sions, or wrong ordering of given and new elements of information
n/a n/a

X4b Content-SD-intrution-too-literal:
The TD contains elements of the SD that are translated too literally

TR-SI-TL Overly literal

X15 Text-clumsy: Lexical choice or phrasing is clumsy, tautologous, or
unnecessarily verbose

LA-ST-∗ Awkward

Level 5: TD register issues
X14 Text-TD-inappropriate-register: Lexical choice, phrasing, or style

is inappropriate for the intended text type of the TD
LA-RE-∗ Style (except “Awkward”),

Local convention,
Grammatical register

Table 2: Our issue typology, with prefixes (context, lexis, grammar, and text) indicating their coarse-
grained classification in the MNH-TT typology (Babych et al., 2012). The two rightmost columns show
the corresponding issue types in the MeLLANGE typology (Castagnoli et al., 2006) and those in MQM
(Lommel et al., 2015), respectively, where “n/a” indicates issue types that are not covered explicitly.

at Level 1. While Level 2 covers issues related
to misunderstandings of the SD, Levels 3 and 4
highlight issues in the language of the TD. Level 5
deals with violation of various requirements im-
posed by the text type of the translated document.

Regarding Table 1 as a strict constraint for the
shape of the decision tree, and the 16 issue types
in the MNH-TT typology as the initial issue types,
we developed our issue classification scheme, us-
ing the OntoNotes method (Hovy et al., 2006).
In other words, we performed the following iter-
ation(s).

Step 1. Annotate issues in the TDs for develop-
ment, using the latest scheme.

Step 2. Terminate the iteration if we meet a satis-
factory agreement ratio (90%, as in Hovy et
al. (2006)).

Step 3. Collect disagreed issues among assessors,
including those newly found, and discuss the
factors of consistent decision making.

Step 4. Update the scheme, including the defini-
tion of each issue type, the conditions for de-
cision making, and their organization in the
form of a decision tree. Record marginal ex-
amples in the example list.

Step 5. Go back to Step 1.

Three of the authors conducted the above pro-
cess using the three document pairs (see §3),
which resulted in the issue typology in Table 2 and
the decision tree in Table 3. A total of 52 typical
and marginal examples were also collected.

It is noteworthy that our issue typology pre-
serves almost perfectly the top-level distinction
of the MeLLANGE typology, i.e., the TR (trans-

60



ID Question Determined type or next questionTrue False
Q1a Is it an unjustified copy of the SD element? X4a Q1b
Q1b Do multiple options remain in the TD? X6 Q2a
Q2a Is all content in the SD translated in proper quantities in a proper way? Q3a Q2b
Q2b Is the error related to a term in the given glossary? X7 X1/X2/X3
Q3a Is it a grammatical issue? Q3b Q4a
Q3b Is it predefined specific type? X8/X10/X11

/X12/X13
X9

Q4a Does it hurt cohesiveness of the TD? X16 Q4b
Q4b Does it hurt fluency? Q4c Q5a
Q4c Is it too literal? X4b X15
Q5a Is it unsuitable for the intended text type of the TD? X14 Q6a
Q6a Is it anyways problematic? “Other issue” “Not an issue”

Table 3: Our decision tree for classifying a given issue: we do not produce questions for distinguishing
X1/X2/X3, and X8/X10/X11/X12/X13, considering that their definitions are clear enough.

fer) and LA (language) issues, described in §2.
The priority of the former over the latter, implic-
itly assumed in the MeLLANGE typology, is also
largely preserved; the sole exceptions are X7 (in-
correct translations of terms) and X4b (too literal).
Table 2 also shows that our typology includes the
following three issue types that are not covered by
MQM.

• X6 (indecision) captures a student habit of of-
fering more than one translation for a given
text, which is not observed in professional
translators.

• X8 (collocation) employs a more specific,
linguistic terminology for diagnosing one
subtype of X15 (clumsy).

• X16 (incohesive), which is also absent from
the MeLLANGE typology but present in the
ATA framework, appears not applicable in
the common (commercial) situation where
sentences are translated without reference to
their context.

During the development process, we decided to
identify and classify only the first occurrence of
identical issues in a single TD. For instance, other
incorrect translations of “ポッドック” for “Potok”
in the same TD as example (1) will not be an-
notated repeatedly. This is because annotations
are made and used by humans, i.e., assessors and
learners, and persistent indications of identical is-
sues may waste the time of assessors and discour-
age learners. This practice differs from ordinary
linguistic annotation, especially that aiming to de-
velop training data for machine learning methods,
which requires exhaustive annotation of the phe-
nomena of interest within given documents. Al-
though there have been several studies on the use

of partial/incomplete annotation, e.g., Tsuboi et
al. (2008), our procedure is nevertheless different
from these in the sense that we leave issues “un-
annotated” only when identical ones are already
annotated.

5 Intrinsic Evaluation of the Scheme

It is hard to make a fair and unbiased comparison
between different annotation schemes that target
the same phenomena, employing the same asses-
sors. We thus evaluated whether our issue classi-
fication scheme leads to sufficiently high level of
inter-assessor agreement, regarding those poor re-
sults described in §2 as baselines, and analyzed the
tendencies of disagreements and the distribution of
issues.

5.1 Validation 1: Classification of Identified
Issues

5.1.1 Inter-Assessor Agreement
We gauged the consistency of classifying identi-
fied issues by the inter-assessor agreement.

First, three of the authors who developed our
scheme identified erroneous text spans in the 17
TDs (see §3) and made a revision proposal for
each, through discussion. Then, four assessors
were independently asked to classify each of the
resulting 575 issues into one of the 16 issue types,
“other issue,” and “not an issue,” following our de-
cision tree in Table 3. Two of them were anony-
mous paid workers (A and B), while the others (C
and D) were two of the above three authors. All
four assessors were native Japanese speakers with
a strong command of English and an understand-
ing of our scheme and translation-related notions.
While they were asked to adhere to our decision

61



ID Background Agreement ratio [%] Cohen’s κvs A vs B vs C vs D vs A vs B vs C vs D
A Bachelor of Engineering (now translation editor) - 67.7 63.3 57.9 - 0.613 0.554 0.490
B Master of Japanese Pedagogy (now translator) 67.7 - 67.1 61.4 0.613 - 0.592 0.523
C Master of Translation Studies 63.3 67.1 - 86.6 0.554 0.592 - 0.831
D Ph.D in Computational Linguistics 57.9 61.4 86.6 - 0.490 0.523 0.831 -

Table 4: Inter-assessor agreement on the identified 575 issues.

tree, no dictionary or glossary was provided.
Table 4 summarizes the agreement ratio and

Cohen’s κ (Cohen, 1960). The most consistent
pair was C and D who agreed on 86.6% (498/575)
of the issues and achieved almost perfect agree-
ment, κ = 0.831, although it is indisputable that
they had some advantages, having been engaged in
developing the scheme and identifying the issues.
Both of the two measures draw a clear distinc-
tion between the anonymous and identified asses-
sors. As our analysis below illustrates, the anony-
mous workers made many careless mistakes, pre-
sumably because the human resource agency did
not offer substantial incentive to pursue accurate
and consistent annotations. Nevertheless, even the
lowest κ value in our experiment, 0.490, was vis-
ibly higher than those achieved using the typolo-
gies with the same level of granularity but without
a tool for consistent decision making (see §2).

Table 5 shows the most frequent disagreement
patterns between each anonymous worker and the
two authors (C and D) on the 498 issues about
which the authors have agreed. The most typi-
cal disagreement was between X3 (distortion) and
X4b (too literal). For instance, “has passed” in
example (2) was mistakenly translated into “通過
した ([bill] passed [legislature]),” resulting in two
exclusive subjects marked with nominative case
marker “が,” i.e., “各州政府 (state after state)” and
“農業口封じ法 (Ag-Gag laws).”

(2) SD: State after state has passed so-called
Ag-Gag laws.

TD: 各州政府がいわゆる農業口封じ法
が通過した (⇒を可決した)。
([bill] passed [legislature] ⇒ [legisla-
ture] passed [bill])

As the TD does not convey the original meaning in
the SD, both C and D classified this issue into X3
(distortion). In contrast, both A and B regarded
them as X4b (too literal), presumably consider-
ing that both of the original translation “通過した”
and the revision proposal “可決した” were appro-
priate lexical translations for “has passed” when

A, B C&D A B
X4b (Level 4) X3 (Level 2) 37 8
X3 (Level 2) X4b (Level 4) 11 24
X1 (Level 2) X3 (Level 2) 13 10
X1 (Level 2) X4b (Level 4) 6 5
X1 (Level 2) X16 (Level 4) 6 4
X1 (Level 2) X7 (Level 2) 5 4

Table 5: Frequent disagreements between anony-
mous workers (A and B) and two of the authors (C
and D) among the 498 identified issues that C and
D classified consistently.

separated from the context. The above results, and
the fact that X3 (distortion) and X4b (too literal)
also produced the most frequent disagreements be-
tween C and D (11 out of 77 disagreements), sug-
gested that question Q2a in Table 3 should be de-
fined more clearly. We plan to make this precise
in our future work.

The other frequent disagreements concerned the
issues classified as X1 (omission) by A and B,
whereas C and D classified them as other types.
For instance, both C and D classified the issue
in (3) as X3 (distortion) since the original word
“sailors” was incorrectly translated as “soldiers,”
and the issue in (4) as X7 (incorrect translation of
terms) since named entities compose a typical sub-
class of term.

(3) SD: We have filed a class action for approx-
imately a hundred sailors.

TD: およそ 100人の兵士 (⇒海兵兵士)の
ための集団訴訟を起こした。(soldiers
⇒ sailors)

(4) SD: President Ronald Reagan vetoed the
bill, but, . . .

TD: レーガン大統領 (⇒ロナルド・レーガ
ン大統領)はその法案を拒否したが、. . .
(President Reagan ⇒ President Ronald
Reagan)

These disagreements imply that the anonymous
workers might not have strictly adhered to our de-
cision tree, and classified them as X1 after merely

62



Issue type n undergrad. (6) grad. (11)avg. s.d. avg. s.d.
Level 1 X4a 3 0.04 0.11 0.01 0.04

X6 0 0.00 0.00 0.00 0.00
Level 2 X7 33 0.39 0.18 0.26 0.31

X1 53 0.73 0.54 0.34 0.29
X2 28 0.33 0.26 0.26 0.32
X3 240 2.67 1.26 2.24 1.41

Level 3 X8 16 0.19 0.23 0.16 0.24
X10 22 0.27 0.24 0.21 0.32
X11 10 0.13 0.11 0.07 0.11
X12 8 0.11 0.15 0.07 0.11
X13 18 0.21 0.15 0.17 0.20
X9 10 0.08 0.10 0.12 0.12

Level 4 X16 18 0.20 0.23 0.14 0.14
X4b 92 0.87 0.69 1.05 0.93
X15 14 0.09 0.07 0.21 0.25

Level 5 X14 28 0.34 0.14 0.25 0.32
Total 593 6.63 2.35 5.58 3.35

Table 6: Total frequency and relative frequency of
each issue type (macro average and standard devi-
ation over TDs).

comparing the marked text span with the revision
proposal at the surface level.

5.1.2 Coverage of the Issue Typology
Through a discussion, the disagreements between
C and D on the 77 issues were resolved and 18
newly identified issues were also classified. We
then calculated relative frequency RF of each is-
sue type, t, in each TD, d, as follows:

RF (t, d) =
(frequency of t in d)

(# of words in the SD of d)/100
.

Table 6 summarizes the frequency of each issue
type; the “n” column shows their total frequency
across all TDs and the remaining columns com-
pares macro average and standard deviation of the
relative frequencies over TDs produced by each
group of students. All the identified issues were
classified into one of the 16 issue types in our
typology, confirming that the MNH-TT typology
had also covered various types of issues appear-
ing in English-to-Japanese translations produced
by learners. As reviewed in §2 and §4, both of
our typology and the MNH-TT typology cover a
broader range of issues than the MeLLANGE ty-
pology. Thus, we can even insist that our scheme
is applicable to translations between several Eu-
ropean languages that Castagnoli et al. (2006)
have investigated. In our preliminary experiments
on assessing English-to-Chinese and Japanese-to-
Korean translations using our scheme, we have not
observed any novel type of issues.

X3 (distortion) occurred significantly more fre-
quently than the others. This is consistent with the
previous investigation based on the MeLLANGE
typology (Castagnoli et al., 2006), considering
that X3 (distortion) in our typology corresponds to
parts of the most frequent type, LA-TL-IN (Lan-
guage, terminology and lexis, incorrect), and the
second-ranked TR-DI (Transfer, distortion). The
other frequent types were X4b (too literal) and X1
(omission), which are both listed in the two exist-
ing typologies in Table 2, and also frequently ob-
served in the learners’ translations between Euro-
pean languages (Castagnoli et al., 2006).

The annotation results revealed that the gradu-
ate students produced issues at Level 2 less fre-
quently than the undergraduate students, while
producing more Level 4 issues. Although the rela-
tive frequencies of issues vary greatly between in-
dividuals, we speculate that less experienced stu-
dents are more likely to struggle at Level 2, i.e.,
properly understanding content in SDs.

5.2 Validation 2: Annotation by a Novice
Learner Translator

We also evaluated our issue classification scheme
in a more realistic setting: the comparison of an
undergraduate learner translator with an experi-
enced assessor.

The learner involved in this experiment, re-
ferred to as assessor E, was also a native Japanese
speaker and had attended some translation classes
at a university. The other assessor was D, who had
participated in the first experiment. The two as-
sessors separately identified erroneous text spans
in the ten TDs (see §3) with a revision proposal,
and classified them following our decision tree.

As a result, D and E respectively annotated
561 and 406 issues. Among these, 340 were for
identical text spans, with not necessarily identi-
cal but similar revision proposals. They consis-
tently classified 289 issues out of 340 (85.0%),
achieving a substantial and notably high agree-
ment, κ = 0.794. These are substantially higher
than those achieved by the anonymous workers
A and B (see Table 4), although they worked on
different TDs. This fact indicates that the iden-
tified assessors in the first experiment (C and D)
did not necessarily have an advantage. More im-
portantly, this experiment verified the understand-
ability of our scheme by actual learner translators.
We expect that learner translators would be able

63



D E # of issues
X4b (Level 4) X3 (Level 2) 6
X3 (Level 2) X15 (Level 4) 6
X4b (Level 4) X15 (Level 2) 5
X1 (Level 2) X3 (Level 2) 3

Table 7: Frequent disagreements between a
learner translator (E) and one of the authors (D)
among the 340 issues they identified consistently.

to perform peer reviewing of their draft transla-
tions, once they have acquired a certain level of
understanding of our scheme. Consequently, as
Kiraly (2000) mentions, they would be able to ef-
fectively develop their translation skills through
playing various roles in the translation work-flow,
including that of assessor.

Typical disagreement patterns are shown in
Table 7. Similarly to the first experiment, dis-
agreement between X3 (distortion) and X4b (too
literal) was frequently observed. E also classified
as X15 (clumsy) 11 issues which D classified as
X3 (distortion) or X4b (too literal). To answer
question Q4c consistently, the literalness needs to
be confirmed, for instance, by using dictionaries.

There were 221 and 66 issues identified only
by D or E, respectively; 171 and 41 out of these
were ignored by the other, including missed issues
and accepted translations, reflecting the different
levels of sensitivity of the assessors. The other
38 and 14 mismatches suggested the necessity of
a guideline to consistently annotate single issues.
For instance, E identified one X3 (distortion) is-
sue in (5), while D annotated two issues there: “情
報が豊富な (with rich information)” as X2 (addi-
tion) and “お天気アプリ (weather application)” as
X3 (distortion).

(5) SD: I put the question to Jeff Masters,
co-founder at Weather Underground,
an Internet weather information service.

TD: 情報量が豊富なお天気アプリ (⇒ 気
象情報を提供するウェブサービス)、ウ
ェザー・アンダーグラウンドの共同設立
者であるジェフ・マスターズ氏に質問を
投げかけた。(a weather application with
rich information ⇒ a Web service which
provides weather information)

6 Translation Exercise at a University

Having created and validated an annotation
scheme, we should ultimately verify its usefulness
in actual practice. We implemented our scheme in

an English-to-Japanese translation exercise course
for undergraduate students at a university in Japan.

6.1 Course Design

Two different types of English texts were used:
travel guides from “Travellerspoint”9 (henceforth,
“Travel”) and columns from “Democracy Now!”
as in §3 (henceforth, “Column”). For each text
type, the instructor of the course sampled three
documents with similar conceptual and linguis-
tic difficulties, and excerpted roughly the first 550
words of each document as SDs.

A total of 27 undergraduate students partici-
pated in the course held over 15 weeks, from April
to July 2015. All of them were native Japanese
speakers; eight had attended translation classes
at a university, while the other 19 were absolute
novices. Each student selected one of the sampled
SDs for each text type. Before starting transla-
tion, they prepared a glossary and collected back-
ground information by themselves, and the in-
structor added any missing information. Each stu-
dent first translated a “Travel” SD into Japanese
over six weeks, referring to the corresponding
glossary and background information, and then a
“Column” SD in the same manner.

During the process of translating one SD, stu-
dents’ translations were assessed every two weeks
(three times per SD); a teaching assistant iden-
tified erroneous text spans with a revision pro-
posal, and classified them following our decision
tree; and then the instructor double-checked them.
While the identified erroneous text spans and the
assigned issue types were fed back to the stu-
dents, revision proposals were not shown (Klaudy,
1996). When the instructor fed back the assess-
ment results to the students, she also explained
our issue typology (Table 2) and decision tree
(Table 3), also using the examples collected dur-
ing the development process.

6.2 Observations

Through the course, 54 TDs were annotated with
1,707 issues, all of which fell into one of the
16 types in our issue typology. Table 8 summa-
rizes the relative frequency of each issue type. X3
(distortion) occurred significantly more frequently
than the others in translating both types of SDs,
as in the results in Table 6 and previous work
(Castagnoli et al., 2006). In other words, transfer-

9http://www.travellerspoint.com/

64



Issue type Travel Columnavg. s.d. avg. s.d.
Level 1 X4a 0.03 0.07 0.03 0.07

X6 0.00 0.00 0.00 0.00
Level 2 X7 1.14 0.79 0.53 0.39

X1 0.47 0.42 0.49 0.31
X2 0.09 0.13 0.16 0.19
X3 2.20 0.95 2.91 1.03

Level 3 X8 0.11 0.11 0.13 0.18
X10 0.19 0.24 0.18 0.27
X11 0.07 0.09 0.12 0.15
X12 0.15 0.16 0.11 0.11
X13 0.09 0.16 0.17 0.21
X9 0.22 0.34 0.03 0.08

Level 4 X16 0.07 0.11 0.03 0.07
X4b 0.53 0.53 0.24 0.22
X15 0.10 0.18 0.08 0.13

Level 5 X14 0.30 0.25 0.45 0.31
Total 5.76 2.17 5.65 1.84

Table 8: Relative frequency of each issue type
(macro average and standard deviation over TDs).

ring content of the given SD is the principal issue
for learner translators in general.

Table 8 also highlights that the relative frequen-
cies of X7 (incorrect translations of terms) and
X4b (too literal) are drastically different for the
“Travel” and “Column” SDs. A student-wise com-
parison of the relative frequencies in Figure 1 re-
vealed that the students who made these two types
of issues more frequently in translating “Travel”
SDs (shown in the right-hand side in the figure)
produced these types of issues significantly less
frequently during translating “Column” SDs. Due
to the difference in text types, we cannot claim
that this demonstrates students’ growth in learn-
ing to translate and that this has been promoted by
our scheme. Nevertheless, our scheme is clearly
useful for quantifying the characteristics of such
students.

7 Conclusion

To consistently assess human translations, es-
pecially focusing on English-to-Japanese trans-
lations produced by learners, we manually cre-
ated an improved issue typology accompanied
by a decision tree through an application of the
OntoNotes method. Two annotation experiments,
involving four assessors and an actual learner
translator, confirmed the potential contribution of
our scheme to making consistent classification of
identified issues, achieving Cohen’s κ values of
between 0.490 (moderate) to 0.831 (almost per-
fect). We also used our scheme in a translation
exercise course at a university in order to assess

 0

 0.5

 1

 1.5

 0  0.5  1  1.5  2  2.5  3

R
e
la

ti
v
e
 f
re

q
u
e
n
c
y
 o

f 
is

s
u
e
s

 (
C

o
lu

m
n
)

Relative frequency of issues (Travel)

X7
X4b

Figure 1: Student-wise comparison of relative fre-
quencies of X7 (incorrect translations of terms)
and X4b (too literal).

learners’ translations. The predefined 16 issue
types in our typology covered all the issues that
appeared in English-to-Japanese translations pro-
duced by undergraduate students, supporting the
applicability of our issue typology to real-world
translation training scenarios.

Our plans for future work include further im-
provements of our issue classification scheme,
such as clarifying questions in the decision tree
and establishing a guideline for annotating single
issues. Its applicability will further be validated
using other text types and other language pairs.
From the pedagogical point of view, monitoring
the effects of assessment is also important (Orozco
and Hurtado Albir, 2002). Given the high agree-
ment ratio in our second experiment (§5.2), we are
also interested in the feasibility of peer reviewing
(Kiraly, 2000). Last but not least, with a view
to efficient assessment with less human labor, we
will also study automatic identification and classi-
fication of erroneous text spans, referring to recent
advances in the field of word- and phrase-level
quality estimation for MT outputs.10

Acknowledgments

We are deeply grateful to anonymous reviewers
for their valuable comments. This work was partly
supported by JSPS KAKENHI Grant-in-Aid for
Scientific Research (A) 25240051.

References
Bogdan Babych, Anthony Hartley, Kyo Kageura, Mar-

tin Thomas, and Masao Utiyama. 2012. MNH-TT:
a collaborative platform for translator training. In
Proceedings of Translating and the Computer 34.
10http://www.statmt.org/wmt16/

quality-estimation-task.html

65



Aljoscha Burchardt and Arle Lommel. 2014. QT-
LaunchPad supplement 1: Practical guidelines for
the use of MQM in scientific research on transla-
tion quality. http://www.qt21.eu/downloads/MQM-
usage-guidelines.pdf.

Sara Castagnoli, Dragos Ciobanu, Kerstin Kunz, Na-
talie Kübler, and Alexandra Volanschi. 2006. De-
signing a learner translator corpus for training pur-
pose. In Proceedings of the 7th International Con-
ference on Teaching and Language Corpora.

Jacob Cohen. 1960. A coefficient of agreement
for nominal scales. Educational and Psychological
Measurement, 20(1):37–46.

Eduard Hovy, Mitchell Marcus, Martha Palmer,
Lance Ramshaw, and Ralph Weischedel. 2006.
OntoNotes: The 90% solution. In Proceedings
of the Human Language Technology Conference of
the North American Chapter of the Association for
Computational Linguistics (HLT-NAACL) Short Pa-
pers, pages 57–60.

ISO/TC27. 2015. ISO 17100:2015 translation ser-
vices: Requirements for translation services.

Donald Kiraly. 2000. A Social Constructivist Ap-
proach to Translator Education: Empowerment
from Theory to Practice. Routledge.

Kinga Klaudy. 1996. Quality assessment in school
vs professional translation. In Cay Dollerup and
Vibeke Appel, editors, Teaching Translation and
Interpreting 3: New Horizons: Papers from the
Third Language International Conference, pages
197–203. John Benjamins.

Arle Lommel, Maja Popović, and Aljoscha Burchardt.
2014. Assessing inter-annotator agreement for
translation error annotation. In Proceedings of the
LREC MTE Workshop on Automatic and Manual
Metrics for Operational Translation Evaluation.

Arle Lommel, Attila Görög, Alan Melby, Hans
Uszkoreit, Aljoscha Burchardt, and Maja
Popović. 2015. QT21 deliverable 3.1:
Harmonised metric. http://www.qt21.eu/wp-
content/uploads/2015/11/QT21-D3-1.pdf.

Brian Mossop. 2014. Revising and Editing for Trans-
lators (3rd Edition). Routledge.

Mariana Orozco and Amparo Hurtado Albir. 2002.
Measuring translation competence acquisition.
Translators’ Journal, 47(3):375–402.

Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: A method for automatic
evaluation of machine translation. In Proceedings
of the 40th Annual Meeting of the Association for
Computational Linguistics (ACL), pages 311–318.

Alina Secarǎ. 2005. Translation evaluation: A state
of the art survey. In Proceedings of the eCoL-
oRe/MeLLANGE Workshop, pages 39–44.

Yuta Tsuboi, Hisashi Kashima, Shinsuke Mori, Hi-
roki Oda, and Yuji Matsumoto. 2008. Training
conditional random fields using incomplete annota-
tions. In Proceedings of the 22nd International Con-
ference on Computational Linguistics (COLING),
pages 897–904.

66


