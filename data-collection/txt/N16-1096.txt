



















































Mapping Verbs in Different Languages to Knowledge Base Relations using Web Text as Interlingua


Proceedings of NAACL-HLT 2016, pages 818–827,
San Diego, California, June 12-17, 2016. c©2016 Association for Computational Linguistics

Mapping Verbs In Different Languages to Knowledge Base Relations
using Web Text as Interlingua

Derry Tanti Wijaya
Carnegie Mellon University

5000 Forbes Avenue
Pittsburgh, PA, 15213

dwijaya@cs.cmu.edu

Tom M. Mitchell
Carnegie Mellon University

5000 Forbes Avenue
Pittsburgh, PA, 15213

tom.mitchell@cs.cmu.edu

Abstract

In recent years many knowledge bases (KBs)
have been constructed, yet there is not yet
a verb resource that maps to these growing
KB resources. A resource that maps verbs
in different languages to KB relations would
be useful for extracting facts from text into
the KBs, and to aid alignment and integration
of knowledge across different KBs and lan-
guages. Such a multi-lingual verb resource
would also be useful for tasks such as machine
translation and machine reading. In this pa-
per, we present a scalable approach to auto-
matically construct such a verb resource us-
ing a very large web text corpus as a kind of
interlingua to relate verb phrases to KB rela-
tions. Given a text corpus in any language
and any KB, it can produce a mapping of that
language’s verb phrases to the KB relations.
Experiments with the English NELL KB and
ClueWeb corpus show that the learned English
verb-to-relation mapping is effective for ex-
tracting relation instances from English text.
When applied to a Portuguese NELL KB and
a Portuguese text corpus, the same method au-
tomatically constructs a verb resource in Por-
tuguese that is effective for extracting relation
instances from Portuguese text.

1 Introduction

In recent years a variety of large knowledge bases
(KBs) have been constructed e.g., Freebase (Bol-
lacker et al., 2008), DBpedia (Auer et al., 2007),
NELL (Carlson et al., 2010), and Yago (Suchanek
et al., 2007). These KBs consist of (1) an on-
tology that defines a set of categories (e.g., Sport-

sTeam, City), (2) another part of the ontology that
defines relations with these categories as argument
types (e.g., teamPlaysInCity(SportsTeam, City)),
(3) KB entities which instantiate these categories
(e.g., Steelers ∈ SportsTeam), and (4) KB entity
pairs which instantiate these relations (e.g., (Steel-
ers, Pittsburgh) ∈ teamPlaysInCity). The KB on-
tology also specifies constraints (e.g., mutual exclu-
sion, subset) among KB categories and relations.

Despite recent progress in KB construction, there
is not yet a verb resource that maps to these KBs:
one that contains verb phrases1 that identify KB re-
lations. Such a verb resource can be useful to aid KB
relation extraction. A distribution of verb phrases
associated with any given KB relation is also a KB-
independent representation of that relation’s seman-
tics which can form the basis of aligning ontologies
across arbitrary KBs (Wijaya et al., 2013). Given
a KB and verb resources in different languages that
map to the KB, we can also begin to align knowl-
edge expressed in different languages.

We introduce here an approach to mapping verb
phrases to KB relations using a very large ClueWeb
corpus (Callan et al., 2009) as a kind of interlin-
gua. Our approach grounds each KB relation in-
stance (e.g., teamPlaysInCity(Steelers, Pittsburgh))
in mentions of its argument pair in this text, then
represents the relation in terms of the verb phrases
that connect these paired mentions (see Fig. 1). For
a high coverage mapping, we train on both labelled
and unlabelled data using expectation maximization
(EM). We introduce argument type checking during

1In this paper we use the term “verb phrase” and “verb” in-
terchangeably; both referring to either verb or verb+preposition

818



Figure 1: Mapping verb phrases to relations in KB through

Web-text as interlingua. Each relation instance is grounded by

its mentions in the Web-text. The verbs that co-occur with men-

tions of the relation’s instances are mapped to that relation.

the EM process to ensure only verbs whose argu-
ment types match the relation’s argument types are
mapped to the relation. We also incorporate con-
straints defined in the KB ontology to find a verb to
relation mapping consistent with these constraints.

Our contributions are: (1) We propose a scal-
able EM-based method that automatically maps verb
phrases to KB relations by using the mentions of
the verb phrases with the relation instances in a
very large unlabeled text corpus. (2) We demon-
strate the effectiveness of the resource for extract-
ing relation instances in NELL KB. Specifically,
it improves the recall of both the supervised- and
the unsupervised- verb-to-relation mapping; demon-
strating the benefit of semi-supervised learning on
unlabeled Web-scale text. (3) We demonstrate the
flexibility of the method, which is both KB- and
language-independent, by using the same method
for constructing English verb resource to automat-
ically construct a Portuguese verb resource. (4) We
make our verb resources publicly available 2.

2 Method

2.1 Terminology

We define a NELL KB to be a 6-tuple
(C, IC , R, IR, Subset,Mutex). C is the set of cate-
gories e.g., SportsTeam i.e., cj ∈ C = {c1, ..., c|C|}.
IC is the set of category instances which are

2http://www.cs.cmu.edu/%7Edwijaya/mapping.html

entity-category pairs e.g., (Cleveland, City) i.e., IC
= {(em, cj) | em ∈ cj , cj ∈ C}.

R is the set of relations e.g., teamPlaysInCity
i.e., ri ∈ R = {r1, ..., r|R|}. We also define ftype
to be a function that when applied to a relation
ri returns the argument type signature of the rela-
tion ftype(ri) = (cj , ck) for some cj , ck ∈ C e.g.,
ftype(teamPlaysInCity) = (SportsTeam, City).

IR is the set of relation instances which
are entity-relation-entity triples e.g., (Cava-
liers, teamPlaysInCity, Cleveland) i.e., IR =
{(em,ri,en) | (em, en) ∈ ri, ri ∈ R, em ∈ cj , en ∈
ck, ftype(ri) = (cj , ck)}; IR = Ir1 ∪ Ir2 ∪ ... Ir|R| .

Subset is the set of all subset constraints among
relations in R i.e., Subset = {(i, k) : Iri ⊆
Irk}. For example {(person, ceoOf, company)} ⊆
{(person, worksFor, company)}.

Mutex is the set of all mutual exclusion con-
straints among relations in R i.e., Mutex = {(i, k) :
Iri ∩ Irk = φ}. For example {(drug, hasSideEffect,
physiologicalCondition)} ∩ {(drug, possiblyTreats,
physiologicalCondition)} = φ.

Each KB entity em can be referred to by one or
more noun phrases (NPs). For example, the entity
Cavaliers, can be referred to in text using either the
NP “Cleveland Cavaliers” or the NP “The Cavs”3.
We define Nen(em) to be the set of English NPs cor-
responding to entity em.

We define SV O to be the English Subject-Verb-
Object (SVO) interlingua4 consisting of tuples of the
form (nps, vp, npo, w), where nps and npo are noun
phrases (NP) corresponding to subject and object,
respectively, vp is a verb phrase that connects them,
and w is the count of the tuple.

2.2 Data Construction

We construct a dataset D for mapping English verbs
to NELL KB relations. First, we convert each tu-
ple in SV O to its equivalent entity pair tuple(s) in
SV O′ = {(em, vp, en, w) | nps ∈ Nen(em), npo ∈
Nen(en), (nps, vp, npo, w) ∈ SV O}. Then, we
construct D from SV O′ as a collection of labeled
and unlabeled instances.

3defined by the canReferTo relation in NELL KB
4We use 600 million SVO triples collected from the entire

ClueWeb (Callan et al., 2009) of about 230 billion tokens with
some filtering described in Section 3.1.

819



The set of labeled instances is D` =
{(y(em,en), v(em,en))} where y(em,en) ∈ {0, 1}

|R|

is a bit vector of label assignment, each bit repre-
senting whether the instance belongs to a particular
relation i.e., yi(em,en) = 1 ⇐⇒ (em, en) ∈ ri and

0 otherwise. v(em,en) ∈ R
|V | is a |V |-dimensional

vector of verb phrase counts that connect em and
en in SV O′ (V is the set of all verb phrases) i.e.,
vp(em,en) is the number of times the verb phrase vp
connects em and en in SV O′.

The collection of unlabeled instances is con-
structed from entity pairs in SV O′ whose label
assignment y are unknown (its bits are all zero)
i.e., Du = {(y(em,en), v(em,en)) | (em, ∗, en, ∗) ∈
SV O′, (em, ∗, en) /∈ IR}.

An instance in our dataset d(em,en) ∈ D is
therefore either a labeled or unlabeled tuple i.e.,
d(em,en) = (y(em,en), v(em,en)).

We let ftype(d(em,en)) return the argument type of
the instance i.e., ftype(d(em,en)) = (cj , ck) where
(em, cj) and (en, ck) ∈ IC .

We let fverb(d(em,en)) return the set of all verb
phrases that co-occur with the instance in SV O′ i.e.,
fverb(d(em,en)) = {vp | (em, vp, en, ∗) ∈ SV O

′}.
When applied to a relation ri, we let fverb(ri) re-

turn the set of all verb phrases that co-occur with
instances in D whose types match that of the rela-
tion i.e., fverb(ri) = {vp | ∃ d(em,en) ∈ D, vp ∈
fverb(d(em,en)), ftype(d(em,en)) = ftype(ri)}.

2.3 Model

We train a Naive Bayes classifier on our dataset.
Given as input a collection D` of labeled instances
and Du of unlabeled instances, it outputs a classi-
fier, θ̂, that takes an unlabeled instance and predicts
its label assignment i.e., for each unlabeled instance
d(em,en) ∈ D

u the classifier predicts the label as-
signment y(em,en) using v(em,en) as features:

P (yi(em,en) = 1 | d(em,en); θ̂)

=
P (ri|θ̂)P (d(em,en)| ri; θ̂)

P (d(em,en)|θ̂)

=

P (ri|θ̂)
|V |∏

p=1
P (vp|ri; θ̂)

v
p
(em,en)

|R|∑

k=1
P (rk|θ̂)

|V |∏

p=1
P (vp|rk; θ̂)

v
p
(em,en)

(1)

If the task is to classify the unlabeled instance into
a single relation, only the bit of the relation with the
highest posterior probability is set i.e, yk(em,en) = 1

where k = arg maxi P (y
i
(em,en)

= 1 | d(em,en); θ̂).

2.3.1 Parameter Estimation

To estimate model parameters (the relation
prior probabilities θ̂ri ≡ P (ri|θ̂) and prob-
abilities of a verb given a relation θ̂vp|ri ≡

P (vp|ri; θ̂)) from both labeled and unlabeled data,
we use an Expectation Maximization (EM) algo-
rithm (Nigam et al., 2006). The estimates are
computed by calculating a maximum a posteriori
estimate of θ, i.e. θ̂ = arg maxθ L(θ|D) =
arg maxθ log(P (D | θ)P (θ)).

The first term, P (D | θ) is calculated by the prod-
uct of all the instance likelihoods:

P (D | θ)

=
∏

d(em,en)∈D
u

|R|∑

i=1

P (ri|θ)P (d(em,en)|ri; θ)

×
∏

d(em,en)∈D
`

∑

{i|yi
(em,en)

=1}

P (ri|θ)P (d(em,en)|ri; θ)

(2)

The second term, P (θ), the prior distribution
over parameters is represented by Dirichlet priors:

P (θ) ∝
|R|∏

i=1
((θri)

α1−1
|V |∏

p=1
(θvp|ri)

α2−1) where α1

and α2 are parameters that effect the strength of
the priors. In this paper we set α1 = 2 and α2 =
1 + σ(P e(vp|ri)), where P e(vp|ri) is the initial bias
of the verb-to-relation mapping. Thus, in this paper
we define P (θ) as:

P (θ) =

|R|∏

i=1

(P (ri|θ)
|V |∏

p=1

(P (vp|ri; θ)
σ(P e(vp|ri))) (3)

We can see from this that σ(P e(vp|ri)) is a con-
jugate prior on P (vp|ri; θ) with σ as the confidence
parameter. This conjugate prior allows incorpora-
tion of any existing knowledge (Section 2.3.2) we
may have about the verb-to-relation mapping.

From Equation 2, we see that log P (D|θ)
contains a log of sums, which makes a maxi-
mization by partial derivatives computationally in-
tractable. Using EM, we instead maximize the ex-
pected log likelihood of the data with respect to

820



the posterior distribution of the y labels given by:
arg maxθ E(y|D;θ)[log P (D|θ)].

In the E-step, we use the current estimates of the
parameters θ̂t to compute ŷt = E[y|D; θ̂t] the ex-
pected label assignments according to the current
model. In practice it corresponds to calculating the
posterior distribution over the y labels for unlabeled
instances P (yi(em,en) = 1 | d(em,en); θ̂

t) (Equation
1) and using the estimates to compute its expected
label assignment ŷt(em,en).

In the M-step, we calculate a new maximum
a posteriori estimate for θ̂(t+1) which maximizes
the expected log likelihood of the complete data,
Lc(θ|D; ŷt) = log(P (θt)) + ŷt [log P (D|θt)]:

Lc(θ|D; ŷt) = log(P (θt))

+
∑

d(em,en)∈D

|R|∑

i=1

yti(em,en) log P (ri|θ)P (d(em,en)|ri; θ)

(4)

Lc(θ|D; y) bounds L(θ|D) from below (by ap-
plication of Jensen’s inequality E[log(X)] ≤
log(EX)). The EM algorithm produces parameter
estimates θ̂ that correspond to a local maximum of
Lc(θ|D; y). The relation prior probabilities are thus
estimated using current label assignments as:

P (ri|θ̂
(t+1)) =

1 +
∑

d(em,en)∈D
yti
(em,en)

|R| + |D|
(5)

The verb-to-relation mapping probabilities are es-
timated in the same manner:

P (vp | ri; θ̂
(t+1)) =

σ
(t+1)
i (P

e(vp | ri)) +
∑

d(em,en)∈D
vp
(em,en)

yti
(em,en)

σ
(t+1)
i +

|V |∑

s=1

∑

d(em,en)∈D
vs
(em,en)

yti
(em,en)

(6)

We start with σ = |V | and gradually reduce the
impact of prior by decaying σ with a decay parame-
ter of 0.8 at each iteration in the manner of (Lu and
Zhai, 2008)). This will allow the EM to gradually
pick up more verbs from the data to map to relations.

EM iteratively computes parameters θ1, ..., θt us-
ing the above E-step and M-step update rule at each
iteration t, halting when there is no further improve-
ment in the value of Lc(θ|D; y).

2.3.2 Prior Knowledge

In our prior P (θ), we incorporate knowledge
about verb-to-relation mappings from the text pat-
terns learned by NELL to extract relations. This is
our way of aligning our verb-to-relation mappings
with NELL’s current extractions. Coupled Pattern
Learner (CPL) (Carlson et al., 2010) is a component
in NELL that learns these contextual patterns for ex-
tracting instances of relations and categories.

We consider only CPL’s extraction patterns that
contain verb phrases. Given a set Eri of CPL’s ex-
traction patterns for a relation ri, and Eri,vp as the
set of extraction patterns in Eri that contains the

verb phrase vp, we compute P e(vp | ri) =
| Eri,vp |
| Eri |

and use them as priors in our classifier (Equation 3).5

2.3.3 Argument Type Checking

Although some verbs are ambiguous (e.g., the
verb “play” may express several relations: mu-
sicianPlaysMusicalInstrument, athletePlaysSport,
actorPlaysMovie, etc), knowing the types of the
verbs’ arguments can help disambiguate the verbs
(e.g., the verb “play” that takes a musicalInstru-
ment type as object is more likely to express the
musicianPlaysMusicalInstrument relation). There-
fore, we incorporate argument type checking in our
EM process to ensure that it maps verbs to relations
whose argument types match:

• In the E-Step, we make sure that unlabeled
instances are only labeled with relations that
have the same argument types as the instance
and that share some verbs with the instance.
In other words, in the E-step we compute
P (yi(em,en) = 1 | d(em,en)) if ftype(ri) =

ftype(d(em,en)) and
(
f(verb)(ri)∪ {vp|Eri,vp 6=

∅}
)
∩ f(verb)(d(em,en)) 6= ∅.

• In the M-step, we make sure that verbs are
only mapped to relations whose argument types
match at least one of the instances that co-occur
with the verbs in SV O′. In other words, in the
M-step we compute P (vp | ri) if vp ∈ fverb(ri)
or Eri,vp 6= ∅.

5We manually add a few verb phrases for relations whose Er
is an empty set when possible, to set the EM process on these
relations with good initial guesses of the parameters. In average,
each relation has about 6 verb patterns in total as priors.

821



2.3.4 Incorporating Constraints

In the E-step, for each unlabeled instance, given
the probabilities over relation labels P (yi(em,en) =

1 | d(em,en); θ̂
t), and Subset and Mutex con-

straints6, similar to (Dalvi et al., 2015), we use a
Mixed-Integer Program (MIP) to produce its bit vec-
tor of label assignment as output: ŷt(em,en).

The constraints among relations are incorporated
as constraints on bits in this bit vector. For exam-
ple, if for an unlabeled instance (Jeff Bezos, Ama-
zon), a bit corresponding to the relation ceoOf is set
then the bit corresponding to the relation worksFor
should also be set due to the subset constraint: ceoOf
⊆ worksFor. For the same instance, the bit cor-
responding to competesWith should not be set due
to the mutual exclusion constraint ceoOf ∩ com-
petesWith = φ. The MIP formulation for each un-
labeled instance thus tries to maximize the sum of
probabilities of selected relation labels after penaliz-
ing for violation of constraints (Equation 7), where
ζik are slack variables for Subset constraints and δik
are slack variables for Mutex constraints:

maximize
y(em,en),ζik,δik

( |R|∑

i=1

yi(em,en) × P (y
i
(em,en)

= 1|d(em,en); θ̂
t)

−
∑

(i,k)∈Subset

ζik −
∑

(i,k)∈Mutex

δik

)

subject to,

yi(em,en) ≤ y
k
(em,en)

+ ζik, ∀(i, k) ∈ Subset

yi(em,en) + y
k
(em,en)

≤ 1 + δik, ∀(i, k) ∈ Mutex

ζik, δik ≥ 0, y
i
(em,en)

∈ {0, 1}, ∀i, k (7)

Our algorithm that includes argument type check-
ing and constraints is summarized in Algorithm 1.

2.4 Portuguese Verb Mapping

To map Portuguese verbs to relations in Portuguese
NELL, which is an automatically and independently
constructed KB separate from English NELL, we
use the Portuguese NELL and Portuguese text cor-
pus SV Opt7 and construct a dataset Dpt. Given

6The Subset and Mutex constraints are obtained
as part of the NELL KB ontology, which is publicly
available at the NELL Read The Web project website:
http://rtw.ml.cmu.edu/resources/.

7We obtain the Portuguese SVO from the NELL-Portuguese
team at Federal University of Sao Carlos.

Algorithm 1 The EM Algorithm for Verb-to-Relation Mapping
Input: D = D` ∪ Du and an initial naive Bayes classifier θ1 from

labeled documents D` only (using Equations 5 and 6)
Output: θT that include verbs to relations mappings given by

P (vp|ri; θT )
1: for t = 1 ... T do
2: E-Step:
3: for d(em,en) ∈ D

u do
4: Compute P (yi

(em,en)
= 1|d(em,en); θ

t) ∀ri ∈ R that
satisfy argument types checking (Equation 1)

5: Find a consistent label assignment yt
(em,en)

by solving
MIP (Equation 7)

6: end for
7: M-step: Recompute model parameters θt+1 based on current

label assignments (Equation 5 and 6) respecting argument type
checking

8: if convergence (Lc(θt+1), Lc(θt)) then
9: break

10: end if
11: end for
12: return θT

English Portuguese Portuguese
NELL NELL NELL+en

|R| 317 302 302
|IR| 135,267 5,675 12,444
|D`| 85,192 2,595 5,412
|Du| 240,490 595,274 1,186,329

Table 1: Statistics of KB facts and dataset constructed

Dpt, we follow the same approach as before to find
a mapping of Portuguese verbs to relations. Since
Portuguese NELL is newly constructed, it contains
fewer facts (category and relation instances) than
English NELL, and hence its dataset D`pt has fewer
labeled instances (see Table 1).

Adding more relation instances to Portuguese
NELL can result in more labeled instances in the
dataset Dpt, a more productive EM, and a better
verb-to-relation mapping. Since each category and
each relation in Portuguese NELL ontology has a
one-to-one mapping in English NELL ontology, we
can add relation instances to Portuguese NELL from
the corresponding English NELL relations.

English NELL however, has only English noun
phrases (NPs) to refer to entities in its relation in-
stances. To add more labeled instances in Dpt us-
ing English relation instances, we need to find in-
stantiations of these English relation instances in
Portuguese SV Opt, which translates to finding Por-
tuguese NPs that refer to English NELL entities. For
example, Portuguese NP: “Artria torcica interna” for
English NELL entity: internal mammary artery.

To automatically translate English NELL enti-

822



Figure 2: Mapping NELL entity Brad Pitt to DBPedia.

ties to Portuguese NPs, we use DBPedia (Auer et
al., 2007) which has structured information about
Wikipedia pages in many languages. The idea is
to map each English NELL entity em to its corre-
sponding English DBPedia page and therefore its
Portuguese DBPedia page8. We use the structured
information of the Portuguese page in DBPedia: its
title and label as the set of Portuguese NPs corre-
sponding to the English entity, Npt(em).

More specifically, for each English NELL en-
tity em with English NPs that can refer to it,
Nen(em), we find candidate English DBPedia pages
that can refer to the entity. We do this by com-
puting Jaccard similarities (Jaccard, 1912; Chap-
man, 2009) of the entity’s NPs with titles and la-
bels of English DBPedia pages. We select pages
with Jaccard similarities of more than 0.6 as can-
didates e.g., for English NELL entity Brad Pitt we
find candidate English pages: http://dbpedia.
org/page/Brad_Pitt (Brad Pitt, the US actor)
and http://dbpedia.org/page/Brad_Pitt_
(boxer) (Brad Pit, the Australian boxer).

Then, we construct a graph containing nodes that
are: (1) the NELL entity that we want to map to
DBPedia, (2) its candidate DBPedia pages, (3) other
entities that have relations to the entity in NELL KB,
and (4) the candidate DBPedia pages of these other
entities (see Fig. 2 for the NELL entity Brad Pitt).

We add as edges to this graph: (1) the can-refer-
to edges between entities in NELL and their can-
didate pages in DBPedia (dashed edges in Fig. 2),
(2) the relation edges between the entities in NELL
KB (black edges), and (3) the hyperlink edges be-

8Almost every DBPedia English page has a corresponding
Portuguese page

tween the pages in DBPedia (gray edges). In this
graph we want to use the knowledge that NELL has
already learned about the entity to narrow its candi-
dates down to the page that the entity refers to. The
idea is that relatedness among the entities in NELL
implies relatedness among the DBPedia pages that
refer to the entities. We use Personalized Page Rank
(Page et al., 1999) to rank candidate DBPedia pages
in this graph and pick the top ranked page as the
page that can refer to the NELL entity.

For example, to find the DBPedia page that can
refer to our NELL entity Brad Pitt, we use NELL’s
knowledge about this entity to rank its candidate
pages. As seen in Fig. 2, DBPedia page of Brad
Pitt, the US actor (dbpedia:brad pitt) is highly con-
nected to other pages (dbpedia:angelina jolie, db-
pedia:douglas pitt, dbpedia:usa) that are in turn
connected to the NELL entity Brad Pitt. dbpe-
dia:brad pitt is thus ranked highest and picked as
the page that can refer to the NELL entity Brad Pitt.

Once we have an English DBPedia page that can
refer to the NELL entity em, we can obtain the cor-
responding Portuguese page from DBPedia. The ti-
tle and label of the Portuguese page becomes the set
of Portuguese NPs that can refer to the NELL en-
tity i.e., Npt(em) (see Table 2 for examples). Us-
ing Npt(em) we find instantiations of English re-
lation instances in SV Opt to add as labeled in-
stances in Dpt. Portuguese NELL enriched with En-
glish NELL (i.e., Portuguese NELL+en) has more
than double the amount of relation instances, la-
beled and unlabeled instances (Table 1) than Por-
tuguese NELL. In the experiments, we observe that
this translates to a better verb-to-relation mapping.

Mapping NELL to DBPedia is also useful because
it can align existing knowledge and add new knowl-
edge to NELL. For example, by mapping to DBPe-
dia, we can resolve abbreviations (e.g., the NELL
entity: COO as “Chief Operations Officer” in En-
glish or “Diretor de Operações” in Portuguese), or
resolve a person entity (e.g., the NELL entity: Uta-
maro as “Kitagawa Utamaro”, the virtual artist).

3 Experiments

3.1 Pre-processing

For better coverage of verbs, we lemmatize verbs in
the English SV O (using Stanford CoreNLP (Man-

823



English NELL entity Portuguese NPs
Amazonian Brown Brocket “Veado-Roxo”, “Fuboca”

COO “Diretor de Operações”
Utamaro “Kitagawa Utamaro”

Notopteridae “Peixe-faca”
1967 Arab Israeli War “Guerra dos Seis Dias”,

“Guerra de 1967”
Food Products “Produtos Alimenticios”,

“Alimento”, “Comida”, ...

Table 2: Example Portuguese NPs learned for NELL entities

ning et al., 2014)). We lemmatize verbs in Por-
tuguese SV Opt (using LemPORT (Rodrigues et al.,
2014)) and expand contracted prepositions.

For better precision and to make our method scale
to a large text corpus, we focus on mapping verbs
that are important for a relation based on how often
the verbs co-occur with entity pairs that match the
relation’s argument type. For each argument type in
the English SV O we consider only the top 50 verbs
(in terms of tf-idf scores) for mapping. We use tf-idf
scores to adjust for the fact that some verbs appear
more frequently in general. For each of these verbs,
we also use only the top 50 entity pairs that co-occur
with the verb in the SV O (in terms of co-occurrence
counts) to construct our dataset D.

For Portuguese verb-to-relation mapping, since
SV Opt is much smaller than the English SV O (i.e.,
it contains only about 22 million entity pair-verb
triples compared to the 600 million triples in the En-
glish SV O), we use all the Portuguese entity pairs
and verbs for mapping. To adjust for the fact that
some verbs appear more frequently in general, we
use tf-idf scores instead of co-occurrence counts for
the values of v(em,en) in the M-step (Equation 6).

3.2 Evaluation

We set aside 10% of D` for testing. Given a test
instance t(em,en) and the trained model, we can pre-
dict the label assignment y(em,en) using Eq. 1. This
simulates the task of relation extraction where we
predict relation(s) that exist between the entity pair
in t(em,en).

We compare predicted labels of these test in-
stances to the actual labels and measure precision,
recall and F1 values of the prediction. We evalu-
ate NELL relations that have more than one labeled
instances in D` (constructed using the method de-
scribed in section 2.2). For experiments on the En-
glish NELL, we evaluate 77 relations, with an aver-

Figure 3: Performance on leaf relations.

age of 23 (and a median of 11) training instances
per relations. For experiments on the Portuguese
NELL+en, which is Portuguese NELL enriched with
relation instances from English NELL, we evaluate
85 relations, with an average of 31 (and a median of
10) training instances per relations. We compare the
prediction produced by our approach: EM with that
of other systems: CPL, DIRT, and NB.

In CPL, we obtain verb-to-relation mapping
weights from NELL’s CPL patterns and hand-
labeled verb phrases (see Section 2.3.2). In DIRT,
we obtain verb-to-relation mapping weights in an
unsupervised manner (Lin and Pantel, 2001) based
on their mutual information over labeled training in-
stances. In Naive Bayes (NB) we learn the verb-to-
relation mapping weights from labeled training in-
stances. In contrast to the other systems, EM allows
learning from both labeled and unlabeled instances.

To make other systems comparable to our pro-
posed method, For NB and DIRT we add CPL
weights as priors to their verb-to-relation mapping
weights. For all these other systems, we also incor-
porate type-checking during prediction in that unla-
beled instances are only labeled with relations that
have the same argument types as the instance.

We show the micro-averaged performance of the
systems on leaf relations of English NELL and Por-
tuguese NELL (Fig. 3), where we do not incorpo-
rate constraints and classify each test instance into
a single relation. We observe in both English and

824



Figure 4: Performance on all relations.

Portuguese NELL that the verb-to-relation mapping
obtained by EM results in predictions that have a
much higher recall and comparable precision.

In Figure 3, we also observe a gain in performance
when we run EM on Portuguese NELL+en which is
Portuguese NELL enriched with relation instances
from English NELL obtained using our DBPedia
linking in section 2.4. More labeled instances results
in higher recall and precision. This shows the useful-
ness of aligning and merging knowledge from many
different KBs to improve verb-to-relation mapping
and relation extraction in general.

We show the micro-averaged performance of the
systems on all relations of English NELL and Por-
tuguese NELL (Fig. 4). Here, we incorporate hi-
erarchical and mutual exclusive constraints between
relations in our EM, allowing a test instance to be
classified into more than one relation while respect-
ing these constraints. Like before, we observe that
the verb-to-relation mapping obtained by EM results
in predictions with a much higher recall and compa-
rable precision to other systems which do not incor-
porate constraints between relations.

In the experiments we also observe that NB per-
forms comparably or better than DIRT. We hypothe-
size that it is because NB obtains its verb-to-relation
mapping in a supervised manner while DIRT ob-
tains its mapping in an unsupervised manner.

We also conduct experiments to investigate how
much influence type-checking has on prediction. We
show performance over instances whose types alone
are not enough to disambiguate their assignments

Figure 5: Performance on English NELL relations with and

without type-checking.

Relation Verbs Proposed
New Instances

bookWriter a1 be written by a2, (Dracula, Bram Stoker),
a2 write a1 (Divine Comedy, Dante)

city- a1 be known as a2, (Amman, Philadelphia),
Also- a2 be known as a1, (Chennai, Madras),

KnownAs a2 be renamed a1, (Southport, Smithville)
liderDe- a1 fundador a2, (Jimmy Wales, Wikipedia),

Organizacao a1 ceo de/em a2 (Chad Hurley, Youtube)
pessoa- a1 ser condenar aa2, (Pedrinho Matador,

Acusada- a1 ser acusar de a2, Homicidios),
DoCrime a1 ser prender (Omid Tahvili,

por a2 Trafico de Drogaso)

Table 3: Some relations’ verbs and proposed new instances

(i.e., when more than one relation shares their ar-
gument type signatures) to see the merits of verb-to-
relation mapping on prediction (Fig. 5). We observe
that verbs learned by EM results in a better predic-
tion even when used without type-checking (EM (-)
Type) than using type-checking alone (by picking
majority class among relations that have the correct
type) (Type Only). Adding type checking improves
performance even further (EM). This shows how
verbs learning is complementary to type-checking.

The results of our experiments also highlight the
merit of learning from a large, though unlabeled cor-
pus to improve the coverage of verb-to-relation map-
ping and hence the recall of predictions. We also
observe the usefulness of incorporating constraints
and for merging knowledge from multiple KBs to
improve performance. Another advantage of EM is
that it produces relation labels for unlabeled data not
yet in NELL KB. We show some of these new pro-
posed relation instances as well as some of the verb-

825



to-relation mapping obtained by EM (Table 3).
EM learns in average 177 English verbs and 3310

Portuguese verbs per relation; and propose in av-
erage 1695 new instances per relation for English
NELL, and 6426 new instances per relation for Por-
tuguese NELL. It learns less English verbs than Por-
tuguese due to the filtering of English data (Sec-
tion 3.1) and a high degree of inflection in Por-
tuguese verbs. The smaller size of Portuguese KB
also means more of its proposed instances are new.

4 Related Work

Existing verb resources are limited in their ability to
map to KBs. Some existing resources classify verbs
into semantic classes either manually (e.g. WordNet
(Miller et al., 1990)) or automatically (e.g. DIRT
(Lin and Pantel, 2001)). However, these classes
are not directly mapped to KB relations. Other re-
sources provide relations between verbs and their ar-
guments in terms of semantic roles (e.g. PropBank
(Kingsbury and Palmer, 2002), VerbNet (Kipper et
al., 2000), FrameNet (Ruppenhofer et al., 2006)).
However, it is not directly clear how the verbs map
to relations in specific KBs.

Most existing verb resources are also manually
constructed and not scalable. A verb resource that
maps to KBs should grow in coverage with the KBs,
possibly by leveraging large corpora such as the
Web for high coverage mapping. One system that
leverages Web-text as an interlingua is (Wijaya et
al., 2013). However, they use it to map KBs to
KBs, and obtain a verb-to-relation mapping only in-
directly. They also compute heuristic confidences
in verb-to-relation mappings from label propagation
scores, which are not probabilities. In contrast, we
map verbs directly to relations, and obtain P (vp|ri)
as an integral part of our EM process.

In terms of systems that learn mappings of tex-
tual patterns to KB relations, CPL (Carlson et al.,
2010) is one system that is most similar to our pro-
posed approach in that it also learns text patterns for
KB relations in a semi-supervised manner and uses
constraints in KB ontology to couple the learning to
produce extractors consistent with these constraints.
However, CPL uses a combination of heuristics in
its learning, while we use EM. In our experiments,
we use CPL patterns that contain verbs as priors and

show that our approach outperforms CPL in terms
of effectiveness for extracting relation instances.

In terms of the relation extraction, there are
distantly-supervised methods that can produce verb
groupings as a by product of relation extraction. One
state-of-the-art uses matrix factorization and univer-
sal schemas to extract relations (Riedel et al., 2013).
In this work, they populate a database of a uni-
versal schema (which involves surface form predi-
cates and relations from pre-existing KBs such as
Freebase) by using matrix factorization models that
learn latent feature vectors for relations and entity
tuples. One can envision obtaining a verb group-
ing for a particular relation by predicting verb sur-
face forms that occur between entity tuples that are
instances of the relation. However, unlike our pro-
posed method that learns mapping between typed-
verbs to relations, they do not incorporate argument
types in their learning, preferring to learn latent en-
tity representation from data. Although this im-
proves relation extraction, they observe that it hurts
performance of surface form prediction because a
single surface pattern (like “visit”) can have mul-
tiple argument types (person-visit-location, person-
visit-person, etc). Unlike our method, it is not clear
in their method how argument types of surface pat-
terns can be dealt with. Furthermore, it is not clear
how useful prior constraints between relations (sub-
set, mutex, etc.) can be incorporated in their method.

5 Conclusion

In this paper, we introduce an EM-based approach
with argument type checking and ontological con-
straints to automatically map verb phrases to KB re-
lations. We demonstrate that our verb resource is
effective for extracting KB relation instances while
improving recall; highlighting the value of learn-
ing from large scale unlabeled Web text. We also
show the flexibility of our method. Being KB-, and
language-independent, our method is able to con-
struct a verb resource for any language, given a KB
and a text corpus in that language. We illustrate this
by building a verb resource in Portuguese and in En-
glish which are both effective for extracting KB rela-
tions. Future work will explore the use of our multi-
lingual verb resource for relation extraction by read-
ing natural language text in multiple languages.

826



Acknowledgments

We thank members of the NELL team at CMU and
Federal University of Sao Carlos for their helpful
datasets, comments, and suggestions. This research
was supported by DARPA under contract number
FA8750-13-2-0005.

References

Sören Auer, Christian Bizer, Georgi Kobilarov, Jens
Lehmann, Richard Cyganiak, and Zachary Ives. 2007.
Dbpedia: A nucleus for a web of open data. In The
semantic web.

Kurt Bollacker, Colin Evans, Praveen Paritosh, Tim
Sturge, and Jamie Taylor. 2008. Freebase: a collabo-
ratively created graph database for structuring human
knowledge. In SIGMOD.

J. Callan, M. Hoy, C. Yoo, and L. Zhao. 2009.
Clueweb09 data set. boston.lti.cs.cmu.edu.

Andrew Carlson, Justin Betteridge, Bryan Kisiel, Burr
Settles, Estevam R Hruschka Jr, and Tom M Mitchell.
2010. Toward an architecture for never-ending lan-
guage learning. In AAAI, volume 5, page 3.

Sam Chapman. 2009. Simmetrics. URL
http://sourceforge. net/projects/simmetrics/. SimMet-
rics is a Similarity Metric Library, eg from edit dis-
tance’s (Levenshtein, Gotoh, Jaro etc) to other met-
rics,(eg Soundex, Chapman). Work provided by UK
Sheffield University funded by (AKT) an IRC spon-
sored by EPSRC, grant number GR N, 15764.

Bhavana Dalvi, Einat Minkov, Partha P Talukdar, and
William W Cohen. 2015. Automatic gloss finding
for a knowledge base using ontological constraints. In
Proceedings of the Eighth ACM International Confer-
ence on Web Search and Data Mining, pages 369–378.
ACM.

Paul Jaccard. 1912. The distribution of the flora in the
alpine zone. New phytologist, 11(2):37–50.

Paul Kingsbury and Martha Palmer. 2002. From tree-
bank to propbank. In LREC. Citeseer.

Karin Kipper, Hoa Trang Dang, and Martha Palmer.
2000. Class-based construction of a verb lexicon. In
AAAI/IAAI, pages 691–696.

Dekang Lin and Patrick Pantel. 2001. Discovery of infer-
ence rules for question-answering. Natural Language
Engineering, 7(04):343–360.

Yue Lu and Chengxiang Zhai. 2008. Opinion integra-
tion through semi-supervised topic modeling. In Pro-
ceedings of the 17th international conference on World
Wide Web, pages 121–130. ACM.

Christopher D. Manning, Mihai Surdeanu, John Bauer,
Jenny Finkel, Steven J. Bethard, and David McClosky.

2014. The Stanford CoreNLP natural language pro-
cessing toolkit. In Proceedings of 52nd Annual Meet-
ing of the Association for Computational Linguistics:
System Demonstrations, pages 55–60.

George A Miller, Richard Beckwith, Christiane Fell-
baum, Derek Gross, and Katherine J Miller. 1990. In-
troduction to wordnet: An on-line lexical database*.
International journal of lexicography, 3(4):235–244.

Kamal Nigam, Andrew McCallum, and Tom Mitchell.
2006. Semi-supervised text classification using em.
Semi-Supervised Learning, pages 33–56.

Lawrence Page, Sergey Brin, Rajeev Motwani, and Terry
Winograd. 1999. The pagerank citation ranking:
bringing order to the web.

Sebastian Riedel, Limin Yao, Andrew McCallum, and
Benjamin M Marlin. 2013. Relation extraction with
matrix factorization and universal schemas.

Ricardo Rodrigues, Hugo Gonçalo Oliveira, and Paulo
Gomes. 2014. Lemport: a high-accuracy cross-
platform lemmatizer for portuguese. Maria João
Varanda Pereira José Paulo Leal, page 267.

Josef Ruppenhofer, Michael Ellsworth, Miriam RL
Petruck, Christopher R Johnson, and Jan Scheffczyk.
2006. Framenet ii: Extended theory and practice.

Fabian M Suchanek, Gjergji Kasneci, and Gerhard
Weikum. 2007. Yago: a core of semantic knowledge.
In Proceedings of the 16th international conference on
World Wide Web, pages 697–706. ACM.

Derry Wijaya, Partha Pratim Talukdar, and Tom Mitchell.
2013. Pidgin: ontology alignment using web text as
interlingua. In Proceedings of the 22nd ACM inter-
national conference on Conference on information &
knowledge management, pages 589–598. ACM.

827


