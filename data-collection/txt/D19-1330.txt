



















































Synchronously Generating Two Languages with Interactive Decoding


Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing
and the 9th International Joint Conference on Natural Language Processing, pages 3350–3355,
Hong Kong, China, November 3–7, 2019. c©2019 Association for Computational Linguistics

3350

Synchronously Generating Two Languages with Interactive Decoding

Yining Wang1,2, Jiajun Zhang1,2∗, Long Zhou1,2
Yuchen Liu1,2 and Chengqing Zong1,2,3

1National Laboratory of Pattern Recognition, CASIA, Beijing, China
2University of Chinese Academy of Sciences, Beijing, China

3CAS Center for Excellence in Brain Science and Intelligence Technology, Beijing, China
{yining.wang, jjzhang, long.zhou}@nlpr.ia.ac.cn

{yuchen.liu, cqzong}@nlpr.ia.ac.cn

Abstract

In this paper, we introduce a novel interactive
approach to translate a source language into
two different languages simultaneously and in-
teractively. Specifically, the generation of one
language relies on not only previously gener-
ated outputs by itself, but also the outputs pre-
dicted in the other language. Experimental re-
sults on IWSLT and WMT datasets demon-
strate that our method can obtain significant
improvements over both conventional Neural
Machine Translation (NMT) model and multi-
lingual NMT model.

1 Introduction

Neural Machine Translation (NMT) based on the
encoder-decoder framework has significantly im-
proved translation quality due to its powerful end-
to-end modeling (Sutskever et al., 2014; Bahdanau
et al., 2015; Vaswani et al., 2017; Gehring et al.,
2017; Hassan et al., 2018; Zhang and Zong, 2015).
This paradigm facilitates the development of mul-
tilingual NMT (Dong et al., 2015; Luong et al.,
2016; Johnson et al., 2017; Ha et al., 2016; Firat
et al., 2016; Lakew et al., 2017; Tan et al., 2019;
Wang et al., 2019), which handles multiple lan-
guage pairs in one model, with the benefit of sim-
plifying offline model training and easing online
maintenance cost.

Although multilingual NMT attempts to utilize
the complementary information of different lan-
guages (Lu et al., 2018; Neubig and Hu, 2018; Pla-
tanios et al., 2018; Wang et al., 2018), all of the
models handle one language pair at each moment
for both training and testing. However, we find
that the generation process of different target lan-
guages can help each other. For example in Figure
1, when decoding the Chinese word “书” meaning
“book” at step t = 5, the predicted Japanese word

∗ Jiajun Zhang is the corresponding author.

我

I
买了

bought 一本
pad

一冊

a
の 本

book
買い

bought

t=1 t=2 t=4 t=5

今天

today

今日

today

t=3

书

book

を

I bought a book today.

English

Chinese

Japanese

pad一本
a

ました

Figure 1: An example of an English sentence trans-
lated into Chinese and Japaneses sentences, in which
two targets can interact with each other.

“本” with same meaning can provide the context
at step t = 4. The reason is that the sentence struc-
ture between the two languages is different. It is
subject-verb-object in Chinese while it is subject-
object-verb in Japanese. Moreover, we find that
two languages are complementary, and if decoders
belonging to two different languages can interact
with each other, the quality of translation will be
improved.

In this work, we present a novel interactive de-
coding algorithm to generate two target languages
simultaneously and interactively. To this end, we
propose a synchronous attention model, in which
the generation of one language can attend to al-
ready generated outputs of another language. As
shown in Figure 1, the two decoders predict their
outputs at the same time. At each moment, word
prediction of each language does not only rely
on previously generated targets itself but also de-
pends on outputs of the other language.

We conduct extensive experiments to verify
the effectiveness of our proposed approaches
on English-to-German/French and English-to-
Chinese/Japanese translation tasks.



3351

Our contributions in this work are two-fold:
(1) We propose a novel synchronous transla-

tion model that can predict outputs of two different
languages simultaneously and interactively, which
can enhance the translation quality of both lan-
guages.

(2) Extensive experiments show the superiority
of our proposed method. Specifically, this syn-
chronous approach can significantly outperform
both the conventional NMT model and the mul-
tilingual NMT model.

2 Background

Owing to powerful modeling ability, our syn-
chronous method relies on Transformer architec-
ture (Vaswani et al., 2017), which is entirely based
on the attention mechanism detailed below.

Scaled Dot-Product Attention: The inputs of
attention mechanism contain queries Q, keys K ,
and values V . This function can be described as
mapping a query and a set of key-value pairs to an
output, and the output is calculated as a weighted
sum of the values.

Attention (Q,K,V) = softmax(QK
T√

dk
)V (1)

where dk is dimension of keys, Q, K , V are ob-
tained by linearly transforming input hidden states
with projection matrices.

3 Synchronous Translation Method

As discussed in Sec. 1, outputs in different lan-
guages can be complementary and can help with
each other. Thus, it is reasonable to improve trans-
lation quality with interactions of two decoders.
In this section, we propose an interactive decod-
ing algorithm and then describe how to implement
it by a new attention mechanism, named as syn-
chronous attention model.

3.1 Interactive Decoding Algorithm

Interactive decoding algorithm can generate trans-
lations of different languages in the same beam.
At each step, each half of the beam produces
translations in one target language conditioning on
source sentence and the predicted tokens in both
target languages. Here, we use two decoders with
separated embeddings and softmax layers to oper-
ate two languages. The two decoders predict each
token in parallel and keep interaction with each

Figure 2: The synchronous self-attention framework,
which simultaneously operates on the keys, queries and
values of two different decoders.

other, which can be formalized as follows:

log P(y1 |x) = log
n−1∏
i=0

p(y1i |x, y
1
0, . . . , y

1
i−1, y

2
0, . . . , y

2
i−1)

log P(y2 |x) = log
n−1∏
i=0

p(y2i |x, y
2
0, . . . , y

2
i−1, y

1
0, . . . , y

1
i−1)

(2)

where x is source sentence, y1, y2 are target sen-
tences corresponding to two different languages.
At time-step i, we have generated the first i − 1 to-
kens of language-1 y1 and the first i − 1 tokens of
language-2 y2. Then both languages predictions
can be utilized together with source sentence to
generate tokens y1i and y

2
i . This interaction be-

tween two languages is realized by synchronous
attention model, which will be detailed in the fol-
lowing subsection.

It should be noted that the two language sen-
tences can be generated in different directions (Liu
et al., 2016; Zhang et al., 2018; Zhou et al., 2019),
which means language-1 can be produced in left-
to-right (L2R) manner while language-2 in right-
to-left (R2L) manner. We will analyze the effect
of different decoding manners in Sec. 5.2.

3.2 Synchronous Attention Model

Synchronous attention model (SyncAtt) is shown
in Figure 2, in which inputs of two decoders con-
tain queries (Q1,Q2), keys (K1,K2), and values
(V1,V2) separately. The new hidden states (H

′
i ) can

be computed by our proposed synchronous atten-



3352

tion as follows:

H
′
1 =SyncAtt (Q1, [K1; K2], [V1; V2])

H
′
2 =SyncAtt (Q2, [K1; K2], [V1; V2])

(3)

where synchronous attention model (SyncAtt) can
be described in detail:

H1 =Attention (Q1,K1,V1)
H̃1 =Attention (Q1,K2,V2)
H2 =Attention (Q2,K2,V2)
H̃2 =Attention (Q2,K1,V1)
H
′
1 = f (H1; H̃1) = H1 + λ × tanh(H̃1)

H
′
2 = f (H2; H̃2) = H2 + λ × tanh(H̃2)

(4)

where λ is a balance weight of hidden states be-
tween two decoders decided by development set.

We apply our synchronous attention model to
replace self-attention sub-layer in Transformer de-
coder, and it also utilizes the residual connections
(He et al., 2016) around each sub-layer, followed
by layer normalization (Ba et al., 2016).

3.3 Training
Since our synchronous method decodes two lan-
guages at the same time, the different decoders can
be optimized simultaneously.

Supposing we have the trilingual datasets D ={
(x, y1, y2)

}
, the objective function is to maxi-

mize the log-likelihood over the two target sen-
tences:

L(θ) =
∑

(x,y1,y2)∈D
(
|y1i |∑
i=1

log P(y1i |x) +
|y2i |∑
i=1

log P(y2i |x)) (5)

When calculating P(y1i |x), except for the con-
text from source side x, our synchronous method
employs not only previous reference y1<i as condi-
tion, but the previous context of the other decoder
reference y2<i. The calculation process of P(y2i |x)
is similar.

However, the practical situation is that the triple
data is limited and hard to be collected. In this
work, we construct the trilingual training cor-
pus by data augmenting method (Sennrich et al.,
2016a; Zhang and Zong, 2016). To achieve this,
we first learn two independent translation mod-
els Model-1 and Model-2 on the bilingual train-
ing data (x1, y1) and (x2, y2) separately. Then,
Model-1 and Model-2 are employed to decode
the input sentences x2 and x1, resulting in pseudo
training data (x2, y1?) and (x1, y2?), respectively.

Thus, we can obtain the triple parallel training data
D =

{
(x1, y1, y2?)

}
∪

{
(x2, y1?, y2)

}
, which can

be used to train our synchronous translation model
mentioned above.

4 Experimental Settings

4.1 Data

We evaluate our proposed synchronous
method on two translation tasks, which in-
clude English→Chinese/Japanese (briefly,
En→Zh/Ja) and English→German/French
(briefly, En→De/Fr) on IWSLT1 datasets. The
IWSLT.TED.tst2013 and IWSLT.TED.tst2014
are employed as devlopment set and test set
respectively. Besides, we also perform En→De/Fr
translation in large scale WMT142 datasets. We
use newstest2014 as test set.

En→Zh/Ja: For this translation task, the train-
ing sets of En→Zh and En→Ja consist of 231K,
223K sentence pairs. We tokenize the English sen-
tences using a script from Moses (Koehn et al.,
2007), and we segment Chinese and Japanese data
by jieba3 and mecab4. We use BPE method (Sen-
nrich et al., 2016b) to encode the source side sen-
tences and the combination of target side sen-
tences respectively and limit the vocabularies of
both sides to the most frequent 10k tokens.

En→De/Fr: We conduct this translation task
on two different settings. One setting is using
training set of IWSLT datasets which contains
206K sentence pairs for En→De and 233K sen-
tence pairs for En→Fr. We follow the common
practice to tokenize and lowercase all words. Sen-
tences are encoded using BPE, which has a shared
vocabulary of 10K tokens. At last, we construct
pseudo triple data by the method described in
Sec. 3.3. For the other setting, we extract the
trilingual subset in WMT14 inspired by Zoph and
Knight (2016), which includes about 2.43M sen-
tence triples. We use 37K shared BPE tokens as
vocabulary.

4.2 Training Details

We implement our synchronous translation based
on the tensor2tensor5 library. We train our models
using the configuration transformer base adopted

1https://wit3.fbk.eu
2http://www.statmt.org/wmt14/translation-task.html
3https://github.com/fxsjy/jieba
4http://taku910.github.io/mecab
5https://github.com/tensorflow/tensor2tensor



3353

Hyper-
paramter (λ)

En-De/Fr En-Zh/Ja
En-De En-Fr En-Zh En-Ja

0.1 30.95 43.01 16.33 18.88
0.2 30.77 42.99 16.06 18.82
0.3 30.55 42.99 15.96 18.38
0.4 29.60 42.52 15.66 18.03
0.5 29.19 41.87 15.17 17.42

Table 1: Experiment results on the development set
with different λs.

by Vaswani et al. (2017), which contains a 6-
layer encoder and a 6-layer decoder with 512-
dimensional hidden representations. During train-
ing, each mini-batch contains roughly 4,096 to-
kens for both source and target sides. We use
Adam optimizer (Kingma and Ba, 2014) with
β1=0.9, β2=0.98, and �=10−9. For decoding, we
set beam size to be k = 4 and length penalty
α = 0.6. All our methods are trained and tested
on single Nvidia P40 GPU.

We investigate the impact of different λs in our
synchronous attention model. As shown in Ta-
ble 1, when λ=0.1, the translation results perform
best on development set for both En→Zh/Ja and
En→De/Fr tasks, and we will use this setting in
the subsequent experiments.

5 Results and Analysis

The translation performance of IWSLT datasets
is evaluated by case-insensitive BLEU4 (Papineni
et al., 2002) for En→De/Fr task and character-
level BLEU5 for En→Zh/Ja task. For WMT14
datasets, we calculate the case-sensitive BLEU4
the same as previous work. In our experiments,
the NMT models trained on individual language
pair are denoted by Indiv.

5.1 Results on IWSLT

Table 2 shows the main translation results of
En→Zh/Ja and En→De/Fr on IWSLT datasets.
We also conduct a typical one-to-many translation
adopting Johnson et al. (2017) method on Trans-
former as our another baseline model, referred to
Multi. Compared with Indiv, we can see that Multi
achieves better results on all cases, which can be
attributed to that the encoder can be enhanced by
extra training data from the other language pair.

As for our proposed method, the synchronous
translation method performs significantly better
than both Indiv and Multi baseline methods, and
it can achieve the improvements up to 2.75 BLEU
points (19.31 vs. 16.56) on En→Ja.

Method En-Zh/Ja En-De/FrEn-Zh En-Ja En-De En-Fr
Indiv 15.68 16.56 27.11 40.62
Indiv + pseudo 16.72 18.02 28.47 40.39
Multi 17.06 18.31 27.79 40.97
Multi + pseudo 17.10 18.40 28.56 40.62
SyncTrans 17.97 19.31 29.16 41.53

Table 2: Translation performance on IWSLT datasets.
SyncTrans represents our proposed synchronous trans-
lation method. All results of our SyncTrans are signifi-
cantly better than both Indiv and Multi (p < 0.01).

To perform synchronous translation, the triple
parallel corpus contains pseudo training data we
construct. For a fair comparison, we also conduct
our baseline methods on the training sentences
augmented by the pseudo corpus. From row 2 and
row 4 in Table 2, our method achieves better per-
formance than both Multi + pseudo method and
Indiv + pseudo method with gains of 0.82 BLEU
and 1.09 points on average, which demonstrates
the effectiveness of our method.

5.2 L2R or R2L Manner

As described in Sec. 3.1, two target languages can
be generated in L2R or R2L manner, which can
provide the future contexts for each other. We fur-
ther conduct an experiment to investigate different
decoding manners in this work.

Figure 3 reports the results. We observe that
when performing En→De/Fr translation, one lan-
guage generated from R2L manner is helpful for
the other language but do harm to itself. However,
for En→Zh/Ja translation, Japanese can achieve
improvements on both L2R and R2L decoding set-
tings. The reason is that Japanese is suited for
translating from right-to-left manner, and it can
take advantage of future outputs from Chinese.

5.3 Results on WMT

We also employ our method on the real triple train-
ing datasets, which can be collected from WMT14
En→De/Fr datasets as described in Sec. 4.1. From
Table 3, we observe that our method consistently
outperforms baseline models. Note that in con-
trast to results on IWSLT datasets, Multi can not
perform on par with Indiv, because the source
side data for two language pairs are the same, and
encoder network can not be enhanced as Multi
method in Sec. 5.1.

Moreover, we construct a large scale pseudo



3354

29.16

41.53

28.43

41.87

29.51

41.18

25

30

35

40

45

En-De En-Fr

BL
EU

 S
co

re
s

En-De/Fr Translation

L2R

R2L-De

R2L-Fr

17.97

19.31

18.01

19.58

17.81

20.13

15
16
17
18
19
20
21
22

En-Zh En-Ja

BL
EU

 S
co

re
s

En-Zh/Ja Translation

L2R
R2L-Zh
R2L-Ja

Figure 3: The comparison of L2R and R2L decoding manners, in which one of target languages generated from
R2L manner.

Method WMT14 (2.43M) WMT14 (4.50M)En-De En-Fr En-De
Indiv 24.33 37.12 26.53
Multi 23.46 36.33 25.81
SyncTrans 24.84†∗ 37.66†∗ 27.01†∗

Table 3: Translation quality of En-De/Fr on WMT14
datasets. The significance values with respect to the
baseline method Indiv and Multi method are denoted by
“∗” and “†” respectively, indicating our proposed Sync-
Trans significantly better than both Indiv (p < 0.05)
and Multi (p < 0.01) methods.

triple data about 4.5M6. The result is demonstrated
in the last column of Table 3, in which our syn-
chronous method performs better than the baseline
methods as well.

6 Conclusion

In this paper, we propose an interactive decod-
ing algorithm to generate two target languages si-
multaneously and interactively. The empirical ex-
periments on four language pairs demonstrate that
our approach can obtain significant improvements
over both the NMT model trained on individual
language pair and multilingual NMT model. For
the future work, we plan to extend our method
on more than two target languages and explore
other effective interactive approaches to improve
the translation quality further.

Acknowledgments

The research work described in this paper has
been supported by the National Key Research and
Development Program of China under Grant No.

6The En-Fr part is entirely created by translating En-
glish of WMT14 En-De datasets into French using the model
trained on WMT14 En-Fr corpus about 37M.

2016QY02D0303, the Natural Science Founda-
tion of China under Grant No. U1836221 and
the Beijing Municipal Science and Technology
Project under Grant No. Z181100008918017.
The research work in this paper has also been
supported by Beijing Advanced Innovation Cen-
ter for Language Resources. We thank the three
anonymous reviewers for their efforts on this
manuscript. We would like to thank Yanfen Zhang
and Junnan Zhu for their invaluable discussions on
this paper.

References

Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hin-
ton. 2016. Layer normalization. arXiv preprint
arXiv:1607.06450.

Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben-
gio. 2015. Neural machine translation by jointly
learning to align and translate. In Proceedings of
ICLR 2015.

Daxiang Dong, Hua Wu, Wei He, Dianhai Yu, and
Haifeng Wang. 2015. Multi-task learning for mul-
tiple language translation. In Proceedings of ACL
2015, pages 1723–1732.

Orhan Firat, Kyunghyun Cho, and Yoshua Bengio.
2016. Multi-way, multilingual neural machine
translation with a shared attention mechanism. In
Proceedings of NAACL 2016, pages 866–875.

Jonas Gehring, Michael Auli, David Grangier, De-
nis Yarats, and Yann N. Dauphin. 2017. Convolu-
tional sequence to sequence learning. arXiv preprint
arXiv:1601.03317.

Thanh-Le Ha, Jan Niehues, and Alexander Waibel.
2016. Toward multilingual neural machine trans-
lation with universal encoder and decoder. In Pro-
ceedings of IWSLT 2016.



3355

Hany Hassan, Anthony Aue, Chang Chen, Vishal
Chowdhary, Jonathan Clark, Christian Feder-
mann, Xuedong Huang, Marcin Junczys-Dowmunt,
William Lewis, Mu Li, et al. 2018. Achieving hu-
man parity on automatic Chinese to English news
translation. arXiv preprint arXiv:1803.05567.

Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian
Sun. 2016. Deep residual learning for image recog-
nition. In proceddings of CVPR 2016.

Melvin Johnson, Mike Schuster, Quoc V. Le, Maxim
Krikun, Yonghui Wu, Zhifeng Chen, Nikhil Thorat,
Fernanda Viégas, Martin Wattenberg, Greg Corrado,
Macduff Hughes, and Jeffrey Dean. 2017. Google’s
multilingual neural machine translation system: En-
abling zero-shot translation. Transactions of the As-
sociation for Computational Linguistics, 5:339–351.

Diederik P Kingma and Jimmy Ba. 2014. Adam: A
method for stochastic optimization. arXiv preprint
arXiv:1412.6980.

Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, et al. 2007. Moses: Open source
toolkit for statistical machine translation. In Pro-
ceedings of ACL 2007.

Surafel M Lakew, ADG Mattia, and F Marcello. 2017.
Multilingual neural machine translation for low re-
source languages. CLiC-it.

Lemao Liu, Masao Utiyama, Andrew Finch, and
Eiichiro Sumita. 2016. Agreement on target-
bidirectional neural machine translation. In Pro-
ceedings of NAACL 2016, pages 411–416.

Yichao Lu, Phillip Keung, Faisal Ladhak, Vikas Bhard-
waj, Shaonan Zhang, and Jason Sun. 2018. A neu-
ral interlingua for multilingual machine translation.
arXiv preprint arXiv:1804.08198.

Minh-Thang Luong, Quoc V Le, Ilya Sutskever, Oriol
Vinyals, and Lukasz Kaiser. 2016. Multi-task se-
quence to sequence learning. In Proceedings of
ICLR 2016.

Graham Neubig and Junjie Hu. 2018. Rapid adaptation
of neural machine translation to new languages. In
Proceedings of EMNLP 2018, pages 875–880.

Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic eval-
uation of machine translation. In Proceedings of
ACL, pages 311–318.

Emmanouil Antonios Platanios, Mrinmaya Sachan,
Graham Neubig, and Tom Mitchell. 2018. Contex-
tual parameter generation for universal neural ma-
chine translation. In Proceedings of EMNLP 2018,
pages 425–435.

Rico Sennrich, Barry Haddow, and Alexandra Birch.
2016a. Improving neural machine translation mod-
els with monolingual data. In Proceedings of ACL
2016, pages 86–96.

Rico Sennrich, Barry Haddow, and Alexandra Birch.
2016b. Neural machine translation of rare words
with subword units. In Proceedings of ACL 2016,
pages 1715–1725.

Ilya Sutskever, Oriol Vinyals, and Quoc V Le. 2014.
Sequence to sequence learning with neural net-
works. In Proceedings of NIPS, pages 3104–3112.

Xu Tan, Yi Ren, Di He, Tao Qin, Zhou Zhao, and Tie-
Yan Liu. 2019. Multilingual neural machine transla-
tion with knowledge distillation. In Proceedings of
ICLR 2019.

Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob
Uszkoreit, Llion Jones, Aidan N. Gomez, and
Łukasz Kaiser. 2017. Attention is all you need. In
Proceedings of NIPS, pages 30–34.

Yining Wang, Jiajun Zhang, Feifei Zhai, Jingfang Xu,
and Chengqing Zong. 2018. Three strategies to im-
prove one-to-many multilingual translation. In Pro-
ceedings of EMNLP 2018, pages 2955–2960.

Yining Wang, Long Zhou, Jiajun Zhang, Feifei Zhai,
Jingfang Xu, and Chengqing Zong. 2019. A com-
pact and language-sensitive multilingual translation
method. In Proceedings of ACL 2019, pages 1213–
1223.

Jiajun Zhang and Chengqing Zong. 2015. Deep neu-
ral networks in machine translation: An overview.
IEEE Intelligent Systems, 30(5):16–25.

Jiajun Zhang and Chengqing Zong. 2016. Exploit-
ing source-side monolingual data in neural machine
translation. In Proceedings of EMNLP, pages 1535–
1545.

Xiangwen Zhang, Jinsong Su, Yue Qin, Yang Liu, Ron-
grong Ji, and Hongji Wang. 2018. Asynchronous
bidirectional decoding for neural machine transla-
tion. In proceddings of AAAI 2018.

Long Zhou, Jiajun Zhang, and Chengqing Zong. 2019.
Synchronous bidirectional neural machine transla-
tion. Transactions of the Association for Compu-
tational Linguistics, 7:91–105.

Barret Zoph and Kevin Knight. 2016. Multi-source
neural translation. In Proceedings of NAACL 2016,
pages 30–34.


