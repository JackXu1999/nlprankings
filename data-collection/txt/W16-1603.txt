



















































A Joint Model for Word Embedding and Word Morphology


Proceedings of the 1st Workshop on Representation Learning for NLP, pages 18–26,
Berlin, Germany, August 11th, 2016. c©2016 Association for Computational Linguistics

A Joint Model for Word Embedding and Word Morphology

Kris Cao and Marek Rei
Computer Lab

University of Cambridge
United Kingdom

kc391@cam.ac.uk

Abstract

This paper presents a joint model for
performing unsupervised morphologi-
cal analysis on words, and learning a
character-level composition function
from morphemes to word embeddings.
Our model splits individual words into
segments, and weights each segment
according to its ability to predict context
words. Our morphological analysis is
comparable to dedicated morphological
analyzers at the task of morpheme bound-
ary recovery, and also performs better
than word-based embedding models at
the task of syntactic analogy answering.
Finally, we show that incorporating
morphology explicitly into character-level
models helps them produce embeddings
for unseen words which correlate better
with human judgments.

1 Introduction

Word embedding models associate each word in a
corpus with a vector in a semantic space. These
vectors can either be learnt to optimize perfor-
mance in a downstream task (Bengio et al., 2003;
Collobert et al., 2011) or learnt via the distri-
butional hypothesis: words with similar contexts
have similar meanings (Harris, 1954; Mikolov et
al., 2013a). Current word embedding models treat
words as atomic. However, words follow a power
law distribution (Zipf, 1935), and word embed-
ding models suffer from the problem of sparsity:
a word like ‘unbelievableness’ does not appear at
all in the first 17 million words of Wikipedia, even
though it is derived from common morphemes.
This leads to three problems:

1. word representations decline in quality for

rarely observed words (Bullinaria and Levy,
2007).

2. word embedding models handle out-of-
vocabulary words badly, typically as a single
‘OOV’ token.

3. the word distribution has a long tail, and
many parameters are needed to capture all of
the words in a corpus (for an embedding size
of 300 with a vocabulary of 10k words, 3 mil-
lion parameters are needed)

One approach to smooth word distributions is
to operate on the smallest meaningful semantic
unit, the morpheme (Lazaridou et al., 2013; Botha
and Blunsom, 2014). However, previous work on
the morpheme level has all used external morpho-
logical analyzers. These require a separate pre-
processing step, and cannot be adapted to suit the
problem at hand.

Another is to operate on the smallest ortho-
graphic unit, the character (Ling et al., 2015; Kim
et al., 2016). However, the link between shape
and meaning is often complicated (de Saussure,
1916), as alphabetic characters carry no inherent
semantic meaning. To account for this, the model
has to learn complicated dependencies between
strings of characters to accurately capture word
meaning. We hypothesize that explicitly introduc-
ing morphology into character-level models can
help them learn morphological features, and hence
word meaning.

In this paper, we introduce a word embedding
model that jointly learns word morphology and
word embeddings. To the best of our knowledge,
this is the first word embedding model that learns
morphology as part of the model. Our guiding in-
tuition is that the words with the same stem have
similar contexts. Thus, when considering word
segments in terms of context-predictive power, the

18



segment corresponding to the stem will have the
most weight.

Our model ‘reads’ the word and outputs a se-
quence of word segments. We weight each seg-
ment, and then combine the segments to obtain
the final word representation. These represen-
tations are trained to predict context words, as
this has been shown to give word representations
which capture word semantics well (Mikolov et
al., 2013b). As the root morpheme has the most
context-predictive power, we expect our model to
assign high weight to this segment, thereby learn-
ing to separate root+affix structures.

One exciting feature of character-level models
is their ability to represent open-vocabulary words.
After training, they can predict a vector for any
word, not just words that they have seen before.
Our model has an advantage in that it can split
unknown words into known and unknown compo-
nents. Hence, it can potentially generalise better
over seen morphemes and words and apply exist-
ing knowledge to new cases.

To evaluate our model, we evaluate its use as
a morphological analyzer (§4.1), test how well it
learns word semantics, including for unseen words
(§4.2), and examine the structure of the embedding
space (§4.3).

2 Related Work

While words are often treated as the fundamental
unit of language, they are in fact themselves com-
positional. The smallest unit of semantics is the
morpheme, while the smallest unit of orthography
is the grapheme, or character. Both have been used
as a method to go beyond word-level models.

2.1 Morphemic analysis and semantics

As word semantics is compositional, one might
ask whether it is possible to learn morpheme
representations, and compose them to obtain
good word representations. Lazaridou et al.
(2013) demonstrated precisely this: one can de-
rive good representations of morphemes distribu-
tionally, and apply tools from compositional dis-
tributional semantics to obtain good word repre-
sentations. Luong et al. (2013) also trained a
morphological composition model based on recur-
sive neural networks. Botha and Blunsom (2014)
built a language model incorporating morphemes,
and demonstrated improvements in language mod-
elling and in machine translation. All of these

approaches incorporated external morphological
knowledge, either in the form of gold standard
morphological analyses such as CELEX (Baayen
et al., 1995) or an external morphological analyzer
such as Morfessor (Creutz and Lagus, 2007).

Unsupervised morphology induction aims to
decide whether two words are morphologically re-
lated or to generate a morphological analysis for a
word (Goldwater et al., 2005; Goldsmith, 2001).
While they may use semantic insights to perform
the morphological analysis (Soricut and Ochs,
2015), they typically are not concerned with ob-
taining a semantic representation for morphemes,
nor of the resulting word.

2.2 Character-level models

Another approach to go beyond words is based on
on character-level neural network models. Both
recurrent and convolutional architectures for de-
riving word representations from characters have
been used, and results in downstream tasks such
as language modelling and POS tagging have been
promising, with reductions in word perplexity for
language modelling and state-of-the-art English
POS tagging accuracy (Ling et al., 2015; Kim
et al., 2016). Ballesteros et al. (2015) train a
character-level model for parsing. Zhang et al.
(2015) do away with words completely, and train
a convolutional neural network to do text classifi-
cation directly from characters.

Excitingly, character-level models seem to cap-
ture morphological effects. Examining nearest
neighbours of morphologically complex words in
character-aware models often shows other words
with the same morphology (Ling et al., 2015; Kim
et al., 2016). Furthermore, morphosyntactic fea-
tures such as capitalization and suffix information
have long been used in tasks such as POS tagging
(Xu et al., 2015; Toutanova et al., 2003). By ex-
plicitly modelling these features, one might expect
good performance gains in many NLP tasks.

What is less clear is how well these models
learn word semantics. Classical word embedding
models seem to capture word semantics, and the
nearest neighbours of a given word are typically
semantically related words (Mikolov et al., 2013a;
Mnih and Kavukcuoglu, 2013). In addition, the
correlation between model word similarity scores
and human similarity judgments is typically high
(Levy et al., 2015). However, no previous work (to
our knowledge) evaluates the similarity judgments

19



Figure 1: A graphical illustration of SGNS. The
target vector for ‘dog’ is learned to have high inner
product with the context vectors for words seen
in the context of ‘dog’ (no shading), while having
low inner product with random negatively sampled
words (shaded)

of character-level models against human annota-
tors.

3 The Char2Vec model

We hypothesize that by incorporating morpho-
logical knowledge directly into a character-level
model, one can improve the ability of character-
level models to learn compositional word seman-
tics. In addition, we hypothesize that incorporat-
ing morphological knowledge helps structure the
embedding space in such a way that affixation cor-
responds to a regular shift in the embedding space.
We test both hypotheses directly in §4.2 and §4.3
respectively.

The starting point for our model is the skip-
gram with negative sampling (SGNS) objective of
Mikolov et al. (2013b). For a vocabulary V of
size |V | and embedding size N , SGNS learns two
embedding tables W,C ∈ RN×|V |, the target and
context vectors. Every time a word w is seen in
the corpus with a context word c, the tables are
updated to maximize

log σ(w · c) +
k∑

i=1

Ec̃i∼P (w)[log σ(−w · c̃i)] (1)

where P (w) is a noise distribution from which we
draw k negative samples. In the end, the target

vector for a word w should have high inner prod-
uct with context vectors for words with which it
is typically seen, and low inner products with con-
text vectors for words it is not typically seen with.
Figure 1 illustrates this for a particular example.
In Mikolov et al. (2013b), the noise distribution
P (w) is proportional to the unigram probability of
a word raised to the 3/4th power (Mikolov et al.,
2013b).

Our innovation is to replace W with a trainable
function f that accepts a sequence of characters
and returns a vector of length N (i.e. f : A<ω →
RN , where A is the alphabet we are considering
and A<ω denotes the finite length strings over the
alphabet A). We still keep the table of context
embeddings C, and our model objective is still to
minimize

log σ(f(w) · c)+
k∑

i=1

Ec̃i∼P (w)[log σ(−f(w) · c̃i)]
(2)

where we now treat w as a sequence of characters.
After training, f can be used to produce an em-
bedding for any sequence of characters, even if it
was not previously seen in training.

The process of calculating f on a word is il-
lustrated in Figure 2. We first pad the word with
beginning and end of word tokens, and then pass
the characters of the word into a character lookup
table. As the link between characters and mor-
phemes is non-compositional and requires essen-
tially memorizing a sequence of characters, we
use LSTMs (Hochreiter and Schmidhuber, 1997)
to encode the letters in the word, as they have
been shown to capture non-local and non-linear
dependencies. We run a forward and a backward
LSTM over the character embeddings. The for-
ward LSTM reads the beginning of word symbol,
but not the end of word symbol, and the backward
LSTM reads the end of word symbol but not the
beginning of word symbol. This is necessary to
align the resulting embeddings, so that the LSTM
hidden states taken together correspond to a parti-
tion of the word into two without overlap.

The LSTMs output two sequences of vectors
hf0 , . . . , h

f
n and hbn, . . . , h

b
0. We then concatenate

the resulir ofting vectors, and pass them through
a shared feed-forward layer to obtain a final se-
quence of vectors hi. Each vector corresponds
to two half-words: one half read by the forward
LSTM, and the other by the backward LSTM.

We then learn an attention model over these hid-

20



Figure 2: An illustration of Char2Vec. A bidirec-
tional LSTM reads the word (start and end of word
symbols represented by ˆ and $ respectively), out-
putting a sequence of hidden states. These are then
passed through a feed-forward layer (not shown),
weighted by an attention model (the square box in
the diagram) and summed to obtain the final word
representation.

den states: given a hidden state hi, we calculate
a weight αi = a(hi) such that

∑
αi = 1, and

then calculate the resulting vector for the word w
as f(w) =

∑
αihi. Following Bahdanau et al.

(2014), we calculate a as

a(hi) =
exp(vT tanh(Whi))∑
j exp(vT tanh(Whj))

(3)

i.e. a softmax over the hidden states.

3.1 Capturing morphology via attention
Previous work on bidirectional LSTM character-
level models used both LSTMs to read the entire
word (Ling et al., 2015; Ballesteros et al., 2015).
This can lead to redundancy, as both LSTMs are
used to capture the full word. In contrast, our
model is capable of splitting the words and op-
timizing the two LSTMs for modelling different
halves. This means one of the LSTMs can spe-
cialize on word prefixes and roots, while the other
memorizes possible suffixes. In addition, when
dealing with an unknown word, it can be split into

Figure 3: An illustration of the attention model
(start and end of word symbols omitted). The root
morpheme contributes the most to predicting the
context, and is upweighted. In contrast, another
potential split is inaccurate, and predicts the wrong
context words. This is downweighted.

known and unknown components. The model can
then use the semantic knowledge it has learnt for
a known component to predict a representation for
the unknown word as a whole.

We hypothesize that the natural place to split
words is on morpheme boundaries, as morphemes
are the smallest unit of language which carry se-
mantic meaning. We test the splitting capabilities
of our model in §4.1.

4 Experiments

We evaluate our model on three tasks: morpho-
logical analysis (§4.1), semantic similarity (§4.2),
and analogy retrieval (§4.3). We trained all of the
models once, and then use the same trained model
for all three tasks – we do not perform hyperpa-
rameter tuning to optimize performance on each
task.

We trained our Char2Vec model on the Text8
corpus, consisting of the first 100MB of a 2006

21



cleaned-up dump of Wikipedia1. We only trained
on words which appeared more than 5 times in our
corpus. We used a context window size of 3 words
either side of the target word, and took 11 nega-
tive samples per positive sample, using the same
smoothed unigram distribution as word2vec.
The model was trained for 3 epochs using the
Adam optimizer (Kingma and Ba, 2015). All ex-
periments were carried out using Keras (Chollet,
2015) and Theano (Bergstra et al., 2010; Bastien
et al., 2012). We initialized the context lookup
table using word2vec2, and kept it fixed during
training. 3 In all character-level models, the char-
acter embeddings have dimension dC = 64, while
the forward and backward LSTMs have dimension
dLSTM = 256. The concatenation of both there-
fore has dimensionality d = 512. The concate-
nated LSTM hidden states are then compressed
down to dword = 256 by a feed-forward layer.

As baselines, we trained a SGNS model on the
same dataset with the same parameters. To test
how much the attention model helps the character-
level model to generalize, we also trained the
Char2Vec model without the attention layer, but
with the same parameters. In this model, the word
embeddings are just the concatenation of the fi-
nal forward and backward states, passed through a
feedforward layer. We refer to this model as C2V-
NO-ATT. We also constructed count-based vec-
tors using SVD on PPMI-weighted co-occurence
counts, with a window size of 3. We kept the top
256 principal components in the SVD decomposi-
tion, to obtain embeddings with the same size as
our other models.

4.1 Morphological awareness
The main innovation of our Char2Vec model com-
pared to existing recurrent character-level models
is the capability to split words and model each half
independently. Here we test whether our model
segmentations correspond to gold-standard mor-
phological analyses.

We obtained morphological analyses for all the
words in our training vocabulary which were in the
English Lexicon Project (Balota et al., 2007). We
then converted these into surface-level segmenta-

1available at mattmahoney.net/dc/text8
2We use the Gensim implementation:

https://radimrehurek.com/gensim/
3We experimented with updating the initialized context

lookup tables, and with randomly initialized context lookups,
but found they were influenced too much by orthographic
similarity from the character encoder.

tions using heuristic affix-matching, and used this
as a gold-standard morphemic analysis. We ended
up with 14682 words, of which 7867 have at least
two morphemes and 1138 have at least three.

Evaluating morphological segmentation is a
long-debated issue (Cotterell et al., 2016). Tra-
ditional hard morphological analyzers are nor-
mally evaluated on border F1 – that is, how many
morpheme borders are recovered. However, our
model does not actually posit any hard morpheme
borders. Instead, it just associates each charac-
ter boundary with a weight. Therefore, we treat
the problem of recovering intra-word morpheme
boundaries as a ranking problem. We rank each
inter-character boundary of a word according to
our model weights, and then evaluate whether our
model ranks morpheme boundaries above non-
morpheme boundaries.

We use mean average precision (MAP) as our
evaluation metric. We first calculate precision at
N for each word, until all the gold standard mor-
pheme boundaries have been recovered. Then, we
average over N to obtain the average precision
(AP) for that word. We then calculate the mean
of the APs across all words to obtain the MAP for
the model.

We report results of a random baseline as a point
of comparison, which randomly places morpheme
boundaries inside the word. We also report the
results of the Porter stemmer4, where we place a
morpheme boundary at the end of the stem, then
randomly thereafter.

Finally, we trained Morfessor 2.05 (Creutz and
Lagus, 2007) on our corpus, using an initial ran-
dom split value of 0.9, and stopping training when
the difference in loss between successive epochs is
less than 0.1% of the total loss. While Morfessor
is no longer state-of-the-art in morpheme recovery
(see, e.g. Narasimhan et al. (2015) for more re-
cent work), it has previously been as a component
in pipelines to build compositional word represen-
tations (Luong et al., 2013; Botha and Blunsom,
2014). We then used our trained Morfessor model
to predict morpheme boundaries6, and randomly
permuted the predicted morpheme boundaries and
ranked them ahead of randomly permuted non-
morpheme boundaries to calculate MAP.

4We used the NLTK implementation
5We used the Python implementation
6We found Morfessor to be quite conservative by default

in its segmentations. The 2nd ranked segmentation gave bet-
ter MAPs, which are the results we describe.

22



Model All word MAP Rich-morphology MAP

Random 0.233 0.261
Porter Stemmer 0.705 0.446

Morfessor 0.631 0.500
Char2Vec 0.593 0.586

Table 1: Results at retrieving intra-word mor-
pheme boundaries.

Word Model analysis Gold-standard analysis

carrying carry |ing carry |ing
leninism lenin |ism lenin |ism

lesbianism lesbia |nism lesbian |ism
buses buse |s bus |es

government gove |rnment govern |ment
unrepentant un |repent |ant un |repent |ant
weaknesses weak |nes |ses weak |ness |es

Table 2: Morphological analyses for sample words
from the corpus. We take the top N model predic-
tions as the split points, where N is the number of
gold-standard morphemes in the word.

As the test set is dominated by words with sim-
ple morphology, we also extracted all the morpho-
logically rich words with 3 or more morphemes,
and created a separate evaluation on this subsec-
tion. We report the results in Table 1.

As the results show, our model performs the
best out of all the methods at analysing morpho-
logically rich words with multiple morphemes. On
these words, our model even outperforms Morfes-
sor, which is explicitly designed as a morpholog-
ical analyzer. This shows that our model learns
splits which correspond well to human morpho-
logical analysis, even though we build no morpho-
logical knowledge into our model. However, when
evaluating on all words, the Porter stemmer has a
great advantage, as it is rule-based and able to give
just the stem of words with great precision, which
is effectively giving a canonical segmentation for
words with just 2 morphemes.

We show some model analyses against the gold
standard in Table 2.

4.2 Capturing semantic similarity
Next, we tested our model similarity scores
against human similarity judgments. For these
datasets, human annotators are asked to judge how
similar two words are on a fixed scale. Model
word vectors are evaluated based on ranking the
word pairs according to their cosine similarity, and
then measuring the correlation (using Spearman’s

Model WordSim353 MEN Test RW

PPMI-SVD 0.607 0.601 0.293
SGNS 0.667 0.557 0.388
C2V-NO-ATT 0.361 0.298 0.317
CHAR2VEC 0.345 0.322 0.282

Table 3: Similarity correlations of in-vocabulary
word pairs between the models and human anno-
tators.

Model WordSim353 MEN Test RW RW OOV

C2V-NO-ATT 0.358 0.292 0.273 0.233
CHAR2VEC 0.340 0.318 0.264 0.243

Table 4: Similarity correlations of all word pairs
between the character-level models and human an-
notators. RW OOV indicates results specifically
on pairs in the RW dataset with at least one word
not seen in the training corpus.

ρ) between model judgments and human judg-
ments (Levy et al., 2015).

We use the WordSim353 dataset (Finkelstein et
al., 2002), the test split of the MEN dataset (Bruni
et al., 2014), and the Rare Word (RW) dataset (Lu-
ong et al., 2013). The word pairs in the Word-
Sim353 and MEN datasets are typically simple,
commonly occurring words denoting basic con-
cepts, whereas the RW dataset contains many mor-
phologically derived words which have low corpus
frequencies. This is reflected by how many of the
test pairs in each dataset contain out of vocabu-
lary (OOV) items: 3/353 and 6/1000 of the word
pairs in WordSim353 and MEN, compared with
1083/2034 for the RW dataset.

We report results for in-corpus word pairs in Ta-
ble 3, and for all word pairs for those models able
to predict vectors for unseen words in Table 4.

Overall, word-based embedding models learn
vectors that correlate better with human judg-
ments, particularly for morphologically simple
words. However, character-based models are
competitive with word-based models on the RW
dataset. While the words in this dataset appear
rarely in our corpus (of the in-corpus words, over
half appear fewer than 100 times), each morpheme
may be common, and the character-level models
can use this information. We note that on the entire
RW dataset (of which over half contain an OOV
word), the character-based models still perform
reasonably. We also note that on word pairs in the
RW test containing at least one OOV word, the

23



In-vocabulary Out-of-Vocabulary
germany football bible foulness definately

Char2Vec unfiltered

germaine footballer bibles illness definitely
germanies footballing testament seriousness indefinitely
germain footballing librarianship sickness enthusiastically
germano foosball literature loudness emphatically

germaniae footballers librarian cuteness consistently

Char2Vec filtered

poland footballer testament illness definitely
german basketball literature blindness consistently
spain tennis hebrew consciousness drastically

germans rugby judaism hardness theoretically
france baseball biblical weakness infinitely

Table 5: Filtered and unfiltered model nearest neighbours for some in-vocabulary and out-of-vocabulary
words

full Char2Vec model outperforms the C2V model
without morphology. This suggests that character-
based embedding models are learning to morpho-
logically analyse complex word forms, even on
unseen words, and that giving the model the capa-
bility to learn word segments independently helps
this process.

We also present some word nearest neighbours
for our Char2Vec model in Table 5, both on the
whole vocabulary and then filtering the nearest
neighbours to only include words which appear
100 times or more in our corpus. This corresponds
to keeping the top 10k words, which is common
among language models (Ling et al., 2015; Kim et
al., 2016). We note that nearest neighbour pre-
dictions include words that are orthographically
distant but semantically similar, showing that our
model has the capability to learn to compose char-
acters into word meanings.

We also note that word nearest neighbours seem
to be more semantically coherent when rarely-
observed words are filtered out of the vocabulary,
and more based on orthographic overlap when the
entire vocabulary is included. This suggests that
for rarely-observed words, the model is basing its
predictions on orthographic analysis, whereas for
more commonly observed words it can ‘memo-
rize’ the mapping between the orthography and
word semantics.

4.3 Capturing syntactic and semantic
regularity

Finally, we evaluate the structure of the embed-
ding space of our various models. In particular,

Model All Acc Sem. Acc Syn. Acc

PPMI-SVD 0.365 0.444 0.341
SGNS 0.436 0.339 0.513
C2V-NO-ATT 0.316 0.016 0.472
CHAR2VEC 0.355 0.025 0.525

Table 6: Results on the Google analogy task

we test whether affixation corresponds to regular
linear shifts in the embedding space.

To do this, we use the Google analogy dataset
(Mikolov et al., 2013a). This consists of 19544
questions of the form “A is to B as C is to X”.
We split this collection into semantic and syntactic
sections, based on whether the analogies between
the words are driven by morphological changes or
deeper semantic shifts. Example semantic ques-
tions are on capital-country relationships (“Paris
is to France as Berlin is to X”) and currency-
country relationships (“pound is to Great Britain
as dollar is to X”). Example syntactic questions
are adjective-adverb relationships (“amazing is to
amazingly as apparent is to X”) and opposites
formed by prefixing a negation particle (“accept-
able is to unacceptable as aware is to X”). This
results in 5537 semantic analogies and 10411 syn-
tactic analogies.

We use the method of Mikolov et al. (2013a) to
answer these questions. We first `2-normalize all
of our word vectors. Then, to answer a question of
the form “A is to B as C is to X”, we find the word

24



w which satisfies

w = argmax
w∈V−{a,b,c}

cos(w, b− a+ c) (4)

where a, b, c are the word vectors for the words A,
B and C respectively.

We report the results in Table 6. The most in-
triguing result is that character-level models are
competitive with word-level models for syntactic
analogy, with our Char2Vec model holding the
best result for syntactic analogy answering. This
suggests that incorporating morphological knowl-
edge explicitly rather than latently helps the model
learn morphological features. However, on the se-
mantic analogies, the character-based models do
much worse than the word-based models. This is
perhaps unsurprising in light of the previous sec-
tion, where we demonstrate that character-based
models do worse at the semantic similarity task
than word-level models.

5 Discussion

We only report results for English. However, En-
glish is a morphologically impoverished language,
with little inflection and relatively few productive
patterns of derivation. Our morphology test set re-
flects this, with over half the words consisting of a
simple morpheme, and over 90% having at most 2
morphemes.

This is unfortunate for our model, as it performs
better on words with richer morphology. It gives
consistently more accurate morphological analy-
ses for these words compared to standard base-
lines, and matches word-level models for seman-
tic similarity on rare words with rich morphol-
ogy. In addition, it seems to learn morphosyntactic
features to help solve the syntactic analogy task.
Most of all, it is language-agnostic, and easy to
port across different languages. We thus expect
our model to perform even better for languages
with a richer morphology than English, such as
Turkish and German.

6 Conclusion

In this paper, we present a model which learns
morphology and word embeddings jointly. Given
a word, it splits the word in to segments and ranks
the segments based on their context-predictive
power. Our model can segment words into mor-
phemes, and also embed the word into a represen-
tation space.

We show that our model is competitive at the
task of morpheme boundary recovery compared to
a dedicated morphological analyzer, beating dedi-
cated analyzers on words with a rich morphology.
We also show that in the representation space word
affixation corresponds to linear shifts, demonstrat-
ing that our model can learn morphological fea-
tures.

Finally, we show that character-level models,
while outperformed by word-level models gener-
ally at the task of semantic similarity, are com-
petitive at representing rare morphologically rich
words. In addition, the character-level models can
predict good quality representations for unseen
words, with the morphologically aware character-
level model doing slightly better.

References
Harald R. Baayen, Richard Piepenbrock, and Leon Gu-

likers. 1995. The CELEX Lexical Database. Re-
lease 2 (CD-ROM). Linguistic Data Consortium,
University of Pennsylvania, Philadelphia, Pennsyl-
vania.

Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben-
gio. 2014. Neural machine translation by jointly
learning to align and translate. ICLR.

Miguel Ballesteros, Chris Dyer, and Noah A Smith.
2015. Improved transition-based parsing by mod-
eling characters instead of words with LSTMs.
EMNLP.

David A. Balota, Melvin J. Yap, Keith A. Hutchi-
son, Michael J. Cortese, Brett Kessler, Bjorn Loftis,
James H. Neely, Douglas L. Nelson, Greg B. Simp-
son, and Rebecca Treiman. 2007. The english lexi-
con project. Behavior Research Methods.

Frédéric Bastien, Pascal Lamblin, Razvan Pascanu,
James Bergstra, Ian J. Goodfellow, Arnaud Berg-
eron, Nicolas Bouchard, and Yoshua Bengio. 2012.
Theano: new features and speed improvements.
Deep Learning and Unsupervised Feature Learning
NIPS 2012 Workshop.

Yoshua Bengio, Réjean Ducharme, Pascal Vincent, and
Christian Janvin. 2003. A neural probabilistic lan-
guage model. J. Mach. Learn. Res., 3:1137–1155,
March.

James Bergstra, Olivier Breuleux, Frédéric Bastien,
Pascal Lamblin, Razvan Pascanu, Guillaume Des-
jardins, Joseph Turian, David Warde-Farley, and
Yoshua Bengio. 2010. Theano: a CPU and
GPU math expression compiler. In Proceedings
of the Python for Scientific Computing Conference
(SciPy), June.

25



Jan A. Botha and Phil Blunsom. 2014. Composi-
tional Morphology for Word Representations and
Language Modelling. In ICML, Beijing, China, jun.
*Award for best application paper*.

Elia Bruni, Nam Khanh Tran, and Marco Baroni. 2014.
Multimodal distributional semantics. J. Artif. Int.
Res.

John A. Bullinaria and Joseph P. Levy. 2007. Ex-
tracting semantic representations from word co-
occurrence statistics: A computational study. Be-
havior Research Methods.

Franois Chollet. 2015. Keras. https://github.
com/fchollet/keras.

Ronan Collobert, Jason Weston, Léon Bottou, Michael
Karlen, Koray Kavukcuoglu, and Pavel Kuksa.
2011. Natural language processing (almost) from
scratch. JMLR, 12:2493–2537, November.

Ryan Cotterell, Tim Vieira, and Hinrich Schütze. 2016.
A joint model of orthography and word segmenta-
tion. In NAACL.

Mathias Creutz and Krista Lagus. 2007. Unsupervised
models for morpheme segmentation and morphol-
ogy learning. ACM Trans. Speech Lang. Process.,
4(1):3:1–3:34, February.

Ferdinand de Saussure. 1916. Course in General Lin-
guistics.

Lev Finkelstein, Evgeniy Gabrilovich, Yossi Matias,
Ehud Rivlin, Zach Solan, Gadi Wolfman, and Ey-
tan Ruppin. 2002. Placing search in context: The
concept revisited. ACM Trans. Inf. Syst.

John Goldsmith. 2001. Unsupervised learning of the
morphology of a natural language. Computational
Linguistics, 27(2):153–198, June.

Sharon Goldwater, Mark Johnson, and Thomas L. Grif-
fiths. 2005. Interpolating between types and tokens
by estimating power-law generators. In Y. Weiss,
B. Schlkopf, and J. Platt, editors, NIPS, pages 459–
466. MIT Press, Cambridge, MA.

Zelig S. Harris. 1954. Distributional structure. Word.

Sepp Hochreiter and Jürgen Schmidhuber. 1997.
Long short-term memory. Neural Computation,
9(8):1735–1780, November.

Yoon Kim, Yacine Jernite, David Sontag, and Alexan-
der M. Rush. 2016. Character-aware neural lan-
guage models. AAAI.

Diederik P. Kingma and Jimmy Ba. 2015. Adam: A
method for stochastic optimization. ICLR.

Angeliki Lazaridou, Marco Marelli, Roberto Zampar-
elli, and Marco Baroni. 2013. Compositional-ly
derived representations of morphologically complex
words in distributional semantics. ACL.

Omer Levy, Yoav Goldberg, and Ido Dagan. 2015. Im-
proving distributional similarity with lessons learned
from word embeddings. Transactions of the Associ-
ation for Computational Linguistics, 3:211–225.

Wang Ling, Tiago Luı́s, Luı́s Marujo, Ramón Fernan-
dez Astudillo, Silvio Amir, Chris Dyer, Alan W.
Black, and Isabel Trancoso. 2015. Finding function
in form: Compositional character models for open
vocabulary word representation. EMNLP.

Minh-Thang Luong, Richard Socher, and Christo-
pher D. Manning. 2013. Better word representa-
tions with recursive neural networks for morphol-
ogy. In CoNLL.

Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey
Dean. 2013a. Efficient estimation of word represen-
tations in vector space. ICLR Workshop.

Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg Cor-
rado, and Jeffrey Dean. 2013b. Distributed repre-
sentations of words and phrases and their composi-
tionality. NIPS.

Andriy Mnih and Koray Kavukcuoglu. 2013. Learning
word embeddings efficiently with noise-contrastive
estimation. In C. J. C. Burges, L. Bottou,
M. Welling, Z. Ghahramani, and K. Q. Weinberger,
editors, Advances in Neural Information Processing
Systems 26, pages 2265–2273. Curran Associates,
Inc.

Karthik Narasimhan, Regina Barzilay, and Tommi S.
Jaakkola. 2015. An unsupervised method for un-
covering morphological chains. TACL.

Radu Soricut and Franz Ochs. 2015. Unsuper-
vised morphology induction using word embed-
dings. NAACL.

Kristina Toutanova, Dan Klein, Christopher D. Man-
ning, and Yoram Singer. 2003. Feature-rich part-of-
speech tagging with a cyclic dependency network.
In In Proceedings of NAACL.

Wenduan Xu, Michael Auli, and Stephen Clark. 2015.
CCG supertagging with a recurrent neural network.
ACL.

Xiang Zhang, Junbo Zhao, and Yann LeCun. 2015.
Character-level convolutional networks for text clas-
sification. NIPS.

George Kingsley Zipf. 1935. The psycho-biology of
language.

26


