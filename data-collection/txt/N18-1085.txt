



















































Neural Particle Smoothing for Sampling from Conditional Sequence Models


Proceedings of NAACL-HLT 2018, pages 929–941
New Orleans, Louisiana, June 1 - 6, 2018. c©2018 Association for Computational Linguistics

Neural Particle Smoothing
for Sampling from Conditional Sequence Models

Chu-Cheng Lin and Jason Eisner
Center for Language and Speech Processing

Johns Hopkins University, Baltimore MD, 21218
{kitsing,jason}@cs.jhu.edu

Abstract

We introduce neural particle smoothing, a
sequential Monte Carlo method for sampling
annotations of an input string from a given
probability model. In contrast to conventional
particle filtering algorithms, we train a proposal
distribution that looks ahead to the end of
the input string by means of a right-to-left
LSTM. We demonstrate that this innovation can
improve the quality of the sample. To motivate
our formal choices, we explain how our neural
model and neural sampler can be viewed as
low-dimensional but nonlinear approximations
to working with HMMs over very large state
spaces.

1 Introduction

Many structured prediction problems in NLP can
be reduced to labeling a length-T input string x
with a length-T sequence y of tags. In some cases,
these tags are annotations such as syntactic parts of
speech. In other cases, they represent actions that
incrementally build an output structure: IOB tags
build a chunking of the input (Ramshaw and Marcus,
1999), shift-reduce actions build a tree (Yamada and
Matsumoto, 2003), and finite-state transducer arcs
build an output string (Pereira and Riley, 1997).

One may wish to score the possible taggings us-
ing a recurrent neural network, which can learn to be
sensitive to complex patterns in the training data. A
globally normalized conditional probability model
is particularly valuable because it quantifies uncer-
tainty and does not suffer from label bias (Lafferty
et al., 2001); also, such models often arise as the
predictive conditional distribution p(y | x) corre-
sponding to some well-designed generative model
p(x,y) for the domain. In the neural case, however,
inference in such models becomes intractable. It is
hard to know what the model actually predicts and
hard to compute gradients to improve its predictions.

In such intractable settings, one generally falls
back on approximate inference or sampling. In the
NLP community, beam search and importance sam-

pling are common. Unfortunately, beam search con-
siders only the approximate-top-k taggings from
an exponential set (Wiseman and Rush, 2016), and
importance sampling requires the construction of a
good proposal distribution (Dyer et al., 2016).

In this paper we exploit the sequential structure
of the tagging problem to do sequential importance
sampling, which resembles beam search in that it
constructs its proposed samples incrementally—one
tag at a time, taking the actual model into account at
every step. This method is known as particle filtering
(Doucet and Johansen, 2009). We extend it here to
take advantage of the fact that the sampler has access
to the entire input string as it constructs its tagging,
which allows it to look ahead or—as we will show—
to use a neural network to approximate the effect
of lookahead. Our resulting method is called neural
particle smoothing.

1.1 What this paper provides

For x = x1 · · ·xT , let x:t and xt: respectively de-
note the prefix x1 · · ·xt and the suffix xt+1 · · ·xT .

We develop neural particle smoothing—a se-
quential importance sampling method which, given
a string x, draws a sample of taggings y from
pθ(y | x). Our method works for any conditional
probability model of the quite general form1

pθ(y | x)
def∝ expGT (1)

where G is an incremental stateful global scoring
model that recursively defines scores Gt of prefixes
of (x,y) at all times 0 ≤ t ≤ T :

Gt
def
= Gt−1 + gθ(st−1, xt, yt) (with G0

def
= 0) (2)

st
def
= fθ(st−1, xt, yt) (with s0 given) (3)

These quantities implicitly depend on x,y and θ.
Here st is the model’s state after observing the pair
of length-t prefixes (x:t,y:t). Gt is the score-so-far

1A model may require for convenience that each input end
with a special end-of-sequence symbol: that is, xT = EOS.

929



of this prefix pair, while GT − Gt is the score-to-
go. The state st summarizes the prefix pair in the
sense that the score-to-go depends only on st and the
length-(T − t) suffixes (xt:,yt:). The local scoring
function gθ and state update function fθ may be
any functions parameterized by θ—perhaps neural
networks. We assume θ is fixed and given.

This model family is expressive enough to capture
any desired p(y | x). Why? Take any distribution
p(x,y) with this desired conditionalization (e.g.,
the true joint distribution) and factor it as

log p(x,y)=
∑T

t=1 log p(xt, yt | x:t−1,y:t−1)
=
∑T

t=1 log p(xt, yt | st−1)︸ ︷︷ ︸
use as gθ(st−1,xt,yt)

= GT (4)

by making st include as much information about
(x:t,y:t) as needed for (4) to hold (possibly st =
(x:t,y:t)).

2 Then by defining gθ as shown in (4), we
get p(x,y) = expGT and thus (1) holds for each
x.

1.2 Relationship to particle filtering
Our method is spelled out in §4 (one may look now).
It is a variant of the popular particle filtering method
that tracks the state of a physical system in discrete
time (Ristic et al., 2004). Our particular proposal
distribution for yt can be found in equations (5), (6),
(25) and (26). It considers not only past observations
x:t as reflected in st−1, but also future observations
xt:, as summarized by the state s̄t of a right-to-left
recurrent neural network f̄ that we will train:

Ĥt
def
= hφ(s̄t+1, xt+1) + Ĥt+1 (5)

s̄t
def
= f̄φ(s̄t+1, xt+1) (with sT given) (6)

Conditioning the distribution of yt on future obser-
vations xt: means that we are doing “smoothing”
rather than “filtering” (in signal processing terminol-
ogy). Doing so can reduce the bias and variance of
our sampler. It is possible so long as x is provided in
its entirety before the sampler runs—which is often
the case in NLP.

1.3 Applications
Why sample from pθ at all? Many NLP systems
instead simply search for the Viterbi sequence y that
maximizes GT and thus maximizes pθ(y | x). If
the space of states s is small, this can be done effi-
ciently by dynamic programming (Viterbi, 1967); if

2Furthermore, st could even depend on all of x (if s0 does),
allowing direct expression of models such as stacked BiRNNs.

not, then A∗ may be an option (see §2). More com-
mon is to use an approximate method: beam search,
or perhaps a sequential prediction policy trained
with reinforcement learning. Past work has already
shown how to improve these approximate search
algorithms by conditioning on the future (Bahdanau
et al., 2017; Wiseman and Rush, 2016).

Sampling is essentially a generalization of
maximization: sampling from exp GTtemperature
approaches maximization as temperature→ 0. It
is a fundamental building block for other algorithms,
as it can be used to take expectations over the whole
space of possible y values. For unfamiliar readers,
Appendix E reviews how sampling is crucially used
in minimum-risk decoding, supervised training,
unsupervised training, imputation of missing data,
pipeline decoding, and inference in graphical
models.

2 Exact Sequential Sampling

To develop our method, it is useful to first consider
exact samplers. Exact sampling is tractable for only
some of the models allowed by §1.1. However, the
form and notation of the exact algorithms in §2 will
guide our development of approximations in §3.

An exact sequential sampler draws yt from
pθ(yt | x,y:t−1) for each t = 1, . . . , T in sequence.
Then y is exactly distributed as pθ(y | x).

For each given x,y:t−1, observe that

pθ(yt | x,y:t−1) (7)
∝ pθ(y:t | x) =

∑
yt:
pθ(y | x) (8)

∝∑yt: expGT (9)
= exp (Gt + log

∑
yt:

exp (GT −Gt)︸ ︷︷ ︸
call this Ht

) (10)

= exp (Gt−1 + gθ(st−1, xt, yt) +Ht) (11)

∝ exp (gθ(st−1, xt, yt) +Ht) (12)

Thus, we can easily construct the needed distribu-
tion (7) by normalizing (12) over all possible values
of yt. The challenging part of (12) is to compute Ht:
as defined in (10), Ht involves a sum over exponen-
tially many futures yt:. (See Figure 1.)

We chose the symbols G and H in homage to
the A∗ search algorithm (Hart et al., 1968). In that
algorithm (which could be used to find the Viterbi
sequence), g denotes the score-so-far of a partial
solution y:t, and h denotes the optimal score-to-
go. Thus, g + h would be the score of the best
sequence with prefix y:t. Analogously, our Gt +

930



x1=“On” x2=“Thursday” … xt-1=“Fed” xt=“raised” xt+1=“interest” xt+2=“rates” …

y1=“PREP” y2=“N” … yt-1=“N”

yt=“ADJ”

…

yt=“V”

yt+1=“V”

…

yt+1=“N”

…

yt+2=“N”

yt+2=“N”

Ht

x

y

gθ(st-1, xt, yt)

Gt-1

Figure 1: Sampling a single particle from a tagging model. y1, . . . , yt−1 (orange) have already been chosen, with a total
model score of Gt−1, and now the sampler is constructing a proposal distribution q (purple) from which the next tag yt will be
sampled. Each yt is evaluated according to its contribution to Gt (namely gθ) and its future score Ht (blue). The figure illustrates
quantities used throughout the paper, beginning with exact sampling in equations (7)–(12). Our main idea (§3) is to approximate
the Ht computation (a log-sum-exp over exponentially many sequences) when exact computation by dynamic programming is
not an option. The form of our approximation uses a right-to-left recurrent neural network but is inspired by the exact dynamic
programming algorithm.

Ht is the log of the total exponentiated scores of
all sequences with prefix y:t. Gt and Ht might be
called the logprob-so-far and logprob-to-go of y:t.

Just as A∗ approximates h with a “heuristic” ĥ,
the next section will approximate Ht using a neural
estimate Ĥt (equations (5)–(6)). However, the spe-
cific form of our approximation is inspired by cases
where Ht can be computed exactly. We consider
those in the remainder of this section.

2.1 Exact sampling from HMMs
A hidden Markov model (HMM) specifies a nor-
malized joint distribution pθ(x,y) = expGT over
state sequence y and observation sequence x,3 Thus
the posterior pθ(y | x) is proportional to expGT ,
as required by equation (1).

The HMM specifically defines GT by equa-
tions (2)–(3) with st = yt and gθ(st−1, xt, yt) =
log pθ(yt | yt−1) + log pθ(xt | yt).4

In this setting, Ht can be computed exactly by
the backward algorithm (Rabiner, 1989). (Details
are given in Appendix A for completeness.)

2.2 Exact sampling from OOHMMs
For sequence tagging, a weakness of (first-order)
HMMs is that the model state st = yt may contain
little information: only the most recent tag yt is
remembered, so the number of possible model states
st is limited by the vocabulary of output tags.

We may generalize so that the data generating
process is in a latent state ut ∈ {1, . . . , k} at each
time t, and the observed yt—along with xt—is gen-
erated from ut. Now k may be arbitrarily large. The

3The HMM actually specifies a distribution over a pair of in-
finite sequences, but here we consider the marginal distribution
over just the length-T prefixes.

4It takes s0 = BOS, a beginning-of-sequence symbol, so
pθ(y1 | BOS) specifies the initial state distribution.

model has the form

pθ(x,y) = expGT (13)

=
∑

u

T∏

t=1

pθ(ut | ut−1) · pθ(xt, yt | ut)

This is essentially a pair HMM (Knudsen and
Miyamoto, 2003) without insertions or deletions,
also known as an “�-free” or “same-length” proba-
bilistic finite-state transducer. We refer to it here as
an output-output HMM (OOHMM).5

Is this still an example of the general model ar-
chitecture from §1.1? Yes. Since ut is latent and
evolves stochastically, it cannot be used as the state
st in equations (2)–(3) or (4). However, we can de-
fine st to be the model’s belief state after observing
(x:t,y:t). The belief state is the posterior probability
distribution over the underlying state ut of the sys-
tem. That is, st deterministically keeps track of all
possible states that the OOHMM might be in—just
as the state of a determinized FSA keeps track of
all possible states that the original nondeterministic
FSA might be in.

We may compute the belief state in terms of a
vector of forward probabilities that starts at α0,

(α0)u
def
=

{
1 if u = BOS (see footnote 4)
0 if u = any other state

(14)

and is updated deterministically for each 0 < t ≤ T
by the forward algorithm (Rabiner, 1989):

(αt)u
def
=

k∑

u′=1

(αt−1)u′ · pθ(u | u′) · pθ(xt, yt | u)
(15)

5This is by analogy with the input-output HMM (IOHMM)
of Bengio and Frasconi (1996), which defines p(y | x) directly
and conditions the transition to ut on xt. The OOHMM instead
defines p(y | x) by conditionalizing (13)—which avoids the
label bias problem (Lafferty et al., 2001) that in the IOHMM,
yt is independent of future input xt: (given the past input x:t).

931



(αt)u can be interpreted as the logprob-so-far if the
system is in state u after observing (x:t,y:t). We
may express the update rule (15) by α>t = α

>
t−1P

where the matrix P depends on (xt, yt), namely
Pu′u

def
= pθ(u | u′) · pθ(xt, yt | u).

The belief state st
def
= JαtK ∈ Rk simply nor-

malizes αt into a probability vector, where JuK def=
u/(u>1) denotes the normalization operator. The
state update (15) now takes the form (3) as desired,
with fθ a normalized vector-matrix product:

s>t = fθ(st−1, xt, yt)
def
= Js>t−1P K (16)

As in the HMM case, we define Gt as the log of
the generative prefix probability,

Gt
def
= log pθ(x:t,y:t) = log

∑
u(αt)u (17)

which has the form (2) as desired if we put

gθ(st−1, xt, yt)
def
= Gt −Gt−1 (18)

= log
α>t−1P1

α>t−11
= log (s>t−1P1)

Again, exact sampling is possible. It suffices to
compute (9). For the OOHMM, this is given by

∑
yt:

expGT = α
>
t βt (19)

where βT
def
= 1 and the backward algorithm

(βt)v
def
= pθ(xt: | ut = u) (20)
=
∑

ut:,yt:

pθ(ut:,xt:,yt: | ut = u)

=
∑

u′
pθ(u

′ | u) · p(xt+1 | u′)︸ ︷︷ ︸
call this Puu′

·(βt+1)u′

for 0 ≤ t < T uses dynamic programming to find
the total probability of all ways to generate the fu-
ture observations xt:. Note that αt is defined for
a specific prefix y:t (though it sums over all u:t),
whereas βt sums over all suffixes yt: (and over all
ut:), to achieve the asymmetric summation in (19).

Define s̄t
def
= JβtK ∈ Rk to be a normalized ver-

sion of βt. The βt recurrence (20) can clearly be ex-
pressed in the form s̄t = JP s̄t+1K, much like (16).

2.3 The logprob-to-go for OOHMMs
Let us now work out the definition of Ht for
OOHMMs (cf. equation (35) in Appendix A for
HMMs). We will write it in terms of Ĥt from §1.2.
Let us define Ĥt symmetrically to Gt (see (17)):

Ĥt
def
= log

∑

u

(βt)u (= log 1
>βt) (21)

which has the form (5) as desired if we put

hφ(s̄t+1, xt+1)
def
= Ĥt − Ĥt+1 = log (1>P s̄t+1)

(22)

From equations (10), (17), (19) and (21), we see

Ht = log
(∑

yt:

expGT
)
−Gt

= log
α>t βt

(α>t 1)(1>βt)
+ log (1>βt)

= log s>t s̄t︸ ︷︷ ︸
call this Ct

+Ĥt (23)

where Ct ∈ R can be regarded as evaluating the
compatibility of the state distributions st and s̄t.

In short, the generic strategy (12) for exact sam-
pling says that for an OOHMM, yt is distributed as

pθ(yt | x,y:t−1) ∝ exp (gθ(st−1, xt, yt) +Ht)
∝ exp ( gθ(st−1, xt, yt)︸ ︷︷ ︸

depends on x:t,y:t

+ Ct︸︷︷︸
on x,y:t

+ Ĥt︸︷︷︸
on xt:

)

∝ exp (gθ(st−1, xt, yt) + Ct) (24)

This is equivalent to choosing yt in proportion to
(19)—but we now turn to settings where it is infea-
sible to compute (19) exactly. There we will use
the formulation (24) but approximate Ct. For com-
pleteness, we will also consider how to approximate
Ĥt, which dropped out of the above distribution
(because it was the same for all choices of yt) but
may be useful for other algorithms (see §4).

3 Neural Modeling as Approximation

3.1 Models with large state spaces
The expressivity of an OOHMM is limited by the
number of states k. The state ut ∈ {1, . . . , k} is a
bottleneck between the past (x:t,y:t) and the future
(xt:,yt:), in that past and future are conditionally
independent given ut. Thus, the mutual information
between past and future is at most log2 k bits.

In many NLP domains, however, the past seems
to carry substantial information about the future.
The first half of a sentence greatly reduces the un-
certainly about the second half, by providing infor-
mation about topics, referents, syntax, semantics,
and discourse. This suggests that an accurate HMM
language model p(x) would require very large k—
as would a generative OOHMM model p(x,y) of
annotated language. The situation is perhaps better
for discriminative models p(y | x), since much of

932



the information for predicting yt: might be available
in xt:. Still, it is important to let (x:t,y:t) contribute
enough additional information about yt:: even for
short strings, making k too small (giving ≤ log2 k
bits) may harm prediction (Dreyer et al., 2008).

Of course, (4) says that an OOHMM can express
any joint distribution for which the mutual informa-
tion is finite,6 by taking k large enough for vt−1 to
capture the relevant info from (x:t−1,y:t−1).

So why not just take k to be large—say, k = 230

to allow 30 bits of information? Unfortunately, eval-
uatingGT then becomes very expensive—both com-
putationally and statistically. As we have seen, if
we define st to be the belief state JαtK ∈ Rk, up-
dating it at each observation (xt, yt) (equation (3))
requires multiplication by a k × k matrix P . This
takes time O(k2), and requires enough data to learn
O(k2) transition probabilities.

3.2 Neural approximation of the model

As a solution, we might hope that for the inputs
x observed in practice, the very high-dimensional
belief states JαtK ∈ Rk might tend to lie near a d-
dimensional manifold where d� k. Then we could
take st to be a vector in Rd that compactly encodes
the approximate coordinates of JαtK relative to the
manifold: st = ν(JαtK), where ν is the encoder.

In this new, nonlinearly warped coordinate sys-
tem, the functions of st−1 in (2)–(3) are no longer
the simple, essentially linear functions given by (16)
and (18). They become nonlinear functions operat-
ing on the manifold coordinates. (fθ in (16) should
now ensure that s>t ≈ ν(J(ν−1(st−1))>P K), and gθ
in (18) should now estimate log (ν−1(st−1))>P1.)
In a sense, this is the reverse of the “kernel trick”
(Boser et al., 1992) that converts a low-dimensional
nonlinear function to a high-dimensional linear one.

Our hope is that st has enough dimensions d� k
to capture the useful information from the true JαtK,
and that θ has enough dimensions� k2 to capture
most of the dynamics of equations (16) and (18).
We thus proceed to fit the neural networks fθ, gθ
directly to the data, without ever knowing the true k
or the structure of the original operators P ∈ Rk×k.

We regard this as the implicit justification for
various published probabilistic sequence models
pθ(y | x) that incorporate neural networks. These
models usually have the form of §1.1. Most simply,
(fθ, gθ) can be instantiated as one time step in an
RNN (Aharoni and Goldberg, 2017), but it is com-

6This is not true for the language of balanced parentheses.

mon to use enriched versions such as deep LSTMs.
It is also common to have the state st contain not
only a vector of manifold coordinates in Rd but also
some unboundedly large representation of (x,y:t)
(cf. equation (4)), so the fθ neural network can refer
to this material with an attentional (Bahdanau et al.,
2015) or stack mechanism (Dyer et al., 2015).

A few such papers have used globally normalized
conditional models that can be viewed as approx-
imating some OOHMM, e.g., the parsers of Dyer
et al. (2016) and Andor et al. (2016). That is the
case (§1.1) that particle smoothing aims to support.
Most papers are locally normalized conditional
models (e.g., Kann and Schütze, 2016; Aharoni and
Goldberg, 2017); these simplify supervised training
and can be viewed as approximating IOHMMs
(footnote 5). For locally normalized models,Ht = 0
by construction, in which case particle filtering
(which estimates Ht = 0) is just as good as particle
smoothing. Particle filtering is still useful for these
models, but lookahead’s inability to help them is
an expressive limitation (known as label bias) of
locally normalized models. We hope the existence
of particle smoothing (which learns an estimate
Ht) will make it easier to adopt, train, and decode
globally normalized models, as discussed in §1.3.

3.3 Neural approximation of logprob-to-go
We can adopt the same neuralization trick to approx-
imate the OOHMM’s logprob-to-go Ht = Ct + Ĥt.
We take s̄t ∈ Rd on the same theory that it is a low-
dimensional reparameterization of JβtK, and define
(f̄φ, hφ) in equations (5)–(6) to be neural networks.
Finally, we must replace the definition of Ct in (23)
with another neural network cφ that works on the
low-dimensional approximations:7

Ct
def
= cφ(st, s̄t) (except that CT

def
= 0) (25)

The resulting approximation to (24) (which does not
actually require hφ) will be denoted qθ,φ:

qθ,φ(yt | x,y:t−1)
def∝ exp (gθ(st−1, xt, yt) + Ct)

(26)

The neural networks in the present section are all
parameterized by φ, and are intended to produce an
estimate of the logprob-to-go Ht—a function of xt:,
which sums over all possible yt:.

By contrast, the OOHMM-inspired neural
networks suggested in §3.2 were used to specify an

7CT = 0 is correct according to (23). Forcing this ensures
HT = 0, so our approximation becomes exact as of t = T .

933



actual model of the logprob-so-far Gt—a function
of x:t and y:t—using separate parameters θ.

Arguably φ has a harder modeling job than θ
because it must implicitly sum over possible futures
yt:. We now consider how to get corrected samples
from qθ,φ even if φ gives poor estimates of Ht, and
then how to train φ to improve those estimates.

4 Particle smoothing

In this paper, we assume nothing about the given
model GT except that it is given in the form of
equations (1)–(3) (including the parameter vector θ).

Suppose we run the exact sampling strategy but
approximate pθ in (7) with a proposal distribution
qθ,φ of the form in (25)–(26). Suppressing the sub-
scripts on p and q for brevity, this means we are
effectively drawing y not from p(y | x) but from

q(y | x) =
T∏

t=1

q(yt | x,y:t−1) (27)

If Ct ≈ Ht+const within each yt draw, then q ≈ p.
Normalized importance sampling corrects

(mostly) for the approximation by drawing many se-
quences y(1), . . .y(M) IID from (27) and assigning
y(m) a relative weight of w(m) def= p(y

(m)|x)
q(y(m)|x) . This

ensemble of weighted particles yields a distribution

p̂(y)
def
=

∑M
m=1 w

(m)I(y=y(m))∑M
m=1 w

(m)
≈ p(y | x) (28)

that can be used as discussed in §1.3. To com-
pute w(m) in practice, we replace the numerator
p(y(m) | x) by the unnormalized version expGT ,
which gives the same p̂. Recall that each GT is a
sum

∑T
t=1 gθ(· · · ).

Sequential importance sampling is an equivalent
implementation that makes t the outer loop and m
the inner loop. It computes a prefix ensemble

Yt
def
= {(y(1):t , w

(1)
t ), . . . , (y

(M)
:t , w

(M)
t )} (29)

for each 0 ≤ t ≤ T in sequence. Initially,
(y

(m)
:0 , w

(m)
0 ) = (�, expC0) for all m. Then for

0 < t ≤ T , we extend these particles in parallel:

y
(m)
:t = y

(m)
:t−1y

(m)
t (concatenation) (30)

w
(m)
t = w

(m)
t−1

exp (gθ(st−1,xt,yt)+Ct−Ct−1)
q(yt|x,y:t−1) (31)

where each y(m)t is drawn from (26). Each Yt yields
a distribution p̂t over prefixes y:t, which estimates
the distribution pt(y:t)

def∝ exp (Gt+Ct). We return

p̂
def
= p̂T ≈ pT = p. This gives the same p̂ as in

(28): the final y(m)T are the same, with the same
final weights w(m)T =

expGT
q(y(m)|x) , whereGT was now

summed up as C0 +
∑T

t=1 gθ(· · · ) + Ct − Ct−1.
That is our basic particle smoothing strategy. If

we use the naive approximation Ct = 0 everywhere,
it reduces to particle filtering. In either case, various
well-studied improvements become available, such
as various resampling schemes (Douc and Cappé,
2005) and the particle cascade (Paige et al., 2014).8

An easy improvement is multinomial resampling.
After computing each p̂t, this replaces Yt with a set
of M new draws from p̂t (≈ pt), each of weight
1—which tends to drop low-weight particles and
duplicate high-weight ones.9 For this to usefully
focus the ensemble on good prefixes y:t, pt should
be a good approximation to the true marginal
p(y:t | x) ∝ exp (Gt+Ht) from (10). That is why
we arranged for pt(y:t) ∝ exp (Gt + Ct). Without
Ct, we would have only pt(y:t) ∝ expGt—which
is fine for the traditional particle filtering setting,
but in our setting it ignores future information in xt:
(which we have assumed is available) and also fa-
vors sequences y that happen to accumulate most of
their global score GT early rather than late (which
is possible when the globally normalized model
(1)–(2) is not factored in the generative form (4)).

5 Training the Sampler Heuristic

We now consider training the parameters φ of our
sampler. These parameters determine the updates f̄φ
in (6) and the compatibility function cφ in (25). As
a result, they determine the proposal distribution q
used in equations (27) and (31), and thus determine
the stochastic choice of p̂ that is returned by the
sampler on a given input x.

In this paper, we simply try to tune φ to yield
good proposals. Specifically, we try to ensure that
qφ(y | x) in equation (27) is close to p(y | x) from
equation (1). While this may not be necessary for
the sampler to perform well downstream,10 it does

8The particle cascade would benefit from an estimate of Ĥt,
as it (like A∗ search) compares particles of different lengths.

9While resampling mitigates the degeneracy problem, it
could also reduce the diversity of particles. In our experiments
in this paper, we only do multinomial resampling when the ef-
fective sample size of p̂t is lower than M2 . Doucet and Johansen
(2009) give a more thorough discussion on when to resample.

10In principle, one could attempt to train φ “end-to-end”
on some downstream objective by using reinforcement learn-
ing or the Gumbel-softmax trick (Jang et al., 2017; Maddison
et al., 2017). For example, we might try to ensure that p̂ closely
matches the model’s distribution p (equation (28))—the “na-

934



guarantee it (assuming that the model p is correct).
Specifically, we seek to minimize

(1−λ)KL(p||qφ) +λKL(qφ||p) (with λ ∈ [0, 1])
(32)

averaged over examples x drawn from a training
set.11 (The training set need not provide true y’s.)

The inclusive KL divergence KL (p||qφ) is an ex-
pectation under p. We estimate it by replacing pwith
a sample p̂, which in practice we can obtain with our
sampler under the current φ. (The danger, then, is
that p̂ will be biased when φ is not yet well-trained;
this can be mitigated by increasing the sample size
M when drawing p̂ for training purposes.)

Intuitively, this term tries to encourage qφ in fu-
ture to re-propose those y values that turned out to
be “good” and survived into p̂ with high weights.

The exclusive KL divergence KL(qφ||p) is an
expectation under qφ. Since we can sample from
qφ exactly, we can get an unbiased estimate of
∇φKL(qφ||p) with the likelihood ratio trick (Glynn,
1990).12 (The danger is that such “REINFORCE”
methods tend to suffer from very high variance.)

This term is a popular objective for variational
approximation. Here, it tries to discourage qφ from
re-proposing “bad” y values that turned out to have
low expGT relative to their proposal probability.

Our experiments balance “recall” (inclusive) and
“precision” (exclusive) by taking λ = 12 (which Ap-
pendix F compares to λ ∈ {0, 1}). Alas, because
of our approximation to the inclusive term, neither
term’s gradient will “find” and directly encourage
good y values that have never been proposed. Ap-
pendix B gives further discussion and formulas.

6 Models for the Experiments

To evaluate our methods, we needed pre-trained
models pθ. We experimented on several models. In
each case, we trained a generative model pθ(x,y),
so that we could try sampling from its posterior dis-
tribution pθ(y | x). This is a very common setting
where particle smoothing should be able to help.
Details for replication are given in Appendix C.
tural” goal of sampling. This objective can tolerate inaccurate
local proposal distributions in cases where the algorithm could
recover from them through resampling. Looking even farther
downstream, we might merely want p̂—which is typically used
to compute expectations—to provide accurate guidance to some
decision or training process (see Appendix E). This might not
require fully matching the model, and might even make it desir-
able to deviate from an inaccurate model.

11Training a single approximation qφ for all x is known as
amortized inference.

12The normalizing constant of p from (1) can be ignored
because the gradient of a constant is 0.

6.1 Tagging models

We can regard a tagged sentence (x,y) as a string
over the “pair alphabet” X × Y . We train an RNN
language model over this “pair alphabet”—this is a
neuralized OOHMM as suggested in §3.2:

log pθ(x,y) =

T∑

t=1

log pθ(xt, yt | st−1) (33)

This model is locally normalized, so that
log pθ(x,y) (as well as its gradient) is straightfor-
ward to compute for a given training pair (x,y).
Joint sampling from it would also be easy (§3.2).

However, p(y | x) is globally renormalized (by
an unknown partition function that depends on x,
namely expH0). Conditional sampling of y is there-
fore potentially hard. Choosing yt optimally re-
quires knowledge of Ht, which depends on the fu-
ture xt:.

As we noted in §1, many NLP tasks can be seen as
tagging problems. In this paper we experiment with
two such tasks: English stressed syllable tagging,
where the stress of a syllable often depends on the
number of remaining syllables,13 providing good
reason to use the lookahead provided by particle
smoothing; and Chinese NER, which is a familiar
textbook application and reminds the reader that our
formal setup (tagging) provides enough machinery
to treat other tasks (chunking).

English stressed syllable tagging This task tags
a sequence of phonemes x, which form a word,
with their stress markings y. Our training examples
are the stressed words in the CMU pronunciation
dictionary (Weide, 1998). We test the sampler on
held-out unstressed words.

Chinese social media NER This task does
named entity recognition in Chinese, by tagging
the characters of a Chinese sentence in a way that
marks the named entities. We use the dataset from
Peng and Dredze (2015), whose tagging scheme is
a variant of the BIO scheme mentioned in §1. We
test the sampler on held-out sentences.

6.2 String source separation

This is an artificial task that provides a discrete ana-
logue of speech source separation (Zibulevsky and
Pearlmutter, 2001). The generative model is that J
strings (possibly of different lengths) are generated

13English, like many other languages, assigns stress from
right to left (Hayes, 1995).

935



IID from an RNN language model, and are then
combined into a single string x according to a ran-
dom interleaving string y.14 The posterior p(y | x)
predicts the interleaving string, which suffices to re-
construct the original strings. The interleaving string
is selected from the uniform distribution over all pos-
sible interleavings (given the J strings’ lengths). For
example, with J = 2, a possible generative story is
that we first sample two strings Foo and Bar from an
RNN language model. We then draw an interleav-
ing string 112122 from the aforementioned uniform
distribution, and interleave the J strings determinis-
tically to get FoBoar.
p(x,y) is proportional to the product of the prob-

abilities of the J strings. The only parameters of
pθ, then, are the parameters of the RNN language
model, which we train on clean (non-interleaved)
samples from a corpus. We test the sampler on ran-
dom interleavings of held-out samples.

The state s (which is provided as an input to cθ
in (25)) is the concatenation of the J states of the
language model as it independently generates the J
strings, and gθ(st−1, xt, yt) is the log-probability of
generating xt as the next character of the ytth string,
given that string’s language model state within st−1.
As a special case, xT = EOS (see footnote 1), and
gθ(sT−1, EOS, EOS) is the total log-probability of
termination in all J language model states.

String source separation has good reason for
lookahead: appending character “o” to a recon-
structed string “ gh” is only advisable if “s” and
“t” are coming up soon to make “ghost.” It also il-
lustrates a powerful application setting—posterior
inference under a generative model. This task conve-
niently allowed us to construct the generative model
from a pre-trained language model. Our constructed
generative model illustrates that the state s and tran-
sition function f can reflect interesting problem-
specific structure.

CMU Pronunciation dictionary The CMU pro-
nunciation dictionary (already used above) provides
sequences of phonemes. Here we use words no
longer than 5 phonemes. We interleave the (un-
stressed) phonemes of J = 5 words.

Penn Treebank The PTB corpus (Marcus et al.,
1993) provides English sentences, from which we
use only the sentences of length ≤ 8. We interleave
the words of J = 2 sentences.

14We formally describe the generative process in Ap-
pendix G.

7 Experiments

In our experiments, we are given a pre-trained scor-
ing model pθ, and we train the parameters φ of a
particle smoothing algorithm.15

We now show that our proposed neural particle
smoothing sampler does better than the particle filter-
ing sampler. To define “better,” we evaluate samplers
on the offset KL divergence from the true posterior.

7.1 Evaluation metrics
Given x, the “natural” goal of conditional sampling
is for the sample distribution p̂(y) to approximate
the true distribution pθ(y | x) = expGT / expH0
from (1). We will therefore report—averaged over
all held-out test examples x—the KL divergence

KL(p̂||p) = Ey∼p̂ [log p̂(y)] (34)
− (Ey∼p̂ [log p̃(y | x)]− logZ(x)),

where p̃(y | x) denotes the unnormalized distribu-
tion given by expGT in (2), and Z(x) denotes its
normalizing constant, expH0 =

∑
y p̃(y | x).

As we are unable to compute logZ(x) in practice,
we replace it with an estimate z(x) to obtain an
offset KL divergence. This change of constant does
not change the measured difference between two
samplers, KL(p̂1||p)−KL(p̂2||p). Nonetheless, we
try to use a reasonable estimate so that the reported
KL divergence is interpretable in an absolute sense.
Specifically, we take z(x) = log

∑
y∈Y p̃(y | x) ≤

logZ, where Y is the full set of distinct particles
y that we ever drew for input x, including samples
from the beam search models, while constructing
the experimental results graph.16 Thus, the offset
KL divergence is a “best effort” lower bound on the
true exclusive KL divergence KL(p̂||p).

7.2 Results
In all experiments we compute the offset KL diver-
gence for both the particle filtering samplers and the
particle smoothing samplers, for varying ensemble
sizes M . We also compare against a beam search
baseline that keeps the highest-scoring M particles
at each step (scored by expGt with no lookahead).
The results are in Figures 2a–2d.

15For the details of the training procedures and the specific
neural architectures in our models, see Appendices C and D.

16Thus, Y was collected across all samplings, iterations,and
ensemble sizes M , in an attempt to make the summation over
Y as complete as possible. For good measure, we added some
extra particles: whenever we drew M particles via particle
smoothing, we drew an additional 2M particles by particle
filtering and added them to Y .

936



101 102
0.25

0.50

0.75

1.00

1.25

1.50

1.75

2.00

2.25 PF
PF:R
BEAM
PS
PS:R

(a) tagging: stressed syllables
101 102

1.4

1.6

1.8

2.0

2.2

2.4

2.6

2.8 PF
PF:R
BEAM
PS
PS:R

(b) tagging: Chinese NER
101 102

2

3

4

5

6

7

PF
PF:R
BEAM
PS
PS:R

(c) source separation: PTB
101 102

8

9

10

11

12
PF
PF:R
PS
PS:R

(d) source separation: CMUdict

Figure 2: Offset KL divergences for the tasks in §§ 6.1 and 6.2. The logarithmic x-axis is the size of particles M (8 ≤M ≤ 128).
The y-axis is the offset KL divergence described in §7.1 (in bits per sequence). The smoothing samplers offer considerable speedup:
for example, in Figure 2a, the non-resampled smoothing sampler achieves comparable offset KL divergences with only 1/4 as many
particles as its filtering counterparts. Abbreviations in the legend: PF=particle filtering. PS=particle smoothing. BEAM=beam
search. ‘:R’ suffixes indicate resampled variants. For readability, beam search results are omitted from Figure 2d, but appear in
Figure 3 of the appendices.

Given a fixed ensemble size, we see the smooth-
ing sampler consistently performs better than the
filtering counterpart. It often achieves comparable
performance at a fraction of the ensemble size.

Beam search on the other hand falls behind on
three tasks: stress prediction and the two source
separation tasks. It does perform better than the
stochastic methods on the Chinese NER task, but
only at small beam sizes. Varying the beam size
barely affects performance at all, across all tasks.
This suggests that beam search is unable to explore
the hypothesis space well.

We experiment with resampling for both the parti-
cle filtering sampler and our smoothing sampler. In
source separation and stressed syllable prediction,
where the right context contains critical information
about how viable a particle is, resampling helps par-
ticle filtering almost catch up to particle smoothing.
Particle smoothing itself is not further improved by
resampling, presumably because its effective sam-
ple size is high. The goal of resampling is to kill
off low-weight particles (which were overproposed)
and reallocate their resources to higher-weight ones.
But with particle smoothing, there are fewer low-
weight particles, so the benefit of resampling may be
outweighted by its cost (namely, increased variance).

8 Related Work

Much previous work has employed sequential im-
portance sampling for approximate inference of in-
tractable distributions (e.g., Thrun, 2000; Andrews
et al., 2017). Some of this work learns adaptive
proposal distributions in this setting (e.g. Gu et al.,
2015; Paige and Wood, 2016). The key difference
in our work is that we consider future inputs, which
is impossible in online decision settings such as
robotics. Klaas et al. (2006) did do particle smooth-
ing, like us, but they did not learn adaptive proposal
distributions.

Just as we use a right-to-left RNN to guide pos-
terior sampling of a left-to-right generative model,
Krishnan et al. (2017) employed a right-to-left RNN
to guide posterior marginal inference in the same
sort of model. Serdyuk et al. (2018) used a right-to-
left RNN to regularize training of such a model.

9 Conclusion

We have described neural particle smoothing, a se-
quential Monte Carlo method for approximate sam-
pling from the posterior of incremental neural scor-
ing models. Sequential importance sampling has
arguably been underused in the natural language pro-
cessing community. It is quite a plausible strategy
for dealing with rich, globally normalized probabil-
ity models such as neural models—particularly if a
good sequential proposal distribution can be found.
Our contribution is a neural proposal distribution,
which goes beyond particle filtering in that it uses a
right-to-left recurrent neural network to “look ahead”
to future symbols of x when proposing each symbol
yt. The form of our distribution is well-motivated.

There are many possible extensions to the work in
this paper. For example, we can learn the generative
model and proposal distribution jointly; we can also
infuse them with hand-crafted structure, or use more
deeply stacked architectures; and we can try training
the proposal distribution end-to-end (footnote 10).
Another possible extension would be to allow each
step of q to propose a sequence of actions, effectively
making the tagset size∞. This extension relaxes our
|y| = |x| restriction from §1 and would allow us to
do general sequence-to-sequence transduction.

Acknowledgements

This work has been generously supported by a
Google Faculty Research Award and by Grant No.
1718846 from the National Science Foundation.

937



References
Roee Aharoni and Yoav Goldberg. 2017. Morphological

inflection generation with hard monotonic attention.
In ACL.

Daniel Andor, Chris Alberti, David Weiss, Aliaksei
Severyn, Alessandro Presta, Kuzman Ganchev, Slav
Petrov, and Michael Collins. 2016. Globally normal-
ized transition-based neural networks. In ACL.

Nicholas Andrews, Mark Dredze, Benjamin Van Durme,
and Jason Eisner. 2017. Bayesian modeling of lexical
resources for low-resource settings. In ACL.

Dzmitry Bahdanau, Philemon Brakel, Kelvin Xu,
Anirudh Goyal, Ryan Lowe, Joelle Pineau, Aaron
Courville, and Yoshua Bengio. 2017. An actor-critic
algorithm for sequence prediction. In ICLR.

Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio.
2015. Neural machine translation by jointly learning
to align and translate. In ICLR.

Yoshua Bengio and Paolo Frasconi. 1996. Input-output
HMMs for sequence processing. IEEE Transactions
on Neural Networks, 7(5):1231–1249.

Bernhard E. Boser, Isabelle M. Guyon, and Vladimir N.
Vapnik. 1992. A training algorithm for optimal margin
classifiers. In COLT.

Alexandre Bouchard-Côté, Percy Liang, Thomas Grif-
fiths, and Dan Klein. 2007. A probabilistic approach
to diachronic phonology. In EMNLP-CoNLL, pages
887–896.

Kyunghyun Cho, Bart van Merrienboer, Çaglar Gülçehre,
Fethi Bougares, Holger Schwenk, and Yoshua Bengio.
2014. Learning phrase representations using RNN
encoder-decoder for statistical machine translation. In
EMNLP.

Ryan Cotterell, John Sylak-Glassman, and Christo Kirov.
2017. Neural graphical models over strings for prin-
cipal parts morphological paradigm completion. In
Proceedings of the 15th Conference of the European
Chapter of the Association for Computational Linguis-
tics: Volume 2, Short Papers, pages 759–765.

Randal Douc and Olivier Cappé. 2005. Comparison of
resampling schemes for particle filtering. In Image
and Signal Processing and Analysis, 2005. ISPA 2005.
Proceedings of the 4th International Symposium on,
pages 64–69. IEEE.

Arnaud Doucet and Adam M. Johansen. 2009. A tutorial
on particle filtering and smoothing: Fifteen years later.
Handbook of Nonlinear Filtering, 12(656-704):3.

Markus Dreyer and Jason Eisner. 2009. Graphical mod-
els over multiple strings. In EMNLP.

Markus Dreyer, Jason R. Smith, and Jason Eisner. 2008.
Latent-variable modeling of string transductions with
finite-state methods. In EMNLP.

Chris Dyer, Miguel Ballesteros, Wang Ling, Austin
Matthews, and Noah A. Smith. 2015. Transition-
based dependency parsing with stack long short-term
memory. In ACL.

Chris Dyer, Adhiguna Kuncoro, Miguel Ballesteros, and
Noah A. Smith. 2016. Recurrent neural network gram-
mars. In HLT-NAACL.

Jenny Rose Finkel, Christopher D. Manning, and An-
drew Y. Ng. 2006. Solving the problem of cascading
errors: Approximate Bayesian inference for linguistic
annotation pipelines. In EMNLP.

Peter W. Glynn. 1990. Likelihood ratio gradient estima-
tion for stochastic systems. Communications of the
ACM, 33(10):75–84.

Shixiang Gu, Zoubin Ghahramani, and Richard E. Turner.
2015. Neural adaptive sequential Monte Carlo. In
NIPS.

Peter E. Hart, Nils J. Nilsson, and Bertram Raphael. 1968.
A formal basis for the heuristic determination of mini-
mal cost paths. 4(2):100–107.

Bruce Hayes. 1995. Metrical Stress Theory: Principles
and Case Studies. University of Chicago Press.

Alexander T. Ihler and David A. McAllester. 2009. Parti-
cle belief propagation. In AISTATS.

Eric Jang, Shixiang Gu, and Ben Poole. 2017. Cate-
gorical reparameterization with Gumbel-softmax. In
ICLR.

Katharina Kann and Hinrich Schütze. 2016. Single-
model encoder-decoder with explicit morphological
representation for reinflection. In ACL.

Diederik P. Kingma and Jimmy Ba. 2015. Adam: A
method for stochastic optimization. In ICLR.

Mike Klaas, Mark Briers, Nando de Freitas, Arnaud
Doucet, Simon Maskell, and Dustin Lang. 2006. Fast
particle smoothing: If I had a million particles. In
ICML.

Bjarne Knudsen and Michael M. Miyamoto. 2003. Se-
quence alignments and pair hidden Markov models
using evolutionary history. Journal of Molecular Bio-
logy, 333(2):453 – 460.

Rahul G. Krishnan, Uri Shalit, and David Sontag. 2017.
Structured inference networks for nonlinear state space
models. In AAAI.

John Lafferty, Andrew McCallum, and Fernando C. N.
Pereira. 2001. Conditional random fields: Probabilis-
tic models for segmenting and labeling sequence data.
In ICML.

Thibaut Lienart, Yee Whye Teh, and Arnaud Doucet.
2015. Expectation particle belief propagation. In
NIPS.

938



Roderick J. A. Little and Donald B. Rubin. 1987. Sta-
tistical Analysis with Missing Data. J. Wiley & Sons,
New York.

Chris J. Maddison, Andriy Mnih, and Yee Whye Teh.
2017. The concrete distribution: A continuous relax-
ation of discrete random variables. In ICLR.

Mitchell P. Marcus, Mary Ann Marcinkiewicz, and Beat-
rice Santorini. 1993. Building a large annotated cor-
pus of English: The Penn treebank. Computational
Linguistics, 19(2):313–330.

Andrew McCallum, Dayne Freitag, and Fernando Pereira.
2000. Maximum entropy Markov models for informa-
tion extraction and segmentation. In Machine Learn-
ing: Proceedings of the 17th International Conference
(ICML 2000), pages 591–598, Stanford, CA.

Tomas Mikolov, Martin Karafiát, Lukas Burget, Jan Cer-
nockỳ, and Sanjeev Khudanpur. 2010. Recurrent neu-
ral network based language model. In Interspeech,
volume 2, page 3.

Brooks Paige and Frank D. Wood. 2016. Inference net-
works for sequential Monte Carlo in graphical models.
In ICML.

Brooks Paige, Frank D. Wood, Arnaud Doucet, and
Yee Whye Teh. 2014. Asynchronous anytime sequen-
tial Monte Carlo. In NIPS.

Nanyun Peng and Mark Dredze. 2015. Named en-
tity recognition for Chinese social media with jointly
trained embeddings. In EMNLP.

Jeffrey Pennington, Richard Socher, and Christopher D.
Manning. 2014. GloVe: Global vectors for word rep-
resentation. In EMNLP.

Fernando C. N. Pereira and Michael D. Riley. 1997.
Speech recognition by composition of weighted finite
automata. Finite-State Language Processing, page
431.

Lawrence R. Rabiner. 1989. A tutorial on hidden Markov
models and selected applications in speech recognition.
Proceedings of IEEE, 77(2):257–285.

Lance A. Ramshaw and Mitchell P. Marcus. 1999. Text
chunking using transformation-based learning. In Na-
tural Language Processing Using Very Large Corpora,
pages 157–176. Springer.

Branko Ristic, Sanjeev Arulampalam, and Neil James
Gordon. 2004. Beyond the Kalman Filter: Particle
Filters for Tracking Applications. Artech House.

Herbert Robbins and Sutton Monro. 1951. A stochastic
approximation method. The Annals of Mathematical
Statistics, pages 400–407.

Dmitriy Serdyuk, Nan Rosemary Ke, Alessandro Sordoni,
Adam Trischler, Chris Pal, and Yoshua Bengio. 2018.
Twin networks: Matching the future for sequence ge-
neration. In ICLR.

Andreas Stuhlmüller, Jacob Taylor, and Noah Goodman.
2013. Learning stochastic inverses. In NIPS.

Sebastian Thrun. 2000. Monte Carlo POMDPs. In NIPS.

Andrew J. Viterbi. 1967. Error bounds for convolutional
codes and an asymptotically optimum decoding al-
gorithm. IEEE Transactions on Information Theory,
IT-13(2):260–269.

Greg C. G. Wei and Martin A. Tanner. 1990. A Monte
Carlo implementation of the EM algorithm and the
poor man’s data augmentation algorithms. Journal
of the American Statistical Association, 85(411):699–
704.

Robert L. Weide. 1998. The CMU pronunciation dictio-
nary, release 0.6.

Ronald J. Williams. 1992. Simple statistical gradient-
following algorithms for connectionist reinforcement
learning. Machine Learning, 8(23).

Sam Wiseman and Alexander M. Rush. 2016. Sequence-
to-sequence learning as beam-search optimization. In
EMNLP.

Hiroyasu Yamada and Yuji Matsumoto. 2003. Statistical
dependency analysis with support vector machines. In
Proceedings of IWPT, volume 3, pages 195–206.

Michael Zibulevsky and Barak A. Pearlmutter. 2001.
Blind source separation by sparse decomposition in
a signal dictionary. Neural Computation, 13(4):863–
882.

939



A The logprob-to-go for HMMs

As noted in §2.1, the logprob-to-go Ht can be com-
puted by the backward algorithm. By the definition
of Ht in equation (10),

expHt =
∑

yt:

exp (GT −Gt) (35)

=
∑

yt:

exp
T∑

j=t+1

gθ(sj−1, xj , yj) (36)

=
∑

yt:

T∏

j=t+1

pθ(yj | yj−1) · pθ(xj | yj)

= (βt)yt (backward prob of yt at time t)

where the vectorβt is defined by base case (βT )y =
1 and for 0 ≤ t < T by the recurrence

(βt)y
def
=
∑

yt:

pθ(xt:,yt: | yt = y) (37)

=
∑

y′
pθ(y

′ | y) · pθ(xt+1 | y′) · (βt+1)y′

The backward algorithm (20) for OOHMMs in
§2.2 is a variant of this.

B Gradients for Training the Proposal
Distribution

For a given x, both forms of KL divergence achieve
their minimum of 0 when (∀y) qφ(y | x) = p(y |
x). However, we are unlikely to be able to find such
a φ; the two metrics penalize qφ differently for mis-
matches. We simplify the notation below by writing
qφ(y) and p(y), suppressing the conditioning on x.

Inclusive KL Divergence The inclusive KL di-
vergence has that name because it is finite only when
support(qφ) ⊇ support(p), i.e., when qφ is capable
of proposing any string y that has positive proba-
bility under p. This is required for qφ to be a valid
proposal distribution for importance sampling.

KL (p||qφ) (38)
= Ey∼p [log p (y)− log qφ(y)]
= Ey∼p [log p (y)]
− Ey∼p [log qφ (y)]

The first term Ey∼p [log p (y)] is a constant with
regard to φ. As a result, the gradient of the above is
just the gradient of the second term:

∇φKL(p||qφ) = ∇φ Ey∼p [− log qφ (y)]︸ ︷︷ ︸
the cross-entropy H(p,qφ)

We cannot directly sample from p. However, our
weighted mixture p̂ from equation (28) (obtained by
sequential importance sampling) could be a good
approximation:

∇φKL(p||qφ) ≈ ∇φEy∼p̂ [− log qφ (y)] (39)

=
T∑

t=1

Ep̂ [−∇φ log qφ(yt | y:t−1,x)]

Following this approximate gradient downhill has an
intuitive interpretation: if a particular yt value ends
up with high relative weight in the final ensemble p̂,
then we will try to adjust qφ so that it would have
had a high probability of proposing that yt value at
step t in the first place.

Exclusive KL Divergence The exclusive diver-
gence has that name because it is finite only when
support(qφ) ⊆ support(p). It is defined by

KL(qφ||p) = Ey∼qφ [log qφ(y)− log p(y)] (40)
= Ey∼qφ [log qφ(y)− log p̃(y)] + logZ
=
∑

y

qφ(y) [log qφ(y)− log p̃(y)]︸ ︷︷ ︸
call this dφ(y)

+ logZ

where p(y) = 1Z p̃(y) for p̃(y) = expGT and Z =∑
y p̃(y). With some rearrangement, we can write

its gradient as an expectation that can be estimated
by sampling from qφ.17 Observing thatZ is constant
with respect to φ, first write

∇φKL(qφ||p) (41)
=
∑

y

∇φ (qφ(y) dφ(y)) (42)

=
∑

y

(∇φqφ(y)) dφ(y)

+
∑

y

qφ(y)∇φ log qφ(y)︸ ︷︷ ︸
=∇φqφ(y)

=
∑

y

(∇φqφ(y)) dφ(y)

where the last step uses the fact that∑
y∇φqφ(y) = ∇φ

∑
y qφ(y) = ∇φ1 = 0.

We can turn this into an expectation with a
second use of Glynn (1990)’s observation that

17This is an extension of the REINFORCE trick (Williams,
1992), which estimates the gradient of Ey∼qφ [reward(y)]
when the reward is independent of φ. In our case, the expecta-
tion is over a quantity that does depend on φ.

940



∇φqφ(y) = qφ(y)∇φ log qφ(y) (the “likelihood
ratio trick”):

∇φKL(qφ||p)
=
∑

y

qφ(y)dφ(y)∇φ log qφ(y)

= Ey∼qφ [dφ(y)∇φ log qφ(y)] (43)
which can, if desired, be further rewritten as

= Ey∼qφ [dφ(y)∇φ dφ(y)]
= Ey∼qφ

[
∇φ
(
1
2dφ(y)

2
)]

(44)

If we regard dφ(y) as a signed error (in the log do-
main) in trying to fit qφ to p̃, then the above gradient
of KL can be interpreted as the gradient of the mean
squared error (divided by 2).18

We would get the same gradient for any rescaled
version of the unnormalized distribution p̃, but the
formula for obtaining that gradient would be dif-
ferent. In particular, if we rewrite the above deriva-
tion but add a constant b to both log p̃(y) and logZ
throughout (equivalent to adding b to GT ), we will
get the slightly generalized expectation formulas

Ey∼qφ [(dφ(y)− b)∇φ log qφ(y)] (45)
Ey∼qφ

[
∇φ
(
1
2 (dφ(y)− b)

2
)]

(46)

in place of equations (43) and (44). By choosing an
appropriate “baseline” b, we can reduce the variance
of the sampling-based estimate of these expectations.
This is similar to the use of a baseline in the REIN-
FORCE algorithm (Williams, 1992). In this work
we choose b using an exponential moving average
of past E [dφ(y)] values: at the end of each training
minibatch, we update b← 0.1 · b+ 0.9 · d̄, where
d̄ is the mean of the estimated Ey∼qφ(·|x) [dφ(y)]
values for all examples x in the minibatch.

C Implementation Details

We implement all RNNs in this paper as GRU net-
works (Cho et al., 2014) with d = 32 hidden units
(state space R32). Each of our models (§6) always
specifies the logprob-so-far in equations (2) and (3)
using a 1-layer left-to-right GRU,19 while the corre-
sponding proposal distribution (§3.3) always spec-
ifies the state st in (6) using a 2-layer right-to-left

18We thank Hongyuan Mei, Tim Vieira, and Sanjeev Khu-
danpur for insightful discussions on this derivation.

19For the tagging task described in §6.1, gθ(st−1, xt, yt)
def
=

log pθ(xt, yt | st−1), where the GRU state st−1 is used to
define a softmax distribution over possible (xt, yt) pairs in the
same manner as an RNN language model (Mikolov et al., 2010).
Likewise, for the source separation task (§6.2), the source lan-
guage models described in Appendix G are GRU-based RNN
language models.

101 102

10

15

20

25

30

PF
PF:R
BEAM
PS
PS:R

Figure 3: Offset KL divergence for the source separation task
on phoneme sequences.

GRU, and specifies the compatibility function Ct in
(23) using a 4-layer feedforward ReLU network.20

For the Chinese social media NER task (§6.1), we
use the Chinese character embeddings provided by
Peng and Dredze (2015), while for the source separa-
tion tasks (§6.2), we use the 50-dimensional GloVe
word embeddings (Pennington et al., 2014). In other
cases, we train embeddings along with the rest of
the network. We optimize with the Adam optimizer
using the default parameters (Kingma and Ba, 2015)
and L2 regularization coefficient of 10−5.

D Training Procedures

In all our experiments, we train the incremental scor-
ing models (the tagging and source separation mod-
els described in §6.1 and §6.2, respectively) on the
training dataset T . We do early stopping, using per-
plexity on a held-out development set D1 to choose
the number of epochs to train (maximum of 3).

Having obtained these model parameters θ, we
train our proposal distributions qθ,φ on T , keeping
θ fixed and only tuning φ. Again we use early stop-
ping, using the KL divergence from §7.1 on a sep-
arate development set D2 to choose the number of
epochs to train (maximum of 20 for the two tag-
ging tasks and source separation on the PTB dataset,
and maximum of 50 for source separation on the
phoneme sequence dataset). We then evaluate qθ∗,φ∗
on the test dataset E.

[Appendices E–G appear in the supplementary
material file.]

20As input to Ct, we actually provide not only st, s̄t but also
the states fθ(st−1, xt, y) (including st) that could have been
reached for each possible value y of yt. We have to compute
these anyway while constructing the proposal distribution, and
we find that it helps performance to include them.

941


