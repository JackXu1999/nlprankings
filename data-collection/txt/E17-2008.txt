



















































Crowd-Sourced Iterative Annotation for Narrative Summarization Corpora


Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 2, Short Papers, pages 46–51,
Valencia, Spain, April 3-7, 2017. c©2017 Association for Computational Linguistics

Crowd-Sourced Iterative Annotation for Narrative Summarization
Corpora

Jessica Ouyang and Serina Chang and Kathleen McKeown
Department of Computer Science, Columbia University, New York, NY 10027

{ouyangj, kathy}@cs.columbia.edu
sc3003@columbia.edu

Abstract

We present an iterative annotation pro-
cess for producing aligned, parallel cor-
pora of abstractive and extractive sum-
maries for narrative. Our approach
uses a combination of trained annotators
and crowd-sourcing, allowing us to elicit
human-generated summaries and align-
ments quickly and at low cost. We
use crowd-sourcing to annotate aligned
phrases with the text-to-text generation
techniques needed to transform each
phrase into the other. We apply this pro-
cess to a corpus of 476 personal narratives,
which we make available on the Web.

1 Introduction

With the tremendous amounts of text published on
the Web every day, automatic text summarization
is more relevant than ever. Web content must com-
pete for readers’ attention, and the existence of
click bait links shows that content providers are
very aware that a short, appealing summary may
be their only chance to attract readers. For the
readers’ part, summaries stating exactly what a
piece of content is about protects them from wast-
ing time on topics that do not interest them.

Research on summarization has long focused
on extraction: selecting the most salient sentences
from a text without any modifications. These sum-
maries can be incoherent or incomprehensible due
to unresolved pronouns and references, and sen-
tences containing irrelevant information (Nenkova
and McKeown, 2011), and this is particularly
problematic with informal web text. Thus abstrac-
tive summarization is critical for the web, with
rewriting of extracted sentences, as humans write
summaries (Jing and McKeown, 1999).

To develop an abstractive summarization sys-
tem, we need data: parallel corpora that align
extractive summaries with abstractive summaries.
Such corpora would allow researchers to develop
text-to-text generation approaches to produce ab-
stractive summaries from extractive ones. While
there are many summarization copora available,
most provide abstractive summaries only (Meyer
et al., 2016), extractive summaries only or un-
aligned abstractive and extractive summaries (e.g.,
as in (Over et al., 2007; Dang and Owczarzak,
2008)).

In this work, we present an iterative annotation
process for producing aligned summaries anno-
tated with text-to-text generation techniques. Fig-
ure 1 shows a human-written abstractive summary
and human-selected extractive summary from our
corpus. The extractive summary suggests the nar-
rator was already uneasy and leaves the reader
wondering why. This information is unimpor-
tant, but the extractive summary must include it
because it is in the same sentence as the bloody
woman, just as it must include an extra character:
the man in medical attire. Text-to-text generation
techniques, such as sentence compression, could
be used to rewrite this extractive summary to more
closely match the abstractive summary.

Abstractive: While driving home I saw a woman cov-
ered in blood standing by the side of the road. As I passed

she attempted to launch herself at my car.

Extractive: As I’m looking around as to what the fuck is
going on, we approach the roundabout and there is a man

in medical attire next to a woman in white pyjamas, with

blood covering her clothing. I go straight, and as we go

past the woman attempts to launch herself at my car.

Figure 1: Abstractive and extractive summaries.

While the extractive summary contains some
extraneous information, it does include every-

46



thing present in the abstractive summary. We use
crowd-sourcing with Amazon Mechanical Turk
(AMT) to produce our extractive summaries, and
workers are given the abstractive summaries as
a prompt, ensuring high-quality extractive sum-
maries despite using inexpensive crowd-sourcing.
In the next stage of our annotation process, we
use AMT workers (Turkers) to align phrases from
the extractive summaries to the abstractive sum-
maries. Finally, we use Turkers to annotate the
aligned phrases with the five rewriting operations
identified by Jing and McKeown (1999) – re-
duction (compression), combination (fusion), syn-
tactic transformation, lexical paraphrasing, gen-
eralization/specification – indicating how best to
rewrite each extracted phrase. We make our cor-
pus available on the Web1.

2 Related Work

Text-to-text generation for abstractive summa-
rization is the task of revising extracted sen-
tences using techniques such as sentence compres-
sion (Knight and Marcu, 2000; Lin, 2003; Zajic et
al., 2007; Liu and Liu, 2009) and fusion (Barzi-
lay and McKeown, 2005). Unfortunately, cor-
pora for text-to-text generation are rare and time-
consuming to produce. Marcu (1999) created a
corpus of nearly 7,000 abstractive and extractive
summaries of news articles by automatically ex-
tracting sentences based on a human-written sum-
mary, building a large corpus at the cost of some
noise. Murray et al (2005) created a corpus of 61
paired, human-written abstractive/extractive sum-
maries of meeting transcripts, but the gain in sum-
mary quality achieved using human annotators is
offset by the small size of the corpus.

This work uses personal narratives, widely
found on social networks, weblogs, and online
forums. The availability of online narrative be-
gins to address a problem facing the text-to-text
generation approach to summarization: lack of
data. Gordon and Swanson (2009) trained a clas-
sifier to identify narratives in blog posts with 75%
precision and built a corpus of 937,994 narra-
tives. Ouyang and McKeown (2015) created a
corpus of 4,647 narratives collected automatically
from Reddit, achieving 94% precision in collect-
ing only narrative text. They argue that the Most
Reportable Event (MRE) is the most salient event

1www.cs.columbia.edu/∼ouyangj/aligned-
summarization-data

and thus the shortest possible summary; they an-
notated a subset of 476 narratives by extracting
sentences that referred to MREs.

The Murray et al corpus includes alignments
between phrases in the extractive summaries and
sentences in the abstractive summaries. How-
ever, none of the corpora described above pro-
vides an analysis of how a summarizer might
transform an extracted phrase into its abstractive
form. While corpora exist for some rewrites in
McKeown and Jing (1999), such as compression
(Ziff-Davis, Filippova and Altun (2013), Kaji-
wara and Komachi (2016)), fusion (McKeown et
al (2010)), and lexical paraphrasing/syntactic re-
ordering (Ganitkevitch et al (2013)), these corpora
exist in isolation. A human summarizer may apply
multiple rewrites to a single phrase, and our work
captures this information with annotations for all
of the rewrites over each alignment.

3 Data Collection

We use the annotated subset of 476 personal nar-
ratives in Ouyang and McKeown (2015), although
we do not use their annotations.

3.1 Stage One: Abstractive Summaries

We partitioned the 476 stories into 7 slices of 68
narratives. The narratives were written for 19 dif-
ferent prompts, which roughly correspond to top-
ics (eg. “Your best ‘Accidentally Racist’ story?”).
We randomly assigned an equal number of narra-
tives from each prompt to each of the seven slices.

We trained four graduate student annotators
from our university’s Department of English and
Comparative Literature. Each was assigned four
slices: one in common with each other annotator,
and one among all annotators. Each participated
in a 30-minute training session: they were told to
imagine they were about to tell a story to a friend
and wanted to ask, “Did I tell you about. . . ?” They
should write one or two sentences to complete the
question and include any context they thought nec-
essary for their friend to understand it.

We evaluated interannotator agreement on this
task using an AMT HIT (Human Intelligence
Task) where Turkers were shown summaries writ-
ten by two different annotators, but not the narra-
tive itself. We then asked the Turkers to decide
whether or not the summaries described the same
event, and if so, whether one or both of the sum-
maries contained important information not found

47



in the other. We required Turkers to complete a
qualification test before working on the HIT, en-
suring they had read and understood the task in-
structions. The test consisted of pairs of example
summaries constructed so that the correct answers
to our two questions were clear: the paired sum-
maries were identical except for pieces of extra in-
formation that we inserted into one or both sum-
maries. Three Turkers worked on each hit, and we
considered a pair of summaries to be in agreement
if at least two out of three Turkers indicated that
the summaries described the same event.

Abstract A: My neighbor’s mom saved me from being
kidnapped into a car when I was six.

Abstract B: Someone tried to kidnap me when I was six,
but a neighbor’s mom grabbed me before they got me.

(a) Agreeing abstractive summaries.

Abstract A: I ran my mouth off at this rude woman.
Abstract B: I held a door for a lady and she told someone
on the phone that I had rudely ran around her.

(b) Disagreeing abstractive summaries.

Figure 2: Examples of agreeing and disagreeing abstractive
summaries for two different narratives.

Our annotators achieved 90.38% observed
agreement, producing a total of 1088 different ab-
stractive summaries. Figure 2 shows a pair of
agreeing and a pair of disagreeing abstractive sum-
maries. With the disagreeing summaries, we see
that annotators A and B focused on different as-
pects of the narrative: A summarized the narra-
tor’s confrontation with the rude woman, while
B explains why the narrator was angry with the
woman. Figure 3 shows a pair of agreeing sum-
maries where Turkers indicated that both sum-
maries contained important information not found
in the other summary. We see that annotator A
focused on the event’s emotional effect on the nar-
rator, while annotator C emphasized the irrespon-
sible friend’s bad behavior.

Abstract A: This one friend never gave back the 360 and
netbook I let him borrow, so now I have a hard time doing

good deeds for other people.

Abstract C: I lent my friend my netbook and xbox 360
and he broke the netbook and claimed the 360 was stolen.

He only ever gave me 100 bucks for it.

Figure 3: Extra information in a agreeing summaries.

3.2 Stage Two: Extractive Summaries

To produce the corresponding extractive sum-
maries, we created another HIT that showed Turk-
ers a narrative, one of its abstractive summaries,
and instructions to compose an equivalent sum-
mary by selecting as few sentences as possible
from the narrative. We once again required Turk-
ers to complete a qualification test before working
on our HITs. The test consisted of a single story
and abstractive summary, written so that the sum-
mary was a word-for-word paraphrase of a single
sentence in the narrative that did not overlap with
any other sentences. We also required that Turk-
ers be at least 18 years old and have completed at
least 10,000 HITs with 98% acceptance on pre-
vious HITs. Three Turkers worked on each of
our HITs, and Turkers achieved substantial agree-
ment on which sentences they selected: Fleiss’s κ
of 0.748. Figure 4 shows an extractive summary
where they achieved perfect agreement.

Abstractive: At a concert, I grabbed a chunk of dirt in
mid-air that was being thrown at a woman, and security

thought I was throwing the dirt.

Extractive: There is a woman standing next to me when
a huge piece of dirt comes flying straight at her face. I

grab the chunk inches from her face mid-air. Security

sees me with a chunk of dirt in my hand and instantly

grab and pull me out of the crowd.

Figure 4: Perfect agreement among Turkers in constructing
the extractive summary.

Combining our abstractive and extractive sum-
maries, we have 476 narratives, 408 with two ab-
stractive summaries and 68 with four. For each
abstractive summary, we have six extractive sum-
maries, one for each Turker and an additional three
created by aggregating the Turkers’ summaries:
sentences selected by at least one (union), two
(majority), and all three (intersect) Turkers.

3.3 Stage Three: Phrase Alignments

We used another AMT HIT to produce phrase
alignments between the extractive and abstractive
summaries. We showed Turkers one of the ab-
stractive summaries produced in Stage One and
its corresponding extractive summary produced in
Stage Two (using union aggregation). The task
was to align phrases between the summaries, and
to submit as many alignments as they could find.

To avoid confusing terminology, the instruc-
tions referred to the abstractive summary as the

48



Figure 5: Highlighting interface for Phrase Alignment HIT.

“summary” and the extractive summary as the “ex-
cerpt.” We defined aligning as “matching phrases
from the summary with phrases from the excerpt
that effectively mean the same things.” The HIT
interface (Figure 5), allowed Turkers to select
phrases by highlighting, save alignments as they
went along, and submit all their saved alignments
at the end. Three Turkers worked on each HIT.

As in the previous stages, we required Turkers
to complete a qualification test where we showed
Turkers one phrase from an abstractive summary
and four phrases from the corresponding extrac-
tive summary and asked them to decide which of
the four extractive options would make a good
alignment with the abstractive phrase. We also
presented them with a link to a demo of the in-
terface, so that they could try the highlighting and
saving functions before working on the actual HIT.

3.4 Stage Four: Rewriting Operations

Our final HIT asked Turkers to review the align-
ments produced in Stage Three, and to identify
the rewrite operation(s) involved in transforming
the extractive phrase into the abstractive phrase.
When performing the task, Turkers were only con-
cerned with one rewrite at a time, and simply
had to select whether the presented alignment em-
ployed that rewrite or not. We designed our task
in this way because an alignment could employ
more than one rewrite, and we wanted the Turk-
ers to consider each rewrite independently.

We defined the rewrite operations for the Turk-
ers as follows, and provided examples of each.
• Reduction keeps key parts word-for-word

and removes less important information.
• Lexical paraphrasing replaces words or

word sequences with paraphrases, ie. other
words that have the same meaning.
• Syntactic reordering changes the grammat-

ical structure (eg. passive vs active).

• Generalization replaces longer strings of de-
tail with shorter, more general descriptions.
• Specification replaces short, general descrip-

tions with longer strings of detail.
As in Stages Two and Three, we tested the

Turkers’ understanding of the task before allow-
ing them to work on the HITs. Since we ask
about one rewrite operation at a time, we designed
separate qualification tests for each rewrite. For
each test, we selected one abstractive/extractive
summary pair and constructed two different align-
ment examples where one alignment employed the
rewrite in question and the other did not. The
Turkers were asked whether or not the rewrite was
used in each of the two alignments.

Rewrite Operation Counts

Reduction 216 Generalization 3359
Lexical Para. 1218 Specification 1250
Syntactic Reor. 916

Table 1: Rewrite operation counts.

For each alignment, we put up four HITs (we
combined generalization and specification so that
Turkers could choose one or neither, but not both).
Table 1 lists each rewrite and how many align-
ments used it; we include an alignment when at
least 2 out of 3 Turkers agreed it used the rewrite.
We found that generalization was by far the most
popular rewrite operation, and reduction was the
least, likely because reduction’s definition was
the most demanding, as it required word-for-word
matching outside of the removed parts. Figure 6
shows an example each of generalization and its
counterpart specification from our annotations.

Generalization: Very rarely do I ever get a ”thanks” or a

smile of appreciation. → I never get any thanks.
Specification: I had the alien abduction dream. → I had
a sleep paralysis dream where I was abducted by aliens.

Figure 6: Examples of the two most common rewrite opera-
tions, generalization and specification.

49



Fusion Reduction Lexical Para. Syntactic Reor. Generalization Specification

Fusion 1052 36 214 151 695 165
Reduction 185 34 32 113 24
Lexical Para. 1068 179 564 237
Syntactic Reor. 772 391 165
Generalization 2802 0
Specification 1093

Table 2: Rewrite co-occurences produced from confident and precise alignments.

3.5 Discussion

We evaluated our Stage Three and Four data from
the Turkers by assigning confidence levels to the
alignments and judging annotator agreement on
the rewrite labels. It would be difficult to deter-
mine interannotator agreement in Stage Three be-
cause Turkers could submit any number of align-
ments of any size for each HIT. Instead, we evalu-
ated on the level of individual alignments. A con-
fident alignment had to agree with another align-
ment, where two alignments agreed if (1) differ-
ent Turkers submitted them; (2) the selected ab-
stractive phrases overlapped enough that at least
half of the shorter phrase was covered by the over-
lap; and (3) the selected extractive phrases over-
lapped enough that at least half of the shorter was
covered. A precise alignment does not contain
an extractive phrase that was over two sentences
long, because the longer the alignment, the more
difficult to identify the rewrite components in-
volved. Thus a confident alignment is one where at
least two different Turkers aligned the same spans,
within a margin of error of a few words, while a
precise alignment is one where it is easier to pin-
point the spans where rewrite operations apply.

Out of the 6173 alignments the Turkers pro-
duced, 5836 (95%) were confident, 5602 (91%)
were precise, and 5281 (86%) were both. When
we evaluated the rewrite labels produced for these
confident and precise alignments, we found that
many were labeled for multiple rewrite techniques
at once, indicating that quality phrase transforma-
tions often involved stitching together rewrites in-
stead of performing them separately. Figure 7
below displays an example of such an align-
ment, which was labeled for lexical paraphrasing
(3/3 Turker agreement), syntactic reordering (2/3
agreement), and generalization (2/3 agreement).

Table 2 further displays the interactions be-
tween rewrites in the form of a co-occurence ma-
trix of the five rewrites we tested on AMT, plus
fusion, which we identified automatically.

Extractive: My SO at the time had been de-
pressed/suicidal and I had been making posts in rele-
vant subs with a different account asking for advice. I
didn’t really have any experience with depression/suicide

at the time, so it was a very scary situation for me . . .

Abstractive: My friend identified some of my Reddit

posts about my suicidal SO at the time, and I was kind
of relieved that I ended up getting to confide in him about

the situation.

Figure 7: A confident and precise alignment (in bold) with
multiple rewrite labels: lexical paraphrasing, syntactic re-
ordering, and generalization. The extractive summary shown
is truncated due to length.

4 Conclusion

We have presented a new corpus of 1088 aligned
abstractive and extractive summaries, totaling
6173 phrase-level alignments, each annotated with
rewrite operations, which we make available on
the Web. Our iterative annotation process uses
trained annotators to generate abstractive sum-
maries and Amazon Mechanical Turk to pro-
duce extractive summaries, phrase alignments,
and rewrite annotations. We found substantial
agreement among annotators and Turkers for all
tasks, demonstrating our ability to elicit high-
quality summaries and alignments despite using
inexpensive crowd-sourcing.

Our corpus provides summaries of a very dif-
ferent type of text from the traditional newswire
articles: personal narratives, a genre that natural
language processing research is just beginning to
explore. This data is widely found on the Web and
brings challenges such as informal language and
extreme content. We hope that others will make
use of these aligned, personal narrative summaries
and their annotated rewrite operations, which we
make available on the Web. Our next step will be
to exploit this data to create an abstractive sum-
marization system using text-to-text generation.
We also hope that the success of our annotation
method, using both trained annotators and crowd-
sourcing, will encourage other researchers to cre-
ate similar corpora.

50



Acknowledgments

This paper is based upon work supported by the
National Science Foundation under Grant No. IIS-
1422863. Any opinions, findings and conclusions
or recommendations expressed in this material are
those of the author(s) and do not necessarily reflect
the views of the National Science Foundation.

References
Regina Barzilay and Kathleen McKeown. 2005. Sen-

tence fusion for multidocument news summariza-
tion. Computational Linguistics, 31(3):297–328.

Hoa Trang Dang and Karolina Owczarzak. 2008.
Overview of the tac 2008 update summarization
task. In Proceedings of Text Analysis Conference.

Katja Filippova and Yasemin Altun. 2013. Overcom-
ing the lack of parallel data in sentence compres-
sion. In Proceedings of the 2013 Conference on
Empirical Methods in Natural Language Process-
ing, pages 1481–1491, Seattle, Washington, USA,
October. Association for Computational Linguistics.

Juri Ganitkevitch, Benjamin Van Durme, and Chris
Callison-Burch. 2013. Ppdb: The paraphrase
database. In Proceedings of the 2013 Conference of
the North American Chapter of the Association for
Computational Linguistics: Human Language Tech-
nologies, pages 758–764, Atlanta, Georgia, June.
Association for Computational Linguistics.

Andrew Gordon and Reid Swanson. 2009. Identify-
ing personal stories in millions of weblog entries. In
Proceedings of the 3rd International Conference on
Weblogs and Social Media, Data Challenge Work-
shop. Association for the Advancement of Artificial
Intelligence.

Hongyan Jing and Kathleen McKeown. 1999. The de-
composition of human-written summary sentences.
In Proceedings of the 22nd ACM SIGIR Conference
on Research and Development in Information Re-
trieval, pages 129–136. Association for Computing
Machinery.

Tomoyuki Kajiwara and Mamoru Komachi. 2016.
Building a monolingual parallel corpus for text sim-
plification using sentence similarity based on align-
ment between word embeddings. In Proceedings
of COLING 2016, the 26th International Confer-
ence on Computational Linguistics: Technical Pa-
pers, pages 1147–1158, Osaka, Japan, December.
The COLING 2016 Organizing Committee.

Kevin Knight and Daniel Marcu. 2000. Statistics-
based summarization-step one: Sentence compres-
sion. In Proceedings of the Twelth Conference
on Innovative Applications of Artificial Intelligence,
pages 703–710. Association for the Advancement of
Artificial Intelligence.

Chin-Yew Lin. 2003. Improving summarization per-
formance by sentence compression — a pilot study.
In Proceedings of the Sixth International Workshop
on Information Retrieval with Asian Languages,
pages 1–8, Sapporo, Japan, July. Association for
Computational Linguistics.

Fei Liu and Yang Liu. 2009. From extractive to ab-
stractive meeting summaries: Can it be done by sen-
tence compression? In Proceedings of the ACL-
IJCNLP 2009 Conference Short Papers, pages 261–
264, Suntec, Singapore, August. Association for
Computational Linguistics.

Daniel Marcu. 1999. The automatic construction of
large-scale corpora for summarization research. In
Proceedings of the 22nd ACM SIGIR Conference
on Research and Development in Information Re-
trieval, pages 137–144. Association for Computing
Machinery.

Kathleen McKeown, Sara Rosenthal, Kapil Thadani,
and Coleman Moore. 2010. Time-efficient creation
of an accurate sentence fusion corpus. In Human
Language Technologies: The 2010 Annual Confer-
ence of the North American Chapter of the Associa-
tion for Computational Linguistics, pages 317–320,
Los Angeles, California, June. Association for Com-
putational Linguistics.

Christian M. Meyer, Darina Benikova, Margot
Mieskes, and Iryna Gurevych. 2016. Mdswriter:
Annotation tool for creating high-quality multi-
document summarization corpora. In Proceedings
of ACL-2016 System Demonstrations, pages 97–102,
Berlin, Germany, August. Association for Computa-
tional Linguistics.

Gabriel Murray, Steve Renals, Jean Carletta, and Jo-
hanna Moore. 2005. Evaluating automatic sum-
maries of meeting recordings. In Proceedings of the
ACL Workshop on Intrinsic and Extrinsic Evaluation
Measures for Machine Translation and/or Summa-
rization, pages 33–40, Ann Arbor, Michigan, June.
Association for Computational Linguistics.

Ani Nenkova and Kathleen McKeown. 2011. Auto-
matic summarization. Foundations and Trends in
Information Retrieval, 5:103–233.

Jessica Ouyang and Kathleen McKeown. 2015. Mod-
eling reportable events as turning points in narrative.
In Proceedings of the 2015 Conference on Empiri-
cal Methods in Natural Language Processing, pages
2149–2158, Lisbon, Portugal, September. Associa-
tion for Computational Linguistics.

Paul Over, Hoa Dang, and Donna Harman. 2007. Duc
in context. Information Processing & Management,
43(6):1506–1520.

David Zajic, Bonnie Dorr, Jimmy Lin, and Richard
Schwartz. 2007. Multi-candidate reduction: Sen-
tence compression as a tool for document summa-
rization tasks. Information Processing & Manage-
ment, 43(6):1549–1570.

51


