



















































DiscoTK: Using Discourse Structure for Machine Translation Evaluation


Proceedings of the Ninth Workshop on Statistical Machine Translation, pages 402–408,
Baltimore, Maryland USA, June 26–27, 2014. c©2014 Association for Computational Linguistics

DiscoTK: Using Discourse Structure for Machine Translation Evaluation

Shafiq Joty Francisco Guzmán Lluı́s Màrquez and Preslav Nakov
ALT Research Group

Qatar Computing Research Institute — Qatar Foundation
{sjoty,fguzman,lmarquez,pnakov}@qf.org.qa

Abstract

We present novel automatic metrics for
machine translation evaluation that use
discourse structure and convolution ker-
nels to compare the discourse tree of an
automatic translation with that of the hu-
man reference. We experiment with five
transformations and augmentations of a
base discourse tree representation based
on the rhetorical structure theory, and we
combine the kernel scores for each of them
into a single score. Finally, we add other
metrics from the ASIYA MT evaluation
toolkit, and we tune the weights of the
combination on actual human judgments.
Experiments on the WMT12 and WMT13
metrics shared task datasets show corre-
lation with human judgments that outper-
forms what the best systems that partici-
pated in these years achieved, both at the
segment and at the system level.

1 Introduction

The rapid development of statistical machine
translation (SMT) that we have seen in recent
years would not have been possible without au-
tomatic metrics for measuring SMT quality. In
particular, the development of BLEU (Papineni
et al., 2002) revolutionized the SMT field, al-
lowing not only to compare two systems in a
way that strongly correlates with human judg-
ments, but it also enabled the rise of discrimina-
tive log-linear models, which use optimizers such
as MERT (Och, 2003), and later MIRA (Watanabe
et al., 2007; Chiang et al., 2008) and PRO (Hop-
kins and May, 2011), to optimize BLEU, or an ap-
proximation thereof, directly. While over the years
other strong metrics such as TER (Snover et al.,
2006) and Meteor (Lavie and Denkowski, 2009)
have emerged, BLEU remains the de-facto stan-
dard, despite its simplicity.

Recently, there has been steady increase in
BLEU scores for well-resourced language pairs
such as Spanish-English and Arabic-English.
However, it was also observed that BLEU-like n-
gram matching metrics are unreliable for high-
quality translation output (Doddington, 2002;
Lavie and Agarwal, 2007). In fact, researchers al-
ready worry that BLEU will soon be unable to dis-
tinguish automatic from human translations.1 This
is a problem for most present-day metrics, which
cannot tell apart raw machine translation output
from a fully fluent professionally post-edited ver-
sion thereof (Denkowski and Lavie, 2012).

Another concern is that BLEU-like n-gram
matching metrics tend to favor phrase-based SMT
systems over rule-based systems and other SMT
paradigms. In particular, they are unable to cap-
ture the syntactic and semantic structure of sen-
tences, and are thus insensitive to improvement
in these aspects. Furthermore, it has been shown
that lexical similarity is both insufficient and not
strictly necessary for two sentences to convey
the same meaning (Culy and Riehemann, 2003;
Coughlin, 2003; Callison-Burch et al., 2006).

The above issues have motivated a large amount
of work dedicated to design better evaluation met-
rics. The Metrics task at the Workshop on Ma-
chine Translation (WMT) has been instrumental in
this quest. Below we present QCRI’s submission
to the Metrics task of WMT14, which consists of
the DiscoTK family of discourse-based metrics.

In particular, we experiment with five different
transformations and augmentations of a discourse
tree representation, and we combine the kernel
scores for each of them into a single score which
we call DISCOTKlight. Next, we add to the com-
bination other metrics from the ASIYA MT eval-
uation toolkit (Giménez and Màrquez, 2010), to
produce the DISCOTKparty metric.

1This would not mean that computers have achieved hu-
man proficiency; it would rather show BLEU’s inadequacy.

402



Finally, we tune the relative weights of the met-
rics in the combination using human judgments
in a learning-to-rank framework. This proved
to be quite beneficial: the tuned version of the
DISCOTKparty metric was the best performing
metric in the WMT14 Metrics shared task.

The rest of the paper is organized as follows:
Section 2 introduces our basic discourse metrics
and the tree representations they are based on.
Section 3 describes our metric combinations. Sec-
tion 4 presents our experiments and results on
datasets from previous years. Finally, Section 5
concludes and suggests directions for future work.

2 Discourse-Based Metrics

In our recent work (Guzmán et al., 2014), we used
the information embedded in the discourse-trees
(DTs) to compare the output of an MT system to
a human reference. More specifically, we used
a state-of-the-art sentence-level discourse parser
(Joty et al., 2012) to generate discourse trees for
the sentences in accordance with the Rhetorical
Structure Theory (RST) of discourse (Mann and
Thompson, 1988). Then, we computed the simi-
larity between DTs of the human references and
the system translations using a convolution tree
kernel (Collins and Duffy, 2001), which efficiently
computes the number of common subtrees. Note
that this kernel was originally designed for syntac-
tic parsing, and the subtrees are subject to the con-
straint that their nodes are taken with all or none
of their children, i.e., if we take a direct descen-
dant of a given node, we must also take all siblings
of that descendant. This imposes some limitations
on the type of substructures that can be compared,
and motivates the enriched tree representations ex-
plained in subsections 2.1–2.4.

The motivation to compare discourse trees, is
that translations should preserve the coherence re-
lations. For example, consider the three discourse
trees (DTs) shown in Figure 1. Notice that the
Attribution relation in the reference translation is
also realized in the system translation in (b) but not
in (c), which makes (b) a better translation com-
pared to (c), according to our hypothesis.

In (Guzmán et al., 2014), we have shown that
discourse structure provides additional informa-
tion for MT evaluation, which is not captured by
existing metrics that use lexical, syntactic and se-
mantic information; thus, discourse should be con-
sidered when developing new rich metrics.

Here, we extend our previous work by devel-
oping metrics that are based on new representa-
tions of the DTs. In the remainder of this section,
we will focus on the individual DT representations
that we will experiment with; then, the following
section will describe the metric combinations and
tuning used to produce the DiscoTK metrics.

2.1 DR-LEX1

Figure 2a shows our first representation of the DT.
The lexical items, i.e., words, constitute the leaves
of the tree. The words in an Elementary Discourse
Unit (EDU) are grouped under a predefined tag
EDU, to which the nuclearity status of the EDU
is attached: nucleus vs. satellite. Coherence re-
lations, such as Attribution, Elaboration, and En-
ablement, between adjacent text spans constitute
the internal nodes of the tree. Like the EDUs, the
nuclearity statuses of the larger discourse units are
attached to the relation labels. Notice that with
this representation the tree kernel can easily be ex-
tended to find subtree matches at the word level,
i.e., by including an additional layer of dummy
leaves as was done in (Moschitti et al., 2007). We
applied the same solution in our representations.

2.2 DR-NOLEX

Our second representation DR-NOLEX (Figure 2b)
is a simple variation of DR-LEX1, where we ex-
clude the lexical items. This allows us to measure
the similarity between two translations in terms of
their discourse structures alone.

2.3 DR-LEX2

One limitation of DR-LEX1 and DR-NOLEX is that
they do not separate the structure, i.e., the skele-
ton, of the tree from its labels. Therefore, when
measuring the similarity between two DTs, they
do not allow the tree kernel to give partial credit
to subtrees that differ in labels but match in their
structures. DR-LEX2, a variation of DR-LEX1, ad-
dresses this limitation as shown in Figure 2c. It
uses predefined tags SPAN and EDU to build the
skeleton of the tree, and considers the nuclearity
and/or relation labels as properties (added as chil-
dren) of these tags. For example, a SPAN has two
properties, namely its nuclearity and its relation,
and an EDU has one property, namely its nucle-
arity. The words of an EDU are placed under the
predefined tag NGRAM.

403



Elaboration ROOT

SPANNucleus AttributionSatellite

Voices are coming from Germany , SPANSatellite SPANNucleus

suggesting that ECB be the last resort creditor .

(a) A reference (human-written) translation.
Attribution

ROOT

SPANSatellite SPANNucleus

In Germany voices , the ECB should be the lender of last resort .

(b) A higher quality (system-generated) translation.

SPANROOT

In Germany the ECB should be for the creditors of last resort .

(c) A lower quality (system-generated) translation.

Figure 1: Three discourse trees for the translations of a source sentence: (a) the reference, (b) a higher
quality automatic translation, and (c) a lower quality automatic translation.

2.4 DR-LEX1.1 and DR-LEX2.1
Although both DR-LEX1 and DR-LEX2 allow the
tree kernel to find matches at the word level, the
words are compared in a bag-of-words fashion,
i.e., if the trees share a common word, the ker-
nel will find a match regardless of its position in
the tree. Therefore, a word that has occurred in
an EDU with status Nucleus in one tree could be
matched with the same word under a Satellite in
the other tree. In other words, the kernel based
on these representations is insensitive to the nu-
clearity status and the relation labels under which
the words are matched. DR-LEX1.1, an exten-
sion of DR-LEX1, and DR-LEX2.1, an extension
of DR-LEX2, are sensitive to these variations at
the lexical level. DR-LEX1.1 (Figure 2d) and DR-
LEX2.1 (Figure 2e) propagate the nuclearity sta-
tuses and/or the relation labels to the lexical items
by including three more subtrees at the EDU level.

3 Metric Combination and Tuning

In this section, we describe our Discourse Tree
Kernel (DiscoTK) metrics. We have two main
versions: DISCOTKlight, which combines the five
DR-based metrics, and DISCOTKparty, which fur-
ther adds the Asiya metrics.

3.1 DISCOTKlight
In the previous section, we have presented several
discourse tree representations that can be used to
compare the output of a machine translation sys-
tem to a human reference. Each representation
stresses a different aspect of the discourse tree.

In order to make our estimations more robust,
we propose DISCOTKlight, a metric that takes ad-
vantage of all the previous discourse representa-
tions by linearly interpolating their scores. Here
are the processing steps needed to compute this
metric:
(i) Parsing: We parsed each sentence in order to
produce discourse trees for the human references
and for the outputs of the systems.
(ii) Tree enrichment/simplification: For each
sentence-level discourse tree, we generated the
five different tree representations: DR-NOLEX,
DR-LEX1, DR-LEX1.1, DR-LEX2, DR-LEX2.1.
(iii) Estimation: We calculated the per-sentence
similarity scores between tree representations of
the system hypothesis and the human reference
using the extended convolution tree kernel as de-
scribed in the previous section. To compute the
system-level similarity scores, we calculated the
average sentence-level similarity; note that this en-
sures that our metric is “the same” at the system
and at the segment level.
(iv) Normalization: In order to make the scores of
the different representations comparable, we per-
formed a min–max normalization2 for each met-
ric and for each language pair.
(v) Combination: Finally, for each sentence, we
computed DISCOTKlight as the average of the
normalized similarity scores of the different repre-
sentations. For system-level experiments, we per-
formed linear interpolation of system-level scores.

2Where x′ = (x−min)/(max−min).

404



(a) DT for DR-LEX1.

ELABORATION-NUCLEUS

EDU-NUCLEUS EDU-SATELLITE

(b) DT for DR-NOLEX.

SPAN

NUC REL EDU EDU

NUCLEUS ELABORATION NUC NGRAM NUC NGRAM

NUCLEUS to better .. titles SATELLITE published

(c) DT for DR-LEX2.

ELABORATION-NUCLEUS

EDU-NUCLEUS ..

LEX LEX:NUC LEX:REL LEX:NUC:REL

to better .. to:N better:N .. to:ELAB better:ELAB .. to:N:ELAB better:N:ELAB ..

(d) DT for DR-LEX1.1.
SPAN

NUC REL EDU EDU

NUCLEUS ELABORATION NUC LEX LEX:NUC LEX:REL LEX:NUC:REL NUC LEX LEX:NUC ..

NUCLEUS to better .. to:N better:N .. to:ELAB better:ELAB .. to:N:ELAB better:N:ELAB .. SATELLITE published published:S

(e) DT for DR-LEX2.1.

Figure 2: Five different representations of the discourse tree (DT) for the sentence “The new organisa-
tional structure will also allow us to enter the market with a joint offer of advertising products, to better
link the creation of content for all the titles published and, last but not least, to continue to streamline
significantly the business management of the company,” added Cermak. Note that to avoid visual clutter,
(b)–(e) show alternative representations only for the highlighted subtree in (a).

3.2 DISCOTKparty

One of the weaknesses of the above discourse-
based metrics is that they use unigram lexical
information, which does not capture reordering.
Thus, in order to make more informed and ro-
bust estimations, we extended DISCOTKlight with
the composing metrics of the ASIYA’s ULC met-
ric (Giménez and Màrquez, 2010), which is a uni-
form linear combination of twelve individual met-
rics and was the best-performing metric at the sys-
tem and at the segment levels at the WMT08 and
WMT09 metrics tasks.

In order to compute the individual metrics from
ULC, we used the ASIYA toolkit,3 but we de-
parted from ASIYA’s ULC by replacing TER
and Meteor with newer versions thereof that take
into account synonymy lookup and paraphras-
ing (‘TERp-A’ and ‘Meteor-pa’ in ASIYA’s ter-
minology). We then combined the five compo-
nents in DISCOTKlight and the twelve individ-
ual metrics from ULC; we call this combination
DISCOTKparty.

3http://nlp.lsi.upc.edu/asiya/

We combined the scores using linear interpola-
tion in two different ways:

(i) Uniform combination of min-max normalized
scores at the segment level. We obtained system-
level scores by computing the average over the
segment scores.

(ii) Trained interpolation at the sentence level.
We determined the interpolation weights for the
above-described combination of 5+12 = 17 met-
rics using a pairwise learning-to-rank framework
and classification with logistic regression, as we
had done in (Guzmán et al., 2014). We obtained
the final test-time sentence-level scores by pass-
ing the interpolated raw scores through a sigmoid
function. In contrast, for the final system-level
scores, we averaged the per-sentence interpolated
raw scores.

We also tried to learn the interpolation weights
at the system level, experimenting with both re-
gression and classification. However, the amount
of data available for this type of training was
small, and the learned weights did not perform sig-
nificantly better than the uniform combination.

405



3.3 Post-processing

Discourse-based metrics, especially DR-NOLEX,
tend to produce many ties when there is not
enough information to do complete discourse
analysis. This contributes to lower τ scores for
DISCOTKlight. To alleviate this issue, we used a
simple tie-breaking strategy, in which ties between
segment scores for different systems are resolved
by using perturbations proportional to the global
system-level scores produced by the same metric,
i.e., x′segsys = xsegsys + �∗

∑
s x

s
sys. Here, � is automati-

cally chosen to avoid collisions with scores not in-
volved in the tie. This post-processing is not part
of the metric; it is only applied to our segment-
level submission to the WMT’14 metrics task.

4 Experimental Evaluation

In this section, we present some of our experi-
ments to decide on the best DiscoTK metric vari-
ant and tuning set. For tuning, testing and compar-
ison, we worked with some of the datasets avail-
able from previous WMT metrics shared tasks,
i.e., 2011, 2012 and 2013. From previous ex-
periments (Guzmán et al., 2014), we know that
the tuned metrics perform very well on cross-
validation for the same-year dataset. We further
know that tuning can be performed by concatenat-
ing data from all the into-English language pairs,
which yields better results than training separately
by language pair. For the WMT14 metrics task,
we investigated in more depth whether the tuned
metrics generalize well to new datasets. Addition-
ally, we tested the effect of concatenating datasets
from different years.

Table 1 shows the main results of our experi-
ments with the DiscoTK metrics. We evaluated
the performance of the metrics on the WMT12
and WMT13 datasets both at the segment and the
system level, and we used WMT11 as an addi-
tional tuning dataset. We measured the perfor-
mance of the metrics in terms of correlation with
human judgements. At the segment level, we eval-
uated using Kendall’s Tau (τ ), recalculated follow-
ing the WMT14 official Kendall’s Tau implemen-
tation. At the system level, we used Spearman’s
rank correlation (ρ) and Pearson’s correlation co-
efficient (r). In all cases, we averaged the results
over all into-English language pairs. The symbol
‘∅’ represents the untuned versions of our metrics,
i.e., applying a uniform linear combination of the
individual metrics.

We trained the tuned versions of the DiscoTK
measures using different datasets (WMT11,
WMT12 and WMT13) in order to study across-
corpora generalization and the effect of training
dataset size. The symbol ‘+’ stands for concatena-
tion of datasets. We trained the tuned versions at
the segment level using Maximum Entropy clas-
sifiers for pairwise ranking (cf. Section 3). For
the sake of comparison, the first group of rows
contains the results of the best-performing met-
rics at the WMT12 and WMT13 metrics shared
tasks and the last group of rows contains the re-
sults of the ASIYA combination of metrics, i.e.,
DISCOTKparty without the discourse components.

Several conclusions can be drawn from Table 1.
First, DISCOTKparty is better than DISCOTKlight
in all settings, indicating that the discourse-based
metrics are very well complemented by the hetero-
geneous metric set from ASIYA. DISCOTKlight
achieves competitive scores at the system level
(which would put the metric among the best par-
ticipants in WMT12 and WMT13); however, as
expected, it is not robust enough at the segment
level. On the other hand, the tuned versions of
DISCOTKparty are very competitive and improve
over the already strong ASIYA in each configu-
ration both at the segment- and the system-level.
The improvements are small but consistent, show-
ing that using discourse increases the correlation
with human judgments.

Focusing on the results at the segment level, it
is clear that the tuned versions offer an advantage
over the simple uniform linear combinations. In-
terestingly, for the tuned variants, given a test set,
the results are consistent across tuning sets, ruling
out over-fitting; this shows that the generalization
is very good. This result aligns well with what
we observed in our previous studies (Guzmán et
al., 2014). Learning with more data (WMT11+12
or WMT12+13) does not seem to help much,
but it does not hurt performance either. Overall,
the τ correlation results obtained with the tuned
DISCOTKparty metric are much better than the
best results of any participant metrics at WMT12
and WMT13 (20.1% and 9.5% relative improve-
ment, respectively).

At the system level, we observe that tuning over
the DISCOTKlight metric is not helpful (results
are actually slightly lower), while tuning the more
complex DISCOTKparty metric yields slightly bet-
ter results.

406



Segment Level System Level
WMT12 WMT13 WMT12 WMT13

Metric Tuning τ τ ρ r ρ r
SEMPOS na – – 0.902 0.922 – –

SPEDE07PP na 0.254 – – – – –
METEOR-WMT13 na – 0.264 – – 0.935 0.950

∅ 0.171 0.162 0.884 0.922 0.880 0.911
WMT11 0.207 0.201 0.860 0.872 0.890 0.909

DISCOTKlight WMT12 – 0.200 – – 0.889 0.910
WMT13 0.206 – 0.865 0.871 – –

WMT11+12 – 0.197 – – 0.890 0.910
WMT11+13 0.207 – 0.865 0.871 – –

∅ 0.257 0.231 0.907 0.915 0.941 0.928
WMT11 0.302 0.282 0.915 0.940 0.934 0.946

DISCOTKparty WMT12 – 0.284 – – 0.936 0.940
WMT13 0.305 – 0.912 0.935 – –

WMT11+12 – 0.289 – – 0.936 0.943
WMT11+13 0.304 – 0.912 0.934 – –

∅ 0.273 0.252 0.899 0.909 0.932 0.922
WMT11 0.301 0.279 0.913 0.935 0.934 0.944

ASIYA WMT12 – 0.277 – – 0.932 0.938
WMT13 0.303 – 0.908 0.932 – –

WMT11+12 – 0.277 – – 0.934 0.940
WMT11+13 0.303 – 0.908 0.933 – –

Table 1: Evaluation results on WMT12 and WMT13 datasets at segment and system level for the main
combined DiscoTK measures proposed in this paper.

The scores of our best metric are higher than
those of the best participants in WMT12 and
WMT13, according to Spearman’s ρ, which was
the official metric in those years. Overall, our met-
rics are comparable to the state-of-the-art at the
system level. The differences between Spearman’s
ρ and Pearson’s r coefficients are not dramatic,
with r values being always higher than ρ.

Given the above results, we submitted the fol-
lowing runs to the WMT14 Metrics shared task:
(i) DISCOTKparty tuned on the concatenation
of datasets WMT11+12+13, as our primary run;
(ii) Untuned DISCOTKparty, to verify that we are
not over-fitting the training set; and (iii) Untuned
DISCOTKlight, to see the performance of a metric
using discourse structures and word unigrams.

The results for the WMT14 Metrics shared task
have shown that our primary run, DISCOTKparty
tuned, was the best-performing metric both at the
segment- and at the system-level (Macháček and
Bojar, 2014). This metric yielded significantly
better results than its untuned counterpart, con-
firming the importance of weight tuning and the
absence of over-fitting during tuning. Finally, the
untuned DISCOTKlight achieved relatively com-
petitive, albeit slightly worse results for all lan-
guage pairs, except for Hindi-English, where sys-
tem translations resembled a “word salad”, and
were very hard to discourse-parse accurately.

5 Conclusion

We have presented experiments with novel auto-
matic metrics for machine translation evaluation
that take discourse structure into account. In par-
ticular, we used RST-style discourse parse trees,
which we compared using convolution kernels.
We further combined these kernels with metrics
from ASIYA, also tuning the weights. The re-
sulting DISCOTKparty tuned metric was the best-
performing at the segment- and system-level at the
WMT14 metrics task.

In an internal evaluation on the WMT12 and
WMT13 metrics datasets, this tuned combina-
tion showed correlation with human judgments
that outperforms the best systems that participated
in these shared tasks. The discourse-only met-
ric ranked near the top at the system-level for
WMT12 and WMT13; however, it is weak at the
segment-level since it is sensitive to parsing errors,
and most sentences have very little internal dis-
course structure.

In the future, we plan to work on an inte-
grated representation of syntactic, semantic and
discourse-based tree structures, which would al-
low us to design evaluation metrics based on more
fine-grained features, and would also allow us to
train such metrics using kernel methods. Further-
more, we want to make use of discourse parse in-
formation beyond the sentence level.

407



References
Chris Callison-Burch, Miles Osborne, and Philipp

Koehn. 2006. Re-evaluating the role of BLEU
in machine translation research. In Proceedings of
the Eleventh Conference of the European Chapter
of the Association for Computational Linguistics,
EACL’06, pages 249–256, Trento, Italy.

David Chiang, Yuval Marton, and Philip Resnik. 2008.
Online large-margin training of syntactic and struc-
tural translation features. In Proceedings of the Con-
ference on Empirical Methods in Natural Language
Processing, EMNLP’08, pages 224–233, Honolulu,
Hawaii.

Michael Collins and Nigel Duffy. 2001. Convolution
Kernels for Natural Language. In Neural Informa-
tion Processing Systems, NIPS’01, pages 625–632,
Vancouver, Canada.

Deborah Coughlin. 2003. Correlating automated and
human assessments of machine translation quality.
In Proceedings of the Machine Translation Summit
IX, MT Summit’03, pages 23–27, New Orleans, LA,
USA.

Christopher Culy and Susanne Riehemann. 2003. The
limits of n-gram translation evaluation metrics. In
Proceedings of the Machine Translation Summit IX,
MT Summit’03, pages 1–8, New Orleans, LA, USA.

Michael Denkowski and Alon Lavie. 2012. Chal-
lenges in predicting machine translation utility for
human post-editors. In Proceedings of the Tenth
Conference of the Association for Machine Trans-
lation in the Americas, AMTA’12, pages 40–49, San
Diego, CA, USA.

George Doddington. 2002. Automatic evaluation
of machine translation quality using n-gram co-
occurrence statistics. In Proceedings of the Sec-
ond International Conference on Human Language
Technology Research, HLT’02, pages 138–145, San
Francisco, CA, USA.

Jesús Giménez and Lluı́s Màrquez. 2010. Linguistic
Measures for Automatic Machine Translation Eval-
uation. Machine Translation, 24(3–4):77–86.

Francisco Guzmán, Shafiq Joty, Lluı́s Màrquez, and
Preslav Nakov. 2014. Using discourse structure
improves machine translation evaluation. In Pro-
ceedings of 52nd Annual Meeting of the Association
for Computational Linguistics, ACL’14, Baltimore,
MD, USA.

Mark Hopkins and Jonathan May. 2011. Tuning as
ranking. In Proceedings of the Conference on Em-
pirical Methods in Natural Language Processing,
EMNLP’11, pages 1352–1362, Edinburgh, Scot-
land, UK.

Shafiq Joty, Giuseppe Carenini, and Raymond Ng.
2012. A Novel Discriminative Framework for

Sentence-Level Discourse Analysis. In Proceed-
ings of the Joint Conference on Empirical Methods
in Natural Language Processing and Computational
Natural Language Learning, EMNLP-CoNLL’12,
pages 904–915, Jeju Island, Korea.

Alon Lavie and Abhaya Agarwal. 2007. METEOR:
An automatic metric for MT evaluation with high
levels of correlation with human judgments. In
Proceedings of the Second Workshop on Statisti-
cal Machine Translation, WMT’07, pages 228–231,
Prague, Czech Republic.

Alon Lavie and Michael Denkowski. 2009. The ME-
TEOR metric for automatic evaluation of machine
translation. Machine Translation, 23(2-3):105–115.

Matouš Macháček and Ondřej Bojar. 2014. Results of
the WMT14 Metrics Shared Task. In Proceedings of
the Ninth Workshop on Statistical Machine Transla-
tion, WMT’14, Baltimore, MD, USA.

William Mann and Sandra Thompson. 1988. Rhetor-
ical Structure Theory: Toward a Functional Theory
of Text Organization. Text, 8(3):243–281.

Alessandro Moschitti, Silvia Quarteroni, Roberto
Basili, and Suresh Manandhar. 2007. Exploiting
syntactic and shallow semantic kernels for question
answer classification. In Proceedings of the 45th
Annual Meeting of the Association of Computational
Linguistics, ACL’07, pages 776–783, Prague, Czech
Republic.

Franz Josef Och. 2003. Minimum error rate training in
statistical machine translation. In Proceedings of the
41st Annual Meeting of the Association for Compu-
tational Linguistics, ACL’03, pages 160–167, Sap-
poro, Japan.

Kishore Papineni, Salim Roukos, Todd Ward, and
Wei-Jing Zhu. 2002. BLEU: a method for auto-
matic evaluation of machine translation. In Pro-
ceedings of 40th Annual Meeting of the Association
for Computational Linguistics, ACL’02, pages 311–
318, Philadelphia, PA, USA.

Matthew Snover, Bonnie Dorr, Richard Schwartz, Lin-
nea Micciulla, and John Makhoul. 2006. A study
of translation edit rate with targeted human annota-
tion. In Proceedings of the Seventh Biennial Con-
ference of the Association for Machine Translation
in the Americas, AMTA’06, pages 223–231, Cam-
bridge, MA, USA.

Taro Watanabe, Jun Suzuki, Hajime Tsukada, and
Hideki Isozaki. 2007. Online large-margin training
for statistical machine translation. In Proceedings of
the Joint Conference on Empirical Methods in Natu-
ral Language Processing and Computational Natu-
ral Language Learning, EMNLP-CoNLL’07, pages
764–773, Prague, Czech Republic.

408


