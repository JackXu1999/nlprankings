










































Identifying hypernyms in distributional semantic spaces


First Joint Conference on Lexical and Computational Semantics (*SEM), pages 75–79,
Montréal, Canada, June 7-8, 2012. c©2012 Association for Computational Linguistics

Identifying hypernyms in distributional semantic spaces

Alessandro Lenci
University of Pisa, Dept. of Linguistics

via S. Maria 36
I-56126, Pisa, Italy

alessandro.lenci@ling.unipi.it

Giulia Benotto
University of Pisa, Dept. of Linguistics

via S. Maria 36
I-56126, Pisa, Italy

mezzanine.g@gmail.com

Abstract
In this paper we apply existing directional
similarity measures to identify hypernyms
with a state-of-the-art distributional semantic
model. We also propose a new directional
measure that achieves the best performance in
hypernym identification.

1 Introduction and related works
Distributional Semantic Models (DSMs) measure
the semantic similarity between words with proxim-
ity in distributional space. However, semantically
similar words in turn differ for the type of relation
holding between them: e.g., dog is strongly similar
to both animal and cat, but with different types of re-
lations. Current DSMs accounts for these facts only
partially. While they may correctly place both ani-
mal and cat among the nearest distributional neigh-
bors of dog, they are not able to characterize the
different semantic properties of these relations, for
instance the fact that hypernymy is an asymmetric
semantic relation, since being a dog entails being an
animal, but not the other way round.

The purpose of this paper is to explore the possi-
bility of identifying hypernyms in DSMs with direc-
tional (or asymmetric) similarity measures (Kotler-
man et al., 2010). These measures all rely on some
variation of the Distributional Inclusion Hypothe-
sis, according to which if u is a semantically nar-
rower term than v, then a significant number of
salient distributional features of u is included in the
feature vector of v as well. Since hypernymy is
an asymmetric relation and hypernyms are seman-
tically broader terms than their hyponyms, then we

can predict that directional similarity measures are
better suited to identify terms related by the hyper-
nymy relation.

Automatic identification of hypernyms in corpora
is a long-standing research line, but most meth-
ods have adopted semi-supervised, pattern-based ap-
proaches (Hearst, 1992; Pantel and Pennacchiotti,
2006). Fully unsupervised hypernym identification
with DSMs is still a largely open field. Various mod-
els to represent hypernyms in vector spaces have
recently been proposed (Weeds and Weir, 2003;
Weeds et al., 2004; Clarke, 2009), usually grounded
on the Distributional Inclusion Hypothesis (for a dif-
ferent approach based on representing word mean-
ing as “regions” in vector space, see Erk (2009a;
2009b)). The same hypothesis has been adopted by
Kotlerman et al. (2010) to identify (substitutable)
lexical entailments” . Within the context of the Tex-
tual Entailment (TE) paradigm, Zhitomirsky-Geffet
and Dagan (2005; 2009) define (substitutable) lex-
ical entailment as a relation holding between two
words, if there are some contexts in which one of
the words can be substituted by the other and the
meaning of the original word can be inferred from
the new one. Its relevance for TE notwithstanding,
this notion of lexical entailment is more general and
looser than hypernymy. In fact, it encompasses sev-
eral standard semantic relations such as synonymy,
hypernymy, metonymy, some cases of meronymy,
etc.

Differently from Kotlerman et al. (2010), here we
focus on applying directional, asymmetric similar-
ity measures to identify hypernyms. We assume the
classical definition of a hypernymy, such that Y is

75



an hypernym of X if and only if X is a kind of Y ,
or equivalently every X is a Y .

2 Directional similarity measures

In the experiments reported in section 3 we have ap-
plied the following directional similarity measures
(F

x

is the set of distributional features of a term x,
w

x

(f) is the weight of the feature f for x):

WeedsPrec (M1) - this is a measure that quantifies
the weighted inclusion of the features of a term u
within the features of a term v (Weeds and Weir,
2003; Weeds et al., 2004; Kotlerman et al., 2010):

WeedsPrec(u, v) =

P
f2Fu\Fv wu(f)P

f2Fu wu(f)
(1)

cosWeeds (M2) - this measure corresponds to the
geometrical average of WeedsPrec and the symmet-
ric similarity between u and v, measured by their
vectors’ cosine:

cosWeeds(u, v) =
q

M1(u, v) ⇤ cos(u, v) (2)

This is actually a variation of the balPrec measure
in Kotlerman et al. (2010), the difference being that
cosine is used as a symmetric similarity measure
instead of the LIN measure (Lin, 1998).

ClarkeDE (M3) - a close variation of M1,
proposed by Clarke (2009):

ClarkeDE(u, v) =

P
f2Fu\Fv min(wu(f), wv(f))P

f2Fu wu(f)
(3)

invCL (M4) - this a new measure that we introduce
and test here for the first time. It takes into account
not only the inclusion of u in v, but also the non-
inclusion of v in u, both measured with ClarkeDE:

invCL(u, v) =
q

M3(u, v) ⇤ (1 � M3(v, u))
(4)

The intuition behind invCL is that, if v is a seman-
tically broader term of u, then the features of u are
included in the features of v, but crucially the fea-
tures of v are also not included in the features of

u. For instance, if animal is a hypernym of lion,
we can expect i.) that a significant number of the
lion-contexts are also animal-contexts, and ii.) that
a significant number of animal-contexts are not lion-
contexts. In fact, being a semantically broader term
of lion, animal should also be found in contexts in
which animals other than lions occur.

3 Experiments

The main purpose of the experiments reported below
is to investigate the ability of the directional similar-
ity measures presented in section 2 to identify the
hypernyms of a given target noun, and to discrim-
inate hypernyms from terms related by symmetric
semantic relations, such as coordinate terms.

We have represented lexical items with distribu-
tional feature vectors extracted from the TypeDM
tensor (Baroni and Lenci, 2010). TypeDM is a par-
ticular instantiation of the Distributional Memory
(DM) framework. In DM, distributional facts are
represented as a weighted tuple structure T , a set
of weighted word-link-word tuples hhw1, l, w2i, �i,
such that w1 and w2 are content words (e.g. nouns,
verbs, etc.), l is a syntagmatic co-occurrence links
between words in a text (e.g. syntactic dependen-
cies, etc.), and � is a weight estimating the statis-
tical salience of that tuple. The TypeDM word set
contains 30,693 lemmas (20,410 nouns, 5,026 verbs
and 5,257 adjectives). The TypeDM link set con-
tains 25,336 direct and inverse links formed by (par-
tially lexicalized) syntactic dependencies and pat-
terns. The weight � is the Local Mutual Informa-
tion (LMI) (Evert, 2005) computed on link type fre-
quency (negative LMI values are raised to 0).

3.1 Test set

We have evaluated the directional similarity mea-
sures on a subset of the BLESS data set (Baroni and
Lenci, 2011), consisting of tuples expressing a re-
lation between a target concept (henceforth referred
to as concept) and a relatum concept (henceforth re-
ferred to as relatum). BLESS includes 200 distinct
English concrete nouns as target concepts, equally
divided between living and non-living entities, and
grouped into 17 broader classes (e.g., BIRD, FRUIT,
FURNITURE, VEHICLE, etc.).

For each concept noun, BLESS includes several

76



relatum words, linked to the concept by one of 5 se-
mantic relations. Here, we have used the BLESS
subset formed by 14,547 tuples with the relatum
attested in the TypeDM word set, and containing
one of these relations: COORD: the relatum is a
noun that is a co-hyponym (coordinate) of the con-
cept: halligator, coord, lizardi; HYPER: the rela-
tum is a noun that is a hypernym of the concept:
halligator, hyper, animali; MERO: the relatum is
a noun referring to a part/component/organ/member
of the concept, or something that the concept con-
tains or is made of: halligator, mero, mouthi;
RANDOM-N: the relatum is a random noun hold-
ing no semantic relation with the target concept:
halligator, random � n, messagei.

Kotlerman et al. (2010) evaluate a set of
directional similarity measure on a data set of
valid and invalid (substitutable) lexical entailments
(Zhitomirsky-Geffet and Dagan, 2009). However,
as we said above, lexical entailment is defined as
an asymmetric relation that covers various types of
classic semantic relations, besides hypernymy . The
choice of BLESS is instead motivated by the fact
that here we focus on the ability of directional simi-
larity measure to identify hypernyms.

3.2 Evaluation and results
For each word x in the test set, we represented
x in terms of a set F

x

of distributional features
hl, w2i, such that in the TypeDM tensor there is a
tuple hhw1, l, w2i, �i, w1 = x. The feature weight
w

x

(f) is equal to the weight � of the original DM
tuple. Then, we applied the 4 directional simi-
larity measures in section 2 to BLESS, with the
goal of evaluating their ability to discriminate hy-
pernyms from other semantic relations, in particular
co-hyponymy. In fact, differently from hypernyms,
coordinate terms are not related by inclusion. There-
fore, we want to test whether directional similarity
measures are able to assign higher scores to hyper-
nyms, as predicted by the Distributional Inclusion
Hypothesis. We used the Cosine as our baseline,
since it is a symmetric similarity measure and it is
commonly used in DSMs.

We adopt two different evaluation methods. The
first is based on the methodology described in Ba-
roni and Lenci (2011). Given the similarity scores
for a concept with all its relata across all relations

in our test set, we pick the relatum with the high-
est score (nearest neighbour) for each relation. In
this way, for each of the 200 BLESS concepts, we
obtain 4 similarity scores, one per relation. In or-
der to factor out concept-specific effects that might
add to the overall score variance, we transform the
8 similarity scores of each concept onto standard-
ized z scores (mean: 0; s.d: 1) by subtracting from
each their mean, and dividing by their standard devi-
ation. After this transformation, we produce a box-
plot summarizing the distribution of scores per rela-
tion across the 200 concepts.

Boxplots for each similarity measure are reported
in Figure 1. They display the median of a distribu-
tion as a thick horizontal line within a box extending
from the first to the third quartile, with whiskers cov-
ering 1.5 of the interquartile range in each direction
from the box, and values outside this extended range
– extreme outliers – plotted as circles (these are the
default boxplotting option of the R statistical pack-
age). To identify significant differences between re-
lation types, we also performed pairwise compar-
isons with the Tukey Honestly Significant Differ-
ence test, using the standard ↵ = 0.05 significance
threshold.

In the boxplots we can observe that all measures
(either symmetric or not) are able to discriminate
truly semantically related pairs from unrelated (i.e.
random) ones. Crucially, Cosine shows a strong
tendency to identify coordinates among the near-
est neighbors of target items. This is actually con-
sistent with its being a symmetric similarity mea-
sure. Instead, directional similarity measures signif-
icantly promote hypernyms over coordinates. The
only exception is represented by cosWeeds, which
again places coordinates at the top, though now the
difference with hypernyms is not significant. This
might be due to the cosine component of this mea-
sure, which reduces the effectiveness of the asym-
metric WeedsPrec. The difference between coor-
dinates and hypernyms is slightly bigger in invCL,
and the former appear to be further downgraded than
with the other directional measures. From the box-
plot analysis, we can therefore conclude that simi-
larity measures based on the Distributional Inclusion
Hypothesis do indeed improve hypernym identifica-
tion in context-feature semantic spaces, with respect
to other types of semantic relations, such as COORD.

77



coord hyper mero random-n

-1
.5

-1
.0

-0
.5

0.
0

0.
5

1.
0

1.
5

Cosine

coord hyper mero random-n

-1
.5

-1
.0

-0
.5

0.
0

0.
5

1.
0

1.
5

WeedsPrec

coord hyper mero random-n

-1
.5

-1
.0

-0
.5

0.
0

0.
5

1.
0

1.
5

cosWeeds

coord hyper mero random-n

-1
.5

-1
.0

-0
.5

0.
0

0.
5

1.
0

1.
5

ClarkeDE

coord hyper mero random-n

-1
.5

-1
.0

-0
.5

0.
0

0.
5

1.
0

1.
5

invCL

Figure 1: Distribution of relata similarity scores across concepts (values on ordinate are similarity scores after concept-
by-concept z-normalization).

The second type of evaluation we have performed
is based on Kotlerman et al. (2010). The similarity
measures have been evaluated with Average Preci-
sion (AP), a method derived from Information Re-
trieval and combining precision, relevance ranking
and overall recall. For each similarity measure, we
computed AP with respect to the 4 BLESS relations.
The best possible score (AP = 1) for a given rela-
tion (e.g., HYPER) corresponds to the ideal case in
which all the relata belonging to that relation have
higher similarity scores than the relata belonging to
the other relations. For every relation, we calculated
the AP for each of the 200 BLESS target concepts.

In Table 1, we report the AP values averaged over
the 200 concepts. On the one hand, these results
confirm the trend illustrated by the boxplots, in par-
ticular the fact that directional similarity measures
clearly outperform Cosine (or cosine-based mea-
sures such as cosWeeds) in identifying hypernyms,
with no significant differences among them. How-
ever, a different picture emerges by comparing the

measure COORD HYPER MERO RANDOM-N
Cosine 0.79 0.23 0.21 0.30

WeedsPrec 0.45 0.40 0.31 0.32
cosWeeds 0.69 0.29 0.23 0.30
ClarkeDE 0.45 0.39 0.28 0.33

invCL 0.38 0.40 0.31 0.34

Table 1: Mean AP values for each semantic relation re-
ported by the different similarity scores.

AP values for HYPER with those for COORD. since
in this case important distinctions among the di-
rectional measures emerge. In fact, even if Weed-
sPrec and ClarkeDE increase the AP for HYPER,
still they assign even higher AP values to COORD.
Conversely, invCL is the only measure that assigns
to HYPER the top AP score, higher than COORD too.

The new directional similarity measure we have
proposed in this paper, invCL, thus reveals a higher
ability to set apart hypernyms from other relations,
coordinates terms included. The latter are expected

78



to share a large number of contexts and this is the
reason why they are strongly favored by symmet-
ric similarity measures, such as Cosine. Asymmet-
ric measures like cosWeeds and ClarkeDE also fall
short of distinguishing hypernyms from coordinates
because the condition of feature inclusion they test
is satisfied by coordinate terms as well. If two sets
share a high number of elements, then many ele-
ments of the former are also included in the latter,
and vice versa. Therefore, coordinate terms too are
expected to have high values of feature inclusions.
Conversely, invCL takes into account not only the
inclusion of u into v, but also the amount of v that
is not included in u. Thus, invCL provides a better
distributional correlate to the central property of hy-
pernyms of having a broader semantic content than
their hyponyms.

4 Conclusions and ongoing research

The experiments reported in this paper support the
Distributional Inclusion Hypothesis as a viable ap-
proach to model hypernymy in semantic vector
spaces. We have also proposed a new directional
measure that actually outperforms the state-of-the-
art ones. Focusing on the contexts that broader terms
do not share with their narrower terms thus appear
to be an interesting direction to explore to improve
hypernym identification. Our ongoing research in-
cludes testing invCL to recognize lexical entailments
and comparing it with the balAPinc measured pro-
posed by Kotlerman et al. (2010) for this task, as
well as designing new distributional methods to dis-
criminate between various other types of semantic
relations.

Acknowledgments

We thank the reviewers for their useful and insight-
ful comments on the paper.

References
Marco Baroni and Alessandro Lenci. 2010. Distri-

butional Memory: A general framework for corpus-
based semantics. Computational Linguistics, 36(4):
673–721.

Marco Baroni and Alessandro Lenci. 2011. How we
BLESSed distributional semantic evaluation. In Pro-
ceedings of the GEMS 2011 Workshop on Geometri-

cal Models of Natural Language Semantics, EMNLP
2011, Edinburgh, Scotland, UK: 1–10.

Daoud Clarke. 2009. Context-theoretic semantics for
natural language: an overview. In Proceedings of the
EACL 2009 Workshop on GEMS: GEometrical Models
of Natural Language Semantics, Athens, Greece: 112–
119.

Katrin Erk. 2009a. Supporting inferences in semantic
space: representing words as regions. In Proceedings
of the 8th International Conference on Computational
Semantics, Tilburg, January: 104–115.

Katrin Erk. 2009b. Representing words as regions in
vector space. In Proceedings of the Thirteenth Con-
ference on Computational Natural Language Learning
(CoNLL), Boulder, Colorado: 57–65.

Stefan Evert. 2005. The Statistics of Word Cooccur-
rences. Ph.D. dissertation, Stuttgart University.

Marti Hearst. 1992. Automatic acquisition of hyponyms
from large text corpora. In Proceedings of COLING
1992, Nantes, France: 539–545.

Lili Kotlerman, Ido Dagan, Idan Szpektor, and Maayan
Zhitomirsky-Geffet. 2010. Directional distributional
similarity for lexical inference. Natural Language En-
gineering, 16(04): 359–389.

Dekang Lin. 1998. Automatic Retrieval and Clustering
of Similar Words. In Proceedings of the COLING-
ACL 1998, Montreal, Canada: 768–774.

Patrick Pantel and Marco Pennacchiotti. 2006. Espresso:
Leveraging generic patterns for automatically har-
vesting semantic relations. In Proceedings of the
COLING-ACL 2006, Sydney, Australia: 113–120.

Idan Szpektor and Ido Dagan. 2008. Learning Entail-
ment Rules for Unary Templates. In Proceedings of
COLING 2008, Manchester, UK: 849–856.

Julie Weeds and David Weir. 2003. A general frame-
work for distributional similarity. In Proceedings of
the EMNLP 2003, Sapporo, Japan: 81–88.

Julie Weeds, David Weir, and Diana McCarthy. 2004.
Characterising measures of lexical distributional sim-
ilarity. In Proceedings of COLING 2004, Geneva,
Switzerland: 1015–1021.

Maayan Zhitomirsky-Geffet and Ido Dagan. 2005. The
distributional inclusion hypotheses and lexical entail-
ment. In Proceedings of ACL 2005, Ann Arbor, MI:
107–114.

Maayan Zhitomirsky-Geffet and Ido Dagan. 2009. Boot-
strapping distributional feature vector quality. Compu-
tational Linguistics, 35(3): 435-461.

79


