










































Statistical Dialogue Management using Intention Dependency Graph


International Joint Conference on Natural Language Processing, pages 962–966,
Nagoya, Japan, 14-18 October 2013.

Statistical Dialogue Management using Intention Dependency Graph

Koichiro Yoshino1,2, Shinji Watanabe1, Jonathan Le Roux1, John R. Hershey1

1Mitsubishi Electric Research Laboratories, 201 Broadway, Cambridge, MA, 02139, USA
{watanabe,leroux,hershey}@merl.com

2School of Informatics, Kyoto University, Sakyo, Kyoto, 606-8501, Japan
yoshino@ar.media.kyoto-u.ac.jp

Abstract
We present a method of statistical dia-
logue management using a directed inten-
tion dependency graph (IDG) in a par-
tially observable Markov decision pro-
cess (POMDP) framework. The transition
probabilities in this model involve infor-
mation derived from a hierarchical graph
of intentions. In this way, we combine
the deterministic graph structure of a con-
ventional rule-based system with a statis-
tical dialogue framework. The IDG also
provides a reasonable constraint on a user
simulation model, which is used when
learning a policy function in POMDP and
dialogue evaluation. Thus, this method
converts a conventional dialogue manager
to a statistical dialogue manager that uti-
lizes task domain knowledge without an-
notated dialogue data.

1 Introduction

Statistical approaches based on reinforcement
learning, such as the Markov decision process
(MDP) and partially observable Markov decision
process (POMDP), have been successfully ap-
plied to dialogue management (Levin et al., 2000;
Williams and Young, 2007; Li, 2012). These ap-
proaches allow us to consider all possible future
actions of a dialogue system, and thus to obtain a
new optimal dialogue strategy which could not be
anticipated in conventional hand-crafted dialogue
systems. Moreover, the statistical dialogue frame-
work can be combined with conventional rule-
based dialogue management in hybrid systems,
(Williams, 2008; Lee et al., 2010), which com-
bine the optimal dialogue strategy in the statistical
approach with the lower cost of data and mainte-
nance of the rule-based approach.

Our research focuses on a practical application
of a hybrid statistical dialogue management based

on POMDP to conventional rule-based dialogue
management via the use of an intention depen-
dency graph (IDG). The IDG derives from the con-
ventional rule-based dialogue system (Dahl et al.,
1994; Bohus and Rudnicky, 2003), and it con-
strains the transition matrix and provides a user
simulation as a substitute for dialogue data.

The object of POMDP optimization is to pro-
duce a policy that maps from user states to sys-
tem actions such that the overall expected cost
of the dialogue is minimized. Such optimiza-
tion typically requires data from dialogue corpora,
which are manually annotated with task-oriented
dialogue-act tags. On the other hand, the benefit
of a hybrid approach is that human domain knowl-
edge can be used to constrain the possible user
states in the dialogue manager. We follow this idea
by using an IDG, which expresses the task-domain
knowledge through a directed graph of states from
more general intention categories to more specific
parameters of the intention categories. Figure 1
shows an example of such a graph, where each
node is associated with a (potentially partial) user
intention. In previous studies, this kind of do-
main knowledge is used to restrict the user state
and system action state space (Lemon et al., 2006;
Williams, 2008; Young et al., 2010; Varges et al.,
2011). However, our approach does not restrict
the possible system action states, but transfers the
information structure to the definition of user sim-
ulation and state transition probabilities. The sys-
tem is allowed to consider all possible system ac-
tions by following the user states that reflect the
IDG.

2 Statistical dialogue management

The main random variables involved at a dialogue
turn t are as follows. st = i ∈ Is is the hidden
true user statte at turn t. It is constrained by the
hidden user goal g ∈ Ig and the true user state at
the previous turn. ot = l ∈ Is is the observation

962



1: ROOT[] (=no specified request)

2: PLAY_MUSIC[artist=null, album=null]

4: PLAY_MUSIC[artist=$artist_name, album=null]

5: PLAY_MUSIC[artist=null, album=$album_name]

3: CONTROL_VOLUME[value=null]

6: CONTROL_VOLUME[value=$up_or_down]

	1

	3	2

	4 	5 6

77: PLAY_MUSIC[artist=$artist_name, album=$album_name]

Figure 1: An example of a directed intention de-
pendency graph.

of the user state by the system. It includes errors
caused by automatic speech recognition (ASR),
natural language understanding (NLU) and inten-
tion understanding (IU). Uncertainty on the ob-
servation ot caused by errors in the preprocessor
(ASR, NLU, and IU) is encompassed in the con-
ditional probability Otli = p(o

t = l|st = i).
at = k ∈ K is the system action. k̂ is the op-
timal system action that is acquired in the learn-
ing step. The goal of statistical dialogue manage-
ment is to output an optimal system action ât = k̂
given an observation ot, based on the probability
of st in a soft decision manner. The probability
of the user state st given an observation sequence
o1:t from 1 to t with confidence O1:t is denoted by
bti = p(s

t = i|o1:t; O1:t), and referred to as “be-
lief”. To avoid clutter, we will usually omit O1:t.

2.1 Belief update
We consider a belief update equation based on the
graphical model shown in Figure 2, assuming that
the system actions a1:t are given. We can obtain
the following update equation from bti to b

t+1
i′ :

bt+1i′ = p(s
t+1 = i′|o1:t+1) (1)

∝
∑

i

p(ot+1, i′|i)(bti)β, (2)

where β is a forgetting factor for the belief, and
0 ≤ β ≤ 1. Then, by introducing the system ac-
tion at = k based on the sum rule, we can rewrite
p(ot+1, i′|i) in Eq. (2) as follows:∑

k

p(ot+1, i′, k|i) =
∑

k

p(ot+1, i′|i, k)δk̂k

= p(ot+1|i′)p(i′|i, k̂) (3)

where p(k|i) = δk̂k is obtained by the decision
making step in the POMDP. We rewrite the dis-
tributions in Eq. (3) as follows: p(i′|i, k̂) = Tii′k̂

�
�

�
�

��

�
���

�
���

�
���

�
���

�
���

�
���

User state

Observation

State transition 

model

System action

Confidence 

score

…… �
�

�
�

�
�

�
�

�� ��

�

��

�
�

�
�

�
���

�
���

����������

Figure 2: Graphical model of user state sequences
given system actions at−1 and at. This graphical
model shows user behavior that is observed from
the system.

and p(ot+1 = l|i′) = Ot+1li′ . Tii′k̂ are the user
state transition probabilities given system action k̂,
and Ot+1li′ are the confidence scores given by the
pre-processor. In conventional studies, the state
transition probabilities Tii′k̂ are learned from an-
notated data. In our scheme, the probabilities can
be obtained by using the IDG, as described in Sec-
tion 3.4. We finally obtain

bt+1i′ ∝ O
t+1
li′

∑
i

Tii′k̂(b
t
i)

β. (4)

Once the system estimates the belief bti, it can out-
put the optimal action ât as ât = π∗({bti}

|Is|
i=1). π

is called a policy function, and π∗ is an optimal
policy function pre-computed in the learning step
described in the following Section.

2.2 Learning step
The aim of the learning step in reinforcement
learning is to acquire the best policy π∗. Many
algorithms formulated to solve the reinforcement
learning problem have been proposed (Shani et
al., 2013). While most advanced algorithms re-
quire transition probabilities Tii′k̂ that are calcu-
lated using annotated corpora, our approach aims
at learning a POMDP without any data. We thus
use one of the most basic algorithms, Q-learning
(Watkins and Dayan, 1992), as it can acquire the
policy without using transition probabilities. Q-
learning relies on the estimation of a Q-function
Q(bt, at), which computes the expected future re-
ward of a system action at at dialogue turn t
given the current belief bt = {bti}

|Is|
i=1 of the user

state. The Q-function can be obtained by itera-
tive updates on training dialogue data. The up-

963



dates do not involve the transition probabilities
Tii′k̂, thus we can acquire the optimal policy with-
out requiring knowledge of this function. Given
the Q-function, the optimal policy is determined
as π∗(bt) = arg max

at
Q(bt, at).

3 Dialogue management using intention
dependency graph

3.1 Intention dependency graph

An intention dependency graph (IDG) is a repre-
sentation of a user’s intention in a hierarchy, with
broad categories of the intention at the top, and
specific instantiations of those categories at the
bottom, as shown in Figure 1. A child node in
the graph represents a more specific intention than
the parent node, so that the flow from top to bot-
tom represents the completion of the full specifi-
cation of an intention. However, the graph is not
necessarily a tree, and hence there may be mul-
tiple paths from a parent node to any descendent
node. A node that is fully specified and actionable
by the system can be considered a user goal. In
node 7, which is a child of node 2, both the album
and artist are specified, and the system has enough
information to perform the desired action. Such a
graph is automatically generated from task knowl-
edge that is usually designed by hand for a conven-
tional rule-based dialogue manager (Dahl et al.,
1994; Bohus and Rudnicky, 2003), as a graphical
user interface, and can be obtained by forming a
taxonomy of the possible system actions. In our
context, a node in this graph represents a hypothe-
sis of the user’s intention and/or goal.

3.2 User simulator

Training a statistical dialogue management system
in the absence of large amounts of dialogue data
requires a user simulator to ensure adequate cov-
erage of possible user states. In a general dialogue,
the system action and the user state would follow a
dialogue history and lead toward a user goal. The
simulator thus samples user states st+1 = i, at ev-
ery time step, tending toward a user goal g, and
depending on the previous system action at = k.
Thus, our approach defines the sampling distribu-
tion p(i|g, k) by using IDG. Our approach gives
uniform distribution to hypotheses that are out-
putted by the IDG. We show an example IDG in
Figure 1 and a dialogue example in Figure 6 of
Appendix.

3.3 Learning without any annotated data
We discuss the learning for the POMDP that uses
our IDG. In our task, no data can be referred to and
we cannot calculate the transition probability that
is generally calculated from an annotated data for
the belief update. This property makes it impos-
sible to establish the exact value of the state-value
function. In standard POMDP learning, sampling
belief point approaches that select a small set of
representative belief points such as point-based
value iteration (PBVI) can be applied (Pineau et
al., 2003). However, it is difficult to sample a
small set of belief points without any tagged data.
Therefore, we calculate the action-value function
Q(bt, at), and simulate the noise with a grid-based
approach (Lovejoy, 1991; Bonet, 2002). The grid-
based approach can select points in accordance
with a grid and a noise parameter η that is released
from the data. In our learning approach, a sam-
ple of belief bti = p(s

t = i|o1:t, O1:t) is given by
p(ot+1 = l|i′) = Ot+1li′ where

Ot+1li′ =

{
1− η l = i′

η
|Is|−1 l ̸= i

′.
(5)

We tried noise η = {0.0, 0.2, 0.4, 0.6, 0.8, 1.0}.
The resulting policy does not reflect the belief up-
date, but we can use the belief update method that
follows the IDG.

3.4 State transition and belief update
The state transition probability Tii′k̂ is one of the
most important components of the belief update
in the POMDP framework. To obtain the transi-
tion probabilities, we usually require user state and
system action data with annotated tags. However,
we cannot calculate the probability because of the
lack of annotated data. Therefore, we define the
state transition probability by using an IDG similar
to user simulation, as discussed in Section 3.2. By
employing time-invariant user goal g, time-variant
user state st = i and time-variant best system ac-
tion at = k̂ in Section 2, we can represent the state
transition probabilities, as follows:

p(i′|i, k̂) =
∑

g

p(i′|g, i, k̂)p(g|i, k̂) (6)

We approximate p(i′|g, i, k̂) by user simulator
p(i′|g, k̂). This means that the next user state i′
does not depend on the previous user state i. We
approximate p(g|i, k̂) ≃ p(g|i) because the user

964



Average rewards of 10000 dialogues

POMDP (β=0.2)

MDP

-100

-80

-60

-40

-20

0

20

40

60

80

0%          20%          40%          60%          80%   Noise

Figure 3: Average rewards of
10000 dialogues between the ob-
tained dialogue manager and the
user simulator.

-30

-20

-10

0

10

20

30

40
β=0

β=0.1

β=0.2

β=0.5

Average rewards of 10000 dialogues

20%               40%               60%               80%    Noise

Figure 4: The effect of forgetting
factor β as regards the average re-
wards of 10000 dialogues.

3.5

4.0

4.5

5.0

5.5

6.0

6.5

β=0

β=0.1

β=0.2

β=0.5

Average dialogue turns in 10000 dialogues

20%               40%               60%               80%    Noise

Figure 5: The effect of forgetting
factor β as regards the average di-
alogue turns of 10000 dialogues.

goal g can be estimated from the user state i by us-
ing the IDG. As a result, Eq. (6) is approximated
as,

(6) ∼=
∑

g

p(i′|g, k̂)︸ ︷︷ ︸
simulator

p(g|i)︸ ︷︷ ︸
goal model

(7)

Here, simulator is the user simulator that is de-
fined in Section 3.2, and goal model is a goal
estimation model that can be calculated from an
IDG. Our user simulator does not perform in ac-
cordance with p(st+1 = i′|g, at = k̂) exactly, but
our model uses the track back of the user simulator
that is defined in Section 3.2. The probability of a
goal estimation model is defined as p(g|i), which
expresses possible goals given a user state st = i.

4 Evaluations

We evaluate our statistical dialogue management
approach, which uses the IDG. These are experi-
mental evaluations with the user simulator that fol-
lows Section 3.2. In the experiment, we used an
IDG that had 957 states including 667 goals.

4.1 Evaluation of average reward
We evaluated dialogue managers in terms of the
average reward for 10000 dialogues between the
user simulator and the obtained dialogue man-
ager. We simulated uniformly distributed noises
that are defined on Eq. (5) for observation. We
tried six grids that suppose a uniform distribu-
tion given by Eq. (5). The parameters (η =
{0.0, 0.2, 0.4, 0.6, 0.8, 1.0}) were sampled in the
Q-learning of the POMDP. We used parameters
γ = 0.8 and ϵ = 0.2. The belief update defined
in Section 3.4 was used for the dialogue evalua-
tion. For comparison, we prepared an MDP based

dialogue manager that learned from observations
without any noise. The average rewards result is
shown in Figure 3. In this experimental result,
the POMDP dialogue manager performed better
than the MDP based dialogue manager (MDP) in
noisy cases. The effects of forgetting factor β
in terms of average reward and average dialogue
turn are shown in Figure 4 and Figure 5. In this
graph, the proposed POMDP framework, which
includes state transition probabilities, works best
at the point β = 0.2. These figures show that the
approach depended on the forgetting factor and the
robust setting of β is left to future work.

Figure 7 in Appendix shows an example of di-
alogue between the user simulator and the dia-
logue manager. This example was obtained with
η = 0.8, β = 0.2.

5 Conclusion and discussion

We have proposed a dialogue management frame-
work that uses a directed IDG. The IDG is hand-
crafted during the construction of the conventional
rule-based dialogue system, and our approach can
easily adapt rule-based systems to a statistical di-
alogue management framework. The proposed
framework does not require annotated dialogue
data in the initial deployment that are essential for
the typical statistical dialogue management frame-
work, and this enables rapid and easy adaptation.
The proposed scheme is developed purely based
on a probability process, and the framework can be
extended to use annotated data to estimate model
parameters, which will be future work. Ongoing
work includes evaluation with real user or realis-
tic user simulator that is constructed from dialogue
logs.

965



References
Dan Bohus and Alexander I. Rudnicky. 2003. Raven-

claw: Dialog management using hierarchical task decom-
position and an expectation agenda. In Proc. of EU-
ROSPEECH.

Blai Bonet. 2002. An e-optimal grid-based algorithm for
partially observable Markov decision processes. In Proc.
of ICML, pages 51–58.

Deborah A. Dahl, Madeleine Bates, Michael Brown, William
Fisher, Kate Hunicke-Smith, David Pallett, Christine Pao,
Alexander Rudnicky, and Elizabeth Shriberg. 1994. Ex-
panding the scope of the ATIS task: The ATIS-3 corpus.
In Proc. of the workshop on Human Language Technol-
ogy, pages 43–48.

Lucie Daubigney, Matthieu Geist, and Olivier Pietquin.
2012. Off-policy learning in large-scale POMDP-based
dialogue systems. In IEEE-ICASSP, pages 4989–4992.

M. Gašić, F. Jurčı́ček, S. Keizer, F. Mairesse, B. Thomson,
K. Yu, and S. Young. 2010. Gaussian processes for fast
policy optimisation of POMDP-based dialogue managers.
In Proc. of SIGDIAL, pages 201–204.

F. Jurcicek, B. Thomson, S. Keizer, F. Mairesse, M. Gasic,
K. Yu, and S. Young. 2010. Natural belief-critic: a rein-
forcement algorithm for parameter estimation in statistical
spoken dialogue systems. In Proc. of INTERSPEECH.

Cheongjae Lee, Sangkeun Jung, Kyungduk Kim, Donghyeon
Lee, and Gary Geunbae Lee. 2010. Recent approaches to
dialogue management for spoken dialog systems. Journal
of Computer Science and Engineering, 4(1):1–22.

Oliver Lemon, Xingkun Liu, Daniel Shapiro, and Carl Tol-
lander. 2006. Hierarchical reinforcement learning of dia-
logue policies in a development environment for dialogue
systems: Reall-dude. In Proc. of the 10th Workshop on the
Semantics and Pragmatics of Dialogue, pages 185–186.

Esther Levin, Roberto Pieraccini, and Wieland Eckert. 2000.
A stochastic model of human-machine interaction for
learning dialog strategies. Speech and Audio Processing,
IEEE Transactions on, 8(1):11–23.

William Li. 2012. Understanding user state and preferences
for robust spoken dialog systems and location-aware assis-
tive technology. Master’s thesis, Massachusetts Institute
of Technology.

William S Lovejoy. 1991. Computationally feasible bounds
for partially observed Markov decision processes. Opera-
tions research, 39(1):162–175.

Teruhisa Misu and Hideki Kashioka. 2012. Simultaneous
feature selection and parameter optimization for training
of dialog policy by reinforcement learning. In Spoken
Language Technology Workshop (SLT), 2012 IEEE, pages
1–6. IEEE.

George E Monahan. 1982. State of the art? a survey of
partially observable Markov decision processes: Theory,
models, and algorithms. Management Science, 28(1):1–
16.

Sébastien Paquet, Ludovic Tobin, and Brahim Chaib-draa.
2005. An online POMDP algorithm for complex multi-
agent environments. In Proc. of AAMAS, pages 970–977.

Joelle Pineau, Geoff Gordon, Sebastian Thrun, et al. 2003.
Point-based value iteration: An anytime algorithm for
POMDPs. In Proc. of IJCAI, volume 18, pages 1025–
1032. LAWRENCE ERLBAUM ASSOCIATES LTD.

ShaoWei Png and Joelle Pineau. 2011. Bayesian reinforce-
ment learning for POMDP-based dialogue systems. In
Proc. of IEEE-ICASSP, pages 2156–2159. IEEE.

Guy Shani, Joelle Pineau, and Robert Kaplow. 2013. A sur-
vey of point-based POMDP solvers. Autonomous Agents
and Multi-Agent Systems, 27(1):1–51.

Matthijs TJ Spaan and Nikos Vlassis. 2005. Perseus: Ran-
domized point-based value iteration for POMDPs. Jour-
nal of artificial intelligence research, 24(1):195–220.

Sebastian Varges, Giuseppe Riccardi, Silvia Quarteroni, and
Alexei V Ivanov. 2011. POMDP concept policies and
task structures for hybrid dialog management. In Proc. of
IEEE-ICASSP, pages 5592–5595.

Christopher JCH Watkins and Peter Dayan. 1992. Q-
learning. Machine learning, 8(3):279–292.

Jason D Williams and Steve Young. 2007. Partially ob-
servable Markov decision processes for spoken dialog sys-
tems. Computer Speech & Language, 21(2):393–422.

Jason D. Williams. 2008. The best of both worlds: Unifying
conventional dialog systems and POMDPs. In Proc. of
INTERSPEECH.

Steve Young, Milica Gašić, Simon Keizer, François Mairesse,
Jost Schatzmann, Blaise Thomson, and Kai Yu. 2010.
The hidden information state model: A practical frame-
work for POMDP-based spoken dialogue management.
Computer Speech & Language, 24(2):150–174.

A Dialogue examples

� = ����: 	7 = PLAY_MUSIC[artist=The Beatles, album= Abbey Road]

�� = 1 : ROOT[]
Ask	question	on	possible	goals	from	1: {��: 1 = “What do you want me to do?”}

�� = ��: 1 = “What do you want me to do?”

�� = 4 : “Play The Beatles” (mumbled)

ASR/NLU/IU output: “Play $unknown_slot”  � ASR mistake

�� = 2 : PLAY_MUSIC[artist=NULL, album=NULL]
Launch	a	possible	command	from	2: {��: 2 = “Please say album and/or artist.”

Confirm: 2 =“Do you want to play music?”}

�� = ��: 2 = “Please say album and/or artist” � System selected ��: 2

�� = 4 : “Play The Beatles” (clearer)

ASR output: “Play The Beatles”; NLU/IU output: “Play $artist=[The Beatles]”

�� = 4 : PLAY_MUSIC[artist=The Beatles, album=NULL]
Launch	a	possible	command	from	4 : {��: 4 = “Please say specific album.”

����: 4 = “I will play all albums of The Beatles.”

���	
��: 4 =“Do you want to play The Beatles?”}

�� = ��: 4 = “Please say specific album”          � System selected ��: 4

�� = 7 : “Play Abbey Road by The Beatles”

ASR/NLU/IU output: “Play $album=[Abbey Road] by $artist=[The Beatles]”

�� = 7 : PLAY_MUSIC[artist=The Beatles, album= Abbey Road]
Launch	a	possible	command	from	7 : {����: 7 = “I will play Abbey Road by The Beatles.”

���	
��: 7 =“Do you want to play Abbey Road by The Beatles?”}

�� = ����: 7 = “I will play Abbey Road by The Beatles” � System selected ����: 7

Figure 6: A dialogue example.

User Simulator

Draw � = ����: ��
Ask �� = � with 	 � 


�� = �� selected

Ask �� = � with 	 � 
,�

�� = �� selected

Ask �� = � with 	 � 
,�

�� = �� selected

System (Dialogue Manager)

Recognize �� = �� in conf. 0.2

Respond �� = ������: ��, 

Belief point [�� = �� conf.=0.2]

Recognize �� = �� in conf. 0.2

Update belief �� = �� in conf. 0.8229

Respond �� = ��: ��,

Belief point [�� = �� conf.=0.8]

Recognize �� = �� in conf. 0.2

Update belief �� = �� in conf. 0.4303

Respond �� = ����: ��, 

Belief point [�� = �� conf.=0.4]

Figure 7: An example of the obtained dialogue be-
tween the user simulator and the our system.

966


