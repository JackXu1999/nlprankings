



















































Comparing Computational Cognitive Models of Generalization in a Language Acquisition Task


Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 96–106,
Austin, Texas, November 1-5, 2016. c©2016 Association for Computational Linguistics

Comparing Computational Cognitive Models of Generalization in a
Language Acquisition Task

Libby Barak, Adele E. Goldberg,
Psychology Department

Princeton University
Princeton, NJ, USA

{lbarak,adele}@princeton.edu

Suzanne Stevenson
Department of Computer Science

University of Toronto
Toronto, Canada

suzanne@cs.toronto.edu

Abstract

Natural language acquisition relies on appro-
priate generalization: the ability to produce
novel sentences, while learning to restrict pro-
ductions to acceptable forms in the language.
Psycholinguists have proposed various prop-
erties that might play a role in guiding appro-
priate generalizations, looking at learning of
verb alternations as a testbed. Several com-
putational cognitive models have explored as-
pects of this phenomenon, but their results are
hard to compare given the high variability in
the linguistic properties represented in their
input. In this paper, we directly compare two
recent approaches, a Bayesian model and a
connectionist model, in their ability to repli-
cate human judgments of appropriate gener-
alizations. We find that the Bayesian model
more accurately mimics the judgments due to
its richer learning mechanism that can exploit
distributional properties of the input in a man-
ner consistent with human behaviour.

1 Introduction

Native speakers of a language are mostly able to
generalize appropriately beyond the observed data
while avoiding overgeneralizations. A testbed area
for studying generalization behavior in language ac-
quisition is verb alternations – i.e., learning the pat-
terns of acceptability of alternative constructions for
expressing similar meanings. For example, English
speakers readily use a new verb like text in both the
double-object (DO) construction (“text me the de-
tails”) and the prepositional-dative (PD) (“text the
details to me”) – an instance of the dative alterna-
tion. However, speakers avoid overgeneralizing the

DO construction to verbs such as explain that resist
its use (“?explain me the details”), even though they
occur with analogous arguments in the PD alterna-
tive (“explain the details to me”). Psycholinguis-
tic studies have focused on the possible properties
of natural language that enable such generalization
while constraining it to acceptable forms.

Initially, children are linguistically conservative:
they generally use verbs in constructions that are
very close to exemplars in the input (Lieven et al.,
1997; Akhtar, 1999; Tomasello, 2003; Boyd and
Goldberg, 2009). Children reach adult-like com-
petence by gradually forming more general as-
sociations of constructions to meaning that allow
them to extend verb usages to unwitnessed forms.
Much work has emphasized the role of verb classes
that capture the regularities across semantically-
similar verbs, enabling appropriate generalization
(e.g., Pinker, 1989; Fisher, 1999; Levin, 1993; Am-
bridge et al., 2008). Usage-based approaches have
argued that such class-based behaviour can arise in
learning through the clustering of observed usages
that share semantic and syntactic properties (e.g.,
Bybee, 2010; Tomasello, 2003; Goldberg, 2006).

A number of studies also reveal that the statistical
properties of the language play a central role in lim-
iting generalization (e.g., Bresnan and Ford, 2010;
Ambridge et al., 2012, 2014). Individual verbs of-
ten show statistical biases that favor their appear-
ance in one construction over another (Ford et al.,
1982; MacDonald et al., 1994; Garnsey et al., 1997;
Trueswell et al., 1993; Losiewicz, 1992; Gahl and
Garnsey, 2004). For example, while both give and
push can occur in either DO or PD constructions,

96



give strongly favors the DO construction (“give me
the box”), while push strongly favors the PD (“push
the box to me”) (Wasow, 2002). Generally, the
more frequent a verb is overall, the less likely speak-
ers are to extend it to an unobserved construction
(Braine and Brooks, 1995). In addition, when a verb
repeatedly occurs in one construction when an al-
ternative construction could have been appropriate,
speakers appear to learn that the verb is inappropri-
ate in the alternative, regardless of its overall fre-
quency (Goldberg, 2011).

Given these observations, it has been argued that
both the semantic and statistical properties of a
verb underlie its degree of acceptability in alternat-
ing constructions (e.g., Braine and Brooks, 1995;
Theakston, 2004; Ambridge et al., 2014). Recently,
Ambridge and Blything (2015) propose a computa-
tional model designed to study the role of verb se-
mantics and frequency in the acquisition of the da-
tive alternation. However, they only evaluate their
model preferences for one of the two constructions,
which does not provide a full picture of the alterna-
tion behaviour; moreover, they incorporate certain
assumptions about the input that may not match the
properties of naturalistic data.

In this paper, we compare the model of Ambridge
and Blything (2015) to the Bayesian model of Barak
et al. (2014) that offers a general framework of verb
construction learning. We replicate the approach
taken in Ambridge and Blything (2015) in order to
provide appropriate comparisons, but we also extend
the experimental settings and analysis to enable a
more fulsome evaluation, on data with more natu-
ralistic statistical properties. Our results show that
the Bayesian model provides a better fit to the psy-
cholinguistic data, which we suggest is due to its
richer learning mechanism: its two-level clustering
approach can exploit distributional properties of the
input in a manner consistent with human generaliza-
tion behaviour.

2 Related Work

Acquisition of the dative alternation – use of the DO
and PD constructions with analogous semantic argu-
ments – has been studied in several computational
cognitive models because it illustrates how people
learn to appropriately generalize linguistic construc-

tions in the face of complex, interacting factors.
As noted by Ambridge et al. (2014), such models
should capture influences of the verb such as its se-
mantic properties, its overall frequency, and its fre-
quency in various constructions.

A focus of computational models has been to
show under what conditions a learner generalizes
to the DO construction having observed a verb in
the PD, and vice versa. For example, the hierar-
chical Bayesian models of Perfors et al. (2010) and
Parisien and Stevenson (2010) show the ability to
generalize from one construction to the other. How-
ever, both models are limited in their semantic repre-
sentations. Perfors et al. (2010) use semantic prop-
erties that directly (albeit noisily) encode the knowl-
edge of the alternating and non-alternating (DO-
only or PD-only) classes. The model of Parisien and
Stevenson (2010) addresses this limitation by learn-
ing alternation classes from the data (including the
dative), but it uses only syntactic slot features that
can be gleaned automatically from a corpus. In ad-
dition, both models use batch processing, failing to
address how learning to generalize across an alter-
nation might be achieved incrementally.

Alishahi and Stevenson (2008) presents an in-
cremental Bayesian model shown to capture vari-
ous aspects of verb argument structure acquisition
(Alishahi and Pyykkon̈en, 2011; Barak et al., 2012,
2013b; Matusevych et al., 2016), but the model
is unable to mimic alternation learning behaviour.
Barak et al. (2014) extends this construction-
learning model to incrementally learn both construc-
tions and classes of alternating verbs, and show the
role of the classes in learning the dative. However,
like Parisien and Stevenson (2010), the input to the
model in this study is limited to syntactic properties,
not allowing for a full analysis of the relevant factors
that influence acquisition of alternations.

Ambridge and Blything (2015) propose the first
computational model of this phenomenon to include
a rich representation of the verb/construction seman-
tics, drawn from human judgments. In evaluation,
however, they only report the ability of the model
to predict the DO usage (i.e., only one pair of the
alternation), which does not give the full picture of
the alternation behaviour. Moreover, their assump-
tions about the nature of the input – including the
use of raw vs. log frequencies and the treatment of

97



Figure 1: A visual representation of the feed-forward
network used by the AB model. (The figure is adapted
from output of the OXlearn package of Ruh and West-
ermann (2009).) The input nodes correspond to the se-
mantic properties of the verbs, the verb lexemes, and a
“transfer” node (explained in the text). The output nodes
correspond to the target constructions.

non-dative construction usages – differ from earlier
models, making it difficult to compare the results.

In this paper, we compare the models of Am-
bridge and Blything (2015) and Barak et al. (2014),
using the same input settings for each, so that, for
the first time, two computational models of this gen-
eralization phenomenon can be directly compared.
Moreover, in contrast to Ambridge and Blything
(2015) and in line with the other studies mentioned
above, we evaluate the ability of the models to gen-
erate both the DO and the PD alternates, on a per
verb basis, in order to more accurately assess the fit
to human judgments.

3 The Computational Models

In this section, we give an overview of the con-
nectionist model of Ambridge and Blything (2015),
hereafter the AB model, and the Bayesian model of
Barak et al. (2014), hereafter the BFS model, fol-
lowed by a comparison of their relevant properties.

3.1 Overview of the Connectionist Model
The AB connectionist model of Ambridge and Bly-
thing (2015) aims to predict the preference of a verb
for each of three target constructions, on the basis
of verb semantics and the observed distribution of
verbs in those constructions in the input. Figure 1
provides an illustration of the 3-layer feed-forward
network, trained using backpropagation. Each input
to the model consists of lexical and semantic fea-
tures of a verb and its usage. The target output is

a 1-hot pattern across output nodes, each of which
represents the use of the verb in the associated con-
struction. The possible constructions are DO, PD, or
other, representing all other constructions the verb
appears in. Training presents the slate of input fea-
tures with the appropriate output node activated rep-
resenting the construction the verb appears in. In a
full sweep of training, the model observes all verbs
in proportion to their frequency in the input; for each
verb, the proportion of training trials with 1 in each
of the output nodes corresponds to the frequency of
the verb in each of those constructions. During test-
ing, only the input nodes are activated (correspond-
ing to a verb and its semantics), and the activation
of output nodes reveals the learned proportional ac-
tivation rate corresponding to the degree of verb bias
toward either the DO or the PD (or other).

The structure of the AB model encodes some as-
sumptions regarding the information and learning
mechanisms available to the learner. The model in-
corporates awareness of individual verbs by having a
node per verb in the input to distinguish the usage of
each verb and its accompanying features. Each verb
is also represented by a vector of semantic features
that capture properties relevant to its meaning when
used in one of the two dative constructions (based
on elicited human judgments from Ambridge et al.,
2014). The “transfer” input node encodes the abil-
ity to distinguish the semantic properties of the da-
tive constructions from other constructions: i.e., this
node is set to 1 for a DO or PD usage, and to 0 oth-
erwise. Representing the construction of the input
usage (DO, PD, or other) on the output nodes re-
flects the formalization of the learning as an associa-
tion of semantic and lexical features with a syntactic
pattern, and the knowledge of the model is demon-
strated by activating the construction output nodes
in response to a lexical/semantic input.

3.2 Overview of the Bayesian Model

The BFS of model Barak et al. (2014) is a Bayesian
clustering model that simultaneously and incremen-
tally learns both constructions and verb classes in a
two-level design; see Figure 2 for an illustration of
each level. In learning, the model processes an input
sequence of verb usages, represented as collections
of semantic and syntactic features, one usage at a
time. The first step of processing each input aims to

98



Figure 2: A visual representation of the the Bayesian
model, with sample input features for verb usages, con-
struction level, and verb class level.

find the best cluster at level one as:

BestCluster(Fi) = argmax
k∈Clusters

P (k|Fi) (1)

where Fi is the set of features for input i, and k
ranges over all existing clusters and a new one. The
number of possible clusters is not set in advanced,
and thus at any step the best choice may be to start a
new cluster (of size 1) with this input.

Using Bayes rule:

P (k|Fi) =
P (k)P (Fi|k)

P (Fi)
∝ P (k)P (Fi|k) (2)

The prior probability of a cluster P (k) is propor-
tional to the number of verb usages clustered to k
so far, thus assigning a higher prior to larger clus-
ters. The likelihood P (Fi|k) is estimated based on
the match of feature values in the current verb usage
to those aggregated in the cluster, where the quality
of the match depends on the frequency and vector
similarity of the two sets of features.

The clusters at this level correspond to construc-
tions of the language – i.e., probabilistic associa-
tions of form and meaning. For example, a clus-
ter emerges from semantically-similar verbs like tell
and ask, in a particular syntax, such as the DO. Cre-
ating a new cluster – forming a new construction –
depends on both the likelihood and the prior. Early
on, the P (Fi|k) term has more influence and differ-
ences in feature values between a new usage and ex-
isting clusters will often trigger a new cluster. Later,
the model will favour adding a new input to an ex-
isting cluster – even if it makes it more heteroge-
neous – because the P (k) term prefers larger clus-
ters as the number of observed inputs increases. This

mechanism mimics human language learning behav-
ior of moving from more verb-specific constructions
to more general constructions (Tomasello, 2003).

Each verb can occur in several clusters in the first
level based on its association with various seman-
tic and syntactic features. For instance, the alternat-
ing verb give can occur in one cluster associating
a transfer meaning with PD syntax and in a second
cluster associating a transfer meaning with DO syn-
tax. To capture the common behaviour of such al-
ternating verbs – where verbs with similar meanings
occur across the same set of clusters – the model rep-
resents the similarity in distributional properties of
the verbs in a second level of representation, which
captures such verb class behaviours.

Formally, after each clustering decision in the first
level, the model calculates the current frequency dis-
tribution of the input verb over all level-one clus-
ters. This distribution vector is used as input for the
second level: the model measures the similarity of
this vector to the weighted average distribution rep-
resented by each second-level cluster, adding the in-
put verb’s distribution to the most similar one:

BestClass(dvt) = argmax
c∈Classes

(1−DJS(dc‖dvt))
(3)

where dvt is the current distribution of the verb v
over the clusters (i.e., at time t), c ranges over all the
classes in the second level, dc is the weighted aver-
age of c given the distributions of its member verb
tokens, and DJS is the Jensen–Shannon divergence.
As in the first level, the model may create a new clus-
ter in the second level if none of the existing clusters
is similar enough to dvt .

The resulting second-level clusters capture verb
class behavior by grouping verbs that share a pat-
tern of usages across constructions, e.g., alternating
verbs that occur with the DO and PD syntax. These
clusters encode a snapshot of the distribution of each
verb each time it occurs in the input, reflecting the
need of the language learner to incrementally update
their knowledge of the distributional behavior of the
verb across constructions.

3.3 Comparison of the Models
Both models capture the semantic and statistical
properties of language proposed as possible fac-
tors in the ability to learn an alternation appropri-

99



ately – i.e., to generalize to new uses but not over-
generalize to inappropriate verbs. Semantic influ-
ences are reflected in the use of meaning features,
and each model incorporates the key idea behind
statistical preemption (Goldberg, 1995), namely
that semantically-appropriate constructions compete
with one another. The statistical effects of over-
all verb frequency and of frequency of the verb-in-
construction are captured by inputting each verb in
proportion to its frequency with each construction.

The models have a crucial difference in how they
reflect the influence of the various features in learn-
ing alternations. As a feed-forward network, the
AB model learns the weight of the semantic features
given the entire set of input, and uses these weight-
ings to shape the prediction of a verb’s preference
for each of the syntactic constructions (represented
by the target output nodes). The BFS model does not
explicitly weight features, but the influence of a fea-
ture is determined by its local context within a clus-
ter. For example, if the value of a feature has high
frequency in a cluster – e.g., the cluster records us-
ages with only the DO syntax – the predictions based
on this cluster would strongly prefer matching us-
ages based on this feature value; a less-frequent fea-
ture would have less influence on this cluster’s pre-
dictions, but could have more influence in a cluster
where it is more represented. This property, along
with the representation of an open-ended set of con-
structions (level one clusters) and verb classes (level
two clusters), enables the model to capture rich in-
teractions among the lexical, semantic, and syntactic
features. We evaluate the role of these differences in
the fit of each model to the task.

4 Experimental Setup

4.1 Input and Training

We base the learning and evaluation on the 281 dis-
tinct verbs used in Ambridge and Blything (2015),
which had been determined to occur in the dou-
ble object (DO) and/or the preposition-dative (PD)
(Pinker, 1989; Levin, 1993). Following Ambridge
and Blything (2015), we consider a third (artificial)
construction labeled as other that corresponds to all
non-DO and non-PD usages of a verb. The mod-
els are trained on usages of the verbs in proportion
to their frequencies in the British National Corpus

Raw freq Log freq
#Verbs DO PD other DO PD other

PD 101 0 93 9964 0 3 5
DO 7 49 0 877 2 0 4
Alt 75 325 1144 13332 3 3 7
Uns 98 0 0 716 0 0 4

Table 1: Frequency data for the dative verbs in the
BNC for non-alternating PD-only and DO-only verbs,
ALTernating verbs, and UNSeen dative-taking verbs that
do not occur with the dative constructions in the BNC.

(BNC) (Leech, 1992). Table 1 summarizes the per-
construction frequency data for the verbs.1 Note that
98 of the verbs can occur in the DO and/or PD but
have no such occurrences in the BNC; these verbs
unseen in the dative are important for judging the
appropriate generalization behavior of the models.

The input to the models include: the lexeme of
the verb, the semantic features of the verb, a “trans-
fer” feature marking the common meaning of the da-
tive constructions, and (in training only) a syntactic
feature. The syntactic feature indicates whether a
verb is used with the DO, PD, or other construc-
tion; in the AB model, this is given as the target
output node in training. The verb semantic features
are those used in Ambridge and Blything (2015).
These vectors are based on the ratings of each verb
on 18 meaning properties relevant to use of the verb
in the dative (e.g., “The verb specifies the means of
transfer” Ambridge et al., 2014), subject to Princi-
pal Component Analysis by Ambridge and Blything
(2015), yielding a vector of 7 dimensions. The trans-
fer feature is 1 for a verb usage in one of the two da-
tive constructions, and 0 for the other construction,
to indicate the shared transfer meaning conveyed by
the DO and the PD. The input to each model is gen-
erated automatically to correspond to the BNC fre-
quencies of each verb in each of the constructions.

It should be noted that, while we adopt the se-
mantic features of Ambridge and Blything (2015),
they reflect the meaning within the two dative con-
structions and may be less applicable to the other
construction. In addition, we found that there are
alternating and non-alternating verbs that have very
similar semantic vectors, indicating that these fea-

1The full list of verbs and their frequencies can be found in
Ambridge and Blything (2015).

100



tures may not sufficiently distinguish the alternation
behaviours.

The models are trained with sufficient input to
converge on stable behavior. We follow Ambridge
and Blything (2015) in training and testing the AB
model using the OXlearn MATLAB package (Ruh
and Westermann, 2009); the input is generated us-
ing a random seed, in random input order without
replacements, and the model is trained with a learn-
ing rate of 0.01 for 1K sweeps for log frequencies;
100K sweeps for raw frequencies. We train the BFS
model using the input generation method described
by Barak et al. (2014), with the features as above.
The model is trained on 5K input verb usages (in
proportion to their frequencies in the constructions).

4.2 Evaluation of the Models
As in Ambridge and Blything (2015), to test the
model preferences for the DO or PD, the models are
presented with an input consisting of a verb lexeme,
its semantic features, and the transfer feature set to 1
(i.e., this is a “transfer” semantics suitable for a da-
tive construction). For the AB model, we measure
preferences for each construction as the activation
rate of each of the corresponding output nodes, as in
Ambridge and Blything (2015). In the BFS model,
the preference for each construction is measured as
its likelihood over the learned clusters given the verb
and its semantic features. Formally, the prediction in
the Bayesian model is:

P (s|Ftest) =
∑

k∈Clusters
P (s|k)P (k|Ftest) (4)

where s is the predicted syntactic construction (DO
or PD) and Ftest is the set of test features represent-
ing a verb v and its corresponding semantic features.
P (s|k) is the probability of the syntactic pattern fea-
ture having the value s in cluster k, calculated as the
proportional occurrences of s in k. P (k|Ftest) is the
probability of cluster k given test features Ftest, cal-
culated as in Eqn. (2). Following Barak et al. (2014),
we calculate P (k|Ftest) in two ways, using just the
constructions (level one) or both the classes (level
two) and the constructions, to see whether verb class
knowledge improves performance. Using solely the
construction level, the probability of k reflects the
frequency with which usages of verb v occur in clus-
ter k. Using the verb class level in addition, the dis-

tribution of the verb over classes in the second level
is combined with the distribution of those classes
over the constructions in level one, to get the like-
lihood of k.

These model preferences of the verbs for a da-
tive construction are compared, using Pearson cor-
relation, to the DO/PD acceptability judgment data
collected from adult participants by Ambridge et al.
(2014). Note that Ambridge and Blything (2015)
only evaluate their model’s preferences for verbs to
take the DO construction. To fully understand the
preference and generalization patterns, we also an-
alyze the results for the PD preference. Even more
importantly, we calculate the difference between the
preferences for the DO and the PD constructions per
verb, and compare these to analogous scores for the
human data, as suggested by Ambridge et al. (2014).
The DO−PD difference scores, which we will refer
to as the verb bias score, are crucial because, as in
the human data, it is these scores that accurately cap-
ture a learner’s relative preference for a construction
given a particular verb.

5 Experiments and Analysis of Results

We examine the ability of each model to match
the dative construction preferences of human judg-
ments, as described just above, under two different
experimental scenarios. In Section 5.1, we follow
the experimental settings of Ambridge and Blything
(2015). We replicate their results on the AB model
showing correlation with human DO preferences,
but find that only the BFS model achieves a signifi-
cant correlation with the crucial verb bias score that
appropriately assesses per-verb preference. We ad-
just the experimental settings in Section 5.2 to use
more naturalistic input data – by training in propor-
tion to raw frequencies and excluding the artificial
other construction – achieving an improvement in
the verb bias score for both models.

5.1 Exp 1: Log Freq Input; 3 Constructions

Results. We first evaluated the models under the
experimental conditions of Ambridge and Blything
(2015), providing input corresponding to the verbs
in 3 constructions (DO, PD, and other), in propor-
tion to their log frequencies; see Table 2. We repli-
cate the positive correlation of the AB model over

101



AB (Connectionist)
BFS (Bayesian)

Level 1 Level 2
DO 0.54 0.24 0.29
PD 0.39 0.30 0.50
DO-PD [-0.02] 0.48 0.53

Table 2: Pearson correlation values between human and
model preferences for each construction and the verb-
bias score (DO−PD); training on log frequencies and 3
constructions. All correlations significant with p-value
< 0.001, except the one value in square brackets. Best
result for each row is marked in boldface.

the ratings for the DO construction found in Am-
bridge and Blything (2015). In addition, our analy-
sis shows that the AB model produces a significant
positive correlation with the PD acceptability rating.
However, the AB model has no correlation with the
verb bias score. Although the model ranks the sep-
arate verb preferences for DO and PD similarly to
humans, the model does not produce the same rel-
ative preference for individual verbs. For exam-
ple, the human data rank give with high acceptabil-
ity in both the DO and the PD, with a higher value
for the DO construction. Although the AB model
has a high preference for both constructions for give
(compared with other verbs), the model erroneously
prefers give in the PD construction.

The BFS model also produces preferences for
verbs in each construction that have a significant
positive correlation with human judgments. While
the AB model shows better correlation with the DO
judgments, the BFS model correlates more strongly
with the PD judgments. Importantly, in contrast to
the AB model, the verb bias score of the BFS model
also significantly correlates with the judgment data.
That is, the BFS model provides a better prediction
of the preference per verb, which is key to producing
a verb in the appropriate syntax.

Analysis. We can explain these results by look-
ing more closely at the properties of the input and
the differences in the learning mechanisms of each
model. Following Ambridge and Blything (2015),
the input presents an artificial other construction in
proportion to the frequency of the verbs with all non-
dative constructions. The very high frequency of this
single artificial construction (see other in Table 1)
results in higher predictions of it for any of the verbs,
even though the “transfer” feature in test inputs has

a value intended to signal one of the dative construc-
tions. As a result, the preferences for the dative con-
structions in both models have a very small range of
values, showing relatively small differences.

The BFS model is also affected by the relatively
compressed semantic space of the input, which is
exacerbated by the use of log frequencies to guide
the input. As noted earlier, we found that the se-
mantic features of alternating verbs can be highly
similar to non-alternating verbs – e.g., give (alternat-
ing) and pull (PO-only) have similar semantic vec-
tors. With such input, the model cannot form suf-
ficiently distinct first-level clusters based on the se-
mantics, particularly when the data is presented with
such a flat distribution (note the small differences in
log frequencies in Table 1). Visual inspection re-
veals that these clusters in the model largely form
around syntactic constructions, with mixed seman-
tic properties. Despite this, the first-level clusters
capture a strong enough association between indi-
vidual verbs and their constructions to yield a good
correlation of the verb bias score with human judg-
ments, and drawing on the second-level (verb-class)
clusters improves the results.

Conclusions. The use of an artificial high-
frequency non-dative construction (other), and the
use of log frequencies, seem to mask the influ-
ence of the semantic and syntactic properties on
learning the verb-bias for each verb. Previous psy-
cholinguistic data and computational models have
found that a skewed naturalistic distribution of the
input is helpful in learning constructions, due to
the high-frequency verbs establishing appropriate
construction-meaning associations (Casenhiser and
Goldberg, 2005; Borovsky and Elman, 2006; Barak
et al., 2013b; Matusevych et al., 2014). To allow
a more direct analysis of the role of statistical and
semantic properties in learning and generalizing the
dative, we adjust the input to the models in the next
section.

5.2 Exp 2: Raw Freq Input; 2 Constructions

Results. Here we perform the same type of experi-
ments, but using input in proportion to the raw fre-
quencies of the verbs (instead of log frequencies)
over occurrences only in the two dative construc-
tions (with no other construction). Since 98 of the
281 verbs do not occur with either dative construc-

102



AB (Connectionist)
BFS (Bayesian)

Level 1 Level 2
DO [0.06] 0.23 0.25
PD 0.33 0.38 0.32
DO-PD 0.39 0.53 0.59

Table 3: Pearson correlation values between human and
model preferences for each construction and the verb-bias
score; training on raw frequencies and 2 constructions.
All correlations significant with p-value < 0.001, except
the one value in square brackets. Best result for each row
is marked in boldface.

tion in the BNC, this also allows us to more strin-
gently test the generalization ability of the models,
by considering their behavior when ˜1/3 of the verbs
are unseen in training.

Table 3 presents the correlation results for the
two models’ preferences for each construction and
the verb bias score; we also show the correlation
plots for the verb bias score in Figure 3. The AB
model does not correlate with the judgments for the
DO. However, the model produces significant posi-
tive correlations with the PD judgments and with the
verb bias score. The BFS model, on the other hand,
achieves significant positive correlations on all mea-
sures, by both levels. As in the earlier experiments,
the best correlation with the verb bias score is pro-
duced by the second level of the BFS model, as Fig-
ure 3 demonstrates.

Analysis. As shown by Barak et al. (2013b),
the Bayesian model is better at learning the distri-
bution pattern of each verb class given a skewed
distribution, as in the raw frequencies here. The
model learns an association of each construction to
the frequently-observed meaning of high-frequency
verbs. For example, the semantics of the DO is most
strongly influenced by the semantics of its most fre-
quently occurring instance: give. The accuracy of
preference judgments benefits from the entrench-
ment of the relevant meaning with the construction.
This supports appropriate generalization – e.g., be-
cause reward is semantically similar to give, it has
a good fit to the human preference judgments even
though it is unseen with the dative (see Figure 3).
But the same factor can serve to limit generaliza-
tion – e.g., because the unseen verb mail is semantic
dissimilar to a frequent PD-only verb like pull, its
preference for the PD syntax is limited, giving it a

(a) AB model

(b) BFS model - construction level

(c) BFS model - verb class level

Figure 3: Correlation of the dative verbs with the verb
bias score of each model in Exp. 2: (a) the AB model (r =
0.39), (b) the first level of the BFS model (r = 0.53), and
(c) the second level of the BFS model (r = 0.59).

good match to human judgments by preventing its
overgeneralization (see Figure 3).

The AB model can also take advantage of high
frequency verbs biasing the preference toward the
frequently observed association. However, the se-
mantic similarity across verbs within alternating or
non-alternating classes is less effective in this model.
The representation of the lexemes as 281 nodes
in the input (compared to less than a dozen other
nodes) make the learning more verb specific, reduc-
ing the ability of the model to generalize to the unat-
tested verbs.

Conclusions. The success of the BFS model,
and especially the results using both constructions
and classes, point to the role of probabilistic con-
structions and verb classes in generalizing exist-

103



ing knowledge while avoiding overgeneralizations.
Moreover, the use of a skewed distribution reveals
the role of the high verb-in-construction frequency
in guiding the association of construction and mean-
ing (see Ambridge et al., 2014, for discussion). Yet
both models would benefit from a richer seman-
tic representation that better captures the distinctive
properties of verbs across various constructions.

6 Discussion

This paper presents a comparative analysis of two
computational cognitive models on the sample task
of learning the dative alternation. This study en-
ables an evaluation of the psycholinguistic plausi-
bility of each model for the given task when facing
identical input and experimental settings. Adopting
the semantic representation of Ambridge and Bly-
thing (2015), our input incorporates both seman-
tic and syntactic properties over a large number of
verbs. By providing the first direct comparison be-
tween two existing models of this phenomenon, we
are the first to demonstrate the complex interac-
tion of various linguistic properties in the input, and
how rich learning mechanisms are required in or-
der to achieve generalizations compatible with hu-
man judgments in this area. Moreover, comparison
of learning mechanisms and of input properties can
inform CL/NLP more generally by shedding light on
potential factors in achieving humanlike behaviours.

We find that the Bayesian model of BFS signifi-
cantly correlates with human judgments on the 3 key
evaluation measures. Importantly, this model out-
performs the connectionist model of AB in the cor-
relation with the verb-bias score (the per-verb differ-
ence between DO and PD preference), which points
to its advantage in choosing the more appropriate
construction per verb. We argue that the fit of the
model relies on a rich learning mechanism that ex-
ploits distributional properties of naturalistic input.

The AB model has a streamlined design to sup-
port learning a particular semantic-syntactic associ-
ation underlying the dative alternation. While the
BFS model is richer computationally, its properties
were motivated in earlier work explaining many hu-
man behaviours. When we consider more natural in-
put, the simple input[semantics]–output[syntax] as-
sociation mechanism of the AB model is unable to

capture the necessary interactions among the verb
semantic properties, the syntactic usages, and their
patterns across different types of verbs. By con-
trast, the two-level design of the BFS model captures
these interactions. The first level learns the verb-
semantics-syntax associations as clusters of similar
configurations of those features. The second level
captures the commonalities of behaviour of sets of
verbs by forming classes of verbs that have simi-
lar distributional patterns over the first-level clus-
ters. We also observe that the replication of adult-
like language competence relies on several naturalis-
tic properties of the input: skewed distribution, and a
rich semantic representation combined with syntac-
tic information. The skewed input enables the for-
mation of clusters representing more entrenched as-
sociations, which are biased towards high-frequency
verbs associated with certain semantic and syntactic
features.

Given the role of these linguistic properties, the
results here call for additional analysis and develop-
ment of the input to computational cognitive models.
The predictions may be improved given more real-
istic syntactic and semantic information about the
verb usages. On the syntax side, the input should
reflect the distribution of verbs across more syntac-
tic constructions, as statistical patterns over such us-
ages can indirectly indicate aspects of a verb’s se-
mantics (cf. Barak et al., 2013a). In the future,
we aim to analyze the role of fuller syntactic dis-
tributions in restricting overgeneralization patterns.
Moreover, the semantic annotations used here repli-
cate the settings originally tested for the AB model,
which correspond to the verb as used in the rele-
vant constructions. This contrasts with typical au-
tomated extractions of verb-meaning representation
(e.g., word2vec, Mikolov et al., 2013), which cap-
ture a more general verb meaning across all its us-
ages. In preliminary experiments, we have found an
advantage in using word2vec representations in ad-
dition to the semantic properties reported here. We
aim to further analyze manual and automated meth-
ods for semantic feature extraction in future work.

Acknowledgments

We are grateful to Ben Ambridge for helpful discus-
sion of his model and for sharing his data with us.

104



References

Nameera Akhtar. 1999. Acquiring basic word order:
Evidence for data-driven learning of syntactic
structure. Journal of child language, 26(02):339–
356.

Afra Alishahi and Pirita Pyykkon̈en. 2011. The on-
set of syntactic bootstrapping in word learning:
Evidence from a computational study. In Pro-
ceedings of the 33st Annual Conference of the
Cognitive Science Society.

Afra Alishahi and Suzanne Stevenson. 2008. A com-
putational model of early argument structure ac-
quisition. Cognitive Science, 32(5):789–834.

Ben Ambridge and Ryan P Blything. 2015. A
connectionist model of the retreat from verb ar-
gument structure overgeneralization. Journal of
child language, pages 1–32.

Ben Ambridge, Julian M Pine, Caroline F Rowland,
and Franklin Chang. 2012. The roles of verb se-
mantics, entrenchment, and morphophonology in
the retreat from dative argument-structure over-
generalization errors. Language, 88(1):45–81.

Ben Ambridge, Julian M Pine, Caroline F Row-
land, Daniel Freudenthal, and Franklin Chang.
2014. Avoiding dative overgeneralisation errors:
Semantics, statistics or both? Language, Cogni-
tion and Neuroscience, 29(2):218–243.

Ben Ambridge, Julian M Pine, Caroline F Row-
land, and Chris R Young. 2008. The effect of
verb semantic class and verb frequency (entrench-
ment) on childrens and adults graded judgements
of argument-structure overgeneralization errors.
Cognition, 106(1):87–129.

Libby Barak, Afsaneh Fazly, and Suzanne Steven-
son. 2012. Modeling the acquisition of mental
state verbs. In Proceedings of the 3rd Workshop
on Cognitive Modeling and Computational Lin-
guistics (CMCL 2012).

Libby Barak, Afsaneh Fazly, and Suzanne Steven-
son. 2013a. Acquisition of desires before beliefs:
A computational investigation. In Proceedings of
CoNLL-2013.

Libby Barak, Afsaneh Fazly, and Suzanne Steven-
son. 2013b. Modeling the emergence of an exem-
plar verb in construction learning. In Proceedings

of the 35rd Annual Meeting of the Cognitive Sci-
ence Society.

Libby Barak, Afsaneh Fazly, and Suzanne Steven-
son. 2014. Learning verb classes in an incremen-
tal model. In Proceedings of the 5th Workshop
on Cognitive Modeling and Computational Lin-
guistics (CMCL 2014). Association for Computa-
tional Linguistics.

Arielle Borovsky and Jeff Elman. 2006. Language
input and semantic categories: A relation between
cognition and early word learning. Journal of
child language, 33(04):759–790.

Jeremy K Boyd and Adele E Goldberg. 2009. Input
effects within a constructionist framework. The
Modern Language Journal, 93(3):418–429.

Martin DS Braine and Patricia J Brooks. 1995. Verb
argument structure and the problem of avoid-
ing an overgeneral grammar. Beyond names for
things: Young children’s acquisition of verbs,
pages 353–376.

Joan Bresnan and Marilyn Ford. 2010. Predicting
syntax: Processing dative constructions in ameri-
can and australian varieties of english. Language,
86(1):168–213.

Joan Bybee. 2010. Language, usage and cognition.
Cambridge University Press.

David Casenhiser and Adele E. Goldberg. 2005. Fast
mapping between a phrasal form and meaning.
Developmental Science, 8(6):500–508.

Cynthia Fisher. 1999. From form to meaning: A
role for structural alignment in the acquisition of
language. Advances in child development and be-
havior, 27:1–53.

Marilyn Ford, Joan W Bresnan, and Ronald Kaplan.
1982. A competence-based theory of syntactic
closure. American Journal of Computational Lin-
guistics, 8(1):49.

Susanne Gahl and Susan M Garnsey. 2004. Knowl-
edge of grammar, knowledge of usage: Syntactic
probabilities affect pronunciation variation. Lan-
guage, pages 748–775.

Susan M Garnsey, Neal J Pearlmutter, Elizabeth My-
ers, and Melanie A Lotocky. 1997. The contri-
butions of verb bias and plausibility to the com-

105



prehension of temporarily ambiguous sentences.
Journal of Memory and Language, 37(1):58–93.

Adele E. Goldberg. 1995. Constructions, A Con-
struction Grammar Approach to Argument Struc-
ture. {Chicago University Press}.

Adele E Goldberg. 2006. Constructions at work:
The nature of generalization in language. Oxford
University Press on Demand.

Adele E Goldberg. 2011. Corpus evidence of the
viability of statistical preemption. Cognitive Lin-
guistics, 22(1):131–153.

Geoffrey Leech. 1992. 100 million words of english:
the british national corpus (BNC). Language Re-
search, 28(1):1–13.

Beth Levin. 1993. English verb classes and alterna-
tions: A preliminary investigation, volume 348.
University of Chicago press Chicago, IL.

Elena VM Lieven, Julian M Pine, and Gillian Bald-
win. 1997. Lexically-based learning and early
grammatical development. Journal of child lan-
guage, 24(01):187–219.

Beth L Losiewicz. 1992. The effect of frequency on
linguistic morphology. University of Texas.

Maryellen C MacDonald, Neal J Pearlmutter, and
Mark S Seidenberg. 1994. The lexical nature of
syntactic ambiguity resolution. Psychological re-
view, 101(4):676.

Yevgen Matusevych, Afra Alishahi, and Ad Backus.
2014. Isolating second language learning factors
in a computational study of bilingual construc-
tion acquisition. In Proceedings of the 36th An-
nual Conference of the Cognitive Science Society,
pages 988–994.

Yevgen Matusevych, Afra Alishahi, and Ad Backus.
2016. The impact of first and second language
exposure on learning second language construc-
tions. Bilingualism: Language and Cognition,
pages 1–22.

Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S
Corrado, and Jeff Dean. 2013. Distributed repre-
sentations of words and phrases and their com-
positionality. In C. J. C. Burges, L. Bottou,
M. Welling, Z. Ghahramani, and K. Q. Wein-
berger, editors, Advances in Neural Information

Processing Systems 26, pages 3111–3119. Curran
Associates, Inc.

Christopher Parisien and Suzanne Stevenson. 2010.
Learning verb alternations in a usage-based
bayesian model. In Proceedings of the 32nd an-
nual meeting of the Cognitive Science Society.

Amy Perfors, Joshua B. Tenenbaum, and Elizabeth
Wonnacott. 2010. Variability, negative evidence,
and the acquisition of verb argument construc-
tions. Journal of Child Language, 37(03):607–
642.

Steven Pinker. 1989. Learnability and cognition:
The acquisition of argument structure. The MIT
Press.

Nicolas Ruh and Gert Westermann. 2009. Oxlearn:
A new matlab-based simulation tool for con-
nectionist models. Behavior research methods,
41(4):1138–1143.

Anna L Theakston. 2004. The role of entrenchment
in childrens and adults performance on grammat-
icality judgment tasks. Cognitive Development,
19(1):15–34.

Michael Tomasello. 2003. Constructing a language:
A usage-based theory of language acquisition.
Harvard University Press.

John C Trueswell, Michael K Tanenhaus, and
Christopher Kello. 1993. Verb-specific con-
straints in sentence processing: separating effects
of lexical preference from garden-paths. Journal
of Experimental Psychology: Learning, Memory,
and Cognition, 19(3):528.

Thomas Wasow. 2002. Postverbal behavior. Stan-
ford Univ Center for the Study.

106


