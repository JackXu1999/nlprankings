



















































Optimizing Generative Dialog State Tracker via Cascading Gradient Descent


Proceedings of the SIGDIAL 2014 Conference, pages 273–281,
Philadelphia, U.S.A., 18-20 June 2014. c©2014 Association for Computational Linguistics

Optimizing Generative Dialog State Tracker
via Cascading Gradient Descent

Byung-Jun Lee1, Woosang Lim1, Daejoong Kim2, Kee-Eung Kim1
1 Department of Computer Science, KAIST 2 LG Electronics

Abstract

For robust spoken dialog management,
various dialog state tracking methods have
been proposed. Although discriminative
models are gaining popularity due to their
superior performance, generative models
based on the Partially Observable Markov
Decision Process model still remain at-
tractive since they provide an integrated
framework for dialog state tracking and
dialog policy optimization. Although a
straightforward way to fit a generative
model is to independently train the com-
ponent probability models, we present a
gradient descent algorithm that simultane-
ously train all the component models. We
show that the resulting tracker performs
competitively with other top-performing
trackers that participated in DSTC2.

1 Introduction

Spoken dialog systems, a field rapidly growing
with the spread of smart mobile devices, has to
deal with challenges to become a primary user in-
terface for natural interaction using conversations.
One of the challenges is to maintain the state of
the dialog in the conversational process, which is
called dialog state tracking. The dialog state en-
capsulates the information needed to successfully
finish the dialog, such as users’ goal or requests,
and thus it is an essential entity in spoken dia-
log systems. However, the error incurred by Au-
tomatic Speech Recognition (ASR) and Spoken
Language Understanding (SLU) makes the true
user utterance not directly observable, and this
makes it difficult to figure out the true dialog state.

Various methods have been used to construct
dialog state trackers. The traditional methods
used in most commercial systems use hand-crafted
rules that typically rely on the most likely result

from SLU. However, these rule-based systems are
prone to frequent errors as the most likely result
is not always correct. Hence, these systems of-
ten drive the users to respond using simple key-
words and to explicitly confirm everything they
say, which is far from a natural conversational in-
teraction. An accurate tracking of the dialog state
is crucial for natural and efficient dialogs. On the
other hand, modern methods take a statistical ap-
proach to calculate the posterior distribution over
the dialog states using multiple results from SLU
in order to overcome the error in the most likely
SLU result.

Statistical dialog state trackers can be catego-
rized into two approaches depending on how the
posterior calculation is modeled. The generative
approach uses the generative model that describes
how the SLU results are generated from the hidden
dialog state and uses the Bayes’ rule to calculate
the posterior. It has been a popular approach for
statistical dialog state tracking, since it naturally
fits into the Partially Observable Markov Decision
Process (POMDP) (Williams and Young, 2007),
an integrated model for dialog state tracking and
dialog strategy optimization. In the POMDP point
of view, the dialog state tracking is essentially be-
lief monitoring, which is the task of calculating
posterior distribution over the hidden state given
the history of observations. Examples of the dia-
log state trackers that take the generative approach
include (Young et al., 2010; Thomson and Young,
2010; Raux and Ma, 2011)

On the other hand, the discriminative approach
directly models the posterior distribution. Since
it avoids modeling of unnecessary aspects of the
task, it typically achieves a better tracking accu-
racy compared to the generative approach. Ex-
amples of discriminative dialog state trackers in-
clude (Lee, 2013; Metallinou et al., 2013). How-
ever, their feature functions often refer to past ob-
servations, and it remains yet to be seen whether

273



the discriminative approach can be successfully
incorporated into POMDP or reinforcement learn-
ing (RL) for dialog strategy optimization.

This paper is concerned with the generative ap-
proach to dialog state tracking. In our earlier
work (Kim et al., 2013), the optimization of the
tracker was carried out independently for each
component model (observation model, user action
model, and belief refinement model) that com-
prised our tracker. This was not exactly a proper
way to train the tracker for overall performance
optimization. In this paper, we present an opti-
mization method, which we call “cascading gra-
dient descent”, that trains component models si-
multaneously. We show that this approach yields
a dialog state tracker that performs on par with the
best ones that participated in the second Dialog
State Tracking Challenge (DSTC2).

The rest of the paper is organized as follows:
We briefly review the background of our work in
section 2, and present our method in section 3. We
then explain the DSTC2 dialog domain and the ex-
perimental settings in section 4, and discuss the re-
sults in section 5. Finally, we conclude the paper
with the summary and the suggestion for future
work in section 6.

2 Background and Related Work

The dialog state tracking is formalized as fol-
lows: In each turn of the dialog, the spoken dia-
log system executes system action a, and the user
with goal g responds to the system with utterance
u. The dialog state in each turn is defined s =
(u, g, h), where h is the dialog history encapsulat-
ing additional information needed for tracking the
dialog state (Williams et al., 2005). The SLU pro-
cesses the user utterance and generates the results
as an N -best list o = [〈ũ1, f1〉, . . . , 〈ũN , fN 〉],
where ũi is the hypothesized user utterance and
fi is its confidence score1. Without loss of gener-
ality, we assume that the last item in the N -best
list is the null item 〈∅, 1−∑N−1i=1 fi〉, representing
the set of unrecognized user utterances. The statis-
tical dialog state tracker maintains the probability
distribution over states, called the belief.

2.1 Discriminative Dialog State Tracking

Dialog state trackers taking the discriminative ap-
proach calculates the belief via trained conditional

1Here we assume that
∑N−1

i=1 fi ≤ 1, acting as a posterior
of N -best list.

models that represent the belief directly. Maxi-
mum Entropy is widely used for the discriminative
approach, which formulates the belief as follows:

b′(g) = P (g|x) = η · exp(wTφ(x)) (1)
where η is the normalizing constant, x =
(u1, a1, g1, . . . , ut, at, gt) is the history of user ac-
tions, system actions, and user goals up to the cur-
rent dialog turn t, φ(·) is the vector of feature
functions on x, and finally, w is the set of model
parameters to be learned from dialog data.

According to the formulation, the posterior
computation has to be carried out for all possible
user goals in order to obtain the normalizing con-
stant η. This is not feasible for real dialog domains
that have a large number of user goals (the DSTC2
dialog domain used on this paper has 371070 user
goals).

Consequently, it is important for the discrimina-
tive approach to reduce the size of the state space.
(Metallinou et al., 2013) adopts the idea behind the
HIS model and confines the set of possible goals
to those appeared in SLU results. (Lee, 2013) as-
sumed conditional independence between dialog
state components to address scalability, and used
conditional random field.

2.2 Generative Dialog State Tracking
In contrast, the generative approach to the dialog
state tracking calculates the belief using Bayes’
rule, with the belief from the last turn b as a
prior and the likelihood given the user utter-
ance hypotheses Pr(o|a, g, h). In the prior work
(Williams et al., 2005), the likelihood is factored
and some independence assumptions are made:

b′(g′, h′) = η
∑
u

Pr(o|u) Pr(u|g′, a) ·∑
h

Pr(h′|g′, u, h, a)
∑
g

Pr(g′|g, a)b(g, h)

(2)

where η is the normalizing constant and u is
marginalized out in the belief.

Scalability became the important issue, just as
in the generative approach. One way to reduce
the amount of computation is to group the states
into partitions, proposed as the Hidden Informa-
tion State (HIS) model (Young et al., 2010). Be-
ginning with one root partition with the probabil-
ity of 1, partitions are split when the distinction
is required by observations, i.e. a user utterance

274



hypothesis from SLU. This confines the possible
goal state to the values that have been appeared as
SLU hypotheses, and provides scalability without
a loss of accuracy when the coverage of N-best list
is large enough to include the true utterance. Us-
ing the HIS model with an additional assumption
that the user goal does not change (goal transition
from one to another is 0), the belief update equa-
tion (2) is reformulated as follows:

b′(ψ′, h′) = η
∑
u

Pr(o|u) Pr(u|ψ′, a) ·∑
h

Pr(h′|ψ′, u, h, a) Pr(ψ′|ψ)b(ψ, h)
(3)

where ψ is a set of user goals that share the same
belief. Each probability model in the above equa-
tion has a name: Pr(o|u) is called the observation
model, Pr(u|ψ′, a) is called the user action model,
Pr(ψ′|ψ) is called the belief refinement model, and
Pr(h′|ψ′, u, h, a) is called the history model.

In this paper, we used the last turn’s belief of di-
alog states as history state and preserved its depen-
dence in the observation model to improve perfor-
mance. With the changes, observation model can
distinguish user actions without their value. For
example, request alternative user action may have
the power of diminishing dominant partitions, and
it can only be learnt by the dependence with par-
tition confidence. The belief update formula used
in this paper becomes:

b′(ψ′) = η
∑
u

Pr(o|u, a, h) ·

Pr(u|ψ′, a) Pr(ψ′|ψ)b(ψ)
(4)

Other approaches to cope with the scalabil-
ity problem in dialog state tracking is to adopt
factorized dynamic Bayesian network by making
conditional independence assumptions among di-
alog state components, and use approximate in-
ference algorithms such as loopy belief propa-
gation (Thomson and Young, 2010) or blocked
Gibbs sampling (Raux and Ma, 2011).

3 Cascading Gradient Descent

Although equation (4) is an elegant formulation
of the dialog state tracking via Bayes rule, there
has not been an integrated learning algorithm that
simultaneously optimizes component probability
models, i.e. the observation, the user action, and
the belief refinement models. Our prior work (Kim

et al., 2013) relied on independently training each
component probability model, and then simply
plugging them into (4). Since the independent op-
timization of component probability models does
not lend itself to the optimization of overall dia-
log state tracking performance, we added an ex-
tra post-processing step called “belief transforma-
tion” in order to fine tune the results obtained
from equation (4). Unfortunately, this effort gen-
erally resulted in overfitting to the training data.
In this paper, we present an integrated learning al-
gorithm that simultaneously optimizes the compo-
nent probability models of the HIS model.

We start with defining an objective function
which measures the error of the dialog state track-
ing:

E =
T∑
t=1

∑
i

1
2
(b(ψti)− rti)2 (5)

where t is the dialog turn, i is the partition index,
rti is the binary label with value 1 if and only if
the partition ψti contains the true user goal. Note
that our objective function coincides with the `2
performance metrics used in DSTC.

We then express component probability models
as functions of features, which are parameterized
by sets of weights, and rewrite equation (4):

b(ψti) =η
t
∑

(ut,f t)∈ot
PrwO(u

t, f t, at, b(ψt−1i )) ·

PrwU(u
t|ψti , at) PrwR(ψti |ψt−1i )b(ψt−1i )

(6)

where wO, wU, and wR represent the set of pa-
rameters for the observation, the user action, and
the belief refinement models, respectively.

Our learning method is basically a gradient de-
scent. The gradient of E with respect to wO is
derived as follows:

∂E

∂wO
=

T∑
t=1

∑
i

(b(ψti)− rti)
∂b(ψti)
∂wO

By convenience, we define:

δti =
∑

(ut,f t)∈ot
PrwO(u

t, f t, at, b(ψt−1i )) ·

PrwU(u
t|ψti , at) PrwR(ψti |ψt−1i )b(ψt−1i )

=
∑

(ut,f t)∈ot
ptOp

t
Up

t
Rb(ψ

t−1
i )

ηt = (
∑
i

δti)
−1, b(ψti) = η

tδti

275



and then obtain:

∂b(ψti)
∂wO

=
∂δti
∂wO

· ηt + ∂η
t

∂wO
· δti

=
∂δti
∂wO

· ηt − b(ψti)
∑
i′

∂δti′

∂wO
· ηt,

where

∂δti
∂wO

=
(
b(ψt−1i )

∑
(ut,f t)∈ot

∂ptO
∂wO

ptUp
t
R

+
∂b(ψt−1i )
∂wO

∑
(ut,f t)∈ot

ptOp
t
Up

t
R

)
.

Gradients for the parameters of other com-
ponent probability models are derived similarly.
We call our algorithm cascading gradient descent
since the gradient ∂b(ψ

t
i)

∂w requires computation of

the gradient in the previous dialog turn ∂b(ψ
t−1
i )

∂w ,
hence reflecting the temporal impact of the param-
eter change in throughout the dialog turns.

Once we obtain the gradients, we update the pa-
rameters using the gradient descent

w′O = wO − α
[
∂E

∂wO

]
,

w′U = wU − α
[
∂E

∂wU

]
,

w′R = wR − α
[
∂E

∂wR

]
where α is the stepsize parameter. α is initially set
to 0.5 and decreased by multiplying 110 whenever
the overall cost function increases.

4 Dialog State Tracking in the
Restaurant Information Domain

This section describes the dialog domain used for
the evaluation of our dialog tracker and the com-
ponent probability models used for the domain.
An instruction on how to obtain the dataset and
a more detailed description on the dialog domain
can be found in the DSTC2 summary paper (Hen-
derson et al., 2014).

4.1 Task Description
We used the DSTC2 dialog domain in which
the user queries the database of local restaurants.
The dataset for the restaurant information domain
were originally collected using Amazon Mechani-
cal Turk. A usual dialog proceeds as follows: the

user specifies the constraints (e.g. type of food,
location, etc) or the name of restaurant he wants,
and the system offers the name of a restaurant that
qualifies the constraints. User then accepts the of-
fer, and requests for additional information about
accepted restaurant. The dialog ends when all the
information requested by the user is provided.

The dialog state tracker should thereby clar-
ify three types of information inside the state:
goal, method, and requested. The goal state is
composed of name, pricerange, area, and food
slots, which is the information of the constraints
that the user has. The method state represents
what method user is using to accomplish his goal,
whose value is one of the none, by constraints, by
alternatives, by name, or finished. Lastly, the re-
quested state represents the information currently
requested by the user, such as the address, phone
number, postal code, etc. In this paper, we restrict
ourselves to tracking the goal states only, but our
tracker can be easily extended to track others as
well.

The dialog state tracker updates the belief turn
by turn, receiving SLU N-best hypotheses each
with an SLU confidence score in every turn. De-
spite the large number of states a dialog can have,
in the most cases, the coverage of N-best hypothe-
ses is enough to limit the consideration of possible
goal state to values that has been observed in SLU
hypotheses. Consequently, the task of the dialog
state tracker is to generate a set of observed val-
ues and their confidence scores for each slot, with
the confidence score corresponding to the poste-
rior probability of the goal state being the true goal
state. The dialog state tracker also maintains a spe-
cial goal state, called None, which represents that
the true goal state has not been observed. Its poste-
rior probability is also computed together with the
observed goal states as a part of the belief update.
For the rest of this section, we describe the models
chosen for each component probabilities.

4.2 Observation Model
The observation model that describes the genera-
tion of SLU result for the user utterance is defined
as

Pr(o = 〈ut, f t〉|u, a, h) =
ηo PrwO(u

t, f t, at, b(ψt−1i ))

= ηo
1

1 + exp(−wTOφO(ut, f t, at, b(ψt−1i ))− bO)

276



user action feature : 34 × system action feature : 5 × type of feature : 3 = 510

Inform action : 12
[food, pricerange, name, area] offer or

× inform Bias tern
[not match, slot match, value match] (always 1)

% consistency check with system action canthelp or
canthelp.exception

Action with values : 8
[confirm, deny] expl-conf or Value of user confidence

× × impl-conf or × f t
[food, pricerange, name, area] request

Action without values : 14 select
[ack, affirm, bye, hello, negate, Value of last turn’s confidence

repeat, reqmore, reqalts, thankyou, confirm-domain or b(ψt−1i )
request, null, confirm, deny, inform] welcomemsg

Table 1: 510 features used in observation model are specified.

where φO(ut, f t, at, b(ψ
t−1
i )) is the vector of fea-

tures taken from the hypothesized user action ut,
its confidence score f t generated from SLU, sys-
tem action at, and the belief of partition we are
dealing with b(ψt−1i ) from history state. Normal-
ization constante ηo can be ignored since it is sub-
sumed by overall normalization constant η. Fea-
ture details are specified in table 1.

4.3 User Action Model

Similar to the observation model, the user action
model that predicts the user action given the pre-
vious system action and user goal is defined as

Pr(ut|ψti , at) = PrwU(ut|ψti , at)

=
exp(wTUφU(u

t, ψti , a
t))∑

u exp(w
T
UφU(u, ψ

t
i , a

t))

where φU(ut, ψti , a
t) ∈ {0, 1}322 is the vector of

features taken from the (hypothesized) user action
ut, system action at, and the partition being up-
dated ψti . Softmax function is used to normal-
ize over possible user actions. Feature details are
specified in table 2.

4.4 Belief Refinement Model

The belief refinement model predicts how the par-
tition of the user goal will evolve at the next dialog
turn. We defined it as a mixture of the empirical
distribution and the uniform distribution obtained

from the training data:

PrwR(ψ
t
i |ψt−1i )

=
1

1 + exp(−wR)
occurrence(ψti , ψ

t−1
i )

occurrence(ψt−1i )

+
(

1− 1
1 + exp(−wR)

) |ψti |
|ψt−1i |

where occurrence(ψti , ψ
t−1
i ) is the number of

consecutive dialog turns in the training data with
user goals being consistent with ψt−1i in the
previous turn and ψti in the current turn, and
occurrence(ψt−1i ) is defined similarly for a single
turn only. The ratio of the two, which corresponds
to the partition split probability used in (Young et
al., 2010), is the first term in the mixture. On the
other hand, if we use this empirical distribution
only, we cannot deal with novel user goals that do
not appear in the training data. Assuming that user
goals are generated from the uniform distribution,
the probability that the user goal is in a partition
ψ is |ψ|N where |ψ| is the number of user goals in
the partition ψ, and N is the total number of user
goals. The probability that ψti gets split from ψ

t−1
i

is then |ψ
t
i |

|ψt−1i |
. Hence, we mix the two probabilities

for the resulting model.

The mixture weight is the only parameter of the
belief refinement model, which is learned as a part
of the cascading gradient descent. Note that we
use the sigmoid function in order to make the op-
timization unconstrained.

277



user action feature : 35 × system action feature : 8 + remaining actions: 42 = 322

Inform action : 24 Confirm/deny action : 16
[food, pricerange, name, area] [confirm, deny]

× [offer or inform, ×
[not match, slot match, value match] canthelp or [food, pricerange, name, area]

% consistency check canthelp.exception, ×
with system action expl-conf or [not match, match]

× impl-conf or % consistency check
[not match, match] request, with partition

% consistency check × select] +
with partition × Remaining system action : 26

[not match, match] [confirm-domain or welcomemsg]
Action without values : 11 % consistency check ×

[ack, affirm, bye, hello, negate, with partition [24 inform actions, null, others]
repeat, reqalts, reqmore, % corresponding user actions
thankyou, request, null]

Table 2: 322 features used in user action model are specified.

5 Experimental Details

5.1 Datasets
The restaurant information domain used in
DSTC2 is arranged into three datasets: train, dev,
test. The first two datasets are labeled with the true
user goals and user actions to optimize the dialog
state tracker before submission. The half of the di-
alogs are created with artificially degraded speech
recognizers, intended to better distinguish the per-
formances of trackers. Details of each dataset are
as below:

• dstc2 train: Composed of 1612 dialogs of
11405 turns, produced from two different
dialog managers with a hand-crafted dialog
policy.

• dstc2 dev: Composed of 506 dialogs of
3836 turns, produced from the dialog man-
agers used in dstc2 train set. Most of dialog
state trackers show lower performance on this
dataset than others.

• dstc2 test: Composed of 1117 dialogs of
9689 turns, produced from the dialog policy
trained by reinforcement learning, which is
not used for the train and dev datasets.

We used both train and dev sets as the training
data, as if they were one big dataset. Although
the true labels for the test dataset were made pub-
lic after the challenge, we did not use these labels
in any way for optimizing our tracker.

5.2 Pre-training
One of the drawbacks in using gradient descent
is convergence to a local optimum. We also ob-

served this phenomena during the training of our
dialog state tracker via cascading gradient descent.
Randomized initialization of parameters is a com-
mon practice for gradient descent, but given the
high-dimensionality of the parameter space, the
randomized initialization had a limited effect in
converging to a sufficiently good local optimum.

We adopted a pre-training phase where the pa-
rameters of each component model are optimized
individually. Once the pre-training is done for
each component model, we gathered the param-
eter values and took them as the initial parameter
value for the cascading gradient descent. This pre-
training phase helped tremendously converging to
a good local optimum, and reduced the number of
iterations as well. We pre-trained the parameters
of each component model as follows:

• Observation Model: True user action labels
in the training set are used as targets for the
observation model. For every user action hy-
pothesis in theN -best list, set the target value
to 1 if the user action hypothesis is the true
user action, and 0 otherwise. A simple gradi-
ent descent was used for pre-training.

• User Action Model: Although the user ac-
tion and the system action labels are avail-
able, the partition of the user goals is not
readily available. However, the latter can be
easily obtained by running an unoptimized
tracker. Thus, using the labels in the train-
ing set and the generated partitions, we set
the target value to 1 if the user action hypoth-
esis is the true user action and the partition is
consistent with the true user action, and 0 oth-

278



(a) Evaluation on accuracy metric (higher is better)

(b) Evaluation on L2 metric (lower is better) (c) Evaluation on ROCV 2,ca05 metric (higher is better)

Figure 1: The overall results of proposed method. Each figure shows the evaluations over dstc2 test
dataset by featured metrics (joint accuracy, joint l2, joint roc.v2) in DSTC2.

erwise. A simple gradient descent was also
used for pre-training.

• Belief Refinement Model: Since there is
only a single parameter for this model, we did
not perform pre-training.

5.3 Results and Discussion
Table 3 shows the test set score of tracker im-
plemented based on proposed algorithm, with the
score of other trackers submitted to DSTC2. We
tried 200 random initialised weights to train model
with proposed algorithm, and learned model with
the lowest training L2 error is picked to show
the result on the test set. Because we only used
live SLU and past data to track dialog state, other
tracker results with the same condition are selected
to compare with our tracker.

The implementation of our algorithm was not
ready until the DSTC2 deadline. We participated
as Team 8 using the old optimization method
in (Kim et al., 2013). As shown in the table 3,
the new algorithm shows a substantial improve-
ment, achieving almost 15% decrease in the L2
error. Since both trackers are fine-tuned, this im-
provement seems to be originated from the new
optimization algorithm.

For all three featured metrics used to evalu-
ate, tracker constructed with our proposed method
shows competitive performance. The key to excel
baseline tracker was to discover the relation be-
tween user action and system action. For exam-
ple, user actions that tell about the same slot sys-
tem was talking about but giving different value
are usually correcting wrong recognitions so far,
which should significantly reduce the belief over
state the system was tracking.

Due to the objective function that is designed to
optimize L2 error, our tracker shows better perfor-
mance at L2 error than the other metrics. For both
all goal metric and joint goal metric, our tracker
shows low L2 error when compared to other track-
ers while the rank of accuracy metric is not so
high. When the fact that our method as a genera-
tive state tracker benefits from the ability to be eas-
ily incorporated into POMDP framework is con-
sidered, only similar performance to other trackers
is satisfactory.

6 Conclusion

In this paper, we propose a simple method that
optimizes overall parameters of generative state
tracker using ”Cascading Gradient Descent” al-

279



All goal

Team 0 1 3 4 6 7 8 9 Ours
Entry 1 2 0 0 3 0 1 2 4 1 0

Accuracy 0.886 0.88 0.837 0.892 0.895 0.884 0.882 0.885 0.894 0.873 0.77 0.886
AvgP 0.865 0.852 0.778 0.856 0.853 0.789 0.833 0.843 0.862 0.827 0.782 0.846
L2 0.192 0.198 0.289 0.189 0.17 0.197 0.189 0.184 0.179 0.227 0.358 0.186
MRR 0.918 0.914 0.87 0.911 0.927 0.917 0.916 0.918 0.922 0.904 0.833 0.918
ROCV 1,ca05 0.777 0.767 0.0 0.778 0.842 0.773 0.786 0.809 0.806 0.635 0.0 0.805
ROCV 1,eer 0.139 0.133 0.0 0.119 0.103 0.135 0.123 0.116 0.116 0.163 0.219 0.120
ROCV 2,ca05 0.0 0.0 0.0 0.0 0.3 0.27 0.417 0.384 0.154 0.0 0.0 0.197
UpdateAcc 0.886 0.881 0.837 0.891 0.895 0.882 0.88 0.883 0.894 0.873 0.769 0.886
UpdatePrec 0.898 0.897 0.846 0.904 0.907 0.898 0.895 0.897 0.903 0.886 0.804 0.896

Table 3: Test set scores averaged over all goal slots of our proposed algorithm and other trackers are
presented. The goal slots are composed of food, pricerange, name and area.

Joint goal

Team 0 1 3 4 6 7 8 9 Ours
Entry 1 2 0 0 3 0 1 2 4 1 0

Accuracy 0.719 0.711 0.601 0.729 0.737 0.713 0.707 0.718 0.735 0.699 0.499 0.726
AvgP 0.678 0.66 0.503 0.659 0.636 0.54 0.619 0.638 0.673 0.583 0.522 0.658
L2 0.464 0.466 0.649 0.452 0.406 0.461 0.447 0.437 0.433 0.498 0.76 0.427
MRR 0.779 0.757 0.661 0.763 0.804 0.767 0.765 0.772 0.787 0.749 0.608 0.775
ROCV 1,ca05 0.332 0.316 0.096 0.32 0.461 0.324 0.395 0.432 0.349 0.22 0.0 0.438
ROCV 1,eer 0.256 0.254 0.382 0.249 0.208 0.281 0.241 0.226 0.243 0.299 0.313 0.218
ROCV 2,ca05 0.0 0.0 0.064 0.0 0.321 0.1 0.223 0.207 0.086 0.067 0.0 0.135
UpdateAcc 0.489 0.487 0.37 0.495 0.507 0.473 0.466 0.476 0.514 0.459 0.325 0.488
UpdatePrec 0.729 0.694 0.677 0.759 0.726 0.748 0.743 0.743 0.703 0.692 0.54 0.71

Table 4: Test set scores of joint goal slot of our proposed algorithm and other trackers are presented. The
joint goal slot is a slot that is treated as correct when every goal slot is correct.

gorithm. Using proposed method on Hidden In-
formation State model, we construct a tracker
that performs competitively with DSTC2 par-
ticipants, who mostly adopt discriminative ap-
proaches. Since generative approach has much
more potential to be extended to more com-
plex models or toward different domains such as
DSTC3, our tracker has the advantage over the
other trackers.

Hidden Information State (HIS) model with
cascading gradient descent has far more steps of
improvement remaining. Although history state
in current paper only includes previous partition
belief due to implementation convenience, utiliz-
ing additional history state is the key to improve
performance even more. History state can in-
clude any information depending on how we de-
fine the state. The reason why the discriminative
state tracking methods generally show good per-
formance in terms of accuracy is rich set of poten-
tially informative features, which can be employed
by the history state.

In addition to the future improvements with his-

tory state, we can consider improving each prob-
ability models. In this paper, probability mod-
els are modeled with sigmoid function or soft-
max function over weighted features, which is in
other words a neural network with no hidden layer.
The model used in this paper can naturally devel-
oped by adding hidden layers, and ultimately deep
learning techniques could be applicable. Apply-
ing deep learning techniques could help the his-
tory state to find out influential hidden features to
employ.

Acknowledgments

This work was supported by the IT R&D program
of MKE/KEIT. [10041678, The Original Technol-
ogy Development of Interactive Intelligent Per-
sonal Assistant Software for the Information Ser-
vice on multiple domains]

References
Matthew Henderson, Blaise Thomson, and Jason

Williams. 2014. The second dialog state tracking

280



challenge. In Proceedings of the SIGdial 2014 Con-
ference, Baltimore, U.S.A., June.

Daejoong Kim, Jaedeug Choi, Kee-Eung Kim, Jungsu
Lee, and Jinho Sohn. 2013. A specific analysis of
a dialog state tracker in a challenge. In Proceedings
of the SIGDIAL 2013 Conference, pages 462–466.

Sungjin Lee. 2013. Structured discriminative model
for dialog state tracking. Proceedings of the SIG-
DIAL 2013 Conference, pages 442–451.

Angeliki Metallinou, Dan Bohus, and Jason D
Williams. 2013. Discriminative state tracking for
spoken dialog systems. In Proceedings of the 51st
Annual Meeting of the Association for Computa-
tional Linguastics, pages 466–475.

Antoine Raux and Yi Ma. 2011. Efficient probabilistic
tracking of user goal and dialog history for spoken
dialog systems. In INTERSPEECH, pages 801–804.

Blaise Thomson and Steve Young. 2010. Bayesian
update of dialogue state: A pomdp framework for
spoken dialogue systems. Computer Speech & Lan-
guage, 24(4):562–588.

Jason D Williams and Steve Young. 2007. Partially
observable markov decision processes for spoken
dialog systems. Computer Speech & Language,
21(2):393–422.

Jason D Williams, Pascal Poupart, and Steve Young.
2005. Factored partially observable markov deci-
sion processes for dialogue management. In 4th
Workshop on Knowledge and Reasoning in Practi-
cal Dialog Systems, International Joint Conference
on Artificial Intelligence (IJCAI), pages 76–82.

Steve Young, Milica Gašić, Simon Keizer, François
Mairesse, Jost Schatzmann, Blaise Thomson, and
Kai Yu. 2010. The hidden information state model:
A practical framework for pomdp-based spoken dia-
logue management. Computer Speech & Language,
24(2):150–174.

281


