



















































Leveraging Sentence-level Information with Encoder LSTM for Semantic Slot Filling


Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 2077–2083,
Austin, Texas, November 1-5, 2016. c©2016 Association for Computational Linguistics

Leveraging Sentence-level Information with Encoder LSTM
for Semantic Slot Filling

Gakuto Kurata
IBM Research

gakuto@jp.ibm.com

Bing Xiang
IBM Watson

bingxia@us.ibm.com

Bowen Zhou
IBM Watson

zhou@us.ibm.com

Mo Yu
IBM Watson

yum@us.ibm.com

Abstract

Recurrent Neural Network (RNN) and one
of its specific architectures, Long Short-Term
Memory (LSTM), have been widely used for
sequence labeling. Explicitly modeling out-
put label dependencies on top of RNN/LSTM
is a widely-studied and effective extension.
We propose another extension to incorpo-
rate the global information spanning over
the whole input sequence. The proposed
method, encoder-labeler LSTM, first encodes
the whole input sequence into a fixed length
vector with the encoder LSTM, and then uses
this encoded vector as the initial state of an-
other LSTM for sequence labeling. With this
method, we can predict the label sequence
while taking the whole input sequence in-
formation into consideration. In the experi-
ments of a slot filling task, which is an es-
sential component of natural language under-
standing, with using the standard ATIS cor-
pus, we achieved the state-of-the-art F1-score
of 95.66%.

1 Introduction

Natural language understanding (NLU) is an essen-
tial component of natural human computer interac-
tion and typically consists of identifying the intent of
the users (intent classification) and extracting the as-
sociated semantic slots (slot filling) (De Mori et al.,
2008). We focus on the latter semantic slot filling
task in this paper.

Slot filling can be framed as a sequential label-
ing problem in which the most probable semantic
slot labels are estimated for each word of the given

word sequence. Slot filling is a traditional task and
tremendous efforts have been done, especially since
the 1980s when the Defense Advanced Research
Program Agency (DARPA) Airline Travel Informa-
tion System (ATIS) projects started (Price, 1990).
Following the success of deep learning (Hinton et
al., 2006; Bengio, 2009), Recurrent Neural Net-
work (RNN) (Elman, 1990; Jordan, 1997) and one
of its specific architectures, Long Short-Term Mem-
ory (LSTM) (Hochreiter and Schmidhuber, 1997),
have been widely used since they can capture tem-
poral dependencies (Yao et al., 2013; Yao et al.,
2014a; Mesnil et al., 2015). The RNN/LSTM-based
slot filling has been extended to be combined with
explicit modeling of label dependencies (Yao et al.,
2014b; Liu and Lane, 2015).

In this paper, we extend the LSTM-based slot
filling to consider sentence-level information. In
the field of machine translation, an encoder-decoder
LSTM has been gaining attention (Sutskever et al.,
2014), where the encoder LSTM encodes the global
information spanning over the whole input sentence
in its last hidden state. Inspired by this idea, we pro-
pose an encoder-labeler LSTM that leverages the en-
coder LSTM for slot filling. First, we encode the in-
put sentence into a fixed length vector by the encoder
LSTM. Then, we predict the slot label sequence by
the labeler LSTM whose hidden state is initialized
with the encoded vector by the encoder LSTM. With
this encoder-labeler LSTM, we can predict the la-
bel sequence while taking the sentence-level infor-
mation into consideration.

The main contributions of this paper are two-
folds:

2077



1. Proposed an encoder-labeler LSTM to leverage
sentence-level information for slot filling.

2. Achieved the state-of-the-art F1-score of
95.66% in the slot filling task of the standard
ATIS corpus.

2 Proposed Method

We first revisit the LSTM for slot filling and enhance
this to explicitly model label dependencies. Then we
explain the proposed encoder-labeler LSTM.

2.1 LSTM for Slot Filling

Figure 1(a) shows a typical LSTM for slot filling and
we call this as labeler LSTM(W) where words are fed
to the LSTM (Yao et al., 2014a).

Slot filling is a sequential labeling task to map a
sequence of T words xT1 to a sequence of T slot
labels yT1 . Each word xt is represented with a V
dimensional one-hot-vector where V is the vocabu-
lary size and is transferred to de dimensional con-
tinuous space by the word embedding matrix E ∈
Rde×V as Ext. Instead of simply feeding Ext into
the LSTM, Context Window is a widely used tech-
nique to jointly consider k preceding and succeeding
words as Ext+kt−k ∈ Rde(2k+1). The LSTM has the
architecture based on Jozefowicz et al. (2015) that
does not have peephole connections and yields the
hidden state sequence hT1 . For each time step t, the
posterior probabilities for each slot label are calcu-
lated by the softmax layer over the hidden state ht.
The word embedding matrix E, LSTM parameters,
and softmax layer parameters are estimated to mini-
mize the negative log likelihood over the correct la-
bel sequences with Back-Propagation Through Time
(BPTT) (Williams and Peng, 1990).

2.2 Explicit Modeling of Label Dependency

A shortcoming of the labeler LSTM(W) is that it
does not consider label dependencies. To explic-
itly model label dependencies, we introduce a new
architecture, labeler LSTM (W+L), as shown in Fig-
ure 1(b), where the output label of previous time step
is fed to the hidden state of current time step, jointly
with words, as Mesnil et al. (2015) and Liu and Lane
(2015) tried with RNN. For model training, one-hot-
vector of ground truth label of previous time step is

fed to the hidden state of current time step and for
evaluation, left-to-right beam search is used.

2.3 Encoder-labeler LSTM for Slot Filling

We propose two types of the encoder-labeler LSTM
that uses the labeler LSTM(W) and the labeler
LSTM(W+L). Figure 1(d) shows the encoder-
labeler LSTM(W). The encoder LSTM, to the left
of the dotted line, reads through the input sentence
backward. Its last hidden state contains the en-
coded information of the input sentence. The la-
beler LSTM(W), to the right of the dotted line, is
the same with the labeler LSTM(W) explained in
Section 2.1, except that its hidden state is initialized
with the last hidden state of the encoder LSTM. The
labeler LSTM(W) predicts the slot label conditioned
on the encoded information by the encoder LSTM,
which means that slot filling is conducted with tak-
ing sentence-level information into consideration.
Figure 1(e) shows the encoder-labeler LSTM(W+L),
which uses the labeler LSTM(W+L) and predicts
the slot label considering sentence-level information
and label dependencies jointly.

Model training is basically the same as with the
baseline labeler LSTM(W), as shown in Section 2.1,
except that the error in the labeler LSTM is propa-
gated to the encoder LSTM with BPTT.

This encoder-labeler LSTM is motivated by the
encoder-decoder LSTM that has been applied to ma-
chine translation (Sutskever et al., 2014), grapheme-
to-phoneme conversion (Yao and Zweig, 2015), text
summarization (Nallapati et al., 2016) and so on.
The difference is that the proposed encoder-labeler
LSTM accepts the same input sequence twice while
the usual encoder-decoder LSTM accepts the in-
put sequence once in the encoder. Note that the
LSTMs for encoding and labeling are different in the
encoder-labeler LSTM, but the same word embed-
ding matrix is used both for the encoder and labeler
since the same input sequence is fed twice.

2.4 Related Work on Considering
Sentence-level Information

Bi-directional RNN/LSTM have been proposed to
capture sentence-level information (Mesnil et al.,
2015; Zhou and Xu, 2015; Vu et al., 2016). While
the bi-directional RNN/LSTM model the preced-
ing and succeeding contexts at a specific word and

2078



O O O O

toneed a SeattleticketI

O B-ToCity

(a) Labeler LSTM(W).

O O O O

OO O OO<B>

O B-ToCity

toneed a SeattleticketI

(b) Labeler LSTM(W+L).
Encoder (backward) LSTM Decoder LSTM

to needa I

O O O O

Seattle ticket OO O OO<B>

O B-ToCity

(c) Encoder-decoder LSTM.

to needa I

O O O O

Seattle ticket toneed a SeattleticketI

O B-ToCity

Encoder LSTM (backward) Labeler LSTM(W)

(d) Encoder-labeler LSTM(W).

to needa I

O O O O

Seattle ticket OO O OO<B>

O B-ToCity

toneed a SeattleticketI

Encoder LSTM (backward) Labeler LSTM(W+L)

(e) Encoder-labeler LSTM(W+L).

Figure 1: Neural network architectures for slot filling. Input sentence is “I need a ticket to Seattle”. “B-ToCity” is slot label for
specific meaning and “O”is slot label without specific meaning. “<B>” is beginning symbol for slot sequence.

Sentence

Slots

show

O

flights

O

from

O

Boston

B-FromCity

to

O

New

B-ToCity

York

I-ToCity

today

B-Date

Figure 2: Example of ATIS sentence and annotated slots.

don’t explicitly encode the whole sentence, our
proposed encoder-labeler LSTM explicitly encodes
whole sentence and predicts slots conditioned on the
encoded information.

Another method to consider the sentence-level in-
formation for slot filling is the attention-based ap-
proach (Simonnet et al., 2015). The attention-based
approach is novel in aligning two sequences of dif-
ferent length. However, in the slot filling task where
the input and output sequences have the same length
and the input word and the output label has strong
relations, the effect of introducing “soft” attention
might become smaller. Instead, we directly fed the
input word into the labeler part with using context
window method as explained in Section 2.3.

3 Experiments

We report two sets of experiments. First we use the
standard ATIS corpus to confirm the improvement
by the proposed encoder-labeler LSTM and com-
pare our results with the published results while dis-
cussing the related works. Then we use a large-scale
data set to confirm the effect of the proposed method
in a realistic use-case.

3.1 ATIS Experiment
3.1.1 Experimental Setup

We used the ATIS corpus, which has been widely
used as the benchmark for NLU (Price, 1990; Dahl
et al., 1994; Wang et al., 2006; Tur et al., 2010).
Figure 2 shows an example sentence and its seman-

tic slot labels in In-Out-Begin (IOB) representation.
The slot filling task was to predict the slot label se-
quences from input word sequences.

The performance was measured by the F1-score:
F1 =

2×Precision×Recall
Precision+Recall , where precision is the ra-

tio of the correct labels in the system’s output and
recall is the ratio of the correct labels in the ground
truth of the evaluation data (van Rijsbergen, 1979).

The ATIS corpus contains the training data of
4,978 sentences and evaluation data of 893 sen-
tences. The unique number of slot labels is 127 and
the vocabulary size is 572. In the following exper-
iments, we randomly selected 80% of the original
training data to train the model and used the remain-
ing 20% as the heldout data (Mesnil et al., 2015).
We reported the F1-score on the evaluation data with
hyper-parameters that achieved the best F1-score on
the heldout data.

For training, we randomly initialized parame-
ters in accordance with the normalized initializa-
tion (Glorot and Bengio, 2010). We used ADAM
for learning rate control (Kingma and Ba, 2014) and
dropout for generalization with a dropout rate of
0.5 (Srivastava et al., 2014; Zaremba et al., 2014).

3.1.2 Improvement by Encoder-labeler LSTM
We conducted experiments to compare the labeler

LSTM(W) (Section 2.1), the labeler LSTM(W+L)
(Section 2.2), and the encoder-labeler LSTM (Sec-
tion 2.3). As for yet another baseline, we tried the
encoder-decoder LSTM as shown in Figure 1(c)1.

For all architectures, we set the initial learn-
ing rate to 0.001 (Kingma and Ba, 2014) and

1Length of the output label sequence is equal to that of the
input word sequence in a slot filling task. Therefore, ending
symbol for slot sequence is not necessary.

2079



the dimension of word embeddings to de = 30.
We changed the number of hidden units in the
LSTM, dh ∈ {100, 200, 300}2, and the size of
the context window, k ∈ {0, 1, 2}3. We used
backward encoding for the encoder-decoder LSTM
and the encoder-labeler LSTM as suggested in
Sutskever et al. (2014). For the encoder-decoder
LSTM, labeler LSTM(W+L), and encoder-labeler
LSTM(W+L), we used the left-to-right beam search
decoder (Sutskever et al., 2014) with beam sizes of
1, 2, 4, and 8 for evaluation where the best F1-score
was reported. During 100 training epochs, we re-
ported the F1-score on the evaluation data with the
epoch when the F1-score for the heldout data was
maximized. Table 1 shows the results.

The proposed encoder-labeler LSTM(W) and
encoder-labeler LSTM(W+L) both outperformed
the labeler LSTM(W) and labeler LSTM(W+L),
which confirms the novelty of considering sentence-
level information with the encoder LSTM by our
proposed method.

Contrary to expectations, F1-score by the
encoder-labeler LSTM(W+L) was not improved
from that by the encoder-labeler LSTM(W). A pos-
sible reason for this is the propagation of label pre-
diction errors. We compared the label prediction ac-
curacy for the words after the first label prediction
error in the evaluation sentences and confirmed that
the accuracy deteriorated from 84.0% to 82.6% by
using pthe label dependencies.

For the encoder-labeler LSTM(W) which was bet-
ter than the encoder-labeler LSTM(W+L), we tried
the deep architecture of 2 LSTM layers (Encoder-
labeler deep LSTM(W)). We also trained the cor-
responding labeler deep LSTM(W). As in Table 1,
we obtained improvement from 94.91% to 95.47%
by the proposed encoder-labeler deep LSTM(W),
which was statistically significant at the 90% level.

Lastly, F1-score by the encoder-decoder LSTM
was worse than other methods as shown in the first
row of Table 1. Since the slot label is closely related
with the input word, the encoder-decoder LSTM was
not an appropriate approach for the slot filling task.

2When using deep architecture later in this section, dh was
tuned for each layer.

3In our preliminary experiments with using the labeler
LSTM(W), F1-scores deteriorated with k ≥ 3.

F1-score
(c) Encoder-decoder LSTM 80.11
(a) Labeler LSTM(W) 94.80
(d) Encoder-labeler LSTM(W) 95.29
(b) Labeler LSTM(W+L) 94.91
(e) Encoder-labeler LSTM(W+L) 95.19

Labeler Deep LSTM(W) 94.91
Encoder-labeler Deep LSTM(W) 95.47

Table 1: Experimental results on ATIS slot filling task. Left-
most column corresponds to Figure 1. Lines with bold fonts

use proposed encoder-labeler LSTM. [%]

3.1.3 Comparison with Published Results
Table 2 summarizes the recently published results

on the ATIS slot filling task and compares them with
the results from the proposed methods.

Recent research has been focusing on RNN and
its extensions. Yao et al. (2013) used RNN and out-
performed methods that did not use neural networks,
such as SVM (Raymond and Riccardi, 2007) and
CRF (Deng et al., 2012). Mesnil et al. (2015) tried
bi-directional RNN, but reported degradation com-
paring with their single-directional RNN (94.98%).
Yao et al. (2014a) introduced LSTM and deep
LSTM and obtained improvement over RNN. Peng
and Yao (2015) proposed RNN-EM that used an ex-
ternal memory architecture to improve the memory
capability of RNN.

Many studies have been also conducted to explic-
itly model label dependencies. Xu and Sarikaya
(2013) proposed CNN-CRF that explicitly models
the dependencies of the output from CNN. Mesnil et
al. (2015) used hybrid RNN that combined Elman-
type and Jordan-type RNNs. Liu and Lane (2015)
used the output label for the previous word to model
label dependencies (RNN-SOP).

Vu et al. (2016) recently proposed to use ranking
loss function over bi-directional RNNs with achiev-
ing 95.47% (R-biRNN) and reported 95.56% by en-
semble (5×R-biRNN).

By comparing with these methods, the main dif-
ference of our proposed encoder-labeler LSTM is
the use of encoder LSTM to leverage sentence-level
information 4.

4Since Simonnet et al. (2015) did not report the experimen-
tal results on ATIS, we could not experimentally compare our
result with their attention-based approach. Theoretical compar-
ison is available in Section 2.4.

2080



F1-score
RNN (Yao et al., 2013) 94.11
CNN-CRF (Xu and Sarikaya, 2013) 94.35
Bi-directional RNN (Mesnil et al., 2015) 94.73
LSTM (Yao et al., 2014a) 94.85
RNN-SOP (Liu and Lane, 2015) 94.89
Hybrid RNN (Mesnil et al., 2015) 95.06
Deep LSTM (Yao et al., 2014a) 95.08
RNN-EM (Peng and Yao, 2015) 95.25
R-biRNN (Vu et al., 2016) 95.47
5×R-biRNN (Vu et al., 2016) 95.56
Encoder-labeler LSTM(W) 95.40
Encoder-labeler Deep LSTM(W) 95.66
Table 2: Comparison with published results on ATIS slot filling
task. F1-scores by proposed method are improved from Table 1

due to sophisticated hyper-parameters. [%]

For our encoder-labeler LSTM(W) and encoder-
labeler deep LSTM(W), we further conducted
hyper-parameter search with a random search strat-
egy (Bergstra and Bengio, 2012). We tuned the di-
mension of word embeddings, de ∈ {30, 50, 75},
number of hidden states in each layer, dh ∈
{100, 150, 200, 250, 300}, size of context window,
k ∈ {0, 1, 2}, and initial learning rate sampled from
uniform distribution in range [0.0001, 0.01]. To the
best of our knowledge, the previously published
best F1-score was 95.56%5 (Vu et al., 2016). Our
encoder-labeler deep LSTM(W) achieved 95.66%
F1-score, outperforming the previously published
F1-score as shown in Table 2.

Note some of the previous results used whole
training data for model training while others used
randomly selected 80% of data for model training
and the remaining 20% for hyper-parameter tuning.
Our results are based on the latter setup.

3.2 Large-scale Experiment

We prepared a large-scale data set by merging
the MIT Restaurant Corpus and MIT Movie Cor-

5There are other published results that achieved better F1-
scores by using other information on top of word features.
Vukotic et al. (2015) achieved 96.16% F1-score by using the
named entity (NE) database when estimating word embeddings.
Yao et al. (2013) and Yao et al. (2014a) used NE features in ad-
dition to word features and obtained improvement with both the
RNN and LSTM upto 96.60% F1-score. Mesnil et al. (2015)
also used NE features and reported F1-score of 96.29% with
RNN and 96.46% with Recurrent CRF.

pus (Liu et al., 2013a; Liu et al., 2013b; Spoken
Laungage Systems Group, 2013) with the ATIS cor-
pus. Since users of the NLU system may pro-
vide queries without explicitly specifying their do-
main, building one NLU model for multiple do-
mains is necessary. The merged data set contains
30,229 training and 6,810 evaluation sentences. The
unique number of slot labels is 191 and the vocab-
ulary size is 16,049. With this merged data set, we
compared the labeler LSTM(W) and the proposed
encoder-labeler LSTM(W) according to the exper-
imental procedure explained in Section 3.1.2. The
labeler LSTM(W) achieved the F1-score of 72.80%
and the encoder-labeler LSTM(W) improved it to
74.41%, which confirmed the effect of the proposed
method in large and realistic data set 6.

4 Conclusion

We proposed an encoder-labeler LSTM that can
conduct slot filling conditioned on the encoded
sentence-level information. We applied this method
to the standard ATIS corpus and obtained the state-
of-the-art F1-score in a slot filling task. We also
tried to explicitly model label dependencies, but it
was not beneficial in our experiments, which should
be further investigated in our future work.

In this paper, we focused on the slot labeling in
this paper. Previous papers reported that jointly
training the models for slot filling and intent classi-
fication boosted the accuracy of both tasks (Xu and
Sarikaya, 2013; Shi et al., 2015; Liu et al., 2015).
Leveraging our encoder-labeler LSTM approach in
joint training should be worth trying.

Acknowledgments

We are grateful to Dr. Yuta Tsuboi, Dr. Ryuki
Tachibana, and Mr. Nobuyasu Itoh of IBM Re-
search - Tokyo for the fruitful discussion and their
comments on this and earlier versions of the paper.
We thank Dr. Ramesh M. Nallapati and Dr. Cicero
Nogueira dos Santos of IBM Watson for their valu-
able suggestions. We thank the anonymous review-
ers for their valuable comments.

6The purpose of this experiment is to confirm the effect of
the proposed method. The absolute F1-scores can not be com-
pared with the numbers in Liu et al. (2013b) since the capitaliza-
tion policy and the data size of the training data were different.

2081



References
Yoshua Bengio. 2009. Learning deep architectures for

AI. Foundations and trends R⃝ in Machine Learning,
2(1):1–127.

James Bergstra and Yoshua Bengio. 2012. Random
search for hyper-parameter optimization. The Journal
of Machine Learning Research, 13(1):281–305.

Deborah A Dahl, Madeleine Bates, Michael Brown,
William Fisher, Kate Hunicke-Smith, David Pallett,
Christine Pao, Alexander Rudnicky, and Elizabeth
Shriberg. 1994. Expanding the scope of the ATIS
task: The ATIS-3 corpus. In Proc. HLT, pages 43–48.

Renato De Mori, Frédéric Bechet, Dilek Hakkani-Tur,
Michael McTear, Giuseppe Riccardi, and Gokhan Tur.
2008. Spoken language understanding. IEEE Signal
Processing Magazine, 3(25):50–58.

Li Deng, Gokhan Tur, Xiaodong He, and Dilek Hakkani-
Tur. 2012. Use of kernel deep convex networks and
end-to-end learning for spoken language understand-
ing. In Proc. SLT, pages 210–215.

Jeffrey L Elman. 1990. Finding structure in time. Cog-
nitive science, 14(2):179–211.

Xavier Glorot and Yoshua Bengio. 2010. Understand-
ing the difficulty of training deep feedforward neural
networks. In Proc. AISTATS, pages 249–256.

Geoffrey E Hinton, Simon Osindero, and Yee-Whye Teh.
2006. A fast learning algorithm for deep belief nets.
Neural computation, 18(7):1527–1554.

Sepp Hochreiter and Jürgen Schmidhuber. 1997. Long
short-term memory. Neural computation, 9(8):1735–
1780.

Michael I Jordan. 1997. Serial order: A parallel dis-
tributed processing approach. Advances in psychol-
ogy, 121:471–495.

Rafal Jozefowicz, Wojciech Zaremba, and Ilya Sutskever.
2015. An empirical exploration of recurrent network
architectures. In Proc. ICML, pages 2342–2350.

Diederik Kingma and Jimmy Ba. 2014. ADAM: A
method for stochastic optimization. arXiv preprint
arXiv:1412.6980.

Bing Liu and Ian Lane. 2015. Recurrent neural net-
work structured output prediction for spoken language
understanding. In Proc. NIPS Workshop on Machine
Learning for Spoken Language Understanding and In-
teractions.

Jingjing Liu, Panupong Pasupat, Scott Cyphers, and
James Glass. 2013a. Asgard: A portable architecture
for multilingual dialogue systems. In Proc. ICASSP,
pages 8386–8390.

Jingjing Liu, Panupong Pasupat, Yining Wang, Scott
Cyphers, and James Glass. 2013b. Query understand-
ing enhanced by hierarchical parsing structures. In
Proc. ASRU, pages 72–77.

Chunxi Liu, Puyang Xu, and Ruhi Sarikaya. 2015. Deep
contextual language understanding in spoken dialogue
systems. In Proc. INTERSPEECH, pages 120–124.

Grégoire Mesnil, Yann Dauphin, Kaisheng Yao, Yoshua
Bengio, Li Deng, Dilek Hakkani-Tur, Xiaodong He,
Larry Heck, Gokhan Tur, Dong Yu, et al. 2015. Us-
ing recurrent neural networks for slot filling in spoken
language understanding. IEEE/ACM Transactions on
Audio, Speech, and Language Processing, 23(3):530–
539.

Ramesh Nallapati, Bowen Zhou, Ça glar Gulçehre, and
Bing Xiang. 2016. Abstractive text summarization us-
ing sequence-to-sequence RNNs and beyond. In Proc.
CoNLL.

Baolin Peng and Kaisheng Yao. 2015. Recurrent neural
networks with external memory for language under-
standing. arXiv preprint arXiv:1506.00195.

Patti Price. 1990. Evaluation of spoken language sys-
tems: The ATIS domain. In Proc. DARPA Speech and
Natural Language Workshop, pages 91–95.

Christian Raymond and Giuseppe Riccardi. 2007. Gen-
erative and discriminative algorithms for spoken lan-
guage understanding. In Proc. INTERSPEECH, pages
1605–1608.

Yangyang Shi, Kaisheng Yao, Hu Chen, Yi-Cheng Pan,
Mei-Yuh Hwang, and Baolin Peng. 2015. Contextual
spoken language understanding using recurrent neural
networks. In Proc. ICASSP, pages 5271–5275.

Edwin Simonnet, Camelin Nathalie, Deléglise Paul, and
Estève Yannick. 2015. Exploring the use of attention-
based recurrent neural networks for spoken language
understanding. In Proc. NIPS Workshop on Machine
Learning for Spoken Language Understanding and In-
teractions.

Spoken Laungage Systems Group. 2013. The
MIT Restaurant Corpus and The MIT Movie
Corpus. https://groups.csail.mit.edu/
sls/downloads/, MIT Computer Science and Ar-
tificial Intelligence Laboratory.

Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky,
Ilya Sutskever, and Ruslan Salakhutdinov. 2014.
Dropout: A simple way to prevent neural networks
from overfitting. The Journal of Machine Learning
Research, 15(1):1929–1958.

Ilya Sutskever, Oriol Vinyals, and Quoc VV Le. 2014.
Sequence to sequence learning with neural networks.
In Proc. NIPS, pages 3104–3112.

Gokhan Tur, Dilek Hakkani-Tur, and Larry Heck. 2010.
What is left to be understood in ATIS? In Proc. SLT,
pages 19–24.

Cornelis Joost van Rijsbergen. 1979. Information Re-
trieval. Butterworth.

2082



Ngoc Thang Vu, Pankaj Gupta, Heike Adel, and Hin-
rich Schütze. 2016. Bi-directional recurrent neural
network with ranking loss for spoken language under-
standing. In Proc. ICASSP, pages 6060–6064.

Vedran Vukotic, Christian Raymond, and Guillaume
Gravier. 2015. Is it time to switch to word embedding
and recurrent neural networks for spoken language un-
derstanding? In Proc. INTERSPEECH, pages 130–
134.

Ye-Yi Wang, Alex Acero, Milind Mahajan, and John
Lee. 2006. Combining statistical and knowledge-
based spoken language understanding in conditional
models. In Proc. COLING-ACL, pages 882–889.

Ronald J Williams and Jing Peng. 1990. An effi-
cient gradient-based algorithm for on-line training of
recurrent network trajectories. Neural Computation,
2(4):490–501.

Puyang Xu and Ruhi Sarikaya. 2013. Convolutional neu-
ral network based triangular CRF for joint intent detec-
tion and slot filling. In Proc. ASRU, pages 78–83.

Kaisheng Yao and Geoffrey Zweig. 2015. Sequence-to-
sequence neural net models for grapheme-to-phoneme
conversion. Proc. INTERSPEECH, pages 3330–3334.

Kaisheng Yao, Geoffrey Zweig, Mei-Yuh Hwang,
Yangyang Shi, and Dong Yu. 2013. Recurrent neu-
ral networks for language understanding. In Proc. IN-
TERSPEECH, pages 2524–2528.

Kaisheng Yao, Baolin Peng, Yu Zhang, Dong Yu, Geof-
frey Zweig, and Yangyang Shi. 2014a. Spoken lan-
guage understanding using long short-term memory
neural networks. In Proc. SLT, pages 189–194.

Kaisheng Yao, Baolin Peng, Geoffrey Zweig, Dong Yu,
Xiaolong Li, and Feng Gao. 2014b. Recurrent con-
ditional random field for language understanding. In
Proc. ICASSP, pages 4077–4081.

Wojciech Zaremba, Ilya Sutskever, and Oriol Vinyals.
2014. Recurrent neural network regularization. arXiv
preprint arXiv:1409.2329.

Jie Zhou and Wei Xu. 2015. End-to-end learning of se-
mantic role labeling using recurrent neural networks.
In Proc. ACL, pages 1127–1137.

2083


