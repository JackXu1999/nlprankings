



















































ADAPT at SemEval-2018 Task 9: Skip-Gram Word Embeddings for Unsupervised Hypernym Discovery in Specialised Corpora


Proceedings of the 12th International Workshop on Semantic Evaluation (SemEval-2018), pages 924–927
New Orleans, Louisiana, June 5–6, 2018. ©2018 Association for Computational Linguistics

ADAPT at SemEval-2018 Task 9: Skip-Gram Word Embeddings for
Unsupervised Hypernym Discovery in Specialised Corpora

Alfredo Maldonado
ADAPT Centre

Trinity College Dublin
Ireland

Filip Klubička
ADAPT Centre

Dublin Institute of Technology
Ireland

firstname.lastname@adaptcentre.ie

Abstract

This paper describes a simple but competitive
unsupervised system for hypernym discovery.
The system uses skip-gram word embeddings
with negative sampling, trained on specialised
corpora. Candidate hypernyms for an input
word are predicted based on cosine similar-
ity scores. Two sets of word embedding mod-
els were trained separately on two specialised
corpora: a medical corpus and a music indus-
try corpus. Our system scored highest in the
medical domain among the competing unsu-
pervised systems but performed poorly on the
music industry domain. Our approach does
not depend on any external data other than raw
specialised corpora.

1 Introduction

The SemEval-2018 shared task on Hypernymy
Discovery sought to study approaches for iden-
tifying words that hold a hypernymic relation
(Camacho-Collados et al., 2018). Two words have
a hypernymic relation if one of the words belongs
to a taxonomical class that is more general than
that of the other word. For example, the word ve-
hicle belongs to a more general taxonomical class
than car does, as car is a type of vehicle. Hy-
pernymy can be seen as an is-a relationship. Hy-
pernymy has been studied from different angles in
the natural language processing literature as it is
related to the human cognitive ability of generali-
sation.

This shared task differs from recent taxonomy
evaluation tasks (Bordea et al., 2015, 2016) by
concentrating on Hypernym Discovery: the task
of predicting (discovering) n hypernym candidates
for a given input word, within the vocabulary of
a specific domain (Espinosa-Anke et al., 2016).
This shared task provided a general language do-
main vocabulary and two specialised domain vo-
cabularies in English: medical and music indus-

try. For each vocabulary, a reference corpus was
also supplied. In addition to these English vo-
cabularies, general language domain vocabularies
for Spanish and Italian were also provided. The
ADAPT team focused on the two specialised do-
main English subtasks by developing an unsuper-
vised system that builds word embeddings from
the supplied reference corpora for these domains.

Word embeddings trained on large corpora have
been shown to capture semantic relations be-
tween words (Mikolov et al., 2013a,b), including
hypernym-hyponym relations. The word embed-
dings built and used by the system presented here
exploit this property. Although these word em-
beddings do not distinguish one semantic relation
from another, we expect that true hypernyms will
constitute a significant proportion of the predicted
candidate hypernyms. Indeed, we show that for
the medical domain subtask, our system beats the
other unsupervised systems, although it still ranks
behind the supervised systems.

Even though unsupervised systems tend to rank
behind supervised systems in NLP tasks in gen-
eral, our motivation to focus on an unsupervised
approach is derived from the fact that they do not
require explicit hand-annotated data, and from the
expectation that they are able to generalise more
easily to unseen hypernym-hyponym pairs.

The rest of this system description paper is or-
ganised as follows: Section 2 briefly surveys the
relevant literature and explains the reasons for
choosing to use a particular flavour of word em-
beddings. Section 3 describes the components of
the system and its settings. Section 4 summarises
the results and offers some insights behind the
numbers. Section 5 concludes and proposes av-
enues for future work.

924



2 Related Work

Modern neural methods for natural language pro-
cessing (NLP) use pre-trained word embeddings
as fixed-sized vector representations of lexical
units in running text as input data (Goldberg, 2017,
ch. 10). However, as mentioned previously, word
embedding vectors can be used on their own to
measure semantic relations between words in an
unsupervised manner by, for example, taking the
cosine similarity of two word embedding vectors
for which semantic similarity is to be measured.

There are several competing approaches for
producing word embedding vectors. One such
approach is skip-gram with negative sampling
(SGNS), introduced by Mikolov et al. (2013a,b)
as part of their Word2Vec software package. The
skip-gram approach assumes that a focus word
occurring in text depends on its context words
(the words the focus word co-occurs with inside a
fixed-sized window), but that those context words
occur independently of each other. This con-
ditional independence assumption in the context
words makes computation more efficient and pro-
duces vectors that work well in practice. The neg-
ative sampling portion of the algorithm is a way
of producing “negative” context words for the fo-
cus word by simply drawing random words from
the corpus. These random words are assumed to
be “bad” context words for the focus word. The
positive and negative examples are used by an ob-
jective function that seeks to maximise the prob-
ability that the positive examples came from the
corpus whilst the negative examples did not.

Cosine measures on word embeddings pairs (or
even on other distributional lexical semantic rep-
resentations) give an indication of the overall se-
mantic relatedness of the word pairs they rep-
resent (Turney and Pantel, 2010), without speci-
fying the type(s) of semantic relation(s) the two
words hold. There have been endeavours to train
word embeddings that emphasise one semantic re-
lation over another. For example, Nguyen et al.
(2016) modified the skip-gram objective function
to train word embeddings that distinguished syn-
onymy from antonymy. In a similar vein, Nguyen
et al. (2017) developed an algorithm called Hy-
pervec by adapting the skip-gram objective func-
tion to emphasise the non-symmetric hypernym-
hyponym relations.

Our team indeed implemented a variant of the
Hypervec method but failed to obtain better per-

formance scores on the training set than those ob-
tained by using traditional SGNS (see Section 4).
Whilst it is possible that a software bug in our im-
plementation could be the cause of this lower per-
formance, we decided to submit the SGNS results
to the official shared task due to time constraints.

3 System Description

Our system consists of two components: a trainer
that learns word vectors using an implementation
of the Skip-Gram with Negative Sampling algo-
rithm, and a predictor that outputs (predicts) the
top 10 hypernyms of an input word based on the
trained vectors. These two components and their
settings are described here.

Trainer The trainer is a modification of Py-
Torch SGNS1, a freely available implementation
of the Skip-Gram with Negative Sampling algo-
rithm. One set of vectors per specialised corpus
(medicine and music industry) were trained on a
vocabulary that consists of the 100,000 most fre-
quent words in each corpus, using a word window
of 5 words to the left and 5 words to the right of
a sliding focus word. The windows do not cross
sentence boundaries. For negative sampling, 20
words were randomly selected from the vocabu-
lary based on their frequency2. All vectors had a
dimensionality of 300.

Predictor For each input word in the test file,
the predictor attempts to produce 10 candidate hy-
pernyms based on the vectors it learned during
training. If there is no vector for an input word,
no output for that word is given. If the input word
is a multiword expression, then the learned vec-
tors for the individual component words are re-
trieved and averaged together. This averaged vec-
tor is interpreted to represent the input multiword
expression. After a vector is retrieved (or com-
puted, in the case of averaged multiword expres-
sions), pairwise cosine similarities are taken be-
tween this vector and all other vectors (i.e. the
vectors corresponding to the other 99,999 most
frequent words). The words represented by the
10 highest ranking cosine similarities are output
as the 10 candidate hypernyms for the input word
or multiword expression.

1https://github.com/theeluwin/pytorch-sgns
2The frequencies were smoothed by raising them to the

power of 0.75 before dividing by the total.

925



Domain Approach MAP MRR P@1 P@3 P@5 P@15
medical SGNS 8.13 20.56 13.20 10.80 8.32 6.33
medical HV 4.40 13.05 10.60 5.60 4.27 3.10
music SGNS 1.88 5.34 4.00 2.40 1.89 1.35
music HV 1.79 5.39 5.00 2.07 1.62 1.28

Table 1: Automatic evaluation results for the submitted system (SGNS) and a Hypervec variant (HV).

As can be seen, our system is completely unsu-
pervised as it does not require corpora with tagged
examples of words holding hypernym-hyponym
relations or any external linguistic or taxonomical
resources.

4 Results

Table 1 shows the results for our SGNS-based ap-
proach, which was submitted to the official shared
task (SGNS), and for our Hypervec variant (HV),
which was not submitted.

Our official submission ranked at eleven out of
eighteen on the medical domain subtask with a
Mean Average Precision (MAP) of 8.13. How-
ever, it ranked first place among all the unsuper-
vised systems on this subtask. On the music indus-
try domain subtask, our system ranked 13th out of
16 places with a MAP of 1.88, ranking 4th among
the unsupervised systems. We believe that one rea-
son why the music industry scores are so much
lower than the medical results is due to our sys-
tem not producing an output for 233 of the music
industry input words (45% of the total), compared
to the 128 medical input words (26%) it failed to
predict.

Another aspect that seems to work against our
system is its simplistic way of handling multiword
expressions, namely by averaging together the in-
dividual word’s vectors. The total number of mul-
tiword expressions in the medical test set is 264,
slightly higher than in the music test set, which
contains 220 multiword expressions. Similarly,
our system does not have a way of predicting mul-
tiword expressions as hypernym candidates, as it
can only output the unigrams for which it has vec-
tor representations. 82% of the medical domain
input words have at least one hypernym that is a
multi-word expression, whilst 92% of the music
industry domain input words have multi-word ex-
pression hypernyms.

5 Conclusions and Future Work

We presented a simple but competitive unsuper-
vised system to predict hypernym candidates for
input words, based on cosine similarity scores of
word embedding vectors trained on specialised
corpora.

Unsupervised systems in general tend to have
lower performance than supervised systems as
they lack explicit information to train on. So we
are encouraged that our system beat other unsu-
pervised systems on one corpus, as this gives us
more avenues to explore.

One such avenue is to revisit our Hypervec im-
plementation. We suspect that it might require
more training epochs than the traditional SGNS
method in order to achieve reasonable results. We
also seek to experiment with refining pre-trained
SGNS word embeddings with Hypervec, rather
than training word embeddings from scratch using
Hypervec directly.

Another avenue to explore involves incorporat-
ing taxonomical information into our word em-
beddings. One way to achieve this is by retrofitting
pre-trained SGNS word embeddings with infor-
mation derived from existing taxonomies like
WordNet (Faruqui et al., 2015). Another way
of incorporating taxonomical information is by
generating a pseudo-corpus via a random walk
over such a taxonomy and then learn SGNS word
embeddings in the usual way (Goikoetxea et al.,
2015).

These approaches (Hypervec, retrofitting and
taxonomy random-walk) however, would relax the
unsupervised constraint we followed in our imple-
mentation. So yet another avenue to explore is
to instead apply different similarity functions that
might be more sensitive to the one-way, general-
specific nature of hypernymic relationships be-
tween words.

Acknowledgements

We thank our anonymous reviewers for their in-
put. The ADAPT Centre for Digital Content Tech-

926



nology is funded under the SFI Research Centres
Programme (Grant 13/RC/2106) and is co-funded
under the European Regional Development Fund.

References
Georgeta Bordea, Paul Buitelaar, Stefano Faralli, and

Roberto Navigli. 2015. Semeval-2015 task 17: Tax-
onomy extraction evaluation (texeval). In Proceed-
ings of the 9th International Workshop on Semantic
Evaluation. Association for Computational Linguis-
tics.

Georgeta Bordea, Els Lefever, and Paul Buitelaar.
2016. Semeval-2016 task 13: Taxonomy extrac-
tion evaluation (texeval-2). In Proceedings of the
10th International Workshop on Semantic Evalua-
tion. Association for Computational Linguistics.

Jose Camacho-Collados, Claudio Delli Bovi, Luis
Espinosa-Anke, Sergio Oramas, Tommaso Pasini,
Enrico Santus, Vered Shwartz, Roberto Navigli,
and Horacio Saggion. 2018. SemEval-2018 Task
9: Hypernym Discovery. In Proceedings of the
12th International Workshop on Semantic Evalua-
tion (SemEval-2018), New Orleans, LA. Association
for Computational Linguistics.

Luis Espinosa-Anke, Jose Camacho-Collados, Clau-
dio Delli Bovi, and Horacio Saggion. 2016. Super-
vised Distributional Hypernym Discovery via Do-
main Adaptation. In Proceedings of the 2016 Con-
ference on Empirical Methods in Natural Language
Processing, pages 424–435, Austin, TX.

Manaal Faruqui, Jesse Dodge, Sujay K Jauhar, Chris
Dyer, Eduard Hovy, and Noah A Smith. 2015.
Retrofitting Word Vectors to Semantic Lexicons. In
Human Language Technologies: The 2015 Annual
Conference of the North American Chapter of the
ACL, pages 1606–1615.

Josu Goikoetxea, Aitor Soroa, and Eneko Agirre.
2015. Random Walks and Neural Network Lan-
guage Models on Knowledge Bases. In Human
Language Technologies: The 2015 Conference of
the North American Chapter of the Association for
Computational Linguistics, pages 1434–1439, Den-
ver, CO.

Yoav Goldberg. 2017. Neural Network Methods for
Natural Language Processing. Morgan & Claypool
Publishers.

Tomas Mikolov, Greg Corrado, Kai Chen, and Jeffrey
Dean. 2013a. Efficient Estimation of Word Repre-
sentations in Vector Space. In Proceedings of the
International Conference on Learning Representa-
tions (ICLR 2013), pages 1–12, Scottsdale, AZ.

Tomas Mikolov, Ilya Stutskever, Kai Chen, Greg Cor-
rado, and Jeffrey Dean. 2013b. Distributed Repre-
sentations of Words and Phrases and their Compo-
sitionality. In Proceedings of the Twenty-Seventh

Annual Conference on Neural Information Process-
ing Systems (NIPS) In Advances in Neural Informa-
tion Processing Systems 26, pages 3111–3119, Lake
Tahoe, NV.

Kim Anh Nguyen, Maximilian Köper, Sabine Schulte
im Walde, and Ngoc Thang Vu. 2017. Hierarchical
Embeddings for Hypernymy Detection and Direc-
tionality. In Proceedings of the 2017 Conference on
Empirical Methods in Natural Language Process-
ing, pages 233–243, Copenhagen.

Kim Anh Nguyen, Sabine Schulte im Walde, and
Ngoc Thang Vu. 2016. Integrating Distribu-
tional Lexical Contrast into Word Embeddings for
Antonym-Synonym Distinction. In Proceedings of
the 54th Annual Meeting of the Association for Com-
putational Linguistics, pages 454–459, Berlin.

Peter D. Turney and Patrick Pantel. 2010. From Fre-
quency to Meaning: Vector Space Models of Se-
mantics. Journal of Artificial Intelligence Research,
37:141–188.

927


