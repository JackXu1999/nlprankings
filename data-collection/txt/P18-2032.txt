



















































Diachronic degradation of language models: Insights from social media


Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 195–200
Melbourne, Australia, July 15 - 20, 2018. c©2018 Association for Computational Linguistics

195

Diachronic degradation of language models:
Insights from social media

Kokil Jaidka
Computer & Information Science

University of Pennsylvania
jaidka@sas.upenn.edu

Niyati Chhaya
Big Data Experience Lab

Adobe Research
nchhaya@adobe.com

Lyle H. Ungar
Computer & Information Science

University of Pennsylvania
ungar@cis.upenn.edu

Abstract

Natural languages change over time be-
cause they evolve to the needs of their
users and the socio-technological envi-
ronment. This study investigates the di-
achronic accuracy of pre-trained language
models for downstream tasks in machine
learning and user profiling. It asks the
question: given that the social media plat-
form and its users remain the same, how
is language changing over time? How
can these differences be used to track the
changes in the affect around a particular
topic? To our knowledge, this is the first
study to show that it is possible to mea-
sure diachronic semantic drifts within so-
cial media and within the span of a few
years.

1 Introduction

Natural languages are dynamic–they are con-
stantly evolving and adapting to the needs of their
users and the environment of their use (Frermann
and Lapata, 2016). The arrival of large-scale col-
lections of historic texts and online libraries and
Google Books have greatly facilitated computa-
tional investigations of language change over the
span of decades. Diachronic differences measure
semantic drift specifically for languages over time.
For instance, the meaning of the word ‘follow’
has changed from a reference, then to surveil-
lance, and finally to the act of subscribing to a
social media user’s feed. In a quantitative anal-
ysis, diachronic differences may explain why pre-
dictive models go ‘stale’. For instance, a sentiment
model trained on Victorian-era language would la-
bel ‘aweful’ as positive sentiment; however, in
contemporary usage, ‘awful’ is considered a nega-
tive word (Wijaya and Yeniterzi, 2011). Thus mo-

tivated, we raise the following research questions:

• How do language models trained at one point
in time, perform at predicting age and gender
on language from a subsequent time?

• What is the practical benefit of measuring di-
achronic differences on Twitter?

To our knowledge, there is no existing work which
has investigated whether, and how, language mod-
els degrade over time, i.e. why predictive models
trained on an older sample of language, may fail
to work on contemporary language. While pre-
vious studies have explored the change in word
meanings spanning decades or hundreds of years,
we address a research gap by exploring finer tem-
poral granularity and using a more accessible lan-
guage corpus. Twitter’s1 discourse is rather differ-
ent from traditional English writing. So far, word
embeddings trained on Twitter (Kulkarni et al.,
2015; Mikolov et al., 2013) have considered it a
static corpus, and have not used it to study short
term changes in word connotations. It contributes
with the following observations:

• Diachronic differences are greater (hence,
language change is faster) for younger social
media users than older social media users.

• Diachronic language differences enable the
measurement of the change in social attitudes
(captured by word embeddings).

In order to study this phenomenon, we define the
notion of temporal cohorts as a set of social me-
dia users who have posted on Twitter during the
same time period, e.g., in the year 2011. In this
study, we evaluate the linguistic differences be-
tween temporal cohorts, e.g. 20-year-olds in 2011
vs. 20-year-olds in 2015.

1https://twitter.com/



196

2 Related work

A number of studies have built language models
to predict users’ age and gender (Sap et al., 2014),
personality (Schwartz et al., 2013) and other traits
(Jaidka et al., 2018a) with high accuracy from a
sample of their social media posts. We offer the
explanation that these language models may have
‘degraded’ due to the diachronic changes in lan-
guage over the past few years, as compared to the
predictions on their posts in 2011, which is closer
to the time period for which their model was actu-
ally trained (see Figure 1).

The work by Frerman and Lapata (2016) quan-
tified meaning change in terms of emerging mean-
ings over many time periods, on a corpus collating
documents spanning the years 1700-2010. Studies
measuring semantic drift using word embedding
models trained on Twitter corpora, such as Twit-
ter GloVe and Word2Vec (Mikolov et al., 2013;
Kulkarni et al., 2015), have considered microblog
posts a static resource, reflective of modern lan-
guage usage at a single point in time. Szymanski
(2017) highlights the need to explore it in contem-
porary language e.g. social media. We illustrate
that it is possible to measure diachronic semantic
drifts within social media and within the span of a
few years. Furthermore, we are arguing that year-
related change affects different cohorts differently.

We use part-of-speech information about word
embeddings to better understand semantic drift in
terms of the adjective and affective words used
to connotate everyday concepts. In doing so, we
follow the approach outlined in previous work by
Gart et al. (2017) and Hamilton et al. (2016a) to
consider different kinds of contexts (e.g., adjec-
tives, verbs and emotion words) to learn and com-
pare distributional representations of target words.

3 Method

We first establish the diachronic validity of
language-based models through predictive evalu-
ations. We then use topic models and word em-
beddings as the quantitative lens through which to
study the diachronic differences in the language
of social media users, and linear methods to eas-
ily interpret the differences between standardized
coefficients as diachronic differences in user trait
prediction from language.

3.1 Predictive validity

We test the predictive performance of language
models trained on a year’s worth of social media
posts from a subset of users who have provided
their age and gender information.

• We train language models on the age- and
gender-labeled primary dataset and evalu-
ate their diachronic validity (Hamilton et al.,
2016b), i.e. their predictive performance on
subsequently collected language samples.

• We identify age groups which drift faster than
others by reporting predictive performance
on users, stratified by year of birth.

3.2 Language insights about diachronic
differences

We use language for the following insights into di-
achronic drift:

• Important changes: For each temporal co-
hort, we identify the language features which
have the most drift in terms of recalibrated
coefficients, in regression models trained in
2011 vs. 2015. In doing so, we follow the
approach described by Rieman et al. (2017)
to compare the standardized coefficients of
the age and gender models trained on the lan-
guage samples from 2011 and 2015.

• High drift concepts: We use word embed-
dings to identify the semantic differences in
the connotations around common concepts,
for two sets of users who are a generation
apart. Following the framework proposed by
Garg et al. (2017), we calculate semantic drift
as the changes in the relative normalized dis-
tance for the context words describing a set
of target concepts.

Target concepts comprise a group of words rep-
resenting a single idea (Garg et al., 2017), for in-
stance, positive emotion, derived from the LIWC
psycholinguistic dictionary (Pennebaker et al.,
2007), and sexual orientation and gender expres-
sion (LGBTQ issues), based on a glossary on
LGBTQ terms provided by the Human Rights
Campaign 2. We use the relative norm distance to
identify the contextual words (mainly adjectives,
adverbs and sentiment words) that are most differ-
ent across the two word embedding models. Reg-
ular expression matching for part-of-speech, sen-
timent and emotion words was based on LIWC
and the NRC emotion lexicon (Mohammad et al.,

2https://www.hrc.org/resources/glossary-of-terms



197

2013). Among a variety of distance metrics, eu-
clidean distances provided the most interpretable
results.

4 Datasets and Pre-processing

Primary data: Our primary dataset consists of
the Twitter posts of adults in the United States
of America who were recruited by Qualtrics (a
crowdsourcing platform similar to Amazon Me-
chanical Turk) for an online survey, and consented
to share access to their Twitter posts. This data
was collected in a previous study by Preoţiuc-
Pietro et al. (2017) and is available online 3. We
restrict our analysis to tweets posted between Jan-
uary 2011 and December 2015, by those users
who indicated English as a primary language, have
written at least 10 posts in their posts in each
year, and have reported age and binary gender as
a part of the survey. This resulted in a dataset of
N = 554 users, who wrote a mean of 265 and a
median of 156 posts per year and over 13.5 million
words collectively. The mean age of the popula-
tion was 33.54 years. 59% of them self-identified
as male.
Decahose (Twitter 10%) dataset: For insights
based on word embeddings, we used the decahose
samples for the years 2011 and 2014 collected
by the TrendMiner project (Preotiuc-Pietro et al.,
2012), which comprises a 10% random sample
of the real-time Twitter firehose using a realtime
sampling algorithm. To match with our primary
data, we used bounding boxes to consider only
those tweets with geolocation information which
were posted in the United States. In this manner,
we obtained 130 and 179 million Twitter posts for
2011 and 2014 respectively.
Pre-processing: In a dataset of 554 users, the ab-
solute vocabulary overlap may be low. By convert-
ing each users string of words into their probabilis-
tic usage of 2000 topics, we expected to get more
stable estimates than using word-based language
models. We represent the language of each user as
a probabilistic distribution of 2000 topics derived
using Latent Dirichlet Allocation (LDA) with α
set to 0.30 to favor fewer topics per document.
These topics are modeled as open-ended clusters
of words from actual distributions in social media
over approximately 18 million Facebook updates,
and are provided as an open-sourced resource in
the DLATK python toolkit (Schwartz et al., 2017).

3https://web.sas.upenn.edu/danielpr/resources/

Predictive evaluation: We use Python’s sklearn
library to conduct a ten-fold cross-validation and
train weighted linear regression models for age,
and binary logistic regression models for gender,
on the LDA-derived features for users in nine
folds, and test on the users in the held out fold.
We use feature selection, elastic-net regulariza-
tion, and randomized PCA to avoid over-fitting.
Although we tested other linguistic features such
as n-grams, the best predictive performance was
for models trained on the topic features.
Word embeddings: We separately train word
embeddings on the language of the Twitter 10%
sample from 2011, and the sample from 2014.
We use Google’s Tensorflow framework (Abadi
et al., 2016) to optimize the prediction of co-
occurrence relationships using an approximate ob-
jective known as skip-gram with negative sam-
pling (Mikolov et al., 2013) with incremental ini-
tialization and optimizing our embeddings with
a stochastic gradient descent. Embeddings were
trained using the top-50000 words by their average
frequency over the entire time period. A similar
threshold has also been applied in previous papers
(Hamilton et al., 2016a,b). We experimented with
different window sizes and parameter settings, fi-
nally choosing a window size of 4, embeddings
with 1000 dimensions, and the negative sample
prior α set to log(5) and the number of negative
samples set to 500.4

5 Results
5.1 Predictive performance

In Figure 1, we report performance error in pre-
dicting age as the mean of (actual−predicted) in
order to better understand the model bias towards
predicting younger or older ages.

We observe that the age- and gender- predic-
tive models by Sap et al. (2014) degrade in per-
formance on language samples from more re-
cent years. They have a lower mean error in age
prediction and higher accuracy in gender predic-
tion on the a language sample from 2011 as com-
pared to 2015; yet, our test sets are ostensibly
drawn from the same corpus as the original train-
ing data. This shows that even models trained
on large datasets show performance degradation
if they are tested against newer language samples
for the same set of users. We observe the same

4The trained word embedding models can be downloaded
from http://www.wwbp.org/publications.html



198

Figure 1: Cross-year performance for predicting (a) age (re-
ported asMeanError = Ageactual−Agepredicted) and (b)
gender (reported as Accuracy). The columns depict the train-
ing set for regression models: language samples posted in a
particular year. The rows depict the test sets. Deeper shades
of blue reflect higher underestimation errors; deeper shades
of red reflect higher overestimation errors. Deeper shades of
green depict higher accuracy.

trends on in-sample models trained and tested on
our primary data (see Figure 1). Age and gender
models perform the best when tested on a sam-
ple from the same year, but age models degrade in
performance over time, with older models tending
to over-predict the age on subsequent samples.
Newer models tend to under-predict the age on
older samples of language. Taken together, these
insights suggest that the rate of change in age (1
unit per year) is less than the rate of change in lan-
guage use. Gender models demonstrate an approx-
imately 7-12% drop in accuracy for subsequent or
older years.
In the next step, we attempt to understand the rate
of change of language for social media users of
different ages, which show larger variance as com-
pared to gender. Figure 2 provides the results for
a language model trained on the language sample
of 2011 and tested to predict age from a language
sample from 2011 and the subsequent years. The
columns depict the birth year for users in the test
set. This figure provides some important insights:

1. The first row has the smallest mean errors,
which is expected since it is an in-sample pre-
diction of a 2011-trained language model on
itself.

2. The largest mean errors are seen at the bottom
left, where the model is consistently under-
predicting the age of older social media
users whose language usage has little change
over 5 years.

3. In the bottom-right, social media users born
between 1992-1997 observe the highest over-
estimation errors as they ‘sound’ older than
they are. Despite their annual increment in
age, the model still overpredicts their age by
approximately twice as many years.

5.2 Insights about Diachronic Differences

We want to understand performance degradation
in terms of the change in the associations of lin-
guistic features associated with higher age or with
one of the genders. The age range in 2011 were
[14, 41] years and in 2015 were [18, 45] years
respectively. To explore diachronic differences
in topic usage across two age-matched popula-
tions, we subset the population to the subjects
to the age range of [18, 41] years during 2011-
2015 (N=429).
Important changes: In Table 1, we compare the
standardized coefficients (p < .001) of the predic-
tors in models trained on the language of the year
2011 against those of 2015. We observe that a lot
of the topics typically associated with older social
media users in the 2011 model, 5 such as swearing,
tiredness and sleep, changed their age bias. On the
other hand, topics popular among younger social
media users – for instance, topics mentioning em-
ployable skills and meetings, percolated upward as
the early adopters of social media grew older. In
the case of gender, topics related to business meet-
ings, the government, computers, and money were
no longer predictive of males, while topics associ-
ated with proms, relationships, and hairstyles were
no longer predictive of females. 6

Age β2011 β2015
Email communication: (send, email, message, contact) 168.5 -53.7
Accommodation (place, stay, found, move) 162.2 -101.8
Sleep (bed, lay, sleep, head, tired) 59.6 -88.5
Swear (wtf, damn, sh**, wth, wrong, pissed) 38.1 -46.0
Tiredness (i’m, sick, tired, feeling, hearing) 33.6 -98.3
Hacking (virus, called, open, steal, worm, system) -99.6 253.5
Software (computer, error, photoshop, server, website) -87.2 80.7
Feeling (feeling, weird, awkward, strange, dunno) -70.5.0 23.2
Meetings (meeting, conference, student, council, board ) -44.0 38.6
Skills (management, business, learning, research) -26.4 158.0

Gender β2011∗ β2015∗
Apple products (iphone, apple, ipad, mac, download) 4.1 (0)
Sports (win, lose, game, betting, streak, change) 3.2 (0)
Bills (pay, money, paid, job, rent) 2.8 (0)
Government (government, freedom, country, democracy) 2.8 (0)
Prom (dress, prom, shopping, formal, homecoming) 1.8 (0)
Hairstyles (hair, blonde, dye, color, highlights) 1.7 (0)
Relationships (amazing, boyfriend, wonderful, absolutely) 1.7 (0)
Negative emotions (inside, deep, feel, heart, pain, empty) 1.6 (0)

Table 1: The features whose coefficients had the biggest
change and flipped sign when comparing the age and gen-
der prediction models trained on 2011 language against those
trained on 2015 language. (0) depicts that the feature was no
longer significant in the 2015 model. *:(X10−4)

High drift concepts: We now illustrate di-
achronic differences in terms of the changing con-
text around the same concept, in Table 2. We have

5See Schwartz et al. (2013)
6We also estimated feature importance through an abla-

tion analysis, according to the difference made to the overall
prediction (

∑
wixβi), which also yielded similar results.



199

Figure 2: Cross-year performance for predicting (a) age (reported as Mean Error = actualage − predictedage). The rows
reflect the test sets: language samples posted in the same or different year. The columns reflect users stratified according to their
year of birth. Deeper shades of blue reflect higher underestimation errors; deeper shades of red reflect higher overestimation
errors.

identified the words which show the largest drift
between 2011-2014, in terms of their association
with LGBTQ issues and positive emotion. We ob-
serve that this method of comparing the relative
differences in distances, proposed by Garg et al.
(2017), is able to capture social attitudes towards
gender issues, as well as the emerging trends in
netspeak. Specifically, in the discussions around
LGBTQ issues in 2014, the words that emerge are
closer to the actual experiences of the group, with
words referring to ‘passing’ (a reference to trans-
sexuals) and ‘coping’, as well as more positive
emotion words (‘yayy’, ‘harmony’).

Concept Year Context words
LGBTQ issues 2011 strippers, conservative, pedophile,

subjective, shocking
2014 coping, passed, balance, yayy, finally, harmony

Positive emotion 2011 fagazy, bomb, totally, awesomeness, tight, fly
2014 kickback, swag, winning,

dontgiveafuck, bi*ch, thicka**

Table 2: Context words for concepts in the language of Twit-
ter 2011 vs. 2014, selected among the words with the highest
relative norm difference in distances from the concepts in the
first column, between the two sets of Twitter embeddings.

6 Discussion

To summarize, our findings show that diachronic
differences in language can be observed on so-
cial media and their effect differs for social me-
dia users of varying ages. In Figure 2, consider
again the users of the same age at different points
of time. For instance, compare the errors for 22-
year old users in 2011 (born in 1989) against those
for 22-year-old users in 2012 (born in 1990), and
so on. The variance in error along these diagonals,
are high in the right side of the table. This suggests
that in every subsequent year, the language of late-
teens and early-twenties is more different from the
language of their contemporaries from the year be-
fore. On the other hand, compare the errors for
37-year-olds in 2011 (born in 1974) against those
for 37-year-olds in 2012 (born in 1975). The er-
rors have a low variance along the diagonals in the

left of the Figure. Among social media users in
their late thirties, the language of each cohort of
35-year-olds changes little over the previous year.

Next, consider the quantitative insights from Ta-
ble 1. The results suggest that over time, young
users from 2011 continued to use certain topics,
while older users adopted newer trends. We found
that if we do not use age-matched samples in this
experiment, the coefficients for other topics are
also flipped, but this effect is noticeably dimin-
ished with an age-matched subset. This suggests
that indeed, a part of the language drift appeared
because 1/5th of the population was shifting along
the temporal axis, which also associates their dis-
tinct topical preferences with an older age group.

7 Conclusion & Future Work
This study offers an empirical study of how gender
and age classifiers degrade over time, a qualitative
study of the features whose coefficients change the
most, and concepts that drift in meaning over time.
The language of social media posts can be used
to study semantic drift over short periods of time,
even from a dataset of 554 social media users.
These methods can also find application in the
study of other linguistic phenomena such as pol-
ysemy (Hamilton et al., 2016b; Szymanski, 2017).
However, there is a need to disentangle which dif-
ferences are due to the changing use of language
from the ones due to changes in topics and trends
on social media.

Language models degrade over time, but it not
always feasible to retrain models with new data. In
future work, we plan to explore whether domain
adaptation techniques can resolve diachronic per-
formance differences, in addition to generalizing
language models to other platforms (Jaidka et al.,
2018b) or scaling to measure communities (Rie-
man et al., 2017).



200

References
Martı́n Abadi, Paul Barham, Jianmin Chen, Zhifeng

Chen, Andy Davis, Jeffrey Dean, Matthieu Devin,
Sanjay Ghemawat, Geoffrey Irving, Michael Isard,
et al. 2016. Tensorflow: A system for large-scale
machine learning. In OSDI. volume 16, pages 265–
283.

Lea Frermann and Mirella Lapata. 2016. A bayesian
model of diachronic meaning change. Transac-
tions of the Association for Computational Linguis-
tics 4:31–45.

Nikhil Garg, Londa Schiebinger, Dan Jurafsky, and
James Zou. 2017. Word embeddings quantify 100
years of gender and ethnic stereotypes. arXiv
preprint arXiv:1711.08412 .

William L Hamilton, Jure Leskovec, and Dan Jurafsky.
2016a. Cultural shift or linguistic drift? comparing
two computational measures of semantic change. In
Proceedings of the Conference on Empirical Meth-
ods in Natural Language Processing. Conference on
Empirical Methods in Natural Language Process-
ing. NIH Public Access, volume 2016, page 2116.

William L Hamilton, Jure Leskovec, and Dan Jurafsky.
2016b. Diachronic word embeddings reveal statisti-
cal laws of semantic change. In Proceedings of the
54th Annual Meeting of the Association for Compu-
tational Linguistics (Volume 1: Long Papers). vol-
ume 1, pages 1489–1501.

Kokil Jaidka, Anneke Buffone, Salvatore Giorgi, Jo-
hannes Eichstaedt, Masoud Rouhizadeh, and Lyle
Ungar. 2018a. Modeling and visualizing locus of
control with facebook language. In Proceedings of
the International AAAI Conference on Web and So-
cial Media.

Kokil Jaidka, Sharath Chandra Guntuku, Anneke Buf-
fone, H. Andrew Schwartz, and Lyle Ungar. 2018b.
Facebook vs. twitter: Differences in self-disclosure
and trait prediction. In Proceedings of the Interna-
tional AAAI Conference on Web and Social Media.

Vivek Kulkarni, Rami Al-Rfou, Bryan Perozzi, and
Steven Skiena. 2015. Statistically significant detec-
tion of linguistic change. In Proceedings of the 24th
International Conference on World Wide Web. In-
ternational World Wide Web Conferences Steering
Committee, pages 625–635.

Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Cor-
rado, and Jeff Dean. 2013. Distributed representa-
tions of words and phrases and their compositional-
ity. In Advances in neural information processing
systems. pages 3111–3119.

Saif M Mohammad, Svetlana Kiritchenko, and Xiao-
dan Zhu. 2013. Nrc-canada: Building the state-
of-the-art in sentiment analysis of tweets. arXiv
preprint arXiv:1308.6242 .

James W Pennebaker, Roger J Booth, and Martha E
Francis. 2007. Linguistic inquiry and word count:
Liwc [computer software]. Austin, TX: liwc. net .

Daniel Preoţiuc-Pietro, Ye Liu, Daniel Hopkins, and
Lyle Ungar. 2017. Beyond binary labels: political
ideology prediction of twitter users. In Proceed-
ings of the 55th Annual Meeting of the Association
for Computational Linguistics (Volume 1: Long Pa-
pers). volume 1, pages 729–740.

Daniel Preotiuc-Pietro, Sina Samangooei, Trevor
Cohn, Nicholas Gibbins, and Mahesan Niranjan.
2012. Trendminer: An architecture for real time
analysis of social media text .

Daniel Rieman, Kokil Jaidka, H Andrew Schwartz, and
Lyle Ungar. 2017. Domain adaptation from user-
level facebook models to county-level twitter pre-
dictions. In Proceedings of the Eighth International
Joint Conference on Natural Language Processing
(Volume 1: Long Papers). volume 1, pages 764–773.

Maarten Sap, Gregory Park, Johannes Eichstaedt, Mar-
garet Kern, David Stillwell, Michal Kosinski, Lyle
Ungar, and Hansen Andrew Schwartz. 2014. Devel-
oping age and gender predictive lexica over social
media. In Proceedings of the 2014 Conference on
Empirical Methods in Natural Language Processing
(EMNLP). pages 1146–1151.

H Andrew Schwartz, Johannes C Eichstaedt, Mar-
garet L Kern, Lukasz Dziurzynski, Stephanie M Ra-
mones, Megha Agrawal, Achal Shah, Michal Kosin-
ski, David Stillwell, Martin EP Seligman, et al.
2013. Personality, gender, and age in the language
of social media: The open-vocabulary approach.
PloS one 8(9):e73791.

H Andrew Schwartz, Salvatore Giorgi, Maarten Sap,
Patrick Crutchley, Lyle Ungar, and Johannes Eich-
staedt. 2017. Dlatk: Differential language analysis
toolkit. In Proceedings of the 2017 Conference on
Empirical Methods in Natural Language Process-
ing: System Demonstrations. pages 55–60.

Terrence Szymanski. 2017. Temporal word analogies:
Identifying lexical replacement with diachronic
word embeddings. In Proceedings of the 55th An-
nual Meeting of the Association for Computational
Linguistics (Volume 2: Short Papers). volume 2,
pages 448–453.

Derry Tanti Wijaya and Reyyan Yeniterzi. 2011. Un-
derstanding semantic change of words over cen-
turies. In Proceedings of the 2011 international
workshop on DETecting and Exploiting Cultural di-
versiTy on the social web. ACM, pages 35–40.


