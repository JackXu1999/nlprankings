










































Tree Parsing with Synchronous Tree-Adjoining Grammars


Proceedings of the 12th International Conference on Parsing Technologies, pages 14–25,
October 5-7, 2011, Dublin City University. c© 2011 Association for Computational Linguistics

Tree Parsing with Synchronous Tree-Adjoining Grammars

Matthias Büchse a and Mark-Jan Nederhof b and Heiko Vogler a

a Department of Computer Science b School of Computer Science

Technische Universität Dresden University of St Andrews

D-01062 Dresden, Germany North Haugh, St Andrews, KY16 9SX, UK

Abstract

Restricting the input or the output of a

grammar-induced translation to a given set

of trees plays an important role in statistical

machine translation. The problem for prac-

tical systems is to find a compact (and in

particular, finite) representation of said re-

striction. For the class of synchronous tree-

adjoining grammars, partial solutions to this

problem have been described, some being

restricted to the unweighted case, some to

the monolingual case. We introduce a for-

mulation of this class of grammars which is

effectively closed under input and output re-

strictions to regular tree languages, i.e., the

restricted translations can again be repre-

sented by grammars. Moreover, we present

an algorithm that constructs these grammars

for input and output restriction, which is in-

spired by Earley’s algorithm.

1 Introduction

Many recent systems for statistical machine trans-

lation (SMT) (Lopez, 2008) use some grammar at

their core. Chiang (2007), e. g., uses synchronous

context-free grammars (SCFG) that derive pairs

of translationally equivalent sentences. Huang

et al. (2006) use tree-to-string transducers (called

xRLNS) that describe pairs of the form (phrase-

structure tree, string). Other systems, such as

(Eisner, 2003; Zhang et al., 2008; Nesson et al.,

2006; DeNeefe and Knight, 2009), use variants

of synchronous tree-adjoining grammars (STAGs)

(Abeille et al., 1990; Joshi and Schabes, 1997)

that derive pairs of dependency or phrase-structure

trees. Common variants of STAGs are syn-

chronous tree-substitution grammars (STSGs) and

synchronous tree-insertion grammars (STIGs).

For grammar-based systems, a variety of tasks

can be described using the general concepts of in-

put product and output product (Maletti, 2010b).

Roughly speaking, these products restrict the

translation described by the grammar to a given

tree or string language on the input or output side.

For practical purposes, the derivations of the re-

stricted translation are represented in a compact

way, e.g., using a weighted regular tree grammar

(WRTG) (Alexandrakis and Bozapalidis, 1987).

The process of obtaining this representation is

called tree parsing or string parsing, depending

on the type of restriction. We illustrate the impor-

tance of input and output product by considering

its role in three essential tasks of SMT.

Grammar Estimation. After the rules of the

grammar have been obtained from a sample of

translation pairs (rule extraction), the probabilities

of the rules need to be determined. To this end,

two approaches have been employed.

Some systems such as those by Chiang (2007)

and DeNeefe and Knight (2009) hypothesize a

canonical derivation for each translation pair, and

apply relative-frequency estimation to the result-

ing derivations to obtain rule probabilities. While

this procedure is computationally inexpensive, it

only maximizes the likelihood of the training data

under the assumption that the canonical deriva-

tions are the true ones.

Other systems such as those by Eisner (2003),

Nesson et al. (2006), and Graehl et al. (2008)

use a variant of the EM algorithm (Dempster et

al., 1977) called Inside-Outside. This algorithm

requires that the set of derivations for a given

translation pair be representable by a WRTG. In

most cases, this can be computed by restricting the

grammar at hand to the given translation pair, that

is, by applying input and output product. Note that

the pair can contain strings or trees or even some

combination thereof.

Feature Weight Estimation. In the systems

mentioned at the beginning of this section, a prob-

ability distribution of the form p(e, d | f) is chosen
from a log-linear model (Berger et al., 1996; Och

and Ney, 2002), where e, d, and f are an English

14



sentence, a derivation, and a foreign sentence, re-

spectively. Such a distribution combines informa-

tion from different sources, called features, such as

the grammar or a probability distribution over En-

glish sentences. The features are represented by

real-valued functions hi(e, d, f). For said combi-
nation, each feature gets a weight λi.

The feature weights are usually estimated us-

ing minimum-error-rate training (Och, 2003). For

this it is necessary to compute, for a given f , the
set Df of n highest ranking derivations generat-
ing f on the foreign side. Roughly speaking, this
set can be computed by applying the input prod-

uct with f , and then applying the n-best algorithm
(Huang and Chiang, 2005; Büchse et al., 2010).

We note that, while f is usually a string, it can in
some circumstances be a phrase-structure tree, as

in (Huang et al., 2006).

Decoding. The actual translation, or decoding,

problem amounts to finding, for a given f ,

ê = argmaxe
∑

d

∏

i hi(e, d, f)
λi .

Even for the simplest grammars, this problem is

NP hard (Casacuberta and de la Higuera, 2000).

As a result, SMT systems use approximations

such as crunching or variational decoding (Li et

al., 2009). Here we focus on the former, which

amounts to restricting the sum in the equation to

the set Df . Since this set is finite, the sum is then
zero for almost all e, which makes the computa-
tion of ê feasible. As mentioned before, the input
product can be used to compute Df .

As we have seen in these tasks, tree parsing is

employed in recent SMT systems. Table 1 lists

five relevant contributions in this area. These con-

tributions can be classified according to a number

of characteristics indicated by the column head-

ings. One of these characteristics is the abstraction

level (AL), which we categorize as follows:

1. language-theoretic result,

2. construction,

3. algorithm,

4. implementation.

The first three entries of Tab. 1 deal with con-

tributions that are restricted to tree substitution.

Huang et al. (2006) show an algorithm for com-

puting the best derivation of the input product of

an xRLNS with a single tree. Graehl et al. (2008)

present an algorithm for computing the derivation

WRTG for the input and output product of a tree-

to-tree transducer (called xRLN) with a single pair

of trees. Eisner (2003) describes an algorithm for

computing the set of derivations for the input and

output product of an STSG with a single pair of

trees.

We note that the grammar classes covered so far

are strictly less powerful than STAGs. This is due

to the fact that STAGs additionally permit an op-

eration called adjoining. As Nesson et al. (2006)

and DeNeefe and Knight (2009) point out, the

adjoining operation has a well-founded linguistic

motivation, and permitting it improves translation

quality.

There are two papers approaching the problem

of tree parsing for STAGs, given in the fourth

and fifth entries of the table. These papers estab-

lish closure properties, that is, their constructions

yield a grammar of the same type as the original

grammar. Since the resulting grammars are com-

pact representations of the derivations of the input

product or output product, respectively, these con-

structions constitute tree parsing.

Nederhof (2009) shows that weighted linear in-

dex grammars (WLIGs) are closed under weighted

intersection with tree languages generated by

WRTGs. WLIGs derive phrase-structure trees,

and they are equivalent to tree-adjoining gram-

mars (TAGs). His construction can be extended

to some kind of synchronous WLIG without prob-

lems. However, synchronization interacts with

the height restriction present for WLIG rules in a

way that makes synchronousWLIGs less powerful

than STAGs.

Maletti (2010a) uses an alternative represen-

tation of STAG, namely as extended tree trans-

ducers (XTT) with explicit substitution. In this

framework, adjoining is encoded into the phrase-

structure trees by introducing special symbols, to

be evaluated in a separate step. He indicates that

his representation of STAG is closed under input

and output product with regular tree languages by

providing a corresponding construction. However,

in his setting, both the translations and the lan-

guages are unweighted.

The advantage of closure properties of the

above kind is that they allow cascades of input

and output products to be constructed in a uniform

way, as well as applying further operations on the

grammars, such as projection. Ultimately, SMT

tasks may be described in this framework, as wit-

nessed by toolboxes that exist for WFSTs (Mohri,

2009) and XTTs (May and Knight, 2006).

15



characteristics

paper AL grammar restriction type result

(Huang et al., 2006) 3–4 xRLNS tree best derivation

(Graehl et al., 2008) 3–4 xRLN (tree, tree) derivation WRTG

(Eisner, 2003) 3–4 STSG (tree, tree) derivations

(Nederhof, 2009) 2 WLIG regular tree language WLIG

(Maletti, 2010a) 2 XTT regular tree language XTT

(this paper) 1–3 WSTAG regular tree language WSTAG

Table 1: Tree-parsing algorithms published so far in comparison with this paper.

In this paper, we propose a weighted formula-

tion of STAGs which is closed under input and

output product withWRTGs, and we present a cor-

responding tree-parsing algorithm. This paper is

organized as follows.

In Sec. 2, we introduce our formulation of

STAGs, which is called weighted synchronous

tree-adjoining grammar (WSTAG). The major dif-

ference with respect to the classical STAGs is two-

fold: (i) we use states and (ii) we encode substi-

tution and adjoining sites as variables in the tree.

The states make intersection with regular proper-

ties possible (without the need for relabeling as

in (Shieber, 2004) and (Maletti, 2010a)). In addi-

tion, they permit implementing all features of con-

ventional STAG/STIG, such as potential adjoining

and left/right adjoining. The variables are used for

synchronization of the input and output sides.

In Sec. 3, we show that WSTAGs are closed un-

der input and output product with tree languages

generated by WRTGs (cf. Theorem 1). We do this

by means of a direct construction (cf. Sec. 4). Our

construction is based on the standard technique

for composing two top-down tree transducers (cf.

page 195 of (Baker, 1979)). This technique has

been extended in Theorem 4.12 of (Engelfriet and

Vogler, 1985) to the composition of a macro tree

transducer and a top-down tree transducer (also cf.

(Rounds, 1970)); in fact, our direct construction is

very similar to the latter one.

Section 5 contains Algorithm 1, which com-

putes our construction (modulo reduction). It is

inspired by a variant of Earley’s algorithm (Ear-

ley, 1970; Graham et al., 1980). In this way we

avoid computation of a certain portion of useless

rules, and we ensure that the complexity is linear

in the size of the input WSTAG. The algorithm is

presented in the framework of deductive parsing

(Goodman, 1999; Nederhof, 2003).

In Sections 6 and 7, we discuss the correctness

of our algorithm and its complexity, respectively.

2 Formalisms

We denote the set of all unranked, ordered, la-

beled trees over some alphabet Σ by UΣ. We
represent trees as well-formed expressions, e.g.,

S(Adv(yesterday), ∗); a graphical representation
of this tree occurs at the very bottom of Fig. 1(a).

Sometimes we assign a rank (or: arity) k ∈ N
to a symbol σ ∈ Σ and then require that every
σ-labeled position of a tree has exactly k succes-
sors. We denote the set of all positions of a tree

t ∈ UΣ by pos(t). A position is represented as
a finite sequence of natural numbers (Gorn nota-

tion). If w ∈ pos(t), then t(w) denotes the label
of t at w, and rkt(w) denotes the number of suc-
cessors of w.

2.1 Weighted Regular Tree Grammars

A weighted regular tree grammar (for short:

WRTG) is a tuple H = (P, Σ, r0, R, p) where P
is a finite set of states, Σ is an alphabet, r0 ∈ P is
the initial state, and R is a finite set of rules; ev-
ery rule ρ has the form r → σ(r1, . . . , rk) where
k ∈ N, r, r1, . . . , rk ∈ P , and σ ∈ Σ (note
that σ(r1, . . . , rk) is a tree over Σ ∪ P ); finally,
p : R → R≥0 is the weight assignment, where
R≥0 is the set of all non-negative real numbers.

A run (of H) is a tree κ ∈ UP . Let t ∈ UΣ
and let κ be a run. We say that κ is a run on t
if pos(κ) = pos(t) and for every position w ∈
pos(t) the rule ρ(κ, t, w) is in R, where ρ(κ, t, w)
is defined as

κ(w)→ t(w)
(

κ(w1), . . . , κ(w rkt(w))
)

.

The weight of a run κ on t is the value wt(κ, t) ∈
R≥0 defined by

wt(κ, t) =
∏

w∈pos(t)

p(ρ(κ, t, w)) .

The weighted tree language generated by

WRTG H is the mapping L(H) : UΣ → R≥0

16



defined by

L(H)(t) =
∑

κ run on t
κ(ε)=r0

wt(κ, t) .

The support supp(L(H)) of L(H) is the set of all
t ∈ UΣ such that L(H)(t) 6= 0.

2.2 Weighted Tree-Adjoining Grammars

In our formulation of STAGs we use states (cf.

Fülöp et al. (2010) for the use of states in

STSGs). The states make intersection with de-

vices of finite-state power possible. More specif-

ically, they allow for a product construction as is

common in automata theory (cf. Sec. 4). More-

over, they permit implementing all features of con-

ventional STAG/STIG, such as potential adjoining

and left/right adjoining. In contrast to the tradi-

tional setting, substitution sites and adjoining sites

are formalized as explicit positions in the right-

hand sides of rules; these positions are labeled by

variables. For this, we assume that x1, x2, . . . and
z1, z2, . . . are two fixed infinite and disjoint se-
quences of pairwise distinct variables. We assume

that every xj has rank 0 and every zj has rank 1.
We use x1, x2, . . . to denote substitution sites and
z1, z2, . . . to denote adjoining sites. The foot node
is labeled by an additional nullary symbol ∗. The
input and output components are synchronized via

these sites and the corresponding states.

A weighted synchronous tree-adjoining gram-

mar with states (for short: WSTAG) is a tuple

G = (Q, F, Σ, q0, R, p) where
• Q and F are disjoint finite sets (of nullary
and unary states, respectively, each denoted

by variants of q and f , respectively),
• Σ is an alphabet (terminal alphabet),
• q0 is a nullary state (initial state),
• R is a finite set of rules of either of the fol-
lowing forms:

q → 〈ζζ ′, q1 · · · qm, f1 · · · fl〉 (α)

f(∗)→ 〈ζζ ′, q1 · · · qm, f1 · · · fl〉 (β)

where ζ and ζ ′ are trees over Σ ∪ V and

V = {x1, . . . , xm, z1, . . . , zl} (α)

V = {x1, . . . , xm, z1, . . . , zl, ∗} (β)

and every element of V occurs exactly once
in each of ζ and ζ ′, and
• p : R→ R≥0 is the weight assignment.

Rules of the forms (α) and (β) are called (m, l)-
rules; ζ and ζ ′ are called the input tree and the
output tree of the rule, respectively. For fixed q
and f , the sets of all rules of the form (α) and (β)
are denoted byRq andRf , resp. Figure 1(a) shows
an example of aWSTAG. In the following, letG =
(Q, F, Σ, q0, R, p) be a WSTAG.

We define the semantics in terms of bimor-

phisms (Shieber, 2006). For this we define a

WRTG HG that generates the weighted tree lan-
guage of derivation trees ofG, and two tree homo-
morphisms h1 and h2 that retrieve from a deriva-
tion tree the derived input tree and output tree, re-

spectively.

The derivation tree WRTG of G is the WRTG
HG = (Q ∪ F, R, q0, R

′, p′) where

• we assign the rank m+ l to every (m, l)-rule,
• R′ is the set of all rules D(ρ) with ρ ∈ R and

– if ρ is of the form (α), then
D(ρ) = q → ρ(q1, . . . , qm, f1, . . . , fl),

– if ρ is of the form (β), then
D(ρ) = f → ρ(q1, . . . , qm, f1, . . . , fl),

• and p′(D(ρ)) = p(ρ).

Recall that L(HG) is the weighted tree lan-
guage generated by HG. We call the trees in
supp(L(HG)) derivation trees. For instance,
with the WSTAG of Fig. 1(a), the tree dex =
ρ1(ρ2, ρ3(ρ4), ρ5) is a derivation tree of the de-
rived trees shown in Fig. 1(b) and Fig. 1(c).

Formally, the derived trees are obtained by the

tree homomorphisms h1 and h2, each of type
supp(L(HG)) → UΣ, which we define induc-
tively as follows:

h1(ρ(d1, . . . , dm, d
′
1, . . . , d

′
l))

= ζ[~x/h1(~d)][[~z/h1(~d
′)]]

where [~x/h1(~d)] abbreviates the m first-order
substitutions [x1/h1(d1)] . . . [xm/h1(dm)], and
[[~z/h1(~d

′)]] abbreviates the l second-order sub-
stitutions [[z1/h1(d

′
1)]] . . . [[zl/h1(d

′
l)]]. The first-

order substitution [x/s] replaces every occurrence
of x by s, and the second-order substitution [[z/s]]
is defined inductively in Fig. 2. The tree homo-

morphism h2 is defined in the same way as h1, but
with ζ ′ instead of ζ. We call d a derivation tree of
the pair (h1(d), h2(d)).

In continuation of our running example, we cal-

culate h1(dex), where we use [1], [2], and [[3]]
as abbreviations for the substitutions [x1/h1(ρ2)],

17



ρ1 : q1 →

〈

z1

S

x1 VP

V

saw

x2

z1

S

VP

V

sah

x1 x2 , q2q2, f

〉

# 1

ρ2 : q2 →

〈
NP

N

Mary

NP

N

Mary

, ε, ε

〉

# 0.6

ρ3 : q2 →

〈

NP

x1 N

man

NP

x1 N

Mann

, q3, ε

〉

# 0.4

ρ4 : q3 →

〈 D

a

D

einen
, ε, ε

〉

# 1

ρ5 : f(∗)→

〈

S

Adv

yesterday

∗

S

Adv

gestern

∗ , ε, ε

〉

# 1

(a)

S

Adv

yesterday

S

NP

N

Mary

VP

V

saw

NP

D

a

N

man

S

Adv

gestern

S

VP

V

sah

NP

N

Mary

NP

D

einen

N

Mann

(b) (c)

Figure 1: (a) Example of a WSTAG (following (Joshi and Schabes, 1997)), (b) input tree, and (c) output tree.

σ(ζ1, . . . , ζk)[[z/s]] = σ(ζ1[[z/s]], . . . , ζk[[z/s]]) xj [[z/s]] = xj ∗[[z/s]] = ∗

zj(ζ)[[z/s]] =

{

s′ if z = zj and s
′ is obtained from s by replacing the ∗ by ζ

zj(ζ[[z/s]]) otherwise.

Figure 2: Second-order substitution [[z/s]], where σ ∈ Σ, xj is a nullary variable, and zj is a unary variable.

18



[x2/h1(ρ3(ρ4))], and [[z1/h1(ρ5)]], respectively:

h1(dex)

= z1(S(x1, VP(V(saw), x2)))[1][2][[3]]

= z1(S(h1(ρ2), VP(V(saw), x2)))[2][[3]]

= z1(S(h1(ρ2), VP(V(saw), h1(ρ3(ρ4))))
︸ ︷︷ ︸

ξ

)[[3]]

= S(Adv(yesterday), ξ) ,

which evaluates to the tree of Fig. 1(b).

The weighted tree transformation specified by

WSTAG G is T (G) : UΣ×UΣ → R≥0 defined by

T (G)(s, t) =
∑

d derivation tree of (s, t)

L(HG)(d).

We note that for every derivation tree d there is
a unique run κd of HG on d: κd(w) is the state
occurring in the left-hand side of the rule d(w),
for every w ∈ pos(d).
As an example, we reconsider dex, which is

a derivation tree of the translation pair (s, t),
given by Fig. 1(b) and Fig. 1(c). Then we have

L(HG)(dex) = wt(κdex , dex) =
∏5

i=1 p(ρi) =
0.24. Since dex is the only derivation tree of (s, t),
we have that T (G)(s, t) = 0.24.
STSGs as defined in Fülöp et al. (2010) are

WSTAGs which only have nullary states. Also

classical STAGs (Abeille et al., 1990; Shieber and

Schabes, 1990) and STIGs (Nesson et al., 2005;

Nesson et al., 2006; DeNeefe and Knight, 2009)

can be viewed as particular WSTAGs. In partic-

ular, potential adjoining as it occurs in classical

STAGs can be simulated, as the following excerpt

of a WSTAG shall illustrate:

q → 〈z1(A(x1)) ζ
′, q′, f〉

f(∗)→ 〈∗ ∗, ε, ε〉

f(∗)→ 〈z1(∗) z1(∗), ε, f
′〉

Moreover, the left-/right adjoining restriction of

STIGs can be handled by keeping appropriate fi-

nite information in the states.

3 Closure Result

First we define the input product and output prod-

uct of a weighted tree transformation T : UΣ ×
UΣ → R≥0 and a weighted tree language
L : UΣ → R≥0. Formally, the input product
of T and L is the weighted tree transformation
L � T : UΣ × UΣ → R≥0 such that

(L � T )(s, t) = L(s) · T (s, t) .

Similarly, we define the output product of T andL
as T � L : UΣ × UΣ → R≥0 such that

(T � L)(s, t) = T (s, t) · L(t) .

We note that the input product and output prod-

uct can be considered as a kind of composition

of weighted relations by viewing L as mapping
L′ : UΣ × UΣ → R≥0 with L

′(s, t) = L(s) if
s = t, and L′(s, t) = 0 otherwise.
Here we prove that WSTAGs are closed under

input product (and output product) with WRTGs.

Theorem 1 For every WSTAG G and WRTG H
there are WSTAGs H � G and G � H such that
• T (H � G) = L(H) � T (G) and
• T (G � H) = T (G) � L(H).

We will only prove the closure under input

product, because the proof for the output product

is similar.

For the unweighted case, the closure result fol-

lows from classical results. The unweighted case

is obtained if we replace the algebra in Sec. 2

by another one: R≥0 is replaced by the set B =
{true, false} and the operations + and · are re-
placed by disjunction and conjunction, respec-

tively. In other words, we replace the inside

semiring (R≥0, +, ·, 0, 1) by the Boolean semiring
(B,∨,∧, false, true). Then L(H) and T (G) be-
come sets L(H) ⊆ UΣ and T (G) ⊆ UΣ × UΣ.
In this setting and using h : supp(L(HG)) →
UΣ × UΣ defined by h(d) = (h1(d), h2(d)), we
have that

L(H) � T (G) = h(h−11 (L(H))) .

Note that h−11 (L(H)) ⊆ supp(L(HG)). Now we
observe that h1 can be computed by a particular
macro tree transducer (Engelfriet, 1980; Courcelle

and Franchi-Zannettacci, 1982); we note that in

(Shieber, 2006) such macro tree transducers were

called embedded tree transducers.

Engelfriet and Vogler (1985) proved in Theo-

rem 7.4 that the class of regular tree languages

is closed under the inverse of macro tree trans-

ducers. Thus, since L(H) is a regular tree lan-
guage, also h−11 (L(H)) is a regular tree language.
Thus L(H) � T (G) = h(L(H̄)) for some regular
tree grammar H̄ . Now it is easy to construct (us-
ing h1 and h2) a STAG H � G from H̄ such that
T (H � G) = h(L(H̄)).
For the weighted case, we give a direct con-

struction in the next section.

19



4 Direct Construction

We now provide our construction of the

WSTAG H � G in Theorem 1 where
G = (Q, F, Σ, q0, R, p) is a WSTAG and
H = (P, Σ, r0, RH , pH) is a WRTG.
First we define an enrichment ofH that can gen-

erate trees like ζ as they occur in rules of G, that
is, including variables xj , zj , and possibly ∗. To
this end, let ρ ∈ R. A (state) assignment for ρ is a
mapping θ that maps each nullary variable in ρ to
a state of H and each unary variable in ρ to a pair
of such states. Likewise, if ∗ occurs in ζ, then θ
maps it to a state of H . For every r ∈ P and as-
signment θ, we define the WRTG H(r, θ), which
is obtained from H by using r as initial state and
adding the following rules with weight 1 for every

r, r′ ∈ P :

r → xj if θ(xj) = r,

r → zj(r
′) if θ(zj) = (r, r

′), and

r → ∗ if θ(∗) = r .

Roughly speaking, for every rule ρ ∈ R, we let
H(r, θ) “run” on the input tree ζ of ρ. Formally,
we define the product WSTAG of H and G as the
WSTAG H � G =

(Q× P, F × (P × P ), Σ, (q0, r0), R
′, p′)

as follows. Let ρ ∈ R, r ∈ P , and θ an assign-
ment for ρ. Then, depending on whether ρ has the
form (α) or (β), the rule

(q, r)→ 〈ζζ ′, u, v〉 (α)

(f, (r, θ(∗)))→ 〈ζζ ′, u, v〉 (β)

is in R′ where
• u = (q1, θ(x1)) · · · (qm, θ(xm)) and
• v = (f1, θ(z1)) · · · (fm, θ(zl)).

We denote this rule by (ρ, r, θ). Its weight is
p′(ρ, r, θ) = p(ρ) · L(H(r, θ))(ζ). There are no
further elements in R′.
We omit a proof for T (H�G) = L(H)�T (G).
We have that |R′| ∈ O(|R| · |P |C) where C =

max{m + 2 · l + y | ∃ρ : ρ is an (m, l)-rule, y =
1 in case (α), y = 2 in case (β)}.

5 Algorithm

Now we present Algorithm 1, which performs the

construction ofH�G. It uses a strategy similar to
that of Earley’s algorithm to construct at least all

useful rules of H �G while avoiding construction

of a certain portion of useless rules. A rule is use-

ful if it occurs in some derivation tree; otherwise it

is useless.

Algorithm 1 Product construction algorithm

Require: G = (Q, F, Σ, q0, R, p) a WSTAG and
H = (P, Σ, r0, RH , pH) a WRTG,

Ensure: Ru contains at least the useful rules ofH�G,
pu coincides with the weight assignment ofH�G
on Ru

⊲ step 1: compute I
1: I ← ∅
2: repeat

3: add items to I by applying the rules in Fig. 3
4: until convergence

⊲ step 2: compute rules
5: Ru ← ∅
6: for [ρ, ε, r, θ] ∈ I do
7: Ru ← Ru ∪ {(ρ, r, θ)} as in Sec. 4

⊲ step 3 (optional): reduce
8: perform reachability analysis to remove useless

rules from Ru
⊲ step 4: compute weights

9: for (ρ, r, θ) ∈ Ru do
10: pu(ρ, r, θ)← p(ρ) · W([ρ, ε, r, θ])

⊲ defined in Fig. 4

Conceptually, the algorithm proceeds in four

steps. Note that, in practice, some of these steps

may be implemented interleaved in order to reduce

constants in the runtime complexity.

The first step is based on a deductive system, or

deductive parsing schema, which is given in Fig. 3.

Its central notion is that of an item, which is a syn-

tactic representation of a proposition. We say that

an item holds if the corresponding proposition is

true. In Sec. 6 we will explain the meaning of

the items in detail. Roughly speaking, the items

drive a depth-first left-to-right simulation of H on
the trees on the input side of rules of G. Items
with round brackets are responsible for top-down

traversal and items with square brackets for hori-

zontal and bottom-up traversal. The deductive sys-

tem contains inference rules which are, as usual,

syntactic representations of conditional implica-

tions (Goodman, 1999; Nederhof, 2003).

The first step of the algorithm computes the

least set I of items that is closed under applica-
tion of the inference rules. This is done in the

usual iterative way, starting with the empty set and

applying rules until convergence. Since there are

only finitely many items, this process will termi-

nate. Note that, given the soundness of the infer-

20



(1)
(q0, r0)

(2q)
(q, r)

(ρ, ε, r)
{ ρ ∈ Rq (2f)

(f, r)

(ρ, ε, r)
{ ρ ∈ Rf

(3q)
(q, r) [ρ, ε, r, θ]

[q, r]
{ ρ ∈ Rq (3f)

(f, r) [ρ, ε, r, θ]

[f, r, θ(∗)]
{ ρ ∈ Rf

(4)
(ρ, w, r)

[ρ, w, 0, r, r1 · · · rk, ∅]

{
ζ(w) ∈ Σ k = rkζ(w)

pH(r → ζ(w)
(
r1, . . . , rk

)
) > 0

(5)
[ρ, w, j, r, r1 · · · rk, θ]

(ρ, w(j + 1), rj+1)
{ ζ(w) ∈ Σ k = rkζ(w) 1 ≤ j + 1 ≤ k

(6)
[ρ, w, j, r, r1 · · · rk, θ] [ρ, w(j + 1), rj+1, θ

′]

[ρ, w, j + 1, r, r1 · · · rk, θ ∪ θ′]
{ ζ(w) ∈ Σ k = rkζ(w)

(7)
[ρ, w, k, r, r1 · · · rk, θ]

[ρ, w, r, θ]
{ ζ(w) ∈ Σ k = rkζ(w)

(8x)
(ρ, w, r)

(qj , r)
{ ζ(w) = xj (8z)

(ρ, w, r)

(fj , r)
{ ζ(w) = zj

(9x)
(ρ, w, r) [qj , r]

[ρ, w, r, {xj 7→ r}]
{ ζ(w) = xj (9z)

(ρ, w, r) [fj , r, r
′]

(ρ, w1, r′)
{ ζ(w) = zj

(10)
(ρ, w, r) [fj , r, r

′] [ρ, w1, r′, θ]

[ρ, w, r, θ ∪ {zj 7→ (r, r′)}]
{ ζ(w) = zj

(11)
(ρ, w, r)

[ρ, w, r, {∗ 7→ r}]
{ ζ(w) = ∗

Note: whenever ρ is mentioned, we implicitly assume that
ρ = q → 〈ζζ ′, q1 · · · qm, f1 · · · fl〉 or
ρ = f(y)→ 〈ζζ ′, q1 · · · qm, f1 · · · fl〉.

Figure 3: Deductive parsing schema for the input product.

If ζ(w) ∈ Σ, with k = rkζ(w), and θj is the restriction of θ to variables below node wj in ρ, then:

W([ρ, w, r, θ]) =
∑

r1, . . . , rk :
[ρ, w1, r1, θ1], . . . , [ρ, wk, rk, θk] ∈ I

pH(r → ζ(w)
(
r1, . . . , rk

)
) ·

∏

j

W([ρ, wj, rj , θj ])

If ζ(w) ∈ {∗, x1, . . . , xm}, thenW([ρ, w, r, θ]) = 1.

If ζ(w) = {zj}, with θ(zj) = (r, r
′), thenW([ρ, w, r, θ]) =W([ρ, w1, r′, θ \ {zj 7→ (r, r

′)}])

Figure 4: Computing the weights of rules in the product grammar H � G.

21



ence rules (cf. Sec. 6), all items in I hold.
In the second step, for each [ρ, ε, r, θ] in I we

construct one rule ofH�G as in (α) or as in (β) in
Sec. 4, depending on whether ρ ∈ Rq or ρ ∈ Rf .
The third step is a reachability analysis to re-

move useless rules. This step is optional—it de-

pends on the application whether the runtime spent

here is amortized by subsequent savings.

In the fourth step, we determine the weight of

each of the rules we have. For a rule (ρ, r, θ) this is
p(ρ) ·W([ρ, ε, r, θ]), whereW is defined in Fig. 4.
The computation can be sped up by storing “back

pointers” for each item, i.e., the items which were

used for its generation. Alternatively, it is possi-

ble to compute the weights on-the-fly during the

first step, thus alleviating the need for a separate

recursive computation.

To this end, items should be prioritized to make

sure that they are generated in the right order for

the computation. To be more precise, one has to

ensure that all items referred to on the right-hand

sides of the equations in Fig. 4 are generated be-

fore the items on the left-hand sides.

6 Meaning of Items

The meaning of the items can best be illustrated by

the concepts of enriched derivation tree and partial

enriched derivation tree.

An enriched derivation tree is a modified

derivation tree in which the labels have the form

(ρ, c) for some rule ρ and some decoration map-
ping c; c maps every position of the input tree ζ
of ρ to a state of the WRTG H . Moreover, c must
be consistent with the rules of H , and positions
that coincide in the derived tree must be decorated

with the same state (cf. Fig. 5, dashed lines). A

partial enriched derivation tree (for short: pedt) is

an enriched derivation tree in which subtrees can

still be missing (represented by ⊥) or the decora-
tion with states from H is not yet complete (i.e.,
some positions are mapped to ?).
Figure 5 shows an example pedt d, where we

represent the decoration at each position of d by
annotating the corresponding input tree. This pedt

can be viewed as representing application of the

following rules:

(1), (2q), (8z), (2f), (4), (5), (4), (5), (4), (7), . . .

Now we make our description of pedts more

precise. Let n be a position of d, ρ be the rule
occurring in the label d(n), and ζ be the input

z1

S

x1 VP

V

saw

x2

NP

N

Mary

⊥

S

Adv

yesterday

∗

ρ1

ρ2
ρ5

r0

r0

r1 ?

?

?

?

r1

?

?

r0

r1 r0

r2

RH : r0 → S(r1, r3)

r1 → Adv(r2)

r2 → yesterday

...

Figure 5: Partial enriched derivation tree.

tree of ρ. Then there is a position w of ζ such
that (1) every position u which is lexicographi-
cally smaller than w is decorated by a state and,
if ζ(u) is a variable xj or zj , then the subtree of n
which corresponds to the variable does not con-

tain ⊥ or ?, and (2) every position v which is lexi-
cographically greater thanw is decorated by ? and,
if ζ(v) is a variable xj or zj , then the child of n
which corresponds to the variable is ⊥. For in-
stance, if we consider the root n = ε of the pedt
in Fig. 5, then w = 11 (i.e., the position with la-
bel x1).

Finally we can describe the meaning of the

items by referring to properties of pedts.

(q, r) (and (f, r)): There are a pedt d, a po-
sition n of d, a rule ρ, and a decoration c such
that d(n) = (ρ, c), ρ ∈ Rq (resp., ρ ∈ Rf ), and
c(ε) = r.

[q, r]: There are a pedt d, a position n of d, a
rule ρ, and a decoration c such that d(n) = (ρ, c),
ρ ∈ Rq, c(ε) = r, and c is a complete decoration.
[f, r, r′]: There are a pedt d, a position n of d, a

rule ρ, and a decoration c such that d(n) = (ρ, c),
ρ ∈ Rf , c(ε) = r, c is a complete decoration,
and c maps the ∗-labeled position of the input tree
of ρ to r′.

(ρ, w, r): There are a pedt d, a position n of d,
and a decoration c such that d(n) = (ρ, c) and
c(w) = r.

[ρ, w, r, θ]: There are a pedt d, a position n
of d, and a decoration c such that d(n) = (ρ, c),

22



c(w) = r and, if a position u below w of the input
tree ζ of ρ is labeled by xj , then c(u) = θ(xj), and
if it is labeled by zj , then (c(u), c(u1)) = θ(zj),
and if it is labeled by ∗, then c(u) = θ(∗).
[ρ, w, j, r, r1 · · · rk, θ]: There are a pedt d,

a position n of d, and a decoration c such that
d(n) = (ρ, c), c(w) = r, and, if a position u of
the input tree ζ of ρ is labeled by some variable y
and u is lexicographically smaller thanwj, c and θ
agree in the same way as in the preceding item.

Given this semantics of items, it is not difficult

to see that the inference rules of the deduction sys-

tem are sound. The completeness of the system

can be derived by means of a small proof by con-

tradiction.

7 Complexity Analysis

In this section, we analyse the worst-case space

and time complexity of step 1 of Algorithm 1.

The space complexity is

O
(
|G|in · |RH | · |P |

C
)

,

which is determined by the number of possible

items of the form [ρ, w, j, r, r1 · · · rk, θ]. The first
factor, |G|in, denotes the input size of G, defined
by

∑

ρ∈R |pos(ζ(ρ))|, where ζ(ρ) is the input tree
of ρ. It captures the components ρ,w, and j in said
items, which together identify exactly one node

of an input tree of G. The factor |RH | captures
the components r and r1 · · · rk. The final factor,
|P |C , captures the θ, where C is given at the end
of Sec. 4.

Following McAllester (2002) we determine the

time complexity by the number of instantiations of

the inference rules. In our case the time complex-

ity coincides with the space complexity.

8 Conclusion

We have introduced a formulation of STAGs that

is closed under input product and output product

with regular weighted tree languages. By the re-

sult of Maletti and Satta (2009), this implies clo-

sure under input product and output product with

regular weighted string languages. We have pro-

vided a direct construction of the STAG that gener-

ates said input product (and, mutatis mutandis, the

output product). No such construction has been

published before that deals with both weights and

synchronization. Moreover, we have presented a

novel algorithm for computing our construction.

This algorithm is inspired by Earley’s algorithm

to the effect that computation of a certain portion

of useless rules is avoided.

The next step towards an implementation would

be to consider pruning. This amounts to partition-

ing the set I of items and imposing a bound on
the size of each partition. Such a technique has

already been presented by Chiang (2007) for his

cube-pruning algorithm.

Another possible future contribution could be

an algorithm specifically tailored to the input prod-

uct with a regular weighted string language. For

this scenario, several contributions exist, requiring

additional restictions however. For instance, Nes-

son et al. (2006) show a CYK-like algorithm for

intersecting a STIG with a pair of strings. Their

algorithm requires that the trees of the grammar

be binarized. As DeNeefe and Knight (2009)

point out, this makes the grammar strictly less

powerful. They in turn propose a construction

which converts the STIG into an equivalent tree-

to-string transducer, and they use corresponding

algorithms for parsing, such as the one by DeNero

et al. (2009). However, their construction relies on

the fact that tree-insertion grammars are weakly

equivalent to context-free grammars. Thus, it is

not applicable to the more general STAGs.

Acknowledgments

We are grateful to the referees for thorough re-

marks and helpful suggestions. The first author

was financially supported by DFG VO 1011/6-1.

References

A. Abeille, Y. Schabes, A.K. Joshi. 1990. Us-

ing lexicalized TAGs for machine translation.

In Proc. of COLING, vol. 3, pp. 1–6, Helsinki,

Finland.

A. Alexandrakis, S. Bozapalidis. 1987. Weighted

grammars and Kleene’s theorem. Inform. Pro-

cess. Letters, 24(1):1–4.

B.S. Baker 1979. Composition of top-down and

bottom-up tree transductions Inform. and Con-

trol, 41(2):186–213.

A.L. Berger, V.J. Della Pietra, S.A. Della Pietra.

1996. A maximum entropy approach to natural

language processing. Comp. Ling., 22:39–71.

M. Büchse, D. Geisler, T. Stüber, H. Vogler.

2010. n-best parsing revisited. In: F. Drewes,

23



M. Kuhlmann (eds.), Proc. of Workshop on

ATANLP, pp. 46–54. ACL.

F. Casacuberta, C. de la Higuera. 2000. Computa-

tional complexity of problems on probabilistic

grammars and transducers. In: A.L. Oliveira

(ed.), Proc. of ICGI, LNAI 1891, pp. 15-24,

Springer-Verlag.

D. Chiang. 2007. Hierarchical phrase-based

translation. Comp. Ling., 33(2):201–228.

B. Courcelle, P. Franchi-Zannettacci. 1982.

Attribute grammars and recursive program

schemes I and II. Theoret. Comput. Sci. 17,

163-191 and 235-257.

A. P. Dempster, N. M. Laird, D. B. Rubin. 1977.

Maximum likelihood from incomplete data via

the EM algorithm. J. of the Royal Statistical

Society. Series B (Methodological), 39(1):1–38.

S. DeNeefe, K. Knight. 2009. Synchronous tree

adjoining machine translation. In: P. Koehn, R.

Mihalcea (eds.), Proc. of EMNLP, pp. 727–736.

ACL.

S. DeNeefe, K. Knight, H. Vogler. 2010. A de-

coder for probabilistic synchronous tree inser-

tion grammars. In: F. Drewes, M. Kuhlmann

(eds.), Proc. of Workshop on ATANLP, pp. 10–

18. ACL.

J. DeNero, M. Bansal, A. Pauls, and D. Klein.

2009. Efficient parsing for transducer gram-

mars. In Proc. NAACL, pp. 227–235.

J. Earley. 1970. An efficient context-free pars-

ing algorithm. Communications of the ACM,

13(2):94–102.

J. Eisner. 2003. Learning non-isomorphic tree

mappings for machine translation. In Proc. of

41st ACL - Volume 2, ACL, pages 205–208,

Stroudsburg, PA, USA. Association for Compu-

tational Linguistics.

J. Engelfriet. 1980. Some open questions and

recent results on tree transducers and tree lan-

guages. In: R.V. Book (ed.), Formal language

theory; perspectives and open problems, New

York, Academic Press, 241-286.

J. Engelfriet, H. Vogler. 1985. Macro tree trans-

ducers. J. Comput. System Sci. 31, 71–146.

Z. Fülöp, A. Maletti, H. Vogler. 2010. Preser-

vation of recognizability for synchronous tree

substitution grammars. In: F. Drewes,

M. Kuhlmann (eds.), Proc. of Workshop on

ATANLP, pp. 1–9. ACL.

M. Galley, M. Hopkins, K. Knight, D. Marcu.

2004. What’s in a translation rule? In:

D. Marcu, S. Dumais, S. Roukos (eds.), Proc.

of HLT-NAACL, pp. 273–280, ACL.

F. Gécseg, M. Steinby. 1997. Tree languages. In:

G. Rozenberg, A. Salomaa (eds.), Handbook of

Formal Languages, vol. 3, chapter 1, pp. 1–68.

Springer-Verlag.

J. Goodman. 1999. Semiring parsing. Comput.

Linguist., 25(4):573–605.

J. Graehl, K. Knight, J. May. 2008. Training tree

transducers. Comput. Linguist., 34(3):391–427.

S. L. Graham, M. Harrison, and W. L. Ruzzo.

1980. An improved context-free recognizer.

ACM Trans. Program. Lang. Syst., 2:415–462,

July.

L. Huang, D. Chiang. 2005. Better k-best parsing.

In Proc. of IWPT, pp. 53–64, ACL.

L. Huang, K. Knight, and A. Joshi. 2006. Statis-

tical syntax-directed translation with extended

domain of locality. In Proc. AMTA, pages 66–

73.

A.K. Joshi, L.S. Levy, M. Takahashi. 1975. Tree

adjunct grammars. Journal of Computer and

System Sciences, 10(1):136–163.

A.K. Joshi, Y. Schabes. 1997. Tree-adjoining

grammars. In: Handbook of Formal Lan-

guages. Chapter 2, pp. 69–123, Springer-

Verlag.

K. Knight. 2007. Capturing practical natural

language transformations. Machine Translation

21, 121–133.

Z. Li, J. Eisner, S. Khudanpur. 2009. Variational

decoding for statistical machine translation. In:

Proc. of ACL-IJCNLP, pp. 593–601, ACL.

A. Lopez. 2008. Statistical Machine Translation.

ACM Computing Surveys 40(3), 8:1–8:49.

24



A. Maletti. 2010a. A tree transducer model

for synchronous tree-adjoining grammars. In

J.-S. Chang and P. Koehn, eds., Proc. ACL,

pp. 1067–1076. ACL.

A. Maletti. 2010b. Input and output products for

weighted extended top-down tree transducers.

In Y. Gao, H. Lu, S. Seki, and S. Yu, eds., Proc.

DLT, LNCS, vol. 6224, pp. 316–327. Springer-

Verlag.

A. Maletti and G. Satta. 2009. Parsing algorithms

based on tree automata. In: Proc. IWPT, pp. 1–

12. ACL.

D.F. Martin and S.A. Vere. 1970. On syntax-

directed transduction and tree transducers. In:

Proc. 2nd ann. Assoc. Comput. Sci. Symp. The-

ory of Comp., pp. 129–135.

J. May and K. Knight. 2006. Tiburon: a weighted

tree automata toolkit. In O.H. Ibarra and H.-

C. Yen, editors, CIAA 2006, volume 4094 of

Lecture Notes in Comput. Sci., pages 102–113.

Springer-Verlag.

D. McAllester. 2002. On the complexity analysis

of static analyses. J. ACM, 49:512–537, July.

M. Mohri. 2009. Weighted automata algorithms.

In M. Droste, W. Kuich, and H. Vogler, edi-

tors, Handbook of Weighted Automata, chap-

ter 6, pages 213–254. Springer-Verlag.

M.-J. Nederhof. 2003. Weighted deductive pars-

ing and Knuth’s algorithm. Computational Lin-

guistics, 29(1):135–143.

M.-J. Nederhof. 2009. Weighted parsing of trees.

In: Proc. of IWPT, pp. 13–24. ACL.

R. Nesson, S.M. Shieber, A. Rush. 2005. Induc-

tion of probabilistic synchronous tree-insertion

grammars. Technical Report TR-20-05, Com-

puter Science Group, Harvard University, Cam-

bridge, Massachusetts.

R. Nesson, S.M. Shieber, A. Rush. Induction of

probabilistic synchronous tree-inserting gram-

mars for machine translation. In: Proc. of the

7th AMTA, 2006.

F.J. Och. 2003. Minimum error rate training in

statistical machine translation. In: Proc. of An-

nual Meeting of ACL, pp. 160–167.

F.J. Och, H. Ney. 2002. Discriminative train-

ing and maximum entropy models for statistical

machine translation. In: Proc. of Annual Meet-

ing of ACL, pp. 295–302. ACL.

W.C. Rounds. 1969. Context-free grammars on

trees. In: Proc. of STOC, pp. 143–148.

W.C. Rounds. 1970. Tree-oriented proofs of

some theorems on context-free and indexed lan-

guages In: 2nd Ann. ACM STOC, pp. 109–116.

S.M. Shieber. 2004. Synchronous grammars

and tree transducers. In: Proc. 7th Workshop

on Tree Adjoining Grammars and Related For-

malisms, pp. 88–95. ACL.

S.M. Shieber. 2006. Unifying synchronous tree-

adjoining grammars and tree transducers via bi-

morphisms. In: Proc. of EACL, pp. 377–384.

ACL.

S.M. Shieber, Y. Schabes. 1990. Synchronous

tree-adjoining grammars. In: Proc. of COL-

ING, vol. 3, pp. 253–258, ACL.

M. Zhang, H. Jiang, A. Aw, H. Li, C.L. Tan, S.

Li. 2008. A tree sequence alignment-based

tree-to-tree translation model. In: J.D. Moore,

S. Teufel, J. Allan, S. Furui, Proc. of Annual

Meeting of ACL, pp. 559–567. ACL.

25


