



















































Assessing the Feasibility of an Automated Suggestion System for Communicating Critical Findings from Chest Radiology Reports to Referring Physicians


Proceedings of the 15th Workshop on Biomedical Natural Language Processing, pages 181–185,
Berlin, Germany, August 12, 2016. c©2016 Association for Computational Linguistics

Assessing the Feasibility of an Automated Suggestion System for
Communicating Critical Findings from Chest Radiology Reports to

Referring Physicians

Brian E. Chapman1, Danielle L. Mowery2, Evan Narasimhan1, Neel Patel1,
Wendy W. Chapman2, Marta E. Heilbrun1

1 University of Utah, Radiology, Salt Lake City, UT
2 University of Utah, Biomedical Informatics, Salt Lake City, UT

firstname.lastname@utah.edu

Abstract

Time-sensitive communication of critical
imaging findings like pneumothorax or
pulmonary embolism to referring physi-
cians is important for patient safety. How-
ever, radiology findings are recorded in
free-text format, relying on verbal com-
munication that is not always successful.
Natural language processing can provide
automated suggestions to radiologists that
new critical findings be added to a follow-
up list. We present a pilot assessment
of the feasibility of an automated crit-
ical finding suggestion system for radi-
ology reporting by assessing suggestions
made by the pyConTextNLP algorithm.
Our evaluation focused on the false alarm
rate to determine feasibility of deploy-
ment without increasing alert fatigue. py-
ConTextNLP identified 77 critical findings
from 1,370 chest exams. Review of the
suggested findings demonstrated a 7.8%
false alarm rate. We discuss the errors,
which would be challenging to address,
and compare pyConTextNLP’s false alarm
rate to false alarm rates of similar systems
from the literature.

1 Introduction

The communication of critical imaging findings
from the radiologist to the referring physician
is a key factor in providing efficacious patient
care (Lakhani et al., 2012). Currently, the most
common form of communication is a physician-
to-physician telephone conversation, initiated by
the radiologist at the time of image interpreta-
tion. This process is tedious, inefficient, and er-
ror prone. A missed communication can result in
progressed disease, hospital readmission, and even

death. In the United States, the American College
of Radiology suggests three hallmarks of effective
methods of communication: a) supporting the or-
dering provider in providing optimal patient care,
b) using methods that are tailored to satisfy the
need for timeliness, and c) implementing methods
to minimize risk of communication errors (Ameri-
can College of Radiology, 2014). Critical findings
may result in death or severe morbidity and require
urgent or emergent attention (Larson et al., 2014).
These critical test results are often documented in
free-text imaging notes. Natural language process-
ing (NLP) can automatically extract, track, and re-
port these findings in a timely manner to support
patient safety efforts.

2 Related Work

Machine learning and ruled-based NLP techniques
have been used to detect critical information from
radiology reports to support timely communica-
tion.

Yetisgen-Yildiz et al. created a machine-
learning based text-processing pipeline that lever-
ages a maximum entropy model to identify and
classify sentences conveying clinically impor-
tant follow-up recommendations concerning un-
expected findings (Yetisgen-Yildiz et al., 2013).
Pham et al. developed an NLP pipeline to detect
and classify mentions of thromboembolic disease
from angiography and venography reports. They
used naive Bayes’ feature selection then support
vector machines and maximum entropy for clas-
sification (Pham et al., 2014). Esuli et al. de-
veloped two novel methods for extracting radio-
logical findings from reports: a cascaded, two-
stage ensemble of taggers generated by linear-
chain conditional random fields (LC-CRFs) and
a confidence-weighted ensemble method combin-
ing standard LC-CRFs and the two-stage method

181



(Esuli et al., 2013).
Rule-based approaches have also been used to

address critical finding detection. Lafourcade et
al. created a linguistic-based algorithm to detect
semantic relations between radiological findings
(Lafourcade and Ramadier, 2016). Lakhani et
al. developed an algorithm with finding-specific
negation dictionaries to identify nine critical find-
ings in impression sections and demonstrated a
mean false alarm rate of 4% (Lakhani et al., 2012).
Lacson et al. adapted and compared performance
of two NLP systems—A Nearly New Informa-
tion Extraction system (ANNIE) and Information
for Searching Content with an Ontology-Utilizing
Toolkit (iSCOUT)—to identify pulmonary nod-
ules, pneumothorax, and pulmonary embolus
with overall false alarm rates of 4% (ANNIE) and
10% (iSCOUT) (Lacson et al., 2012).

In this study, we applied a simple NLP system
that leverages regular expressions by extending the
lexicon for critical findings. We addressed a larger
set of critical findings than in prior studies and
evaluated not only the accuracy of the NLP sys-
tem, but the appropriateness of generating a sug-
gestion for critical finding communication.

Our long-term goal is to develop a communica-
tion system that identifies a variety of critical find-
ings from radiology exams, facilitates appropriate
communication of the findings to referring clini-
cians, and supports radiologist follow-up regard-
ing the communicated findings. The short-term
goal of this study is to build upon prior work by 1)
adapting an NLP algorithm to automatically iden-
tify critical findings in radiology reports and sug-
gest them to the radiologist for communication to
referring physicians, 2) assessing the false alarm
rate of the critical findings suggestion system, and
3) characterizing the errors generated by the sys-
tem to determine feasibility of deploying the sug-
gestion system in a radiology clinic. In this paper,
we limit our analysis to imaging of the chest.

3 Methods and Materials

3.1 Data Set

In this IRB-approved study, we obtained all radi-
ology reports from Oct-Dec 2013 generated by a
large medical center in the United States. We ex-
cluded non-diagnostic exams (e.g., interventional
procedures), as well as reports generated from ser-
vices other than radiology, and reports with empty
impressions sections, resulting in 54,459 exams.

Mentions of critical findings in radiology reports
are not common. Only 14,815 of the 54,479 re-
ports (27%) contained critical finding expressions
from our original knowledge base. Only a small
portion of critical finding expressions would be
expected to be observations of a new critical find-
ing, because the majority are negated or chronic
findings. For instance, from previous studies, we
found that approximately 90% of pulmonary em-
bolism mentions in radiology reports are negated.

From the 14,815 reports with critical finding
expressions, we selected approximately half of
the reports (7,176) for annotation. We built a
Flask1 web application for document-level anno-
tation of the reports. Annotators used the tool
to assign the following attributes to each finding
mentioned in a report: Existence (definite negated
existence, probable negated existence, ambivalent
existence, probable existence, definite existence)
and Historicity (new, chronic, historical) (Patel et
al., 2016).

We annotated 39 critical findings occurring
within abdomen/pelvis, chest, extremity, neuro,
and spine exams. Eighteen of these critical find-
ings were relevant to the chest: aneursym, aor-
tic dissection, cancer, ectasia, epiglottitis, frac-
ture, free air, infarct, inflammation, mediasti-
nal emphysema, pneumonia, pneumothorax,
pulmonary embolism, retropharyngeal abscess,
ruptured aneurysm, splenic infarct, tension
pneumothorax, and thrombosis.

With supervision by an attending radiologist
(Author MH), two medical students (Authors NP,
EN) independently annotated the impression sec-
tions of reports for any of 39 critical findings un-
til acceptable agreement level between annotators
was reached (>0.70). Each annotator then anno-
tated reports independently, completing two-thirds
of the 7,176 reports for a total 4,786 annotated re-
ports.

From the full set of 7,176 reports, we sampled
only reports from chest exams for this study, pro-
viding a development and test set of 1,538 chest
exam reports. We randomly selected 168 anno-
tated exams as a development set and further ex-
tended pyConTextNLP’s knowledge base by re-
viewing pyConTextNLP’s disagreements with the
annotations. We then tested on the remaining blind
set of 1,370 reports.

We split the impression section into sen-

1http://flask.pocoo.org/

182



tences using the Python text processing package
TextBlob2 then applied pyConTextNLP 3 to iden-
tify acute, positive critical findings.

3.2 Developing an automated critical findings
suggestion system with pyConTextNLP

3.2.1 pyConTextNLP
We adapted an existing NLP algorithm, pyCon-
TextNLP, to identify critical findings and their at-
tributes from radiology imaging reports (Chap-
man et al., 2011). pyConTextNLP is an exten-
sion of the NegEx (Chapman et al., 2001) and
ConText (Harkema et al., 2009) algorithms and
relies on user-defined knowledge bases of targets
(e.g., critical finding terms such as “pulmonary
embolism”), modifiers (e.g., existence terms such
as “may represent”), and lexical terms (e.g., “but”)
that terminate the scope of the modifiers.

3.2.2 Adapting and refining pyConTextNLP
The pyConTextNLP GitHub repository has a num-
ber of database files that have been created for
previous projects4. We modified existing knowl-
edge bases by comparing our automated classifi-
cations using pyConTextNLP against the annota-
tor classifications. First, we reviewed false nega-
tive findings in the development set and added new
terms to the knowledge base. The number of false
negatives in the development set was small. To
address potential alert fatigue from false alarms,
we then focused our development on evaluating
false positives in the development set. An acute,
positive critical finding was defined as a mention
of a critical finding with the following attributes:
Historicity-new and Existence-probable or definite
existence. If there was more than one mention of a
given finding in a report, we assigned the report
the same value as that of the most positive and
most new mention. In reviewing the classifica-
tions, we examined the entire pyConTextNLP doc-
ument for the report so that we could determine if
the classification error occurred due to the knowl-
edge base, classification rules, or algorithm imple-
mentation. Modifications to the code and knowl-
edge bases were made iteratively to improve pos-
itive predictive performance compared to the an-
notations. Changes to pyConTextNLP primarily

2https://textblob.readthedocs.org/
3https://pypi.python.org/pypi/

pyConTextNLP
4https://github.com/chapmanbe/

pyConTextNLP/tree/master/KB

consisted of modifying synonyms and variants for
critical findings and corresponding attributes.

3.3 Evaluating pyConTextNLP
We ran pyConTextNLP over the test set and
flagged documents with acute, positive critical
findings for review. A radiologist (Author MH)
was provided the flagged findings and their as-
sociated imaging report then asked the question,
“Would you include this critical finding in a list
of findings to communicate to the referring physi-
cian?” This question goes beyond analyzing accu-
racy of pyConTextNLP’s annotations to the more
stringent question of whether the finding should
be communicated to another physician, which de-
pends not only on accurate identification of the
finding, but also on contextual information. We
calculated precision (Eq. 1) and false alarm rate
(Eq. 2) where FP (false positive) = rejected sug-
gestion and TP (true positive) = accepted sugges-
tion.

precision =
TP

(TP + FP )
(1)

false alarm rate = 1− precision (2)
4 Results

Our primary goal was to assess the false alarm rate
of the critical findings suggestion system and to
characterize the errors generated by the system.

In total, we detected 77 findings requiring crit-
ical communication. These findings came from
only five of our 18 categories. The most prevalent
flagged findings were pneumothorax and pneu-
monia (Table 1).

Table 1: Distribution of flagged critical findings
critical finding count (%)
pneumothorax 38 (49%)
pneumonia 29 (38%)
fracture 6 (8%)
cancer 3 (4%)
aneursym 1 (1%)
total 77 (100%)

Of the 77 observed critical findings, we ob-
served 6 false positives, resulting in a false alarm
rate of 7.8% and precision of 92.2%. Of the six
false positives three were cancer, two were pneu-
monia, and one was an aneurysm.

183



5 Discussion and Conclusion

Our false alarm rate (7.8%) resides within the false
alarm rates (4%-10%) reported by (Lacson et al.,
2012), demonstrating promising results. Our false
alarm analysis revealed several challenges for the
task of critical finding identification.

For cancer, all three cases identified by pyCon-
TextNLP were considered chronic by the radiolo-
gist. One report requires coreference resolution to
determine that the tumor was not new: “Now with
multiple nodular lesions within the bilateral lungs,
demonstrating both enlarging of previously seen
nodules, and development of new nodules. This is
consistent with metastatic melanoma to the lung
parenchyma.” The other two reports didn’t con-
tain explicit linguistic cues indicating chronicity:
the radiologist inferred from context that the find-
ings were chronic. One report described two lung
lesions consistent with metastases then described
another finding, “lytic t11 lesion,” that should be
correlated with a prior MRI. In the other report,
a separate finding was described as unchanged:
“a small right apical pneumothorax persists, and
when allowing for differences in angulation, this is
either unchanged or slightly increased.” The men-
tion of a previous exam, even though not directly
in reference to the metastases or pneumothorax,
implied that the findings were identified previ-
ously. pyConTextNLPs regular-expression-based
algorithm cannot address coreference or inference.

For aneursym, error resolution would require
either mapping different findings to different lev-
els of severity or dropping the general synonym
of “dilation” for an aneurysm: “mild dilatation
of the main pulmonary artery, suggestive of pul-
monary arterial hypertension.” Identifying new
cases of pneumonia poses similar challenges: two
of 29 reports flagged with a new pneumonia were
false positives. One was due to a missed negation,
due to an implementation issue related to pruning
targets: “no focal consolidation to suggest pneu-
monia.” In the second, pneumonia was considered
present, but as a side effect of cancer and not an in-
fection that should be included in a critical finding
follow-up list.

Limitations of this study include review by
a single radiologist and only evaluating false
alarms. Based on our iterative development, py-
ConTextNLP also missed valid critical findings,
and a follow-up study will evaluate annotated re-
ports to quantify and characterize false negatives.

Successful critical finding identification relies
on negation detection, ignoring findings that are
mentioned as the reason for exam, accurate differ-
entiation of acute vs chronic findings, and model-
ing of uncertainty indicated by explicit cues (e.g.,
“may represent”) as well as by linguistic vari-
ants used to describe the observation (e.g., “patchy
opacity” vs. “pneumonia”). With a false posi-
tive rate of 7.8%, we believe pyConTextNLP could
feasibly be deployed to suggest critical findings
for communication to referring physicians without
inducing alert fatigue or irritating radiologists with
obvious errors. However, we will formally assess
this hypothesis and determine how referring physi-
cians would like to be presented with system rec-
ommendations in future user studies. Future work
will also include assessment of false negatives, ex-
tension and evaluation of all 39 critical findings
across all report types, and evaluation of execution
speed and work flow integration.

Acknowledgments

We would like to thank the anonymous review-
ers for valuable comments. This work was partly
funded by the Department of Veteran Affairs
(CRE 12-312) and University of Utah Healthcare
System Hospital Project funds.

References
American College of Radiology. 2014. ACR practice

parameter for communication of diagnostic imaging
findings.

Wendy W. Chapman, Will Bridewell, Paul Hanbury,
Gregory F. Cooper, and Bruce G. Buchanan. 2001.
A simple algorithm for identifying negated findings
and diseases in discharge summaries. Journal of
Biomedical Informatics, 34(5):301–310.

Brian E. Chapman, Sean Lee, Hyunseok Peter Kang,
and Wendy Webber Chapman. 2011. Document-
level classification of CT pulmonary angiogra-
phy reports based on an extension of the con-
text algorithm. Journal of Biomedical Informatics,
44(5):728–737.

Andrea Esuli, Diego Marcheggiani, and Fabrizio Se-
bastiani. 2013. An enhanced CRFs-based system
for information extraction from radiology reports.
Journal of Biomedical Informatics, 46(3):425 – 435.

Henk Harkema, John N. Dowling, Tyler Thornblade,
and Wendy W. Chapman. 2009. Context: An al-
gorithm for determining negation, experiencer, and
temporal status from clinical reports. Journal of
Biomedical Informatics, 42(5):839–851, Oct.

184



Ronilda Lacson, Nathanael Sugarbaker, Luciano M
Prevedello, Ivan IP, Wendy Mar, Katherine P An-
driole, and Ramin Khorasani. 2012. Retrieval of ra-
diology reports citing critical findings with disease-
specific customization. The Open Medical Infor-
matics Journal, 6:28–35.

Mathieu Lafourcade and Lionel Ramadier. 2016. Se-
mantic relation extraction with semantic patterns ex-
periment on radiology reports. In Nicoletta Cal-
zolari (Conference Chair), Khalid Choukri, Thierry
Declerck, Sara Goggi, Marko Grobelnik, Bente
Maegaard, Joseph Mariani, Helene Mazo, Asun-
cion Moreno, Jan Odijk, and Stelios Piperidis, edi-
tors, Proceedings of the Tenth International Confer-
ence on Language Resources and Evaluation (LREC
2016), Paris, France, may. European Language Re-
sources Association (ELRA).

Paras Lakhani, Woojin Kim, and Curtis P. Langlotz.
2012. Automated detection of critical results in
radiology reports. Journal of Digital Imaging,
25(1):30–36.

Paul A. Larson, Lincoln L. Berland, Brent Griffith,
Charles E. Kahn Jr, and Lawrence A. Liebscher.
2014. Actionable findings and the role of it sup-
port: Report of the acr actionable reporting work
group. Journal of the American College of Radi-
ology, 11(6):552–558, June.

Neel Patel, Evan Narasimhan, Danielle L. Mow-
ery, Wendy W. Chapman, Brian E. Chapman, and
Marta E. Heilbrun. 2016. Annotation of critical
findings from radiology reports: towards automated
communication through the electronic health record.
Portland, OR. Society for Imaging Informatics in
Medicine.

Anne-Dominique Pham, Aurélie Névéol, Thomas
Lavergne, Daisuke Yasunaga, Olivier Clément, Guy
Meyer, Rémy Morello, and Anita Burgun. 2014.
Natural language processing of radiology reports for
the detection of thromboembolic diseases and clin-
ically relevant incidental findings. BMC Bioinfor-
matics, 15:266.

Meliha Yetisgen-Yildiz, Martin L. Gunn, Fei Xia, and
Thomas H. Payne. 2013. A text processing pipeline
to extract recommendations from radiology reports.
Journal of Biomedical Informatics, 46(2):354–362,
April.

185


