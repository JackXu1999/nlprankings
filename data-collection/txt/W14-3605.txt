



















































The First QALB Shared Task on Automatic Text Correction for Arabic


Proceedings of the EMNLP 2014 Workshop on Arabic Natural Langauge Processing (ANLP), pages 39–47,
October 25, 2014, Doha, Qatar. c©2014 Association for Computational Linguistics

The First QALB Shared Task on Automatic Text Correction for Arabic

Behrang Mohit1∗, Alla Rozovskaya2∗, Nizar Habash3, Wajdi Zaghouani1, Ossama Obeid1

1Carnegie Mellon University in Qatar
2Center for Computational Learning Systems, Columbia University

3New York University Abu Dhabi
behrang@cmu.edu, alla@ccls.columbia.edu, nizar.habash@nyu.edu

wajdiz@qatar.cmu.edu,owo@qatar.cmu.edu

Abstract

We present a summary of the first shared
task on automatic text correction for Ara-
bic text. The shared task received 18 sys-
tems submissions from nine teams in six
countries and represented a diversity of ap-
proaches. Our report includes an overview
of the QALB corpus which was the source
of the datasets used for training and eval-
uation, an overview of participating sys-
tems, results of the competition and an
analysis of the results and systems.

1 Introduction

The task of text correction has recently gained a
lot of attention in the Natural Language Process-
ing (NLP) community. Most of the effort in this
area concentrated on English, especially on errors
made by learners of English as a Second Lan-
guage. Four competitions devoted to error cor-
rection for non-native English writers took place
recently: HOO (Dale and Kilgarriff, 2011; Dale
et al., 2012) and CoNLL (Ng et al., 2013; Ng et
al., 2014). Shared tasks of this kind are extremely
important, as they bring together researchers who
focus on this problem and promote development
and dissemination of key resources, such as bench-
mark datasets.

Recently, there have been several efforts aimed
at creating data resources related to the correc-
tion of Arabic text. Those include human anno-
tated corpora (Zaghouani et al., 2014; Alfaifi and
Atwell, 2012), spell-checking lexicon (Attia et al.,
2012) and unannotated language learner corpora
(Farwaneh and Tamimi, 2012). A natural exten-
sion to these resource production efforts is the cre-
ation of robust automatic systems for error correc-
tion.

* These authors contributed equally to this work.

In this paper, we present a summary of the
QALB shared task on automatic text correction
for Arabic. The Qatar Arabic Language Bank
(QALB) project1 is one of the first large scale data
and system development efforts for automatic cor-
rection of Arabic which has resulted in annota-
tion of the QALB corpus. In conjunction with the
EMNLP Arabic NLP workshop, the QALB shared
task is the first community effort for construction
and evaluation of automatic correction systems for
Arabic.

The results of the competition indicate that the
shared task attracted a lot of interest and generated
a diverse set of approaches from the participating
teams.

In the next section, we present the shared task
framework. This is followed by an overview of
the QALB corpus (Section 3). Section 4 describes
the shared task data, and Section 5 presents the ap-
proaches adopted by the participating teams. Sec-
tion 6 discusses the results of the competition. Fi-
nally, in Section 7, we offer a brief analysis and
present preliminary experiments on system com-
bination.

2 Task Description

The QALB shared task was created as a forum for
competition and collaboration on automatic error
correction in Modern Standard Arabic. The shared
task makes use of the QALB corpus (Zaghouani et
al., 2014), which is a manually-corrected collec-
tion of Arabic texts. The shared task participants
were provided with training and development data
to build their systems, but were also free to make
use of additional resources, including corpora, lin-
guistic resources, and software, as long as these
were publicly available.

For evaluation, a standard framework devel-
1http://nlp.qatar.cmu.edu/qalb/

39



Original Corrected

�éªK @QË @ �HCJ
Êj�JË @ �è
	Yë �éK @Q�̄ Y 	J« ú


�GXAª ø
 YÓ @ðPñ
�J�K B

ø
 X
ð

@ 	à@ é�<Ë @ 	áÓ ú


	æÒ�JK. �I	J» ð H. A � ú

	G @


B �éÓQ��jÖÏ @ ð

YJ
ªK. @
	Yë 	à@ @ðYJ. K
 	àA¿ ð ú
æ

�̄B@ Yj. ÖÏAK. @PðQÓ �èQÒªË@
	áºÜØ ½	K @ Èñ�®J
K. 	àA¿ �éJ
 	JÓB@ ©Ò�
 Yg ú


	̄ AÓ É¾ 	̄ ÈA 	JÖÏ @
. �éÊJ
j��Ó ½�J�
 	JÓ@ 	à


BAëñ�®�®m�'
 ¼XA 	®k


@ XA 	®k


@ 	à@ ú


	æÒ�J�K

�éªK @QË @ �HCJ
Êj�JË @ è
	Yë �èZ @Q�̄ Y 	J« ú


�GXAª øYÓ @ðPñ�J�K B
�èQÒªË@ ø
 X

ð

@ 	à


@ é�<Ë @ 	áÓ ú 	æÖ �ß


@ �I	J»ð H. A � ú


	æ 	K

B . �éÓQ��jÖÏ @ð

, ÈA 	JÖÏ @ YJ
ªK. @
	Yë 	à


@ ðYJ. K
 	àA¿ð , úæ�̄


B@ Yj. ÖÏAK. @PðQÓ

	à

@ ú 	æÒ�J�K 	à


@ 	áºÜØ ½	K


@ Èñ�®K
 	àA¿ �éJ
 	JÓ


B@ ©Ò�
 Yg@ð É¾ 	̄

. �éÊJ
j��Ó ½�J�
 	JÓ

@ 	à


B Aëñ�®�®m�'
 ¼XA 	®k


@ XA 	®k


@

lA ttSwrwA mdy1 sςAdty ςnd qrAŷh̄2 hðh̄3

AltHlylAt AlrAŷςh̄ w AlmHtrmh̄4 lÂAny6 šAb
w knt7 btmny8 mn Allh An9 Âŵdy Alςmrh̄ mr-
wrA bAlmsjd AlAqSy10 w kAn12 ybdwA13 An14

hðA bςyd AlmnAl fkl mA16 fy17 Hd18 ysmς
AlAmnyh̄19 kAn byqwl20 Ank21 mmkn ttmny23

An24 ÂHfAd ÂHfAdk yHqqwhAlÂn25 Amnytk26

mstHylh̄.

lA ttSwrwA mdý1 sςAdty ςnd qrA’h̄2 hðh3

AltHlylAt AlrAŷςh̄ wAlmHtrmh̄4.5 lÂnny6 šAb
wknt7 Âtmný8 mn Allh Ân9 Âŵdy Alςmrh̄
mrwrA bAlmsjd AlÂqSý10,11 wkAn12 ybdw13

Ân14 hðA bςyd AlmnAl,15 fkl wAHd18 ysmς
AlÂmnyh̄19 kAn yqwl20 Ânk21 mmkn Ân22

ttmný23 Ân24 ÂHfAd ÂHfAdk yHqqwhA lÂn25

Âmnytk26 mstHylh̄.
Translation

You cannot imagine the extent of my happiness when I read these wonderful and respectful analyses
because I am a young man and I wish from God to perform Umrah passing through the Al-Aqsa
Mosque; and it seemed that this was elusive that when anyone heard the wish, he would say that you
can wish that your great grandchildren may achieve it because your wish is impossible.

Table 1: A sample of an original (erroneous) text along with its manual correction and English translation.
The indices in the table are linked with those in Table 2 and the Appendix.

# Error Correction Error Type Correction Action
#1 mdy ø
 YÓ mdý øYÓ Ya/Alif-Maqsura Spelling Edit
#9 An 	à@ Ân 	à


@ Alif-Hamza Spelling Edit

#11 Missing Comma , Punctuation Add_before
#12 w kAn 	àA¿ ð wkAn 	àA¿ð Extra Space Merge
#13 ybdwA @ðYJ. K
 ybdw ðYJ. K
 Morphology Edit
#20 byqwl Èñ�®J
K. yqwl Èñ�®K
 Dialectal Edit
#25 yHqqwhAlÂn 	à


BAëñ�®�®m�'
 yHqqwhA lÂn 	à


B Aëñ�®�®m�'
 Missing Space Split

Table 2: Error type and correction action for a few examples extracted from the sentence pair in Table 1.
The indices are linked to those in Table 1 and the Appendix.

oped for similar error correction competitions is
adopted: system outputs are compared against
gold annotations using Precision, Recall and F1.
Systems are ranked based on the F1 scores ob-
tained on the test set.

After the initial registration, the participants
were provided with training and development sets
and evaluation scripts. During the test period, the
teams were given test data on which they needed
to run their systems. Following the announcement
of system results, the answer key to the test set was
released. Participants authored description papers
which will be presented in the Arabic NLP work-
shop.

3 The QALB Corpus

One of the goals of the QALB project is to create
a large manually corrected corpus of errors for a
variety of Arabic texts, including user comments
on news web sites, native and non-native speaker
essays, and machine translation output. Within the
framework of this project, comprehensive annota-
tion guidelines and a specialized web-based anno-
tation interface have been developed (Zaghouani
et al., 2014; Obeid et al., 2013).

The annotation process includes an initial au-
tomatic pre-processing step followed by an auto-
matic correction of common spelling errors by the

40



morphological analysis and disambiguation sys-
tem MADA (v3.2) (Habash and Rambow, 2005;
Habash et al., 2009). The pre-processed files are
then assigned to a team of expert (human) annota-
tors.

For a given sentence, the annotators were
instructed to correct all errors; these include
spelling, punctuation, word choice, morphology,
syntax, and dialectal usage. It should be noted that
the error classification was only used for guiding
the annotation process; the annotators were not in-
structed to mark the type of error but only needed
to specify an appropriate correction.

Once the annotation was complete, the correc-
tions were automatically grouped into the follow-
ing seven action categories based on the action
required to correct the error: {Edit, Add, Merge,
Split, Delete, Move, Other}.2 Table 1 presents
a sample erroneous Arabic news comment along
with its manually corrected form, its romanized
transliteration,3 and the English translation. The
errors in the original and the corrected forms are
underlined and co-indexed. Table 2 presents a sub-
set of the errors from the example shown in Table 1
along with the error types and annotation actions.
The Appendix at the end of the paper lists all an-
notation actions for that example.

To ensure high annotation quality, the annota-
tors went through multiple phases of training; the
inter-annotator agreement was reviewed routinely.
Zaghouani et al. (2014) report an average Word
Error Rate (WER) of 3.8% for all words (exclud-
ing punctuation), which is quite high. When punc-
tuation was included, the WER rose to 11.3%. The
high level of agreement indicates that the anno-
tations are reliable and the guidelines are useful
in producing homogeneous and consistent data.
Punctuation, however, remains a challenge.

4 Shared Task Data

The shared task data comes from the QALB cor-
pus and consists of user comments from the Al-
jazeera News webpage written in Modern Stan-
dard Arabic.

Comments belonging to the same article were

2In the shared task, we specified two Add categories: ad-
dBefore and addAfter. Most of the add errors fall into the
first category, and we combine these here into a single Add
category.

3Arabic transliteration is presented in the Habash-Soudi-
Buckwalter scheme (Habash et al., 2007): (in alphabetical
order) AbtθjHxdðrzsšSDTĎςγfqklmnhwy and the additional
symbols: ’ Z, Â


@, Ǎ @, Ā

�
@, ŵ ð', ŷ Zø', h̄ �è, ý ø.

Statistics Train. Dev. Test
Number of docs. 19,411 1,017 968
Number of words 1M 54K 51K
Number of errors 306K 16K 16K

Table 3: Statistics on the shared task data.

included only in one of the shared task subsets
(i.e., training, development or test). Furthermore,
we split the data by the annotation time. Conse-
quently, the training data is comprised of com-
ments annotated between June and December,
2013; the development data includes texts anno-
tated in December 2013; the test data includes
documents annotated in the Spring of 2014.

We refer to each comment in the shared task
data as document and assign it a special ID that
indicates the ID of the article to which the com-
ment refers and the comment’s number.

The data was made available to the participants
in three versions: (1) plain text, one document per
line; (2) text with annotations specifying errors
and the corresponding corrections; (3) feature files
specifying morphological information obtained by
running MADAMIRA, a tool for morphological
analysis and disambiguation of Modern Standard
Arabic (Pasha et al., 2014). MADAMIRA per-
forms morphological analysis and contextual dis-
ambiguation. Using the output of MADAMIRA,
we generated for each word thirty-three features.
The features specify various properties: the part-
of-speech (POS), lemma, aspect, person, gender,
number, and so on.

Among its features, MADAMIRA produces
forms that correct a large subset of a special class
of spelling mistakes in words containing the let-
ters Alif and final Ya. These letters are a source of
the most common spelling types of spelling errors
in Arabic and involve Hamzated Alifs and Alif-
Maqsura/Ya confusion (Habash, 2010; El Kholy
and Habash, 2012). We refer to these errors as
Alif/Ya errors (see also Section 6).

Table 3 presents statistics on the shared task
data. The training data contains over one mil-
lion words of text; the development and test data
contain slightly over 50,000 words each. Table 4
shows the distribution of annotations by the action
type. The majority of corrections (over 50%) be-
long to the type Edit. This is followed by mistakes
that require several words to be merged together
(about a third of all errors).

41



Data Error type (%)Edit Add Merge Split Delete Move Other
Train. 55.34 32.36 5.95 3.48 2.21 0.14 0.50
Dev. 53.51 34.24 5.97 3.67 2.03 0.08 0.49
Test 51.94 34.73 5.89 3.48 3.32 0.15 0.49

Table 4: Distribution of annotations by type in the shared task data. Error types denotes the action
required in order to correct the error.

Team Name Affiliation
CLMB (Rozovskaya et al., 2014) Columbia University (USA)
CMUQ (Jeblee et al., 2014) Carnegie Mellon University in Qatar (Qatar)
CP13 (Tomeh et al., 2014) Université Paris 13 (France)
CUFE (Nawar and Ragheb, 2014) Computer Engineering Department, Cairo University

(Egypt)
GLTW (Zerrouki et al., 2014) Bouira University (Algeria), The National Computer Sci-

ence Engineering School (Algeria), and Tabuk University
(KSA)

GWU (Attia et al., 2014) George Washington University (USA)
QCRI (Mubarak and Darwish, 2014) Qatar Computing Research Institute (Qatar)
TECH (Mostefa et al., 2014) Techlimed.com (France)
YAM (Hassan et al., 2014) Faculty of Engineering, Cairo University (Egypt)

Table 5: List of the nine teams that participated in the shared task.

Team Approach External Resources

CLMB

Corrections proposed by MADAMIRA; a Maximum Likeli-
hood model trained on the training data; regular expressions;
a decision-tree classifier for punctuation errors trained on the
training data; an SVM character-level error correction model; a
Naïve Bayes classifier trained on the training data and the Ara-
bic Gigaword corpus

Arabic Gigaword Fourth Edition (Parker et al.,
2009)

CMUQ
A pipeline consisting of rules, corrections proposed by
MADAMIRA, a language model for spelling mistakes, and a
statistical machine-translation system

AraComLex dictionary (Attia et al., 2012)

CP13

A pipeline that consists of an error detection SVM clas-
sifier that uses MADAMIRA features and language model
scores; a character-level back-off correction model imple-
mented as a weighted finite-state transducer; a statistical
machine-translation system; a discriminative re-ranker; a de-
cision tree classifier for inserting missing punctuation

None

CUFE Rules extracted from the Buckwalter morphological analyser;their probabilities are learned using the training data
Buckwalter morphological analyzer Version 2.0
(Buckwalter, 2004)

GLTW Regular expressions and word lists AraComLex dictionary (Attia et al., 2012); in-house resources; Ayaspell dictionary

GWU
A CRF model for punctuation errors; a dictionary and a lan-
guage model for spelling errors; normalization rules

AraComLex Extended dictionary (Attia et al.,
2012); Arabic Gigaword Fourth Edition (Parker
et al., 2009)

QCRI
Word errors: a language model trained on Arabic Wikipedia
and Aljazeera data; punctuation mistakes: a CRF model and a
frequency-based model trained on the shared task data

Arabic Wikipedia; Aljazeera articles

TECH

Off-the-shelf spell checkers and a statistical machine-
translation model

Newspaper articles from Open Source Arabic
Corpora; other corpora collected online; Hun-
spell

YAM

Edit errors: a Naïve Bayes classifier that uses the following fea-
tures: a character confusion matrix based on the training data; a
collocation model that uses the target lemma and the surround-
ing POS tags; a co-occurrence model that uses lemmata of the
surrounding words; Split and Merge errors: a language model
trained on the training data; Add errors: a classifier

AraComLex dictionary (Attia et al., 2012);
Buckwalter Analyzer Version 1.0 (Buckwalter,
2002); Arabic stoplists

Table 6: Approaches adopted by the participating teams.

42



5 Participants and Approaches

Nine teams from six countries participated in the
shared task. Table 5 presents the list of partici-
pating institutions and their names in the shared
task. Each team was allowed to submit up to
three outputs. Overall, we received eighteen out-
puts. The submitted systems included a diverse
set of approaches that incorporated rule-based
frameworks, statistical machine translation and
machine learning models, as well as hybrid sys-
tems. The teams that scored at the top employed
a variety of techniques and attempted to classify
the errors in some way using that classification
in developing their systems: the CLMB system
combined machine-learning modules with rules
and MADAMIRA corrections; the CUFE system
extracted rules from the morphological analyzer
and learned their probabilities using the training
data; and the CMUQ system combined statistical
machine-translation with a language model, rules,
and MADAMIRA corrections. Table 6 summa-
rizes the approaches adopted by each team.

6 Results

In this section, we present the results of the com-
petition. For evaluation, we adopted the stan-
dard Precision (P), Recall (R), and F1 metric that
was used in recent shared tasks on grammatical
error correction in English: HOO competitions
(Dale and Kilgarriff, 2011; Dale et al., 2012) and
CoNLL (Ng et al., 2013). The results are com-
puted using the M2 scorer (Dahlmeier and Ng,
2012) that was also used in the CoNLL shared
tasks.

Table 7 presents the official results of the evalu-
ation on the test set. The results are sorted accord-
ing to the F1 scores obtained by the systems. The
range of the scores is quite wide – from 20 to 67
F1 – but the the majority of the systems stay in the
50-60 range.

It is interesting to note that these results are con-
siderably higher than those shown on the similar
shared tasks on English non-native data. For in-
stance, the highest performance in the CoNLL-
2013 shared task that also used the same evalua-
tion metric was 31.20 (Rozovskaya et al., 2013).4

The highest score in the HOO-2011 shared task
(Dale and Kilgarriff, 2011) that addressed all er-

4This year CoNLL was an extension of the CoNLL-2013
competition for all errors but in its evaluation favored preci-
sion twice as much as recall, so we are not comparing to this
setting.

rors was 21.1 (Rozovskaya et al., 2011). Of
course, the setting was different,as we are deal-
ing with texts written by native speakers. But, in
addition to that, we hypothesize that our data con-
tains a set of language-specific errors that may be
“easier”, e.g Alif/Ya errors.

We also asked the participants for the outputs of
their systems on the development set. We show the
results in Table 8. While these results are not used
for ranking, since the development set was used
for tuning the parameters of the systems, it is in-
teresting to see how much the performance differs
from the results obtained on the test. In general,
we do not observe substantial differences in the
performance and the rankings, with a few excep-
tions. In particular, CP13 submissions did much
better on the development set, as well as the CUFE
system: the CUFE system suffers a major drop in
precision on the test set, while the CP13 systems
lose in recall. For more details, we refer the reader
to the system description papers.

In addition to the official rankings, it is also in-
teresting to analyze system performance for dif-
ferent types of mistakes. Note that here we are
not interested in the annotation classification by
action type. Instead, we automatically assign er-
rors to one of the following categories: punctu-
ation errors;errors involving Alif and Ya; and all
other errors. Punctuation errors account for 39%
of all errors in the data5 . Table 7 shows the perfor-
mance of the teams in three settings: with punctu-
ation errors removed; with Alif /Ya errors removed;
and when both punctuation and Alif /Ya errors are
removed. Observe that when punctuation errors
are not taken into account, the CUFE team gets
the first ranking (for each the results of the best-
performing system were chosen).

7 Analysis of System Output

We conducted a couple of experiments to analyze
the task challenges and system errors.

The Most and Least Challenging Sentences
We examined some of the most, and the least chal-
lenging parts of the test data for the shared task
systems. To identify these subsets, we ranked all
sentences using their average sentence-level F1
score and selected the top and bottom 50 sen-
tences. Our manual examination of these two

5For example, there are a lot of missing periods at the end
of a sentence that may be due to the fact that the data was
collected online.

43



Rank Team P R F1
1 CLMB-1 73.34 63.23 67.91
2 CLMB-2 70.86 62.21 66.25
3 CUFE-1 87.49 52.63 65.73
4 CMUQ-1 77.97 56.35 65.42
5 CLMB-3 71.45 60.00 65.22
6 QCRI-1 71.70 56.86 63.43
7 GWU-1 75.47 52.98 62.25
8 GWU-2 75.34 52.99 62.22
9 QCRI-2 62.86 60.32 61.57
10 YAM-1 63.52 57.61 60.42
11 QCRI-3 60.66 59.28 59.96
12 TECH-1 73.46 50.56 59.89
13 TECH-2 73.50 50.53 59.88
14 TECH-3 72.34 50.51 59.49
15 CP13-2 76.85 47.33 58.58
16 CP13-1 77.85 38.77 51.76
17 GLTW-1 75.15 23.15 35.40
18 GLTW-2 69.80 12.33 20.96

Table 7: Official results on the test set. Column 1
shows the system rank according to the F1 score.

sets shows that the differences between them re-
late to both the density and the type of errors.
The more challenging sentences (with the lowest
system performance) contain more errors in gen-
eral, and their errors tend to be complex and chal-
lenging, e.g., the correction of the erroneous two-
token string �I« �HX


@ (Âdt ςt) requires a charac-

ter deletion and a merge to produce �I«X@ (Adςt).
In contrast the less challenging sentences tend to
have fewer and simpler errors such as the common
Alif/Ya errors.

System Combination We took the 18 systems’
output and conducted two system combination ex-
periments: (a) an oracle upper-bound estimation
and (b) a simple majority vote system combina-
tion. For these experiments we isolated and eval-
uated each sentence output individually to form a
new combined system output.

In the oracle experiment, we combined differ-
ent systems by selecting the output of the best per-
forming system for each individual sentence. For
that, we evaluated sentences individually for each
system and chose the system output with the high-
est F1 score. The combined output holds the best
output of all systems for the test set. This is an
oracle system combination which allows us to es-
timate an upper-bound combination of all 18 sys-
tems.

Rank (test) Rank (dev) Team P R F1
1 2 CLMB-1 72.22 62.79 67.18
2 3 CLMB-2 69.49 61.73 65.38
3 1 CUFE-1 94.11 53.74 68.42
4 4 CMUQ-1 76.17 56.59 64.94
5 5 CLMB-3 69.71 59.42 64.15
6 6 QCRI-1 70.83 57.34 63.38
7 9 GWU-1 73.15 53.18 61.59
8 10 GWU-2 73.01 53.13 61.50
9 8 QCRI-2 62.21 61.30 61.75
10 14 YAM-1 57.81 59.19 58.49
11 12 QCRI-3 60.47 60.65 60.56
12 13 TECH-1 70.86 50.04 58.66
13 15 TECH-2 70.66 49.65 58.32
14 16 TECH-3 70.65 48.83 57.75
15 7 CP13-2 74.85 54.15 62.84
16 11 CP13-1 75.73 51.33 61.19
17 17 GLTW-1 73.83 22.80 34.84
18 18 GLTW-2 67.85 11.09 19.06

Table 8: Results on the development set.
Columns 1 and 2 show the rank of the system ac-
cording to F1 score obtained on the test set shown
in Table 7 and the development set, respectively.

In the majority vote experiment, we combined
system output based on majority vote of various
systems at sentence level. For every sentence,
we choose the output that is agreed by most sys-
tems. If all systems have different output for a sen-
tence, we back-off to the best performing system
(CLMB-1).

Table 10 compares the results of these two
experiments against the best performing system
(CLMB-1). We observe a large boost of perfor-
mance in the oracle experiment. This promis-
ing result reflects the complementary nature of the
different methods that have been applied to the
shared task, and it motivates further research on
system combination. The result for the majority-
vote system combination is very close to the
CLMB-1’s performance. This is not surpris-
ing; since, for 92% of sentences, there was no
sentence-level agreement among systems. As a re-
sult, the combined system was very close to the
back-off CLMB-1 system.

8 Conclusion

We have described the framework and results of
the first shared task on automatic correction of
Arabic, which used data from the QALB corpus.
The shared task received 18 systems submissions

44



Team No punc. errors No Alif/Ya errors No punc.No Alif/Ya errors
P R F1 P R F1 P R F1

CLMB-1 82.63 72.50 77.24 64.05 50.86 56.70 76.99 49.91 60.56
CMUQ-1 82.89 68.69 75.12 68.32 40.51 50.86 74.25 41.46 53.21
CP13-2 80.51 59.97 68.74 65.09 28.00 39.16 68.67 25.34 37.02
CUFE-1 85.22 78.79 81.88 83.34 36.21 50.48 80.63 63.25 70.89
GLTW-1 65.18 34.84 45.41 48.52 15.29 23.26 49.25 26.78 34.70
GWU-1 76.28 64.17 69.70 64.67 39.61 49.13 59.07 41.48 48.74
QCRI-1 76.74 74.93 75.82 59.66 41.90 49.23 63.22 55.10 58.88
TECH-1 81.23 62.99 70.95 59.39 34.59 43.72 64.93 35.69 46.06
YAM-1 77.38 63.99 70.05 50.77 43.43 46.81 64.63 34.71 45.17

Table 9: Results on the test set in different settings: with punctuation errors removed from evaluation;
normalization errors removed; and when both punctuation and normalization errors are removed. Only
the best output from each team is shown.

System Precision Recall F1
Oracle 83.25 68.72 75.29

Majority-Vote 73.96 62.88 67.97
CLMB-1 73.34 63.23 67.91

Table 10: Comparing the best performing system
with two experimental hybrid systems.

from nine teams in six countries. We are pleased
with the extent of participation, the quality of re-
sults and the diversity of approaches. We plan to
release the output of all systems. Such dataset and
all the methods used in this shared task are ex-
pected to introduce new directions in automatic
correction of Arabic. We feel motivated to ex-
tend the shared task’s framework and text domain
to conduct new research competitions in the near
future.

9 Acknowledgments

We would like to thank the organizing commit-
tee of EMNLP-2014 and its Arabic NLP work-
shop and also the shared task participants for their
ideas and support. We thank Al Jazeera News (and
especially, Khalid Judia) for providing the user
comments portion of the QALB corpus. We also
thank the QALB project annotators: Hoda Fathy,
Dhoha Abid, Mariem Fekih, Anissa Jrad, Hoda
Ibrahim, Noor Alzeer, Samah Lakhal, Jihene Wefi,
Elsherif Mahmoud and Hossam El-Husseini. This
publication was made possible by grants NPRP-
4-1058-1-168 and YSREP-1-018-1-004 from the
Qatar National Research Fund (a member of the
Qatar Foundation). The statements made herein

are solely the responsibility of the authors. Nizar
Habash performed most of his contribution to this
paper while he was at the Center for Computa-
tional Learning Systems at Columbia University.

References
A. Alfaifi and E. Atwell. 2012. Arabic Learner Cor-

pora (ALC): A Taxonomy of Coding Errors. In The
8th International Computing Conference in Arabic.

M. Attia, P. Pecina, Y. Samih, K. Shaalan, and J. van
Genabith. 2012. Improved Spelling Error Detection
and Correction for Arabic. In Proceedings of COL-
ING.

M. Attia, M. Al-Badrashiny, and M. Diab. 2014.
GWU-HASP: Hybrid Arabic Spelling and Punctua-
tion Corrector. In Proceedings of EMNLP Workshop
on Arabic Natural Language Processing: QALB
Shared Task.

T. Buckwalter. 2002. Buckwalter Arabic Morpho-
logical Analyzer Version 1.0.

T. Buckwalter. 2004. Buckwalter Arabic Morpho-
logical Analyzer Version 2.0.

D. Dahlmeier and H. T. Ng. 2012. Better Evaluation
for Grammatical Error Correction. In Proceedings
of NAACL.

R. Dale and A. Kilgarriff. 2011. Helping Our Own:
The HOO 2011 Pilot Shared Task. In Proceedings of
the 13th European Workshop on Natural Language
Generation.

R. Dale, I. Anisimoff, and G. Narroway. 2012. A Re-
port on the Preposition and Determiner Error Cor-
rection Shared Task. In Proceedings of the NAACL
Workshop on Innovative Use of NLP for Building
Educational Applications.

45



A. El Kholy and N. Habash. 2012. Orthographic and
morphological processing for English–Arabic sta-
tistical machine translation. Machine Translation,
26(1-2).

S. Farwaneh and M. Tamimi. 2012. Arabic Learn-
ers Written Corpus: A Resource for Research and
Learning. The Center for Educational Resources in
Culture, Language and Literacy.

N. Habash and O. Rambow. 2005. Arabic Tok-
enization, Part-of-Speech Tagging and Morphologi-
cal Disambiguation in One Fell Swoop. In Proceed-
ings of ACL.

N. Habash, A. Soudi, and T. Buckwalter. 2007.
On Arabic Transliteration. In A. van den Bosch
and A. Soudi, editors, Arabic Computational Mor-
phology: Knowledge-based and Empirical Methods.
Springer.

N. Habash, O. Rambow, and R. Roth. 2009.
MADA+TOKAN: A Toolkit for Arabic Tokeniza-
tion, Diacritization, Morphological Disambiguation,
POS Tagging, Stemming and Lemmatization. In
Proceedings of the Second International Conference
on Arabic Language Resources and Tools.

N. Habash. 2010. Introduction to Arabic Natural Lan-
guage Processing. Morgan & Claypool Publishers.

Y. Hassan, M. Aly, and A. Atiya. 2014. Arabic
Spelling Correction using Supervised Learning. In
Proceedings of EMNLP Workshop on Arabic Natu-
ral Language Processing: QALB Shared Task.

S. Jeblee, H. Bouamor, W. Zaghouani, and K. Oflazer.
2014. CMUQ@QALB-2014: An SMT-based Sys-
tem for Automatic Arabic Error Correction. In Pro-
ceedings of EMNLP Workshop on Arabic Natural
Language Processing: QALB Shared Task.

D. Mostefa, O. Asbayou, and R. Abbes. 2014. TECH-
LIMED System Description for the Shared Task on
Automatic Arabic Error Correction. In Proceedings
of EMNLP Workshop on Arabic Natural Language
Processing: QALB Shared Task.

H. Mubarak and K. Darwish. 2014. Automatic Cor-
rection of Arabic Text: a Cascaded Approach. In
Proceedings of EMNLP Workshop on Arabic Natu-
ral Language Processing: QALB Shared Task.

M. Nawar and M. Ragheb. 2014. Fast and Robust
Arabic Error Correction System. In Proceedings
of EMNLP Workshop on Arabic Natural Language
Processing: QALB Shared Task.

H. T. Ng, S. M. Wu, Y. Wu, Ch. Hadiwinoto, and
J. Tetreault. 2013. The CoNLL-2013 Shared Task
on Grammatical Error Correction. In Proceedings
of CoNLL: Shared Task.

H. T. Ng, S. M. Wu, T. Briscoe, C. Hadiwinoto, R. H.
Susanto, and C. Bryant. 2014. The CoNLL-2014
Shared Task on Grammatical Error Correction. In
Proceedings of CoNLL: Shared Task.

O. Obeid, W. Zaghouani, B. Mohit, N. Habash,
K. Oflazer, and N. Tomeh. 2013. A Web-based An-
notation Framework For Large-Scale Text Correc-
tion. In The Companion Volume of the Proceedings
of IJCNLP 2013: System Demonstrations. Asian
Federation of Natural Language Processing.

R. Parker, D. Graff, K. Chen, J. Kong, and K. Maeda.
2009. Arabic Gigaword Fourth Edition. LDC Cata-
log No.: LDC2009T30, ISBN: 1-58563-532-4.

A. Pasha, M. Al-Badrashiny, M. Diab, A. El Kholy,
R. Eskander, N. Habash, M. Pooleery, O. Rambow,
and R. Roth. 2014. MADAMIRA: A Fast, Compre-
hensive Tool for Morphological Analysis and Dis-
ambiguation of Arabic. In Proceedings of the Ninth
International Conference on Language Resources
and Evaluation (LREC).

A. Rozovskaya, M. Sammons, J. Gioja, and D. Roth.
2011. University of Illinois System in HOO Text
Correction Shared Task. In Proceedings of the Eu-
ropean Workshop on Natural Language Generation
(ENLG).

A. Rozovskaya, K.-W. Chang, M. Sammons, and
D. Roth. 2013. The University of Illinois System
in the CoNLL-2013 Shared Task. In Proceedings of
CoNLL Shared Task.

A. Rozovskaya, N. Habash, R. Eskander, N. Farra, and
W. Salloum. 2014. The Columbia System in the
QALB-2014 Shared Task on Arabic Error Correc-
tion. In Proceedings of EMNLP Workshop on Ara-
bic Natural Language Processing: QALB Shared
Task.

N. Tomeh, N. Habash, R. Eskander, and J. Le Roux.
2014. A Pipeline Approach to Supervised Error
Correction for the QALB-2014 Shared Task. In Pro-
ceedings of EMNLP Workshop on Arabic Natural
Language Processing: QALB Shared Task.

W. Zaghouani, B. Mohit, N. Habash, O. Obeid,
N. Tomeh, A. Rozovskaya, N. Farra, S. Alkuhlani,
and K. Oflazer. 2014. Large Scale Arabic Error An-
notation: Guidelines and Framework. In Proceed-
ings of the Ninth International Conference on Lan-
guage Resources and Evaluation (LREC). European
Language Resources Association (ELRA).

T. Zerrouki, K. Alhawiti, and A. Balla. 2014. Au-
tocorrection Of Arabic Common Errors For Large
Text Corpus. In Proceedings of EMNLP Workshop
on Arabic Natural Language Processing: QALB
Shared Task.

46



Appendix A: Sample annotation file

Below is the complete list of correction actions for the example in Table 1 as they appear in the training
and evaluation data. The first two columns are the error index linking to Table 1 and the original word,
respectively. Only the column titled Correction Action is in the training and evaluation data. The two
numbers following the A specify the start and end positions of the sentence token string to change.
Following that (and delimited by |||) are the action type and the correction string. The last three fields
are irrelevant to this discussion.

Error Index Original Word Correction Action
#1 ø
 YÓ A 2 3|||Edit|||øYÓ|||REQUIRED|||-NONE-|||0
#2 �éK @Q�̄ A 5 6|||Edit||| �èZ @Q�̄|||REQUIRED|||-NONE-|||0
#3 �è 	Yë A 6 7|||Edit||| è 	Yë|||REQUIRED|||-NONE-|||0
#4 �éÓQ��jÖÏ @ ð A 9 11|||Merge||| �éÓQ��jÖÏ @ð|||REQUIRED|||-NONE-|||0
#5 A 11 11|||Add_before|||.|||REQUIRED|||-NONE-|||0
#6 ú


	G @

B A 11 12|||Edit|||ú


	æ 	K

B|||REQUIRED|||-NONE-|||0

#7 �I	J» ð A 13 15|||Merge||| �I	J»ð|||REQUIRED|||-NONE-|||0
#8 ú


	æÒ�JK. A 15 16|||Edit|||ú 	æÖ
�ß@|||REQUIRED|||-NONE-|||0

#9 	à@ A 18 19|||Edit||| 	à

@|||REQUIRED|||-NONE-|||0

#10 ú
æ
�̄B@ A 23 24|||Edit|||úæ�̄


B@|||REQUIRED|||-NONE-|||0

#11 A 24 24|||Add_before|||,|||REQUIRED|||-NONE-|||0
#12 	àA¿ ð A 24 26|||Merge||| 	àA¿ð|||REQUIRED|||-NONE-|||0
#13 @ðYJ. K
 A 26 27|||Edit|||ðYJ. K
|||REQUIRED|||-NONE-|||0
#14 	à@ A 27 28|||Edit||| 	à


@|||REQUIRED|||-NONE-|||0

#15 A 31 31|||Add_before|||,|||REQUIRED|||-NONE-|||0
#16 AÓ A 32 33|||Delete||||||REQUIRED|||-NONE-|||0
#17 ú


	̄
A 33 34|||Delete||||||REQUIRED|||-NONE-|||0

#18 Yg A 34 35|||Edit|||Yg@ð|||REQUIRED|||-NONE-|||0
#19 �éJ
 	JÓB@ A 36 37|||Edit||| �éJ
 	JÓ


B@|||REQUIRED|||-NONE-|||0

#20 Èñ�®J
K. A 38 39|||Edit|||Èñ�®K
|||REQUIRED|||-NONE-|||0
#21 ½	K@ A 39 40|||Edit|||½	K


@|||REQUIRED|||-NONE-|||0

#22 A 41 41|||Add_before||| 	à

@|||REQUIRED|||-NONE-|||0

#23 ú

	æÒ�J�K A 41 42|||Edit|||ú 	æÒ�J�K|||REQUIRED|||-NONE-|||0

#24 	à@ A 42 43|||Edit||| 	à

@|||REQUIRED|||-NONE-|||0

#25 	à

BAëñ�®�®m�'
 A 45 46|||Split||| 	à


B Aëñ�®�®m�'
|||REQUIRED|||-NONE-|||0

#26 ½�J�
 	JÓ@ A 46 47|||Edit|||½�J�
 	JÓ

@|||REQUIRED|||-NONE-|||0

47


