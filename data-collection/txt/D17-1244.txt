



















































Dimensions of Interpersonal Relationships: Corpus and Experiments


Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 2307–2316
Copenhagen, Denmark, September 7–11, 2017. c©2017 Association for Computational Linguistics

Dimensions of Interpersonal Relationships: Corpus and Experiments

Farzana Rashid and Eduardo Blanco
Human Intelligence and Language Technologies Lab

University of North Texas
Denton, TX, 76203

farzanarashid@my.unt.edu, eduardo.blanco@unt.edu

Abstract

This paper presents a corpus and exper-
iments to determine dimensions of inter-
personal relationships. We define a set of
dimensions heavily inspired by work in so-
cial science. We create a corpus by retriev-
ing pairs of people, and then annotating
dimensions for their relationships. A cor-
pus analysis shows that dimensions can be
annotated reliably. Experimental results
show that given a pair of people, values to
dimensions can be assigned automatically.

1 Introduction

The task of information extraction (IE) consists in
creating structured representations from unstruc-
tured text. These representations usually consist
of relations explicitly stated in text, and involve
two or more arguments. For example, IE sys-
tems would extract SPOUSE(John, Mary) or MAR-
RIED(John, Mary, 1994 ) from John and Mary
have been married since 1994. IE systems have
a long history, and became popular after eval-
uations such as MUC (Grishman and Sundheim,
1996) and ACE (Doddington et al., 2004).

Traditional IE systems are supervised and ex-
tract relations defined before training takes place
(Peng and McCallum, 2004). More recently, open
IE systems have been proposed to extract all re-
lations explicitly stated in text in an unsuper-
vised manner and without defining relations a
priori (Mausam et al., 2012). Regarding inter-
personal relations—relations that take as argu-
ments two people—both IE approaches extract re-
lations such as RELATIVE, FRIEND and COMMU-
NICATES WITH. Open IE systems are domain in-
dependent and would extract, in principle, rela-
tions such as CLASSMATES and ADVISOR from
students’ diaries or biographies of scientists.

While useful for applications such as ques-
tion answering (Yao and Van Durme, 2014), these
dyadic relations only provide a generic under-
standing of the relationship between two people.
For example, COMMUNICATES WITH may relate
people who have an intense or superficial rela-
tionship (e.g., engaged couples talking about wed-
ding plans vs. home owners discussing remodels
with contractors), pleasure- or task-oriented rela-
tionships (e.g., friends planning a backpacking trip
vs. software developers discussing the next deliv-
ery), and may be spatially near or distant (e.g.,
inviduals having an in-person meeting vs. those
exchanging emails or talking on the phone).

These elemental properties of interpersonal re-
lationships are called dimensions in social science,
and have been studied for decades (Wish et al.,
1976). In those studies, the goal is to understand
how relationships are perceived by people, not to
extract them. Note that unlike interpersonal re-
lationships, their dimensions are usually implic-
itly stated in text, thus extracting them is chal-
lenging. Also, extracting dimensions of interper-
sonal relationships requires text understanding be-
yond the event in which two people participate.
As shown above, two people who communicate
may have different dimension values depending on
what they talk about or the communication device.

In this paper, we target dimensions of inter-
personal relationships that characterize the nature
of relationships beyond a name per relationship.
The main contributions are: (a) set of dimensions
of interpersonal relationships, including dimen-
sions from previous work outside computational
linguistics and novel ones; (b) corpus consisting
of pairs of people and values for the dimensions of
their relationships (Cohen’s kappa: 0.68); (c) de-
tailed corpus analysis; and (d) experimental results
showing that the dimensions can be extracted au-
tomatically from text.

2307



Dimension Other descriptors and aliases Ref.
Cooperative vs. Competitive friendly vs. hostile, promotive vs. contrient [1]

Equal vs. Hierarchical autonomy vs. control, submission vs. dominance [1]
Intense vs. Superficial important vs. insignificant, influential vs. trivial [1]

Pleasure vs. Task Oriented personal vs. impersonal, emotionally involved vs. detached [1]
Active vs. Passive direct vs. indirect, unequivocal vs. equivocal [2]

Intimate vs. Unintimate emotionally close vs. distant, indifferent, random [3]
Temporary vs. Enduring momentary vs. lasting, provisional vs. permanent [4]
Concurrent vs. Nonconcurrent convergent vs. divergent, synchronous vs. asynchronous New

Spatially Near vs. Distant nearby vs. faraway, attached vs. detached New

Table 1: Dimensions of interpersonal relationships targeted in this paper. [1] stands for (Wish et al.,
1976), [2] for (Kelley, 2013), [3] for (Adamopoulos, 2012), and [4] for (Deutsch, 2011). New indicates
a dimension discovered after analyzing several examples and pilot annotations.

2 Related Work

Extracting relations between entities such as
people, organizations and locations is the core
goal of the task of information extraction. A
few competitions have served as evaluation
benchmarks (Grishman and Sundheim, 1996;
Doddington et al., 2004; Kulick et al., 2014;
Surdeanu and Heng, 2014), and include interper-
sonal relationships such as BUSINESS, SPOUSE
and CHILDREN. Aguilar et al. (2014) compare
several evaluations, and automated approaches
to relation extraction—also referred to as link
prediction and knowledge base completion—
include (Yu and Lam, 2010; Nguyen et al., 2016;
West et al., 2014). Open information extraction
(Wu and Weld, 2010; Angeli et al., 2015) has
emerged as an unsupervised domain-independent
approach to extract relations. Regardless of
details, all these previous efforts extract explicit
relations, and do not attempt to characterize
instances of relations with dimensions.

Besides extracting relations per se, there
have been efforts within computational lin-
guistics involving interpersonal relationships.
Voskarides et al. (2015) extract human-readable
descriptions of relations in a knowledge graph
by ranking sentences that justify the relations.
Iyyer et al. (2016) propose an unsupervised algo-
rithm to extract relationship trajectories of fic-
tional characters, i.e., how interpersonal rela-
tionships evolve over time in fictional stories.
Bracewell et al. (2012) introduce 9 social acts
(e.g., agreement, undermining) designed to char-
acterize relationships between individuals exhibit-
ing adversarial and collegial behavior (similar to
our cooperative vs. competitive dimension).

Researchers have studied from a com-
putational perspective how people com-

municate with each other. For example,
Danescu-Niculescu-Mizil et al. (2012) study how
power differences affects language style in online
communities, and Prabhakaran and Rambow
(2014) present a classifier to detect power re-
lationships in email threads. Similarly, Gilbert
(2012) explores how people in hierarchical
relationships communicate through email, and
Bramsen et al. (2011) focus on identifying
power relationships in social networks. Po-
liteness in online forums has also been studied
(Danescu-Niculescu-Mizil et al., 2013). While
power (similar to our equal vs. hierarchical
dimension, Section 3) and politeness could be
considered dimensions, these works exploit
structural and linguistic features derived from
communications between two individuals. Unlike
all of them, we extract 9 dimensions of interper-
sonal relationships from sentences describing an
event involving two people, and without needing
language samples from them.

3 Dimensions of Interpersonal
Relationships

Dimensions of interpersonal relationships have
been studied for decades outside of computational
linguistics, mostly in psychology and social sci-
ence in general (Wish et al., 1976). The set of di-
mensions is by no means agreed upon, and neither
is the terminology to refer to what apparently is
the same dimension. For example, the terms domi-
nance, submission, potency, autonomy and control
are used to describe the distribution of power in a
relationship (Deutsch, 2011).

The dimensions we work with in this paper are
primarily borrowed from previous works in social
science, although we add two new dimensions. In-
terestingly, the previous works which define these
dimensions do so from a theoretical point of view

2308



or after conducting experiments with subjects to
reveal how they perceive interpersonal relation-
ships. The latter was done using multidimensional
scaling analysis after subjects compared 25 rela-
tionships, e.g., between a parent and child, be-
tween business partners (Wish et al., 1976).

Table 1 presents the nine dimensions targeted
in this paper along with the original references
and aliases found in the literature. Social scien-
tists have proposed additional dimensions, e.g.,
voluntary vs. involuntary, public vs. private and
licit vs. illicit (Deutsch, 2011), or self-benefiting
vs. service-oriented (Adamopoulos, 2012). We
discarded these additional dimensions because we
discovered that they are not applicable to most
pairs of people we work with (Section 4).

We provide below brief descriptions of the nine
dimensions of interpersonal relationships we work
with. Note that these dimensions are not com-
pletely independent, for example, enduring rela-
tionships are usually intense and intimate, and in-
tense and pleasure-oriented relationship are al-
most always intimate. Section 4.3 presents exam-
ples, and Section 4.4 discusses inter-dimensional
correlations and inter-annotator agreement. We
point the reader to the references in Table 1 for
additional details, rationales, and examples.

Cooperative vs. Competitive A relationship is
cooperative if both people (a) have a common
interest or goal, (b) like each other, (c) bene-
fit from the relationship, or (d) think alike or
have similar views. Otherwise, the relation-
ship is competitive.

Equal vs. Hierarchical A relationship is equal if
both people (a) have the same social sta-
tus, (b) are at the same level in the power
structure, (c) share similar responsibilities, or
(d) have the same role. Otherwise, the rela-
tionship is hierarchical.

Intense vs. Superficial A relationship is intense
if both people interact with each other fre-
quently, i.e., they are involved repeatedly.
Otherwise, the relationship is superficial.

Pleasure vs. Task Oriented A relationship is
pleasure oriented if both people interact so-
cially and their relationship is not bound by
professional rules or regulations. Otherwise,
the relationship is task oriented.

Active vs. Passive A relationship is active if both
people are involved in a shared activity or
event that grants the relationship. Otherwise,

the relationship is passive. For example, in-
dividuals commuting to work in the same car
have an active relationship, but those who
happen to take the same subway line to work
have a passive relationship.

Intimate vs. Unintimate A relationship is inti-
mate if both people are emotionally close and
warm to each other. Otherwise, the relation-
ship in unintimate.

Temporary vs. Enduring A relationship is tem-
porary if it lasts less than a day. A relation-
ship is enduring if it lasts over a month. Oth-
erwise (if it lasts more than a day and less
than a month), this dimension is undefined.

Concurrent vs. Nonconcurrent A relationship
is concurrent if both people are involved
in an event or action at the same time.
Otherwise, the relationship is nonconcurrent.

Spatially Near vs. Distant A relationship is spa-
tially near (or near for short) if both people
are at the same location during the event that
grants the relationship. Otherwise, the rela-
tionship is spatially distant (or distant).

4 Building a Corpus of Dimensions of
Interpersonal Relationships

Existing corpora annotating relations (Section 2)
only consider selected interpersonal relationships
and do not target dimensions. Our goal is to tar-
get dimensions of interpersonal relationships be-
tween any two individuals, from weak links (e.g.,
journalists interviewing celebrities) to strong ties
(e.g., close friends). Thus, we create a corpus1 by
first retrieving pairs of people, and then annotating
dimensions for their relationships.

We decided to add our annotations to
OntoNotes (Hovy et al., 2006). Doing so has
several advantages. First, OntoNotes contains
texts from several domains and genres (e.g.,
conversational telephone speech, weblogs, broad-
cast), thus we not only work with newspaper
articles. Second, OntoNotes includes part-of-
speech tags, named entities and coreference
chains, three annotation layers that allow us to
streamline the corpus creation process.

4.1 Retrieving Pairs of People

We retrieve pairs of people within each sentence
in OntoNotes following four steps:

1Available at http://hilt.cse.unt.edu/

2309



Figure 1: Frequencies of the top 20 most frequent verbs after retrieving pairs of people (Section 4.1). We
discard verbs with frequency <4, and randomly select up to 26 pairs per verb for a total of 1,048 pairs.

1. Collect all instances of personal pronouns
(part-of-speech tag PRP) I, he and she.

2. Collect all named entities PERSON.
3. Keep one mention per coreference chain, giv-

ing priority to named entities over pronouns.
4. Generate combinations of 2 elements from

the union of the pronouns and named entities
subject to the following constraints: at least
(a) one is a PERSON named entity, and
(b) one is the nsubj (nominal subject syntac-

tic dependency) of a verb.
The elements of the pair that satisfy restric-
tions (4a) and (4b) need not be the same.

Note that removing duplicate mentions (Step 3)
does not reduce the number of relationships tar-
geted, it simply avoids duplicate pairs. Also, the
only syntactic constraint is that one person in the
pair must be the nominal subject of a verb. Thus,
we work with relationships between individuals
from different clauses, and connected with a va-
riety of syntactic paths (see Examples in Table 2).

The total number of pairs generated skipping
Steps 3 and 4 would be 4,886. After removing du-
plicate mentions (Step 3), the number is reduced
to 3,481; restrictions (4a) and (4b) further reduce
the number to 3,143 and 2,696 respectively. Exe-
cuting all steps yields 2,364 pairs.

Figure 1 presents verb frequencies for the top 20
most frequent verbs in the 2,364 pairs. In order to
reduce the annotation effort and account for a vari-
ety of verbs, we set to annotate 1,000 pairs. After
trying several thresholds, we retrieved 1,048 pairs
by selecting pairs from verbs that occur at least 4
times, and randomly selecting up to 26 pairs per
verb (most verbs occur less than 26 times).

4.2 Annotating Dimensions of Interpersonal
Relationships

After generating pairs, annotators determine val-
ues for each dimension of interpersonal relation-
ships. The annotation interface shows the sentence

from which the pair was generated, and the pre-
vious and next sentence to provide some context.
The pair of people of interest were highlighted,
but no additional information was shown (e.g., the
verb of which one person is the subject).

Annotators assign a value to each dimension
based on the relationship between the two individ-
uals at the time the verbal event of which one of
the individuals is the subject takes place. They
were trained to take into account context (previ-
ous and next sentences), and to interpret the text
as they normally would. Therefore, they assign
values using world knowledge that may not be ex-
plicitly stated in the text. For example, two peo-
ple talking on the phone would have a spatially
distant relationship because (most likely) they are
not next to each other while talking. Annotating
the changes over time of the dimensions is outside
the scope of this paper.

During the first batch of annotations, we discov-
ered that for a given pair of people, dimensions
sometimes cannot be determined because (a) there
is not enough evidence in the text provided (i.e.,
sentence from which the pair was generated, pre-
vious and next sentences) or (b) the pair is invalid
and assigning dimensions is nonsensical. We use
0 label in the former case, and inv in the latter.
For example, in the sentence [He]y [criticized]verb
[Ken Starr]x, the value for the dimension spatially
near (vs. distant) was marked 0 as there is not
enough information to determine whether He and
Ken Starr are at the same location when criticized
took place. The most frequent example of inv is
when God is marked as a PERSON named entity
in the gold annotations in OntoNotes.

In the rest of the paper, we refer to dimensions
by the first descriptor in Table 1, and use 1 if the
first descriptor of a dimension is true, and -1 if
the second descriptor is true. For example label
-1 applied to dimension temporary means that the
realtionship is enduring.

2310



Sentence C
oo

pe
ra

tiv
e

E
qu

al

In
te

ns
e

P
le

as
ur

e
O

ri
en

te
d

A
ct

iv
e

In
ti

m
at

e
Te

m
po

ra
ry

C
on

cu
rr

en
t

S
pa

ti
al

ly
N

ea
r

1 [Cheney]x [got]verb a telephone call from his democratic counterpart [Joseph
Lieberman]y wishing him a speedy recovery.

1 1 -1 -1 1 -1 1 1 -1

2 [. . . ] [I]x [interviewed]verb one of the nation’s top jockies [Shane Sellers]y about
the battle he waged everyday to control his weight.

1 -1 -1 -1 1 -1 1 1 1

3 [I]x have always remembered the encouragement which Mr. [Yu Youren]y
[gave]verb me as a young reporter. He said that to be a fearless champion of
social justice, as is expected of a journalist, the [. . . ]

1 -1 1 -1 1 1 -1 1 1

4 One day, Dingxiang took the opportunity to again urge him to change his ways
[. . . ]. After this, [Zhang Sheng]x [threw]verb out [Dingxiang]y , sold off the family
possessions, and spent his days living a life of dissipation.

-1 1 -1 1 1 -1 -1 1 1

Table 2: Annotation examples for pairs of people (x, y). We refer to dimensions by their first descriptor
(Section 3); 1 (-1) indicates that the first (second) descriptor is true, and 0 that the value is unknown.

69.9%

20.9%

26.4%

65.9%

27.4%

66.2%

14.4%

79.3%

58.4%

35.3%

Cooperative Equal Intense Pleasure Oriented Active

15%

79.3%
80.1%

14.5%

46.4%

47.1%

40.4%

51.4%

1

-1

Intimate Temporary Concurrent Spatially Near

Figure 2: Label distribution per dimension of interpersonal relationships. The missing portion of each
pie chart corresponds to labels 0 and inv, which always amount to less than 5% each.

4.3 Annotation Examples

We present annotation examples from our cor-
pus in Table 2, including context if it is relevant.
We acknowledge that some annotations are am-
biguous, and discuss label distributions and inter-
annotator agreement in Section 4.4.

Sentences (1) and (2) encode a COMMUNICA-
TION relationships between two individuals, and
both are cooperative, superficial, work oriented,
active, unintimate, temporary, and concurrent.
The values for two dimensions, however, are dif-
ferent. Two counterparts (Sentence 1) are at the
same level in the power structure (equal), but inter-
viewer and interviewee are not (Sentence 2). Sim-
ilarly, talking on the phone entails that the individ-
uals are spatially distant (Sentence 1), but inter-

viewing (most likely) means that they were spa-
tially near (Sentence 2). One could argue that
0 would be a better label for spatially near in
Sentence (2), but annotators interpreted that inter-
viewed refers to an in-person interview.

Sentence (3) in context describes one person
(Yu Youren) encouraging another one (I). Annota-
tors indicate that this relationship, unlike the ones
in Sentences (1) and (2), is intense (frequent inter-
action), intimate (emotionally close), and endur-
ing (lasting over a month). These values are not
explicitly stated, but they are understood given the
long-lasting impact Yu Youren had on I.

Finally, Sentence (4) exemplifies a competitive
relationship. The context describes a struggling
relationship between Dingxiang and Zhang Sheng.
When the latter threw the former out, the rela-

2311



C
oo

pe
ra

tiv
e

E
qu

al

In
te

ns
e

P
le

as
ur

e
O

r.

A
ct

iv
e

In
ti

m
at

e

Te
m

po
ra

ry

C
on

cu
rr

en
t

Cooperative –
Equal -.06 –
Intense .24 .11 –
Pleasure Or. .14 .29 .39 –
Active .18 .22 .43 .25 –
Intimate .18 .17 .59 .62 .31 –
Temporary -.17 .10 -.56 -.43 -.27 -.61 –
Concurrent .17 .22 .31 .21 .74 .26 -.17 –
Spat. Near .21 .18 .30 .25 .68 .28 .19 .89

Table 3: Pearson correlations between dimensions
of interpersonal relationships in our corpus.

Dimension Agreement κ coefficient
Cooperative 86% 0.74
Equal 81% 0.63
Intense 84% 0.73
Pleasure Oriented 86% 0.70
Active 82% 0.59
Intimate 83% 0.68
Temporary 76% 0.61
Concurrent 84% 0.72
Spatially Near 80% 0.67
All 82% 0.68

Table 4: Inter-annotator agreement per dimension
of interpersonal relationships. κ values in the
0.60–0.80 range are considered substantial, over
0.80 would be perfect (Landis and Koch, 1977).

tionship was superficial and unintimate, but (most
likely) existed for longer than a month (enduring).

4.4 Corpus Analysis

Label Distribution. Figure 2 shows the percent-
age of label 1 (first descriptor) and -1 (second de-
scriptor) per dimension in our corpus. The per-
centage of Label 0 ranges from 0.86% (tempo-
rary) to 4.3% (cooperative) depending on the di-
mension, and the percentage of inv is 4.6% (not
shown in Figure 2). Importantly, annotators as-
signed a useful value (either 1 or -1) to most pairs
of people (>90%) for all dimensions.

The distributions of 1 and -1 clearly show that
most dimensions are biased towards one label. For
example, few relationships are pleasure oriented
(14.4%) or enduring (14.5%). The exceptions are
concurrent vs. nonconcurrent, with roughly the
same percentages (46.4% and 47.1%), spatially
near vs. distant (40.4% and 51.4%) and active vs.
passive (58.4% and 35.3%). These distributions
are not a representative sample of all interpersonal
relationships, we would expect many pleasure ori-

ented and intimate relationship if we work with
personal diaries instead of OntoNotes.

Inter-Dimension Correlations. While the dimen-
sions we work with have a long tradition in social
science (Section 2), to the best of our knowledge,
they have not been extensively annotated in text
before. Table 3 shows inter-dimensional correla-
tions for all pairs of dimensions in our corpus. Not
surprisingly, some dimensions correlate with each
other. For example, enduring relationships tend
to also be intense (0.56) and intimate (0.61), and
concurrent relationships tend to be active (0.74).
The highest correlation is between concurrent and
spatially near (0.89), indicating that if two people
participate in a common event at the same time,
usually they are at the same location (see coun-
terexample in Table 2, Sentence 1). Note, how-
ever, that most correlations are low, and some di-
mensions (e.g., cooperative, equal) have low cor-
relations (<0.30) with all dimensions.

Inter-Annotator Agreement. The annotations
were done by two graduate students. They started
annotating small batches of pairs of people, and
discussed disagreements with each other. After
several iterations, they annotated independently
10% of all pairs of people generated. Table 4
depicts the inter-annotator agreements obtained.
Overall Cohen’s kappa coefficient is 0.68, and the
coefficients range between 0.59 to 0.74 depend-
ing on the dimension. Note that kappa coefficients
in the range 0.60–0.80 are considered substantial,
and over 0.80 would be perfect (Landis and Koch,
1977). Given these high agreements, the rest of
pairs were annotated once.

5 Experiments and Results

We conduct experiments using standard super-
vised machine learning. Each pair of people be-
come an instance, and we split instances into train-
ing (80%) and test (20%). As a learning algorithm,
we use SVM with RBF kernel as implemented in
scikit-learn (Pedregosa et al., 2011).

We report results in the test set after tuning the
SVM parameters (C and γ) using 10-fold cross-
validation with the training set. More specifically,
we train one classifier per dimension, and exper-
iment with all instances but the ones annotated
inv. Thus, each classifier predicts 3 labels: 1 (the
first descriptor applies), -1 (the second descriptor
applies), and 0 (neither descriptor applies).

2312



Feature Description

Verb

word, tag Word form and part-of-speech tag of the verb
dep out Outgoing syntactic dependency from verb
deps in Flags indicating incoming syntactic dependencies to the verb
lex name Name of the WordNet lexical file of the verb
token before Word form and part-of-speech tag of the token before the verb
token after Word form and part-of-speech tag of the token after the verb

Person

words, tags Concatenation of word forms and part-of-speech tags
type Flag indicating whether the person is a pronoun or named entity
dep out Outgoing syntactic dependency
distance verb Number of tokens between the person and the verb
first token Word form and part-of-speech tag of the first token in the person
last token Word form and part-of-speech tag of the last token in the person
token before Word form and part-of-speech tag of the token before the person
token after Word form and part-of-speech tag of the token after the person

Personx Persony
direction Flag indicating whether x occurs before or after y
type Flag indicating whether x and y are PERSON NEs, or either one is a pronoun

Table 5: Feature set used to determine dimensions of interpersonal relationships between pairs of people
(x, y). Verb features are extracted from the verb of which either x or y is the subject, Person features are
extracted from x and y independently, and Persons features are extracted from x and y.

5.1 Feature Set

The features we work with are summarized in Ta-
ble 5. Most features are standard and have been
used before to extract relations from text (Section
2). Following the notation in Table 2, we refer to
the pair of people as x and y.

Verb features capture information about the
verb to which x or y attach. We include words
and part-of-speech tags (verb, and tokens before
and after), the name of the WordNet lexical file to
which the verb belongs, and dependencies.

Person features are extracted from x and y inde-
pendently, and consists mostly of words and part-
of-speech tags. We also include a flag indicating
whether the person is a pronoun or named entity
(type feature), and the number of tokens between
the person and the verb (distance verb).

Personx Persony features capture information
of both x and y. They capture (a) whether x occurs
before or after y in the sentence, and (b) whether
they are both named entities or one is a pronoun
and the other one a named entity (type feature).

5.2 Results

We present overall results (averages of the classi-
fiers for each dimension) using the majority base-
line and with several feature combinations in Table
6. Then, we present detailed results per dimension
with the best feature combination in Table 7. We
only present results obtained in the test set.
Overall Results and Feature Ablation (Table
6). The majority baseline obtains 0.53 aver-
age F-measure. Recall that we build a classifier
per dimension, thus the combination of the nine

Features Label P R F

Majority
Baseline

1 0.43 0.76 0.55
0 0.00 0.00 0.00
-1 0.57 0.59 0.64
Avg. 0.45 0.65 0.53

Verb

1 0.62 0.72 0.64
0 0.00 0.00 0.00
-1 0.71 0.78 0.65
Avg. 0.62 0.71 0.65

Verb, Personx

1 0.68 0.75 0.69
0 0.00 0.00 0.00
-1 0.78 0.79 0.78
Avg. 0.70 0.74 0.71

Verb, Persony

1 0.64 0.72 0.67
0 0.00 0.00 0.00
-1 0.74 0.76 0.74
Avg. 0.66 0.70 0.67

Verb, Personx,
Persony ,
Personx Persony

1 0.69 0.74 0.70
0 0.00 0.00 0.00
-1 0.77 0.80 0.76
Avg. 0.71 0.76 0.72

Table 6: Results obtained for all dimensions with
several combinations of features.

majority-baseline classifiers predict two labels: 1
(0.55 F-measure) and -1 (0.64 F-measure).

Models trained with any combination of fea-
tures outperform the majority baseline, but they
never learn to predict label 0. Since 0 occurs be-
tween 0.86% and 4.3% depending on the dimen-
sion (Section 4.4), this limitation does not affect
overall performance substantially.

2313



Dimension
1 (1st descriptor) 0 (unknown) -1 (2nd descriptor) All
P R F P R F P R F P R F

Cooperative 0.73 0.96 0.83 0.00 0.00 0.00 0.60 0.19 0.29 0.66 0.72 0.65
Equal 0.56 0.10 0.17 0.00 0.00 0.00 0.74 0.97 0.84 0.68 0.74 0.66
Intense 0.39 0.30 0.34 0.00 0.00 0.00 0.78 0.85 0.82 0.67 0.71 0.69
Pleasure 0.40 0.28 0.33 0.00 0.00 0.00 0.87 0.93 0.90 0.79 0.82 0.80
Active 0.69 0.85 0.76 0.00 0.00 0.00 0.68 0.51 0.58 0.67 0.69 0.67
Intimate 0.44 0.17 0.24 0.00 0.00 0.00 0.88 0.98 0.93 0.81 0.86 0.83
Temporary 0.85 0.96 0.91 0.00 0.00 0.00 0.33 0.10 0.16 0.77 0.83 0.79
Concurrent 0.72 0.80 0.76 0.00 0.00 0.00 0.77 0.75 0.76 0.71 0.74 0.73
Spat. Near 0.66 0.68 0.67 0.00 0.00 0.00 0.73 0.79 0.76 0.66 0.70 0.68
Average 0.69 0.74 0.70 0.00 0.00 0.00 0.77 0.80 0.76 0.71 0.76 0.72

Table 7: Results obtained for each dimension with the best combination of features for all dimensions
(Verb + Personx + Persony + Personx Persony , boldfaced in Table 6)

Verb features alone yield a 0.65 average F-
measure (1: 0.64, -1: 0.65). Adding features de-
rived from x (Verb + Personx) improves perfor-
mance (0.71 average F-measure), and adding fea-
tures derived from y (Verb + Persony) slightly im-
proves performance (0.67 average F-measure). In
both cases, -1 is predicted more accurately than 1
(0.78 vs. 0.69 and 0.74 vs. 0.67).

Finally, adding all features (Verb + Personx +
Persony + Personx Persony) yields the best results
(average F-measure: 0.72), although by a minimal
margin with respect to Verb + Personx.
Detailed Results per Dimension. Table 7
presents results per dimension with the best over-
all combination of features (Verb + Personx +
Persony + Personx Persony).

All dimensions obtain overall F-measures be-
tween 0.65 and 0.83 (last column). Results per
label are heavily biased towards the most frequent
label per dimension (Figure 2), although it is the
case that the models we experiment with predict
both 1 and -1 for all dimensions. As stated above,
none of them predict 0, but this limitation does
not substantially penalize overall performance be-
cause of the low frequency of this label.

The model obtains the same F-measures for 1
and -1 with concurrent dimension (0.76), and the
labels of this dimension are virtually distributed
uniformly (46.4% vs. 47.1%, Figure 2). Similarly,
F-measures for 1 and -1 with spatially near and
active dimensions are similar (0.67 vs. 0.76 and
0.76 vs. 0.58), and the labels are distributed rela-
tively evenly in our corpus (40.4% vs 51.4% and.
58.4% vs. 35.3%).

Finally, F-measures per label with other dimen-

sion are biased towards the most frequent label.
For example, only 15% of all pairs of people have
an enduring relationship (Figure 2), and the F-
measure for 1 with temporary dimension is much
higher (0.91) that for -1 (0.16).

6 Conclusions

We have presented a set of nine dimensions of
interpersonal relationships, including dimensions
with a long tradition in social science and new
ones. These dimensions allow us to differenti-
ate core characteristics of the relationship between
two individuals. For example, people that commu-
nicate may be spatially near or spatially distant
(asking questions in class vs. chatting online), and
have a pleasure-oriented or work-oriented rela-
tionship (somebody wishing good luck to a friend
vs. interviewer and interviewee).

Our annotations show that assigning values to
dimensions can be done reliably (Cohen’s kappa:
0.68), and that useful values (1 and -1 labels) are
assigned to dimensions in most pairs of people
(>90%). Experimental results following a stan-
dard supervised machine learning approach show
that assigning values to dimensions can be auto-
mated (0.72 overall F-measure), and that results
per label and dimensions are biased towards the
most frequent label.

We believe that extracting dimensions of inter-
personal relationships complements previous ef-
forts that extract relationships. Our future plans
include studying values of dimensions for selected
relationships (e.g., COWORKER), and investigat-
ing changes on the dimensions of the relationship
over time. The latter would allow us to, for exam-

2314



ple, analyze how the relationship between two in-
dividuals changes over time, and determine which
events make a relationship go from superficial to
intense and vice versa.

References

John Adamopoulos. 2012. The emergence of social
meaning: A theory of action construal. In Handbook
of Social Resource Theory: Theoretical Extensions,
Empirical Insights, and Social Applications, pages
255–272. Springer New York, New York, NY.

Jacqueline Aguilar, Charley Beller, Paul McNamee,
Benjamin Van Durme, Stephanie Strassel, Zhiyi
Song, and Joe Ellis. 2014. A comparison of the
events and relations across ace, ere, tac-kbp, and
framenet annotation standards. In Proceedings of
the Second Workshop on EVENTS: Definition, De-
tection, Coreference, and Representation, pages 45–
53, Baltimore, Maryland, USA. Association for
Computational Linguistics.

Gabor Angeli, Melvin Jose Johnson Premkumar, and
Christopher D. Manning. 2015. Leveraging lin-
guistic structure for open domain information ex-
traction. In Proceedings of the 53rd Annual Meet-
ing of the Association for Computational Linguistics
(ACL), pages 344–354, Beijing, China. Association
for Computational Linguistics.

David B. Bracewell, Marc T. Tomlinson, Mary Brun-
son, Jesse Plymale, Jiajun Bracewell, and Daniel
Boerger. 2012. Annotation of adversarial and col-
legial social actions in discourse. In Proceedings of
the Sixth Linguistic Annotation Workshop, LAW VI
’12, pages 184–192, Stroudsburg, PA, USA. Associ-
ation for Computational Linguistics.

Philip Bramsen, Martha Escobar-Molano, Ami Patel,
and Rafael Alonso. 2011. Extracting social power
relationships from natural language. In Proceedings
of the 49th Annual Meeting of the Association for
Computational Linguistics: Human Language Tech-
nologies, pages 773–782, Portland, Oregon, USA.
Association for Computational Linguistics.

Cristian Danescu-Niculescu-Mizil, Lillian Lee,
Bo Pang, and Jon Kleinberg. 2012. Echoes of
power: Language effects and power differences
in social interaction. In Proceedings of the 21st
International Conference on World Wide Web,
WWW ’12, pages 699–708, New York, NY, USA.
ACM.

Cristian Danescu-Niculescu-Mizil, Moritz Sudhof,
Dan Jurafsky, Jure Leskovec, and Christopher Potts.
2013. A computational approach to politeness with
application to social factors. In Proceedings of the
51st Annual Meeting of the Association for Compu-
tational Linguistics (Volume 1: Long Papers), pages
250–259, Sofia, Bulgaria. Association for Computa-
tional Linguistics.

Morton Deutsch. 2011. Interdependence and psycho-
logical orientation. In Conflict, Interdependence,
and Justice, pages 247–271. Springer.

George Doddington, Alexis Mitchell, Mark Przybocki,
Lance Ramshaw, Stephanie Strassel, and Ralph
Weischedel. 2004. The automatic content extraction
(ace) program tasks, data, and evaluation. In Pro-
ceedings of the Fourth International Conference on
Language Resources and Evaluation (LREC-2004),
Lisbon, Portugal. European Language Resources
Association (ELRA). ACL Anthology Identifier:
L04-1011.

Eric Gilbert. 2012. Phrases that signal workplace hier-
archy. In Proceedings of the ACM 2012 Conference
on Computer Supported Cooperative Work, CSCW
’12, pages 1037–1046, New York, NY, USA. ACM.

Ralph Grishman and Beth Sundheim. 1996. Message
understanding conference-6: A brief history. In Pro-
ceedings of the 16th Conference on Computational
Linguistics - Volume 1, COLING ’96, pages 466–
471, Stroudsburg, PA, USA. Association for Com-
putational Linguistics.

Eduard Hovy, Mitchell Marcus, Martha Palmer, Lance
Ramshaw, and Ralph Weischedel. 2006. OntoNotes:
the 90% Solution. In NAACL ’06: Proceedings of
the Human Language Technology Conference of the
NAACL, Companion Volume: Short Papers on XX,
pages 57–60, Morristown, NJ, USA. Association for
Computational Linguistics.

Mohit Iyyer, Anupam Guha, Snigdha Chaturvedi, Jor-
dan Boyd-Graber, and Hal Daumé III. 2016. Feud-
ing families and former friends: Unsupervised learn-
ing for dynamic fictional relationships. In Proceed-
ings of the 2016 Conference of the North Ameri-
can Chapter of the Association for Computational
Linguistics: Human Language Technologies, pages
1534–1544, San Diego, California. Association for
Computational Linguistics.

Harold H Kelley. 2013. Personal relationships: Their
structures and processes. Psychology Press.

Seth Kulick, Ann Bies, and Justin Mott. 2014. Inter-
annotator agreement for ere annotation. In Proceed-
ings of the Second Workshop on EVENTS: Defini-
tion, Detection, Coreference, and Representation,
pages 21–25, Baltimore, Maryland, USA. Associa-
tion for Computational Linguistics.

J. Richard Landis and Gary G. Koch. 1977. The mea-
surement of observer agreement for categorical data.
Biometrics, 33(1).

Mausam, Michael Schmitz, Robert Bart, Stephen
Soderland, and Oren Etzioni. 2012. Open language
learning for information extraction. In Proceedings
of the 2012 Joint Conference on Empirical Methods
in Natural Language Processing and Computational
Natural Language Learning, EMNLP-CoNLL ’12,
pages 523–534, Stroudsburg, PA, USA. Association
for Computational Linguistics.

2315



Dat Quoc Nguyen, Kairit Sirts, Lizhen Qu, and Mark
Johnson. 2016. Stranse: a novel embedding model
of entities and relationships in knowledge bases. In
Proceedings of the 2016 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics: Human Language Technologies,
pages 460–466, San Diego, California. Association
for Computational Linguistics.

F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel,
B. Thirion, O. Grisel, M. Blondel, P. Pretten-
hofer, R. Weiss, V. Dubourg, J. Vanderplas, A. Pas-
sos, D. Cournapeau, M. Brucher, M. Perrot, and
E. Duchesnay. 2011. Scikit-learn: Machine learning
in Python. Journal of Machine Learning Research,
12:2825–2830.

Fuchun Peng and Andrew McCallum. 2004. Accurate
information extraction from research papers using
conditional random fields. In HLT-NAACL 2004:
Main Proceedings, pages 329–336, Boston, Mas-
sachusetts, USA. Association for Computational
Linguistics.

Vinodkumar Prabhakaran and Owen Rambow. 2014.
Predicting power relations between participants in
written dialog from a single thread. In Proceed-
ings of the 52nd Annual Meeting of the Association
for Computational Linguistics (Volume 2: Short Pa-
pers), pages 339–344, Baltimore, Maryland. Asso-
ciation for Computational Linguistics.

Mihai Surdeanu and Ji Heng. 2014. Overview of the
english slot filling track at the tac2014 knowledge
base population evaluation. In Proceedings of the
TAC-KBP 2014 Workshop.

Nikos Voskarides, Edgar Meij, Manos Tsagkias,
Maarten de Rijke, and Wouter Weerkamp. 2015.
Learning to explain entity relationships in knowl-
edge graphs. In Proceedings of the 53rd Annual
Meeting of the Association for Computational Lin-
guistics and the 7th International Joint Conference
on Natural Language Processing (Volume 1: Long
Papers), pages 564–574, Beijing, China. Associa-
tion for Computational Linguistics.

Robert West, Evgeniy Gabrilovich, Kevin Murphy,
Shaohua Sun, Rahul Gupta, and Dekang Lin. 2014.
Knowledge base completion via search-based ques-
tion answering. In Proceedings of the 23rd Interna-
tional Conference on World Wide Web, WWW ’14,
pages 515–526, New York, NY, USA. ACM.

Myron Wish, Morton Deutsch, and Susan J Kaplan.
1976. Perceived dimensions of interpersonal rela-
tions. Journal of Personality and Social Psychology,
33(4):409.

Fei Wu and Daniel S. Weld. 2010. Open information
extraction using wikipedia. In Proceedings of the
48th Annual Meeting of the Association for Com-
putational Linguistics, ACL ’10, pages 118–127,
Stroudsburg, PA, USA. Association for Computa-
tional Linguistics.

Xuchen Yao and Benjamin Van Durme. 2014. In-
formation extraction over structured data: Question
answering with freebase. In Proceedings of the
52nd Annual Meeting of the Association for Com-
putational Linguistics (Volume 1: Long Papers),
pages 956–966, Baltimore, Maryland. Association
for Computational Linguistics.

Xiaofeng Yu and Wai Lam. 2010. Jointly identify-
ing entities and extracting relations in encyclopedia
text via a graphical model approach. In Proceed-
ings of the 23rd International Conference on Com-
putational Linguistics: Posters, COLING ’10, pages
1399–1407, Stroudsburg, PA, USA. Association for
Computational Linguistics.

2316


