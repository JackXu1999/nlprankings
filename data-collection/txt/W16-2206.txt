



















































Alignment-Based Neural Machine Translation


Proceedings of the First Conference on Machine Translation, Volume 1: Research Papers, pages 54–65,
Berlin, Germany, August 11-12, 2016. c©2016 Association for Computational Linguistics

Alignment-Based Neural Machine Translation

Tamer Alkhouli, Gabriel Bretschner,
Jan-Thorsten Peter, Mohammed Hethnawi, Andreas Guta and Hermann Ney

Human Language Technology and Pattern Recognition Group
RWTH Aachen University, Aachen, Germany
{surname}@cs.rwth-aachen.de

Abstract

Neural machine translation (NMT) has
emerged recently as a promising statis-
tical machine translation approach. In
NMT, neural networks (NN) are directly
used to produce translations, without re-
lying on a pre-existing translation frame-
work. In this work, we take a step to-
wards bridging the gap between conven-
tional word alignment models and NMT.
We follow the hidden Markov model
(HMM) approach that separates the align-
ment and lexical models. We propose
a neural alignment model and combine
it with a lexical neural model in a log-
linear framework. The models are used
in a standalone word-based decoder that
explicitly hypothesizes alignments during
search. We demonstrate that our system
outperforms attention-based NMT on two
tasks: IWSLT 2013 German→English and
BOLT Chinese→English. We also show
promising results for re-aligning the train-
ing data using neural models.

1 Introduction

Neural networks have been gaining a lot of at-
tention recently in areas like speech recognition,
image recognition and natural language process-
ing. In machine translation, NNs are applied in
two main ways: In N -best rescoring, the neural
model is used to score the first-pass decoding out-
put, limiting the model to a fixed set of hypotheses
(Le et al., 2012; Sundermeyer et al., 2014a; Hu et
al., 2014; Guta et al., 2015). The second approach
integrates the NN into decoding, potentially allow-
ing it to directly determine the search space.

There are two approaches to use neural mod-
els in decoding. The first integrates the mod-

els into phrase-based decoding, where the mod-
els are used to score phrasal candidates hypothe-
sized by the decoder (Vaswani et al., 2013; Devlin
et al., 2014; Alkhouli et al., 2015). The second
approach is referred to as neural machine trans-
lation, where neural models are used to hypoth-
esize translations, word by word, without relying
on a pre-existing framework. In comparison to the
former approach, NMT does not restrict NNs to
predetermined translation candidates, and it does
not depend on word alignment concepts that have
been part of building state-of-the-art phrase-based
systems. In such systems, the HMM and the IBM
models developed more than two decades ago are
used to produce Viterbi word alignments, which
are used to build standard phrase-based systems.
Existing NMT systems either disregard the no-
tion of word alignments entirely (Sutskever et al.,
2014), or rely on a probabilistic notion of align-
ments (Bahdanau et al., 2015) independent of the
conventional alignment models.

Most recently, Cohn et al. (2016) designed neu-
ral models that incorporate concepts like fertility
and Markov conditioning into their structure. In
this work, we also focus on the question whether
conventional word alignment concepts can be used
for NMT. In particular, (1) We follow the HMM
approach to separate the alignment and translation
models, and use neural networks to model align-
ments and translation. (2) We introduce a lexical-
ized alignment model to capture source reorder-
ing information. (3) We bootstrap the NN training
using Viterbi word alignments obtained from the
HMM and IBM model training, and use the trained
neural models to generate new alignments. The
new alignments are then used to re-train the neural
networks. (4) We design an alignment-based de-
coder that hypothesizes the alignment path along
with the associated translation. We show com-
petitive results in comparison to attention-based

54



models on the IWSLT 2013 German→English and
BOLT Chinese→English task.

1.1 Motivation

Attention-based NMT computes the translation
probability depending on an intermediate compu-
tation of an alignment distribution. The alignment
distribution is used to choose the positions in the
source sentence that the decoder attends to dur-
ing translation. Therefore, the alignment model
can be considered as an implicit part of the trans-
lation model. On the other hand, separating the
alignment model from the lexical model has its
own advantages: First, this leads to more flexi-
bility in modeling and training: not only can the
models be trained separately, but they can also
have different model types, e.g. neural models,
count-based models, etc. Second, the separation
avoids propagating errors from one model to the
other. In attention-based systems, the translation
score is based on the alignment distribution, which
risks propagating errors from the alignment part to
the translation part. Third, using separate models
makes it possible to assign them different weights.
We exploit this and use a log-linear framework
to combine them. We still retain the possibility
of joint training, which can be performed flexibly
by alternating between model training and align-
ment generation. The latter can be performed us-
ing forced-decoding.

In contrast to the count-based models used in
HMMs, we use neural models, which allow cov-
ering long context without having to explicitly ad-
dress the smoothing problem that arises in count-
based models.

2 Related Work

Most recently, NNs have been trained on large
amounts of data, and applied to translate indepen-
dent of the phrase-based framework. Sutskever et
al. (2014) introduced the pure encoder-decoder ap-
proach, which avoids the concept of word align-
ments. Bahdanau et al. (2015) introduced an atten-
tion mechanism to the encoder-decoder approach,
allowing the decoder to attend to certain source
words. This method was refined in (Luong et al.,
2015) to allow for local attention, which makes the
decoder attend to representations of source words
residing within a window. These translation mod-
els have shown competitive results, outperforming
phrase-based systems when using ensembles on

tasks like IWSLT English→German 2015 (Luong
and Manning, 2015).

In this work, we follow the same standalone
neural translation approach. However, we have
a different treatment of alignments. While the
attention-based soft-alignment model computes
an alignment distribution as an intermediate step
within the neural model, we follow the hard align-
ment concept used in phrase extraction. We sepa-
rate the alignment model from the lexical model,
and train them independently. At translation time,
the decoder hypothesizes and scores the alignment
path in addition to the translation.

Cohn et al. (2016) introduce several modifi-
cations to the attention-based model inspired by
traditional word alignment concepts. They mod-
ify the network architecture, adding a first-order
dependence by making the attention vector com-
puted for a target position directly dependent on
that of the previous position. Our alignment model
has a first-order dependence that takes place at the
input and output of the model, rather than an ar-
chitectural modification of the neural network.

Yang et al. (2013) use NN-based lexical and
alignment models, but they give up the probabilis-
tic interpretation and produce unnormalized scores
instead. Furthermore, they model alignments us-
ing a simple distortion model that has no depen-
dence on lexical context. The models are used to
produce new alignments which are in turn used
to train phrase systems. This leads to no sig-
nificant difference in terms of translation perfor-
mance. Tamura et al. (2014) propose a lexicalized
RNN alignment model. The model still produces
non-probabilistic scores, and is used to generate
word alignments used to train phrase-based sys-
tems. In this work, we develop a feed-forward
neural alignment model that computes probabilis-
tic scores, and use it directly in standalone de-
coding, without constraining it to the phrase-based
framework. In addition, we use the neural models
to produce alignments that are used to re-train the
same neural models.

Schwenk (2012) proposed a feed-forward net-
work that computes phrase scores offline, and the
scores were added to the phrase table of a phrase-
based system. Offline phrase scoring was also
done in (Alkhouli et al., 2014) using semantic
phrase features obtained using simple neural net-
works. In comparison, our work does not rely on
the phrase-based system, rather, the neural net-

55



works are used to hypothesize translation candi-
dates directly, and the scores are computed online
during decoding.

We use the feed-forward joint model introduced
in (Devlin et al., 2014) as a lexical model, and in-
troduce a lexicalized alignment model based on
it. In addition, we modify the bidirectional joint
model presented in (Sundermeyer et al., 2014a)
and compare it to the feed-forward variant. These
lexical models were applied in phrase-based sys-
tems. In this work, we apply them in a standalone
NMT framework.

Forced alignment was applied to train phrase ta-
bles in (Wuebker et al., 2010; Peitz et al., 2012).
We generate forced alignments using a neural de-
coder, and use them to re-train neural models.

Tackling the costly normalization of the out-
put layer during decoding has been the focus of
several papers (Vaswani et al., 2013; Devlin et
al., 2014; Jean et al., 2015). We propose a sim-
ple method to speed up decoding using a class-
factored output layer with almost no loss in trans-
lation quality.

3 Statistical Machine Translation

In statistical machine translation, the target word
sequence eI1 = e1, ..., eI of length I is assigned
a probability conditioned on the source word se-
quence fJ1 = f1, ..., fJ of length J . By introduc-
ing word alignments as hidden variables, the pos-
terior probability p(eI1|fJ1 ) can be computed using
a lexical and an alignment model as follows.

p(eI1|fJ1 )
=

∑

bI1

p(eI1, b
I
1|fJ1 )

=
∑

bI1

I∏

i=1

p(ei, bi|bi−11 , ei−11 , fJ1 )

=
∑

bI1

I∏

i=1

p(ei|bi1, ei−11 , fJ1 )︸ ︷︷ ︸
lexical model

· p(bi|bi−11 , ei−11 , fJ1 )︸ ︷︷ ︸
alignment model

where bI1 = b1, ..., bI denotes the alignment path,
such that bi aligns the target word ei to the source
word fbi . In this general formulation, the lexi-
cal model predicts the target word ei conditioned
on the source sentence, the target history, and the
alignment history. The alignment model is lexical-
ized using the source and target context as well.
The sum over alignment paths is replaced by the
maximum during decoding (cf. Section 5).

4 Neural Network Models

There are two common network architectures used
in machine translation: feed-forward NNs (FFNN)
and recurrent NNs (RNN). In this section we will
discuss alignment-based feed-forward and recur-
rent neural networks. These networks are condi-
tioned on the word alignment, in addition to the
source and target words.

4.1 Feed-forward Joint Model

We adopt the feed-forward joint model (FFJM)
proposed in (Devlin et al., 2014) as the lexical
model. The authors demonstrate the model has
a strong performance when applied in a phrase-
based framework. In this work we explore its
performance in standalone NMT. The model was
introduced along with heuristics to resolve un-
aligned and multiply aligned words. We denote
the heuristic-based source alignment point corre-
sponding to the target position i by b̂i. The model
is defined as

p(ei|bi1, ei−11 , fJ1 ) = p(ei|ei−1i−n, f b̂i+mb̂i−m ) (1)

and it computes the probability of a target word
ei at position i given the n-gram target history
ei−1i−n = ei−n, ..., ei−1, and a window of 2m + 1

source words f b̂i+m
b̂i−m

= fb̂i−m, ..., fb̂i+m centered
around the word fb̂i .

As the heuristics have implications on our
alignment-based decoder, we explain them by the
examples shown in Figure 1. We mark the source
and target context by rectangles on the x- and y-
axis, respectively. The left figure shows a sin-
gle source word ‘Jungen’ aligned to a single tar-
get word ‘offspring’, in which case, the original
source position is used, i.e., b̂i = bi. If the tar-
get word is aligned to multiple source words, as
it is the case with the words ‘Mutter Tiere’ and
‘Mothers’ in the middle figure, then b̂i is set to
the middle alignment point. In this example, the
left alignment point associated with ‘Mutter’ is se-
lected. The right figure shows the case of the un-
aligned target word ‘of’. b̂i is set to the source
position associated with the closest aligned tar-
get word ‘full’, preferring right to left. Note that
this model does not have special handling of un-
aligned source words. While these words can be
covered indirectly by source windows associated
with aligned source words, the model does not ex-
plicitly score them.

56



Figure 1: Examples on resolving word alignments to obtain word affiliations.

Computing normalized probabilities is done us-
ing the softmax function, which requires comput-
ing the full output layer first, and then comput-
ing the normalization factor by summing over the
output scores of the full vocabulary. This is very
costly for large vocabularies. To overcome this,
we adopt the class-factored output layer consisting
of a class layer and a word layer (Goodman, 2001;
Morin and Bengio, 2005). The model in this case
is defined as

p(ei|ei−1i−n, f b̂i+mb̂i−m ) =

p(ei|c(ei), ei−1i−n, f b̂i+mb̂i−m ) · p(c(ei)|e
i−1
i−n, f

b̂i+m

b̂i−m
)

where c denotes a word mapping that assigns each
target word to a single class, where the number
of classes is chosen to be much smaller than the
vocabulary size |C| << |V |. Even though the
full class layer needs to be computed, only a sub-
set of the significantly-larger word layer has to be
considered, namely the words that share the same
class c(ei) with the target word ei. This helps
speeding up training on large-vocabulary tasks.

4.2 Bidirectional Joint Model
The bidirectional RNN joint model (BJM) pre-
sented in (Sundermeyer et al., 2014a) is another
lexical model. The BJM uses the full source sen-
tence and the full target history for prediction, and
it is computed by reordering the source sentence
following the target order. This requires the com-
plete alignment information to compute the model
scores. Here, we introduce a variant of the model
that is conditioned on the alignment history in-
stead of the full alignment path. This is achieved
by computing forward and backward representa-
tions of the source sentence in its original order,
as done in (Bahdanau et al., 2015). The model is
given by

p(ei|bi1, ei−11 , fJ1 ) = p(ei|b̂i1, ei−11 , fJ1 )

Note that we also use the same alignment heuris-
tics presented in Section 4.1. As this variant does
not require future alignment information, it can be
applied in decoding. However, in this work we
apply this model in rescoring and leave decoder
integration to future work.

4.3 Feed-forward Alignment Model

We propose a neural alignment model to score
alignment paths. Instead of predicting the abso-
lute positions in the source sentence, we model
the jumps from one source position to the next po-
sition to be translated. The jump at target posi-
tion i is defined as ∆i = b̂i − b̂i−1, which cap-
tures the jump from the source position b̂i−1 to b̂i.
We modify the FFNN lexical model to obtain a
feed-forward alignment model. The feed-forward
alignment model (FFAM) is given by

p(bi|bi−11 , ei−11 , fJ1 ) = p(∆i|ei−1i−n, f
b̂i−1+m
b̂i−1−m

) (2)

This is a lexicalized alignment model condi-
tioned on the n-gram target history and the
(2m+ 1)-gram source window. Note that, dif-
ferent from the FFJM, the source window of this
model is centered around the source position b̂i−1.
This is because the model needs to predict the
jump to the next source position b̂i to be translated.
The alignment model architecture is shown in Fig-
ure 2.

In contrast to the lexical model, the output vo-
cabulary of the alignment model is much smaller,
and therefore we use a regular softmax output
layer for this model without class-factorization.

4.4 Feed-forward vs. Recurrent Models

RNNs have been shown to outperform feed-
forward variants in language and translation mod-
eling. Nevertheless, feed-forward networks have
their own advantages: First, they are typically

57



f
b̂i−1+2
b̂i−1−2

ei−1i−3

p(∆i|ei−1i−3, f
b̂i−1+2
b̂i−1−2

)

Figure 2: A feed-forward alignment NN, with 3
target history words, 5-gram source window, a
projection layer, 2 hidden layers, and a small out-
put layer to predict jumps.

faster to train due to their simple architecture,
and second, they are more flexible to integrate
into beam search decoders. This is because feed-
forward networks only depend on a limited con-
text. RNNs, on the other hand, are conditioned on
an unbounded context. This means that the com-
plete hypotheses during decoding have to be main-
tained without any state recombination. Since
feed-forward networks allow the use of state re-
combination, they are potentially capable of ex-
ploring more candidates during beam search.

5 Alignment-based Decoder

In this section we present the alignment-based de-
coder. This is a beam-search word-based decoder
that predicts one target word at a time. As the
models we use are alignment-based, the decoder
hypothesizes the alignment path. This is different
from the NMT approaches present in the literature,
which are based on models that either ignore word
alignments or compute alignments as part of the
attention-based model.

In the general case, a word can be aligned to
a single word, multiple words, or it can be un-
aligned. However, we do not use the general word
alignment notion, rather, the models are based on
alignments derived using the heuristics discussed
in Section 4. These heuristics simplify the task
of the decoder, as they induce equivalence classes
over the alignment paths, reducing the number of
possible alignments the decoder has to hypothe-
size significantly. As a result of using these heuris-
tics, the task of hypothesizing alignments is re-

Algorithm 1 Alignment-based Decoder
1: procedure TRANSLATE(fJ1 , beamSize)
2: hyps← initHyp .previous set of partial hypotheses
3: newHyps← ∅ .current set of partial hypotheses
4: while GETBEST(hyps) not terminated do
5: .compute alignment distribution in batch mode
6: alignDists←ALIGNMENTDISTRIBUTION(hyps)
7: .hypothesize source alignment points
8: for pos From 1 to J do
9: .compute lexical distributions of all

10: .hypotheses in hyps in batch mode
11: dists← LEXICALDISTRIBUTION(hyps, pos)
12: .expand each of the previous hypotheses
13: for hyp in hyps do
14: jmpCost← SCORE(alignDists, hyp, pos)
15: dist← GETDISTRIBUTION(dists, hyp)
16: dist← PARTIALSORT(dist,beamSize)
17: cnt← 0
18: .hypothesize new target word
19: for word in dist do
20: if cnt > beamSize then
21: break
22: newHyp←EXTEND(hyp,word,pos,jmpCost)
23: newHyps.INSERT(newHyp)
24: cnt← cnt+ 1
25: PRUNE(newHyps, beamSize)
26: hyps← newHyps
27:
28: .return the best scoring hypothesis
29: return GETBEST(hyps)

duced to enumerating all J source positions a tar-
get word can be aligned to. The following is a list
of the possible alignment scenarios and how the
decoder covers them.

• Multiply-aligned target words: the heuris-
tic chooses the middle link as an alignment
point. Therefore, the decoder is able to cover
these cases by hypothesizing J many source
positions for each target word hypothesis.

• Unaligned target words: the heuristic aligns
these words using the nearest aligned target
word in training (cf. Figure 1, right). In de-
coding, these words are handled as aligned
words.

• Multiply-aligned source words: covered by
revisiting a source position that has already
been translated.

• Unaligned source words: result if no target
word is generated using a source window
centered around the source word in question.

The decoder is shown in Algorithm 1. It in-
volves hypothesizing alignments and translation

58



words. Alignments are hypothesized in the loop
starting at line 8. Once an alignment point is set to
position pos, the lexical distribution over the full
target vocabulary is computed using this position
in line 11. The distribution is sorted and the best
candidate translations lying within the beam are
used to expand the partial hypotheses.

We batch the NN computations, calling the
alignment and lexical networks for all partial hy-
potheses in a single call to speed up computations
as shown in lines 6 and 11. We also exploit the
beam and apply partial sorting in line 16, instead
of completely sorting the list. Partial sorting has a
linear complexity on average, and it returns a list
whose first beamSize words have better scores
compared to the rest of the list.

We terminate translation if the best scoring par-
tial hypothesis ends with the sentence end symbol.
If a hypothesis terminates but it scores worse than
other hypotheses, it is removed from the beam, but
it still competes with non-terminated hypotheses.
Note that we do not have any explicit coverage
constraints. This means that a source position can
be revisited many times, hence generating one-to-
many alignment cases. This also allows having un-
aligned source words.

In the alignment-based decoder, an alignment
distribution is computed, and word alignments are
hypothesized and scored using this distribution,
leading alignment decisions to become part of
beam search. The search space is composed of
both alignment and translation decisions. In con-
trast, the search space in attention-based decoding
is composed of translation decisions only.

Class-Factored Output Layer in Decoding
The large output layer used in language and trans-
lation modeling is a major bottleneck in evaluating
the network. Several papers discuss how to evalu-
ate it efficiently during decoding using approxima-
tions. In this work, we exploit the class-factored
output layer to speed up training. At decoding
time, the network needs to hypothesize all target
words, which means the full output layer should
be evaluated. In the case of using a class-factored
output layer, this results in an additional compu-
tational overhead from computing the class layer.
In order to speed up decoding, we propose to use
the class layer to choose the top scoring k classes,
then we evaluate the word layer for each of these
classes only. We show this leads to a significant
speed up with minimal loss in translation quality.

Model Combination
We embed the models in a log-linear framework,
which is commonly used in phrase-based systems.
The goal of the decoder is to find the best scoring
hypothesis as follows.

êÎ1 = arg max
I,eI1

{
max
b̂I1

M∑

m=1

λmhm(f
J
1 , e

I
1, b̂

I
1)

}

where λm is the model weight associated with the
model hm, and M is the total number of models.
The model weights are automatically tuned using
minimum error rate training (MERT) (Och, 2003).
Our main system includes a lexical neural model,
an alignment neural model, and a word penalty,
which is the count of target words. The word
penalty becomes important at the end of transla-
tion, where hypotheses in the beam might have
different final lengths.

6 Forced-Alignment Training

Since the models we use require alignments for
training, we initially use word alignments pro-
duced using HMM/IBM models using GIZA++ as
initial alignments. At first, the FFJM and the
FFAM are trained separately until convergence,
then the models are used to generate new word
alignments by force-decoding the training data as
follows.

b̃I1(f
J
1 , e

I
1) = argmax

bI1

I∏

i=1

pλ1(∆i|ei−1i−n, f
bi−1+m
bi−1−m )

· pλ2(ei|ei−1i−n, f bi+mbi−m )

where λ1 and λ2 are the model weights. We mod-
ify the decoder to only compute the probabilities
of the target words in the reference sentence. The
for loop in line 19 of Algorithm 1 collapses to a
single iteration. We use both the the feed-forward
joint model (FFJM) and the feed-forward align-
ment model (FFAM) to perform force-decoding,
and the new alignments are used to retrain the
models, replacing the initial GIZA++ alignments.

Retraining the neural models using the forced-
alignments has two benefits. First, since the align-
ments are produced using both of the lexical and
alignment models, this can be viewed as joint
training of the two models. Second, since the neu-
ral decoder generates these alignments, training
neural models based on them yields models that
are more consistent with the neural decoder. We
verify this claim in the experiments section.

59



IWSLT BOLT
De En Zh En

Sentences 4.32M 4.08M
Run. Words 108M 109M 78M 86M
Vocab. 836K 792K 384K 817K

FFNN/BJM Vocab. 173K 149K 169K 128K
Attention Vocab. 30K 30K 30K 30K

FFJM params 177M 159M
BJM params 170M 153M
FFAM params 101M 94M
Attention params 84M 84M

Table 1: Corpora and NN statistics.

7 Experiments

We carry out experiments on two tasks: the
IWSLT 2013 German→English shared transla-
tion task,1 and the BOLT Chinese→English task.
The corpora statistics are shown in Table 1. The
IWSLT phrase-based baseline system is trained on
all available bilingual data, and uses a 4-gram LM
with modified Kneser-Ney smoothing (Kneser and
Ney, 1995; Chen and Goodman, 1998), trained
with the SRILM toolkit (Stolcke, 2002). As ad-
ditional data sources for the LM, we selected parts
of the Shuffled News and LDC English Giga-
word corpora based on the cross-entropy differ-
ence (Moore and Lewis, 2010), resulting in a to-
tal of 1.7 billion running words for LM training.
The phrase-based baseline is a standard phrase-
based SMT system (Koehn et al., 2003) tuned with
MERT (Och, 2003) and contains a hierarchical re-
ordering model (Galley and Manning, 2008). The
in-domain data consists of 137K sentences.

The BOLT Chinese→English task is evaluated
on the “discussion forum” domain. We use a 5-
gram LM trained on 2.9 billion running words in
total. The in-domain data consists of a subset of
67.8K sentences. We used a set of 1845 sentences
as a tune set. The evaluation set test1 contains
1844 and test2 contains 1124 sentences.

We use the FFNN architecture for the lexical
and alignment models. Both models use a win-
dow of 9 source words, and 5 target history words.
Both models use two hidden layers, the first has
1000 units and the second has 500 units. The lex-
ical model uses a class-factored output layer, with
1000 singleton classes dedicated to the most fre-
quent words, and 1000 classes shared among the
rest of the words. The classes are trained using a
separate tool to optimize the maximum likelihood

1http://www.iwslt2013.org

training criterion with the bigram assumption. The
alignment model uses a small output layer of 201
nodes, determined by a maximum jump length of
100 (forward and backward). 300 nodes are used
for word embeddings. Each of the FFNN models
is trained on CPUs using 12 threads, which takes
up to 3 days until convergence. We train with
stochastic gradient descent using a batch size of
128. The learning rate is halved when the devel-
opment perplexity increases.

Each BJM has 4 LSTM layers: two for the for-
ward and backward states, one for the target state,
and one after merging the source and target states.
The size of the word embeddings and hidden lay-
ers is 350 nodes. The output layers are identical to
those of the FFJM models.

We compare our system to an attention-based
baseline similar to the networks described in (Bah-
danau et al., 2015). All such systems use single
models, rather than ensembles. The word embed-
ding dimension is 620, each direction of the en-
coder and the decoder has a layer of 1000 gated
recurrent units (Cho et al., 2014). Unknowns and
numbers are carried out from the source side to the
target side based on the largest attention weight.

To speed up decoding of long sentences, the
decoder hypothesizes 21 and 41 source positions
around the diagonal, for the IWSLT and the BOLT
tasks, respectively. We choose these numbers
such that the translation quality does not degrade.
The beam size is set to 16 in all experiments.
Larger beam sizes did not lead to improvements.
We apply part-of-speech-based long-range verb
reordering rules to the German side in a pre-
processing step for all German→English systems
(Popović and Ney, 2006), including the baselines.
The Chinese→English systems use no such pre-
ordering. We use the GIZA++ word alignments
to train the models. The networks are fine-tuned
by training additional epochs on the in-domain
data only (Luong and Manning, 2015). The LMs
are only used in the phrase-based systems in both
tasks, but not in the NMT systems.

All translation experiments are performed with
the Jane toolkit (Vilar et al., 2010; Wuebker et al.,
2012). The alignment-based NNs are trained using
an extension of the rwthlm toolkit (Sundermeyer et
al., 2014b). We use an implementation based on
Blocks (van Merriënboer et al., 2015) and Theano
(Bergstra et al., 2010; Bastien et al., 2012) for the
attention-based experiments. All results are mea-

60



test 2010 eval 2011
# system BLEU TER BLEU TER

1 phrase-based system 28.9 51.0 32.9 46.3
2 + monolingual data 30.4 49.5 35.4 44.2

3 attention-based RNN 27.9 51.4 31.8 46.5
4 +fine-tuning 29.8 48.9 32.9 45.1

5 FFJM+dp+wp 21.6 56.9 24.7 53.8
6 FFJM+FFAM+wp 26.1 53..1 29.9 49.4
7 +fine-tuning 29.3 50.5 33.2 46.5
8 +BJM Rescoring 30.0 48.7 33.8 44.8
9 BJM+FFAM+wp+fine-tuning 29.8 49.5 33.7 45.8

Table 2: IWSLT 2013 German→English results in
BLEU [%] and TER [%].

sured in case-insensitive BLEU [%] (Papineni et
al., 2002) and TER [%] (Snover et al., 2006) on
a single reference. We used the multeval toolkit
(Clark et al., 2011) for evaluation.

7.1 IWSLT 2013 German→English
Table 2 shows the IWSLT German→English re-
sults. FFJM refers to feed-forward lexical model.
We compare against the phrase-based system with
an LM trained on the target side of the bilin-
gual data (row #1), the phrase-based system with
an LM trained on additional monolingual data
(row #2), the attention-based system (row #3),
and the attention-based system after fine-tuning
towards the in-domain data (row #4). First, we ex-
periment with a system using the FFJM as a lex-
ical model and a linear distortion penalty (dp) to
encourage monotone translation as the alignment
model. We also include a word penalty (wp). This
system is shown in row #5. In comparison, if the
distortion penalty is replaced by the feed-forward
alignment model (FFAM), we observe large im-
provements of 4.5% to 5.2% BLEU (row #5 vs.
#6). This highlights the significant role of the
alignment model in our system. Moreover, it in-
dicates that the FFAM is able to model alignments
beyond the simple monotone alignments preferred
by the distortion penalty.

Fine-tuning the neural networks towards in-
domain data improves the system by up to 3.3%
BLEU and 2.9% TER (row #6 vs #7). The gain
from fine-tuning is larger than the one observed
for the attention-based system. This is likely due
to the fact that our system has two neural models,
and each of them is fine-tuned.

We apply the BJM in 1000-best list rescoring
(row #8). Which gives another boost, leading our
system to outperform the attention-based system
by 0.9% BLEU on eval 2011, while a compa-

rable performance is achieved on test 2010.
In order to highlight the difference between us-
ing the FFJM and the BJM, we replace the FFJM
scores after obtaining the N -best lists with the
BJM scores and apply rescoring (row #9). In com-
parison to row #7, we observe up to 0.5% BLEU
and 1.0% TER improvement. This is expected
as the BJM captures unbounded source and tar-
get context in comparison to the limited context of
the FFJM. This calls for a direct integration of the
BJM into decoding, which we intend to do in fu-
ture work. Our best system (row #8) outperforms
the phrase-based system (row #1) by up to 1.1%
BLEU and 2.3% TER. While the phrase-based
system can benefit from training the LM on addi-
tional monolingual data (row #1 vs. #2), exploit-
ing monolingual data in NMT systems is still an
open research question.

7.2 BOLT Chinese→English
The BOLT Chinese→English experiments are
shown in Table 3. Again, we observe large im-
provements when including the FFAM in compar-
ison to the distortion penalty (row #5 vs #6), and
fine-tuning improves the results considerably. In-
cluding the BJM in rescoring improves the system
by up to 0.4% BLEU. Our best system (row #8)
outperforms the attention-based model by up to
0.4% BLEU and 2.8% TER. We observe that the
length ratio of our system’s output to the reference
is 93.3-94.9%, while it is 99.1-102.6% for the
attention-based system. In light of the BLEU and
TER scores, the attention-based model does not
benefit from matching the reference length. Our
system (row #8) still lags behind the phrase-based
system (row #1). Note, however, that in the WMT
2016 evaluation campaign,2 it was demonstrated
that NMT can outperform phrase-based systems
on several tasks including German→English and
English→German. Including monolingual data
(Sennrich et al., 2016) in training neural transla-
tion models can boost performance, and this can
be applied to our system.

7.3 Neural Alignments

Next, we experiment with re-aligning the train-
ing data using neural networks as described in
Section 6. We use the fine-tuned FFJM and
FFAM to realign the in-domain data of the IWSLT
German→English task. These models are initially

2http://matrix.statmt.org/

61



test1 test2
# system BLEU TER BLEU TER

1 phrase-based system 17.6 68.3 16.9 67.4
2 + monolingual data 17.9 67.9 17.0 67.1

3 attention-based RNN 14.8 76.1 13.6 76.9
4 +fine-tuning 16.1 73.1 15.4 72.3

5 FFJM+dp+wp 10.1 77.2 9.8 75.8
6 FFJM+FFAM+wp 14.4 71.9 13.7 71.3
7 +fine-tuning 15.8 70.3 15.4 69.4
8 +BJM Rescoring 16.0 70.3 15.8 69.5
9 BJM+FFAM+wp+fine-tuning 16.0 70.4 15.7 69.7

Table 3: BOLT Chinese→English results in BLEU
[%] and TER [%].

test 2010 eval 2011
Alignment Source BLEU TER BLEU TER

GIZA++ 25.6 53.6 29.3 49.7
Neural Forced decoding 25.9 52.4 29.5 49.4

Table 4: Re-alignment results in BLEU [%] and
TER [%] on the IWSLT 2013 German→English
in-domain data. Each system includes FFJM,
FFAM and word penalty.

trained using GIZA++ alignments. We train new
models using the re-aligned data and compare the
translation quality before and after re-alignment.
We use 0.7 and 0.3 as model weights for the FFJM
and FFAM, respectively. These values are based
on the model weights obtained using MERT. The
results are shown in Table 4. Note that the base-
line is worse than the one in Table 2 as the models
are only trained on the in-domain data. We ob-
serve that re-aligning the data improves translation
quality by up to 0.3% BLEU and 1.2% TER. The
new alignments are generated using the neural de-
coder, and using them to train the neural networks
results in training that is more consistent with de-
coding. As future work, we intend to re-align the
full bilingual data and use it for neural training.

7.4 Class-Factored Output Layer

Figure 3 shows the trade-off between speed and
performance when evaluating words belonging to
the top classes only. Limiting the evaluation to
words belonging to the top class incurs a perfor-
mance loss of 0.4% BLEU only when compared to
the full evaluation of the output layer. However,
this corresponds to a large speed-up. The system
is about 30 times faster, with a translation speed
of 0.4 words/sec. In conclusion, not only does
the class layer speed up training, but it can also be
used to speed up decoding considerably. We use
the top 3 classes throughout our experiments.

27

27.5

28

28.5

29

29.5

1 10 100 1000
0

5

10

15

20

25

30

35

B
L

E
U

[%
]

sp
ee

d-
up

fa
ct

or

classes

BLEU[%]
speed-up factor

Figure 3: Decoding speed-up and translation qual-
ity using top scoring classes in a class-factored
output layer. The results are computed for the
IWSLT German→English dev dataset.

and
the

proposal
was

to
build

a
lot
of

other
coal

factories

.

und
war
der
Vorschlag
, zu bauen
viele
weitere
Kohle
Fabriken
.

Figure 4: A translation example produced by
our system. The shown German sentence is pre-
ordered.

8 Analysis

We show an example from the German→English
task in Figure 4, along with the alignment path.
The reference translation is ‘and the proposal has
been to build a lot more coal plants .’. Our sys-
tem handles the local reordering of the word ‘was’,
which is produced in the correct target order. An
example on the one-to-many alignments is given
by the correct translation of ‘viele’ to ‘a lot of’.

As an example on handling multiply-aligned
target words, we observe the translation of ‘Nord
Westen’ to ‘northwest’ in our output. This is pos-
sible because the source window allows the FFNN
to translate the word ‘Westen’ in context of the
word ‘Nord’.

Table 5 lists some translation examples pro-
duced by our system and the attention-based sys-
tem, where maximum attention weights are used

62



1

source sie würden verhungern nicht , und wissen Sie was ?
reference they wouldn ’t starve , and you know what ?
attention NMT you wouldn ’t interview , and guess what ?
our system they wouldn ’t starve , and you know what ?

2

source denn sie sind diejenigen , die sind auch Experten für Geschmack .
reference because they ’re the ones that are experts in flavor , too .
attention NMT because they ’re the ones who are also experts .
our system because they ’re the ones who are also experts in flavor .

3

source es ist ein Online Spiel , in dem Sie müssen überwinden eine Ölknappheit .
reference this is an online game in which you try to survive an oil shortage .
attention NMT it ’s an online game where you need to get through a UNKOWN .
our system it ’s an online game in which you have to overcome an astrolabe .

4

source es liegt daran , dass gehen nicht Möglichkeiten auf diesem Planeten zurück, sie gehen vorwärts .
reference it ’s because possibilities on this planet , they don ’t go back , they go forward .
attention NMT it ’s because there ’s no way back on this planet , they ’re going to move forward .
our system it ’s because opportunities don ’t go on this planet , they go forward .

Table 5: Sample translations from the IWSLT German→English test set using the attention-based
system (Table 2, row #4) and our system (Table 2, row #7). We highlight the (pre-ordered) source words
and their aligned target words. We underline the source words of interest, italicize correct translations,
and use bold-face for incorrect translations.

as alignment. While we use larger vocabularies
compared to the attention-based system, we ob-
serve incorrect translations of rare words. E.g.,
the German word Ölknappheit in sentence 3 oc-
curs only 7 times in the training data among 108M
words, and therefore it is an unknown word for
the attention system. Our system has the word in
the source vocabulary but fails to predict the right
translation. Another problem occurs in sentence
4, where the German verb “zurückgehen” is split
into “gehen ... zurück”. Since the feed-forward
model uses a source window of size 9, it cannot
include both words when it is centered at any of
them. Such insufficient context might be resolved
when integrating the bidirectional RNN in decod-
ing. Note that the attention-based model also fails
to produce the correct translation here.

9 Conclusion

This work takes a step towards bridging the gap
between conventional word alignment concepts
and NMT. We use an HMM-inspired factoriza-
tion of the lexical and alignment models, and em-
ploy the Viterbi alignments obtained using con-
ventional HMM/IBM models to train neural mod-
els. An alignment-based decoder is introduced
and a log-linear framework is used to combine the
models. We use MERT to tune the model weights.
Our system outperforms the attention-based sys-
tem on the German→English task by up to 0.9%
BLEU, and on Chinese→English by up to 2.8%

TER. We also demonstrate that re-aligning the
training data using the neural decoder yields better
translation quality.

As future work, we aim to integrate alignment-
based RNNs such as the BJM into the alignment-
based decoder. We also plan to develop a bidirec-
tional RNN alignment model to make jump deci-
sions based on unbounded context. In addition, we
want to investigate the use of coverage constraints
in alignment-based NMT. Furthermore, we con-
sider the re-alignment experiment promising and
plan to apply re-alignment on the full bilingual
data of each task.

Acknowledgments

This paper has received funding from the Euro-
pean Union’s Horizon 2020 research and innova-
tion programme under grant agreement no 645452
(QT21). Tamer Alkhouli was partly funded by the
2016 Google PhD Fellowship for North America,
Europe and the Middle East.

References
Tamer Alkhouli, Andreas Guta, and Hermann Ney.

2014. Vector space models for phrase-based ma-
chine translation. In EMNLP 2014 Eighth Work-
shop on Syntax, Semantics and Structure in Statis-
tical Translation, pages 1–10, Doha, Qatar, October.

Tamer Alkhouli, Felix Rietig, and Hermann Ney. 2015.
Investigations on phrase-based decoding with recur-
rent neural network language and translation mod-

63



els. In Proceedings of the EMNLP 2015 Tenth Work-
shop on Statistical Machine Translation, pages 294–
303, Lisbon, Portugal, September.

Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben-
gio. 2015. Neural machine translation by jointly
learning to align and translate. In International Con-
ference on Learning Representations, San Diego,
Calefornia, USA, May.

Frédéric Bastien, Pascal Lamblin, Razvan Pascanu,
James Bergstra, Ian J. Goodfellow, Arnaud Berg-
eron, Nicolas Bouchard, and Yoshua Bengio. 2012.
Theano: new features and speed improvements. De-
cember.

James Bergstra, Olivier Breuleux, Frédéric Bastien,
Pascal Lamblin, Razvan Pascanu, Guillaume Des-
jardins, Joseph Turian, David Warde-Farley, and
Yoshua Bengio. 2010. Theano: a CPU and GPU
math expression compiler. In Proceedings of the
Python for Scientific Computing Conference, Austin,
TX, USA, June.

Stanley F. Chen and Joshua Goodman. 1998. An
Empirical Study of Smoothing Techniques for Lan-
guage Modeling. Technical Report TR-10-98, Com-
puter Science Group, Harvard University, Cam-
bridge, MA, August.

Kyunghyun Cho, Bart van Merrienboer, Caglar Gul-
cehre, Dzmitry Bahdanau, Fethi Bougares, Holger
Schwenk, and Yoshua Bengio. 2014. Learning
phrase representations using RNN encoder–decoder
for statistical machine translation. In Proceedings of
the 2014 Conference on Empirical Methods in Nat-
ural Language Processing, pages 1724–1734, Doha,
Qatar, October.

Jonathan H. Clark, Chris Dyer, Alon Lavie, and
Noah A. Smith. 2011. Better hypothesis testing for
statistical machine translation: Controlling for opti-
mizer instability. In 49th Annual Meeting of the As-
sociation for Computational Linguistics, pages 176–
181, Portland, Oregon, June.

Trevor Cohn, Cong Duy Vu Hoang, Ekaterina Vy-
molova, Kaisheng Yao, Chris Dyer, and Gholamreza
Haffari. 2016. Incorporating structural alignment
biases into an attentional neural translation model.
In Proceedings of the 2016 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics: Human Language Technologies,
pages 876–885, San Diego, California, June.

Jacob Devlin, Rabih Zbib, Zhongqiang Huang, Thomas
Lamar, Richard Schwartz, and John Makhoul. 2014.
Fast and Robust Neural Network Joint Models for
Statistical Machine Translation. In 52nd Annual
Meeting of the Association for Computational Lin-
guistics, pages 1370–1380, Baltimore, MD, USA,
June.

Michel Galley and Christopher D. Manning. 2008. A
simple and effective hierarchical phrase reordering

model. In Proceedings of the Conference on Empiri-
cal Methods in Natural Language Processing, pages
848–856, Honolulu, Hawaii, USA, October.

Joshua Goodman. 2001. Classes for fast maximum en-
tropy training. In Proceedings of the IEEE Interna-
tional Conference on Acoustics, Speech, and Signal
Processing, volume 1, pages 561–564, Utah, USA,
May.

Andreas Guta, Tamer Alkhouli, Jan-Thorsten Peter, Jo-
ern Wuebker, and Hermann Ney. 2015. A Com-
parison between Count and Neural Network Mod-
els Based on Joint Translation and Reordering Se-
quences. In Conference on Empirical Methods on
Natural Language Processing, pages 1401–1411,
Lisbon, Portugal, September.

Yuening Hu, Michael Auli, Qin Gao, and Jianfeng Gao.
2014. Minimum translation modeling with recur-
rent neural networks. In Proceedings of the 14th
Conference of the European Chapter of the Associ-
ation for Computational Linguistics, pages 20–29,
Gothenburg, Sweden, April.

Sébastien Jean, Kyunghyun Cho, Roland Memisevic,
and Yoshua Bengio. 2015. On using very large tar-
get vocabulary for neural machine translation. In
Proceedings of the Annual Meeting of the Associa-
tion for Computational Linguistics, pages 1–10, Bei-
jing, China, July.

Reinhard Kneser and Hermann Ney. 1995. Improved
backing-off for M-gram language modeling. In Pro-
ceedings of the International Conference on Acous-
tics, Speech, and Signal Processing, volume 1, pages
181–184, May.

Philipp Koehn, Franz J. Och, and Daniel Marcu.
2003. Statistical Phrase-Based Translation. In Pro-
ceedings of the 2003 Meeting of the North Ameri-
can chapter of the Association for Computational
Linguistics, pages 127–133, Edmonton, Canada,
May/June.

Hai Son Le, Alexandre Allauzen, and François Yvon.
2012. Continuous Space Translation Models with
Neural Networks. In Conference of the North Amer-
ican Chapter of the Association for Computational
Linguistics: Human Language Technologies, pages
39–48, Montreal, Canada, June.

Minh-Thang Luong and Christopher D. Manning.
2015. Stanford neural machine translation systems
for spoken language domains. In Proceedings of the
International Workshop on Spoken Language Trans-
lation, pages 76–79, Da Nag, Vietnam, December.

Minh-Thang Luong, Hieu Pham, and Christopher D.
Manning. 2015. Effective approaches to attention-
based neural machine translation. In Conference on
Empirical Methods in Natural Language Process-
ing, pages 1412–1421, Lisbon, Portugal, September.

64



R.C. Moore and W. Lewis. 2010. Intelligent Selec-
tion of Language Model Training Data. In Proceed-
ings of the 48th Annual Meeting of the Association
for Computational Linguistics, pages 220–224, Up-
psala, Sweden, July.

Frederic Morin and Yoshua Bengio. 2005. Hierarchi-
cal probabilistic neural network language model. In
Proceedings of the international workshop on artifi-
cial intelligence and statistics, pages 246–252, Bar-
bados, January.

Franz Josef Och. 2003. Minimum Error Rate Train-
ing in Statistical Machine Translation. In Proceed-
ings of the 41th Annual Meeting of the Association
for Computational Linguistics, pages 160–167, Sap-
poro, Japan, July.

Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a Method for Automatic
Evaluation of Machine Translation. In Proceed-
ings of the 41st Annual Meeting of the Associa-
tion for Computational Linguistics, pages 311–318,
Philadelphia, Pennsylvania, USA, July.

Stephan Peitz, Arne Mauser, Joern Wuebker, and Her-
mann Ney. 2012. Forced derivations for hierarchi-
cal machine translation. In International Confer-
ence on Computational Linguistics, pages 933–942,
Mumbai, India, December.

Maja Popović and Hermann Ney. 2006. POS-based
word reorderings for statistical machine transla-
tion. In Language Resources and Evaluation, pages
1278–1283, Genoa, Italy, May.

Holger Schwenk. 2012. Continuous Space Translation
Models for Phrase-Based Statistical Machine Trans-
lation. In 25th International Conference on Com-
putational Linguistics, pages 1071–1080, Mumbai,
India, December.

Rico Sennrich, Barry Haddow, and Alexandra Birch.
2016. Improving neural machine translation models
with monolingual data. In Proceedings of 54th An-
nual Meeting of the Association for Computational
Linguistics, August.

Matthew Snover, Bonnie Dorr, Richard Schwartz, Lin-
nea Micciulla, and John Makhoul. 2006. A Study of
Translation Edit Rate with Targeted Human Annota-
tion. In Proceedings of the 7th Conference of the As-
sociation for Machine Translation in the Americas,
pages 223–231, Cambridge, Massachusetts, USA,
August.

Andreas Stolcke. 2002. SRILM – An Extensible
Language Modeling Toolkit. In Proceedings of the
International Conference on Speech and Language
Processing, volume 2, pages 901–904, Denver, CO,
September.

Martin Sundermeyer, Tamer Alkhouli, Joern Wuebker,
and Hermann Ney. 2014a. Translation Modeling
with Bidirectional Recurrent Neural Networks. In

Conference on Empirical Methods on Natural Lan-
guage Processing, pages 14–25, Doha, Qatar, Octo-
ber.

Martin Sundermeyer, Ralf Schlüter, and Hermann Ney.
2014b. rwthlm - the RWTH Aachen university neu-
ral network language modeling toolkit. In Inter-
speech, pages 2093–2097, Singapore, September.

Ilya Sutskever, Oriol Vinyals, and Quoc V. V Le.
2014. Sequence to sequence learning with neu-
ral networks. In Advances in Neural Information
Processing Systems 27, pages 3104–3112, Montréal,
Canada, December.

Akihiro Tamura, Taro Watanabe, and Eiichiro Sumita.
2014. Recurrent neural networks for word align-
ment model. In 52nd Annual Meeting of the Asso-
ciation for Computational Linguistics, pages 1470–
1480, Baltimore, MD, USA.

Bart van Merriënboer, Dzmitry Bahdanau, Vincent Du-
moulin, Dmitriy Serdyuk, David Warde-Farley, Jan
Chorowski, and Yoshua Bengio. 2015. Blocks
and fuel: Frameworks for deep learning. CoRR,
abs/1506.00619.

Ashish Vaswani, Yinggong Zhao, Victoria Fossum,
and David Chiang. 2013. Decoding with large-
scale neural language models improves translation.
In Proceedings of the 2013 Conference on Empiri-
cal Methods in Natural Language Processing, pages
1387–1392, Seattle, Washington, USA, October.

David Vilar, Daniel Stein, Matthias Huck, and Her-
mann Ney. 2010. Jane: Open source hierarchi-
cal translation, extended with reordering and lexi-
con models. In ACL 2010 Joint Fifth Workshop on
Statistical Machine Translation and Metrics MATR,
pages 262–270, Uppsala, Sweden, July.

Joern Wuebker, Arne Mauser, and Hermann Ney.
2010. Training phrase translation models with
leaving-one-out. In Proceedings of the Annual
Meeting of the Association for Computational Lin-
guistics, pages 475–484, Uppsala, Sweden, July.

Joern Wuebker, Matthias Huck, Stephan Peitz, Malte
Nuhn, Markus Freitag, Jan-Thorsten Peter, Saab
Mansour, and Hermann Ney. 2012. Jane 2: Open
source phrase-based and hierarchical statistical ma-
chine translation. In International Conference on
Computational Linguistics, pages 483–491, Mum-
bai, India, December.

Nan Yang, Shujie Liu, Mu Li, Ming Zhou, and Neng-
hai Yu. 2013. Word alignment modeling with con-
text dependent deep neural network. In 51st Annual
Meeting of the Association for Computational Lin-
guistics, pages 166–175, Sofia, Bulgaria, August.

65


