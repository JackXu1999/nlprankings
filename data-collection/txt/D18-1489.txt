



















































Direct Output Connection for a High-Rank Language Model


Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 4599–4609
Brussels, Belgium, October 31 - November 4, 2018. c©2018 Association for Computational Linguistics

4599

Direct Output Connection for a High-Rank Language Model

Sho Takase† Jun Suzuki†‡ Masaaki Nagata†
†NTT Communication Science Laboratories

‡Tohoku University
{takase.sho, nagata.masaaki}@lab.ntt.co.jp

jun.suzuki@ecei.tohoku.ac.jp

Abstract

This paper proposes a state-of-the-art recur-
rent neural network (RNN) language model
that combines probability distributions com-
puted not only from a final RNN layer but
also from middle layers. Our proposed method
raises the expressive power of a language
model based on the matrix factorization in-
terpretation of language modeling introduced
by Yang et al. (2018). The proposed method
improves the current state-of-the-art language
model and achieves the best score on the
Penn Treebank and WikiText-2, which are
the standard benchmark datasets. Moreover,
we indicate our proposed method contributes
to two application tasks: machine translation
and headline generation. Our code is pub-
licly available at: https://github.com/nttcslab-
nlp/doc lm.

1 Introduction

Neural network language models have played a
central role in recent natural language processing
(NLP) advances. For example, neural encoder-
decoder models, which were successfully ap-
plied to various natural language generation tasks
including machine translation (Sutskever et al.,
2014), summarization (Rush et al., 2015), and dia-
logue (Wen et al., 2015), can be interpreted as con-
ditional neural language models. Neural language
models also positively influence syntactic pars-
ing (Dyer et al., 2016; Choe and Charniak, 2016).
Moreover, such word embedding methods as Skip-
gram (Mikolov et al., 2013) and vLBL (Mnih and
Kavukcuoglu, 2013) originated from neural lan-
guage models designed to handle much larger vo-
cabulary and data sizes. Neural language models
can also be used as contextualized word representa-
tions (Peters et al., 2018). Thus, language modeling
is a good benchmark task for investigating the gen-
eral frameworks of neural methods in NLP field.

In language modeling, we compute joint prob-
ability using the product of conditional probabili-
ties. Let w1:T be a word sequence with length T :
w1, ..., wT . We obtain the joint probability of word
sequence w1:T as follows:

p(w1:T ) = p(w1)

T−1∏
t=1

p(wt+1|w1:t). (1)

p(w1) is generally assumed to be 1 in this literature,
that is, p(w1) = 1, and thus we can ignore its cal-
culation. See the implementation of Zaremba et al.
(2014)1, for an example. RNN language models
obtain conditional probability p(wt+1|w1:t) from
the probability distribution of each word. To com-
pute the probability distribution, RNN language
models encode sequence w1:t into a fixed-length
vector and apply a transformation matrix and the
softmax function.

Previous researches demonstrated that RNN lan-
guage models achieve high performance by using
several regularizations and selecting appropriate
hyperparameters (Melis et al., 2018; Merity et al.,
2018). However, Yang et al. (2018) proved that
existing RNN language models have low expres-
sive power due to the Softmax bottleneck, which
means the output matrix of RNN language mod-
els is low rank when we interpret the training of
RNN language models as a matrix factorization
problem. To solve the Softmax bottleneck, Yang
et al. (2018) proposed Mixture of Softmaxes (MoS),
which increases the rank of the matrix by com-
bining multiple probability distributions computed
from the encoded fixed-length vector.

In this study, we propose Direct Output Con-
nection (DOC) as a generalization of MoS. For
stacked RNNs, DOC computes the probability dis-
tributions from the middle layers including input
embeddings. In addition to raising the rank, the

1https://github.com/wojzaremba/lstm

https://github.com/nttcslab-nlp/doc_lm
https://github.com/nttcslab-nlp/doc_lm
https://github.com/wojzaremba/lstm


4600

proposed method helps weaken the vanishing gra-
dient problem in backpropagation because DOC
provides a shortcut connection to the output.

We conduct experiments on standard benchmark
datasets for language modeling: the Penn Treebank
and WikiText-2. Our experiments demonstrate that
DOC outperforms MoS and achieves state-of-the-
art perplexities on each dataset. Moreover, we in-
vestigate the effect of DOC on two applications:
machine translation and headline generation. We
indicate that DOC can improve the performance of
an encoder-decoder with an attention mechanism,
which is a strong baseline for such applications. In
addition, we conduct an experiment on the Penn
Treebank constituency parsing task to investigate
the effectiveness of DOC.

2 RNN Language Model

In this section, we briefly overview RNN language
models. Let V be the vocabulary size and let
Pt ∈ RV be the probability distribution of the vo-
cabulary at timestep t. Moreover, let Dhn be the
dimension of the hidden state of the n-th RNN, and
let De be the dimensions of the embedding vectors.
Then the RNN language models predict probability
distribution Pt+1 by the following equation:

Pt+1 = softmax(Wh
N
t ), (2)

hnt = f(h
n−1
t , h

n
t−1), (3)

h0t = Ext, (4)

where W ∈ RV×DhN is a weight matrix2, E ∈
RDe×V is a word embedding matrix, xt ∈ {0, 1}V
is a one-hot vector of input word wt at timestep t,
and hnt ∈ RDhn is the hidden state of the n-th RNN
at timestep t. We define hnt at timestep t = 0 as a
zero vector: hn0 = 0. Let f(·) represent an abstract
function of an RNN, which might be the Elman net-
work (Elman, 1990), the Long Short-Term Memory
(LSTM) (Hochreiter and Schmidhuber, 1997), the
Recurrent Highway Network (RHN) (Zilly et al.,
2017), or any other RNN variant. In this research,
we stack three LSTM layers based on Merity et al.
(2018) because they achieved high performance.

3 Language Modeling as Matrix
Factorization

Yang et al. (2018) indicated that the training of
language models can be interpreted as a matrix

2Actually, we apply a bias term in addition to the weight
matrix but we omit it to simplify the following discussion.

factorization problem. In this section, we briefly
introduce their description. Let word sequence
w1:t be context ct. Then we can regard a nat-
ural language as a finite set of the pairs of a
context and its conditional probability distribu-
tion: L = {(c1, P ∗(X|c1)), ..., (cU , P ∗(X|cU ))},
where U is the number of possible contexts and
X ∈ {0, 1}V is a variable representing a one-
hot vector of a word. Here, we consider matrix
A ∈ RU×V that represents the true log probability
distributions and matrix H ∈ RU×DhN that con-
tains the hidden states of the final RNN layer for
each context ct:

A =


logP ∗(X|c1)
logP ∗(X|c2)

...
logP ∗(X|cU )

 ;H =

hNc1
hNc2
...
hNcU

 . (5)
Then we obtain set of matrices F (A) = {A +
ΛS}, where S ∈ RU×V is an all-ones matrix, and
Λ ∈ RU×U is a diagonal matrix. F (A) contains
matrices that shifted each row of A by an arbitrary
real number. In other words, if we take a matrix
from F (A) and apply the softmax function to each
of its rows, we obtain a matrix that consists of true
probability distributions. Therefore, for some A′ ∈
F (A), training RNN language models is to find the
parameters satisfying the following equation:

HW> = A′. (6)

Equation 6 indicates that training RNN language
models can also be interpreted as a matrix factor-
ization problem. In most cases, the rank of matrix
HW> is DhN because DhN is smaller than V and
U in common RNN language models. Thus, an
RNN language model cannot express true distribu-
tions if DhN is much smaller than rank(A′).

Yang et al. (2018) also argued that rank(A′) is
as high as vocabulary size V based on the following
two assumptions:

1. Natural language is highly context-dependent.
In addition, since we can imagine many kinds
of contexts, it is difficult to assume a basis
that represents a conditional probability dis-
tribution for any contexts. In other words,
compressing U is difficult.

2. Since we also have many kinds of semantic
meanings, it is difficult to assume basic mean-
ings that can create all other semantic mean-
ings by such simple operations as addition and
subtraction; compressing V is difficult.



4601

wt

xt

2nd LSTM layer 

wt+1

xt+1

…

…

…

…

Pt+1

Equation (7)

Input word

One-hot vector

Embedding layer

1st LSTM layer 

Equation (11)

 

Figure 1: Overview of the proposed method: DOC.
This figure represents the example of N = 2 and
i0 = i1 = i2 = 3.

In summary, Yang et al. (2018) indicated that DhN
is much smaller than rank(A) because its scale is
usually 102 and vocabulary size V is at least 104.

4 Proposed Method: Direct Output
Connection

To construct a high-rank matrix, Yang et al. (2018)
proposed Mixture of Softmaxes (MoS). MoS com-
putes multiple probability distributions from the
hidden state of final RNN layer hN and regards
the weighted average of the probability distribu-
tions as the final distribution. In this study, we
propose Direct Output Connection (DOC), which
is a generalization method of MoS. DOC computes
probability distributions from the middle layers in
addition to the final layer. In other words, DOC
directly connects the middle layers to the output.

Figure 1 shows an overview of DOC, that uses
the middle layers (including word embeddings) to
compute the probability distributions. Figure 1
computes three probability distributions from all
the layers, but we can vary the number of proba-
bility distributions for each layer and select some
layers to avoid. In our experiments, we search for
the appropriate number of probability distributions
for each layer.

Formally, instead of Equation 2, DOC computes
the output probability distribution at timestep t+ 1
by the following equation:

Pt+1 =
J∑
j=1

πj,ct softmax(W̃kj,ct), (7)

s.t.

J∑
j=1

πj,ct = 1, (8)

where πj,ct is a weight for each probability distri-
bution, kj,ct ∈ Rd is a vector computed from each
hidden state hn, and W̃ ∈ RV×d is a weight matrix.
Thus, Pt+1 is the weighted average of J probability
distributions. We define the U ×U diagonal matrix
whose elements are weight πj,c for each context c
as Φ. Then we obtain matrix Ã ∈ RU×V :

Ã = log

J∑
j=1

Φ softmax(KjW̃
>), (9)

where Kj ∈ RU×d is a matrix whose rows are vec-
tor kj,c. Ã can be an arbitrary high rank because the
righthand side of Equation 9 computes not only the
matrix multiplication but also a nonlinear function.
Therefore, an RNN language model with DOC can
output a distribution matrix whose rank is identical
to one of the true distributions. In other words, Ã
is a better approximation of A′ than the output of a
standard RNN language model.

Next we describe how to acquire weight πj,ct
and vector kj,ct . Let πct ∈ RJ be a vector whose
elements are weight πj,ct . Then we compute πct
from the hidden state of the final RNN layer:

πct = softmax(Wπh
N
t ), (10)

where Wπ ∈ RJ×DhN is a weight matrix. We next
compute kj,ct from the hidden state of the n-th
RNN layer:

kj,ct = Wjh
n
t , (11)

where Wj ∈ Rd×Dhn is a weight matrix. In addi-
tion, let in be the number of kj,ct from h

n
t . Then

we define the sum of in for all n as J ; that is,∑N
n=0 in = J . In short, DOC computes J proba-

bility distributions from all the layers, including the
input embedding (h0). For iN = J , DOC becomes
identical to MoS. In addition to increasing the rank,
we expect that DOC weakens the vanishing gra-
dient problem during backpropagation because a
middle layer is directly connected to the output,
such as with the auxiliary classifiers described in
Szegedy et al. (2015).

For a network that computes the weights for sev-
eral vectors, such as Equation 10, Shazeer et al.
(2017) indicated that it often converges to a state
where it always produces large weights for few
vectors. In fact, we observed that DOC tends to
assign large weights to shallow layers. To prevent
this phenomenon, we compute the coefficient of



4602

variation of Equation 10 in each mini-batch as a reg-
ularization term following Shazeer et al. (2017). In
other words, we try to adjust the sum of the weights
for each probability distribution with identical val-
ues in each mini-batch. Formally, we compute the
following equation for a mini-batch consisting of
wb, wb+1, ..., wb̃:

B =

b̃∑
t=b

πct (12)

β =

(
std(B)

avg(B)

)2
, (13)

where functions std(·) and avg(·) are functions
that respectively return an input’s standard devia-
tion and its average. In the training step, we add
λβ multiplied by weight coefficient β to the loss
function.

5 Experiments on Language Modeling

We investigate the effect of DOC on the language
modeling task. In detail, we conduct word-level
prediction experiments and show that DOC im-
proves the performance of MoS, which only uses
the final layer to compute the probability distribu-
tions. Moreover, we evaluate various combinations
of layers to explore which combination achieves
the best score.

5.1 Datasets
We used the Penn Treebank (PTB) (Marcus et al.,
1993) and WikiText-2 (Merity et al., 2017) datasets,
which are the standard benchmark datasets for
the word-level language modeling task. Mikolov
et al. (2010) and Merity et al. (2017) respectively
published preprocessed PTB3 and WikiText-24

datasets. Table 1 describes their statistics. We used
these preprocessed datasets for fair comparisons
with previous studies.

5.2 Hyperparameters
Our implementation is based on the averaged
stochastic gradient descent Weight-Dropped LSTM
(AWD-LSTM)5 proposed by Merity et al. (2018).
AWD-LSTM consists of three LSTMs with various
regularizations. For the hyperparameters, we used
the same values as Yang et al. (2018) except for the

3http://www.fit.vutbr.cz/ imikolov/rnnlm/
4https://einstein.ai/research/the-wikitext-long-term-

dependency-language-modeling-dataset
5https://github.com/salesforce/awd-lstm-lm

PTB WikiText-2
Vocab 10,000 33,278

Train 929,590 2,088,628
#Token Valid 73,761 217,646

Test 82,431 245,569

Table 1: Statistics of PTB and WikiText-2.

Hyperparameter PTB WikiText-2
Learning rate 20 15
Batch size 12 15
Non-monotone interval 60 60
De 280 300
Dh1 960 1150
Dh2 960 1150
Dh3 620 650
Dropout rate for xt 0.1 0.1
Dropout rate for h0t 0.4 0.65
Dropout rate for h1t , h2t 0.225 0.2
Dropout rate for h3t 0.4 0.4
Dropout rate for kj,ct 0.6 0.6
Recurrent weight dropout 0.50 0.50

Table 2: Hyperparameters used for training DOC.

#DOC
i3 i2 i1 i0 λβ Valid Test
15 0 0 0 0 56.54† 54.44†
20 0 0 0 0 56.88‡ 54.79‡
15 0 0 5 0 56.21 54.28
15 0 5 0 0 55.26 53.52
15 5 0 0 0 54.87 53.15
15 5 0 0 0.0001 54.95 53.16
15 5 0 0 0.001 54.62 52.87
15 5 0 0 0.01 55.13 53.39
10 5 0 5 0 56.46 54.18
10 5 5 0 0 56.00 54.37

Table 3: Perplexities of AWD-LSTM with DOC on the
PTB dataset. We varied the number of probability dis-
tributions from each layer in situation J = 20 except
for the top row. The top row (†) represents MoS scores
reported in Yang et al. (2018) as a baseline. ‡ represents
the perplexity obtained by the implementation of Yang
et al. (2018)6 with identical hyperparameters except for
i3.

dropout rate for vector kj,ct and the non-monotone
interval. Since we found that the dropout rate for
vector kj,ct greatly influences β in Equation 13, we
varied it from 0.3 to 0.6 with 0.1 intervals. We
selected 0.6 because this value achieved the best
score on the PTB validation dataset. For the non-
monotone interval, we adopted the same value as
Zolna et al. (2018). Table 2 summarizes the hyper-
parameters of our experiments.

5.3 Results

Table 3 shows the perplexities of AWD-LSTM with
DOC on the PTB dataset. Each value of columns in

6https://github.com/zihangdai/mos

http://www.fit.vutbr.cz/~imikolov/rnnlm/
https://einstein.ai/research/the-wikitext-long-term-dependency-language-modeling-dataset
https://einstein.ai/research/the-wikitext-long-term-dependency-language-modeling-dataset
https://github.com/salesforce/awd-lstm-lm
https://github.com/zihangdai/mos


4603

λβ Valid Test
0 0.276 0.279

0.0001 0.254 0.252
0.001 0.217 0.213

0.01 0.092 0.086

Table 4: Coefficient of variation of Equation 10:
√
β in

validation and test sets of PTB.

Model Valid Test
AWD-LSTM 401 401
AWD-LSTM-MoS 10000 10000
AWD-LSTM-DOC 10000 10000

Table 5: Rank of output matrix (Ã in Equation 9) on
the PTB dataset. D3 of AWD-LSTM is 400.

0 50 100 150 200 250 300
Epoch

50

60

70

80

90

100

P
er

pl
ex

ity

Model
AWD-LSTM
AWD-LSTM-MoS
Proposed: AWD-LSTM-DOC

Figure 2: Perplexities of each method on the PTB vali-
dation set.

represents the number of probability distributions
from hidden state hnt . To find the best combination,
we varied the number of probability distributions
from each layer by fixing their total to 20: J = 20.
Moreover, the top row of Table 3 shows the per-
plexity of AWD-LSTM with MoS reported in Yang
et al. (2018) for comparison. Table 3 indicates that
language models using middle layers outperformed
one using only the final layer. In addition, Table
3 shows that increasing the distributions from the
final layer (i3 = 20) degraded the score from the
language model with i3 = 15 (the top row of Ta-
ble 3). Thus, to obtain a superior language model,
we should not increase the number of distributions
from the final layer; we should instead use the mid-
dle layers, as with our proposed DOC.

Table 3 shows that the i3 = 15, i2 = 5 setting
achieved the best performance and the other set-
tings with shallow layers have a little effect. This
result implies that we need some layers to output ac-
curate distributions. In fact, most previous studies
adopted two LSTM layers for language modeling.
This suggests that we need at least two layers to
obtain high-quality distributions.

Model Valid Test
AWD-LSTM† 58.88 56.36
AWD-LSTM-MoS† 56.36 54.26
AWD-LSTM-MoS‡ 55.67 53.75
AWD-LSTM-DOC 54.62 52.87
AWD-LSTM-DOC (fin) 54.12 52.38

Table 6: Perplexities of our implementations and re-
runs on the PTB dataset. We set the non-monotone
interval to 60. † represents results obtained by original
implementations with identical hyperparameters except
for non-monotone interval. ‡ indicates the result ob-
tained by our AWD-LSTM-MoS implementation with
identical dropout rates as AWD-LSTM-DOC. For (fin),
we repeated fine-tuning until convergence.

For the i3 = 15, i2 = 5 setting, we explored
the effect of λβ in {0, 0.01, 0.001, 0.0001}. Al-
though Table 3 shows that λβ = 0.001 achieved
the best perplexity, the effect is not consistent. Ta-
ble 4 shows the coefficient of variation of Equa-
tion 10, i.e.,

√
β in the PTB dataset. This table

demonstrates that the coefficient of variation de-
creases with growth in λβ . In other words, the
model trained with a large λβ assigns balanced
weights to each probability distribution. These
results indicate that it is not always necessary to
equally use each probability distribution, but we
can acquire a better model in some λβ . Hereafter,
we refer to the setting that achieved the best score
(i3 = 15, i2 = 5, λβ = 0.001) as AWD-LSTM-
DOC.

Table 5 shows the ranks of matrices containing
log probability distributions from each method. In
other words, Table 5 describes Ã in Equation 9 for
each method. As shown by this table, the output
of AWD-LSTM is restricted to D37. In contrast,
AWD-LSTM-MoS (Yang et al., 2018) and AWD-
LSTM-DOC outputted matrices whose ranks equal
the vocabulary size. This fact indicates that DOC
(including MoS) can output the same matrix as the
true distributions in view of a rank.

Figure 2 illustrates the learning curves of each
method on PTB. This figure contains the valida-
tion scores of AWD-LSTM, AWD-LSTM-MoS,
and AWD-LSTM-DOC at each training epoch. We
trained AWD-LSTM and AWD-LSTM-MoS by
setting the non-monotone interval to 60, as with
AWD-LSTM-DOC. In other words, we used hyper-
parameters identical to the original ones to train
AWD-LSTM and AWD-LSTM-MoS, except for
the non-monotone interval. We note that the opti-

7Actually, the maximum rank size of an ordinary RNN
language model is DN + 1 when we use a bias term.



4604

Model #Param Valid Test
LSTM (medium) (Zaremba et al., 2014) 20M 86.2 82.7
LSTM (large) (Zaremba et al., 2014) 66M 82.2 78.4
Variational LSTM (medium) (Gal and Ghahramani, 2016) 20M 81.9 ± 0.2 79.7 ± 0.1
Variational LSTM (large) (Gal and Ghahramani, 2016) 66M 77.9 ± 0.3 75.2 ± 0.2
Variational RHN (Zilly et al., 2017) 32M 71.2 68.5
Variational RHN + WT (Zilly et al., 2017) 23M 67.9 65.4
Variational RHN + WT + IOG (Takase et al., 2017) 29M 67.0 64.4
Neural Architecture Search (Zoph and Le, 2017) 54M - 62.4
LSTM with skip connections (Melis et al., 2018) 24M 60.9 58.3
AWD-LSTM (Merity et al., 2018) 24M 60.0 57.3
AWD-LSTM + Fraternal Dropout (Zolna et al., 2018) 24M 58.9 56.8
AWD-LSTM-MoS (Yang et al., 2018) 22M 56.54 54.44
Proposed method: AWD-LSTM-DOC 23M 54.62 52.87
Proposed method: AWD-LSTM-DOC (fin) 23M 54.12 52.38
Proposed method (ensemble): AWD-LSTM-DOC × 5 114M 49.99 48.44
Proposed method (ensemble): AWD-LSTM-DOC (fin) × 5 114M 48.63 47.17

Table 7: Perplexities of each method on the PTB dataset.

Model #Param Valid Test
Variational LSTM + IOG (Takase et al., 2017) 70M 95.9 91.0
Variational LSTM + WT + AL (Inan et al., 2017) 28M 91.5 87.0
LSTM with skip connections (Melis et al., 2018) 24M 69.1 65.9
AWD-LSTM (Merity et al., 2018) 33M 68.6 65.8
AWD-LSTM + Fraternal Dropout (Zolna et al., 2018) 34M 66.8 64.1
AWD-LSTM-MoS (Yang et al., 2018) 35M 63.88 61.45
Proposed method: AWD-LSTM-DOC 37M 60.97 58.55
Proposed method: AWD-LSTM-DOC (fin) 37M 60.29 58.03
Proposed method (ensemble): AWD-LSTM-DOC × 5 185M 56.14 54.23
Proposed method (ensemble): AWD-LSTM-DOC (fin) × 5 185M 54.91 53.09

Table 8: Perplexities of each method on the WikiText-2 dataset.

mization method converts the ordinary stochastic
gradient descent (SGD) into the averaged SGD at
the point where convergence almost occurs. In Fig-
ure 2, the turning point is the epoch when each
method drastically decreases the perplexity. Figure
2 shows that each method similarly reduces the per-
plexity at the beginning. AWD-LSTM and AWD-
LSTM-MoS were slow to decrease the perplexity
from 50 epochs. In contrast, AWD-LSTM-DOC
constantly decreased the perplexity and achieved a
lower value than the other methods with ordinary
SGD. Therefore, we conclude that DOC positively
affects the training of language modeling.

Table 6 shows the AWD-LSTM, AWD-LSTM-
MoS, and AWD-LSTM-DOC results in our con-
figurations. For AWD-LSTM-MoS, we trained
our implementation with the same dropout rates
as AWD-LSTM-DOC for a fair comparison. AWD-
LSTM-DOC outperformed both the original AWD-
LSTM-MoS and our implementation. In other
words, DOC outperformed MoS.

Since the averaged SGD uses the averaged pa-
rameters from each update step, the parameters
of the early steps are harmful to the final parame-
ters. Therefore, when the model converges, recent

studies and ours eliminate the history of and then
retrains the model. Merity et al. (2018) referred
to this retraining process as fine-tuning. Although
most previous studies only conducted fine-tuning
once, Zolna et al. (2018) argued that two fine-
tunings provided additional improvement. Thus,
we repeated fine-tuning until we achieved no more
improvements in the validation data. We refer to
the model as AWD-LSTM-DOC (fin) in Table 6,
which shows that repeated fine-tunings improved
the perplexity by about 0.5.

Tables 7 and 8 respectively show the perplex-
ities of AWD-LSTM-DOC and previous studies
on PTB and WikiText-28. These tables show that
AWD-LSTM-DOC achieved the best perplexity.
AWD-LSTM-DOC improved the perplexity by al-
most 2.0 on PTB and 3.5 on WikiText-2 from the
state-of-the-art scores. The ensemble technique
provided further improvement, as described in pre-

8We exclude models that use the statistics of the test
data (Grave et al., 2017; Krause et al., 2017) from these tables
because we regard neural language models as the basis of NLP
applications and consider it unreasonable to know correct out-
puts during applications, e.g., machine translation. In other
words, we focus on neural language models as the foundation
of applications although we can combine the method using
the statistics of test data with our AWD-LSTM-DOC.



4605

vious studies (Zaremba et al., 2014; Takase et al.,
2017), and improved the perplexity by at least 4
points on both datasets. Finally, the ensemble of
the repeated finetuning models achieved 47.17 on
the PTB test and 53.09 on the WikiText-2 test.

6 Experiments on Application Tasks

As described in Section 1, a neural encoder-decoder
model can be interpreted as a conditional language
model. To investigate the effect of DOC on an
encoder-decoder model, we incorporate DOC into
the decoder and examine its performance.

6.1 Dataset

We conducted experiments on machine translation
and headline generation tasks. For machine transla-
tion, we used two kinds of sentence pairs (English-
German and English-French) in the IWSLT 2016
dataset9. The training set respectively contains
about 189K and 208K sentence pairs of English-
German and English-French. We experimented in
four settings: from English to German (En-De), its
reverse (De-En), from English to French (En-Fr),
and its reverse (Fr-En).

Headline generation is a task that creates a short
summarization of an input sentence(Rush et al.,
2015). Rush et al. (2015) constructed a headline
generation dataset by extracting pairs of first sen-
tences of news articles and their headlines from the
annotated English Gigaword corpus (Napoles et al.,
2012). They also divided the extracted sentence-
headline pairs into three parts: training, validation,
and test sets. The training set contains about 3.8M
sentence-headline pairs. For our evaluation, we
used the test set constructed by Zhou et al. (2017)
because the one constructed by Rush et al. (2015)
contains some invalid instances, as reported in
Zhou et al. (2017).

6.2 Encoder-Decoder Model

For the base model, we adopted an encoder-decoder
with an attention mechanism described in Kiyono
et al. (2017). The encoder consists of a 2-layer
bidirectional LSTM, and the decoder consists of a
2-layer LSTM with attention proposed by Luong
et al. (2015). We interpreted the layer after com-
puting the attention as the 3rd layer of the decoder.
We refer to this encoder-decoder as EncDec. For
the hyperparameters, we followed the setting of
Kiyono et al. (2017) except for the sizes of hidden

9https://wit3.fbk.eu/

Model En-De De-En En-Fr Fr-En
EncDec 23.05 28.18 34.37 34.07
EncDec+DOC (i3 = 2) 23.62 29.12 36.09 34.41
EncDec+DOC (i3 = i2 = 2) 23.97 29.33 36.11 34.72

Table 9: BLEU scores on test sets in the IWSLT 2016
dataset. We report averages of three runs.

Model RG-1 RG-2 RG-L
EncDec 46.77 24.87 43.58
EncDec+DOC (i3 = 2) 46.91 24.91 43.73
EncDec+DOC (i3 = i2 = 2) 46.99 25.29 43.83
ABS (Rush et al., 2015) 37.41 15.87 34.70
SEASS (Zhou et al., 2017) 46.86 24.58 43.53
Kiyono et al. (2017) 46.34 24.85 43.49

Table 10: ROUGE F1 scores in headline generation
test data provided by Zhou et al. (2017). RG in table
denotes ROUGE. For our implementations (the upper
part), we report averages of three runs.

states and embeddings. We used 500 for machine
translation and 400 for headline generation. We
constructed a vocabulary set by using Byte-Pair-
Encoding10 (BPE) (Sennrich et al., 2016). We set
the number of BPE merge operations at 16K for
the machine translation and 5K for the headline
generation.

In this experiment, we compare DOC to the base
EncDec. We prepared two DOC settings: using
only the final layer, that is, a setting that is identical
to MoS, and using both the final and middle layers.
We used the 2nd and 3rd layers in the latter setting
because this case achieved the best performance on
the language modeling task in Section 5.3. We set
i3 = 2 and i2 = 2, i3 = 2. For this experiment,
we modified a publicly available encode-decoder
implementation11.

6.3 Results

Table 9 shows the BLEU scores of each method.
Since an initial value often drastically varies the
result of a neural encoder-decoder, we reported
the average of three models trained from different
initial values and random seeds. Table 9 indicates
that EncDec+DOC outperformed EncDec.

Table 10 shows the ROUGE F1 scores of each
method. In addition to the results of our imple-
mentations (the upper part), the lower part repre-
sents the published scores reported in previous stud-
ies. For the upper part, we reported the average of
three models (as in Table 9). EncDec+DOC outper-
formed EncDec on all scores. Moreover, EncDec

10https://github.com/rsennrich/subword-nmt
11https://github.com/mlpnlp/mlpnlp-nmt/

https://wit3.fbk.eu/
https://github.com/rsennrich/subword-nmt
https://github.com/mlpnlp/mlpnlp-nmt/


4606

outperformed the state-of-the-art method (Zhou
et al., 2017) on the ROUGE-2 and ROUGE-L F1
scores. In other words, our baseline is already very
strong. We believe that this is because we adopted
a larger embedding size than Zhou et al. (2017). It
is noteworthy that DOC improved the performance
of EncDec even though EncDec is very strong.

These results indicate that DOC positively influ-
ences a neural encoder-decoder model. Using the
middle layer also yields further improvement be-
cause EncDec+DOC (i3 = i2 = 2) outperformed
EncDec+DOC (i3 = 2).

7 Experiments on Constituency Parsing

Choe and Charniak (2016) achieved high F1 scores
on the Penn Treebank constituency parsing task
by transforming candidate trees into a symbol se-
quence (S-expression) and reranking them based
on the perplexity obtained by a neural language
model. To investigate the effectiveness of DOC,
we evaluate our language models following their
configurations.

7.1 Dataset

We used the Wall Street Journal of the Penn Tree-
bank dataset. We used the section 2-21 for train-
ing, 22 for validation, and 23 for testing. We ap-
plied the preprocessing codes of Choe and Char-
niak (2016)12 to the dataset and converted a token
that appears fewer than ten times in the training
dataset into a special token unk. For reranking, we
prepared 500 candidates obtained by the Charniak
parser (Charniak, 2000).

7.2 Models

We compare AWD-LSTM-DOC with AWD-
LSTM (Merity et al., 2018) and AWD-LSTM-
MoS (Yang et al., 2018). We trained each model
with the same hyperparameters from our language
modeling experiments (Section 5). We selected
the model that achieved the best perplexity on the
validation set during the training.

7.3 Results

Table 11 shows the bracketing F1 scores on the
PTB test set. This table is divided into three
parts by horizontal lines; the upper part describes
the scores by single language modeling based
rerankers, the middle part shows the results by en-
sembling five rerankers, and the lower part repre-

12https://github.com/cdg720/emnlp2016

F1
Model Base Rerank

Reranking with single model
Choe and Charniak (2016) 89.7 92.6
AWD-LSTM 89.7 93.2
AWD-LSTM-MoS 89.7 93.2
AWD-LSTM-DOC 89.7 93.3

Reranking with model ensemble
AWD-LSTM × 5 (ensemble) 89.7 93.4
AWD-LSTM-MoS × 5 (ensemble) 89.7 93.4
AWD-LSTM-DOC × 5 (ensemble) 89.7 93.5
AWD-LSTM-DOC × 5 (ensemble) 91.2 94.29
AWD-LSTM-DOC × 5 (ensemble) 93.12 94.47

State-of-the-art results
Dyer et al. (2016) 91.7 93.3
Fried et al. (2017) (ensemble) 92.72 94.25
Suzuki et al. (2018) (ensemble) 92.74 94.32
Kitaev and Klein (2018) 95.13 -

Table 11: Bracketing F1 scores on the PTB test set (Sec-
tion 23). This table includes reranking models trained
on the PTB without external data.

sents the current state-of-the-art scores in the set-
ting without external data. The upper part also
contains the score reported in Choe and Char-
niak (2016) that reranked candidates by the simple
LSTM language model. This part indicates that
our implemented rerankers outperformed the sim-
ple LSTM language model based reranker, which
achieved 92.6 F1 score (Choe and Charniak, 2016).
Moreover, AWD-LSTM-DOC outperformed AWD-
LSTM and AWD-LSTM-MoS. These results corre-
spond to the language modeling task.

The middle part shows that AWD-LSTM-DOC
also outperformed AWD-LSTM and AWD-LSTM-
MoS in the ensemble setting. In addition, we can
improve the performance by exchanging the base
parser with a stronger one. In fact, we achieved
94.29 F1 score by reranking the candidates from
retrained Recurrent Neural Network Grammars
(RNNG) (Dyer et al., 2016)13, that achieved 91.2
F1 score in our configuration. Moreover, the low-
est row of the middle part indicates the result by
reranking the candidates from the retrained neural
encoder-decoder based parser (Suzuki et al., 2018).
Our base parser has two different parts from Suzuki
et al. (2018). First, we used the sum of the hidden
states of the forward and backward RNNs as the
hidden layer for each RNN14. Second, we tied the
embedding matrix to the weight matrix to compute

13The output of RNNG is not in descending order because
it samples candidates based on their scores. Thus, we pre-
pared more candidates (i.e., 700) to be able to obtain correct
instances as candidates.

14We used the deep bidirectional encoder described at
http://opennmt.net/OpenNMT/training/models/ instead of a
basic bidirectional encoder.

https://github.com/cdg720/emnlp2016
http://opennmt.net/OpenNMT/training/models/


4607

the probability distributions in the decoder. The
retrained parser achieved 93.12 F1 score. Finally,
we achieved 94.47 F1 score by reranking its candi-
dates with AWD-LSTM-DOC. We expect that we
can achieve even better score by replacing the base
parser with the current state-of-the-art one (Kitaev
and Klein, 2018).

8 Related Work

Bengio et al. (2003) are pioneers of neural language
models. To address the curse of dimensionality
in language modeling, they proposed a method
using word embeddings and a feed-forward neu-
ral network (FFNN). They demonstrated that their
approach outperformed n-gram language models,
but FFNN can only handle fixed-length contexts.
Instead of FFNN, Mikolov et al. (2010) applied
RNN (Elman, 1990) to language modeling to ad-
dress the entire given sequence as a context. Their
method outperformed the Kneser-Ney smoothed
5-gram language model (Kneser and Ney, 1995;
Chen and Goodman, 1996).

Researchers continue to try to improve the per-
formance of RNN language models. Zaremba et al.
(2014) used LSTM (Hochreiter and Schmidhuber,
1997) instead of a simple RNN for language mod-
eling and significantly improved an RNN language
model by applying dropout (Srivastava et al., 2014)
to all the connections except for the recurrent con-
nections. To regularize the recurrent connections,
Gal and Ghahramani (2016) proposed variational
inference-based dropout. Their method uses the
same dropout mask at each timestep. Zolna et al.
(2018) proposed fraternal dropout, which mini-
mizes the differences between outputs from dif-
ferent dropout masks to be invariant to the dropout
mask. Melis et al. (2018) used black-box opti-
mization to find appropriate hyperparameters for
RNN language models and demonstrated that the
standard LSTM with proper regularizations can
outperform other architectures.

Apart from dropout techniques, Inan et al. (2017)
and Press and Wolf (2017) proposed the word tying
method (WT), which unifies word embeddings (E
in Equation 4) with the weight matrix to compute
probability distributions (W in Equation 2). In ad-
dition to quantitative evaluation, Inan et al. (2017)
provided a theoretical justification for WT and pro-
posed the augmented loss technique (AL), which
computes an objective probability based on word
embeddings. In addition to these regularization

techniques, Merity et al. (2018) used DropCon-
nect (Wan et al., 2013) and averaged SGD (Polyak
and Juditsky, 1992) for an LSTM language model.
Their AWD-LSTM achieved lower perplexity than
Melis et al. (2018) on PTB and WikiText-2.

Previous studies also explored superior archi-
tecture for language modeling. Zilly et al. (2017)
proposed recurrent highway networks that use high-
way layers (Srivastava et al., 2015) to deepen re-
current connections. Zoph and Le (2017) adopted
reinforcement learning to construct the best RNN
structure. However, as mentioned, Melis et al.
(2018) established that the standard LSTM is supe-
rior to these architectures. Apart from RNN archi-
tecture, Takase et al. (2017) proposed the input-to-
output gate (IOG), which boosts the performance
of trained language models.

As described in Section 3, Yang et al. (2018) in-
terpreted training language modeling as matrix fac-
torization and improved performance by computing
multiple probability distributions. In this study, we
generalized their approach to use the middle lay-
ers of RNNs. Finally, our proposed method, DOC,
achieved the state-of-the-art score on the standard
benchmark datasets.

Some studies provided methods that boost per-
formance by using statistics obtained from test data.
Grave et al. (2017) extended a cache model (Kuhn
and De Mori, 1990) for RNN language models.
Krause et al. (2017) proposed dynamic evaluation
that updates parameters based on a recent sequence
during testing. Although these methods might also
improve the performance of DOC, we omitted such
investigation to focus on comparisons among meth-
ods trained only on the training set.

9 Conclusion

We proposed Direct Output Connection (DOC), a
generalization method of MoS introduced by Yang
et al. (2018). DOC raises the expressive power
of RNN language models and improves quality of
the model. DOC outperformed MoS and achieved
the best perplexities on the standard benchmark
datasets of language modeling: PTB and WikiText-
2. Moreover, we investigated its effectiveness on
machine translation and headline generation. Our
results show that DOC also improved the perfor-
mance of EncDec and using a middle layer posi-
tively affected such application tasks.



4608

References
Yoshua Bengio, Réjean Ducharme, Pascal Vincent, and

Christian Janvin. 2003. A neural probabilistic lan-
guage model. Journal of Machine Learning Re-
search, 3:1137–1155.

Eugene Charniak. 2000. A maximum-entropy-inspired
parser. In 1st Meeting of the North American Chap-
ter of the Association for Computational Linguistics
(NAACL 2000), pages 132–139.

Stanley F. Chen and Joshua Goodman. 1996. An em-
pirical study of smoothing techniques for language
modeling. In Proceedings of the 34th Annual Meet-
ing on Association for Computational Linguistics
(ACL 1996), pages 310–318.

Do Kook Choe and Eugene Charniak. 2016. Parsing as
language modeling. In Proceedings of the 2016 Con-
ference on Empirical Methods in Natural Language
Processing (EMNLP 2016), pages 2331–2336.

Chris Dyer, Adhiguna Kuncoro, Miguel Ballesteros,
and Noah A. Smith. 2016. Recurrent neural network
grammars. In Proceedings of the 2016 Conference
of the North American Chapter of the Association
for Computational Linguistics: Human Language
Technologies (NAACL-HLT 2016), pages 199–209.

Jeffrey L Elman. 1990. Finding Structure in Time.
Cognitive science, 14(2):179–211.

Daniel Fried, Mitchell Stern, and Dan Klein. 2017. Im-
proving neural parsing by disentangling model com-
bination and reranking effects. In Proceedings of the
55th Annual Meeting of the Association for Compu-
tational Linguistics (ACL 2017), pages 161–166.

Yarin Gal and Zoubin Ghahramani. 2016. A Theoreti-
cally Grounded Application of Dropout in Recurrent
Neural Networks. In Advances in Neural Informa-
tion Processing Systems 29 (NIPS 2016).

Edouard Grave, Armand Joulin, and Nicolas Usunier.
2017. Improving Neural Language Models with a
Continuous Cache. In Proceedings of the 5th Inter-
national Conference on Learning Representations
(ICLR 2017).

Sepp Hochreiter and Jürgen Schmidhuber. 1997.
Long Short-Term Memory. Neural Computation,
9(8):1735–1780.

Hakan Inan, Khashayar Khosravi, and Richard Socher.
2017. Tying Word Vectors and Word Classifiers:
A Loss Framework for Language Modeling. In
Proceedings of the 5th International Conference on
Learning Representations (ICLR 2017).

Nikita Kitaev and Dan Klein. 2018. Constituency pars-
ing with a self-attentive encoder. In Proceedings
of the 56th Annual Meeting of the Association for
Computational Linguistics (ACL 2018), pages 2676–
2686.

Shun Kiyono, Sho Takase, Jun Suzuki, Naoaki
Okazaki, Kentaro Inui, and Masaaki Nagata. 2017.
Source-side prediction for neural headline genera-
tion. CoRR.

Reinhard Kneser and Hermann Ney. 1995. Improved
backing-off for m-gram language modeling. In
In Proceedings of the IEEE International Confer-
ence on Acoustics, Speech and Signal Processing
(ICASSP 1995), pages 181–184.

Ben Krause, Emmanuel Kahembwe, Iain Murray, and
Steve Renals. 2017. Dynamic evaluation of neural
sequence models. CoRR.

Roland Kuhn and Renato De Mori. 1990. A cache-
based natural language model for speech recogni-
tion. 12:570–583.

Thang Luong, Hieu Pham, and Christopher D. Man-
ning. 2015. Effective approaches to attention-based
neural machine translation. In Proceedings of the
2015 Conference on Empirical Methods in Natural
Language Processing (EMNLP 2015), pages 1412–
1421.

Mitchell P. Marcus, Mary Ann Marcinkiewicz, and
Beatrice Santorini. 1993. Building a Large Anno-
tated Corpus of English: The Penn Treebank. Com-
putational Linguistics, 19(2):313–330.

Gábor Melis, Chris Dyer, and Phil Blunsom. 2018. On
the state of the art of evaluation in neural language
models. Proceedings of the 6th International Con-
ference on Learning Representations (ICLR 2018).

Stephen Merity, Nitish Shirish Keskar, and Richard
Socher. 2018. Regularizing and Optimizing LSTM
Language Models. In Proceedings of the 6th Inter-
national Conference on Learning Representations
(ICLR 2018).

Stephen Merity, Caiming Xiong, James Bradbury,
and Richard Socher. 2017. Pointer Sentinel Mix-
ture Models. In Proceedings of the 5th Inter-
national Conference on Learning Representations
(ICLR 2017).

Tomas Mikolov, Martin Karafiát, Lukás Burget, Jan
Cernocký, and Sanjeev Khudanpur. 2010. Recurrent
neural network based language model. In Proceed-
ings of the 11th Annual Conference of the Interna-
tional Speech Communication Association (INTER-
SPEECH 2010), pages 1045–1048.

Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Cor-
rado, and Jeff Dean. 2013. Distributed Representa-
tions of Words and Phrases and their Compositional-
ity. In Advances in Neural Information Processing
Systems 26 (NIPS 2013), pages 3111–3119.

Andriy Mnih and Koray Kavukcuoglu. 2013. Learn-
ing Word Embeddings Efficiently with Noise-
Contrastive Estimation. In Advances in Neural
Information Processing Systems 26 (NIPS 2013),
pages 2265–2273.



4609

Courtney Napoles, Matthew Gormley, and Benjamin
Van Durme. 2012. Annotated gigaword. In Pro-
ceedings of the Joint Workshop on Automatic Knowl-
edge Base Construction and Web-scale Knowledge
Extraction, pages 95–100.

Matthew Peters, Mark Neumann, Mohit Iyyer, Matt
Gardner, Christopher Clark, Kenton Lee, and Luke
Zettlemoyer. 2018. Deep contextualized word repre-
sentations. In Proceedings of the 2018 Conference
of the North American Chapter of the Association
for Computational Linguistics: Human Language
Technologies (NAACL 2018), pages 2227–2237.

Boris T Polyak and Anatoli B Juditsky. 1992. Ac-
celeration of Stochastic Approximation by Averag-
ing. SIAM Journal on Control and Optimization,
30(4):838–855.

Ofir Press and Lior Wolf. 2017. Using the Output Em-
bedding to Improve Language Models. In Proceed-
ings of the 15th Conference of the European Chap-
ter of the Association for Computational Linguistics
(EACL 2017), pages 157–163.

Alexander M. Rush, Sumit Chopra, and Jason Weston.
2015. A Neural Attention Model for Abstractive
Sentence Summarization. In Proceedings of the
2015 Conference on Empirical Methods in Natural
Language Processing (EMNLP 2015), pages 379–
389.

Rico Sennrich, Barry Haddow, and Alexandra Birch.
2016. Improving neural machine translation mod-
els with monolingual data. In Proceedings of the
54th Annual Meeting of the Association for Compu-
tational Linguistics (ACL 2016), pages 86–96.

Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz,
Andy Davis, Quoc V. Le, Geoffrey E. Hinton, and
Jeff Dean. 2017. Outrageously large neural net-
works: The sparsely-gated mixture-of-experts layer.
In Proceedings of the 5th International Conference
on Learning Representations (ICLR 2017).

Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky,
Ilya Sutskever, and Ruslan Salakhutdinov. 2014.
Dropout: A simple way to prevent neural networks
from overfitting. Journal of Machine Learning Re-
search, 15(1):1929–1958.

Rupesh Kumar Srivastava, Klaus Greff, and Jürgen
Schmidhuber. 2015. Highway networks. In Pro-
ceedings of the Deep Learning Workshop in ICML
15.

Ilya Sutskever, Oriol Vinyals, and Quoc V. Le. 2014.
Sequence to Sequence Learning with Neural Net-
works. In Advances in Neural Information Process-
ing Systems 27 (NIPS 2014), pages 3104–3112.

Jun Suzuki, Sho Takase, Hidetaka Kamigaito, Makoto
Morishita, and Masaaki Nagata. 2018. An empirical
study of building a strong baseline for constituency
parsing. In Proceedings of the 56th Annual Meet-
ing of the Association for Computational Linguistics
(ACL 2018), pages 612–618.

Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Ser-
manet, Scott Reed, Dragomir Anguelov, Dumitru Er-
han, Vincent Vanhoucke, and Andrew Rabinovich.
2015. Going deeper with convolutions. In Proceed-
ings of the 28th IEEE Conference on Computer Vi-
sion and Pattern Recognition (CVPR 2015), pages
1–9.

Sho Takase, Jun Suzuki, and Masaaki Nagata. 2017.
Input-to-output gate to improve rnn language mod-
els. In Proceedings of the Eighth International Joint
Conference on Natural Language Processing (IJC-
NLP 2017), pages 43–48.

Li Wan, Matthew Zeiler, Sixin Zhang, Yann L Cun, and
Rob Fergus. 2013. Regularization of Neural Net-
works using DropConnect. In Proceedings of the
30th International Conference on Machine Learning
(ICML 2013), pages 1058–1066.

Tsung-Hsien Wen, Milica Gasic, Nikola Mrkšić, Pei-
Hao Su, David Vandyke, and Steve Young. 2015. Se-
mantically Conditioned LSTM-based Natural Lan-
guage Generation for Spoken Dialogue Systems. In
Proceedings of the 2015 Conference on Empirical
Methods in Natural Language Processing (EMNLP
2015), pages 1711–1721.

Zhilin Yang, Zihang Dai, Ruslan Salakhutdinov, and
William W. Cohen. 2018. Breaking the softmax
bottleneck: A high-rank RNN language model. In
Proceedings of the 6th International Conference on
Learning Representations (ICLR 2018).

Wojciech Zaremba, Ilya Sutskever, and Oriol Vinyals.
2014. Recurrent neural network regularization. In
Proceedings of the 2nd International Conference on
Learning Representations (ICLR 2014).

Qingyu Zhou, Nan Yang, Furu Wei, and Ming Zhou.
2017. Selective encoding for abstractive sentence
summarization. In Proceedings of the 55th Annual
Meeting of the Association for Computational Lin-
guistics (ACL 2017), pages 1095–1104.

Julian Georg Zilly, Rupesh Kumar Srivastava, Jan
Koutnı́k, and Jürgen Schmidhuber. 2017. Recurrent
Highway Networks. Proceedings of the 34th Inter-
national Conference on Machine Learning (ICML
2017), pages 4189–4198.

Konrad Zolna, Devansh Arpit, Dendi Suhubdy, and
Yoshua Bengio. 2018. Fraternal dropout. In Pro-
ceedings of the 6th International Conference on
Learning Representations (ICLR 2018).

Barret Zoph and Quoc V. Le. 2017. Neural Archi-
tecture Search with Reinforcement Learning. In
Proceedings of the 5th International Conference on
Learning Representations (ICLR 2017).


