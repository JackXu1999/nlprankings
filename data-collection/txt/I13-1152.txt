










































Dirichlet Processes for Joint Learning of Morphology and PoS Tags


International Joint Conference on Natural Language Processing, pages 1087–1091,
Nagoya, Japan, 14-18 October 2013.

Dirichlet Processes for Joint Learning of Morphology and PoS Tags

Burcu Can

Department of Computer Engineering
Hacettepe University

Beytepe, Ankara 06800 Turkey
burcu.can@hacettepe.edu.tr

Suresh Manandhar

Department of Computer Science
University of York

Heslington, York, YO10 5GH, UK
suresh.manandhar@york.ac.uk

Abstract

This paper presents a joint model for
learning morphology and part-of-speech
(PoS) tags simultaneously. The proposed
method adopts a finite mixture model that
groups words having similar contextual
features thereby assigning the same PoS
tag to those words. While learning PoS
tags, words are analysed morphologically
by exploiting similar morphological fea-
tures of the learned PoS tags. The results
show that morphology and PoS tags can
be learned jointly in a fully unsupervised
setting.

1 Introduction

The morphology of a word is an important indica-
tor that determines its PoS tag, meanwhile the PoS
tag of a word helps in identifying the correct mor-
phological segmentation of the word. This rela-
tionship between morphology and syntax has been
beneficial in both morphology learning with the
exploitation of the syntactic features and in PoS
tagging with the adoption of morphological fea-
tures.

There has been a number of research that have
performed PoS tagging by making use of morpho-
logical information (Clark (2003), Hasan and Ng
(2009), Abend et al. (2010), Christodoulopoulos
et al. (2011), etc.). There has been also a number
of other research that have performed morpholog-
ical segmentation by adopting syntactic informa-
tion (Hu et al. (2005), Can and Manandhar (2009),
Lee et al. (2011), etc.). However, there is a small
number of research that combines two tasks in a
single framework.

Sirts and Alumäe (2012) share a similar goal

with us in joining PoS tagging and morphologi-
cal segmentation in a single framework. They use
hierarchical Dirichlet process for infinite HMMs
to induce both PoS tags and morphological seg-
mentation. Their model is type-based, whereas
our model is token based. In our model, we use
finite mixture models for PoS tagging and Dirich-
let processes for segmentation.

2 Model Definition

The generative story of the model goes as follows:

1. Draw a PoS tag c
i

.
2. Generate a word w

i

that belongs to c
i

.
3. Generate the context c

i�1,i+1 of the word wi
from c

i

.
4. From the possible splits of w

i

, generate a
suffix m

i

conditioned on c
i

, such that w
i

=
s
i

+ m
i

, where s
i

denotes the stem.

The generative story is summarised as follows:

p(c
i

, c
i�1,i+1, wi, s, m) = p(ci)p(ci�1,i+1|ci)

p(w
i

|c
i

)p(m|c
i

)p(s)

2.1 PoS Tagging

The model adopts a finite mixture model for PoS
tagging (see Figure 1). Each mixture component
represents a PoS tag that shares a set of features
with other members in the same component. Each
mixture component c

i

consists of 1. a distribution
over contexts and 2. a distribution over words.
Each context is a PoS tag pair < c

i�1, ci+1 >
where the previous word w

i�1 belongs to ci�1 and
the following word w

i+1 belongs to ci+1. We em-
ploy a token-based approach for PoS tagging due
to the significance of the context. The model is

1087



π

φ

ci

wi

K

Ci-1,i+1

κ

θw
Wci

β

θc,c’
L

si mi

γs

Hs
S

γm

Hm
M

W

wi

PoS Tagging

Morphology Learning

Figure 1: The complete joint model.

defined formally as follows:
c
i

⇠ Mult(�) (1)
� ⇠ Dir(⇡) (2)

w
i

|c
i

⇠ Mult(✓
w

) (3)
✓
w

⇠ Dir() (4)
c
i�1,i+1|ci ⇠ Mult(✓

c,c

0) (5)
✓
c,c

0 ⇠ Dir(�) (6)

Class indicators c
i

are drawn from a Multino-
mial distribution with parameters � (which have
a Dirichlet prior distribution with hyperparame-
ters ⇡). Each c

i

involves a set of words w
i

drawn
from a Multinomial distribution with parameters
✓
w

(which have a Dirichlet prior distribution with
hyperparameters ). Each c

i

also involves a distri-
bution over contexts c

i�1,i+1 drawn from a Multi-
nomial distribution with parameters: ✓

c,c

0 (which
have a prior distribution with hyperparameters �).

2.2 Morphology Learning

We model morphology using a Dirichlet process
(DP) in order to split each word into a stem and
a suffix (see Figure 1). Stems are generated by
DP (�

s

, H
s

) with concentration parameter �
s

and
base distribution H

s

, whereas suffixes are gener-
ated by DP (�

m

, H
m

) with concentration param-
eter �

m

and base distribution H
m

. Hence, the

model is defined formally as follows:
s
i

⇠ DP (�
s

, H
s

)

m
i

|c
i

⇠ DP (�
m

, H
m

)

Base distributions are length priors that favour
shorter morphs (Creutz and Lagus, 2005):

H
x

(x
i

) = p(c
ij

)|xi| (7)
where x

i

is a morph and |x
i

| is the length of x
i

in
letters. Each character has a probability of p(c

ij

),
where characters are assumed to be distributed
uniformly in the alphabet. We also assume that
each morph ends with a special character; i.e. end
of morph marker.

Here, DP (�
s

, H
s

) is a global Dirichlet process
where stems may belong to any PoS tag, whereas
DP (�

m

, H
m

) is defined locally for each PoS tag.
The reason is that stems are shared amongst dif-
ferent PoS tags. However, words belonging to
the same PoS tag usually have similar endings,
thereby leading to local distributions.

3 Inference

In our model, we assign values to the hyperpa-
rameters ⇡, , �, �

s

, �
m

empirically, and we inte-
grate out the parameters �, ✓

w

, ✓
c,c

0 by using the
Multinomial-Dirichlet conjugacy.

We use Gibbs sampling to infer POS tags, stems
and suffixes. We perform inference in two steps:
1. a PoS tag is sampled for the word, 2. a stem and
a suffix are sampled for the word.

3.1 Inferring PoS tags

Each word’s PoS tag is sampled subject to its con-
text. Let a word be w

i

and imagine that it occurs
in context < w

i�1, wi+1 > where wi�1 belongs
to c

i�1 and wi+1 belongs to ci+1. We define the
sampling probability of c

i

for w
i

as follows:

p(c
i

| < w
i�1, wi+1 >, wi) / p(< wi�1, wi+1 >, wi|ci)p(ci)

/ p(w
i

|c
i

)p(< w
i�1, wi+1 > |ci)

p(c
i

)

We also assume that < w
i�1, wi+1 > and wi are

independent since it is possible to remove w
i

from
< w

i�1, wi+1 > and insert another word instead.
In order to calculate p(w

i

|c
i

), w
i

is removed
from the corpus:

p(w
i

|c�wi
i

, ) =
n

wi,c
�wi
i

+ 

N�wi
ci + W

c

�wi
i

↵
(8)

where c�wi
i

denotes the mixture component c
i

that
excludes w

i

, n
wi,c

�wi
i

is the number of the word-
tag pairs < w

i

, c
i

>, N�wi
ci

is the number of word

1088



50 100 150 200 250

32
36

40
44

Corpus Size

M
an

y-
to

-1
 A

cc
ur

ac
y

Figure 2: Many-to-1 accuracy scores obtained
from corpora of size 24K, 36K, 48K, 60K, 72K,
84K, 96K, 120K, and 250K.

tokens having the PoS tag c
i

, W
c

�wi
i

is the num-
ber of word types that are tagged with c

i

. p(c
i

) is
computed as follows:

p(c
i

|c�wi , ⇡) =
n

c

�wi
i

+ ⇡

N�wi + K⇡
(9)

where N�wi denotes the number of word tokens in
the model excluding w

i

, K is the number of class
indicators (i.e. number of PoS tags).

In order to mitigate the sparsity within the con-
text probabilities, we use the approximation intro-
duced by Clark (2000):

p(< w
i�1, wi+1 > |ci) = p(< ci�1, ci+1 > |ci) (10)

p(w
i�1|ci�1)p(wi+1|ci+1)

where, p(< c
i�1, ci+1 > |ci) is computed such

that:

p(< c
i�1, ci+1 > |cx, cy, cz, ci, �) =

n
c

i�1,ci,ci+1 + �

k
c

i

+ L�
(11)

Here, c
x

is c�<ci�1,ci+1>
i

, c
y

is c�<ci�2,ci>
i�1 , cz is

c
�<ci,ci+2>
i+1 , kci is the number of contexts in ci,

and L denotes the possible number of different
contexts in the model (i.e. K ⇤K).

3.2 Inferring Morphology

Two latent variables are inferred for morphology:
stems and suffixes. The sampling probability for
morphology is defined as follows:

p(w
i

= s
i

+ m
i

|s�i, m�i
c

i

) = p(s
i

|s�i)p(m
i

|m�i
c

i

) (12)

where s�i is the set of stems excluding s
i

, m�i
ci

is
the set of suffixes assigned with c

i

excluding m
i

.
The conditional probability of a stem is:

p(s
i

|s�i, �
s

, H
s

) =
f s

�i
+ �

s

H
s

(s
i

)

T s�i + M s�i�
s

(13)

50 100 150 200 250

4.
95

5.
05

5.
15

Corpus Size

V
ar

ia
tio

n 
of

 In
fo

rm
at

io
n

Figure 3: Variation of Information (VI) obtained
from corpora of size 24K, 36K, 48K, 60K, 72K,
84K, 96K, 120K and 250K.

where f s�i is the frequency of the stem type s
i

al-
ready generated, T s�i is the number of all stems in
the model, and M s�i is the number of stem types
generated excluding s

i

. Similarly, the conditional
probability of a suffix is computed as follows:

p(m
i

|m�mi
ci

, �
m

) =
fm

�i
ci

+ �
m

H
m

(m
i

)

Tm�i
ci

+ Mm�i�
m

(14)

where fm�i
ci

is the frequency of the suffix type m
i

already generated in c
i

, Tm�i
ci

is the number of all
suffixes assigned with PoS tag c

i

, and Mm�i is the
number of suffix types already generated exclud-
ing m

i

.
In the algorithm, initially each word is assigned

a PoS tag and split randomly. The algorithm goes
through each word by sampling a PoS tag, a stem,
and a suffix. All constituents of the respective
word (tag, stem, suffix, context, contexts of ad-
jacent words) are removed from the model before-
hand. This process is repeated for a number of
iterations until a convergence is ensured.

4 Experiments & Evaluation

We used small portions of the Penn WSJ tree-
bank (Marcus et al., 1993) for the experiments.
We manually set the hyperparameters and con-
centration parameters for each experiment: ⇡ =
10�6, � = 10�6,  = 10�6, �

s

= 10�6, �
m

=
10�6. These values were set empirically through
several experiments. We also inserted a special
character at the end of each sentence and assigned
it a distinct PoS tag. No other words could be as-
signed this tag.

4.1 PoS Tagging Results

In our experiments we fixed the number of PoS
tags to 45, which is the number of PoS tags in

1089



V-measure Many-to-one

Christ.11 48.6 57.8
Joint 41.11 59.67
Clark2 63.8 68.8

Christ.2 (Best Pub.)3 67.7 72.0
1 Christodoulopoulos et al. (2011)
2 Clark (2003)
3 Christodoulopoulos et al. (2010)

Table 1: PoS tagging scores.

missing extra wrong correct

Joint 0.72% 28.55% 10.13% 60.60%
Morfessor 15.07% 7.23% 10.22% 67.48%

Table 2: Morphological segmentation scores.

Penn WSJ treebank. We applied many-to-one ac-
curacy by assigning each result tag a gold standard
tag having the highest frequency among the words
assigned with this result tag (see Figure 2). Sec-
ond, we applied one-to-one accuracy which have
similar results with many-to-one scores.

We also measured the variation of information
(VI) (Rosenberg and Hirschberg, 2007) (see Fig-
ure 3). Although there is not a smooth decrement
in VI measure, it improves with the larger datasets
in average1.

Results show that determiners, modal verbs,
prepositions, pronouns, conjunctions, and num-
bers are discovered generally correctly. The most
common error type is due the confusion of nouns
and adjectives. Normally, nouns are distributed
over several PoS tags. Verbs and adverbs are also
generally confused and spread over different tags.

We report our results with a comparison to
other systems in Table 1 by using a dataset of
250K words. We use a small portion of Penn
WSJ treebank for the comparison. The dataset
involves 250K words where the number of word
types is 20957. The other systems are also tested
on a small portion of WSJ involving 16850 word
types, which is reported in Christodoulopoulos et
al. (2011).

Our system outperforms Christodoulopoulos
et al. (2011) with the many-to-one evaluation,
whereas Christodoulopoulos et al. (2011) perform
better than our system based on V-measure eval-
uation. It should be noted that Clark (2003) and
Christodoulopoulos et al. (2010) are both type-
based.

1Although, Figure 3 shows that results for 36k words are
better than results for 48k words, this could be due to the
particular choice of training sets we used.

N
U
LL e ed d in
g s es n en

ot
he
r

Found

NULL

e

ed

d

ing

s

es

n

en

other

Tr
ue

Figure 4: Confusion matrix shows the correla-
tion between found morphs and true morphs. The
shades reflect the number of matchings.

4.2 Morphological Segmentation Results

We performed the evaluation of morphological
segmentation on verbs. We adopted some heuris-
tics that strip off common verb endings such as
-ed, -d, -ing, -s, -es from verbs in order to build
the gold standard. Irregular verbs are introduced
exceptionally and left as they are.

The results obtained from the 96K setting were
used for the evaluation. We ran Morfessor Base-
line (Creutz and Lagus, 2002; Creutz and Lagus,
2005; Creutz and Lagus, 2007) on the verbs in
the same dataset. Table 2 gives the scores where
missing types refers to the case that gold stan-
dard suggests a suffix but no suffix is identified in
the results, extra suffixes means that gold standard
does not identify any suffixes but the results con-
tain suffixes, wrong suffixes implies that both gold
standard and results identify suffixes but they are
not the same, and correct types means that both
gold standard and results contain suffixes and they
match. Our model identifies 12257 suffix types,
whereas Morfessor Baseline identifies 2309 due to
undersegmentation. In addition, confusion matrix
that depicts the result morphs against true morphs
is given in Figure 4.

5 Conclusion

We proposed a model that jointly learns PoS tags
and morphology. The results show that learning
PoS tags and morphology can be performed coop-
eratively.

1090



References

Omri Abend, Roi Reichart, and Ari Rappoport. 2010.
Improved unsupervised pos induction through pro-
totype discovery. In Proceedings of the 48th Annual
Meeting of the Association for Computational Lin-
guistics, ACL ’10, pages 1298–1307, Stroudsburg,
PA, USA. Association for Computational Linguis-
tics.

Burcu Can and Suresh Manandhar. 2009. Cluster-
ing morphological paradigms using syntactic cate-
gories. In Working Notes for the CLEF 2009 Work-
shop, September.

Christos Christodoulopoulos, Sharon Goldwater, and
Mark Steedman. 2010. Two decades of unsuper-
vised pos induction: how far have we come? In
Proceedings of the 2010 Conference on Empirical
Methods in Natural Language Processing, EMNLP
’10, pages 575–584, Stroudsburg, PA, USA. Associ-
ation for Computational Linguistics.

Christos Christodoulopoulos, Sharon Goldwater, and
Mark Steedman. 2011. A Bayesian mixture model
for part-of-speech induction using multiple features.
In Proceedings of the Conference on Empirical
Methods in Natural Language Processing.

Alexander Simon Clark. 2000. Inducing syntactic cat-
egories by context distribution clustering. pages 91–
94.

Alexander Clark. 2003. Combining distributional and
morphological information for part of speech induc-
tion. In Proceedings of the tenth conference on Eu-
ropean chapter of the Association for Computational
Linguistics - Volume 1, EACL ’03, pages 59–66,
Stroudsburg, PA, USA. Association for Computa-
tional Linguistics.

Mathias Creutz and Krista Lagus. 2002. Unsuper-
vised discovery of morphemes. In Proceedings of
the ACL-02 workshop on Morphological and phono-
logical learning - Volume 6, MPL ’02, pages 21–
30, Stroudsburg, PA, USA. Association for Compu-
tational Linguistics.

Mathias Creutz and Krista Lagus. 2005. Unsupervised
morpheme segmentation and morphology induction
from text corpora using morfessor 1.0. Technical
Report A81.

Mathias Creutz and Krista Lagus. 2007. Unsupervised
models for morpheme segmentation and morphol-
ogy learning. ACM Trans. Speech Lang. Process.,
4:3:1–3:34, February.

John Goldsmith. 2001. Unsupervised learning of the
morphology of a natural language. Computational
Linguistics, 27(2):153–198.

Kazi Saidul Hasan and Vincent Ng. 2009.
Weakly supervised part-of-speech tagging for
morphologically-rich, resource-scarce languages. In
Proceedings of the 12th Conference of the European

Chapter of the Association for Computational Lin-
guistics, EACL ’09, pages 363–371, Stroudsburg,
PA, USA. Association for Computational Linguis-
tics.

Yu Hu, Irina Matveeva, John Goldsmith, and Colin
Sprague. 2005. Using morphology and syntax
together in unsupervised learning. In Proceed-
ings of the Workshop on Psychocomputational Mod-
els of Human Language Acquisition, PMHLA ’05,
pages 20–27, Stroudsburg, PA, USA. Association
for Computational Linguistics.

Yoong Keok Lee, Aria Haghighi, and Regina Barzilay.
2011. Modeling syntactic context improves mor-
phological segmentation. In Proceedings of the Fif-
teenth Conference on Computational Natural Lan-
guage Learning, CoNLL ’11, pages 1–9, Strouds-
burg, PA, USA. Association for Computational Lin-
guistics.

Mitchell P. Marcus, Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1993. Building a large annotated
corpus of English: The Penn treebank. Computa-
tional Linguistics, 19(2):313–330.

Andrew Rosenberg and Julia Hirschberg. 2007. V-
measure: A conditional entropy-based external clus-
ter evaluation measure. In Empirical Methods in
Natural Language Processing.

Kairit Sirts and Tanel Alumäe. 2012. A hierarchical
dirichlet process model for joint part-of-speech and
morphology induction. In Proceedings of the 2012
Conference of the North American Chapter of the
Association for Computational Linguistics: Human
Language Technologies, NAACL HLT ’12, pages
407–416, Stroudsburg, PA, USA. Association for
Computational Linguistics.

1091


