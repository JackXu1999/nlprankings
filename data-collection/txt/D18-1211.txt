



















































Deep Relevance Ranking Using Enhanced Document-Query Interactions


Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 1849–1860
Brussels, Belgium, October 31 - November 4, 2018. c©2018 Association for Computational Linguistics

1849

Deep Relevance Ranking Using Enhanced Document-Query Interactions

Ryan McDonald1,2, Georgios-Ioannis Brokos1 and Ion Androutsopoulos1

1Dept. of Informatics, Athens University of Economics and Business, Greece
2Google AI

Abstract

We explore several new models for document
relevance ranking, building upon the Deep
Relevance Matching Model (DRMM) of Guo
et al. (2016). Unlike DRMM, which uses
context-insensitive encodings of terms and
query-document term interactions, we inject
rich context-sensitive encodings throughout
our models, inspired by PACRR’s (Hui et al.,
2017) convolutional n-gram matching fea-
tures, but extended in several ways including
multiple views of query and document inputs.
We test our models on datasets from the
BIOASQ question answering challenge (Tsat-
saronis et al., 2015) and TREC ROBUST 2004
(Voorhees, 2005), showing they outperform
BM25-based baselines, DRMM, and PACRR.

1 Introduction

Document relevance ranking, also known as ad-
hoc retrieval (Harman, 2005), is the task of rank-
ing documents from a large collection using the
query and the text of each document only. This
contrasts with standard information retrieval (IR)
systems that rely on text-based signals in con-
junction with network structure (Page et al., 1999;
Kleinberg, 1999) and/or user feedback (Joachims,
2002). Text-based ranking is particularly impor-
tant when (i) click-logs do not exist or are small,
and (ii) the network structure of the collection is
non-existent or not informative for query-focused
relevance. Examples include various domains in
digital libraries, e.g., patents (Azzopardi et al.,
2010) or scientific literature (Wu et al., 2015; Tsat-
saronis et al., 2015); enterprise search (Hawking,
2004); and personal search (Chirita et al., 2005).

We investigate new deep learning architectures
for document relevance ranking, focusing on term-
based interaction models, where query terms (q-
terms for brevity) are scored relative to a docu-

q1 q2 q3

Doc-Aware Query
Term Encodings

Document Terms

Dense Layers

Term Score Aggregation
Term Gating

Relevance Score

Doc-Query Interaction

d1 d2 … dm

Figure 1: Illustration of DRMM (Guo et al., 2016) for a
query of three terms and a document of m terms.

ment’s terms (d-terms) and their scores are aggre-
gated to produce a relevance score for the docu-
ment. Specifically, we use the Deep Relevance
Matching Model (DRMM) of Guo et al. (2016)
(Fig. 1), which was shown to outperform strong
IR baselines and other recent deep learning meth-
ods. DRMM uses pre-trained word embeddings
for q-terms and d-terms, and cosine similarity his-
tograms (outputs of ⊗ in Fig. 1), each capturing
the similarity of a q-term to all the d-terms of a
particular document. The histograms are fed to
an MLP (dense layers of Fig. 1) that produces the
(document-aware) score of each q-term. Each q-
term score is then weighted using a gating mech-
anism (topmost box nodes in Fig. 1) that exam-
ines properties of the q-term to assess its impor-
tance for ranking (e.g., common words are less im-
portant). The sum of the weighted q-term scores
is the relevance score of the document. This ig-
nores entirely the contexts where the terms occur,
in contrast to recent position-aware models such
as PACRR (Hui et al., 2017) or those based on re-
current representations (Palangi et al., 2016).

In order to enrich DRMM with context-sensitive
representations, we need to change fundamentally
how q-terms are scored. This is because rich



1850

context-sensitive representations – such as input
term encodings based on RNNs or CNNs – require
end-to-end training and histogram construction is
not differentiable. To account for this we inves-
tigate novel query-document interaction mecha-
nisms that are differentiable and show empirically
that they are effective ways to enable end-to-end
training of context-sensitive DRMM models. This
is the primary contribution of this paper.

Overall, we explore several extensions to
DRMM, including: PACRR-like convolutional n-
gram matching features (§3.1); context-sensitive
term encodings (§3.2); query-focused attention-
based document representations (§3.3); pooling to
reward denser term matches and turn rich term
representations into fixed-width vectors (§3.4);
multiple views of terms, e.g., context sensitive, in-
sensitive, exact matches (§3.5).

We test our models on data from the BIOASQ
biomedical question answering challenge (Tsat-
saronis et al., 2015) and TREC ROBUST 2004
(Voorhees, 2005), showing that they outper-
form strong BM25-based baselines (Robertson and
Zaragoza, 2009), DRMM, and PACRR.1

2 Related Work

Document ranking has been studied since the
dawn of IR; classic term-weighting schemes were
designed for this problem (Sparck Jones, 1988;
Robertson and Sparck Jones, 1988). With the
advent of statistical NLP and statistical IR, prob-
abilistic language and topic modeling were ex-
plored (Zhai and Lafferty, 2001; Wei and Croft,
2006), followed recently by deep learning IR
methods (Lu and Li, 2013; Hu et al., 2014; Palangi
et al., 2016; Guo et al., 2016; Hui et al., 2017).

Most document relevance ranking methods fall
within two categories: representation-based, e.g.,
Palangi et al. (2016), or interaction-based, e.g.,
Lu and Li (2013). In the former, representations
of the query and document are generated inde-
pendently. Interaction between the two only hap-
pens at the final stage, where a score is gener-
ated indicating relevance. End-to-end learning
and backpropagation through the network tie the
two representations together. In the interaction-
based paradigm, explicit encodings between pairs
of queries and documents are induced. This al-

1The code and data of our experiments, including word
embeddings, are available at https://github.com/
nlpaueb/deep-relevance-ranking.

lows direct modeling of exact- or near-matching
terms (e.g., synonyms), which is crucial for rele-
vance ranking. Indeed, Guo et al. (2016) showed
that the interaction-based DRMM outperforms pre-
vious representation-based methods. On the other
hand, interaction-based models are less efficient,
since one cannot index a document representation
independently of the query. This is less important,
though, when relevance ranking methods rerank
the top documents returned by a conventional IR
engine, which is the scenario we consider here.

One set of our experiments ranks biomedical
texts. Several methods have been proposed for
the BIOASQ challenge (Tsatsaronis et al., 2015),
mostly based on traditional IR techniques. The
most related work is of Mohan et al. (2017), who
use a deep learning architecture. Unlike our work,
they focus on user click data as a supervised sig-
nal, and they use context-insensitive representa-
tions of document-query interactions. The other
dataset we experiment with, TREC ROBUST 2004
(Voorhees, 2005), has been used extensively to
evaluate traditional and deep learning IR methods.

Document relevance ranking is also related to
other NLP tasks. Passage scoring for question an-
swering (Surdeanu et al., 2008) ranks passages by
their relevance to the question; several deep net-
works have been proposed, e.g., Tan et al. (2015).
Short-text matching/ranking is also related and
has seen recent deep learning solutions (Lu and
Li, 2013; Hu et al., 2014; Severyn and Moschitti,
2015). In document relevance ranking, though,
documents are typically much longer than queries,
which makes methods from other tasks that con-
sider pairs of short texts not directly applicable.

Our starting point is DRMM, to which we add
richer representations inspired by PACRR. Hence,
we first discuss DRMM and PACRR further.

2.1 DRMM
We have already presented an overview of DRMM.
For gating (topmost box nodes of Fig. 1), Guo
et al. (2016) use a linear self-attention:

gi = softmax
(
wTg φg(qi); q1, . . . , qn

)
where φg(qi) is the embedding e(qi) of the i-th q-
term, or its IDF, idf(qi), andwg is a weights vector.
Gating aims to weight the (document-aware) score
of each q-term (outputs of dense layers in Fig. 1)
based on the importance of the term. We found
that φg(qi) = [e(qi); idf(qi)], where ‘;’ is concate-
nation, was optimal for all DRMM-based models.

https://github.com/nlpaueb/deep-relevance-ranking
https://github.com/nlpaueb/deep-relevance-ranking


1851

convolution

row-wise 
k-max pooling

max 
pooling

DocumentQuery

...

...

concatenation
softmax

Query Terms’ IDF

Relevance 
Score

Linear
Layer

...

Relevance 
Score

...

Concatenated Doc-Aware 
Query Term Encodings

...
...

...

Dense
Layers

...
...

(A) PACRR
(B) PACRR-DRMM

Doc-Aware 
Query Term Encodings

...

Figure 2: PACRR (Hui et al., 2017) and PACRR-DRMM.

The crux of the original DRMM are the bucketed
cosine similarity histograms (outputs of ⊗ nodes
in Fig. 1), each capturing the similarity of a q-term
to all the d-terms. In each histogram, each bucket
counts the number of d-terms whose cosine sim-
ilarity to the q-term is within a particular range.
Consider a document with three terms, with co-
sine similarities, s, to a particular q-term qi 0.5,
0.1, −0.3, respectively. If we used two buckets
−1 ≤ s < 0 and 0 ≤ s ≤ 1, then the input to the
dense layers for qi would be 〈1, 2〉. The fixed num-
ber of buckets leads to a fixed-dimension input
for the dense layers and makes the model agnos-
tic to different document and query lengths – one
of DRMM’s main strengths. The main disadvan-
tage is that bucketed similarities are independent
of the contexts where terms occur. A q-term ‘regu-
lated’ will have a perfect match with a d-term ‘reg-
ulated’, even if the former is ‘up regulated’ and the
latter is ‘down regulated’ in context. Also, there is
no reward for term matches that preserve word or-
der, or multiple matches within a small window.

2.2 PACRR

In PACRR (Hui et al., 2017), a query-document
term similarity matrix sim is computed (Fig. 2A).
Each cell (i, j) of sim contains the cosine similar-
ity between the embeddings of a q-term qi and a d-
term dj . To keep the dimensions lq×ld of sim fixed
across queries and documents of varying lengths,
queries are padded to the maximum number of q-
terms lq, and only the first ld terms per document
are retained.2 Then, convolutions (Fig. 2A) of dif-
ferent kernel sizes n × n (n = 2, . . . , lg) are ap-
plied to sim to capture n-gram query-document
similarities. For each size n × n, multiple ker-

2We use PACRR-firstk, which Hui et al. (2017) recommend
when documents fit in memory, as in our experiments.

nels (filters) are used. Max pooling is then applied
along the dimension of the filters (max value of
all filters), followed by row-wise k-max pooling
to capture the strongest k signals between each q-
term and all the d-terms. The resulting matrices
are concatenated into a single matrix where each
row is a document-aware q-term encoding; the IDF
of the q-term is also appended, normalized by ap-
plying a softmax across the IDFs of all the q-terms.
Following Hui et al. (2018), we concatenate the
rows of the resulting matrix into a single vector,
which is passed to an MLP (Fig. 2A, dense layers)
that produces a query-document relevance score.3

The primary advantage of PACRR over DRMM
is that it models context via the n-gram convo-
lutions, i.e., denser n-gram matches and matches
preserving word order are encoded. However, this
context-sensitivity is weak, as the convolutions
operate over the similarity matrix, not directly on
terms or even term embeddings. Also, unlike
DRMM, PACRR requires padding and hyperparam-
eters for maximum number of q-terms (lq) and d-
terms (ld), since the convolutional and dense lay-
ers operate over fixed-size matrices and vectors.
On the other hand, PACRR is end-to-end trainable
– though Hui et al. (2017) use fixed pre-trained
embeddings – unlike DRMM where the bucketed
histograms are not differentiable.

3 New Relevance Ranking Models

3.1 PACRR-DRMM
In a DRMM-like version of PACRR, instead of us-
ing an MLP (dense layers, Fig. 2A) to score the
concatenation of all the (document-aware) q-term
encodings, the MLP independently scores each q-
term encoding (the same MLP for all q-terms,
Fig. 2B); the resulting scores are aggregated via
a linear layer. This version, PACRR-DRMM, per-
forms better than PACRR, using the same number
of hidden layers in the MLPs. Likely this is due to
the fewer parameters of its MLP, which is shared
across the q-term representations and operates on
shorter input vectors. Indeed, in early experiments
PACRR-DRMM was less prone to over-fitting.

In PACRR-DRMM, the scores of the q-terms
(outputs of dense layers, Fig. 2B) are not weighted
by a gating mechanism, unlike DRMM (Fig. 1).
Nevertheless, the IDFs of the q-terms, which are
appended to the q-term encodings (Fig. 2B), are a

3Hui et al. (2017) used an additional LSTM, which was
later replaced by the final concatanation (Hui et al., 2018).



1852

form of term-gating (shortcut passing on informa-
tion about the terms, here their IDFs, to upper lay-
ers) applied before scoring the q-terms. By con-
trast, in DRMM (Fig. 1) term-gating is applied after
q-term scoring, and operates on [e(qi); idf(qi)].

3.2 Context-sensitive Term Encodings

In their original incarnations, DRMM and PACRR
use pre-trained word embeddings that are insen-
sitive to the context of a particular query or doc-
ument where a term occurs. This contrasts with
the plethora of systems that use context-sensitive
word encodings (for each particular occurrence of
a word) in virtually all NLP tasks (Bahdanau et al.,
2014; Plank et al., 2016; Lample et al., 2016). In
general, this is achieved via RNNs, e.g., LSTMs
(Gers et al., 2000), or CNNs (Bai et al., 2018).

In the IR literature, context-sensitivity is typi-
cally viewed through two lenses: term proximity
(Büttcher et al., 2006) and term dependency (Met-
zler and Croft, 2005). The former assumes that
the context around a term match is also relevant,
whereas the latter aims to capture when multiple
terms (e.g., an n-gram) must be matched together.
An advantage of neural network architectures like
RNNs and CNNs is that they can capture both.

In the models below (§§3.3–3.4), an encoder
produces the context-sensitive encoding of each q-
term or d-term from the pre-trained embeddings.
To compute this we use a standard BILSTM encod-
ing scheme and set the context-sentence encod-
ing as the concatenation of the last layer’s hidden
states of the forward and backward LSTMs at each
position. As is common for CNNs and even re-
cent RNN term encodings (Peters et al., 2018), we
use the original term embedding e(ti) as a resid-
ual and combine it with the BILSTM encodings.
Specifically, if

−→
h (ti) and

←−
h (ti) are the last layer’s

hidden states of the left-to-right and right-to-left
LSTMs for term ti, respectively, then we set the
context-sensitive term encoding as:

c(ti) = [
−→
h (ti) + e(ti);

←−
h (ti) + e(ti)] (1)

Since we are adding the original term embedding
to each LSTM hidden state, we require the dimen-
sionality of the hidden layers to be equal to that
of the original embedding. Other methods were
tried, including passing all representations through
an MLP, but these had no effect on performance.

This is an orthogonal way to incorporate context
into the model relative to PACRR. PACRR creates

a query-document similarity matrix and computes
n-gram convolutions over the matrix. Here we in-
corporate context directly into the term encodings;
hence similarities in this space are already context-
sensitive. One way to view this difference is the
point at which context enters the model – directly
during term encoding (Eq. 1) or after term similar-
ity scores have been computed (PACRR, Fig. 2).

3.3 ABEL-DRMM
Using the context-sensitive q-term and d-term en-
codings of §3.2 (Eq. 1), our next extension to
DRMM is to create document-aware q-term encod-
ings that go beyond bucketed histograms of cosine
similarities, the stage in Fig. 1 indicated by⊗. We
focus on differentiable encodings to facilitate end-
to-end training from inputs to relevance scores.

Figure 3 shows the sub-network that computes
the document-aware encoding of a q-term qi in the
new model, given a document d = 〈d1, . . . , dm〉
of m d-terms. We first compute a dot-product4

attention score ai,j for each dj relative to qi:

ai,j = softmax
(
c(qi)

T c(dj); d1, . . . , dm
)

(2)

where c(t) is the context-sensitive encoding of t
(Eq. 1). We then sum the context-sensitive encod-
ings of the d-terms, weighted by their attention
scores, to produce an attention-based representa-
tion dqi of document d from the viewpoint of qi:

dqi =
∑
j

ai,j c(dj) (3)

The Hadamard product (element-wise multiplica-
tion, �) between the (L2-normalized) document
representation dqi and the q-term encoding c(qi)
is then computed and used as the fixed-dimension
document-aware encoding φH(qi) of qi (Fig. 3):

φH(qi) =
dqi
||dqi ||

� c(qi)
||c(qi)||

(4)

The ⊗ nodes and lower parts of the DRMM net-
work of Fig. 1 are now replaced by (multiple
copies of) the sub-network of Fig. 3 (one copy
per q-term), with the � nodes replacing the ⊗
nodes. We call the resulting model Attention-
Based ELement-wise DRMM (ABEL-DRMM).

Intuitively, if the document contains one or
more terms dj that are similar to qi, the attention

4Dot-products have a larger range than other similarity
functions, encouraging low entropy attention distributions.



1853

D
oc

um
en

t
Te

rm
s

...

Q
ue

ry
 

Te
rm

Attention
via dot-product

Attention-based
Doc encoding Doc-Aware

Query Term
Encoding

Element-wise
Function
(e.g., Hadamard Product)

softmax

Figure 3: ABEL-DRMM sub-net. From context-aware
q-term and d-term encodings (Eq. 1), it generates fixed-
dimension document-aware q-term encodings to be
used in DRMM (Fig. 1, replacing ⊗ nodes).

mechanism will have emphasized mostly those
terms and, hence, dqi will be similar to c(qi),
otherwise not. This similarity could have been
measured by the cosine similarity between dqi
and c(qi), but the cosine similarity assigns the
same weight to all the dimensions, i.e., to all the
(L2 normalized) element-wise products in φH(qi),
which cosine similarity just sums. By using the
Hadamard product, we pass on to the upper layers
of DRMM (the dense layers of Fig. 1), which score
each q-term with respect to the document, all the
(normalized) element-wise products of φH(qi), al-
lowing the upper layers to learn which element-
wise products (or combinations of them) are im-
portant when matching a q-term to the document.

Other element-wise functions can also be used
to compare dqi to c(qi), instead of the Hadamard
product (Eq. 4). For example, a vector contain-
ing the squared terms of the Euclidean distance
between dqi and c(qi) could be used instead of
φH(qi). This change had no effect on ABEL-
DRMM’s performance on development data. We
also tried using [dqi ; c(qi)] instead of φH(qi), but
performance on development data deteriorated.

ABEL-DRMM is agnostic to document length,
like DRMM. ABEL-DRMM, however, is trainable
end-to-end, unlike the original DRMM. Still, both
models do not reward higher density matches.

3.4 POSIT-DRMM

Ideally, we want models to reward both the maxi-
mum match between a q-term and a document, but
also the average match (between several q-terms
and the document) to reward documents that have
a higher density of matches. The document-aware
q-term scoring of ABEL-DRMM does not account
for this, as the attention summation hides whether
a single or multiple terms were matched with high
similarity. We also want models to be end-to-end
trainable, like ABEL-DRMM.

Figure 4 (context-sensitive box) outlines a sim-
ple network that produces document-aware q-

One-hot

Position-aware

k-max           avg

max

D
oc

um
en

t
Te

rm
s

...

Query Term

cos-sim(       ,       )

 

D
oc-A

w
are

Q
uery Term

 Encoding
PoolingContext-sensitive

Context-insensitive

Figure 4: POSIT-DRMM with multiple views (+MV).
Three two-dimensional document-aware q-term encod-
ings, one from each view, are produced, concatenated,
and used in DRMM (Fig. 1, replacing ⊗ nodes).

term encodings, replacing the ABEL-DRMM sub-
network of Fig. 3 in the DRMM framework. We
call the resulting model POoled SImilariTy DRMM
(POSIT-DRMM). As in ABEL-DRMM, we compute
an attention score ai,j for each dj relative to qi,
now using cosine similarity (cf. Eq. 2):

ai,j =
c(qi)

T c(dj)

||c(qi)|| ||c(dj)||
(5)

However, we do not use the ai,j scores to compute
a weighted average of the encodings of the d-terms
(cf. Eq. 3), which is also why there is no softmax
in ai,j above (cf. Eq. 2).5 Instead, we concatenate
the attention scores of the m d-terms:

ai = 〈ai,1, . . . , ai,j , . . . , ai,m〉T

and we apply two pooling steps on ai to create a 2-
dimensional document-aware encoding φP (qi) of
the q-term qi (Fig. 4). First max-pooling, which
returns the single best match of qi in the docu-
ment. Then average pooling over a k-max-pooled
version of ai, which represents the average simi-
larity for the top k matching terms:

φP (qi) =
〈

max(ai), avg
(

k-max(ai)
)〉T

POSIT-DRMM has many fewer parameters than
the other models. The input to the upper q-
term scoring dense layers of the DRMM framwork
(Fig. 1) for ABEL-DRMM has the same dimension-
ality as pre-trained term embeddings, on the order
of hundreds. By contrast, the input dimensionality
here is 2. Hence, POSIT-DRMM does not require
deep dense layers, but uses a single layer (depth
1). More information on hyperparameters is pro-
vided in Appendix A (supplementary material).

POSIT-DRMM is closely related to PACRR (and
PACRR-DRMM). Like POSIT-DRMM, PACRR first

5The ai,js still need to be normalized for input to the up-
per layers, but they do not need to be positive summing to
1. This is why we use cosine similarity in Eq. 5 instead of
dot-products combined with softmax of Eq. 2.



1854

computes cosine similarities between all q-terms
and d-terms (Fig. 2). It then applies n-gram con-
volutions to the similarity matrix to inject context-
awareness, and then pooling to create document-
aware q-term representations. Instead, POSIT-
DRMM relies on the fact that the term encodings
are now already context sensitive (Eq. 1) and thus
skips the n-gram convolutions. Again, this is a
choice of when context is injected – during term
encoding or after computing similarity scores.

Mohan et al.’s work (2017) is related in the
sense that for each q-term, document-aware en-
codings are built over the best matching (Eu-
clidean distance) d-term. But again, term encod-
ings are context-insensitive pre-trained word em-
beddings and the model is not trained end-to-end.

3.5 Multiple Views of Terms (+MV)
An extension to ABEL-DRMM and POSIT-DRMM
(or any deep model) is to use multiple views of
terms. The basic POSIT-DRMM produces a two-
dimensional document-aware encoding of each q-
term (Fig. 4, context-sensitive box) viewing the
terms as their context-sensitive encodings (Eq. 1).
Another two-dimensional document-aware q-term
encoding can be produced by viewing the terms
directly as their pre-trained embeddings without
converting them to context-sensitive encodings
(Fig. 4, context-insensitive box). A third view uses
one-hot vector representations of terms, which al-
lows exact term matches to be modeled, as op-
posed to near matches in embedding space. Con-
catenating the outputs of the 3 views, we obtain
6-dimensional document-aware q-term encodings,
leading to a model dubbed POSIT-DRMM+MV. An
example of this multi-view document-aware query
term representation is given in Fig. 5 for a query-
document pair from BIOASQ’s development data.

The multi-view extension of ABEL-DRMM
(ABEL-DRMM+MV) is very similar, i.e., it uses
context-sensitive term encodings, pre-trained term
embeddings, and one-hot term encodings in its
three views. The resulting three document-aware
q-term embeddings can be summed or concate-
nated, though we found the former more effective.

3.6 Alternative Network Structures
The new models (§§3.1–3.5) were selected by ex-
perimenting on development data. Many other ex-
tensions were considered, but not ultimately used
as they were not beneficial empirically, includ-
ing deeper and wider RNNs or CNN encoders (Bai

et al., 2018); combining document-aware encod-
ings from all models; different attention mecha-
nisms, e.g., multi-head (Vaswani et al., 2017).

Pointer Networks (Vinyals et al., 2015) use the
attention scores directly to select an input compo-
nent. POSIT-DRMM does this via max and aver-
age pooling, not argmax. We implemented Pointer
Networks – argmax over ABEL-DRMM attention to
select the best d-term encoding – but empirically
this was similar to ABEL-DRMM. Other architec-
tures considered in the literature include the K-
NRM model of Xiong et al. (2017). This is similar
to both ABEL-DRMM and POSIT-DRMM in that it
can be viewed as an end-to-end version of DRMM.
However, it uses kernels over the query-document
interaction matrix to produce features per q-term.

The work of Pang et al. (2017) is highly related
and investigates many different structures, specif-
ically aimed at incorporating context-sensitivity.
However, unlike our work, Pang et al. first ex-
tract contexts (n-grams) of documents that match
q-terms. Multiple interaction matrices are then
constructed for the entire query relative to each
of these contexts. These document contexts may
match one or more q-terms allowing the model to
incorporate term proximity. These interaction ma-
trices can also be constructed using exact string
match similar to POSIT-DRMM+MV.

4 Experiments

We experiment with ad-hoc retrieval datasets with
hundreds of thousands or millions of documents.
As deep learning models are computationally ex-
pensive, we first run a traditional IR system6 using
the BM25 score (Robertson and Zaragoza, 2009)
and then re-rank the top N returned documents.

4.1 Methods Compared
All systems use an extension proposed by Sev-
eryn and Moschitti (2015), where the relevance
score is combined via a linear model with a set
of extra features. We use four extra features: z-
score normalized BM25 score; percentage of q-
terms with exact match in the document (regular
and IDF weighted); and percentage of q-term bi-
grams matched in the document. The latter three
features were taken from Mohan et al. (2017).

In addition to the models of §§2.1, 2.2, 3.1–3.5,
we used the following baselines: Standard Okapi

6We used Galago (http://www.lemurproject.
org/galago.php, v.3.10). We removed stop words and
applied Krovetz’s stemmer (Krovetz, 1993).

http://www.lemurproject.org/galago.php
http://www.lemurproject.org/galago.php


1855

BM25 (BM25); and BM25 re-ranked with a linear
model over the four extra features (BM25+extra).
These IR baselines are very strong and most re-
cently proposed deep learning models do not beat
them.7 DRMM and PACRR are also strong base-
lines and have shown superior performance over
other deep learning models on a variety of data
(Guo et al., 2016; Hui et al., 2017).8

All hyperparameters were tuned on develop-
ment data and are available in Appendix A. All
models were trained using Adam (Kingma and Ba,
2014) with batches containing a randomly sam-
pled negative example per positive example9 and a
pair-wise loss. As the datasets contain only docu-
ments marked as relevant, negative examples were
sampled from the top N documents (returned by
BM25) that had not been marked as relevant.

We evaluated the models using the TREC ad-hoc
retrieval evaluation script10 focusing on MAP, Pre-
cision@20 and nDCG@20 (Manning et al., 2008).
We trained each model five times with different
random seeds and report the mean and standard
deviation for each metric on test data; in each run,
the model selected had the highest MAP on the de-
velopment data. We also report results for an ora-
cle, which re-ranks the N documents returned by
BM25 placing all human-annotated relevant doc-
uments at the top. To test for statistical signif-
icance between two systems, we employed two-
tailed stratified shuffling (Smucker et al., 2007;
Dror et al., 2018) using the model with the highest
development MAP over the five runs per method.

4.2 BioASQ Experiments

Our first experiment used the dataset of the
document ranking task of BIOASQ (Tsatsaronis
et al., 2015), years 1–5.11 It contains 2,251 En-
glish biomedical questions, each formulated by a
biomedical expert, who searched (via PubMed12)
for, and annotated relevant documents. Not all rel-
evant documents were necessarily annotated, but
the data includes additional expert relevance judg-

7See, for example, Table 2 of Guo et al. (2016).
8For PACRR/PACRR-DRMM, we used/modified the code

released by Hui et al. (2017, 2018). We use our own im-
plementation of DRMM, which performs roughly the same as
Guo et al. (2016), though the results are not directly compa-
rable due to different random partitions of the data.

9We limit positive examples to be in the top N documents.
10https://trec.nist.gov/trec_eval/ (v9.0)
11http://bioasq.org/.
12https://www.ncbi.nlm.nih.gov/pubmed/

ments made during the official evaluation.13

The document collection consists of approx.
28M ‘articles’ (titles and abstracts only) from the
‘MEDLINE/PubMed Baseline 2018’ collection.14

We discarded the approx. 10M articles that con-
tained only titles, since very few of these were an-
notated as relevant. For the remaining 18M arti-
cles, a document was the concatenation of each
title and abstract. Consult Appendix B for further
statistics of the dataset. Word embeddings were
pre-trained by applying word2vec (Mikolov et al.,
2013) (see Appendix A for hyper-parameters) to
the 28M ‘articles’ of the MEDLINE/PubMed col-
lection. IDF values were computed over the 18M
articles that contained both titles and abstracts.

The 1,751 queries of years 1–4 were used for
training, the first 100 queries of year 5 (batch 1)
for development, and the remaining 400 queries of
year 5 (batches 2–5) as test set. We set N = 100,
since even using only the top 100 documents of
BM25, the oracle scores are high. PubMed arti-
cles published after 2015 for the training set, and
after 2016 for the development and test sets, were
removed from the top N (and replaced by lower
ranked documents up to N ), as these were not
available at the time of the human annotation.

Table 1 reports results on the BIOASQ test set,
averaged over five runs as well as the single best
run (by development MAP) with statistical signif-
icance. The enhanced models of this paper per-
form better than BM25 (even with extra features),
PACRR, and DRMM. There is hardly any differ-
ence between PACRR and DRMM, but our combi-
nation of the two (PACRR-DRMM) surpasses them
both on average, though the difference is statisti-
cally significant (p < 0.05) only when comparing
to PACRR. Models that use context-sensitive term
encodings (ABEL-DRMM, POSIT-DRMM) outper-
form other models, even PACRR-style models that
incorporate context at later stages in the network.
This is true both on average and by statistical sig-
nificance over the best run. The best model on av-
erage is POSIT-DRMM+MV, though it is not signif-
icantly different than POSIT-DRMM.

4.3 TREC Robust 2004 Experiments

Our primary experiments were on the BIOASQ
dataset as it has one of the largest sets of queries

13Our results are, thus, not comparable to those of partici-
pating systems, since experts did not consider our outputs.

14See https://www.nlm.nih.gov/databases/
download/pubmed_medline.html.

https://trec.nist.gov/trec_eval/
http://bioasq.org/
https://www.ncbi.nlm.nih.gov/pubmed/
https://www.nlm.nih.gov/databases/download/pubmed_medline.html
https://www.nlm.nih.gov/databases/download/pubmed_medline.html


1856

AVERAGE OVER FIVE RUNS WITH STD. DEV.
System MAP P@20 nDCG@20

Traditional IR Baselines
BM25 46.1± .0 25.5± .0 55.4± .0
BM25+extra 48.7± .0 26.6± .0 58.1± .0

Deep Learning Baselines
PACRR 49.1± .2 27.1± .1 58.5± .2
DRMM 49.3± .2 27.2± .2 58.5± .3

Deep Learning with Enhanced Interactions
PACRR-DRMM 49.9± .1 27.4± .1 59.3± .1
ABEL-DRMM 50.3± .2 27.5± .1 59.6± .2

+MV 50.4± .2 27.4± .2 59.7± .3
POSIT-DRMM 50.7± .2 27.8± .1 60.1± .2

+MV 51.0± .1 27.9± .1 60.3± .2
Oracle 72.8± .0 37.5± .0 80.7± .0

BEST RUN WITH STAT. SIG.
System MAP P@20 nDCG@20

Traditional IR Baselines
BM25 46.1 25.5 55.4
BM25+extra 48.7 (1) 26.6 (1) 58.1 (1)

Deep Learning Baselines
PACRR 49.1 (1) 27.0 (1-2) 58.6 (1)
DRMM 49.3 (1-2) 27.1 (1-2) 58.8 (1-2)

Deep Learning with Enhanced Interactions
PACRR-DRMM 50.0 (1-3) 27.3 (1-2) 59.4 (1-3)
ABEL-DRMM 50.2 (1-4) 27.5 (1-4) 59.4 (1-3)

+MV 50.5 (1-4) 27.6 (1-4) 59.8 (1-4)
POSIT-DRMM 50.7 (1-4,6) 27.9 (1-7) 60.1 (1-4,6)

+MV 51.0 (1-7) 27.7 (1-4) 60.3 (1-7)
Oracle 72.8 37.5 80.7

Table 1: Performance on BIOASQ test data. Statistically significant (p < 0.05) difference from BM251;
BM25+extra2; PACRR3; DRMM4; PACRR-DRMM5; ABEL-DRMM6; ABEL-DRMM+MV7.

AVERAGE OVER FIVE RUNS WITH STD. DEV.
System MAP P@20 nDCG@20

Traditional IR Baselines
BM25 23.8± .0 35.4± .0 42.5± .0
BM25+extra 25.0± .0 36.7± .0 43.2± .0

Deep Learning Baselines
PACRR 25.8± .2 37.2± .4 44.3± .4
DRMM 25.6± .6 37.0± .8 44.4± .6

Deep Learning with Enhanced Interactions
PACRR-DRMM 25.9± .4 37.3± .7 44.4± .7
ABEL-DRMM 26.3± .4 38.0± .6 45.6± .4

+MV 26.5± .4 38.0± .5 45.5± .4
POSIT-DRMM 27.0± .4 38.3± .6 45.7± .5

+MV 27.2± .3 38.6± .6 46.1± .4
Oracle 68.0± .0 82.1± .0 93.1± .0

BEST RUN WITH STAT. SIG.
System MAP P@20 nDCG@20

Traditional IR Baselines
BM25 23.8 35.4 42.5
BM25+extra 25.0 (1) 36.7 (1) 43.2 (1)

Deep Learning Baselines
PACRR 25.8 (1-2) 37.4 (1-2) 44.5 (1-2)
DRMM 25.9 (1-2) 37.2 (1-2) 44.4 (1-2)

Deep Learning with Enhanced Interactions
PACRR-DRMM 25.9 (1-2) 37.6 (1-2,4) 44.5 (1-2)
ABEL-DRMM 26.1 (1-5) 38.0 (1-5) 45.4 (1-5)

+MV 26.4 (1-6) 38.2 (1-5) 45.8 (1-6)
POSIT-DRMM 27.1 (1-7) 38.8 (1-7) 46.2 (1-7)

+MV 27.1 (1-7) 38.9 (1-7) 46.4 (1-7)
Oracle 68.0 82.1 93.1

Table 2: Performance on TREC ROBUST test data. Statistically significant (p < 0.05) difference from BM251;
BM25+extra2; PACRR3; DRMM4; PACRR-DRMM5; ABEL-DRMM6; ABEL-DRMM+MV7.

(with manually constructed relevance judgments)
and document collections, making it a particularly
realistic dataset. However, in order to ground our
models in past work we also ran experiments on
TREC ROBUST 2004 (Voorhees, 2005), which is a
common benchmark. It contains 250 queries15 and
528K documents. As this dataset is quite small,
we used a 5-fold cross-validation. In each fold,
approx. 35 of the queries were used for training,

1
5

for development, and 15 for testing. We applied
word2vec to the 528K documents to obtain pre-
trained embeddings. IDF values were computed
over the same corpus. Here we used N = 1000,
as the oracle scores for N = 100 were low.

Table 2 shows the TREC ROBUST results,
which largely mirror those of BIOASQ. POSIT-
DRMM+MV is still the best model, though again
not significantly different than POSIT-DRMM. Fur-
thermore, ABEL-DRMM and POSIT-DRMM are
clearly better than the deep learning baselines,16

15We used the ‘title’ fields of the queries.
16The results we report for our implementation of DRMM

are slightly different than those of Guo et al. (2016). There

but unlike BIOASQ, there is no statistically signif-
icant difference between PACRR-DRMM and the
two deep learning baselines. Even though the
scores are quite close (particularly MAP) both
ABEL-DRMM and POSIT-DRMM are statistically
different from PACRR-DRMM, which was not the
case for BIOASQ. ABEL-DRMM+MV is signifi-
cantly different than ABEL-DRMM on the best run
for MAP and nDCG@20, unlike BIOASQ where
there was no statistically significant difference be-
tween the two methods. However, on average over
5 runs, the systems show little difference.

5 Discussion

An interesting question is how well the deep mod-
els do without the extra features. For BIOASQ,
the best model’s (POSIT-DRMM+MV) MAP score
drops from 48.1 to 46.2 on the development set,
which is higher than the BM25 baseline (43.7),
but on-par with BM25+EXTRA (46.0). We should

are a number of reasons for why this might be the case: there
is no standard split of the data; non-standard preprocessing
of the documents; the original DRMM paper reranks the top
documents returned by Query Likelihood and not BM25.



1857

does
vitamin

d
induce

autophagy

does
vitamin

d
induce

autophagy

vit
am

in d

ind
uce

s

au
top

ha
gy of

pa
ncr

ea
tic
-ce

llsan
d

en
ha

nce
s

ins
ulin

sec
ret

ion

ep
ide

mi
olo

gic
al

ev
ide

nce

ind
ica

testha
t

vit
am

in d is

inv
olv

ed in

de
fen

se

ag
ain

st

dia
be

tes

ho
we

ve
r
the

pre
cis

e

un
de

rly
ing

me
cha

nis
m

rem
ain

s to be

elu
cid

ate
d in the

pre
sen

t
stu

dy the
eff

ect o
f

vit
am

in d on the

pa
tho

ge
ne

sis of

dia
be

teswa
s

inv
est

iga
tedwit

h an

does
vitamin

d
induce

autophagy

contex
t-sensi

tive-ma
x

contex
t-sensi

tive-av
g-k-ma

x

contex
t-insen

sitive-m
ax

contex
t-insen

sitive-a
vg-k-m

ax

exact-m
atch-m

ax

exact-m
atch-av

g-k-ma
x

does

vitamin

d

induce

autophagy

Figure 5: Left: Cosine similarities (POSIT-DRMM attention) of query and document terms, with context-sensitive,
context-insensitive, and exact match views of the terms (top to bottom). Document truncated to 50 words. White
is stronger. Right: Corresponding POSIT-DRMM+MV 6-dimensional document-aware query term encodings.

contex
t-sensi

tive-ma
x

contex
t-sensi

tive-av
g-k-ma

x

contex
t-insen

sitive-m
ax

contex
t-insen

sitive-a
vg-k-m

ax

exact-m
atch-m

ax

exact-m
atch-av

g-k-ma
x

does

autophagy

effect

apoptisis

defense

Figure 6: POSIT-DRMM+MV 6-dimensional document-
aware q-term encodings for ‘Does autophagy induce
apoptosis defense?’ and the same document as Fig. 5.

note, however, that on this set, the DRMM baseline
without the extra features (which include BM25)
is actually lower than BM25 (MAP 42.5), though it
is obviously adding a useful signal, since DRMM
with the extra features performs better (46.5).

We also tested the contribution of context-
sensitive term encodings (Eq. 1). Without them,
i.e., using directly the pre-trained embeddings,
MAP on BIOASQ development data dropped from
47.6 to 46.3, and from 48.1 to 47.0 for ABEL-
DRMM and POSIT-DRMM, respectively.

Fig. 5 shows the cosine similarities (attention
scores, Eq. 5) between q-terms and d-terms, using
term encodings of the three views (Fig. 4), for a
query “Does Vitamin D induce autophagy?” and
a relevant document from the BIOASQ develop-
ment data. POSIT-DRMM indeed marks this as rel-
evant. In the similarities of the context-insensitive
view (middle left box) we see multiple matches
around ‘vitamin d’ and ‘induce autophagy’. The
former is an exact match (white squares in lower
left box) and the latter a soft match. The context-
sensitive view (upper left box) smooths things out
and one can see a straight diagonal white line
matching ‘vitamin d induce autophagy’. The right
box of Fig. 5 shows the 6 components (Fig. 4) of
the document-aware q-term encodings. Although

some terms are not matched exactly, the con-
text sensitive max and average pooled components
(two left-most columns) are high for all q-terms.
Interestingly, ‘induce’ and ‘induces’ are not an ex-
act match (leading to black cells for ‘induce’ in
the two right-most columns) and the correspond-
ing context-insensitive component of (third cell
from left) is low. However, the two components of
the context-sensitive view (two left-most cells of
‘induce’) are high, esp. the max-pooling compo-
nent (left-most).Finally, ‘vitamin d’ has multiple
matches leading to a high average k-max pooled
value, which indicates that the importance of that
phrase in the document.

Fig. 6 shows the 6 components of the document-
aware q-term encodings for another query and
the same document, which is now irrelevant. In
the max pooling columns of the exact match and
context-insensitive view (columns 3, 5), the val-
ues look quite similar to those of Fig. 5. However,
POSIT-DRMM scores this query-document pair low
for two reasons. First, in the average-k-max pool-
ing columns (columns 2, 4, 6) we get lower values
than Fig. 5, indicating that there is less support for
this pair in terms of density. Second, the context
sensitive values (columns 1, 2) are much worse,
indicating that even though many exact matches
exist, in context, the meaning is not the same.

We conclude by noting there is still quite a large
gap between the current best models and the oracle
re-ranking scores. Thus, there is head room for
improvements through more data or better models.

Acknowledgements
We thank the reviewers for their constructive feed-
back that greatly improved this work. Oscar
Täckström gave thorough input on an early draft
of this work. Finally, AUEB’s NLP group provided
many suggestions over the course of the work.



1858

References
Leif Azzopardi, Wim Vanderbauwhede, and Hideo

Joho. 2010. Search system requirements of patent
analysts. In Proceedings of the 33rd International
ACM SIGIR Conference on Research and Devel-
opment in Information Retrieval, pages 775–776,
Geneva, Switzerland.

Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben-
gio. 2014. Neural machine translation by jointly
learning to align and translate. arXiv preprint
arXiv:1409.0473.

Shaojie Bai, J Zico Kolter, and Vladlen Koltun.
2018. An empirical evaluation of generic convolu-
tional and recurrent networks for sequence model-
ing. arXiv preprint arXiv:1803.01271.

Stefan Büttcher, Charles LA Clarke, and Brad Lush-
man. 2006. Term proximity scoring for ad-hoc re-
trieval on very large text collections. In Proceedings
of the 29th annual international ACM SIGIR confer-
ence on Research and development in information
retrieval, pages 621–622. ACM.

Paul Alexandru Chirita, Rita Gavriloaie, Stefania
Ghita, Wolfgang Nejdl, and Raluca Paiu. 2005. Ac-
tivity based metadata for semantic desktop search.
In European Semantic Web Conference, pages 439–
454, Heraklion, Greece.

Rotem Dror, Gili Baumer, Segev Shlomov, and Roi Re-
ichart. 2018. The hitchhiker’s guide to testing statis-
tical significance in natural language processing. In
Proceedings of the 56th Annual Meeting of the As-
sociation for Computational Linguistics (Volume 1:
Long Papers), pages 1383–1392, Melbourne, Aus-
tralia.

Felix A. Gers, Jürgen A. Schmidhuber, and Fred A.
Cummins. 2000. Learning to forget: Contin-
ual prediction with LSTM. Neural Computation,
12(10):2451–2471.

Jiafeng Guo, Yixing Fan, Qingyao Ai, and W Bruce
Croft. 2016. A deep relevance matching model
for ad-hoc retrieval. In Proceedings of the 25th
ACM International on Conference on Information
and Knowledge Management, pages 55–64, Indi-
anapolis, IN.

Donna K Harman. 2005. The TREC ad hoc experi-
ments. Technical report, National Institute of Stan-
dards and Technology (NIST).

David Hawking. 2004. Challenges in enterprise search.
In Proceedings of the 15th Australasian Database
Conference-Volume 27, pages 15–24, Dunedin, New
Zealand.

Baotian Hu, Zhengdong Lu, Hang Li, and Qingcai
Chen. 2014. Convolutional neural network archi-
tectures for matching natural language sentences. In
Advances in Neural Information Processing Systems
27, pages 2042–2050.

Kai Hui, Andrew Yates, Klaus Berberich, and Gerard
de Melo. 2017. PACRR: A position-aware neural IR
model for relevance matching. In Proceedings of the
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 1049–1058, Copenhagen,
Denmark.

Kai Hui, Andrew Yates, Klaus Berberich, and Gerard
de Melo. 2018. Co-PACRR: A context-aware neural
IR model for ad-hoc retrieval. In Proceedings of the
11th ACM International Conference on Web Search
and Data Mining, pages 279–287, Marina Del Rey,
CA.

Thorsten Joachims. 2002. Optimizing search engines
using clickthrough data. In Proceedings of the 8th
ACM SIGKDD International Conference on Knowl-
edge Discovery and Data Mining, pages 133–142,
Edmonton, Canada.

Diederik P Kingma and Jimmy Ba. 2014. Adam: A
method for stochastic optimization. arXiv preprint
arXiv:1412.6980.

Jon M. Kleinberg. 1999. Authoritative sources in
a hyperlinked environment. Journal of the ACM
(JACM), 46(5):604–632.

Robert Krovetz. 1993. Viewing morphology as an in-
ference process. In Proceedings of the 16th Annual
International ACM SIGIR Conference on Research
and Development in Information Retrieval, pages
191–202, Pittsburgh, PA.

Guillaume Lample, Miguel Ballesteros, Sandeep Sub-
ramanian, Kazuya Kawakami, and Chris Dyer. 2016.
Neural architectures for named entity recognition.
In Proceedings of the 2016 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics: Human Language Technologies,
pages 260–270, San Diego, CA.

Zhengdong Lu and Hang Li. 2013. A deep architec-
ture for matching short texts. In Proceedings of the
26th International Conference on Neural Informa-
tion Processing Systems - Volume 1, pages 1367–
1375, Lake Tahoe, NV.

Christopher D. Manning, Prabhakar Raghavan, and
Hinrich Schütze. 2008. Introduction to Information
Retrieval. Cambridge University Press.

Donald Metzler and W Bruce Croft. 2005. A markov
random field model for term dependencies. In Pro-
ceedings of the 28th annual international ACM SI-
GIR conference on Research and development in in-
formation retrieval, pages 472–479. ACM.

Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Cor-
rado, and Jeff Dean. 2013. Distributed representa-
tions of words and phrases and their compositional-
ity. In Proceedings of the 26th International Con-
ference on Neural Information Processing Systems -
Volume 2, pages 3111–3119, Lake Tahoe, Nevada.



1859

Sunil Mohan, Nicolas Fiorini, Sun Kim, and Zhiyong
Lu. 2017. Deep learning for biomedical information
retrieval: Learning textual relevance from click logs.
In BioNLP 2017, pages 222–231.

Lawrence Page, Sergey Brin, Rajeev Motwani, and
Terry Winograd. 1999. The pagerank citation rank-
ing: Bringing order to the web. Technical report,
Stanford InfoLab.

Hamid Palangi, Li Deng, Yelong Shen, Jianfeng Gao,
Xiaodong He, Jianshu Chen, Xinying Song, and
Rabab Ward. 2016. Deep sentence embedding using
long short-term memory networks: Analysis and ap-
plication to information retrieval. IEEE/ACM Trans-
actions on Audio, Speech and Language Processing
(TASLP), 24(4):694–707.

Liang Pang, Yanyan Lan, Jiafeng Guo, Jun Xu, Jing-
fang Xu, and Xueqi Cheng. 2017. Deeprank: A
new deep architecture for relevance ranking in infor-
mation retrieval. In Proceedings of the 2017 ACM
on Conference on Information and Knowledge Man-
agement, pages 257–266. ACM.

Matthew Peters, Mark Neumann, Mohit Iyyer, Matt
Gardner, Christopher Clark, Kenton Lee, and Luke
Zettlemoyer. 2018. Deep contextualized word repre-
sentations. In Proceedings of the 2018 Conference
of the North American Chapter of the Association
for Computational Linguistics: Human Language
Technologies, Volume 1 (Long Papers), volume 1,
pages 2227–2237.

Barbara Plank, Anders Søgaard, and Yoav Goldberg.
2016. Multilingual part-of-speech tagging with
bidirectional long short-term memory models and
auxiliary loss. In Proceedings of the 54th Annual
Meeting of the Association for Computational Lin-
guistics (Volume 2: Short Papers), pages 412–418,
Berlin, Germany.

Stephen Robertson and Hugo Zaragoza. 2009. The
probabilistic relevance framework: BM25 and be-
yond. Foundations and Trends in Information Re-
trieval, 3(4):333–389.

Stephen E. Robertson and Karen Sparck Jones. 1988.
Document retrieval systems. chapter Relevance
Weighting of Search Terms, pages 143–160. Taylor
Graham Publishing, London, UK.

Aliaksei Severyn and Alessandro Moschitti. 2015.
Learning to rank short text pairs with convolutional
deep neural networks. In Proceedings of the 38th
International ACM SIGIR Conference on Research
and Development in Information Retrieval, pages
373–382, Santiago, Chile.

Mark D Smucker, James Allan, and Ben Carterette.
2007. A comparison of statistical significance tests
for information retrieval evaluation. In Proceed-
ings of the sixteenth ACM conference on Conference
on information and knowledge management, pages
623–632. ACM.

Karen Sparck Jones. 1988. Document retrieval sys-
tems. chapter A Statistical Interpretation of Term
Specificity and Its Application in Retrieval, pages
132–142. Taylor Graham Publishing, London, UK.

Mihai Surdeanu, Massimiliano Ciaramita, and Hugo
Zaragoza. 2008. Learning to rank answers on large
online QA collections. In Proceedings of the 46th
Annual Meeting for the Association for Computa-
tional Linguistics: Human Language Technologies
(ACL-08: HLT), pages 719–727, Columbus, OH.

Ming Tan, Cicero dos Santos, Bing Xiang, and Bowen
Zhou. 2015. LSTM-based deep learning models
for non-factoid answer selection. arXiv preprint
arXiv:1511.04108.

George Tsatsaronis, Georgios Balikas, Prodromos
Malakasiotis, Ioannis Partalas, Matthias Zschunke,
Michael R. Alvers, Dirk Weissenborn, Anastasia
Krithara, Sergios Petridis, Dimitris Polychronopou-
los, Yannis Almirantis, John Pavlopoulos, Nico-
las Baskiotis, Patrick Gallinari, Thierry Artiéres,
Axel-Cyrille Ngonga Ngomo, Norman Heino, Eric
Gaussier, Liliana Barrio-Alvers, Michael Schroeder,
Ion Androutsopoulos, and Georgios Paliouras. 2015.
An overview of the BioASQ large-scale biomedical
semantic indexing and question answering competi-
tion. BMC Bioinformatics, 16(1):138.

Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob
Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz
Kaiser, and Illia Polosukhin. 2017. Attention is all
you need. In Advances in Neural Information Pro-
cessing Systems 30, pages 5998–6008, Long Beach,
CA.

Oriol Vinyals, Meire Fortunato, and Navdeep Jaitly.
2015. Pointer networks. In Advances in Neural In-
formation Processing Systems 28, pages 2692–2700,
Montreal, Canada.

Ellen M Voorhees. 2005. The TREC robust retrieval
track. ACM SIGIR Forum, 39(1):11–20.

Xing Wei and W Bruce Croft. 2006. LDA-based docu-
ment models for ad-hoc retrieval. In Proceedings of
the 29th Annual International ACM SIGIR Confer-
ence on Research and Development in Information
Retrieval, pages 178–185, Seattle, WA.

Jian Wu, Kyle Mark Williams, Hung-Hsuan Chen, Ma-
dian Khabsa, Cornelia Caragea, Suppawong Tuarob,
Alexander G Ororbia, Douglas Jordan, Prasenjit Mi-
tra, and C Lee Giles. 2015. Citeseerx: AI in a digital
library search engine. AI Magazine, 36(3):35–48.

Chenyan Xiong, Zhuyun Dai, Jamie Callan, Zhiyuan
Liu, and Russell Power. 2017. End-to-end neural
ad-hoc ranking with kernel pooling. In Proceed-
ings of the 40th International ACM SIGIR Confer-
ence on Research and Development in Information
Retrieval, pages 55–64. ACM.



1860

Chengxiang Zhai and John Lafferty. 2001. A study of
smoothing methods for language models applied to
ad hoc information retrieval. In Proceedings of the
24th Annual International ACM SIGIR Conference
on Research and Development in Information Re-
trieval, pages 334–342, New Orleans, LA.


