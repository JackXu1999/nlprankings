



















































Random Decision Syntax Trees at SemEval-2018 Task 3: LSTMs and Sentiment Scores for Irony Detection


Proceedings of the 12th International Workshop on Semantic Evaluation (SemEval-2018), pages 560–564
New Orleans, Louisiana, June 5–6, 2018. ©2018 Association for Computational Linguistics

Random Decision Syntax Trees at SemEval-2018 Task 3: LSTMs and
Sentiment Scores for Irony Detection

Aidan San
University of Illinois at Urbana-Champaign

asan2@illinois.edu

Abstract

We propose a Long Short Term Memory Neu-
ral Network model for irony detection in
tweets in this paper. Our model is trained us-
ing word embeddings and emoji embeddings.
We show that adding sentiment scores to our
model improves the F1 score of our baseline
LSTM by approximately .012, and therefore
show that high-level features can be used to
improve word embeddings in certain Natural
Language Processing applications. Our model
ranks 24/43 for binary classification and 5/31
for multiclass classification. We make our
model easily accessible to the research com-
munity1.

1 Introduction

Recently, irony detection has become an increas-
ingly important problem with new applications ap-
pearing every day. In discourse analysis, it is ex-
tremely important to understand if a politician is
making an ironic or a literal response, or under-
standing can be completely lost. In chatbots, if
a chatbot misinterprets an unhappy sarcastic com-
ment from a customer, the customer could become
even more frustrated. In sentiment analysis, if
irony is not taken into account, the actual senti-
ment could be the opposite of the prediction.

The aim of irony detection is generally a binary
classfication problem: Is the piece of text ironic or
literal? In SemEval2018 Task 3 (Van Hee et al.,
2018), there are two subtasks. First, subtask A, is
our standard binary classification problem. We are
provided with a corpus of tweets annotated with
0 or 1’s to specify if a tweet is ironic or literal.
Second, in subtask B, we are tasked with a more
challenging problem. In subtask B, participants
must determine which type of irony a particular
tweet contains. Is it verbal irony based on polarity,

1github.com/Muakasan/RDST-semeval2018-task3

another type of verbal irony, situational irony or
not ironic at all?

Recently, research has shown that neural ap-
proaches are particularly effective in sarcasm de-
tection (Ghosh and Veale, 2016) and similar prob-
lems such as sentiment analysis (Rosenthal et al.,
2017). In this paper, we present neural network
model designed to tackle the challenge of irony
detection.

In subsection 2.1, we give a brief overview of
the entire model. In subsection 2.2 and 2.3, we
describe our neural architecture and approach. In
subsection 2.5 and 2.6, we describe the embed-
dings and sentiment scores that are used as the in-
put to our neural network. In section 3, we de-
scribe our results, and give quantitative evidence
of the effectiveness of our features. In section 4
we conclude, and in section 5, we describe ways
that our approach could be improved.

2 System Description

2.1 Overview

Our system is composed of three stages: Prepro-
cessing, Feature Creation, and the Neural Net-
work. Preprocessing is composed of tokenizing
the input text. Feature creation is generating the
word and emoji embeddings and sentiment scores
and concatenating them together to form a feature
vector. Finally in the third stage, the feature vec-
tor is fed into the neural network which makes a
prediction. Our full model is illustrated in Figure
1.

2.2 Long Short Term Memory

In traditional Feed Forward Neural Networks,
there is not a good way to represent temporal in-
puts. Recurrent Neural Network (RNN) cells are
like traditional neural network cells, except they
have the key difference of feeding their output

560



Figure 1: Overview of the entire model.

Figure 2: The various gates of the LSTM Cell and
the connections between them. The data path is shown
from the input wc to the output yc. Taken from Gers
et al. (1999).

back into themselves. Each time a new word rep-
resentation is passed into the RNN, the RNN will
take its output and pass it back into the RNN, so
the RNN receives 2 inputs (the current word and
the output from the previous iteration). The output
of the previous iteration can be thought of as a rep-
resentation of the previous words in the sentence.
This is repeated until the sentence is finished. Ad-
ditionally, context units can be added to the cell to
imitate the idea of memory (Elman, 1990). This
improves language modeling, because when read-
ing a sentence, prior context of previous words is
very important for the understanding of the cur-
rent word. When we use an RNN, our model can
be fed in with context from previous words.

The Long Short Term Memory Model (LSTM)
improves over the naive RNN, with the addition of
a memory cell and input and output gates (Hochre-
iter and Schmidhuber, 1997). In the naive RNN, it
is easy to lose important information over many
iterations of the cell. In an LSTM cell, the mem-
ory cell combats this problem. The original LSTM
cell is composed of two gates. The output gate
can protect other units from irrelevant information
from the current iteration, and the input gate can
control which information is passed into the cur-
rent iteration. Additionally, Gers et al. (1999) in-
clude a forget gate which can reset memory once
it is no longer relevant. Figure 2 taken from Gers
et al. (1999) illustrates the architecture of the cell.

2.3 Neural Network Architecture

Our model is built with the high-level Python neu-
ral network library called Keras (Chollet et al.,
2015). We use a 5 layer model. Our first layer
is the input layer, where we feed integer ranks
(a number representing how often a word appears
in the corpus where “1” is the most common) of

561



Figure 3: An unrolled Neural Network. X1 to X30 rep-
resent the first 30 words in the input tweet. Y repre-
sents the irony prediction of the given tweet.

words into the layer. Then we pass the inputs into
an embedding layer. This layer takes our inputs
and converts them into a word or emoji embedding
representation. Our third and fourth layers are
both Long Short Term Memory layers. We take
the outputs of the first LSTM layer and feed them
into our second layer. Both LSTM layers are given
a dropout of .25 (chosen by experimentation) to
prevent overfitting (Srivastava et al., 2014). The
output of the second LSTM is then passed into a
single Dense layer. Our Dense layer has a sigmoid
and softmax activation for subtask A and B respec-
tively. We use binary cross entropy and categori-
cal cross entropy for subtask A and B respectively.
We train our model over 30 epochs (approximately
when the model converged) with a batch size of
32. Our architecture is shown in Figure 3.

2.4 Preprocessing

We preprocess using Pandas (McKinney, 2010)
for csv reading along with basic data processing.
We then run our tweets through the Keras Tok-
enizer (Chollet et al., 2015). The Keras Tokenizer
handles splitting, removing punctuation and low-
ercasing words.

2.5 Word/Emoji Embeddings

When dealing with a relatively small training set,
such as a small number of tweets, pretrained word
embeddings can be a very effective way of encod-

ing additional information into the model with-
out needing additional training data. We used
the Google News pretrained Word2Vec (Mikolov
et al., 2013) word embeddings for our model. The
word embeddings were trained using 100 billion
words taken from the Google News dataset. We
additionally added Emoji2Vec (Eisner et al., 2016)
embeddings. As is shown by Hogenboom et al.
(2013), emoticons encode a large amount of sen-
timent information. As the descendants of emoti-
cons, we hypothesize emojis can encode sentiment
information as well.

2.6 Sentiment Scores

We use NLTK (Bird and Loper, 2004) to get sen-
timent scores using the SentiWordNet (Esuli and
Sebastiani, 2007) corpus. SentiWordNet is a li-
brary which uses a semi-supervised approach to
give synsets (groupings of words which have the
same meaning) an objective, positive, and nega-
tive sentiment score. Riloff et al. (2013) show that
sentiment can be an effective way to improve sar-
casm detection in tweets. For simplicity, we al-
ways use the first synset of a particular word. We
take the positive and negative scores from Senti-
WordNet and then concatenate the results with our
word/emoji embedding.

2.7 Memory Constraints

Standard practice is to pass the word embedding
matrix with every word as a layer into the Neu-
ral Network. In this task, we had the additional
constraint of a relatively small amount of RAM
(about 8GB). To resolve this problem, instead of
building our embedding matrix and then tokeniz-
ing our dataset, we tokenized first. Then, rather
than building our embedding matrix with every
possible entry, we could simply look through the
words that we had seen during tokenization, and
only put those vectors into our matrix. To put this
into context, while we were storing all the em-
beddings in memory we were using about 6GB
of RAM. After we created our much smaller em-
bedding matrix, we were only using about 3GB of
RAM.

562



3 Results

Task F1 Accuracy
A (Binary) .5822(24) .6173(21)

B (Multiclass) .4352(6) .6327(6)

Table 1: Official results from the competition on the
test set. The numbers between parentheses indicate
ranking compared to other models.
As shown in Table 1, in the binary classification
task, we achieved an F1 score of .5822 and an ac-
curacy of .6173, and in the multiclass classifica-
tion task, we achieved an F1 score of .4352 and an
accuracy of .6327. This earned us a rank of 24 and
6 in tasks A, and B respectively, because teams
were ranked using F1 score.

Model F1 Accuracy
Without Emoji .6127 .6121

Without Sentiment .6093 .6097
Emoji & Sentiment .6210 .6091

Table 2: Accuracy and F1 scores of various models
tested on a 75%-25% split of the training data.

To test the effectiveness of the emoji embeddings
feature and sentiment scores feature, we tested our
binary classification model without each feature
separately and compared these models to our com-
bined model. We ran our tests over the training set
with a 75%-25% split, where 75% of the training
data was used to train the model and 25% of the
model was designated as validation data, to verify
the effectiveness of the model on unseen data. We
ran 10 trials and took the average accuracy and F1
score over the 10 trials for each of the 3 models.

The task was ranked using F1 score, so we op-
timized for F1 score. As can be seen in Table 2,
the combined model achieves an F1 score .0083
higher than not including emoji embeddings and
an F1 score .0117 higher than not including senti-
ment scores. Interestingly, we see that the com-
bined model actually achieves a lower accuracy
score than both of the individual models. We
can draw the conclusion that the combined model
has more balanced predictions between the two
classes, which overall creates a higher F1 score at
the cost of lower accuracy in one of the classes.

4 Conclusion

In this paper, we have described our system for
SemEval-2018 Task 3. We discussed LSTM-based
neural models, and how we incorporate sentiment
features and emoji embeddings. We show that
additional high-level features such as sentiment
scores can improve neural based models. We
achieve rank 24/43 in subtask A and 6/31 in sub-
task B.

5 Future Work

Possible improvements to the model fall under two
primary categories: the neural architecture and ad-
ditional high-level features.

To implement our neural architecture, we exper-
imented with other types of layers such as Bidi-
rectional LSTMs and CNNs, but we would like to
further explore these approaches in the future. We
especially think Attention Layers used by models
such as Baziotis et al. (2017) could be particu-
larly effective in irony detection, because of their
effectiveness in sentiment analysis. We could also
try different activation functions in different layers
and test out different batch sizes. Another possi-
ble direction would be to use ensemble methods
which have been shown to be particularly effec-
tive by Cliche (2017) for similar tasks.

Regarding additional high-level features, we
could also include sentence-level features. Goel et
al. (2017) showed that high-level features could be
included in a Feed Forward Neural Network which
improved accuracy in an ensemble method for the
task of emotion detection. Additionally, we could
continue in the vein of the model we created and
add other word-level features such as capitaliza-
tion. To improve our sentiment features, we could
also use a more advanced method of choosing the
correct synset for a better fitting sentiment score.

Acknowledgments

We would like to thank the task organizers for
providing the dataset and putting together the
competition. We would like to thank Assma
Boughoula and Julia Hockenmaier for their help
in this project.

References
Christos Baziotis, Nikos Pelekis, and Christos Doulk-

eridis. 2017. Datastories at semeval-2017 task

563



4: Deep lstm with attention for message-level and
topic-based sentiment analysis. In Proceedings of
the 11th International Workshop on Semantic Eval-
uation (SemEval-2017), pages 747–754.

Steven Bird and Edward Loper. 2004. Nltk: the nat-
ural language toolkit. In Proceedings of the ACL
2004 on Interactive poster and demonstration ses-
sions, page 31. Association for Computational Lin-
guistics.

François Chollet et al. 2015. Keras. https://
github.com/fchollet/keras.

Mathieu Cliche. 2017. Bb twtr at semeval-2017 task 4:
Twitter sentiment analysis with cnns and lstms. In
Proceedings of the 11th International Workshop on
Semantic Evaluation (SemEval-2017), pages 573–
580.

Ben Eisner, Tim Rocktäschel, Isabelle Augenstein,
Matko Bošnjak, and Sebastian Riedel. 2016.
emoji2vec: Learning emoji representations from
their description. In Conference on Empirical Meth-
ods in Natural Language Processing, page 48.

Jeffrey L Elman. 1990. Finding structure in time. Cog-
nitive science, 14(2):179–211.

Andrea Esuli and Fabrizio Sebastiani. 2007. Senti-
wordnet: a high-coverage lexical resource for opin-
ion mining. Evaluation, pages 1–26.

Felix A Gers, Jürgen Schmidhuber, and Fred Cummins.
1999. Learning to forget: Continual prediction with
lstm.

Aniruddha Ghosh and Tony Veale. 2016. Fracking
sarcasm using neural network. In Proceedings of
the 7th Workshop on Computational Approaches to
Subjectivity, Sentiment and Social Media Analysis,
pages 161–169.

Pranav Goel, Devang Kulshreshtha, Prayas Jain, and
Kaushal Kumar Shukla. 2017. Prayas at emoint
2017: An ensemble of deep neural architectures
for emotion intensity prediction in tweets. In Pro-
ceedings of the 8th Workshop on Computational Ap-
proaches to Subjectivity, Sentiment and Social Me-
dia Analysis, pages 58–65.

Sepp Hochreiter and Jürgen Schmidhuber. 1997. Lstm
can solve hard long time lag problems. In Ad-
vances in neural information processing systems,
pages 473–479.

Alexander Hogenboom, Daniella Bal, Flavius Fras-
incar, Malissa Bal, Franciska de Jong, and Uzay
Kaymak. 2013. Exploiting emoticons in sentiment
analysis. In Proceedings of the 28th Annual ACM
Symposium on Applied Computing, pages 703–710.
ACM.

Wes McKinney. 2010. Data structures for statistical
computing in python. In Proceedings of the 9th
Python in Science Conference, pages 51 – 56.

Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Cor-
rado, and Jeff Dean. 2013. Distributed representa-
tions of words and phrases and their compositional-
ity. In Advances in neural information processing
systems, pages 3111–3119.

Ellen Riloff, Ashequl Qadir, Prafulla Surve, Lalin-
dra De Silva, Nathan Gilbert, and Ruihong Huang.
2013. Sarcasm as contrast between a positive senti-
ment and negative situation. In Proceedings of the
2013 Conference on Empirical Methods in Natural
Language Processing, pages 704–714.

Sara Rosenthal, Noura Farra, and Preslav Nakov.
2017. Semeval-2017 task 4: Sentiment analysis in
twitter. In Proceedings of the 11th International
Workshop on Semantic Evaluation (SemEval-2017),
pages 502–518.

Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky,
Ilya Sutskever, and Ruslan Salakhutdinov. 2014.
Dropout: A simple way to prevent neural networks
from overfitting. The Journal of Machine Learning
Research, 15(1):1929–1958.

Cynthia Van Hee, Els Lefever, and Véronique Hoste.
2018. SemEval-2018 Task 3: Irony Detection in
English Tweets. In Proceedings of the 12th Interna-
tional Workshop on Semantic Evaluation, SemEval-
2018, New Orleans, LA, USA. Association for
Computational Linguistics.

564


