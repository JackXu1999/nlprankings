



















































Audience size and contextual effects on information density in Twitter conversations


Proceedings of CMCL 2015, pages 19–28,
Denver, Colorado, June 4, 2015. c©2015 Association for Computational Linguistics

Audience size and contextual effects on information density in Twitter
conversations

Gabriel Doyle
Dept. of Psychology
Stanford University

Stanford, CA, USA, 94305
gdoyle@stanford.edu

Michael C. Frank
Dept. of Psychology
Stanford University

Stanford, CA, USA, 94305
mcfrank@stanford.edu

Abstract

The “uniform information density” (UID) hy-
pothesis proposes that language producers aim
for a constant rate of information flow within
a message, and research on monologue-like
written texts has found evidence for UID in
production. We consider conversational mes-
sages, using a large corpus of tweets, and
look for UID behavior. We do not find ev-
idence of UID behavior, and even find con-
text effects that are opposite that of previous,
monologue-based research. We propose that
a more collaborative conception of informa-
tion density and careful consideration of chan-
nel noise may be needed in the information-
theoretic framework for conversation.

1 Introduction

Linguistic communication can be viewed from an
information theoretic standpoint as communication
via a noisy channel. If humans are approximately
rational in their communications and the noisy chan-
nel model is appropriate, then we expect to see com-
munication follow an approximately constant rate of
information flow. This is the Uniform Information
Density (UID) hypothesis.

Evidence in favor of UID has been found in
many levels of language production. At the level
of within-sentence context, there is clear evidence
from phonology that speakers reduce more pre-
dictable sounds (Aylett and Turk, 2004; Aylett and
Turk, 2006; Bell et al., 2003; Demberg et al.,
2012), suggesting that they are giving more “air
time” to less predictable material to equalize infor-
mation density. And in syntax, speakers tend to

drop optional materials (like the word “that” as a
sentence-complementizer) in more predictable sce-
narios (Levy and Jaeger, 2007; Frank and Jaeger,
2008; Jaeger, 2010), again implying a process of al-
locating communication time relative to predictabil-
ity. These effects appear in both monologues and
dialogues, suggesting that local linguistic context
shapes message complexity.

There is also some evidence for UID based on
broader, discourse-level context. Genzel and Char-
niak (2002) showed that word-by-word complexity
(measured by a standard n-gram language model)
increases across sequences of sentences. They hy-
pothesized that this increase was due to a corre-
sponding increase in non-linguistic information that
would make even more complex linguistic structures
easier to predict. Follow-ups have shown that this
same complexity increase effect is attested in dif-
ferent document types and across languages (Genzel
and Charniak, 2003; Qian and Jaeger, 2012). How-
ever, these studies draw almost exclusively from
long, well-structured written texts that function as
monologues from writer to reader.

This leaves an important gap in these tests of
the UID hypothesis: little work has looked at the
influence of discourse-level context on information
structure in interpersonal dialogue, the archetype of
human linguistic communication. With the excep-
tion of one preliminary study that provided a partial
replication of the original complexity increase ef-
fect using the Switchboard corpus (Vega and Ward,
2009), to our knowledge no work has explored how
the broader dynamics of conversation interact with
UID.

19



The present study applies information-theoretic
analysis to a corpus of social media microblog posts
that include a large number of natural dialogues.
Surprisingly, we do not see clear evidence of the
UID hypothesis in these dialogues. Instead, we pro-
pose that differences in the discourse-level struc-
ture of conversation compared to monologues, such
as the desire to establish that mutual understand-
ing has been reached, may interfere with attaining
UID in the standard formulation. A more collabora-
tive view of UID, encompassing content generation
and grounding (Clark and Schaefer, 1987), may be
needed to fully represent conversational structure.

1.1 Conversations, context, and content
One common motivation for the UID hypothesis is
a rational analysis based on a noisy-channel model
of communication (Levy and Jaeger, 2007).1 In the
noisy-channel analysis, the amount of noise in the
channel sets an optimal value for information den-
sity to obtain fast, error-free transmission. For a
noise level α, we will refer to the optimal informa-
tion content per discourse unit Yi as Hα(Yi). Dis-
course units, depending on the analysis, range from
syllables to whole documents; in our analyses, we
focus on words and tweets as our discourse units.

In the course of a message, as argued by Genzel
and Charniak (2002), the actual information content
per discourse unit is predicted by the entropy of the
random variable Xi representing the precise word
choice or choices within the discourse unit, condi-
tioned on the available context. The precise extent
of this context is difficult to pin down.

We estimate context for our studies by thinking in
terms of the common ground that a rational speaker
believes to exist, given the expected audience of
their message. Common ground is defined as the
knowledge that participants in a discourse have and
that participants know other participants have, in-
cluding the current conversational context (Clark,
1996). This common ground can be built from a
combination of linguistic and non-linguistic context,
including previous messages within the discourse,
preceding interactions between the conversation par-
ticipants, and world knowledge.

1The other common motivation is a surprisal-based argu-
ment (Levy, 2008): maintaining UID also minimizes the lis-
tener’s comprehension effort.

To formalize this relationship, let Ci be the com-
mon ground that exists prior to the production of
discourse unit Yi, and let α be the expected noise
level in the channel that Yi is transmitted through.
Then optimality within a noisy channel model pre-
dicts that the noise-dependent optimal information
rate Hα(Yi) is related to the actual information rate
as follows:

Hα(Yi|Ci) = H(Xi)− I(Xi;Ci) (1)

Here, H(Xi) is the apparent decontextualized en-
tropy of the discourse unit independent of the com-
mon ground. This quantity is often estimated from
a language model that uses only local context, not
higher-level discourse context or common ground.
We use a trigram Markov model in this study.

I(Xi;Ci) is the mutual information of the dis-
course unit random variable Xi and the common
ground Ci—essentially how much more predictable
the next discourse unit becomes from knowing the
common ground. Common ground is difficult to
quantify—both in the particular datasets we con-
sider and more generally—so we rely on the as-
sumption that more common ground is correlated
with greater mutual information, as in Genzel and
Charniak (2002).

Then, based on this assumption, Eq. 1 allows us to
make two UID-based predictions. First, as channel
noise increases, transmission error should increase,
which in turn should cause the optimal information
transfer rate Hα(Yi) to decrease. Thus, to main-
tain equality with rising noise, the apparent entropy
H(Xi) should decrease. This prediction translates
into communicators “slowing down” their speech
(albeit in terms of information per word, rather than
per unit time) to account for increased errors.

Second, as common ground increases, I(Xi;Ci)
should increase. To maintain equality with rising
common ground, H(Xi) should thus also increase,
so as not to convey information slower than neces-
sary. This prediction translates into communicators
“going faster” (e.g., packing more information into
each word) because of an assumption that listeners
share more common ground with them.

20



1.2 The current study

We take advantage of the conversational structure
of the popular social media microblogging platform
Twitter (http://twitter.com) to test these
predictions. Twitter allows users to post 140 char-
acter “tweets” in a number of different conversa-
tional contexts. In particular, because some tweets
are replies to previous tweets, we can use this re-
ply structure to build conversational trees, and to
track the number of participants. In addition, spe-
cific choices in tweet production can affect what au-
dience is likely to see the tweet. These variables are
discussed in depth in Section 2.2.

To test the entropy effects predicted by Eq. 1, we
first examine different types of tweets that reach dif-
ferent audience sizes. We then restrict our analysis
to reply tweets with varying audience sizes to ana-
lyze audience size independently of noise. Finally,
we look at the effects of common ground (by way
of conversation structure) on tweet entropy. Con-
trary to previous UID findings, we do not see a clear
increase in apparent entropy estimates due to more
extensive common ground, as had been found in pre-
vious non-conversational work (Genzel and Char-
niak, 2002; Qian and Jaeger, 2012; Doyle and Frank,
2015).

We propose two factors that may be influenc-
ing conversational content in addition to UID fac-
tors. First, achieving conversational goals may
be more dependent on certain discourse units that
carry low linguistic informativity but substantial so-
cial/conversational importance. Second, consider-
ing and adapting to two different types of noise—
message loss and message corruption—may cause
tweeters to make large-scale decisions that over-
whelm UID effects.

2 Corpus

Randomly sampling conversations on a medium like
Twitter is a difficult problem. Twitter users routinely
use the medium to converse in smaller groups via the
mention functionality (described in more detail be-
low). Yet such conversations are not uniformly dis-
tributed: A random sample of tweets—perhaps cho-
sen because they contain the word “the” or a simi-
larly common token (Doyle, 2014)—yields mostly
isolated tweets rather than complete dialogues. Di-

Seed Users Category
@camerondallas

Youtube stars
@rickypdillon
@edsheeran

Musicians
@yelyahwilliams
@felixsalmon

Journalists
@tanehisicoates

@jahimes
Politicians@jaredpolis

@leezeldin
@larrymishel

Economists
@paulnvandewater

@neiltyson
Scientists@profbriancox

@richardwiseman

Table 1: Seed users for our dataset.

alogues depend on users interacting back and forth
within communities.

2.1 Seed strategy

To sample such interactions, we developed a “seed”
strategy where we identified popular Twitter ac-
counts and then downloaded a large sample of their
tweets, then downloaded a sample of the tweets of
all the users they mentioned. This strategy allowed
us to reconstruct a relatively dense sample of dia-
logues (reply chains).

We began by choosing a set of 14 seed Twitter
accounts (Table 1) that spanned a variety of genres,
were popular enough to elicit replies, and interacted
with other users often enough to build up a commu-
nity.

To build conversations, we needed to obtain
tweets directed to and from these seed users. For
each seed user, we downloaded their last 1500
tweets, extracted all users mentioned within those
tweets, and downloaded each of their last 1500
tweets. To capture tweets that failed to start con-
versations with the seed users, we also added the
last 1000 tweets mentioning each seed user’s handle.
Tweets that appeared in multiple communities were
removed. Each reply contains the ID of the tweet
it replies to, so we could rebuild conversation trees
back to their roots, so long as all of the preceding
tweets were made by users in our communities.

21



2.2 Conversation structure and visibility

Twitter conversations follow a basic tree structure
with a unique root node. Each tweet is marked as a
reply or not; for replies, the user and tweet IDs of
the tweet it replies to is stored. Each tweet can be
a reply to at most one other tweet, so a long con-
versation resembles a linked list with a unique root
node. “Mentions,” the inclusion of a username in a
tweet, are included in tweets by default throughout
a conversation unless a tweeter chooses to remove
some of them, so tweets deep in a conversation may
be primarily composed of mentions rather than new
information.

After some processing described below, our sam-
pling process resulted in 5.5 million tweets, of which
3.3 million were not part of a conversation (not a
reply, and received no replies). Within this data,
we found 63,673 conversations that could be traced
back to a root tweet, spanning 228,923 total tweets.
Unfortunately, Twitter only tracks replies up a tree,
so while we know with certainty whether a tweet is
a reply (even if it is to a user outside our commu-
nities), we do not know with certainty that a tweet
has received no replies, especially from users out-
side our communities. If anything, this fact makes
our analyses conservative, as they may understate
differences between reply and non-reply tweets. The
remaining 2 million tweets were replies whose con-
versations could not be traced back to the root.

2.3 Information content estimation

To estimate the information content of a tweet,
we first tokenized the tweets using Twokenizer
(Owoputi et al., 2013). We then removed any num-
ber of mentions at the beginning or end of a tweet, as
these are usually used to address certain users rather
than to convey information themselves. (Tweets that
only contained mentions were removed.) Tweet-
medial mentions were retained but masked with the
single type [MENTION] to reduce sparsity. Links
were similarly masked as [URL]. Punctuation and
emoji were retained. We then built trigram lan-
guage models using SRILM with default settings
and Kneser-Ney discounting. Types with fewer than
5 tokens were treated as out-of-vocabulary items.

For each community, the training set was the set
of all tweets from all other communities. This train-

ing set provides tweets that are contemporaneous to
the test set and cover some of the same topics with-
out containing the same users’ tweets.

3 Analyses

We describe the results of three sets of analyses
looking at the influence of audience size and avail-
able context on apparent tweet entropy. The first
examines the effect of expected audience size at a
coarse level, comparing tweets directed at a small
subset of users, all one’s followers, or the wider
realm of a hashtag. The second examines the effect
of finer differences in known audience size on appar-
ent informativity. The third examines the effects of
conversational context and length on informativity.

3.1 Expected audience size

First, we consider three different types of tweets and
their expected audience size. Tweets whose first
character is a mention (whether or not it is a reply)
do not show up by default when browsing a user’s
tweets, unless the browser follows both the tweeter
and first-mentioned user.2 We will refer to these
as “invisible” tweets as they are invisible to follow-
ers by default. A tweeter making an initial-mention
tweet thus should expect such a tweet to have a rel-
atively limited audience, with a focus on the men-
tioned users.3

On the other side, a hashtag serves as a categoriza-
tion mechanism so that interested users can discover
new content. Hashtags are often used to expand the
potential audience for a tweet to include the feeds
of users tracking that hashtag, regardless of whether
they follow the original tweeter, and so a tweeter us-
ing a hashtag should expect a larger audience than

2This behavior varies slightly depending on what applica-
tion is used to view Twitter. On the website, mention-first
tweets do not appear in lists and only appear after clicking the
’tweets & replies’ option on a timeline. On the Twitter mobile
app, mention-first tweets appear by default on a timeline but
still not in lists.

3Some Twitter users consciously manipulate audience using
these markers: many tweets have an initial period or other punc-
tuation mark to prevent it from being hidden. Some users rou-
tinely switch between initial-mention replies and “dot”-replies
in the course of a conversation to change the audience, presum-
ably depending on their estimate of the wider relevance of a
remark.

22



Type Tweet Per-word entropy

invisible

[MENTION] [MENTION] this is so accurate tho 6.00
[MENTION] can you come to my high school ? ;3 7.61
[MENTION] Hi Kerry , Please send us your email address in order to dis-
cuss this matter further . Thanks !

8.58

post your best puns in the comments of my latest instagram photo : [URL] 7.44

baseline
I wish I could start a blog dedicated to overly broad and sweeping introduc-
tory sentences

9.98

this new year’s eve in NYC , keep an eye peeled 4 Sad Michael Stipe .
[URL] already found him : [URL]

7.17

I will probably be quitting my job when #GTAV comes out 7.63

hashtagged
#UMAlumni what is the number one thing graduating seniors should know
? #MGoGrad

6.80

Brilliant interactive infographic : shows cone of uncertainty for #climate-
change [URL] #howhotwillitget

12.1

Table 2: Example tweets from each category.

normal.4 Finally, we have baseline tweets which
contain neither mentions nor hashtags and whose ex-
pected audience size is approximately one’s follow-
ers.

Intuitively, common ground is higher for smaller
audiences. It should be highest for the invisible
tweets, where the audience is limited and has seen
or can readily access the previous tweets in the con-
versation. It should be lowest for the hashtagged
tweets, where the audience is the largest and will
likely contain many users who are completely unfa-
miliar with the tweeter. If contextualized UID is the
driving force affecting information content, then the
invisible tweets should have the highest entropy and
hashtagged tweets should have the lowest.

In this analysis, we use the full 5.5 million tweet
database. Figure 1 plots the entropy of tweets for
these three audience sizes. Per-word and per-tweet
entropy both significantly increase with expected
audience size (p < .001 by likelihood-ratio test), the
opposite direction of our prediction. We discuss this
finding below in the context of our next analyses.

4Not all hashtags are intended for categorization; some are
used for emphasis or metalinguistic comment (e.g. #notmyfa-
voritefridaymeal, #toomuchinformation). These comments are
probably not intended to broaden the tweet’s audience. The
presence of such hashtags should, if anything, cause our analy-
sis to underestimate variability across audience types.

●

●

●

7.8

8.2

8.6

9.0

invisible
baseline

hashtagged

tweet type

pe
r−

w
or

d 
pe

rp
le

xi
ty

 (
bi

ts
)

●

●

●

100

110

120

130

invisible
baseline

hashtagged

tweet type

pe
r−

tw
ee

t p
er

pl
ex

ity
 (

bi
ts

)
Figure 1: Per-word (left) and per-tweet (right) entropy
are higher for tweets with larger expected audience size.
Error bars (in some cases smaller than plotting marker)
show by-user 95% confidence intervals.

3.2 Known audience size

The results from expected audience size in Section
3.1 have a potential explanation: different tweet
types are received and viewed in different ways,
which may encourage different kinds of commu-
nicative behavior. Tweets with mentions are highly
likely to be seen by the mentioned user (unless
the mentioned user is very popular), whereas the
likelihood of a given hashtagged tweet being seen
through the hashtag-searching mechanism is very
low. This uncertainty about audience may lead a
rational tweeter to package information into tweets
differently: they may include more redundant in-
formation across tweets when the likelihood of any

23



●

●

●

●

●

●

●

●

● ●

8.0

8.1

8.2

8.3

8.4

1 2 4
# of mentions

pe
r−

w
or

d 
pe

rp
le

xi
ty

 (
bi

ts
)

visibility

●

●

invisible

visible

Figure 2: Per-word entropy of tweets with different num-
bers of mentions and different visibility. Invisible tweets’
entropy increases with mentions, while visible tweets’
entropy decreases. Logarithmic fits with 95% confidence
intervals; x-axis is log-scaled.

given tweet being read is low.
To assess audience size effects in a more con-

trolled setting, we look at invisible tweets with vary-
ing numbers of mentions. Invisible tweets provide
a quantifiable audience size; those with few men-
tions have a smaller audience than those with more
mentions. Visible tweets, on the other hand, have
approximately the same audience size regardless of
the number of mentions, since all of a user’s fol-
lowers can see them. Visible mentions can be used
for a wide range of discourse functions (e.g., self-
promotion, bringing one’s followers into an argu-
ment, entering contests), and so we do not have a
clear prediction of their behavior. But invisible men-
tions should, under the UID hypothesis, show de-
creased common ground as the number of conver-
sation participants grows and it is harder to achieve
consensus on what all participants know.

Figure 2 shows that the per-word entropy of invis-
ible tweets goes up with the logarithm of the number
of mentions. We look only at tweets with between
one and five mentions, as invisible tweets must have
at least one mention, and five mentions already sub-
stantially cut into the 140-character limit.5 This
leaves 1.4 million tweets.

The fact that invisible tweet entropy increases

5Usernames can be up to 15 characters (plus a space and an
symbol per mention); even if each username is only 7 charac-
ters, five mentions use almost one-third of the character limit.

●

8.0

8.2

8.4

0 1 3
reply level

pe
r−

w
or

d 
pe

rp
le

xi
ty

 (
bi

ts
)

maximum
depth

● 0

1

2

3

4

5

●

90

100

110

120

130

140

150

0 1 3
reply level

pe
r−

tw
ee

t p
er

pl
ex

ity
 (

bi
ts

)

maximum
depth

● 0

1

2

3

4

5

Figure 3: Per-word (top) and per-tweet (bottom) entropy
decrease with reply level and increase with conversation
length. Logarithmic fits with 95% confidence intervals;
x-axis is log-scaled.

with number of mentions, even as visible tweet en-
tropy decreases, suggests that audience size is hav-
ing an effect. However, this effect is causing en-
tropy to increase as common ground should be de-
creasing due to the larger number of conversation
participants. Furthermore, this effect is not driven
by reply level (Sect 3.3); there is a significant in-
crease (p < .001) in explanatory power from adding
number of mentions to a mixed-effects model with
fixed-effects of reply level and a by-user random in-
tercept.

3.3 Reply level and conversation length

We next turn to our second UID prediction: that
information content should increase as common
ground increases. As common ground is assumed to
increase in dialogues (Clark, 1996), we thus predict
that Twitter conversations should show increases in

24



information content that scale with reply level, the
number of replies between the current tweet and the
conversation root. Such a result would constitute a
replication of Genzel and Charniak (2002) in the dis-
course context, and would confirm preliminary re-
sults on the Switchboard corpus by Vega and Ward
(2009). As is clear from our analysis below, that is
not what we found.

Figure 3 plots mean perplexities for different re-
ply levels and conversation lengths, with confidence
intervals based on by-user means. Increasing the re-
ply level decreases the information content of the
tweet, while increasing the conversation length in-
creases the information content.

We fit a linear mixed-effects regression model to
per-word and per-tweet perplexity. Control factors
were the logarithm of the tweet reply level and the
logarithm of the conversation length, along with a
separate binary variable for whether the tweet was
part of a conversation at all, and random by-user
intercepts. Both log reply level and log conversa-
tion length had significant effects by likelihood-ratio
tests.

Log reply level had negative effects on per-word
and per-tweet perplexity (per-word: −.341 ± .009;
per-tweet: −39.6± .3; both p < .001). Log conver-
sation length had positive effects on per-word and
per-tweet perplexity (per-word: .285 ± .010; per-
tweet: 35.1± .3; both p < .001).

To summarize these effects, all conversations lose
entropy as they go along, and tweets that start longer
conversations tend to have higher entropy to start.
Whereas previous work has suggested that messages
become more unpredictable as context builds up,
Twitter conversations appear to shift to more pre-
dictable messages as context builds up, and seem to
go until messages get sufficiently predictable. We
discuss these results below.

4 General Discussion

Previous work supports the UID hypothesis that ra-
tional communicators adjust their messages so as
to spread information as uniformly as possible in
response to local context (Aylett and Turk, 2004;
Levy and Jaeger, 2007), as well as to discourse-level
context in monologic writing (Genzel and Charniak,
2002; Qian and Jaeger, 2012).

Our current work synthesizes these two bodies
of work by looking for evidence of discourse-level
UID effects in a large corpus of Twitter conver-
sations, including dialogues and many-party con-
versations. Contrary to expectations, we failed to
find UID effects; in fact, we often found informa-
tion rate increasing when context changes would
have predicted decreasing information rates. Specif-
ically, we found that messages to smaller audiences,
which should have lower noise and greater context
and hence higher information density, actually have
lower information density than messages to larger,
noisier, and less context-sharing audiences. Further-
more, we found that later messages within a reply
chain, which should have greater context, have less
information. This last result is especially surprising
because UID context effects have been repeatedly
found in less conversational texts.

So should we give up on UID? While our re-
sults were unexpected, as we discuss below, we be-
lieve that they instead encourage more reflection on
how speakers conceptualize information for conver-
sational UID. We consider two aspects of these con-
versations: first, that the collaborative nature of con-
versation introduces rational uses for messages with
low (lexicosyntactic) information density; second,
that rational behaviors resulting from the nature of
noise in social media communication complicates
our evaluations of UID.

4.1 Information in terms of contributions
Why do Twitter conversations look different from
the increasingly-informative texts studied in previ-
ous work? For one, our dataset contains true con-
versations, whereas almost all of the sentence-level
context informativity results were based on single-
author texts.

In monologues, there is neither an ability nor a
need to check that common ground has been es-
tablished. Participants in a conversation, though,
must both produce content and establish ground-
ing (Clark and Schaefer, 1987). Participants em-
ploy many methods to establish grounding, includ-
ing backchannels (Yngve, 1970; Schegloff, 1982;
Iwasaki, 1997), which have little lexicosyntactic
information but provide crucial turn-taking and
grounding cues.

Dialogues are often reactive; for instance, a re-

25



ply may be a clarification question, such as this re-
ply in our dataset: [MENTION] What do you mean
you saw the pattern? Such replies are typically
shorter and more predictable than the original state-
ment, and are often part of adjacency or coordinate
pairs (Schegloff and Sacks, 1973; Clark and French,
1981), where one participant’s utterance massively
constrains the other’s next utterance. Such pairs
cover a wide range of low-entropy messages that are
likely to appear in multi-party conversation but not
in monologic text, including question-and-answers,
offer-and-acceptances, and goodbyes.

As a result, Clark and Schaefer (1987) argue for
a collaborative view of conversation structure, with
conversations best viewed not as a series of utter-
ances but as a series of contributions—sets of utter-
ances that, combined, both specify some new con-
tent and establish it as part of the common ground.
UID as a rational behavior is based on the idea that
a rational speaker seeks to maximize linguistic in-
formation transfer, which would seem to be the pri-
mary goal during the content-specification portion
of a contribution. During the grounding portion of
a contribution, though, the primary goal is likely to
be to establish common ground as quickly as pos-
sible. This goal is potentially more complex, as it
depends on a variety of factors including the quality
of the content-specification portion. If the content
specification was simple and clear, grounding can be
acheived with low-entropy backchannels (mm-hmm,
right, etc.); if it was complex or unclear, grounding
will require more messages and greater message en-
tropy.

Furthermore, conversations may contain ex-
changes that have little linguistic context but serve
important social ends. Many response tweets in
our dataset are single, common words (haha, lol)
or emoji/emoticons. These provide important emo-
tional information in a very low lexicosyntactic en-
tropy package, much as a backchannel or meta-
linguistic cue (e.g., a smile) might in face-to-face
conversation. This suggests that the information
measure within UID may not be strictly based on lit-
eral lexicosyntactic information but rather a combi-
nation of linguistic and metalinguistic information.

In sum, this conception suggests that in discourse,
UID may operate as usual for parts of a contri-
bution, but not necessarily throughout it. Ratio-

nal conversational behavior may resemble an error-
checking system in which UID may be observed at
a contribution-by-contribution level.

4.2 Multiple types of noise

In most of the previously-studied genres, the authors
of the texts could reasonably expect their readers to
be both focused and unlikely to stop while reading.
Tweets, however, are often read and responded to
while doing other tasks, reducing focus and increas-
ing disengagement rates. Interestingly, the one genre
where Genzel and Charniak (2003) found a nega-
tive effect of sentence number on informativity was
tabloid newspapers, where readers are likely to be
distractable and disengaged.

It may be that Twitter requires an idiosyncratic
adjustment to the noisy-channel model: perhaps the
locus of the noise in tweets should not be in com-
prehension of the tweet per se (or at least not exclu-
sively on comprehension). Instead, the main source
of noise for Twitter users may be whether a reader
engages with the tweet at all. Many Twitter users
follow an enormous number of users, so outside of
directed mentions and replies, there is a substantial
chance that any given tweet will go unread by the
larger part of its intended audience.6

The decreases we observed may have to do with
users optimizing the amount of information content
relative to the likelihood of an audience-member
seeing more than one message. For tweets that
go the largest audience, it is unlikely that multiple
tweets would all be seen; thus it makes more sense
to send information-rich tweets that can stand alone.
In contrast, for replies, the intended audience should
notice each sent tweet.

Evidence in favor of the conversation- or noise-
based explanation could be obtained by compar-
ing the Twitter reply chain effects against a corpus
of conversations in which message reception is es-
sentially certain, as in person-to-person chat logs
(e.g., Potts 2012). If noise at the message level
accounts for the anomalous Twitter behavior, then

6As a result, tweeters often create tweets that include their
own context; for instance, a reply may quote part of its preced-
ing tweet, or a user may talk about a recent event and include an
explanatory link. This example from the corpus does both: Pls
help if you can! RT [MENTION]: Henry broke his foot [URL]
Please donate: [URL].

26



chat logs should show the UID effect of increas-
ing entropy through the conversation. If turn-taking
or meta-linguistic discourse functions drive it, chat
logs would show decreasing entropy, as in our data.

4.3 Conclusions

We tested the Uniform Information Density hy-
pothesis, which has been robustly demonstrated in
monologue-like settings, on dialogues in Twitter.
Surprisingly, we failed to find the predicted effects
of context within these dialogues, and at times found
evidence for effects going in the opposite direction.
We proposed that this behavior may indicate a cru-
cial difference in how information flow is structured
between monologues and conversations, as well as
how rational adaptation to noise manifests in differ-
ent conversational settings.

Acknowledgments

We gratefully acknowledge the support of ONR
Grant N00014-13-1-0287.

References
Matthew Aylett and Alice Turk. 2004. The smooth sig-

nal redundancy hypothesis: A functional explanation
for relationships between redundancy, prosodic promi-
nence, and duration in spontaneous speech. Language
and Speech, 47(1):31–56.

Matthew Aylett and Alice Turk. 2006. Language redun-
dancy predicts syllabic duration and the spectral char-
acteristics of vocalic syllable nuclei. The Journal of
the Acoustical Society of America, 119(5):3048–3058.

Alan Bell, Daniel Jurafsky, Eric Fosler-Lussier, Cynthia
Girand, Michelle Gregory, and Daniel Gildea. 2003.
Effects of disfluencies, predictability, and utterance
position on word form variation in English conversa-
tion. The Journal of the Acoustical Society of America,
113(2):1001–1024.

Herbert H. Clark and J. Wade French. 1981. Telephone
goodbyes. Language in Society, 10:1–19.

Herbert H. Clark and Edward F. Schaefer. 1987. Collab-
orating on contributions to conversations. Language
and Cognitive Processes, 2:19–41.

Herbert H. Clark. 1996. Using language, volume 1996.
Cambridge University Press Cambridge.

Vera Demberg, Asan Sayeed, Phillip Gorinski, and Niko-
laos Engonopoulos. 2012. Syntactic surprisal af-
fects spoken word duration in conversational con-
texts. In Proceedings of the 2012 Joint Conference

on Empirical Methods in Natural Language Process-
ing and Computational Natural Language Learning,
pages 356–367.

Gabriel Doyle and Michael C. Frank. 2015. Shared
common ground influences information density in mi-
croblog texts. In Proceedings of NAACL-HLT.

Gabriel Doyle. 2014. Mapping dialectal variation by
querying social media. In Proceedings of the Euro-
pean Chapter of the Association for Computational
Linguistics.

Austin Frank and T. Florian Jaeger. 2008. Speaking
rationally: Uniform information density as an opti-
mal strategy for language production. In Proceed-
ings of the 30th Annual Meeting of the Cognitive Sci-
ence Society, pages 933–938. Cognitive Science Soci-
ety Washington, DC.

Dmitriy Genzel and Eugene Charniak. 2002. Entropy
rate constancy in text. In Proceedings of the 40th
Annual Meeting of the Association for Computational
Linguistics, pages 199–206. Association for Computa-
tional Linguistics.

Dmitriy Genzel and Eugene Charniak. 2003. Variation
of entropy and parse trees of sentences as a function of
the sentence number. In Proceedings of the 2003 con-
ference on Empirical Methods in Natural Language
Processing, pages 65–72. Association for Computa-
tional Linguistics.

Shoichi Iwasaki. 1997. The northridge earthquake
conversations: The floor structure and the ’loop’ se-
quence in japanese conversation. Jouranl of Pragmat-
ics, 28:661–693.

T. Florian Jaeger. 2010. Redundancy and reduction:
Speakers manage syntactic information density. Cog-
nitive Psychology, 61(1):23–62.

Roger Levy and T. Florian Jaeger. 2007. Speakers opti-
mize information density through syntactic reduction.
In Advances in Neural Information Processing Sys-
tems, pages 849–856.

Roger Levy. 2008. Expectation-based syntactic compre-
hension. Cognition, 106(3):1126–1177.

Olutobi Owoputi, Brendan O’Connor, Chris Dyer, Kevin
Gimpel, Nathan Schneider, and Noah A. Smith. 2013.
Improved part-of-speech tagging for online conver-
sational text with word clusters. In Proceedings of
NAACL-HLT.

Christopher Potts. 2012. Goal-driven answers in the
Cards dialogue corpus. In Nathan Arnett and Ryan
Bennett, editors, Proceedings of the 30th West Coast
Conference on Formal Linguistics, Somerville, MA.
Cascadilla Press.

Ting Qian and T. Florian Jaeger. 2012. Cue effective-
ness in communicatively efficient discourse produc-
tion. Cognitive Science, 36(7):1312–1336.

27



Emanuel Schegloff and Harvey Sacks. 1973. Opening
up closings. Semiotica, 8:289–327.

Emanuel Schegloff. 1982. Discourse as an interactional
achievement: Some uses of ’uh huh’ and other things
that come between sentences. In Deborah Tannen, ed-
itor, Analyzing Discourse: Text and Talk, pages 71–93.
Georgetown Univ. Press.

Alejandro Vega and Nigel Ward. 2009. Looking for en-
tropy rate constancy in spoken dialog. Technical Re-
port UTEP-CS-09-19, UTEP.

Victor Yngve. 1970. On getting a word in edgewise. In
Papers from the Sixth Regional Meeting of the Chicago
Linguistic Society, pages 567–578. Univ. of Chicago
Dept. of Linguistics.

28


