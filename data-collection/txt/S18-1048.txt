



















































SSN MLRG1 at SemEval-2018 Task 1: Emotion and Sentiment Intensity Detection Using Rule Based Feature Selection


Proceedings of the 12th International Workshop on Semantic Evaluation (SemEval-2018), pages 324–328
New Orleans, Louisiana, June 5–6, 2018. ©2018 Association for Computational Linguistics

SSN MLRG1 at SemEval-2018 Task 1: Emotion and Sentiment Intensity
Detection Using Rule Based Feature Selection

Angel Deborah S, Rajalakshmi S, S Milton Rajendram, Mirnalinee T T
SSN College of Engineering

Chennai 603 110, Tamil Nadu, India
angeldeborahs@ssn.edu.in, rajalakshmis@ssn.edu.in,

miltonrs@ssn.edu.in, mirnalineett@ssn.edu.in

Abstract

The system developed by the SSN MLRG1
team for Semeval-2018 task 1 on affect in
tweets uses rule based feature selection and
one-hot encoding to generate the input fea-
ture vector. Multilayer Perceptron was used to
build the model for emotion intensity ordinal
classification, sentiment analysis ordinal clas-
sification and emotion classfication subtasks.
Support Vector Regression was used to build
the model for emotion intensity regression and
sentiment intensity regression subtasks.

1 Introduction

Twitter is a huge microblogging service with more
than 500 million tweets per day from differ-
ent locations of the world and in different lan-
guages (Saif and Felipe, 2017). Tweets are of-
ten used to convey ones emotions, opinions to-
wards products, and stance over issues (Nabil
et al., 2016). Automatically detecting emotion
intensities in tweets has several applications, in-
cluding commerce (Jansen et al., 2009), crisis
management (Verma et al., 2011), tracking brand
and product perception, tracking support for is-
sues and policies, and tracking public health and
well-being (Chew and Eysenbach, 2010). The
task is challenging because of the informal writing
style, the semantic diversity of content as well as
the “unconventional” grammar. These challenges
in building a classification model and regression
model can be handled by using proper approaches
to feature generation and machine learning.

There are several machine learning techniques
that can be used for sentiment intensity predic-
tion or emotion intensity prediction. Some of
the approaches inlclude Artificial Neural Network
(ANN) (Sudipta et al., 2017), Random Forests,
Support Vector Machine (SVM), Naive Bayes
(NB) (Tabari et al., 2017), Multi-Kernel Gaussian

Process (MKGP) (Angel Deborah et al., 2017a,b),
AdaBoost Regressor (ABR), Bagging Regres-
sor (BR)(Jiang et al., 2017) and Deep Learning
(DL)(Pivovarova et al., 2017).

2 Multi-Layer Perceptron

A Multilayer Perceptron (MLP) as shown in Fig-
ure 1 is a feed-forward Neural Network model that
maps input data sets onto appropriate output sets.
An MLP has many layers of nodes in a directed
graph, with each layer connected to the next layer.
A neuron is a processing element with activation

Input1

Input2

Input3

Output1

Output2

Figure 1: Multi-Layer Perceptron.

function (in the input layer the activation func-
tion is not applied). The output layer has as many
nuerons as the number of class labels in the prob-
lem. Each connection has a weight assigned to
it. Output of each neuron is calculated by apply-
ing the activation function on the weighted sum
of the inputs. Linear, sigmoid, tanh, elu, soft-
plus, softmax and relu are some of the commonly
used activation functions. The supervised learning
problem of the MLP can be solved with the back-
propagation algorithm (Haykin, 1998). The algo-
rithm consists of two steps. In the forward pass,
the predicted outputs are calculated for the given
inputs . In the backward pass, partial derivatives

324



of the cost function with respect to the weight pa-
rameters are propagated back through the network.
The chain rule of differentiation gives very similar
computational rules for the backward pass as the
ones for the forward pass. The network weights
can then be adapted using any gradient-based op-
timisation algorithm.

MLP was used in implementing for the follow-
ing subtasks:

1. EI-oc (an emotion intensity ordinal classifi-
cation task): Given a tweet and an emotion
E, classify the tweet into one of four ordinal
intensity classes of E that best represents the
mental state of the tweeter.

2. V-oc (a sentiment analysis, ordinal classifi-
cation task): Given a tweet, classify it into
one of seven ordinal classes, corresponding
to various levels of positive and negative sen-
timent intensity, that best represents the men-
tal state of the tweeter.

3. E-c (an emotion classification task): Given a
tweet, classify it as neutral or no emotion or
as one, or more, of eleven given emotions that
best represent the mental state of the tweeter.

3 Support Vector Regression

Support Vector Machines (SVM) are character-
ized by usage of kernels, absence of local min-
ima, sparseness of the solution and capacity con-
trol obtained by acting on the margin or on num-
ber of support vectors, etc. As in classification,
support vector regression (SVR) is characterized
by the use of kernels, sparse solution, and Vapnik-
Chervonenkis control of the margin and the num-
ber of support vectors. Although less popular
than SVM, SVR has been proven to be an effec-
tive tool in real-value function estimation (Awad
and Khanna, 2015). The idea of SVR is based
on the computation of a linear regression func-
tion in a high dimensional feature space where the
input data are mapped via a non-linear function.
It contains all the main features that characterize
maximum margin algorithm: a non-linear func-
tion is learned by a linear learning machine map-
ping into high dimensional kernel-induced feature
space. The capacity of the system is controlled by
parameters that do not depend on the dimensional-
ity of feature space. Instead of minimizing the ob-
served training error, Support Vector Regression
(SVR) attempts to minimize the generalization er-
ror bound so as to achieve generalization perfor-

mance.
SVR was used in implementing for the follow-

ing subtasks:
1. EI-reg (an emotion intensity regression

task): Given a tweet and an emotion E, de-
termine the intensity of E that best represents
the mental state of the tweeter, a real-valued
score between 0 (least E) and 1 (most E).

2. V-reg (a sentiment intensity regression task):
Given a tweet, determine the intensity of sen-
timent or valence (V) that best represents
the mental state of the tweeter, a real-valued
score between 0 (most negative) and 1 (most
positive).

4 System Overview

The system consists of the following modules:
data extraction, preprocessing, rule based feature
selection, feature vector generation and building
the model – Multilayer Perceptron model for clasi-
fication subtasks and Support Vector Regression
for regression subtasks. The algorithm for prepro-
cessing of the data is outlined below:
Algorithm: Data extraction and Preprocessing.
Input: Input dataset.
Output: Tokenized words and their parts of
speech.
begin

1. Separate labels and sentences
2. Perform tokenization using

word tokenize, the function for tok-
enizing in the NLTK toolkit.

3. Perform Parts of Speech tagging using
pos tag function from the NLTK toolkit.

4. Return the tokenized words and their parts of
speech as inputs to rule based feature selec-
tion.

end
The algorithm for rule based feature selection

and feature vector generation is outlined below:
Algorithm: Rule based feature selection and fea-
ture vector generation.
Input: Tokenized words and their parts of speech.
Output: Feature vector.
begin
For each of the tokenized words, falling under one
of the categories listed in Table 1, do the follow-
ing steps.

1. Lemmatize the word using WordNet
Lemmatizer from the NLTK toolkit.

2. Insert the lemmatized word into the dictio-

325



nary.
3. Represent each sentence as a feature vector

using one-hot encoding by looking up the
dictionary.

4. Return the feature vector generated as the in-
put to build the model.

end

Abbreviation Parts of Speech
VB verb, base form
VBZ verb, 3rd person sing. present
VBP verb, non 3rd sing. present
VBD verb, past tense
VBG verb, gerund/present participle
VBN verb, past participle
JJ adjective
JJR adjective, comparative
JJS adjective, superlative
RB adverb, very
RBR adverb, comparative
RBS adverb, superlative
NN noun, singular
NNP proper noun, singular
NNS noun, plural
NNPS proper noun, plural

Table 1: Parts of speech categories.

The Multi-Layer Perceptron (MLP) model is
built by the following algorithm.
Algorithm: Build a Multi-Layer Perceptron
model.
Input: Feature vectors and actual output labels.
Output: Learned MLP model.
begin

1. Represent feature vectors as XTrain and ac-
tual output label as YTrain.

2. Build the initial classification model with two
hidden layers and the output layer, using
relu and softmax activation functions, re-
spectively.

3. Optimize the classification model using
nadam optimizer of keras package

4. Return the learned model.
end

The Support Vector Regression(SVR) model is
bulit with the following algorithm.
Algorithm: Build a SVR model.
Input: Feature vectors and actual output labels.
Output: Learned SVR model.
begin

1. Represent feature vectors as XTrain and ac-

tual output label as YTrain.
2. Build the initial classification model using

appropriate kernel function.
3. Optimize the model parameters using a

heuristic approach.
4. Return the learned model.

end

5 Performance Evaluation

We evaluated the system only for English lan-
guage. The results obtained using MLP and SVR
for the subtasks are tabulated in Table 2 to Table 6.
From Table 2 which shows the Pearson scores ob-
tained for SVR, we can infer that SVR predicts joy
better compared to anger, fear and sadness. Simi-
larly, from Table 3 which shows the Pearson scores
obtained for MLP, we observe that MLP model
predicts joy better compared to anger, fear and
sadness. The Pearson score for valence intensity
regression and sentiment intensity ordinal classifi-
cation are given in Table 4 and 5 respectively.

anger fear joy sadness macro
averaged

0.490 0.490 0.502 0.461 0.486

Table 2: Pearson score for EI-reg (emotion intensity
regression) using SVR.

anger fear joy sadness macro
averaged

0.365 0.363 0.390 0.383 0.375

Table 3: Pearson score for EI-oc (emotion intensity or-
dinal classification) using MLP.

Pearson (all instances) Pearson (gold in 0.5-1)
0.582 0.424

Table 4: Pearson score for V-reg (valence intensity re-
gression) using SVR.

Pearson (all classes) Pearson (some-emotion)
0.427 0.479

Table 5: Pearson score for V-oc (Sentiment intensity
ordinal classification) using MLP.

The Pearson score r is calculated using the
Equation 1.

r =

∑n
i=1(Yi − Ȳ )(yi − ȳ)√∑n

i=1(Yi − Ȳ )2
√∑n

i=1(yi − ȳ)2
(1)

326



Accuracy Micro-avg F1 Macro-avg F1
0.468 0.595 0.476

Table 6: Pearson score for E-c (multi-label emotion
class) using MLP.

where Y is actual output and y is predicted out-
put. The accuracy, micro-averaged F score and
macro-avearged F score for emotion classifica-
tion are given in Table 6. The metrics are defined
from Equations 2 to 5.

Accuracy =
1

|T |
∑

t∈T

|Gt ∩ Pt|
|Gt ∪ Pt|

(2)

where Gt is the set of the gold labels for tweet t,
Pt is the set of the predicted labels for tweet t, and
T is the set of tweets.

Micro-avearge F =
2×micro-P ×micro-R

micro-P ×micro-R
(3)

where micro-P is micro-averaged precision and
micro-R is micro-averaged recall

F =
2× Pe ×Re
Pe ×Re

(4)

Macro-average F =
1

|E|
∑

e∈E
Fe (5)

where Pe is precision, Re is recall and E is the
given set of eleven emotions.

6 Conclusion

We have presented the results of using MultiLayer
Perceptron for emotion intensity ordinal classi-
fication, sentiment analysis ordinal classification
and emotion classification. We built a basic MLP,
which has an input layer, two hidden layers with
128 and 64 neurons, and an output layer with as
many neurons as the number of class labels. We
have used nadam optimizer with learning rate as
0.01. We have also presented the results of using
Support Vector Regression for emotion intenisty
and sentiment intensity regression. It is observed
that both MLP and SVR predict joy more accu-
rately when compared to anger, fear and sadness.
We analyzed the feature vectors generated for var-
ious emotions. Feature vectors generated for joy
helps to achieve better results than for other emo-
tions. We used rule based feature selection and
one-hot encoding to generate input feature vectors
for buliding the models. The results obtained can

be enhanced by using different feature selection
approaches and incorporating sentiment lexicons.

References
S Angel Deborah, S Milton Rajendram, and T T Mir-

nalinee. 2017a. Ssn mlrg1 at semeval-2017 task
4: Sentiment analysis in twitter using multi-kernel
gaussian process classifier. In Proceedings the 11th
International Workshop on Semantic Evaluation
(SemEval-2017), pages 709–712. ACL,Vancouver,
Canada.

S Angel Deborah, S Milton Rajendram, and T T Mir-
nalinee. 2017b. Ssn mlrg1 at semeval-2017 task
5: Fine-grained sentiment analysis using multiple
kernel gaussian process regression model. In Pro-
ceedings the 11th International Workshop on Se-
mantic Evaluation (SemEval-2017), pages 823–826.
ACL,Vancouver, Canada.

M Awad and R Khanna. 2015. Support Vector Re-
gression. In: Efficient Learning Machines. Apress,
Berkeley, CA.

C. Chew and G. Eysenbach. 2010. Pandemics in the
age of twitter: content analysis of tweets during the
2009 h1n1 outbreak. PloS ONE, 5(11):1–13.

Simon Haykin. 1998. Neural Networks – A Compre-
hensive Foundation. Second Edition, Prentice-Hall,
Englewood Cliffs, NJ.

B. J. Jansen, K. Sobel M. Zhang, and A. Chowdury.
2009. Twitter power: Tweets as electronic word of
mouth. Journal of the American Society for Infor-
mation Science and Technology, 60(11):2169–2188.

Mengxiao Jiang, Man Lan1, and Yuanbin Wu. 2017.
Ecnu at semeval-2017 task 5: An ensemble of re-
gression algorithms with effective features for fine-
grained sentiment analysis in financial domain. In
Proceedings the 11th International Workshop on Se-
mantic Evaluation (SemEval-2017), pages 888–893.
ACL,Vancouver, Canada.

Mahmoud Nabil, Mohamed Aly, and Amir F. Atiya.
2016. Cufe at semeval-2016 task 4: A gated recur-
rent model for sentiment classification. In Proceed-
ings of SemEval-2016.ACL, Vancouver, Canada.,
pages 52–57.

Lidia Pivovarova, Llorenc Escoter, Arto Klami, and
Roman Yangarber. 2017. Hcs at semeval-2017 task
5: Sentiment detection in business news using con-
volutional neural networks. In Proceedings the 11th
International Workshop on Semantic Evaluation
(SemEval-2017), pages 842–846. ACL,Vancouver,
Canada.

M. Mohammad Saif and Bravo-Marquez Felipe. 2017.
Emotion intensities in tweets. In Proceedings of
sixth joint conference on lexical and computational
semantics (*Sem). Vancouver, Canada., pages 65–
77.

327



Kar Sudipta, Suraj Maharjan, and Thamar Solorio.
2017. Ritual-uh at semeval-2017 task 5: Sentiment
analysis on financial data using neural networks. In
Proceedings the 11th International Workshop on Se-
mantic Evaluation (SemEval-2017), pages 877–882.
ACL,Vancouver, Canada.

Narges Tabari, Armin Seyeditabari, and Wlodek
Zadrozny. 2017. Sentiheros at semeval-2017 task
5: An application of sentiment analysis on finan-
cial tweets. In Proceedings the 11th International
Workshop on Semantic Evaluation (SemEval-2017),
pages 857–860. ACL,Vancouver, Canada.

S. Verma, S. Vieweg, W. J. Corvey, L. Palen, J. H.
Martin, M. Palmer, A. Schram, and K. M. Ander-
son. 2011. Natural language processing to the res-
cue extracting situational awareness tweets during
mass emergency. In Proceedings of 5th Interna-
tional Conference on Web and Social Media.

328


