



















































Learning Topic Hierarchies for Wikipedia Categories


Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics
and the 7th International Joint Conference on Natural Language Processing (Short Papers), pages 346–351,

Beijing, China, July 26-31, 2015. c©2015 Association for Computational Linguistics

Learning Topic Hierarchies for Wikipedia Categories
Linmei Hu†, Xuzhong Wang§, Mengdi Zhang†, Juanzi Li†, Xiaoli Li‡,

Chao Shao†, Jie Tang†, Yongbin Liu†
† Dept. of Computer Sci. and Tech. Tsinghua University, China

§ State Key Laboratory of Math. Eng. and Advanced Computing, China
‡ Institute for Infocomm Research(I2R), A*STAR, Singapore

{hulinmei1991,koodoneko,mdzhangmd,lijuanzi2008}@gmail.com
xlli@i2r.a-star.edu.sg, birdlinux@gmail.com

jietang@tsinghua.edu.cn, yongbinliu03@gmail.com

Abstract

Existing studies have utilized Wikipedia
for various knowledge acquisition tasks.
However, no attempts have been made to
explore multi-level topic knowledge con-
tained in Wikipedia articles’ Contents ta-
bles. The articles with similar subjects
are grouped together into Wikipedia cat-
egories. In this work, we propose novel
methods to automatically construct com-
prehensive topic hierarchies for given cat-
egories based on the structured Contents
tables as well as corresponding unstruc-
tured text descriptions. Such a hierarchy is
important for information browsing, doc-
ument organization and topic prediction.
Experimental results show our proposed
approach, incorporating both the structural
and textual information, achieves high
quality category topic hierarchies.

1 Introduction

As a free-access online encyclopedia, written
collaboratively by people all over the world,
Wikipedia (abbr. to Wiki) offers a surplus of rich
information. Millions of articles cover various
concepts and instances 1. Wiki has been wide-
ly used for various knowledge discovery tasks.
Some good examples include knowledge mining
from Wiki infoboxes (Lin et al., 2011; Wang et al.,
2013), and taxonomy deriving from Wiki category
system (Zesch and Gurevych, 2007).

We observe that, in addition to Wiki’s infobox-
es and category system, Wiki articles’ Contents
tables (CT for short) also provide valuable struc-
tured topic knowledge with different levels of
granularity. For example, in the article “2010
Haiti Earthquake”, shown in Fig.1, the left Con-
tents zone is a CT formed in a topic hierarchy for-

1http://en.wikipedia.org/wiki/Encyclopedia

Figure 1: The Wiki article “2010 Haiti Earth-
quake” with structured Contents table and corre-
sponding unstructured text descriptions.

mat. If we view “2010 Haiti earthquake” as the
root topic, the first-level “Geology” and “Dam-
age to infrastructure” tags can be viewed as it-
s subtopics, and the second-level “Tsunami” and
“Aftershocks” tags underneath “Geology” are the
subtopics of “Geology”. Clicking any of the tags
in Contents, we can jump to the corresponding text
description. Wiki articles contain a wealth of this
kind of structured and unstructured information.
However, to our best knowledge, little work has
been done to leverage the knowledge in CT.

In Wiki, similar articles (each with their own
CT) belonging to the same subject are grouped
together into a Wiki category. We aim to inte-
grate multiple topic hierarchies represented by C-
T (from the articles under the same Wiki catego-
ry) into a comprehensive category topic hierar-
chy (CTH). While there also exist manually built
CTH represented by CT in corresponding Wik-
i articles, they are still too high-level and incom-
plete. Take the “Earthquake” category as an exam-
ple, its corresponding Wiki article 2 only contains

2http://en.wikipedia.org/wiki/Earthquake

346



some major and common topics. It does not in-
clude the subtopic “nuclear power plant”, which
is an important subtopic of the “2011 Japan earth-
quake”. A comprehensive CTH is believed to be
more useful for information browsing, document
organization and topic extraction in new text cor-
pus (Veeramachaneni et al., 2005). Thus, we pro-
pose to investigate the Wiki articles of the same
category to automatically build a comprehensive
CTH to enhance the manually built CTH.

Clearly, it is very challenging to learn a CTH
from multiple topic hierarchies in different articles
due to the following 3 reasons: 1) A topic can be
denoted by a variety of tags in different articles
(e.g., “foreign aids” and “aids from other coun-
tries”); 2) Structural/hierarchical information can
be inconsistent (or even opposite) across differen-
t articles (e.g., “response subtopicOf aftermath”
and “aftermath subtopicOf response” in different
earthquake event articles); 3) Intuitively, text de-
scriptions of the topics in Wiki articles are sup-
posed to be able to help determine subtopic rela-
tions between topics. However, how can we model
the textual correlation?

In this study, we propose a novel approach to
build a high-quality CTH for any given Wiki cate-
gory. We use a Bayesian network to model a CTH,
and map the CTH learning problem as a structure
learning problem. We leverage both structural and
textual information of topics in the articles to in-
duce the optimal tree structure. Experiments on 3
category data demonstrate the effectiveness of our
approach for CTH learning.

2 Preliminaries

Our problem is to learn a CTH for a Wiki catego-
ry from multiple topic hierarchies represented by
CT in the Wiki articles of the category. For ex-
ample, consider the category “earthquake”. There
are a lot of Wikipedia articles about earthquake
events which are manually created by human ex-
perts. In these articles, the CTs imply hierarchi-
cal topic knowledge in the events. However, due
to crowdsourcing nature, these knowledge is het-
erogeneous across different articles. We want to
integrate these knowledge represented by CTs in
different earthquake event articles to form a com-
prehensive understanding of the category “earth-
quake” (CTH).

Specifically, our input consists of a set of Wiki
articles Ac = {a}, belonging to a Wiki category

c. As shown in Fig.1, each article a ∈ Ac con-
tains a CT (topic tree hierarchy) Ha = {Ta, Ra},
where Ta is a set of topics, each denoted by a tag
g and associated with a text description dg, and
Ra = {(gi, gj)}, gi, gj ∈ Ta is a set of subtopic
relations (gj is a subtopic of gi). The output is
an integrated comprehensive CTH Hc = {Tc, Rc}
where Tc = {t} is a set of topics, each denot-
ed by a set of tags t = {g} and associated by
a text description dt aggregated by {dg}g∈t, and
Rc = {(ti, tj)}, ti, tj ∈ Tc is a set of subtopic
relations (tj is a subtopic of ti).

We map the problem of learning the output Hc
from the input {Ha}, a ∈ Ac, as a structure learn-
ing problem. We first find clusters of similar tags
Tc (each cluster represents a topic) and then derive
hierarchical relations Rc among these clusters.

Particularly, given a category c, we first col-
lect relevant Wiki articles Ac = {a}. This can
be done automatically since each Wiki article has
links to its categories. We can also manually find
the Wikipage which summarizes the links of Ac
(e.g., http://en.wikipedia.org/wiki/
Lists_of_earthquakes) and then collec-
t Ac according to the links.

Then we can get a global tag set G = {g} con-
taining all the tags including titles in the articles
Ac. We cluster the same or similar tags from dif-
ferent articles using single-pass incremental clus-
tering (Hammouda and Kamel, 2003) to construct
the topic set Tc, with cosine similarity computed
based on the names of tags g and their text de-
scriptions dg. Note that titles of all the articles be-
longing to the same cluster corresponds to a root
topic.

Next, the issue is how to induce a CTH Hc =
{Tc, Rc} from a set of topics Tc.

3 Topic Hierarchy Construction

We first present a basic method to learn Hc and
then describe a principled probabilistic model in-
corporating both structural and textual information
for CTH learning.

3.1 Basic Method

After replacing the tags in a CT (see Fig.1) with
the topics they belong to, we can then get a top-
ic hierarchy Ha = {Ta, Ra} for each article
a. For each subtopic relation (ti, tj) ∈ Ra, we
can calculate a count/weight n(ti, tj), represent-
ing the number of articles in Ac containing the

347



relation. We then construct a directed complete
graph with a weight w(ti, tj) = n(ti, tj) on each
edge (ti, tj). Finally, we apply the widely used
Chu-Liu/Edmonds algorithm (Chu and Liu, 1965;
Edmonds, 1967) to find an optimal tree with the
largest sum of weights from our constructed graph,
meaning that the overall subtopic relations in the
tree is best supported by all the CT/articles. The
Chu-Liu/Edmonds algorithm works as follows.
First, it selects, for each node, the maximum-
weight incoming edge. Next, it recursively breaks
cycles with the following idea: nodes in a cycle are
collapsed into a pseudo-node and the maximum-
weight edge entering the pseudo-node is selected
to replace the other incoming edge in the cycle.
During backtracking, pseudo-nodes are expanded
into an acyclic directed graph, i.e., our final cate-
gory topic hierarchy Hc.

However, the basic method has a problem.
Consider if n(“earthquake”, “damages to hos-
pitals”)=10 and n(“earthquake”) =100, while
n(“damages”, “damages to hospitals”)=5 and
n(“damages”)=5. We would prefer “damages” to
be the parent topic of “damages to hospitals” with
a higher confidence level (5/5=1 vs 10/100=0.1).
However, the above basic method will choose
“earthquake” which maximizes the weight sum.
An intuitive solution is to normalize the weights.
In Subsection 3.2, we present our proposed princi-
pled probabilistic model which can derive normal-
ized structure based weights. In addition, it can
be easily used to incorporate and combine textual
information of topics into CTH learning.

3.2 Probabilistic Model for CTH Learning

We first describe the principled probabilistic mod-
el for a CTH. Then we present how to encode
structural dependency and textual correlation be-
tween topics. Last, we present our final approach
combining both structural dependency and textual
correlation for CTH construction.

3.2.1 Modeling a Category Topic Hierarchy

In a topic hierarchy, each node represents a top-
ic. We consider each node as a variable and the
topic hierarchy as a Bayesian network. Then the
joint probability distribution of nodes N given a
particular tree H is

P (N |H) = P (root)
∏

n∈N\root
P (n|parH(n)) ,

where P (n|parH(n)) is the conditional probabili-
ty of node n given its parent node parH(n) in H .
Given the nodes, this is actually the likelihood of
H . Maximizing the likelihood with respect to the
tree structure gives the optimal tree:

H∗ = argmaxHP (N |H)
= argmaxHP (root)

∏
n∈N\root

P (n|parH(n))

= argmaxH
∑
n∈N

logP (n|parH(n))
(1)

Encoding Structural Dependency. Consider-
ing tj is a subtopic of ti, we define the structural
conditional probability:

Pstruc(tj |ti) = n(ti, tj) + α
n(ti) + α · |Tc − 1| , (2)

where n(ti, tj) is the count of articles containing
relation (ti, tj) and n(ti) is the count of articles
containing topic ti. The parameter α = 1.0 is the
Laplace smoothing factor, and |Tc − 1| is the to-
tal number of possible relations taking ti as their
parent topic.

Encoding Textual Correlation. Considering
a topic text description dt as a bag of words,
we use the normalized word frequencies ϕt =
{ϕt,w}w∈V s.t.

∑
w∈V ϕt,w = 1 to represent a top-

ic t. To capture the subtopic relationship (ti, tj),
we prefer a model where the expectation of the
distribution for the child is exactly same with the
distribution for its parent, i.e., E(ϕtj ) = ϕti .
This naturally leads to the hierarchical Dirichlet
model (Wang et al., 2014; Veeramachaneni et al.,
2005), formally, ϕtj |ϕti ∼ Dir(βϕti) in which β
3 is the concentration parameter which determines
how concentrated the probability density is likely
to be. Thus we have:

Ptext(tj |ti) = 1
Z

∏
w∈V

ϕ
βϕti,w−1
tj ,v

, (3)

where Z =
∏

w∈V Γ(αϕti,w)
Γ(

∑
w∈V αϕti,w)

is a normalization fac-
tor and Γ(·) is the standard Gamma distribution.
We note that for the root node we have the uniform
prior instead of the prior coming from the parent.

3.2.2 Combining Structural and Textual
Information

Substituting Eq.2 into Eq.1, we can solve
the optimal tree structure by applying Chu-

3Experimental results are insensitive to β, we set β=5

348



Liu/Edmonds algorithm to the directed com-
plete graph with structure based weights
wstruc=log(Pstruc(tj |ti) = log n(ti,tj)+αn(ti)+α·|Tc−1|
on the edges (ti, tj). While this solves the
problem of the basic method, it only considers
structural dependency and does not consider
textual correlation which is supposed to be useful.

Therefore, we calculate text based weights
wtext=log(Ptext(tj |ti) =

∑
w∈V logϕ

αϕti,w−1
tj ,v

−
logZ similarly. Then we combine structural in-
formation and textual information by defining
the weights w(ti, tj) of the edges (ti, tj) as a
simple weighted average of wstruc(ti, tj) and
wtext(ti, tj). Specifically, we define:

w(ti, tj) = λwtext(ti, tj) + (1− λ)wstruc(ti, tj) ,

where λ controls the impacts of text correla-
tion and structure dependency in optimal structure
learning. Note that wtext and wstruc should be s-
caled 4 first before applying Chu-Liu/Edmonds al-
gorithm to find an optimal topic hierarchy.

4 Experiments

We evaluate the CTH automatically generated by
our proposed methods via comparing it with a
manually constructed ground-truth CTH.

4.1 Data and Evaluation Metric

Data. We evaluate our methods on 3 categories,
i.e., English “earthquake” and “election” cate-
gories containing 293 and 60 articles, and Chi-
nese “earthquake” category containing 48 articles
5. After removing noisy tags such as “references”
and “see also”, they contain 463, 79 and 426 u-
nique tags respectively. After tag clustering 6, we
can get 176, 57 and 112 topics for each category.

Evaluation Metric. We employ the precision
measure to evaluate the performance of our meth-
ods. Let R and Rs be subtopic relation sets of
our generated result and ground-truth result re-
spectively, then precison=|R ∩ Rs|/|R|. Due to
the number of relations |R|=|Rs| = |Tc − 1|, we
have precison=recall=F1=|R ∩ Rs|/|R|.

We compare three methods, including our basic
method (Basic) which uses only non-normalized
structural information, our proposed probabilis-
tic method considering only structural information

4We use min-max normalization x∗ = x−min
max−min

5We filter articles with little information in Contents.
6We use an incremental clustering algorithm

(λ = 0) (Pro+S), and considering both structural
and textual information (0 < λ < 1) (Pro+ST).

4.2 Results and Analysis
Quantitative Analysis. From Table 1, we observe
that our approach Pro+ST (with best λ values as
shown in Fig.2) significantly outperforms Basic
and Pro+S which only utilize the structural infor-
mation (+24.3% and +5.1% on average, p <0.025
with t-test). Pro+S which normalizes structural in-
formation also achieves significant higher preci-
sion than Basic (+19.2% on average, p <0.025).

Earth.(En) Elect.(En) Earth.(Ch)
Basic 0.5965 0.7719 0.7143
Pro+S 0.8971 0.8596 0.9017

Pro+ST 0.9543 0.9298 0.9286

Table 1: Precision of different methods on 3 cate-
gories

Figure 2: The precision of CTH with different λ
values

To examine the influence of λ, we show the
performance of our approach Pro+ST with dif-
ferent λ values on 3 categories in Fig.2. All the
curves grow up first and then decrease dramatical-
ly as we emphasize more on textual information.
They can always get consistent better results when
0.2≤ λ ≤0.3. When λ approaches 1, the precision
declines fast to near 0. The reason is that the top-
ics with short (or null) text descriptions are likely
to be a parent node of all other nodes and influ-
ence the results dramatically, but if we rely mostly
on structural information and use the textual infor-
mation as auxiliary for correcting minor errors in
some ambiguous cases, we can improve the preci-
sion of the resultant topic hierarchy.

Qualitative Analysis. Due to space limitation,
we only show the topic hierarchy for “Election”
with smaller topic size in Fig.3. As we can see,

349



Figure 3: The category topic hierarchy for presi-
dential elections. Topics are labeled by tags sepa-
rated by “#”.

the root topic “presidential elections” includes
subtopics “results”, “vote”, “official candidates”,
etc. Furthermore, ‘official candidates” contain-
s subtopics “debates, “rejected candidates”, “un-
successful candidates”, etc. The above mentioned
examples are shown with red edges. However,
there are also a few (7%) mistaken relations (black
edges) such as “comparison” (should be “official
candidates” instead) → “official candidate web-
sites”. Overall, the above hierarchy aligns well
with human knowledge.

5 Related Work

To our best knowledge, our overall problem set-
ting is novel and there is no previous work using
Wiki articles’ contents tables to learn topic hier-
archies for categories. Existing work mainly fo-
cused on learning topic hierarchies from texts only
and used traditional hierarchical clustering meth-
ods (Chuang and Chien, 2004) or topic models
such as HLDA (Griffiths and Tenenbaum, 2004),
HPAM (Mimno et al., 2007), hHDP (Zavitsanos
et al., 2011), and HETM (Hu et al., 2015). Differ-
ently, we focus on structured contents tables with
corresponding text descriptions.

Our work is also different from ontology (tax-
onomy) construction (Li et al., 2007; Tang et al.,
2009; Zhu et al., 2013; Navigli et al., 2011; Wu
et al., 2012) as their focus is concept hierarchies
(e.g. isA relation) rather than thematic topic hier-
archies. For example, given the “animals” cate-
gory, they may derive “cats” and “dogs”, etc. as
subcategories, while our work aims to derive the-
matic topics “animal protection” and “animal ex-
tinction”, etc. as subtopics. Our work enables a

fresher to quickly familiarize himself/herself with
any new category, and is very useful for informa-
tion browsing, organization and topic extraction.

6 Conclusion

In this paper, we propose an innovative problem,
i.e., to construct high quality comprehensive top-
ic hierarchies for different Wiki categories using
their associated Wiki articles. Our novel approach
is able to model a topic hierarchy and to incorpo-
rate both structural dependencies and text correla-
tions into the optimal tree learning. Experimental
results demonstrate the effectiveness of our pro-
posed approach. In future work, we will inves-
tigate how to update the category topic hierarchy
incrementally with the creation of new related ar-
ticles.

Acknowledgments

The work is supported by 973 Program
(No. 2014CB340504), NSFC-ANR (No.
61261130588), Tsinghua University Initiative
Scientific Research Program (No. 20131089256),
Science and Technology Support Program (No.
2014BAK04B00), China Postdoctoral Science
Foundation (No. 2014M550733), National Natu-
ral Science Foundation of China (No. 61309007)
and THU-NUS NExT Co-Lab.

References
Yoeng-Jin Chu and Tseng-Hong Liu. 1965. On short-

est arborescence of a directed graph. Scientia Sini-
ca, 14(10):1396.

Shui-Lung Chuang and Lee-Feng Chien. 2004. A
practical web-based approach to generating topic hi-
erarchy for text segments. In Proceedings of the
thirteenth ACM international conference on Infor-
mation and knowledge management, pages 127–
136. ACM.

Jack Edmonds. 1967. Optimum branchings. Journal
of Research of the National Bureau of Standards B,
71(4):233–240.

DMBTL Griffiths and MIJJB Tenenbaum. 2004. Hier-
archical topic models and the nested chinese restau-
rant process. Advances in NIPS, 16:17.

Khaled M Hammouda and Mohamed S Kamel. 2003.
Incremental document clustering using cluster sim-
ilarity histograms. In Web Intelligence, 2003. WI
2003. Proceedings. IEEE/WIC International Con-
ference on, pages 597–601. IEEE.

350



Linmei Hu, Juanzi Li, Jing Zhang, and Chao Shao.
2015. o-hetm: An online hierarchical entity topic
model for news streams. In Advances in Knowledge
Discovery and Data Mining - 19th Pacific-Asia Con-
ference, PAKDD 2015, Proceedings, Part I, pages
696–707.

Rui Li, Shenghua Bao, Yong Yu, Ben Fei, and Zhong
Su. 2007. Towards effective browsing of large s-
cale social annotations. In Proceedings of the 16th
international conference on World Wide Web, pages
943–952. ACM.

Wen-Pin Lin, Matthew Snover, and Heng Ji. 2011. Un-
supervised language-independent name translation
mining from wikipedia infoboxes. In Proceedings
of the First workshop on Unsupervised Learning in
NLP, pages 43–52. Association for Computational
Linguistics.

David Mimno, Wei Li, and Andrew McCallum. 2007.
Mixtures of hierarchical topics with pachinko allo-
cation. In Proceedings of the 24th ICML, pages
633–640. ACM.

Roberto Navigli, Paola Velardi, and Stefano Faralli.
2011. A graph-based algorithm for inducing lexi-
cal taxonomies from scratch. In IJCAI, pages 1872–
1877.

Jie Tang, Ho-fung Leung, Qiong Luo, Dewei Chen,
and Jibin Gong. 2009. Towards ontology learn-
ing from folksonomies. In IJCAI, volume 9, pages
2089–2094.

Sriharsha Veeramachaneni, Diego Sona, and Paolo
Avesani. 2005. Hierarchical dirichlet model for
document classification. In Proceedings of the 22nd
ICML, pages 928–935. ACM.

Zhigang Wang, Zhixing Li, Juanzi Li, Jie Tang, and
Jeff Z Pan. 2013. Transfer learning based cross-
lingual knowledge extraction for wikipedia. In ACL
(1), pages 641–650.

Jingjing Wang, Changsung Kang, Yi Chang, and Jiawei
Han. 2014. A hierarchical dirichlet model for tax-
onomy expansion for search engines. In Proceed-
ings of the 23rd international conference on WWW,
pages 961–970. International World Wide Web Con-
ferences Steering Committee.

Wentao Wu, Hongsong Li, Haixun Wang, and Kenny Q
Zhu. 2012. Probase: A probabilistic taxonomy for
text understanding. In Proceedings of the 2012 ACM
SIGMOD International Conference on Management
of Data, pages 481–492. ACM.

Elias Zavitsanos, Georgios Paliouras, and George A
Vouros. 2011. Non-parametric estimation of top-
ic hierarchies from texts with hierarchical dirichlet
processes. The Journal of Machine Learning Re-
search, 12:2749–2775.

Torsten Zesch and Iryna Gurevych. 2007. Analysis
of the wikipedia category graph for nlp application-
s. In Proceedings of the TextGraphs-2 Workshop
(NAACL-HLT 2007), pages 1–8.

Xingwei Zhu, Zhao-Yan Ming, Xiaoyan Zhu, and Tat-
Seng Chua. 2013. Topic hierarchy construction for
the organization of multi-source user generated con-
tents. In Proceedings of the 36th international ACM
SIGIR conference on Research and development in
information retrieval, pages 233–242. ACM.

351


