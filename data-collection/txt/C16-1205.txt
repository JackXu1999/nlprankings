



















































Interactive Attention for Neural Machine Translation


Proceedings of COLING 2016, the 26th International Conference on Computational Linguistics: Technical Papers,
pages 2174–2185, Osaka, Japan, December 11-17 2016.

Interactive Attention for Neural Machine Translation
Fandong Meng1∗ Zhengdong Lu2 Hang Li2 Qun Liu3,4

1AI Platform Department, Tencent Technology Co., Ltd.
fandongmeng@tencent.com

2Noah’s Ark Lab, Huawei Technologies
{Lu.Zhengdong,HangLi.HL}@huawei.com

3ADAPT Centre, School of Computing, Dublin City University
4Key Lab of Intelligent Information Processing, Institute of Computing Technology, CAS

qliu@computing.dcu.ie
Abstract

Conventional attention-based Neural Machine Translation (NMT) conducts dynamic alignment
in generating the target sentence. By repeatedly reading the representation of source sentence,
which keeps fixed after generated by the encoder (Bahdanau et al., 2015), the attention mech-
anism has greatly enhanced state-of-the-art NMT. In this paper, we propose a new attention
mechanism, called INTERACTIVE ATTENTION, which models the interaction between the de-
coder and the representation of source sentence during translation by both reading and writing
operations. INTERACTIVE ATTENTION can keep track of the interaction history and therefore
improve the translation performance. Experiments on NIST Chinese-English translation task
show that INTERACTIVE ATTENTION can achieve significant improvements over both the pre-
vious attention-based NMT baseline and some state-of-the-art variants of attention-based NMT
(i.e., coverage models (Tu et al., 2016)). And neural machine translator with our INTERACTIVE
ATTENTION can outperform the open source attention-based NMT system Groundhog by 4.22
BLEU points and the open source phrase-based system Moses by 3.94 BLEU points averagely
on multiple test sets.

1 Introduction

Neural Machine Translation (NMT) has made promising progress in recent years (Sutskever et al., 2014;
Bahdanau et al., 2015; Luong et al., 2015a; Jean et al., 2015; Luong et al., 2015b; Tang et al., 2016; Wang
et al., 2016; Li et al., 2016; Tu et al., 2016; Shen et al., 2016; Zhou et al., 2016), in which attention model
plays an increasingly important role. Attention-based NMT represents the source sentence as a sequence
of vectors after a RNN or bi-directional RNN (Schuster and Paliwal, 1997), and then simultaneously
conducts dynamic alignment with a gating neural network and generation of the target sentence with
another RNN. Usually NMT with attention model is more efficient than its attention-free counterpart: it
can achieve comparable results with far less parameters and training instances (Jean et al., 2015). This
superiority in efficiency comes mainly from the mechanism of dynamic alignment, which avoids the
need to represent the entire source sentence with a fixed-length vector (Sutskever et al., 2014).

However, conventional attention model is conducted on the representation of source sentence (fixed af-
ter generated) only with reading operation (Bahdanau et al., 2015; Luong et al., 2015a). This may let the
decoder tend to ignore past attention information, and lead to over-translation and under-translation (Tu
et al., 2016). To address this problem, Tu et al. (2016) proposed to maintain tag vectors in source rep-
resentation to keep track of the attention history, which encourages the attention-based NMT system
to consider more untranslated source words. Inspired by neural turing machines (Graves et al., 2014),
we propose INTERACTIVE ATTENTION model from the perspective of memory reading-writing, which
provides a conceptually simpler and practically more effective mechanism for attention-based NMT. The
NMT with INTERACTIVE ATTENTION is called NMTIA, which can keep track of the interaction history
with the representation of source sentence by both reading and writing operations during translation.

∗ The majority of this work was completed when the first author studied at Institute of Computing Technology, Chinese
Academy of Sciences.

This work is licensed under a Creative Commons Attribution 4.0 International Licence. Licence details:
http://creativecommons.org/licenses/by/4.0/

2174



…

…

Encoder

St-1 St St+1… …

Yt-1 Yt Yt+1
Decoder

Xj-1

hj-1

hj-1

hj

hj

hj+1

hj+1

… …Xj Xj+1

Ct Attention

at,j-1 at,j at,i+j

…

…

Figure 1: Illustration for attention-based NMT.

This interactive mechanism may be helpful for the decoder to automatically distinguish which parts have
been translated and which parts are under-translated.

We test the efficacy of NMTIA on NIST Chinese-English translation task. Experiment results show that
NMTIA can significantly outperform both the conventional attention-based NMT baseline (Bahdanau et
al., 2015) and coverage models (Tu et al., 2016). And neural machine translator with our INTERACTIVE
ATTENTION can outperform the open source attention-based NMT system Groundhog by 4.22 BLEU
points and the open source phrase-based system Moses by 3.94 BLEU points.

RoadMap: In the remainder of this paper, we will start with a brief overview of attention-based neural
machine translation in Section 2. Then in Section 3, we will detail the INTERACTIVE ATTENTION-based
NMT (NMTIA). In Section 4, we report our empirical study of NMTIA on a Chinese-English translation
task, followed by Section 5 and 6 for related work and conclusion.

2 Background

Our work is built upon the attention-based NMT (Bahdanau et al., 2015), which takes a sequence of
vector representations of the source sentence generated by a RNN or bi-directional RNN as input, and
then jointly learns to align and translate by reading the vector representations during translation with a
RNN decoder. Therefore, we take an overview of the attention-based NMT in this section before detail
the NMTIA in next section.

2.1 Attention-based Neural Machine Translation

Figure 1 shows the framework of attention-based NMT. Formally, given an input source sequence x=
{x1, x2, · · · , xN} and the previously generated target sequence y<t={y1, y2, · · · , yt−1}, the probability
of the next target word yt is

p(yt|y<t,x) = softmax(f(ct, yt−1, st)) (1)

2175



…

…

Encoder

St-1 St St+1

… …

Yt-1 Yt Yt+1
Decoder

Xj-1

hj-1

hj-1

hj

hj

hj+1

hj+1

… …Xj Xj+1

Ct Improved Attention

at,j-1 at,j at,i+j

…

…

GRU GRU GRU

Figure 2: Illustration for improved attention model of NMT.

where f(·) is a non-linear function, and st is the state of decoder RNN at time step t which is calculated
as

st = g(st−1, yt−1, ct) (2)

where g(·) can be any activation function, here we adopt a more sophisticated dynamic operator as in
Gated Recurrent Unit (GRU) (Cho et al., 2014). In the remainder of the paper, we will also use GRU to
stand for the operator. And ct is a distinct source representation for time t, calculated as a weighted sum
of the source annotations:

ct =
N∑

j=1

at,jhj (3)

Formally, hj = [
−→
hjT ,

←−
hjT ]T is the annotation of xj , which is computed by a bi-directional RNN (Schus-

ter and Paliwal, 1997) with GRU and contains information about the whole input sequence with a strong
focus on the parts surrounding xj . And its weight at,j is computed by

at,j =
exp(et,j)∑N

k=1 exp(et,k)
(4)

where et,j = vTa tanh(Wast−1 + Uahj) scores how well st−1 and hj match. This is called automatic
alignment (Bahdanau et al., 2015) or attention model (Luong et al., 2015a), but it is essentially reading
with content-based addressing defined in (Graves et al., 2014). With the attention model, it releases the
need to summarize the entire sentence with a single fixed-length vector (Sutskever et al., 2014; Cho et
al., 2014). Instead, it lets the decoding network focus on one particular segment in source sentence at
one moment, and therefore better resolution.

2.2 Improved Attention Model
The alignment model at,j scores how well the output at position t matches the inputs around position j
based on st−1 and hj . Intuitively, it should be beneficial to directly exploit the information of yt−1 when

2176



reading from the representation of source sentence, which is not implemented in the original attention-
based NMT (Bahdanau et al., 2015). As illustrated in Figure 2, we add this implementation into the
attention model, inspired by the latest implementation of attention-based NMT1. This kind of attention
model can find a more effective alignment path by using both previous hidden state st−1 and the previous
context word yt−1. Then, the calculation of e(t, j) becomes

et,j = vTa tanh(Was̃t−1 + Uahj) (5)

where s̃t−1 = GRU(st−1, eyt−1) is an intermediate state tailored for reading from the representation
of source sentence with the information of yt−1 (its word embedding being eyt−1) added. And the
calculation of update-state st becomes

st = GRU(s̃t−1, ct) (6)

3 Interactive Attention

In this section, we will elaborate on the proposed INTERACTIVE ATTENTION-based NMT, called
NMTIA. Figure 3 shows the framework of NMTIA with two rounds of interactive read-write operations
(indicated by the yellow and red arrows respectively), which adopts the same prediction model (Eq. 1)
with improved attention-based NMT. With annotations H = {h1,h2, . . . ,hN} of the source sentence
x={x1, x2, · · · , xN}, we take H as a memory, which contains N cells with the jth cell being hj . As
illustrated in Figure 3, INTERACTIVE ATTENTION in NMTIA contains two key parts at each time step t:
1) attentive reading from H, and 2) attentive writing to H. Since the content in H changes with time, we
will add time stamp on H (hence H(t)) and its cells (hence h(t)j ).

At time t, the state st−1 first meets the prediction yt−1 to form an “intermediate” state s̃t−1, which can
be calculated as follows

s̃t−1 = GRU(st−1, eyt−1) (7)

where eyt−1 is the word-embedding associated with the previous prediction word yt−1. This “intermedi-
ate” state s̃t−1 is used to read the source memory H(t−1)

ct = Read(s̃t−1,H(t−1)) (8)

After that, s̃t−1 is combined with ct to update the new state

st = GRU(s̃t−1, ct) (9)

Finally, the new state st is used to update the source memory by writing to it to finish the interaction in
a round of state-update

H(t) = Write(st,H(t−1)) (10)

The details of Read and Write in Eq. 8 and 10 will be described later in next section.
From the whole framework of NMTIA, we can see that the new attention mechanism can timely update

the representation of source sentence along with the update-chain of the decoder RNN state. This may
let the decoder keep track of the attention history during translation. Clearly, INTERACTIVE ATTENTION
can subsume the coverage models in (Tu et al., 2016) as special cases while conceptually simpler. More-
over, with the attentive writing, INTERACTIVE ATTENTION potentially can modify and add more on the
source representation than just history of attention, and is therefore a more powerful model for machine
translation, as empirically verified in Section 4.

1https://github.com/nyu-dl/dl4mt-tutorial/tree/master/session2

2177



Ht-1

St-1 St St+1

read read

Yt-1 Yt Yt+1

Decoder

write write

H0 Ht Ht+1 …

… …

Encoder

Xj-1

hj-1

hj-1

hj

hj

hj+1

hj+1

… …Xj Xj+1

…

GRU

… …

GRU GRU

… …

Figure 3: Illustration for the NMTIA. The yellow and red arrows indicate two rounds of interactive
read-write operations.

3.1 Read and Write of Interactive Attention
Attentive Read Formally, H(t′) ∈ Rn×m is the memory in time t′ after the decoder RNN state update,
where n is the number of memory cells and m is the dimension of vector in each cell. Before the state s
update at time t, the output of reading ct is given by

ct =
n∑

j=1

wRt (j)h
(t−1)
j (11)

where wRt ∈ Rn specifies the normalized weights assigned to the cells in H(t−1). We can use content-
based addressing to determine wRt as described in (Graves et al., 2014) or (quite similarly) use the reading
mechanism such as the attention model in Section 2. In this paper, we adopt the latter one.2

Attentive Write Inspired by the writing operation of neural turing machines (Graves et al., 2014), we
define two types of operation on writing to the memory: FORGET and UPDATE. FORGET is similar
to the forget gate in GRU, which determines the content to be removed from memory cells. More
specifically, the vector Ft ∈ Rm specifies the values to be forgotten or removed on each dimension
in memory cells, which is then assigned to each cell through normalized weights wWt . Formally, the
memory (“intermediate”) after FORGET operation is given by

h̃(t)i = h
(t−1)
i (1−wWt (i) · Ft), i = 1, 2, · · · , n (12)

where

• Ft = σ(WF , st) is parameterized with WF ∈ Rm×m, and σ stands for the Sigmoid activation
function;

• wWt ∈ Rn specifies the normalized weights assigned to the cells in H(t), and wWt (i) specifies the
weight associated with the ith cell in the same parametric form as wRt .

2 Wang et al. (2016) verified the former one for the read operation on the external memory.

2178



UPDATE is similar to the update gate in GRU, deciding how much current information should be
written to the memory as the added content

h(t)i = h̃
(t)
i + w

W
t (i) ·Ut, i = 1, 2, · · · , n (13)

where Ut = σ(WU , st) is parameterized with WU ∈ Rm×m, and Ut ∈ Rm. In our experiments, the
weights for reading (i.e., wRt ) and writing (i.e., w

W
t ) at time t are shared when conducting interaction

with the source memory.

3.2 Optimization
The parameters to be optimized include the embedding of words on source and target languages, the pa-
rameters for the encoder, the decoder and other operations of NMTIA. The optimization is conducted via
the standard back-propagation (BP) aiming to maximize the likelihood of the target sequence. In prac-
tice, we use the standard stochastic gradient descent (SGD) and mini-batch with learning rate controlled
by AdaDelta (Zeiler, 2012).

4 Experiments

We report our empirical study of NMTIA on Chinese-to-English translation task in this section. The
experiments are designed to answer the following questions:

• Can NMTIA achieve significant improvements over the conventional attention-based NMT?
• Can NMTIA outperform the attention-based NMT with coverage model (Tu et al., 2016)?

4.1 Data and Metric
Our training data consist of 1.25M sentence pairs extracted from LDC corpora3, with 27.9M Chinese
words and 34.5M English words respectively. We choose NIST 2002 (MT02) dataset as our development
set, which is used to monitor the training process and decide the early stop condition. And the NIST
2003 (MT03), 2004 (MT04), 2005 (MT05), 2006 (MT06) datasets are used as our test sets. The numbers
of sentences in NIST MT02, MT03, MT04, MT05 and MT06 are 878, 919, 1788, 1082, and 1664
respectively. We use the case-insensitive 4-gram NIST BLEU4 as our evaluation metric, with statistical
significance test (sign-test (Collins et al., 2005)) between the proposed models and the baselines.

4.2 Training Details
In training the neural networks, we limit the source and target vocabulary to the most frequent 30K words
for both Chinese and English, covering approximately 97.7% and 99.3% of two corpus respectively.
All the out-of-vocabulary words are mapped to a special token UNK. We initialize the recurrent weight
matrices as random orthogonal matrices. All the bias vectors are initialized to zero. For other parameters,
we initialize them by sampling each element from the Gaussian distribution of mean 0 and variance
0.012. The parameters are updated by SGD and mini-batch (size 80) with learning rate controlled by
AdaDelta (Zeiler, 2012) (� = 1e−6 and ρ = 0.95). We train the NMT systems with the sentences of
length up to 50 words in training data, and set the dimension of word embedding to 620 and the size
of the hidden layer to 1000, following the settings in (Bahdanau et al., 2015). We also use dropout for
our baseline NMT systems and NMTIA to avoid over-fitting (Hinton et al., 2012). In our experiments,
dropout was applied on the output layer with dropout rate setting to 0.5.

Inspired by the effort on easing the training of very deep architectures (Hinton and Salakhutdinov,
2006), we use a simple pre-training strategy to train our NMTIA. First we train a regular attention-based
NMT model (Bahdanau et al., 2015). Then we use the trained NMT model to initialize the parameters of
NMTIA except for those related to the operations of INTERACTIVE ATTENTION. After that, we fine-tune
all the parameters of NMTIA.

3The corpora include LDC2002E18, LDC2003E07, LDC2003E14, Hansards portion of LDC2004T07, LDC2004T08 and
LDC2005T06.

4ftp://jaguar.ncsl.nist.gov/mt/resources/mteval-v11b.pl

2179



SYSTEMS MT03 MT04 MT05 MT06 AVERAGE
Moses 31.61 33.48 30.75 31.07 31.73
Groundhog 30.96 33.09 30.61 31.12 31.45
RNNsearch? 33.42 36.04 33.60 32.24 33.83
NMTIA 35.09* 37.73* 35.53* 34.32* 35.67

Table 1: BLEU-4 scores (%) of the phrase-based SMT system (Moses), NMT baselines: Groundhog
and RNNsearch? (our implementation of improved attention model as described in Section 2.2), and our
INTERACTIVE ATTENTION model (NMTIA). The “*” indicates that the results are significantly (p<0.01)
better than those of all the baseline systems.

4.3 Comparison Systems
We compare our NMTIA with four systems:

• Moses (Koehn et al., 2007): an open source phrase-based translation system5 with default con-
figuration. The word alignments are obtained with GIZA++ (Och and Ney, 2003) on the training
corpora in both directions, using the “grow-diag-final-and” balance strategy (Koehn et al., 2003).
The 4-gram language model with modified Kneser-Ney smoothing is trained on the target portion
of training data with the SRILM toolkit (Stolcke and others, 2002),

• Groundhog: an open source NMT system6 implemented with the conventional attention mod-
el (Bahdanau et al., 2015).

• RNNsearch?: our in-house implementation of NMT system with the improved conventional atten-
tion model as described in Section 2.2.

• Coverage Model: state-of-the-art variants of attention-based NMT model (Tu et al., 2016) which
improve the attention mechanism through modeling a soft coverage on the source representation by
maintain a coverage vector to keep track of the attention history during translation.

4.4 Main Results
The main results of different models are given in Table 1. Before proceeding to more detailed compar-
isons, we first observe that

• RNNsearch? outperforms Groundhog, which is implemented with the conventional attention model
as described in Section 2.1, by 2.38 BLEU points averagely on four test sets;

• RNNsearch? only exploit sentences of length up to 50 words with 30K vocabulary, but can achieve
averagely 2.10 BLEU points higher than the open source phrase-based system Moses, which is
trained with full training data.

Clearly from Table 1, NMTIA can achieve significant improvements over RNNsearch? by 1.84 BLEU
points averagely on four test sets. We conjecture it is because our INTERACTIVE ATTENTION mechanism
can keep track of the interaction history between the decoder and the representation of source sentence
during translation, which may be helpful for the decoder to automatically distinguish which parts have
been translated and which parts are under-translated.

4.5 INTERACTIVE ATTENTION Vs. Coverage Model
Tu et al. (2016) proposed two coverage models to let the NMT system to consider more about untranslat-
ed source words. Basically, they maintain a coverage vector for each hidden state for source to keep track
of the attention history and feed the coverage vector to the attention model to help adjust future attention.

5http://www.statmt.org/moses/
6https://github.com/lisa-groundhog/GroundHog

2180



 24

 27

 30

 33

 36

0 10 20 30 40 50 60

B
L

E
U

(%
)

sentence length (Merge)

RNNSearch*-80
NN-Cover-80

NMTIA-80

Figure 4: The BLEU-4 scores (%) of generated translations on the merged four test sets with respect to
the lengths of source sentences. The numbers on X-axis of the figure stand for sentences longer than the
corresponding length, e.g., 40 for source sentences with > 40 words.

SYSTEMS MT03 MT04 MT05 MT06 AVERAGE
RNNsearch?-80 33.34 37.10 33.38 33.70 34.38
NN-Cover-80 33.69 38.05 35.01 34.83 35.40
NMTIA-80 35.69*+ 39.24*+ 35.74*+ 35.10* 36.44

Table 2: BLEU-4 scores (%) of the conventional attention-based model (RNNsearch?-80), the neural
network based coverage model (NN-Cover-80) (Tu et al., 2016) and our INTERACTIVE ATTENTION
model (NMTIA-80). “-80” means the models are trained with the sentences of length up to 80 words,
which is consistent with the setting in (Tu et al., 2016). The “*” and “+” denote that the results are
significantly (p<0.01) better than those of RNNsearch?-80 and NN-Cover-80 respectively.

Although we do not maintain a coverage vector, our INTERACTIVE ATTENTION can potentially do simi-
lar things, therefore subsuming coverage models as special cases. We hence compare our INTERACTIVE
ATTENTION model with the coverage model in (Tu et al., 2016). There are two coverage models pro-
posed in (Tu et al., 2016), including linguistic coverage model and neural network based coverage model
(NN-Cover). Since the neural network based coverage model generally yields better results, we mainly
compare with the neural network based coverage model. Although the coverage models are originally
implemented on Groundhog in (Tu et al., 2016), they can be easily adapted to the “RNNsearch?”. Fol-
lowing the setting in (Tu et al., 2016), we conduct the comparison with the training sentences of length
up to 80 words. Clearly from Table 2, our NMTIA-80 outperforms the NN-Cover-80 by +1.04 BLEU
scores averagely on four test sets.

A more detailed comparison between conventional attention model (RNNsearch?-80), neural network
based coverage model (NN-Cover-80) (Tu et al., 2016) and NMTIA-80 suggests that our NMTIA-80 is
quite consistent on outperforming the conventional attention model and the coverage model. Figure 4
shows the BLEU scores of generated translations on the test sets with respect to the length of the source
sentences. In particular, we test the BLEU scores on sentences longer than {0, 10, 20, 30, 40, 50, 60}
in the merged test set of MT03, MT04, MT05 and MT06. Clearly, on sentences with different length,
NMTIA-80 always yields consistently higher BLEU scores than the conventional attention-based NMT
and the enhanced version with the neural network based coverage model. We conjecture that with the
attentive writing (described in Section 3.1), INTERACTIVE ATTENTION potentially can modify and add
more on the source representation than just history of attention, and is therefore a more powerful model
for machine translation.

2181



We also provide some actual translation examples (see Appendix) to show that our INTERACTIVE
ATTENTION can get better performance then baselines, especially on solving under-translation problem.
We think the interactive mechanism of NMTIA is helpful for the decoder to automatically distinguish
which parts have been translated and which parts are under-translated.

5 Related Work

Our work is related to recent works that focus on improving attention models (Luong et al., 2015a; Cohn
et al., 2016; Feng et al., 2016). Luong et al. (2015a) proposed to use global and local attention models
to improve translation performance. They use a global one to attend to all source words and a local one
to look at a subset of source words at a time. Cohn et al. (2016) extended the attention-based NMT
to include structural biases from word-based alignment models, which achieved improvements across
several language pairs. Feng et al. (2016) added implicit distortion and fertility models to attention-
based NMT to achieve translation improvements. These works are different with our INTERACTIVE
ATTENTION approach, as we use a rather generic attentive reading while at the same time performing
attentive writing.

Our work is inspired by recent efforts on attaching an external memory to neural networks, such
as neural turing machines (Graves et al., 2014), memory networks (Weston et al., 2014; Meng et al.,
2015) and exploiting an external memory (Tang et al., 2016; Wang et al., 2016) during translation. Tang
et al. (2016) exploited a phrase memory for NMT, which stores phrase pairs in symbolic form. They
let the decoder utilize a mixture of word-generating and phrase-generating component, to generate a
sequence of multiple words all at once. Wang et al. (2016) extended the NMT decoder by maintaining an
external memory, which is operated by reading and writing operations of neural turing machines (Graves
et al., 2014), while keeping a read-only copy of the original source annotations along side the “read-
write” memory. These powerful extensions have been verified on Chinese-English translation tasks. Our
INTERACTIVE ATTENTION is different from previous works. We take the annotations of source sentence
as a memory instead of using an external memory, and we design a mechanism to directly read from and
write to it during translation. Therefore, the original source annotations are not accessible in later steps.
More specially, our model inherited the notation and some simple operations for writing from (Graves
et al., 2014), while NMTIA extends it to “unbounded” memory for representing the source. In addition,
although the read-write operations in INTERACTIVE ATTENTION are not exactly the same with those
in (Graves et al., 2014; Wang et al., 2016), our model can also achieve good performance.

6 Conclusion

We propose a simple yet effective INTERACTIVE ATTENTION approach, which models the interaction
between the decoder and the representation of source sentence during translation by using reading and
writing operations. Our empirical study on Chinese-English translation shows that INTERACTIVE AT-
TENTION can significantly improve the performance of NMT.

Acknowledgements

Liu is partially supported by the Science Foundation Ireland (Grant 13/RC/2106) as part of the ADAP-
T Centre at Dublin City University. We sincerely thank the anonymous reviewers for their thorough
reviewing and valuable suggestions.

References
Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. 2015. Neural machine translation by jointly learning to

align and translate. In Proceedings of ICLR.

Kyunghyun Cho, Bart van Merrienboer, Caglar Gulcehre, Fethi Bougares, Holger Schwenk, and Yoshua Ben-
gio. 2014. Learning phrase representations using rnn encoder-decoder for statistical machine translation. In
Proceedings of EMNLP, pages 1724–1734.

2182



Trevor Cohn, Cong Duy Vu Hoang, Ekaterina Vymolova, Kaisheng Yao, Chris Dyer, and Gholamreza Haffari.
2016. Incorporating structural alignment biases into an attentional neural translation model. In Proceedings of
the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human
Language Technologies, pages 876–885, San Diego, California, June.

Michael Collins, Philipp Koehn, and Ivona Kučerová. 2005. Clause restructuring for statistical machine transla-
tion. In Proceedings of ACL, pages 531–540.

Shi Feng, Shujie Liu, Mu Li, and Ming Zhou. 2016. Implicit distortion and fertility models for attention-based
encoder-decoder NMT model. CoRR, abs/1601.03317.

Alex Graves, Greg Wayne, and Ivo Danihelka. 2014. Neural turing machines. arXiv preprint arXiv:1410.5401.

Geoffrey E Hinton and Ruslan R Salakhutdinov. 2006. Reducing the dimensionality of data with neural networks.
Science, 313(5786):504–507.

Geoffrey E Hinton, Nitish Srivastava, Alex Krizhevsky, Ilya Sutskever, and Ruslan R Salakhutdinov. 2012. Im-
proving neural networks by preventing co-adaptation of feature detectors. arXiv preprint arXiv:1207.0580.

Sébastien Jean, Kyunghyun Cho, Roland Memisevic, and Yoshua Bengio. 2015. On using very large target
vocabulary for neural machine translation. In Proceedings of the 53rd Annual Meeting of the Association for
Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Volume
1: Long Papers), pages 1–10, Beijing, China, July.

Philipp Koehn, Franz Josef Och, and Daniel Marcu. 2003. Statistical phrase-based translation. In Proceedings of
NAACL, pages 48–54.

Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris Callison-Burch, Marcello Federico, Nicola Bertoldi, Brooke
Cowan, Wade Shen, Christine Moran, Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra Constantin, and
Evan Herbst. 2007. Moses: Open source toolkit for statistical machine translation. In Proceedings of ACL on
interactive poster and demonstration sessions, pages 177–180, Prague, Czech Republic, June.

Xiaoqing Li, Jiajun Zhang, and Chengqing Zong. 2016. Towards zero unknown word in neural machine transla-
tion. In Proceedings of IJCAI.

Thang Luong, Hieu Pham, and Christopher D. Manning. 2015a. Effective approaches to attention-based neu-
ral machine translation. In Proceedings of the 2015 Conference on Empirical Methods in Natural Language
Processing, pages 1412–1421.

Thang Luong, Ilya Sutskever, Quoc Le, Oriol Vinyals, and Wojciech Zaremba. 2015b. Addressing the rare
word problem in neural machine translation. In Proceedings of the 53rd Annual Meeting of the Association for
Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Volume
1: Long Papers), pages 11–19, Beijing, China, July.

Fandong Meng, Zhengdong Lu, Zhaopeng Tu, Hang Li, and Qun Liu. 2015. Neural transformation machine: A
new architecture for sequence-to-sequence learning. CoRR, abs/1506.06442.

Franz Josef Och and Hermann Ney. 2003. A systematic comparison of various statistical alignment models.
Computational linguistics, 29(1):19–51.

Mike Schuster and Kuldip K Paliwal. 1997. Bidirectional recurrent neural networks. Signal Processing, IEEE
Transactions on, 45(11):2673–2681.

Shiqi Shen, Yong Cheng, Zhongjun He, Wei He, Hua Wu, Maosong Sun, and Yang Liu. 2016. Minimum risk
training for neural machine translation. In Proceedings of ACL, pages 1683–1692, Berlin, Germany, August.

Andreas Stolcke et al. 2002. SRILM-an extensible language modeling toolkit. In Proceedings of ICSLP, volume 2,
pages 901–904.

Ilya Sutskever, Oriol Vinyals, and Quoc VV Le. 2014. Sequence to sequence learning with neural networks. In
Advances in Neural Information Processing Systems, pages 3104–3112.

Yaohua Tang, Fandong Meng, Zhengdong Lu, Hang Li, and Philip L. H. Yu. 2016. Neural machine translation
with external phrase memory. CoRR, abs/1606.01792.

Zhaopeng Tu, Zhengdong Lu, Yang Liu, Xiaohua Liu, and Hang Li. 2016. Modeling coverage for neural machine
translation. In Proceedings of ACL, pages 76–85, Berlin, Germany, August. Association for Computational
Linguistics.

2183



Mingxuan Wang, Zhengdong Lu, Hang Li, and Qun Liu. 2016. Memory-enhanced decoder for neural machine
translation. In Proceedings of EMNLP.

Jason Weston, Sumit Chopra, and Antoine Bordes. 2014. Memory networks. CoRR, abs/1410.3916.

Matthew D Zeiler. 2012. Adadelta: an adaptive learning rate method. arXiv preprint arXiv:1212.5701.

Jie Zhou, Ying Cao, Xuguang Wang, Peng Li, and Wei Xu. 2016. Deep recurrent models with fast-forward
connections for neural machine translation. CoRR, abs/1606.04199.

2184



APPENDIX: Actual Translation Examples

In appendix we give some example translations from RNNsearch?-80, NN-Cover-80 and NMTIA-80, and
compare them against the reference. We highlight some correct translation segments (or under-translated
by baseline systems) in blue color and wrong ones in red color.

Example Translations

src 
!"#$%#&#'#()#*+#,#-#./#01#2#34#56# 7# 89#1:#

;<#=>#?#@#A#B6#CDE#F#

ref 
North Korea said the nuclear stalemate is a bilateral topic of discussion with United 
States only.  The interference of other countries will only complicate the issue. 

RNNsearch
!
-80 

north korea claimed that the nuclear stalemate was only involved in bilateral issues in 

the united states , and other countries will find it more complicated . 

NN-Cover-80 
the north korea said that it had only involved bilateral talks in the united states and 
other countries would interfere with the issue . 

NMTIA -80 
north korea claimed that this nuclear stalemate was only related to the us bilateral 

agenda , and interference in other countries could only complicate the problem . 
!

src 
GH#IJ#KL#<M#NO#P#Q#7#RS1#TU#VW@#2#X#Y#Z[#V

W1#\#]#^#_#`a#IJ#>bc#de#<M#F#

ref 
Four days after Pyongyang made the above move, five permanent members of the UN 

Security Council have all taken preventive diplomatic actions on this crisis. 

RNNsearch
!
-80 

pyongyang has taken these actions four days ago , and the five permanent members of 
the un security council have taken precautions against this crisis . 

NN-Cover-80 
in a four - day operation , the five permanent members of the un security council have 

taken preventive diplomatic actions for the crisis . 

NMTIA -80 
in the four days after pyongyang took the above action , the five permanent members 
of the un security council have taken preventive diplomatic actions for this crisis . 

!

src 
fgh#ij#kl#mn#o#l#p#qO#r#sC#tu#vw#_#W#x#fy#

z<#{|#}w#F#

ref 
The Philippine government originally planned to hold preliminary discussions with 
the Philippine communists on the resumption of formal peace talks later this month. 

RNNsearch
!
-80 

the philippine government originally planned to hold a preliminary meeting with 

<UNK> on friday . 

NN-Cover-80 
the philippine government plans to resume formal peace talks with <UNK> later this 

month . 

NMTIA -80 
the philippine government originally planned to hold a preliminary discussion on the 

resumption of formal peace talks later this month . 
!

src 
9#~####2#7####2#_##W####T

U##1E#7##_##W####ij#e#RS1#F##

ref 

He said: "Obviously, the first thing we need to do is to internationalize the security 

force in Iraq.  The other thing is to turn the transitional government over to the United 
Nations." 

RNNsearch
!
-80 

he said : " obviously , the first thing we need to do is to <UNK> iraqi security forces 

to the united nations . " 

NN-Cover-80 
he said : " obviously , we need the first thing to internationalize the security forces in 
iraq , and another thing is to hand over the transitional government to the united 

nations . " 

NMTIA -80 
he said : " obviously , the first thing we need to do is to internationalize the security 
forces in iraq , and the other is to send the transitional government to the united 

nations . " 

!

!

2185


