



















































Recurrent Attention Network on Memory for Aspect Sentiment Analysis


Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 452–461
Copenhagen, Denmark, September 7–11, 2017. c©2017 Association for Computational Linguistics

Recurrent Attention Network on Memory for Aspect Sentiment Analysis

Peng Chen Zhongqian Sun Lidong Bing∗ Wei Yang
AI Lab

Tencent Inc.
{patchen, sallensun, lyndonbing, willyang}@tencent.com

Abstract

We propose a novel framework based on
neural networks to identify the sentiment
of opinion targets in a comment/review.
Our framework adopts multiple-attention
mechanism to capture sentiment features
separated by a long distance, so that it
is more robust against irrelevant informa-
tion. The results of multiple attentions
are non-linearly combined with a recur-
rent neural network, which strengthens the
expressive power of our model for han-
dling more complications. The weighted-
memory mechanism not only helps us
avoid the labor-intensive feature engineer-
ing work, but also provides a tailor-made
memory for different opinion targets of a
sentence. We examine the merit of our
model on four datasets: two are from Se-
mEval2014, i.e. reviews of restaurants and
laptops; a twitter dataset, for testing its
performance on social media data; and a
Chinese news comment dataset, for testing
its language sensitivity. The experimental
results show that our model consistently
outperforms the state-of-the-art methods
on different types of data.

1 Introduction

The goal of aspect sentiment analysis is to iden-
tify the sentiment polarity (i.e., negative, neutral,
or positive) of a specific opinion target expressed
in a comment/review by a reviewer. For exam-
ple, in “I bought a mobile phone, its camera is
wonderful but the battery life is short”, there are
three opinion targets, “camera”, “battery life”, and
“mobile phone”. The reviewer has a positive senti-
ment on the “camera”, a negative sentiment on the

∗Corresponding author.

“battery life”, and a mixed sentiment on the “mo-
bile phone”. Sentence-oriented sentiment analysis
methods (Socher et al., 2011; Appel et al., 2016)
are not capable to capture such fine-grained senti-
ments on opinion targets.

In order to identify the sentiment of an individ-
ual opinion target, one critical task is to model ap-
propriate context features for the target in its orig-
inal sentence. In simple cases, the sentiment of
a target is identifiable with a syntactically nearby
opinion word, e.g. “wonderful” for “camera”.
However, there are many cases in which opinion
words are enclosed in more complicated contexts.
E.g., “Its camera is not wonderful enough” might
express a neutral sentiment on “camera”, but not
negative. Such complications usually hinder con-
ventional approaches to aspect sentiment analysis.

To model the sentiment of the above phrase-
like word sequence (i.e. “not wonderful enough”),
LSTM-based methods are proposed, such as target
dependent LSTM (TD-LSTM) (Tang et al., 2015).
TD-LSTM might suffer from the problem that af-
ter it captures a sentiment feature far from the
target, it needs to propagate the feature word by
word to the target, in which case it’s likely to lose
this feature, such as the feature “cost-effective”
for “the phone” in “My overall feeling is that the
phone, after using it for three months and consid-
ering its price, is really cost-effective”.1 Attention
mechanism, which has been successfully used in
machine translation (Bahdanau et al., 2014), can
enforce a model to pay more attention to the im-
portant part of a sentence. There are already some
works using attention in sentiment analysis to ex-
ploit this advantage (Wang et al., 2016; Tang et al.,
2016). Another observation is that some types of

1 Although LSTM could keep information for a long dis-
tance by preventing the vanishing gradient problem, it usually
requires a large training corpus to capture the flexible usage
of parenthesis.

452



sentence structures are particularly challenging for
target sentiment analysis. For example, in “Ex-
cept Patrick, all other actors don’t play well”, the
word “except” and the phrase “don’t play well”
produce a positive sentiment on “Patrick”. It’s
hard to synthesize these features just by LSTM,
since their positions are dispersed. Single atten-
tion based methods (e.g. (Wang et al., 2016)) are
also not capable to overcome such difficulty, be-
cause attending multiple words with one attention
may hide the characteristic of each attended word.

In this paper, we propose a novel framework
to solve the above problems in target sentiment
analysis. Specifically, our framework first adopts
a bidirectional LSTM (BLSTM) to produce the
memory (i.e. the states of time steps generated
by LSTM) from the input, as bidirectional recur-
rent neural networks (RNNs) were found effec-
tive for a similar purpose in machine translation
(Bahdanau et al., 2014). The memory slices are
then weighted according to their relative positions
to the target, so that different targets from the
same sentence have their own tailor-made mem-
ories. After that, we pay multiple attentions on the
position-weighted memory and nonlinearly com-
bine the attention results with a recurrent network,
i.e. GRUs. Finally, we apply softmax on the out-
put of the GRU network to predict the sentiment
on the target.

Our framework introduces a novel way of ap-
plying multiple-attention mechanism to synthesize
important features in difficult sentence structures.
It’s sort of analogous to the cognition procedure
of a person, who might first notice part of the
important information at the beginning, then no-
tices more as she reads through, and finally com-
bines the information from multiple attentions to
draw a conclusion. For the above sentence, our
model may attend the word “except” first, and
then attends the phrase “don’t play well”, finally
combines them to generate a positive feature for
“Patrick”. Tang et al. (2016) also adopted the idea
of multiple attentions, but they used the result of
a previous attention to help the next attention at-
tend more accurate information. Their vector fed
to softmax for classification is only from the final
attention, which is essentially a linear combination
of input embeddings (they did not have a memory
component). Thus, the above limitation of single
attention based methods also holds for (Tang et al.,
2016). In contrast, our model combines the results

of multiple attentions with a GRU network, which
has different behaviors inherited from RNNs, such
as forgetting, maintaining, and non-linearly trans-
forming, and thus allows a better prediction accu-
racy.

We evaluate our approach on four datasets: the
first two come from SemEval 2014 (Pontiki et al.,
2014), containing reviews of restaurant domain
and laptop domain; the third one is a collection of
tweets, collected by (Dong et al., 2014); to exam-
ine whether our framework is language-insensitive
(since languages show differences in quite a few
aspects in expressing sentiments), we prepared a
dataset of Chinese news comments with people
mentions as opinion targets. The experimental re-
sults show that our model performs well for differ-
ent types of data, and consistently outperforms the
state-of-the-art methods.

2 Related Work

The task of aspect sentiment classification belongs
to entity-level sentiment analysis. Conventional
representative methods for this task include rule-
based methods (Ding et al., 2008) and statistic-
based methods (Jiang et al., 2011; Zhao et al.,
2010). Ganapathibhotla and Liu (2008) extracted
2-tuples of (opinion target, opinion word) from
comments and then identified the sentiment of
opinion targets. Deng and Wiebe (2015) adopted
Probabilistic Soft Logic to handle the task. There
are also statistic-based approaches which employ
SVM (Jiang et al., 2011) or MaxEnt-LDA (Zhao
et al., 2010). These methods need either labo-
rious feature engineering work or massive extra-
linguistic resources.

Neural Networks (NNs) have the capability of
fusing original features to generate new represen-
tations through multiple hidden layers. Recursive
NN (Rec-NN) can conduct semantic compositions
on tree structures, which has been used for syntac-
tic analysis (Socher et al., 2010) and sentence sen-
timent analysis (Socher et al., 2013). (Dong et al.,
2014; Nguyen and Shirai, 2015) adopted Rec-NN
for aspect sentiment classification, by converting
the opinion target as the tree root and propagating
the sentiment of targets depending on the context
and syntactic relationships between them. How-
ever, Rec-NN needs dependency parsing which
is likely ineffective on nonstandard texts such as
news comments and tweets. (Chen et al., 2016)
employed Convolution NNs to identify the senti-

453



s1 si st sj sT... ... ... ...

Embedding

target

Memory

Forward 
LSTM

Backward 
LSTM

Location
Weighted
Memory

Attention
Layer

Attention
Layer

Episode 0

Episode 1

Episode n

Recurrent Attention On Memory

Aspect
Sentiment

Softmax

...

i1AL

i2AL

e0
e0

e1
e1

en

M

Figure 1: Model architecture. The dotted lines on the right indicate a layer may or may not be added.

ment of a clause which is then used to infer the
sentiment of the target. The method has an as-
sumption that an opinion word and its target lie
in the same clause. TD-LSTM (Tang et al., 2015)
utilizes LSTM to model the context information
of a target by placing the target in the middle and
propagating the state word by word from the be-
ginning and the tail to the target respectively to
capture the information before and after it. Never-
theless, TD-LSTM might not work well when the
opinion word is far from the target, because the
captured feature is likely to be lost ((Cho et al.,
2014) reported similar problems of LSTM-based
models in machine translation).

(Graves et al., 2014) introduced the concept of
memory for NNs and proposed a differentiable
process to read and write memory, which is called
Neural Turing Machine (NTM). Attention mech-
anism, which has been used successfully in many
areas (Bahdanau et al., 2014; Rush et al., 2015),
can be treated as a simplified version of NTM be-
cause the size of memory is unlimited and we only
need to read from it. Single attention or multiple
attentions were applied in aspect sentiment clas-
sification in some previous works (Wang et al.,
2016; Tang et al., 2016). One difference between
our method and (Tang et al., 2016) is that we in-
troduce a memory module between the attention
module and the input module, thus our method
can synthesize features of word sequences such as

sentiment phrases (e.g. “not wonderful enough”).
More importantly, we combine the results of at-
tentions in a nonlinear way. (Wang et al., 2016)
only uses one attention, while our model uses mul-
tiple attentions. The effectiveness of multiple at-
tentions was also investigated in QA task (Kumar
et al., 2015), which shows that multiple attentions
allow a model to attend different parts of the input
during each pass. (Kumar et al., 2015) assigns at-
tention scores to memory slices independently and
their attention process is more complex, while we
produce a normalized attention distribution to at-
tend information from the memory.

3 Our Model

The architecture of our model is shown in Fig-
ure 1, which consists of five modules: input mod-
ule, memory module, position-weighted memory
module, recurrent attention module, and output
module. Suppose the input sentence is s =
{s1, . . . , sτ−1, sτ , sτ+1, . . . , sT }, the goal of our
model is to predict the sentiment polarity of the
target sτ . For simplicity, we notate a target as one
word here, where necessary, we will elaborate how
to handle phrase-form targets, e.g. “battery life”.

3.1 Input Embedding

Let L ∈ Rd×|V | be an embedding lookup ta-
ble generated by an unsupervised method such
as GloVe (Pennington et al., 2014) or CBOW

454



(Mikolov et al., 2013), where d is the dimension
of word vectors and |V | is the vocabulary size.
The input module retrieves the word vectors from
L for an input sequence and gets a list of vec-
tors {v1, . . . , vt, . . . , vT } where vt ∈ Rd. L may
or may not be tuned in the training of our frame-
work. If it is not tuned, the model can utilize the
words’ similarity revealed in the original embed-
ding space. If it is tuned, we expect the model
would capture some intrinsic information that is
useful for the sentiment analysis task.

3.2 BLSTM for Memory Building

MemNet (Tang et al., 2016) simply used the se-
quence of word vectors as memory, which cannot
synthesize phrase-like features in the original sen-
tence. It is straightforward to achieve the goal with
the models of RNN family. In this paper, we use
Deep Bidirectional LSTM (DBLSTM) to build the
memory which records all information to be read
in the subsequent modules.

At each time step t, the forward LSTM not only
outputs the hidden state

−→
h lt at its layer l (

−→
h 0t = vt)

but also maintains a memory −→c lt inside its hidden
cell. The update process at time t is as follows:

i = σ(
−→
W i
−→
h l−1t +

−→
U i
−→
h lt−1) (1)

f = σ(
−→
W f
−→
h l−1t +

−→
U f
−→
h lt−1) (2)

o = σ(
−→
W o
−→
h l−1t +

−→
U o
−→
h lt−1) (3)

g = tanh(
−→
W g
−→
h l−1t +

−→
U g
−→
h lt−1) (4)

−→c lt = f �−→c lt−1 + i� g (5)−→
h lt = o� tanh(−→c lt) (6)

where σ and tanh are sigmoid and hyperbolic tan-
gent functions,

−→
W i,
−→
W f ,

−→
W o,
−→
W g ∈ R

−→
d l×
−→
d l−1 ,−→

U i,
−→
U f ,
−→
U o,
−→
U g ∈ R

−→
d l×
−→
d l , and

−→
d l is the num-

ber of hidden cells at the layer l of the forward
LSTM. The gates i, f, o ∈ R

−→
d l simulate binary

switches that control whether to update the infor-
mation from the current input, whether to forget
the information in the memory cells, and whether
to reveal the information in memory cells to the
output, respectively. The backward LSTM does
the same thing, except that its input sequence
is reversed. If there are L layers stacked in
the BLSTM, the final memory generated in this
module is M∗ = {m∗1, . . . ,m∗t , . . . ,m∗T }, where
m∗t = (

−→
h Lt ,
←−
h Lt ) ∈ R

−→
d L+

←−
d L . In our framework,

we use 2 layers of BLSTM to build the memory, as

it generally performs well in NLP tasks (Karpathy
et al., 2015).

3.3 Position-Weighted Memory
The memory generated in the above module is the
same for multiple targets in one comment, which
is not flexible enough for predicting respective
sentiments of these targets. To ease this problem,
we adopt an intuitive method to edit the memory
to produce a tailor-made input memory for each
target. Specifically, the closer to the target a word
is, the higher its memory slide is weighted. We de-
fine the distance as the number of words between
the word and the target. One might want to use
the length of the path from the specific word to
the target in the dependency tree as the distance,
which is a worthwhile option to try in the future
work, given the condition that dependency parsing
on the input text is effective enough. Precisely, the
weight for the word at position t is calculated as:

wt = 1− |t− τ |
tmax

(7)

where tmax is truncation length of the input. We
also calculate ut = t−τtmax to memorize the rela-
tive offset between each word and the target. If
the target is a phrase, the distance (i.e. t − τ )
is calculated with its left or right boundary in-
dex according to which side wt locates. The
final position-weighted memory of a target is
M = {m1, . . . ,mt, . . . ,mT } where mt = (wt ·
m∗t , ut) ∈ R

−→
d L+

←−
d L+1. The weighted memory

is designed to up-weight nearer sentiment words,
and the recurrent attention module, discussed be-
low, attends long-distance sentiment words. Thus,
they work together to expect a better prediction ac-
curacy.

3.4 Recurrent Attention on Memory
To accurately predict the sentiment of a target,
it is essential to: (1) correctly distill the related
information from its position-weighted memory;
and (2) appropriately manufacture such informa-
tion as the input of sentiment classification. We
employ multiple attentions to fulfil the first aspect,
and a recurrent network for the second which non-
linearly combines the attention results with GRUs
(since GRUs have less number of parameters). For
example, “except” and “don’t play well” in “Ex-
cept Patrick, all other actors don’t play well” are
attended by different attentions, and combined to
produce a positive sentiment on “Patrick”.

455



Particularly, we employ a GRU to update the
episode e after each attention. Let et−1 denote the
episode at the previous time and iALt is the current
information attended from the memoryM , and the
process of updating et is as follows:

r = σ(WriALt + Uret−1) (8)

z = σ(WziALt + Uzet−1) (9)

ẽt = tanh(WxiALt +Wg(r � et−1)) (10)
et = (1− z)� et−1 + z � ẽt (11)

where Wr,Wz ∈ RH×(−→d L +←−d L + 1), Ur, Uz ∈
RH×H , Wg ∈ RH×(

−→
d L+

←−
d L+1), Wx ∈ RH×H ,

and H is the hidden size of GRU. As we can see
from Equations (10) and (11), the state of episode
et is the interpolation of et−1 and the candidate
hidden vector ẽt. A vector of 0’s is used as e0.

For calculating the attended information iALt at
t, the input of an attention layer (AL for short) in-
cludes the memory slices mj(1 ≤ j ≤ T ) and
the previous episode et−1. We first calculate the
attention score of each memory slice as follows:

gtj = W
AL
t (mj , et−1[, vτ ]) + b

AL
t , (12)

where [, vτ ] indicates when the attention result re-
lies on particular aspects such as those of products,
we also add the target vector vτ because different
product aspects have different preference on opin-
ion words; when the target is a person, there is no
need to do so. If the target is a phrase, vτ takes
the average of word embeddings. We utilize the
previous episode for the current attention, since it
can guide the model to attend different useful in-
formation. (Tang et al., 2016) also adopts multiple
attentions, but they don’t combine the results of
different attentions.

Then we calculate the normalized attention
score of each memory slice as:

αtj =
exp(gtj)∑
k exp(g

t
k)
. (13)

Finally, the inputs to a GRU (i.e. Eqs. 8 to 11) at
time t are the episode et−1 at time t − 1 and the
content iALt , which is read from the memory as:

iALt =
T∑
j=1

αtjmj . (14)

3.5 Output and Model Training
After N -time attentions on the memory, the final
episode eN serves as the feature and is fed into a
softmax layer to predict the target sentiment.

The model is trained by minimizing the cross
entropy plus an L2 regularization term:

L =
∑

(x,y)∈D

∑
c∈C

yc log f c(x; θ) + λ ‖ θ ‖2 (15)

where C is the sentiment category set, D is the
collection of training data, y ∈ R|C| is a one-hot
vector where the element for the true sentiment is
1, f(x; θ) is the predicted sentiment distribution
of the model, λ is the weight of L2 regularization
term. We also adopt dropout and early stopping to
ease overfitting.

4 Experiments

4.1 Experimental Setting
We conduct experiments on four datasets, as
shown in Table 1. The first two are from Se-
mEval 2014 (Pontiki et al., 2014), containing re-
views of restaurant and laptop domains, which are
widely used in previous works. The third one is
a collection of tweets, collected by (Dong et al.,
2014). The last one is prepared by us for testing
the language sensitivity of our model, which con-
tains Chinese news comments and has politicians
and entertainers as opinion targets. We purposely
add more negation, contrastive, and question com-
ments to make it more challenging. Each com-
ment is annotated by at least two annotators, and
only if they agree with each other, the comment
will be added into our dataset. Moreover, we re-
place each opinion target (i.e. word/phrase of pro-
noun or person name) with a placeholder, as did
in (Dong et al., 2014). For the first two datasets,
we removed a few examples having the “conflict
label”, e.g., “Certainly not the best sushi in New
York, however, it is always fresh” (Pontiki et al.,
2014).

We use 300-dimension word vectors pre-trained
by GloVe (Pennington et al., 2014) (whose vocab-
ulary size is 1.9M2) for our experiments on the En-
glish datasets, as previous works did (Tang et al.,
2016). Some works employed domain-specific
training corpus to learn embeddings for better per-
formance, such as TD-LSTM (Tang et al., 2015)
on the tweet dataset. In contrast, we prefer to use

2http://nlp.stanford.edu/projects/glove/

456



Dataset Negative Neutral Positive

Laptop reviews
Training 858 454 980
Testing 128 171 340

Restaurant reviews
Training 800 632 2,159
Testing 195 196 730

Tweets
Training 1,563 3,127 1,567
Testing 174 346 174

News comments
Training 6,001 8,403 5,633
Testing 838 1,054 732

Table 1: Details of the experimental datasets.

the general embeddings from (Pennington et al.,
2014) for all datasets, so that the experimental re-
sults can better reveal the model’s capability and
the figures are directly comparable across different
papers. The embeddings for Chinese experiments
are trained with a corpus of 1.4 billion tokens with
CBOW3.

4.2 Compared Methods

We compare our proposed framework of Recur-
rent Attention on Memory (RAM) with the fol-
lowing methods:
• Average Context: There are two versions of

this method. The first one, named AC-S,
averages the word vectors before the target
and the word vectors after the target sepa-
rately. The second one, named AC, averages
the word vectors of the full context.
• SVM (Kiritchenko et al., 2014): The tradi-

tional state-of-the-art method using SVMs on
surface features, lexicon features and parsing
features, which is the best team in SemEval
2014.
• Rec-NN (Dong et al., 2014): It firstly uses

rules to transform the dependency tree and
put the opinion target at the root, and then
performs semantic composition with Recur-
sive NNs for sentiment prediction.
• TD-LSTM (Tang et al., 2015): It uses a for-

ward LSTM and a backward LSTM to ab-
stract the information before and after the
target. Finally, it takes the hidden states of
LSTM at last time step to represent the con-
text for prediction. We reproduce its results
on the tweet dataset with our embeddings,
and also run it for the other three datasets.
• TD-LSTM-A: We developed TD-LSTM to

make it have one attention on the outputs of
3https://github.com/svn2github/word2vec

forward and backward LSTMs, respectively.
• MemNet (Tang et al., 2016): It applies atten-

tion multiple times on the word embeddings,
and the last attention’s output is fed to soft-
max for prediction, without combining the
results of different attentions. We produce its
results on all four datasets with the code re-
leased by the authors.4

For each method, the maximum number of train-
ing iterations is 100, and the model with the mini-
mum training error is utilized for testing. We will
discuss different settings of RAM later.

4.3 Main Results

The first evaluation metric is Accuracy, which is
used in (Tang et al., 2016). Because the datasets
have unbalanced classes as shown in Table 1,
Macro-averaged F-measure is also reported, as did
in (Dong et al., 2014; Tang et al., 2015). As shown
by the results in Table 2, our RAM consistently
outperforms all compared methods on these four
datasets. AC and AC-S perform poorly, because
averaging context is equivalent to paying identi-
cal attention to each word which would hide the
true sentiment word. Rec-NN is better than TD-
LSTM but not as good as our method. The ad-
vantage of Rec-NN is that it utilizes the result of
dependency parsing which might shorten the dis-
tance between the opinion target and the related
opinion word. However, dependency parsing is
not guaranteed to work well on irregular texts such
as tweets, which may still result in long path be-
tween the opinion word and its target, so that the
opinion features would also be lost while being
propagated. TD-LSTM performs less competitive
than our method on all the datasets, particularly
on the tweet dataset, because in this dataset sen-
timent words are usually far from person names,

4 http://ir.hit.edu.cn/∼dytang

457



Method
Laptop Restaurant Tweet Comments

Acc Macro-F1 Acc Macro-F1 Acc Macro-F1 Acc Macro-F1
AC 0.6729 0.6186 0.7504 0.6396 0.6228 0.5912 0.6231 0.6182
AC-S 0.6839 0.6217 0.7585 0.6379 0.6329 0.6009 0.6425 0.6376
SVM 0.7049* NA 0.8016* NA 0.6340\ 0.6330\ 0.6524 0.6499
Rec-NN NA NA NA NA 0.6630* 0.6590* NA NA
TD-LSTM 0.7183 0.6843 0.7800 0.6673 0.6662 0.6401 0.7275 0.7260
TD-LSTM-A 0.7214 0.6745 0.7889 0.6901 0.6647 0.6404 0.7206 0.7195
MemNet 0.7033 0.6409 0.7816 0.6583 0.6850 0.6691 0.6247 0.6117
RAM 0.7449 0.7135 0.8023 0.7080 0.6936 0.6730 0.7389 0.7385

Table 2: Main results. The results with ‘*’ are retrieved from the papers of compared methods, and those
with ‘\’ are retrieved from Rec-NN paper.

No. of AL Laptop Restaurant Tweet Comments
RAM-1AL 0.7074 0.7996 0.6864 0.7336
RAM-2AL 0.7465 0.7889 0.6922 0.7363
RAM-3AL 0.7449 0.8023 0.6936 0.7389
RAM-4AL 0.7293 0.8059 0.6879 0.7325
RAM-5AL 0.7293 0.7960 0.6864 0.7325

Table 3: The impacts of attention layers. (Word embeddings are not tuned in the training stage.)

for which case the multiple-attention mechanism
is designed to work. TD-LSTM-A also performs
worse than our method, because its two attentions,
i.e. one for the text before the target and the other
for the after, cannot tackle some cases where more
than one features being attended are at the same
side of the target.

Our method steadily performs better than Mem-
Net on all four datasets, particularly on the News
comment dataset, its improvement is more than
10%. MemNet adopts multiple attentions in or-
der to improve the attention results, given the as-
sumption that the result of an attention at a later
hop should be better than that at the beginning.
MemNet doesn’t combine the results of multiple
attentions, and the vector fed to softmax is the
result of the last attention, which is essentially
the linear combination of word embeddings. As
we described before, attending too many words
in one time may hide the characteristic of each
word, moreover, the sentiment transition usually
combines features in a nonlinear way. Our model
overcomes this shortcoming with a GRU network
to combine the results of multiple attentions. The
feature-based SVM, which needs labor-intensive
feature engineering works and a mass of extra lin-
guistic resources, doesn’t display its advantage,
because the features for aspect sentiment analy-

sis cannot be extracted as easily as for sentence
or document level sentiment analysis.

4.4 Effects of Attention Layers
One major setting that affects the performance of
our model is the number of attention layers. We
evaluate our framework with 1 to 5 attention lay-
ers, and the results are given in Table 3, where
NAL means using N attentions. In general, our
model with 2 or 3 attention layers works better,
but the advantage is not always there for different
datasets. For example, for the Restaurant dataset,
our model with 4 attention layers performs the
best. Using 1 attention is always not as good as
using more, which shows that one-time attention
might not be sufficient to capture the sentiment
features in complicated cases. One the other hand,
the performance is not monotonically increasing
with respect to the number of attentions. RAM-
4AL is generally not as good as RAM-3AL, it
is because as the model’s complexity increases,
the model becomes more difficult to train and less
generalizable.

4.5 Effects of Embedding Tuning
The compared embedding tuning strategies are:
• RAM-3AL-T-R: It does not pre-train word

embeddings, but initializes embeddings ran-
domly and then tunes them in the supervised

458



Embedding Laptop Restaurant Tweet Comment
RAM-3AL-T-R 0.5806 0.7129 0.6272 0.6749
RAM-3AL-T 0.6854 0.7522 0.6402 0.7283
RAM-3AL-NT 0.7449 0.8023 0.6936 0.7389

Table 4: The impact of different embedding tuning strategies.

(a) Example of multiple attentions. The target is “windows”.

(b) Example of single attention. The target is “windows”.

Figure 2: Comparison of single attention and multiple attentions. Attention score by Eq. 13 is used as
the color-coding.

training stage.
• RAM-3AL-T: Using the pre-trained embed-

dings initially, and they are also tuned in the
training.
• RAM-3AL-NT: The pre-trained embeddings

are not tuned in the training.

From Table 4, we can see that RAM-3AL-T-
R performs very poorly, especially when the size
of training data is smaller. The reason could
be threefold: (1) The amount of labelled sam-
ples in the four experimental datasets is too small
to tune reliable embeddings from scratch for the
in-vocabulary words (i.e. existing in the train-
ing data); (2) A lot of out-of-vocabulary (OOV)
words, i.e. absent from the training data, but exist
in the testing data; (3) It increases the risk of over-
fitting after adding the embedding parameters to
the solution space (it requires the embeddings not
only to fit model parameters, but also to capture
the similarity among words). During training, we
indeed observed that the training error converges
too fast in RAM-3AL-T-R. RAM-3AL-T can uti-
lize the embedding similarity among words at the
beginning of training, but fine tuning will destroy
this similarity during training. On the other hand,
the initial embeddings of OOV words in the test-
ing data are not tuned, so that their similarity with
vocabulary words are also destroyed. In addition,

RAM-3AL-T also suffers from the risk of overfit-
ting. RAM-3AL-NT performs the best on all four
datasets, and we also observe that the training er-
ror converges gradually while the model parame-
ters are being updated with the error signal from
the output layer.

4.6 Case Study

We pick some testing examples from the datasets
and visualize their attention results. To make the
visualized results comprehensible, we remove the
BLSTM memory module to make the attention
module directly work on the word embeddings,
thus we can check whether the attention results
conform with our intuition. The visualization re-
sults are shown in Figures 2 and 3.

Figures 2a and 2b present the differences be-
tween using two attentions and using one atten-
tion, which show that multiple attentions are use-
ful to attend correct features. As shown in Fig-
ure 2a, in order to identify the sentiment of “win-
dows”, the model firstly notices “welcomed” and
secondly notices “compared” before the aspect tar-
get “windows”. Finally it combines them with the
GRU network, and generates a negative sentiment
because the compared item (i.e. “windows”) after
a positive sentiment word (i.e. “welcomed”) is less
preferred. While the attention result of the model

459



(a) Example of a Chinese contrastive sentence, whose translation is “$T$’s quality and ability are absolutely stronger than
$PEOPLE$!!!”. The target is “$T$”.

(b) The sentence from 3a with a different target, i.e. “$PEOPLE$’s quality and ability are absolutely stronger than $T$!!!”.

Figure 3: Example of multiple opinion targets. Attention score by Eq. 13 is used as the color-coding.

with only one attention, as shown in Figure 2a, is a
sort of uniform distribution and mingles too many
word vectors in a linear way, which would ruin the
characteristic of each word.

Figures 3a and 3b present a case that there
are more than one opinion targets in a comment,
which cannot be analyzed with sentence-level sen-
timent analysis methods properly. Specifically, it’s
a comparative sentence in which the reviewer has
a positive sentiment on the first commented per-
son, but a negative sentiment on the second per-
son, and our model predicts both of them correctly.
Although all useful information (e.g. “than” and
“stronger”) is attended in both cases, the attention
procedures of them show some interesting differ-
ences. They mainly attend important information
after the target $T$ in the first attention layer AL1.
After that, Figure 3b attends more information be-
fore $T$ in AL2. Since the same words in Figures
3a and 3b have different memory slices due to po-
sition weighting and augmented offset feature, as
described in Section 3.3, our model predicts oppo-
site sentiments on the two persons. For example in
Figure 3b, the model first attends a positive word
“stronger” and then attends “than” before the tar-
get, so it reverses the sentiment and finally predicts
a negative sentiment.

5 Conclusions and Future Work

In this paper, we proposed a framework to iden-
tify the sentiment of opinion targets. The model

first runs through the input to generate a memory,
in the process of which it can synthesize the word
sequence features. And then, the model pays mul-
tiple attentions on the memory to pick up impor-
tant information to predict the final sentiment, by
combining the features from different attentions
non-linearly. We demonstrated the efficacy of our
model on four datasets, and the results show that
it can outperform the state-of-the-art methods.

Although multiple-attention mechanism has the
potential to synthesize features in complicated
sentences, enforcing the model to pay a fix number
of attentions to the memory is unnatural and even
sort of unreasonable for some cases. Therefore,
we need a mechanism to stop the attention pro-
cess automatically if no more useful information
can be read from the memory. We may also try
other memory weighting strategies to distinguish
multiple targets in one comment more clearly.

References
Orestes Appel, Francisco Chiclana, Jenny Carter, and

Hamido Fujita. 2016. A hybrid approach to the
sentiment analysis problem at the sentence level.
Knowl.-Based Syst., 108:110–124.

Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua
Bengio. 2014. Neural machine translation by
jointly learning to align and translate. CoRR,
abs/1409.0473.

Peng Chen, Bing Xu, Muyun Yang, and Sheng Li.
2016. Clause sentiment identification based on con-
volutional neural network with context embedding.

460



In Natural Computation, Fuzzy Systems and Knowl-
edge Discovery (ICNC-FSKD), 2016 12th Interna-
tional Conference on, pages 1532–1538.

KyungHyun Cho, Bart van Merrienboer, Dzmitry Bah-
danau, and Yoshua Bengio. 2014. On the properties
of neural machine translation: Encoder-decoder ap-
proaches. CoRR, abs/1409.1259.

Lingjia Deng and Janyce Wiebe. 2015. Joint prediction
for entity/event-level sentiment analysis using prob-
abilistic soft logic models. In Proceedings of the
2015 Conference on Empirical Methods in Natural
Language Processing.

Xiaowen Ding, Bing Liu, and Philip S Yu. 2008. A
holistic lexicon-based approach to opinion mining.
In Proceedings of the 2008 international conference
on web search and data mining, pages 231–240.

Li Dong, Furu Wei, Chuanqi Tan, Duyu Tang, Ming
Zhou, and Ke Xu. 2014. Adaptive recursive neural
network for target-dependent twitter sentiment clas-
sification. In Proceedings of the 52nd Annual Meet-
ing of the Association for Computational Linguistics
(Volume 2: Short Papers).

Murthy Ganapathibhotla and Bing Liu. 2008. Mining
opinions in comparative sentences. In Proceedings
of the 22nd International Conference on Computa-
tional Linguistics-Volume 1, pages 241–248.

Alex Graves, Greg Wayne, and Ivo Danihelka. 2014.
Neural turing machines. CoRR, abs/1410.5401.

Long Jiang, Mo Yu, Ming Zhou, Xiaohua Liu, and
Tiejun Zhao. 2011. Target-dependent twitter senti-
ment classification. In Proceedings of the 49th An-
nual Meeting of the Association for Computational
Linguistics: Human Language Technologies-Volume
1, pages 151–160.

Andrej Karpathy, Justin Johnson, and Fei-Fei Li. 2015.
Visualizing and understanding recurrent networks.
CoRR, abs/1506.02078.

Svetlana Kiritchenko, Xiaodan Zhu, Colin Cherry, and
Saif Mohammad. 2014. Nrc-canada-2014: Detect-
ing aspects and sentiment in customer reviews. In
Proceedings of the 8th International Workshop on
Semantic Evaluation (SemEval 2014), pages 437–
442.

Ankit Kumar, Ozan Irsoy, Jonathan Su, James Brad-
bury, Robert English, Brian Pierce, Peter Ondruska,
Ishaan Gulrajani, and Richard Socher. 2015. Ask
me anything: Dynamic memory networks for natu-
ral language processing. CoRR, abs/1506.07285.

Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey
Dean. 2013. Efficient estimation of word represen-
tations in vector space. CoRR, abs/1301.3781.

Thien Hai Nguyen and Kiyoaki Shirai. 2015.
Phrasernn: Phrase recursive neural network for
aspect-based sentiment analysis. In Proceedings of
the 2015 Conference on Empirical Methods in Nat-
ural Language Processing.

Jeffrey Pennington, Richard Socher, and Christo-
pher D. Manning. 2014. Glove: Global vectors for
word representation. In Empirical Methods in Nat-
ural Language Processing (EMNLP), pages 1532–
1543.

Maria Pontiki, Dimitris Galanis, John Pavlopoulos,
Harris Papageorgiou, Ion Androutsopoulos, and
Suresh Manandhar. 2014. Semeval-2014 task 4: As-
pect based sentiment analysis. In Proceedings of the
8th international workshop on semantic evaluation
(SemEval 2014), pages 27–35.

Alexander M. Rush, Sumit Chopra, and Jason Weston.
2015. A neural attention model for abstractive sen-
tence summarization. CoRR, abs/1509.00685.

Richard Socher, Christopher D Manning, and An-
drew Y Ng. 2010. Learning continuous phrase
representations and syntactic parsing with recursive
neural networks. In Proceedings of the NIPS-2010
Deep Learning and Unsupervised Feature Learning
Workshop, pages 1–9.

Richard Socher, Jeffrey Pennington, Eric H. Huang,
Andrew Y. Ng, and Christopher D. Manning. 2011.
Semi-supervised recursive autoencoders for predict-
ing sentiment distributions. In Proceedings of the
2011 Conference on Empirical Methods in Natural
Language Processing, EMNLP 2011, pages 151–
161.

Richard Socher, Alex Perelygin, Jean Wu, Jason
Chuang, Christopher D. Manning, Andrew Ng, and
Christopher Potts. 2013. Recursive deep models
for semantic compositionality over a sentiment tree-
bank. In Proceedings of the 2013 Conference on
Empirical Methods in Natural Language Process-
ing.

Duyu Tang, Bing Qin, Xiaocheng Feng, and Ting
Liu. 2015. Target-dependent sentiment classifi-
cation with long short term memory. CoRR,
abs/1512.01100.

Duyu Tang, Bing Qin, and Ting Liu. 2016. Aspect
level sentiment classification with deep memory net-
work. In Proceedings of the 2016 Conference on
Empirical Methods in Natural Language Process-
ing.

Yequan Wang, Minlie Huang, xiaoyan zhu, and
Li Zhao. 2016. Attention-based lstm for aspect-level
sentiment classification. In Proceedings of the 2016
Conference on Empirical Methods in Natural Lan-
guage Processing.

Wayne Xin Zhao, Jing Jiang, Hongfei Yan, and Xiaom-
ing Li. 2010. Jointly modeling aspects and opin-
ions with a maxent-lda hybrid. In Proceedings of the
2010 Conference on Empirical Methods in Natural
Language Processing, EMNLP ’10, pages 56–65.

461


