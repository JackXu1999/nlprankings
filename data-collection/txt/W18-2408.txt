



















































NEWS 2018 Whitepaper


Proceedings of the Seventh Named Entities Workshop, pages 47–54
Melbourne, Australia, July 20, 2018. c©2018 Association for Computational Linguistics

47

Whitepaper on NEWS 2018 Shared Task on Machine Transliteration

Nancy Chen†, Xiangyu Duan‡, Min Zhang‡, Rafael E. Banchsη, Haizhou Li?
†Singapore University of Technology and Design, Singapore

‡Soochow University, China 215006
ηNanyang Technological University, Singapore
?National University of Singapore, Singapore

Abstract

Transliteration is defined as the phonetic
translation of names across languages.
Transliteration of Named Entities (NEs)
is a necessary subtask in many applica-
tions, such as machine translation, cor-
pus alignment, cross-language IR, infor-
mation extraction and automatic lexicon
acquisition. All such systems call for high-
performance transliteration, which is the
focus of the shared task in NEWS 2018.

1 Task Description

The objective of the Shared Task on Named En-
tity Transliteration at NEWS 2018 is to promote
machine transliteration research by providing a
common benchmarking platform for the research
community to evaluate state-of-the-art approaches
to this problem. The task is to develop machine
transliteration and/or back-transliteration systems
in one or more of the provided language pairs.

For each language pair, training and develop-
ment data sets containing source and target name
pairs are released for participating teams to train
their systems. At the evaluation time, test sets
of source names only will be released, on which
participants are expected to produce a ranked list
of transliteration and/or back-transliteration can-
didates in the target language. The results will be
automatically evaluated by using the same metrics
used in previous editions of the shared task.

This year’s shared task focuses mainly on “stan-
dard” submissions, i.e. output results from sys-
tems that have been trained only with the data pro-
vided by the shared task organizing team. This
will ensure that all results for the same task are
comparable across the different systems. Partici-
pants may submit several “standard” runs for each
of the task they participate in. Those participants
interested in submitting “non-standard” runs, i.e.

output results from systems that use additional
data during the training phase, still will be able
to do so. However such runs will be evaluated and
reported separately.

2 Important Dates

Train/Development data release 12 March 2018
Test data release 07 May 2018
Results Submission Due 14 May 2018
Task (short) Papers Due 21 May 2018
Acceptance Notification 28 May 2018
Camera-Ready Deadline 04 June 2018
Workshop Date 20 July 2018

3 Participation

1. Registration (12 March 2018). Prospec-
tive participants are to register through the
NEWS 2018 website by requesting the
datasets from 12 March onwards.

2. Train/Development Data (12 March 2018).
Registered participants are to obtain train and
development data from the shared task regis-
tration form and/or the designated copyright
owners of databases. All registered partici-
pants are required to participate in the evalu-
ation of at least one language pair, submit the
results, prepare a short paper and attend the
workshop at ACL 2018.

3. Test Data (07 May 2018). The test data
would be released on 07 May 2018, and the
participants have a maximum of 7 days to
submit their results to the competition site.
NEWS 2018 shared task will be run on Co-
daLab. Participants need to create a codalab
account and register into the NEWS 2018
competition in order to be able to submit their
system results. Only “standard” runs will be

http://workshop.colips.org/news2018/
http://codalab.org/
http://codalab.org/


48

processed this year. According to this, par-
ticipants are required to use only the train-
ing and development data provided within the
shared task to train their systems.

Participants can submit several runs for each
individual language pair at the competition
site. However, the total number of submis-
sions per language pair will be limited to
a maximum of 3 submissions per day, with
a total maximum of 15 submissions during
the whole period of the competition. From
all submissions done to each individual lan-
guage pair, each participant must select one
to be posted on the leaderboard. Results on
the leaderboard (by the last day of the shared
task on 14 May 2018) will constitute the final
official results of the shared task.

Each submission must be saved in a file
named ”results.xml” and submitted into the
system in a ”.zip” compressed file format.
Each ”results.xml” file can contain up to 10
output candidates in a ranked list for each
corresponding input entry in the test file (re-
fer to Appendix B for more details on file for-
mating and naming conventions).

Those participants interested in submitting
“non-standard” runs, i.e. transliteration re-
sults from systems that use additional data
during the training phase, still will be able
to do so. However such runs will be evalu-
ated and reported separately (please contact
the organizers).

4. Results (14 May 2018). Leaderboard results,
as on 14 May 2018, will be considered the
official evaluation results of the NEWS 2018
shared task. These results will be published
on the workshop website and proceedings.

Note that only the scores (evaluation metrics)
of the participating systems on each language
pair will be published, and no explicit refer-
ence to the participating teams will be pro-
vided. Furthermore, all participants should
agree on not to reveal identities of other par-
ticipants in any of their publications unless
permission from the other respective partici-
pants is granted. By default, all participants
remain anonymous in published results. Par-
ticipating teams are allowed to reveal only
their own identity in their publications.

5. Shared Task Short Papers (21 May 2018).
Each participant is required to submit a 4-
page system paper (short paper) describing
their system, the used approach, submissions
and results. Peer reviews will be conducted
to improve paper quality and readability and
make sure the authors’ ideas and methods can
be understood by the workshop participants.

We are aiming at accepting all system papers,
and selected ones will be presented orally in
the workshop. All participants are required
to register and attend the workshop to present
their work. All paper submission and reviews
will be managed electronically through https:
//www.softconf.com/acl2018/NEWS/.

4 Language Pairs

The different evaluation tasks within the NEWS
2018 shared task focus on transliteration and/or
back-transliteration of personal and place names
from a source language into a target language as
summarized in Table 1. This year, the shared task
offers 19 evaluation tasks, including 9 translitera-
tion tasks, 6 back-transliteration tasks and 4 hybrid
tasks. NEWS 2018 will release training, devel-
opment and testing data for each of the language
pairs. Within the 19 evaluation tasks, NEWS 2018
includes the 14 tasks that were evaluated in the
previous year editions. In such cases, the training
and development datasets are augmented versions
of the previous year ones. New test dataset will be
used in NEWS 2018 evaluations.

The names given in the training sets for Thai
(T-EnTh & B-ThEn), Persian (T-EnPe & B-PeEn),
Chinese (T-EnCh & B-ChEn), Hebrew (T-EnHe
& B-HeEn), Vietnamese (T-EnVi), Japanese (T-
EnJa) and Korean (T-EnKo) are Western names
and their respective transliterations.

The training sets in the Persian (T-PeEn & B-
EnPe) tasks are names of Persian origin. The train-
ing set in the English to Japanese Kanji (B-JnJk)
task consists only of native Japanese names. The
training set in the Arabic to English (T-ArEn) task
consists only of native Arabic names. Finally, the
training sets for the English to Indian languages
Hindi (M-EnHi), Tamil (M-EnTa), Kannada (M-
EnKa) and Bangla (M-EnBa) tasks consist of a
mix of both Indian and Western names.

http://workshop.colips.org/news2018/contact.html
http://workshop.colips.org/news2018/contact.html
http://workshop.colips.org/news2018/
https://www.softconf.com/acl2018/NEWS/
https://www.softconf.com/acl2018/NEWS/


49

Name origin Source script Target script Type of Task Dataset Size Task IDTrain Dev Test

Western English Thai Transliteration 30781 1000 1000 T-EnTh
Western Thai English Back-transliteration 27273 1000 1000 B-ThEn

Western English Persian Transliteration 13386 1000 1000 T-EnPe
Western Persian English Back-transliteration 15677 1000 1000 B-PeEn

Western English Chinese Transliteration 41318 1000 1000 T-EnCh
Western Chinese English Back-transliteration 32002 1000 1000 B-ChEn
Western English Vietnamese Transliteration 3256 500 500 T-EnVi

Mixed English Hindi Mixed trans/back 12937 1000 1000 M-EnHi
Mixed English Tamil Mixed trans/back 10957 1000 1000 M-EnTa
Mixed English Kannada Mixed trans/back 10955 1000 1000 M-EnKa
Mixed English Bangla Mixed trans/back 13623 1000 1000 M-EnBa
Western English Hebrew Transliteration 10501 1000 1000 T-EnHe
Western Hebrew English Back-transliteration 9447 1000 1000 B-HeEn

Western English Japanese Katakana Transliteration 28828 1000 1000 T-EnJa
Japanese English Japanese Kanji Back-transliteration 10514 1000 1000 B-JnJk
Western English Korean Hangul Transliteration 7387 1000 1000 T-EnKo
Arabic Arabic English Transliteration 31354 1000 1000 T-ArEn
Persian Persian English Transliteration 6000 1000 1000 T-PeEn
Persian English Persian Back-transliteration 11204 1000 1000 B-EnPe

Table 1: Source and target languages for the shared task on transliteration.

5 Standard Datasets

Training Data (Parallel)
Paired names between source and target lan-
guages; size 3K – 41K.
Training data is used for training a basic
transliteration system.

Development Data (Parallel)
Paired names between source and target lan-
guages; size 1K (500 for T-EnVi).
Development data is in addition to the train-
ing data, which is used for fine-tuning the
system parameters, in case of need. Partici-
pants are allowed to use it as part of the train-
ing data for their final submissions.

Testing Data
Source names only; size 1K (500 for T-EnVi).
This is a held-out set, which will be used for
evaluating the quality of the transliterations.

Participants will need to obtain licenses from
the respective copyright owners of the different
datasets and/or agree to the terms and conditions
of use that are given on the downloading web-
site (Li et al., 2004; MSRI, 2010; CJKI, 2010).
NEWS 2018 will provide the contact details for
each dataset group.

The data would be provided in Unicode UTF-
8 encoding, in XML format. The results are ex-
pected to be submitted in UTF-8 encoding also in

XML format. The required XML format details
are available in the Appendix A.

Note that name pairs are distributed as-is, as
provided by the respective creators. While the
datasets are mostly manually checked, there may
be still inconsistencies (that is, non-standard us-
age, region-specific usage, errors, etc.) or incom-
pleteness (that is, not all right variations may be
covered). The participants are allowed to use any
method of their preference to further clean up the
data provided:

• For any participant conducting a manual
clean up, we appeal that such data be pro-
vided back to the organizers for redistribution
to all the participating groups in that language
pair. Such sharing benefits all participants!

• If automatic clean up were used, such clean
up will be considered part of the system im-
plementation, and hence it is not required to
be shared with all participants.

All participants are required to use only the
dataset (parallel names) provided by the shared
task organizers for training their systems. This
“standard” submission procedure will ensure a fair
evaluation in term of score comparison across the
different systems. Those participants wanting to
additionally evaluate “non-standard” runs need to
contact the organizers

http://workshop.colips.org/news2018/contact.html
http://workshop.colips.org/news2018/contact.html
http://workshop.colips.org/news2018/contact.html


50

6 Evaluation Metrics

As in previous editions of the shared task, the qual-
ity of the submitted results will be evaluated by us-
ing the following 4 metrics. Each individual name
result might include up to 10 output candidates in
a ranked list.

Since a given source name may have multiple
correct target transliterations, all these alternatives
are treated equally in the evaluation. That is, any
of these alternatives are considered as a correct
transliteration, and the first correct transliteration
in the ranked list is accepted as a correct hit.

The following notation is further assumed:
N : Total number of names (source

words) in the test set.
ni : Number of reference transliterations

for i-th name in the test set (ni ≥ 1).
ri,j : j-th reference transliteration for i-th

name in the test set.
ci,k : k-th candidate transliteration (system

output) for i-th name in the test set
(1 ≤ k ≤ 10).

Ki : Number of candidate transliterations
produced by a transliteration system.

1. Word Accuracy in Top-1 (ACC) Also
known as Word Error Rate. It measures correct-
ness of the first transliteration candidate in the can-
didate list produced by a transliteration system.
ACC = 1 means that all top candidates are cor-
rect transliterations i.e. they match one of the ref-
erences, and ACC = 0 means that none of the top
candidates are correct.

ACC =
1

N

N∑
i=1

{
1 if ∃ ri,j : ri,j = ci,1;
0 otherwise

}
(1)

2. Fuzziness in Top-1 (Mean F-score) The
mean F-score measures how different, on average,
the top transliteration candidate is from its closest
reference. F-score for each source word is a func-
tion of Precision and Recall and equals 1 when the
top candidate matches one of the references, and
0 when there are no common characters between
the candidate and any of the references.

Precision and Recall are calculated based on the
length of the Longest Common Subsequence be-
tween a candidate and a reference:

LCS(c, r) =
1

2
(|c|+ |r| − ED(c, r)) (2)

where ED is the edit distance and |x| is the length
of x. For example, the longest common subse-
quence between “abcd” and “afcde” is “acd” and
its length is 3. The best matching reference, that
is, the reference for which the edit distance has
the minimum value, is taken for calculation. If the
best matching reference is given by

ri,m = argmin
j

(ED(ci,1, ri,j)) (3)

then Recall, Precision and F-score for i-th word
are calculated as follows:

Ri =
LCS(ci,1, ri,m)

|ri,m|
(4)

Pi =
LCS(ci,1, ri,m)

|ci,1|
(5)

Fi = 2
Ri × Pi
Ri + Pi

(6)

• The length is computed in distinct Unicode
characters.

• No distinction is made among different char-
acter types of a language (e.g. vowel vs. con-
sonants vs. combining diereses etc.)

3. Mean Reciprocal Rank (MRR) Measures
traditional MRR for any right answer produced by
the system, from among the candidates. 1/MRR
tells approximately the average rank of the correct
transliteration. MRR closer to 1 implies that the
correct answer is mostly produced close to the top
of the n-best lists.

RRi =

{
minj

1
j if ∃ri,j , ci,k : ri,j = ci,k;

0 otherwise

}
(7)

MRR =
1

N

N∑
i=1

RRi (8)

4. MAPref Measures tightly the precision in the
n-best candidates for i-th source name, for which
reference transliterations are available. If all of
the references are produced, then the MAP is 1.
Let’s denote the number of correct candidates for
the i-th source word in k-best list as num(i, k).
MAPref is then given by

MAPref =
1

N

N∑
i

1

ni

(
ni∑
k=1

num(i, k)

)
(9)



51

7 Paper Format

Paper submissions to NEWS 2018 should follow
the ACL 2018 paper submission policy, includ-
ing paper format, blind review policy and title and
author conventions. Full papers (research papers)
must be in two-column format without exceeding
eight (8) pages of content plus two (2) extra pages
for references and short papers (research and
shared task papers) must also be in two-column
format without exceeding four (4) pages content
plus two (2) extra pages for references. Submis-
sion must conform to the official ACL 2018 style
guidelines. For details, please refer to the ACL
2018 website: http://acl2018.org/call-for-papers/.

8 Contact Us

If you have any questions about the share task and
the datasets, please contact any of the workshop
organizers. Contact information is available at the
NEWS 2018 website http://workshop.colips.org/
news2018/contact.html

References
[CJKI2010] CJK Institute. 2010. http://www.cjk.org/.

[Li et al.2004] Haizhou Li, Min Zhang, and Jian Su.
2004. A joint source-channel model for machine
transliteration. In Proc. 42nd ACL Annual Meeting,
pages 159–166, Barcelona, Spain.

[MSRI2010] MSRI. 2010. Microsoft Research India.
http://research.microsoft.com/india.

[AILab2018] Artificial Intelligence Laboratory (AILab)
2018. Ho Chi Minh City University of Science
(VNU-HCMUS). https://www.ailab.hcmus.edu.vn/

[Cao et al.2010] Nam X. Cao, Nhut M. Pham, Quan H.
Vu. 2010. Comparative analysis of transliteration
techniques based on statistical machine translation
and joint-sequence model. In Proc. Symposium on
Information and Comunication Technology, pages
59–63, ACM.

[Ngo et al.2015] Hoang Gia Ngo, Nancy F. Chen,
Nguyen Binh Minh, Bin Ma, Haizhou Li. 2015.
Phonology-Augmented Statistical Transliteration
for Low-Resource Languages. Interspeech, 2015.

http://acl2018.org/
http://acl2018.org/
http://acl2018.org/call-for-papers/
http://workshop.colips.org/news2018/contact.html
http://workshop.colips.org/news2018/contact.html


52

A Appendix: Data Formats

• File Naming Conventions:
NEWS18 Z-XXYY trn.xml
NEWS18 Z-XXYY dev.xml

– Z: Type of task (T: transliteration, B:
back-transliteration, M: mixed)

– XX: Source Language
– YY: Target Language

• File formats:
All data will be made available in XML for-
mats as illustrated in Figure 1.

• Data Encoding Formats:
The data will be in Unicode UTF-8 encod-
ing files without byte-order mark, and in the
XML format specified.

B Appendix: Submission of Results

• File Naming Conventions:
Each submission must be saved in a file
named ”results.xml” and submitted into the
NEWS 2018 CodaLab competition in a ”.zip”
compressed file. Each ”results.xml” file can
contain up to 10 output candidates in a ranked
list for each corresponding input entry in the
test file.

• File formats:
All data will be provided in XML formats as
illustrated in Figure 2.

• Data Encoding Formats:
The results are expected to be submitted in
UTF-8 encoded files without byte-order mark
only, and in the XML format specified.



53

<?xml version = "1.0" encoding = "UTF-8"?>

<TransliterationCorpus
CorpusFormat = "UTF-8"
CorpusID = "[task_id]"
CorpusSize = "[total_number_of_names_in_file]"
CorpusType = "[Training|Development]"
NameSource = "[name_origin]"
SourceLang = "[source_language]"
TargetLang = "[target_language]">

<Name ID="1">
<SourceName>[source_name_1]</SourceName>
<TargetName ID="1">[target_name_1_1]</TargetName>
<TargetName ID="2">[target_name_1_2]</TargetName>
...
<TargetName ID="n">[target_name_1_n]</TargetName>

</Name>

<Name ID="2">
<SourceName>[source_name_2]</SourceName>
<TargetName ID="1">[target_name_2_1]</TargetName>
<TargetName ID="2">[target_name_2_2]</TargetName>
...
<TargetName ID="k">[target_name_2_k]</TargetName>

</Name>

...
<!-- rest of the names to follow -->
...

</TransliterationCorpus>

Figure 1: Example of training and development data format.



54

<?xml version="1.0" encoding="UTF-8"?>

<TransliterationTaskResults
SourceLang = "[source_language]"
TargetLang = "[target_language]"
GroupID = "[your_institution_name]"
RunID = "[your_submission_number]"
RunType = "Standard"
Comments = "[your_comments_here]"
TaskID = "[task_id]">

<Name ID="1">
<SourceName>[test_name_1]</SourceName>
<TargetName ID="1">[your_system_result_1_1]</TargetName>
<TargetName ID="2">[your_system_result_1_2]</TargetName>
...
<TargetName ID="10">[your_system_result_1_10]</TargetName>

</Name>

<Name ID="2">
<SourceName>[test_name_2]</SourceName>
<TargetName ID="1">[your_system_result_2_1]</TargetName>
<TargetName ID="2">[your_system_result_2_2]</TargetName>
...
<TargetName ID="10">[your_system_result_2_10]</TargetName>

</Name>

...
<!-- All names in test corpus to follow -->
...

</TransliterationTaskResults>

Figure 2: Example of submission result format.


	Task Description
	Important Dates
	Participation
	Language Pairs
	Standard Datasets
	Evaluation Metrics
	Paper Format
	Contact Us
	Appendix: Data Formats
	Appendix: Submission of Results

