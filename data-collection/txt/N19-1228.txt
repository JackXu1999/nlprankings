



















































Sequential Attention with Keyword Mask Model for Community-based Question Answering


Proceedings of NAACL-HLT 2019, pages 2201–2211
Minneapolis, Minnesota, June 2 - June 7, 2019. c©2019 Association for Computational Linguistics

2201

Sequential Attention with Keyword Mask Model
for Community-based Question Answering

Jianxin Yang1,2, Wenge Rong1,2, Libin Shi3, Zhang Xiong1,2
1State Key Laboratory of Software Development Environment, Beihang University, China

2School of Computer Science and Engineering, Beihang University, China
3Microsoft, China

{yjx17,w.rong,xiongz}@buaa.edu.cn, libshi@microsoft.com

Abstract

In Community-based Question Answering
system(CQA), Answer Selection(AS) is a crit-
ical task, which focuses on finding a suit-
able answer within a list of candidate answers.
For neural network models, the key issue is
how to model the representations of QA text
pairs and calculate the interactions between
them. We propose a Sequential Attention
with Keyword Mask model(SAKM) for CQA
to imitate human reading behavior. Question
and answer text regard each other as context
within keyword-mask attention when encod-
ing the representations, and repeat multiple
times(hops) in a sequential style. So the QA
pairs capture features and information from
both question text and answer text, interact-
ing and improving vector representations it-
eratively through hops. The flexibility of the
model allows to extract meaningful keywords
from the sentences and enhance diverse mutual
information. We perform on answer selection
tasks and multi-level answer ranking tasks.
Experiment results demonstrate the superiority
of our proposed model on community-based
QA datasets.

1 Introduction

Answering selection(AS) is one of the most fun-
damental challenges in community-based question
answering(CQA) services. Given a question and a
list of candidate answers, its aim is to choose the
most matching one to the question. During this
process of matching questions and answer candi-
dates, how to encode the question and answer(QA)
into meaningful and semantic representations im-
pacts on the results directly.

Earlier conventional statistic methods are nor-
mally based on feature engineering and resource
toolkits. Though these methods are easy in im-
plementation, they require extra efforts and hand-
crafted features(Heilman and Smith, 2010; Ty-

moshenko and Moschitti, 2015). Recently, with
the development of neural network, deep learn-
ing based models attract much attention in vari-
ous tasks(Krizhevsky et al., 2012; Sutskever et al.,
2014). In question answering field, the convo-
lutional neural networks(CNNs)(Yu et al., 2014;
Hu et al., 2014; Severyn and Moschitti, 2015) and
recurrent neural networks(RNNs)(Wang and Ny-
berg, 2015; Feng et al., 2015) are widely employed
to convert the question and answer text into vec-
tors and define a feed-forward multi-layer percep-
tron to compute the interactions between them.
These models construct sentences in an end-to-
end fashion with less manual involvement. To
capture fine-grained features, on the one hand,
some works are concerned with matching QA
pairs relationship in a more complex and diverse
way, e.g., CNTN(Qiu and Huang, 2015) and MV-
LSTM(Wan et al., 2016). On the other hand, latent
representation models aim to jointly learn lexical
and semantic information from QA sentences and
influence the vector generation directly, e.g., atten-
tion mechanism(Bahdanau et al., 2015).

Attention mechanism learns attention weights
of each words pairs between QA sentences. Af-
terwards it can calculate the weighted sum of hid-
den states over all time steps(dos Santos et al.,
2016). This approach has shown promising re-
sults, while challenges still exist. For example,
questions and answers in CQA services are gen-
erally long sentences, as such it is still difficult
to compress all information into a fixed-length
vector. To solve this problem, Sha et al. (2018)
proposes co-attention view which brings improve-
ment, and Zhang et al. (2018) further proposes
a two-step attention to build dynamic question
vectors based on various answer words. These
kinds of methods usually require more parame-
ters to learn representations. More importantly,
when computing attention weights, every words



2202

in QA pairs are involved. This word-to-word pat-
tern takes meaningless noise into consideration,
such as informal language usage or text irrelevant
to the question. To alleviate this problem, Chen
et al. (2018) proposes a context-aligned model to
align phrase in QA relying on overlapped words
and Stanford Core NLP tools1. Inspired by co-
attention, we extend it to sequential style to learn
better representations and try to extract useful key-
words with less parameters and resource toolkits.

In this work, we propose a Sequential Atten-
tion with Keyword Mask(SAKM) model for an-
swer selection task. We encode sentences similar
to human reading behavior. When generating the
question, our model refers to the answer and com-
bines the mutual information. It is the same pro-
cessing for producing answer representations. So
when encoding a question, answer text is used as
context and vice versa. We term this co-attention
view as one “hop”. Afterwards we repeat this pro-
cess several times(hops) in a sequential style. As
such QA pairs review each other recurrently to re-
mind of mutual information and refine the sen-
tence representations to be more precise across
hops. Besides, the Keyword Mask modifies the at-
tention mechanism such that the attention is com-
puted over keywords instead of all words in the
QA pair. So only keywords in the long context are
extracted at each time step.

The contributions in this paper are three fold-
ers: 1) We extend attention mechanism to sequen-
tial structure, so the question and answer review
each other recurrently to improve the sentence rep-
resentations. 2) Different from standard soft at-
tention, we propose sequential attention with key-
word mask(SAKM) model. Besides, our model
focuses on the significant words and filters other
meaningless data. 3) We analyse the proposed
SAKM model not only on classical answer selec-
tion tasks and but also multi-level answer ranking
tasks. Experiment results show that our model
tends to encode more rich semantic representa-
tions with less parameters.

2 Related Work

In Community-based Question Answering(CQA)
services, since normally there exists a large num-
ber of question and answer pairs in repository, an-
swer selection(AS) is a critical task, which focuses

1https://stanfordnlp.github.io/
CoreNLP/

on finding a suitable answer within a list of can-
didate answers. As for traditional methods, fea-
ture engineering is a core work, but also a time-
consuming and laborious task. BM25(Robertson
et al., 1994) calculates relevance with item fre-
quency, while language models(Ponte and Croft,
2017; Zhai and Lafferty, 2004) use the maximum
likelihood of a word estimated from the ques-
tion. Translation-based language models(Jeon
et al., 2005; Xue et al., 2008) further improve.
Considering the syntactic structure, some prior
works(Heilman and Smith, 2010; Tymoshenko
and Moschitti, 2015) convert the sentence into a
tree structure by dependency parsing. Addition-
ally, linguistics resources such as WordNet are
utilized to enhance lexical features. Classifica-
tion models like chain Conditional Random Fields
have been used to match the questions and an-
swers(Kiritchenko et al., 2014).

Recently, neural network based models have
shown effectiveness in various fields, such as com-
puter vision(Krizhevsky et al., 2012) and natural
language processing(Kim, 2014). Different from
aforementioned approaches, deep neural architec-
tures(Hu et al., 2014; Wang and Nyberg, 2015)
map each word into an embedding space, and
compress the whole sentence into a low dimension
vector. Then a similarity function is defined to cal-
culate the interactions between QA pairs. Closer
vectors in the embedding space represent much
more relevant text.

To model fine-grained features, Qiu and Huang
(2015) combines CNN with neural tensor net-
work(NTN) to learn complex interactions between
QA pairs. But NTN increases a lot of parameters
and costs more runtime and memory. MV-LSTM
proposed by Wan et al. (2016) uses bi-direction
LSTM to generate a positional representation at
each time step. Subsequently these representa-
tions from questions and answers are fed into a
tensor layer. Shen et al. (2017) learns word repre-
sentations in an embedding space by a translation
matrix, and calculates relevance of each word pair
in QA to compute a similarity matrix. Then CNN
maps this matrix to a score scalar. Recently, (Tay
et al., 2018b) applies the hyperbolic distance func-
tion to model the relationship between QA.

Other latent representation models construct in-
teractions between QA when encoding sentences.
More mutual information is extracted to learn a
better latent representation. Miao et al. (2016) pro-

https://stanfordnlp.github.io/CoreNLP/
https://stanfordnlp.github.io/CoreNLP/


2203

Figure 1: The architecture of 3 hops SAKM model. For simplification, we omit the lines in Sentence Representa-
tion Layer of Question part.

poses neural variational inference network. Wang
et al. (2016) takes question context into consid-
eration in RNN cell of answer network with gate
mechanism. Yin et al. (2016) and dos Santos
et al. (2016) propose attention-based CNN mod-
els to add attention weight matrix between a QA
pair as a feature map. Additionally, Zhang et al.
(2017) extends attention weight matrix to 3D ten-
sor including more diverse information. Sha et al.
(2018) proves co-attention view can significantly
outperform the single attention. Further more,
Zhang et al. (2018) constructs two-step attention
to obtain question aware vectors based on various
words in an answer sentence.

3 Sequential Attention with Keyword
Mask Model

Given a question, which may contain one or more
clauses, it can be denoted as Q = (q1, q2, ..., qn).
Similarly, an answer can be denoted as A =
(a1, a2, ..., an). A+ and A− represent a positive
answer and a negative answer, respectively. Fig. 1
describes the overall architecture of the proposed
Sequential Attention with Keyword Mask(SAKM)
model(in this figure, we use three hops as illustra-
tion). We extend our network in a sequential style.
For each hop, a serial of stacked layers are con-
structed for the questions and the answers.

3.1 Embedding and Dropout

Firstly the questions and the answers need to be
fed into the embedding layer and each word in sen-
tences corresponds to an one-hot vector. Given a
look-up table, each word is converted into an em-
bedding space. The index of the low dimensional
vector in the look-up table is the same as one-hot
vector. We denote the embedding vectors of the
QA pairs as Qemb = (x1, x2, ..., xn) ∈ Rd×|Q|
and Aemb = (y1, y2, ..., yn) ∈ Rd×|A|, where d is
the embedding size, |Q| and |A| denote the length
of the question and answer respectively.

In order to mitigate the risk of overfitting, we
employ dropout layer to randomly ignore different
part of neurons in different hops during training.
This process learns better representations of local
regions and leads to better generalization during
testing.

3.2 Sequential Attention

Gated Recurrent Unit(GRU) To encode a sen-
tence into a single vector, we choose gated re-
current unit(Cho et al., 2014) and construct Q-
GRU and A-GRU for the questions and answers,
respectively. Given an input sentence S =
(s1, s2, ..., sn) ∈ Rd×|S|, GRU handles each word
recurrently and at time step t the operation is de-



2204

fined as follow:

rt = σ(Wrst + Urht−1 + br)

zt = σ(Wzst + Uzht−1 + bz)

ht = zt � ht−1 + (1− zt)� h̃t

(1)

where

h̃t = tanh (Whst + Uh(rt � ht−1) + bh) (2)

In the above equations, Wr, Wz , Wh ∈ Rm×d and
Ur, Uz , Uh ∈ Rm×m are parameters in the neu-
rons. m is the dimension size of the hidden states.
br, bz and bh ∈ Rm are bias. σ is sigmoid function
and � means element-wise product.

Figure 2: soft attention details.

Attention Mechanism During the GRU encod-
ing process, an attention mechanism helps to com-
bine context information with the current hidden
states. For the standard soft attention mecha-
nism(Bahdanau et al., 2015), as demonstrated in
Fig. 2, the hidden state at time t computes atten-
tion weights with all of the context hidden states,
and then obtains alignment scores after softmax
operation. This mechanism takes all of the words
in the context into consideration, while our model
expects to extract some keywords to the current
word and ignore other meaningless or noisy seg-
ments. The keyword mask relies on the attention
weights and reserves top percent of words to ac-
count for alignment scores. It can be formulated

as:

eij = v
T tanh(Wa[hi; ĥj ])

emaskij = fmask top k(eij ,−inf)

aij =
exp(emaskij )∑|S|
k=1 exp(e

mask
ik )

ĉi =

|S|∑
j=1

aij ĥj

ci = tanh(Wc[ĉi;hi])

(3)

where [; ] is the operator of concatenation, and
fmask top k denotes the function that the top per-
cent of values are reserved while others are
masked as value−inf . So these masked positions
in aij become 0 after softmax operation, which
represents no influence to the current hidden state
hi. Fig. 3 shows the details of this attention mech-
anism. We will discuss the keywords percentage
in more detail in Section 5.2.

Figure 3: keyword-mask attention details.

After obtaining the representation of hidden
neuron at time step t, we concat ci with next word
as input corresponding to the dotted line described
in Fig. 3. When the whole sentence is processed,
the final hidden output does not become represen-
tation directly because it loses much information
about the beginning of the sentence. Instead, av-
erage operation over all hidden outputs is taken to
produce the final representation.

Sequential Extension As shown in Fig. 1, A-
GRU regards the question sentences as context to
compute attention and representations. Likewise,
Q-GRU reviews the answer contents to tune the
representations. We extend this process in a se-
quential style to capture features and enhance in-
formation both from question text and answer text.



2205

All of the parameters across hops are shared. For
each hop, the vectors of QA pairs interact and im-
prove. Compared to single direction attention or
single hop attention, our model gets much more
flexibility. It is capable of updating the represen-
tations towards the correct direction with the guide
of a loss function and gradients across hops. We
rename the Eq. 3 as MaskAttention(), as such
the equations of the sequential extension is defined
as:

Qh =MaskAttention(Qhemb, A
h−1)

Ah =MaskAttention(Ahemb, Q
h)

(4)

where h is the current number of hop. The model
iteratively updates the joint representations of the
question and answer pair and obtains different out-
puts across hops.

Sentence Representation In the sentence
representation layer, Qhrepresentation and
Ahrepresentation denote the final representa-
tion outputs of QA pairs at the hop h. Since Qh

and Ah only contain the information extracted at
hop h, more meaningful content would be lost af-
ter more hops. But we expect to remember it from
the beginning hops. To convey more information
across hops, we do not simply take Qh and Ah

as the sentence representations. Instead, all of
the previous outputs from MaskAttention are
involved. They can be calculated as:

Qhrepresentation =
1

h

h∑
j=1

Qh

Ahrepresentation =
1

h

h∑
j=1

Ah

(5)

3.3 Similarity Calculation

Finally, we design a weighted loss strategy to com-
pute the relevance and the loss value between QA
pairs. For each hop, we have a pair of QA sentence
representations and pass them through a similarity
function described as:

sh(Q,A) =
Qhrepresentation

T
Ahrepresentation

||Qhrepresentation||2 · ||Ahrepresentation||2
(6)

where sh(Q,A) is the matching score between QA
pairs at hop h. || · ||2 means euclidean distance. As
for the loss function during training, given a ques-
tion, we use pair-wise margin-based ranking loss

for a triple (Q,A+, A−). Thus the mathematical
expression is:

Lh(Q,A+, A−) = max(0,m− (sh(Q,A+)
−sh(Q,A−)))

(7)

where m is the predefined margin.
Since we expect that vector representations gen-

erated from posterior hops are more precisely
than the ones produced from prior hops, relatively
small tolerance to the risk of matching incorrect
QA pairs is accepted for posterior hops. There-
fore, the loss values take increasing weights across
hops. We denote rh as loss weights. The objective
loss function can be defined as:

L(Q,A+, A−) =
H∑

h=1

rhL
h(Q,A+, A−) (8)

4 Experimental Study

In this section, we test the proposed model on clas-
sical answer selection task and also multi-level an-
swer ranking task to validate the model’s effective-
ness2.

4.1 Answer Selection

Dataset & Implementation Details In this task,
we use a community-based question answering
dataset YahooCQA provided by Tay et al. (2017).
It is an open-domain community forum, and the
dataset contains 142,627 QA pairs. Sentences in
YahooCQA are generally long and noisy. We fol-
low the preprocessing in their work without extra
process. Four negative answers are generated for
a question using Lucene. Table 1 demonstrates the
statistics of YahooCQA.

Dataset YahooCQA ZhihuCQA
# of Qns 50.1k / 6.2k / 6.2k 16k / 2k / 2k
# of Pairs 253k / 31.7k / 31.7k 80k / 10k / 10k

Table 1: Statistics of Datasets.

For our model, we tune the hidden size to 300,
and the numbers of GRU layers for modeling
questions and answers are both 1. Dropout is 0.5
and word embedding is pre-trained by skip-gram
model. For the Sequential Extension layer, the
number of hops is 3. For the Similarity Calcu-
lation, margin is 0.1 and weights for all hops are

2Our code is available at https://github.com/
sheep-for/question_answer_matching

https://github.com/sheep-for/question_answer_matching
https://github.com/sheep-for/question_answer_matching


2206

set as (0.2, 0.3, 0.5). Weights are set to constants
because we promise to put more weight on later
hops and keep reasonable tolerance for prior ones.
Batch size is 20. All of the parameters are opti-
mized by Back Propagation and Momentum.

Baselines We compare our model against
several advanced deep neural network models.
CNTN(Qiu and Huang, 2015), NTN-LSTM, HD-
LSTM(Tay et al., 2017) and HyperQA(Tay et al.,
2018b) are interaction focused methods, while
AP-CNN, AP-BiLSTM(dos Santos et al., 2016),
QRNN(Bradbury et al., 2017), CTRN(Tay et al.,
2018a) and two-step attention(Zhang et al., 2018)
are latent representation models. Additionally, we
choose two traditional methods Random Guess
and BM25(Robertson et al., 1994).

Evaluation Metrics For YahooCQA, we
use Precision@1(P@1) and Mean Reciprocal
Rank(MRR) to evaluate our model and the metrics
are defined as:

P@1 =
1

N

N∑
i=1

δ(r(A+) = 1)

MRR =
1

N

N∑
i=1

1

r(qi)

(9)

where δ is indicator function, N is the number of
all queries and r(qi) is the rank of the first correct
answer to question qi.

Model P@1 MRR
Random Guess 20.0% 45.7%
BM25 22.5% 49.3%
CNN 41.3% 63.2%
LSTM 46.5% 66.9%
CNTN 46.5% 63.2%
NTN-LSTM 54.5% 73.1%
AP-CNN 56.0% 72.6%
AP-BiLSTM 56.8% 73.1%
QRNN 57.3% 73.6%
HD-LSTM 55.7% 73.5%
CTRN 60.1% 75.5%
Two-step attention 62.2% 77.4%
HyperQA 68.3% 80.1%
SA 66.0% 79.2%
SAKM 69.3% 81.4%

Table 2: Experiment results on YahooCQA. SA is the
sequential attention baseline without keyword-mask
operation. SAKM is our sequential attention with key-
word mask model.

Experiment Results The results are shown in
Table 2. Firstly, it is observed that deep neu-
ral network models outperform traditional models.
Most latent representation models obtain better re-
sults than interaction focused models, indicating
that earlier interactions when encoding sentences
produces semantic vectors. Most importantly, the
proposed SAKM model achieves best results on
both P@1 and MRR. Our basic SA model outper-
forms two-step attention model by 3.8% in terms
of P@1 and 1.8% in terms of MRR, which shows
that our sequential extension structure is effective.
Furthermore, our SAKM model outperforms Hy-
perQA model by 1.0% in terms of P@1 and 1.3%
in terms of MRR. Since HyperQA is an interac-
tion focused model which adopts the hyperbolic
distance function to model the relevance between
QA. we can combine it with our SAKM to obtain
better performance in further study. The experi-
ment results agree with our intuition that extract-
ing meaningful keywords in attention mechanism
helps to generate more precise representations.

4.2 Multi-Level Answer Ranking

Relevant relationship in answer selection datasets
is binary, only including relevance and irrelevance.
However, in the real CQA applications, it is diffi-
cult to verify whether the answers are completely
correct or not. This scenario has caused a chal-
lenge called multi-level answer ranking(Liu et al.,
2018). These answers for one question are anno-
tated as several levels corresponding to the thumb-
up numbers.

Dataset & Implementation Details To test
the proposed model in multi-level answer ranking
task, we choose the dataset ZhihuCQA provided
by Liu et al. (2018). Zhihu3 is a popular and pro-
fessional Chinese QA community platform with
more than millions of users and QA pairs. Ta-
ble 1 describes the statistics of ZhihuCQA. For
each question, top five answers are selected and
ranked by the thumb-up numbers. We replace
margin based ranking loss with RankNet(Burges
et al., 2005).

In this task, we use the jieba4 toolkits for word
segmentation and tune the hidden size to 200, and
the numbers of GRU layers for modeling ques-
tions and answers are both 2. Other settings are
the same as YahooCQA.

3https://www.zhihu.com/
4https://github.com/fxsjy/jieba

https://www.zhihu.com/
https://github.com/fxsjy/jieba


2207

Baselines We compare our model against avail-
able advanced methods in (Liu et al., 2018). ARC-
II learns hierarchical pattern based on ARC-I(Hu
et al., 2014). Skip-Thoughts model(Kiros et al.,
2015) trains an encoder-decoder model to con-
struct sentence vectors. Attentive LSTM, ABCNN
and Compare-Aggregate(Wang and Jiang, 2017)
are attention-based models and Rewrite+Rank is
based on generative adversarial network.

Evaluation Metrics In this task, since the la-
bels for an answer are not binary, we choose
normalized discounted cumulative gain(NDCG)
and expected reciprocal rank(ERR) for evaluation.
(o1, o2, ..., oM ) denotes the predicted orders of an-
swers to a question. NDCG is defined as:

NDCG =
DCG

iDCG

DCG =

M∑
i=1

2oi − 1
log(1 + i)

(10)

where iDCG is the ideal DCG calculated from the
correct orders (l1, l2, ..., lM ). ERR is defined as:

ERR =

M∑
r=1

Rr
r

r−1∏
i=1

(1−Ri)

Ri =
2oi − 1
2om

(11)

where om is the maximum of degree values.

Model NDCG ERR
Random Guess 41.7% 30.3%
ARC-I 64.4% 56.4%
ARC-II 68.5% 58.8%
Skip-Thoughts 69.1% 60.1%
Attentive LSTM 71.4% 61.1%
ABCNN 72.5% 62.0%
Compare-Aggregate 74.8% 63.2%
Rewrite+Rank 77.2% 65.0%
SA 80.7% 68.2%
SAKM 81.0% 68.4%

Table 3: Experiment results on ZhihuCQA.

Experiment Results Table 3 reports the results
on ZhihuCQA. Similar to YahooCQA, attention-
based models perform better than other base-
lines. Our SA model outperforms Rewrite+Rank
by 3.6% in terms of NDCG and 3.2% in terms
of ERR. SAKM model achieves slightly improve-
ment compared to SA version. Results show that

on multi-level answer ranking task, our review
mechanism allows question and answer interac-
tion while encoding in a more fine-grain aspect
and leads to better performance.

5 Discussion and Analysis

In this section, we divide our discussion into three
parts, including the trade-off between information
transmission and avoidance of overfitting, the re-
lationship between sentence length and keyword
percentage, the advantages of our SAKM model.

5.1 Trade-off between Information
Transmission and Avoidance of
Overfitting

Our model processes QA text in a sequential style.
For the first hop, the original contents are fed as
inputs. Afterwards, the representations are up-
dated based on previous outputs across hops, thus
it is significant to convey rich mutual information.
Meanwhile, since the sentences are long and re-
dundant, it is necessary to avoid overfitting.

Information Transmission across Hops In or-
der to utilize context better, We propose sequential
style to refine the sentence vectors through mul-
tiple hops. When calculating the sentence rep-
resentations, we take the average over outputs of
all time steps instead of selecting the final hid-
den state, and get the final sentence representa-
tions based on all hops. Additionally, in embed-
ding layer we pretrain the word embeddings on
the corpus using word2vec(Mikolov et al., 2013).
To guide the vectors update in a correct direction
based on gradients, the loss function is calculated
over all hops, and puts more weight on later ones.

Avoidance of Overfitting There are some tricks
to reduce the risk of overfitting. Our model ex-
tracts some keywords according to the attention
weights, and applies dropout to ignore different
neurons in different hops. Besides, the GRU layer
is shallow and the number of hops is suitable.

5.2 Relationship between Sentence Length
and Keyword Percentage

In the attention mechanism, we use fmask top k to
reserve top percent of attention weights. In this
part, we propose two strategies to explore the rela-
tionship between the sentence length and keyword
percentage.

Fixed-Percentage: We set the number of key-
words based on the statistics of the dataset. Firstly,



2208

we count the lengths of all questions and answers
respectively, and then sort them in an ascending
order. We choose the value of the third quartile as
the number of keywords in a question, while the
value of the first quartile as the number of key-
words in an answer. This strategy allows us not to
calculate percentage according to various lengths
in the dataset. Our experiments empirically show
that it works well.

Variable-Percentage: The second strategy is to
compute the number of keywords for all sentences.

Question: Since the answers are produced based
on question sequences, the question generally con-
tains more meaningful information such as inquiry
type, inquiry main verb, topic and so on. We em-
pirically calculate the number of keywords k in a
question of length x as follow:

k = min(10 lnx, x) (12)

Answer: As for answers submitted by users
in community forum, there exists redundant con-
tents, typos errors, emoticon and other informal
language usage. Inspired by TF-IDF algorithm, in
our experiments, we propose a heuristics rule to
calculate the number of keywords k. The length
of an answer is denoted as x. For the first part, we
compute the Total-Length(TL) term as follow:

TL(x) = xblg xc (13)

where b·c is rounding down operation. TL value
increases monotonously with length of sentence.
For the second part, we compute the Inverse-
Noise-Frequency(INF) term as follow:

INF (x) =
1

lg 2x
(14)

This term represents the percent of the meaning-
ful words, which is an inverse proportion to noisy
words. Finally, the number of keywords k can be
obtained by multiplying these two terms.

k = min(TL(x) · INF (x), x) (15)

5.3 Advantages of SAKM model
Simplicity: Our SAKM model is simple but out-
performs on large CQA datasets. The network is
shallow and all of the parameters across hops are
shared. Our model is not complicated and has
less parameters than other mentioned neural net-
work models. Table 4 demonstrates the complex-
ity analysis of some models.

Our model is an end-to-end neural network, and
trained via back-propagation automatically. The
SAKM model could be an universal way to learn
sentence vectors effectively and integrated in other
larger neural network models. It could be a useful
tool in building neural architecture based represen-
tations for text sequences.

Model Complexity Parameters
AP-BiLSTM 4(md+ d2) + 4d2 1.08M
NTN 2dk + 2k + d2k 2.1M
CTRN 3kdm+ 2dh+ h 1.05M
Two-step 6md+ 10d2 + 12d2k 1.31M
SAKM (Our) 3(md+ d2) + 4d2 0.9M

Table 4: Complexity analysis on YahooCQA.m is em-
bedding size, k is the filter width, d is the output dimen-
sion and h is the size of full-connected layer.

Convergency: As the sentence representations
are tuned and improved across hops in one epoch,
it costs less epochs for our model to converge. In
our experiments, performance improve quickly in
first ten epochs.

Effectiveness: The QA pairs capture features
and information both from question text and
answer text, iteratively updating and improving
question and answer representations through hops.

At the test time, choosing the outputs of the
last hop from sentence representation layer as sen-
tence vectors can obtain better results on P@1 and
MRR, demonstrating the effectiveness of improve-
ment across hops. SAKM model outperforms SA
model by more than 3% on P@1 and 2% on MRR.
It proves that extracting keywords is significant
and necessary. Besides, even if we choose the first
hop of SA model, the gains are significant com-
pared to two-way attention model. It means that
our refinement procedure leads to better represen-
tations for all hops. Table 5 shows the details.

Representation P@1 MRR
SA hop 1 64.5% 78.2%
SA hop last 66.0% 79.2%
SA concat hops 65.7% 79.0%
SAKM hop 1 68.7% 81.0%
SAKM hop last 69.3% 81.4%
SAKM concat hops 69.2% 81.3%

Table 5: Experiment results on YahooCQA using vari-
ous representations. SAKM hop 1 denotes that SAKM
model tests with the sentence representations of the first
hop.



2209

(a) Visualization of SA at first hop (b) Visualization of SA at last hop (c) Visualization of SAKM at last hop

Figure 4: Attention Visualization.

Given a pair of QA as example. Q: How can
i get a list of glenville high school graduates in
clvevland ohio. A: Try google with the school
name locationor use classmatescom theres links
there for back dated yearbooks. Fig. 4 displays
the heatmap of the attention weights. We can ob-
serve that compared to the first hop of SA model,
the last hop puts more weights on phrase glenville
high school graduates in clvevland ohio. Further
more, the last hop of SAKM model focuses on
keywords how can, glenville high school gradu-
ates. It shows that the inquiry type words and topic
words achieve more attention. From this heatmap,
it indicates that our SAKM model is reasonable
and works well.

6 Conclusion

In this work, we propose a sequential attention
with keyword mask model for CQA. Our model
handles answer selection task similar to human
reading behavior. The questions and answers re-
view each other recurrently to improve the rep-
resentations. This proposed attention mechanism
focuses on some keywords and filters other mean-
ingless data. We evaluate our model on two tasks,
answering selection and multi-level answer rank-
ing. The experiment results demonstrate that our
model outperforms on CQA datasets and enhance
mutual information between QA pairs effectively.

Acknowledgments

This work was supported in part by the State
Key Laboratory of Software Development Envi-
ronment of China (No. SKLSDE-2019ZX-16).

References

Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben-
gio. 2015. Neural machine translation by jointly
learning to align and translate. Proceedings of 2015
International Conference on Learning Representa-
tions.

James Bradbury, Stephen Merity, Caiming Xiong, and
Richard Socher. 2017. Quasi-recurrent neural net-
works. Proceedings of 2017 International Confer-
ence on Learning Representations.

Christopher J. C. Burges, Tal Shaked, Erin Renshaw,
Ari Lazier, Matt Deeds, Nicole Hamilton, and Gre-
gory N. Hullender. 2005. Learning to rank using
gradient descent. In Proceedings of 22nd Interna-
tional Conference on Machine Learning, pages 89–
96.

Qin Chen, Qinmin Hu, Jimmy Xiangji Huang, and
Liang He. 2018. CA-RNN: using context-aligned
recurrent neural networks for modeling sentence
similarity. In Proceedings of the 32nd AAAI Con-
ference on Artificial Intelligence, pages 265–273.

Kyunghyun Cho, Bart Van Merrienboer, Caglar Gul-
cehre, Dzmitry Bahdanau, Fethi Bougares, Holger
Schwenk, and Yoshua Bengio. 2014. Learning
phrase representations using rnn encoder–decoder
for statistical machine translation. In Proceedings of
the 2014 Conference on Empirical Methods in Nat-
ural Language Processing, pages 1724–1734.

Minwei Feng, Bing Xiang, Michael R Glass, Lidan
Wang, and Bowen Zhou. 2015. Applying deep
learning to answer selection: A study and an open
task. In Proceedings of 2015 IEEE Workshop on
Automatic Speech Recognition and Understanding,
pages 813–820.

Michael Heilman and Noah A Smith. 2010. Tree edit
models for recognizing textual entailments, para-
phrases, and answers to questions. In Proceedings
of 2010 Conference of the North American Chap-
ter of the Association of Computational Linguistics,
pages 1011–1019.



2210

Baotian Hu, Zhengdong Lu, Hang Li, and Qingcai
Chen. 2014. Convolutional neural network architec-
tures for matching natural language sentences. In
Proceedings of 2014 Annual Conference on Neural
Information Processing Systems, pages 2042–2050.

Jiwoon Jeon, W Bruce Croft, and Joon Ho Lee. 2005.
Finding similar questions in large question and an-
swer archives. In Proceedings of the 2005 ACM
CIKM International Conference on Information and
Knowledge Management, pages 84–90.

Yoon Kim. 2014. Convolutional neural networks for
sentence classification. In Proceedings of the 2014
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 1746–1751.

Svetlana Kiritchenko, Xiaodan Zhu, Colin Cherry, and
Saif M Mohammad. 2014. Nrc-canada-2014: De-
tecting aspects and sentiment in customer reviews.
In Proceedings of the 8th International Workshop on
Semantic Evaluation, pages 437–442.

Ryan Kiros, Yukun Zhu, Ruslan Salakhutdinov,
Richard S Zemel, Antonio Torralba, Raquel Urta-
sun, and Sanja Fidler. 2015. Skip-thought vectors.
In Proceedings of 2015 Annual Conference on Neu-
ral Information Processing Systems, pages 3294–
3302.

Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hin-
ton. 2012. Imagenet classification with deep con-
volutional neural networks. In Proceedings of 2012
Annual Conference on Neural Information Process-
ing Systems, pages 1097–1105.

Yang Liu, Wenge Rong, and Zhang Xiong. 2018. Im-
proved text matching by enhancing mutual infor-
mation. In Proceedings of the Thirty-Second AAAI
Conference on Artificial Intelligence.

Yishu Miao, Lei Yu, and Phil Blunsom. 2016. Neu-
ral variational inference for text processing. In Pro-
ceedings of 2016 International Conference on Ma-
chine Learning, pages 1727–1736.

Tomas Mikolov, Wentau Yih, and Geoffrey Zweig.
2013. Linguistic regularities in continuous space
word representations. In Proceedings of 2013 Con-
ference of the North American Chapter of the As-
sociation of Computational Linguistics, pages 746–
751.

Jay M. Ponte and W. Bruce Croft. 2017. A language
modeling approach to information retrieval. SIGIR
Forum, 51(2):202–208.

Xipeng Qiu and Xuanjing Huang. 2015. Convolutional
neural tensor network architecture for community-
based question answering. In Proceedings of the
24th International Joint Conference on Artificial In-
telligence, pages 1305–1311.

Stephen E Robertson, Steve Walker, Susan Jones,
Micheline Hancockbeaulieu, and Mike Gatford.
1994. Okapi at TREC-3. In Proceedings of The
Third Text REtrieval Conference, pages 109–126.

Cı́cero Nogueira dos Santos, Ming Tan, Bing Xiang,
and Bowen Zhou. 2016. Attentive pooling net-
works. CoRR, abs/1602.03609.

Aliaksei Severyn and Alessandro Moschitti. 2015.
Learning to rank short text pairs with convolutional
deep neural networks. In Proceedings of the 38th
International ACM SIGIR Conference on Research
and Development in Information Retrieval, pages
373–382.

Lei Sha, Jinge Yao, Sujian Li, Baobao Chang, and Zhi-
fang Sui. 2018. A multi-view fusion neural network
for answer selection. In Proceedings of the Thirty-
Second AAAI Conference on Artificial Intelligence,
pages 5422–5429.

Yikang Shen, Wenge Rong, Nan Jiang, Baolin Peng,
Jie Tang, and Zhang Xiong. 2017. Word embedding
based correlation model for question/answer match-
ing. In Proceedings of the Thirty-First AAAI Con-
ference on Artificial Intelligence, pages 3511–3517.

Ilya Sutskever, Oriol Vinyals, and Quoc V Le. 2014.
Sequence to sequence learning with neural net-
works. In Proceedings of 2014 Annual Conference
on Neural Information Processing Systems, pages
3104–3112.

Yi Tay, Anh Tuan Luu, and Siu Cheung Hui. 2018a.
Cross temporal recurrent networks for ranking ques-
tion answer pairs. In Proceedings of the 32nd AAAI
Conference on Artificial Intelligence, pages 5512–
5519.

Yi Tay, Minh C Phan, Luu Anh Tuan, and Siu Cheung
Hui. 2017. Learning to rank question answer pairs
with holographic dual lstm architecture. In Proceed-
ings of the 40th International ACM SIGIR Confer-
ence on Research and Development in Information
Retrieval, pages 695–704.

Yi Tay, Luu Anh Tuan, and Siu Cheung Hui. 2018b.
Hyperbolic representation learning for fast and effi-
cient neural question answering. In Proceedings of
the Eleventh ACM International Conference on Web
Search and Data Mining, WSDM 2018, Marina Del
Rey, CA, USA, February 5-9, 2018, pages 583–591.

Kateryna Tymoshenko and Alessandro Moschitti.
2015. Assessing the impact of syntactic and seman-
tic structures for answer passages reranking. In Pro-
ceedings of the 24th ACM International Conference
on Information and Knowledge Management, pages
1451–1460.

Shengxian Wan, Yanyan Lan, Jiafeng Guo, Jun Xu,
Liang Pang, and Xueqi Cheng. 2016. A deep ar-
chitecture for semantic matching with multiple po-
sitional sentence representations. In Proceedings of
the 30th AAAI Conference on Artificial Intelligence,
pages 2835–2841.

Bingning Wang, Kang Liu, and Jun Zhao. 2016. In-
ner attention based recurrent neural networks for an-
swer selection. In Proceedings of the 54th Annual



2211

Meeting of the Association for Computational Lin-
guistics, pages 1288–1297.

Di Wang and Eric Nyberg. 2015. A long short-term
memory model for answer sentence selection in
question answering. In Proceedings of the 53rd An-
nual Meeting of the Association for Computational
Linguistics, pages 707–712.

Shuohang Wang and Jing Jiang. 2017. A compare-
aggregate model for matching text sequences. In
Proceedings of 2017 International Conference on
Learning Representations.

Xiaobing Xue, Jiwoon Jeon, and W Bruce Croft. 2008.
Retrieval models for question and answer archives.
In Proceedings of the 31st Annual International
ACM SIGIR Conference on Research and Develop-
ment in Information Retrieval, pages 475–482.

Wenpeng Yin, Hinrich Schutze, Bing Xiang, and
Bowen Zhou. 2016. Abcnn: Attention-based convo-
lutional neural network for modeling sentence pairs.
Transactions of the Association for Computational
Linguistics, 4:259–272.

Lei Yu, Karl Moritz Hermann, Phil Blunsom, and
Stephen Pulman. 2014. Deep learning for answer
sentence selection. CoRR, abs/1412.1632.

Chengxiang Zhai and John D Lafferty. 2004. A study
of smoothing methods for language models applied
to information retrieval. ACM Transactions on In-
formation Systems, 22(2):179–214.

Pengqing Zhang, Yuexian Hou, Zhan Su, and Yi Su.
2018. Two-step multi-factor attention neural net-
work for answer selection. In Proceedings of 15th
Pacific Rim International Conference on Artificial
Intelligence, pages 658–670.

Xiaodong Zhang, Sujian Li, Lei Sha, and Houfeng
Wang. 2017. Attentive interactive neural networks
for answer selection in community question answer-
ing. In Proceedings of the Thirty-First AAAI Con-
ference on Artificial Intelligence, pages 3525–3531.


