



















































Jointly Learning to Embed and Predict with Multiple Languages


Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 2019–2028,
Berlin, Germany, August 7-12, 2016. c©2016 Association for Computational Linguistics

Jointly Learning to Embed and Predict with Multiple Languages

Daniel C. Ferreira∗ André F. T. Martins†∗] Mariana S. C. Almeida∗]
∗Priberam Labs, Alameda D. Afonso Henriques, 41, 2o, 1000-123 Lisboa, Portugal
†Unbabel Lda, Rua Visconde de Santarém, 67-B, 1000-286 Lisboa, Portugal

]Instituto de Telecomunicações, Instituto Superior Técnico, 1049-001 Lisboa, Portugal
{dcf,mla}@priberam.pt, {andre.martins}@unbabel.com

Abstract

We propose a joint formulation for learn-
ing task-specific cross-lingual word em-
beddings, along with classifiers for that
task. Unlike prior work, which first
learns the embeddings from parallel data
and then plugs them in a supervised
learning problem, our approach is one-
shot: a single optimization problem com-
bines a co-regularizer for the multilin-
gual embeddings with a task-specific loss.
We present theoretical results showing
the limitation of Euclidean co-regularizers
to increase the embedding dimension,
a limitation which does not exist for
other co-regularizers (such as the `1-
distance). Despite its simplicity, our
method achieves state-of-the-art accura-
cies on the RCV1/RCV2 dataset when
transferring from English to German, with
training times below 1 minute. On the
TED Corpus, we obtain the highest re-
ported scores on 10 out of 11 languages.

1 Introduction

Distributed representations of text (embeddings)
have been the target of much research in natural
language processing (Collobert and Weston, 2008;
Mikolov et al., 2013; Pennington et al., 2014; Levy
et al., 2015). Word embeddings partially capture
semantic and syntactic properties of text in the
form of dense real vectors, making them apt for
a wide variety of tasks, such as language model-
ing (Bengio et al., 2003), sentence tagging (Turian
et al., 2010; Collobert et al., 2011), sentiment anal-
ysis (Socher et al., 2011), parsing (Chen and Man-
ning, 2014), and machine translation (Zou et al.,
2013).

At the same time, there has been a consis-
tent progress in devising “universal” multilin-
gual models via cross-lingual transfer techniques
of various kinds (Hwa et al., 2005; Zeman and
Resnik, 2008; McDonald et al., 2011; Ganchev
and Das, 2013; Martins, 2015). This line of re-
search seeks ways of using data from resource-
rich languages to solve tasks in resource-poor
languages. Given the difficulty of handcrafting
language-independent features, it is highly appeal-
ing to obtain rich, delexicalized, multilingual rep-
resentations embedded in a shared space.

A string of work started with Klementiev et al.
(2012) on learning bilingual embeddings for text
classification. Hermann and Blunsom (2014) pro-
posed a noise-contrastive objective to push the
embeddings of parallel sentences to be close in
space. A bilingual auto-encoder was proposed by
Chandar et al. (2014), while Faruqui and Dyer
(2014) applied canonical correlation analysis to
parallel data to improve monolingual embeddings.
Other works optimize a sum of monolingual and
cross-lingual terms (Gouws et al., 2015; Soyer
et al., 2015), or introduce bilingual variants of
skip-gram (Luong et al., 2015; Coulmance et al.,
2015). Recently, Pham et al. (2015) extended the
non-compositional paragraph vectors of Le and
Mikolov (2014) to a bilingual setting, achieving
a new state of the art at the cost of more expensive
(and non-deterministic) prediction.

In this paper, we propose an alternative joint
formulation that learns embeddings suited to a par-
ticular task, together with the corresponding clas-
sifier for that task. We do this by minimizing a
combination of a supervised loss function and a
multilingual regularization term. Our approach
leads to a convex optimization problem and makes
a bridge between classical co-regularization ap-
proaches for semi-supervised learning (Sindhwani
et al., 2005; Altun et al., 2005; Ganchev et al.,

2019



2008) and modern representation learning. In
addition, we show that Euclidean co-regularizers
have serious limitations to learn rich embeddings,
when the number of task labels is small. We es-
tablish this by proving that the resulting embed-
ding matrices have their rank upper bounded by
the number of labels. This limitation does not ex-
ist for other regularizers (convex or not), such as
the `1-distance and noise-contrastive distances.

Our experiments in the RCV1/RCV2 dataset
yield state-of-the-art accuracy (92.7%) with this
simple convex formulation, when transferring
from English to German, without the need of neg-
ative sampling, extra monolingual data, or non-
additive representations. For the reverse direction,
our best number (79.3%), while far behind the re-
cent para_doc approach (Pham et al., 2015), is
on par with current compositional methods.

On the TED corpus, we obtained general pur-
pose multilingual embeddings for 11 target lan-
guages, by considering the (auxiliary) task of
reconstructing pre-trained English word vectors.
The resulting embeddings led to cross-lingual
multi-label classifiers that achieved the highest re-
ported scores on 10 out of these 11 languages.1

2 Cross-Lingual Text Classification

We consider a cross-lingual classification frame-
work, where a classifier is trained on a dataset
from a source language (such as English) and ap-
plied to a target language (such as German). Later,
we generalize this setting to multiple target lan-
guages and to other tasks besides classification.

The following data are assumed available:

1. A labeled dataset Dl := {〈x(m), y(m))}Mm=1,
consisting of text documentsx in the source lan-
guage categorized with a label y ∈ {1, . . . , L}.

2. An unlabeled parallel corpus Du :=
{(s(n), t(n))}Nn=1, containing sentences s in the
source language paired with their translations t
in the target language (but no information about
their categories).

Let VS and VT be the vocabulary size of the source
and target languages, respectively. Throughout,
we represent sentences s ∈ RVS and t ∈ RVT
as vectors of word counts, and documents x as
an average of sentence vectors. We assume that

1We provide the trained embeddings at http://www.
cs.cmu.edu/~afm/projects/multilingual_
embeddings.html.

the unlabeled sentences largely outnumber the la-
beled documents, N � M , and that the number
of labels L is relatively small. The goal is to use
the data above to learn a classifier h : RVT →
{1, . . . , L} for the target language.

This problem is usually tackled with a two-stage
approach: in the first step, bilingual word embed-
dings P ∈ RVS×K and Q ∈ RVT×K are learned
from Du, where each row of these matrices con-
tains a Kth dimensional word representation in a
shared vector space. In the second step, a standard
classifier is trained onDl, using the source embed-
dings P ∈ RVS×K . Since the embeddings are in
a shared space, the trained model can be applied
directly to classify documents in the target lan-
guage. We describe next these two steps in more
detail. We assume throughout an additive repre-
sentation for sentences and documents (denoted
ADD by Hermann and Blunsom (2014)). These
representations can be expressed algebraically as
P>x,P>s,Q>t ∈ RK , respectively.
Step 1: Learning the Embeddings. The cross-
lingual embeddings P and Q are trained so that
the representations of paired sentences (s, t) ∈
Du have a small (squared) Euclidean distance

d`2(s, t) =
1
2
‖P>s−Q>t‖2. (1)

Since a direct minimization of Eq. 1 leads to a de-
generate solution (P = 0,Q = 0), Hermann and
Blunsom (2014) use instead a noise-contrastive
large-margin distance obtained via negative sam-
pling,

dns(s, t,n) = [m+ d`2(s, t)− d`2(s,n)]+, (2)

wheren is a random (unpaired) target sentence,m
is a “margin” parameter, and [x]+ := max{0, x}.
Letting J be the number of negative examples in
each sample, they arrive at the following objective
function to be minimized:

Rns(P ,Q) := 1
N

N∑
n=1

J∑
j=1

dns(s(n), t(n),n(n,j)).

(3)
This minimization can be carried out efficiently
with gradient-based methods, such as stochastic
gradient descent or AdaGrad (Duchi et al., 2011).
Note however that the objective function in Eq. 3
is not convex. Therefore, one may land at different
local minima, depending on the initialization.

2020



Step 2: Training the Classifier. Once we have
the bilingual embeddings P and Q, we can com-
pute the representation P>x ∈ RK of each docu-
ment x in the labeled dataset Dl. Let V ∈ RK×L
be a matrix of parameters (weights), with one col-
umn vy per label. A linear model is used to make
predictions, according to

ŷ = argmaxy∈{1,...,L}v
>
y P

>x

= argmaxy∈{1,...,L}w
>
y x, (4)

wherewy is a column of the matrixW := PV ∈
RVS×L. In prior work, the perceptron algorithm
was used to learn the weights V from the labeled
examples inDl (Klementiev et al., 2012; Hermann
and Blunsom, 2014). Note that, at test time, it
is not necessary to store the full embeddings: if
L � K, we may simply precompute W := PV
(one weight per word and label) if the input is in
the source language—orQV , if the input is in the
target language—and treat this as a regular bag-of-
words linear model.

3 Jointly Learning to Embed and Classify

Instead of a two-stage approach, we propose to
learn the bilingual embeddings and the classifier
jointly on Dl ∪ Du, as described next.

Our formulation optimizes a combination of a
co-regularization function R, whose goal is to
push the embeddings of paired sentences in Du to
stay close, and a loss function L, which fits the
model to the labeled data in Dl.

The simplest choice forR is a simple Euclidean
co-regularization function:

R`2(P ,Q) =
1
N

N∑
n=1

d`2(s
(n), t(n)) (5)

=
1

2N

N∑
n=1

‖P>s(n) −Q>t(n)‖2.

An alternative is the `1-distance:

R`1(P ,Q) =
1
N

N∑
n=1

‖P>s(n)−Q>t(n)‖1. (6)

One possible advantage of R`1(P ,Q) over
R`2(P ,Q) is that the `1-distance is more robust
to outliers, hence it is less sensitive to differences
in the parallel sentences. Note that both functions
in Eqs. 5–6 are jointly convex on P and Q, un-
like the one in Eq. 3. They are also simpler and do

not require negative sampling. While these func-
tions have a degenerate behavior in isolation (since
they are both minimized by P = 0 and Q = 0),
we will see that they become useful when plugged
into a joint optimization framework.

The next step is to define the loss function L to
leverage the labeled data inDl. We consider a log-
linear model P (y |x;W ) ∝ exp(w>y x), which
leads to the following logistic loss function:

LLL(W ) = − 1
M

M∑
m=1

logP (y(m) |x(m);W ).
(7)

We impose thatW is of the formW = PV for a
fixedV ∈ RK×L, whose choice we discuss below.

Putting the pieces together and adding some ex-
tra regularization terms, we formulate our joint ob-
jective function as follows:

F(P ,Q) = µR(P ,Q) + L(PV )
+
µS
2
‖P ‖2F +

µT
2
‖Q‖2F,

(8)

where µ, µS, µT ≥ 0 are regularization constants.
By minimizing a combination of L(PV ) and
R(P ,Q), we expect to obtain embeddings Q∗
that lead to an accurate classifier h for the target
language. Note that P = 0 and Q = 0 is no
longer a solution, due to the presence of the loss
term L(PV ) in the objective.
Choice of V . In Eq. 8, we chose to keep V fixed
rather than optimize it. The rationale is that there
are many more degrees of freedom in the embed-
ding matrices P and Q than in V (concretely,
O(K(VS + VT)) versus O(KL), where we are as-
suming a small number of labels, L � VS + VT).
Our assumption is that we have enough degrees of
freedom to obtain an accurate model, regardless of
the choice of V . These claims will be backed in
§4 by a more rigorous theoretical result. Keeping
V fixed has another important advantage: it al-
lows to minimize F with respect to P andQ only,
which makes it a convex optimization problem if
we choose R and L to be both convex—e.g., set-
tingR ∈ {R`2 ,R`1} and L := LLL.
Relation to Multi-View Learning. An interest-
ing particular case of this formulation arises if
K = L and V = IL (the identity matrix). In
that case, we have W = P and the embedding
matrices P and Q are in fact weights for every
pair of word and label, as in standard bag-of-word

2021



models. In this case, we may interpret the co-
regularizerR(P ,Q) in Eq. 8 as a term that pushes
the label scores of paired sentences P>s(n) and
Q>t(n) to be similar, while the source-based log-
linear model is fit via L(W ). The same idea un-
derlies various semi-supervised co-regularization
methods that seek agreement between multiple
views (Sindhwani et al., 2005; Altun et al., 2005;
Ganchev et al., 2008). In fact, we may regard
the joint optimization in Eq. 8 as a generalization
of those methods, making a bridge between those
methods and representation learning.

Multilingual Embeddings. It is straightforward
to extend the framework herein presented to the
case where there are multiple target languages (say
R of them), and we want to learn one embedding
matrix for each, {Q1, . . . ,QR}. The simplest way
is to consider a sum of pairwise co-regularizers,

R′(P , {Q1, . . . ,QR}) :=
R∑

r=1

R(P ,Qr). (9)

If R is additive over the parallel sentences (which
is the case for R`2 , R`1 and Rns), then this pro-
cedure is equivalent to concatenating all the par-
allel sentences (regardless of the target language)
and adding a language suffix to the words to dis-
tinguish them. This reduces directly to a problem
in the same form as Eq. 8.

Pre-Trained Source Embeddings. In practice,
it is often the case that pre-trained embeddings for
the source language are already available (let P̄
be the available embedding matrix). It would be
foolish not to exploit those resources. In this sce-
nario, the goal is to use P̄ and the dataset Du
to obtain “good” embeddings for the target lan-
guages (possibly tweaking the source embeddings
too, P ≈ P̄ ). Our joint formulation in Eq. 8 can
also be used to address this problem. It suffices to
set K = L and V = IL (as in the multi-view
learning case discussed above) and to define an
auxiliary task that pushes P and P̄ to be similar.
The simplest way is to use a reconstruction loss:

L`2(P , P̄ ) :=
1
2
‖P − P̄ ‖2F. (10)

The resulting optimization problem has resem-
blances with the retrofitting approach of Faruqui
et al. (2015), except that the goal here is to ex-
tend the embeddings to other languages, instead
of pushing monolingual embeddings to agree with

a semantic lexicon. We will present some experi-
ments in §5.2 using this framework.

4 Limitations of the Euclidean
Co-Regularizer

One may wonder how much the embedding di-
mension K influences the learned classifier. The
next proposition shows the (surprising) result that,
with the formulation in Eq. 8 with R = R`2 , it
makes absolutely no difference to increase K past
the number of labels L. Below, T ∈ RVT×N de-
notes the matrix with columns t(1), . . . , t(N).

Proposition 1. Let R = R`2 and assume T has
full row rank.2 Then, for any choice of V ∈
RK×L, possibly with K > L, the following holds:

1. There is an alternative, low-dimensional, V ′ ∈
RK′×L with K ′ ≤ L such that the classifier ob-
tained (for both languages) by optimizing Eq. 8
using V ′ is the same as if using V .3

2. This classifier depends on V only via the L-by-
L matrix V >V .

3. IfP ∗,Q∗ are the optimal embeddings obtained
with V , then we always have rank(P ∗) ≤ L
and rank(Q∗) ≤ L regardless of K.

Proof. See App. A.1 in the supplemental material.

Let us reflect for a moment on the practical im-
pact of Prop. 1. This result shows the limitation
of the Euclidean co-regularizerR`2 in a very con-
crete manner: when R = R`2 , we only need to
consider representations of dimension K ≤ L.

Note also that a corollary of Prop. 1 arises when
V >V = IL, i.e., when V is chosen to have
orthonormal columns (a sensible choice, since it
corresponds to seeking embeddings that leave the
label weights “uncorrelated”). Then, the second
statement of Prop. 1 tells us that the resulting clas-
sifier will be the same as if we had simply set
V = IL (the particular case discussed in §3). We
will see in §5.1 that, despite this limitation, this
classifier is actually a very strong baseline. Of
course, if the number of labels L is large enough,

2This assumption is not too restrictive: it holds if N ≥ VT
and if no target sentence can be written as a linear combina-
tion of the others (this can be accomplished if we remove
redundant parallel sentences).

3Let P ∗,Q∗ and P ′∗,Q′∗ be the optimal embeddings
obtained with V and V ′, respectively. Since we are working
with linear classifiers, the two classifiers are the same in the
sense that P ∗V = P ′∗V ′ and Q∗V = Q′∗V ′.

2022



this limitation might not be a reason for concern.4

An instance will be presented in §5.2, where we
will see that the Euclidean co-regularizer excels.

Finally, one might wonder whether Prop. 1 ap-
plies only to the (Euclidean) `2 norm or if it holds
for arbitrary regularizers. In fact, we show in
App. A.2 that this limitation applies more gener-
ally to Mahalanobis-Frobenius norms, which are
essentially Euclidean norms after a linear trans-
formation of the vector space. However, it turns
out that for general norms such limitation does not
exist, as shown below.

Proposition 2. IfR = R`1 in Eq. 8, then the anal-
ogous to Proposition 1 does not hold. It also does
not hold for the `∞-norm and the `0-“norm.”

Proof. See App. A.3 in the supplemental material.

This result suggests that, for other regulariz-
ers R 6= R`2 , we may eventually obtain bet-
ter classifiers by increasing K past L. As such,
in the next section, we experiment with R ∈
{R`2 ,R`1 ,Rns}, where Rns is the (non-convex)
noise-contrastive regularizer of Eq. 3.

5 Experiments

We report results on two experiments: one
on cross-lingual classification on the Reuters
RCV1/RCV2 dataset, and another on multi-label
classification with multilingual embeddings on the
TED Corpus.5

5.1 Reuters RCV1/RCV2

We evaluate our framework on the cross-lingual
document classification task introduced by Kle-
mentiev et al. (2012). Following prior work,
our dataset Du consists of 500,000 parallel sen-
tences from the Europarl v7 English-German cor-
pus (Koehn, 2005); and our labeled dataset Dl
consists of English and German documents from
the RCV1/RCV2 corpora (Lewis et al., 2004),
each categorized with one out of L = 4 labels. We
used the same split as Klementiev et al. (2012):
1,000 documents for training, of which 200 are
held out as validation data, and 5,000 for testing.

4For regression tasks (such as the one presented in the last
paragraph of 3), instead of the “number of labels,” L should
be regarded as the number of output variables to regress.

5Our code is available at https:
//github.com/dcferreira/
multilingual-joint-embeddings.

Note that, in this dataset, we are classifying
documents based on their bag-of-word representa-
tions, and learning word embeddings by bringing
the bag-of-word representations of parallel sen-
tences to be close together. In this sense, we are
bringing together these multiple levels of repre-
sentations (document, sentence and word).

We experimented with the joint formulation in
Eq. 8, with L := LLL and R ∈ {R`2 ,R`1 ,Rns}.
We optimized with AdaGrad (Duchi et al., 2011)
with a stepsize of 1.0, using mini-batches of 100
Reuters RCV1/RCV2 documents and 50,000 Eu-
roparl v7 parallel sentences. We found no need to
run more than 100 iterations, with most of our runs
converging under 50. Our vocabulary has 69,714
and 175,650 words for English and German, re-
spectively, when training on the English portion
of the Reuters RCV1/RCV2 corpus, and 61,120
and 183,888 words for English and German, when
training in the German portion of the corpus. This
difference is due to the inclusion of words in the
training data into the vocabulary. We do not re-
move any words from the vocabulary, for simplic-
ity. We used the validation set to tune the hyper-
parameters {µ, µS, µT} and to choose the iteration
number. When using K = L, we chose V = IL;
otherwise, we chose V randomly, sampling its en-
tries from a Gaussian N (0, 0.1).

Table 1 shows the results. We include for com-
parison the most competitive systems published
to date. The first thing to note is that our joint
system with Euclidean co-regularization performs
very well for this task, despite the theoretical lim-
itations shown in §4. Although its embedding size
is only K = 4 (one dimension per label), it out-
performed all the two-stage systems trained on the
same data, in both directions.

For the EN→DE direction, our joint system
with `1 co-regularization achieved state-of-the-art
results (92.7%), matching two-stage systems that
use extra monolingual data, negative sampling, or
non-additive document representations. It is con-
ceivable that the better results of R`1 over R`2
come from its higher robustness to differences in
the parallel sentences.

For the DE→EN direction, our best result
(79%) was obtained with the noise-contrastive co-
regularizer, which outperformed all systems ex-
cept para_doc (Pham et al., 2015). While
the accuracy of para_doc is quite impressive,
note that it requires 500-dimensional embeddings

2023



K EN→DE DE→EN
I-Matrix [KTB12] 40 77.6 71.1
ADD [HB14] 40 83.7 71.4
ADD [HB14] 128 86.4 74.7
BI [HB14] 40 83.4 69.2
BI [HB14] 128 86.1 79.0
BilBOWA [GBC15] 40 86.5 75.0
Binclusion [SSA15] 40 86.8 76.7
Bincl.+RCV [SSA15] (‡) 40 92.7 84.4
CLC-WA [SLLS15] (†) 40 91.3 77.2
para_sum [PLM15] (†) 100 90.6 78.8
para_doc [PLM15] (†) 500 92.7 91.5
Joint,R`2 4 91.2 78.2
Joint,R`1 4 92.7 76.0
Joint,R`1 40 92.7 76.2
Joint,Rns 4 91.2 76.8
Joint,Rns 40 91.4 79.3

Table 1: Accuracies in the RCV1/RCV2 dataset. Shown for comparison are Klementiev et al. (2012)
[KTB12], Hermann and Blunsom (2014) [HB14], Gouws et al. (2015) [GBC15], Soyer et al. (2015)
[SSA15], Shi et al. (2015) [SLLS15], and Pham et al. (2015) [PLM15]. Systems marked with (†) used
the full 1.8M parallel sentences in Europarl. The one with (‡) used additional target monolingual data
from RCV1/RCV2. The bottom rows refer to our joint method, with Euclidean (`2), `1, and noise-
contrastive co-regularization.

(hence many more parameters), was trained on
more parallel sentences, and requires more expen-
sive (and non-deterministic) computation at test
time to compute a document’s embedding. Our
method has the advantage of being simple and
very fast to train: it took less than 1 minute to
train the joint-R`1 system for EN→DE, using a
single core on an Intel Xeon @2.5 GHz. This can
be compared with Klementiev et al. (2012), who
took 10 days on a single core, or Coulmance et al.
(2015), who took 10 minutes with 6 cores.6

Although our theoretical results suggest that in-
creasing K when using the `1 norm may increase
the expressiveness of our embeddings, our results
do not support this claim (the improvements in
DE→EN from K = 4 to K = 40 were tiny).
However, it led to a gain of 2.5 points when us-
ing negative sampling. For K = 40, this system is
much more accurate than Hermann and Blunsom
(2014), which confirms that learning the embed-
dings together with the task is highly beneficial.

6Coulmance et al. (2015) reports accuracies of 87.8%
(EN→DE) and 78.7% (DE→EN), when using 10,000 train-
ing documents from the RCV1/RCV2 corpora.

5.2 TED Corpus

To assess the ability of our framework to han-
dle multiple target languages, we ran a second
set of experiments on the TED corpus (Cettolo
et al., 2012), using the training and test parti-
tions created by Hermann and Blunsom (2014),
downloaded from http://www.clg.ox.ac.
uk/tedcorpus. The corpus contains English
transcriptions and multilingual, sentence-aligned
translations of talks from the TED conference in
12 different languages, with 12,078 parallel docu-
ments in the training partition (totalling 1,641,985
parallel sentences). Following their prior work, we
used this corpus both as parallel data (Du) and as
the task dataset (Dl). There are L = 15 labels and
documents can have multiple labels.

We experimented with two different strategies:

• A one-stage system (Joint), which jointly trains
the multilingual embeddings and the multi-label
classifier (similarly as in §5.1). To cope with
multiple target languages, we used a sum of
pairwise co-regularizers as described in Eq. 9.
For classification, we use multinomial logistic
regression, where we select those labels with a
posterior probability above 0.18 (tuned on vali-

2024



dation data).

• A two-stage approach (Joint w/ Aux), where we
first obtain multilingual embeddings by apply-
ing our framework with an auxiliary task with
pre-trained English embeddings (as described in
Eq. 10 and in the last paragraph of §3), and then
use the resulting multilingual representations to
train the multi-label classifier. We address this
multi-label classification problem with indepen-
dent binary logistic regressors (one per label),
trained by running 100 iterations of L-BFGS
(Liu and Nocedal, 1989). At test time, we se-
lect those labels whose posterior probability are
above 0.5.

For the Joint w/ Aux strategy, we used the
300-dimensional GloVe-840B vectors (Penning-
ton et al., 2014), downloaded from http://
nlp.stanford.edu/projects/glove/.

Table 2 shows the results for cross-lingual clas-
sification, where we use English as source and
each of the other 11 languages as target. We
compare our two strategies above with the strong
Machine Translation (MT) baseline used by Her-
mann and Blunsom (2014) (which translates the
input documents to English with a state-of-the-
art MT system) and with their two strongest sys-
tems, which build document-level representations
from embeddings trained bilingually or multi-
lingually (called DOC/ADD single and DOC/ADD
joint, respectively).7 Overall, our Joint system
with `2 regularization outperforms both Hermann
and Blunsom (2014)’s systems (but not the MT
baseline) for 8 out of 11 languages, performing
generally better than our `1-regularized system.
However, the clear winner is our `2-regularized
Joint w/ Aux system, which wins over all systems
(including the MT baseline) by a substantial mar-
gin, for all languages. This shows that pre-trained
source embeddings can be extremely helpful in
bootstrapping multilingual ones.8 On the other
hand, the performance of the Joint w/ Aux sys-
tem with `1 regularization is rather disappointing.
Note that the limitations of R`2 shown in §4 are
not a concern here, since the auxiliary task has

7Note that, despite the name, the Hermann and Blunsom
(2014)’s joint systems are not doing joint training as we are.

8Note however that, overall, our Joint w/ Aux systems
have access to more data than our Joint systems and also
than Hermann and Blunsom (2014)’s systems, since the pre-
trained embeddings were trained on a large amount of En-
glish monolingual data. Yet, the amount of target language
data is the same.

L = 300 dimensions (the dimension of the pre-
trained embeddings). A small sample of the mul-
tilingual embeddings produced by the winner sys-
tem is shown in Table 4.

Finally, we did a last experiment in which we
use our multilingual embeddings obtained with
Joint w/ Aux to train monolingual systems for each
language. This time, we compare with a bag-of-
words naïve Bayes system (reported by Hermann
and Blunsom (2014)), a system trained on the
Polyglot embeddings from Al-Rfou et al. (2013)
(which are multilingual, but not in a shared rep-
resentation space), and the two systems developed
by Hermann and Blunsom (2014). The results are
shown in Table 3. We observe that, with the excep-
tion of Turkish, our systems consistently outper-
form all the competitors. Comparing the bottom
two rows of Tables 2 and 3 we also observe that,
for the `2-regularized system, there is not much
degradation caused by cross-lingual training ver-
sus training on the target language directly (in fact,
for Spanish, Polish, and Brazilian Portuguese, the
former scores are even higher). This suggests that
the multilingual embeddings have high quality.

6 Conclusions

We proposed a new formulation which jointly
minimizes a combination of a supervised loss
function with a multilingual co-regularization
term using unlabeled parallel data. This allows
learning task-specific multilingual embeddings to-
gether with a classifier for the task. Our method
achieved state-of-the-art accuracy on the Reuters
RCV1/RCV2 cross-lingual classification task in
the English to German direction, while being ex-
tremely simple and computationally efficient. Our
results in the Reuters RCV1/RCV2 task, obtained
using Europarl v7 as parallel data, show that our
method has no trouble handling different levels
of representations simutaneously (document, sen-
tence and word). On the TED Corpus, we obtained
the highest reported scores for 10 out of 11 lan-
guages, using an auxiliary task with pre-trained
English embeddings.

Acknowledgments

We would like to thank the three anonymous
reviewers. This work was partially sup-
ported by the European Union under H2020
project SUMMA, grant 688139, and by Fun-
dação para a Ciência e Tecnologia (FCT),

2025



Ara. Ger. Spa. Fre. Ita. Dut. Pol. Br. Pt. Rom. Rus. Tur.
MT Baseline [HB14] 42.9 46.5 51.8 52.6 51.4 50.5 44.5 47.0 49.3 43.2 40.9
DOC/ADD single [HB14] 41.0 42.4 38.3 47.6 48.5 26.4 40.2 35.4 41.8 44.8 45.2
DOC/ADD joint [HB14] 39.2 40.5 44.3 44.7 47.5 45.3 39.4 40.9 44.6 47.6 41.7
Joint,R`2 , K = 15 41.8 46.6 46.6 46.0 48.7 52.5 39.5 40.8 47.6 44.9 47.2
Joint,R`1 , K = 15 44.0 44.7 49.4 40.1 46.1 49.4 35.7 43.5 40.5 42.2 43.4
Joint w/ Aux,R`2 , K = 300 46.9 52.0 59.4 54.6 56.0 53.6 51.0 51.7 53.9 52.3 49.5
Joint w/ Aux,R`1 , K = 300 44.0 40.4 40.4 39.5 38.6 38.1 43.2 36.6 35.1 44.3 44.4

Table 2: Cross-lingual experiments on the TED Corpus using English as a source language. Reported
are the micro-averaged F1 scores for a machine translation baseline and the two strongest systems of
Hermann and Blunsom (2014), our one-stage joint system (Joint), and our two-stage system that trains
the multilingual embeddings jointly with the auxiliary task of fitting pre-trained English embeddings
(Joint w/ Aux), with both `1 and `2 regularization. Bold indicates the best result for each target language.

Ara. Ger. Spa. Fre. Ita. Dut. Pol. Br. Pt. Rom. Rus. Tur.
BOW baseline [HB14] 46.9 47.1 52.6 53.2 52.4 52.2 41.5 46.5 50.9 46.5 51.3
Polyglot [HB14] 41.6 27.0 41.8 36.1 33.2 22.8 32.3 19.4 30.0 40.2 29.5
DOC/ADD Single [HB14] 42.2 42.9 39.4 48.1 45.8 25.2 38.5 36.3 43.1 47.1 43.5
DOC/ADD Joint [HB14] 37.1 38.6 47.2 45.1 39.8 43.9 30.4 39.4 45.3 40.2 44.1
Joint w/ Aux,R`2 , K = 300 48.6 54.4 57.5 55.8 56.9 54.5 46.1 51.3 56.5 53.0 49.5
Joint w/ Aux,R`1 , K = 300 52.4 47.8 57.8 50.0 53.3 52.3 47.6 49.0 49.2 51.4 50.9

Table 3: Monolingual experiments on the TED Corpus. Shown are the micro-averaged F1 scores for a
bag-of-words baseline, a system trained on Polyglot embeddings, the two strongest systems of Hermann
and Blunsom (2014), and our Joint w/ Aux system with `1 and `2 regularization.

january_en science_en oil_en road_en speak_en
januari_nl �`lw_ar óleo_pb route_fr spreken_nl

şubat_tr �`l_ar olie_nl strada_it fala_pb
gennaio_it ciência_pb petrolio_it weg_nl �k®_ar

februarie_ro science_fr öl_de drum_ro gesproken_nl
br�§r_ar ştiinţa_ro pétrole_fr �syr_ar habla_es

ianuarie_ro wetenschap_nl petrol_tr estrada_pb konuşma_tr
febrero_es scienza_it petróleo_es drogi_pl ãîâîðèòü_ru
janvier_fr ciencia_es �nfX_ar lopen_nl horen_nl
§nA§r_ar wissenschaft_de petróleo_pb strade_it mowy_pl

janeiro_pb científica_pb petrol_ro drodze_pl vorbească_ro
enero_es nauka_pl aceite_es wegen_nl spreekt_nl

september_nl bilim_tr ropę_pl yol_tr ��d§�_ar
settembre_it s, tiint,a_ro íåôòü_ru camino_es sprechen_de

septiembre_es s, tiint,ă_ro petrolul_ro conduce_ro ii_ro
september_de nauki_pl íåôòè_ru andar_pb discours_fr

ekim_tr íàóêà_ru fX_ar ïóòè_ru sentire_it
Fbtmbr_ar l_ar ropy_pl �syr_ar contar_pb
febbraio_it lw_ar E§_ar äàëåêî_ru ñåáÿ_ru

septembrie_ro scientifica_it ulei_ro yolculuk_tr J�P_ar
setembro_pb scienze_it �z§ yola_tr poser_fr

Table 4: Examples of nearest neighbor words for the multilingual embeddings trained with our Joint
w/ Aux system with `2 regularization. Shown for each English word are the 20 closest target words in
Euclidean distance, regardless of language.

2026



through contracts UID/EEA/50008/2013,
through the LearnBig project (PTDC/EEI-
SII/7092/2014), and the GoLocal project (grant
CMUPERI/TIC/0046/2014).

References

Rami Al-Rfou, Bryan Perozzi, and Steven Skiena.
2013. Polyglot: Distributed word represen-
tations for multilingual NLP. arXiv preprint
arXiv:1307.1662 .

Yasemin Altun, Mikhail Belkin, and David A.
McAllester. 2005. Maximum Margin Semi-
Supervised Learning for Structured Variables.
In Advances in Neural Information Processing
Systems 18. pages 33–40.

Yoshua Bengio, Réjean Ducharme, Pascal Vin-
cent, and Christian Janvin. 2003. A neural prob-
abilistic language model. Journal of Machine
Learning Research 3:1137–1155.

Mauro Cettolo, Christian Girardi, and Marcello
Federico. 2012. Wit3: Web inventory of tran-
scribed and translated talks. In Proc. of the
16th Conference of the European Association
for Machine Translation. pages 261–268.

Sarath Chandar, Stanislas Lauly, Hugo Larochelle,
Mitesh M. Khapra, Balaraman Ravindran,
Vikas Raykar, and Amrita Saha. 2014. An
Autoencoder Approach to Learning Bilingual
Word Representations.

Danqi Chen and Christopher D Manning. 2014. A
fast and accurate dependency parser using neu-
ral networks. In Proc. of Empirical Methods for
Natural Language Processing. pages 740–750.

Ronan Collobert and Jason Weston. 2008. A uni-
fied architecture for natural language process-
ing: Deep neural networks with multitask learn-
ing. In Proc. of the International Conference on
Machine Learning. ACM, pages 160–167.

Ronan Collobert, Jason Weston, Léon Bottou,
Michael Karlen, Koray Kavukcuoglu, and Pavel
Kuksa. 2011. Natural language processing (al-
most) from scratch. Journal of Machine Learn-
ing Research 12:2493–2537.

Jocelyn Coulmance, Jean-Marc Marty, Guillaume
Wenzek, and Amine Benhalloum. 2015. Trans-
gram, Fast Cross-lingual Word-embeddings.
Proceedings of the 2015 Conference on Empir-
ical Methods in Natural Language Processing
(EMNLP) pages 1109–1113.

John Duchi, Elad Hazan, and Yoram Singer. 2011.
Adaptive Subgradient Methods for Online
Learning and Stochastic Optimization. Journal
of Machine Learning Research 12:2121–2159.

Manaal Faruqui, Jesse Dodge, Sujay K Jauhar,
Chris Dyer, Eduard Hovy, and Noah A Smith.
2015. Retrofitting word vectors to semantic lex-
icons. In Proc. of Annual Meeting of the North-
American Chapter of the Association for Com-
putational Linguistics.

Manaal Faruqui and Chris Dyer. 2014. Improving
vector space word representations using multi-
lingual correlation. In Proc. of Annual Meet-
ing of the European Chapter of the Association
for Computational Linguistics. Association for
Computational Linguistics.

Kuzman Ganchev and Dipanjan Das. 2013. Cross-
lingual discriminative learning of sequence
models with posterior regularization. In Proc.
of Empirical Methods in Natural Language Pro-
cessing.

Kuzman Ganchev, Joao Graca, John Blitzer, and
Ben Taskar. 2008. Multi-view learning over
structured and non-identical outputs. In Proc.
of Conference on Uncertainty in Artificial Intel-
ligence.

Stephan Gouws, Yoshua Bengio, and Greg Cor-
rado. 2015. BilBOWA: Fast Bilingual Dis-
tributed Representations without Word Align-
ments. Proceedings of the 32nd International
Conference on Machine Learning (2015) pages
748–756.

Karl Moritz Hermann and Phil Blunsom. 2014.
Multilingual Models for Compositional Dis-
tributed Semantics. Proceedings of ACL pages
58–68.

Rebecca Hwa, Philip Resnik, Amy Weinberg,
Clara Cabezas, and Okan Kolak. 2005. Boot-
strapping parsers via syntactic projection across
parallel texts. Natural language engineering
11(3):311–325.

Alexandre Klementiev, Ivan Titov, and Binod
Bhattarai. 2012. Inducing crosslingual dis-
tributed representations of words. 24th Inter-
national Conference on Computational Linguis-
tics - Proceedings of COLING 2012: Technical
Papers (2012) pages 1459–1474.

Philipp Koehn. 2005. Europarl: A parallel corpus

2027



for statistical machine translation. MT summit
11.

Quoc Le and Tomas Mikolov. 2014. Distributed
Representations of Sentences and Documents.
International Conference on Machine Learning
- ICML 2014 32:1188–1196.

Omer Levy, Yoav Goldberg, and Ido Dagan. 2015.
Improving distributional similarity with lessons
learned from word embeddings. Transactions of
the Association for Computational Linguistics
3:211–225.

David D. Lewis, Yiming Yang, Tony G. Rose, and
Fan Li. 2004. RCV1: A New Benchmark Col-
lection for Text Categorization Research. Jour-
nal of Machine Learning Research 5:361–397.

D. C. Liu and J. Nocedal. 1989. On the limited
memory BFGS method for large scale optimiza-
tion. Mathematical Programming 45:503–528.

Minh-Thang Luong, Hieu Pham, and Christo-
pher D. Manning. 2015. Bilingual Word Repre-
sentations with Monolingual Quality in Mind.
Workshop on Vector Modeling for NLP pages
151–159.

André F. T. Martins. 2015. Transferring corefer-
ence resolvers with posterior regularization. In
ACL.

Ryan McDonald, Slav Petrov, and Keith Hall.
2011. Multi-source transfer of delexicalized de-
pendency parsers. In Proc. of Empirical Meth-
ods in Natural Language Processing.

Tomas Mikolov, Greg Corrado, Kai Chen, and Jef-
frey Dean. 2013. Efficient Estimation of Word
Representations in Vector Space. Proceedings
of the International Conference on Learning
Representations (ICLR 2013) pages 1–12.

Jeffrey Pennington, Richard Socher, and Christo-
pher D Manning. 2014. GloVe: Global Vectors
for Word Representation. In Proceedings of the
2014 Conference on Empirical Methods in Nat-
ural Language Processing. pages 1532–1543.

Kaare Brandt Petersen and Michael Syskind Ped-
ersen. 2012. The Matrix Cookbook.

Hieu Pham, Minh-Thang Luong, and Christo-
pher D. Manning. 2015. Learning Dis-
tributed Representations for Multilingual Text
Sequences. Workshop on Vector Modeling for
NLP pages 88–94.

Tianze Shi, Zhiyuan Liu, Yang Liu, and Maosong
Sun. 2015. Learning Cross-lingual Word Em-
beddings via Matrix Co-factorization. Annual
Meeting of the Association for Computational
Linguistics pages 567–572.

Vikas Sindhwani, Partha Niyogi, and Mikhail
Belkin. 2005. A co-regularization approach to
semi-supervised learning with multiple views.
In Proceedings of ICML workshop on learning
with multiple views. Citeseer, pages 74–79.

Richard Socher, Jeffrey Pennington, and
Eh Huang. 2011. Semi-supervised recur-
sive autoencoders for predicting sentiment
distributions. In Conference on Empirical
Methods in Natural Language Processing.
pages 151–161.

Hubert Soyer, Pontus Stenetorp, and Akiko
Aizawa. 2015. Leveraging Monolingual Data
for Crosslingual Compositional Word Repre-
sentations. Proceedings of the 2015 Interna-
tional Conference on Learning Representations
(ICLR) .

Joseph Turian, Lev Ratinov, and Yoshua Bengio.
2010. Word representations: a simple and gen-
eral method for semi-supervised learning. In
Proc. of the Annual Meeting of the Association
for Computational Linguistics.

Daniel Zeman and Philip Resnik. 2008. Cross-
language parser adaptation between related lan-
guages. In IJCNLP. pages 35–42.

Will Y Zou, Richard Socher, Daniel M Cer, and
Christopher D Manning. 2013. Bilingual word
embeddings for phrase-based machine transla-
tion. In Proc. of Empirical Methods for Natural
Language Processing. pages 1393–1398.

2028


