




































DIVINE: A Generative Adversarial Imitation Learning Framework for Knowledge Graph Reasoning


Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing
and the 9th International Joint Conference on Natural Language Processing, pages 2642–2651,
Hong Kong, China, November 3–7, 2019. c©2019 Association for Computational Linguistics

2642

DIVINE: A Generative Adversarial Imitation Learning Framework for
Knowledge Graph Reasoning

Ruiping Li and Xiang Cheng∗
State Key Laboratory of Networking and Switching Technology

Beijing University of Posts and Telecommunications, Beijing 100876, China
{liruiping,chengxiang}@bupt.edu.cn

Abstract
Knowledge graphs (KGs) often suffer from
sparseness and incompleteness. Knowledge
graph reasoning provides a feasible way to ad-
dress such problems. Recent studies on knowl-
edge graph reasoning have shown that rein-
forcement learning (RL) based methods can
provide state-of-the-art performance. Howev-
er, existing RL-based methods require numer-
ous trials for path-finding and rely heavily on
meticulous reward engineering to fit specific
dataset, which is inefficient and laborious to
apply to fast-evolving KGs. In this paper, we
present DIVINE, a novel plug-and-play frame-
work based on generative adversarial imita-
tion learning for enhancing existing RL-based
methods. DIVINE guides the path-finding
process, and learns reasoning policies and re-
ward functions self-adaptively through imitat-
ing the demonstrations automatically sampled
from KGs. Experimental results on two bench-
mark datasets show that our framework im-
proves the performance of existing RL-based
methods without extra reward engineering.

1 Introduction

Knowledge graphs (Suchanek et al., 2007; Auer
et al., 2007; Bollacker et al., 2008; Carlson et al.,
2010; Vrandečić and Krötzsch, 2014), typically
composed of massive relational triples, are useful
resources for many downstream natural language
processing applications such as information ex-
traction and question answering. Although exist-
ing KGs have an extraordinarily large scale, they
are still highly incomplete (Min et al., 2013), lead-
ing extensive research efforts on automated infer-
ence of missing information from observed evi-
dence. In this paper, we focus on the problem of
multi-hop reasoning in KGs, which learns explicit
inference formulas from existing triples to com-
plete missing ones.

∗Corresponding author

To tackle multi-hop reasoning, various path-
based methods (Lao et al., 2011; Gardner et al.,
2013, 2014; Guu et al., 2015; Neelakantan et al.,
2015; Toutanova et al., 2016; Das et al., 2017)
have been proposed, which leverage the elaborate-
ly selected relational paths in KGs as the reason-
ing evidence. However, such evidential paths are
obtained by random walks, which inevitably intro-
duces inferior or even noisy paths. To address this
problem, the RL-based methods, such as Deep-
Path (Xiong et al., 2017) and MINERVA (Das
et al., 2018), strive for more reliable evidential
paths by policy-conditioned walking and achieve
state-of-the-art performance. They formulate the
path-finding problem as a Markov decision pro-
cess where their policy-based agents continuously
choose the most promising relation for state transi-
tion based on the current state and reasoning pol-
icy. Once a relational path found, the reasoning
policy is updated by a reward function according
to the path quality. Finally, through such a trial-
and-error process, the well-trained policy-based a-
gent can be used to find evidential paths for pre-
dictions.

However, these RL-based methods still suffer
from the following pain points. Firstly, they tend
to require numerous trials from scratch to find a
reliable evidential path since the action space can
be very large due to the complexity of KGs, which
leads to poor convergence properties. Secondly,
and most importantly, an efficient trial-and-error
optimization in RL requires designing a reward
function manually to fit the specific dataset. How-
ever, such reward engineering depends on meticu-
lous artificial design with domain expertise, which
can be significantly challenging in practice (Ng
et al., 2000). In particular, these RL-based meth-
ods are extremely sensitive to their reward func-
tions, where a little variation may lead to a sig-
nificant fluctuation of the reasoning performance.



2643

Therefore, for different datasets, the reward func-
tions in the RL-based methods need manual ad-
justments to achieve a good performance, which
will not only be inefficient and laborious but also
difficult to adapt to the rapid evolutions of real-
world KGs (Shi and Weninger, 2018).

In this paper, we present a novel plug-and-play
framework based on generative adversarial imita-
tion learning (GAIL) (Ho and Ermon, 2016) for
enhancing existing RL-based methods, which is
referred to as DIVINE for “Deep Inference via
Imitating Non-human Experts”. DIVINE trains a
reasoner, consisting of a generator and a discrim-
inator, from demonstrations by employing gener-
ative adversarial training, where the generator can
be any of the policy-based agents in existing RL-
based methods and the discriminator can be con-
sidered as a self-adaptive reward function. In this
way, for different datasets, the reward function-
s can be automatically tuned to approximate the
optimal performance, eliminating extra reward en-
gineering and manual interventions. In particular,
to enable the policy-based agent to find more di-
verse evidential paths for predictions, we propose
a path-based GAIL method, which can learn the
reasoning policy by imitating the path-level se-
mantic features of the demonstrations. In addi-
tion, to acquire demonstrations without extra man-
ual labor, we design an automated sampler for our
framework to dynamically sample relational paths
from KGs as the demonstrations according to the
specific environment of each entity.

In summary, our contributions are threefold:

• We present a plug-and-play framework based
on GAIL to enhance existing RL-based rea-
soning in KGs by learning reasoning poli-
cies and reward functions through imitating
demonstrations. To the best of our knowl-
edge, we are the first to introduce GAIL into
the field of knowledge graph reasoning.

• We propose a path-based GAIL method to
encourage the diversity of evidential path-
s and design an automated sampler for our
framework to sample demonstrations without
extra manual labor.

• We conduct extensive experiments on two
benchmark datasets. The experimental re-
sults illustrate that our framework improves
the performance of the current state-of-the-
art RL-based methods while eliminating ex-
tra reward engineering.

2 Related Work

Automated reasoning on KGs has been a long-
standing task for natural language processing. In
recent years, various embedding-based method-
s using tensor factorization (Nickel et al., 2011;
Bordes et al., 2013; Riedel et al., 2013; Yang et al.,
2014; Trouillon et al., 2017) or neural network
models (Socher et al., 2013) have been develope-
d, where they learn a projection which maps the
triples into a continuous vector space for further
tensor operations. Despite the impressive results
they achieved, most of them lack the ability to cap-
ture chains of multi-hop reasoning patterns con-
tained in paths.

To address the limitation of the embedding-
based methods, a series of path-based method-
s have been proposed, which consider the select-
ed relational paths as reasoning evidence. Lao
et al. (2011) propose the Path-Ranking Algorithm
(PRA) which uses random walks for path-finding.
Gardner et al. (2013, 2014) propose a variation on
PRA which computes feature similarity in the vec-
tor space. To combine the embedding-based meth-
ods, several neural multi-hop models (Neelakan-
tan et al., 2015; Guu et al., 2015; Toutanova et al.,
2015, 2016; Das et al., 2017) are proposed which
perform a hybrid reasoning. Nevertheless, the ev-
idential paths they used are gathered by random
walks, which might be inferior and noisy.

Recently, DeepPath (Xiong et al., 2017) and
MINERVA (Das et al., 2018) were proposed to ad-
dress the problem above by using reinforcemen-
t learning, where they are committed to learn a
policy which guides the agent to find more su-
perior evidential paths to maximize the expect-
ed reward. Specifically, DeepPath parameterizes
its policy with a fully-connected neural network
and uses manual reward criteria, including global
accuracy, efficiency and diversity, to evaluate the
path quality. In the training phase, DeepPath ap-
plies the linear combination of these criteria as the
positive reward while using a hand-craft constant
as the negative penalty. As for MINERVA, it pa-
rameterizes its policy with a long short-term mem-
ory network (LSTM) and considers the path va-
lidity as the only reward criterion. In the training
phase, MINERVA uses boolean value as a terminal
signal to evaluate whether the current path reach-
es the target entity and manually tunes a moving
average of cumulative discounted reward on dif-
ferent datasets for variance reduction.



2644

3 Preliminaries

3.1 Knowledge Graph Reasoning

Given an incomplete knowledge graph G =
{(h, r, t)|h ∈ E , t ∈ E , r ∈ R}, where E and R
denote the entity set and relation set, respective-
ly. There are two main tasks in knowledge graph
reasoning, namely link prediction and fact predic-
tion. Link prediction involves inferring the tail
entity t given the head entity h and the query
relation rq, while fact prediction seeks to pre-
dict whether an unknown fact (h, rq, t) holds or
not. Recently, RL-based reasoning has become
a popular approach for knowledge graph reason-
ing, which achieves state-of-the-art performance.
In general, RL-based reasoning methods strive to
find relational paths to tune their reasoning poli-
cies for predictions and formulate the path-finding
problem as a Markov decision process (MDP).
In such a process, the policy-based agent decides
to take an action a ∈ A from the current state
(i.e., the current entity and its context information)
s ∈ S to reach the next one according to its reason-
ing policy π, where the action space is defined as
all the relations in G. In particular, each relational
chain in the relational paths can be considered as
a reasoning chain.

3.2 Imitation Learning

Imitation learning focuses on learning policies
from demonstrations, which has achieved great
success in solving reward engineering. The classi-
cal approach is to find the optimal reward function
by inverse reinforcement learning (IRL) (Russel-
l, 1998; Ng et al., 2000) to explain expert behav-
iors. However, IRL requires solving RL inside a
learning loop, which can be expensive to run in
large environments. Therefore, generative adver-
sarial imitation learning (GAIL) (Ho and Ermon,
2016) has recently been proposed, which learns
the expert policy with generative adversarial net-
work (GAN) (Goodfellow et al., 2014), eliminat-
ing any intermediate IRL steps.

In GAIL, a generator Gθ is trained to gener-
ate trajectories matching the distribution of ex-
pert trajectories (i.e., demonstrations). Each tra-
jectory τ is represented as a state-action sequence
[(st, at)]

∞
t=0 (st ∈ S, at ∈ A). In addition, a dis-

criminator Dω is learned to distinguish between
the generated policy πθ and expert policy πE . For
each training epoch, the discriminator is updated

first with the gradient as:

Eτ [∇ωlog(D(s,a))]+EτE [∇ωlog(1−D(s,a))], (1)

where τE denotes the expert trajectories generated
by πE and the trajectory expectation can be calcu-
lated in the γ-discounted infinite horizon as:

Eτ [D(s, a)] =
∞∑
t=0

E[γtD(st, at)], (2)

where the discriminator here can be interpreted as
a local reward function to provide feedback for the
policy learning process. Then, the generator is up-
dated with the cost function log(D(s, a)) using the
trust region policy optimization (TRPO) (Schul-
man et al., 2015). After sufficient adversarial
training, the optimal policy π̂ can be found by
GAIL to rationalize the expert policy πE .

4 Methodology

4.1 Framework Overview
As shown in Figure 1, our framework DIVINE
consists of two modules, namely a generative ad-
versarial reasoner and a demonstration sampler. In
particular, the reasoner is composed of a genera-
tor and a discriminator. The generator can be any
of the policy-based agents in existing RL-based
methods and the discriminator can be interpreted
as a self-adaptive reward function. For each query
relation, the sampler and the generator are adopted
respectively to automatically extract demonstra-
tions and generate relational paths from the giv-
en KG. The discriminator is then used to evalu-
ate the semantic similarity between the generated
paths and demonstrations to update the generator.

After sufficient adversarial training between the
generator and the discriminator alternatively, the
well-trained policy-based agent (i.e., generator)
can be used to find evidential paths matching the
distribution of the demonstrations and make pre-
dictions by synthesizing these evidential paths.

4.2 Generative Adversarial Reasoner
In our framework, the reasoner is learned from
the demonstrations through generative adversarial
training. A straightforward approach is to direct-
ly apply GAIL to train the reasoner. In particular,
the policy-based agent in the reasoner is trained to
find evidential paths by imitating the state-action
pairs in each expert trajectory (i.e., demonstra-
tion). However, such an approach may lead to



2645

����������	
�������������������������� �����������������������������������
������

��������
���������������

������ �����������������������������������
����������

��	
���
Figure 1: Overview of our framework DIVINE, where the generator can be any of the existing policy-based agent
for knowledge graph reasoning and the discriminator can be considered as a self-adaptive reward function.

poor performance. The main reason lies in that
the agent will tend to choose the same actions as
in the expert trajectories under certain states, while
ignoring many valuable evidential paths which are
semantically similar to the expert trajectories but
contain different reasoning chains.

Therefore, to encourage the agent to find more
diverse evidential paths, it is desirable to train the
agent by imitating each trajectory instead of each
of its state-action pairs. In addition, in the scenari-
o of knowledge graph reasoning, since the reason-
ing chains consist of only relations, the demon-
strations do not necessarily contain the state infor-
mation. In other words, the demonstrations can be
composed of only relational paths.

Based on the above analysis, we propose a path-
based GAIL method, where the reasoning policy is
learned by imitating the path-level semantic fea-
tures of the demonstrations which are composed
of only relational paths.

In what follows, we first describe the two com-
ponents of the reasoner, i.e., the generator and the
discriminator. Then, we show how to extract the
path-level semantic features.

Generator
The generator can be any of the policy-based agent
in existing RL-based methods. We strive to enable
the generator to find more diverse evidential paths
matching the distribution of the demonstrations in
the semantic space.

Discriminator
To better semantically distinguish between gener-
ated paths and demonstrations, we choose the con-
volutional neural networks (CNNs) to construct

our discriminator D, as CNNs have shown high
performance in semantic feature extraction from
natural languages (Kim, 2014).

Semantic Feature Extraction
For each positive entity pair, we respectively pack
the current generated paths and corresponding
demonstrations in the same package form. For
each package P = {x1, x2, ..., xN} containing N
relational paths, we encode the package to a real-
valued matrix as:

p = x1 ⊕ x2 ⊕ ...⊕ xN , (3)

where xn ∈ Rk is the k-dimensional path embed-
ding and ⊕ denotes the concatenation operator for
the package representation p ∈ RN×k. In partic-
ular, given a relational path x = {r1, r2, ...rt, ...},
the path embedding x is encoded as:

x =
∑
rt∈x

rt, (4)

where each relation rt is mapped into a real-valued
embedding rt ∈ Rk pre-trained by TransE (Bor-
des et al., 2013).

After packing, we feed the package representa-
tion p into our discriminator D to parameterize its
semantic features D(p). Specifically, a convolu-
tional layer activated by ReLU nonlinearity is first
used to extract local features by sliding a kernel
ω ∈ Rh×l for a new feature map:

c = ReLU(Conv(p,ω) + bc), (5)

where bc denotes the bias term. Then, a fully-
connected hidden layer and an output layer are
used for further semantic feature extraction:

D(p) = σ(W2ReLU(W1c)), (6)



2646

where the corresponding biases are not shown
above for brevity and the output layer is normal-
ized by a sigmoid function while other layers are
activated by ReLU nonlinearity.

4.3 Demonstration Sampler

For imitation learning, the first prerequisite is to
have high-quality demonstrations. However, due
to the large scale and complexity of KGs, man-
ually constructing a large number of reasoning
demonstrations requires considerable time and ex-
pert efforts. Therefore, we design an automat-
ed sampler to sample reliable reasoning demon-
strations from KGs without supervision and extra
manual labor.

Static Demo Sampling
For each query relation, we use all positive enti-
ty pairs to sample demonstration candidates from
the given KG. Specifically, for each positive entity
pair, we use bi-directional breadth-first search to
explore the shortest path between two entities. In
particular, since the shorter paths incline to char-
acterize more direct correlations between two enti-
ties, we prefer to use them for initialization to en-
sure the quality of demonstration candidates. As
for the longer paths, despite their potential utility
values, they are more likely to contain worthless
or even noisy inference steps, thus we learn them
only in the training phase. In doing so, we can
get a demonstration set ΩE which contains all the
candidates we sampled. Finally, to accommodate
the fixed input dimension of the discriminator D,
we can simply select a subset Pe ⊆ ΩE with the
top N occurrence frequency, where N is normally
much smaller than |ΩE |.

Dynamic Demo Sampling
Despite the simplicity of the static demo sam-
pling method, the obtained demonstrations by this
method are fixed and ignore the specific environ-
ment of each entity in the given KG. Therefore,
we propose an improving method to dynamically
sample demonstrations by taking the topological
correlations of entities into consideration.

Given a positive entity pair ⟨ehead, etail⟩, we in-
troduce a relational set Rh which contains all rela-
tions directly connected to ehead. For each reason-
ing attempt, Rh can be considered as the region
of interest (ROI) of the agent to start reasoning,
where the ROI related paths tend to be more rele-
vant to the current entity pair. Thus, we refine the

demonstration set by filtering out the demonstra-
tions which begin from Rh:

Ω′E={x | r1(x) ∈ Rh, x ∈ ΩE} (Rh ̸=∅), (7)

where ΩE is generated by the static demo sam-
pling method and r1(x) denotes the first relation
in relational path x = {r1, r2, ...rt, ...}.

In most cases, we can obtain enough demonstra-
tions in Ω′E to select a subset Pe ⊆ Ω′E in the same
way as the static demo sampling method. How-
ever, due to the sparsity of data in KG, we may
get insufficient demonstrations on long-tail enti-
ties. To solve this problem, we perform semantic
matching to explore more demonstrations from the
remaining candidates CE = ΩE \ Ω′E . Since the
reasoning policy is updated based on the seman-
tic similarities between the generated paths and
demonstrations, candidates which are semantical-
ly similar to the current demonstrations are also
instructive for the imitation process.

Inspired by the neighborhood attention for one-
shot imitation learning (Duan et al., 2017), we use
each demonstration in Ω′E to query other candi-
dates in correlation to itself. We adopt the dot
product to measure the semantic matching simi-
larity between two path embeddings:

αi =
∑

xj∈Ω′E

x̄i · xj i = 1, 2, ..., |CE |, (8)

where αi represents the sum of matching scores
between the current candidate x̄i and existing
demonstrations in Ω′E . Finally, we iteratively s-
elect the candidate with the highest α to pad the
refined demonstration set Ω′E until accommodat-
ing the required input dimension N .

4.4 Training

In the training phase, all the positive entity pairs
are used to generate demonstration candidates ΩE
for the imitation learning process. Specifically, for
each positive entity pair, the demonstration sam-
pler is required first to choose the corresponding
demonstrations, while the generator is conduct-
ed to generate some relational paths. Then, the
demonstrations are packed into package Pe and
the generated paths are packed into different pack-
ages {Pg | Pg ⊆ ΩG} according to their validity,
i.e., whether the agent can reach the target entity
along the current path, where ΩG is the collection
of all generated paths.



2647

For each package pair ⟨Pg,Pe⟩, we train the dis-
criminator D by minimizing its loss and expect it
to be expert in distinguishing between Pe and Pg.
In addition, to make the adversarial training pro-
cess more stable and effective, we adopt the loss
function proposed in WGAN-GP (Gulrajani et al.,
2017) to update the discriminator:

LC = E[D(pg)]− E[D(pe)];
LP = λE[(∥ ∇p̃D(p̃) ∥2 −1)2];
LD = LC + LP ,

(9)

where LC , LP and LD respectively denote the o-
riginal critic loss, gradient penalty and the loss of
discriminator, λ is the gradient penalty coefficien-
t and p̃ is sampled uniformly along straight lines
between pg and pe. According to the feedback of
the discriminator, we calculate the reward RG as:

RG = δg max{E[D(pg)]− E[D(pn)], 0}, (10)

δg =

{
1, Pg ⊆ Ω+G
0, otherwise

, (11)

where pn denotes a noise embedding composed
of random noise with continuous uniform distri-
bution, δg is a characteristic function which char-
acterizes the validity of package Pg, Ω+G is the
collection of all valid generated paths. We only
give positive rewards for partial valid paths that
at least have higher expectations than noise em-
bedding pn, which filters out the paths of inferi-
or quality to improve the convergence efficiency
of the training process. Once the reward obtained,
we updated the generator G by maximizing the ex-
pected cumulative reward with Monte-Carlo Poli-
cy Gradient (i.e., REINFORCE) (Williams, 1992).

We use mini-batch stochastic gradient descent
(SGD) to optimize the loss function of discrimina-
tor, while the generator is updated with the Adam
algorithm (Kingma and Ba, 2014).

5 Experiments

5.1 Datasets and Evaluation Metrics

Dataset # Ent. # Rel. # Facts # Tasks
NELL-995 75,492 200 154,213 12
FB15K-237 14,505 237 310,116 20

Table 1: Statistics of the datasets. # Ent. denotes the
number of unique entities and # Rel. denotes the num-
ber of relations.

The experiments are conducted on two bench-
mark datasets: NELL-995 (Xiong et al., 2017) and
FB15K-237 (Toutanova et al., 2015). The details
of the two datasets are described in Table 1. In par-
ticular, NELL-995, which is known to be a simple
dataset for reasoning tasks, is generated from the
995th iteration of the NELL system (Carlson et al.,
2010) by selecting the triples with Top-200 fre-
quently occurring relations. Compared to NELL-
995, FB15K-237 is more challenging and closer
to real-world scenarios, where its facts are creat-
ed from FB15K (Bordes et al., 2013) with redun-
dant relations removed. For each triple (h, r, t),
both datasets contain the inverse triple (h, r−1, t)
such that the agent can step backward in KGs,
which makes it possible to recover from a poten-
tially wrong decision that has been taken before.
For each reasoning task with a query relation rq,
all the triples with rq or r−1q are removed from the
KG and split into train and test samples.

Similar to recent works (Das et al., 2018; X-
iong et al., 2017), we use mean average precision
(MAP), mean reciprocal rank (MRR) and Hits@k
to evaluate the reasoning performance, where Hit-
s@k is the fraction of positive instances ranked in
the top k positions.

5.2 Baselines and Implementation Details

In our experiments, we consider two state-of-the-
art RL-based methods as baselines: DeepPath (X-
iong et al., 2017) and MINERVA (Das et al.,
2018). Deep path feeds the gathered evidential
paths to PRA (Lao et al., 2011) for both link pre-
diction and fact prediction tasks, while MINERVA
directly applies the well-trained agent to the link
prediction task for question answering. For Deep-
Path, we use the code released by Xiong et al.
(2017). For MINERVA, we use the code released
by Das et al. (2018). The experiment settings for
the baselines are set according to the suggestions
in the original papers.

In the implementation of our framework, we set
the path number N to 5 for each path package P ,
while the path dimension k is set to 200 which is
the same as the relation dimension in baselines.
For the discriminator, we set the convolution ker-
nel size to 3×5, the hidden layer size to 1024, and
the output layer size to the path dimension k, while
the gradient penalty coefficient λ is set to 5 and L2
regularization is also used to avoid over-fitting.

During testing, we also rank the answer triples



2648

Link Prediction Fact Prediction
Data Metric DeepPath Div(DeepPath) MINERVA Div(MINERVA) DeepPath Div(DeepPath)

NELL-995

Hits@1 0.718 0.749 0.799/0.663† 0.840/0.668† 0.687 0.701
Hits@3 0.847 0.878 0.936/0.773† 0.936/0.778† 0.839 0.841
MRR 0.784 0.811 0.852/0.725† 0.874/0.731† 0.764 0.774
MAP 0.796 0.821 0.885 0.893 0.774/0.493‡ 0.785/0.494‡

FB15K-237

Hits@1 0.417 0.520 0.427/0.217† 0.444/0.223† 0.394 0.421
Hits@3 0.646 0.740 0.582/0.329† 0.590/0.331† 0.565 0.599
MRR 0.560 0.639 0.546/0.293† 0.556/0.296† 0.522 0.545
MAP 0.572 0.658 0.553 0.568 0.536/0.311‡ 0.558/0.339‡

Table 2: Overall results on NELL-995 and FB15K-237. “†” denotes the results with settings for question answering
and “‡” denotes the results of directly ranking all the positive and negative triples given a query relation.

against the negative triples used in DeepPath and
MINERVA. In particular, there are approximately
10 corresponding negative triples for each positive
ones. Each negative triple is generated by replac-
ing the answer entity t with a faked one t′ given a
positive triple (h, r, t).

5.3 Results
The main results on the two datasets are shown in
Table 2. We use “Div(*)” to denote the RL-based
method “*” which adopts our framework DIVINE.
For a fair comparison, we follow MINERVA to re-
port the Hits@k and MRR scores (denoted with
“†”) to evaluate the link prediction performance
for question answering, which ranks entities ac-
cording to the probability that the agent can reach
the entity along evidential paths. Moreover, we al-
so follow DeepPath to report the MAP scores (de-
noted with “‡”) on the fact prediction task, which
directly ranks all the positive and negative triples
for a given query relation.

From the results shown in Table 2, we can ob-
serve that our framework produces consistent im-
provements for the two RL-based methods under
varying degrees on both link prediction and fac-
t prediction tasks. On the one hand, for existing
RL-based methods, their results on FB15K-237
are generally lower than those on NELL-995 s-
ince FB15K-237 is more complex and arguably
more difficult to design proper reward function-
s manually. However, our framework reliefs this
problem to some extent by dynamically learning
superior reward functions, thus we make greater
improvements on challenging FB15K-237. On the
other hand, for different datasets, the improve-
ments our framework makes for DeepPath vary a
lot while MINERVA not. This is because MIN-
ERVA manually adjusts its hyper-parameters ac-
cordingly when calculating cumulative discounted

reward, while DeepPath keeps the same. Obvious-
ly, it validates the necessity for existing RL-based
methods to adjust their reward functions accord-
ingly to fit different datasets. Enhanced by our
framework, these RL-based methods no longer re-
quire additional manual adjustments for different
datasets, which reveals great robustness.

Similar to existing RL-based methods, we also
report the decomposed results of link prediction
and use MAP to evaluate the performance for each
query relation on NELL-995 in Table 3. From the
results, we can observe that the results on differen-
t relations are of high variance and the enhanced
RL-based methods achieve better or comparable
performance for all query relations.

5.4 Effectiveness of the Path-Based GAIL
Method

1 2 3 4 5 6 7 8 9 10 11 12
�����������������

0

20

40

60

��
�
��
���

���
��
��

��
�������������
��	������������

Figure 2: Statistics of path-finding on NELL-995.

MAP
Path Avg. # Link # Fact

# Path-Based GAIL 24 0.821 0.494
# Original GAIL 18 0.814 0.468

Table 4: Results of different imitation learning settings.

To investigate the effectiveness of the path-based
GAIL method, we train the policy-based agent in
DeepPath on NELL-995 by our path-based GAIL
method and the original GAIL method, respective-
ly. In particular, in the process of training the a-



2649

(ID) Query Relations DeepPath Div(DeepPath) MINERVA Div(MINERVA)
(01) AgentBelongsToOrg 0.576 0.674 0.860 0.876
(02) AthleteHomeStadium 0.890 0.877 0.895 0.916
(03) AthletePlaysForteam 0.750 0.779 0.824 0.813
(04) AthletePlaysInLeague 0.960 0.962 0.970 0.965
(05) AthletePlaysSport 0.957 0.960 0.985 0.986
(06) OrgHeadquarteredInCity 0.790 0.801 0.946 0.936
(07) OrgHiredPerson 0.742 0.758 0.851 0.860
(08) PersonBornInLocation 0.757 0.780 0.793 0.827
(09) PersonLeadsOrg 0.794 0.810 0.851 0.881
(10) TeampPlaysInLeague 0.881 0.896 0.970 0.957
(11) TeampPlaysSport 0.739 0.816 0.846 0.874
(12) WorksFor 0.711 0.741 0.825 0.822
Overall 0.796 0.821 0.885 0.893

Table 3: Results of link prediction decomposed over different query relations on NELL-995.

gent by the original GAIL method, the demonstra-
tions are composed of state-action trajectories. For
each state-action pair (st, at), the state representa-
tion st is calculated by (et, etail − et), where et
and etail denote the embeddings of the current en-
tity and the tail entity, respectively. In Figure 2, we
show the statistics of the evidential path set Pnew
which are found by the agent and different from
the demonstrations. In Table 4, we compare the
average path number of Pnew and the reasoning
performance on the two prediction tasks.

As shown in Figure 2 and Table 4, we can ob-
serve that our path-based GAIL method obtains
more evidential paths for most query relations and
achieves better performance on both link and fac-
t predictions, which validates the effectiveness of
our path-based GAIL method and the rationality
of encouraging the agent to find more diverse evi-
dential paths.

5.5 Ablation Studies

We conduct ablation studies by embedding Deep-
Path into our framework to quantify the role of
components. Specifically, we re-train our frame-
work by ablating certain components:

• W/O Semantic Matching, where no seman-
tic matching is performed on long-tail enti-
ties. Instead, we directly extract some paths
from the remaining demonstration candidates
CE according to their occurrence frequency.

• W/O Dynamic Sampling, where no dynamic
demo sampling is performed to incorporate
the local environment of entities. In other
words, we only adopt the static demo sam-
pling method to obtain demonstrations.

• W/O Demo Sampling, where no demonstra-
tion is used for imitation learning, which de-
generates to DeepPath.

MAP

Configuration NELL-995 FB15K-237

Div(DeepPath) 0.821 0.658

W/O Semantic Matching 0.818 0.651

W/O Dynamic Sampling 0.806 0.640

W/O Demo Sampling 0.796 0.572

Table 5: Ablation on different components.

We use MAP to evaluate the link prediction
performance on both NELL-995 and FB15K-237
in Table 5. From the results, we can observe
that: (1) Based on imitation learning, our frame-
work can effectively improve the reasoning per-
formance, even if we use the static demo sampling
method to obtain demonstrations; (2) High-quality
demonstrations are crucial for imitation learning,
which indicates both topology filtering and seman-
tic matching play important roles in the demon-
stration sampler of our framework.

6 Conclusion

In this paper, we proposed a novel plug-and-play
framework DIVINE for knowledge graph rea-
soning based on generative adversarial imitation
learning, which enables existing RL-based meth-
ods to learn reasoning policies and reward func-
tions self-adaptively to adapt the fast evolutions of
real-world KGs. The experimental results show
that our framework improves the performance of
existing RL-based methods while eliminating ex-
tra reward engineering.



2650

Acknowledgments

This work was supported by National Natural Sci-
ence Foundation of China under Grant 61872045.
We acknowledge anonymous reviewers for their
valuable comments.

References
Sören Auer, Christian Bizer, Georgi Kobilarov, Jen-

s Lehmann, Richard Cyganiak, and Zachary Ives.
2007. Dbpedia: A nucleus for a web of open da-
ta. In The semantic web, pages 722–735. Springer.

Kurt Bollacker, Colin Evans, Praveen Paritosh, Tim S-
turge, and Jamie Taylor. 2008. Freebase: a collab-
oratively created graph database for structuring hu-
man knowledge. In Proceedings of the 2008 ACM
SIGMOD international conference on Management
of data, pages 1247–1250.

Antoine Bordes, Nicolas Usunier, Alberto Garcia-
Duran, Jason Weston, and Oksana Yakhnenko.
2013. Translating embeddings for modeling multi-
relational data. In Advances in Neural Information
Processing Systems 26, pages 2787–2795.

Andrew Carlson, Justin Betteridge, Bryan Kisiel, Bur-
r Settles, Estevam R Hruschka Jr, and Tom M
Mitchell. 2010. Toward an architecture for never-
ending language learning. In Proceedings of the
Twenty-Fourth AAAI Conference on Artificial Intel-
ligence, pages 1306–1313.

Rajarshi Das, Shehzaad Dhuliawala, Manzil Zaheer,
Luke Vilnis, Ishan Durugkar, Akshay Krishna-
murthy, Alex Smola, and Andrew McCallum. 2018.
Go for a walk and arrive at the answer: Reasoning
over paths in knowledge bases using reinforcement
learning. In International Conference on Learning
Representations.

Rajarshi Das, Arvind Neelakantan, David Belanger,
and Andrew McCallum. 2017. Chains of reasoning
over entities, relations, and text using recurrent neu-
ral networks. pages 132–141.

Yan Duan, Marcin Andrychowicz, Bradly Stadie, Ope-
nAI Jonathan Ho, Jonas Schneider, Ilya Sutskever,
Pieter Abbeel, and Wojciech Zaremba. 2017. One-
shot imitation learning. In Advances in Neural In-
formation Processing Systems 30, pages 1087–1098.

Matt Gardner, Partha Talukdar, Jayant Krishnamurthy,
and Tom Mitchell. 2014. Incorporating vector space
similarity in random walk inference over knowledge
bases. In Proceedings of the 2014 Conference on
Empirical Methods in Natural Language Process-
ing, pages 397–406.

Matt Gardner, Partha Pratim Talukdar, Bryan Kisiel,
and Tom Mitchell. 2013. Improving learning and in-
ference in a large knowledge-base using latent syn-
tactic cues. In Proceedings of the 2013 Conference

on Empirical Methods in Natural Language Pro-
cessing, pages 833–838.

Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza,
Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron
Courville, and Yoshua Bengio. 2014. Generative ad-
versarial nets. In Advances in Neural Information
Processing Systems 27, pages 2672–2680.

Ishaan Gulrajani, Faruk Ahmed, Martin Arjovsky, Vin-
cent Dumoulin, and Aaron C Courville. 2017. Im-
proved training of wasserstein gans. In Proceedings
of the 31st International Conference on Neural In-
formation Processing Systems, pages 5767–5777.

Kelvin Guu, John Miller, and Percy Liang. 2015.
Traversing knowledge graphs in vector space. In
Proceedings of the 2015 Conference on Empirical
Methods in Natural Language Processing, pages
318–327.

Jonathan Ho and Stefano Ermon. 2016. Generative ad-
versarial imitation learning. In Advances in Neu-
ral Information Processing Systems 29, pages 4565–
4573.

Yoon Kim. 2014. Convolutional neural networks for
sentence classification. In Proceedings of the 2014
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 1746–1751.

Diederik P Kingma and Jimmy Ba. 2014. Adam: A
method for stochastic optimization. arXiv preprint
arXiv:1412.6980.

Ni Lao, Tom Mitchell, and William W Cohen. 2011.
Random walk inference and learning in a large s-
cale knowledge base. In Proceedings of the Con-
ference on Empirical Methods in Natural Language
Processing, pages 529–539.

Bonan Min, Ralph Grishman, Li Wan, Chang Wang,
and David Gondek. 2013. Distant supervision for
relation extraction with an incomplete knowledge
base. In Proceedings of the 2013 Conference of
the North American Chapter of the Association for
Computational Linguistics: Human Language Tech-
nologies, pages 777–782.

Arvind Neelakantan, Benjamin Roth, and Andrew Mc-
Callum. 2015. Compositional vector space models
for knowledge base inference. In 2015 aaai spring
symposium series.

Andrew Y Ng, Stuart J Russell, et al. 2000. Algorithms
for inverse reinforcement learning. In Proceedings
of the Seventeenth International Conference on Ma-
chine Learning, pages 663–670.

Maximilian Nickel, Volker Tresp, and Hans-Peter
Kriegel. 2011. A three-way model for collective
learning on multi-relational data. In Proceedings of
the 28th International Conference on International
Conference on Machine Learning, volume 11, pages
809–816.



2651

Sebastian Riedel, Limin Yao, Andrew McCallum, and
Benjamin M Marlin. 2013. Relation extraction with
matrix factorization and universal schemas. In Pro-
ceedings of the 2013 Conference of the North Amer-
ican Chapter of the Association for Computational
Linguistics: Human Language Technologies, pages
74–84.

Stuart Russell. 1998. Learning agents for uncertain en-
vironments. In Proceedings of the eleventh annual
conference on Computational learning theory, pages
101–103.

John Schulman, Sergey Levine, Pieter Abbeel, Michael
Jordan, and Philipp Moritz. 2015. Trust region pol-
icy optimization. In International Conference on
Machine Learning, pages 1889–1897.

Baoxu Shi and Tim Weninger. 2018. Open-world
knowledge graph completion. In Thirty-Second
AAAI Conference on Artificial Intelligence.

Richard Socher, Danqi Chen, Christopher D Manning,
and Andrew Ng. 2013. Reasoning with neural ten-
sor networks for knowledge base completion. In
Advances in Neural Information Processing Systems
26, pages 926–934.

Fabian M Suchanek, Gjergji Kasneci, and Gerhard
Weikum. 2007. Yago: a core of semantic knowl-
edge. In Proceedings of the 16th international con-
ference on World Wide Web, pages 697–706.

Kristina Toutanova, Danqi Chen, Patrick Pantel, Hoi-
fung Poon, Pallavi Choudhury, and Michael Gamon.
2015. Representing text for joint embedding of text
and knowledge bases. In Proceedings of the 2015
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 1499–1509.

Kristina Toutanova, Victoria Lin, Wen-tau Yih, Hoi-
fung Poon, and Chris Quirk. 2016. Compositional
learning of embeddings for relation paths in knowl-
edge base and text. In Proceedings of the 54th An-
nual Meeting of the Association for Computational
Linguistics, pages 1434–1444.

Théo Trouillon, Christopher R Dance, Éric Gaussier,
Johannes Welbl, Sebastian Riedel, and Guillaume
Bouchard. 2017. Knowledge graph completion vi-
a complex tensor factorization. The Journal of Ma-
chine Learning Research, 18(1):4735–4772.

Denny Vrandečić and Markus Krötzsch. 2014. Wiki-
data: a free collaborative knowledgebase. Commu-
nications of the ACM, 57(10):78–85.

Ronald J Williams. 1992. Simple statistical gradient-
following algorithms for connectionist reinforce-
ment learning. Machine learning, 8(3-4):229–256.

Wenhan Xiong, Thien Hoang, and William Yang
Wang. 2017. Deeppath: A reinforcement learning
method for knowledge graph reasoning. In Proceed-
ings of the 2017 Conference on Empirical Methods
in Natural Language Processing, pages 564–573.

Bishan Yang, Wen-tau Yih, Xiaodong He, Jianfeng
Gao, and Li Deng. 2014. Embedding entities and
relations for learning and inference in knowledge
bases. arXiv preprint arXiv:1412.6575.


