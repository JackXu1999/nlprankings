



















































Role-specific Language Models for Processing Recorded Neuropsychological Exams


Proceedings of NAACL-HLT 2018, pages 746–752
New Orleans, Louisiana, June 1 - 6, 2018. c©2018 Association for Computational Linguistics

Role-specific Language Models for
Processing Recorded Neuropsychological Exams

Tuka Alhanai†, Rhoda Au‡§, and James Glass†
†Computer Science and Artificial Intelligence Laboratory,

Massachusetts Institute of Technology, Cambridge MA, USA
‡Departments of Anatomy & Neurobiology, Neurology, and Epidemiology,
Boston University School of Medicine and Public Health, Boston MA, USA

§The Framingham Heart Study, Framingham MA, USA
tuka@mit.edu, rhodaau@bu.edu, glass@mit.edu

Abstract

Neuropsychological examinations are an im-
portant screening tool for the presence of cog-
nitive conditions (e.g. Alzheimer’s, Parkin-
son’s Disease), and require a trained tester to
conduct the exam through spoken interactions
with the subject. While audio is relatively
easy to record, it remains a challenge to auto-
matically diarize (who spoke when?), decode
(what did they say?), and assess a subject’s
cognitive health. This paper demonstrates a
method to determine the cognitive health (im-
paired or not) of 92 subjects, from audio that
was diarized using an automatic speech recog-
nition system trained on TED talks and on the
structured language used by testers and sub-
jects. Using leave-one-out cross validation
and logistic regression modeling we show that
even with noisily decoded data (81% WER)
we can still perform accurate enough diariza-
tion (0.02 % confusion rate) to determine the
cognitive state of a subject (0.76 AUC).

1 Introduction

Cognitive impairment is a decline in mental abil-
ities that is severe enough to interfere with daily
life (Nussbaum and Ellis, 2003). Such conditions
are particularly debilitating, with costs of up to
$200 billion in the USA alone (Prince et al., 2011;
Leifer, 2003; Alzheimers, 2015), and come second
only to spinal-cord injuries and terminal cancer in
the severity of their effects (Organization, 2003;
Ferri et al., 2006).

Several methods exist to screen for cognitive
conditions (e.g. Alzheimer’s, Parkinson’s), rang-
ing from laboratory measures to brain imaging
scans (Quadri et al., 2004; Van Himbergen et al.,
2012), with the baseline being set by neuropsy-
chological examinations. These exams are com-
posed of multiple components that measure a spe-
cific domain of cognition such as: thinking, recall,

speech, and physical movement. Each exam com-
ponent is assigned a score by the tester according
to the established rubric. While this exam can be
comprehensive, there is an additional dimension
of information that can be passively recorded - the
audio of the spoken interactions. Utilizing such
data would allow for the identification of spoken
language biomarkers of cognitive impairment.

However, with richer information comes addi-
tional complexity (Fitch et al., 2016). The appli-
cation of automatic speech processing technolo-
gies to medical domains requires a pipeline with
multiple stages. Such a system requires audio
pre-processing to locate speech and speaker seg-
ments (i.e. diarization) (Anguera et al., 2012), the
transcription of spoken utterances (Besacier et al.,
2014), and feature representation and modeling of
the speaker’s latent condition to determine disease
biomarkers for classification purposes (Cummins
et al., 2015).

Research in this domain can be categorized
into two areas. First is the utilization of acous-
tic and linguistic information to perform speaker
diarization and verification using standard cor-
pora (e.g. Switchboard, NIST) (Stolcke et al.,
2006; Reynolds et al., 2003). The second category
of work seeks to evaluate speech and language
biomarkers for the detection of cognitive impair-
ment utilizing measures such as speaking rate,
pauses, n-grams, and Word Error Rates (WERs)
(Pakhomov et al., 2010; Lehr et al., 2012; Fraser
et al., 2014; Pakhomov and Hemmy, 2014; Vincze
et al., 2016), as well as Automatic Speech Recog-
nition (ASR) for phonetic alignment and acoustic
feature extraction (Tóth et al., 2015). However,
systems from the speech community are devel-
oped using well-curated data with healthy speak-
ers, while the clinical community develops sys-
tems using manually transcribed data, with some
exceptions (Tóth et al., 2015; Weiner et al., 2016).

746



Our paper seeks to bridge the two areas by au-
tomating data curation for clinical use.

We hypothesize that it is possible to automate
data curation for clinical use by conditioning on
speaker roles, because speakers (subject/tester)
during neuropsychological exams have different
word usage and speaking patterns due to the ques-
tion and answer nature of the evaluation. We
also hypothesize that not all segments of the exam
will be equally valuable in evaluating for cognitive
conditions, due to potential confusion between
speakers when automatically annotating speaker
segments, polluting the features used for model-
ing cognitive conditions.

Our study differentiates itself from prior work
by combining speaker-specific language modeling
and ASR for speaker diarization, with the ulti-
mate goal of assessing the cognitive condition of
the subjects using the acoustic information con-
tained in the hypothesized (and less than ideal)
segments. This is an extension of work by Alhanai
et al. that used gold standard speaker segmenta-
tions and transcriptions to evaluate cognitive out-
comes. Further details on feature selection, mod-
eling, and the relation to previous work in that do-
main are described in (Alhanai et al., 2017). This
approach captures real-world scenarios where au-
tomatically diarized and transcribed data may not
be at human parity but its usage is necessary for
deploying screening technologies at scale. More-
over, audio recordings are often sub-optimal, us-
ing digital recorders on a desk, which is the case
of the data used in this study. Therefore the ability
to detect cognitive conditions must accommodate
the presence of noisy data, of which we sought to
evaluate.

1.1 Objectives

Our objectives were to (1) automatically extract
and identify segments of speech that were most
likely to belong to the subject, and (2) to evalu-
ate the type of segments that were most predictive
of a subject’s cognitive condition.

2 Methods

2.1 Data

The data used in this work was collected from the
Framingham Heart Study, an on-going longitudi-
nal population study of 15,447 subjects from 1948
to the present (Mahmood et al., 2014). Since 1999
a subset of subjects have undergone neuropsycho-

logical examinations (Satizabal et al., 2016), and
as of 2005, it became standard to record audio of
these examinations. The neuropsychological ex-
aminations include multiple components to assess
memory, attention, executive function, language,
reasoning, visuoperceptual skills, and premorbid
intelligence. All participants provided written in-
formed consent, with study protocols and consent
forms approved by the institutional review board
at the Boston University Medical Center.

Our study used 92 mono-channel audio record-
ings of neuropsychological examinations that had
available text transcripts. The exams were com-
posed of several tests measuring memory, recall,
logical and thinking. Further details and a full ex-
ample are found in (Satizabal et al., 2016). The
recordings were on average, 65 minutes in dura-
tion, contained 2,496 words, with a vocabulary
size of 527 words.

Transcripts for each audio file were generated
manually. Transcribers were instructed to include
timestamps for each speaker turn (subject/tester),
indicate who spoke when, transcribe speech ortho-
graphically (e.g. nineteen dollars instead of $19),
include tags to highlight moments such as filled
pauses (<um>), and to insert punctuation.

2.2 Outcome of Interest

Our overarching goal was to determine whether
the subject being evaluated was cognitively im-
paired, but we also needed to determine who spoke
when (subject or tester). To this end, we modeled
two levels of outcomes. Our first outcome of in-
terest was a binary indicator of the speaker type
(subject or tester), with the subject coded as 1.

Our second outcome of interest was a binary
indicator of cognitive impairment, with impair-
ment coded as 1. We labeled subjects as cogni-
tively impaired if the date of impairment (as con-
cluded by the dementia diagnostic review panel
(Seshadri et al., 2006) was on or before the date
of the neuropsychological examination where the
audio recording took place. Using this criteria, 21
subjects (22.8%) were cognitively impaired. Ten
of these subjects had a severity rating less than
mild, six were mild, five were moderate, and none
were severe . Fourteen subject were diagnosed as
having Alzheimer’s disease using the NINCDS-
ADRDA criteria (McKhann et al., 2011), and
five were diagnosed with Vascular dementia based
on the NINCDS-AIRENS criteria (Román et al.,

747



1993).

2.3 Model Choice and Evaluation Metrics

To evaluate speaker diarization we used the Di-
arization Error Rate (DER) metric, as well as
the percentage of speech classified as non-speech
(Miss), the percentage of non-speech classified
as speech (False Alarm), and the percentage of
speech misclassified as belonging to the other
speaker (Confusion Rate) (Tranter and Reynolds,
2006). We used a time-based diarization ap-
proach, ignoring segments less than 250ms in du-
ration. To evaluate the performance of the ASR
system we used the Word Error Rate (WER) met-
ric. Given the importance of model interpretability
for detecting spoken language biomarkers, logis-
tic regression was chosen as our modeling frame-
work. The evaluation metrics we used for detect-
ing cognitive impairment was the Area Under the
Receiver Operating Characteristic Curve (AUC)
which has the advantage of evaluating model per-
formance across the whole range of probability
cutoffs, rather than a single point estimate such as
accuracy or F1 score (Huang and Ling, 2005). To
assess the generalizability and robustness of our
modeling techniques, we performed leave-one-out
cross-validation.

3 Experiment 1: Speaker ID from Text

We first investigated the language patterns of
speakers to determine whether a subject or tester
was speaking (i.e., a 2 class problem). We started
with the segmentation from the speaker turns la-
beled in the transcripts. We trained a trigram lan-
guage model with Knesser Ney discounting for
each speaker type. The language models were
then used to generate the language perplexity of
the spoken (text) segment. The training and test-
ing was performed with leave-one-out validation
(i.e. 92 folds, one fold for each of the 92 subject-
tester interactions). Six features were used in the
logistic regression model:

• OOV-rate (x2): The Out-of-Vocabulary rate
of the subjects’ and testers’ vocabulary (from
their respective training sets).

• Perplexity (x2): The language model per-
plexity for the subjects and testers.

• Perplexity sans <s> (x2): The language
model perplexity for the subjects and testers,

excluding the start and end of sentence tags
(<s>,</s>).

This resulted in a classification accuracy of 84%
(±0.06), and an AUC of 0.93 (±0.07) These re-
sults motivated further investigation into classify-
ing speakers from the audio directly.

4 Experiment 2: Speaker ID from ASR

For this experiment, we decoded the audio using
an Automatic Speech Recognition (ASR) system
with a language model trained on each speaker
(subject/tester), and an acoustic model trained on
the TEDLIUM corpus. Each component of the
ASR system was developed as follows:

• Acoustic Model: The TEDLIUM corpus
contains over 1,400 audio recordings and text
transcription of TED talks, for a total of 120
hours of data and 1.7M words (Rousseau
et al., 2012). Using this corpus, we trained
the acoustic model as a feedforward Neu-
ral Network (6 layers x 2048 hidden units)
with the Minimum Bayes Risk (MBR) cri-
terion using 40 mel filterbank features, via
the Kaldi speech recognition toolkit using the
‘s5’ TEDLIUM recipe (Povey et al., 2011;
Rousseau et al., 2012).

• Language Model: A tri-gram language
model was trained for each of the speaker and
tester using the SRILM toolkit (Stolcke et al.,
2002).

• Lexicon: We generated the word pronuncia-
tions using the LOGIOS lexical tool1.

We decoded the audio in three ways:

1. Oracle: A language model was trained
across all 92 transcripts, and utterances were
segmented according to manually generated
speaker turns.

2. Leave-one-out: A language model was
trained on all transcripts excluding the tran-
script of the audio being decoded. Utterances
were segmented according to manually gen-
erated speaker turns.

3. Leave-one-out + automatic segmentation:
A language model was trained on all tran-
scripts excluding the transcript of the audio

1http://www.speech.cs.cmu.edu/tools/
lextool.html

748



being decoded. Utterances were not seg-
mented by speaker turns, the full audio was
decoded as a single segment.

The results are displayed in Table 4. Our Oracle
system performed with a WER of 66.7%, while
decoding without language modeling information
(of the audio being decoded) resulted in a WER of
68.6%. This relatively small difference in perfor-
mance (68.6% vs. 66.7%) indicated that the lan-
guage usage across the audio recordings was con-
sistent.

We also compared the Diariziation Error Rate
(DER) across the different setups (Table 4). This
helped us evaluate how well a speaker could
be identified given various levels of information
about the underlying segments being decoded.

ALL SEGMENTS

Oracle Loocv Loocvauto seg.
WER (%) 66.7 68.6 81.3
DER (%) 35.8 (± 5.9) 37.2 (± 5.5) 40.5 (± 05.4)

Miss 00.2 (± 0.4) 00.2 (± 0.4) 00.2 (± 15.3)
False Alarm 03.9 (± 1.2) 04.1 (± 1.3) 03.7 (± 01.3)
Confusion 31.7 (± 5.9) 32.9 (± 5.5) 36.7 (± 05.3)

Cognitive ID
AUC 0.72 0.70 0.68

OPTIMUM SEGMENTS
95% subj.

& 10+ words
Top 9

longest
DER (%) - 98.2 (± 1.6) 99.9 (± 0.2)

Miss - 97.9 (± 2.1) 99.9 (± 0.4)
False Alarm - 00.0 (± 0.0) 00.0 (± 0.0)
Confusion - 00.3 (± 0.6) 0.02 (± 0.2)

Cognitive ID
AUC - 0.75 0.76

Table 1: ASR, Speaker ID, and Cognitive ID

5 Experiment 3: Cognitive ID

Using the classified speaker segments, we were
interested in determining the subject’s cognitive
condition (impaired or not). We modeled each
segment using logistic regression and 220 acoustic
features capturing prosody (pitch, zero-crossing
rate, jitter, harmonic-to-noise ration) and energy
in the speech (energy, spectral energy, shimmer).
Full details on the acoustic feature set, and method
for extraction can be found in (Alhanai et al.,
2017). To calculate model performance, we took
the mean predicted probability across all seg-
ments as a single value representing the probabil-
ity of a subject’s cognitive impairment. For this

experiment, we performed leave-one-out cross-
validation.

5.0.1 Speaker Turn Segmentations
For the experimental setup that used segmenta-
tions by speaker turn, we modeled cognitive im-
pairment within a grid search space along two di-
mensions: (a) the total number of words that were
decoded, and (b) by the percentage of words de-
coded that were hypothesized to belong to the sub-
ject. The results of evaluating cognitive impair-
ment in this search space can be viewed in Figure
1. The highest AUC (of 0.75) was found when
modeling with segments that had been decoded
with at least 10 words, and 95% of which were
hypothesized to belong to the subject.

AUC Heatmap

0.5 0.55 0.6 0.65 0.7 0.75 0.8 0.85 0.9 0.95 1

% of words decoded as subject`s

10

9

8

7

6

5

4

3

2

1

# 
of

 w
or

ds
 d

ec
od

ed

0.66

0.67

0.68

0.69

 0.7

0.71

0.72

0.73

0.74

0.75

Figure 1: Cognitive ID Heatmap. Heatmap of Subject
AUC across two thresholds, (y-axis) minimum number
of words decoded, and (x-axis) percentage of words in
a segment classified as the subject’s.

5.0.2 Discarding Speaker Segmentations
For the experimental setup that was decoded with-
out oracle speaker turn segmentation, we first seg-
mented the decoded hypothesis along silences that
were longer than 1.5 seconds, and then segmented
according to the hypothesized speaker. For mod-
eling, we selected the top N longest segments that
were hypothesized to be the subject’s, where N
was evaluated from 1 to 15. 99% of segments hy-
pothesized were under 25 seconds in duration, and
as a pre-processing step we discarded the longest
1% of hypothesized segments, which were many
minutes long and several standard deviations be-
yond the mean (i.e. spurious decodings). The
highest AUC (of 0.76) was found when modeling
the 9 longest segments hypothesized as the sub-

749



ject’s. This was an average of 150 seconds (±20
sec) of audio per subject, or 7% of a subject’s total
audio duration.

0 5 10 15

number of subject segments

0.5

0.55

0.6

0.65

0.7

0.75

0.8

A
U

C

auto seg.
oracle

Figure 2: Cognitive ID by Number of Segments. Plot
of AUC (y-axis) with respect to the number of seg-
ments per-subject (by descending order of length) used
for modeling their cognitive impairment (x-axis). Red
points indicate best performance (AUC 0.72 and 0.75
for oracle and automatic segmentation systems respec-
tively).

6 Discussion

Utilizing audio recordings of spoken interactions
between subjects and tester, the work in this paper
sought to: (1) automatically extract and identify
segments of speech that were most likely to be-
long to the subject, and (2) to evaluate the type of
segments that were most predictive of a subject’s
cognitive condition.

6.1 Experiment 1: Speaker ID from Text

Our results from the first experiment showed that
language usage between the subject and tester dif-
fered significantly, and that each speaker’s lan-
guage style was consistent across recordings (i.e.
subjects consistently spoke like other subjects,
and testers consistently spoke like other testers).
Therefore, with the availability of highly accurate
transcriptions of the same structure (neuropsy-
chological exams), a highly accurate text-based
speaker diarization can be conducted.

6.2 Experiment 2: Speaker ID from ASR

Our second set of experiments validated the obser-
vation from the previous experiment on language
usage patterns across speaker roles (i.e. subjects

consistently spoke like other subjects, testers con-
sistently spoke like other testers, and subjects and
testers did not speak like each other). Also, seem-
ingly high WERs (between 66.7% and 81.3%) still
contained information that was robust enough for
further usage in diarization and modeling of cog-
nitive impairment.

6.3 Experiment 3: Cognitive ID
Our last experiment showed that it was possible to
perform modeling of cognitive impairment utiliz-
ing automatically segmented subject speaker turns
that was on par with the oracle speaker segmenta-
tion, and that 9 segments was sufficient for evalu-
ation. As shown in Figure 2, we also found that
not all diarization was equal, nor were all seg-
ment lengths equally powerful at modeling sub-
jects’ cognitive state. In the case where no or-
acle segmentation was available, and automatic
segmentation was utilized, longer segments con-
tained information that was more discriminative
(AUC 0.68 vs. 0.76). For the oracle system, the
longest system was the most and equally predic-
tive of cognitive impairment, as all segments taken
together. This highlights that tests that elicited
longer responses allowed for more robust diariza-
tion, were evaluating cognitive performance that
was (via speech) most strongly associated with the
outcome, and/or that longer spoken segments pro-
vided more opportunity to capture patterns associ-
ated with cognitive impairment.

Furthermore, the modeling paradigm we ex-
plored was robust enough that neither the under-
lying neuropsychological test need be explicitly
modeled (Lehr et al., 2012), nor do the features
utilized require word or phone alignments (align-
ments which require accurate transcriptions in or-
der to generate) (Tóth et al., 2015).

Acknowledgments

The authors thank David Stuck, Maggie San-
doval, Alessio Signorini, and Christine Lemke
from Evidation Health Inc., and Ida Xu, Brynna
Wasserman, Maulika Kohli, Nancy Heard-Costa,
Yulin Liu, Karen Mutalik, Mia Lavallee, Christina
Nowak, Alvin Ang, and Spencer Hardy from
Boston University and The Framingham Heart
Study. Data curation was sponsored by
DARPA and NIH grants AG016495, AG008122,
AG033040. T. Alhanai thanks the Abu Dhabi Ed-
ucation Council for sponsoring her studies.

750



References
Tuka Alhanai, Rhoda Au, and James Glass. 2017. Spo-

ken language biomarkers for detecting cognitive im-
pairment.

Association Alzheimers. 2015. 2015 alzheimer’s dis-
ease facts and figures. Alzheimer’s & dementia: the
journal of the Alzheimer’s Association 11(3):332.

Xavier Anguera, Simon Bozonnet, Nicholas Evans,
Corinne Fredouille, Gerald Friedland, and Oriol
Vinyals. 2012. Speaker diarization: A review of re-
cent research. IEEE Transactions on Audio, Speech,
and Language Processing 20(2):356–370.

Laurent Besacier, Etienne Barnard, Alexey Karpov,
and Tanja Schultz. 2014. Automatic speech recog-
nition for under-resourced languages: A survey.
Speech Communication 56:85–100.

Nicholas Cummins, Stefan Scherer, Jarek Krajewski,
Sebastian Schnieder, Julien Epps, and Thomas F
Quatieri. 2015. A review of depression and suicide
risk assessment using speech analysis. Speech Com-
munication 71:10–49.

Cleusa P Ferri, Martin Prince, Carol Brayne, Henry
Brodaty, Laura Fratiglioni, Mary Ganguli, Kath-
leen Hall, Kazuo Hasegawa, Hugh Hendrie, Yue-
qin Huang, et al. 2006. Global prevalence of de-
mentia: a delphi consensus study. The lancet
366(9503):2112–2117.

W Tecumseh Fitch, Bart de Boer, Neil Mathur, and
Asif A Ghazanfar. 2016. Monkey vocal tracts are
speech-ready. Science advances 2(12):e1600723.

Kathleen C Fraser, Jed A Meltzer, Naida L Graham,
Carol Leonard, Graeme Hirst, Sandra E Black, and
Elizabeth Rochon. 2014. Automated classification
of primary progressive aphasia subtypes from narra-
tive speech transcripts. cortex 55:43–60.

Jin Huang and Charles X Ling. 2005. Using auc and
accuracy in evaluating learning algorithms. IEEE
Transactions on knowledge and Data Engineering
17(3):299–310.

Maider Lehr, Emily Prud’hommeaux, Izhak Shafran,
and Brian Roark. 2012. Fully automated neuropsy-
chological assessment for detecting mild cognitive
impairment. In Thirteenth Annual Conference of the
International Speech Communication Association.

Bennett P Leifer. 2003. Early diagnosis of alzheimer’s
disease: clinical and economic benefits. Journal of
the American Geriatrics Society 51(5s2).

Syed S Mahmood, Daniel Levy, Ramachandran S
Vasan, and Thomas J Wang. 2014. The framing-
ham heart study and the epidemiology of cardiovas-
cular disease: a historical perspective. The Lancet
383(9921):999–1008.

Guy M McKhann, David S Knopman, Howard
Chertkow, Bradley T Hyman, Clifford R Jack, Clau-
dia H Kawas, William E Klunk, Walter J Ko-
roshetz, Jennifer J Manly, Richard Mayeux, et al.
2011. The diagnosis of dementia due to alzheimers
disease: Recommendations from the national in-
stitute on aging-alzheimers association workgroups
on diagnostic guidelines for alzheimer’s disease.
Alzheimer’s & dementia 7(3):263–269.

Robert L Nussbaum and Christopher E Ellis. 2003.
Alzheimer’s disease and parkinson’s disease. New
England Journal of Medicine 348(14):1356–1364.

World Health Organization. 2003. The world health
report 2003: shaping the future. World Health Or-
ganization.

Serguei VS Pakhomov and Laura S Hemmy. 2014. A
computational linguistic measure of clustering be-
havior on semantic verbal fluency task predicts risk
of future dementia in the nun study. Cortex 55:97–
106.

Serguei VS Pakhomov, Glenn E Smith, Susan Marino,
Angela Birnbaum, Neill Graff-Radford, Richard
Caselli, Bradley Boeve, and David S Knopman.
2010. A computerized technique to assess language
use patterns in patients with frontotemporal demen-
tia. Journal of neurolinguistics 23(2):127–144.

Daniel Povey, Arnab Ghoshal, Gilles Boulianne, Lukas
Burget, Ondrej Glembek, Nagendra Goel, Mirko
Hannemann, Petr Motlicek, Yanmin Qian, Petr
Schwarz, et al. 2011. The kaldi speech recogni-
tion toolkit. In IEEE 2011 workshop on automatic
speech recognition and understanding. IEEE Signal
Processing Society, EPFL-CONF-192584.

Martin Prince, Renata Bryce, and Cleusa Ferri. 2011.
World Alzheimer Report 2011: The benefits of early
diagnosis and intervention. Alzheimer’s Disease In-
ternational.

Pierluigi Quadri, Claudia Fragiacomo, Rita Pezzati,
Enrica Zanda, Gianluigi Forloni, Mauro Tettamanti,
and Ugo Lucca. 2004. Homocysteine, folate, and vi-
tamin b-12 in mild cognitive impairment, alzheimer
disease, and vascular dementia. The American jour-
nal of clinical nutrition 80(1):114–122.

Douglas Reynolds, Walter Andrews, Joseph Campbell,
Jiri Navratil, Barbara Peskin, Andre Adami, Qin Jin,
David Klusacek, Joy Abramson, Radu Mihaescu,
et al. 2003. The supersid project: Exploiting high-
level information for high-accuracy speaker recog-
nition. In Acoustics, Speech, and Signal Processing,
2003. Proceedings.(ICASSP’03). 2003 IEEE Inter-
national Conference on. IEEE, volume 4, pages IV–
784.

Gustavo C Román, Thomas K Tatemichi, T Erkinjuntti,
JL Cummings, JC Masdeu, JHet al Garcia, L Ama-
ducci, J-M Orgogozo, A Brun, Aea Hofman, et al.

751



1993. Vascular dementia diagnostic criteria for re-
search studies: Report of the ninds-airen interna-
tional workshop. Neurology 43(2):250–250.

Anthony Rousseau, Paul Deléglise, and Yannick Es-
teve. 2012. Ted-lium: an automatic speech recog-
nition dedicated corpus. In LREC. pages 125–129.

Claudia L Satizabal, Alexa S Beiser, Vincent Chouraki,
Geneviève Chêne, Carole Dufouil, and Sudha Se-
shadri. 2016. Incidence of dementia over three
decades in the framingham heart study. New Eng-
land Journal of Medicine 374(6):523–532.

Sudha Seshadri, Alexa Beiser, Margaret Kelly-Hayes,
Carlos S Kase, Rhoda Au, William B Kannel, and
Philip A Wolf. 2006. The lifetime risk of stroke.
Stroke 37(2):345–350.

Andreas Stolcke, Klaus Ries, Noah Coccaro, Eliza-
beth Shriberg, Rebecca Bates, Daniel Jurafsky, Paul
Taylor, Rachel Martin, Carol Van Ess-Dykema, and
Marie Meteer. 2006. Dialogue act modeling for au-
tomatic tagging and recognition of conversational
speech. Dialogue 26(3).

Andreas Stolcke et al. 2002. Srilm-an extensible lan-
guage modeling toolkit. In Interspeech. volume
2002, page 2002.

Laszló Tóth, Gábor Gosztolya, Veronika Vincze, Ildikó
Hoffmann, Gréta Szatlóczki, Edit Biró, Fruzsina
Zsura, Magdolna Pákáski, and János Kálmán. 2015.
Automatic detection of mild cognitive impairment
from spontaneous speech using asr. In Sixteenth An-
nual Conference of the International Speech Com-
munication Association.

Sue E Tranter and Douglas A Reynolds. 2006. An
overview of automatic speaker diarization systems.
IEEE Transactions on audio, speech, and language
processing 14(5):1557–1565.

Thomas M Van Himbergen, Alexa S Beiser, Masumi
Ai, Sudha Seshadri, Seiko Otokozawa, Rhoda Au,
Nuntakorn Thongtang, Philip A Wolf, and Ernst J
Schaefer. 2012. Biomarkers for insulin resistance
and inflammation and the risk for all-cause dementia
and alzheimer disease: results from the framingham
heart study. Archives of neurology 69(5):594–600.

Veronika Vincze, Gabor Gosztolya, Laszlo Toth, Ildiko
Hoffmann, Greta Szatloczki, Zoltan Banreti, Mag-
dolna Pakaski, and Janos Kalman. 2016. Detecting
mild cognitive impairment by exploiting linguistic
information from transcripts. In Proceedings of the
54th Annual Meeting of the Association for Compu-
tational Linguistics. volume 2, pages 181–187.

Jochen Weiner, Christian Herff, and Tanja Schultz.
2016. Speech-based detection of alzheimer’s dis-
ease in conversational german. In INTERSPEECH.
pages 1938–1942.

752


