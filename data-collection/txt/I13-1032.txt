








































Tuning SMT with A Large Number of Features via Online Feature
Grouping

Lemao Liu1, Tiejun Zhao1, Taro Watanabe2, Eiichiro Sumita2
1School of Computer Science and Technology
Harbin Institute of Technology, Harbin, China

2National Institute of Information and Communications Technology
3-5 Hikari-dai, Seika-cho, Soraku-gun, Kyoto, Japan
{lmliu|tjzhao}@mtlab.hit.edu.cn

{taro.watanabe|eiichiro.sumita}@nict.go.jp

Abstract

In this paper, we consider the tuning of sta-
tistical machine translation (SMT) mod-
els employing a large number of features.
We argue that existing tuning methods for
these models suffer serious sparsity prob-
lems, in which features appearing in the
tuning data may not appear in the test-
ing data and thus those features may be
over tuned in the tuning data. As a result,
we face an over-fitting problem, which
limits the generalization abilities of the
learned models. Based on our analysis, we
propose a novel method based on feature
grouping via OSCAR to overcome these
pitfalls. Our feature grouping is imple-
mented within an online learning frame-
work and thus it is efficient for a large
scale (both for features and examples) of
learning in our scenario. Experiment re-
sults on IWSLT translation tasks show that
the proposed method significantly outper-
forms the state of the art tuning methods.

1 Introduction

Since the introduction of log-linear based SMT
(Och and Ney, 2002), tuning has been a hot topic.
Various methods have been explored: their objec-
tives are either error rates (Och, 2003), hinge loss
(Watanabe et al., 2007; Chiang et al., 2008) or
ranking loss (Hopkins and May, 2011), and they
are either batch training or online training meth-
ods. In this paper, we consider tuning translation
models with a large number of features such as
lexical, n-gram level and rule level features, where
the number of features is largely greater than the
number of bilingual sentences. Practically, exist-
ing tuning methods such as PRO and MIRA might

This joint work was done while the first author visited
NICT.

be applied in our scenario, however, they will suf-
fer from some pitfalls as well, which have been
less investigated in previous works.

One of pitfalls is that these features are so sparse
that many features which are potentially useful for
a test set may not be included in a given tuning set,
and many useless features for testing will be over
tuned on the developement set meanwhile. As a
result, the generalization abilities of features are
limited due to the mismatch between the testing
data and the tuning data, and over-fitting occurs.
One practice is to tune translation models on a
larger tuning set, such as the entire training data
(Xiao et al., 2011; Simianer et al., 2012), in the
hope that more features would be included during
tuning. However, tuning robust weights for trans-
lation models has additional requirements to a tun-
ing set. Firstly, multiple reference translations in
the tuning data are helpful for better tuning, es-
pecially when testing data contains multiple refer-
ence translations. Secondly, the closeness between
the tuning set and a test set is also important for
better testing performance (Li et al., 2010). These
requirements can explain why tuning on the train-
ing data leads to unsatisfactory performance on the
IWSLT translation task, as will be shown in our
experiments later. Therefore, enlarging a tuning
set is not always a sufficient solution for robust
tuning, since it would be impractical to create a
large scale tuning set with these requirements.

We propose a novel tuning method by grouping
a large number of features to leverage the above
pitfalls. Instead of directly taking the large num-
ber of atomic features into translation model, we
firstly learn their group structure on the training
data to alleviate their serious sparsity. Then, we
tune the translation model consisting of grouped
features on a multi-reference development set to
ensure robust tuning. Unlike unsupervised cluster-
ing methods such as k-means (MacQueen, 1967)
for feature clustering, we group the features with



the OSCAR (Octagonal Shrinkage and Clustering
Algorithm for Regression) method (Bondell and
Reich, 2008), which directly relates the objective
of feature grouping to translation evaluation met-
rics such as BLEU (Papineni et al., 2002) and
thus grouped features are optimized with respect
to BLEU. Due to the large number of features
and large number of training examples, efficient
grouping is not simple. We apply the online gradi-
ent projection method under the FOBOS (forward-
backward splitting) framework (Duchi and Singer,
2009) to accelerate feature grouping.

We employ a large number of features by treat-
ing each translation rule in a synchronous-CFG as
a single feature. Experiments on IWSLT Chinese-
to-English translation tasks show that, with the
help of grouping these features, our method can
overcome the above pitfalls and thus achieves sig-
nificant improvements.

2 Tuning Method

We propose a novel tuning method for translation
models with a large number of features, which in-
corporates feature grouping. Our assumption is
that although a feature which is useful for a test
set does not appear in the tuning set, another simi-
lar feature may exist. Therefore, grouping similar
features can alleviate sparsity in this way. The pro-
posed tuning method consists of two steps: first, it
tries to learn a group structure for atomic features;
second, it treats each feature group as a single fea-
ture and tunes the translation model on a given tun-
ing set using off-the-shelf toolkits such as PRO. In
the first step, we learn a group structure of atomic
features in the large training data for better cov-
erage. In the second step, we tune a translation
model with the grouped features on a given devel-
opment set with multiple references to ensure the
robust tuning.

Before describing our tuning algorithm, we
present notations for the rest of this paper.
Suppose H is a feature set consisting of
atomic features {h1, h2, · · · , hd} or their in-
dex set {1, 2, · · · , d} for simplicity; H =
〈h1, h2, · · · , hd〉 is a d-dimensional feature vector
function with respect to H , and W is its weight
vector with each component Wi and dimension
d; G = {g1, g2, ..gM} is a group of H , where
each element gi is a power set of H . Similarly,
G = 〈g1, g2, · · · , gM 〉 is an M -dimensional fea-
ture vector function with respect to G and W G is

its weight with with each component W Gi . In this
paper, we consider the disjoint G , i.e. gi ∩ gj = ∅
if i 6= j. Further, suppose ∆(W ) is a set of the
index i such thatWi 6= 0, and |·| is either the num-
ber of elements in a set S or the absolute value of
a real number x.

Algorithm 1 Tuning Algorithm
Input: training data, dev, W ini, T

1: Initialize W 1 = W ini

2: for all i such that 1 ≤ i ≤ T do
3: Decode on training data with W i to obtain

a k-best-list and merge k-best-lists
4: Update the group set G based on the

merged k-best-list . Call Algorithm 2
5: Tune the translation model with G as the

feature set on dev with PRO to update W G

6: Unpack W G to W i+1

7: end for
8: W = W T+1

Output: W

Algorithm 1 describes our two-step tuning pro-
cedure for a translation model with H as its fea-
ture set. It inputs a training data set, a development
set, initial weight W 1 with respect to H , and
maximal iterations T ; and outputs a weight W . It
initializes with W 1 in line 1; from line 2 to line
7, it iteratively obtains a k-best-list by decoding
with W i, updates the group set G , tunes the trans-
lation weight W G based on G , and unpacks the
W G to obtainW i+1. At the end, it returns the final
weight W . In particular, the k-best-list is obtained
using H as a feature vector with its weight vector
W derived from grouped weightsW G through un-
packing: if hj ∈ gk, then Wj=W Gk . The grouping
algorithm in line 3 will be introduced in the next
section.

In this paper, we use a hierarchical phrase based
translation model, which consists of 8 default fea-
tures: translation probabilities, lexical translation
probabilities, word penalty, glue rule penalty, syn-
chronous rule penalty and language model. In ad-
dition, we also employ a large number rule iden-
tify ( id ) features: each rule itself is a feature, and
if a translation contains a rule for x times, then the
value of this rule id feature is x. In line 4 we group
these id features and impose that each default fea-
ture itself is a group.

3 Online Feature grouping



Algorithm 2 Feature Grouping Algorithm
Input: λ1,λ2,k-best-list, W 1, n

1: Collect a set of tuples
{
〈f, e′, e∗〉

}
from k-

best-list
2: for all i such that 1 ≤ t ≤ n do
3: Randomly select 〈f, e′, e∗〉 from the tuple

set
4: W t+1/2 = W t +∇W δ(f, e′, e∗,W t)/t
5: Minimize Q(W ; 2W t+1/2, t+ 1, λ1, λ2) to

obtain (W t+1,G )
. Group optimization

6: end for
Output: G

Suppose f is a sentence in a development set, C is
a set of translations for f , and r is a set of refer-
ence translations for f . Following PRO, we define
ranking loss function as follows:

L(W ) =
1

N

∑
f

∑
e∗,e′

δ(f, e′, e∗,W ), (1)

with

δ(f, e′, e∗,W ) =

max
W

{(
H(f, e

′
)−H(f, e∗)

)
·W + 1, 0

}
,

where e′, e∗ ∈ C such that BLEU(e∗, r) >
BLEU(e′, r), and N is the number of all tuples
〈e∗, e′, f〉.

To achieve group structure and avoid the spar-
sity in H , we apply the OSCAR over the above
loss function, and obtain the function:

L(W )+λ1

d∑
i=1

|Wi|+λ2
∑

1≤i<j≤d
max{|Wi| , |Wj |},

(2)
where d is the dimension of feature vector H or
its weight W , λ1 and λ2 are two hyperparameters
for two regularizers taking positive value. Mini-
mization of Eq.2 makes some components in W
equal and thus achieves a feature grouping effect.
In other words, Wi = Wj means that hi and hj
lie in the same group, i.e. hi, hj ∈ gk for some
gk ∈ D(W ), where D(W ) denotes the group
derived from W as follows. Given W , we first
sort its components Wi to obtain a permutation
{ik}dk=1 such that Wi1 ≤ Wi2 · · · ≤ Wid with
1 ≤ ik ≤ d; then we can easily obtain D(W )
after traversing {Wik}dk=1. For example, W =

〈1, 3, 1, 3, 1〉, then D(W ) =
{
{1, 3, 5}, {2, 4}

}
.

One advantage of OSCAR over unsupervised clus-
tering methods (e.g. k-means) is that it relates the
objective of grouping to an error metric, such as
BLEU, and thus can achieve an optimal grouping
towards BLEU.

Bondell and Reich (2008) firstly proposed two
approaches for OSCAR. The first one casts the
problem into a quadratic program (QP) consist-
ing of O(d2) variables and O(d2) constraints. The
second one tries to optimize a sequence of (poten-
tially smaller) QP’s with more constraints, which
can be up to O(d!) in the worst case. Zhong
and Kwok (2011) explored a much faster approach
which is based on the accelerated gradient and
projection method. Its complexity is reduced to
O(d log d). Since the dimension d of H is large
enough in our scenario where d is up to hundred
of thousands, these existing optimization methods
are inefficient to minimize Eq.2. Here, based on
(Zhong and Kwok, 2011), we employ an online
gradient projection algorithm under the FOBOS
framework for faster learning. The framework of
FOBOS is a type of online learning, in which it
is theoretically guaranteed to solve such a prob-
lem as in Equation 2: the objectives consisting of
two additive terms, in which one is non-smooth
but convex and the other is smooth and convex1.
FOBOS contains two steps: it first performs a gra-
dient descent operator, and then updates weight by
a proximity (or projection) operator.

Algorithm 2 describes the online training of fea-
ture grouping. It requires some inputs: two regu-
larizer parameters λ1 and λ2; a k-best-list transla-
tions; an initial weight W 1; and a maximum iter-
ations n. It firstly collects a set of tuples encoded
with translation pairs from k-best-list following
the strategy implemented in the PRO toolkit in line
1. It repeatedly updates weight W t and feature
group G from line 2 to line 6: it randomly sam-
ples a tuple 〈f, e′, e∗〉 from the collected tuple set
in line 3, it performs a gradient descent operator in
line 4 where ∇W δ(f, e′, e∗,W t) denotes the sub-
gradient of δ(f, e′, e∗,W t) at current weight W t,
and it optimizes (W t+1,G ) by a proximity oper-
ator for group optimization in line 5. At last it
returns the group result G .

In particular, the subgradient of δ(f, e′, e∗,W t)

1For Eq.2, the non-smooth but convex term is the entire
of Eq.2, and the smooth and convex term can be considered
as 0.



in line 4 is defined via the following equation:

∇W δ(f, e′, e∗,W ) ={
H(f, e′)−H(f, e∗), if δ(f, e′, e∗,W ) > 0;

0, else.

The main technique is the proximity operator for
group optimization in line 5, which tries to mini-
mize the function Q(·; a, t, λ1, λ2) with a = 2 ×(
W t +∇W δ(f, e′, e∗,W t)/t

)
:

Q(W ; a, t, λ1, λ2) = (W − a)>W+

2

t

(
λ1

d∑
i=1

|Wi|+λ2
∑

1≤i<j≤d
max{|Wi| , |Wj |}

)
.

(3)

In the next Section, we will present the details of
this proximity for group optimization.

4 Group Optimization

To derive an efficient algorithm with large d
for group optimization, we present the following
lemma with its proof attached in appendix.

Lemma 1. In Eq.3, if ak=0, then its minimal so-
lution Ŵ suffices to Ŵk = 0.

Suppose W t is sparse, i.e. d is largely greater
than the number of its non-zero components
(|∆(W t)|), then W t+1/2 in line 4 is also sparse
since H is sparse. The above Lemma states that
the optimal solution W t+1 in line 5 of Algorithm
2 is also a sparse vector. Therefore, it is desirable
to optimize the W t+1 in a low complexity inde-
pendent on d. If so, we can easily see that if we set
W 1 as a sparse vector, W t is sparse for all t > 1
by a mathematical induction. Based on these anal-
ysis, the efficiency of proximity operator for group
optimization only requires an assumption that its
proximity step can be efficiently solved in a low
complexity independent on a large d.

Let u = |∆(a)|, and p be a one-to-one map2
p : {1, · · · , u} → {1, · · · , d}, s.t. |ap(1)| ≥
|ap(2)| ≥ · · · ≥ |ap(u)| > 0. Followed by Lemma
1, minimizing Eq.2 is equivalent to minimizing the
following equation if we ignore the zero compo-

2For easier understanding, i′(j′) denotes the index in
{1, · · · , u}, while i(j) denotes the index in {1, · · · , d}.

nents in the optimal solutions of both equations:

Q̄(W ; a, t, λ1, λ2) =
u∑

i′=1

W 2p(i′)−
u∑

i′=1

ap(i′)Wp(i′)

+

u∑
i′=1

2
(
λ1 + λ2(d− u)

)
t

|Wp(i′)|+

2λ2
t

∑
1≤i′<j′≤u

max{
∣∣Wp(i′)∣∣ , ∣∣Wp(j′)∣∣}.

The advantage of optimizing Q̄(W ; a, t, λ1, λ2)
instead of Q(W ; a, t, λ1, λ2) is that it explicitly
reduces the size of active components in W into
u rather than d, and thus it is more direct to
expect a faster optimization algorithm. Further,
Proposition 1 in (Zhong and Kwok, 2011) states
that the minimal solution Ŵ of such an equa-
tion as Q̄(W ; a, t, λ1, λ2) suffices to the constraint
|Ŵp(1)| ≥ · · · ≥ |Ŵp(u)|. Therefore, minimizing
Q(W ; a, t, λ1, λ2) is also equivalent to optimizing
the following constraint programming:

minimize
W

Q̄(W ; a, t, λ1, λ2)

subject to |Wp(1)| ≥ · · · ≥ |Wp(u)|,

where Q̄(W ; a, t, λ1, λ2) defined on the constraint
is rewritten as

Q̄(W ; a, t, λ1, λ2) =

u∑
i′=1

W 2p(i′)−
u∑

i′=1

ap(i′)Wp(i′)

+
u∑

i′=1

2
(
λ1 + λ2(d− i′)

)
t

|Wp(i′)|. (4)

Now, we can implement line 5 in Algorithm 2 as
summarized by Algorithm 3, after some modifica-
tions over the projection algorithm in (Zhong and
Kwok, 2011). Algorithm 3 requires some vari-
ables λ1, λ2, a and t. Firstly, it sorts |ai| for the
indice in ∆(a) to obtain the map p in line 1, and
initializes G as

{
{p(1)}

}
in line 2. From line

3 to line 10, it goes into a merging loop where
it repeatedly merges two group members to pre-
calculate G : for each i′, it iteratively merges the
member g initialized as {p(i′)} and the top mem-
ber in the stack, updates g with the merged mem-
ber, and substitutes the top member in the stack
with g, if the v value (will be defined later) of g
is greater than that of the top member. Then, it
begins to calculate W initialized as 0 and G . For
each index i in each member g of G , it assigns Wi



Algorithm 3 Group Optimization
Input: λ1,λ2,a,t

1: Sort
{
|ai| : i ∈ ∆(a)

}
to obtain p . See the

definition of ∆ in Section 2
2: Initialize stack of group set G =

{
{p(1)}

}
3: for all i′ such that 2 ≤ i′ ≤ |∆(a)| do
4: g = {p(i′)}
5: while G 6= ∅ and v(g) ≥ v(top(G )) do
6: g = g ∪ top(G ) . Merge g
7: Pop top(G )
8: end while
9: Push g onto G

10: end for . Pre-calculate G
11: W = 0
12: for all g ∈ G do
13: for all i ∈ g do
14: Wi = sign(ai)max

(
v(g), 0

)
15: end for
16: end for . Calculate W
17: G =D(W ) . Calculate G
Output: W,G . W minimizes Eq.3

according to the sign3 of ai and v(g) in line 14. In
line 17 it calculates G = D(W ) as discussed in
Section 3. At last it returns the pair 〈W,G 〉.

In particular, the v value v(g) in line 5 is defined
as

v(g) =

∑
i∈g

(
|ai| − 2

(
λ1 + λ2(d− p∗(i))

)
/t

)
2 |g|

,

where p∗(i) denotes the inversion of p such that
p
(
p∗(i)

)
= i. And v(g) can be intuitively in-

terpreted as the group averaged sub-gradient of(∑u
i′=1W

2
p(i′) − Q̄(W ; a, t, λ1, λ2)

)
/2. In ad-

dition, an intuitive explanation of merging loop
is that the value of objective in Eq.4 will be de-
creased after each merging step in line 6.

In summary, if we use a sparse representation
for vector a in Algorithm 3, then its complexity is
O
(
|∆(a)|log(|∆(a)|)

)
, which is independent of d.

Therefore, the whole tuning algorithm (Algorithm
1) with feature grouping is efficient even with a
large value of d.

5 Experiments

We conduct experiments on the IWSLT2008
Chinese-to-English translation tasks, whose train-
ing data consists of about 30K bilingual sentence

3The reason is attributed to the Eq.5 in (Zhong and Kwok,
2011).

pairs. Test sets 2003, 2004 and 2008 are used as
the development set, development test (devtest) set
and test set, respectively; and all of them contain
16 references. A 5-gram language model is trained
on the training data with the SRILM toolkit, and
word alignment is obtained with GIZA++. In
our experiments, the translation performances are
measured by the case-insensitive BLEU4 metric.
The significance testing is performed by paired
bootstrap re-sampling (Koehn, 2004).

We use an in-house developed hierarchical
phrase-based translation (Chiang, 2005) as our
baseline decoder, and we use the state of the art
tuning methods MERT and PRO as our compar-
ison methods4. Based on our in-house decoder,
we implement three translation models with differ-
ent feature sets: default features (default); default
features plus rule id features (+id) ; and default
features plus group features of rule id (+group).
On the IWSLT training data, the number of rule id
features is 500K, i.e. d = 500K, which is signif-
icantly greater than the number of bilingual sen-
tences 30K. Our proposed tuning method is with
the following setting by tuning on the dev-test set:
λ1 = 1e − 10, λ2 = 3e − 8, and T = 15,
n = 20×N , i.e. 20 passes over k-best-lists.

From Table 1, we can see that tuning the trans-
lation model on the development set is much better
(improvements of 4.3 BLEU scores) than that on
the training data under the default features setting.
Its main reason, as presented in Section 1, may be
that multiple references and closeness5 of tuning
sets are much helpful for translation tasks. Fur-
ther, the id features do not achieve improvements
and even decreases 0.9 BLEU scores when tuned
on the development set, due to its serious sparsity.
However, after grouping id features, the groups
learned by our method can alleviate the feature
sparsity and thus significantly obtain gains of 0.7
BLEU scores over default feature setting.

Further, we implement another tuning method6

for comparison, i.e. L1 regularization method
(Tsuruoka et al., 2009) based on the ranking loss
L(W ) defined in Eq.1. We tune the translation

4Both of them are derived from the Moses toolkit:
http://www.statmt.org/moses/.

5If the tuning set and test set are close enough or identi-
cally distributed, it is possible to get gains by sparse discrim-
inative features without using feature grouping(Chiang et al.,
2009).

6It is similar to dtrain implemented in the cdec toolkit:
http://cdec-decoder.org/, except that it does not use the dis-
tributed learning framework.



Methods Tuning set Feature set
# Features BLEU4

Runtimes
Active Reused devtest test

MERT dev default 8 8 45.7 40.6 15
PRO dev default 8 8 46.3 41.1 34
PRO train default 8 8 42.8 36.8 834
PRO dev +id 11081 4534 45.5 40.2 47
L1 train +id 584 71 42.7 36.9 975
L1 dev +id 443 248 46.2 41.0 39

OSCAR – +group 503 425 46.9 41.8 1256

Table 1: BLEU scores on the test set and tuning runtimes (minutes) for the different tuning methods with
different settings. Tuning sets dev and train denote the development and training data sets, respectively.
”Active” denotes the number of active features for all methods except OSCAR or active grouped features
for OSCAR; and ”Reused” denotes the number of active (or grouped) features which also appear during
1000-best decoding on the test set. Boldface BLEU means our method OSCAR is significantly better
than other methods with p < 0.05.

model with the +id feature setting on both the de-
velopment set and training data set, respectively,
and their hyperameters are tuned on the dev-test
set. As depicted in Table 1, our method signifi-
cantly outperforms the L1 method.

In addition, Table 1 presents the number of both
active and reused features for each method on dif-
ferent settings. We can see that the active fea-
tures (503 grouped features) in OSCAR method
are much less than those (11081 features) in PRO
with +id setting, which means that OSCAR has
lower model complexity. Further, most (84.5%) of
active features tuned on dev set are be used during
testing for OSCAR, which means that OSCAR is
more efficient to address feature sparsity problem
compared with both L1 and PRO.

At last, Table 1 also shows the runtimes for each
tuning method. Tuning on training data is much
inefficient compared with tuning on dev set, since
it requires repeatedly decoding on a much larger
dataset. Furthermore, the efficiency of our OS-
CAR method is comparable to that of tuning on
training data. Anyway, distributed training is a
reasonable approach to improve the efficiency of
OSCAR, as suggested by Simianer et al. (2012).

6 Conclusion and Future Work

This paper proposes a novel training method for a
translation model with a large number of features,
which is the main contribution of this paper. This
method is based on automatic feature grouping,
which is implemented within an online learning
method and thus is efficient for large scale training
in SMT. The other contribution is that we success-

fuly extend OSCAR to a large scale of learning
setting. In future work, we will investigate dis-
tributed learning for OSCAR and then testify it on
larger scale training data.

Acknowledgments

We would like to thank our colleagues in both
HIT and NICT for insightful discussions, and
three anonymous reviewers for many invaluable
comments and suggestions to improve our paper.
This work is supported by National Natural Sci-
ence Foundation of China (61173073, 61100093,
61073130, 61272384), and the Key Project of the
National High Technology Research and Develop-
ment Program of China (2011AA01A207).

References
H. D. Bondell and B. J. Reich. 2008. Simultaneous

regression shrinkage, variable selection, and super-
vised clustering of predictors with oscar. Biomet-
rics, 64(1):115–123.

David Chiang, Yuval Marton, and Philip Resnik. 2008.
Online large-margin training of syntactic and struc-
tural translation features. In Proc. of EMNLP. ACL.

David Chiang, Kevin Knight, and Wei Wang. 2009.
11,001 new features for statistical machine transla-
tion. In NAACL, NAACL ’09, pages 218–226.

David Chiang. 2005. A hierarchical phrase-based
model for statistical machine translation. In ACL,
ACL ’05, pages 263–270.

John Duchi and Yoram Singer. 2009. Efficient online
and batch learning using forward backward splitting.
J. Mach. Learn. Res., 10:2899–2934, December.



Mark Hopkins and Jonathan May. 2011. Tuning as
ranking. In EMNLP, pages 1352–1362, July.

Philipp Koehn. 2004. Statistical significance tests for
machine translation evaluation. In Proc. of EMNLP.
ACL.

Mu Li, Yinggong Zhao, Dongdong Zhang, and Ming
Zhou. 2010. Adaptive development data selection
for log-linear model in statistical machine transla-
tion. In COLING, COLING ’10, pages 662–670.

J. B. MacQueen. 1967. Some methods for classifi-
cation and analysis of multivariate observations. In
Proc. of 5-th Berkeley Symposium on Mathematical
Statistics and Probability.

Franz Josef Och and Hermann Ney. 2002. Discrimina-
tive training and maximum entropy models for sta-
tistical machine translation. In Proc. of ACL.

Franz Josef Och. 2003. Minimum error rate training in
statistical machine translation. In Proc. of ACL.

Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic eval-
uation of machine translation. In Proc. of ACL.

Patrick Simianer, Stefan Riezler, and Chris Dyer. 2012.
Joint feature selection in distributed stochastic learn-
ing for large-scale discriminative training in smt. In
ACL, ACL ’12, pages 11–21.

Yoshimasa Tsuruoka, Jun’ichi Tsujii, and Sophia Ana-
niadou. 2009. Stochastic gradient descent training
for l1-regularized log-linear models with cumulative
penalty. In ACL-IJCNLP, ACL ’09, pages 477–485.

Taro Watanabe, Jun Suzuki, Hajime Tsukada, and
Hideki Isozaki. 2007. Online large-margin train-
ing for statistical machine translation. In Proc. of
EMNLP-CoNLL.

Xinyan Xiao, Yang Liu, Qun Liu, and Shouxun Lin.
2011. Fast generation of translation forest for large-
scale smt discriminative training. In EMNLP, pages
880–888.

Wenliang Zhong and James Kwok. 2011. Efficient
sparse modeling with automatic feature grouping. In
ICML, ICML ’11, pages 9–16.

Appendix

Proof. Suppose Ŵk 6= 0, and thus |Ŵk| > 0. Set
Ŵ ′ as another weight such that Ŵ ′j = Ŵj for all

j(j 6= k), and Ŵ ′k = 0. Then, for each i, j the
following equations hold:

|Ŵi| ≥ |Ŵ ′i |,

and

max{|Ŵi|, |Ŵj |} ≥ max{|Ŵ ′i |, |Ŵ ′j |}.

Thus, the following equations hold based on the
above equations by simple algebraic operations:

Q(Ŵ ; a, t, λ1, λ2)−Q(Ŵ ′; a, t, λ1, λ2)

= Ŵk × Ŵk +
2λ1
t

d∑
i=1

(
|Ŵi| − |Ŵ ′i |

)
+

2λ2
t
×∑

1≤i<j≤d

(
max{|Ŵi|, |Ŵj |} −max{|Ŵ ′i |, |Ŵ ′j |}

)
≥ Ŵk × Ŵk > 0.

Therefore, we conclude that Q(Ŵ ; a, t, λ1, λ2) >
Q(Ŵ ′; a, t, λ1, λ2). This contradicts the assump-
tion that Ŵ is the minimal solution of Eq.3.


