



















































Structural Embedding of Syntactic Trees for Machine Comprehension


Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 815–824
Copenhagen, Denmark, September 7–11, 2017. c©2017 Association for Computational Linguistics

Structural Embedding of Syntactic Trees for Machine Comprehension

Rui Liu∗, Junjie Hu∗, Wei Wei∗, Zi Yang∗, Eric Nyberg
School of Computer Science
Carnegie Mellon University

5000 Forbes Ave, Pittsburgh PA 15213, USA
{ruil, junjieh, weiwei, ziy, ehn}@cs.cmu.edu

Abstract

Deep neural networks for machine com-
prehension typically utilizes only word
or character embeddings without explic-
itly taking advantage of structured linguis-
tic information such as constituency trees
and dependency trees. In this paper, we
propose structural embedding of syntactic
trees (SEST), an algorithm framework to
utilize structured information and encode
them into vector representations that can
boost the performance of algorithms for
the machine comprehension. We evaluate
our approach using a state-of-the-art neu-
ral attention model on the SQuAD dataset.
Experimental results demonstrate that our
model can accurately identify the syntactic
boundaries of the sentences and extract an-
swers that are syntactically coherent over
the baseline methods.

1 Introduction

Reading comprehension such as SQuAD (Ra-
jpurkar et al., 2016) or NewsQA (Trischler et al.,
2016) requires identifying a span from a given
context, which is an extension to the traditional
question answering task, aiming at responding
questions posed by human with natural language
(Nyberg et al., 2002; Ferrucci et al., 2010; Liu,
2017; Yang, 2017). Many works have been pro-
posed to leverage deep neural networks for such
question answering tasks, most of which involve
learning the query-aware context representations
(Dhingra et al., 2016; Seo et al., 2017; Wang and
Jiang, 2016; Xiong et al., 2017). Although deep
learning based methods demonstrated great poten-
tials for question answering, none them take syn-
tactic information of the sentences such as con-

∗Authors contributed equally to this work.

stituency tree and dependency tree into considera-
tion. Such techniques have been proven to be use-
ful in many natural language understanding tasks
in the past and illustrated noticeable improvements
such as the work by (Rajpurkar et al., 2016). In
this paper, we adopt similar ideas but apply them
to a neural attention model for question answering.

The constituency tree (Manning et al., 1999)
of a sentence defines internal nodes and termi-
nal nodes to represent phrase structure grammars
and the actual words. Figure 1 illustrates the con-
stituency tree of the sentence “the architect or en-
gineer acts as the project coordinator”. Here, “the
architect or engineer” and “the project coordina-
tor” are labeled as noun phrases (“NP”), which is
critical for answering the question below. Here,
the question asks for the name of certain occu-
pation that can be best answered using an noun
phrase. Utilizing the know ledge of a constituency
relations, we can reduce the size of the candidate
space and help the algorithm to identify the correct
answer.

Whose role is to design the works, prepare
the specifications and produce construction
drawings, administer the contract, tender the
works, and manage the works from inception
to completion?

On the other hand, a dependency tree (Manning
et al., 1999) is constructed based on the depen-
dency structure of a sentence. Figure 2 displays
the dependency tree for sentence

The Annual Conference, roughly the equiv-
alent of a diocese in the Anglican Commu-
nion and the Roman Catholic Church or a
synod in some Lutheran denominations such
as the Evangelical Lutheran Church in Amer-
ica, is the basic unit of organization within
the UMC.

815



S

NP

the architect
or engineer

VP

VBZ

acts

PP

IN

as

NP

the project
coordinator

Figure 1: The constituency tree of context “the ar-
chitect or engineer acts as the project coordinator”

“The Annual Conference” being the subject of
“the basic unit of organization within the UMC”
provides a critical clue for the model to skip over
a large chunk of the text when answering the
question “What is the basic unit of organization
within the UMC”. As we show in the analysis
section, adding dependency information dramati-
cally helps identify dependency structures within
the sentence, which is otherwise difficult to learn.

In this paper, we propose Structural Embedding
of Syntactic Trees (SEST) that encode syntactic
information structured by constituency tree and
dependency tree into neural attention models for
the question answering task. Experimental results
on SQuAD dataset illustrates that the syntactic in-
formation helps the model to choose the answers
that are both succinct and grammatically coherent,
which boosted the performance on both qualita-
tive studies and numerical results. Our focus is
to show adding structural embedding can improve
baseline models, rather than directly compare to
published SQuAD results. Although the methods
proposed in the paper are demonstrated using syn-
tactic trees, we note that similar approaches can
be used to encode other types of tree structured in-
formation such as knowledge graphs and ontology
relations.

2 Methodology

The general framework of our model is illustrated
in Figure 3. Here the input of the model is the
embedding of the context and question while the
output is two indices begin and end which indi-
cate the begin and end indices of the answer in the
context space.

The input of the model contains two parts: the
word/character model and the syntactic model.

The shaded portion of our model in Figure 3
represents the encoded syntactic information of
both context and question that are fed into the
model. To gain an insight of how the encoding
works, consider a sentence which syntactic tree
consists of four nodes (o1, o2, o3, o4). A specific
word is represented to be a sequence of nodes
from its leave all the way to the root. We cover
how this process work in detail in Section 3.1.1
and 3.1.2. Another input that will be fed into deep
learning model is the embedding information for
words and characters respectively. There are many
ways to convert words in a sentence into a high-
dimensional embedding. We choose GloVe (Pen-
nington et al., 2014b) to obtain a pre-trained and
fixed vector for each word. Instead of using a
fixed embedding, we use Convolutional Neural
Networks (CNN) to model character level embed-
ding, which values can be changed during train-
ing (Kim, 2014). To integrate both embeddings
into the deep neural model, we feed the concate-
nation of them for the question and the context to
be the input of the model.

The inputs are processed in the embedding layer
to form more abstract representations. Here we
choose a multi-layer bi-directional Long Short
Term Memory (LSTM) (Hochreiter and Schmid-
huber, 1997) to obtain more abstract representa-
tions for words in the contexts and questions.

After that, we employ an attention layer to fuse
information from both the contexts and the ques-
tions. Various matching mechanisms using atten-
tions have been extensively studied for machine
comprehension tasks (Xiong et al., 2017; Seo
et al., 2017; Wang et al., 2016; Wang and Jiang,
2016). We use the Bi-directional Attention flow
model (Seo et al., 2017) which performs context-
to-question and question-to-context attentions in
both directions. The context-to-question attention
signifies which question words are most relevant
to each context word. For each context word, the
attention weight is first computed by a softmax
function with question words, and the attention
vector of each context word is then computed by a
weighted sum of the question words’ embeddings
obtained from the embedding layer. The question-
to-context attention summarizes a context vector
by performing a soft attention with context words
given the question. We then represent each context
word as the concatenation of the embedding vec-
tor obtained from the embedding layer, the atten-

816



The Annual Conference , ... , is the basic unit of organization within the UMC .

DET

AMOD

NSUBJ

APPOS

COP

DET

AMOD CASE

NMOD CASE

DET

NMOD

Figure 2: Partial dependency parse tree of an example context “The Annual Conference, roughly the
equivalent of a diocese in the Anglican Communion and the Roman Catholic Church or a synod in
some Lutheran denominations such as the Evangelical Lutheran Church in America, is the basic unit of
organization within the UMC.”

tion vector obtained from the context-to-question
attention and the context vector obtained from the
question-to-context attention. We then feed the
concatenated vectors to a stacked bi-directional
LSTM with two layers to obtain the final repre-
sentations of context words. We note that our pro-
posed structural embedding of syntactic trees can
be easily applied to any attention approaches men-
tioned above.

For the machine comprehension task in this pa-
per, the answer to the question is a phrase in the
context. In the output layer, we use two softmax
functions over the output of the attention layer to
predict the begin and end indices of the phrase in
the context.

3 Structural Embedding of Syntactic
Tree

We detail the procedures of two alternative im-
plementation of our methods: the Structural Em-
bedding of Constituency Trees model (SECT) and
the Structural Embedding of Dependency Trees
model (SEDT). We assume that the syntactic in-
formation has already been generated in the pre-
processing step using tools such as the Stanford
CoreNLP (Manning et al., 2014).

3.1 Syntactic Sequence Extraction

We first extract a syntactic collection C(p) for
each word p, which consists of a set of nodes
{o1, o2, . . . , od−1, od} in the syntactic parse tree
T . Each node oi can be a word, a grammatical
category (e.g., part-of-speech tagging), or a depen-
dency link label, depending on the type of syn-
tactic tree we use. To construct syntactic embed-
dings, the first thing we need to do is to define a
specific processing order A over the syntactic col-
lection C(p), in which way we can extract a syn-
tactic sequence S(p) for the word p.

3.1.1 Structural Embedding of Constituency
Trees (SECT)

The constituency tree is a syntactic parse tree con-
structed by phrase structure grammars (Manning
et al., 1999), which defines the way to hierarchi-
cally construct a sentence from words in a bottom-
up manner based on constituency relations. Words
in the contexts or the questions are represented
by leaf nodes in the constituency tree while the
non-terminal nodes are labeled by categories of
the grammar. Non-terminal nodes summarize the
grammatical function of the sub-tree. Figure 1
shows an example of the constituency tree with
“the architect or engineer” being annotated as a
noun phrase (NP).

A path originating from the leaf node to the root
node captures the syntactic information in the con-
stituency tree in a hierarchical way. The higher the
node is, the longer span of words the sub-tree of
this node covers. Hence, to extract the syntactic
sequence S(p) for a leaf node p, it is reasonable
to define the processing order A(p) from the leaf
p all the way to its root. For example, the syntac-
tic sequence for the phrase “the project coordina-
tor” in Figure 1 is detected as (NP, PP, VP, S). In
practice, we usually take the last hidden units of
Bi-directional encoding mechanisms such as Bi-
directional LSTM to represent the sequence state,
as is indicated in Figure 4 (a). We set a win-
dow size to limit the amount of information that
is used in our models. For example, if we choose
the window size as 2, then the syntactic sequence
becomes (NP, PP). This process is introduced for
both performance and memory utilization consid-
eration, which is discussed in detail in Section 4.5.

In addition, a non-terminal node at a particu-
lar position in the syntactic sequence defines the
begin and end indices of a phrase in the context.
By measuring the similarity between syntactic se-

817



Output Layer

Attention Layers

Embedding Layer

Structural 

Embedding

Begin End

Word/Character 

Embeddings

o1-o2-o3

o

o1 o2
o3

o4

Figure 3: Model Framework. The neural network
for training and testing is built by components with
solid lines, which includes the embedding layer,
attention layer, and output layer. The shaded area
highlights the part of the framework that involves
syntactic information. Components with dashed
lines is an example to illustrate how syntactic in-
formation is decoded. Here a sentence is decom-
posed into a syntactic tree with four nodes and
the syntactic information for a specific word is
recorded as the path from its position in the syn-
tactic tree to the root, i.e. (o1, o2, o3) in this case.

quences S(p) extracted for each word p of both the
question and the context, we are able to locate the
boundaries of the answer span. This is done in the
attention layer shown in Figure 3.

3.1.2 Structural Embedding of Dependency
Trees (SEDT)

The dependency tree is a syntactic tree constructed
by dependency grammars (Manning et al., 1999),
which defines the way to connect words by di-
rected links that represent dependencies. A de-
pendency link is able to capture both long and
short distance dependencies of words. Relations
on links vary in their functions and are labeled
with different categories. For example, in the de-
pendency tree plotted in Figure 2, the link from
“unit” to “Conference” indicates that the target
node is a nominal subject (i.e. NSUBJ) of the
source node.

The syntactic collection C(p) for dependency
tree is defined as p’s children, each represented
by its word embedding concatenated with a vec-

tor that uniquely identifies the dependency label.
The processing order A(p) for dependency tree is
then defined to be the dependent’s original order
in the sentence.

Take the word “unit” as an example, we encode
the dependency sub-tree using a Bi-directional
LSTM, as indicated in Figure 4 (b). In such as
a sub-tree, since children are directly linked to the
root, they are position according to the original se-
quence in the sentence. Similar to the syntactic
encoding of C-Tree, we take the last hidden states
as its embedding.

Similar to SECT, we use a window of size l to
limit the amount of syntactic information for the
learning models by choosing only the l-nearest de-
pendents, which is again reported in Section 4.5.

3.2 Syntactic Sequence Encoding
Similar to previous work (Cho et al., 2014; Kim,
2014), we use a neural network to encode a
variable-length syntactic sequence into a fixed-
length vector representation. The encoder can be
a Recurrent Neural Network (RNN) or a Convolu-
tional Neural Network (CNN) that learns a struc-
tural embedding for each node such that embed-
ding of nodes under similar syntactic trees are
close in their embedding space.

We can use a Bi-directional LSTM as our RNN
encoder, where the hidden state vpt is updated ac-
cording to Eq. 1. Here xpt is the t

th node in the
syntactic sequence of the word p, which is a vector
that uniquely identifies each syntactic node. We
obtain the structural embedding of the given word
p, vpBi-LSTM = v

p
T to be the final hidden state.

vpt = Bi-LSTM(v
p
t−1,x

p
t ) (1)

Alternatively, we can also use CNN to obtain
embeddings from a sequence of syntactic nodes.
We denote l as the length of the filter of the CNN
encoder. We define xpi:i+l as the concatenation of
the vectors from xpi to x

p
i+l−1 within the filter. The

ith element in the jth feature map can be obtained
in Eq. 2. Finally we obtain the structural embed-
ding of the given word p by vpCNN in Eq. 3.

cpi,j = f(wj · xi:i+l−1 + bj) (2)
vpCNN = maxrow(c

p) (3)

where wj and bj are the weight and bias of the
jth filter respectively, f is a non-linear activation
function and maxrow(·) takes the maximum value
along rows in a matrix.

818



coordinator NP PP VP S

u0

v4v1

x1

u1

v2

x2

u2

v3

x3

u3

Conference

u0

v1

x1

u1

v2

x2

u2

v3

x3

u3

is basic organization

u4

v4

x4

v6

unit

u5

v5

x5

the

(a) A SECT example (b) A SEDT example

Figure 4: Two examples are used to illustrate how the syntactic information is encoded for SECT and
SEDT respectively. Take Bi-directional LSTM as examples, where x is a vector such as word embed-
ding, v and u are the outputs of the forward and backward LSTMs respectively. For SECT, we encode
the syntactic sequence (NP, PP, VP) for the word “coordinator” in Figure 1. We use fixed vectors for
syntactic tags (e.g., NP, PP and VP), initialized with multivariate normal distribution. The final repre-
sentation for the target word “coordinator” can be represented as the concatenation [Ew; u0; v4], where
Ew is the word embedding for “coordinator” that is 100 dimensions in our experiments and each of
the encoded vector u and v can be 30 dimensional. For SEDT, we encode the word “unit” in Figure 2
with its dependent nodes including “Conference”, “is”, “the”, “basic”, “organization”, ordered by their
positions in the original sentence. Each word is represented with its word embedding. Similar to SECT,
the final representation is the concatenation [Ew; u0; v6], which will be sent to the input layer of a neural
network.

4 Experiments

We conducted systematic experiments on the
SQuAD dataset (Rajpurkar et al., 2016). We com-
pared our methods against Bi-Directional Atten-
tion Flow (BiDAF), as well as the SEST models
described in Section 3.

4.1 Preprocessing
A couple of preprocessing steps is performed to
ensure that the deep neural models get the cor-
rect input. We segmented context and questions
into sentences by using NLTK’s Punkt sentence
segmenter1. Words in the sentences were then
converted into symbols by using PTB Tokenizer2.
Syntactic information including POS tags and syn-
tactic trees were acquired by Stanford CoreNLP
utilities (Manning et al., 2014). For the parser, we
collected constituent relations and dependency re-
lations for each word by using tree annotation and
enhanced dependencies annotation respectively.
To generate syntactic sequence, we removed se-
quences whose first node is a punctuation (“$”,
“:”, “#”, “.”, “ ” ”, “ “ ”, “,”). To use depen-
dency labels, we removed all the subcategories
(e.g., “nmod:poss”⇒ “nmod”).

1http://www.nltk.org/api/nltk.tokenize.html
2http://nlp.stanford.edu/software/tokenizer.shtml

4.2 Experiment Setting

We run our experiments on a machine that con-
tains a single GTX 1080 GPU with 8GB VRAM.
All of the models being compared have the same
settings on character embedding and word embed-
ding. As introduced in Section 2, we use a variable
character embedding with a fixed pre-trained word
embedding to serve as part of the input into the
model. The character embedding is implemented
using CNN with a one-dimensional layer consists
of 100 units with a channel size of 5. It has an
input depth of 8. The max length of SQuAD is
16 which means there are a maximum 16 words in
a sentence. The fixed word embedding has a di-
mension of 100, which is provided by the GloVe
data set (Pennington et al., 2014a). The settings
for syntactic embedding are slightly different for
each model. The BiDAF model does not deal with
syntactic information. The POS model contains
syntactic information with 39 different POS tags
that serve as both input and output. For SECT
and SEDT the input of the model has a size of
8 with 30 units to be output. Both of them has
a maximum length size that is set to be 10 and
20 respectively, which values will be further dis-
cussed in Section 4.5. They also have two different
ways to encode the syntactic information as indi-

819



cated in Section 3: LSTM and CNN. We apply the
same sets of parameters when we experiment them
with the two models. We report the results on the
SQuAD development set and the blind test set.

4.3 Predictive Performance

We first compared the performance of single mod-
els between the baseline approach BiDAF and
the proposed SEST approaches, including SE-
POS, SECT-LSTM, SECT-CNN, SEDT-LSTM,
and SEDT-CNN, on the development dataset of
SQuAD. For each model, we conducted 5 dif-
ferent single experiments and evaluated them us-
ing two metrics: “Exact match” (EM), which cal-
culates the ratio of questions that are answered
correctly by strict string comparison, and the F1
score, which calculates the harmonic mean of the
precision and recall between predicted answers
and ground true answers at the character level. As
shown in Table 1, we reported the maximum, the
mean, and the standard deviation of EM and F1
scores across all single runs for each approach,
and highlighted the best model using bold font.
SECT-LSTM is the second best method, which
confirms the predictive powers of different types
of syntactic information. We could see that SEDT-
LSTM model outperforms the baseline method
and other proposed methods in terms of both EM
and F1. Another observation is that our propose
models achieve higher relative improvements in
EM scores than F1 scores over the baseline meth-
ods, providing the evidence that syntactic infor-
mation can accurately locate the boundaries of the
answer.

Moreover, we found that both SECT-LSTM and
SEDT-LSTM have better performance than their
CNN counterparts, which suggests that LSTM can
more effectively preserve the syntactic informa-
tion. As a result, we conducted further analysis
of only SECT-LSTM and SEDT-LSTM models
in the subsequent subsections and drop the suf-
fix “-LSTM” for abbreviation. We built an en-
semble model from the 5 single models for the
baseline method BiDAF and our proposed meth-
ods SEPOS, SECT-LSTM, and SEDT-LSTM. The
ensemble model choose the answer with the high-
est sum of confidence scores among the 5 single
models for each question. We compared these
models on both the development set and official
test set and reported the results in Table 2. We
found that the models have higher performance on

the test set than the development set, which coin-
cides with the previous results on the same data set
(Seo et al., 2017; Xiong et al., 2017).

4.4 Contribution of Syntactic Sequence
To take a closer look at how syntactic sequences
affect the performance, we removed the charac-
ter/word embedding from our model seen in Fig-
ure 3 and conducted experiments based on the
syntactic input alone. In particular, we are inter-
ested in two aspects related to syntactic sequences:
First, the ability to predict answers of questions
of syntactic sequences compared to complete ran-
dom sequences. Second, the amount of impacts
brought by our proposed ordering introduced in
Section 3.1.1 and Section 3.1.2 compared to ran-
dom ordering.

We compared the performance of the models
using syntactic information along in their orig-
inal order (i.e. SECT-Only and SEDT-Only)
against their counterparts with the same syntac-
tic tree nodes but with randomly shuffled order
(i.e. SECT-Random-Order and SEDT-Random-
Order) as well as the baselines with randomly gen-
erated tree nodes (i.e. SECT-Random and SEDT-
Random). Here we choose the length of window
size to be 10. The predictive results in terms of
EM and F1 metrics are reported in Table 3. From
the table we see that both the ordering and the
contents of the syntactic tree are important for the
models to work properly: constituency and depen-
dency trees achieved over 20% boost on perfor-
mance compared to the randomly generated ones
and our proposed ordering also out-performed the
random ordering. It also worth mentioning that the
ordering of dependency trees seems to have less
impact on the performance compared to that of
the constituency trees. This is because sequences
extracted from constituency trees contain hierar-
chical information, which ordering will affect the
output of the model significantly. However, se-
quences extracted from dependency trees are all
children nodes, which are often interchangeable
and don’t seem to be affected by ordering much.

4.5 Window Size Analysis
As we have mentioned in the earlier sections, lim-
iting the window size is an important technique
to prevent excessive usage on VRAM. In practice,
we found that limiting the window size also ben-
efits the performance of our models. In Table 4
we compared the predictive performance of SECT

820



Method
Single Ensemble

EM F1 EM F1Max Mean (±SD) Max Mean (±SD)
BiDAF 67.10 66.92 (±0.23) 76.92 76.79 (±0.08) 70.97 79.53
SEPOS 67.65 66.05 (±2.94) 77.25 75.80 (±2.65) 71.46 79.70
SECT-LSTM 67.91 67.65 (±0.31)• 77.47 77.19 (±0.21) • 71.76 80.09
SEDT-LSTM 68.13 67.89 (±0.10)• 77.58 77.42 (±0.19) • 72.03 80.28
SECT-CNN 67.29 64.04 (±4.28) 76.91 73.99 (±3.89) 69.70 78.49
SEDT-CNN 67.88 66.53 (±1.91) 77.27 76.21 (±1.67) 71.58 79.80

Table 1: Performance comparison on the development set. Each setting contains five runs trained
consecutively. Standard deviations across five runs are shown in the parenthesis for single models. Dots
indicate the level of significance.

Method Single EnsembleEM F1 EM F1
BiDAF 67.69 77.07 72.33 80.33
SECT-LSTM 68.12 77.21 72.83 80.58
SEDT-LSTM 68.48 77.97 73.02 80.84

Table 2: Performance comparison on the official
blind test set. Ensemble models are trained over
the five single runs with the identical network and
hyper-parameters.

Method EM F1
SECT-Random 5.64 12.85
SECT-Random-Order 30.04 39.98
SECT-Only 34.21 44.53
SEDT-Random 0.92 8.82
SEDT-Random-Order 31.82 43.65
SEDT-Only 32.96 44.37

Table 3: Performance comparisons of models
with only syntactic information against their coun-
terparts with randomly shuffled node sequences
and randomly generated tree nodes using the
SQuAD Dev set

and SEDT models by varying the length of their
window sizes from 1 to maximum on the develop-
ment set. In general the results illustrate that per-
formances of the models increase with the length
of the window. However, we found that for SECT
model, its mean performance reached the peak
while standard deviations narrowed when window
size reaches 10. We also observed that larger win-
dow size does not generate predictive results that
is as good as the one with window size set to 10.
This suggests that there exists an optimal window
size for the constituency tree. One possible ex-

MethodLen EM F1

SECT

1 65.58 (± 2.58) 75.31 (± 2.39)
5 65.74 (± 3.77) 75.48 (± 3.39)

10 67.51 (± 0.34) 77.14 (± 0.39)
Max 67.48 (± 0.33) 77.09 (± 0.45)

SEDT
1 66.23 (± 2.5) 73.85 (± 2.22)

10 67.39 (± 0.09) 76.93 (± 0.21)

Table 4: Performance means and standard devi-
ations of different window sizes on the develop-
ment set.

planation is increasing the window size leads to
the increase in the number of syntactic nodes in
the extracted syntactic sequence. Although sub-
trees might be similar between context and ques-
tion, it is very unlikely that the complete trees are
the same. Because of that, allowing the syntac-
tic sequence to extend beyond the certain heights
will introduce unnecessary noise into the learned
representation, which will compromise the perfor-
mance of the models. Similar conclusion holds
for the SEDT model, which has an improved per-
formance and decreased variance with the window
size is set to 10. We did not perform experiments
with window size beyond 10 for SEDT since it will
consume VRAM that exceeds the capacity of our
computing device.

4.6 Overlapping Analysis
To further understand the performance benefits of
incorporating syntactic information into the ques-
tion answering problem, we can take a look at the
questions on which models disagree. Figure 5 is
the Venn Diagram on the questions that have been
corrected identified by SECT, SEDT and the base-
line BiDAF model. Here we see that the vast

821



Question BiDAF SECT
Whose role is to design the works, pre-
pare the specifications and produce con-
struction drawings, administer the con-
tract, tender the works, and manage the
works from inception to completion?

[the architect or engineer]NP
[acts as [the project
coordinator]NP]VP

[the architect or
engineer]NP

What did Luther think the study of law
meant?

[represented
[uncertainty]NP]VP

[uncertainty]NP

What caused the dynamos to be burnt out? [the powerful high fre-
quency currents]NP [set up [in
[them]NP]PP]VP

[powerful high fre-
quency currents]NP

Table 5: Questions that are correctly answered by SECT but not BiDAF

Question Context BiDAF SEDT
In the layered
model of the Earth,
the mantle has two
layers below it.
What are they?

These advances led to the development of a
layered model of the Earth, with a crust and
lithosphere on top, the mantle below (sepa-
rated within itself by seismic discontinuities
at 410 and 660 kilometers), and the outer core
and inner core below that.

seismic dis-
continuities at
410 and 660
kilometers),
and the outer
core and inner
core

the outer
core and
inner
core

What percentage
of farmland grows
wheat?

More than 50% of this area is sown for wheat,
33% for barley and 7% for oats.

33% 50%

What is the basic
unit of organization
within the UMC?

The Annual Conference, roughly the equiva-
lent of a diocese in the Anglican Communion
and the Roman Catholic Church or a synod
in some Lutheran denominations such as the
Evangelical Lutheran Church in America, is
the basic unit of organization within the UMC.

Evangelical
Lutheran
Church in
America

The
Annual
Confer-
ence

Table 6: Questions that are correctly answered by SEDT but not BiDAF

358

413 410

448

503

477

5783

BiDAF

SEDT SECT

Figure 5: Venn Diagram on the number of correct
answers predicted by BiDAF, SECT and SEDT

majority of the correctly answered questions are
shared across all three models. The rest of them
indicates questions that models disagree and are
distributed fairly evenly.

To understand the types of the questions that

syntactic models can do better, we extracted three
questions that were correctly answered by SECT
and SEDT but not the baseline model. In Table 5,
all of the three questions are “Wh-questions” and
expect the answer of a noun phrase (NP). With-
out knowing the syntactic information, BiDAF an-
swered questions with unnecessary structures such
as verb phrases (vp) (e.g. “acts as · · · ”, “repre-
sented · · · ”) or prepositional phrases (pp) (e.g.
“in · · · ”) in addition to NPs (e.g. “the architect
engineer”, “uncertainty” and “powerful high fre-
quency currents”) that normal human would an-
swer. For that reason, answers provided by BiDAF
failed the exact match although its answers are
semantically equivalent to the ones provided by
SECT. Having incorporated constituency informa-
tion provided an huge advantage in inferring the
answers that are most natural for a human.

822



The advantages of using the dependency tree in
our model can be illustrated using the questions
in Table 6. Here again we listed the ones that are
correctly identified by SEDT but not BiDAF. As
we can see that the answer provided by BiDAF
for first question broke the parenthesis incorrectly,
this problem that can be easily solved by utiliz-
ing dependency information. In the second ex-
ample, BiDAF failed to identify the dependency
structures between “50%” and the keyword be-
ing asked “wheat”, which resulted in an incorrect
answer that has nothing to do with the question.
SEDT, on the other hand, answered the question
correctly. In the third question, the key to the an-
swer is to correctly identify the subject of ques-
tion phrase “is the basic unit of organization”. Us-
ing the dependency tree as illustrated in Figure 2,
SEDT is able to identify the subject phrase cor-
rectly, namely “The Annual Conference”. How-
ever, BiDAF failed to anwer the question correctly
and selected a noun phrase as the answer.

5 Related Work

Reading Comprehension. Reading comprehen-
sion is a challenging task in NLP research. Since
the release of the SQuAD data set, many works
have been done to construct models on this mas-
sive question answering data set. Rajpurkar et. al.
are among the first authors to explore the SQuAD.
They used logistic regression with pos tagging in-
formation (Rajpurkar et al., 2016) and provided
a strong baseline for all subsequent models. A
steep improvement was given by the RaSoR model
(Lee et al., 2016) which utilized recurrent neu-
ral networks to consider all possible subphrases
of the context and evaluated them one by one. To
avoid comparing all possible candidates and to im-
prove the performance, Match-LSTM (Wang and
Jiang, 2016) was proposed by using a pointer net-
work (Vinyals et al., 2015) to extract the answer
span from the context. The same idea was taken to
the BiDAF (Seo et al., 2017) model by introducing
a bi-directional attention mechanism. Despite the
above-mentioned strong models for the machine
comprehension task, none of them considers syn-
tactic information into their prediction models.

Representations of Texts and Words. One
of the main issues in reading comprehension is
to identify the latent representations of texts and
words (Cui et al., 2016; Lee et al., 2016; Wang
et al., 2016; Xiong et al., 2017; Yu et al., 2016).

Many pre-trained libraries such as word2vec
(Mikolov et al., 2013) and Glove (Pennington
et al., 2014a) have been widely used to map words
into a high dimensional embedding space. An-
other approach is to generate embeddings by us-
ing neural networks models such as Character Em-
bedding (Kim, 2014) and Tree-LSTM (Tai et al.,
2015). One thing that worth mentioning is that
although Tree-LSTM does utilize syntactic infor-
mation, it targets at the phrases or sentences level
embedding other than the word level embedding
we have discussed in this paper. Many machine
comprehension models include both pre-trained
embeddings and variable embeddings that can be
changed through a training stage (Seo et al., 2017).

6 Conclusion

In this paper, we proposed methods to embed syn-
tactic information into the deep neural models to
improve the accuracy of our model in the ma-
chine comprehension task. We formally defined
our SEST framework and proposed two instances
to it: the structural embedding of constituency
trees (SECT) and the structural embedding of de-
pendency trees (SEDT). Experimental results on
SQuAD data set showed that our proposed ap-
proaches outperform the state-of-the-art BiDAF
model, proving that the proposed embeddings play
a significant part in correctly identifying answers
for the machine comprehension task. In particular,
we found that our model can perform especially
well on exact match metrics, which requires syn-
tactic information to accurately locate the bound-
aries of the answers. Similar approaches can be
used to encode other tree structures such as knowl-
edge graphs and ontology relations.

This work opened several potential new lines of
research: 1) In the experiments of our paper we
utilized the BiDAF model to retrieve answers from
the context. Since there are no structures in the
BiDAF models to specifically optimize for syntac-
tic information, an attention mechanism that is de-
signed for to utilize syntactic information should
be studied. 2) Another direction of research is
to incorporate SEST with deeper neural networks
such as VD-CNN (Conneau et al., 2017) to im-
prove learning capacity for syntactic embedding.
3) Tree structured information such as knowledge
graphs and ontology structure should be studied
and improve question answering tasks using simi-
lar techniques to the ones proposed in the paper.

823



References
Kyunghyun Cho, Bart van Merrienboer, Çaglar

Gülçehre, Dzmitry Bahdanau, Fethi Bougares, Hol-
ger Schwenk, and Yoshua Bengio. 2014. Learning
phrase representations using RNN encoder-decoder
for statistical machine translation. In EMNLP. pages
1724–1734.

Alexis Conneau, Holger Schwenk, Loı̈c Barrault, and
Yann Lecun. 2017. Very deep convolutional net-
works for text classification. In EACL.

Yiming Cui, Zhipeng Chen, Si Wei, Shijin Wang,
Ting Liu, and Guoping Hu. 2016. Attention-over-
attention neural networks for reading comprehen-
sion. arXiv preprint arXiv:1607.04423 .

Bhuwan Dhingra, Hanxiao Liu, William W Cohen,
and Ruslan Salakhutdinov. 2016. Gated-attention
readers for text comprehension. arXiv preprint
arXiv:1606.01549 .

David Ferrucci, Eric Brown, Jennifer Chu-Carroll,
James Fan, David Gondek, Aditya A Kalyanpur,
Adam Lally, J William Murdock, Eric Nyberg, John
Prager, et al. 2010. Building watson: An overview
of the deepqa project. AI magazine 31(3):59–79.

Sepp Hochreiter and Jürgen Schmidhuber. 1997. Long
short-term memory. Neural computation .

Yoon Kim. 2014. Convolutional neural networks for
sentence classification. In EMNLP. pages 1746–
1751.

Kenton Lee, Tom Kwiatkowski, Ankur Parikh, and Di-
panjan Das. 2016. Learning recurrent span repre-
sentations for extractive question answering. arXiv
preprint arXiv:1611.01436 .

Rui Liu. 2017. A Phased Ranking Model for Informa-
tion Systems. Ph.D. thesis, Carnegie Mellon Univer-
sity.

Christopher D Manning, Hinrich Schütze, et al. 1999.
Foundations of statistical natural language process-
ing, volume 999. MIT Press.

Christopher D. Manning, Mihai Surdeanu, John
Bauer, Jenny Finkel, Steven J. Bethard,
and David McClosky. 2014. The Stanford
CoreNLP natural language processing toolkit.
In Association for Computational Linguistics
(ACL) System Demonstrations. pages 55–60.
http://www.aclweb.org/anthology/P/P14/P14-5010.

Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Cor-
rado, and Jeff Dean. 2013. Distributed representa-
tions of words and phrases and their compositional-
ity. In Advances in neural information processing
systems. pages 3111–3119.

Eric Nyberg, Teruko Mitamura, Jaime G Carbonell,
Jamie Callan, Kevyn Collins-Thompson, Krzysztof
Czuba, Michael Duggan, Laurie Hiyakumoto, N Hu,
Yifen Huang, et al. 2002. The javelin question-
answering system at trec 2002. In TREC.

Jeffrey Pennington, Richard Socher, and Christopher
Manning. 2014a. Glove: Global vectors for word
representation. In EMNLP. Association for Compu-
tational Linguistics, pages 1532–1543.

Jeffrey Pennington, Richard Socher, and Christo-
pher D. Manning. 2014b. Glove: Global vectors for
word representation. In Empirical Methods in Nat-
ural Language Processing (EMNLP). pages 1532–
1543. http://www.aclweb.org/anthology/D14-1162.

Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and
Percy Liang. 2016. Squad: 100,000+ questions for
machine comprehension of text. In EMNLP.

Minjoon Seo, Aniruddha Kembhavi, Ali Farhadi, and
Hannaneh Hajishirzi. 2017. Bidirectional attention
flow for machine comprehension. In ICLR.

Kai Sheng Tai, Richard Socher, and Christopher D.
Manning. 2015. Improved semantic representations
from tree-structured long short-term memory net-
works. In ACL.

Adam Trischler, Tong Wang, Xingdi Yuan, Justin Har-
ris, Alessandro Sordoni, Philip Bachman, and Ka-
heer Suleman. 2016. Newsqa: A machine compre-
hension dataset. arXiv preprint arXiv:1611.09830 .

Oriol Vinyals, Meire Fortunato, and Navdeep Jaitly.
2015. Pointer networks. In NIPS. pages 2692–2700.

Shuohang Wang and Jing Jiang. 2016. Machine com-
prehension using match-lstm and answer pointer.
arXiv preprint arXiv:1608.07905 .

Zhiguo Wang, Haitao Mi, Wael Hamza, and Radu
Florian. 2016. Multi-perspective context match-
ing for machine comprehension. arXiv preprint
arXiv:1612.04211 .

Caiming Xiong, Victor Zhong, and Richard Socher.
2017. Dynamic coattention networks for question
answering. In ICLR.

Zi Yang. 2017. Analytics Meta Learning. Ph.D. thesis,
Carnegie Mellon University.

Yang Yu, Wei Zhang, Kazi Hasan, Mo Yu, Bing Xiang,
and Bowen Zhou. 2016. End-to-end answer chunk
extraction and ranking for reading comprehension.
arXiv preprint arXiv:1610.09996 .

824


