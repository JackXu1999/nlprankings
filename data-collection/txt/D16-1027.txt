



















































Memory-enhanced Decoder for Neural Machine Translation


Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 278–286,
Austin, Texas, November 1-5, 2016. c©2016 Association for Computational Linguistics

Memory-enhanced Decoder for Neural Machine Translation

Mingxuan Wang1 Zhengdong Lu2 Hang Li2 Qun Liu3,1
1Key Laboratory of Intelligent Information Processing,

Institute of Computing Technology, Chinese Academy of Sciences
{wangmingxuan,liuqun}@ict.ac.cn

2Noah’s Ark Lab, Huawei Technologies
{Lu.Zhengdong,HangLi.HL}@huawei.com

3ADAPT Centre, School of Computing, Dublin City University

Abstract

We propose to enhance the RNN decoder
in a neural machine translator (NMT) with
external memory, as a natural but power-
ful extension to the state in the decoding
RNN. This memory-enhanced RNN de-
coder is called MEMDEC. At each time
during decoding, MEMDEC will read from
this memory and write to this memory
once, both with content-based addressing.
Unlike the unbounded memory in previ-
ous work(Bahdanau et al., 2014) to store
the representation of source sentence, the
memory in MEMDEC is a matrix with pre-
determined size designed to better cap-
ture the information important for the de-
coding process at each time step. Our
empirical study on Chinese-English trans-
lation shows that it can improve by 4.8
BLEU upon Groundhog and 5.3 BLEU
upon on Moses, yielding the best perfor-
mance achieved with the same training set.

1 Introduction

The introduction of external memory has greatly
expanded the representational capability of neu-
ral network-based model on modeling se-
quences(Graves et al., 2014), by providing flex-
ible ways of storing and accessing information.
More specifically, in neural machine translation,
one great improvement came from using an array
of vectors to represent the source in a sentence-
level memory and dynamically accessing relevant
segments of them (alignment) (Bahdanau et al.,

2014) through content-based addressing (Graves
et al., 2014). The success of RNNsearch demon-
strated the advantage of saving the entire sen-
tence of arbitrary length in an unbounded mem-
ory for operations of next stage (e.g., decoding).

In this paper, we show that an external memory
can be used to facilitate the decoding/generation
process thorough a memory-enhanced RNN de-
coder, called MEMDEC. The memory in
MEMDEC is a direct extension to the state in
the decoding, therefore functionally closer to the
memory cell in LSTM(Hochreiter and Schmid-
huber, 1997). It takes the form of a matrix with
pre-determined size, each column (“a memory
cell”) can be accessed by the decoding RNN with
content-based addressing for both reading and
writing during the decoding process. This mem-
ory is designed to provide a more flexible way
to select, represent and synthesize the informa-
tion of source sentence and previously generated
words of target relevant to the decoding. This
is in contrast to the set of hidden states of the
entire source sentence (which can viewed as an-
other form of memory) in (Bahdanau et al., 2014)
for attentive read, but can be combined with it to
greatly improve the performance of neural ma-
chine translator. We apply our model on English-
Chinese translation tasks, achieving performance
superior to any published results, SMT or NMT,
on the same training data (Xie et al., 2011; Meng
et al., 2015; Tu et al., 2016; Hu et al., 2015)

Our contributions are mainly two-folds

• we propose a memory-enhanced decoder for

278



neural machine translator which naturally
extends the RNN with vector state.

• our empirical study on Chinese-English
translation tasks show the efficacy of the
proposed model.

Roadmap In the remainder of this paper, we
will first give a brief introduction to attention-
based neural machine translation in Section 2,
presented from the view of encoder-decoder,
which treats the hidden states of source as an
unbounded memory and the attention model as
a content-based reading. In Section 3, we
will elaborate on the memory-enhanced decoder
MEMDEC. In Section 4, we will apply NMT with
MEMDEC to a Chinese-English task. Then in
Section 5 and 6, we will give related work and
conclude the paper.

2 Neural machine translation with
attention

Our work is built on attention-based
NMT(Bahdanau et al., 2014), which repre-
sents the source sentence as a sequence of
vectors after being processed by RNN or bi-
directional RNNs, and then conducts dynamic
alignment and generation of the target sentence
with another RNN simultaneously.

Attention-based NMT, with RNNsearch as its
most popular representative, generalizes the con-
ventional notion of encoder-decoder in using a
unbounded memory for the intermediate repre-
sentation of source sentence and content-based
addressing read in decoding, as illustrated in
Figure 1. More specifically, at time step t,
RNNsearch first get context vector ct after read-
ing from the source representation MS, which is
then used to update the state, and generate the
word yt (along with the current hidden state st,
and the previously generated word yi−1).

Formally, given an input sequence x =
[x1, x2, . . . , xTx ] and the previously generated
sequence y<t = [y1, y2, . . . , yt−1], the probabil-
ity of next word yt is

p(yt|y<t;x) = f(ct, yt−1, st), (1)

Figure 1: RNNsearch in the encoder-decoder view.

where st is state of decoder RNN at time step t
calculated as

st = g(st−1, yt−1, ct). (2)

where g(·) can be an be any activation function,
here we adopt a more sophisticated dynamic op-
erator as in Gated Recurrent Unit (GRU, (Cho et
al., 2014)). In the remainder of the paper, we will
also use GRU to stand for the operator. The read-
ing ct is calculated as

ct =

j=Tx∑

j=1

αt,jhj , (3)

where hj is the jth cell in memory MS. More
formally, hj = [

←−
hj
>,
−→
hj
>]> is the annotations

of xj and contains information about the whole
input sequence with a strong focus on the parts
surrounding xj , which is computed by a bidirec-
tional RNN. The weight αt,j is computed by

αt,j =
exp(et,j)∑k=Tx

k=1 exp(et,k)
.

where ei,j = vTa tanh(Wast−1 + Uahj) scores
how well st−1 and the memory cell hj match.
This is called automatic alignment (Bahdanau et
al., 2014) or attention model (Luong et al., 2015),
but it is essentially reading with content-based
addressing defined in (Graves et al., 2014). With
this addressing strategy the decoder can attend to
the source representation that is most relevant to
the stage of decoding.

279



Figure 2: Diagram of the proposed decoder MEMDEC with details.

2.1 Improved Attention Model

The alignment model αt,j scores how well the
output at position tmatches the inputs around po-
sition j based on st−1 and hj . It is intuitively
beneficial to exploit the information of yt−1 when
reading from MS, which is missing from the im-
plementation of attention-based NMT in (Bah-
danau et al., 2014). In this work, we build a
more effective alignment path by feeding both
previous hidden state st−1 and the context word
yt−1 to the attention model, inspired by the re-
cent implementation of attention-based NMT1.
Formally, the calculation of et,j becomes

et,j = v
T
a tanh(Was̃t−1 +Uahj),

where

• s̃t−1 = H(st−1, eyt−1) is an intermediate
state tailored for reading from MS with the
information of yt−1 (its word embedding be-
ing eyt−1) added;

• H is a nonlinear function, which can be
as simple as tanh or as complex as GRU.
In our preliminary experiments, we found
GRU works slightly better than tanh func-
tion, but we chose the latter for simplicity.

1github.com/nyu-dl/dl4mt-tutorial/
tree/master/session2

3 Decoder with External Memory

In this section we will elaborate on the proposed
memory-enhanced decoder MEMDEC. In ad-
dition to the source memory MS, MEMDEC is
equipped with a buffer memory MB as an ex-
tension to the conventional state vector. Fig-
ure 3 contrasts MEMDEC with the decoder in
RNNsearch (Figure 1) on a high level.

Figure 3: High level digram of MEMDEC.

In the remainder of the paper, we will refer to
the conventional state as vector-state (denoted st)
and its memory extension as memory-state (de-
noted as MBt ). Both states are updated at each
time step in a interweaving fashion, while the out-
put symbol yt is predicted based solely on vector-
state st (along with ct and yt−1). The diagram of
this memory-enhanced decoder is given in Figure
2.

280



Vector-State Update At time t, the vector-state
st is first used to read MB

rt−1 = readB(st−1,MBt−1) (4)

which then meets the previous prediction yt−1 to
form an “intermediate” state-vector

s̃t = tanh(Wrrt−1 +Wyeyt−1). (5)

where eyt−1 is the word-embedding associated
with the previous prediction yt−1. This pre-state
s̃t is used to read the source memory MS

ct = readS(s̃t,MS). (6)

Both readings in Eq. (4) & (6) follow content-
based addressing(Graves et al., 2014) (details
later in Section 3.1). After that, rt−1 is combined
with output symbol yt−1 and ct to update the new
vector-state

st = GRU(rt−1,yt−1, ct) (7)

The update of vector-state is illustrated in Fig-
ure 4.

Figure 4: Vector-state update at time t.

Memory-State Update As illustrated in Fig-
ure 5, the update for memory-state is simple after
the update of vector-state: with the vector-state
st+1 the updated memory-state will be

MBt = write(st,M
B
t−1) (8)

The writing to the memory-state is also content-
based, with same forgetting mechanism sug-
gested in (Graves et al., 2014), which we will
elaborate with more details later in this section.

Figure 5: Memory-state update at time t.

Prediction As illustrated in Figure 6, the pre-
diction model is same as in (Bahdanau et al.,
2014), where the score for word y is given by

score(y) = DNN([st, ct, eyt−1 ])
>ωy (9)

where ωy is the parameters associated with the
word y. The probability of generating word y at
time t is then given by a softmax over the scores

p(y|st, ct, yt−1) =
exp(score(y))∑
y′ exp(score(y′))

.

Figure 6: Prediction at time t.

3.1 Reading Memory-State

Formally MBt′ ∈ Rn×m is the memory-state at
time t′ after the memory-state update, where n is
the number of memory cells and m is the dimen-
sion of vector in each cell. Before the vector-state
update at time t, the output of reading rt is given
by

rt =

j=n∑

j=1

wRt (j)M
B
t−1(j)

where wRt ∈ Rn specifies the normalized weights
assigned to the cells in MBt . Similar with the
reading from MS ( a.k.a. attention model), we
use content-based addressing in determining wRt .

281



More specifically, wRt is also updated from the
one from previous time wRt−1 as

wRt = g
R
t w

R
t−1 + (1− gRt )w̃Rt , (10)

where

• gRt = σ(wRgst) is the gate function, with pa-
rameters wRg ∈ Rm;

• w̃t gives the contribution based on the cur-
rent vector-state st

w̃Rt = softmax(a
R
t ) (11)

aRt (i) = v
>(WRaM

B
t−1(i) +U

R
ast−1), (12)

with parameters WRa,U
R
a ∈ Rm×m and v ∈

Rm.

3.2 Writing to Memory-State
There are two types of operation on writing to
memory-state: ERASE and ADD. Erasion is simi-
lar to the forget gate in LSTM or GRU, which de-
termines the content to be remove from memory
cells. More specifically, the vector µERSt ∈ Rm
specifies the values to be removed on each dimen-
sion in memory cells, which is than assigned to
each cell through normalized weights wWt . For-
mally, the memory-state after ERASE is given by

M̃Bt (i) = M
B
t−1(i)(1−wWt (i) · µERSt ) (13)

i = 1, · · · , n
where

• µERSt = σ(WERSst) is parametrized with
WERS ∈ Rm×m;

• wWt (i) specifies the weight associated with
the ith cell in the same parametric form as
in Eq. (10)-(12) with generally different pa-
rameters.

ADD operation is similar with the update gate in
LSTM or GRU, deciding how much current in-
formation should be written to the memory.

MBt (i) = M̃
B
t (i) +w

W
t (i)µ

ADD
t

µADDt = σ(W
ADDst)

where µADDt ∈ Rm and WADD ∈ Rm×m.

In our experiments, we have a peculiar but in-
teresting observation: it is often beneficial to use
the same weights for both reading (i.e., wRt in
Section 3.1) and writing (i.e., wWt in Section 3.2
) for the same vector-state st. We conjecture that
this acts like a regularization mechanism to en-
courage the content of reading and writing to be
similar to each other.

3.3 Some Analysis
The writing operation in Eq. (13) at time t
can be viewed as an nonlinear way to combine
the previous memory-state MBt−1 and the newly
updated vector-state st, where the nonlinearity
comes from both the content-based addressing
and the gating. This is in a way similar to the
update of states in regular RNN, while we con-
jecture that the addressing strategy in MEMDEC
makes it easier to selectively change some con-
tent updated (e.g., the relatively short-term con-
tent) while keeping other content less modified
(e.g., the relatively long-term content).

The reading operation in Eq. (10) can “extract”
the content from MBt relevant to the alignment
(reading from MS) and prediction task at time t.
This is in contrast with the regular RNN decoder
including its gated variants, which takes the en-
tire state vector to for this purpose. As one ad-
vantage, although only part of the information in
MBt is used at t, the entire memory-state, which
may store other information useful for later, will
be carry over to time t + 1 for memory-state up-
date (writing).

4 Experiments on Chinese-English
Translation

We test the memory-enhanced decoder to task of
Chinese-to-English translation, where MEMDEC
is put on the top of encoder same as in (Bahdanau
et al., 2014).

4.1 Datasets and Evaluation metrics
Our training data for the translation task con-
sists of 1.25M sentence pairs extracted from LDC
corpora2, with 27.9M Chinese words and 34.5M

2The corpora include LDC2002E18, LDC2003E07,
LDC2003E14, Hansards portion of LDC2004T07,

282



English words respectively. We choose NIST
2002 (MT02) dataset as our development set,
and the NIST 2003 (MT03), 2004 (MT04) 2005
(MT05) and 2006 (MT06) datasets as our test
sets. We use the case-insensitive 4-gram NIST
BLEU score as our evaluation metric as our eval-
uation metric (Papineni et al., 2002).

4.2 Experiment settings

Hyper parameters In training of the neural
networks, we limit the source and target vocab-
ularies to the most frequent 30K words in both
Chinese and English, covering approximately
97.7% and 99.3% of the two corpora respectively.
The dimensions of word embedding is 512 and
the size of the hidden layer is 1024. The dimem-
sion of each cell in MB is set to 1024 and the
number of cells n is set to 8.

Training details We initialize the recurrent
weight matrices as random orthogonal matrices.
All the bias vectors were initialize to zero. For
other parameters, we initialize them by sampling
each element from the Gaussian distribution of
mean 0 and variance 0.012. Parameter optimiza-
tion is performed using stochastic gradient de-
scent. Adadelta (Zeiler, 2012) is used to auto-
matically adapt the learning rate of each param-
eter (� = 10−6 and ρ = 0.95). To avoid gra-
dients explosion, the gradients of the cost func-
tion which had `2 norm larger than a predefined
threshold 1.0 was normalized to the threshold
(Pascanu et al., 2013). Each SGD is of a mini-
batch of 80 sentences. We train our NMT model
with the sentences of length up to 50 words in
training data, while for moses system we use the
full training data.

Memory Initialization Each memory cell is
initialized with the source sentence hidden state
computed as

MB(i) = m+ νi (14)

m = σ(WINI

i=Tx∑

i=0

hi)/Tx (15)

LDC2004T08 and LDC2005T06.

where WINI ∈ Rm×2·m; σ is tanh function. m
makes a nonlinear transformation of the source
sentence information. νi is a random vector sam-
pled from N (0, 0.1).
Dropout we also use dropout for our NMT
baseline model and MEMDEC to avoid over-
fitting (Hinton et al., 2012). The key idea is to
randomly drop units (along with their connec-
tions) from the neural network during training.
This prevents units from co-adapting too much.
In the simplest case, each unit is omitted with
a fixed probability p, namely dropout rate. In
our experiments, dropout was applied only on the
output layer and the dropout rate is set to 0.5. We
also try other strategy such as dropout at word
embeddings or RNN hidden states but fail to get
further improvements.

Pre-training For MEMDEC, the objective
function is a highly non-convex function of
the parameters with more complicated land-
scape than that for decoder without exter-
nal memory, rendering direct optimization over
all the parameters rather difficult. Inspired
by the effort on easing the training of very
deep architectures (Hinton and Salakhutdi-
nov, 2006), we propose a simple pre-training
strategyFirst we train a regular attention-based
NMT model without external memory. Then
we use the trained NMT model to initialize
the parameters of encoder and parameters of
MEMDEC, except those related to memory-state
(i.e., {WRa,URa,v,wRg ,WERS,WADD}). After
that, we fine-tune all the parameters of NMT
with MEMDEC decoder, including the parame-
ters initialized with pre-training and those associ-
ated with accessing memory-state.

4.3 Comparison systems
We compare our method with three state-of-the-
art systems:

• Moses: an open source phrase-based trans-
lation system 3: with default configuration
and a 4-gram language model trained on the
target portion of training data.

3http://www.statmt.org/moses/

283



SYSTEM MT03 MT04 MT05 MT06 AVE.
Groundhog 31.92 34.09 31.56 31.12 32.17
RNNsearch? 33.11 37.11 33.04 32.99 34.06
RNNsearch? + coverage 34.49 38.34 34.91 34.25 35.49
MEMDEC 36.16 39.81 35.91 35.98 36.95
Moses 31.61 33.48 30.75 30.85 31.67

Table 1: Case-insensitive BLEU scores on Chinese-English translation. Moses is the state-of-the-art phrase-based statistical
machine translation system. For RNNsearch, we use the open source system Groundhog as our baseline. The strong

baseline, denoted RNNsearch?, also adopts feedback attention and dropout. The coverage model on top of RNNsearch? has

significantly improved upon its published version (Tu et al., 2016), which achieves the best published result on this training

set. For MEMDEC the number of cells is set to 8.

pre-training n MT03 MT04 MT05 MT06 Ave.
N 4 35.29 37.36 34.58 33.32 35.11
Y 4 35.39 39.16 35.33 35.02 36.22
Y 6 35.63 39.29 35.61 34.92 36.58
Y 8 36.16 39.81 35.91 35.98 36.95
Y 10 36.46 38.86 34.46 35.00 36.19
Y 12 35.92 39.09 35.31 35.12 36.37

Table 2: MEMDEC performances of different memory size.

• RNNSearch: an attention-based NMT
model with default settings. We use the open
source system GroundHog as our NMT
baseline4.

• Coverage model: a state-of-the-art variant
of attention-based NMT model (Tu et al.,
2016) which improves the attention mecha-
nism through modelling a soft coverage on
the source representation.

4.4 Results
The main results of different models are given
in Table 1. Clearly MEMDEC leads to remark-
able improvement over Moses (+5.28 BLEU) and
Groundhog (+4.78 BLEU). The feedback atten-
tion gains +1.06 BLEU score on top of Ground-
hog on average, while together with dropout adds
another +0.83 BLEU score, which constitute the
1.89 BLEU gain of RNNsearch? over Ground-
hog. Compared to RNNsearch? MEMDEC is
+2.89 BLEU score higher, showing the model-
ing power gained from the external memory. Fi-

4https://github.com/lisa-groundhog/
GroundHog

nally, we also compare MEMDEC with the state-
of-the-art attention-based NMT with COVERAGE
mechanism(Tu et al., 2016), which is about 2
BLEU over than the published result after adding
fast attention and dropout. In this comparison
MEMDEC wins with big margin (+1.46 BLEU
score).

4.5 Model selection

Pre-training plays an important role in optimiz-
ing the memory model. As can be seen in Tab.2,
pre-training improves upon our baseline +1.11
BLEU score on average, but even without pre-
training our model still gains +1.04 BLEU score
on average. Our model is rather robust to the
memory size: with merely four cells, our model
will be over 2 BLEU higher than RNNsearch?.
This further verifies our conjecture the the exter-
nal memory is mostly used to store part of the
source and history of target sentence.

4.6 Case study

We show in Table 5 sample translations from
Chinese to English, comparing mainly MEMDEC

284



src
恩达依兹耶说:“签署(2003年11月停火)协定的各方,最迟必须在元月五日
以前把战士的驻扎地点安顿完毕。”

ref
“All parties that signed the (November 2003 ceasefire) accord should finish
the cantoning of their fighters by January 5, 2004, at the latest,” Ndayizeye
said.

MEMDEC
UNK said, “ the parties involved in the ceasefire agreement on November
2003 will have to be completed by January 5, 2004. ”

base
“The signing of the agreement (UNK-fire) agreement in the November
2003 ceasefire must be completed by January 5, 2004.

src
代表团成员告诉今日美国报说,布希政府已批准美国代表团预定元月六
日至十日展开的北韩之行。

ref
Members of the delegation told US Today that the Bush administration had
approved the US delegation’ s visit to North Korea from January 6 to 10.

MEMDEC
The delegation told the US today that the Bush administration has approved
the US delegation’s visit to north Korea from 6 to 10 january .

base
The delegation told the US that the Bush administration has approved the US
to begin his visit to north Korea from 6 to 10 January.

Table 3: Sample translations-for each example, we show the source(src), the human translation (ref),the translation from
our memory model MEMDEC and the translation from RNNsearch(equipped with fast attention and dropout).We italicise

some correct translation segments and highlight a few wrong ones in bold.

and the RNNsearch model for its pre-training. It
is appealing to observe that MEMDEC can pro-
duce more fluent translation results and better
grasp the semantic information of the sentence.

5 Related Work

There is a long thread of work aiming to im-
prove the ability of RNN in remembering long se-
quences, with the long short-term memory RNN
(LSTM) (Hochreiter and Schmidhuber, 1997) be-
ing the most salient examples and GRU (Cho et
al., 2014) being the most recent one. Those works
focus on designing the dynamics of the RNN
through new dynamic operators and appropri-
ate gating, while still keeping vector form RNN
states. MEMDEC, on top of the gated RNN, ex-
plicitly adds matrix-form memory equipped with
content-based addressing to the system, hence
greatly improving the power of the decoder RNN
in representing the information important for the
translation task.

MEMDEC is obviously related to the recent ef-
fort on attaching an external memory to neural
networks, with two most salient examples be-
ing Neural Turing Machine (NTM) (Graves et
al., 2014) and Memory Network (Weston et al.,
2014). In fact MEMDEC can be viewed as a

special case of NTM, with specifically designed
reading (from two different types of memory)
and writing mechanism for the translation task.
Quite remarkably MEMDEC is among the rare
instances of NTM which significantly improves
upon state-of-the-arts on a real-world NLP task
with large training corpus.

Our work is also related to the recent work on
machine reading (Cheng et al., 2016), in which
the machine reader is equipped with a memory
tape, enabling the model to directly read all the
previous hidden state with an attention mecha-
nism. Different from their work, we use an ex-
ternal bounded memory and make an abstraction
of previous information. In (Meng et al., 2015),
Meng et. al. also proposed a deep architecture for
sequence-to-sequence learning with stacked lay-
ers of memory to store the intermediate represen-
tations, while our external memory was applied
within a sequence.

6 Conclusion

We propose to enhance the RNN decoder in
a neural machine translator (NMT) with exter-
nal memory. Our empirical study on Chinese-
English translation shows that it can significantly
improve the performance of NMT.

285



References
[Bahdanau et al.2014] Dzmitry Bahdanau,

Kyunghyun Cho, and Yoshua Bengio. 2014. Neu-
ral machine translation by jointly learning to align
and translate. arXiv preprint arXiv:1409.0473.

[Cheng et al.2016] Jianpeng Cheng, Li Dong, and
Mirella Lapata. 2016. Long short-term memory-
networks for machine reading. arXiv preprint
arXiv:1601.06733.

[Cho et al.2014] Kyunghyun Cho, Bart
Van Merriënboer, Caglar Gulcehre, Dzmitry
Bahdanau, Fethi Bougares, Holger Schwenk,
and Yoshua Bengio. 2014. Learning phrase
representations using rnn encoder-decoder for
statistical machine translation. arXiv preprint
arXiv:1406.1078.

[Graves et al.2014] Alex Graves, Greg Wayne, and Ivo
Danihelka. 2014. Neural turing machines. arXiv
preprint arXiv:1410.5401.

[Hinton and Salakhutdinov2006] Geoffrey E Hinton
and Ruslan R Salakhutdinov. 2006. Reducing the
dimensionality of data with neural networks. Sci-
ence, 313(5786):504–507.

[Hinton et al.2012] Geoffrey E Hinton, Nitish Srivas-
tava, Alex Krizhevsky, Ilya Sutskever, and Ruslan
Salakhutdinov. 2012. Improving neural networks
by preventing co-adaptation of feature detectors.

[Hochreiter and Schmidhuber1997] Sepp Hochreiter
and Jürgen Schmidhuber. 1997. Long short-term
memory. Neural computation, 9(8):1735–1780.

[Hu et al.2015] Baotian Hu, Zhaopeng Tu, Zhengdong
Lu, and Hang Li. 2015. Context-dependent trans-
lation selection using convolutional neural net-
work.

[Luong et al.2015] Minh-Thang Luong, Hieu Pham,
and Christopher D Manning. 2015. Effective ap-
proaches to attention-based neural machine trans-
lation. arXiv preprint arXiv:1508.04025.

[Meng et al.2015] Fandong Meng, Zhengdong
Lu, Zhaopeng Tu, Hang Li, and Qun Liu.
2015. A deep memory-based architecture for
sequence-to-sequence learning. arXiv preprint
arXiv:1506.06442.

[Papineni et al.2002] Kishore Papineni, Salim
Roukos, Todd Ward, and Wei-Jing Zhu. 2002.
Bleu: a method for automatic evaluation of
machine translation. In Proceedings of the 40th
annual meeting on association for computa-
tional linguistics, pages 311–318. Association for
Computational Linguistics.

[Pascanu et al.2013] Razvan Pascanu, Caglar Gul-
cehre, Kyunghyun Cho, and Yoshua Bengio. 2013.

How to construct deep recurrent neural networks.
arXiv preprint arXiv:1312.6026.

[Tu et al.2016] Zhaopeng Tu, Zhengdong Lu, Yang
Liu, Xiaohua Liu, and Hang Li. 2016. Model-
ing coverage for neural machine translation. ArXiv
eprints, January.

[Weston et al.2014] Jason Weston, Sumit Chopra, and
Antoine Bordes. 2014. Memory networks. arXiv
preprint arXiv:1410.3916.

[Xie et al.2011] Jun Xie, Haitao Mi, and Qun Liu.
2011. A novel dependency-to-string model for sta-
tistical machine translation.

[Zeiler2012] Matthew D Zeiler. 2012. Adadelta:
an adaptive learning rate method. arXiv preprint
arXiv:1212.5701.

286


