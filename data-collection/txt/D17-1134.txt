



















































Multi-task Attention-based Neural Networks for Implicit Discourse Relationship Representation and Identification


Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 1299–1308
Copenhagen, Denmark, September 7–11, 2017. c©2017 Association for Computational Linguistics

Multi-task Attention-based Neural Networks for Implicit Discourse
Relationship Representation and Identification

Man Lan1,2, Jianxiang Wang1,3∗, Yuanbin Wu1,2∗, Zheng-Yu Niu3∗ , Haifeng Wang3
1 School of Computer Science and Software Engineering, East China Normal University

2 Shanghai Key Laboratory of Multidimensional Information Processing, P.R.China
3 Baidu Inc., Beijing, P.R.China

{mlan,ybwu}@cs.ecnu.edu.cn
{wangjianxiang01,niuzhengyu,wanghaifeng}@baidu.com

Abstract

We present a novel multi-task attention-
based neural network model to address
implicit discourse relationship representa-
tion and identification through two type-
s of representation learning, an attention-
based neural network for learning dis-
course relationship representation with t-
wo arguments and a multi-task framework
for learning knowledge from annotated
and unannotated corpora. The extensive
experiments have been performed on t-
wo benchmark corpora (i.e., PDTB and
CoNLL-2016 datasets). Experimental re-
sults show that our proposed model out-
performs the state-of-the-art systems on
benchmark corpora.

1 Introduction

The task of implicit discourse relation (or rhetor-
ical relation) identification is to recognize how t-
wo adjacent text spans without explicit discourse
marker (i.e., connective, e.g., because or but ) be-
tween them are logically connected to one anoth-
er (e.g., cause or contrast). It is considered to
be a crucial step for discourse analysis and lan-
guage generation and helpful to many downstream
NLP applications, e.g., QA, MT, sentiment analy-
sis, machine comprehension, etc.

With the release of PDTB 2.0 (Prasad et al.,
2008), lots of work has been done for discourse re-
lation identification on natural (i.e., genuine) dis-
course data (Pitler et al., 2009; Lin et al., 2009;
Wang et al., 2010; Zhou et al., 2010; Braud and
Denis, 2015; Fisher and Simmons, 2015) with the
use of traditional NLP linguistically informed fea-
tures and machine learning algorithms. Recently,
more and more researchers resorted to neural net-
works for implicit discourse recognition (Zhang

et al., 2015; Chen et al., 2016; Liu et al., 2016b;
Qin et al., 2016a; Liu and Li, 2016; Braud and
Denis, 2016; Wu et al., 2016). Meanwhile, to
alleviate the shortage of labeled data, researcher-
s explored multi-task learning with the aid of u-
nannotated data for implicit discourse recognition
either in traditional machine learning framework
(Collobert and Weston, 2008; Lan et al., 2013) or
recently in neural network framework (Wu et al.,
2016; Liu et al., 2016b).

In this work, we present a novel multi-task
attention-based neural network to address implic-
it discourse relationship representation and recog-
nition. It performs two types of representation
learning at the same time. An attention-based neu-
ral network conducts discourse relationship repre-
sentation learning through interaction between t-
wo discourse arguments. Meanwhile, a multi-task
learning framework leverages knowledge from
auxiliary task to enhance the performance of main
task. Furthermore, these two types of learning
are integrated into one neural network framework
and work together to maximize the overall perfor-
mance.

The contributions of this work are listed as fol-
lows.

• We propose a multi-task attention-based neu-
ral network model to address implicit dis-
course relationship representation and recog-
nition, which benefits from both the interac-
tion between discourse arguments and the in-
teraction between different learning tasks;

• Our method achieves the best results on two
benchmark corpora in comparison with the
state-of-the-art systems so far.

The organization of this work is as follows.
Section 2 describes the proposed novel multi-task
neural network. Section 3 introduces the exper-

1299



imental settings in detail. Section 4 reports the
comprehensive experimental results on two bench-
mark corpora. Section 5 summarized related work.
Finally, Section 6 concludes this work.

2 Multi-task Attention-based Neural
Networks Models

2.1 Motivation

The idea of learning two types of interactive
knowledge from arguments and from multi-tasks
is motivated by the following observations and
analysis.

On the one hand, to recognize the discourse re-
lationships, our system needs to understand the
meaning of each argument and infer the discourse
sense transferred between two arguments (denoted
as Arg-1 and Arg-2). Learning the semantic rep-
resentation of each argument (sentence) has been
studied with the use of many neural network mod-
els and their variants (e.g., CNN, RNN, LSTM,
Bi-LSTM, ect). However, learning the complicat-
ed and various types of discourse relationships be-
tween arguments may not be performed by simply
summing up or concatenating two argument repre-
sentations. We analyze the discourse with contrast
relationship and find that the contrast information
may result from different parts of sentence, e.g.,
tenses (e.g., previous vs. now), entities (their vs
our), or even the whole arguments, etc. There-
fore, in order to learn the relationship represen-
tation between two arguments, we propose an at-
tention mechanism that can select out the most im-
portant part from two arguments and perform the
information interaction between two arguments.

On the other hand, one common issue involved
in implicit discourse relationship identification is
the lack of labeled data. In this work, we state
that the relevant information from unlabelled da-
ta might be helpful and we present a novel multi-
task learning framework. In contrast with previ-
ous multi-task learning framework in traditional
machine learning, we improve multi-task learning
framework with representation learning for better
discourse relationship representation.

Inspired by the above considerations, we
present a novel multi-task attention-based neural
network model by integrating attention mechanis-
m with multi-task learning for information inter-
action between arguments and between tasks.

2.2 Learning Discourse Representation

To learn the semantic representation of each argu-
ment in discourse, a lot of neural network mod-
els and their variants have been proposed, such
as, convolutional neural network (CNN), recurren-
t neural network (RNN) and so on. As a variant
of RNN, long-short term memory (LSTM) neural
network specifically addresses the issue of learn-
ing long-term dependencies and is good at model-
ing over a sequence of words with consideration
of the contextual information. Therefore, in this
work we adopt LSTM to model discourse argu-
ment.

2.2.1 LSTM for Argument Representation

Figure 1 shows the traditional LSTM model for
representation learning of arguments. First of al-

word embedding word embedding

+

Arg-1 Arg-2

softmax

LSTM LSTM

Loss

RArg1 RArg2

Figure 1: LSTM for discourse argument pair rep-
resentation learning.

l, through the embedding layer, we associate each
word w in the vocabulary with a vector represen-
tation xw ∈ Rdw . Let x1i (x2i ) be the i-th word
vector in Arg-1 (Arg-2), then these two discourse
arguments are represented as:

Arg-1: [x11,x
1
2, · · · ,x1L1 ] (1)

Arg-2: [x21,x
2
2, · · · ,x2L2 ] (2)

where Arg-1 (Arg-2) has L1 (L2) words.
Given the word representations of the argument

[x1,x2, · · · ,xL] as the input sequence, an LSTM
computes the state sequence [h1,h2, · · · ,hL] for

1300



each time step i using the following formulation:

ii = σ(Wi[xi,hi−1] + bi) (3)
fi = σ(Wf [xi,hi−1] + bf ) (4)
oi = σ(Wo[xi,hi−1] + bo) (5)

c̃i = tanh(Wc[xi,hi−1] + bc) (6)
ci = ii � c̃i + fi � ci−1 (7)

hi = oi � tanh(ci) (8)

where [ ] means the concatenation operation of
vectors, σ denotes the sigmoid function and � de-
notes element-wise product. Besides, ii, fi, oi
and ci denote the input gate, forget gate, output
gate and memory cell, respectively. Moreover, we
also use bidirectional LSTM (Bi-LSTM) which is
able to capture the context from both past and fu-
ture rather than LSTM which only considers the
context information from the past. Therefore, at
each position i of the sequence, we obtain two s-
tates

−→
h i and

←−
h i, where

−→
h i,
←−
h i ∈ Rdh . Then

we concatenate them to get the intermediate state,
i.e. hi = [

−→
h i,
←−
h i]. After that, we sum up the

sequence states [h1,h2, · · · ,hL] to get the repre-
sentations of Arg-1 and Arg-2 respectively as fol-
lows:

RArg1 =
L1∑
i=1

h1i (9)

RArg2 =
L2∑
i=1

h2i (10)

Finally we concatenate the two argument repre-
sentations RArg1 and RArg2 as the argument pair
representation, i.e., Rpair = [RArg1 ,RArg2 ].

Clearly, in this way, there is no any correla-
tion and interaction between the two arguments.
That is, whatever the types of discourse relation-
ship they hold, the argument pair representation
Rpair is independent from RArg1 or RArg2 .

2.2.2 Attention Neural Network for
Relationship Representation

In order to effectively capture the complicated and
various types of relationships between arguments,
we proposed a novel attention-based neural net-
work model shown in Figure 2.

To do it, we first compute the match between
RArg1 (RArg2) and each state h

2
i (h

1
i ) of Arg-2

(Arg-1) by taking the inner product followed by a

word embedding word embedding

x x x x x

Arg-1 Arg-2

x x x x

+

softmax

LSTM LSTM

Loss

RArg2RArg1

p
1

p
2

′RArg1 ′RArg2

Figure 2: Attention Neural Network for represen-
tation learning of arguments.

softmax as follows:

p1i = Softmax(R
T
Arg2h

1
i ) (11)

p2i = Softmax(R
T
Arg1h

2
i ) (12)

where Softmax(zi) = ezi/
∑

j e
zi . Here p is an

attention (probability) vector over the inputs and
can be viewed as the weights of the words mea-
suring to what degree our model should pay atten-
tion to. It is worth noting that p1 and p2 are de-
termined by RArg2 and RArg1 respectively, which
means the representation of one argument depends
on the representation of the other.

Next, we sum over the state hi weighted by the
attention vector p to compute the new representa-
tions for Arg-1 and Arg-2 respectively as below:

R′Arg1 =
L1∑
i=0

h1i p
1
i (13)

R′Arg2 =
L2∑
i=0

h2i p
2
i (14)

The representation of Arg-2 (RArg2) is used to
compute the weights of words in Arg-1 (i.e., p1)
and RArg1 is used to compute the weights of
words in Arg-2 (i.e., p2). In this way, the new
representations of the two arguments interact with
each other. Therefore, this attention mechanism
enables our model to focus on specific spans in the
two arguments, which is crucial to recognize the
discourse relations. We then concatenate R′Arg1

1301



and R′Arg2 to get the argument pair representation
Rpair = [R′Arg1 ,R

′
Arg2

].
Finally, we feed the argument pair vector Rpair

to a fully-connected softmax layer which outputs
the probabilities of different classes for the clas-
sification task. Here we choose the cross-entropy
loss between the outputs of the softmax layer and
the ground-truth class labels as our loss function.

2.3 Multi-task Attention-based Neural
Networks

The model presented in Section 2.2 can perfor-
m implicit discourse relation recognition in it-
self. However, similar with many models in deep
learning, one big issue is the lack of labeled da-
ta. Therefore, we propose a multi-task attention-
based neural network by integrating the aforemen-
tioned model into a multi-task learning framework
to address the implicit discourse relation recogni-
tion with the aid of large amount of unlabelled da-
ta. Figure 3 shows the general framework of our
proposed multi-task attention-based neural net-
work model.

Arg Pair

representation

Arg Pair

representation

Arg-1 Arg-2 Arg-1 Arg-2

Share

hidden layer

softmax

Main Task Aux Task

Loss
main

Loss
aux

R
main

R
aux

Figure 3: The framework of our proposed multi-
task attention-based neural network model.

We use the aforementioned attention-based
neural network to map the argument pair in-
to a low-dimensional vector (Rpair) denoted
as Arg Pair representation componen-
t in Figure 3. Under the multi-task learning
framework, the parameters of the Arg Pair
representation components are shared be-
tween the main task and the auxiliary tasks. We
denote Rmain and Raux as the representations of
argument pair for main and auxiliary tasks, respec-
tively. And we add a hidden layer after Rmain and

Raux to learn the task-specific representations fol-
lowed by the softmax layers used to compute the
loss of the main task (Lossmain) and the loss of
the auxiliary task (Lossaux), respectively.

Regarding the strategy of sharing knowledge
learnt from auxiliary to main task, we propose the
following three methods.

2.3.1 Equal Share
A simple and straightforward way is to equally
share the knowledge learned from main task and
auxiliary task. Therefore, the total loss of the
multi-task neural network is calculated as:

Loss = Lossmain + Lossaux (15)

where Lossaux has the same weight as Lossmain.

2.3.2 Weighted Share
Another method is to give different weights to the
main and auxiliary task as below:

Loss = Lossmain + w ∗ Lossaux (16)

where w ∈ (0, 1] is a weight parameter. Clearly, a
lower value of w means less importance of auxil-
iary task.

2.3.3 Sigmoid (Gated) Interaction
The above two ways of sharing knowledge actual-
ly have no deep interaction between the main and
auxiliary tasks. They only share equal or weighted
contributions from tasks to final result. Therefore,
we propose a model that can perform interaction
between tasks, which is shown in Figure 4.

We introduce two important parameters
Winter ∈ Rdpair×dpair and binter ∈ Rdpair
(dpair is the length of the argument pair repre-
sentation vector Rpair) to fulfil the interaction
between main and auxiliary tasks. As shown in
the following Formula (17) and (18), the new
representation of argument pair R′main is updated
by the combination of Winter and Raux using a
Sigmoid function.

R′main = Rmain � σ(WinterRaux + binter)
(17)

R′aux = Raux � σ(WinterRmain + binter)
(18)

Winter and the Sigmoid function (σ) work to-
gether to make information interacted between t-
wo tasks and select useful relevant information out
of the opposite tasks as well. Clearly, Winter is

1302



Arg Pair

representation

Arg Pair

representation

Arg-1 Arg-2 Arg-1 Arg-2

Share

hidden layer

softmax

Main Task Aux Task

WR
main

R
aux

′R
aux

′R
main

Loss
main

Loss
aux

Figure 4: Sigmoid (Gated) interaction shared in
multi-task framework (GShare).

a parameter to be trained. This mechanism act-
s as a gate to determine how much the informa-
tion would pass through to the final result. There-
fore, under the framework of multi-task and gated
mechanism, the main and auxiliary tasks are capa-
ble of not only sharing the parameters of learning
argument pair representation but also interacting
the representations learning from each other.

2.4 Parameter Learning

We tried various settings of word embeddings
trained on the BLLIP corpus with different dimen-
sions dWE = [50, 100, 150, 200] by word2vec
tool1 and finally set dimensionality as 50 based
on the results on development set. we also ex-
plored the hidden state dh = [50, 100, 150, 200]
and the size of hidden layer in multi-task frame-
work dmulti−task = [50, 80, 120, 150]. Finally,
for binary classification and four way classifica-
tion on PDTB, we chose dh = 50 and dmulti−task =
80. For multi-class classification on CoNLL-2016,
we chose dh = 100 and dmulti−task = 120. We
applied dropout to the penultimate layer and set
the dropout rate as 0.5. These parameters remain
the same in experiments except the share weight
w varies which will be discussed later. We chose
the cross-entropy loss as loss function and adopt-
ed AdaGrad (Duchi et al., 2011) with a learning
rate of 0.001 and a minibatch size of 64 to train
the model.

1http://www.code.google.com/p/word2vec

3 Experiment Settings

3.1 Datasets

We adopted three corpora: PDTB 2.0 and CoNLL-
2016 datasets are annotated for discourse relation
recognition evaluation, and the BLLIP corpus is
unlabeled and used as auxiliary task.

PDTB 2.0 is the largest annotated corpus of dis-
course relations, which contains 2, 312 Wall Street
Journal (WSJ) articles. The sense label of dis-
course relations is hierarchically with three lev-
els, i.e., class, type and sub-type. The top level
contains four major semantic classes: Comparison
(denoted as Comp.), Contingency (Cont.), Expan-
sion (Exp.) and Temporal (Temp.). For each class,
a set of types is used to refine relation sense. The
set of subtypes is to further specify the semantic
contribution of each argument. We focus on the
top level (class) relations. Following (Pitler et al.,
2009), we used sections 2-20 as training set, sec-
tions 21-22 as test set, and sections 0-1 as develop-
ment set. Table 1 summarizes the statistics of four
top level implicit discourse relations in PDTB.

Relation Train Dev Test
Comp. 1942 197 152
Cont. 3342 295 279
Exp. 7004 671 574
Temp. 760 64 85

Table 1: The statistics of four top level implicit
discourse relations in PDTB 2.0.

The CoNLL-2016 Shared Task focuses on
shallow discourse parsing, which provides two test
datasets, i.e., one from PDTB section 23 denot-
ed as CoNLL-Test set, and the other from a sim-
ilar source and domain (English Wikinews2) de-
noted as CoNLL-Blind test set. Different from
the sense labels in PDTB, the CoNLL-Test set has
three sense levels and the EntRel label. Moreover,
it merges several labels in the original annotation
to reduce some sparsity without losing too much
of the utility and the semantics of the sense.

BLLIP The North American News Text (Com-
plete) is used as unlabeled data source to generate
synthetic labeled data for auxiliary task. We re-
move the explicit discourse connectives from raw
texts and grant the explicit relations as the synthet-
ic implicit relations. We obtain a resulting corpus
with 100, 000 implicit relations by random sam-
pling.

2https://en.wikinews.org/

1303



3.2 Evaluation Measures
We adopt precision (P), recall (R) and their har-
monic mean, i.e., F1 for performance evaluation.
We also report accuracy for direct comparison
with previous works.

4 Results and Discussion

4.1 Results on PDTB in multiple binary
classification

To be consistent with previous work, we first per-
form multiple binary classification (one-versus-
other) on the four top level classes in PDTB. Sev-
eral previous studies merged EntRel with Expan-
sion, which is also explored in our study and noted
as Exp+. Table 2 shows the results of our proposed
three models in terms of F1 (%) on PDTB using
multiple binary classification, where STL means
single task learning, Eshare, Wshare and Gshare
denote the equal share, weighted share and gated
interaction share under multi-task framework re-
spectively, Imp denotes the standard implicit re-
lations dataset in PDTB (similarly, Imp denotes
standard implicit relations dataset in the CoNLL
dataset when we perform experiments on the CoN-
LL dataset) used for training, Exp denotes all ex-
plicit relations in sections 00-24 in PDTB (similar-
ly, all explicit relations in the CoNLL dataset when
we perform experiments on the CoNLL dataset),
and BLLIP denotes the synthetic implicit relations
extracted from BLLIP. For example, Imp + BLLIP
indicates that Imp is used for main task and BLLIP
is for auxiliary task.

The first three rows in Table 2 list the result-
s of LSTM, Bi-LSTM and attention neural net-
work in the single task learning (STL) framework,
which act as baselines for comparison with multi-
task learning. We see that Bi-LSTM achieve s-
lightly better performance than LSTM, which is
consistent with previous work as Bi-LSTM con-
siders the forward and backward direction contex-
tual information while LSTM only considers the
forward information. Compared with LSTM and
Bi-LSTM, the attention neural network achieves
much better performance. This indicates the effec-
tiveness of attention mechanism for capturing the
interaction between discourse arguments, which is
crucial for relationship representation.

Generally, under the multi-task neural network
framework, the three proposed multi-task neural
networks, i.e., Eshare, Wshare and Gshare, out-
perform the single task learning methods. Com-

paring with Eshare and Wshare, we see that us-
ing a low value of w is able to boost the perfor-
mance and reduce the negative influence brought
by auxiliary task. We then use the best w value
in Wshare to construct the loss of Gshare and the
Gshare achieves the best performance among all
methods through information interaction between
main and auxiliary tasks.

Comparing Imp + Exp with Imp + BLLIP, we
see that using Exp as auxiliary task achieves low-
er performance than using BLLIP and even hurt-
s the performance compared with the single task.
The possible reasons may result from (1) there is
difference between explicit and implicit discourse
relations and (2) the size of Exp dataset is much
smaller than that of BLLIP and thus it is not large
enough to boost the performance.

4.2 Results on PDTB and CoNLL-2016 in
multi-class classification

We also perform multi-class classification on
PDTB and CoNLL-2016. That is, a four-way clas-
sification on the four top-level classes in PDTB
and a 15-way classification on the 15 sense label-
s in CoNLL dataset. Table 3 shows the results of
multi-class classification on PDTB and CoNLL-
2016 corpora in terms of accuracy (%) and macro-
averaged F1 (%).

The results of multi-class classification are con-
sistent with the results of binary classification.
First, the attention neural network achieves bet-
ter performance than LSTM and Bi-LSTM. Sec-
ond, the multi-task learning methods outperform
the single-task learning method. Thrid, the Gshare
method achieves the best performance.

4.3 Comparison with the state-of-the-art
Systems

Table 4 lists the performance of our best mod-
el with the reported state-of-the-art systems on
PDTB and CoNLL-2016. We see that our mod-
el achieves F1 improvements of 1.64% on Con-
t., 0.97% on Exp., and 1.35% on Exp.+ against
the best reported systems in binary classification.
And in multi-class classification, our model also
achieves the best performance of F1 in four-way
classification and accuracy in CoNLL-2016 Blind
test set, which indicates that our model has good
generality.

Specially, (Liu et al., 2016b) and (Liu and
Li, 2016) listed in Table 4, which adopted neu-
ral network-based multi-task framework, are quite

1304



Comp. Cont. Exp. Exp+ Temp

STL
LSTM 33.50 52.09 67.51 76.12 27.88
Bi-LSTM 33.82 52.30 67.47 76.36 29.01
Attention 38.15 56.07 70.53 79.80 36.72

Eshare Imp + Exp 35.07 54.62 69.97 79.15 34.57Imp + BLLIP 37.67 56.82 70.81 80.43 35.48

Wshare Imp + Exp 37.51 (w=0.1) 55.83 (w=0.2) 70.37 (w=0.3) 80.22(w=0.2) 35.71 (w=0.3)Imp + BLLIP 39.13 (w=0.2) 57.78(w=0.2) 71.88(w=0.1) 80.84 (w=0.3) 37.76(w=0.3)

Gshare Imp + Exp 38.91 56.91 71.41 80.02 36.92Imp + BLLIP 40.73 58.96 72.47 81.36 38.50

Table 2: Performance of multiple binary classification on the top level classes in PDTB corpus in terms
of F1 (%).

PDTB (Four way) CoNLL-Test (Acc) CoNLL-Blind (Acc)

STL
LSTM F1: 36.16; Acc: 56.12 34.45 35.07
Bi-LSTM F1: 36.54; Acc: 54.30 34.85 35.83
Attention F1: 45.57; Acc: 57.55 37.41 38.36

Eshare Imp + Exp F1: 44.17; Acc: 55.65 35.56 37.06Imp + BLLIP F1: 44.57; Acc: 55.85 36.66 38.28

Wshare Imp + Exp F1: 45.03; Acc: 56.21 (w=0.3) 36.24 (w=0.2) 37.34 (w=0.3)Imp + BLLIP F1: 45.80; Acc: 58.95 (w=0.2) 38.13 (w=0.1) 39.14 (w=0.4)

Gshare Imp + Exp F1: 45.70; Acc: 57.17 37.84 38.10Imp + BLLIP F1: 47.80; Acc: 57.39 39.40 40.12

Table 3: Performance of multi-class classification on PDTB and CoNLL-2016 in terms of accuracy (Acc)
(%) and macro-averaged F1 (%).

Binary Classification (F1) Multi-class Classification (Acc)
Comp. Cont. Exp. Exp+ Temp PDTB (Four way) CoNLL-Test(Acc) CoNLL-Blind(Acc)

(Chen et al., 2016) 40.17 54.76 - 80.62 31.32 - - -
(Qin et al., 2016b) 41.55 57.32 71.50 80.96 35.43 - - -
(Liu and Li, 2016) 39.86 54.48 70.43 80.86 38.84 F1: 46.29; Acc: 57.57 - -
(Wu et al., 2016) - - - - - F1: 42.50; Acc: - - -
(Qin et al., 2016a) 38.67 54.91 - 80.66 32.76 - - -
(Liu et al., 2016b) 37.91 55.88 69.97 - 37.17 F1: 44.98; Acc: 57.27 - -
(Lan et al., 2013) 31.53 47.52 70.01 - 29.51 - - -
(Wang and Lan, 2016) - - - - - - 40.91 34.20
(Rutherford and Xue, 2016) - - - - - - 36.13 37.67
Our model 40.73 58.96 72.47 81.36 38.50 F1: 47.80; Acc: 57.39 39.40 40.12

Table 4: Comparison with the state-of-the-art systems reported on PDTB and CoNLL-2016, where -
means N.A.

relevant to this work. (Liu et al., 2016b) present-
ed a multi-task neural network, which considered
information sharing between the main and auxil-
iary task. Different from their work, our work inte-
grates the attention-based interaction between ar-
guments and the multi-task based interaction be-
tween tasks into the final model. This is the
main reason why our model achieves better per-
formance in all types of relations, which shows the
effectiveness of integrating gated mechanism into
multi-task framework. Besides, (Liu and Li, 2016)
used a complicated multi-level attention mecha-
nism and the performance of our attention neural
network in the single task is comparable to their
results. Our multi-task attention model achieves
better performance in most types with the aid of

multi-task framework.
Besides, our previous work in (Lan et al.,

2013) listed in Table 4, also presented a multi-
task framework with traditional machine learning
method to address implicit discourse recognition
using BLLIP to obtain synthetic data. Clearly, un-
der neural network-based multi-task framework,
the attention and gated mechanism significantly
improved the results and outperformed traditional
machine learning method in all types of relations.

4.4 Effects of parameters w
Figure 5 shows the performance of four binary
classification on four top level classes influenced
by different share weights w in Wshare multi-task
framework. We see that the best performance is
achieved when we use a lower value of w. This

1305



F
1

0.36

0.37

0.38

0.39

0.4

Weight

0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1

Comp.

F
1
 

0.55

0.558

0.565

0.573

0.58

Weight

0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1

Cont.

F
1

0.69

0.698

0.705

0.713

0.72

Weight

0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1

Exp.

F
1

0.34

0.35

0.36

0.37

0.38

Weight

0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1

Temp.

Figure 5: Results of top level implicit discourse
relations in PDTB 2.0 with different weights w.

indicates that a low value of w can boost perfor-
mance and reduce the negative influence brought
by auxiliary task and enable our model to pay
more attention to the main task.

5 Related Work

5.1 Implicit Discourse

With the release of PDTB 2.0, a number of studies
performed discourse relation recognition on nat-
ural (i.e., genuine) discourse data with the use of
traditional NLP techniques to extract linguistically
informed features and traditional machine learn-
ing algorithms (Pitler et al., 2009; Lin et al., 2009;
Wang et al., 2010; Braud and Denis, 2015; Fisher
and Simmons, 2015).

Later, to make a full use of unlabelled data, sev-
eral studies performed multi-task or unsupervised
learning methods (Lan et al., 2013; Braud and De-
nis, 2015; Fisher and Simmons, 2015; Rutherford
and Xue, 2015).

Recently, with the development of deep learn-
ing, researchers resorted to neural networks meth-
ods (Ji and Eisenstein, 2015; Zhang et al., 2015;
Chen et al., 2016; Liu et al., 2016b; Qin et al.,
2016a; Liu and Li, 2016; Braud and Denis, 2016;
Wu et al., 2016).

5.2 Multi-task learning

Multi-task learning framework adopts traditional
machine learning with human-selected effective
knowledge and the shared part is integrated into
the cost function to prefer the main task learning.
(Collobert and Weston, 2008) proposed a multi-

task neural network trained jointly on the rele-
vant tasks using weight-sharing (sharing the word
embeddings with tasks). (Liu et al., 2016a) pro-
posed the multi-task neural network by modifying
the recurrent neural network for text classification
tasks. (Lan et al., 2013) present a multi-task learn-
ing based system which can effectively use syn-
thetic data for implicit discourse relation recogni-
tion. (Wu et al., 2016) use bilingually-constrained
synthetic implicit data for implicit discourse rela-
tion recognition a multi-task neural network. (Liu
et al., 2016b) propose a convolutional neural net-
work embedded multi-task learning system to im-
prove the performance of implicit discourse iden-
tification.

5.3 Deep learning with Attention

Recently deep learning with attention has been
widely adopted by NLP researchers. (Zhou et al.,
2016) proposed an attention-based Bi-LSTM for
relation classification. (Wang et al., 2016c) pro-
posed an attention-based LSTM for aspect-level
sentiment classification. (Tan et al., 2016) pro-
posed a attentive LSTMs for Question Answer
Matching. (Wang et al., 2016a) proposed an in-
ner attention based RNN (add attention informa-
tion before RNN hidden representation) for An-
swer Selection in QA. (Wang et al., 2016b) pro-
posed multi-level attention CNNs for relation clas-
sification. (Yin et al., 2016) proposed an attentive
convolutional neural network for QA.

6 Concluding Remarks

We present a novel multi-task attention-based neu-
ral network model for implicit discourse relation-
ship representation and identification. Our method
captures both the discourse relationships through
interactions between discourse arguments and the
complementary knowledge through interactions
between annotated and unannotated data. The ex-
perimental results showed that our proposed mod-
el outperforms the state-of-the-art systems on two
benchmark corpora.

Acknowledgments

This work is supported by grants from Science
and Technology Commission of Shanghai Munici-
pality (14DZ2260800 and 15ZR1410700), Shang-
hai Collaborative Innovation Center of Trustwor-
thy Software for Internet of Things (ZF1213) and
NSFC (61402175).

1306



References
Chloé Braud and Pascal Denis. 2015. Comparing word

representations for implicit discourse relation classi-
fication. In Empirical Methods in Natural Language
Processing (EMNLP 2015).

Chloé Braud and Pascal Denis. 2016. Learning
connective-based word representations for implicit
discourse relation identification. In Proceedings of
the 2016 Conference on Empirical Methods in Nat-
ural Language Processing, pages 203–213, Austin,
Texas. Association for Computational Linguistics.

Jifan Chen, Qi Zhang, Pengfei Liu, Xipeng Qiu, and X-
uanjing Huang. 2016. Implicit discourse relation de-
tection via a deep architecture with gated relevance
network. In Proceedings of ACL.

Ronan Collobert and Jason Weston. 2008. A unified
architecture for natural language processing: Deep
neural networks with multitask learning. In Pro-
ceedings of the 25th international conference on
Machine learning, pages 160–167. ACM.

John Duchi, Elad Hazan, and Yoram Singer. 2011.
Adaptive subgradient methods for online learning
and stochastic optimization. The Journal of Ma-
chine Learning Research, 12:2121–2159.

Robert Fisher and Reid Simmons. 2015. Spectral semi-
supervised discourse relation classification. Volume
2: Short Papers.

Yangfeng Ji and Jacob Eisenstein. 2015. One vector is
not enough: Entity-augmented distributed semantics
for discourse relations. Transactions of the Associa-
tion for Computational Linguistics, 3:329–344.

Man Lan, Yu Xu, and Zheng-Yu Niu. 2013. Leveraging
synthetic discourse data via multi-task learning for
implicit discourse relation recognition. In ACL (1),
pages 476–485. Citeseer.

Ziheng Lin, Min-Yen Kan, and Hwee Tou Ng. 2009.
Recognizing implicit discourse relations in the pen-
n discourse treebank. In Proceedings of the 2009
Conference on Empirical Methods in Natural Lan-
guage Processing: Volume 1-Volume 1, pages 343–
351. Association for Computational Linguistics.

Pengfei Liu, Xipeng Qiu, and Xuanjing Huang. 2016a.
Recurrent neural network for text classification
with multi-task learning. arXiv preprint arX-
iv:1605.05101.

Yang Liu and Sujian Li. 2016. Recognizing implicit
discourse relations via repeated reading: Neural net-
works with multi-level attention. In Proceedings of
the 2016 Conference on Empirical Methods in Natu-
ral Language Processing, pages 1224–1233, Austin,
Texas. Association for Computational Linguistics.

Yang Liu, Sujian Li, Xiaodong Zhang, and Zhifang Sui.
2016b. Implicit discourse relation classification vi-
a multi-task neural networks. arXiv preprint arX-
iv:1603.02776.

Emily Pitler, Annie Louis, and Ani Nenkova. 2009.
Automatic sense prediction for implicit discourse re-
lations in text. In Proceedings of the Joint Confer-
ence of the 47th Annual Meeting of the ACL and the
4th International Joint Conference on Natural Lan-
guage Processing of the AFNLP: Volume 2-Volume
2, pages 683–691. Association for Computational
Linguistics.

Rashmi Prasad, Nikhil Dinesh, Alan Lee, Eleni Milt-
sakaki, Livio Robaldo, Aravind K Joshi, and Bon-
nie L Webber. 2008. The Penn Discourse TreeBank
2.0. In LREC. Citeseer.

Lianhui Qin, Zhisong Zhang, and Hai Zhao. 2016a.
Implicit discourse relation recognition with con-
textaware character-enhanced embeddings. In the
26th International Conference on Computational
Linguistics (COLING), Osaka, Japan, December.

Lianhui Qin, Zhisong Zhang, and Hai Zhao. 2016b. A
stacking gated neural architecture for implicit dis-
course relation classification. In Proceedings of the
2016 Conference on Empirical Methods in Natu-
ral Language Processing (EMNLP), Austin, USA,
November.

Attapol Rutherford and Nianwen Xue. 2015. Improv-
ing the inference of implicit discourse relations via
classifying explicit discourse connectives. In Pro-
ceedings of the NAACL-HLT.

Attapol T Rutherford and Nianwen Xue. 2016. Robust
non-explicit neural discourse parser in english and
chinese. ACL 2016, page 55.

Ming Tan, Cicero dos Santos, Bing Xiang, and Bowen
Zhou. 2016. Improved representation learning for
question answer matching. In Proceedings of the
54th Annual Meeting of the Association for Compu-
tational Linguistics (Volume 1: Long Papers), pages
464–473, Berlin, Germany. Association for Compu-
tational Linguistics.

Bingning Wang, Kang Liu, and Jun Zhao. 2016a. Inner
attention based recurrent neural networks for answer
selection. In Proceedings of the 54th Annual Meet-
ing of the Association for Computational Linguistics
(Volume 1: Long Papers), pages 1288–1297, Berlin,
Germany. Association for Computational Linguis-
tics.

Jianxiang Wang and Man Lan. 2016. Two end-to-end
shallow discourse parsers for english and chinese in
conll-2016 shared task. Proceedings of the CoNLL-
16 shared task, pages 33–40.

Linlin Wang, Zhu Cao, Gerard de Melo, and Zhiyuan
Liu. 2016b. Relation classification via multi-level
attention cnns. In Proceedings of the 54th Annu-
al Meeting of the Association for Computational
Linguistics (Volume 1: Long Papers), pages 1298–
1307, Berlin, Germany. Association for Computa-
tional Linguistics.

1307



WenTing Wang, Jian Su, and Chew Lim Tan. 2010. K-
ernel based discourse relation recognition with tem-
poral ordering information. In Proceedings of the
48th Annual Meeting of the Association for Compu-
tational Linguistics, pages 710–719. Association for
Computational Linguistics.

Yequan Wang, Minlie Huang, xiaoyan zhu, and
Li Zhao. 2016c. Attention-based lstm for aspect-
level sentiment classification. In Proceedings of the
2016 Conference on Empirical Methods in Natu-
ral Language Processing, pages 606–615, Austin,
Texas. Association for Computational Linguistics.

Changxing Wu, xiaodong shi, Yidong Chen, Yanzhou
Huang, and jinsong su. 2016. Bilingually-
constrained synthetic data for implicit discourse re-
lation recognition. In Proceedings of the 2016 Con-
ference on Empirical Methods in Natural Language
Processing, pages 2306–2312, Austin, Texas. Asso-
ciation for Computational Linguistics.

Wenpeng Yin, Mo Yu, Bing Xiang, Bowen Zhou, and
Hinrich Schütze. 2016. Simple question answering
by attentive convolutional neural network. In Pro-
ceedings of COLING 2016, the 26th International
Conference on Computational Linguistics: Techni-
cal Papers, pages 1746–1756, Osaka, Japan. The
COLING 2016 Organizing Committee.

Biao Zhang, Jinsong Su, Deyi Xiong, Yaojie Lu, Hong
Duan, and Junfeng Yao. 2015. Shallow convolu-
tional neural network for implicit discourse relation
recognition. In Proceedings of the 2015 Conference
on Empirical Methods in Natural Language Pro-
cessing, pages 2230–2235.

Peng Zhou, Wei Shi, Jun Tian, Zhenyu Qi, Bingchen
Li, Hongwei Hao, and Bo Xu. 2016. Attention-
based bidirectional long short-term memory net-
works for relation classification. In Proceedings
of the 54th Annual Meeting of the Association for
Computational Linguistics (Volume 2: Short Paper-
s), pages 207–212, Berlin, Germany. Association for
Computational Linguistics.

Zhi-Min Zhou, Yu Xu, Zheng-Yu Niu, Man Lan, Jian
Su, and Chew Lim Tan. 2010. Predicting discourse
connectives for implicit discourse relation recogni-
tion. In Proceedings of the 23rd International Con-
ference on Computational Linguistics, pages 1507–
1514, Stroudsburg, PA, USA. Association for Com-
putational Linguistics.

1308


