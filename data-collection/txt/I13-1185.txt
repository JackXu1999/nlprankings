










































Source and Translation Classification using Most Frequent Words


International Joint Conference on Natural Language Processing, pages 1299–1305,
Nagoya, Japan, 14-18 October 2013.

Source and Translation Classification using Most Frequent Words

Zahurul Islam
AG Texttechnology

Institut für Informatik
Goethe-Universität Frankfurt

zahurul@em.uni-frankfurt.de

Armin Hoenen
AG Texttechnology

Institut für Informatik
Goethe-Universität Frankfurt

hoenen@em.uni-frankfurt.de

Abstract

Recently, translation scholars have made
some general claims about translation
properties. Some of these are source lan-
guage independent while others are not.
Koppel and Ordan (2011) performed em-
pirical studies to validate both types of
properties using English source texts and
other texts translated into English. Ob-
viously, corpora of this sort, which focus
on a single language, are not adequate for
claiming universality of translation prop-
erties. In this paper, we are validating both
types of translation properties using origi-
nal and translated texts from six European
languages.

1 Introduction

Even though it is content words that are seman-
tically rich, function words also play an impor-
tant role in a text. Function words are more fre-
quent and predictable than content words. Gen-
erally, function words carry grammatical informa-
tion about content words. High frequency func-
tion words are relatively shorter than mid/low fre-
quency function words (Bell et al., 2008). Due to
their high frequency in texts and their grammatical
role, function words also indicate authorial style
(Argamon and Levitan, 2005). These words could
play an important role in translated text and in the
translation process.

Source and translation classification is useful
for some Natural Language Processing (NLP) ap-
plications. Lembersky et al. (2011) have shown
that a language model from translated text im-
proves the performace of a Machine Translation
(MT) system. A source and translation classifier

can be used to identify translated text. This appli-
cation also can be used to detect plagiarism where
the plagiarised text is translated from another lan-
guage.

From the early stage of translation studies
research, translation scholars proposed different
kinds of properties of source text and translated
text. Recently, scholars in this area identified sev-
eral properties of the translation process with the
aid of corpora (Baker, 1993; Baker, 1996; Olohan,
2001; Laviosa, 2002; Hansen, 2003; Pym, 2005).
These properties are subsumed under four key-
words: explicitation, simplification, normalization
and levelling out. They focus on the general ef-
fects of the translation process.

Toury (1995) has a different theory from these.
He stated that some interference effects will be
observable in the translated text. That is, a trans-
lated text will carry some fingerprints of its source
language. Specific properties of the English lan-
guage are visible in user manuals that have been
translated to other languages from English (for in-
stance, word order) (Lzwaini, 2003). Recently,
Pastor et al. (2008) and Ilisei et al. (2009; 2010)
have provided empirical evidence of simplification
translation properties using a comparable corpus
of Spanish.

Koppel and Ordan (2011) perform empirical
studies to validate both theories, using a sub-
corpus extracted from the Europarl (Koehn, 2005)
and IHT corpora (Koppel and Ordan, 2011). They
used a comparable corpus of original English and
English translated from five other European lan-
guages. In addition, original English and English
translated from Greek and Korean was also used
in their experiment. They have found that a trans-
lated text contains both source language depen-
dent and independent features.

1299



Obviously, corpora of this sort, which focus
on a single language (e.g., English), are not ade-
quate for claiming the universal validity of transla-
tion properties. Different languages (and language
families) have different linguistic properties. A
corpus that contains original and translated texts
from different source languages will be ideal for
this kind of study. In this paper, we are validating
both types of translation properties using original
and translated texts from six European languages.
As features, we used frequencies of the 100 most
frequent words of each target language.

The paper is organized as follows: Section 2
discusses related work, followed by an introduc-
tion of our corpus in Section 3. The experiment
and evaluation in Section 4 are followed by a dis-
cussion in Section 5. Finally, we present conclu-
sions and future work in Section 6.

2 Related Work

Corpus-based translation studies is a recent field
of research with a growing interest within the field
of computational linguistics. Baroni and Bernar-
dini (2006) started corpus-based translation stud-
ies empirically, where they work on a corpus of
geo-political journal articles. A Support Vector
Machine (SVM) was used to distinguish original
and translated Italian text using n-gram based fea-
tures. According to their results, word bigrams
play an important role in the classification task.

Van Halteren (2008) uses the Europarl corpus
for the first time to identify the source language
of text for which the source language marker was
missing. Support vector regression was the best
performing method.

Pastor et al. (2008) and Ilisei et al. (2009;
2010) perform classification of Spanish original
and translated text. The focus of their works is to
investigate the simplification relation that was pro-
posed by (Baker, 1996). In total, 21 quantitative
features (e.g. a number of different POS, average
sentence length, the parse-tree depth etc.) were
used where, nine (9) of them are able to grasp the
simplification translation property.

Koppel and Ordan (2011) have built a clas-
sifier that can identify the correct source of the
translated text (given different possible source lan-
guages). They have built another classifier which
can identify source text and translated text. Fur-
thermore, they have shown that the degree of dif-
ference between two translated texts, translated

from two different languages into the same target
language reflects, the degree of difference of the
source languages. They have gained impressive
results for both of the tasks. However, the limita-
tion of this study is that they only used a corpus
of English original text and English text translated
from various European languages. A list of 300
function words (Pennebaker et al., 2001) was used
as feature vector for these classifications.

Popescu (2011) uses string kernels (Lodhi et al.,
2002) to study translation properties. A classi-
fier was built to classify English original texts and
English translated texts from French and German
books that were written in the nineteenth century.
The p-spectrum normalized kernel was used for
the experiment. The system works on a charac-
ter level rather than on a word level. The system
performs poorly when the source language of the
training corpus is different from the one of the test
corpus.

We can not compare our findings directly with
Koppel and Ordan (2011) even though we use text
from the same corpus and similar techniques. The
English language is not considered for this study
due to unavailability of English translations for
some languages included in this work. Further-
more, instead of the list of 300 function words
used by Koppel and Ordan (2011), we used the
100 most frequent words for each candidate lan-
guage.

3 Data

The field of translation studies lacks a multilin-
gual corpus that can be used to validate translation
properties proposed by translation scholars. There
are many multilingual corpora available used for
different NLP applications. A customized version
of the Europarl corpus (Islam and Mehler, 2012) is
freely available for corpus-based translation stud-
ies. However, this corpus is not suitable for the
experiment we are performing here. We extract a
suitable corpus from the Europarl corpus in a way
similar to Lembersky et al. (2011) and Koppel and
Ordan (2011). Our target is to extract texts that
are translated from and to the languages consid-
ered here. We trust the source language marker
that has been put by the respective translator, as
did Lembersky et al.(2011) and Koppel and Ordan
(2011).

To experiment with stylistic differences in
translated text, a list of function words and their

1300



German Dutch French Spanish Polish Czech

German - 2,574,110 4,757,076 2,035,736 584,114 215,212
Dutch 4,881,949 - 4,386,270 2,682,935 446,702 149,235
French 5,241,411 659,001 - 2,724,897 659,001 226,435
Spanish 4,020,898 1,925,157 3,696,393 - 662,718 247,219
Polish 451,357 112,274 695,360 194,724 - 82,312
Czech 378,300 105,058 684,061 187,236 214,959 -

Table 1: The customized corpus for source lan-
guage identification (number of words per lan-
guage)

respective native frequencies is necessary. Since
for many languages such a list does not exist,
we pursue an alternative strategy. A list of the
100 most frequent words is available for many
languages and since at the same time the major-
ity of these first 100 most frequent words of any
language are function words, we use these lists.
The 100 most frequent German words are taken
from the Deutscher Wortschatz.1 The most fre-
quent Czech word list is taken from the freely
available Czech national corpus.2 The 100 most
frequent Spanish words are taken from the book A
Frequency Dictionary of Spanish: Core Vocabu-
lary for Learners (Davies, 2006). The French most
frequent words are taken from the A Frequency
Dictionary of French: Core Vocabulary for Learn-
ers (Lonsdale and Bras, 2009). The 100 most fre-
quent Dutch words are taken from snowball.3 The
most frequent Polish word list are collected from
the Polish scientific publisher PWN.4

4 Experiment

In order to validate two different kinds of transla-
tion properties mentioned in Section 1, two differ-
ent experiments will be performed. For the first
experiment, our hypothesis is that texts translated
into the same language from different source lan-
guages have different properties, a trained classi-
fier will be able to classify texts based on different
sources. Our second hypothesis is that translated
texts are distinguishable from source texts; a clas-
sifier can be trained to identify translated and orig-
inal texts. Note that we use the Naive Bayes multi-
nomial classifier (Mccallum and Nigam, 1998) in
WEKA (Hall et al., 2009) for classification. To
overcome the data over-fitting problem, we ran-
domly generate training and test set N times and
calculate the weighted average of F-Score and Ac-

1http://wortschatz.informatik.uni-leipzig.de/
2http://ucnk.ff.cuni.cz/syn2005.php
3http://snowball.tartarus.org/algorithms/dutch/stop.txt
4http://korpus.pwn.pl/stslow en

German Dutch French Spanish Polish Czech

German - 197 197 198 201 197
Dutch 197 - 197 198 198 191
French 148 147 - 148 149 157
Spanish 148 147 148 - 148 148
Polish 151 141 149 148 - 129
Czech 140 164 149 148 151 -

Table 2: Source language identification corpus
(chunks)

curacy. In this experiment the value of N is 100.
The randomly generated training sets contain 80%
of the data while the remaining data is used as a
test set. To evaluate the classification results, we
use standard F-Score and Accuracy measures.

4.1 Source Language Identification

In this experiment, our goal is to validate the trans-
lation properties postulated by Toury (1995). He
stated that a translated text inherits some finger-
prints from the source language. The experimen-
tal result of Koppel and Ordan (2011) shows that
text translated into English holds this property. If
this characteristic also holds for text translated into
other languages, then it will corroborate the claim
by Toury (1995). If it does not hold for a single
language then it might be claimed that this trans-
lation property is not universal. In order to train
a classifier, we use texts translated into the same
language from different source languages. Table 1
shows the statistics of the corpus used for source
language identification experiments. Later, each
corpus is divided into a number of chunks (see Ta-
ble 2). Each chunk contains at least seven sen-
tences. Our hypothesis is again similar to Kop-
pel and Ordan (2011), that is, if the classifier’s ac-
curacy is close to 20%, then we cannot say that
there is an interference effect in translated text.
If the classifier’s accuracy is close to 100% then
our conclusion will be that interference effects ex-
ist in translated text. Table 3 and Table 4 show
the evaluation results. Table 3 shows the F-Scores
for translated text from different source languages.
Rows represent translated texts and columns rep-
resent source languages.

A first minor observation can be made, in that
the consistency of the results increases when an-
alyzing them with respect to the concept of lan-
guage family. The term language family is broadly
used in linguistics as a denomination of groups
of languages that have descended fom a common

1301



German Dutch French Spanish Polish Czech

German - 0.97 0.95 0.95 0.80 0.72
Dutch 0.90 - 0.90 0.89 0.62 0.67
French 0.96 0.96 - 0.95 0.78 0.71
Spanish 0.95 0.96 0.87 - 0.74 0.69
Polish 0.53 0.41 0.61 0.48 - 0.49
Czech 0.47 0.36 0.54 0.39 0.67 -

Table 3: Source language identification evaluation
(F-Score)

Translated Text Accuracy

German 88.2%
Dutch 81.1%
French 87.4%
Spanish 84.7%
Polish 51.3%
Czech 50.5%

Table 4: Source language identification evaluation
(Accuracy)

ancestor. In the vast majority of cases, members
of the same language family share a considerable
number of words and grammatical structures. In
the experiment, we consider three language fam-
ilies: Romance languages (French and Spanish),
Germanic languages (German and Dutch), and
Slavic languages (Polish and Czech).

With a Romance target language,5 the identi-
fication of other Romance and of Germanic lan-
guages as translation sources performs high, with
an F-Score of between 0.86 and 0.95. However, a
noticeable drop in performance concerns the iden-
tification of the Slavic languages.

When we take a look at the confusion matrices
for the respective classifications, we find that, for
instance, most misclassifications in the French tar-
get language data are between the sources of Pol-
ish and Czech. For Germanic target languages, the
pattern repeats: when translated into German or
Dutch, Polish and Czech texts are hardest to iden-
tify as the correct source.

The Slavic target languages show a different
pattern. Even in another Slavic target language,
a Slavic source language cannot reliably be iden-
tified in our setting. In addition to this, transla-
tions into Slavic are harder to distinguish from
each other. Misclassifications in this case show
language family specific patterns: German is, for
instance, most often misclassified as Dutch in both
the Czech and the Polish data.

5Target language refers to text translated into the language

4.2 Source Translation Classification

Translated texts have distinctive features that make
them different from original or non translated text.
According to Baker (1993; 1996), Olohan (2001),
Lavisoa (2002), Hansen (2003), and Pym (2005)
there are some general properties of translations
that are responsible for the difference between
these two text types. Some of these properties are
source and target language independent. Accord-
ing to their findings, a translated text will be sim-
ilar to another translated text but will be different
from a source text. In the past, researchers have
used comparable corpora to validate these transla-
tion properties (Baroni and Bernardini, 2006; Pas-
tor et al., 2008; Ilisei et al., 2009; Ilisei et al.,
2010; Koppel and Ordan, 2011). Most of them
used comparable corpora for two-class classifica-
tion, distinguishing translated texts from the origi-
nal texts. Only Koppel and Ordan (Koppel and Or-
dan, 2011) used English texts translated from mul-
tiple source languages. We perform similar exper-
iments only for six European languages as shown
in Table 1. In this experiment, the translated text in
our training and test set will be a combination of
all languages other than the target language. For
example: when the original class contains orig-
inal texts (source) in German, then the transla-
tion class contains texts that are translated Ger-
man texts, translated from French, Dutch, Span-
ish, Polish, and Czech texts. Each class contains
200 chunks of texts, where as the translated class
has 40 chunks from each of the source languages.
The source language texts are extracted for the
corresponding languages in a similar way from
the Europarl corpus. Koppel and Ordan (2011)
received the highest accuracy (96.7%) among all
works noted above. The training and test data are
generated in similar ways as in our previous ex-
periment. That is, 80% of the data is randomly ex-
tracted for training and the rest of the data is used
for testing. Expected F-Scores are calculated from
100 samples. Table 5 shows the evaluation results.
Even though the classifier for German achieves
around 99% accuracy, we cannot compare the re-
sult with Koppel and Ordan (Koppel and Ordan,
2011) as the amount of chunks for the classes are
different. The classifiers for other languages also
display very high accuracy.

The result of Table 5 shows that general transla-
tion properties exist for all languages used in this
experiment.

1302



Language Accuracy F-Score

German 99.9% 0.99
Dutch 95.1% 0.95
French 81.9% 0.81
Spanish 94.4% 0.94
Polish 93.3% 0.93
Czech 81.1% 0.81

Table 5: Source translation classification

5 Discussion

The results show that training a classifier based on
the 100 most frequent words of a language is suf-
ficient to obtain interpretable results. We find our
results to be compatible with Koppel and Ordan
(2011) who used 300 function words. A list of the
100 most frequent words is easily obtainable for
a vast number of languages, while lists consisting
strictly of function words are rare and cannot be
produced without considerable additional effort.

While the 100 most frequent words of a lan-
guage are sufficient to train a classifier for Ger-
manic or Romance languages, it fails to perform
equally well for Slavic languages. Koppel and Or-
dan (2011) claim that Toury’s (1995) findings of
interference of a translation hold true; we find the
assumption to be too simplistic, since for Slavic
text either as a source or target language this state-
ment cannot supported.

Although function words do exist in all the lan-
guages we examined, the language families dif-
fer in the degree to which it is necessary to use
them. For instance, French lacks a case system
(Dryer and Haspelmath, 2011), and makes instead
use of prepositions. On the other hand, Polish
and Czech most extensively use (inflectional) af-
fixes (Kulikov et al., 2006). Regarding the dis-
tribution of word frequencies, for both Polish and
Czech, the use of affixes causes a flatter Zipf
curve. Kwapien et al. (2010) put it so :“...typical
Polish texts have smaller α [as exponent of the for-
mula f(r) ∼ r−α] than typical English texts (on
average: 0.95 vs. 1.05).” This means that on aver-
age a more frequent word does not differ as much
in its frequency from a word 10 ranks further down
in Polish as it does in English. Consequently, there
will be fewer instances of the 100 most frequent
words in the same portion of text. This is an ob-
vious reason why a classifier’s training must re-
main weaker in comparison to languages with a
steeper Zipf curve. There is a positive correlation
to language family when considering the probabil-

ity of finding the same strategy (e.g. prepositions
vs. affixes). In summary, the fact that Slavic uses
more affixes, or is more inflectional in linguistic
terms, explains to some extent why the classifier
performs worst for Slavic target text.

However, for Slavic source texts, the classifica-
tion results are equally unsatisfactory, which has
to be explained differently. One phenomenon con-
tributing here could be that Romance and Ger-
manic have a recent history of mutual loans and
calques, which increases the probability of finding
synonyms where one has a Romance origin and
one a Germanic origin. In the case of a translation,
the translator, when confronted with such a syn-
onym, might choose the item similar to the source
language within the target language, as this min-
imizes the translation effort, complies thus to an
economy principle and has virtually no effect on
the translation.6 Making this choice, the translator
unintentionally distorts the native frequency pat-
terns for the target language. This could be one
of the processes generating an imprint of trans-
lated text in the frequency spectrum, since func-
tion words are also subject to loaning and syn-
onymy.

If the translator has a choice for translating a
preposition/affix and neither of the possibilities is
similar to the source language, nor a loanword
or structurally similar, he/she will go for the pre-
dominant word or structure of the target language
(since he/she is a native of the target language by
translation industry standard), making the transla-
tion less different from native text. The data can
be influenced by many additional variables such
as differing translation paradigms influencing the
choice of structures (free translation vs. faithful
translation), different industry standards, the size
of the chunks,7 the quality of the translation source
marking, the native tongue of the translator(s), the
time pressure for delivery, the payment, the mem-
bership of all sample languages to the European
subbranch of Indo-European languages, the quali-
ties of the lists of the most frequent 100 words, the
genre of the Europarl corpus, and possibly many
more.

This said, we believe the best hypothesis for the

6For French to English translations an example would
be the translators choice of “intelligent” as a translation for
French “intelligent” in a place where “smart” would have
been slightly more natural.

7Since a short text contains fewer anaphora and thus per-
sonal pronouns.

1303



interpretation of the data is that a good classifica-
tion result is reached firstly for languages with a
more isolating structure, since they make less use
of affixes and therefore more of function words,
and should display steeper Zipf curves. Secondly,
the classification result should be better, the more
instances the text contains, where the translator for
one token (or for one structure) of the source lan-
guage has the choice between at least two words
or structures in the target language with one of
those being similar to the source language, the
other being different. The number of such in-
stances most probably correlates positively with
the degree and quality of language relationship
and language contact since the number of cog-
nates, loans and calques does. However, this num-
ber can also be “accidentally high” for two unre-
lated languages when they overlap in grammatical
structure. As has been postulated for instance by
Croft (2003), languages undergo a cyclic devel-
opment from structurally more isolating towards
agglutinative to inflectional and then back to iso-
lating. When a language is in a state of transi-
tion, which practically all languages are, they offer
two structural encoding possibilities for one spe-
cific grammatical property, e.g., a genitive (for in-
stance, inflectional (an affix) as in Peter’s house
and isolating (a preposition) as in the house of Pe-
ter). All languages should share structural prop-
erties, since there are only three types and each
language has practically at least two.

Corroborating this rather complex hypothesis,
we examine data on Bulgarian and Romanian. We
take Bulgarian as the target language. The data
showed that the classifier classifies Czech text no
worse than Dutch or German and only slightly
worse than French. When we replace Czech with
Bulgarian and Spanish with Romanian in the Ger-
man target language, the language family depen-
dent pattern gets blurred and the identification of
Polish performs quite well, that of Romanian rel-
atively poor, while French is identified reliably.
This together with the observation that Romanian
is misclassified either as Polish or Bulgarian and
Bulgarian is mostly misclassified as Romanian
seems to be a strong hint towards the impact of
the language specific usage of function words, lin-
guistic structure, and the importance of language
contact. Bulgarian and Romanian constitute the
core of the most prominent linguistic contact zone
or sprachbund ever written on the Balkans. This

suggests that Romanian and Bulgarian translators
may, due to grammatical convergence of their lan-
guages make, given two equivalent structures in
any target language, the same (structurally moti-
vated) choices and hence leave a very similar im-
print. That is, sprachbund membership as well
as language family could be decisive factors for
a classifiers performance.

6 Conclusion

We have shown that interference as originally pro-
posed by Toury (1995) is not supported by the data
without making further assumptions. Language
family and language contact should be considered
separately for each language pair as sources for
possible weak results of a classifier even when op-
erating with function words as should be general
structural similarity. As for the properties of trans-
lated text being universal, we found support for
this in our data in a real n-ary validation setting.
We have also shown that the much more easily ob-
tainable lists of the 100 most frequent words work
almost as well for classification as do longer lists
that contain only function words.

7 Acknowledgments

We would like to thank Prof. Moshe Koppel and
Dr. Noam Ordan for providing their data, and
Prof. Dr. Alexander Mehler for suggestions and
comments. We also thank Dr. Timothy Price for
checking English and three anonymous review-
ers. This work is funded by the LOEWE Digital-
Humanities project at the Goethe-Universität
Frankfurt.

References
Shlomo Argamon and Shlomo Levitan. 2005. Measur-

ing the usefulness of function words for authorship
attribution. In The Joint Conference of the Associa-
tion for Computers and the Humanities and the As-
sociation for Literary and Linguistic Computing.

Mona Baker. 1993. Corpus linguistics and translation
studies - implications and applications. In Mona
Baker, Gill Francis, and Elena Tognini-Bonelli, ed-
itors, Text and Technology. In Honour of John Sin-
clair, pages 233–354. John Benjamins.

Mona Baker. 1996. Corpus-based translation studies:
The challenges that lie ahead. In LSP and Trans-
lation: Studies in Language Engineering in Hon-
our of Juan C. Sager, pages 175–186. Amsterdam
& Philadelphia: John Benjamins.

1304



Marco Baroni and Silvia Bernardini. 2006. A new ap-
proach to the study of translationese: Machinelearn-
ing the difference between original and translated
text. Literary and Linguistic Computing, 21(3):259–
274.

Alan Bell, Jason M. Brenier, Michelle Gregory, Cyn-
thia Girand, and Dan Jurafsky. 2008. Predictability
effects on durations of content and function words in
conversational english. Elsevier Journal of Memory
and Language, 60:92–111.

William Croft. 2003. Typology and Universals. Cam-
bridge textbooks in linguistics. Cambridge Univer-
sity Press.

Mark Davies. 2006. A Frequency Dictionary of Span-
ish: Core Vocabulary for Learners. Taylor & Fran-
cis.

Matthew S. Dryer and Martin Haspelmath. 2011. The
world atlas of language structures online.

Mark Hall, Eibe Frank, Geoffrey Holmes, Bernhard
Pfahringer, Peter Reutemann, and Ian H. Witten.
2009. The WEKA data mining software: an update.
ACM SIGKDD Explorations, 11(1):10–18.

Silvia Hansen. 2003. The Nature of Translated Text:
An Interdisciplinary Methodology for the Investiga-
tion of the Specific Properties of Translations. Ph.D.
thesis, University of Saarland.

Iustina Ilisei, Diana Inkpen, Gloria Corpas Pastor, and
Ruslan Mitkov. 2009. Towards simplification: A
supervised learning approach. In Proceedings of
Machine Translation 25 Years On, London, United
Kingdom, November 21-22.

Iustina Ilisei, Diana Inkpen, Gloria Corpas Pastor, and
Ruslan Mitkov, 2010. Identification of transla-
tionese: A machine learning approach, pages 503–
511. Springer.

Zahurul Islam and Alexander Mehler. 2012. Cus-
tomization of the europarl corpus for translation
studies. In Proceedings of the 8th International
Conference on Language Resources and Evaluation
(LREC).

Philipp Koehn. 2005. Europarl: A parallel corpus for
statistical machine translation. In MT Summit.

Moshe Koppel and Noam Ordan. 2011. Translationese
and its dialects. In 49th Annual Meeting of the As-
sociation for Computational Linguistics (ACL).

Leonid Kulikov, Andrej Malchukov, and Peter
de Swart, editors. 2006. Case, Valency and Transi-
tivity, volume 77 of Studies in Language Companion
Series.

Jaroslaw Kwapien, Stanislaw Drozdz, and Adam Or-
czyk. 2010. Linguistic complexity: English vs. pol-
ish, text vs. corpus. CoRR, abs/1007.0936.

Sara Laviosa. 2002. Corpus-based translation stud-
ies. Theory, findings, applications. Amsterdam/New
York: Rodopi.

Gennadi Lembersky, Noam Ordan, and Shuly Wintner.
2011. Language models for machine translation:
Original vs. translated texts. In Empirical Methods
in Natural Language Processing (EMNLP).

Huma Lodhi, Craig Saunders, John Shawe-Taylor,
Nello Cristianini, and Chris Watkins. 2002. Text
classification using string kernels. Journal of Ma-
chine Learning Research, 2:419–444.

Deryle Lonsdale and Yvon Le Bras. 2009. A Fre-
quency Dictionary of French: Core Vocabulary for
Learners. Routledge.

Sattar Lzwaini. 2003. Building specialised corpora
for translation studies. In Workshop on Multilin-
gual Corpora: Linguistic Requirements and Tech-
nical Perspectives, Corpus Linguistics.

Andrew Mccallum and Kamal Nigam. 1998. A com-
parison of event models for naive bayes text classifi-
cation. In AAAI-98 Workshop on ’Learning for Text
Categorization’.

Maeve Olohan. 2001. Spelling out the optionals in
translation:a corpus study. In Corpus Linguistics
2001 conference. UCREL Technical Paper number
13. Special issue.

Gloria Corpas Pastor, Ruslan Mitkov, Naveed Afzal,
and Viktor Pekar. 2008. Translation universals:
do they exist? a corpus-based NLP study of con-
vergence and simplification. In Proceedings of the
Eighth Conference of the Association for Machine
Translation in the Americas (AMTA-08).

Jams W. Pennebaker, Martha E. Francis, and Roger J.
Booth. 2001. Linguistic Inquiry and Word Count
(LIWC): LIWC2001 Manual. Erlbaum Publishers.

Marius Popescu. 2011. Studying translationese at the
character level. In Recent Advances in Natural Lan-
guage Processing.

Anthony Pym. 2005. Explaining explicitation. In New
Trends in Translation Studies. In Honour of Kinga
Klaudy, pages 29–34. Akadmia Kiad.

Gideon Toury. 1995. Descriptive Translation
Studies and Beyond. John Benjamins, Amster-
dam/Philadelphia.

Hans van Halteren. 2008. Source language markers in
europarl translations. In International Conference
inComputational Linguistics(COLING), pages 937–
944.

1305


