



















































Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics


Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, pages 1415–1425
Vancouver, Canada, July 30 - August 4, 2017. c©2017 Association for Computational Linguistics

https://doi.org/10.18653/v1/P17-1130

Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, pages 1415–1425
Vancouver, Canada, July 30 - August 4, 2017. c©2017 Association for Computational Linguistics

https://doi.org/10.18653/v1/P17-1130

Cross-lingual Distillation for Text Classification

Ruochen Xu
Carnegie Mellon Universit
ruochenx@cs.cmu.edu

Yiming Yang
Carnegie Mellon Universit
yiming@cs.cmu.edu

Abstract

Cross-lingual text classification(CLTC) is
the task of classifying documents written
in different languages into the same tax-
onomy of categories. This paper presents
a novel approach to CLTC that builds on
model distillation, which adapts and ex-
tends a framework originally proposed for
model compression. Using soft proba-
bilistic predictions for the documents in a
label-rich language as the (induced) super-
visory labels in a parallel corpus of docu-
ments, we train classifiers successfully for
new languages in which labeled training
data are not available. An adversarial fea-
ture adaptation technique is also applied
during the model training to reduce dis-
tribution mismatch. We conducted experi-
ments on two benchmark CLTC datasets,
treating English as the source language
and German, French, Japan and Chinese
as the unlabeled target languages. The
proposed approach had the advantageous
or comparable performance of the other
state-of-art methods.

1 Introduction

The availability of massive multilingual data on
the Internet makes cross-lingual text classification
(CLTC) increasingly important. The task is de-
fined as to classify documents in different lan-
guages using the same taxonomy of predefined
categories.

CLTC systems build on supervised machine
learning require a sufficiently amount of labeled
training data for every domain of interest in each
language. But in reality, labeled data are not
evenly distributed among languages and across
domains. English, for example, is a label-rich lan-

guage in the domains of news stories, Wikipedia
pages and reviews of hotels, products, etc. But
many other languages do not necessarily have such
rich amounts of labeled data. This leads to an
open challenge in CLTC, i.e., how can we effec-
tively leverage the trained classifiers in a label-rich
source language to help the classification of docu-
ments in other label-poor target languages?

Existing methods in CLTC use either a bilingual
dictionary or a parallel corpus to bridge language
barriers and to translate classification models (Xu
et al., 2016) or text data(Zhou et al., 2016a).
There are limitations and challenges in using ei-
ther type of resources. Dictionary-based meth-
ods often ignore the dependency of word mean-
ing and its context, and cannot leverage domain-
specific disambiguation when the dictionary on
hand is a general-purpose one. Parallel-corpus
based methods, although more effective in de-
ploying context (when combined with word em-
bedding in particular), often have an issue of do-
main mismatch or distribution mismatch if the
available source-language training data, the paral-
lel corpus (human-aligned or machine-translation
induced one) and the target documents of in-
terest are not in exactly the same domain and
genre(Duh et al., 2011). How to solve such do-
main/distribution mismatch problems is an open
question for research.

This paper proposes a new parallel-corpus
based approach, focusing on the reduction of do-
main/distribution matches in CLTC. We call this
approach Cross-lingual Distillation with Feature
Adaptation or CLDFA in short. It is inspired
by the recent work in model compression (Hin-
ton et al., 2015) where a large ensemble model
is transformed to a compact (small) model. The
assumption of knowledge distillation for model
compression is that the knowledge learned by the
large model can be viewed as a mapping from in-

1415

https://doi.org/10.18653/v1/P17-1130
https://doi.org/10.18653/v1/P17-1130


put space to output (label) space. Then, by train-
ing with the soft labels predicted by the large
model, the small model can capture most of the
knowledge from the large model. Extending this
key idea to CLTC, if we see parallel documents
as different instantiations of the same semantic
concepts in different languages, a target-language
classifier should gain the knowledge from a well-
trained source classifier by training with the target-
language part of the parallel corpus and the soft
labels made by the source classifier on the source
language side. More specifically, we propose to
distillate knowledge from the source language to
the target language in the following 2-step process:

• Firstly, we train a source-language classi-
fier with both labeled training documents and
adapt it to the unlabeled documents from the
source-language side of the parallel corpus.
The adaptation enforces our classifier to ex-
tract features that are: 1) discriminative for
the classification task and 2) invariant with
regard to the distribution shift between train-
ing and parallel data.

• Secondly, we use the trained source-language
classifier to obtain the soft labels for a par-
allel corpus, and the target-language part of
the parallel corpus to train a target classifier,
which yields a similar category distribution
over target-language documents as that over
source-language documents. We also use un-
labeled testing documents in the target lan-
guage to adapt the feature extractor in this
training step.

Intuitively, the first step addresses the potential
domain/distribution mismatch between the labeled
data and the unlabeled data in the source language.
The second step addresses the potential mismatch
between the target-domain training data (in the
parallel corpus) and the test data (not in the par-
allel corpus). The soft-label based training of tar-
get classifiers makes our approach unique among
parallel-corpus based CLTC methods (Section 2.1.
The feature adaptation step makes our framework
particularly robust in addressing the distributional
difference between in-domain documents and par-
allel corpus, which is important for the success of
CLTC with low-resource languages.

The main contributions in this paper are the fol-
lowing:

• We propose a novel framework (CLDFA) for
knowledge distillation in CLTC through a
parallel corpus. It has the flexibility to be
built on a large family of existing monolin-
gual text classification methods and enables
the use of a large amount of unlabeled data
from both source and target language.

• CLDFA has the same computational com-
plexity as the plug-in text classification
method and hence is very efficient and scal-
able with the proper choice of plug-in text
classifier.

• Our evaluation on benchmark datasets shows
that our method had a better or at least com-
parable performance than that of other state-
of-art CLTC methods.

2 Related Work

Related work can be outlined with respect to
the representative work in CLTC and the recent
progress in deep learning for knowledge distilla-
tion.

2.1 CLTC Methods

One branch of CLTC methods is to use lexical
level mappings to transfer the knowledge from the
source language to the target language. The work
by Bel et al. (Bel et al., 2003) was the first ef-
fort to solve CLTC problem. They translated the
target-language documents to source language us-
ing a bilingual dictionary. The classifier trained
in the source language was then applied on those
translated documents. Similarly, Mihalcea et al.
(Mihalcea et al., 2007) built cross-lingual classi-
fier by translating subjectivity words and phrases
in the source language into the target language.
Shi et al. (Shi et al., 2010) also utilized a bilingual
dictionary. Instead of translating the documents,
they tried to translate the classification model from
source language to target language. Prettenhofer
and Stein. (Prettenhofer and Stein, 2010) also used
the bilingual dictionary as a word translation ora-
cle and built their CLTC system on structural cor-
respondence learning, a theory for domain adap-
tation. A more recent work by (Xu et al., 2016)
extended seminal bilingual dictionaries with unla-
beled corpora in low-resource languages. Chen et
al. (Chen et al., 2016) used bilingual word em-
bedding to map documents in source and target

1416



language into the same semantic space, and adver-
sarial training was applied to enforce the trained
classifier to be language-invariant.

Some recent efforts in CLTC focus on the
use of automatic machine translation (MT) tech-
nology. For example, Wan (Wan, 2009) used
machine translation systems to give each doc-
ument a source-language and a target-language
version, where one version is machine-translated
from the another one. A co-training (Blum and
Mitchell, 1998) algorithm was applied on two ver-
sions of both source and target documents to it-
erative train classifiers in both languages. MT-
based CLTC also include the work on multi-view
learning with different algorithms, such as ma-
jority voting(Amini et al., 2009), matrix com-
pletion(Xiao and Guo, 2013) and multi-view co-
regularization(Guo and Xiao, 2012a).

Another branch of CLTC methods focuses on
representation learning or the mapping of the in-
duced representations in cross-language settings
(Guo and Xiao, 2012b; Zhou et al., 2016a, 2015,
2016b; Xiao and Guo, 2013; Jagarlamudi et al.,
2011; De Smet et al., 2011; Vinokourov et al.,
2002; Platt et al., 2010; Littman et al., 1998). For
example, Meng et al. (Meng et al., 2012) and Lu et
al. (Lu et al., 2011) used a parallel corpus to learn
word alignment probabilities in a pre-processing
step. Some other work attempts to find a language-
invariant (or interlingua) representation for words
or documents in different languages using vari-
ous techniques, such as latent semantic indexing
(Littman et al., 1998), kernel canonical correlation
analysis (Vinokourov et al., 2002), matrix comple-
tion(Xiao and Guo, 2013), principal component
analysis (Platt et al., 2010) and Bayesian graphi-
cal models (De Smet et al., 2011).

2.2 Knowledge Distillation

The idea of distilling knowledge in a neural net-
work was proposed by Hinton et al (Hinton et al.,
2015), in which they introduced a student-teacher
paradigm. Once the cumbersome teacher network
was trained, the student network was trained ac-
cording to soften predictions of the teacher net-
work. In the field of computer vision, it has been
empirically verified that student network trained
by distillation performs better than the one trained
with hard labels. (Hinton et al., 2015; Romero
et al., 2014; Ba and Caruana, 2014). Gupta et
al.(Gupta et al., 2015) transfers supervision be-

tween images from different modalities(e.g. from
RGB image to depth image). There are also some
recent works applied distillation in the field of nat-
ural language. For example, Lili et al. (Mou
et al., 2015) distilled task specific knowledge from
a set of high-dimensional embeddings to a low-
dimensional space. Zhiting et al. used an iterative
distillation method to transfer the structured infor-
mation of logic rules into the weights of a neural
network. Kim et al. (Kim and Rush, 2016) applied
knowledge distillation approaches in the field of
machine translation to reduce the size of neural
machine translation model. Our framework shares
the same purpose of existing works that trans-
fer knowledge between models of different prop-
erties, such as model complexity, modality, and
structured logic. However, our transfer happens
between models working on different languages.
To the best of knowledge, this is the first work us-
ing knowledge distillation to bridge the language
gap for NLP tasks.

3 Preliminary

3.1 Task and Notation

CLTC aims to use the training data in the source
language to build a model applicable in the target
language. In our setting, we have labeled data in
source language Lsrc = {xi, yi}Li=1, where xi is
the labeled document in source language and yi
is the label vector. We then have our test data
in the target language, given by Ttgt = {x′i}Ti=1.
Our framework can also use unlabeled documents
from both languages in transductive learning set-
tings. We use Usrc = {xi}Mi=1 to denote source-
language unlabeled documents,Utgt = {x′i}Ni=1 to
denote target-language unlabeled documents, and
Uparl = {(xi, x′i)}Pi=1 to denote a unlabeled bilin-
gual parallel corpus where xi and x′i are paired
document translations of each other. We assume
that the unlabeled parallel corpus does not overlap
with the source-language training documents and
the target-language test documents.

3.2 Convolutional Neural Network (CNN) as
a Plug-in Classifier

We use a state-of-the-art CNN-based neural net-
work classifier (Kim, 2014) as the plug-in classi-
fier in our framework. Instead of using a bag-of-
words representation for each document, the CNN
model concatenates the word embeddings (verti-
cal vectors) of each input document into a n × k

1417



matrix, where n is the length (number of word oc-
currences) of the document, and k is the dimension
of word embedding. Denoting by

x1:n = x1 ⊕ x2 ⊕ ...⊕ xn

as the resulted matrix, with ⊕ the concatena-
tion operator. One-dimensional convolutional fil-
ter w ∈ Rhk with window size h operates on ev-
ery consecutive h words, with non-linear function
f and bias b. For window of size h started at index
i, the feature after convolutional filter is given by:

ci = f(w · xi:i+h−1 + b)

A max-over-time pooling (Collobert et al., 2011)
is applied on c over all possible positions such that
each filter extracts one feature. The model uses
multiple filters with different window sizes. The
concatenated outputs from filters consist the fea-
ture of each document. We can see the convolu-
tional filters and pooling layers as feature extractor
f = Gf (x, θf ), where θf contains parameters for
embedding layer and convolutional layer. Theses
features are then passed to a fully connected soft-
max layer to produce probability distributions over
labels. We see the final fully connected softmax
layer as a label classifier Gy(f , θy) that takes the
output f from the feature extractor. The final out-
put of model is given byGy(Gf (x, θf ), θy), which
is jointly parameterized by {θf , θy}

We want to emphasize that our choice of the
plug-in classifier here is mainly for its simplic-
ity and scalability to demonstrate our framework.
There is a large family of neural classifiers for
monolingual text classification that could be used
in our framework as well, including other convo-
lutional neural networks by (Johnson and Zhang,
2014), the recurrent neural networks by (Lai et al.,
2015; Zhang et al., 2016; Johnson and Zhang,
2016; Sutskever et al., 2014; Dai and Le, 2015),
the attention mechanism by (Yang et al., 2016),
the deep dense network by (Iyyer et al., 2015), and
more.

4 Proposed Framework

Let us introduce two versions of our model for
cross-language knowledge distillation, i.e., the
vanilla version and the full version with feature
adaptation. Both are supported by the proposed
framework. We denote the former by CLD-KCNN
and the latter by CLDFA-KCNN.

4.1 Vanilla Distillation

Without loss of generality, assume we are learning
a multi-class classifier for the target language. We
have y ∈ 1, 2, ..., |v |where v is the set of all possi-
ble classes. We assume the base classification net-
work produces real number logits qj for each class.
For example, for the case of CNN text classifier,
the logits can be produced by a linear transforma-
tion which takes features extracted max-pooling
layer and outputs a vector of size |v |. The logits
are converted into probabilities of classes through
the softmax layer, by normalizing each qj with all
other logits.

pj =
exp(qj/T )∑|v |
k=1 exp(qk/T )

(1)

where T is a temperature and is normally set to
1. Using a higher value of T produces a softer
probability distribution over classes.

The first step of our framework is to train the
source-language classifier on labeled source docu-
ments Lsrc. We use standard temperature T = 1
and cross-entropy loss as the objective to mini-
mize. For each example and its label (xi, yi) from
the source training set, we have:

L(θsrc) =

−
∑

(xi,yi)∈Lsrc

|v |∑

k=1

1{yi = k} log p(y = k|xi; θsrc)

(2)

where p(y = k|x; θsrc) is source model con-
trolled by parameter θsrc and 1{·} is the indicator
function.

In the second step, the knowledge captured in
θsrc is transferred to the distilled model in the
target language by training it on the parallel cor-
pus. The intuition is that paired documents in
parallel corpus should have the same distribution
of class predicted by the source model and tar-
get model. In the simplest version of our frame-
work, for each source-language document in the
parallel corpus, we predict a soft class distribution
by source model with high temperature. Then we
minimize the cross-entropy between soft distribu-
tion produced by source model and the soft dis-
tribution produced by target model on the paired
documents in the target language. More formally,
we optimize θtgt according to the following loss

1418



function for each document pair (xi, x′i) in paral-
lel corpus.

L(θtgt) = −
∑

(xi,x′i)∈Uparl
|v |∑

k=1

p(y = k|xi; θsrc) log p(y = k|x′i; θtgt)
(3)

During distillation, the same high temperature is
used for training target model. After it has been
trained, we set the temperature to 1 for testing.

We can show that under some assumptions, the
two-step cross-lingual distillation is equivalent to
distilling a target-language classifier in the target-
language input space.

Lemma 1. Assume the parallel corpus {xi, x′i} ∈
Uparl is generated by x′i ∼ p(X ′; η) and xi =
t(x′i), where η controls the marginal distribution
of xi and t is a differentiable translation func-
tion with integrable derivative. Let fθsrc(t(x

′))
be the function that outputs soft labels of p(y =
k|t(x′); θsrc). The distillation given by equation 3
can be interpreted as distillation of a target lan-
guage classifier fθsrc(t(x

′)) on target language
documents sampled from p(X ′; η).

fθsrc(t(x
′)) is the classifier that takes input of

target documents, translates them into source doc-
uments through t and makes prediction using the
source classifier. If we further assume the test-
ing documents have the same marginal distribu-
tion P (X ′; η), then the distilled classifier should
have similar generalization power as fθsrc(t(x

′)).

Theorem 2. Let source training data xi ∈
Lsrc has marginal distribution p(X;λ). Un-
der the assumptions of lemma 1, further as-
sume p(t(x′);λ) = p(x′; η), p(y|t(x′)) =
p(y|x′) and t′(x′) ≈ C, where C is a
constant. Then fθsrc(t(x

′)) actually mini-
mizes the expected loss in target language data
Ex′∼p(X;η),y∼p(Y |x′)[L

(
y, f(t(x′))

)
].

Proof. By definition of equation 2,
fθsrc(x) minimizes the expected loss
Ex∼p(X;λ),y∼p(Y |x)[L

(
y, f(x)

)
], where L is

cross-entropy loss in our case. Then we can write

Ex∼p(X;λ),y∼p(Y |x)[L
(
y, f(x)

)
]

=

∫
p(x;λ)

∑

y

p(y|x)L
(
y, f(x)

)
dx

=

∫
p(t(x′);λ)

∑

y

p(y|t(x′))L
(
y, f(t(x′))

)
t′(x′)dx′

≈C
∫
p(x′; η)

∑

y

p(y|x′)L
(
y, f(t(x′))

)
dx′

=CEx′∼p(X;η),y∼p(Y |x′)[L
(
y, f(t(x′))

)
]

4.2 Distillation with Adversarial Feature
Adaptation

15 10 5 0 5 10 15 20
25

20

15

10

5

0

5

10

15

Figure 1: Extracted features for source-language
documents in the English-Chinese Yelp Hotel Re-
view dataset. Red dots represent features of the
documents in Lsrc and green dots represent the
features of documents in Uparl, which is a general-
purpose parallel corpus.

Although vanilla distillation is intuitive and
simple, it cannot handle distribution mismatch is-
sues. For example, the marginal feature distribu-
tions of source-language documents in Lsrc and
Uparl could be different, so are the distributions of
target-language documents in Uparl and Ttgt. Ac-
cording to theorem 2, the vanilla distillation works
for the best performance under unrealistic assump-
tion: p(t(x′)|λ) = p(x′|η). To further illustrate
our point, we trained a CNN classifier according
to equation 2 and used the features extracted by
Gf to present the source-language documents in
both Lsrc and Uparl. Then we projected the high-
dimensional features onto a 2-dimensional space
via t-Distributed Stochastic Neighbor Embedding
(t-SNE)(Maaten and Hinton, 2008). This resulted

1419



the visualization of the project data in Figures 1
and 2.

It is quite obvious in Figure 1 that the general-
purpose parallel corpus has a very different fea-
ture distribution from that of the labeled source
training set. Even for machine-translated paral-
lel data from the same domain, as shown in fig-
ure 2, there is still a non-negligible distribution
shift from the source language to the target lan-
guage for the extracted features. Our interpreta-
tion of this observation is that when the MT sys-
tem (e.g. Google Translate) is a general-purpose
one, it non-avoidably add translation ambiguities
which would lead the distribution shift from the
original domain. To address the distribution di-
vergence brought by either a general-purpose par-
allel corpus or an imperfect MT system, we seek
to adapt the features extraction part of our neu-
ral classifier such that the feature distributions on
both sides should be close as possible in the newly
induced feature space. We adapt the adversarial
training method by (Ganin and Lempitsky, 2014)
to the cross-lingual settings in our problems.

Given a set of training set of L =
{xi, yi}i=1,...,N and an unlabeled set U =
{x′i}i=1,...,M , our goal is to find a neural classifier
Gy(Gf (x, θf ), θy), which has good discriminative
performance on L and also extracts features which
have similar distributions on L and U . One way
to maximize the similarity of two distributions is
to maximize the loss of a discriminative classifier
whose job is to discriminate the two feature dis-
tributions. We denote this classifier by Gd(·, θd),
which is parameterized by θd.

At training time, we seek θf to minimize the
loss of Gy and maximize the loss of Gd. Mean-
while, θy and θd are also optimized to minimize
their corresponding loss. The overall optimization
could be summarized as follows:

E(θf , θy, θd) =
∑

xi,yi∈L
Ly(yi, Gy(Gf (xi, θf ), θy))

− α
∑

xi∈L
Ld(0, Gd(Gf (xi, θf ), θd))

− α
∑

xj∈U
Ld(1, Gd(Gf (xj , θf ), θd))

where Ly is the loss function for true labels
y, Ld is loss function for binary labels indicat-
ing the source of data and α is the hyperparam-
eter that controls the relative importance of two

losses. We optimize θf , θy for minimizing E
and optimize θd for maximizing E. We jointly
optimize θf , θy, θd through the gradient reversal
layer(Ganin and Lempitsky, 2014).

We use this feature adaptation technique to
firstly adapt the source-language classifier to the
source-language documents of the parallel cor-
pus. When training the target-language classifier
by matching soft labels on the parallel corpus, we
also adapt the classifier to the target testing docu-
ments. We use cross-entropy loss functions as Ly
and Ld for both feature adaptation.

5 Experiments and Discussions

5.1 Dataset

Our experiments used two benchmark datasets, as
described below.

(1) Amazon Reviews

Language Domain # of Documents

English
book 50000
DVD 30000
music 25220

German
book 165470
DVD 91516
music 60392

French
book 32870
DVD 9358
music 15940

Japanese
book 169780
DVD 68326
music 55892

Table 1: Dataset Statistics for the Amazon reviews
dataset

We used the multilingual multi-domain Ama-
zon review dataset created by Prettenhofer and
Stein (Prettenhofer and Stein, 2010). The dataset
contains Amazon reviews in three domains: book,
DVD and music. Each domain has the reviews in
four different languages: English, German, French
and Japanese. We treated English as the source
language and the rest three as the target languages,
respectively. This gives us 9 tasks (the product
of the 3 domains and the 3 target languages) in
total. For each task, there are 1000 positive and
1000 negative reviews in English and the target
language, respectively. (Prettenhofer and Stein,
2010) also provides 2000 parallel reviews per task,

1420



15 10 5 0 5 10 15 20
20

15

10

5

0

5

10

15

(a) Germany:DVD

15 10 5 0 5 10 15
20

15

10

5

0

5

10

15

20

(b) French:Music

15 10 5 0 5 10 15
15

10

5

0

5

10

15

20

(c) Japanese:Book

Figure 2: Extracted features for the source-language documents in the Amazon Reviews dataset. Red
dots represent the features of the labeled training documents inLsrc, and green dots represent the features
of the documents in Uparl, which are the machine-translated documents from a target language. Below
each figure is the target language and the domain of review (Section 5.1).

that were generated using Google Translate 1, and
used by us for cross-language distillation. There
are also several thousands of unlabeled reviews in
each language. The statistics of unlabeled data is
summarized in Table 1. All the reviews are tok-
enized using standard regular expressions except
for Japanese, for which we used a publicly avail-
able segmenter 2.

(2) English-Chinese Yelp Hotel Reviews
This dataset was firstly used for CLTC by (Chen
et al., 2016). The task is to make sentence-level
sentiment classification with 5 labels(rating scale
from 1 to 5), using English as the source language
and Chinese as the target language. The labeled
English data consists of balanced labels of 650k
Yelp reviews from Zhang et al. (Zhang et al.,
2015). The Chinese data includes 20k labeled
Chinese hotel reviews and 1037k unlabeled ones
from (Lin et al., 2015). Following the approach
by (Chen et al., 2016), we use 10k of labeled Chi-
nese data as validation set and another 10k hotel
reviews as held-out test data. We a random sample
of 500k parallel sentences from UM-courpus(Tian
et al., 2014), which is a general-purpose corpus
designed for machine translation.

5.2 Baselines

We compare the proposed method with other state-
of-the-art methods as outlined below.

(1) Parallel-Corpus based CLTC Methods
Methods in this category all use an unlabeled par-
allel corpus. Methods named PL-LSI (Littman

1translate.google.com
2https://pypi.python.org/pypi/tinysegmenter

et al., 1998), PL-OPCA (Platt et al., 2010) and
PL-KCAA (Vinokourov et al., 2002) learn la-
tent document representations in a shared low-
dimensional space by performing the Latent
Semantic Indexing (LSI), the Oriented Princi-
pal Component Analysis (OPCA) and a kernel
(namely KCAA) for the parallel text. PL-MC
(Xiao and Guo, 2013) recovers missing features
via matrix Completion, and also uses LSI to in-
duce a latent space for parallel text. All these
methods train a classifier in the shared feature
space with labeled training data from both the
source and target languages.

(2) MT-based CLTC Methods
The methods in this category all use an MT sys-
tem to translate each test document in the tar-
get language to the source language in the test-
ing phase. The prediction on each translated
document is made by a source-language classi-
fier, which can be a Logistic Regression model
(MT+LR) (Chen et al., 2016) or a deep averaging
network (MT+DAN) (Chen et al., 2016).

(3) Adversarial Deep Averaging Network
Similar to our approach, the adversarial Deep Av-
eraging Network (ADAN) also exploits adversar-
ial training for CLTC (Chen et al., 2016). How-
ever, it does not have the parallel-corpus based
knowledge distillation part (which we do). In-
stead, it uses averaged bilingual embeddings of
words as its input and adapts the feature extractor
to produce similar features in both languages.

We also include the results of mSDA for the
Yelp Hotel Reviews dataset. mSDA (Chen et al.,
2012) is a domain adaptation method based on

1421



Target Language Domain PL-LSI PL-KCCA PL-OPCA PL-MC CLD-KCNN CLDFA-KCNN

German
book 77.59 79.14 74.72 79.22 82.54 83.95*
DVD 79.22 76.73 74.59 81.34 82.24 83.14*
music 73.81 79.18 74.45 79.39 74.65 79.02

French
book 79.56 77.56 76.55 81.92 81.6 83.37
DVD 77.82 78.19 70.54 81.97 82.41 82.56
music 75.39 78.24 73.69 79.3 83.01 83.31*

Janpanese
book 72.68 69.46 71.41 72.57 74.12 77.36*
DVD 72.55 74.79 71.84 76.6 79.67 80.52*
music 73.44 73.54 74.96 76.21 73.69 76.46

Averaged Accuracy 75.78 76.31 73.64 78.72 79.33 81.08*

Table 2: Accuracy scores of methods on the Amazon Reviews dataset: the best score in each row (a
task) is highlighted in bold face. If the score of CLDFA-KCNN is statistically significantly better (in
one-sample proportion tests) than the best among the baseline methods, it is marked using a star.

Model Accuracy
mSDA 31.44%
MT-LR 34.01%
MT-DAN 39.66%
ADAN 41.04%

CLD-KCNN 40.96%
CLDFA-KCNN 41.82%

Table 3: Accuracy scores of methods on the
English-Chinese Yelp Hotel Reviews dataset

stacked denoising autoencoders, which has been
proved to be effective in cross-domain sentiment
classification evaluations. We show the results re-
ported by (Chen et al., 2012), where they used
bilingual word embedding as input for mSDA.

5.3 Implementation Detail

We pre-trained both the source and target classi-
fier with unlabeled data in each language. We ran
word2vec(Mikolov et al., 2013) 3 on the tokenized
unlabeled corpus. The learned word embeddings
are used to initialize the word embedding look-up
matrix, which maps input words to word embed-
dings and concatenates them into input matrix.

We fine-tuned the source-language classifier
on the English training data with 5-fold cross-
validation. For English-Chinese Yelp-hotel review
dataset, the temperature T (Section 4.1) in distil-
lation is tuned on validation set in the target lan-
guage. For Amazon review dataset, since there is
no default validation set, we set temperature from
low to high in {1, 3, 5, 10} and take the average
among all predictions.

3https://code.google.com/archive/p/word2vec/

5.4 Main Results

In tables 2 and 3 we compare the results of our
methods (the vanilla version CLD-KCNN and the
full version CLDFA-KCNN) with those of other
methods based on the published results in the lit-
erature. The baseline methods are different in
these two tables as they were previously evaluated
(by their authors) on different benchmark datasets.
Clearly, CLDFA-KCNN outperformed the other
methods on all except one task in these two
datasets, showing that knowledge distillation is
successfully carried out in our approach. Noticing
that CLDFA-KCNN outperformed CLD-KCNN,
showing the effectiveness of adversarial feature
extraction in reducing the distribution mismatch
between the parallel corpus and the train/test data
in the target domain. We should also point out
that in Table 2, the four baseline methods (PL-LSI,
PL-KCCA, PL-OPCA and PL-MC) were evalu-
ated under the condition of using additional 100
labeled target documents for training, according
to the author’s report (Xiao and Guo, 2013). On
the other hand, our methods (CLD-KCNN and
CLDFA-KCNN) were evaluated under a tougher
condition, i.e., not using any labeled data in the
target domains.

We also test our framework when a few train-
ing documents in the target language are available.
A simple way to utilize the target-language super-
vision is to fit the target-language model with la-
beled target data after optimizing with our cross-
lingual distillation framework. The performance
of CLD-KCNN and CLDFA-KCNN trained with
different sizes of labeled target-language data is
shown in figure 3. We also compare the perfor-
mance of training the same classifier using only

1422



the target-language labels(Target Only in figure
3). As we can see, our framework can efficiently
utilize the extra supervision and improve the per-
formance over the training using only the target-
language labels. The margin is most significant
when the size of the target-language label is rela-
tively small.

0 100 200 300 400 500 600 700 800
Size of labeled target data

0.60

0.65

0.70

0.75

0.80

0.85

0.90

Ac
cu

ra
cy

Target Only
CLD-KCNN
CLDFA-KCNN

Figure 3: Accuracy scores of methods using vary-
ing sizes of target-language labeled data on the
Amazon review dataset. The target language is
German and the domain is music. The parallel cor-
pus has a fixed size of 1000 and the size of the la-
beled target-language documents is shown on the
x-axis

6 Conclusion

This work introduces a novel framework for dis-
tillation of discriminative knowledge across lan-
guages, providing effective and efficient algorith-
mic solutions for addressing domain/distribution
mismatch issues in CLTC. The excellent perfor-
mance of our approach is evident in our evalua-
tion on two CLTC benchmark datasets, compared
to that of other state-of-the-art methods.

Acknowledgement

We thank the reviewers for their helpful com-
ments. This work is supported in part by De-
fense Advanced Research Projects Agency Infor-
mation Innovation Oce (I2O), the Low Resource
Languages for Emergent Incidents (LORELEI)
Program, Issued by DARPA/I2O under Contract
No. HR0011-15-C-0114, by the National Science
Foundation (NSF) under grant IIS-1546329.

References
Massih Amini, Nicolas Usunier, and Cyril Goutte.

2009. Learning from multiple partially observed

views-an application to multilingual text categoriza-
tion. In Advances in neural information processing
systems. pages 28–36.

Jimmy Ba and Rich Caruana. 2014. Do deep nets really
need to be deep? In Advances in neural information
processing systems. pages 2654–2662.

Nuria Bel, Cornelis HA Koster, and Marta Ville-
gas. 2003. Cross-lingual text categorization. Re-
search and Advanced Technology for Digital Li-
braries pages 126–139.

Avrim Blum and Tom Mitchell. 1998. Combining la-
beled and unlabeled data with co-training. In Pro-
ceedings of the eleventh annual conference on Com-
putational learning theory. ACM, pages 92–100.

Minmin Chen, Zhixiang Xu, Kilian Weinberger, and
Fei Sha. 2012. Marginalized denoising autoen-
coders for domain adaptation. arXiv preprint
arXiv:1206.4683 .

Xilun Chen, Ben Athiwaratkun, Yu Sun, Kilian Wein-
berger, and Claire Cardie. 2016. Adversarial deep
averaging networks for cross-lingual sentiment clas-
sification. arXiv preprint arXiv:1606.01614 .

Ronan Collobert, Jason Weston, Léon Bottou, Michael
Karlen, Koray Kavukcuoglu, and Pavel Kuksa.
2011. Natural language processing (almost) from
scratch. Journal of Machine Learning Research
12(Aug):2493–2537.

Andrew M Dai and Quoc V Le. 2015. Semi-supervised
sequence learning. In Advances in Neural Informa-
tion Processing Systems. pages 3079–3087.

Wim De Smet, Jie Tang, and Marie-Francine Moens.
2011. Knowledge transfer across multilingual cor-
pora via latent topics. In Pacific-Asia Conference on
Knowledge Discovery and Data Mining. Springer,
pages 549–560.

Kevin Duh, Akinori Fujino, and Masaaki Nagata. 2011.
Is machine translation ripe for cross-lingual senti-
ment classification? In Proceedings of the 49th
Annual Meeting of the Association for Computa-
tional Linguistics: Human Language Technologies:
short papers-Volume 2. Association for Computa-
tional Linguistics, pages 429–433.

Yaroslav Ganin and Victor Lempitsky. 2014. Unsuper-
vised domain adaptation by backpropagation. arXiv
preprint arXiv:1409.7495 .

Yuhong Guo and Min Xiao. 2012a. Cross language
text classification via subspace co-regularized multi-
view learning. arXiv preprint arXiv:1206.6481 .

Yuhong Guo and Min Xiao. 2012b. Transductive rep-
resentation learning for cross-lingual text classifica-
tion. In Data Mining (ICDM), 2012 IEEE 12th In-
ternational Conference on. IEEE, pages 888–893.

1423



Saurabh Gupta, Judy Hoffman, and Jitendra Malik.
2015. Cross modal distillation for supervision trans-
fer. arXiv preprint arXiv:1507.00448 .

Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. 2015.
Distilling the knowledge in a neural network. arXiv
preprint arXiv:1503.02531 .

Mohit Iyyer, Varun Manjunatha, Jordan Boyd-Graber,
and Hal Daumé III. 2015. Deep unordered compo-
sition rivals syntactic methods for text classification.
In Proceedings of the Association for Computational
Linguistics.

Jagadeesh Jagarlamudi, Raghavendra Udupa, Hal
Daumé III, and Abhijit Bhole. 2011. Improving
bilingual projections via sparse covariance matri-
ces. In Proceedings of the Conference on Empirical
Methods in Natural Language Processing. Associa-
tion for Computational Linguistics, pages 930–940.

Rie Johnson and Tong Zhang. 2014. Effective
use of word order for text categorization with
convolutional neural networks. arXiv preprint
arXiv:1412.1058 .

Rie Johnson and Tong Zhang. 2016. Supervised and
semi-supervised text categorization using lstm for
region embeddings. In Proceedings of The 33rd In-
ternational Conference on Machine Learning. pages
526–534.

Yoon Kim. 2014. Convolutional neural net-
works for sentence classification. arXiv preprint
arXiv:1408.5882 .

Yoon Kim and Alexander M Rush. 2016. Sequence-
level knowledge distillation. arXiv preprint
arXiv:1606.07947 .

Siwei Lai, Liheng Xu, Kang Liu, and Jun Zhao. 2015.
Recurrent convolutional neural networks for text
classification. In AAAI. pages 2267–2273.

Yiou Lin, Hang Lei, Jia Wu, and Xiaoyu Li. 2015. An
empirical study on sentiment classification of chi-
nese review using word embedding. arXiv preprint
arXiv:1511.01665 .

Michael L Littman, Susan T Dumais, and Thomas K
Landauer. 1998. Automatic cross-language in-
formation retrieval using latent semantic indexing.
In Cross-language information retrieval, Springer,
pages 51–62.

Bin Lu, Chenhao Tan, Claire Cardie, and Benjamin K
Tsou. 2011. Joint bilingual sentiment classifica-
tion with unlabeled parallel corpora. In Proceed-
ings of the 49th Annual Meeting of the Association
for Computational Linguistics: Human Language
Technologies-Volume 1. Association for Computa-
tional Linguistics, pages 320–330.

Laurens van der Maaten and Geoffrey Hinton. 2008.
Visualizing data using t-sne. Journal of Machine
Learning Research 9(Nov):2579–2605.

Xinfan Meng, Furu Wei, Xiaohua Liu, Ming Zhou,
Ge Xu, and Houfeng Wang. 2012. Cross-lingual
mixture model for sentiment classification. In Pro-
ceedings of the 50th Annual Meeting of the Associ-
ation for Computational Linguistics: Long Papers-
Volume 1. Association for Computational Linguis-
tics, pages 572–581.

Rada Mihalcea, Carmen Banea, and Janyce M Wiebe.
2007. Learning multilingual subjective language via
cross-lingual projections .

Tomas Mikolov, Kai Chen, Greg Corrado, and Jef-
frey Dean. 2013. Efficient estimation of word
representations in vector space. arXiv preprint
arXiv:1301.3781 .

Lili Mou, Ge Li, Yan Xu, Lu Zhang, and Zhi Jin. 2015.
Distilling word embeddings: An encoding approach.
arXiv preprint arXiv:1506.04488 .

John C Platt, Kristina Toutanova, and Wen-tau Yih.
2010. Translingual document representations from
discriminative projections. In Proceedings of the
2010 Conference on Empirical Methods in Natu-
ral Language Processing. Association for Compu-
tational Linguistics, pages 251–261.

Peter Prettenhofer and Benno Stein. 2010. Cross-
language text classification using structural corre-
spondence learning. In Proceedings of the 48th An-
nual Meeting of the Association for Computational
Linguistics. Association for Computational Linguis-
tics, pages 1118–1127.

Adriana Romero, Nicolas Ballas, Samira Ebrahimi Ka-
hou, Antoine Chassang, Carlo Gatta, and Yoshua
Bengio. 2014. Fitnets: Hints for thin deep nets.
arXiv preprint arXiv:1412.6550 .

Lei Shi, Rada Mihalcea, and Mingjun Tian. 2010.
Cross language text classification by model trans-
lation and semi-supervised learning. In Proceed-
ings of the 2010 Conference on Empirical Meth-
ods in Natural Language Processing. Association
for Computational Linguistics, pages 1057–1067.

Ilya Sutskever, Oriol Vinyals, and Quoc V Le. 2014.
Sequence to sequence learning with neural net-
works. In Advances in neural information process-
ing systems. pages 3104–3112.

Liang Tian, Derek F Wong, Lidia S Chao, Paulo
Quaresma, Francisco Oliveira, and Lu Yi. 2014.
Um-corpus: A large english-chinese parallel corpus
for statistical machine translation. In LREC. pages
1837–1842.

Alexei Vinokourov, John Shawe-Taylor, and Nello
Cristianini. 2002. Inferring a semantic representa-
tion of text via cross-language correlation analysis.
In NIPS. volume 1, page 4.

Xiaojun Wan. 2009. Co-training for cross-lingual sen-
timent classification. In Proceedings of the Joint
Conference of the 47th Annual Meeting of the ACL

1424



and the 4th International Joint Conference on Natu-
ral Language Processing of the AFNLP: Volume 1-
Volume 1. Association for Computational Linguis-
tics, pages 235–243.

Min Xiao and Yuhong Guo. 2013. A novel two-step
method for cross language representation learning.
In Advances in Neural Information Processing Sys-
tems. pages 1259–1267.

Ruochen Xu, Yiming Yang, Hanxiao Liu, and An-
drew Hsi. 2016. Cross-lingual text classification via
model translation with limited dictionaries. In Pro-
ceedings of the 25th ACM International on Confer-
ence on Information and Knowledge Management.
ACM, pages 95–104.

Zichao Yang, Diyi Yang, Chris Dyer, Xiaodong He,
Alex Smola, and Eduard Hovy. 2016. Hierarchical
attention networks for document classification. Pro-
ceedings of the 2016 Conference of the North Amer-
ican Chapter of the Association for Computational
Linguistics: Human Language Technologies. .

Rui Zhang, Honglak Lee, and Dragomir Radev. 2016.
Dependency sensitive convolutional neural networks
for modeling sentences and documents. arXiv
preprint arXiv:1611.02361 .

Xiang Zhang, Junbo Zhao, and Yann LeCun. 2015.
Character-level convolutional networks for text clas-
sification. In Advances in neural information pro-
cessing systems. pages 649–657.

Huiwei Zhou, Long Chen, Fulin Shi, and Degen
Huang. 2015. Learning bilingual sentiment word
embeddings for cross-language sentiment classifica-
tion. ACL.

Xinjie Zhou, Xianjun Wan, and Jianguo Xiao. 2016a.
Cross-lingual sentiment classification with bilingual
document representation learning .

Xinjie Zhou, Xiaojun Wan, and Jianguo Xiao. 2016b.
Attention-based lstm network for cross-lingual sen-
timent classification .

1425


	Cross-lingual Distillation for Text Classification

