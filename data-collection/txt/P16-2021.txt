



















































Vocabulary Manipulation for Neural Machine Translation


Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 124–129,
Berlin, Germany, August 7-12, 2016. c©2016 Association for Computational Linguistics

Vocabulary Manipulation for Neural Machine Translation

Haitao Mi Zhiguo Wang Abe Ittycheriah
T.J. Watson Research Center

IBM
{hmi, zhigwang, abei}@us.ibm.com

Abstract

In order to capture rich language phenom-
ena, neural machine translation models
have to use a large vocabulary size, which
requires high computing time and large
memory usage. In this paper, we alleviate
this issue by introducing a sentence-level
or batch-level vocabulary, which is only a
very small sub-set of the full output vocab-
ulary. For each sentence or batch, we only
predict the target words in its sentence-
level or batch-level vocabulary. Thus,
we reduce both the computing time and
the memory usage. Our method simply
takes into account the translation options
of each word or phrase in the source sen-
tence, and picks a very small target vocab-
ulary for each sentence based on a word-
to-word translation model or a bilingual
phrase library learned from a traditional
machine translation model. Experimen-
tal results on the large-scale English-to-
French task show that our method achieves
better translation performance by 1 BLEU
point over the large vocabulary neural ma-
chine translation system of Jean et al.
(2015).

1 Introduction

Neural machine translation (NMT) (Bahdanau et
al., 2014) has gained popularity in recent two
years. But it can only handle a small vocabulary
size due to the computational complexity. In or-
der to capture rich language phenomena and have
a better word coverage, neural machine translation
models have to use a large vocabulary.

Jean et al. (2015) alleviated the large vocabu-
lary issue by proposing an approach that partitions
the training corpus and defines a subset of the full
target vocabulary for each partition. Thus, they
only use a subset vocabulary for each partition in
the training procedure without increasing compu-
tational complexity. However, there are still some

drawbacks of Jean et al. (2015)’s method. First,
the importance sampling is simply based on the
sequence of training sentences, which is not lin-
guistically motivated, thus, translation ambiguity
may not be captured in the training. Second, the
target vocabulary for each training batch is fixed
in the whole training procedure. Third, the target
vocabulary size for each batch during training still
needs to be as large as 30k, so the computing time
is still high.

In this paper, we alleviate the above issues by
introducing a sentence-level vocabulary, which is
very small compared with the full target vocab-
ulary. In order to capture the translation am-
biguity, we generate those sentence-level vocab-
ularies by utilizing word-to-word and phrase-to-
phrase translation models which are learned from
a traditional phrase-based machine translation sys-
tem (SMT). Another motivation of this work is to
combine the merits of both traditional SMT and
NMT, since training an NMT system usually takes
several weeks, while the word alignment and rule
extraction for SMT are much faster (can be done
in one day). Thus, for each training sentence,
we build a separate target vocabulary which is the
union of following three parts:
• target vocabularies of word and phrase trans-

lations that can be applied to the current sen-
tence. (to capture the translation ambiguity)
• top 2k most frequent target words. (to cover

the unaligned target words)
• target words in the reference of the current

sentence. (to make the reference reachable)
As we use mini-batch in the training procedure,
we merge the target vocabularies of all the sen-
tences in each batch, and update only those re-
lated parameters for each batch. In addition, we
also shuffle the training sentences at the begin-
ning of each epoch, so the target vocabulary for
a specific sentence varies in each epoch. In the
beam search for the development or test set, we
apply the similar procedure for each source sen-
tence, except the third bullet (as we do not have

124



�↵t1 ↵tl

st�1 st… ot

Ht =

lX
i=1

(↵ti · �h i)
lX

i=1

(↵ti ·�!h i)

x1 xl

 �
h1

 �
hl
�!
hl

�!
h1

…

…

…x2

�!
h2

 �
h2

↵t2

y⇤t�1

Vo

Figure 1: The attention-based NMT architecture.←−
hi and

−→
hi are bi-directional encoder states. αtj is

the attention prob at time t, position j. Ht is the
weighted sum of encoding states. st is the hidden
state. ot is an intermediate output state. A single
feedforward layer projects ot to a target vocabu-
lary Vo, and applies softmax to predict the proba-
bility distribution over the output vocabulary.

the reference) and mini-batch parts. Experimen-
tal results on large-scale English-to-French task
(Section 5) show that our method achieves signifi-
cant improvements over the large vocabulary neu-
ral machine translation system.

2 Neural Machine Translation

As shown in Figure 1, neural machine translation
(Bahdanau et al., 2014) is an encoder-decoder net-
work. The encoder employs a bi-directional recur-
rent neural network to encode the source sentence
x = (x1, ..., xl), where l is the sentence length,
into a sequence of hidden states h = (h1, ..., hl),
each hi is a concatenation of a left-to-right

−→
hi and

a right-to-left
←−
hi ,

hi =

[←−
h i−→
h i

]
=

[←−
f (xi,

←−
h i+1)−→

f (xi,
−→
h i−1)

]
,

where
←−
f and

−→
f are two gated recurrent units

(GRU).
Given h, the decoder predicts the target transla-

tion by maximizing the conditional log-probability
of the correct translation y∗ = (y∗1, ...y∗m), where
m is the length of target sentence. At each time t,
the probability of each word yt from a target vo-
cabulary Vy is:

p(yt|h, y∗t−1..y∗1) ∝ exp(g(st, y∗t−1, Ht)), (1)

where g is a multi layer feed-forward neural net-
work, which takes the embedding of the previous
word y∗t−1, the hidden state st, and the context
state Ht as input. The output layer of g is a tar-
get vocabulary Vo, yt ∈ Vo in the training pro-
cedure. Vo is originally defined as the full target
vocabulary Vy (Cho et al., 2014). We apply the
softmax function over the output layer, and get the
probability of p(yt|h, y∗t−1..y∗1). In Section 3, we
differentiate Vo from Vy by adding a separate and
sentence-dependent Vo for each source sentence.
In this way, we enable to maintain a large Vy, and
use a small Vo for each sentence.

The st is computed as:

st = q(st−1, y∗t−1, ct) (2)

ct =

[∑l
i=1 (αti ·

←−
h i)∑l

i=1 (αti ·
−→
h i)

]
, (3)

where q is a GRU, ct is a weighted sum of h, the
weights, α, are computed with a feed-forward neu-
ral network r:

αti =
exp{r(st−1, hi, y∗t−1)}∑l

k=1 exp{r(st−1, hk, y∗t−1)}
(4)

3 Target Vocabulary

The output of function g is the probability distri-
bution over the target vocabulary Vo. As Vo is de-
fined as Vy in Cho et al. (2014), the softmax func-
tion over Vo requires to compute all the scores for
all words in Vo, and results in a high computing
complexity. Thus, Bahdanau et al. (2014) only
uses top 30k most frequent words for both Vo and
Vy, and replaces all other words as unknown words
(UNK).

3.1 Target Vocabulary Manipulation
In this section, we aim to use a large vocabulary
of Vy (e.g. 500k, to have a better word cover-
age), and, at the same, to reduce the size of Vo
as small as possible (in order to reduce the com-
puting time). Our basic idea is to maintain a sep-
arate and small vocabulary Vo for each sentence
so that we only need to compute the probability
distribution of g over a small vocabulary for each
sentence. Thus, we introduce a sentence-level vo-
cabulary Vx to be our Vo, which depends on the
sentence x. In the following part, we show how
we generate the sentence-dependent Vx.

The first objective of our method aims to cap-
ture the real translation ambiguity for each word,

125



and the target vocabulary of a sentence Vo = Vx
is supposed to cover as many as those possible
translation candidates. Take the English to Chi-
nese translation for example, the target vocabulary
for the English word bank should contain yı́nháng
(a financial institution) and héàn (sloping land) in
Chinese.

So we first use a word-to-word translation dic-
tionary to generate some target vocaularies for x.
Given a dictionary D(x) = [y1, y2, ...], where x is
a source word, [y1, y2, ...] is a sorted list of candi-
date translations, we generate a target vocabulary
V Dx for a sentence x = (x1, ..., xl) by merging all
the candidates of all words x in x.

V Dx =
l⋃

i=1

D(xi)

As the word-to-word translation dictionary only
focuses on the source words, it can not cover
the target unaligned functional or content words,
where the traditional phrases are designed for this
purpose. Thus, in addition to the word dictio-
nary, given a word aligned training corpus, we
also extract phrases P (x1...xi) = [y1, ..., yj ],
where x1...xi is a consecutive source words, and
[y1, ..., yj ] is a list of target words1. For each sen-
tence x, we collect all the phrases that can be ap-
plied to sentence x, e.g. x1...xi is a sub-sequence
of sentence x.

V Px =
⋃

∀xi...xj∈subseq(x)
P (xi...xj),

where subseq(x) is all the possible sub-sequence
of x with a length limit.

In order to cover target un-aligned functional
words, we need top n most common target words.

V Tx = T (n).

Training: in our training procedure, our op-
timization objective is to maximize the log-
likelihood over the whole training set. In order
to make the reference reachable, besides V Dx , V

P
x

and V Tx , we also need to include the target words
in the reference y,

V Rx =
⋃
∀yi∈y

yi,

1Here we change the definition of a phrase in traditional
SMT, where the [y1, ...yj ] should also be a consecutive target
words. But our task in this paper is to get the target vocabu-
lary, so we only care about the target word set, not the order.

where x and y are a translation pair. So for each
sentence x, we have a target vocabulary Vx:

Vx = V Dx ∪ V Px ∪ V Tx ∪ V Rx
Then, we start our mini-batch training by ran-
domly shuffling the training sentences before each
epoch. For simplicity, we use the union of all Vx
in a batch,

Vo = Vb = Vx1 ∪ Vx2 ∪ ...Vxb ,

where b is the batch size. This merge gives an
advantage that Vb changes dynamically in each
epoch, which leads to a better coverage of param-
eters.

Decoding: different from the training, the target
vocabulary for a sentence x is

Vo = Vx = V Dx ∪ V Px ∪ V Tx ,

and we do not use mini-batch in decoding.

4 Related Work

To address the large vocabulary issue in NMT,
Jean et al. (2015) propose a method to use differ-
ent but small sub vocabularies for different parti-
tions of the training corpus. They first partition
the training set. Then, for each partition, they cre-
ate a sub vocabulary Vp, and only predict and ap-
ply softmax over the vocabularies in Vp in training
procedure. When the training moves to the next
partition, they change the sub vocabulary set ac-
cordingly.

Noise-contrastive estimation (Gutmann and Hy-
varinen, 2010; Mnih and Teh, 2012; Mikolov et
al., 2013; Mnih and Kavukcuoglu, 2013) and hi-
erarchical classes (Mnih and Hinton, 2009) are in-
troduced to stochastically approximate the target
word probability. But, as suggested by Jean et al.
(2015), those methods are only designed to reduce
the computational complexity in training, not for
decoding.

5 Experiments

5.1 Data Preparation
We run our experiments on English to French (En-
Fr) task. The training corpus consists of approx-
imately 12 million sentences, which is identical
to the set of Jean et al. (2015) and Sutskever et
al. (2014). Our development set is the concatena-
tion of news-test-2012 and news-test-2013, which

126



set V Px
V Dx V

P
x ∪ V Dx V Px ∪ V Dx ∪ V Tx

10 20 50 10 20 50 10 20 50
train 73.6 82.1 87.8 93.5 86.6 89.4 93.7 92.7 94.2 96.2

development 73.5 80.0 85.5 91.0 86.6 88.4 91.7 91.7 92.7 94.3

Table 1: The average reference coverage ratios (in word-level) on the training and development sets. We
use fixed top 10 candidates for each phrase when generating V Px , and top 2k most common words for
V Tx . Then we check various top n (10, 20, and 50) candidates for the word-to-word dictionary for V

D
x .

has 6003 sentences in total. Our test set has 3003
sentences from WMT news-test 2014. We evalu-
ate the translation quality using the case-sensitive
BLEU-4 metric (Papineni et al., 2002) with the
multi-bleu.perl script.

Same as Jean et al. (2015), our full vocabu-
lary size is 500k, we use AdaDelta (Zeiler, 2012),
and mini-batch size is 80. Given the training set,
we first run the ‘fast align’ (Dyer et al., 2013) in
one direction, and use the translation table as our
word-to-word dictionary. Then we run the reverse
direction and apply ‘grow-diag-final-and’ heuris-
tics to get the alignment. The phrase table is ex-
tracted with a standard algorithm in Moses (Koehn
et al., 2007).

In the decoding procedure, our method is very
similar to the ‘candidate list’ of Jean et al. (2015),
except that we also use bilingual phrases and we
only include top 2k most frequent target words.
Following Jean et al. (2015), we dump the align-
ments for each sentence, and replace UNKs with
the word-to-word dictionary or the source word.

5.2 Results

5.2.1 Reference Reachability

The reference coverage or reachability ratio is very
important when we limit the target vocabulary for
each source sentence, since we do not have the ref-
erence in the decoding time, and we do not want
to narrow the search space into a bad space. Ta-
ble 1 shows the average reference coverage ratios
(in word-level) on the training and development
sets. For each source sentence x, V ∗x here is a
set of target word indexes (the vocabulary size is
500k, others are mapped to UNK). The average
reference vocabulary size V Rx for each sentence is
23.7 on the training set (22.6 on the dev. set). The
word-to-word dictionary V Dx has a better cover-
age than phrases V Px , and when we combine the
three sets we can get better coverage ratios. Those
statistics suggest that we can not use each of them
alone due to the low reference coverage ratios.
The last three columns show three combinations,

system
train dev.

sentence mini-batch sentence
Jean (2015) 30k 30k 30k

Ours 2080 6153 2067

Table 2: Average vocabulary size for each sen-
tence or mini-batch (80 sentences). The full vo-
cabulary is 500k, all other words are UNKs.

all of which have higher than 90% coverage ratios.
As there are many combinations, training an NMT
system is time consuming, and we also want to
keep the output vocabulary size small (the setting
in the last column in Table 1 results in an average
11k vocabulary size for mini-batch 80), thus, in
the following part, we only run one combination
(top 10 candidates for both V Px and V

D
x , and top

2k for V Tx ), where the full sentence coverage ratio
is 20.7% on the development set.

5.2.2 Average Size of Vo
With the setting shown in bold column in Ta-
ble 1, we list average vocabulary size of Jean et al.
(2015) and ours in Table 2. Jean et al. (2015) fix
the vocabulary size to 30k for each sentence and
mini-batch, while our approach reduces the vocab-
ulary size to 2080 for each sentence, and 6153 for
each mini-batch. Especially in the decoding time,
our vocabulary size for each sentence is about 14.5
times smaller than 30k.

5.2.3 Translation Results
The red solid line in Figure 2 shows the learn-
ing curve of our method on the development set,
which picks at epoch 7 with a BLEU score of
30.72. We also fix word embeddings at epoch
5, and continue several more epochs. The corre-
sponding blue dashed line suggests that there is no
significant difference between them.

We also run two more experiments: V Dx ∪ V Tx
and V Px ∪V Tx separately (always have V Rx in train-
ing). The final results on the test set are 34.20
and 34.23 separately. Those results suggest that
we should use both the translation dictionary and
phrases in order to get better translation quality.

127



top n common words 50 200 500 1000 2000 10000
BLEU on dev. 30.61 30.65 30.70 30.70 30.72 30.69

avg. size of Vo = V Px ∪ V Dx ∪ V Tx 202 324 605 1089 2067 10029
Table 3: Given a trained NMT model, we decode the development set with various top n most common
target words. For En-Fr task, the results suggest that we can reduce the n to 50 without losing much in
terms of BLEU score. The average size of Vo is reduced to as small as 202, which is significant lower
than 2067 (the default setting we use in our training).

 27

 27.5

 28

 28.5

 29

 29.5

 30

 30.5

 31

 31.5

 1  2  3  4  5  6  7  8  9  10  11

7th

B
L

E
U

epoch

learning curve
fixed word-embeddings

Figure 2: The learning curve on the development
set. An epoch means a complete update through
the full training set.

single system dev. test
Moses from Cho et al. (2014) N/A 33.30

Jean (2015)
candidate list 29.32 33.36

+UNK replace 29.98 34.11

Ours
voc. manipulation 30.15 34.45

+UNK replace 30.72 35.11
best from Durrani et al. (2014) N/A 37.03

Table 4: Single system results on En-Fr task.

Table 4 shows the single system results on En-
Fr task. The standard Moses in Cho et al. (2014)
on the test set is 33.3. Our target vocabulary ma-
nipulation achieves a BLEU score of 34.45 on the
test set, and 35.11 after the UNK replacement. Our
approach improves the translation quality by 1.0
BLEU point on the test set over the method of
Jean et al. (2015). But our single system is still
about 2 points behind of the best phrase-based sys-
tem (Durrani et al., 2014).

5.2.4 Decoding with Different Top n Most
Common Target Words

Another interesting question is what is the perfor-
mance if we vary the size top n most common
target words in V Tx . As the training for NMT is
time consuming, we vary the size n only in the de-
coding time. Table 3 shows the BLEU scores on
the development set. When we reduce the n from
2000 to 50, we only loss 0.1 points, and the av-

erage size of sentence level Vo is reduced to 202,
which is significant smaller than 2067 (shown in
Table 2). But we should notice that we train our
NMT model in the condition of the bold column in
Table 2, and only test different n in our decoding
procedure only. Thus there is a mismatch between
the training and testing when n is not 2000.

5.2.5 Speed

In terms of speed, as we have different code bases2

between Jean et al. (2015) and us, it is hard to con-
duct an apple to apple comparison. So, for sim-
plicity, we run another experiment with our code
base, and increase Vb size to 30k for each batch
(the same size in Jean et al. (2015)). Results show
that increasing the Vb to 30k slows down the train-
ing speed by 1.5 times.

6 Conclusion

In this paper, we address the large vocabulary is-
sue in neural machine translation by proposing to
use a sentence-level target vocabulary Vo, which
is much smaller than the full target vocabulary Vy.
The small size of Vo reduces the computing time of
the softmax function in each predict step, while the
large vocabulary of Vy enable us to model rich lan-
guage phenomena. The sentence-level vocabulary
Vo is generated with the traditional word-to-word
and phrase-to-phrase translation libraries. In this
way, we decrease the size of output vocabulary Vo
under 3k for each sentence, and we speedup and
improve the large-vocabulary NMT system.

Acknowledgment

We thank the anonymous reviewers for their com-
ments.

2Two code bases share the same architecture, initial states,
and hyper-parameters. We simulate Jean et al. (2015)’s work
with our code base in the both training and test procedures,
the final results of our simulation are 29.99 and 34.16 on dev.
and test sets respectively. Those scores are very close to Jean
et al. (2015).

128



References
D. Bahdanau, K. Cho, and Y. Bengio. 2014. Neural

Machine Translation by Jointly Learning to Align
and Translate. ArXiv e-prints, September.

Kyunghyun Cho, Bart van Merrienboer, Caglar Gul-
cehre, Dzmitry Bahdanau, Fethi Bougares, Holger
Schwenk, and Yoshua Bengio. 2014. Learning
phrase representations using rnn encoder–decoder
for statistical machine translation. In Proceedings of
EMNLP, pages 1724–1734, Doha, Qatar, October.

Nadir Durrani, Barry Haddow, Philipp Koehn, and
Kenneth Heafield. 2014. Edinburghs phrase-based
machine translation systems for wmt-14. In Pro-
ceedings of WMT, pages 97–104, Baltimore, Mary-
land, USA, June.

Chris Dyer, Victor Chahuneau, and Noah A. Smith.
2013. A simple, fast, and effective reparameteriza-
tion of ibm model 2. In Proceedings of the 2013
Conference of the North American Chapter of the
Association for Computational Linguistics: Human
Language Technologies, pages 644–648, Atlanta,
Georgia, June. Association for Computational Lin-
guistics.

Michael Gutmann and Aapo Hyvarinen. 2010. Noise-
contrastive estimation: A new estimation principle
for unnormalized statistical models. In Proceedings
of AISTATS.

Sébastien Jean, Kyunghyun Cho, Roland Memisevic,
and Yoshua Bengio. 2015. On using very large tar-
get vocabulary for neural machine translation. In
Proceedings of ACL, pages 1–10, Beijing, China,
July.

P. Koehn, H. Hoang, A. Birch, C. Callison-Burch,
M. Federico, N. Bertoldi, B. Cowan, W. Shen,

C. Moran, R. Zens, C. Dyer, O. Bojar, A. Constantin,
and E. Herbst. 2007. Moses: open source toolkit
for statistical machine translation. In Proceedings
of ACL.

Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey
Dean. 2013. Efficient estimation of word represen-
tations in vector space. In International Conference
on Learning Representations: Workshops Track.

Andriy Mnih and Geoffrey Hinton. 2009. A scal-
able hierarchical distributed language model. In Ad-
vances in Neural Information Processing Systems,
volume 21, pages 1081–1088.

Andriy Mnih and Koray Kavukcuoglu. 2013. Learning
word embeddings efficiently with noise-contrastive
estimation. In Proceedings of NIPS, pages 2265–
2273.

Andriy Mnih and Yee Whye Teh. 2012. A fast and
simple algorithm for training neural probabilistic
language models. In Proceedings of the 29th In-
ternational Conference on Machine Learning, pages
1751–1758.

Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic
evaluation of machine translation. In Proceedings
of ACL, pages 311–318, Philadephia, USA, July.

Ilya Sutskever, Oriol Vinyals, and Quoc V. Le. 2014.
Sequence to sequence learning with neural net-
works. In Proceedings of NIPS, pages 3104–3112,
Quebec, Canada, December.

Matthew D. Zeiler. 2012. ADADELTA: an adaptive
learning rate method. CoRR.

129


