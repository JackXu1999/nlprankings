



















































Solving Hard Coreference Problems


Human Language Technologies: The 2015 Annual Conference of the North American Chapter of the ACL, pages 809–819,
Denver, Colorado, May 31 – June 5, 2015. c©2015 Association for Computational Linguistics

Solving Hard Coreference Problems

Haoruo Peng∗ and Daniel Khashabi∗ and Dan Roth
University of Illinois, Urbana-Champaign

Urbana, IL, 61801
{hpeng7,khashab2,danr}@illinois.edu

Abstract

Coreference resolution is a key problem in
natural language understanding that still es-
capes reliable solutions. One fundamental dif-
ficulty has been that of resolving instances
involving pronouns since they often require
deep language understanding and use of back-
ground knowledge. In this paper we pro-
pose an algorithmic solution that involves a
new representation for the knowledge required
to address hard coreference problems, along
with a constrained optimization framework
that uses this knowledge in coreference de-
cision making. Our representation, Predicate
Schemas, is instantiated with knowledge ac-
quired in an unsupervised way, and is com-
piled automatically into constraints that im-
pact the coreference decision. We present
a general coreference resolution system that
significantly improves state-of-the-art perfor-
mance on hard, Winograd-style, pronoun reso-
lution cases, while still performing at the state-
of-the-art level on standard coreference reso-
lution datasets.

1 Introduction

Coreference resolution is one of the most impor-
tant tasks in Natural Language Processing (NLP).
Although there is a plethora of works on this task
(Soon et al., 2001a; Ng and Cardie, 2002a; Ng,
2004; Bengtson and Roth, 2008; Pradhan et al.,
2012; Kummerfeld and Klein, 2013; Chang et al.,
2013), it is still deemed an unsolved problem due to
intricate and ambiguous nature of natural language

∗These authors contributed equally to this work.

text. Existing methods perform particularly poorly
on pronouns, specifically when gender or plurality
information cannot help. In this paper, we aim to
improve coreference resolution by addressing these
hard problems. Consider the following examples:

Ex.1 [A bird]e1 perched on the [limb]e2 and
[it]pro bent.
Ex.2 [Robert]e1 was robbed by [Kevin]e2 , and
[he]pro is arrested by police.
In both examples, one cannot resolve the pro-

nouns based on only gender or plurality informa-
tion. Recently, Rahman and Ng (2012) gathered a
dataset containing 1886 sentences of such challeng-
ing pronoun resolution problems (referred to later
as the Winograd dataset, following Winograd (1972)
and Levesque et al. (2011)). As an indication to the
difficulty of these instances, we note that a state-of-
the-art coreference resolution system (Chang et al.,
2013) achieves precision of 53.26% on it. A special
purpose classifier (Rahman and Ng, 2012) trained
on this data set achieves 73.05%. The key contribu-
tion of this paper is a general purpose, state-of-the-
art coreference approach which, at the same time,
achieves precision of 76.76% on these hard cases.

Addressing these hard coreference problems re-
quires significant amounts of background knowl-
edge, along with an inference paradigm that can
make use of it in supporting the coreference deci-
sion. Specifically, in Ex.1 one needs to know that
“a limb bends” is more likely than “a bird bends”.
In Ex.2 one needs to know that the subject of the
verb “rob” is more likely to be the object of “ar-
rest” than the object of the verb “rob” is. The
knowledge required is, naturally, centered around

809



the key predicates in the sentence, motivating the
central notion proposed in this paper, that of Pred-
icate Schemas. In this paper, we develop the no-
tion of Predicate Schemas, instantiate them with au-
tomatically acquired knowledge, and show how to
compile it into constraints that are used to resolve
coreference within a general Integer Linear Pro-
gramming (ILP) driven approach to coreference res-
olution. Specifically, we study two types of Predi-
cate Schemas that, as we show, cover a large frac-
tion of the challenging cases. The first specifies one
predicate with its subject and object, thus providing
information on the subject and object preferences of
a given predicate. The second specifies two pred-
icates with a semantically shared argument (either
subject or object), thus specifies role preferences of
one predicate, among roles of the other. We instanti-
ate these schemas by acquiring statistics in an unsu-
pervised way from multiple resources including the
Gigaword corpus, Wikipedia, Web Queries and po-
larity information.

A lot of recent work has attempted to utilize sim-
ilar types of resources to improve coreference reso-
lution (Rahman and Ng, 2011a; Ratinov and Roth,
2012; Bansal and Klein, 2012; Rahman and Ng,
2012). The common approach has been to inject
knowledge as features. However, these pieces of
knowledge provide relatively strong evidence that
loses impact in standard training due to sparsity. In-
stead, we compile our Predicate Schemas knowl-
edge automatically, at inference time, into con-
straints, and make use of an ILP driven framework
(Roth and Yih, 2004) to make decisions. Using con-
straints is also beneficial when the interaction be-
tween multiple pronouns is taken into account when
making global decisions. Consider the following ex-
ample:

Ex.3 [Jack]e1 threw the bags of [John]e2
into the water since [he]pro1 mistakenly asked
[him]pro2 to carry [his]pro3 bags.
In order to correctly resolve the pronouns in Ex.3,

one needs to have the knowledge that “he asks him”
indicates that he and him refer to different entities
(because they are subject and object of the same
predicate; otherwise, himself should be used instead
of him). This knowledge, which can be easily repre-
sented as constraints during inference, then impacts
other pronoun decisions in a global decision with re-

spect to all pronouns: pro3 is likely to be different
from pro2, and is likely to refer to e2. This type of
inference can be easily represented as a constraint
during inference, but hard to inject as a feature.

We then incorporate all constraints into a general
coreference system (Chang et al., 2013) utilizing the
mention-pair model (Ng and Cardie, 2002b; Bengt-
son and Roth, 2008; Stoyanov et al., 2010). A classi-
fier learns a pairwise metric between mentions, and
during inference, we follow the framework proposed
in Chang et al. (2011) using ILP.

The main contributions of this paper can be sum-
marized as follows:

1. We propose the Predicate Schemas representa-
tion and study two specific schemas that are im-
portant for coreference.

2. We show how, in a given context, Predicate
Schemas can be automatically compiled into
constraints and affect inference.

3. Consequently, we address hard pronoun resolu-
tion problems as a standard coreference prob-
lem and develop a system1 which shows signif-
icant improvement for hard coreference prob-
lems while achieving the same state-of-the-art
level of performance on standard coreference
problems.

The rest of the paper is organized as follows. We
describe our Predicate Schemas in Section 2 and ex-
plain the inference framework and automatic con-
straint generation in Section 3. A summary of our
knowledge acquisition steps is given in Section 4.
We report our experimental results and analysis in
Section 5, and review related work in Section 6.

2 Predicate Schema

In this section we present multiple kinds of knowl-
edge that are needed in order to improve hard coref-
erence problems. Table 1 provides two example sen-
tences for each type of knowledge. We use m to
refer to a mention. A mention can either be an en-
tity e or a pronoun pro. predm denotes the pred-
icate of m (similarly, predpro and prede for pro-
nouns and entities, respectively). For instance, in
sentence 1.1 in Table 1, the predicate of e1 and e2

1Available at http://cogcomp.cs.illinois.edu/page/software
view/Winocoref

810



Category # Sentence

1
1.1 [The bird]e1 perched on the [limb]e2 and [it]pro bent.
1.2 [The bee]e1 landed on [the flower]e2 because [it]pro had pollen.

2
2.1 [Bill]e1 was robbed by [John]e2 , so the officer arrested [him]pro.
2.2 [Jimbo]e1 was afraid of [Bobbert]e2 because [he]pro gets scared around new people.

3
3.1 [Lakshman]e1 asked [Vivan]e2 to get him some ice cream because [he]pro was hot.
3.2 Paula liked [Ness]e1 more than [Pokey]e2 because [he]pro was mean to her.

Table 1: Example sentences for each schema category. The annotated entities and pronouns are hard coref-
erence problems.

Type Schema form Explanation of examples from Table 1

1 predm (m,a)
Example 1.2: It is enough to know that:
S (have (m = [the flower], a = [pollen])) >
S (have (m = [the bee], a = [pollen]))

2 predm (m,a) |p̂redm (m, â) , cn
Example 2.2: It is enough to know that:

S (be afraid of (m = ∗, a = ∗) |get scared (m = ∗, â = ∗) , because) >
S (be afraid of (a = ∗,m = ∗) |get scared (m = ∗, â = ∗) , because)

Table 2: Predicate Schemas and examples of the logic behind the schema design. Here ∗ indicates that the
argument is dropped, and S(.) denotes the scoring function defined in the text.

Ty
pe

1

S (predm (m, a))
S (predm (a,m))
S (predm (m, ∗))
S (predm (∗,m))

Ty
pe

2

S
(

predm (m, a) |p̂redm (m, â) , cn
)

S
(

predm (a,m) |p̂redm (m, â) , cn
)

S
(

predm (m, a) |p̂redm (â,m) , cn
)

S
(

predm (a,m) |p̂redm (â,m) , cn
)

S
(

predm (m, ∗) |p̂redm (m, ∗) , cn
)

...
Table 3: Possible variations for scoring function
statistics. Here ∗ indicates that the argument is
dropped.

is prede1 = prede2 =“perch on”. cn refers to the
discourse connective (cn=“and” in sentence 1.1). a
denotes an argument of predm other than m. For
example, in sentence 1.1, assuming thatm = e1, the
corresponding argument is a = e2.

We represent the knowledge needed with two
types of Predicate Schemas (as depicted in Table 2).
To solve the assignment of [it]pro in sentence 1.1,
as mentioned in Section 1, we need the knowledge
that “a limb bends” is more reasonable than “a bird
bends”. Note that the predicate of the pronoun is
playing a key role here. Also the entity mention it-

self is essential. Similarly, for sentence 1.2, to re-
solve [it]pro, we need the knowledge that “bee had
pollen” is more reasonable than “flower had pollen”.
Here, in addition to entity mention and the predi-
cate (of the pronoun), we need the argument which
shares the predicate with the pronoun. To formally
define the type of knowledge needed we denote it
with “predm(m, a)” where m and a are a mention
and an argument, respectively2. We use S(.) to de-
note the score representing how likely the combina-
tion of the predicate-mention-argument is. For each
schema, we use several variations by either chang-
ing the order of the arguments (subj. vs obj.) or
dropping either of them. We score the various Type
1 and Type 2 schemas (shown in Table 3) differently.
The first row of Table 2 shows how Type 1 schema
is being used in the case of Sentence 1.2.

For sentence 2.2, we need to have the knowledge
that the subject of the verb phrase “be afraid of” is
more likely than the object of the verb phrase “be
afraid of” to be the subject of the verb phrase “get
scared”. The structure here is more complicated
than that of Type 1 schema. To make it clearer, we
analyze sentence 2.1. In this sentence, the object
of “be robbed by” is more likely than the subject

2Note that the order of m and a relative to the predicate is
a critical issue. To keep things general in the schemas defini-
tion, we do not show the ordering; however, when using scores
in practice the order between a mention and an argument is a
critical issue.

811



of the verb phrase “be robbed by” to be the object
of “the officer arrest”. We can see in both exam-
ples (and for the Type 2 schema in general), that
both predicates (the entity predicate and the pronoun
predicate) play a crucial role. Consequently, we de-
sign the Type 2 schema to capture the interaction
between the entity predicate and the pronoun pred-
icate. In addition to the predicates, we may need
mention-argument information. Also, we stress the
importance of the discourse connective between en-
tity mention and pronoun; if in either sentence 2.1
or 2.2, we change the discourse connective to “al-
though”, the coreference resolution will completely
change. Overall, we can represent the knowledge
as “predm (m, a) |p̂redm (m, â) , cn”. Just like
for Type 1 schema, we can represent Type 2 schema
with a score function for different variations of argu-
ments (lower half of Table 3). In Table 2, we exhibit
this for sentence 2.2.

Type 3 contains the set of instances which can-
not be solved using schemas of Type 1 or 2. Two
such examples are included in Table 1. In sentence
3.1 and 3.2, the context containing the necessary in-
formation goes beyond our triple representation and
therefore this instance cannot be resolved with ei-
ther of the two schema types. It is important to note
that the notion of Predicate Schemas is more general
than the Type 1 and Type 2 schemas introduced here.
Designing more informative and structured schemas
will be essential to resolving additional types of hard
coreference instances.

3 Constrained ILP Inference

Integer Linear Programming (ILP) based formula-
tions of NLP problems (Roth and Yih, 2004) have
been used in a board range of NLP problems and,
particularly, in coreference problems (Chang et al.,
2011; Denis and Baldridge, 2007). Our formulation
is inspired by Chang et al. (2013). Let M be the
set of all mentions in a given text snippet, and P the
set of all pronouns, such that P ⊂ M. We train a
coreference model by learning a pairwise mention
scoring function. Specifically, given a mention-pair
(u, v) ∈ M (u is the antecedent of v), we learn
a left-linking scoring function fu,v = w>φ(u, v),
where φ(u, v) is a pairwise feature vector and w is
the weight vector. We then follow the Best-Link ap-

proach (Section 2.3 from Chang et al. (2011)) for in-
ference. The ILP problem that we solve is formally
defined as follows:

arg max
y

∑
u∈M,v∈M

fu,vyu,v

s.t. yu,v ∈ {0, 1}, ∀u, v ∈M∑
u<v,u∈M yu,v ≤ 1, ∀v ∈M

Constraints from Predicate Schemas Knowledge

Constraints between pronouns.

Here, u, v are mentions and yu,v is the decision
variable to indicate whether or not mention u and
mention v are coreferents. As the first constraint
shows, yu,v is a binary variable. yu,v equals 1 if u, v
are coreferents and 0 otherwise. The second con-
straint indicates that we only choose at most one
antecedent to be coreferent with each mention v.
(u < v represents that u appears beore v, thus u
is an antecedent of v.) In this work, we add con-
straints from Predicate Schemas Knowledge and be-
tween pronouns.

The Predicate Schemas knowledge provides a
vector of score values S(u, v) for mention pairs
{(u, v)|(u ∈ M, v ∈ P}, which concatenates all
the schemas involving u and v. Entries in the score
vector are designed so that the larger the value is, the
more likely u and v are to be coreferents. We have
two ways to use the score values: 1) Augumenting
the feature vector φ(u, v) with these scores. 2) Cast-
ing the scores as constraints for the coreference res-
olution ILP in one of the following forms:{

if si(u, v) ≥ αisi(w, v)⇒ yu,v ≥ yw,v,
if si(u, v) ≥ si(w, v) + βi ⇒ yu,v ≥ yw,v,

(1)

where si(.) is the i-th dimension of the score vector
S(.) corresponding to the i-th schema represented
for a given mention pair. αi and βi are threshold
values which we tune on a development set.3 If an
inequality holds for all relevant schemas (that is, all
the dimensions of the score vector), we add an in-
equality between the corresponding indicator vari-
ables inside the ILP.4 As we increase the value of a

3For the ith dimension of the score vector, we choose either
αi or βi as the threshold.

4If the constraints dictated by any two dimensions of S are
contradictory, we ignore both of them.

812



threshold, the constraints in (1) become more con-
servative, thus it leads to fewer but more reliable
constraints added into the ILP. We tune the thresh-
old values such that their corresponding scores at-
tain high enough accuracy, either in the multiplica-
tive form or the additive form.5 Note that, given a
pair of mentions and context, we automatically in-
stantiate a collection of relevant schemas, and then
generate and evaluate a set of corresponding con-
straints. To the best of our knowledge, this is the
first work to use such automatic constraint gener-
ation and tuning method for coreference resolution
with ILP inference. In Section 4, we describe how
we acquire the score vectors S(u, v) for the Predi-
cate Schemas in an unsupervised fashion.

We now briefly explain the pre-processing step re-
quired in order to extract the score vector S(u, v)
from a pair of mentions. Define a triple structure
tm , predm(m, am) for anym ∈M. The subscript
m for pred and a, emphasizes that they are extracted
as a function of the mention m. The extraction of
triples is done by utilizing the dependency parse
tree from the Easy-first dependency parser (Gold-
berg and Elhadad, 2010). We start with a mention
m, and extract its related predicate and the other ar-
gument based on the dependency parse tree and part-
of-speech information. To handle multiword predi-
cates and arguments, we use a set of hand-designed
rules. We then get the score vector S(u, v) by con-
catenating all scores of the Predicate Schemas given
two triples tu, tv. Thus, we can expand the score
representation for each type of Predicate Schemas
given in Table 2: 1) For Type 1 schema, S(u, v) ≡
S(predv(m = u, a = av)) 6 2) For Type 2 schema,
S(u, v) ≡ S(predu(m = u, a = au)|p̂redv(m =
v, a = av), cn).

In additional to schema-driven constraints, we
also apply constraints between pairs of pronouns
within a fixed distance7. For two pronouns that are
semantically different (e.g. he vs. it), they must refer
to different antecedents. For two non-possesive pro-
nouns that are related to the same predicate (e.g. he

5The choice is made based on the performance on the devel-
opment set.

6In predv(m = u, a = av) the argument and the predicate
are extracted relative to v but the mention m is set to be u.

7We set the distance to be 3 sentences.

saw him), they must refer to different antecedents.8

4 Knowledge Acquisition

One key point that remains to be explained is
how to acquire the knowledge scores S(u, v). In
this section, we propose multiple ways to ac-
quire these scores. In the current implementa-
tion, we make use of four resources. Each of
them generates its own score vector. Therefore,
the overall score vector is the concatenation of
the score vector from each resource: S(u, v) =
[Sgiga(u, v) Swiki(u, v) Sweb(u, v) Spol(u, v)].
4.1 Gigaword Co-occurence
We extract triples tm , predm(m, am) (explained
in Section 3) from Gigaword data (4,111,240 docu-
ments). We start by extracting noun phrases using
the Illinois-Chunker (Punyakanok and Roth, 2001).
For each noun phrase, we extract its head noun and
then extract the associated predicate and argument
to form a triple.

We gather the statistics for both schema types af-
ter applying lemmatization on the predicates and
arguments. Using the extracted triples, we get
a score vector from each schema type: Sgiga =
[S(1)giga S(2)giga].

To extract scores for Type 1 Predicate Schemas,
we create occurence counts for each schema in-
stance. After all scores are gathered, our goal is to
query S(1)giga(u, v) ≡ S(predv(m = u, a = av))
from our knowledge base. The returned score is the
log(.) of the number of occurences.

For Type 2 Predicate Schemas, we gather the
statistics of triple co-occurence. We count the co-
occurrence of neighboring triples that share at least
one linked argument. We consider two triples to be
neighbors if they are within a distance of three sen-
tences. We use two heuristic rules to decide whether
a pair of arguments between two neighboring triples
are coreferents or not: 1) If the head noun of two ar-
guments can match, we consider them coreferents.
2) If one argument in the first triple is a person name
and there is a compatible pronoun (based on its gen-
der and plurality information) in the second triple,
they are also labeled as coreferents. We also extract
the discourse connectives between triples (because,

8Three cases are considered: he-him, she-her, they-them

813



spol(u, v) =

1{Po(pu) = + AND Po(pv) = +} OR 1{Po(pu) = − AND Po(pv) = −}1{Po(pu) = + AND Po(pv) = +}
1{Po(pu) = − AND Po(pv) = −}


Table 4: Extrating the polarity score given polarity information of a mention-pair (u, v). To be brief, we use
the shorthand notation pv , predv and pu , predu. 1{·} is an indicator function. spol(u, v) is a binary
vector of size three.

therefore, etc.) if there are any. To avoid sparsity,
we only keep the mention roles (only subj or obj; no
exact strings are kept). Two triple-pairs are consid-
ered different if they have different predicates, dif-
ferent roles, different coreferred argument-pairs, or
different discourse connectives. The co-occurrence
counts extracted in this form correspond to Type 2
schemas in Table 2. During inference, we match
a Type 2 schema for S(2)giga(u, v) ≡ S(predu(m =
u, a = au)|p̂redv(m = u, a = av), cn).

Our method is related, but different from the pro-
posal in Balasubramanian et al. (2012), who sug-
gested to extract triples using an OpenIE system
(Mausam et al., 2012). We extracted triples by start-
ing from a mention, then extract the predicate and
the other argument. An OpenIE system does not
easily provide this ability. Our Gigaword counts
are gathered also in a way similar to what has been
proposed in Chambers and Jurafsky (2009), but we
gather much larger amounts of data.

4.2 Wikipedia Disambiguated Co-occurence

One of the problems with blindly extracting triple
counts is that we may miss important semantic in-
formation. To address this issue, we use the publicly
avaiable Illinois Wikifier (Cheng and Roth, 2013;
Ratinov et al., 2011), a system that disambiguates
mentions by mapping them into correct Wikipedia
pages, to process the Wikipedia data. We then ex-
tract from the Wikipedia text all entities, verbs and
nouns, and gather co-occurrence statistics with these
syntactic variations: 1) immediately after 2) imme-
diately before 3) before 4) after. For each of these
variations, we get the probability and count9 of a
pair of words (e.g. probability10/count for “bend”
immediately following “limb”) as separate dimen-
sions of the score vector.

9We use the log(.) of the counts here.
10Conditional probability of “limb” immediately following

the given verb “bend”.

Given the co-occurrence information, we get
a score vector Swiki(u, v) corresponding to Type
1 Predicate Schemas, and hence S(u, v)wiki ≡
S(predv(m = u, a = av)).

4.3 Web Search Query Count

Our third source of score vectors is web queries that
we implement using Google queries. We extract a
score vector Sweb(u, v) ≡ S(predv(m = u, a =
av)) (Type 1 Predicate Schemas) by querying for 1)
“u av” 2) “u predv” 3) “u predv av” 4) “av u”11.
For each variation of nouns (plural and singular) and
verbs (different tenses) we create a different query
and average the counts over all queries. Concatenat-
ing the counts (each is a separate dimension) would
give us the score vector Sweb(u, v).

4.4 Polarity of Context

Another rich source of information is the polarity of
context, which has been previously used for Wino-
grad schema problems (Rahman and Ng, 2012).
Here we use a slightly modified version. The polar-
ity scores are used for Type 1 Predicate Schemas and
therefore we want to get Spol(u, v) ≡ S(predv(m =
u, a = av)). We first extract polarity values for
Po(predu) and Po(predv) by repeating the follow-
ing procedures for each of them:

• We extract initial polarity information given the
predicate (using the data provided by Wilson et
al. (2005)).
• If the role of the mention is object, we negate

its polarity.
• If there is a polarity-reversing discourse con-

nective (such as “but”) preceding the predicate,
we reverse the polarity.
• If there is a negative comparative adverb (such

as “less”, “lower”) we reverse the polarity.

11We query this only when av is an adjective and predv is a
to-be verb.

814



# Doc # Train # Test # Mention # Pronoun # Predictions for Pronoun
Winograd 1886 1212 674 5658 1886 1348
WinoCoref 1886 1212 674 6404 2595 2118
ACE 375 268 107 23247 3862 13836
OntoNotes 3150 2802 348 175324 58952 37846

Table 5: Statistics of Winograd, WinoCoref, ACE and OntoNotes. We give the total number of mentions and
pronouns, while the number of predictions for pronoun is specific for the test data. We added 746 mentions
(709 among them are pronouns) to WinoCoref compared to Winograd.

Given the polarity values Po(predu) and Po(predv),
we construct the score vector Spol(u, v) following
Table 4.

5 Experiments

In this section, we evaluate our system for both hard
coreference problems and general coreference prob-
lems, and provide detailed anaylsis on the impact of
our proposed Predicate Schemas. Since we treat re-
solving hard pronouns as part of the general corefer-
ence problems, we extend the Winograd dataset with
a more complete annotation to get a new dataset.
We evaluate our system on both datasets, and show
significant improvemnt over the baseline system and
over the results reported in Rahman and Ng (2012).
Moreover, we show that, at the same time, our sys-
tem achieves the state-of-art performance on stan-
dard coreference datasets.

5.1 Experimental Setup
Datasets: Since we aim to solve hard coreference
problems, we choose to test our system on the Wino-
grad dataset12 (Rahman and Ng, 2012). It is a chal-
lenging pronoun resolution dataset which consists
of sentence pairs based on Winograd schemas. The
original annotation only specifies one pronoun and
two entites in each sentence, and it is considered as a
binary decision for each pronoun. As our target is to
model and solve them as general coreference prob-
lems, we expand the annotation to include all pro-
nouns and their linked entities as mentions (We call
this new re-annotated dataset WinoCoref 13). Ex.3 in
Section 1 is from the Winograd dataset. It originally
only specifies he as the pronoun in question, and
we added him and his as additional target pronouns.
We also use two standard coreference resolution

12Available at http://www.hlt.utdallas.edu/˜vince/data/emnlp12/
13Available at http://cogcomp.cs.illinois.edu/page/data/

Systems Learning Method Inference Method
Illinois BLMP BLL
IlliCons BLMP ILP

KnowFeat BLMP+SF BLL
KnowCons BLMP ILP+SC
KnowComb BLMP+SF ILP+SC

Table 6: Summary of learning and inference meth-
ods for all systems. SF stands for schema features
while SC represents constraints from schema knowl-
edge.

datasets ACE(2004) (NIST, 2004) and OntoNotes-
5.0 (Pradhan et al., 2011) for evaluation. Statistics
of the datasets are provided in Table 5.
Baseline Systems: We use the state-of-art Illinois
coreference system as our baseline system (Chang
et al., 2013). It includes two different versions. One
employs Best-Left-Link (BLL) inference method
(Ng and Cardie, 2002b), and we name it Illinois14;
while the other uses ILP with constraints for infer-
ence, and we name it IlliCons. Both systems use
Best-Link Mention-Pair (BLMP) model for training.
On Winograd dataset, we also treat the reported re-
sult from Rahman and Ng (2012) as a baseline.
Developed Systems: We present three variations of
the Predicate Schemas based system developed here.
We inject Predicate Schemas knowledge as mention-
pair features and retrain the system (KnowFeat).
We use the original coreference model and Predi-
cate Schemas knowledge as constraints during infer-
ence (KnowCons). We also have a combined system
(KnowComb), which uses the schema knowledge to
add features for learning as well as constraints for
inference. A summary of all systems is provided in
Table 6.

14In implementation, we use the L3M model proposed in
Chang et al. (2013), which is slightly different. It can be seen
as an extension of BLL inference method.

815



Dataset Metric Illinois IlliCons Rahman and Ng (2012) KnowFeat KnowCons KnowComb
Winograd Precision 51.48 53.26 73.05 71.81 74.93 76.41
WinoCoref AntePre 68.37 74.32 —– 88.48 88.95 89.32

Table 7: Performance results on Winograd and WinoCoref datasets. All our three systems are trained on
WinoCoref, and we evaluate the predictions on both datasets. Our systems improve over the baselines by
over than 20% on Winograd and over 15% on WinoCoref.

Evaluation Metrics: When evaluating on the full
datasets of ACE and OntoNotes, we use the widely
recoginzed metrics MUC (Vilain et al., 1995),
BCUB (Bagga and Baldwin, 1998), Entity-based
CEAF (CEAFe) (Luo, 2005) and their average.
As Winograd is a pronoun resolution dataset, we
use precision as the evaluation metric. Although
WinoCoref is more general, each coreferent clus-
ter only contains 2-4 mentions and all are within the
same sentence. Since traditional coreference metrics
cannot serve as good metrics, we extend the preci-
sion metric and design a new one called AntePre.
Suppose there are k pronouns in the dataset, and
each pronoun has n1, n2, · · · , nk antecedents, re-
spectively. We can view predicted coreference clus-
ters as binary decisions on each antecedent-pronoun
pair (linked or not). The total number of binary deci-
sions is

∑k
i=1 ni. We then meaure how many binary

decisions among them are correct; letm be the num-
ber of correct decisions, then AnrePre is computed
as: m∑k

i=1 ni
.

5.2 Results for Hard Coreference Problems
Performance results on Winograd and WinoCoref
datasets are shown in Table 7. The best perform-
ing system is KnowComb. It improves by over
20% over a state-of-art general coreference system
on Winograd and also outperforms Rahman and Ng
(2012) by a margin of 3.3%. On the WinoCoref
dataset, it improves by 15%. These results show sig-
nificant performance improvement by using Predi-
cate Schemas knowledge on hard coreference prob-
lems. Note that the system developed in Rahman
and Ng (2012) cannot be used on the WinoCoref
dataset. The results also show that it is better to com-
pile knowledge into constraints when the knowledge
quality is high than add them as features.

5.3 Results for Standard Coreference Problems
Performance results on standard ACE and
OntoNotes datasets are shown in Table 8. Our

System MUC BCUB CEAFe AVG
ACE

IlliCons 78.17 81.64 78.45 79.42
KnowComb 77.51 81.97 77.44 78.97

OntoNotes
IlliCons 84.10 78.30 68.74 77.05
KnowComb 84.33 78.02 67.95 76.76

Table 8: Performance results on ACE and OntoNotes
datasets. Our system gets the same level of per-
formance compared to a state-of-art general coref-
erence system.

Category Cat1 Cat2 Cat3
Size 317 1060 509
Portion 16.8% 56.2% 27.0%

Table 9: Distribution of instances in Winograd
dataset of each category. Cat1/Cat2 is the subset of
instances that require Type 1/Type 2 schema knowl-
edge, respectively. All other instances are put into
Cat3. Cat1 and Cat2 instances can be covered by
our proposed Predicate Schemas.

KnowComb system achieves the same level of
performance as does the state-of-art general
coreference system we base it on. As hard coref-
erence problems are rare in standard coreference
datasets, we do not have significant performance
improvement. However, these results show that
our additional Predicate Schemas do not harm the
predictions for regular mentions.

5.4 Detailed Analysis

To study the coverage of our Predicate Schemas
knowledge, we label the instances in Winograd
(which also applies to WinoCoref ) with the type of
Predicate Schemas knowledge required. The distri-
bution of the instances is shown in Table 9. Our
proposed Predicate Schemas cover 73% of the in-
stances.

We also provide an ablation study on the

816



Schema AntePre(Test) AntePre(Train)
Type 1 76.67 86.79
Type 2 79.55 88.86
Type 1 (Cat1) 90.26 93.64
Type 2 (Cat2) 83.38 92.49

Table 10: Ablation Study of Knowledge Schemas on
WinoCoref. The first line specifies the preformance
for KnowComb with only Type 1 schema knowl-
edge tested on all data while the third line speci-
fies the preformance using the same model but tested
on Cat1 data. The second line specifies the prefor-
mance results for KnowComb system with only Type
2 schema knowledge on all data while the fourth line
specifies the preformance using the same model but
tested on Cat2 data.

WinoCoref dataset in Table 10. These results use
the best performing KnowComb system. They show
that both Type 1 and Type 2 schema knowledge have
higher precision on Category 1 and Category 2 data
instances, respectively, compared to that on full data.
Type 1 and Type 2 knowledge have similiar perfor-
mance on full data, but the results show that it is
harder to solve instances in category 2 than those
in category 1. Also, the performance drop between
Cat1/Cat2 and full data indicates that there is a need
to design more complicated knowledge schemas and
to refine the knowledge acquisition for further per-
formance improvement.

6 Related Work

Winograd Schema: Winograd (1972) showed that
small changes in context could completely change
coreference decisions. Levesque et al. (2011) pro-
posed to assemble a set of sentences which com-
ply with Winograd’s schema. Specifically, there are
pairs of sentences which are identical except for mi-
nor differences which lead to different references of
the same pronoun in both sentences. These refer-
ences can be easily solved by humans, but are hard,
he claimed, for computer programs.
Anaphora Resolution: There has been a lot of
work on anaphora resolution in the past two decades.
Many of the early rule-based systems like Hobbs
(1978) and Lappin and Leass (1994) gained consid-
erable popularity. The early designs were easy to
understand and the rules were designed manually.

With the development of machine learning based
models (Connolly et al., 1994; Soon et al., 2001b;
Ng and Cardie, 2002a), attention shifted to solving
standard coreference resolution problems. However,
many hard coreference problems involve pronouns.
As Winograd’s schema shows, there is still a need
for further investigation in this subarea.
World Knowledge Acquisition: Many tasks in
NLP (such as Textual Entailment, Question Answer-
ing, etc.) require World Knowledge. Although
there are many existing works on acquiring them
(Schwartz and Gomez, 2009; Balasubramanian et
al., 2013; Tandon et al., 2014), there is still no con-
sensus on how to represent, gather and utilize high
quality World Knowledge. When it comes to corefer-
ence resolution, there are a handful of works which
either use web query information or apply align-
ment to an external knowledge base (Rahman and
Ng, 2011b; Kobdani et al., 2011; Ratinov and Roth,
2012; Bansal and Klein, 2012; Zheng et al., 2013).
With the introduction of Predicate Schema, our goal
is to bring these different approaches together and
provide a coherent view.

Acknowledgments

The authors would like to thank Kai-Wei Chang, Al-
ice Lai, Eric Horn and Stephen Mayhew for com-
ments that helped to improve this work. This work
is partly supported by NSF grant #SMA 12-09359
and by DARPA under agreement number FA8750-
13-2-0008. The U.S. Government is authorized to
reproduce and distribute reprints for Governmen-
tal purposes notwithstanding any copyright nota-
tion thereon. The views and conclusions contained
herein are those of the authors and should not be
interpreted as necessarily representing the official
policies or endorsements, either expressed or im-
plied, of DARPA or the U.S. Government.

References

A. Bagga and B. Baldwin. 1998. Algorithms for scoring
coreference chains. In The First International Con-
ference on Language Resources and Evaluation Work-
shop on Linguistics Coreference, pages 563–566.

N. Balasubramanian, S. Soderland, O. Etzioni, et al.
2012. Rel-grams: a probabilistic model of relations
in text. In Proceedings of the Joint Workshop on Au-

817



tomatic Knowledge Base Construction and Web-scale
Knowledge Extraction, pages 101–105. Association
for Computational Linguistics.

N. Balasubramanian, S. Soderland, Mausam, and O. Et-
zioni. 2013. Generating coherent event schemas at
scale. In EMNLP, pages 1721–1731.

M. Bansal and D. Klein. 2012. Coreference seman-
tics from web features. In Proceedings of the Annual
Meeting of the Association for Computational Linguis-
tics (ACL), Jeju Island, South Korea, July.

E. Bengtson and D. Roth. 2008. Understanding the value
of features for coreference resolution. In Proceedings
of the Conference on Empirical Methods for Natural
Language Processing (EMNLP), pages 294–303, Oct.

N. Chambers and D. Jurafsky. 2009. Unsupervised
learning of narrative schemas and their participants.
In Proceedings of the Joint Conference of the 47th
Annual Meeting of the ACL and the 4th International
Joint Conference on Natural Language Processing of
the AFNLP, volume 2, pages 602–610. Association for
Computational Linguistics.

K. Chang, R. Samdani, A. Rozovskaya, N. Rizzolo,
M. Sammons, and D. Roth. 2011. Inference protocols
for coreference resolution. In Proceedings of the An-
nual Conference on Computational Natural Language
Learning (CoNLL), pages 40–44, Portland, Oregon,
USA. Association for Computational Linguistics.

K. Chang, R. Samdani, and D. Roth. 2013. A con-
strained latent variable model for coreference reso-
lution. In Proceedings of the 2013 Conference on
Empirical Methods in Natural Language Processing,
pages 601–612. Association for Computational Lin-
guistics.

X. Cheng and D. Roth. 2013. Relational inference for
wikification. In Proceedings of the 2013 Conference
on Empirical Methods in Natural Language Process-
ing, pages 1787–1796. Association for Computational
Linguistics.

D. Connolly, J. D. Burger, and D. S. Day. 1994. A ma-
chine learning approach to anaphoric reference. In
Proceedings of the International Conference on New
Methods in Language Processing (NeMLaP). ACL.

P. Denis and J. Baldridge. 2007. Joint determination of
anaphoricity and coreference resolution using integer
programming. In Proceedings of the Annual Meeting
of the North American Association of Computational
Linguistics (NAACL).

Y. Goldberg and M. Elhadad. 2010. An efficient algo-
rithm for easy-first non-directional dependency pars-
ing. In Human Language Technologies: The 2010
Annual Conference of the North American Chapter of
the Association for Computational Linguistics, pages
742–750. Association for Computational Linguistics.

J. R Hobbs. 1978. Resolving pronoun references. Lin-
gua, 44(4):311–338.

H. Kobdani, H. Schuetze, M. Schiehlen, and H. Kamp.
2011. Bootstrapping coreference resolution using
word associations. In Proceedings of the 49th Annual
Meeting of the Association for Computational Linguis-
tics: Human Language Technologies, pages 783–792.
Association for Computational Linguistics.

K. J. Kummerfeld and D. Klein. 2013. Error-driven anal-
ysis of challenges in coreference resolution. In Pro-
ceedings of the 2013 Conference on Empirical Meth-
ods in Natural Language Processing, pages 265–277.
Association for Computational Linguistics.

S. Lappin and H. J. Leass. 1994. An algorithm for
pronominal anaphora resolution. Computational lin-
guistics, 20(4):535–561.

H. J. Levesque, E. Davis, and L. Morgenstern. 2011. The
winograd schema challenge. In AAAI Spring Sympo-
sium: Logical Formalizations of Commonsense Rea-
soning.

X. Luo. 2005. On coreference resolution perfor-
mance metrics. In Proceedings of the Conference on
Empirical Methods for Natural Language Processing
(EMNLP).

Mausam, M. Schmitz, R. Bart, S. Soderland, and O. Et-
zioni. 2012. Open language learning for informa-
tion extraction. In Proceedings of Conference on Em-
pirical Methods in Natural Language Processing and
Computational Natural Language Learning (EMNLP-
CoNLL).

V. Ng and C. Cardie. 2002a. Identifying anaphoric
and non-anaphoric noun phrases to improve coref-
erence resolution. In Proceedings of the 19th in-
ternational conference on Computational linguistics-
Volume 1, pages 1–7. Association for Computational
Linguistics.

V. Ng and C. Cardie. 2002b. Improving machine learn-
ing approaches to coreference resolution. In Proceed-
ings of the 40th Annual Meeting of the Association for
Computational Linguistics.

V. Ng. 2004. Learning noun phrase anaphoricity to im-
prove conference resolution: Issues in representation
and optimization. In Proceedings of the 42nd Annual
Meeting of the Association for Computational Linguis-
tics (ACL-04).

NIST. 2004. The ACE evaluation plan.
S. Pradhan, L. Ramshaw, M. Marcus, M. Palmer,

R. Weischedel, and N. Xue. 2011. Conll-2011 shared
task: Modeling unrestricted coreference in ontonotes.
In Proceedings of the Annual Conference on Compu-
tational Natural Language Learning (CoNLL).

S. Pradhan, A. Moschitti, N. Xue, O. Uryupina, and
Y. Zhang. 2012. CoNLL-2012 shared task: Modeling

818



multilingual unrestricted coreference in OntoNotes. In
Proceedings of the Annual Conference on Computa-
tional Natural Language Learning (CoNLL).

V. Punyakanok and D. Roth. 2001. The use of classi-
fiers in sequential inference. In The Conference on
Advances in Neural Information Processing Systems
(NIPS), pages 995–1001. MIT Press.

A. Rahman and V. Ng. 2011a. Coreference resolu-
tion with world knowledge. In Proceedings of the
49th Annual Meeting of the Association for Compu-
tational Linguistics: Human Language Technologies,
pages 814–824. Association for Computational Lin-
guistics.

A. Rahman and V. Ng. 2011b. Coreference resolution
with world knowledge. In ACL, pages 814–824.

A. Rahman and V. Ng. 2012. Resolving complex
cases of definite pronouns: the winograd schema chal-
lenge. In Proceedings of the 2012 Joint Conference
on Empirical Methods in Natural Language Process-
ing and Computational Natural Language Learning,
pages 777–789. Association for Computational Lin-
guistics.

L. Ratinov and D. Roth. 2012. Learning-based multi-
sieve co-reference resolution with knowledge. In Pro-
ceedings of the Conference on Empirical Methods for
Natural Language Processing (EMNLP).

Lev Ratinov, Dan Roth, Doug Downey, and Mike An-
derson. 2011. Local and global algorithms for dis-
ambiguation to wikipedia. In Proceedings of the 49th
Annual Meeting of the Association for Computational
Linguistics: Human Language Technologies, pages
1375–1384, Portland, Oregon, USA, June. Association
for Computational Linguistics.

D. Roth and W. Yih. 2004. A linear programming formu-
lation for global inference in natural language tasks. In
Hwee Tou Ng and Ellen Riloff, editors, Proceedings
of the Annual Conference on Computational Natural
Language Learning (CoNLL), pages 1–8. Association
for Computational Linguistics.

H. A. Schwartz and F. Gomez. 2009. Acquiring applica-
ble common sense knowledge from the web. In Pro-
ceedings of the Workshop on Unsupervised and Mini-
mally Supervised Learning of Lexical Semantics, UM-
SLLS ’09, pages 1–9, Stroudsburg, PA, USA. Associ-
ation for Computational Linguistics.

W. M. Soon, D. C. Y. Lim, and H. T. Ng. 2001a. A
machine learning approach to coreference resolution
of noun phrases. Computational Linguistics, Volume
27, Number 4, December 2001.

Wee Meng Soon, Hwee Tou Ng, and Daniel Chung Yong
Lim. 2001b. A machine learning approach to coref-
erence resolution of noun phrases. Computational lin-
guistics, 27(4):521–544.

V. Stoyanov, C. Cardie, N. Gilbert, E. Riloff, D. Buttler,
and D. Hysom. 2010. Coreference resolution with
reconcile. In Proceedings of the ACL 2010 Conference
Short Papers, pages 156–161. Association for Compu-
tational Linguistics.

N. Tandon, G. de Melo, and G. Weikum. 2014. Acquir-
ing comparative commonsense knowledge from the
web. In Proceedings of AAAI Conference on Artificial
Intelligence. AAAI.

M. Vilain, J. Burger, J. Aberdeen, D. Connolly, and
L. Hirschman. 1995. A model-theoretic coreference
scoring scheme. In Proceedings of the 6th conference
on Message understanding.

T. Wilson, P. Hoffmann, S. Somasundaran, J. Kessler,
J. Wiebe, Y. Choi, C. Cardie, E. Riloff, and S. Pat-
wardhan. 2005. Opinionfinder: A system for sub-
jectivity analysis. In Proceedings of HLT/EMNLP on
interactive demonstrations, pages 34–35. Association
for Computational Linguistics.

T. Winograd. 1972. Understanding natural language.
Cognitive psychology, 3(1):1–191.

J. Zheng, L. Vilnis, S. Singh, J. D. Choi, and A. Mc-
Callum. 2013. Dynamic knowledge-base alignment
for coreference resolution. In Proceedings of the Sev-
enteenth Conference on Computational Natural Lan-
guage Learning (CoNLL), pages 153–162, Sofia, Bul-
garia, August. Association for Computational Linguis-
tics.

819


