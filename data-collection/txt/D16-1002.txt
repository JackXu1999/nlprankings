



















































Rule Extraction for Tree-to-Tree Transducers by Cost Minimization


Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 12–22,
Austin, Texas, November 1-5, 2016. c©2016 Association for Computational Linguistics

Rule Extraction for Tree-to-Tree Transducers
by Cost Minimization

Pascual Martı́nez-Gómez1
pascual.mg@aist.go.jp

Yusuke Miyao1,2,3
yusuke@nii.ac.jp

1Artificial Intelligence Research Center, AIST
2National Institute of Informatics and JST, PRESTO

3The Graduate University for Advanced Studies (SOKENDAI)
Tokyo, Japan

Abstract

Tree transducers that model expressive
linguistic phenomena often require word-
alignments and a heuristic rule extractor to
induce their grammars. However, when the
corpus of tree/string pairs is small compared
to the size of the vocabulary or the com-
plexity of the grammar, word-alignments
are unreliable. We propose a general rule
extraction algorithm that uses cost functions
over tree fragments, and formulate the extrac-
tion as a cost minimization problem. As a
by-product, we are able to introduce back-off
states at which some cost functions generate
right-hand-sides of previously unseen left-
hand-sides, thus creating transducer rules
“on-the-fly”. We test the generalization power
of our induced tree transducers on a QA task
over a large Knowledge Base, obtaining a
reasonable syntactic accuracy and effectively
overcoming the typical lack of rule coverage.

1 Introduction

Tree transducers are general and solid theoreti-
cal models that have been applied to a variety of
NLP tasks, such as machine translation (Knight and
Graehl, 2005), text summarization (Cohn and Lap-
ata, 2009), question answering (Jones et al., 2012),
paraphrasing and textual entailment (Wu, 2005).
One strategy to obtain transducer rules is by exhaus-
tive enumeration; however, this method is ineffec-
tive when there is a high structural language vari-
ability and we wish to have an expressive model.
Another strategy is to heuristically extract rules from
a corpus of tree/string pairs and word-alignments, as

GHKM algorithm does (Galley et al., 2004); how-
ever, word-alignments are difficult to estimate when
the corpus is small. This would be the case, for ex-
ample, of machine translation for low-resourced lan-
guages where there is often small numbers of paral-
lel sentences, or in Question Answering (QA) tasks
where the number of Knowledge Base (KB) identi-
fiers (concepts) is much larger than QA datasets.

Our main contribution is an algorithm that formu-
lates the rule extraction as a cost minimization prob-
lem, where the search for the best rules is guided
by an ensemble of cost functions over pairs of tree
fragments. In GHKM, a tree fragment and a se-
quence of words are extracted together if they are
minimal and their word alignments do not fall out-
side of their respective boundaries. However, given
that alignment violations are not allowed, the qual-
ity of the extracted rules degrades as the rate of
misaligned words increases. In our framework, we
can mimic GHKM by assigning an infinite cost to
pairs of tree fragments that violate such conditions
on word alignments and by adding a cost regular-
izer on the size of the tree fragments. Smoother cost
functions, however, would permit controlled mis-
alignments, contributing to generalization. Given
the generality of these cost functions, we believe that
the applicability of tree transducers will be extended.

A by-product of introducing these cost functions
is that some of them may act as rule back-offs,
where transducer rules are built “on-the-fly” when
the transducer is at a predefined back-off state but
there is no rule whose left-hand-side (lhs) matches
the input subtree. These back-off states can be seen
as functions that are capable of generating right-

12



hand-sides (rhs) for unseen input subtrees.
Our rule extraction algorithm and back-off

scheme are general, in the sense that they can be
applied to any tree transformation task. However,
in this paper, we extrinsically evaluate the quality
of the extracted rules in a QA task, where the ob-
jective is to transform syntactic trees of questions
into constituent trees that represent Sparql queries
on Freebase, a large Knowledge Base. Implement-
ing all components of a QA system at a sufficient
level is out of the scope of this paper; for that reason,
in order to evaluate our contribution in isolation, we
use the FREE917 corpus released by Cai and Yates
(2013), for which an entity and predicate lexicon is
available1. We show that a tree-to-tree transducer in-
duced using our rule extraction and back-off scheme
is accurate and generalizes well, which was not pre-
viously achieved with tree transducers in semantic
parsing tasks such as QA over large KBs.

2 Related Work

Tree transducers were first proposed by Rounds
(1970) and Thatcher (1970), and have been greatly
developed recently (Knight and Graehl, 2005).
Jones et al. (2012) used tree transducers to seman-
tically parse narrow-domain questions into Prolog
queries for GeoQuery (Wong and Mooney, 2006),
a small database of 700 geographical facts. Rules
were exhaustively enumerated, which was possible
given the small size of the database and low variabil-
ity of questions. Another strategy is that of Li et al.
(2013), where they used a variant of GHKM to in-
duce tree transducers that parse into λ-SCFG. Word-
to-node alignments could be reliably estimated with
the IBM models (Brown et al., 1993) given, again,
the small vocabulary and database size of GeoQuery.
In such small-scale tasks, our rule extraction and
back-off scheme offers no obvious advantage. How-
ever, when doing QA over larger and more realistic
KBs (and other tasks with similar characteristics),
exhaustive enumeration of rules or reliable estima-
tions of alignments are not possible, which prevents
the application of tree transducers. Thus, it is on the
latter type of tasks where we focus our contribution.

A similar problem has been considered in the tree

1The entity lexicon was released by the authors of FREE917,
and the predicate lexicon is ours.

mapping literature in the form of the tree-to-tree edit
distance. In that formulation, three edit operations
are defined, namely, deleting and inserting single
nodes, and replacing the label of a node. These edit
operations have a cost associated to them, and the
task consists of finding the minimum edit cost and its
corresponding edit script2 that transforms a source
into a target tree. The problem was first solved by
Tai (1979), and later Zhang and Shasha (1989) pro-
posed a simpler and faster dynamic programming al-
gorithm that operates in polynomial time, and that
has inspired multiple variations (Bille, 2005).

However, we need edit operations that involve
tree fragments (e.g., noun phrases or parts of verb
phrases), rather than single nodes, when searching
for the best mappings. We address this problem by
searching for non-isomorphic tree mappings, in line
with Eisner (2003), except that our rule extraction
algorithm is guided by an ensemble of cost functions
over pairs of tree fragments. This algorithm is capa-
ble of extracting rules more robustly than GHKM
by permitting misalignments in a controlled man-
ner. Finding a tree mapping solves simultaneously
the alignment and the rule extraction problem.

There is a wide array of tree transducers with dif-
ferent expressive capabilities (Knight and Graehl,
2005). We consider extended3 root-to-frontier4 lin-
ear5 transducers (Maletti et al., 2009), possibly with
deleting6 operations. In this paper, we syntactically
parse the natural language question and transform it
into a meaning representation, similarly to Ge and
Mooney (2005). But instead of using Prolog formu-
lae or λ-SCFG, we use constituent representations of
λ−DCS expressions (Liang, 2013), which is a for-
mal language convenient to represent Sparql queries
where variables are eliminated by making existential
quantifications implicit (see example in Figure 1).

Another challenge is to construct transducers with
sufficient rule coverage, which would require bil-
lions of lexical rules that map question phrases to
database entities or relations. Even if those rules
were available, estimating their rule probabilities
would be difficult given the small data sets of ques-

2Sequence of edit operations.
3lhs may have depth larger than 1.
4Top-down transformations.
5lhs variables appear at most once in the rhs.
6Some variables on the lhs may not appear in the rhs.

13



tions paired with their logical representations. We
solve the problem by constructing lexical rules “on-
the-fly” at the decoding stage, similarly to the candi-
date generation stage of entity linking systems (Ling
et al., 2015). Rule weights are also predicted on-the-
fly given rule features and model parameters similar
to Cohn and Lapata (2009).

3 Background

Tree transducers apply to general tree transforma-
tion problems, but for illustrative purposes, we use
the tree pair s and t in Figure 1 (from FREE917) as
a running example. s is the syntactic constituent tree
of the question “how many teams participate in the
uefa”, whereas t is a constituent tree of an executable
meaning representation in the λ−DCS formalism:

count(Team.League.Uefa)

Its corresponding lambda expression is

count(λx.∃a.Team(x, a) ∧ League(a, Uefa))

which can be converted into a Sparql KB query:

SELECT COUNT(?x) WHERE {
?a Team ?x .

?a League Uefa . }

Following the terminology of Graehl and Knight
(2004), we define a tree-to-tree transducer as a 5-
tuple (Q,Σ,∆, qstart,R) where Q is the set of
states, Σ and ∆ are the sets of symbols of the in-
put and output languages, qstart is the initial state,
and R is the set of rules. For convenience, define
TΣ as the set of trees with symbols in Σ, TΣ(A) the
set of trees with symbols in Σ ∪ A where symbols
in A only appear in the leaves, X as the set of vari-
ables {x1, . . . , xn}, and A.B for the cross-product
of two sets A and B. A rule r ∈ R has the form
q.ti

s→ to, where q ∈ Q is a state, ti ∈ TΣ(X ) is the
left-hand-side (lhs) tree pattern (or elementary tree),
to ∈ T∆(Q.X ) the right-hand-side (rhs), and s ∈ R
the rule score.

Tree-to-tree transducers apply a sequence of rules
to transform a source s into a target t tree. A root-to-
frontier transducer starts at the root of the source tree
and searches R for a rule whose i) tree pattern ti on
the lhs matches the root of the source tree, and ii) the

Figure 1: (s) Constituent tree of a question; (t) executable
meaning representation; r1 - r5 are typical transducer rules ex-

tracted by our algorithm, where q is a generic state, pred and

bridge are predicate and bridged entity back-off states.

state q of the rule is the initial state of the transducer.
An incipient target tree is created by copying the rhs
of the rule. Then, the transducer recursively and in-
dependently visits the subtrees of the source tree at
the lhs variable positions of the rule from their new
states, and copies the results into the same variable
on the target tree.

In Figure 1, the sequential application of rules r1
to r5 is a derivation that transforms the question s
into the query t. For example, rule r1 consumes a
tree fragment of s (e.g. “how”, “many”, “WRB”,
etc.) and produces a tree fragment with terminals
(“COUNT”, x1, x2) and non-terminals (“ID”) with

14



a specific structure. Rules r2 and r3 only consume
but do not produce symbols (other than variables).
The rhs of rules are target tree fragments that con-
nect to each other at the frontier nodes (those with
variables). Rules r4 and r5 are terminal rules, where
r4 produces the predicate Team and rule r5 pro-
duces the entity Uefa and a disambiguating predi-
cate League that has no lexical support on the source
side, similarly to the role that bridging predicates
play in Berant et al. (2013).

Given a corpus of source and target tree pairs, the
learning stage aims to obtain rules such as r1−r5 in
Figure 1 and their associated probabilities or scores.
We discuss our novel approach to rule extraction in
Section 5. For the assignment of rule scores, we
adopt the latent variable averaged structured percep-
tron, a discriminative procedure similar to Tsochan-
taridis et al. (2005) and Cohn and Lapata (2009).
Here, we instantiate feature values f for every rule,
and reward the weights w of rules that participate in
a derivation (latent variable) that transforms a train-
ing source tree into a meaning representation that
retrieves the correct answer.

At decoding stage, rule scores can be predicted as
s = w · f . However, we cannot expect to have ex-
tracted all necessary rules at the training stage given
the small training data and large-scale KB. For that
reason, we propose in Section 4 a novel rule back-off
scheme to alleviate coverage problems.

4 Back-off rules

As an illustrative example, consider the question
“how many teams participate in the nba”, and the
rules r1 to r5 in Figure 1. When the transducer at-
tempts to transform the noun phrase (NP (DT the)
(NN nba)), no rule’s lhs matches it. However, since
the transducer is at state bridge (as specified by the
rhs of r3), it should be able to produce a list of
bridged entities, among which the target subtree (ID
League NBA) will be hopefully included. Thus, the
following rule should be created for the occasion:

This mechanism produces rules “on-the-fly”, allow-
ing us to compensate low rule coverage by consum-
ing and producing tree fragments that were not nec-

essarily observed in the training data.
Back-off rules are produced when the transducer

is at a back-off state qb ∈ Qb ⊂ Q, similarly as the
back-off mechanisms in finite-state language models
where we produce estimates (probabilities) of input
structures (sequences) under less conditioning. In
our scheme, a back-off state (or function) qb pro-
duces estimates that are target structures t2 ∈ T∆
with score s ∈ R, given some information of the
source tree fragment t1 ∈ TΣ. That is, a func-
tion qb : TΣ → {(T∆,R), . . .}. In our QA appli-
cation, we only use the leaves of the input subtree
t1 and use lexicons or entity/predicate linkers to re-
trieve KB entities, KB relations or a compound of
a disambiguating relation and an entity from back-
off states ent, pred and bridge, respectively. Other
back-off functions would transliterate the leaves of
the input tree in machine translation, or produce syn-
onyms/hypernyms in a paraphrasing application.

We associate a score s to these newly created
rules, which we learn to predict using the discrim-
inative training procedure suggested by Tsochan-
taridis et al. (2005), as described in Section 3.

Back-off rules are then constructed on-demand as
qb.t1

s→ t2, and the discrete set of rules R is aug-
mented with them. It remains now to recognize
those back-off states when inducing tree transducer
grammars, which is covered in Section 5.1.

5 Rule Extraction

Given a pair of trees, our rule extraction algorithm
finds a tree mapping that implicitly describes the
rules that transform a source into a target tree. In the
search of the best mapping, we need to explore the
space of edit operations, which are substitutions of
source by target tree fragments. We define cost func-
tions for these edit operations, and formulate the tree
mapping as a cost minimization problem. Whereas
our tree mapping algorithm and back-off scheme are
generic and can be used in any tree transformation
task, cost functions depend on the application.

5.1 Cost functions

In general, cost functions are defined over edit op-
erations, which are pairs of source and target tree
fragments, cost : TΣ(X ) × T∆(Q.X ) → R≥0, and
they are equivalent to feature functions. Some cost

15



functions are defined over all pairs of tree fragments.
For this QA application, these are:

csize(t1, t2) = |nodes(t1)|2 + |nodes(t2)|2

which acts as a tree size regularizer, returning a cost
quadratic to the size of the tree fragments, thus en-
couraging small rules. The cost function ccount as-
signs zero cost if (i) “how” and “many” appear in t1,
and (ii) “COUNT” appears in t2. If only either (i) or
(ii), the cost is a positive constant. Similarly, other
operators (max, min, argmax, etc.) could be recog-
nized, but this dataset did not require them.

Other cost functions only apply to some pairs of
tree fragments. These are the back-off functions de-
scribed in Section 4, but instead of returning scores
for every target tree fragment, they return a cost, e.g.
cent : TΣ × T∆ → R≥0. An ensemble will produce
up to three different costs for every pair of tree frag-
ments, depending on what back-off functions were
triggered. In the case of the entity cost function:

γent(t1, t2) = λ1 · csize(t1, t2)
+ λ2 · ccount(t1, t2)
+ λ3 · cent(t1, t2)

(1)

where λi ∈ R≥0 are scaling factors. In the search
of the lowest-cost mapping, the labels of the cost
functions that are derived from the back-off func-
tions (e.g. γent, γpred) are memorized for the pairs
(t1, t2) for which they were defined and for which
they outputted a cost. These labels are then used as
back-off rule state names when constructing rules.

5.2 Tree Mapping: Optimization Problem
Intuitively, the cost of mapping a source node ns to
a target node nt is equal to the cost of transforming
a tree fragment TΣ(X ) rooted at node ns into a tree
fragment T∆(Q.X ) rooted at node nt, plus the sum
of costs of mapping the frontier nodes rooted at the
variables. In order to formalize our tree mapping,
we need a more precise definition of a tree frag-
ment where the locations of variables X are spec-
ified by paths. The notation to specify subtrees is
taken from (Graehl and Knight, 2004), and we in-
troduce the ⊥ operator for convenience.

A path p is a tuple, equivalent to a Gorn address,
that uniquely identifies the node of a tree by speci-
fying the sequence of child indices to the node from

the root. In the tree s of Figure 1, the path to the
VP node is (1, 0), whereas in t, the path to League
is (1, 1, 0). The path p = () refers to the root of
a tree. We denote by s ↓ p the subtree of tree s
that is rooted at path p and that has no variables. In
Figure 1, the left-hand-side (lhs) of r5 is the sub-
tree s ↓ (1, 0, 1, 1). In order to introduce variables,
we generalize the notion of subtree into a tree pat-
tern s ↓ p ⊥ {p1, . . . , pn}, where n variables re-
place subtrees s ↓ pi at subpaths pi ∈ {p1, . . . , pn}.
For example, the lhs of r1 can be represented with
the tree pattern s ↓ () ⊥ {(0, 1), (1)}, and r2 with
s ↓ (1) ⊥ {(1, 0, 1)}. Note that the order of sub-
paths {p1, . . . , pn} matters. A tree pattern with no
subpaths s ↓ p ⊥ {} is simply a subtree s ↓ p, such
as the lhs of rules r4 and r5; a tree pattern with only
one subpath equal to its path s ↓ p ⊥ {p} is a single
variable, such as the rhs of rules r2 and r3. Note that
in s ↓ p ⊥ {p1, . . . , pn}, all paths pi to variables are
prefixed by p, and that no variables are descendants
of any other variable in the same tree pattern. In
other words, p = {p1, . . . , pn} are disjoint subpaths
given p, where p denotes a list of paths.

We can now formalize the tree mapping
algorithm as an optimization problem. Let
γ (s ↓ ps ⊥ p, t ↓ pt ⊥ p′) be the cost to transform
a source into a target tree pattern, as defined in
Equation 1. To transform s ↓ ps into t ↓ pt, we
need to find the best combination of source sub-
trees rooted at {p1, . . . , pn} that can be transformed
at minimum cost to the best combination of target
subtrees at {p′1, . . . , p′n}. The transformation cost of
a certain tree pattern s ↓ ps ⊥ {p1, . . . , pn} into
t ↓ pt ⊥ {p′1, . . . , p′n} is equal to the cost of trans-
forming the source tree pattern into the target tree
pattern, plus the minimum cost to transform s ↓ pi
into t ↓ p′i, for i ≥ 1. That is:

C (s ↓ ps, t ↓ pt) =
min
p,p′
{γ
(
s ↓ ps ⊥ p, t ↓ pt ⊥ p′

)
+

|p|∑

i=1

C
(
s ↓ pi, t ↓ p′i

)
} (2)

subject to |p| = |p′|, that is, source and tar-
get tree patterns having the same number of vari-
ables. Then, the cost of transforming the source
into the target tree would be given by the expression

16



C (s ↓ (), t ↓ ()). Since we are only interested in the
pairs of source and target tree patterns that lead to
the minimum cost, we keep track of subpaths p and
p′ of tree pattern pairs that minimize the cost.

5.3 Algorithm

5.3.1 Overview
This problem can be solved for small depths of

tree patterns and a small number of variables by stor-
ing intermediate results in the computation of Eq. 2.
However, an exact implementation needs to enumer-
ate all pairs of source and target disjoint subpaths (p
and p′), which has a computational complexity that
grows combinatorially with |p| (variable permuta-
tions), and exponentially with the number of descen-
dant nodes of ps and pt (powerset of variables).

Instead, we use a beam search algorithm (see Al-
gorithm 1)7 that constructs source and target disjoint
paths (p and p′) hierarchically (function GENER-
ATEDISJOINT) in a bottom-up order, for any given
path pair (ps, pt). First, n-best solutions (pairs of
disjoint paths) are computed for children; then those
partial solutions are combined into their parent us-
ing the cross-product. Solutions (with their associ-
ated cost) for every pair of paths (ps, pt) are stored in
a weighted hypergraph, from which we can extract
n-best derivations (sequences of rules). In the pseu-
docode, we use a helper function, paths(s ↓ ps),
which denotes the list of subtree paths in bottom-up
order: from the leaves up to ps (including the latter).

5.3.2 Detailed Description
For a certain path pair (ps, pt), there are three

cases. The first case (line 34-35) considers a pair of
empty disjoint subpaths (p,p′) = ({}, {}), where
the cost c of transforming s ↓ ps ⊥ {} into t ↓ pt ⊥
{} is evaluated and the empty disjoint subpaths are
added to the priority queue P , indexed with ps. Such
indexing is useful to retrieve the n-best pairs of dis-
joint subpaths accumulated at every tree node.

The second case (line 28 to 31) evaluates the
cost of transforming single-variable tree patterns:
s ↓ ps ⊥ {pc} into t ↓ pt ⊥ {p′c}. In this
case, variables substitute entire subtrees rooted at
paths pc and p′c on the source and target tree pat-
terns, respectively. Note that pc ranges over all node

7https://github.com/pasmargo/t2t-qa

Algorithm 1 Extraction of optimal sequence of rules
to transform a source s into a target tree t.
Input: Trees s and t, and ensemble of cost functions γ.
Output: Sequence of optimal rules for s⇒∗ t.

1: let H = (V,E) be a hypergraph of solutions with
V ← {} vertices and E ← {} hyperedges.

2: for (ps, pt) ∈ paths(s)× paths(t) do
3: add vertex v = (ps, pt) to V
4: PP ← GENERATEDISJOINT(s ↓ ps, t ↓ pt, γ)
5: for (p,p′) ∈ PP do
6: . Get cost of tree pattern pair.
7: c← γ (s ↓ ps ⊥ p, t ↓ pt ⊥ p′)
8: add edge (ps, pt)

c→ (p,p′) to E
9: end for

10: end for
11: return HYPERGRAPHSEARCH(H)

12: function GENERATEDISJOINT(s ↓ ps, t ↓ pt, γ)
13: P ← {} a priority queue of partial disjoint paths.
14: for every pc ∈ paths(s ↓ ps) do
15: . Costs when variables combined from children.
16: for every pic immediate child of pc (if any) do
17: . Retrieve n-best subpaths p and p′ from pic.
18: C ← arg minn(p,p′){c | (pic,p,p′, c) ∈ P}
19: . Combine subpaths with those accumulated
20: . from previous siblings and stored at path pc.
21: A← arg minn(p,p′){c | (pc,p,p′, c) ∈ P}
22: for (p,p′) ∈ (C ∪ (C.A)) do
23: c← γ (s ↓ ps ⊥ p, t ↓ pt ⊥ p′)
24: add (pc,p,p′, c) to priority queue P
25: end for
26: end for
27: . Cost of tree patterns with one variable.
28: for every p′c ∈ paths(t ↓ pt) do
29: c← γ (s ↓ ps ⊥ {pc}, t ↓ pt ⊥ {p′c})
30: add (pc, {pc}, {p′c}, c) to priority queue P
31: end for
32: end for
33: . Cost of tree patterns with no variables.
34: c← γ (s ↓ ps ⊥ {}, t ↓ pt ⊥ {})
35: add (ps, {}, {}, c) to priority queue P
36: return arg minn(p,p′){c | (ps,p,p′, c) ∈ P}
37: end function

addresses that are descendant of ps (including ps),
and similarly for p′c. The pairs of disjoint subpaths
(p,p′) = ({pc}, {p′c}) are added into the priority
queue, indexed by pc.

The third case (line 16 to 26) performs the com-
bination of subpaths hierarchically from children to
parents, and incrementally across children. For ev-
ery path pc ∈ paths(s ↓ ps), it visits its imme-

17



diate children pic one by one, and retrieves into C
the n-best disjoint subpaths (line 18) that have al-
ready been obtained during previous iterations for
pic. Then, we retrieve into A the n-best disjoint sub-
paths indexed at pc, which is a list of the best sub-
paths that were combined from previous immediate
children (the list is empty if this is the first immedi-
ate child that we visit). The cross-product of disjoint
subpaths in C and A, that is C.A, is then evaluated
and the best combinations are stored in the priority
queue indexed at pc.

As an example of a cross-product between two
lists C and A of pairs of disjoint paths, let C =
{(p1,p1′), (p2,p2′)} and A = {(p3,p3′)}. Then
the cross-product C.A would be:

C.A = {(p1 · p3,p′1 · p′3), (p2 · p3,p′2 · p′3)}

where p1 · p3 = {(0, 1), (0, 2), (0, 3), (0, 4)} if
p1 = {(0, 1), (0, 2)} and p3 = {(0, 3), (0, 4)}. At
this stage, subpaths pi or p′i that are not disjoint are
discarded, together with disjoint paths that produce
tree patterns with depth larger than a certain user-
defined threshold, or whose number of subpaths is
larger than the number of variables allowed.

In line 24, the disjoint subpaths of C (in addition
to their cross-product C.A) are also evaluated and
added to the priority queue indexed by pc, to propa-
gate upwards in the hierarchy of solutions the deci-
sion of not combining disjoint subpaths.

Finally, GENERATEDISJOINT returns the n-best
pairs of disjoint subpaths of minimum cost (p, p′)
that accumulated in the priority queue P for path ps.

5.3.3 Other Considerations
The n-best source and target pairs of disjoint sub-

paths are stored at every pair of source and target
paths (ps, pt) (lines 2-10), forming a hypergraph, as
in Figure 2. Then, with a hypergraph search (Huang
and Chiang, 2007) we can retrieve at least n-best
sequences of rules (derivations) that transform the
source into the target tree (line 11).

To maintain diversity of partial disjoint subpaths,
we divide P into a matrix of buckets with as many
rows and columns as the number of non-variable ter-
minals of the source and target tree patterns, trading
memory for more effective search (Koehn, 2015).
This operation is implicit in lines 24, 30 and 35.

Figure 2: Hypergraph with 2-best pairs of disjoint subpaths for
(ps, pt). Vertices are pairs of source and target paths. Hyper-

edges are pairs of tree patterns. The hyperedge with cost .3 de-

notes the pair s ↓ ps ⊥ {p1, p2, p3} and t ↓ pt ⊥ {p′1, p′2, p′3}.
The one with cost .5, s ↓ ps ⊥ {p1, p3} and t ↓ pt ⊥ {p′2, p′3}.

6 Experiments

6.1 Experiment Settings
Data The training data is a corpus of questions an-
notated with their logical forms that can be exe-
cuted on Freebase to obtain a precise answer. For
an unseen set of questions, the task is to obtain au-
tomatically their logical forms and retrieve the cor-
rect answer. Our objective is to evaluate the gen-
eralization capabilities of a transducer induced us-
ing our rule extraction on an unseen open-domain
test set. We parsed questions from FREE917 into
source constituent trees using the Stanford caseless
model (Klein and Manning, 2003). Target con-
stituent (meaning) representations were obtained by
a simple heuristic conversion from the λ−DCS ex-
pressions released by Berant et al. (2013). We evalu-
ate on the same training and testing split as in Berant
et al. (2013). Tree pairs (2.9%) for which the gold
executable meaning representation did not retrieve
valid results were filtered out.

Baselines We compared to two baselines. The
first one is SEMPRE (Berant et al., 2013), a state-
of-the-art semantic parser that uses a target language
grammar to over-generate trees, and a log-linear
model to estimate the parameters that guide the de-
coder towards trees that generate correct answers.
For FREE917, SEMPRE uses a manually-created
entity lexicon released by (Cai and Yates, 2013), but
an automatically generated predicate lexicon. In-

18



stead, our system and the second baseline use manu-
ally created entity and predicate lexicons, where the
latter was created by selecting all words from every
question that relate to the target predicate. For ex-
ample, for the question “what olympics has egypt
participated in”, we created an entry that maps the
discontinuous phrase “olympics participated in” to
the predicate OlympicsParticipatedIn.

The second baseline is a tree-to-tree transducer
whose rules are extracted using a straightforward
adaptation of the GHKM algorithm (Galley et al.,
2004) for pairs of trees. Word-to-concept align-
ments are extracted using three different strategies:
i) ghkm-g uses the IBM models (Brown et al., 1993)
as implemented in GIZA++ (Och and Ney, 2003),
ii) ghkm-m maps KB concepts (target leaves) to as
many source words as present in the entity/predicate
lexicons, and iii) ghkm-c maps KB concepts as in ii)
but only retaining the longest contiguous sequence
of source words (or right-most sequence if there is
a tie). Bridging predicates are assumed when a KB
concept does not align (according to the lexicon) to
any source word. Finally, rule state names are set
according to the mechanism described in Section 5.

Our ent, pred and bridge cost/back-off functions
assign a low cost (or high score) to source and target
tree patterns with no variables whose leaves appear
in either the entity or the predicate lexicons. Scal-
ing factors λi (see Eq. 1) were subjectively tuned on
20 training examples. When used as back-off func-
tions, they generate as many rhs as entities or pred-
icates can be retrieved from the lexicons by at least
one of the words in the source tree pattern. Bridging
predicates are dispreferred by adding an extra con-
stant cost. At back-off, this score function generates
a variable predicate, acting as a wildcard in Sparql.

Our system t2t For the rule extraction, we use a
beam size of 10, and we output 100 derivations for
every tree pair. We do not impose any limit in the
depth of lhs or rhs, or in the number of variables.
To increase the coverage of our rules, we produce
deleting tree transducers by replacing fully lexical-
ized branches that are directly attached to the root of
a lhs with a deleting variable.

For the parameter estimation, we used 3 iterations
of the latent variable averaged structured perceptron,
where the number of iterations was selected on 20%
of held-out training data. To assess the equality be-

tween the gold and the decoded tree, we compare
their denotations. The features for the discrimina-
tive training were the lhs and rhs roots, the number
of variables, deleting variables and leaves, the pres-
ence of entities or predicates in the rhs, the rule state
and children states, and several measures of charac-
ter overlaps between the leaves of the source and in-
formation associated to leaves in target tree patterns.

For decoding, we used standard techniques
(Graehl and Knight, 2004) to constrain and prune
weighted regular tree grammars given a tree trans-
ducer and a source tree, and used the cube-growing
algorithm to generate 10, 000 derivations, converted
them to Sparql queries, and retained those that were
valid (either syntactically correct or that retrieved
any result). We compute the accuracy of the system
as the percentage of questions for which the 1-best
output tree retrieves the correct answer, and the cov-
erage as the percentage for which the correct answer
is within the 10, 000 best derivations. The average
rule extraction time per tree pair when using beam
size 1 was 0.46 seconds (median 0.35, maximum
2.94 seconds). When using beam size 10, the aver-
age was 4.7 seconds (median 2.02, maximum 104.4
seconds), which gives us a glimpse of how the beam
size influences the computational complexity for the
typical tree size of FREE917 questions.

6.2 Results

Results are in Table 1. Note that although we
compare our results to those obtained with SEM-
PRE (Berant et al., 2013), the systems cannot really
be compared since Berant et al. (2013) did not have
access to a manually created lexicon of predicates.
When comparing the average number of entity and
predicate rules that the back-off functions generate,
we see that the number of predicate rules is much
larger, implying a higher ambiguity. Despite this,
our base system still produces promising results in
terms of accuracy and coverage.

We also carried out several ablation experiments
to investigate what are the characteristics of the sys-
tem that contribute the most to the accuracy: In no
nbest, we only extract one sequence of rules that
transform a source into a target tree. In no del, we
do not introduce deleting variables. In beam 1, we
use beam size 1 in rule extraction. In no size, no
tree size regularizer cost function is used. And in

19



Systems Acc. Cov. # Preds. # Ents. # Rules
SEMPRE .62 − − − −
ghkm-c .49 .80 155 14 384
ghkm-m .48 .77 147 14 399
ghkm-g .08 .57 102 5 135
t2t .64 .78 187 19 437
t2t-e .69 .85 191 20 430
no del .64 .78 187 19 437
no size .59 .78 195 19 483
no nbest .58 .70 93 5 128
beam 1 .53 .65 84 5 112
no back .00 .01 0 0 175
train-600 .62 .78 187 19 429
train-500 .61 .77 184 19 413
train-400 .62 .75 178 19 390
train-300 .59 .75 177 18 363
train-200 .52 .74 169 17 317
train-100 .52 .67 138 14 317

Table 1: Accuracy and coverage results; average number of
predicate rules, entity rules and all rules per input tree.

no back, no rule back-offs are used. As we see,
removing the back-off rule capabilities is critical in
this setting and makes the QA task unfeasible. We
also studied the impact of the size of the training
data in the generalization of our system, by train-
ing the system in {100, 200, . . . , 600} examples. We
found that the accuracy saturates at only 400 train-
ing instances, which might be advantageous in tasks
where training resources are scarce. Finally, in or-
der to estimate the upper bound in the coverage and
accuracy of our approach on FREE917, we also run
our pipeline t2t-e with a refined version of Cai and
Yates (2013)’s entity lexicon, where 65 missing en-
tities are added (7.8% of the total). We can observe
a significant increase in the accuracy and coverage
of the system, suggesting that the bottleneck may lie
in the entity/predicate linking procedures.

7 Future Work

One step further in the generalization of the rule ex-
traction is to remove the necessity of explicitly pro-
viding cost functions such as word-to-word hard-
alignments or costs between tree fragments. This
would allow us to remove the bias introduced by en-
gineered cost functions and to obtain rules that are
globally optimal. In this setup, the parameters of the
cost functions are to be learned with the objective
to maximize the likelihood on the training data or

a downstream application performance. However,
since rules (or tree mappings) would become hid-
den variables, this generalized rule extraction may
require faster methods to enumerate plausible rules.
Another extension would be to make the rule extrac-
tion more robust against parsing errors, using pairs
of forests instead of pairs of trees, similarly as in Liu
et al. (2009).

Regarding the QA application, there are two nat-
ural extensions that we want to address, namely to
develop general and automatic entity and predicate
linking mechanisms for large knowledge bases, and
to test our approach in datasets that require higher
levels of compositionality such as the QALD chal-
lenges (Unger et al., 2015) or those datasets pro-
duced by Wang et al. (2015).

8 Conclusion

We proposed to induce tree to tree transducers us-
ing a rule extraction algorithm that uses cost func-
tions over pairs of tree fragments (instead of word-
alignments), which increases the applicability of
these models. Some cost functions may act as
rule back-offs, generating new rhs given unseen lhs,
thus producing transducer rules “on-the-fly”. The
scores of these rules are obtained on demand using
a discriminative training procedure that estimates
weights for rule features. This strategy was useful to
compensate the lack of rule coverage when inducing
tree transducers from small tree corpora.

As a proof-of-concept, we tested the tree trans-
ducer induced with our algorithm on a QA task over
a large KB, a domain in which tree transducers have
not been effective before. In this task, lexicon map-
pings were naturally introduced as cost functions
and rule back-offs, without loss of generality. De-
spite using a manually created lexicon of predicates,
we showed a high accuracy and coverage of non-
final rules, which are promising results.

Acknowledgments

This paper is based on results obtained from a
project commissioned by the New Energy and
Industrial Technology Development Organization
(NEDO), and is also supported by JSPS KAKENHI
Grant Number 16K16111. We thank Yoshimasa
Tsuruoka, Yo Ehara and the anonymous reviewers
for their helpful comments.

20



References
Jonathan Berant, Andrew Chou, Roy Frostig, and Percy

Liang. 2013. Semantic parsing on Freebase from
question-answer pairs. In Proceedings of the 2013
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 1533–1544, Seattle, Wash-
ington, USA, October. Association for Computational
Linguistics.

Philip Bille. 2005. A survey on tree edit distance
and related problems. Theoretical Computer Science,
337(1–3):217 – 239.

Peter F. Brown, Vincent J. Della Pietra, Stephen A. Della
Pietra, and Robert L. Mercer. 1993. The mathematics
of statistical machine translation: Parameter estima-
tion. Computational linguistics, 19(2):263–311.

Qingqing Cai and Alexander Yates. 2013. Large-scale
semantic parsing via schema matching and lexicon ex-
tension. In Proceedings of the 51st Annual Meeting
of the Association for Computational Linguistics (Vol-
ume 1: Long Papers), pages 423–433, Sofia, Bulgaria,
August. Association for Computational Linguistics.

Trevor Anthony Cohn and Mirella Lapata. 2009. Sen-
tence compression as tree transduction. Journal of Ar-
tificial Intelligence Research, 34:637–674.

Jason Eisner. 2003. Learning non-isomorphic tree map-
pings for machine translation. In Proceedings of the
41st Annual Meeting on Association for Computa-
tional Linguistics - Volume 2, ACL ’03, pages 205–
208, Stroudsburg, PA, USA. Association for Compu-
tational Linguistics.

Michel Galley, Mark Hopkins, Kevin Knight, and Daniel
Marcu. 2004. What’s in a translation rule. In HLT-
NAACL’2004: Main Proceedings, pages 273–280.

Ruifang Ge and Raymond J. Mooney. 2005. A statisti-
cal semantic parser that integrates syntax and seman-
tics. In Proceedings of the Ninth Conference on Com-
putational Natural Language Learning, CONLL ’05,
pages 9–16, Stroudsburg, PA, USA. Association for
Computational Linguistics.

Jonathan Graehl and Kevin Knight. 2004. Training
tree transducers. In Daniel Marcu Susan Dumais
and Salim Roukos, editors, HLT-NAACL 2004: Main
Proceedings, pages 105–112, Boston, Massachusetts,
USA, May 2 - May 7. Association for Computational
Linguistics.

Liang Huang and David Chiang. 2007. Forest rescoring:
Faster decoding with integrated language models. In
Proceedings of the 45th Annual Meeting of the Asso-
ciation of Computational Linguistics, pages 144–151,
Prague, Czech Republic, June. Association for Com-
putational Linguistics.

Bevan Keeley Jones, Mark Johnson, and Sharon Goldwa-
ter. 2012. Semantic parsing with bayesian tree trans-
ducers. In Proceedings of the 50th Annual Meeting of

the Association for Computational Linguistics: Long
Papers - Volume 1, ACL ’12, pages 488–496, Strouds-
burg, PA, USA. Association for Computational Lin-
guistics.

Dan Klein and Christopher D. Manning. 2003. Accu-
rate unlexicalized parsing. In Proceedings of the 41st
Annual Meeting of the Association for Computational
Linguistics, pages 423–430, Sapporo, Japan, July. As-
sociation for Computational Linguistics.

Kevin Knight and Jonathan Graehl. 2005. An overview
of probabilistic tree transducers for natural language
processing. In Alexander Gelbukh, editor, Compu-
tational Linguistics and Intelligent Text Processing,
volume 3406 of Lecture Notes in Computer Science,
pages 1–24. Springer Berlin Heidelberg.

Philipp Koehn. 2015. Moses Manual.
Peng Li, Yang Liu, and Maosong Sun. 2013. An ex-

tended GHKM algorithm for inducing Lambda-SCFG.
pages 605–611.

Percy Liang. 2013. Lambda dependency-based compo-
sitional semantics. CoRR, abs/1309.4408.

Xiao Ling, Sameer Singh, and Daniel Weld. 2015. De-
sign challenges for entity linking. Transactions of
the Association for Computational Linguistics, 3:315–
328.

Yang Liu, Yajuan Lü, and Qun Liu. 2009. Improving
tree-to-tree translation with packed forests. In Pro-
ceedings of the Joint Conference of the 47th Annual
Meeting of the ACL and the 4th International Joint
Conference on Natural Language Processing of the
AFNLP, pages 558–566, Suntec, Singapore, August.
Association for Computational Linguistics.

Andreas Maletti, Jonathan Graehl, Mark Hopkins, and
Kevin Knight. 2009. The power of extended top-
down tree transducers. SIAM Journal on Computing,
39(2):410–430.

Franz Josef Och and Hermann Ney. 2003. A system-
atic comparison of various statistical alignment mod-
els. Computational Linguistics, 29(1):19–51.

William C. Rounds. 1970. Mappings and grammars on
trees. Mathematical systems theory, 4(3):257–287.

Kuo-Chung Tai. 1979. The tree-to-tree correction prob-
lem. J. ACM, 26(3):422–433, July.

James W. Thatcher. 1970. Generalized sequential ma-
chine maps. Journal of Computer and System Sci-
ences, 4(4):339 – 367.

Ioannis Tsochantaridis, Thorsten Joachims, Thomas Hof-
mann, and Yasemin Altun. 2005. Large margin meth-
ods for structured and interdependent output variables.
In Journal of Machine Learning Research, volume 6,
pages 1453–1484.

21



Christina Unger, Corina Forascu, Vanessa Lopez, Axel-
Cyrille Ngonga Ngomo, Elena Cabrio, Philipp Cimi-
ano, and Sebastian Walter. 2015. Question Answer-
ing over Linked Data (QALD-5). In Linda Cappellato,
Nicola Ferro, Gareth Jones, and Eric San Juan, editors,
Working Notes of CLEF 2015 - Conference and Labs
of the Evaluation forum, volume 1391. Working Notes
of CLEF 2015 - Conference and Labs of the Evalua-
tion forum.

Yushi Wang, Jonathan Berant, and Percy Liang. 2015.
Building a semantic parser overnight. In Proceed-
ings of the 53rd Annual Meeting of the Association
for Computational Linguistics and the 7th Interna-
tional Joint Conference on Natural Language Process-
ing (Volume 1: Long Papers), pages 1332–1342, Bei-
jing, China, July. Association for Computational Lin-
guistics.

Yuk Wah Wong and Raymond J. Mooney. 2006. Learn-
ing for semantic parsing with statistical machine trans-
lation. In Proceedings of the Main Conference on Hu-
man Language Technology Conference of the North
American Chapter of the Association of Computa-
tional Linguistics, HLT-NAACL ’06, pages 439–446,
Stroudsburg, PA, USA. Association for Computational
Linguistics.

Dekai Wu. 2005. Recognizing paraphrases and textual
entailment using inversion transduction grammars. In
Proceedings of the ACL Workshop on Empirical Mod-
eling of Semantic Equivalence and Entailment, EM-
SEE ’05, pages 25–30, Stroudsburg, PA, USA. Asso-
ciation for Computational Linguistics.

Kaizhong Zhang and Dennis Shasha. 1989. Simple
fast algorithms for the editing distance between trees
and related problems. SIAM Journal on Computing,
18(6):1245–1262.

22


