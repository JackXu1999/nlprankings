



















































Sentiment Classification via a Response Recalibration Framework


Proceedings of the 6th Workshop on Computational Approaches to Subjectivity, Sentiment and Social Media Analysis (WASSA 2015), pages 175–180,
Lisboa, Portugal, 17 September, 2015. c©2015 Association for Computational Linguistics.

Sentiment Classification via a Response Recalibration Framework

Phillip Smith & Mark Lee
School of Computer Science,
University of Birmingham,

Birmingham,
UK

Abstract

Probabilistic learning models have the
ability to be calibrated to improve the per-
formance of tasks such as sentiment clas-
sification. In this paper, we introduce a
framework for sentiment classification that
enables classifier recalibration given the
presence of related, context-bearing doc-
uments. We investigate the use of prob-
abilistic thresholding and document simi-
larity based recalibration methods to yield
classifier improvements. We demonstrate
the performance of our proposed recalibra-
tion methods on a dataset of online clinical
reviews from the patient feedback domain
that have adjoining management responses
that yield sentiment bearing information.
Experimental results show the proposed
recalibration methods outperform uncali-
brated supervised machine learning mod-
els trained for sentiment analysis, and
yield significant improvements over a ro-
bust baseline.

1 Introduction

Probabilistic classifiers are a typically used
method for the classification of documents by the
sentiment they convey (Pang et al., 2002). Given
an unlabelled document, a trained probabilistic
model is able to determine an appropriate labelling
in relation to a given confidence for the proposed
labelling. In the two-class sentiment classifica-
tion problem a labelling confidence that is greater
than 0.5 will lead to a particular sentiment being
attached to the input document. However, it is
questionable whether a classifier confidence out-
put of 0.51 is sufficiently suitable for the applica-
tion of any given label. This low confidence poses
a problem for sentiment classification, and can of-
ten lead to documents being labelled incorrectly,
to the detriment of a sentiment analysis system.

A low classifier confidence in sentiment analy-
sis may be produced due to inherent linguistic dif-
ficulties that plague systems developed for natu-
ral language processing. For example, documents
where a sentiment is conveyed implicitly, ambigu-
ously, or in a sarcastic manner can cause prob-
lems for machine learning approaches to senti-
ment classification. Methods have been proposed
to deal with such facets of language in sentiment
analysis. These tend to focus on hand-crafted lexi-
cons (Balahur et al., 2011) or intra-document con-
textual cues (Greene and Resnik, 2009) to disam-
biguate the polarity of a document. We propose a
method that takes into consideration related doc-
uments in the classification process, and duly ad-
justs classification output using a sentiment recal-
ibration framework.

Our proposed method takes into account exter-
nal but relevant documents during the sentiment
recalibration process. We use these documents to
make adjustments to classifier outputs, in an ad-
justment and correction phase. To our knowledge,
this is the first work in sentiment classification to
attempt the recalibration of a sentiment classifier
given relevant documents. We attribute this ability
to the dataset used for our experiments.

The remainder of the paper is structured as fol-
lows: in Section 2 we describe the data and an-
notation experiments devised to observe the rele-
vance of a response to a comment. Section 3 then
outlines the motivation for sentiment recalibration,
and section 4 details our proposed methodology
for the construction of a calibration framework for
sentiment classification. Section 5 gives the base-
line for evaluation. Section 6 details the results
of experimentation with the framework, and dis-
cusses the implications. Section 7 describes re-
lated work and we conclude and give direction for
future work in Section 8.

175



2 Data and Annotation

The monologic nature of current datasets for eval-
uating sentiment classifiers, while valuable to the
development of the field, are not applicable to our
proposed recalibration framework. Most relevant
to our work is the forum post data set (Murakami
and Raymond, 2010). However, this is too general
for the purposes we are examining due to deviation
in discourse topic. Therefore we have developed a
dataset for sentiment classification with the related
documents that are required for the response recal-
ibration framework. We use patient feedback data
provided by the National Health Service (NHS).
This has been used before (Smith and Lee, 2014),
however author responses were not a feature ex-
amined in this work. In this dataset, each feed-
back item consists of a patient’s comment and a re-
sponse from the NHS. Unlike other online reviews
used to investigate the potency of sentiment clas-
sification algorithms, this dataset does not contain
a user ranking or score to accompany their com-
ment. An annotation phase is therefore required in
order to use the documents as an evaluation dataset
for our algorithms.

We annotate a subset of 4,059 comments
and their related responses for their sentiments
expressed and responded to, at the document
level. The comments contained 254,611, of
which 10,325 were unique. Responses contained
403,315 words, of which 9,115 were unique. De-
spite a larger average document size, the response
vocabulary was smaller that the comment vocabu-
lary. This indicates that the responses given were
constrained in nature. An initial pass of the data
highlighted that reviews were not merely binary,
but often weighed up mixed sentiments before giv-
ing a conclusion. Due to this observation, we ini-
tially annotate the data with a five-class annotation
scheme. This includes, neutral, mixed-positive
and mixed-negative categories. The mixed cate-
gories denote that varying sentiments are present
in the document, but one sentiment is more salient
than the other.

Results of this annotation are presented in ta-
ble 1. Given the annotations, we calculate inter-
category agreement using Cohen’s kappa coeffi-
cient. Between all categories κ = 0.4294, and
observing positive and negative only κ = 0.761, a
good level of agreement. This agreement is indica-
tive of the level to which the sentiment expressed
in a comment is mirrored and acknowledged in a

SResponse
-2 -1 0 +1 +2

-2 3 139 6 5 12
-1 8 2,022 92 33 117

SComment
0 0 153 25 102 44

+1 4 251 83 671 187
+2 1 68 1 15 17

Table 1: Comment-response sentiment label con-
fusion matrix.

related response. Due to this result, we proceed
with our experiments using only the positive and
negative data.

3 Sentiment Classification Recalibration

This work examines the potential for the out-
come of sentiment classifiers to be recalibrated
given the presence of a related document. In this
work, the response to a sentiment-bearing com-
ment is the related document under consideration.
Typical approaches to recalibration may rely on
the Platt scaling or binning methods. Platt scal-
ing trains a logistic regression model on the out-
put of an SVM classifier, enabling the produc-
tion of posterior classification probabilities (Platt
and others, 1999). Binning is another calibration
method that is particularly effective for classifica-
tion (Zadrozny and Elkan, 2001). Such recalibra-
tion methods focus on statistical methods of recal-
ibrating classifier output. However, when dealing
with related natural language documents, we can
use inferences from the content of the related text
to guide the recalibration process. We therefore
propose the use of the response to recalibrate the
labelling of the initial comment. This takes a re-
sponse directed at a comment, and uses the out-
come of its classification as a starting point for re-
calibration. We discuss in further detail the recali-
bration protocols in the following section.

4 Method

The notion of applying supervised learning meth-
ods to classify related documents (Taskar et al.,
2001; Jensen et al., 2004) and the post-processing
of classification (Benferhat et al., 2014) has been
examined in the literature. Based on this, we pro-
pose three approaches to leverage the acknowl-
edged sentiment in a response in the recalibration
process. The first two methods observe the out-
comes of probabilistic classifiers appointed with

176



the role of classifying comment and response sen-
timent individually. A probabilistic classifier will
output a confidence associated with a particular
label. Given a confidence greater than .5 a label
will be applied. However, a confidence of .51 is
probably of no more use than a random guess in
the two-class sentiment classification task. Given
a complete iteration of the confidence thresholds
at .01 confidence intervals, we examine the clas-
sifier performance. At each interval, given each
instance, where the response label differs to the
comment’s we assign this label to the review. The
third method judges the level of lexical similarity
between the comment and response before making
a judgement regarding recalibration.

4.1 Probabilistic Threshold Calibration
In classification, the probability of labelling a doc-
ument with a certain category is just as impor-
tant as the labelling itself. A classifier may not
be overly confident with its initial labelling, and
so an external but relevant source of information
may help guide and recalibrate the outcome of the
initial calibration. Recalibration methods attempt
to determine at what threshold the labelling would
be most effective. These are typically guided by
a line of best fit related to a posterior probability
(Zadrozny and Elkan, 2001). We propose a recali-
bration framework to examine the effects of recal-
ibrating the threshold. The first approach is to iter-
ate over the probabilities at intervals of 0.01, and
recalibrate the labelling if the confidence of given
labelling is below the threshold. The recalibration
is given through the labelling of the response rela-
tive to the original comment instance. In the pro-
tocol experiments, the confidence of the response
classifier is not observed, only the labelling. The
label is then commuted to the comment.

The second set of experiments observing proba-
bility thresholds imposes the constraint that the re-
sponse classifier must yield a more confident clas-
sification outcome than that of the comment clas-
sifier. Both may exhibit the same sentiment, but in
order to overcome any confusion due to ambigu-
ous or implicit expressions we commute the re-
sponse labelling if and only if the confidence out-
put by the response classifier is higher than that of
the comment classifier.

4.2 Document Similarity
Classifier confidence is just one potential method
of determining cases for instance relabelling

where sentiment classifiers may yield incorrect
classifications. Another method that deserves con-
sideration is determining the level of lexical sim-
ilarity between the comment and response. The
assumption is made that is that if a response is
replying to the content of the original comment,
there will be elements of language reuse in the re-
sponse. Then, the greater the similarity, the more
likely the relative document sentiments are homo-
geneous. We implement the Greedy String Tiling
algorithm (Wise, 1993) as a measure of document
similarity. The algorithm outputs a score between
[0,1], given the level of similarity. As with the pre-
vious experiments with relabelling given a classi-
fier confidence, we take the same approach here.
Our experiment iterates over varying thresholds,
with a 0.01 interval at each step. However, we
do not make any adjustments for classifier confi-
dence, only taking the binary labelling as the pri-
mary label.

4.3 Baseline

Comments in our dataset receive responses that
both acknowledge and concisely respond to the
content of the original message. We identify fea-
tures of these responses that are useful to the sen-
timent classification process. We employ a rule-
based system based on these observations to test
the hypothesis that given the presence of these fea-
tures, the sentiment of the response mirrors that
of the original comment. Using a small set of
regular expressions for frequent word stems we
achieve a recall of .9004. Given the categorisation
of the terms we then classify the sentiment of the
response and compare the labelling to the gold-
standard labelling of the comment. This yields
an accuracy of 0.6634. We also cross-validate
the three classifiers on the dataset to form another
baseline, results of which are shown in Table 2.

Acc. Prec. Recall F1
Comment

NB +1 0. 692 0.502 0.765 0.606
NB -1 0.862 0.659 0.747

MNB +1 0.871 0.784 0.805 0.794
MNB -1 0.911 0.9 0.906
SMO +1 0.856 0.771 0.759 0.765
SMO -1 0.893 0.899 0.896

Table 2: Baseline for sentiment classification (+1
= positive -1 = negative)

177



0.6 0.8 1

0.7

0.8

0.9

Probability

A
cc

ur
ac

y

0 0.5 1

0.7

0.8

0.9

Similarity
0.6 0.8 1

0.7

0.8

0.9

Probability
(a) (b) (c)

NB MNB SVM

Figure 1: Accuracy comparison graphs for our recalibration methods: (a) confidence threshold (b) simi-
larity threshold (c) confidence threshold where Pr(R) > Pr(A).

5 Results and discussion

Results of the probabilistic relabelling experiment
highlight an improvement in classifier accuracy as
the probability threshold increases, contrary to ex-
pectation. We must look to both Figures 1 and 2
in order to understand this result. One would ex-
pect that a classifier outputting a low confidence of
classification would yield a higher accuracy given
the recalibration process. Results highlight the rel-
ative over-confidence of all classifiers when pre-
dicting the class of an instance as the number of
candidates eligible for relabelling is relatively low.
In particular, if we observe the candidates returned
for the NB this classifier does not exhibit a great
variance in confidence, with the majority of la-
bellings being ≥ 0.99. The hubristic nature of the
NB labelling confidence is not beneficial where re-
sults are unable to be recalibrated. Given the total
relabelling scenario for the NB classifier, whereby
all labels from the responses are commuted to an-
notate the comment, there is a significant increase
in classification accuracy of 0.15. In the case of
the other two classifiers, such a scenario leads
to a decrease in performance. This indicates the
poor quality of model initially produced by the
NB learner. This also shows the relative strength
in model building qualities of the MNB and SVM
learners.

The SVM outperforms the NB, but falls short of
MNB performance. Figure 2 indicates that poten-
tially poor relabelling choices contribute to this.
The success ratio drops dramatically as SVM con-
fidence tends towards 1. This trait is similarly
present in both the NB and MNB, also. This is

0.5 0.6 0.7 0.8 0.9 1

0.6

0.8

1

Probability Threshold

R
el

ab
el

lin
g

Su
cc

es
s

R
at

io

NB
MNB
SVM

Figure 2: Relabelling success rate given varying
classifier confidence thresholds.

to be expected however, and suggests that a highly
confident initial judgement by the comment senti-
ment classifier should not be altered.

Contrasting the probabilistic thresholding re-
sults with the similarity threshold relabelling ex-
periments we notice that classifier confidence is
a substantially better calibration method. Given
the similarity thresholding experiments, decreases
in classification accuracy are shown for MNB and
SVM. However for NB, significant gains in classi-
fier accuracy are made. This is in contention with
the results of the probabilistic thresholding exper-
iments, where gains for NB classification were
minimal. We can attribute this to the relaxed string
matching method of the implemented similarity
measure.

Precision and recall results (although not shown
due to space constraints) highlight the dominance

178



of the MNB for in the recalibration framework.
Recall increases over all iterations ≤ 0.99, which
then yields a drop when full relabelling is ap-
plied. Precision peaks at a threshold of 0.78 and
decreases following this. The SVM follows suit,
however performance degrades almost 0.06 when
full relabelling is applied. Precision sees a drop for
the negative class, although the NB exhibits recon-
ciling traits. The SVM suffers from a drop in pre-
cision of 0.05. The positive class however shows
signs of strengthening as the similarity threshold
increases. The MNB remains the dominant clas-
sifier throughout precision comparison. The recall
follows an inverse pattern. As similarity threshold
increases for in our negative class experiments, re-
call gradually increases for MNB and SVM, how-
ever makes gains of 0.22 for the NB. The positive
recall drops however for all classifiers.

The strong response classification experiments
impose a constraint that labelling of the comment
can only be commuted from the response classi-
fication if and only if the response classifier con-
fidence is higher than that of the comment clas-
sification of a given instance. The constraint ap-
pears to have a stabilising quality. Comparing
graphs (a) and (c) in Figure 1, we see a substan-
tially smoother gradient to the curve of the strong
response classification curves in comparison to the
general threshold commutation experiments. We
do not see the drops in performance for the MNB
and SVM classifiers, much to the benefit of the
overall classification, but similarly, we do not see
the steep climb in classifier accuracy demonstrated
by the NB. The precision and recall rates achieved
by strong response classification only mimics that
of the general probabilistic threshold experiments.
Closer comparison of the two shows marginal dif-
ferences.

Results indicate that there is no requirement for
the confidence of the response classifier output to
be higher than that yielded for the correspond-
ing instance classified by the model trained on the
comment data. Comparing Figure 1 with the com-
ment baseline given in Table 2, accuracy results
from the classifiers in the experiments marginally
succeed the baseline for the MNB classifier, but
for the SVM and NB, accuracy is detrimentally
effected. We can conclude that in this case of re-
sponse relabelling the constraint is too strict.

6 Related Work

Work has observed the useful nature of relation-
ships between documents hen classifying stocks
based on the contents of related posts on social
networks (Si et al., 2014) and classifying senti-
ment in posts on online forums based on user re-
lationships, or user stances in online debates (Mu-
rakami and Raymond, 2010).

Work on bagging in sentiment classification is
somewhat related to our work (Dai et al., 2011;
Nguyen et al., 2013). Bagging trains a number
of models on a similar set of training data. Dur-
ing classification, each model then classifies the
given instance, and a voting protocol labels the
instance with the majority label suggested. Our
framework however does not train multiple clas-
sifiers, although the framework could be extended
to incorporate this. Instead, a related document is
used to guide and recalibrate the outcome of the
initial classification. Our method does not suffer
from the issue of low classifier trustworthiness, as
we have shown the results of response only clas-
sification to be reliable in our baselines. The need
for further methods such as stacking is therefore
eliminated.

The use of management response in online re-
views has been examined to empirically determine
the effectiveness in improving a firm’s reputation
(Proserpio and Zervas, 2014). Analysis has shown
moderate improvements where a management re-
sponse was given. This work did not computation-
ally evaluate the content of reviews, however.

7 Conclusion

We have examined the role of sentiment recalibra-
tion in the domain of patient feedback. The pro-
posed classification recalibration method consid-
ered acknowledged sentiment in a comment re-
sponse in order to recalibrate classifier output.
Our framework examined three methods for re-
calibration, two probabilistic and one similarity
based. We found that all classifiers exhibited
improvements in classification performance when
subject to recalibration over varying probability
thresholds. Results suggest that the MNB clas-
sifier is most suited to the recalibration methods,
and yields the best performance, with a 4.2% in-
crease in classification accuracy over our base-
line. Our proposed method is suitable where a
dataset contains a number of related documents.
As the wealth of data for sentiment classification

179



increases, we would like to examine and evaluate
our method on additional datasets.

References
Alexandra Balahur, M. Jesús Hermida, and Andrès

Montoyo. 2011. Detecting implicit expressions
of sentiment in text based on commonsense knowl-
edge. In Proceedings of the Second Workshop on
Computational Approaches to Subjectivity and Sen-
timent Analysis, pages 53–60. ACL.

Salem Benferhat, Karim Tabia, Mouaad Kezih, and
Mahmoud Taibi. 2014. Post-processing a classi-
fier’s predictions: Strategies and empirical evalua-
tion. In ECAI, pages 965–966.

Lin Dai, Hechun Chen, and Xuemei Li. 2011. Improv-
ing sentiment classification using feature highlight-
ing and feature bagging. In ICDMW, pages 61–66.
IEEE.

Stephan Greene and Philip Resnik. 2009. More than
words: Syntactic packaging and implicit sentiment.
In ACL-HLT, pages 503–511.

David Jensen, Jennifer Neville, and Brian Gallagher.
2004. Why collective inference improves relational
classification. In ICKDDM, pages 593–598.

Akiko Murakami and Rudy Raymond. 2010. Support
or oppose?: Classifying positions in online debates
from reply activities and opinion expressions. In
COLING, pages 869–875.

Quoc Dai Nguyen, Quoc Dat Nguyen, and Bao Son
Pham. 2013. A two-stage classifier for sentiment
analysis. In IJCNLP, pages 897–901.

Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan.
2002. Thumbs up? sentiment classification using
machine learning techniques. In EMNLP.

John Platt et al. 1999. Probabilistic outputs for sup-
port vector machines and comparisons to regularized
likelihood methods. Advances in large margin clas-
sifiers, 10(3):61–74.

Davide Proserpio and Georgios Zervas. 2014. Online
reputation management: Estimating the impact of
management responses on consumer reviews. Tech-
nical Report 2521190, Boston U. School of Manage-
ment.

Jianfeng Si, Arjun Mukherjee, Bing Liu, Jialin Sinno
Pan, Qing Li, and Huayi Li. 2014. Exploiting so-
cial relations and sentiment for stock prediction. In
EMNLP, pages 1139–1145.

Phillip Smith and Mark Lee. 2014. Acknowledging
discourse function for sentiment analysis. In CI-
CLING, volume 8404 of LNCS, pages 45–52.

Benjamin Taskar, Eran Segal, and Daphne Koller.
2001. Probabilistic classification and clustering in
relational data. In IJCAI, volume 17, pages 870–
878.

Michael J Wise. 1993. String similarity via greedy
string tiling and running karp-rabin matching. On-
line Preprint, Dec, 119.

Bianca Zadrozny and Charles Elkan. 2001. Obtaining
calibrated probability estimates from decision trees
and naive bayesian classifiers. In ICML, volume 1,
pages 609–616.

180


