












































Relevant and Informative Response Generation using Pointwise Mutual Information


Proceedings of the 1st Workshop on NLP for Conversational AI, pages 133‚Äì138
Florence, Italy, August 1, 2019. c¬©2019 Association for Computational Linguistics

133

Relevant and Informative Response Generation
using Pointwise Mutual Information

Junya Takayama‚Ä† and Yuki Arase‚Ä†‚Ä°
‚Ä†Graduate School of Information Science and Technology, Osaka University

‚Ä°Artificial Intelligence Research Center (AIRC), AIST
{takayama.junya, arase}@ist.osaka-u.ac.jp

Abstract

A sequence-to-sequence model tends to gen-
erate generic responses with little information
for input utterances. To solve this problem, we
propose a neural model that generates relevant
and informative responses. Our model has
simple architecture to enable easy application
to existing neural dialogue models. Specifi-
cally, using positive pointwise mutual infor-
mation, it first identifies keywords that fre-
quently co-occur in responses given an utter-
ance. Then, the model encourages the de-
coder to use the keywords for response gener-
ation. Experiment results demonstrate that our
model successfully diversifies responses rela-
tive to previous models.

1 Introduction

Neural networks are common approaches to build-
ing chat-bots. Vinyals and Le (2015) have pro-
posed a neural dialogue model using sequence-
to-sequence (Seq2Seq) networks (Sutskever et al.,
2014) and achieved fluent response generation.
Because a Seq2Seq model uses a word-by-word
loss function at the time of training, any words
outside the reference are penalized equally. Con-
sequently, the Seq2Seq model tends to generate
generic responses that consist of frequent words,
such as ‚ÄúYes‚Äù and ‚ÄúI don‚Äôt know.‚Äù This is a central
concern in neural dialogue generation. To tackle
this problem, Li et al. (2016) proposed a model for
considering mutual dependency between an utter-
ance and response modeled by maximum mutual
information (MMI). However, their model disre-
garded the aspect of informativeness of responses,
which is also important for user experience of
chat-bots.

To solve this problem, we propose a response
generation model that outputs diverse words while
preserving relevance in response to the input ut-
terance. In our model, Positive Pointwise Mutual

Information (PPMI) identifies keywords from a
large-scale conversational corpus that are likely to
appear in the response to an input utterance. Then,
the model modifies the loss function in a Seq2Seq
model to reward responses using the identified
keywords. In order to calculate the loss function
using the words output by the decoder, we need
to sample words from the probability distribution
of the output layer. Hence, we apply the Gumbel-
Softmax trick (Jang et al., 2017) as a differentiable
pseudo-sampling method.

Experiments using a Japanese dialogue corpus
crawled from Twitter and OpenSubtitles revealed
that the proposed model outperformed (Li et al.,
2016) for all automatic evaluation metrics for cor-
respondence to references and diversity in outputs.

2 Related Work

The generic response problem has been actively
studied. Yao et al. (2016) and Nakamura et al.
(2019) proposed models that constrain decoders
to directly suppress generation of frequent words.
Yao et al. (2016) diversified the response by a
loss function in which words with high inverse
document frequency values are preferred. Naka-
mura et al. (2019) proposed a loss function that
adds weights based on the inverse of the word fre-
quency. Xing et al. (2017) proposed a model us-
ing topic words extracted from utterances. Their
model ensembles words predicted using the topic
words and the words predicted by the decoder.

All of the methods described above only fo-
cus on the amount of a information in a response.
Therefore, generated responses tend to lack rele-
vance to input utterances. MMI-bidi (Li et al.,
2016) solves this problem by approximating the
PMI between the utterance Q and the generated



134

Encoder Decoder

Utterance Reply

PPMI Matrix

Keyword
Handler

Backward

1 ‚àí ùúÜ

ùúÜ

1 ‚àí ùõº

ùõº

MLP

Figure 1: Outline of the proposed model

response R as follows:

PMI(Q,R) = (1‚àí Œª) logP (R|Q) + Œª logP (Q|R).
(1)

Here, both P (R|Q) and P (Q|R) are computed
by independent Seq2Seq models. Specifically, the
N -best candidate responses generated by the for-
mer model are re-ranked by Equation (1). MMI-
bidi exhibited a strong performance for diversi-
fying responses while preserving relevance to an
input utterance. However, its effects depend on
the diversities of the N-best candidate responses.
If these responses are diverse, MMI-bidi can im-
prove futher.

3 Proposed Model

Figure 1 shows the outline of the proposed model.
It first identifies keywords that strongly co-occur
between utterances and their responses in a train-
ing corpus using PPMI (section 3.1). The decoder
then uses Gumbel-Softmax to sample words in the
output layer (section 3.3). Finally, it computes
the proportion of output words matching the key-
words, and add weights to the loss function (sec-
tion 3.4).

3.1 Keywords Retrieval Based on Positive
Pointwise Mutual Information

The keyword handler retrieves words that are
likely to appear in the response to a certain input

utterance based on PPMI, calculated in advance
from an entire training corpus. Let PQ(x) and
PR(x) be probabilities that the word x will ap-
pear in a certain utterance and response sentences,
respectively. Also, let P (x, y) be the probability
that the words x and y exist in the utterance and
response sentence pair. PPMI is calculated as fol-
lows:

PPMI(x, y) = max

(
log2

P (x, y)

PQ(x) ¬∑ PR(y)
, 0

)
.

The pair of x and y and its PPMI score are saved
in the PPMI Matrix in Figure 1. At the time of
response generation, the keyword handler looks
up the PPMI Matrix. Let the word set of a cer-
tain utterance sentence be Q = {q1, q2, . . . , qL},
and the vocabulary in the decoder be VR =
{vR1 , vR2 , . . . , vRN }. The keyword-score of a
word vRn ‚àà VR is defined as follows:‚àë

q‚ààQ
PPMI (q, vRn) .

Keyword-scores are calculated for all words in VR.
Then top-k words are set as keywords VPred used
in the loss function.

3.2 Decoding Response Sentences using
Retrieved Keywords

The decoder first receives a vector vf consisting
of keyword-scores for all words in the vocabulary,
and non-linearly transforms vf through a multi-
layer perceptron (MLP). This vector is concate-
nated with the output of the encoder, and then set
to the initial state of the decoder. By doing so,
we expect that the decoder considers the keyword-
scores. In order to directly boost the probability
to output the keywords, we add weighted vf to the
decoder output vector œÄi at each time step i. The
final decoder output œÄÃÉi is represented by the fol-
lowing equation:

œÄÃÉi = (1‚àí Œªi) ¬∑ œÄi + Œªi ¬∑ vf .

Œªi balances the effects of the decoder output and
vf . Œªi is calculated as follows based on the current
intermediate state hi of the decoder:

Œªi = œÉ
(
W gatehi + b

gate
)
,

where W gate is a trainable weight matrix, bgate is
a bias term, and œÉ(¬∑) is a sigmoid function.



135

3.3 Pseudo-sampling of Generated Words
using Gumbel-Softmax

In order to determine whether the decoder gen-
erated words in VP red, it is necessary to sample
words generated by the decoder. However, sam-
pling based on argmax, which is generally used at
the decoder, disallows back propagation because
of its discrete nature. Jang et al. (2017) pro-
posed Gumbel-Softmax which performs pseudo
sampling from the probability distribution to al-
low back propagation. Gumbel-Softmax performs
the following calculations for a probability distri-
bution œÄ (corresponding to the output layer in the
decoder) for k classes:

yi =
exp((log(œÄi) + gi)/œÑ)‚àëk
j=1 exp((log(œÄj) + gj)/œÑ)

.

Here, œÑ is a hyperparameter called temperature.
Smaller tau makes the vector closer to one-hot but
the dispersion of the gradient becomes larger. gi is
obtained by the following calculation using uni-
form distribution ui ‚àº Uniform(0, 1):

gi = ‚àí log(‚àí log(ui)).

In the proposed model, Gumbel-Softmax is ap-
plied to the final decoder output vector œÄÃÉ at each
time step i as in Equation (2). Then, we obtain the
differentiable pseudo-bag-of-words vector B.

B =

T‚àë
i=1

GumbelSoftmax(œÄÃÉi). (2)

3.4 Loss function
We design a loss function lv which value decreases
as words contained in VPred are generated. Thus,
the decoder outputs more words that strongly co-
occur with the input utterance. Specifically, when
t(bn) is the word corresponding to the n-th index
in B, lv is defined as follows.

lv = ‚àí
N‚àë

n=0

f(bn, VPred),

f(bn, VPred) =

{
min(bn, 1) (t(bn) ‚àà VPred),
0 (otherwise).

(3)
We use min(bn, 1) in Equation (3) to avoid

adding a reward when a keyword is generated mul-
tiple times. This aims to suppress the decoder out-
puts the same word many times.

Finally, the loss function L is defined as a liner
interpolation of lCE of the cross-entropy error and
the lv:

L = (1‚àí Œ±) ¬∑ lCE + Œ± ¬∑ lv.

Œ± is a hyperparameter that balances the degree of
rewards based on the keywords.

4 Experiments

We empirically evaluate how our model avoids
generic responses to generate relevant and infor-
mative responses.

4.1 Datasets

We used two datasets, OpenSubtitles (English)
and Twitter (Japanese). The details of each dataset
are as follows.

OpenSubtitles OpenSubtitles (Tiedemann,
2009) is a large scale open-domain corpus
composed of movie subtitles.

Like Vinyal et al. (Vinyals and Le, 2015)
and Li et al (Li et al., 2016), we assumed
that each line of the subtitles represents
an independent utterance, and constructed
a single-turn dialogue corpus by regarding
two consecutive utterances as an utterance-
response pair. We randomly sampled 2 mil-
lion utterance-response pairs. All sentences
were tokenized using the Punkt Sentence To-
kenizer of nltk 1.

Twitter We crawled conversations in Japanese
Twitter using ‚Äú@‚Äù mention as a clue. A
single-turn dialogue corpus was constructed
by regarding a tweet and its reply as an
utterance-response pair. The dataset consists
of about 1.3 million utterance-response pairs.
All sentences were tokenized by MeCab 2.

In both datasets, 10k utterance-response pairs
were separated as validation data, another 10k
were separated as test data, and the rest were used
as training data.

4.2 Comparison Methods

We compared our model to previous models. The
baseline is the standard Seq2Seq (Seq2Seq). We
also compared to MMI-bidi (Seq2Seq + MMI)

1https://www.nltk.org/
2http://taku910.github.io/mecab/



136

BLEU NIST dist-1 dist-2 ent-4 length repetition
Proposed + MMI 1.577 0.872 0.050 0.187 8.536 8.064 1.551
Proposed 1.569 0.837 0.044 0.148 7.327 7.520 1.377
Seq2Seq + MMI 1.373 0.739 0.009 0.032 5.600 7.566 1.223
Seq2Seq 1.374 0.687 0.005 0.015 4.070 8.025 1.095
Reference 100.000 16.498 0.086 0.482 10.647 7.671 1.000

Table 1: Results on the OpenSubtitle corpus (English)

BLEU NIST dist-1 dist-2 ent-4 length repetition
Proposed + MMI 2.611 0.573 0.071 0.204 8.979 7.913 1.832
Proposed 2.591 0.583 0.068 0.188 8.738 8.044 1.902
Seq2Seq + MMI 2.262 0.304 0.043 0.102 7.578 6.791 1.416
Seq2Seq 2.237 0.318 0.040 0.091 7.103 6.920 1.518
Reference 100.000 16.562 0.105 0.496 11.311 12.262 1.000

Table 2: Results on the Twitter corpus (Japanese)

because it is the most relevant method for diver-
sifying responses. In addition, we combined our
model with MMI-bidi (Proposed + MMI) to see
whether it contributes to diversification of the N-
best candidates.

4.3 Evaluation Metrics
We employed several automatic evaluation met-
rics. BLEU and NIST measure the validity of
generated sentences in comparison with refer-
ences. BLEU (Papineni et al., 2002) measures the
correspondence between n-grams in generated re-
sponses and those in reference sentences. Follow-
ing Papineni et al. (2002), we used the average of
BLEU scores from 1-gram to 4-gram in the experi-
ment. NIST (Doddington, 2002) also measures the
correspondence between generated responses and
reference sentences. Unlike BLEU, NIST places
lower weights on frequent n-grams, i.e., NIST re-
gards content words as more important than func-
tion words. In the experiment, we used the average
of NIST from 1-gram to 5-gram.

In addition, dist and ent measure the diversity
of generated responses. Dist (Li et al., 2016) is
defined as the number of distinct n-grams in gen-
erated responses divided by the total number of
generated tokens. On the other hand, ent (Zhang
et al., 2018) considers the frequency of n-grams in
generated responses:

ent = ‚àí 1‚àë
w F (w)

‚àë
w‚ààX

F (w) log
F (w)‚àë
w F (w)

,

where X is a set of n -grams output by the sys-

tem, and F (w) computes the frequency of each
n-gram.

In this paper, we focus on automatic evaluation.
Human evaluation is our future work.

4.4 Parameter Settings

For all models, we implemented the encoder and
decoder of each model using 1-layer GRUs. The
dimension of the GRU was set to 512. How-
ever, only the decoder of the proposed model used
1024-dimensional GRU. This is because the ini-
tial state of the decoder is the concatenation of the
keyword-score vector and the output from the en-
coder (512-dimension for each). Both the encoder
and decoder had a word embedding layer of 256
dimensions.

The vocabulary consisted of words that ap-
peared more than 15 times in the training data.
Words that occured less than 15 times were re-
placed with the ‚Äú<unk>‚Äù token. The vocabulary
size was 41.5k for the Twitter model and 20.9k for
the OpenSubtitles model.

4.5 Results and Error Analysis

The left sides of Tables 1 and 2 show BLEU, NIST,
dist, and ent scores for OpenSubtitles and Twit-
ter, respectively. Our model (Proposed) outper-
formed Seq2Seq and MMI-bidi (Seq2Seq) in all
evaluation metrics across the datasets. Further-
more, our model combined with MMI-bidi (Pro-
posed+MMI) achieved the best performance, ex-
cept for NIST, on the Twitter dataset. This result
demonstrates that our method successfully gener-



137

Input oh, we just actually wanted to be alone
Reference get me out of here
Proposed+MMI i wanted to be alone
Seq2Seq+MMI you know what ?
Input should we call an ambulance ?
Reference praise the lord !
Proposed+MMI we ‚Äôr e going to have a call from the ambulance
Seq2Seq+MMI i don ‚Äôt think so
Input are you tired already ?
Reference you ‚Äôr e such a weakling
Proposed+MMI i ‚Äôm tired of getting warmed up
Seq2Seq+MMI yes i ‚Äôm fine
Input i have no idea how it was spread
Reference not from me
Proposed+MMI i don ‚Äôt know how much longer i can stand up
Seq2Seq+MMI i <unk>never been able to find a way
Input why do you have to carry the burden of someone else ‚Äôs life ?
Reference he ‚Äôs my father
Proposed+MMI because you ‚Äôr e the best commodity in the living room
Seq2Seq+MMI i don ‚Äôt know
Input the little guy was kinda funny lookin
Reference in what way ?
Proposed+MMI i kinda like him kinda funny funny
Seq2Seq+MMI i don ‚Äôt know what you ‚Äôr e talking about
Input bishop wood at st <unk>‚Äôs cathedral
Reference holy christ blanche
Proposed+MMI he was baptized in a parson ‚Äôs church and he was baptized
Seq2Seq+MMI he was the king of <unk><unk>

Table 3: Example responses generated by the proposed model and (Li et al., 2016)

ates diverse responses, which effectively improves
the N -best candidates reranked by MMI-bidi. It
is notable that improvements on NIST, which ap-
preciates less frequent n-grams, support this idea
the proposed model improves the informativeness
of responses. The improvement is larger on the
Twitter dataset, where the proposed method (Pro-
posed) achieved NIST score 0.265 points higher
than Seq2Seq even though MMI-bidi is inferior to
Seq2Seq.

The example responses generated by Pro-
posed+MMI and Seq2Seq+MMI using OpenSub-
titles are shown in Table 3. The examples from the
top to the third rows show that the proposed model
generates more content words relevant to the con-
tent words in the utterance. On the other hand,
Seq2Seq+MMI ended up generating fewer infor-
mative responses using generic words. The fourth
and fifth examples show that the proposed model
generated responses with little relevance to the in-

put, although they were more informative than the
responses generated by Seq2Seq+MMI.

The last two examples show a drawback of the
proposed model, i.e., which is over-generation of
the same word. For quantitative evaluation, we
computed the repetition rate (Le et al., 2017) on
the test data, which measures the meaningless rep-
etition of words. The repetition rate is defined as:

repetition rate =
1

N

N‚àë
i=1

1 + r (yÃÉi)

1 + r(Yi)
,

where yÃÉi is the i-th generated sentence in the test
data, Yi is its reference, and N is the total number
of test sentences. The function r(¬∑) measures the
repetition as the difference between the number of
words and that of unique words in a sentence:

r(X) = len(X)‚àí len(set(X)),

where X means words in a sentence, len(X) com-
putes the number of items in X , and set(X) re-



138

moves duplicate items in X . The average lengths
of generated responses and repetition rates are
shown on the right sides of Tables 1 and 2. The
results show that the proposed models (Proposed
and Proposed+MMI) tend to generate longer re-
sponses than Seq2Seq, but their repetition rates are
also higher. This may be caused by time-invariant
keyword-scores, despite the fact that the decoder
output changes over time. In the future, we will
update the keyword-score vector to avoid repeti-
tion in responses.

5 Conclusion

Aiming at generating diverse responses while pre-
serving relevance to the input, we proposed a
model that identifies keywords using PPMI and
promoted their generation in the decoder. Eval-
uation results using English and Japanese conver-
sational corpora show that in comparison with (Li
et al., 2016), the proposed model achieved better
performance in terms of correspondence to refer-
ences and diversity of output. On the other hand,
we found that the proposed model has a tendency
of over-generation.

As future work, we will conduct human eval-
uation and qualitative analysis. We will also in-
vestigate the effects of the hyper-parameter Œ± on
overall performance. We also plan to develop a
mechanism for suppressing over-generation.

Acknowledgments

This project is funded by Microsoft Research
Asia, Microsoft Japan Co., Ltd., and JSPS KAK-
ENHI Grant Number JP18K11435.

References
George Doddington. 2002. Automatic Evaluation of

Machine Translation Quality Using N-gram Co-
Occurrence Statistics. In Proceedings of the second
international conference on Human Language Tech-
nology Research (HLT 2002).

Eric Jang, Shixiang Gu, and Ben Poole. 2017. Cat-
egorical Reparametarization with Gumbel-Softmax.
In Proceedings of The 5th International Conference
on Learning Representations (ICLR 2017).

An Nguyen Le, Ander Martinez, Akifumi Yoshimoto,
and Yuji Matsumoto. 2017. Improving Sequence
to Sequence Neural Machine Translation by Utiliz-
ing Syntactic Dependency Information. In Proceed-
ings of the The 8th International Joint Conference
on Natural Language Processing (IJCNLP 2017),
pages 21‚Äì29.

Jiwei Li, Michel Galley, Chris Brockett, Jianfeng Gao,
and Bill Dolan. 2016. A Diversity-Promoting Ob-
jective Function for Neural Conversation Models.
In Proceedings of The 15th Annual Conference of
the North American Chapter of the Association for
Computational Linguistics: Human Language Tech-
nologies (NAACL-HLT 2016), pages 110‚Äì119.

Ryo Nakamura, Katsuhito Sudoh, Koichiro Yoshino,
and Satoshi Nakamura. 2019. Another Diversity-
Promoting Objective Function for Neural Dialogue
Generation. In Proceedings of The Second AAAI
Workshop on Reasoning and Learning for Human-
Machine Dialogues (DEEP-DIAL).

Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: A Method for Automatic
Evaluation of Machine Translation. In Proceedings
of the 40th Annual Meeting of the Association for
Computational Linguistics (ACL 2002), pages 311‚Äì
318.

Ilya Sutskever, Oriol Vinyals, and Quoc V Le. 2014.
Sequence to sequence learning with neural net-
works. In Proceedings of The Twenty-eighth Con-
ference on Neural Information Processing Systems
(NeurIPS 2014), pages 3104‚Äì3112.

JoÃàrg Tiedemann. 2009. News from opus-A collection
of multilingual parallel corpora with tools and in-
terfaces 1 Index of Subjects and Terms 13 vi News
from OPUS-A Collection of Multilingual Parallel
Corpora with Tools and Interfaces. Recent advances
in natural language processing, 5:237‚Äì248.

Oriol Vinyals and Quoc V Le. 2015. A Neural Conver-
sational Model. In Proceedings of the 32nd Inter-
national Conference on Machine Learning (ICML
2015).

Chen Xing, Wei Chung Wu, Yu Ping Wu, Jie Liu, Yalou
Huang, Ming Zhou, and Wei-Ying Ma. 2017. Topic
Aware Neural Response Generation. In Proceedings
of the Thirty-First AAAI Conference on Artificial In-
telligence (AAAI 2017).

Kaisheng Yao, Baolin Peng, Geoffrey Zweig, and
Kam-Fai Wong. 2016. An Attentional Neural Con-
versation Model with Improved Specificity. arXiv
preprint arXiv:1606.01292.

Yizhe Zhang, Michel Galley, Jianfeng Gao, Zhe Gan,
Xiujun Li, Chris Brockett, and Bill Dolan. 2018.
Generating Informative and Diverse Conversational
Responses via Adversarial Information Maximiza-
tion. In Proceedings of 32nd Conference on Neural
Information Processing Systems (NeurIPS 2018).

http://arxiv.org/abs/1811.08100v2
http://arxiv.org/abs/1811.08100v2
http://arxiv.org/abs/1811.08100v2
http://arxiv.org/abs/1606.01292v1
http://arxiv.org/abs/1606.01292v1

