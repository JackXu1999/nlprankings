



















































Neural Syntactic Generative Models with Exact Marginalization


Proceedings of NAACL-HLT 2018, pages 942–952
New Orleans, Louisiana, June 1 - 6, 2018. c©2018 Association for Computational Linguistics

Neural Syntactic Generative Models with Exact Marginalization

Jan Buys1,2 and Phil Blunsom1,3
1Department of Computer Science, University of Oxford

2Paul G. Allen School of Computer Science & Engineering, University of Washington
3DeepMind

jbuys@cs.washington.edu, phil.blunsom@cs.ox.ac.uk

Abstract

We present neural syntactic generative mod-
els with exact marginalization that support
both dependency parsing and language mod-
eling. Exact marginalization is made tractable
through dynamic programming over shift-
reduce parsing and minimal RNN-based fea-
ture sets. Our algorithms complement previ-
ous approaches by supporting batched train-
ing and enabling online computation of next
word probabilities. For supervised depen-
dency parsing, our model achieves a state-
of-the-art result among generative approaches.
We also report empirical results on unsuper-
vised syntactic models and their role in lan-
guage modeling. We find that our model for-
mulation of latent dependencies with exact
marginalization do not lead to better intrinsic
language modeling performance than vanilla
RNNs, and that parsing accuracy is not cor-
related with language modeling perplexity in
stack-based models.

1 Introduction

We investigate the feasibility of neural syntactic
generative models with structured latent variables
in which exact inference is tractable. Recent mod-
els have added structure to recurrent neural net-
works at the cost of giving up exact inference, or
through using soft structure instead of latent vari-
ables (Dyer et al., 2016; Yogatama et al., 2016;
Grefenstette et al., 2015). We propose genera-
tive models in which syntactic structure is mod-
elled with a discrete stack which can be marginal-
ized as a latent variable through dynamic program-
ming. This enables us to investigate the trade-off
between model expressivity and exact marginal-
ization in probabilistic models based on recurrent
neural networks (RNNs).

While Long Short-term Memory (Hochreiter
and Schmidhuber, 1997) (LSTM) RNNs have

driven strong improvements in intrinsic language
modelling performance, they fail at capturing cer-
tain long-distance dependencies, such as those
required for modelling subject-verb agreement
(Linzen et al., 2016) or performing synthetic trans-
duction tasks based on context-free grammars
(Grefenstette et al., 2015). We propose genera-
tive models, based on transition-based dependency
parsing (Nivre, 2008), a widely used framework
for incremental syntactic parsing, that are able to
capture desirable dependencies.

Our generative approach to dependency parsing
encodes sentences with an RNN and estimate tran-
sition and next word probability distributions by
conditioning on a small number of features repre-
sented by RNN encoder vectors. In contrast to pre-
vious syntactic language models such as RNNG
(Dyer et al., 2016), marginal word probabilities
can be computed both online and exactly. A GPU
implementation which exploits parallelization en-
ables unsupervised learning and fast training and
decoding. The price of exact inference is that our
models are less expressive than RNNG, as the re-
currence is not syntax-dependent.

Our generative models are based on the arc-
eager and arc-hybrid transition systems, with
O(n3) dynamic programs based on Kuhlmann
et al. (2011). Previous work on dynamic pro-
gramming for transition-based parsing either re-
quired approximate inference due to a too high
polynomial order run-time complexity (Huang and
Sagae, 2010), or had too restrictive feature spaces
to be used as accurate models (Kuhlmann et al.,
2011; Cohen et al., 2011). Recent work showed
that bidirectional RNNs enable accurate graph-
based and transition-based dependency parsing
using minimal feature spaces (Kiperwasser and
Goldberg, 2016; Cross and Huang, 2016; Dozat
and Manning, 2017). Shi et al. (2017) further
showed that under this approach exact decoding

942



ROOT The girls from school play football

det

nsubj

case

nmod

root

obj

Figure 1: A dependency tree (arcs above words) to-
gether with dependencies captured by the generative
model for word prediction (arcs below words).

and globally-normalized discriminative training is
tractable with dynamic programming.

While discriminative neural network-based
models obtain state-of-the-art parsing accura-
cies (Dozat and Manning, 2017), generative mod-
els for structured prediction have a number of ad-
vantages: They do not suffer from label bias or
explaining away effects (Yu et al., 2017), have
lower sample complexity (Yogatama et al., 2017),
are amenable to unsupervised learning and can
model uncertainty and incorporate prior knowl-
edge through latent variables.

As a supervised parser our model obtains state
of the art performance in transition-based gener-
ative dependency parsing. While its intrinsic lan-
guage modelling performance is worse than that of
a well-tuned vanilla RNN, we see that the formu-
lation of the generative model has a large impact
on both the informedness of the syntactic structure
and the parsing accuracy of the model. Further-
more, there is a discrepancy between the model
structure most suitable for parsing and for lan-
guage modeling. Our analysis shows that there ex-
ist informative syntactically-motivated dependen-
cies which LSTMs are not capturing, even though
our syntactic models are not able to predict them
accurately enough during online processing to im-
prove language modelling performance. Our im-
plementation is available at https://github.
com/janmbuys/ndp-parser.

2 Generative shift-reduce parsing

We start by defining a shift-reduce transition sys-
tem which does not predict dependency arcs, but
simply processes the words in a sentence left to
right through shifting words onto a stack and re-
ducing (popping) them from the stack. We define

Stack σ Index β − 1 Prediction
ROOT ROOT sh(The)
ROOT, The The la(det)
ROOT The sh(girls)
ROOT, girls girls sh(from)
ROOT, girls, from from la(case)
ROOT, girls from sh(school)
ROOT, girls, school school ra(nmod)
ROOT, girls school la(nsubj)
ROOT school sh(play)
ROOT, play play sh(football)
ROOT, play, football football ra(obj)
ROOT, play football ra(root)
ROOT football re

Table 1: Arc-hybrid transition system derivation for the
sentence “The girls from school play football.” The
transitions are shift (sh), reduce (re), left-arc (la) and
right-arc (ra).

a generative model for this transition system and
a dynamic program to perform inference over all
possible shift-reduce transitions to process a given
sentence. An example dependency tree is given in
Figure 1, along with the dependencies our genera-
tive model is capturing when making word predic-
tions. The arc-hybrid transition sequence for the
example is given in Table 1.

Let sentence w0:n be a sequence of words,
where w0 is always the designated root symbol
ROOT and wn the end-of-sentence symbol EOS.
The state variables of the transition system are the
stack σ, consisting of word indexes, and a current
word index β, also referred to as the buffer. The
first and second elements on the stack are referred
to as σ0 and σ1, respectively. We use the nota-
tion σ|j to indicate that j is on top of the stack.
The initial state (σ, β) is ([0], 1) and the final state
is ([], n). There are two transition actions, shift
and reduce. Shift updates the transition state from
(σ, j) to (σ|j, j + 1). Reduce changes the state
from (σ|i, j) to (σ, j).

2.1 Generative model
The generative model for this transition system is
defined by a probability distribution over w,

p(w) =
∑

t

p(w0:n, t0:2n), (1)

where t0:2n is a transition sequence that processes
the sentence. Shift actions predict (assign proba-
bility to) the next word in the sentence. The end-

943



of-sentence symbol is generated implicitly when
ROOT is reduced from the stack.

The sentence is encoded left-to-right by an
LSTM RNN taking the word embedding of the last
predicted word as input at each time step, indepen-
dent of t. The RNN hidden states h0:n represent
each sentence position in its linear context. The
probability of a shift action and the word that it
predicts is

ptr(sh|hσ0 , hβ−1)pgen(w|hσ0 , hβ−1).

Reduce is predicted with probability
ptr(re|hσ0 , hβ−1) = 1− ptr(sh|hσ0 , hβ−1).

The transition and word probability distribu-
tions are estimated by non-linear output layers that
take the context-depending RNN representations
of positions in the transition system as input,

ptr = sigmoid(rT relu(Wtshσ0 +Wtbhβ−1))
(2)

pgen = softmax(RT tanh(Wgshσ0 +Wtbhβ−1)),
(3)

where R and the W ’s are neural network parame-
ter matrices and r is a parameter vector.

The model has two ways of representing con-
text: The RNN encoding, which has a recency
bias, and the stack, which can represent long range
dependencies and has a syntactic distance bias.
The choice of RNN states (corresponding to stack
elements) to condition on is restricted by our goal
of making the dynamic programming tractable.

We propose two formulations of the generative
model: In the first, referred to as stack-next, shift
generates the word pushed on the stack, which is
currently at position β, as in the equations above.
In the second formulation, referred to as buffer-
next, shift generates the word at position β+1, i.e.,
the next word on the buffer. The first formulation
has a more intuitive generative story as the genera-
tion of a word is conditioned on the top of the stack
when it is generated (see Table 1), but the sec-
ond formulation has the advantage that transition
predictions are conditioned on the current word at
position β, which is more informative for parsing
predictions. Models are defined using stack-next
unless stated otherwise.

2.2 Dynamic program

We now define a dynamic program for this model,
based on the algorithms proposed by Kuhlmann

et al. (2011) and their application to generative de-
pendency parsing (Cohen et al., 2011).

The key to the dynamic program is the decom-
position of the transition sequence into push com-
putations. Each push computation is a sequence
of transitions which results in a single node hav-
ing been pushed to the stack. The simplest push
computation is a single shift operation. Push com-
putations can be composed recursively: combin-
ing two consecutive push computations followed
by a reduce transition yields a new push opera-
tion. Therefore the derivation of a sentence under
the transition system can be seen as a composition
of push computations.

Items in the deduction system (Shieber et al.,
1995) of the dynamic program have the form [i, j],
which has the interpretation that there exists a push
computation between actions ak and al such that
β = i at time step k and σ0 = i and β = j at
time step l. In the deduction system [0, 1] is an
axiom, [0, n] is the goal and the deduction rules
corresponding to the transitions are

[i, j − 1]→ [j − 1, j] (shift)
[i, k][k, j]→ [i, j] (reduce).

The marginal probability distribution is com-
puted by defining the inside score I(i, j) =
p(wi:j−1) for every deduction system item. Com-
puting the sentence probability corresponds to
computing the inside score of the goal, I(0, n) =
p(w0:n−1), followed by computing the final re-
duce probability.

Reduce probabilities are computed conditioned
on positions k and j, which are accessible through
the dynamic program deduction rule. However the
shift probabilities cannot be computed at the shift
rule for deducing [j − 1, j], as it does not have
access there to the top of the stack. One solution
is to extend the deduction system to a three-tuple
that can track the value of an additional position,
leading to a O(n4) dynamic program. Instead Shi
et al. (2017) showed that the computation can be
performed in the O(n3) algorithm by computing
the shift probability of word k during the reduce
deduction, as it was generated when i was on top
of the stack. The inside algorithm is given in Al-
gorithm 1.

To train the model without supervised transition
sequences, we can optimize the negative log like-
lihood of p(w0:n) directly with gradient-based op-
timization using automatic differentiation, which

944



Algorithm 1 Inside algorithm for the shift-reduce
transition-based generative model.
1: I(0, 1)← 1
2: for j = 2, . . . , n do
3: I(j−1, j)← 1
4: for i = j − 2, . . . , 0 do
5: for k = i + 1, . . . , j − 1 do
6: T (k)← ptr(sh|hi, hk−1)pgen(wk|hi, hk−1)
7: end for
8: I(i, j)←∑j−1k=i+1 I(i, k)I(k, j)ptr(re|hk, hj−1)T (k)
9: end for
10: end for
11: return I(0, n) + ptr(re|h0, hn−1)

is equivalent to computing the gradients with the
outside algorithm (Eisner, 2016). For decoding we
perform Viterbi search over the dynamic program
by maximizing rather than summing over different
split positions (values of k when reducing).

The buffer-next generative formulation, where
shift generates the next word β, can also be com-
puted with the dynamic program. Here w1 is pre-
dicted at the initial state in I(0, 1), while the end-
of-sentence token is generated explicitly when a
shift action results in buffer being set to position
n, regardless of the state of the stack.

3 Transition-based dependency parsing

The arc-eager (Nivre, 2008) and arc-
hybrid (Kuhlmann et al., 2011) transition
systems for projective dependency parsing use the
same shift-reduce operations but predict left- and
right-arcs at different time steps. We propose gen-
erative models for these transition systems based
on the dynamic program for shift-reduce parsing
proposed above, again following Kuhlmann et al.
(2011). For supervised training we optimize the
joint probability distribution p(w, t), where an
oracle is used to derive transition sequence t
from the training examples. In cases of spurious
ambiguity arcs are added as soon as possible.

3.1 Arc-hybrid parser

The arc-hybrid transition system has three actions:
Shift, left-arc and right-arc (see Table 2 for def-
initions). Left-arc and right-arc are both reduce
actions, but they add arcs between different word
pairs. Arc label predictions are conditioned on the
same context as transition predictions. Right-arc
adds a dependency of which σ1 is the head, but
the dynamic program does not allow conditioning
on it when making transition decisions. However,
we found that this does not actually decrease per-
formance.

The dynamic program for the arc-hybrid parser
has the same structure as the shift-reduce model.
The marginal probability is independent of arc di-
rectionality, as it does not influence future deci-
sions. Consequently unsupervised training based
on this model cannot learn to predict arc direc-
tions. Exact decoding is performed with the
Viterbi algorithm: At every item [i, j] the highest
scoring arc direction is recorded. After the most
likely transition sequence is extracted, arc labels
are predicted greedily.

3.2 Arc-eager parser

The arc-eager parser has four transitions, as de-
fined in Table 2. Shift and right-arc are shift ac-
tions, while left-arc and reduce are reduce actions.
However the two reduce actions, reduce and left-
arc, are always mutually exclusive; the former is
only valid if the stack top has already been as-
signed a head (through a previous right-arc) and
the latter only if the stack top is not headed. To
keep track of which actions are valid, the state
configuration and the dynamic program are aug-
mented to record whether elements on the stack
are headed. As with arc-hybrid, we decompose the
transition probability into deciding between shift-
ing and reducing, and then predicting direction-
ality. In this case, the shift decision decomposes
into shift and right-arc transitions, where shift is
implicitly deciding that the shifted word will be
reduced through a left-arc. Consequently the only
real difference between the arc-hybrid and arc-
eager transition systems under dynamic program-
ming is the information conditioned on when arc
directionality is predicted.

A different deduction system is defined for arc-
eager, although it follows the same structure as
the shift-reduce one. Items have the form [ic, j],
where c is a binary variable indicating whether
node i is headed. The axiom and goal are [00, n]
and [00, 1], respectively. The deduction rules are

[ic, j]→ [j0, j + 1] (shift)
[ic, j]→ [j1, j + 1] (right-arc)

[ic, k][k0, j]→ [ic, j] (left-arc)
[ic, k][k1, j]→ [ic, j] (reduce)

The inside algorithm for arc-eager parsing is
given in Algorithm 2. The algorithm is structured
such that the inner loop computations (lines 8−22)
can be vectorized, which is crucial for efficient

945



Action State before State after Arc added Probability

Shift (σ|i, j) (σ|i|j, j + 1) - ptr(sh|hi, hj−1)pgen(wj |hi, hj−1)
Left-arc (σ|i, j) (σ, j) j → i ptr(re|hi, hj−1)pdir(la|hi, hj−1)
Right-arc (σ|l|i, j) (σ|l, j) l→ i ptr(re|hi, hj−1)pdir(ra|hi, hj−1)
Shift (σ|ib, j) (σ|ib|j0, j + 1) - ptr(sh|hi, hj−1)pdir(la|hi, hj−1)pgen(wj |hi, hj−1)
Right-arc (σ|ib, j) (σ|ib|j1, j + 1) i→ j ptr(sh|hi, hj−1)pdir(ra|hi, hj−1)pgen(wj |hi, hj−1)
Left-arc (σ|i0, j) (σ, j) j → i ptr(re|hi, hj−1)
Reduce (σ|i1, j) (σ, j) - ptr(re|hi, hj−1)

Table 2: The arc-hybrid (above) and arc-eager (below) transition systems. States represent (stack, current index).

Algorithm 2 Inside algorithm for arc-eager parser.
1: for j = 0, . . . , n− 1 do
2: I(j0, j+1)← 1
3: I(j1, j+1)← 1
4: end for
5: for gap = 2, . . . , n do
6: for i = 0, . . . , n− gap do
7: j = i + gap
8: for c = 0, 1 do
9: for k = i + 1, . . . , j − 1 do
10: if j > 0 then
11: W (k)← ptr(sh|hi, hk−1)
12: ·pdir(ra|hi, hk−1)pgen(wk|hi, hk−1)
13: T (k)← I(k1, j)ptr(re|hk, hj−1)W (k)
14: end if
15: if j < n then
16: V (k)← ptr(sh|hi, hk−1)
17: ·pdir(la|hi, hk−1)pgen(wk|hi, hk−1)
18: T (k) ← T (k) +

I(k0, j)ptr(re|hk, hj−1))V (k)
19: end if
20: end for
21: I(ic, j)←∑j−1k=i+1 I(ic, k)T (k)
22: end for
23: end for
24: end for
25: return I(0, n) + ptr(re|h0, hn−1)

GPU implementation. At β = n, the dynamic pro-
gram is restricted to allow only reduce transitions,
requiring the remaining stack elements (apart from
ROOT) to be headed. The Viterbi algorithm again
follows the same structure as the inside algorithm:
For every item [ic, j] the highest scoring splitting
item kb is recorded, where k is the splitting point
and b indicates whether word k is headed or not,
which corresponds to whether a reduce or left-arc
is performed.

4 Experiments

We follow the standard setup for English depen-
dency parsing, training on sections 2-21 of the
Penn Treebank (PTB) Wall Street Journal cor-
pus, using section 22 for development and sec-
tion 23 for testing. Dependency trees follow the
Stanford dependency (SD) representation (version
3.3.0) used in recent parsing research (Chen and
Manning, 2014; Dyer et al., 2015). We also re-
port some results using the older representation

of Yamada and Matsumoto (2003) (YM). We fol-
low Buys and Blunsom (2015b) and Dyer et al.
(2016) in replacing training singletons and un-
known words in the test set with unknown word
class tokens based to their surface forms, follow-
ing the rules implemented in the Berkeley parser.1

Our models are implemented in PyTorch, which
constructs computation graphs dynamically.2 Dur-
ing training, sentences are shuffled at each epoch,
and minibatches are constructed of sentences of
the same length. We base the hyperparameters of
our models primarily on the language models of
Zaremba et al. (2014). Models are based on two-
layer LSTMs with embedding and hidden state
size 650 with dropout of 0.5 on the RNN inputs
and outputs. For all models weights are initial-
ized randomly from the uniform distribution over
[−0.05, 0.05]. Gradient norms are clipped to 5.0.
The supervised parsers are trained with batch size
16, with an initial learning rate 1.0, which is de-
creased by a factor of 1.7 for every epoch after
6 initial epochs. The sequential LSTM baseline
is trained with the same parameters, except that
the learning rate decay is 1.4. The unsupervised
models are trained with an initial learning rate 0.1,
which is decreased by a factor of 2.0 for every
epoch, with batch size 8.

We train and execute our models on a GPU,
obtaining significant speed improvements over
CPUs. For supervised training we also perform
batch processing: After the sentences are encoded
with an RNN, we extract the inputs to the transi-
tion, word and relation prediction models across
the batch, and then perform the neural network
computations in parallel. The supervised models’
training speed is about 3 minutes per epoch.

1http://github.com/slavpetrov/
berkeleyparser

2http://pytorch.org/

946



Model Greedy Exact

Arc-Hybrid uniRNN 84.12/81.54 84.21/81.61
Arc-Eager uniRNN 79.90/77.67 81.37/79.08
Arc-Hybrid biRNN 92.85/90.42 92.89/90.47
Arc-Eager biRNN 92.82/90.63 92.90/90.68

Arc-Hybrid Gen stack-next 56.98/52.22 82.77/78.01
Arc-Hybrid Gen buffer-next 85.25/82.83 91.19/88.66
Arc-Eager Gen buffer-next 80.79/78.56 87.34/84.84

Table 3: PTB development set parsing results (SD
dependencies), reporting unlabelled and labelled at-
tachment scores (UAS/LAS). Discriminative models
(above the line) use either unidirectional or bidirec-
tional RNNs.

Model SD YM

Buys and Blunsom (2015b) 90.10/87.74 90.16/88.83
Titov and Henderson (2007) 91.43/89.02 90.75/89.29

Arc-eager 88.20/85.91 87.61/86.36
Arc-hybrid 91.01/88.54 90.71/88.68

Table 4: PTB test set parsing results with supervised
generative models, on the Stanford (SD) and Yamada
and Matsumoto (2003) (YM) dependencies. The mod-
els from Buys and Blunsom (2015b) and Titov and
Henderson (2007) were retrained to make results di-
rectly comparable.

4.1 Parsing
In order to benchmark parsing performance, we
train discriminative baselines using the same fea-
ture space as the generative models. Unidirec-
tional or bidirectional RNNs can be used; we see
that the bidirectional encoder is crucial for accu-
racy (Table 3). The performance of our implemen-
tation is on par with than that of the arc-hybrid
transition-based parser of Kiperwasser and Gold-
berg (2016), which obtains 93.2/91.2 UAS/LAS
on the test set against 93.29/90.83 for our arc-
hybrid model. State of the art parsing perfor-
mance is 95.7%/94.1 UAS/LAS (Dozat and Man-
ning, 2017).

Exact decoding is only marginally more ac-
curate than greedy decoding, giving further evi-
dence of the label bias problem. Andor et al.
(2016) similarly showed that a locally normalised
model without lookahead features cannot ob-
tain good performance even with beam-search
(81.35% UAS), while their globally normalised
model can reach close to optimal performance
without look-ahead. Shi et al. (2017) showed that
globally normalised training improves the accu-
racy of these discriminative models.

Exact decoding is crucial to the performance of

ROOT Another $ 20 billion would be raised through treasury bonds

Figure 2: Sentence with dependencies induced by the
unsupervised model.

the generative models (Table 3). They are much
more accurate than the unidirectional discrimina-
tive models, which shows that the word prediction
model benefits parsing accuracy. The arc-hybrid
model is more accurate than arc-eager, as was the
case for the unidirectional discriminative models.
This can be explained by arc-eager making attach-
ment decisions earlier in the transition sequence
than arc-hybrid, which means that it has access to
less context to condition these predictions on.

Our best generative model outperforms a pre-
vious incremental generative dependency parser
based on feed-forward neural networks and ap-
proximate inference (Buys and Blunsom, 2015b)
(Table 4). It is competitive with a previous RNN-
based generative parser with a much more com-
plex architecture than our model, including recur-
rent connections based on parsing decision (Titov
and Henderson, 2007). Our exact decoding algo-
rithm is also actually faster than the beam-search
approaches for previous models, as it is imple-
mented on GPU. Our arc-hybrid model parses 7.4
sentences per second, against 4 sentences per sec-
ond for Buys and Blunsom (2015b) and approxi-
mately 1 sentence per second for Titov and Hen-
derson (2007).

We also train the model as an unsupervised
parser by directly optimizing the marginal sen-
tence probability. The limitation of our approach
is that our models cannot learn arc directional-
ity without supervision, so we interpret shift as
adding a (right-arc) dependency between top of
the stack and the word being generated. In our
experiments the model did not succeed in learning
informative, non-trivial tree structures – in most
cases it learns to attach words either to the im-
mediate previous word or to the root. However,
unsupervised dependency parsers usually require
elaborate initialization schemes or biases to pro-
duce non-trivial trees (Klein and Manning, 2004;
Spitkovsky et al., 2010; Bisk and Hockenmaier,
2015). An example dependency tree predicted by
the unsupervised model is given in Figure 2.

947



Model Perplexity

Interpolated Kneser-Ney 5-gram 170.09
Sequential LSTM (unbatched) 118.69
Sequential LSTM (batched) 100.67

Buys and Blunsom (2015b) 138.62
RNNG (Kuncoro et al., 2017) 101.2

Supervised (SD) shift-reduce buffer-next 111.53
Supervised (SD) shift-reduce stack-next 107.61
Unsupervised shift-reduce stack-next 125.20

Table 5: Language modelling perplexity results on the
PTB parsing test set.

4.2 Language modelling

We apply our model to language modelling
with both supervised and unsupervised training.
The supervised models are trained as arc-hybrid
parsers; the performance of arc-eager is almost
identical as arc labels and directionality are not
predicted. The unsupervised model is trained with
only shift and reduce transitions as latent.

We evaluate language models with a sentence
i.i.d. assumption. In contrast, the standard evalu-
ation setup for RNN language models treats the
entire corpus as single sequence. To evaluate
the consequence of the sentence independence as-
sumption, we trained a model on the most widely
used PTB language modelling setup (Chelba and
Jelinek, 2000; Mikolov et al., 2011), which uses
a different training/testing split and preprocessing
which limits the vocabulary to 10k. Our baseline
LSTM obtains 92.71 test perplexity on this setup,
against 78.4 of Zaremba et al. (2014), which uses
the same hyperparameters without a sentence i.i.d.
assumption. The syntactic neural language model
of Emami and Jelinek (2005) obtained 131.3.

Results are reported in Table 5. Perplexity is
obtained by exponentiating the negative log likeli-
hood per token; end of sentence symbols are pre-
dicted but excluded from the token counts. As
baselines without syntactic structure we use the in-
terpolated Kneser-Ney n-gram model (Kneser and
Ney, 1995) and vanilla LSTMs, trained with or
without batching.

The LSTM baselines already outperform the
syntactic feed-forward neural model of Buys and
Blunsom (2015b). We see that there is a sig-
nificant difference between training with or with-
out mini-batching for the baseline; similarly our
model’s perplexities also improve when trained

with batching. The batched baseline performs
slightly better than Recurrent Neural Network
Grammars (RNNG) (Dyer et al., 2016; Kuncoro
et al., 2017), a constituency syntax-based RNN
language model trained without batching.3

The results show that our syntactic language
models perform slightly worse than the LSTM
baseline. We experimented with different de-
pendency representations on the development set,
including SD, YM and Universal Dependen-
cies (Nivre et al., 2016). We found little difference
in language modelling performance between the
dependency representations. Unsupervised train-
ing does not lead to better perplexity than the su-
pervised models; however, due to much longer
training times we did less hyperparameter tuning
for the unsupervised model.

4.3 Analysis

We further analyze the probability distribution
that our model is learning by calculating some
perplexity-related quantities. We compare the per-
plexity of the marginal distribution p(w) to the
perplexity based only on the most likely transition
sequence a = argmax p(w,a), based on either the
joint distribution p(w,a) or the conditional distri-
bution p(w|a). Note that while the former is a
bound on the marginal perplexity, the latter is not a
true perplexity but simply helps us to quantify the
contribution of the syntactic structure to reducing
the uncertainty in the prediction.

The results (Table 6) show that the difference
between the joint and marginal perplexities are rel-
atively small for the supervised models, indicat-
ing that the distribution is very peaked around the
most likely parse trees. However the conditional
quantity shows that the syntactic structure encoded
by the stack-next model is much more informa-
tive than that of the buffer-next model, although
the only difference between them is the choice of
elements to condition on when predicting the next
word. Although the stack-next model has a better
marginal perplexity, the disadvantage is that it has
more uncertainty in the syntactic structure it is pre-
dicting (as can be seen by lower parsing accuracy)
even though that structure is more informative.

The strength of RNNG over our approach is
that it computes a compositional representation of

3Our experimental setup is the same as Dyer et al. (2016),
except for a minor implementation difference in unknown
word clustering; Dyer et al. (2016) reports 169.31 perplex-
ity on the same IKN model.

948



Model Argmax parse Argmax parse
Marginal ppl joint ppl conditional ppl

RNNG (Dyer et al., 2016) 104.10 107.58 41.60

Supervised (SD) shift-reduce buffer-next 111.53 120.09 102.28
Supervised (SD) shift-reduce stack-next 107.61 119.20 71.27
Unsupervised shift-reduce stack-next 125.20 350.01 169.87

Table 6: Language modelling perplexity analysis on the PTB test set.

the stack and the partially constructed parse tree,
while our model can only make use of the position
on top of the stack and otherwise has to rely on the
sequentially computed RNN representations. The
disadvantage of RNNG is that inference can only
be performed over entire sentences, as the pro-
posal distribution for their importance sampling
method is a discriminative parser. Exact inference
allows our models to estimate next word probabil-
ities from partially observed sequences.

5 Related work

5.1 Syntactic generative models

Chelba and Jelinek (2000) and Emami and Je-
linek (2005) proposed incremental syntactic lan-
guage models that predict binarized constituency
trees with a shift-reduce model, parameterized by
interpolated n-gram smoothing and feed-forward
neural networks, respectively. Language mod-
elling probabilities were approximated incremen-
tally using beam-search. Rastrow et al. (2012) ap-
plied a transition-based dependency n-gram lan-
guage model to speech recognition. These models
obtained perplexity improvements primarily when
interpolated with standard n-gram models, and
were not employed as parsers.

Henderson (2004) proposed an incremental
constituency parser based on recurrent neural net-
works that have additional connections to previous
recurrent states based on the parser configuration
at each time step. The generative version of this
model was more accurate than the discriminative
one. Titov and Henderson (2007) applied a similar
approach to dependency parsing. Buys and Blun-
som (2015a) and Buys and Blunsom (2015b) pro-
posed generative syntactic models that are applied
to both dependency parsing and language mod-
elling, using Bayesian and feed-forward neural
networks, respectively. Recurrent Neural Network
Grammar (RNNG) (Dyer et al., 2016) is a genera-

tive transition-based constituency parser based on
stack LSTMs (Dyer et al., 2015), that was also ap-
plied as a language model.

Recently, Shen et al. (2017) proposed an RNN-
based language model that uses a soft gating
mechanism to learn structure that can be inter-
preted as constituency trees, reporting strong lan-
guage modelling performance. There has also
been work on non-incremental syntactic language
modelling: Mirowski and Vlachos (2015) pro-
posed a dependency neural language model where
each word is conditioned on its ancestors in the
dependency tree, and showed that this model
achieves strong performance on a sentence com-
pletion task.

5.2 Neural models with latent structure

There have been a number of recent proposals for
neural abstract machines that augment RNNs with
external memory, including stacks and other data
structures that are operated on with differentiable
operations to enable end-to-end learning. Neural
Turing machines (Graves et al., 2014) have read-
write memory that is updated at each timestep.
Grefenstette et al. (2015) proposed a neural stack
that is operated on with differentiable push and
pop computations.

Another strand of recent work which our mod-
els are related to has proposed neural models
with structured latent variables: Rastogi et al.
(2016) incorporated neural context into weighted
finite-state transducers with a bidirectional RNN,
while Tran et al. (2016) proposed a neural hidden
Markov model for Part-of-Speech (POS) induc-
tion. Yu et al. (2016) proposed a neural transduc-
tion model with polynomial-time inference where
the alignment is a latent variable. Kim et al.
(2017) proposed structured attention mechanisms
that compute features by taking expectations over
latent structure. They define a tree-structured
model with a latent variable for head selection,

949



along with projectivity constraints. The soft head
selection learned by the model is used as features
in an attention-based decoder.

Reinforcement learning has been proposed to
learn compositional tree-based representations in
the context of an end task (Andreas et al., 2016;
Yogatama et al., 2016), but this approach has high
variance and provide no guarantees of finding op-
timal trees.

6 Conclusion

We proposed a new framework for generative
models of syntactic structure based on recurrent
neural networks. We presented efficient algo-
rithms for training these models with or without
supervision, and to apply them to make online
predictions for language modelling through exact
marginalization. Results show that the model ob-
tains state-of-the-art performance on supervised
generative dependency parsing, but does not ob-
tain better intrinsic language modelling perfor-
mance than a standard RNN.

Acknowledgments
We thank members of the Oxford NLP group for
discussions, Yejin Choi for valuable feedback, and
the anonymous reviewers for their comments.

References
Daniel Andor, Chris Alberti, David Weiss, Aliaksei

Severyn, Alessandro Presta, Kuzman Ganchev,
Slav Petrov, and Michael Collins. 2016. Globally
normalized transition-based neural networks. In
Proceedings of ACL. Association for Computational
Linguistics, Berlin, Germany, pages 2442–2452.
http://www.aclweb.org/anthology/
P16-1231.

Jacob Andreas, Marcus Rohrbach, Trevor Darrell, and
Dan Klein. 2016. Learning to compose neural
networks for question answering. In Proceedings
of NAACL. pages 1545–1554. http://www.
aclweb.org/anthology/N16-1181.

Yonatan Bisk and Julia Hockenmaier. 2015. Probing
the linguistic strengths and limitations of unsuper-
vised grammar induction. In Proceedings of ACL.
Beijing,China, pages 1395–1404.

Jan Buys and Phil Blunsom. 2015a. A Bayesian model
for generative transition-based dependency parsing.
In Proceedings of the 3rd International Conference
on Dependency Linguistics (Depling).

Jan Buys and Phil Blunsom. 2015b. Generative incre-
mental dependency parsing with neural networks. In

Proceedings of ACL-IJCNLP (short papers). pages
863–869.

Ciprian Chelba and Frederick Jelinek. 2000. Struc-
tured language modeling. Computer Speech & Lan-
guage 14(4):283–332.

Danqi Chen and Christopher D Manning. 2014. A fast
and accurate dependency parser using neural net-
works. In EMNLP.

Shay B. Cohen, Carlos Gómez-Rodrı́guez, and Giorgio
Satta. 2011. Exact inference for generative proba-
bilistic non-projective dependency parsing. In Pro-
ceedings of EMNLP. pages 1234–1245.

James Cross and Liang Huang. 2016. Incremental
parsing with minimal features using bi-directional
LSTM. In Proceedings of ACL. page 32.

Timothy Dozat and Christopher D. Manning. 2017.
Deep biaffine attention for neural dependency pars-
ing. In Proceedings of ICLR. http://arxiv.
org/abs/1611.01734.

Chris Dyer, Miguel Ballesteros, Wang Ling, Austin
Matthews, and Noah A. Smith. 2015. Transition-
based dependency parsing with stack long
short-term memory. In Proceedings of ACL.
pages 334–343. http://www.aclweb.org/
anthology/P15-1033.

Chris Dyer, Adhiguna Kuncoro, Miguel Ballesteros,
and Noah A. Smith. 2016. Recurrent neural network
grammars. In Proceedings of NAACL.

Jason Eisner. 2016. Inside-outside and forward-
backward algorithms are just backprop (tutorial pa-
per). In Proceedings of the Workshop on Struc-
tured Prediction for NLP. Association for Computa-
tional Linguistics, Austin, TX, pages 1–17. http:
//aclweb.org/anthology/W16-5901.

Ahmad Emami and Frederick Jelinek. 2005. A neu-
ral syntactic language model. Machine Learn-
ing 60(1-3):195–227. https://doi.org/10.
1007/s10994-005-0916-y.

Alex Graves, Greg Wayne, and Ivo Danihelka.
2014. Neural turing machines. arXiv preprint
arXiv:1410.5401 .

Edward Grefenstette, Karl Moritz Hermann, Mustafa
Suleyman, and Phil Blunsom. 2015. Learning to
transduce with unbounded memory. In Advances
in Neural Information Processing Systems. pages
1828–1836.

James Henderson. 2004. Discriminative training of
a neural network statistical parser. In Proceedings
of ACL. Association for Computational Linguistics,
page 95.

Sepp Hochreiter and Jürgen Schmidhuber. 1997.
Long short-term memory. Neural computation
9(8):1735–1780.

950



Liang Huang and Kenji Sagae. 2010. Dynamic pro-
gramming for linear-time incremental parsing. In
Proceedings of ACL. pages 1077–1086. http://
www.aclweb.org/anthology/P10-1110.

Yoon Kim, Carl Denton, Luong Hoang, and Alexan-
der M. Rush. 2017. Structured attention networks.
In Proceedings of ICLR.

Eliyahu Kiperwasser and Yoav Goldberg. 2016. Sim-
ple and accurate dependency parsing using bidirec-
tional lstm feature representations. Transactions
of the Association for Computational Linguistics
4:313–327.

Dan Klein and Christopher D Manning. 2004. Corpus-
based induction of syntactic structure: Models of de-
pendency and constituency. In ACL. pages 478–586.

Reinhard Kneser and Hermann Ney. 1995. Improved
backing-off for m-gram language modeling. In
ICASSP. IEEE, volume 1, pages 181–184.

Marco Kuhlmann, Carlos Gómez-Rodrı́guez, and Gior-
gio Satta. 2011. Dynamic programming algorithms
for transition-based dependency parsers. In Pro-
ceedings of ACL. pages 673–682.

Adhiguna Kuncoro, Miguel Ballesteros, Lingpeng
Kong, Chris Dyer, Graham Neubig, and Noah A.
Smith. 2017. What do recurrent neural network
grammars learn about syntax? In Proceedings of
EACL. Association for Computational Linguistics,
Valencia, Spain, pages 1249–1258. http://www.
aclweb.org/anthology/E17-1117.

Tal Linzen, Emmanuel Dupoux, and Yoav Goldberg.
2016. Assessing the ability of lstms to learn
syntax-sensitive dependencies. Transactions of the
Association for Computational Linguistics 4:521–
535. https://www.transacl.org/ojs/
index.php/tacl/article/view/972.

Tomáš Mikolov, Stefan Kombrink, Lukáš Burget, Jan
Černockỳ, and Sanjeev Khudanpur. 2011. Exten-
sions of recurrent neural network language model.
In ICASSP. IEEE, pages 5528–5531.

Piotr Mirowski and Andreas Vlachos. 2015. Depen-
dency recurrent neural language models for sen-
tence completion. CoRR abs/1507.01193. http:
//arxiv.org/abs/1507.01193.

Joakim Nivre. 2008. Algorithms for deterministic in-
cremental dependency parsing. Computational Lin-
guistics 34(4):513–553.

Joakim Nivre, Marie-Catherine de Marneffe, Filip Gin-
ter, Yoav Goldberg, Jan Hajič, Christopher D Man-
ning, Ryan McDonald, Slav Petrov, Sampo Pyysalo,
Natalia Silveira, et al. 2016. Universal dependencies
v1: A multilingual treebank collection. In Tenth In-
ternational Conference on Language Resources and
Evaluation (LREC 2016).

Pushpendre Rastogi, Ryan Cotterell, and Jason Eisner.
2016. Weighting finite-state transductions with neu-
ral context. In Proceedings of NAACL.

Ariya Rastrow, Mark Dredze, and Sanjeev Khu-
danpur. 2012. Efficient structured language
modeling for speech recognition. In INTER-
SPEECH. http://interspeech2012.org/
accepted-abstract.html?id=1436.

Yikang Shen, Zhouhan Lin, Chin-Wei Huang, and
Aaron C. Courville. 2017. Neural language model-
ing by jointly learning syntax and lexicon. CoRR
abs/1711.02013. http://arxiv.org/abs/
1711.02013.

Tianze Shi, Liang Huang, and Lillian Lee. 2017.
Fast(er) exact decoding and global training for
transition-based dependency parsing via a minimal
feature set. In Proceedings of the 2017 Conference
on Empirical Methods in Natural Language Pro-
cessing. Association for Computational Linguistics,
Copenhagen, Denmark, pages 12–23. https://
www.aclweb.org/anthology/D17-1002.

Stuart M Shieber, Yves Schabes, and Fernando CN
Pereira. 1995. Principles and implementation of de-
ductive parsing. The Journal of logic programming
24(1):3–36.

Valentin I Spitkovsky, Hiyan Alshawi, and Daniel Ju-
rafsky. 2010. From baby steps to leapfrog: how
less is more in unsupervised dependency parsing.
In Human Language Technologies: The 2010 An-
nual Conference of the North American Chapter
of the Association for Computational Linguistics.
pages 751–759.

Ivan Titov and James Henderson. 2007. A la-
tent variable model for generative dependency
parsing. In Proceedings of the Tenth In-
ternational Conference on Parsing Technologies.
pages 144–155. http://www.aclweb.org/
anthology/W/W07/W07-2218.

Ke M. Tran, Yonatan Bisk, Ashish Vaswani, Daniel
Marcu, and Kevin Knight. 2016. Unsupervised
neural hidden markov models. In Proceedings
of the Workshop on Structured Prediction for
NLP. Association for Computational Linguistics,
Austin, TX, pages 63–71. http://aclweb.
org/anthology/W16-5907.

Hiroyasu Yamada and Yuji Matsumoto. 2003. Statis-
tical dependency analysis with support vector ma-
chines. In Proceedings of the International Confer-
ence on Parsing Technologies. volume 3.

D. Yogatama, C. Dyer, W. Ling, and P. Blunsom.
2017. Generative and discriminative text classi-
fication with recurrent neural networks. CoRR
abs/1703.01898. http://arxiv.org/abs/
1703.01898.

951



Dani Yogatama, Phil Blunsom, Chris Dyer, Edward
Grefenstette, and Wang Ling. 2016. Learning to
compose words into sentences with reinforcement
learning. CoRR abs/1611.09100.

Lei Yu, Phil Blunsom, Chris Dyer, Edward Grefen-
stette, and Tomas Kocisky. 2017. The neural noisy
channel. Proceedings of ICLR .

Lei Yu, Jan Buys, and Phil Blunsom. 2016. Online seg-
ment to segment neural transduction. In Proceed-
ings of EMNLP. pages 1307–1316.

Wojciech Zaremba, Ilya Sutskever, and Oriol Vinyals.
2014. Recurrent neural network regularization.
CoRR abs/1409.2329. http://arxiv.org/
abs/1409.2329.

952


