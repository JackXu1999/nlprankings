



















































Higher-order Relation Schema Induction using Tensor Factorization with Back-off and Aggregation


Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers), pages 1575–1584
Melbourne, Australia, July 15 - 20, 2018. c©2018 Association for Computational Linguistics

1575

Higher-order Relation Schema Induction using Tensor Factorization with
Back-off and Aggregation

Madhav Nimishakavi
Indian Institute of Science

Bangalore
madhav@iisc.ac.in

Manish Gupta
Microsoft
Hyderabad

manishg@microsoft.com

Partha Talukdar
Indian Institute of Science

Bangalore
ppt@iisc.ac.in

Abstract

Relation Schema Induction (RSI) is the
problem of identifying type signatures
of arguments of relations from unlabeled
text. Most of the previous work in this
area have focused only on binary RSI,
i.e., inducing only the subject and object
type signatures per relation. However, in
practice, many relations are high-order,
i.e., they have more than two arguments
and inducing type signatures of all ar-
guments is necessary. For example, in
the sports domain, inducing a schema
win(WinningPlayer, OpponentPlayer,
Tournament, Location) is more informa-
tive than inducing just win(WinningPlayer,
OpponentPlayer). We refer to this prob-
lem as Higher-order Relation Schema
Induction (HRSI). In this paper, we
propose Tensor Factorization with Back-
off and Aggregation (TFBA), a novel
framework for the HRSI problem. To the
best of our knowledge, this is the first
attempt at inducing higher-order relation
schemata from unlabeled text. Using the
experimental analysis on three real world
datasets, we show how TFBA helps in
dealing with sparsity and induce higher
order schemata.

1 Introduction

Building Knowledge Graphs (KGs) out of un-
structured data is an area of active research. Re-
search in this has resulted in the construction of
several large scale KGs, such as NELL (Mitchell
et al., 2015), Google Knowledge Vault (Dong
et al., 2014) and YAGO (Suchanek et al., 2007).
These KGs consist of millions of entities and be-
liefs involving those entities. Such KG construc-

tion methods are schema-guided as they require
the list of input relations and their schemata (e.g.,
playerPlaysSport(Player, Sport)). In other words,
knowledge of schemata is an important first step
towards building such KGs.

While beliefs in such KGs are usually binary
(i.e., involving two entities), many beliefs of in-
terest go beyond two entities. For example, in the
sports domain, one may be interested in beliefs of
the form win(Roger Federer, Nadal, Wimbledon,
London), which is an instance of the high-order
(or n-ary) relation win whose schema is given
by win(WinningPlayer, OpponentPlayer, Tourna-
ment, Location). We refer to the problem of induc-
ing such relation schemata involving multiple ar-
guments as Higher-order Relation Schema Induc-
tion (HRSI). In spite of its importance, HRSI is
mostly unexplored.

Recently, tensor factorization-based methods
have been proposed for binary relation schema in-
duction (Nimishakavi et al., 2016), with gains in
both speed and accuracy over previously proposed
generative models. To the best of our knowledge,
tensor factorization methods have not been used
for HRSI. We address this gap in this paper.

Due to data sparsity, straightforward adaptation
of tensor factorization from (Nimishakavi et al.,
2016) to HRSI is not feasible, as we shall see in
Section 3.1. We overcome this challenge in this
paper, and make the following contributions.

• We propose Tensor Factorization with Back-
off and Aggregation (TFBA), a novel tensor
factorization-based method for Higher-order
RSI (HRSI). In order to overcome data spar-
sity, TFBA backs-off and jointly factorizes
multiple lower-order tensors derived from an
extremely sparse higher-order tensor.

• As an aggregation step, we propose a con-
strained clique mining step which constructs



1576

the higher-order schemata from multiple bi-
nary schemata.

• Through experiments on multiple real-world
datasets, we show the effectiveness of TFBA
for HRSI.

Source code of TFBA is available at https:
//github.com/madhavcsa/TFBA.

The remainder of the paper is organized as fol-
lows. We discuss related work in Section 2. In
Section 3.1, we first motivate why a back-off strat-
egy is needed for HRSI, rather than factorizing the
higher-order tensor. Further, we discuss the pro-
posed TFBA framework in Section 3.2. In Sec-
tion 4, we demonstrate the effectiveness of the pro-
posed approach using multiple real world datasets.
We conclude with a brief summary in Section 5.

2 Related Work

In this section, we discuss related works in two
broad areas: schema induction, and tensor and ma-
trix factorizations.

Schema Induction: Most work on inducing
schemata for relations has been in the binary set-
ting (Mohamed et al., 2011; Movshovitz-Attias
and Cohen, 2015; Nimishakavi et al., 2016). Mc-
Donald et al. (2005) and Peng et al. (2017) extract
n-ary relations from Biomedical documents, but
do not induce the schema, i.e., type signature of
the n-ary relations. There has been significant
amount of work on Semantic Role Labeling (Lang
and Lapata, 2011; Titov and Khoddam, 2015; Roth
and Lapata, 2016), which can be considered as n-
ary relation extraction. However, we are inter-
ested in inducing the schemata, i.e., the type signa-
ture of these relations. Event Schema Induction is
the problem of inducing schemata for events in the
corpus (Balasubramanian et al., 2013; Chambers,
2013; Nguyen et al., 2015). Recently, a model for
event representations is proposed in (Weber et al.,
2018).

Cheung et al. (2013) propose a probabilistic
model for inducing frames from text. Their no-
tion of frame is closer to that of scripts (Schank
and Abelson, 1977). Script learning is the pro-
cess of automatically inferring sequence of events
from text (Mooney and DeJong, 1985). There is
a fair amount of recent work in statistical script
learning (Pichotta and Mooney, 2016), (Pichotta
and Mooney, 2014). While script learning deals
with the sequence of events, we try to find the

schemata of relations at a corpus level. Ferraro and
Durme (2016) propose a unified Bayesian model
for scripts, frames and events. Their model tries to
capture all levels of Minsky Frame structure (Min-
sky, 1974), however we work with the surface se-
mantic frames.

Tensor and Matrix Factorizations: Matrix
factorization and joint tensor-matrix factorizations
have been used for the problem of predicting links
in the Universal Schema setting (Riedel et al.,
2013; Singh et al., 2015). Chen et al. (2015) use
matrix factorizations for the problem of finding
semantic slots for unsupervised spoken language
understanding. Tensor factorization methods are
also used in factorizing knowledge graphs (Chang
et al., 2014; Nickel et al., 2012). Joint matrix and
tensor factorization frameworks, where the ma-
trix provides additional information, is proposed
in (Acar et al., 2013) and (Wang et al., 2015).
These models are based on PARAFAC (Harsh-
man, 1970), a tensor factorization model which
approximates the given tensor as a sum of rank-
1 tensors. A boolean Tucker decomposition for
discovering facts is proposed in (Erdos and Miet-
tinen, 2013). In this paper, we use a modified ver-
sion (Tucker2) of Tucker decomposition (Tucker,
1963).

RESCAL (Nickel et al., 2011) is a simplified
Tucker model suitable for relational learning. Re-
cently, SICTF (Nimishakavi et al., 2016), a vari-
ant of RESCAL with side information, is used for
the problem of schema induction for binary rela-
tions. SICTF cannot be directly used to induce
higher order schemata, as the higher-order tensors
involved in inducing such schemata tend to be ex-
tremely sparse. TFBA overcomes these challenges
to induce higher-order relation schemata by per-
forming Non-Negative Tucker-style factorization
of sparse tensor while utilizing a back-off strategy,
as explained in the next section.

3 Higher Order Relation Schema
Induction using Back-off Factorization

In this section, we start by discussing the approach
of factorizing a higher-order tensor and provide
the motivation for back-off strategy. Next, we
discuss the proposed TFBA approach in detail.
Please refer to Table 1 for notations used in this
paper.

https://github.com/madhavcsa/TFBA
https://github.com/madhavcsa/TFBA


1577

Notation Definition
R+ Set of non-negative reals.
X ∈ Rn1×n2×...×nN+ N th -order non-negative tensor.
X(i) mode-i matricization of tensor X . Please see (Kolda and Bader, 2009) for details.
A ∈ Rn×r+ Non-negative matrix of order n× r.
∗ Hadamard product: (A ∗ B)i,j = Ai,j × Bi,j .

Table 1: Notations used in the paper.

Figure 1: Overview of Step 1 of TFBA. Rather than factorizing the higher-order tensor X , TFBA
performs joint Tucker decomposition of multiple 3-mode tensors, X 1, X 2, and X 3, derived out of X .
This joint factorization is performed using shared latent factors A, B, and C. This results in binary
schemata, each of which is stored as a cell in one of the core tensors G1, G2, and G3. Please see Section
3.2.1 for details.

3.1 Factorizing a Higher-order Tensor

Given a text corpus, we use OpenIEv5 (Mausam,
2016) to extract tuples. Consider the following
sentence “Federer won against Nadal at Wimble-
don.”. Given this sentence, OpenIE extracts the
4-tuple (Federer, won, against Nadal, at Wimble-
don). We lemmatize the relations in the tuples and
only consider the noun phrases as arguments. Let
T represent the set of these 4-tuples. We can con-
struct a 4-order tensor X ∈ Rn1×n2×n3×m+ from
T. Here, n1 is the number of subject noun phrases
(NPs), n2 is the number of object NPs, n3 is the
number of other NPs, and m is the number of re-
lations in T. Values in the tensor correspond to the
frequency of the tuples. In case of 5-tuples of the
form (subject, relation, object, other-1, other-2),
we split the 5-tuples into two 4-tuples of the form
(subject, relation, object, other-1) and (subject, re-
lation, object, other-2) and frequency of these 4-
tuples is considered to be same as the original 5-
tuple. Factorizing the tensor X results in discov-
ering latent categories of NPs, which help in in-

ducing the schemata. We propose the following
approach to factorize X .

min
G,A,B,C

‖X − G ×1 A×2 B×3 C×4 I‖2F

+ λa ‖A‖2F + λb ‖B‖
2
F + λc ‖C‖

2
F ,

where,

A ∈ Rn1×r1+ ,B ∈ R
n2×r2
+ ,C ∈ R

n3×r3
+ ,

G ∈ Rr1×r2×r3×m+ , λa ≥ 0, λb ≥ 0 and λc ≥ 0.

Here, I is the identity matrix. Non-negative up-
dates for the variables can be obtained following
(Lee and Seung, 2000). Similar to (Nimishakavi
et al., 2016), schemata induced will be of the form
relation 〈Ai,Bj ,Ck〉. Here, Pi represents the ith
column of a matrix P. A is the embedding matrix
of subject NPs in T (i.e., mode-1 of X ), r1 is the
embedding rank in mode-1 which is the number of
latent categories of subject NPs. Similarly, B and



1578

Figure 2: Overview of Step 2 of TFBA. Induction
of higher-order schemata from the tri-partite graph
formed from the columns of matrices A, B, and
C. Triangles in this graph (solid) represent a 3-ary
schema, n-ary schemata for n > 3 can be induced
from the 3-ary schemata. Please refer to Section
3.2.2 for details.

C are the embedding matrices of object NPs and
other NPs respectively. r2 and r3 are the number
of latent categories of object NPs and other NPs
respectively. G is the core tensor. λa, λb and λc
are the regularization weights.

However, the 4-order tensors are heavily sparse
for all the datasets we consider in this work. The
sparsity ratio of this 4-order tensor for all the
datasets is of the order 1e-7. As a result of the
extreme sparsity, this approach fails to learn any
schemata. Therefore, we propose a more success-
ful back-off strategy for higher-order RSI in the
next section.

3.2 TFBA: Proposed Framework
To alleviate the problem of sparsity, we construct
three tensors X 3, X 2, and X 1 from T as follows:

• X 3 ∈ Rn1×n2×m+ is constructed out of the
tuples in T by dropping the other argu-
ment and aggregating resulting tuples, i.e.,
X 3i,j,p =

∑n3
k=1Xi,j,k,p. For example, 4-

tuples 〈(Federer, Win, Nadal, Wimbledon),
10〉 and 〈(Federer, Win, Nadal, Australian
Open), 5〉 will be aggregated to form a triple
〈(Federer, Win, Nadal), 15〉.

• X 2 ∈ Rn1×n3×m+ is constructed out of the
tuples in T by dropping the object argument

and aggregating resulting tuples i.e., X 2i,j,p =∑n2
k=1Xi,k,j,p.

• X 1 ∈ Rn2×n3×m+ constructed out of the tu-
ples in T by dropping the subject argument
and aggregating resulting tuples i.e., X 1i,j,p =∑n1

k=1Xk,i,j,p.

The proposed framework TFBA for inducing
higher order schemata involves the following two
steps.

• Step 1: In this step, TFBA factorizes multi-
ple lower-order overlapping tensors, X 1, X 2,
and X 3, derived from X to induce binary
schemata. This step is illustrated in Figure
1 and we discuss details in Section 3.2.1.

• Step 2: In this step, TFBA connects multiple
binary schemata identified above to induce
higher-order schemata. The method accom-
plishes this by solving a constrained clique
problem. This step is illustrated in Figure 2
and we discuss the details in Section 3.2.2.

3.2.1 Step 1: Back-off Tensor Factorization
A schematic overview of this step is shown in Fig-
ure 1. TFBA first preprocesses the corpus and ex-
tracts OpenIE tuple set T out of it. The 4-mode
tensor X is constructed out of T. Instead of per-
forming factorization of the higher-order tensor X
as in Section 3.1, TFBA creates three tensors out
of X : X 1n2×n3×m,X

2
n1×n3×m and X

3
n1×n2×m.

TFBA performs a coupled non-negative Tucker
factorization of the input tensors X 1,X 2 and X 3
by solving the following optimization problem.

min
A,B,C

G1,G2,G3

f(X 3,G3,A,B) + f(X 2,G2,A,C)

+ f(X 1,G1,B,C)
+ λa ‖A‖2F + λb ‖B‖

2
F + λc ‖C‖

2
F , (1)

where,

f(X i,Gi,P,Q) =
∥∥X i − Gi ×1 P×2 Q×3 I∥∥2F

A ∈ Rn1×r1+ ,B ∈ R
n2×r2
+ ,C ∈ R

n3×r3
+

G1 ∈ Rr2×r3×m+ ,G2 ∈ R
r1×r3×m
+ ,G3 ∈ R

r1×r2×m
+ .

We enforce non-negativity constraints on the ma-
trices A,B,C and the core tensors Gi (i ∈
{1, 2, 3}). Non-negativity is essential for learning
interpretable latent factors (Murphy et al., 2012).



1579

Each slice of the core tensor G3 corresponds to
one of the m relations. Each cell in a slice cor-
responds to an induced schema in terms of the
latent factors from matrices A and B. In other
words, G3i,j,k is an induced binary schema for re-
lation k involving induced categories represented
by columns Ai and Bj . Cells in G1 and G2 may be
interpreted accordingly.

We derive non-negative multiplicative updates
for A,B and C following the NMF updating rules
given in (Lee and Seung, 2000). For the update of
A, we consider the mode-1 matricization of first
and the second term in Equation 1 along with the
regularizer.

A ← A ∗
X 3(1)G

>
BA

+X 2(1)G
>
CA

A[GBAG>BA + GCAG
>
CA

] + λaA
,

where,

GBA = (G
3 ×2 B)(1), GCA = (G

2 ×2 C)(1).

In order to estimate B, we consider mode-2 ma-
tricization of first term and mode-1 matricization
of third term in Equation 1, along with the regu-
larization term. We get the following update rule
for B

B ← B ∗
X 3(2)G

>
AB

+X 1(1)G
>
CB

B[GABG>AB + GCBG
>
CB

] + λbB
,

where,

GAB = (G
3 ×1 A)(2), GCB = (G

1 ×2 C)(1).

For updating C, we consider mode-2 matriciza-
tion of second and third terms in Equation 1 along
with the regularization term, and we get

C ← C ∗
X 3(2)G

>
BC

+X 2(2)G
>
AC

C[GACG>AC + GBCG
>
BC

] + λcC
,

where,

GAC = (G
3 ×1 B)(2), GBC = (G

2 ×1 A)(2).

Finally, we update the three core tensors in
Equation 1 following (Kim and Choi, 2007) as fol-
lows,

G1 ← G1 ∗ X
1 ×1 B> ×2 C>

G1 ×1 B>B×2 C>C
,

G2 ← G2 ∗ X
2 ×1 A> ×2 C>

G2 ×1 A>A×2 C>C
,

G3 ← G3 ∗ X
3 ×1 A> ×2 B>

G3 ×1 A>A×2 B>B
.

In all the above updates, PQ represents element-
wise division and I is the identity matrix.

Initialization: For initializing the component
matrices A,B, and C, we first perform a non-
negative Tucker2 Decomposition of the individ-
ual input tensors X 1,X 2, and X 3. Then com-
pute the average of component matrices obtained
from each individual decomposition for initializa-
tion. We initialize the core tensors G1,G2, and G3
with the core tensors obtained from the individual
decompositions.

3.2.2 Step 2: Binary to Higher-Order
Schema Induction

In this section, we describe how a higher-order
schema is constructed from the factorization de-
scribed in the previous sub-section. Each relation
k has three representations given by the slices G1k ,
G2k and G3k from each core tensor. We need a prin-
cipled way to produce a joint schema from these
representations. For a relation, we select top-n in-
dices (i, j) with highest values from each matrix.
The indices i and j from G3k correspond to column
numbers of A and B respectively, indices from G2k
correspond to columns from A and C and columns
from G1k correspond to columns from B and C.

We construct a tri-partite graph with the col-
umn numbers from each of the component matri-
ces A, B and C as the vertices belonging to in-
dependent sets, the top-n indices selected are the
edges between these vertices. From this tri-partite
graph, we find all the triangles which will give
schema with three arguments for a relation, illus-
trated in Figure 2. We find higher order schemata,
i.e., schemata with more than three arguments by
merging two third order schemata with same col-
umn number from A and B. For example, if we
find two schemata (A2,B4,C10) and (A2,B4,C8)
then we merge these two to give (A2,B4,C10,C8)
as a higher order schema. This can be continued
further for even higher order schemata. This pro-
cess may be thought of as finding a constrained



1580

clique over the tri-partite graph. Here the con-
straint is that in the maximal clique, there can
only be one edge between sets corresponding to
columns of A and columns of B.

The procedure above is inspired by (McDonald
et al., 2005). However, we note that (McDonald
et al., 2005) solved a different problem, viz., n-ary
relation instance extraction, while our focus is on
inducing schemata. Though we discuss the case of
back-off from 4-order to 3-order, ideas presented
above can be extended for even higher orders de-
pending on the sparsity of the tensors.

4 Experiments

In this section, we evaluate the performance of
TFBA for the task of HRSI. We also propose a
baseline model for HRSI called HardClust.
HardClust: We propose a baseline model called
the Hard Clustering Baseline (HardClust) for the
task of higher order relation schema induction.
This model induces schemata by grouping per-
relation NP arguments from OpenIE extractions.
In other words, for each relation, all the Noun
Phrases (NPs) in first argument form a cluster that
represents the subject of the relation, all the NPs
in the second argument form a cluster that repre-
sents object and so on. Then from each cluster,
the top most frequent NPs are chosen as the repre-
sentative NPs for the argument type. We note that
this method is only able to induce one schema per
relation.

Datasets: We run our experiments on three
datasets. The first dataset (Shootings) is a collec-
tion of 1,335 documents constructed from a pub-
licly available database of mass shootings in the
United States. The second is New York Times
Sports (NYT Sports) dataset which is a collection
of 20,940 sports documents from the period 2005
and 2007. And the third dataset (MUC) is a set of
1300 Latin American newswire documents about
terrorism events. After performing the processing
steps described in Section 3, we obtained 357,914
unique OpenIE extractions from the NYT Sports
dataset, 10,847 from Shootings dataset, and 8,318
from the MUC dataset. However, in order to prop-
erly analyze and evaluate the model, we consider
only the 50 most frequent relations in the datasets
and their corresponding OpenIE extractions. This
is done to avoid noisy OpenIE extractions to yield
better data quality and to aid subsequent manual
evaluation of the data. We construct input tensors

following the procedure described in Section 3.2.
Details on the dimensions of tensors obtained are
given in Table 2.

Model Selection: In order to select appropriate
TFBA parameters, we perform a grid search over
the space of hyper-parameters, and select the set of
hyper-parameters that give best Average FIT score
(AvgFIT).

AvgFIT(G1,G2,G3,A,B,C,X 1,X 2,X 3) =
1

3
{FIT(X 1,G1,B,C) + FIT(X 2,G2,A,C)

+ FIT(X 3,G3,A,B)},

where,

FIT(X ,G,P,Q) = 1−‖X − G ×1 P×2 Q‖F
‖X‖F

.

We perform a grid search for the rank param-
eters between 5 and 20, for the regularization
weights we perform a grid search over 0 and 1.
Table 3 provides the details of hyper-parameters
set for different datasets.

Evaluation Protocol: For TFBA, we follow
the protocol mentioned in Section 3.2.2 for con-
structing higher order schemata. For every rela-
tion, we consider top 5 binary schemata from the
factorization of each tensor. We construct a tri-
partite graph, as explained in Section 3.2.2, and
mine constrained maximal cliques from the tripar-
tite graphs for schemata. Table 4 provides some
qualitative examples of higher-order schemata in-
duced by TFBA. Accuracy of the schemata in-
duced by the model is evaluated by human evalua-
tors. In our experiments, we use human judgments
from three evaluators. For every relation, the first
and second columns given in Table 4 are presented
to the evaluators and they are asked to validate the
schema. We present top 50 schemata based on the
score of the constrained maximal clique induced
by TFBA to the evaluators. This evaluation proto-
col was also used in (Movshovitz-Attias and Co-
hen, 2015) for evaluating ontology induction. All
evaluations were blind, i.e., the evaluators were
not aware of the model they were evaluating.

Difficulty with Computing Recall: Even
though recall is a desirable measure, due to the
lack of availability of gold higher-order schema
annotated corpus, it is not possible to compute re-
call. Although the MUC dataset has gold annota-
tions for some predefined list of events, it does not
have annotations for the relations.



1581

Dataset X 1shape X 2shape X 3shape
Shootings 3365× 1295× 50 2569× 1295× 50 2569× 3365× 50
NYT Sports 57, 820× 20, 356× 50 49, 659× 20, 356× 50 49, 659× 57, 820× 50
MUC 2825× 962× 50 2555× 962× 50 2555× 2825× 50

Table 2: Details of dimensions of tensors constructed for each dataset used in the experiments.

Dataset (r1, r2, r3) (λa, λb, λc)
Shootings (10, 20,15) (0.3, 0.1, 0.7)
NYT Sports (20, 15, 15) (0.9, 0.5, 0.7)
MUC (15, 12, 12) (0.7, 0.7, 0.4)

Table 3: Details of hyper-parameters set for dif-
ferent datasets.

Experimental results comparing performance of
various models for the task of HRSI are given in
Table 5. We present evaluation results from three
evaluators represented as E1, E2 and E3. As can
be observed from Table 5, TFBA achieves better
results than HardClust for the Shootings and NYT
Sports datasets, however HardClust achieves bet-
ter results for the MUC dataset. Percentage agree-
ment of the evaluators for TFBA is 72%, 70%
and 60% for Shootings, NYT Sports and MUC
datasets respectively.

HardClust Limitations: Even though Hard-
Clust gives better induction for MUC corpus, this
approach has some serious drawbacks. HardClust
can only induce one schema per relation. This is
a restrictive constraint as multiple senses can exist
for a relation. For example, consider the schemata
induced for the relation shoot as shown in Table
4. TFBA induces two senses for the relation, but
HardClust can induce only one schema. For a
set of 4-tuples, HardClust can only induce ternary
schemata; the dimensionality of the schemata can-
not be varied. Since the latent factors induced
by HardClust are entirely based on frequency, the
latent categories induced by HardClust are dom-
inated by only a fixed set of noun phrases. For
example, in NYT Sports dataset, subject category
induced by HardClust for all the relations is 〈team,
yankees, mets〉. In addition to inducing only one
schema per relation, most of the times HardClust
only induces a fixed set of categories. Whereas for
TFBA, the number of categories depends on the
rank of factorization, which is a user provided pa-
rameter, thus providing more flexibility to choose
the latent categories.

4.1 Using Event Schema Induction for HRSI

Event schema induction is defined as the task of
learning high-level representations of events, like
a tournament, and their entity roles, like winning-
player etc, from unlabeled text. Even though the
main focus of event schema induction is to induce
the important roles of the events, as a side result
most of the algorithms also provide schemata for
the relations. In this section, we investigate the
effectiveness of these schemata compared to the
ones induced by TFBA.

Event schemata are represented as a set of (Ac-
tor, Rel, Actor) triples in (Balasubramanian et al.,
2013). Actors represent groups of noun phrases
and Rels represent relations. From this style of
representation, however, the n-ary schemata for re-
lations cannot be induced. Event schemata gen-
erated in (Weber et al., 2018) are similar to that
in (Balasubramanian et al., 2013). Event schema
induction algorithm proposed in (Nguyen et al.,
2015) doesn’t induce schemata for relations, but
rather induces the roles for the events. For this
investigation we experiment with the following al-
gorithm.

Chambers-13 (Chambers, 2013): This model
learns event templates from text documents. Each
event template provides a distribution over slots,
where slots are clusters of NPs. Each event tem-
plate also provides a cluster of relations, which is
most likely to appear in the context of the afore-
mentioned slots. We evaluate the schemata of
these relation clusters.

As can be observed from Table 5, the proposed
TFBA performs much better than Chambers-13.
HardClust also performs better than Chambers-13
on all the datasets. From this analysis we infer
that there is a need for algorithms which induce
higher-order schemata for relations, a gap we fill
in this paper. Please note that the experimental
results provided in (Chambers, 2013) for MUC
dataset are for the task of event schema induction,
but in this work we evaluate the relation schemata.
Hence the results in (Chambers, 2013) and re-
sults in this paper are not comparable. Example



1582

Relation Schema NPs from the induced categories Evaluator Judgment (Human) Suggested Label
Shootings

leave〈A6, B0, C7〉
A6: shooting, shooting incident, double shooting

valid
< shooting >

B0: one person, two people, three people < people >
C7: dead, injured, on edge <injured >

identify〈A1, B1, C5, C6〉

A1: police, officers, huntsville police

valid

< police >
B1: man, victims, four victims < victim(s)>
C5: sunday, shooting staurday, wednesday afternoon <day/time >
C6: apartment, bedroom, building in the neighborhood <place >

shoot〈A7, B6, C1〉
A7: gunman, shooter, smith

valid
< perpetrator >

B6: freeman, slain woman, victims <victim >
C1: friday, friday night, early monday morning < time>

shoot〈A4, B2, C13〉
A4: <num>-year-old man, <num>-year-old george reavis,
<num>-year-old brockton man valid

< victim>

B2: in the leg, in the head, in the neck < body part>
C13: in macon, in chicago, in an alley < location >

say〈A1, B1, C5〉
A1: police, officers, huntsville police

invalid –B1: man, victims, four victims
C5: sunday, shooting staurday, wednesday afternoon

NYT sports

spend〈A0, B16, C3〉
A0: yankees, mets, jets

valid
< team >

B14: $ <num> million, $ <num>, $ <num> billion < money >
C3: <num>, year, last season < year >

win〈A2, B10, C3〉
A2: red sox, team, yankees

valid
< team >

B10: world series, title, world cup < championship >
C3: <num>, year, last season < year >

get〈A4, B4, C1〉
A4: umpire, mike cameron, andre agassi

invalid –B4: ball, lives, grounder
C1: back, forward, <num>-yard line

MUC

tell〈A7, B2, C0〉
A7: medardo gomez, jose azcona, gregorio roza chavez

valid
< politician >

B2: media, reporters, newsmen <media >
C0: today, at <num>, tonight < day/time >

occur〈A9, B5, C10〉
A9: bomb, blast, explosion

valid
< bombing >

B5: near san salvador, here in madrid, in the same office < place >
C10: at <num>, this time, simultaneously < time >

suffer〈A5, B4, C4)
A5: justice maria elena diaz, vargas escobar, judge sofia de roldan

invalid –B4: casualties , car bomb, grenade
C4: settlement of refugees, in san roman, now

Table 4: Examples of schemata induced by TFBA. Please note that some of them are 3-ary while others
are 4-ary. For details about schema induction, please refer to Section 3.2.

Shootings NYT Sports MUC
E1 E2 E3 Avg E1 E2 E3 Avg E1 E2 E3 Avg

HardClust 0.64 0.70 0.64 0.66 0.42 0.28 0.52 0.46 0.64 0.58 0.52 0.58
Chambers-13 0.32 0.42 0.28 0.34 0.08 0.02 0.04 0.07 0.28 0.34 0.30 0.30

TFBA 0.82 0.78 0.68 0.76 0.86 0.6 0.64 0.70 0.58 0.38 0.48 0.48

Table 5: Higher-order RSI accuracies of various methods on the three datasets. Induced schemata for
each dataset and method are evaluated by three human evaluators, E1, E2, and E3. TFBA performs
better than HardClust for Shootings and NYT Sports datasets. Even though HardClust achieves better
accuracy on MUC dataset, it has several limitations, see Section 4 for more details. Chambers-13 solves
a slightly different problem called event schema induction, for more details about the comparison with
Chambers-13 see Section 4.1.

schemata induced by TFBA and (Chambers-13)
are provided as part of the supplementary mate-
rial.

5 Conclusion

Higher order Relation Schema Induction (HRSI)
is an important first step towards building domain-
specific Knowledge Graphs (KGs). In this pa-
per, we proposed TFBA, a tensor factorization-
based method for higher-order RSI. To the best
of our knowledge, this is the first attempt at in-
ducing higher-order (n-ary) schemata for relations
from unlabeled text. Rather than factorizing a

severely sparse higher-order tensor directly, TFBA
performs back-off and jointly factorizes multiple
lower-order tensors derived out of the higher-order
tensor. In the second step, TFBA solves a con-
strained clique problem to induce schemata out
of multiple binary schemata. We are hopeful that
the backoff-based factorization idea exploited in
TFBA will be useful in other sparse factorization
settings.



1583

Acknowledgment

We thank the anonymous reviewers for their in-
sightful comments and suggestions. This research
has been supported in part by the Ministry of Hu-
man Resource Development (Government of In-
dia), Accenture, and Google.

References
Evrim Acar, Morten Arendt Rasmussen, Francesco Sa-

vorani, Tormod Ns, and Rasmus Bro. 2013. Under-
standing data fusion within the framework of cou-
pled matrix and tensor factorizations. Chemometrics
and Intelligent Laboratory Systems 129:53–63.

Niranjan Balasubramanian, Stephen Soderland,
Mausam, and Oren Etzioni. 2013. Generating
coherent event schemas at scale. In EMNLP.

Nathanael Chambers. 2013. Event schema induction
with a probabilistic entity-driven model. In EMNLP.

Kai-Wei Chang, Wen tau Yih, Bishan Yang, and
Christopher Meek. 2014. Typed tensor decompo-
sition of knowledge bases for relation extraction. In
EMNLP.

Yun-Nung Chen, William Yang Wang, Anatole Gersh-
man, and Alexander I. Rudnicky. 2015. Matrix fac-
torization with knowledge graph propagation for un-
supervised spoken language understanding. In ACL.

Jackie Chi Kit Cheung, Hoifung Poon, and Lucy Van-
derwende. 2013. Probabilistic frame induction. In
NAACL-HLT .

Xin Dong, Evgeniy Gabrilovich, Geremy Heitz, Wilko
Horn, Ni Lao, Kevin Murphy, Thomas Strohmann,
Shaohua Sun, and Wei Zhang. 2014. Knowledge
vault: A web-scale approach to probabilistic knowl-
edge fusion. In KDD.

Dora Erdos and Pauli Miettinen. 2013. Discovering
facts with boolean tensor tucker decomposition. In
CIKM.

Francis Ferraro and Benjamin Van Durme. 2016. A
unified bayesian model of scripts, frames and lan-
guage. In AAAI.

R. A. Harshman. 1970. Foundations of the PARAFAC
procedure: Models and conditions for an” explana-
tory” multi-modal factor analysis. UCLA Working
Papers in Phonetics 16(1):84.

Yong-Deok Kim and Seungjin Choi. 2007. Nonnega-
tive tucker decomposition. In CVPR.

Tamara G Kolda and Brett W Bader. 2009. Ten-
sor decompositions and applications. SIAM review
51(3):455–500.

Joel Lang and Mirella Lapata. 2011. Unsupervised se-
mantic role induction via split-merge clustering. In
NAACL-HLT .

Daniel D. Lee and H. Sebastian Seung. 2000. Al-
gorithms for non-negative matrix factorization. In
NIPS.

Mausam. 2016. Open information extraction systems
and downstream applications. In IJCAI.

Ryan McDonald, Fernando Pereira, Seth Kulick, Scott
Winters, Yang Jin, and Pete White. 2005. Simple
algorithms for complex relation extraction with ap-
plications to biomedical ie. In ACL.

Marvin Minsky. 1974. A framework for representing
knowledge. Technical report.

T. Mitchell, W. Cohen, E. Hruschka, P. Talukdar, J. Bet-
teridge, A. Carlson, B. Dalvi, M. Gardner, B. Kisiel,
J. Krishnamurthy, N. Lao, K. Mazaitis, T. Mohamed,
N. Nakashole, E. Platanios, A. Ritter, M. Samadi,
B. Settles, R. Wang, D. Wijaya, A. Gupta, X. Chen,
A. Saparov, M. Greaves, and J. Welling. 2015.
Never-ending learning. In AAAI.

Thahir P. Mohamed, Jr. Estevam R. Hruschka, and
Tom M. Mitchell. 2011. Discovering relations be-
tween noun categories. In EMNLP.

Raymond Mooney and Gerald DeJong. 1985. Learning
schemata for natural language processing. In IJCAI.

Dana Movshovitz-Attias and William W. Cohen. 2015.
Kb-lda: Jointly learning a knowledge base of hierar-
chy, relations, and facts. In ACL.

Brian Murphy, Partha Talukdar, and Tom Mitchell.
2012. Learning effective and interpretable seman-
tic models using non-negative sparse embedding. In
COLING.

Kiem-Hieu Nguyen, Xavier Tannier, Olivier Ferret,
and Romaric Besançon. 2015. Generative event
schema induction with entity disambiguation. In
ACL.

Maximilian Nickel, Volker Tresp, and Hans-Peter
Kriegel. 2011. A three-way model for collective
learning on multi-relational data. In ICML.

Maximilian Nickel, Volker Tresp, and Hans-Peter
Kriegel. 2012. Factorizing yago: Scalable machine
learning for linked data. In WWW.

Madhav Nimishakavi, Uday Singh Saini, and Partha
Talukdar. 2016. Relation schema induction us-
ing tensor factorization with side information. In
EMNLP.

Nanyun Peng, Hoifung Poon, Chris Quirk, Kristina
Toutanova, and Wen-tau Yih. 2017. Cross-sentence
n-ary relation extraction with graph lstms. TACL
5:101–115.



1584

Karl Pichotta and Raymond J. Mooney. 2014. Statis-
tical script learning with multi-argument events. In
EACL.

Karl Pichotta and Raymond J. Mooney. 2016. Learn-
ing statistical scripts with lstm recurrent neural net-
works. In AAAI.

Sebastian Riedel, Limin Yao, Andrew McCallum, and
Benjamin M. Marlin. 2013. Relation extraction
with matrix factorization and universal schemas. In
NAACL-HLT .

Michael Roth and Mirella Lapata. 2016. Neural se-
mantic role labeling with dependency path embed-
dings. In ACL.

R. Schank and R. Abelson. 1977. Scripts, plans, goals
and understanding: An inquiry into human knowl-
edge structures. Lawrence Erlbaum Associates,
Hillsdale, NJ.

Sameer Singh, Tim Rocktäschel, and Sebastian Riedel.
2015. Towards Combined Matrix and Tensor Fac-
torization for Universal Schema Relation Extraction.
In NAACL Workshop on Vector Space Modeling for
NLP (VSM).

Fabian M Suchanek, Gjergji Kasneci, and Gerhard
Weikum. 2007. Yago: a core of semantic knowl-
edge. In WWW.

Ivan Titov and Ehsan Khoddam. 2015. Unsupervised
induction of semantic roles within a reconstruction-
error minimization framework. In NAACL-HLT .

L. R. Tucker. 1963. Implications of factor analysis of
three-way matrices for measurement of change. In
Problems in measuring change., University of Wis-
consin Press, Madison WI, pages 122–137.

Yichen Wang, Robert Chen, Joydeep Ghosh, Joshua C.
Denny, Abel N. Kho, You Chen, Bradley A. Malin,
and Jimeng Sun. 2015. Rubik: Knowledge guided
tensor factorization and completion for health data
analytics. In KDD.

Noah Weber, Niranjan Balasubramanian, and
Nathanael Chambers. 2018. Event representa-
tions with tensor-based compositions. In AAAI.


