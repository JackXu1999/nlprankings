



















































Learning Bilingual Sentence Embeddings via Autoencoding and Computing Similarities with a Multilayer Perceptron


Proceedings of the 4th Workshop on Representation Learning for NLP (RepL4NLP-2019), pages 61–71
Florence, Italy, August 2, 2019. c©2019 Association for Computational Linguistics

61

Learning Bilingual Sentence Embeddings via Autoencoding
and Computing Similarities with a Multilayer Perceptron

Yunsu Kim2 Hendrik Rosendahl1,2 Nick Rossenbach2
Jan Rosendahl2 Shahram Khadivi1 Hermann Ney2

1eBay, Inc., Aachen, Germany
{hrosendahl,skhadivi}@ebay.com

2RWTH Aachen University, Aachen, Germany
{surname}@cs.rwth-aachen.de

Abstract

We propose a novel model architecture and
training algorithm to learn bilingual sentence
embeddings from a combination of parallel
and monolingual data. Our method connects
autoencoding and neural machine translation
to force the source and target sentence embed-
dings to share the same space without the help
of a pivot language or an additional trans-
formation. We train a multilayer perceptron
on top of the sentence embeddings to extract
good bilingual sentence pairs from nonparallel
or noisy parallel data. Our approach shows
promising performance on sentence align-
ment recovery and the WMT 2018 parallel
corpus filtering tasks with only a single model.

1 Introduction

Data crawling is increasingly important in ma-
chine translation (MT), especially for neural net-
work models. Without sufficient bilingual data,
neural machine translation (NMT) fails to learn
meaningful translation parameters (Koehn and
Knowles, 2017). Even for high-resource language
pairs, it is common to augment the training data
with web-crawled bilingual sentences to improve
the translation performance (Bojar et al., 2018).

Using crawled data in MT typically involves
two core steps: mining and filtering. Mining
parallel sentences, i.e. aligning source and tar-
get sentences, is usually done with lots of heuris-
tics and features: document/URL meta infor-
mation (Resnik and Smith, 2003; Esplá-Gomis
and Forcada, 2009), sentence lengths with self-
induced lexicon (Moore, 2002; Varga et al., 2005;
Etchegoyhen and Azpeitia, 2016), word alignment
statistics and linguistic tags (S. tefănescu et al.,
2012; Kaufmann, 2012).

Filtering aligned sentence pairs also often in-
volves heavy feature engineering (Taghipour et al.,

2011; Xu and Koehn, 2017). Most of the partici-
pants in the WMT 2018 parallel corpus filtering
task use large-scale neural MT models and lan-
guage models as the features (Koehn et al., 2018).

Bilingual sentence embeddings can be an ele-
gant and unified solution for parallel corpus min-
ing and filtering. They compress the information
of each sentence into a single vector, which lies
in a shared space between source and target lan-
guages. Scoring a source-target sentence pair is
done by computing similarity between the source
embedding vector and the target embedding vec-
tor. It is much more efficient than scoring by de-
coding, e.g. with a translation model.

Bilingual sentence embeddings have been stud-
ied primarily for transfer learning of monolingual
downstream tasks across languages (Hermann and
Blunsom, 2014; Pham et al., 2015; Zhou et al.,
2016). However, few papers apply it to bilin-
gual corpus mining; many of them require paral-
lel training data with additional pivot languages
(Espana-Bonet et al., 2017; Schwenk, 2018) or
lack an investigation into similarity between the
embeddings (Guo et al., 2018).

This work solves these issues as follows:

• We propose a simple end-to-end training
approach of bilingual sentence embeddings
with parallel and monolingual data only of
the corresponding language pair.

• We use a multilayer perceptron (MLP) as a
trainable similarity measure to match source
and target sentence embeddings.

• We compare various similarity measures for
embeddings in terms of score distribution,
geometric interpretation, and performance in
downstream tasks.

• We demonstrate competitive performance in
sentence alignment recovery and parallel cor-



62

pus filtering tasks without a complex combi-
nation of translation/language models.

• We analyze the effect of negative examples
on training an MLP similarity, using different
levels of negativity.

2 Related Work

Bilingual representation of a sentence was at first
built by averaging pre-trained bilingual word em-
beddings (Huang et al., 2012; Klementiev et al.,
2012). The compositionality from words to sen-
tences is integrated into end-to-end training in
Hermann and Blunsom (2014).

Explicit modeling of a sentence-level bilingual
embedding was first discussed in Chandar et al.
(2013), training an autoencoder on monolingual
sentence embeddings of two languages. Pham
et al. (2015) jointly learn bilingual sentence and
word embeddings by feeding a shared sentence
embedding to n-gram models. Zhou et al. (2016)
add document-level alignment information to this
model as a constraint in training.

Recently, sequence-to-sequence NMT models
were adapted to learn cross-lingual sentence em-
beddings. Schwenk and Douze (2017) connect
multiple source encoders to a shared decoder of
a pivot target language, forcing the consistency of
encoder representations. Schwenk (2018) extend
this work to use a single encoder for many source
languages. Both methods rely on N -way parallel
training data, which are seriously limited to cer-
tain languages and domains. Artetxe and Schwenk
(2018b) relax this data condition to pairwise paral-
lel data including the pivot language, but it is still
unrealistic for many scenarios (see Section 4.2).
In contrast, our method needs only parallel and
monolingual data for source and target languages
of concern without any pivot languages.

Hassan et al. (2018) train a bidirectional NMT
model with a single encoder-decoder, taking the
average of top-layer encoder states as the sentence
embedding. They do not include any details on the
data or translation performance before/after the fil-
tering with this embedding. Junczys-Dowmunt
(2018) apply this method to WMT 2018 paral-
lel corpus filtering task, yet showing significantly
worse performance than a combination of trans-
lation/language models. Our method shows com-
parable results to such model combinations in the
same task.

Guo et al. (2018) replace the decoder with a

feedforward network and use the parallel sen-
tences as input to the two encoders. Similarly to
our work, the feedforward network measures the
similarity of sentence pairs, except that the source
and target sentence embeddings are combined via
dot product instead of concatenation. Their model,
however, is not directly optimizing the source and
target sentences to be translations of each other;
it only attaches two encoders in the output level
without a decoder.

Based on the model of Artetxe and Schwenk
(2018b), Artetxe and Schwenk (2018a) scale
cosine similarity between sentence embeddings
with average similarity of the nearest neighbors.
Searching for the nearest neighbors among hun-
dreds of millions of sentences may cause a huge
computational problem. On the other hand, our
similarity calculation is much quicker and support
batch computation while preserving strong perfor-
mance in parallel corpus filtering.

Neither of the above-mentioned methods utilize
monolingual data. We integrate autoencoding into
NMT to maximize the usage of parallel and mono-
lingual data together in learning bilingual sentence
embeddings.

3 Bilingual Sentence Embeddings

A bilingual sentence embedding function maps
sentences from both the source and target lan-
guage into a single joint vector space. Once we
obtain such a space, we can search for a similar
target sentence embedding given a source sentence
embedding, or vice versa.

3.1 Model

In this work, we learn bilingual sentence embed-
dings via NMT and autoencoding given parallel
and monolingual corpora. Since our purpose is
to pair source and target sentences, translation is
a natural base task to connect sentences in two
different languages. We adopt a basic encoder-
decoder approach from Sutskever et al. (2014).
The encoder produces a fixed-length embedding
of a source sentence, which is used by the decoder
to generate the target hypothesis.

First, the encoder takes a source sentence fJ1 =
f1, ..., fj , ..., fJ (length J) as input, where each fj
is a source word. It computes hidden representa-
tions hj ∈ RD for all source positions j:

hJ1 = encsrc(f
J
1 ) (1)



63

. . .

Element-wise 
Max-pooling

. . .

. . .

Element-wise 
Max-pooling

f1 f2 fJ e1 e2 eI

h1 h2 hJ h̄1 h̄2 h̄I

e0 ei−1

s0 s1 sI

e1 eI

Figure 1: Our proposed model for learning bilingual
sentence embeddings. A decoder (above) is shared over
two encoders (below). The decoder accepts a max-
pooled representation from either one of the encoders
as its first state s0, depending on the training objective
(Equation 7 and 8).

encsrc is implemented as a bidirectional recurrent
neural network (RNN). We denote a target output
sentence by eI1 = e1, ..., ei, ..., eI (length I). The
decoder is an unidirectional RNN whose internal
state for a target position i is:

si = dec(si−1, ei−1) (2)

where its initial state is element-wise max-pooling
of the encoder representations hJ1 :

s0 = maxpool(h
J
1 )

=

[
max

j=1,...,J
hj1, ... , max

j=1,...,J
hjD

]>
(3)

We empirically found that the max-pooling per-
forms much better than averaging or choosing the
first (h1) or last (hJ ) representation. Finally, an
output layer predicts a target word ei:

pθ(ei|ei−11 , f
J
1 ) = softmax(linear(si)) (4)

where θ denotes a set of model parameters.
Note that the decoder has access to the source

sentence only through s0, which we take as the
sentence embedding of fJ1 . This assumes that
the source sentence embedding contains sufficient
information for translating to a target sentence,
which is desired for a bilingual embedding space.

However, this plain NMT model can generate
only source sentence embeddings through the en-
coder. The decoder cannot process a new target

sentence without a proper source language input.
We can perform decoding with an empty source
input and take the last decoder state sI as the
sentence embedding of eI1, but it is not compati-
ble with the source embedding and contradicts the
way in which the model is trained.

Therefore, we attach another encoder of the tar-
get language to the same (target) decoder:

h̄I1 = enctgt(e
I
1) (5)

s0 =

[
max
i=1,...,I

h̄i1, ... , max
i=1,...,I

h̄iD

]>
(6)

enctgt has the same architecture as encsrc. The
model has now an additional information flow
from a target input sentence to the same target
(output) sentence, also known as sequential au-
toencoder (Li et al., 2015).

Figure 1 is a diagram of our model. A decoder
is shared between NMT and autoencoding parts; it
takes either source or target sentence embedding
and does not differentiate between the two when
producing an output. The two encoders are con-
strained to provide mathematically consistent rep-
resentations over the languages (to the decoder).

Note that our model does not have any attention
component (Bahdanau et al., 2014). The atten-
tion mechanism in NMT makes the decoder attend
to encoder representations at all source positions.
This is counterintuitive for our purpose; we need
to optimize the encoder to produce a single rep-
resentation vector, but the attention model allows
the encoder to distribute information over many
different positions. In our initial experiments, the
same model with the attention mechanism showed
exorbitantly bad performance, so we removed it in
the main experiments of Section 4.

3.2 Training and Inference

Let θencsrc , θenctgt , and θdec the parameters of
the source encoder, the target encoder, and the
(shared) decoder, respectively. Given a paral-
lel corpus P and a target monolingual corpus
Mtgt, the training criterion of our model is the
cross-entropy on two input-output paths. The
NMT objective (Equation 7) is for training θ1 =
{θencsrc , θdec}, and the autoencoding objective
(Equation 8) is for training θ2 = {θenctgt , θdec}:

Lemb(θ) = −
∑

(fJ1 ,e
I
1)∈P

log pθ1(e
I
1|fJ1 ) (7)



64

−
∑

eI1∈Mtgt

log pθ2(e
I
1|eI1) (8)

where θ = {θ1, θ2}. During the training, each
mini-batch contains examples of the both objec-
tives with a 1:1 ratio. In this way, we prevent
one encoder from being optimized more than the
other, forcing the two encoders produce balanced
sentence embeddings that fit to the same decoder.

The autoencoding part can be trained with a
separate target monolingual corpus. To provide a
stronger training signal for the shared embedding
space, we use also the target side of P; the model
learns to produce the same target sentence from
the corresponding source and target inputs.

In order to guide the training to bilingual repre-
sentations, we initialize the word embedding lay-
ers with a pre-trained bilingual word embedding.
The word embedding for each language is trained
with a skip-gram algorithm (Mikolov et al., 2013),
later mapped across the languages with adversarial
training (Conneau et al., 2018) and self-dictionary
refinements (Artetxe et al., 2017).

Our model can be built also in the opposite di-
rection, i.e. with a target-to-source NMT model
and a source autoencoder:

Lemb(θ) = −
∑

(fJ1 ,e
I
1)∈P

log pθ2(f
J
1 |eI1) (9)

−
∑

fJ1 ∈Msrc

log pθ1(f
J
1 |fJ1 ) (10)

Once the model is trained, we need only the en-
coders to query sentence embeddings. Let a and
b be embeddings of a source sentence fJ1 and a
target sentence eI1, respectively:

a = maxpool(encsrc(f
J
1 )) (11)

b = maxpool(enctgt(e
I
1)) (12)

3.3 Computing Similarities
The next step is to evaluate how close the two em-
beddings are to each other, i.e. to compute a sim-
ilarity measure between them. In this paper, we
consider two types of similarity measures.

Predefined mathematical functions Cosine
similarity is a conventional choice for measuring
the similarity in vector space modeling of infor-
mation retrieval or text mining (Singhal, 2001). It
computes the angle between two vectors (rotation)
and ignore the lengths:

cos(a,b) =
a · b
‖a‖‖b‖

(13)

Euclidean distance indicates how much distance
must be traveled to move from the end of a vector
to that of the other (transition). We reverse this
distance to use it as a similarity measure:

Euclidean(a,b) = −‖a− b‖ (14)

However, these simple measures, i.e. a single
rotation or transition, might not be sufficient to
define the similarity of complex natural language
sentences across different languages. Also, the
learned joint embedding space is not necessarily
perfect in the sense of vector space geometry; even
if we train it with a decent algorithm, the structure
and quality of the embedding space are highly de-
pendent on the amount of parallel training data and
its domain. This might hinder the simple functions
from working well for our purpose.

Trainable multilayer perceptron To model re-
lations of sentence embeddings by combining ro-
tation, shift, and even nonlinear transformations,
We train a small multilayer perceptron (MLP)
(Bishop et al., 1995) and use it as a similarity mea-
sure. We design the MLP network q(a,b) as a
simple binary classifier whose input is a concate-
nation of source and target sentence embeddings:
[a;b]>. It is passed through feedforward hidden
layers with nonlinear activations. The output layer
has a single node with sigmoid activation, repre-
senting how probable the source and target sen-
tences are translations of each other.

To train this model, we must have positive ex-
amples (real parallel sentence pairs, Ppos) and
negative examples (nonparallel or noisy sentence
pairs, Pneg). The training criterion is:

Lsim = −
∑

(a,b)∈Ppos

log q(a,b)

−
∑

(a,b)∈Pneg

(1− log q(a,b)) (15)

which naturally fits to the main task of interest:
parallel corpus filtering (Section 4.2). Note that
the output of the MLP can be quite biased to the
extremes (0 or 1) in order to clearly distinguish
good and bad examples. This has both advantages
and disadvantages as explained in Section 5.1.

Our MLP similarity can be optimized differ-
ently for each embedding space. Furthermore, the
user can inject domain-specific knowledge into the
MLP similarity by training only with in-domain
parallel data. The resulting MLP would devalue



65

not only nonparallel sentence pairs but also out-
of-domain instances.

4 Evaluation

We evaluated our bilingual sentence embedding
and the MLP similarity on two tasks: sentence
alignment recovery and parallel corpus filtering.
The sentence embedding was trained with WMT
2018 English-German parallel data and 100M
German sentences from the News Crawl mono-
lingual data1, where we use German as the au-
toencoded language. All sentences were lower-
cased and limited to the length of 60. We learned
the byte pair encoding (Sennrich et al., 2016)
jointly for the two languages with 20k merge oper-
ations. We pre-trained bilingual word embeddings
on 100M sentences from the News Crawl data
for each language using FASTTEXT (Bojanowski
et al., 2017) and MUSE (Conneau et al., 2018).

Our sentence embedding model has 1-layer
RNN encoder/decoder, where the word embed-
ding and hidden layers have a size of 512. The
training was done with stochastic gradient descent
with initial learning rate of 1.0, batch size of 120
sentences, and maximum 800k updates. After
100k updates, we reduced the learning rate by a
factor of 0.9 for every 50k updates.

Our MLP similarity model has 2 hidden lay-
ers of size 512 with ReLU (Nair and Hinton,
2010), trained with SCIKIT-LEARN (Pedregosa
et al., 2011) with maximum 1,000 updates. For a
positive training set, we used newstest2007-2015
from WMT (around 21k sentences). Unless other-
wise noted, we took a comparable size of negative
examples from the worst-scored sentence pairs of
ParaCrawl2 English-German corpus. The scoring
was done with our bilingual sentence embedding
and cosine similarity.

Note that the negative examples are selected via
cosine similarity but the similarity values are not
used in the MLP training (Equation 15). Thus
it does not learn to mimic the cosine similarity
function again, but has a new sorting of sentence
pairs—also encoding the domain information.

4.1 Sentence Alignment Recovery

In this task, we corrupt the sentence alignments of
a parallel test set by shuffling one side, and find

1http://www.statmt.org/wmt18/translation-task.html
2https://www.paracrawl.eu/

the original alignments; also known as corpus re-
construction (Schwenk and Douze, 2017).

Given a source sentence, we compute a simi-
larity score with every possible target sentence in
the data and take the top-scored one as the align-
ment. The error rate is the number of incorrect
sentence alignments divided by the total number
of sentences. We compute this also in the oppo-
site direction and take an average of the two error
rates. It is an intrinsic evaluation for parallel cor-
pus mining. We choose two test sets: WMT new-
stest2018 (2998 lines) and IWSLT tst2015 (1080
lines).

As baselines, we used character-level Leven-
shtein distance and length-normalized posterior
scores of German→English/English→German
NMT models. Each NMT model is a 3-layer base
Transformer (Vaswani et al., 2017) trained on the
same training data as the sentence embedding.

Error [%]

Method WMT IWSLT

Levenshtein distance 37.4 54.6
NMT de-en + en-de 1.7 13.3

Our method (Cosine similarity) 4.3 13.8
Our method (MLP similarity) 89.9 72.6

Table 1: Sentence alignment recovery results. Our
method results use cosine similarity except the last row.

Table 1 shows the results. The Levenshtein dis-
tance gives a poor performance. NMT models
are better than the other methods, but takes too
long to compute posteriors for all possible pairs
of source and target sentences (about 12 hours for
the WMT test set). This is absolutely not feasible
for a real mining task with hundreds of millions of
sentences.

Our bilingual sentence embeddings (with us-
ing cosine similarity) show error rates close to the
NMT models, especially in the IWSLT test set.
Computing similarities between embeddings is ex-
tremely fast (about 3 minutes for the WMT test
set), which perfectly fits to mining scenarios.

However, the MLP similarity performs bad in
aligning sentence pairs. Given a source sentence,
it puts all reasonably similar target sentences to the
score 1 and does not precisely distinguish between
them. Detailed investigation of this behavior is in
Section 5.1. As we will find out, this is ironically
very effective in parallel corpus filtering.



66

BLEU [%]
10M words 100M words

Method test2017 test2018 test2017 test2018

Random sampling 19.1 23.1 23.2 29.3
Pivot-based embedding (Schwenk and Douze, 2017) 26.1 32.4 30.0 37.5
NMT + LM, 4 models (Rossenbach et al., 2018) 29.1 35.2 31.3 38.2

Our method (cosine similarity) 23.0 28.4 27.9 34.4
Our method (MLP similarity) 29.2 35.4 30.6 37.5

Table 2: Parallel corpus filtering results (German→English).

4.2 Parallel Corpus Filtering

We also test our methods in the WMT 2018 paral-
lel corpus filtering task (Koehn et al., 2018).

Data The task is to score each line of a very
noisy, web-crawled corpus of 104M parallel lines
(ParaCrawl English-German). We pre-filtered the
given raw corpus with the heuristics of Rossen-
bach et al. (2018). Only the data for WMT 2018
English-German news translation task is allowed
to train scoring models. The evaluation procedure
is: subsample top-scored lines which amounts to
10M/100M words, train a small NMT model with
the subsampled data, and check its translation per-
formance. We follow the official pipeline except
that we train 3-layer Transformer NMT model us-
ing Sockeye (Hieber et al., 2017) for evaluation.

Baselines We have three comparative baselines:
1) random sampling, 2) bilingual sentence em-
bedding learned with a third pivot target lan-
guage (Schwenk and Douze, 2017), 3) combina-
tion of source-to-target/target-to-source NMT and
source/target LM (Rossenbach et al., 2018), a top-
ranked system in the official evaluation.

Note that the second method violates the of-
ficial data condition of the task since it requires
parallel data in German-Pivot and English-Pivot.
This method is not practical when learning mul-
tilingual embeddings for English and other lan-
guages, since it is hard to collect pairwise parallel
data involving a non-English pivot language (ex-
cept among European languages). We trained this
method with N -way parallel UN corpus (Ziemski
et al., 2016) with French as the pivot language.
The size of this model is the same as that of our
autoencoding-based model except the word em-
bedding layers.

The results are shown in Table 2, where cosine

similarity was used by default for sentence embed-
ding methods except the last row. Pivot-based sen-
tence embedding (Schwenk and Douze, 2017) im-
proves upon the random sampling, but it has an
impractical data condition. The four-model com-
bination of NMT models and LMs (Rossenbach
et al., 2018) provide 1-3% more BLEU improve-
ment. Note that, for the third method, each model
costs 1-2 weeks to train.

Our bilingual sentence embedding method
greatly improves over the random sampling base-
line up to 5.3% BLEU in the 10M-word case and
5.1% BLEU in the 100M-word case. With our
MLP similarity, the improvement in BLEU is up
to 12.3% and 8.2% in the 10M-word case and the
100M-word case, respectively. It outperforms the
pivot-based embedding method significantly and
gets close to the performance of the four-model
combination. Note that we use only a single model
trained with only given parallel/monolingual data
for the corresponding language pair, i.e. English-
German. In contrast to sentence alignment recov-
ery experiments, the MLP similarity boosts the fil-
tering performance by a large margin.

5 Analysis

In this section, we provide more in-depth analyses
to compare 1) various similarity measures and 2)
different choices of the negative training set for the
MLP similarity model.

5.1 Similarity Measures
In Table 3, we compare sentence alignment re-
covery performance with different similarity mea-
sures.

Euclidean distance shows a worse performance
than cosine similarity. This means that in a sen-
tence embedding space, we should consider ro-
tation more than transition when comparing two



67

Error [%]
Similarity de-en en-de Average

Euclidean 7.9 99.8 53.8
Cosine 4.3 4.2 4.3
CSLS 1.9 2.2 2.1
MLP 85.0 94.8 89.9

Table 3: Sentence alignment recovery results with dif-
ferent similarity measures (newstest2018).

vectors. Particularly, the English→German direc-
tion has a peculiarly bad result with Euclidean dis-
tance. This is due to a hubness problem in a high-
dimensional space, where some vectors are highly
likely to be nearest neighbors of many others.

a1
a2

a3

a4

b1 b2

b3

b4

Figure 2: Schematic diagram of the hubness problem.
Filled circles indicate German sentence embeddings,
while empty circles denote English sentence embed-
dings. All embeddings are assumed to be normalized.

Figure 2 illustrates that Euclidean distance is
more prone to the hubs than cosine similarity. As-
sume that German sentence embeddings an and
English sentence embeddings bn should match to
each other with the same index n, e.g. (a1,b1)
is a correct match. With cosine similarity, the
nearest neighbor of an is always bn for all n =
1, ..., 4 and vice versa, considering only the an-
gles between the vectors. However, when us-
ing Euclidean distance, there is a discrepancy be-
tween German→English and English→German
directions: The nearest neighbor of each an is
bn, but the nearest neighbor of all bn is always
a4. This leads to a serious performance drop only
in English→German. The figure is depicted in a
two-dimensional space for simplicity, but the hub-
ness problem becomes worse for an actual high-
dimensional space of sentence embeddings.

Cross-domain similarity local scaling (CSLS) is
developed to counteract the hubness problem by

0 1 2 3 4
0

0.2

0.4

0.6

0.8

1

sentence index (×107)

si
m

ila
ri

ty
sc

or
e

(a) Cosine similarity

0 1 2 3 4
0

0.2

0.4

0.6

0.8

1

sentence index (×107)

si
m

ila
ri

ty
sc

or
e

(b) MLP similarity

Figure 3: The score distribution of similarity measures.
The sentences are sorted by their similarity scores. Co-
sine similarity values are linearly rescaled to [0, 1].

penalizing similarity values in dense areas of the
embedding distribution (Conneau et al., 2018):

CSLS(a,b) = 2 · cos(a,b) (16)

− 1
K

∑
b′∈NN(a)

cos(a,b′) (17)

− 1
K

∑
a′∈NN(b)

cos(a′,b) (18)

where K is the number of nearest neighbors.
CSLS outperforms cosine similarity in our exper-
iments. For a large-scale mining scenario, how-
ever, the measure requires heavy computations for
the penalty terms (Equation 17 and 18), i.e. near-
est neighbor search in all combinations of source
and target sentences and sorting the scores over
e.g. a few hundred million instances.

The MLP similarity is not performing well as
opposed to its results in parallel corpus filtering.
To explain this, we depict score distributions of
cosine and MLP similarity over the ParaCrawl cor-
pus in Figure 3. As for cosine similarity, only



68

Similarity

German sentence English sentence Cosine MLP

the requested URL / dictionary / m /
mar eisimpleir.htm was not found on
this server.

additionally, a 404 Not Found error was
encountered while trying to use an Er-
rorDocument to handle the request.

0.185 0.000

becoming Prestigious In The Right Way how I Feel About School 0.199 0.000

nach dieser Aussage sollte die türkische
Armee somit eine internationale Inter-
vention gegen Syrien provozieren .

according to his report, the Turkish
army was aiming to provoke an inter-
national intervention against Syria.

0.563 1.000

allen Menschen und Beschäftigten, die
um Freiheit kämpfen oder bei Kundge-
bungen ums Leben kamen, Achtung
zu bezeugen und die unverzügliche
Freilassung aller Inhaftierten zu fordern

to pay tribute to all people and work-
ers who have been fighting for freedom
or fallen in demonstrations and demand
the immediate release of all detainees

0.427 0.999

Table 4: Example sentence pairs in the ParaCrawl corpus (Section 4.2) with their similarity values.

a small fraction of the corpus is given low- or
high-range scores (smaller than 0.2 or larger than
0.6). The remaining sentences are distributed al-
most uniformly within the score range inbetween.

The distribution curve of the MLP similarity has
a completely different shape. It has a strong ten-
dency to classify a sentence pair to be extremely
bad or extremely good: nearly 80% of the corpus
is scored with zero and only 3.25% gets scores be-
tween 0.99 and 1.0. Table 4 shows some example
sentence pairs with extreme MLP similarity val-
ues.

This is the reason why the MLP similarity does
a good job in filtering, especially in selecting a
small portion (10M-word) of good parallel sen-
tences. Table 4 compares cosine similarities and
the MLP scores for some sentence pairs in the raw
corpus for our filtering task (Section 4.2). The
first two sentence pairs are absolutely nonparallel;
both similarity measures give low scores, while
the MLP similarity emphasizes the bad quality
with zero scores. The third example is a decent
parallel sentence pair with a minor ambiguity, i.e.
his in English can be a translation of dieser in Ger-
man or not, depending on the document-level con-
text. Both measures see this sentence pair as a pos-
itive example.

The last example is parallel but the translation
involves severe reordering: long-distance changes
in verb positions, switching the order of relative
clauses, etc. Here, cosine similarity has trouble
in rating this case highly even if it is perfectly

parallel, eventually filtering it out from the train-
ing data. On the other hand, our MLP similarity
correctly evaluates this difficult case by giving a
nearly perfect score.

However, the MLP is not optimized for precise
differentiation among the good parallel matches.
It is thus not appropriate for sentence alignment
recovery that requires exact 1-1 matching of po-
tential source-target pairs. A steep drop in the
curve of Figure 3b also explains why it performs
slightly inferior to the best system in the 100M-
word filtering task (Table 2). The subsampling ex-
ceeds the dropping region and includes many zero-
scored sentence pairs, where the MLP similarity
cannot measure the quality well.

5.2 Negative Training Examples
In the MLP similarity training, we can use pub-
licly available parallel corpora as the positive sets.
For the negative sets, however, it is not clear which
dataset we should use: entirely nonparallel sen-
tences, partly parallel sentences, or sentence pairs
of quality inbetween. We experimented with nega-
tive examples of different quality in Table 5. Here
is how we vary the negativity:

1. Score the sentence pairs of the ParaCrawl
corpus with our bilingual sentence embed-
ding using cosine similarity.

2. Sort the sentence pairs by the scores.

3. Divide the sorted corpus into five portions by
top-scored cut of 20%, 40%, 60%, 80%, and



69

Negative examples BLEU [%]

Random sampling 33.3

20% worst 29.9
40% worst 33.3
60% worst 33.7
80% worst 32.1
100% worst 25.7

Table 5: Parallel corpus filtering results (10M-word
task) with different negative sets for training MLP sim-
ilarity (newstest2016, i.e. the validation set).

100%.

4. Take the last 100k lines for each portion.

A negative set from the 20%-worst part stands
for relatively less problematic sentence pairs, in-
tending for elaborate classification among perfect
parallel sentences (positive set) and almost perfect
ones. With the 100%-worst examples, we focus
on removing absolutely nonsense pairing of sen-
tences. As a simple baseline, we also take 100k
sentences randomly without scoring, representing
mixed levels of negativity.

The results in Table 5 show that a moderate
level of negativity (60%-worst) is most suitable
for training an MLP similarity model. If the neg-
ative set contains too many excellent examples,
the model may mark acceptable parallel sentence
pairs with zero scores. If the negative set con-
sists only of certainly nonparallel sentence pairs,
the model is weak in discriminating mid-quality
instances, some of which are crucial to improve
the translation system.

Random selection of sentence pairs also works
surprisingly well compared to carefully tailored
negative sets. It does not require us to score and
sort the raw corpus, so it is very efficient, sacrific-
ing performance slightly. We hypothesize that the
average negative level of this random set is also
moderate and similar to that of the 60%-worst.

6 Conclusion

In this work, we present a simple method to
train bilingual sentence embeddings by combin-
ing vanilla RNN NMT (without attention compo-
nent) and sequential autoencoder. By optimizing a
shared decoder with combined training objectives,
we force the source and target sentence embed-
dings to share their space. Our model is trained

with parallel and monolingual data of the corre-
sponding language pair, with neither pivot lan-
guages nor N -way parallel data. We also propose
to use a binary classification MLP as a similarity
measure for matching source and target sentence
embeddings.

Our bilingual sentence embeddings show con-
sistently strong performance in both sentence
alignment recovery and the WMT 2018 parallel
corpus filtering tasks with only a single model. We
compare various similarity measures for bilingual
sentence matching, verifying that cosine similarity
is preferred for a mining task and our MLP simi-
larity is very effective in a filtering task. We also
show that a moderate level of negativity is appro-
priate for training the MLP similarity, using either
random examples or mid-range scored examples
from a noisy parallel corpus.

Future work would be regularizing the MLP
training to obtain a smoother distribution of the
similarity scores, which could supplement the
weakness of the MLP similarity (Section 5.1).
Furthermore, we plan to adjust our learning pro-
cedure towards the downstream tasks, e.g. with an
additional training objective to maximize the co-
sine similarity between the source and target en-
coders (Arivazhagan et al., 2019). Our method
should be tested also on many other language pairs
which do not have parallel data involving a pivot
language.

Acknowledgments

This work has received funding from the Euro-
pean Research Council (ERC) (under the Euro-
pean Union’s Horizon 2020 research and inno-
vation programme, grant agreement No 694537,
project ”SEQCLAS”), the Deutsche Forschungs-
gemeinschaft (DFG; grant agreement NE 572/8-1,
project ”CoreTec”), and eBay Inc. The GPU clus-
ter used for the experiments was partially funded
by DFG Grant INST 222/1168-1. The work re-
flects only the authors’ views and none of the
funding agencies is responsible for any use that
may be made of the information it contains.



70

References
Naveen Arivazhagan, Ankur Bapna, Orhan Firat, Roee

Aharoni, Melvin Johnson, and Wolfgang Macherey.
2019. The missing ingredient in zero-shot neural
machine translation. arXiv:1903.07091.

Mikel Artetxe, Gorka Labaka, and Eneko Agirre. 2017.
Learning bilingual word embeddings with (almost)
no bilingual data. In Proceedings of the 55th An-
nual Meeting of the Association for Computational
Linguistics (ACL 2017), volume 1, pages 451–462.

Mikel Artetxe and Holger Schwenk. 2018a. Margin-
based parallel corpus mining with multilingual sen-
tence embeddings. arXiv:1811.01136.

Mikel Artetxe and Holger Schwenk. 2018b. Mas-
sively multilingual sentence embeddings for
zero-shot cross-lingual transfer and beyond.
arXiv:1812.10464.

Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben-
gio. 2014. Neural machine translation by jointly
learning to align and translate. arXiv, pages arXiv–
1409.

Christopher M Bishop et al. 1995. Neural networks for
pattern recognition. Oxford university press.

Piotr Bojanowski, Edouard Grave, Armand Joulin, and
Tomas Mikolov. 2017. Enriching word vectors with
subword information. Transactions of the Associa-
tion for Computational Linguistics, 5:135–146.

Ondřej Bojar, Christian Federmann, Mark Fishel,
Yvette Graham, Barry Haddow, Matthias Huck,
Philipp Koehn, and Christof Monz. 2018. Find-
ings of the 2018 conference on machine translation
(wmt18). In Proceedings of the Third Conference
on Machine Translation, Volume 2: Shared Task Pa-
pers, pages 272–307, Belgium, Brussels.

AP Sarath Chandar, Mitesh M Khapra, Balaraman
Ravindran, Vikas Raykar, and Amrita Saha. 2013.
Multilingual deep learning. In Deep Learning Work-
shop at NIPS.

Alexis Conneau, Guillaume Lample, Marc’Aurelio
Ranzato, Ludovic Denoyer, and Hervé Jégou. 2018.
Word translation without parallel data. In Proceed-
ings of 6th International Conference on Learning
Representations (ICLR 2018).

Dan S. tefănescu, Radu Ion, and Sabine Hunsicker.
2012. Hybrid parallel sentence mining from com-
parable corpora. In Proceedings of the 16th Con-
ference of the European Association for Machine
Translation, pages 137–144.

Cristina Espana-Bonet, Adám Csaba Varga, Alberto
Barrón-Cedeño, and Josef van Genabith. 2017. An
empirical analysis of nmt-derived interlingual em-
beddings and their use in parallel sentence identifi-
cation. IEEE Journal of Selected Topics in Signal
Processing, 11(8):1340–1350.

Miquel Esplá-Gomis and Mikel L. Forcada. 2009. Bi-
textor, a free/open-source software to harvest trans-
lation memories from multilingual websites. In Pro-
ceedings of MT Summit XII, Ottawa, Canada.

Thierry Etchegoyhen and Andoni Azpeitia. 2016. Set-
theoretic alignment for comparable corpora. In Pro-
ceedings of the 54th Annual Meeting of the Associa-
tion for Computational Linguistics (Volume 1: Long
Papers), volume 1, pages 2009–2018.

Mandy Guo, Qinlan Shen, Yinfei Yang, Heming
Ge, Daniel Cer, Gustavo Hernandez Abrego, Keith
Stevens, Noah Constant, Yun-hsuan Sung, Brian
Strope, et al. 2018. Effective parallel corpus mining
using bilingual sentence embeddings. In Proceed-
ings of the Third Conference on Machine Transla-
tion: Research Papers, pages 165–176.

Hany Hassan, Anthony Aue, Chang Chen, Vishal
Chowdhary, Jonathan Clark, Christian Feder-
mann, Xuedong Huang, Marcin Junczys-Dowmunt,
William Lewis, Mu Li, et al. 2018. Achieving hu-
man parity on automatic chinese to english news
translation. arXiv preprint arXiv:1803.05567.

Karl Moritz Hermann and Phil Blunsom. 2014. Multi-
lingual models for compositional distributed seman-
tics. In Proceedings of the 52nd Annual Meeting of
the Association for Computational Linguistics (Vol-
ume 1: Long Papers), volume 1, pages 58–68.

Felix Hieber, Tobias Domhan, Michael Denkowski,
David Vilar, Artem Sokolov, Ann Clifton, and Matt
Post. 2017. Sockeye: A toolkit for neural machine
translation. arXiv preprint arXiv:1712.05690.

Eric H Huang, Richard Socher, Christopher D Man-
ning, and Andrew Y Ng. 2012. Improving word
representations via global context and multiple word
prototypes. In Proceedings of the 50th Annual Meet-
ing of the Association for Computational Linguis-
tics: Long Papers-Volume 1, pages 873–882. Asso-
ciation for Computational Linguistics.

Marcin Junczys-Dowmunt. 2018. Dual conditional
cross-entropy filtering of noisy parallel corpora. In
Proceedings of the Third Conference on Machine
Translation: Shared Task Papers, pages 888–895.

Max Kaufmann. 2012. Jmaxalign: A maximum en-
tropy parallel sentence alignment tool. Proceedings
of COLING 2012: Demonstration Papers, pages
277–288.

Alexandre Klementiev, Ivan Titov, and Binod Bhat-
tarai. 2012. Inducing crosslingual distributed rep-
resentations of words. In Proceedings of COLING
2012, pages 1459–1474.

Philipp Koehn, Huda Khayrallah, Kenneth Heafield,
and Mikel L Forcada. 2018. Findings of the wmt
2018 shared task on parallel corpus filtering. In Pro-
ceedings of the Third Conference on Machine Trans-
lation: Shared Task Papers, pages 726–739.



71

Philipp Koehn and Rebecca Knowles. 2017. Six chal-
lenges for neural machine translation. In Proceed-
ings of the 1st ACL Workshop on Neural Machine
Translation (WNMT 2017), pages 28–39.

Jiwei Li, Thang Luong, and Dan Jurafsky. 2015. A
hierarchical neural autoencoder for paragraphs and
documents. In Proceedings of the 53rd Annual
Meeting of the Association for Computational Lin-
guistics and the 7th International Joint Conference
on Natural Language Processing (Volume 1: Long
Papers), volume 1, pages 1106–1115.

Tomas Mikolov, Kai Chen, Greg Corrado, and Jef-
frey Dean. 2013. Efficient estimation of word
representations in vector space. arXiv preprint
arXiv:1301.3781.

Robert C Moore. 2002. Fast and accurate sentence
alignment of bilingual corpora. In Conference of the
Association for Machine Translation in the Ameri-
cas, pages 135–144. Springer.

Vinod Nair and Geoffrey E Hinton. 2010. Rectified
linear units improve restricted boltzmann machines.
In Proceedings of the 27th international conference
on machine learning (ICML-10), pages 807–814.

F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel,
B. Thirion, O. Grisel, M. Blondel, P. Pretten-
hofer, R. Weiss, V. Dubourg, J. Vanderplas, A. Pas-
sos, D. Cournapeau, M. Brucher, M. Perrot, and
E. Duchesnay. 2011. Scikit-learn: Machine learning
in Python. Journal of Machine Learning Research,
12:2825–2830.

Hieu Pham, Thang Luong, and Christopher Manning.
2015. Learning distributed representations for mul-
tilingual text sequences. In Proceedings of the 1st
Workshop on Vector Space Modeling for Natural
Language Processing, pages 88–94.

Philip Resnik and Noah A Smith. 2003. The web as a
parallel corpus. Computational Linguistics, 29(3).

Nick Rossenbach, Jan Rosendahl, Yunsu Kim, Miguel
Graça, Aman Gokrani, and Hermann Ney. 2018.
The rwth aachen university filtering system for the
wmt 2018 parallel corpus filtering task. In Proceed-
ings of the Third Conference on Machine Transla-
tion: Shared Task Papers, pages 946–954.

Holger Schwenk. 2018. Filtering and mining paral-
lel data in a joint multilingual space. In Proceed-
ings of the 56th Annual Meeting of the Association
for Computational Linguistics (Volume 2: Short Pa-
pers), pages 228–234.

Holger Schwenk and Matthijs Douze. 2017. Learn-
ing joint multilingual sentence representations with
neural machine translation. In Proceedings of the
2nd Workshop on Representation Learning for NLP,
pages 157–167.

Rico Sennrich, Barry Haddow, and Alexandra Birch.
2016. Neural machine translation of rare words with
subword units. In Proceedings of the 54th Annual
Meeting of the Association for Computational Lin-
guistics (Volume 1: Long Papers), volume 1, pages
1715–1725.

Amit Singhal. 2001. Modern information retrieval: A
brief overview. Bulletin of the Technical Committee
on, page 35.

Ilya Sutskever, Oriol Vinyals, and Quoc V Le. 2014.
Sequence to sequence learning with neural net-
works. In Proceedings of the 27th Interna-
tional Conference on Neural Information Processing
Systems-Volume 2, pages 3104–3112. MIT Press.

Kaveh Taghipour, Shahram Khadivi, and Jia Xu. 2011.
Parallel corpus refinement as an outlier detection al-
gorithm. In Proceedings of the 13th Machine Trans-
lation Summit (MT Summit XIII), pages 414–421.

Dániel Varga, András Kornai, Viktor Nagy, László
Németh, and Viktor Trón. 2005. Parallel corpora
for medium density languages. In Proceedings of
RANLP 2005, pages 590–596.

Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob
Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz
Kaiser, and Illia Polosukhin. 2017. Attention is all
you need. In Advances in Neural Information Pro-
cessing Systems, pages 5998–6008.

Hainan Xu and Philipp Koehn. 2017. Zipporah: a fast
and scalable data cleaning system for noisy web-
crawled parallel corpora. In Proceedings of the 2017
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 2945–2950.

Xinjie Zhou, Xiaojun Wan, and Jianguo Xiao. 2016.
Cross-lingual sentiment classification with bilingual
document representation learning. In Proceedings
of the 54th Annual Meeting of the Association for
Computational Linguistics (Volume 1: Long Pa-
pers), volume 1, pages 1403–1412.

Michal Ziemski, Marcin Junczys-Dowmunt, and Bruno
Pouliquen. 2016. The united nations parallel corpus
v1.0. In Proceedings of Language Resources and
Evaluation (LREC 2016), Portoroz̆, Slovenia.


