



















































Outta Control: Laws of Semantic Change and Inherent Biases in Word Representation Models


Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 1136–1145
Copenhagen, Denmark, September 7–11, 2017. c©2017 Association for Computational Linguistics

Outta Control: Laws of Semantic Change and Inherent Biases in Word
Representation Models

Haim Dubossarsky1, Eitan Grossman2 and Daphna Weinshall3
1 Edmond and Lily Safra Center for Brain Sciences

2 Department of Linguistics
3 School of Computer Science and Engineering

The Hebrew University of Jerusalem, 91904 Jerusalem, Israel
haim.dub@gmail.com, {eitan.grossman,daphna}@mail.huji.ac.il

Abstract

This article evaluates three proposed laws
of semantic change. Our claim is that in
order to validate a putative law of seman-
tic change, the effect should be observed
in the genuine condition but absent or re-
duced in a suitably matched control condi-
tion, in which no change can possibly have
taken place. Our analysis shows that the
effects reported in recent literature must be
substantially revised: (i) the proposed neg-
ative correlation between meaning change
and word frequency is shown to be largely
an artefact of the models of word represen-
tation used; (ii) the proposed negative cor-
relation between meaning change and pro-
totypicality is shown to be much weaker
than what has been claimed in prior art;
and (iii) the proposed positive correlation
between meaning change and polysemy
is largely an artefact of word frequency.
These empirical observations are corrob-
orated by analytical proofs that show that
count representations introduce an inher-
ent dependence on word frequency, and
thus word frequency cannot be evaluated
as an independent factor with these repre-
sentations.

1 Introduction

The increasing availability of digitized histori-
cal corpora, together with newly developed tools
of computational analysis, make the quantitative
study of language change possible on a larger scale
than ever before. Thus, many important ques-
tions may now be addressed using a variety of
NLP tools that were originally developed to study
synchronic similarities between words. This has
catalyzed the evolution of an exciting new field

of historical distributional semantics, which has
yielded findings that inform our understanding of
the dynamic structure of language (Sagi et al.,
2009; Wijaya and Yeniterzi, 2011; Mitra et al.,
2014; Hilpert and Perek, 2015; Frermann and La-
pata, 2016; Dubossarsky et al., 2016). Recent
research has even proposed laws of change that
predict the conditions under which the meaning
of words is likely to change (Dubossarsky et al.,
2015; Xu and Kemp, 2015; Hamilton et al., 2016).
This is an important development, as traditional
historical linguistics has generally been unable to
provide predictive models of semantic change.

However, these preliminary results should be
addressed with caution. To date, analyses of
changes in words’ meanings have relied on the
comparison of word representations at different
points in time. Thus any proposed change in
meaning is contingent on a particular model of
word representation and the method used to mea-
sure change. Distributional semantic models typi-
cally count words and their co-occurrence statis-
tics (explicit models) or predict the embedding
contexts of words (implicit models). In this paper,
we show that the choice of model may introduce
biases into the analysis. We therefore suggest that
empirical findings may be used to support laws of
semantic change only after a proper control can be
shown to eliminate artefactual factors as the un-
derlying cause of the empirical observations.

Regardless of the specific representation used,
a frequent method of measuring the semantic
change a word has undergone (Gulordava and Ba-
roni, 2011; Jatowt and Duh, 2014; Kim et al.,
2014; Dubossarsky et al., 2015; Kulkarni et al.,
2015; Hamilton et al., 2016) is to compare the
word’s vector representations between two points
in time using the cosine distance:

cosDist(x, y) = 1− x · y‖x‖2‖y‖2 (1)

1136



This choice naturally assumes that greater dis-
tances correspond to greater semantic changes.
However, this measure introduces biases that may
affect our interpretation of meaning change.

We examine various representations of word
meaning, in order to identify inherent confounds
when meaning change is evaluated using the co-
sine distance. In addition to the empirical evalua-
tion, in Section 5 we provide an analytical account
of the influence of word frequency on cosine dis-
tance scores when using these representations.

In our empirical investigation, we highlight the
critical role of control conditions in the validation
of experimental findings. Specifically, we argue
that every observation about a change of mean-
ing over time should be subjected to a control test.
The control condition described in Section 2.1 is
based on the construction of an artificially gener-
ated corpus, which resembles the historical corpus
in most respects but where no change of mean-
ing over time exists. In order to establish the va-
lidity of an observation about meaning change -
and even more importantly, the validity of a law-
like generalization about meaning change - the re-
sult obtained in a genuine experimental condition
should be demonstrated to be lacking (or at least
significantly diminished) in the control condition.

As we show in Section 4, some recently re-
ported laws of historical meaning change do not
survive this proposed test. In other words, sim-
ilar results are obtained in the genuine and con-
trol conditions. These include the correlation of
meaning change with word frequency, polysemy
(the number of different meanings a word has),
and prototypicality (how representative a word is
of its category). These factors lie at the basis of
the following proposed laws of semantic change:

• The Law of Conformity, according to which
frequency is negatively correlated with se-
mantic change (Hamilton et al., 2016).

• The Law of Innovation, according to which
polysemy is positively correlated with se-
mantic change (Hamilton et al., 2016).

• The Law of Prototypicality, according to
which prototypicality is negatively correlated
with semantic change (Dubossarsky et al.,
2015).

Our analysis shows that these laws have only
residual effects, suggesting that frequency and

prototypicality may play a smaller role in semantic
change than previously claimed. The main artefact
underlying the emergence of the first two laws in
both the genuine and control conditions may be
due to the SVD step used for the embedding of the
PPMI word representation (see Section 2.5).

2 Methods

The historical corpus used here is Google Books
5-grams of English fiction. Equally sized sam-
ples of 10 million 5-grams per year were ran-
domly sampled for the period of 1900-1999 (Kim
et al., 2014) to prevent the more prolific publi-
cation years from biasing the results, and were
grouped into ten-year bins. Uncommon words
were removed, keeping the 100,000 most frequent
words as the vocabulary for subsequent model
learning. All words were lowercased and stripped
of punctuation.

This corpus served as the genuine condition,
and was used to replicate and evaluate findings
from previous studies. In this corpus, words are
expected to change their meaning between decadal
bins, as they do in a truly random sample of texts.
According to the distributional hypothesis (Firth,
1957), one can extract a word’s meaning from the
contexts in which it appears. Therefore, if words’
meanings change over time, as has been argued
at least since Reisig (1839), it follows that the
words’ contexts should change accordingly, and
this change should be detected by our model.

2.1 Control condition setup

Complementary to the genuine condition, a con-
trol condition was created where no change of
meaning is expected. Therefore, any observed
change in a word’s meaning in the control con-
dition can only stem from random “noise“, while
changes in meaning in the genuine condition are
attributed to “real“ semantic change in addition to
“noise“. Two methods were used to construct the
corpus in the control condition:

Chronologically shuffled corpus (shuffle): 5-
grams were randomly shuffled between decadal
bins, so that each bin contained 5-grams from all
the decades evenly. This was chosen as a control
condition for two reasons. First, this condition re-
sembles the genuine condition in size of the vocab-
ulary, size of the corpus, overall variance in words’
usage, and size of the decadal bins. Second and

1137



crucially, words are not expected to show any ap-
parent change in their meaning between decades
in the control condition, because their various us-
age contexts are shuffled across decades.

One synchronous corpus (subsample): All 5-
grams of the year 1999, which amount to 250 mil-
lion 5-grams, were selected from Google Books
English fiction. 10 million 5-grams were ran-
domly subsampled from this selection, and this
process was repeated 30 times. This is suggested
as an additional control condition since the under-
lying assumption is always that words in the same
year do not change their meaning. Again, unlike
in the genuine condition, any changes that are ob-
served based on these 30 subsamples can be at-
tributed only to ”noise” that stems from random
sampling, rather than real change in meaning.

2.2 Measures of interest
Meaning change: Meaning change was evalu-
ated as the cosine distance between vector rep-
resentations of the same word in consecutive
decades. This was done separately for each pro-
cessing stage (see Section 2.5). For the subsample
condition, this was defined as the average cosine
distance between the vectors in all 30 samples.

Frequency: Words’ frequencies were computed
separately for each decadal bin as the number of
times a word appeared divided by the total number
of words in that decade. For the subsample control
condition, it was computed as the number of times
a word appeared among the 250 million 5-grams,
divided by the total number of words.

2.3 Construct validity
To establish the adequacy of our control condition,
we compared the meaning change scores (before
log-transformation and standardization) between
the genuine and the shuffled control conditions.
Change scores were obtained by taking the aver-
age meaning change over all words in each decade
using the representation of the final processing
stage (SVD). An adequate control condition will
exhibit a lower degree of change compared to the
genuine condition, and is expected to show a fixed
rate of change across decades (see 3a).

2.4 Statistical analysis
Following common practice (Hamilton et al.,
2016), the 10k most frequent words, as measured
by their average decadal bin frequencies, were

used for the analysis of semantic change. Change
scores and frequencies were log-transformed, and
all variables were subsequently standardized.

A linear mixed effects model was used to evalu-
ate meaning change in both the genuine and shuf-
fled control conditions. Frequency was set as a
fixed effect while random intercepts were set per
word. The model attempts to account for semantic
change scores using frequency, while controlling
for the variability between words by assuming that
each word’s behavior is strongly correlated across
decades and independent across words as follows:

∆w(t)i = β0 + βffreq
(t)
wi + zwi + ε

(t)
wi (2)

Here ∆w(t)i is the semantic change score of the
i’th word measured between two specific consec-
utive decades, β0 is the model’s intercept, βf is
the fixed-effect predictor coefficient for frequency,
zwi ∼ N(0, σ) is a random intercept for the i’th
word, and ε(t)wi is an error term associated with the
i’th word. We report the predictor coefficient as
well as the proportion of variance explained1 by
each model. Only statistically significant results
(p < .01) are reported. All statistical tests are per-
formed in R (lme4 and MuMln packages).

2.5 Word meaning representation
We used a cascade of processing stages based
on the explicit meaning representation of words
(i.e., word counts, PPMI, SVD, as explained be-
low) as commonly practiced (Baroni et al., 2014;
Levy et al., 2015). For each of these stages, we
sought to evaluate the relationship between word
frequency and meaning change, by computing the
corresponding correlations between these two fac-
tors in the subsample control condition.

Counts: Co-occurrence counts were collected
for all the words in the vocabulary per decade.

PPMI: Sparse square matrices of vocabulary
size containing positive pointwise mutual infor-
mation (PPMI) scores were constructed for each
decade based on the co-occurrence counts. We
used the context distribution smoothing parameter
α = 0.75, as recommended by (Levy et al., 2015),
using the following procedure:

PPMIα(w, c) = max

(
log

(
P̂ (w, c)

P̂ (w)P̂α(c)

)
, 0

)
1R2 for mixed linear models (Nakagawa and Schielzeth,

2013)

1138



(a) (b) (c)

Figure 1: Correlations in the control condition between change scores in the year 1999 and word fre-
quency for three word representation types, based on: (a) Counts, (b) PPMI, (c) SVD. Correlation coef-
ficients are reported above each subplot. LS regression lines are shown in dashed green.

where P̂ (w, c) denotes the probability that word c
appears as a context word of w, while P̂ (w) and
P̂α(c) =

#(c)α∑
C #(c)

α denote the marginal probabili-
ties of the word and its context, respectively.

SVD: Each PPMI matrix was approximated by
a truncated singular value decomposition as de-
scribed in (Levy et al., 2015). This embedding was
shown to improve results on downstream tasks
(Baroni et al., 2014; Bullinaria and Levy, 2012;
Turney and Pantel, 2010). Specifically, the top 300
elements of the diagonal matrix of singular values
Σ, denoted Σd, were retained to represent a new,
dense embedding of the word vectors, using the
truncated left hand orthonormal matrix Ud:

WSV Di = (Ud · Σd)i (3)
These representations were subsequently

aligned with the orthogonal Procrustes method
following (Hamilton et al., 2016).

Relation to other models: (Levy and Gold-
berg) have shown that the Skip-Gram with Neg-
ative Sampling (SGNS) embedding model, e.g.
word2vec (Mikolov et al., 2013) - perhaps the
most popular model of word meaning representa-
tion, implicitly factorizes the values of the word-
context PMI matrix. Hence, the optimization goal
and the sources of information available to SGNS
and our model are in fact very similar. We there-
fore hypothesize that conclusions similar to those
reported below can be drawn for SGNS models.

3 Results

3.1 Confound of frequency

There are many factors that may confound the
measurement of meaning change. Here we focus

Figure 2: Cosine distances between PPMI and ap-
proximated PPMI representations (y-axis), plotted
against frequency (x-axis). Correlation coefficient
is reported above the plot.

on frequency, and investigate the existence of an
artefactual relation between frequency and mean-
ing change. This is done by evaluating this re-
lation in the subsample control condition. Any
changes observed in this condition must be the
consequence of inherent noise, since this con-
trol condition contains random samples from the
same year (and the baseline assumption is that no
change can be observed within the same year).

We first plotted the change scores that use the
representation based on word count vs. word fre-
quency. This resulted in a robust correlation (r =
−0.915) between the two variables, as shown in
Fig. 1a (see the analytical account in Section 5).
We repeated the same procedure using the PPMI
representation, which showed a much weaker cor-
relation with frequency (r = −0.295), see Fig. 1b.

Finally, we repeated the same procedure us-
ing the final explicit representation after SVD em-
bedding2, see Fig. 1c. Surprisingly, the negative
correlation with frequency was reinstated (r =
−0.793). To investigate how this came about,

2Similar results were obtained for the implicit embedding
(word2vec-SGNS) described in Section 2.5.

1139



(a) (b) (c)

Figure 3: (a) Average change score per decade for the genuine and control conditions. Bars represent
standard deviations. (b-c) Change scores (y-axis), relative to their frequency (x-axis): (b) genuine his-
torical corpus, (c) chronologically shuffled historical corpus. LS regression lines are shown in dashed
green.

we computed the change in the PPMI vectors be-
fore and after the low-rank SVD embedding using
the cosine-distance. As apparent from Fig. 2, it
turns out that the SVD procedure distorts data in
an uneven manner - frequent words are distorted
less than infrequent words. Thus we demonstrate
that this reinstatement of correlation between fre-
quency and change scores is merely an artefactual
consequence of the truncated SVD factorization.

3.2 Construct validity

Potential confounding factors can be addressed by
comparing any experimental finding to a validated
control condition. Here we validate the use of the
shuffled condition as a proper control. To this end,
the average change scores of words per decade in
both the genuine and shuffled conditions are com-
pared within each processing stage. In the genuine
condition, words appear in different usage con-
texts between decades, while in the shuffled condi-
tion they do not, because the random shuffling cre-
ates a homogeneous corpus. Therefore, the valid-
ity of the control condition is established if: (a) the
change scores are diminished as compared to the
genuine condition; (b) change scores are uniform
across decades (since decades are shuffled); (c) the
variance of change scores is smaller that in the
genuine condition. As seen in Fig. 3a, all these re-
quirements are met by the control condition. Note
that the change scores in the shuffled condition are
all significantly positive, namely, meaning change
allegedly exists in this control condition. This sup-
ports the claim that any measurement is signifi-
cantly affected by unrelated noise.

Thus, we have established that the shuffled con-
dition is a suitable control for meaning change.

While validity was established for each of the pro-
cessing stages, the most robust effect was seen for
the PPMI representation, following by SVD and
word counts.

3.3 Accounting for the frequency confound

In Section 3.1 we used the subsample control con-
dition to establish the confounding effect of fre-
quency on meaning change. We now examine the
extent to which this frequency confound exists in a
historical corpus. We do so by comparing the fre-
quency confound between the genuine historical
corpus and the shuffled historical corpus.

To visualize the frequency confound in a man-
ner comparable to the analysis presented in Sec-
tion 3.1, we again plot change scores vs. fre-
quency, ignoring the time dimension of the data.
Fig. 3b presents this plot for the genuine condi-
tion. The same analysis is repeated in the shuffled
condition, see Fig. 3c.

Both plots reveal a highly significant correla-
tion between change scores and frequency. Fur-
thermore, the fact that the correlation coefficients
are virtually identical in the genuine and shuffled
conditions, with r = −0.748 and r = −0.747 re-
spectively, suggests that they are due to artefactual
factors in both conditions and not to true change
of meaning over time. In fact, this pattern of re-
sults is reminiscent of the spurious pattern we see
in Fig. 1c.

The relation between frequency and meaning
change can also be represented by a linear mixed
effect model, with the benefit that this model en-
ables the addition of more explanatory variables to
the data. The regression model found frequency
to have a negative influence on change scores,

1140



PPMI + SVD PPMI
Genuine Shuffled Genuine Shuffled

Frequency
(one-predictor)

β -0.91 -0.75 -0.29 0.06
explained variance (σ2) 67% 56% 8% 0%

Frequency +
Polysemy
(two-predictor)

β frequency -1.22 -1.12 -0.69 0.53
β polysemy 0.43 0.40 0.49 -0.52
explained variance (σ2) 68% 60% 9% 4%

Frequency +
Prototypicality
(two-predictor)

β frequency -0.71 -0.70 -0.02 0.07
β polysemy 0.22 0.21 0.12 0.02
explained variance (σ2) 65% 60% 2% 0%

Table 1: Results of one-predictor and two-predictor regression analysis in all conditions.

with βf=-0.91 and βf=-0.75, for the genuine and
shuffled conditions respectively. Importantly, fre-
quency accounted for 67% of the variance in the
change scores in the genuine condition, and was
only slightly diminished in the shuffled condition,
accounting for 56% of the variance. Similar re-
sults were obtained for the PPMI representation
(see Table 1).

4 Revisiting previous studies

We replicated three recent results that were af-
fected by this frequency effect, since they all de-
fine change as the word’s cosine distance relative
to itself at two time points. These studies report
laws of semantic change that measure the role of
frequency in semantic change either directly (Law
of Conformity), or indirectly through another lin-
guistic variable that is dependent on frequency
(Laws of Innovation and Prototypicality).

4.1 Laws of conformity and innovation

Continuing the work described in Section 3.1, we
replicated the model and analysis procedure de-
scribed in (Hamilton et al., 2016), where two pre-
dictors were used together to explain the change
scores: frequency and polysemy. Polysemy, which
describes the number of different senses a word
has, naturally differs among words, where some
words are more polysemous than others (com-
pare bank and date to wine). Following (Hamil-
ton et al., 2016), we defined polysemy as the
words’ secondary connections patterns - the con-
nections between each word’s co-occurring words
(using the entries in the PPMI representation for
that word). The more interconnected these sec-
ondary connections are, the less polysemic a word
is, and vice versa. Polysemy scores were com-

puted using the authors’ provided code3. We then
log-transformed and standardized the polysemy
scores. Next, frequency and polysemy were set as
two fixed effect predictors in a linear mixed effect
model, like the one described in Section 2.4.

Thus we were able to replicate the results in
the genuine condition as reported in (Hamilton
et al., 2016). Interestingly, the same pattern of
results emerged, again, in the shuffled condition
(see Table 1). Importantly, the difference in ef-
fect size between conditions, as evaluated by the
explained variance of frequency and polysemy to-
gether, showed a modest effect of 8% over the
shuffled condition, pointing to the conclusion that
the putative effects may indeed be real, but to a far
lesser extent than had been claimed. We conclude
that adding polysemy to the analysis contributed
very little to the model’s predictive power.

Since the PPMI representation (the explicit rep-
resentation without dimensionality reduction with
SVD) seems much less affected by spurious ef-
fects correlated with frequency (see Fig. 1b), we
repeated the analysis of frequency described here
and in Section 3.1 while using this representation.
The results are listed in Table 1, showing a similar
pattern of rather small frequency effect.

4.2 Prototypicality

Prototypicality is the degree to which a word is
representative of the category of which it is a
member (a robin is a more prototypical bird than
a parrot). According to the proposed Law of Pro-
totypicality, words with more prototypical mean-
ings will show less semantic change, and vice
versa. Following (Dubossarsky et al., 2015), we
computed words’ prototypicality scores for each
decade as the cos-distance between a word’s vec-

3https://github.com/williamleif/histwords

1141



tor and its k-means cluster’s centroid, and ex-
tended the analysis to encompass the entire 20th
century. The previous regression model assumed
independence between words, and therefore as-
signed words to a random effect variable. How-
ever, when modeling prototypicality, this assump-
tion is invalid as relations between words are what
inherently define prototypicality. We therefore
designed a model in which decades, rather than
words, are the random effect variable.

With this analysis the prototypicality effect
seems to be substantiated in two ways. First, the
addition of prototypicality explains an additional
5% of the variance. Second, the effect of proto-
typicality meets the more stringent requirement of
being diminished in the shuffle condition (see Ta-
ble 1). Nevertheless, here too the effect originally
reported was found to be drastically reduced after
being compared with the proper control.

5 Theoretical analysis

We show in Section 5.1 that the average cosine dis-
tance between two vectors representing the same
word is equivalent to the variance of the popula-
tion of vectors representing the same word in inde-
pendent samples, and is therefore always positive.
This is true for any word vector representation.

In Sections 5.2-5.3 we prove that the average
cosines distance between two count vectors rep-
resenting the same word is negatively correlated
with the frequency of the word, and positively cor-
related with the polysemy score of the word.

5.1 Sampling variability and the cos distance

Lemma 1. Assume two random variables x, y of
length ‖x‖2 = ‖y‖2 = 1, distributed iid with ex-
pected value µ and covariance matrix Σ. The ex-
pected value of the cosine distance between them
is equal to the sum of the diagonal elements of Σ.

Proof.

E(x− y)2 =E(x− µ)2 + E(y − µ)2+
2E(x− µ)(y − µ)

=2
∑

E(xi − µi)2 = 2
∑

V ar(xi)

E(x− y)2 =E(x2) + E(y2)− 2E(x · y)

=2− 2E
(

x · y
‖x‖2‖y‖2

)
=2E(cosDist(x, y))

It follows that

E(cosDist(x, y)) =
∑

V ar(xi) (4)

Implication: The average cosine distance be-
tween two samples of the same random variables
is directly related to the variance of the variable,
or the sampling noise. This variance should be
measured empirically whenever cosine distance is
used, since only distances that are larger than the
empirical variance can be relied upon to support
significant observations.

5.2 Cos distance of count vectors: frequency
Next, we analyze the cosine distance between 2
iid samples from a normalized multinomial ran-
dom variable. This distribution models the dis-
tribution of the count vector representation. Let
ki, 1 ≤ i ≤ m denote the number of times word
i appeared in the context of word w, and let m
denote the size of the dictionary not including w.
Let n =

∑
ki denote the number of words in the

count vector of w; n determines the word’s fre-
quency score. Assume that the counts are sampled
from the distribution Multinomial(n, ~p), namely

Prob(k1, · · · , km) =
(

n

k1 · · · , km

)
pk11 · · · pkmm

Lemma 2. The expected value of the cosine dis-
tance between two count vectors x, y sampled iid
from this distribution is monotonically decreasing
with n.

Proof. By definition, 1−E[cosDist(x, y)] equals

E

[
x · y

‖x‖2‖y‖2

]
=
∑
i

[
E

xi
‖x‖2

]2
=
∑
i

E2i (5)

We compute the expected value of Ei directly:

Ei =
∑

(k1,··· ,km)

ki√∑
j k

2
j

(
n

k1 · · · , km

)
pk11 · · · pkmm

Using Taylor expansion:

ki√∑
j k

2
j

=
ki
n√

(
∑

j
kj
n )

2 −∑l 6=j kjkln2
=
ki
n

1√
1−∑l 6=j kjkln2

=
ki
n

(
1 +

ε

2
+O(ε2)

)
(6)

1142



where ε =
∑

l 6=j
kjkl
n2

.
The expected value of the 0-order term with re-

spect to ε in (6) equals pi, which is independent of
n. We conclude the proof by focusing on the first
order term with respect to ε in (6), to be denoted
f1, showing that its expected value is monotoni-
cally decreasing with n. Specifically:

f1 =
∑
~k

∑
l 6=j

ki
n

kj
n

kl
n

(
n

k1 · · · , km

)
pk11 · · · pkmm

We switch the summation order and compute
each expression in the external sum, considering
two cases separately: when l 6= j 6= i∑

(k1,··· ,km)

ki
n

kj
n

kl
n

(
n

k1 · · · , km

)
pk11 · · · pkmm

=
n(n− 1)(n− 2)

n3
pipjpl

When l 6= j = i w.l.g, we rewrite kikj =
ki(ki − 1) + ki, and the sum above becomes
n(n−1)(n−2)

n3
p2i pl +

n(n−1)
n2

pipl. Thus

f1 =
n− 1
n

pi

n− 2
n

∑
l,j:l 6=j

pjpl + (1− pi)


and it readily follows that f1 is monotonically in-
creasing with n.

Since n measures the frequency score of word
w, it follows from (5) that the expected value of the
cosine distance between two iid samples from the
distribution of the count vector of w is monotoni-
cally decreasing with the word’s frequency.

5.3 Cos distance of count vectors: polysemy
We start our investigation of polysemy by mod-
eling the distribution of the parameters of the
multinomial distribution from which count vec-
tors are sampled. A common prior distribution
on the vector ~pw in m-simplex, which defines the
multinomial distribution generating the context of
word w, is the Dirichlet distribution f(~pw; ~αw) =
f(p1, · · · , pm;α1, · · · , αm).
~αw is a sparse vector of prior counts on all

the words in the dictionary, by which the co-
occurrence context of word w is modeled. We
divide the set of none-zero indices of ~αw into
two subsets: i1, · · · , im0 correspond to the words
which always appear in the context of w, while
j1, · · · , im1 correspond to the words which appear
in the context of w in one given meaning. If w is

polysemous and has two meanings, then there is a
third set of indices k1, · · · , km2 which correspond
to the words appearing in the context of w in its
second meaning. If w has more then two mean-
ings, they can be modeled with additional sets of
disjoint indices.

Lemma 3. Under certain conditions specified in
the proof, given two count vectors x, y sampled
iid from the above distribution of w, the expected
value of the cosine distance between them in-
creases with the number of sets of disjoint indices
which represent different meanings of w.

Proof. We will prove that when w has two mean-
ings, the expected value of the cosine distance is
larger than in the case of a single meaning. The
proof for the general case immediately follows.

Starting from (6) while keeping only the 0-order
term in ε, it follows from the derivations in the
proof of Lemma 2 that the expected cosine dis-
tance between two count vector samples of w, to
be denotedM , is 1−∑ p2i . In our current model ~p
is a random variable, and we shall compute the ex-
pected value of this random variable under the two
conditions, when w has either one or two mean-
ings.

We start by observing that, given the definition
of the Dirichlet distribution, it follows that

E(p2i ) =V ar(pi) + E(pi)
2 =

αi(1 + αi)
α0(1 + α0)

αo =
∑

αi

=⇒M =
∑

E(p2i ) =
α0 +

∑
α2i

α0(1 + α0)
(7)

Considering the different sets of indices in iso-
lation, let ϕo =

∑im0
i=i1

αi, ϕ1 =
∑jm1

i=j1
αi, and

ϕ2 =
∑km2

i=k1
αi. Let ψo =

∑im0
i=i1

α2i , ψ1 =∑jm1
i=j1

α2i , and ψ2 =
∑km2

i=k1
α2i .

We rewrite (7) for the two conditions:

1. w has one meaning:

M (1) =
ϕ0 + ϕ1 + ψ0 + ψ1

(ϕ0 + ϕ1)(1 + ϕ0 + ϕ1)

2. w has two meanings:

M (2) =
ϕ0 + ϕ1 + ϕ2 + ψ0 + ψ1 + ψ2

(ϕ0 + ϕ1 + ϕ2)(1 + ϕ0 + ϕ1 + ϕ2)

1143



With some algebraic manipulations, it can be
shown that M (1) > M (2) if the following holds:

(ϕ0 + ϕ1)2ϕ2 + (ψ0 + ψ1)ϕ22 (8)
+2(ψ0 + ψ1)(ϕ0 + ϕ1)ϕ2 + (ψ0 + ψ1)ϕ2
+(ϕ0 + ϕ1)(ϕ22 − ψ2) > ψ2(ϕ0 + ϕ1)2

Thus when (8) holds, the average cosine distance
between two samples of a certain word w gets
larger as w acquires more meanings.

(8) readily holds under reasonable conditions,
e.g., when the prior counts for each meaning are
similar (as a set) and much bigger than the prior
counts of the joint context words (i.e., ϕ0 = ψ0 =
ε, ϕ1 = ϕ2, ψ1 = ψ2).

6 Conclusions and discussion

In this article we have shown that some reported
laws of semantic change are largely spurious re-
sults of the word representation models on which
they are based. While identifying such laws is
probably within the reach of NLP analyses of mas-
sive digital corpora, we argued that a more strin-
gent standard of proof is necessary in order to put
them on a firm footing. Specifically, it is nec-
essary to demonstrate that any proposed law of
change has to be observable in the genuine con-
dition, but to be diminished or absent in a control
condition. We replicated previous studies claim-
ing to establish such laws, which propose that se-
mantic change is negatively correlated with fre-
quency and prototypicality, and positively corre-
lated with polysemy. None of these laws - at least
in their strong versions - survived the more strin-
gent standard of proof, since the observed correla-
tions were found in the control conditions.

In our analysis, the Law of Conformity, which
claims a negative correlation between word fre-
quency and meaning change, was shown to have a
much smaller effect size than previously claimed.
This indicates that word frequency probably does
play a role - but a small one - in semantic change.
According to the Law of Innovation, polysemy
was claimed to correlate positively with meaning
change. However, our analysis showed that pol-
ysemy is highly collinear with frequency, and as
such, did not demonstrate independent contribu-
tion to semantic change. For similar reasons, the
alleged role of prototypicality was diminished.

These results may be more consonant than pre-
vious ones with the findings of historical linguis-

tics, as it is commonly assumed that the factors
leading to semantic change are more diverse than
purely distributional factors. For example, socio-
cultural, political, and technological changes are
known to impact semantic change (Bochkarev
et al., 2014; Newman, 2015). Furthermore, some
regularities of semantic change have been imputed
to ‘channel bias‘, inherent biases of utterance pro-
duction and interpretation on the part of speakers
and listeners, e.g., (Moreton, 2008). As such, it
would be surprising if word frequency, polysemy,
and prototypicality were to capture too high a de-
gree of variance. In other words, since semantic
change may result from the interaction of many
factors, small effects may be a priori more credi-
ble than large ones.

The results of our empirical analysis showed
that the spurious effects of frequency were
much weaker for the explicit PPMI representa-
tion unaugmented by SVD dimensionality reduc-
tion. We therefore conclude that the artefactual
frequency effects reported are inherent to the type
of word representations upon which these analy-
ses are based. As the analytical proof in Section 5
demonstrates, it is count vectors that introduce an
artefactual dependence on word frequency.

Intuitively, one might expect that the average
value for the cosine distance between a given
word’s vector in any two samples would be 0.
However, Lemma 1 above shows that this is not
the case, and the average distance is the vari-
ance of the population of vectors representing the
same word. This result is independent of the spe-
cific method used to represent words as vectors.
Lemma 2 proves that the average cosine distance
between two samples of the same word, when us-
ing count vector representations, is negatively cor-
related with the word’s frequency. Thus, the role
of frequency cannot be evaluated as an indepen-
dent predictor in any model based on count vector
representations. It remains for future research to
establish whether other approaches to word repre-
sentation, e.g. (Blei et al., 2003; Mikolov et al.,
2013), have inherent biases.

While our findings may seem to be mainly nega-
tive, since they invalidate proposed laws of seman-
tic change, we would like to point to the positive
contribution made by articulating more stringent
standards of proof and devising replicable control
conditions for future research on language change
based on distributional semantics representations.

1144



References
Marco Baroni, Georgiana Dinu, and Germán

Kruszewski. 2014. Don’t count, predict! A
systematic comparison of context-counting vs.
context-predicting semantic vectors. In Proceedings
of ACL, pages 238–247.

David M. Blei, Andrew Y. Ng, and Michael I. Jordan.
2003. Latent Dirichlet Allocation. Journal of Ma-
chine Learning Research, 3(4-5):993–1022.

Vladimir Bochkarev, Valery Solovyev, and Sören
Wichmann. 2014. Universals versus historical con-
tingencies in lexical evolution. Journal of The Royal
Society Interface, 11:1–23.

John A Bullinaria and Joseph P Levy. 2012. Extracting
semantic representations from word co-occurrence
statistics: stop-lists, stemming, and svd. Behavior
research methods, 44(3):890–907.

Haim Dubossarsky, Yulia Tsvetkov, Chris Dyer, and
Eitan Grossman. 2015. A bottom up approach to
category mapping and meaning change. In Net-
WordS 2015 Word Knowledge and Word Usage,
pages 66–70.

Haim Dubossarsky, Daphna Weinshall, and Eitan
Grossman. 2016. Verbs change more than nouns:
A bottom up computational approach to semantic
change. Lingue e Linguaggio, 1:5–25.

John Rupert Firth. 1957. Papers in Linguistics 1934–
1951. Oxford University Press, London.

Lea Frermann and Mirella Lapata. 2016. A Bayesian
model of diachronic meaning change. TACL, 4:31–
45.

Kristina Gulordava and Marco Baroni. 2011. A distri-
butional similarity approach to the detection of se-
mantic change in the Google Books Ngram corpus.
In Proceedings of the GEMS 2011 Workshop on GE-
ometrical Models of Natural Language Semantics,
pages 67–71. Association for Computational Lin-
guistics.

William L Hamilton, Jure Leskovec, and Dan Jurafsky.
2016. Diachronic word embeddings reveal statisti-
cal laws of semantic change. In Proceedings of ACL.

Martin Hilpert and Florent Perek. 2015. Meaning
change in a petri dish: constructions, semantic vec-
tor spaces, and motion charts. Linguistics Vanguard,
1(1):339–350.

Adam Jatowt and Kevin Duh. 2014. A framework for
analyzing semantic change of words across time. In
Proceedings of the 14th ACM/IEEE-CS Joint Con-
ference on Digital Libraries, pages 229–238.

Yoon Kim, Yi-I Chiu, Kentaro Hanaki, Darshan Hegde,
and Slav Petrov. 2014. Temporal analysis of lan-
guage through neural language models. In Proceed-
ings of ACL, pages 61–65.

Vivek Kulkarni, Rami Al-Rfou, Bryan Perozzi, and
Steven Skiena. 2015. Statistically significant detec-
tion of linguistic change. In Proceedings of the 24th
International Conference on World Wide Web, pages
625–635.

Omer Levy and Yoav Goldberg. Neural word embed-
ding as implicit matrix factorization. In Advances
in Neural Information Processing Systems (NIPS),
pages 2177–2185.

Omer Levy, Yoav Goldberg, and Ido Dagan. 2015. Im-
proving distributional similarity with lessons learned
from word embeddings. TACL, 3:211–225.

Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg Cor-
rado, and Jeffrey Dean. 2013. Distributed represen-
tations of words and phrases and their composition-
ality. In Advances in Neural Information Processing
Systems (NIPS), pages 3111–3119.

Sunny Mitra, Ritwik Mitra, Martin Riedl, Chris Bie-
mann, Animesh Mukherjee, and Pawan Goyal.
2014. That’s sick dude!: Automatic identification
of word sense change across different timescales. In
Proceedings of ACL, pages 1020–1029.

Elliott Moreton. 2008. Analytic bias and phonological
typology. Phonology, 25(1):83–127.

Shinichi Nakagawa and Holger Schielzeth. 2013. A
general and simple method for obtaining r2 from
generalized linear mixed-effects models. Methods
in Ecology and Evolution, 4(2):133–142.

John Newman. 2015. Semantic shift. In Nick Rimer,
editor, The Routledge Handbook of Semantics, pages
266–280. Routledge, New York.

Eyal Sagi, Stefan Kaufmann, and Brady Clark. 2009.
Semantic density analysis: Comparing word mean-
ing across time and phonetic space. In Proceedings
of the Workshop on Geometrical Models of Natu-
ral Language Semantics, pages 104–111. Associa-
tion for Computational Linguistics.

Peter D Turney and Patrick Pantel. 2010. From fre-
quency to meaning: Vector space models of se-
mantics. Journal of artificial intelligence research,
37:141–188.

Derry Tanti Wijaya and Reyyan Yeniterzi. 2011. Un-
derstanding semantic change of words over cen-
turies. In Proceedings of the 2011 international
workshop on DETecting and Exploiting Cultural di-
versiTy on the social web, pages 35–40. ACM.

Yang Xu and Charles Kemp. 2015. A computational
evaluation of two laws of semantic change. In
CogSci.

1145


