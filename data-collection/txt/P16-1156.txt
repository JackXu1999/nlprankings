



















































Morphological Smoothing and Extrapolation of Word Embeddings


Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 1651–1660,
Berlin, Germany, August 7-12, 2016. c©2016 Association for Computational Linguistics

Morphological Smoothing and Extrapolation of Word Embeddings

Ryan Cotterell
Department of Computer Science
Johns Hopkins University, USA
ryan.cotterell@jhu.edu

Hinrich Schütze
CIS

LMU Munich, Germany
inquiries@cis.lmu.org

Jason Eisner
Department of Computer Science
Johns Hopkins University, USA

jason@cs.jhu.edu

Abstract

Languages with rich inflectional morphol-
ogy exhibit lexical data sparsity, since the
word used to express a given concept will
vary with the syntactic context. For in-
stance, each count noun in Czech has 12
forms (where English uses only singular and
plural). Even in large corpora, we are un-
likely to observe all inflections of a given
lemma. This reduces the vocabulary cover-
age of methods that induce continuous rep-
resentations for words from distributional
corpus information. We solve this prob-
lem by exploiting existing morphological
resources that can enumerate a word’s com-
ponent morphemes. We present a latent-
variable Gaussian graphical model that al-
lows us to extrapolate continuous represen-
tations for words not observed in the train-
ing corpus, as well as smoothing the repre-
sentations provided for the observed words.
The latent variables represent embeddings
of morphemes, which combine to create em-
beddings of words. Over several languages
and training sizes, our model improves the
embeddings for words, when evaluated on
an analogy task, skip-gram predictive accu-
racy, and word similarity.

1 Introduction

Representations of words as high-dimensional real
vectors have been shown to benefit a wide variety
of NLP tasks. Because of this demonstrated utility,
many aspects of vector representations have been
explored recently in the literature. One of the most
interesting discoveries is that these representations
capture meaningful morpho-syntactic and seman-
tic properties through very simple linear relations:
in a semantic vector space, we observe that

vtalked − vtalk ≈ vdrank − vdrink. (1)
That this equation approximately holds across

many morphologically related 4-tuples indicates

bebieron

comieron

bebemos

comemos

Figure 1: A visual depiction of the vector offset method for
morpho-syntactic analogies in R2. We expect bebieron and
bebemos to have the same relation (vector offset shown as
solid vector) as comieron and comemos.

that the learned embeddings capture a feature
of English morphology—adding the past tense
feature roughly corresponds to adding a certain
vector. Moreover, manipulating this equation
yields what we will call the vector offset method
(Mikolov et al., 2013c) for approximating other
vectors. For instance, if we only know the vectors
for the Spanish words comieron (ate), comemos
(eat) and bebieron (drank), we can produce an ap-
proximation of the vector for bebemos (drink), as
shown in Figure 1.

Many languages exhibit much richer morphol-
ogy than English. While English nouns com-
monly take two forms – singular and plural—
Czech nouns take 12 and Turkish nouns take over
30. This increase in word forms per lemma creates
considerable data sparsity. Fortunately, for many
languages there exist large morphological lexi-
cons, or better yet, morphological tools that can
analyze any word form—meaning that we have
analyses (usually accurate) for forms that were un-
observed or rare in our training corpus.

Our proposed method runs as a fast post-
processor (taking under a minute to process 100-
dimensional embeddings of a million observed
word types) on the output of any existing tool that
constructs word embeddings, such as WORD2VEC.

1651



Indicative Subjunctive
Sg Pl Sg Pl

1 bebo bebemos beba bebamos
2 bebes bebéis bebas bebáis
3 bebe beben beba beban

Table 1: The paradigm of the Spanish verb BEBER (to drink).
The paradigm actually consists of> 40 word forms; only the
present tense portion is shown here.

In this output, some embeddings are noisy or miss-
ing, due to sparse training data. We correct these
problems by using a Gaussian graphical model
that jointly models the embeddings of morpholog-
ically related words. Inference under this model
can smooth the noisy embeddings that were ob-
served in the WORD2VEC output. In the limiting
case of a word for which no embedding was ob-
served (equivalent to infinite noise), inference can
extrapolate one based on the observed embeddings
of related words—a kind of global version of the
vector offset method. The structure of our graphi-
cal model is defined using morphological lexicons,
which supply analyses for each word form.

We conduct a comprehensive study of our abil-
ity to modify and generate vectors across five lan-
guages. Our model also dramatically improves
performance on the morphological analogy task in
many cases: e.g., accuracy at selecting the nom-
inative plural forms of Czech nouns is 89%, ten
times better than the standard analogy approach.

2 Background: Inflectional Morphology

Many languages require every verb token to be in-
flected for certain properties, such as person, num-
ber, tense, and mood. A verbal paradigm such
as Table 1 lists all the inflected forms of a given
verb. We may refer to this verb in the abstract by
its lemma, BEBER—but when using it in a sen-
tence, we must instead select from its paradigm the
word type, such as bebéis, that expresses the con-
textually appropriate properties. Noun tokens in a
language may similarly be required to be inflected
for properties such as case, gender, and number.

A content word is chosen by specifying a lemma
(which selects a particular paradigm) together
with some inflectional attributes (which select a
particular slot within that paradigm). For example,
[Lemma=EAT, Person=3, Number=SINGULAR,
Tense=PRESENT ] is a bundle of attribute-value
pairs that would be jointly expressed in English by

the word form eats (Sylak-Glassman et al., 2015).
The regularities observed by Mikolov et al.

(2013c) hold between words with similar attribute-
value pairs. In Spanish, the word beben “they
drink” (Table 1) can be analyzed as expressing
the bundle [Lemma=BEBER, Person=3, Num-
ber=PLURAL, Tense=PRESENT ]. Its vector sim-
ilarity to bebemos “we drink” is due to the fact
that both word forms have the same lemma BE-
BER. Likewise, the vector similarity of beben
to comieron “they ate” is due to the conceptual
similarity of their lemmas, BEBER “drink” and
COMER “eat”. Conversely, that beben is similar to
preguntan “they ask” is caused by shared inflec-
tional attributes [Person=3, Number=PLURAL,
Tense=PRESENT ]. Under cosine similarity, the
most similar words are often related on both axes
at once: e.g., one of the word forms closest to
beben typically is comen “they eat”.

3 Approach

Following this intuition, we fit a directed Gaussian
graphical model (GGM) that simultaneously con-
siders (i) each word’s embedding (obtained from
an embedding model like WORD2VEC) and (ii)
its morphological analysis (obtained from a lexi-
cal resource). We then use this model to smooth
the provided embeddings, and to generate embed-
dings for unseen inflections. For a lemma cov-
ered by the resource, the GGM can produce em-
beddings for all its forms (if at least one of these
forms has a known embedding); this can be ex-
tended to words not covered using a guesser like
MORFESSOR (Creutz and Lagus, 2007) or CHIP-
MUNK (Cotterell et al., 2015a).

A major difference of our approach from re-
lated techniques is that our model uses existing
morphological resources (e.g., morphological lex-
icons or finite-state analyzers) rather than seman-
tic resources (e.g., WordNet (Miller et al., 1990)
and PPDB (Ganitkevitch et al., 2013)). The for-
mer tend to be larger: we often can analyze more
words than we have semantic representations for.

It would be possible to integrate our GGM into
the training procedure for a word embedding sys-
tem, making that system sensitive to morpholog-
ical attributes. However, the postprocessing ap-
proach in our present paper lets us use any exist-
ing word embedding system as a black box. It is
simple to implement, and turns out to get excellent
results, which will presumably improve further as

1652



veatsvran

weatswatewrunswran wrunning

minfl=vbgminfl=vbd mlem=eatminfl=vbp

weating

mlem=run

veating

Figure 2: A depiction of our directed Gaussian graphical model (GGM) for the English verbal paradigm. Each variable
represents a vector in Rn; thus, this is not the traditional presentation of a GGM in which each node would be a single real-
valued random variable, but each node represents a real-valued random vector. The shaded nodes vi at the bottom are observed
word embeddings. The nodes wi at the middle layer are smoothed or extrapolated word embeddings. The nodes mk at the top
are latent embeddings of morphemes.

better black boxes become available.

4 A Generative Model

Figure 2 draws our GGM’s structure as a Bayes
net. In this paper, we loosely use the term “mor-
pheme” to refer to an attribute-value pair (possi-
bly of the form Lemma=. . . ). Let M be the set
of all morphemes. In our model, each morpheme
k ∈ M has its own latent embedding mk ∈ Rn.
These random variables are shown as the top layer
of Figure 2. We impose an IID spherical Gaussian
prior on them (similar to L2 regularization with
strength λ > 0):

mk ∼ N (0, λ−1I), ∀k (2)
Let L be the lexicon of all word types that ap-

pear in our lexical resource. (The noun and verb
senses of bat are separate entries in L.) In our
model, each word i ∈ L has a latent embedding
wi ∈ Rn. These random variables are shown as
the middle layer of Figure 2. We assume that each
wi is simply a sum of the mk for its component
morphemes Mi ⊆ M (shown in Figure 2 as wi’s
parents), plus a Gaussian perturbation:

wi ∼ N (
∑

k∈Mi
mk,Σi), ∀i (3)

This perturbation models idiosyncratic usage of
word i that is not predictable from its morphemes.
The covariance matrix Σi is shared for all words i
with the same coarse POS (e.g., VERB).

Our system’s output will be a guess of all of the
wi. Our system’s input consists of noisy estimates
vi for some of the wi, as provided by a black-box

word embedding system run on some large corpus
C. (Current systems estimate the same vector for
both senses of bat.) These observed random vari-
ables are shown as the bottom layer of Figure 2.
We assume that the black-box system would have
recovered the “true” wi if given enough data, but
instead it gives a noisy small-sample estimate

vi ∼ N (wi, 1ni Σ
′
i), ∀i (4)

where ni is the count of word i in training corpus
C.

This formula is inspired by the central limit
theorem, which guarantees that vi’s distribution
would approach (4) (as ni → ∞) if it were es-
timated by averaging a set of ni noisy vectors
drawn IID from any distribution with meanwi (the
truth) and covariance matrix Σ′i. A system like
WORD2VEC does not precisely do that, but it does
choose vi by aggregating (if not averaging) the in-
fluences from the contexts of the ni tokens.

The parameters λ,Σi,Σ′i now have likelihood

p(v) =
∫
p(v,w,m) dw dm, where (5)

p(v,w,m) =
∏

k∈M
p(mk) ·

∏
i∈L

p(wi |mk : k ∈Mi)
· p(vi |wi) (6)

Here m = {mk : k ∈ M} represents the collec-
tion of all latent morpheme embeddings, and sim-
ilarly w = {wi : i ∈ L} and v = {vi : i ∈ L}.
We take p(vi | wi) = 1 if no observation vi exists.

How does the model behave qualitatively? If
ŵi is the MAP estimate of wi, then ŵi → vi as
ni → ∞, but ŵi →

∑
k∈Mi mk as ni → 0. This

1653



is because (3) and (4) are in tension; when ni is
small, (4) is weaker and we get more smoothing.
The morpheme embeddings mk are largely deter-
mined from the observed embeddings vi of the fre-
quent words (since mk aims via (2)–(3) to explain
wi, which ≈ vi when i is frequent). That deter-
mines the compositional embedding

∑
k∈Mi mk

toward which the wi of a rarer word is smoothed
(away from vi). If vi is not observed or if ni = 0,
then ŵi =

∑
k∈Mi mk exactly.

5 Inference

Suppose first that the model parameters are
known, and we want to reconstruct the latent vec-
tors wi. Because the joint density p(v,w,m) in
(6) is a product of (sometimes degenerate) Gaus-
sian densities, it is itself a highly multivariate
Gaussian density over all elements of all vectors.1

Thus, the posterior marginal distribution of each
wi is Gaussian as well. A good deal is known
about how to exactly compute these marginal dis-
tributions of a Gaussian graphical model (e.g., by
matrix inversion) or at least their means (e.g., by
belief propagation) (Koller and Friedman, 2009).

For this paper, we adopt a simpler method—
MAP estimation of all latent vectors. That is, we
seek the w,m that jointly maximize (6). This is
equivalent to minimizing∑

k

λ||mk||22 +
∑

i

||wi −
∑

k∈Mi
mk||2Σi

+
∑

i

||vi − wi||2Σ′i/ni , (7)

which is a simple convex optimization problem.2

We apply block coordinate descent until numerical
convergence, in turn optimizing each vector mk
or wi with all other vectors held fixed. This finds
the global minimum (convex objective) and is ex-
tremely fast even when we have over a hundred
million real variables. Specifically, we update

mk ←
(
λI +

∑
i∈Wk

Σ

i

)−1 ∑
i∈Wk

Σ

i(wi −
∑

j∈Mi,j 6=k
mj),

where

Σdef= Σ−1 is the inverse covariance matrix
and Wk

def= {i : k ∈ Mi}. This updates mk so
1Its inverse covariance matrix is highly sparse: its pat-

tern of non-zeros is related to the graph structure of Figure 2.
(Since the graphical model in Figure 2 is directed, the inverse
covariance matrix has a sparse Cholesky decomposition that
is even more directly related to the graph structure.)

2By definition, ||x||2A def= xTAx.

the partial derivatives of (7) with respect to the
components of mk are 0. In effect, this updates
mk to a weighted average of several vectors. Mor-
pheme k participates in words i ∈ Wk, so its vec-
tor mk is updated to the average of the contribu-
tions (wi−

∑
j∈Mi,j 6=k mj) that mk would ideally

make to the embeddings wi of those words. The
contribution of wi is “weighted” by the inverse co-
variance matrix

Σ

i. Because of prior (2), 0 is also
included in the average, “weighted” by λI .

Similarly, the update rule for wi is

wi ← (ni Σ′i +

Σ

i)−1
(
ni

Σ′
ivi +

Σ

i

∑
k∈Mi

mk
)
,

which can similarly be regarded as a weighted av-
erage of the observed and compositional represen-
tations.3 See Appendix C for the derivations.

6 Parameter Learning

We wish to optimize the model parameters
λ,Σi,Σ′i by empirical Bayes. That is, we do not
have a prior on these parameters, but simply do
maximum likelihood estimation. A standard ap-
proach is the Expectation-Maximization or EM al-
gorithm (Dempster et al., 1977) to locally maxi-
mize the likelihood. This alternates between re-
constructing the latent vectors given the parame-
ters (E step) and optimizing the parameters given
the latent vectors (M step). In this paper, we use
the Viterbi approximation to the E step, that is,
MAP inference as described in section 5. Thus,
our overall method is Viterbi EM.

As all conditional probabilities in the model
are Gaussian, the M step has closed form. MLE
estimation of a covariance matrix is a standard
result—in our setting the update to Σi takes the
form:

Σc ← 1
Nc

∑
i:c∈C(i)

(wi−
∑

k∈Mi
mk)(wi−

∑
k∈Mi

mk)T ,

where C(i) are i’s POS tags, Nc = |{i|c ∈ C(i)}|
and Σc is the matrix for the cth POS tag (the ma-
trices are tied by POS). In this paper we simply
fix Σ′i = I rather than fitting it.

4 Also, we tune
the hyperparameter λ on a development set, using
grid search over the values {0.1, 0.5, 1.0}.

3If vi is not observed, take ni = 0. In fact it is not neces-
sary to represent this wi during optimization. Simply omit i
from all Wk. After convergence, set wi ←

∑
k∈Mi mk.

4Note that it is not necessary to define it as λ′I , introduc-
ing a new scale parameter λ′, since doubling λ′ would have
the same effect on the MAP update rules as halving λ and Σi.

1654



Viterbi EM can be regarded as block coordinate
descent on the negative log-likelihood function,
with E and M steps both improving this common
objective along different variables. We update the
parameters (M step above) after each 10 passes of
updating the latent vectors (section 5’s E step).

7 Related Work

Our postprocessing strategy is inspired by Faruqui
et al. (2015), who designed a retrofitting procedure
to modify pre-trained vectors such that their rela-
tions match those found in semantic lexicons. We
focus on morphological resources, rather than se-
mantic lexicons, and employ a generative model.
More importantly, in addition to modifying vec-
tors of observed words, our model can generate
vectors for forms not observed in the training data.

Wieting et al. (2015) compute compositional
embeddings of phrases, with their simplest
method being additive (like ours) over the phrase’s
words. Their embeddings are tuned to fit observed
phrase similarity scores from PPDB (Ganitkevitch
et al., 2013), which allows them to smooth and ex-
tend PPDB just as we do to WORD2VEC output.

Using morphological resources to enhance em-
beddings at training time has been examined by
numerous authors. Luong et al. (2013) used MOR-
FESSOR (Creutz and Lagus, 2007), an unsuper-
vised morphological induction algorithm, to seg-
ment the training corpus. They then trained a re-
cursive neural network (Goller and Kuchler, 1996;
Socher, 2014) to generate compositional word em-
beddings. Our model is much simpler and faster
to train. Their evaluation was limited to English
and focused on rare English words. dos Santos and
Zadrozny (2014) introduced a neural tagging ar-
chitecture (Collobert et al., 2011) with a character-
level convolutional layer. Qiu et al. (2014) and
Botha and Blunsom (2014) both use MORFESSOR
segmentations to augment WORD2VEC and a log-
bilinear (LBL) language model (Mnih and Hinton,
2007), respectively. Similar to us, they have an
additive model of the semantics of morphemes,
i.e., the embedding of the word form is the sum
of the embeddings of its constituents. In contrast
to us, however, both include the word form itself
in the sum. Finally, Cotterell and Schütze (2015)
jointly trained an LBL language model and a mor-
phological tagger (Hajič, 2000) to encourage the
embeddings to encode rich morphology. With the
exception of (Cotterell and Schütze, 2015), all of

the above methods use unsupervised methods to
infuse word embeddings with morphology. Our
approach is supervised in that we use a morpho-
logical lexicon, i.e., a manually built resource.

Our model is also related to other generative
models of real vectors common in machine learn-
ing. The simplest of them is probabilistic prin-
cipal component analysis (Roweis, 1998; Tipping
and Bishop, 1999), a generative model of matrix
factorization that explains a set of vectors via la-
tent low-dimensional vectors. Probabilistic canon-
ical correlation analysis similarly explains a set of
pairs of vectors (Bach and Jordan, 2005).

Figure 2 has the same topology as our graphical
model in (Cotterell et al., 2015b). In that work, the
random variables were strings rather than vectors.
Morphemes were combined into words by con-
catenating strings rather than adding vectors, and
then applying a stochastic edit process (modeling
phonology) rather than adding Gaussian noise.

8 Experiments

We perform three experiments to test the ability
of our model to improve on WORD2VEC. To re-
iterate, our approach does not generate or ana-
lyze a word’s spelling. Rather, it uses an existing
morphological analysis of a word’s spelling (con-
structed manually or by a rule-based or statistical
system) as a resource to improve its embedding.

In our first experiment, we attempt to identify a
corpus word that expresses a given set of morpho-
logical attributes. In our second experiment, we
attempt to use a word’s embedding to predict the
words that appear in its context, i.e., the skip-gram
objective of Mikolov et al. (2013a). Our third ex-
ample attempts to use word embeddings to predict
human similarity judgments.

We experiment on 5 languages: Czech, English,
German, Spanish and Turkish. For each language,
our corpus data consists of the full Wikipedia text.
Table 5 in Appendix A reports the number of types
and tokens and their ratio. The lexicons we use are
characterized in Table 6: MorfFlex CZ for Czech
(Hajič and Hlaváčová, 2013), CELEX for English
and German (Baayen et al., 1993) and lexicons for
Spanish and Turkish that were scraped from Wik-
tionary by Sylak-Glassman et al. (2015).

Given a finite training corpus C and a lexi-
con L,5 we generate embeddings vi for all word

5L is finite in our experiments. It could be infinite (though
still incomplete) if a morphological guesser were used.

1655



N
om

S
g

N
om

P
l

G
en

S
g

G
en

P
l

D
at

S
g

D
at

P
l

A
cc

S
g

A
cc

P
l

In
s

S
g

In
s

P
l

V
o
c

S
g

V
o
c

P
l

Voc Pl

Voc Sg

Ins Pl

Ins Sg

Acc Pl

Acc Sg

Dat Pl

Dat Sg

Gen Pl

Gen Sg

Nom Pl

Nom Sg

GGM

4.7 0 0 3.5 1.3 5.5 2.2 0 2.1 3.7 0 –

0 0 0 0 0 5 5 0 0.83 0 – 0

2 6 3 2.5 1.7 4.3 3.6 7.7 0.69 – 0 4.6

4 1.6 1.6 2.3 2.7 0.36 1.6 2.4 – 0.18 0.83 2.9

1.3 15 1.2 6.2 2.6 2.6 2.2 – 2.8 2.7 0 0

3.9 2.7 0.74 2.1 1.7 0 – 2.4 0.58 2.5 5 2.2

3.8 4.8 2.4 8 0.86 – 0.33 7.2 1 2.7 0 5

2 2 0 3.3 – 0 1.3 0.61 2.8 0 0 2

1.8 4.9 4.8 – 4.7 2.5 3.3 7.2 3.1 0.92 1.7 3.7

2 1.7 – 2.9 0.67 0.5 0.98 0.5 1.3 1.6 0 0.33

4.6 – 1.7 3.4 1.6 2.2 2.3 10 1.9 7 0 0

– 4.4 2.6 1.7 3.9 0.33 2.1 1.6 3.5 0.59 0 5.2

48 89 81 43 30 33 36 87 33 48 49 89

1p
s

P
s

2p
s

P
s

3p
s

P
s

1p
p

P
s

2p
p

P
s

3p
p

P
s

1p
s

P
t

2p
s

P
t

3p
s

P
t

1p
p

P
t

2p
p

P
t

3p
p

P
t

3pp Pt

2pp Pt

1pp Pt

3ps Pt

2ps Pt

1ps Pt

3pp Ps

2pp Ps

1pp Ps

3ps Ps

2ps Ps

1ps Ps

GGM

7.3 1.3 7.2 3.4 0 13 1.4 0 37 1.8 0 –

0 0 0 0 0 0 0 0 0 0 – 0

1.7 0 3.3 0 0 2.8 1.7 0 3.4 – 0 2

7.6 0 13 1.3 0 7.2 0.86 0 – 3.5 0 36

0 0 0 0 0 0 0 – 0 0 0 0

0.14 0.5 2 3.2 0 0.33 – 0 1.4 0.83 0 1.7

1.6 0 30 4 0 – 0.24 0 8 4.4 0 14

0 0 0 0 – 0 0 0 0 0 0 0

0.24 0 1.9 – 0 2.7 2.1 0 2.6 0 0 2.2

1.2 9.3 – 1.2 0 29 1.7 3.3 13 2.5 0 8.2

0 – 14 0.5 5 0.091 0.83 0 0.17 0 0 0.28

– 0 0.79 2.1 0 0.33 0.83 0 3.4 1 0 3

8.8 4.5 30 14 10 49 9.4 8.1 41 9.5 0 43

Table 2: The two tables show how the Gaussian graphical model (GGM) compares to various analogies on Czech nouns (left)
and Spanish verbs (right).The numbers in each cell represent the accuracy (larger is better). The columns represent the inflection
of the word i to be predicted. Our GGM model is the top row. The other rows subdivide the baseline analogy results according
to the inflection of source word a. Abbreviations: in the Czech noun table (left), the first word indicates the case and the second
the number, e.g., Dat Sg = Dative Singular. In the Spanish verb table (right), the first word is the person and number and the
second the tense, e.g., 3pp Pt = 3rd-person plural past.

types i ∈ C, using the GENSIM implementation
(Řehůřek and Sojka, 2010) of the WORD2VEC hi-
erarchical softmax skip-gram model (Mikolov et
al., 2013a), with a context size of 5. We set the
dimension n to 100 for all experiments.6

We then apply our GGM to generate smoothed
embeddings wi for all word types i ∈ C ∩ L. (Re-
call that the noun and verb sense of bats are sep-
arate types in L, even if conflated in C, and get
separate embeddings.) How do we handle other
word types? For an out-of-vocabulary (OOV) test
word i 6∈ C, we will extrapolate wi ←

∑
k∈Mi mk

on demand, as the GGM predicts, provided i ∈ L.
If any of these morphemes mk were themselves
never seen in C, we back off to the mode of the
prior to take mk = 0.7 Our experiments also en-
counter out-of-lexicon (OOL) test words i 6∈ L,
for which we have no morphological analysis;
here we take wi = vi (unsmoothed) if i ∈ C and
wi = 0 otherwise.

8.1 Experiment 1: Extrapolation vs. Analogy

Our first set of experiments uses embeddings for
word selection. Our prediction task is to iden-
tify the unique word i ∈ C that expresses the

6An additional important hyperparameter is the number
of epochs. The default value in the GENSIM package is 5,
which is suitable for larger corpora. We use this value for Ex-
periments 1 and 3. Experiment 2 involves training on smaller
corpora and we found it necessary to set the number of epochs
to 10.

7One could in principle learn “backoff mor-
phemes.” For instance, if borogoves is analyzed as
[ Lemma=OOV NOUN,Num=PL ], we might want
mLemma=OOV NOUN 6= 0 to represent novel nouns.

morphological attributes Mi. To do this, we pre-
dict a target embedding x, and choose the most
similar unsmoothed word by cosine distance, ı̂ =
argmaxj∈C vj · x. We are scored correct if ı̂ = i.
Our experimental design ensures that i 6∈ L, since
if it were, we could trivially find i simply by con-
sulting L. The task is to identify missing lexical
entries, by exploiting the distributional properties
in C.8 Given the input bundle Mi, our method pre-
dicts the embedding x =

∑
k∈Mi mk, and so looks

for a word j ∈ C whose unsmoothed embedding
vj ≈ x. The GGM’s role here is to predict that the
bundle Mi will be realized by something like x.

The baseline method is the analogy method of
equation (1). This predicts the embedding x via
the vector-offset formula va + (vb − vc), where
a, b, c ∈ C ∩ L are three other words sharing
i’s coarse part of speech such that Mi can be
expressed as Ma + (Mb − Mc).9 Specifically,
the baseline chooses a, b, c uniformly at random
from all possibilities. (This is not too inefficient:
given a, at most one choice of (b, c) is possible.)
Note that the baseline extrapolates from the un-
smoothed embeddings of 3 other words, whereas
the GGM considers all words in C ∩ L that share
i’s morphemes.

8The argmax selection rule does not exploit the fact that
the entry is missing: it is free to incorrectly return some ı̂ ∈
L.

9More formally, Mi = Ma + (Mb −Mc), if we define
M by (M)k = I(k ∈ M) for all morphemes k ∈ M. This
converts morpheme bundleM to a {0, 1} indicator vectorM
overM.

1656



an
al

og
y

G
G

M

an
al

og
y

G
G

M

an
al

og
y

G
G

M

an
al

og
y

G
G

M

an
al

og
y

G
G

M

InsP .067 .343? AccS .131 .582? VBD .052 .202? MascS Part .178 .340? LocS .002 .036?

GenS .059 .632? NomS .136 .571? VBG .06 .102? FemP Part .230 .286 AblS .001 .019?

GenP .078 .242? DatP .074 .447? NN .051 .080? FemS Part .242 .235 DatS .001 .037?

NomP .076 .764? AccP .075 .682? VBN .052 .218? AdjS .201 .286? AccS .001 .023?

NomS .066 .290? GenP .075 .666? NNS .052 .114? GerundS .186 .449? InsS .001 .004
VocP .063 .789? NomP .064 .665? VB .056 .275? GerundP .172 .311? NomS .001 .023?

Czech German English Spanish Turkish
nouns nouns nouns & verbs nouns, verbs & adj’s nouns

Table 3: Test results for Experiment 1. The rows indicate the inflection of the test word i to be predicted (superscript P indicates
plural, superscript S singular). The columns indicate the prediction method. Each number is an average over 10 training-test
splits. Improvements marked with a ? are statistically significant (p < 0.05) under a paired permutation test over these 10 runs.

Experimental Setup: A lexical resource con-
sists of pairs (word form i, analysis Mi). For each
language, we take a random 80% of these pairs to
serve as the training lexicon L that is seen by the
GGM. The remaining pairs are used to construct
our prediction problems (givenMi, predict i), with
a random 10% each as dev and test examples. We
compare our method against the baseline method
on ten such random training-test splits. We are re-
leasing all splits for future research.

For some dev and test examples, the baseline
method has no choice of the triple a, b, c. Rather
than score these examples as incorrect, our base-
line results do not consider them at all (which in-
flates performance). For each remaining example,
to reduce variance, the baseline method reports the
average performance on up to 100 a, b, c triples
sampled uniformly without replacement.

The automatically created analogy problems
(a, b, c → i) solved by the baseline are simi-
lar to those of Mikolov et al. (2013c). How-
ever, most previous analogy evaluation sets evalu-
ate only on 4-tuples of frequent words (Nicolai et
al., 2015), to escape the need for smoothing, while
ours also include infrequent words. Previous eval-
uation sets also tend to be translations of the orig-
inal English datasets—leaving them impoverished
as they therefore only test morpho-syntactic prop-
erties found in English. E.g., the German analogy
problems of Köper et al. (2015) do not explore the
four cases and two numbers in the German adjec-
tival system. Thus our baseline analogy results are
useful as a more comprehensive study of the vec-
tor offset method for randomly sampled words.

Results: Overall results for 5 languages are
shown in Table 3. Additional rows break down

performance by the inflection of the target word i.
(The inflections shown are the ones for which the
baseline method is most accurate.)

For almost all target inflections, GGM is sig-
nificantly better than the analogy baseline. An
extreme case is the vocative plural in Czech, for
which GGM predicts vectors better by more than
70%. In other cases, the margin is slimmer; but
GGM loses only on predicting the Spanish fem-
inine singular participle. For Czech, German,
English and Spanish the results are clear—GGM
yields better predictions. This is not surprising as
our method incorporates information from multi-
ple morphologically related forms.

More detailed results for two languages are
given in Table 2. Here, each row constrains the
source word a to have a certain inflectional tag;
again we average over up to 100 analogies, now
chosen under this constraint, and again we discard
a test example i from the test set if no such analogy
exists. The GGM row considers all test examples.

Past work on morphosyntactic analogies has
generally constrained a to be the unmarked
(lemma) form (Nicolai et al., 2015). However, we
observe that it is easier to predict one word form
from another starting from a form that is “closer”
in morphological space. For instance, it is easier to
predict Czech forms inflected in the genitive plu-
ral from forms in nominative plural, rather than the
nominative singular. Likewise, it is easier to pre-
dict a singular form from another singular form
rather than from a plural form. It also is easier to
predict partially syncretic forms, i.e., two inflected
forms that share the same orthographic string; e.g.,
in Czech the nominative plural and the accusative
plural are identical for inanimate nouns.

1657



105 106 107 108
2000

4000

6000

8000

10000
Pe

rp
le

xi
ty

Pe
r

W
or

d English

105 106 107
0

5000
10000
15000
20000
25000
30000
35000

Czech

105 106 107 108
1000
2000
3000
4000
5000
6000
7000
8000 Spanish [0,∞)-U

[0, 1)-U
[1, 10)-U
[10, 20)-U
[0,∞)-S
[0, 1)-S
[1, 10)-S
[10, 20)-S

Figure 3: Results for the WORD2VEC skip-gram objective score (perplexity per predicted context word) on a held-out test
corpus. The x-axis measures the size in tokens of the training corpus used to generate the model. We plot the held-out
perplexity for the skip-gram model with Unsmoothed observed vectors v (solide) and Smoothed vectors w (barredc). The
thickest, darkest curves show aggregate performance. The thinner, lighter versions show a breakdown according to whether
the predicting word’s frequency in the smallest training corpus falls in the range [0, 1), [1, 10), or [10, 20) (from lightest to
darkest and roughly from top to bottom). (These are the words whose representations we smooth; footnote 10 explains why
we do not smooth the predicted context word.) We do not show [20,∞) since WORD2VEC randomly removes some tokens of
high-frequency words (“subsampling”), similar in spirit to removing stop words. See Appendix B for more graphs.

8.2 Experiment 2: Held-Out Evaluation
We now evaluate the smoothed and extrapolated
representations wi. Fundamentally, we want to
know if our approach improves the embeddings
of the entire vocabulary, as if we had seen more
evidence. But we cannot simply compare our
smoothed vectors to “gold” vectors trained on
much more data, since two different runs of
WORD2VEC will produce incomparable embed-
ding schemes. We must ask whether our embed-
dings improve results on a downstream task.

To avoid choosing a downstream task with
a narrow application, we evaluate our embed-
ding using the WORD2VEC skip-gram objective
on held-out data—as one would evaluate a lan-
guage model. If we believe that a better score
on the WORD2VEC objective indicates generally
more useful embeddings—which indeed we do
as we optimize for it—then improving this score
indicates that our smoothed vectors are superior.
Concretely, the objective is∑

s

∑
t

∑
j∈[t−5,t+5],j 6=t

log2 pword2vec(Tsj |Tst), (8)

where Ts is the sth sentence in the test corpus, t in-
dexes its tokens, and j indexes tokens near t. The
probability model pword2vec is defined in Eq. (3) of
(Mikolov et al., 2013b). It relies on an embedding
of the word form Tst.10 Our baseline approach

10In the hierarchical softmax version, it also relies on a
separate embedding for a variable-length bit-string encod-
ing of the context word Tsj . Unfortunately, we do not cur-
rently know of a way to smooth these bit-string encodings
(also found by WORD2VEC). However, it might be possible
to directly incorporate morphology into the construction of
the vocabulary tree that defines the bit-strings.

simply uses WORD2VEC’s embeddings (or 0 for
OOV words Tst 6∈ C). Our GGM approach substi-
tutes “better” embeddings when Tst appears in the
lexicon L (if Tst is ambiguous, we use the mean
wi vector from all i ∈ L with spelling Tst).

Note that (8) is itself a kind of task of predicting
words in context, resembling language modeling
or a “cloze” task. Also, Taddy (2015) showed how
to use this objective for document classification.

Experimental Setup: We evaluate GGM on the
same 5 languages, but now hold out part of the
corpus instead of part of the lexicon. We take
the training corpus C to be the initial portion of
Wikipedia of size 105, 106, 107 or 108. (We skip
the 108 case for the smaller datasets: Czech and
Turkish). The 107 tokens after that are the dev cor-
pus; the next 107 tokens are the test corpus.

Results: We report results on three languages
in Figure 3 and all languages in Appendix B.
Smoothing from vi to wi helps a lot, reducing per-
plexity by up to 48% (Czech) with 105 training
tokens and up to 10% (Spanish) even with 108

training tokens. This roughly halves the perplex-
ity, which in the case of 105 training tokens, is
equivalent to 8×more training data. This is a clear
win for lower-resource languages. We get larger
gains from smoothing the rarer predicting words,
but even words with frequency ≥ 10−4 benefit.
(The exception is Turkish, where the large gains
are confined to rare predicting words.) See Ap-
pendix B for more analysis.

1658



English German Spanish
Forms / Lemma 1.8 6.3 8.1
Skip-Gram 58.9 36.2 37.8
GGM 58.9 37.6 40.3

Table 4: Word similarity results (correlations) using the WS-
353 dataset in the three languages, in which it is available.
Since all the words in WS-353 are lemmata, we report the
average inflected form to lemma ratio for forms appearing in
the datasets.

8.3 Experiment 3: Word Similarity
As a third and final experiment, we consider word
similarity using the WS-353 data set (Finkelstein
et al., 2001), translated into Spanish (Hassan and
Mihalcea, 2009) and German (Leviant, 2016).11

The datasets are composed of 353 pairs of words.
Multiple native speakers were then asked to give
an integral value between 1 and 10 indicating the
similarity of that pair, and those values were then
averaged. In each case, we train the GGM on the
whole Wikipedia corpus for the language. Since
in each language every word in the WS-353 set
is in fact a lemma, we use the latent embedding
our GGM learns in the experiment. In Span-
ish, for example, we use the learned latent mor-
pheme embedding for the lemma BEBER (recall
this takes information from every element in the
paradigm, e.g., bebemos and beben), rather than
the embedding for the infinitival form beber. In
highly inflected languages we expect this to im-
prove performance, because to get the embedding
of a lemma, it leverages the distributional signal
from all inflected forms of that lemma, not just a
single one. Note that unlike previous retrofitting
approaches, we do not introduce new semantic in-
formation into the model, but rather simply allow
the model to better exploit the distributional prop-
erties already in the text, by considering words
with related lemmata together. In essence, our
approach embeds a lemma as the average of all
words containing that lemma, after “correcting”
those forms by subtracting off their other mor-
phemes (e.g., inflectional affixes).

Results: As is standard in the literature, we re-
port Spearman’s correlation cofficient ρ between
the averaged human scores and the cosine distance
between the embeddings. We report results in Ta-
ble 4. We additionally report the average num-

11This dataset has yet to be translated into Czech or Turk-
ish, nor are there any comparable resources in these lan-
guages.

ber of forms per lemma. We find that we improve
performance on the Spanish and German datasets
over the original skip-gram vectors, but only equal
the performance on English. This is not surprising
as German and Spanish have roughly 3 and 4 times
more forms per lemma than English. We spec-
ulate that cross-linguistically the GGM will im-
prove word similarity scores more for languages
with richer morphology.

9 Conclusion and Future Work

For morphologically rich languages, we generally
will not observe, even in a large corpus, a high
proportion of the word forms that exist in lex-
ical resources. We have presented a Gaussian
graphical model that exploits lexical relations doc-
umented in existing morphological resources to
smooth vectors for observed words and extrapo-
late vectors for new words. We show that our
method achieves large improvements over strong
baselines for the tasks of morpho-syntactic analo-
gies and predicting words in context. Future work
will consider the role of derivational morphology
in embeddings as well as noncompositional cases
of inflectional morphology.

Acknowledgments

The first author was funded by a DAAD Long-
Term Research Grant. This work was also par-
tially supported by Deutsche Forschungsgemein-
schaft (grat SCHU-2246/2-2 WordGraph) and by
the U.S. National Science Foundation under Grant
No. 1423276.

References
R Harald Baayen, Richard Piepenbrock, and Rijn van

H. 1993. The CELEX lexical data base on CD-
ROM.

Francis R Bach and Michael I Jordan. 2005. A proba-
bilistic interpretation of canonical correlation analy-
sis. Technical report, UC Berkeley.

Jan A. Botha and Phil Blunsom. 2014. Composi-
tional Morphology for Word Representations and
Language Modelling. In ICML.

Ronan Collobert, Jason Weston, Léon Bottou, Michael
Karlen, Koray Kavukcuoglu, and Pavel Kuksa.
2011. Natural language processing (almost) from
scratch. JMLR, 12.

Ryan Cotterell and Hinrich Schütze. 2015. Morpho-
logical word embeddings. In NAACL.

1659



Ryan Cotterell, Thomas Müller, Alexander Fraser, and
Hinrich Schütze. 2015a. Labeled morphological
segmentation with semi-Markov models. In CoNLL.

Ryan Cotterell, Nanyun Peng, and Jason Eisner.
2015b. Modeling word forms using latent underly-
ing morphs and phonology. TACL, 3.

Mathias Creutz and Krista Lagus. 2007. Unsuper-
vised models for morpheme segmentation and mor-
phology learning. ACM Transactions on Speech and
Language Processing, 4(1):3.

Arthur P Dempster, Nan M Laird, and Donald B Ru-
bin. 1977. Maximum likelihood from incomplete
data via the EM algorithm. Journal of the Royal Sta-
tistical Society B, pages 1–38.

Cıcero Nogueira dos Santos and Bianca Zadrozny.
2014. Learning character-level representations for
part-of-speech tagging. In ICML.

Manaal Faruqui, Jesse Dodge, Sujay K Jauhar, Chris
Dyer, Eduard Hovy, and Noah A Smith. 2015.
Retrofitting word vectors to semantic lexicons. In
NAACL.

Lev Finkelstein, Evgeniy Gabrilovich, Yossi Matias,
Ehud Rivlin, Zach Solan, Gadi Wolfman, and Ey-
tan Ruppin. 2001. Placing search in context: The
concept revisited. In WWW. ACM.

Juri Ganitkevitch, Benjamin Van Durme, and Chris
Callison-Burch. 2013. PPDB: The paraphrase
database. In NAACL.

Christoph Goller and Andreas Kuchler. 1996. Learn-
ing task-dependent distributed representations by
backpropagation through structure. Neural Net-
works.

Jan Hajič and Jaroslava Hlaváčová. 2013. MorfFlex
CZ. LINDAT/CLARIN digital library at Institute of
Formal and Applied Linguistics, Charles University
in Prague.

Jan Hajič. 2000. Morphological tagging: Data vs. dic-
tionaries. In NAACL.

Samer Hassan and Rada Mihalcea. 2009. Cross-
lingual semantic relatedness using encyclopedic
knowledge. In EMNLP.

Daphne Koller and Nir Friedman. 2009. Probabilistic
Graphical Models: Principles and Techniques. MIT
press.

Maximilian Köper, Christian Scheible, and
Sabine Schulte im Walde. 2015. Multilingual
reliability and semantic structure of continuous
word spaces. In IWCS.

Ira Leviant. 2016. Separated by an Un-common Lan-
guage: Towards Judgment Language Informed Vec-
tor Space Modeling. Ph.D. thesis, Technion.

Minh-Thang Luong, Richard Socher, and C Manning.
2013. Better word representations with recursive
neural networks for morphology. In CoNLL.

Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey
Dean. 2013a. Efficient estimation of word represen-
tations in vector space. In ICLR Workshop.

Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg Cor-
rado, and Jeffrey Dean. 2013b. Distributed repre-
sentations of words and phrases and their composi-
tionality. In NIPS.

Tomas Mikolov, Wen-tau Yih, and Geoffrey Zweig.
2013c. Linguistic regularities in continuous space
word representations. In NAACL.

George A. Miller, Richard Beckwith, Christiane
Fellbaum, Derek Gross, and Katherine J Miller.
1990. Introduction to WordNet: An on-line lexi-
cal database. International Journal of Lexicogra-
phy, 3(4):235–244.

Andriy Mnih and Geoffrey Hinton. 2007. Three new
graphical models for statistical language modelling.
In ICML.

Garrett Nicolai, Colin Cherry, and Grzegorz Kondrak.
2015. Morpho-syntactic regularities in continuous
word representations: A multilingual study. In Pro-
ceedings of Workshop on Vector Space Modeling for
NLP, pages 129–134.

Siyu Qiu, Qing Cui, Jiang Bian, Bin Gao, and Liu Tie-
Yan. 2014. Co-learning of word representations and
morpheme representations. In COLING.

Radim Řehůřek and Petr Sojka. 2010. Software
Framework for Topic Modelling with Large Cor-
pora. In Proceedings of the LREC 2010 Workshop
on New Challenges for NLP Frameworks, pages 45–
50.

Sam Roweis. 1998. EM algorithms for PCA and
SPCA. In NIPS.

Richard Socher. 2014. Recursive Deep Learning for
Natural Language Processing and Computer Vision.
Ph.D. thesis, Stanford University.

John Sylak-Glassman, Christo Kirov, David Yarowsky,
and Roger Que. 2015. A language-independent fea-
ture schema for inflectional morphology. In ACL.

Matt Taddy. 2015. Document classification by in-
version of distributed language representations. In
ACL.

Michael E Tipping and Christopher M Bishop. 1999.
Probabilistic principal component analysis. Journal
of the Royal Statistical Society B, 61(3):611–622.

John Wieting, Mohit Bansal, Kevin Gimpel, Karen
Livescu, and Dan Roth. 2015. From paraphrase
database to compositional paraphrase model and
back. TACL.

1660


