



















































A Multi-answer Multi-task Framework for Real-world Machine Reading Comprehension


Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 2109–2118
Brussels, Belgium, October 31 - November 4, 2018. c©2018 Association for Computational Linguistics

2109

A Multi-answer Multi-task Framework for Real-world Machine Reading
Comprehension

Jiahua Liu1,2, Wan Wei2, Maosong Sun1, Hao Chen2, Yantao Du2, Dekang Lin2∗
1State Key Laboratory of Intelligent Technology and Systems,

Tsinghua National Laboratory for Information Science and Technology,
Department of Computer Science and Technology, Tsinghua University, Beijing, China

2Naturali Ltd, Beijing, China

Abstract

The task of machine reading comprehension
(MRC) has evolved from answering simple
questions from well-edited text to answering
real questions from users out of web data.
In the real-world setting, full-body text from
multiple relevant documents in the top search
results are provided as context for questions
from user queries, including not only ques-
tions with a single, short, and factual an-
swer, but also questions about reasons, pro-
cedures, and opinions. In this case, multiple
answers could be equally valid for a single
question and each answer may occur multiple
times in the context, which should be taken
into consideration when we build MRC sys-
tem. We propose a multi-answer multi-task
framework, in which different loss functions
are used for multiple reference answers. Min-
imum Risk Training is applied to solve the
multi-occurrence problem of a single answer.
Combined with a simple heuristic passage ex-
traction strategy for overlong documents, our
model increases the ROUGE-L score on the
DuReader dataset from 44.18, the previous
state-of-the-art, to 51.09.

1 Introduction

Machine reading comprehension (MRC) or ques-
tion answering (QA) has been a long-standing goal
in Natural Language Processing. There is a surge
of interest in this area due to new end-to-end mod-
eling techniques and the release of several large-
scale, open-domain datasets.

In earlier datasets (Hermann et al., 2015; Hill
et al., 2016; Yang et al., 2015; Rajpurkar et al.,
2016), the questions did not arise from actual end
users. Instead, they were constructed in cloze style
or created by crowdworkers given a short passage
from well-edited sources such as Wikipedia and
CNN/Daily Mail. As a consequence, the questions

∗∗Corresponding author: D. Lin (lindek@naturali.io).

are usually well-formed and about simple facts,
and the answers are guaranteed to exist as short
spans in the given candidate passages.

In MS-MARCO (Nguyen et al., 2016), the
questions were sampled from actual search
queries, which may have typos and may not be
phrased as questions.1 Multiple short passages,
which might have the answer to the query, were
extracted from webpages by a separate informa-
tion retrieval system.

He et al. (2017) made the DuReader dataset a
more realistic reflection of the real-world prob-
lem by including not only questions with relatively
short and factual answers, but also questions about
complex descriptions, procedures, opinions, etc.
which may have multiple, much longer answers,
or no answer at all. Furthermore, full-body text
from webpages listed in top search results are di-
rectly provided as context. These documents tend
to be much noiser than Wikipedia and CNN. They
are much longer (5 times longer than those in
MS-MARCO on average) and contain many para-
graphs that are irrelevant to the query.

New problems arise as we now consider the
task of machine reading comprehension in a much
more challenging real-world setting. First, multi-
ple valid answers to a single question are not only
possible but quite common. Figure 1 shows some
examples of questions with multiple answers from
the DuReader dataset. There could be multiple
ways to perform the same task (Question 1), mul-
tiple opinions about the same subject (Question 2),
or multiple explanations for the same observation
(Question 3). However, few works have been done
with multiple answers in machine reading com-
prehension. To address this problem, we propose
a multi-answer multi-task scheme which incorpo-
rates multiple reference answers in the objective

1We use these two terms, question and query, interchange-
ably in the following content



2110

Question 1: 
word  
character spacing in Word 
Answers: 
1. “ ” “ ” “ ”

“ ”  
2.  
3. “ ”→“ ”→“ ”  
1. Select the text you want to indent, and right-click on it to select Font, open the Font dialog box, and then select the Character Spacing tab, 
select the Kerning for fonts check box, and enter a number, at last click OK. 
2. Select the text you want to set character spacing, and right-click on it to select Font, and then switch to Character Spacing. 
3. click Format from the upper menu bar, select Font, select Character Spacing, and then change the point size.  

Question 2: 
       Is LONGZHU worth watching  

Answers: 
1.   1. It is, the pace is good and it's adorable and sweet. 
2.  2. It’s not, personally I don’t like that kind of TV series. 
3.    3. It’s ok, it’s fine to follow if you don’t care about the historical accuracy. 

Question 3: 
      Got “User Busy” message after one ring 

Answers: 
1.      1. Your number is in that person’s blacklist. 
2.       2. The person you called hung up on you. 

Figure 1: Examples of questions with multiple answers from the DuReader dataset

function (but still predicts a single answer in de-
coding time). We propose three different kinds of
multi-answer loss functions and compare their per-
formance through experiment.

Another problem is the multiple occurrences of
the same answer. As rich context is provided for a
single question, the same answer could occur more
than one time in different passages, or even at dif-
ferent places of the same passage. In this case,
using only one gold span for the answer could
be problematic, as the model is forced to choose
one span over others that contain the same con-
tent. To solve this problem, we propose to apply
Minimum Risk Training (MRT), which uses the
expected metric as the loss and gives reward to all
spans that are similar with the gold answer.

In this paper, we present a multi-answer, multi-
task objective function to train an end-to-end
MRC/QA system. We experiment with various al-
ternatives on the DuReader dataset and show that
our model out-performs other competing systems
and increases the state-of-the-art ROUGE-L score
by about 7 points.

2 Related Work

Various datasets have been released in recent
years, which fuel the research for reading compre-
hension and question answering. The CNN/Daily-
Mail dataset (Hermann et al., 2015) and the Chil-

dren’s Book Test (Hill et al., 2016) evaluate com-
prehension by filling in missing words from well-
edited texts. SQuAD (Rajpurkar et al., 2016) is
one of the most popular datasets for reading com-
prehension, where a span in a Wikipedia passage
is to be extracted to answer questions generated
by annotators. The WikiQA (Yang et al., 2015)
is another dataset from Wikipedia, where one sin-
gle sentence is to be selected to answer ques-
tions from search engine logs. Different from the
above datasets, the MS-MARCO dataset (Nguyen
et al., 2016) was built in a real-world setting. The
questions were real anonymized Bing queries and
multiple passages are extracted from related web
pages by a separate system. The DuReader (He
et al., 2017) is a Chinese dataset, similarly con-
structed from user queries as MS MACRCO, but
in a more realistic setting using Baidu Web Search
and Baidu Answers (Zhidao) data. While a small
proportion of questions were labeled with multiple
answers in MS MARCO (9.93%), more than half
of the DuReader queries were annotated with mul-
tiple answers, which provides the perfect setup for
our work.

Great effort has been put into the development
of sophisticated neural models for machine read-
ing comprehension. The attention mechanism was
first introduced by Hermann et al. (2015) into
reading comprehension and soon became the dom-



2111

inating model. Wang and Jiang (2017) proposed
to solve machine comprehension using Match-
LSTM and answer pointer. Seo et al. (2017)
and Xiong et al. (2017) applied different ways
to match the question and the context with bi-
directional attention. Hu et al. (2017) used iter-
ative aligner to match the question and the pas-
sage with feature-rich encoder. Cui et al. (2017)
employed one more layer of attention over the
bi-directional attention mechanism. Wang et al.
(2017) applied a self-matching mechanism to ag-
gregate evidence from the context. Tan et al.
(2018) proposed to generate answer from ex-
tracted answer span. Yu et al. (2018) proposed to
use convolution with self-attention instead of re-
current models in reading comprehension.

Recently there are some emerging works start-
ing to touch the reading comprehension task from
the answer side. Wang et al. (2018a) proposed to
use evidence aggregation to re-rank answer candi-
dates extracted from different passages, and Wang
et al. (2018b) proposed Cross-Passage Answer
Verification model for the same purpose. Neither
of them involved multiple answers as in this work.

Minimum Risk Training (MRT) has been
widely used in various tasks in NLP. Shen et al.
(2016) introduced MRT into Neural Machine
Translation, and Ayana et al. (2016) applied it in
Text Summarization.

3 Our Approach

In this section we describe in details the architec-
ture of our model which is depicted in Figure 2.

3.1 Passage Extraction

Unlike most other datasets where the source of an-
swers is a short passage with a few hundred words,
the DuReader dataset provides up to 5 full docu-
ments, which may contain up to 100K words. This
incurs exorbitant demand on memory and train-
ing time. To deal with this issue, previous ap-
proaches select a single representative paragraph
for each document, on which the answer extrac-
tion is performed. The original paper of DuReader
(He et al., 2017) employed a simple heuristic strat-
egy, and Wang et al. (2018b) trained a paragraph
ranking model, while Clark and Gardner (2017)
applied TF-IDF based method for the TriviaQA
dataset (Joshi et al., 2017) which was in a simi-
lar situation. However, answers could come from
more than one paragraph. We apply a simple yet

effective method to extract contents from multi-
ple paragraphs of the document, aiming to include
as much information for the answer extraction as
possible.

We concatenate the title and the whole docu-
ment as the passage if it is shorter than a prede-
fined maximum length. If not, we employ passage
extraction in the following way:

• The title of the document is extracted.
Whether a document is relevant to the ques-
tion could be easily seen from the title.

• We compute BLEU-4 score of each para-
graph relative to the question, and select the
one that appears first in the document among
paragraphs with top-k scores.

• We extract the full body of this selected para-
graph and the next paragraph.

• For each of the following paragraphs, the first
sentence is extracted as it probably contains
the main point.

• We concatenate all the extracted contents to
form the extracted passage, and it is truncated
to the maximum length if it is longer than the
predefined value.

We apply our model on the basis of the extracted
passages.

3.2 Representation of Word

Given a word sequence of question Q = {wqt }
m
t=1,

and a word sequence of extracted passage P =
{wpt }

n
t=1, we combine different useful information

to form the representation of each question word
wqt and passage word w

p
t :

• Word-level embedding: each word w in the
question and passage is mapped to its corre-
sponding n-dimensional embedding we.

• POS tag embedding: we use a POS tagger to
tag each word in the question and passage.
Each POS tag is mapped to a m-dimensional
embedding pe.

• Word-in-question feature: following Chen
et al. (2017) and Weissenborn et al. (2017),
we use one additional binary feature wiq for
each passage word, indicating whether this
word occurs in the question.



2112

Figure 2: Model Architecture

Each question word is represented as the con-
catenation of the word embedding we, and the
POS tag embedding pe, denoted as xq = [we; pe].
Each passage word is additionally concatenated
with the word-in-question feature wiq: xp =
[we; pe;wiq].

It should be noted that, character-level embed-
ding is an important part of word representation
in English MRC models (Seo et al., 2017; Weis-
senborn et al., 2017; Wang et al., 2017; Tan et al.,
2018). Character sequence would give informa-
tion which helps to relieve the OOV problem, as
many English words share the same stem and dif-
fer only in prefix or suffix. However, this is not the
case in Chinese, and we observe no significant im-
provement incorporating character-level embed-
ding into our system.

3.3 Encoding Layer

Following previous work, we use a bi-directional
LSTM to obtain contextual encoding for each

word in the question and passage respectively:

uqi = BiLSTM(u
q
i−1, x

q
i ) (1)

upj = BiLSTM(u
p
j−1, x

p
j ) (2)

3.4 Match Layer
To fuse question encoding and passage encoding,
we adopt the Attention Flow Layer (Seo et al.,
2017) with a simpler similarity function. The sim-
ilarity score between the contextual encoding for
a query word uqi and that for a passage word u

p
j is

defined as:
sij = u

q
i
T · upj (3)

The context-to-query attention vectors cpj are
computed from the similarity scores:

aij =
exp(sij)∑m
k=1 exp(skj)

(4)

cpj =

m∑
i=1

aiju
q
i (5)



2113

The query-to-context attention vector dp is com-
puted as:

zj = max
i
sij (6)

bj =
exp(zj)∑n
k=1 exp(zk)

(7)

dp =

n∑
j=1

bju
p
j (8)

Another BiLSTM is applied on top of them to
get the question-aware passage representation:

hpj = BiLSTM(h
p
j−1, [u

p
j ; c

p
j ;u

p
j ◦ c

p
j ;u

p
j ◦ d

p])
(9)

3.5 Multi-answer multi-task loss function

3.5.1 Answer prediction with multi-answer
A reading comprehension model is typically
trained as an extractor of an answer span from a
candidate passage. In DuReader dataset, multiple
reference answers are provided for a single ques-
tion. For each of the reference answers, we add the
span with the highest F1 score to the gold answer
spans. For models considering only a single an-
swer span (baseline model), the gold answer span
is the one with the highest F1 score relative to any
of the reference answers (He et al., 2017; Wang
et al., 2018b).

In the boundary model with pointer network
(Wang and Jiang, 2017; Wang et al., 2017; Tan
et al., 2018), two probability distributions y1j and
y2j (j = 1 . . . n), which denote the probability that
position j is the beginning or the end of the answer
span respectively, are computed as follows:

stj = v
T tanh(W phh

p
j +W

P
a h

a
j−1) (10)

ytj =
exp(stj)∑n
k=1 exp(s

t
k)

(11)

ct =
n∑
j=1

ytjh
p
j (12)

hat = BiLSTM(h
a
t−1, ct) (13)

where t = 1, 2, and the initial hidden state ha0 is
generated by an attention-pooling over the ques-
tion representation following Wang et al. (2017):

si = v
T tanh(W quu

q
i + b) (14)

ai =
exp(si)∑m
k=1 exp(sk)

(15)

ha0 =
m∑
i=1

aiu
q
i (16)

Note that all passages for the same question are
concatenated in order to predict one answer span.
The loss is defined as the sum of negative log prob-
abilities of the ground truth start and end position
based on the predicted distributions:

L = −(log y1start + log y2end) (17)

We propose three different ways to incorporate
multiple answers. A simple solution is to compute
the average loss for multiple answer spans:

Lavg = −
1

A

A∑
k=1

(log y1startk + log y
2
endk

) (18)

Lavg treats all answer spans as equally good.
However, some of them may be closer to human-
generated answers than others. We therefore de-
fine the weighted average loss as follow:

Lwavg = −
A∑
k=1

wk(log y
1
startk

+ log y2endk) (19)

where wk is the F-score between the answer span
and the corresponding human-generated answer,
normalized by the sum of the scores of each an-
swer.

Another solution is to use the minimal value of
the loss from each span:

Lmin = min
k

(−(log y1startk + log y
2
endk

)) (20)

Instead of predicting all answer spans, this loss
will encourage the model to predict only the easi-
est answer span for it.

The answer span prediction loss Lap is defined
as the average of any of the loss functions de-
scribed above over the training set.

3.5.2 Passage selection with multi-answer
Tan et al. (2018) showed that their single-answer,
multi-passage MRC model benefits from using
multi-task learning by adding an auxiliary loss to
predict the correct passage to extract the answer
from. We adapt the idea to compute passage se-
lection loss Lps in multi-answer setting.



2114

We first apply attention-pooling over the pas-
sage representation {hpj}nj=1, and then calculate a
matching score g for each passage:

sj = v
T tanh(W puh

p
j + b) (21)

aj =
exp(sj)∑n
k=1 exp(sk)

(22)

rp =
n∑
j=1

ajh
p
j (23)

g = sigmoid(vTspr
p) (24)

Since multiple answers are provided in the
DuReader dataset, multiple passages may contain
correct answers. The match score g for different
passages are not in competition against one an-
other. We therefore used pointwise sigmoid func-
tion instead of the softmax function (as in Tan
et al. (2018)) in the passage selection loss Lps:

Lps = −
1

K

K∑
k=1

(yk log gk

+(1− yk) log(1− gk))

(25)

where yk = 1 if one of the gold span comes from
this passage, yk = 0 otherwise.

3.5.3 Joint training
We train our model by jointly optimizing answer
span prediction loss and passage selection loss:

L = Lap + λpsLps (26)

where λps is a hyper-parameter tuned on the dev
set.

3.6 Minimum Risk Training
Minimum Risk Training (MRT) has been widely
used in various tasks in NLP. The basic idea is
to directly optimize the evaluation metric instead
of maximizing the log likelihood of training data
using Maximum Likelihood Estimation (MLE) as
described above. In MRT, the object is to mini-
mize the expected loss with respect to the posterior
distribution:

JMRT (θ) =
1

N

N∑
i=1

Eyi|xi;θ[∆(yi, y
∗
i )] (27)

where ∆(yi, y∗i ) is a function which indicates the
difference between the predicted result yi and the
label result y∗i .

In this work, we apply MRT to solve the prob-
lem of multi-occurrence of answer in machine
reading comprehension, directly using the metric
(ROUGE-L) as ∆. As an answer occurs multi-
ple times in the context, each span in which the
answer occurs will have minimum difference with
the answer, and is thus given a high score by a
model trained with MRT.

In machine translation and many other tasks, to
compute the expected metric with respect to the
posterior distribution is often intractable. Thus
sampling methods are commonly used in MRT
training. However, in our span extraction model,
we use all spans without sampling.

Formally, the MRT loss in our model is defined
as:

JMRT (θ) =
1

N

N∑
i=1

|P |∑
k=1

|P |∑
l=k+1

y1ky
2
l ∆(Pk,l, A)

(28)
As in Hu et al. (2017), we minimize the linear

combination of MLE and MRT loss:

J(θ) = JMLE(θ) + λJMRT (θ) (29)

where JMLE(θ) refers to L in equation 26 and λ is
a hyper-parameter tuned on the development set.

4 Experiment

We conduct our experiment on the DuReader
dataset (He et al., 2017), where multiple passages
containing full-body text are provided for each
question, and over half of the questions have mul-
tiple answers.

4.1 Dataset and Evaluation Metrics
The DuReader dataset consists of 201574 ques-
tions in total, with 181574 in the training set,
10000 in the development set, and 10000 in the
test set. The questions are sampled from fre-
quently occurring queries from Baidu search en-
gines, and the full-body text of top-5 search results
from the web are provided as the context.

BLEU-4 and ROUGE-L are used in evaluation
on DuReader. However, the implementations for
the two metrics are quite different in the official
evaluation tool. As in MS-MARCO, the BLEU-
4 score is normalized across all questions, essen-
tially giving different weights to different ques-
tions, while the ROUGE-L is averaged across dif-
ferent questions. We mainly focus on ROUGE-
L as each question in a reading comprehension



2115

Model ROUGE-L BLEU-4
BiDAF baseline 37.68 35.51
+ passage extraction 44.57 38.03
+ rich feature 48.93 42.17

Table 1: The influence of passage extraction and rich-
feature representation on the development set

Loss ROUGE-L ∆
single answer 48.93 -
Lmin 49.05 + 0.12
Lavg 49.67 + 0.74
Lwavg 49.77 + 0.84

Table 2: Comparison among different choices for the
loss function with multiple answers on the development
set

dataset should have equal weight in evaluation
(Tan et al., 2018). For a single question with mul-
tiple reference answers, the maximum score with
any reference answers is used, as implemented in
the official tool for ROUGE-L. This is reasonable
as providing one valid answer is good enough in
many cases.

4.2 Implementation Details
4.2.1 Word and POS Tag Embedding
We train a segmentation model with one-layer
BiLSTM using the DuReader dataset, and apply
it to a subset of SogouT corpus2, which con-
tains a large amount of Chinese web pages(Liu
et al., 2012). 256-dimension word embeddings are
trained on this data with language model task us-
ing one-layer BiLSTM model.

As for POS tag, we use a POS tagger trained on
the Chinese Treebank (CTB) data to tag each word
in questions and passages in the DuReader dataset.
64-dimension POS tag embeddings are trained on
this data using one-layer BiLSTM model.

We keep all word and POS tag embeddings
fixed during training.

4.2.2 Training and Parameters
The maximum length of each passage is set to be
500. The batch size is set to be 32. The dimen-
sion of hidden vector is set to be 150 for all lay-
ers. Dropout (Srivastava et al., 2014) is applied be-
tween layers, with a dropout rate of 0.15. We use
λps = 3 for passage selection loss and λ = 10 for

2http://www.sogou.com/labs/resource/t.
php

Model ROUGE-L ∆
single-answer baseline 48.93 -
+ multi-answer loss 49.77 + 0.84
+ passage selection loss 49.96 + 1.03
+ MRT 50.62 + 1.69

Table 3: Results with multi-answer multi-task loss and
Minimum Risk Training on the development set

MRT. All parameters are tuned on the DuReader
development set.

As MRT training is more time-consuming than
MLE training, our MRT model is initialized with
model trained with MLE. It usually obtains the
best result in just one epoch, which results in fea-
sible training time.

Our model is optimized with Adam algorithm
(Kingma and Ba, 2014), and the learning rate is
fixed to 0.001 during training.

4.3 Results

4.3.1 Single-Answer Baseline

Table 1 shows the results for passage extraction
and rich-feature representation (pre-trained word,
POS, and word-in-query embeddings) on the de-
velopment set. Both of them dramatically in-
crease ROUGE-L and BLEU-4 score over the
BiDAF baseline from the original DuReader pa-
per. Together they form our single-answer base-
line, on which we test the effectiveness of the
multi-answer multi-task loss and Minimum Risk
Training.

4.3.2 Different loss functions with
multi-answer

Table 2 shows the experimental results with three
different multi-answer loss functions introduced
in Section 3.5.1. All of them offer improvement
over the single-answer baseline, which shows the
effectiveness of utilizing multiple answers. The
average loss performs better than the min loss,
which suggests that forcing the model to predict
all possible answers is better than asking it to just
find the easiest one. Not surprisingly, by taking
into account the quality of different answer spans,
the weighted average loss outperforms the average
loss and achieves the best result among the three.
All later experiments are conducted based on the
weighted average loss.

http://www.sogou.com/labs/resource/t.php
http://www.sogou.com/labs/resource/t.php


2116

Model ROUGE-L BLEU-4
BiDAF (He et al., 2017) 39.0 31.8
Match-LSTM (He et al., 2017) 39.2 31.9
PR+BiDAF (Wang et al., 2018b) 41.81 37.55
PE+BiDAF (ours) 45.93 38.86
V-Net (Wang et al., 2018b) 44.18 40.97
Our complete model 51.09 43.76
Human 57.4 56.1

Table 4: Performance of our model and competing models on the DuReader test set

ROUGE-L
Model Qs Qm

single-answer 38.01 53.8
multi-answer 38.66 54.65

Table 5: Results on Qs and Qm

4.3.3 Multi-task Loss and Minimum Risk
Training

As we can see in Table 3, the ROUGE-L score on
the DuReader development set increases to 49.77
by incorporating multi-answer into the loss func-
tion. Joint learning with passage selection loss
yields an increase of 0.19. And with Minimum
Risk Training, our model can reach a ROUGE-L
score of 50.62, with a further increment of 0.66.

4.3.4 Comparison with State-of-the-art
Table 4 shows the performance of our model and
other state-of-the-art models on the DuReader test
set. First, we compare our passage extraction
method with the paragraph ranking model from
Wang et al. (2018b). Based on the same BiDAF
model described in Section 3.4, our method
(PE+BiDAF) significantly outperforms the trained
model from Wang et al. (2018b) (PR+BiDAF)
on the DuReader test set. As we can see, our
complete model achieves the state-of-the-art per-
formance in both ROUGE-L and BLEU-4, and
greatly narrows the performance gap between
MRC system and human in the challenging real-
world setting.

4.3.5 Further Analysis
For further analysis, we construct two sets from
the development set. Qs contains 2787 questions
with a single reference answer, and Qm contains
6650 questions with more than one reference an-
swer. 563 questions from the development set are
labeled with no answer, and thus not included in

Qs or Qm. Table 5 shows the performance of
our model on Qs and Qm. It can be seen that
even questions with single answer (Qs) can ben-
efit from using multiple answers in training. The
improvement for Qm is higher than that for Qs.

5 Conclusion

In this paper, we focus on real-world machine
reading comprehension. We propose a multi-
answer multi-task framework to tackle the multi-
answer problem which is common in everyday
world. Minimum Risk Training is applied to solve
the multi-occurrence problem of the answer. We
also propose a simple method for passage extrac-
tion which solves the length issue of the pas-
sage. Experimental results indicate that our model
achieves state-of-the-art performance in the chal-
lenging DuReader dataset.

Despite using multiple answers in training, our
system only predicts a single answer in decoding
time. However, in some cases (e.g. for questions
about opinion), finding all possible answers may
be desirable. In the future, we plan to design mod-
els which could generate all possible answers for
a single question.

References
Ayana, Shiqi Shen, Zhiyuan Liu, and Maosong Sun.

2016. Neural headline generation with minimum
risk training. Computing Research Repository,
arXiv:1604.01904. Version 2.

Danqi Chen, Adam Fisch, Jason Weston, and Antoine
Bordes. 2017. Reading wikipedia to answer open-
domain questions. In Proceedings of the 55th An-
nual Meeting of the Association for Computational
Linguistics (Volume 1: Long Papers), pages 1870–
1879. Association for Computational Linguistics.

Christopher Clark and Matt Gardner. 2017. Simple and
effective multi-paragraph reading comprehension.



2117

Computing Research Repository, arXiv:1710.10723.
Version 2.

Yiming Cui, Zhipeng Chen, Si Wei, Shijin Wang,
Ting Liu, and Guoping Hu. 2017. Attention-over-
attention neural networks for reading comprehen-
sion. In Proceedings of the 55th Annual Meeting of
the Association for Computational Linguistics (Vol-
ume 1: Long Papers), pages 593–602. Association
for Computational Linguistics.

Wei He, Kai Liu, Yajuan Lyu, Shiqi Zhao, Xinyan
Xiao, Yuan Liu, Yizhong Wang, Hua Wu, Qiaoqiao
She, Xuan Liu, et al. 2017. Dureader: a chinese
machine reading comprehension dataset from real-
world applications. Computing Research Reposi-
tory, arXiv:1711.05073.

Karl Moritz Hermann, Tomas Kocisky, Edward
Grefenstette, Lasse Espeholt, Will Kay, Mustafa Su-
leyman, and Phil Blunsom. 2015. Teaching ma-
chines to read and comprehend. In C. Cortes, N. D.
Lawrence, D. D. Lee, M. Sugiyama, and R. Garnett,
editors, Advances in Neural Information Processing
Systems 28, pages 1693–1701. Curran Associates,
Inc.

Felix Hill, Antoine Bordes, Sumit Chopra, and Jason
Weston. 2016. The goldilocks principle: Reading
children’s books with explicit memory representa-
tions. In Proceedings of International Conference
on Learning Representations.

Minghao Hu, Yuxing Peng, and Xipeng Qiu. 2017.
Reinforced mnemonic reader for machine com-
prehension. Computing Research Repository,
arXiv:1705.02798. Version 5.

Mandar Joshi, Eunsol Choi, Daniel S Weld, and
Luke Zettlemoyer. 2017. Triviaqa: A large scale
distantly supervised challenge dataset for reading
comprehension. Computing Research Repository,
arXiv:1705.03551.

Diederik P Kingma and Jimmy Ba. 2014. Adam: A
method for stochastic optimization. Computing Re-
search Repository, arXiv:1412.6980.

Yiqun Liu, Fei Chen, Weize Kong, Huijia Yu, Min
Zhang, Shaoping Ma, and Liyun Ru. 2012. Iden-
tifying web spam with the wisdom of the crowds.
ACM Transactions on the Web (TWEB), 6(1):2.

Tri Nguyen, Mir Rosenberg, Xia Song, Jianfeng Gao,
Saurabh Tiwary, Rangan Majumder, and Li Deng.
2016. Ms marco: A human generated machine read-
ing comprehension dataset. Computing Research
Repository, arXiv:1611.09268.

Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and
Percy Liang. 2016. Squad: 100,000+ questions for
machine comprehension of text. In Proceedings of
the 2016 Conference on Empirical Methods in Nat-
ural Language Processing, pages 2383–2392. Asso-
ciation for Computational Linguistics.

Minjoon Seo, Aniruddha Kembhavi, Ali Farhadi, and
Hannaneh Hajishirzi. 2017. Bidirectional attention
flow for machine comprehension. In Proceedings of
International Conference on Learning Representa-
tions.

Shiqi Shen, Yong Cheng, Zhongjun He, Wei He, Hua
Wu, Maosong Sun, and Yang Liu. 2016. Minimum
risk training for neural machine translation. In Pro-
ceedings of the 54th Annual Meeting of the Associa-
tion for Computational Linguistics (Volume 1: Long
Papers), pages 1683–1692. Association for Compu-
tational Linguistics.

Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky,
Ilya Sutskever, and Ruslan Salakhutdinov. 2014.
Dropout: A simple way to prevent neural networks
from overfitting. The Journal of Machine Learning
Research, 15(1):1929–1958.

Chuanqi Tan, Furu Wei, Nan Yang, Bowen Du,
Weifeng Lv, and Ming Zhou. 2018. S-net: From
answer extraction to answer synthesis for machine
reading comprehension.

Shuohang Wang and Jing Jiang. 2017. Machine com-
prehension using match-lstm and answer pointer. In
Proceedings of International Conference on Learn-
ing Representations.

Shuohang Wang, Mo Yu, Jing Jiang, Wei Zhang,
Xiaoxiao Guo, Shiyu Chang, Zhiguo Wang, Tim
Klinger, Gerald Tesauro, and Murray Campbell.
2018a. Evidence aggregation for answer re-ranking
in open-domain question answering. In Proceedings
of International Conference on Learning Represen-
tations.

Wenhui Wang, Nan Yang, Furu Wei, Baobao Chang,
and Ming Zhou. 2017. Gated self-matching net-
works for reading comprehension and question an-
swering. In Proceedings of the 55th Annual Meet-
ing of the Association for Computational Linguistics
(Volume 1: Long Papers), pages 189–198. Associa-
tion for Computational Linguistics.

Yizhong Wang, Kai Liu, Jing Liu, Wei He, Yajuan
Lyu, Hua Wu, Sujian Li, and Haifeng Wang. 2018b.
Multi-passage machine reading comprehension with
cross-passage answer verification. Computing Re-
search Repository, arXiv:1805.02220.

Dirk Weissenborn, Georg Wiese, and Laura Seiffe.
2017. Making neural qa as simple as possible but
not simpler. In Proceedings of the 21st Confer-
ence on Computational Natural Language Learn-
ing (CoNLL 2017), pages 271–280. Association for
Computational Linguistics.

Caiming Xiong, Victor Zhong, and Richard Socher.
2017. Dynamic coattention networks for question
answering. In Proceedings of International Confer-
ence on Learning Representations.



2118

Yi Yang, Wen-tau Yih, and Christopher Meek. 2015.
Wikiqa: A challenge dataset for open-domain ques-
tion answering. In Proceedings of the 2015 Con-
ference on Empirical Methods in Natural Language
Processing, pages 2013–2018. Association for Com-
putational Linguistics.

Adams Wei Yu, David Dohan, Minh-Thang Luong, Rui
Zhao, Kai Chen, Mohammad Norouzi, and Quoc V
Le. 2018. Qanet: Combining local convolution with
global self-attention for reading comprehension. In
Proceedings of International Conference on Learn-
ing Representations.


