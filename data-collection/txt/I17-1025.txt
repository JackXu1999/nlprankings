



















































Unsupervised Segmentation of Phoneme Sequences based on Pitman-Yor Semi-Markov Model using Phoneme Length Context


Proceedings of the The 8th International Joint Conference on Natural Language Processing, pages 243–252,
Taipei, Taiwan, November 27 – December 1, 2017 c©2017 AFNLP

Unsupervised Segmentation of Phoneme Sequences based on
Pitman-Yor Semi-Markov Model using Phoneme Length Context

Ryu Takeda and Kazunori Komatani
The Institute of Scientific and Industrial Research, Osaka University

8-1, Mihogaoka, Ibaraki, Osaka 567-0047, Japan
{rtakeda, komatani}@sanken.osaka-u.ac.jp

Abstract

Unsupervised segmentation of phoneme
sequences is an essential process to ob-
tain unknown words during spoken dia-
logues. In this segmentation, an input
phoneme sequence without delimiters is
converted into segmented sub-sequences
corresponding to words. The Pitman-Yor
semi-Markov model (PYSMM) is promis-
ing for this problem, but its performance
degrades when it is applied to phoneme-
level word segmentation. This is be-
cause of insufficient cues for the seg-
mentation, e.g., homophones are improp-
erly treated as single entries and their dif-
ferent contexts are also confused. We
propose a phoneme-length context model
for PYSMM to give a helpful cue at
the phoneme-level and to predict suc-
ceeding segments more accurately. Our
experiments showed that the peak per-
formance with our context model out-
performed those without such a context
model by 0.045 at most in terms of F-
measures of estimated segmentation.

1 Introduction

1.1 Motivation

The final goal of our current project is to achieve
the development of robots or systems that ac-
quire knowledge during spoken interactions be-
tween them and human beings in the open world.
Unknown or new words appear frequently in our
daily lives, and because their meanings may be
different for the systems deployed in different ar-
eas, automatic lexicon acquisition is a useful func-
tion for maintenance-free spoken dialogue sys-
tems.

I ate tempura

yesterday

Goal:  Unsupervised lexicon acquisition through spoken dialogue 

Focus of this study:  Segmentation of phoneme sequences

Word

Phoneme

I ate témpərə yesterday

άɪ  éɪt témpərə jéstɚdèɪ

άɪéɪttémpərəjéstɚdèɪ

Signal

segmentation

témpərə

is unknown

Phoneme recognition

Phoneme
sequence

Input:

Segmented
tokens

Output:

Word conversion & context understanding

Symbol

Figure 1: Our target problem

In this paper, we focus on the phoneme-level
representation of utterance – not in the signal-level
– to relax the problem, which results in an issue
with phoneme sequence segmentation. The seg-
mentation converts an input phoneme sequence
into segmented sub-sequences corresponding to
individual words. The focus of our study is illus-
trated in Figure 1. When the robot listens to a hu-
man utterance that includes an unknown word like
“tempura,” the robot will estimate the unknown
segment “tempura” by the iterative search based
on trial-and-error of many hypotheses among dif-
ferent layers. Here, the phoneme sequence of
an unknown word is necessary as an intermedi-
ate representation between signals and lexicon be-
cause we cannot directly obtain the spelling of
an unknown word just from sound information.
Note that our assumption of a phoneme sequence
given is partly supported by the high accuracy of
state-of-the-art speech and phoneme recognition
(Dahl et al., 2012; Hinton et al., 2012; Seide et al.,
2011a,b).

Approaches based on Bayesian nonparametrics
are promising methods to achieve lexical acqui-
sition from unsegmented characters or phonemes.
These methods estimate the segmentation labels of

243



άɪ

éɪt
jéstɚdèɪ

si:

si:

I

ate

yesterday

see

sea

2

3
8

2

2

Word
(character) Phoneme

Phoneme
length

I      see   him  [EOS]Word

Length

άɪ si: hím [EOS]Phoneme

0   2      2      3      0

Context
(Bi-gram) (2,0) (2,2) (3,2) (0,3)

Our idea

Figure 2: Example of phoneme-length and its con-
text

phonemes corresponding to words with an unsu-
pervised manner. The label represents the bound-
ary of each word. Mochihashi et al. proposed
the nested Pitman-Yor language model (NPYLM)
(Mochihashi et al., 2009), or Pitman-Yor semi-
Markov model (PYSMM) in other words. The
model achieved high computational efficiency
and high segmentation accuracy compared with
a previous method based on the hierarchical
Dirichlet process using simple Gibbs sampling
(Goldwater et al., 2006). Uchiumi et al. also
proposed a method that estimates the segmenta-
tion labels and part-of-speech tagging of words
at the same time based on Pitman-Yor hidden
semi-Markov models (PYHSMM) for character-
level segmentation (Murphy, 2002; Uchiumi et al.,
2015). PYHSMM has not been applied to the seg-
mentation of phoneme sequences.

The difficulty with phoneme sequence segmen-
tation is insufficient cues to distinguish or predict
the context and segmentation labels. For example,
the homophones are improperly treated as single
entry and their different contexts are also confused
in phoneme-level segmentation. This is a sim-
ilar situation with the homographs in character-
level segmentation, but it occurs much more fre-
quently in phoneme-level segmentation, resulting
in more serious problem. Although NPYLM and
PYHSMM have been applied to character-level
segmentation, they do not utilize cues useful for
phoneme-level segmentation. We need to deter-
mine such useful cues to achieve accurate segmen-
tation of phoneme sequences. Note that the per-
formance comparison of NPYLM and PYHSMM
methods in phoneme-level segmentation have not
been conducted. We believe that comparing these
methods on the basis of phoneme sequences is also
useful for further improvement of the model.

We propose a phoneme-length context model
for segmentation, which was not used in the
NPYLM and PYHSMM. Note that the length is
not the duration of a phoneme (the phoneme ‘a’
continues for three frames in the time axis, for

example), which is used in signal-level segmen-
tation (Lee and Glass, 2012). Figure 2 illustrates
phoneme length and its contexts in the case of
bi-grams. The phoneme sequence of ‘see’ and
‘sea’ is same and both lengths are two as shown
in the left side of Fig. 2. The context of phoneme
length is the sequence of these lengths. For ex-
ample, if ‘him’ succeeds to ’see’, the bi-gram of
phoneme length is the pair of 3 and 2 where 3 is
the length of phonemes ‘him’ and 2 is the length
of phonemes ’si:’. We denote the pair as (3, 2) as
shown in the right side of Fig. 2. Since the length
of each segmented phoneme also depends on the
previously segmented phonemes, this context rep-
resents one aspect of parts of speech. For exam-
ple, the phoneme-length context captures the ten-
dency that the length of the adposition is usually
short and the length of the succeeding segment
will be relatively long. We expect the phoneme-
length context to be another cue for segmenta-
tion because the phoneme-length context is more
abstract than word-level context. This phoneme-
length model is expected to capture a rhythmic as-
pect of language.

We model the phoneme-length context as a
prior probability distribution of sequential seg-
mentation labels. This is because the probability
distribution is expected to control how long each
segmented phoneme becomes. Since the joint
prior probability distribution of sequential seg-
mentation labels were decomposed into factorized
probabilities like N -gram, the phoneme-length
model follows the Markov process and has a tran-
sition probability. The transition probability is
also modeled and smoothed by using the Pitman-
Yor N -gram model as other language models did.
Our method, using NPYLM and PYHSMM, is
evaluated by using a conversational corpus in En-
glish and Japanese in terms of the F-measures of
the estimated segmentation labels. Because the
corpus contains fillers and hesitations, the prop-
erty of utterance used for evaluation matches our
research purpose.

1.2 Related Work

There are several approaches based on Bayesian
nonparametrics to achieve lexical acquisi-
tion from raw audio signals (Neubig et al.,
2010; Lee and Glass, 2012; Kamper et al.,
2016; Taniguchi et al., 2016), unsegmented
phonemes, or words (Mochihashi et al., 2009;

244



ά ɪ é ɪ t t é m p ə r ə j é s t ɚ d è ɪc
1 2 3 … cLIndex

z 0 1 0 0 1 0 0 0   0  0 0 1 0 0 0 0 1   0 0 1

άɪ  éɪt témpərə jéstɚ dèɪSegmented
phoneme seq.

Figure 3: Word segmentation

Goldwater et al., 2009; Elsner et al., 2013;
Uchiumi et al., 2015). The lexical acquisition
technique is necessary in other areas, such as
dialogue system that acquires knowledge through
dialogue (Ono et al., 2016).

The advantages of Bayesian approach com-
pared with other approaches (Kuo et al., 2007;
Räsänen et al., 2015) are that a) the number of
words in the system’s vocabulary can be increased
automatically in accordance with the amount of
data and b) the semi-supervised learning of seg-
mentation labels is easy to apply to utilize our
knowledge of language. A typical estimation
procedure is a Gibbs-sampling-based iteration of
1) the estimation of borders (segmentation la-
bels) given a temporal N -gram language model
(Goodman, 2001) and 2) the estimation of an N -
gram language model given the temporal segmen-
tation labels.

2 Unsupervised Segmentation and
Baselines

We explain the overview and segmentation al-
gorithm of NPYLM and PYHSMM as baseline
methods. PYHSMM is an extended model of
NPYLM to estimate the part-of-speech tagging of
segmented words at the same time.

2.1 Overview

The unsupervised segmentation problem is de-
fined finding the latent segmentation labels z =
[z1, ..., zLc ]T that correspond to each phoneme in
the phoneme sequence c = [c1, ..., cLc ]T with
length Lc. If the binary label zi = 1, the phoneme
sequence is separated after the phoneme ci. Fig-
ure 3 illustrates the role of z. Other latent param-
eters m = [m1, ...,mLm ] with length Lm are also
introduced to represent part of speech labels for
each segmented phoneme sequences if necessary.
The number of classes of part of speech label, M ,
is defined in advance of this study. The latent pa-
rameters are estimated by maximizing the follow-

ing probabilities:

arg max
z

p(z|c) ∝ p(c|z) or (1)
arg max

z,m
p(z,m|c) ∝ p(c|z,m)p(m), (2)

where NPYLM uses Eq. (1) and PYHSMM uses
Eq. (2). The definition of each likelihood, such as
p(c|z) and p(c|z,m), is important. Because the
border of phonemes z is given in these models, the
likelihood can be factorized like N -gram probabil-
ity. For example, the likelihood can be factorized
as p(ci+1, ..., cN |c1, ..., ci, z)p(c1, ..., ci|z), where
the phoneme segments are considered to be two
word segments w1 = c1...ci and w2 = ci+1...cLc .
This N -gram modeling is also adopted to decom-
pose the part-of-speech label m, and this controls
the grammar and the number of words.

The nested hierarchical Pitman-Yor language
model (NPYLM) is used to represent the fac-
torized N -gram probability (Mochihashi et al.,
2009). Here, we represent the context of N -gram
as �h and the depth of the hierarchical context tree
of �h as |�h|. Given the seating arrangement of cus-
tomers that are represented by hidden variables
�s in the hierarchical Chinese restaurant process
(CRP), the conditional probability of a word seg-
ment w with the context �h is defined as follows:

p(w|�s,�h) =
c�hw − d|�h|t�hw

c�h∗ + θ|�h|
+

θ�h + d|�h|t�h∗
c�h∗ + θ|�h|

p(w|�s,�h′), (3)

where c�hw is the count of word w at context
�h, and

c�h∗ =
∑

w c�hw is its sum.
�h′ is the reduced con-

text of �h, in which the relationship |�h′| = |�h| − 1
exists. t�hw is the number of tables at context

�h,
and t�h∗ is its sum. θ|�h| and d|�h| are the common

parameters of �h with the same depth |�h|. Here,
the uni-gram segment probability p(w = ci...cj) is
smoothed by the phoneme-level N -gram probabil-
ity p(ci, ..., cj) = p(cj |ci, ..., cj−1)p(ci, ..., cj−1).
Please see the work of Teh (2006) for the sampling
algorithm of seating arrangement.

The segmentation labels z and other parameters
such as the N -gram language model are updated
iteratively. If the segmentation labels are given, we
can calculate the statistics of the N -gram model.
If the N -gram model is given, we can estimate the
probability of the segmentation labels. Because
the update of the N -gram model is well known,
we explain the update of the estimation of the seg-
mentation labels in the latter parts of this section.

245



Algorithm 1 Backward sampling: Θ represents
parameter set
Require: t← N, i← 0, w0 ← E

while t > 0 do
Draw k ∝ p(wi|ctt−k+1, Θ)α[t][k]
Set wi ← ctt−k+1
Set t← t− k, i← i + 1

end while

2.2 Inference for NPYLM

Mochihashi et al. (2009) proposed introducing the
forward-backward inference to estimate the seg-
mentation labels efficiently. This method uses a
semi-Markov model, and it considers the prob-
lem as a sequential estimation of the hidden labels.
The procedure consists of two steps: forward fil-
tering and backward sampling.

The forward filtering calculates the forward
probability that is used for the Bayesian learn-
ing of the hidden Markov model (HMM) (Scott,
2002). The following equation denotes α[t][k] as
the probability of generating the partial phonemes
c1, ..., ct of c using the last k phonemes in the case
of bi-grams.

α[t][k] =
t−k∑
j=1

p(ctt−k+1|ct−kt−k−j+1)α[t − k][j], (4)

where α[0][0] = 1 and cn, ..., cm = cmn .
Backward sampling is achieved by drawing a

phoneme segment w from the end of a sentence
by using forward probability α[t][k]. Because the
end of sentence is represented by the special sym-
bol E, we can start sampling a word with the prob-
ability proportional to p(E|cNN−k). The algorithm
is summarized in Alg. 1.

Note that we do not use the correction of
phoneme-level N -gram probability based on the
phoneme length using the Poisson distribution in
the NPYLM. This is because the length property
is embedded into our model naturally.

2.3 Inference for PYHSMM

The PYHSMM, which is an extended model of
NPYLM, estimates the parts of speech of each
segmented phoneme. We expect that the per-
formance of PYHSMM is better than that of
NPYLM. The forward probability, α[t][k][m], is
newly introduced in the case of bi-grams, and the
following equation denotes it as the probability
of generating the partial phonemes c1, ..., ct with

part-of-speech m from the last k phonemes.

α[t][k][m] =
t−k∑
j=1

M∑
r=0

p(ctt−k+1|ct−kt−k−j+1,m)

p(m|r)α[t − k][j][r] (5)
where p(m|r) is the transition probability of the
latent parts of speech and assumed the hierarchical
Pitman-Yor language model.

The algorithm of the backward sampling is sim-
ilar to that of NPYLM. The parts of speech are
sampled as well as the segmentation label. Be-
cause the end of the sentence and its parts of
speech are represented using the special sym-
bol E, we can start sampling a word with the
probability proportional to p(E|cNN−k, E)p(E|m)
like NPYLM. Note that the computational cost of
PYHSMM is larger than that of NPYLM due to the
search part-of-speech labels.

3 Analysis and Our Approach

We focus on the distribution of phoneme length to
distinguish the confused contexts. If we have two
different words with the same pronunciation, we
can sometimes distinguish the phoneme represen-
tations of them on the basis of the lenght of the
preceding or succeeding phoneme segments. The
phoneme-length context will capture the tendency
that the length of the adposition is usually short
and the length of the succeeding segment will be
relatively long.

Figure 4 illustrates the real phoneme-length dis-
tribution in the English and Japanese spoken-
dialogue transcriptions used in our evaluation
(Sec. 5.2). Given that the function len(w) re-
turns the phoneme length of word w, the ma-
trices represent the bi-gram length probability
p(len(wn)|len(wn−1)), and the horizontal axis
is len(wn−1) and the vertical axis is len(wn).
wn represents the n-th word in each sentences.
The line graphs represent the uni-gram length
probability p(len(wn)). These probabilities were
calculated on the basis of maximum likeli-
hood estimation. Verb and Noun represent the
phoneme-length probability of verbs and nouns
in Japanese data, respectively. The definitions
of bi-gram and uni-gram probability for Verb and
Noun are p(len(wn)|len(wn−1), pos(wn)) and
p(len(wn|pos(wn)), where pos(w) is a function
that returns a part-of-speech tag of the word w.

We determined that the phoneme-length prob-
ability depends on 1) language, 2) context, and

246



5 10 15

5

10

15

0

0.1

0.2

0.3

0.4

0.7 0

English

2 10 12
2
4
6
8

10
12

0

0.2

0.4

0.6

0.8

864

Japanese

2 10 12
2
4
6
8

10
12

0

0.2

0.4

0.6

0.8

4 6 8

Japanese (Verb)

2 10 12
2
4
6
8

10
12

0

0.2

0.4

0.6

0.8

1

4 6 80.7 0

0.7 0

Japanese (Noun)

Phoneme length of wn-1 

Ph
on

em
e 

le
ng

th
 o

f w
n 

Prob.

Phoneme length of wn-1 
Ph

on
em

e 
le

ng
th

 o
f w

n 
Prob.

Phoneme length of wn-1 

Ph
on

em
e 

le
ng

th
 o

f w
n 

Prob.

Phoneme length of wn-1 

Ph
on

em
e 

le
ng

th
 o

f w
n 

Prob.

U
ni

-g
ra

m

U
ni

-g
ra

m
U

ni
-g

ra
m

U
ni

-g
ra

m

Bi-gram Bi-gram

Bi-gram Bi-gram

0.7 0

Figure 4: Real phoneme-length distribution

3) parts of speech. The bi-gram phoneme-length
probabilities in English are relatively similar to
each other but different from those in Japanese.
Some bi-gram probabilities have several peaks,
and they vary in accordance with parts of speech.
If we utilize this information, we will achieve an
accurate segmentation.

The straightforward approach to exploit
phoneme-length information is to utilize the prior
distribution of the segmentation labels z that is not
used in either NPYLM and PYSHMM. Because
the prior probability is considered to be the source
that determines the length of each phoneme
segment, embedding this prior into a model is
expected to improve the segmentation perfor-
mance. Therefore, we need to construct a model
that considers the prior of the segmentation labels
and should also reveal the performance of these
models for phoneme-level word segmentation.

4 Phoneme-Length Context Model for
Pitman-Yor Semi-Markov Models

We extend the NPYLM and PYHSMM to exploit
the phoneme-length patterns of each phoneme
segment. First, we explain our problem state-
ment for unsupervised segmentation of phoneme
sequences. Next, we derive the context model
of the phoneme length and show the forward-
backward algorithms for our extended NPYLM

and PYHSMM.

4.1 Problem Statement and Model

We exploit the probability of phoneme length in
estimating latent segmentation labels z and latent
part-of-speech labels m. The parameters are esti-
mated by maximizing the following probabilities:

arg max
z

p(z|c) ∝ p(c|z)p(z) or (6)
arg max

z,m
p(z,m|c) ∝ p(c|z,m)p(z|m)p(m). (7)

The former objective function is for NPYLM, and
the latter is for PYHSMM. The probabilities of
segmentation labels p(z) in Eq. (6) and p(z|m) in
Eq. (7) are used in our objective functions. p(z)
is a prior probability distribution of segmentation
labels z in Eq. (6).

We decompose each joint probability into N -
gram probabilities. For an easy explanation of this
decomposition, here we use the length of part-
of-speech labels Lm and the correct segmenta-
tion labels as if these are given, which are actu-
ally searched for during training. The non-zero
indices of segmentation labels z are represented
by g = [g1, ..., gW ], where W is the number of
“true” phoneme segments. W equals Lm in the
case of part-of-speech estimation. We also define
g′i = gi + 1. The factorized models in the case

247



of bi-grams for NPYLM and PYHSMM are repre-
sented as follows:

p(c|z)=
∏

i

p(cgi
g′i−1

|cgi−1
g′i−2

, zgi
g′i−1

, z
gi−1
g′i−2

), (8)

p(z)=
∏

i

p(zgig′i−1 |z
gi−1
g′i−2

), (9)

p(c|z,m)=
∏

i

p(cgi
g′i−1

|cgi−1
g′i−2

, zgi
g′i−1

, z
gi−1
g′i−2

,mi),(10)

p(z|m)=
∏

i

p(zgi
g′i−1

|zgi−1
g′i−2

,mi), (11)

p(m)=
∏

i

p(mi|mi−1), (12)

where p(mi|mi−1) is a transition probability of la-
tent part-of-speech labels, zgi

g′i−1
= zgi−1+1, ..., zgi

and p(zgi
g′i−1

|zgi−1
g′i−2

) and p(zgi
g′i−1

|zgi−1
g′i−2

,mi) are tran-
sition probabilities of the segmentation labels. The
transition probability of segmentation labels is de-
rived naturally. The latent variables for the seating
arrangement of N -gram probability in Eq. (3) are
omitted in these equations.

We design the transition probability of segmen-
tation labels, such as p(zgig′i−1 |z

gi−1
g′i−2

), to depend on
the length of each phoneme segment. Because
the length of each segment can be represented us-
ing the non-zero indices g, the bi-gram transition
probability is rewritten as

p(zgig′i−1 |z
gi−1
g′i−2

) = p(gi − gi−1|gi−1 − gi−2) (13)

where mgi is omitted in the case of PYHSMM,
and each integer, such as gi − gi−1, is considered
as a symbol or label. This transition probability is
the phoneme-length bi-gram probability as men-
tioned in Sec. 3, and it is also modeled by the hi-
erarchical Pitman-Yor language model (HPYLM)
not by a Poisson distribution (Mochihashi et al.,
2009) because HPYLM is a count-based represen-
tation, which is appropriate for multimodal distri-
bution. Such probability for duration modeling is
also seen in (Kuo et al., 2007).

4.2 Inference

We derived the forward-backward algorithms to
estimate the segmentation labels and part-of-
speech labels. The inference for NPYLM is in-
troduced first; then, the inference of PYHSMM is
explained. Note that the segmentation label z and
part-of-speech labels m are estimated simultane-
ously.

The forward probability α[t][k] of NPYLM
with phoneme-length context is modified as fol-
lows.

α[t][k] =
t−k∑
j=1

p(ctt−k+1|ct−kt−k−j+1, k, j)

p(k|j)α[t − k][j] (14)

where p(k|j) is a transition probability of the
length of each phoneme segment. The forward
probability is modified by the bi-gram probabil-
ity of lengths. We can use p(ctt−k+1|ct−kt−k−j+1, k)
instead of p(ctt−k+1|ct−kt−k−j+1, k, j) because infor-
mation of length j is included in the phoneme se-
quence representation, ct−kt−k−j+1.

As with NPYLM, the context of phoneme
lengths can be embedded into PYHSMM. The for-
ward probability is also represented as

α[t][k][m]=
t−k∑
j=1

M∑
r=0

p(ctt−k+1|ct−kt−k−j+1, k, j,m)

p(k|j,m)p(m|r)α[t − k][j][r] (15)

where m represents a part of speech. The
forward probability is also biased by a
transition probability of the length of each
phoneme segment p(k|j,m). We can also use
p(ctt−k+1|ct−kt−k−j+1, k,m). Because the number
of parameters is large in this case of latent parts
of speech, the convergence speed will degrade
compared with NPYLM. Backward sampling
of both cases is achieved in the same way as in
NPYLM. The details are omitted due to space
limitation.

4.3 Substitution

We substitute the conditional probability into sim-
pler one by ignoring the dependency on length k
in this work as follows:

p(ctt−k+1|ct−kt−k−j+1, k):=p(ctt−k+1|ct−kt−k−j+1).(16)

The probability should be ideally normalized only
on the tokens that have length k, and this sub-
stitution makes a double count of the length in-
formation of token ctt−k+1. On the other hand,
the difference between NPYLM and our pro-
posed model in the inference is clear. The tran-
sition probability p(k|j) is added in our model,
and its implementation becomes simple. Our
strict model will be evaluated in the future work.

248



We also use p(ctt−k+1|ct−kt−k−j+1,m) instead of
p(ctt−k+1|ct−kt−k−j+1, k,m).

Note that there are several options on the
back-off structure of the conditional probabil-
ity p(ctt−k+1|ct−kt−k−j+1, k). For example, the
word uni-gram p(ctt−k+1|k) might be smoothed
by un-conditioned word uni-gram or by the k-
conditioned character N -gram. If the amount of
data is limited, the parameter estimation of the
conditional character N -gram may fail. We adopt
the existing model, NPYLM, as the structure in
this work. The optimal structure should also be
investigated in the future work.

5 Experiments

5.1 Evaluation Procedure
We evaluated each model by comparing the es-
timated segmented labels with the correct seg-
mented original phoneme text (transcription of
speech corpus). Utterances in the corpus were di-
vided manually, and each utterance was treated as
one sentence. The unsegmented phoneme text is
generated by replacing the word-segmented tran-
scription text into phoneme text with a dictionary
and by removing whitespaces.

The criteria for the evaluations were the F-
measures of the estimated lexicon set and segmen-
tation label set. These F-measures are the har-
monic mean of recall and precision; therefore, we
could consider the recall and precision at the same
time. The lexicon set after unsupervised segmen-
tation was compared with that of the original seg-
mented phoneme text. The estimated set of seg-
mentation labels was also compared with that of
the original phoneme text.

We used a development set to monitor the max-
imum performance of segmentation methods. The
1% data set was randomly selected from each
test set, and its F-measure of segmentation la-
bels was used to determine the epochs for cal-
culating F-measures. Since we can obtain some
correctly-transcribed text in a real situation, this
evaluation process is reasonable. First, we ran
each method over a sufficient number of epochs.
Next, we calculated the F-measure of each devel-
opment set’s segmentation label and identified the
20-epoch section where the averaged F-measure
was the highest. We calculated the F-measures of
test sets that averaged over 20 epochs correspond-
ing to the identified 20-epoch section. Note that
each method is based on sampling and the max-

Table 1: Parameters of experiment
English Japanese

Target text SwitchBoard CSJ
# of sentences 5,239 17,493
# of segments 88,127 132,900
# of phonemes 276,329 264,544
Vocab. size (word) 6,203 8,325
Vocab. size (phoneme) 5,422 6,589
Phoneme set 43 79

imum likelihood estimation sometimes does not
match the segmentation on the basis of linguistic
definitions.

5.2 Data
We used two types of speech transcription in En-
glish and Japanese for evaluation. This is because
the distribution of phoneme length also differs in
languages as mentioned in Sec. 3.

We used the Switchboard-1 Telephone Speech
Corpus (Godfrey et al., 1992) for the English set,
which includes the transcription of conversational
dialogue speech1. We selected 5,239 sentences
from the session “ID 20,” which included 88,127
word segments with 6,203 unique words. These
words were converted into phonemes, totaling
276,329 phoneme characters. The vocabulary size
in terms of phoneme representation was 5,422, and
this was a unique number of phoneme sequences
of words. For example, because the pronuncia-
tion of the words “see” and “sea” is the same “si:”,
the phoneme sequence “si:” is considered to be a
unique vocabulary item. The phoneme set used in
the English corpus included 43 phonemes in total
including end-of-sentence symbols. The proper-
ties of the corpus are summarized in Table 1.

We used the Corpus of Spontaneous Japanese
(CSJ) for the Japanese set, which is a collection
of spoken dialogue recordings and their transcrip-
tions (Maekawa, 2003). We used 17,493 sen-
tences, including 132,900 word segments with
8,325 of them being unique words. The phoneme
set for Japanese includes the combination of con-
sonants and vowels and almost completely corre-
sponds to “katakana” in Japanese to remove re-
dundancy. The words were also transformed into
phonemes (“katakana”), resulting in 264,544 of
them. The vocabulary size in terms of phoneme
representation was 6,589, and this was the unique
number of phoneme sequences of words. The
phoneme set used in the Japanese corpus included

1http://www.isip.msstate.edu/projects/switchboard/releases/

249



79 phonemes in total including end-of-sentence
symbols. The properties of the corpus are also
summarized in Table 1.

5.3 Parameter Settings
The parameters of NPYLM were the same for
all models. The hyper parameters of the word
language model were initialized as θ|h| = 2.0,
d|h| = 0.5, and the other parameters of prior prob-
ability distribution were all set to 1.0, such as the
parameters of the beta distribution in NPYLM.

The hyper parameters of the phoneme (char-
acter) language, part-of-speech model and length
model were the same as those in the language
model. We set the maximum length of the
phoneme sequence Lc to 10 due to the computa-
tional complexity. The number of classes of part
of speech label, M , was set to 4 due to the small
corpus size and computational cost. The initial la-
bels of parts of speech were initialized randomly
within the number of classes.

5.4 Results
The maximum F-measures of the lexicon and seg-
mentation are listed in Tables 2 and 3 for the En-
glish and Japanese test sets. The notations Lex.
and Seg. represent the F-measures of the lexi-
con and segmentation, respectively. NPYLM-D
denotes the proposed NPYLM with our phoneme-
length context model in Table 2, and PYHSMM-D
denotes the proposed PYHSMM with our context
model in Table 3.

The F-measures of the proposed NPYLM-D
outperformed the NPYLM for both the English
and Japanese test sets as shown in Table 2. The im-
provements in the Japanese corpus, 0.067 (Lex.)
and 0.045 (Seg.), were larger than those in the En-
glish corpus, 0.003 (Lex.) and 0.01 (Seg.). This
is because the bi-gram probability of phoneme
length varies more in Japanese than in English,
and the NPYLM-D could capture such tenden-
cies. The NPYLM does not use any information
other than the context of a segmented phoneme se-
quence. Therefore, the length model is useful to
model the phoneme-level features. The lower per-
formance of NPYLM-D after convergence might
be caused by the conditional probability substitu-
tion and its double-count of length information.

The F-measures of the proposed PYHSMM-D
were worse than those of PYHSMM for both the
English and Japanese test sets as shown in Ta-
ble 3. The performances of these methods were

Table 2: F-measures of segmentation by NPYLM
and NPYLM-D

NPYLM NPYLM-D
(baseline) (proposed)

English Lex. 0.602 0.605Seg. 0.897 0.907

Japanese Lex. 0.344 0.411Seg. 0.748 0.793

Table 3: F-measures of segmentation by
PYHSMM and PYHSMM-D

PYHSMM PYHSMM-D
(baseline) (proposed)

English Lex. 0.528 0.471Seg. 0.825 0.788

Japanese Lex. 0.202 0.158Seg. 0.499 0.437

worse than those of NPYLM and NPYLM-D. The
reasons for this are due to 1) the smaller amount
of data to treat latent context variable m and 2)
the overlap of contextual information between the
phoneme length and the latent variable m. Since
the latent variable m represents the class of con-
text, a sufficient amount of data will be required
for achieving stable performance compared with
NPYLM. Moreover, the context information rep-
resented by the latent variable m possibly includes
our phoneme length context. Thus, it might be
difficult for the PYHSMM-D to separate these
two contexts in the case of completely unsuper-
vised training. These problems may be solved in
the semi-supervised case where we exploit pre-
labeled data with already segmented words and
their tagged parts of speech.

The F-measures of segmentation during training
for the English and Japanese test sets are shown
in Figures 5 and 6, respectively. The horizontal
axis represents the epoch of Gibbs sampling, and
the vertical axis represents the F-measures for the
segmentation label set. The Gibbs sampling was
stopped after at least ten days. Note that the F-
measure of segmentation does not necessarily cor-
relate with the likelihood and all methods were
based on stochastic segmentation.

The F-measures of NPYLM and PYHSMM for
the English and Japanese corpora improved in pro-
portion to the number of epochs, and those of
PYHSMM and PYHSMM-D did not converge as
shown in Figures 5 and 6. Because the number of
hidden parameters of PYHSMM and PYHSMM-
D was large, their convergence speeds were slow.

250



 0.3

 0.4

 0.5

 0.6

 0.7

 0.8

 0.9

 1  10  100  1000  10000

F-
m

ea
su

re

Epoch

NPYLM (baseline)
NPYLM-D (proposed)
PYHSMM (baseline)
PYHSMM-D (proposed)

Figure 5: F-measures of segmentation for English
test set

 0.3

 0.4

 0.5

 0.6

 0.7

 0.8

 0.9

 1  10  100  1000  10000

F-
m

ea
su

re

Epoch

NPYLM (baseline)

NPYLM-D (proposed)

PYHSMM (baseline)

PYHSMM-D (proposed)

Figure 6: F-measures of segmentation for
Japanese test set

The proposed NPYLM-D had peak F-measures at
100–200 epochs, and they went down and con-
verged as the number of epochs increased. This
was mainly because the phoneme-length context
model accelerated the segmentation in the early
epochs, and the small number of observations
and un-supervised condition caused over-fitting.
Therefore, the performance of the NPYLM-D is
expected to be improved by using a larger corpus
or by optimizing hyper parameters to match the
actual prior probability of phoneme length.

The semi-supervised training would be effec-
tive for segmentation in practical use because it
matches the actual use case. Evaluating the per-
formance under such a condition is planned for
future work. We expect the PYHSMM-D to work
well more after 1000 epochs, but it requires a large
computational cost. Its results could also be im-
proved with a larger corpus and semi-supervised
condition.

6 Conclusions

Unsupervised segmentation of phoneme se-
quences is an essential process to obtain unknown
words during spoken dialogues with users. The
PYSMM is a promising model to achieve unsuper-
vised segmentation, but its performance degrades
when it is applied to phoneme-level word segmen-
tation. We proposed a phoneme-length context
model for PYSMM to give a helpful cue at the
phoneme-level and to predict succeeding phoneme
segmentation more accurately. Our experiments
showed that the peak performances with our con-
text model outperformed those without such a
context model by 0.045 at most in terms of F-
measures of estimated segmentation labels.

There are the several future works on 1) op-
timization of parameters, 2) evaluation of semi-
supervised training and other languages, and 3)
improvement of our model and inference method.
To further improve our method, we must investi-
gate the hyper parameters setting for the estima-
tion of segmentation labels to operate efficiently.
The semi-supervised training will also improve the
performance of our method. We will evaluate our
method not only for English and Japanese but also
other languages, such as African languages be-
cause the rhythmic information might be vivid.
The performance of our strict model and the op-
timal back-off structure should be investigated to
reveal the limitation of our model. The modifica-
tion of inference algorithm will be required due to
the computational efficiency when we use longer
context information more than tri-gram. Since the
rhythm information of latent segmentation might
not be captured well by bi-grams, the challenge of
using longer context is one of the important issues
for our purpose.

Acknowledgments

We thank anonymous reviewers for their insight-
ful comments. This work was supported partly by
JSPS KAKENHI Grant Number JP16H02869.

References
George E Dahl, Dong Yu, Li Deng, and Alex Acero.

2012. Context-dependent pre-trained deep neural
networks for large-vocabulary speech recognition.
IEEE Transactions on Audio, Speech, and Language
Processing, 20(1):30–42.

Micha Elsner, Sharon Goldwater, Naomi Feldman, and
Frank Wood. 2013. A joint learning model of word

251



segmentation, lexical acquisition, and phonetic vari-
ability. In Proceedings of the Conference on Em-
pirical Methods on Natural Language Processing,
pages 42–54.

John J Godfrey, Edward C Holliman, and Jane Mc-
Daniel. 1992. Switchboard: Telephone speech cor-
pus for research and development. In Proceesings
of IEEE International Conference on Acoustics,
Speech, and Signal Processing, volume 1, pages
517–520.

Sharon Goldwater, Thomas L Griffiths, and Mark John-
son. 2006. Contextual dependencies in unsuper-
vised word segmentation. In Proceedings of the 21st
International Conference on Computational Lin-
guistics and the 44th annual meeting of the Associa-
tion for Computational Linguistics, pages 673–680.

Sharon Goldwater, Thomas L Griffiths, and Mark John-
son. 2009. A bayesian framework for word segmen-
tation: Exploring the effects of context. Cognition,
112(1):21–54.

Joshua T. Goodman. 2001. A bit of progress in lan-
guage modeling. Computer Speech & Language,
15(4):403–434.

Geoffrey Hinton, Li Deng, Dong Yu, George E Dahl,
Abdel-rahman Mohamed, Navdeep Jaitly, Andrew
Senior, Vincent Vanhoucke, Patrick Nguyen, Tara N
Sainath, et al. 2012. Deep neural networks for
acoustic modeling in speech recognition: The shared
views of four research groups. Signal Processing
Magazine, 29(6):82–97.

Herman Kamper, Aren Jansen, and Sharon Goldwa-
ter. 2016. Unsupervised word segmentation and
lexicon discovery using acoustic word embeddings.
IEEE/ACM Trans. on Audio, Speech, and Language
Processing, 24(4):669–679.

Jen-Wei Kuo, Hung-Yi Lo, and Hsin-Min Wang.
2007. Improved HMM/SVM methods for automatic
phoneme segmentation. In Proceedings of Inter-
speech, pages 2057–2060.

Chia-ying Lee and James Glass. 2012. A nonpara-
metric bayesian approach to acoustic model discov-
ery. In Proceedings of the 50th Annual Meeting
of the Association for Computational Linguistics:
Long Papers-Volume 1, pages 40–49. Association
for Computational Linguistics.

Kikuo Maekawa. 2003. Corpus of spontaneous
Japanese: Its design and evaluation. In Inproceed-
ings of the ISCA & IEEE Workshop on Spontaneous
Speech Processing and Recognition.

Daichi Mochihashi, Takeshi Yamada, and Naonori
Ueda. 2009. Bayesian unsupervised word segmen-
tation with nested Pitman-Yor language modeling.
In Proceedings of the Joint Conference of the 47th
Annual Meeting of the ACL and the 4th International
Joint Conference on Natural Language Processing
of the AFNLP: Volume 1-Volume 1, pages 100–108.

Kevin P Murphy. 2002. Hidden semi-Markov models.

Graham Neubig, Masato Mimura, Shinsuke Mori, and
Tatsuya Kawahara. 2010. Learning a language
model from continuous speech. In Proceesings of
Interspeech, pages 1053–1056.

Kohei Ono, Ryu Takeda, Eric Nichols, Mikio Nakano,
and Kazunori Komatani. 2016. Toward lexical
acquisition during dialogues through implicit con-
firmation for closed-domain chatbots. In Second
Workshop on Chatbots and Conversational Agent
Technologies.

Okko Räsänen, Gabriel Doyle, and Michael C Frank.
2015. Unsupervised word discovery from speech
using automatic segmentation into syllable-like
units. In Proceedings of Interspeech, pages 3204–
3208.

Steven L Scott. 2002. Bayesian methods for hidden
markov models: Recursive computing in the 21st
century. Journal of the American Statistical Asso-
ciation, 97(457):337–351.

Frank Seide, Gang Li, Xie Chen, and Dong Yu. 2011a.
Feature engineering in context-dependent deep neu-
ral networks for conversational speech transaction.
In Proceedings of the IEEE Workshop on Automatic
Speech Recognition and Understanding, pages 24–
29.

Frank Seide, Gang Li, and Dong Yu. 2011b. Conversa-
tional speech transcription using context-dependent
deep neural network. In Proceedings of Interspeech,
pages 437–440.

Tadahiro Taniguchi, Shogo Nagasaka, and Ryo
Nakashima. 2016. Nonparametric bayesian double
articulation analyzer for direct language acquisition
from continuous speech signals. IEEE Transactions
on Cognitive and Developmental Systems, 8(3):171–
185.

Yee Whey Teh. 2006. A bayesian interpretation of in-
terpolated kneser-ney. Technical Report TRA2/06,
School of Computing, NUS.

Kei Uchiumi, Hiroshi Tsukahara, and Daichi Mochi-
hashi. 2015. Inducing word and part-of-speech with
Pitman-Yor hidden semi-Markov models. In Pro-
ceedings of the 53rd Annual Meeting of the Associ-
ation for Computational Linguistics and the 7th In-
ternational Joint Conference on Natural Language
Processing, volume 1, pages 1774–1782.

252


