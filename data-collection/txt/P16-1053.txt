



















































A Sentence Interaction Network for Modeling Dependence between Sentences


Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 558–567,
Berlin, Germany, August 7-12, 2016. c©2016 Association for Computational Linguistics

A Sentence Interaction Network for Modeling Dependence between
Sentences

Biao Liu1, Minlie Huang1∗, Song Liu2, Xuan Zhu2, Xiaoyan Zhu1
1State Key Lab. of Intelligent Technology and Systems
1National Lab. for Information Science and Technology

1Dept. of Computer Science and Technology, Tsinghua University, Beijing, China
2Samsung R&D Institute, Beijing, China

1liubiao2638@gmail.com, {aihuang, zxy-dcs}@tsinghua.edu.cn

Abstract

Modeling interactions between two sen-
tences is crucial for a number of natu-
ral language processing tasks including
Answer Selection, Dialogue Act Analy-
sis, etc. While deep learning methods
like Recurrent Neural Network or Convo-
lutional Neural Network have been proved
to be powerful for sentence modeling,
prior studies paid less attention on inter-
actions between sentences. In this work,
we propose a Sentence Interaction Net-
work (SIN) for modeling the complex in-
teractions between two sentences. By in-
troducing “interaction states” for word and
phrase pairs, SIN is powerful and flexi-
ble in capturing sentence interactions for
different tasks. We obtain significant im-
provements on Answer Selection and Dia-
logue Act Analysis without any feature en-
gineering.

1 Introduction

There exist complex interactions between sen-
tences in many natural language processing (NLP)
tasks such as Answer Selection (Yu et al., 2014;
Yin et al., 2015), Dialogue Act Analysis (Kalch-
brenner and Blunsom, 2013), etc. For instance,
given a question and two candidate answers below,
though they are all talking about cats, only the first

Q What do cats look like?
A1 Cats have large eyes and furry bodies.
A2 Cats like to play with boxes and bags.

answer correctly answers the question about cats’
appearance. It is important to appropriately model
the relation between two sentences in such cases.

∗ Correspondence author

For sentence pair modeling, some methods first
project the two sentences to fix-sized vectors sep-
arately without considering the interactions be-
tween them, and then fed the sentence vectors
to other classifiers as features for a specific task
(Kalchbrenner and Blunsom, 2013; Tai et al.,
2015). Such methods suffer from being unable to
encode context information during sentence em-
bedding.

A more reasonable way to capture sentence in-
teractions is to introduce some mechanisms to uti-
lize information from both sentences at the same
time. Some methods attempt to introduce an at-
tention matrix which contains similarity scores be-
tween words and phrases to approach sentence in-
teractions (Socher et al., 2011; Yin et al., 2015).
While the meaning of words and phrases may drift
from contexts to contexts, simple similarity scores
may be too weak to capture the complex interac-
tions, and a more powerful interaction mechanism
is needed.

In this work, we propose a Sentence Interaction
Network (SIN) focusing on modeling sentence in-
teractions. The main idea behind this model is
that each word in one sentence may potentially in-
fluence every word in another sentence in some
degree (the word “influence” here may refer to
“answer” or “match” in different tasks). So, we
introduce a mechanism that allows information
to flow from every word (or phrase) in one sen-
tence to every word (or phrase) in another sen-
tence. These “information flows” are real-valued
vectors describing how words and phrases interact
with each other, for example, a word (or phrase)
in one sentence can modify the meaning of a word
(or phrase) in another sentence through such “in-
formation flows”.

Specifically, given two sentences s1 and s2, for
every word xt in s1, we introduce a “candidate
interaction state” for every word xτ in s2. This

558



state is regarded as the “influence” of xτ to xt, and
is actually the “information flow” from xτ to xt
mentioned above. By summing over all the “can-
didate interaction states”, we generate an “interac-
tion state” for xt, which represents the influence of
the whole sentence s2 to word xt . When feeding
the “interaction state” and the word embedding to-
gether into Recurrent Neural Network (with Long
Short-Time Memory unit in our model), we ob-
tain a sentence vector with context information
encoded. We also add a convolution layer on
the word embeddings so that interactions between
phrases can also be modeled.

SIN is powerful and flexible for modeling sen-
tence interactions in different tasks. First, the “in-
teraction state” is a vector, compared with a single
similarity score, it is able to encode more informa-
tion for word or phrase interactions. Second, the
interaction mechanism in SIN can be adapted to
different functions for different tasks during train-
ing, such as “word meaning adjustment” for Di-
alogue Act Analysis or “Answering” for Answer
Selection.

Our main contributions are as follows:

• We propose a Sentence Interaction Network
(SIN) which utilizes a new mechanism to
model sentence interactions.

• We add convolution layers to SIN, which im-
proves the ability to model interactions be-
tween phrases.

• We obtain significant improvements on An-
swer Selection and Dialogue Act Analysis
without any handcrafted features.

The rest of the paper is structured as follows:
We survey related work in Section 2, introduce our
method in Section 3, present the experiments in
Section 4, and summarize our work in Section 5.

2 Related Work

Our work is mainly related to deep learning for
sentence modeling and sentence pair modeling.

For sentence modeling, we have to first repre-
sent each word as a real-valued vector (Mikolov et
al., 2010; Pennington et al., 2014) , and then com-
pose word vectors into a sentence vector. Several
methods have been proposed for sentence model-
ing. Recurrent Neural Network (RNN) (Elman,
1990; Mikolov et al., 2010) introduces a hidden
state to represent contexts, and repeatedly feed the

hidden state and word embeddings to the network
to update the context representation. RNN suf-
fers from gradient vanishing and exploding prob-
lems which limit the length of reachable context.
RNN with Long Short-Time Memory Network
unit (LSTM) (Hochreiter and Schmidhuber, 1997;
Gers, 2001) solves such problems by introducing
a “memory cell” and “gates” into the network. Re-
cursive Neural Network (Socher et al., 2013; Qian
et al., 2015) and LSTM over tree structures (Zhu et
al., 2015; Tai et al., 2015) are able to utilize some
syntactic information for sentence modeling. Kim
(2014) proposed a Convolutional Neural Network
(CNN) for sentence classification which models a
sentence in multiple granularities.

For sentence pair modeling, a simple idea is to
first project the sentences to two sentence vectors
separately with sentence modeling methods, and
then feed these two vectors into other classifiers
for classification (Tai et al., 2015; Yu et al., 2014;
Yang et al., 2015). The drawback of such meth-
ods is that separately modeling the two sentences
is unable to capture the complex sentence inter-
actions. Socher et al. (2011) model the two sen-
tences with Recursive Neural Networks (Unfold-
ing Recursive Autoencoders), and then feed sim-
ilarity scores between words and phrases (syntax
tree nodes) to a CNN with dynamic pooling to cap-
ture sentence interactions. Hu et al. (2014) first
create an “interaction space” (matching score ma-
trix) by feeding word and phrase pairs into a multi-
layer perceptron (MLP), and then apply CNN to
such a space for interaction modeling. Yin et al.
(2015) proposed an Attention based Convolutional
Neural Network (ABCNN) for sentence pair mod-
eling. ABCNN introduces an attention matrix be-
tween the convolution layers of the two sentences,
and feed the matrix back to CNN to model sen-
tence interactions. There are also some methods
that make use of rich lexical semantic features for
sentence pair modeling (Yih et al., 2013; Yang
et al., 2015), but these methods can not be easily
adapted to different tasks.

Our work is also related to context modeling.
Hermann et al. (2015) proposed a LSTM-based
method for reading comprehension. Their model
is able to effectively utilize the context (given by
a document) to answer questions. Ghosh et al.
(2016) proposed a Contextual LSTM (CLSTM)
which introduces a topic vector into LSTM for
context modeling. The topic vector in CLSTM is

559



Figure 1: RNN (a) and LSTM (b) 1

computed according to those already seen words,
and therefore reflects the underlying topic of the
current word.

3 Method

3.1 Background: RNN and LSTM

Recurrent Neural Network (RNN) (Elman, 1990;
Mikolov et al., 2010), as depicted in Figure 1(a), is
proposed for modeling long-distance dependence
in a sequence. Its hidden layer is connected to it-
self so that previous information is considered in
later times. RNN can be formalized as

ht = f(Wxxt +Whht−1 + bh)

where xt is the input at time step t and ht is the
hidden state. Though theoretically, RNN is able
to capture dependence of arbitrary length, it tends
to suffer from the gradient vanishing and explod-
ing problems which limit the length of reachable
context. In addition, an additive function of the
previous hidden layer and the current input is too
simple to describe the complex interactions within
a sequence.

RNN with Long Short-Time Memory Network
unit (LSTM, Figure 1(b)) (Hochreiter and Schmid-
huber, 1997; Gers, 2001) solves such problems by
introducing a “memory cell” and “gates” into the
network. Each time step is associated with a sub-
net known as a memory block in which a “memory
cell” stores the context information and “gates”
control which information should be added or dis-
carded or reserved. LSTM can be formalized as

ft = σ(Wf · [xt, ht−1] + bf )
it = σ(Wi · [xt, ht−1] + bi)
C̃t = tanh(WC · [xt, ht−1] + bC)

1This figure referred to http://colah.github.io/posts/2015-
08-Understanding-LSTMs/

Ct = ft ∗ Ct−1 + it ∗ C̃t
ot = σ(Wo · [xt, ht−1] + bo)
ht = ot ∗ tanh(Ct)

where ∗ means element-wise multiplication,
ft, it, ot is the forget, input and output gate that
control which information should be forgot, input
and output, respectively. C̃t is the candidate infor-
mation to be added to the memory cell state Ct. ht
is the hidden state which is regarded as a represen-
tation of the current time step with contexts.

In this work, we use LSTM with peephole con-
nections, namely adding Ct−1 to compute the for-
get gate ft and the input gate it, and adding Ct to
compute the output gate ot.

3.2 Sentence Interaction Network (SIN)
Sentence Interaction Network (SIN, Figure 2)
models the interactions between two sentences in
two steps.

First, we use a LSTM (referred to as LSTM1)
to model the two sentences s1 and s2 separately,
and the hidden states related to the t-th word in s1
and the τ -th word in s2 are denoted as z

(1)
t and

z
(2)
τ respectively. For simplicity, we will use the

position (t, τ ) to denote the corresponding words
hereafter.

Second, we propose a new mechanism to model
the interactions between s1 and s2 by allowing
information to flow between them. Specifically,
word t in s1 may be potentially influenced by all
words in s2 in some degree. Thus, for word t in
s1, a candidate interaction state c̃

(i)
tτ and an input

gate i(i)tτ are introduced for each word τ in s2 as
follows:

c̃
(i)
tτ = tanh(W

(i)
c · [z(1)t , z(2)τ ] + b(i)c )

i
(i)
tτ = σ(W

(i)
i · [z(1)t , z(2)τ ] + b(i)i )

here, the superscript “i” indicates “interaction”.
W

(i)
c ,W

(i)
i , b

(i)
c , b

(i)
i are model parameters. The

interaction state c(i)t for word t in s1 can then be
formalized as

c
(i)
t =

|s2|∑
τ=1

c̃
(i)
tτ ∗ i(i)tτ

where |s2| is the length of sentence s2, and c(i)t
can be viewed as the total interaction information
received by word t in s1 from sentence s2. The
interaction states of words in s2 can be similarly

560



Figure 2: SIN for modeling sentence s1 at timestep t. First, we model s1 and s2 separately with LSTM1
and obtain the hidden states z(1)t for s1 and z

(2)
τ for s2. Second, we compute interaction states based on

these hidden states, and incorporate c(i)t into LSTM2. Information flows (interaction states) from s1 to
s2 are not depicted here for simplicity.

computed by exchanging the position of z(1)t and
z
(2)
τ in c̃

(i)
tτ and i

(i)
tτ while sharing the model param-

eters.
We now introduce the interaction states into

another LSTM (referred to as LSTM2) to com-
pute the sentence vectors. Therefore, information
can flow between the two sentences through these
states. For sentence s1, at timestep t, we have

ft = σ(Wf · [xt, ht−1, c(i)t , Ct−1] + bf )
it = σ(Wi · [xt, ht−1, c(i)t , Ct−1] + bi)
C̃t = tanh(WC · [xt, ht−1, c(i)t ] + bC)
Ct = ft ∗ Ct−1 + it ∗ C̃t
ot = σ(Wo · [xt, ht−1, c(i)t , Ct] + bo)
ht = ot ∗ tanh(Ct)

By averaging all hidden states of LSTM2, we ob-
tain the sentence vector vs1 of s1, and the sentence
vector vs2 of s2 can be computed similarly. vs1
and vs2 can then be used as features for different
tasks.

In SIN, the candidate interaction state c̃(i)tτ rep-
resents the potential influence of word τ in s2 to
word t in s1, and the related input gate i

(i)
tτ con-

trols the degree of the influence. The element-wise
multiplication c̃(i)tτ ∗i(i)tτ is then the actual influence.
By summing over all words in s2, the interaction

state c(i)t gives the influence of the whole sentence
s2 to word t.

3.3 SIN with Convolution (SIN-CONV)

SIN is good at capturing the complex interactions
of words in two sentences, but not strong enough
for phrase interactions. Since convolutional neural
network is widely and successfully used for mod-
eling phrases, we add a convolution layer before
SIN to model phrase interactions between two sen-
tences.

Let v1, v2, ..., v|s| be the word embeddings of a
sentence s, and let ci ∈ Rwd, 1 ≤ i ≤ |s| − w +
1, be the concatenation of vi:i+w−1, where w is
the window size. The representation pi for phrase
vi:i+w−1 is computed as:

pi = tanh(F · ci + b)

where F ∈ Rd×wd is the convolution filter, and d
is the dimension of the word embeddings.

In SIN-CONV, we first use a convolution layer
to obtain phrase representations for the two sen-
tences s1 and s2, and the SIN interaction proce-
dure is then applied to these phrase representations
as before to model phrase interactions. The aver-
age of all hidden states are treated as sentence vec-
tors vcnns1 and v

cnn
s2 . Thus, SIN-CONV is SIN with

word vectors substituted by phrase vectors. The

561



two phrase-based sentence vectors are then fed to a
classifier along with the two word-based sentence
vectors together for classification.

The LSTM and interaction parameters are not
shared between SIN and SIN-CONV.

4 Experiments

In this section, we test our model on two tasks:
Answer Selection and Dialogue Act Analysis.
Both tasks require to model interactions between
sentences. We also conduct auxiliary experiments
for analyzing the interaction mechanism in our
SIN model.

4.1 Answer Selection
Selecting correct answers from a set of candidates
for a given question is quite crucial for a number
of NLP tasks including question-answering, natu-
ral language generation, information retrieval, etc.
The key challenge for answer selection is to appro-
priately model the complex interactions between
the question and the answer, and hence our SIN
model is suitable for this task.

We treat Answer Selection as a classification
task, namely to classify each question-answer pair
as “correct” or “incorrect”. Given a question-
answer pair (q, a), after generating the question
and answer vectors vq and va using SIN, we feed
them to a logistic regression layer to output a prob-
ability. And we maximize the following objective
function:

pθ(q, a) = σ(W · [vq, va]) + b)
L =

∑
(q,a)

ŷq,a log pθ(q, a)+

(1− ŷq,a) log(1− pθ(q, a))

where ŷq,a is the true label for the question-answer
pair (q, a) (1 for correct, 0 for incorrect). For SIN-
CONV, the sentence vector vcnnq and v

cnn
a are also

fed to the logistic regression layer.
During evaluation, we rank the answers of a

question q according to the probability pθ(q, a).
The evaluation metrics are mean average precision
(MAP) and mean reciprocal rank (MRR).

4.1.1 Dataset
The WikiQA2(Yang et al., 2015) dataset is used
for this task. Following Yin et al. (2015), we
filtered out those questions that do not have any

2http://aka.ms/WikiQA

Q QA pair A/Q correct A/Q
Train 2,118 20,360 9.61 0.49
Dev 126 1,130 8.97 1.11
Test 243 2,351 9.67 1.21

Table 1: Statistics of WikiQA (Q=Question,
A=Answer)

correct answers from the development and test set.
Some statistics are shown in Table 1.

4.1.2 Setup
We use the 100-dimensional GloVe vectors3 (Pen-
nington et al., 2014) to initialize our word embed-
dings, and those words that do not appear in Glove
vectors are treated as unknown. The dimension of
all hidden states is set to 100 as well. The window
size of the convolution layer is 2. To avoid overfit-
ting, dropout is introduced to the sentence vectors,
namely setting some dimensions of the sentence
vectors to 0 with a probability p (0.5 in our experi-
ment) randomly. No handcrafted features are used
in our methods and the baselines.

Mini-batch Gradient Descent (30 question-
answer pairs for each mini batch), with AdaDelta
tuning learning rate, is used for model training.
We update model parameters after every mini
batch, check validation MAP and save model af-
ter every 10 batches. We run 10 epochs in to-
tal, and the model with highest validation MAP
is treated as the optimal model, and we report the
corresponding test MAP and MRR metrics.

4.1.3 Baselines
We compare our SIN and SIN-CONV model with
5 baselines listed below:

• LCLR: The model utilizes rich semantic and
lexical features (Yih et al., 2013).

• PV: The cosine similarity score of paragraph
vectors of the two sentences is used to rank
answers (Le and Mikolov, 2014).

• CNN: Bigram CNN (Yu et al., 2014).
• ABCNN: Attention based CNN, no hand-

crafted features are used here (Yin et al.,
2015).

• LSTM: The question and answer are modeled
by a simple LSTM. Different from SIN, there
is no interaction between sentences.

3http://nlp.stanford.edu/projects/glove/

562



4.1.4 Results
Results are shown in Table 2. SIN performs much
better than LSTM, PV and CNN, this justifies that
the proposed interaction mechanism well captures
the complex interactions between the question and
the answer. But SIN performs slightly worse than
ABCNN because it is not strong enough at model-
ing phrases. By introducing a simple convolution
layer to improve its phrase-modeling ability, SIN-
CONV outperforms all the other models.

For SIN-CONV, we do not observe much im-
provements by using larger convolution filters
(window size ≥ 3) or stacking more convolution
layers. The reason may be the fact that interactions
between long phrases is relatively rare, and in ad-
dition, the QA pairs in the WikiQA dataset may
be insufficient for training such a complex model
with long convolution windows.

4.2 Dialogue Act Analysis
Dialogue acts (DA), such as Statement, Yes-No-
Question, Agreement, indicate the sentence prag-
matic role as well as the intention of the speakers
(Williams, 2012). They are widely used in natu-
ral language generation (Wen et al., 2015), speech
and meeting summarization (Murray et al., 2006;
Murray et al., 2010), etc. In a dialogue, the DA
of a sentence is highly relevant to the content of
itself and the previous sentences. As a result, to
model the interactions and long-range dependence
between sentences in a dialogue is crucial for dia-
logue act analysis.

Given a dialogue (n sentences) d =
[s1, s2, ..., sn], we first use a LSTM (LSTM1)
to model all the sentences independently. The
hidden states of sentence si obtained at this step
are used to compute the interaction states of
sentence si+1, and SIN will generate a sentence
vector vsi using another LSTM (LSTM2) for each
sentence si in the dialogue (see Section 3.2) .
These sentence vectors can be used as features
for dialogue act analysis. We refer to this method
as SIN (or SIN-CONV for adding a convolution
layer).

For dialogue act analysis, we add a softmax
layer on the sentence vector vsi to predict the prob-
ability distribution:

pθ(yj |vsi) =
exp(vTsi · wj + bj)∑
k exp(vTsi · wk + bk)

4With extra handcrafted features, ABCNN’s performance
is: MAP(0.692), MRR(0.711).

Model MAP MRR
LCLR 0.599 0.609

PV 0.511 0.516
CNN 0.619 0.628

ABCNN 0.660 0.677
LSTM 0.634 0.648

SIN 0.657 0.672
SIN-CONV 0.674 0.693

Table 2: Results on answer selection4.

Figure 3: SIN-LD for dialogue act analysis.
LSTM1 is not shown here for simplicity. x

(sj)
t

means word t in sj , c
(i,sj)
t means the interaction

state for word t in sj .

where yj is the j-th DA tag,wj and bj is the weight
vector and bias corresponding to yj . We maximize
the following objective function:

L =
∑
d∈D

|d|∑
i=1

log pθ(ŷsi |vsi)

where D is the training set, namely a set of dia-
logues, |d| is the length of the dialogue, si is the
i-th sentence in d, ŷsi is the true dialogue act label
of si.

In order to capture long-range dependence in
the dialogue, we can further join up the sentence
vector vsi with another LSTM (LSTM3). The
hidden state hsi of LSTM3 are treated as the fi-
nal sentence vector, and the probability distri-
bution is given by substituting vsi with hsi in
pθ(yj |vsi). We refer to this method as SIN-LD (or
SIN-CONV-LD for adding a convolution layer),
where LD means long-range dependence. Figure
3 shows the whole structure (LSTM1 is not shown
here for simplicity).

563



Dialogue Act Example Train(%) Test(%)
Statement-non-Opinion Me, I’m in the legal department. 37.0 31.5
Backchannel/Acknowledge Uh-huh. 18.8 18.3
Statement-Opinion I think it’s great 12.8 17.2
Abandoned/Uninterpretable So,- 7.6 8.6
Agreement/Accept That’s exactly it. 5.5 5.0
Appreciation I can imagine. 2.4 1.8
Yes-No-Question Do you have to have any special training? 2.3 2.0
Non-Verbal [Laughter], [Throat-clearing] 1.8 2.3
Yes-Answers Yes. 1.5 1.7
Conventional-closing Well, it’s been nice talking to you. 1.3 1.9
Other Labels(32) 9.1 9.8
Total number of sentences 196258 4186
Total number of dialogues 1115 19

Table 3: Dialogue act labels

4.2.1 Dataset
We use the Switch-board Dialogue Act (SwDA)
corpus (Calhoun et al., 2010) in our experiments5.
SwDA contains the transcripts of several people
discussing a given topic on the telephone. There
are 42 dialogue act tags in SwDA,6 and we list the
10 most frequent tags in Table 3.

The same data split as in Stolcke et al. (2000)
is used in our experiments. There are 1,115 dia-
logues in the training set and 19 dialogues in the
test set7. We also randomly split the original train-
ing set as a new training set (1,085 dialogues) and
a validation set (30 dialogues).

4.2.2 Setup
The setup is the same as that in Answer Selection
except: (1) Only the most common 10,000 words
are used, other words are all treated as unknown.
(2) Each mini batch contains all sentences from
3 dialogues for Mini-batch Gradient Descent. (3)
The evaluation metric is accuracy. (4) We run 30
epochs in total. (5) We use the last hidden state of
LSTM2 as sentence representation since the sen-
tences here are much shorter compared with those
in Answer Selection.

4.2.3 Baselines
We compare with the following baselines:

• unigram, bigram, trigram LM-HMM: HMM
variants (Stolcke et al., 2000).

5http://compprag.christopherpotts.net /swda.html.
6SwDA actually contains 43 tags in which “+” should not

be treated as a valid tag since it means continuation of the
previous sentence.

7http://web.stanford.edu/%7ejurafsky/ws97/

Model Accuracy(%)
unigram LM-HMM 68.2
bigram LM-HMM 70.6
trigram LM-HMM 71.0

RCNN 73.9
LSTM 72.8

SIN 74.8
SIN-CONV 75.1

SIN-LD 76.0
SIN-CONV-LD 76.5

Table 4: Accuracy on dialogue act analysis. Inter-
annotator agreement is 84%.

• RCNN: Recurrent Convolutional Neural Net-
works (Kalchbrenner and Blunsom, 2013).
Sentences are first separately embedded with
CNN, and then joined up with RNN.

• LSTM: All sentences are modeled separately
by one LSTM. Different from SIN, there is
no sentence interactions in this method.

4.2.4 Results
Results are shown in Table 4. HMM variants,
RCNN and LSTM model the sentences separately
during sentence embedding, and are unable to cap-
ture the sentence interactions. With our inter-
action mechanism, SIN outperforms LSTM, and
proves that well modeling the interactions be-
tween sentences in a dialogue is important for di-
alogue act analysis. After introducing a convo-
lution layer, SIN-CONV performs slightly better
than SIN. SIN-LD and SIN-CONV-LD model the

564



Figure 4: L2-norm of the interaction states from question to answer (linearly mapped to [0, 1]).

Q: what creates a cloud
A: in meteorology , a cloud is a visible mass

of liquid droplets or frozen crystals made
of water or various chemicals suspended
in the atmosphere above the surface of a
planetary body.

Table 5: A question-answer pair example.

long-range dependence in the dialogue with an-
other LSTM, and obtain further improvements.

4.3 Interaction Mechanism Analysis
We investigate into the interaction states of SIN
for Answer Selection to see how our proposed in-
teraction mechanism works.

Given a question-answer pair in Table 5, for
SIN, there is a candidate interaction state c̃(i)τt and
an input gate i(i)τt from each word t in the ques-
tion to each word τ in the answer. We investigate
into the L2-norm ||c̃(i)τt ∗ i(i)τt ||2 to see how words
in the two sentences interact with each other. Note
that we have linearly mapped the originalL2-norm
value to [0, 1] as follows:

f(x) =
x− xmin

xmax − xmin
As depicted in Figure 4, we can see that the

word “what” in the question has little impact to
the answer through interactions. This is reason-
able since “what” appears frequently in questions,
and does not carry much information for answer
selection8. On the contrary, the phrase “creates
a cloud”, especially the word “cloud”, transmits
much information through interactions to the an-
swer, this conforms with human knowledge since

8Our statements focus on the interaction, in a sense of
“answering” or “matching”. Definitely, such words like
“what” and “why” are very important for answering ques-
tions from the general QA perspective since they determine
the type of answers.

we rely on these words to answer the question as
well.

In the answer, interactions concentrate on the
phrase “a cloud is a visible mass of liquid
droplets” which seems to be a good and com-
plete answer to the question. Although there are
also other highly related words in the answer, they
are almost ignored. The reason may be failing to
model such a complex phrase (three relatively sim-
ple sentences joined by “or”) or the existence of
the previous phrase which is already a good an-
swer.

This experiment clearly shows how the interac-
tion mechanism works in SIN. Through interac-
tion states, SIN is able to figure out what the ques-
tion is asking about, namely to detect those highly
informative words in the question, and which part
in the answer can answer the question.

5 Conclusion and Future Work

In this work, we propose Sentence Interaction Net-
work (SIN) which utilizes a new mechanism for
modeling interactions between two sentences. We
also introduce a convolution layer into SIN (SIN-
CONV) to improve its phrase modeling ability so
that phrase interactions can be handled. SIN is
powerful and flexible to model sentence interac-
tions for different tasks. Experiments show that
the proposed interaction mechanism is effective,
and we obtain significant improvements on An-
swer Selection and Dialogue Act Analysis without
any handcrafted features.

Previous works have showed that it is important
to utilize the syntactic structures for modeling sen-
tences. We also find out that LSTM is sometimes
unable to model complex phrases. So, we are go-
ing to extend SIN to tree-based SIN for sentence
modeling as future work. Moreover, applying the
models to other tasks, such as semantic relatedness
measurement and paraphrase identification, would

565



also be interesting attempts.

6 Acknowledgments

This work was partly supported by the National
Basic Research Program (973 Program) under
grant No. 2012CB316301/2013CB329403, the
National Science Foundation of China under grant
No. 61272227/61332007, and the Beijing Higher
Education Young Elite Teacher Project. The work
was also supported by Tsinghua University – Bei-
jing Samsung Telecom R&D Center Joint Labora-
tory for Intelligent Media Computing.

References
Sasha Calhoun, Jean Carletta, Jason M Brenier, Neil

Mayo, Dan Jurafsky, Mark Steedman, and David
Beaver. 2010. The nxt-format switchboard corpus:
a rich resource for investigating the syntax, seman-
tics, pragmatics and prosody of dialogue. Language
resources and evaluation, 44(4):387–419.

Jeffrey L Elman. 1990. Finding structure in time.
Cognitive science, 14(2):179–211.

Felix Gers. 2001. Long short-term memory in re-
current neural networks. Unpublished PhD disser-
tation, École Polytechnique Fédérale de Lausanne,
Lausanne, Switzerland.

Shalini Ghosh, Oriol Vinyals, Brian Strope, Scott Roy,
Tom Dean, and Larry Heck. 2016. Contextual
lstm (clstm) models for large scale nlp tasks. arXiv
preprint arXiv:1602.06291.

Karl Moritz Hermann, Tomas Kocisky, Edward
Grefenstette, Lasse Espeholt, Will Kay, Mustafa Su-
leyman, and Phil Blunsom. 2015. Teaching ma-
chines to read and comprehend. In Advances in Neu-
ral Information Processing Systems, pages 1684–
1692.

Sepp Hochreiter and Jürgen Schmidhuber. 1997.
Long short-term memory. Neural computation,
9(8):1735–1780.

Baotian Hu, Zhengdong Lu, Hang Li, and Qingcai
Chen. 2014. Convolutional neural network archi-
tectures for matching natural language sentences.
In Advances in Neural Information Processing Sys-
tems, pages 2042–2050.

Nal Kalchbrenner and Phil Blunsom. 2013. Recurrent
convolutional neural networks for discourse compo-
sitionality. arXiv preprint arXiv:1306.3584.

Yoon Kim. 2014. Convolutional neural net-
works for sentence classification. arXiv preprint
arXiv:1408.5882.

Quoc V Le and Tomas Mikolov. 2014. Distributed
representations of sentences and documents. arXiv
preprint arXiv:1405.4053.

Tomas Mikolov, Martin Karafiát, Lukas Burget, Jan
Cernockỳ, and Sanjeev Khudanpur. 2010. Recur-
rent neural network based language model. In IN-
TERSPEECH 2010, 11th Annual Conference of the
International Speech Communication Association,
Makuhari, Chiba, Japan, September 26-30, 2010,
pages 1045–1048.

Gabriel Murray, Steve Renals, Jean Carletta, and Jo-
hanna Moore. 2006. Incorporating speaker and
discourse features into speech summarization. In
Proceedings of the main conference on Human Lan-
guage Technology Conference of the North Amer-
ican Chapter of the Association of Computational
Linguistics, pages 367–374. Association for Com-
putational Linguistics.

Gabriel Murray, Giuseppe Carenini, and Raymond Ng.
2010. Generating and validating abstracts of meet-
ing conversations: a user study. In Proceedings of
the 6th International Natural Language Generation
Conference, pages 105–113. Association for Com-
putational Linguistics.

Jeffrey Pennington, Richard Socher, and Christo-
pher D. Manning. 2014. Glove: Global vectors for
word representation. In Empirical Methods in Nat-
ural Language Processing (EMNLP), pages 1532–
1543.

Qiao Qian, Bo Tian, Minlie Huang, Yang Liu, Xuan
Zhu, and Xiaoyan Zhu. 2015. Learning tag em-
beddings and tag-specific composition functions in
recursive neural network. In Proceedings of the
53rd Annual Meeting of the Association for Compu-
tational Linguistics and the 7th International Joint
Conference on Natural Language Processing, vol-
ume 1, pages 1365–1374.

Richard Socher, Eric H Huang, Jeffrey Pennin, Christo-
pher D Manning, and Andrew Y Ng. 2011. Dy-
namic pooling and unfolding recursive autoencoders
for paraphrase detection. In Advances in Neural In-
formation Processing Systems, pages 801–809.

Richard Socher, Alex Perelygin, Jean Y Wu, Jason
Chuang, Christopher D Manning, Andrew Y Ng,
and Christopher Potts. 2013. Recursive deep mod-
els for semantic compositionality over a sentiment
treebank. In Proceedings of the conference on
empirical methods in natural language processing
(EMNLP), volume 1631, page 1642. Citeseer.

Andreas Stolcke, Klaus Ries, Noah Coccaro, Eliza-
beth Shriberg, Rebecca Bates, Daniel Jurafsky, Paul
Taylor, Rachel Martin, Carol Van Ess-Dykema, and
Marie Meteer. 2000. Dialogue act modeling for
automatic tagging and recognition of conversational
speech. Computational linguistics, 26(3):339–373.

566



Kai Sheng Tai, Richard Socher, and Christopher D
Manning. 2015. Improved semantic representa-
tions from tree-structured long short-term memory
networks. arXiv preprint arXiv:1503.00075.

Tsung-Hsien Wen, Milica Gasic, Nikola Mrksic, Pei-
Hao Su, David Vandyke, and Steve Young. 2015.
Semantically conditioned lstm-based natural lan-
guage generation for spoken dialogue systems.
arXiv preprint arXiv:1508.01745.

Jason D Williams. 2012. A belief tracking challenge
task for spoken dialog systems. In NAACL-HLT
Workshop on Future Directions and Needs in the
Spoken Dialog Community: Tools and Data, pages
23–24. Association for Computational Linguistics.

Yi Yang, Wen-tau Yih, and Christopher Meek. 2015.
Wikiqa: A challenge dataset for open-domain ques-
tion answering. In Proceedings of the Conference on
Empirical Methods in Natural Language Process-
ing. Citeseer.

Wen-tau Yih, Ming-Wei Chang, Christopher Meek, and
Andrzej Pastusiak. 2013. Question answering using
enhanced lexical semantic models.

Wenpeng Yin, Hinrich Schütze, Bing Xiang, and
Bowen Zhou. 2015. Abcnn: Attention-based con-
volutional neural network for modeling sentence
pairs. arXiv preprint arXiv:1512.05193.

Lei Yu, Karl Moritz Hermann, Phil Blunsom, and
Stephen Pulman. 2014. Deep learning for answer
sentence selection. arXiv preprint arXiv:1412.1632.

Xiaodan Zhu, Parinaz Sobhani, and Hongyu Guo.
2015. Long short-term memory over tree structures.
arXiv preprint arXiv:1503.04881.

567


