




































Doc2hash: Learning Discrete Latent variables for Documents Retrieval


Proceedings of NAACL-HLT 2019, pages 2235–2240
Minneapolis, Minnesota, June 2 - June 7, 2019. c©2019 Association for Computational Linguistics

2235

Doc2hash: Learning Discrete Latent Variables for Document Retrieval

Yifei Zhang
Vector Lab
JD Finance

zhangyifei11@jd.com

Hao Zhu
Vector Lab
JD Finance

zhuhao6@jd.com

Abstract
Learning to hash via generative models has be-
come a powerful paradigm for fast similarity
search in documents retrieval. To get binary
representation (i.e., hash codes), the discrete
distribution prior (i.e., Bernoulli Distribution)
is applied to train the variational autoencoder
(VAE). However, the discrete stochastic layer
is usually incompatible with the backpropaga-
tion in the training stage and thus causes a gra-
dient flow problem. In this paper, we propose
a method, Doc2hash, that solves the gradient
flow problem of the discrete stochastic layer
by using continuous relaxation on priors, and
trains the generative model in an end-to-end
manner to generate hash codes. In qualita-
tive and quantitative experiments, we show the
proposed model outperforms other state of the
art methods.

1 Introduction

A popular theme for deep learning is that of rep-
resentation learning, whose goal is to use exist-
ing data to learn a compact and meaningful repre-
sentation when building classifiers or other predic-
tors. Learning continuous representation has been
achieving a great success in various NLP tasks, in-
cluding text classification, word and sentence rep-
resentation (Mikolov et al., 2013; Le and Mikolov,
2014), yet learning discrete representation is po-
tentially more suitable for tasks we are interested
in such as learning to hash.

Hashing serves as a fast solution for similarity
search also called approximate nearest neighbor
search. In document retrieval, semantic hashing
is the strategy that turn the document into binary
codes (hash codes) which capture semantic infor-
mation. One can use a query (a document) to re-
trieve similar documents by calculating the ham-
ming distances between their hash codes. Given
the fast calculation process and the efficient stor-
age property (one modern PC can execute millions

Figure 1: The end-to-end semantic hashing frame-
work: Training the cycle x −→ z −→ x to ob-
tain the latent variables z. The one hot representa-
tion of discrete latent variables z has been relaxed to
{(0.05, 0.95), (0.92, 0.08), (0.99, 0.01)} by the tempt-
ing softmax. Hash codes zh = {1, 0, 0} is obtained via
taking the argmax of z. Y is the label/tag used in the
supervised hashing

of hamming distance computations in just a few
milliseconds). This semantic hashing strategy is
very attractive.

Inspired by the recent success of modeling
latent variables via generative models in solv-
ing various NLP problems (van den Oord et al.,
2017; Semeniuta et al., 2017; Bowman et al.,
2015), Some approaches obtain binary codes of
documents from a generative perspective via pa-
rameterizing models using neural networks and
stochastic optimization using gradient-based tech-
niques: (Chaidaroon and Fang, 2017) designed
a two-stage training procedures to generate hash
codes with variational autoencoder: (i) it first
infers continuous representations of text through
VAE with isotropic Gaussian distribution prior (ii)
Obtain hash codes via binarizing the continuous
representation of texts. Since the model parame-
ters are not learned in an end-to-end manner, the
two-stage training strategy may result in a subop-
timal local optima. (Shen et al., 2018) replaced
the Gaussian distribution prior with Bernoulli dis-
tribution prior so that the stochastic layer of the



2236

VAE can directly produce binary codes in the la-
tent space and train hash codes in an end-to-end
manner. Unfortunately, the Bernoulli stochastic
layer is non-differentiable. Although the Straight-
Through (ST) estimator is adapted to propa-
gate gradients where it skips the gradient of the
stochastic layer during backpropagation (Bengio
et al., 2013). The ST estimator is still a biased esti-
mator which introduces high-variance biased gra-
dients of objectives during the training.

In this paper, we propose a generative model
with a categorical distribution prior for semantic
hashing which learns binary codes of documents
in an end-to-end manner. To train the genera-
tive model, instead of using the ST estimator, we
use the Gumbel-Softmax trick to overcome the in-
ability by applying the re-parameterization trick
to discrete latent variables. There are two advan-
tages: 1) a nice parameterization for a discrete
(or categorical) distribution is given in terms of
the Gumbel distribution (the Gumbel trick); and
2) although the corresponding function is non-
continuous, it can be made continuous by ap-
plying using a continuous approximation that de-
pends on a temperature parameter. Therefore,
it produces low-variance biased gradients of the
stochastic layer in the backpropagation. Our ex-
periment shows our model achieves the state of the
art performance on three standard datasets for se-
mantic hashing.

2 Methodology

2.1 Discrete NVI Framework For Semantic
Hashing

In this section, we introduce the neural variational
inference (NVI) framework (Shen et al., 2018)
with discrete latent variables for semantic hashing
in detail. First, a generative model whose encod-
ing distribution and decoding distribution are de-
fined as p(z|x), p(x|z). We approximate these two
distributions by defining approximations qφ(z|x),
pθ(x|z) and model them as the inference (encod-
ing) network gφ(x) and the generative network
(decoding) gθ(z). x ∈ RV is the bag of word
representation of a document where V is vocab-
ulary size. Specially, the weight schema of bag
of words can be binary, TF, TFIDF. The infer-
ence network gφ(x) produces latent variables z
from x approximating the true posterior distribu-
tion p(z|x) as qφ(z|x). Then, the generative net-
work gθ(z) maps latent variable z back to x by

approximating p(z|x) as pθ(x|z). We model the
conditional probability over the word wi as soft-
max:

p(xi = wi|z) =
exp(wigθ(z))∑
V exp(wigθ(z))

(1)

where wi ∈ {0, 1}V is represented as an one-hot
vector indicating the i-th word in the total vocab-
ulary and the likelihood pθ(x|z) =

∏
i p(xi =

wi|z). Training the generative model that synthe-
sizes the observed data x, we can obtain the mean-
ingful latent variables z.

To learn discrete latent variables under the NVI
framework for semantic hashing, we cast z to be
the discrete random variables and assume a multi-
variate categorical prior on z: p(z) ∼ CatCatCat(π) =∏l
i I(z = k)πik, where πik is the k-th class prob-

ability on i-th component of parameters π. In this
way, the posterior distribution approximated by
the encoding network is constrained in the form of
qφ(z|x) = CatCatCat(h), where h = gφ(x) is the output
of the encoding network.

2.2 Variational Low Bound of Discrete
Latent Variable Training

To train the parameters θ, φ of the encoding net-
work qθ(z|x) and the decoding network pφ(x|z),
we maximize the variational low bound as same
as in the VAE framework (Kingma and Welling,
2013):

LELBO = Ez∼qφ(z|x)[log
qθ(x|z)p(z)
qφ(z|x)

] (2)

= Ez∼qφ(z|x)[log qθ(x|z)]−KL(qφ(z|x)||p(z))
(3)

the first term is the reconstruction loss, which en-
courages the decoder network to learn the map-
ping from latent variables z to the original doc-
ument x. The second term is Kullback-Leibler
(KL) divergence KL(qφ(z|x)||p(z))], which en-
courages the posterior distribution qφ(z|x) to be
close to the multivariate categorical prior p(z). we
can write the KL term in the following form:

KL(qφ(z|x)||p(z)) =
∑
i

∑
k

gikφ (x) log
gikφ (x)

πik
(4)

2.3 Gradients of Discrete Stochastic Layer
To evaluate Ez∼qφ(z|x), the reparameterization
trick is often employed to make the sampling pro-



2237

cess z ∼ qφ(z|x) compatible with backpropaga-
tion (Kingma and Welling, 2013). But sampling
from a discrete distribution using reparameteri-
zation tricks still has problem during backprop-
agation. Due to the sampling from the stochas-
tic layer usually incorporate hard threshold oper-
ators such as sign(.)sign(.)sign(.) and argmax(.)argmax(.)argmax(.) (Jang et al.,
2016; Maddison et al., 2016), the stochastic layer
becomes non-differentiable. In general, the so-
lution to this issue is using the Straight-Through
(ST) estimator where a biased path derivative es-
timator can be utilized even when z is not repa-
rameterizable as shown in Figure 2. For exam-
ple, NASH (Shen et al., 2018) passes the gradients
through the Bernoulli stochastic layer using ST es-
timator. Specifically, they do:

dL

dφ
=
dL

dz

dz

dgφ(x)

dgφ(x)

dφ
≈ dL
dz

dgφ(x)

dφ
(5)

where the ST estimator lets the gradients of the
stochastic layer dzdgφ(x) ≈ 1. However, the ST
estimator is backpropagating with respect to the
sample-independent. It may cause discrepancies
between the forward and backward pass, and thus
lead to higher variance (Jang et al., 2016).

2.4 Continuous Relaxation of Discrete
Stochastic Layer

As mention above, we want to learn discrete La-
tent variables (hash codes) by maximizing the
variational low bound assuming a multivariate cat-
egorical prior. Supposed the output of the encod-
ing network, h = gφ(x), h ∈ Rl×2, represents the
parameters of the multivariate categorical poste-
rior q(z|x). We can use reparameterization trick
to draw samples of z from the categorical pos-
terior using Gumbel distribution (Gumbel, 1954;
Jang et al., 2016):

zi = argmax
k

(Gk + logh
k
i ), k ∈ {0, 1} (6)

G1, G2 is drawn i.i.d. from Gumbel (0, 1) =
log(− log(U(0, 1)) where U is the uniform distri-
bution. We further represent zi as one-hot vector
with 2 dimensions:

zi = OneHot[argmax
k

(Gk + logh
k
i )], k ∈ {0, 1}

(7)
discrete latent variables z can be seen as concate-
nation of l one-hot vectors.

In the forward pass, we sample z from the
stochastic discrete layer via Eq. 7. However, in

Figure 2: Gradients estimation in stochastic computa-
tion graphs (1) Gumbel-Softmax trick (2) The Straight-
Through estimator, used for Bernoulli discrete vari-
ables, skips stochastic node by approximating dzdgφ ≈ 1

backpropagation, we cannot pass gradient through
the hard threshold operator, argmax, since it is
non-differentiable. The derivative of argmax is 0
everywhere except the boundary of state change,
where it is undefined. To end this, instead of em-
ploying ST estimator to skip gradients, we use the
tempting softmax as a continuous relaxation of the
argmax computation:

zi =
exp((Gk + log h

k
i )/γ)∑

k exp((Gk + log h
k
i )/γ)

, k ∈ {0, 1} (8)

This relaxation approximates the exactly discrete
argmax computation as temperature parameter
γ −→ 0 yet keeps the relative order of (Gk +
log hki ). similar relaxation techniques have been
introduced as the Gumbel-Softmax trick in (Jang
et al., 2016; Maddison et al., 2016). Since Eq. 8 is
differentiable, we can train the generative model in
an end-to-end way via stochastic gradient descent
to get the discrete latent variables (hash codes).
The whole framework is summarized in the Fig-
ure 1 and the difference against the ST estimator
is exemplified in the Figure 2.

2.5 Supervised Hashing

We extend our method to the supervised setting,
where the mapping from latent variables z to the
label y is learned, here parameterized by a two-
layer multilayer perceptron (MLP) followed by a
fully-connected softmax layer. To balance maxi-
mizing the variational lower bound and minimiz-
ing the discriminative loss, the following joint
training objective is employed:

L = −LELBO(θ, φ,xxx) + αLdis(η,zzz, y) (9)

where η refers to parameters of the MLP classi-
fier and α controls the relative weight between the



2238

variational lower bound (LELBO) and the discrim-
inative loss (Ldis). We assume a general multi-
label classification setting where each document
could have multiple labels/tags. P (yj |f(zzz; η)) can
be modeled by the logistic function defined as:

P (yj |f(zzz, η)) =
1

1 + exp(−yTj ηjzzz)
(10)

3 Experiment

3.1 Baseline and Setting

Unsupervised Hashing
Model 8bits 16bits 32bits 64bits 128bits
LSH 0.4388 0.4393 0.4514 0.4553 0.4773
S-RBM 0.4846 0.5108 0.5166 0.5190 0.5137
SpH 0.5807 0.6055 0.6281 0.6143 0.5891
STH 0.3723 0.3947 0.4105 0.4181 0.4123
VDSH 0.4330 0.6853 0.7108 0.4410 0.5847
NASH-DN 0.6358 0.6956 0.7327 0.7010 0.6325
Doc2hash 0.6965 0.7224 0.7473 0.7532 0.7595

Supervised Hashing
Model 8bits 16bits 32bits 64bits 128bits
KSH 0.6608 0.6842 0.7047 0.7175 0.7243
SHTTM 0.6299 0.6571 0.6485 0.6893 0.6474
VDSH-SP 0.7498 0.7798 0.7891 0.7888 0.7970
NASH-DN-S 0.7438 0.7946 0.7987 0.801 0.8139
Doc2hash-S 0.8140 0.8472 0.8490 0.8492 0.8439

Table 1: Precision of the top 100 retrieved documents
on TMC dataset.

Unsupervised Hashing
Model 8bits 16bits 32bits 64bits 128bits
LSH 0.2802 0.3215 0.3862 0.4667 0.5194
S-RBM 0.5113 0.5740 0.6154 0.6177 0.6452
SpH 0.6080 0.6340 0.6513 0.6290 0.6045
STH 0.6616 0.7351 0.7554 0.7350 0.6986
VDSH 0.6859 0.7165 0.7753 0.7456 0.7318
NASH-DN 0.7470 0.8013 0.8418 0.8297 0.7924
Doc2hash 0.7543 0.8102 0.8487 0.8361 0.8344

Supervised Hashing
Model 8bits 16bits 32bits 64bits 128bits
KSH 0.7840 0.8376 0.8480 0.8537 0.8620
SHTTM 0.7992 0.8520 0.8323 0.8271 .8150
VDSH-SP 0.8890 0.9326 0.9283 0.9286 0.9395
NASH-DN-S 0.9214 0.9327 0.9455 0.9589 0.9502
Doc2hash-S 0.9134 0.9338 0.9557 0.9602 0.9598

Table 2: Precision of the top 100 retrieved documents
on Reuters dataset.

We evaluate the proposed method named
Doc2hash in both supervised and unsupervised
setting for semantic hashing. For unsuper-
vised task, these baselines are selected: Local-
ity Sensitive Hashing (LSH) (Datar et al., 2004),
Stack Restricted Boltzmann Machines (Salakhut-
dinov and Hinton, 2009), Spectral Hashing (Weiss
et al., 2009), Self taught Hashing (STH) (Zhang
et al., 2010), Variational Deep Semantic Hashing

Unsupervised Hashing
Model 8bits 16bits 32bits 64bits 128bits
LSH 0.4180 0.4352 0.4716 0.5214 0.5877
S-RBM 0.5106 0.5743 0.6130 0.6463 0.6531
SpH 0.5093 0.7121 0.7475 0.7559 0.7423
STH 0.3975 0.4898 0.5592 0.5945 0.5946
VDSH 0.7976 0.7944 0.8481 0.8951 0.8444
Doc2hash-S 0.8495 0.8858 0.9001 0.9123 0.9167

Supervised Hashing
Model 8bits 16bits 32bits 64bits 128bits
KSH 0.9126 0.9146 0.9221 0.9333 0.9350
SHTTM 0.8820 0.9038 0.9258 0.9459 0.9447
VDSH-SP 0.9666 0.9757 0.9788 0.9805 0.9794
Doc2hash-S 0.9720 0.98001 0.9810 0.9802 0.9797

Table 3: Precision of the top 100 retrieved documents
on RCV1 dataset.

(VDSH) (Chaidaroon and Fang, 2017), and the
best variant of Neural Architecture for Genera-
tive Semantic Hashing (Shen et al., 2018) (NASH-
DN). For the supervised task, we also compare
the proposed method with kinds of baselines: Su-
pervised Hashing with Kernels (KSH) (Liu et al.,
2012), Semantic Hashing using Tags and Topic
Modeling (SHTTM) (Wang et al., 2013), Super-
vised VDSH (VDSH-SP) and Supervised NASH
(NASH-DN-S).

We follow the same experimental protocol and
setting used in (Shen et al., 2018; Chaidaroon and
Fang, 2017). Three standard public datasets of
documents are chosen for training and evaluation:
Reuters (Lewis, 1997), TMC (Oza, 2010), and
RCV1 (Lewis et al., 2004). We evaluate the pro-
posed model by calculating the precision of top
100 retrieved documents based on hamming dis-
tance of their hash codes. The final precision score
is then averaged over all test documents. To make
comparable results with previous works, we ex-
periment with latent variables size of 8, 32, 64 and
128.

3.2 Semantic Hashing Evaluation

In this section, we evaluate Doc2hash over various
number of bits on the three datasets. In the unsu-
pervised hashing, Doc2hash outperforms NASH-
DN, the current state of the art model for semantic
hashing, and other methods. The Table 1 shows
the result of TMC dataset. Doc2hash shows its
ability to assign the similar data (with the same
label) to the hash codes of which hamming dis-
tances are small. The same trend and superior-
ity of Doc2hash are also observed in both Reuters
and RCV1 datasets as shown in Tables 2 and 3.
Note that we do not compare with NASH-DN in
RCV1 because NASH doesn’t report any results



2239

on RCV1 dataset and doesn’t release any code to
reproduce their result. We compare the remind-
ing methods with Doc2hash. Table 3 shows our
approach improves the precision with significant
margin compared with other models. We note that
the retrieval results tend to drop when we set the
length of hash codes to be 64 or larger, which also
happens for some baselines. It is probably because
of over-fitting. Compared with other methods, the
proposed method is robust when the codes length
increase. Our approach performs better than other
methods in 64 and 128 bits setting, suggesting that
Doc2hash can effectively generate hash codes to
documents even with limited training data.

We also evaluate Doc2hash in the supervised
hashing setting with the same datasets. We make
use of the label/tag information during training.
As shown in Tables 1, 2 and 3, Doc2hash yields
better results than the other baseline models in dif-
ferent bits length.

3.3 Quantitative Analysis

In this section, we explore the reason why the pro-
posed method outperforms the counterpart based
on the ST estimator. We try to analyze the distribu-
tion of latent variables that trained with ST estima-
tor (NASH) and Gumbel softmax relaxation (our
method) respectively to quantitatively estimate the
coding efficiency. To obtain efficient hash codes
in the unsupervised setting, an expected pattern is
that we get the same amount of 1 as well as 0.
Because according to information theory, such a
uniform distribution produces the maximum en-
tropy in Bernoulli distribution. Motivated by this
idea, we make a comparison between NASH and
Doc2hash in the conditional probabilistic distribu-
tion of latent variables (i.e., p(z|x)). As we can
see in Figure 4(a), the distribution of p(z|x) in
NASH is significantly asymmetric while the pro-
posed method generates a well-balanced distribu-
tion (as Shown in Figure 4(b)). This comparison
demonstrates that our approach is more close to
the expected target in the probabilistic distribu-
tion of p(z|x). Furthermore, we calculate the pro-
portion of 1 in both methods. The proportion of
NASH is 40%, while our method is 49%. Thus,
it shows our approach is able to produce more ef-
ficient hash codes by using Gumbel softmax re-
laxation because it is very close to the up-bound
of information theory for Bernoulli distribution.
We demonstrate this point more clearly by show-

Figure 3: The distribution of gφ(x) in NASH

(a) (b)

Figure 4: the distribution of p(z|x): (a) is the distribu-
tion of NASH. (b) is the distribution of Doc2hash

ing the distribution of gφ(x) in NASH. It is asym-
metric because the peak value tilts to the left side.
And quantization based on thresholding around
the zero point implies that the hash codes pro-
duced by NASH contain more number of 0 than
the number of 1.

4 Conclusion

In this paper, we present a generative model with
discrete latent variables to learn binary representa-
tion for semantic hashing. We show that compared
with the Straight-Through estimator, the Gumbel-
Softmax relaxation provides a better solution to
learn hash codes and thus gains the state of the art
performance in three different datasets. We also
explore the reason why the proposed method pro-
duces more efficient hash codes compared with the
counterpart based on the ST estimator.

Acknowledgements

We sincerely thank anonymous reviewers for your
constructive suggestions to improve the quality of
this paper.



2240

References
Yoshua Bengio, Nicholas Léonard, and Aaron

Courville. 2013. Estimating or propagating gradi-
ents through stochastic neurons for conditional com-
putation. arXiv preprint arXiv:1308.3432.

Samuel R Bowman, Luke Vilnis, Oriol Vinyals, An-
drew M Dai, Rafal Jozefowicz, and Samy Ben-
gio. 2015. Generating sentences from a continuous
space. arXiv preprint arXiv:1511.06349.

Suthee Chaidaroon and Yi Fang. 2017. Variational
deep semantic hashing for text documents. In Pro-
ceedings of the 40th International ACM SIGIR Con-
ference on Research and Development in Informa-
tion Retrieval, pages 75–84. ACM.

Mayur Datar, Nicole Immorlica, Piotr Indyk, and Va-
hab S Mirrokni. 2004. Locality-sensitive hashing
scheme based on p-stable distributions. In Proceed-
ings of the twentieth annual symposium on Compu-
tational geometry, pages 253–262. ACM.

Emil Julius Gumbel. 1954. Statistical theory of ex-
treme values and some practical applications. NBS
Applied Mathematics Series, 33.

Eric Jang, Shixiang Gu, and Ben Poole. 2016. Categor-
ical reparameterization with gumbel-softmax. arXiv
preprint arXiv:1611.01144.

Diederik P Kingma and Max Welling. 2013. Auto-
encoding variational bayes. arXiv preprint
arXiv:1312.6114.

Quoc Le and Tomas Mikolov. 2014. Distributed rep-
resentations of sentences and documents. In Inter-
national Conference on Machine Learning, pages
1188–1196.

David D. Lewis. 1997. Reuters-21578 text categoriza-
tion collection data set.

David D Lewis, Yiming Yang, Tony G Rose, and Fan
Li. 2004. Rcv1: A new benchmark collection for
text categorization research. Journal of machine
learning research, 5(Apr):361–397.

Wei Liu, Jun Wang, Rongrong Ji, Yu-Gang Jiang, and
Shih-Fu Chang. 2012. Supervised hashing with ker-
nels. In CVPR, pages 2074–2081. IEEE.

Chris J Maddison, Andriy Mnih, and Yee Whye Teh.
2016. The concrete distribution: A continuous
relaxation of discrete random variables. arXiv
preprint arXiv:1611.00712.

Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Cor-
rado, and Jeff Dean. 2013. Distributed representa-
tions of words and phrases and their compositional-
ity. In Advances in neural information processing
systems, pages 3111–3119.

Aaron van den Oord, Oriol Vinyals, et al. 2017. Neu-
ral discrete representation learning. In Advances
in Neural Information Processing Systems, pages
6306–6315.

Nikunj Oza. 2010. Siam 2007 text mining competition
dataset.

Ruslan Salakhutdinov and Geoffrey Hinton. 2009. Se-
mantic hashing. IJAR, 50(7):969–978.

Stanislau Semeniuta, Aliaksei Severyn, and Erhardt
Barth. 2017. A hybrid convolutional variational
autoencoder for text generation. arXiv preprint
arXiv:1702.02390.

Dinghan Shen, Qinliang Su, Paidamoyo Chapfuwa,
Wenlin Wang, Guoyin Wang, Ricardo Henao, and
Lawrence Carin. 2018. Nash: Toward end-to-end
neural architecture for generative semantic hashing.
In Proceedings of the 56th Annual Meeting of the
Association for Computational Linguistics (Volume
1: Long Papers), pages 2041–2050. Association for
Computational Linguistics.

Qifan Wang, Dan Zhang, and Luo Si. 2013. Semantic
hashing using tags and topic modeling. In Proceed-
ings of the 36th international ACM SIGIR confer-
ence on Research and development in information
retrieval, pages 213–222. ACM.

Yair Weiss, Antonio Torralba, and Rob Fergus. 2009.
Spectral hashing. In Advances in neural information
processing systems, pages 1753–1760.

Dell Zhang, Jun Wang, Deng Cai, and Jinsong Lu.
2010. Self-taught hashing for fast similarity search.
In Proceedings of the 33rd international ACM SI-
GIR conference on Research and development in in-
formation retrieval, pages 18–25. ACM.


