




































Shifting the Baseline: Single Modality Performance on Visual Navigation & QA


Proceedings of NAACL-HLT 2019, pages 1977–1983
Minneapolis, Minnesota, June 2 - June 7, 2019. c©2019 Association for Computational Linguistics

1977

Shifting the Baseline:
Single Modality Performance on Visual Navigation & QA

Jesse Thomason Daniel Gordon Yonatan Bisk
Paul G. Allen School of Computer Science and Engineering
{jdtho,xkcd,ybisk}@cs.washington.edu

Abstract

We demonstrate the surprising strength of uni-
modal baselines in multimodal domains, and
make concrete recommendations for best prac-
tices in future research. Where existing work
often compares against random or majority
class baselines, we argue that unimodal ap-
proaches better capture and reflect dataset bi-
ases and therefore provide an important com-
parison when assessing the performance of
multimodal techniques. We present unimodal
ablations on three recent datasets in visual nav-
igation and QA, seeing an up to 29% absolute
gain in performance over published baselines.

1 Introduction

All datasets have biases. Baselines should cap-
ture these regularities so that outperforming them
indicates a model is actually solving a task. In
multimodal domains, bias can occur in any sub-
set of the modalities. To address this, we argue
it is not sufficient for researchers to provide ran-
dom or majority class baselines; instead we rec-
ommend presenting results for unimodal models.
We investigate visual navigation and question an-
swering tasks, where agents move through simu-
lated environments using egocentric (first person)
vision. We find that unimodal ablations (e.g., lan-
guage only) in these seemingly multimodal tasks
can outperform corresponding full models (§4.1).

This work extends observations made in both
the Computer Vision (Goyal et al., 2018; Cirik
et al., 2018) and Natural Language (Mudrakarta
et al., 2018; Glockner et al., 2018; Poliak et al.,
2018; Gururangan et al., 2018; Kaushik and Lip-
ton, 2018) communities that complex models of-
ten perform well by fitting to simple, unintended
correlations in the data, bypassing the complex
grounding and reasoning that experimenters hoped
was necessary for their tasks.

F L R U D E F L R U D E

F L R U D EF L R U D E

t1 t2

t3 t4

Actions: Forward, turn Left & Right, tilt Up & Down, End

Figure 1: Navigating without vision leads to sensible
navigation trajectories in response to commands like
“walk past the bar and turn right”. At t3, “forward” is
unavailable as the agent would collide with the wall.

We ablate models from three recent papers:
(1) navigation (Figure 1) using images of real
homes paired with crowdsourced language de-
scriptions (Anderson et al., 2018); and (2, 3) navi-
gation and egocentric question answering (Gordon
et al., 2018; Das et al., 2018a) in simulation with
synthetic questions. We find that unimodal abla-
tions often outperform the baselines that accom-
pany these tasks.

Recommendation for Best Practices: Our find-
ings show that in the new space of visual nav-
igation and egocentric QA, all modalities, even
an agent’s action history, are strongly informative.
Therefore, while many papers ablate either lan-
guage or vision, new results should ablate both.
Such baselines expose possible gains from uni-
modal biases in multimodal datasets irrespective
of training and architecture details.

2 Ablation Evaluation Framework

In the visual navigation and egocentric question
answering tasks, at each timestep an agent receives
an observation and produces an action. Actions
can move the agent to a new location or heading



1978

forward

turn left

turn right

tilt up

tilt down

end

start

for
war

d

tur
n l

eft

tur
n r

igh
t

til
t u

p

til
t d

own

end sta
rt

C
on

di
tio

na
l

M
ar
gi
na

l

.36

.44

.43

.54

.54

.393 .255 .257

.22 .22 .16

.071.012 .012 .001

.00

.00 .00

.00.00

.00

.00

.02 .02

.01 .01

.01.01

Figure 2: P (act = col|prev = row) and marginal ac-
tion distributions in Matterport training. Peaked distri-
butions enable agents to memorize simple rules like not
turning left immediately after turning right, or moving
forward an average number of steps.

(e.g., turn left), or answer questions (e.g., answer
‘brown’). At timestep t, a multimodal model M
takes in a visual input Vt and language question or
navigation command L to predict the next action
at. The navigation models we examine also take in
their action from the previous timestep, at−1, and
‘minimally sensed’ world information W specify-
ing which actions are available (e.g., that forward
is unavailable if the agent is facing a wall).

at ←M(Vt,L, at−1;W ) (1)

In each benchmark, M corresponds to the au-
thor’s released code and training paradigm. In ad-
dition to their full model, we evaluate the role of
each input modality by removing those inputs and
replacing them with zero vectors. Formally, we
define the full model and three ablations:

Full Model is M(Vt,L, at−1;W ) (2)
A is M( ~0 , ~0 , at−1;W ) (3)
A+ V is M(Vt, ~0 , at−1;W ) (4)
A+ L is M( ~0 ,L, at−1;W ) (5)

corresponding to models with access to Action in-
puts, V ision inputs, and Language inputs. These
ablations preserve the architecture and number of
parameters ofM by changing only its inputs.

3 Benchmark Tasks

We evaluate on navigation and question answering
tasks across three benchmark datasets: Matterport
Room-to-Room (no question answering compo-
nent), and IQUAD V1 and EQA (question answer-
ing that requires navigating to the relevant scene in
the environment) (Anderson et al., 2018; Gordon

Is there an apple in the fridge?
Yes No

Figure 3: IQA data construction attempts to make both
the question and image necessary for QA.

et al., 2018; Das et al., 2018a). We divide the latter
two into separate navigation and question answer-
ing components. We then train and evaluate mod-
els separately per subtask to analyze accuracy.

3.1 Matterport Room-to-Room

An agent is given a route in English and navigates
through a discretized map to the specified desti-
nation (Anderson et al., 2018). This task includes
high fidelity visual inputs and crowdsourced natu-
ral language routes.

Published Full Model: At each timestep an
LSTM decoder uses a ResNet-encoded image Vt
and previous action at−1 to attend over the states
of an LSTM language encoder (L) to predict nav-
igation action at (seen in Figure 2).

Published Baseline: The agent chooses a ran-
dom direction and takes up to five forward actions,
turning right when no forward action is available.

3.2 Interactive Question Answering

IQUAD V1 (Gordon et al., 2018) contains three
question types: existence (e.g., Is there a ...?),
counting (e.g., How many ...?) where the answer
ranges from 0 to 3, and spatial relation: (e.g., Is
there a ... in the ...?). The data was constructed
via randomly generated configurations to weaken
majority class baselines (Figure 3). To evalu-
ate the navigation subtask, we introduce a new
THOR-Nav benchmark.1 The agent is placed in
a random location in the room and must approach
one of fridge, garbage can, or microwave in re-
sponse to a natural language question.

Although we use the same full model as Gordon
et al. (2018), our QA results are not directly com-
parable. In particular, Gordon et al. (2018) do not
quantify the effectiveness of the QA component
independent of the scene exploration (i.e. naviga-
tion and interaction). To remove the scene explo-

1Formed from a subset of IQUAD V1 questions.



1979

ration steps of Gordon et al. (2018), we provide a
complete ground-truth view of the environment.2

We use ground-truth rather than YOLO (Redmon
et al., 2016) due to speed constraints.

Nav Full Model: The image and ground-truth
semantic segmentation mask Vt, tiled question L,
and previous action at−1 are encoded via a CNN
which outputs a distribution over actions. Optimal
actions are learned via teacher forcing.

Nav Baseline: The agent executes 100 randomly
chosen navigation actions then terminates. In
AI2THOR (Kolve et al., 2017), none of the
kitchens span more than 5 meters. With a step-size
of 0.25 meters, we observed that 100 actions was
significantly shorter than the shortest path length.

Published QA Full Model: The question en-
coding L is tiled and concatenated with a top-
down view of the ground truth location of all ob-
jects in the scene V . This is fed into several convo-
lutions, a spatial sum, and a final fully connected
layer which outputs a likelihood for each answer.

Published QA Baseline: We include the major-
ity class baseline from Gordon et al. (2018).

3.3 Embodied Question Answering
EQA (Das et al., 2018a) questions are program-
matically generated to refer to a single, unambigu-
ous object for a specific environment, and are fil-
tered to avoid easy questions (e.g., What room is
the bathtub in?). At evaluation, an agent is placed
a fixed number of actions away from the object.

Published Nav Full Model: At each timestep, a
planner LSTM takes in a CNN encoded image Vt,
LSTM encoded question L, and the previous ac-
tion at−1 and emits an action at. The action is ex-
ecuted in the environment, and then a lower-level
controller LSTM continues to take in new vision
observations and at, either repeating at again or
returning control to the planner.

Published Nav Baseline: This baseline model is
trained and evaluated with the same inputs as the
full model, but does not pass control to a lower-
level controller, instead predicting a new action
using the planner LSTM at each timestep (i.e., no
hierarchical control). Das et al. (2018a) name this
baseline LSTM+Question.

2This approximates the agent having visited every possi-
ble location, interacted with all possible objects, and looked
in all possible directions before answering.

Matterport↑ THOR-Nav↑ EQA↓
(%) (%) (m)

Model Seen Un Seen Un Un

Pu
b. Full Model 27.1 19.6 77.7 18.08 4.17

Baseline 15.9 16.3 2.18 1.54 4.21

U
ni

A 18.5 17.1 4.53 2.88 4.53
A + V 21.2 16.6 35.6 7.50 ∗4.11
A + L 23.0 ∗22.1 4.03 3.46 4.64

∆ Uni – Base +7.1 +5.8 +33.4 +5.96 -0.10

Table 1: Navigation success (Matterport, THOR-Nav)
(%) and remaining distance to target (EQA) (m). Best
unimodal in bold; better than reported baseline; ∗better
than full model.

Published QA Full Model: Given the last five
image encodings along the gold standard naviga-
tion trajectory, Vt−4 . . .Vt, and the question en-
coding L, image-question similarities are calcu-
lated via a dot product and converted via attention
weights to a summary weight V̄ , which is concate-
nated with L and used to predict the answer. Das
et al. (2018a) name this oracle-navigation model
ShortestPath+VQA.

QA Baseline: Das et al. (2018a) provide no ex-
plicit baseline for the VQA component alone. We
use a majority class baseline inspired by the data’s
entropy based filtering.

4 Experiments

Across all benchmarks, unimodal baselines out-
perform baseline models used in or derived from
the original works. Navigating unseen environ-
ments, these unimodal ablations outperform their
corresponding full models on the Matterport (ab-
solute ↑ 2.5% success rate) and EQA (↓ 0.06m
distance to target).

4.1 Navigation
We evaluate our ablation baselines on Matterport,3

THOR-Nav, and EQA (Table 1),4 and discover
that some unimodal ablations outperform their
corresponding full models. For Matterport and
THOR-Nav, success rate is defined by proximity
to the target. For EQA, we measure absolute dis-
tance from the target in meters.

Unimodal Performance: Across Matterport,
THOR-Nav, and EQA, either A + V or A + L

3We report on Matterport-validation since this allows
comparing Seen versus Unseen house performance.

4For consistency with THOR-Nav and EQA, we here
evaluate Matterport using teacher forcing.



1980

Matterport↑
(%)

Model Seen Unseen

Pu
b. Full Model 38.6 21.8

Baseline 15.9 16.3
U

ni
A 4.1 3.2
A + V 30.6 13.3
A + L 15.4 13.9

∆ Uni – Base +14.7 -2.4

Table 2: Navigation results for Matterport when trained
using student forcing. Best unimodal in bold; better
than reported baseline.

achieves better performance than existing base-
lines. In Matterport, the A + L ablation per-
forms better than the Full Model in unseen en-
vironments. The diverse scenes in this simula-
tor may render the vision signal more noisy than
helpful in previously unseen environments. The
A + V model in THOR-Nav and EQA is able to
latch onto dataset biases in scene structure to nav-
igate better than chance (for IQA), and the non-
hierarchical baseline (in EQA). In EQA, A + V
also outperforms the Full Model;5 the latent in-
formation about navigation from questions may be
too distant for the model to infer.

The agent with access only to its action history
(A) outperforms the baseline agent in Matterport
and THOR-Nav environments, suggesting it learns
navigation correlations that are not captured by
simple random actions (THOR-Nav) or program-
matic walks away from the starting position (Mat-
terport). Minimal sensing (which actions are avail-
able, W ) coupled with the topological biases in
trajectories (Figure 2), help this nearly zero-input
agent outperform existing baselines.6

Matterport Teacher vs Student forcing With
teacher forcing, at each timestep the navigation
agent takes the gold-standard action regardless of
what action it predicted, meaning it only sees steps
along gold-standard trajectories. This paradigm is
used to train the navigation agent in THOR-Nav
and EQA. Under student forcing, the agent sam-
ples the action to take from its predictions, and
loss is computed at each time step against the ac-
tion that would have put the agent on the shortest

5EQA full & baseline model performances do not exactly
match those in Das et al. (2018a) because we use the ex-
panded data updated by the authors https://github.
com/facebookresearch/EmbodiedQA/.

6This learned agent begins to relate to work in minimally
sensing robotics (O’Kane and LaValle, 2006).

dT ↓ dmin ↓
(m) (m)

Model T−10 T−30 T−50 T−10 T−30 T−50

Pu
b. Full 0.971 4.17 8.83 0.291 2.43 6.45

Baseline 1.020 4.21 8.73 0.293 2.45 6.38

U
ni

A ∗0.893 4.53 9.56 ∗0.242 3.16 7.99
A + V ∗0.951 ∗4.11 †8.83 ∗0.287 2.51 ∗6.44
A + L 0.987 4.64 9.51 ∗0.240 3.19 7.96

∆ U – B -0.127 -0.10 +0.10 -0.053 +0.06 +0.06

Table 3: Final distances to targets (dT) and minimum
distance from target achieved along paths (dmin) in
EQA navigation. Best unimodal in bold; better than
reported baseline; ∗better than full model; †tied with
full model.

path to the goal. Thus, the agent sees more of the
scene, but can take more training iterations to learn
to move to the goal.

Table 2 gives the highest validation success
rates across all epochs achieved in Matterport by
models trained using student forcing. The uni-
modal ablations show that the Full Model, possi-
bly because with more exploration and more train-
ing episodes, is better able to align the vision and
language signals, enabling generalization in un-
seen environments that fails with teacher forcing.

EQA Navigation Variants Table 3 gives the av-
erage final distance from the target (dT, used as
the metric in Table 1) and average minimum dis-
tance from target achieved along the path (dmin)
during EQA episodes for agents starting 10, 30,
and 50 actions away from the target in the EQA
navigation task. At 10 actions away, the unimodal
ablations tend to outperform the full model on
both metrics, possibly due to the shorter length of
the episodes (less data to train the joint parame-
ters). The A + V ablation performs best among
the ablations, and ties with or outperforms the Full
Model in all but one setting, suggesting that the
EQA Full Model is not taking advantage of lan-
guage information under any variant.

4.2 Question Answering

We evaluate our ablation baselines on IQUAD V1
and EQA, reporting top-1 QA accuracy (Table 4)
given gold standard navigation information as V .
These decoupled QA models do not take in a pre-
vious action, so we do not consider A ONLY abla-
tions for this task.

Unimodal Performance: On IQUAD V1, due
to randomization in its construction, model ab-



1981

IQUAD V1↑ EQA ↑
Model Unseen Seen Unseen

Pu
b. Full Model 88.3 89.3 64.0

Baseline 41.7 41.7 19.8
U

ni V ONLY 43.5 42.8 44.2
L ONLY 41.7 41.7 48.8

∆ Uni – Base +1.8 +1.1 +29.0

Table 4: Top-1 QA accuracy. Best unimodal in bold;
better than reported baseline.

lations perform nearly at chance.7 The V ONLY
model with access to the locations of all scene ob-
jects only improves by 2% over random guessing.

For EQA, single modality models perform sig-
nificantly better than the majority class baseline.
The vision-only model is able to identify salient
colors and basic room features that allow it to re-
duce the likely set of answers to an unknown ques-
tion. The language only models achieve nearly
50%, suggesting that despite the entropy filtering
in Das et al. (2018a) each question has one answer
that is as likely as all other answers combined (e.g.
50% of the answers for What color is the bathtub?
are grey, and other examples in Figure 4).

5 Related Work

Historically, semantic parsing was used to map
natural language instructions to visual navigation
in simulation environments (Chen and Mooney,
2011; MacMahon et al., 2006). Modern ap-
proaches use neural architectures to map natu-
ral language to the (simulated) world and execute
actions (Paxton et al., 2019; Chen et al., 2018;
Nguyen et al., 2018; Blukis et al., 2018; Fried
et al., 2018; Mei et al., 2016). In visual ques-
tion answering (VQA) (Antol et al., 2015; Hud-
son and Manning, 2019) and visual commonsense
reasoning (VCR) (Zellers et al., 2019), input im-
ages are accompanied with natural language ques-
tions. Given the question, egocentric QA requires
an agent to navigate and interact with the world to
gather the relevant information to answer the ques-
tion. In both cases, end-to-end neural architectures
make progress on these tasks.

For language annotations, task design, diffi-
culty, and annotator pay can introduce unintended
artifacts which can be exploited by models to
“cheat” on otherwise complex tasks (Glockner

7Majority class and chance for IQUAD V1 both achieve
50%, 50%, 25% when conditioned on question type; our
Baseline model achieves the average of these.

Fi
na

l I
m

ag
e

Question

V Only

L Only

Maj Class

Full Model

Answer

What room is the 
iron located in?

What color is
the dresser?

What color is the 
loudspeaker …?

What room is the 
fruit bowl located in?

Brown Kitchen Brown Kitchen

Brown Green Living Room Kitchen

Brown Bathroom Brown Kitchen

Brown Brown Brown Brown

Brown Bathroom Brown Kitchen

What color is the 
bathtub … ?

Grey

Bathroom

Grey

Brown

Grey

Figure 4: Qualitative results on the EQA task. The
language only model can pick out the most likely an-
swer for a question. The vision only model finds salient
color and room features, but is unaware of the question.

et al., 2018; Poliak et al., 2018). Such issues also
occur in multimodal data like VQA (Goyal et al.,
2018), where models can answer correctly with-
out looking at the image. In image captioning,
work has shown competitive models relying only
on nearest-neighbor lookups (Devlin et al., 2015)
as well as exposed misalignment between caption
relevance and text-based metrics (Rohrbach et al.,
2018). Our unimodal ablations of visual naviga-
tion and QA benchmarks uncover similar biases,
which deep architectures are quick to exploit.

6 Conclusions

In this work, we introduce an evaluation frame-
work and perform the missing analysis from sev-
eral new datasets. While new state-of-the-art mod-
els are being introduced for several of these do-
mains (e.g., Matterport: (Ma et al., 2019a; Ke
et al., 2019; Wang et al., 2019; Ma et al., 2019b;
Tan et al., 2019; Fried et al., 2018), and EQA:
(Das et al., 2018b)), they lack informative, indi-
vidual unimodal ablations (i.e., ablating both lan-
guage and vision) of the proposed models.

We find a performance gap between baselines
used in or derived from the benchmarks examined
in this paper and unimodal models, with unimodal
models outperforming those baselines across all
benchmarks. These unimodal models can even
outperform their multimodal counterparts. In light
of this, we recommend all future work include uni-
modal ablations of proposed multimodal models
to vet and highlight their learned representations.

Acknowledgements

This work was supported by NSF IIS-1524371,
1703166, NRI-1637479, IIS-1338054, 1652052,
ONR N00014-13-1-0720, and the DARPA CwC
program through ARO (W911NF-15-1-0543).



1982

References
Peter Anderson, Qi Wu, Damien Teney, Jake Bruce,

Mark Johnson, Niko Sünderhauf, Ian Reid, Stephen
Gould, and Anton van den Hengel. 2018. Vision-
and-Language Navigation: Interpreting visually-
grounded navigation instructions in real environ-
ments. In Conference on Computer Vision and Pat-
tern Recognition (CVPR).

Stanislaw Antol, Aishwarya Agrawal, Jiasen Lu, Mar-
garet Mitchell, Dhruv Batra, C. Lawrence Zitnick,
and Devi Parikh. 2015. VQA: Visual Question An-
swering. In International Conference on Computer
Vision (ICCV).

Valts Blukis, Dipendra Misra, Ross A. Knepper, and
Yoav Artzi. 2018. Mapping navigation instructions
to continuous control actions with position visita-
tion prediction. In Proceedings of the Conference
on Robot Learning.

David L. Chen and Raymond J. Mooney. 2011. Learn-
ing to interpret natural language navigation instruc-
tions from observations. In AAAI Conference on Ar-
tificial Intelligence (AAAI).

Howard Chen, Alane Shur, Dipendra Misra, Noah
Snavely, and Yoav Artzi. 2018. Touchdown: Natural
language navigation and spatial reasoning in visual
street environments. In NeurIPS Workshop on Visu-
ally Grounded Interaction and Language (ViGIL).

Volkan Cirik, Louis-Philippe Morency, and Taylor
Berg-Kirkpatrick. 2018. Visual referring expression
recognition: What do systems actually learn? In
Proc. of the North American Chapter of the Associ-
ation for Computational Linguistics (NAACL).

Abhishek Das, Samyak Datta, Georgia Gkioxari, Ste-
fan Lee, Devi Parikh, and Dhruv Batra. 2018a.
Embodied Question Answering. In Conference on
Computer Vision and Pattern Recognition (CVPR).

Abhishek Das, Georgia Gkioxari, Stefan Lee, Devi
Parikh, and Dhruv Batra. 2018b. Neural Modular
Control for Embodied Question Answering. In Con-
ference on Robot Learning (CoRL).

Jacob Devlin, Saurabh Gupta, Ross Girshick, Margaret
Mitchell, and C Lawrence Zitnick. 2015. Exploring
nearest neighbor approaches for image captioning.
arXiv preprint arXiv:1505.04467.

Daniel Fried, Ronghang Hu, Volkan Cirik, Anna
Rohrbach, Jacob Andreas, Louis-Philippe Morency,
Taylor Berg-Kirkpatrick, Kate Saenko, Dan Klein,
and Trevor Darrell. 2018. Speaker-follower models
for vision-and-language navigation. In Neural In-
formation Processing Systems (NeurIPS).

Max Glockner, Vered Shwartz, and Yoav Goldberg.
2018. Breaking NLI systems with sentences that re-
quire simple lexical inferences. In Proc. of the As-
sociation for Computational Linguistics (ACL).

Daniel Gordon, Aniruddha Kembhavi, Mohammad
Rastegari, Joseph Redmon, Dieter Fox, and Ali
Farhadi. 2018. Iqa: Visual question answering in in-
teractive environments. In Conference on Computer
Vision and Pattern Recognition (CVPR).

Yash Goyal, Tejas Khot, Aishwarya Agrawal, Douglas
Summers-Stay, Dhruv Batra, and Devi Parikh. 2018.
Making the V in VQA matter: Elevating the role of
image understanding in visual question answering.
International Journal of Computer Vision (IJCV).

Suchin Gururangan, Swabha Swayamdipta, Omer
Levy, Roy Schwartz, Samuel Bowman, and Noah A.
Smith. 2018. Annotation artifacts in natural lan-
guage inference data. In Proc. of the North Amer-
ican Chapter of the Association for Computational
Linguistics (NAACL).

Drew A. Hudson and Christopher D. Manning. 2019.
GQA: a new dataset for compositional question an-
swering over real-world images. In Conference on
Computer Vision and Pattern Recognition (CVPR).

Divyansh Kaushik and Zachary C. Lipton. 2018. How
much reading does reading comprehension require?
a critical investigation of popular benchmarks. In
Conference on Empirical Methods in Natural Lan-
guage Processing (EMNLP).

Liyiming Ke, Xiujun Li, Yonatan Bisk, Ari Holtz-
man, Zhe Gan, Jingjing Liu, Jianfeng Gao, Yejin
Choi, and Siddhartha Srinivasa. 2019. Tactical
rewind: Self-correction via backtracking in vision-
and-language navigation. In Conference on Com-
puter Vision and Pattern Recognition (CVPR).

Eric Kolve, Roozbeh Mottaghi, Daniel Gordon, Yuke
Zhu, Abhinav Gupta, and Ali Farhadi. 2017. AI2-
THOR: An Interactive 3D Environment for Visual
AI. arXiv.

Chih-Yao Ma, Jiasen Lu, Zuxuan Wu, Ghassan Al-
Regib, Zsolt Kira, Richard Socher, and Caim-
ing Xiong. 2019a. Self-aware visual-textual co-
grounded navigation agent. In International Con-
ference on Learning Representations (ICLR).

Chih-Yao Ma, Zuxuan Wu, Ghassan AlRegib, Caiming
Xiong, and Zsolt Kira. 2019b. The regretful agent:
Heuristic-aided navigation through progress estima-
tion. In Conference on Computer Vision and Pattern
Recognition (CVPR).

Matt MacMahon, Brian Stankiewicz, and Benjamin
Kuipers. 2006. Walk the talk: Connecting language,
knowledge, and action in route instructions. In AAAI
Conference on Artificial Intelligence (AAAI).

Hongyuan Mei, Mohit Bansal, and Matthew R. Wal-
ter. 2016. Listen, attend, and walk: Neural mapping
of navigational instructions to action sequences. In
AAAI Conference on Artificial Intelligence (AAAI).



1983

Pramod Kaushik Mudrakarta, Ankur Taly, Mukund
Sundararajan, and Kedar Dhamdhere. 2018. Did the
model understand the question? In Proc. of the As-
sociation for Computational Linguistics (ACL).

Khanh Nguyen, Debadeepta Dey, Chris Brockett, and
Bill Dolan. 2018. Vision-based navigation with
language-based assistance via imitation learning
with indirect intervention. In Conference on Com-
puter Vision and Pattern Recognition (CVPR).

Jason M O’Kane and Steven M. LaValle. 2006. On
comparing the power of mobile robots. In Robotics:
Science and Systems (RSS).

Chris Paxton, Yonatan Bisk, Jesse Thomason, Arunk-
umar Byravan, and Dieter Fox. 2019. Prospection:
Interpretable plans from language by predicting the
future. In International Conference on Robotics and
Automation (ICRA).

Adam Poliak, Jason Naradowsky, Aparajita Haldar,
Rachel Rudinger, and Benjamin Van Durme. 2018.
Hypothesis Only Baselines in Natural Language In-
ference. In Joint Conference on Lexical and Com-
putational Semantics (StarSem).

Joseph Redmon, Santosh Divvala, Ross Girshick, and
Ali Farhadi. 2016. You only look once: Unified,
real-time object detection. In Conference on Com-
puter Vision and Pattern Recognition (CVPR).

Anna Rohrbach, Lisa Anne Hendricks, Kaylee Burns,
Trevor Darrell, and Kate Saenko. 2018. Object hal-
lucination in image captioning. In Conference on
Empirical Methods in Natural Language Processing
(EMNLP).

Hao Tan, Licheng Yu, and Mohit Bansal. 2019. Learn-
ing to navigate unseen environments: Back transla-
tion with environmental dropout. In North Ameri-
can Chapter of the Association for Computational
Linguistics (NAACL).

Xin Wang, Qiuyuan Huang, Asli Celikyilmaz, Jian-
feng Gao, Dinghan Shen, Yuan-Fang Wang,
William Yang Wang, and Lei Zhang. 2019. Re-
inforced cross-modal matching and self-supervised
imitation learning for vision-language navigation.
In Conference on Computer Vision and Pattern
Recognition (CVPR).

Rowan Zellers, Yonatan Bisk, Ali Farhadi, and Yejin
Choi. 2019. From recognition to cognition: Visual
commonsense reasoning. In Conference on Com-
puter Vision and Pattern Recognition (CVPR).


