



















































Neural Models of Factuality


Proceedings of NAACL-HLT 2018, pages 731–744
New Orleans, Louisiana, June 1 - 6, 2018. c©2018 Association for Computational Linguistics

Neural models of factuality

Rachel Rudinger
Johns Hopkins University

Aaron Steven White
University of Rochester

Benjamin Van Durme
Johns Hopkins University

Abstract

We present two neural models for event fac-
tuality prediction, which yield significant per-
formance gains over previous models on three
event factuality datasets: FactBank, UW, and
MEANTIME. We also present a substantial
expansion of the It Happened portion of the
Universal Decompositional Semantics dataset,
yielding the largest event factuality dataset to
date. We report model results on this extended
factuality dataset as well.

1 Introduction

A central function of natural language is to convey
information about the properties of events. Per-
haps the most fundamental of these properties is
factuality: whether an event happened or not.

A natural language understanding system’s abil-
ity to accurately predict event factuality is impor-
tant for supporting downstream inferences that are
based on those events. For instance, if we aim to
construct a knowledge base of events and their par-
ticipants, it is crucial that we know which events
to include and which ones not to.

The event factuality prediction task (EFP) in-
volves labeling event-denoting phrases (or their
heads) with the (non)factuality of the events de-
noted by those phrases (Saurı́ and Pustejovsky,
2009, 2012; de Marneffe et al., 2012). Figure
1 exemplifies such an annotation for the phrase
headed by leave in (1), which denotes a factual
event (⊕=factual, 	=nonfactual).
(1) Jo failed to leave no trace. ⊕
In this paper, we present two neural models of
event factuality (and several variants thereof). We
show that these models significantly outperform
previous systems on four existing event factual-
ity datasets – FactBank (Saurı́ and Pustejovsky,
2009), the UW dataset (Lee et al., 2015), MEAN-
TIME (Minard et al., 2016), and Universal De-

failed

Jo leave

to trace

no

.

+

Inside context

Outside context

Figure 1: Event factuality (⊕=factual) and inside v.
outside context for leave in the dependency tree.

compositional Semantics It Happened v1 (UDS-
IH1; White et al., 2016) – and we demonstrate the
efficacy of multi-task training and ensembling in
this setting. In addition, we collect and release an
extension of the UDS-IH1 dataset, which we refer
to as UDS-IH2, to cover the entirety of the English
Universal Dependencies v1.2 (EUD1.2) treebank
(Nivre et al., 2015), thereby yielding the largest
event factuality dataset to date.1

We begin with theoretical motivation for the
models we propose as well as discussion of prior
EFP datasets and systems (§2). We then describe
our own extension of the UDS-IH1 dataset (§3),
followed by our neural models (§4). Using the
data we collect, along with the existing datasets,
we evaluate our models (§6) in five experimental
settings (§5) and analyze the results (§7).

2 Background

2.1 Linguistic description

Words from effectively every syntactic category
can convey information about the factuality of an
event. For instance, negation (2a), modal auxil-
iaries (2b), determiners (2c), adverbs (2d), verbs
(2e), adjectives (2f), and nouns (2g) can all con-

1Data available at decomp.net.

731



vey that a particular event – in the case of (2), a
leaving event – did not happen.

(2) a. Jo didn’t leave.
b. Jo might leave.
c. Jo left no trace.
d. Jo never left.
e. Jo failed to leave.
f. Jo’s leaving was fake.
g. Jo’s leaving was a hallucination.

Further, such words can interact to yield non-
trivial effects on factuality inferences: (3a) con-
veys that the leaving didn’t happen, while the su-
perficially similar (3b) does not.

(3) a. Jo didn’t remember to leave. 	
b. Jo didn’t remember leaving. ⊕

A main goal of many theoretical treatments of fac-
tuality is to explain why these sorts of interactions
occur and how to predict them. It is not possible to
cover all the relevant literature in depth, and so we
focus instead on the broader kind of interactions
our models need to be able to capture in order to
correctly predict the factuality of an event denoted
by a particular predicate—namely, interactions be-
tween that predicate’s outside and inside context,
exemplified in Figure 1.

Outside context Factuality information coming
from the outside context is well-studied in the
domain of clause-embedding predicates, which
break into at least four categories: factives, like
know and love (Kiparsky and Kiparsky, 1970;
Karttunen, 1971b; Hintikka, 1975); implicatives,
like manage and fail (Karttunen, 1971a, 2012,
2013; Karttunen et al., 2014), veridicals, like prove
and verify (Egré, 2008; Spector and Egré, 2015),
and non-veridicals, like hope and want.

Consider the factive-implicative verb forget
(Karttunen, 1971a; White, 2014).

(4) a. Jo forgot that Bo left. ⊕
b. Jo forgot to leave. 	

(5) a. Jo didn’t forget that Bo left. ⊕
b. Jo didn’t forget to leave. ⊕

When a predicate directly embedded by forget is
tensed, as in (4a) and (5a), we infer that that predi-
cate denotes a factual event, regardless of whether
forget is negated. In contrast, when a predicate di-
rectly embedded by forget is untensed, as in (4b)
and (5b), our inference is dependent on whether
forget is negated. Thus, any model that correctly
predicts factuality will need to not only be able to

represent the effect of individual words in the out-
side context on factuality inferences, it will fur-
thermore need to represent their interaction.

Inside context Knowledge of the inside context
is important for integrating factuality information
coming from a predicate’s arguments—e.g. from
determiners, like some and no.

(6) a. Some girl ate some dessert. ⊕
b. Some girl ate no dessert. 	
c. No girl ate no dessert. ⊕

In simple monoclausal sentences like those in (6),
the number of arguments that contain a negative
quantifier, like no, determine the factuality of the
event denoted by the verb. An even number (or
zero) will yield a factuality inference and an odd
number will yield a nonfactuality inference. Thus,
as for outside context, any model that correctly
predicts factuality will need to integrate interac-
tions between words in the inside context.

The (non)necessity of syntactic information
One question that arises in the context of inside
and outside information is whether syntactic infor-
mation is strictly necessary for capturing the rele-
vant interactions between the two. To what extent
is linear precedence sufficient for accurately com-
puting factuality?

We address these questions using two bidirec-
tional LSTMs—one that has a linear chain topol-
ogy and another that has a dependency tree topol-
ogy. Both networks capture context on either side
of an event-denoting word, but each does it in
a different way, depending on its topology. We
show below that, while both networks outperform
previous models that rely on deterministic rules
and/or hand-engineered features, the linear chain-
structured network reliably outperforms the tree-
structured network.

2.2 Event factuality datasets

Saurı́ and Pustejovsky (2009) present the Fact-
Bank corpus of event factuality annotations, built
on top of the TimeBank corpus (Pustejovsky et al.,
2006). These annotations (performed by trained
annotators) are discrete, consisting of an epistemic
modal {certain, probable, possible} and a polar-
ity {+,−}. In FactBank, factuality judgments are
with respect to a source; following recent work,
here we consider only judgments with respect to
a single source: the author. The smaller MEAN-
TIME corpus (Minard et al., 2016) includes sim-

732



Dataset Train Dev Test Total

FactBank 6636 2462 663 9761
MEANTIME 967 210 218 1395
UW 9422 3358 864 13644
UDS-IH2 22108 2642 2539 27289

Table 1: Number of annotated predicates.

ilar discrete factuality annotations. de Marneffe
et al. (2012) re-annotate a portion of FactBank us-
ing crowd-sourced ordinal judgments to capture
pragmatic effects on readers’ factuality judgments.

Lee et al. (2015) construct an event factuality
dataset – henceforth, UW – on the TempEval-3
data (UzZaman et al., 2013) using crowdsourced
annotations on a [−3, 3] scale (certainly did not
happen to certainly did), with over 13,000 pred-
icates. Adopting the [−3, 3] scale of Lee et al.
(2015), Stanovsky et al. (2017) assemble a Uni-
fied Factuality dataset, mapping the discrete anno-
tations of both FactBank and MEANTIME onto
the UW scale. Each scalar annotation corresponds
to a token representing the event, and each sen-
tence may have more than one annotated token.

The UDS-IH1 dataset (White et al., 2016) con-
sists of factuality annotations over 6,920 event to-
kens, obtained with another crowdsourcing proto-
col. We adopt this protocol, described in §3, to col-
lect roughly triple this number of annotations. We
train and evaluate our factuality prediction models
on this new dataset, UDS-IH2, as well as the uni-
fied versions of UW, FactBank, and MEANTIME.

Table 1 shows the number of annotated predi-
cates in each split of each factuality dataset used in
this paper. Annotations relevant to event factuality
and polarity appear in a number of other resources,
including the Penn Discourse Treebank (Prasad
et al., 2008), MPQA Opinion Corpus (Wiebe and
Riloff, 2005), the LU corpus of author belief com-
mitments (Diab et al., 2009), and the ACE and
ERE formalisms. Soni et al. (2014) annotate Twit-
ter data for factuality.

2.3 Event factuality systems

Nairn et al. (2006) propose a deterministic algo-
rithm based on hand-engineered lexical features
for determining event factuality. They associate
certain clause-embedding verbs with implication
signatures (Table 2), which are used in a recur-
sive polarity propagation algorithm. TruthTeller
is also a recursive rule-based system for factual-
ity (“predicate truth”) prediction using implication
signatures, as well as other lexical- and depen-

dency tree-based features (Lotan et al., 2013).
Several systems use supervised models trained

over rule-based features. Diab et al. (2009) and
Prabhakaran et al. (2010) use SVMs and CRFs
over lexical and dependency features for predict-
ing author belief commitments, which they treat as
a sequence tagging problem. Lee et al. (2015) train
an SVM on lexical and dependency path features
for their factuality dataset. Saurı́ and Pustejovsky
(2012) and Stanovsky et al. (2017) train support
vector models over the outputs of rule-based sys-
tems, the latter with TruthTeller.

3 Data collection

Even the largest currently existing event factuality
datasets are extremely small from the perspective
of related tasks, like natural language inference
(NLI). Where FactBank, UW, MEANTIME, and
the original UDS-IH1 dataset have on the order of
30,000 labeled examples combined, standard NLI
datasets, like the Stanford Natural Language Infer-
ence (SNLI; Bowman et al. 2015) dataset, have on
the order of 500,000.

To begin to remedy this situation, we collect an
extension of the UDS-IH1 dataset. The resulting
UDS-IH2 dataset covers all predicates in EUD1.2.
Beyond substantially expanding the amount of
publicly available event factuality annotations, an-
other major benefit is that EUD1.2 consists en-
tirely of gold parses and has a variety of other an-
notations built on top of it, making future multi-
task modeling possible.

We use the protocol described by White et al.
(2016) to construct UDS-IH2. This protocol in-
volves four kinds of questions for a particular
predicate candidate:

1. UNDERSTANDABLE: whether the sentence is
understandable

2. PREDICATE: whether or not a particular word
refers to an eventuality (event or state)

3. HAPPENED: whether or not, according to the
author, the event has already happened or is
currently happening

4. CONFIDENCE: how confident the annotator is
about their answer to HAPPENED from 0-4

If an annotator answers no to either UNDER-
STANDABLE or PREDICATE, HAPPENED and
CONFIDENCE do not appear.

The main differences between this protocol and
the others discussed above are: (i) instead of ask-
ing about annotator confidence, the other proto-

733



●●●

●

●●●

●

●●●
●

●

●●
●

●
●●● ●

●

●

●
●

●

●

●

●

●

●

●

−3 −2 −1 0 1 2 3

●

●

●

●

FactBank
UW
MEANTIME
UDS−IH2

Figure 2: Relative frequency of factuality ratings in
training and development sets.

cols ask the annotator to judge either source con-
fidence or likelihood; and (ii) factuality and confi-
dence are separated into two questions. We choose
to retain White et al.’s protocol to maintain consis-
tency with the portions of EUD1.2 that were al-
ready annotated in UDS-IH1.

Annotators We recruited 32 unique annotators
through Amazon’s Mechanical Turk to annotate
20,580 total predicates in groups of 10. Each pred-
icate was annotated by two distinct annotators. In-
cluding UDS-IH1, this brings the total number of
annotated predicates to 27,289.

Raw inter-annotator agreement for the HAP-
PENED question was 0.84 (Cohen’s κ=0.66)
among the predicates annotated only for UDS-
IH2. This compares to the raw agreement score of
0.82 reported by White et al. (2016) for UDS-IH1.

To improve the overall quality of the annota-
tions, we filter annotations from annotators that
display particularly low agreement with other an-
notators on HAPPENED and CONFIDENCE. (See
the Supplementary Materials for details.)

Pre-processing To compare model results on
UDS-IH2 to those found in the unified datasets
of Stanovsky et al. (2017), we map the HAP-
PENED and CONFIDENCE ratings to a single FAC-
TUALITY value in [-3,3] by first taking the mean
confidence rating for each predicate and mapping
FACTUALITY to 34CONFIDENCE if HAPPENED and
-34CONFIDENCE otherwise.

Response distribution Figure 2 plots the distri-
bution of factuality ratings in the train and dev
splits for UDS-IH2, alongside those of FactBank,
UW, and MEANTIME. One striking feature of
these distributions is that UDS-IH2 displays a
much more entropic distribution than the other
datasets. This may be due to the fact that, un-

like the newswire-heavy corpora that the other
datasets annotate, EUD1.2 contains text from gen-
res – weblogs, newsgroups, email, reviews, and
question-answers – that tend to involve less report-
ing of raw facts. One consequence of this more en-
tropic distribution is that, unlike the datasets dis-
cussed above, it is much harder for systems that
always guess 3 – i.e. factual with high confi-
dence/likelihood – to perform well.

4 Models

We consider two neural models of factuality: a
stacked bidirectional linear chain LSTM (§4.1)
and a stacked bidirectional child-sum dependency
tree LSTM (§4.2). To predict the factuality vt for
the event referred to by a word wt, we use the hid-
den state at t from the final layer of the stack as
the input to a two-layer regression model (§4.3).

4.1 Stacked bidirectional linear LSTM
We use a standard stacked bidirectional linear
chain LSTM (stacked L-biLSTM), which extends
the unidirectional linear chain LSTM (Hochreiter
and Schmidhuber, 1997) by adding the notion of
a layer l ∈ {1, . . . , L} and a direction d ∈ {→
,←} (Graves et al., 2013; Sutskever et al., 2014;
Zaremba and Sutskever, 2014).

f
(l,d)
t = σ

(
W

(l,d)
f

[
h
(l,d)
prevd(t)

;x
(l,d)
t

]
+ b

(l,d)
f

)

i
(l,d)
t = σ

(
W

(l,d)
i

[
h
(l,d)
prevd(t)

;x
(l,d)
t

]
+ b

(l,d)
i

)

o
(l,d)
t = σ

(
W(l,d)o

[
h
(l,d)
prevd(t)

;x
(l,d)
t

]
+ b(l,d)o

)

ĉ
(l,d)
t = g

(
W(l,d)c

[
h
(l,d)
prevd(t)

;x
(l,d)
t

]
+ b(l,d)c

)

c
(l,d)
t = i

(l,d)
t ◦ ĉ

(l,d)
t + f

(l,d)
t ◦ c

(l,d)
prevd(t)

h
(l,d)
t = o

(l,d)
t ◦ g

(
c
(l,d)
t

)

where ◦ is the Hadamard product; prev→(t) =
t − 1 and prev←(t) = t + 1, and x(l,d)t = xt
if l = 1; and x(l,d)t = [h

(l−1,→)
t ;h

(l−1,←)
t ] other-

wise. We set g to the pointwise nonlinearity tanh.

4.2 Stacked bidirectional tree LSTM
We use a stacked bidirectional extension to the
child-sum dependency tree LSTM (T-LSTM; Tai
et al., 2015), which is itself an extension of a stan-
dard unidirectional linear chain LSTM (L-LSTM).
One way to view the difference between the L-
LSTM and the T-LSTM is that the T-LSTM re-
defines prev→(t) to return the set of indices that

734



correspond to the children of wt in some depen-
dency tree. Because the cardinality of these sets
varies with t, it is necessary to specify how multi-
ple children are combined. The basic idea, which
we make explicit in the equations for our exten-
sion, is to define ftk for each child index k ∈
prev→(t) in a way analogous to the equations in
§4.1 – i.e. as though each child were the only child
– and then sum across k within the equations for
it, ot, ĉt, ct, and ht.

Our stacked bidirectional extension (stacked T-
biLSTM) is a minimal extension to the T-LSTM
in the sense that we merely define the downward
computation in terms of a prev←(t) that returns
the set of indices that correspond to the parents of
wt in some dependency tree (cf. Miwa and Bansal
2016, who propose a similar, but less minimal,
model for relation extraction). The same method
for combining children in the upward computa-
tion can then be used for combining parents in
the downward computation. This yields a minimal
change to the stacked L-biLSTM equations.

f
(l,d)
tk = σ

(
W

(l,d)
f

[
h
(l,d)
k ;x

(l,d)
t

]
+ b

(l,d)
f

)

ĥ
(l,d)
t =

∑

k∈prevd(t)
h
(l,d)
k

i
(l,d)
t = σ

(
W

(l,d)
i

[
ĥ
(l,d)
t ;x

(l,d)
t

]
+ b

(l,d)
i

)

o
(l,d)
t = σ

(
W(l,d)o

[
ĥ
(l,d)
t ;x

(l,d)
t

]
+ b(l,d)o

)

ĉ
(l,d)
t = g

(
W(l,d)c

[
ĥ
(l,d)
t ;x

(l,d)
t

]
+ b(l,d)c

)

c
(l,d)
t = i

(l,d)
t ◦ ĉ

(l,d)
t +

∑

k∈prevd(t)
f
(l,d)
tk ◦ c

(l,d)
k

h
(l,d)
t = o

(l,d)
t ◦ g

(
c
(l,d)
t

)

We use a ReLU pointwise nonlinearity for g.
These minimal changes allow us to represent the
inside and the outside contexts of word t (at layer
l) as single vectors: ĥ(l,→)t and ĥ

(l,←)
t .

An important thing to note here is that – in con-
trast to other dependency tree-structured T-LSTMs
(Socher et al., 2014; Iyyer et al., 2014) – this T-
biLSTM definition does not use the dependency
labels in any way. Such labels could be straight-
forwardly incorporated to determine which param-
eters are used in a particular cell, but for current
purposes, we retain the simpler structure (i) to
more directly compare the L- and T-biLSTMs and
(ii) because a model that uses dependency labels
substantially increases the number of trainable pa-

rameters, relative to the size of our datasets.

4.3 Regression model
To predict the factuality vt for the event referred
to by a word wt, we use the hidden states from the
final layer of the stacked L- or T-biLSTM as the
input to a two-layer regression model.

h
(L)
t = [h

(L,→)
t ;h

(L,←)
t ]

v̂t = V2 g
(
V1h

(L)
t + b1

)
+ b2

where v̂t is passed to a loss function L(v̂t, vt): in
this case, smooth L1 – i.e. Huber loss with δ = 1.
This loss function is effectively a smooth variant
of the hinge loss used by Lee et al. (2015) and
Stanovsky et al. (2017).

We also consider a simple ensemble method,
wherein the hidden states from the final layers of
both the stacked L-biLSTM and the stacked T-
biLSTM are concatenated and passed through the
same two-layer regression model. We refer to this
as the H(ybrid)-biLSTM.2

5 Experiments

Implementation We implement both the L-
biLSTM and T-biLSTM models using pytorch
0.2.0. The L-biLSTM model uses the stock
implementation of the stacked bidirectional lin-
ear chain LSTM found in pytorch, and the T-
biLSTM model uses a custom implementation,
which we make available at decomp.net.

Word embeddings We use the 300-dimensional
GloVe 42B uncased word embeddings (Penning-
ton et al., 2014) with an UNK embedding whose
dimensions are sampled iid from a Uniform[-1,1].
We do not tune these embeddings during training.

Hidden state sizes We set the dimension of the
hidden states h(l,d)t and cell states c

(l,d)
t to 300

for all layers of the stacked L- and stacked T-
biLSTMs – the same size as the input word em-
beddings. This means that the input to the regres-
sion model is 600-dimensional, for the stacked L-
and T-biLSTMs, and 1200-dimensional, for the
stacked H-biLSTM. For the hidden layer of the
regression component, we set the dimension to
half the size of the input hidden state: 300, for

2See Miwa and Bansal 2016; Bowman et al. 2016 for al-
ternative ways of hybridizing linear and tree LSTMs for se-
mantic tasks. We use the current method since it allows us
to make minimal changes to the architectures of each model,
which in turn allows us to assess the two models’ ability to
capture different aspects of factuality.

735



Verb Signature Type Example

know +|+ fact. Jo knew that Bo ate.
manage +|− impl. Jo managed to go.
neglect −|+ impl. Jo neglected to call Bo.
hesitate ◦|+ impl. Jo didn’t hesitate to go.
attempt ◦|− impl. Jo didn’t attempt to go.

Table 2: Implication signature features from Nairn
et al. (2006). As an example, a signature of −|+ indi-
cates negative implication under positive polarity (left
side) and positive implication under negative polarity
(right side); ◦ indicates neither positive nor negative
implication.

the stacked L- and T-biLSTMs, and 600, for the
stacked H-biLSTM.

Bidirectional layers We consider stacked L-, T-
, and H-biLSTMs with either one or two layers. In
preliminary experiments, we found that networks
with three layers badly overfit the training data.

Dependency parses For the T- and H-biLSTMs,
we use the gold dependency parses provided in
EUD1.2 when training and testing on UDS-IH2.
On FactBank, MEANTIME, and UW, we follow
Stanovsky et al. (2017) in using the automatic de-
pendency parses generated by the parser in spaCy
(Honnibal and Johnson, 2015).3

Lexical features Recent work on neural mod-
els in the closely related domain of generic-
ity/habituality prediction suggests that inclusion of
hand-annotated lexical features can improve clas-
sification performance (Becker et al., 2017). To
assess whether similar performance gains can be
obtained here, we experiment with lexical features
for simple factive and implicative verbs (Kiparsky
and Kiparsky, 1970; Karttunen, 1971a). When in
use, these features are concatenated to the net-
work’s input word embeddings so that, in princi-
ple, they may interact with one another and inform
other hidden states in the biLSTM, akin to how
verbal implicatives and factives are observed to in-
fluence the factuality of their complements. The
hidden state size is increased to match the input
embedding size. We consider two types:

Signature features We compute binary features
based on a curated list of 92 simple implicative
and 95 factive verbs including their their type-level
“implication signatures,” as compiled by Nairn
et al. (2006).4 These signatures characterize the

3In rebuilding the Unified Factuality dataset (Stanovsky
et al., 2017), we found that sentence splitting was potentially
sensitive to the version of spaCy used. We used v1.9.0.

4http://web.stanford.edu/group/csli_

implicative or factive behavior of a verb with re-
spect to its complement clause, how this behav-
ior changes (or does not change) under negation,
and how it composes with other such verbs under
nested recursion. We create one indicator feature
for each signature type.

Mined features Using a simplified set of pattern
matching rules over Common Crawl data (Buck
et al., 2014), we follow the insights of Pavlick and
Callison-Burch (2016) – henceforth, PC – and use
corpus mining to automatically score verbs for im-
plicativeness. The insight of PC lies in Karttunen’s
(1971a) observation that “the main sentence con-
taining an implicative predicate and the comple-
ment sentence necessarily agree in tense.”

Accordingly, PC devise a tense agreement score
– effectively, the ratio of times an embedding pred-
icate’s tense matches the tense of the predicate
it embeds – to predict implicativeness in English
verbs. Their scoring method involves the use of
fine-grained POS tags, the Stanford Temporal Tag-
ger (Chang and Manning, 2012), and a number
of heuristic rules, which resulted in a confirma-
tion that tense agreement statistics are predictive
of implicativeness, illustrated in part by observing
a near perfect separation of a list of implicative and
non-implicative verbs from Karttunen (1971a).

dare to 1.00 intend to 0.83
bother to 1.00 want to 0.77
happen to 0.99 decide to 0.75
forget to 0.99 promise to 0.75
manage to 0.97 agree to 0.35
try to 0.96 plan to 0.20
get to 0.90 hope to 0.05
venture to 0.85

Table 3: Implicative (bold) and non-implicative (not
bold) verbs from Karttunen (1971a) are nearly sepa-
rable by our tense agreement scores, replicating the re-
sults of PC.

We replicate this finding by employing a simpli-
fied pattern matching method over 3B sentences
of raw Common Crawl text. We efficiently search
for instances of any pattern of the form: I $VERB
to * $TIME, where $VERB and $TIME are
pre-instantiated variables so their corresponding
tenses are known, and ‘*’ matches any one to three
whitespace-separated tokens at runtime (not pre-
instantiated).5 Our results in Table 3 are a close

lnr/Lexical_Resources
5To instantiate $VERB, we use a list of 1K clause-

embedding verbs compiled by (White and Rawlins, 2016)
as well as the python package pattern-en to conjugate
each verb in past, present progressive, and future tenses; all
conjugations are first-person singular. $TIME is instantiated

736



FactBank UW Meantime UDS-IH2
MAE r MAE r MAE r MAE r

All-3.0 0.8 NAN 0.78 NAN 0.31 NAN 2.255 NAN
Lee et al. 2015 - - 0.511 0.708 - - - -
Stanovsky et al. 2017 0.59 0.71 0.42† 0.66 0.34 0.47 - -
L-biLSTM(2)-S 0.427 0.826 0.508 0.719 0.427 0.335 0.960† 0.768
T-biLSTM(2)-S 0.577 0.752 0.600 0.645 0.428 0.094 1.101 0.704
L-biLSTM(2)-G 0.412 0.812 0.523 0.703 0.409 0.462 - -
T-biLSTM(2)-G 0.455 0.809 0.567 0.688 0.396 0.368 - -
L-biLSTM(2)-S+lexfeats 0.429 0.796 0.495 0.730 0.427 0.322 1.000 0.755
T-biLSTM(2)-S+lexfeats 0.542 0.744 0.567 0.676 0.375 0.242 1.087 0.719
L-biLSTM(2)-MultiSimp 0.353 0.843 0.503 0.725 0.345 0.540 - -
T-biLSTM(2)-MultiSimp 0.482 0.803 0.599 0.645 0.545 0.237 - -
L-biLSTM(2)-MultiBal 0.391 0.821 0.496 0.724 0.278 0.613† - -
T-biLSTM(2)-MultiBal 0.517 0.788 0.573 0.659 0.400 0.405 - -
L-biLSTM(1)-MultiFoc 0.343 0.823 0.516 0.698 0.229† 0.599 - -
L-biLSTM(2)-MultiFoc 0.314 0.846 0.502 0.710 0.305 0.377 - -
T-biLSTM(2)-MultiFoc 1.100 0.234 0.615 0.616 0.395 0.300 - -
L-biLSTM(2)-MultiSimp w/UDS-IH2 0.377 0.828 0.508 0.722 0.367 0.469 0.965 0.771†
T-biLSTM(2)-MultiSimp w/UDS-IH2 0.595 0.716 0.598 0.609 0.467 0.345 1.072 0.723
H-biLSTM(2)-S 0.488 0.775 0.526 0.714 0.442 0.255 0.967 0.768
H-biLSTM(1)-MultiSimp 0.313† 0.857† 0.528 0.704 0.314 0.545 - -
H-biLSTM(2)-MultiSimp 0.431 0.808 0.514 0.723 0.401 0.461 - -
H-biLSTM(2)-MultiBal 0.386 0.825 0.502 0.713 0.352 0.564 - -
H-biLSTM(2)-MultiSimp w/UDS-IH2 0.393 0.820 0.481 0.749† 0.374 0.495 0.969 0.760

Table 4: All 2-layer systems, and 1-layer systems if best in column. State-of-the-art in bold; † is best in column
(with row shaded in purple). Key: L=linear, T=tree, H=hybrid, (1,2)=# layers, S=single-task specific, G=single-
task general, +lexfeats=with all lexical features, MultiSimp=multi-task simple, MultiBal=multi-task balanced,
MultiFoc=multi-task focused, w/UDS-IH2=trained on all data incl. UDS-IH2. All-3.0 is the constant baseline.

replication of PC’s findings. Prior work such as by
PC is motivated in part by the potential for corpus-
linguistic findings to be used as fodder in down-
stream predictive tasks: we include these agree-
ment scores as potential input features to our net-
works to test whether contemporary models do in
fact benefit from this information.

Training For all experiments, we use stochastic
gradient descent to train the LSTM parameters and
regression parameters end-to-end with the Adam
optimizer (Kingma and Ba, 2015), using the de-
fault learning rate in pytorch (1e-3). We con-
sider five training regimes:6

1. SINGLE-TASK SPECIFIC (-S) Train a sepa-
rate instance of the network for each dataset,
training only on that dataset.

2. SINGLE-TASK GENERAL (-G) Train one in-
stance of the network on the simple con-
catenation of all unified factuality datasets,
{FactBank, UW, MEANTIME}.

3. MULTI-TASK SIMPLE (-MULTISIMP) Same

with each of five past tense phrases (“yesterday,” “last week,”
etc.) and five corresponding future tense phrases (“tomor-
row,” “next week,” etc). See Supplement for further details.

6Multi-task can have subtly different meanings in the NLP
community; following terminology from Mou et al. (2016),
our use is best described as “semantically equivalent transfer”
with simultaneous (MULT) network training.

as SINGLE-TASK GENERAL, except the net-
work maintains a distinct set of regression
parameters for each dataset; all other param-
eters (LSTM) remain tied. “w/UDS-IH2” is
specified if UDS-IH2 is included in training.

4. MULTI-TASK BALANCED (-MULTIBAL)
Same as MULTI-TASK SIMPLE but upsam-
pling examples from the smaller datasets to
ensure that examples from those datasets are
seen at the same rate.

5. MULTI-TASK FOCUSED (-MULTIFOC) Same
as MULTI-TASK SIMPLE but upsampling ex-
amples from a particular target dataset to
ensure that examples from that dataset are
seen 50% of the time and examples from
the other datasets are seen 50% (evenly dis-
tributed across the other datasets).

Calibration Post-training, network predictions
are monotonically re-adjusted to a specific dataset
using isotonic regression (fit on train split only).

Evaluation Following Lee et al. (2015) and
Stanovsky et al. (2017), we report two evalua-
tion measures: mean absolute error (MAE) and
Pearson correlation (r). We would like to note,
however, that we believe correlation to be a bet-
ter indicator of performance for two reasons: (i)
for datasets with a high degree of label imbalance

737



Mean Linear Tree
Modal Negated Label MAE MAE #

NONE no 1.00 0.93 1.03 2244
NONE yes -0.19 1.40 1.69 98
may no -0.38 1.00 0.99 14
would no -0.61 0.85 0.99 39
ca(n’t) yes -0.72 1.28 1.55 11
can yes -0.75 0.99 0.86 6
(wi)’ll no -0.94 1.47 1.14 8
could no -1.03 0.97 1.32 20
can no -1.25 1.02 1.21 73
might no -1.25 0.66 1.06 6
would yes -1.27 0.40 0.86 5
should no -1.31 1.20 1.01 22
will no -1.88 0.75 0.86 75

Table 5: Mean gold labels, counts, and MAE for L-
biLSTM(2)-S and T-biLSTM(2)-S model predictions
on UDS-IH2-dev, grouped by modals and negation.

(Figure 2), a baseline that always guesses the mean
or mode label can be difficult to beat in terms of
MAE but not correlation, and (ii) MAE is harder
to meaningfully compare across datasets with dif-
ferent label mean and variance.

Development Under all regimes, we train the
model for 20 epochs – by which time all models
appear to converge. We save the parameter values
after the completion of each epoch and then score
each set of saved parameter values on the devel-
opment set for each dataset. The set of parameter
values that performed best on dev in terms of Pear-
son correlation for a particular dataset were then
used to score the test set for that dataset.

6 Results

Table 4 reports the results for all of the 2-layer L-,
T-, and H-biLSTMs.7 The best-performing sys-
tem for each dataset and metric are highlighted in
purple, and when the best-performing system for a
particular dataset was a 1-layer model, that system
is included in Table 4.

New state of the art For each dataset and met-
ric, with the exception of MAE on UW, we achieve
state of the art results with multiple systems. The
highest-performing system for each is reported in
Table 4. Our results on UDS-IH2 are the first re-
ported numbers for this new factuality resource.

Linear v. tree topology On its own, the biL-
STM with linear topology (L-biLSTM) performs
consistently better than the biLSTM with tree

7Full results are reported in the Supplementary Materials.
Note that the 2-layer networks do not strictly dominate the
1-layer networks in terms of MAE and correlation.

Mean
Relation Label L-biLSTM T-biLSTM #

root 1.07 1.03 0.96 949
conj 0.37 0.44 0.46 316
advcl 0.46 0.53 0.45 303
xcomp -0.42 -0.57 -0.49 234
acl:relcl 1.28 1.40 1.31 193
ccomp 0.11 0.31 0.34 191
acl 0.77 0.59 0.58 159
parataxis 0.44 0.63 0.79 127
amod 1.92 1.88 1.81 76
csubj 0.36 0.38 0.27 37

Table 6: Mean predictions for linear (L-biLSTM-S(2))
and tree models (T-biLSTM-S(2)) on UDS-IH2-dev,
grouped by governing dependency relation. Only the
10 most frequent governing dependency relations in
UDS-IH2-dev are shown.

topology (T-biLSTM). However, the hybrid topol-
ogy (H-biLSTM), consisting of both a L- and T-
biLSTM is the top-performing system on UW for
correlation (Table 4). This suggests that the T-
biLSTM may be contributing something comple-
mentary to the L-biLSTM.

Evidence of this complementarity can be seen
in Table 6, which contains a breakdown of system
performance by governing dependency relation,
for both linear and tree models, on UDS-IH2-dev.
In most cases, the L-biLSTM’s mean prediction is
closer to the true mean. This appears to arise in
part because the T-biLSTM is less confident in its
predictions – i.e. its mean prediction tends to be
closer to 0. This results in the L-biLSTM being too
confident in certain cases – e.g. in the case of the
xcomp governing relation, where the T-biLSTM
mean prediction is closer to the true mean.

Lexical features have minimal impact Adding
all lexical features (both SIGNATURE and MINED)
yields mixed results. We see slight improve-
ments on UW, while performance on the other
datasets mostly declines (compare with SINGLE-
TASK SPECIFIC). Factuality prediction is precisely
the kind of NLP task one would expect these types
of features to assist with, so it is notable that, in
our experiments, they do not.

Multi-task helps Though our methods achieve
state of the art in the single-task setting, the best
performing systems are mostly multi-task (Table
4 and Supplementary Materials). This is an ideal
setting for multi-task training: each dataset is
relatively small, and their labels capture closely-
related (if not identical) linguistic phenomena.
UDS-IH2, the largest by a factor of two, reaps the
smallest gains from multi-task.

738



Attribute #

Grammatical error present, incl. run-ons 16
Is an auxiliary or light verb 14
Annotation is incorrect 13
Future event 12
Is a question 5
Is an imperative 3
Is not an event or state 2
One or more of the above 43

Table 7: Notable attributes of 50 instances from UDS-
IH2-dev with highest absolute prediction error (using
H-biLSTM(2)-MultiSim w/UDS-IH2).

7 Analysis

As discussed in §2, many discrete linguistic phe-
nomena interact with event factuality. Here we
provide a brief analysis of some of those inter-
actions, both as they manifest in the UDS-IH2
dataset, as well as in the behavior of our models.
This analysis employs the gold dependency parses
present in EUD1.2.

Table 5 illustrates the influence of modals and
negation on the factuality of the events they have
direct scope over. The context with the high-
est factuality on average is no direct modal and
no negation (first row); all other modal contexts
have varying degrees of negative mean factuality
scores, with will as the most negative. This is
likely a result of UDS-IH2 annotation instructions
to mark future events as not having happened.

Table 7 shows results from a manual error anal-
ysis on 50 events from UDS-IH2-dev with high-
est absolute prediction error (using H-biLSTM(2)-
MultiSim w/UDS-IH2). Grammatical errors (such
as run-on sentences) in the underlying text of
UDS-IH2 appear to pose a particular challenge for
these models; informal language and grammatical
errors in UDS-IH2 is a substantial distinction from
the other factuality datasets used here.

manage to 2.78 agree to -1.00
happen to 2.34 forget to -1.18
dare to 1.50 want to -1.48
bother to 1.50 intend to -2.02
decide to 0.10 promise to -2.34
get to -0.23 plan to -2.42
try to -0.24 hope to -2.49

Table 8: UDS-IH2-train: Infinitival-taking verbs sorted
by the mean annotation scores of their complements
(xcomp), with direct negation filtered out. Implica-
tives are in bold.

In §6 we observe that the linguistically-motivated
lexical features that we test (+lexfeats) do not have
a big impact on overall performance. Tables 8 and
9 help nuance this observation.

Verb L-biLSTM(2)-S +lexfeats #

decide to 3.28 2.66 2
forget to 0.67 0.48 2
get to 1.55 1.43 9
hope to 1.35 1.23 5
intend to 1.18 0.61 1
promise to 0.40 0.49 1
try to 1.14 1.42 12
want to 1.22 1.17 24

Table 9: MAE of L-biLSTM(2)-S and L-biLSTM(2)-
S+lexfeats, for predictions on events in UDS-IH2-dev
that are xcomp-governed by an infinitival-taking verb.

Table 8 shows that we can achieve simi-
lar separation between implicatives and non-
implicatives as the feature mining strategy pre-
sented in §5. That is, those features may be re-
dundant with information already learnable from
factuality datasets (UDS-IH2). Despite the un-
derperformance of these features overall, Table 9
shows that they may still improve performance in
the subset of instances where they appear.

8 Conclusion

We have proposed two neural models of event
factuality prediction – a bidirectional linear-chain
LSTM (L-biLSTM) and a bidirectional child-
sum dependency tree LSTM (T-biLSTM) – which
yield substantial gains over previous models based
on deterministic rules and hand-engineered fea-
tures. We found that both models yield such
gains, though the L-biLSTM outperforms the T-
biLSTM; for some datasets, an ensemble of the
two (H-biLSTM) improves over either alone.

We have also extended the UDS-IH1 dataset,
yielding the largest publicly-available factuality
dataset to date: UDS-IH2. In experiments, we see
substantial gains from multi-task training over the
three factuality datasets unified by Stanovsky et al.
(2017), as well as UDS-IH2. Future work will fur-
ther probe the behavior of these models, or extend
them to learn other aspects of event semantics.

Acknowledgments

This research was supported by the JHU HLT-
COE, DARPA LORELEI, DARPA AIDA, and
NSF-GRFP (1232825). The U.S. Government is
authorized to reproduce and distribute reprints for
Governmental purposes. The views and conclu-
sions contained in this publication are those of the
authors and should not be interpreted as represent-
ing official policies or endorsements of DARPA or
the U.S. Government.

739



References

Maria Becker, Michael Staniek, Vivi Nastase, Alexis
Palmer, and Anette Frank. 2017. Classifying Se-
mantic Clause Types: Modeling Context and Genre
Characteristics with Recurrent Neural Networks and
Attention. In Proceedings of the 6th Joint Con-
ference on Lexical and Computational Semantics
(*SEM 2017). pages 230–240.

Samuel R Bowman, Jon Gauthier, Abhinav Ras-
togi, Raghav Gupta, Christopher D Manning, and
Christopher Potts. 2016. A fast unified model for
parsing and sentence understanding. In Proceedings
of the 54th Annual Meeting of the Association for
Computational Linguistics. Association for Compu-
tational Linguistics, Berlin, Germany, pages 1466–
1477.

Samuel R. Bowman, Christopher Potts, and Christo-
pher D. Manning. 2015. Learning distributed word
representations for natural logic reasoning. In Pro-
ceedings of the AAAI Spring Symposium on Knowl-
edge Representation and Reasoning.

Christian Buck, Kenneth Heafield, and Bas van Ooyen.
2014. N-gram Counts and Language Models from
the Common Crawl. In Proceedings of the Ninth In-
ternational Conference on Language Resources and
Evaluation (LREC-2014). European Language Re-
sources Association (ELRA), Reykjavik, Iceland.

Angel X. Chang and Christopher Manning. 2012. Su-
time: A library for recognizing and normalizing
time expressions. In Nicoletta Calzolari (Con-
ference Chair), Khalid Choukri, Thierry Declerck,
Mehmet Uğur Doğan, Bente Maegaard, Joseph Mar-
iani, Asuncion Moreno, Jan Odijk, and Stelios
Piperidis, editors, Proceedings of the Eight Interna-
tional Conference on Language Resources and Eval-
uation (LREC’12). European Language Resources
Association (ELRA), Istanbul, Turkey.

Marie-Catherine de Marneffe, Christopher D. Man-
ning, and Christopher Potts. 2012. Did it happen?
The pragmatic complexity of veridicality assess-
ment. Computational Linguistics 38(2):301–333.

Mona T Diab, Lori Levin, Teruko Mitamura, Owen
Rambow, Vinodkumar Prabhakaran, and Weiwei
Guo. 2009. Committed belief annotation and tag-
ging. In Proceedings of the Third Linguistic An-
notation Workshop. Association for Computational
Linguistics, pages 68–73.

Paul Egré. 2008. Question-embedding and factivity.
Grazer Philosophische Studien 77(1):85–125.

Alex Graves, Navdeep Jaitly, and Abdel-rahman Mo-
hamed. 2013. Hybrid speech recognition with deep
bidirectional LSTM. In Automatic Speech Recogni-
tion and Understanding (ASRU), 2013 IEEE Work-
shop on. IEEE, pages 273–278.

Jaakko Hintikka. 1975. Different Constructions in
Terms of the Basic Epistemological Verbs: A Sur-
vey of Some Problems and Proposals. In The In-
tentions of Intentionality and Other New Models for
Modalities, Dordrecht: D. Reidel, pages 1–25.

Sepp Hochreiter and Jürgen Schmidhuber. 1997.
Long short-term memory. Neural Computation
9(8):1735–1780.

Matthew Honnibal and Mark Johnson. 2015. An Im-
proved Non-monotonic Transition System for De-
pendency Parsing. In Proceedings of the 2015 Con-
ference on Empirical Methods in Natural Language
Processing. Association for Computational Linguis-
tics, Lisbon, Portugal, pages 1373–1378.

Mohit Iyyer, Jordan Boyd-Graber, Leonardo Claudino,
Richard Socher, and Hal Daumé III. 2014. A neural
network for factoid question answering over para-
graphs. In Proceedings of the 2014 Conference on
Empirical Methods in Natural Language Processing
(EMNLP). pages 633–644.

Lauri Karttunen. 1971a. Implicative verbs. Language
pages 340–358.

Lauri Karttunen. 1971b. Some observations on factiv-
ity. Papers in Linguistics 4(1):55–69.

Lauri Karttunen. 2012. Simple and phrasal implica-
tives. In Proceedings of the First Joint Conference
on Lexical and Computational Semantics. Associa-
tion for Computational Linguistics, pages 124–131.

Lauri Karttunen. 2013. You will be lucky to break
even. In Tracy Holloway King and Valeria dePaiva,
editors, From Quirky Case to Representing Space:
Papers in Honor of Annie Zaenen, pages 167–180.

Lauri Karttunen, Stanley Peters, Annie Zaenen, and
Cleo Condoravdi. 2014. The Chameleon-like Na-
ture of Evaluative Adjectives. In Christopher Piñón,
editor, Empirical Issues in Syntax and Semantics 10.
CSSP-CNRS, pages 233–250.

Diederik Kingma and Jimmy Ba. 2015. Adam: A
Method for Stochastic Optimization. In Proceed-
ings of the 3rd International Conference on Learn-
ing Representations (ICLR). San Diego, CA, USA.

Paul Kiparsky and Carol Kiparsky. 1970. Fact. In
Manfred Bierwisch and Karl Erich Heidolph, edi-
tors, Progress in Linguistics: A collection of papers,
Mouton, The Hague, pages 143–173.

Kenton Lee, Yoav Artzi, Yejin Choi, and Luke Zettle-
moyer. 2015. Event Detection and Factuality As-
sessment with Non-Expert Supervision. In Proceed-
ings of the 2015 Conference on Empirical Methods
in Natural Language Processing. Association for
Computational Linguistics, Lisbon, Portugal, pages
1643–1648.

740



Amnon Lotan, Asher Stern, and Ido Dagan. 2013.
TruthTeller: Annotating Predicate Truth. In Pro-
ceedings of the 2013 Conference of the North Amer-
ican Chapter of the Association for Computational
Linguistics: Human Language Technologies. As-
sociation for Computational Linguistics, Atlanta,
Georgia, pages 752–757.

Anne-Lyse Minard, Manuela Speranza, Ruben Urizar,
Begoña Altuna, Marieke van Erp, Anneleen Schoen,
and Chantal van Son. 2016. MEANTIME, the
NewsReader Multilingual Event and Time Corpus.
In Nicoletta Calzolari (Conference Chair), Khalid
Choukri, Thierry Declerck, Sara Goggi, Marko Gro-
belnik, Bente Maegaard, Joseph Mariani, Helene
Mazo, Asuncion Moreno, Jan Odijk, and Stelios
Piperidis, editors, Proceedings of the Tenth Interna-
tional Conference on Language Resources and Eval-
uation (LREC 2016). European Language Resources
Association (ELRA), Paris, France, pages 23–28.

Makoto Miwa and Mohit Bansal. 2016. End-to-End
Relation Extraction using LSTMs on Sequences and
Tree Structures. In Proceedings of the 54th Annual
Meeting of the Association for Computational Lin-
guistics. Association for Computational Linguistics,
Berlin, Germany, pages 1105–1116.

Lili Mou, Zhao Meng, Rui Yan, Ge Li, Yan Xu,
Lu Zhang, and Zhi Jin. 2016. How transferable are
neural networks in nlp applications? In Proceed-
ings of the 2016 Conference on Empirical Meth-
ods in Natural Language Processing. Association
for Computational Linguistics, Austin, Texas, pages
479–489.

Rowan Nairn, Cleo Condoravdi, and Lauri Karttunen.
2006. Computing relative polarity for textual in-
ference. In Proceedings of the Fifth International
Workshop on Inference in Computational Semantics
(ICoS-5). Association for Computational Linguis-
tics, Buxton, England, pages 20–21.

Joakim Nivre, Zeljko Agic, Maria Jesus Aranzabe,
Masayuki Asahara, Aitziber Atutxa, Miguel Balles-
teros, John Bauer, Kepa Bengoetxea, Riyaz Ah-
mad Bhat, Cristina Bosco, Sam Bowman, Giuseppe
G. A. Celano, Miriam Connor, Marie-Catherine
de Marneffe, Arantza Diaz de Ilarraza, Kaja Do-
brovoljc, Timothy Dozat, Tomaž Erjavec, Richárd
Farkas, Jennifer Foster, Daniel Galbraith, Filip
Ginter, Iakes Goenaga, Koldo Gojenola, Yoav
Goldberg, Berta Gonzales, Bruno Guillaume, Jan
Hajič, Dag Haug, Radu Ion, Elena Irimia, An-
ders Johannsen, Hiroshi Kanayama, Jenna Kan-
erva, Simon Krek, Veronika Laippala, Alessan-
dro Lenci, Nikola Ljubešić, Teresa Lynn, Christo-
pher Manning, Cătălina Mărănduc, David Mareček,
Héctor Martı́nez Alonso, Jan Mašek, Yuji Mat-
sumoto, Ryan McDonald, Anna Missilä, Verginica
Mititelu, Yusuke Miyao, Simonetta Montemagni,
Shunsuke Mori, Hanna Nurmi, Petya Osenova, Lilja
Øvrelid, Elena Pascual, Marco Passarotti, Cenel-
Augusto Perez, Slav Petrov, Jussi Piitulainen, Bar-
bara Plank, Martin Popel, Prokopis Prokopidis,

Sampo Pyysalo, Loganathan Ramasamy, Rudolf
Rosa, Shadi Saleh, Sebastian Schuster, Wolfgang
Seeker, Mojgan Seraji, Natalia Silveira, Maria Simi,
Radu Simionescu, Katalin Simkó, Kiril Simov,
Aaron Smith, Jan Štěpánek, Alane Suhr, Zsolt
Szántó, Takaaki Tanaka, Reut Tsarfaty, Sumire
Uematsu, Larraitz Uria, Viktor Varga, Veronika
Vincze, Zdeněk Žabokrtský, Daniel Zeman, and
Hanzhi Zhu. 2015. Universal Dependencies 1.2.
http://universaldependencies.github.io/docs/ .

Ellie Pavlick and Chris Callison-Burch. 2016. Tense
Manages to Predict Implicative Behavior in Verbs.
In Proceedings of the 2016 Conference on Empirical
Methods in Natural Language Processing. Associa-
tion for Computational Linguistics, Austin, Texas,
pages 2225–2229.

Jeffrey Pennington, Richard Socher, and Christo-
pher D. Manning. 2014. Glove: Global Vectors for
Word Representation. In Proceedings of the 2014
Conference on Empirical Methods in Natural Lan-
guage Processing (EMNLP). Association for Com-
putational Linguistics, Doha, Qatar, pages 1532–
1543.

Vinodkumar Prabhakaran, Owen Rambow, and Mona
Diab. 2010. Automatic committed belief tagging.
In Proceedings of the 23rd International Conference
on Computational Linguistics: Posters. Association
for Computational Linguistics, pages 1014–1022.

Rashmi Prasad, Nikhil Dinesh, Alan Lee, Eleni Milt-
sakaki, Livio Robaldo, Aravind Joshi, and Bon-
nie Webber. 2008. The Penn Discourse Tree-
bank 2.0. In Proceedings of the 6th International
Conference on Language Resources and Evaluation
(LREC). European Language Resources Association
(ELRA), Marrakech, Morocco.

James Pustejovsky, Marc Verhagen, Roser Saurı́, Jes-
sica Littman, Robert Gaizauskas, Graham Katz, In-
derjeet Mani, Robert Knippen, and Andrea Setzer.
2006. TimeBank 1.2. Linguistic Data Consortium
40.

Roser Saurı́ and James Pustejovsky. 2009. FactBank:
a corpus annotated with event factuality. Language
Resources and Evaluation 43(3):227.

Roser Saurı́ and James Pustejovsky. 2012. Are you
sure that this happened? assessing the factuality de-
gree of events in text. Computational Linguistics
38(2):261–299.

Richard Socher, Andrej Karpathy, Quoc V Le, Christo-
pher D Manning, and Andrew Y Ng. 2014.
Grounded compositional semantics for finding and
describing images with sentences. Transactions
of the Association of Computational Linguistics
2(1):207–218.

Sandeep Soni, Tanushree Mitra, Eric Gilbert, and Ja-
cob Eisenstein. 2014. Modeling Factuality Judg-
ments in Social Media Text. In Proceedings of the

741



52nd Annual Meeting of the Association for Compu-
tational Linguistics (Volume 2: Short Papers). As-
sociation for Computational Linguistics, Baltimore,
Maryland, pages 415–420.

Benjamin Spector and Paul Egré. 2015. A uniform
semantics for embedded interrogatives: An answer,
not necessarily the answer. Synthese 192(6):1729–
1784.

Gabriel Stanovsky, Judith Eckle-Kohler, Yevgeniy
Puzikov, Ido Dagan, and Iryna Gurevych. 2017. In-
tegrating Deep Linguistic Features in Factuality Pre-
diction over Unified Datasets. In Proceedings of the
55th Annual Meeting of the Association for Compu-
tational Linguistics (Volume 2: Short Papers). As-
sociation for Computational Linguistics, pages 352–
357.

Ilya Sutskever, Oriol Vinyals, and Quoc V Le. 2014.
Sequence to sequence learning with neural net-
works. In Advances in Neural Information Process-
ing Systems. pages 3104–3112.

Kai Sheng Tai, Richard Socher, and Christopher D
Manning. 2015. Improved semantic representations
from tree-structured long short-term memory net-
works. In Proceedings of the 53rd Annual Meet-
ing of the Association for Computational Linguistics
and the 7th International Joint Conference on Natu-
ral Language Processing. Association for Compu-
tational Linguistics, Beijing, China, pages 1556–
1566.

Naushad UzZaman, Hector Llorens, James Allen, Leon
Derczynski, Marc Verhagen, and James Pustejovsky.
2013. SemEval-2013 Task 1: TempEval-3: Evaluat-
ing Time Expressions, Events, and Temporal Rela-
tions. In Second Joint Conference on Lexical and
Computational Semantics (*SEM), Volume 2: Sev-
enth International Workshop on Semantic Evalua-
tion (SemEval 2013). Association for Computational
Linguistics, Atlanta, Georgia, pages 1–9.

Aaron Steven White. 2014. Factive-implicatives and
modalized complements. In Jyoti Iyer and Leland
Kusmer, editors, Proceedings of the 44th annual
meeting of the North East Linguistic Society. Uni-
versity of Connecticut, pages 267–278.

Aaron Steven White and Kyle Rawlins. 2016. A com-
putational model of S-selection. Semantics and Lin-
guistic Theory 26:641–663.

Aaron Steven White, Drew Reisinger, Keisuke Sak-
aguchi, Tim Vieira, Sheng Zhang, Rachel Rudinger,
Kyle Rawlins, and Benjamin Van Durme. 2016.
Universal decompositional semantics on universal
dependencies. In Proceedings of the 2016 Confer-
ence on Empirical Methods in Natural Language
Processing. Association for Computational Linguis-
tics, Austin, TX, pages 1713–1723.

Janyce Wiebe and Ellen Riloff. 2005. Creating Subjec-
tive and Objective Sentence Classifiers from Unan-
notated Texts. In Proceedings of the 6th Interna-
tional Conference on Computational Linguistics and

Intelligent Text Processing (CICLing-05). Springer-
Verlag, Mexico City, Mexico, pages 486–497.

Wojciech Zaremba and Ilya Sutskever. 2014. Learning
to execute. arXiv preprint arXiv:1410.4615 .

742



A Appendix

A.1 Dataset filtering
We filter our dataset to remove annotators with
very low agreement in two ways: (i) based on the
their agreement with other annotators on the HAP-
PENED question; and (ii) based on the their agree-
ment with other annotators on the CONFIDENCE
question.

For the HAPPENED question, we computed, for
each pair of annotators and each item that both of
those annotators annotated, whether the two re-
sponses were equal. We then fit a random ef-
fects logistic regression to response equality with
random intercepts for annotator. The Best Lin-
ear Unbiased Predictors (BLUPs) for each anno-
tator were then extracted and z-scored. Annota-
tors were removed if their z-scored BLUP was less
than -2.

For the CONFIDENCE question, we first ridit-
scored the ratings by annotator; and for each pair
of annotators and each item that both of those an-
notators annotated, we computed the difference
between the two ridit-scored confidences. We then
fit a random effects linear regression to the result-
ing difference after logit-transformation with ran-
dom intercepts for annotator. The same BLUP-
based exclusion procedure was then used.

This filtering results in the exclusion of one an-
notator, who is excluded for low agreement on
HAPPENED. 4,179 annotations are removed in the
filtering, but because we remove only a single an-
notator, there remains at least one annotation for
every predicate.

A.2 Mining Implicatives
All options for instantiating the $TIME pattern
variable, described in §5, are listed here.

• Past Tense Phrases: earlier today, yesterday,
last week, last month, last year

• Future Tense Phrases: later today, tomorrow,
next week, next month, next year

A.3 Full Results
Table 10 presents the full set of results, including
all 1-layer and 2-layer models, and performance
on development splits.

743



Fa
ct

B
an

k
U

W
M

ea
nt

im
e

U
D

de
v

te
st

de
v

te
st

de
v

te
st

de
v

te
st

M
A

E
r

M
A

E
r

M
A

E
r

M
A

E
r

M
A

E
r

M
A

E
r

M
A

E
r

M
A

E
r

A
ll-

3.
0

-
-

0.
8

0
-

-
0.

78
0

-
-

0.
31

0
-

-
2.

25
5

0
L

ee
et

al
.2

01
5

-
-

-
-

0.
46

2
0.

74
9

0.
51

1
0.

70
8

-
-

-
-

-
-

-
-

St
an

ov
sk

y
et

al
.2

01
7

-
-

0.
59

0.
71

-
-

0.
42

0.
66

-
-

0.
31

0.
47

-
-

-
-

L
-b

iL
ST

M
(1

)-
S

0.
41

1
0.

78
0.

39
9

0.
81

6
0.

43
5

0.
79

7
0.

50
8

0.
71

8
0.

23
9

0.
63

1
0.

33
7

0.
35

9
0.

97
8

0.
76

8
0.

98
6

0.
76

5
L

-b
iL

ST
M

(2
)-

S
0.

48
2

0.
77

2
0.

42
7

0.
82

6
0.

42
6

0.
79

9
0.

50
8

0.
71

9
0.

35
7

0.
60

1
0.

42
7

0.
33

5
0.

95
0.

77
1

0.
96

0.
76

8
T-

bi
L

ST
M

(1
)-

S
0.

56
4

0.
71

1
0.

48
0.

74
8

0.
49

1
0.

73
4

0.
57

4
0.

65
2

0.
29

7
0.

56
4

0.
36

0.
15

5
1.

04
3

0.
73

9
1.

05
3

0.
72

5
T-

bi
L

ST
M

(2
)-

S
0.

59
5

0.
71

0.
57

7
0.

75
2

0.
49

7
0.

73
5

0.
6

0.
64

5
0.

35
1

0.
37

1
0.

42
8

0.
09

4
1.

06
0.

71
9

1.
10

1
0.

70
4

L
-b

iL
ST

M
(1

)-
G

0.
43

2
0.

79
8

0.
38

3
0.

81
9

0.
42

6
0.

80
7

0.
51

7
0.

71
7

0.
25

2
0.

62
5

0.
34

3
0.

50
5

-
-

-
-

L
-b

iL
ST

M
(2

)-
G

0.
43

9
0.

79
9

0.
41

2
0.

81
2

0.
42

0.
80

9
0.

52
3

0.
70

3
0.

29
1

0.
60

4
0.

40
9

0.
46

2
-

-
-

-
T-

bi
L

ST
M

(1
)-

G
0.

46
8

0.
75

8
0.

40
5

0.
82

0.
47

2
0.

76
0.

57
1

0.
66

2
0.

33
6

0.
50

9
0.

40
8

0.
38

4
-

-
-

-
T-

bi
L

ST
M

(2
)-

G
0.

49
8

0.
76

4
0.

45
5

0.
80

9
0.

48
1

0.
75

7
0.

56
7

0.
68

8
0.

29
8

0.
52

7
0.

39
6

0.
36

8
-

-
-

-
L

-b
iL

ST
M

(1
)-

S+
le

xf
ea

ts
:s

ig
n

0.
42

3
0.

78
0.

39
6

0.
80

5
-

-
-

-
-

-
-

-
-

-
-

-
L

-b
iL

ST
M

(2
)-

S+
le

xf
ea

ts
:s

ig
n

0.
45

9
0.

76
8

0.
42

3
0.

82
-

-
-

-
-

-
-

-
-

-
-

-
T-

bi
L

ST
M

(1
)-

S+
le

xf
ea

ts
:s

ig
n

0.
54

0.
71

8
0.

51
0.

76
2

-
-

-
-

-
-

-
-

-
-

-
-

T-
bi

L
ST

M
(2

)-
S+

le
xf

ea
ts

:s
ig

n
0.

55
2

0.
73

1
0.

55
8

0.
74

8
-

-
-

-
-

-
-

-
-

-
-

-
L

-b
iL

ST
M

(1
)-

S+
le

xf
ea

ts
:m

in
e

0.
46

8
0.

78
1

0.
45

3
0.

80
1

-
-

-
-

-
-

-
-

-
-

-
-

L
-b

iL
ST

M
(2

)-
S+

le
xf

ea
ts

:m
in

e
0.

41
6

0.
76

8
0.

37
3

0.
80

8
-

-
-

-
-

-
-

-
-

-
-

-
T-

bi
L

ST
M

(1
)-

S+
le

xf
ea

ts
:m

in
e

0.
54

6
0.

72
5

0.
52

5
0.

75
1

-
-

-
-

-
-

-
-

-
-

-
-

T-
bi

L
ST

M
(2

)-
S+

le
xf

ea
ts

:m
in

e
0.

56
7

0.
72

7
0.

57
3

0.
72

-
-

-
-

-
-

-
-

-
-

-
-

L
-b

iL
ST

M
(1

)-
S+

le
xf

ea
ts

:b
ot

h
0.

44
3

0.
78

1
0.

41
3

0.
80

5
0.

42
8

0.
80

3
0.

50
7

0.
72

2
0.

31
9

0.
48

1
0.

37
3

0.
36

9
1.

00
2

0.
76

2
1.

00
2

0.
76

6
L

-b
iL

ST
M

(2
)-

S+
le

xf
ea

ts
:b

ot
h

0.
48

5
0.

76
4

0.
42

9
0.

79
6

0.
43

3
0.

79
2

0.
49

5
0.

73
0.

35
6

0.
66

2
0.

42
7

0.
32

2
0.

97
7

0.
76

7
1

0.
75

5
T-

bi
L

ST
M

(1
)-

S+
le

xf
ea

ts
:b

ot
h

0.
50

3
0.

72
8

0.
44

9
0.

79
3

0.
48

5
0.

74
3

0.
58

9
0.

64
3

0.
28

2
0.

49
3

0.
34

8
0.

19
1

1.
04

0.
73

8
1.

07
3

0.
71

8
T-

bi
L

ST
M

(2
)-

S+
le

xf
ea

ts
:b

ot
h

0.
56

5
0.

72
4

0.
54

2
0.

74
4

0.
48

1
0.

74
7

0.
56

7
0.

67
6

0.
35

2
0.

40
4

0.
37

5
0.

24
2

1.
04

9
0.

73
8

1.
08

7
0.

71
9

L
-b

iL
ST

M
(1

)-
M

ul
tiS

im
p

0.
40

8
0.

80
4

0.
36

5
0.

83
4

0.
41

4
0.

82
5

0.
50

6
0.

73
6

0.
24

1
0.

50
6

0.
28

6
0.

45
3

-
-

-
-

L
-b

iL
ST

M
(2

)-
M

ul
tiS

im
p

0.
39

3
0.

81
1

0.
35

3
0.

84
3

0.
41

7
0.

81
7

0.
50

3
0.

72
5

0.
31

4
0.

56
0.

34
5

0.
54

-
-

-
-

T-
bi

L
ST

M
(1

)-
M

ul
tiS

im
p

0.
46

4
0.

75
6

0.
40

8
0.

80
7

0.
47

2
0.

75
4

0.
55

5
0.

67
0.

24
8

0.
54

6
0.

31
8

0.
35

7
-

-
-

-
T-

bi
L

ST
M

(2
)-

M
ul

tiS
im

p
0.

51
7

0.
75

3
0.

48
2

0.
80

3
0.

49
3

0.
75

4
0.

59
9

0.
64

5
0.

47
4

0.
52

0.
54

5
0.

23
7

-
-

-
-

L
-b

iL
ST

M
(1

)-
M

ul
tiB

al
0.

38
7

0.
80

5
0.

33
2

0.
84

1
0.

41
2

0.
82

2
0.

52
0.

72
2

0.
23

2
0.

57
0.

25
6

0.
54

4
-

-
-

-
L

-b
iL

ST
M

(2
)-

M
ul

tiB
al

0.
44

1
0.

8
0.

39
1

0.
82

1
0.

41
4

0.
81

5
0.

49
6

0.
72

4
0.

25
1

0.
62

4
0.

27
8

0.
61

3
-

-
-

-
T-

bi
L

ST
M

(1
)-

M
ul

tiB
al

0.
47

5
0.

74
6

0.
40

5
0.

81
7

0.
47

2
0.

75
2

0.
57

8
0.

62
9

0.
23

7
0.

56
0.

34
4

0.
26

6
-

-
-

-
T-

bi
L

ST
M

(2
)-

M
ul

tiB
al

0.
56

0.
73

0.
51

7
0.

78
8

0.
49

9
0.

73
4

0.
57

3
0.

65
9

0.
25

2
0.

56
7

0.
4

0.
40

5
-

-
-

-
L

-b
iL

ST
M

(1
)-

M
ul

tiF
oc

0.
37

8
0.

79
0.

34
3

0.
82

3
0.

41
4

0.
81

3
0.

51
6

0.
69

8
0.

25
6

0.
48

0.
22

9
0.

59
9

-
-

-
-

L
-b

iL
ST

M
(2

)-
M

ul
tiF

oc
0.

37
9

0.
80

8
0.

31
4

0.
84

6
0.

40
9

0.
81

0.
50

2
0.

71
0.

22
7

0.
52

4
0.

30
5

0.
37

7
-

-
-

-
T-

bi
L

ST
M

(1
)-

M
ul

tiF
oc

0.
46

9
0.

74
8

0.
40

1
0.

81
0.

47
4

0.
75

4
0.

57
9

0.
65

4
0.

29
0.

53
3

0.
35

4
0.

29
3

-
-

-
-

T-
bi

L
ST

M
(2

)-
M

ul
tiF

oc
1.

09
1

0.
23

1
1.

1
0.

23
4

0.
50

8
0.

73
1

0.
61

5
0.

61
6

0.
29

3
0.

45
6

0.
39

5
0.

3
-

-
-

-
L

-b
iL

ST
M

(1
)-

M
ul

tiS
im

p
w

/U
D

S-
IH

2
0.

41
7

0.
80

2
0.

38
1

0.
81

3
0.

42
1

0.
80

2
0.

48
6

0.
74

1
0.

38
5

0.
51

0.
35

3
0.

56
5

0.
97

2
0.

77
1

0.
97

7
0.

76
5

L
-b

iL
ST

M
(2

)-
M

ul
tiS

im
p

w
/U

D
S-

IH
2

0.
43

9
0.

79
4

0.
37

7
0.

82
8

0.
41

8
0.

80
3

0.
50

8
0.

72
2

0.
30

5
0.

54
1

0.
36

7
0.

46
9

0.
95

9
0.

77
4

0.
96

5
0.

77
1

T-
bi

L
ST

M
(1

)-
M

ul
tiS

im
p

w
/U

D
S-

IH
2

0.
53

5
0.

73
2

0.
49

8
0.

77
8

0.
49

2
0.

74
6

0.
61

1
0.

61
0.

37
7

0.
44

0.
41

3
0.

39
5

1.
06

1
0.

73
5

1.
06

9
0.

72
8

T-
bi

L
ST

M
(2

)-
M

ul
tiS

im
p

w
/U

D
S-

IH
2

0.
59

7
0.

71
7

0.
59

5
0.

71
6

0.
52

6
0.

70
6

0.
59

8
0.

60
9

0.
42

7
0.

47
1

0.
46

7
0.

34
5

1.
04

8
0.

73
6

1.
07

2
0.

72
3

H
-b

iL
ST

M
(1

)-
S

0.
42

0.
78

9
0.

37
8

0.
83

1
0.

42
7

0.
80

4
0.

51
8

0.
70

4
0.

34
9

0.
43

7
0.

40
5

0.
08

5
0.

98
9

0.
76

7
0.

99
2

0.
76

5
H

-b
iL

ST
M

(2
)-

S
0.

50
5

0.
73

9
0.

48
8

0.
77

5
0.

46
7

0.
76

5
0.

52
6

0.
71

4
0.

35
2

0.
59

5
0.

44
2

0.
25

5
0.

94
8

0.
77

5
0.

96
7

0.
76

8
H

-b
iL

ST
M

(1
)-

M
ul

tiS
im

p
0.

39
5

0.
80

2
0.

31
3

0.
85

7
0.

41
7

0.
82

1
0.

52
8

0.
70

4
0.

26
7

0.
60

1
0.

31
4

0.
54

5
-

-
-

-
H

-b
iL

ST
M

(2
)-

M
ul

tiS
im

p
0.

47
2

0.
77

0.
43

1
0.

80
8

0.
43

1
0.

79
2

0.
51

4
0.

72
3

0.
35

9
0.

54
7

0.
40

1
0.

46
1

-
-

-
-

H
-b

iL
ST

M
(1

)-
M

ul
tiB

al
0.

39
8

0.
80

3
0.

33
4

0.
85

3
0.

40
2

0.
82

9
0.

49
7

0.
73

3
0.

22
9

0.
59

0.
26

4
0.

43
2

-
-

-
-

H
-b

iL
ST

M
(2

)-
M

ul
tiB

al
0.

42
0.

79
7

0.
38

6
0.

82
5

0.
41

8
0.

81
1

0.
50

2
0.

71
3

0.
30

2
0.

57
1

0.
35

2
0.

56
4

-
-

-
-

H
-b

iL
ST

M
(1

)-
M

ul
tiS

im
p

w
/U

D
S-

IH
2

0.
43

1
0.

78
5

0.
36

5
0.

83
3

0.
43

1
0.

8
0.

51
3

0.
73

3
0.

27
7

0.
56

9
0.

34
1

0.
28

6
0.

98
2

0.
76

5
0.

98
0.

76
1

H
-b

iL
ST

M
(2

)-
M

ul
tiS

im
p

w
/U

D
S-

IH
2

0.
44

0.
79

0.
39

3
0.

82
0.

42
2

0.
81

5
0.

48
1

0.
74

9
0.

30
6

0.
55

6
0.

37
4

0.
49

5
0.

97
0.

76
4

0.
96

9
0.

76

Ta
bl

e
10

:F
ul

lt
ab

le
of

re
su

lts
,i

nc
lu

di
ng

al
l1

-l
ay

er
an

d
2-

la
ye

rm
od

el
s.

744


