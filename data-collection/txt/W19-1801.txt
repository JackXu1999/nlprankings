



















































Adversarial Regularization for Visual Question Answering: Strengths, Shortcomings, and Side Effects


Proceedings of the Second Workshop on Shortcomings in Vision and Language, pages 1–13
Minneapolis, USA, June 6, 2019. c©2017 Association for Computational Linguistics

1

Adversarial Regularization for Visual Question Answering:
Strengths, Shortcomings, and Side Effects

Gabriel Grand1 and Yonatan Belinkov1,2
1Harvard John A. Paulson School of Engineering and Applied Sciences

2MIT Computer Science and Artificial Intelligence Laboratory
Cambridge, MA, USA

ggrand@alumni.harvard.edu, belinkov@seas.harvard.edu

Abstract

Visual question answering (VQA) models
have been shown to over-rely on linguis-
tic biases in VQA datasets, answering ques-
tions “blindly” without considering visual
context. Adversarial regularization (AdvReg)
aims to address this issue via an adversary sub-
network that encourages the main model to
learn a bias-free representation of the question.
In this work, we investigate the strengths and
shortcomings of AdvReg with the goal of bet-
ter understanding how it affects inference in
VQA models. Despite achieving a new state-
of-the-art on VQA-CP, we find that AdvReg
yields several undesirable side-effects, includ-
ing unstable gradients and sharply reduced
performance on in-domain examples. We
demonstrate that gradual introduction of regu-
larization during training helps to alleviate, but
not completely solve, these issues. Through
error analyses, we observe that AdvReg im-
proves generalization to binary questions, but
impairs performance on questions with hetero-
geneous answer distributions. Qualitatively,
we also find that regularized models tend to
over-rely on visual features, while ignoring
important linguistic cues in the question. Our
results suggest that AdvReg requires further
refinement before it can be considered a viable
bias mitigation technique for VQA.

1 Introduction

In recent years, the Visual Question Answering
(VQA) community has grown increasingly cog-
nizant of the confounding role that bias plays in
VQA research. Many popular VQA datasets have
been shown to contain systematic language biases
that enable models to cheat by answering ques-
tions “blindly” without considering visual context
(Agrawal et al., 2016; Zhang et al., 2016; Goyal
et al., 2017; Agrawal et al., 2018).

Efforts to address this problem have mainly
focused on constructing more balanced datasets
(Zhang et al., 2016; Goyal et al., 2017; John-
son et al., 2017; Chao et al., 2018). However,
any benchmark that involves crowdsourced data
is likely to encode certain cognitive and/or social
biases (van Miltenburg, 2016; Misra et al., 2016;
Eickhoff, 2018). An alternate approach is to de-
velop models that can generalize to novel domains
with different biases. In this spirit, Agrawal et al.
(2018) introduced VQA under Changing Priors
(VQA-CP), a new benchmark in which the dis-
tribution of answers varies significantly between
train and test splits. Existing models, which tend
to rely heavily on the distribution of answers in the
training set, perform poorly on VQA-CP (Agrawal
et al., 2018).

One approach to mitigating bias that has re-
cently gained interest is a technique called adver-
sarial regularization (AdvReg). In AdvReg, an
adversary sub-network performs an inference task
based on a subset of the input features; in this case,
the adversary attempts to predict answers based
only on the question. Successful performance by
the adversary indicates that the main network has
learned a biased input representation. Negated
gradient updates from the adversary are backprop-
agated to a shared encoder to encourage the main
network to learn a bias-neutral representation of
the question. Recently, Ramakrishnan et al. (2018)
applied AdvReg to VQA and found that it im-
proves generalization to out-of-domain examples
on VQA-CP test.

Despite this initial success, AdvReg is still a
relatively new methodology, and its effects on
representation learning in neural networks re-
main largely unknown. In this study, we explore
AdvReg with the goal of better understanding how
this technique affects inference in VQA models.
We apply AdvReg to the Pythia VQA architec-



2

ture (Jiang et al., 2018b), achieving a new state-
of-the-art on VQA-CP v1 and v2. However, we
find that AdvReg yields a number of previously
unreported and undesirable side-effects. We first
observe that AdvReg introduces significant noise
into gradient updates that creates instability dur-
ing training. This finding motivates the introduc-
tion of a new scheduling technique that gradu-
ally introduces regularization over the course of
training. We find that scheduling improves gra-
dient stability in the early phases of adversarial
training and improves performance on VQA-CP
v2. However, even with scheduling, AdvReg sig-
nificantly reduces performance on in-domain ex-
amples. This side-effect suggests that like many
statistical regularization methods, AdvReg offers
a trade-off between in-domain and out-of-domain
performance.

To investigate the strengths and weaknesses of
regularized models, we perform quantitative and
qualitative error analyses. We find that AdvReg
is especially helpful with Yes/No questions, but
reduces performance on questions with heteroge-
neous answers. We also visualize a number of
successes and failures of AdvReg, revealing that
regularized models often ignore linguistic cues in
the question and are heavily swayed by salient vi-
sual features. These findings suggest an under-
utilization of key information in the question.

The contributions of this work are two-fold.
First, we share practical tips for dealing with the
idiosyncrasies of AdvReg. Second, we highlight
some core drawbacks of AdvReg that have not pre-
viously been reported in the literature. By drawing
attention to these shortcomings, we hope to moti-
vate future efforts to refine AdvReg.

2 Related Work

Biases in VQA datasets A growing body of
work points to the existence of biases in popular
VQA datasets (Agrawal et al., 2016; Zhang et al.,
2016; Jabri et al., 2016; Goyal et al., 2017; John-
son et al., 2017; Chao et al., 2018; Agrawal et al.,
2018; Thomason et al., 2018). In VQA v1 (An-
tol et al., 2015), for instance, for questions of the
form, “What sport is...?”, the correct answer is
“tennis” 41% of the time, and for questions begin-
ning with “Do you see a...?” the correct answer
is“yes” 87% of the time (Zhang et al., 2016). By
exploiting these biases, models can disregard the
image and still achieve high VQA scores.

Biases in other language tasks Language bi-
ases have also been reported in natural lan-
guage inference (NLI) (Gururangan et al., 2018;
Tsuchiya, 2018; Poliak et al., 2018), reading com-
prehension (Kaushik and Lipton, 2018), and story
cloze completion (Schwartz et al., 2017). Many of
these tasks are concerned with inferring the rela-
tionship between two objects. As in VQA, mod-
els can often succeed by learning biases associated
with one of these objects, while ignoring the other.

Biases in other vision tasks Images can also
encode certain associative biases. For instance,
the Commmon Objects in Context (COCO) im-
age dataset (Lin et al., 2014), which is used in
VQA, has been shown to contain prominent gen-
der biases (Zhao et al., 2017; Hendricks et al.,
2018). Recently, Hendricks et al. (2018) intro-
duced a technique that encourages the assignment
of equal gender probability when gender informa-
tion is occluded from an image. Their Appearance
Confusion Loss can be viewed as a vision caption-
ing analogue to AdvReg for VQA.

Mitigating bias Initial efforts to address bias
in VQA focused on debiasing existing datasets.
VQA v2 introduced complimentary examples with
different answers to every question (Goyal et al.,
2017). While VQA v2 resulted in a near 50/50
balance for Yes/No questions, the distribution for
non-binary questions (e.g., “What type of...?”;
“What sport is...?”) remains skewed towards a
handful of top answers (Goyal et al., 2017).

Given the difficulty of isolating bias from
crowdsourced data, researchers have instead be-
gun to emphasize generalization to new domains
with different biases. In this line, Agrawal et al.
(2018) introduced VQA-CP, a re-division of the
existing VQA datasets in which the distribution
of answers per question type is inverted between
train and test splits. For instance, in the VQA-CP
v1 train split, “tennis” is the most frequent answer
for the question “What sport is...?”, while “ski-
ing” is very uncommon; in the test split, this prior
is reversed. Most relevant to our work, Ramakr-
ishnan et al. (2018) applied AdvReg to VQA-CP,
and found that it improved test performance over a
non-regularized model. Similarly, Belinkov et al.
(2019) analyzed the effects of using AdvReg to ad-
dress bias in NLI. In this work, we analyze the ef-
fects of AdvReg on VQA models in further detail,
complement AdvReg with a scheduling scheme,
and point to remaining limitations in its behavior.



3

3 Methods

3.1 Adversarial Regularization
Many modern VQA architectures adhere to a com-
mon modular design (Jiang et al., 2018b) consist-
ing of the following four components:

• fv(I; θv) : I 7→ v Image encoder

• fq(Q; θq) : Q 7→ q Question encoder

• fz(v, q; θz) : v, q 7→ z Multimodal fusion

• gVQA(z; θVQA) : z 7→ P (a) Answer classifier

Composing these components, we obtain the fol-
lowing expression for the base VQA model. This
model is trained to minimize cross entropy loss:1

P (a|I,Q) = gVQA(fz(fv(I), fq(Q))) (1)

LVQA = −
∑
i

ai logP (ai|I,Q) (2)

In AdvReg, we introduce an adversarial clas-
sifier gADV(q; θADV), which attempts to infer the
correct answer from only the question features.
gADV shares the same question feature extractor fq
as the base VQA model. However, fq and gADV
are separated by a gradient reversal layer (GRL).
The GRL is a pseudo-function that negates gra-
dients on the backward pass; otherwise, it leaves
inputs unchanged:

GRLλ(x) = x
∂GRLλ
∂x

= −λGRL (3)

where λGRL is a hyperparameter. As above, the
adversary is trained to minimize the cross entropy
loss LADV:

P (a|Q) = gADV(GRLλ(fq(Q))) (4)

LADV = −
∑
i

ai logP (ai|Q) (5)

The adversarial relationship between the main
model and the adversary can be expressed as:

min
θv,q,z,VQA

max
θq,ADV

L = LVQA − λADVLADV (6)

where the regularization coefficient λADV ≥ 0
controls the trade-off between performance on
VQA and robustness to language bias. Addition-
ally, λGRL ≥ 0 (from Eq. 3) scales the reversed

1Since the VQA evaluation metric includes ground truth
answers from 10 different subjects, we follow the top-
performing models in using a soft target, multi-label variant
of the cross entropy objective (see Teney et al. 2018).

Figure 1: Schematic diagram of adversarial VQA ar-
chitecture. Right and left arrows represent forward and
backward propagation, respectively. The red arrow in-
dicates the gradient reversal layer.

gradients. These two hyperparameters perform re-
lated, but different, functions. Setting either or
both to zero disables the regularization, since fq
receives no gradients from the adversary. This
combination is equivalent to the baseline model.
Meanwhile, setting λADV > 0, λGRL > 0 enables
AdvReg. This setting is the main focus of our ex-
periments.

3.2 Gradient Reversal Layer Scheduling
Because the GRL counteracts the main gradient
updates, AdvReg produces noisy gradients that
can interfere with learning, as we observe in the
experiments below (Fig. 4). To improve stability
during the early stages of training, we experiment
with a scheduling regime for the gradient rever-
sal layer similar to that used in domain-adversarial
neural networks (Ganin et al., 2016). During train-
ing, we delay the introduction of regularization for
the first µ iterations, which allows fq to receive
clean gradients from the VQA model. Next, we
have a warmup phase forw iterations, in which we
increase λGRL linearly from 0 to some constant c:

λGRL(t) =


0 t ≤ µ
c(t−µ)
w µ ≤ t ≤ µ+ w

c t > µ+ w

(7)

GRL scheduling introduces two new hyperparam-
eters, µ andw, which we set by grid search; further
details are given in Appendix A.2.

4 Experimental Setup

4.1 Data
We evaluated the performance of our AdvReg
setup on VQA-CP v1 and v2 (Agrawal et al.,
2018). We also retrained our best-performing
models with the same hyperparameter settings on



4

VQA-CP v1 (test) VQA-CP v1 (val) VQA v1 (val)

Model λADV λGRL Overall Yes/No Num. Other Overall Yes/No Num. Other Overall

Baseline 0 0 37.87 42.58 14.16 42.71 65.79 86.98 40.06 56.41 62.68
+ AdvReg 0.01 0.1 45.69 77.64 13.21 26.97 46.94 65.32 32.95 37.22 46.34
+ GRL Sch. 0.01 0.1 44.09 75.01 13.40 25.67 46.45 67.28 29.11 35.71 46.71

VQA-CP v2 (test) VQA-CP v2 (val) VQA v2 (val)

Baseline 0 0 38.80 41.70 12.17 44.59 67.76 84.76 49.22 57.04 63.27
+ AdvReg 0.005 1 36.33 59.33 14.01 30.41 50.63 67.39 38.81 38.37 48.78
+ GRL Sch. 0.005 1 42.33 59.74 14.78 40.76 56.90 69.23 42.50 49.36 51.92

Table 1: Performance comparison of baseline and adversarially-trained models on VQA-CP/VQA v1 and v2
datasets using the best-performing hyperparameters.

VQA v1 (Antol et al., 2015) and v2 (Goyal et al.,
2017) in order to evaluate performance on datasets
without changing priors.

One difficulty of working with VQA-CP is the
lack of validation sets. Ramakrishnan et al. (2018)
explain that VQA-CP does not provide validation
sets due to the difficulty in varying the answer
distributions of binary questions across more than
two splits. The authors note that, in place of early
stopping, they train their models “until conver-
gence.”2 Although the nonstandard structure of
VQA-CP makes validation tricky, we believe it is
important to have some mechanism to distinguish
between overfitting to language priors and overfit-
ting to the examples in the training set (the latter
may occur regardless of the presence of language
biases). Our solution is to train models on 90% of
the training data and reserve the remaining 10%
(sampled randomly) for validation. Score on the
val split is useful as an early stopping metric, but
does not forecast test performance. In this way, we
are able to prevent our models from overfitting to
the training data, while remaining agnostic to the
distribution of priors in the test set.

While the addition of a VQA-CP val set en-
ables early stopping, models that perform best on
the val set will tend to be under-regularized, since
AdvReg reduces in-domain performance. We con-
sidered creating a second val set derived from
VQA-CP test for model selection. However, in
addition to introducing additional complexity, this
approach would both compromise our ability to
remain agnostic to the test set and make our re-
sults incomparable with prior work. Therefore,
we follow Ramakrishnan et al. (2018) and per-

2In correspondence, the authors clarified that they trained
for a fixed interval determined by the number of iterations to
reach peak performance on VQA v2. Since overfitting tends
to occur more rapidly on VQA-CP, we view an in-domain val
split as a more reliable early stopping metric.

form model selection on VQA-CP test. However,
to increase transparency, we report results across
a broad range of hyperparameters. We hope that
recognition of these challenges will motivate the
introduction of a standard val set for VQA-CP.

4.2 Implementation

Our experimental setup is based on the Pythia im-
plementation of the Bottom-Up / Top-Down VQA
model (Jiang et al., 2018a; Anderson et al., 2018).3

The adversarial classifier gADV is implemented as
a two-layer fully-connected network with 512 hid-
den units and ReLU activation. Unless otherwise
noted, we use the default hyperparameters from
Pythia. Additional details are available in Ap-
pendix A.1.

5 Results

5.1 Strengths of AdvReg

Table 1 summarizes the results of the baseline
model and the best performing adversarially reg-
ularized models. On the VQA-CP v1 test set,
our best AdvReg model outperforms the baseline
by 7.82%, attaining a new state-of-the-art for this
task. On the VQA-CP v2 test set, our best AdvReg
model performs worse than the baseline; however,
with GRL scheduling, it surpasses the baseline by
3.53%, again setting a new state-of-the-art. Note
that in both cases, our models perform better than
Ramakrishnan et al. (2018), who report scores of
43.43% and 41.17% on VQA-CP v1 and v2 test,
despite the fact that we use only 90% of the avail-
able training data. This result indicates that allo-
cating 10% for validation helps prevent overfitting
to the training examples.

To highlight how AdvReg mitigates overfitting,
Fig. 2 plots loss curves of the baseline (blue)

3Our code is available at https://github.com/
gabegrand/adversarial-vqa

https://github.com/gabegrand/adversarial-vqa
https://github.com/gabegrand/adversarial-vqa


5

Figure 2: Comparison of regularized (red) and baseline (blue) models on VQA-CP v1 train, val, and test. The
baseline model exhibits severe overfitting on both the val and test splits. In contrast, the regularized model overfits
much less and achieves a higher score on VQA-CP test.

and regularized (red) models during training. The
baseline model exhibits severe overfitting on both
VQA-CP v1 val and test. Note that overfitting on
the test set appears around 2000 iterations as the
model begins to over-rely on language priors. In
contrast, overfitting on the val set appears later
(around 3500 iterations) as the model begins to
memorize the training examples.

In general, AdvReg works well out-of-box on
VQA-CP v1. Many of the hyperparameter com-
binations we tested (Fig. 3) outperform the base-
line on VQA-CP v1 test. The key to successful
regularization appears to be balancing λADV and
λGRL. As Fig. 3 reveals, large values of λADV per-
form better with small values of λGRL, and vice-
versa. However, when λADV is too small, AdvReg
fails to improve performance; none of the models
we tested with λADV = 0.001 outperformed the
baseline. On the other hand, when λADV is too
large, training becomes unstable; for λADV > 1
(not shown), we observed many training runs fail-
ing to converge due to exploding gradient values.

5.2 Shortcomings of AdvReg

The improved performance on the out-of-domain
test sets comes at the expense of performance
on the in-domain validation sets. As Table 1
shows, on both VQA-CP v1 and v2 val, AdvReg
models significantly under-performed the baseline
(-18.85% and -10.66%, respectively). Retrain-
ing with the same hyperparameters on the original
VQA v1 and v2 datasets yielded similar results.

Notably, these findings differ from Ramakrish-
nan et al. (2018), who report only minimal re-
ductions in performance on VQA v1 and v2 from
AdvReg. One explanation is that the gains we
observed on VQA-CP test relative to Ramakrish-
nan et al. resulted in diminished performance on
VQA-CP val. Indeed, across all runs of our exper-
iments, we found that score on VQA-CP v1 test
correlated negatively with score on the val split
(r2 = -0.355, p = 0.013).4 In their work, Ramakr-
ishnan et al. also introduce a secondary “differ-
ence of entropies” (DoE) regularizer, which they
find improves in-domain performance and helps
to stabilize adversarial training. However, even
without DoE, they report margins of only 1-4%
between their AdvReg and baseline models. Ulti-
mately, these unaccounted differences may be due
to implementation details, suggesting the need for
a closer comparison.5

Our results also highlight interesting differences
between VQA-CP v1 and v2. On VQA-CP test,
the gains due to AdvReg were more significant on
v1 as compared to v2. However, on the valida-
tion sets, the losses were also greater. This pattern
also applied with respect to the original versions of
these datasets (i.e., VQA v1 and v2). These find-
ings support the notion that VQA v2 is indeed less
biased than v1.

4We did not find a significant correlation between test and
val performance on VQA-CP v2 (r2 = 0.237, p = 0.141).

5To our knowledge, code from (Ramakrishnan et al.,
2018) is not public at present.



6

Figure 3: Hyperparameter sweep on VQA-CP v1 and v2 test. Each line is a different setting of λADV; lighter/darker
red indicates less/more regularization, respectively. λGRL is varied along the x-axis. Blue dashes: baseline score.

5.3 Effect of GRL scheduling

Without GRL scheduling, none of the AdvReg
hyperparameter combinations we tested outper-
formed the baseline on VQA-CP v2 test (see
Fig. 3). This finding may be attributed to the
substantial amount of noise that the adversary in-
jects into the gradient updates for the question
encoder, as demonstrated by recording gradient
norms throughout training.

As Fig. 4 illustrates, on VQA-CP v2, GRL
scheduling reduces gradient instability early in
training, allowing the model to converge to a lower
loss value. In the best-performing schedule, regu-
larization was delayed until µ = 2000 iterations,
and slowly warmed up for the following w = 4000
steps. This schedule resulted in a 6.00% perfor-
mance increase on VQA-CP v2 test compared to
using the same regularization coefficients without
GRL scheduling, and a 3.53% improvement over
the baseline (see Table 1).

On VQA-CP v1, we did not observe commen-
surate improvements from GRL scheduling. We
hypothesize that introducing AdvReg on a delay
may not be as effective on v1 due to the more
prominent biases in this dataset. Note that the
baseline model begins to overfit roughly twice as
quickly on VQA-CP v1 as on VQA-CP v2 (Fig. 4,
Baseline loss). Accordingly, in addition to sweep-
ing the same hyperparameters tested on VQA-CP
v2, we experimented with accelerated GRL sched-
ules for VQA-CP v1. While five of the runs out-
performed the baseline, three of these were with
no start delay. Moreover, all of the runs with GRL
scheduling performed worse than a model with the
same regularization coefficients with static λGRL.
Finally, many of the runs on VQA-CP v1, and es-

Figure 4: Gradient norms and loss during adversarial
training. On VQA-CP v2, GRL scheduling helps to re-
duce gradient noise early in training (bottom left), lead-
ing to lower loss values (bottom right). On VQA-CP
v1, the baseline (blue, top right) overfits more quickly;
hence, delaying the regularization is less effective.

pecially those with fewer warm-up iterations, di-
verged due to exploding gradients. These findings
suggest that the stronger the biases in a dataset,
the earlier AdvReg must be introduced in order to
counter overfitting effectively.

6 Error Analysis
We performed quantitative and qualitative error
analyses to understand how AdvReg affects model
inferences on different kinds of examples. To best
highlight the effect of AdvReg, both analyses were
performed on VQA-CP v1 test, where the change
in priors is more pronounced. In both analyses,
we compare our best AdvReg model (which did
not use GRL scheduling) and the baseline model.

6.1 Quantitative Analysis

We first explore how model performance differs
by question type. In the VQA datasets, each ques-



7

AdvReg >> Baseline AdvReg << Baseline
Question type Ans. N Base. Reg. ∆ Question type Ans. N Base. Reg. ∆

is there a Yes/No 6501 16.75 93.41 49.83 is this Yes/No 13063 76.96 64.85 -15.82
is this a Yes/No 7177 29.70 86.27 40.60 what color is the Other 4418 47.71 21.36 -11.64
are the Yes/No 5037 24.99 87.07 31.27 what Other 8646 38.48 25.28 -11.42
does the Yes/No 3525 24.02 94.34 24.79 what is the Other 6363 41.49 28.51 -8.26
is Yes/No 3154 32.84 92.38 18.78 is the Other 1148 50.44 4.40 -5.29
are they Yes/No 1577 27.96 89.40 9.69 what kind of Other 3141 51.43 35.51 -5.00
do you Yes/No 1083 26.14 92.32 7.17 how many Number 15917 15.90 13.01 -4.60
is there Yes/No 5265 68.83 78.45 5.06 what type of Other 1995 54.74 36.30 -3.68
is the person Yes/No 757 41.64 92.46 3.85 none of the above Other 2057 29.65 13.66 -3.29
how many people are Number 2118 11.96 21.08 1.93 what color are the Other 1435 56.93 35.74 -3.04

Table 2: Comparison of relative strengths and weaknesses of regularized and baseline models. The top 10 question
types for which the regularized model outperforms the baseline are shown on the left, and vice versa on the right.

tion is assigned a type corresponding to the 64
most common prefixes (e.g., “Is there a...?”) or
“none of the above.” Additionally, each example
is given an answer type (Yes/No, Number, Other).6

To quantify the relative performance of the
AdvReg and baseline models, we computed a dif-
ference metric, weighted by the number of ques-
tions N of the given type:

∆ = N100
(
scorebaseline − scoreregularized

)
Table 2 shows the question types with the largest
and smallest ∆ values, respectively. Compared to
the baseline, the AdvReg model excels at Yes/No
examples, but suffers on Other examples. Over-
all, AdvReg improves Yes/No test performance by
35.06 points, but reduces Other performance by
15.74 points (Table 1). Additionally, AdvReg re-
duces Number test performance by 0.95%, though
in general both models score poorly on counting
questions—a known shortcoming of many VQA
models (Chattopadhyay et al., 2017; Trott et al.,
2018; Zhang et al., 2018).

These results suggest that much of the observed
advantage of AdvReg on VQA-CP test is due to
the extreme biases present in the dataset. In VQA-
CP, Yes/No questions encode very strong priors
(e.g., “no” is the answer to roughly 90% of the
questions beginning with “Is there a...?” in the
v1 training set). Because this prior is inverted,
any learned association between question prefixes
and answers becomes harmful at test time. That
AdvReg scores well above chance (77.64%) on
Yes/No examples suggests that this model has, to a
certain degree, learned to answer binary questions
without relying on language priors.

6Note that the mapping between question types and an-
swer types is not exactly one-to-one. However, for a given
question type, a single answer type typically predominates;
therefore, we are able to draw an approximate correspon-
dence between question and answer types.

In contrast, the 15.74% drop on Other-type ex-
amples implies that AdvReg impairs the model’s
ability to make inferences about questions with
heterogeneous answers. Other-type questions typ-
ically have 3–20 top answers. This finding sug-
gests that AdvReg interferes with learning of lan-
guage cues in the question that yield key informa-
tion about the answer.

6.2 Qualitative Analysis

In this section, we examine individual examples
to highlight common success and failure modes
of AdvReg. We consider different question types
and compare the prior answer distribution in the
train/test sets to the posterior distribution assigned
by the AdvReg and baseline models. Expanding
on the visualization format introduced by Ramakr-
ishnan et al. (2018, Fig. 3), Fig. 5 shows examples
where the AdvReg model successfully answered
the question while the baseline model was wrong.
In these cases, the baseline model prediction re-
lies on the prior answer distribution in the train
set, while the AdvReg model is able to overcome
these priors to infer the correct answer.

Turning to failures, we investigate what kinds
of errors the AdvReg model makes on Other-type
examples—the largest source of errors according
to Section 6.1. We randomly selected instances
where the regularized model produced an incor-
rect answer, and manually grouped these examples
into four approximate categories corresponding to
different failure modes. Fig. 6 shows represen-
tative examples for each of these failure modes;
more examples are available in Appendix A.3.

Fig. 6a shows an example where the regularized
model fails to infer the correct form of the answer
from the question, answering “beach” to a ques-
tion that entails animal answers. In Fig. 6b, the
regularized model struggles with a question that



8

Figure 5: Visualization of AdvReg success cases. In each example, the leftmost two bars show the prior distribution
over answers for the given question type (in bold). The rightmost two bars show the scores assigned to different
answers by the baseline and AdvReg models for a particular example of the given type. The baseline model
frequently assigns high probability to incorrect answers that are prominent in the training distribution. In contrast,
the regularized model is able to make correct inferences in cases where the ground truth answer has low prior
probability. Additional examples are provided in Appendix A.3.

(a) AdvReg model fails to infer the correct form of the answer. (b) AdvReg model fails to utilize real-world language priors.

(c) AdvReg model distracted by visually-salient image fea-
tures.

(d) AdvReg model relies on image features, while baseline
model relies on language priors.

Figure 6: Common failure modes of adversarial regularization. Additional examples are provided in Appendix A.3.



9

relies on real-world language priors (i.e., mustard
is yellow). In Fig. 6c, the parrot’s salient orange
color distracts the regularized model from attend-
ing to the correct image region. Fig. 6d shows
an example where the regularized model relies on
visual features (the cat), while the baseline re-
lies on language priors (tennis is a common an-
swer to sport questions). These findings suggest
that AdvReg may encourage models to rely on vi-
sual features at the expense of learning to interpret
task-relevant linguistic information.

7 Conclusion

In this work, we investigated several strengths and
limitations of adversarial regularization, a recently
introduced technique for reducing language bi-
ases in VQA models. Though we find AdvReg
improves performance on out-of-domain exam-
ples in VQA-CP, one concern is that the pendu-
lum has swung too far: there are both quantita-
tive and qualitative signs that our models are over-
regularized. Quantitatively, the performance of
our AdvReg models suffers on in-domain exam-
ples in VQA-CP and the original VQA datasets.
Additionally, while AdvReg boosts performance
on binary questions, it impairs performance on
other question types. Qualitatively, we observe
that AdvReg models draw on salient image fea-
tures while ignoring important linguistic cues in
questions. These results demonstrate that AdvReg
interferes with certain key aspects of reasoning.

Our findings highlight the need for further re-
search in two areas: datasets and modeling. The
lack of a validation set in VQA-CP makes it dif-
ficult to perform hyperparameter tuning in a prin-
cipled way. Moreover, the exaggerated biases in
the existing VQA-CP splits may encourage over-
regularization, as evidenced by the sharp discrep-
ancy between AdvReg performance on binary and
non-binary question types. To address these is-
sues, future iterations of VQA-CP could contain
three or more splits with moderate but distinct ra-
tios of Yes/No answers. Restructuring VQA-CP
in this way would help balance the importance of
binary and non-binary questions, while providing
researchers with more sound evaluation metrics.

On the modeling side, our findings suggest that
AdvReg requires further refinement to avoid im-
pairing learning of task-relevant linguistic infor-
mation. One possible approach would be to use
attention to apply different amounts of regulariza-

tion to different words in the question. In this
way, regularization could be focused on the first
few words of the question (e.g., “Is there a...?”)
that encode answer distribution biases, while pre-
serving other useful linguistic information. Such
enhancements could lead to more targeted regu-
larization techniques that preserve the benefits of
AdvReg while reducing the drawbacks discussed
in this work.

Acknowledgements

We would like to thank Alexander Rush for pro-
viding helpful advice and comments throughout
our work on this project. GG and YB were sup-
ported by the Harvard Mind, Brain, and Behavior
Initiative.

References
Aishwarya Agrawal, Dhruv Batra, and Devi Parikh.

2016. Analyzing the Behavior of Visual Question
Answering Models. In EMNLP, pages 1955–1960.

Aishwarya Agrawal, Dhruv Batra, Devi Parikh, and
Aniruddha Kembhavi. 2018. Don’t Just Assume;
Look and Answer: Overcoming Priors for Visual
Question Answering. In CVPR, pages 4971–4980.

Peter Anderson, Xiaodong He, Chris Buehler, Damien
Teney, Mark Johnson, Stephen Gould, and Lei
Zhang. 2018. Bottom-Up and Top-Down Attention
for Image Captioning and Visual Question Answer-
ing. In CVPR, volume 3, page 6.

Stanislaw Antol, Aishwarya Agrawal, Jiasen Lu, Mar-
garet Mitchell, Dhruv Batra, C Lawrence Zitnick,
and Devi Parikh. 2015. VQA: Visual Question An-
swering. In ICCV, pages 2425–2433.

Yonatan Belinkov, Adam Poliak, Stuart M. Shieber,
Benjamin Van Durme, and Alexander Rush. 2019.
On Adversarial Removal of Hypothesis-only Bias in
Natural Language Inference. In The Eighth Joint
Conference on Lexical and Computational Seman-
tics (*SEM).

Wei-Lun Chao, Hexiang Hu, and Fei Sha. 2018. Be-
ing Negative but Constructively: Lessons Learnt
from Creating Better Visual Question Answering
Datasets. In NAACL-HLT, volume 1, pages 431–
441.

Prithvijit Chattopadhyay, Ramakrishna Vedantam,
Ramprasaath R Selvaraju, Dhruv Batra, and Devi
Parikh. 2017. Counting Everyday Objects in Every-
day Scenes. In CVPR, pages 1135–1144.

Carsten Eickhoff. 2018. Cognitive Biases in Crowd-
sourcing. In WSDM, pages 162–170. ACM.



10

Yaroslav Ganin, Evgeniya Ustinova, Hana Ajakan,
Pascal Germain, Hugo Larochelle, François Lavio-
lette, Mario Marchand, and Victor Lempitsky. 2016.
Domain-Adversarial Training of Neural Networks.
JMLR, 17(1):2096–2030.

Yash Goyal, Tejas Khot, Douglas Summers-Stay,
Dhruv Batra, and Devi Parikh. 2017. Making the V
in VQA matter: Elevating the Role of Image Under-
standing in Visual Question Answering. In CVPR,
pages 6904–6913.

Suchin Gururangan, Swabha Swayamdipta, Omer
Levy, Roy Schwartz, Samuel Bowman, and Noah A.
Smith. 2018. Annotation Artifacts in Natural Lan-
guage Inference Data. In NAACL-HLT, pages 107–
112, New Orleans, Louisiana. Association for Com-
putational Linguistics.

Lisa Anne Hendricks, Kaylee Burns, Kate Saenko,
Trevor Darrell, and Anna Rohrbach. 2018. Women
also Snowboard: Overcoming Bias in Captioning
Models. In ECCV, pages 771–787.

Allan Jabri, Armand Joulin, and Laurens van der
Maaten. 2016. Revisiting Visual Question Answer-
ing Baselines. In ECCV, pages 727–739. Springer.

Yu Jiang, Vivek Natarajan, Xinlei Chen, Mar-
cus Rohrbach, Dhruv Batra, and Devi Parikh.
2018a. Pythia. https://github.com/
facebookresearch/pythia.

Yu Jiang, Vivek Natarajan, Xinlei Chen, Marcus
Rohrbach, Dhruv Batra, and Devi Parikh. 2018b.
Pythia v0.1: The Winning Entry to the VQA Chal-
lenge 2018. arXiv preprint arXiv:1807.09956.

Justin Johnson, Bharath Hariharan, Laurens van der
Maaten, Li Fei-Fei, C Lawrence Zitnick, and Ross
Girshick. 2017. CLEVR: A Diagnostic Dataset
for Compositional Language and Elementary Visual
Reasoning. In CVPR, pages 1988–1997. IEEE.

Divyansh Kaushik and Zachary C. Lipton. 2018. How
Much Reading Does Reading Comprehension Re-
quire? A Critical Investigation of Popular Bench-
marks. In EMNLP, pages 5010–5015, Brussels, Bel-
gium. ACL.

Tsung-Yi Lin, Michael Maire, Serge Belongie, James
Hays, Pietro Perona, Deva Ramanan, Piotr Dollár,
and C Lawrence Zitnick. 2014. Microsoft COCO:
Common Objects in Context. In ECCV, pages 740–
755. Springer.

Emiel van Miltenburg. 2016. Stereotyping and
Bias in the Flickr30k Dataset. arXiv preprint
arXiv:1605.06083.

Ishan Misra, C Lawrence Zitnick, Margaret Mitchell,
and Ross Girshick. 2016. Seeing Through the Hu-
man Reporting Bias: Visual Classifiers from Noisy
Human-Centric Labels. In CVPR, pages 2930–
2939.

Adam Poliak, Jason Naradowsky, Aparajita Haldar,
Rachel Rudinger, and Benjamin Van Durme. 2018.
Hypothesis Only Baselines in Natural Language In-
ference. In *SEM, pages 180–191, New Orleans,
Louisiana. ACL.

Sainandan Ramakrishnan, Aishwarya Agrawal, and
Stefan Lee. 2018. Overcoming Language Priors in
Visual Question Answering with Adversarial Regu-
larization. In NIPS, pages 1548–1558.

Roy Schwartz, Maarten Sap, Ioannis Konstas, Leila
Zilles, Yejin Choi, and Noah A. Smith. 2017. Story
Cloze Task: UW NLP System. In LSDSem.

Damien Teney, Peter Anderson, Xiaodong He, and An-
ton van den Hengel. 2018. Tips and Tricks for Vi-
sual Question Answering: Learnings from the 2017
Challenge. In CVPR, pages 4223–4232.

Jesse Thomason, Daniel Gordan, and Yonatan Bisk.
2018. Shifting the Baseline: Single Modality
Performance on Visual Navigation & QA. arXiv
preprint arXiv:1811.00613.

Alexander Trott, Caiming Xiong, and Richard Socher.
2018. Interpretable Counting for Visual Question
Answering. In ICLR.

Masatoshi Tsuchiya. 2018. Performance Impact
Caused by Hidden Bias of Training Data for Rec-
ognizing Textual Entailment. In LREC.

Peng Zhang, Yash Goyal, Douglas Summers-Stay,
Dhruv Batra, and Devi Parikh. 2016. Yin and Yang:
Balancing and Answering Binary Visual Questions.
In CVPR, pages 5014–5022. IEEE.

Yan Zhang, Jonathon Hare, and Adam Prgel-Bennett.
2018. Learning to Count Objects in Natural Images
for Visual Question Answering. In ICLR.

Jieyu Zhao, Tianlu Wang, Mark Yatskar, Vicente Or-
donez, and Kai-Wei Chang. 2017. Men Also Like
Shopping: Reducing Gender Bias Amplification us-
ing Corpus-level Constraints. In EMNLP.

http://www.aclweb.org/anthology/N18-2017
http://www.aclweb.org/anthology/N18-2017
https://github.com/facebookresearch/pythia
https://github.com/facebookresearch/pythia
http://www.aclweb.org/anthology/D18-1546
http://www.aclweb.org/anthology/D18-1546
http://www.aclweb.org/anthology/D18-1546
http://www.aclweb.org/anthology/D18-1546
http://www.aclweb.org/anthology/S18-2023
http://www.aclweb.org/anthology/S18-2023
https://openreview.net/forum?id=S1J2ZyZ0Z
https://openreview.net/forum?id=S1J2ZyZ0Z
https://openreview.net/forum?id=B12Js_yRb
https://openreview.net/forum?id=B12Js_yRb


11

A Appendix

A.1 Implementation Details
Here, we provide additional details of our implementation. We experimented with different numbers of
hidden layers N = 1, 2, 3 and hidden units h = 256, 512, 1024, 2048 in the adversarial classifier. We
found the details of the adversary architecture to have little impact on performance, with the exception
that adversaries with N > 1 hidden layers were more effective than one-layer adversaries. Both the
adversary and the base VQA model are randomly initialized with a fixed seed at the start of training.
We co-train the networks for 16k iterations with two separate PyTorch Adamax optimizers with batch
size 512 and learning rate 0.001. Unlike Jiang et al. (2018b), we keep the learning rate fixed throughout
training to minimize the possibility of gradient scaling mismatch between the base model and the adver-
sary. While this modification causes the performance of the baseline VQA model to drop 1.1%, it greatly
improves stability and convergence during adversarial training.

A.2 GRL Scheduling Details
For both VQA-CP v1 and v2, we performed a grid search to determine the optimal hyperparameters µ and
w for the GRL schedule. We tested all combinations of delay µ = 0, 1000, 2000, 3000, 4000, 5000, 6000
and warmup duration w = 1000, 2000, 3000, 4000. Given that the baseline model demonstrates
signs of overfitting on VQA-CP v1 as early as 2000 iterations into training, we tested an addi-
tional set of accelerated GRL schedules for VQA-CP v1 that consisted of all combinations of µ =
500, 1000, 1500, 2000, 2500, 3000, 3500 and w = 500, 1000, 2000, 4000.

Sometimes when AdvReg is introduced on a delayed schedule (especially if the value of µ is large),
overfitting occurs before AdvReg takes effect. To avoid ending training prematurely, we always train for
at least µ iterations before early stopping can be triggered. For instance, if µ = 3000, then the earliest
that we will stop training is t = 4000. For the purposes of evaluation, we also consider only scores from
t > µ when scoring models under GRL scheduling.



12

A.3 Additional Examples

Figure 7: Visualization of AdvReg success cases. In each example, the leftmost two bars show the prior distribution
over answers for the given question type (in bold). The rightmost two bars show the scores assigned to different
answers by the baseline and AdvReg models for a particular example of the given type. The baseline model
frequently assigns high probability to incorrect answers that are prominent in the training distribution. In contrast,
the regularized model is able to make correct inferences in cases where the ground truth answer has low prior
probability.



13

Figure 8: Common failure modes of adversarial regularization. First row: the regularized model fails to infer the
correct form of the answer from the question, answering “beach” and “wedding” to questions that entail animal
answers. Second row: the regularized model struggles with questions that rely on real-world language priors; i.e.,
mustard is yellow, sunset is orange. Third row: salient colors in the image distract the regularized model from
attending to the correct image regions. Fourth row: both the baseline and regularized models perform poorly on
questions where the answer relates to a localized image region (i.e., inside a TV) as opposed to the global image. In
these cases, the regularized model relies on generic visual features in the image in its inferences, while the baseline
model relies on language priors.


