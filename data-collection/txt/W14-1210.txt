



















































An Open Corpus of Everyday Documents for Simplification Tasks


Proceedings of the 3rd Workshop on Predicting and Improving Text Readability for Target Reader Populations (PITR) @ EACL 2014, pages 84–93,
Gothenburg, Sweden, April 26-30 2014. c©2014 Association for Computational Linguistics

An Open Corpus of Everyday Documents for Simplification Tasks

David Pellow and Maxine Eskenazi
Language Technologies Institute, Carnegie Mellon University

Pittsburgh PA USA
dpellow@cs.cmu.edu, max@cs.cmu.edu

Abstract

In recent years interest in creating statisti-
cal automated text simplification systems
has increased. Many of these systems have
used parallel corpora of articles taken from
Wikipedia and Simple Wikipedia or from
Simple Wikipedia revision histories and
generate Simple Wikipedia articles. In this
work we motivate the need to construct a
large, accessible corpus of everyday docu-
ments along with their simplifications for
the development and evaluation of simpli-
fication systems that make everyday doc-
uments more accessible. We present a de-
tailed description of what this corpus will
look like and the basic corpus of every-
day documents we have already collected.
This latter contains everyday documents
from many domains including driver’s li-
censing, government aid and banking. It
contains a total of over 120,000 sentences.
We describe our preliminary work evaluat-
ing the feasibility of using crowdsourcing
to generate simplifications for these docu-
ments. This is the basis for our future ex-
tended corpus which will be available to
the community of researchers interested in
simplification of everyday documents.

1 Introduction

People constantly interact with texts in everyday
life. While many people read for enjoyment, some
texts must be read out of necessity. For example, to
file taxes, open a bank account, apply for a driver’s
license or rent a house, one must read instructions
and the contents of forms, applications, and other
documents. For people with limited reading ability
- whether because they are not native speakers of
the language, have an incomplete education, have
a disability, or for some other reason - the reading

level of these everyday documents can limit acces-
sibility and affect their well-being.

The need to present people with texts that are
at a reading level which is suitable for them has
motivated research into measuring readability of
any given text in order to assess whether automatic
simplification has rendered a more difficult text
into a more readable one. Readability can be mea-
sured using tools which assess the reading level of
a text. We define simplification as the process of
changing a text to lower its reading level without
removing necessary information or producing an
ungrammatical result. This is similar to the def-
inition of (cf. (Zhu et al., 2010)), except that we
avoid defining a specific, limited, set of simplifica-
tion operations. The Related Work section details
research into measures of readability and work on
automatic simplification systems.

We have begun to construct a large, accessi-
ble corpus of everyday documents. This corpus
will eventually contain thousands of these doc-
uments, each having statistics characterising its
contents, and multiple readability measures. Mul-
tiple different simplifications will be collected for
the original documents and their content statistics
and readability measures will be included in the
corpus. This type of large and accessible corpus is
of vital importance in driving development of au-
tomated text simplification. It will provide training
material for the systems as well as a common basis
of evaluating results from different systems.

Thus far, we have collected a basic corpus of ev-
eryday documents from a wide variety of sources.
We plan to extend this basic corpus to create the
much larger and more structured corpus that we
describe here. We have also carried out a pre-
liminary study to evaluate the feasibility of using
crowdsourcing as one source of simplifications in
the extended corpus. We have used Amazon Me-
chanical Turk (AMT) and collected 10 simplifica-
tions each for 200 sentences from the basic cor-

84



pus to determine feasibility, a good experimental
design, quality control of the simplifications, and
time and cost effectiveness.

In the next section we discuss related work rel-
evant to creating and evaluating a large corpus of
everyday documents and their simplifications. In
Section 3 we further demonstrate the need for a
corpus of everyday documents. Section 4 presents
a description of our existing basic corpus. Section
5 describes the details of the extended corpus and
presents our evaluation of the feasibility of using
crowdsourcing to generate human simplifications
for the corpus. Section 6 shows how the extended
corpus will be made accessible. Section 7 con-
cludes and outlines the future work that we will
undertake to develop the extended corpus.

2 Related Work

2.1 Readability Evaluation

Measures of readability are important because
they help us assess the reading level of any doc-
ument, provide a target for simplification systems,
and help evaluate and compare the performance
of different simplification systems. Several mea-
sures of readability have been proposed; DuBay
(2004) counted 200 such measures developed by
the 1980s and the number has grown, with more
advanced automated measures introduced since
then.

Early measures of readability such as the
Flesch-Kincaid grade level formula (Kincaid et
al., 1975) use counts of surface features of the text
such as number of words and number of sentences.
While these older measures are less sophisticated
than more modern reading level classifiers, they
are still widely used and reported and recent work
has shown that they can be a good first approxi-
mation of more complex measures (Štajner et al.,
2012).

More recent approaches use more complicated
features and machine learning techniques to learn
classifiers that can predict readability. For exam-
ple, Heilman et al. (2007) combine a naive Bayes
classifier that uses a vocabulary-based language
model with a k-Nearest Neighbors classifier us-
ing grammatical features and interpolate the two to
predict reading grade level. Feng et al. (2010) and
François and Miltsakaki (2012) examine a large
number of possible textual features at various lev-
els and compare SVM and Linear Regression clas-
sifiers to predict grade level. Vajjala and Meurers

(2012) reported significantly higher accuracy on a
similar task using Multi-level Perceptron classifi-
cation.

The above two methods of measuring readabil-
ity can be computed directly using the text of a
document itself. To evaluate the performance of
a simplification system which aims to make texts
easier to read and understand, it is also useful
to measure improvement in individuals’ reading
and comprehension of the texts. Siddharthan and
Katsos (2012) recently studied sentence recall to
test comprehension; and Temnikova and Maneva
(2013) evaluated simplifications using the readers’
ability to answer multiple choice questions about
the text.

2.2 Automated Text Simplification Systems
Since the mid-90s several systems have been de-
veloped to automatically simplify texts. Early sys-
tems used hand-crafted syntactic simplification
rules; for example, Chandrasekar et al. (1996),
one of the earliest attempts at automated simpli-
fication. Rule-based systems continue to be used,
amongst others, Siddharthan (2006), Aluisio and
Gasperin (2010), and Bott et al. (2012).

Many of the more recent systems are
statistically-based adapting techniques devel-
oped for statistical machine translation. Zhu
et al. (2010) train a probabilistic model of a
variety of sentence simplification rules using
expectation maximization with a parallel corpus
of aligned sentences from Wikipedia and Simple
Wikipedia. Woodsend and Lapata (2011) present
a system that uses quasi-synchronous grammar
rules learned from Simple Wikipedia edit histo-
ries. They solve an integer linear programming
(ILP) problem to select both which sentences are
simplified (based on a model learned from aligned
Wikipedia-Simple Wikipedia articles) and what
the best simplification is. Feblowitz and Kauchak
(2013) use parallel sentences from Wikipedia
and Simple Wikipedia to learn synchronous tree
substitution grammar rules.

2.3 Corpora for Text Simplification
Presently there are limited resources for statisti-
cal simplification methods that need to train on a
parallel corpus of original and simplified texts. As
mentioned in the previous section, common data
sources are Simple Wikipedia revision histories
and aligned sentences from parallel Wikipedia and
Simple Wikipedia articles. Petersen and Ostendorf

85



(2007) present an analysis of a corpus of 104 orig-
inal and abridged news articles, and Barzilay and
Elhadad (2003) present a system for aligning sen-
tences trained on a corpus of parallel Encyclope-
dia Britannica and Britannica Elementary articles.
Other work generates parallel corpora of original
and simplified texts in languages other than En-
glish for which Simple Wikipedia is not available.
For example, Klerke and Søgaard (2012) built a
sentence-aligned corpus from 3701 original and
simplified Danish news articles, and Klaper et al.
(2013) collected 256 parallel German and simple
German articles.

2.4 Crowdsourcing for Text Simplification
and Corpus Generation

Crowdsourcing uses the aggregate of work per-
formed by many non-expert workers on small
tasks to generate high quality results for some
larger task. To the best of our knowledge crowd-
sourcing has not previously been explored in
detail to generate text simplifications. Crowd-
sourcing has, however, been used to evaluate
the quality of automatically generated simplifica-
tions. Feblowitz and Kauchak (2013) used AMT
to collect human judgements of the simplifica-
tions generated by their system and De Clercq et
al. (2014) performed an extensive evaluation of
crowdsourced readability judgements compared to
expert judgements.

Crowdsourcing has also been used to gener-
ate translations. The recent statistical machine
translation-inspired approaches to automated sim-
plification motivate the possibility of using crowd-
sourcing to collect simplifications. Ambati and
Vogel (2010) and Zaidan and Callison-Burch
(2011) both demonstrate the feasibility of collect-
ing quality translations using AMT. Post et al.
(2012) generated parallel corpora between English
and six Indian languages using AMT.

3 The Need for a Corpus of Everyday
Documents

A high quality parallel corpus is necessary to drive
research in automated text simplification and eval-
uation. As shown in the Related Work section,
most statistically driven simplification systems
have used parallel Wikipedia - Simple Wikipedia
articles and Simple Wikipedia edit histories. The
resulting systems take Wikipedia articles as in-
put and generate simplified versions of those ar-

ticles. While this demonstrates the possibility of
automated text simplification, we believe that a
primary goal for simplification systems should
be to increase accessibility for those with poor
reading skills to the texts which are most impor-
tant to them. Creating a corpus of everyday doc-
uments will allow automated simplification tech-
niques to be applied to texts from this domain. In
addition, systems trained using Simple Wikipedia
only target a single reading level - that of Simple
Wikipedia. A corpus containing multiple different
simplifications at different reading levels for any
given original will allow text simplification sys-
tems to target specific reading levels.

The research needs that this corpus aims to meet
are:

• A large and accessible set of original every-
day documents to:

• provide a training and test set for auto-
mated text simplification

• A set of multiple human-generated simpli-
fications at different reading levels for the
same set of original documents to provide:

• accessible training data for automated
text simplification systems

• the ability to model how the same doc-
ument is simplified to different reading
levels

• An accessible location to share simplifica-
tions of the same documents that have been
generated by different systems to enable:

• comparative evaluation of the perfor-
mance of several systems

• easier identification and analysis of spe-
cific challenges common to all systems

4 Description of the Basic Corpus of
Everyday Documents

We have collected a first set of everyday docu-
ments. This will be extended to generate the cor-
pus described in the following section. The present
documents are heavily biased to the domain of
driving since they include driving test preparation
materials from all fifty U.S. states. This section
presents the information collected about each doc-
ument and its organisation in the basic corpus. The
basic corpus is available at: https://dialrc.
org/simplification/data.html.

86



4.1 Document Fields

Each document has a name which includes
information about the source, contents, and
type of document. For example the name
of the Alabama Driver Manual document is
al dps driver man. The corpus entry for each
document also includes the full title, the document
source (url for documents available online), the
document type and domain, the date retrieved, and
the date added to the corpus. For each document
the number of sentences, the number of words, the
average sentence length, the Flesch-Kincaid grade
level score, and the lexical (L) and grammatical
(G) reading level scores described in Heilman et
al. (2007) are also reported. An example of an en-
try for the Alabama Driver Manual is shown in
Table 1. The documents are split so that each sen-
tence is on a separate line to enable easy align-
ments between the original and simplified versions
of the documents.

Document Name al dps driver man
Full Title Alabama Driver Manual
Document Type Manual
Domain Driver’s Licensing
# Sentences 1,626
# Words 28,404
Avg. # words/sent 17.47
F-K Grade Level 10.21
Reading Level (L) 10
Reading Level (G) 8.38
Source http://1.usa.gov/1jjd4vw
Date Added 10/01/2013
Date Accessed 10/01/2013

Table 1: Example basic corpus entry for Alabama
Driver Manual

4.2 Corpus Statistics

There is wide variation between the different doc-
uments included in the corpus, across documents
from different domains and also for documents
from the same domain. This includes variability
in both document length and reading level. For ex-
ample, the driving manuals range from a lexical
reading level of 8.2 for New Mexico to 10.4 for
Nebraska. Table 2 shows the statistics for the dif-
ferent reading levels for the documents which have
been collected, using the lexical readability mea-
sure and rounding to the nearest grade level. Ta-
ble 3 shows the different domains for which docu-
ments have been collected and the statistics for the
documents in those domains.

Reading Level (L) # Documents # Sentences
4 1 23
5 0 0
6 4 200
7 1 695
8 6 1,869
9 30 36,783
10 54 83,123
11 4 1,457
12 1 461

Table 2: Corpus statistics by lexical reading level

5 Description of an Extended Corpus of
Everyday Documents

To meet the needs described in Section 3 the ba-
sic corpus will be extended significantly. We are
starting to collect more everyday documents from
each of the domains in the basic corpus and to
extend the corpus to other everyday document
domains including prescription instructions, ad-
vertising materials, mandatory educational test-
ing, and operating manuals for common products.
We are also collecting human-generated simpli-
fications for these documents. We will open up
the corpus for outside contributions of more doc-
uments, readability statistics and simplifications
generated by various human and automated meth-
ods. This section describes what the extended cor-
pus will contain and the preliminary work to gen-
erate simplified versions of the documents we
presently have.

5.1 Document Fields

The extended corpus includes both original doc-
uments and their simplified versions. The original
documents will include all the same information as
the basic corpus, listed in Section 4.1. Novel read-
ability measures for each document can be con-
tributed. For each readability measure that is con-
tributed, the name of the measure, document score,
date added, as well as relevant references to the
system used to calculate it will be included.

Multiple simplified versions of each original
document can be contributed. The simplification
for each sentence in the original document will be
on the same line in the simplified document as the
corresponding sentence in the original document.
Each simplified version will include a brief de-
scription of how it was simplified and relevant ref-
erences to the simplification method. As with the
original documents, the date added, optional com-
ments and the same document statistics and read-

87



Domain # Doc-
uments

Avg. #
Sentences

Avg. #
Words

Avg. #
words/sent

Total #
Sentences

Total #
Words

Avg. F-K
Grade Level

Avg. Read-
ability (L)

Avg. Read-
ability (G)

Driver’s
Licensing

60 1927.6 30,352.6 16.1 115,657 1,821,155 9.54 9.6 7.9

Vehicles 3 46.7 1,118.3 22.5 140 3355 13.3 8.2 7.9
Government
Documents

11 150 2,242.8 16.4 1650 24,671 10.5 8.6 8.4

Utilities 5 412.8 8,447.2 21.5 2,064 42,236 13.4 9.8 8.9
Banking 3 158 2,900 17.6 474 8,700 11.4 10.5 8.9
Leasing 4 101 2,386.8 23.8 404 9,547 13.7 9.0 8.7
Government
Aid

10 317.4 5,197.5 17.4 3,174 51,975 10.7 9.2 8.8

Shopping 3 281 5,266.7 19.7 843 15,800 12.2 9.9 9.0
Other 2 102.5 1,634 16.0 205 3268 9.7 8.8 8.2
All 101 1,233.8 19,611.0 17.2 124,611 1,980,707 10.4 9.4 8.2

Table 3: Corpus statistics for the basic corpus documents

ability metrics will be included. Additional read-
ability metrics can also be contributed and docu-
mented.

5.2 Generating Simplifications Using
Crowdsourcing

We conducted a preliminary study to determine
the feasibility of collecting simplifications using
crowdsourcing. We used AMT as the crowdsourc-
ing platform to collect sentence-level simplifica-
tion annotations for sentences randomly selected
from the basic corpus of everyday documents.

5.2.1 AMT Task Details
We collected 10 simplification annotations for
each of the 200 sentences which we posted in
two sets of Human Intelligence Tasks (HITs) to
AMT. Each HIT included up to four sentences and
included an optional comment box that allowed
workers to submit comments or suggestions about
the HIT. Workers were paid $0.25 for each HIT,
and 11 workers were given a $0.05 bonus for sub-
mitting comments which helped improve the task
design and remove design errors in the first itera-
tion of the HIT design. The first set of HITs was
completed in 20.5 hours and the second set in only
6.25 hours. The total cost for all 2000 simplifica-
tion annotations was $163.51 for 592 HITs, each
with up to four simplifications. The breakdown of
this cost is shown in Table 4.

Item Cost
592 HITs $148.00
11 bonuses $0.55
AMT fees $14.96
Total $163.51

Table 4: Breakdown of AMT costs

5.2.2 Quality Control Measures
To ensure quality, we provided a training session
which shows workers explanations, examples, and
counter-examples of multiple simplification tech-
niques. These include lexical simplification, re-
ordering, sentence splitting, removing unneces-
sary information, adding additional explanations,
and making no change for sentences that are al-
ready simple enough. One of the training examples
is the following:

Original Sentence: ”Do not use only parking lights, day or

night, when vehicle is in motion.”

Simplification: ”When your vehicle is moving do not use

only the parking lights. This applies both at night and dur-

ing the day.”

The explanations demonstrated how lexical sim-
plification, sentence splitting, and reordering tech-
niques were used.

The training session also tested workers’ abili-
ties to apply these techniques. Workers were given
four test sentences to simplify. Test 1 required lex-
ical simplification. Test 2 was a counter-example
of a sentence which did not require simplifica-
tion. Test 3 required sentence splitting. Test 4 re-
quired either moving or deleting an unclear modi-
fying clause. We chose the test sentences directly
from the corpus and modified them where neces-
sary to ensure that they contained the features be-
ing tested. Workers could take the training session
and submit answers as many times as they wanted,
but could not work on a task without first success-
fully completing the entire session. After complet-
ing the training session once, workers could com-
plete as many HITs as were available to them.

In addition to the training session, we blocked
submissions with empty or garbage answers (de-
fined as those with more than 15% of the words

88



not in a dictionary). We also blocked copy-paste
functions to discourage worker laziness. Workers
who submitted multiple answers that were either
very close to or very far from the original sentence
were flagged and their submissions were manually
reviewed to determine whether to approve them.
Similarity was measured using the ratio of Leven-
shtein distance to alignment length; Levenshtein
distance is a common, simple metric for measur-
ing the edit distance between two strings. The
Levenshtein ratio

(
1− Levenshtein dist.alignment length

)
provides

a normalised similarity measure which is robust
to length differences in the inputs. We also asked
workers to rate their confidence in each simplifi-
cation they submitted on a five point scale ranging
from “Not at all” to “Very confident”.

5.2.3 Effectiveness of Quality Control
Measures

To determine the quality of the AMT simplifica-
tions, we examine the effectiveness of the qual-
ity control measures described in the previous sec-
tion.
Training: In addition to providing training and
simplification experience to workers who worked
on the task, the training session effectively blocked
workers who were not able to complete it and
spammers. Of the 358 workers who looked at the
training session only 184 completed it (51%) and
we found that no bots or spammers had completed
the training session. Tables 5 and 6 show the per-
formance on the four tests in the training session
for workers who completed the training session
and for those who did not, respectively.

# of workers 181
Avg. # Attempts Test 1 1.1
Avg. # Attempts Test 2 1.5
Avg. # Attempts Test 3 1.6
Avg. # Attempts Test 4 1.4

Table 5: Training statistics for workers who com-
pleted training

# of workers 174
# Completed Test 1 82
# Completed Test 2 47
# Completed Test 3 1

Table 6: Training statistics for workers who did not
complete training

Blocking empty and garbage submissions:
Empty simplifications and cut-paste functions

were blocked using client-side scripts and we did
not collect statistics of how many workers at-
tempted either of these actions. One worker sub-
mitted a comment requesting that we do not block
copy-paste functions. In total only 0.6% of sub-
missions were detected as garbage and blocked.
Manual reviews: We (the first author) reviewed
workers who were automatically flagged five or
more times. We found that this was not an effective
way to detect work to be rejected since there were
many false positives and workers who did more
HITs were more likely to get flagged. None of the
workers flagged for review had submitted simpli-
fications that were rejected.

5.2.4 Evaluating Simplification Quality
To determine whether it is feasible to use crowd-
sourced simplifications to simplify documents for
the extended corpus, we examine the quality of
the simplifications submitted. The quality control
measures described in the previous sections are
designed to ensure that workers know what is
meant by simplification and how to apply some
simplification techniques, to block spammers, and
to limit worker laziness. However, workers were
free to simplify sentences creatively and encour-
aged to use their judgement in applying any tech-
niques that seem best to them.

It is difficult to verify the quality of the simplifi-
cation annotations that were submitted or to deter-
mine how to decide what simplification to chose
as the “correct” one for the corpus. For any given
sentence there is no “right” answer for what the
simplification should be; there are many different
possible simplifications, each of which could be
valid. For example, below is an original sentence
taken from a driving manual with two of the sim-
plifications that were submitted for it.

Original Sentence: ”Vehicles in any lane, except the right

lane used for slower traffic, should be prepared to move

to another lane to allow faster traffic to pass.”

Simplification 1: ”Vehicles that are not in the right lane

should be prepared to move to another lane in order to

allow faster traffic to pass.”

Simplification 2: ”Vehicles not in the right lane should be

ready to move to another lane so faster traffic can pass

them. The right lane is for slower traffic.”

There are a number of heuristics that could
be used to detect which simplifications are most
likely to be the best choice to use in the corpus.

The average time for workers to complete one
HIT of up to four simplifications was 3.85 min-

89



utes. This includes the time to complete the train-
ing session during a worker’s first HIT; excluding
this, we estimate the average time per HIT is ap-
proximately 2.75 minutes. Simplifications which
are completed in significantly less time, especially
when the original sentence is long, can be flagged
for review or simply thrown out if there are enough
other simplifications for the sentence.

Workers’ confidence in their simplifications can
also be used to exclude simplifications which were
submitted with low confidence (using worker con-
fidence as a quality control filter was explored by
Parent and Eskenazi (2010)). Table 7 shows the
statistics for the worker-submitted confidences.
Again, simplifications with very low confidence

Confidence Level # of answers
1 (Not at all) 9
2 (Somewhat confident) 143
3 (Neutral) 251
4 (Confident) 1030
5 (Very confident) 567

Table 7: Self-assessed worker confidences in their
simplifications

can either be reviewed or thrown out if there are
enough other simplifications for the sentence.

Worker agreement can also be used to detect
simplifications that are very different from those
submitted by other workers. Using the similarity
ratio of Levenshtein distance to alignment length,
we calculated which simplifications had at most
one other simplification with which they have a
similarity ratio above a specific threshold (here re-
ferred to as ‘outliers’). Table 8 reports how many
simplifications are outliers while varying the sim-
ilarity threshold. Since there are many different

Threshold 90% 85% 75% 65% 50%
# Outliers 1251 927 500 174 12

Table 8: Number of outlier simplifications with
similarity ratio above the threshold for at most one
other simplification

valid simplifications possible for any given sen-
tence this is not necessarily the best way to de-
tect poor quality submissions. For example, one
of the outliers, using the 50% threshold, was a
simplification of the sentence “When following a
tractor-trailer, observe its turn signals before try-
ing to pass” which simplified by using a negative
- “Don’t try to pass ... without ...”. This outlier
was the only simplification of this sentence which

used the negative but it is not necessarily a poor
one. However, the results in Table 7 do show that
there are many simplifications which are similar to
each other, indicating that multiple workers agree
on one simplification. One of these similar sim-
plifications could be used in the corpus, or multi-
ple different possible simplifications could be in-
cluded.

To further verify that usable simplifications can
be generated using AMT the first author manu-
ally reviewed the 1000 simplifications of 100 sen-
tences submitted for the first set of HITs. We
judged whether each simplification was grammat-
ical and whether it was a valid simplification. This
is a qualitative judgement, but simplifications were
judged to be invalid simplifications if they had sig-
nificant missing or added information compared
to the original sentence or added significant ex-
tra grammatical or lexical complexity for no ap-
parent reason. The remaining grammatical, valid
simplifications were judged as more simple, neu-
tral, or less simple than the original for each of the
following features: length, vocabulary, and gram-
matical structure. The results of this review are
shown in Table 9. These results show that approx-
imately 15% of the simplifications were ungram-
matical or invalid, further motivating the need to
use the other features, such as worker agreement
and confidence, to automatically remove poor sim-
plifications.

5.2.5 Extending the Corpus Using
Crowdsourcing

The preliminary work undertaken demonstrates
that it is feasible to quickly collect multiple sim-
plifications for each sentence relatively inexpen-
sively. We have also presented an evaluation of
the quality of the crowdsourced simplifications
and several methods of determining which sim-
plifications could be used in the extended corpus.
More work is still needed to determine the most
cost effective way of getting simplification results
that are of sufficient quality to use without gather-
ing overly redundant simplifications for each sen-
tence. Additionally, simplifications of more sen-
tences are needed to assess improvements in read-
ing level since the reading level measures we use
are not accurate for very short input texts.

90



Un-
grammatical

Invalid
(excludes
ungrammatical)

Simpler
vocabulary

Less
simple
vocabulary

Equivalent
vocabulary

Grammatically
simpler

Less
grammatically
simple

Equivalent
grammar

Longer Shorter Same
length

35 122 383 21 596 455 21 524 99 537 364

Table 9: Manual evaluation of 1000 AMT simplifications. Numbers of simplifications with each feature.

6 Contributing to & Accessing the
Corpus

6.1 Contributing to the Extended Corpus
The following items can be contributed to the cor-
pus: original everyday copyright-free documents,
manual or automated simplifications of the orig-
inal documents (or parts of the documents), and
readability scores for original or simplified docu-
ments.

Original documents submitted to the corpus can
be from any domain. Our working definition of an
everyday document is any document which peo-
ple may have a need to access in their everyday
life. Examples include government and licensing
forms and their instructions, banking forms, pre-
scription instructions, mandatory educational test-
ing, leasing and rental agreements, loyalty pro-
gram sign-up forms and other similar documents.
We excluded Wikipedia pages because we found
that many article pairs actually had few parallel
sentences. Documents should be in English and of
North American origin to avoid dialect-specific is-
sues.

Hand generated or automatically generated sim-
plifications of everyday documents are also wel-
come. They should be accompanied the informa-
tion detailed in Section 5.1. The document statis-
tics listed in Sections 4 and 5 will be added for
each simplified document.

Readability scores can be contributed for any of
the documents.They should also include the infor-
mation detailed in Section 5.1 and pertinent infor-
mation about the system that generated the scores.

6.2 Accessing the Extended Corpus
The extended corpus will be made publicly acces-
sible at the same location as the basic corpus. The
names and statistics of each of the documents will
be tabulated and both the original and simplified
documents, and their statistics, will be available to
download. Users will submit their name or organi-
zational affiliation along with a very brief descrip-
tion of how they plan to use the data. This will
allow us to keep track of how the corpus is be-
ing used and how it could be made more useful to

those researching simplification.
The goal of this corpus is to make its contents as

accessible as possible. However, many of the orig-
inal documents from non-governmental sources
may not be freely distributed and will instead be
included under a data license, unlike the remain-
der of the corpus and the simplifications1.

7 Conclusions & Future Work

In this paper we have given the motivation for cre-
ating a large and publicly accessible corpus of ev-
eryday documents and their simplifications. This
corpus will advance research into automated sim-
plification and evaluation for everyday documents.
We have already collected a basic corpus of every-
day documents and demonstrated the feasibility of
collecting large numbers of simplifications using
crowdsourcing. We have defined what information
the extended corpus will contain and how contri-
butions can be made to it.

There is significantly more work which must be
completed in the future to create an extended cor-
pus which meets the needs described in this paper.
There are three tasks that we plan to undertake in
order to complete this corpus: we will collect sig-
nificantly more everyday documents; we will man-
age a large crowdsourcing task to generate simpli-
fications for thousands of the sentences in these
documents; and we will create a website to enable
access and contribution to the extended simplifi-
cation corpus. By making this work accessible we
hope to motivate others to contribute to the corpus
and to use it to advance automated text simplifica-
tion and evaluation techniques for the domain of
everyday documents.

Acknowledgments

The authors would like to thank the anonymous
reviewers for their detailed and helpful feedback
and comments on the paper.

1Thanks to Professor Jamie Callan for explaining some
of the issues with including these types of documents in our
dataset.

91



References
Sandra Aluisio and Caroline Gasperin. 2010. Foster-

ing digital inclusion and accessibility: The porsim-
ples project for simplification of portuguese texts.
In Proc. of the NAACL HLT 2010 Young Investi-
gators Workshop on Computational Approaches to
Languages of the Americas, pages 46–53. Associa-
tion for Computational Linguistics.

Vamshi Ambati and Stephan Vogel. 2010. Can crowds
build parallel corpora for machine translation sys-
tems? In Proc. of the NAACL HLT 2010 Workshop
on Creating Speech and Language Data with Ama-
zon’s Mechanical Turk, pages 62–65. Association
for Computational Linguistics.

Regina Barzilay and Noemie Elhadad. 2003. Sentence
alignment for monolingual comparable corpora. In
Proc. of the 2003 Conference on Empirical Meth-
ods in Natural Language Processing, EMNLP ’03,
pages 25–32. Association for Computational Lin-
guistics.

Stefan Bott, Horacio Saggion, and David Figueroa.
2012. A hybrid system for spanish text simplifi-
cation. In Proc. of the Third Workshop on Speech
and Language Processing for Assistive Technolo-
gies, pages 75–84. Association for Computational
Linguistics.

R. Chandrasekar, Christine Doran, and B. Srinivas.
1996. Motivations and methods for text simplifica-
tion. In Proc. of the 16th Conference on Compu-
tational Linguistics - Volume 2, COLING ’96, pages
1041–1044. Association for Computational Linguis-
tics.

Orphée De Clercq, Veronique Hoste, Bart Desmet,
Philip van Oosten, Martine De Cock, and Lieve
Macken. 2014. Using the crowd for readabil-
ity prediction. Natural Language Engineering,
FirstView:1–33.

William H. DuBay. 2004. The Princi-
ples of Readability. Costa Mesa, CA:
Impact Information, http://www.impact-
information.com/impactinfo/readability02.pdf.

Dan Feblowitz and David Kauchak. 2013. Sentence
simplification as tree transduction. In Proc. of the
Second Workshop on Predicting and Improving Text
Readability for Target Reader Populations, pages 1–
10. Association for Computational Linguistics.

Lijun Feng, Martin Jansche, Matt Huenerfauth, and
Noémie Elhadad. 2010. A comparison of fea-
tures for automatic readability assessment. In Col-
ing 2010: Posters, pages 276–284. Coling 2010 Or-
ganizing Committee.

Thomas François and Eleni Miltsakaki. 2012. Do nlp
and machine learning improve traditional readability
formulas? In Proc. of the First Workshop on Predict-
ing and Improving Text Readability for target reader
populations, pages 49–57. Association for Compu-
tational Linguistics.

Michael Heilman, Kevyn Collins-Thompson, Jamie
Callan, and Maxine Eskenazi. 2007. Combin-
ing lexical and grammatical features to improve
readability measures for first and second language
texts. In HLT-NAACL 2007: Main Proceedings,
pages 460–467. Association for Computational Lin-
guistics.

J. Peter Kincaid, Robert P. Fishburne Jr., Richard L.
Rogers, and Brad S. Chissom. 1975. Derivation
of new readability formulas (automated readability
index, fog count and flesch reading ease formula)
for navy enlisted personnel. Technical report, Naval
Technical Training Command, Millington Tn.

David Klaper, Sarah Ebling, and Martin Volk. 2013.
Building a german/simple german parallel corpus
for automatic text simplification. In Proc. of the
Second Workshop on Predicting and Improving Text
Readability for Target Reader Populations, pages
11–19. Association for Computational Linguistics.

Sigrid Klerke and Anders Søgaard. 2012. Dsim, a dan-
ish parallel corpus for text simplification. In Proc.
of the Eighth Language Resources and Evaluation
Conference (LREC 2012), pages 4015–4018. Euro-
pean Language Resources Association (ELRA).

Gabriel Parent and Maxine Eskenazi. 2010. Toward
better crowdsourced transcription: Transcription of
a year of the let’s go bus information system data.
In SLT, pages 312–317. IEEE.

Sarah E Petersen and Mari Ostendorf. 2007. Text sim-
plification for language learners: a corpus analysis.
In Proc. of Workshop on Speech and Language Tech-
nology for Education, pages 69–72.

Matt Post, Chris Callison-Burch, and Miles Osborne.
2012. Constructing parallel corpora for six indian
languages via crowdsourcing. In Proc. of the Sev-
enth Workshop on Statistical Machine Translation,
pages 401–409. Association for Computational Lin-
guistics.

Advaith Siddharthan and Napoleon Katsos. 2012. Of-
fline sentence processing measures for testing read-
ability with users. In Proc. of the First Workshop
on Predicting and Improving Text Readability for
target reader populations, pages 17–24. Association
for Computational Linguistics.

Advaith Siddharthan. 2006. Syntactic simplification
and text cohesion. Research on Language and Com-
putation, 4(1):77–109.

Irina Temnikova and Galina Maneva. 2013. The c-
score – proposing a reading comprehension metrics
as a common evaluation measure for text simplifica-
tion. In Proc. of the Second Workshop on Predicting
and Improving Text Readability for Target Reader
Populations, pages 20–29. Association for Compu-
tational Linguistics.

92



Sowmya Vajjala and Detmar Meurers. 2012. On im-
proving the accuracy of readability classification us-
ing insights from second language acquisition. In
Proc. of the Seventh Workshop on Building Educa-
tional Applications Using NLP, pages 163–173. As-
sociation for Computational Linguistics.

Sanja Štajner, Richard Evans, Constantin Orasan, , and
Ruslan Mitkov. 2012. What can readability mea-
sures really tell us about text complexity? In Proc.
of the Workshop on Natural Language Processing
for Improving Textual Accessibility (NLP4ITA).

Kristian Woodsend and Mirella Lapata. 2011. Wik-
isimple: Automatic simplification of wikipedia arti-
cles. In Proc. of the Twenty-Fifth AAAI Conference
on Artificial Intelligence, pages 927–932.

Omar F. Zaidan and Chris Callison-Burch. 2011.
Crowdsourcing translation: Professional quality
from non-professionals. In Proc. of the 49th An-
nual Meeting of the Association for Computational
Linguistics: Human Language Technologies, pages
1220–1229. Association for Computational Linguis-
tics.

Zhemin Zhu, Delphine Bernhard, and Iryna Gurevych.
2010. A monolingual tree-based translation model
for sentence simplification. In Proc. of the 23rd In-
ternational Conference on Computational Linguis-
tics (Coling 2010), pages 1353–1361.

93


