



















































JoBimViz: A Web-based Visualization for Graph-based Distributional Semantic Models


Proceedings of ACL-IJCNLP 2015 System Demonstrations, pages 103–108,
Beijing, China, July 26-31, 2015. c©2015 ACL and AFNLP

JOBIMVIZ: A Web-based Visualization
for Graph-based Distributional Semantic Models

Eugen Ruppert and Manuel Kaufmann and Martin Riedl and Chris Biemann
FG Language Technology

Computer Science Department, Technische Universität Darmstadt,
Hochschulsstrasse 10, D-62489 Darmstadt, Germany

{eugen.ruppert,riedl,biem}@cs.tu-darmstadt.de, mtk@kisad.de

Abstract

This paper introduces a web-based visual-
ization framework for graph-based distri-
butional semantic models. The visualiza-
tion supports a wide range of data struc-
tures, including term similarities, sim-
ilarities of contexts, support of multi-
word expressions, sense clusters for terms
and sense labels. In contrast to other
browsers of semantic resources, our visu-
alization accepts input sentences, which
are subsequently processed with language-
independent or language-dependent ways
to compute term-context representations.
Our web demonstrator currently contains
models for multiple languages, based on
different preprocessing such as depen-
dency parsing and n-gram context repre-
sentations. These models can be accessed
from a database, the web interface and via
a RESTful API. The latter facilitates the
quick integration of such models in re-
search prototypes.

1 Introduction

Statistical semantics has emerged as a field of
computational linguistics, aiming at automatically
computing semantic representations for words,
sentences and phrases from (large) corpora. While
early approaches to distributional semantics were
split into symbolic, graph-based approaches (Lin,
1998) and vector-based approaches (Schütze,
1993), recent trends have mainly concentrated on
optimizing the representation of word meanings in
vector spaces and how these account for composi-
tionality (cf. Baroni and Lenci (2010); Turney and
Pantel (2010)).

While dense vector representations, obtained by
singular value decomposition (cf. Rapp (2002))
or neural embeddings (Mikolov et al., 2010), have
gained popularity due to successes in modelling
semantic and relational similarity, we propose to
revisit graph-based approaches to distributional
semantics in the tradition of Lin (1998), Curran
(2002) and Biemann and Riedl (2013) – at least as
an additional alternative – for the following rea-
sons:

• (dense) vector representations are not inter-
pretable, thus it cannot be traced why two
terms are similar

• vectors do not make word senses explicit, but
represent ambiguous words as a mix of their
senses

• graph-based models can be straightforwardly
structured and extended, e.g. to represent tax-
onomic or other relationships

In this demonstration paper, we describe JO-
BIMVIZ, a visualization and interactive demon-
strator for graph-based models of distributional
semantics. Our models comprise similarities be-
tween terms (a.k.a. distributional thesaurus) and
multiword units, similarities between context fea-
tures, clustered word senses and their labeling
with taxonomic is-a relations. The demonstra-
tor transforms input sentences into a term-context
representation and allows to browse parts of the
underlying model relevant to the sentence at hand.
Requests are handled through a RESTful API,
which allows to use all available information
in custom prototypes via HTTP requests. The
demonstrator, as well as the computation of the
models, is fully available open source under a per-
missive license.

103



2 Related Work

Aside from providing a convenient lookup for hu-
man users, the visualization of semantic models
provides an important tool for debugging models
visually. Additionally, prototyping of semantic ap-
plications based on these models is substantially
accelerated.

VISUALSYNONYMS1 and THESAURUS.COM2

offer lookup possibilities to retrieve various in-
formation for input words. Users can view in-
formation, like WordNet or Wikipedia definitions
and related words (synonyms, hypernyms, etc.).
BABELNET3 (Navigli and Ponzetto, 2012) uses
such information to compile multilingual defini-
tions and translations for input words. Here, the
words are differentiated by senses, with taxonomi-
cal labels. BABELNET offers a SPARQL endpoint
and APIs for web access.

SERELEX4 (Panchenko et al., 2013) is a graph-
ical thesaurus viewer. Users can enter a term
for different languages and retrieve related words.
The similarity graph displays the similarity links
between similar items (‘secondary links’). The
items can be expanded for a denser graph. The
input terms map to nominal phrases, allowing the
interface to display short definitions and disam-
biguations from Wikipedia. An API with JSON
output for similar words is provided.

SKETCH ENGINE5 (Kilgarriff et al., 2004) of-
fers access to pre-processed corpora. For each
corpus, the user can view concordances and sim-
ilar terms (thesaurus) for a given term. SKETCH
ENGINE also features statistical information, like
word frequency and co-occurrence counts. Fur-
thermore it shows meta information for a corpus.
One drawback of the SKETCH ENGINE is that the
tools and the models are not freely available.

NETSPEAK6 (Stein et al., 2010) is a search en-
gine for words in context. Access is possible via a
graphical UI, a RESTful interface and a Java API.
Users can enter wildcards and other meta sym-
bols into the input phrases and thus retrieve all the
words and phrases that occur in a given context.
The information is displayed with corpus statis-
tics.

1http://www.visualsynonyms.com
2http://www.thesaurus.com
3http://babelnet.org/
4http://serelex.org/
5http://www.sketchengine.co.uk/
6http://www.netspeak.org/

A novel aspect of JOBIMVIZ is that it in-
corporates several different aspects of symbolic
methods (distributional thesaurus, context feature
scores, sense clusters), and all of these methods
are derived from the input corpus alone, without
relying on external resources, which are not avail-
able for every language/domain. Furthermore, we
provide domain-based sense clusterings with is-a
labels (cf. Figure 4), which is not performed by
the other discussed tools.

Our interface features a generic interactive vi-
sualization that is adaptable to different kinds of
parses and also handles multiword units in the vi-
sualization. All of this information is made freely
available by the API, enabling rapid prototyp-
ing techniques. To our knowledge, the presented
demonstrator is the only online tool that combines
technical accessibility (open source project, open
API) with rich, flexible preprocessing options (cf.
Section 3) and graph-based, structured semantic
models that contain context similarities.

3 Computation of distributional models

The visualization is based on distributional mod-
els computed with the JoBimText framework (Bie-
mann and Riedl, 2013)7; however it can also be
used for other semantic models of similar struc-
ture. One of the major components of the frame-
work is a method for the computation of distribu-
tional thesauri. This method consists of two steps:
a holing operation and a similarity computation.

The holing operation processes text and yields
a representation consisting of jos and bims. Jos
and bims are normally instantiated by a term (jo)
and its context features (bims), but the definition
extends to arbitrary splits of the input perception
that mutually characterize each other distribution-
ally. The holing operation executes a preprocess-
ing pipeline that can be as simple as text segmen-
tation or complex like POS tagging and depen-
dency parsing, depending on the complexity of the
context features. As the preprocessing is defined
in UIMA (Ferrucci and Lally, 2004) pipeline de-
scriptors, it is straightforward to exchange com-
ponents or define new preprocessing operations.
Using this processed and annotated text, the hol-
ing annotator generates the term–feature represen-
tation of the input text, e.g. by using the neighbor-
ing words (‘trigram holing’) or dependency paths

7The framework is available under the permissive ASL
2.0 license at http://sf.net/p/jobimtext.

104



Request-/
Reply
Queue

DatabaseAPI-Workers

Holing-
workers

UIMA-
Pipeline

Java EE Webserver

RESTful
API

User

GUI

Figure 1: Architecture of JOBIMVIZ, with the
three components GUI, Web Server and Workers.

between terms (‘dependency holing’). A graphi-
cal example is given in Figure 5, where the holing
operation yields four context features for the term
example#NN. Different term representations are
possible, like surface text, lemma, lemma+POS
and also multiword expressions. This flexibility
allows, on the one hand, domain- and language-
independent similarity computations using general
holing operations, while on the other hand allow-
ing complex, language-specific processing in the
holing operations for higher quality models (e.g.
using parsing and lemmatization).

The second part consists of the similarity com-
putation, which relies on MapReduce (Dean and
Ghemawat, 2004) for scalability and employs ef-
ficient pruning strategies to keep runtime feasible
even for very large corpora. After the computa-
tion of similarities, sense clusters for each term
are computed using Chinese Whispers graph clus-
tering (Biemann, 2006), as described in Biemann
et al. (2013). Furthermore, the sense clusters are
also labeled with hypernyms (is-a relations) de-
rived from Hearst patterns (Hearst, 1992), imple-
mented using the UIMA Ruta (Kluegl et al., 2014)
pattern matching engine8.

4 Web-based Demonstrator

4.1 Architecture and Technology

The architecture of JOBIMVIZ consists of three
main components (see Figure 1). The central el-
ement is a Java EE based web server, which pro-
vides a RESTful interface (Fielding, 2000) for ac-
cessing API resources, such as sentence holing op-
erations and the distributional models.

To handle many parallel requests for long run-
ning holing operations like dependency parsing,
we use an Apache ActiveMQ9 based request/reply

8https://uima.apache.org/ruta.html
9http://activemq.apache.org/

queue. All requests to the web server are stored
in the queue and processed by one of the worker
processes, which can be distributed on differ-
ent machines and handle the requests in parallel.
These workers form the second component of our
system. The holing workers execute the UIMA
pipelines that define the holing operations. Us-
age of UIMA descriptors provides great flexibil-
ity, since one type of workers can run every holing
operation. Every model defines a custom UIMA
pipeline to ensure the same holing operation for
the input and the model. To speed up the hol-
ing operation we cache frequently queried holing
outputs into the in-memory database Redis10. Re-
quests to the API are processed by another type of
workers, which retrieve the relevant data from the
models database.

The third component of the software is a
HTML5-based GUI with an overall layout based
on the Bootstrap11 framework. The front-end uses
Ajax requests to connect to the RESTful interface
of the web server and retrieves responses in the
JSON format. The received data is processed by a
Javascript application, which uses D3.js (Bostock
et al., 2011) to visualize the graphs.

4.2 Visualization

In the demonstrator, users can enter sentences or
phrases, run different holing operations on the in-
put and browse distributional models. The demon-
strator can be accessed online12.

Figure 2 shows the application layout. First,
the user can decide on the model, which consists
of a corpus and a holing operation, and select it
via the dropdown holing operation selector (3).
Then, the user enters a sentence in the text field
(1) and activates the processing by clicking the
Parse button (2). The holing output is presented
as a graph (4) with marked terms and context fea-
tures. Other views are available in tab bar at the
top (5). To retrieve term similarities, a term in
the displayed sentence can be selected (4a), acti-
vating the information boxes (6, 7, 8, 9). Context
similarities can be viewed by selecting the corre-
sponding feature arc (4b). The frequency of the
selected term/feature is presented on the top right
(6). Similar items are displayed in the first box
in the lower pane (7). Similarity scores between

10http://redis.io
11http://www.getbootstrap.com/
12http://goo.gl/V2ZEly

105



Figure 2: Overview of the visualization with a collapsed dependency parse of the input sentence This is
an example for the paper; the selected term is paper#NN.

Figure 3: Similar bims (left) and most signifi-
cant jos (right) for the dependency parse context
give#VB-dobj (direct object of give).

the selected and similar terms are shown, includ-
ing self-similarities as an upper limit for similarity
of the selected item.

The most relevant context features for the term
are displayed with the significance score and their
term-feature count in the corpus (8). When select-
ing a context feature, the most relevant terms for
a context feature are shown (cf. Figure 3). For
terms, there is a box displaying sense clusters (9).
These are often available in different granularities
to match the application requirements (e.g. ‘CW’
or ‘CW-finer’)13. When a user selects a sense clus-
ter, a list of related terms for the selected cluster
and a list of hypernyms (is-a relations) with fre-
quency scores are displayed (cf. Figure 4). But-
tons for API calls are displayed for all data display
in the GUI (10). This enables users to get comfort-
able with the models and the API before deploying
it in an application. The buttons feature selectors

13Here, ‘CW’ is referring to sense clusters computed with
Chinese Whispers (Biemann, 2006).

Figure 4: Different word senses for paper, with
hypernym terms (sense 1 paper:publication, sense
2 paper:material), as accessed from field (9) in
Figure 2. The tabs “CW/CW-finer” provide access
to different sense clustering inventories, e.g. with
a different granularity.

for different output data format options, i.e. TSV,
XML, JSON and RDF. For the boxes with list con-
tent, there is a ‘maximize’ button next to the API
button that brings up a screen-filling overlay.

4.2.1 Sentence Holing Operations

For the graphical representation of holing opera-
tions, the web demo offers views for single terms
(Figure 5 and 6) as well as support for n-grams
(Figure 7). Figure 5 shows the tree representation
for a dependency parser using collapsed depen-
dencies. Figure 6 exemplifies a holing operation
considering the left and right neighboring words
as one context feature (‘trigram holing’). Figure 7
shows the result of the same holing operation, ap-
plied for n-grams, where several different possi-
ble left and right multiword items can be selected
as context. Here, the demonstrator identified mul-

106



Figure 5: Collapsed dependency (de Marneffe et
al., 2006) holing result for This is an example for
the paper; the preposition for is collapsed into the
prep for dependency.

Figure 6: Trigram holing (unigram) result for This
is an example for the paper.

tiword expressions that are present in the corre-
sponding distributional model (acute lymphoblas-
tic leukemia, lymphoblastic leukemia cells and hu-
man bones). These expressions can be selected
like single word items. Furthermore, there is a fil-
tering function for n-grams, where users can refine
the display of n-grams by selecting the desired n-
gram lengths.

4.2.2 Model Access
The demonstrator features a selection of mod-
els for different languages (currently: German,
English, Hindi, Bengali) and different domains,
like news, encyclopedia or medical domain. Be-
sides term similarities, typical context features and
sense clusters are also part of these models. Dis-
tributional similarities for context features can be
viewed as well. By selecting an arc that represents
the feature relation, the user can view similar fea-
tures in the GUI. In Figure 3, the context features

Figure 7: Trigram holing (n-gram) result for mi-
gration of acute lymphoblastic leukemia cells into
human bones with display of multiwords that are
part of the model.

that are most similar to give#VB#-dobj (direct
object of verb give) are displayed. Here, the most
significant words for a feature are shown. To our
knowledge, we are the first ones to explicitly pro-
vide similarities of contexts in distributional se-
mantic models.

Holing operations and models can be accessed
via an open RESTful API. The access URIs con-
tain the model identifier (consisting of dataset and
holing operation), the desired method, like sen-
tence holing or similar terms, and the input sen-
tence, term or context feature. The distributional
project also features a Java API to access models
via the web-based API14.

5 Conclusion and Future Work

In this paper we have introduced a new web-based
tool that can be used to browse and to access
graph-based semantic models based on distribu-
tional semantics. In its current form, it can dis-
play data from a distributional thesaurus, simi-
larities of context features, sense clusters labeled
with taxonomic relations, and provides the dis-
play of multiword expressions. Additionally, it
provides the functionality to transform sentences
into term–context representations. The web demo
can give a first impression for people who are
interested in the JoBimText framework for dis-
tributional semantics. Providing a RESTful in-
terface for accessing all information with state-

14For an overview of available API methods, see http:
//goo.gl/l6K6Gu.

107



less requests allows for an easy integration into
prototypes. The RESTful API can also be ac-
cessed using our Java API, which also can access
other back-ends such as on-disk and in-memory
databases. The complete project is available under
the permissive Apache ASL 2.0 license and mod-
els for several languages are available for down-
load15.

Whereas at the moment similar terms are glob-
ally ranked, we will add visualization support for
a contextualization method, in order to rank sim-
ilar terms regarding their context within the sen-
tence. Furthermore, we are working on incorpo-
rating more complex pre-processing for the hol-
ing operation in the visualization, e.g. aggregating
context features over co-reference chains, as well
as relation extraction and frame-semantic parsing
for term–context representations.

Acknowledgments

This work has been supported by the German Fed-
eral Ministry of Education and Research (BMBF)
within the context of the Software Campus project
LiCoRes under grant No. 01IS12054 and by an
IBM Shared University Research award.

References
Marco Baroni and Alessandro Lenci. 2010. Distributional

memory: A general framework for corpus-based seman-
tics. Computational Linguistics, 36(4):673–721.

Chris Biemann and Martin Riedl. 2013. Text: Now in 2D! a
framework for lexical expansion with contextual similar-
ity. Journal of Language Modelling, 1(1):55–95.

Chris Biemann, Bonaventura Coppola, Michael R. Glass, Al-
fio Gliozzo, Matthew Hatem, and Martin Riedl. 2013.
JoBimText Visualizer: A graph-based approach to con-
textualizing distributional similarity. In Proc. TextGraphs
2013, pages 6–10, Seattle, Washington, USA.

Chris Biemann. 2006. Chinese Whispers – an efficient
graph clustering algorithm and its application to natural
language processing problems. In Proc. TextGraphs 2006,
pages 73–80, New York City, NY, USA.

Michael Bostock, Vadim Ogievetsky, and Jeffrey Heer. 2011.
D3: Data-driven documents. IEEE Transactions on Visu-
alization & Computer Graphics, 17(12):2301–2309.

James R. Curran. 2002. Ensemble methods for automatic
thesaurus extraction. In Proc. EMNLP’02/ACL-2002,
pages 222–229, Philadelphia, PA, USA.

15Selection of models available under http:
//sf.net/projects/jobimtext/files/data/
models/.

Marie-Catherine de Marneffe, Bill MacCartney, and Christo-
pher D. Manning. 2006. Generating typed dependency
parses from phrase structure parses. In Proc. LREC-2006,
pages 449–454, Genova, Italy.

Jeffrey Dean and Sanjay Ghemawat. 2004. MapReduce:
Simplified Data Processing on Large Clusters. In Proc.
OSDI ’04, pages 137–150, San Francisco, CA, USA.

David Ferrucci and Adam Lally. 2004. UIMA: An Architec-
tural Approach to Unstructured Information Processing in
the Corporate Research Environment. Natural Language
Engineering, 10(3-4):327–348.

Roy Thomas Fielding. 2000. Architectural Styles and the
Design of Network-based Software Architectures. Doc-
toral dissertation, University of California, Irivne.

Marti A. Hearst. 1992. Automatic acquisition of hyponyms
from large text corpora. In Proc. COLING-1992, pages
539–545, Nantes, France.

Adam Kilgarriff, Pavel Rychlý, Pavel Smrž, and David Tug-
well. 2004. The Sketch Engine. In Proc. EURALEX
2004, pages 105–116, Lorient, France.

Peter Kluegl, Martin Toepfer, Philip-Daniel Beck, Georg
Fette, and Frank Puppe. 2014. UIMA Ruta: Rapid devel-
opment of rule-based information extraction applications.
Natural Language Engineering.

Dekang Lin. 1998. Automatic retrieval and clustering of
similar words. In Proc. COLING-98, pages 768–774,
Montréal, Quebec, Canada.

Tomas Mikolov, Martin Karafiát, Lukas Burget, Jan Cer-
nockỳ, and Sanjeev Khudanpur. 2010. Recurrent neural
network based language model. In Proc. INTERSPEECH
2010, pages 1045–1048, Makuhari, Chiba, Japan.

Roberto Navigli and Simone P. Ponzetto. 2012. BabelNet:
The automatic construction, evaluation and application of
a wide-coverage multilingual semantic network. Artificial
Intelligence, 193:217–250.

Alexander Panchenko, Pavel Romanov, Olga Morozova, Hu-
bert Naets, Andrey Philippovich, Alexey Romanov, and
Fairon Cédrick. 2013. Serelex: Search and visualization
of semantically related words. In Proc. ECIR 2013, pages
837–840, Moscow, Russia.

Reinhard Rapp. 2002. The computation of word asso-
ciations: Comparing syntagmatic and paradigmatic ap-
proaches. In Proc. COLING ’02, pages 1–7, Taipei, Tai-
wan.

Hinrich Schütze. 1993. Word space. In Advances in Neural
Information Processing Systems 5, pages 895–902, Den-
ver, Colorado, USA.

Benno Stein, Martin Potthast, and Martin Trenkmann. 2010.
Retrieving Customary Web Language to Assist Writers.
In Advances in Information Retrieval, ECIR 10, pages
631–635, Milton Keynes, UK.

Peter D. Turney and Patrick Pantel. 2010. From frequency
to meaning: Vector space models of semantics. Journal of
artificial intelligence research, 37(1):141–188.

108


