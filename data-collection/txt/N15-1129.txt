



















































Multi-Task Word Alignment Triangulation for Low-Resource Languages


Human Language Technologies: The 2015 Annual Conference of the North American Chapter of the ACL, pages 1221–1226,
Denver, Colorado, May 31 – June 5, 2015. c©2015 Association for Computational Linguistics

Multi-Task Word Alignment Triangulation for Low-Resource Languages

Tomer Levinboim and David Chiang
Department of Computer Science and Engineering

University of Notre Dame
{levinboim.1,dchiang}@nd.edu

Abstract

We present a multi-task learning approach
that jointly trains three word alignment mod-
els over disjoint bitexts of three languages:
source, target and pivot. Our approach builds
upon model triangulation, following Wang et
al., which approximates a source-target model
by combining source-pivot and pivot-target
models. We develop a MAP-EM algorithm
that uses triangulation as a prior, and show
how to extend it to a multi-task setting. On
a low-resource Czech-English corpus, using
French as the pivot, our multi-task learning ap-
proach more than doubles the gains in both F-
and Bleu scores compared to the interpolation
approach of Wang et al. Further experiments
reveal that the choice of pivot language does
not significantly affect performance.

1 Introduction

Word alignment (Brown et al., 1993; Vogel et al.,
1996) is a fundamental task in the machine transla-
tion (MT) pipeline. To train good word alignment
models, we require access to a large parallel corpus.
However, collection of parallel corpora has mostly
focused on a small number of widely-spoken lan-
guages. As such, resources for almost any other pair
are either limited or non-existent.

To improve word alignment and MT in a low-
resource setting, we design a multitask learning
approach that utilizes parallel data of a third lan-
guage, called the pivot language (§3). Specifi-
cally, we derive an efficient and easy-to-implement
MAP-EM-like algorithm that jointly trains source-
target, source-pivot and pivot-target alignment mod-
els, each on its own bitext, such that each model ben-
efits from observations made by the other two.

Our method subsumes the model interpolation ap-
proach of Wang et al. (2006), who independently

train these three models and then interpolate the
source-target model with an approximate source-
target model, constructed by combining the source-
pivot and pivot-target models.

Pretending that Czech-English is low-resource,
we conduct word alignment and MT experi-
ments (§4). With French as the pivot, our approach
significantly outperforms the interpolation method
of Wang et al. (2006) on both alignment F- and Bleu
scores. Somewhat surprisingly, we find that our ap-
proach is insensitive to the choice of pivot language.

2 Triangulation and Interpolation

Wang et al. (2006) focus on learning a word align-
ment model without a source-target corpus. To do
so, they assume access to both source-pivot and
pivot-target bitexts on which they independently
train a source-pivot word alignment model Θsp and a
pivot-target model Θpt. They then combine the two
models by marginalizing over the pivot language, re-
sulting in an approximate source-target model Θs̃t.
This combination process is referred to as triangu-
lation (see §5).

In particular, they construct the triangulated
source-target t-table ts̃t from the source-pivot and
pivot-target t-tables tsp, tpt using the following ap-
proximation:

ts̃t(t | s) =
∑

p

t(t | p, s) · t(p | s)

≈
∑

p

tpt(t | p) · tsp(p | s) (1)

Subsequently, if a source-target corpus is available,
they train a standard source-target model Θst, and
tune the interpolation

t̂st = λinterptst + (1 − λinterp)ts̃t
with respect to λinterp to reduce alignment error rate
(Koehn, 2005) over a hand-aligned development set.

1221



Wang et al. (2006) propose triangulation heuris-
tics for other model parameters; however, in this pa-
per, we consider only t-table triangulation.

3 Our Method

We now discuss two approaches that better exploit
model triangulation. In the first, we use the triangu-
lated t-table to construct a prior on the source-target
t-table. In the second, we place a prior on each of
the three models and train them jointly.

3.1 Triangulation as a Fixed Prior
We first propose to better utilize the triangulated t-
table ts̃t (Eq. 1) by using it to construct an informa-
tive prior for the source-target t-table tst ∈ Θst.

Specifically, we modify the word alignment gen-
erative story by placing Dirichlet priors on each of
the multinomial t-table distributions tst(· | s):

tst(· | s) ∼ Dirichlet(αs) for all s. (2)
Here, each αs = (. . . , αst, . . .) denotes a hyperparam-
eter vector which will be defined shortly.

Fixing this prior, we optimize the model posterior
likelihood P(Θst | bitextst) to find a maximum-a-
posteriori (MAP) estimate. This is done according
the MAP-EM framework (Dempster et al., 1977),
which differs slightly from standard EM. The E-
step remains as is: fixing the model Θst, we collect
expected counts E[c(s, t)] for each decision in the
generative story. The M-step is modified to max-
imize the regularized expected complete-data log-
likelihood with respect to the model parameters Θst,
where the regularizer corresponds to the prior.

Due to the conjugacy of the Dirichlet priors with
the multinomial t-table distributions, the sole modi-
fication to the regular EM implementation is in the
M-step update rule of the t-table parameters:

tst(t | s) = E[c(s, t)] + αst − 1∑
t(E[c(s, t)] + αst − 1) (3)

where E[c(s, t)] is the expected number of times
source word s aligns with target word t in the source-
target bitext. Moreover, through Eq. 3, we can view
αst − 1 as a pseudo-count for such an alignment.

To define the hyperparameter vector αs we de-
compose it as follows:

αs = Cs · ms + 1 (4)

where Cs > 0 is a scalar parameter, ms is a proba-
bility vector, encoding the mode of the Dirichlet and
1 denotes an all-one vector. Roughly, when Cs is
high, samples drawn from the Dirichlet are likely to
concentrate near the mode ms. Using this decompo-
sition, we set for all s:

ms = ts̃t(· | s) (5)
Cs = λ · c(s)γ ·

∑
s′ c(s′)∑

s′ c(s′)γ
(6)

where c(s) is the count of source word s in the
source-target bitext, and the scalar hyperparameters
λ, γ > 0 are to be tuned (We experimented with com-
pletely eliminating the hyperparameters γ, λ by di-
rectly learning the parameters Cs. To do so, we im-
plemented the algorithm of Minka (2000) for learn-
ing the Dirichlet prior, but only learned the parame-
ters Cs while keeping the means ms fixed to the trian-
gulation. However, preliminary experiments showed
performance degradation compared to simple hyper-
parameter tuning). Thus, the distribution tst(· | s)
arises from a Dirichlet with mode ts̃t(· | s) and will
tend to concentrate around this mode as a function
of the frequency of s.

The hyperparameter λ linearly controls the
strength of all priors. The last term in Eq. 6 keeps
the sum of Cs insensitive to γ, such that

∑
s Cs =

λ
∑

s c(s). In all our experiments we fixed γ = 0.5.
Setting γ < 1 down-weights the parameter Cs of fre-
quent words s compared to rare ones. This makes
the Dirichlet prior relatively weaker for frequent
words, where we can let the data speak for itself,
and relatively stronger for rare ones, where a good
prior is needed.

Finally, note that this EM procedure reduces to
an interpolation method similar to that of Wang et
al. by applying Eq. 3 only at the very last M-step,
with αs, ms as above and Cs = λ

∑
t E[c(s, t)].

3.2 Joint Training

Next, we further exploit the triangulation idea in de-
signing a multi-task learning approach that jointly
trains the three word alignment models Θst, Θsp,
and Θpt.

To do so, we view each model’s t-table as orig-
inating from Dirichlet distributions defined by the
triangulation of the other two t-tables. We then train

1222



Algorithm 1 Joint training of Θst,Θsp,Θpt
Parameters: λ, γ > 0

• Initialize
{
Θ

(0)
st ,Θ

(0)
sp ,Θ

(0)
pt

}
• Initialize {Cs}, {Cp}, {Ct} as in Eq. 6
• For each EM iteration i:

Estimate hyperparameters α:
1. Compute t(i)

s̃t
from t(i−1)sp and t

(i−1)
pt (Eq. 1)

2. Set α(i)st := Cs · t(i)s̃t(t | s) + 1
E: collect expected counts E[c(·)](i) from Θ(i−1)st
M: Update Θ(i)st using E[c(·)](i) and α(i)st (Eq. 3)
Repeat for Θ(i)sp,Θ

(i)
pt using Eq. 7 as required

the models in a MAP-EM like manner, updating
both the model parameters and their prior hyperpa-
rameters at each iteration. Roughly, this approach
aims at maximizing the posterior likelihood of the
three models with respect to both model parameters
and their hyperparameters (see Appendix).

Procedurally, the idea is simple: In the E-step, ex-
pected counts E[c(·)] are collected from each model
as usual. In the M-step, each t-table is updated ac-
cording to Eq. 3 using the current expected counts
E[c(·)] and an estimate of α from the triangulation
of the most recent version of the other two models.
See Algorithm 1.

Note, however, that we cannot obtain the triangu-
lated t-tables ts̃p, tp̃t by simply applying the trian-
gulation equation (Eq. 1). For example, to construct
ts̃p we need both source-to-target and target-to-pivot
distributions. While we have the former in tst, we
do not have ttp. To resolve this issue, we simply
approximate ttp from the reverse t-table tpt ∈ Θpt
as follows:

ttp(p | t) := c(p)tpt(t | p)∑
p c(p)tpt(t | p) (7)

where c(p) denotes the unigram frequency of the
word p. A similar transformation is done on tsp to
obtain tps, which is then used in computing tp̃t.

3.3 Adjustment of the t-table
Note that a t-table resulting from the triangulation
equation (Eq. 1) is both noisy and dense. To see

why, consider that ts̃t(t | s) is non-zero whenever
there is a pivot word p that co-occurs with both s
and t. This is very likely to occur, for example, if p
is a function word.

To adjust for both density and noise, we pro-
pose a simple product-of-experts re-estimation that
relies on the available source-target parallel data.
The two experts are the triangulated t-table as de-
fined by Eq. 1 and the exponentiated pointwise mu-
tual information (PMI), derived from simple token
co-occurrence statistics of the source-target bitext.
That is, we adjust:

ts̃t(t | s) := ts̃t(t | s) ·
p(s, t)

p(s)p(t)

and normalize the result to form valid conditional
distributions.

Note that the sparsity pattern of the adjusted t-
table matches that of a co-occurrence t-table. We
applied this adjustment in all of our experiments.

4 Experimental Results

Pretending that Czech-English is a low-resource
pair, we conduct two experiments. In the first, we set
French as the pivot language and compare our fixed-
prior (Sec. §3.1) and joint training (Sec. §3.2) ap-
proaches against the interpolation method of Wang
et al. and a baseline HMM word alignment model
(Vogel et al., 1996).

In the second, we examine the effect of the pivot
language identity on our joint training approach,
varying the pivot language over French, German,
Greek, Hungarian, Lithuanian and Slovak.

4.1 Data

For word alignment, we use the Czech-English
News Commentary corpus, along with a develop-
ment set of 460 hand aligned sentence pairs. For
the MT experiments, we use the WMT10 tuning
set (2051 parallel sentences), and both WMT09/10
shared task test sets. See Table 1.

For each of the 6 pivot languages, we created
Czech-pivot and pivot-English bitexts of roughly the
same size (ranging from 196k sentences for English-
Greek to 223k sentences for Czech-Lithuanian).
Each bitext was created by forming a Czech-pivot-
English tritext, consisting of about 500k sentences

1223



from the Europarl corpus (Koehn, 2005) which was
then split into two disjoint Czech-pivot and pivot-
English bitexts of equal size. Sentences of length
greater than 40 were filtered out from all training
corpora.

4.2 Experiment 1: Method Comparison
We trained word alignment models in both source-
to-target and target-to-source directions. We used
5 iterations of IBM Model 1 followed by 5 itera-
tions of HMM. We tuned hyperparameters to max-
imize alignment F-score of the hand-aligned devel-
opment set. Both interpolation parameters λinterp and
λ were tuned over the range [0, 1]. For our methods,
we fixed γ = 0.5, which we found effective during
preliminary experiments. Alignment F-scores using
grow-diag-final-and (gdfa) symmetrization (Koehn,
2010) are reported in Table 2, column 2.

We conducted MT experiments using the Moses
translation system (Koehn, 2005). We used a 5-gram
LM trained on the Xinhua portion of English Giga-
word (LDC2007T07). To tune the decoder, we used
the WMT10 tune set. MT Bleu scores are reported
in Table 2, columns 3–4.

Both our methods outperform the baseline and the
interpolation approach. In particular, the joint train-
ing approach more than doubles the gains obtained
by the interpolation approach, on both F- and Bleu.

We also evaluated the Czech-French and French-
English alignments produced as a by-product of our
joint method. While our French-to-English MT ex-
periments showed no improvement in Bleu, we saw
a +0.6 (25.6 to 26.2) gain in Bleuon the Czech-to-
French translation task. This shows that joint train-
ing may lead to some improvements even on high-
resource bitexts.

4.3 Other Pivot Languages
We examined how the choice of pivot language af-
fects the joint training approach by varying it over
6 languages (French, German, Greek, Hungarian,

train dev WMT09 WMT10
sentences 85k 460 2525 2489
cz tokens 1.63M 9.7k 55k 53k
en tokens 1.78M 10k 66k 62k

Table 1: Czech-English sentence and token statistics.

F Bleu
method/dataset dev WMT09 WMT10

baseline 63.8 16.2 16.6
interpolation (Wang) 66.2 16.6 17.1

fixed-prior (§3.1) 67.3 16.9 17.3
joint (§3.2) 70.1 17.2 17.7

Table 2: F- and Bleu scores for Czech-English via
French. The joint training method outperforms all other
methods tested.

fr fr, sk fr, el fr, sk, el all 6
Tune 16.1 16.4 16.4 16.4 16.4

WMT09 17.2 17.2 17.2 17.3 17.4
WMT10 17.7 17.8 17.8 17.8 17.8

Table 3: Czech-English Bleu scores over pivot language
combinations. Key: fr=French, sk=Slovak, el=Greek.

Lithuanian and Slovak), while keeping the size of
the pivot language resources roughly the same.

Somewhat surprisingly, all models achieved an
F-score of about 70%, which resulted in Bleu
scores comparable to those reported with French
(Table 2). Subsequently, we combined all pivot lan-
guages by simply concatenating the aligned paral-
lel texts across pairs, triples and all pivot languages.
Combining all pivots yielded modest Bleu score im-
provements of +0.2 and +0.1 on the test datasets
(Table 3).

Considering the low variance in F- and Bleu
scores across pivot languages, we computed the
pairwise F-scores between the predicted alignments:
All scores ranged around 97–98%, indicating that
the choice of pivot language had little effect on the
joint training procedure.

To further verify, we repeated this experiment
over Greek-English and Lithuanian-English as the
source-target task (85k parallel sentences), using the
same pivot languages as above, and with comparable
amounts of parallel data (∼200k sentences). We ob-
tained similar results: In all cases, pairwise F-scores
were above 97%.

5 Related Work

The term “triangulation” comes from the phrase-
table triangulation literature (Cohn and Lapata,
2007; Razmara and Sarkar, 2013; Dholakia and

1224



Sarkar, 2014), in which source-pivot and pivot-target
phrase tables are triangulated according to Eq. 1
(with words replaced by phrases). The resulting tri-
angulated phrase table can then be combined with an
existing source-target phrase table, and is especially
useful in increasing the source language vocabulary
coverage, reducing OOVs. In our case, since word
alignment is a closed vocabulary task, OOVs are
never an issue.

In word alignment, Kumar et al. (2007) uses
multilingual parallel data to compute better source-
target alignment posteriors. Filali and Bilmes (2005)
tag each source token and target token with their
most likely translation in a pivot language, and then
proceed to align (source word, source tag) tuple se-
quences to (target word, target tag) tuple sequences.
In contrast, our word alignment method can be ap-
plied without multilingual parallel data, and does not
commit to hard decisions.

6 Conclusion and Future Work

We presented a simple multi-task learning algorithm
that jointly trains three word alignment models over
disjoint bitexts. Our approach is a natural extension
of a mathematically sound MAP-EM algorithm we
originally developed to better utilize the model tri-
angulation idea. Both algorithms are easy to imple-
ment (with closed-form solutions for each step) and
require minimal effort to integrate into an EM-based
word alignment system.

We evaluated our methods on a low-resource
Czech-English word alignment task using additional
Czech-French and French-English corpora. Our
multi-task learning approach significantly improves
F- and Bleu scores compared to both baseline and
the interpolation method of Wang et al. (2006). Fur-
ther experiments showed our approach is insensitive
to the choice of pivot language, producing roughly
the same alignments over six different pivot lan-
guage choices.

For future work, we plan to improve word align-
ment and translation quality in a more data restricted
case where there are very weak source-pivot re-
sources: for example, word alignment of Malagasy-
English via French, using only a Malagasy-French
dictionary, or Pashto-English via Persian.

Acknowledgements

The authors would like to thank Kevin Knight,
Daniel Marcu and Ashish Vaswani for their com-
ments and insights as well as the anonymous re-
viewers for their valuable feedback. This work
was partially supported by DARPA grants DOI/NBC
D12AP00225 and HR0011-12-C-0014 and a Google
Faculty Research Award to Chiang.

Appendix: Joint Training Generative Story

We argue that our joint training procedure can be
seen as optimizing the posterior likelihood of the
three models. Specifically, suppose we place Dirich-
let priors on each of the t-tables tst, tsp, tpt as be-
fore, but define the prior parameterization using a
single hyperparameter α = {αspt} and its marginals
such that:

tst(· | s) ∼ D(. . . , αs·t, . . .) αs·t = ∑pαspt
tsp(· | s) ∼ D(. . . , αsp·, . . .) αsp· = ∑tαspt
tpt(· | p) ∼ D(. . . , α·pt, . . .) α·pt = ∑sαspt

Intuitively, αspt represents the number of times a
source-pivot-target triplet (s, p, t) was observed.

With this prior, we can maximize the posterior
likelihood of the three models given the three bitexts
(denoted data = {bitextst, bitextsp, bitextpt})
with respect to all parameters and hyperparameters:

arg max
Θ,α

P(Θ | α, data) =
arg max

Θ,α

∏
d∈{st,sp,pt} P(bitextd | Θd)P(Θd | α)

Under the generative story, we need only observe the
marginals αs·t, αsp·, α·pt of α. Therefore, instead of
explicitly optimizing over α, we can optimize over
the marginals while keeping them consistent (via
constraints such as

∑
t αs·t =

∑
p αsp· for all s).

In our joint training algorithm (Algorithm 1)
we abandon these consistency constraints in fa-
vor of closed form estimates of the marginals
αs·t, αsp·, α·pt.

References
Peter F. Brown, Vincent J.Della Pietra, Stephen A. Della

Pietra, and Robert. L. Mercer. 1993. The mathematics
of statistical machine translation: Parameter estima-
tion. Computational Linguistics, 19:263–311.

1225



Trevor Cohn and Mirella Lapata. 2007. Machine trans-
lation by triangulation: Making effective use of multi-
parallel corpora. In Proc. ACL 2007.

A. P. Dempster, N. M. Laird, and D. B. Rubin. 1977.
Maximum likelihood from incomplete data via the EM
algorithm. Journal of the Royal Statistical Society, Se-
ries B, 39(1):1–38.

Rohit Dholakia and Anoop Sarkar. 2014. Pivot-based
triangulation for low-resource languages. In Proc.
AMTA.

Karim Filali and Jeff Bilmes. 2005. Leveraging multiple
languages to improve statistical MT word alignments.
In Proc. IEEE Automatic Speech Recognition and Un-
derstanding Workshop (ASRU).

P. Koehn. 2005. Europarl: A parallel corpus for statisti-
cal machine translation. In Proc. Machine Translation
Summit X, pages 79–86.

Philipp Koehn. 2010. Statistical Machine Translation.
Cambridge University Press, New York, NY, USA, 1st
edition.

Shankar Kumar, Franz Och, and Wolfgang Macherey.
2007. Improving word alignment with bridge lan-
guages. In Proc. EMNLP-CoNLL.

Thomas P. Minka. 2000. Estimating a Dirichlet distribu-
tion. Technical report, MIT.

Majid Razmara and Anoop Sarkar. 2013. Ensemble tri-
angulation for statistical machine translation. In Proc.
IJCNLP, pages 252–260.

Stephan Vogel, Hermann Ney, and Christoph Tillmann.
1996. HMM-based word alignment in statistical trans-
lation. In Proc. COLING, pages 836–841.

Haifeng Wang, Hua Wu, and Zhanyi Liu. 2006. Word
alignment for languages with scarce resources using
bilingual corpora of other language pairs. In Proc.
COLING/ACL, pages 874–881.

1226


