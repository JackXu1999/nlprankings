



















































A tool for extracting sense-disambiguated example sentences through user feedback


Proceedings of the EACL 2017 Software Demonstrations, Valencia, Spain, April 3-7 2017, pages 69–72
c©2017 Association for Computational Linguistics

A tool for extracting sense-disambiguated example sentences
through user feedback

Beto Boullosa† and Richard Eckart de Castilho†
and Alexander Geyken‡ and Lothar Lemnitzer‡

and Iryna Gurevych†

†Ubiquitous Knowledge Processing Lab
Department of Computer Science
Technische Universität Darmstadt
http://www.ukp.tu-darmstadt.de

‡Berlin-Brandenburg
Academy of Sciences

http://www.bbaw.de

Abstract

This paper describes an application system
aimed to help lexicographers in the ex-
traction of example sentences for a given
headword based on its different senses.
The tool uses classification and clustering
methods and incorporates user feedback to
refine its results.

1 Introduction

Language is subject to constant evolution and
change. Hence, lexicographers are always sev-
eral steps behind the current state of language
in discovering new words, new senses of ex-
isting words, cataloging them, and illustrating
them using good example sentences. To facili-
tate this work, lexicographers increasingly rely on
automatic approaches that allow sifting efficiently
through the ever growing body of digitally avail-
able text, something that has brought important
gains, including time saving and better utilizing
limited financial and personal resources.

Among the tasks that benefit from the increas-
ing automation in lexicography is the automatic
extraction of suitable corpus-based example sen-
tences for the headwords in the dictionary. Our
paper describes an innovative system that han-
dles this task by incorporating user feedback to
a computer-driven process of sentence extraction
based on a combination of unsupervised and su-
pervised machine learning, in contrast to current
approaches that do not include user feedback. Our
tool allows querying sentences containing a spe-
cific lemma, clustering these sentences by topical
similarity to initialize a sense classifier, and inter-
actively refining the sense assignments, continu-
ally updating the classifier in the background.

In the next section, we contextualize the task of
example extraction; section 3 describes our sys-

tem; section 4 is devoted to evaluation; section 5
summarizes the conclusions and future work.

2 Extraction of Dictionary Examples

Example sentences can help understanding the nu-
ances of the usage of a certain term, specially
in the presence of polysemy. This has become
rather important in the last decades, with the shift
that has occurred, in the field of dictionary mak-
ing, from a content-centered to a user-centered
perspective (Lew, 2015). With the popularization
of online dictionaries, space-saving considerations
have lost the importance once held, making it eas-
ier to add example sentences to a given headword.

Didakowski et al. (2012) argue that a system
for example extraction should ideally act “like a
lexicographer”, i.e., it should fully understand the
examples themselves, something arguably beyond
the scope of current NLP technology. Instead, op-
erational criteria must be used to define “good”
examples, as seen also in the work of Kilgarriff
et al. (2008), criteria like presence of typical col-
locations of the target word, characteristics of the
sentence itself, and guaranteeing that all senses of
the target word are represented by the extracted
example sentences.

Several methods to automate the task have been
developed, the most popular being GDEX (”Good
Dictionary EXamples”) (Kilgarriff et al., 2008).
GDEX is a rule based software tool that suggests
”good” corpus examples to the lexicographer ac-
cording to predefined criteria, including sentence
length, word frequency and the presence/absence
of pronouns or named entities. The goal of GDEX
is to reduce the number of corpus examples to be
inspected by extracting only the n-”best” exam-
ples, the default being 100 sentences. It has been
used and adapted for languages other than English.

Didakowski et al. (2012) presented an extrac-

69



tor of good examples based on a hand-written,
rule-based grammar, determining the quality of
a sentence according to its grammatical structure
combined with some simpler features as used by
GDEX. None of those works, however, focused on
differentiating example sentences according to the
word senses or the target words.

Cook et al. (2013) used Word Sense Induction
(WSI) to identify novel senses for words in a dic-
tionary. They utilized hierarchical LDA (Latent
Dirichlet Allocation) (Teh et al., 2006), a varia-
tion of the original LDA (Blei et al., 2003) topic
model, to identify novel word senses, later com-
bining this approach with GDEX, to allow extract-
ing good example sentences according to word
senses (Cook et al., 2014). However, they obtained
”encouraging rather than conclusive” results, spe-
cially due to limitations of the LDA approach in
linking identified topics with word senses.

Our work explores and develops a similar ap-
proach of using topic modeling and WSI to cluster
sentences according to the senses of a target word,
but we take a step further, using the initial clus-
ters as seed for a series of interactive classification
steps. The training data for each classification step
are sentences whose confidence scores calculated
by the system exceed a threshold, and sentences
manually labeled by the user. The process leads to
a user-driven refinement in the labeling process.

3 System description

The computer-assisted, interactive sense disam-
biguation process supported by our system in-
volves: 1) import sentences into a searchable in-
dex; 2) retrieve sentences containing a specific
word (lemma); 3) cluster selected sentences, pro-
viding a starting point for interactive classifica-
tion; 4) train a multi-class classifier from the ini-
tial clusters - that trains on the most representative
sentences for each cluster and is then used to la-
bel the rest of the sentences; a sentence is ”rep-
resentative” if the confidence score calculated by
the system exceeds a configurable threshold; 5) re-
fine the classifier by interactively correcting sense
assignments for selected sentences.

The system supports multiple users working in
so called projects, which define, among other
things, the list of stopwords available for cluster-
ing and classification and the location where the
sentences should be retrieved from.

A project contains jobs, corresponding to tasks

performed over a certain headword (actually de-
fined by its lemma and POS tag). Tasks include
searching for initial sentences, clustering and clas-
sification. Users can work on many jobs in parallel
and isolated, which allows calculating inter-rater
agreement on the sense disambiguation task.

As for the technologies, the tool was developed
using Java Wicket and relying on Solr1, Mallet 2,
DKPro Core (Eckart de Castilho and Gurevych,
2014), Tomcat and MySQL. The next subsections
describe the system in more detail.

3.1 Searching

After starting a job, the user goes to the Search
page to look for sentences containing the desired
lemma. They are shown in a KWIC (Keyword in
Context) table with their ids. When satisfied with
the results, the user selects the stopwords to use in
the next steps and goes to the Clustering phase.

3.2 Clustering

In the clustering page, the selected sentences are
automatically divided into clusters corresponding
to topics that ideally relate to the senses of the
target word. The user manually configures how
many clusters the topic modeling generates and
control the hyper-parameters to fine-tune the pro-
cess. We currently use Mallet’s LDA implementa-
tion to topic modeling (McCallum, 2002).

The clustering page lists the selected sentences
according to the generated clusters. Each cluster is
shown in its own column, with a word cloud on the
top, containing the main words related to it, which
helps the user to assess cluster’s quality and mean-
ing. The word sizes correspond to their LDA-
calculated weights. The user can change hyper-
parameters and regenerate clusters as often as de-
sired, before proceeding to the classification step.

3.3 Classification

In the classification step, the user interactively re-
fines the results, giving feedback to the automatic
classification in order to improve labeling of the
sentences according to word senses. The initial
automatic labels correspond to the results of the
clustering phase. The user starts analyzing each
sentence and decides if the automatically assigned
sense label is appropriate or if a different label
needs to be assigned manually. The initial default

1http://lucene.apache.org/solr/
2http://mallet.cs.umass.edu

70



Figure 1: Classification page

User Assist Accuracy Time (mm:ss)
Annot. 1 No 0.96 8:05
Annot. 1 Yes 0.95 6:05
Annot. 2 No 0.90 10:05
Annot. 2 Yes 0.90 7:55

Table 1: Evaluation results

value for a sentence’s manual label is “unseen”,
indicating that the user has still not evaluated that
sentence. Besides the available sense labels, two
special labels can be assigned to a sentence: “nei-
ther”, to indicate that none of the available labels
is applicable; “unknown”, meaning that the user
does not know how to label it.

The classification page (figure 1 - the numbers
below correspond to its elements) has a table list-
ing each sentence with its id 1 ; automatically as-
signed sense labels in the previous 5 and current
6 iterations; confidence scores (weights) of the

sense label in the previous 3 and current 4 iter-
ations; manually assigned sense label 7 ; sentence
text 8 ; and an indicator to tell if the sense label
has changed between the previous and current it-
eration 2 . The page has also a widget detailing
the different word senses currently available 10 .
Besides editing the label of a sense, the user can
also add new manual senses 11 .

After modifying parameters, adding senses and
manually labeling sentences, a new classification
iteration can be started. The classifier uses the
threshold 9 to identify the training data - the con-
fidence scores are calculated by the classifier (al-
though in the initial classification they come from
the topic modeling). Furthermore, manually la-
beled sentences are also used as training data. We
use the Naive Bayes algorithm, a classical ap-

proach for Word Sense Disambiguation, known
for its efficiency (Hristea, 2013). We use the Mal-
let implementation of Naive Bayes, with the sen-
tence tokens as features.

4 Evaluation

To evaluate the tool, we conducted experiments
in two different scenarios: 1) using no assis-
tive features, annotators classified sentences iden-
tifying the word senses by their own; 2) using
automatically-generated clusters, annotators let
the system suggest senses and then manually as-
signed labels to sentences, helped by the feedback-
based multi-class classifier. Every annotator ap-
plied the two scenarios to a target word, namely
”Galicia”3, with three senses: a) the Spanish au-
tonomous community, b) the region in Central-
Eastern Europe; c) a football club in Brazil.

The senses were randomly distributed over 97
sentences in the first scenario (38/46/13 sentences
for the respective senses) and 99 in the second
(43/41/15). Sentences in both scenarios did not
overlap, and were taken from a larger dataset of
manually annotated sentences. The experiments
were performed by two non-lexicographers (com-
puter scientists with NLP background). We mea-
sured the time taken in every scenario and calcu-
lated the accuracy of the final results compared to
the manually annotated gold standard.

Results (table 1) indicated that the time was sig-
nificantly reduced when working with full assis-
tance, compared to working without assistance.
Although accuracy did vary very little, there was

3Although proper nouns are not present in conventional
dictionaries, but rather in onomastic dictionaries, which usu-
ally do not make use of examples, it serves well, for method-
ological reasons, to our evaluation purposes.

71



a slight loss of quality in the results of the first
annotator when using assistance. This might indi-
cate a negative influence of system suggestions on
the annotator or it could be attributable to a more
difficult random selection of the samples in cer-
tain cases. These effects call for further investiga-
tion. However, using the tool outside the evalua-
tion setup, we noted subjective speedups from de-
veloping smart strategies to optimize the use of the
information provided by the machine (e.g. quickly
annotating sentences containing specific context
words or sorting by sense and confidence score).
Thus, we expect the automatic assistance to have
a larger impact in actual use than the present eval-
uation can show.

5 Conclusions and future work

We have introduced a novel system for interactive
word sense disambiguation in the context of ex-
ample sentences extraction. Using a combination
of unsupervised and supervised methods, corpus
processing and user feedback, our approach aims
at helping lexicographers to properly assess and
refine a list of pre-analyzed example sentences ac-
cording to the senses of a target word, alleviating
the burden of doing this manually. Using vari-
ous state-of-the-art techniques, including cluster-
ing and classification methods for word sense in-
duction, and incorporating user feedback into the
process of shaping word senses and associating
sentences to them, the tool can be a valuable ad-
dition to a lexicographer’s toolset. As next steps,
we plan to focus on the extraction of “good” exam-
ples, adding support for ranking sentences in dif-
ferent sense clusters according to operational cri-
teria like in Didakowski et al. (2012). We also plan
to extend the evaluation and to observe the strate-
gies that users develop, in order to discover if they
can inform further improvements to the system.

Acknowledgments

This work has received funding from the Euro-
pean Union’s Horizon 2020 research and innova-
tion programme (H2020-EINFRA-2014-2) under
grant agreement No. 654021. It reflects only the
author’s views and the EU is not liable for any
use that may be made of the information con-
tained therein. It was further supported by the
German Federal Ministry of Education and Re-
search (BMBF) under the promotional reference
01UG1416B (CEDIFOR) and by the German Re-

search Foundation under grant No. EC 503/1-1
and GU 798/21-1 (INCEpTION).

References
David M. Blei, Andrew Y. Ng, and Michael I. Jordan.

2003. Latent dirichlet allocation. Journal of Ma-
chine Learning Research, 3:993–1022, March.

Paul Cook, Jey Han Lau, Michael Rundell, Diana Mc-
Carthy, and Timothy Baldwin. 2013. A lexico-
graphic appraisal of an automatic approach for de-
tecting new word-senses. In Proceedings of eLex
2013, pages 49–65, Tallinn, Estonia.

Paul Cook, Michael Rundell, Jey Han Lau, and Tim-
othy Baldwin. 2014. Applying a word-sense in-
duction system to the automatic extraction of di-
verse dictionary examples. In Proceedings of the
XVI EURALEX International Congress, pages 319–
328, Bolzano, Italy.

Jörg Didakowski, Lothar Lemnitzer, and Alexander
Geyken. 2012. Automatic example sentence ex-
traction for a contemporary german dictionary. In
Proceedings of the XV EURALEX International
Congress, pages 343–349, Oslo, Norway.

Richard Eckart de Castilho and Iryna Gurevych. 2014.
A broad-coverage collection of portable NLP com-
ponents for building shareable analysis pipelines. In
Proceedings of the Workshop on Open Infrastruc-
tures and Analysis Frameworks for HLT, pages 1–
11, Dublin, Ireland, August.

Florentina T. Hristea. 2013. The naı̈ve bayes model in
the context of word sense disambiguation. In The
Naı̈ve Bayes Model for Unsupervised Word Sense
Disambiguation: Aspects Concerning Feature Se-
lection, pages 9–16. Springer, Heidelberg, Germany.

Adam Kilgarriff, Milo Husk, Katy McAdam, Michael
Rundell, and Pavel Rychlý. 2008. Gdex: Automat-
ically finding good dictionary examples in a corpus.
In Proceedings of the XIII EURALEX International
Congress, pages 425–432, Barcelona, Spain.

Robert Lew. 2015. Dictionaries and their users. In
International Handbook of Modern Lexis and Lexi-
cography. Springer, Heidelberg, Germany.

Andrew Kachites McCallum. 2002. MAL-
LET: A machine learning for language toolkit.
http://mallet.cs.umass.edu.

Yee Whye Teh, Michael I Jordan, Matthew J Beal, and
David M Blei. 2006. Hierarchical dirichlet pro-
cesses. Journal of the American Statistical Associa-
tion, 101(476):1566–1581.

72


