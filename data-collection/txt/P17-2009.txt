



















































Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics


Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 51–57
Vancouver, Canada, July 30 - August 4, 2017. c©2017 Association for Computational Linguistics

https://doi.org/10.18653/v1/P17-2009

Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 51–57
Vancouver, Canada, July 30 - August 4, 2017. c©2017 Association for Computational Linguistics

https://doi.org/10.18653/v1/P17-2009

Incorporating Dialectal Variability
for Socially Equitable Language Identification

David Jurgens
Stanford University

Yulia Tsvetkov
Stanford University

{jurgens,tsvetkov,jurafsky}@stanford.edu

Dan Jurafsky
Stanford University

Abstract

Language identification (LID) is a criti-
cal first step for processing multilingual
text. Yet most LID systems are not de-
signed to handle the linguistic diversity of
global platforms like Twitter, where lo-
cal dialects and rampant code-switching
lead language classifiers to systematically
miss minority dialect speakers and mul-
tilingual speakers. We propose a new
dataset and a character-based sequence-to-
sequence model for LID designed to sup-
port dialectal and multilingual language
varieties. Our model achieves state-of-the-
art performance on multiple LID bench-
marks. Furthermore, in a case study us-
ing Twitter for health tracking, our method
substantially increases the availability of
texts written by underrepresented popula-
tions, enabling the development of “so-
cially inclusive” NLP tools.

1 Introduction

Language identification (LID) is an essential first
step for NLP on multilingual text. In global set-
tings like Twitter, this text is written by authors
from diverse linguistic backgrounds, who may
communicate with regional dialects (Gonçalves
and Sánchez, 2014) or even include parallel trans-
lations in the same message to address different
audiences (Ling et al., 2013, 2016). Such di-
alectal variation is frequent in all languages and
even macro-dialects such as American and British
English are composed of local dialects that vary
across city and socioeconomic development level
(Labov, 1964; Orton et al., 1998). Yet current sys-
tems for broad-coverage LID—trained on dozens
of languages—have largely leveraged European-
centric corpora and not taken into account demo-

1. @username R u a wizard or wat gan sef: in d mornin -
u tweet, afternoon - u tweet, nyt gan u dey tweet. beta
get ur IT placement wiv twitter

2. Be the lord lantern jaysus me heart after that match!!!
3. Aku hanya mengagumimu dari jauh sekarang . RDK

({}) * last tweet about you - - , maybe

Figure 1: Challenges for socially-equitable LID in Twitter
include dialectal text, shown from Nigeria (#1) and Ireland
(#2), and multilingual text (Indonesian and English) in #3.

graphic and dialectal variation. As a result, these
systems systematically misclassify texts from pop-
ulations with millions of speakers whose local
speech differs from the majority dialects (Hovy
and Spruit, 2016; Blodgett et al., 2016).

Multiple systems have been proposed for broad-
coverage LID at the global level (McCandless,
2010; Lui and Baldwin, 2012; Brown, 2014; Jaech
et al., 2016). However, only a handful of tech-
niques have addressed the challenge of linguis-
tic variability of global data, such as the dialec-
tal variability and multilingual text seen in Fig-
ure 1. These techniques have typically focused
only on limited aspects of variability, e.g., indi-
vidual dialects like African American Vernacu-
lar English (Blodgett et al., 2016), online speech
(Nguyen and Doğruöz, 2013), similar languages
(Bergsma et al., 2012; Zampieri et al., 2014a), or
word-level code switching (Solorio et al., 2014;
Rijhwani et al., 2017).

In this work, our goal is to devise a socially
equitable LID, that will enable a massively mul-
tilingual, broad-coverage identification of popu-
lations speaking underrepresented dialects, mul-
tilingual messages, and other linguistic varieties.
We first construct a large-scale dataset of Twit-
ter posts across the world (§2). Then, we intro-
duce an LID system, EQUILID, that produces per-
token language assignments and obtains state-of-
the-art performance on four LID tasks (§3), out-
performing broad-coverage LID benchmarks by

51

https://doi.org/10.18653/v1/P17-2009
https://doi.org/10.18653/v1/P17-2009


up to 300%. Finally, we present a case study on us-
ing Twitter for health monitoring and show that (1)
current widely-used systems suffer from lower re-
call rates for texts from developing countries, and
(2) our system substantially reduces this disparity
and enables socially-equitable LID.

2 Curating Socially-Representative Text

Despite known linguistic variation in languages,
current broad-coverage LID systems are trained
primarily on European-centric sources (e.g., Lui
and Baldwin, 2014), often due to data availabil-
ity. Further, even when training incorporates
seemingly-global texts from Wikipedia, their au-
thors are still primarily from highly-developed
countries (Graham et al., 2014). This latent bias
can significantly affect downstream applications
(as we later show in §4), since language ID is often
assumed to be a solved problem (McNamee, 2005)
and most studies employ off-the-shelf LID sys-
tems without considering how they were trained.

We aim to create a socially-representative cor-
pus for LID that captures the variation within a
language, such as orthography, dialect, formality,
topic, and spelling. Motivated by the recent lan-
guage survey of Twitter (Trampus, 2016), we next
describe how we construct this corpus for 70 lan-
guages along three dimensions: geography, social
and topical diversity, and multilinguality.
Geographic Diversity We create a large-scale
dataset of geographically-diverse text by boot-
strapping with a people-centric approach (Bam-
man, 2015) that treats location and languages-
spoken as demographic attributes to be inferred
for authors. By inferring both for Twitter users
and then collecting documents from monolingual
users, we ensure that we capture regional variation
in a language, rather than focusing on a particular
aspect of linguistic variety.

Individuals’ locations are inferred using the
method of Compton et al. (2014) as implemented
by Jurgens et al. (2015). The method first identi-
fies the individuals who have reliable ground truth
locations from geotagged tweets and then infers
the locations of other individuals as the geographic
center of their friends’ locations, iteratively apply-
ing this inference method to the whole social net-
work. The method is accurate to within tens of
kilometers on urban and rural users (Johnson et al.,
2017), which is sufficient for the city-level analy-
sis we use here. We use a network of 2.3B edges

from reciprocal mentions to locate 132M users.
To identify monolingual users, we classify mul-

tiple tweets by the same individual and consider an
author monolingual if they had at least 20 tweets
and 95% were labeled with one language `. All
tweets by that author are then treated as being `.
We use this relabeling process to automatically
identify misclassified tweets, which when aggre-
gated geographically, can potentially capture re-
gional dialects and topics.1 We construct separate
sets of monolinguals using langid.py and CLD2 as
classifiers to mitigate the biases of each.
Social and Topical Diversity Authors modulate
their writing style for different social registers
(Eisenstein, 2015; Tatman, 2015). Therefore, we
include corpora from different levels of formality
across a wide range of topics. Texts were gathered
for all of the 70 languages from (1) Wikipedia arti-
cles and their more informal Talk pages, (2) Bible
and Quran translations (3) JRC-Acquis (Stein-
berger et al., 2006), a collection of European leg-
islation, (4) the UN Declaration of Human Rights,
(5) the Watchtower online magazines, (6) the 2014
and 2015 iterations of the Distinguishing Simi-
lar Languages shared task (Zampieri et al., 2014b,
2015), and (7) the Twitter70 dataset (Trampus,
2016). We also include single-language corpora
drawn from slang websites (e.g., Urban Dictio-
nary) and the African American Vernacular En-
glish data from Blodgett et al. (2016). For all
sources, we extract instances sequentially by ag-
gregating sentences up to 140 characters.
Multilingual Diversity Authors are known to
generate multilingual texts on Twitter (Ling et al.,
2013, 2014), with Rijhwani et al. (2017) estimat-
ing that 3.5% of tweets are code-switched. To cap-
ture the potential diversity in multilingual docu-
ments, we perform data augmentation to synthet-
ically construct multilingual documents of tweet
length by (1) sampling texts for two languages
from arbitrary sources, (2) with 50% chance for
each, truncating a text at the first occurrence of
phrasal punctuation, and (3) concatenating the two
texts together and adding it to the dataset (if ≤
140 characters). We create only sentence-level
or phrase-level code-switching rather than word-
level switches to avoid classifier ambiguity for
loan words, which is known to be a significant
challenge (Çetinoğlu et al., 2016).

1A manual analysis of 500 tweets confirmed that nearly
all cases (98.6%) where the classifier’s label differed from
the author’s inferred language were misclassifications.

52



Corpus Summary The geographically-diverse
corpus was constructed from two Twitter datasets:
1.3B tweets drawn from a 10% sample of all
tweets from March 2014 and 14.2M tweets drawn
from 1% sample of all geotagged tweets from
November 2016. Ultimately, we collected 97.8M
tweets from 1.5M users across 197 countries and
in 53 languages. After identifying monolingual
authors in the dataset, 9.4% of the instances
(9.1M) were labeled by CLD2 or langid.py with
a different language than that spoken by its au-
thor; since nearly all are misclassifications, we
view these posts as valuable data to correct sys-
tematic bias.

A total of 258M instances were collected for the
topically and socially-diverse corpora. Multilin-
gual instances were created by sampling text from
all language pairs; a total of 3.2M synthetic in-
stances were created. Full details are reported in
Supplementary Material.

3 Equitable LID Classifier

We introduce EQUILID, and evaluate it on mono-
lingual and multilingual tweet-length text.
Model Character-based neural network architec-
tures are particularly suitable for LID, as they
facilitate modeling nuanced orthographic and
phonological properties of languages (Jaech et al.,
2016; Samih et al., 2016), e.g., capturing regu-
lar morpheme occurrences within the words of a
language. Further, character-based methods sig-
nificantly reduce the model complexity compared
to word-based methods; the latter require sepa-
rate neural representations for each word form and
therefore are prohibitive in multilingual environ-
ments that easily contain tens of millions of unique
words. We use an encoder–decoder architecture
(Cho et al., 2014; Sutskever et al., 2014) with
an attention mechanism (Bahdanau et al., 2015).
The encoder and the decoder are 3-layer recurrent
neural networks with 512 gated recurrent units
(Chung et al., 2014). The model is trained to to-
kenize character sequence input based on white
space and output a sequence with each token’s
language, with extra token types for punctuation,
hashtags, and user mentions.
Setup The data from our socially-representative
corpus (§2) was split into training, development,
and test sets (80%/10%/10%, respectively), sepa-
rately partitioning the data from each source (e.g.,
Wikipedia). Due to different sizes, we imposed

a maximum of 50K instances per source and lan-
guage to reduce training bias. A total 52.3M in-
stances were used for the final datasets. Multi-
lingual instances were generated from texts within
their respective split to prevent test-train leakage.
For the Twitter70 dataset, we use identical train-
ing, development, and test splits as Jaech et al.
(2016). The same trained model is used for all
evaluations. All parameter optimization was per-
formed on the development set using adadelta
(Zeiler, 2012) with mini-batches of size 64 to train
the models. The model was trained for 2.7M steps,
which is roughly three epochs.

Comparison Systems We compare against two
broad-coverage LID systems, langid.py (Lui and
Baldwin, 2012) and CLD2 (McCandless, 2010),
both of which have been widely used for Twit-
ter within in the NLP community. CLD2 is
trained on web page text, while langid.py was
trained on newswire, JRC-Acquis, web pages, and
Wikipedia. As neither was designed for Twitter,
we preprocess text to remove user mentions, hash-
tags, and URLs for a more fair comparison. For
multilingual documents, we substitute langid.py
(Lui and Baldwin, 2012) with its extension, Poly-
glot, described in Lui et al. (2014) and designed
for that particular task.

We also include the results reported in Jaech
et al. (2016), who trained separate models for two
benchmarks used here. Their architecture uses
a convolutional network to transform each input
word into a vector using its characters and then
feed the word vectors to an LSTM encoder that de-
codes to per-word soft-max distributions over lan-
guages. These word-language distributions are av-
eraged to identify the most-probable language for
the input text. In contrast, our architecture uses
only character-based representations and produces
per-token language assignments.

Benchmarks We test the monolingual setting
with three datasets: (1) the test portion of the
geographically-diverse corpus from §2, which
covers 53 languages (2) the test portion of the
Twitter70 dataset, which covers 70 languages and
(3) the TweetLID shared task (Zubiaga et al.,
2016), which covers 6 languages. The Tweet-
LID data includes Galician, which is not one of
the 70 languages we include due to its relative in-
frequency. Therefore, we report results only on
the non-Galician portions of the data. Multilin-
gual LID is tested using the test data portion of the

53



Geo.-Diverse Tweets Tweet 70 TweetLID† Multilingual Tweets
System Macro-F1 Micro-F1 Macro-F1 Micro-F1 Macro-F1 Macro-F1 Micro-F1

langid.py� 0.234 0.960 0.378 0.769 0.580 0.302 0.240
CLD2 0.217 0.930 0.497 0.741 0.544 0.360 0.629

Jaech et al. (2016)‡ 0.912 0.787
EQUILID 0.598 0.982 0.920 0.905 0.796 0.886 0.853

Table 1: Results on the four benchmarks. ‡ results reported in Jaech et al. (2016) are separate models optimized for each
benchmark † excludes Galician. � For multilingual tweets, we use the extension to langid.py described in Lui et al. (2014).

synthetically-constructed multilingual data from
70 languages. Models are evaluated using macro-
averaged and micro-averaged F1. Macro-averaged
F1 denotes the average F1 for each language, in-
dependent of how many instances were seen for
that language. Micro-averaged F1 denotes the F1
measured from all instances and is sensitive to the
skew in the distribution of languages in the dataset.
Results EQUILID attains state-of-the-art perfor-
mance over the other broad-coverage LID systems
on all benchmarks. We attribute this increase to
more representative training data; indeed, Jaech
et al. (2016) reported langid.py obtains a substan-
tially higher F1 of 0.879 when retrained only on
Twitter70 data, underscoring the fact that broad-
coverage systems are typically not trained on data
as linguistically diverse as seen in social media.
Despite being trained for general-purpose, EQUI-
LID also outperformed the benchmark-optimized
models of Jaech et al. (2016).

In the multilingual setting, EQUILID substan-
tially outperforms both Polyglot and CLD2, with
over a 300% increase in Macro-F1 over the former.
Further, because our model can also identify the
spans in each language, we view its performance
as an important step towards an all-languages
solution for detecting sentence and phrase-level
switching between languages. Indeed, in the
Twitter70 dataset, EQUILID found roughly 5% of
the test data are unmarked instances of code-
switching, one of which is the third example in
Figure 1.
Error Analysis To identify main sources of clas-
sification errors, we manually analyzed the out-
puts of EQUILID on the test set of Twitter70. The
dataset contains 9,572 test instances, 90.5% of
which were classified correctly by our system; we
discuss below sources of errors in the remaining
909 misclassified instances.

Classification of closely related languages with
overlapping vocabularies written in a same script
is the biggest source of errors (374 misclassified
instances, 41.1% of all errors). Slavic languages

are the most challenging, with 177 Bosnian and
65 Slovenian tweets classified as Croatian. This
is unsurprising, considering that even for a human
annotator this task is challenging (or impossible).
For example, a misclassified Bosnian tweet Sočni
čokoladni biskvit recept (“juicy chocolate biscuit
recipe”) would be the same in Croatian. Indo-
Iranian languages contribute 39 errors, with Ben-
gali, Marathi, Nepali, Punjabi, and Urdu tweets
classified as Hindi. Among Germanic languages,
Danish, Norwegian, and Swedish are frequently
confused, contributing 22 errors.

Another major source of errors is due to translit-
eration and code switching with English: 328 mes-
sages in Hindi, Urdu, Tagalog, Telugu, and Pun-
jabi were classified as English, contributing 36.1%
of errors. A Hindi-labeled tweet dost tha or ra-
hega ... dont wory ... but dherya rakhe (“he was
and will remain a friend ... don’t worry ... but
have faith”) is a characteristic example, misclas-
sified by our system as English. Reducing these
types of errors is currently difficult due to the lack
of transliterated examples for these languages.

4 Case Study: Health Monitoring

We conclude with a real-world case study on us-
ing Twitter posts as a real-time source of infor-
mation for tracking health and well-being trends
(Paul and Dredze, 2011; Achrekar et al., 2011;
Aramaki et al., 2011). This information is es-
pecially critical for regions where local authori-
ties may not have sufficient resources to identify
trends otherwise. Commonly, trend-tracking ap-
proaches first apply language identification to se-
lect language-specific content, and then apply so-
phisticated NLP techniques to identify content re-
lated to their target phenomena, e.g., distinguish-
ing a flu comment from a hangover-related one.
This setting is where socially-inclusive LID sys-
tems can make real, practical impact: LID systems
that effectively classify languages of underrepre-
sented dialects can substantially increase the re-

54



call of data for trend-tracking approaches, and thus
help reveal dangerous trends in infectious diseases
in the areas that need it most.

Language varieties are associated, among other
factors, with social class (Labov, 1964; Ash, 2002)
and ethnic identity (Rose, 2006; Mendoza-Denton,
1997; Dubois and Horvath, 1998). As a case
study, we evaluate the efficacy of LID systems in
identifying English tweets containing health lex-
icons, across regions with varying Human De-
velopment Index (HDI).2 We compare EQUILID
against langid.py and CLD2.
Setup A list of health-related terms was com-
piled from lexicons for influenza (Lamb et al.,
2013); psychological well-being (Smith et al.,
2016; Preoţiuc-Pietro et al., 2015); and temporal
orientation lexica correlated with age, gender and
personality traits (Park et al., 2016). We incorpo-
rate the 100 highest-weighted alphanumeric terms
from each lexicon, for a total of 385 unique terms.

To analyze the possible effect of regional lan-
guage, we selected 25 countries with English-
speaking populations and constructed 62 bounding
boxes for major cities therein for study (listed in
Supplementary Material). Using the Gnip API, a
total of 984K tweets were collected during January
2016 which used at least one term and were au-
thored within one of the bounding boxes. As these
tweets are required to contain domain-specific
terms, the vast majority are English.3 We there-
fore measure each system’s performance accord-
ing to what percent of these tweets they classify as
English, which estimates their Recall.
Results To understand how Human Development
Index relates to LID performance, we train a Logit
Regression to predict whether a tweet with one of
the target terms will be recognized as English ac-
cording to the HDI of the tweet’s origin country.
Figure 2 reveals increasing disparity in LID accu-
racy for developing countries by the two baseline
models. In contrast, EQUILID outperforms both
systems at all levels of HDI and provides 30%
more observations for countries with the lowest
development levels. This performance improve-
ment is increasingly critical in the global environ-
ment as more English text is generated from pop-
ulous developing countries such as Nigeria (HDI

2HDI is a composite of life expectancy, education, and
income per capita indicators, used to rank countries into tiers
of human development.

3A manual analysis of a random sample of 1000 tweets
showed that 99.4% were in English.

0.4 0.5 0.6 0.7 0.8 0.9 1.0
Human Development Index

of Text's Origin Country

0.6

0.7

0.8

0.9

1.0

R
ec

al
l o

f E
ng

lis
h 

Tw
ee

ts
by

 L
an

gu
ag

e 
ID

 S
ys

te
m

s

classifier
langid.py
CLD2
EquiLID

Figure 2: Estimated recall of tweets with health-related
terms according to a logit regression on the Human Devel-
opment Index of the tweet’s origin country; bands show 95%
confidence interval.

0.527) and India (HDI 0.624), which have tens of
millions of anglophones each. EQUILID provides
a 23.9% and 17.4% improvement in recall of En-
glish tweets for each country, respectively. This
study corroborates our hypothesis that socially-
equitable training corpora are an essential first step
towards socially-equitable NLP.

5 Conclusion

Globally-spoken languages often vary in how they
are spoken according to regional dialects, topics,
or sociolinguistic factors. However, most LID sys-
tems are not designed and trained for this linguis-
tic diversity, which has downstream consequences
for what types of text are considered a part of the
language. In this work, we introduce a socially-
equitable LID system, EQUILID, built by (1) cre-
ating a dataset representative of the types of di-
versity within languages and (2) explicitly mod-
eling multilingual and codes-switched communi-
cation for arbitrary language pairs. We demon-
strate that EQUILID significantly outperforms cur-
rent broad-coverage LID systems and, in a real-
world case study on tracking health-related con-
tent, show that EQUILID substantially reduces the
LID performance disparity between developing
and developed countries. Our work continues a
recent emphasis on NLP for social good by en-
suring NLP tools fully represent all people. The
EQUILID system is publicly available at https:
//github.com/davidjurgens/equilid and data
is available upon request.

55



Acknowledgments
We thank the anonymous reviewers, the Stanford Data Sci-
ence Initiative, and Twitter and Gnip for providing access
to part of data used in this study. This work was sup-
ported by the National Science Foundation through awards
IIS-1514268, IIS-1159679, and IIS-1526745.

References
Harshavardhan Achrekar, Avinash Gandhe, Ross

Lazarus, Ssu-Hsin Yu, and Benyuan Liu. 2011. Pre-
dicting flu trends using Twitter data. In Proc. IEEE
Computer Communications Workshops. pages 702–
707.

Eiji Aramaki, Sachiko Maskawa, and Mizuki Morita.
2011. Twitter catches the flu: detecting influenza
epidemics using Twitter. In Proc. EMNLP. pages
1568–1576.

Sharon Ash. 2002. Social class. The handbook of lan-
guage variation and change 24:402.

Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben-
gio. 2015. Neural machine translation by jointly
learning to align and translate. In Proc. ICLR.

David Bamman. 2015. People-Centric Natural Lan-
guage Processing. Ph.D. thesis, Carnegie Mellon
University.

Shane Bergsma, Paul McNamee, Mossaab Bagdouri,
Clayton Fink, and Theresa Wilson. 2012. Language
identification for creating language-specific Twitter
collections. In Proc. of the Second Workshop on
Language in Social Media. pages 65–74.

Su Lin Blodgett, Lisa Green, and Brendan O’Connor.
2016. Demographic dialectal variation in social me-
dia: A case study of African-American English. In
Proc. EMNLP.

Ralf D Brown. 2014. Non-linear mapping for im-
proved identification of 1300+ languages. In Proc.
EMNLP. pages 627–632.

Özlem Çetinoğlu, Sarah Schulz, and Ngoc Thang Vu.
2016. Challenges of computational processing of
code-switching. In Proc. of the Second Workshop
on Computational Approaches to Code Switching.

Kyunghyun Cho, Bart van Merrienboer, Caglar Gul-
cehre, Dzmitry Bahdanau, Fethi Bougares, Holger
Schwenk, and Yoshua Bengio. 2014. Learning
phrase representations using RNN encoder–decoder
for statistical machine translation. In Proc. EMNLP.

Junyoung Chung, Caglar Gulcehre, KyungHyun Cho,
and Yoshua Bengio. 2014. Empirical evaluation of
gated recurrent neural networks on sequence model-
ing. In Proc. NIPS Deep Learning workshop.

Ryan Compton, David Jurgens, and David Allen. 2014.
Geotagging one hundred million twitter accounts
with total variation minimization. In Big Data

(Big Data), 2014 IEEE International Conference on.
IEEE, pages 393–401.

Sylvie Dubois and Barbara M Horvath. 1998. From
accent to marker in Cajun English: A study of di-
alect formation in progress. English World-Wide
19(2):161–188.

Jacob Eisenstein. 2015. Systematic patterning
in phonologically-motivated orthographic variation.
Journal of Sociolinguistics 19(2):161–188.

Bruno Gonçalves and David Sánchez. 2014. Crowd-
sourcing dialect characterization through Twitter.
PloS one 9(11):e112074.

Mark Graham, Bernie Hogan, Ralph K Straumann, and
Ahmed Medhat. 2014. Uneven geographies of user-
generated information: patterns of increasing in-
formational poverty. Annals of the Association of
American Geographers 104(4):746–764.

Dirk Hovy and Shannon L Spruit. 2016. The social im-
pact of natural language processing. In Proc. ACL.
pages 591–598.

Aaron Jaech, George Mulcaire, Shobhit Hathi, Mari
Ostendorf, and Noah A Smith. 2016. Hierarchical
character-word models for language identification.
In Proc. of the 2nd Workshop on Computational Ap-
proaches to Code Switching.

I. Johnson, C. McMahon, J. Schning, and B. Hecht.
2017. The effect of population and “structural” bi-
ases on social media-based algorithms – a case study
in geolocation inference across the urban-rural spec-
trum. In Proc. CHI.

David Jurgens, Tyler Finnethy, James McCorriston,
Yi Tian Xu, and Derek Ruths. 2015. Geolocation
prediction in twitter using social networks: A criti-
cal analysis and review of current practice. In Proc.
ICWSM.

William Labov. 1964. The social stratification of En-
glish in New York City. Ph.D. thesis, Columbia uni-
versity.

Alex Lamb, Michael J Paul, and Mark Dredze. 2013.
Separating fact from fear: Tracking flu infections on
Twitter. In Proc. HLT-NAACL. pages 789–795.

Wang Ling, Luis Marujo, Chris Dyer, Alan Black, and
Isabel Trancoso. 2014. Crowdsourcing high-quality
parallel data extraction from Twitter. In Proc. WMT .

Wang Ling, Luı́s Marujo, Chris Dyer, Alan W Black,
and Isabel Trancoso. 2016. Mining parallel corpora
from Sina Weibo and Twitter. Computational Lin-
guistics .

Wang Ling, Guang Xiang, Chris Dyer, Alan W Black,
and Isabel Trancoso. 2013. Microblogs as parallel
corpora. In Proc. ACL. pages 176–186.

56



Marco Lui and Timothy Baldwin. 2012. langid.py: An
off-the-shelf language identification tool. In Proc.
ACL (system demonstrations). pages 25–30.

Marco Lui and Timothy Baldwin. 2014. Accurate lan-
guage identification of Twitter messages. In Proc.
of the 5th Workshop on Language Analysis for So-
cial Media. pages 17–25.

Marco Lui, Jey Han Lau, and Timothy Baldwin. 2014.
Automatic detection and language identification of
multilingual documents. TACL 2:27–40.

Michael McCandless. 2010. Accuracy and perfor-
mance of Google’s compact language detector. Blog
post.

Paul McNamee. 2005. Language identification: a
solved problem suitable for undergraduate instruc-
tion. Journal of Computing Sciences in Colleges
20(3):94–101.

Norma Catalina Mendoza-Denton. 1997. Chi-
cana/Mexicana identity and linguistic variation: An
ethnographic and sociolinguistic study of gang affil-
iation in an urban high school. Ph.D. thesis, Stan-
ford University.

Dong-Phuong Nguyen and A Seza Doğruöz. 2013.
Word level language identification in online mul-
tilingual communication. In Proc. EMNLP. pages
857–862.

Harold Orton, Stewart Sanderson, and John Widdow-
son. 1998. The linguistic atlas of England. Psy-
chology Press.

Gregory Park, H Andrew Schwartz, Maarten Sap, Mar-
garet L Kern, Evan Weingarten, Johannes C Eich-
staedt, Jonah Berger, David J Stillwell, Michal
Kosinski, Lyle H Ungar, et al. 2016. Living in the
past, present, and future: Measuring temporal orien-
tation with language. Journal of personality .

Michael J Paul and Mark Dredze. 2011. You are what
you tweet: Analyzing Twitter for public health. In
Proc. ICWSM.

Daniel Preoţiuc-Pietro, Svitlana Volkova, Vasileios
Lampos, Yoram Bachrach, and Nikolaos Aletras.
2015. Studying user income through language,
behaviour and affect in social media. PloS one
10(9):e0138717.

Shruti Rijhwani, Royal Sequiera, Monojit Choud-
hury, Kalika Bali, and Chandra Sekhar Maddila.
2017. Estimating code-switching on twitter with
a novel generalized word-level language detection
technique. In Proc. ACL.

Mary Aleene Rose. 2006. Language, place and identity
in later life. Stanford University.

Younes Samih, Suraj Maharjan, Mohammed Attia,
Laura Kallmeyer, and Thamar Solorio. 2016. Mul-
tilingual code-switching identification via LSTM re-
current neural networks. In Proc. of the 2nd Work-
shop on Computational Approaches to Code Switch-
ing.

Laura K. Smith, Salvatore Giorgi, Rishi Solanki,
Johannes C. Eichstaedt, H. Andrew Schwartz,
Muhammad Abdul-Mageed, Anneke Buffone, and
Lyle H. Ungar. 2016. Does ‘well-being’ translate on
Twitter? In Proc. EMNLP.

Thamar Solorio, Elizabeth Blair, Suraj Mahar-
jan, Steven Bethard, Mona Diab, Mahmoud
Gohneim, Abdelati Hawwari, Fahad AlGhamdi, Ju-
lia Hirschberg, Alison Chang, and Pascale Fung.
2014. Overview for the first shared task on lan-
guage identification in code-switched data. In Proc.
of the First Workshop on Computational Approaches
to Code Switching. pages 62–72.

Ralf Steinberger, Bruno Pouliquen, Anna Widiger,
Camelia Ignat, Tomaz Erjavec, Dan Tufis, and
Dániel Varga. 2006. The jrc-acquis: A multilingual
aligned parallel corpus with 20+ languages. arXiv
preprint cs/0609058 .

Ilya Sutskever, Oriol Vinyals, and Quoc VV Le. 2014.
Sequence to sequence learning with neural net-
works. In Proc. NIPS.

Rachael Tatman. 2015. # go awn: Sociophonetic vari-
ation in variant spellings on twitter. Working Papers
of the Linguistics Circle 25(2):97–108.

Mitja Trampus. 2016. Evaluating language
identification performance. Blog post.
Https://blog.twitter.com/2015/evaluating-language-
identification-performance.

Marcos Zampieri, Liling Tan, Nikola Ljubešic, and
Jörg Tiedemann. 2014a. A report on the DSL shared
task 2014. In Proc. of the First Workshop on Apply-
ing NLP Tools to Similar Languages, Varieties and
Dialects. pages 58–67.

Marcos Zampieri, Liling Tan, Nikola Ljubešic, and
Jörg Tiedemann. 2014b. A report on the dsl shared
task 2014. In Proc. of the First Workshop on Apply-
ing NLP Tools to Similar Languages, Varieties and
Dialects. pages 58–67.

Marcos Zampieri, Liling Tan, Nikola Ljubešic, Jörg
Tiedemann, and Preslav Nakov. 2015. Overview
of the DSL shared task 2015. In Proc. of the Joint
Workshop on Language Technology for Closely Re-
lated Languages, Varieties and Dialects. pages 1–9.

Matthew D Zeiler. 2012. Adadelta: an adaptive learn-
ing rate method. arXiv preprint arXiv:1212.5701 .

Arkaitz Zubiaga, Inaki San Vicente, Pablo Gamallo,
José Ramom Pichel, Inaki Alegria, Nora Aranberri,
Aitzol Ezeiza, and Vı́ctor Fresno. 2016. TweetLID:
a benchmark for tweet language identification. Lan-
guage Resources and Evaluation 50(4):729–766.

57


	Incorporating Dialectal Variability for Socially Equitable Language Identification

