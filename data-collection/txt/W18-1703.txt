



















































Multi-hop Inference for Sentence-level TextGraphs: How Challenging is Meaningfully Combining Information for Science Question Answering?


Proceedings of the Twelfth Workshop on Graph-Based Methods for Natural Language Processing (TextGraphs-12), pages 12–17
New Orleans, Louisiana, June 6, 2018. c©2018 Association for Computational Linguistics

Multi-hop Inference for Sentence-level TextGraphs:
How Challenging is Meaningfully Combining Information for

Science Question Answering?

Peter A. Jansen
School of Information, University of Arizona, Tucson, AZ

pajansen@email.arizona.edu

Abstract

Question Answering for complex questions
is often modelled as a graph construction or
traversal task, where a solver must build or tra-
verse a graph of facts that answer and explain
a given question. This “multi-hop” inference
has been shown to be extremely challenging,
with few models able to aggregate more than
two facts before being overwhelmed by “se-
mantic drift”, or the tendency for long chains
of facts to quickly drift off topic. This is a ma-
jor barrier to current inference models, as even
elementary science questions require an aver-
age of 4 to 6 facts to answer and explain. In
this work we empirically characterize the dif-
ficulty of building or traversing a graph of sen-
tences connected by lexical overlap, by eval-
uating chance sentence aggregation quality
through 9,784 manually-annotated judgements
across knowledge graphs built from three free-
text corpora (including study guides and Sim-
ple Wikipedia). We demonstrate semantic drift
tends to be high and aggregation quality low, at
between 0.04% and 3%, and highlight scenar-
ios that maximize the likelihood of meaning-
fully combining information.

1 Introduction

Question answering (QA) is a task where mod-
els must find answers to natural language ques-
tions, either by retrieving these answers from a
corpus, or inferring them by some inference pro-
cess. Retrieval methods model QA as an answer
sentence selection task, where a solver must find
a sentence or short continuous passage of text
in a corpus that answers the question (Moschitti
et al., 2007; Severyn and Moschitti, 2012, inter
alia). These methods often fall short for ques-
tions requiring complex inference, such as those
in the science domain, where nearly 80% of even
4th grade science exam questions require some
form of causal, model-based, or otherwise com-

“a girl means a
human girl”

“humans are living
organisms”

Girl

“a girl means a
human girl”

“humans are living
organisms”

Girl

AND
“eating is when

an organism takes
in nutrients in the

form of food”

Eating

AND
“an apple is a 
kind of fruit”

“fruits are foods”

Apple

Q: Which of the following is an example of an organism
taking in nutrients?

(A) A dog burying a bone
(B) A girl eating an apple

(C) An insect crawling on a leaf
(D) A boy planting tomatoes

Figure 1: An example multiple choice 4th grade sci-
ence question from the NY Regents exam, and a graph
of 5 sentences that answer and explain the answer to
this question. Edges represent lexical overlap.

plex inference to answer and explain (Clark et al.,
2013; Jansen et al., 2016), and a single continuous
passage of text rarely describes the reasoning re-
quired to move from question to correct answer. In
these cases, multiple sentences, often from differ-
ent parts of a text, different documents, or differ-
ent knowledge bases must be aggregated together
to build a complete answer and explanation.

Aggregating knowledge to support inference
and complex question answering is often framed
as a graph construction or traversal problem (e.g.
Khashabi et al., 2016), where the solver must
find paths that link sentences that contain question
terms with sentences that contain answer terms
through some number of intermediate sentences
(see Figure 1). In these knowledge graphs, nodes
represent facts or single sentences, and edges be-
tween nodes represent some signal that the facts
are interrelated, such as having lexical overlap.

Information aggregation or “multi-hop” graph
traversal has been shown to be extremely chal-
lenging, with QA solvers generally showing only
modest performance benefits when aggregating in-
formation, and diminishing returns as the amount
of aggregation increases. In the elementary sci-

12



ence domain, current estimates suggest that an av-
erage of 4 to 6 sentences are required to answer
and explain a given question (Jansen et al., 2016,
2018), while recent QA solvers generally strug-
gle to meaningfully aggregate more than two free-
text sentences (Jansen et al., 2017), even when
using alternate representations including semi-
structured tables (Khashabi et al., 2016) or graphs
of words or syntactic dependencies traversed us-
ing monolingual alignment or PageRank variants
in open-domain QA (Fried et al., 2015). Fried et
al. (2015) suggest these performance limitations
are due to “semantic drift”, where as the number
of sentences being aggregated increases, so do the
chances of making a misstep in the aggregation
– for example, aggregating a sentence about seed
funding for a company when making an inference
about the stages of plant growth. This appears
to occur across a variety of solvers, representa-
tions, and methods for aggregation, and is leading
to both the development of datasets specifically
designed for multi-hop QA (Jansen et al., 2016,
2018; Welbl et al., 2017), as well as methods of
controlling for semantic drift in knowledge graphs
constructed from (for example) OpenIE triples us-
ing either support graphs (Khot et al., 2017) or
drift-sensitive random walks (Kwon et al., 2018).

In an effort to better understand the challenges
of inference and explanation construction for QA,
here we characterize the difficultly of the informa-
tion aggregation task in the context of science ex-
ams. The contributions of this work are:

1. We provide the first empirical characteriza-
tion of the difficulty of information aggrega-
tion by manually evaluating sentence aggre-
gation quality using 9,784 annotated judge-
ments across 14 representative exam ques-
tions, highlighting specific patterns of lexical
overlap between question, answer, and can-
didate sentence that maximize the chances of
successful aggregation.

2. We evaluate aggregation difficulty across
three knowledge resources, and empirically
demonstrate that while moving to open do-
main resources increases knowledge cover-
age, it also increases the difficulty of the in-
formation aggregation task by more than an
order of magnitude.

3. We evaluate aggregating up to three sen-
tences that connect terms in the question to

terms in the answer, and show that this suf-
fers both from sparsity (even on Wikipedia-
scale corpora), as well as a very low proba-
bility of producing meaningful aggregations
(0.04% to 3%) through lexical overlap alone.

2 Approach

Questions: Due to the magnitude of manual an-
notation, we drew 14 representative questions an-
notated as likely requiring inference1 from the
432 training questions in the AI2 Open Elemen-
tary Science Questions set2, originally drawn from
standardized science exams in 12 US states. Ques-
tions span 14 common curriculum topics, includ-
ing changes of state, planetary motion, environ-
mental adaptations, the life cycle, inherited traits,
magnetism, and measurement. For context, to
date, the best-performing systems report answer-
ing just under 60% of elementary science ques-
tions correctly (Jauhar et al., 2016).

2.1 Corpora
We generate and evaluate three separate graphs
constructed from three independent corpora:
Science Explanations Corpus: An explanation
corpus of 1,364 sentences from Jansen et al.
(2016) designed to construct high-quality explana-
tions for the AI2 question set through aggregation.
Study Guide Corpus: An in-domain corpus of
2,503 sentences from two study guides for the
New York and Virginia standardized exams.
Simple Wikipedia: A large open-domain cor-
pus of 848,920 sentences retrieved from Simple
Wikipedia and included in the AristoMini corpus2.

2.2 Methods
Here we simulate the graph-based inference pro-
cess by creating short chains of sentences inter-
connected based on shared words between those
sentences. Specifically, two sentences are said to
be connected if they share at least one content
lemma (noun, verb, or adjective) in common. Sen-
tences with the same lemma but different parts of
speech are not connected (e.g. a sentence con-
taining plant VB is not connected to a sentence
containing plant NN). Lemmatization and part-
of-speech tagging are provided by the Stanford
CoreNLP toolkit (Manning et al., 2014).

1Our results did not substantially change when data from
only half the questions were used, suggesting the aggregate
statistics from the 9,784 manual judgements are stable.

2http://allenai.org/data.html

13



Q: What is the main purpose of the flowers of a peach tree?
A: to attract bees for pollination.

Example Ratings:
High: The flower helps the plant reproduce because it con-
tains the pollen and eggs.
Possible: Seeds grow in the center of a flower and continue
to develop there after the petals fall off the plant.
Topical/Unlikely: There are four major parts of a plant:
roots, stem, leaves, and flower.
Offtopic: The average life span of a worker bee is 1 year.

Table 1: Example ratings on a 4-point rating scale de-
scribing the perceived utility of each sentence towards
an explanation for why the answer is correct (high, pos-
sible, topical, offtopic). Sentences are from the Study
Guide corpus, and each have lexical overlap with the
question and/or answer.

For a given question, sentences in one corpus
are identified that have lexical overlap with either
the question terms, answer terms, or both ques-
tion and answer terms. We then manually rate the
relevance of each sentence on a 4-point scale us-
ing the following criterion: “What is the likelihood
that this knowledge would contribute to an expla-
nation for why the answer is correct?”. Example
ratings are included in Table 1.

2.3 Connectivity Characterization
Here, we denote the question text as Q, the correct
answer text as A, and a sentence from the corpus
with overlapping terms as Sx, where x is either
Q or A. We characterize the utility of sentences
towards building an explanation in five scenarios:

Direct lexical overlap:

1. Q ↔ SQ: Sentences that have lexical overlap
with the question.

2. SA ↔ A: Sentences that have lexical overlap
with the answer.

3. Q ↔ SQA ↔ A: Sentences that have lexical
overlap with both question and answer.

Indirect (aggregating) overlap:

4. Q ↔ SQ ↔ SA ↔ A: Aggregating two
sentences that individually have lexical over-
lap with the question or answer, and that also
have lexical overlap with each other.

5. Q ↔ SQ ↔ SO ↔ SA ↔ A: Aggregating
three sentences: two sentences that individu-
ally have lexical overlap with the question or
answer, and that are connected by a third sen-
tence SO that has lexical overlap with both
SQ and SA, but not with Q or A.

3 Results and Discussion

What proportion of sentences with direct lexi-
cal overlap to the question and answer contain
highly relevant information? The results of the
direct characterization are shown in Table 2. The
overall proportion of corpus sentences containing
relevant information to the question are low, with
5.5% of sentences rated as highly useful in the ex-
planation corpus, 1.7% in the Study Guide corpus,
and only 0.1% in the large Simple Wikipedia cor-
pus. Sentence utility increases as the lexical over-
lap (number of terms matched) increases. Simi-
larly, sentences with terms from the answer are
3 to 5 times more likely to be highly relevant
than sentences with question terms. Sentences that
overlap on both question and answer terms have a
substantially increased probability of being rated
highly relevant compared to sentences with a sin-
gle question or answer term (e.g. 21.4% vs 1.7%
and 5.2%, respectively, for the Study Guide cor-
pus), but are sparse, occurring an average of ap-
proximately once per question.

When aggregating two sentences, what propor-
tion will contain highly relevant information?
The probability of aggregating two sentences that
individually lexically overlap with the question or
answer, and also lexically overlap with each other,
Q ↔ SQ ↔ SA ↔ A, is shown in Table 3. The
likelihood of aggregating two sentences from the
Study Guide corpus that were both highly rated
and that lexically overlap by at least one term is
3.0%, and when expanding this to allow for ag-
gregating sentences with high or possible ratings
(bolded square), this likelihood increases to 6.6%.
For the Simple Wikipedia corpus these probabili-
ties are one to two orders of magnitude lower, at
0.04% and 0.3%, respectively.

When restricting 2-sentence aggregations to
cases of moderate lexical overlap, where SQ ↔
SA overlap by 2 or more lemmas not found in the
question or answer, quality improves substantially

3The scale of the Simple Wikipedia corpus makes man-
ual evaluation intractable. Here we subsample to rate 50
sentences with each pattern of lexical overlap, and limit our
analysis to 7 questions. For example, for the question in in
Table 1, we rate 50 sentences that have lexical overlap only
with the word flowers NN, another 50 that overlap with flow-
ers NN and purpose NN, and so on. In practice, due to the
relative sparsity of multiword matches, we evaluate nearly
all cases where the lexical overlap consists of two or more
words, and the subsampling only affects estimates of single
overlapping lemma matches for this corpus (leftmost columns
in table).

14



1 overlapping lemma 2 overlapping lemmas 3+ overlapping lemmas
SQ SA SQA SQ SA SQA SQ SA SQA

Explanation Corpus (1,364 sentences)
Highly likely 5.5% 18.4% - 18.2% 66.7% 65.6% 40.0% - 100%

Possible 4.8% 8.5% - 22.7% 6.7% 9.4% 40.0% - 0%
Topical 14.1% 26.9% - 18.2% 0% 18.8% 0% - 0%

Off topic 75.5% 46.2% - 40.9% 26.7% 6.3% 20.0% - 0%

N (Samples) 992 223 - 44 15 32 5 0 7

Study Guide Corpus (2,503 sentences)
Highly likely 1.7% 5.2% - 6.6% 55.6% 21.4% 27.8% - 58.8%

Possible 2.1% 6.3% - 12.4% 5.6% 12.5% 27.8% - 17.6%
Topical 6.7% 9.6% - 24.1% 16.7% 12.5% 11.1% - 5.9%

Off topic 89.6% 79.0% - 56.9% 22.2% 53.6% 33.3% - 17.6%

N (Samples) 2133 480 - 137 18 56 18 0 17

Simple Wikipedia Corpus (848,920 sentences, subsampled3)
Highly likely 0.1% 0.5% - 0.4% 11.2% 1.7% 0.6% 50.0% 16.1%

Possible 0.2% 1.1% - 1.8% 6.7% 3.3% 2.5% 50.0% 19.4%
Topical 0.8% 3.4% - 3.4% 17.9% 8.2% 5.0% 0.0% 14.5%

Off topic 98.9% 95.0% - 94.4% 64.2% 86.8% 91.9% 0.0% 50.0%

N (Samples) 2102 880 - 1399 134 599 161 2 62

Table 2: Observed frequencies for sentences with given utility ratings for the three categories of direct (lexical
overlap) connections: Q ↔ SQ, SA ↔ A, and Q ↔ SQA ↔ A, and various degrees of lexical overlap.

Explanation Corpus (1,979 samples)
SA Rating

Highly Possible Topical OffTopic

Highly 13.4% 3.8% 3.4% 1.1%
Possible 3.4% 0.4% 1.1% 0.6%
Topical 7.5% 1.3% 8.0% 4.5%

S
Q

R
at

in
g

Offtopic 16.3% 11.0% 9.1% 14.9%

Study Guide Corpus (8,096 samples)
SA Rating

Highly Possible Topical OffTopic

Highly 3.0% 0.7% 0.7% 1.3%
Possible 2.1% 0.8% 0.8% 1.2%
Topical 3.8% 2.0% 2.7% 4.4%

S
Q

R
at

in
g

Offtopic 17.0% 6.4% 10.9% 42.2%

Simple Wikipedia Corpus (23,750 samples)
SA Rating

Highly Possible Topical OffTopic

Highly 0.04% 0.04% 0.06% 0.0%
Possible 0.1% 0.1% 0.3% 1.7%
Topical 0.1% 0.02% 0.2% 2.1%

S
Q

R
at

in
g

Offtopic 2.4% 2.0% 7.2% 82.9%

Table 3: Observed frequencies for aggregating two
sentences together with specific utility ratings in the
Q ↔ SQ ↔ SA ↔ A condition across each corpus.
Here, one sentence in the pair has overlapping terms in
the question, the other sentence has overlapping terms
in the answer, and both sentences lexically overlap with
each other on one or more terms that are not found in
either the question or answer. Axes represent the indi-
vidual (nonaggregated) ratings of each sentence (Q or
A). The bolded square represents the proportion of lex-
ically connected sentence pairs where utility ratings for
both sentences are either high or possible.

on the Study Guide corpus, with 12.5% of these
aggregates containing sentences both rated highly
relevant (N=1,262), or an average of 11 per ques-
tion. The pattern is similar for the Explanation and
SimpleWiki corpora, but scaled up by a factor of
2-4, and down by a factor of 10-40, respectively.4

When aggregating three sentences, what pro-
portion of intermediate sentences are highly
relevant? To characterize the number of pos-
sible 3-sentence aggregations of the form Q ↔
SQ ↔ SO ↔ SA ↔ A, with each sentence
rated as having a highly relevant or possible util-
ity for explanations, we retrieved all intermediate
sentences SO in the corpus such that (a) SO con-
tains overlapping lemmas with both SQ and SA
that are not found in the question or answer, and
(b) both SQ and SA have ratings of either highly
relevant or possible. The overall number of inter-
mediate sentences meeting this criterion was small
(17 for the Study Guide corpus across all 14 ques-
tions, and 251 for the Simple Wikipedia corpus).
We manually rated these intermediate sentences,
finding a small proportion had favourable utility
ratings, with 1.5% receiving ratings of highly rel-
evant and 2% receiving possible. This suggests
that both sparsity and drift make aggregating three
sentences highly unlikely, even in large million-
sentence-scale corpora such as Simple Wikipedia.

4Due to space limitations, this table is not shown.

15



Overall, what is chance performance for com-
bining information to generate real explana-
tions? Previous work suggests that real explana-
tions for elementary science questions require ag-
gregating an average of 4 to 6 separate facts to an-
swer and explain (Jansen et al., 2016, 2018), with
this value ranging between 1 fact to more than a
dozen facts per question, depending on the amount
of question-specific knowledge and world knowl-
edge required. Extrapolating from our empirical
analysis5 suggests that the chance of generating a
4-fact aggregation of the form Q ↔ SQ ↔ SO ↔
SO ↔ SA ↔ A is likely to be extremely improb-
able, at approximately 1 in 187,000 for the Study
Guide corpus, and 1 in 17 million with the Simple
Wikipedia corpus, in the case of sentences having
a single overlapping lemma. Where SQ and SA
share two overlapping lemmas with the question,
this increases to approximately 1 in 7,000 for the
Study Guide corpus, and 1 in 207,000 for the Sim-
ple Wikipedia corpus, but is still improbable.

Building graphs based solely on lexical overlap
captures only a fraction of the possible mean-
ingful connections between knowledge in a cor-
pus. How might this limitation affect this em-
pirical analysis? Lexical overlap is a common
method of building knowledge graphs for QA (e.g.
Khashabi et al., 2016; Jansen et al., 2017), as two
sentences having the same words has been re-
garded as a strong signal that they may contain
mutually beneficial content for the inference task.
While other methods of connection, such as Word-
Net synsets to capture synonymy or word embed-
dings to capture associative relations, are likely to
increase the recall of sentences in a corpus relevant
to a given question, we hypothesize that lexical
overlap – as poorly as we have shown it performs
empirically – is likely a higher precision method of
creating meaningful connections than these other
connection methods. In this way we propose lex-
ical overlap can be viewed as a baseline for other
knowledge graph connection methodologies to be
evaluated against.

Evaluating the proportion of meaningful con-
nections in graphs built from specific knowl-
edge resources provides only a partial under-
standing of the challenges of information aggre-

5This extrapolation uses the empirically derived probabil-
ity of a meaningful SO transition to be 3.5% (1.5% highly +
2.0% possible). Similarly, probabilities for SQ and SA add
both highly and possible transition probabilities from Table
2.

gation, because it doesn’t capture how well spe-
cific inference methods may perform on a given
knowledge graph. A central limitation of this
empirical evaluation is that it evaluates the prob-
ability of meaningfully assembling knowledge in
three specific knowledge resources, rather than the
empirical performance of specific inference algo-
rithms on assembling knowledge towards the QA
and explanation construction task with these spe-
cific resources. Combining information to form
inferences is one of the central challenges in con-
temporary question answering, and few models
appear able to consistently aggregate more than
two facts in support of this inference task. While
a variety of different methods of information ag-
gregation have been proposed, our ultimate eval-
uation metric for many of these models has been
the overall proportion of questions answered cor-
rectly, rather than a targeted evaluation of the in-
formation aggregation mechanism. Methods such
as evaluating inference performance as the num-
ber of aggregation steps increases (e.g. Fried et al.,
2015; Jansen et al., 2017) begin to provide insight
on the efficacy of specific methods of informa-
tion aggregation, but these methods must be paired
with a knowledge graph with known connectivity
properties to provide a detailed characterization of
the performance of specific aggregation methods
on the information aggregation task.

4 Conclusion

We empirically demonstrate that aggregating mul-
tiple sentences together to support inference for
QA is extremely challenging. For the in-domain
study guide corpus, only 3% of 2-sentence Q ↔
SQ ↔ SA ↔ A aggregations were rated as
highly useful, while this falls to 0.04% for the
open domain corpus. In spite of the size of Sim-
ple Wikipedia, 3-sentence aggregations are sparse,
and substantially reduce the chance of meaning-
fully aggregating sentences to the point of improb-
ability. Taken together, our analysis suggests the
ability to generate inferences incorporating 4 to 6
facts required for the average question is unlikely
without high-precision means of concept match-
ing beyond lexical overlap, and methods of con-
trolling for drift, or reducing drift through pairing
with close-domain corpora. Our ratings for the
open Explanation and Simple Wikipedia corpora
are available at http://cognitiveai.org/
explanationbank/ .

16



References

Peter Clark, Philip Harrison, and Niranjan Balasubra-
manian. 2013. A study of the knowledge base re-
quirements for passing an elementary science test.
In Proceedings of the 2013 Workshop on Automated
Knowledge Base Construction, AKBC’13, pages
37–42.

Daniel Fried, Peter Jansen, Gustave Hahn-Powell, Mi-
hai Surdeanu, and Peter Clark. 2015. Higher-
order lexical semantic models for non-factoid an-
swer reranking. Transactions of the Association for
Computational Linguistics, 3:197–210.

Peter Jansen, Niranjan Balasubramanian, Mihai Sur-
deanu, and Peter Clark. 2016. What’s in an expla-
nation? characterizing knowledge and inference re-
quirements for elementary science exams. In Pro-
ceedings of COLING 2016, the 26th International
Conference on Computational Linguistics: Techni-
cal Papers, pages 2956–2965, Osaka, Japan. The
COLING 2016 Organizing Committee.

Peter Jansen, Rebecca Sharp, Mihai Surdeanu, and Pe-
ter Clark. 2017. Framing qa as building and ranking
intersentence answer justifications. Computational
Linguistics.

Peter Jansen, Elizabeth Wainwright, Steven Mar-
morstein, and Clayton T. Morrison. 2018.
Worldtree: A corpus of explanation graphs for
elementary science questions supporting multi-hop
inference. In Proceedings of the 11th International
Conference on Language Resources and Evaluation
(LREC).

Sujay Kumar Jauhar, Peter D Turney, and Eduard H
Hovy. 2016. Tables as semi-structured knowledge
for question answering. In Proceedings of the 54th
Annual Meeting of the Association for Computa-
tional Linguistics (ACL).

Daniel Khashabi, Tushar Khot, Ashish Sabharwal, Pe-
ter Clark, Oren Etzioni, and Dan Roth. 2016. Ques-
tion answering via integer programming over semi-
structured knowledge. In Proceedings of the Inter-
national Joint Conference on Artificial Intelligence,
IJCAI’16, pages 1145–1152.

Tushar Khot, Ashish Sabharwal, and Peter Clark. 2017.
Answering complex questions using open informa-
tion extraction. In Proceedings of the 55th Annual
Meeting of the Association for Computational Lin-
guistics, ACL 2017, Vancouver, Canada, July 30 -
August 4, Volume 2: Short Papers, pages 311–316.

Heeyoung Kwon, Harsh Trivedi, Peter Jansen, Mi-
hai Surdeanu, and Niranjan Balasubramanian. 2018.
Controlling information aggregation for complex
question answering. In Proceedings of the 40th
European Conference on Information Retrieval
(ECIR).

Christopher D Manning, Mihai Surdeanu, John Bauer,
Jenny Finkel, Steven J Bethard, and David Mc-
Closky. 2014. The stanford corenlp natural language
processing toolkit. In Proceedings of 52nd Annual
Meeting of the Association for Computational Lin-
guistics: System Demonstrations, pages 55–60.

Alessandro Moschitti, Silvia Quarteroni, Roberto
Basili, and Suresh Manandhar. 2007. Exploiting
syntactic and shallow semantic kernels for ques-
tion/answer classification. In Proceedings of the
45th Annual Meeting of the Association for Compu-
tational Linguistics (ACL), pages 776–783, Prague,
Czech Republic.

Aliaksei Severyn and Alessandro Moschitti. 2012.
Structural relationships for large-scale learning of
answer re-ranking. In SIGIR.

Johannes Welbl, Pontus Stenetorp, and Sebastian
Riedel. 2017. Constructing datasets for multi-hop
reading comprehension across documents. arXiv
preprint arXiv:1710.06481.

17


