



















































A Deep Factorization of Style and Structure in Fonts


Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing
and the 9th International Joint Conference on Natural Language Processing, pages 2195‚Äì2205,
Hong Kong, China, November 3‚Äì7, 2019. c¬©2019 Association for Computational Linguistics

2195

A Deep Factorization of Style and Structure in Fonts

Akshay Srivatsan1 Jonathan T. Barron2 Dan Klein3 Taylor Berg-Kirkpatrick4

1Language Technologies Institute, Carnegie Mellon University, asrivats@cs.cmu.edu
2Google Research, barron@google.com

3Computer Science Division, University of California, Berkeley, klein@cs.berkeley.edu
4Computer Science and Engineering, University of California, San Diego, tberg@eng.ucsd.edu

Figure 1: Example fonts from the Capitals64 dataset. The task of font reconstruction involves generating missing
glyphs from partially observed novel fonts.

Abstract
We propose a deep factorization model for ty-
pographic analysis that disentangles content
from style. Specifically, a variational infer-
ence procedure factors each training glyph into
the combination of a character-specific con-
tent embedding and a latent font-specific style
variable. The underlying generative model
combines these factors through an asymmet-
ric transpose convolutional process to gener-
ate the image of the glyph itself. When trained
on corpora of fonts, our model learns a man-
ifold over font styles that can be used to an-
alyze or reconstruct new, unseen fonts. On
the task of reconstructing missing glyphs from
an unknown font given only a small num-
ber of observations, our model outperforms
both a strong nearest neighbors baseline and
a state-of-the-art discriminative model from
prior work.

1 Introduction

One of the most visible attributes of digital lan-
guage data is its typography. A font makes use
of unique stylistic features in a visually consistent
manner across a broad set of characters while pre-
serving the structure of each underlying form in
order to be human readable ‚Äì as shown in Figure 1.
Modeling these stylistic attributes and how they
compose with underlying character structure could
aid typographic analysis and even allow for auto-
matic generation of novel fonts. Further, the vari-
ability of these stylistic features presents a chal-
lenge for optical character recognition systems,

which typically presume a library of known fonts.
In the case of historical document recognition, for
example, this problem is more pronounced due
to the wide range of lost, ancestral fonts present
in such data (Berg-Kirkpatrick et al., 2013; Berg-
Kirkpatrick and Klein, 2014). Models that capture
this wide stylistic variation of glyph images may
eventually be useful for improving optical charac-
ter recognition on unknown fonts.

In this work we present a generative model ca-
pable of disentangling stylistic features of fonts
from the underlying structure of each character.
Our model represents the style of each font as
a vector-valued latent variable, and parameterizes
the structure of each character as a learned embed-
ding. Critically, each style latent variable is shared
by all characters within a font, while character em-
beddings are shared by characters of the same type
across all fonts. Thus, our model is related to deep
matrix factorization techniques (Xue et al., 2017).
While some prior work on font manifolds (Camp-
bell and Kautz, 2014) relies on outline represen-
tations of observed glyphs, our approach directly
models pixel values in rasterized glyph represen-
tations and allows us to more easily generalize to
fonts with variable glyph topologies.

Inspired by modern neural methods‚Äô ability to
disentangle loosely coupled phenomena in other
domains, including both language and vision (Hu
et al., 2017; Yang et al., 2017; Gatys et al., 2016;
Zhu et al., 2017), we parameterize the distribution
that combines style and structure in order to gener-



2196

ate glyph images as a transpose convolutional neu-
ral decoder (Dumoulin and Visin, 2016). Further,
the decoder is fed character embeddings early on
in the process, while the font latent variables di-
rectly parameterize the convolution filters. This
architecture biases the model to capture the asym-
metric process by which structure and style com-
bine to produce an observed glyph.

We evaluate our learned representations on the
task of font reconstruction. After being trained
on a set of observed fonts, the system recon-
structs missing glyphs in a set of previously un-
seen fonts, conditioned on a small observed sub-
set of glyph images. In contrast with past work
on this task (Azadi et al., 2018), our model is
generative and font reconstruction is performed
via posterior inference. Since our model‚Äôs pos-
terior is intractable, we demonstrate how a vari-
ational inference procedure can be used to per-
form both learning and accurate font reconstruc-
tion. In experiments, we find that our proposed
latent variable model is able to substantially out-
perform both a strong nearest-neighbors baseline
as well as a state-of-the-art discriminative system
on a standard dataset for font reconstruction. Fur-
ther, in qualitative analysis, we demonstrate how
the learned latent space can be used to interpolate
between fonts, hinting at the practicality of more
creative applications.

Finally, while we focus our evaluation on font
reconstruction, our approach has an important re-
lationship with style transfer ‚Äì the goal of our anal-
ysis is to learn a smooth manifold of font styles
that allows for stylistic inference given a small
sample of glyphs. Many style transfer tasks in the
language domain (Shen et al., 2017) suffer from
ambiguity surrounding the underlying division be-
tween style and semantic content. By contrast, in
this setting the distinction is clearly defined, with
content (i.e. the character) observed as a categor-
ical label denoting the coarse overall shape of a
glyph, and style (i.e. the font) explaining lower-
level visual features such as boldness, texture, and
serifs. The modeling approach taken here might
inform work on more complex domains where the
division is less clear.

2 Font Reconstruction

We can view a collection of fonts as a matrix,
X , where each column corresponds to a particu-
lar character type, and each row corresponds to a

specific font. Each entry in the matrix, xij , is an
image of the glyph for character i in the style of a
font j, which we observe as a 64 √ó 64 grayscale
image as shown in Figure 1. In a real world set-
ting, the equivalent matrix would naturally have
missing entries wherever the encoding for a char-
acter type in a font is undefined. In general, not all
fonts contain renderings of all possible character
types; many will only support one particular lan-
guage or alphabet and leave out uncommon sym-
bols. Further, for many commercial applications,
only the small subset of characters that appears in
a specific advertisement or promotional message
will have been designed by the artist ‚Äì the major-
ity of glyphs are missing. As a result, we may wish
to have models that can infer these missing glyphs,
a task referred to as font reconstruction.

Following past work (Azadi et al., 2018), we
define the task setup as follows: During training
we have access to a large collection of observed
fonts for the complete character set. At test time
we are required to predict the missing glyph im-
ages in a collection of previously unseen fonts with
the same character set. Each test font will contain
observable glyph images for a small randomized
subset of the character set. Based on the style of
this subset, the model must reconstruct glyphs for
the rest of the character set.

Font reconstruction can be thought of as a form
of matrix completion; given various observations
in both a particular row and column, we wish to
reconstruct the element at their intersection. Alter-
natively we can view it as a few-shot style transfer
task, in that we want to apply the characteristic
attributes of a new font (e.g. serifs, italicization,
drop-shadow) to a letter using a small number of
examples to infer those attributes. Past work on
font reconstruction has focused on discriminative
techniques. For example Azadi et al. (2018) used
an adversarial network to directly predict held out
glyphs conditioned on observed glyphs. By con-
trast, we propose a generative approach using a
deep latent variable model. Under our approach
fonts are generated based on an unobserved style
embedding, which we can perform inference over
given any number of observations.

3 Model

Figure 2 depicts our model‚Äôs generative process.
Given a collection of images of glyphs consisting
of I character types across J fonts, our model hy-



2197

ùëí1
ùëí2
ùëí3

ùëí3
ùëß4

ùëß1 ùëß2
ùëß3 ùëß4

ùëß4ùëß3ùëß2ùëß1
Gaussian

Prior

Font Embedding
Latent Variables

Character 
Embedding
Parameters Decoder Architecture

Observed Glyphs

Transpose 
Conv

Figure 2: Depiction of the generative process of our model. Each observed glyph image is generated conditioned
on the latent variable of the corresponding font and the embedding parameter of the corresponding character type.
For a more detailed description of the decoder architecture and hyperparameters, see Appendix A.

pothesizes a separation of character-specific struc-
tural attributes and font-specific stylistic attributes
into two different representations. Since all char-
acters are observed in at least one font, each char-
acter type is represented as an embedding vector
which is part of the model‚Äôs parameterization. In
contrast, only a subset of fonts is observed during
training and our model will be expected to gen-
eralize to reconstructing unseen fonts at test time.
Thus, our representation of each font is treated as
a vector-valued latent variable rather than a deter-
ministic embedding.

More specifically, for each font in the collec-
tion, a font embedding variable, zj ‚àà Rk, is
sampled from a fixed multivariate Gaussian prior,
p(zj) = N (0, Ik). Next, each glyph image, xij ,
is generated independently, conditioned on the
corresponding font variable, zj , and a character-
specific parameter vector, ei ‚àà Rk, which we re-
fer to as a character embedding. Thus, glyphs of
the same character type share a character embed-
ding, while glyphs of the same font share a font
variable. A corpus of I ‚àó J glyphs is modeled
with only I character embeddings and J font vari-
ables, as seen in the left half of Figure 2. This
modeling approach can be thought of as a form of
deep matrix factorization, where the content at any
given cell is purely a function of the vector repre-
sentations of the corresponding row and column.
We denote the full corpus of glyphs as a matrix

X = ((x11, ..., x1J) , ..., (xI1, ...xIJ)) and denote
the corresponding character embeddings as E =
(e1, ..., eI) and font variables as Z = (z1, ..., zJ).

Under our model, the probability distribution
over each image, conditioned on zj and ei, is pa-
rameterized by a neural network, described in the
next section and depicted in Figure 2. We denote
this decoder distribution as p(xij |zj ; ei, œÜ), and
let œÜ represent parameters, shared by all glyphs,
that govern how font variables and character em-
beddings combine to produce glyph images. In
contrast, the character embedding parameters, ei,
which feed into the decoder, are only shared by
glyphs of the same character type. The font vari-
ables, zj , are unobserved during training and will
be inferred at test time. The joint probability under
our model is given by:

p(X,Z;E, œÜ) =
‚àè
i,j

p(xij |zj ; ei, œÜ)p(zj)

3.1 Decoder Architecture

One way to encourage the model to learn disen-
tangled representations of style and content is by
choosing an architecture that introduces helpful
inductive bias. For this domain, we can think of
the character type as specifying the overall shape
of the image, and the font style as influencing the
finer details; we formulate our decoder with this
difference in mind. We hypothesize that a glyph



2198

can be modeled in terms of a low-resolution char-
acter representation to which a complex operator
specific to that font has been applied.

The success of transpose convolutional layers at
generating realistic images suggests a natural way
to apply this intuition. A transpose convolution1

is a convolution performed on an undecimated in-
put (i.e. with zeros inserted in between pixels in
alternating rows and columns), resulting in an up-
scaled output. Transpose convolutional architec-
tures generally start with a low resolution input
which is passed through several such layers, it-
eratively increasing the resolution and decreasing
the number of channels until the final output di-
mensions are reached. We note that the asymme-
try between the coarse input and the convolutional
filters closely aligns with the desired inductive bi-
ases, and therefore use this framework as a starting
point for our architecture.

Broadly speaking, our architecture represents
the underlying shape that defines the specific char-
acter type (but not the font) as coarse-grained in-
formation that therefore enters the transpose con-
volutional process early on. In contrast, the stylis-
tic content that specifies attributes of the specific
font (such as serifs, drop shadow, texture) is rep-
resented as finer-grained information that enters
into the decoder at a later stage, by parameteriz-
ing filters, as shown in the right half of Figure 2.
Specifically we form our decoder as follows: first
the character embedding is projected to a low res-
olution matrix with a large number of channels.
Following that, we apply several transpose convo-
lutional layers which increase the resolution, and
reduce the number of channels. Critically, the con-
volutional filter at each step is not a learned param-
eter of the model, but rather the output of a small
multilayer perceptron whose input is the font la-
tent variable z. Between these transpose convo-
lutions, we insert vanilla convolutional layers to
fine-tune following the increase in resolution.

Overall, the decoder consists of four blocks,
where each block contains a transpose convolu-
tion, which upscales the previous layer and re-
duces the number of channels by a factor of two,
followed by two convolutional layers. Each (trans-
pose) convolution is followed by an instance norm
and a ReLU activation. The convolution filters all
have a kernel size of 5 √ó 5. The character em-
bedding is reshaped via a two-layer MLP into a

1sometimes erroneously referred to as a ‚Äúdeconvolution‚Äù

8 √ó 8 √ó 256 tensor before being fed into the de-
coder. The final 64√ó 64 dimensional output layer
is treated as a grid of parameters which defines
the output distribution on pixels. We describe the
specifics of this distribution in the next section.

3.2 Projected Loss

The conventional approach for computing loss on
image observations is to use an independent out-
put distribution, typically a Gaussian, on each
pixel‚Äôs intensity. However, deliberate analysis of
the statistics of natural images has shown that im-
ages are not well-described in terms of statistically
independent pixels, but are instead better modeled
in terms of edges (Field, 1987; Huang and Mum-
ford, 1999). It has also been demonstrated that im-
ages of text have similar statistical distributions as
natural images (Melmer et al., 2013). Following
this insight, as our reconstruction loss we use a
heavy-tailed (leptokurtotic) distribution placed on
a transformed representation of the image, similar
to the approach of Barron (2019). Modeling the
statistics of font glyphs in this fashion results in
sharper samples, while modeling independent pix-
els with a Gaussian distribution results in blurry,
oversmoothed results.

More specifically, we adopt one of the strate-
gies employed in Barron (2019), and transform
image observations using the orthonormal variant
of the 2-Dimensional Discrete Cosine Transform
(2-D DCT-II) (Ahmed et al., 1974), which we de-
note as f : R64√ó64 ‚Üí R64√ó64 for our 64 √ó 64 di-
mensional image observations. We transform both
the observed glyph image and the corresponding
grid or parameters produced by our decoder be-
fore computing the observation‚Äôs likelihood.

This procedure projects observed images onto a
grid of orthogonal bases comprised of shifted and
scaled 2-dimensional cosine functions. Because
the DCT-II is orthonormal, this transformation is
volume-preserving, and so likelihoods computed
in the projected space correspond to valid mea-
surements in the original pixel domain.

Note that because the DCT-II is simply a rota-
tion in our vector space, imposing a normal dis-
tribution in this transformed space should have lit-
tle effect (ignoring the scaling induced by the di-
agonal of the covariance matrix of the Gaussian
distribution) as Euclidean distance is preserved
under rotations. For this reason we impose a
heavy-tailed distribution in this transformed space,



2199

ùí©(ùúá, Œ£)
Observation
Subsampling

ùëí1

ùëí3
Pointwise

Max

Latent Space ùëí1
ùëí2
ùëí3

ùïÇùïÉ

ùîºùëû logùëù

Transpose Convolutional 
Decoder

ùí©(0, I)
Discrete Cosine 

Transform

Convolutional 
Encoder

Figure 3: Depiction of the computation graph of the amortized variational lower bound (for simplicity, only one
font is shown). The encoder approximates the generative model‚Äôs true posterior over the font style latent variables
given the observations. It remains insensitive to the number of observations by pooling high-level features across
glyphs. For a more specific description of the encoder architecture details, see Appendix A.

specifically a Cauchy distribution. This gives the
following probability density function

g(x; xÃÇ, Œ≥) =
1

œÄŒ≥

(
1 +

(
f(x)‚àíf(xÃÇ)

Œ≥

)2)
where x is an observed glyph, xÃÇ is the location

parameter grid output by our decoder, and Œ≥ is a
hyperparameter which we set to Œ≥ = 0.001.

The Cauchy distribution accurately captures the
heavy-tailed structure of the edges in natural im-
ages. Intuitively, it models the fact that images
tend to be mostly smooth, with a small amount of
non-smooth variation in the form of edges. Com-
puting this heavy-tailed loss over the frequency
decomposition provided by the DCT-II instead of
the raw pixel values encourages the decoder to
generate sharper images without needing either
an adversarial discriminator or a vectorized rep-
resentation of the characters during training. Note
that while our training loss is computed in DCT-II
space, at test time we treat the raw grid of param-
eter outputs xÃÇ as the glyph reconstruction.

4 Learning and Inference

Note that in our training setting, the font variables
Z are completely unobserved, and we must induce
their manifold with learning. As our model is gen-
erative, we wish to optimize the marginal proba-
bility of just the observed X with respect to the

model parameters E and œÜ:

p(X;E, œÜ) =

‚à´
Z
p(X,Z;E, œÜ)dZ

However, the integral over Z is computation-
ally intractable, given that the complex relation-
ship between Z and X does not permit a closed
form solution. Related latent variable models
such as Variational Autoencoders (VAE) (Kingma
and Welling, 2014) with intractable marginals
have successfully performed learning by opti-
mizing a variational lower bound on the log
marginal likelihood. This surrogate objective,
called the evidence lower bound (ELBO), intro-
duces a variational approximation, q(Z|X) =‚àè
j q(zi|x1j , . . . , xIj) to the model‚Äôs true poste-

rior, p(Z|X). Our model‚Äôs ELBO is as follows:

ELBO =
‚àë
j

Eq[log p(x1j , . . . , xIj |zj)]

‚àíKL(q(zj |x1j , . . . , xIj)||p(zj))

where the approximation q is parameterized via
a neural encoder network. This lower bound can
be optimized by stochastic gradient ascent if q is
a Gaussian, via the reparameterization trick de-
scribed in (Kingma and Welling, 2014; Rezende
et al., 2014) to sample the expectation under q
while still permitting backpropagation.

Practically speaking, a key property which we
desire is the ability to perform consistent inference
over z given a variable number of observed glyphs



2200

in a font. We address this in two ways: through
the architecture of our encoder, and through a spe-
cial masking process in training; both of which are
shown in Figure 3.

4.1 Posterior Approximation

Observation Subsampling: To get reconstruc-
tions from only partially observed fonts at test
time, the encoder network must be able to infer zj
from any subset of (x1j , . . . , xIj). One approach
for achieving robustness to the number of obser-
vations is through the training procedure. Specifi-
cally when computing the approximate posterior
for a particular font in our training corpus, we
mask out a randomly selected subset of the char-
acters before passing them to the encoder. This
incentivizes the encoder to produce reasonable es-
timates without becoming too reliant on the fea-
tures extracted from any one particular character,
which more closely matches the setup at test time.
Encoder Architecture: Another way to encour-
age this robustness is through inductive bias in the
encoder architecture. Specifically we use a convo-
lutional neural network which takes in a batch of
characters from a single font, concatenated with
their respective character type embedding. Fol-
lowing the final convolutional layer, we perform
an elementwise max operation across the batch,
reducing to a single vector representation for the
entire font which we pass through further fully-
connected layers to obtain the output parameters
of q as shown in Figure 3. By including this ac-
cumulation across the elements of the batch, we
combine the features obtained from each charac-
ter in a manner that is largely invariant to the
total number and types of characters observed.
This provides an inductive bias that encourages the
model to extract similar features from each charac-
ter type, which should therefore represent stylistic
as opposed to structural properties.

Overall, the encoder over each glyph consists of
three blocks, where each block consists of a con-
volution followed by a max pool with a stride of
two, an instance norm (Ulyanov et al., 2016), and
a ReLU. The activations are then pooled across the
characters via an elementwise max into a single
vector, which is then passed through four fully-
connected layers, before predicting the parameters
of the Gaussian approximate posterior.
Reconstruction via Inference: At test time, we
pass an observed subset of a new font to our en-

coder in order to estimate the posterior over zj ,
and take the mean of that distribution as the in-
ferred font representation. We then pass this en-
coding to the decoder along with the full set of
character embeddings E in order to produce re-
constructions of every glyph in the font.

5 Experiments

We now provide an overview of the specifics of
the dataset and training procedure, and describe
our experimental setup and baselines.

5.1 Data

We compare our model against baseline sys-
tems at font reconstruction on the Capitals64
dataset (Azadi et al., 2018), which contains the 26
capital letters of the English alphabet as grayscale
64 √ó 64 pixel images across 10, 682 fonts. These
are broken down into training, dev, and test splits
of 7649, 1473, and 1560 fonts respectively.

Upon manual inspection of the dataset, it is
apparent that several fonts have an almost visu-
ally indistinguishable nearest neighbor, making
the reconstruction task trivial using a naive algo-
rithm (or an overfit model with high capacity) for
those particular font archetypes. Because these
datapoints are less informative with respect to a
model‚Äôs ability to generalize to previously unseen
styles, we additionally evaluate on a second test set
designed to avoid this redundancy. Specifically,
we choose the 10% of test fonts that have max-
imal L2 distance from their closest equivalent in
the training set, which we call ‚ÄúTest Hard‚Äù.

5.2 Baselines

As stated previously, many fonts fall into visually
similar archetypes. Based on this property, we use
a nearest neighbors algorithm for our first base-
line. Given a partially observed font at test time,
this approach simply ‚Äúreconstructs‚Äù by searching
the training set for the font with the lowest L2 dis-
tance over the observed characters, and copy its
glyphs verbatim for the missing characters.

For our second comparison, we use the Glyph-
Net model from Azadi et al. (2018). This ap-
proach is based on a generative adversarial net-
work, which uses discriminators to encourage the
model to generate outputs that are difficult to
distinguish from those in the training set. We
test from the publicly available epoch 400 check-
point, with modifications to the evaluation script



2201

Test Full Test Hard

Observations 1 2 4 8 1 2 4 8

NN 483.13 424.49 386.81 363.97 880.22 814.67 761.29 735.18
GlyphNet 669.36 533.97 455.23 416.65 935.01 813.50 718.02 653.57

Ours (FC) 353.63 316.47 293.67 281.89 596.57 556.21 527.50 513.25
Ours (Conv) 352.07 300.46 271.03 254.92 615.87 556.03 511.05 489.58

Table 1: L2 reconstruction per glyph by number of observed characters. ‚ÄúFull‚Äù includes the entire test set while
‚ÄúHard‚Äù is measured only over the 10% of test fonts with the highest L2 distance from the closest font in train.

to match the setup described above.
We also perform an ablation using fully-

connected instead of convolutional layers. For
more architecture details see Appendix A.

5.3 Training Details

We train our model to maximize the expected
log likelihood using the Adam optimization al-
gorithm (Kingma and Ba, 2015) with a step size
of 10‚àí5 (default settings otherwise), and perform
early stopping based on the approximate log like-
lihood on a hard subset of dev selected by the
process described earlier. To encourage robust-
ness in the encoder, we randomly drop out glyphs
during training with a probability of 0.7 (reject-
ing samples where all characters in a font are
dropped). All experiments are run with a dimen-
sionality of 32 for the character embeddings and
font latent variables. Our implementation is built
in PyTorch (Paszke et al., 2017) version 1.1.0.

6 Results

We now present quantitative results from our ex-
periments in both automated and human annotated
metrics, and offer qualitative analysis of recon-
structions and the learned font manifold.

6.1 Quantitative Evaluation

Automatic Evaluation: We show font reconstruc-
tion results for our system against nearest neigh-
bors and GlyphNet in Table 1. Each model is given
a random subsample of glyphs from each test font
(we measure at 1, 2, 4, and 8 observed characters),
with their character labels. We measure the av-
erage L2 distance between the image reconstruc-
tions for the unobserved characters and the ground
truth, after scaling intensities to [0, 1].

Our system achieves the best performance for
both the overall and hard subset of test for all num-

bers of observed glyphs. Nearest neighbors pro-
vides a strong baseline on the full test set, even
outperforming GlyphNet. However it performs
much worse on the hard subset. This makes sense
as we expect nearest neighbors to do extremely
well on any test fonts that have a close equiva-
lent in train, but suffer in fidelity on less tradi-
tional styles. GlyphNet similarly performs worse
on test hard, which could reflect the missing modes
problem of GANs failing to capture the full di-
versity of the data distribution (Che et al., 2016;
Tolstikhin et al., 2017). The fully-connected ab-
lation is also competitive, although we see that
the convolutional architecture is better able to in-
fer style from larger numbers of observations. On
the hard test set, the fully-connected network even
outperforms the convolutional system when only
one observation is present, perhaps indicating that
its lower-capacity architecture better generalizes
from very limited data.

Human Evaluation: To measure how consis-
tent these perceptual differences are, we also per-
form a human evaluation of our model‚Äôs recon-
structions against GlyphNet using Amazon Me-
chanical Turk (AMT). In our setup, turkers were
asked to compare the output of our model against
the output of GlyphNet for a single font given one
observed character, which they were also shown.
Turkers selected a ranking based on which recon-
structed font best matched the style of the ob-
served character, and a separate ranking based on
which was more realistic. On the full test set (1560
fonts, with 5 turkers assigned to each font), hu-
mans preferred our system over GlyphNet 81.3%
and 81.8% of the time for style and realism respec-
tively. We found that on average 76% of turkers
shown a given pair of reconstructions selected the
same ranking as each other on both criteria, sug-
gesting high annotator agreement.



2202

Figure 4: Reconstructions of partially observed fonts in
the hard subset from our model, GlyphNet, and nearest
neighbors. Given images of glyphs for ‚ÄòA‚Äô and ‚ÄòB‚Äô in
each font, we visualize reconstructions of the remain-
ing characters. Fonts are chosen such that the L2 loss
of our model on these examples closely matches the
average loss over the full evaluation set.

6.2 Qualitative Analysis

In order to fully understand the comparative be-
havior of these systems, we also qualitatively com-
pare the reconstruction output of these systems to
analyze their various failure modes, showing ex-
amples in Figure 4. We generally find that our
approach tends to produce reconstructions that,
while occasionally blurry at the edges, are gener-
ally faithful at reproducing the principal stylistic
features of the font. For example, we see that for
font (1) in Figure 4, we match not only the overall
shape of the letters, but also the drop shadow and
to an extent the texture within the lettering, while
GlyphNet does not produce fully enclosed letters
or match the texture. The output of nearest neigh-
bors, while well-formed, does not respect the style
of the font as closely as it fails to find a font in
training that matches these stylistic properties. In
font (2) the systems all produce a form of gothic
lettering, but the output of GlyphNet is again lack-
ing in certain details, and nearest neighbors makes
subtle but noticeable changes to the shape of the
letters. In the final example (3) we even see that
our system appears to attempt to replicate the pix-
elated outline, while nearest neighbors ignores this
subtlety. GlyphNet is in this case somewhat in-

Figure 5: t-SNE projection of latent font variables in-
ferred for the full training set, colored by k-means clus-
tering with k = 10. The glyph for the letter ‚ÄúA‚Äù for
each centroid is shown in overlay.

consistent, doing reasonably well on some letters,
but much worse on others. Overall, nearest neigh-
bors will necessarily output well-formed glyphs,
but with lower fidelity to the style, particularly on
more unique fonts. While GlyphNet does pick up
on some subtle features, our model tends to pro-
duce the most coherent output on harder fonts.

Since our model attempts to learn a smooth
manifold over the latent style, we can also perform
interpolation between the inferred font represen-
tations, something which is not directly possible
using either of the baselines. In this analysis, we
take two fonts from the same font family, which
differ along one property, and pass them through
our encoder to obtain the latent font variable for
each. We then interpolate between these values,
passing the result at various steps into our decoder
to produce new fonts that exists in between the ob-
servations. In Figure 6 we see how our model can
apply serifs, italicization, and boldness gradually
while leaving the font unchanged in other respects.
This demonstrates that our manifold is smooth and
interpretable, not just at the points corresponding
to those in our dataset. This could be leveraged to
modify existing fonts with respect to a particular
attribute to generate novel fonts efficiently.

6.3 Analysis of Learned Manifold
Beyond looking at the quality of reconstructions,
we also wish to analyze properties of the latent
space learned by our model. To do this, we use
our encoder to infer latent font variables z for
each of the fonts in our training data, and use



2203

Figure 6: Interpolation be-
tween font variants from
the same font family, show-
ing smoothness of the latent
manifold. Linear combina-
tions of the embedded fonts
correspond to outputs that
lie intuitively ‚Äúin between‚Äù.

ÔøΩÔøΩÔøΩ
ÔøΩÔøΩÔøΩ
ÔøΩÔøΩÔøΩ
ÔøΩÔøΩÔøΩ
ÔøΩÔøΩÔøΩ
ÔøΩÔøΩÔøΩ
ÔøΩÔøΩÔøΩ
ÔøΩÔøΩÔøΩ
ÔøΩÔøΩÔøΩ
ÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩ

ÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩ
ÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩ
ÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩ
ÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩ
ÔøΩÔøΩÔøΩÔøΩÔøΩ
ÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩ

Figure 7: t-SNE projection of latent font variables inferred on Google Fonts, colored by weight and category.

a t-SNE projection (Van Der Maaten, 2013) to
plot them in 2D, shown in Figure 5. Since the
broader, long-distance groupings may not be pre-
served by this transformation, we perform a k-
means clustering (Lloyd, 1982) with k = 10 on
the high-dimensional representations to visualize
these high-level groupings. Additionally, we dis-
play a sample glyph for the centroid for each clus-
ter. We see that while the clusters are not com-
pletely separated, the centroids generally corre-
spond to common font styles, and are largely ad-
jacent to or overlapping with those with similar
stylistic properties; for example script, handwrit-
ten, and gothic styles are very close together.

To analyze how well our latent embeddings cor-
respond to human defined notions of font style,
we also run our system on the Google Fonts 2

dataset which despite containing fewer fonts and
less diversity in style, lists metadata including nu-
merical weight and category (e.g. serif, hand-
writing, monospace). In Figure 7 we show t-
SNE projections of latent font variables from our
model trained on Google Fonts, colored accord-
ingly. We see that the embeddings do generally

2https://github.com/google/fonts

cluster by weight as well as category suggesting
that our model is learning latent information con-
sistent with how humans perceive font style.

7 Conclusion

We presented a latent variable model of glyphs
which learns disentangled representations of the
structural properties of underlying characters from
stylistic features of the font. We evaluated our
model on the task of font reconstruction and
showed that it outperformed both a strong nearest
neighbors baseline and prior work based on GANs
especially for fonts highly dissimilar to any in-
stance in the training set. In future work, it may be
worth extending this model to learn a latent mani-
fold on content as well as style, which could allow
for reconstruction of previously unseen character
types, or generalization to other domains where
the notion of content is higher dimensional.

Acknowledgments

This project is funded in part by the NSF under
grant 1618044 and by the NEH under grant HAA-
256044-17. Special thanks to Si Wu for help con-
structing the Google Fonts figures.

https://github.com/google/fonts


2204

References
Nasir Ahmed, T Natarajan, and Kamisetty R Rao.

1974. Discrete cosine transform. IEEE Transac-
tions on Computers.

Samaneh Azadi, Matthew Fisher, Vladimir G Kim,
Zhaowen Wang, Eli Shechtman, and Trevor Darrell.
2018. Multi-content GAN for few-shot font style
transfer. CVPR.

Jonathan T. Barron. 2019. A general and adaptive ro-
bust loss function. CVPR.

Taylor Berg-Kirkpatrick, Greg Durrett, and Dan Klein.
2013. Unsupervised transcription of historical doc-
uments. ACL.

Taylor Berg-Kirkpatrick and Dan Klein. 2014. Im-
proved typesetting models for historical OCR. ACL.

Neill DF Campbell and Jan Kautz. 2014. Learning a
manifold of fonts. ACM TOG.

Tong Che, Yanran Li, Athul Paul Jacob, Yoshua
Bengio, and Wenjie Li. 2016. Mode regularized
generative adversarial networks. arXiv preprint
arXiv:1612.02136.

Vincent Dumoulin and Francesco Visin. 2016. A guide
to convolution arithmetic for deep learning. arXiv
preprint arXiv:1603.07285.

David J. Field. 1987. Relations between the statistics
of natural images and the response properties of cor-
tical cells. JOSA A.

Leon A Gatys, Alexander S Ecker, and Matthias
Bethge. 2016. Image style transfer using convolu-
tional neural networks. CVPR.

Zhiting Hu, Zichao Yang, Xiaodan Liang, Ruslan
Salakhutdinov, and Eric P Xing. 2017. Controllable
text generation. arXiv preprint arXiv:1703.00955,
7.

Jinggang Huang and David Mumford. 1999. Statistics
of natural images and models. CVPR.

Diederik P Kingma and Jimmy Ba. 2015. Adam: A
method for stochastic optimization. ICLR.

Diederik P Kingma and Max Welling. 2014. Auto-
encoding variational bayes. ICLR.

Stuart Lloyd. 1982. Least squares quantization in pcm.
IEEE Transactions on Information Theory.

Tamara Melmer, Seyed Ali Amirshahi, Michael Koch,
Joachim Denzler, and Christoph Redies. 2013.
From regular text to artistic writing and artworks:
Fourier statistics of images with low and high aes-
thetic appeal. Frontiers in Human Neuroscience.

Adam Paszke, Sam Gross, Soumith Chintala, Gre-
gory Chanan, Edward Yang, Zachary DeVito, Zem-
ing Lin, Alban Desmaison, Luca Antiga, and Adam
Lerer. 2017. Automatic differentiation in PyTorch.
In NIPS Autodiff Workshop.

Danilo Jimenez Rezende, Shakir Mohamed, and Daan
Wierstra. 2014. Stochastic backpropagation and
approximate inference in deep generative models.
ICML.

Tianxiao Shen, Tao Lei, Regina Barzilay, and Tommi
Jaakkola. 2017. Style transfer from non-parallel text
by cross-alignment. NIPS.

Ilya O Tolstikhin, Sylvain Gelly, Olivier Bous-
quet, Carl-Johann Simon-Gabriel, and Bernhard
SchoÃàlkopf. 2017. Adagan: Boosting generative
models. In Advances in Neural Information Pro-
cessing Systems, pages 5424‚Äì5433.

Dmitry Ulyanov, Andrea Vedaldi, and Victor Lem-
pitsky. 2016. Instance normalization: The miss-
ing ingredient for fast stylization. arXiv preprint
arXiv:1607.08022.

Laurens Van Der Maaten. 2013. Barnes-Hut-SNE.
arXiv preprint arXiv:1301.3342.

Hong-Jian Xue, Xinyu Dai, Jianbing Zhang, Shujian
Huang, and Jiajun Chen. 2017. Deep matrix factor-
ization models for recommender systems. IJCAI.

Zichao Yang, Zhiting Hu, Ruslan Salakhutdinov, and
Taylor Berg-Kirkpatrick. 2017. Improved varia-
tional autoencoders for text modeling using dilated
convolutions. ICML.

Jun-Yan Zhu, Taesung Park, Phillip Isola, and Alexei A
Efros. 2017. Unpaired image-to-image translation
using cycle-consistent adversarial networks. ICCV.

A Appendix

A.1 Architecture Notation

We now provide further details on the specific ar-
chitectures used to parameterize our model and in-
ference network. The following abbreviations are
used to represent various components:
‚Ä¢ Fi : fully-connected layer with i hidden units
‚Ä¢ R : ReLU activation
‚Ä¢ M : batch max pool
‚Ä¢ S : 2√ó 2 spatial max pool
‚Ä¢ Ci : convolution filters with i filters of 5√ó 5,
2 pixel zero-padding, stride of 1, dilation of 1
‚Ä¢ I : instance normalization
‚Ä¢ Ti,j,k : transpose convolution with i filters of
5√ó5, 2 pixel zero-padding, stride of j, k pixel
output padding, dilation of 1, where kernel
and bias are the output of an MLP (described
below)
‚Ä¢ H : reshape to 26√ó 256√ó 8√ó 8



2205

A.2 Network Architecture
Our fully-connected encoder is:
F128-R-F128-R-F128-R-F1024-R-M-F128-R-F128-
R-F128-R-F64

Our convolutional encoder is:
C64-S-I-R-C128-S-I-R-C256-S-I-R-F1024-M-R-
F128-R-F128-R-F128-R-F64

Our fully-connected decoder is:
F128-R-F128-R-F128-R-F128-R-F128-R-F4096

Our transpose convolutional decoder is:
F128-R-F16384-R-H-T256,2,1-R-C256-I-R-C256-I-
R-T128,2,1-R-C128-I-R-C128-I-R-T64,2,1-I-R-C64-
I-R-C64-I-R-T32,1,0-I-R-C32-I-R-C1

MLP to compute a transpose convolutional param-
eter of size j is:
F128-R-Fj


