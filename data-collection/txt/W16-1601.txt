



















































Explaining Predictions of Non-Linear Classifiers in NLP


Proceedings of the 1st Workshop on Representation Learning for NLP, pages 1–7,
Berlin, Germany, August 11th, 2016. c©2016 Association for Computational Linguistics

Explaining Predictions of Non-Linear Classifiers in NLP

Leila Arras1, Franziska Horn2, Grégoire Montavon2,
Klaus-Robert Müller2,3, and Wojciech Samek1

1Machine Learning Group, Fraunhofer Heinrich Hertz Institute, Berlin, Germany
2Machine Learning Group, Technische Universität Berlin, Berlin, Germany

3Department of Brain and Cognitive Engineering, Korea University, Seoul, Korea
{leila.arras, wojciech.samek}@hhi.fraunhofer.de

klaus-robert.mueller@tu-berlin.de

Abstract

Layer-wise relevance propagation (LRP)
is a recently proposed technique for ex-
plaining predictions of complex non-linear
classifiers in terms of input variables. In
this paper, we apply LRP for the first time
to natural language processing (NLP).
More precisely, we use it to explain the
predictions of a convolutional neural net-
work (CNN) trained on a topic categoriza-
tion task. Our analysis highlights which
words are relevant for a specific prediction
of the CNN. We compare our technique
to standard sensitivity analysis, both qual-
itatively and quantitatively, using a “word
deleting” perturbation experiment, a PCA
analysis, and various visualizations. All
experiments validate the suitability of LRP
for explaining the CNN predictions, which
is also in line with results reported in re-
cent image classification studies.

1 Introduction

Following seminal work by Bengio et al. (2003)
and Collobert et al. (2011), the use of deep learn-
ing models for natural language processing (NLP)
applications received an increasing attention in re-
cent years. In parallel, initiated by the computer
vision domain, there is also a trend toward under-
standing deep learning models through visualiza-
tion techniques (Erhan et al., 2010; Landecker et
al., 2013; Zeiler and Fergus, 2014; Simonyan et
al., 2014; Bach et al., 2015; Lapuschkin et al.,
2016a) or through decision tree extraction (Krish-
nan et al., 1999). Most work dedicated to under-
standing neural network classifiers for NLP tasks
(Denil et al., 2014; Li et al., 2015) use gradient-
based approaches. Recently, a technique called
layer-wise relevance propagation (LRP) (Bach et

al., 2015) has been shown to produce more mean-
ingful explanations in the context of image classi-
fications (Samek et al., 2015). In this paper, we ap-
ply the same LRP technique to a NLP task, where
a neural network maps a sequence of word2vec
vectors representing a text document to its cat-
egory, and evaluate whether similar benefits in
terms of explanation quality are observed.

In the present work we contribute by (1) ap-
plying the LRP method to the NLP domain, (2)
proposing a technique for quantitative evaluation
of explanation methods for NLP classifiers, and
(3) qualitatively and quantitatively comparing two
different explanation methods, namely LRP and a
gradient-based approach, on a topic categorization
task using the 20Newsgroups dataset.

2 Explaining Predictions of Classifiers

We consider the problem of explaining a predic-
tion f(x) associated to an input x by assigning to
each input variable xd a scoreRd determining how
relevant the input variable is for explaining the
prediction. The scores can be pooled into groups
of input variables (e.g. all word2vec dimensions of
a word, or all components of a RGB pixel), such
that they can be visualized as heatmaps of high-
lighted texts, or as images.

2.1 Layer-Wise Relevance Propagation

Layer-wise relevance propagation (Bach et al.,
2015) is a newly introduced technique for obtain-
ing these explanations. It can be applied to various
machine learning classifiers such as deep convolu-
tional neural networks. The LRP technique pro-
duces a decomposition of the function value f(x)
on its input variables, that satisfies the conserva-
tion property:

f(x) =
∑

dRd. (1)

1



The decomposition is obtained by performing a
backward pass on the network, where for each
neuron, the relevance associated with it is redis-
tributed to its predecessors. Considering neurons
mapping a set of n inputs (xi)i∈[1,n] to the neuron
activation xj through the sequence of functions:

zij = xiwij +
bj
n

zj =
∑

izij

xj = g(zj)

where for convenience, the neuron bias bj has
been distributed equally to each input neuron, and
where g(·) is a monotonously increasing activation
function. Denoting by Ri and Rj the relevance
associated with xi and xj , the relevance is redis-
tributed from one layer to the other by defining
messages Ri←j indicating how much relevance
must be propagated from neuron xj to its input
neuron xi in the lower layer. These messages are
defined as:

Ri←j =
zij +

s(zj)
n∑

i zij + s(zj)
Rj

where s(zj) = � · (1zj≥0 − 1zj<0) is a stabilizing
term that handles near-zero denominators, with �
set to 0.01. The intuition behind this local rele-
vance redistribution formula is that each input xi
should be assigned relevance proportionally to its
contribution in the forward pass, in a way that the
relevance is preserved (

∑
iRi←j = Rj).

Each neuron in the lower layer receives rele-
vance from all upper-level neurons to which it con-
tributes

Ri =
∑

jRi←j .

This pooling ensures layer-wise conservation:∑
iRi =

∑
j Rj . Finally, in a max-pooling

layer, all relevance at the output of the layer
is redistributed to the pooled neuron with max-
imum activation (i.e. winner-take-all). An im-
plementation of LRP can be found in (La-
puschkin et al., 2016b) and downloaded from
www.heatmapping.org1.

2.2 Sensitivity Analysis
An alternative procedure called sensitivity analy-
sis (SA) produces explanations by scoring input
variables based on how they affect the decision
output locally (Dimopoulos et al., 1995; Gevrey

1Currently the available code is targeted on image data.

et al., 2003). The sensitivity of an input variable is
given by its squared partial derivative:

Rd =
( ∂f
∂xd

)2
.

Here, we note that unlike LRP, sensitivity analysis
does not preserve the function value f(x), but the
squared l2-norm of the function gradient:

‖∇xf(x)‖22 =
∑

dRd. (2)

This quantity is however not directly related to
the amount of evidence for the category to de-
tect. Similar gradient-based analyses (Denil et al.,
2014; Li et al., 2015) have been recently applied in
the NLP domain, and were also used by Simonyan
et al. (2014) in the context of image classification.
While recent work uses different relevance defini-
tions for a group of input variables (e.g. gradient
magnitude in Denil et al. (2014) or max-norm of
absolute value of simple derivatives in Simonyan
et al. (2014)), in the present work (unless other-
wise stated) we employ the squared l2-norm of
gradients allowing for decomposition of Eq. 2 as
a sum over relevances of input variables.

3 Experiments

For the following experiments we use the 20news-
bydate version of the 20Newsgroups2 dataset con-
sisting of 11314/7532 train/test documents evenly
distributed among twenty fine-grained categories.

3.1 CNN Model
As a document classifier we employ a word-based
CNN similar to Kim (2014) consisting of the fol-
lowing sequence of layers:

Conv −→ ReLU −→ 1-Max-Pool −→ FC

By 1-Max-Pool we denote a max-pooling
layer where the pooling regions span the whole
text length, as introduced in (Collobert et al.,
2011). Conv, ReLU and FC denote the con-
volutional layer, rectified linear units activation
and fully-connected linear layer. For building
the CNN numerical input we concatenate horizon-
tally 300-dimensional pre-trained word2vec3 vec-
tors (Mikolov et al., 2013), in the same order the
corresponding words appear in the pre-processed

2http://qwone.com/%7Ejason/20Newsgroups/
3GoogleNews-vectors-negative300,

https://code.google.com/p/word2vec/

2



document, and further keep this input representa-
tion fixed during training. The convolutional oper-
ation we apply in the first neural network layer is
one-dimensional and along the text sequence di-
rection (i.e. along the horizontal direction). The
receptive field of the convolutional layer neurons
spans the entire word embedding space in verti-
cal direction, and covers two consecutive words in
horizontal direction. The convolutional layer filter
bank contains 800 filters.

3.2 Experimental Setup

As pre-processing we remove the document head-
ers, tokenize the text with NLTK4, filter out punc-
tuation and numbers5, and finally truncate each
document to the first 400 tokens. We train
the CNN by stochastic mini-batch gradient de-
scent with momentum (with l2-norm penalty and
dropout). Our trained classifier achieves a classifi-
cation accuracy of 80.19%6.

Due to our input representation, applying LRP
or SA to our neural classifier yields one relevance
value per word-embedding dimension. From these
single input variable relevances to obtain word-
level relevances, we sum up the relevances over
the word embedding space in case of LRP, and
(unless otherwise stated) take the squared l2-norm
of the corresponding word gradient in case of
SA. More precisely, given an input document d
consisting of a sequence (w1, w2, ..., wN ) of N
words, each word being represented by a D-
dimensional word embedding, we compute the rel-
evance R(wt) of the tth word in the input docu-
ment, through the summation:

R(wt) =
D∑

i=1

Ri,t (3)

where Ri,t denotes the relevance of the input vari-
able corresponding to the ith dimension of the tth

word embedding, obtained by LRP or SA as spec-
ified in Sections 2.1 & 2.2.

4We employ NLTK’s version 3.1 recommended tok-
enizers sent tokenize and word tokenize, module
nltk.tokenize.

5We retain only tokens composed of the following char-
acters: alphabetic-character, apostrophe, hyphen and dot, and
containing at least one alphabetic-character.

6To the best of our knowledge, the best published
20Newsgroups accuracy is 83.0% (Paskov et al., 2013). How-
ever we notice that for simplification we use a fixed-length
document representation, and our main focus is on explain-
ing classifier decisions, not on improving the classification
state-of-the-art.

In particular, in case of SA, the above word rel-
evance can equivalently be expressed as:

RSA(wt) = ‖∇wtf(d)‖22 (4)
where f(d) represents the classifier’s prediction
for document d.

Note that the resulting LRP word relevance is
signed, while the SA word relevance is positive.

In all experiments, we use the term target class
to identify the function f(x) to analyze in the rel-
evance decomposition. This function maps the
neural network input to the neural network output
variable corresponding to the target class.

3.3 Evaluating Word-Level Relevances
In order to evaluate different relevance models, we
perform a sequence of “word deletions” (hereby
for deleting a word we simply set the word-vector
to zero in the input document representation), and
track the impact of these deletions on the classifi-
cation performance. We carry out two deletion ex-
periments, starting either with the set of test docu-
ments that are initially classified correctly, or with
those that are initially classified wrongly7. We es-
timate the LRP/SA word relevances using as target
class the true document class. Subsequently we
delete words in decreasing resp. increasing order
of the obtained word relevances.

Fig. 1 summarizes our results. We find that
LRP yields the best results in both deletion exper-
iments. Thereby we provide evidence that LRP
positive relevance is targeted to words that sup-
port a classification decision, while LRP negative
relevance is tuned upon words that inhibit this de-
cision. In the first experiment the SA classifica-
tion accuracy curve decreases significantly faster
than the random curve representing the perfor-
mance change when randomly deleting words, in-
dicating that SA is able to identify relevant words.
However, the SA curve is clearly above the LRP
curve indicating that LRP provides better expla-
nations for the CNN predictions. Similar results
have been reported for image classification tasks
(Samek et al., 2015). The second experiment indi-
cates that the classification performance increases
when deleting words with the lowest LRP rele-
vance, while small SA values points to words that
have less influence on the classification perfor-
mance than random word selection. This result

7For the deletion experiments we consider only the test
documents whose pre-processed length is greater or equal to
100 tokens, this amounts to a total of 4963 documents.

3



0 10 20 30 40 50
Number of deleted words

0.0

0.1

0.2

0.3

0.4

0.5

0.6

0.7

0.8

0.9

1.0

A
cc

u
ra

cy
  
  
(4

1
5
4
 d

o
cu

m
e
n
ts

)

LRP

SA

random

0 10 20 30 40 50
Number of deleted words

0.0

0.1

0.2

0.3

0.4

0.5

0.6

0.7

0.8

0.9

1.0

A
cc

u
ra

cy
  
  
(8

0
9
 d

o
cu

m
e
n
ts

)

LRP

SA

random

Figure 1: Word deletion on initially correct (left)
and false (right) classified test documents, using
either LRP or SA. The target class is the true
document class, words are deleted in decreasing
(left) and increasing (right) order of their rele-
vance. Random deletion is averaged over 10 runs
(std < 0.0141). A steep decline (left) and incline
(right) indicate informative word relevances.

can partly be explained by the fact that in contrast
to SA, LRP provides signed explanations. More
generally the different quality of the explanations
provided by SA and LRP can be attributed to their
different objectives: while LRP aims at decompos-
ing the global amount of evidence for a class f(x),
SA is build solely upon derivatives and as such
describes the effect of local variations of the in-
put variables on the classifier decision. For a more
detailed view of SA, as well as an interpretation
of the LRP propagation rules as a deep Taylor de-
composition see Montavon et al. (2015).

3.4 Document Highlighting
Word-level relevances can be used for highlighting
purposes. In Fig. 2 we provide such visualizations
on one test document for different relevance target
classes, using either LRP or SA relevance mod-
els. We can observe that while the word ride
is highly negative-relevant for LRP when the tar-
get class is not rec.motorcycles, it is pos-
itively highlighted (even though not heavily) by
SA. This suggests that SA does not clearly dis-
criminate between words speaking for or against
a specific classifier decision, while LRP is more
discerning in this respect.

3.5 Document Visualization
Word2vec embeddings are known to exhibit lin-
ear regularities representing semantic relation-

ships between words (Mikolov et al., 2013). We
explore if these regularities can be transferred to
a document representation, when using as a docu-
ment vector a linear combination of word2vec em-
beddings. As a weighting scheme we employ LRP
or SA scores, with the classifier’s predicted class
as the target class for the relevance estimation. For
comparison we perform uniform weighting, where
we simply sum up the word embeddings of the
document words (SUM).

For SA we use either the l2-norm or squared l2-
norm for pooling word gradient values along the
word2vec dimensions, i.e. in addition to the stan-
dard SA word relevance defined in Eq. 4, we use
as an alternative RSA(l2)(wt) = ‖∇wtf(d)‖2 and
denote this relevance model by SA(l2).

For both LRP and SA, we employ different
variations of the weighting scheme. More pre-
cisely, given an input document d composed of
the sequence (w1, w2, ..., wN ) of D-dimensional
word2vec embeddings, we build new document
representations d′ and d′e.w.8 by either using word-
level relevances R(wt) (as in Eq. 3), or through
element-wise multiplication of word embeddings
with single input variable relevances (Ri,t)i∈[1,D]
(we recall that Ri,t is the relevance of the input
variable corresponding to the ith dimension of the
tth word in the input document d). More formally
we use:

d′ =
N∑

t=1

R(wt) · wt

or

d′e.w. =
N∑

t=1


R1,t
R2,t

...
RD,t

� wt
where � is an element-wise multiplication. Fi-
nally we normalize the document vectors d′ resp.
d′e.w. to unit l2-norm and perform a PCA projec-
tion. In Fig. 3 we label the resulting 2D-projected
test documents using five top-level document cat-
egories.

For word-based models d′, we observe that
while standard SA and LRP both provide simi-
lar visualization quality, the SA variant with sim-
ple l2-norm yields partly overlapping and dense
clusters, still all schemes are better than uniform9

8The subscript e.w. stands for element-wise.
9We also performed a TFIDF weighting of word embed-

dings, the resulting 2D-visualization was very similar to uni-
form weighting (SUM).

4



LRP SA
It is the body's reaction to a strange environment.  It appears to be induced
partly to physical discomfort and part to mental distress.  Some people are 
more prone to it than others, like some people are more prone to get sick 
on a roller coaster ride than others.  The mental part is usually induced by 
a lack of clear indication of which way is up or down, ie: the Shuttle is 
normally oriented with its cargo bay pointed towards Earth, so the Earth 
(or ground) is "above" the head of the astronauts.  About 50% of the astronauts 
experience some form of motion sickness, and NASA has done numerous tests in 
space to try to see how to keep the number of occurances down.

It is the body's reaction to a strange environment.  It appears to be induced
partly to physical discomfort and part to mental distress.  Some people are 
more prone to it than others, like some people are more prone to get sick 
on a roller coaster ride than others.  The mental part is usually induced by 
a lack of clear indication of which way is up or down, ie: the Shuttle is 
normally oriented with its cargo bay pointed towards Earth, so the Earth 
(or ground) is "above" the head of the astronauts.  About 50% of the astronauts 
experience some form of motion sickness, and NASA has done numerous tests in 
space to try to see how to keep the number of occurances down.

It is the body's reaction to a strange environment.  It appears to be induced
partly to physical discomfort and part to mental distress.  Some people are 
more prone to it than others, like some people are more prone to get sick 
on a roller coaster ride than others.  The mental part is usually induced by 
a lack of clear indication of which way is up or down, ie: the Shuttle is 
normally oriented with its cargo bay pointed towards Earth, so the Earth 
(or ground) is "above" the head of the astronauts.  About 50% of the astronauts 
experience some form of motion sickness, and NASA has done numerous tests in 
space to try to see how to keep the number of occurances down.

It is the body's reaction to a strange environment.  It appears to be induced
partly to physical discomfort and part to mental distress.  Some people are 
more prone to it than others, like some people are more prone to get sick 
on a roller coaster ride than others.  The mental part is usually induced by 
a lack of clear indication of which way is up or down, ie: the Shuttle is 
normally oriented with its cargo bay pointed towards Earth, so the Earth 
(or ground) is "above" the head of the astronauts.  About 50% of the astronauts 
experience some form of motion sickness, and NASA has done numerous tests in 
space to try to see how to keep the number of occurances down.

It is the body's reaction to a strange environment.  It appears to be induced
partly to physical discomfort and part to mental distress.  Some people are 
more prone to it than others, like some people are more prone to get sick 
on a roller coaster ride than others.  The mental part is usually induced by 
a lack of clear indication of which way is up or down, ie: the Shuttle is 
normally oriented with its cargo bay pointed towards Earth, so the Earth 
(or ground) is "above" the head of the astronauts.  About 50% of the astronauts 
experience some form of motion sickness, and NASA has done numerous tests in 
space to try to see how to keep the number of occurances down.

It is the body's reaction to a strange environment.  It appears to be induced
partly to physical discomfort and part to mental distress.  Some people are 
more prone to it than others, like some people are more prone to get sick 
on a roller coaster ride than others.  The mental part is usually induced by 
a lack of clear indication of which way is up or down, ie: the Shuttle is 
normally oriented with its cargo bay pointed towards Earth, so the Earth 
(or ground) is "above" the head of the astronauts.  About 50% of the astronauts 
experience some form of motion sickness, and NASA has done numerous tests in 
space to try to see how to keep the number of occurances down.

 
 
s
c
i
.
s
p
a
c
e

 
 
 
 
 
 
 
s
c
i
.
m
e
d

 
 
 
 
r
e
c
.
m
o
t
o
r
c
y
c
l
e
s

Figure 2: Heatmaps for the test document sci.space 61393 (correctly classified), using either layer-
wise relevance propagation (LRP) or sensitivity analysis (SA) for highlighting words. Positive relevance
is mapped to red, negative to blue. The target class for the LRP/SA explanation is indicated on the left.

LRP e.w. LRP SUM

comp

rec

sci

politics

religion

SA e.w. SA SA (l2 )

Figure 3: PCA projection of the 20Newsgroups test documents formed by linearly combining word2vec
embeddings. The weighting scheme is based on word-level relevances, or on single input variable rel-
evances (e.w.), or uniform (SUM). The target class for relevance estimation is the predicted document
class. SA(l2) corresponds to a variant of SA with simple l2-norm pooling of word gradient values. All
visualizations are provided on the same equal axis scale.

5



weighting. In case of SA note that, even though
the power to which word gradient norms are raised
(l2 or l22) affects the present visualization experi-
ment, it has no influence on the earlier described
“word deletion” analysis.

For element-wise models d′e.w., we observe
slightly better separated clusters for SA, and a
clear-cut cluster structure for LRP.

4 Conclusion

Through word deleting we quantitatively evalu-
ated and compared two classifier explanation mod-
els, and pinpointed LRP to be more effective than
SA. We investigated the application of word-level
relevance information for document highlighting
and visualization. We derive from our empirical
analysis that the superiority of LRP stems from the
fact that it reliably not only links to determinant
words that support a specific classification deci-
sion, but further distinguishes, within the preemi-
nent words, those that are opposed to that decision.

Future work would include applying LRP to
other neural network architectures (e.g. character-
based or recurrent models) on further NLP tasks,
as well as exploring how relevance information
could be taken into account to improve the clas-
sifier’s training procedure or prediction perfor-
mance.

Acknowledgments

This work was supported by the German Ministry
for Education and Research as Berlin Big Data
Center BBDC (01IS14013A) and the Brain Korea
21 Plus Program through the National Research
Foundation of Korea funded by the Ministry of
Education.

References
S. Bach, A. Binder, G. Montavon, F. Klauschen, K.-

R. Müller, and W. Samek. 2015. On Pixel-Wise
Explanations for Non-Linear Classifier Decisions by
Layer-Wise Relevance Propagation. PLoS ONE,
10(7):e0130140.

Y. Bengio, R. Ducharme, P. Vincent, and C. Jauvin.
2003. A Neural Probabilistic Language Model.
JMLR, 3:1137–1155.

R. Collobert, J. Weston, L. Bottou, M. Karlen,
K. Kavukcuoglu, and P. Kuksa. 2011. Natural Lan-
guage Processing (Almost) from Scratch. JMLR,
12:2493–2537.

M. Denil, A. Demiraj, and N. de Freitas. 2014. Extrac-
tion of Salient Sentences from Labelled Documents.
Technical report, University of Oxford.

Y. Dimopoulos, P. Bourret, and S. Lek. 1995. Use of
some sensitivity criteria for choosing networks with
good generalization ability. Neural Processing Let-
ters, 2(6):1–4.

D. Erhan, A. Courville, and Y. Bengio. 2010. Under-
standing Representations Learned in Deep Architec-
tures. Technical report, University of Montreal.

M. Gevrey, I. Dimopoulos, and S. Lek. 2003. Review
and comparison of methods to study the contribu-
tion of variables in artificial neural network models.
Ecological Modelling, 160(3):249–264.

Y. Kim. 2014. Convolutional Neural Networks for
Sentence Classification. In Proc. of EMNLP, pages
1746–1751.

R. Krishnan, G. Sivakumar, and P. Bhattacharya. 1999.
Extracting decision trees from trained neural net-
works. Pattern Recognition, 32(12):1999–2009.

W. Landecker, M. Thomure, L. Bettencourt,
M. Mitchell, G. Kenyon, and S. Brumby. 2013. In-
terpreting Individual Classifications of Hierarchical
Networks. In IEEE Symposium on Computational
Intelligence and Data Mining (CIDM), pages
32–38.

S. Lapuschkin, A. Binder, G. Montavon, K.-R. Müller,
and W. Samek. 2016a. Analyzing Classifiers:
Fisher Vectors and Deep Neural Networks. In Proc.
of the IEEE Conference on Computer Vision and
Pattern Recognition (CVPR).

S. Lapuschkin, A. Binder, G. Montavon, K.-R. Müller,
and W. Samek. 2016b. The Layer-wise Relevance
Propagation Toolbox for Artificial Neural Networks.
JMLR. in press.

J. Li, X. Chen, E. Hovy, and D. Jurafsky. 2015. Visu-
alizing and Understanding Neural Models in NLP.
arXiv, (1506.01066).

M. Mikolov, K. Chen, G. Corrado, and J. Dean. 2013.
Efficient Estimation of Word Representations in
Vector Space. In Workshop Proc. ICLR.

G. Montavon, S. Bach, A. Binder, W. Samek, and K.-R.
Müller. 2015. Explaining NonLinear Classification
Decisions with Deep Taylor Decomposition. arXiv,
(1512.02479).

H.S. Paskov, R. West, J.C. Mitchell, and T. Hastie.
2013. Compressive Feature Learning. In Adv. in
NIPS.

W. Samek, A. Binder, G. Montavon, S. Bach, and K.-
R. Müller. 2015. Evaluating the visualization of
what a Deep Neural Network has learned. arXiv,
(1509.06321).

6



K. Simonyan, A. Vedaldi, and A. Zisserman. 2014.
Deep Inside Convolutional Networks: Visualising
Image Classification Models and Saliency Maps. In
Workshop Proc. ICLR.

M. D. Zeiler and R. Fergus. 2014. Visualizing and
Understanding Convolutional Networks. In ECCV,
pages 818–833.

7


