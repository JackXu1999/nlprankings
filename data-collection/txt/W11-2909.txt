










































Bayesian Network Automata for Modelling Unbounded Structures


Proceedings of the 12th International Conference on Parsing Technologies, pages 63–74,
October 5-7, 2011, Dublin City University. c© 2011 Association for Computational Linguistics

Bayesian Network Automata
for Modelling Unbounded Structures

James Henderson
Department of Computer Science

University of Geneva
Geneva, Switzerland

James.Henderson@unige.ch

Abstract
This paper proposes a framework which
unifies graphical model theory and formal
language theory through automata theory.
Specifically, we propose Bayesian Network
Automata (BNAs) as a formal framework
for specifying graphical models of arbitrar-
ily large structures, or equivalently, spec-
ifying probabilistic grammars in terms of
graphical models. BNAs use a formal au-
tomaton to specify how to construct an arbi-
trarily large Bayesian Network by connect-
ing multiple copies of a bounded Bayesian
Network. Using a combination of results
from graphical models and formal language
theory, we show that, for a large class of au-
tomata, the complexity of inference with a
BNA is bounded by the complexity of in-
ference in the bounded Bayesian Network
times the complexity of inference for the
equivalent stochastic automaton. This il-
lustrates that BNAs provide a useful frame-
work for developing and analysing models
and algorithms for structure prediction.

1 Introduction

Work in Computational Linguistics has devel-
oped increasingly sophisticated probabilistic mod-
els of language. For example, Latent Probabilistic
Context-Free Grammars (LPCFGs) (Matsuzaki et
al., 2005) have been developed with latent head
labels (Prescher, 2005), multiple latent variables
decorating each nonterminal (Musillo and Merlo,
2008), and a hierarchy of latent nonterminal sub-
categories (Liang et al., 2007). In this paper we
propose a general framework which facilitates the
specification and parsing of such complex mod-
els by exploiting graphical models to express local
bounded statistical relationships, while still allow-
ing the use of grammar formalisms to express the
unbounded nature of natural language.

Graphical models were developed as a unifica-
tion of probability theory and graph theory. They

have proved a powerful framework for specifying
and reasoning about probabilistic models. Dy-
namic Bayesian Networks (Ghahramani, 1998)
extend this framework to models which describe
arbitrarily long sequences. There has been work
applying ideas from graphical models to more
complex unbounded structures, such as natural
language parse trees (e.g. (Henderson and Titov,
2010)), but the power of the architectures pro-
posed for such extensions have not been formally
characterised.

The formal power of systems for specifying ar-
bitrarily large structures has been studied exten-
sively in the area of formal language theory. For-
mal language theory has proved a wide range of
equivalences and subsumptions between grammar
formalisms, the most well known example be-
ing the Chomsky hierarchy (i.e. finite languages
< regular languages < context-free languages <
context-sensitive languages < recursively enumer-
able languages). It has also demonstrated the
equivalence between formal grammars and for-
mal automata (e.g. finite state automata gener-
ate regular languages, push-down automata gener-
ate context-free languages, Turing machines gen-
erate recursively enumerable languages). While
grammar formalisms are generally more readable
than automata, they appear in a wide variety of
notational variants, whereas automata provide a
relatively consistent framework in which differ-
ent grammar formalisms can be compared. Au-
tomata also provide a clearer connection to Dy-
namic Bayesian Networks. For these reasons, this
paper uses automata theory rather than grammars,
although many of the ideas are trivially transfer-
able to grammars.

We propose Bayesian Network Automata
(BNAs) as a framework for specifying stochastic
automata which generate arbitrarily large (i.e. un-
bounded) structures. A BNA is a standard stochas-
tic automaton, but uses a Bayesian Network (BN)

63



to specify its stochastic transition function. For ex-
ample, the specification of a stochastic push-down
automaton (PDA) requires a specification of the
conditional probability distribution over push(X)
and pop actions given the current state, input sym-
bol, and top stack symbol. This distribution can be
specified in a BN. While not changing the theoreti-
cal properties of the stochastic automaton, this use
of BNs allows us to merge algorithms and theoret-
ical results from BNs with those for the stochastic
automaton, and thereby with those for the equiva-
lent probabilistic grammar formalism. In the PDA
example, the algorithm discussed in section 5.2 al-
lows us to sum over all possible values for X in
push(X) while at the same time summing over all
possible parse tree structures, as is used in unsu-
pervised grammar induction. We argue that this
merging with graphical model theory simplifies
the specification of more complex stochastic tran-
sition functions or grammar rules (e.g. (Musillo
and Merlo, 2008)), and reduces the need for ad-
hoc solutions to such inference problems.

BNAs can also be seen as a generalisation of
Dynamic Bayesian Networks to more complex un-
bounded structures. BNAs simplify the specifica-
tion and analysis of these more complex BNs by
drawing a distinction between a finite set of statis-
tical relationships, represented in a Bayesian Net-
work, and the recursive nature of the unbounded
structures, represented by the control mechanisms
of the automaton. To illustrate the usefulness of
this framework, we exploit the separation between
the automaton and the Bayesian Network to pro-
vide bounds on the complexity of inference in
some classes of BNAs. In particular, for a large
class of grammar formalisms, the complexity of
marginalising over variables in the Bayesian Net-
work and the complexity of marginalising over
structures from the automaton are independent
factors in the complexity of inference with the
BNA. We also exploit results from formal lan-
guage theory to characterise the power of previ-
ously proposed models in terms of the generative
capacity of their automata.

In the rest of this paper, we first define the
framework of Bayesian Network Automata, and
discuss its key properties. This provides the for-
mal mechanisms we need to compare various
Bayesian Network architectures that have been
previously proposed. We then provide results on
the efficiency of inference in BNAs.

2 Bayesian Network Automata

Bayesian Network Automata is a framework for
specifying stochastic automata which generate un-
bounded structures. Formally, a BNA is simply
an alternative notation for its equivalent stochastic
automaton. In this section, we provide a formal
definition of BNAs, without limiting ourselves to
any specific class of automata.

2.1 Generating Unbounded Structures with
BNAs

The purpose of a BNA is to specify a probabil-
ity distribution over an infinite set of unbound-
edly large structures. It does this by specifying an
equivalent stochastic automaton. Each complete
run of the automaton generates a single structure,
which is derived from the semantics of the opera-
tions performed by the automaton. The sequence
of operations performed in a complete run of the
automaton is called a derivation for its associated
structure. Typically automata are designed so that
derivations are isomorphic to structures, but even
if multiple derivations map to the same structure,
to specify a probability distribution over structures
it is enough to specify a probability distribution
over derivations. Stochastic automata do this in
terms of a generative process for derivations, gen-
erating each operation incrementally conditioned
on the derivation prefix of preceding operations.

Standard specifications of stochastic automata
include specific operations acting over specific
data structures. These data structures are used to
represent each state of the automaton (called the
configuration) as it proceeds through the compu-
tation. For example, the configuration of a finite
state machine is a single state symbol, and the con-
figuration of a push-down automaton is a single
state symbol plus an arbitrarily deep stack of stack
symbols.1 Here we do not want to restrict attention
to any specific class of automata, so the operations
and data structures of BNAs are necessarily ab-
stract (see the top half of table 1 below). For con-
creteness, we will illustrate the definitions with the
example of push-down automata for Probabilis-
tic Context-Free Grammars (PCFGs), illustrated

1Normally the configuration of an automaton includes an
input tape, whose string is either accepted or rejected by the
automaton. Because we are using stochastic automata as a
generative process, this string is considered part of the au-
tomaton’s output and therefore is not included in the config-
uration.

64



in figure 1. The left side of figure 1(a) illustrates
the data structures used in a PDA’s configuration.

Standard non-stochastic automata specify how
to choose which operation to apply in a given con-
figuration using a nondeterministic mapping spec-
ified in a finite transition table. The symbols input
to this mapping are determined by pointers into
the configuration, called read-heads.2 Stochas-
tic automata simply add a probability distribu-
tion over each nondeterministic choice of opera-
tion given read-head values. Thus, the probabilis-
tic transition table defines the conditional distribu-
tion over operations used in each step of generat-
ing a derivation.

The central difference between BNAs and
stochastic automata is that a BNA specifies its
probabilistic transition table in a Bayesian Net-
work (BN) (T in table 1). Such a BN is illus-
trated on the right side of figure 1(a), discussed
below. To determine the probability distribution
to be used for generating the next operation, the
values for the BN’s input variables must be set
to the values pointed to by the associated read-
heads in the automaton’s current configuration.
Since these values were themselves determined
by previous operations, setting the input variable
values is equivalent to equating the input vari-
ables with the relevant previous output variables
from these previous operations. The resulting
compound Bayesian network is illustrated in fig-
ure 1(b) as the graph of circles and arrows.

2.2 BNA Configurations
The configurations of BNAs make use of this com-
pound BN in specifying the intermediate states of
derivations. Each configuration includes a com-
pound BN that has been constructed by instantiat-
ing copies of the transition table BN T and equat-
ing their input variables with the appropriate pre-
vious output variables. To be more precise, this
compound BN gt is iteratively constructed by tak-
ing the BN gt−1 from the previous configuration,
setting the values 〈bt−1, wt−1, v1t−1, . . . , vkt−1〉
chosen in the previous operation, adding a new in-
stantiation of the transition table BN T , and equat-
ing its input variables with the appropriate vari-
ables from gt−1. This construction process is per-
formed by the inst function in table 1. In the exam-
ple in figure 1, the BN in (b) includes four copies

2For consistency, we generalise the notion of read-head
to include all inputs to the transition function, including the
state.

stack

top

state

Structure

j

i

...

k

Operation

top

state

argument2

argument1

output

action/state

Inputs

(a)

s−1

ø

ø

The

S

start

e

DET

s−1+2

e

NP

VP

s−1+2

N NP

S

VP

The

DET N

(b)

Figure 1: Illustrations of (a) a BNA specification and
(b) its configuration after generating a partial tree

of the BN in (a). It includes all the instances of
variables needed to specify the generation of the
portion of tree shown on the right of figure 1(b),
plus one set for choosing the next operation. The
actions are “s-1” for popping one symbol from the
stack and “s-1+2” for popping one and pushing
two symbols. The pattern of edges reflects which
variable instance was pointed to by the top of the
stack at the time when each copy of the BN in (a)
was added to the configuration’s BN.

To determine which variables from gt−1 should
be equated with each input variable in the new
instantiation of T , BNA configurations include a
second component that records information for the
read-heads. This structural component ct is equiv-
alent to the data structures employed in standard
automata, such as the stack of a PDA. The differ-
ence is that, instead of including symbols directly,
these data structures include indices of variables
in the compound BN gt. The values of these vari-
ables are the symbols of the equivalent configura-
tion in a standard automaton. This structural com-
ponent is illustrated at the top of figure 1(b), where
the indices are shown as dashed arrows. The read-
heads R read indexes from ct−1 and pass them to
the function inst so that it knows where to equate
the associated variables from the new instantiation
of T .

After constructing a new compound BN gt, we
can use it to determine the distribution over the
possible next operations by looking at the output
variables in the new instantiation of T . For book-

65



Table 1: Bayesian Network Automata specifications

Γ a finite set of symbols
Σ a subset of Γ which are output symbols
B a finite set of actions
f a halt action, f ∈ B
O a finite set of operations
〈b, w, v1, . . . , vk〉, s.t. b ∈ B, w ∈ Σ∗,
vi ∈ Γ ∪ {∅}

sc a start configuration structure
trans a mapping from configuration-

structure,action,Ik+2 triples to con-
figuration structures

R a mapping from configurations to read-
head vectors of indices Ir

sg a start configuration BN spec-
ifying P (b, w, v1, . . . , vk), s.t.
〈b, w, v1, . . . , vk〉 ∈ O

sI The k+2 indices in sg of the variables
b, w, v1, . . . , vk

T a Bayesian Network specifying
P (b, w, v1, . . . , vk|u1, . . . , ur), s.t.
〈b, w, v1, . . . , vk〉 ∈ O and ui ∈ Γ ∪ {∅}

inst a mapping from BN,BN,Ir,Ik+2 quadru-
ples to BNs, where inst(T, g,H, I) in-
stantiates T in g with u1, . . . , ur in-
dexed by H and b, w, v1, . . . , vk indexed
by I

keeping purposes, the indices It of these output
variables are stored in a third component of a BNA
configuration. These variables are shown in fig-
ure 1(b) as bold circles.

In summary, a BNA configuration consists of
three components 〈ct, gt, It〉, the structure ct, the
BN gt, and the next operation variables It in gt.

2.3 BNA Specifications

A formal definition of the generative process for
BNA derivations is given in figure 2, according
to the definitions given in table 1. The step of
stochastically generating an operation of the au-
tomaton is given in line 3, which chooses a com-
pletely specified operation from the finite set of
operations, O, which the equivalent stochastic au-
tomaton can perform. These operations specify
the basic action b (e.g. push or pop), any argu-
ments to that action v1, . . . , vk, and any string w
which should be concatenated to the output of the
automaton.3 The semantics of operations is de-

3In addition to generating the elements of a standard au-
tomaton’s “input tape”, this specification is notationally dif-

〈c0, g0, I0〉 ← 〈sc, sg, sI〉
For t = 0, 1, 2, . . .

Stochastically generate an operation
〈bt, wt, v1t, . . . , vkt〉 from distribution
defined by variables It in gt

Write wt to the output
If bt = f , then halt
ct+1 ← trans(ct, bt, It)
Deterministically generate unique It+1 ∈

Ik+2

gt+1 ← inst(T, gt, R(ct+1), It+1)
Set the values of variables It in gt+1 to
〈bt, wt, v1t, . . . , vkt〉

Figure 2: Pseudo-code for generating BNA derivations

fined by the function trans. As indicated in fig-
ure 2, trans(ct, bt, It) computes the next configu-
ration structure ct+1 given the previous configu-
ration structure ct, the operation’s action bt, and
the indices It of the variables bt, wt, v1t, . . . , vkt
which select the action and its arguments. Any
complete sequence of allowable operations is a
derivation.

When choosing the next operation to perform,
the stochastic automaton’s transition function can
only look at those symbols pointed to by its read-
heads, specified in R in table 1. For example,
the read-heads for a PDA identify the top of the
stack and the state. In a BNA, this transition
function is specified in the BN T , which has as
many input variables as there are read-heads (r),
and as many output variables as there are terms in
an operation (k + 2). Conditioning on the vari-
ables pointed to by the automaton’s read-heads is
achieved by the inst function. As indicated in fig-
ure 2, inst(T, gt, R(ct+1), It+1) computes the next
configuration BN gt+1 by adding to the previous
configuration BN gt an instance of T , such that
the input variables of T are equated with the vari-
ables R(ct+1) in gt pointed to by the read-heads,
and the output variables of T are assigned the new
unique indices It+1. Any other variables in T are
also assigned new unique indices.

The BN T is required to have no edges whose

ferent from standard ones in that there is a distinction between
the action b and the arguments to that action v1, . . . , vk.
As discussed below, this is to distinguish between decisions
which change the structure of statistical dependencies (the
actions) and decisions which only change the labels which
decorate that structure (the arguments).

66



destination is one of the input variables, so as
to prevent edges whose direction conflicts with
the temporal order of the automaton’s operations.
This property is important for efficient inference
in BNAs, as will be discussed in the next section.

3 Bounded Versus Unbounded
Generalisations

Bayesian networks capture generalisations by us-
ing the same parameters for more than one pattern
of variable values. By specifying how parameters
are reused across different cases, they specify how
to generalise from data on one pattern to previ-
ously unseen patterns. This specification is done
in the BN’s model structure. Because this model
structure is finite, BN’s can only express generali-
sations over a bounded number of cases.

In contrast, formal language theory captures
generalisations with formalisms that can handle
an infinite number of patterns. By proving that a
formalism can handle an infinite number of cases
for some variation, one proves that the formalism
must be capable of generalising across that varia-
tion. For example, we know that regular grammars
and finite state automata can generalise across po-
sitions in a string, because they can generate arbi-
trarily large strings despite having a finite specifi-
cation. They can model a finite number of specific
string positions, but there will always be a larger
string position for which they have no special case,
and therefore they must treat it in the same way as
some other smaller string position. Proofs of this
form are called pumping lemmas. Context-Free
Grammars and push-down automata generalise to
arbitrarily long dependencies within a string, us-
ing arbitrarily deep tree structures. Tree Adjoining
Grammars generalise to arbitrarily long dependen-
cies within these trees, using arbitrarily deep trees
which themselves generate trees.

3.1 Conditionally Bounded Models

Previous work has developed formalisms for spec-
ifying graphical models that generalise to an in-
finite number of cases. Dynamic Bayesian Net-
works (DBNs) (Ghahramani, 1998), such as Hid-
den Markov Models, and linear-chain Conditional
Random Fields (Lafferty et al., 2001) generalise to
unbounded string lengths. These models specify
both a template model structure which is used for
each position in the string, and the model struc-
ture which connects two adjacent position in the

string (or any finite window of contiguous posi-
tions). In addition, the strings are padded with
start and stop symbols, with the constraint that no
positions can exist beyond these symbols. These
symbols act like the halt action for BNAs, and
are used to ensure a proper probability distribution
over sequence lengths. Given a string of a specific
length, it is possible to construct the entire relevant
model structure for that string, and then apply nor-
mal graphical model inference techniques to this
constructed model.

If you are given the string length, it is some-
times possible to take this approach even for
more complex models, such as Probabilistic Con-
text Free Grammars. It is common practice with
PCFGs to transform them into a form where the
depth of the tree is bounded by the length of
the string. This transformation (such as binari-
sation, or Chomsky normal form) does not gen-
erate the same tree structures, but it does gener-
ate the same strings. With the transformed PCFG,
given the string length, it is possible to construct
the whole relevant bounded-depth model structure
for that string, for example using case-factor di-
agrams (McAllester et al., 2004), or sum-product
networks (Poon and Domingos, 2011). Inference
can then be done, for example, using belief propa-
gation (Smith and Eisner, 2008).

However, this approach is limited to infer-
ence problems where the string length is known,
which in turn limits the potential training methods.
We cannot necessarily use this pre-construction
method if we want to take a generative approach,
where the string length is determined by a genera-
tive process, or if we want to do incremental inter-
pretation, where we want to do inference on pre-
fixes of the string without knowing to total string
length. In this situation, we might need to con-
sider an infinite amount of model structure, for all
the possible string lengths. In particular, this is
true for undirected graphical models, such as Con-
ditional Random Fields, which are globally nor-
malised (c.f. (Rohanimanesh et al., 2009)).

3.2 Model Structure Prediction

BNAs allow us to do generative, incremental infer-
ence because they use directed graphical models,
in particular Bayesian Networks. Because BNs are
locally normalised, it is possible to solve some in-
ference problems independently of the infinite un-
constrained portion of the model structure. For ex-

67



ample, a Hidden Markov Model can be applied to
any string prefix to infer the probability distribu-
tion over the following element of the string, with-
out needing to know the length of the total string.
In general, an inference can be done independently
of the unconstrained portion of the model structure
provided there are no edges directed from the un-
constrained portion to the portion where informa-
tion is given. More precisely, consider the set G
of complete model structures consistent with the
given visible variables V , and let G be the inter-
section of all the G′ ∈ G. If for all G′ ∈ G, for
all h ∈ (G′ − G), and for all v ∈ V , there is no
directed path from h to v, then for all G′ ∈ G,
P (V |G′) = P (V |G). This follows directly from
well known properties of BNs, which rely on the
fact that normalising P (V |G) can be done locally
to G. Thus

∑
G′ P (G

′)P (V |G′) = P (V |G), and
therefore we can do inference looking only at the
known portion G of the model structure.4

For BNAs, this means that, given a prefix of
an automaton’s derivation, we can determine a
unique model structure which is sufficient to com-
pute the probability distribution for the automa-
ton’s next operation. There is no need to consider
all possible future derivations.

Interestingly, not all variables in the derivation
prefix must be given in order to determine a finite
specification of the model structure. In some for-
malisms, it is sufficient to know the length of the
string generated by the derivation prefix. But this
approach may not take full advantage of existing
algorithms for doing inference with grammars or
automata. On the other extreme, the approach we
took in section 2 of completely specifying all vari-
ables does not take full advantage of existing algo-
rithms for doing inference in graphical models. As
we will see in section 5.2, BNAs provide a frame-
work where these two types of algorithms can be
combined in a conceptually and computationally
modular way.

More specifically, BNA derivations make a dis-
tinction between variables bt which specify the ac-
tion and variables wt, v1t, . . . , vkt which specify
the arguments to the action. The action (e.g. push
versus pop) must be sufficient to determine the
structure ct of the configuration. The configura-

4This argument for directed models was recognised pre-
viously in (Titov and Henderson, 2007). Garg and Hender-
son (2011) proposes a model which mixes directed and undi-
rected edges, but which still has this property because undi-
rected edges are all local to individual derivation steps.

tion structure in turn determines the placement of
the read-heads R(ct), which determines the model
structure of the configuration’s BN gt. Therefore,
the sequence of derivation actions b0, . . . bn is suf-
ficient to uniquely determine the model structure
of the final configuration’s BN gn+1.

In BNA derivations, the arguments (e.g.
push(A) versus push(B)) only affect the labels
in the configuration’s BN gt. We can exploit this
fact by only choosing specific values for the action
variables, and leaving the argument variables un-
specified. The constructed BN gt will then specify
a distribution over values for the argument vari-
ables. For example, a BNA configuration for a
PDA must specify a specific depth for the stack,
but could specify a distribution over the symbols
in each position on a stack of that depth.

To illustrate this distinction between actions and
arguments, consider an alternative two-pass gen-
erative process for BNA derivations. In the first
pass of the generative process, it chooses the se-
quence of actions b0, . . . bn for a derivation, which
predicts the model structure of the final configura-
tion’s BN gn+1. The probability distributions for
generating this sequence is defined by marginal-
ising out the values for the argument variables.
The probability distribution over argument vari-
ables defined by the final configuration’s BN gn+1
then defines a distribution over argument variable
values. In the second pass of the generative pro-
cess, the argument values are chosen according to
this distribution. This two-pass generative process
will generate the same distributions over deriva-
tions as the characterisation in section 2.3.

4 Related Architectures and Grammars

In this section we discuss how BNAs are related to
some previous proposals for the probabilistic mod-
elling of unbounded structures.

Dynamic Bayesian Networks extend BNs to
arbitrarily long sequences. A DBN specifies a
bounded BN that models one position in the se-
quence, including input and output variables. A
new instance of this BN is created for each po-
sition in the given sequence, with the outputs for
one position’s BN equated with the inputs of the
subsequent position’s BN. DBNs are equivalent to
BNAs with finite state automata. The bounded
BN of the DBN corresponds to the T of the BNA.
The DBN has no need for an explicit representa-
tion of the BNA’s configuration structure because

68



the configuration of a finite state automaton con-
sists only of a state variable with a bounded num-
ber of values, so all information about the config-
uration can be encoded in the bounded BN. The
only structure-modifying actions which are neces-
sary are continue versus stop, which are used to
generate the length of the sequence.

Switching DBNs (Ghahramani, 1998; Murphy,
2002) are DBNs which include variables that
switch between multiple BN models during each
position in the sequence. They are also covered
by finite state BNAs, by the same argument. Al-
though different BN models may be chosen for
different positions, there are only a bounded num-
ber of possible BNs, so they can all be included
in a single T . The switching decisions must be
encoded in the structure-modifying actions.

The previous work on graphical models which
is closest to our proposal is Incremental Sigmoid
Belief Networks (ISBNs) (Henderson and Titov,
2010). Sigmoid Belief Networks (SBNs) (Neal,
1992) are a type of Bayesian Network, and IS-
BNs use the same technique as here for modelling
arbitrarily large structures, namely incrementally
constructing an SBN that generates the deriva-
tion of the structure. Henderson and Titov (2010)
used this framework to reinterpret previous work
on neural network parsing (Henderson, 2003) as
an approximation to inference in ISBNs, and pro-
posed another approximate inference method for
natural language parsing. However, the control
mechanism needed to construct an SBN for a
derivation was not formalised.

Without any formal specification of how to con-
struct an SBN for a derivation, it is hard to deter-
mine the power of the ISBN architecture. The spe-
cific ISBN models developed for natural language
parsing cannot be expressed with a finite state con-
troller, and thus they are not DBNs. They re-
quire at least a push-down automaton. The deriva-
tions modelled in (Henderson and Titov, 2010)
are predictive LR derivations, which are equiva-
lent in power to context-free derivations. How-
ever, the statistical dependencies which are speci-
fied as edges in their constructed SBN are not re-
stricted to the context-free structure of the deriva-
tions. Edges may refer to any “structurally local”
variables decorating the derived tree, even if they
are not immediately local (such as the leftmost sib-
ling). To translate such a model into a BNA, the
read-head R would have to have access to symbols

which are not on the top of the stack, and there-
fore the derivation structure would not be context-
free. Given the restriction of locality, it is probably
possible to devise chains of variables which pass
the necessary information through the context-free
structure of the tree. But without such a transfor-
mation, our analysis of inference algorithms in the
next section suggests that it would be difficult to
devise efficient algorithms for exact inference with
this model. Henderson and Titov (2010) avoid this
question by only considering approximate infer-
ence methods, since inference in the bounded SBN
of their model is already intractable even before
combining it with a parsing algorithm.

Koller et al. (1997) propose a framework for
specifying complex recursive probabilistic models
in terms of a stochastic functional programming
language. As with BNAs, these models are lo-
cally normalised. They propose an inference al-
gorithm for this framework, and discuss Bayesian
networks and context-free grammars as examples
of what can be implemented in it. The use of a
functional programming language suggests a close
relationship to the context-free derivation struc-
tures discussed in the next section.

From the grammar formalism side, the model
that is closest to our proposal is Latent Probabilis-
tic Context-Free Grammars (LPCFGs) (Matsuzaki
et al., 2005). LPCFGs are PCFGs where the non-
terminal labels are augmented with a latent vari-
able. There has been much work recently on dif-
ferent training methods and different restrictions
to the pattern of values which the latent variables
are allowed to take (e.g. (Matsuzaki et al., 2005;
Prescher, 2005; Petrov et al., 2006; Musillo and
Merlo, 2008)). LPCFGs can be modelled as BNAs
with push-down automata. The bounded BN of
the equivalent BNA includes both a variable for
the visible nonterminal label of the LPCFG and
a variable for the latent extension of the nonter-
minal label. As a more specific example, the la-
tent head labels of (Prescher, 2005) could be spec-
ified using a BN with a switching variable that
selects which child’s head variable is propagated
to the head variable of the parent. Musillo and
Merlo (2008) extend LPCFGs to include multiple
latent variables decorating each nonterminal, with
linguistically motivated constraints on how they
can be related to each other. The BN of BNAs
would provide a more perspicuous method to ex-
press these constraints.

69



5 Inference in Bayesian Network
Automata

To illustrate the usefulness of the BNA framework,
we look at algorithms for exact inference in BNAs.
The way in which BNAs allow us to separate is-
sues of structure from issues of labelling greatly
facilitates the analysis and design of algorithms for
calculating probabilities and exploring the space
of derivations. In many important cases, previ-
ous algorithms for grammars can be combined di-
rectly with previous algorithms for Bayesian Net-
works, and the resulting algorithm has a complex-
ity where the complexity of its two components
are independent factors.

A BNA is a specification of a generative proba-
bilistic model, and as such can be used to answer
many questions about the probability distributions
for some variables given others. We might want
to find the most probable derivation given some
sequence of output symbols (as in statistical pars-
ing, or decoding), to marginalise over the deriva-
tions (as in language modelling), or to find the
most probable values for a subset of the variables
while marginalising over others (as in latent vari-
able models of parsing). Although in some ways
the most interesting, this last class of problems is
in general NP-hard. Even for models based on fi-
nite state automata (e.g. HMMs), mixing the max-
imisation of some probabilities with the sum over
others results is an NP-hard problem (Lyngsø and
Pedersen, 2002). Because for reasons of space we
are limiting our discussion to exact inference, we
therefore also limit our discussion to the first two
types of problems. However, the arguments given
in section 5.2 for marginalising over all the un-
known variables can also be applied to many ap-
proximations to latent variable models of parsing.

5.1 Fully-Specified BN Computation

The first case we consider is when the space of in-
puts and operations of the BN T is small enough
that it is feasible to compute the complete condi-
tional probability distribution of all possible op-
erations given each possible input. In this case,
the conditional probability distribution can be pre-
computed and used to fill the transition table for
the equivalent stochastic automaton. Then any
standard algorithm for the stochastic automaton
can be applied. Under this assumption, if the
complexity of inferring the complete conditional
probability distribution for T is O(FB), and the

complexity of the standard algorithm is O(FA),
then the complexity of the complete problem is
O(FB + FA). In other words, computation time
is dominated by whichever of these complexities
is worse. For most previously proposed statistical
parsing models, the parsing complexity does dom-
inate and this pre-compilation strategy is in effect
adopted. However, the interest of the BNA frame-
work is in allowing the specification of more com-
plicated models, where inference in the BN T is
not so simple.

The second case we consider also uses any stan-
dard algorithm for the stochastic automaton, but
computes probabilities with the BN T on-line dur-
ing the running of the algorithm. This strategy
may require forward inference of the distribution
over operations given a specific input, or backward
inference of the distribution over inputs given a
specific operation, but in general it avoids the need
to compute the complete joint distribution over
both. In this case, if the complexity of the nec-
essary inference in T is O(F ′B), and the complex-
ity of the standard algorithm is O(FA), then the
complexity of the complete problem is O(F ′BFA).
This is a good strategy if O(F ′B) is much smaller
than O(FB) from the previous case, as might be
the case for lexicalised models, where O(FB) is a
function of the vocabulary size but O(F ′B) is only
a function of the number of words in the sentence.

5.2 Marginalisation in the BN

Both of the above cases require computing prob-
abilities given completely-specified values for all
the inputs and/or all the outputs of the BN T . To
fully exploit previous work on inference in BNs,
we would like to develop inference algorithms
which work directly with probability distributions
over values for both the inputs and outputs of T .
A full treatment of such algorithms is beyond the
scope of this paper, but here we provide some re-
sults on cases where inference algorithms for BNs
can be extended to BNAs. In particular, we look
at what classes of automata can be combined with
belief propagation (Pearl, 1988).

Belief propagation is a general method for
efficiently computing marginal probabilities in
Bayesian Networks (i.e. summing over latent vari-
ables). Provided that the BN has a tree struc-
ture (ignoring the directionality of the edges), this
algorithm is guaranteed to stop with the exact
marginal probability in time linear in the size of

70



the BN. Therefore, if we can guarantee that the
configuration BN gt constructed by a BNA is al-
ways a tree, then we can apply belief propagation
to inference in gt, or in any sub-graph of gt.

The structure of gt reflects the structure of con-
ditioning allowed by the derivations of the au-
tomaton. In formal language theory, this is called
the derivation structure. If necessary, we can ab-
stract away from any undirected cycles that might
be introduced by the specific form of the BN T by
collapsing all the non-input non-action variables
in T into a single variable. By using this collapsed
version of T to construct gt, the structure of gt
becomes isomorphic to the derivation structure.5

Thus, we can guarantee that (a version of) gt will
have a tree structure if the derivation structures of
the automaton are trees, or more precisely, if the
automaton has context-free derivation structures.

The most important class of grammar for-
malisms which have context-free derivation struc-
tures is Linear Context-Free Rewriting Systems
(LCFRS) (Weir, 1988). In addition to Context-
Free Grammars, popular examples of LCFRS in-
clude Synchronous Context-Free Grammars and
Tree Adjoining Grammars (Joshi, 1987). This lat-
ter grammar formalism can specify classes of lan-
guages much larger than context-free languages,
and Synchronous CFGs express languages over
pairs of strings. For our purposes, the observations
can be of any form and the formalism can have
any method for combining the observations gen-
erated by sub-derivations, provided the derivation
structures are context-free and there exists an algo-
rithm for marginalising over the derivation struc-
tures given an observation.

Given an automaton whose derivations have a
context-free tree structure, we can apply belief
propagation to compute the marginal probability
for any derivation generated by that automaton
(if necessary collapsing the non-input non-action
variables in T into a single variable). This will
be feasible if it is feasible to propagate beliefs
through one instance of T . This requirement is
different from the one in the previous subsection in
that the given information is a probability distribu-
tion (the belief), whereas above it was assumed to

5Note that all action variables bt′ in gt will have known
values, because this is necessary in order to know the model
structure. Thus, the concern here is the structure of the
interdependencies between the possibly-unknown variables
wt′ , v1t′ , . . . , vkt′ in gt, because these are the variables we
need to marginalise over using belief propagation.

In(i− 1, i, a) = P (a⇒wi|a)
In(i, k, a) =

k−1∑
j=i+1

∑
b,c

P (a⇒bc|a)In(i, j, b)In(j, k, c)

Out(0, |w|, c) = 1 if c = S; 0 otherwise
Out(i, k, c) =

i−1∑
j=0

∑
a,b

Out(j, k, a)In(j, i, b)P (a⇒bc|a) +

|w|∑
j=k+1

∑
a,b

Out(i, j, a)P (a⇒cb|a)In(k, j, b)

Figure 3: The Inside-Outside algorithm for PCFGs,
where a, b, c are symbols, i, j, k are indices in string
w, and a⇒bc, a⇒wi are CFG rules

be completely specified values. So, assuming that
it is feasible to propagate beliefs through T and
given an automaton with context-free derivations,
we can feasibly compute the marginal probability
of any derivation using belief propagation.

So far in this subsection we have only con-
sidered inference for a given derivation structure.
In general, inference in BNAs requires marginal-
ising both over labellings and over the struc-
ture itself, as is commonly required for unsu-
pervised or partially-supervised grammar induc-
tion. Marginalising over structures can be done
with the inside-outside algorithm for Context-Free
Grammars (Baker, 1979) (given in figure 3) or
the inside-outside algorithm for Tree-Adjoining
Grammars (Schabes, 1992). These are dynamic
programming algorithms. The inside calculations
are very similar to bottom-up parsing algorithms
such as CKY (Younger, 1967), except they com-
pute sums of probabilities instead of taking the
maximum probability. The outside calculations
are also done in a single pass, but top-down.

Both inside and outside calculations work by
incrementally considering equivalence classes of
increasingly large sub-graphs of the derivation
structure. For example in the equations in fig-
ure 3, each step computes a sum over the vari-
ous ways that an operation can be used to con-
struct a larger sub-derivation out of smaller ones,
such that they all have the same start i, end k
and label. As discussed at the end of section 3.2,
with BNAs we can easily represent In(i, k, ·) and
Out(i, k, ·) as distributions over labels, i.e. the
beliefs. The sums over symbols in these equa-

71



tions (
∑

b,c P (a⇒ bc|a)In(i, j, b)In(j, k, c) and∑
a,b Out(i, j, a)P (a⇒cb|a)In(k, j, b)) can then

be done using belief propagation through the BN
representation of P (a⇒cb|a).

If the configuration BN gt is a tree, then any
connected sub-graph of gt will also be a tree, so we
can apply belief propagation to the sub-graph of gt
for any of the sub-derivations. In addition, belief
propagation can be applied either forward or back-
ward through the edges of gt, so the probability
distribution for any node in a sub-graph of gt can
be computed in a single pass over the sub-graph
by propagating beliefs towards the desired node.
This allows belief propagation to be integrated into
an inside-outside algorithm; whenever the inside-
outside algorithm considers building larger sub-
derivations out of smaller ones, belief propaga-
tion is applied to continue the propagation of be-
liefs out of the smaller sub-derivations’ graphs and
into the instance of T for the operation used in the
combination. This belief propagation can then be
interleaved with the sums over structural alterna-
tives (the sums over j in figure 3).

Because each use of an operation considered
by the inside-outside algorithm corresponds to be-
lief propagation through a single instance of T ,
the time complexity of performing belief propaga-
tion through T is a constant factor in the complex-
ity of the entire algorithm. For the inside part of
the algorithms, only backward inference through
T is required, and then for the outside computa-
tions forward inference is required. Thus, given
an O(FA) inside-outside inference algorithm and
an O(F ′′B) inference algorithm for propagating be-
liefs (forward or backward) through T , the com-
plexity of parsing will be O(F ′′BFA). As with
the results from the previous subsection, this com-
plexity result factors the two components of the
algorithm.

6 Conclusions

In this article we have proposed a framework
for specifying and reasoning with complex prob-
abilistic models of unbounded structures, called
Bayesian Network Automata. The BNA frame-
work combines the theory of graphical models
with automata theory and formal language theory.
It uses Bayesian Networks to provide a perspic-
uous representation of local statistical generalisa-
tions. It uses automata to provide a precise spec-
ification of how to generalise to arbitrarily large

model structures, even when the model structure
must be predicted during inference. Together they
provide a precise and perspicuous representation
of probability distributions over unbounded struc-
tures.

Using this framework, we have clarified the
power of various previously proposed probabilis-
tic models of unbounded structures. Without any
additional control structure, Dynamic Bayesian
Networks are equivalent to BNAs with finite state
automata. This limited power also applies to
switching DBNs. Incremental Sigmoid Belief
Networks potentially have greater power, but pre-
vious work did not formally characterise the na-
ture of the additional control structure which they
employ. The model of parsing which has been pro-
posed for ISBNs appears to be a BNA with a push-
down automaton, but the nature of the BN used
prevents direct application of the efficient parsing
methods we have discussed.

We have also used this framework to explore
how the complexity of inference with the bounded
Bayesian Network interacts with the complexity
of inference with the automaton. We have shown
that for a large class of automata (which can gen-
erate more than just context-free languages), the
complexity of inference with a BNA is simply the
multiplication of the complexity of inference in
the Bayesian Network times the complexity of in-
ference with the automaton.

BNAs have the potential to greatly expand the
class of problems which we can effectively model
with graphical models, through their simple mech-
anism for increasing the power of these models
and the large body of existing theory and algo-
rithms that help us limit this power in ways that
retain tractability. They provide a useful frame-
work for future work on many issues, includ-
ing approximate inference methods that interface
well with parsing algorithms. We believe that the
BNA framework will be most useful with large
Bayesian Networks where only approximate infer-
ence methods are tractable.

Acknowledgements

The author would like to thank the many review-
ers that have contributed to making this a better
paper. This work was partly funded by Euro-
pean Community FP7 grant 216594 (CLASSiC,
www.classic-project.org) and Swiss NSF grant
200021 125137.

72



References

J. K. Baker. 1979. Trainable grammars for speech
recognition. The Journal of the Acoustical So-
ciety of America, 65(S1):S132–S132.

Nikhil Garg and James Henderson. 2011. Tem-
poral restricted boltzmann machines for depen-
dency parsing. In Proc. 49th Annual Meeting of
the Association for Computational Linguistics:
Human Language Technologies, pages 11–17.

Zoubin Ghahramani. 1998. Learning dy-
namic bayesian networks. In C. Giles and
M. Gori, editors, Adaptive Processing of Se-
quences and Data Structures, pages 168–197.
Springer-Verlag, Berlin, Germany.

James Henderson and Ivan Titov. 2010. Incre-
mental sigmoid belief networks for grammar
learning. Journal of Machine Learning Re-
search, 11(Dec):3541–3570.

James Henderson. 2003. Inducing history rep-
resentations for broad coverage statistical pars-
ing. In Proc. joint meeting of North American
Chapter of the Association for Computational
Linguistics and the Human Language Technol-
ogy Conf., pages 103–110.

Aravind K. Joshi. 1987. An introduction to
tree adjoining grammars. In Alexis Manaster-
Ramer, editor, Mathematics of Language. John
Benjamins, Amsterdam.

D. Koller, D. McAllester, and A. Pfeffer. 1997.
Effective Bayesian inference for stochastic pro-
grams. In Proc. 14th National Conf. on Artifi-
cial Intelligence (AAAI 1997), pages 740–747.

John Lafferty, Andrew McCallum, and Fernando
Pereira. 2001. Conditional random fields:
Probabilistic models for segmenting and label-
ing sequence data. In Proc. 18th International
Conf. on Machine Learning, pages 282–289.
Morgan Kaufmann, San Francisco, CA.

Percy Liang, Slav Petrov, Michael Jordan, and
Dan Klein. 2007. The infinite PCFG using hi-
erarchical dirichlet processes. In Joint Conf. on
Empirical Methods in Natural Language Pro-
cessing and Computational Natural Language
Learning, pages 688–697, Prague, Czech Re-
public.

Rune B. Lyngsø and Christian N.S. Pedersen.
2002. The consensus string problem and the
complexity of comparing hidden markov mod-
els. Journal of Computer and System Sciences,
65:545–569.

Takuya Matsuzaki, Yusuke Miyao, and Jun’ichi
Tsujii. 2005. Probabilistic CFG with latent an-
notations. In Proc. 43rd Annual Meeting on As-
sociation for Computational Linguistics, pages
75–82.

David McAllester, Michael Collins, and Fernando
Pereira. 2004. Case-factor diagrams for struc-
tured probabilistic modeling. In Proc. 20th
Conf. on Uncertainty in Artificial Intelligence,
pages 382–391.

Kevin P. Murphy. 2002. Dynamic Belief Net-
works: Representation, Inference and Learning.
Ph.D. thesis, University of California, Berkeley,
CA.

Gabriele Musillo and Paola Merlo. 2008. Unlex-
icalised hidden variable models of split depen-
dency grammars. In Proc. 46th Annual Meet-
ing of the Association for Computational Lin-
guistics: Human Language Technologies, pages
213–216.

Radford Neal. 1992. Connectionist learning of
belief networks. Artificial Intelligence, 56:71–
113.

Judea Pearl. 1988. Probabilistic Reasoning in In-
telligent Systems: Networks of Plausible Infer-
ence. Morgan Kaufmann.

Slav Petrov, Leon Barrett, Romain Thibaux, and
Dan Klein. 2006. Learning accurate, com-
pact, and interpretable tree annotation. In Proc.
44th annual meeting of the ACL and the 21st
Int. Conf. on Computational Linguistics, pages
433–440.

Hoifung Poon and Pedro Domingos. 2011. Sum-
product networks: A new deep architecture. In
Proc. 12th Conf. on Uncertainty in Artificial In-
telligence, pages 337–346.

Detlef Prescher. 2005. Head-driven PCFGs with
latent-head statistics. In Proc. 9th Int. Work-
shop on Parsing Technologies, pages 115–124.

73



Khashayar Rohanimanesh, Michael Wick, and
Andrew McCallum. 2009. Inference and learn-
ing in large factor graphs with adaptive proposal
distributions and a rank-based objective. Tech-
nical Report UM-CS-2009-008, University of
Massachusetts.

Yves Schabes. 1992. Stochastic Tree-Adjoining
Grammars. In Proc. workshop on Speech and
Natural Language, pages 140–145.

David A. Smith and Jason Eisner. 2008. Depen-
dency parsing by belief propagation. In Proc.
2008 Conference on Empirical Methods in Nat-
ural Language Processing, pages 145–156.

Ivan Titov and James Henderson. 2007. Incre-
mental bayesian networks for structure predic-
tion. In Proc. 24th International Conf. on Ma-
chine Learning, pages 887–894.

David Weir. 1988. Characterizing Mildly
Context-Sensitive Grammar Formalisms. Ph.D.
thesis, University of Pennsylvania, Philadel-
phia, PA.

Daniel H. Younger. 1967. Recognition and pars-
ing of context-free languages in time n3. Infor-
mation and Control, 10(2):189–208.

74


