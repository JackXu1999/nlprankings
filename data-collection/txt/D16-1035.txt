



















































Discourse Parsing with Attention-based Hierarchical Neural Networks


Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 362–371,
Austin, Texas, November 1-5, 2016. c©2016 Association for Computational Linguistics

Discourse Parsing with Attention-based Hierarchical Neural Networks

Qi Li Tianshi Li Baobao Chang
Key Laboratory of Computational Linguistics, Ministry of Education

School of Electronics Engineering and Computer Science, Peking University
No.5 Yiheyuan Road, Haidian District, Beijing, 100871, China

Collaborative Innovation Center for Language Ability, Xuzhou, 221009, China
qi.li@pku.edu.cn lts 417@hotmail.com chbb@pku.edu.cn

Abstract

RST-style document-level discourse parsing
remains a difficult task and efficient deep
learning models on this task have rarely been
presented. In this paper, we propose an
attention-based hierarchical neural network
model for discourse parsing. We also incor-
porate tensor-based transformation function to
model complicated feature interactions. Ex-
perimental results show that our approach ob-
tains comparable performance to the contem-
porary state-of-the-art systems with little man-
ual feature engineering.

1 Introduction

A document is formed by a series of coherent text
units. Document-level discourse parsing is a task to
identify the relations between the text units and to
determine the structure of the whole document the
text units form. Rhetorical Structure Theory (RST)
(Mann and Thompson, 1988) is one of the most in-
fluential discourse theories. According to RST, the
discourse structure of a document can be represented
by a Discourse Tree (DT). Each leaf of a DT denotes
a text unit referred to as an Elementary Discourse
Unit (EDU) and an inner node of a DT represents
a text span which is constituted by several adjacent
EDUs. DTs can be utilized by many NLP tasks in-
cluding automatic document summarization (Louis
et al., 2010; Marcu, 2000), question-answering (Ver-
berne et al., 2007) and sentiment analysis (Somasun-
daran, 2010) etc.

Much work has been devoted to the task of RST-
style discourse parsing and most state-of-the-art ap-

proaches heavily rely on manual feature engineer-
ing (Joty et al., 2013; Feng and Hirst, 2014; Ji
and Eisenstein, 2014). While neural network mod-
els have been increasingly focused on for their abil-
ity to automatically extract efficient features which
reduces the burden of feature engineering, there is
little neural network based work for RST-style dis-
course parsing except the work of Li et al. (2014a).
Li et al. (2014a) propose a recursive neural network
model to compute the representation for each text
span based on the representations of its subtrees.
However, vanilla recursive neural networks suffer
from gradient vanishing for long sequences and the
normal transformation function they use is weak at
modeling complicated interactions which has been
stated by Socher et al. (2013). As many docu-
ments contain more than a hundred EDUs which
form quite a long sequence, those weaknesses may
lead to inferior results on this task.

In this paper, we propose to use a hierarchical
bidirectional Long Short-Term Memory (bi-LSTM)
network to learn representations of text spans. Com-
paring with vanilla recursive/recurrent neural net-
works, LSTM-based networks can store information
for a long period of time and don’t suffer from gra-
dient vanishing problem. We apply a hierarchical
bi-LSTM network because the way words form an
EDU and EDUs form a text span is different and
thus they should be modeled separately and hierar-
chically. On top of that, we apply attention mecha-
nism to attend over all EDUs to pick up prominent
semantic information of a text span. Besides, we use
tensor-based transformation function to model com-
plicated feature interactions and thus it can produce

362



combinatorial features.
We summarize contributions of our work as fol-

lows:

• We propose to use a hierarchical bidirectional
LSTM network to learn the compositional se-
mantic representations of text spans, which nat-
urally matches and models the intrinsic hierar-
chical structure of text spans.

• We extend our hierarchical bi-LSTM network
with attention mechanism to allow the network
to focus on the parts of input containing promi-
nent semantic information for the composi-
tional representations of text spans and thus al-
leviate the problem caused by the limited mem-
ory of LSTM for long text spans.

• We adopt a tensor-based transformation func-
tion to allow explicit feature interactions and
apply tensor factorization to reduce the param-
eters and computations.

• We use two level caches to intensively acceler-
ate our probabilistic CKY-like parsing process.

The rest of this paper is organized as follows: Sec-
tion 2 gives the details of our parsing model. Section
3 describes our parsing algorithm. Section 4 gives
our training criterion. Section 5 reports the experi-
mental results of our approach. Section 6 introduces
the related work. Conclusions are given in section 7.

2 Parsing Model

Given two successive text spans, our parsing model
evaluates the probability to combine them into a
larger span, identifies which one is the nucleus and
determines what is the relation between them. As
with the work of Ji and Eisenstein (2014), we set
three classifiers which share the same features as in-
put to deal with those problems. The whole pars-
ing model is shown in Figure 1. Three classi-
fiers are on the top. The semantic representations
of the two given text spans which come from the
output of attention-based hierarchical bi-LSTM net-
work with tensor-based transformation function is
the main part of input to the classifiers. Additionally,
following the previous practice of Li et al. (2014a),
a small set of handcrafted features is introduced to
enhance the model.

Figure 1: Schematic structure of our parsing model.

2.1 Hierarchical Bi-LSTM Network for Text
Span Representations

Long Short-Term Memory (LSTM) networks have
been successfully applied to a wide range of NLP
tasks for the ability to handle long-term dependen-
cies and to mitigate the curse of gradient vanishing
(Hochreiter and Schmidhuber, 1997; Bahdanau et
al., 2014; Rocktäschel et al., 2015; Hermann et al.,
2015). A basic LSTM can be described as follows.
A sequence {x1, x2, ..., xn} is given as input. At
each time-step, the LSTM computation unit takes in
one token xt as input and it keeps some information
in a cell state Ct and gives an output ht. They are
calculated in this way:

it = σ(Wi[ht−1;xt] + bi) (1)

ft = σ(Wf [ht−1;xt] + bf ) (2)

C̃t = tanh(WC [ht−1;xt] + bC) (3)

Ct = ft � Ct−1 + it � C̃t (4)
ot = σ(Wo[ht−1;xt] + bo) (5)

ht = ot � tanh(Ct) (6)

where Wi, bi,Wf , bf ,Wc, bC ,Wo, bo are LSTM pa-
rameters,� denotes element-wise product and σ de-
notes sigmoid function. The output at the last token,
i.e., hn is taken as the representation of the whole
sequence.

363



Figure 2: Bi-LSTM for computing the compositional semantic
representation of an EDU.

Since an EDU is a sequence of words, we de-
rive the representation of an EDU from the sequence
constituted by concatenation of word embeddings
and the POS tag embeddings of the words as Figure
2 shows. Previous work on discourse parsing tends
to extract some features from the beginning and end
of text units partly because discourse clues such as
discourse markers(e.g., because, though) are often
situated at the beginning or end of text units(Feng
and Hirst, 2014; Ji and Eisenstein, 2014; Li et al.,
2014a; Li et al., 2014b; Heilman and Sagae, 2015).
Considering the last few tokens of a sequence nor-
mally have more influence on the representation of
the whole sequence learnt with LSTM because they
get through less times of forget gate from the LSTM
computation unit, to effectively capture the informa-
tion from both beginning and end of an EDU, we
use bidirectional LSTM to learn the representation
of an EDU. In other words, one LSTM takes the
word sequence in forward order as input, the other
takes the word sequence in reversed order as input.
The representation of a sequence is the concatena-
tion of the two vector representations calculated by
the two LSTMs.

Since a text span is a sequence of EDUs, its
meaning can be computed from the meanings of
the EDUs. So we use another bi-LSTM to derive
the compositional semantic representation of a text
span from the EDUs it contains. The two bi-LSTM
networks form a hierarchical structure as Figure 1
shows.

2.2 Attention
The representation of a sequence computed by bi-
LSTMs is always a vector with fixed dimension de-
spite the length of the sequence. Thus when dealing
with a text span with hundreds of EDUs, bi-LSTM
may not be enough to capture the whole semantic in-
formation with its limited output vector dimension.
Attention mechanism can attend over the output at
every EDU with global context and pick up promi-
nent semantic information and drop the subordinate
information for the compositional representation of
the span, so we employ attention mechanism to al-
leviate the problem caused by the limited memory
of LSTM networks. The attention mechanism is in-
spired by the work of Rocktäschel et al. (2015). Our
attention-based bi-LSTM network is shown in Fig-
ure 3.

We combine the last outputs of the span level bi-
LSTM to be hs = [

−→
h en ,

←−
h e1 ]. We also combine

the outputs of the two LSTM at every EDU of the
span: ht = [

−→
h t,
←−
h t] and thus get a matrix H =

[h1;h2; ...;hn]
T . Taking H ∈ Rd×n and hs ∈ Rd as

inputs, we get a vector α ∈ Rn standing for weights
of EDUs to the text span and use it to get a weighted
representation of the span r ∈ Rd:

M = tanh(WyH +Wlhs ⊗ en) (7)
α = softmax(wTαM) (8)

r = Hα (9)

where⊗ denotes Cartesian product , M ∈ Rk×n, en
is a n dimensional vector of all 1s and we use the
Cartesian product Wlhs ⊗ en to repeat the result of
Wlhs n times in column to form a matrix and Wy ∈
Rk×d,Wl,∈ Rk×d, wα ∈ Rk are parameters.

We synthesize the information of r and hs to get
the final representation of the span:

wh = σ(Whrr +Whhhs) (10)

h = wh � hs + (1− wh)� r (11)

where Whr,Whh ∈ Rd×d are parameters, wh ∈ Rd
is a computed vector representing the element-wise
weight of hs and the element-wise weighted sum-
mation h ∈ Rd is the final representation of the text
span computed by the attention-based bidirectional
LSTM network.

364



Figure 3: Attention-based bi-LSTM for computing the compo-
sitional semantic representation of a text span.

2.3 Classifiers
We concatenate the representations of the two given
spans: h = [hs1, hs2] and feed h into a full connec-
tion hidden layer to obtain a higher level representa-
tion v which is the input to the three classifiers:

v = Relu(Wh[hs1, hs2] + bh) (12)

For each classifier, we firstly transform v ∈ Rl
into a hidden layer:

vsp = Relu(Whsv + bhs) (13)

vnu = Relu(Whnv + bhn) (14)

vrel = Relu(Whrv + bhr) (15)

where Whs,Whn,Whr ∈ Rh×l are transformation
matrices and bhs, bhn, bhr ∈ Rh are bias vectors.

Then we feed these vectors into the respective
output layer:

ysp = σ(wsvsp + bs) (16)

ynu = softmax(Wnvnu + bn) (17)

yrel = softmax(Wrvrel + br) (18)

where ws ∈ Rh, bs ∈ R,Wn ∈ R3×h,Wn ∈
R3×h, bn ∈ R3,Wr ∈ Rnr×h, bn ∈ Rnr are pa-
rameters and nr is the number of different discourse
relations.

The first classifier is a binary classifier which out-
puts the probability the two spans should be com-
bined. The second classifier is a multiclass classifier

which identifies the nucleus to be span 1, span 2 or
both. The third classifier is also a multiclass classi-
fier which determines the relation between the two
spans.

2.4 Tensor-based Transformation

Tensor-based transformation function has been suc-
cessfully utilized in many tasks to allow complicated
interaction between features (Sutskever et al., 2009;
Socher et al., 2013; Pei et al., 2014). Based on
the intuition that allowing complicated interaction
between the features of the two spans may help to
identify how they are related, we adopt tensor-based
transformation function to strengthen our model.

A tensor-based transformation function on x ∈
Rd1 is as follows:

y =Wx+ xTT [1:d2]x+ b (19)

yi =
∑

j

Wijxj +
∑

j,k

T
[i]
j,kxjxk + bi (20)

where y ∈ Rd2 is the output vector, yi ∈ R is the
ith element of y, W ∈ Rd2×d1 is the transformation
matrix, T [1:d2] ∈ Rd1×d1×d2 is a 3rd-order transfor-
mation tensor. A normal transformation function in
neural network models only has the first term Wx
with the bias term. It means for normal transfor-
mation function each unit of the output vector is
the weighted summation of the input vector and this
only allows additive interaction between the units of
the input vector. With the tensor multiplication term,
each unit of the output vector is augmented with the
weighted summation of the multiplication of the in-
put vector units and thus we incorporate multiplica-
tive interaction between the units of the input vector.

Inevitably, the incorporation of tensor leads to
side effects which include the increase in parameter
number and computational complexity. To remedy
this, we adopt tensor factorization in the same way
as Pei et al. (2014): we use two low rank matrices to
approximate each tensor slice T [i] ∈ Rd1×d1 :

T [i] ⇒ P [i]Q[i] (21)

where P [i] ∈ Rd1×r, Q[i] ∈ Rr×d1 and r � d1.
In this way, we drastically reduce parameter number
and computational complexity.

365



We apply the factorized tensor-based transforma-
tion function to the combined text span representa-
tion h = [hs1, hs2] to make the features of the two
spans explicitly interact with each other:

v = Relu(Wh[hs1, hs2] +

[hs1, hs2]
TP

[1:d]
h Q

[1:d]
h [hs1, hs2] + bh) (22)

Comparing with Eq. 12, the transformation function
is added with a tensor term.

2.5 Handcrafted Features
Most previously proposed state-of-the-art systems
heavily rely on handcrafted features (Hernault et al.,
2010; Feng and Hirst, 2014; Joty et al., 2013; Ji and
Eisenstein, 2014; Heilman and Sagae, 2015). Li et
al. (2014a) show that some basic features are still
necessary to get a satisfactory result for their recur-
sive deep model. Following their practice, we adopt
minimal basic features which are utilized by most
systems to further strengthen our model. We list
these features in Table 1. We apply the factorized
tensor-based transformation function to Word/POS
features to allow more complicated interaction be-
tween them.

3 Parsing Algorithm

In this section, we describe our parsing algorithm
which utilizes the parsing model to produce the
global optimal DT for a segmented document.

3.1 Probabilistic CKY-like Algorithm
We adopt a probabilistic CKY-like bottom-up algo-
rithm which is also adopted in (Joty et al., 2013;
Li et al., 2014a) to produce a DT for a document.
This parsing algorithm is a dynamic programming
algorithm and produces the global optimal DT with
our parsing model. Given a text span which is
constituted by [ei, ei+1, ..., ej ] and the possible sub-
trees of [ei, ei+1, ..., ek] and [ek+1, ek+2, ..., ej ] for
all k ∈ {i, i+1, ..., j−1}with their probabilities, we
choose k and combine the corresponding subtrees to
form a combined DT with the following recurrence
formula:

k = argmax
k
{Psp(i, k, j)Pi,kPk+1,j} (23)

where Pi,k and Pk+1,j are the probabilities of
the most probable subtrees of [ei, ei+1, ..., ek] and

[ek+1, ek+2, ..., ej ] respectively, Psp(i, k, j) is the
probability which is predicted by our parsing model
to combine those two subtrees to form a DT.

The probability of the most probable DT of
[ei, ei+1, ..., ej ] is:

Pi,j = max
k
{Psp(i, k, j)Pi,kPk+1,j} (24)

3.2 Parsing Acceleration
Computational complexity of the original proba-
bilistic CKY-like algorithm is O(n3) where n is the
number of EDUs of the document. But in this work,
given each pair of text spans, we compute the rep-
resentations of them with hierarchical bi-LSTM net-
work at the expense of an additional O(n) computa-
tions. So the computational complexity of our parser
becomesO(n4) and it is unacceptable for long docu-
ments. However, most computations are duplicated,
so we use two level caches to drastically accelerate
parsing.

Firstly, we cache the outputs of the EDU level
bi-LSTM which are the semantic representations of
EDUs. As for the forward span level LSTM, after
we get the semantic representation of a span, we
cache it too and use it to compute the representation
of an extended span. For example, after we get the
representation of span constituted by [e1, e2, e3], we
take it with semantic representation of e4 to com-
pute the representation of the span constituted by
[e1, e2, e3, e4] in one LSTM computation step. For
the backward span level LSTM, we do it the same
way just in reversed order. Thus we decrease the
computational complexity of computing the seman-
tic representations for all possible span pairs which
is the most time-consuming part of the original pars-
ing process from O(n4) to O(n2).

Secondly, it can be seen that before we apply
Relu to the tensor-based transformation function,
many calculations from the two spans which include
a large part of tensor multiplication are independent.
The multiplication between the elements of the rep-
resentations of the two spans caused by the tensors
and the element-wise non-linear activation function
Relu terminate the independence between them. So
we can further cache the independent calculation re-
sults before Relu operation for each span. Thus we
decrease the computational complexity of a large
part of tensor-based transformation from O(n3) to

366



Word/POS Features
One-hot representation of the first two words and of the last word of each span.
One-hot representation of POS tags of the first two words and of the last word of each span.
Shallow Features
Number of EDUs of each span.
Number of words of each span.
Predicted relations of the two subtrees’ roots.
Whether each span is included in one sentence.
Whether both spans are included in one sentence.

Table 1: Handcrafted features used in our parsing model.

O(n2) which is the second time-consuming part of
the original parsing process.

The remaining O(n3) computations include a lit-
tle part of tensor-based transformation computa-
tions,Relu operation and the computations from the
three classifiers. These computations take up only a
little part of the original parsing model computations
and thus we greatly accelerate our parsing process.

4 Max-Margin Training

We use Max-Margin criterion for our model train-
ing. We try to learn a function that maps: X → Y ,
where X is the set of documents and Y is the set of
possible DTs. We define the loss function for pre-
dicting a DT ŷi given the correct DT yi as:

4(yi, ŷi) =
∑

r∈ŷi
κ1{r 6∈ yi} (25)

where r is a span specified with nucleus and relation
in the predicted DT, κ is a hyperparameter referred
to as discount parameter and 1 is indicator function.
We expect the probability of the correct DT to be a
larger up to a margin to other possible DTs:

Prob(x, yi) ≥ Prob(xi, ŷi) +4(yi, ŷi) (26)

The objective function for m training examples is
as follows:

J(θ) =
1

m

m∑

i=1

li(θ), where (27)

li(θ) = max
ŷi

(Prob(xi, ŷi) +4(yi, ŷi))

−Prob(xi, yi) (28)

where θ denotes all the parameters including our
neural network parameters and all embeddings.

The probabilities of the correct DTs increase and
the probabilities of the most probable incorrect DTs
decrease during training. We adopt Adadelta (Zeiler,
2012) with mini-batch to minimize the objective
function and set the initial learning rate to be 0.012.

5 Experiments

We evaluate our model on RST Discourse Treebank1

(RST-DT) (Carlson et al., 2003). It is partitioned
into a set of 347 documents for training and a set
of 38 documents for test. Non-binary relations are
converted into a cascade of right-branching binary
relations. The standard metrics of RST-style dis-
course parsing evaluation include blank tree struc-
ture referred to as span (S), tree structure with nu-
clearity (N) indication and tree structure with rhetor-
ical relation (R) indication. Following other RST-
style discourse parsing systems, we evaluate the re-
lation metric in 18 coarse-grained relation classes.
Since our work focus does not include EDU segmen-
tation, we evaluate our system with gold-standard
EDU segmentation and we apply the same setting
on this to other discourse parsing systems for fair
comparison.

5.1 Experimental Setup
The dimension of word embeddings is set to be
50 and the dimension of POS embeddings is set to
be 10. We pre-trained the word embeddings with
GloVe (Pennington et al., 2014) on English Giga-
word2 and we fine-tune them during training. Con-
sidering some words are pretrained by GloVe but

1https://catalog.ldc.upenn.edu/LDC2002T07
2https://catalog.ldc.upenn.edu/LDC2011T07

367



don’t appear in the RST-DT training set, we want to
use their embeddings if they appear in test set. Fol-
lowing Kiros et al. (2015), we expand our vocabu-
lary with those words using a matrix W ∈ R50×50
that maps word embeddings from the pre-trained
word embedding space to the fine-tuned word em-
bedding space. The objective function for training
the matrix W is as follows:

min
W,b
||Vtuned − VpretrainedW − b||22 (29)

where Vtuned, Vpretrained ∈ R|V |×50 contain fine-
tuned and pre-trained embeddings of words appear-
ing in training set respectively, |V | is the size of
RST-DT training set vocabulary and b is the bias
term also to be trained.

We lemmatize all the words appeared and rep-
resent all numbers with a special token. We use
Stanford CoreNLP toolkit (Manning et al., 2014) to
preprocess the text including lemmatization, POS
tagging etc. We use Theano library (Bergstra et
al., 2010) to implement our parsing model. We
randomly initialize all parameters within (-0.012,
0.012) except word embeddings. We adopt dropout
strategy (Hinton et al., 2012) to avoid overfitting and
we set the dropout rate to be 0.3.

5.2 Results and Analysis
To show the effectiveness of the components in-
corporated into our model, we firstly test the per-
formance of the basic hierarchical bidirectional
LSTM network without attention mechanism (ATT),
tensor-based transformation (TE) and handcrafted
features (HF). Then we add them successively. The
results are shown in Table 2.

The performance is improved by adding each
component to our basic model and that shows the ef-
fectiveness of attention mechanism and tensor-based
transformation function. Even without handcrafted
features, the performance is still competitive. It
indicates that the semantic representations of text
spans produced by our attention-based hierarchical
bi-LSTM network are effective and the handcrafted
features are complementary to semantic representa-
tions produced by the network.

We also experiment without mapping the OOV
word embeddings and use the same embedding for
all OOV words. The result is shown in Table

System Setting S N R
Basic 82.7 69.7 55.6

Basic+ATT 83.6* 70.2* 56.0*
Basic+ATT+TE 84.2* 70.4 56.3*

Basic+ATT+TE+HF 85.8* 71.1* 58.9*
Table 2: Performance comparison for different settings of
our system on RST-DT. ’Basic’ denotes the basic hierarchical

bidirectional LSTM network; ’+ATT’ denotes adding attention

mechanism; ’+TE’ denotes adopting tensor-based transforma-

tion; ’+HF’ denotes adding handcrafted features. * indicates

statistical significance in t-test compared to the result in the line

above (p < 0.05).

System Setting S N R
Without OOV mapping 85.1 70.7 58.2

Full version 85.8* 71.1* 58.9*
Table 3: Performance comparison for whether to map OOV
embeddings.

3. Without mapping the OOV word embeddings
the performance decreases slightly, which demon-
strates that the relation between pre-trained embed-
ding space and the fine-tuned embedding space can
be learnt and it is beneficial to train a matrix to trans-
form OOV word embeddings from the pre-trained
embedding space to the fine-tuned embedding space.

We compare our system with other state-of-the-art
systems including (Joty et al., 2013; Ji and Eisen-
stein, 2014; Feng and Hirst, 2014; Li et al., 2014a;
Li et al., 2014b; Heilman and Sagae, 2015). Systems
proposed by Joty et al. (2013), Heilman (2015) and
Feng and Hirst (2014) are all based on variants of
CRFs. Ji and Eisenstein (2014) use a projection ma-
trix acting on one-hot representations of features to
learn representations of text spans and build Support
Vector Machine (SVM) classifier on them. Li et al.
(2014b) adopt dependency parsing methods to deal
with this task. These systems are all based on hand-
crafted features. Li et al. (2014a) adopt a recursive
deep model and use some basic handcrafted features
to improve their performances which has been stated
before.

Table 4 shows the performance for our system
and those systems. Our system achieves the best
result in span and relatively lower performance in
nucleus and relation identification comparing with
the corresponding best results but still better than

368



System S N R
Joty et al. (2013) 82.7 68.4 55.7

Ji and Eisenstein (2014) 82.1 71.1 61.6
Feng and Hirst (2014) 85.7 71.0 58.2

Li et al. (2014a) 84.0 70.8 58.6
Li et al. (2014b) 83.4 73.8 57.8

Heilman and Sagae (2015) 83.5 68.1 55.1
Ours 85.8 71.1 58.9

Human 88.7 77.7 65.8
Table 4: Performance comparison with other state-of-the-art
systems on RST-DT.

System S N R
Li et al. (2014a) (no feature) 82.4 69.2 56.8

Ours (no feature) 84.2 70.4 56.3
Table 5: Performance comparison with the deep learning model
proposed in Li et al. (2014a) without handcrafted features.

most systems. No system achieves the best result
on all three metrics. To further show the effective-
ness of the deep learning model itself without hand-
crafted features, we compare the performance be-
tween our model and the model proposed by Li et al.
(2014a) without handcrafted features and the results
are shown in Table 5. It shows our overall perfor-
mance outperforms the model proposed by Li et al.
(2014a) which illustrates our model is effective.

Table 6 shows an example of the weights (W) of
EDUs (see Eq. 8) derived from our attention model.
For span1 the main semantic meaning is expressed
in EDU32 under the condition described in EDU31.
Besides, it is EDU32 that explicitly manifests the
contrast relation between the two spans. As can
be seen, our attention model assigns less weight to

Span1 (EDU30∼EDU32) W
That means that 0.13
if the offense deals with one part of the
business,

0.38

you don’t attempt to seize the whole busi-
ness;

0.49

Span2 (EDU33) W
you attempt to seize assets related to the
crime,

1.0

Table 6: An example of the weights derived from our attention
model. The relation between span1 and span2 is Contrast.

EDU30 and focuses more on EDU32 which is rea-
sonable according to our analysis above.

6 Related Work

Two most prevalent discourse parsing treebanks
are RST Discourse Treebank (RST-DT) (Carlson et
al., 2003) and Penn Discourse TreeBank (PDTB)
(Prasad et al., 2008). We evaluate our system on
RST-DT which is annotated in the framework of
Rhetorical Structure Theory (Mann and Thompson,
1988). It consists of 385 Wall Street Journal arti-
cles and is partitioned into a set of 347 documents
for training and a set of 38 documents for test. 110
fine-grained and 18 coarse-grained relations are de-
fined on RST-DT. Parsing algorithms published on
RST-DT can mainly be categorized as shift-reduce
parsers and probabilistic CKY-like parsers. Shift-
reduce parsers are widely used for their efficiency
and effectiveness and probabilistic CKY-like parsers
lead to the global optimal result for the parsing
models. State-of-the-art systems belonging to shift-
reduce parsers include (Heilman and Sagae, 2015;
Ji and Eisenstein, 2014). Those belonging to prob-
abilistic CKY-like parsers include (Joty et al., 2013;
Li et al., 2014a). Besides, Feng and Hirst (2014)
adopt a greedy bottom-up approach as their pars-
ing algorithm. Lexical, syntactic, structural and se-
mantic features are extracted in these systems. SVM
and variants of Conditional Random Fields (CRFs)
are mostly used in these models. Li et al. (2014b)
distinctively propose to use dependency structure to
represent the relations between EDUs. Recursive
deep model proposed by Li et al. (2014a) has been
the only proposed deep learning model on RST-DT.

Incorporating attention mechanism into RNN
(e.g., LSTM, GRU) has been shown to learn bet-
ter representation by attending over the output vec-
tors and picking up important information from rel-
evant positions of a sequence and this approach has
been utilized in many tasks including neural ma-
chine translation (Kalchbrenner and Blunsom, 2013;
Bahdanau et al., 2014; Hermann et al., 2015), text
entailment recognition (Rocktäschel et al., 2015)
etc. Some work also uses tensor-based transforma-
tion function to make stronger interaction between
features and learn combinatorial features and they
get performance boost in their tasks (Sutskever et

369



al., 2009; Socher et al., 2013; Pei et al., 2014).

7 Conclusion

In this paper, we propose an attention-based hier-
archical neural network for discourse parsing. Our
attention-based hierarchical bi-LSTM network pro-
duces effective compositional semantic representa-
tions of text spans. We adopt tensor-based trans-
formation function to allow complicated interaction
between features. Our two level caches accelerate
parsing process significantly and thus make it prac-
tical. Our proposed system achieves comparable re-
sults to state-of-the-art systems. We will try extend-
ing attention mechanism to obtain the representation
of a text span by referring to another text span at
minimal additional cost.

Acknowledgments

We thank the reviewers for their instructive feed-
back. We also thank Jiwei Li for his helpful
discussions. This work is supported by National
Key Basic Research Program of China under Grant
No.2014CB340504 and National Natural Science
Foundation of China under Grant No.61273318.
The Corresponding author of this paper is Baobao
Chang.

References
Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben-

gio. 2014. Neural machine translation by jointly
learning to align and translate. CoRR, abs/1409.0473.

James Bergstra, Olivier Breuleux, Frédéric Bastien, Pas-
cal Lamblin, Razvan Pascanu, Guillaume Desjardins,
Joseph Turian, David Warde-Farley, and Yoshua Ben-
gio. 2010. Theano: a CPU and GPU math expression
compiler. In Proceedings of the Python for Scientific
Computing Conference (SciPy), June. Oral Presenta-
tion.

Lynn Carlson, Daniel Marcu, and Mary Ellen Okurowski.
2003. Building a discourse-tagged corpus in the
framework of rhetorical structure theory. In Current
and new directions in discourse and dialogue, pages
85–112. Springer.

Vanessa Wei Feng and Graeme Hirst. 2014. A linear-
time bottom-up discourse parser with constraints and
post-editing. In ACL (1), pages 511–521.

Michael Heilman and Kenji Sagae. 2015. Fast
rhetorical structure theory discourse parsing. CoRR,
abs/1505.02425.

Karl Moritz Hermann, Tomá s Kociský, Edward Grefen-
stette, Lasse Espeholt, Will Kay, Mustafa Suleyman,
and Phil Blunsom. 2015. Teaching machines to read
and comprehend. CoRR, abs/1506.03340.

Hugo Hernault, Helmut Prendinger, David A DuVerle,
and Mitsuru Ishizuka. 2010. Hilda: a discourse parser
using support vector machine classification. Dialogue
and Discourse, 1(3):1–33.

Geoffrey E. Hinton, Nitish Srivastava, Alex Krizhevsky,
Ilya Sutskever, and Ruslan Salakhutdinov. 2012. Im-
proving neural networks by preventing co-adaptation
of feature detectors. CoRR, abs/1207.0580.

Sepp Hochreiter and Jürgen Schmidhuber. 1997. Long
short-term memory. Neural computation, 9(8):1735–
1780.

Yangfeng Ji and Jacob Eisenstein. 2014. Representation
learning for text-level discourse parsing. In ACL (1),
pages 13–24.

Shafiq R. Joty, Giuseppe Carenini, Raymond T. Ng, and
Yashar Mehdad. 2013. Combining intra- and multi-
sentential rhetorical parsing for document-level dis-
course analysis. In ACL.

Daniel Jurafsky and James H Martin. 2008. Speech and
language processing, chapter 14. In Prentice Hall.

Nal Kalchbrenner and Phil Blunsom. 2013. Recurrent
continuous translation models. In EMNLP.

Ryan Kiros, Yukun Zhu, Ruslan Salakhutdinov,
Richard S. Zemel, Antonio Torralba, Raquel Urtasun,
and Sanja Fidler. 2015. Skip-thought vectors. CoRR,
abs/1506.06726.

Jiwei Li, Rumeng Li, and Eduard H Hovy. 2014a. Re-
cursive deep models for discourse parsing. In EMNLP,
pages 2061–2069.

Sujian Li, Liang Wang, Ziqiang Cao, and Wenjie Li.
2014b. Text-level discourse dependency parsing. In
ACL (1), pages 25–35.

Annie Louis, Aravind Joshi, and Ani Nenkova. 2010.
Discourse indicators for content selection in summa-
rization. In Proceedings of the 11th Annual Meeting
of the Special Interest Group on Discourse and Dia-
logue, pages 147–156. Association for Computational
Linguistics.

William C Mann and Sandra A Thompson. 1988.
Rhetorical structure theory: Toward a functional the-
ory of text organization. Text-Interdisciplinary Jour-
nal for the Study of Discourse, 8(3):243–281.

Christopher D. Manning, Mihai Surdeanu, John Bauer,
Jenny Rose Finkel, Steven Bethard, and David Mc-
Closky. 2014. The stanford corenlp natural language
processing toolkit. In ACL.

Daniel Marcu. 2000. The theory and practice of dis-
course parsing and summarization. MIT press.

370



Wenzhe Pei, Tao Ge, and Baobao Chang. 2014. Max-
margin tensor neural network for chinese word seg-
mentation. In ACL (1), pages 293–303.

Jeffrey Pennington, Richard Socher, and Christopher D.
Manning. 2014. Glove: Global vectors for word rep-
resentation. In EMNLP.

Rashmi Prasad, Nikhil Dinesh, Alan Lee, Eleni Milt-
sakaki, Livio Robaldo, Aravind K Joshi, and Bonnie L
Webber. 2008. The penn discourse treebank 2.0. In
LREC. Citeseer.

Tim Rocktäschel, Edward Grefenstette, Karl Moritz Her-
mann, Tomá s Kociský, and Phil Blunsom. 2015. Rea-
soning about entailment with neural attention. CoRR,
abs/1509.06664.

Richard Socher, Alex Perelygin, Jean Y Wu, Jason
Chuang, Christopher D Manning, Andrew Y Ng, and
Christopher Potts. 2013. Recursive deep models for
semantic compositionality over a sentiment treebank.
In Proceedings of the conference on empirical meth-
ods in natural language processing (EMNLP), volume
1631, page 1642. Citeseer.

Swapna Somasundaran. 2010. Discourse-level relations
for Opinion Analysis. Ph.D. thesis, University of Pitts-
burgh.

Ilya Sutskever, Ruslan Salakhutdinov, and Joshua B.
Tenenbaum. 2009. Modelling relational data using
bayesian clustered tensor factorization. In NIPS.

Suzan Verberne, Lou Boves, Nelleke Oostdijk, and Peter-
Arno Coppen. 2007. Evaluating discourse-based an-
swer extraction for why-question answering. In Pro-
ceedings of the 30th annual international ACM SIGIR
conference on Research and development in informa-
tion retrieval, pages 735–736. ACM.

Matthew D. Zeiler. 2012. Adadelta: An adaptive learn-
ing rate method. CoRR, abs/1212.5701.

371


