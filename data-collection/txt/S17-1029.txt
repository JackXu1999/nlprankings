



















































Learning to Solve Geometry Problems from Natural Language Demonstrations in Textbooks


Proceedings of the 6th Joint Conference on Lexical and Computational Semantics (*SEM 2017), pages 251–261,
Vancouver, Canada, August 3-4, 2017. c©2017 Association for Computational Linguistics

Learning to Solve Geometry Problems from Natural Language
Demonstrations in Textbooks

Mrinmaya Sachan Eric P. Xing
School of Computer Science
Carnegie Mellon University

{mrinmays, epxing}@cs.cmu.edu

Abstract

Humans as well as animals are good at
imitation. Inspired by this, the learning
by demonstration view of machine learn-
ing learns to perform a task from detailed
example demonstrations. In this paper,
we introduce the task of question answer-
ing using natural language demonstra-
tions where the question answering system
is provided with detailed demonstrative
solutions to questions in natural language.
As a case study, we explore the task of
learning to solve geometry problems using
demonstrative solutions available in text-
books. We collect a new dataset of demon-
strative geometry solutions from textbooks
and explore approaches that learn to in-
terpret these demonstrations as well as to
use these interpretations to solve geometry
problems. Our approaches show improve-
ments over the best previously published
system for solving geometry problems.

1 Introduction

Cognitive science emphasizes the importance of
imitation or learning by example (Meltzoff and
Moore, 1977; Meltzoff, 1995) in human learn-
ing. When a teacher signals a pedagogical inten-
tion, children tend to imitate the teacher’s actions
(Buchsbaum et al., 2011; Butler and Markman,
2014). Inspired by this phenomenon, the learn-
ing by demonstration view of machine learning
(Schaal, 1997; Argall et al., 2009; Goldwasser and
Roth, 2014) assumes training data in the form of
example demonstrations. A task is demonstrated
by a teacher and the learner generalizes from these
demonstrations in order to execute the task.

In this paper, we introduce the novel task
of question answering using natural language

Text Description:

measure(   MAO, 30o)
isCircle(O)

radius(O, 4 cm)
?x

Diagram:

liesOn( A, circle O), liesOn( B, circle O), 
liesOn( C, circle O), liesOn( D, circle O)

isLine(AB), isLine(BC), isLine(CA), isLine(BD), isLine(DA)
isTriangle(ABC), isTriangle(ABD), isTriangle(AOM)

measure(   ADB, x), measure(   MAO, 30o)
measure(   AMO, 90o)

…

Figure 1: Above: An example SAT style geometry problem
with the text description, corresponding diagram and (option-
ally) answer candidates. Below: A logical expression that
represents the meaning of the text description and the dia-
gram in the problem. GEOS derives a weighted logical ex-
pression where each predicates also carry a weighted score
but we do not show them here for clarity.

demonstrations. Research in question answer-
ing has traditionally focused on learning from
question-answer pairs (Burger et al., 2001). How-
ever, it is well-established in the educational psy-
chology literature (Allington and Cunningham,
2010; Felder et al., 2000) that children tend to
learn better and faster from concrete illustrations
and demonstrations. In this paper, we raise the
question – “Can we leverage demonstrative solu-
tions for questions as provided by a teacher to im-
prove our question answering systems?”

As a case study, we propose the task of learn-
ing to solve SAT geometry problems (such as the
one in Figure 1) using demonstrative solutions
to these problems (such as the one in Figure 2).
Such demonstrations are common in textbooks as
they help students learn how to solve geometry
problems effectively. We build a new dataset of
demonstrative solutions of geometry problems and
show that it can be used to improve GEOS (Seo
et al., 2015), the state-of-the-art in solving geom-

251



1.	Sum	of	interior	angles	of	a	triangle	is	
1800	

=>	 OAM	+	 AMO	+	 MOA	=	1800	

=>	 MOA	=	600	
	

2.	Similar	triangle	theorem	
=>	 MOB	~	 MOA	

=>	 MOB	=	 MOA	=	600	
	

3.	 AOB	=	 MOB	+	 MOA	
=> AOB	=	1200	
	

4.	Angle	subtended	by	a	chord	at	the	
center	is	twice	the	angle	subtended	at	
the	circumference	
=>	 ADB	=	0.5	x	 AOB	
	 						=	600	

Figure 2: An example demonstration on how to solve the
problem in Figure 1: (1) Use the theorem that the sum of in-
terior angles of a triangle is 180◦and additionally the fact that
∠AMO is 90◦to conclude that ∠MOA is 60◦. (2) Conclude
that4MOA∼4MOB (using a similar triangle theorem) and
then, conclude that ∠MOB = ∠MOA = 60◦(using the theo-
rem that corresponding angles of similar triangles are equal).
(3) Use angle sum rule to conclude that ∠AOB = ∠MOB +
∠MOA = 120◦. (4) Use the theorem that the angle subtended
by an arc of a circle at the centre is double the angle sub-
tended by it at any point on the circle to conclude that ∠ADB
= 0.5×∠AOB = 60◦.

etry problems.
We also present a technique inspired from re-

cent work in situated question answering (Krish-
namurthy et al., 2016) that jointly learns how to
interpret the demonstration and use this interpre-
tation to solve geometry problems. We model the
interpretation task (the task of recognizing various
states in the demonstration) as a semantic parsing
task. We model state transitions in the demonstra-
tion via a deduction model that treats each appli-
cation of a theorem of geometry as a state tran-
sition. We describe techniques to learn the two
models separately as well as jointly from various
kinds of supervision: (a) when we only have a set
of question-answer pairs as supervision, (b) when
we have a set of questions and demonstrative so-
lutions for them, and (c) when we have a set of
question-answer pairs and a set of demonstrations.

An important benefit of our approach is ‘in-
terpretability’. While GEOS is uninterpretable,
our approach utilizes known theorems of geom-
etry to deductively solve geometry problems. Our
approach also generates demonstrative solutions
(like Figure 2) as a by-product which can be pro-

vided to students on educational platforms such as
MOOCs to assist in their learning.

We present an experimental evaluation of our
approach on the two datasets previously intro-
duced in Seo et al. (2015) and a new dataset col-
lected by us from a number of math textbooks in
India. Our experiments show that our approach
of leveraging demonstrations improves GEOS. We
also performed user studies with a number of
school students studying geometry, who found that
our approach is more interpretable as well as more
useful in comparison to GEOS.

2 Background: GEOS

GEOS solves geometry problems via a multi-stage
approach. It first learns to parse the problem text
and the diagram to a formal problem description
compatible with both of them. The problem de-
scription is a first-order logic expression (see Fig-
ure 1) that includes known numbers or geometrical
entities (e.g. 4 cm) as constants, unknown num-
bers or geometrical entities (e.g. O) as variables,
geometric or arithmetic relations (e.g. isLine, is-
Triangle) as predicates and properties of geomet-
rical entities (e.g. measure, liesOn) as functions.
The parser first learns a set of relations that poten-
tially correspond to the problem text (or diagram)
along with confidence scores. Then, a subset of
relations that maximize the joint text and diagram
score are picked as the problem description.

For diagram parsing, GEOS uses a publicly
available diagram parser for geometry problems
(Seo et al., 2014) that provides confidence scores
for each literal to be true in the diagram. We use
the diagram parser from GEOS to handle in our
work too.

Text parsing is performed in three stages. The
parser first maps words or phrases in the text to
their corresponding concepts. Then, it identifies
relations between identified concepts. Finally, it
performs relation completion which handles im-
plications and coordinating conjunctions.

Finally, GEOS uses a numerical approach to
check the satisfiablity of literals, and to answer
the multiple-choice question. While this solver
is grounded in coordinate geometry and indeed
works well, it has some issues: GEOS requires
an explicit mapping of each predicate to a set of
constraints over point coordinates. For example,
the predicate isPerpendicular(AB, CD) is mapped
to the constraint yB−yAxB−xA ×

yD−yC
xD−xC = −1. These con-

252



Axiom Premise Conclusion
Midpoint Definition midpoint(M, AB) length(AM) = length(MB)

Angle Addition interior(D, ABC) angle(ABC) = angle(ABD) + angle(DBC)
Supplementary Angles perpendicular(AB,CD) ∧ liesOn(C,AB) angle(ACD) + angle(DCB) = 180◦
Vertically Opp. Angles intersectAt(AB, CD, M) angle(AMC) = angle(BMD)

Table 1: Examples of geometry theorems as horn clause rules.

straints can be non-trivial to write and often re-
quire manual engineering. As a result, GEOS’s
constraint set is incomplete and it cannot solve a
number of SAT style geometry problems. Further-
more, this solver is not interpretable. As our user
studies show, it is not natural for a student to un-
derstand the solution of these geometry problems
in terms of satisfiability of constraints over coor-
dinates. A more natural way for students to under-
stand and reason about these problems is through
deductive reasoning using well-known axioms and
theorems of geometry. This kind of deductive rea-
soning is used in explanations in textbooks. In
contrast to GEOS which uses supervised learning,
our approach learns to solve geometry problems
by interpreting natural language demonstrations of
the solution. These demonstrations illustrate the
process of solving the geometry problem via step-
wise application of geometry theorems.

3 Theorems as Horn Clause Rules

We represent theorems as horn clause rules that
map a premise in the logical language to a conclu-
sion in the same language. Table 1 gives some
examples of geometry theorems written as horn
clause rules. The free variables in the theorems
are universally quantified. The variables are also
typed. For example, ABC can be of type triangle
or angle but not line. Let T be the set of theo-
rems. Formally, each theorem t ∈ T maps a log-
ical formula l(pr)t corresponding to the premise to
a logical formula l(co)t corresponding to the con-
clusion. The demonstration can be seen as a pro-
gram – a sequence of horn clause rule applications
that lead to the solution of the geometry problem.
Given a current state, theorem t can be applied
to the state if there exists an assignment to free
variables in l(pr)t that is true in the state. Each
theorem application also has a probability asso-
ciated with it; in our case, these probabilities are
learned by a trained model. The state diagram for
the demonstration in Figure 2 is shown in Figure
3. Now, we describe the various components of
our learning from demonstrations approach: a se-

Figure 3: State sequence corresponding to the demonstration
in Figure 2. Theorems applied are marked in green and the
state information is marked in red. Here S0 corresponds to the
state derived from question interpretation and each theorem
application subsequently adds new predicates to the logical
formula corresponding to S0. The final state contains the an-
swer: measure(ADB, 60◦). This annotation of states and the-
orem applications is provided only for illustrative purposes.
It is not required by our model.

mantic parser to interpret the demonstration and a
deductive solver that learns to chain theorems.

4 Approach

4.1 Interpretation via Semantic Parsing

We first describe a semantic parser that maps a
piece of text (in the geometry question or a demon-
stration) to a logical expression such as the one
shown in Figure 1. Our semantic parser uses
a part-based log-linear model inspired from the
multi-step approach taken in GEOS, which, in-
turn is closely related to prior work in relation ex-
traction and semantic role labeling. However, un-
like GEOS, our parser combines the various steps
in a joint model. Our parser first maps words or
phrases in the input text x to corresponding con-
cepts in the geometry language. Then, it identi-
fies relations between identified concepts. Finally,
it performs relation completion to handle implica-
tions and coordinating conjunctions. We choose
a log-linear model over the parses which decom-
poses into two parts. Let p = {p1, p2} where p1
denotes the concepts identified in p and p2 de-
notes the identified relations. The relation com-
pletion is performed by using a similar rule-based
approach as in GEOS. The log-linear model also

253



factorizes into two components for concept and re-
lation identification:

P(p|x;θθθ p) = 1Z(x;θθθ p) exp
(
θθθ Tpφφφ(p,x)

)
θθθ Tpφφφ(p,x) = θθθ

T
p1φ1φ1φ1(p1,x)+θθθ

T
p2φφφ 2(p2,x)

Z(x;θθθ p) is the partition function of the log-linear
model and φφφ is the concatenation [φφφ 1 φφφ 2]. The
complexity of searching for the highest scoring
latent parse is exponential. Hence, we use beam
search with a fixed beam size (100) for inference.
That is, in each step, we only expand the ten most
promising candidates so far given by the current
score. We first infer p1 to identify a beam of
concepts. Then, we infer p2 to identify relations
among candidate concepts. We find the optimal
parameters θθθ p using maximum-likelihood estima-
tion with L2 regularization:

θθθ ∗p = argmax
θθθ p

∑
(x,p)∈Train

logP(p|x;θθθ p)−λ ||θθθ p||22

We use L-BFGS to optimize the objective. Fi-
nally, relation completion is performed using a de-
terministic rule-based approach as in GEOS which
handles implicit concepts like the “Equals” rela-
tion in the sentence “Circle O has a radius of 5”
and coordinating conjunctions like “bisect” be-
tween the two lines and two angles in “AM and
CM bisect BAC and BCA”. We refer the interested
reader to section 4.3 in Seo et al. (2015) for details.

This semantic parser is used to identify program
states in demonstrations as well as to map geome-
try questions to logical expressions.

4.1.1 State and Axiom Identification
Given a demonstrative solution of a geometry
problem in natural language such as the one shown
in Figure 2, we identify theorem applications by
two simple heuristics. Often, theorem mentions
in demonstrations collected from textbooks are la-
beled as references to theorems previously intro-
duced in the textbook (for example, “Theorem
3.1”). In this case, we simply label the theo-
rem application as the referenced theorem. Some-
times, the theorems are mentioned verbosely in
the demonstration. To identify these mentions, we
collect a set of theorem mentions from textbooks.
Each theorem is also represented as a set of the-
orem mentions. Then, we use an off-the-shelf se-
mantic text similarity system (Šarić et al., 2012)
and check if a contiguous sequence of sentences

in the demonstration is a paraphrase of any of the
gold theorem mentions. If the degree of similar-
ity of a contiguous sequence of sentences in the
demonstration with any of the gold theorem men-
tions is above a threshold, our system labels the se-
quence of sentences as the theorem. The text sim-
ilarity system is tuned on the training dataset and
the threshold is tuned on the development set. This
heuristic works well and has a small error (< 10%)
on our development set.

For state identification, we use our semantic
parser. The initial state corresponds to the logical
expression corresponding to the question. Subse-
quent states are derived by parsing sentences in the
demonstration. The identified state sequences are
used to train our deductive solver.

4.2 Deductive Solver

Our deductive solver, inspired from Krishna-
murthy et al. (2016), uses the parsed state and
axiom information (when provided) and learns to
score the sequence of axiom applications which
can lead to the solution of the problem. Our solver
uses a log-linear model over the space of possible
axiom applications. Given a set of theorems T
and optionally demonstration d, we assume T =
[t1, t2, . . . tk] to be a sequence of theorem applica-
tions. Each theorem application leads to a change
in state. Let s0 be the initial state determined by
the logical formula derived from the question text
and the diagram. Let s = [s1,s2, . . .sk] be the se-
quence of program states after corresponding the-
orem applications. The final state sk contains the
answer to the question. We define the model score
of the deduction as:

P(s|T,d;θθθ ex) =
1

Z(T,d;θθθ ex)

k

∏
i=1

exp
(
θθθ Texψψψ(si−1,si, ti,d)

)
Here, θθθ ex represents the model parameters and ψψψ
represents the feature vector that depends on the
successive states si−1 and si, the demonstration
d and the corresponding theorem application ti.
We find optimal parameters θθθ ex using maximum-
likelihood estimation with L2 regularization:

θθθ ∗ex = argmax
θθθ ex

∑
s∈Train

logP(s|T,d;θθθ ex)−µ||θθθ ex||22

We use beam search for inference and L-BFGS to
optimize the objective.

254



4.3 Joint Semantic Parsing and Deduction

Finally, we describe a joint model for semantic
parsing and problem solving that parses the geom-
etry problem text, the demonstration when avail-
able, and learns a sequence of theorem applica-
tions that can solve the problem.

In this case, we use a joint log-linear model for
semantic parsing and deduction. The model com-
prises of factors that scores semantic parses of the
question and the demonstration (when provided)
and the other that scores various possible theo-
rem applications. The model predicts the answer
a given the question q (and possibly demonstra-
tion d) using two latent variables: p represents
the latent semantic parse of the question and the
demonstration which involves identifying the log-
ical formula for the question (and for every state
in the demonstration when provided) and s repre-
sents the (possibly latent) program.

P(p,s|q,a,d;θθθ) ∝ fp(p|{q,a,d};θθθ p)
× fs(s|T,d, ;θθθ s)

Here, θθθ = {θθθ p,θθθ ex}. fp and fs represent
the factors for semantic parsing and deduc-
tion. fp(p|{q,a,d};θθθ p) ∝ exp

(
θθθ Tpφφφ(p,{q,a,d})

)
and fs(s|T,d, ;θθθ s) ∝

k
∏
i=1

exp
(
θθθ Texψψψ(si−1,si, ti,d)

)
as defined in Sections 4.1 and 4.2. Next, we de-
scribe approaches to learn the joint model with
various kinds of supervision.

4.4 Learning from Types of Supervision

Our joint model for parsing and deduction can be
learned using various kinds of supervision. We
provide a learning algorithm when (a) we only
have geometry question-answer pairs as supervi-
sion, (b) when we have geometry questions and
demonstrations for solving them, and (c) mixed
supervision: when we have a set of geometry
question-answer pairs in addition to some geom-
etry questions and demonstrations. To do this,
we implement two supervision schemes (Krishna-
murthy et al., 2016). The first supervision scheme
only verifies the answer and treats other states in
the supervision as latent. The second scheme ver-
ifies every state in the program. We combine both
kinds of supervision when provided. Given super-
vision {qi,ai}ni=1 and {qi,ai.di}mi=1, we define the

following L2 regularized objective:

J (θθθ) = ν
n

∑
i=1

log ∑
p,s

P(p,s|qi,ai;θθθ)×1exec(s)=ai

+(1−ν)
m

∑
i=1

log ∑
p,s

P(p,s|qi,ai,di;θθθ)×1s(di)=s

−λ ||θθθ p||22−µ||θθθ ex||22
For learning from answers, we set ν = 1. For
learning from demonstrations, we set ν = 0. We
tune hyperparameters λ , µ and ν on a held out
dev set. We use L-BFGS, using beam search for
inference for training all our models. To avoid re-
peated usage of unnecessary theorems in the so-
lution, we constrain the next theorem application
to be distinct from previous theorem applications
during beam search.

4.5 Features
Next, we define our feature set: φφφ 1, φφφ 2 for learn-
ing the semantic parser and ψψψ for learning the de-
duction model. Semantic parser features φφφ 1 and
φφφ 2 are inspired from GEOS. The deduction model
features ψψψ score consecutive states in the deduc-
tion si−1, si and the theorem ti which when applied
to si−1 leads to si. ψψψ comprises of features that
score if theorem ti is applicable on state si−1 and if
the application of ti on state si−1 leads to state si.
Table 2 lists the feature set.

5 Demonstrations Dataset

We collect a new dataset of demonstrations for
solving geometry problems from a set of grade 6-
10 Indian high school math textbooks by four pub-
lishers/authors – NCERT1, R S Aggarwal2, R D
Sharma3 and M L Aggarwal4 – a total of 5× 4 =
20 textbooks as well as a set of online geometry
problems and solutions from three popular edu-
cational portals: Tiwari Academy5, School Lamp6

and Oswaal Books7 for grade 6-10 students in In-
dia. Millions of students in India study geome-
try from these books and portals every year and
these materials are available online. We manually

1http://epathshala.nic.in/
e-pathshala-4/flipbook/

2http://www.amazon.in/
Books-R-S-Aggarwal/

3http://www.amazon.in/Books-R-Sharma/
4http://www.amazon.in/

Books-Aggarwal-M-L/
5http://www.tiwariacademy.com/
6http://www.schoollamp.com
7http://www.oswaalbooks.com

255



φφ φ
1

Lexicon Map Indicator that the word or phrase maps to a predicate in a lexicon created in
GEOS. GEOS derives correspondences between words/phrases and geometry
keywords and concepts in the geometry language using manual annotations
in its training data. For instance, the lexicon contains (“square”, square, Is-
Square) including all possible concepts for the phrase “square”.

Regex for num-
bers and explicit
variables

Indicator that the word or phrase satisfies a regular expression to detect num-
bers or explicit variables (e.g. “5”, “AB”, “O”). These regular expressions
were built as a part of GEOS.

φφ φ
2

Dependency
tree distance

Shortest distance between the words of the concept nodes in the dependency
tree. We use indicator features for distances of -3 to 3. Positive distance
shows if the child word is at the right of the parentâĂŹs in the sentence, and
negative otherwise.

Word distance Distance between the words of the concept nodes in the sentence.
Dependency
edge

Indicator functions for outgoing edges of the parent and child for the shortest
path between them.

Part of speech
tag

Indicator functions for the POS tags of the parent and the child

Relation type Indicator functions for unary / binary parent and child nodes.
Return type Indicator functions for the return types of the parent and the child nodes. For

example, return type of Equals is boolean, and that of LengthOf is numeric.

ψψ ψ

State and the-
orem premise
predicates

Treat the state si−1 and theorem premise l
(pr)
ti as multi-sets of predicates. The

feature is given by div(si−1||l(pr)ti ), the divergence between the two multi-
sets. div(A,B), the divergence between multi-sets A and B is given by
∑k

min(Ak,Bk)
Bk

which measures the degree to which the elements in A satisfy
the pre-condition in B.

State and the-
orem premise
predicate-
arguments

Now treat the state si−1 and theorem premise l
(pr)
ti as two multi-sets over

predicate-arguments. The feature is given by div(si−1||l(pr)ti ), the divergence
between the two multi-sets.

State and theo-
rem conclusion
predicates

Now treat the state si and theorem conclusion l
(co)
ti as two multi-sets over

predicate-arguments. The feature is given by div(si||l(co)ti ), the divergence
between the two multi-sets.

State and theo-
rem conclusion
predicate-
arguments

Now treat the state si and theorem conclusion l
(co)
ti as two multi-sets over

predicate-arguments. The feature is given by div(si||l(co)ti ), the divergence
between the two multi-sets.

State and theo-
rem conclusion
predicates

Treat the state si and theorem conclusion l
(co)
ti as two distributions over predi-

cates. The feature is the total variation distance between the two distributions.

State and theo-
rem conclusion
predicate-
arguments

Now treat the state ei and theorem conclusion l
(co)
ti as two distributions over

predicate-arguments. The feature is the total variation distance between the
two distributions.

Product Fea-
tures

We additionally use three product features: ψψψ1ψψψ3ψψψ5, ψψψ2ψψψ4ψψψ6 and
ψψψ1ψψψ2ψψψ3ψψψ4ψψψ5ψψψ6

Table 2: The feature set for our joint semantic-parsing and deduction model. Features φφφ 1 and φφφ 2 are motivated from GEOS

256



marked chapters relevant for geometry in these
books and then parsed them using Adobe Acro-
bat’s pdf2xml parser. Then, we manually extracted
example problems leading to a total of 2235 geom-
etry problems with demonstrations. We also an-
notated 1000 demonstrations by labeling the var-
ious states and theorem applications. We manu-
ally collected a set of theorems of geometry by go-
ing through the textbooks, and wrote them as horn
clause rules. A total of 293 unique theorems were
collected. Then, we marked contiguous sentences
in the demonstration texts as one of these 293 the-
orems or as states. An example annotation for the
running example in Figures 1 and 2 is provided in
Figure 3. Note that the annotation of states and
theorem applications is not used in training our
models and is only used for testing the accuracy
of the programs induced by our model.

6 Experiments

We use three geometry question datasets for eval-
uating our system: practice and official SAT style
geometry questions used in GEOS, and an ad-
ditional dataset of geometry questions collected
from the aforementioned textbooks. We selected
a total of 1406 SAT style questions across grades
6-10. This dataset is approximately 7.5 times
the size of the datasets used in Seo et al. (2015).
We split the dataset into training (350 questions),
development (150 questions) and test (906 ques-
tions) with equal proportion of grade 6-10 ques-
tions. We also annotated the training and de-
velopment set questions with ground-truth logi-
cal forms. GEOS used 13 types of entities, 94
functions and predicates. We added some more
entities, functions and predicates to cover other
more complex concepts in geometry not covered
in GEOS. Thus, we obtained a final set of 19 en-
tity types and 115 functions and predicates. We
use the training set to train our semantic parser
with expanded set of entity types, functions and
predicates. We used Stanford CoreNLP (Manning
et al., 2014) for linguistic pre-processing. We also
adapted the GEOS solver to the expanded set of
entities, functions and predicates for comparison
purposes. We call this system GEOS++.

6.1 Quantitative Results

We evaluated our joint model of semantic parsing
and deduction with various settings for training:
training on question-answer pairs or demonstra-

P O T
GEOS 61 49 32

GEOS++ 62 49 44
O.S. (QA Pairs) 63 52 47

O.S. (Demonstrations) 66 55 56
O.S. (QA + Demonstrations) 67 57 58

Table 3: Scores of various approaches on the SAT practice
(P) and official (O) datasets and a dataset of questions from
the 20 textbooks (T). We use SATâĂŹs grading scheme that
rewards a correct answer with a score of 1.0 and penalizes a
wrong answer with a negative score of 0.25. O.S. represents
our system trained on question-answer (QA) pairs, demon-
strations, or a combination of QA pairs and demonstrations.

tions alone, or with a combination of question-
answer pairs and demonstrations. We compare
our joint semantic parsing and deduction models
against GEOS and GEOS++.

In the first setting, we only use question-answer
pairs as supervision. We compare our seman-
tic parsing and deduction model to GEOS and
GEOS++ on practice and official SAT style ge-
ometry questions from Seo et al. (2015) as well
as the dataset of geometry questions collected
from the 20 textbooks (see Table 3). On all the
three datasets, our system outperforms GEOS and
GEOS++. Especially on the dataset from the 20
textbooks (which is a harder dataset and includes
more problems which require complex reasoning
supported by our deduction model), GEOS and
GEOS++ do not perform very well whereas our
system achieves a very good score.

Next, we only use demonstrations to train our
joint model (see Table 3). We test this model
on the aforementioned datasets and compare it
to GEOS and GEOS++ trained on respective
datasets. Again, our system outperforms GEOS
and GEOS++ on all three datasets. Especially on
the textbook dataset, this model trained on demon-
strations has significant improvements as our se-
mantic parsing and deduction model trains the de-
duction model as well and learns to reason about
geometry using axiomatic knowledge.

Finally, we train our semantic parsing and
deduction model on a combination of question
answer-pairs and demonstrations. This model
trained on question-answer pairs and demonstra-
tions leads to further improvements over mod-
els trained only question-answer pairs or only on
demonstrations. These results (shown in Table 3)
hold on all the three datasets.

We tested the correctness of the parses and the

257



P R F1
GEOS 0.82 0.63 0.71

O.S. (Parser) 0.88 0.75 0.81
O.S. (Joint) 0.89 0.80 0.84

Table 4: Precision, Recall and F1 scores of the parses induced
by GEOS and our models when only the parsing model or the
joint model is used.

Deduction Joint
QA Pairs 0.56 0.61

Demonstrations 0.64 0.68
QA + Demonstrations 0.68 0.70

Table 5: Accuracy of the programs induced by various ver-
sions of our joint model trained on question-answer pairs,
demonstrations or a combination of the two. We provide re-
sults when we use the deduction model or the joint model.

deductive programs induced by our models. First,
we compared the parses induced by our models
with gold parses on the development set. Table
4 reports the Precision, Recall and F1 scores of
the parses induced by our models when only the
parsing model or when the joint model is used and
compares it with GEOS. We conclude that both
our models perform better as compared to GEOS
in parsing. Furthermore, our joint model of pars-
ing and deduction further improves the parsing ac-
curacy. Then, we compared the programs induced
by the aforementioned models with gold program
annotations on the textbook dataset. Table 5 re-
ports the accuracy of programs induced by var-
ious versions of our models. Our models when
trained on demonstrations induces more accurate
programs as compared to the semantic parsing and
deduction model when trained on question-answer
pairs. Moreover, the semantic parsing and deduc-
tion model when trained on question-answer pairs
as well as demonstrations achieves an even better
accuracy. Our joint model of parsing and deduc-
tion induces more accurate programs as compared
to the deduction model alone.

6.2 User Study on Interpretability

A key benefit of our axiomatic solver is that it
provides an easy-to-understand student-friendly
demonstrative solution to geometry problems.
This is important because students typically learn
geometry by rigorous deduction whereas numeri-
cal solvers do not provide such interpretability.

To test the interpretability of our axiomatic
solver, we asked 50 grade 6-10 students (10 stu-

Interpretability Usefulness
GEOS++ O.S. GEOS++ O.S.

Grade 6 2.7 3.0 2.9 3.2
Grade 7 3.0 3.7 3.3 3.6
Grade 8 2.7 3.6 3.1 3.5
Grade 9 2.4 3.4 3.0 3.6
Grade 10 2.8 3.1 3.2 3.7
Overall 2.7 3.4 3.1 3.5

Table 6: User study ratings for GEOS++ and our system
(O.S.) trained on question-answer pairs and demonstrations
by a number of grade 6-10 student subjects. Ten students in
each grade were asked to rate the two systems on a scale of
1-5 on two facets: ‘interpretability’ and ‘usefulness’. Each
cell shows the mean rating computed over ten students in that
grade for that facet.

dents in each grade) to use GEOS++ and our best
performing system trained on question-answer
pairs and demonstrations as a web-based assistive
tool. They were each asked to rate how ‘inter-
pretable’ and ‘useful’ the two systems were for
their studies on a scale of 1-5. Table 6 shows
the mean rating by students in each grade on the
two facets. We can observe that students of each
grade found our system to be more interpretable as
well as more useful to them than GEOS++. This
study supports the need and the efficacy of an in-
terpretable solution for geometry problems. Our
solution can be used as an assistive tool for help-
ing students learn geometry on MOOCs.

7 Related Work

Solving Geometry Problems: Standardized tests
have been recently proposed as ‘drivers for
progress in AI’ (Clark and Etzioni, 2016). These
tests are easily accessible, and measurable, and
hence have attracted several NLP researchers.
There is a growing body of work on solving stan-
dardized tests such as reading comprehensions
(Richardson et al., 2013, inter alia), science ques-
tion answering (Clark, 2015; Schoenick et al.,
2016, inter alia), algebra word problems (Kush-
man et al., 2014; Roy and Roth, 2015, inter alia),
geometry problems (Seo et al., 2014, 2015) and
pre-university entrance exams (Fujita et al., 2014;
Arai and Matsuzaki, 2014).

While the problem of using computers to
solve geometry questions is old (Feigenbaum and
Feldman, 1963; Schattschneider and King, 1997;
Davis, 2006), NLP and vision techniques were
first used to solve geometry problems in Seo et al.
(2015). While Seo et al. (2014) only aligned ge-
ometric shapes with their textual mentions, Seo

258



et al. (2015) also extracted geometric relations and
built GEOS. We improve GEOS by building an ax-
iomatic solver that performs deductive reasoning
by learning from demonstrative problem solutions.

Learning from Demonstration: Our work fol-
lows the learning from demonstration view of ma-
chine learning (Schaal, 1997) which stems from
the work on social learning in developmental psy-
chology (Meltzoff and Moore, 1977; Meltzoff,
1995). Learning from demonstration is a popu-
lar way of learning policies from example state
to action mappings in robotics applications. Im-
itation learning (Schaal, 1999; Abbeel and Ng,
2004; Ross et al., 2011) is a popular instance of
learning from demonstration where the algorithm
observes a human expert perform a series of ac-
tions to accomplish the task and learns a policy
that “imitates” the expert with the purpose of gen-
eralizing to unseen data. Imitation learning is in-
creasingly being used in NLP (Vlachos and Clark,
2014; Berant and Liang, 2015; Augenstein et al.,
2015; Beck et al., 2016; Goodman et al., 2016a,b).
However, all these models focus on learning re-
spective NLP models from the final supervision
e.g. semantic parses or denotations. However,
we provide a technique to learn from demonstra-
tions by learning a joint semantic parsing and de-
duction model. Another related line of work is
Hixon et al. (2015) who acquire knowledge in the
form of knowledge graphs for question answering
from natural language dialogs and (Goldwasser
and Roth, 2014) who propose a technique called
learning from natural instructions. Learning from
natural instructions allows human teachers to in-
teract with an automated learner using natural in-
structions, allowing the teacher to communicate
the domain expertise to the learner via natural lan-
guage. However, this work was evaluated on a
very simple Freecell game with a very small num-
ber of concepts (3). On the other hand, our model
is evaluated on a real task of solving SAT style ge-
ometry problems.

Semantic Parsing: Semantic parsing is the
NLP task of learning to map language to a formal
meaning representation. Early semantic parsers
learnt the parsing model from natural language
utterances paired with logical forms (Zelle and
Mooney, 1993, 1996; Kate et al., 2005, inter alia).
However, recently indirect supervision, such as
denotations (Liang et al., 2011; Berant et al., 2013,
inter alia) and natural language directions for robot

navigation (Shimizu and Haas, 2009; Matuszek
et al., 2010; Chen and Mooney, 2011, inter alia)
are being used to train these semantic parsers. In
most of the above examples, the execution model
is fairly simple (e.g. execution of a SQL query
in a database, or binary feedback for interaction
of the robot with the environment). However, our
work uses demonstrations such as those given in
textbooks for learning a semantic parser. Further-
more, our work learns the semantic parser along
with the execution model. In our case, the exe-
cution model is a program sequence constructed
from a set of theorem applications. Thus, our
work provides a way to integrate semantic pars-
ing with probabilistic programming. This integra-
tion has been pursued before for science diagram
question-answering on food-web networks (Krish-
namurthy et al., 2016) – which is closely related
to our work. Technically, our deductive solver and
the approach of learning from different kinds of
supervision are the same as the execution model
in Krishnamurthy et al. (2016). While Krishna-
murthy et al. (2016) only has two program encod-
ings, our work involves a much larger number of
programs. We also provide an approach for learn-
ing from demonstrations.

8 Conclusion

We described an approach that learns to solve SAT
style geometry problems using detailed demon-
strative solutions in natural language. The ap-
proach learns to jointly interpret demonstrations
as well as how to use this interpretation to deduc-
tively solve geometry problems using axiomatic
knowledge. Our approach showed significant im-
provements over the best previously published
work on a number of datasets. A user-study con-
ducted on a number of school students studying
geometry found our approach to be more inter-
pretable and useful than its predecessors. In the
future, we would like to extend our work in other
domains such as science QA (Jansen et al., 2016)
and use our work to assist student learning on plat-
forms such as MOOCs.

Acknowledgments

We thank the anonymous reviewers for their valu-
able comments and suggestions. This work was
supported by the following research grants: NSF
IIS1447676, ONR N000141410684 and ONR
N000141712463.

259



References
Pieter Abbeel and Andrew Y Ng. 2004. Apprentice-

ship learning via inverse reinforcement learning. In
Proceedings of the twenty-first international confer-
ence on Machine learning. ACM, page 1.

R.L. Allington and P.M. Cunningham. 2010. Children
benefit from modeling, demonstration, and explana-
tion .

Noriko H Arai and Takuya Matsuzaki. 2014. The im-
pact of ai on education–can a robot get into the uni-
versity of tokyo? In Proc. ICCE. pages 1034–1042.

Brenna D Argall, Sonia Chernova, Manuela Veloso,
and Brett Browning. 2009. A survey of robot learn-
ing from demonstration. Robotics and autonomous
systems 57(5):469–483.

Isabelle Augenstein, Andreas Vlachos, and Diana
Maynard. 2015. Extracting relations between non-
standard entities using distant supervision and im-
itation learning. In Proceedings of the 2015 Con-
ference on Empirical Methods in Natural Language
Processing. Association for Computational Linguis-
tics, pages 747–757.

Daniel Beck, Andreas Vlachos, Gustavo Paetzold, and
Lucia Specia. 2016. SHEF-MIME: word-level qual-
ity estimation using imitation learning. In Proceed-
ings of the First Conference on Machine Translation,
WMT 2016, colocated with ACL 2016, August 11-
12, Berlin, Germany. pages 772–776.

Jonathan Berant, Andrew Chou, Roy Frostig, and Percy
Liang. 2013. Semantic parsing on freebase from
question-answer pairs. In Proceedings of the 2013
Conference on Empirical Methods in Natural Lan-
guage Processing, EMNLP 2013, 18-21 October
2013, Grand Hyatt Seattle, Seattle, Washington,
USA, A meeting of SIGDAT, a Special Interest Group
of the ACL. pages 1533–1544.

Jonathan Berant and Percy Liang. 2015. Imitation
learning of agenda-based semantic parsers. Trans-
actions of the Association for Computational Lin-
guistics 3:545–558.

Daphna Buchsbaum, Alison Gopnik, Thomas L Grif-
fiths, and Patrick Shafto. 2011. ChildrenâĂŹs im-
itation of causal action sequences is influenced by
statistical and pedagogical evidence. Cognition
120(3):331–340.

John Burger, Claire Cardie, Vinay Chaudhri, Robert
Gaizauskas, Sanda Harabagiu, David Israel, Chris-
tian Jacquemin, Chin-Yew Lin, Steve Maiorano,
et al. 2001. Issues, tasks and program structures to
roadmap research in question & answering (q&a) .

Lucas P Butler and Ellen M Markman. 2014.
Preschoolers use pedagogical cues to guide radical
reorganization of category knowledge. Cognition
130(1):116–127.

David L. Chen and Raymond J. Mooney. 2011. Learn-
ing to interpret natural language navigation instruc-
tions from observations. In Proceedings of the 25th
AAAI Conference on Artificial Intelligence (AAAI-
2011). pages 859–865.

Peter Clark. 2015. Elementary School Science and
Math Tests as a Driver for AI:Take the Aristo Chal-
lenge! In Proceedings of IAAI.

Peter Clark and Oren Etzioni. 2016. My computer is
an honor student - but how intelligent is it? stan-
dardized tests as a measure of ai. In Proceedings of
AI Magazine.

Tom Davis. 2006. Geometry with computers. Techni-
cal report.

Edward A Feigenbaum and Julian Feldman. 1963.
Computers and thought. The AAAI Press.

Richard M Felder, Donald R Woods, James E Stice,
and Armando Rugarcia. 2000. The future of en-
gineering education ii. teaching methods that work.
Chemical Engineering Education pages 26–39.

Akira Fujita, Akihiro Kameda, Ai Kawazoe, and
Yusuke Miyao. 2014. Overview of todai robot
project and evaluation framework of its nlp-based
problem solving. World History 36:36.

Dan Goldwasser and Dan Roth. 2014. Learning from
natural instructions. Machine Learning 94(2):205–
232.

James Goodman, Andreas Vlachos, and Jason Narad-
owsky. 2016a. Noise reduction and targeted explo-
ration in imitation learning for abstract meaning rep-
resentation parsing. In Proceedings of the 54th An-
nual Meeting of the Association for Computational
Linguistics (Volume 1: Long Papers).

James Goodman, Andreas Vlachos, and Jason Narad-
owsky. 2016b. Ucl+ sheffield at semeval-2016 task
8: Imitation learning for amr parsing with an α-
bound. Proceedings of SemEval pages 1167–1172.

Ben Hixon, Peter Clark, and Hannaneh Hajishirzi.
2015. Learning knowledge graphs for question an-
swering through conversational dialog. In NAACL
HLT 2015, The 2015 Conference of the North Amer-
ican Chapter of the Association for Computational
Linguistics: Human Language Technologies, Den-
ver, Colorado, USA, May 31 - June 5, 2015. pages
851–861. http://aclweb.org/anthology/N/N15/N15-
1086.pdf.

Peter Jansen, Niranjan Balasubramanian, Mihai Sur-
deanu, and Peter Clark. 2016. What’s in an
explanation? characterizing knowledge and in-
ference requirements for elementary science ex-
ams. In COLING 2016, 26th International Con-
ference on Computational Linguistics, Proceed-
ings of the Conference: Technical Papers, Decem-
ber 11-16, 2016, Osaka, Japan. pages 2956–2965.
http://aclweb.org/anthology/C/C16/C16-1278.pdf.

260



Rohit J Kate, Yuk Wah, Wong Raymond, and
J Mooney. 2005. Learning to transform natural to
formal languages. In Proceedings of AAAI-05. Cite-
seer.

Jayant Krishnamurthy, Oyvind Tafjord, and Aniruddha
Kembhavi. 2016. Semantic parsing to probabilistic
programs for situated question answering. In Jian
Su, Xavier Carreras, and Kevin Duh, editors, Pro-
ceedings of the 2016 Conference on Empirical Meth-
ods in Natural Language Processing, EMNLP 2016,
Austin, Texas, USA, November 1-4, 2016. The As-
sociation for Computational Linguistics, pages 160–
170.

Nate Kushman, Yoav Artzi, Luke Zettlemoyer, and
Regina Barzilay. 2014. Learning to automatically
solve algebra word problems. In Proceedings of
the Annual Meeting of the Association for Compu-
tational Linguistics.

Percy Liang, Michael I Jordan, and Dan Klein. 2011.
Learning dependency-based compositional seman-
tics. In Proceedings of the 49th Annual Meeting of
the Association for Computational Linguistics: Hu-
man Language Technologies-Volume 1. Association
for Computational Linguistics, pages 590–599.

Christopher D. Manning, Mihai Surdeanu, John
Bauer, Jenny Finkel, Steven J. Bethard,
and David McClosky. 2014. The Stanford
CoreNLP natural language processing toolkit.
In Association for Computational Linguistics
(ACL) System Demonstrations. pages 55–60.
http://www.aclweb.org/anthology/P/P14/P14-5010.

Cynthia Matuszek, Dieter Fox, and Karl Koscher.
2010. Following directions using statistical ma-
chine translation. In 2010 5th ACM/IEEE Inter-
national Conference on Human-Robot Interaction
(HRI). IEEE, pages 251–258.

Andrew N Meltzoff. 1995. Understanding the inten-
tions of others: re-enactment of intended acts by
18-month-old children. Developmental psychology
31(5):838.

Andrew N. Meltzoff and M. Keith Moore. 1977.
Imitation of facial and manual gestures by
human neonates. Science 198(4312):75–78.
https://doi.org/10.1126/science.198.4312.75.

Matthew Richardson, Christopher JC Burges, and Erin
Renshaw. 2013. Mctest: A challenge dataset for
the open-domain machine comprehension of text. In
Proceedings of Empirical Methods in Natural Lan-
guage Processing (EMNLP).

Stéphane Ross, Geoffrey J. Gordon, and Drew Bag-
nell. 2011. A reduction of imitation learning and
structured prediction to no-regret online learning. In
Proceedings of the Fourteenth International Confer-
ence on Artificial Intelligence and Statistics, AIS-
TATS 2011, Fort Lauderdale, USA, April 11-13,
2011. pages 627–635.

Subhro Roy and Dan Roth. 2015. Solving gen-
eral arithmetic word problems. In Proceedings of
EMNLP.

Stefan Schaal. 1997. Learning from demonstration.
In M. I. Jordan and T. Petsche, editors, Advances
in Neural Information Processing Systems 9, MIT
Press, pages 1040–1046.

Stefan Schaal. 1999. Is imitation learning the route
to humanoid robots? Trends in cognitive sciences
3(6):233–242.

Doris Schattschneider and James King. 1997. Geom-
etry Turned On: Dynamic Software in Learning,
Teaching, and Research. Mathematical Association
of America Notes.

Carissa Schoenick, Peter Clark, Oyvind Tafjord,
Peter D. Turney, and Oren Etzioni. 2016.
Moving beyond the turing test with the allen
AI science challenge. CoRR abs/1604.04315.
http://arxiv.org/abs/1604.04315.

Min Joon Seo, Hannaneh Hajishirzi, Ali Farhadi, and
Oren Etzioni. 2014. Diagram understanding in ge-
ometry questions. In Proceedings of AAAI.

Min Joon Seo, Hannaneh Hajishirzi, Ali Farhadi, Oren
Etzioni, and Clint Malcolm. 2015. Solving geome-
try problems: combining text and diagram interpre-
tation. In Proceedings of EMNLP.

Nobuyuki Shimizu and Andrew R. Haas. 2009. Learn-
ing to follow navigational route instructions. In
IJCAI 2009, Proceedings of the 21st Interna-
tional Joint Conference on Artificial Intelligence,
Pasadena, California, USA, July 11-17, 2009. pages
1488–1493.

Andreas Vlachos and Stephen Clark. 2014. A new cor-
pus and imitation learning framework for context-
dependent semantic parsing. Transactions of the As-
sociation for Computational Linguistics 2:547–559.

Frane Šarić, Goran Glavaš, Mladen Karan, Jan Šna-
jder, and Bojana Dalbelo Bašić. 2012. Takelab:
Systems for measuring semantic text similar-
ity. In Proceedings of the Sixth International
Workshop on Semantic Evaluation (SemEval
2012). Association for Computational Lin-
guistics, Montréal, Canada, pages 441–448.
http://www.aclweb.org/anthology/S12-1060.

John M. Zelle and Raymond J. Mooney. 1993. Learn-
ing semantic grammars with constructive inductive
logic programming. In Proceedings of the 11th Na-
tional Conference on Artificial Intelligence. Wash-
ington, DC, USA, July 11-15, 1993.. pages 817–822.

John M Zelle and Raymond J Mooney. 1996. Learn-
ing to parse database queries using inductive logic
programming. In In Proceedings of the Thirteenth
National Conference on Artificial Intelligence.

261


