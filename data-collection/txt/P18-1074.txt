




































A Multi-lingual Multi-task Architecture for Low-resource Sequence Labeling


Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers), pages 799–809
Melbourne, Australia, July 15 - 20, 2018. c©2018 Association for Computational Linguistics

799

A Multi-lingual Multi-task Architecture for Low-resource Sequence
Labeling

Ying Lin1 ∗, Shengqi Yang2, Veselin Stoyanov3, Heng Ji1
1 Computer Science Department, Rensselaer Polytechnic Institute, Troy, NY, USA

{liny9,jih}@rpi.edu
2 Intelligent Advertising Lab, JD.com, Santa Clara, CA, USA

sheqiyan@gmail.com
3 Applied Machine Learning, Facebook, Menlo Park, CA, USA

vesko.st@gmail.com

Abstract

We propose a multi-lingual multi-task ar-
chitecture to develop supervised models
with a minimal amount of labeled data for
sequence labeling. In this new architec-
ture, we combine various transfer mod-
els using two layers of parameter shar-
ing. On the first layer, we construct the
basis of the architecture to provide uni-
versal word representation and feature ex-
traction capability for all models. On the
second level, we adopt different parame-
ter sharing strategies for different transfer
schemes. This architecture proves to be
particularly effective for low-resource set-
tings, when there are less than 200 train-
ing sentences for the target task. Using
Name Tagging as a target task, our ap-
proach achieved 4.3%-50.5% absolute F-
score gains compared to the mono-lingual
single-task baseline model. 1

1 Introduction

When we use supervised learning to solve Natu-
ral Language Processing (NLP) problems, we typ-
ically train an individual model for each task with
task-specific labeled data. However, our target
task may be intrinsically linked to other tasks. For
example, Part-of-speech (POS) tagging and Name
Tagging can both be considered as sequence la-
beling; Machine Translation (MT) and Abstrac-
tive Text Summarization both require the ability
to understand the source text and generate natu-
ral language sentences. Therefore, it is valuable to
transfer knowledge from related tasks to the tar-
get task. Multi-task Learning (MTL) is one of

∗* Part of this work was done when the first author was
on an internship at Facebook.

1The code of our model is available at https://github.
com/limteng-rpi/mlmt

the most effective solutions for knowledge trans-
fer across tasks. In the context of neural network
architectures, we usually perform MTL by sharing
parameters across models (Ruder, 2017).

Previous studies (Collobert and Weston, 2008;
Dong et al., 2015; Luong et al., 2016; Liu et al.,
2018; Yang et al., 2017) have proven that MTL
is an effective approach to boost the performance
of related tasks such as MT and parsing. However,
most of these previous efforts focused on tasks and
languages which have sufficient labeled data but
hit a performance ceiling on each task alone. Most
NLP tasks, including some well-studied ones such
as POS tagging, still suffer from the lack of train-
ing data for many low-resource languages. Ac-
cording to Ethnologue2, there are 7, 099 living lan-
guages in the world. It is an unattainable goal to
annotate data in all languages, especially for tasks
with complicated annotation requirements. Fur-
thermore, some special applications (e.g., disaster
response and recovery) require rapid development
of NLP systems for extremely low-resource lan-
guages. Therefore, in this paper, we concentrate
on enhancing supervised models in low-resource
settings by borrowing knowledge learned from re-
lated high-resource languages and tasks.

In (Yang et al., 2017), the authors simulated
a low-resource setting for English and Spanish
by downsampling the training data for the tar-
get task. However, for most low-resource lan-
guages, the data sparsity problem also lies in re-
lated tasks and languages. Under such circum-
stances, a single transfer model can only bring lim-
ited improvement. To tackle this issue, we propose
a multi-lingual multi-task architecture which com-
bines different transfer models within a unified ar-
chitecture through two levels of parameter sharing.
In the first level, we share character embeddings,

2https://www.ethnologue.com/guides/
how-many-languages

https://github.com/limteng-rpi/mlmt
https://github.com/limteng-rpi/mlmt
https://www.ethnologue.com/guides/how-many-languages
https://www.ethnologue.com/guides/how-many-languages


800

character-level convolutional neural networks, and
word-level long-short term memory layer across
all models. These components serve as a basis
to connect multiple models and transfer univer-
sal knowledge among them. In the second level,
we adopt different sharing strategies for different
transfer schemes. For example, we use the same
output layer for all Name Tagging tasks to share
task-specific knowledge (e.g., I-PER3 should not
be assigned to the first word in a sentence).

To illustrate our idea, we take sequence label-
ing as a case study. In the NLP context, the goal
of sequence labeling is to assign a categorical label
(e.g., POS tag) to each token in a sentence. It un-
derlies a range of fundamental NLP tasks, includ-
ing POS Tagging, Name Tagging, and chunking.

Experiments show that our model can effec-
tively transfer various types of knowledge from
different auxiliary tasks and obtains up to 50.5%
absolute F-score gains on Name Tagging com-
pared to the mono-lingual single-task baseline.
Additionally, our approach does not rely on a large
amount of auxiliary task data to achieve the im-
provement. Using merely 1% auxiliary data, we
already obtain up to 9.7% absolute gains in F-
score.

2 Model

2.1 Basic Architecture

The goal of sequence labeling is to assign a
categorical label to each token in a given sen-
tence. Though traditional methods such as Hidden
Markov Models (HMMs) and Conditional Ran-
dom Fields (CRFs) (Lafferty et al., 2001; Rati-
nov and Roth, 2009; Passos et al., 2014) achieved
high performance on sequence labeling tasks, they
typically relied on hand-crafted features, therefore
it is difficult to adapt them to new tasks or lan-
guages. To avoid task-specific engineering, (Col-
lobert et al., 2011) proposed a feed-forward neu-
ral network model that only requires word embed-
dings trained on a large scale corpus as features.
After that, several neural models based on the
combination of long-short term memory (LSTM)
and CRFs (Ma and Hovy, 2016; Lample et al.,
2016; Chiu and Nichols, 2016) were proposed and

3We adopt the BIOES annotation scheme. Prefixes B-, I-
, E-, and S- represent the beginning of a mention, inside of
a mention, the end of a mention and a single-token mention
respectively. The O tag is assigned to a word which is not part
of any mention.

achieved better performance on sequence labeling
tasks.

Figure 1: LSTM-CNNs: an LSTM-CRFs-based
model for Sequence Labeling

LSTM-CRFs-based models are well-suited for
multi-lingual multi-task learning for three reasons:
(1) They learn features from word and character
embeddings and therefore require little feature en-
gineering; (2) As the input and output of each
layer in a neural network are abstracted as vec-
tors, it is fairly straightforward to share compo-
nents between neural models; (3) Character em-
beddings can serve as a bridge to transfer mor-
phological and semantic information between lan-
guages with identical or similar scripts, without
requiring cross-lingual dictionaries or parallel sen-
tences.

Therefore, we design our multi-task multi-
lingual architecture based on the LSTM-CNNs
model proposed in (Chiu and Nichols, 2016). The
overall framework is illustrated in Figure 1. First,
each word wi is represented as the combination
xi of two parts, word embedding and character
feature vector, which is extracted from character
embeddings of the characters in wi using convo-
lutional neural networks (CharCNN). On top of
that, a bidirectional LSTM processes the sequence
x = {x1, x2, ...} in both directions and encodes
each word and its context into a fixed-size vec-
tor hi. Next, a linear layer converts hi to a score
vector yi, in which each component represents the
predicted score of a target tag. In order to model
correlations between tags, a CRFs layer is added
at the top to generate the best tagging path for the
whole sequence. In the CRFs layer, given an in-
put sentence x of length L and the output of the
linear layer y, the score of a sequence of tags z is



801

defined as:

S(x,y, z) =
L∑

t=1

(Azt−1,zt + yt,zt),

where A is a transition matrix in which Ap,q rep-
resents the binary score of transitioning from tag
p to tag q, and yt,z represents the unary score of
assigning tag z to the t-th word. Given the ground
truth sequence of tags z, we maximize the follow-
ing objective function during the training phase:

O = logP (z|x)

= S(x,y, z)− log
∑
z̃∈Z

eS(x,y,z̃),

where Z is the set of all possible tagging paths.
We emphasize that our actual implementation

differs slightly from the LSTM-CNNs model.
We do not use additional word- and character-
level explicit symbolic features (e.g., capitaliza-
tion and lexicon) as they may require additional
language-specific knowledge. Additionally, we
transform character feature vectors using high-
way networks (Srivastava et al., 2015), which is
reported to enhance the overall performance by
(Kim et al., 2016) and (Liu et al., 2018). High-
way networks is a type of neural network that can
smoothly switch its behavior between transform-
ing and carrying information.

2.2 Multi-task Multi-lingual Architecture
MTL can be employed to improve performance on
multiple tasks at the same time, such as MT and
parsing in (Luong et al., 2016). However, in our
scenario, we only focused on enhancing the per-
formance of a low-resource task, which is our tar-
get task or main task. Our proposed architecture
aims to transfer knowledge from a set of auxiliary
tasks to the main task. For simplicity, we refer to
a model of a main (auxiliary) task as a main (aux-
iliary) model.

To jointly train multiple models, we perform
multi-task learning using parameter sharing. Let
Θi be the set of parameters for model mi and
Θi,j = Θi ∩Θj be the shared parameters between
mi and mj . When optimizing model mi, we up-
date Θi and hence Θi,j . In this way, we can par-
tially train model mj as Θi,j ⊆ Θj . Previously,
each MTL model generally uses a single transfer
scheme. In order to merge different transfer mod-
els into a unified architecture, we employ two lev-
els of parameter sharing as follows.

On the first level, we construct the basis of
the architecture by sharing character embeddings,
CharCNN and bidirectional LSTM among all
models. This level of parameter sharing aims to
provide universal word representation and feature
extraction capability for all tasks and languages.

Character Embeddings and Character-level
CNNs. Character features can represent morpho-
logical and semantic information; e.g., the En-
glish morpheme dis- usually indicates negation
and reversal as in “disagree” and “disapproval”.
For low-resource languages lacking in data to
suffice the training of high-quality word embed-
dings, character embeddings learned from other
languages may provide crucial information for la-
beling, especially for rare and out-of-vocabulary
words. Take the English word “overflying” (flying
over) as an example. Even if it is rare or absent
in the corpus, we can still infer the word meaning
from its suffix over- (above), root fly, and prefix
-ing (present participle form). In our architecture,
we share character embeddings and the CharCNN
between languages with identical or similar scripts
to enhance word representation for low-resource
languages.

Bidirectional LSTM. The bidirectional LSTM
layer is essential to extract character, word, and
contextual information from a sentence. However,
with a large number of parameters, it cannot be
fully trained only using the low-resource task data.
To tackle this issue, we share the bidirectional
LSTM layer across all models. Bear in mind that
because our architecture does not require aligned
cross-lingual word embeddings, sharing this layer
across languages may confuse the model as it
equally handles embeddings in different spaces.
Nevertheless, under low-resource circumstances,
data sparsity is the most critical factor that affects
the performance.

On top of this basis, we adopt different pa-
rameter sharing strategies for different transfer
schemes. For cross-task transfer, we use the same
word embedding matrix across tasks so that they
can mutually enhance word representations. For
cross-lingual transfer, we share the linear layer
and CRFs layer among languages to transfer task-
specific knowledge, such as the transition score
between two tags.

Word Embeddings. For most words, in addi-
tion to character embeddings, word embeddings
are still crucial to represent semantic informa-



802

Figure 2: Multi-task Multi-lingual Architecture

tion. We use the same word embedding matrix for
tasks in the same language. The matrix is initial-
ized with pre-trained embeddings and optimized
as parameters during training. Thus, task-specific
knowledge can be encoded into the word embed-
dings by one task and subsequently utilized by an-
other one. For a low-resource language even with-
out sufficient raw text, we mix its data with a re-
lated high-resource language to train word embed-
dings. In this way, we merge both corpora and
hence their vocabularies.

Recently, Conneau et al. (2017) proposed a
domain-adversarial method to align two mono-
lingual word embedding matrices without cross-
lingual supervision such as a bilingual dictionary.
Although cross-lingual word embeddings are not
required, we evaluate our framework with aligned
embeddings generated using this method. Experi-
ment results show that the incorporation of cross-
lingual embeddings substantially boosts the per-
formance under low-resource settings.

Linear Layer and CRFs. As the tag set varies
from task to task, the linear layer and CRFs can
only be shared across languages. We share these
layers to transfer task-specific knowledge to the
main model. For example, our model corrects [S-
PER Charles] [S-PER Picqué] to [B-PER Charles]
[E-PER Picqué] because the CRFs layer fully
trained on other languages assigns a low score to
the rare transition S-PER→S-PER and promotes
B-PER→E-PER. In addition to the shared linear
layer, we add an unshared language-specific lin-
ear layer to allow the model to behave differently

toward some features for different languages. For
example, the suffix -ment usually indicates nouns
in English whereas indicates adverbs in French.

We combine the output of the shared linear layer
yu and the output of the language-specific linear
layer ys using:

y = g ⊙ ys + (1− g)⊙ yu,

where g = σ(W gh + bg). W g and bg are op-
timized during training. h is the LSTM hidden
states. As W g is a square matrix, y, ys, and yu

have the same dimension.
Although we only focus on sequence labeling

in this work, our architecture can be adapted for
many NLP tasks with slight modification. For ex-
ample, for text classification tasks, we can take the
last hidden state of the forward LSTM as the sen-
tence representation and replace the CRFs layer
with a Softmax layer.

In our model, each task has a separate object
function. To optimize multiple tasks within one
model, we adopt the alternating training approach
in (Luong et al., 2016). At each training step, we
sample a task di with probability ri∑

j rj
, where ri

is the mixing rate value assigned to di. In our ex-
periments, instead of tuning ri, we estimate it by:

ri = µiζi
√

Ni ,

where µi is the task coefficient, ζi is the language
coefficient, and Ni is the number of training ex-
amples. µi (or ζi) takes the value 1 if the task



803

(or language) of di is the same as that of the tar-
get task; Otherwise it takes the value 0.1. For ex-
ample, given English Name Tagging as the target
task, the task coefficient µ and language coeffi-
cient ζ of Spanish Name Tagging are 0.1 and 1
respectively.

While assigning lower mixing rate values to
auxiliary tasks, this formula also takes the amount
of data into consideration. Thus, auxiliary tasks
receive higher probabilities to reduce overfitting
when we have a smaller amount of main task data.

3 Experiments

3.1 Data Sets
For Name Tagging, we use the following data
sets: Dutch (NLD) and Spanish (ESP) data from the
CoNLL 2002 shared task (Tjong Kim Sang, 2002),
English (ENG) data from the CoNLL 2003 shared
task (Tjong Kim Sang and De Meulder, 2003),
Russian (RUS) data from LDC2016E95 (Rus-
sian Representative Language Pack), and Chechen
(CHE) data from TAC KBP 2017 10-Language
EDL Pilot Evaluation Source Corpus4. We se-
lect Chechen as another target language in addi-
tion to Dutch and Spanish because it is a truly
under-resourced language and its related language,
Russian, also lacks NLP resources.

Code Train Dev Test
NLD 202,931 (13,344) 37,761 (2,616) 68,994 (3,941)
ESP 207,484 (18,797) 51,645 (4,351) 52,098 (3,558)
ENG 204,567 (23,499) 51,578 (5,942) 46,666 (5,648)
RUS 66,333 (3,143) 8,819 (413) 7,771 (407)
CHE 98,355 (2,674) 12,265 (312) 11,933 (366)

Table 1: Name Tagging data set statistics: #token
and #name (between parentheses).

For POS Tagging, we use English, Dutch, Span-
ish, and Russian data from the CoNLL 2017
shared task (Zeman et al., 2017; Nivre et al.,
2017). In this data set, each token is annotated
with two POS tags, UPOS (universal POS tag) and
XPOS (language-specific POS tag). We use UPOS
because it is consistent throughout all languages.

3.2 Experimental Setup
We use 50-dimensional pre-trained word embed-
dings and 50-dimensional randomly initialized
character embeddings. We train word embeddings
using the word2vec package5. English, Span-

4https://tac.nist.gov/2017/KBP/data.html
5https://github.com/tmikolov/word2vec

ish, and Dutch embeddings are trained on corre-
sponding Wikipedia articles (2017-12-20 dumps).
Russian embeddings are trained on documents in
LDC2016E95. Chechen embeddings are trained
on documents in TAC KBP 2017 10-Language
EDL Pilot Evaluation Source Corpus. To learn a
mapping between mono-lingual word embeddings
and obtain cross-lingual embeddings, we use the
unsupervised model in the MUSE library6 (Con-
neau et al., 2017). Although word embeddings are
fine-tuned during training, we update the embed-
ding matrix in a sparse way and thus do not have
to update a large number of parameters.

We optimize parameters using Stochastic Gra-
dient Descent with momentum, gradient clipping
and exponential learning rate decay. At step t, the
learning rate αt is updated using αt = α0 ∗ ρt/T ,
where α0 is the initial learning rate, ρ is the decay
rate, and T is the decay step.7 To reduce overfit-
ting, we apply Dropout (Srivastava et al., 2014) to
the output of the LSTM layer.

We conduct hyper-parameter optimization by
exploring the space of parameters shown in Ta-
ble 2 using random search (Bergstra and Bengio,
2012). Due to time constraints, we only perform
parameter sweeping on the Dutch Name Tagging
task with 200 training examples. We select the set
of parameters that achieves the best performance
on the development set and apply it to all models.

Layer Range Final
CharCNN Filter Number [10, 30] 20
Highway Layer Number [1, 2] 2
Highway Activation Function ReLU, SeLU SeLU
LSTM Hidden State Size [50, 200] 171
LSTM Dropout Rate [0.3, 0.8] 0.6
Learning Rate [0.01, 0.2] 0.02
Batch Size [5, 25] 19

Table 2: Hyper-parameter search space.

3.3 Comparison of Different Models
In Figure 3, 4, and 5, we compare our model with
the mono-lingual single-task LSTM-CNNs model
(denoted as baseline), cross-task transfer model,
and cross-lingual transfer model in low-resource
settings with Dutch, Spanish, and Chechen Name
Tagging as the main task respectively. We use En-
glish as the related language for Dutch and Span-
ish, and use Russian as the related language for

6https://github.com/facebookresearch/MUSE
7Momentum β, gradient clipping threshold, ρ, and T are

set to 0.9, 5.0, 0.9, and 10000 in the experiments.

https://tac.nist.gov/2017/KBP/data.html
https://github.com/tmikolov/word2vec
https://github.com/facebookresearch/MUSE


804

Chechen. For cross-task transfer, we take POS
Tagging as the auxiliary task. Because the CoNLL
2017 data does not include Chechen, we only use
Russian POS Tagging and Russian Name Tagging
as auxiliary tasks for Chechen Name Tagging.

We take Name Tagging as the target task for
three reasons: (1) POS Tagging has a much lower
requirement for the amount of training data. For
example, using only 10 training sentences, our
baseline model achieves 75.5% and 82.9% pre-
diction accuracy on Dutch and Spanish; (2) Com-
pared to POS Tagging, Name Tagging has been
considered as a more challenging task; (3) Exist-
ing POS Tagging resources are relatively richer
than Name Tagging ones; e.g., the CoNLL 2017
data set provides POS Tagging training data for 45
languages. Name Tagging also has a higher anno-
tation cost as its annotation guidelines are usually
more complicated.

We can see that our model substantially outper-
forms the mono-lingual single-task baseline model
and obtains visible gains over single transfer mod-
els. When trained with less than 50 main tasks
training sentences, cross-lingual transfer consis-
tently surpasses cross-task transfer, which is not
surprising because in the latter scheme, the linear
layer and CRFs layer of the main model are not
shared with other models and thus cannot be fully
trained with little data.

Because there are only 20,400 sentences in
Chechen documents, we also experiment with
the data augmentation method described in Sec-
tion 2.2 by training word embeddings on a mix-
ture of Russian and Chechen data. This method
yields additional 3.5%-10.0% absolute F-score
gains. We also experiment with transferring from
English to Chechen. Because Chechen uses Cyril-
lic alphabet , we convert its data set to Latin script.
Surprisingly, although these two languages are not
close, we get more improvement by using English
as the auxiliary language.

In Table 3, we compare our model with state-of-
the-art models using all Dutch or Spanish Name
Tagging data. Results show that although we de-
sign this architecture for low-resource settings, it
also achieves good performance in high-resource
settings. In this experiment, with sufficient train-
ing data for the target task, we perform another
round of parameter sweeping. We increase the em-
bedding sizes and LSTM hidden state size to 100
and 225 respectively.

0 10 20 30 40 50 100 200
#Duch Name Tagging Training Sentences

0

10

20

30

40

50

60

70

F-
sc

or
e 

(%
)

Baseline
Cross-task
Cross-lingual
Our Model
Our Model*

Figure 3: Performance on Dutch Name Tagging.
We scale the horizontal axis to show more details
under 100 sentences. Our Model*: our model with
MUSE cross-lingual embeddings.

0 10 20 30 40 50 100 200
#Spanish Name Tagging Training Sentences

0

20

40

60

80

F-
sc

or
e 

(%
)

Baseline
Cross-task
Cross-lingual
Our Model
Our Model*

Figure 4: Performance on Spanish Name Tagging.

0 10 20 30 40 50 100 200
#Chechen Name Tagging Training Sentences

0

10

20

30

40

50

F-
sc

or
e 

(%
)

Baseline
Cross-lingual
Our Model
Our Model + Mixed Raw Data
Our Model (Auxiliary language: English)

Figure 5: Performance on Chechen Name Tag-
ging.

3.4 Qualitative Analysis

In Table 4, we compare Name Tagging results
from the baseline model and our model, both
trained with 100 main task sentences.

The first three examples show that shared
character-level networks can transfer different lev-
els of morphological and semantic information.



805

Language Model F-score
Dutch Gillick et al. (2016) 82.84

Lample et al. (2016) 81.74
Yang et al. (2017) 85.19
Baseline 85.14
Cross-task 85.69
Cross-lingual 85.71
Our Model 86.55

Spanish Gillick et al. (2016) 82.95
Lample et al. (2016) 85.75
Yang et al. (2017) 85.77
Baseline 85.44
Cross-task 85.37
Cross-lingual 85.02
Our Model 85.88

Table 3: Comparison with state-of-the-art models.

In example #1, the baseline model fails to iden-
tify “Palestijnen”, an unseen word in the Dutch
data, while our model can recognize it because
the shared CharCNN represents it in a way similar
to its corresponding English word “Palestinians”,
which occurs 20 times. In addition to mentions,
the shared CharCNN can also improve represen-
tations of context words, such as “staat” (state) in
the example. For some words dissimilar to corre-
sponding English words, the CharCNN may en-
hance their word representations by transferring
morpheme-level knowledge. For example, in sen-
tence #2, our model is able to identify “Rusland”
(Russia) as the suffix -land is usually associated
with location names in the English data; e.g., Fin-
land. Furthermore, the CharCNN is capable of
capturing some word-level patterns, such as capi-
talized hyphenated compound and acronym as ex-
ample #3 shows. In this sentence, neither “PMS-
centra” nor “MST” can be found in auxiliary task
data, while we observe a number of similar expres-
sions, such as American-style and LDP.

The transferred knowledge also helps reduce
overfitting. For example, in sentence #4, the
baseline model mistakenly tags “sección” (sec-
tion) and “consellerı́a” (department) as organiza-
tions because their capitalized forms usually ap-
pear in Spanish organization names. With knowl-
edge learned in auxiliary tasks that a lowercased
word is rarely tagged as a proper noun, our model
is able to avoid overfitting and correct these errors.
Sentence #5 shows an opposite situation, where
the capitalized word “campesinos” (farm worker)
never appears in Spanish names.

In Table 5, we show differences between cross-

lingual transfer and cross-task transfer. Although
the cross-task transfer model recognizes “Inge-
borg Marx” missed by the baseline model, it mis-
takenly assigns an S-PER tag to “Marx”. Instead,
from English Name Tagging, the cross-lingual
transfer model borrows task-specific knowledge
through the shared CRFs layer that (1) B-PER→S-
PER is an invalid transition, and (2) even if we as-
sign S-PER to “Ingeborg”, it is rare to have con-
tinuous person names without any conjunction or
punctuation. Thus, the cross-lingual model pro-
motes the sequence B-PER→E-PER.

In Figure 6, we depict the change of tag dis-
tribution with the number of training sentences.
When trained with less than 100 sentences, the
baseline model only correctly predicts a few tags
dominated by frequent types. By contrast, our
model has a visibly higher recall and better pre-
dicts infrequent tags, which can be attributed to
the implicit data augmentation and inductive bias
introduced by MTL (Ruder, 2017). For example,
if all location names in the Dutch training data
are single-token ones, the baseline model will in-
evitably overfit to the tag S-LOC and possibly la-
bel “Caldera de Taburiente” as [S-LOC Caldera]
[S-LOC de] [S-LOC Taburiente], whereas with the
shared CRFs layer fully trained on English Name
Tagging, our model prefers B-LOC→I-LOC→E-
LOC, which receives a higher transition score.

0 10 20 30 40 50 100 200 500 all
0

2k

4k

0 10 20 30 40 50 100 200 500 all
#Dutch Name Tagging Training Sentences

0

2k

4k

#C
or

re
ct

ly
 P

re
di

ct
ed

 T
ag

s Baseline

Our Model

Figure 6: The distribution of correctly predicted
tags on Dutch Name Tagging. The height of each
stack indicates the number of a certain tag.

3.5 Ablation Studies
In order to quantify the contributions of individ-
ual components, we conduct ablation studies on
Dutch Name Tagging with different numbers of
training sentences for the target task. For the ba-
sic model, we we use separate LSTM layers and



806

#1 [DUTCH]: If a Palestinian State is, however, the first thing the Palestinians will do.
⋆ [B] Als er een Palestijnse staat komt, is dat echter het eerste wat de Palestijnen zullen doen

⋆ [A] Als er een [S-MISC Palestijnse] staat komt, is dat echter het eerste wat de [S-MISC Palestijnen] zullen doen

#2 [DUTCH]: That also frustrates the Muscovites, who still live in the proud capital of Russia but can not look at the soaps
that the stupid farmers can see on the outside.
⋆ [B] Ook dat frustreert de Moskovieten , die toch in de fiere hoofdstad van Rusland wonen maar niet naar de soaps kunnen
kijken die de domme boeren op de buiten wel kunnen zien
⋆ [A] Ook dat frustreert de [S-MISC Moskovieten] , die toch in de fiere hoofdstad van [S-LOC Rusland] wonen maar niet
naar de soaps kunnen kijken die de domme boeren op de buiten wel kunnen zien
#3 [DUTCH]: And the PMS centers are merging with the centers for school supervision, the MSTs.
⋆ [B] En smelten de PMS-centra samen met de centra voor schooltoezicht, de MST’s .
⋆ [A] En smelten de [S-MISC PMS-centra] samen met de centra voor schooltoezicht, de [S-MISC MST’s] .

#4 [SPANISH]: The trade union section of CC.OO. in the Department of Justice has today denounced more attacks of students
to educators in centers dependent on this department ...
⋆ [B] La [B-ORG sección] [I-ORG sindical] [I-ORG de] [S-ORG CC.OO.] en el [B-ORG Departamento] [I-ORG de] [E-ORG

Justicia] ha denunciado hoy ms agresiones de alumnos a educadores en centros dependientes de esta [S-ORG consellerı́a]
...
⋆ [A] La sección sindical de [S-ORG CC.OO.] en el [B-ORG Departamento] [I-ORG de] [E-ORG Justicia] ha denunciado

hoy ms agresiones de alumnos a educadores en centros dependientes de esta consellerı́a ...
#5 [SPANISH]: ... and the Single Trade Union Confederation of Peasant Workers of Bolivia, agreed upon when the state of
siege was ended last month.
⋆ [B] ... y la [B-ORG Confederación] [I-ORG Sindical] [I-ORG Unica] [I-ORG de] [E-ORG Trabajadores] Campesinos de

[S-ORG Bolivia] , pactadas cuando se dio fin al estado de sitio, el mes pasado .

⋆ [A] .. y la [B-ORG Confederación] [I-ORG Sindical] [I-ORG Unica] [I-ORG de] [I-ORG Trabajadores] [I-ORG Campesinos]

[I-ORG de] [E-ORG Bolivia] , pactadas cuando se dio fin al estado de sitio, el mes pasado .

Table 4: Name Tagging results, each of which contains an English translation, result of the baseline
model (B), and result of our model (A). The GREEN ( RED ) highlight indicates a correct (incorrect) tag.

[DUTCH] ... Ingeborg Marx is her name, a formidable
heavy weight to high above her head!
⋆ [B] ... Zag ik zelfs onlangs niet dat een lief, mooi
vrouwtje, Ingeborg Marx is haar naam, een formidabel
zwaar gewicht tot hoog boven haar hoofd stak!
⋆ [CROSS-TASK] ... Zag ik zelfs onlangs niet dat een lief,
mooi vrouwtje, [B-PER Ingeborg] [S-PER Marx] is haar
naam, een formidabel zwaar gewicht tot hoog boven haar
hoofd stak!
⋆ [CROSS-LINGUAL] ... Zag ik zelfs onlangs niet dat een
lief, mooi vrouwtje, [B-PER Ingeborg] [E-PER Marx] is
haar naam, een formidabel zwaar gewicht tot hoog boven
haar hoofd stak!

Table 5: Comparing cross-task transfer and cross-
lingual transfer on Dutch Name Tagging with 100
training sentences.

remove the character embeddings, highway net-
works, language-specific layer, and Dropout layer.
As Table 6 shows, adding each component usu-
ally enhances the performance (F-score, %), while
the impact also depends on the size of the tar-
get task data. For example, the language-specific
layer slightly impairs the performance with only
10 training sentences. However, this is unsurpris-

ing as it introduces additional parameters that are
only trained by the target task data.

Model 0 10 100 200 All
Basic 2.06 20.03 47.98 51.52 77.63
+C 1.69 24.22 48.53 56.26 83.38
+CL 9.62 25.97 49.54 56.29 83.37
+CLS 3.21 25.43 50.67 56.34 84.02
+CLSH 7.70 30.48 53.73 58.09 84.68
+CLSHD 12.12 35.82 57.33 63.27 86.00

Table 6: Performance comparison between mod-
els with different components (C: character em-
bedding; L: shared LSTM; S: language-specific
layer; H: highway networks; D: dropout).

3.6 Effect of the Amount of Auxiliary Task
Data

For many low-resource languages, their related
languages are also low-resource. To evaluate our
model’s sensitivity to the amount of auxiliary task
data, we fix the size of main task data and down-
sample all auxiliary task data with sample rates
from 1% to 50%. As Figure 7 shows, the perfor-
mance goes up when we raise the sample rate from



807

1% to 20%. However, we do not observe signif-
icant improvement when we further increase the
sample rate. By comparing scores in Figure 3 and
Figure 7, we can see that using only 1% auxiliary
data, our model already obtains 3.7%-9.7% abso-
lute F-score gains. Due to space limitations, we
only show curves for Dutch Name Tagging, while
we observe similar results on other tasks. There-
fore, we may conclude that our model does not
heavily rely on the amount of auxiliary task data.

0 0.2 0.4 0.6 0.8 1
Sample Rate for Auxiliary Task Data

0

20

40

60

F-
sc

or
e 

(%
)

10 Training Sentences
50 Training Sentences
200 Training Sentences

Figure 7: The effect of the amount of auxiliary
task data on Dutch Name Tagging.

4 Related Work

Multi-task Learning has been applied in differ-
ent NLP areas, such as machine translation (Lu-
ong et al., 2016; Dong et al., 2015; Domhan
and Hieber, 2017), text classification (Liu et al.,
2017), dependency parsing (Peng et al., 2017),
textual entailment (Hashimoto et al., 2017), text
summarization (Isonuma et al., 2017) and se-
quence labeling (Collobert and Weston, 2008;
Søgaard and Goldberg, 2016; Rei, 2017; Peng and
Dredze, 2017; Yang et al., 2017; von Däniken and
Cieliebak, 2017; Aguilar et al., 2017; Liu et al.,
2018)

Collobert and Weston (2008) is an early attempt
that applies MTL to sequence labeling. The au-
thors train a CNN model jointly on POS Tag-
ging, Semantic Role Labeling, Name Tagging,
chunking, and language modeling using parame-
ter sharing. Instead of using other sequence la-
beling tasks, Rei (2017) and Liu et al. (2018)
take language modeling as the secondary train-
ing objective to extract semantic and syntactic
knowledge from large scale raw text without ad-
ditional supervision. In (Yang et al., 2017), the
authors propose three transfer models for cross-
domain, cross-application, and cross-lingual trans-

fer for sequence labeling, and also simulate a low-
resource setting by downsampling the training
data. By contrast, we combine cross-task trans-
fer and cross-lingual transfer within a unified ar-
chitecture to transfer different types of knowledge
from multiple auxiliary tasks simultaneously. In
addition, because our model is designed for low-
resource settings, we share components among
models in a different way (e.g., the LSTM layer
is shared across all models). Differing from most
MTL models, which perform supervisions for all
tasks on the outermost layer, (Søgaard and Gold-
berg, 2016) proposes an MTL model which super-
vised tasks at different levels. It shows that su-
pervising low-level tasks such as POS Tagging at
lower layer obtains better performance.

5 Conclusions and Future Work

We design a multi-lingual multi-task architecture
for low-resource settings. We evaluate the model
on sequence labeling tasks with three language
pairs. Experiments show that our model can ef-
fectively transfer different types of knowledge to
improve the main model. It substantially out-
performs the mono-lingual single-task baseline
model, cross-lingual transfer model, and cross-
task transfer model.

The next step of this research is to apply this
architecture to other types of tasks, such as Event
Extract and Semantic Role Labeling that involve
structure prediction. We also plan to explore the
possibility of integrating incremental learning into
this architecture to adapt a trained model for new
tasks rapidly.

Acknowledgments

This work was supported by the U.S. DARPA
LORELEI Program No. HR0011-15-C-0115 and
U.S. ARL NS-CTA No. W911NF-09-2-0053. The
views and conclusions contained in this document
are those of the authors and should not be inter-
preted as representing the official policies, either
expressed or implied, of the U.S. Government.
The U.S. Government is authorized to reproduce
and distribute reprints for Government purposes
notwithstanding any copyright notation here on.

References
Gustavo Aguilar, Suraj Maharjan, Adrian Pastor López

Monroy, and Thamar Solorio. 2017. A multi-task



808

approach for named entity recognition in social me-
dia data. In Proceedings of the 3rd Workshop on
Noisy User-generated Text.

James Bergstra and Yoshua Bengio. 2012. Random
search for hyper-parameter optimization. Journal of
Machine Learning Research, 13(Feb):281–305.

Jason P. C. Chiu and Eric Nichols. 2016. Named entity
recognition with bidirectional LSTM-CNNs. TACL,
4:357–370.

Ronan Collobert and Jason Weston. 2008. A unified
architecture for natural language processing: Deep
neural networks with multitask learning. In ICML.

Ronan Collobert, Jason Weston, Léon Bottou, Michael
Karlen, Koray Kavukcuoglu, and Pavel Kuksa.
2011. Natural language processing (almost) from
scratch. Journal of Machine Learning Research,
12(Aug):2493–2537.

Alexis Conneau, Guillaume Lample, Marc’Aurelio
Ranzato, Ludovic Denoyer, and Hervé Jégou. 2017.
Word translation without parallel data. arXiv
preprint arXiv:1710.04087.

Pius von Däniken and Mark Cieliebak. 2017. Transfer
learning and sentence level features for named en-
tity recognition on tweets. In Proceedings of the 3rd
Workshop on Noisy User-generated Text.

Tobias Domhan and Felix Hieber. 2017. Using target-
side monolingual data for neural machine translation
through multi-task learning. In EMNLP.

Daxiang Dong, Hua Wu, Wei He, Dianhai Yu, and
Haifeng Wang. 2015. Multi-task learning for mul-
tiple language translation. In ACL.

Dan Gillick, Cliff Brunk, Oriol Vinyals, and Amarnag
Subramanya. 2016. Multilingual language process-
ing from bytes. In NAACL HLT.

Kazuma Hashimoto, Yoshimasa Tsuruoka, Richard
Socher, et al. 2017. A joint many-task model: Grow-
ing a neural network for multiple nlp tasks. In
EMNLP.

Masaru Isonuma, Toru Fujino, Junichiro Mori, Yutaka
Matsuo, and Ichiro Sakata. 2017. Extractive sum-
marization using multi-task learning with document
classification. In EMNLP.

Yoon Kim, Yacine Jernite, David Sontag, and Alexan-
der M Rush. 2016. Character-aware neural language
models. In AAAI.

John D. Lafferty, Andrew McCallum, and Fernando
C. N. Pereira. 2001. Conditional random fields:
Probabilistic models for segmenting and labeling se-
quence data. In ICML.

Guillaume Lample, Miguel Ballesteros, Sandeep Sub-
ramanian, Kazuya Kawakami, and Chris Dyer. 2016.
Neural architectures for named entity recognition.
In NAACL HLT.

Liyuan Liu, Jingbo Shang, Frank Xu, Xiang Ren, Huan
Gui, Jian Peng, and Jiawei Han. 2018. Empower
sequence labeling with task-aware neural language
model. In AAAI.

Pengfei Liu, Xipeng Qiu, and Xuanjing Huang. 2017.
Adversarial multi-task learning for text classifica-
tion. In ACL.

Minh-Thang Luong, Quoc V Le, Ilya Sutskever, Oriol
Vinyals, and Lukasz Kaiser. 2016. Multi-task se-
quence to sequence learning. In ICLR.

Xuezhe Ma and Eduard Hovy. 2016. End-to-end
sequence labeling via bi-directional LSTM-CNNs-
CRF. In ACL.

Joakim Nivre, Željko Agić, Lars Ahrenberg, Lene
Antonsen, Maria Jesus Aranzabe, Masayuki Asa-
hara, Luma Ateyah, Mohammed Attia, Aitz-
iber Atutxa, Liesbeth Augustinus, Elena Bad-
maeva, Miguel Ballesteros, Esha Banerjee, Sebas-
tian Bank, Verginica Barbu Mititelu, John Bauer,
Kepa Bengoetxea, Riyaz Ahmad Bhat, Eckhard
Bick, Victoria Bobicev, Carl Börstell, Cristina
Bosco, Gosse Bouma, Sam Bowman, Aljoscha Bur-
chardt, Marie Candito, Gauthier Caron, Gülşen
Cebiroğlu Eryiğit, Giuseppe G. A. Celano, Savas
Cetin, Fabricio Chalub, Jinho Choi, Silvie Cinková,
Çağrı Çöltekin, Miriam Connor, Elizabeth David-
son, Marie-Catherine de Marneffe, Valeria de Paiva,
Arantza Diaz de Ilarraza, Peter Dirix, Kaja Do-
brovoljc, Timothy Dozat, Kira Droganova, Puneet
Dwivedi, Marhaba Eli, Ali Elkahky, Tomaž Erjavec,
Richárd Farkas, Hector Fernandez Alcalde, Jennifer
Foster, Cláudia Freitas, Katarı́na Gajdošová, Daniel
Galbraith, Marcos Garcia, Moa Gärdenfors, Kim
Gerdes, Filip Ginter, Iakes Goenaga, Koldo Go-
jenola, Memduh Gökırmak, Yoav Goldberg, Xavier
Gómez Guinovart, Berta Gonzáles Saavedra, Ma-
tias Grioni, Normunds Grūzı̄tis, Bruno Guillaume,
Nizar Habash, Jan Hajič, Jan Hajič jr., Linh Hà Mỹ,
Kim Harris, Dag Haug, Barbora Hladká, Jaroslava
Hlaváčová, Florinel Hociung, Petter Hohle, Radu
Ion, Elena Irimia, Tomáš Jelı́nek, Anders Jo-
hannsen, Fredrik Jørgensen, Hüner Kaşıkara, Hi-
roshi Kanayama, Jenna Kanerva, Tolga Kayade-
len, Václava Kettnerová, Jesse Kirchner, Na-
talia Kotsyba, Simon Krek, Veronika Laippala,
Lorenzo Lambertino, Tatiana Lando, John Lee,
Phng Lê H`ông, Alessandro Lenci, Saran Lertpra-
dit, Herman Leung, Cheuk Ying Li, Josie Li, Key-
ing Li, Nikola Ljubešić, Olga Loginova, Olga Lya-
shevskaya, Teresa Lynn, Vivien Macketanz, Aibek
Makazhanov, Michael Mandl, Christopher Man-
ning, Cătălina Mărănduc, David Mareček, Katrin
Marheinecke, Héctor Martı́nez Alonso, André Mar-
tins, Jan Mašek, Yuji Matsumoto, Ryan McDon-
ald, Gustavo Mendonça, Niko Miekka, Anna Mis-
silä, Cătălin Mititelu, Yusuke Miyao, Simonetta
Montemagni, Amir More, Laura Moreno Romero,
Shinsuke Mori, Bohdan Moskalevskyi, Kadri
Muischnek, Kaili Müürisep, Pinkey Nainwani,
Anna Nedoluzhko, Gunta Nešpore-Bērzkalne, Lng



809

Nguy˜ên Thi., Huy`ên Nguy˜ên Thi. Minh, Vitaly
Nikolaev, Hanna Nurmi, Stina Ojala, Petya Osen-
ova, Robert Östling, Lilja Øvrelid, Elena Pascual,
Marco Passarotti, Cenel-Augusto Perez, Guy Per-
rier, Slav Petrov, Jussi Piitulainen, Emily Pitler,
Barbara Plank, Martin Popel, Lauma Pretkalniņa,
Prokopis Prokopidis, Tiina Puolakainen, Sampo
Pyysalo, Alexandre Rademaker, Loganathan Ra-
masamy, Taraka Rama, Vinit Ravishankar, Livy
Real, Siva Reddy, Georg Rehm, Larissa Rinaldi,
Laura Rituma, Mykhailo Romanenko, Rudolf Rosa,
Davide Rovati, Benoı̂t Sagot, Shadi Saleh, Tanja
Samardžić, Manuela Sanguinetti, Baiba Saulı̄te, Se-
bastian Schuster, Djamé Seddah, Wolfgang Seeker,
Mojgan Seraji, Mo Shen, Atsuko Shimada, Dmitry
Sichinava, Natalia Silveira, Maria Simi, Radu
Simionescu, Katalin Simkó, Mária Šimková, Kiril
Simov, Aaron Smith, Antonio Stella, Milan Straka,
Jana Strnadová, Alane Suhr, Umut Sulubacak,
Zsolt Szántó, Dima Taji, Takaaki Tanaka, Trond
Trosterud, Anna Trukhina, Reut Tsarfaty, Francis
Tyers, Sumire Uematsu, Zdeňka Urešová, Larraitz
Uria, Hans Uszkoreit, Sowmya Vajjala, Daniel van
Niekerk, Gertjan van Noord, Viktor Varga, Eric
Villemonte de la Clergerie, Veronika Vincze, Lars
Wallin, Jonathan North Washington, Mats Wirén,
Tak-sum Wong, Zhuoran Yu, Zdeněk Žabokrtský,
Amir Zeldes, Daniel Zeman, and Hanzhi Zhu. 2017.
Universal dependencies 2.1. LINDAT/CLARIN
digital library at the Institute of Formal and Ap-
plied Linguistics (ÚFAL), Faculty of Mathematics
and Physics, Charles University.

Alexandre Passos, Vineet Kumar, and Andrew McCal-
lum. 2014. Lexicon infused phrase embeddings for
named entity resolution. In CoNLL.

Hao Peng, Sam Thomson, and Noah A Smith. 2017.
Deep multitask learning for semantic dependency
parsing. In ACL.

Nanyun Peng and Mark Dredze. 2017. Multi-task do-
main adaptation for sequence tagging. In Proceed-
ings of the 2nd Workshop on Representation Learn-
ing for NLP.

Lev Ratinov and Dan Roth. 2009. Design challenges
and misconceptions in named entity recognition. In
CoNLL.

Marek Rei. 2017. Semi-supervised multitask learning
for sequence labeling. In ACL.

Sebastian Ruder. 2017. An overview of multi-task
learning in deep neural networks. arXiv preprint
arXiv:1706.05098.

Anders Søgaard and Yoav Goldberg. 2016. Deep
multi-task learning with low level tasks supervised
at lower layers. In ACL.

Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky,
Ilya Sutskever, and Ruslan Salakhutdinov. 2014.
Dropout: A simple way to prevent neural networks
from overfitting. The Journal of Machine Learning
Research, 15(1):1929–1958.

Rupesh Kumar Srivastava, Klaus Greff, and Jürgen
Schmidhuber. 2015. Highway networks. ICML.

Erik F. Tjong Kim Sang. 2002. Introduction to the
CoNLL-2002 shared task: Language-independent
named entity recognition. In COLING.

Erik F. Tjong Kim Sang and Fien De Meulder.
2003. Introduction to the CoNLL-2003 shared task:
Language-independent named entity recognition. In
NAACL HLT.

Zhilin Yang, Ruslan Salakhutdinov, and William W
Cohen. 2017. Transfer learning for sequence tag-
ging with hierarchical recurrent networks. In ICLR.

Daniel Zeman, Martin Popel, Milan Straka, Jan Ha-
jic, Joakim Nivre, Filip Ginter, Juhani Luotolahti,
Sampo Pyysalo, Slav Petrov, Martin Potthast, et al.
2017. CoNLL 2017 shared task: multilingual pars-
ing from raw text to universal dependencies. In
CoNLL.

http://hdl.handle.net/11234/1-2515

