



















































Summary Cloze: A New Task for Content Selection in Topic-Focused Summarization


Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing
and the 9th International Joint Conference on Natural Language Processing, pages 3720–3729,
Hong Kong, China, November 3–7, 2019. c©2019 Association for Computational Linguistics

3720

Summary Cloze: A New Task for Content Selection in Topic-Focused
Summarization

Daniel Deutsch and Dan Roth
Department of Computer and Information Science

University of Pennsylvania
{ddeutsch,danroth}@seas.upenn.edu

Abstract

A key challenge in topic-focused summariza-
tion is determining what information should
be included in the summary, a problem known
as content selection. In this work, we pro-
pose a new method for studying content se-
lection in topic-focused summarization called
the summary cloze task. The goal of the sum-
mary cloze task is to generate the next sen-
tence of a summary conditioned on the begin-
ning of the summary, a topic, and a reference
document(s). The main challenge is deciding
what information in the references is relevant
to the topic and partial summary and should
be included in the summary. Although the
cloze task does not address all aspects of the
traditional summarization problem, the more
narrow scope of the task allows us to collect
a large-scale datset of nearly 500k summary
cloze instances from Wikipedia. We report ex-
perimental results on this new dataset using
various extractive models and a two-step ab-
stractive model that first extractively selects a
small number of sentences and then abstrac-
tively summarizes them. Our results show that
the topic and partial summary help the models
identify relevant content, but the task remains
a significant challenge.

1 Introduction

Topic-focused multi-document summarization
(MDS) has long been a goal for natural language
processing (Dang, 2005, 2006). In contrast to
generic summarization, topic-focused summa-
rization systems attempt to summarize a set of
reference documents with respect to a specific
topic or information need.

Recent research on summarization has mostly
focused on generic summarization, in part due
to the size of the available datasets. Work on
topic-focused summarization (Lin and Bilmes,
2011; Ma et al., 2016; Feigenblat et al., 2017) is

… Federer received an honorary doctorate 
awarded to him by his home university, the 
University of Basel.   He received the title in 
recognition for his role in increasing the 
international reputation of Basel and 
Switzerland, and also his engagement for 
children in Africa through his charitable 
foundation.

Roger Federer, Philanthropy and outreach

The current world number two tennis player, 
who was born in Basel, received the title 
from the medical faculty, which praised him 
for his role in increasing the international 
reputation of Basel and Switzerland…In a 
press release it also highlighted the 36-year-
old sportsman's engagement for children in 
Africa through his charitable foundation…

to
pi

c
re

fe
re

nc
e 

do
cu

m
en

t
co

nt
ex

t
cl

oz
e

input
output

Figure 1: Given a topic, reference document, and a par-
tial summary (the context), the objective of the sum-
mary cloze task is to predict the next sentence of the
summary, known as the cloze.

largely based on the DUC 2005 and 2006 datasets
(Dang, 2005, 2006), which are orders of magni-
tude smaller than comparable generic summariza-
tion datasets (Nallapati et al., 2016; Grusky et al.,
2018). Because the available corpora are so small,
it is difficult to use them to train recent state-
of-the-art summarization systems, which require
large amounts of training data.

Instead of focusing on the full topic-focused
MDS problem, we address one aspect of the task
known as content selection, the problem of decid-
ing what information should be included in the
summary (Nenkova, 2006b). Narrowing the scope
of the problem makes it easier to collect a large-
scale dataset that is tailored to the specific task and
thus build upon recent work on summarization.

To that end, we formulate a new task for content
selection in topic-focused summarization which



3721

we call the summary cloze task (§2). Given a topic,
a partial summary, and a reference document(s),
the goal of the summary cloze task is to produce
the next sentence in the summary (referred to as
the cloze). The underlying assumption of the task
is that the content of the cloze is known to come
from the reference document, and thus the primary
challenge of the task is to decide what informa-
tion in the reference document(s) is relevant to the
topic and partial summary. An example instance
of the summary cloze task is presented in Figure 1.

Then, we collected a new large-scale summary
cloze dataset from Wikipedia, called the WIKI-
CITE dataset (§3). Each paragraph in Wikipedia
can be viewed as a topic-focused summary of the
references cited within the paragraph, where the
topic is defined as the article title and section
headings. The citations provide supervision at the
sentence-level that indicates where the content of
the preceding sentence came from. We scraped
hundreds of thousands of Wikipedia articles and
corresponding references to collect nearly 500k
summary cloze instances.1

With a new task definition and corresponding
large-scale dataset, we then propose an extrac-
tive model and a two-step abstractive model, both
of which are based on recent successful work on
generic summarization (§4). The extractive model
combines representations of the topic and partial
summary with representations of the document
sentences through an attention mechanism to ex-
tract one reference sentence. The two-step model
first reduces the length of the input data by ex-
tractively selecting a small number of sentences,
then abstractively summarizes them using a de-
coder that has an initial hidden state which de-
pends on the partial summary. Our experimental
results (§6) show that the topic and context help
the models identify relevant information, but the
task remains a significant challenge.

The contributions of this work are three-fold:
(1) We formalize the summary cloze task as
a method to develop content selection models
for topic-focused summarization; (2) we release
the WIKICITE dataset, which contains nearly
500k summary cloze instances collected from
Wikipedia; and (3) we propose sensible baseline
extractive and abstractive models for content se-
lection in topic-focused summarization.

1Available for download at http://cogcomp.org/
page/publication_view/880

2 The Summary Cloze Task

One of the main challenges for a topic-focused
MDS system is selecting what content from the
reference documents to include in the summary.
Intuitively, this decision largely depends on the
topic itself and the content of the summary gener-
ated thus far. Therefore, being able to predict the
next sentence of a summary is an important com-
ponent of a full topic-focused MDS system and
a worthwhile task on its own. With this motiva-
tion in mind, we now formally define the summary
cloze task.

Task Definition Given a partial summary (the
context) and a reference document or documents
(the references), the goal of the summary cloze
task is to generate the next sentence of the sum-
mary (the cloze). It is assumed that the informa-
tion in the cloze comes from the provided refer-
ence documents. This is the most general defini-
tion of the summary cloze task. However, because
we are interested in topic-focused summarization,
in this work, it is also assumed that a high-level
topic is provided as input, such as in Figure 1.

One advantage that the summary cloze task has
over the traditional topic-focused MDS problem
is the availability of data. Topic-focused MDS re-
quires collecting example summaries of a set of
documents about a particular topic. This data is
hard to collect from human annotators and is dif-
ficult to find occurring naturally. The cloze task
requires sentence-level supervision for which we
were able to collect a large-scale dataset using
Wikipedia (§3). By conditioning the cloze gen-
eration on a partial summary and working at the
sentence-level, we are able to get around the prob-
lem of data scarcity.

Evaluation Evaluating summary cloze models
is challenging; The difficulties of evaluating sum-
marization systems (Nenkova, 2006a) still persist
for the summary cloze task. We propose for the
main evaluation metric to be ROUGE (Lin, 2004),
which has been shown to be an effective metric
for summarization (Owczarzak et al., 2012). Ad-
ditionally, for abstractive models we use perplex-
ity to quantify how likely the ground-truth cloze is
according to the model.

Because the cloze is only one sentence long, the
chance of the ground-truth and model clozes be-
ing equally valid but very different to each other is
likely higher than for other summarization tasks.

http://cogcomp.org/page/publication_view/880
http://cogcomp.org/page/publication_view/880


3722

It is desirable to have a good automatic evalua-
tion metric that could capture the coherence of the
model’s cloze with the context in such situations.
However, developing such an automatic metric for
measuring summary coherence that correlates well
with human judgments is an open problem for
summarization in general, and thus we leave it for
future work.

3 The WikiCite Dataset

In order to build content selection models, we col-
lected a large-scale dataset of summary cloze in-
stances using Wikipedia.

Wikipedia as a Summary Each paragraph in a
Wikipedia article can be viewed as a topic-focused
multi-document summary. Many Wikipedia arti-
cles have citations to external sources that provide
supporting evidence for the information within the
article. The articles’ paragraphs can be viewed
as summarizing the documents cited within the
paragraph with respect to the page entity and
section headings, which together form the topic
of the summary. Since the citations are at the
sentence-level, they provide supervision that iden-
tifies where the information in the preceding sen-
tence came from, and thus can be used to generate
summary cloze instances.

For example, consider a paragraph from the
“Family and personal life” section of Barack
Obama’s Wikipedia page, presented in Figure 2.
Nearly all of the sentences in the paragraph are
supported with citations, and these citations con-
tain the same information written in the paragraph.
Each sentence with a citation can be used to pro-
duce a summary cloze instance where the pre-
ceding sentences are the context and the topic is
“Barack Obama, Family and personal life.” Al-
though not all paragraphs on Wikipedia are as
well-cited as the example in Figure 2, only one
citation is necessary to create a cloze instance.

Dataset Collection To collect a summary cloze
dataset, we first parsed the articles and citations
from the January 2019 Wikipedia dump. Only ar-
ticles marked with the category “Living people”
were kept because these articles are more likely to
have reference documents which can be scraped
automatically. The reference document HTMLs
were collected from 12 months of Common Crawl
data.2 Then, the text of each document was ex-

2http://commoncrawl.org/

In June 1989, Obama met Michelle Robinson when he was 
employed as a summer associate at the Chicago law firm 
of Sidley Austin.[62] Robinson was assigned for three 
months as Obama's adviser at the firm, and she joined him 
at several group social functions but declined his initial 
requests to date.[63] They began dating later that summer, 
became engaged in 1991, and were married on October 3, 
1992.[64] The couple's first daughter, Malia Ann, was born in 
1998,[65] followed by a second daughter, Natasha ("Sasha"), 
in 2001.[66] The Obama daughters attended the University 
of Chicago Laboratory Schools. When they moved to 
Washington, D.C., in January 2009, the girls started at 
the Sidwell Friends School.[67] The Obamas have 
two Portuguese Water Dogs; the first, a male named Bo, 
was a gift from Senator Ted Kennedy.[68] In 2013, Bo was 
joined by Sunny, a female.[69]

62: [Michelle] Obama was assigned to mentor Barack, who 
was a summer intern at Sidley & Austin…

68: … the Obamas have chosen a 6-month-old Portuguese 
water dog — a gift from Senator Edward M. Kennedy…
The girls have named the dog Bo…

67: … Barack Obama and his wife have chosen Sidwell 
Friends School for their two daughters

Figure 2: A paragraph from the “Family and personal
life” section of Barack Obama’s Wikipedia page and
selected excerpts from the cited documents which pro-
vide supporting evidence.

tracted from the HTML using unfluff.3

In order to filter low-quality instances (e.g.,
HTML was a 404 page, the body text was incor-
rectly extracted, etc.), we labeled around 500 in-
stances based on whether or not the reference doc-
ument provided supporting evidence for the corre-
sponding cloze. We trained a simple linear classi-
fier with length and BM25 features that achieved
an F1 score of 85 to filter the data. The remain-
ing data was split between training, 10k valida-
tion, and 10k testing instances, ensuring that no
reference document or Wikipedia article appears
in more than one set.

The dataset statistics compared to those of the
DUC 2005 and 2006 datasets are presented in Ta-
ble 1. Other than the number of instances, the
biggest differences between the two datasets are
the length of the reference documents and top-
ics. The WIKICITE documents are significantly
longer, over double the length DUC documents at
one standard deviation above the mean. The DUC
topics, which are generally text that describe an
information need, are much longer than the WIKI-
CITE topics, which are high-level topic keywords.

Examples from the dataset are provided in Ap-
pendix A.

3https://github.com/ageitgey/
node-unfluff

http://commoncrawl.org/
https://github.com/ageitgey/node-unfluff
https://github.com/ageitgey/node-unfluff


3723

Statistic WikiCite DUC 2005 +DUC 2006

#Instances 486,015 100
#Document Tokens 1083.2 (2105.0) 723.3 (629.0)
#Document Sents 51.4 (137.1) 30.7 (30.1)
#Topic Tokens 5.7 (3.4) 29.5 (11.3)
#Context Tokens 65.5 (56.1) -
#Context Sents 2.8 (2.3) -
#Cloze/Summ. Tokens 24.6 (12.0) 277.8 (19.9)
#Cloze/Summ. Sents 1 13.5 (3.4)

Table 1: The WIKICITE and DUC 2005 and 2006
dataset statistics. The average values are reported with
standard deviations in parentheses.

4 Content Selection Models

We propose two sensible extensions to successful
single-document summarization systems for the
summary cloze task, an extractive model and a
two-step abstractive model.

4.1 Extractive Model

The extractive model is based on those presented
in Kedzie et al. (2018). The base model works
as follows. For every sentence xi in the refer-
ence document, a sentence-level representation hi
is created by a sentence encoder (e.g., by averag-
ing word vectors, by using a recurrent neural net-
work, etc.). Then, a sentence extractor creates a
document-level representation for each sentence
di, for example, by running a second RNN over
the sentence-level representations. The document-
level representation is passed through a multi-
layer perceptron and a sigmoid unit to calculate
the probability of extracting a particular sentence
under the model.

Adding Topics and Context We extended the
standard extractive model by incorporating the
topic and context information through an attention
mechanism. First, each of the individual topics is
encoded into a vector representation tj by aver-
aging the word vectors for each word in topic j.
Then, a bidirectional RNN encodes the context to-
kens into a vector representation ck for each token.

The topic and context token representations
are combined with the sentence-level sentence
representations through an attention mechanism.
Specifically, an attention score is computed be-
tween each hi and the topic and context represen-
tations. Then, the subsequent attention scores are
used to create a weighted sum of the topic and con-
text representations, ai.

Finally, the sentence representation h̃i that in-
cludes the attention mechanism is computed as

h̃i = Wa[hi;ai] (1)

where Wa is a matrix of learned parameters.
In this extended version of the model, h̃i is used

to compute the extraction probabilities instead of
hi. A graphical representation of the full extrac-
tive model is presented in Figure 3.

Training & Inference To convert the abstrac-
tive clozes into extractive reference sentence la-
bels, we follow the procedure of Nallapati et al.
(2017) and greedily select reference sentences that
maximize ROUGE-1 recall until no more sen-
tences improve the score. The training objective is
the weighted negative log-likelihood of the labels
with the same weighting as Kedzie et al. (2018).
At inference, the sentence with the highest proba-
bility of extraction is selected.

4.2 Two-Step Abstractive Model
Since the reference documents in the WIKICITE
dataset are rather long (see Table 1), we chose
to use a two-step approach to build an abstractive
system for the summary cloze task.

Extractive Step The first step uses the same ex-
tractive model from the previous subsection to sig-
nificantly reduce the amount of input text. Instead
of selecting just one sentence at inference, the ex-
tractive model repeatedly selects sentences until
a pre-specified number of words has been met.
Then, only the sentences which were extracted are
passed as input to the abstractive step.

Abstractive Step For the abstractive step, we
extended the Pointer-Generator + Coverage net-
work (See et al., 2017) for single-document sum-
marization to include the context.

The Pointer-Generator network is built on a
sequence-to-sequence model with attention. The
reference document is encoded using an RNN, and
the summary is produced using a second RNN.
The model is augmented with a copy mechanism
by including a soft switch that allows the atten-
tion distribution to influence the decoder’s proba-
bility distribution over the vocabulary, thus mak-
ing it easier to copy words from the input.

Then, the coverage mechanism discourages the
attention weights from repeatedly assigning high
values to the same input tokens across decoding
time steps. This is accomplished by adding a new



3724

Those left scratching their heads …

At the end of the new spot …

The second in Microsoft's …
…

In
2008,
Gates

appeared
in
a

series
 of

ads
…

The second in Microsoft's series of 
new ads airs Thursday night, 
featuring Bill Gates and Jerry Seinfeld 
moving in with a family of "real 
people" in order to connect with 
them. At the end of the new spot, 
Seinfeld again asks Gates to give him 
a sign if he's on the right track in 
guessing what's next…

Sentence Representation,Sentence Encoder

Context Encoder Context Token Representation,

Sentence Representation with Attention,

Topic Representation,

Document-level Encoder Document-Level Sentence Representation,
Bill Gates Appearance

in ads

Topic Encoder

Extraction Probability

hi
<latexit sha1_base64="+6kkaep2Lb0B/hCvTkKPhsiIVAI=">AAAB83icbVBNS8NAFHypX7V+VT16WSyCp5KIoMeiF48VbC00oWy2m3bpZhN2X4QS+je8eFDEq3/Gm//GTZuDtg4sDDPv8WYnTKUw6LrfTmVtfWNzq7pd29nd2z+oHx51TZJpxjsskYnuhdRwKRTvoEDJe6nmNA4lfwwnt4X/+MS1EYl6wGnKg5iOlIgEo2gl348pjsMoH88GYlBvuE13DrJKvJI0oER7UP/yhwnLYq6QSWpM33NTDHKqUTDJZzU/MzylbEJHvG+pojE3QT7PPCNnVhmSKNH2KSRz9fdGTmNjpnFoJ4uMZtkrxP+8fobRdZALlWbIFVscijJJMCFFAWQoNGcop5ZQpoXNStiYasrQ1lSzJXjLX14l3Yum5za9+8tG66asowoncArn4MEVtOAO2tABBik8wyu8OZnz4rw7H4vRilPuHMMfOJ8/ZtCR6A==</latexit><latexit sha1_base64="+6kkaep2Lb0B/hCvTkKPhsiIVAI=">AAAB83icbVBNS8NAFHypX7V+VT16WSyCp5KIoMeiF48VbC00oWy2m3bpZhN2X4QS+je8eFDEq3/Gm//GTZuDtg4sDDPv8WYnTKUw6LrfTmVtfWNzq7pd29nd2z+oHx51TZJpxjsskYnuhdRwKRTvoEDJe6nmNA4lfwwnt4X/+MS1EYl6wGnKg5iOlIgEo2gl348pjsMoH88GYlBvuE13DrJKvJI0oER7UP/yhwnLYq6QSWpM33NTDHKqUTDJZzU/MzylbEJHvG+pojE3QT7PPCNnVhmSKNH2KSRz9fdGTmNjpnFoJ4uMZtkrxP+8fobRdZALlWbIFVscijJJMCFFAWQoNGcop5ZQpoXNStiYasrQ1lSzJXjLX14l3Yum5za9+8tG66asowoncArn4MEVtOAO2tABBik8wyu8OZnz4rw7H4vRilPuHMMfOJ8/ZtCR6A==</latexit><latexit sha1_base64="+6kkaep2Lb0B/hCvTkKPhsiIVAI=">AAAB83icbVBNS8NAFHypX7V+VT16WSyCp5KIoMeiF48VbC00oWy2m3bpZhN2X4QS+je8eFDEq3/Gm//GTZuDtg4sDDPv8WYnTKUw6LrfTmVtfWNzq7pd29nd2z+oHx51TZJpxjsskYnuhdRwKRTvoEDJe6nmNA4lfwwnt4X/+MS1EYl6wGnKg5iOlIgEo2gl348pjsMoH88GYlBvuE13DrJKvJI0oER7UP/yhwnLYq6QSWpM33NTDHKqUTDJZzU/MzylbEJHvG+pojE3QT7PPCNnVhmSKNH2KSRz9fdGTmNjpnFoJ4uMZtkrxP+8fobRdZALlWbIFVscijJJMCFFAWQoNGcop5ZQpoXNStiYasrQ1lSzJXjLX14l3Yum5za9+8tG66asowoncArn4MEVtOAO2tABBik8wyu8OZnz4rw7H4vRilPuHMMfOJ8/ZtCR6A==</latexit><latexit sha1_base64="+6kkaep2Lb0B/hCvTkKPhsiIVAI=">AAAB83icbVBNS8NAFHypX7V+VT16WSyCp5KIoMeiF48VbC00oWy2m3bpZhN2X4QS+je8eFDEq3/Gm//GTZuDtg4sDDPv8WYnTKUw6LrfTmVtfWNzq7pd29nd2z+oHx51TZJpxjsskYnuhdRwKRTvoEDJe6nmNA4lfwwnt4X/+MS1EYl6wGnKg5iOlIgEo2gl348pjsMoH88GYlBvuE13DrJKvJI0oER7UP/yhwnLYq6QSWpM33NTDHKqUTDJZzU/MzylbEJHvG+pojE3QT7PPCNnVhmSKNH2KSRz9fdGTmNjpnFoJ4uMZtkrxP+8fobRdZALlWbIFVscijJJMCFFAWQoNGcop5ZQpoXNStiYasrQ1lSzJXjLX14l3Yum5za9+8tG66asowoncArn4MEVtOAO2tABBik8wyu8OZnz4rw7H4vRilPuHMMfOJ8/ZtCR6A==</latexit>

tj
<latexit sha1_base64="0QWFXCSUEAKXeS2U7XNlLPhZp7E=">AAAB83icbVDLSsNAFL2pr1pfVZduBovgqiQi6LLoxmUF+4CmlMl00o6dTMLMjVBCf8ONC0Xc+jPu/BsnbRbaemDgcM693DMnSKQw6LrfTmltfWNzq7xd2dnd2z+oHh61TZxqxlsslrHuBtRwKRRvoUDJu4nmNAok7wST29zvPHFtRKwecJrwfkRHSoSCUbSS70cUx0GY4WzwOKjW3Lo7B1klXkFqUKA5qH75w5ilEVfIJDWm57kJ9jOqUTDJZxU/NTyhbEJHvGepohE3/WyeeUbOrDIkYaztU0jm6u+NjEbGTKPATuYZzbKXi/95vRTD634mVJIiV2xxKEwlwZjkBZCh0JyhnFpCmRY2K2FjqilDW1PFluAtf3mVtC/qnlv37i9rjZuijjKcwCmcgwdX0IA7aEILGCTwDK/w5qTOi/PufCxGS06xcwx/4Hz+AHqokfU=</latexit><latexit sha1_base64="0QWFXCSUEAKXeS2U7XNlLPhZp7E=">AAAB83icbVDLSsNAFL2pr1pfVZduBovgqiQi6LLoxmUF+4CmlMl00o6dTMLMjVBCf8ONC0Xc+jPu/BsnbRbaemDgcM693DMnSKQw6LrfTmltfWNzq7xd2dnd2z+oHh61TZxqxlsslrHuBtRwKRRvoUDJu4nmNAok7wST29zvPHFtRKwecJrwfkRHSoSCUbSS70cUx0GY4WzwOKjW3Lo7B1klXkFqUKA5qH75w5ilEVfIJDWm57kJ9jOqUTDJZxU/NTyhbEJHvGepohE3/WyeeUbOrDIkYaztU0jm6u+NjEbGTKPATuYZzbKXi/95vRTD634mVJIiV2xxKEwlwZjkBZCh0JyhnFpCmRY2K2FjqilDW1PFluAtf3mVtC/qnlv37i9rjZuijjKcwCmcgwdX0IA7aEILGCTwDK/w5qTOi/PufCxGS06xcwx/4Hz+AHqokfU=</latexit><latexit sha1_base64="0QWFXCSUEAKXeS2U7XNlLPhZp7E=">AAAB83icbVDLSsNAFL2pr1pfVZduBovgqiQi6LLoxmUF+4CmlMl00o6dTMLMjVBCf8ONC0Xc+jPu/BsnbRbaemDgcM693DMnSKQw6LrfTmltfWNzq7xd2dnd2z+oHh61TZxqxlsslrHuBtRwKRRvoUDJu4nmNAok7wST29zvPHFtRKwecJrwfkRHSoSCUbSS70cUx0GY4WzwOKjW3Lo7B1klXkFqUKA5qH75w5ilEVfIJDWm57kJ9jOqUTDJZxU/NTyhbEJHvGepohE3/WyeeUbOrDIkYaztU0jm6u+NjEbGTKPATuYZzbKXi/95vRTD634mVJIiV2xxKEwlwZjkBZCh0JyhnFpCmRY2K2FjqilDW1PFluAtf3mVtC/qnlv37i9rjZuijjKcwCmcgwdX0IA7aEILGCTwDK/w5qTOi/PufCxGS06xcwx/4Hz+AHqokfU=</latexit><latexit sha1_base64="0QWFXCSUEAKXeS2U7XNlLPhZp7E=">AAAB83icbVDLSsNAFL2pr1pfVZduBovgqiQi6LLoxmUF+4CmlMl00o6dTMLMjVBCf8ONC0Xc+jPu/BsnbRbaemDgcM693DMnSKQw6LrfTmltfWNzq7xd2dnd2z+oHh61TZxqxlsslrHuBtRwKRRvoUDJu4nmNAok7wST29zvPHFtRKwecJrwfkRHSoSCUbSS70cUx0GY4WzwOKjW3Lo7B1klXkFqUKA5qH75w5ilEVfIJDWm57kJ9jOqUTDJZxU/NTyhbEJHvGepohE3/WyeeUbOrDIkYaztU0jm6u+NjEbGTKPATuYZzbKXi/95vRTD634mVJIiV2xxKEwlwZjkBZCh0JyhnFpCmRY2K2FjqilDW1PFluAtf3mVtC/qnlv37i9rjZuijjKcwCmcgwdX0IA7aEILGCTwDK/w5qTOi/PufCxGS06xcwx/4Hz+AHqokfU=</latexit>

ck
<latexit sha1_base64="uSCypJxdC1eusWXXZ1tZf846MJE=">AAAB83icbVBNS8NAFHypX7V+VT16WSyCp5KIoMeiF48VbC00oWy2m3bpZhN2X4QS+je8eFDEq3/Gm//GTZuDtg4sDDPv8WYnTKUw6LrfTmVtfWNzq7pd29nd2z+oHx51TZJpxjsskYnuhdRwKRTvoEDJe6nmNA4lfwwnt4X/+MS1EYl6wGnKg5iOlIgEo2gl348pjsMoZ7PBZFBvuE13DrJKvJI0oER7UP/yhwnLYq6QSWpM33NTDHKqUTDJZzU/MzylbEJHvG+pojE3QT7PPCNnVhmSKNH2KSRz9fdGTmNjpnFoJ4uMZtkrxP+8fobRdZALlWbIFVscijJJMCFFAWQoNGcop5ZQpoXNStiYasrQ1lSzJXjLX14l3Yum5za9+8tG66asowoncArn4MEVtOAO2tABBik8wyu8OZnz4rw7H4vRilPuHMMfOJ8/YjWR5Q==</latexit><latexit sha1_base64="uSCypJxdC1eusWXXZ1tZf846MJE=">AAAB83icbVBNS8NAFHypX7V+VT16WSyCp5KIoMeiF48VbC00oWy2m3bpZhN2X4QS+je8eFDEq3/Gm//GTZuDtg4sDDPv8WYnTKUw6LrfTmVtfWNzq7pd29nd2z+oHx51TZJpxjsskYnuhdRwKRTvoEDJe6nmNA4lfwwnt4X/+MS1EYl6wGnKg5iOlIgEo2gl348pjsMoZ7PBZFBvuE13DrJKvJI0oER7UP/yhwnLYq6QSWpM33NTDHKqUTDJZzU/MzylbEJHvG+pojE3QT7PPCNnVhmSKNH2KSRz9fdGTmNjpnFoJ4uMZtkrxP+8fobRdZALlWbIFVscijJJMCFFAWQoNGcop5ZQpoXNStiYasrQ1lSzJXjLX14l3Yum5za9+8tG66asowoncArn4MEVtOAO2tABBik8wyu8OZnz4rw7H4vRilPuHMMfOJ8/YjWR5Q==</latexit><latexit sha1_base64="uSCypJxdC1eusWXXZ1tZf846MJE=">AAAB83icbVBNS8NAFHypX7V+VT16WSyCp5KIoMeiF48VbC00oWy2m3bpZhN2X4QS+je8eFDEq3/Gm//GTZuDtg4sDDPv8WYnTKUw6LrfTmVtfWNzq7pd29nd2z+oHx51TZJpxjsskYnuhdRwKRTvoEDJe6nmNA4lfwwnt4X/+MS1EYl6wGnKg5iOlIgEo2gl348pjsMoZ7PBZFBvuE13DrJKvJI0oER7UP/yhwnLYq6QSWpM33NTDHKqUTDJZzU/MzylbEJHvG+pojE3QT7PPCNnVhmSKNH2KSRz9fdGTmNjpnFoJ4uMZtkrxP+8fobRdZALlWbIFVscijJJMCFFAWQoNGcop5ZQpoXNStiYasrQ1lSzJXjLX14l3Yum5za9+8tG66asowoncArn4MEVtOAO2tABBik8wyu8OZnz4rw7H4vRilPuHMMfOJ8/YjWR5Q==</latexit><latexit sha1_base64="uSCypJxdC1eusWXXZ1tZf846MJE=">AAAB83icbVBNS8NAFHypX7V+VT16WSyCp5KIoMeiF48VbC00oWy2m3bpZhN2X4QS+je8eFDEq3/Gm//GTZuDtg4sDDPv8WYnTKUw6LrfTmVtfWNzq7pd29nd2z+oHx51TZJpxjsskYnuhdRwKRTvoEDJe6nmNA4lfwwnt4X/+MS1EYl6wGnKg5iOlIgEo2gl348pjsMoZ7PBZFBvuE13DrJKvJI0oER7UP/yhwnLYq6QSWpM33NTDHKqUTDJZzU/MzylbEJHvG+pojE3QT7PPCNnVhmSKNH2KSRz9fdGTmNjpnFoJ4uMZtkrxP+8fobRdZALlWbIFVscijJJMCFFAWQoNGcop5ZQpoXNStiYasrQ1lSzJXjLX14l3Yum5za9+8tG66asowoncArn4MEVtOAO2tABBik8wyu8OZnz4rw7H4vRilPuHMMfOJ8/YjWR5Q==</latexit>

di
<latexit sha1_base64="zrkZPirrGIVueyZrRFQfEezjaVg=">AAAB83icbVBNS8NAFHypX7V+VT16WSyCp5KIoMeiF48VbC00oWy2L+3SzSbsboQS+je8eFDEq3/Gm//GTZuDtg4sDDPv8WYnTAXXxnW/ncra+sbmVnW7trO7t39QPzzq6iRTDDssEYnqhVSj4BI7hhuBvVQhjUOBj+HktvAfn1BpnsgHM00xiOlI8ogzaqzk+zE14zDKh7MBH9QbbtOdg6wSryQNKNEe1L/8YcKyGKVhgmrd99zUBDlVhjOBs5qfaUwpm9AR9i2VNEYd5PPMM3JmlSGJEmWfNGSu/t7Iaaz1NA7tZJFRL3uF+J/Xz0x0HeRcpplByRaHokwQk5CiADLkCpkRU0soU9xmJWxMFWXG1lSzJXjLX14l3Yum5za9+8tG66asowoncArn4MEVtOAO2tABBik8wyu8OZnz4rw7H4vRilPuHMMfOJ8/YLSR5A==</latexit><latexit sha1_base64="zrkZPirrGIVueyZrRFQfEezjaVg=">AAAB83icbVBNS8NAFHypX7V+VT16WSyCp5KIoMeiF48VbC00oWy2L+3SzSbsboQS+je8eFDEq3/Gm//GTZuDtg4sDDPv8WYnTAXXxnW/ncra+sbmVnW7trO7t39QPzzq6iRTDDssEYnqhVSj4BI7hhuBvVQhjUOBj+HktvAfn1BpnsgHM00xiOlI8ogzaqzk+zE14zDKh7MBH9QbbtOdg6wSryQNKNEe1L/8YcKyGKVhgmrd99zUBDlVhjOBs5qfaUwpm9AR9i2VNEYd5PPMM3JmlSGJEmWfNGSu/t7Iaaz1NA7tZJFRL3uF+J/Xz0x0HeRcpplByRaHokwQk5CiADLkCpkRU0soU9xmJWxMFWXG1lSzJXjLX14l3Yum5za9+8tG66asowoncArn4MEVtOAO2tABBik8wyu8OZnz4rw7H4vRilPuHMMfOJ8/YLSR5A==</latexit><latexit sha1_base64="zrkZPirrGIVueyZrRFQfEezjaVg=">AAAB83icbVBNS8NAFHypX7V+VT16WSyCp5KIoMeiF48VbC00oWy2L+3SzSbsboQS+je8eFDEq3/Gm//GTZuDtg4sDDPv8WYnTAXXxnW/ncra+sbmVnW7trO7t39QPzzq6iRTDDssEYnqhVSj4BI7hhuBvVQhjUOBj+HktvAfn1BpnsgHM00xiOlI8ogzaqzk+zE14zDKh7MBH9QbbtOdg6wSryQNKNEe1L/8YcKyGKVhgmrd99zUBDlVhjOBs5qfaUwpm9AR9i2VNEYd5PPMM3JmlSGJEmWfNGSu/t7Iaaz1NA7tZJFRL3uF+J/Xz0x0HeRcpplByRaHokwQk5CiADLkCpkRU0soU9xmJWxMFWXG1lSzJXjLX14l3Yum5za9+8tG66asowoncArn4MEVtOAO2tABBik8wyu8OZnz4rw7H4vRilPuHMMfOJ8/YLSR5A==</latexit><latexit sha1_base64="zrkZPirrGIVueyZrRFQfEezjaVg=">AAAB83icbVBNS8NAFHypX7V+VT16WSyCp5KIoMeiF48VbC00oWy2L+3SzSbsboQS+je8eFDEq3/Gm//GTZuDtg4sDDPv8WYnTAXXxnW/ncra+sbmVnW7trO7t39QPzzq6iRTDDssEYnqhVSj4BI7hhuBvVQhjUOBj+HktvAfn1BpnsgHM00xiOlI8ogzaqzk+zE14zDKh7MBH9QbbtOdg6wSryQNKNEe1L/8YcKyGKVhgmrd99zUBDlVhjOBs5qfaUwpm9AR9i2VNEYd5PPMM3JmlSGJEmWfNGSu/t7Iaaz1NA7tZJFRL3uF+J/Xz0x0HeRcpplByRaHokwQk5CiADLkCpkRU0soU9xmJWxMFWXG1lSzJXjLX14l3Yum5za9+8tG66asowoncArn4MEVtOAO2tABBik8wyu8OZnz4rw7H4vRilPuHMMfOJ8/YLSR5A==</latexit>

h̃i
<latexit sha1_base64="XlOXLZ9PUCZr3K0ys7mU3sJHbfM=">AAAB/XicbVDLSsNAFJ3UV62v+Ni5GSyCq5KIoMuiG5cV7AOaECaTm3bo5MHMRKgh+CtuXCji1v9w5984abPQ1gMDh3Pu5Z45fsqZVJb1bdRWVtfWN+qbja3tnd09c/+gJ5NMUOjShCdi4BMJnMXQVUxxGKQCSORz6PuTm9LvP4CQLInv1TQFNyKjmIWMEqUlzzxyFOMB5E5E1NgP83FReMwzm1bLmgEvE7siTVSh45lfTpDQLIJYUU6kHNpWqtycCMUoh6LhZBJSQidkBENNYxKBdPNZ+gKfaiXAYSL0ixWeqb83chJJOY18PVmGlIteKf7nDTMVXrk5i9NMQUznh8KMY5XgsgocMAFU8akmhAqms2I6JoJQpQtr6BLsxS8vk955y7Za9t1Fs31d1VFHx+gEnSEbXaI2ukUd1EUUPaJn9IrejCfjxXg3PuajNaPaOUR/YHz+AGGnlc8=</latexit><latexit sha1_base64="XlOXLZ9PUCZr3K0ys7mU3sJHbfM=">AAAB/XicbVDLSsNAFJ3UV62v+Ni5GSyCq5KIoMuiG5cV7AOaECaTm3bo5MHMRKgh+CtuXCji1v9w5984abPQ1gMDh3Pu5Z45fsqZVJb1bdRWVtfWN+qbja3tnd09c/+gJ5NMUOjShCdi4BMJnMXQVUxxGKQCSORz6PuTm9LvP4CQLInv1TQFNyKjmIWMEqUlzzxyFOMB5E5E1NgP83FReMwzm1bLmgEvE7siTVSh45lfTpDQLIJYUU6kHNpWqtycCMUoh6LhZBJSQidkBENNYxKBdPNZ+gKfaiXAYSL0ixWeqb83chJJOY18PVmGlIteKf7nDTMVXrk5i9NMQUznh8KMY5XgsgocMAFU8akmhAqms2I6JoJQpQtr6BLsxS8vk955y7Za9t1Fs31d1VFHx+gEnSEbXaI2ukUd1EUUPaJn9IrejCfjxXg3PuajNaPaOUR/YHz+AGGnlc8=</latexit><latexit sha1_base64="XlOXLZ9PUCZr3K0ys7mU3sJHbfM=">AAAB/XicbVDLSsNAFJ3UV62v+Ni5GSyCq5KIoMuiG5cV7AOaECaTm3bo5MHMRKgh+CtuXCji1v9w5984abPQ1gMDh3Pu5Z45fsqZVJb1bdRWVtfWN+qbja3tnd09c/+gJ5NMUOjShCdi4BMJnMXQVUxxGKQCSORz6PuTm9LvP4CQLInv1TQFNyKjmIWMEqUlzzxyFOMB5E5E1NgP83FReMwzm1bLmgEvE7siTVSh45lfTpDQLIJYUU6kHNpWqtycCMUoh6LhZBJSQidkBENNYxKBdPNZ+gKfaiXAYSL0ixWeqb83chJJOY18PVmGlIteKf7nDTMVXrk5i9NMQUznh8KMY5XgsgocMAFU8akmhAqms2I6JoJQpQtr6BLsxS8vk955y7Za9t1Fs31d1VFHx+gEnSEbXaI2ukUd1EUUPaJn9IrejCfjxXg3PuajNaPaOUR/YHz+AGGnlc8=</latexit><latexit sha1_base64="XlOXLZ9PUCZr3K0ys7mU3sJHbfM=">AAAB/XicbVDLSsNAFJ3UV62v+Ni5GSyCq5KIoMuiG5cV7AOaECaTm3bo5MHMRKgh+CtuXCji1v9w5984abPQ1gMDh3Pu5Z45fsqZVJb1bdRWVtfWN+qbja3tnd09c/+gJ5NMUOjShCdi4BMJnMXQVUxxGKQCSORz6PuTm9LvP4CQLInv1TQFNyKjmIWMEqUlzzxyFOMB5E5E1NgP83FReMwzm1bLmgEvE7siTVSh45lfTpDQLIJYUU6kHNpWqtycCMUoh6LhZBJSQidkBENNYxKBdPNZ+gKfaiXAYSL0ixWeqb83chJJOY18PVmGlIteKf7nDTMVXrk5i9NMQUznh8KMY5XgsgocMAFU8akmhAqms2I6JoJQpQtr6BLsxS8vk955y7Za9t1Fs31d1VFHx+gEnSEbXaI2ukUd1EUUPaJn9IrejCfjxXg3PuajNaPaOUR/YHz+AGGnlc8=</latexit>

Context token encoding

Reference document
sentence encoding

Attention
mechanism

Extraction
probabilities

Extractive summary of the
reference document

Document-level
sentence encoding

Feed-Forward
with sigmoid

Topic encoding

Figure 3: The extractive model uses three separate encoders create representations for the reference document sen-
tences, context tokens, and topics. These are combined through an attention mechanism, encoded at a document-
level, and passed through a feed-forward layer to compute an extraction probability for each reference sentence.

term to the loss function which penalizes such be-
havior and a new term to the attention score cal-
culation that informs the model about the previous
steps’ attention weights. The coverage mechanism
has the effect of decreasing the amount of redun-
dancy in the generated summary.

Conditioning on Context In order to condition
the generation of the cloze on the context, we use
the context tokens to initialize the decoder’s hid-
den state representation by first forcing the tokens
through the decoder.

Concretely, let st be the decoder’s hidden state
for decoding step t (with s0 initialized to the final
encoder hidden state), dθ(·) be the decoder func-
tion, and c1, . . . , cT be the context tokens. The
context tokens are passed through the decoder to
create a new decoder hidden state representation
using the following recursive formula

st = dθ(ct, st−1) ∀t = 1, . . . , T (2)

Then, sT is used as the initial hidden state that the
decoder uses to generate the cloze tokens.

Priming the decoder with the context tokens has
the benefit of allowing the model to condition the
cloze generation on a representation of the context.
Additionally, because the context will impact the
decoder’s coverage calculation, it will discourage
the cloze from repeating information already in the
context.

Training & Inference Following See et al.
(2017), the training objective is the negative log-
likelihood of the ground-truth cloze tokens with

the added coverage penalty. Inference is done us-
ing beam search.

5 Experimental Setup

Our experiments aim to better understand the
WIKICITE dataset, evaluation metric, and perfor-
mance of the extractive and abstractive models.
Numbers in bold indicate statistical significance
based on bootstrap resampling with p = 0.05.

5.1 Dataset Ablation & Human Performance

In order to better understand the WIKICITE
dataset, we estimate how well models can perform
without the reference documents and context and
establish human performance on the task.

No Reference Document We used the publicly
available large-scale language model from Rad-
ford et al. (2019) (the 345M parameter version)
with the default parameters to generate the cloze.
The language model (which we call NO REFER-
ENCE) was conditioned on the context tokens and
one full sentence was generated from the model
without access to the reference document.

No Context We trained an abstractive model
(§4.2) to summarize the first 200 tokens of the ref-
erence documents in one sentence, using the cloze
as ground-truth, without access to the context.
This is equivalent to training a single-document
summarization system on the input text. This
model is referred to as NO CONTEXT.



3725

Human Performance We manually wrote
clozes for 50 different instances with and without
access to the topic and context to establish human
performance on this task. We also had each cloze
judged by 5 crowd workers to rate from 1 to 5
how coherent the topic, context, and cloze are.
We report this average quality score alongside
ROUGE with the HUMAN label.

5.2 Extractive Models

In addition to an ablation study on the extractive
model (referred to as CONTENTSELECTOR; §4.1)
where we remove the topic and context, we also
evaluate several different extractive models on this
task, described below.

Lead & Oracle The LEAD-1 chooses the first
sentence from the reference documents as the
cloze, a common baseline for summarization.

The ORACLE-1 model selects the one sen-
tence from the reference set which maximizes the
ROUGE score using the ground-truth. A different
oracle model was created for each individual met-
ric. The performance of the oracle represents the
upper-bound ROUGE score that a system which
extracts exactly one sentence can achieve.

BM25 BM25 (Robertson et al., 1995) is an in-
formation retrieval-based score used to rank doc-
uments in search results for an input query. We
selected the reference document sentence with the
highest BM25 score for the context, treating the
context as the query, the sentences as the doc-
uments, and computing the document frequency
scores over the corpus of reference documents.

SumFocus SumFocus (Vanderwende et al.,
2007) is an unsupervised model for topic-focused
extractive summarization. The model constructs
a unigram distribution over tokens by interpolat-
ing the unigram distributions of the documents and
topic. Then, the model iteratively selects a sen-
tence that has the highest probability token, dis-
counts the probabilities of tokens in the selected
sentence, and repeats. We extend SumFocus by
additionally interpolating between the context un-
igram distribution in addition to the document and
topic distributions.

5.3 Abstractive Models

Then, we measured how well the two-step abstrac-
tive system performs with and without access to
the context and with different extractive models.

Extractive Step Evaluation We use the extrac-
tive models above to preprocess the reference set
to select the top scoring sentences, up to 200 to-
kens. The preprocessed documents are evaluated
against the ground-truth cloze by calculating the
recall variant of ROUGE. This will measure what
percent of the cloze n-grams are contained within
the text input to the abstractive step.

In addition to the extractive models, we also re-
port lead and extractive oracle scores. The LEAD-
200 extractive step selects the first 200 tokens
of the reference document. The extractive oracle
model preprocesses the reference sentences by se-
lecting only those which were labeled positively in
the heuristically-generated extractive labels, lim-
ited to 200 tokens. This model, called HEURISTIC
LABELS, serves to demonstrate how well the ab-
stractive model would perform with a perfect ex-
tractive step.

Abstractive Step Evaluation Using the prepro-
cessed documents from various extractive models,
we trained abstractive models (§4.2) to produce
the ground-truth cloze, both with and without ac-
cess to the context. These systems were evaluated
with the F1 variant of ROUGE and perplexity.

5.4 Implementation Details
Our models were implemented using PyTorch
(Paszke et al., 2017) in the AllenNLP frame-
work (Gardner et al., 2017).4 We used fixed
200-dimensional GloVe embeddings (Pennington
et al., 2014). The RNNs were implemented with
LSTMs (Hochreiter and Schmidhuber, 1997) of
size 256. The models were trained with Adam
(Kingma and Ba, 2014) using a learning rate of
1e-4 and 1e-3 and batch sizes of 32 and 16 for
290k and 62.5k iterations for the extractive and ab-
stractive models, respectively. Following See et al.
(2017), we trained the abstractive models without
the coverage loss until convergence, then by 3k it-
erations with a coverage loss weight of 1.

6 Experimental Results

6.1 Human Performance & Evaluation
Table 2 shows the ROUGE and quality scores for
the human-written clozes with and without access
to the topic and context. For an additional compar-
ison, we included a random human baseline that
randomly shuffled the human-written clozes.

4All code available at http://cogcomp.org/
page/publication_view/880

http://cogcomp.org/page/publication_view/880
http://cogcomp.org/page/publication_view/880


3726

Model R1 R2 RL Qual.

GROUND-TRUTH 100 100 100 4.0

HUMAN 31.78 16.94 28.32 3.9
-TOPIC,-CONTEXT 24.49 7.55 20.11 3.7

RANDOM HUMAN 9.96 0.08 8.24 2.3

Table 2: The ROUGE F1 scores for the human clozes
with and without access to the topic and context and
crowdsourced quality judgments (on a 1-5 scale) of
how well the cloze continues the corresponding text.
The standard deviations for the quality judgments
(omitted for space) were each around 0.9.

Model R1 R2 RL

NO REFERENCE 14.47 1.43 11.41
NO CONTEXT 21.79 6.18 17.83

Table 3: The ROUGE F1 results of the baseline models
that do not have access to the references or context.

The ROUGE scores for when the human cloze
writers had access to the topic and context are
higher by 7.3 R1 points, a significant margin.
From this observation, we can infer that, when
provided with the topic and context, humans do
a better job at identifying the same information
present in the ground-truth cloze. This is indica-
tive that the topic and context should also be nec-
essary for a system to do the same.

However, the crowd worker quality judgments
are roughly equal for the ground-truth and both
human-written clozes, and human performance is
at 31.78 R1, which is somewhat low on an abso-
lute scale. We believe this reflects the difficulty of
evaluating this task and summarization in general.
Because there are many possible valid clozes (or
valid summaries of documents in the more gen-
eral case) and only one ground-truth cloze, evalu-
ation with automatic metrics is difficult. Although
ROUGE is imperfect, it does positively correlate
with the crowd worker quality scores, with a Pear-
son’s coefficient of 0.44.

6.2 Dataset Ablation

The two baseline models that do not have access
to the references or context, respectively, are eval-
uated in Table 3. Although the NO REFERENCE
model is a high-quality language model, it per-
forms significantly worse than the NO CONTEXT
model. This is likely due to the fact that the NO
CONTEXT model was trained for this task.

Model R1 R2 RL

ORACLE-1 44.32 26.17 37.62

LEAD-1 17.50 4.70 13.45
BM25 21.33 5.76 16.18
SUMFOCUS 17.78 4.61 13.86
CONTENTSELECTOR 25.13 9.48 19.65

-TOPIC 25.21 9.49 19.68
-CONTEXT 22.11 6.53 16.66
-TOPIC,-CONTEXT 21.29 6.01 16.05

Table 4: The ROUGE F1 scores for the extractive mod-
els, all of which are significantly lower than the oracle
model, indicating there is room for improvement.

6.3 Extractive Models

The performance of the different extractive models
on the summary cloze task is presented in Table 4.

The CONTENTSELECTOR model achieves the
best score overall with around 25 R1. Providing
CONTENTSELECTOR with the topic and context
improves the R1 score by nearly 4 points, however,
this improvement is mostly due to the context. The
topic does not change the model’s performance by
a statistically significant amount, which is an indi-
cation that it is not being properly utilized.

Among the symbolic models, BM25 out-
performs SUMFOCUS by 3.55 R1. One reason for
this could be that BM25 uses a token weighting
to score each sentence based on how frequently
the token appears in the entire reference corpora,
whereas the standard SUMFOCUS implementation
treats all tokens equally.

Although the models perform well, they are still
significantly below the oracle model by at least 19
R1 and 16 R2 points. Such a large gap indicates
that there is some signal in the data that is not yet
captured by the extractive models.

6.4 Abstractive Models

Extractive Step The ROUGE recall scores for
the different extractive models used to preprocess
the input to the abstractive model are shown in Ta-
ble 5. The results are largely consistent with the
extractive model scores, with CONTENTSELEC-
TOR performing the best, the topic causing no sta-
tistically significant difference, and a large gap re-
maining between current models and a perfect sys-
tem (the heuristic labels).

Table 5 also contains an ablation study for the
SUMFOCUS in which the topic did make a signifi-
cant difference. This suggests that it may be easier
to use the topic in a symbolic model and a model



3727

Model R1-R R2-R RL-R

HEURISTIC LABELS 80.25 35.75 55.11

LEAD-200 60.89 22.31 44.97
BM25 64.19 25.04 56.24
SUMFOCUS 61.79 22.77 54.15

-TOPIC 60.95 22.38 53.61
-CONTEXT 61.03 22.08 53.32
-TOPIC,-CONTEXT 59.11 20.62 51.93

CONTENTSELECTOR 66.27 27.59 58.25
-TOPIC 66.32 27.63 58.32
-CONTEXT 63.86 24.48 55.80
-TOPIC,-CONTEXT 63.11 23.76 55.09

Table 5: An evaluation using the recall variant of
ROUGE of the different extractive preprocessing steps.

may benefit from combining symbolic and contin-
uous representations.

Abstractive Step The ROUGE scores and per-
plexities for the abstractive models that use three
different extractive steps are presented in Table 6.
We observe that the CONTENTSELECTOR extrac-
tive step outperforms LEAD-200 in both ROUGE
and perplexity. Additionally, including the con-
text improves performance across all models, with
nearly a 2 R1 gain with the CONTENTSELECTOR.
This result is a clear signal that priming the de-
coder with the context produces a better cloze.

Example output from the CONTENTSELECTOR
model that uses the context can been seen in Fig-
ure 4. In general, the model tends to copy long se-
quences from the reference document, often pre-
ferring somewhat formulaic sentences that begin
with phrases such as “he became,” “she gradu-
ated,” or “she received” and are similar in style to
Wikipedia articles.

Similar to the extractive results, there is a siz-
able gap between the performance of the abstrac-
tive model when it uses the heuristic labels versus
the extractive model for preprocessing. This indi-
cates that improving the extractive model will pro-
vide large downstream abstractive improvements.

7 Related Work

Summarization Recent work on generic sum-
marization has focused on single-document mod-
els on the CNN/DailyMail dataset (Nallapati et al.,
2016), focusing on different neural architectures
and loss functions for extractive and abstractive
summarization (Cheng and Lapata, 2016; Nallap-
ati et al., 2017; Chopra et al., 2016; Nallapati et al.,
2016; See et al., 2017, inter alia). Our models were

Ext. Model Abs. R1 R2 RL PPL

HEUR. LAB. +C 34.30 16.34 28.73 18.08–C 33.62 15.80 27.96 18.71

LEAD-200 +C 23.56 7.15 19.3 39.73–C 21.79 6.18 17.83 40.90

CONTSEL. +C 24.67 8.55 20.44 33.22–C 22.71 7.33 18.71 32.77

Table 6: The ROUGE F1 and perplexity results for ab-
stractive models with and without the context (+/–C)
with heuristic labels, lead, and CONTENTSELECTOR
extractive preprocessing steps.

based on the successful approaches of See et al.
(2017) and those described in Kedzie et al. (2018).

Most work on topic-focused (also referred to
as “query-focused”) summarization uses the DUC
2005 and 2006 datasets (Dang, 2005, 2006).
Approaches are mostly extractive, with some
work selecting sentences based on token fre-
quency/salience (Otterbacher et al., 2005; Van-
derwende et al., 2007), diversity with submod-
ular functions (Lin and Bilmes, 2011), or using
Bayesian networks (Daumé III and Marcu, 2006).
To the best of our knowledge, ours is the first work
which builds end-to-end models on a large-scale
dataset for topic-focused summarization.

Generating Wikipedia In contrast to our work
focusing on content selection for topic-focused
summaries, there have been previous work inter-
ested in generating Wikipedia articles. Sauper
and Barzilay (2009) try to generate articles from
a small set of domains by learning to generate
templates from similar articles using an integer
linear program. Similarly, Banerjee and Mitra
(2016) build topic classifiers based on sections on
Wikipedia to identify content in search results for
topic keywords as part of a pipeline to generate
full articles. Like us, Liu et al. (2018) also employ
a two-step abstractive approach, but their goal is
to generate the lead Wikipedia paragraph from all
of the citations and web search results.

8 Conclusion

In this work, we propose the summary cloze task,
a new task for studying content selection in topic-
focused summarization. By narrowing the scope
from the full topic-focused summarization task
to one at the sentence-level, we are able to col-
lect a large-scale summary cloze dataset from
Wikipedia. Our experimental results demonstrate



3728

On May 15 , 2011 , Welts publicly came out 
as gay in an interview with The New York 
Times .   He is the first prominent American 
sports executive to come out and be openly 
gay .

Rick Welts, Biography

Rick Welts , the president of the Phoenix 
Suns , made history yesterday . Not 
because of something his team did … In 
doing so , he became the first executive in 
any professional sport to be openly gay …  
It 's what made this risk the right one to take 
and will help an athlete to have the courage 
to come out in the near future .

to
pi

c
re

fe
re

nc
e 

do
cu

m
en

t
co

nt
ex

t
cl

oz
e

input
truth

ground-

he became the first executive in any 
professional sport to be openly gay in a 
position like the one .

output
m

odel

The New York Times described Weaver as   
" a long - trusted adviser to Mr. Sanders …   
In July 2016 , after Sanders endorsed Hillary 
Clinton for president , Weaver promised " to 
help organize voters " , but did not join her 
campaign staff .

Jeff Weaver (political staffer), Career

Bernie Sanders ' longtime top aide Jeff 
Weaver has agreed to help Hillary Clinton 's 
team organize voters … " Like the senator I 
am fully behind the secretary and … the 
millions of Sanders supporters who are 
obviously disappointed that the senator    
did n't win , " Weaver said .

to
pi

c
re

fe
re

nc
e 

do
cu

m
en

t
co

nt
ex

t
cl

oz
e

input
truth

ground-

weaver stated that sanders " fully behind the 
secretary of state in the general election 
fight against donald trump . "

output
m

odel

Figure 4: Example outputs from the abstractive model
that uses the context. The model often copies se-
quences from the references which are sometimes cor-
rect (top) or incorrect but sensible (bottom), highlight-
ing the difficulty of automatic evaluation. (Documents
shortened for space. Sentences which are underlined
were selected by the extraction step.)

that the topic and partial summary help the extrac-
tive and abstractive models, but the task remains a
significant challenge with room for improvement
in future work.

Acknowledgements

The authors would like to thank the anonymous
reviewers for their helpful feedback and sugges-
tions.

Research was sponsored by the Army Research
Laboratory and was accomplished under Coopera-
tive Agreement Number W911NF-09-2-0053 (the
ARL Network Science CTA). The views and con-
clusions contained in this document are those of

the authors and should not be interpreted as rep-
resenting the official policies, either expressed or
implied, of the Army Research Laboratory or the
U.S. Government. The U.S. Government is autho-
rized to reproduce and distribute reprints for Gov-
ernment purposes notwithstanding any copyright
notation here on.

References
Siddhartha Banerjee and Prasenjit Mitra. 2016. Wiki-

Write: Generating Wikipedia Articles Automati-
cally. In IJCAI, pages 2740–2746.

Jianpeng Cheng and Mirella Lapata. 2016. Neural
Summarization by Extracting Sentences and Words.
In Proceedings of the 54th Annual Meeting of the As-
sociation for Computational Linguistics (Volume 1:
Long Papers), pages 484–494. Association for Com-
putational Linguistics.

Sumit Chopra, Michael Auli, and Alexander M. Rush.
2016. Abstractive Sentence Summarization with At-
tentive Recurrent Neural Networks. In Proceed-
ings of the 2016 Conference of the North American
Chapter of the Association for Computational Lin-
guistics: Human Language Technologies, pages 93–
98. Association for Computational Linguistics.

Hoa Trang Dang. 2005. Overview of DUC 2005. In
Proceedings of the document understanding confer-
ence, volume 2005, pages 1–12. Citeseer.

Hoa Trang Dang. 2006. Overview of DUC 2006. In
Document Understanding Conference.

Hal Daumé III and Daniel Marcu. 2006. Bayesian
Query-Focused Summarization. In Proceedings of
the 21st International Conference on Computational
Linguistics and 44th Annual Meeting of the Asso-
ciation for Computational Linguistics, pages 305–
312, Sydney, Australia. Association for Computa-
tional Linguistics.

Guy Feigenblat, Haggai Roitman, Odellia Boni, and
David Konopnicki. 2017. Unsupervised Query-
Focused Multi-Document Summarization Using the
Cross Entropy Method. In Proceedings of the 40th
International ACM SIGIR Conference on Research
and Development in Information Retrieval, SIGIR
’17, pages 961–964, New York, NY, USA. ACM.

Matt Gardner, Joel Grus, Mark Neumann, Oyvind
Tafjord, Pradeep Dasigi, Nelson F. Liu, Matthew
Peters, Michael Schmitz, and Luke S. Zettlemoyer.
2017. AllenNLP: A Deep Semantic Natural Lan-
guage Processing Platform.

Max Grusky, Mor Naaman, and Yoav Artzi. 2018.
NEWSROOM: A Dataset of 1.3 Million Summaries
with Diverse Extractive Strategies. In Proceed-
ings of the 2018 Conference of the North Ameri-
can Chapter of the Association for Computational

https://doi.org/10.18653/v1/P16-1046
https://doi.org/10.18653/v1/P16-1046
https://doi.org/10.18653/v1/N16-1012
https://doi.org/10.18653/v1/N16-1012
http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.184.1617&rep=rep1&type=pdf
https://duc.nist.gov/pubs/2006papers/duc2006.pdf
https://doi.org/10.3115/1220175.1220214
https://doi.org/10.3115/1220175.1220214
https://doi.org/10.1145/3077136.3080690
https://doi.org/10.1145/3077136.3080690
https://doi.org/10.1145/3077136.3080690
http://arxiv.org/abs/arXiv:1803.07640
http://arxiv.org/abs/arXiv:1803.07640
http://aclweb.org/anthology/N18-1065
http://aclweb.org/anthology/N18-1065


3729

Linguistics: Human Language Technologies, pages
708–719, New Orleans, Louisiana. Association for
Computational Linguistics.

Sepp Hochreiter and Jürgen Schmidhuber. 1997.
Long short-term memory. Neural computation,
9(8):1735–1780.

Chris Kedzie, Kathleen McKeown, and Hal Daume III.
2018. Content Selection in Deep Learning Models
of Summarization. In Proceedings of the 2018 Con-
ference on Empirical Methods in Natural Language
Processing, pages 1818–1828, Brussels, Belgium.
Association for Computational Linguistics.

Diederik P Kingma and Jimmy Ba. 2014. Adam: A
method for stochastic optimization. arXiv preprint
arXiv:1412.6980.

Chin-Yew Lin. 2004. ROUGE: A Package for Auto-
matic Evaluation of Summaries. In Text Summa-
rization Branches Out: Proceedings of the ACL-04
Workshop, pages 74–81, Barcelona, Spain. Associa-
tion for Computational Linguistics.

Hui Lin and Jeff Bilmes. 2011. A Class of Submodu-
lar Functions for Document Summarization. In Pro-
ceedings of the 49th Annual Meeting of the Associ-
ation for Computational Linguistics: Human Lan-
guage Technologies, pages 510–520, Portland, Ore-
gon, USA. Association for Computational Linguis-
tics.

Peter J Liu, Mohammad Saleh, Etienne Pot, Ben
Goodrich, Ryan Sepassi, Lukasz Kaiser, and Noam
Shazeer. 2018. Generating Wikipedia by Summariz-
ing Long Sequences. International Conference on
Learning Representations 2018.

Shulei Ma, Zhi-Hong Deng, and Yunlun Yang. 2016.
An Unsupervised Multi-Document Summarization
Framework Based on Neural Document Model. In
Proceedings of COLING 2016, the 26th Interna-
tional Conference on Computational Linguistics:
Technical Papers, pages 1514–1523, Osaka, Japan.
The COLING 2016 Organizing Committee.

Ramesh Nallapati, Feifei Zhai, and Bowen Zhou. 2017.
Summarunner: A Recurrent Neural Network based
Sequence Model for Extractive Summarization of
Documents. In Thirty-First AAAI Conference on Ar-
tificial Intelligence.

Ramesh Nallapati, Bowen Zhou, Cicero dos Santos,
Caglar Gulcehre, and Bing Xiang. 2016. Ab-
stractive Text Summarization using Sequence-to-
sequence RNNs and Beyond. In Proceedings of The
20th SIGNLL Conference on Computational Natural
Language Learning, pages 280–290, Berlin, Ger-
many. Association for Computational Linguistics.

Ani Nenkova. 2006a. Summarization evaluation for
text and speech: issues and approaches. In INTER-
SPEECH.

Ani Nenkova. 2006b. Understanding the process of
multi-document summarization: content selection,
rewriting and evaluation. Columbia University.

Jahna Otterbacher, Gunes Erkan, and Dragomir Radev.
2005. Using Random Walks for Question-focused
Sentence Retrieval. In Proceedings of Human Lan-
guage Technology Conference and Conference on
Empirical Methods in Natural Language Process-
ing, pages 915–922, Vancouver, British Columbia,
Canada. Association for Computational Linguistics.

Karolina Owczarzak, John M. Conroy, Hoa Trang
Dang, and Ani Nenkova. 2012. An Assessment of
the Accuracy of Automatic Evaluation in Summa-
rization. In Proceedings of Workshop on Evaluation
Metrics and System Comparison for Automatic Sum-
marization, pages 1–9, Montréal, Canada. Associa-
tion for Computational Linguistics.

Adam Paszke, Sam Gross, Soumith Chintala, Gre-
gory Chanan, Edward Yang, Zachary DeVito, Zem-
ing Lin, Alban Desmaison, Luca Antiga, and Adam
Lerer. 2017. Automatic differentiation in PyTorch.
In NIPS-W.

Jeffrey Pennington, Richard Socher, and Christo-
pher D. Manning. 2014. GloVe: Global Vectors for
Word Representation. In Empirical Methods in Nat-
ural Language Processing (EMNLP), pages 1532–
1543.

Alec Radford, Jeff Wu, Rewon Child, David Luan,
Dario Amodei, and Ilya Sutskever. 2019. Language
Models are Unsupervised Multitask Learners.

Stephen E Robertson, Steve Walker, Susan Jones,
Micheline M Hancock-Beaulieu, Mike Gatford,
et al. 1995. Okapi at TREC-3. Nist Special Pub-
lication Sp, 109:109.

Christina Sauper and Regina Barzilay. 2009. Automat-
ically Generating Wikipedia Articles: A Structure-
Aware Approach. In Proceedings of the Joint Con-
ference of the 47th Annual Meeting of the ACL and
the 4th International Joint Conference on Natural
Language Processing of the AFNLP, pages 208–
216, Suntec, Singapore. Association for Computa-
tional Linguistics.

Abigail See, Peter J. Liu, and Christopher D. Man-
ning. 2017. Get To The Point: Summarization with
Pointer-Generator Networks. In Proceedings of the
55th Annual Meeting of the Association for Com-
putational Linguistics (Volume 1: Long Papers),
pages 1073–1083, Vancouver, Canada. Association
for Computational Linguistics.

Lucy Vanderwende, Hisami Suzuki, Chris Brockett,
and Ani Nenkova. 2007. Beyond SumBasic: Task-
focused summarization with sentence simplification
and lexical expansion. Information Processing &
Management, 43(6):1606–1618.

https://www.aclweb.org/anthology/D18-1208
https://www.aclweb.org/anthology/D18-1208
https://arxiv.org/pdf/1412.6980.pdf
https://arxiv.org/pdf/1412.6980.pdf
https://www.aclweb.org/anthology/W04-1013
https://www.aclweb.org/anthology/W04-1013
https://www.aclweb.org/anthology/P11-1052
https://www.aclweb.org/anthology/P11-1052
https://arxiv.org/pdf/1801.10198.pdf
https://arxiv.org/pdf/1801.10198.pdf
https://www.aclweb.org/anthology/C16-1143
https://www.aclweb.org/anthology/C16-1143
https://arxiv.org/pdf/1611.04230.pdf
https://arxiv.org/pdf/1611.04230.pdf
https://arxiv.org/pdf/1611.04230.pdf
http://www.aclweb.org/anthology/K16-1028
http://www.aclweb.org/anthology/K16-1028
http://www.aclweb.org/anthology/K16-1028
https://pdfs.semanticscholar.org/6dab/83e7042740a2e04f8f9c9465f92f63b43e23.pdf?_ga=2.136196722.273848981.1556135901-511383223.1516733187
https://pdfs.semanticscholar.org/6dab/83e7042740a2e04f8f9c9465f92f63b43e23.pdf?_ga=2.136196722.273848981.1556135901-511383223.1516733187
http://www1.cs.columbia.edu/~ani/ani-thesis.pdf
http://www1.cs.columbia.edu/~ani/ani-thesis.pdf
http://www1.cs.columbia.edu/~ani/ani-thesis.pdf
https://www.aclweb.org/anthology/H05-1115
https://www.aclweb.org/anthology/H05-1115
https://www.aclweb.org/anthology/W12-2601
https://www.aclweb.org/anthology/W12-2601
https://www.aclweb.org/anthology/W12-2601
http://www.aclweb.org/anthology/D14-1162
http://www.aclweb.org/anthology/D14-1162
https://d4mucfpksywv.cloudfront.net/better-language-models/language-models.pdf
https://d4mucfpksywv.cloudfront.net/better-language-models/language-models.pdf
https://pdfs.semanticscholar.org/fcdc/4d29df4877d6381b0ecfc4c94acaea041770.pdf?_ga=2.149259318.1266728981.1557413840-511383223.1516733187
https://www.aclweb.org/anthology/P09-1024
https://www.aclweb.org/anthology/P09-1024
https://www.aclweb.org/anthology/P09-1024
https://doi.org/10.18653/v1/P17-1099
https://doi.org/10.18653/v1/P17-1099
https://www.cis.upenn.edu/~nenkova/papers/ipm.pdf
https://www.cis.upenn.edu/~nenkova/papers/ipm.pdf
https://www.cis.upenn.edu/~nenkova/papers/ipm.pdf

