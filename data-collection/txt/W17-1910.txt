



















































One Representation per Word - Does it make Sense for Composition?


Proceedings of the 1st Workshop on Sense, Concept and Entity Representations and their Applications, pages 79–90,
Valencia, Spain, April 4 2017. c©2017 Association for Computational Linguistics

One Representation per Word — Does it make Sense for Composition?

Thomas Kober, Julie Weeds, John Wilkie, Jeremy Reffin and David Weir
TAG laboratory, Department of Informatics, University of Sussex

Brighton, BN1 9RH, UK
{t.kober, j.e.weeds, jw478, j.p.reffin, d.j.weir}@sussex.ac.uk

Abstract

In this paper, we investigate whether an
a priori disambiguation of word senses is
strictly necessary or whether the meaning
of a word in context can be disambiguated
through composition alone. We evaluate
the performance of off-the-shelf single-
vector and multi-sense vector models on
a benchmark phrase similarity task and a
novel task for word-sense discrimination.
We find that single-sense vector models
perform as well or better than multi-sense
vector models despite arguably less clean
elementary representations. Our findings
furthermore show that simple composition
functions such as pointwise addition are
able to recover sense specific information
from a single-sense vector model remark-
ably well.

1 Introduction

Distributional word representations based on
counting co-occurrences have a long history in
natural language processing and have successfully
been applied to numerous tasks such as sentiment
analysis, recognising textual entailment, word-
sense disambiguation and many other important
problems. More recently low-dimensional and
dense neural word embeddings have received a
considerable amount of attention in the research
community and have become ubiquitous in numer-
ous NLP pipelines in academia and industry. One
fundamental simplifying assumption commonly
made in distributional semantic models, however,
is that every word can be encoded by a single rep-
resentation. Combining polysemous lexemes into
a single vector has the consequence of essentially
creating a weighted average of all observed mean-
ings of a lexeme in a given text corpus.

Therefore a number of proposals have been
made to overcome the issue of conflating several
different senses of an individual lexeme into a
single representation. One approach (Reisinger
and Mooney, 2010; Huang et al., 2012) is to try
directly inferring a predefined number of senses
from data and subsequently label any occurrences
of a polysemous lexeme with the inferred inven-
tory. Similar approaches are proposed by Reddy
et al. (2011) and Kartsaklis et al. (2013) who show
that appropriate sense selection or disambigua-
tion typically improves performance for compo-
sition of noun phrases (Reddy et al., 2011) and
verb phrases (Kartsaklis et al., 2013). Dinu and
Lapata (2010) proposed a model that represents
the meaning of a word as a probability distribu-
tion over latent senses which is modulated based
on contextualisation, and report improved perfor-
mance on a word similarity task and the lexi-
cal substitution task. Other approaches leverage
an existing lexical resource such as BabelNet or
WordNet to obtain sense labels a priori to creat-
ing word representations (Iacobacci et al., 2015),
or as a postprocessing step after obtaining initial
word representations (Chen et al., 2014; Pilehvar
and Collier, 2016). While these approaches have
exhibited strong performance on benchmark word
similarity tasks (Huang et al., 2012; Iacobacci et
al., 2015) and some downstream processing tasks
such as part-of-speech tagging and relation identi-
fication (Li and Jurafsky, 2015), they have been
weaker than the single-vector representations at
predicting the compositionality of multi-word ex-
pressions (Salehi et al., 2015), and at tasks which
require the meaning of a word to be considered
in context; e.g, word sense disambiguation (Ia-
cobacci et al., 2016) and word similarity in con-
text (Iacobacci et al., 2015).

In this paper we consider what happens when
distributional representations are composed to

79



form representations for larger units of meaning.
In a compositional phrase, the meaning of the
whole can be inferred from the meaning of its
parts. Thus, assuming compositionality, the rep-
resentation of a phrase such as black mood, should
be directly inferable from the representations for
black and for mood. Further, one might suppose
that composing the correct senses of the individ-
ual lexemes would result in a more accurate repre-
sentation of that phrase. However, our counter-
hypothesis is that the act of composition con-
textualises or disambiguates each of the lexemes
thereby making the representations of individual
senses redundant. We investigate this hypothe-
sis by evaluating the performance of single-vector
representations and multi-sense representations at
both a benchmark phrase similarity task and at a
novel word-sense discrimination task.

Our contributions in this work are thus as fol-
lows. First, we provide quantitative and qualita-
tive evidence that even simple composition func-
tions have the ability to recover sense-specific in-
formation from a single-vector representation of
a polysemous lexeme in context. Second, we in-
troduce a novel word-sense discrimination task1,
which can be seen as the first stage of word-sense
disambiguation. The goal is to find whether the
occurrences of a lexeme in two or more senten-
tial contexts belong to the same sense or not, with-
out necessarily labelling the senses. While it has
received relatively little attention in recent years,
it is an important natural language understanding
problem and can provide important insights into
the process of semantic composition.

2 Evaluating Distributional Models of
Composition

For evaluation we use several readily available
off-the-shelf word embeddings, that have al-
ready been shown to work well for a number
of different NLP applications. We compare the
300-dimensional skip-gram word2vec (Mikolov
et al., 2013) word embeddings2 to the depen-
dency based version of word2vec — henceforth
dep2vec3 (Levy and Goldberg, 2014) — and the

1Our task is available from https://github.com/
tttthomasssss/sense2017

2Available from: https://code.google.com/p/
word2vec/

3Available from: https://levyomer.
wordpress.com/2014/04/25/
dependency-based-word-embeddings/

SENSEMBED model4 by Iacobacci et al. (2015),
which creates word-sense embeddings by per-
forming word-sense disambiguation prior to run-
ning word2vec.

We note that word2vec and dep2vec use
a single vector per word approach and therefore
conflate the different senses of a polysemous lex-
eme. On the other hand, SENSEMBED utilises Ba-
belfy (Moro et al., 2014) as an external knowledge
source to perform word-sense disambiguation and
subsequently creates one vector representation per
word sense.

For composition we use pointwise addition for
all models as this has been shown to be a strong
baseline in a number of studies (Hashimoto et
al., 2014; Hill et al., 2016). We also experi-
mented with pointwise multiplication as compo-
sition function but, similar to Hill et al. (2016),
found its performance to be very poor (results not
reported). We model any out-of-vocabulary items
as a vector consisting of all zeros and determine
proximity of composed meaning representations
in terms of cosine similarity. We lowercase and
lemmatise the data in our task but do not perform
number or date normalisation, or removal of rare
words.

3 Phrase Similarity

Our first evaluation task is the benchmark phrase
similarity task of Mitchell and Lapata (2010). This
dataset consists of 108 adjective-noun (AN), 108
noun-noun (NN) and 108 verb-object (VO) pairs.
The task is to compare a compositional model’s
similarity estimates with human judgements by
computing Spearman’s ρ. An average ρ of 0.47-
0.48 represents the current state-of-the-art perfor-
mance on this task (Hashimoto et al., 2014; Kober
et al., 2016; Wieting et al., 2015).

For single-sense representations, the strategy
for carrying out this task is simple. For each
phrase in each pair, we compose the constituent
representations and then compute the similarity
of each pair of phrases using the cosine similar-
ity. For multi-sense representations, we adapted
the strategy which has been used successfully in
various word similarity experiments (Huang et al.,
2012; Iacobacci et al., 2015). Typically, for each
word pair, all pairs of senses are considered and
the similarity of the word pair is considered to be

4Available from: http://lcl.uniroma1.it/
sensembed/

80



the similarity of the closest pair of senses. The
fact that this strategy works well suggests that
when humans are asked to judge word similar-
ity, the pairing automatically primes them to se-
lect the closest senses. Extending this to phrase
similarity requires us to compose each possible
pair of senses for each phrase and then select
the sense configuration which results in maximal
phrase similarity. For comparison, we also give
results for the configuration which results in mini-
mal phrase similarity and the arithmetic mean5 of
all sense configurations.

3.1 Results

Model AN NN VO Average
word2vec 0.47 0.46 0.45 0.46
dep2vec 0.48 0.46 0.45 0.46
SENSEMBED:max 0.39 0.39 0.32 0.37
SENSEMBED:min 0.24 0.12 0.22 0.19
SENSEMBED:mean 0.46 0.35 0.37 0.39

Table 1
Results for the Mitchell and Laptata (2010) dataset.

Table 1 shows that the simple strategy of adding
high quality single-vector representations is very
competitive with the state-of-the-art for this task.
None of the strategies for selecting a sense config-
uration for the multi-sense representations could
compete with the single sense representations on
this task. One possible explanation is that the com-
monly adopted closest sense strategy is not effec-
tive for composition since the composition of in-
correct senses may lead to spuriously high similar-
ities (for two “implausible” sense configurations).

Table 2 lists a number of example phrase pairs
with low average human similarity scores in the
Mitchell and Lapata (2010) test set. The results
show the tendency of the closest sense strategy
with SENSEMBED (SE) to overestimate the sim-
ilarity of dissimilar phrase pairs. For a compari-
son we manually labelled the lexemes in the sam-
ple phrases with the appropriate BabelNet senses
prior to composition (SE*). Human (H) similar-
ity scores are normalised and averaged for an eas-
ier comparison, model estimates represent cosine
similarities.

4 Word Sense Discrimination

Word-sense discrimination can be seen as the first
stage of word-sense disambiguation, where the

5We also tried the geometric mean and the median but
these performed comparably with the arithmetic mean.

Phrase 1 Phrase 2 SE SE* H
buy land leave house 0.49 0.28 0.26
close eye stretch arm 0.40 0.31 0.25
wave hand leave company 0.42 0.08 0.20
drink water use test 0.29 0.04 0.18
european state present position 0.28 -0.03 0.19
high point particular case 0.41 0.10 0.21

Table 2
Tendency of SENSEMBED (SE) to overestimate the similarity
on phrase pairs with low average human similarity when the
closest sense strategy is used.

goal is to find whether two or more occurrences of
the same lexeme express identical senses, without
necessarily labelling the senses yet. It has received
relatively little attention despite its potential for
providing important insights into semantic com-
position, focusing in particular on to the ability
of compositional distributional semantic models to
appropriately contextualise a polysemous lexeme.

Work on word-sense discrimination has suf-
fered from the absence of a benchmark task as well
as a clear evaluation methodology. For example
Schütze (1998) evaluated his model on a dataset
consisting of 20 polysemous words (10 naturally
ambiguous lexemes and 10 artificially ambiguous
“pseudo-lexemes”) in terms of accuracy for coarse
grained sense distinctions, and an information re-
trieval task. Pantel and Lin (2002), and Van de
Cruys (2008) used automatically extracted words
from various newswire sources and evaluated the
output of their models in comparison to WordNet
and EuroWordNet, respectively. Purandare and
Pedersen (2004) used a subset of the words from
the SENSEVAL-2 task and evaluated their models
in terms of precision, recall and F1-score of how
well available sense tags match with clusters dis-
covered by their algorithms. Akkaya et al. (2012)
used the concatenation of the SENSEVAL-2 and
SENSEVAL-3 tasks and evaluated their models in
terms of cluster purity and accuracy. Finally,
Moen et al. (2013) used the semantic textual sim-
ilarity (STS) 2012 task, which is based on human
judgements of the similarity between two sen-
tences.

One contribution of our work is a novel word-
sense discrimination task, evaluated on a number
of robust baselines in order to facilitate future re-
search in that area. In particular, our task offers a
testbed for assessing the contextualisation ability
of compositional distributional semantic models.
The goal is, for a given polysemous lexeme in con-
text, to identify the sentence from a list of options

81



that is expressing the same sense of that lexeme
as the given target sentence. These two sentences
— the target and the “correct answer” — can ex-
hibit any degree of semantic similarity as long as
they convey the same sense of the target lexeme.
Table 3 shows an example of the polysemous ad-
jective black in our task. The goal of any model
would be to determine that the expressed sense of
black in the sentence She was going to set him
free from all of the evil and black hatred he had
brought to the world is identical to the expressed
sense of black in the target sentence Or should they
rebut the Democrats’ black smear campaign with
the evidence at hand.

Our task assesses the ability of a model to dis-
criminate a particular sense in a sentential context
from any other senses and thus provides an excel-
lent testbed for evaluating multi-sense word vec-
tor models as well as compositional distributional
semantic models. By composing the representa-
tion of a target lexeme with its surrounding con-
text, it should be possible to determine its sense.
For example, composing black smear campaign
should lead to a compositional representation that
is closer to the composed representation of black
hatred than to black mood, black sense of humour
or black coffee. This essentially uses the similarity
of the compositional representation of a lexeme’s
context to determine its sense. Similar approaches
to word-sense disambiguation have already been
successfully used in past works (Akkaya et al.,
2012; Basile et al., 2014).

4.1 Task Construction

For the construction of our dataset we made use of
data from two english dictionaries (Oxford Dictio-
nary and Collins Dictionary), accessible via their
respective web APIs6, as well as examples from
the sense annotated corpus SemCor (Miller et al.,
1993). Our use of dictionary data is motivated by
a number of favourable properties which make it a
very suitable data source for our proposed task:

• The content is of very high-quality and cu-
rated by expert lexicographers.

• All example sentences are carefully crafted in
order to unambiguously illustrate the usage

6https://developer.
oxforddictionaries.com for the Oxford Dictionary,
https://www.collinsdictionary.com/api/ for
the Collins Dictionary. We use NLTK 3.2 to access SemCor.

of a particular sense for a given polysemous
lexeme.

• The granularity of the sense inventory reflects
common language use7.

• The example sentences are typically free of
any domain bias wherever possible.

• The data is easily accessible via a web API.

By using the data from curated resources we were
able to avoid a setup as a sentence similarity task
and any potentially noisy crowd-sourced human
similarity judgements.

We were furthermore able to collect data from
varying frequency bands, enabling an assessment
of the impact of frequency on any model. Fig-
ure 1 shows the number of target lexemes per fre-
quency band. While the majority of lexemes, with
reference to a cleaned October 2013 Wikipedia
dump8, is in the middle band, there is a consid-
erable amount of less frequent lexemes. The most
frequent target lexeme in our task is the verb be
with ≈1.8m occurrences in Wikipedia, and the
least frequent lexeme is the verb ruffle with only
57 occurrences. The average target lexeme fre-
quency is ≈95k for adjectives, and ≈45k−46k for
nouns and verbs9.

Figure 1: Binned frequency distribution of the polysemous
target lexemes in our task.

7The Oxford dictionary lists 5 different senses for the
noun “bank”, whereas WordNet 3.0 lists 10 synsets, for ex-
ample distinguishing “bank” as the concept for a financial
institution and “bank” as a reference to the building where
financial transactions take place.

8We removed any articles with fewer than 20 page views.
9The overall number of unique word types is smaller than

the number of examples in our task as there are a number of
lexemes that can occur with more than one part-of-speech.

82



Sense Definition Sentence
Target full of anger or hatred Or should they rebut the Democrats’ black smear campaign with

the evidence at hand?
Option 1 full of anger or hatred She was going to set him free from all of the evil and black hatred

he had brought to the world.
Option 2 (of a person’s state of mind) I’ve been in a black mood since September 2001, it’s hanging over

full of gloom or misery; very depressed me like a penumbra.
Option 3 (of humour) presenting tragic or harrowing Over the years I have come to believe that fate either hates me, or

situations in comic terms has one hell of a black sense of humour.
Option 4 (of coffee or tea) served without milk The young man was reading a paperback novel and sipping a

steaming mug of hot, black coffee.

Table 3: Example of the polysemous adjective black in our task. The goal for any model is to predict option 1 as expressing the
same sense of black as the target sentence.

4.2 Task Setup Details
We collected data for 3 different parts-of-speech:
adjectives, nouns and verbs. We furthermore cre-
ated task setups with varying numbers of senses
to distinguish (2-5 senses) for a given target lex-
eme. This is to evaluate how well a model is able
to discriminate different degrees of polysemy of
any lexeme. For any task setup evaluating for n
senses, we included all lexemes with > n senses
and randomly sampled n senses from its inventory.
For each lexeme, we furthermore ensured that it
had at least 2 example sentences per sense. For
the available senses of any given lexeme, we ran-
domly chose a sense as the target sense, and from
its list of example sentences randomly sampled 2
sentences, one as the target example and one as
the “correct answer” for the list of candidate sen-
tences. Finally we once again randomly sampled
the required number of other senses and example
sentences to complete the task setup. Using ran-
dom sampling of word senses and targets aims to
avoid a predominant sense bias.

For each part-of-speech we created a develop-
ment split for parameter tuning and a test split for
the final evaluation. Table 4 shows the number of
examples for each setup variant of our task. The
biggest category are polysemous nouns, represent-
ing roughly half of the data, followed by verbs
representing another third, and the smallest cate-
gory are adjectives taking up the remaining≈17%.
We measure performance in terms of accuracy of

2 senses 3 senses 4 senses 5 senses
Adjective 66/209 47/170 37/137 28/115
Noun 170/618 125/499 100/412 74/345
Verb 127/438 71/354 72/295 56/256
Total 363/1265 263/1023 209/844 164/716

Table 4: Number of examples per part-of-speech and number
of senses (#dev examples/#train examples).

correctly predicting which two sentences share the

same sense of a given target lexeme. Accuracy has
the advantage of being much easier to interpret —
in absolute terms as well as in the relative differ-
ence between models — in comparison to other
commonly used evaluation metrics such as clus-
ter purity measures or correlation metrics such as
Spearman ρ and Pearson r.

4.3 Experimental Setup

In this paper we compare the compositional mod-
els outlined earlier with two baselines, a random
baseline and a word-overlap baseline of the ex-
tracted contexts. For the single-vector represen-
tations, we composed the target lexeme with all of
the words in the context window and compared it
with the equivalent representation of each of the
options (lexeme plus context words). The option
with the highest cosine similarity was deemed to
be the selected sense. For SENSEMBED, we com-
posed all sense vectors of a target lexeme with the
given context and then used the closest sense strat-
egy (Iacobacci et al., 2015) on composed represen-
tations to choose the predicted sense10. The word-
overlap baseline is simply the number of words in
common between the context window for the tar-
get and each of the options.

We experimented with symmetric linear bag-
of-words contexts of size 1, 2 and 4 around the
target lexeme. We also experimented with de-
pendency contexts, where first-order dependency
contexts performed almost identical to using a 2-
word bag-of-words context window (results not
reported). We excluded stop words prior to ex-
tracting the context window in order to maximise
the number of content words. We break ties for
any of the methods — including the baselines —
by randomly picking one of the options with the

10We also tried an all-by-all senses composition, however
found this to be computationally not tractable.

83



highest similarity to the composed representation
of the target lexeme with its context. Statistical
significance between the best performing model
and the word overlap baseline is computed by us-
ing a randomised pairwise permutation test (Efron
and Tibshirani, 1994).

4.4 Results

Table 5 shows the results for all context window
sizes across all parts-of-speech and number of
senses. All models substantially outperform the
random baseline for any number of senses. In-
terestingly the word overlap baseline is compet-
itive for all context window sizes. While it is
a very simple method, it has already been found
to be a strong baseline for paraphrase detection
and semantic textual similarity (Dinu and Thater,
2012). One possible explanation for its robust
performance on our task is an occurrence of the
one-sense-per-collocation hypothesis (Yarowsky,
1993). The performance of all other models
is roughly in the same ballpark for all parts-of-
speech and number of senses, suggesting that they
form robust baselines for future models. While
the results are relatively mixed for adjectives,
word2vec appears to be the strongest model for
polysemous nouns and verbs.

The perhaps most interesting observation in Ta-
ble 5 is that word2vec and dep2vec are per-
forming as well or better than SENSEMBED de-
spite the fact that the former conflate the senses of
a polysemous lexeme in a single vector represen-
tation. Figure 2 shows the average performance of
all models across parts-of-speech per number of
senses and for all context window sizes.

SENSEMBED and Babelfy

One possible explanation for SENSEMBED not
outperforming the other methods despite its
cleaner encoding of different word senses in the
above experiments is that at train time, it had ac-
cess to sense labels from Babelfy. At test time
on our task however, it did not have any sense
labels available. We therefore sense tagged the
5-sense noun subtask with Babelfy and re-ran
SENSEMBED. As Table 6 shows, access to sense
labels at test time did not give a substantive perfor-
mance boost, representing further support for our
hypothesis that composition in single-sense vector
models might be sufficient to recover sense spe-
cific information.

Frequency Range

We chose the 2-sense noun subtask to estimate the
degree sensitivity of target lexeme frequency on
our task we merged the [1, 1k) and [1k, 10k), and
[50k, 100k) and [100k,∞) frequency bands from
Figure 1, and sampled an equal number of target
words from each band. Table 7 reports the results
for this experiment. All methods outperform the
random and word overlap baseline and appear to
be working better for less frequent lexemes. One
possible explanation for this behaviour is that less
frequent lexemes have fewer senses and poten-
tially less subtle sense differences than more fre-
quent lexemes, which would make them easier to
discriminate by distributional semantic methods.

5 Discussion

Our results suggest that pointwise addition in a
single-sense vector model such as word2vec is
able to discriminate the sense of a polysemous
lexeme in context in a surprisingly effective way
and represents a strong baseline for future work.
Distributional composition can therefore be inter-
preted as a process of contextualising the mean-
ing of a lexeme. This way, composition does not
only act as a way to represent the meaning of a
phrase as a whole, but also as a local discrimina-
tor for any lexemes in the phrase. For example
the composed representation of dry clothes should
only keep contexts that dry shares with clothes
while suppressing contexts it shares with weather
or wine. Hence, one would expect the same to hap-
pen with a polysemous lexeme such as bank in the
context of river and account, respectively.

Recent work by Arora et al. (2016) has shown
that the different senses of a polysemous lexeme
reside in a linear substructure within a single vec-
tor and are recoverable by sparse coding. There is
furthermore evidence that additive composition in
low-dimensional word embeddings approximates
an intersection of the contexts of two distributional
word vectors (Tian et al., 2015). It therefore seems
plausible that an intersective composition function
should be able to recover sense specific informa-
tion.

To qualitatively analyse this hypothesis we
used the word2vec and SENSEMBED vectors
to compose a small number of example phrases
by pointwise addition and calculated their top 5
nearest neighbours in terms of cosine similarity.
For SENSEMBED we manually sense tagged the

84



Symmetric context window of size 1
Adjective Noun Verb

Senses 2 3 4 5 2 3 4 5 2 3 4 5
Random 0.53 0.32 0.25 0.14 0.47 0.32 0.23 0.19 0.47 0.31 0.23 0.18
Word Overlap 0.63 0.46 0.47 0.40 0.55 0.40 0.37 0.34 0.54 0.44 0.38 0.29
word2vec 0.70 0.56 0.61† 0.54† 0.66‡ 0.52‡ 0.50‡ 0.44‡ 0.63‡ 0.56‡ 0.52‡ 0.43‡

dep2vec 0.65 0.64‡ 0.57 0.57‡ 0.64‡ 0.50‡ 0.49‡ 0.48‡ 0.63‡ 0.55‡ 0.50‡ 0.43‡

SENSEMBED 0.67 0.54 0.56 0.56† 0.64‡ 0.49‡ 0.50‡ 0.43‡ 0.62† 0.53‡ 0.49‡ 0.38†

Symmetric context window of size 2
Adjective Noun Verb

Senses 2 3 4 5 2 3 4 5 2 3 4 5
Random 0.53 0.32 0.25 0.14 0.47 0.32 0.23 0.19 0.47 0.31 0.23 0.18
Word Overlap 0.66 0.51 0.55 0.43 0.59 0.47 0.43 0.41 0.58 0.51 0.45 0.36
word2vec 0.70 0.64† 0.58 0.55 0.71‡ 0.63‡ 0.59‡ 0.54‡ 0.68‡ 0.64‡ 0.58‡ 0.49‡

dep2vec 0.71 0.65‡ 0.58 0.57‡ 0.70‡ 0.57‡ 0.55‡ 0.55‡ 0.66† 0.64‡ 0.54† 0.46†

SENSEMBED 0.72‡ 0.62 0.61 0.52 0.69‡ 0.56† 0.57‡ 0.51† 0.67‡ 0.65† 0.57 0.45
Symmetric context window of size 4

Adjective Noun Verb
Senses 2 3 4 5 2 3 4 5 2 3 4 5
Random 0.53 0.32 0.25 0.14 0.47 0.32 0.23 0.19 0.47 0.31 0.23 0.18
Word Overlap 0.67 0.55 0.58 0.51 0.62 0.50 0.49 0.45 0.59 0.55 0.50 0.40
word2vec 0.71 0.65† 0.65 0.57 0.73‡ 0.61‡ 0.62‡ 0.57‡ 0.71‡ 0.62† 0.57 0.53‡

dep2vec 0.72 0.66† 0.60 0.54 0.71‡ 0.55 0.56† 0.53† 0.67 0.62 0.54 0.50
SENSEMBED 0.75 0.59 0.62 0.55 0.69‡ 0.57† 0.58‡ 0.53† 0.68‡ 0.62† 0.55 0.47

Table 5
Performance overview for all parts-of-speech and number of senses, ‡ statistically significant at the p < 0.01 level in com-
parison to the Word Overlap baseline; † statistically significant at the p < 0.05 level in comparison to the Word Overlap
baseline.

Figure 2: Average performance across parts-of-speech per number of senses and context window.

Noun - 5 Senses
Context Window Size 1 2 4
word2vec 0.44 0.54 0.57
dep2vec 0.48 0.55 0.53
SENSEMBED 0.43 0.51 0.53
SENSEMBED & Babelfy 0.45 0.49 0.54

Table 6
Results on the 5-sense noun subtask with SENSEMBED hav-
ing access to Babelfy sense labels at test time.

phrases with the appropriate BabelNet sense la-
bels prior to composition. We omitted the Babel-
Net sense labels in the neighbour list for brevity,

Noun - 2 Senses, context window size = 2
Frequency Band < 10k 10k – 50k ≥ 50k
Random 0.51 0.51 0.51
Word Overlap 0.66 0.60 0.56
word2vec 0.81 0.64 0.66
dep2vec 0.77 0.67 0.66
SENSEMBED 0.74 0.68 0.60

Table 7
Results on a subsample of the 2-sense noun subtask across
frequency bands.

however they were consistent with the intended
sense in all cases. Table 8 supports the view of
composition as a way of contextualising the mean-

85



ing of a lexeme. In all cases in our example the
word2vec neighbours reflect the intended sense
of the polysemous lexeme, providing evidence for
the linear substructure of word senses in a single
vector as discovered by Arora et al. (2016), and
suggesting that distributional composition is able
to recover sense specific information from a poly-
semous lexeme. The very fine-grained sense-level
vector space of SENSEMBED is giving rise to a
very focused neighbourhood, however there does
not seem to be any advantage over word2vec
from a qualitative point of view when using simple
additive composition.

6 Related Work

The perhaps most popular tasks for evaluating the
ability of a model to capture or encode the differ-
ent senses of a polysemous lexeme in a given con-
text are the english lexical substitution task (Mc-
Carthy and Navigli, 2007) and the Microsoft sen-
tence completion challenge (Zweig and Burges,
2011). Both tasks require any model to fill an ap-
propriate word into a pre-defined slot in a given
sentential context. The sentence completion chal-
lenge provides a list of candidate words while the
english lexical substitution task does not. How-
ever, neither task focuses on polysemy and the en-
glish lexical substitution task conflates the prob-
lems of discriminating word senses and finding
meaning preserving substitutes.

Dictionary definitions have previously been
used to evaluate compositional distributional se-
mantic models where the goal is to match a dictio-
nary entry with its corresponding definition (Kart-
saklis et al., 2012; Polajnar and Clark, 2014).
These datasets are commonly set up as retrieval
tasks, but generally do not test the ability of a
model to disambiguate a polysemous word in con-
text, or discriminate multiple definitions of the
same word.

Our task also provides a novel evaluation
for compositional distributional semantic models,
where the predominant strategy is to estimate the
similarity of two short phrases (Bernardi et al.,
2013; Grefenstette and Sadrzadeh, 2011; Kart-
saklis and Sadrzadeh, 2014; Mitchell and Lap-
ata, 2008; Mitchell and Lapata, 2010) or sen-
tences (Agirre et al., 2016; Huang et al., 2012;
Marelli et al., 2014) in comparison to human pro-
vided gold-standard judgements. One problem
with these similarity tasks is that the similarity

or relatedness of two sentences is very difficult
to judge — especially on a fine-grained scale —
even for humans. This frequently results in a rel-
atively high variance of judgements and low inter-
annotator agreement (Batchkarov et al., 2016).
The short phrase datasets typically have a fixed
structure that only test a very small fraction of the
possible grammatical constructions in which a lex-
eme can occur, and furthermore provide very little
context. The use of full sentences remedies the
lack of context and grammatical variation, how-
ever can still contain a significant level of noise
due to the automatic construction of the dataset or
the variance in human ratings. In contrast, our
task is not set up as a sentence similarity task
and therefore avoids the use of human similarity
judgements.

Our task is similar to word-sense induction
(WSI), however we only focus on discriminating
the sense of a polysemous lexeme in context rather
than inducing a set of senses from raw data and
appropriately tagging subsequent occurrences of
polysemous instances with the inferred inventory.
Separating the sense discrimination task from the
problem of sense induction has the advantage of
making our task applicable to evaluating composi-
tional distributional semantic models in order to
test their ability to appropriately contextualise a
polysemous lexeme. Due to not requiring any
models to perform an extra step for sense induc-
tion, our task is easier to evaluate as no matching
between sense clusters identified by a model and
some gold standard sense classes needs to be per-
formed, as typically proposed in the WSI litera-
ture (Agirre and Soroa, 2007; Manandhar et al.,
2010).

Most closely related to our task are the Stan-
ford Contextual Word Similarity (SCWS) dataset
by Huang et al. (2012) and the Usage Similar-
ity (USim) task by Erk et al. (2009). The goal
in both tasks is to estimate the similarity of two
polysemous words in context in comparison to
human provided gold standard judgements. In
the SCWS dataset typically two different lexemes
are considered whereas in USim and our task the
same lexemes with different contexts are com-
pared. Instead of relying on crowd-sourced hu-
man gold-standard similarity judgements, which
can be prone to a considerable amount of noise11,

11For example the average standard deviation of human
ratings in the SCWS dataset is ≈3 on a 10-point scale, and

86



Phrase word2vec neighbours SENSEMBED neighbours
river bank bank, river, creek, lake, rivers bank, river, stream, creek, river basin
bank account account, bank, accounts, banks, citibank bank, banks, the bank, pko bank polski, handlowy
dry weather weather, dry, wet weather, wet, unreasonably warm dry, weather, humid, cold, cool
dry clothes dry, clothes, clothing, rinse thoroughly, wet dry, clothes, warm, cold, wet
capital city capital, city, cities, downtown, town city, capital, the capital city, town, provincial capital
capital asset capital, asset, assets, investment, worth capital, asset, investment, assets, investor
power plant plant, power, plants, coalfired, megawatt power, plant, near-limitless, pulse-power, power of the wind
garden plant plant, garden, plants, gardens, vegetable garden plant, garden, plants, oakville assembly, solanaceous
window bar bar, window, windows, doorway, door window, bar, windows, glass window, wall
sandwich bar bar, sandwich, restaurant, burger, diner sandwich, bar, restaurant, hot dog, cake
gasoline tank gasoline, tank, fuel, gallon, tanks gasoline, tank, fuel, petrol, kerosene
armored tank armored, tank, tanks, M1A1 Abrams, armored vehicle armored, armoured, tank, tanks, light tank
desert rock rock, desert, rocks, desolate expanse, arid desert desert rock, the desert, deserts, badlands
rock band rock, band, rockers, bands, indie rock band, rock, group, the band, rock group

Table 8
Nearest neighbours of composed phrases for word2vec and SENSEMBED. Distributional composition in word2vec is able
to recover sense specific information remarkably well. Some neighbours are phrases because they have been encoded as a
single token in the original vector space.

we leverage the high-quality content of available
english dictionaries. Furthermore, our task is not
formulated as estimating the similarity between
two lexemes in context, but identifying the sen-
tences that use the same sense of a given polyse-
mous lexeme.

7 Conclusion

While elementary multi-sense representations of
words might capture a more fine grained semantic
picture of a polysemous word, that advantage does
not appear to transfer to distributional composi-
tion in a straightforward way. Our experiments
on a popular phrase similarity benchmark and our
novel word-sense discrimination task have demon-
strated that semantic composition does not appear
to benefit from a fine grained sense inventory, but
that the ability to contextualise a polysemous lex-
eme in single-sense vector models is sufficient for
superior performance. We furthermore have pro-
vided qualitative and quantitative evidence that an
intersective composition function such as point-
wise addition for neural word embeddings is able
to discriminate the meaning of a word in context,
and is able to recover sense specific information
remarkably well.

Lastly, our experiments have uncovered an im-
portant question for multi-sense vector models,
namely how to exploit the fine-grained sense
level representations for distributional composi-
tion. Our novel word-sense discrimination task
provides an excellent testbed for compositional
distributional semantic models, both following
a single-sense or multi-sense vector modelling

can be up to 4–5 in some cases.

paradigm, due to its focus on assessing the abil-
ity of a model to appropriately contextualise the
meaning of a word. Our task furthermore pro-
vides another evaluation option away from intrin-
sic evaluations which are based on often noisy hu-
man similarity judgements, while also not being
embedded in a downstream task.

In future work we aim to extend our eval-
uation to more complex compositional distribu-
tional semantic models such as the lexical func-
tion model (Paperno et al., 2014) or the Anchored
Packed Dependency Tree framework (Weir et al.,
2016). We furthermore want to investigate how
far the sense-discriminating ability of composition
can be leveraged for other tasks.

Acknowledgments

We would like to thank our anonymous reviewers
for their helpful comments.

References
Eneko Agirre and Aitor Soroa. 2007. Semeval-2007

task 02: Evaluating word sense induction and dis-
crimination systems. In Proceedings of the Fourth
International Workshop on Semantic Evaluations
(SemEval-2007), pages 7–12, Prague, Czech Repub-
lic, June. Association for Computational Linguis-
tics.

Eneko Agirre, Carmen Banea, Daniel Cer, Mona
Diab, Aitor Gonzalez-Agirre, Rada Mihalcea, Ger-
man Rigau, and Janyce Wiebe. 2016. Semeval-
2016 task 1: Semantic textual similarity, mono-
lingual and cross-lingual evaluation. In Proceed-
ings of the 10th International Workshop on Seman-
tic Evaluation (SemEval-2016), pages 497–511, San
Diego, California, June. Association for Computa-
tional Linguistics.

87



Cem Akkaya, Janyce Wiebe, and Rada Mihalcea.
2012. Utilizing semantic composition in distribu-
tional semantic models for word sense discrimina-
tion and word sense disambiguation. In Proceedings
of ICSC, pages 45–51.

Sanjeev Arora, Yuanzhi Li, Yingyu Liang, Tengyu Ma,
and Andrej Risteski. 2016. Linear algebraic struc-
ture of word senses, with applications to polysemy.
CoRR, abs/1601.03764.

Pierpaolo Basile, Annalina Caputo, and Giovanni Se-
meraro. 2014. An enhanced lesk word sense dis-
ambiguation algorithm through a distributional se-
mantic model. In Proceedings of COLING 2014,
the 25th International Conference on Computational
Linguistics: Technical Papers, pages 1591–1600,
Dublin, Ireland, August. Dublin City University and
Association for Computational Linguistics.

Miroslav Batchkarov, Thomas Kober, Jeremy Reffin,
Julie Weeds, and David Weir. 2016. A critique of
word similarity as a method of evaluating distribu-
tional semantic models. In Proceedings of the 1st
Workshop on Evaluating Vector-Space Representa-
tions for NLP, pages 7–12. Association for Compu-
tational Linguistics.

Raffaella Bernardi, Georgiana Dinu, Marco Marelli,
and Marco Baroni. 2013. A relatedness benchmark
to test the role of determiners in compositional dis-
tributional semantics. In Proceedings of the 51st An-
nual Meeting of the Association for Computational
Linguistics (Volume 2: Short Papers), pages 53–57,
Sofia, Bulgaria, August. Association for Computa-
tional Linguistics.

Xinxiong Chen, Zhiyuan Liu, and Maosong Sun.
2014. A unified model for word sense represen-
tation and disambiguation. In Proceedings of the
2014 Conference on Empirical Methods in Natu-
ral Language Processing (EMNLP), pages 1025–
1035, Doha, Qatar, October. Association for Com-
putational Linguistics.

Georgiana Dinu and Mirella Lapata. 2010. Measur-
ing distributional similarity in context. In Proceed-
ings of the 2010 Conference on Empirical Methods
in Natural Language Processing, pages 1162–1172,
Cambridge, MA, October. Association for Compu-
tational Linguistics.

Georgiana Dinu and Stefan Thater. 2012. Saarland:
Vector-based models of semantic textual similarity.
In *SEM 2012: The First Joint Conference on Lexi-
cal and Computational Semantics – Volume 1: Pro-
ceedings of the main conference and the shared task,
and Volume 2: Proceedings of the Sixth Interna-
tional Workshop on Semantic Evaluation (SemEval
2012), pages 603–607, Montréal, Canada, 7-8 June.
Association for Computational Linguistics.

Bradley Efron and Robert Tibshirani. 1994. An Intro-
duction to the Bootstrap. CRC press.

Katrin Erk, Diana McCarthy, and Nicholas Gaylord.
2009. Investigations on word senses and word us-
ages. In Proceedings of the Joint Conference of the
47th Annual Meeting of the ACL and the 4th In-
ternational Joint Conference on Natural Language
Processing of the AFNLP, pages 10–18, Suntec, Sin-
gapore, August. Association for Computational Lin-
guistics.

Edward Grefenstette and Mehrnoosh Sadrzadeh. 2011.
Experimental support for a categorical composi-
tional distributional model of meaning. In Proceed-
ings of the 2011 Conference on Empirical Meth-
ods in Natural Language Processing, pages 1394–
1404, Edinburgh, Scotland, UK., July. Association
for Computational Linguistics.

Kazuma Hashimoto, Pontus Stenetorp, Makoto Miwa,
and Yoshimasa Tsuruoka. 2014. Jointly learn-
ing word representations and composition functions
using predicate-argument structures. In Proceed-
ings of the 2014 Conference on Empirical Methods
in Natural Language Processing (EMNLP), pages
1544–1555, Doha, Qatar, October. Association for
Computational Linguistics.

Felix Hill, KyungHyun Cho, Anna Korhonen, and
Yoshua Bengio. 2016. Learning to understand
phrases by embedding the dictionary. Transactions
of the Association for Computational Linguistics,
4:17–30.

Eric Huang, Richard Socher, Christopher Manning,
and Andrew Ng. 2012. Improving word represen-
tations via global context and multiple word proto-
types. In Proceedings of the 50th Annual Meeting of
the Association for Computational Linguistics (Vol-
ume 1: Long Papers), pages 873–882, Jeju Island,
Korea, July. Association for Computational Linguis-
tics.

Ignacio Iacobacci, Mohammad Taher Pilehvar, and
Roberto Navigli. 2015. Sensembed: Learning
sense embeddings for word and relational similarity.
In Proceedings of the 53rd Annual Meeting of the
Association for Computational Linguistics and the
7th International Joint Conference on Natural Lan-
guage Processing (Volume 1: Long Papers), pages
95–105, Beijing, China, July. Association for Com-
putational Linguistics.

Ignacio Iacobacci, Mohammad Taher Pilehvar, and
Roberto Navigli. 2016. Embeddings for word sense
disambiguation: An evaluation study. In Proceed-
ings of the 54th Annual Meeting of the Association
for Computational Linguistics (Volume 1: Long Pa-
pers), pages 897–907, Berlin, Germany, August. As-
sociation for Computational Linguistics.

Dimitri Kartsaklis and Mehrnoosh Sadrzadeh. 2014. A
study of entanglement in a categorical framework of
natural language. In Proceedings of the 11th Work-
shop on Quantum Physics and Logic (QPL).

Dimitri Kartsaklis, Mehrnoosh Sadrzadeh, and Stephen
Pulman. 2012. A unified sentence space for

88



categorical distributional-compositional semantics:
Theory and experiments. In Proceedings of COL-
ING 2012: Posters, pages 549–558, Mumbai, India,
December. The COLING 2012 Organizing Commit-
tee.

Dimitri Kartsaklis, Mehrnoosh Sadrzadeh, and Stephen
Pulman. 2013. Separating disambiguation from
composition in distributional semantics. In Pro-
ceedings of the Seventeenth Conference on Com-
putational Natural Language Learning, pages 114–
123, Sofia, Bulgaria, August. Association for Com-
putational Linguistics.

Thomas Kober, Julie Weeds, Jeremy Reffin, and David
Weir. 2016. Improving sparse word representations
with distributional inference for semantic composi-
tion. In Proceedings of the 2016 Conference on Em-
pirical Methods in Natural Language Processing,
pages 1691–1702, Austin, Texas, November. Asso-
ciation for Computational Linguistics.

Omer Levy and Yoav Goldberg. 2014. Dependency-
based word embeddings. In Proceedings of the 52nd
Annual Meeting of the Association for Computa-
tional Linguistics (Volume 2: Short Papers), pages
302–308, Baltimore, Maryland, June. Association
for Computational Linguistics.

Jiwei Li and Dan Jurafsky. 2015. Do multi-sense em-
beddings improve natural language understanding?
In Proceedings of the 2015 Conference on Empiri-
cal Methods in Natural Language Processing, pages
1722–1732, Lisbon, Portugal, September. Associa-
tion for Computational Linguistics.

Suresh Manandhar, Ioannis Klapaftis, Dmitriy Dligach,
and Sameer Pradhan. 2010. Semeval-2010 task 14:
Word sense induction & disambiguation. In Pro-
ceedings of the 5th International Workshop on Se-
mantic Evaluation, pages 63–68, Uppsala, Sweden,
July. Association for Computational Linguistics.

Marco Marelli, Stefano Menini, Marco Baroni, Luisa
Bentivogli, Raffaella bernardi, and Roberto Zam-
parelli. 2014. A sick cure for the evaluation of
compositional distributional semantic models. In
Nicoletta Calzolari, Khalid Choukri, Thierry De-
clerck, Hrafn Loftsson, Bente Maegaard, Joseph
Mariani, Asuncion Moreno, Jan Odijk, and Stelios
Piperidis, editors, Proceedings of the Ninth Interna-
tional Conference on Language Resources and Eval-
uation (LREC’14), pages 216–223, Reykjavik, Ice-
land, May. European Language Resources Associa-
tion (ELRA). ACL Anthology Identifier: L14-1314.

Diana McCarthy and Roberto Navigli. 2007. Semeval-
2007 task 10: English lexical substitution task. In
Proceedings of the Fourth International Workshop
on Semantic Evaluations (SemEval-2007), pages
48–53, Prague, Czech Republic, June. Association
for Computational Linguistics.

Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Cor-
rado, and Jeff Dean. 2013. Distributed representa-

tions of words and phrases and their composition-
ality. In C.J.C. Burges, L. Bottou, M. Welling,
Z. Ghahramani, and K.Q. Weinberger, editors, Ad-
vances in Neural Information Processing Systems
26, pages 3111–3119. Curran Associates, Inc.

George A. Miller, Claudia Leacock, Randee Tengi, and
Ross T. Bunker. 1993. A semantic concordance. In
Proceedings of the Arpa Workshop on Human Lan-
guage Technology, pages 303–308.

Jeff Mitchell and Mirella Lapata. 2008. Vector-based
models of semantic composition. In Proceedings of
the 46th Annual Meeting of the Association for Com-
putational Linguistics: Human Language Technol-
ogy Conference, pages 236–244, Columbus, Ohio,
June. Association for Computational Linguistics.

Jeff Mitchell and Mirella Lapata. 2010. Composition
in distributional models of semantics. Cognitive Sci-
ence, 34(8):1388–1429.

Hans Moen, Erwin Marsi, and Björn Gambäck. 2013.
Towards dynamic word sense discrimination with
random indexing. In Proceedings of the Workshop
on Continuous Vector Space Models and their Com-
positionality, pages 83–90, Sofia, Bulgaria, August.
Association for Computational Linguistics.

Andrea Moro, Alessandro Raganato, and Roberto Nav-
igli. 2014. Entity linking meets word sense disam-
biguation: A unified approach. Transactions of the
Association for Computational Linguistics, 2:231–
244.

Patrick Pantel and Dekang Lin. 2002. Discovering
word senses from text. In Proceedings of the Eighth
ACM SIGKDD International Conference on Knowl-
edge Discovery and Data Mining, KDD ’02, pages
613–619, New York, NY, USA. ACM.

Denis Paperno, Nghia The Pham, and Marco Baroni.
2014. A practical and linguistically-motivated ap-
proach to compositional distributional semantics. In
Proceedings of the 52nd Annual Meeting of the As-
sociation for Computational Linguistics (Volume 1:
Long Papers), pages 90–99, Baltimore, Maryland,
June. Association for Computational Linguistics.

Mohammad Taher Pilehvar and Nigel Collier. 2016.
De-conflated semantic representations. In Proceed-
ings of the 2016 Conference on Empirical Methods
in Natural Language Processing, pages 1680–1690,
Austin, Texas, November. Association for Computa-
tional Linguistics.

Tamara Polajnar and Stephen Clark. 2014. Improv-
ing distributional semantic vectors through context
selection and normalisation. In Proceedings of the
14th Conference of the European Chapter of the As-
sociation for Computational Linguistics, pages 230–
238, Gothenburg, Sweden, April. Association for
Computational Linguistics.

89



Amruta Purandare and Ted Pedersen. 2004. Word
sense discrimination by clustering contexts in vec-
tor and similarity spaces. In Hwee Tou Ng
and Ellen Riloff, editors, HLT-NAACL 2004 Work-
shop: Eighth Conference on Computational Natu-
ral Language Learning (CoNLL-2004), pages 41–
48, Boston, Massachusetts, USA, May 6 - May 7.
Association for Computational Linguistics.

Siva Reddy, Ioannis Klapaftis, Diana McCarthy, and
Suresh Manandhar. 2011. Dynamic and static pro-
totype vectors for semantic composition. In Pro-
ceedings of 5th International Joint Conference on
Natural Language Processing, pages 705–713, Chi-
ang Mai, Thailand, November. Asian Federation of
Natural Language Processing.

Joseph Reisinger and Raymond J. Mooney. 2010.
Multi-prototype vector-space models of word mean-
ing. In Human Language Technologies: The 2010
Annual Conference of the North American Chap-
ter of the Association for Computational Linguistics,
pages 109–117, Los Angeles, California, June. As-
sociation for Computational Linguistics.

Bahar Salehi, Paul Cook, and Timothy Baldwin. 2015.
A word embedding approach to predicting the com-
positionality of multiword expressions. In Proceed-
ings of the 2015 Conference of the North Ameri-
can Chapter of the Association for Computational
Linguistics: Human Language Technologies, pages
977–983, Denver, Colorado, May–June. Association
for Computational Linguistics.

Hinrich Schütze. 1998. Automatic word sense dis-
crimination. Computational Linguistics, 24(1):97–
123, mar.

Ran Tian, Naoaki Okazaki, and Kentaro Inui. 2015.
The mechanism of additive composition. CoRR,
abs/1511.08407.

Tim Van de Cruys. 2008. Using three way data for
word sense discrimination. In Proceedings of the
22nd International Conference on Computational
Linguistics (Coling 2008), pages 929–936, Manch-
ester, UK, August. Coling 2008 Organizing Com-
mittee.

David Weir, Julie Weeds, Jeremy Reffin, and Thomas
Kober. 2016. Aligning packed dependency trees: a
theory of composition for distributional semantics.
Computational Linguistics, special issue on Formal
Distributional Semantics, 42(4):727–761, Decem-
ber.

John Wieting, Mohit Bansal, Kevin Gimpel, and Karen
Livescu. 2015. From paraphrase database to com-
positional paraphrase model and back. Transactions
of the Association for Computational Linguistics,
3:345–358.

David Yarowsky. 1993. One sense per collocation. In
Proceedings of the Workshop on Human Language
Technology, HLT ’93, pages 266–271, Stroudsburg,

PA, USA. Association for Computational Linguis-
tics.

Geoffrey Zweig and J.C. Chris Burges. 2011. The
microsoft research sentence completion challenge.
Technical report, Microsoft Research.

90


