



















































Recognizing Implicit Discourse Relations via Repeated Reading: Neural Networks with Multi-Level Attention


Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 1224–1233,
Austin, Texas, November 1-5, 2016. c©2016 Association for Computational Linguistics

Recognizing Implicit Discourse Relations via Repeated Reading:
Neural Networks with Multi-Level Attention

Yang Liu1,2, Sujian Li1
1 Key Laboratory of Computational Linguistics, Peking University, MOE, China

2 ILCC, School of Informatics, University of Edinburgh, United Kingdom
{cs-ly, lisujian}@pku.edu.cn

Abstract

Recognizing implicit discourse relations is a
challenging but important task in the field
of Natural Language Processing. For such
a complex text processing task, different
from previous studies, we argue that it is
necessary to repeatedly read the arguments
and dynamically exploit the efficient features
useful for recognizing discourse relations.
To mimic the repeated reading strategy, we
propose the neural networks with multi-level
attention (NNMA), combining the attention
mechanism and external memories to gradu-
ally fix the attention on some specific words
helpful to judging the discourse relations.
Experiments on the PDTB dataset show that
our proposed method achieves the state-of-
art results. The visualization of the attention
weights also illustrates the progress that our
model observes the arguments on each level
and progressively locates the important words.

1 Introduction

Discourse relations (e.g., contrast and causality)
support a set of sentences to form a coherent
text. Automatically recognizing discourse relations
can help many downstream tasks such as question
answering and automatic summarization. Despite
great progress in classifying explicit discourse
relations where the discourse connectives (e.g.,
“because”, “but”) explicitly exist in the text,
implicit discourse relation recognition remains a
challenge due to the absence of discourse connec-
tives. Previous research mainly focus on exploring
various kinds of efficient features and machine
learning models to classify the implicit discourse

relations (Soricut and Marcu, 2003; Baldridge and
Lascarides, 2005; Subba and Di Eugenio, 2009;
Hernault et al., 2010; Pitler et al., 2009; Joty et
al., 2012). To some extent, these methods simulate
the single-pass reading process that a person quickly
skim the text through one-pass reading and directly
collect important clues for understanding the text.
Although single-pass reading plays a crucial role
when we just want the general meaning and do
not necessarily need to understand every single
point of the text, it is not enough for tackling
tasks that need a deep analysis of the text. In
contrast with single-pass reading, repeated reading
involves the process where learners repeatedly read
the text in detail with specific learning aims, and
has the potential to improve readers’ reading fluency
and comprehension of the text (National Institute
of Child Health and Human Development, 2000;
LaBerge and Samuels, 1974). Therefore, for the task
of discourse parsing, repeated reading is necessary,
as it is difficult to generalize which words are really
useful on the first try and efficient features should
be dynamically exploited through several passes of
reading .

Now, let us check one real example to elaborate
the necessity of using repeated reading in discourse
parsing.
Arg-1 : the use of 900 toll numbers has been

expanding rapidly in recent years

Arg-2 : for a while, high-cost pornography lines
and services that tempt children to dial (and
redial) movie or music information earned the
service a somewhat sleazy image

(Comparison - wsj 2100)

To identify the “Comparison” relation between

1224



the two arguments Arg-1 and Arg-2, the most crucial
clues mainly lie in some content, like “expanding
rapidly” in Arg-1 and “earned the service a
somewhat sleazy image” in Arg-2, since there exists
a contrast between the semantic meanings of these
two text spans. However, it is difficult to obtain
sufficient information for pinpointing these words
through scanning the argument pair left to right in
one pass. In such case, we follow the repeated
reading strategy, where we obtain the general
meaning through reading the arguments for the first
time, re-read them later and gradually pay close
attention to the key content.

Recently, some approaches simulating repeated
reading have witnessed their success in different
tasks. These models mostly combine the attention
mechanism that has been originally designed to
solve the alignment problem in machine trans-
lation (Bahdanau et al., 2014) and the external
memory which can be read and written when
processing the objects (Sukhbaatar et al., 2015).
For example, Kumar et al. (2015) drew attention to
specific facts of the input sequence and processed
the sequence via multiple hops to generate an
answer. In computation vision, Yang et al. (2015)
pointed out that repeatedly giving attention to
different regions of an image could gradually lead
to more precise image representations.

Inspired by these recent work, for discourse
parsing, we propose a model that aims to repeatedly
read an argument pair and gradually focus on
more fine-grained parts after grasping the global
information. Specifically, we design the Neural
Networks with Multi-Level Attention (NNMA)
consisting of one general level and several attention
levels. In the general level, we capture the
general representations of each argument based on
two bidirectional long short-term memory (LSTM)
models. For each attention level, NNMA generates
a weight vector over the argument pair to locate
the important parts related to the discourse relation.
And an external short-term memory is designed to
store the information exploited in previous levels
and help update the argument representations. We
stack this structure in a recurrent manner, mimicking
the process of reading the arguments multiple times.
Finally, we use the representation output from the
highest attention level to identify the discourse

relation. Experiments on the PDTB dataset show
that our proposed model achieves the state-of-art
results.

2 Repeated Reading Neural Network with
Multi-Level Attention

In this section, we describe how we use the neural
networks with multi-level attention to repeatedly
read the argument pairs and recognize implicit
discourse relations.

First, we get the general understanding of the
arguments through skimming them. To implement
this, we adopt the bidirectional Long-Short Term
Memory Neural Network (bi-LSTM) to model each
argument, as bi-LSTM is good at modeling over a
sequence of words and can represent each word with
consideration of more contextual information. Then,
several attention levels are designed to simulate the
subsequent multiple passes of reading. On each
attention level, an external short-term memory is
used to store what has been learned from previous
passes and guide which words should be focused on.
To pinpoint the useful parts of the arguments, the
attention mechanism is used to predict a probability
distribution over each word, indicating to what
degree each word should be concerned. The overall
architecture of our model is shown in Figure 1. For
clarity, we only illustrate two attention levels in the
figure. It is noted that we can easily extend our
model to more attention levels.

2.1 Representing Arguments with LSTM

The Long-Short Term Memory (LSTM) Neural
Network is a variant of the Recurrent Neural
Network which is usually used for modeling a
sequence. In our model, we adopt two LSTM neural
networks to respectively model the two arguments:
the left argument Arg-1 and the right argument Arg-
2.

First of all, we associate each word w in our
vocabulary with a vector representation xw ∈ RDe .
Here we adopt the pre-trained vectors provided by
GloVe (Pennington et al., 2014). Since an argument
can be viewed as a sequence of word vectors, let x1i
(x2i ) be the i-th word vector in argument Arg-1 (Arg-

1225



Arg-1 Arg-2

Attention Attention

Attention Attention

General 
Level

Attention 
Level 1

Attention 
Level 2

Mean 
Pooling

12R

11R 21R

22R

1M

2M

softmax

11a

12a

21a

22a

Weighted 
Pooling

10h

20R10R

1
1Lh11h

 1ih10h

11h 1ih


1
1Lh

20h 2ih21h 22Lh


2
2Lh2ih21h20h

Discourse 
Relation

Figure 1: Neural Network with Multi-Level
Attention. (Two attention levels are given here.)

2) and the two arguments can be represented as,

Arg-1 : [x11,x
1
2, · · · ,x1L1 ]

Arg-2 : [x21,x
2
2, · · · ,x2L2 ]

where Arg-1 has L1 words and Arg-2 has L2 words.
To model the two arguments, we briefly introduce

the working process how the LSTM neural networks
model a sequence of words. For the i-th time step,
the model reads the i-th word xi as the input and
updates the output vector hi as follows (Zaremba
and Sutskever, 2014).

ii = sigmoid(Wi[xi,hi−1] + bi) (1)

fi = sigmoid(Wf [xi,hi−1] + bf ) (2)

oi = sigmoid(Wo[xi,hi−1] + bo) (3)

c̃i = tanh(Wc[xi,hi−1] + bc) (4)

ci = ii ∗ c̃i + fi ∗ ci−1 (5)
hi = oi ∗ tanh(ci) (6)

where [ ] means the concatenation operation of
several vectors. i,f ,o and c denote the input
gate, forget gate, output gate and memory cell

respectively in the LSTM architecture. The input
gate i determines how much the input xi updates the
memory cell. The output gate o controls how much
the memory cell influences the output. The forget
gate f controls how the past memory ci−1 affects
the current state. Wi,Wf ,Wo,Wc, bi, bf , bo, bc
are the network parameters.

Referring to the work of Wang and Nyberg
(2015), we implement the bidirectional version
of LSTM neural network to model the argument
sequence. Besides processing the sequence in
the forward direction, the bidirectional LSTM (bi-
LSTM) neural network also processes it in the
reverse direction. As shown in Figure 1, using two
bi-LSTM neural networks, we can obtain h1i =
[~h1i ,

~h1i ] for the i-th word in Arg-1 and h
2
i = [

~h2i ,
~h2i ]

for the i-th word in Arg-2, where ~h1i , ~h
2
i ∈ Rd

and ~h1i , ~h
2
i ∈ Rd are the output vectors from two

directions.
Next, to get the general-level representations of

the arguments, we apply a mean pooling operation
over the bi-LSTM outputs, and obtain two vectors
R10 and R

2
0, which can reflect the global information

of the argument pair.

R10 =
1

L1

L1∑

i=0

h1i (7)

R20 =
1

L2

L2∑

i=0

h2i (8)

2.2 Tuning Attention via Repeated Reading
After obtaining the general-level representations
by treating each word equally, we simulate the
repeated reading and design multiple attention
levels to gradually pinpoint those words particularly
useful for discourse relation recognition. In each
attention level, we adopt the attention mechanism
to determine which words should be focused on.
An external short-term memory is designed to
remember what has seen in the prior levels and guide
the attention tuning process in current level.

Specifically, in the first attention level, we
concatenate R10, R

2
0 and R

1
0−R20 and apply a

non-linear transformation over the concatenation to
catch the general understanding of the argument
pair. The use of R10−R20 takes a cue from the
difference between two vector representations which

1226



has been found explainable and meaningful in many
applications (Mikolov et al., 2013). Then, we get
the memory vector M1 ∈ Rdm of the first attention
level as

M1 = tanh(Wm,1[R
1
0,R

2
0,R

1
0−R20]) (9)

where Wm,1 ∈ Rdm×6d is the weight matrix.
With M1 recording the general meaning of

the argument pair, our model re-calculates the
importance of each word. We assign each word a
weight measuring to what degree our model should
pay attention to it. The weights are so-called
“attention” in our paper. This process is designed to
simulate the process that we re-read the arguments
and pay more attention to some specific words with
an overall understanding derived from the first-pass
reading. Formally, for Arg-1, we use the memory
vector M1 to update the representation of each
word with a non-linear transformation. According
to the updated word representations o11, we get the
attention vector a11.

h1 = [h10,h
1
1, · · · ,h1L1 ] (10)

o11 = tanh(W
1
a,1h

1 + W 1b,1(M1 ⊗ e)) (11)
a11 = softmax(W

1
s,1o

1
1) (12)

where h1 ∈ R2d×L1 is the concatenation of all
LSTM output vectors of Arg-1. e ∈ RL1 is a
vector of 1s and the M1 ⊗ e operation denotes that
we repeat the vector M1 L1 times and generate a
dm × L1 matrix. The attention vector a11 ∈ RL1
is obtained through applying a softmax operation
over o11. Wa,1

1 ∈ R2d×2d,Wb,11 ∈ R2d×dm and
Ws,1

1 ∈ R1×2d are the transformation weights. It is
noted that the subscripts denote the current attention
level and the superscripts denote the corresponding
argument. In the same way, we can get the attention
vector a21 for Arg-2.

Then, according to a11 and a
2
1, our model re-reads

the arguments and get the new representations R11
and R21 for the first attention level.

R11 = h
1(a11)

T (13)

R21 = h
2(a21)

T (14)

Next, we iterate the “memory-attention-
representation” process and design more attention

levels, giving NNMA the ability to gradually infer
more precise attention vectors. The processing
of the second or above attention levels is slightly
different from that of the first level, as we update
the memory vector in a recurrent way. To formalize,
for the k-th attention level (k ≥ 2), we use the
following formulae for Arg-1.

Mk = tanh(Wm,k[R
1
k−1,R2k−1,R1k−1−R2k−1,Mk−1])

(15)

o1k = tanh(W
1
a,kh

1 + W 1b,k(Mk ⊗ e)) (16)
a1k = softmax(W

1
s,ko

1
k) (17)

R1k = h
1(a1k)

T (18)

In the same way, we can computer o2k,a
2
k and R

2
k

for Arg-2.
Finally, we use the newest representation derived

from the top attention level to recognize the
discourse relations. Suppose there are totally K
attention levels and n relation types, the predicted
discourse relation distribution P ∈ Rn is calculated
as

P = softmax(Wp[R
1
K ,R

2
K ,R

1
K−R2K ] + bp)

(19)

where Wp ∈ Rn×6d and bp ∈ Rn are the
transformation weights.

2.3 Model Training
To train our model, the training objective is defined
as the cross-entropy loss between the outputs of
the softmax layer and the ground-truth class labels.
We use stochastic gradient descent (SGD) with
momentum to train the neural networks.

To avoid over-fitting, dropout operation is applied
on the top feature vector before the softmax layer.
Also, we use different learning rates λ and λe
to train the neural network parameters Θ and the
word embeddings Θe referring to (Ji and Eisenstein,
2015). λe is set to a small value for preventing over-
fitting on this task. In the experimental part, we will
introduce the setting of the hyper-parameters.

3 Experiments

3.1 Preparation
We evaluate our model on the Penn Discourse
Treebank (PDTB) (Prasad et al., 2008). In our work,

1227



we experiment on the four top-level classes in this
corpus as in previous work (Rutherford and Xue,
2015). We extract all the implicit relations of PDTB,
and follow the setup of (Rutherford and Xue, 2015).
We split the data into a training set (Sections 2-
20), development set (Sections 0-1), and test set
(Section 21-22). Table 1 summarizes the statistics of
the four PDTB discourse relations, i.e., Comparison,
Contingency, Expansion and Temporal.

Relation Train Dev Test
Comparison 1855 189 145
Contingency 3235 281 273
Expansion 6673 638 538
Temporal 582 48 55
Total 12345 1156 1011

Table 1: Statistics of Implicit Discourse Relations in
PDTB.

We first convert the tokens in PDTB to lowercase.
The word embeddings used for initializing the word
representations are provided by GloVe (Pennington
et al., 2014), and the dimension of the embeddings
De is 50. The hyper-parameters, including the
momentum δ, the two learning rates λ and λe,
the dropout rate q, the dimension of LSTM output
vector d, the dimension of memory vector dm are all
set according to the performance on the development
set Due to space limitation, we do not present the
details of tuning the hyper-parameters and only give
their final settings as shown in Table 2.

δ λ λe q d dm
0.9 0.01 0.002 0.1 50 200

Table 2: Hyper-parameters for Neural Network with
Multi-Level Attention.

To evaluate our model, we adopt two kinds of
experiment settings. The first one is the four-
way classification task, and the second one is the
binary classification task, where we build a one-
vs-other classifier for each class. For the second
setting, to solve the problem of unbalanced classes
in the training data, we follow the reweighting
method of (Rutherford and Xue, 2015) to reweigh
the training instances according to the size of each
relation class. We also use visualization methods to
analyze how multi-level attention helps our model.

3.2 Results

First, we design experiments to evaluate the effec-
tiveness of attention levels and how many attention
levels are appropriate. To this end, we implement
a baseline model (LSTM with no attention) which
directly applies the mean pooling operation over
LSTM output vectors of two arguments without
any attention mechanism. Then we consider
different attention levels including one-level, two-
level and three-level. The detailed results are shown
in Table 3. For four-way classification, macro-
averaged F1 and Accuracy are used as evaluation
metrics. For binary classification, F1 is adopted to
evaluate the performance on each class.

System
Four-way Binary
F1 Acc. Comp. Cont. Expa. Temp.

LSTM 39.40 54.50 33.72 44.79 68.74 33.14
NNMA

(one-level)
43.48 55.59 34.72 49.47 68.52 36.70

NNMA
(two-level)

46.29 57.17 36.70 54.48 70.43 38.84

NNMA
(three-level)

44.95 57.57 39.86 53.69 69.71 37.61

Table 3: Performances of NNMA with Different
Attention Levels.

From Table 3, we can see that the basic LSTM
model performs the worst. With attention levels
added, our NNMA model performs much better.
This confirms the observation above that one-pass
reading is not enough for identifying the discourse
relations. With respect to the four-way F1 measure,
using NNMA with one-level attention produces a
4% improvement over the baseline system with
no attention. Adding the second attention level
gives another 2.8% improvement. We perform
significance test for these two improvements, and
they are both significant under one-tailed t-test (p <
0.05). However, when adding the third attention
level, the performance does not promote much and
almost reaches its plateau. We can see that three-
level NNMA experiences a decease in F1 and a
slight increase in Accuracy compared to two-level
NNMA. The results imply that with more attention
levels considered, our model may perform slightly
better, but it may incur the over-fitting problem
due to adding more parameters. With respect to
the binary classification F1 measures, we can see

1228



System
Four-way Binary
F1 Acc. Comp. Cont. Expa. Expa.+EntRel Temp.

P&C2012 - - 31.32 49.82 - 79.22 26.57
J&E2015 - - 35.93 52.78 - 80.02 27.63

Zhang2015 38.80 55.39 32.03 47.08 68.96 80.22 20.29
R&X2014 38.40 55.50 39.70 54.40 70.20 80.44 28.70
R&X2015 40.50 57.10 41.00 53.80 69.40 - 33.30
B&D2015 - - 36.36 55.76 61.76 - 27.30
Liu2016 44.98 57.27 37.91 55.88 69.97 - 37.17
Ji2016 42.30 59.50 - - - - -

NNMA(two-level) 46.29 57.17 36.70 54.48 70.43 80.73 38.84
NNMA(three-level) 44.95 57.57 39.86 53.69 69.71 80.86 37.61

Table 4: Comparison with the State-of-the-art Approaches.

that the “Comparison” relation needs more passes
of reading compared to the other three relations.
The reason may be that the identification of the
“Comparison” depends more on some deep analysis
such as semantic parsing, according to (Zhou et al.,
2010).

Next, we compare our models with six state-of-
the-art baseline approaches, as shown in Table 4.
The six baselines are introduced as follows.

• P&C2012: Park and Cardie (2012) designed
a feature-based method and promoted the
performance through optimizing the feature
set.
• J&E2015: Ji and Eisenstein (2015) used two

recursive neural networks on the syntactic
parse tree to induce the representation of the
arguments and the entity spans.
• Zhang2015: Zhang et al. (2015) proposed

to use shallow convolutional neural networks
to model two arguments respectively. We
replicated their model since they used a
different setting in preprocessing PDTB.
• R&X2014, R&X2015: Rutherford and Xue

(2014) selected lexical features, production
rules, and Brown cluster pairs, and fed them
into a maximum entropy classifier. Rutherford
and Xue (2015) further proposed to gather extra
weakly labeled data based on the discourse
connectives for the classifier.
• B&D2015: Braud and Denis (2015) combined

several hand-crafted lexical features and word
embeddings to train a max-entropy classifier.

• Liu2016: Liu et al. (2016) proposed to better
classify the discourse relations by learning
from other discourse-related tasks with a multi-
task neural network.

• Ji2016: Ji et al. (2016) proposed a neural
language model over sequences of words and
used the discourse relations as latent variables
to connect the adjacent sequences.

It is noted that P&C2012 and J&E2015 merged
the “EntRel” relation into the “Expansion” rela-
tion1. For a comprehensive comparison, we also
experiment our model by adding a Expa.+EntRel vs
Other classification. Our NNMA model with two
attention levels exhibits obvious advantages over the
six baseline methods on the whole. It is worth
noting that NNMA is even better than the R&X2015
approach which employs extra data.

As for the performance on each discourse
relation, with respect to the F1 measure, we can
see that our NNMA model can achieve the best
results on the “Expansion”, “Expansion+EntRel”
and “Temporal” relations and competitive results on
the “Contingency” relation . The performance of
recognizing the “Comparison” relation is only worse
than R&X2014 and R&X2015. As (Rutherford and
Xue, 2014) stated, the “Comparison” relation is
closely related to the constituent parse feature of the
text, like production rules. How to represent and

1EntRel is the entity-based coherence relation which is
independent of implicit and explicit relations in PDTB.
However some research merges it into the implicit Expansion
relation.

1229



exploit these information in our model will be our
next research focus.

3.3 Analysis of Attention Levels
The multiple attention levels in our model greatly
boost the performance of classifying implicit dis-
course relations. In this subsection, we perform both
qualitative and quantitative analysis on the attention
levels.

First, we take a three-level NNMA model for
example and analyze its attention distributions on
different attention levels by calculating the mean
Kullback-Leibler (KL) Divergence between any two
levels on the training set. In Figure 3, we use
klij to denote the KL Divergence between the ith

and the jthattention level and use klui to denote
the KL Divergence between the uniform distribution
and the ith attention level. We can see that each
attention level forms different attention distributions
and the difference increases in the higher levels.
It can be inferred that the 2nd and 3rd levels in
NNMA gradually neglect some words and pay more
attention to some other words in the arguments. One
point worth mentioning is that Arg-2 tends to have
more non-uniform attention weights, since klu2 and
klu3 of Arg-2 are much larger than those of Arg-
1. And also, the changes between attention levels
are more obvious for Arg-2 through observing the
values of kl12, kl13 and kl23. The reason may be
that Arg-2 contains more information related with
discourse relation and some words in it tend to
require focused attention, as Arg-2 is syntactically
bound to the implicit connective.

At the same time, we visualize the attention levels
of some example argument pairs which are analyzed
by the three-level NNMA. To illustrate the kth

attention level, we get its attention weights a1k and
a2k which reflect the contribution of each word and
then depict them by a row of color-shaded grids in
Figure 2.

We can see that the NNMA model focuses
on different words on different attention levels.
Interestingly, from Figure 2, we find that the 1st and
3rd attention levels focus on some similar words,
while the 2nd level is relatively different from them.
It seems that NNMA tries to find some clues (e.g.
“moscow could be suspended” in Arg-2a; “won
the business” in Arg-1b; “with great aplomb he

considers not only” in Arg-2c) for recognizing the
discourse relation on the 1st level, looking closely
at other words (e.g. “misuse of psychiatry against
dissenters” in Arg-2a; “a third party that” in Arg-1b;
“and support of hitler” in Arg-2c) on the 2nd level,
and then reconsider the arguments, focus on some
specific words (e.g. “moscow could be suspended”
in Arg-2a; “has not only hurt” in Arg-2b) and make
the final decision on the last level.

4 Related Work

4.1 Implicit Discourse Relation Classification

The Penn Discourse Treebank (PDTB) (Prasad et
al., 2008), known as the largest discourse corpus, is
composed of 2159 Wall Street Journal articles. Each
document is annotated with the predicate-argument
structure, where the predicate is the discourse
connective (e.g. while) and the arguments are two
text spans around the connective. The discourse
connective can be either explicit or implicit. In
PDTB, a hierarchy of relation tags is provided for
annotation. In our study, we use the four top-level
tags, including Temporal, Contingency, Comparison
and Expansion. These four core relations allow us
to be theory-neutral, since they are almost included
in all discourse theories, sometimes with different
names (Wang et al., 2012).

Implicit discourse relation recognition is often
treated as a classification problem. The first work to
tackle this task on PDTB is (Pitler et al., 2009). They
selected several surface features to train four binary
classifiers, each for one of the top-level PDTB
relation classes. Extending from this work, Lin et
al. (2009) further identified four different feature
types representing the context, the constituent parse
trees, the dependency parse trees and the raw text
respectively. Rutherford and Xue (2014) used brown
cluster to replace the word pair features for solving
the sparsity problem. Ji and Eisenstein (2015)
adopted two recursive neural networks to exploit
the representation of arguments and entity spans.
Very recently, Liu et al. (2016) proposed a two-
dimensional convolutional neural network (CNN) to
model the argument pairs and employed a multi-
task learning framework to boost the performance
by learning from other discourse-related tasks. Ji
et al. (2016) considered discourse relations as

1230



 
Arg-1a

th
e

w
or

ld
ps

yc
hi

at
ric

as
so

ci
at

io
n

vo
te

d
at an at

he
ns

pa
rle

y
to co

nd
iti

on
al

ly
re

ad
m

it
th

e

so
vi

et
un

io
n

1
2
3

Attention
Level

 
Arg-2a

m
os

co
w

co
ul

d
be su

sp
en

de
d

if th
e

m
isu

se
of ps

yc
hi

at
ry

ag
ai

ns
t

di
ss

en
te

rs
is di

sc
ov

er
ed

du
rin

g
a re

vi
ew

w
ith

in
a ye

ar

1
2
3

Attention
Level

(a) Example with Comparison relation

 
Arg-1b

bu
t

ib
m

w
ou

ld
ha

ve
w

on
th

e

bu
si

ne
ss

an
yw

ay
as a sa

le

to a th
ird

pa
rt

y
th

at
w

ou
ld

ha
ve

th
en

le
as

ed
th

e

eq
ui

pm
en

t
to th

e

cu
st

om
er

1
2
3

Attention
Level

 
Arg-2b

ib
m

ha
s

no
t

on
ly

hu
rt

its sh
or

t-
te

rm
re

ve
nu

e
ou

tlo
ok

bu
t

ha
s

al
so

be
en

lo
sin

g
m

on
ey

on its le
as

es

1
2
3

Attention
Level

(b) Example with Contingency relation

 
Arg-1c

no
w

sh
ift

in
g

hi
s

sc
en

e
fr

om
th

e

co
un

tr
y

he le
ft

at fiv
e

to th
e

en
gl

an
d

he ha
s

liv
ed

in fo
r

ne
ar

ly
30 ye

ar
s

, he ha
s

fa
sh

io
ne

d
a no

ve
l

in th
e

m
od

e
of he

nr
y

ja
m

es
an

d

e.
m

.
fo

rs
te

r

1
2
3

Attention
Level

 
Arg-2c

w
ith

gr
ea

t
ap

lo
m

b
he co

ns
id

er
s

no
t

on
ly

fil
ia

l
de

vo
tio

n
an

d

( ut
te

rly
re

pr
es

se
d

) se
xu

al
lo

ve
, bu

t

br
iti

sh
an

ti-
se

m
iti

sm
, th

e

ge
nt

ry
's im

pa
tie

nc
e

w
ith

de
m

oc
ra

cy
an

d

su
pp

or
t

of hi
tle

r
, an

d

th
e

m
or

al
pr

ob
le

m
at

ic
s

of lo
ya

lty

1
2
3

Attention
Level

(c) Example with Expansion relation

Figure 2: Visualization Examples: Illustrating Attentions Learned by NNMA. (The blue grid means the
the attention on this word is lower than the value of a uniform distribution and the red red grid means the
attention is higher than that.)

Arg-1 Arg-2
kl_12 0.00749 0.04177
kl_23 0.099784 0.502146
kl_13 0.085394 0.35212
kl_u1 0.123413 0.136228
kl_u2 0.156619 0.254844
kl_u3 0.162379 0.467976

0

0.1

0.2

0.3

0.4

0.5

0.6
Arg-1 Arg-2

l

Figure 3: KL-divergences between attention levels

latent variables connecting two token sequences and
trained a discourse informed language model.

4.2 Neural Networks and Attention
Mechanism

Recently, neural network-based methods have
gained prominence in the field of natural language
processing (Kim, 2014). Such methods are primar-
ily based on learning a distributed representation
for each word, which is also called a word
embedding (Collobert et al., 2011).

Attention mechanism was first introduced into
neural models to solve the alignment problem

between different modalities. Graves (2013)
designed a neural network to generate handwriting
based on a text. It assigned a window on the input
text at each step and generate characters based on the
content within the window. Bahdanau et al. (2014)
introduced this idea into machine translation, where
their model computed a probabilistic distribution
over the input sequence when generating each target
word. Tan et al. (2015) proposed an attention-
based neural network to model both questions and
sentences for selecting the appropriate non-factoid
answers.

In parallel, the idea of equipping the neural model
with an external memory has gained increasing
attention recently. A memory can remember what
the model has learned and guide its subsequent
actions. Weston et al. (2015) presented a neural
network to read and update the external memory in
a recurrent manner with the guidance of a question
embedding. Kumar et al. (2015) proposed a similar
model where a memory was designed to change the
gate of the gated recurrent unit for each iteration.

5 Conclusion

As a complex text processing task, implicit dis-
course relation recognition needs a deep analysis

1231



of the arguments. To this end, we for the first
time propose to imitate the repeated reading strategy
and dynamically exploit efficient features through
several passes of reading. Following this idea,
we design neural networks with multiple levels of
attention (NNMA), where the general level and the
attention levels represent the first and subsequent
passes of reading. With the help of external
short-term memories, NNMA can gradually update
the arguments representations on each attention
level and fix attention on some specific words
which provide effective clues to discourse relation
recognition. We conducted experiments on PDTB
and the evaluation results show that our model
can achieve the state-of-the-art performance on
recognizing the implicit discourse relations.

Acknowledgments

We thank all the anonymous reviewers for their
insightful comments on this paper. This work was
partially supported by National Key Basic Research
Program of China (2014CB340504), and National
Natural Science Foundation of China (61273278 and
61572049). The correspondence author of this paper
is Sujian Li.

References

Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua
Bengio. 2014. Neural machine translation by
jointly learning to align and translate. arXiv preprint
arXiv:1409.0473.

Jason Baldridge and Alex Lascarides. 2005. Proba-
bilistic head-driven parsing for discourse structure. In
Proceedings of CoNLL.

Chloé Braud and Pascal Denis. 2015. Comparing
word representations for implicit discourse relation
classification. In Proceedings of EMNLP.

Ronan Collobert, Jason Weston, Léon Bottou, Michael
Karlen, Koray Kavukcuoglu, and Pavel Kuksa. 2011.
Natural language processing (almost) from scratch.
The Journal of Machine Learning Research, 12:2493–
2537.

Alex Graves. 2013. Generating sequences with recurrent
neural networks. arXiv preprint arXiv:1308.0850.

Hugo Hernault, Helmut Prendinger, David A duVerle,
Mitsuru Ishizuka, et al. 2010. Hilda: a discourse
parser using support vector machine classification.
Dialogue and Discourse, 1(3):1–33.

Yangfeng Ji and Jacob Eisenstein. 2015. One
vector is not enough: Entity-augmented distributed
semantics for discourse relations. Transactions of
the Association for Computational Linguistics, 3:329–
344.

Yangfeng Ji, Gholamreza Haffari, and Jacob Eisenstein.
2016. A latent variable recurrent neural network for
discourse relation language models. arXiv preprint
arXiv:1603.01913.

Shafiq Joty, Giuseppe Carenini, and Raymond T Ng.
2012. A novel discriminative framework for sentence-
level discourse analysis. In Proceedings of EMNLP.

Yoon Kim. 2014. Convolutional neural networks for
sentence classification. In Proceedings of EMNLP.

Ankit Kumar, Ozan Irsoy, Jonathan Su, James Bradbury,
Robert English, Brian Pierce, Peter Ondruska, Ishaan
Gulrajani, and Richard Socher. 2015. Ask me
anything: Dynamic memory networks for natural lan-
guage processing. arXiv preprint arXiv:1506.07285.

Ziheng Lin, Min-Yen Kan, and Hwee Tou Ng. 2009.
Recognizing implicit discourse relations in the penn
discourse treebank. In Proceedings of EMNLP.

Yang Liu, Sujian Li, Xiaodong Zhang, and Zhifang Sui.
2016. Implicit discourse relation classification via
multi-task neural network. In Proceedings of AAAI.

Tomas Mikolov, Wen-tau Yih, and Geoffrey Zweig.
2013. Linguistic regularities in continuous space word
representations. In Proceedings of NAACL.

Joonsuk Park and Claire Cardie. 2012. Improving
implicit discourse relation recognition through feature
set optimization. In Proceedings of SigDial.

Jeffrey Pennington, Richard Socher, and Christopher D.
Manning. 2014. Glove: Global vectors for word
representation. In Proceedings of EMNLP 2014.

Emily Pitler, Annie Louis, and Ani Nenkova. 2009.
Automatic sense prediction for implicit discourse
relations in text. In Proceedings of ACL.

Rashmi Prasad, Nikhil Dinesh, Alan Lee, Eleni
Miltsakaki, Livio Robaldo, Aravind K. Joshi, and
Bonnie L. Webber. 2008. The Penn Discourse
TreeBank 2.0. In Proceedings of LREC.

Attapol Rutherford and Nianwen Xue. 2014. Dis-
covering implicit discourse relations through brown
cluster pair representation and coreference patterns. In
Proceedings of EACL.

Attapol T Rutherford and Nianwen Xue. 2015.
Improving the inference of implicit discourse relations
via classifying explicit discourse connectives. In
Proceedings of NAACL.

Radu Soricut and Daniel Marcu. 2003. Sentence
level discourse parsing using syntactic and lexical
information. In Proceedings of NAACL.

1232



Rajen Subba and Barbara Di Eugenio. 2009. An
effective discourse parser that uses rich linguistic
information. In Proceedings of NAACL.

Sainbayar Sukhbaatar, Jason Weston, Rob Fergus, et al.
2015. End-to-end memory networks. In Proceedings
of NIPS.

Ming Tan, Bing Xiang, and Bowen Zhou. 2015. Lstm-
based deep learning models for non-factoid answer
selection. arXiv preprint arXiv:1511.04108.

Di Wang and Eric Nyberg. 2015. A long short-
term memory model for answer sentence selection in
question answering. In Proceedings of EMNLP.

Xun Wang, Sujian Li, Jiwei Li, and Wenjie Li. 2012.
Implicit Discourse Relation Recognition by Selecting
Typical Training Examples. In Proceedings of
COLING.

Jason Weston, Sumit Chopra, and Antoine Bordes. 2015.
Memory networks. Proceedings of ICLR.

Zichao Yang, Xiaodong He, Jianfeng Gao, Li Deng,
and Alexander J. Smola. 2015. Stacked attention
networks for image question answering. arXiv
preprint arXiv:1511.02274.

Wojciech Zaremba and Ilya Sutskever. 2014. Learning
to execute. arXiv preprint arXiv:1410.4615.

Biao Zhang, Jinsong Su, Deyi Xiong, Yaojie Lu,
Hong Duan, and Junfeng Yao. 2015. Shallow
convolutional neural network for implicit discourse
relation recognition. In Proceedings of EMNLP.

Zhi-Min Zhou, Yu Xu, Zheng-Yu Niu, Man Lan, Jian
Su, and Chew Lim Tan. 2010. Predicting discourse
connectives for implicit discourse relation recognition.
In Proceedings of the ICCL, pages 1507–1514.

1233


