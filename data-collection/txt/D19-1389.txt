











































BottleSum: Unsupervised and Self-supervised Sentence Summarization using the Information Bottleneck Principle


Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing
and the 9th International Joint Conference on Natural Language Processing, pages 3752–3761,
Hong Kong, China, November 3–7, 2019. c©2019 Association for Computational Linguistics

3752

BottleSum: Unsupervised and Self-supervised Sentence Summarization
using the Information Bottleneck Principle

Peter West1 Ari Holtzman1,2 Jan Buys1 Yejin Choi1,2
1Paul G. Allen School of Computer Science & Engineering, University of Washington

2Allen Institute for Artificial Intelligence
{pawest, ahai, jbuys, yejin}@cs.washington.edu

Abstract

The principle of the Information Bottleneck
(Tishby et al., 1999) is to produce a summary
of information X optimized to predict some
other relevant information Y . In this paper, we
propose a novel approach to unsupervised sen-
tence summarization by mapping the Informa-
tion Bottleneck principle to a conditional lan-
guage modelling objective: given a sentence,
our approach seeks a compressed sentence that
can best predict the next sentence. Our iter-
ative algorithm under the Information Bottle-
neck objective searches gradually shorter sub-
sequences of the given sentence while max-
imizing the probability of the next sentence
conditioned on the summary. Using only pre-
trained language models with no direct super-
vision, our approach can efficiently perform
extractive sentence summarization over a large
corpus.

Building on our unsupervised extractive sum-
marization (BottleSumEx), we then present
a new approach to self-supervised abstrac-
tive summarization (BottleSumSelf ), where a
transformer-based language model is trained
on the output summaries of our unsupervised
method. Empirical results demonstrate that
our extractive method outperforms other un-
supervised models on multiple automatic met-
rics. In addition, we find that our self-
supervised abstractive model outperforms un-
supervised baselines (including our own) by
human evaluation along multiple attributes.

1 Introduction

Recent approaches based on neural networks have
brought significant advancements for both extrac-
tive and abstractive summarization (Rush et al.,
2015; Nallapati et al., 2016). However, their suc-
cess relies on large-scale parallel corpora of input
text and output summaries for direct supervision.
For example, there are ~280,000 training instances

Autoencoder

Information Bottleneck 

Hong Kong, a bustling metropolis 
with a population over 7 million, 

was once under British Rule.

Hong Kong has population over 7 million, 
was once under British Rule.

Hong Kong was once under British Rule.

Summary

predict loss

predict loss

Source

Summary

Next The city returned to 
Chinese control in 

1997.sentence

Figure 1: Example contrasting the Autoencoder (AE)
and Information Bottleneck (IB) approaches to summa-
rization. While AE (top) preserves any detail that helps
to reconstruct the original, such as population size in
this example, IB (bottom) uses context to determine
which information is relevant, which results in a more
appropriate summary.

in the CNN/Daily Mail dataset (Nallapati et al.,
2016; Hermann et al., 2015), and ~4,000,000 in-
stances in the sentence summarization dataset of
Rush et al. (2015). Because it is too costly to
have humans write gold summaries at this scale,
existing large-scale datasets are based on naturally
occurring pairs of summary-like text paired with
source text, for instance using news titles or high-
lights as summaries for news-text. A major draw-
back to this approach is that these pairs must al-
ready exist in-domain, which is often not true.

The sample inefficiency of current neural ap-
proaches limits their impact across different tasks
and domains, motivating the need for unsuper-
vised or self-supervised alternatives (Artetxe et al.,
2017; LeCun, 2018; Schmidhuber, 1990). Fur-
ther, for summarization in particular, the current
paradigm requiring millions of supervision exam-
ples is almost counter-intuitive; after all, humans
don’t need to see a million summaries to know
how to summarize, or what information to include.

In this paper, we present BottleSum, consisting



3753

of a pair of novel approaches, BottleSumEx and
BottleSum

Self for unsupervised extractive and self-
supervised abstractive summarization, respec-
tively. Core to our approach is the principle of the
Information Bottleneck (Tishby et al., 1999), pro-
ducing a summary for information X optimized to
predict some other relevant information Y. In par-
ticular, we map (conditional) language modeling
objectives to the Information Bottleneck principle
to guide the unsupervised model on what to keep
and what to discard.

The key intuition of our bottleneck-based sum-
marization is that a good sentence summary con-
tains information related to the broader context
while discarding less significant details. Figure 1
demonstrates this intuition. Given input sentence
“Hong Kong, a bustling metropolis with a popula-
tion over 7 million, ...”, which is followed by the
next sentence “The city returned to Chinese con-
trol in 1997”, the information bottleneck would
suggest that minute details such as the city’s pop-
ulation being over 7 million are relatively less im-
portant to keep. In contrast, the continued discus-
sion of the city’s governance in the next sentence
suggests its former British rule is important here.

This intuition contrasts with that of
autoencoder-based approaches where the goal is
to minimize the reconstruction loss of the input
sentence when constructing the summary (Miao
and Blunsom, 2016; Wang and Lee, 2018; Fevry
and Phang, 2018; Baziotis et al., 2019). Under
the reconstruction loss, minute but specific details
such as the city’s population being over 7 million
will be difficult to discard from the summary,
because they are useful for reconstruction.

Concretely, BottleSumEx is an extractive and un-
supervised sentence summarization method using
the next sentence, a sample of nearby context,
as guidance to relevance, or what information to
keep. We capture this with a conditional language
modelling objective, allowing us to benefit from
powerful deep neural language models that are
pre-trained over an extremely large-scale corpus.
Under the Information Bottleneck objective, we
present an iterative algorithm that searches grad-
ually shorter subsequences of the source sentence
while maximizing the probability of the next sen-
tence conditioned on the summary. The benefit of
this approach is that it requires no domain-specific
supervision or fine-turning.

Building on our unsupervised extractive sum-

marization, we then present BottleSumSelf , a new
approach to self-supervised abstractive summa-
rization. This method also uses a pretrained lan-
guage model, but turns it into an abstractive sum-
marizer by fine-tuning on the output summaries
generated by BottleSumEx paired with their orig-
inal input sentences. The goal is to generalize the
summaries generated by an extractive method by
training a language model on them, which can then
produce abstractive summaries as its generation is
not constrained to be extractive.

Together, BottleSumEx and BottleSumSelf are
BottleSum methods for unsupervised sentence
summarization. Empirical results demonstrate
that BottleSumEx outperforms other unsupervised
methods on multiple automatic metrics, closely
followed by BottleSumSelf . Furthermore, test-
ing on a large unsupervised corpus, we find
BottleSum

Self outperforms unsupervised baselines
(including our own BottleSumEx) on human evalu-
ation along multiple attributes.

2 The Information Bottleneck Principle

Unsupervised summarization requires formulating
an appropriate learning objective that can be opti-
mized without supervision (example summaries).
Recent work has treated unsupervised summariza-
tion as an autoencoding problem with a recon-
struction loss (Miao and Blunsom, 2016; Baziotis
et al., 2019). The goal is then to produce a com-
pressed summary from which the source sentence
can be accurately predicted, i.e. to maximize:

E
p(s̃|s) log p(s|s̃), (1)

where s is the source sentence, s̃ is the generated
summary and p(s̃|s) the learned summarization
model. The exact form of this loss may be more
elaborate depending on the system, for example
including an auxiliary language modeling loss, but
the main aim is to produce a summary from which
the source can be reconstructed.

The intuitive limitation of this approach is that
it will always prefer to retain all informative con-
tent from the source. This goes against the fun-
damental goal of summarization, which crucially
needs to forget all but the “relevant” information.
It should be detrimental to keep tangential infor-
mation, as illustrated by the example in Figure 1.
As a result, autoencoding systems need to intro-
duce additional loss terms to augment the recon-
struction loss (e.g. length penalty, or the topic loss



3754

of Baziotis et al. (2019)).
The premise of our work is that the Information

Bottleneck (IB) principle (Tishby et al., 1999) is a
more natural fit for summarization. Unlike recon-
struction loss, which requires augmentative terms
to summarize, IB naturally incorporates a tradeoff
between information selection and pruning. These
approaches are compared directly in section 5.

At its core, IB is concerned with the problem of
maximal compression while defining a formal no-
tion of information relevance. This is introduced
with an external variable Y . The key is that ˜S, the
summary of source S, contains only information
useful for predicting Y . This can be posed for-
mally as learning a conditional distribution p( ˜S|S)
minimizing:

I(

˜

S;S)� �I( ˜S;Y ), (2)

where I denotes mutual information between
these variables.

A notion of information relevance comes from
the second term, the relevance term: with a pos-
itive coefficient �, this is encouraging summaries
˜

S to contain information shared with Y . The first
term, or pruning term, ensures that irrelevant in-
formation is discarded. By minimizing the mutual
information between summary ˜S and source S,
any information about the source that is not cred-
ited by the relevance term is thrown away. The
statistical structure of IB makes this compressive
by forcing the summary to only contain informa-
tion shared with the source.1

In sum, IB relies on 3 principles:

1. Encouraging relevant information with a rel-
evance term.

2. Discouraging extra information with a prun-
ing term.

3. Strictly summarizing the source.

To clarify the difference from a reconstructive
loss, suppose there is irrelevant information in S
(i.e. unrelated to relevance variable Y ), call this
Z. With the IB objective (eq 3), there is no benefit
to keeping any information from Z, which strictly
makes the first term worse (more mutual informa-
tion between source and summary) and does not
affect the second (Z is unrelated to Y ). In contrast,

1In IB, this is a strict statistical relationship.

because Z contains information about S, includ-
ing it in ˜S could easily benefit the reconstructive
loss (eq. 1) despite being irrelevant.

As a relevance variable we will use the sentence
following the source in the document in which it
occurs. This choice is motivated by linguistic co-
hesion, in which we expect more broadly relevant
information to be common between consecutive
sentences, while less relevant information and de-
tails are often not carried forward.

We use these principles to derive two meth-
ods for sentence summarization. Our first method
(§3) enforces strict summarization through being
extractive. Additionally, it does not require any
training, so can be applied directly without the
availability of domain-specific data. The second
method (§4) generalizes IB-based summarization
to abstractive summarization that can be trained
on large unsupervised datasets, learning an ex-
plicit summarization function p(s̃|s) over a distri-
bution of inputs.

3 Unsupervised Extractive
Summarization

We now use the Information Bottleneck princi-
ple to propose BottleSumEx, an unsupervised ex-
tractive approach to sentence summarization. Our
approach does not require any training; only a
pretrained language model is required to satisfy
the IB principles of (2), and the stronger the lan-
guage model, the stronger our approach will be.
In section 5 we demonstrate the effectiveness of
this method using GPT-2, the pretrained language
model of Radford et al. (2019). 2

3.1 IB for Extractive Summarization
Here, we take advantage of the natural parallel be-
tween the Information Bottleneck and summariza-
tion developed in section 2. Working from the 3
IB principles stated there, we derive a set of ac-
tionable principles for a concrete sentence summa-
rization method.

We approach the task of summarizing a single
sentence s using the following sentence s

next

as
the relevance variable. The method will be a deter-
ministic function mapping s to the summary s̃, so
instead of learning a distribution over summaries,
we take p(s̃|s) = 1 for the summary we arrive at.
Our goal is then to optimize the IB equation (Eq 2)

2We use the originally released “small” 117M parameter
version.



3755

for a single example rather a distribution of inputs
(as in the original IB method).

In this setting, to minimize equation 2 we can
equivalently minimize:

� log p(s̃)��1p(snext|s̃)p(s̃) log p(snext|s̃), (3)

where coefficient �1 > 0 controls the trade-off be-
tween keeping relevant information and pruning.
See appendix A for the derivation of this equa-
tion. Similar to eq 2, the first term encourages
pruning, while the second encourages information
about the relevance variable, s

next

. Both unique
values in eq 3 (p(s̃) and p(s

next

|s̃)) can be esti-
mated directly by a pretrained language model, a
result of the summary being natural language as
well as our choice of relevance variable. This will
give us a direct path to enforcing IB principles 1
and 2 from section 2.

To interpret principle 3 for text, we consider
what attributes are important to strict textual sum-
marization. Simply, a strict textual summary
should be shorter than the source, while agree-
ing semantically. The first condition is straightfor-
ward but the second is currently infeasible to en-
sure with automatic systems, and so we instead en-
force extractive summarization to ensure the first
and encourage the second.

Without a supervised validation set, there is no
clear way to select a value for �1 in Eq 3 and so
no way to optimize this directly. Instead, we opt
to ensure both terms improve as our method pro-
ceeds. Thus, we are not comparing the pruning
and relevance terms directly (only ensuring mu-
tual progress), and so we optimize simpler quanti-
ties monotonic in the two terms instead: p(s̃) for
pruning and p(y|s̃) for relevance.

We perform extractive summarization by itera-
tively deleting words or phrases, starting with the
original sentence. At each elimination step, we
only consider candidate deletions which decrease
the value of the pruning term, i.e., increase the lan-
guage model score of the candidate summary. This
ensures progress on the pruning term, and also en-
forces the notion that word deletion should reduce
the information content of the summary. The rel-
evance term is optimized through only expanding
candidates that have the highest relevance scores
at each iteration, and picking the candidate with
the highest relevance score as final summary.

Altogether, this gives 3 principles for extractive
summarization with IB.

1. Maximize relevance term by maximizing
p(s

next

|s̃).

2. Prune information and enforce compression
by bounding: p(s̃

i+1) > p(s̃i).

3. Enforce strict summarization by extractive
word elimination.

3.2 Method

Algorithm 1 BottleSumEx method
Require: sentence s and context s

next

1: C  {s} . set of summary candidates
2: for l in length(s)...1 do
3: C

l

 {s0 2 C|len(s0) = l}
4: sort C

l

descending by p(s
next

|s0)
5: for s0 in C

l

[1 :k] do
6: l0  length(s0)
7: for j in 1...m do
8: for i in 1...(l0 � j) do
9: s00  s0[1 : i�1] � s0[i+j : l0]

10: if p(s00) > p(s0) then
11: C  C + {s00}
12: return argmax

s

02C
p(s

next

|s0)

We turn these principles into a concrete method
which iteratively produces summaries of decreas-
ing length by deleting consecutive words in can-
didate summaries (Algorithm 1). The relevance
term is optimized in two ways: first, only the top-
scoring summaries of each length are used to gen-
erate new, shorter summaries (line 5). Second, the
final summary is chosen explicitly by this measure
(line 12).

In order to satisfy the second condition, each
candidate must contain less self-information (i.e.,
have higher probability) than the candidate that
derives it. This ensures that each deletion (line 9)
strictly removes information. The third condition,
strict extractiveness, is satisfied per definition.

The algorithm has two parameters: m is the
max number of consecutive words to delete when
producing new summary candidates (line 9), and
k is the number of candidates at each length used
to generate shorter candidates by deletion (line 5).

4 Abstractive Summarization with
Extractive Self-Supervision

Next, we extend the unsupervised summarization
of BottleSumEx to abstractive summarization with



3756

BottleSum

Self , based on a straightforward tech-
nique for self-supervision. Simply, a large cor-
pus of unsupervised summaries is generated with
BottleSum

Ex using a strong language model, then
the same language model is tuned to produce sum-
maries from source sentences on that dataset.

The conceptual goal of BottleSumSelf is to use
BottleSum

Ex as a guide to learn the notion of in-
formation relevance as expressed through IB, but
in a way that (a) removes the restriction of extrac-
tiveness, to produce more natural outputs and (b)
learns an explicit compression function not requir-
ing a next sentence for decoding.

4.1 Extractive Dataset

The first step of BottleSumSelf is to produce a
large-scale dataset for self-supervision using the
BottleSum

Ex method set out in §3.2. The only re-
quirement for the input corpus is that next sen-
tences need to be available.

In our experiments, we generate a cor-
pus of 100,000 sentence-summary pairs with
BottleSum

Ex, using the same parameter settings as
in section 3. The resulting summaries have an av-
erage compression ratio (by character length) of
approximately 0.55.

4.2 Abstractive Fine-tuning

The second step of BottleSumSelf is fine-tuning the
language model on its extractive summary dataset.
The tuning data is formed by concatenating source
sentences with generated summaries, separated by
a delimiter and followed by an end token. The
model (GPT-2) is fine-tuned with a simple lan-
guage modeling objective over the full sequence.

As a delimiter, we use TL;DR: , following
Radford et al. (2019) who found that this induces
summarization behavior in GPT-2 even without
tuning. We use a tuning procedure closely related
to Radford et al. (2018), training for 10 epochs.
We take the trained model weights that minimize
loss on a held-out set of 7000 extractive sum-
maries.

To generate from this model, we use a standard
beam search decoder, keeping the top candidates
at each iteration. Unless otherwise specified, as-
sume we use a beam size of 5. We restrict pro-
duced summaries to be at least 5 tokens long, and
no longer than the source sentence.

5 Experiments

We evaluate our BottleSum methods using both au-
tomatic metrics and human evaluation. We find
our methods dominant over a range of baselines in
both categories.

5.1 Setup
We evaluate our methods and baselines using auto-
matic ROUGE metrics (1,2,L) on the DUC-2003
and DUC-2004 datasets (Over et al., 2007), simi-
lar to the evaluation used by Baziotis et al. (2019).
DUC-2003 and DUC-2004 consist of 624 and
500 sentence-summary pairs respectively. Sen-
tences are taken from newstext, and each summary
consists of 4 human-written reference summaries
capped at 75 bytes. We recover next-sentences
from DUC articles for BottleSumEx.

We also employ human evaluation as a point of
comparison between models. This is both to com-
bat known issues with ROUGE metrics (Schluter,
2017) and to experiment beyond limited super-
vised domains. Studying unsupervised methods
allows for comparison over a much wider range of
data where training summary pairs are not avail-
able, which we take advantage of here by summa-
rizing sentences from the non-anonymized CNN
corpus (Hermann et al., 2015; Nallapati et al.,
2016; See et al., 2017).

We use Amazon Mechanical Turk (AMT) for
human evaluation, summarizing on 100 sentences
sampled from a held out set. Evaluation between
systems is primarily done as a pairwise compari-
son between BottleSum models and baselines, over
3 attributes: coherence, conciseness, and agree-
ment with the input. AMT workers are then asked
to make a final judgement of which summary has
higher overall quality. Each comparison is done by
3 different workers. Results are aggregated across
workers and examples.

5.2 Models
In both experiments, BottleSumEx is executed as
described in section 3.2. In experiments on DUC
datasets, next-sentences are recovered from origi-
nal news sources, while we limit test sentences in
the CNN dataset to those with an available next-
sentence (this includes over 95% of sentences).
We set parameter k = 1 (i.e. expand a single
candidate at each step) with up to m = 3 consecu-
tive words deleted per expansion. GPT-2 (small) is
used as the method’s pretrained language model,



3757

with no task-specific tuning. To clarify, the only
difference between how BottleSumEx runs on the
datasets tested here is the input sentences; no data-
specific learning is required.

As with BottleSumEx, we use GPT-2 (small) as
the base for BottleSumSelf . To produce source-
summary pairs for self supervision, we generate
over 100,000 summaries using BottleSumEx with
the parameters above, on both the Gigaword sen-
tence dataset (for automatic evaluation) and CNN
training set (for human evaluation). BottleSumSelf

is tuned on the respective set for 10 epoch with a
procedure similar to Radford et al. (2019). When
generating summaries, BottleSumSelf uses beam-
search with beam size of 5, and outputs con-
strained to be at least 5 tokens long.

We include a related model, ReconEx as a sim-
ple autoencoding baseline comparable in setup to
BottleSum

Ex. ReconEx follows the procedure of
BottleSum

Ex, but replaces the next-sentence with
the source sentence. This aims to take advan-
tage of the tendency of language models to se-
mantically repeat in to substitute the Informa-
tion Bottleneck objective in BottleSumEx with a
reconstruction-inspired loss. While this is not a
perfect autoencoder by any means, we include it
to probe the role of the next-sentence in the suc-
cess of BottleSumEx, particularly compared to a re-
constructive method. As ReconEx tends to have
a best reconstructive loss by retaining the entire
source as its summary, we constrain its length to
be as close as possible to the BottleSumEx sum-
mary for the same sentence.

As an unsupervised neural baseline, we include
SEQ3 (Baziotis et al., 2019), which is trained with
an autoencoding objective paired with a topic loss
and language model prior loss. SEQ3 had the
highest comparable unsupervised results on the
DUC datasets that we are aware of, which we cite
directly. For human evaluation, we retrained the
model with released code on the training portion
of the CNN corpus.

We use the ABS model of Rush et al. (2015)
as a baseline for automatic and human evalua-
tion. For automatic evaluation, this model is the
best published supervised result we are aware of
on the DUC-2003 dataset, and we include it as
a point of reference for the gap between super-
vised and unsupervised performance. We cite
their results directly. For human evaluation, this
model demonstrates the performance gap for out-

Method R-1 R-2 R-L

Supervised
ABS 28.18 8.49 23.81
Li et al. (2017) 31.79 10.75 27.48

Unsupervised
PREFIX 20.91 5.52 18.20
INPUT 22.18 6.30 19.33
SEQ3 22.13 6.18 19.3
ReconEx 21.97 5.70 18.81

BottleSum

Ex 22.85 5.71 19.87
BottleSum

Self 22.30 5.84 19.60

Table 1: Averaged ROUGE scores on the DUC-2004
dataset

of-domain summarization. Specifically, it requires
supervision (unavailable for the CNN dataset), and
so we use the model as originally trained on the
Gigaword sentence dataset. This constitutes a sig-
nificant domain-shift from the first-sentences of
articles with limited vocabulary to arbitrary article
sentences with diverse vocabulary.

We include the result of Li et al. (2017) on
DUC-2004, who achieved the best supervised
performance we are aware of. This is intended as
a point of reference for supervised performance.

Finally, for automatic metrics we include com-
mon baseline PREFIX, the first 75 bytes of the
source sentence. To take into account lack of strict
length constraints and possible bias of ROUGE to-
wards longer sequences, we include INPUT, the
full input sentence. Because our model is extrac-
tive, we know its outputs will be no longer than the
input, but may exceed the length of other meth-
ods/baselines.

5.3 Results
In automatic evaluation, we find BottleSumEx

achieves the highest R-1 and R-L scores for un-
supervised summarization on both datasets. This
is promising in terms of the effectiveness of
the Information Bottleneck (IB) as a framework.
BottleSum

Self achieves the second highest scores
in both of these categories, further suggesting
that the tuning process used here is able to cap-
ture some of this benefit. The superiority of
BottleSum

Ex suggests possible benefit to having
access to a relevance variable (next-sentence) to
the effectiveness of IB on these datasets.

The R-2 scores for BottleSumEx on both bench-



3758

Source ABS SEQ3 BottleSumEx
Okari, for instance, told CNN that 

he saw about 200 people sitting 

in the scorching mid-90-degree 

heat Thursday in a corner of a 

Garissa airstrip, surrounded by 

military officials.

witnesses tell cnn he 
saw a corner in the 

south africa 's heat ( for 

use of boston globe

officials for , told cnn
that cnn was people 

who about sitting on the 

in the okari , instance

Okari saw the scorching 
mid-90-degree heat 

Thursday in a Garissa 

airstrip, surrounded by 
military officials.

Okari, told CNN he saw 
about 200 people in the 

scorching corner of 

Garissa airstrip, 
surrounded by officials.

For instance, told CNN he 

saw about 200 people 

sitting in the scorching 

mid-90-degree heat in a 

Garissa airstrip,.

ReconEx BottleSumSelf

Griner told The Daily Dot that her 

husband was diagnosed with 

kidney disease in 2006 and 

sufferedcomplete kidney failure 

three years later.

kidney disease patient 's 
husband dies at age # % 

this year ( for use the 

boston globe :

later the told that was 
husband her diagnosed 

suffered kidney surgery 

in dot daily

Griner was diagnosed in 
2006 and suffered 

complete kidney failure 

three years later.

Griner told The Daily Dot 
that her husband was 

diagnosed with 

complete kidney failure.

Fletcher told The Daily 
Dot that her husband 

was diagnosed with 

kidney disease.

But the event raises broad, 

troubling questions about how 

often such incidents take place 

without the benefit of a third-

party recording.

( new york ) raises 
questions about safety 

of third-party recording 

but does n't know how 
to question itself

third-party event the , 
about questions 

troubling how the such 

often broad raises

But the event raises 
broad, troubling 

questions about 

recording

But raises broad, 
troubling questions 

about how incidents 

take place.

The event raises troubling 

questions about how 

often such incidents take 

place without the benefit 

of recording

Sitoula, who talked to CNN 

through an interpreter, says she 

remained confident she would 

survive throughout her ordeal 

amid the rubble.

china says she remains 
confident about her life 

in the rubble of cnn 's 

last year ( # )

rubble who , cnn
correspondent an 

through , remained in 

the she to talked

Sitoula to CNN, says she 
remained confident she 

would survive 

throughout her ordeal 
amid the rubble.

Sitoula, who talked to 
CNN through an 

interpreter, says she 

would survive her ordeal 
amid the rubble.

Talked to CNN through an 

interpreter, says she 

survived amid the rubble

Figure 2: Representative example generations from the summarization systems compared

Method R-1 R-2 R-L

Supervised
ABS 28.48 8.91 23.97

Unsupervised
PREFIX 21.14 6.35 18.74
INPUT 20.83 6.15 18.44
SEQ3 20.90 6.08 18.55
ReconEx 21.11 5.77 18.33

BottleSum

Ex 21.80 5.63 19.19
BottleSum

Self 21.54 5.93 18.96

Table 2: Averaged ROUGE scores on the DUC-2003
dataset

mark sets were lower than baselines, possibly due
to a lack of fluency in the outputs of the extrac-
tive approach used. PREFIX and INPUT both copy
human text directly and so should be highly flu-
ent, while Rush et al. (2015) and Baziotis et al.
(2019) have the benefit of abstractive summariza-
tion, which is less restrictive in word order. Fur-
ther, the fact that BottleSumSelf is abstractive and
surpasses R-2 scores of both new extractive meth-
ods tested here (BottleSumEx, ReconEx) supports
this idea. ReconEx, also extractive, has similar
R-2 scores to BottleSumEx.

The performance of ReconEx, our simple re-
constructive baseline, is mixed. It does succeed
to some extent (e.g. surpassing R-1 for all other
baselines but PREFIX on DUC-2003) but not as
consistently as either BottleSum method. This sug-
gests that while some benefit may come from the
extractive process of BottleSumEx alone (which
ReconEx shares), there is significant benefit to us-
ing a strong relevance variable (specifically in con-
trast to a reconstructive loss).

Next, we consider model results on human eval-

uation. BottleSumSelf and BottleSumEx both show
reliably stronger performance compared to mod-
els from related work (ABS and SEQ3 in Table 3).
While BottleSumSelf seems superior to ReconEx

other than in conciseness (in accordance with
their compression ratios in Table 4), BottleSumEx

appears roughly comparable to ReconEx and
slightly inferior to BottleSumSelf .

The inversion of dominance between
BottleSum

Ex and BottleSumSelf on automatic
and human evaluation may cast light on compet-
ing advantages. BottleSumEx captures reference
summaries more effectively, while BottleSumSelf ,
through a combination of abstractivness and
learning a cohesive underlying mechanism of
summarization, writes more favorable summaries
for a human audience. Further analysis and
accounting for known limitations of ROUGE
metrics may clarify these competing advantages.

In comparing these models, there are also prac-
tical considerations (summarized in table 5). ABS
can be quite effective, but requires learning on a
large supervised training set (as demonstrated by
its poor out-of-domain performance in Table 3).
While SEQ3 is unsupervised, it still needs exten-
sive training on a large corpus of in-domain text.
BottleSum

Ex, whose outputs were preferred over
both by humans, requires neither of these. Given
a strong pretrained language model (GPT-2 small
is used here) it only requires a source and next-
sentence to summarize. BottleSumSelf requires in-
domain text for self-supervision, but its superior
performance by human evaluation and summariza-
tion without next-sentence are clear advantages.
Further, its beam-search decoding is more com-
putationally efficient than BottleSumEx, which re-
quires evaluating conditional next-sentence per-
plexity over a large grid of extractive summary
candidates.



3759

Models Attributes Overall

Model Comparison coherence conciseness agreement better equal worse

BottleSum

Ex vs. ABS +0.45 +0.48 +0.52 60% 31% 9%
SEQ3 +0.61 +0.57 +0.56 61% 34% 5%

ReconEx -0.05 +0.01 -0.05 37% 22% 41%

BottleSum

Self vs. ABS +0.47 +0.39 +0.48 62% 26% 12%
SEQ3 +0.56 +0.45 +0.53 65% 26% 9%

ReconEx +0.11 -0.05 +0.09 47% 14% 39%
BottleSum

Ex +0.14 +0.06 +0.11 43 % 27% 30%

Table 3: Human evaluation on 100 CNN test sentences (pairwise comparison of model outputs). Attribute scores
are averaged over a scale of 1 (better), 0 (equal) and -1 (worse). We also report the overall preferences as percent-
ages.

Another difference from BottleSumEx is the abil-
ity of BottleSumSelf to be abstractivene (Table 4).
Other baselines have a higher degree of abstrac-
tiveness than BottleSumSelf , but this can be mis-
leading. Consider the examples in figure 2. While
many of the phrases introduced by other models
are technically abstractive, they are often off-topic
and confusing.

This hints at an advantage of BottleSum meth-
ods. In only requiring the base model to be a
(tunable) language model, they are architecture-
agnostic and can incorporate as powerful a lan-
guage model as is available. Here, incorporat-
ing GPT-2 (small) carries benefits like strong pre-
trained weights and robust vocabulary handling
by byte pair encoding, allowing them to process
the diverse language of the non-anonymized CNN
corpus with ease. The specific benefits of GPT-
2 are less central, however; any such language
model could be used for BottleSumEx immediately,
and BottleSumSelf with some tuning. This is in
contrast architecture-specific models like ABS and
SEQ3, which would require significant restructur-
ing to fully incorporate a new model.

As a first work to study the Information Bot-
tleneck principle for unsupervised summarization,
our results suggest this is a promising direction for
the field. It yielded two methods with unique per-
formance benefits (Table 1, 2, 3) and practical ad-
vantages (table 5). We believe this concept war-
rants further exploration in future work.

6 Related Work

6.1 Sentence Compression and
Summarization

Rush et al. (2015) first proposed abstractive sen-
tence compression with neural sequence to se-

Model Abstractive Compression
Tokens % Ratio %

BottleSum

Ex - 51
ReconEx - 52

BottleSum

Self 5.8 56
SEQ3 12.6 58
ABS 60.4 64

Table 4: Abstractiveness and compression of CNN
summaries. Abstractiveness is omitted for strictly ex-
tractive approaches

quence models, trained on a large corpus of head-
lines with the first sentences of newspaper arti-
cles as supervision. This followed early work
on approaching headline generation as statistical
machine translation (Banko et al., 2000). Subse-
quently, recurrent neural networks with pointer-
generator decoders became the standard model for
this task, and focus shifted to document-level sum-
marization (Nallapati et al., 2016; See et al., 2017).

Pointer-based neural models have also been
proposed for extractive summarization (Cheng and
Lapata, 2016). The main limitations of this ap-
proach stem from the fact that the training data is
constructed heuristically, covering a very specific
type of sentence summarization (headline genera-
tion). Thus, these supervised models do not gen-
eralize well to other kinds of sentence summariza-
tion or domains. In contrast, our method is ap-
plicable to any domain for which examples of the
inputs to be summarized are available in context.

6.2 Unsupervised Summarization

Miao and Blunsom (2016) framed sentence com-
pression as an autoencoder problem, where the
compressed sentence is a latent variable from



3760

Model
Name

Model archi-
tecture

Data for training Data for summa-
rizing

When to use

ABS (Rush
et al., 2015)

Seq2Seq Large scale, paired
source-summaries
(supervised)

source sentence Large scale supervised training set available

SEQ3
(Baziotis
et al., 2019)

Seq2Seq2Seq Large scale, un-
supervised source
sentences

source sentence Large scale unsupervised (no summaries) data
available, without next-sentences

BottleSum

Ex Pre-trained LMs No training data
needed

source sentence
and next-sentence

No training data available, and next-sentences
are available for sentences to summarize.

BottleSum

Self Pre-trained LMs
(fined-tuned
on data from
BottleSum

Ex)

Large scale, un-
supervised source
sentences with
next sentences

source sentence Large scale unsupervised (no summaries) data
available, with next-sentences and/or no next-
sentences available for sentences to summarize

Table 5: Comparison of sentence summarization methods.

which the input sentence can be reconstructed.
They proposed extractive and pointer-generator
models, regularizing the autoencoder with a lan-
guage model to encourage compression and op-
timizing the variational objective with the REIN-
FORCE algorithm. While their extractive model
can be trained without supervision, results are only
reported for semi-supervised training, which re-
quires less supervised data than purely supervised
training. Fevry and Phang (2018) applied denois-
ing autoencoders to fully unsupervised summa-
rization, while Wang and Lee (2018) proposed
an autoencoder with a discriminator for distin-
guising well-formed and ill-formed compressions
in a Generative Adversarial Network (GAN) set-
ting, as an alternative to using a language model.
However, their discriminator was trained using
unpaired summaries, so while they obtain better
results than purely unsupervised approaches like
ours their results are not directly comparable. Re-
cently Baziotis et al. (2019) proposed a differ-
entiable autoencoder using a gumbel-softmax to
represent the distribution over summaries. The
model is trained with a straight-through estima-
tor as an alternative to reinforcement learning, ob-
taining better results on unsupervised summariza-
tion. All of these approaches have in common
autoencoder-based training, which we argue does
not naturally capture an appropriate notion of rel-
evance for summarization.

Recently, Zhou and Rush (2019) introduced a
promising method for summarization using con-
textual matching with pretrained language mod-
els. While contextual matching requires pretrained
language models to generate contextual vectors,
BottleSum methods do not have specific architec-

tural constraints. Also, like Wang and Lee (2018)
it trains with unpaired summaries and so is not di-
rectly comparable to us.

7 Conclusion

We have presented BottleSumEx, an unsuper-
vised extractived approach to sentence summa-
rization, and extended this to BottleSumSelf , a self-
supervised abstractive approach. BottleSumEx,
which can be applied without any training,
achieves competitive performance on automatic
and human evaluations, compared to unsupervised
baselines. BottleSumSelf , trained on a new do-
main, obtains stronger performance by human
evaluation than unsupervised baselines as well as
BottleSum

Ex. Our results show that the Informa-
tion Bottleneck principle, by encoding a more ap-
propriate notion of relevance than autoencoders,
offers a promising direction for progress on unsu-
pervised summarization.

8 Acknowledgments

We thank anonymous reviewers for many help-
ful comments. This research is supported in part
by the Natural Sciences and Engineering Research
Council of Canada (NSERC) (funding reference
number 401233309), NSF (IIS-1524371), DARPA
CwC through ARO (W911NF15-1-0543), Darpa
MCS program N66001-19-2-4031 through NIWC
Pacific (N66001-19-2-4031), Samsung AI Re-
search, and Allen Institute for AI.

References
Mikel Artetxe, Gorka Labaka, Eneko Agirre, and

Kyunghyun Cho. 2017. Unsupervised neural ma-

http://arxiv.org/abs/1710.11041


3761

chine translation. CoRR, abs/1710.11041.

Michele Banko, Vibhu O. Mittal, and Michael J. Wit-
brock. 2000. Headline generation based on statisti-
cal translation. In Proceedings of the 38th Annual
Meeting of the Association for Computational Lin-
guistics, pages 318–325, Hong Kong. Association
for Computational Linguistics.

Christos Baziotis, Ion Androutsopoulos, Ioannis Kon-
stas, and Alexandros Potamianos. 2019. SEQ3: Dif-
ferentiable sequence-to-sequence-to-sequence au-
toencoder for unsupervised abstractive sentence
compression. In Proceedings of the Conference of
the North American Chapter of the Association for
Computational Linguistics: Human Language Tech-
nologies (NAACL:HLT), Minneapolis, USA. To ap-
pear.

Jianpeng Cheng and Mirella Lapata. 2016. Neural
summarization by extracting sentences and words.
In Proceedings of the 54th Annual Meeting of the
Association for Computational Linguistics (Volume
1: Long Papers), pages 484–494, Berlin, Germany.
Association for Computational Linguistics.

Thibault Fevry and Jason Phang. 2018. Unsuper-
vised sentence compression using denoising auto-
encoders. In Proceedings of the 22nd Conference on
Computational Natural Language Learning, pages
413–422, Brussels, Belgium. Association for Com-
putational Linguistics.

Karl Moritz Hermann, Tomas Kocisky, Edward
Grefenstette, Lasse Espeholt, Will Kay, Mustafa Su-
leyman, and Phil Blunsom. 2015. Teaching ma-
chines to read and comprehend. In Advances in
neural information processing systems, pages 1693–
1701.

Yann LeCun. 2018. Self-supervised learning: could
machines learn like humans? https://www.
youtube.com/watch?v=7I0Qt7GALVk.

Piji Li, Wai Lam, Lidong Bing, and Zihao Wang. 2017.
Deep recurrent generative decoder for abstractive
text summarization. In Proceedings of the 2017
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 2091–2100, Copenhagen,
Denmark. Association for Computational Linguis-
tics.

Yishu Miao and Phil Blunsom. 2016. Language as a
latent variable: Discrete generative models for sen-
tence compression. In Proceedings of the 2016 Con-
ference on Empirical Methods in Natural Language
Processing, pages 319–328, Austin, Texas. Associa-
tion for Computational Linguistics.

Ramesh Nallapati, Bowen Zhou, Cicero dos Santos,
Caglar Gulcehre, and Bing Xiang. 2016. Ab-
stractive text summarization using sequence-to-
sequence rnns and beyond. In Proceedings of The
20th SIGNLL Conference on Computational Natu-
ral Language Learning, pages 280–290.

Paul Over, Hoa Dang, and Donna Harman. 2007. Duc
in context. Information Processing & Management,
43(6):1506–1520.

Alec Radford, Karthik Narasimhan, Tim Salimans, and
Ilya Sutskever. 2018. Improving language under-
standing by generative pre-training. Unpublished
manuscript.

Alec Radford, Jeffrey Wu, Rewon Child, David Luan,
Dario Amodei, and Ilya Sutskever. 2019. Language
models are unsupervised multitask learners. Unpub-
lished manuscript.

Alexander M Rush, Sumit Chopra, and Jason Weston.
2015. A neural attention model for abstractive sen-
tence summarization. In Proceedings of the 2015
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 379–389.

Natalie Schluter. 2017. The limits of automatic sum-
marisation according to rouge. In Proceedings of
the 15th Conference of the European Chapter of the
Association for Computational Linguistics: Volume
2, Short Papers, pages 41–45.

Jurgen Schmidhuber. 1990. Making the world differen-
tiable: On using self-supervised fully recurrent neu-
ral networks for dynamic reinforcement learning and
planning in non-stationary environments (tr fki-126-
90).

Abigail See, Peter J. Liu, and Christopher D. Manning.
2017. Get to the point: Summarization with pointer-
generator networks. In Proceedings of the 55th An-
nual Meeting of the Association for Computational
Linguistics, ACL 2017, Vancouver, Canada, July 30
- August 4, Volume 1: Long Papers, pages 1073–
1083.

Naftali Tishby, Fernando C. Pereira, and William
Bialek. 1999. The information bottleneck method.
In Proc. of the 37-th Annual Allerton Conference
on Communication, Control and Computing, pages
368–377.

Yaushian Wang and Hung-yi Lee. 2018. Learning
to encode text as human-readable summaries using
generative adversarial networks. In Proceedings of
the 2018 Conference on Empirical Methods in Nat-
ural Language Processing, pages 4187–4195, Brus-
sels, Belgium. Association for Computational Lin-
guistics.

Jiawei Zhou and Alexander M Rush. 2019. Simple un-
supervised summarization by contextual matching.
In Proceedings of the 57th Conference of the Asso-
ciation for Computational Linguistics, pages 5101–
5106.

http://arxiv.org/abs/1710.11041
https://doi.org/10.3115/1075218.1075259
https://doi.org/10.3115/1075218.1075259
https://arxiv.org/abs/1904.03651
https://arxiv.org/abs/1904.03651
https://arxiv.org/abs/1904.03651
https://arxiv.org/abs/1904.03651
https://doi.org/10.18653/v1/P16-1046
https://doi.org/10.18653/v1/P16-1046
https://www.aclweb.org/anthology/K18-1040
https://www.aclweb.org/anthology/K18-1040
https://www.aclweb.org/anthology/K18-1040
https://papers.nips.cc/paper/5945-teaching-machines-to-read-and-comprehend.pdf
https://papers.nips.cc/paper/5945-teaching-machines-to-read-and-comprehend.pdf
https://www.youtube.com/watch?v=7I0Qt7GALVk
https://www.youtube.com/watch?v=7I0Qt7GALVk
https://doi.org/10.18653/v1/D17-1222
https://doi.org/10.18653/v1/D17-1222
https://doi.org/10.18653/v1/D16-1031
https://doi.org/10.18653/v1/D16-1031
https://doi.org/10.18653/v1/D16-1031
https://www.aclweb.org/anthology/K16-1028
https://www.aclweb.org/anthology/K16-1028
https://www.aclweb.org/anthology/K16-1028
https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf
https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf
https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf
https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf
https://www.aclweb.org/anthology/D15-1044
https://www.aclweb.org/anthology/D15-1044
https://doi.org/10.18653/v1/P17-1099
https://doi.org/10.18653/v1/P17-1099
https://www.cs.huji.ac.il/labs/learning/Papers/allerton.pdf
https://www.aclweb.org/anthology/D18-1451
https://www.aclweb.org/anthology/D18-1451
https://www.aclweb.org/anthology/D18-1451

