



















































Robust Part-of-speech Tagging of Arabic Text


Proceedings of the Second Workshop on Arabic Natural Language Processing, pages 173–182,
Beijing, China, July 26-31, 2015. c©2014 Association for Computational Linguistics

Robust Part-of-Speech Tagging of Arabic Text

Hanan Aldarmaki and Mona Diab
Department of Computer Science

The George Washington University
{aldarmaki;mtdiab}@gwu.edu

Abstract

We present a new and improved part of
speech tagger for Arabic text that incor-
porates a set of novel features and con-
straints. This framework is presented
within the MADAMIRA software suite, a
state-of-the-art toolkit for Arabic language
processing. Starting from a linear SVM
model with basic lexical features, we add a
range of features derived from morpholog-
ical analysis and clustering methods. We
show that using these features significantly
improves part-of-speech tagging accuracy,
especially for unseen words, which re-
sults in better generalization across genres.
The final model, embedded in a sequential
tagging framework, achieved 97.15% ac-
curacy on the main test set of newswire
data, which is higher than the current
MADAMIRA accuracy of 96.91% while
being 30% faster.

1 Introduction

Part-of-speech (POS) tagging is an essential en-
abling technology and a precursor for most Nat-
ural Language Processing (NLP) tasks such as
syntactic parsing, semantic role labeling, machine
translation, and information extraction. POS tag-
ging ranges in its complexity depending on the
morphological richness of the targeted language.
For morphologically rich languages, POS tagging
poses a significant challenge, especially when
moving away from formal textual genres to more
informal genres. In this paper, we present a suite
of linear supervised learning methods and features
used to enhance POS tagging for Modern Standard
Arabic (MSA) text using a relatively complex POS
tag set. The novel POS tagger is presented within
the context of the MADAMIRA suite framework
(Pasha et. al., 2014). MADAMIRA is a combina-
tion of two well established approaches: AMIRA

(Diab, 2009) and MADA (Roth et al, 2008).
AMIRA is a relatively simple cascaded system
that performs clitic segmentation, segmentation
correction or normalization, and POS tagging as
three separate steps performed sequentially, while
MADA performs all three steps in one fell swoop.
Both systems are based on supervised learning.
However, the MADA approach relies on opti-
mizing the results of a morphological analyzer
while AMIRA does not rely on external resources.
While the current MADAMIRA release only in-
cludes the MADA system, the goal is to combine
both systems to improve MADAMIRA further.
For the remainder of this paper, we will refer to the
two systems as MD-MADA and MD-AMIRA, as
both are presented within the MADAMIRA soft-
ware suite.

In MD-AMIRA, POS tagging is implemented
for MSA using linear classification with lexical
features, and it results in reasonable accuracy
within familiar contexts. However, the perfor-
mance degrades when the model encounters words
unseen in training. In this paper, we attempt
to enhance the performance of this model, es-
pecially for unseen words, by including features
from various external resources while maintaining
the simplicity of the linear model. We refer to
this enhanced model as MD-AMIRA+EX. Using
a morphological analyzer, we extract morpholog-
ical features as well as valid part-of-speech tags
for input tokens. We also use these tags to impose
soft constraints on the output. In addition, we in-
clude word clusters using two clustering methods
applied to a large unlabeled data set. The basic
POS tagging model and the additional features and
constraints are described in Section 4.

MSA exhibits affixival and agglutinative mor-
phology, where various forms of prepositions, ar-
ticles, and pronouns are merged with words as
clitics. A surface space-delimited word such as

173



”wsyktbwnhA”,1 ‘and they will write it’, packs
what would be considered several words in a lan-
guage such as English. The different words are
expressed as agglutinated morphemes or clitics
broken up as follows: ”w+ s+ yktbwn +hA”,
‘and+ will+ write plural +it fem’. Due to limited
amounts of labeled data, separating clitics from
words, i.e. tokenization, is essential in reducing
sparsity and enhancing the accuracy of POS tag-
ging. In this paper, we address MSA tokenization
as a precursor to POS tagging.

In MD-AMIRA, MSA tokenization is split into
two steps: clitic segmentation and segmentation
normalization. In the segmentation step, described
in Section 3.1.1, we break each input word into
segments corresponding to the clitics and the stem
that make up that word. In MSA, some mor-
phemes change in form as a result of affixation,
and we render them to their original underly-
ing forms in the segmentation normalization step,
which is described in Section 3.1.2.

We evaluate the performance of the separate
steps and the whole pipeline from tokenization to
part-of-speech tagging in Sections 6 to 8. We show
that this approach is robust and efficient as it com-
pares to state of the art accuracy while exhibiting
robust performance on unseen words.

2 Related Work

The sequential NLP process presented in this pa-
per is adapted from the AMIRA toolkit, which is
described in (Diab, 2009) and (Diab et al., 2004),
and is publicly available. Another data-driven
part-of-speech tagger for Arabic was presented in
(Kopru, 2011), which uses an HMM to learn an
efficient classifier using surface features.

The alternative approach is MADA, which re-
lies on deep morphological analysis and disam-
biguation, as described in (Habah et al., 2005) and
(Roth et al, 2008). Several SVM classifiers are
trained to predict morphological features in the
first stage. These features are then used to rank
the morphological analyses retrieved from a dic-
tionary, and the analysis with the highest score is
taken as the final analysis for the given word. This
deep analysis results in accurate and detailed tag-
ging albeit slower than simple SVM methods. Fi-
nally, the problem of classification and incorporat-
ing structural constraints on the output is studied

1We transliterate Arabic text using Buckwalter romaniza-
tion scheme: http://www.qamus.org

in (Punyakanok et al, 2005). This is related to the
constrained POS tagging attempted here, where
external inference is used to maintain consistency
after learning. A related example of incorporating
external resources to constrain the learned classi-
fiers is presented in (Do and Roth, 2010)

3 Approach

We adapt the AMIRA tagger approach by using
linear support vector machines (SVM) as our ba-
sic classification machinery for both MD-AMIRA
and MD-AMIRA+EX. We approach both Tok-
enization and POS tagging as classification prob-
lems. The basic models directly follow the imple-
mentation details described in (Diab, 2009).

3.1 Tokenization

3.1.1 Clitic Segmentation
Clitics are independent meaning-bearing units that
are phonologically and orthographically merged
with words, either as prefixes (proclitics) or suf-
fixes (enclitics). Clitics are different from deriva-
tional or inflectional affixes, which either change
the meaning or the syntactic role of their stems and
are not segmented here. A word, in this context,
refers to a stem and its inflectional and derivational
affixes, and clitic segmentation is the process of
separating clitics from words. Since clitics have
their own meaning and part-of-speech tags, sepa-
rating them reduces sparsity in the input space.

In MSA, a word can have up to three proclitics
and one enclitic. Table 1 shows some of the word
classes that serve as clitics in MSA.

Type Category Examples

Proclitic Definite article Al
Proclitic Prepositions b, l, k
Proclitic Conjunctions w, f
Proclitic Future marker s
Enclitics Pronouns h, hm, hmA, etc.

Table 1: Examples of clitics in MSA

Set up: Segmentation is modeled as a classifi-
cation problem at the character level, where each
character is given a tag. Similar to the AMIRA
framework, we adopt an IOB chunk/segment
tagging scheme. The tag set is defined as follows:

174



Tag set: {B-PRE, I-PRE, B-WORD, I-WORD, B-
SUFF, I-SUFF, O}
WORD: stem+inflectional affixes
PRE: enclitic
SUFF: proclitic
B- : beginning of segment
I- : Inside segment
O: outside of segment (word boundaries)

The input consists of Buckwalter (BW) translit-
erated Arabic characters (Habah et al., 2007) with
word boundary markers, preprocessed by digit
normalization (converting all digits into ’8’) and
removal of diacritics, if any. The features are as
follows: (1) contextual features: [-5,+5] charac-
ters in context, the previous [-5, -1] tag decisions,
and (2) lexical features: the whole space-delimited
word, and character N-grams, N≤4, within that
word.

3.1.2 Segmentation Normalization
Segmentation normalization is a correction step
that attempts to restore citation forms of some
words that have been transformed as a result of the
morphotactics of clitic affixation. This task aims
to reduce sparsity in the input space, and is in-
spired by the AMIRA tokenization lemmatization
step (Diab et al., 2004), but we include additional
forms of normalization. More details on Arabic
orthographic and morphological adjustment rules
can be found in (El Kholy and Habash, 2010).

Some forms of correction are deterministic,
such as restoring the definite article ”Al” (‘the’)
from its reduced form ”l” when it’s preceded by
the preposition ”l” (‘for’). Another example is the
restoration of the trailing ”n” in prepositions such
as ”mn” (from’) and ”En” (‘about’) when fol-
lowed by the suffix ”mA” (‘what’), as in ”mmA”
→ ”mn+mA”. These are cases of clitic lemma-
tization, and since they are observed on a closed
class of tokens, they are easily addressed as a post-
tokenization processing step.

On the other hand, segmentation of open-class
word forms cannot be restored deterministically.
In MSA, words that end with the feminine marker
character Taa Marbuta ”p”, are transformed into
”t” when followed by suffixes, as in: ”klmp”
(‘word’) → ”klmt+h” (‘his word’). A stem that
ends with a ”t” could either be a transformed
”p” or a word that originally ends with ”t”, as in
”byt” (house). The other type of word ending that
is transformed in affixation is the character Alef

Maqsura ”Y”, which is transformed into ”y” or
”A” if followed by suffixes (e.g.: ”ElY” (‘on’),
→ ”Ely+h” (’on him’)). The problem is more
complex for words that can correspond to multi-
ple lemmas such as ”ESAhm”, which could corre-
spond to the verb ”ESY+hm” (’disobeyed+them’)
or the noun ”ESA+hm” (‘stick +their’).

The restoration of Taa Marbuta and Alef Maq-
sura is not a deterministic process and it requires
both contextual information and/or deeper knowl-
edge of the language. The segmentation normal-
ization step attempts to achieve this type of cor-
rection by learning to distinguish these cases from
contextual data as follows.

Set up: The problem of segmentation normal-
ization is addressed as a classification problem on
the token level cascaded from the prior segmen-
tation step. The input consists of a list of tokens,
with proclitic and enclitic markers–the ’+’ marker
indicating a segmentation point. The feature vec-
tor consists of [-2,+2] tokens in context, character
N-grams, N≤4, for the current token, and the pre-
vious 2 tag decisions. Each token is assigned one
of the following tags:

CIP: Change trailing t to p
CIY: Change trailing y or A to Y
NA: Do nothing

4 Part of Speech Tagging

POS tagging is performed on the resulting
tokenized text; that is, after performing clitic
segmentation and segmentation normalization.
The tag set used is a modified version of ERTS
(Diab, 2007), which explicitly encodes several
morphological features like determiner definite-
ness, gender, and number for nominals. We
extend the tagset to include person, gender,
number, and voice for verbs, and we refer to the
new tagset as ERTS2. These fine-grained tags can
be easily reduced to broad part-of-speech classes
after prediction, which makes them suitable for
a range of applications. They encode the full
part-of-speech tag information provided in the
LDC Arabic Treebank, excluding syntactic mood,
syntactic case, and construct state definiteness.
The following are some examples of ERTS2 tags,
which illustrate the level of encoded details—refer
to the appendix for a full listing of possible tags:

175



PV+3 MASC SG: Third-person singular mascu-
line perfective verb
IV PASS+3 MASC SG: Passive-voice mascu-
line singular imperfective verb
ADJ+FEM PL: Feminine plural adjective

The input to this classification problem consists
of a list of digit-normalized tokens with explicit
enclitic and proclitic affixes marked with ’+’ at
segmentation points. The feature vector consists
of [-2,+2] tokens in context, character N-grams,
N≤4, for the current token, the type of the current
token {alpha, numeric}, and the previous 2 tag de-
cisions.

We add two more components (constraints
and features) over the MD-AMIRA POS tagging
pipeline as follows.

4.1 ALMOR for Constrained Tagging
ALMOR (Habah, 2007) is a morphological anal-
ysis and generation system for MSA and dialectal
Arabic. Given a word, ALMOR retrieves all possi-
ble analyses for that word and a list of characteris-
tics, including part-of-speech tags, for each anal-
ysis. ALMOR constructs the analyses by gener-
ating all possible segmentations and verifying the
validity and compatibility of the segments on an
underlying database of valid stems and affixes.

In our POS tagging model, ALMOR is used as a
source of external knowledge to constrain the sta-
tistical SVM tagger: the retrieved ALMOR part-
of-speech tags are used as constraints on the SVM
decision function by penalizing tags that do not
appear in ALMOR analyses set. Given k tags, the
POS tag yi for a word wi, as given by the original
SVM decision function, is the tag with the maxi-
mum SVM score

argmax
yi,i∈{1,..,k}

(score(yi))

Using ALMOR, we retrieve the set of possible
part-of-speech tags, Si, and penalize the tags that
are not found in this set by reducing their SVM
score. Accordingly, the final tag is given by the
modified decision function:

argmax
yi,i∈{1,..,k}

(score(yi)− ρ ISCi (yi))

where I is the indicator function of the comple-
ment of Si, and ρ is the penalty parameter. This
modification is implemented only in the prediction
step, so the experiment doesn’t require re-training
of the models.

4.2 Additional Features
The following sets of features were extracted from
external resources and tested separately as well as
in combination.

Morphological features: The top m part-of-
speech tags from the set of analyses Si, as
described in the previous section, are used as
features. The optimal number of tags, m, to
include is tuned from the data. Additional mor-
phological features extracted from ALMOR are
voice, gender, person, and number.

Clustering features: We add cluster IDs re-
trieved from a large unlabeled dataset using two
clustering methods: Brown clustering (Brown et
al., 1992), and word2vec K-means clustering
(Mikolov et. al., 2013).

Named-entity-related features: To support
proper noun identification, we add binary features
for exact and partial match in a gazetteer, and
capitalization in the English gloss in any one of
ALMOR analyses.

5 Experimental Set Up

5.1 Data set
The data sets used for training the models are
LDC’s Arabic Treebank (ATB) parts 1,2, and 3
(Maamouri et. al., 2004), which consist of MSA
newswire data. The data is split as follows: 10%
development set, 80% training set, and 10% test
set. For cross-genre evaluation, we use the test
set from ATB parts 5,6,7 and 10, which consist of
MSA broadcast news and a small portion from the
Weblog genres. The data sets are pre-processed
using the approach described in (Habah et al.,
2005) to correct annotation inconsistencies.

5.2 SVM Classification
Linear SVM classification is implemented using
Cogent (Pasha et. al., 2014), a java utility and a
wrapper around Liblinear (Fan et. al., 2008). Co-
gent pre-processes the input and converts text fea-
tures into binary feature vectors for linear classi-
fication. In these experiments, Cogent is config-
ured to keep a maximum of 100,000 features, so
features are filtered to keep the maximum value
within that range by removing the least frequent
feature-value pairs. This limitation is imposed to

176



keep the models more manageable during training
and prediction.

6 Evaluation

The performance of our systems, MD-AMIRA
and MD-AMIRA+EX, are evaluated and com-
pared against the performance of MD-MADA
(Pasha et. al., 2014), on the same tasks. MD-
MADA produces highly sophisticated and accu-
rate analysis of raw text, which includes a large
number of morphological features reflecting the
full spectrum of part-of-speech tags used in ATB,
which is more specified than the ERTS2 tag set
used in this work. Moreover, MD-MADA pro-
duces lemmas and their corresponding diacritiza-
tion forms. We report comparative results on Tok-
enization and POS tagging using a subset of MD -
MADA outputs that correspond directly to our
output specifications.

6.1 Clitic Segmentation

We evaluate the performance of MD-AMIRA as
described in Section 3.1.1. Table 2 shows the
overall performance of MD-AMIRA segmentation
model compared with MD-MADA using the har-
monic mean F-score metric. We perform clitic
segmentation at the most detailed segmentation
level, D3, which is ATB tokenization in addition
to segmenting out the definite article Al (Habah et
al., 2006). The overall F score of our linear seg-
mentation is over 99 on ATB1-2-3 test set, com-
parable to the F score achieved by MD-MADA.
Both models perform worse on cross-genre data,
i.e. ATB5-6-7-10 test set, and MD-AMIRA per-
forms worse on this set.

Model F score on test setATB1-2-3 ATB5-6-7-10
MD-MADA 99.20 98.54
MD-AMIRA 99.24 97.76

Table 2: Overall segmentation performance on
held-out test data

We report precision and recall results at the chunk
level, PRE, WORD, SUFF in Figure 1. On ATB1-
2-3 test set, MD-AMIRA has higher precision and
lower recall rates over all segment types. On
cross-genre data, MD-AMIRA precision drops for
all types, with a notable drop in suffix segmen-
tation. This set consists primarily of broadcast
news transcriptions, and it includes filled pauses

transcribed as ”>h” (’uh’), which are not encoun-
tered in the formal newswire training data. In MD-
AMIRA, the ”h” in this interjection is incorrectly
segmented as a possessive pronoun ”+h”, (’his’),
and this is responsible for about 60% of the drop
in suffix precision.

PRE WORD SUFF
85

90

95

100

ATB1-2-3

Precision %

MD-AMIRA

MD-MADA

PRE WORD SUFF
85

90

95

100

ATB1-2-3

Recall %

PRE WORD SUFF
85

90

95

100

ATB5-6-7-10
PRE WORD SUFF

85

90

95

100

ATB5-6-7-10

Figure 1: Chunk-level segmentation performance
on held-out test data

CIP CIY
85

90

95

100

ATB1-2-3

F score

CIP CIY
85

90

95

100

ATB5-6-7-10

F score

Figure 2: Segmentation Normalization perfor-
mance on held-out test data

6.2 Segmentation Normalization

Figure 2 shows the performance of normaliza-
tion conditions CIP and CIY using both systems
on each test set. On ATB1-2-3 test set, the
performance MD-AMIRA is comparable to MD-
MADA. On cross-genre data, the performance of
MD-AMIRA in CIP normalization is significantly
lower than MD-MADA. Around half the errors in
CIP identification are caused by words unseen in
training since MD-AMIRA does not use any ex-

177



ternal resources in this step. Note that these results
are evaluated after performing automatic segmen-
tation with each system, so some errors are propa-
gated from the clitic segmentation step.

7 Part of Speech Tagging

We first analyze the performance of the POS tag-
ging module on the development set indepen-
dently using gold tokenization. The purpose of
this analysis is to tune the model without the effect
of errors cascaded from automatic tokenization. In
Section 8, we evaluate the performance of the fi-
nalized POS tagging model within the pipelined
MD-AMIRA system and compare it with MD-
MADA.

7.1 ALMOR Constrained Tagging
As discussed in Section 4.1, we modify the SVM
scores to prioritize the tags retrieved by ALMOR.
Figure 3 shows the performance as a function of
ρ on the development set (the y-axis is divided
and scaled for clarity). Without a constraint, the
overall accuracy is 97.3%. Adding the constraints
initially improves the overall accuracy, which
peaks around ρ = 1, then drops considerably.
Breaking up the accuracy on seen versus unseen
words in training, the accuracy of unseen words
increases generally, and is maximized around
ρ = 2. For seen words, where the accuracy is
close to 98% to start with, adding the constraints
degrades performance.

95
96
97
98

A
cc

ur
ac

y

overall
seen

0 2 4 6 8
78
80
82
84 unseen

Figure 3: Accuracy as a function of ρ

Accordingly, we impose the constraints only on
words that are unseen in training. This achieves an
overall accuracy of 97.5%, which is a statistically
significant improvement.2

2We test statistical significance using an exact test for one

7.2 ALMOR Tags as Features

An alternative use for the part-of-speech tags
retrieved from ALMOR is to include the top m
tags as features. Figure 4 shows the accuracy
with m tags. Adding a single tag significantly
improves the overall accuracy, which continues to
improve up to m = 4. While adding more tags as
features slightly improves the accuracy, limiting
the number of retrieved tags improves the speed
of the prediction model. Since the improvement
beyond m = 2 is not statistically significant, we
keep the number of tags at m = 2, which achieves
an accuracy of 97.64%.

1 2 3 4 5 6
97.3

97.4

97.5

97.6

97.7

m

A
cc

ur
ac

y

Figure 4: Accuracy as a function of m

7.3 Impact of additional features

In addition to POS tags, additional morphological
features can be extracted from ALMOR analyses.
We extracted the following set of features: num-
ber, gender, person, and voice, from the top two
analyses, and included them as features on top of
the basic set of lexical features.

We also experimented with a set of named-
entity-related features: a binary feature for having
a match in a set of gazetteers, and a binary fea-
ture for capitalization in the English gloss in one
of ALMOR analyses (which is equivalent to hav-
ing proper noun as one of the analyses). These fea-
tures are added to help identify proper nouns. We
tested these two sets of features separately, and the
overall accuracy as well as the accuracy for unseen
words are shown in Table 3.

Feature Set Overall Unseen

MD-AMIRA 96.58% 77.60
Morph. Features 97.50% 84.36%
NE Features 97.31% 77.76%

Table 3: Performance with Additional Features

sample binomial proportions, at the 0.05 significance level.

178



Adding morphological features results in a sta-
tistically significant improvement in accuracy and
around 7% reduction in error rate for unseen
words. Named-entity features, on the other hand,
do not improve performance. Both gazetteer
matches and capitalization are features that could
be triggered by adjectives, nouns, and proper
nouns as they have similar word forms in MSA.
The neutral result suggests that these features add
noise which offsets any improvement from proper
noun identification.

7.4 Clustering

We performed Brown clustering as well as
Google’s word2vec K-means clustering using an
automatically tokenized version of LDC’s Arabic
Gigaword dataset (Graff, 2003). The number of
clusters, k, is empirically set to 500. Table 4 shows
the effect of adding these cluster IDs as features on
top of the basic model. Both clustering methods
result in a statistically significant improvement in
accuracy, especially for unseen words. Combin-
ing both clustering methods as features achieves
additional gains in performance, suggesting that
the two clustering methods provide complemen-
tary information.

Clustering Method AccuracyOverall Unseen
MD-AMIRA 97.31% 77.60%

+Brown Clustering 97.49% 83.96%
+Word2Vec 97.44% 82.36%

+Brown & Word2Vec 97.52% 84.76%

Table 4: POS Tagging Accuracy with Clustering
Features

7.5 Combining Features

We now evaluate the models with a combination
of these features. Table 5 shows the performance
of the different models as evaluated on the devel-
opment set. Starting from the basic model, MD-
AMIRA, with only lexical features, we add the
feature sets one at a time and compare the accu-
racy.

Each set of features incrementally improves
the performance, and the highest improvement
is achieved by adding two tags from ALMOR.
Adding more features can improve the perfor-
mance, but the improvements are less evident
when combined with the existing features. Adding
morphological features in M2, for example, does

not help since the morphological features are im-
plied in the POS tags already included in M1, and
in this case the accuracy drops slightly. In M3,
where we combine POS tags, morphological fea-
tures, and cluster IDs, the accuracy improves for
unseen words, and it performs better than M3b
where we exclude morphological features.

In M4, we re-tune the penalty parameter ρ over
M3. Adding this penalty does not significantly
improve the performance as it is outweighed
by the improvements from the other features.
Moreover, adding these soft constraints reduces
the speed of the prediction model. Thus, we
choose M3 as our final model–note that the
difference between M3 and M3b is not statistically
significant and both have the same prediction
speed. Using M3, the accuracy of tagging words
unseen in training is around 90%, a considerable
gain over the baseline. We use M3 as the POS
tagging model in MD-AMIRA+EX.

Model AccuracyOverall Unseen Words
MD-AMIRA 97.309% 77.60%

M1 97.637% 88.96%
M2 97.628% 88.72%
M3 97.682% 90.64%
M3b 97.677% 90.40%
M4 97.686% 90.76%

Table 5: Performance of POS tagging models on
ATB1-2-3 development set. MD-AMIRA: base-
line model with surface features. M1: basic fea-
tures + top two tags from ALMOR. M2:The fea-
tures in M1 + morphological features. M3: The
features in M2 + clustering. M3b: The features in
M1 + clustering. M4: The features in M3 + penalty
ρ = 0.45.

8 Overall Performance

We evaluate the performance of the system as a
whole process from tokenization to part-of-speech
tagging. The performance of our final system
MD-AMIRA+EX on ATB1-2-3 held out test set is
compared against two systems: the baseline of ba-
sic lexical features, MD-AMIRA, and the state-of-
the-art system, MD-MADA. In order to compare
MD-MADA to our system, we reduce the MD-
MADA tag set to the ERTS2 tag set.

Table 6 shows the overall accuracy of these sys-
tems in addition to the tagging speed in tokens per

179



Model ERTS Accuracy Broad Tags
3 Speed

Overall Unseen Accuracy tokens\sec
MD-AMIRA 96.78% 73.38% 98.01% ~2415

MD-AMIRA+EX 97.15% 89.22% 98.24% ~1395
MD-MADA 96.91% 85.28% 98.19% ~1050

Table 6: Performance on ATB1-2-3 held out test set

second, which is evaluated on the same hardware.

The fastest system is MD-AMIRA, which can
tag at least 70% more tokens per second than the
other models, but results in lower accuracy. The
performance on unseen words, which make up
about 2.5% of tokens in this set, is particularly
bad. MD-AMIRA+EX processes about 30% more
tokens per second than MD-MADA while achiev-
ing a higher accuracy in this set, which is statisti-
cally significant. The large improvement on un-
seen tokens reflects the generalization power of
this model compared to the baseline. Note that
in each model, some errors are due to segmen-
tation, but since MD-AMIRA, MD-AMIRA+EX,
and MD-MADA systems achieved high segmen-
tation accuracy on this set, the effect is mini-
mal. As a demonstration of this effect, MD-
AMIRA+EX achieved 97.64% accuracy on this
set using gold tokenization; segmentation errors
reduced the overall accuracy by about 0.5%. For
unseen words, the accuracy using gold tokeniza-
tion is 91.78%, more than 3% relative increase
in accuracy compared to automatic segmentation.
This indicates that we have a relatively robust and
efficient POS tagging model.

Most of the errors in POS tagging are due to
confusion between the main classes: nouns, ad-
jectives, and verbs. Interestingly, MD-MADA
have lower recall for proper nouns than MD-
AMIRA+EX. Table 7 shows the number of proper
noun misclassifications using both systems. We
only show the count of proper nouns that are incor-
rectly classified as either adjective or verb, which
account for the majority of errors related to proper
nouns. The table illustrate one category in which
MD-AMIRA+EX outperform MD-MADA in POS
tagging.

Model ADJ VERB
MD-AMIRA+EX 69 50

MD-MADA 125 107

Table 7: Proper noun misclassifications

Table 8 shows the accuracy on cross-genre data.
MD-AMIRA+EX achieves a significantly higher
accuracy than MD-AMIRA with a large improve-
ment in accuracy for unseen words, which make
up about 5% of tokens in this set. Compared to
MD-MADA, MD-AMIRA+EX performed worse
on this set. This decline in performance is mostly
attributed to the segmentation errors from MD-
AMIRA+EX tokenization, which is worse than
MD-MADA tokenization in this set as shown
in Section 6.1. Using gold tokenization, MD-
AMIRA+EX resulted in 95.4% POS tagging ac-
curacy on this set; segmentation errors reduced
the overall accuracy by more than 1%. For un-
seen words, the accuracy using gold tokenization
is 80.16%, an increase of more than 5% over the
accuracy using automatic segmentation. This is
further evidence that MD-AMIRA+EX POS tag-
ging model is robust as it achieves close to state-
of-the-art accuracy in spite of having more seg-
mentation errors.

Model ERTS AccuracyOverall Unseen Words
MD-AMIRA 93.35% 55.92%

MD-AMIRA+EX 94.38% 75.53%
MD-MADA 94.71% 77.19%

Table 8: Accuracy on cross-genre data

9 Conclusions

We experimented with various feature sets to im-
prove the performance and generalization power
of linear part-of-speech tagging. Adding a cou-
ple of part-of-speech tags from a morphological
analyzer as features greatly reduced the error rate
and achieved the largest gain in performance in
our final model. Adding morphological features
from the same analyzer, while it achieved signifi-
cant improvements when tested separately, did not
achieve large gains in the final model’s accuracy

3broad part-of-speech classes, such as noun, verb, etc.

180



since these features are mostly redundant given the
POS tags. Similarly, using the POS tags as soft
constraints on the SVM decision function did not
achieve significant gains on the model that already
incorporates these tags as features. Adding cluster
IDs, on the other hand, reduced the error rate, par-
ticularly for unseen words and genres, even when
combined with the other features.

The clustering methods we experimented with
were implemented using a large dataset of
newswire data: the same genre used for train-
ing. To achieve better generalization over different
genre, clustering data from various genre would
be an interesting experiment for future work. Fur-
thermore, part-of-speech tagging performance de-
pends on the accuracy of segmentation. Our fi-
nal model achieved lower accuracy on cross-genre
data due to segmentation errors. Improving the
performance of tokenization can be another way
to improve the final model. Overall, the model
achieved close to state-of-the-art performance and
good generalization over unseen words while be-
ing reasonably fast.

References
P. F. Brown, P. V. deSouza, R. L. Mercer, V. J. D. Pietra,

and J. C. Lai. Class-based n-gram models of natu-
ral language. Computational Linguistics, 18(4):467-
479, 1992.

M. Diab, K. Hacioglu, and D. Jurafsky. Automatic tag-
ging of arabic text: From raw text to base phrase
chunks. In Proceedings of HLT-NAACL 2004: Short
Papers , pages 149152. Association for Computa-
tional Linguistics, 2004.

M. Diab. Improved Arabic base phrase chunking with a
new enriched pos tag set. In Proceedings of the 2007
Workshop on Computational Approaches to Semitic
Languages.

M. Diab. Second generation amira tools for arabic
processing: Fast and robust tokenization, pos tag-
ging, and base phrase chunking. In Proceedings of
the Second International Conference on Arabic Lan-
guage Resources and Tools , pages 285288, 2009.

Q.X. Do and D. Roth. Constraints based taxonomic re-
lation classification. In Proceedings of EMNLP2010
, pages 10991109. Association for Computational
Linguistics, 2010.

A. El Kholy and N. Habash. Techniques for Arabic
morphological detokenization and orthographic de-
normalization. In LREC 2010 Workshop on Lan-
guage Resources and Human Language Technology
for Semitic Languages.

Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-
Rui Wang, and Chih-Jen Lin. LIBLINEAR: A Li-
brary for Large Linear Classification. Journal of Ma-
chine Learning Research, 9:18711874. 2008.

David Graff. Arabic Gigaword, LDC Catalog No.:
LDC2003T12. Linguistic Data Consortium, Univer-
sity of Pennsylvania. 2003.

N. Habash and O. Rambow. Arabic tokenization, part-
of-speech tagging and morphological disambigua-
tion in one fell swoop. In Proceedings of the ACL-05
, pages 573580. Association for Computational Lin-
guistics, 2005.

N. Habash, and F. Sadat. Arabic Preprocessing
Schemes for Statistical Machine Translation. In Pro-
ceedings of the 7th Meeting of the North American
Chapter of the Association for Computational Lin-
guistics/Human Language Technologies Conference
(HLT-NAACL06), p. 4952, New York, NY. 2006.

N, Habash, A. Soudi, and T. Buckwalter. On Ara-
bic Transliteration. In A. van den Bosch and A.
Soudi, editors, Arabic Computational Mor- phology:
Knowledge-based and Empirical Methods. Springer
(2007).

N. Habash. Arabic morphological representations for
machine translation. In A. van den Bosch, A.
Soudi (Eds.), Arabic Computational Morphology:
Knowledge-based and Empirical Methods, Springer
(2007).

M. Maamouri, A. Bies, and T. Buckwalter. The penn
arabic treebank : Building a large- scale annotated
arabic corpus. In NEMLAR Confer- ence on Arabic
Language Resources and Tools, Cairo, Egypt. 2004.

T. Mikolov, K. Chen, G. Corrado, and J. Dean. Effi-
cient Estimation of Word Representations in Vector
Space. In Proceedings of Workshop at ICLR, 2013.

S. Kopru. An efficient part-of-speech tagger for arabic.
In Computational Linguistics and Intelligent Text
Processing , pages 202213, 2011.

A. Pasha, M. Al-Badrashiny, M. Diab, A. El Kholy,
R. Eskander, N. Habash, M. Pooleery, O. Rambow,
and R. Roth. MADAMIRA: A Fast, Comprehensive
Tool for Morphological Analysis and Disambigua-
tion of Arabic. Proceedings of the Ninth Interna-
tional Conference on Language Resources and Eval-
uation (LREC-2014).

V. Punyakanok, D. Roth, W. Yih, and D. Zimak. Learn-
ing and inference over constrained output. In IJCAI
, pages 11241129, 2005.

R. Roth, O. Rambow, N. Habash, M. Diab, and C.
Rudin. Arabic morphological tagging, diacritization,
and lemmatization using lexeme models and feature
ranking. In Proceedings of ACL-08: HLT, Short Pa-
pers , pages 117120, 2008.

181



Appendix: ERTS2 Tagset

◦ ABBREV ◦ ADJ ◦ ADJ+FEM DU
◦ ADJ+FEM PL ◦ ADJ+FEM SG ◦ ADJ+MASC DU
◦ ADJ+MASC PL ◦ ADJ COMP ◦ ADJ COMP+FEM SG
◦ ADJ COMP+MASC PL ◦ ADJ NUM ◦ ADJ NUM+FEM DU
◦ ADJ NUM+FEM PL ◦ ADJ NUM+FEM SG ◦ ADJ NUM+MASC DU
◦ ADJ NUM+MASC PL ◦ ADJ VN ◦ ADJ VN+FEM DU
◦ ADJ VN+FEM PL ◦ ADJ VN+FEM SG ◦ ADJ VN+MASC DU
◦ ADJ VN+MASC PL ◦ ADV ◦ ADV INTERROG
◦ ADV REL ◦ CONJ ◦ CV
◦ CV+2 FEM SG ◦ CV+2 MASC PL ◦ CV+2 MASC SG
◦ DET ◦ INTERJ ◦ IV
◦ IV+1 PL ◦ IV+1 SG ◦ IV+2 DU
◦ IV+2 FEM PL ◦ IV+2 FEM SG ◦ IV+2 MASC PL
◦ IV+2 MASC SG ◦ IV+3 FEM DU ◦ IV+3 FEM PL
◦ IV+3 FEM SG ◦ IV+3 MASC DU ◦ IV+3 MASC PL
◦ IV+3 MASC SG ◦ IV PASS ◦ IV PASS+1 PL
◦ IV PASS+1 SG ◦ IV PASS+2 FEM SG ◦ IV PASS+2 MASC SG
◦ IV PASS+3 FEM SG ◦ IV PASS+3 MASC DU ◦ IV PASS+3 MASC PL
◦ IV PASS+3 MASC SG ◦ NOUN ◦ NOUN+FEM DU
◦ NOUN+FEM PL ◦ NOUN+FEM SG ◦ NOUN+MASC DU
◦ NOUN+MASC PL ◦ NOUN+PRN+1 SG ◦ NOUN+PRN+1 SG+FEM DU
◦ NOUN+PRN+1 SG+MASC DU ◦ NOUN+PRN+1 SG+MASC PL ◦ NOUN NUM
◦ NOUN NUM+FEM DU ◦ NOUN NUM+FEM PL ◦ NOUN NUM+FEM SG
◦ NOUN NUM+MASC DU ◦ NOUN NUM+MASC PL ◦ NOUN PROP
◦ NOUN PROP+FEM DU ◦ NOUN PROP+FEM PL ◦ NOUN PROP+FEM SG
◦ NOUN PROP+MASC DU ◦ NOUN PROP+MASC PL ◦ NOUN QUANT
◦ NOUN QUANT+FEM SG ◦ NOUN QUANT+MASC DU ◦ NOUN VN
◦ NOUN VN+FEM DU ◦ NOUN VN+FEM PL ◦ NOUN VN+FEM SG
◦ NOUN VN+MASC DU ◦ NOUN VN+MASC PL ◦ OTH
◦ PART ◦ PART FOC ◦ PART FUT
◦ PART INTERROG ◦ PART NEG ◦ PART VERB
◦ PART VOC ◦ PREP ◦ PREP+NOUN
◦ PREP+PRN+1 SG ◦ PRN ◦ PRN+1 PL
◦ PRN+1 SG ◦ PRN+2 DU ◦ PRN+2 FEM PL
◦ PRN+2 FEM SG ◦ PRN+2 MASC PL ◦ PRN+2 MASC SG
◦ PRN+3 DU ◦ PRN+3 FEM PL ◦ PRN+3 FEM SG
◦ PRN+3 MASC PL ◦ PRN+3 MASC SG ◦ PRN DEM
◦ PRN DEM+FEM ◦ PRN DEM+FEM DU ◦ PRN DEM+FEM SG
◦ PRN DEM+MASC DU ◦ PRN DEM+MASC PL ◦ PRN DEM+MASC SG
◦ PRN DEM+PL ◦ PRN DO+1 PL ◦ PRN DO+1 SG
◦ PRN DO+2 FEM SG ◦ PRN DO+2 MASC PL ◦ PRN DO+2 MASC SG
◦ PRN DO+3 DU ◦ PRN DO+3 FEM SG ◦ PRN DO+3 MASC PL
◦ PRN DO+3 MASC SG ◦ PRN INTERROG ◦ PRN INTERROG+FEM SG
◦ PRN REL ◦ PRN REL+FEM SG ◦ PUNC
◦ PV ◦ PV+1 PL ◦ PV+1 SG
◦ PV+2 FEM SG ◦ PV+2 MASC PL ◦ PV+2 MASC SG
◦ PV+3 FEM DU ◦ PV+3 FEM PL ◦ PV+3 FEM SG
◦ PV+3 MASC DU ◦ PV+3 MASC PL ◦ PV+3 MASC SG
◦ PV PASS ◦ PV PASS+1 PL ◦ PV PASS+1 SG
◦ PV PASS+3 FEM DU ◦ PV PASS+3 FEM PL ◦ PV PASS+3 FEM SG
◦ PV PASS+3 MASC DU ◦ PV PASS+3 MASC PL ◦ PV PASS+3 MASC SG
◦ SUB CONJ

182


