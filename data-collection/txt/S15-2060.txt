



















































EL92: Entity Linking Combining Open Source Annotators via Weighted Voting


Proceedings of the 9th International Workshop on Semantic Evaluation (SemEval 2015), pages 355–359,
Denver, Colorado, June 4-5, 2015. c©2015 Association for Computational Linguistics

EL92: Entity Linking Combining Open Source Annotators  

via Weighted Voting 

 

 

Pablo Ruiz and Thierry Poibeau 
Laboratoire LATTICE 

CNRS, École Normale Supérieure, U. Paris 3 Sorbonne Nouvelle 

1, rue Maurice Arnoux 

92120 Montrouge, France 
{pablo.ruiz.fabo,thierry.poibeau}@ens.fr 

 
 

 

 

  

Abstract 

Our participation at SemEval’s Multilingual 

All-Words Sense Disambiguation and Entity 

Linking task is described. An English entity 

linking (EL) system is presented, which com-

bines the annotations of four public open 

source EL services. The annotations are com-

bined through a weighted voting scheme in-

spired on the ROVER method, which had not 

been previously tested on EL outputs. Results 

on the task’s EL items were competitive.  

1 Introduction 

The paper describes our participation at SemEval 

2015, Task 13 (Moro and Navigli, 2015): Multilin-

gual all-words Sense Disambiguation (WSD) and 

Entity Linking (EL). Systems performing both 

tasks, or either one, can participate. The preferred 

word-sense and entity inventory is Babelnet (Navi-

gli and Ponzetto, 2012); other inventories are al-

lowed. Our system performs English EL to 

Wikipedia, combining the output of open-source, 

publicly available EL systems via weighted voting. 

The system is relevant to the task’s interest in 

comparing the results of EL systems that apply 

encyclopedic knowledge only, like ours, and sys-

tems that jointly exploit encyclopedic and lexico-

graphic resources for EL.  

The paper’s structure is the following: Section 2 

discusses related work, and Section 3 describes the 

system. Sections 4 and 5 present the results and a 

conclusion. 

2 Related Work 

General surveys on EL can be found in (Cornolti et 

al., 2013) and (Rao et al., 2013). Work on combin-

ing NLP annotators and on evaluating EL systems 

is particularly relevant for our submission.  

The goal of combining different NLP systems is 

obtaining combined results that are better than the 

results of each individual system. Fiscus (1997) 

created the ROVER method, with weighted voting 

to improve speech recognition outputs. A ROVER 

was found to improve parsing results by De la 

Clergerie et al. (2008). Rizzo et al. (2014) im-

proved Named Entity Recognition results, combin-

ing systems via different machine learning 

algorithms. Our approach is inspired on the 

ROVER method, which had not been previously 

attempted for EL to our knowledge. Systems that 

combine entity linkers exist (NERD, Rizzo and 

Troncy, 2012). However, a difference in our sys-

tem is that the set of linkers we combine is public 

and open-source. A second difference is the set of 

methods we employed to combine annotations.  

EL evaluation work (Cornolti et al., 2013), 

(Usbeck et al., 2015) has highlighted to what an 

extent EL systems’ performance can differ depend-

ing on characteristics of the corpus. This motivates 

testing whether different EL systems, properly 

combined, can complement each other.   

355



3 System Description 

The system performs English EL to Wikipedia, 

combining the outputs of the following EL sys-

tems: Tagme 2
1
 (Ferragina and Scaiella, 2010), 

DBpedia Spotlight
2
 (Mendes et al. 2011), Wikipe-

dia Miner
3
 (Milne and Witten, 2008) and Babelfy

4
 

(Moro et al. 2014). Babelfy outputs were only con-

sidered if they started with a WIKI prefix or their 

first character was uppercase.
5
 Details about each 

of our workflow’s steps follow.  

3.1 Individual Systems’ Thresholds 

First of all, a client requests the annotations for a 

text from each linker’s web-service, using the ser-

vices’ default settings except for the confidence 

threshold, which is configured in our system. An-

notations whose confidence is below a threshold 

are eliminated. 

All of the linkers used, except Babelfy, output 

confidence scores for their annotations. Cornolti et 

al., (2013) reported optimal confidence-score 

thresholds for all our linkers (except Babelfy). Us-

ing Cornolti’s BAT Framework, we verified that 

the thresholds are still valid.
6
 We adopted the 

weak-annotation match thresholds for the IITB 

dataset, since we consider the IITB corpus close to 

the task’s data, in text-length and topical variety. 

Our thresholds were 0.102 for Tagme, 0.023 for 

Spotlight, and 0.219 for Wikipedia Miner. Since 

Babelfy does not output confidence scores, all of 

its annotations were accepted to the next step in the 

workflow.  

3.2 Ranking the Systems to Combine 

Our method for combining annotators’ outputs re-

quires the annotators to be previously ranked for 

precision on an annotated reference set. It is not 

viable to annotate a reference set for each new cor-

pus. To help overcome this issue, we adopt the fol-

lowing heuristic: We have ranked the annotators 

                                                           
1 http://tagme.di.unipi.it/tagme_help.html 
2 https://github.com/dbpedia-spotlight/dbpedia-spotlight/wiki 
3 http://wikipedia-miner.cms.waikato.ac.nz/ 
4 http://babelfy.org/download.jsp 
5 Babelfy was a late addition to our pipeline; the reader will 

note that we made some ad-hoc decisions to benefit from its 

outputs while complying with previously defined features in 

our workflow. 
6 https://github.com/marcocor/bat-framework 

on a series of very different reference corpora. To 

perform EL on a new corpus, our heuristic consid-

ers the following criteria: First, the types of EL 

annotations needed by the user. Second, how simi-

lar the new corpus is (along dimensions described 

below) to the reference corpora on which we have 

pre-ranked the annotators. To apply the workflow 

to a new corpus, the heuristic chooses the annota-

tor-ranking obtained with the reference corpus that 

is most similar to that new corpus, while still re-

specting the annotation-types needed by the user.  

The reference corpora on which we pre-ranked 

the annotators are AIDA/CoNLL Test B (Hoffart 

et al., 2011), and IITB (Kulkarni et al., 2009). 

These corpora are very different to each other, in 

terms of character length, topical variety, and re-

garding whether they annotate common-noun men-

tions or not. Moreover, some EL systems obtain 

opposite results when evaluated on AIDA/CoNLL 

B vs. IITB, as tests by Cornolti et al. (2013) and on 

the GERBIL platform
7
 have shown.  

The heuristic’s first criterion is the types of an-

notations needed: If the user needs annotations for 

common-noun mentions, the IITB ranking is used, 

since IITB is the only one in our reference-datasets 

that was annotated for such mentions. If the user 

does not need common noun annotations, our heu-

ristic compares the user’s corpus with our two ref-

erence corpora in terms of character length and of 

a measure of lexical cohesion. Both factors have 

been argued to influence linkers’ uneven results 

across corpora (Cornolti et al., 2013). 

We accepted common-noun annotations for the 

task, as they were relevant for the task’s domains 

(e.g. disease names for the biomedical texts). Ac-

cordingly, the heuristic ranked annotators as per 

their IITB results: 1
st
 Wikipedia Miner (0.568 pre-

cision), 2
nd

 Babelfy (0.493), 3
rd

 Spotlight (0.462), 

4
th
 Tagme (0.452).

8
 

3.3 Weighting and Selecting Annotations 

Using the linker ranking from the previous step, 

the annotations are voted, and selected for final 

output or rejected based on the vote. We used two 

                                                           
7 See http://gerbil.aksw.org/gerbil/overview at the site for the 

GERBIL platform (Usbeck et al., 2015):  
8 The precision is from tests in Cornolti et al., 2013, using 

weak-annotation-match. Babefly was not tested. In order to be 

able to rank it, instead of its precision we assigned it the aver-

age of all other annotators’ precisions.  

356



voting schemes. The first one relies on each anno-

tation’s confidence score, weighted by the annota-

tor’s rank and precision on the ranking datasets 

from 3.2. The rationale is that a high-confidence 

annotation for a low-ranked annotator can be better 

than a low-confidence annotation for a higher-

ranked annotator. The definition is in Figure 1: For 

each annotation (m, e) in the results, m is its men-

tion,
9
 e is the entity paired with m, and Ωm is the 

set of annotations in the results whose mentions 

overlap
10

 with m. If the size of Ωm is 1, the scaled 

confidence
11

 oscf of Ωm’s unique annotation ο must 

reach threshold tuniq in order for ο to be accepted. 

Threshold tuniq is the average of the scaled confi-

dence scores for all annotations in the corpus. If 

Ωm has more than one annotation, the voting is 

thus: For each annotation ο in Ωm, ο’s vote is a 

product determined by several factors: oscf is o’s 

scaled confidence.
12

 N is the total number of anno-

tators we combine (i.e. 4). Operand roant is the rank 

of annotator oant, which produced annotation o. 

Poant is that annotator’s precision on the ranking 

reference corpus (3.2 above). For roant, 0 is the best 

rank and N – 1 the worst. Parameter α influences 

the distance between the annotations’ votes based 

on their annotators’ rank, and was set at 0. The 

annotation with the highest vote in Ωm is accepted; 

the rest are rejected. 
 

for each set Ωm of overlapping annotations: 
   if |Ω

m
| = 1 

       for o ∊ Ωm:  if oscf  ≥  tuniq    accept o 

                          else                    reject o 
   else  

       select max
 o ∊ Ωm

[(o
scf 

 ∙ (N − ( roant − α )) ∙ Poant] 

Figure 1: Annotation voting scheme used in Run 1. 

                                                           
9 The string of characters in the text that the annotation is 

based on (the term mention is often used in EL for this notion). 
10 Assume two mentions (p1, e1) and (p2, e2), where p1 and 

p2 are the mentions’ first character indices, and e1 and e2 are 

the mentions’ last character indices. The mentions overlap iff 

((p1 = p2) ˄ (e1 = e2)) ˅ ((p1 = p2) ˄ (e1 < e2)) ˅ ((p1 = p2) ˄ 

(e2 < e1)) ˅ ((e1 = e2) ˄ (p1 < p2)) ˅ ((e1 = e2) ˄ (p2 < p1)) ˅ 

((p1 < p2) ˄ (p2 < e1)) ˅ ((p2 < p1) ˄ (p1 < e2)). 
11 Since the range of confidence-scores output by each annota-

tor was different, we minmax-scaled all original (orig) confi-

dence scores to a 0-1 range: scaled_confidence  =  

(orig_confidence – corpus_min_orig_confidence) /  

(corpus_max_orig_confidence – corpus_min_orig_confidence) 
12 As Babelfy does not provide confidence scores, its annota-

tions were assigned the average over the whole result-set of 

the scaled confidence-scores output by the other annotators. 

The second voting scheme is similar to the 

ROVER method in (De la Clergerie et al., 2008). 

The method assesses annotations based on how 

many linkers have produced them, using the link-

ers’ rank, and their precision on the ranking-sets, 

as weights. If enough lower-ranked annotators 

have linked to an entity, this entity can win over an 

entity proposed by a higher-ranked annotator.  

The voting is defined in Figure 2. For each an-

notation (mention m, entity e), Ωm is the set of an-

notations whose mentions overlap
10

 with m. Based 

on the different entities in Ωm’s annotations, Ωm is 

divided into disjoint subsets, each of which con-

tains annotations linking to a different entity. Each 

of these subsets L is voted by vote(L). In vote(L), 

for each annotation o in L, terms N, roant, α, Poant 

have the same meaning as the terms bearing the 

same names in Figure 1, and are described above. 

 

for each set Ωm of overlapping annotations: 
 

 for L ∊ Ωm: 

      vote(L) = 
∑  (N −  ( roant −  α )) ∙ Poanto ∈ L

N
 

 if max
  L ∊ Ωm

(vote(L) ) > Pmax  : select argmax
  L ∊ Ωm

(vote(L)) 

Figure 2: Entity voting scheme used in Runs 2 and 3. 

The entity for the subset L which obtains the 

highest vote among Ωm’s subsets is selected if its 

vote is higher than Pmax, i.e. the maximum preci-

sion in the ranking dataset (0.568, see Section 3.2). 

After selecting the winning entity, we still need to 

select a mention for it. The mention is selected at 

random among the mentions of the annotations in 

the winning subset L. This implementation of men-

tion selection is meant as a baseline that can be 

refined in the future. Two initial factors to consider 

in mention selection would be mention length and 

the annotators having chosen each mention.  

3.4 Entity Classification 

After the vote, entities in the selected annotations 

are classified before final output. The classification 

is rule-based. It exploits the category or type labels 

output by the EL services we combined—except 

Babelfy, which does not output such information.  

The classification-rules are based on type labels 

in the NERD ontology (Rizzo and Troncy, 2012)
13

 

                                                           
13 http://nerd.eurecom.fr/ontology/nerd-v0.5.n3 

357



and on a subset of the DBpedia ontology classes 

(Mendes et al. 2011)
14

 relevant for the task’s do-

mains. For types Person, Location, Organization, 

Wikipedia category labels were also exploited.  

Some rules involve an exact match against the 

annotations’ categories or types, e.g. “Assign type 

Location if the annotation has type DBpe-

dia:Place”. Some rules involve a partial match, 

e.g. “Assign type Person if one of the Wikipedia 

category labels for the entity contains births”. 

For Babelfy outputs, Wikipedia category labels 

and DBpedia types were obtained through Wikipe-

dia Miner’s
3
 and DBpedia’s

15
 APIs. 

4 Results and Discussion 

Since the task was open to systems doing either 

WSD or EL, or both, the corpus targeted both 

WSD and EL. Participant systems were evaluated 

on a different set of items depending on their na-

ture (EL only, WSD only, both). The corpus con-

tained 4 generic and domain-specific documents 

with 1094 single-word instances, 82 multi-words 

and 86 named entities (NE). 

Our system was conceived and evaluated as an 

EL system. Table 1 shows our precision, recall and 

F1 for all three runs. Column TopF1 is the maxi-

mum F1 attained by a participant on the EL items.  

 

EL P R F1 TopF1 

Run1 100 75.6 86.1 

88.9 Run2 98.3 66.3 79.2 

Run3 100 66.3 79.7 

Table 1: English EL results for all domains. 

Run 1 results were competitive, ranking 3
rd

 of 10, 

if we compare all participants’ best runs. Runs 2 

and 3 lag behind, due to lower recall. Run 1 em-

ployed the voting scheme in Figure 1. Runs 2 and 

3 correspond to the scheme defined in Figure 2, 

with parameter α set to 0 in Run 2 and to 1 in 

Run 3. In spite of its results, the voting scheme 

from Figure 2 has advantages over the first one: It 

does not require confidence scores, so it accom-

modates linkers that don’t score their annotations. 

Also, it does not need a separate threshold to de-

cide on annotations produced by one annotator on-

ly. More work is needed to determine the reason 

                                                           
14 http://mappings.dbpedia.org/server/ontology/classes/ 
15 http://dbpedia.org/sparql 

for this difference in results, i.e. whether the sec-

ond approach itself is not useful to combine EL 

annotations, or whether its worse results were re-

lated to our implementation. 

One of the task’s purposes was to compare sys-

tems’ performance across domains. Table 2 shows 

our best run’s results per domain. Column N re-

flects the number of EL items in the corpus for 

each domain. All other columns have the same 

meaning as in Table 1, but considering the per-

domain results.  
 

 N P R F1 TopF1 

Biomedical 48 100 83.3 90.9 100 

Math & Computer 22 100 54.4 70.6 74.3 

General 16 100 81.3 89.7 90.3 

Table 2: English EL Run 1 results by domain. 

Note that the small number of EL items availa-

ble for each domain limits in our opinion the relia-

bility of interpretations for these results. 

Since our workflow combines several EL sys-

tems, it would be interesting to compare results for 

each individual system by itself vs. the results for 

the combined system. In later work (Ruiz and 

Poibeau, 2015), using an improved version of the 

system described here, and larger EL golden-sets, 

we performed such comparisons, finding signifi-

cant improvements in the combined system vs. the 

individual ones.  

5 Conclusion 

The entity linking (EL) system presented was 

ranked 3rd (out of 10) on the task’s EL items. The 

system combines the outputs of four public open 

source EL services. Two weighted voting methods 

were described to combine the outputs. The first 

method relies on annotations’ confidence scores; 

the second one is a weighted majority vote. The 

first method obtained better results, but the second 

one has the advantage of being easily applicable to 

non-scored annotations. More work is needed to 

assess the reasons for the methods’ differential per-

formance. Future work also includes adding other 

public open source systems to the workflow.  

Acknowledgments 

Pablo Ruiz was supported by a PhD scholarship 

from Région Île-de-France.  

358



References 

Marco Cornolti, Paolo Ferragina, and Massimiliano 

Ciaramita. (2013). A framework for benchmarking 

entity-annotation systems. In Proc. of WWW, 249–

260. 

Éric Vilemonte De La Clergerie, Olivier Hamon, Dja-

mel Mostefa, Christelle Ayache, Patrick Paroubek, 

and Anne Vilnat. (2008). Passage: from French par-

ser evaluation to large sized treebank. In Proc. of 

LREC 2008, 3570–3576. 

Paolo Ferragina and Ugo Scaiella. (2010). Tagme: on-

the-fly annotation of short text fragments (by wikipe-

dia entities). In Proc. of CIKM’10, 1625–1628. 

Jonathan G Fiscus. (1997). A post-processing system to 

yield reduced word error rates: Recognizer output 

voting error reduction (ROVER). In Proc. of the 

IEEE Workshop on Automatic Speech Recognition 

and Understanding, 1997, 347–354. 

Johannes Hoffart, Mohamed Amir Yosef, Ilaria Bordi-

no, Hagen Fürstenau, Manfred Pinkal, Marc Spaniol, 

Bilyana Taneva, Stefan Thater, and Gerhard 

Weikum. (2011). Robust disambiguation of named 

entities in text. In Proc. of EMNLP, 782–792. 

Sayali Kulkarni, Amit Singh, Ganesh Ramakrishnan, 

and Soumen Chakrabarti. (2009). Collective annota-

tion of Wikipedia entities in web text. In Proc. ACM 

SIGKDD, 457–466. 

Pablo N. Mendes, Max Jakob, Andrés García-Silva, and 

Christian Bizer. (2011). DBpedia spotlight: shedding 

light on the web of documents. In Proc. of the 7th 

Int. Conf. on Semantic Systems, I-SEMANTICS’11, 

1–8. 

David Milne and Ian H. Witten. (2008). An effective, 

low-cost measure of semantic relatedness obtained 

from Wikipedia links. In Proc. of AAAI Workshop on 

Wikipedia and Artificial Intelligence: an Evolving 

Synergy, 25–30 

Andrea Moro and Roberto Navigli (2015) SemEval-

2015 Task 13: Multilingual All-Words Sense Disam-

biguation and Entity Linking. In Proc. of SemEval-

2015. 

Andrea Moro, Alessandro Raganato, and Roberto Navi-

gli. (2014). Entity Linking meets Word Sense Dis-

ambiguation: A Unified Approach. Transactions of 

the ACL, 2, 231–244. 

Roberto Navigli and Simone Ponzetto. (2012). 

BabelNet: The automatic construction, evaluation 

and application of a wide-coverage multilingual se-

mantic network. Artificial Intelligence, 193, 217–

250. 

Delip Rao, Paul McNamee, and Mark Dredze. (2013). 

Entity linking: Finding extracted entities in a 

knowledge base. In Multi-source, Multilingual In-

formation Extraction and Summarization, 93–115. 

Springer.  

Giuseppe Rizzo and Raphaël Troncy. (2012). NERD: a 

framework for unifying named entity recognition and 

disambiguation extraction tools. In Proc. of the 

Demonstrations at EACL’12, 73–76. 

Giuseppe Rizzo, Marieke van Erp, and Raphaël Troncy. 

(2014). Benchmarking the Extraction and Disambig-

uation of Named Entities on the Semantic Web. In 

Proc. of LREC 2014, 4593–4600. 

Pablo Ruiz and Thierry Poibeau. (2015). Combining 

Open Source Annotators for Entity Linking through 

Weighted Voting. In Proceedings of *SEM 2015. 

Fourth Joint Conference on Lexical and Computa-

tional Semantics. Denver, U.S.  

Ricardo Usbeck, Michael Röder, Axel-Cyrille Ngonga, 

Ciro Baron, Andrea Both, Martin Brümmer, Diego 

Ceccarelli, Marco Cornolti, Didier Cherix, Bernd 

Eickmann, Paolo Ferragina, Christiane Lemke, An-

drea Moro, Roberto Navigli, Francesco Piccino, 

Giuseppe Rizzo, Harald Sack, René Speck, Raphaël 

Troncy, Jörg Waitelonis, and Lars Wesemann. 

(2015). GERBIL–General Entity Annotator Bench-

marking Framework. In Proc. of WWW.  

359


