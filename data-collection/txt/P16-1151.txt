



















































Discovery of Treatments from Text Corpora


Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 1600–1609,
Berlin, Germany, August 7-12, 2016. c©2016 Association for Computational Linguistics

Discovery of Treatments from Text Corpora

Christian Fong
Stanford University

655 Serra Street
Stanford, CA 94305, USA

christianfong@stanford.edu

Justin Grimmer
Stanford University

616 Serra Street
Stanford, CA 94305, USA

jgrimmer@stanford.edu

Abstract

An extensive literature in computational
social science examines how features of
messages, advertisements, and other cor-
pora affect individuals’ decisions, but
these analyses must specify the relevant
features of the text before the experiment.
Automated text analysis methods are able
to discover features of text, but these meth-
ods cannot be used to obtain the estimates
of causal effects—the quantity of inter-
est for applied researchers. We introduce
a new experimental design and statistical
model to simultaneously discover treat-
ments in a corpora and estimate causal ef-
fects for these discovered treatments. We
prove the conditions to identify the treat-
ment effects of texts and introduce the su-
pervised Indian Buffet process to discover
those treatments. Our method enables us
to discover treatments in a training set us-
ing a collection of texts and individuals’
responses to those texts, and then esti-
mate the effects of these interventions in
a test set of new texts and survey respon-
dents. We apply the model to an exper-
iment about candidate biographies, recov-
ering intuitive features of voters’ decisions
and revealing a penalty for lawyers and a
bonus for military service.

1 Introduction

Computational social scientists are often inter-
ested in inferring how blocks of text, such as mes-
sages from political candidates or advertising con-
tent, affect individuals’ decisions (Ansolabehere
and Iyengar, 1995; Mutz, 2011; Tomz and Weeks,
2013). To do so, they typically attempt to estimate
the causal effect of the text: they model the out-

come of interest, Y , as a function of the block of
text presented to the respondent, t, and define the
treatment effect of t relative to some other block
of text t′ as Y (t) − Y (t′) (Rubin, 1974; Hol-
land, 1986). For example, in industrial contexts
researchers design A/B tests to compare two po-
tential texts for a use case. Academic researchers
often design one text that has a feature of inter-
est and another text that lacks that feature but is
otherwise identical (for example, (Albertson and
Gadarian, 2015)). Both kinds of experiments as-
sume researchers already know the features of text
to vary and offer little help to researchers who
would like to discover the features to vary.

Topic models and related methods can discover
important features in corpora of text data, but they
are constructed in a way that makes it difficult
to use the discovered features to estimate causal
effects (Blei et al., 2003). Consider, for exam-
ple, supervised latent Dirichlet allocation (sLDA)
(Mcauliffe and Blei, 2007). It associates a topic-
prevalence vector, θ, with each document where
the estimated topics depend upon both the con-
tent of documents and a label associated with each
document. If K topics are included in the model,
then θ is defined on the K − 1-dimensional unit
simplex. It is straightforward to define a treatment
effect as the difference between two treatments θ
and θ

′
(or points on the simplex) Y (θ) − Y (θ′).

It is less clear how to define the marginal effect
of any one dimension. This is because bigger
values on some dimensions implies smaller val-
ues on other dimensions, making the effect of any
one topic necessarily a combination of the differ-
ences obtained when averaging across all the di-
mensions (Aitchison, 1986; Katz and King, 1999).
This problem will befall all topic models because
the zero-sum nature of the topic-prevalence vector
implies that increasing the prevalence of any one
topic necessarily decreases the prevalence of some

1600



other topic. The result is that it is difficult (or im-
possible) to interpret the effect of any one topic
marginalizing over the other topics. Other appli-
cations of topic models to estimate causal effects
treat text as the response, rather than the treatment
(Roberts et al., 2016). And still other methods re-
quire a difficult to interpret assumption of how text
might affect individuals’ responses (Beauchamp,
2011).

To facilitate the discovery of treatments and
to address the limitation of existing unsupervised
learning methods, we introduce a new experimen-
tal design, framework, and statistical model for
discovering treatments within blocks of text and
then reliably inferring the effects of those treat-
ments. By doing so, we combine the utility of
discovering important features in a topic model
with the scientific value of causal treatment ef-
fects estimated in a potential outcomes frame-
work. We present a new statistical model—the su-
pervised Indian Buffet Process—to both discover
treatments in a training set and infer the effects
treatments in a test set (Ghahramani and Griffiths,
2005). We prove that randomly assigning blocks
of text to respondents in an experiment is suffi-
cient to identify the effects of latent treatments that
comprise blocks of text.

Our framework provides the first of its kind
approach to automatically discover treatment ef-
fects in text, building on literatures in both social
science and machine learning (Blei et al., 2003;
Beauchamp, 2011; Mcauliffe and Blei, 2007;
Roberts et al., 2016). The use of the training
and test set ensures that this discovery does not
come at the expense of credibly inferring causal
effects, insulating the research design from con-
cerns about “p-hacking” and overfitting (Ioanni-
dis, 2005; Humphreys et al., 2013; Franco et al.,
2014). Critically, we use a theoretical justification
for our methodology: we select our particular ap-
proach because it enables us to estimate causal ef-
fect of interest. Rather than demonstrating that our
method performs better at some predictive task,
we prove that our method is able to estimate useful
causal effects from the data.

We apply our framework to study how features
of a political candidate’s background affect voters’
decisions. We use a collection of candidate biogra-
phies collected from Wikipedia to automatically
discover treatments in the biographies and then in-
fer their effects. This reveals a penalty for lawyers

and career politicians and a bonus for military ser-
vice and advanced degrees. While we describe
our procedure throughout the paper, we summa-
rize our experimental protocol and strategy for dis-
covering treatment effects in Table 1.

Table 1: Experimental Protocol for Discovering
and Estimating Treatment Effects

1) Randomly assign texts, Xj , to respon-
dents

2) Obtain response Yi for each respondent.

3) Divide texts and responses into training
and test set

4) In training set:

a) Use supervised Indian Buffet Pro-
cess (sIBP) applied to documents
and responses to infer latent treat-
ments in texts

b) Model selection via quantitative fit
and qualitative assessment

5) In test set:

a) Use sIBP trained on training set to
infer latent treatments on test set
documents

b) Estimate effect of treatments with
regression, with a bootstrap proce-
dure to estimate uncertainty

2 A Framework for Discovering
Treatments from Text

Our goal is to discover a set of features—
treatments—underlying texts and then estimate
the effect of those treatments on some response
from an individual. We first show that randomly
assigning texts to respondents is sufficient to iden-
tify treatment effects. We then provide a statistical
model for using both the text and responses to dis-
cover latent features in the text that affect the re-
sponse. Finally, we show that we can use the map-
ping from text to features discovered on a training
set to estimate the presence of features in a test
set, which allows us to estimate treatment effects
in the test set.

1601



2.1 Randomizing Texts Identifies Underlying
Treatment Effects

When estimating treatment effects, researchers of-
ten worry that the respondents who received one
treatment systematically differ from those who re-
ceived some other treatment. In a study of adver-
tising, if all of the people who saw one advertise-
ment were men and all of the people who saw a
different advertisement were women, it would be
impossible to tell whether differences in their re-
sponses were driven by the fact that they saw dif-
ferent advertisements or by their pre-existing dif-
ferences. Randomized experiments are the gold
standard for overcoming this problem (Gerber and
Green, 2012). However, in text experiments, in-
dividuals are randomly assigned to blocks of text
rather than to the latent features of the text that we
analyze as the treatments. In this section, we show
that randomly assigning blocks of text is sufficient
to identify treatment effects.

To establish our result, we suppose we have a
corpora of J texts, X . We represent a specific
text with Xj ∈ X , with Xj ∈ <D. Through-
out we will assume that we have standardized the
variableXj to be a per-document word usage rate
with each column normalized to have mean zero
and variance one. We have a sample of N respon-
dents from a population, with the response of in-
dividual i to text j[i] given by the potential out-
come Yi(Xj[i]). We use the notation j[i] because
multiple individuals may be assigned to the same
text; if i and i′ are assigned to the same text, then
j[i] = j[i′]. We suppose that for each document j
there is a corresponding vector of K binary treat-
ments Zj ∈ Z where Z contains all 2K possible
combinations of treatments, {0, 1}K . The func-
tion g : X → Z maps from the texts to the set
of binary treatments: we will learn this function
using the supervised Indian Buffet process intro-
duced in the next section. Note that distinct ele-
ments of X may map to the same element of Z.

To establish our identification result, we assume
(Assumption 1) Yi(X) = Yi(Xj[i]) for all i. This
assumption ensures that each respondent’s treat-
ment assignment depends only on her assigned
text, a version of the Stable Unit Treatment Value
Assumption (SUTVA) for our application (Ru-
bin, 1986). We also assume (Assumption 2) that
Yi(Xj[i]) = Yi(g(Xj[i])) for allXj[i] ∈ X and all
i, or that Zj[i] is sufficent to describe the effect of
a document on individual i’s response. Stated dif-

ferently, we assume an individual would respond
in the same way to two different texts if those
texts have the same latent features. We further
suppose (Assumption 3) that texts are randomly
assigned to respondents according to probability
measure h, ensuring that Yi(g(Xj[i])) ⊥⊥ Xj[i]
for allXj[i] ∈ X and for all individuals i. This as-
sumption ensures unobserved characteristics of in-
dividuals are not confounding inferences about the
effects of texts. The random assignment of texts
to individuals induces a distribution over a prob-
ability measure on treatment vectors Z, f(Z) =∫
X 1(Z = g(X))h(X)dX . Finally, we assume

(Assumption 4) that f(Z) > 0 for all Z ∈ Z .1
This requires that every combination of treatment
effects is possible from the documents in our cor-
pus. In practice, when designing our study we
want to ensure that the treatments are not aliased
or perfectly correlated. If perfect correlation exists
between factors, we are unable to disentangle the
effect of individual factors.

In this paper we focus on estimating the
Average Marginal Component Specific Effect
for factor k (AMCEk) (Hainmueller et al.,
2014).2 The AMCEk is useful for finding
the effect of one feature, k, when k interacts
with the other features in some potentially
complicated way. It is defined as the differ-
ence in outcomes when the feature is present
and when it is not present, averaged over the val-
ues of all of the other features. Formally, AMCEk =∫
Z−k

E [Y (Zk = 1,Z−k)− Y (Zk = 0,Z−k)]m(Z−k)dZ−k

where m(Z−k) is some analyst-defined density
on all elements but k of the treatment vector.
For example, m(·) can be chosen as the density
of Z−k in the population to obtain the marginal
component effect of k in the empirical population.
The most commonly used m(·) in applied work
is uniform across all Z−k’s, and we follow this
convention here.

We now prove that assumptions 1, 2, 3, and 4
are sufficient to identify the AMCEk for all k.

Proposition 1. Assumptions 1, 2, 3, and 4 are suf-

1Note for this assumption to hold it is necessary, but not
sufficient that g is a surjection from X onto Z .

2The procedure here can be understood as a method for
discovering the treatments that are imposed by assumption in
conjoint analysis, as presented by (Hainmueller et al., 2014).
We deploy the regression estimator used in conjoint analy-
sis as a subroutine of our procedure (see Step 5b in Table
1), but otherwise our experimental design, statistical method,
and proof is distinct.

1602



ficient to identify the AMCEk for arbitrary k.

Proof. To obtain a useful form, we first
marginalize over the documents to obtain,∫
Z−k

∫
X E [Y (Zk = 1,Z−k)] f(Z−k|Zk =

1,X) − E [Y (Zk = 0,Z−k)] f(Z−k|Zk =
0,X)h(X)dXdZ−k =∫

Z−k
E [Y (Zk = 1,Z−k)] f(Z−k|Zk = 1)

−E [Y (Zk = 0,Z−k)] f(Z−k|Zk = 0)dZ−k
.
Where f(Z−k|Zk = 1) and f(Z−k|Zk = 0)
are the induced distributions over latent features
from averaging over documents. If f(Z−k|Zk =
0) = f(Z−k|Zk = 1) = m(Z−k) then this is the
AMCEk. Otherwise consider m(Z) > 0 for all
Z ∈ Z . Because f(Z) > 0, f(Z−k|Zk = 0) > 0
and f(Z−k|Zk = 1) > 0. Thus, there exists con-
ditional densities h(Z|Zk = 1) and h(Z|Zk =
0) such that f(Z−k|Zk=1)h(Z−k|Zk=1) =

f(Z−k|Zk=0)
h(Z−k|Zk=0) =

m(Z−k)

2.2 A Statistical Model for Identifying
Features

The preceding section shows that if we are able
to discover features in the data, we can estimate
their AMCEs by randomly assigning texts to re-
spondents. We now present a statistical model for
discovering those features. As we argued in the in-
troduction, it is difficult to use the topics obtained
from topic models like sLDA because the topic
vector exists on the simplex. When we compare
the outcomes associated with two different topic
vectors, we do not know whether the change in
the response is caused by increasing the degree to
which the document about one topic or decreas-
ing the degree to which it is about another, be-
cause the former mathematically entails the latter.
Other models, such as LASSO regression, would
necessarily suppose that the presence and absence
of words are the treatments (Hastie et al., 2001;
Beauchamp, 2011). This is problematic substan-
tively, because it is hard to know exactly what the
presence or absence of a single word implies as a
treatment in text.

We therefore develop the supervised Indian
Buffet Process (sIBP) to discover features in the
document. For our purposes, the sIBP has two es-
sential properties. First, it produces a binary topic
vector, avoiding the complications of treatments
assigned on the simplex. Second, unlike the Indian

Buffet Process upon which it builds (Ghahramani
and Griffiths, 2005), it incorporates information
about the outcome associated with various texts,
and therefore discovers features that explain both
the text and the response.3

Figure 1 describes the posterior distribution for
the sIBP and a summary of the posterior is given in
Equation 1. We describe the model in three steps:
the treatment assignment process, document cre-
ation, and response. The result is a model that
creates a link between document content and re-
sponse through a vector of treatment assignments.

Treatment Assignment We assume that π is a
K-vector (where we take the limit as K → ∞)
where πk describes the population proportion of
documents that contain latent feature k. We sup-
pose that π is generated by the stick-breaking con-
struction (Doshi-Velez et al., 2009). Specifically,
we suppose that ηk ∼ Beta(α, 1) for all K. We
label π1 = η1 and for each remaining topic, we
assume that πk =

∏k
z=1 ηz . For document j and

topic k, we suppose that zj,k ∼ Bernoulli(πk),
which importantly implies that the occurrence of
treatments are not zero sum. We collect the treat-
ment vector for document j into Zj and collect
all the treatment vectors into Z an Ntexts × K
binary matrix, where Ntexts refers to number of
unique documents. Throughout we will assume
that Ntexts = N or that the number of documents
and responses are equal and index the documents
with i.

Document Creation We suppose that the doc-
uments are created as a combination of latent
factors. For topic k we suppose that Ak is a
D−dimensional vector that maps latent features
onto observed text. We collect the vectors into
A, a K × D matrix, and suppose that Xi ∼
MVN(ZiA, σ2nID), where Xi,d is the standard-
ized number of times word d appears in docu-
ment i. While it is common to model texts as
draws from multinomial distributions, the multi-

3We note that there is a different model also called the
supervised Indian Buffet Process (Quadrianto et al., 2013).
There are fundamental differences between the model pre-
sented here and the sIBP in (Quadrianto et al., 2013). Their
outcome is a preference relation tuple, while ours is a real-
valued scalar. Because of this difference, the two models are
fundamentally different. This leads to a distinct data gener-
ating process, model inference procedures, and inferences of
features on the test set. To leverage the analogy between LDA
and sLDA vis a vis IBP and sIBP, we overload the term sIBP
in our paper. We expect that in future applications of sIBP, it
will be clear from the context which sIBP is being employed.

1603



Z

X

A π

Y

β τ

Figure 1: Graphical Model for the Supervised In-
dian Buffet Process

variate normal distribution is useful for our pur-
poses for two reasons. First, we normalize our data
by transforming each column X ·,d to be mean 0
and variance 1, ensuring that the multivariate nor-
mal distribution captures the overall contours of
the data. Note that this implies that Xi,d can be
negative. Second, we show that assuming a multi-
variate normal for document generation results in
parameters that capture the distinctive rate words
are used for each latent feature (Doshi-Velez et al.,
2009).

Response to Treatment Vector We assume that
a K−vector of parameters β describes the re-
lationship between the treatment vector and re-
sponse. Specifically, we use a standard parameter-
ization and suppose that τ ∼ Gamma(a, b), β ∼
MVN(0, τ−1) and that Yi ∼ Normal(Ziβ, τ−1).

πk ∼ Stick-Breaking (α)
zi,k ∼ Bernoulli(πk)

Xi|Zi,A ∼ MVN(ZiA, σ2XID)
Ak ∼ MVN(0, σ2AID)

Yi|Zi,β ∼ Normal(Ziβ, τ−1)
τ ∼ Gamma(a, b)

β|τ ∼ MVN(0, τ−1IK) (1)

2.2.1 Inference for the Supervised Indian
Buffet Process

We approximate the posterior distribution
with a variational approximation, building
on the algorithm introduced in (Doshi-Velez
et al., 2009). We approximate the non-
parametric posterior setting K to be large
and use a factorized approximation, assuming

that p(π,Z,A,β, τ |X,Y , α, σ2A, σ2X , a, b) =
q(π)q(A)q(Z)q(β, τ)

A standard derivation that builds on (Doshi-
Velez et al., 2009) leads to the following distribu-
tional forms and update steps:

• q(πK) = Beta(πk|λk). The update values
are λk,1 = αK +

∑N
i=1 νi,k and λk,2 = 1 +∑N

i=1(1− νi,k).
• q(Ak) = Multivariate Normal(Ak|φ̄k,Φk).

The updated parameter values are,

φ̄k =[
1
σ2X

∑N
i=1 νi,k

(
Xi −

(∑
l:l 6=k νi,lφ̄l

))]
Φk

Φk =
(

1
σ2A

+
∑N

i=1 νi,k
σ2X

)−1
I

• q(β, τ ) = Multivariate Normal(β|m,S) ×
Gamma(τ |c, d). The updated parameter val-
ues are,

m = SE[ZT ]Y
S = (E[ZTZ] + IK)−1

c = a+
N

2

d = b+
YTY− YTE[Z]SE[ZT ]Y

2

Where typical element of E[ZT ]j,k =
νj,k and typical on-diagonal element of
E[ZTZ]k,k =

∑N
i=1 νi,k and off-diagonal el-

ement is E[ZTZ]j,k =
∑N

i=1 νi,jνi,k.

• q(zi,k) = Bernoulli(zi,k|νi,k). The updated
parameter values are
vi,k = ψ(λk,1)− ψ(λk,2)− 12σ2X [−2φ̄kX

T
i

+(tr(Φk) + φ̄kφ̄Tk ) + 2φ̄k
(∑

l:l 6=k νi,lφ̄
T
l

)
]

− c2d(−2mkYi +
(
dSk,k
c−1 +m

T
kmk

)
+

2mk
(∑

l:l 6=k νi,lml
)

)

νi,k = 11+exp(−vi,k)
where ψ(·) is the digamma function. We re-
peat the algorithm until the change in the pa-
rameter vector drops below a threshold.

To select the final model using the training set
data, we perform a two-dimensional line search
over values of α and σX .4 We then run the model

4We assign σA, a, and b values which lead to diffuse pri-
ors.

1604



several times for each combination of values for α
and σX to evaluate the output at several different
local modes. To create a candidate set of models,
we use a quantitative measure that balances coher-
ence and exclusivity (Mimno et al., 2011; Roberts
et al., 2014). Let Ik be the set of documents for
which νi,k ≥ 0.5, and let ICk be the complement
of this set. We identify the top ten words for inter-
vention k as the ten words with the largest value
in Ak, tk and define Nk =

∑N
i=1 I{νi,k ≥ 0.5}.

We then obtain measure CE for a particular model
CE =

∑K
k=1Nk

∑
l,c∈tk cov(XIk,l,XIk,c) −∑K

k=1(N −Nk)
∑

l,c∈tk cov(XICk ,l,XICk ,c)
where here XIk,l refers to the l

th column and Ikth
rows ofX . We make a final model selection based
on the model that provides the most substantively
clear treatments.

2.3 Inferring Treatments and Estimating
Effects in Test Set

To discover the treatment effects, we first suppose
that we have randomly assigned a set of respon-
dents a text based treatmentXi according to some
probability measure h(·) and that we have ob-
served their response Yi. We collect the assigned
texts into X and the responses into Y . As we de-
scribe below, we will often assign each respondent
their own distinctive message, with the probabil-
ity of receiving any one message at 1N for all re-
spondents and messages. We use the sIBP model
trained our training set documents and responses
to infer the effect of those treatments among the
test set documents. Separating the documents and
responses into training and test sets ensures that
Assumption 1, SUTVA, holds. We learn the map-
ping from texts to binary vectors in the training
set, ĝ(·) and then apply this mapping to the test
set to infer the latent treatments present in the test
set documents, without considering the test set re-
sponses. Dividing texts and responses into training
and test sets provides a solution to SUTVA viola-
tions present in other attempts at causal inference
in text analysis (Roberts et al., 2014).

We approximate the posterior distribution
for the treatment vectors using the variational
approximation from the training set parameters
(λ̂, ̂̄φ, Φ̂, m̂, Ŝ, ĉ, d̂, σ̂2X , σ̂2A) and a modified
update step on q(ztesti,k ). In this modified update
step, we remove the component of the update
that incorporates information about the outcome.
Specifically for individual i in the test set for

category k we have the following update step
vtesti,k = ψ(λ̂k,1)− ψ(λ̂k,2)− 12(σ̂2X)

×
[−2̂̄φk(XTi ) + (tr(Φ̂k) + ̂̄φk(̂̄φk)T ) +

2̂̄φk (∑l:l 6=k νi,l ( ̂̄φlT))]
ν testi,k =

1
1+exp(−vtesti,k)

.

For each text in the test set we repeat this update
several times until ν test has converged. Note that
for the test set we have excluded the component
of the model that links the latent features to the
response, ensuring that SUTVA holds.

With the approximating distribution q(Z test) we
then measure the effect of the treatments in the test
set. Using the treatments, the most straightforward
model to estimate assumes that there are no inter-
actions between each of the components. Under
the no interactions assumption, we estimate the ef-
fects of the treatments and infer confidence inter-
vals using the following bootstrapping procedure
that incorporates uncertainty both from estimation
of treatments and uncertainty about the effects of
those treatments:

1) For each respondent i and component k we
draw z̃i,k ∼ Bernoulli(ν testi,k ), resulting in ma-
trix Z̃.

2) Given the matrix Z̃, we sample (Y test, Z̃)
with replacement and for each sample esti-
mate the regression Y test = βtestZ̃ + �.

We repeat the bootstrap steps 1000 times, keeping
βtest for each iteration. The result of the proce-
dure is a point estimate of the effects and confi-
dence interval of the treatments under no interac-
tions. Technically, it is possible to estimate the
treatment effects in our variational approximation.
But we estimate the effects in a second-stage re-
gression because variational approximations tend
to understate uncertainty, the bootstrap provides a
straightforward method for including uncertainty
from estimation of the latent features and the ef-
fect estimates, and it ensures that SUTVA is not
violated.

3 Application: Voter Evaluations of an
Ideal Candidate

We demonstrate our method in an experiment to
assess how features of a candidate’s background
affect respondents evaluations of the candidates.
There is a rich literature in political science about

1605



the ideal attributes of political candidates (Canon,
1990; Popkin, 1994; Carnes, 2012; Campbell and
Cowley, 2014). We build on this literature and use
a collection of candidate biographies to discover
features of candidates’ backgrounds that voters
find appealing. To uncover the features of can-
didate biographies that voters are responsive to
we acquired a collection of 1,246 Congressional
candidate biographies from Wikipedia. We then
anonymize the biographies—replacing names and
removing other identifiable information—to en-
sure that the only information available to the re-
spondent was explicitly present in the text.

In Section 2.1 we show that a necessary con-
dition for this experiment to uncover latent treat-
ments is that each vector of treatments has non-
zero probability of occuring. This is equivalent to
assuming that none of the treatments are aliased,
or perfectly correlated (Hainmueller et al., 2014).
Aliasing would be more likely if there are only
a few distinct texts that are provided to partici-
pants in our experiment. Therefore, we assign
each respondent in each evaluation round a dis-
tinct candidate biography. To bolster our statis-
tical power, we ask our respondents to evaluate
up to four distinct candidate biographies, resulting
in each respondent evaluating 2.8 biographies on
average.5 After presenting the respondents with
a candidate’s biography, we ask each respondent
to rate the candidate using a feeling thermometer:
a well-established social science scale that goes
from 0 when a respodent is “cold” to a candidate
to 100 when a respondent is “warm” to the candi-
date.

We recruited a sample of 1,886 participants us-
ing Survey Sampling International (SSI), an online
survey platform. Our sample is census matched
to reflect US demographics on sex, age, race, and
education. Using the sample we obtain 5,303 to-
tal observations. We assign 2,651 responses to the
training set and 2,652 to the test set. We then apply
the sIBP process to the training data. To apply the
model, we standardize the feeling thermometer to
have mean zero and standard deviation 1. We set
K to a relatively low value (K = 10) reflecting
a quantitative and qualitative search over K. We
then select the final model varying the parameters

5The multiple evaluations of candidate biographies is
problematic if there is spillover across rounds of our exper-
iment. We have little reason to believe observing one can-
didate biography would systematically affect the response in
subsequent rounds.

and evaluating the CE score.

Table 2 provides the top words for each of the
ten treatments the sIBP discovered in the training
set. We selected ten treatments using a combina-
tion of guidance from the sIBP, assessment using
CE scores, and our own qualitative assessment of
the models (Grimmer and Stewart, 2013). While
it is true that our final selection depends on human
input, some reliance on human judgment at this
stage is appropriate. If one set includes a treat-
ment about military service but not legal training
and another set includes a treatment about legal
training but not military service, then model selec-
tion is tantamount to deciding which hypotheses
are most worthy of investigation. Our CE scores
identify sets of treatments that are most likely to be
interesting, but the human analyst should make the
final decision about which hypotheses he would
like to test. However, it is extremely important
for the analyst to select a set of treatments first
and only afterwards estimate the effects of those
treatments. If the analyst observes the effects of
some treatments and then decides he would like to
test other sets, then the integrity of any p-values
he might calculate are undermined by the multi-
ple testing problem. A key feature of our proce-
dure is that it draws a clear line between the selec-
tion of hypotheses to test (which leverages human
judgment) and the estimation of effects (which is
purely mechanical).

The estimated treatments cover salient features
of Congressional biographies from the time period
that we analyze. For example, treatments 6 and 10
capture a candidate’s military experience. Treat-
ment 5 and 7 are about previous political experi-
ence and Treatment 3 and 9 refer to a candidate’s
education experience. Clearly, there are many fea-
tures of a candidate’s background missing, but the
treatments discovered provide a useful set of di-
mensions to assess how voters respond to a candi-
date’s background. Further, the discovered treat-
ments are a combination of those that are both
prevalent in the biographies and have an effect on
the thermometer rating. The absence of biograph-
ical features that we might think matters for can-
didate evaluation could be because there are few
of those biographies in our data set, or because the
respondents were unresponive to those features.

After training the model on the training set, we
apply it to the test set to infer the treatments in the
biographies. We assume there are no interactions

1606



Treatment 1 Treatment 2 Treatment 3 Treatment 4 Treatment 5
appointed fraternity director received elected

school graduated distinguished university washington university house
governor war ii received years democratic
worked chapter president death seat
older air force master arts company republican

law firm phi phd training served
elected reserve policy military committee

grandfather delta public including appointed
office air master george washington defeated
legal states air affairs earned bachelors office

Treatment 6 Treatment 7 Treatment 8 Treatment 9 Treatment 10
united states republican star law war

military democratic bronze school law enlisted
combat elected germany law school united states

rank appointed master arts juris doctor assigned
marine corps member awarded student army

medal incumbent played earned juris air
distinguished political yale earned law states army

air force father football law firm year
states air served maternal university school service

air state division body president officer

Table 2: Top Words for 10 Treatments sIBP Discovered

−2.5

0.0

2.5

5.0

1 2 3 4 5 6 7 8 9 10

Feature

E
ff

ec
t 

o
n

 F
ee

lin
g

 T
h

er
m

o
m

et
er

Figure 2: 95% Confidence Intervals for Effects
of Discovered Treatments: The mean value of the
feeling thermometer is 62.3

between the discovered treatments in order to es-
timate their effects.6 Figure 2 shows the point es-
timate and 95-percent confidence intervals, which
take into account uncertainty in inferring the treat-
ments from the texts and the relationship between
those treatments and the response.

The treatment effects reveal intuitive, though in-
teresting, features of candidate biographies that af-
fect respondent’s evaluations. For example, Fig-
ure 2 reveals a distaste for political and legal
experience—even though a large share of Con-
gressional candidates have previous political ex-

6This assumption is not necessary for the framework we
propose here. Interaction effects could be modeled, but it
would require us to make much stronger parametric assump-
tions using a method for heterogeneous treatments such as
(Imai and Ratkovic, 2013).

perience and a law degree. Treatment 5, which de-
scribes a candidate’s previous political experience,
causes an 2.26 point reduction in feeling ther-
mometer evaluation (95 percent confidence inter-
val, [-4.26,-0.24]). Likewise, Treatment 9 shows
that respondents dislike lawyers, with the presence
of legal experience causing a 2.34 point reduction
in feeling thermometer (95-percent confidence in-
terval, [-4.28,-0.29]). The aversion to lawyers is
not, however, an aversion to education. Treat-
ment 3, a treatment that describes advanced de-
grees, causes a 2.43 point increase in feeling ther-
mometer evaluations (95-percent confidence inter-
val, [0.49,4.38]).

In contrast, Figure 2 shows that there is a con-
sistent bonus for military experience. This is
consistent with intuition from political observers
that the public supports veterans. For exam-
ple, treatment 6, which describes a candidate’s
military record, causes a 3.21 point increase in
feeling thermometer rating (95-percent confidence
interval, [1.34,5.12]) and treatment 10 causes a
4.00 point increase (95-percent confidence inter-
val, [1.53,6.45]).

Because simultaneously discovering treatments
from labeled data and estimating their average
marginal component effects is a novel task, we
cannot compare the performance of our frame-
work against any benchmark. Even so, one natu-
ral question is whether the user could obtain much
more coherent topics by foresaking the estimation
of causal effects and using a more traditional topic

1607



modeling method. We provide the topics discov-
ered by sLDA in Table 3. sIBP discovered most
of the same features sLDA did. Both find military
service, legal training, political background, and
higher education. The Greek life feature is less co-
herent in sIBP than it is in sLDA, and sLDA finds
business and ancestry features that sIBP does not.
Both have a few incoherent treatments. This com-
parison suggests that sIBP does almost as well as
sLDA at identifying coherent latent features, while
also facilitating the estimation of marginal treat-
ment effects.

4 Conclusion

We have presented a methodology for discover-
ing treatments in text and then inferring the ef-
fect of those treatments on respondents’ decisions.
We prove that randomizing texts is sufficient to
identify the underlying treatments and introduce
the supervised Indian Buffet process for discover-
ing the effects. The use of a training and test set
ensures that our method provides accurate confi-
dence intervals and avoids the problems of over-
fitting or “p-hacking” in experiments. In an ap-
plication to candidate biographies, we discover a
penalty for political and legal experience and a
bonus for military service and non-legal advanced
degrees.

Our methodology has a wide variety of appli-
cations. This includes numerous alternative ex-
perimental designs, providing a methodology that
computational social scientists could use widely
to discover and then confirm the effects of mes-
sages in numerous domains—including images
and other high dimensional data. The method-
ology is also useful for observational data—for
studying the effects of complicated treatments,
such as how a legislator’s roll call voting record
affects their electoral support.

Acknowledgments

This material is based upon work supported by the
National Science Foundation Graduate Research
Fellowship under Grant No. DGE-114747. Any
opinions, findings, and conclusions or recommen-
dations expressed in this material are those of the
authors and do not necessarily reflect the views of
the National Science Foundation

References
John Aitchison. 1986. The Statistical Analysis of Com-

positional Data. Chapman and Hall.

Bethany Albertson and Shana Kushner Gadarian.
2015. Anxious Politics: Democratic Citizenship in
a Threatening World. Cambridge University Press.

Stephen Ansolabehere and Shanto Iyengar. 1995. Go-
ing Negative: How Political Advertisements Shrink
and Polarize The Electorate. Simon & Schuster, Inc.

Nick Beauchamp. 2011. A bottom-up approach to
linguistic persuasion in advertising. The Political
Methodologist.

David Blei, Andrew Ng, and Michael Jordan. 2003.
Latent dirichlet allocation. Journal of Machine
Learning and Research, 3:993–1022.

Rosie Campbell and Philip Cowley. 2014. What voters
want: Reactions to candidate characteristics in a sur-
vey experiment. Political Studies, 62(4):745–765.

David T. Canon. 1990. Actors, Athletes, and As-
tronauts: Political Amateurs in the United States
Congress. University of Chicago Press.

Nicholas Carnes. 2012. Does the numerical underrep-
resentation of the working class in congress matter?
Legislative Studies Quarterly, 37(1):5–34.

Finale Doshi-Velez, Kurt T. Miller, Jurgen Van Gael,
and Yee Whye Teh. 2009. Variational inference for
the indian buffet process. Technical Report, Univer-
sity of Cambridge.

Annie Franco, Neil Malhotra, and Gabor Simonovits.
2014. Publication bias in the social sciences: Un-
locking the file drawer. Science, 345(6203):1502–
1505.

Alan S. Gerber and Donald P. Green. 2012. Field
Experiment: Design, Analysis, and Interpretation.
W.W. Norton & Company.

Zoubin Ghahramani and Thomas L Griffiths. 2005. In-
finite latent feature models and the indian buffet pro-
cess. In Advances in neural information processing
systems, pages 475–482.

Justin Grimmer and Brandon M. Stewart. 2013. Text
as data: The promise and pitfalls of automatic con-
tent analysis methods for political texts. Political
Analysis, 21(3):267–297.

Jens Hainmueller, Daniel Hopkins, and Teppei Ya-
mamoto. 2014. Causal inference in conjoint anal-
ysis: Understanding multi-dimensional choices via
stated preference experiments. Political Analysis,
22(1):1–30.

Trevor Hastie, Robert Tibshirani, and Jerome Fried-
man. 2001. The Elements of Statistical Learning.
Springer.

1608



Treatment 1 Treatment 2 Treatment 3 Treatment 4 Treatment 5
years father school family united states
work née medical father states army

national mother college white served united
worked business public schools parents war
board irish attended year war ii
young son county mother army
local long city brother served

director family born married service
social ancestry schools years lieutenant

community descent studied played military
Treatment 6 Treatment 7 Treatment 8 Treatment 9 Treatment 10

fraternity elected company board law school
member republican officer graduated law
student served united staets college school law

phi army mexico bachelor arts attorney
delta democratic air force harvard juris doctor
kappa member years state university bar
hall house representatives military professor law firm
chi state national guard masters degree court

graduated senate insurance high school law degree
son election business economics judge

Table 3: Top Words for 10 Treatments sLDA Discovered

Paul Holland. 1986. Statistics and causal infer-
ence. Journal of the American Statistical Associa-
tion, 81(396):945–960.

Macartan Humphreys, Raul Sanchez de la Sierra, and
Peter van der Windt. 2013. Fishing, commitment,
and communication: A proposal for comprehensive
nonbinding research registration. Political Analysis,
21(1):1–20.

Kosuke Imai and Marc Ratkovic. 2013. Estimating
treatment effect heterogeneity in randomized pro-
gram evaluation. The Annals of Applied Statistics,
7(1):443–470.

John P. A. Ioannidis. 2005. Why most published re-
search findings are false. PLoS Medicine, 2(8):696–
701.

Jonathan Katz and Gary King. 1999. A statistical
model for multiparty electoral data. The American
Political Science Review, 93(1):15–32.

Jon D. Mcauliffe and David M. Blei. 2007. Supervised
topic models. In Advances in Neural Information
Processing Systems 20 (NIPS 2007).

David Mimno, Hanna M Wallach, Edmund Talley,
Miriam Leenders, and Andrew McCallum. 2011.
Optimizing semantic coherence in topic models. In
Proceedings of the Conference on Empirical Meth-
ods in Natural Language Processing, pages 262–
272.

Diana C Mutz. 2011. Population-Based Survey Exper-
iments. Princeton University Press.

Samuel L Popkin. 1994. The Reasoning Voter: Com-
munication and Persuasion in Presidential Cam-
paigns. University of Chicago Press.

Novi Quadrianto, Viktoriia Sharmanska, David A.
Knowles, and Zoubin Ghahramani. 2013. The su-
pervised ibp: Neighbourhood preserving infinite la-
tent feature models. page 101.

Margaret E Roberts, Brandon M Stewart, Dustin Tin-
gley, Chris Lucas, Jetson Leder-Luis, Bethany Al-
bertson, Shana Gadarian, and David Rand. 2014.
Topic models for open ended survey responses with
applications to experiments. American Journal of
Political Science.

Margaret E. Roberts, Brandon M. Stewart, and Edo M.
Airoldi. 2016. A model of text for experimentation
in the social sciences. Journal of the American Sta-
tistical Association. Forthcoming.

Don Rubin. 1974. Estimating causal effects of treat-
ments in randomized and nonrandomized studies.
Journal of Educational Psychology, 66:688–701.

Donald B. Rubin. 1986. Statistics and causal in-
ference: Comment: Which ifs have causal an-
swers. Journal of the American Statistical Associ-
ation, 81(396):961–962.

Michael Tomz and Jessica Weeks. 2013. Public opin-
ion and the democratic peace. American Political
Science Review, 107(4):849–865.

1609


