



















































Leveraging Diverse Lexical Chains to Construct Essays for Chinese College Entrance Examination


Proceedings of the The 8th International Joint Conference on Natural Language Processing, pages 355–360,
Taipei, Taiwan, November 27 – December 1, 2017 c©2017 AFNLP

Leveraging Diverse Lexical Chains to Construct Essays
for Chinese College Entrance Examination

Liunian Li Xiaojun Wan Jin-ge Yao Siming Yan
Institute of Computer Science and Technology, Peking University, Beijing 100871, China

The MOE Key Laboratory of Computational Linguistics, Peking University
{liliunian, wanxiaojun, yaojinge, dantes}@pku.edu.cn

Abstract

In this work we study the challenging
task of automatically constructing essays
for Chinese college entrance examination
where the topic is specified in advance.
We explore a sentence extraction frame-
work based on diversified lexical chains to
capture coherence and richness. Experi-
mental analysis shows the effectiveness of
our approach and reveals the importance
of information richness in essay writing.

1 Introduction

Chinese National Higher Education Entrance Ex-
amination, a.k.a. Gaokao (“高考”) in Chinese,
has a similar format to the American SAT, except
that it lasts more than twice as long. The nine-
hour test is offered just once a year and is the
sole determinant for admission to virtually all Chi-
nese colleges and universities. It emphasizes sev-
eral subjects including math and science, but also
measures knowledge of written Chinese and En-
glish. It includes various types of questions such
as multiple-choice questions, short-answer ques-
tions and essays. In this work, we focus on the
Chinese essay writing questions, typically in the
format of writing a topically rich but coherent es-
say when specified a topical word or title. De-
veloping a system that can construct essays in the
context of exams is not for mimicking or surpass-
ing human writing, but to provide analytical as-
sistance for students and high school teachers to
improve essay writing. The task is challenging
as it requires analyzing of the given topic and the
ability to organize coherent descriptions in sen-
tences and paragraphs, while the content should
cover rich aspects and discussions but still con-
forms to the given topic. As a preliminary study
we only explore sentence extraction to get a sense

of how well automatic approaches could achieve
when evaluated by professional evaluators.

We explore an approach based on lexical chains,
i.e., sequences of words containing a series of con-
ceptually related words in a discourse. Lexical
chains could be useful to assist analyzing topical
coherence and we will show their effectiveness in
essay construction. For a given topic, we first re-
trieve a few topically relevant documents, from
which the lexical chains will be built. Each lexical
chain corresponds to a subtopic. We would like to
have each subtopic representative, while the over-
all subtopics are diverse enough to cover as many
topically related aspects as possible. We leverage
a diversified ranking algorithm to calculate the im-
portance weights for different lexical chains, then
form the essay by selecting and organizing sen-
tences to cover the important chains.

In this paper we provide a focused study on a
specific topic. Our contributions can be summa-
rized as follows:

• To the best of our knowledge, we are the first
study Chinese essay generation for Chinese
college entrance exams. We utilize sentence
extraction as a viable step towards essay gen-
eration for exams.

• Considering the nature of the problem, we
propose a framework based on diversified se-
lection of lexical chains, to cover rich and di-
verse aspects of the given topical word.

• Manual evaluation from experienced high
school teachers shows the feasibility for au-
tomatic essay generation, as well as the ef-
fectiveness and the potential of our approach,
revealing the importance of information rich-
ness and diversity for essay writing in the
context of Chinese college entrance exams.

355



2 Approach

In this preliminary study, we directly use sen-
tences from a large source corpus to construct the
final essay. This can be treated as a viable inter-
mediate step towards full generation. We design a
framework that consists of article retrieval, lexical
chain construction, sentence extraction and infor-
mation ordering. Note that in this study the spec-
ified topic for essay writing is in the form of one
single topical word while our approach relies on
vectorial representations. Nevertheless, our solu-
tion can naturally generalize to sentential inputs,
since we could use sentence embedding models
(e.g. RNNs, skip-thought vectors) to get a vec-
torized representaion.

2.1 Article Retrieval

In this study we find that a simple retrieval model
based on latent semantic analysis (Deerwester
et al., 1990, LSA) works relatively well. Specif-
ically, we get semantic vector representations for
each document and the given topical word by
performing singular value decomposition on the
term-document matrix, where each element cor-
responds to the tf-idf value for a particular word
in the document. Articles with the highest simi-
larity scores with the given topic word will be re-
trieved for the next steps. We also tried an even
simpler approach to directly use averaged word
vectors to represent a document and search for the
documents that lead to the largest cosine similarity
values with the given topical word. This approach
turns out to prefer shorter articles and yields less
accurate performance compared with LSA.

2.2 Building Lexical Chains

The main task studied in this paper is to construct
an essay for a specified topic word. A lexical
chain (Morris and Hirst, 1991) is a sequence of
words that consists of a series of conceptually re-
lated words in a discourse. A lexical chain can
be used to model topical contexts as well as text
coherence. In our study, we adapt the calculation
method used by Barzilay and Elhadad (1999) for
our purpose. Due to the shortage of high-coverage
thesaurus, we utilize vector semantics to capture
lexical relationships, and find that pairwise sim-
ilarities defined by word vectors can be used to
build reliable lexical chains while being more flex-
ible.

We treat each retrieved article as a list of words.

For the purpose of this study, we only consider ad-
jectives and nouns when constructing the chains.
Since in Chinese articles, most frequently used ad-
verbs (such as “那么”-so) and verbs (such as “成
为”-becomes, “有”-exist) are used for syntactic in-
tegrity and do not contain topically related infor-
mation.

Given an article, we take out each word one
by one and check whether and where it should be
placed. If this word has not been included in any
chain, we treat it as a candidate and traverse all
current chains to calculate the similarity between
the candidate and the chain. The candidate word
will be attached to the chain with the highest sim-
ilarity value if it surpasses a threshold. Here the
similarity between a candidate word w and a chain
C is defined as the similarity between w and the
last word in C. 1

2.3 Importance Estimation for Chains

Not all of the constructed lexical chains should be
used for further processing, as some may not cover
important aspects of the given topic. There could
be many ways for estimating the importance of
different lexical chains. In this work we model
the problem using graph-based ranking, treating
each candidate chain as a node in a graph. The
edge weight is assigned to be pairwise similar-
ity between two chains C1 and C2, defined as
cos( 1#C1

∑
w∈C1 vec(w), ), i.e. the similarity be-

tween their average word vectors.
In order to cover more aspects about the topic

and to avoid redundancy, we would like to as-
sign high important scores on more diverse chains.
Therefore we utilize DivRank (Mei et al., 2010), a
well-known diversified ranking algorithm, for cal-
culating the ranking scores for different chains.
Specifically, we utilize the pointwise variant of Di-
vRank. At time T , the transition probability from
node u to node v is defined as follows:

pT (u, v) = (1− λ)p∗(v) + λp0(u, v)pT (v)
DT (u)

(1)

where p∗(v) is a distribution which represents
the prior preference of visiting vertex v, p0(u, v)
is the initialized transition matrix estimated from
a regular time-homogenous random walk, and

1We find a more straightforward definition
maxt∈C sim(w, t) less effective as it encourages a rich-
gets-richer effect which leads to long chains but incoherent
thematic meanings.

356



DT (u) =
∑

v∈V p0(u, v)pT (v) is a normaliz-
ing factor for the second term. The weights for
each node are initialized to be the cosine sim-
ilarity between the corresponding chain and the
topic, which can in some sense reflect the prior
relevance. After the algorithm converges, i.e.
pT (v) ≈ pT−1(u)pT−1(u, v), we can select the
top-ranking chains as subtopics for essay con-
struction according to the values of pT (v).

2.4 Sentence Selection
We now have our subtopics, i.e. important lexical
chains, prepared. The next step is to select sen-
tences to cover those subtopics. For each candi-
date sentence, we use the average vector of nouns
and adjectives as its vectorial representation, de-
noted as s. Let c and t be the average word vector
of the current lexical chain and the vector of the
topical word, respectively. We define the weight
for sentence s to be a linear combination of chain
similarity and topical word similarity:

weight(s) =
cos(s, t) + ρ · cos(s, c)

1 + ρ
(2)

We empirically set the ratio parameter ρ to be 0.8
and observe that the selected sentences can cover
the subtopic well while being coherent with the
topical word.

An essay in Chinese college entrance exams
normally has a length between 800 and 1,200 Chi-
nese characters. The most typical essays contain
around 1,000 characters. We find that taking seven
top-ranking subtopics (chains) can lead to a good
coverage of the given topic, and selecting four sen-
tences for each subtopic to form a paragraph will
make the overall length just around 1,000. There-
fore we limit the numbers for subtopic and sen-
tence selection as such.

2.5 Sentence Ordering
After selecting sentences for each subtopic, we
need to order them to form paragraphs and or-
der the paragraphs to construct the full essay.
A straightforward way is to greedily select ele-
ments based on similarity between each candi-
date and the previously selected sentence or para-
graph. Preliminary experiments suggest that most
elements share high similarity values due to our
selection criteria in previous steps, causing sim-
ple greedy selection strategy to fail. Therefore, we
consider a different method: ordering sentences
and paragraphs according to its position in the

original article. The position of each candidate
can be represented as a rational number, dividing
the current position number by the total number
of sentences in that paragraph. We find readabil-
ity within a paragraph largely increased after such
strategy for sentence ordering. The intuition is that
sentences in the front or at the end typically con-
tain more general discussion while sentences in
the middle tend to describe specific details or ex-
pansive contents. For paragraph ordering, we take
a similar approach by calculating relative positions
in the original document.

3 Experimental Study

3.1 Settings

3.1.1 Data
Since we study approaches based on sentence ex-
traction in this preliminary work, We collected a
source corpus that contains 800 articles in Chi-
nese, with the overall size around 800,000 Chinese
characters. The source corpus consists of essays
written by various authors, discussing relatively
diverse topics. 2

Since the similarity calculations in our frame-
work involves vectorial representations for each
word, we trained 300 dimensional GloVe vectors
(Pennington et al., 2014) on the Chinese Giga-
word corpus (Graff and Chen, 2005). We used the
Stanford Chinese Segmenter for word segmenta-
tion (Tseng et al., 2005).

For evaluation, 10 topics which have once ap-
peared in previous Chinese college entrance ex-
ams will be provided for all the experimented
essay construction systems. We have manually
checked that there indeed exists several sentences
in the source corpus that are relevant to the given
topic. A good system should detect such sentences
and use them to generate the essay that well re-
sponds to the specified topic.

3.1.2 Evaluation
Given a topical word, every student will write a
completely different essay. The diversity of possi-
ble essays makes automatic evaluation metrics that
count on content overlaps impossible since sys-
tem outputs can then only be compared with rather
limited references. Therefore we leave proper de-
sign of automatic metrics as future work and only

2To promote related experimental and educational studies,
we have attached the corpus in the supplementary materials
which could be found in the ACL Anthology.

357



perform manual evaluation in this study.
We conduct manual ratings on a few important

metrics (in a 1-10 rating system, the higher the bet-
ter) in generic generation systems that also should
be emphasized in this study, including

• Topical consistency (const.): on how much
is the output consistent with the given topic.

• Overall readability (read.): overall read-
ability of the essay in terms of text coherence.

• Content diversity (div.): whether the essay
covering multiple aspects of the topic or just
repeating the same argument.

For the purpose of this study, we also evalu-
ate the output essays using the evaluation criteria
for the Chinese college entrance exams. The to-
tal rating score is 60 points, assigned for the basic
level and the advanced level respectively. The ba-
sic level (40 points in total) considers whether the
most basic requirements have been fulfilled, such
as whether the essay conforms to the given topic,
describing with structural integrity and using cor-
rect punctuation. The advanced level (20 points in
total) measures how much depth, richness, literary
grace and novelty there exist in the content of the
essay. We ask 10 high school teachers who are ex-
perienced in such essay evaluation settings to con-
duct manual scoring. Note that in evaluations of
the exams, the above points will not be strictly as-
signed one by one, only an overall score will be
seen. We conform with this scoring approach in
this study.

3.1.3 Baselines

To verify the effectiveness of our proposed ap-
proach, we compare with two baseline systems:
The system (Baseline1) that utilizes topically re-
lated words and clusters rather than our pro-
posed diversified lexical chains for subtopic rep-
resentation, and a more straightforward baseline
(Baseline2) that select sentences which have the
most similar vectorial representations with the
given topical word. The former baseline can be
treated as a reimplementation of the very recent
Chinese essay generating system proposed by Qin
et al. (2015). All systems are evaluated on the
given 10 topics, producing 30 essays in total.

3.2 Results
Table 1 lists the manual rating scores (average and
standard deviation 3 of the scores from the 10 high
school teachers) for the outputs from different sys-
tems. The differences between systems are statis-
tically significant in Bonferroni adjusted pairwise-
t testing with p < 0.01. We can see that our
proposed framework outperforms the baseline sys-
tems in all evaluation criteria. We also provide the
example outputs in the appendix.

Baseline1 Baseline2 Proposed
Basic (40) 29.42±4.43 32.75±1.84 34.82±1.50
Adv (20) 11.65±2.53 13.1±1.79 14.97±1.23
Score (60) 41.07±6.22 45.85±3.51 49.78±2.65
const. (10) 5.38±0.98 6.47±0.84 6.90±0.76
read. (10) 5.23±0.81 6.27±0.75 6.78±0.68
div. (10) 5.15±0.74 5.8±0.87 6.92±0.75

Table 1: Evaluation results for different systems.
Each cell contains the average and standard devia-
tion of the ten scores assigned to an output.

3.3 Discussion
Here we provide some qualitative analysis for our
use of lexical chains and diversified importance
ranking. Given the topic word “挫折”(setback),
we can find 40 chains in total. the top-ranking
chains from diversified ranking are displayed in
Figure 1a, along with the top-ranking subchains
produced by the PageRank algorithm (Page et al.,
1999) in Figure 1b. We can observe that chains in
(1a) contain direct explanations as well as conse-
quential attitudes and related association of words,
while most chains in (1b) only cover the literal
meaning of the word “挫折”(setback), without ex-
tensions in depth.

We also made some statistics to make sure
that all systems are not directly using the original
source documents. All systems produced essays
using sentences from multiple articles between 9
and 21, with the overlapping proportion for single
source document no more than 15%. This is in-
tuitively a side evidence on that if a student wants
to write a good essay, he or she may have to read
a lot of good materials for preparation of expres-
sions, wording choices as well as ideas.

4 Related Work

To the best of our knowledge, there exist few
studies on automatically challenging the Chinese

3The standard deviation reflects the variance of different
evaluators on each output, therefore also reflects agreements.

358



曲折(intricate) - 坎坷(bumpy) - 漫长(long-term) - 艰辛(hardship) - 历程(progress)   
勇敢(brave) - 毅力(determination) - 勇气(courage) - 坚强(adamancy) - 坚韧(tenacity) 
     - 自信(confident) - 信心(confidence)  
意志(willpower) - 斗志(fighting will) - 顽强(indomitable) - 艰难(tough) - 困苦(tribulation)   
可怕(dreadful) - 悲剧(tragedy) - 灾难(disaster) - 灾害(calamity) - 严峻(rigorous) 
     - 因素(factors)   
特殊(special) - 困难(difficulty) - 困境(straits) - 低谷(trough)   
人生(life) - 理想(cause) - 精神(spirit) - 理念(ethic) - 思维(thinking) -观念(concept)   
态度(attitude) - 理性(rational) - 冷静(calm) - 理智(wise)  

(a)

曲折(intricate) - 坎坷(bumpy) - 漫长(long-term) - 艰辛(hardship) - 历程(progress)   
可怕(dreadful) - 悲剧(tragedy) - 灾难(disaster) - 灾害(calamity) - 严峻(rigorous) 
     - 因素(factors)   
特殊(extraordinary) - 困难(difficulty) - 困境(straits) - 低谷(trough)   
意志(willpower) - 斗志(fighting will) - 顽强(indomitable) - 艰难(tough) - 困苦(tribulation)   
痛苦(suffering) - 悲伤(sorrow) - 伤痛(pain) - 痛楚(agony)    
悲哀(grieve) - 难过(sad) - 失望(disappointed) - 绝望(despair) - 无助(helpless) 
     - 孤独(loneliness)   
茫然(at a loss) - 迷茫(confused) - 困惑(puzzled) - 尴尬(embarrassed) 

(b)

Figure 1: Lexical chains formulated by (a) DivRank and (b) PageRank

college entrance tests. One recent work focuses
on multiple choice questions in that context (Guo
et al., 2017).

The approach of selecting sentence for con-
structing essays share similar methodological na-
ture with extractive summarization, where clas-
sic graph-based ranking has been shown useful
(Erkan and Radev, 2004). Diversified selection
could further improve information coverage (Mei
et al., 2010; Lin and Bilmes, 2011; Hong et al.,
2014). Note that the goal of essay writing is dif-
ferent with summarization. The task in this study
is to generate a rich but coherent article, and ev-
ery student or system could write a very different
essay, while the goal of summarization is to con-
dense documents, in which case the output results
should be similar in content, covering the same
important facts.

The closest study with the main theme of this
paper is perhaps the recent work by Qin et al.
(2015) on essay generation, which directly utilizes
words as subtopic representations rather than our
proposed usage of diversified lexical chains.

5 Conclusion and Future Work

In this paper, we study the challenging task of es-
say construction for Chinese college entrance ex-
ams, propose a framework based on diversified
lexical chains and show its effectiveness.

Our framework is simple in nature and is by
no means perfect. For example, structural co-
herence is not explicitly modeled in our method
since lexical chains could only capture topical co-
herence. We leave more elaborated strategies for
content planning as future work. Also, we would
like to extend the framework for more difficult ti-
tles or topics by exploring proper vectorial repre-
sentations, and to collect manual data for super-
vised learning. Methods beyond sentence extrac-
tion should also be explored to utilize more elabo-
rative syntactic and discursive structures.

Acknowledgments

This work was supported by 863 Program of China
(2015AA015403), NSFC (61331011), and Key
Laboratory of Science, Technology and Standard
in Press Industry (Key Laboratory of Intelligent
Press Media Technology). We thank the anony-
mous reviewers for helpful comments. Xiaojun
Wan is the corresponding author.

359



References
Regina Barzilay and Michael Elhadad. 1999. Using

lexical chains for text summarization. Advances in
automatic text summarization, pages 111–121.

Scott Deerwester, Susan T Dumais, George W Fur-
nas, Thomas K Landauer, and Richard Harshman.
1990. Indexing by latent semantic analysis. Jour-
nal of the American society for information science,
41(6):391.

Günes Erkan and Dragomir R. Radev. 2004. Lexrank:
Graph-based lexical centrality as salience in text
summarization. Journal of Artifitial Intelligence Re-
search (JAIR), 22:457–479.

David Graff and Ke Chen. 2005. Chinese gigaword.
LDC Catalog No.: LDC2003T09, ISBN, 1:58563–
58230.

Shangmin Guo, Xiangrong Zeng, Shizhu He, Kang
Liu, and Jun Zhao. 2017. Which is the effective
way for gaokao: Information retrieval or neural net-
works? In Proceedings of the 15th Conference of
the European Chapter of the Association for Compu-
tational Linguistics: Volume 1, Long Papers, pages
111–120, Valencia, Spain. Association for Compu-
tational Linguistics.

Kai Hong, John Conroy, Benoit Favre, Alex Kulesza,
Hui Lin, and Ani Nenkova. 2014. A repository of
state of the art and competitive baseline summaries
for generic news summarization. In Proceedings
of the Ninth International Conference on Language
Resources and Evaluation (LREC’14), pages 1608–
1616, Reykjavik, Iceland.

Hui Lin and Jeff Bilmes. 2011. A class of submodu-
lar functions for document summarization. In Pro-
ceedings of the 49th Annual Meeting of the Associ-
ation for Computational Linguistics: Human Lan-
guage Technologies, pages 510–520, Portland, Ore-
gon, USA. Association for Computational Linguis-
tics.

Qiaozhu Mei, Jian Guo, and Dragomir Radev. 2010.
Divrank: the interplay of prestige and diversity in
information networks. In Proceedings of the 16th
ACM SIGKDD international conference on Knowl-
edge discovery and data mining, pages 1009–1018.
Acm.

Jane Morris and Graeme Hirst. 1991. Lexical cohe-
sion computed by thesaural relations as an indicator
of the structure of text. Computational linguistics,
17(1):21–48.

Lawrence Page, Sergey Brin, Rajeev Motwani, and
Terry Winograd. 1999. The pagerank citation rank-
ing: Bringing order to the web. Technical report,
Stanford InfoLab.

Jeffrey Pennington, Richard Socher, and Christopher
Manning. 2014. Glove: Global vectors for word

representation. In Proceedings of the 2014 Con-
ference on Empirical Methods in Natural Language
Processing (EMNLP), pages 1532–1543, Doha,
Qatar. Association for Computational Linguistics.

Bing Qin, Duyu Tang, Xinwei Geng, Dandan Ning, Ji-
ahao Liu, and Ting Liu. 2015. A planning based
framework for essay generation. arXiv preprint
arXiv:1512.05919.

Huihsin Tseng, Pichuan Chang, Galen Andrew, Daniel
Jurafsky, and Christopher Manning. 2005. A condi-
tional random field word segmenter for sighan bake-
off 2005. In Proceedings of the fourth SIGHAN
workshop on Chinese language Processing, volume
171. Citeseer.

360


