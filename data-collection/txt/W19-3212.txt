



















































Transfer Learning for Health-related Twitter Data


Proceedings of the 4th Social Media Mining for Health Applications (#SMM4H) Workshop & Shared Task, pages 89–92
Florence, Italy, August 2, 2019. c©2019 Association for Computational Linguistics

89

Transfer learning for health-related Twitter data

Anne Dirkson & Suzan Verberne
LIACS, Leiden University

Niels Bohrweg 1, Leiden, the Netherlands
{a.r.dirkson, s.verberne}@liacs.leidenuniv.nl

Abstract
Transfer learning is promising for many NLP
applications, especially in tasks with limited
labeled data. This paper describes the meth-
ods developed by team TMRLeiden for the
2019 Social Media Mining for Health Appli-
cations (SMM4H) Shared Task. Our methods
use state-of-the-art transfer learning methods
to classify, extract and normalise adverse drug
effects (ADRs) and to classify personal health
mentions from health-related tweets. The code
and fine-tuned models are publicly available.1

1 Introduction

Transfer learning is promising for NLP applica-
tions, as it enables the use of universal pre-trained
language models (LMs) for domains that suffer
from a shortage of annotated data or resources,
such as health-related social media. Universal
LMs have recently achieved state-of-the-art re-
sults on a range of NLP tasks, such as classifi-
cation (Howard and Ruder, 2018) and named en-
tity recognition (NER) (Akbik et al., 2018). For
the Shared Task of the 2019 Social Media Min-
ing for Health Applications (SMM4H) workshop
team TMRLeiden focused on employing state-of-
the-art transfer learning from universal LMs to in-
vestigate its potential in this domain.

2 Task descriptions

ADR extraction The purpose of Subtask 1 (S1)
is to classify tweets as containing an adverse drug
response (ADR) or not. Subsequently, these ADR
mentions are extracted in Subtask 2 (S2) and nor-
malized to MedDRA concept IDs in Subtask 3
(S3). MedDRA (Medical Dictionary for Regu-
latory Activities) is an international, standardized
medical terminology.2

1https://github.com/AnneDirkson/
SharedTaskSMM4H2019

2https://www.meddra.org/

Personal Health Mention Extraction The goal
of Subtask 4 (S4) is to identify tweets that are
personal health mentions, i.e. posts that mention
a person who is affected as well as their specific
condition (Karisani and Agichtein, 2018), as op-
posed to posts discussing health issues in general.
Generalisability to both future data and different
health domains is evaluated by including data from
the same domain collected years after the training
data, as well as data from entirely different disease
domain.

3 Our approach

3.1 Preprocessing

We preprocessed all Twitter data using the lexical
normalization pipeline by Sarker (2017). We also
employed an in-house spelling correction method
(Dirkson et al., 2019). Additionally, punctuation
and non-UTF-8 characters were removed using
regular expressions.

3.2 Additional Data

Personal Health Mentions For S4, the training
data consists of data from one disease domain,
namely influenza, in two contexts: having a flu in-
fection and getting a flu vaccination. To improve
generalisability, we supplemented this data with
six labelled data sets from different disease do-
mains (Karisani and Agichtein, 2018). We refer
to this combined data set as S4+. For each subset,
10% was used for a combined validation set. For
fine-tuning the ULMfit universal language model
based on 28,595 Wikipedia articles (Wikitext-103)
(Merity et al., 2017b), the DIEGO Drug Chat-
ter corpus (Sarker and Gonzalez, 2017) was com-
bined with the data from S1 and S4+ to form a
larger unsupervised corpus of health-related Twit-
ter data (‘TwitterHealth’). For S4, fine-tuning was
also attempted with only the S4+ data.

https://github.com/AnneDirkson/SharedTaskSMM4H2019
https://github.com/AnneDirkson/SharedTaskSMM4H2019
https://www.meddra.org/


90

S1 S2* S3 S4 S4+
Dev - 130 76 - -
Train 14,634 910 1,756 6,996 11,832
Validation 1,626 130 76 777 1,314
Test 5000 1000 1000 TBA TBA

Table 1: Data sets. *Only tweets containing ADRs
were used for developing the system. TBA: To be an-
nounced

Concept Normalization The MedDRA concept
names and their aliases in both MedDRA and the
Consumer Health Vocabulary3 were used to sup-
plement the data from S3. This data set is hereafter
called S3+.

3.3 Text Classification

Text classification was performed with fast.ai
ULMfit (Howard and Ruder, 2018). As recom-
mended, the initial learning rate (LR) of 0.01 was
determined manually by inspecting the log LR
compared to the loss. Default language models
were fine-tuned using AWD LSTM (Merity et al.,
2017a) with (1) 1 cycle (LR = 0.01) for the last
layer and then (2) 10 cycles (LR = 0.001) for all
layers.

Subsequently, this model is used to train a clas-
sifier with F1 as the metric, a dropout of 0.5 and
a momentum of (0.8,0.7), in line with the recom-
mendations. Training is done with (1) 1 cycle
(LR = 0.02) on the last layer; (2) unfreezing of
the second-to-last layer; (3) another cycle running
from a 10-fold decrease of the previous LR to this
LR divided by 2.64 (as recommended in the fast.ai
MOOC).4 This is repeated for the next layer and
then for all layers. The last step consists of multi-
ple cycles until F1 starts to drop.

As an alternative classifier for S1, we used the
absence of ADRs (noADE) according to the Bert
embeddings NER method (see below) which was
developed for the subsequent sub-task (S2) and
aims to extract these ADR mentions. As a base-
line for text classification, we used a Linear SVC
with unigrams as features. The C parameter was
tuned with a grid of 0.0001 to 1000 (steps of x10).

3.4 Named Entity Recognition

For S2, we experimented with different combi-
nations of state-of-the-art Flair embeddings (Ak-
bik et al., 2018), classical Glove embeddings and

3https://www.nlm.nih.gov/research/
umls/sourcereleasedocs/current/CHV/

4 https://course.fast.ai/

Bert embeddings (Devlin et al., 2018) using the
Flair package. We used pre-trained Flair embed-
dings based on a mix of Web data, Wikipedia
and subtitles; and the ‘bert-base-uncased’ variant
of Bert embeddings. We also experimented with
Flair embeddings combined with Glove embed-
dings (dimensionality of 100) based on FastText
embeddings trained on Wikipedia (GloveWiki) or
on Twitter data (GloveTwitter). Training for all
embeddings was done with initial LR of 0.1, batch
size of 32 and max epochs set to 150.

As a baseline for NER, we used a CRF with
the default L-BFGS training algorithm with Elas-
tic Net regularization. As features for the CRF,
we used the lowercased word, its suffix, the word
shape and its POS tag.5

3.5 Concept normalization

For S3, pre-trained Glove embeddings were used
to train document embeddings on the extracted
ADR entities in the S3 data including or exclud-
ing the aliases from CHV (S3+) with concept IDs
as labels. We used the default RNN in Flair
with a hidden size of 512. Glove embeddings
(dim = 100) were based on FastText embeddings
trained on Wikipedia. Token embeddings were
re-projected (dim = 256) before inputting to the
RNN.

4 Results

Method F1 (range) P R
Average* 0.502

(0.331)
0.535 0.505

Run1 ULMfit1 0.533 0.642 0.455
Run2 noADE 0.418 0.284 0.792

Table 2: Results for ADR Classification (S1). *over all runs
submitted 1TwitterHealth data

For all four subtasks, our best transfer learning
system consistently performs better than the aver-
age over all runs submitted to SMM4H. For classi-
fying ADR mentions, our overall best performing
system is a ULMfit model trained on the Twitter-
Health corpus (see Table 2). Yet, the highest re-
call is attained by using the absence of named en-
tities (noADE) as a classifier. This is in line with
our validation results (see Table 6). For extracting
ADRs, our best system is a combination of Bert
with Flair embeddings without a separate classifier

5https://sklearn-crfsuite.readthedocs.
io/en/latest/tutorial.html

https://www.nlm.nih.gov/research/umls/sourcereleasedocs/current/CHV/
https://www.nlm.nih.gov/research/umls/sourcereleasedocs/current/CHV/
https://course.fast.ai/
https://sklearn-crfsuite.readthedocs.io/en/latest/tutorial.html
https://sklearn-crfsuite.readthedocs.io/en/latest/tutorial.html


91

Method relaxed F1 (range) relaxed P relaxed R strict F1 (range) strict P strict R
Average* 0.538 (0.486) 0.513 0.615 0.317 (0.422) 0.303 0.358
Run1 Bert+Flair+ 0.625 0.555 0.715 0.431 0.381 0.495
Run2 Bert+ 0.622 0.560 0.701 0.427 0.382 0.484
Run3 Bert+ADRClassifier 0.604 0.718 0.521 0.417 0.494 0.360

Table 3: Results for ADR Extraction(S2). *over all runs submitted +No separate classifier for sentences containing ADRs

Method relaxed F1 (range) relaxed P relaxed R strict F1 (range) strict P strict R
Average* 0.297 (0.242) 0.291 0.312 0.212 (0.247) 0.205 0.224
Run1+ RNN Docembeddings 0.312 0.370 0.270 0.250 0.296 0.216
Run2+ RNN Docembeddings 0.303 0.272 0.343 0.244 0.218 0.277
Run3+ RNN Docembeddings 0.302 0.267 0.347 0.246 0.218 0.283

Table 4: Results for concept normalization (S3). *over all runs submitted +Runs same as S2 prior to concept normalization

Method Acc. (range) F1 (range) P R
Average* 0.781 (0.263) 0.701 (0.464) 0.902 0.585

Run1 ULMfit with S4+ data

Domain1 0.869 0.859 0.952 0.781
Domain2 0.638 0.419 0.750 0.290
Domain3 0.786 0.539 1.000 0.368
Mean 0.793 0.726 0.940 0.591

Run2 ULMfit with TwitterHealth data

Domain1 0.863 0.849 0.969 0.756
Domain2 0.609 0.342 0.700 0.226
Domain3 0.768 0.480 1.000 0.316
Mean 0.786 0.716 0.928 0.583

Table 5: Results for personal health mention classification (S4). *over all runs submitted

for sentences containing ADR mentions (see Table
3). However, using Bert embeddings alone with
the ULMfit classifier from S1 appears to be more
precise. During validation, we found that com-
binations of Glove embeddings (based on Twit-
ter or Wikipedia) and Flair embeddings performed
poorly compared to the submitted systems (see Ta-
ble 7). For mapping the ADRs to MedDRA con-
cepts, we only submitted one system with different
preceding NER models (see Table 4), since adding
the alias information (S3+) decreased both preci-
sion and recall (see Table 8). Our RNN document
embeddings with only the S3 data, however, per-
formed better than average. Lastly, for the classi-
fication of personal health mentions, our best clas-
sifier was a ULMfit model fine-tuned on the S4+
data (see Table 5), which outperformed the aver-
age result and the ULMfit model trained on the
larger TwitterHealth corpus on all metrics. This
system similarly outperformed the other ULMfit
model on the validation data (see Table 9).

Method F1 P R
Baseline: Linear SVC (C=1.0) 0.475 0.526 0.433
ULMfit1 0.574 0.574 0.574
noADE 0.330 0.207 0.823

Table 6: Validation results for ADR classification (S1)
1TwitterHealth data

Method Micro-F1 P R
Baseline: CRF 0.235 0.560 0.149
Flair+ GloveWiki 0.596 0.666 0.540
Flair+ GloveTwitter 0.577 0.655 0.515
Bert 0.640 0.699 0.590
Bert+Flair 0.649 0.699 0.606

Table 7: Validation results for ADR extraction (S2)

Method F1 P R
RNNDocembeddings with S3 0.623 0.566 0.694
RNNDocembeddings with S3+ 0.253 0.171 0.482

Table 8: Validation results for concept normalization (S3)

Method F1 P R
Baseline: Linear SVC (C=0.1) 0.615 0.678 0.572
ULMfit with S4+ data 0.712 0.743 0.701
ULMfit with TwitterHealth data 0.692 0.738 0.676

Table 9: Mean validation results for personal health mention
classification (S4) averaged over eight data sets of S4+

5 Conclusions

Transfer learning using default and recommended
settings offers above average results for various
NLP tasks using health-related Twitter data. More
research is necessary to investigate whether state-
of-the-art performance may be possible with fur-
ther domain-specific adaptation, for instance by
tuning hyper-parameters, training embeddings on
medical data or by dealing with domain-specific
vocabulary absent in the language model.



92

References
Alan Akbik, Duncan Blythe, and Roland Vollgraf.

2018. Contextual string embeddings for sequence
labeling. In Proceedings of the 27th International
Conference on Computational Linguistics, pages
1638–1649, Santa Fe, New Mexico, USA. Associ-
ation for Computational Linguistics.

Jacob Devlin, Ming-Wei Chang, Kenton Lee, and
Kristina Toutanova. 2018. BERT: Pre-training of
Deep Bidirectional Transformers for Language Un-
derstanding. ArXiv.

Anne Dirkson, Suzan Verberne, Gerard van Oort-
merssen, Hans van Gelderblom, and Wessel Kraaij.
2019. Lexical Normalization of User-Generated
Medical Forum Data. In Proceedings of 2019 ACL
workshop Social Media Mining 4 Health (SMM4H).

Jeremy Howard and Sebastian Ruder. 2018. Universal
language model fine-tuning for text classification.
In Proceedings of the 56th Annual Meeting of the
Association for Computational Linguistics (Volume
1: Long Papers), pages 328–339, Melbourne, Aus-
tralia. Association for Computational Linguistics.

Payam Karisani and Eugene Agichtein. 2018. Did you
really just have a heart attack?: Towards robust de-
tection of personal health mentions in social media.
In Proceedings of the 2018 World Wide Web Con-
ference, WWW ’18, pages 137–146, Republic and
Canton of Geneva, Switzerland. International World
Wide Web Conferences Steering Committee.

Stephen Merity, Nitish Shirish Keskar, and Richard
Socher. 2017a. Regularizing and Optimizing LSTM
Language Models. ArXiv.

Stephen Merity, Caiming Xiong, James Bradbury, and
Richard Socher. 2017b. Pointer Sentinel Mixture
Models. In ICLR 2017.

Abeed Sarker. 2017. A customizable pipeline for social
media text normalization. Social Network Analysis
and Mining, 7(1):45.

Abeed Sarker and Graciela Gonzalez. 2017. A cor-
pus for mining drug-related knowledge from Twitter
chatter: Language models and their utilities. Data
in Brief, 10:122–131.

https://www.aclweb.org/anthology/C18-1139
https://www.aclweb.org/anthology/C18-1139
http://arxiv.org/abs/1810.04805
http://arxiv.org/abs/1810.04805
http://arxiv.org/abs/1810.04805
https://www.aclweb.org/anthology/P18-1031
https://www.aclweb.org/anthology/P18-1031
https://doi.org/10.1145/3178876.3186055
https://doi.org/10.1145/3178876.3186055
https://doi.org/10.1145/3178876.3186055
http://arxiv.org/abs/1708.02182
http://arxiv.org/abs/1708.02182
https://arxiv.org/abs/1609.07843
https://arxiv.org/abs/1609.07843
https://doi.org/10.1007/s13278-017-0464-z
https://doi.org/10.1007/s13278-017-0464-z
https://doi.org/10.1016/J.DIB.2016.11.056
https://doi.org/10.1016/J.DIB.2016.11.056
https://doi.org/10.1016/J.DIB.2016.11.056

