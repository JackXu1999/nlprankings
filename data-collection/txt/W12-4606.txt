



















































Synchronous Tree Unification Grammar


Proceedings of the 11th International Workshop on Tree Adjoining Grammars and Related Formalisms (TAG+11), pages 46–54,
Paris, September 2012.

Synchronous Tree Unification Grammar

Timm Lichte
Collaborative Research Center 991

University of Düsseldorf
lichte@phil.hhu.de

Abstract

This paper presents a novel grammar
formalism, Synchronous Tree Unification
Grammar (STUG), that borrows ideas from
two rather distinct exemplars of tree-based
grammar formalisms, namely Synchronous
Tree Adjoining Grammar and Tree Unifi-
cation Grammar. At the same time STUG
differs considerably from those in that it al-
lows for a clean separation of syntax and
valency. Exploiting this potential in the
modelling of natural language grammar has
a number of interesting consequences that
we will sketch in the course of this paper.

1 Motivation

The underlying motivation for the development of
Synchronous Tree Unification Grammar (STUG)
is to model syntax and valency as separated, yet
linked dimensions of natural language signs. This
sharply contrasts with the lexical amalgamation of
syntax and valency found within the TAG frame-
work (and other main stream syntactic frame-
works). Very generally speaking, we take va-
lency to be a mapping from semantic roles to sets
of morpho-syntactic properties and some marker
for indicating necessity. Following common ter-
minology, realizations of valency roles are also
called arguments.

In TAG elementary trees, valency properties
of the lexical anchor are commonly mapped bi-
jectively onto non-terminal leaves due to well-
formedness conditions (Abeillé, 1988; Frank,
1992; Abeillé and Rambow, 2000; Frank, 2002),
while the realization of optional valency roles is
reflected across the set of elementary trees with
the same lexical anchor. However, the correspon-
dence between elementary tree and valency frame

is blurred by functional items such as comple-
mentizers, determiners and auxiliary verbs, which
commonly anchor an elementary tree of their
own.

This way of amalgamating elementary trees
and valency information can be held responsible
for a couple of difficulties that have shown up in
various aspects of the TAG framework – amongst
them the following:

Since elementary trees for predicative verbs en-
force the surface realization of the verbal head
and its obligatory arguments, TAG accounts have
to cope with elliptical structures (i.e. with gap-
ping) by means of more or less far reaching con-
cessions: either tangled trees are generated from
elementary trees using a non-trivial contraction
operation (Sarkar and Joshi, 1996; Sarkar and
Joshi, 1997), or one falls back on an infinitely
ambiguous lexicon (Seddah, 2008; Seddah et al.,
2010), or one includes empty words as a result of
a deletion-like operation (Lichte and Kallmeyer,
2010) or as a result of lexical insertion using ex-
tra elementary trees (Sarkar, 1997; Seddah and
Sagot, 2006).

Since alternative valency frames and alterna-
tive linearizations thereof multiply the set of (yet
unanchored) tree templates (Prolo, 2002), the
use of a metagrammar system in broad-coverage
grammars is practically inevitable (XTAG Re-
search Group, 2001; Duchier et al., 2004). Im-
portant syntactic generalisations are therefore not
expressed directly in a TAG, but emerge indirectly
across elementary trees. Furthermore the factor-
ization by means of metagrammars makes the in-
clusion of empty words attractive, since they in-
crease the reusability of tree fragments.

Finally, since elementary trees span over an ex-

46



tended domain of locality and may relate a lexical
anchor to more than one preceding constituent, it
is far from obvious how incremental parsing can
be performed. Proposals so far either add unlexi-
calized trees (“prediction trees”) and a verification
operation (Demberg and Keller, 2008; Demberg,
2010), or place the lexical anchor at the left edge
of an elementary tree, the head of which can be
left underspecified (Mazzei et al., 2007).

Each of these difficulties may seem resolvable
in one of the mentioned ways. But in my view
the sum of necessary adaptations and concessions
makes it worth thinking about accounts that relate
syntax and valency more indirectly. STUG repre-
sents the first result of this line of thought.

2 The STUG formalism

2.1 The elementary structures

STUG and Synchronous Tree-Adjoining Gram-
mar (STAG) (Shieber and Schabes, 1990; Shieber,
1994; Nesson and Shieber, 2008) share the idea,
that syntactic and semantic representations are
joined in the lexicon by making up a set of pairs
of trees or multi-component structures. Further-
more there is some way of directly linking nodes
of the syntactic domain and the semantic domain
within an elementary pair.

To provide an example, a STUG pair for the
verb laughs is shown in Figure 1. We call the
first element of the STUG pair the syntactic tree,
and the second element the valency tree. While
the syntactic tree corresponds to an elementary
and derived tree known from TAG, the valency
tree resembles a TAG derivation tree in that it
is unordered and may have edge labels, which
here indicate semantic roles. The valency tree for
laughs in Figure 1 mentions two argument roles,
namely A(GENT) and P(ATIENT), that are spec-
ified along the feature structures in the respec-
tive nodes. Note that features can be polarized
in sense of (Guillaume and Perrier, 2009): fea-
tures that must be specified carry an exclamation
mark, while the #-symbol is attached to “neutral-
ized” features which may not unify with another
neutralized feature. Links, finally, are represented
by circled numbers, i e. 1© . . . n©.

Speaking more formally, a STUG consists
of tuples 〈σ, {φ1, . . . , φn},⌢〉 with σ being
a syntactic tree, with a set of valency trees
{φ1, . . . , φn} and a linking relation ⌢, for which

〈 VP 1©

V

laughs

,

[
LEM laugh
MODE ind

]1©




LEM !
CASE nom
AGR 3rdsing




A

[
PREP at

]

P

〉

Figure 1: STUG pair for laughs.

the following holds:

• ⌢: P (Vσ) → 2Vφ1∪...∪Vφn , where P (Vσ) is
the partition of the set of nodes from σ, and
where 2Vφ1∪...∪Vφn is the power set of the
union of the sets of nodes from φ1, . . . , φn.

• For every valency tree φi ∈ {φ1, . . . , φn}
with nodes Vi there exists at least one
〈Vsyn, Vval〉 ∈ ⌢ with Vi ∩ Vval 6= ∅.

In other words, (i) a link relates two sets of
nodes, (ii) every syntactic node participates at
most in one link and (iii) every valency tree must
be linked with the syntactic tree. Note that we
omit set braces around valency trees whenever a
STUG pair includes only one valency tree, as is
the case in Figure 1.

2.2 The combinatorial operations

While STAG uses substitution and adjunction in
both domains, STUG combines syntactic trees by
substitution and fusion, and valency trees by tree
unification.

The fusion operation (Lichte, 2010) is a kind
of tree unification where only single nodes unify,
hereafter further limited to root nodes: When syn-
tactic trees γi, γj with root nodes vi, vj are fused,
the resulting syntactic tree γ′ with root node v′

only includes γ′i and γ
′
j , where γ

′
i is γi with vi re-

placed by v′, and where γ′j is γj with vj replaced
by v′. Furthermore every node from γ′i linearly
precedes every node from γ′j . A sample derivation
of John sometimes laughs using fusion and sub-
stitution is shown in Figure 2, which also demon-
strates, that fusion allows the generated syntactic
structures to be flat.

The order of fusion is controlled via finite state
automata (FSA) that are assigned to nodes based
on their syntactic label. A sample FSA for label
VP is depicted in Figure 3. It restricts the linear

47



s0 s1 s2 s3 s4 s5
PP NP ADV V PP

NP V

Figure 3: Sample finite state automaton for category VP.

VP VP VP

NP ADV V

NP sometimes laughs

N

John ;

VP

NP ADV V

N sometimes laughs

John

Figure 2: Derivation of John sometimes laughs.
Dashed horizontal lines stand for fusion and dashed
vertical lines stand for substitution.

order of daughter nodes in the following way: Let
lV (v) be the label of v and let fsa(lV (v)) be the
FSA assigned to v. If v1 . . . vn is the sequence of
daughter nodes of v in the final derived tree, the
word lV (v1) . . . lV (vn) must be in L(fsa(lV (v))).
To come back to our example, the derived tree
in Figure 2 is licit, since the label sequence NP
ADV V of the daughters of the VP node is in the
language of the corresponding FSA shown in Fig-
ure 3.

In the process of fusion, links in the syntac-
tic trees are collapsed in the following way: Let
〈Vsyni , Vvali〉 and 〈Vsynj , Vvalj 〉 be the links asso-
ciated with nodes vi, vj that are replaced by v′.
After fusion there is a new link 〈Vsyni \ vi ∪
Vsynj \ {vj} ∪ {v′}, Vvali ∪ Vvalj 〉. Some-
thing similar applies during substitution.

The combination of valency trees falls back
upon the more general notion of (tree) unification,
as is known, e. g., from Tree Unification Grammar
(TUG) (Popowich, 1989; Gerdes, 2004). Unify-
ing trees γ1 and γ2 to obtain tree γ3 implies that
the unifying and the resulting nodes form one iso-
morphic subtree in γ1, γ2 and γ3 respectively. In
order to narrow down the space of results, only

tree unifications with a maximal number of uni-
fied nodes are considered. In STUG, the linking
structure poses further constraints on tree unifica-
tion: (i) unifying valency trees must be co-linked;
(ii) if two unifying nodes carry links, they must
be co-linked. When two nodes unify, their fea-
tures structures unify, just like the label of unify-
ing edges.

Finally, it needs to be specified, what happens
to the links when unifying nodes of valency trees.
Roughly, the links of the resulting node form the
set union of the links of the unifying nodes. More
precisely, if nodes vi, vj are replaced by v, every
link 〈Vsyn, Vval〉 with vi ∈ Vval or vj ∈ Vval is
replaced by 〈Vsyn, Vval \ {vi, vj} ∪ {v}〉.

Coming back to the STUG derivation of the
sentence John sometimes laughs, Figure 4 dis-
plays the required elementary STUG pairs. Note
that the valency structure of sometimes specifies
a T(ENSE)-role and that the unlexicalized STUG
pair solely serves to embed NPs in a VP. The
STUG derivation can be processed in two steps:
first the syntactic tree is generated according to
Figure 2, and after that the collected valency trees
get unified into one. This two step approach is
pursued in Figure 5.

3 Expressive power

STUG is powerful enough to account for ill-
nested dependencies such as in (1):

(1) a b c d e

Assuming that the language contains only this
string, the corresponding STUG in Figure 6 ex-
ploits the fact that all words differ and every word
has a unique valency structure. Then in a flat
structure, licensed by a simple FSA directly on
the words, every node of the valency structure is
linked to the S node, so that polarized features
bring about the intended dependency relations and
nothing more.

On the other side, STUG seems not capable
of generating the counting language {an bn|n >
0} with crossed dependencies (and thus also

48



〈
VP 1©

NP 2© ADV V

N sometimes laughs

John

,





[ ]1©

[ ]2©

[
LEM laugh
MODE ind

]1©

[
CASE nom
AGR 3rdsing

]
A

[
PREP at

]

P

[
LEM John
PREP null

]2© [ ]1©

[
LEM sometimes

]
T





〉

;

〈
VP 1©

NP 2© ADV V

N sometimes laughs

John

,





[
LEM laugh

MODE ind

]1©




LEM John
PREP null
CASE nom

AGR 3rdsing




2©
A

[
PREP at

]

P

[
LEM sometimes

]

T





〉

Figure 5: STUG derivation of John sometimes laughs.

not the copy language). Apparently linking is
not selective enough to relate ai only with bi,
while both can be in arbitrarily distant parts
of the syntactic tree. Something similar also
holds for the scrambling language SCRind =
{σ(NP1, . . . ,NPm)Vm . . . V1|m ≥ 1 and σ is a
permutation }, where every Vi + 1 is supposed
to govern Vi and which is beyond the expressive
power of LCFRS (Becker et al., 1991). The count-
ing language {an bn|n > 0} with nested depen-
dencies is different is this respect, as can be seen
from the STUG in Figure 7.

If we make use of neutralized polarity in fea-
tures (indicated by #), which prevents unifica-
tion of neutralized features and therefore helps
to keep apart certain nodes in the valency tree,
it is possible to generate the MIX-language, i.e.
{w|w ∈ {a, b, c}∗, |a|w = |b|w = |c|w}, as Fig-
ure 8 proves. Furthermore the grammar in Fig-
ure 8 can be easily adapted to derive the counting
language {an bn cn dn en|n > 0} which also lies
beyond the expressive power of TAG.

Hence, STUG seems to be both more and less
powerful than TAG.

4 Formalism related questions

4.1 Valency structure = dependency
structure?

The valency structure of a sentence and its de-
pendency structure are not isomorphic. As Fig-
ure 9 shows, the contribution of functional ele-
ments such as the complementizer that and the
passive auxiliary is can be diverse: that only con-
tributes to the morpho-syntactic properties of the
predicate node that is linked to the VP node in
syntax; is furthermore specifies certain roles of
the corresponding predicate. In both cases, how-
ever, functional elements do not get represented
as nodes in the valency tree. This follows from
the concept of valency as a mapping based solely
on semantic roles. In contrast, the dependency
structure would include also functional elements
as single nodes, as it is a graph over words by def-
inition.

49



〈 VP 1©

NP 2© ,

[ ]1©

[ ]2©

〉

〈 VP 1©

V

laughs

,

[
LEM laugh
MODE ind

]1©

[
CASE nom
AGR 3rdsing

]
A

[
PREP at

]
P

〉

〈 NP 1©

N

John

,

[
LEM John
PREP null

]
1©
〉

〈 VP 1©

ADV

sometimes

,

[ ]1©

[
LEM sometimes

]
T

〉

Figure 4: Elementary STUG pairs for John sometimes
laughs.

One of the benefits in choosing valency struc-
tures might be that it circumvents the notoriously
unclear (i.e. somewhat arbitrary) status of func-
tional elements in dependency representations.

Note that derived valency structures do not only
include semantic roles of arguments, but also se-
mantic roles of adjuncts (see Figure 5). Conse-
quently, they seem to have much in common with
descriptions known from scenes-and-frames se-
mantics (Fillmore, 1977a; Fillmore, 1977b).

4.2 Sister adjunction instead of fusion?

Both fusion and sister adjunction (Rambow et al.,
1995; Chiang, 2003) support the generation of
flat structures. However, they do not seem to
be interchangeable in the context of STUG (see
(Lichte, 2010) for a related discussion concerning
TT-MCTAG). While fusion merges the root nodes
of trees, sister adjunction rather draws a new edge
between nodes. Choosing sister adjunction in-
stead of fusion therefore considerably reduces the

〈 S 1©

a
,

[
LEM a

]1©

[
LEM !b

]1© [
LEM !c

]1©

〉

〈 S 1©

b
,

[
LEM b

]1©

[
LEM !d

]1©

〉 〈 S 1©

c
,

[
LEM c

]1©

[
LEM !e

]1©

〉

〈 S 1©

d
,
[

LEM d
]1©〉 〈 S 1©

e
,
[

LEM e
]1©〉

S:

s0 s1 s2 s3 s4 s5
a b c d e

Figure 6: STUG for the ill-nested dependency struc-
ture in (1).

〈 S 1©

a
,

[
LEM a

]1©

[
LEM !b

]

〉 〈 S 1©

b
,

[ ]1©

[
LEM b

]

〉

〈 S 1©

S 2©
,

[ ]1©

[ ]1©

〉

S:

s0 s1 s2 s3
a S b

b

Figure 7: STUG for the string an bn with nested de-
pendencies.

number of nodes, to which links can be attached
in the lexicon. But since linking is a cornerstone
of STUG, it is hard to see, how a STUG with sis-
ter adjunction for the mentioned cases would look
like, let alone whether this would perform any bet-
ter.

4.3 Feature structures instead of trees?

So far valency structures are represented as un-
ordered trees whose edges carry role labels and
stand for semantic relations, some of them func-
tional in nature. It is thus debatable, whether one
should choose a representation based on features
structures instead, as they account for functional
relations more straightforwardly. To give an ex-

50



〈 S 1©

a
,




A #+
B !+

C !+




1©

[ ]1©

〉 S:

s0

a, b, c

〈 S 1©

b
,




A !+
B #+
C !+




1©〉 〈 S 1©

c
,




A !+
B !+
C #+




1©〉

Figure 8: An STUG with neutralized features, marked
up by exclamation marks, for deriving the MIX-
language.

〈 VP 1©

COMP

that

,

[
COMP that
MODE !ind

]1©
〉

〈 VP 1©

V

is

,

[
PART !+
MODE ind

]1©

[
PREP by

]
A

[
CASE nom

]
P

〉

Figure 9: STUG pairs for complementizer that and
passive auxiliary is. Exclamation marks indicate po-
larized features.

ample, the valency tree for laughs in Figure 1
could be replaced by the feature structure repre-
sentation in Figure 10.

On the other side, a tree-based representation
seems to pay off in cases where semantic roles
are to be underspecified or where relations are
non-functional, i.e. a single semantic role gets
assigned to several constituents. The latter hap-
pens most prominently with temporal or loca-
tional roles.

5 Some consequences: ellipsis, grammar
size and incrementality

Contrary to the amalgamation of syntax and va-
lency in the TAG framework, STUG allows for
a clear separation of syntax and valency, accord-
ing to which syntax is only concerned with the




LEM laugh
MODE ind

A




LEM !
CASE nom
AGR 3rdsing




P
[

PREP at
]




1©

Figure 10: The valency tree of Figure 1 as feature
structure.

〈
VP 1©

NP 2© ADV

N sometimes

John

,

[ ]1©

[
LEM John
PREP null

]2© [
LEM sometimes

]
T

〉

Figure 11: Derived STUG pair for John sometimes,
which could be the fragmentary answer to the question
Who laughs?, based on the elementary STUG pairs in
Figure 4.

linearization of words (based on their syntactic
category) and the determination of the local do-
main (based on the linking). Argument linking,
however, completely rests on the unification of va-
lency trees. This kind of separation has interest-
ing consequences for the model of syntax. In the
following we will discuss three of them involving
ellipsis, grammar size and incrementality.

5.1 Ellipsis

Since syntactic trees can combine without there
being any direct valency relation, STUG supports
the base generation of ellipsis straightforwardly,
as is shown in Figure 11 using the example of gap-
ping. While the root node in the valency tree re-
mains unspecified, the syntactic tree does not in-
clude any kind of empty placeholder for the miss-
ing verb. Furthermore neither extra rules nor ex-
tra lexical entries are involved in the process of
derivation. Reconstruction, however, has to be
guided by context, which can be thought of as set
of more or less salient valency structures.

This analysis differs fundamentally from syn-
tactic models that adhere to the amalgamation of

51



syntax and valency. They start out from com-
plete syntactic representations of valency frames,
which they adapt in cases of incompleteness. In
this respect TAG accounts behave similar to ac-
counts from GB, CCG or HPSG. Either two com-
plete syntactic representations are “contracted”
(Sarkar and Joshi, 1996; Sarkar and Joshi, 1997),
or one complete syntactic representation is aug-
mented with empty words as a result of deletion
(Lichte and Kallmeyer, 2010) or insertion (Sed-
dah and Sagot, 2006), or the lexicon includes
incomplete syntactic representations as defective
variants (Sarkar, 1997). None of these strategies
is pursued in STUG.

In favour of contraction accounts one could ar-
gue, that contraction and reconstruction of ellipsis
go hand in hand. Hence no extra mechanism for
reconstruction is required. However, contraction
is only applicable in cases where ellipsis and its
antecedent can be located in the same sentence.
It is therefore hardly applicable to fragementary
answers or fragmentary corrections (Ginzburg
and Sag, 2001; Schlangen, 2003; Ginzburg and
Cooper, 2004), not to speak of discourse-inital
fragments (Stainton, 1998; Stainton, 2006). For
these cases a separate reconstruction mechanism
based on the surrounding discourse is needed any-
ways. (Lichte and Kallmeyer, 2010) propose to
relate reconstruction to the derivation tree of the
antecedent sentence.

Summing up it can be said that the sketched
STUG account to ellipsis looks promising, as no
extra syntactic mechanism is used and reconstruc-
tion relates to valency structures, which seem at
least as suitable as derivation trees.

5.2 Grammar size

For generating and maintaining a large-coverage
TAG, the use of a metagrammar system is al-
most inevitable due to the size of the grammar.
Regarding, for example, XTAG (XTAG Research
Group, 2001), (Prolo, 2002) counts 97 tree tem-
plates for intransitive, transitive and ditransitive
subcategorization frames. Taking all subcatego-
rization frames (including e. g. those for idioms)
into account, XTAG contains even 1008 verbal
tree templates. This is due to the fact, that alter-
native argument realizations tend to be derived by
means of different tree templates. In other words,
the set of tree templates is some subset of the
“Cartesian product” (Prolo, 2002) of subcatego-

rization frames, alternative linearizations and ac-
tive/passive alternations.

On top of that, optional arguments further in-
crease lexical ambiguity by joining different tree
families. For example, the finite verb laughs an-
chors intransitive tree families with and without
PP argument, thus at least ten tree templates: two
of the base configuration, three for extraction in-
cluding preposition stranding, and five for relative
clauses.

STUG helps to eliminate these two sources for
large grammars and lexical ambiguity. Alterna-
tive linearizations can be represented in one FSA,
i. e. outside elementary structures, while optional
and obligatory arguments can be differentiated lo-
cally within a valency tree. Therefore the num-
ber of elementary structures reduces substantially
compared to TAG. This already becomes apparent
with regard to the lexical STUG pair in Figure 1,
which, in combination with the simplistic FSA in
Figure 3, suffices to cover four out of the ten men-
tioned tree templates for the finite verb laughs.

But it is not only the reduction of elementary
structures that makes STUG attractive. Another
source for grammar complexity lies in the rich
feature structures with which nodes of TAG ele-
mentary trees may be equipped. XTAG defines
around 50 features and uses, for example, no less
than nine features to get the sequencing of deter-
miners in NPs right. But also the verbal projection
is equipped with an impressive number of fea-
tures. Some of them help to constrain lineariza-
tion (e. g. INV and COMP), while others pass on
case or agreement restrictions, such as ASSIGN-
CASE and AGR, or just display morphological
properties of a phrasal head, auch as MODE.

By contrast, the snippets of STUG presented
above have already shown, that no features get
percolated around in the syntax tree of a STUG
pair. Instead their main purpose is to specify
nodes in the valency tree directly. Accordingly,
mediating features like ASSIGN-CASE seem to be
obsolete, as even raising verbs can directly access
the raised subject in the valency structure. This
is exemplified in Figure 12. Finally, the work of
features that constrain linearization is now done
elsewhere, namely in FSAs, making them obso-
lete as well. Hence, STUG seems to allow for
a more precise, more transparent use of features,
and it also seems to require a smaller number of
features compared to TAG/XTAG.

52



〈 VP 1©

V

seems

,

[
LEM seem
MODE ind

]1©

[
LEM !
MODE inf

]1©
TH

[
CASE nom
AGR 3rdsing

]

〉

Figure 12: Lexical STUG pair for the raising verb
seems.

5.3 Incrementality

Following (Sturt and Lombardo, 2005) incremen-
tal processing does not only imply that a sen-
tence is parsed left-to-right on a word-by-word
basis, but also that a connected syntactic repre-
sentation is available for every prefix of the sen-
tence. (Sturt and Lombardo, 2005) call this prop-
erty full connectedness. Parsing with TAG, how-
ever, does not meet full connectedness out of the
box, since, e. g., elementary trees of an argument
and a modifier cannot be connected when pre-
ceding their governor. To fill this gap, two TAG
variants have been proposed so far, namely Dy-
namic TAG (Mazzei, 2005; Mazzei et al., 2007)
and PLTAG (Demberg and Keller, 2008; Dem-
berg, 2010). Note that this is not at all a prob-
lem specific to TAG, but also comparable gram-
mar formalisms such as CCG and Dependency
Grammar are affected (Demberg, 2010), which
similarly pursue an amalgamation of syntax and
valency.

Contrary to TAG, parsing with STUG can be
conducted in a fully connected manner even for
sentences like John sometimes laughs, as the
derivation in Figure 2 can be read off from the left
to the right. This is mainly due to three factors:
(i) syntactic trees of governors are spinal (i. e., the
extended domain of locality is not found in the
syntactic trees, but in the valency trees) and (ii)
arguments and modifiers can always be connected
without mediation of the governor. Finally, (iii)
the derivation of the syntactic tree and the valency
tree can be done asynchronously. It remains to
be seen, how STUG compares to advanced psy-
cholinguistic models such as PLTAG. This seems
particularly interesting, for STUG does not use
traces and, moreover, has a stronger affinity for

free word order languages.

6 Conclusion

We presented a novel tree-based grammar for-
malism, Synchronous Tree Unification Grammar
(STUG), which differs significantly from usual
grammar formalisms such as TAG in that it allows
for a separation of syntax and valency. After hav-
ing described the functionality of STUG and hav-
ing explored its expressive power, we briefly dis-
cussed some prospects concerning the modeling
of ellipsis, the size and complexity of the gram-
mar, and incremental, fully connected parsing. As
encouraging as they may be, there is no doubt that
many details are still unclear and need to be elab-
orated on in future work.

References

Anne Abeillé and Owen Rambow. 2000. Tree adjoin-
ing grammar: An overview. pages 1–68.

Anne Abeillé. 1988. A lexicalized tree adjoin-
ing grammar for French: The general framework.
Technical Report MS-CIS-88-64, Department of
Computer and Information Science , University of
Pennsylvania.

Tilman Becker, Aravind K. Joshi, and Owen Rambow.
1991. Long-distance scrambling and tree adjoining
grammars. In Proceedings of EACL-91.

David Chiang. 2003. Statistical parsing with an au-
tomatically extracted tree adjoining grammar. In
Rens Bod, Remko Scha, and Khalil Sima’an, edi-
tors, Data Oriented Parsing, pages 299–316. CSLI
Publications.

Vera Demberg and Frank Keller. 2008. A psycholin-
guistically motivated version of TAG. In Proceed-
ings of TAG+9, pages 25–32, Tübingen.

Vera Demberg. 2010. A Broad-Coverage Model of
Prediction in Human Sentence Processing. Ph.D.
thesis, The University of Edinburgh.

Denys Duchier, Joseph Le Roux, and Yannick Par-
mentier. 2004. The Metagrammar Compiler: An
NLP Application with a Multi-paradigm Architec-
ture. In Second International Mozart/Oz Confer-
ence (MOZ’2004).

Charles J. Fillmore. 1977a. The case for case re-
opened. In Peter Cole and Jerrold M. Sadock, ed-
itors, Grammatical Relations, volume 8 of Syntax
and Semantics, pages 59–81. Academic Press, New
York.

Charles J. Fillmore. 1977b. Scenes-and-frames se-
mantics. In Antonio Zampolli, editor, Linguis-
tic Structures Processing, volume 5, pages 55–81.
North Holland, Amsterdam.

53



Robert Frank. 1992. Syntactic Locality and Tree Ad-
joining Grammar: Grammatical, Acquisition and
Processing Perspectives. Ph.D. thesis, Department
of Computer and Information Science, University
of Pennsylvania.

Robert Frank. 2002. Phrase Structure Composition
and Syntactic Dependencies. MIT Press, Cam-
bridge,MA.

Kim Gerdes. 2004. Tree Unification Grammar. In
Lawrence S. Moss and Richard T. Oehrle, editors,
Electronic Notes in Theoretical Computer Science,
volume 53. Elsevier. Proceedings of the joint meet-
ing of the 6th Conference on Formal Grammar and
the 7th Conference on Mathematics of Language.

Jonathan Ginzburg and Robin Cooper. 2004. Clarifi-
cation, ellipsis, and the nature of contextual updates.
Linguistics and Philosophy, 27(3):297–366.

Jonathan Ginzburg and Ivan A. Sag. 2001. Interrog-
ative Investigations: The Form, Meaning, and Use
of English Interrogatives. CSLI Publications, Stan-
ford, CA.

Bruno Guillaume and Guy Perrier. 2009. Interaction
Grammars. Research on Language and Computa-
tion, 7(2–4):171–208.

Timm Lichte and Laura Kallmeyer. 2010. Gapping
through TAG derivation trees. In Proceedings of
TAG+10, pages 93–100, New Haven, CT.

Timm Lichte. 2010. From partial VP fronting towards
Spinal TT-MCTAG. In Proceedings of TAG+10,
pages 85–92, New Haven, CT.

Alessandro Mazzei, Vincenzo Lombardo, and Patrick
Sturt. 2007. Dynamic TAG and lexical depen-
dencies. Research on Language and Computation,
5(3):309–332.

Alessandro Mazzei. 2005. Formal and Empirical Is-
sues of Applying Dynamics to Tree Adjoining Gram-
mars. Ph.D. thesis, University of Torino.

Rebecca Nesson and Stuart Shieber. 2008. Syn-
chronous vector-TAG for natural language syn-
tax and semantics. In Proceedings of TAG+9,
Tübingen, Germany, 7–8 June.

Fred Popowich. 1989. Tree Unification Grammar. In
Proceedings of ACL-89, pages 228–236, Vancou-
ver, British Columbia, Canada, June.

Carlos A. Prolo. 2002. Generating the XTAG En-
glish grammar using metarules. In Proceedings of
COLING-02, pages 814–820, Taipei. Taiwan.

Owen Rambow, K. Vijay-Shanker, and David Weir.
1995. D-tree grammars. In Proceedings of ACL-
95, Cambridge, MA.

Anoop Sarkar and Aravind Joshi. 1996. Coordina-
tion in tree adjoining grammars: Formalization and
implementation. In Proceedings of COLING-96,
pages 610–615, Copenhagen, August 5-9.

Anoop Sarkar and Aravind Joshi. 1997. Handling
coordination in a tree adjoining grammar. Longer
version of paper in Proceedings of COLING 1996.
Draft of August 19, 1997.

Anoop Sarkar. 1997. Seperating dependency from
constituency in a tree rewriting system. In Tilman
Becker and Hans-Ulrich Krieger, editors, Proceed-
ings of the Fifth Meeting on Mathematics of Lan-
guage, pages 153–160, Saarbrücken.

David Schlangen. 2003. A Coherence-Based Ap-
proach to the Interpretation of Non-Sentential Ut-
terances in Dialogue. Ph.D. thesis, University of
Edinburgh.

Djamé Seddah and Benoı̂t Sagot. 2006. Modeling
and analysis of elliptic coordination by dynamic ex-
ploitation of derivation forests in LTAG parsing. In
Proceedings of TAG+8, pages 147–152, Syndey.

Djamé Seddah, Benoı̂t Sagot, and Laurence Danlos.
2010. Control verb, argument cluster coordina-
tion and multi component TAG. In Proceedings of
TAG+10, pages 101–109, New Haven, CT.

Djamé Seddah. 2008. The use of MCTAG to process
elliptic coordination. In Proceedings of TAG+9,
pages 81–88, Tübingen.

Stuart M. Shieber and Yves Schabes. 1990. Syn-
chronous tree-adjoining grammars. In Proceed-
ings of COLING-90, volume 3, pages 253–258,
Helsinki, Finland.

Stuart M. Shieber. 1994. Restricting the weak-
generative capacity of synchronous Tree-Adjoining
Grammar. Computational Intelligence, 10(4):371–
385.

Robert J. Stainton. 1998. Quantifier phrases, mean-
ingfulness “in isolation”, and ellipsis. Linguistics
and Philosophy, 21:311–340.

Robert J. Stainton. 2006. Words and Thoughts.
Subsentences, Ellipsis, and the Philosophy of Lan-
guage. Oxford Univ. Press.

Patrick Sturt and Vicenzo Lombardo. 2005. Process-
ing coordinated structures: Incrementality and con-
nectedness. Cognitive Science, 29(2):291–305.

XTAG Research Group. 2001. A Lexicalized Tree
Adjoining Grammar for English. Technical report,
Institute for Research in Cognitive Science, Univer-
sity of Pennsylvania, Philadelphia, PA.

54


