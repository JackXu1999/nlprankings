



















































The University of Alicante at MultiLing 2015: approach, results and further insights


Proceedings of the SIGDIAL 2015 Conference, pages 250–259,
Prague, Czech Republic, 2-4 September 2015. c©2015 Association for Computational Linguistics

The University of Alicante at MultiLing 2015: approach, results and
further insights

Marta Vicente
University of Alicante
Apdo. de correos 99

E-03080 Alicante, Spain
mvicente@dlsi.ua.es

Óscar Alcón
University of Alicante
Apdo. de correos 99

E-03080 Alicante, Spain
oalcon@dlsi.ua.es

Elena Lloret
University of Alicante
Apdo. de correos 99

E-03080 Alicante, Spain
elloret@dlsi.ua.es

Abstract

In this paper we present the approach and
results of our participation in the 2015
MultiLing Single-document Summariza-
tion task. Our approach is based on
the Principal Component Analysis (PCA)
technique enhanced with lexical-semantic
knowledge. For testing our approach, dif-
ferent configurations were set up, thus
generating different types of summaries
(i.e., generic and topic-focused), as well as
testing some language-specific resources
on top of the language-independent basic
PCA approach, submitting a total of 6 runs
for each selected language (English, Ger-
man, and Spanish). Our participation in
MultiLing has been very positive, ranking
at intermediate positions when compared
to the other participant systems, showing
that PCA is a good technique for gen-
erating language-independent summaries,
but the addition of lexical-semantic knowl-
edge may heavily depend on the size and
quality of the resources available for each
language.

1 Introduction

Currently, the amount of on-line information gen-
erated per week reaches the same quantity of data
that the one produced in the Internet between its
inception and 2003, time of the Social Network
emergency (Cambria and White, 2014). More-
over, the production of such volume of data is de-
livered in multiple languages, and accessing the
relevant content of information or extracting the
main features of documents in a competitive time
is more and more challenging. Therefore, auto-
matic tasks that can help processing all this infor-
mation, such as multilingual text summarization
techniques, are now becoming essential.

Back in 2011, the Text Analysis Conference
MultiLing Pilot task1 was first introduced as an ef-
fort of the community to promote and support the
development of multilingual document summa-
rization research. Considering the impact of this
shared tasks in the progress of natural language
processing technologies, a mutlilingual summa-
rization workshop was also organized in 20132.

Nowadays, in 2015, we take part in the 3rd
MultiLing event3. In this edition, new tasks
have been added in order to adapt to social re-
quirements. There were the traditional Multilin-
gual Multi-document and Single-document Sum-
marization (MMS and MSS), coming from previ-
ous events, but also new summarization tasks re-
lated to Online Fora (OnForumS) - on how to deal
with reader comments- and Call Center Conversa-
tion (CCCS) - from spoken conversations to tex-
tual synopses.

Taking into consideration the interest that mul-
tilingual summarization approaches is gaining
among the research community, and the positive
impact and benefits it may have for the society,
the objective of this paper is to present a multi-
lingual summarization approach within the Mul-
tiLing 2015 competition, discussing its potentials
and limitations, and providing some insights of the
future of this type of summarization based on the
average results obtained by us and other partici-
pants as well.

The remaining of the paper is organized as fol-
lows. In Section 2 we review the most relevant
multilingual summarization approaches, some of
them participating in previous MultiLing events.
In Section 3, we explain our multilingual sum-
marization approach and the required language-
dependent knowledge. Section 4 describes the

1http://www.nist.gov/tac/2011/Summarization/
2http://multiling.iit.demokritos.gr/pages/view/662/multiling-

2013
3http://multiling.iit.demokritos.gr/pages/view/1516/multiling-

2015

250



task in which we participated, and the experiments
performed. Furthermore, the results together with
their discussion and comparison to other partici-
pants are provided in Section 5, followed by an
analysis of the potentials and limitations of our ap-
proach in Section 6. Finally, the main conclusions
are outlined in Section 7.

2 Related work

Eight teams participated in the Multilingual Pilot
task in 2011, five of them testing their approaches
for all the proposed languages (Arabic, Czech, En-
glish, French, Greek, Hebrew, and Hindi) (Gian-
nakopoulos et al., 2011). Two systems are worth
mentioning. On the one hand, the CLASSY sys-
tem (Conroy et al., 2011) that ranked 2nd or 3rd
in 5 out 7 languages. The main feature of this ap-
proach was that a model was first trained on a cor-
pus of newswire taken from Wikinews, and then
term scoring was limited to the naive Bayes term
weighting. The final process of sentence selection
was performed using non-negative matrix factor-
ization and integer programming techniques. On
the other hand, the best system on average was the
one in (Steinberger et al., 2011), performing the
1st in five of the seven languages, and 4th in the
two remaining ones. This approach did not used
any language-dependent resources, apart from a
stopword list for each language, and it relied on
Latent Semantic Analysis and Singular Value De-
composition.

In the 2013 MultiLing edition, four teams par-
ticipated submitting six systems to the task (Gian-
nakopoulos, 2013). For their assessment in (Ku-
bina et al., 2013), they were denoted as MUSE,
MD, AIS and LAN. We briefly reviewed these
approaches. MUSE (Litvak and Last, 2013), is
a supervised learning approach that scores sets
of sentences by means of a genetic algorithm.
MD (Conroy et al., 2013) developed techniques
both for MMS and MSS, examining the impact
of dimensionality reduction and offering differ-
ent weighting methods in the experiments: either
considering the frequency of terms or applying
a variant of TextRank, among others. Adapting
their techniques to Arabic and English languages,
the LAN team (El-Haj and Rayson, 2013) im-
plements a system that recovers the most signif-
icant sentences for the summary using word fre-
quency and keyness score, introducing a statis-
tic approach that extracts those sentences with

the maximum sum of log likelihood. Contrary
to the previously described systems, mostly based
in frequency of terms, AIS (Anechitei and Ignat,
2013) presented an approach based on the anal-
ysis of the discourse structure, exploiting, there-
fore, cohesion and coherence properties from the
source articles. Although some of these partici-
pants performed well, achieving similar results as
the ones obtained by human summaries, the WBU
approach (Steinberger, 2013), was again the best
performing summarization system in this MultiL-
ing edition, reaching the first position in 5 of the 10
languages. Specifically, it was an improved ver-
sion of the best-performing approach in MultiLing
2011 (Steinberger et al., 2011).

Outside the MultiLing competitions, other re-
search works have been recently proposed, obtain-
ing better results than existing commercial multi-
lingual summarizers. An example of this can be
found in (Lloret and Palomar, 2011) were three
different approaches were analyzed and tested: i)
one using language-independent techniques; ii)
one with language-dependent resources; and iii)
one using machine translation to monolingual
summarization. The results obtained showed that
having high-quality language specific resources
often led to the best results; however, a simple
language-independent approach based on term fre-
quency was competitive enough, avoiding the ef-
fort needed to develop and/or obtain the particular
resources for each language, when they were not
available.

Having revised different multilingual summa-
rization approaches, the main contribution of our
paper is to propose a novel approach based on the
Principal Component Analysis (PCA) technique,
studying the influence of lexical-semantic knowl-
edge to the base approach. To the best of our
knowledge, although PCA has been already used
for text summarization (for instance, in (Lee et al.,
2003)), it has never been tested with the addition
of semantic knowledge, nor in the context of mul-
tilingual summarization. Given that it bears some
relation to LSA and SVD techniques, and it has
been shown that such techniques are very compet-
itive, MultiLing 2015 is the perfect context to test
it.

3 The UA-DLSI Approach

In this Section, we present our proposed multilin-
gual summarization approach (i.e., UA-DLSI ap-

251



proach).
As it was previously mentioned, the main tech-

nique that characterise the UA-DLSI approach is
the Principal Component Analysis (PCA). PCA is
a statistical technique focused on the synthesis of
information to compress and interpret the data (Es-
tellés Arolas et al., 2010).

As a method for developing summarization sys-
tems, PCA provides a way to determine the most
relevant key terms of a document. It has been often
employed in conjunction with other data mining
techniques, such as Semantic Vector Space model
(Vikas et al., 2008) or Singular Value Decompo-
sition (Lee et al., 2005), using term-based fre-
quency methods. Our main difference with respect
to other summarization PCA-based approaches is
the incorporation of lexical-semantic knowledge
into the PCA technique, since it is necessary to go
beyond the terms, and determine the meaningful
sentences. Moreover, to finish the process, some
strategies for selecting relevant information (in our
case, choosing the most relevant sentences) needs
to be defined as well.

For developing our UA-DLSI approach, we re-
lied on the summary process stages outlined in
(Sparck-Jones, 1999): 1) interpretation, 2) trans-
formation and, finally, 3) the summary generation.

Interpretation. The first stage of our approach
includes a linguistic and lexical-semantic process-
ing (this latter part is optional). For the linguistic
processing, sentence segmentation, tokenization
and stopwords removal is applied. For the lexical-
semantic processing, a named entity recognizer
(Standford Named Entity Recognizer4) and se-
mantic resources, such as WordNet (Miller, 1995)
and EuroWordNet (Vossen, 2004) are employed.
Whereas named entity recognizers mainly provide
the identification of person, organization and place
names in a document (Tjong et al., 2003), the
semantic resources used comprises a set of syn-
onyms grouped by means of the synsets that allow
us to work with concept better than just with terms.
In this manner, we group a set of synonyms under
the same concept. For instance, detonation and
explosion are different words but their share the
same synset (07323181), so we would keep them
as a single concept. For identifying concepts, we
relied on the most frequent sense approach, and
therefore, the process searches for the first synset

4http://nlp.stanford.edu/software/
CRF-NER.shtml

of each word in the document, which corresponds
to its most probable meaning. If two words have
the same first synset, we will assume that they are
synonyms and their occurrences will be added to-
gether.

The result of this stage is to build an initial
lexical-semantic matrix, where for each sentence
(rows in our matrix), we identify the units that will
be later taken into account (i.e., terms, named enti-
ties, and/or concepts) which will correspond to the
columns.

Transformation. It is in the transformation
stage that we use the PCA method. In our ap-
proach, PCA is applied using the PCA transform
Java library5 to process the covariance matrix that
is computed from the lexical-semantic matrix ob-
tained in the previous stage. Once PCA has been
applied over the covariance matrix, the principal
components (eigenvectors) and its corresponding
weight (eigenvalue) are obtained. The eigenvec-
tors are composed by the contribution of each
variable, which determines the importance of the
variable in the eigenvector. Moreover, the eigen-
vectors are derived in decreasing order of impor-
tance. In this manner, an eigenvector with high
eigenvalue carries a great amount of information.
Therefore, the first eigenvectors collect the major
part of the information extracted from the covari-
ance matrix, and they will be used for determining
the most important sentences in the document, as
it will be next shown.

Summary generation. In this final stage, the
relevant sentences are selected and extracted, thus
producing an extractive summary. Since from the
previous stage, only the key elements (e.g., con-
cepts) were determined, it is necessary to define
some strategies for deciding which sentences con-
taining these elements will be finally taking part in
the summary.

Two strategies were proposed for selecting and
ordering the most relevant sentences from the doc-
ument, leading to two types of summaries: one
generic and one topic-focused. In this manner, tak-
ing into account the element with the highest value
for each eigenvector from the PCA matrix, we se-
lect and extract:

• one sentence (searching in order of ap-
pearance in the original text) in which

5https://github.com/mkobos/pca_
transform

252



such concept6 appears. During this
process, if a sentence had been al-
ready selected by a previous concept
to take part in the summary, we would
select and extract the following sentence
in which the concept appears (generic
summary).

• all the sentences (searched in order of ap-
pearance in the original text) in which such
concept appears (topic-focused summary).

Regarding these strategies, it is worth mention-
ing that if we found different concepts with the
same highest value for the same eigenvector, we
would extract the corresponding sentences for all
these concepts. In the same manner, if a synset
is represented by several synonyms, we would ex-
tract the corresponding sentences for each of these
synonyms.

4 Experimental Setup

This section describes the MultiLing 2015 task in
which we participated, together with the dataset
employed, and the explanation of the different
variants of our approach submitted to the compe-
tition.

4.1 MSS - Multilingual Single-Document
Summarization Task

The Multilingual Single Document Summariza-
tion task was initially proposed in MultiLing 2013,
targeting the same goal in the current edition: to
evaluate the performance of participant systems
whose work is focused on generating a single doc-
ument summary for all the given Wikipedia arti-
cles in some of the languages provided (at least
the participants should select three languages). In
the context of MultiLing 2015, two datasets were
provided for the MSS task: a training dataset, con-
taining 30 articles for each of the 38 available lan-
guages with their corresponding human-generated
summaries; and a test dataset, which contains the
same number of documents per language, but dif-
ferent from the training dataset, the human sum-
maries were not provided. For both datasets, the
character length that the target automatic sum-
maries should aim was also provided (i.e., the tar-
get length), which coincided with the length of the
human summaries that will be later used in the

6Concepts here refer to the possible elements that the ma-
trix can have, e.g. named entities, synsets, or terms

evaluation. Each automatic summary had to be as
close to the target length provided as possible, and
summaries exceeding the given target length were
truncated to it.

In order to prove the adequacy of our approach
to select the relevant sentences from a document,
we decided to start testing it within small goals
to be able to analyze and further improve the pro-
posed approach. This was the main reason for par-
ticipating in the MSS task rather than in the MMS,
which had implied more complexity.

Concerning the language choice, since one of
our main objectives was to evaluate the impact of
lexical-semantic knowledge in the summary gen-
eration, some language-dependent resources were
necessary (e.g. WordNet and EuroWordNet). The
availability of these resources also conditioned the
languages that were chosen for testing our appo-
rach, in our case: English, German, and Spanish.

For each language considered, we computed the
average length of the Wikipedia articles in the test
corpus, both in characters and words. These fig-
ures are shown in Table 1. In addition, we also
provide the target summary length (in characters)
and the compression ratio for the summaries. As
it can be seen, the length of the summaries com-
pared to the original length of the Wikipedia arti-
cles (i.e., compression ratio) is very short, always
below 10%. This means that generated summaries
have to be very concise and precise in selecting the
most relevant information.

English Spanish German
Characters 25850 39202 38905
Words 4223 6271 5245
Target length 1858 2044 1071
Compression ratio 7.19% 5.21% 2.75%

Table 1: Average length (words and characters) of
the test dataset, and target length and compression
ratio for the summaries

4.2 Configuring the UA-DLSI approach to
the MSS task

Having provided the information about the general
multilingual summarization process in Section 3,
and since each participant in the MSS task was
allowed to submit up to six approaches, different
versions of our approach were set to participate in
MultiLing 2015.

Apart of the two types of summaries that could

253



be generated with our approach (T1: generic sum-
mary; T3: topic-focused summary), the incorpo-
ration of lexical-semantic knowledge was an op-
tional substage, so we decided to test our ap-
proach also without any type of semantic knowl-
edge, other than a list of stopwords for each
language (LI: language-independent; LEX: us-
ing lexical knowlege (named entity recognition);
SEM: using semantic knowledge (i.e., WordNet
and EuroWordNet)). This way the performance of
a fully language-independent summarization ap-
proach based on PCA could be also analyzed.
Moreover, due to the nature of the test dataset
(Wikipedia articles), all documents included head-
ings for structuring different sections within them,
so we opt for taking advantage of this information,
considering only the words in these headings for
the matrix construction (OWFH), instead of work-
ing with all words in the document, except stop-
words (AW). Headings usually contain important
concepts that reflect the main topic of the section
that follows. Considering only this words, we also
reduce the amount of information we have to pro-
cess by 99% of the PCA matrix.

Therefore, given the impossibility to test all the
variations taking into account these issues, our
submitted approaches for MultiLing 2015, speci-
fying also their priority, were the following:

• T1 LI AW (UA-DLSI-lang-1): generic
language-independent summarizer consider-
ing all words in the documents.

• T1 LI OWFH (UA-DLSI-lang-3): generic
language-independent summarizer consider-
ing only the words included in the headings
of the documents.

• T1 LEXSEM AW (UA-DLSI-lang-4):
generic summarizer, including lexical-
semantic knowledge into the interpretation
stage, and considering all words in the
documents.

• T3 LI OWFH (UA-DLSI-lang-5): topic-
focused language-independent summarizer
considering only the words included in the
headings of the documents.

• T3 LEXSEM AW (UA-DLSI-lang-6):
topic-focused summarizer, including lexical-
semantic knowledge into the interpretation
stage, and considering all words in the
documents.

• T3 LEXSEM OWFH (UA-DLSI-lang-2):
topic-focused summarizer, including lexical-
semantic knowledge into the interpretation
stage, but considering only the words
included in the headings of the documents.

5 Results and Analysis

After all participants submitted their runs to the
MultiLing 2015 MSS task over the test dataset, the
summaries were evaluated via automatic methods.
ROUGE tool (Lin, 2004) was employed for auto-
matic content evaluation, which allows the com-
parison between automatic and model summaries
based on different types of n-grams. Specifically
the ROUGE 1 (unigrams), 2 (bigrams), 3 (tri-
grams), and 4 (quadrigrams), ROUGE-SU4 (bi-
gram similarity skipping unigrams) scores were
computed. The files contain the overall and indi-
vidual summary scores.

Moreover, two additional systems were pro-
posed by the organizers. On the one hand, a sys-
tem called “Lead”, which was the baseline sum-
mary used for the evaluation process. This ap-
proach selects the leading substring of the article’s
body text having the same length as the human
summary of the article. On the other hand, a sys-
tem called “Oracles” was also developed, where
sentences were selected from the body text to max-
imally cover the tokens in the human summary us-
ing as few sentences as possible until its size ex-
ceeded the human summary, upon which it was
truncated.

In this edition, five systems participated in the
MSS task (details about their implementation have
not made available yet). Three of them were ap-
plied to 38 languages, including English, Spanish
and German. They are named as CCS - that im-
plements five variations for each language- LCS-
IESI and EXB. The fourth one, BGU-SCE has been
proven for Arabic and Hebrew, besides English.

Table 2, Table 3, and Table 4 show the results
obtained by all participants, and the two meth-
ods proposed by the organizers in the MultiLing
2015 competition for English, German, and Span-
ish. Due to size constraints, only the average re-
sults for the recall, precision and F-measure met-
rics of ROUGE 1 are shown, since this ROUGE
metric takes into account the common vocabulary
between the automatic and the human summaries,
without taking into account stopwords.

Focusing only on the analysis of our six ver-
sions of our approach (UA-DLIS-lang-priority),

254



System R1 recall R1 precision R1 F-measure
UA-DLSI-en-1 0.45488 0.45827 0.45605
UA-DLSI-en-2 0.42111 0.43774 0.42703
UA-DLSI-en-3 0.37175 0.49104 0.40551
UA-DLSI-en-4 0.45641 0.45673 0.45627
UA-DLSI-en-5 0.41994 0.43334 0.42419
UA-DLSI-en-6 0.42439 0.43093 0.42727
BGU-SCE-M-en-1 0.49195 0.48354 0.48744
BGU-SCE-M-en-2 0.47826 0.47953 0.47868
BGU-SCE-M-en-3 0.45955 0.46053 0.45974
BGU-SCE-M-en-4 0.46819 0.46651 0.46713
BGU-SCE-M-en-5 0.49982 0.48813 0.49361
BGU-SCE-P-en-1 0.46247 0.44367 0.45269
BGU-SCE-P-en-2 0.49420 0.47512 0.48425
BGU-SCE-P-en-3 0.46546 0.45039 0.45753
CCS-en-1 0.49507 0.47662 0.48539
CCS-en-2 0.49041 0.47299 0.48132
CCS-en-3 0.49130 0.47455 0.48255
CCS-en-4 0.48849 0.47211 0.47986
CCS-en-5 0.48689 0.47600 0.48117
EXB-en-1 0.49471 0.46692 0.48022
LCS-IESI-en-1 0.45556 0.46144 0.45811
NTNU-en-1 0.45585 0.46966 0.46213
Lead-en-1 0.43381 0.42495 0.42907
Oracles-en-1 0.61917 0.60114 0.60983

Table 2: Average results for English (recall, precision and F-measure ROUGE 1 (R1) values.

we observe that our approach with priority 3 is one
of our best performing approaches considering the
precision for the three tested languages. This
version corresponds to T1 LI OWFH approach
- generic language-independent summarizer con-
sidering only the words included in the headings of
the documents, and this means that the title head-
ings of the Wikipedia articles do contain enough
meaningful information of the documents. This
is an interesting finding, because we are reducing
the amount of information to be processed by al-
most 99%. Moreover, this also outlines the poten-
tial of the studied PCA technique for developing
completely language-independent summarizers.

Other versions of our proposed approach, such
as the ones submitted as priority 4, and priority
1 may obtained also competitive results for some
languages. Again, the submission with priority
1 correspond to a generic language-independent
summarizer considering all words in the docu-
ments (T1 LI AW). It can be shown that when con-
sidering all words in the documents, instead of
only the words in the headings, recall values im-

prove, but for some languages, e.g. German, to
take into account all the words does not have a
positive influence in general. Regarding the sub-
mission with priority 4 (T1 LEXSEM AW), the in-
clusion of lexical-semantic knowledge has been
beneficial for the English results, but not for the
other languages. This may be due to the type of
semantic knowledge that is being used. WordNet
for English is much bigger in size than for Ger-
man and Spanish, and therefore, this could influ-
ence the results, not obtaining the expected im-
provements that were expected by using language-
dependent resources. Generally speaking, from
our approaches, apart from the previously men-
tioned findings, we can also observe that when
summarizing Wikipedia articles, generic summa-
rization has been shown to be more appropriate.

Analyzing all the results achieved by the other
participants, we can observe that German is the
language, among the three analyzed languages
within our scope, that obtains poorer ROUGE re-
sults. This could occur since the summaries had
a compression ratio lower than 3%, which is a

255



System R1 recall R1 precision R1 F-measure
UA-DLSI-de-1 0.33993 0.34401 0.34110
UA-DLSI-de-2 0.33207 0.34331 0.33725
UA-DLSI-de-3 0.36126 0.36448 0.36236
UA-DLSI-de-4 0.33492 0.35565 0.34317
UA-DLSI-de-5 0.33023 0.33927 0.33437
UA-DLSI-de-6 0.34401 0.34807 0.34553
CCS-de-1 0.40140 0.36441 0.38163
CCS-de-2 0.40025 0.36601 0.38203
CCS-de-3 0.40257 0.37118 0.38575
CCS-de-4 0.40587 0.37234 0.38803
CCS-de-5 0.39356 0.38055 0.38665
EXB-de-1 0.37909 0.35621 0.36692
LCS-IESI-de-1 0.34844 0.36285 0.35504
Lead-de-1 0.33010 0.31562 0.32230
Oracles-de-1 0.54342 0.51331 0.52759

Table 3: Average results for German (recall, precision and F-measure ROUGE 1 (R1) values.

very low compression ratio for the summarization
task. Moreover, it can be seen from the tables, that
all systems overperformed the “Lead” baseline,
but none of them surpassed the “Oracles” sys-
tem. This was expected since the “Oracles” sys-
tem was kind of upper boundary for the MSS task.
Among the systems, the best performing ones tak-
ing into account the ROUGE 1 F-measure value
were: the BGU-SCE team with their submission
BGU-SCE-M-en-5 for English; CCS team, with
CCS-de-4 for German; and again CCS team with
CCS-es-3 for Spanish. Taking into account the dif-
ferent submissions, our versions were not among
the best performing approaches, despite obtaining
results in line of the other participants. In gen-
eral, there were not very big differences in re-
sults between the teams. In this sense, according
to ROUGE 1 F-measure, we ranked7 15th out of
22nd for English with our UA-DLSI-en-4 submis-
sion; 7th out of 13th for German with our UA-
DLSI-de-3 submission; and 8th out of 13th with
our UA-DLSI-es-1 submission. As it was previ-
ously discussed, for German and Spanish, the best
submissions were the ones without using any type
of lexical-semantic knowledge, whereas for En-
glish the use of a named entity recognizer, and a
semantic knowledge base led to an improvement
over the language-independent approach.

7The two systems provided by the organization has not
been taken into account for the ranking.

6 Potentials and Limitations of the
UA-DLSI Approach

From our participation in MultiLing 2015, we
have tested our approach in a real competition and
compared its performance with respect to state-
of-the-art multilingual summarizers. Although in
general terms, the best versions of our approach
ranked at intermediate positions, the participation
and evaluation process has been a positive issue
for learning from errors, as well as gaining some
insights into potentials and limitations that our ap-
proach and in general the multilingual summariza-
tion task may have.

After analyzing the performance of the different
system configurations, it becomes clear that some
of our assumptions need to be reviewed. Neverthe-
less, good positions were achieved when reducing
the words to compute the PCA algorithm, which
let us infer that article section headings contain
enough information to produce accurate and pre-
cise summaries, while decreasing the amount of
information to be processed by the system. More-
over, our results indicate that using PCA present
advantages when language independent process-
ing is required.

On the other hand, the limitations encountered
are mostly related to inclusion of lexical-semantic
knowledge. As it requires the use of external re-
sources, the system performance becomes depen-
dent of some aspects such as their quality, avail-
ability and size. The version of the system tak-

256



System R1 recall R1 precision R1 F-measure
UA-DLSI-es-1 0.48273 0.49799 0.48977
UA-DLSI-es-2 0.46191 0.48250 0.47141
UA-DLSI-es-3 0.45203 0.50965 0.46979
UA-DLSI-es-4 0.47795 0.49211 0.48454
UA-DLSI-es-5 0.46748 0.48820 0.47691
UA-DLSI-es-6 0.46657 0.47827 0.47193
CCS-es-1 0.52817 0.50834 0.51783
CCS-es-2 0.53135 0.51065 0.52057
CCS-es-3 0.52430 0.50440 0.51388
CCS-es-4 0.53234 0.51121 0.52126
CCS-es-5 0.52410 0.51321 0.51835
EXB-es-1 0.53018 0.49760 0.51310
LCS-IESI-es-1 0.50057 0.50575 0.50213
Lead-es-1 0.46826 0.46419 0.46599
Oracles-es-1 0.62557 0.60875 0.61691

Table 4: Average results for Spanish (recall, precision and F-measure ROUGE 1 (R1) values.

ing into account this kind of background obtains
better results in English language, for which re-
sources as WordNet have reached a state of ma-
turity higher than for other languages. In addi-
tion, and regarding the format of the source doc-
uments (Wikipedia articles), topic-focused sum-
maries have been shown to be less adequate than
generic summarization.

Concerning the multilingual summarization
task from a broader perspective, it is worth stress-
ing that this is a challenging task. On the one hand,
language-independent methods exist, and they of-
fer more capabilities to be employed for a wide
range of languages; however, this type of tech-
niques do not take into account any semantic anal-
ysis, so it is difficult that only with these tech-
niques, abstractive summaries can be produced,
thus limiting mostly to extractive summarization.

In the context of the MSS task, the summary
compression ratio was extremely low, compared to
the length of the original documents. This posed
the task even more challenging, since the gener-
ated summaries had to be very concise as well as
precise. Nevertheless, it is of great value to or-
ganize this type of events and have the possibility
to participate in order to advance the state of the
art, addressing difficult summarization challenges
necessary in the current society.

7 Conclusions

In this paper we described our participation in
MultiLing 2015 - Multilingual Single-document

Summarization task, presenting our approach and
comparing and discussing the results obtained
with respect to the other participants in the task.

Our initial development was focused on the ap-
plication of the PCA technique, given its suit-
ability for developing language-independent ap-
proaches. Although some related work has been
done on summarization, we contributed to the
state of the art extending the PCA scope by the
inclusion of lexical and semantic knowledge in its
implementation and testing it in a multilingual sce-
nario.

Our approach was tested in three languages, En-
glish, German, and Spanish, and six different con-
figurations were submitted to the competition, ob-
taining average results when compared to other
participants.

From our participation in MultiLing 2015, and
the further analysis of our PCA based approach
given the results obtained, three main conclusions
can be drawn: i) PCA is a good technique for
generating language-independent summaries; ii)
generic summaries were more appropriate for the
type of documents dealt with (i.e., Wikipedia doc-
uments); and iii) the title headings of Wikipedia
articles were meaningful enough to build the PCA
matrix in the summarization process, discarding
the remaining words of the document. Although
this version of our approach worked with very few
content, it was shown to be one of our best per-
forming approaches.

257



Acknowledgments

This research work has been partially funded by
the University of Alicante, Generalitat Valenciana,
Spanish Government and the European Commis-
sion through the projects, “Tratamiento inteligente
de la información para la ayuda a la toma de deci-
siones” (GRE12-44), “Explotación y tratamiento
de la información disponible en Internet para
la anotación y generación de textos adapta-
dos al usuario” (GRE13-15), DIIM2.0 (PROME-
TEOII/2014/001), ATTOS (TIN2012-38536-C03-
03), LEGOLANG-UAGE (TIN2012-31224), and
SAM(FP7-611312).

References
Daniel Anechitei and Eugen Ignat, 2013. Proceed-

ings of the MultiLing 2013 Workshop on Multilin-
gual Multi-document Summarization, chapter Mul-
tilingual summarization system based on analyzing
the discourse structure at MultiLing 2013, pages 72–
76. Association for Computational Linguistics.

Erik Cambria and Bebo White. 2014. Jumping nlp
curves: A review of natural language processing re-
search. IEEE Computational Intelligence Magazine,
9(2):48–57.

John M. Conroy, Judith D. Schlesinger, and Jeff Ku-
bina. 2011. CLASSY 2011 at TAC: Guided and
Multi-lingual Summaries and Evaluation Metrics.
In Proceedings of the Text Analysis Conference (TAC
2011).

John Conroy, T. Sashka Davis, Jeff Kubina, Yi-Kai
Liu, P. Dianne O’Leary, and D. Judith Schlesinger,
2013. Proceedings of the MultiLing 2013 Work-
shop on Multilingual Multi-document Summariza-
tion, chapter Multilingual Summarization: Dimen-
sionality Reduction and a Step Towards Optimal
Term Coverage, pages 55–63. Association for Com-
putational Linguistics.

Mahmoud El-Haj and Paul Rayson, 2013. Proceed-
ings of the MultiLing 2013 Workshop on Multilin-
gual Multi-document Summarization, chapter Using
a Keyness Metric for Single and Multi Document
Summarisation, pages 64–71. Association for Com-
putational Linguistics.

Enrique Estellés Arolas, Fernando González Ladrón
De Guevara, and Antonio Falcó Montesinos. 2010.
Principal Component Analysis for Automatic Tag
Suggestion. Technical report.

George Giannakopoulos, Mahmoud El-Haj, Benoit
Favre, Litvak Marina, Josef Steinberger, and Vadu-
seva Varma. 2011. TAC2011 MultiLing Pilot
Overview. In Proceedings of the Text Analysis Con-
ference (TAC 2011).

George Giannakopoulos. 2013. Multi-document mul-
tilingual summarization and evaluation tracks in acl
2013 multiling workshop. In Proceedings of the
MultiLing 2013 Workshop on Multilingual Multi-
document Summarization, pages 20–28, Sofia, Bul-
garia, August. Association for Computational Lin-
guistics.

Jeff Kubina, John M Conroy, and Judith D Schlesinger.
2013. Acl 2013 multiling pilot overview. Pro-
ceedings of MultiLing 2013 Workshop on Multi-
lingual Multi-document Summarization, Sofia, Bul-
garia, pages 29–38.

Chang Beom Lee, Min Soo Kim, and Hyuk Ro Park.
2003. Automatic Summarization Based on Principal
Component Analysis. Progress in Artificial Intelli-
gence, pages 409–413.

Chang B. Lee, Hyukro Park, and Cheolyoung Ock.
2005. Significant Sentence Extraction by Euclidean
Distance Based on Singular Value Decomposition.
In Proceedings of the Natural Language Processing-
IJCNLP 2005, pages 636–645.

Chin-Yew Lin. 2004. Rouge: A package for automatic
evaluation of summaries. In Marie-Francine Moens,
S. S., editor, Text Summarization Branches Out: Pro-
ceedings of the ACL-04 Workshop, pages 74–81.

Marina Litvak and Mark Last. 2013. Multilingual
single-document summarization with muse. Multi-
Ling 2013, page 77.

Elena Lloret and Manuel Palomar. 2011. Finding
the Best Approach for Multi-lingual Text Summari-
sation: A Comparative Analysis. In International
Conference Recent Advances in Natural Language
Processing, pages 194–201.

George A Miller. 1995. WordNet: A Lexical
Database for English. Communications of the ACM,
38(11):39–41.

Karen Sparck-Jones. 1999. Automatic summarising :
factors and directions. Advances in automatic text
summarisation, pages 1–21.

Josef Steinberger, Mijail Kabadjov, Ralf Steinberger,
Hristo Tanev, Marco Turchi, and Vanni Zaravella.
2011. JRC’s Participation at TAC 2011: Guided and
MultiLingual Summarization Tasks. In Proceedings
of the Text Analysis Conference (TAC 2011).

Josef Steinberger. 2013. The uwb summariser at
multiling-2013. In Proceedings of the MultiL-
ing 2013 Workshop on Multilingual Multi-document
Summarization, pages 50–54, Sofia, Bulgaria, Au-
gust. Association for Computational Linguistics.

Erik Tjong, Kim Sang, and Fien De Meulder. 2003.
Introduction to the CoNLL-2003 Shared Task:
Language-Independent Named Entity Recognition.
In 7th conference on Natural language learning at
HLT-NAACL 2003, volume 4, pages 142–147.

258



Om Vikas, Akhil K Meshram, Girraj Meena, and Amit
Gupta. 2008. Multiple Document Summariza-
tion Using Principal Component Analysis Incorpo-
rating Semantic Vector Space Model. Computa-
tional Linguistics and Chinese Language Process-
ing, 13(2):141–156.

Piek Vossen. 2004. Eurowordnet: A multilin-
gual database of autonomous and language-specific
wordnets connected via an inter-lingual index. In-
ternational Journal of Lexicography Vol.17, 2:161–
173.

259


