



















































Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics


Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 554–559
Vancouver, Canada, July 30 - August 4, 2017. c©2017 Association for Computational Linguistics

https://doi.org/10.18653/v1/P17-2088

Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 554–559
Vancouver, Canada, July 30 - August 4, 2017. c©2017 Association for Computational Linguistics

https://doi.org/10.18653/v1/P17-2088

On the Equivalence of Holographic and Complex
Embeddings for Link Prediction

Katsuhiko Hayashi
NTT Communication Science Laboratories, Seika-cho, Kyoto 619 0237, Japan

hayashi.katsuhiko@lab.ntt.co.jp

Masashi Shimbo
Nara Institute of Science and Technology, Ikoma, Nara 630 0192, Japan

shimbo@is.naist.jp

Abstract

We show the equivalence of two state-
of-the-art models for link prediction/
knowledge graph completion: Nickel et
al’s holographic embeddings and Trouil-
lon et al.’s complex embeddings. We first
consider a spectral version of the holo-
graphic embeddings, exploiting the fre-
quency domain in the Fourier transform
for efficient computation. The analysis of
the resulting model reveals that it can be
viewed as an instance of the complex em-
beddings with a certain constraint imposed
on the initial vectors upon training. Con-
versely, any set of complex embeddings
can be converted to a set of equivalent
holographic embeddings.

1 Introduction

Recently, there have been efforts to build and
maintain large-scale knowledge bases represented
in the form of a graph (knowledge graph) (Auer
et al., 2007; Bollacker et al., 2008; Suchanek et al.,
2007). Although these knowledge graphs con-
tain billions of relational facts, they are known
to be incomplete. Knowledge graph completion
(KGC) (Nickel et al., 2015) aims at augmenting
missing knowledge in an incomplete knowledge
graph automatically. It can be viewed as a task
of link prediction (Liben-Nowell and Kleinberg,
2003; Hasan and Zaki, 2011) studied in the field of
statistical relational learning (Getoor and Taskar,
2007). In recent years, methods based on vec-
tor embeddings of graphs have been actively pur-
sued as a scalable approach to KGC (Bordes et al.,
2011; Socher et al., 2013; Guu et al., 2015; Yang
et al., 2015; Nickel et al., 2016; Trouillon et al.,
2016b).

In this paper, we investigate the connection

between two models of graph embeddings that
have emerged along this line of research: The
holographic embeddings (Nickel et al., 2016) and
the complex embeddings (Trouillon et al., 2016b).
These models are simple yet achieve the current
state-of-the-art performance in KGC.

We begin by showing that holographic embed-
dings can be trained entirely in the frequency do-
main induced by the Fourier transform, thereby
reducing the time needed to compute the scoring
function from O(n log n) to O(n), where n is the
dimension of the embeddings.

The analysis of the resulting training method
reveals that the Fourier transform of holographic
embeddings can be regarded as an instance of the
complex embeddings, with a specific constraint
(viz. conjugate symmetry property) cast on on the
initial values.

Conversely, we also show that every set of com-
plex embeddings has a set of holographic embed-
dings (with real vectors) that is equivalent, in the
sense that their scoring functions are equal up to
scaling.

2 Preliminaries

Let i denote the imaginary unit, R be the set of
real values, and C the set of complex values. We
write [v] j to denote the jth component of vector v.
A superscript T (e.g., vT) represents vector/matrix
transpose. For a complex scalar z, vector z, and
matrix Z, z, z, and Z represent their complex con-
jugate, and Re(z), Re(z), and Re(Z) denote their
real parts, respectively.

Let x = [x0 · · · xn−1]T ∈ Rn and y =
[y0 · · · yn−1]T ∈ Rn. Note that the vector indices
start from 0 for notational convenience. The cir-
cular convolution of x and y, denoted by x ∗ y, is

554

https://doi.org/10.18653/v1/P17-2088
https://doi.org/10.18653/v1/P17-2088


defined by

[x ∗ y] j =
n−1∑

k=0

x[( j−k) mod n]yk, (1)

where mod denotes modulus. Likewise, circular
correlation x ? y is defined by

[x ? y] j =
n−1∑

k=0

x[(k− j) mod n]yk. (2)

While circular convolution is commutative, cir-
cular correlation is not; i.e., x ∗ y = y ∗ x, but
x ? y , y ? x in general. As it can be verified
with Eqs. (1) and (2), x ? y = flip(x) ∗ y, where
flip(x) = [xn−1 · · · x0]T is a vector obtained by
arranging the components of x in reverse.

For n-dimensional vectors, naively computing
circular convolution/correlation by Eqs. (1) and
(2) requires O(n2) multiplications. However, we
can take advantage of the Fast Fourier Transform
(FFT) algorithm to accelerate the computation:
For circular convolution, first compute the discrete
Fourier transform (DFT) of x and y, and then com-
pute the inverse DFT of their elementwise vector
product, i.e.,

x ∗ y = F−1(F(x) � F(y)),

where F : Rn → Cn and F−1 : Cn → Rn re-
spectively denote the DFT and inverse DFT, and
� denotes the elementwise product. DFT and in-
verse DFT can be computed in O(n log n) time
with the FFT algorithm, and thus the computation
time for circular convolution is also O(n log n).
The same can be said of circular correlation. Since
F(flip(x)) = F(x), we have

x ? y = F−1(F(x) � F(y)). (3)

By analogy to how the Fourier transform is used
in signal processing, the original real space Rn is
called the “time” domain, and the complex space
Cn where DFT vectors reside is called the “fre-
quency” domain.

3 Holographic embeddings for
knowledge graph completion

3.1 Knowledge graph completion
Let E and R be the finite sets of entities and (bi-
nary) relations over entities, respectively. For each
relation r ∈ R and each pair s, o ∈ E of entities,

Table 1: Correspondence between operations in
time and frequency domains. r ↔ ρ indicates
ρ = F(r) (and also r = F−1(ρ)).

operation time frequency

scalar mult. αx ←→ αF(x)
summation x + y ←→ F(x) + F(y)

flip flip(x) ←→ F(x)
convolution x ∗ y ←→ F(x) � F(y)
correlation x ? y ←→ F(x) � F(y)

dot product x · y = 1n F(x) · F(y)

we are interested in whether r(s, o) holds1 or not;
we write r(s, o) = +1 if it holds, and r(s, o) = −1
if not. To be precise, given a training set D =
R × E × E × {−1,+1} such that (r, s, o, y) ∈ D
indicates y = r(s, o), our task is to design a scor-
ing function f : R × E × E → R such that for
each of the triples (r, s, o) not observed in D, func-
tion f should give a higher value if r(s, o) = +1 is
more likely, and a smaller value for those that are
less likely. If necessary, f (r, s, o) can be converted
to probability by P[r(s, o) = +1] = σ( f (r, s, o)),
where σ : R→ (0, 1) is a sigmoid function.

Dataset D can be regarded as a directed graph in
which nodes represent entities E and edges are la-
beled by relations R. Thus, the task is essentially
that of link prediction (Liben-Nowell and Klein-
berg, 2003; Hasan and Zaki, 2011). Often, it is
also called knowledge graph completion.

3.2 Holographic embeddings (HolE)
Nickel et al. (2016) proposed holographic em-
beddings (HolE) for knowledge graph completion.
Using training data D, this method learns the vec-
tor embeddings ek ∈ Rn of entities k ∈ E and the
embeddings wr ∈ Rn of relations r ∈ R. The score
for triple (r, s, o) is then given by

fHolE(r, s, o) = wr · (es ? eo). (4)

Eq. (4) can be evaluated in time O(n log n) if es?eo
is computed by Eq. (3).

4 Spectral training of HolE

To compute the circular correlation in the scoring
function of Eq. (4) efficiently, Nickel et al. (2016)
used Eq. (3) in Section 2 and FFT. In this sec-
tion, we extend this technique further, and con-

1Depending on the context, letter r is used either as an
index to an element in R or the binary relation it signifies.

555



sider training HolE solely in the frequency do-
main. That is, real-valued embeddings ek,wr ∈
Rn in the original “time” domain are abolished,
and instead we train their DFT counterparts εk =
F(ek) ∈ Cn and ωk = F(wr) ∈ Cn in the frequency
domain. This formulation eliminates the need of
FFT and inverse FFT, which are the major com-
putational bottleneck in HolE. As a result, Eq. (4)
can be computed in time O(n) directly from εk and
ωk.

Indeed, equivalent counterparts in the frequency
domain exist for not only convolution/correlation
but all other computations needed for HolE: scalar
multiplication, summation (needed when vectors
are updated by stochastic gradient descent), and
dot product (used in Eq. (4)). The frequency-
domain equivalents for these operations are sum-
marized in Table 1. All of these can be performed
efficiently (in linear time) in the frequency do-
main.

In particular, the following relation holds for the
dot product between any “time” vectors x, y ∈ Rn.

x · y = 1
n
F(x) · F(y), (5)

where the dot product on the right-hand side is the
complex inner product defined by a · b = aTb.
Eq. (5) is known as Parseval’s theorem (also called
the power theorem in (Smith, 2007)), and it states
that dot products in two domains are equal up to
scaling.

After embeddings εk,ωr ∈ Cn are learned in
the frequency domain, their time-domain coun-
terparts ek = F−1(εk) and wr = F−1(ωr) can be
recovered if needed, but this is not required as
far as computation of the scoring function is con-
cerned. Thanks to Parseval’s theorem, Eq. (4) can
be directly computed from the frequency vectors
εk,ωr ∈ Cn by

fHolE(r, s, o) =
1
n
ωr · (εs � εo). (6)

4.1 Conjugate symmetry of spectral
components

A complex vector ξ = [ξ0 · · · ξn−1]T ∈ Cn is
said to be conjugate symmetric (or Hermitian) if
ξ j = ξ[(n− j) mod n] for j = 0, . . . , n − 1, or, in other
words, if it can be written in the form

ξ =



[
ξ0 γ flip(γ)

]T
, if n is odd,

[
ξ0 γ ξn/2 flip(γ)

]T
, if n is even,

for some γ ∈ Cdn/2e−1 and ξ0, ξn/2 ∈ R.
The DFT F(x) is conjugate symmetric if and

only if x is a real vector. Thus, maintaining con-
jugate symmetry of “frequency” vectors is the key
to ensure their “time” counterparts remain in real
space. Below, we verify that this property is in-
deed preserved with stochastic gradient descent.
Moreover, conjugate symmetry provides a suffi-
cient condition under which dot product takes a
real value. It also has implications on space re-
quirement. These topics are covered in the rest of
this section.

4.2 Vector initialization and update in
frequency domain

Typically, at the beginning of training HolE, each
individual embedding is initialized by a random
vector. When we train HolE in the frequency do-
main, we could first generate a random real vector,
regard them as a HolE vector in the time domain,
and compute its DFT as the initial value in the fre-
quency domain. An alternative, easier approach
is to directly generate a random complex vector
that is conjugate symmetric, and use it as the ini-
tial frequency vector. This guarantees the inverse
DFT to be a real vector, i.e., there exists a valid
corresponding image in the time domain.

Given a training set D (see Section 3.1),
HolE is trained by minimizing the following
objective function over parameter matrix Θ =
[e1 · · · e|E | w1 · · ·w|R|] ∈ Rn×(|E |+|R|):

∑

(r,s,o,y)∈D
log{1+exp(−y fHolE(r, s, o))}+λ||Θ||2F (7)

where λ ∈ R is the hyperparameter controlling
the degree of regularization, and ‖ · ‖F denotes the
Frobenius norm.

In our version of spectral training of HolE,
the parameters matrix consists of frequency vec-
tors εk and ωr instead of ek and wr, i.e., Θ =
[ε1 · · · ε|E | ω1 · · ·ω|R|] ∈ Cn×(|E |+|R|). Let us dis-
cuss the stochastic gradient descent (SGD) update
with respect to these frequency vectors. In partic-
ular, we are interested in whether conjugate sym-
metry of vectors is preserved by the update.

Suppose vectors ωr, εs, εo are conjugate sym-
metric. Neglecting the contribution from the
regularization term2 in Eq. (7), we see that in

2It can be easily verified that the contribution from the
regularization term to SGD update do not violate conjugate
symmetry.

556



an SGD update step, α∂ fHoLE/∂ωr, α∂ fHoLE/∂εs,
and α∂ fHoLE/∂εo are respectively subtracted from
ωr, εs, εo, where α ∈ R is a factor not depending
on these parameters. Noting the equalities

wr · (es ? eo) = es · (wr ? eo) = eo · (wr ∗ es)
(see (Nickel et al., 2016, Eq. (12), p. 1958)) and
their frequency counterparts

ωr · (εs � εo) = εs · (ωr � εo) = εo · (ωr � εs),
obtained through the translation of Table 1, we can
derive

∂ fHolE
∂ωr

= εs � εo,
∂ fHolE
∂εs

= ωr � εo,
∂ fHolE
∂εo

= ωr � εs.

As seen from above, conjugation, scalar multipli-
cation, summation, and elementwise product are
used in the SGD update. And it is straightforward
to verify that all these operations preserve con-
jugate symmetry. It follows that if ωr, εs, εo are
initially conjugate symmetric, they will remain so
during the course of training, which assures that
the inverse DFTs of the learned embeddings are
real vectors.

4.3 Real-valued dot product
In the scoring function of HolE (Eq. (4)), dot prod-
uct is used for generating a real-valued “score”
out of two vectors, wr and es � eo. Likewise,
in Eq. (6), the dot product is applied to ωr and
εs � εo, which are complex-valued. However, pro-
vided that the conjugate symmetry of these vec-
tors is maintained, their dot product is always real.
This follows from Parseval’s theorem; the inverse
DFTs of these frequency vectors are real, and thus
their dot product is also real. Therefore, the dot
product of the corresponding frequency vectors is
real as well, according to Eq. (5).

4.4 Space requirement
A general complex vector ξ ∈ Cn can be stored
in memory as 2n floating-point numbers, i.e., one
each for the real and imaginary part of a compo-
nent. In our spectral representation of HolE, how-
ever, the first bn/2c components suffice to specify
the frequency vector ξ, since the vector is con-
jugate symmetric. Moreover, ξ0 (and ξn/2 if n

is even) are real values. Thus, a spectral repre-
sentation of HolE can be specified with exactly n
floating-point numbers, which can be stored in the
same amount of memory as needed by the original
HolE.

5 Relation to Trouillon et al.’s complex
embeddings

5.1 Complex embeddings (ComplEx)
Trouillon et al. (2016b) proposed a model of
embedding-based knowledge graph completion,
called complex embeddings (ComplEx). The ob-
jective is similar to Nickel et al.’s; the embed-
dings ek of entities and wr of relations are to be
learned. In their model, however, these vectors
are complex-valued, and are based on the eigen-
decomposition of complex matrix Xr = EWrE

T

that encodes relation r ∈ R over pairs of entities,
where Xr ∈ C|E |×|E |, E = [e1, . . . , e|E |]T ∈ C|E |×n,
and Wr = diag(wr) ∈ Cn×n is a diagonal matrix
(with diagonal elements wr ∈ Cn). In practice,
Xr needs to be a real matrix, because its (r, s)-
component must define the score for r(s, o). To
this end, Trouillon et al. simply extracted the
real part; i.e., Xr = Re(EWrE

T
). Trouillon et al.

(2016a) advocated this approach, by showing that
any real matrix Xr can be expressed in this form.

With this formulation, the score for triple
(r, s, o) is given by

fComplEx(r, s, o) = Re


n−1∑

j=0

[wr] j[es] j[eo] j

 . (8)

5.2 Equivalence of holographic and complex
embeddings

Now let us rewrite Eq. (8). Noting the definition
of complex dot product, i.e., a · b = aTb, we have

n−1∑

j=0

[wr] j[es] j[eo] j = (es � eo)Twr

= (es � eo) · wr (∵ a · b = aTb)
= (es � eo) · wr
= wr · (es � eo) (∵ a · b = b · a)

and since Re(z) = Re(z),

Re(wr · (es � eo)) = Re(wr · (es � eo)).
Thus, Eq. (8) can be written as

fComplEx(r, s, o) = Re
(
wr · (es � eo)) . (9)

557



Here, a marked similarity is noticeable between
Eq. (9) and Eq. (6), the scoring function of our
spectral version of HolE (spectral HolE); Com-
plEx extracts the real part of complex dot prod-
uct, whereas in the spectral HolE, dot product is
guaranteed to be real because all embeddings sat-
isfy conjugate symmetry. Indeed, Eq. (6) can be
equally written as

fHolE(r, s, o) =
1
n

Re
(
ωr · (εs � εo)) . (10)

although the operator Re(·) in this formula is re-
dundant, since the inner product is guaranteed to
be real-valued. Nevertheless, Eq. (10) elucidates
the fact that the spectral HolE can be viewed as an
instance of ComplEx, with the embeddings con-
strained to be conjugate symmetric to make the
inner product in Eq. (10) real-valued.

Conversely, given a set of complex embeddings
for entities and relations, we can construct their
equivalent holographic embeddings, in the sense
that fComplEx(r, s, o) = c fHolE(r, s, o) for every
r, s, o, where c > 0 is a constant. For each n-
dimensional complex embeddings x ∈ {ek}k∈E ∪
{wr}r∈R ⊂ Cn computed by ComplEx, we make a
corresponding HolE h(x) ∈ R2n+1 as follows: For a
given complex embedding x = [x0 · · · xn−1] ∈ Cn,
first compute s(x) ∈ C2n+1 by

s(x) =
[
0 x0 · · · xn−1 xn−1 · · · x0

]T

=
[
0 x flip(x)

]T
(11)

and then define h(x) = F−1(s(x)). Since s(x) is
conjugate symmetric, h(x) is a real vector.

To verify if this conversion defines an equiva-
lent scoring function for any triple (r, s, o), let us
now suppose complex embeddings wr ∈ Cn and
es, eo ∈ Cn are given. Since we regard real vec-
tors h(wr), h(es), h(eo) ∈ R2n+1 as the holographic
embeddings of r, s and o, respectively, the HolE
score for the triple (r, s, o) is given as

fHolE(r, s, o)

= h(wr) · (h(es) ? h(eo))
=

1
n
s(wr) · (s(es) � s(eo)) (∵ Eq. (6))

=
1
n
s(wr) ·

[
0 es � eo flip(es � eo)

]T
(∵ Eq. (11))

=
1
n

[
0 wr flip(wr)

]T ·
[
0 es � eo flip(es � eo)

]T

=
1
n

(
wr · (es � eo) + flip(wr) · flip(es � eo)

)

=
1
n

(
wr · (es � eo) + wr · (es � eo)

)

=
1
n

(
wr · (es � eo) + wr · (es � eo)

)

=
2
n

Re
(
wr · (es � eo))

=
2
n

fComplEx(r, s, o),

which shows that h(·) (or s(·)) gives the desired
conversion from ComplEx to HolE.

6 Conclusion

In this paper, we have shown that the holographic
embeddings (HolE) can be trained entirely in the
frequency domain. If stochastic gradient descent
is used for training, the conjugate symmetry of
frequency vectors is preserved, which ensures the
existence of the corresponding holographic em-
bedding in the original real space (time domain).
Also, this training method eliminates the need of
FFT and inverse FFT, thereby reducing the compu-
tation time of the scoring function from O(n log n)
to O(n).

Moreover, we have established the equivalence
of HolE and the complex embeddings (ComplEx):
The spectral version of HolE is subsumed by Com-
plEx as a special case in which conjugate symme-
try is imposed on the embeddings. Conversely, ev-
ery set of complex embeddings can be converted
to equivalent holographic embeddings.

Many systems for natural language process-
ing, such as those for semantic parsing and ques-
tion answering, benefit from access to information
stored in knowledge graphs. We plan to further in-
vestigate the property of spectral HolE and Com-
plEx in these applications.

Acknowledgments

We thank the anonymous reviewers for helpful
comments. This work was partially supported by
JSPS Kakenhi Grants 26730126 and 15H02749.

References
Sören Auer, Christian Bizer, Georgi Kobilarov, Jens

Lehmann, Richard Cyganiak, and Zachary Ives.
2007. DBpedia: A nucleus for a web of open data.
In The Semantic Web: Proceedings of the 6th In-
ternational Semantic Web Conference and the 2nd
Asian Semantic Web Conference (ISWC ’07/ASWC
’07). Springer, Lecture Notes in Computer Sci-
ence 4825, pages 722–735.

558



Kurt Bollacker, Colin Evans, Praveen Paritosh, Tim
Sturge, and Jamie Taylor. 2008. Freebase: A collab-
oratively created graph database for structuring hu-
man knowledge. In Proceedings of the 2008 ACM
SIGMOD International Conference on Management
of Data (SIGMOD ’08). pages 1247–1250.

Antoine Bordes, Jason Weston, Ronan Collobert, and
Yoshua Bengio. 2011. Learning structured em-
beddings of knowledge bases. In Proceedings of
the 25th AAAI Conference on Artificial Intelligence
(AAAI ’11). pages 301–306.

Lise Getoor and Ben Taskar. 2007. Introduction to Sta-
tistical Relational Learning. Adaptive Computation
and Machine Learning. The MIT Press.

Kelvin Guu, John Miller, and Percy Liang. 2015.
Traversing knowledge graphs in vector space. In
Proceedings of the 2015 Conference on Empirical
Methods in Natural Language Processing (EMNLP
’15). pages 318–327.

Mohammad Al Hasan and Mohammed J. Zaki. 2011.
A survey of link prediction in social networks. In
Charu C. Aggarwal, editor, Social Network Data An-
alytics, Springer, chapter 9, pages 243–275.

David Liben-Nowell and Jon Kleinberg. 2003. The
link prediction problem for social networks. In
Proceedings of the 12nd Annual ACM International
Conference on Information and Knowledge Man-
agement (CIKM ’03). pages 556–559.

Maximilian Nickel, Kevin Murphy, Volker Tresp, and
Evgeniy Gabrilovich. 2015. A review of relational
machine learning for knowledge graphs. Proceed-
ings of the IEEE pages 1–18.

Maximilian Nickel, Lorenzo Rosasco, and Tomaso
Poggio. 2016. Holographic embeddings of knowl-
edge graphs. In Proceedings of the 30th AAAI Con-
ference on Artificial Intelligence (AAAI ’16). pages
1955–1961.

III Julius O. Smith. 2007. Mathematics of the Discrete
Fourier Transform (DFT): with Audio Applications.
W3K Publishing, 2nd edition.

Richard Socher, Danqi Chen, Christopher D. Manning,
and Andrew Y. Ng. 2013. Reasoning with neural
tensor networks for knowledge base completion. In
Advances in Neural Information Processing Systems
26 (NIPS ’13). pages 926–934.

Fabian M. Suchanek, Gjergji Kasneci, and Gerhard
Weikum. 2007. YAGO: A core of semantic knowl-
edge unifying Wordnet and Wikipedia. In Proceed-
ings of the 16th International Conference on World
Wide Web (WWW ’07). pages 697–706.

Théo Trouillon, Christopher R. Dance, Éric Gaussier,
and Guillaume Bouchard. 2016a. Decomposing
real square matrices via unitary diagonalization.
arXiv.math eprint 1605.07103, arXiv.

Théo Trouillon, Johannes Welbl, Sebastian Riedel, Éric
Gaussier, and Guillaume Bouchard. 2016b. Com-
plex embeddings for simple link prediction. In Pro-
ceedings of the 33rd International Conference on
Machine Learning (ICML ’16). pages 2071–2080.

Bishan Yang, Wen tau Yih, Xiaodong He, Jianfeng
Gao, and Li Deng. 2015. Embedding entities and
relations for learning and inference in knowledge
bases. In Proceedings of the 3rd International Con-
ference on Learning Representations (ICLR ’15).

559


	On the Equivalence of Holographic and Complex Embeddings for Link Prediction

