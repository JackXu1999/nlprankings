



















































Robust Representation Learning of Biomedical Names


Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 3275‚Äì3285
Florence, Italy, July 28 - August 2, 2019. c¬©2019 Association for Computational Linguistics

3275

Robust Representation Learning of Biomedical Names

Minh C. Phan Aixin Sun Yi Tay
Nanyang Technological University, Singapore

phan0050@e.ntu.edu.sg; axsun@ntu.edu.sg; ytay017@e.ntu.edu.sg

Abstract

Biomedical concepts are often mentioned in
medical documents under different name vari-
ations (synonyms). This mismatch between
surface forms is problematic, resulting in dif-
ficulties pertaining to learning effective rep-
resentations. Consequently, this has tremen-
dous implications such as rendering down-
stream applications inefficacious and/or poten-
tially unreliable. This paper proposes a new
framework for learning robust representations
of biomedical names and terms. The idea
behind our approach is to consider and en-
code contextual meaning, conceptual mean-
ing, and the similarity between synonyms dur-
ing the representation learning process. Via
extensive experiments, we show that our pro-
posed method outperforms other baselines on
a battery of retrieval, similarity and relatedness
benchmarks. Moreover, our proposed method
is also able to compute meaningful representa-
tions for unseen names, resulting in high prac-
tical utility in real-world applications.

1 Introduction

Representation learning of words (Mikolov et al.,
2013; Pennington et al., 2014), and/or sen-
tences (Kiros et al., 2015; Hill et al., 2016; Lo-
geswaran and Lee, 2018) forms the bedrock of
many modern NLP applications. These tech-
niques, largely relying on context information,
have a huge impact on downstream applications.
To this end, learning effective and useful represen-
tations has been a highly fruitful area of research.

Biomedical names1, however, are different from
standard words and sentences. These names have
both contextual and conceptual meanings. Con-
textual meaning reflects the contexts where the
names appear, and it is specifically granted to each

1Biomedical names refer to surface forms that represent
biomedical concepts. They can be official names in biomedi-
cal vocabularies or unofficial names mentioned in text.

Concept (CUI) and their names Source

C0343047: leiner‚Äôs disease, complement
component 5 deficiency, c5d, complement
5 dysfunction, infantile seborrheic der-
matitis, erythroderma desquamativum.

UMLS

C0154832: coats‚Äô disease, abnormal reti-
nal vascular development, unilateral retinal
telangiectasis, coats telangiectasis

NCBI-
Disease

C0019168: hepatitis b virus surface anti-
gen, hepatitis-b surface antigen, hbs ag,
hbsag, hepatitis b surface antigen

BC5CDR-
Chemical

Table 1: Example of biomedical concepts and
their names taken from one vocabulary (UMLS (Li
et al., 2016)) and two annotated datasets (NCBI-
Disease (DogÃÜan et al., 2014) and BC5CDR-Chemical-
Chemical (Li et al., 2016)). The concepts are listed by
concept unique identifiers (CUI) defined in UMLS.

name. Names of a broad and popular concept of-
ten have slightly different contextual meanings.
On the other hand, conceptual meaning maps to
the definitions/contexts of the names‚Äô associated
concepts, i.e., CUIs as shown in Table 1. As such,
names of the same concepts share the common
conceptual meanings, although they can own dif-
ferent contextual information.

As illustrated in Table 1, biomedical concepts
appear in the text under various names. Repre-
sentations of the names are also expected to be
well clustered in their distributional space, i.e.,
names of the same concepts are close to each
other and distant from those of other concepts.
Learning such conceptually grounded representa-
tions is highly desired for a wide range of applica-
tions, e.g., synonym retrieval/discovery, biomedi-
cal name normalization, and query expansion.

For the first time, we investigate the problem
of biomedical name embedding. Our goal is to
derive meaningful and robust representations for
biomedical names from their surface forms. Un-
fortunately, this task is not trivial since two names



3276

can be strongly related but not necessarily belong
to the same concept (e.g., ‚Äòcomplement compo-
nent 5 deficiency‚Äô and ‚Äòcomplement component
5‚Äô). Furthermore, names of a concept can be
completely different regarding their surface forms
(e.g., ‚Äòleiner‚Äôs disease‚Äô and ‚Äòc5d‚Äô). As such, we es-
tablish the key desiderata for learning robust rep-
resentations. First, the output representations need
to be both conceptually and contextually meaning-
ful. Second, name representations that belong to
the same concepts should be similar to each other,
i.e., conceptual grounding.

To this end, our proposed encoding framework
incorporates three new objectives, namely context,
concept, and synonym-based objectives. We for-
mulate the representation learning process as a
synonym prediction task, with context and con-
ceptual losses acting as regularizers, preventing
two synonyms from collapsing into semantically
meaningless representations. As illustrated in Fig-
ure 1, synonym-based objective enforces simi-
lar representations between synonymous names,
while concept-based objective pulls the name‚Äôs
representations closer to its concept‚Äôs centroid. On
the other hand, context-based objective aims to
minimize the difference between the derived rep-
resentation and its specific contextual representa-
tion. More concretely, our approach adopts a re-
current sequence encoding model to extract the se-
mantics of biomedical names, and to learn the al-
ternative naming of biomedical concepts. Our ap-
proach does not need any additional annotations
on biomedical text. To be specific, we do not need
the biomedical names to be pre-annotated in the
text. Instead, we utilize available synonym sets in
a metathesaurus vocabulary (e.g., UMLS), as the
only additional resource for training.

Our main contributions in this work are sum-
marized as follows. For the first time, we investi-
gate the problem of biomedical name embedding
and its applications. We pay attention to the simi-
larity between semantically related names as well
as the names of the same concept. Furthermore,
we define and distinguish three aspects constitut-
ing to quality of biomedical name representations.
We propose a novel encoding framework that con-
siders all these aspects in the representation learn-
ing. Finally, we evaluate the proposed encoder
in biomedical synonym retrieval, name normal-
ization, and semantic similarity and relatedness
benchmarks. In most of these experiments, our

Context(ùíî)

Synonym s‚ÄôConcept(ùíî)

ùìõùíÖùíÜùíá

ùìõùíÑùíïùíô

ùìõsyn

s

Figure 1: Illustration of three aspects, which are asso-
ciated to three training objectives, for computing repre-
sentation of biomedical name s. Intuitively, the repre-
sentation is supposed to be similar to its synonym‚Äôs as
well as its conceptual and contextual representations.

model significantly outperforms other baselines.

2 Related Work

Our problem setting of name embedding is differ-
ent from recent works in biomedical word embed-
dings (Chiu et al., 2016; Wang et al., 2018) and
concept embeddings (Beam et al., 2018; Cai et al.,
2018). Our goal is to derive meaningful represen-
tation for a sequence of words that likely repre-
sents a concept. This setting is also orthogonal to
works that only focus on estimating the matching
between names (Li et al., 2017; Liu et al., 2018).

There are several options to encode variable-
length names/phrases into fixed-sized vector rep-
resentations. Existing approaches range from
phrase-level extensions of word embeddings,
compositions of pre-trained word representations
to sequence encoding neural networks.

Contextual Word Embeddings. We revisit
skip-gram model (Mikolov et al., 2013), as one
of the most popular context-based embedding ap-
proaches. The model computes the representa-
tions for both target word wt, and context word
wc by maximizing the following log-likelihood:

LW =
‚àë

wt,wc‚ààCwt

log p(wc|wt) (1)

The probability of observing wc in the local con-
text of wt is defined as follows:

p(wc|wt) =
exp(v·µÄwcuwt)‚àë

w‚ààW
exp(v·µÄwuwt)

where uw and vw are the ‚Äòinput‚Äô and ‚Äòoutput‚Äô vec-
tor representations of w. In this work, we refer to
the input representations as contextual representa-
tions of words, or in short, word embeddings.



3277

The skip-gram model is extensible to names (or
phrases) by treating them as special tokens:

LS =
‚àë

wt,wc‚ààCwt

log p(wc|wt)+
‚àë

s,wc‚ààCs

log p(wc|s)

(2)
where s is a special name token. Training of this
model results in word and name embeddings.

Average of Contextual Word Embeddings.
Another simple and effective method to compute
name embeddings is taking the average of their
constituent word embeddings. Since words in
a biomedical name are usually descriptive about
its meaning, this simple baseline is expected to
produce quality representations. FastText (Bo-
janowski et al., 2017) leverages this idea by
considering character n-grams instead of words.
Therefore, the model can derive representations
for names that contain unseen words. The effec-
tiveness of simple compositions such as taking av-
erage or power mean have also been verified in
phrase and sentence embeddings (Wieting et al.,
2016; Arora et al., 2017; RuÃàckleÃÅ et al., 2018).

Sequence Encoding Models. Sequence encod-
ing models aim to capture more sophisticated se-
mantics of character and word sequences. These
models range from multilayer feed-forward net-
works (Iyyer et al., 2015) to convolutional (Kalch-
brenner et al., 2014), recursive and recurrent neu-
ral networks (Socher et al., 2011; Tai et al., 2015).
They also differ by the types of supervision used in
training. Context-based sentence encoders (Kiros
et al., 2015; Hill et al., 2016; Logeswaran and
Lee, 2018) is based on distributional hypothesis.
The training utilizes sentences and their contexts
(surrounding sentences), which can be extracted
from an unlabeled corpus. Similar to contex-
tual word embeddings, the derived sentence em-
beddings are expected to carry the contextual in-
formation. However, this contextual information
does not fully reflect paraphrastic characteristic,
i.e., semantically similar sentences do not nec-
essarily have identical meanings. These embed-
dings, therefore, are not favorable in applications
that demand strong synonym identification. In
contrast, supervised or semi-supervised represen-
tation leaning requires annotated corpus, such as
paraphrastic sentences or natural language infer-
ence data (Conneau et al., 2017; Wieting and Gim-
pel, 2017; Clark et al., 2018; Subramanian et al.,
2018; Cer et al., 2018). However, most of these

works focus on learning representations for sen-
tences.

The closest work to our problem setting is (Wi-
eting et al., 2015). In this proposed model, the au-
thors utilize pairs of paraphrastic phrases as train-
ing data, e.g., ‚Äòdoes not exceed‚Äô and ‚Äòis no more
than‚Äô. To prevent the trained model from over-
fitting, authors introduce regularization terms that
applied on encoder‚Äôs parameters as well as the dif-
ference between the initial and trainable word em-
beddings. Their evaluation, however, only consid-
ers the paraphrastic similarity of phrases.

Discussion. Our proposed encoder is based on
BiLSTM (Graves and Schmidhuber, 2005), al-
though it can be replaced by another sequence en-
coding model as mentioned above. Our approach
utilizes synonym sets in UMLS to learn name rep-
resentations, while also enforces the learned rep-
resentation to be similar to their contextual and
conceptual representations. The idea is related to
word vector specialization (retrofitting) (Faruqui
et al., 2015; MrksÃåicÃÅ et al., 2017; VulicÃÅ et al., 2018).
The difference is that we focus on learning rep-
resentation for multi-word concept names, hence
the contextual and conceptual constraints are es-
sential, in addition to the synonymous similarity.
In contrast, most retrofitting approaches mainly
aim to improve word representations. These mod-
els map initial word embeddings into a new vec-
tor space that satisfy the synonymous similarity
desiderata, while also constrain the new represen-
tations to be similar to the initial ones. Since the
initial word representations can be assumed to en-
code both contextual and conceptual information
of the words, these retrofitting approaches can be
viewed as special cases of our proposed encoding
framework.

3 Biomedical Name Encoder

For ease of presentation, we use three generic
terms, uw, us and uc, to denote pre-trained
word, name and concept embeddings, respec-
tively. These embeddings will be used as inputs
in our encoding framework. Note that there are
several options to calculate these embeddings and
our encoder can be adapted to different calcula-
tion results. Before going to details, we present
an extension of skip-gram, which will serve as a
baseline. Furthermore, the outputs of this baseline
will be used as pre-trained embeddings in one of
the framework‚Äôs configurations.



3278

name ùë†

ùìõùíîùíöùíè

context ùë• concept ùëêname ùë†‚Ä≤

ùìõùíÑùíïùíô ùìõùíÖùíÜùíá

Bi-LSTM

ùëû(ùë•) ùëì(ùë†) ùëì(ùë†‚Ä≤) ùëî(ùëê)ùë°11 ùë°12 ùë°13

Bi-LSTM

ùë¢ùë§1 t21 t22 t23

Bi-LSTM

ùë¢ùë§2

Non-trainable 
word embedding

Trainable character 
embeddings

BNE: Biomedical Name Encoder Three objectives used to train the encoder

Figure 2: Our proposed biomedical name encoding framework. The main encoder (BNE) is based on two-level
BiLSTM to capture both character and word-level information of an input name. BNE parameters are learned by
considering three training objectives. Synonym-based objectiveLsyn enforces similar representations between two
synonymous names (s and s‚Ä≤). Concept-based objective Ldef , and context-based objectives Lctx apply similarity
constraints on representations of names (s or s‚Ä≤, which are interchangeable) and their conceptual and contextual
representations (g(c) and g(x), respectively). Details about g(c) and g(x) calculations are discussed in Section 3.2.

3.1 Skip-gram with Context and Concept

The skip-gram model described by Equation 2
uses context words to calculate embeddings for
names. Apart from the context words, we also
considers the name‚Äôs conceptual information in
this new baseline. We leverage two sources of con-
ceptual information: words in a name, and name‚Äôs
associated concept. We assume that names con-
taining similar words tend to have similar mean-
ing. Furthermore, names of the same concepts will
also share common meaning.

We introduce a new token type for concepts.
The concept embeddings are trained in a similar
way as name embeddings. Specifically, for this
baseline, we utilize a pre-annotated corpus where
names appearing in the training text are labeled
with their associated concepts. We convert the
annotated texts into sequences of words, name,
and concept tokens to be used as inputs to the
skip-gram model. For example, consider a pseudo
sentence that has 4 words and contains a bigram
name: wl w1 w2 wr, we map the annotated name
w1 w2 to a name token si, and its annotated con-
cept is denoted by ci. We create two sequences of
tokens corresponding to this original sentence:

‚Ä¢ wl, si, ci, w1, w2, wr
‚Ä¢ wl, w1, w2, si, ci, wr

The name and concept tokens are placed on the left
and right sides of the annotated name to avoid be-
ing biased toward any single side. These token se-
quences are subsequently fed as inputs to the skip-
gram baseline (the training details are presented
in Section 4). Outputs of this baseline are word,
name and concept embeddings.

3.2 Biomedical Name Encoder with Context,
Concept, and Synonym

Our proposed framework is illustrated in Figure 2.
The encoder unit is based on BiLSTM to aggregate
information from both character and word levels.
The encoded representations are constrained by
three objectives, namely synonym, context, and
concept-based objectives. The model utilizes syn-
onym sets in UMLS as training data. We denote all
the synonym sets as U = {Sc}, where Sc includes
all names of concept c, i.e., Sc = {si}.

Biomedical Name Encoder (BNE). The en-
coder extracts a fixed-sized representation for a
given name (or surface form) s. We use one BiL-
STM unit with last-pooling to encode character-
level information of each word. The represen-
tation is then concatenated with the pre-trained
word embedding to form a word-level represen-
tation. Another BiLSTM unit with max-pooling is
used to aggregate the semantics from the sequence
of words‚Äô representations. Finally the aggregated
representation is passed through a linear transfor-
mation. Mathematically, the encoding function is
expressed as follows:

hwi = [uwi ‚äï last(BiLSTM(ti,1, .., ti,m))]
hs = max(BiLSTM(hw1 , .., hwn))

f(s) = Whs + b

where uwi represents the pre-trained word embed-
ding of word wi in name s. ti,j is a trainable char-
acter embedding in wi. ‚äï denotes vector concate-
nation. W and b are parameters of the last trans-
formation. Next, we detail three objectives used to
train the encoder.



3279

Synonym-based Similarity. Representations of
names that belong to the same concept should be
similar to each other. We formulate this objective
using the following loss function:

Lsyn =
‚àë

(s,s‚Ä≤)‚ààSc√óSc

d(f(s), f(s‚Ä≤)) (3)

where d(¬∑, ¬∑) is a function that measures the differ-
ence between two representations.

As mentioned in the introduction, training the
encoder using only this synonym-based objective
will lead to biased representations. Specifically,
the encoder will be trained to act like a hash func-
tion, which performs well on determining whether
two names are synonym of each other. However, it
likely loses the semantics of names. As a remedy,
we further introduce concept and context-based
objectives to regularize the representations.

Conceptual Meaningfulness. Representations
of biomedical names should be similar to those
of their associated concepts. This objective com-
plements the synonym-based objective introduced
earlier. The latter not only shifts the synonymous
embeddings close to each other, but also pulls
them near to its concept‚Äôs centroid, expressed as:

Ldef =
‚àë

c, s‚ààSc

d(f(s), g(c)) (4)

where g(c) returns a vector that encodes concep-
tual information of the corresponding concept c.
There are several options for this representation.
It can be a mapping to pre-trained concept embed-
dings learned from a large corpus, i.e., g(c) = uc.
Another option is taking composition (e.g., aver-
age) of all its name embeddings (see Table 1), i.e.,
g(c) = 1|Sc|

‚àë
s‚ààSc us. Furthermore, when defini-

tion of the concept is available, g(c) can be mod-
eled as another encoding function that extracts the
conceptual meaning from the definition.

Contextual Meaningfulness. Each name repre-
sentation should accommodate specific contextual
information owned by the name, formulated as:

Lctx =
‚àë

s,x‚ààXs

d(f(s), q(x)) (5)

where Xs represents all local contexts of name s,
and q(x) returns contextual representation of lo-
cal context x. A straightforward way to model Xs
is using local context words of s. However, this

modeling is computationally expensive since the
training will need to iterate through all the con-
text words of the name. Alternatively, the contex-
tual information can be modeled using 1-hop ap-
proximation of the name‚Äôs local contexts, which
is mapped to the name‚Äôs contextual representa-
tion, i.e., Xs = {s} and q(x) = q(s) = us.
We also consider another approximation where the
contextual representation is further approximated
by its pre-trained word embeddings, i.e., q(s) =

1

|T (s)|
‚àë

w‚ààT (s) uw where T (s) represents words

in name s. Intuitively, in these two approxima-
tions, we assume that the pre-trained name or
word embeddings carry local contextual informa-
tion since they are trained by context-based ap-
proaches (see Section 2).

Combined Loss Function. The final loss func-
tion combines all the introduced losses:

LBNE = Lsyn + Ldef + Lctx (6)

For simplicity, we ignore weighting factors that
control the contribution of each loss. However, ap-
plying and fine-tuning these factors will shift the
encoding results more on either semantic similar-
ity or synonym-based similarity direction.

Choices of g(c) and q(x). Several options to
calculate the conceptual and contextual represen-
tations are discussed earlier. Note that the two
representations should be placed in the same dis-
tributional space. As such, the implicit relations
between them are encoded in, and can be decoded
from, their presentations. For efficiency, we model
the local contexts Xs using contextual information
encoded in the name itself, i.e., Xs = {s} and
q(x) = q(s). To this end, we focus on studying
two combinations of g(c) and q(s):

‚Ä¢ Option 1: Both g(c) and q(s) directly map
to the pre-trained concept and name em-
beddings, respectively, i.e., g(c) = uc and
q(s) = us. These embeddings are the out-
puts of our proposed extension of skip-gram
model (see Section 3.1). This option requires
annotated biomedical corpus.

‚Ä¢ Option 2: The contextual presentation q(s)
is approximated by the average of pre-
trained words embeddings, i.e., q(s) =

1
|T (s)|

‚àë
w‚ààTs uw; and g(c) is the average of

all contextual presentations associated to the



3280

concept, i.e., g(c) = 1|Sc|
‚àë

s‚ààSc q(s). These
computations only require pre-trained word
embeddings, and a dictionary of names and
concepts, e.g., UMLS.

Distance Function and Optimization. Dis-
tance function d can be Euclidean distance or
Kullback-Leibler divergence. Alternatively, the
optimization can be modeled as binary classifi-
cation, motivated by its efficiency and effective-
ness (Conneau et al., 2017; Wieting and Gim-
pel, 2017; Logeswaran and Lee, 2018). Another
benefit of using classification is to align the en-
coded BNE vectors to the pre-trained word, name,
and concept embeddings. The pre-trained embed-
dings are derived by skip-gram with negative sam-
pling (Mikolov et al., 2013), which is also formu-
lated as classification. In a similar way, we adopt
logistic loss with dot product classifier for all the
objectives. For example, the updated loss function
for Lsyn is rewritten as follows:

`(f(s‚Ä≤)·µÄf(s)) +
‚àë
sÃÑ‚ààNs

`(‚àíf(sÃÑ)·µÄf(s))

where ` is the logistic loss function ` : x 7‚Üí
log(1 + e‚àíx). Negative name sÃÑ is sampled from
a mini-batch during optimization, similar to (Wi-
eting et al., 2015). In a similar way, the loss func-
tions Ldef and Lctx are also updated accordingly.

4 Experiments

We first detail the implementations of baselines
and the proposed BNE model. We then evaluates
all the models with 4 different tasks in retrieval,
embedding similarity and relatedness benchmarks.

Skip-gram Baselines. We consider three vari-
ants of skip-gram (with negative sampling). SGW
obtains word embeddings by training the very ba-
sic skip-gram model (see Equation 1). To get the
representation for a name, we simply take the av-
erage of its associated word embeddings. SGS is
another variant that considers names as special to-
kens. The model obtains embeddings for word and
names concurrently (see Equation 2). SGS train-
ing requires input text to be segmented into names
and regular words. SGS.C is our proposed exten-
sion of skip-gram model. As introduced in Sec-
tion 3.1, this baseline requires an annotated corpus
where the names are labeled with their associated
concepts.

Training of Skip-gram Baselines. We use
PubMed corpus, which consists of 29 million
biomedical abstracts, to train SGW . For SGS and
SGS.C , we further utilize the annotations provided
in Pubtator (Wei et al., 2013). The annotations
(names and their associated concepts) come with
five categories: disease, chemical, gene, species,
and mutation. We use annotations of the two pop-
ular classes: disease and chemical. In prepro-
cessing, text is tokenized and lowercased. Words
that appear less than 3 times are ignored. We use
spaCy library for this parsing. In total, our vocabu-
lary contains approximately 3 millions words, 700
thousand names, and 85 thousand CUIs. We use
Gensim library to train all the skip-gram baselines.
The embedding dimension is 200, and the context
window size is 6. Negative sampling is used with
the number of negatives set to 5.

Biomedical Named Encoder (BNE). We set the
character embedding dimension to 50, and initial-
ize their values randomly. We use 200 dimensions
for the outputted name embeddings. The hidden
states‚Äô dimensions for both character and word-
level BiLSTM are 200. We use Adam optimizer
with the learning rate of 0.001, and gradient clip-
ping threshold set to 5.0. Training batch size is
64. Dropout with the rate of 0.5 is used to regular-
ize the model. Average performance on validation
sets of biomedical name normalization experiment
(see Section 4.3) is used as a criteria to stop the
model training.

Training of BNE. Our proposed model is
trained using only the synonym sets in UMLS2,
i.e., U = {Sc}. We limit the synonyms to those
of disease concepts3. We intentionally leave the
chemical concepts out for out-domain evaluation.
As a result, approximately 16 thousand synonym
sets (associated to that number of disease con-
cepts) are collected for training. These synonym
sets include 156 thousand disease names in total.
In each training batch, one positive and one neg-
ative pairs are sampled separately for each loss.
The pre-trained word (or name/concept) embed-
dings are taken from the skip-gram baselines as
described before. We denote two configurations,
associated to Options 1 and 2 (see Section 3.2),
as BNE + SGS.C and BNE + SGW, respectively.
Next, we present the evaluations of these models.

2We use the 2018AA version released in May, 2018.
3We consider the diseases that exist in the CTD‚Äôs MEDIC

disease vocabulary (Davis et al., 2014).



3281

SGW SGS.C BNE + SGW BNE + SGS.C

cardiotoxicity
hypertrophic cardiomyopathy (*)

endotoxemia
ischemic colitis (*)

hematologic diseases
parkinson disease (*)

lead poisoning
pseudotumor cerebri (*)

paranoid disorders
rheumatic diseases (*)

Figure 3: t-SNE visualization of 254 name embeddings. These names belong to 10 disease concepts in which 5 of
these concepts appear in the training data, while the other 5 concepts (marked with (*)) do not. It can be observed
that BNE projects names of the same concept close to each others. The model also retains closeness between
names of related concepts, such as ‚Äòparkinson disease‚Äô and ‚Äòparanoid disorders‚Äô (see the blue and olive plus signs).

 0.2

 0.4

 0.6

 0.8

 1

 1  4  16  64  256 1024

M
e

a
n

 C
o
v
e

ra
g

e
 a

t 
k

k

SGW
SGS

SGS.C
BNE + SGW

BNE + SGS.C

(a) Diseases (in-domain)

 0.2

 0.4

 0.6

 0.8

 1

 1  4  16  64  256 1024

M
e

a
n

 C
o
v
e

ra
g

e
 a

t 
k

k

SGW
SGS

SGS.C
BNE + SGW

BNE + SGS.C

(b) Chemicals (out-domain)

Figure 4: Mean coverage at k: average ratio of correct
synonyms that are found in k-nearest neighbors, which
are estimated by cosine similarity of name embeddings.
Note that names in these disease and chemical test sets
are not seen in the training data.

4.1 Closeness Analysis of Synonymous
Embeddings

We propose a measure to estimate the closeness
between name embeddings of the same concept.
For each name, we consider its k most similar
names estimated by cosine similarity of their em-
beddings. We define coverage at k as ratio of
correct synonyms that are found in the k-nearest
neighbors. We report the average score of all
query names, as mean coverage at k.

We create two test sets for this experiment, one
for disease names and one for chemical names.
Given the CTD‚Äôs MEDIC disease vocabulary, we
randomly select 1000 concepts and all their cor-
responding names in UMLS. In this experiment,
we exclude these 1000 concepts from the synonym
sets used to train BNE encoder. Furthermore, to
ensure the quality of the selected names, we only
consider the ones that appear in the high-quality
biomedical phrases collected by Kim et al. (2018).
Similarly, we create another test set for chemical
names. This chemical set is used to evaluate out-

domain performance since our model is trained us-
ing only disease synonyms.

As shown in Figure 4, BNE outperforms other
embedding baselines that do not consider the
synonym-based objective. More importantly, the
model also generalizes well to out-domain data
(chemical names). Furthermore, among the skip-
gram baselines, the context-based name embed-
ding model (SGS) is worse than the average word
embedding baseline (SGW). The result again in-
dicates that words in biomedical names are more
indicative about their conceptual identities.

The embedding plots in Figure 3 further illus-
trate the effectiveness of our encoder in enhanc-
ing the similarity between synonymous represen-
tations. By investigating name embeddings of
an unseen concept ‚Äòpseudotumor cerebri‚Äô, we ob-
serve that BNE is robust to the morphology of
biomedical names, such as ‚Äòbenign hypertension
intracranial‚Äô and ‚Äò benign intracran hypt‚Äô. The
model is also aware of word importance in long
names such as ‚Äòintracranial pressure increased (be-
nign)‚Äô. Moreover, since BNE is trained using syn-
onym sets, the encoder is equipped with knowl-
edge about alternative expressions of biomedical
terms, e.g., ‚Äòintracranial hypertension‚Äô and ‚Äòin-
tracranial increased pressure‚Äô. The knowledge can
be used to infer quality representations for new
synonyms. However, similar to skip-gram base-
lines, BNE faces serious challenges if the names
are unpopular and contain words that do not re-
flect their conceptual meanings. For example,
for this ‚Äòpseudotumor cerebri‚Äô concept, the name
‚ÄúNonne‚Äôs syndrome‚Äù4 is distant from its concept
cluster (see the red square locating near the blue
plus signs in Figure 3).

4Dr. Max Nonne coined the name ‚Äòpseudotumor cerebri‚Äô
in 1904.



3282

Models NCBI(Disease)
BC5CDR
(Disease)

BC5CDR
(Chemical)

Jaccard 0.424 0.410 0.607
SGW 0.499 0.494 0.598
SGW + WMD 0.532 0.526 0.637

SGS 0.487 0.472 0.623
SGS.C 0.531 0.510 0.628
BNE + SGW 0.695 0.718 0.664
BNE + SGS.C 0.713 0.734 0.672

Table 2: Mean average precision (MAP) performance
on the synonym retrieval task. The best and second best
results are in boldface and underlined, respectively.

4.2 Synonym Retrieval

We evaluate the embeddings in synonym re-
trieval application: given a biomedical mention
(or name), retrieving all its synonyms from a con-
trolled vocabulary by ranking. We use NCBI-
Disease (DogÃÜan et al., 2014) and BC5CDR (Li
et al., 2016) datasets in this evaluation. NCBI-
Disease contains disease mentions extracted from
PubMed abstracts, while BC5CDR contains both
disease and chemical mentions. These mentions
are used as queries in this synonym retrieval task.
Note that, different from the closeness evaluation,
a disease name may or may not appear in the syn-
onym sets used to train BNE encoder. On the other
hand, chemical queries are completely unseen dur-
ing the model training. For each query, we retrieve
a list of potentially associated concepts. A con-
cept is retrieved if one of its names is similar to
the query (estimated by BM25 score). We col-
lect all names of the top-20 retrieved concepts as
a synonym candidate set. Cosine similarity is then
used to rank the candidates. We also evaluate the
results with Jaccard and Word‚Äôs Mover Distance
(WMD) (Kusner et al., 2015) measures.

As shown in Table 2, SGW+WMD outper-
forms Jaccard baseline (in MAP score), mainly
because of its ability to capture semantic match-
ing. However, both baselines are non-parametric.
In contrast, BNE+SGW learns additional knowl-
edge about the synonym matching by using syn-
onyms sets in UMLS as training data. Although
the model is trained on only disease names, it
also generalizes well to chemical names. Fur-
thermore, comparing between the two configura-
tions of BNE, both BNE+SGW and BNE+SGSC
models yield comparable performances. However,
BNE+SGW is simpler since it does not require
pre-trained name and concept embeddings.

Models NCBI(Di)
BC5CDR

(Di)
BC5CDR

(Ch)

Jaccard 0.843 0.772 0.935
SGW 0.800 0.725 0.771
SGW + WMD 0.779 0.731 0.919

SGS 0.815 0.790 0.929
SGS.C 0.838 0.811 0.929
BNE + SGW 0.854 0.829 0.930
BNE + SGS.C 0.857 0.829 0.934
Wieting et al. (2015) 0.822 0.813 0.930

D‚ÄôSouza and Ng (2015) 0.847 0.841 -
Leaman and Lu (2016) 0.877‚Ä† 0.889‚Ä† 0.941
Wright et al. (2019) 0.878‚Ä† 0.880‚Ä† -
BNE + SGW + XM 0.873 0.905 0.954
BNE + SGS.C + XM 0.877 0.906 0.958

Table 3: Name normalization accuracy on disease (Di)
and chemical (Ch) datasets. The last row group in-
cludes the results of supervised models that utilize
training annotations in each specific dataset. XM de-
notes the use of ‚Äòexact match‚Äô rule to assign the corre-
sponding concept to a mention if the mention is found
in the training data. ‚Ä† indicates the results reported
by Wright et al. (2019).

4.3 Biomedical Name Normalization

Biomedical name normalization (a.k.a., biomedi-
cal concept linking) aims to map each biomedical
mention appearing in text to its associated con-
cept in a dictionary. We use NCBI-Disease and
BC5CDR datasets in this evaluation. Similar to
previous works, we use Ab3P (Sohn et al., 2008)
to resolve local abbreviations. Composite men-
tions (such as ‚Äòpineal and retinal tumors‚Äô) are split
into separate mentions (‚Äòpineal tumors‚Äô and ‚Äòreti-
nal tumors‚Äô) using simple patterns as described
in (D‚ÄôSouza and Ng, 2015). For each mention, we
find the concept CUI (in UMLS) that has the most
similar name. The selected CUI is then mapped to
its associated MeSH or OMIM ID in the CTD dic-
tionary for evaluation. We only consider mentions
whose associated concepts exist in the CTD dictio-
nary and report the accuracy aggregated from all
mentions in test set. Apart from existing baselines,
we also re-implement compositional paraphrase
model, proposed by Wieting et al. (2015). The dif-
ference is that we use word-level BiLSTM instead
of recursive neural network. Furthermore, L2 reg-
ularizations with the weights of 10‚àí3 and 10‚àí4 are
applied on the BiLSTM‚Äôs parameters and the dif-
ference between the trainable and initial word em-
beddings, respectively.

Different from the lexical (Jaccard) and seman-
tic matching (WMD and SGW) baselines, BNE ob-



3283

tains high scores in both accuracy and ranking-
based (MAP) metrics (see Tables 2, and 3). The
result indicates that BNE has encoded both lexical
and semantic information of names into their em-
beddings. Table 3 also includes performances of
other state-of-the-art baselines in biomedical name
normalization, such as sieve-based (D‚ÄôSouza and
Ng, 2015), supervised semantic indexing (Leaman
and Lu, 2016), and coherence-based neural net-
work Wright et al. (2019) approaches. Note that
all these baselines require human annotated la-
bels, and the models are specifically tuned for each
dataset. On the other hand, BNE utilizes only
the existing synonym sets in UMLS for training.
When the dataset-specific annotations are utilized,
even the simple exact matching rule can boost the
performance of our model to surpass other base-
lines (see the last two rows in Table 3).

4.4 Semantic Similarity and Relatedness

We evaluate the correlation between embedding
cosine similarity and human judgments, regard-
ing semantic similarity and relatedness. Differ-
ent from previous evaluations, this experiment
aims to evaluate the conceptual similarity and
relatedness. We use two biomedical datasets:
MayoSRS (Pakhomov et al., 2011) and UMN-
SRS (Pakhomov et al., 2016). The former contains
multi-word name pairs of related concepts, e.g.,
‚Äòmorning stiffness‚Äô (C0457086) and ‚Äôrheumatoid
arthriits‚Äô (C0003873). The latter contains only
single-word name pairs and is spitted into sim-
ilarity and relatedness partitions. For example,
a pair with high similarity score are ‚Äòweakness‚Äô
(C1883552) and ‚Äòparesis‚Äô (C0030552). For these
two datasets, the names in each pair comes from
different concepts, hence they do not appear in the
synonym pairs used to train our encoder. Further-
more, the coverage of pre-trained word embed-
dings in baselines such as SGW are 100% and 97%
for UMNSRS and MayoSRS, respectively.

Table 4 shows that BNE models perform espe-
cially well on the multi-word relatedness test set
(MayoSRS). Conceptual information has been uti-
lized by these models to enrich the name represen-
tations. On the other hand, when the training is
performed solely on the synonym pairs (only use
Lsyn), the trained model is overfitted to the train-
ing task and do not generalize to other test cases.

SGW is still a strong baseline in these bench-
marks. Other skip-gram and fastText embed-

Models UMNSRS(sim)
UMNSRS

(rel)
MayoSRS

(rel)

SGW 0.645 0.584 0.518
Pakhomov et al. (2016) 0.620 0.580 -
Chen et al. (2018) 0.630 0.575 0.501
Beam et al. (2018) 0.411 0.334 0.427

SGS 0.614 0.566 0.516
SGS.C 0.654 0.592 0.557
BNE + SGW 0.606 0.580 0.626
BNE + SGS.C 0.637 0.593 0.602
BNE + SGS.C (Lsyn) 0.496 0.445 0.564
Wieting et al. (2015) 0.639 0.565 0.595

Table 4: Spearman‚Äôs rank correlation coefficient be-
tween cosine similarly scores of name embeddings
and human judgments, reported on semantic similarity
(sim) and relatedness (rel) benchmarks.

dings (Pakhomov et al., 2016; Chen et al., 2018),
which are trained on a similar corpus, do not
achieve better results. Beam et al. (2018) use a
SVD-based word2vec model (Levy et al., 2015)
to compute embeddings for biomedical concepts.
Although the embeddings are trained on a much
larger multimodal medical data, their results are
lower than other baselines. Further investigation
reveals that many concepts in the test sets do not
exist in their pre-trained concept embeddings.

5 Conclusion

By learning to encode names of the same con-
cepts into similar representations, while preserv-
ing their conceptual and contextual meanings, our
encoder is able to extract meaningful represen-
tations for unseen names. The core unit of our
encoder (in this work) is BiLSTM. Alternatively,
sequence encoding models such as GRU, CNN,
transformer, or even encoders with contextualized
word embeddings like BERT (Devlin et al., 2018),
or ELMo (Peters et al., 2018) can be used to re-
place this BiLSTM, however, with additional com-
putation cost. We also discuss different ways of
representing the contextual and conceptual infor-
mation in our framework. In implementation, we
use the simple aggregation of pre-trained embed-
dings. The experiment results show that this ap-
proach is both efficient and effective.

Acknowledgments

We thank the anonymous reviewers for their in-
sightful suggestions. This work was supported by
Data Science & Artificial Intelligence Research
Centre, NTU Singapore.



3284

References
Sanjeev Arora, Yingyu Liang, and Tengyu Ma. 2017.

A simple but tough-to-beat baseline for sentence em-
beddings. In ICLR.

Andrew L Beam, Benjamin Kompa, Inbar Fried,
Nathan P Palmer, Xu Shi, Tianxi Cai, and Isaac S
Kohane. 2018. Clinical concept embeddings learned
from massive sources of medical data. CoRR,
abs/1804.01486.

Piotr Bojanowski, Edouard Grave, Armand Joulin, and
Tomas Mikolov. 2017. Enriching word vectors with
subword information. TACL, 5:135‚Äì146.

Xiangrui Cai, Jinyang Gao, Kee Yuan Ngiam,
Beng Chin Ooi, Ying Zhang, and Xiaojie Yuan.
2018. Medical concept embedding with time-aware
attention. In IJCAI, pages 3984‚Äì3990.

Daniel Cer, Yinfei Yang, Sheng-yi Kong, Nan Hua,
Nicole Limtiaco, Rhomni St John, Noah Constant,
Mario Guajardo-Cespedes, Steve Yuan, Chris Tar,
et al. 2018. Universal sentence encoder. CoRR,
abs/1803.11175.

Qingyu Chen, Yifan Peng, and Zhiyong Lu. 2018.
Biosentvec: creating sentence embeddings for
biomedical texts. CoRR, abs/1810.09302.

Billy Chiu, Gamal Crichton, Anna Korhonen, and
Sampo Pyysalo. 2016. How to train good word
embeddings for biomedical nlp. In BioNLP, pages
166‚Äì174.

Kevin Clark, Minh-Thang Luong, Christopher D Man-
ning, and Quoc Le. 2018. Semi-supervised se-
quence modeling with cross-view training. In
EMNLP, pages 1914‚Äì1925.

Alexis Conneau, Douwe Kiela, Holger Schwenk, Loƒ±Ãàc
Barrault, and Antoine Bordes. 2017. Supervised
learning of universal sentence representations from
natural language inference data. In EMNLP, pages
670‚Äì680.

Allan Peter Davis, Cynthia J Grondin, Kelley Lennon-
Hopkins, Cynthia Saraceni-Richards, Daniela Sci-
aky, Benjamin L King, Thomas C Wiegers, and
Carolyn J Mattingly. 2014. The comparative tox-
icogenomics database‚Äôs 10th year anniversary: up-
date 2015. Nucleic acids research, 43(D1):D914‚Äì
D920.

Jacob Devlin, Ming-Wei Chang, Kenton Lee, and
Kristina Toutanova. 2018. Bert: Pre-training of deep
bidirectional transformers for language understand-
ing. CoRR, abs/1810.04805.

Rezarta Islamaj DogÃÜan, Robert Leaman, and Zhiyong
Lu. 2014. Ncbi disease corpus: a resource for dis-
ease name recognition and concept normalization.
Journal of biomedical informatics, 47:1‚Äì10.

Jennifer D‚ÄôSouza and Vincent Ng. 2015. Sieve-based
entity linking for the biomedical domain. In ACL ‚Äî
IJCNLP, volume 2, pages 297‚Äì302.

Manaal Faruqui, Jesse Dodge, Sujay Kumar Jauhar,
Chris Dyer, Eduard Hovy, and Noah A Smith. 2015.
Retrofitting word vectors to semantic lexicons. In
NAACL-HLT, pages 1606‚Äì1615.

Alex Graves and JuÃàrgen Schmidhuber. 2005. Frame-
wise phoneme classification with bidirectional lstm
and other neural network architectures. Neural Net-
works, 18(5-6):602‚Äì610.

Felix Hill, Kyunghyun Cho, and Anna Korhonen. 2016.
Learning distributed representations of sentences
from unlabelled data. In NAACL-HLT, pages 1367‚Äì
1377.

Mohit Iyyer, Varun Manjunatha, Jordan Boyd-Graber,
and Hal DaumeÃÅ III. 2015. Deep unordered compo-
sition rivals syntactic methods for text classification.
In ACL ‚Äî IJCNLP, volume 1, pages 1681‚Äì1691.

Nal Kalchbrenner, Edward Grefenstette, and Phil Blun-
som. 2014. A convolutional neural network for
modelling sentences. In ACL, volume 1, pages 655‚Äì
665.

Sun Kim, Lana Yeganova, Donald C Comeau, W John
Wilbur, and Zhiyong Lu. 2018. Pubmed phrases, an
open set of coherent phrases for searching biomedi-
cal literature. Scientific data, 5:180104.

Ryan Kiros, Yukun Zhu, Ruslan R Salakhutdinov,
Richard Zemel, Raquel Urtasun, Antonio Torralba,
and Sanja Fidler. 2015. Skip-thought vectors. In
NeurIPS, pages 3294‚Äì3302.

Matt Kusner, Yu Sun, Nicholas Kolkin, and Kilian
Weinberger. 2015. From word embeddings to docu-
ment distances. In ICML, pages 957‚Äì966.

Robert Leaman and Zhiyong Lu. 2016. Taggerone:
joint named entity recognition and normaliza-
tion with semi-markov models. Bioinformatics,
32(18):2839‚Äì2846.

Omer Levy, Yoav Goldberg, and Ido Dagan. 2015. Im-
proving distributional similarity with lessons learned
from word embeddings. TACL, 3:211‚Äì225.

Haodi Li, Qingcai Chen, Buzhou Tang, Xiaolong
Wang, Hua Xu, Baohua Wang, and Dong Huang.
2017. Cnn-based ranking for biomedical entity nor-
malization. BMC bioinformatics, 18(11):385.

Jiao Li, Yueping Sun, Robin J Johnson, Daniela Sci-
aky, Chih-Hsuan Wei, Robert Leaman, Allan Peter
Davis, Carolyn J Mattingly, Thomas C Wiegers, and
Zhiyong Lu. 2016. Biocreative v cdr task corpus:
a resource for chemical disease relation extraction.
Database, 2016.



3285

Miaofeng Liu, Jialong Han, Haisong Zhang, and Yan
Song. 2018. Domain adaptation for disease phrase
matching with adversarial networks. In BioNLP,
pages 137‚Äì141.

Lajanugen Logeswaran and Honglak Lee. 2018. An
efficient framework for learning sentence represen-
tations. CoRR, abs/1803.02893.

Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Cor-
rado, and Jeff Dean. 2013. Distributed representa-
tions of words and phrases and their compositional-
ity. In NeurIPS, pages 3111‚Äì3119.

Nikola MrksÃåicÃÅ, Ivan VulicÃÅ, Diarmuid OÃÅ SeÃÅaghdha, Ira
Leviant, Roi Reichart, Milica GasÃåicÃÅ, Anna Korho-
nen, and Steve Young. 2017. Semantic special-
ization of distributional word vector spaces using
monolingual and cross-lingual constraints. TACL,
pages 309‚Äì324.

Serguei VS Pakhomov, Greg Finley, Reed McEwan,
Yan Wang, and Genevieve B Melton. 2016. Corpus
domain effects on distributional semantic model-
ing of medical terms. Bioinformatics, 32(23):3635‚Äì
3644.

Serguei VS Pakhomov, Ted Pedersen, Bridget
McInnes, Genevieve B Melton, Alexander Rug-
gieri, and Christopher G Chute. 2011. Towards a
framework for developing semantic relatedness ref-
erence standards. Journal of biomedical informat-
ics, 44(2):251‚Äì265.

Jeffrey Pennington, Richard Socher, and Christopher
Manning. 2014. Glove: Global vectors for word
representation. In EMNLP, pages 1532‚Äì1543.

Matthew Peters, Mark Neumann, Mohit Iyyer, Matt
Gardner, Christopher Clark, Kenton Lee, and Luke
Zettlemoyer. 2018. Deep contextualized word repre-
sentations. In NAACL-HLT, volume 1, pages 2227‚Äì
2237.

Andreas RuÃàckleÃÅ, Steffen Eger, Maxime Peyrard, and
Iryna Gurevych. 2018. Concatenated p-mean word
embeddings as universal cross-lingual sentence rep-
resentations. CoRR, abs/1803.01400.

Richard Socher, Eric H Huang, Jeffrey Pennin, Christo-
pher D Manning, and Andrew Y Ng. 2011. Dy-
namic pooling and unfolding recursive autoencoders
for paraphrase detection. In NeurIPS, pages 801‚Äì
809.

Sunghwan Sohn, Donald C Comeau, Won Kim, and
W John Wilbur. 2008. Abbreviation definition iden-
tification based on automatic precision estimates.
BMC bioinformatics, 9(1):402.

Sandeep Subramanian, Adam Trischler, Yoshua Ben-
gio, and Christopher J Pal. 2018. Learning gen-
eral purpose distributed sentence representations via
large scale multi-task learning. In ICLR.

Kai Sheng Tai, Richard Socher, and Christopher D
Manning. 2015. Improved semantic representations
from tree-structured long short-term memory net-
works. In ACL ‚Äî IJCNLP, volume 1, pages 1556‚Äì
1566.

Ivan VulicÃÅ, Goran GlavasÃå, Nikola MrksÃåicÃÅ, and Anna
Korhonen. 2018. Post-specialisation: Retrofitting
vectors of words unseen in lexical resources. In
NAACL-HLT, pages 516‚Äì527.

Yanshan Wang, Sijia Liu, Naveed Afzal, Majid
Rastegar-Mojarad, Liwei Wang, Feichen Shen, Paul
Kingsbury, and Hongfang Liu. 2018. A compari-
son of word embeddings for the biomedical natural
language processing. Journal of biomedical infor-
matics, 87:12‚Äì20.

Chih-Hsuan Wei, Hung-Yu Kao, and Zhiyong Lu.
2013. Pubtator: a web-based text mining tool
for assisting biocuration. Nucleic acids research,
41(W1):W518‚ÄìW522.

John Wieting, Mohit Bansal, Kevin Gimpel, and Karen
Livescu. 2015. From paraphrase database to compo-
sitional paraphrase model and back. TACL, 3:345‚Äì
358.

John Wieting, Mohit Bansal, Kevin Gimpel, and Karen
Livescu. 2016. Towards universal paraphrastic sen-
tence embeddings. In ICLR.

John Wieting and Kevin Gimpel. 2017. Revisiting re-
current networks for paraphrastic sentence embed-
dings. In ACL, pages 2078‚Äì2088.

Dustin Wright, Yannis Katsis, Raghav Mehta, and
Chun-Nan Hsu. 2019. Normco: Deep disease nor-
malization for biomedical knowledge base construc-
tion. AKBC.

‚Äò


