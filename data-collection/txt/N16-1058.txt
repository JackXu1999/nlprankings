



















































Transition-Based Syntactic Linearization with Lookahead Features


Proceedings of NAACL-HLT 2016, pages 488–493,
San Diego, California, June 12-17, 2016. c©2016 Association for Computational Linguistics

Transition-Based Syntactic Linearization with Lookahead Features

Ratish Puduppully †∗, Yue Zhang ‡, Manish Shrivastava †
†Kohli Center on Intelligent Systems (KCIS),

International Institute of Information Technology, Hyderabad (IIIT Hyderabad)
‡Singapore University of Technology and Design

ratish.surendran@research.iiit.ac.in yue zhang@sutd.edu.sg
m.shrivastava@iiit.ac.in

Abstract

It has been shown that transition-based meth-
ods can be used for syntactic word order-
ing and tree linearization, achieving signifi-
cantly faster speed compared with traditional
best-first methods. State-of-the-art transition-
based models give competitive results on ab-
stract word ordering and unlabeled tree lin-
earization, but significantly worse results on
labeled tree linearization. We demonstrate that
the main cause for the performance bottle-
neck is the sparsity of SHIFT transition actions
rather than heavy pruning. To address this is-
sue, we propose a modification to the stan-
dard transition-based feature structure, which
reduces feature sparsity and allows lookahead
features at a small cost to decoding efficiency.
Our model gives the best reported accuracies
on all benchmarks, yet still being over 30
times faster compared with best-first-search.

1 Introduction

Word ordering is the abstract language modeling
task of making a grammatical sentence by ordering a
bag of words (White, 2004; Zhang and Clark, 2015;
De Gispert et al., 2014; Bohnet et al., 2010; Filip-
pova and Strube, 2007; He et al., 2009), which is
practically relevant to text-to-text applications such
as summarization (Wann et al., 2009) and machine
translation (Blackwood et al., 2010). Zhang (2013)
built a discriminative word ordering model, which
takes a bag of words, together with optional POS
and dependency arcs on a subset of input words, and

∗Part of the work was done when the author was a visiting
student at Singapore University of Technology and Design.

yields a sentence together with its dependency parse
tree that conforms to input syntactic constraints. The
system is flexible with respect to input constraints,
performing abstract word ordering when no con-
straints are given, but gives increasingly confined
outputs when more POS and dependency relations
are specified. It has been applied to syntactic lin-
earization (Song et al., 2014) and machine transla-
tion (Zhang et al., 2014).

One limitation of Zhang (2013) is relatively low
time efficiency, due to the use of time-constrained
best-first-search (White and Rajkumar, 2009) for de-
coding. In practice, the system can take seconds to
order a bag of words in order to obtain reasonable
output quality. Recently, Liu et al. (2015) proposed
a transition-based model to address this issue, which
uses a sequence of state transitions to build the out-
put. The system of Liu et al. (2015) achieves signifi-
cant speed improvements without sacrificing accura-
cies when working with unlabeled dependency trees.
With labeled dependency trees as input constraints,
however, the system of Liu et al. (2015) gives much
lower accuracies compared with Zhang (2013).

While the low accuracy can be attributed to heavy
pruning, we show that it can be mitigated by modi-
fying the feature structure of the standard transition-
based framework, which scores the output transi-
tion sequence by summing the scores of each tran-
sition action. Transition actions are treated as an
atomic output component in each feature instance.
This works effectively for most structured prediction
tasks, including parsing (Zhang and Clark, 2011a).
For word ordering, however, transition actions are
significantly more complex and sparse compared

488



with parsing, which limits the power of the tradi-
tional feature model.

We instead break down complex actions into
smaller components, merging some components
into configuration features which reduces sparsity in
the output action and allows flexible lookahead fea-
tures to be defined according to the next action to
be applied. On the other hand, this change in the
feature structure prevents legitimate actions to be
scored simultaneously for each configuration state,
thereby reducing decoding efficiency. Experiments
show that our method is slightly slower compared
with Liu et al. (2015), but achieves significantly bet-
ter accuracies. It gives the best results for all stan-
dard benchmarks, being over thirty times faster than
Zhang (2013). The new feature structures can be ap-
plied to other transition-based systems also.

2 Transition-based linearization

Liu et al. (2015) uses a transition-based model for
word ordering, building output sentences using a se-
quence of state transitions. Instead of scoring out-
put syntax trees directly, it scores the transition ac-
tion sequence for structural disambiguation. Liu et
al.’s transition system extends from transition-based
parsers (Nivre and Scholz, 2004; Chen and Man-
ning, 2014), where a state consists of a stack to hold
partially built outputs. Transition-based parsers use
a queue to maintain input word sequences. How-
ever, for word ordering, the input is a set without
order. Accordingly, Liu et al. uses a set to maintain
the input. The transition actions are:
• SHIFT-Word-POS, which removes Word from

the set, assigns POS to it and pushes it onto the
stack as the top word S0;
• LEFTARC-LABEL, which removes the second

top of stack S1 and builds a dependency arc
S1

LABEL←−−−−− S0;
• RIGHTARC-LABEL, which removes the top

of stack S0 and builds a dependency arc
S1

LABEL−−−−−→ S0.
Using the state transition system, the bag of
words {John, loves, Mary} can be ordered by
(SHIFT-John-NNP, SHIFT-loves-VBZ, LEFTARC-
SBJ, SHIFT-Mary-NNP, RIGHTARC-OBJ).

Liu et al. (2015) use a discriminative perceptron
model with beam search (Zhang and Clark, 2011a),

Unigrams
S0w; S0p; S0,lw; S0,lp; S0,rw; S0,rp;
S0,l2w; S0,l2p; S0,r2w; S0,r2p;
S1w; S1p; S1,lw; S1,lp; S1,rw; S1,rp;
S1,l2w; S1,l2p; S1,r2w; S1,r2p;
Bigram
S0wS0,lw; S0wS0,lp; S0pS0,lw; S0pS0,lp;
S0wS0,rw; S0wS0,rp; S0pS0,rw; S0pS0,rp;
S1wS1,lw; S1wS1,lp; S1pS1,lw; S1pS1,lp;
S1wS1,rw; S1wS1,rp; S1pS1,rw; S1pS1,rp;
S0wS1w; S0wS1p; S0pS1w; S0pS1p
Trigram
S0wS0pS0,lw; S0wS0,lwS0,lp; S0wS0pS0,lp;
S0pS0,lwS0,lp; S0wS0pS0,rw; S0wS0,lwS0,rp;
S0wS0pS0,rp; S0pS0,rwS0,rp;
S1wS1pS1,lw; S1wS1,lwS1,lp; S1wS1pS1,lp;
S1pS1,lwS1,lp; S1wS1pS1,rw; S1wS1,lwS1,rp;
S1wS1pS1,rp; S1pS1,rwS1,rp;
Linearization
w0; p0; w−1w0; p−1p0; w−2w−1w0; p−2p−1p0;
S0,lwS0,l2w; S0,lpS0,l2p; S0,r2wS0,rw; S0,r2pS0,rp;
S1,lwS1,l2w; S1,lpS1,l2p; S1,r2wS1,rw; S1,r2pS1,rp;

Table 1: Base feature templates.

designing decoding algorithms that accommodate
flexible constraints. The features include word(w),
pos(p) and dependency label(l) information of words
on the stack (S0, S1, ... from the top). For example,
the word on top of stack is S0w and the POS of the
stack top is S0p. The full set of feature templates
can be found in Table 2 of Liu et al. (2015), repro-
duced here in Table 1. These templates are called
configuration features. When instantiated, they are
combined with each legal output action to score the
action. Therefore, actions are atomic in feature in-
stances.

Formally, given a configuration C, the score of a
possible action a is calculated as:

Score(a) = ~θ · ~Φ(C, a),
where ~θ is the model parameter vector of the model
and ~Φ(C, a) denotes a sparse feature vector that con-
sists of features with configuration and action com-
ponents i.e ~Φ(C, a) is sparse. ~θ has to be loaded for
each a.

For efficiency considerations and following
transition-based models, Liu et al. (2015) scores
all possible actions given a configuration simultane-
ously. This is effectively the same as formulating the

489



score into

Score(a) = ~θa · ~Φ(C), a ∈ A.
Here A is the full set of actions and ~Φ(C) is fixed,
and ~θa for all a can be loaded simultaneously. In
a hash-based parameter model, it significantly im-
proves the time efficiency.

3 Feature structure modification

3.1 Two limitations of the baseline model

There are two major limitations in the feature struc-
ture of Liu et al. (2015). First, the SHIFT actions,
which consist of the word to shift and its POS, are
highly sparse. Since the action is combined with all
configuration features, there will be no active feature
for disambiguating the shift actions for OOV words.
This issue does not exist in transition-based parsers
because words are not a part of their transition ac-
tions. Second, input constraints are not leveraged by
the feature model. Although the dependency rela-
tions of the word to shift can be given as inputs, they
are used only as constraints to the decoder, but not
as features to guide the shift action. Such lookahead
information on the to-be-shifted word can be highly
useful for disambiguation.

For example, consider the bag of words {John,
loves, Mary}. Without constraints, both ‘John loves
Mary’ and ‘Mary loves John’ are valid word order-
ing results. However, given the constraint (John,
SBJ, loves), the correct answer is reduced to the
former. The first action to build the two examples
are (SHIFT-John-NNP) and (SHIFT-Mary-NNP), re-
spectively. According to Liu et al.’s feature model,
there is no feature to disambiguate the first SHIFT
action if both ‘John’ and ‘Mary’ are OOV words.
The system has to maintain both hypotheses and rely
on the search algorithm to disambiguate them after
the dependency arcs (John, SBJ, loves) and (Mary,
OBJ, loves) are built. However, given the syntac-
tic constraint that ‘John’ is the subject, the disam-
biguation can be done right when performing the
first SHIFT action. This requires the dependency arc
label to be extracted for the word to shift e.g.(John,
Mary), which is a lookahead feature. In addition, the
OOV word ‘John’ must be excluded from the feature
instance, which implies that the SHIFT-John-NNP
action must be simplified.

set of label and POS of child nodes of L
Lcls;Lclns;Lcps;Lcpns;
S0wLcls;S0pLcls;S1wLcls;S1pLcls;
S0wLclns;S0pLclns;S1wLclns;S1pLclns;
S0wLcps;S0pLcps;S1wLcps;S1pLcps;
S0wLcpns;S0pLcpns;S1wLcpns;S1pLcpns;
set of label and POS of siblings of L
Lsls;Lslns;Lsps;Lspns;
S0wLsls;S0pLsls;S1wLsls;S1pLsls;
S0wLslns;S0pLslns;S1wLslns;S1pLslns;
S0wLsps;S0pLsps;S1wLsps;S1pLsps;
S0wLspns;S0pLspns;S1wLspns;S1pLspns;
parent label, POS and word of L
LpsLlp; LpsLpp; LpsLwp;
S0wLpsLlp;S0pLpsLlp;S1wLpsLlp;S1pLpsLlp;
S0wLpsLpp;S0pLpsLpp;S1wLpsLpp;S1pLpsLpp;
S0wLpsLwp;S0pLpsLwp;S1wLpsLwp;S1pLpsLwp;
set of label and POS of child nodes of S0
Scls;Sclns;Scps;Scpns;
S0wScls;S0pScls;S1wScls;S1pScls;
S0wSclns;S0pSclns;S1wSclns;S1pSclns;
S0wScps;S0pScps;S1wScps;S1pScps;
S0wScpns;S0pScpns;S1wScpns;S1pScpns;
set of label and POS of siblings of S0
Ssls;Sslns;Ssps;Sspns;
S0wSsls;S0pSsls;S1wSsls;S1pSsls;
S0wSslns;S0pSslns;S1wSslns;S1pSslns;
S0wSsps;S0pSsps;S1wSsps;S1pSsps;
S0wSspns;S0pSspns;S1wSspns;S1pSspns;
parent label and POS of S0
SpsSlp; SpsSpp;
S0wSpsSlp;S0pSpsSlp;S1wSpsSlp;S1pSpsSlp;
S0wSpsSpp;S0pSpsSpp;S1wSpsSpp;S1pSpsSpp;

Table 2: Lookahead feature templates

As a second example, information about depen-
dents can also be useful for disambiguating SHIFT
actions. In the above case, the fact that the subject
has not been shifted onto the stack can be a useful
indicator for not shifting the verb ‘loves’ onto the
stack in the beginning. Inspired by the above, we
exploit a range of lookahead features from syntactic
constraints.

3.2 New feature structure for SHIFT actions

We modify the feature structure of Liu et al. (2015)
by breaking down the SHIFT-Word-POS action into
three components, namely SHIFT, Word and POS,
using only the action type SHIFT as the output ac-
tion component in feature instances, while combin-

490



no pos 50% pos all pos no pos 50% pos all pos no pos 50% pos all pos
no dep no dep no dep 50% dep 50% dep 50% dep all dep all dep all dep

BL SP BL SP BL SP BL SP BL SP BL SP BL SP BL SP BL SP
Z13 42.9 4872 43.4 4856 44.7 4826 50.5 4790 51.4 4737 52.2 4720 73.3 4600 74.7 4431 76.3 4218
L15 47.5 155 47.9 119 48.8 74 54.8 132 55.2 91 56.2 41 77.8 40 79.1 28 81.1 22
Ours 48.0 175 49.0 156 51.5 148 59.0 144 62.0 160 67.1 171 82.8 62 86.2 68 89.9 70

Table 3: Development partial-tree linearization results. BL – BLEU score; SP – number of milliseconds per
sentence. Z13 – best-first system of Zhang (2013) and L15 – transition-based system of Liu et al. (2015).

ing Word and POS with other configuration features
to form a set of lookahead features.

For example, consider the configuration feature
S0w, which captures the word on the top of the
stack. Under the feature structure of Liu et al., it is
combined with each possible action to form features
for scoring the action. As a result, for scoring the
action SHIFT-Lw-Lp, S0w is instantiated into S0w-
SHIFT-Lw-Lp, where Lw is the word to shift and
Lp is its POS. Under our new feature structure, the
action component is reduced to SHIFT only, while
Lw and Lp should be used in lookahead features.
Now a effectively equivalent configuration feature
to Liu et al.’s S0w is S0w-Lw-Lp, with the looka-
head Lw and Lp. It gives S0w-Lw-Lp-SHIFT when
combined with the action SHIFT.

This new feature structure reformulates the SHIFT
action features only. The LEFTARC/ RIGHTARC ac-
tions remain LEFTARC/ RIGHTARC-LABEL since
they are not sparse. Note that the change is in the
action features rather than the actions themselves.
Given the bag of words {John, loves, Mary}, the ac-
tion SHIFT-John-NNP is still different from the ac-
tion SHIFT-Mary-NNP. However, the action compo-
nent of the features becomes SHIFT only, and the
words John/ Mary must be used as lookahead con-
figuration features for their disambiguation.

The new feature structure can reduce feature spar-
sity by allowing lookahead features without word
information. For example, a configuration feature
S0w-Lp, which contains only the stack top word and
the POS of the lookahead word, can still fire even
if the word to shift is OOV, thereby disambiguating
OOV words of different POS. In addition, the looka-
head Lw and Lp do not have to be combined with
every other configuration feature, as with Liu et al.
(2015), thereby allowing more flexible feature com-
bination and a leaner model.

3.3 The new features

The new feature structure includes two types of fea-
tures. The first is the same feature set as Liu et al.
(2015), but with the SHIFT action component not
having Word and POS information. We call this type
of features as base features. The second is a set
of lookahead features, which are shown in Table 2.
Here Lcls represents set of arc labels on child nodes
(of the word L to shift) that have been shifted on
to the stack, Lclns represents set of labels on child
nodes that have not been shifted, Lsls the label set
of shifted sibling nodes, Lslns the label set of un-
shifted sibling nodes, Lcps the POS set of shifted
child nodes, Lcpns the POS set of unshifted child
nodes, Lsps the POS set of shifted sibling nodes and
Lspns the POS set of unshifted sibling nodes. Lps
is a binary feature indicating if the parent has been
shifted. Llp represents label on the parent, Lpp POS
of parent and Lwp the parent word form. We define
similar lookahead features for S0. These features are
instantiated only for SHIFT actions.

The new feature structure prevents all possible ac-
tions from being scored simultaneously, because the
lookahead Word and POS are now in configuration
features, rather than output actions, making it neces-
sary to score the shifting of different words or POS
separately. This leads to reduced search speed. Nev-
ertheless, our experiments show that they give a de-
sirable tradeoff between efficiency and accuracy.

Note that the new features are much less than a
full Cartesian product of lookahead features and the
original features. This is a result of manual feature
engineering, which allows similar accuracies to be
achieved using a much smaller model, thereby in-
creasing the time efficiency.

491



unlabeled labeled
no pos all pos all pos all pos
no dep no dep all dep all dep

W09 - 33.7 - -
Z11 - 40.1 - -
Z13 44.7 46.8 76.2 89.3
L15 49.4 50.8 82.3 82.9
This paper 50.5 53.0 91.0 91.8

Table 4: Final results. W09 – Wann et al. (2009),
Z11 – Zhang and Clark (2011b)

4 Experiments

Following previous work we conduct experiments
on the Penn TreeBank (PTB), using Wall Street
Journal sections 2-21 for training, 22 for develop-
ment and 23 for testing. Gold-standard dependency
trees are derived from bracketed sentences using
Penn2Malt, and base noun phrases are treated as a
single word. The BLEU score is used to evaluate
the performance of linearization.

Table 4 shows a difference in scores between
transition-based linearization system of Liu et al.
(2015) (L15) and best-first system of Zhang (2013)
(Z13). L15 performs better for word ordering with
unlabeled dependency arcs, but poorly for the task
of labeled syntactic linearization.

Table 3 shows a series of development experi-
ments comparing our system with Z13 and L15.
We vary the amount of input syntactic constraints
by randomly sampling from POS and dependency
labels of the development set. Our system gives
consistently higher accuracies when compared with
both Z13 and L15. Compared to L15, the increase
in scores for unconstrained word ordering is due to
the introduction of reduced feature sparsity. The im-
provements on tree linearization tasks involving par-
tial to full dependency constraints are also due to
lookahead features that leverage tree information to
reduce ambiguity early. Though slower than L15,
our system is over 30 times faster compared to Z13.

We compare final test scores with previous meth-
ods in the literature in Table 4. Our system im-
proves upon the previous best scores by 8.7 BLEU
points for the task of unlabeled syntactic lineariza-
tion. For the task of labeled syntactic linearization,
we achieve the score of 91.8 BLEU points, the high-
est results reported so far.

Table 5 contains examples of fully constrained

Fully constrained output
ref. The spinoff also will compete with Fujitsu
L15 The spinoff with Fujitsu compete also will
Ours The spinoff also will compete with Fujitsu
ref. Dr. Talcott led a team of researchers from

the National Cancer Institute .
L15 a team of researchers from the National

Cancer Institute led Dr. Talcott .
Ours Dr. Talcott led a team of researchers from

the National Cancer Institute .

Table 5: Example outputs.

output . In the first example ‘will’ is the ROOT node
with two child nodes ‘also’ and ‘compete’. Looka-
head feature for child dependency labels Lcls, Lclns
on the node ‘will’ can help order the segment ‘also
will compete’ correctly in our system. Without such
features, the system of L15 yields an output that
starts with ‘The spinoff with Fujitsu’ which is locally
fluent, but leaving the words ‘also’ and ‘will’ diffi-
cult to handle. In the second example, ‘Dr. Talcott’
is OOV. Hence system of L15 is not able to score it
and thus order it correctly. Our system makes use of
both POS and dependency label of ‘Dr. Talcott’ to
order it correctly.

5 Conclusion

We identified a feature sparsity issue in state-of-the-
art transition-based word ordering, proposing a so-
lution by redefining the feature structure and intro-
ducing lookahead features. The new method gives
the best accuracies on a set of benchmarks, which
show that transition-based methods are a fast and
accurate choice for syntactic linearization. Future
work include the testing of this model in a lineariza-
tion shared task (Belz et al., 2011) and investigating
the integration of large scale training data (Zhang et
al., 2012; Liu and Zhang, 2015).

We release our source code under GPL at
https://github.com/SUTDNLP/ZGen/
releases/tag/v0.2.

Acknowledgments

We thank Yijia Liu for helpful discussions and for
sharing the Latex templates and the anonymous re-
viewers for their constructive comments. This work
was supported by the Singapore Ministry of Educa-
tion (MOE) AcRF Tier 2 grant T2MOE201301.

492



References

Anja Belz, Michael White, Dominic Espinosa, Eric Kow,
Deirdre Hogan, and Amanda Stent. 2011. The first
surface realisation shared task: Overview and evalu-
ation results. In Proceedings of the 13th European
workshop on natural language generation, pages 217–
226. Association for Computational Linguistics.

Graeme Blackwood, Adrià De Gispert, and William
Byrne. 2010. Fluency constraints for minimum bayes-
risk decoding of statistical machine translation lattices.
In Proceedings of the 23rd International Conference
on Computational Linguistics, pages 71–79. Associa-
tion for Computational Linguistics.

Bernd Bohnet, Leo Wanner, Simon Mille, and Alicia
Burga. 2010. Broad coverage multilingual deep sen-
tence generation with a stochastic multi-level realizer.
In Proceedings of the 23rd International Conference
on Computational Linguistics, pages 98–106. Associ-
ation for Computational Linguistics.

Danqi Chen and Christopher D Manning. 2014. A
fast and accurate dependency parser using neural net-
works. Proceedings of the 2014 Conference on
Empirical Methods in Natural Language Processing
(EMNLP), 1:740–750.

A De Gispert, M Tomalin, and W Byrne. 2014. Word or-
dering with phrase-based grammars. 14th Conference
of the European Chapter of the Association for Com-
putational Linguistics 2014, EACL 2014, pages 259–
268.

Katja Filippova and Michael Strube. 2007. Gener-
ating constituent order in german clauses. In An-
nual Meeting-Association for Computational Linguis-
tics, volume 45, page 320.

Wei He, Haifeng Wang, Yuqing Guo, and Ting Liu. 2009.
Dependency based chinese sentence realization. In
Proceedings of the Joint Conference of the 47th An-
nual Meeting of the ACL and the 4th International
Joint Conference on Natural Language Processing of
the AFNLP: Volume 2, pages 809–816.

Jiangming Liu and Yue Zhang. 2015. An empirical com-
parison between n-gram and syntactic language mod-
els for word ordering. In Proceedings of the 2015 Con-
ference on Empirical Methods in Natural Language
Processing, pages 369–378, Lisbon, Portugal, Septem-
ber. Association for Computational Linguistics.

Yijia Liu, Yue Zhang, Wanxiang Che, and Bing Qin.
2015. Transition-based syntactic linearization. In
NAACL HLT 2015, The 2015 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics: Human Language Technologies,
Denver, Colorado, USA, May 31 - June 5, 2015, pages
113–122.

Joakim Nivre and Mario Scholz. 2004. Deterministic
dependency parsing of english text. In Proceedings
of the 20th international conference on Computational
Linguistics, page 64. Association for Computational
Linguistics.

Linfeng Song, Yue Zhang, Kai Song, and Qun Liu.
2014. Joint morphological generation and syntactic
linearization. Proceedings of the Twenty-Eighth AAAI
Conference on Artificial Intelligence, pages 1522–
1528.

Stephen Wann, Mark Dras, Robert Dale, and Cécile Paris.
2009. Improving grammaticality in statistical sentence
generation: Introducing a dependency spanning tree
algorithm with an argument satisfaction model. In
Proceedings of the 12th Conference of the European
Chapter of the Association for Computational Linguis-
tics, pages 852–860.

Michael White and Rajakrishnan Rajkumar. 2009. Per-
ceptron reranking for ccg realization. In Proceedings
of the 2009 Conference on Empirical Methods in Nat-
ural Language Processing: Volume 1-Volume 1, pages
410–419. Association for Computational Linguistics.

Michael White. 2004. Reining in ccg chart realiza-
tion. In Natural Language Generation, pages 182–
191. Springer Berlin Heidelberg.

Yue Zhang and Stephen Clark. 2011a. Syntactic process-
ing using the generalized perceptron and beam search.
Computational linguistics, 37(1):105–151.

Yue Zhang and Stephen Clark. 2011b. Syntax-based
grammaticality improvement using ccg and guided
search. In Proceedings of the Conference on Empir-
ical Methods in Natural Language Processing, pages
1147–1157. Association for Computational Linguis-
tics.

Yue Zhang and Stephen Clark. 2015. Discriminative
syntax-based word ordering for text generation. Com-
putational Linguistics, 41(3):503–538.

Yue Zhang, Graeme Blackwood, and Stephen Clark.
2012. Syntax-based word ordering incorporating a
large-scale language model. In Proceedings of the
13th Conference of the European Chapter of the Asso-
ciation for Computational Linguistics, pages 736–746.
Association for Computational Linguistics.

Yue Zhang, Kai Song, Linfeng Song, Jingbo Zhu, and
Qun Liu. 2014. Syntactic smt using a discriminative
text generation model. In Proceedings of the 2014
Conference on Empirical Methods in Natural Lan-
guage Processing (EMNLP), pages 177–182, Doha,
Qatar, October. Association for Computational Lin-
guistics.

Yue Zhang. 2013. Partial-tree linearization: generalized
word ordering for text synthesis. In Proceedings of the
Twenty-Third international joint conference on Artifi-
cial Intelligence, pages 2232–2238. AAAI Press.

493


