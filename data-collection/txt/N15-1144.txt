



















































Unsupervised POS Induction with Word Embeddings


Human Language Technologies: The 2015 Annual Conference of the North American Chapter of the ACL, pages 1311–1316,
Denver, Colorado, May 31 – June 5, 2015. c©2015 Association for Computational Linguistics

Unsupervised POS Induction with Word Embeddings

Chu-Cheng Lin Waleed Ammar Chris Dyer Lori Levin
School of Computer Science
Carnegie Mellon University

{chuchenl,wammar,cdyer,lsl}@cs.cmu.edu

Abstract

Unsupervised word embeddings have been
shown to be valuable as features in supervised
learning problems; however, their role in unsu-
pervised problems has been less thoroughly ex-
plored. In this paper, we show that embeddings
can likewise add value to the problem of unsu-
pervised POS induction. In two representative
models of POS induction, we replace multi-
nomial distributions over the vocabulary with
multivariate Gaussian distributions over word
embeddings and observe consistent improve-
ments in eight languages. We also analyze the
effect of various choices while inducing word
embeddings on “downstream” POS induction
results.

1 Introduction

Unsupervised POS induction is the problem of as-
signing word tokens to syntactic categories given
only a corpus of untagged text. In this paper we ex-
plore the effect of replacing words with their vector
space embeddings1 in two POS induction models:
the classic first-order HMM (Kupiec, 1992) and the
newly introduced conditional random field autoen-
coder (Ammar et al., 2014). In each model, instead
of using a conditional multinomial distribution2 to
generate a word token wi ∈ V given a POS tag ti ∈ T ,
we use a conditional Gaussian distribution and gen-
erate a d-dimensional word embedding vwi ∈ Rd
given ti .

1Unlike Yatbaz et al. (2014), we leverage easily obtainable
and widely used embeddings of word types.

2Also known as a categorical distribution.

Our findings suggest that, in both models, sub-
stantial improvements are possible when word em-
beddings are used rather than opaque word types.
However, the independence assumptions made by
the model used to induce embeddings strongly deter-
mines its effectiveness for POS induction: embedding
models that model short-range context are more ef-
fective than those that model longer-range contexts.
This result is unsurprising, but it illustrates the lack
of an evaluation metric that measures the syntactic
(rather than semantic) information in word embed-
dings. Our results also confirm the conclusions of
Sirts et al. (2014) who were likewise able to improve
POS induction results, albeit using a custom clus-
tering model based on the the distance-dependent
Chinese restaurant process (Blei and Frazier, 2011).

Our contributions are as follows: (i) reparameter-
ization of token-level POS induction models to use
word embeddings; and (ii) a systematic evaluation
of word embeddings with respect to the syntactic
information they contain.

2 Vector Space Word Embeddings

Word embeddings represent words in a language’s
vocabulary as points in a d-dimensional space such
that nearby words (points) are similar in terms of their
distributional properties. A variety of techniques for
learning embeddings have been proposed, e.g., matrix
factorization (Deerwester et al., 1990; Dhillon et al.,
2011) and neural language modeling (Mikolov et al.,
2011; Collobert and Weston, 2008).

For the POS induction task, we specifically need
embeddings that capture syntactic similarities. There-
fore we experiment with two types of embeddings

1311



that are known for such properties:

• Skip-gram embeddings (Mikolov et al., 2013) are
based on a log bilinear model that predicts an un-
ordered set of context words given a target word.
Bansal et al. (2014) found that smaller context win-
dow sizes tend to result in embeddings with more
syntactic information. We confirm this finding in
our experiments.

• Structured skip-gram embeddings (Ling et al.,
2015) extend the standard skip-gram embeddings
(Mikolov et al., 2013) by taking into account the
relative positions of words in a given context.

We use the tool word2vec3 and Ling et al. (2015)’s
modified version4 to generate both plain and struc-
tured skip-gram embeddings in nine languages.

3 Models for POS Induction

In this section, we briefly review two classes of mod-
els used for POS induction (HMMs and CRF autoen-
coders), and explain how to generate word embed-
ding observations in each class. We will represent a
sentence of length ` as w = 〈w1,w2, . . . ,w`〉 ∈ V `
and a sequence of tags as t = 〈t1, t2, . . . , t`〉 ∈ T `.
The embeddings of word type w ∈ V will be written
as vw ∈ Rd .
3.1 Hidden Markov Models
The hidden Markov model with multinomial emis-
sions is a classic model for POS induction. This
model makes the assumption that a latent Markov pro-
cess with discrete states representing POS categories
emits individual words in the vocabulary according
to state (i.e., tag) specific emission distributions. An
HMM thus defines the following joint distribution
over sequences of observations and tags:

p(w, t) =
∏̀
i=1

p(ti | ti−1) × p(wi | ti ) (1)

where distributions p(ti | ti−1) represents the transi-
tion probability and p(wi | ti ) is the emission prob-
ability, the probability of a particular tag generating
the word at position i.5

We consider two variants of the HMM as baselines:
3https://code.google.com/p/word2vec/
4https://github.com/wlin12/wang2vec/
5Terms for the starting and stopping transition probabilities

are omitted for brevity.

• p(wi | ti ) is parameterized as a “naïve multino-
mial” distribution with one distinct parameter for
each word type.

• p(wi | ti ) is parameterized as a multinomial logis-
tic regression model with hand-engineered features
as detailed in (Berg-Kirkpatrick et al., 2010).

Gaussian Emissions. We now consider incorporat-
ing word embeddings in the HMM. Given a tag t ∈ T ,
instead of generating the observed word w ∈ V , we
generate the (pre-trained) embedding vw ∈ Rd of that
word. The conditional probability density assigned
to vw | t follows a multivariate Gaussian distribution
with mean µt and covariance matrix Σt :

p(vw ; µt ,Σt ) =
exp

(
− 12 (vw − µt )>Σ−1t (vw − µt )

)
√

(2π)d |Σt |
(2)

This parameterization makes the assumption that em-
beddings of words which are often tagged as t are
concentrated around some point µt ∈ Rd , and the
concentration decays according to the covariance ma-
trix Σt .6

Now, the joint distribution over a sequence of
observations v = 〈vw1 ,vw2 . . . ,vw` 〉 (which corre-
sponds to word sequence w = 〈w1,w2, . . . ,w`,〉) and
a tag sequence t = 〈t1, t2 . . . , t`〉 becomes:

p(v, t) =
∏̀
i=1

p(ti | ti−1) × p(vwi ; µti ,Σti )

We use the Baum–Welch algorithm to fit the µt
and Σti parameters. In every iteration, we update µt∗
as follows:

µnewt∗ =

∑
v∈T

∑
i=1...` p(ti = t∗ | v) × vwi∑

v∈T
∑

i=1...` p(ti = t∗ | v) (3)

where T is a data set of word embedding sequences
v each of length |v| = `, and p(ti = t∗ | v) is the
posterior probability of label t∗ at position i in the
sequence v. Likewise the update to Σt∗ is:

Σnewt∗ =

∑
v∈T

∑
i=1...` p(ti = t∗ | v) × δδ>∑

v∈T
∑

i=1...` p(ti = t∗ | v) (4)

where δ = vwi − µnewt∗ .
6“Essentially, all models are wrong, but some are useful.” —

George E. P. Box

1312



3.2 Conditional Random Field Autoencoders

The second class of models this work extends is
called CRF autoencoders, which we recently pro-
posed in (Ammar et al., 2014). It is a scalable family
of models for feature-rich learning from unlabeled
examples. The model conditions on one copy of the
structured input, and generates a reconstruction of
the input via a set of interdependent latent variables
which represent the linguistic structure of interest. As
shown in Eq. 5, the model factorizes into two distinct
parts: the encoding model p(t | w) and the recon-
struction model p(ŵ | t); where w is the structured
input (e.g., a token sequence), t is the linguistic struc-
ture of interest (e.g., a sequence of POS tags), and
ŵ is a generic reconstruction of the input. For POS
induction, the encoding model is a linear-chain CRF
with feature vector λ and local feature functions f.

p(ŵ,t | w) = p(t | w) × p(ŵ | t)

∝ p(ŵ | t) × expλ ·
|w |∑
i=1

f(ti , ti−1,w) (5)

In (Ammar et al., 2014), we explored two kinds of
reconstructions ŵ: surface forms and Brown clusters
(Brown et al., 1992), and used “stupid multinomials”
as the underlying distributions for re-generating ŵ.

Gaussian Reconstruction. In this paper, we use d-
dimensional word embedding reconstructions ŵi =
vwi ∈ Rd , and replace the multinomial distribution of
the reconstruction model with the multivariate Gaus-
sian distribution in Eq. 2. We again use the Baum–
Welch algorithm to estimate µt∗ and Σt∗ similar to
Eq. 3. The only difference is that posterior label prob-
abilities are now conditional on both the input se-
quence w and the embeddings sequence v, i.e., re-
place p(ti = t∗ | v) in Eq. 2 with p(ti = t∗ | w,v).

4 Experiments

In this section, we attempt to answer the following
questions:

• §4.1: Do syntactically-informed word embeddings
improve POS induction? Which model performs
best?

• §4.2: What kind of word embeddings are suitable
for POS induction?

4.1 Choice of POS Induction Models
Here, we compare the following models for POS
induction:

• Baseline: HMM with multinomial emissions (Ku-
piec, 1992),

• Baseline: HMM with log-linear emissions (Berg-
Kirkpatrick et al., 2010),

• Baseline: CRF autoencoder with multinomial re-
constructions (Ammar et al., 2014),7

• Proposed: HMM with Gaussian emissions, and

• Proposed: CRF autoencoder with Gaussian recon-
structions.

Data. To train the POS induction models, we used
the plain text from the training sections of the
CoNLL-X shared task (Buchholz and Marsi, 2006)
(for Danish and Turkish), the CoNLL 2007 shared
task (Nivre et al., 2007) (for Arabic, Basque, Greek,
Hungarian and Italian), and the Ukwabelana corpus
(Spiegler et al., 2010) (for Zulu). For evaluation, we
obtain the corresponding gold-standard POS tags by
deterministically mapping the language-specific POS
tags in the aforementioned corpora to the correspond-
ing universal POS tag set (Petrov et al., 2012). This
is the same set up we used in (Ammar et al., 2014).

Setup. In this section, we used skip-gram (i.e.,
word2vec) embeddings with a context window size
= 1 and with dimensionality d = 100, trained with
the largest corpora for each language in (Quasthoff
et al., 2006), in addition to the plain text used to train
the POS induction models.8 In the proposed models,
we only show results for estimating µt , assuming
a diagonal covariance matrix Σt (k, k) = 0.45∀k ∈
{1, . . . ,d}.9 While the CRF autoencoder with multi-
nomial reconstructions were carefully initialized as

7We use the configuration with best performance which re-
constructs Brown clusters.

8We used the corpus/tokenize-anything.sh script in
the cdec decoder (Dyer et al., 2010) to tokenize the corpora
from (Quasthoff et al., 2006). The other corpora were already
tokenized. In Arabic and Italian, we found a lot of discrepancies
between the tokenization used for inducing word embeddings
and the tokenization used for evaluation. We expect our results
to improve with consistent tokenization.

9Surprisingly, we found that estimating Σt significantly de-
grades the performance. This may be due to overfitting (Shi-
nozaki and Kawahara, 2007). Possible remedies include using a
prior (Gauvain and Lee, 1994).

1313



Arabic Basque Danish Greek Hungarian Italian Turkish Zulu Average

V
−m

ea
su

re

0.
0

0.
2

0.
4

0.
6

0.
8

Multinomial HMM
Multinomial Featurized HMM

Multinomial CRF Autoencoder
Gaussian HMM

Gaussian CRF Autoencoder

Arabic Basque Danish Greek Hungarian Italian Turkish Zulu Average

V
−m

ea
su

re

0.
0

0.
2

0.
4

0.
6

0.
8

HMM (standard skip−gram)
CRF Autoencoder (standard skip−gram)

HMM (structured skip−gram)
CRF Autoencoder (structured skip−gram)

Figure 1: POS induction results. (V-measure, higher is better.) Window size is 1 for all word embeddings.
Left: Models which use standard skip-gram word embeddings (i.e., Gaussian HMM and Gaussian CRF
Autoencoder) outperform all baselines on average across languages. Right: comparison between standard
and structured skip-grams on Gaussian HMM and CRF Autoencoder.

discussed in (Ammar et al., 2014), CRF autoencoder
with Gaussian reconstructions were initialized uni-
formly at random in [−1,1]. All HMM models were
also randomly initialized. We tuned all hyperparame-
ters on the English PTB corpus, then fixed them for
all languages.

Evaluation. We use the V-measure evaluation met-
ric (Rosenberg and Hirschberg, 2007) to evaluate the
predicted syntactic classes at the token level.10

Results. The results in Fig. 1 clearly suggest that
we can use word embeddings to improve POS induc-
tion. Surprisingly, the feature-less Gaussian HMM
model outperforms the strong feature-rich baselines:
Multinomial Featurized HMM and Multinomial CRF
Autoencoder. One explanation is that our word em-
beddings were induced using larger unlabeled cor-
pora than those used to train the POS induction mod-
els. The best results are obtained using both word em-
beddings and feature-rich models using the Gaussian
CRF autoencoder model. This set of results suggest
that word embeddings and hand-engineered features
play complementary roles in POS induction. It is
worth noting that the CRF autoencoder model with
Gaussian reconstructions did not require careful ini-
tialization.11

10We found the V-measure results to be consistent with the
many-to-one evaluation metric (Johnson, 2007). We only show
one set of results for brevity.

11In (Ammar et al., 2014), we found that careful initialization
for the CRF autoencoder model with multinomial reconstructions
is necessary.

4.2 Choice of Embeddings

Standard skip-gram vs. structured skip-gram.
On Gaussian HMMs, structured skip-gram embed-
dings score moderately higher than standard skip-
grams. And as context window size gets larger, the
gap widens (as shown in Fig. 2.) The reason may
be that structured skip-gram embeddings give each
position within the context window its own project
matrix, so the smearing effect is not as pronounced
as the window grows when compared to the standard
embeddings. However the best performance is still
obtained when window size is small.12

Dimensions = 20 vs. 200. We also varied the
number of dimensions in the word vectors (d ∈
{20,50,100,200}). The best V-measure we obtain
is 0.504 (d = 20) and the worst is 0.460 (d = 100).
However, we did not observe a consistent pattern as
shown in Fig. 3.

Window size = 1 vs. 16. Finally, we varied the win-
dow size for the context surrounding target words
(w ∈ {1,2,4,8,16}). w = 1 yields the best average
V-measure across the eight languages as shown in
Fig. 2. This is true for both standard and structured

12In preliminary experiments, we also compared standard skip-
gram embeddings to SENNA embeddings (Collobert et al., 2011)
(which are trained in a semi-supervised multi-task learning setup,
with one task being POS tagging) on a subset of the English
PTB corpus. As expected, the induced POS tags are much better
when using SENNA embeddings, yielding a V-measure score of
0.57 compared to 0.51 for skip-gram embeddings. Since SENNA
embeddings are only available in English, we did not include it
in the comparison in Fig. 1.

1314



1 2 4 8 16
Window size

av
g

. V
−m

ea
su

re
0.

30
0.

45
standard skip−gram structured skip−gram

Figure 2: Effect of window size and embeddings type
on POS induction over the languages in Fig. 1. d =
100. The model is HMM with Gaussian emissions.

skip-gram models. Notably, larger window sizes ap-
pear to produce word embeddings with less syntactic
information. This result confirms the observations of
Bansal et al. (2014).

4.3 Discussion

We have shown that (re)generating word embeddings
does much better than generating opaque word types
in unsupervised POS induction. At a high level, this
confirms prior findings that unsupervised word em-
beddings capture syntactic properties of words, and
shows that different embeddings capture more syn-
tactically salient information than others. As such,
we contend that unsupervised POS induction can be
seen as a diagnostic metric for assessing the syntactic
quality of embeddings.

To get a better understanding of what the multi-
variate Gaussian models have learned, we conduct a
hill-climbing experiment on our English dataset. We
seed each POS category with the average vector of
10 randomly sampled words from that category and
train the model. Seeding unsurprisingly improves tag-
ging performance. We also find words that are the
nearest to the centroids generally agree with the cor-
rect category label, which validate our assumption
that syntactically similar words tend to cluster in the
high-dimensional embedding space. It also shows
that careful initialization of model parameters can
bring further improvements.

However we also find that words that are close
to the centroid are not necessarily representative of
what linguists consider to be prototypical. For exam-
ple, Hopper and Thompson (1983) show that physical,
telic, past tense verbs are more prototypical with re-
spect to case marking, agreement, and other syntactic

20 50 100 200
Dimension size

V
−m

ea
su

re
0.

30
0.

45

Figure 3: Effect of dimension size on POS induction
on a subset of the English PTB corpus. w = 1. The
model is HMM with Gaussian emissions.

behavior. However, the verbs nearest our centroid all
seem rather abstract. In English, the nearest 5 words
in the verb category are entails, aspires, attaches,
foresees, deems. This may be because these words
seldom serve functions other than verbs; and plac-
ing the centroid around them incurs less penalty (in
contrast to physical verbs, e.g. bite, which often also
act as nouns). Therefore one should be cautious in
interpreting what is prototypical about them.

5 Conclusion

We propose using a multivariate Gaussian model to
generate vector space representations of observed
words in generative or hybrid models for POS induc-
tion, as a superior alternative to using multinomial
distributions to generate categorical word types. We
find the performance from a simple Gaussian HMM
competitive with strong feature-rich baselines. We
further show that substituting the emission part of the
CRF autoencoder can bring further improvements.
We also confirm previous findings which suggest
that smaller context windows in skip-gram models
result in word embeddings which encode more syn-
tactic information. It would be interesting to see if we
can apply this approach to other tasks which require
generative modeling of textual observations such as
language modeling and grammar induction.

Acknowledgements

This work was sponsored by the U.S. Army Research
Laboratory and the U.S. Army Research Office un-
der contract/grant numbers W911NF-11-2-0042 and
W911NF-10-1-0533. The statements made herein are
solely the responsibility of the authors.

1315



References
Waleed Ammar, Chris Dyer, and Noah A. Smith. 2014.

Conditional random field autoencoders for unsuper-
vised structured prediction. In NIPS.

Mohit Bansal, Kevin Gimpel, and Karen Livescu. 2014.
Tailoring continuous word representations for depen-
dency parsing. In Proc. of ACL.

Taylor Berg-Kirkpatrick, Alexandre Bouchard-Côté, John
DeNero, and Dan Klein. 2010. Painless unsupervised
learning with features. In Proc. of NAACL.

David M. Blei and Peter I. Frazier. 2011. Distance depen-
dent Chinese restaurant processes. JMLR.

Peter F. Brown, Peter V. deSouza, Robert L. Mercer, Vin-
cent J. Della Pietra, and Jenifer C. Lai. 1992. Class-
based n-gram models of natural language. Computa-
tional Linguistics.

Sabine Buchholz and Erwin Marsi. 2006. CoNLL-X
shared task on multilingual dependency parsing. In
CoNLL-X.

Ronan Collobert and Jason Weston. 2008. A unified ar-
chitecture for natural language processing: Deep neural
networks with multitask learning. In Proc. of ICML.

Ronan Collobert, Jason Weston, Léon Bottou, Michael
Karlen, Koray Kavukcuoglu, and Pavel Kuksa. 2011.
Natural language processing (almost) from scratch.
JMLR, 12:2493–2537, November.

Scott Deerwester, Susan T. Dumais, George W. Furnas,
Thomas K. Landauer, and Richard Harshman. 1990.
Indexing by latent semantic analysis. Journal of the
American society for information science, 41(6):391–
407.

Paramveer S. Dhillon, Dean Foster, and Lyle Ungar. 2011.
Multi-view learning of word embeddings via CCA. In
NIPS, volume 24.

Chris Dyer, Adam Lopez, Juri Ganitkevitch, Johnathan
Weese, Ferhan Ture, Phil Blunsom, Hendra Setiawan,
Vladimir Eidelman, and Philip Resnik. 2010. cdec: A
decoder, alignment, and learning framework for finite-
state and context-free translation models. In Proc. of
ACL.

J. Gauvain and Chin-Hui Lee. 1994. Maximum a pos-
teriori estimation for multivariate Gaussian mixture
observations of Markov chains. Speech and Audio Pro-
cessing, IEEE Transactions on, 2(2):291–298, Apr.

Paul Hopper and Sandra Thompson. 1983. The iconicity
of the universal categories “noun” and “verb”. In John
Haiman, editor, Iconicity in Syntax: Proceedings of a
symposium on iconicity in syntax.

Mark Johnson. 2007. Why doesn’t EM find good HMM
POS-taggers? In Proc. of EMNLP.

Julian Kupiec. 1992. Robust part-of-speech tagging us-
ing a hidden Markov model. Computer Speech and
Language, 6:225–242.

Wang Ling, Chris Dyer, Alan Black, and Isabel Trancoso.
2015. Two/too simple adaptations of word2vec for
syntax problems. In Proc. of NAACL.

Tomas Mikolov, Stefan Kombrink, Anoop Deoras, Lukar
Burget, and J Cernocky. 2011. RNNLM — recurrent
neural network language modeling toolkit. In Proc. of
the 2011 ASRU Workshop, pages 196–201.

Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey
Dean. 2013. Efficient estimation of word represen-
tations in vector space. ArXiv e-prints, January.

Joakim Nivre, Johan Hall, Sandra Kubler, Ryan McDonald,
Jens Nilsson, Sebastian Riedel, and Deniz Yuret. 2007.
The CoNLL 2007 shared task on dependency parsing.
In Proc. of CoNLL.

Slav Petrov, Dipanjan Das, and Ryan McDonald. 2012.
A universal part-of-speech tagset. In Proc. of LREC,
May.

Uwe Quasthoff, Matthias Richter, and Christian Biemann.
2006. Corpus portal for search in monolingual corpora.
In Proc. of LREC, pages 1799–1802.

Andrew Rosenberg and Julia Hirschberg. 2007. V-
measure: A conditional entropy-based external cluster
evaluation measure. In EMNLP-CoNLL.

T. Shinozaki and T. Kawahara. 2007. HMM training
based on CV-EM and CV gaussian mixture optimiza-
tion. In Proc. of the 2007 ASRU Workshop, pages 318–
322, Dec.

Kairit Sirts, Jacob Eisenstein, Micha Elsner, and Sharon
Goldwater. 2014. POS induction with distribu-
tional and morphological information using a distance-
dependent Chinese restaurant process. In Proc. of ACL.

Sebastian Spiegler, Andrew van der Spuy, and Peter A.
Flach. 2010. Ukwabelana: An open-source morpho-
logical Zulu corpus. In Proc. of COLING, pages 1020–
1028.

Mehmet Ali Yatbaz, Enis Rıfat Sert, and Deniz Yuret.
2014. Unsupervised instance-based part of speech in-
duction using probable substitutes. In Proc. of COL-
ING.

1316


