



















































Multilingual Modal Sense Classification using a Convolutional Neural Network


Proceedings of the 1st Workshop on Representation Learning for NLP, pages 111–120,
Berlin, Germany, August 11th, 2016. c©2016 Association for Computational Linguistics

Multilingual Modal Sense Classification using a Convolutional Neural
Network

Ana Marasović and Anette Frank
Research Training Group AIPHES

Department of Computational Linguistics
Heidelberg University

69120 Heidelberg, Germany
{marasovic,frank}@cl.uni-heidelberg.de

Abstract

Modal sense classification (MSC) is a
special WSD task that depends on the
meaning of the proposition in the modal’s
scope. We explore a CNN architecture
for classifying modal sense in English and
German. We show that CNNs are superior
to manually designed feature-based clas-
sifiers and a standard NN classifier. We
analyze the feature maps learned by the
CNN and identify known and previously
unattested linguistic features. We bench-
mark the CNN on a standard WSD task,
where it compares favorably to models us-
ing sense-disambiguated target vectors.

1 Introduction

Factuality recognition (de Marneffe et al., 2012;
Lee et al., 2015) is a subtask in information extrac-
tion that differentiates facts from hypotheses and
speculation, expressed through signals of modal-
ity, most prominently, modal verbs and adverbs.
Modal verbs are, however, ambiguous between an
epistemic sense (possibility) as opposed to non-
epistemic deontic (permission/obligation) or dy-
namic (capability) senses, as in: He could be at
home (epistemic), You can enter now (deontic) and
Only John can solve this problem (capability).

Modal sense classification (MSC) is a special
case of sense disambiguation that is also relevant
in areas of dialogue act and plan recognition in AI,
as well as novel tasks such as argumentation min-
ing. Prior work (Ruppenhofer and Rehbein, 2012;
Zhou et al., 2015) addressed the task with feature-
based classification. However, even with carefully
designed semantic features the models have diffi-
culties beating the majority sense baseline in cases
of difficult sense distinctions and when applying
the models to heterogenous text genres.

We cast modal sense classification as a novel se-
mantic sentence classification task using a convo-
lutional neural network (CNN) architecture. Our
contributions are: (i) our experiments on MSC
confirm the adequacy of CNNs for modeling
propositions in semantic sentence classification
tasks (cf. Kim (2014)); (ii) we show that automati-
cally learned features in a CNN outperform manu-
ally designed features for difficult modal verbs and
novel genres; (iii) we demonstrate that the CNN
approach can be generalized across languages, by
adapting the model to German. (iv) We offer in-
sights into the linguistic properties captured by the
learned feature maps. Finally, (v) we benchmark
the CNN on a standard WSD task, comparing it
to a WSD model using rich sense-disambiguated
embeddings and obtain comparable results.

2 Prior and related work

Modal sense classification (MSC). We focus
on disambiguation of modal verbs, adopting the
sense inventory established in formal seman-
tics: epistemic, deontic/bouletic and circumstan-
tial/dynamic.1 We compare to prior work in Rup-
penhofer and Rehbein (2012) and follow-up work
in Zhou et al. (2015) (henceforth, R&R and Z+).
R&R induced modal sense classifiers from man-
ual annotations on the MPQA corpus (Wiebe et al.,
2005) using word-based and syntactic features. Z+
propose an extended semantically informed model
that significantly outperforms R&R’s results. Z+
also create heuristically sense-annotated training
data from parallel corpora, to overcome sparsity
and bias in the MPQA corpus. However, their
models do not beat the majority sense baseline for
the difficult modal verbs, may, can and could.

1These senses correspond to (Baker et al., 2010)’s modal
categories (with deontic split into requirement and permis-
sive), and R&Rs inventory, with regrouping of concessive,
conditional and circumstantial, cf. Zhou et al. (2015).

111



Modal sense classification interacts with genre
and domain differences. Prabhakaran et al.
(2012) observe strong cross-genre effects and
missing generalization capacities when applying
their modality classifier to out-of-domain genres.

Word Embeddings and Sense Disambiguation.
Taghipour and Ng (2015) investigate the impact
of word embeddings on classical WSD, using pre-
trained embeddings and tuning them to the task us-
ing a NN. Both variants, integrated into the state-
of-the-art system IMS (Zhong and Ng, 2012), im-
prove WSD performance on benchmark tasks.

Ordinary word embeddings do not differenti-
ate word senses. Rothe and Schütze (2015) ex-
plore supervised WSD using sense-specific em-
beddings, which they induce by exploiting sense
encodings and constraints given by a lexical re-
source.2 Integrating the sense-specific vectors into
IMS yields significant improvements and small
gains relative to Taghipour and Ng (2015). Hence,
word embeddings – tuned to the task or sense-
specific – prove beneficial for supervised WSD.

The CNN approach we investigate in our work
does not employ a fixed feature space or a pre-
defined window around the target word. It flexi-
bly learns feature maps for variable window sizes
over the embedding matrix for the full sentence. In
contrast to Rothe and Schütze (2015), embeddings
used by our CNNs models are knowledge-lean and
do not encode senses of the target words.

Sentence classification using CNNs. Recent
work investigates NN architectures and their abil-
ity to capture the semantics of sentences for vari-
ous classification tasks. Kalchbrenner et al. (2014)
construct a dynamic CNN that builds on unparsed
input and achieves performance beyond strong
baselines for sentiment and question type classi-
fication. By contrast, recursive neural networks
(Socher et al., 2013) take parsed input, recursively
generate representations for intermediate phrases,
and perform classification on the basis of the full
sentence representation.

Kim (2014) evaluates a one-layer CNN on var-
ious benchmark tasks for sentence classification.
CNNs trained on pre-trained (static) embeddings
perform well and can be further improved by tun-
ing them to the task (non-static). Using two chan-

2Modal verbs are not or not systematically covered in
WordNets or VerbNet; FrameNet relates modal verbs to their
predominant sense only. Also, FrameNet’s frame-to-frame
relations are known to lack coverage (Burchardt et al., 2009).

nels did not significantly improve results. Overall,
the CNNs show consistently strong performance,
improving on state-of-the-art results in 4 out of 7
tasks, i.a., sentiment and opinion classification.

3 A CNN for modal sense classification

We aim at a NN approach to MSC that (i) im-
proves over existing feature-based classifiers, (ii)
alleviates manual crafting of features, (iii) gener-
alizes over various text genres, and (iv) is easily
portable to novel languages. Besides this, MSC is
a special kind of WSD, in that modal verbs have a
restricted sense inventory shared across languages,
and act as operators that take a full proposition
as argument. We thus cast MSC as a semantic
sentence classification task in a CNN architecture,
adopting the one-layer CNN model of Kim (2014),
a variant of Collobert et al. (2011). Unlike Kim
(2014) we will use only one channel, but experi-
ment with various types of word vectors.

A CNN represents a sentence with a fixed size
vector, passed to classifier to classify the sentence
into task-specific target categories. In our case, it
will classify sentences into three modal sense cate-
gories. The input layer is a matrix x ∈ Rs×d, with
each row corresponding to a d-dimensional word
embedding xi ∈ Rd of a word in the sentence
of length s. Word embeddings can be randomly
initialized or pre-trained vectors, e.g. word2vec
(Mikolov et al., 2013) or dependency-based (Levy
and Goldberg, 2014) embeddings. Based on the
input layer, a CNN builds up one or more convo-
lutional layers. A convolution is an operation be-
tween sub-matrices of the input matrix x ∈ Rs×d
and a filter parametrised by a weight matrix w ∈
Rn×d, that returns a vector usually referred to as
a feature map. Formally, let xi−n+1:i be the sub-
matrix of the input matrix x from the (i−n+1)-th
row to the i-th row and let 〈. , .〉F denote the sum
of elements of the component-wise inner product
of two matrices, known as Frobenius inner prod-
uct. The i-th component of the feature map c is
obtained by taking the Frobenius inner product of
the sub-matrix xi−n+1:i with the filter matrix w

ci = 〈xi−n+1:i,w〉F , (1)
for i ∈ {n, . . . , s}3. Afterwards, we add a bias
term, b ∈ R to every component of the feature
map and apply an activation function f ,

c̃i = f(ci + b) . (2)
3We apply the narrow type of convolution.

112



Finally, max-over-time pooling (Collobert et al.,
2011) is applied over a single feature map that ex-
tracts the maximum value ĉ = max{c̃}, which
represents the chosen feature for this feature map.
Like Kim (2014) we don’t use just one filter as
described, but multiple filters with different re-
gion sizes n, resulting in multiple feature maps.
Features obtained through max-pooling from each
feature map are concatenated to a vector represen-
tation of the input sentence that is passed to the
softmax layer. Parameters to learn are elements of
the filter matrices and the input matrix when word
vectors are tuned.

Filters are trained to be especially active when
they encounter a sequence of words relevant for
the given classification task. Kalchbrenner et al.
(2014) present n-grams of different feature de-
tectors that capture positive or negative sentiment
phrases, and also more abstract semantic cate-
gories, such as negation or degree particles (’too’)
that are relevant in compositional sentiment detec-
tion. In the modal sense classification task, we
expect the feature maps to capture semantic cat-
egories found to be relevant in prior work, such
as tense, aspectual classes, negation and seman-
tic properties of verbs and phrases. Moreover,
prior work has shown that MSC profits from fea-
tures that model the wider syntactic context, esp.
subject and embedded verb and their semantics
(abstractness, semantic class, aspect, tense). Ex-
plicit modeling of these features as in Z+ improves
performance, but requires feature design for each
new language. Also, modeling semantic features
through lexical resources is subject to sparsity, and
relying on parsed input leads to lack of robustness.

Given that MSC profits from semantic features
in the wider syntactic context, we expect that a
CNN that applies filters of variable sizes to vari-
ous regions of the sentence to learn feature maps
can capture diverse linguistic features, and of-
fers greater flexibility compared to a conventional
WSD model with a fixed window size centered
around the target word. To investigate these spe-
cial properties of the CNN model, we test it on En-
glish and German data. While in English, subject,
modal and embedded verb are in a close syntactic
context, in German, they can be distributed over
wider distances, and the feature maps are expected
to capture properties over wider distances.

We perform experiments for MSC for English
and German, using various data sets. Section 4

presents the data, experimental settings and the
model variations we investigate. We perform de-
tailed quantitative and qualitative evaluation of our
experimental results. In Section 5, we evaluate the
CNN approach in a lexical sample WSD task, to
benchmark its performance on a well-studied data
set, and to investigate the potential advantage of
learning feature maps based on flexible window
sizes. To our knowledge, this constitutes the first
attempt to apply a CNN model in a WSD task.

4 Modal sense classification

4.1 Data
Our experiments are based on three data sets.
Their basic composition is given in Table 1.4

1) MPQA + EPOSE The English benchmark
data set MPQA from R&R was further enriched
through balanced heuristically tagged training
data, EPOSE , by Z+. The EPOSE data set was
obtained using a cross-lingual sense projection
approach. Z+ identified paraphrases for modal
senses (e.g. brauchen-need; erlauben-permit for
deontic, schaffen-able to for dynamic sense), ex-
tracted sentences from a parallel corpus with a
modal verb aligned to a sense-identifying para-
phrase, and tagged them with the identified modal
sense. Z+ measured 0.92 accuracy on 420 in-
stances of the heuristically tagged corpora. To
alleviate distributional bias stemming from the
MPQA dataset, Z+ balanced the blend of MPQA
with EPOSE using under- and oversampling. We
experiment with both versions (± balanced).5

2) MASC A subset of the multi-genre corpus
MASC (Ide et al., 2008), consisting of 19 genres
was manually annotated (Anonymous) with modal
senses for the same modal verbs. The annotated
data consists of ≈100 instances for each genre.6

3) EPOSG Following the method of Z+, we con-
structed a German data set EPOSG from the Eu-
roparl and OpenSubtitles corpora of OPUS (Tiede-
mann, 2012) by projecting modal sense categories
from English to German, using selected modal
sense identifying English paraphrases. The result-
ing corpus with sense-tagged German modal verbs

4More detailed information will be provided through ac-
companying material with the final version. The annotated
MASC and EPOSG data sets will be made publicly available.

5Their data is publicly available through their website. We
omit shall from MPQA, due to low number of occurrences.

6Exceptions with less than 100 instances are journal,
newspaper, technical, travel guides, and telephone.

113



can could may must should

MPQA
ep 2 156 130 11 26
de 115 17 9 83 248
dy 271 67 – – –

EPOSE
ep 150 40 950 800 150
de 150 40 950 800 150
dy 150 40 – – –

MASC
ep 88 144 217 29 27
de 72 16 43 115 224
dy 710 251 3 – –

dürfen können müssen sollen

EPOSG (train)
ep 1000 1000 1000 1000
de 1000 1000 1000 1000
dy – 1000 – –

EPOSG (test)
ep 98 100 32 100
de 98 47 100 100
dy – 100 – –

Table 1: Composition of MPQA, EPOSE , MASC
and EPOSG

können (can), müssen (must), sollen (should),
dürfen (may) consists of a manually validated test
section consisting of up to 100 instances for each
sense. Annotation was done by two independent
judges and one adjudicator. Balanced training data
of 1000 instances per sense for each modal verb
was constructed from heuristically tagged sen-
tences that were judged high-quality by validating
20 instances for each paraphrase. For modal verbs
with rare extractions, we added training data from
modal verbs of shared senses, changing their verb
forms to the verb form of the target verb.7

4.2 Experimental settings
MSC on MPQA using CNN-EB and CNN-EU,
CV For MSC we benchmark the CNN approach
against the latest state-of-the-art results in Z+.
We reimplemented their maximum entropy clas-
sifier (henceforth, MaxEnt) and trained it on their
balanced and unbalanced blend of MPQA and
EPOS.8 As in Z+ we train independent classifiers
for each modal verb on their respective training
data.9 For evaluation, we perform 5-fold cross val-
idation as in Z+. Each fold for training holds a
stratified 80% section of the MPQA data together
with the full EPOSE data set, and we use the re-
maining 20% of MPQA data for testing. We refer
to the CNN models trained on the ±balanced ver-
sions of this data as CNN-EB and CNN-EU.

MSC on MASC using CNN-EB and CNN-EU
Besides MPQA, we evaluate the CNN on the

7Replacing e.g. könnte with dürfte in Es könnte Dir
gefallen extracted from You might get a taste for it.

8We omit shall with a small number of instances.
9This holds for all our experiments.

multi-genre MASC (sub)corpus. For compara-
bility with Z+, for training we use one training
fold from the previous setting,10 and evaluate on
MASC as test. We analyze the performance of the
CNN model overall and on different genre subcor-
pora (not reported here).

Both English data sets are characterized by
modest training set sizes and involve a consider-
able distributional biases, with high most frequent
sense majority baselines (cf. Tables 3 and 4).

MSC on EPOSG using CNN-G In constrast to
the English data sets, the German EPOSG data set
provides larger training set sizes of 1000 instances
for all modal verbs and senses. This eliminates
distributional bias from the data, so that the dis-
criminating power of the classifier model is not
masqued by distributional information.

4.3 Model variations
Hyperparameters Model-specific hyperparam-
eters of the CNN are the number of filters, filter
region size, and the depth of the network. We re-
strict our model to a one-dimensional CNN archi-
tecture.

Following the advices in Zhang and Wallace
(2015), we used following setting: ReLU (recti-
fied linear unit) as activation function, filter re-
gion sizes of 3, 4, and 5 with 100 feature maps
each, dropout keep probability of 0.5, l2 regulari-
sation coefficient of 10−3, number of iterations of
100111 and mini-batch size of 50. Training is done
with the Adam optimisation algorithm (Kingma
and Ba, 2014) with learning rate of 10−4. Filter
weights are initialized using Glorot-Bengio strat-
egy (Glorot and Bengio, 2010). We experimented
with some parameter variations (using nested CV),
but found no consistently better results. In all fol-
lowing MSC experiments we thus used this hyper-
parameter setting for CNN training.

Word embeddings In the first and third ex-
perimental setting we investigate the impact of
static and tuned versions of different word vectors:
word2vec (Mikolov et al., 2013), dependency-
based (Levy and Goldberg, 2014) and randomly
initialized embeddings.

We used publicly available word2vec vec-
tors that were trained on Google News for En-

10Hence, one 80% fold of MPQA plus EPOSE . Despite
this small difference, we refer to the CNN models as above,
as CNN-EB and CNN-EU.

11We did not perform early stopping.

114



glish12 and various datasets for German (Reimers
et al., 2014)13, as well as English dependency-
based vectors trained on Wikipedia14. The Ger-
man dependency-based embeddings were trained
on the SdeWaC corpus (Faaß and Eckart, 2013),
parsed with Malt parser. We used 300 dimensions
for English embeddings and 100 for German.

For words without a pre-trained vector and in
the random initialization setting, each dimension
of the random vector was sampled from U ∼
[−a, a] with parameter a picked such that the vari-
ance of the uniform distribution equals the vari-
ance of the available pre-trained vectors.

Baselines For MPQA and MASC, the classifiers
are compared against strong majority sense base-
lines, BLmaj , due to skewed sense distributions in
the training data. Further, we compare the CNN
results to the reconstructed MaxEnt classifier from
Z+, trained on the blend of MPQA and EPOS with
R&R’s shallow lexical and syntactic path features
and the newly designed semantic features of Z+.

To our knowledge, there is no work on modal
sense classification using a neural network. We
thus compare our CNN models with a simple, one-
layer neural network NN to investigate the impact
offered by the more complex CNN architecture.

Input to the NN is the sum of all vectors of the
words in the sentence. As for the CNN, we exper-
imented with different types of word vectors.

The hyperparameter setting for the NN is:
ReLU as activation function, l2 regularisation co-
efficient of 10−3, hidden layer size of 1024, num-
ber of iterations of 3001, dropout keep probabil-
ity of 0.5, and mini-batch size of 50. Training is
again done with the Adam optimisation algorithm
(Kingma and Ba, 2014) with learning rate of 10−4.
Weights are initialized using Glorot-Bengio strat-
egy (Glorot and Bengio, 2010).15

4.4 Results
English
In Table 2 we report results for CNN-EB and
CNN-EU with diverse input representations. For
balanced training, dependency based vectors yield
the best (can, could) or equally good results (may,

12https://code.google.com/archive/p/word2vec
13https://www.ukp.tu-darmstadt.de/research/ukp-in-

challenges/germeval-2014
14https://levyomer.wordpress.com/2014/04/25/dependency-

based-word-embeddings
15This is clearly not shown to be the best hyperparameter

setting, as we chose it heuristically without tuning.

CNN-EB can could may must should

w2v-static 65.02 51.67 93.57 93.82 90.77
w2v-tuned 63.73 54.17 93.57 93.82 90.77
dep-static 65.78 56.67 93.57 93.82 90.77
dep-tuned 59.89 67.50 93.57 93.29 90.42
rand-static 63.99 46.67 93.57 92.79 90.77
rand-tuned 64.50 48.33 93.57 92.79 90.77

CNN-EU can could may must should

w2v-static 70.10 65.27 93.49 94.97 90.59
w2v-tuned 70.62 66.10 93.49 94.97 90.59
dep-static 69.85 65.27 93.49 94.46 90.59
dep-tuned 69.59 66.55 93.49 93.95 90.59
rand-static 70.36 64.45 93.49 93.45 90.59
rand-tuned 70.87 64.86 93.49 93.45 90.59

CNN-G dürfen können müssen sollen

w2v-static 91.92 68.82 77.61 71.64
w2v-tuned 99.49 74.09 83.58 72.14
dep-static 91.92 63.56 75.37 73.13
dep-tuned 97.47 73.28 82.83 74.63
rand-static 96.46 77.33 81.34 74.13
rand-tuned 98.48 78.95 85.07 73.63

Table 2: CV accuracy for CNN-EB, CNN-EU, test
accuracy for CNN-G, with different input repre-
sentations.

must, should). Could is the only case with large
performance differences depending on the choice
of embeddings. For can and could choosing ei-
ther static or tuned versions of vectors is benefi-
cial. With unbalanced training, dependency-based
vectors are outperformed by word2vec for must
and by randomly initialized vectors for can. Large
differences in the results for could w.r.t. the choice
of embeddings, are no longer present.

In Table 3 we report overall results for CNN-
EB and CNN-EU on MPQA compared to the base-
lines. As representations for the NN and CNN we
selected, for each modal verb, the embedding type
that yielded the best results (Table 2)16.

For each training data set, scores of the CNN
which are significantly better17 than the next lower
score among the baselines are underlined. If CNN
does not yield the best results, significance be-
tween the baseline with the best score and CNN is
reported. Overlining is used if CNN with unbal-
anced training performs significantly better than
CNN with balanced training, and vice versa.

With balanced training, CNN outperforms all
baselines for every modal verb and in terms of mi-
cro average. However, differences between CNN

16For NN the impact of word vectors was investigated as
well.

17By conducting the mid-p-value McNemar test (Fager-
land et al., 2013) with p <0.05.

115



can could may must should micro

BLrand 33.33 33.33 50.00 50.00 50.00 41.49
MaxEnt 59.64 61.25 92.14 87.60 90.11 74.88

NN 56.01 55.42 90.00 75.24 88.68 69.74

CNN-EB 65.78 67.50 93.57 93.82 90.77 79.29

can could may must should micro

BLmaj 69.92 65.00 93.57 94.32 90.81 80.18
MaxEnt 64.76 63.33 92.14 92.78 91.48 78.01

NN 67.29 66.08 94.23 86.37 90.96 77.93

CNN-EU 70.87 66.55 93.49 94.97 90.59 80.74

Table 3: Comparison of CV accuracies on MPQA of CNN-
EB (upper table) and CNN-EU (lower table) with baselines.

and MaxEnt are significant only for can, could
and micro average. Moving to unbalanced train-
ing, CNN has difficulties beating the baselines (cf.
may, should), but yields the best micro average.
Unbalanced training for CNN outperforms bal-
anced training in terms of micro averages, how-
ever the difference is not significant.

Table 4 summarizes the evaluation of CNN-EB
and CNN-EU on the MASC corpus. Note that
CNN with unbalanced training, CNN-EU, does not
have enough generalization capability when ap-
plied to different genres. This behavior coincides
with changes of the predominant sense between
training and test. CNN-EU, as well as MaxEnt,
is highly sensitive to such distributional changes.
Even though balanced training for CNN leads to
a slightly worse micro average when evaluated on
MPQA, on MASC CNN–EB yields a +3pp gain in
micro average compared to unbalanced training.18

In sum, our evaluation shows that the CNN
model is able to outperform strong baselines in
most configurations. Balanced training shows
more consistent results beyond the baselines and
is competitive with unbalanced training, without
significant difference except for can. In view of
genre differences in MASC, the CNN–EB model
is more robust against sense changes, and yields
overall better results. The strong behaviour on bal-
anced training data shows that the CNN model is
able to learn meaningful structure from the data.

German
In Table 2 we report results for CNN-G with
diverse input representations. Reasons for the
slightly weaker performance of dependency-based
vectors compared to word2vec (1-2 pp.) can be

18In contrast to MaxEnt, which does not profit from bal-
anced training.

can could may must should micro

BLrand 33.52 33.82 48.67 46.87 46.01 38.63
MaxEnt 66.74 62.86 87.83 83.33 84.06 72.25

CNN-EB 80.46 64.48 86.69 84.72 88.84 79.33

can could may must should micro

BLmaj 81.61 35.04 82.51 79. 86 89.24 72.86
MaxEnt 73.17 55.34 87.45 86.11 89.64 74.41

CNN-EU 81.03 49.15 86.31 86.80 89.24 76.49

Table 4: Accuracies on MASC dataset of classi-
fiers trained on MPQA+EPOSE .

dürfen können müssen sollen micro

BLrand 50.00 33.33 50.00 50.00 39.10
NN 77.73 43.32 73.88 50.25 57.69

CNN-G 99.49 78.95 85.07 74.63 84.10

Table 5: Average accuracy on EPOSG.

seen in the smaller size of the training corpus, and
possibly greater noise due to parsing errors.

In Table 5 we report overall results for CNN-G
compared to the NN baseline.19 The CNN outper-
forms both baselines by large margins, per modal
verb and in terms of micro average. Given we em-
ployed perfectly balanced training data, the classi-
fier performances reflect their ability to learn char-
acteristic information for the classes. Indeed, the
NN has great difficulties distinguishing the senses
for können (3 senses) and sollen, and is outper-
formed by CNN-G by +35.6 and +24.4 pp. gains.
The confusion matrices for CNN-G show a clear
separation of these classes, in contrast to the NN.

While German is a more difficult language than
English due to its syntactic properties (word or-
der, degree of inflection), CNN-G reaches overall
higher performance levels compared to English,
especially for difficult cases.20 One reason can be
the morphological distinction between indicative
and subjunctive (Konjunktiv), which – in interac-
tion with tense and other factors – can ease the dis-
tinction of epistemic vs. deontic/dynamic sense.
For sollen this morphological division is masqued,
and this can explain the weaker results compared
to other binary classes. Generally, CNN-G profits
from larger and perfectly balanced training data.

19We did not construct a MaxEnt classifier for German.
For NN and CNN-G we chose the best performing embed-
ding types per modal verb.

20Clearly, we cannot draw any strict comparison here.

116



4.5 Semantic feature detectors

Z+ provided a thorough analysis of the impact of
semantic features by ablating individual feature
groups. Their ablation analysis confirmed that fea-
ture groups relating to tense and aspect of the em-
bedded verb, negation, abstractness of the subject
and semantic features of the embedded verb yield
significant effects on classification performance.

For must, Z+ found clear patterns for the occur-
rence of specific features and the ability to prop-
erly classify a specific sense. However, they did
not identify precise features that differentiate epis-
temic and dynamic readings with can. We spefi-
cically investigated whether the learned filters for
must can be related to the semantic categories Z+
found to be important for distinguishing its senses.
In addition, we investigated whether the CNN is
able to capture unattested features that differenti-
ate epistemic and dynamic readings with can.

For every modal verb and every filter, we sort
sentences in the training data by the maximum
value obtained by applying 1-max pooling to the
feature map acquired by applying the respective
filter to a sentence. For each filter and each of the
top-ranked 15 sentences, we extract the ngram that
corresponds to the maximum value w.r.t. the fil-
ter, i.e. the argmax of the feature map. The ngram
vector is the sum of all vectors of words in the
ngram. The obtained ngram vectors were plotted
using the t-SNE algorithm (Van der Maaten and
Hinton, 2008) and textually displayed with their
surrounding context.

For must we found many feature detectors that
relate to observations in Z+. Many filters detect
past (you must have been out last night; ep) vs.
non-past (we must make further efforts; de) and a
dynamic event (we must develop a policy; de) vs.
stative (you must think me a perfect fool; ep) read-
ing of the embedded verb. Among others the fea-
ture detectors capture passive constructions (ac-
tual steps must be taken; de) and negation (we
must not fear; de). Some filters were trained to
capture domain vocabulary which intuitively goes
along with deontic sense (European parliament;
present regulation; fisheries policy). One filter
captures telic clauses (to address these problems;
to prevent both forum; to exert maximum influ-
ence), identifying deontic sense. Novel features
not considered in Z+ are discourse markers (but;
and (then)) that correlate with deontic sense. All
in all, the CNN learns meaningful features that are

known to be important for differentiating senses
for must, and in contrast to manual feature design,
it detects relevant unattested features by itself.

For can many filters recognise accomplish-
ments which go along with dynamic sense, e.g.
You can do it/make it to NY. Others detect words
indicating possibility (ep), negation (de), discourse
markers, animate subject (de and dy), passive con-
struction (de and dy). However, without a system-
atic classification of these features it remains un-
clear how important they are for differentiating the
senses of can. Also, similar to Z+ we did not find
clear-cut features that recognize epistemic sense.

We performed a corresponding analysis of fea-
ture maps for German, following the same extrac-
tion procedure. We found the typical state (ep) vs.
event (de) contrast for the embedded verb, nega-
tion and tense, and again previously unattested
factors such as discourse relation markers21 (but;
without; thereby; in order to (dy)). For Ger-
man we identified various indicators for epistemic
sense (for müssen and können): attitude predi-
cates (believe, not know; tell me; have an idea,
be afraid), adverbials (possibly), conditionals (if);
counterfactual and negative polarity contexts (not
be the case; how; ever). Further detectors for epis-
temic sense are abstract subjects: placeholders for
propositions (it), abstract concepts (idea; music;
grades; application); indefinite subjects (one). We
find a tendency for 1st or 2nd person subjects to
co-occur with de/dy and 3rd person pronouns with
ep. For können (dy) we find achievements (present
report; move mountains; find compromise). For
deontic readings, next to negation with 1st and 2nd
person we find typical verb-object combinations
for actions that can be granted: use telephone;
communicate with third parties.

We extracted statistics about the distance of the
extracted ngrams from the modal verb (distance
overall; to the left/right and ngrams starting with
the modal). There are no greater overall distances
for German compared to English. However, for
German we find significantly more ngrams that
include the modal verb, especially for epistemic
readings of können, müssen, dürfen that clearly
mark subjunctive mood, whereas for sollen, with
ambiguous forms for subjunctive and past tense,
no such tendency is observed. Thus, the feature
maps identify subjunctive marking (in conjunction
with other factors) as relevant for classifying epis-

21For reasons of space we provide translations to English.

117



temic sense, whereas for sollen the lack of this in-
dicator goes along with lower performance. Fi-
nally, we observe, for English and German, strik-
ingly larger distances to the left of the modal verb
for epistemic readings compared to non-epistemic
readings. This can be traced back to indicators
in the wider left-embedding context: embedding
predicates, subjects, if clauses, etc.

5 Word sense disambiguation

Next to modal sense classification, we evaluate
our CNN model in a classical WSD task. As
benchmark corpus we chose the SensEval-3 lexi-
cal sample data set (Mihalcea et al., 2004), which
was recently applied in Rothe and Schütze (2015)
(henceforth R&S) and Taghipour and Ng (2015),
using sense-specific embeddings and a NN archi-
tecture, respectively (cf. Section 2).

The training data size for the 57 target word
types ranges from 14 to 263 instances. Sense
labels of test instances of a given target word
are predicted using the CNN model trained on
the training instances for the respective word
type.22 We set the CNN hyperparameters to
be the same as for MSC, except for mini-batch
size and region sizes. Since the training data for
some words is below 50 instances, mini-batch
size was set to 10. For tuning of the region
sizes, we split the training data for each word
(80:20 for training and validation) and used static
word2vec for the input representation. Among
{(1, 2, 3), (2, 3, 4), (3, 4, 5), (4, 5, 6), (5, 6, 7)}
the best results were obtained for (5, 6, 7).23

The final hyperparameter setting was used to in-
vestigate the impact of representations. Among
word2vec, dependency-based and randomly ini-
tialised, word2vec performed the best, the tuned
version being slightly better than static vectors.
We report results for tuned word2vec vectors.

We compare our results to the results R&S
obtained when using only sense-specific embed-
dings. These are not the state-of-the-art WSD re-
sults they obtain with additional features, namely
POS tags of words in a small window around the
target word, their discrete representation and local
collocations. For sentence representation, R&S
used every word in the target word sentence. For

22Training instances in the SensEval-3 dataset can have
more than one sense label. For training we randomly picked
one of possible labels. Instances which contain more than
one marked target word were omitted.

23However, the differences in the results were minor.

Snaive-prod 62.20 S-prod 64.30
S-cosine 60.50 S-raw 63.10

CNN 66.50

Table 6: WSD accuracy on SensEval-3 dataset.

sense prediction, they used the following feature
vectors that are fed into a linear SVM classifier:

S-cosine = 〈cos(c, s(1)), . . . , cos(c, s(k))〉 ,
S-product = 〈c1s(1)1 , . . . , cns(1)n , . . . , c1s(k)1 , . . . , cns(k)n 〉 ,

S-raw = 〈c1, . . . , cn, . . . , s(k)1 , . . . , s(k)n 〉 ,
where w is a target word with k senses, c is the
centroid defined as the sum of all word2vec vec-
tors of words in the sentence and s(j) is the em-
bedding of the j-th synset of w.24 They propose
a variant of the S-prod feature vector, Snaive-prod,
for which the synset embeddings are the sum of
the word2vec vectors of all words in that sysnet.

The results are summarised in Table 6. The
CNN model compares favorably to the competi-
tor models of R&S using AutoExtend embeddings
for WSD. It achieves slightly higher results with-
out explicitly marking the target word, whereas
the AutoExtend embeddings encode much richer
information: what is the target word, how many
possible sense it has, and knowledge-intense sense
embeddings for each of its synsets. The CNN is
able to compete with the rich AutoExtend model,
and future work needs to investigate whether –
similar to the S-product setting in R&S – the CNN
model can achieve competitive state-of-the-art re-
sults by incorporating features corresponding to
those of the IMS system of Zhong and Ng (2010).

6 Conclusion and future work

We presented an account for multilingual modal
sense classification using a CNN architecture. We
apply the same architecture in a standard WSD
task and achieve competitive results compared to
a system using richer embedding information.

Our one-layer CNN architecture outperforms
strong baselines and prior art for MSC in English,
including a NN and MaxEnt model, and proves
particularly robust in cross-genre classification.

We applied the CNN model to German, on
a data set of modest size, obtained using cross-
lingual projection techniques. The CNN-G clas-
sifier outperforms a NN model by large margins.

24Obtained using the AutoExtend method of R&S.

118



Our approach can be easily generalized to novel
languages without tedious and resource-intensive
feature engineering. Through analysis of learned
feature maps we gave evidence that the CNN
learns both known and novel features for MSC.

The attractiveness of the CNN framework lies in
its ability to learn (semantic) features from flexible
window regions without syntactic processing, and
the ensuing robustness on difficult text genres and
its ease in generalizing to novel languages.

Acknowledgments

We thank Mengfei Zhou for her support with the
German corpus construction. This work has been
supported by the German Research Foundation as
part of the Research Training Group ”Adaptive
Preparation of Information from Heterogeneous
Sources” (AIPHES) under grant No. GRK 1994/1.

References
Kathryn Baker, Michael Bloodgood, Bonnie J Dorr,

Nathaniel W Filardo, Lori Levin, and Christine Pi-
atko. 2010. A Modality Lexicon and its use in Au-
tomatic Tagging. In Proceedings of LREC, pages
1402–1407.

Aljoscha Burchardt, Marco Pennacchiotti, Stefan
Thater, and Manfred Pinkal. 2009. Assessing the
impact of frame semantics on textual entailment.
Natural Langugae Engineering, 15(4):527–550.

Ronan Collobert, Jason Weston, Léon Bottou, Michael
Karlen, Koray Kavukcuoglu, and Pavel Kuksa.
2011. Natural language processing (almost) from
scratch. The Journal of Machine Learning Re-
search, 12:2493–2537.

Marie-Catherine de Marneffe, Christopher D. Man-
ning, and Christopher Potts. 2012. Did It Happen?
The Pragmatic Complexity of Veridicality Assess-
ment. Computational Linguistics, 38(2):301–333.
Special Issue: Modality and Negation.

Gertrud Faaß and Kerstin Eckart. 2013. Language
Processing and Knowledge in the Web: 25th In-
ternational Conference, GSCL 2013, Darmstadt,
Germany, September 25-27, 2013. Proceedings.
Springer Berlin Heidelberg, Berlin, Heidelberg.

Morten W Fagerland, Stian Lydersen, and Petter Laake.
2013. The mcnemar test for binary matched-pairs
data: mid-p and asymptotic are better than exact
conditional. BMC medical research methodology,
13(1):1.

Xavier Glorot and Yoshua Bengio. 2010. Understand-
ing the difficulty of training deep feedforward neural
networks. In International conference on artificial
intelligence and statistics, pages 249–256.

Nancy Ide, Collin Baker, Christiane Fellbaum, and
Charles Fillmore. 2008. MASC: The manually an-
notated sub-corpus of American English. In Pro-
ceedings of the Sixth International Conference on
Language Resources and Evaluation (LREC-2008).

Nal Kalchbrenner, Edward Grefenstette, and Phil Blun-
som. 2014. A convolutional neural network for
modelling sentences. In Proceedings of the 52nd
Annual Meeting of the Association for Computa-
tional Linguistics, pages 655–665, Baltimore, Mary-
land.

Yoon Kim. 2014. Convolutional neural networks for
sentence classification. In Proceedings of the 2014
Conference on Empirical Methods in Natural Lan-
guage Processing (EMNLP), page 17461751, Doha,
Qatar.

Diederik Kingma and Jimmy Ba. 2014. Adam: A
method for stochastic optimization. arXiv preprint
arXiv:1412.6980.

Kenton Lee, Yoav Artzi, Yejin Choi, and Luke Zettle-
moyer. 2015. Event detection and factuality as-
sessment with non-expert supervision. In Proceed-
ings of the 2015 Conference on Empirical Methods
in Natural Language Processing, pages 1643–1648,
Lisbon, Portugal, September.

Omer Levy and Yoav Goldberg. 2014. Dependency-
based word embeddings. In ACL (2), pages 302–
308.

R. Mihalcea, T. Chklovski, and A. Kilgarriff. 2004.
The Senseval-3 English lexical sample task. In
Proceedings of SENSEVAL-3: Third International
Workshop on the Evaluation of Systems for the Se-
mantic Analysis of Text [CD-ROM], pages 25–28.

Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Cor-
rado, and Jeff Dean. 2013. Distributed representa-
tions of words and phrases and their compositional-
ity. In Advances in neural information processing
systems, pages 3111–3119.

Vinodkumar Prabhakaran, Michael Bloodgood, Mona
Diab, Bonnie Dorr, Lori Levin, Christine D. Piatko,
Owen Rambow, and Benjamin Van Durme. 2012.
Statistical modality tagging from rule-based anno-
tations and crowdsourcing. In Proceedings of the
Workshop on Extra-Propositional Aspects of Mean-
ing in Computational Linguistics, pages 57–64, Jeju,
Republic of Korea, July.

Nils Reimers, Judith Eckle-Kohler, Carsten Schnober,
Jungi Kim, and Iryna Gurevych. 2014. Germeval-
2014: Nested named entity recognition with neu-
ral networks. In Gertrud Faaß and Josef Rup-
penhofer, editors, Workshop Proceedings of the
12th Edition of the KONVENS Conference, pages
117–120, Hildesheim, October. Universitätsverlag
Hildesheim.

119



Sascha Rothe and Hinrich Schütze. 2015. Autoex-
tend: Extending word embeddings to embeddings
for synsets and lexemes. In Proceedings of the
53rd Annual Meeting of the Association for Compu-
tational Linguistics and the 7th International Joint
Conference on Natural Language Processing (Vol-
ume 1: Long Papers), pages 1793–1803, Beijing,
China.

Josef Ruppenhofer and Ines Rehbein. 2012. Yes we
can !? Annotating the senses of English modal
verbs. In Proceedings of the LREC 2012 Confer-
ence, pages 1538–1545.

Richard Socher, Alex Perelygin, Jean Wu, Jason
Chuang, Christopher D. Manning, Andrew Ng, and
Christopher Potts. 2013. Recursive deep models
for semantic compositionality over a sentiment tree-
bank. In Proceedings of the 2013 Conference on
Empirical Methods in Natural Language Process-
ing, pages 1631–1642, Seattle, Washington, USA.

Kaveh Taghipour and Hwee Tou Ng. 2015. Semi-
supervised word sense disambiguation using word
embeddings in general and specific domains. In
The 2015 Annual Conference of the North Ameri-
can Chapter of the Association for Computational
Linguistics, pages 314–323.

Jörg Tiedemann. 2012. Parallel Data, Tools and In-
terfaces in OPUS. In Nicoletta Calzolari, Khalid
Choukri, Thierry Declerck, Mehmet Uğur Doğan,
Bente Maegaard, Joseph Mariani, Jan Odijk, and
Stelios Piperidis, editors, Proceedings of LREC-
2012, pages 2214–2218, Istanbul, Turkey, May.

Laurens Van der Maaten and Geoffrey Hinton. 2008.
Visualizing data using t-sne. Journal of Machine
Learning Research, 9(2579-2605):85.

Janyce Wiebe, Theresa Wilson, and Claire Cardie.
2005. Annotating expressions of opinions and emo-
tions in language. Language resources and evalua-
tion, 39(2-3):165 – 210.

Ye Zhang and Byron C. Wallace. 2015. A sensi-
tivity analysis of (and practitioners’ guide to) con-
volutional neural networks for sentence classifica-
tion. Technical report, University of Texas at Austin.
arXiv:1510.03820v2.

Zhi Zhong and Hwee Tou Ng. 2010. It makes sense:
A wide-coverage word sense disambiguation system
for free text. In Proceedings of the ACL 2010 Sys-
tem Demonstrations, pages 78–83. Association for
Computational Linguistics.

Zhi Zhong and Hwee Tou Ng. 2012. Word sense dis-
ambiguation improves information retrieval. In Pro-
ceedings of the 50th Annual Meeting of the Associa-
tion for Computational Linguistics (Volume 1: Long
Papers), pages 273–282, Jeju Island, Korea.

Mengfei Zhou, Anette Frank, Annemarie Friedrich,
and Alexis Palmer. 2015. Semantically Enriched

Models for Modal Sense Classification. In Proceed-
ings of the EMNLP 2015 Workshop LSDSem: Link-
ing Models of Lexical, Sentential and Discourse-
level Semantics, Lisbon, Portugal.

120


