



















































A Translation-Based Knowledge Graph Embedding Preserving Logical Property of Relations


Proceedings of NAACL-HLT 2016, pages 907–916,
San Diego, California, June 12-17, 2016. c©2016 Association for Computational Linguistics

A Translation-Based Knowledge Graph Embedding
Preserving Logical Property of Relations

Hee-Geun Yoon, Hyun-Je Song, Seong-Bae Park, Se-Young Park
School of Computer Science and Engineering

Kyungpook National University
Daegu, 41566, Korea

{hkyoon, hjsong, sbpark, sypark}@sejong.knu.ac.kr

Abstract

This paper proposes a novel translation-based
knowledge graph embedding that preserves
the logical properties of relations such as tran-
sitivity and symmetricity. The embedding
space generated by existing translation-based
embeddings do not represent transitive and
symmetric relations precisely, because they
ignore the role of entities in triples. Thus,
we introduce a role-specific projection which
maps an entity to distinct vectors according to
its role in a triple. That is, a head entity is pro-
jected onto an embedding space by a head pro-
jection operator, and a tail entity is projected
by a tail projection operator. This idea is ap-
plied to TransE, TransR, and TransD to pro-
duce lppTransE, lppTransR, and lppTransD,
respectively. According to the experimental
results on link prediction and triple classifica-
tion, the proposed logical property preserving
embeddings show the state-of-the-art perfor-
mance at both tasks. These results prove that
it is critical to preserve logical properties of
relations while embedding knowledge graphs,
and the proposed method does it effectively.

1 Introduction

Representing knowledge as a graph is one of the
most effective ways to utilize human knowledge
with a machine, and various large-scale knowledge
graphs such as Freebase (Bollacker et al., 2008) and
Yago (Suchanek et al., 2007) are available these
days. However, the sparsity of the graphs makes it
difficult to utilize them in real world applications.
In spite of their huge volume, the relations among
entities in the graphs are insufficient, which results

in very limited inference of the knowledge of the
graphs. Therefore, it is of importance to resolve such
sparsity of knowledge graphs.

One of the most promising methods to complete
knowledge graphs is to embed the graphs in a low-
dimensional continuous vector space. This method
learns a vector representation of a knowledge graph,
and the plausibility of a certain knowledge within
the graph is measured with algebraic operations in
the vector space. Thus, new knowledge can be
harvested from the space by finding knowledge in-
stances with high plausibility.

The translation-based model among various
knowledge-embedding models shows the state-of-
the-art performance of knowledge graph completion
(Bordes et al., 2013; Wang et al., 2014; Lin et al.,
2015; Ji et al., 2015). TransE (Bordes et al., 2013) is
one of the well-known translation-based approaches
to this problem. When a set of knowledge triples
(h, r, t) composed of a relation (r) and two enti-
ties (h and t) is given, it finds vector representations
of h, t, and r by compelling the vector of t to be
same with the sum of the vectors of h and r. While
TransE embeds all relations in a single vector space,
TransH (Wang et al., 2014) and TransR (Lin et al.,
2015) assume that each relation has its own embed-
ding space. On the other hand, Ji et al. (2015) found
out that even a single relation or a single entity usu-
ally has multiple types. Thus, they have proposed
TransD which allows multiple mapping matrices of
entities and relations.

Even though these translation-based models
achieve high performance in knowledge graph com-
pletion, they all ignore logical properties of rela-

907



tions. That is, transitive relations and symmetric
relations lose their transitivity or symmetricity in
the vector space generated by the translation-based
models. As a result, the models can not complete
only new knowledge with such relations, but also
new knowledge with a relation affected by the rela-
tions. In most knowledge graphs, transitive or sym-
metric relations are common. For instance, FB15K,
one of the benchmark datasets for knowledge graph
completion, has a number of transitive and symmet-
ric relations. About 20% of triples in FB15K have
a transitive or a symmetric relation. Therefore, the
ignorance of logical properties of relations becomes
a serious problem in knowledge graph completion.

The main reason why existing translation-based
embeddings can not reflect logical properties of rela-
tions is that they do not consider the role of entities.
An entity should be a different vector in the embed-
ding space according to its role. Therefore, the so-
lution to preserve the logical properties of relations
in the embedding space is to distinguish the role of
entities while embedding entities and relations.

In this paper, we propose a role-specific projec-
tion to preserve logical properties of relations in an
embedding space. This can be implemented by pro-
jecting a head entity onto an embedding space by a
head projection operator and a tail entity by a tail
projection operator. As a result, an identical en-
tity is represented as two distinct vectors. This idea
can be applied to various translation-based models
including TransE, TransR, and TransD. Therefore,
we also propose how to modify existing translation-
based models to preserve logical properties. The
effectiveness of the proposed idea is verified with
two tasks of link prediction and triple classification
using standard benchmark datasets of WordNet and
Freebase. According to the experimental results, the
logical property preserving embeddings achieve the
state-of-the-art performance in both tasks.

2 Related Work

The sparsity of knowledge graphs is one of the most
critical issues in utilizing them in real-world appli-
cations. Thus, there have been a number of stud-
ies on completing knowledge graphs as a solution
to overcome the sparsity. Link prediction is one of
the promising ways for knowledge graph comple-

tion. This task predicts new relations between enti-
ties on a knowledge graph by investigating existing
relations of the graph (Nickel et al., 2015; Neelakan-
tan and Chang, 2015). The methods used for link
prediction can be categorized into three groups. One
group consists of the methods based on graph fea-
tures. The observable features used in these methods
are the paths between entity pairs (Lao and Cohen,
2010; Lao et al., 2011) and subgraphs (Gardner and
Mitchell, 2015). The other group is composed of the
methods based on Markov random fields. The stud-
ies belonging to this group inference new relations
by probabilistic soft logic (Pujara et al., 2013) and
first-order logic (Jiang et al., 2012).

Knowledge graph embedding is another promi-
nent method for link prediction (Bordes et al., 2011;
Nickel et al., 2011; Guo et al., 2015; Neelakantan et
al., 2015). It embeds entities of a knowledge graph
into a continuous low dimensional space as vectors,
and embeds relations as vectors or matrices. These
vectors are optimized by a score function of each
knowledge graph embedding model. The Semantic
Matching Energy (SME) model proposed by Bor-
des et al. (2014) finds vector representations of en-
tities and relations using a neural network. When
a triple (h, r, t) is given, SME makes two relation-
dependent embeddings of (h, r) and (r, t). Its score
function for a triple is the similarity between the
embeddings. Since relations are expressed as vec-
tors instead of matrices, the complexity of SME is
relatively low compared to other embedding meth-
ods. Jenatton et al. (2012) suggested Latent Factor
Model (LFM). In order to capture both the first-order
and the second-order interactions between two en-
tities, LFM adopts a bilinear function as its score
function. In addition, it represents a relation as a
weighted sum of sparse latent factors to work with
a large number of relations. Socher et al. (2013)
proposed the Neural Tensor Network (NTN) model.
NTN is a highly expressive embedding model which
has a bilinear tensor layer instead of a standard lin-
ear neural network layer. As a result, it can process
various interactions between entity vectors. How-
ever, it is difficult to process large-scale knowledge
graphs with NTN due to its high complexity.

The current main stream of knowledge graph em-
bedding is a translation-based embedding approach.
The basic idea of this embedding is that entities and

908



(a) (b) (c)

Figure 1: An example of entity vectors trained wrong by a transitive relation.

relations are represented as vectors, and relations are
treated as operators to translate entities into other
positions on an embedding space. Thus, they try
to find vector representations of h, t, and r so that
the vector of t becomes the sum of the vectors of h
and r. TransE (Bordes et al., 2013) is the simplest
translation-based graph embedding. It assumes that
all vectors of entities and relations lie on a single
vector space. As a result, it fails in dealing with the
reflexivity and the multiplicities of relations except
1-to-1. The solution to this problem is to allow the
entities to play different roles according to relations,
but TransE is unable to do it.

An entity plays multiple roles in TransH (Wang et
al., 2014), since TransH allows entities to have mul-
tiple vector representations. In order to obtain mul-
tiple representations of an entity, TransH projects
an entity vector into relation-specific hyperplanes.
TransR (Lin et al., 2015) also solves the problems of
TransE by introducing relation spaces. It allows an
entity to have various vector representations by map-
ping an entity vector into relation-specific spaces
rather than relation-specific hyperplanes. Although
both TransH and TransR overcome the limitations
of TransE, they are still not able to handle multi-
ple types of relations which is determined by head
and tail entities of each relation. For example, let
us consider two triples of (California, part of, USA)
and (arm, part of, body). Both triples have a re-
lation part of in common, but the relation should
be interpreted differently in each triple. Ji et al.
(2015) have proposed TransD in which a relation
can have multiple relation spaces according to its en-
tities. TransD constructs relation mapping matrices

dynamically by considering entities and a relation si-
multaneously. For this, it introduces projection vec-
tors for entities and relations, and then constructs the
mapping matrices by multiplying these entity and re-
lation projection vectors. As a result, every relation
in TransD has multiple entity-specific spaces.

3 Loss of Logical Properties in
Translation-Based Embeddings

Translation-based embeddings aim to find vector
representations of knowledge graph entities in an
embedding space by regarding relations as transla-
tion of entities in the space. Since they map the enti-
ties onto a vector space regardless of the role of the
entities, they do not express logical properties of re-
lations such as transitivity and symmetricity. That
is, the vectors of transitive or symmetric relations
do not deliver transitivity or symmetricity in the em-
bedding spaces from translation-based embeddings.

For instance, let us consider a transitive rela-
tion. Assume that we have three triples (e1, r1, e2),
(e2, r1, e3), and (e1, r1, e3) and r1 is a transitive re-
lation. When the vector of r1 is not a zero vec-
tor, there could be three types of entity vectors as
shown in Figure 1. In Figure 1-(a), e1, e2, and e3 are
placed linearly. In this case, (e1, r1, e3) can not be
expressed in this figure. When e1 and e2 are placed
at the same point like Figure 1-(b), (e1, r1, e2) can
not be expressed. In Figure 1-(c), (e2, r1, e3) can not
be expressed when e2 and e3 are same. In a similar
way, translation-based embeddings can not express
symmetric relations perfectly.

The problems caused by wrong expression of
transitive and symmetric relations are two-folds.

909



Figure 2: Simple illustration on a transitive relation with role-specific projections.

Figure 3: Simple illustration on a symmetric relation with role-specific projections.

One fold is that the relations with logical properties
are common in knowledge bases. Two benchmark
datasets of FB15K and WN18 in Table 2 prove it.
There are 483,142 triples in FB15K, and 84,172 (=
47,841 + 36,331) triples among them have a transi-
tive or symmetric relation. That is, the translation-
based embeddings do not express triples precisely
for about 17% of triples in FB15K. 22.4% of triples
in another dataset WN18 also have a transitive or
symmetric relation. The other is that transitive or
symmetric relations do not affect the entities that
are directly connected by the relations, but affect
also other entities shared by non-transitive and non-
symmetric relations through the entities. Therefore,
it is of importance in translation-based embeddings
to represent transitive and symmetric relations pre-
cisely.

4 Logical Property Preserving Embedding

4.1 Role-Specific Projection of Entity Vectors

The main reason why transitive or symmetric re-
lations are not represented precisely by existing
translation-based embeddings is that they ignore the
role of entities in embedding them onto a vector
space. That is, when a triple (h, r, t) is given, h and

t plays different roles. However, the existing em-
beddings treat them equally and embed them into a
space in the same way. Therefore, in order to ex-
press entities and relations more precisely, entities
should be represented differently according to their
role in a triple.

Figure 2 shows how entities can be represented
according to their role. In this figure, solid lines
represent entity mappings as head roles and dotted
lines mean that entities are mapped as tails. As-
sume that three triples of (e1, r1, e2), (e2, r1, e3),
and (e1, r1, e3) are given with a transitive relation
r1. e1 plays only a head role and e3 plays only a
tail role, while e2 plays both roles. Then, the entity
vectors in the entity space are mapped into the space
of r1 using two mapping matrices Mr1h and Mr1t.
That is, head entities are mapped by Mr1h, while
tail entities are projected by Mr1t. Let e

h
1⊥ and e

h
2⊥

be the projected vectors of e1 and e2 respectively
by Mr1h, and let e

t
2⊥ and e

t
3⊥ be the projected vec-

tors of e2 and e3 respectively by Mr1t. e
h
1⊥ and e

h
2⊥

are placed at the same point in the space of r1 from
(e2, r1, e3) and (e1, r1, e3). Similarly, et2⊥ and e

t
3⊥

are same from (e1, r1, e2) and (e1, r1, e3). Since e2
is used as both a head and a tail, it is mapped dif-
ferently as eh2⊥ and e

t
2⊥, respectively. Note that all

910



three triples are well expressed in this space.
Symmetric relations also can be expressed pre-

cisely by logical property preserving knowledge
graph embedding. Assume that two triples of
(e4, r2, e5) and (e5, r2, e4) are given with a symmet-
ric relation r2. Figure 3 shows how the triples are
well represented. The solid lines imply that entities
are mapped by Mr2h while the dotted lines mean
that entities are mapped by Mr2t. By placing e

h
4⊥

and eh5⊥ at the same point and imposing e
t
4⊥ and

et5⊥ at the same point, r2 is precisely expressed as
a symmetric relation in the embedding space.

4.2 Realization of Logical Property Preserving
Embedding

Due to the simplicity of role-specific projection
of entity vectors, it can be applied to various
translation-based embeddings. In this paper, we ap-
ply it to TransE, TransR, and TransD.

4.2.1 TransE
The score function of TransE is

fEr (h, t) = ‖h + r− t‖l1/2 ,

where h, t ∈ Rn, and r ∈ Rn are the vectors of a
head entity, a tail entity, and a relation on a single
embedding space. In the logical property preserv-
ing TransE (lppTransE), h and t should be mapped
differently. For this purpose, we adopt a head and
a tail space mapping matrices of Mh ∈ Rn×n and
Mt ∈ Rn×n. As a result, the score function of lpp-
TransE becomes

f lppEr (h, t) = ‖Mhh + r−Mtt‖l1/2 .

This is similar to the score function of TransR. The
difference between lppTransE and TransR is that
both h and t are mapped by a single mapping ma-
trix for r in TransR, while h is mapped by Mh and
t is by Mt in lppTransE.

4.2.2 TransR
The entities in TransR are mapped into vectors

in different relation space according to a relation.
Thus, its score function is defined as

fRr (h, t) = ‖Mrh + r−Mrt‖l1/2 ,

where Mr ∈ Rm×n is a mapping matrix for a rela-
tion r which is represented as r ∈ Rm. Thus, in the
logical property preserving TransR (lppTransR), the
mapping matrix of each relation is split into a head
mapping matrix Mrh ∈ Rm×n and a tail mapping
matrix Mrt ∈ Rm×n. Then, the score function of
lppTransR is

f lppRr (h, t) = ‖Mrhh + r−Mrtt‖l1/2 .

With these two distinct mapping matrices, entities
can have two different vector representations in the
same relation space.

4.2.3 TransD

TransD maps entity vectors into different vectors
in relation spaces according to entity and relation
types. That is, the entity vectors are mapped by
entity-relation specific mapping matrices. Thus, its
score function is defined as

fDr (h, t) = ‖Mrhh + r−Mrtt‖l1/2 ,

where Mrh ∈ Rm×n and Mrt ∈ Rm×n are entity-
relation specific mapping matrices. These mapping
matrices are computed by multiplying projection
vectors of an entity and a relation as follows.

Mrh = rphTp + I
m×n,

Mrt = rptTp + I
m×n,

where hp, tp and rp, are the projection vectors for a
head, a tail and a relation.

The logical property preserving TransD (lpp-
TransD) divides rp into two projection vectors rph
and rpt to reflect the role of entities. The mapping
matrices then becomes

M′rh = rphh
T
p + I

m×n,

M′rt = rptt
T
p + I

m×n.

Then, its score function is

f lppDr (h, t) =
∥∥M′rhh + r−M′rtt∥∥l1/2 .

911



Table 2: A simple statistics on datasets.

Dataset #Rel #Ent #Train #Valid #Test #Transitive #Symmetric Ratio
WN11 11 38,696 112,581 2,609 10,544 822 1,597 2.2%
FB13 13 75,043 316,232 5,908 23,733 262 4,152 1.4%
WN18 18 40,943 141,442 5,000 5,000 2,001 29,667 22.4%
FB15K 1,345 14,951 483,142 50,000 59,071 47,841 36,331 17.4%

Table 1: Complexity of knowledge graph embed-
ding models.

Model No. of parameters
TransE O (Nen+Nrn)
TransR O (Nen+Nr (m+ 1)n)
TransD O (2Nen+ 2Nrm)

lppTransE O (Nen+Nrn+ n2)
lppTransR O (Nen+Nr (2m+ 1)n)
lppTransD O (2Nen+ 3Nrm)

4.3 Training Logical Property Preserving
Embeddings

Since all logical property preserving embeddings
are based on the score function used by previ-
ous translation-based knowledge graph embeddings,
they can be trained using a margin-based ranking
loss defined as

L =
∑

(h,r,t)∈P

∑
(h′,r,t′)∈N

max(0, f∗r (h, t)+γ−f∗r (h′, t′)),

where f∗r is the score function of a corresponding
logical property preserving embedding. Here, P and
N are sets of correct and incorrect triples, and γ is
a margin. N is constructed by replacing a head or a
tail entity in an existing triple because a knowledge
graph has only correct triples. All logical property
preserving embeddings are optimized by stochastic
gradient descent.

The complexities of the logical property preserv-
ing embeddings are shown in Table 1. Ne and Nr
in this table are the number of entities and relations,
and n and m are the dimensions of entity and re-
lation embedding spaces. Their complexity mainly
depends on the number of relations. Thus, the in-
creased complexities of the logical property preserv-
ing embeddings is not significant, when compared
with TransE, TransR, and TransD.

5 Experiments

The superiority of the proposed logical property pre-
serving embeddings is shown through two kinds of
tasks. The first task is link prediction (Bordes et al.,
2013). This task predicts the missing entity when
there is a missing entity in a given triple. The other is
triple classification (Socher et al., 2013). This task
aims to decide whether a given triple is correct or
not.

5.1 Data Sets

Two popular knowledge graphs of WordNet (Miller,
1995) and Freebase (Bollacker et al., 2008) are used
for evaluating embeddings. WordNet provides the
semantic relations among words, and there exist its
two widely-used subsets which are WN11 (Socher et
al., 2013) and WN18 (Bordes et al., 2014). WN18
is used for link prediction, and WN11 is adopted
for triple classification. Freebase represents gen-
eral facts about the world. It has two subsets of
FB13 (Socher et al., 2013) and FB15K (Bordes et
al., 2014). FB15K is used for both triple classifi-
cation and link prediction, while FB13 is employed
only for triple classification.

Table 2 summarizes a simple statistics of each
dataset. #Triples is the number of training triples in
each benchmark dataset. #Transitive and #Symmet-
ric are the number of triples which have transitive
and symmetric relations, respectively. Ratio denotes
the proportion of triples of which relation is transi-
tive or symmetric. As shown in this table, the triples
with a transitive or symmetric relation take a large
proportion in WN18 and FB15K.

5.2 Link Prediction

For the evaluation of link prediction, we followed
the evaluation protocols and metrics used in previ-
ous studies (Socher et al., 2013; Bordes et al., 2013;
Lin et al., 2015; Ji et al., 2015). To compare the

912



Table 4: Experimental results on link prediction.
Dataset WN18 FB15K

Metric Mean Rank Hits@10 (%) Mean Rank Hits@10 (%)Raw Filter Raw Filter Raw Filter Raw Filter
TransE 263 251 75.4 89.2 243 125 34.9 47.1

TransH (unif) 318 301 75.4 86.7 211 84 42.5 58.5
TransH (bern) 401 388 73.0 82.3 212 87 45.7 64.4
TransR (unif) 232 219 78.3 91.7 226 78 43.8 65.5
TransR (bern) 238 225 79.8 92.0 198 77 48.2 68.7
TransD (unif) 242 229 79.2 92.5 211 67 49.4 74.2
TransD (bern) 224 212 79.6 92.2 194 91 53.4 77.3

lppTransE (unif) 336 323 77.7 (+2.3) 89.5 (+0.3) 228 78 47.4 (+12.5) 72.9 (+25.8)
lppTransE (bern) 342 329 79.5 (+4.1) 92.7 (+3.5) 215 95 48.9 (+14.0) 73.1 (+26.0)
lppTransR (unif) 331 317 79.2 (+0.9) 92.7 (+1.0) 238 79 47.2 (+3.4) 74.4 (+8.9)
lppTransR (bern) 334 321 79.6 (-0.2) 92.8 (+0.8) 219 92 50.4 (+2.2) 77.2 (+8.5)
lppTransD (unif) 342 328 79.3 (+0.1) 93.6 (+1.1) 218 69 49.6 (+0.2) 77.4 (+3.2)
lppTransD (bern) 283 270 80.5 (+0.9) 94.3 (+2.1) 195 78 53.0 (-0.4) 78.7 (+1.4)

Table 3: Parameter values in link prediction.
Dataset Model α B γ n,m D.S

WN18
lppTransE 0.001 1,440 1 50 L1
lppTransR 0.001 1,440 1 50 L1
lppTransD 0.001 1,440 2 50 L1

FB15K
lppTransE 0.001 480 1 100 L1
lppTransR 0.001 4,800 1 100 L1
lppTransD 0.0001 4,800 2 100 L1

methods for this task, two metrics of mean rank and
Hits@10 are used. The mean rank measures the av-
erage rank of all correct entities, and Hits@10 is the
proportion of correct triples ranked in top 10. Since
there are two evaluation settings of “raw” and “fil-
ter” in this task (Bordes et al., 2013), we report both
results. In addition, we report the results for two
sampling methods of “bern” and “unif” (Wang et al.,
2014) as the previous studies did.

There are five parameters in the proposed property
preserving embeddings. They are a learning rate α,
the number of training triples in each mini-batchB1,
a margin γ, the embedding dimension for entities
and relations (n and m), and a dissimilarity measure
in embedding score functions (D.S). The parameter
values used in our experiments are given at Table 3.
The iteration number of stochastic gradient descent
is 1,000.

Table 4 shows the results on link prediction. The
results of previous studies are referred from their
report, since the same datasets are used. The val-

1α and B are related with stochastic gradient descent.

ues between parentheses are the improvement over
their base models. The logical property preserv-
ing embeddings outperform all other methods for
both “bern” and “unif” on WN18 except lppTransR
with “bern” in the raw setting. In the raw set-
ting, lppTransE, lppTransR, and lppTransD achieve
79.5%, 79.6%, and 80.5% of Hits@10 respectively
in “bern”, which are 4.1%, -0.2%, and 0.9% higher
than those of TransE, TransR, and TransD. The
logical property preserving embeddings show even
higher performance in the filter setting. Hits@10 of
lppTransD in “bern” is 94.3%, while that of TransD
in “unif” is 92.5% and that in “bern” is 92.2%. Thus,
lppTransD improves 2.1% over TransD in “bern”.
Especially, this performance of lppTransD is 1.8%
higher than that of TransD in “unif”, the previous
state-of-the-art performance.

The logical property preserving embeddings out-
perform their base models also on FB15K. TransE
is improved most significantly with this dataset. The
improvements by lppTransE in the raw setting are
12.5% in “unif” and 14.0% in “bern”, while those
in the filter setting are 25.8% in “unif” and 26.0%
in “bern”. In addition, its Hits@10 exceeds those
of TransH and TransR. That is, even if TransH
and TransR were proposed to tackle the problem of
TransE, the proposed lppTransE solves the problem
better than TransH and TransR. lppTransD achieves
just a little bit lower Hits@10 than TransD in “bern”,
but the improvements in the filter setting are notice-

913



Table 6: Parameter values in triple classification.
Dataset Model α B γ n,m D.S

WN11
lppTransE 0.01 120 2 20 L1
lppTransR 0.001 120 4 20 L1
lppTransD 0.0001 1,000 1 100 L2

FB13
lppTransE 0.001 30 1 100 L1
lppTransR 0.0001 300 1 100 L1
lppTransD 0.0001 300 1 100 L2

FB15K
lppTransE 0.001 480 1 100 L1
lppTransR 0.001 4,800 1 100 L1
lppTransD 0.0001 4,800 2 100 L1

able. Since Hits@10 of TransD in “bern” is the
best performance ever reported, that of lppTransD in
“bern” becomes a new state-of-the-art performance.

Table 5 exhibits Hits@10s according to mapping
property of the relations of FB15K. The notable
trend of this table is that the logical property preserv-
ing embeddings show much higher Hits@10 than
their base models in N-to-1 and N-to-N, while their
Hits@10s are similar to those of their base models in
1-to-1 and 1-to-N. This is notable with TransD and
lppTransD. lppTransD improves TransD, the previ-
ous state-of-the-art method by 7.3% (N-to-1) and
3.7% (N-to-N) in predicting head, and by 0.9% (N-
to-1) and 0.3% (N-to-N). Note that it is important
to verify if logical property preserving embeddings
achieve good performances in N-to-N, since all tran-
sitive relations and some symmetric relations are, in
general, N-to-N. According to this table, Hits@10s
of most logical property preserving embeddings are
improved significantly in N-to-N, which proves that
the proposed method solves transitivity and sym-
metricity problem of previous embeddings.

5.3 Triple Classification

Three datasets of WN11, FB13, and FB15K are
used in this task. WN11 and FB13 have negative
triples, but FB15K has only positives. Thus, we
generated negative triples for FB15K by following
the strategy of (Socher et al., 2013). As a result,
the classification accuracies on FB15K can not be
compared directly with previous studies, and the ac-
curacies on FB15K in this table are those obtained
with our dataset. The parameter values for training
TransE, TransH, TransR, and TransD are borrowed
from their reports, and those for logical property pre-
serving embeddings are shown in Table 6.

Table 7 shows the accuracies of triple classifi-

Table 7: Accuracies on triple classification. (%)
Dataset WN11 FB13 FB15K

TransE (unif) 75.9 70.9 80.3
TransE (bern) 75.9 81.5 80.8
TransH (unif) 77.7 76.5 81.9
TransH (bern) 78.8 83.3 82
TransR (unif) 85.5 74.7 82.6
TransR (bern) 85.9 82.5 82.7
TransD (unif) 85.6 85.9 84.2
TransD (bern) 86.4 89.1 84.8

lppTransE (unif) 81.3 (+5.4) 72.3 (+1.4) 83.2 (+2.9)
lppTransE (bern) 81.3 (+5.4) 83.4 (+1.9) 83.6 (+2.8)
lppTransR (unif) 85.5 (+0.0) 79.5 (+4.8) 83.4 (+0.8)
lppTransR (bern) 85.5 (-0.4) 83.1 (+0.6) 84.6 (+1.9)
lppTransD (unif) 86.1 (+0.5) 86.6 (+0.7) 84.7 (+0.5)
lppTransD (bern) 86.2 (-0.2) 88.6 (-0.5) 85.3 (+0.5)

cation on the three datasets. The logical property
preserving embeddings in general outperform their
base models. lppTransE always shows higher ac-
curacy than TransE, lppTransR than TransR except
for WN11, and lppTransD than TransD in FB15K.
One thing to note is that the improvements by the
logical property preserving embeddings are always
observed in FB15K, while those in WN11 and FB13
are small or slightly negative. This can be explained
with the number of triples with a transitive or sym-
metric relation in the datasets. As shown in Table
2, the triples with such a relation take just a small
portion of WN11 and FB13. The ratio of those
triples is less than 2.3% in these datasets. How-
ever, as noted before, more than 17% triples are such
ones in FB15K. Thus, the improvement in FB15K
is remarkable. The other thing to note is that lpp-
TransD shows the best accuracy in FB15K. These
results imply that the proposed logical property pre-
serving embeddings solve the problems of existing
translation-based embeddings effectively.

6 Conclusion

This paper has proposed a new translation-based
knowledge graph embedding that preserves logical
properties of relations. Transitivity and symmetric-
ity are very important characteristics of relations
for representing and inferring knowledge, and the
triples with such a relation take a large proportion
of real-world knowledge graphs. In order to pre-
serve the logical properties in an embedding space,
an entity is forced to have multiple vector represen-

914



Table 5: Experimental results on FB15K according to mapping properties of relations. (%)

Tasks Predicting Head (Hits@10) Predicting Tail (Hits@10)
Relation Category 1-to-1 1-to-N N-to-1 N-to-N 1-to-1 1-to-N N-to-1 N-to-N

TransE 43.7 65.7 18.2 47.2 43.7 19.7 66.7 50.0
TransH (unif) 66.7 81.7 30.2 57.4 63.7 30.1 83.2 60.8
TransH (bern) 66.8 87.6 28.7 64.5 65.5 39.8 83.3 67.2
TransR (unif) 76.9 77.9 38.1 66.9 76.2 38.4 76.2 69.1
TransR (bern) 78.8 89.2 34.1 69.2 79.2 37.4 90.4 72.1
TransD (unif) 80.7 85.8 47.1 75.6 80.0 54.5 80.7 77.9
TransD (bern) 86.1 95.5 39.8 78.5 85.4 50.6 94.4 81.2

lppTransE (unif) 77.5 86.5 43.1 75.9 76.7 46.4 79.4 75.5
lppTransE (bern) 78.3 93.5 35.4 74.7 78.1 45.9 80.3 76.6
lppTransR (unif) 75.2 88.1 41.5 75.2 76.6 44.9 82.7 72.6
lppTransR (bern) 84.3 94.2 49.7 79.2 84.3 46.9 91.9 79.6
lppTransD (unif) 82.7 89.5 53.2 81.6 82.3 52.6 84.8 81.5
lppTransD (bern) 86.0 94.2 54.4 82.2 79.7 43.2 95.3 79.7

tations according to its role in a triple. This idea has
been applied to TransE, TransR, and TransD, and
they are called as lppTransE, lppTransR, and lpp-
TransD. Their superiority was shown through two
tasks of link prediction and triple classification. The
logical property preserving embeddings showed the
improved performance over their base models2. Es-
pecially, lppTransD showed the state-of-the-art per-
formance in both tasks. These results imply that the
proposed role-specific projection is plausible to pre-
serve logical properties of relations.

Acknowledgments

This work was supported by Institute for Infor-
mation & communications Technology Promotion
(IITP) grant funded by the Korea government
(MSIP) (No. R0101-15-0054, WiseKB: Big data
based self-evolving knowledge base and reasoning
platform).

References

[Bollacker et al.2008] Kurt Bollacker, Colin Evans,
Praveen Paritosh, Tim Sturge, and Jamie Taylor.
2008. Freebase: a collaboratively created graph
database for structuring human knowledge. In
Proceedings of the 2008 ACM SIGMOD Interna-

2 The source codes and resources can be downloaded from
http://ml.knu.ac.kr/lppKE.

tional Conference on Management of Data, pages
1247–1250.

[Bordes et al.2011] Antoine Bordes, Jason Weston, Ro-
nan Collobert, and Yoshua Bengio. 2011. Learning
structured embeddings of knowledge bases. In Pro-
ceedings of the 28th AAAI Conference on Artificial In-
telligence, pages 301–306.

[Bordes et al.2013] Antoine Bordes, Nicolas Usunier,
Alberto Garcia-Duran, Jason Weston, and Oksana
Yakhnenko. 2013. Translating embeddings for mod-
eling multi-relational data. In Advances in Neural In-
formation Processing Systems, pages 2787–2795.

[Bordes et al.2014] Antoine Bordes, Xavier Glorot, Ja-
son Weston, and Yoshua Bengio. 2014. A seman-
tic matching energy function for learning with multi-
relational data. Machine Learning, 94(2):233–259.

[Gardner and Mitchell2015] Matt Gardner and Tom
Mitchell. 2015. Efficient and expressive knowledge
base completion using subgraph feature extraction.
In Proceedings of the 2015 Conference on Empirical
Methods in Natural Language Processing, pages
1488–1498.

[Guo et al.2015] Shu Guo, Quan Wang, Bin Wang, Li-
hong Wang, and Li Guo. 2015. Semantically smooth
knowledge graph embedding. In Proceedings of the
53rd Annual Meeting of the Association for Computa-
tional Linguistics and the 7th International Joint Con-
ference on Natural Language Processing, pages 84–
94.

[Jenatton et al.2012] Rodolphe Jenatton, Nicolas L.
Roux, Antoine Bordes, and Guillaume R. Obozinski.
2012. A latent factor model for highly multi-relational
data. In Advances in Neural Information Processing
Systems, pages 3167–3175.

915



[Ji et al.2015] Guoliang Ji, Shizhu He, Liheng Xu, Kang
Liu, and Jun Zhao. 2015. Knowledge graph embed-
ding via dynamic mapping matrix. In Proceedings of
the 53rd Annual Meeting of the Association for Com-
putational Linguistics and the 7th International Joint
Conference on Natural Language Processing, pages
687–696.

[Jiang et al.2012] Shangpu Jiang, Daniel Lowd, and De-
jing Dou. 2012. Learning to refine an automati-
cally extracted knowledge base using markov logic. In
Proceedings of the IEEE International Conference on
Data Mining, pages 912–917.

[Lao and Cohen2010] Ni Lao and William W. Cohen.
2010. Relational retrieval using a combination of
path-constrained random walks. Machine Learning,
81(1):53–67.

[Lao et al.2011] Ni Lao, Tom Mitchell, and William W.
Cohen. 2011. Random walk inference and learning
in a large scale knowledge base. In Proceedings of
the 2011 Conference on Empirical Methods in Natural
Language Processing, pages 529–539.

[Lin et al.2015] Yankai Lin, Zhiyuan Liu, Maosong Sun,
Yang Liu, and Xuan Zhu. 2015. Learning entity and
relation embeddings for knowledge graph completion.
In Proceedings of the 29th AAAI Conference on Artifi-
cial Intelligence, pages 2181–2187.

[Miller1995] George A. Miller. 1995. WordNet: A lexi-
cal database for english. Communications of the ACM,
38(11):39–41.

[Neelakantan and Chang2015] Arvind Neelakantan and
Ming-Wei Chang. 2015. Inferring missing entity type
instances for knowledge base completion: New dataset
and methods. In Proceedings of the 2015 Conference
of the North American Chapter of the Association for
Computational Linguistics: Human Language Tech-
nologies, pages 515–525.

[Neelakantan et al.2015] Arvind Neelakantan, Benjamin
Roth, and Andrew McCallum. 2015. Compositional
vector space models for knowledge base completion.
In Proceedings of the 53rd Annual Meeting of the As-
sociation for Computational Linguistics and the 7th
International Joint Conference on Natural Language
Processing, pages 156–166.

[Nickel et al.2011] Maximilian Nickel, Volker Tresp, and
Hans peter Kriegel. 2011. A three-way model for col-
lective learning on multi-relational data. In Proceed-
ings of the 28st International Conference on Machine
Learning, pages 809–816.

[Nickel et al.2015] Maximilian Nickel, Kevin Murphy,
Volker Tresp, and Evgeniy Gabrilovich. 2015. A
review of relational machine learning for knowl-
edge graphs: From multi-relational link prediction
to automated knowledge graph construction. CoRR,
abs/1503.00759.

[Pujara et al.2013] Jay Pujara, Hui Miao, Lise Getoor, and
William Cohen. 2013. Knowledge graph identifica-
tion. In Proceedings of the 12th International Seman-
tic Web Conference, pages 542–557.

[Socher et al.2013] Richard Socher, Danqi Chen, Christo-
pher D. Manning, and Andrew Ng. 2013. Reasoning
with neural tensor networks for knowledge base com-
pletion. In Advances in Neural Information Process-
ing Systems, pages 926–934.

[Suchanek et al.2007] Fabian M. Suchanek, Gjergji Kas-
neci, and Gerhard Weikum. 2007. YAGO: A core of
semantic knowledge unifying wordnet and wikipedia.
In Proceedings of the 16th International Conference
on World Wide Web, pages 697–706.

[Wang et al.2014] Zhen Wang, Jianwen Zhang, Jianlin
Feng, and Zheng Chen. 2014. Knowledge graph em-
bedding by translating on hyperplanes. In Proceedings
of the 28th AAAI Conference on Artificial Intelligence,
pages 1112–1119.

916


