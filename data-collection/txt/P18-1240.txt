



















































On the Automatic Generation of Medical Imaging Reports


Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers), pages 2577–2586
Melbourne, Australia, July 15 - 20, 2018. c©2018 Association for Computational Linguistics

2577

On the Automatic Generation of Medical Imaging Reports

Baoyu Jing†* Pengtao Xie†* Eric P. Xing†
†Petuum Inc, USA

*School of Computer Science, Carnegie Mellon University, USA
{baoyu.jing, pengtao.xie, eric.xing}@petuum.com

Abstract

Medical imaging is widely used in clin-
ical practice for diagnosis and treat-
ment. Report-writing can be error-prone
for unexperienced physicians, and time-
consuming and tedious for experienced
physicians. To address these issues, we
study the automatic generation of medical
imaging reports. This task presents sev-
eral challenges. First, a complete report
contains multiple heterogeneous forms of
information, including findings and tags.
Second, abnormal regions in medical im-
ages are difficult to identify. Third, the re-
ports are typically long, containing mul-
tiple sentences. To cope with these chal-
lenges, we (1) build a multi-task learning
framework which jointly performs the pre-
diction of tags and the generation of para-
graphs, (2) propose a co-attention mecha-
nism to localize regions containing abnor-
malities and generate narrations for them,
(3) develop a hierarchical LSTM model to
generate long paragraphs. We demonstrate
the effectiveness of the proposed methods
on two publicly available datasets.

1 Introduction

Medical images, such as radiology and pathol-
ogy images, are widely used in hospitals for the
diagnosis and treatment of many diseases, such
as pneumonia and pneumothorax. The reading
and interpretation of medical images are usually
conducted by specialized medical professionals.
For example, radiology images are read by ra-
diologists. They write textual reports (Figure 1)
to narrate the findings regarding each area of the
body examined in the imaging study, specifically

Figure 1: An exemplar chest x-ray report. In the
impression section, the radiologist provides a di-
agnosis. The findings section lists the radiology
observations regarding each area of the body ex-
amined in the imaging study. The tags section lists
the keywords which represent the critical informa-
tion in the findings. These keywords are identified
using the Medical Text Indexer (MTI).

whether each area was found to be normal, abnor-
mal or potentially abnormal.

For less-experienced radiologists and patholo-
gists, especially those working in the rural area
where the quality of healthcare is relatively low,
writing medical-imaging reports is demanding.
For instance, to correctly read a chest x-ray im-
age, the following skills are needed (Delrue et al.,
2011): (1) thorough knowledge of the normal
anatomy of the thorax, and the basic physiology
of chest diseases; (2) skills of analyzing the radio-
graph through a fixed pattern; (3) ability of eval-
uating the evolution over time; (4) knowledge of
clinical presentation and history; (5) knowledge of
the correlation with other diagnostic results (labo-
ratory results, electrocardiogram, and respiratory
function tests).

For experienced radiologists and pathologists,
writing imaging reports is tedious and time-
consuming. In nations with large population such
as China, a radiologist may need to read hundreds



2578

of radiology images per day. Typing the findings
of each image into computer takes about 5-10 min-
utes, which occupies most of their working time.
In sum, for both unexperienced and experienced
medical professionals, writing imaging reports is
unpleasant.

This motivates us to investigate whether it is
possible to automatically generate medical image
reports. Several challenges need to be addressed.
First, a complete diagnostic report is comprised
of multiple heterogeneous forms of information.
As shown in Figure 1, the report for a chest x-
ray contains impression which is a sentence, find-
ings which are a paragraph, and tags which are a
list of keywords. Generating this heterogeneous
information in a unified framework is technically
demanding. We address this problem by building
a multi-task framework, which treats the predic-
tion of tags as a multi-label classification task, and
treats the generation of long descriptions as a text
generation task.

Second, how to localize image-regions and at-
tach the right description to them are challeng-
ing. We solve these problems by introducing a
co-attention mechanism, which simultaneously at-
tends to images and predicted tags and explores
the synergistic effects of visual and semantic in-
formation.

Third, the descriptions in imaging reports are
usually long, containing multiple sentences. Gen-
erating such long text is highly nontrivial. Rather
than adopting a single-layer LSTM (Hochreiter
and Schmidhuber, 1997), which is less capable
of modeling long word sequences, we leverage
the compositional nature of the report and adopt
a hierarchical LSTM to produce long texts. Com-
bined with the co-attention mechanism, the hierar-
chical LSTM first generates high-level topics, and
then produces fine-grained descriptions according
to the topics.

Overall, the main contributions of our work are:

• We propose a multi-task learning framework
which can simultaneously predict the tags
and generate the text descriptions.

• We introduce a co-attention mechanism for
localizing sub-regions in the image and gen-
erating the corresponding descriptions.

• We build a hierarchical LSTM to generate
long paragraphs.

• We perform extensive experiments to show
the effectiveness of the proposed methods.

The rest of the paper is organized as follows.
Section 2 reviews related works. Section 3 intro-
duces the method. Section 4 present the experi-
mental results and Section 5 concludes the paper.

2 Related Works

Textual labeling of medical images There have
been several works aiming at attaching “texts” to
medical images. In their settings, the target “texts”
are either fully-structured or semi-structured (e.g.
tags, templates), rather than natural texts. Kisilev
et al. (2015) build a pipeline to predict the at-
tributes of medical images. Shin et al. (2016)
adopt a CNN-RNN based framework to predict
tags (e.g. locations, severities) of chest x-ray
images. The work closest to ours is recently
contributed by (Zhang et al., 2017), which aims
at generating semi-structured pathology reports,
whose contents are restricted to 5 predefined top-
ics.

However, in the real-world, different physicians
usually have different writing habits and different
x-ray images will represent different abnormali-
ties. Therefore, collecting semi-structured reports
is less practical and thus it is important to build
models to learn from natural reports. To the best
of our knowledge, our work represents the first
one that generates truly natural reports written by
physicians, which are usually long and cover di-
verse topics.

Image captioning with deep learning Image
captioning aims at automatically generating text
descriptions for given images. Most recent im-
age captioning models are based on a CNN-RNN
framework (Vinyals et al., 2015; Fang et al., 2015;
Karpathy and Fei-Fei, 2015; Xu et al., 2015; You
et al., 2016; Krause et al., 2017).

Recently, attention mechanisms have been
shown to be useful for image captioning (Xu et al.,
2015; You et al., 2016). Xu et al. (2015) introduce
a spatial-visual attention mechanism over image
features extracted from intermediate layers of the
CNN. You et al. (2016) propose a semantic atten-
tion mechanism over tags of given images. To bet-
ter leverage both the visual features and semantic
tags, we propose a co-attention mechanism for re-
port generation.

Instead of only generating one-sentence caption



2579

Figure 2: Illustration of the proposed model. MLC denotes a multi-label classification network. Seman-
tic features are the word embeddings of the predicted tags. The boldfaced tags “calcified granuloma” and
“granuloma” are attended by the co-attention network.

for images, Krause et al. (2017) and Liang et al.
(2017) generate paragraph captions using a hier-
archical LSTM. Our method also adopts a hierar-
chical LSTM for paragraph generation, but unlike
Krause et al. (2017), we use a co-attention network
to generate topics.

3 Methods

3.1 Overview

A complete diagnostic report for a medical image
is comprised of both text descriptions (long para-
graphs) and lists of tags, as shown in Figure 1. We
propose a multi-task hierarchical model with co-
attention for automatically predicting keywords
and generating long paragraphs. Given an image
which is divided into regions, we use a CNN to
learn visual features for these patches. Then these
visual features are fed into a multi-label classifi-
cation (MLC) network to predict the relevant tags.
In the tag vocabulary, each tag is represented by a
word-embedding vector. Given the predicted tags
for a specific image, their word-embedding vec-
tors serve as the semantic features of this image.
Then the visual features and semantic features are
fed into a co-attention model to generate a context
vector that simultaneously captures the visual and
semantic information of this image. As of now, the
encoding process is completed.

Next, starting from the context vector, the de-
coding process generates the text descriptions.
The description of a medical image usually con-
tains multiple sentences, and each sentence fo-
cuses on one specific topic. Our model leverages
this compositional structure to generate reports in
a hierarchical way: it first generates a sequence
of high-level topic vectors representing sentences,
then generates a sentence from each topic vector.
Specifically, the context vector is inputted into a

sentence LSTM, which unrolls for a few steps and
produces a topic vector at each step. A topic vector
represents the semantics of a sentence to be gen-
erated. Given a topic vector, the word LSTM takes
it as input and generates a sequence of words to
form a sentence. The termination of the unrolling
process is controlled by the sentence LSTM.

3.2 Tag Prediction

The first task of our model is predicting the tags
of the given image. We treat the tag prediction
task as a multi-label classification task. Specifi-
cally, given an image I , we first extract its features
{vn}Nn=1 ∈ RD from an intermediate layer of a
CNN, and then feed {vn}Nn=1 into a multi-label
classification (MLC) network to generate a distri-
bution over all of the L tags:

pl,pred(li = 1|{vn}Nn=1) ∝ exp(MLCi({vn}Nn=1)) (1)

where l ∈ RL is a tag vector, li = 1/0 denote the
presence and absence of the i-th tag respectively,
and MLCi means the i-th output of the MLC net-
work.

For simplicity, we extract visual features from
the last convolutional layer of the VGG-19 model
(Simonyan and Zisserman, 2014) and use the last
two fully connected layers of VGG-19 for MLC.

Finally, the embeddings of the M most likely
tags {am}Mm=1 ∈ RE are used as semantic features
for topic generation.

3.3 Co-Attention

Previous works have shown that visual attention
alone can perform fairly well for localizing ob-
jects (Ba et al., 2015) and aiding caption gener-
ation (Xu et al., 2015). However, visual attention



2580

does not provide sufficient high level semantic in-
formation. For example, only looking at the right
lower region of the chest x-ray image (Figure 1)
without accounting for other areas, we might not
be able to recognize what we are looking at, not
to even mention detecting the abnormalities. In
contrast, the tags can always provide the needed
high level information. To this end, we propose a
co-attention mechanism which can simultaneously
attend to visual and semantic modalities.

In the sentence LSTM at time step s, the joint
context vector ctx(s) ∈ RC is generated by a
co-attention network fcoatt({vn}Nn=1, {am}Mm=1,
h
(s−1)
sent ), where h

(s−1)
sent ∈ RH is the sentence

LSTM hidden state at time step s − 1. The co-
attention network fcoatt uses a single layer feed-
forward network to compute the soft visual atten-
tions and soft semantic attentions over input image
features and tags:

αv,n ∝ exp(Wvatt tanh(Wvvn +Wv,hh
(s−1)
sent )) (2)

αa,m ∝ exp(Waatt tanh(Waam +Wa,hh
(s−1)
sent )) (3)

where Wv, Wv,h, and Wvatt are parameter ma-
trices of the visual attention network. Wa, Wa,h,
and Waatt are parameter matrices of the semantic
attention network.

The visual and semantic context vectors are
computed as:

v
(s)
att =

N∑
n=1

αv,nvn, a
(s)
att =

M∑
m=1

αa,mam.

There are many ways to combine the visual and
semantic context vectors such as concatenation
and element-wise operations. In this paper, we
first concatenate these two vectors as [v(s)att;a

(s)
att],

and then use a fully connected layer Wfc to ob-
tain a joint context vector:

ctx(s) = Wfc[v
(s)
att;a

(s)
att]. (4)

3.4 Sentence LSTM
The sentence LSTM is a single-layer LSTM that
takes the joint context vector ctx ∈ RC as its
input, and generates topic vector t ∈ RK for
word LSTM through topic generator and deter-
mines whether to continue or stop generating cap-
tions by a stop control component.

Topic generator We use a deep output layer
(Pascanu et al., 2014) to strengthen the context in-
formation in topic vector t(s), by combining the
hidden state h(s)sent and the joint context vector
ctx(s) of the current step:

t(s) = tanh(Wt,hsenth
(s)
sent +Wt,ctxctx

(s)) (5)

where Wt,hsent and Wt,ctx are weight parame-
ters.

Stop control We also apply a deep output layer
to control the continuation of the sentence LSTM.
The layer takes the previous and current hidden
state h(s−1)sent , h

(s)
sent as input and produces a distri-

bution over {STOP=1, CONTINUE=0}:

p(STOP |h(s−1)sent ,h
(s)
sent) ∝

exp{Wstop tanh(Wstop,s−1h(s−1)sent +Wstop,sh
(s)
sent)}

(6)

where Wstop, Wstop,s−1 and Wstop,s are parame-
ter matrices. If p(STOP |h(s−1)sent ,h

(s)
sent) is greater

than a predefined threshold (e.g. 0.5), then the sen-
tence LSTM will stop producing new topic vec-
tors and the word LSTM will also stop producing
words.

3.5 Word LSTM
The words of each sentence are generated by a
word LSTM. Similar to (Krause et al., 2017), the
topic vector t produced by the sentence LSTM and
the special START token are used as the first and
second input of the word LSTM, and the subse-
quent inputs are the word sequence.

The hidden state hword ∈ RH of the word
LSTM is directly used to predict the distribution
over words:

p(word|hword) ∝ exp(Wouthword) (7)

where Wout is the parameter matrix. After each
word-LSTM has generated its word sequences, the
final report is simply the concatenation of all the
generated sequences.

3.6 Parameter Learning
Each training example is a tuple (I , l, w) where I
is an image, l denotes the ground-truth tag vector,
and w is the diagnostic paragraph, which is com-
prised of S sentences and each sentence consists
of Ts words.



2581

Given a training example (I , l, w), our model
first performs multi-label classification on I and
produces a distribution pl,pred over all tags. Note
that l is a binary vector which encodes the pres-
ence and absence of tags. We can obtain the
ground-truth tag distribution by normalizing l:
pl = l/||l||1. The training loss of this step is a
cross-entropy loss `tag between pl and pl,pred.

Next, the sentence LSTM is unrolled for S steps
to produce topic vectors and also distributions over
{STOP, CONTINUE}: psstop. Finally, the S topic
vectors are fed into the word LSTM to generate
words ws,t. The training loss of caption gen-
eration is the combination of two cross-entropy
losses: `sent over stop distributions psstop and `word
over word distributions ps,t. Combining the pieces
together, we obtain the overall training loss:

`(I, l,w) = λtag`tag

+ λsent

S∑
s=1

`sent(p
s
stop, I{s = S})

+ λword

S∑
s=1

Ts∑
t=1

`word(ps,t, ws,t)

(8)

In addition to the above training loss, there is
also a regularization term for visual and seman-
tic attentions. Similar to (Xu et al., 2015), let
α ∈ RN×S and β ∈ RM×S be the matrices of vi-
sual and semantic attentions respectively, then the
regularization loss over α and β is:

`reg = λreg[

N∑
n

(1−
S∑
s

αn,s)
2+

M∑
m

(1−
S∑
s

βm,s)
2] (9)

Such regularization encourages the model to pay
equal attention over different image regions and
different tags.

4 Experiments

In this section, we evaluate the proposed model
with extensive quantitative and qualitative experi-
ments.

4.1 Datasets
We used two publicly available medical image
datasets to evaluate our proposed model.

IU X-Ray The Indiana University Chest X-
Ray Collection (IU X-Ray) (Demner-Fushman
et al., 2015) is a set of chest x-ray images paired
with their corresponding diagnostic reports. The

dataset contains 7,470 pairs of images and reports.
Each report consists of the following sections: im-
pression, findings, tags1, comparison, and indica-
tion. In this paper, we treat the contents in impres-
sion and findings as the target captions2 to be gen-
erated and the Medical Text Indexer (MTI) anno-
tated tags as the target tags to be predicted (Figure
1 provides an example).

We preprocessed the data by converting all to-
kens to lowercases, removing all of non-alpha to-
kens, which resulting in 572 unique tags and 1915
unique words. On average, each image is asso-
ciated with 2.2 tags, 5.7 sentences, and each sen-
tence contains 6.5 words. Besides, we find that
top 1,000 words cover 99.0% word occurrences in
the dataset, therefore we only included top 1,000
words in the dictionary. Finally, we randomly se-
lected 500 images for validation and 500 images
for testing.

PEIR Gross The Pathology Education Informa-
tional Resource (PEIR) digital library3 is a pub-
lic medical image library for medical education.
We collected the images together with their de-
scriptions in the Gross sub-collection, resulting in
the PEIR Gross dataset that contains 7,442 image-
caption pairs from 21 different sub-categories.
Different from the IU X-Ray dataset, each caption
in PEIR Gross contains only one sentence. We
used this dataset to evaluate our model’s ability of
generating single-sentence report.

For PEIR Gross, we applied the same prepro-
cessing as IU X-Ray, which yields 4,452 unique
words. On average, each image contains 12.0
words. Besides, for each caption, we selected 5
words with the highest tf-idf scores as tags.

4.2 Implementation Details

We used the full VGG-19 model (Simonyan and
Zisserman, 2014) for tag prediction. As for
the training loss of the multi-label classification
(MLC) task, since the number of tags for semantic
attention is fixed as 10, we treat MLC as a multi-
label retrieval task and adopt a softmax cross-
entropy loss (a multi-label ranking loss), similar
to (Gong et al., 2013; Guillaumin et al., 2009).

1There are two types of tags: manually generated (MeSH)
and Medical Text Indexer (MTI) generated.

2The impression and findings sections are concatenated
together as a long paragraph, since impression can be viewed
as a conclusion or topic sentence of the report.

3PEIR is c©University of Alabama at Birmingham, De-
partment of Pathology. (http://peir.path.uab.edu/library/)



2582

Dataset Methods BLEU-1 BLEU-2 BLEU-3 BLEU-4 METEOR ROUGE CIDER

IU X-Ray

CNN-RNN (Vinyals et al., 2015) 0.316 0.211 0.140 0.095 0.159 0.267 0.111
LRCN (Donahue et al., 2015) 0.369 0.229 0.149 0.099 0.155 0.278 0.190
Soft ATT (Xu et al., 2015) 0.399 0.251 0.168 0.118 0.167 0.323 0.302
ATT-RK (You et al., 2016) 0.369 0.226 0.151 0.108 0.171 0.323 0.155
Ours-no-Attention 0.505 0.383 0.290 0.224 0.200 0.420 0.259
Ours-Semantic-only 0.504 0.371 0.291 0.230 0.207 0.418 0.286
Ours-Visual-only 0.507 0.373 0.297 0.238 0.211 0.426 0.300
Ours-CoAttention 0.517 0.386 0.306 0.247 0.217 0.447 0.327

PEIR Gross

CNN-RNN (Vinyals et al., 2015) 0.247 0.178 0.134 0.092 0.129 0.247 0.205
LRCN (Donahue et al., 2015) 0.261 0.184 0.136 0.088 0.135 0.254 0.203
Soft ATT (Xu et al., 2015) 0.283 0.212 0.163 0.113 0.147 0.271 0.276
ATT-RK (You et al., 2016) 0.274 0.201 0.154 0.104 0.141 0.264 0.279
Ours-No-Attention 0.248 0.180 0.133 0.093 0.131 0.242 0.206
Ours-Semantic-only 0.263 0.191 0.145 0.098 0.138 0.261 0.274
Ours-Visual-only 0.284 0.209 0.156 0.105 0.149 0.274 0.280
Ours-CoAttention 0.300 0.218 0.165 0.113 0.149 0.279 0.329

Table 1: Main results for paragraph generation on the IU X-Ray dataset (upper part), and single sentence
generation on the PEIR Gross dataset (lower part). BLUE-n denotes the BLEU score that uses up to
n-grams.

In paragraph generation, we set the dimensions
of all hidden states and word embeddings as 512.
For words and tags, different embedding matri-
ces were used since a tag might contain multi-
ple words. We utilized the embeddings of the 10
most likely tags as the semantic feature vectors
{am}M=10m=1 . We extracted the visual features from
the last convolutional layer of the VGG-19 net-
work, which yields a 14× 14× 512 feature map.

We used the Adam (Kingma and Ba, 2014)
optimizer for parameter learning. The learning
rates for the CNN (VGG-19) and the hierarchi-
cal LSTM were 1e-5 and 5e-4 respectively. The
weights (λtag, λsent, λword and λreg) of different
losses were set to 1.0. The threshold for stop con-
trol was 0.5. Early stopping was used to prevent
over-fitting.

4.3 Baselines

We compared our method with several state-
of-the-art image captioning models: CNN-RNN
(Vinyals et al., 2015), LRCN (Donahue et al.,
2015), Soft ATT (Xu et al., 2015), and ATT-RK
(You et al., 2016). We re-implemented all of these
models and adopt VGG-19 (Simonyan and Zis-
serman, 2014) as the CNN encoder. Consider-
ing these models are built for single sentence cap-
tions and to better show the effectiveness of the
hierarchical LSTM and the attention mechanism
for paragraph generation, we also implemented a
hierarchical model without any attention: Ours-
no-Attention. The input of Ours-no-Attention is
the overall image feature of VGG-19, which has
a dimension of 4096. Ours-no-Attention can be
viewed as a CNN-RNN (Vinyals et al., 2015)
equipped with a hierarchical LSTM decoder. To

further show the effectiveness of the proposed co-
attention mechanism, we also implemented two
ablated versions of our model: Ours-Semantic-
only and Ours-Visual-only, which takes solely the
semantic attention or visual attention context vec-
tor to produce topic vectors.

4.4 Quantitative Results

We report the paragraph generation (upper part of
Table 1) and one sentence generation (lower part
of Table 1) results using the standard image cap-
tioning evaluation tool 4 which provides evalua-
tion on the following metrics: BLEU (Papineni
et al., 2002), METEOR (Denkowski and Lavie,
2014), ROUGE (Lin, 2004), and CIDER (Vedan-
tam et al., 2015).

For paragraph generation, as shown in the upper
part of Table 1, it is clear that models with a single
LSTM decoder perform much worse than those
with a hierarchical LSTM decoder. Note that the
only difference between Ours-no-Attention and
CNN-RNN (Vinyals et al., 2015) is that Ours-
no-Attention adopts a hierarchical LSTM decoder
while CNN-RNN (Vinyals et al., 2015) adopts
a single-layer LSTM. The comparison between
these two models directly demonstrates the ef-
fectiveness of the hierarchical LSTM. This re-
sult is not surprising since it is well-known that a
single-layer LSTM cannot effectively model long
sequences (Liu et al., 2015; Martin and Cundy,
2018). Additionally, employing semantic atten-
tion alone (Ours-Semantic-only) or visual atten-
tion alone (Ours-Visual-only) to generate topic
vectors does not seem to help caption generation
a lot. The potential reason might be that visual at-

4https://github.com/tylin/coco-caption



2583

Figure 3: Illustration of paragraph generated by Ours-CoAttention, Ours-no-Attention, and Soft Atten-
tion models. The underlined sentences are the descriptions of detected abnormalities. The second image
is a lateral x-ray image. Top two images are positive results, the third one is a partial failure case and the
bottom one is failure case. These images are from test dataset.

tention can only capture the visual information of
sub-regions of the image and is unable to correctly
capture the semantics of the entire image. Se-
mantic attention is inadequate of localizing small
abnormal image-regions. Finally, our full model
(Ours-CoAttention) achieves the best results on all
of the evaluation metrics, which demonstrates the
effectiveness of the proposed co-attention mecha-
nism.

For the single-sentence generation results
(shown in the lower part of Table 1), the ab-
lated versions of our model (Ours-Semantic-only
and Ours-Visual-only) achieve competitive scores
compared with the state-of-the-art methods. Our
full model (Ours-CoAttention) outperforms all of
the baseline, which indicates the effectiveness of
the proposed co-attention mechanism.

4.5 Qualitative Results

4.5.1 Paragraph Generation
An illustration of paragraph generation by three
models (Ours-CoAttention, Ours-no-Attention
and Soft Attention models) is shown in Figure 3.

We can find that different sentences have different
topics. The first sentence is usually a high level de-
scription of the image, while each of the following
sentences is associated with one area of the image
(e.g. “lung”, “heart”). Soft Attention and Ours-
no-Attention models detect only a few abnormal-
ities of the images and the detected abnormali-
ties are incorrect. In contrast, Ours-CoAttention
model is able to correctly describe many true ab-
normalities (as shown in top three images). This
comparison demonstrates that co-attention is bet-
ter at capturing abnormalities.

For the third image, Ours-CoAttention model
successfully detects the area (“right lower lobe”)
which is abnormal (“eventration”), however, it
fails to precisely describe this abnormality. In ad-
dition, the model also finds abnormalities about
“interstitial opacities” and “atheroscalerotic calci-
fication”, which are not considered as true abnor-
mality by human experts. The potential reason for
this mis-description might be that this x-ray image
is darker (compared with the above images), and
our model might be very sensitive to this change.



2584

Figure 4: Visualization of co-attention for three examples. Each example is comprised of four things: (1)
image and visual attentions; (2) ground truth tags and semantic attention on predicted tags; (3) generated
descriptions; (4) ground truth descriptions. For the semantic attention, three tags with highest attention
scores are highlighted. The underlined tags are those appearing in the ground truth.

The image at the bottom is a failure case of
Ours-CoAttention. However, even though the
model makes the wrong judgment about the ma-
jor abnormalities in the image, it does find some
unusual regions: “lateral lucency” and “left lower
lobe”.

To further understand models’ ability of detect-
ing abnormalities, we present the portion of sen-
tences which describe the normalities and abnor-
malities in Table 2. We consider sentences which
contain “no”, “normal”, “clear”, “stable” as sen-
tences describing normalities. It is clear that Ours-
CoAttention best approximates the ground truth
distribution over normality and abnormality.

Method Normality Abnormality Total
Soft Attention 0.510 0.490 1.0

Ours-no-Attention 0.753 0.247 1.0
Ours-CoAttention 0.471 0.529 1.0

Ground Truth 0.385 0.615 1.0

Table 2: Portion of sentences which describe the
normalities and abnormalities in the image.

4.5.2 Co-Attention Learning

Figure 4 presents visualizations of co-attention.
The first property shown by Figure 4 is that the
sentence LSTM can generate different topics at
different time steps since the model focuses on
different image regions and tags for different sen-
tences. The next finding is that visual attention
can guide our model to concentrate on relevant re-



2585

gions of the image. For example, the third sen-
tence of the first example is about “cardio”, and
the visual attention concentrates on regions near
the heart. Similar behavior can also be found for
semantic attention: for the last sentence in the first
example, our model correctly concentrates on “de-
generative change” which is the topic of the sen-
tence. Finally, the first sentence of the last exam-
ple presents a mis-description caused by incorrect
semantic attention over tags. Such incorrect atten-
tion can be reduced by building a better tag pre-
diction module.

5 Conclusion

In this paper, we study how to automatically gen-
erate textual reports for medical images, with the
goal to help medical professionals produce reports
more accurately and efficiently. Our proposed
methods address three major challenges: (1) how
to generate multiple heterogeneous forms of in-
formation within a unified framework, (2) how to
localize abnormal regions and produce accurate
descriptions for them, (3) how to generate long
texts that contain multiple sentences or even para-
graphs. To cope with these challenges, we propose
a multi-task learning framework which jointly pre-
dicts tags and generates descriptions. We intro-
duce a co-attention mechanism that can simultane-
ously explore visual and semantic information to
accurately localize and describe abnormal regions.
We develop a hierarchical LSTM network that can
more effectively capture long-range semantics and
produce high quality long texts. On two medical
datasets containing radiology and pathology im-
ages, we demonstrate the effectiveness of the pro-
posed methods through quantitative and qualita-
tive studies.

References

Jimmy Ba, Volodymyr Mnih, and Koray Kavukcuoglu.
2015. Multiple object recognition with visual atten-
tion. ICLR.

Louke Delrue, Robert Gosselin, Bart Ilsen,
An Van Landeghem, Johan de Mey, and Philippe
Duyck. 2011. Difficulties in the interpretation of
chest radiography. In Comparative Interpretation of
CT and Standard Radiography of the Chest, pages
27–49. Springer.

Dina Demner-Fushman, Marc D Kohli, Marc B Rosen-
man, Sonya E Shooshan, Laritza Rodriguez, Sameer

Antani, George R Thoma, and Clement J McDon-
ald. 2015. Preparing a collection of radiology ex-
aminations for distribution and retrieval. Journal
of the American Medical Informatics Association,
23(2):304–310.

Michael Denkowski and Alon Lavie. 2014. Meteor
universal: Language specific translation evaluation
for any target language. In Proceedings of the ninth
workshop on statistical machine translation, pages
376–380.

Jeffrey Donahue, Lisa Anne Hendricks, Sergio Guadar-
rama, Marcus Rohrbach, Subhashini Venugopalan,
Kate Saenko, and Trevor Darrell. 2015. Long-term
recurrent convolutional networks for visual recogni-
tion and description. In Proceedings of the IEEE
conference on computer vision and pattern recogni-
tion, pages 2625–2634.

Hao Fang, Saurabh Gupta, Forrest Iandola, Rupesh K
Srivastava, Li Deng, Piotr Dollár, Jianfeng Gao, Xi-
aodong He, Margaret Mitchell, John C Platt, et al.
2015. From captions to visual concepts and back.
In Proceedings of the IEEE conference on computer
vision and pattern recognition, pages 1473–1482.

Yunchao Gong, Yangqing Jia, Thomas Leung, Alexan-
der Toshev, and Sergey Ioffe. 2013. Deep con-
volutional ranking for multilabel image annotation.
ICLR.

Matthieu Guillaumin, Thomas Mensink, Jakob Ver-
beek, and Cordelia Schmid. 2009. Tagprop: Dis-
criminative metric learning in nearest neighbor mod-
els for image auto-annotation. In Computer Vision,
2009 IEEE 12th International Conference on, pages
309–316. IEEE.

Sepp Hochreiter and Jürgen Schmidhuber. 1997.
Long short-term memory. Neural computation,
9(8):1735–1780.

Andrej Karpathy and Li Fei-Fei. 2015. Deep visual-
semantic alignments for generating image descrip-
tions. In Proceedings of the IEEE Conference on
Computer Vision and Pattern Recognition, pages
3128–3137.

Diederik P Kingma and Jimmy Ba. 2014. Adam: A
method for stochastic optimization. arXiv preprint
arXiv:1412.6980.

Pavel Kisilev, Eugene Walach, Ella Barkan, Boaz
Ophir, Sharon Alpert, and Sharbell Y Hashoul.
2015. From medical image to automatic medical re-
port generation. IBM Journal of Research and De-
velopment, 59(2/3):2–1.

Jonathan Krause, Justin Johnson, Ranjay Krishna, and
Li Fei-Fei. 2017. A hierarchical approach for gen-
erating descriptive image paragraphs. In The IEEE
Conference on Computer Vision and Pattern Recog-
nition (CVPR).



2586

Xiaodan Liang, Zhiting Hu, Hao Zhang, Chuang
Gan, and Eric P. Xing. 2017. Recurrent topic-
transition gan for visual paragraph generation. In
The IEEE International Conference on Computer Vi-
sion (ICCV).

Chin-Yew Lin. 2004. Rouge: A package for auto-
matic evaluation of summaries. In Text summariza-
tion branches out: Proceedings of the ACL-04 work-
shop, volume 8. Barcelona, Spain.

Pengfei Liu, Xipeng Qiu, Xinchi Chen, Shiyu Wu,
and Xuanjing Huang. 2015. Multi-timescale long
short-term memory neural network for modelling
sentences and documents. In Proceedings of the
2015 conference on empirical methods in natural
language processing, pages 2326–2335.

Eric Martin and Chris Cundy. 2018. Parallelizing
linear recurrent neural nets over sequence length.
ICLR.

Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic eval-
uation of machine translation. In Proceedings of
the 40th annual meeting on association for compu-
tational linguistics, pages 311–318. Association for
Computational Linguistics.

Razvan Pascanu, Caglar Gulcehre, Kyunghyun Cho,
and Yoshua Bengio. 2014. How to construct deep
recurrent neural networks. ICLR.

Hoo-Chang Shin, Kirk Roberts, Le Lu, Dina Demner-
Fushman, Jianhua Yao, and Ronald M Summers.
2016. Learning to read chest x-rays: recurrent neu-
ral cascade model for automated image annotation.
In Proceedings of the IEEE Conference on Com-
puter Vision and Pattern Recognition, pages 2497–
2506.

K. Simonyan and A. Zisserman. 2014. Very deep con-
volutional networks for large-scale image recogni-
tion. CoRR, abs/1409.1556.

Ramakrishna Vedantam, C Lawrence Zitnick, and Devi
Parikh. 2015. Cider: Consensus-based image de-
scription evaluation. In Proceedings of the IEEE
conference on computer vision and pattern recog-
nition, pages 4566–4575.

Oriol Vinyals, Alexander Toshev, Samy Bengio, and
Dumitru Erhan. 2015. Show and tell: A neural im-
age caption generator. In Proceedings of the IEEE
conference on computer vision and pattern recogni-
tion, pages 3156–3164.

Kelvin Xu, Jimmy Ba, Ryan Kiros, Kyunghyun Cho,
Aaron Courville, Ruslan Salakhudinov, Rich Zemel,
and Yoshua Bengio. 2015. Show, attend and tell:
Neural image caption generation with visual at-
tention. In International Conference on Machine
Learning, pages 2048–2057.

Quanzeng You, Hailin Jin, Zhaowen Wang, Chen Fang,
and Jiebo Luo. 2016. Image captioning with seman-
tic attention. In Proceedings of the IEEE Conference
on Computer Vision and Pattern Recognition, pages
4651–4659.

Zizhao Zhang, Yuanpu Xie, Fuyong Xing, Mason Mc-
Gough, and Lin Yang. 2017. Mdnet: A semantically
and visually interpretable medical image diagnosis
network. In Proceedings of the IEEE Conference
on Computer Vision and Pattern Recognition, pages
6428–6436.


