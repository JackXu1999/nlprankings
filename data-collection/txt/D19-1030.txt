




































Cross-lingual Structure Transfer for Relation and Event Extraction


Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing
and the 9th International Joint Conference on Natural Language Processing, pages 313–325,
Hong Kong, China, November 3–7, 2019. c©2019 Association for Computational Linguistics

313

Cross-lingual Structure Transfer for Relation and Event Extraction
Ananya Subburathinam1*, Di Lu1* , Heng Ji2,

Jonathan May3, Shih-Fu Chang4, Avirup Sil5, Clare Voss6
1Computer Science Department, Rensselaer Polytechnic Institute

2Computer Science Department, University of Illinois at Urbana-Champaign
3University of Southern California Information Sciences Institute

4Electrical Engineering Department, Columbia University
5IBM T.J. Watson Research Center

6Army Research Laboratory
hengji@illinois.edu

Abstract

The identification of complex semantic struc-
tures such as events and entity relations, al-
ready a challenging Information Extraction
task, is doubly difficult from sources written
in under-resourced and under-annotated lan-
guages. We investigate the suitability of cross-
lingual structure transfer techniques for these
tasks. We exploit relation- and event-relevant
language-universal features, leveraging both
symbolic (including part-of-speech and depen-
dency path) and distributional (including type
representation and contextualized representa-
tion) information. By representing all entity
mentions, event triggers, and contexts into this
complex and structured multilingual common
space, using graph convolutional networks, we
can train a relation or event extractor from
source language annotations and apply it to
the target language. Extensive experiments
on cross-lingual relation and event transfer
among English, Chinese, and Arabic demon-
strate that our approach achieves performance
comparable to state-of-the-art supervisedmod-
els trained on up to 3,000 manually annotated
mentions: up to 62.6% F-score for Relation
Extraction, and 63.1% F-score for Event Ar-
gument Role Labeling. The event argument
role labeling model transferred from English
to Chinese achieves similar performance as the
model trained from Chinese. We thus find
that language-universal symbolic and distribu-
tional representations are complementary for
cross-lingual structure transfer.

1 Introduction

Advanced Information Extraction (IE) tasks entail
predicting structures, such as relations between en-
tities, and events involving entities. Given a pair of
entity mentions, Relation Extraction aims to iden-
tify the relation between the mentions and clas-
sify it by predefined type. Event Extraction aims
to identify event triggers and their arguments in

Figure 1: Information Network example constructed by
Event (blue) and Relation (green dashed) Extraction.

unstructured texts and classify them respectively
by predefined types and roles. As Figure 1 il-
lustrates, both tasks entail predicting an informa-
tion network (Li et al., 2014) for each sentence,
where the entity mentions and event triggers are
nodes, and the relations and event-argument links
are edges labeled with their relation and argument
roles, respectively.
There are certain relations and events that are

of primary interest to a given community and so
are reported predominantly in the low-resource
language data sources available to that commu-
nity. For example, though English language news
will occasionally discuss the Person Aung San
Suu Kyi, the vast majority of Physical-Located
relations and Meeting events involving her are
only reported locally in Burmese news, and thus,
without Burmese relation and event extraction,
a knowledge graph of this person will lack this
available information. Unfortunately, publicly-
available gold-standard annotations for relation
and event extraction exist for only a few languages
(Doddington et al., 2004; Getman et al., 2018), and
Burmese is not among them. Compared to other
IE tasks such as name tagging, the annotations for
Relation and Event Extraction are also more costly
to obtain, because they are structured and require
a rich label space.
Recent research (Lin et al., 2017) has found that

relational facts are typically expressed by identi-
fiable patterns within languages and has shown



314

Tree Representations

Graph Convolutional Networks Encoder

Max-pooling

Softmax

Common Space 
Structural Representations

ff f f

hm1 hsent hm2ht

TRANSPORT 
(event)

Origin-ArgArtifact-Arg PHYS-LOCATED(relation)

Concatenation

DET

AUX

ADP

DET

VERB

nsubj:pass

det

obl

aux:pass

amod
case

det

VERB

NOUN
(PER)

NOUN
(FAC)

Команды врачей были замечены в упакованных
отделениях скорой помощи

(teams of doctors were seen in packed emergency rooms)

NOUN

AUX

ADP

NOUN

VERB

nsubj:pass

nmod

obl

aux:pass

amod
case

nmod

VERB

NOUN
(PER)

NOUN
(FAC)

amod multilingual wordembedding
part-of-speech
embedding
entity-type
embedding

ADJ

ff f f

hm1 hsent hm2ht

PHYS-LOCATED
(relation)

The detainees were taken to a
processing center

dependency
embedding

Figure 2: Multilingual common semantic space and cross-lingual structure transfer.

that the consistency in patterns observed across
languages can be used to improve relation extrac-
tion. Inspired by their results, we exploit language-
universal features relevant to relation and event ar-
gument identification and classification, by way
of both symbolic and distributional representa-
tions. For example, language-universal POS tag-
ging and universal dependency parsing is avail-
able for 76 languages (Nivre et al., 2018), en-
tity extraction is available for 282 languages (Pan
et al., 2017), and multi-lingual word embeddings
are available for 44 languages (Bojanowski et al.,
2017; Joulin et al., 2018). As shown in Figure 2,
even for distinct pairs of entity mentions (colored
pink and blue, in both English and Russian), the
structures share similar language-universal sym-
bolic features, such as a common labeled depen-

dency path, as well as distributional features, such
as multilingual word embeddings.

Based on these language-universal representa-
tions, we then project all entity mentions, event
triggers and their contexts into one multilingual
common space. Unlike recent work on multilin-
gual common space construction that makes use of
linear mappings (Mikolov et al., 2013; Rothe et al.,
2016; Zhang et al., 2016; Lazaridou et al., 2015;
Xing et al., 2015; Smith et al., 2017) or canonical
correlation analysis (CCA) (Ammar et al., 2016;
Faruqui and Dyer, 2014; Lu et al., 2015) to trans-
fer surface features across languages, our major
innovation is to convert the text data into struc-
tured representations derived from universal de-
pendency parses and enhanced with distributional
information to capture individual entities as well



315

as the relations and events involving those entities,
so we can share structural representations across
multiple languages.
Then we construct a novel cross-lingual struc-

ture transfer learning framework to project source
language (SL) training data and target language
(TL) test data into the common semantic space, so
that we can train a relation or event extractor from
SL annotations and apply the resulting extractor
to TL texts. We adopt graph convolutional net-
works (GCN) (Kipf and Welling, 2017; Marcheg-
giani and Titov, 2017) to encode graph structures
over the input data, applying graph convolution
operations to generate entity and word represen-
tations in a latent space. In contrast to other en-
coders such a Tree-LSTM (Tai et al., 2015), GCN
can cover more complete contextual information
from dependency parses because, for each word,
it captures all parse tree neighbors of the word,
rather than just the child nodes of the word. Using
this shared encoder, we treat the two tasks of re-
lation extraction and event argument role labeling
as mappings from the latent space to relation type
and to event type and argument role, respectively.
Extensive experiments on cross-lingual relation

and event transfer among English, Chinese, and
Arabic show that our approach achieves highly
promising performance on both tasks.

2 Model

2.1 Overview

Our cross-lingual structure transfer approach (see
Figure 2) consists of four steps: (1) Convert each
sentence in any language into a language-universal
tree structure based on universal dependency pars-
ing. (2) For each node in the tree structure, cre-
ate a representation from the concatenation of
multilingual word embedding, language-universal
POS embedding, dependency role embedding and
entity-type embedding, so that all sentences, inde-
pendent of their language, are represented within
one shared semantic space. (3) Adopt GCN to
generate contextualized word representations by
leveraging information from neighbors derived
from the dependency parsing tree, and (4) Using
this shared semantic space, train relation and event
argument extractors with high-resource language
training data, and apply the resulting extractors to
texts of low-resource languages that do not have
any relation or event argument annotations.

2.2 Tree Representations

Most previous approaches regard a sentence as
a linear sequence of words, and incorporate
language-specific information such as word or-
der. Unlike sequence representations, tree repre-
sentations such as constituency trees and depen-
dency trees are typically constructed following a
combination of syntactic principles and annotation
guidelines designed by linguists. The resulting
structures, such as the verb – subject relation
and the verb – object relation, are found across
languages. In this paper, we choose dependency
trees as the sentence representations because the
community has made great efforts at developing
language-universal dependency parsing resources
across 83 languages (Nivre et al., 2016).
We define the dependency-based tree represen-

tation for a sentence as G = (V,E), where
V = {v1, v2, ..., vN} is a set of words, and E =
{e1, e2, ..., eM} is a set of language-universal syn-
tactic relations. N is the number of words in the
sentence and M is the number of dependency re-
lations between words. To make this tree repre-
sentation language-universal, we first convert each
tree node into a vector which is a concatenation of
three language-universal representations at word-
level: multilingual word embedding, POS embed-
ding (Nivre et al., 2016), entity-type embedding,
and dependency relation embedding. More details
are reported in Section 3.2.

2.3 GCN Encoder

Structural information is important for relation ex-
traction and event argument role labeling, thus we
aim to generate contextualized word representa-
tions by leveraging neighbors in dependency trees
for each node.
Our GCN encoder is based on the monolingual

design by Zhang et al. (2018b). The graphical
sentence representation obtained from dependency
parsing of a sentence with N tokens is converted
into an N × N adjacency matrix A, with added
self-connections at each node to help capture infor-
mation about the current node itself, as in Kipf and
Welling (2017). Here, Ai,j = 1 denotes the pres-
ence of a directed edge from node i to node j in the
dependency tree. Initially, each node contains dis-
tributional information about the ith word, includ-
ingword embeddingxwi , embeddings for symbolic
information including its POS tag xpi , dependency
relation xdi and entity type xei .



316

We represent this initial representation h0i as fol-
lows:

h
(0)
i = x

w
i ⊕ x

p
i ⊕ x

d
i ⊕ xei

Then, at the kth layer of convolution, the hidden
representation is derived from the representations
of its neighbors at the (k − 1)th layer. Thus, the
hidden representation at the kth layer for the ith
node is computed as:

h
(k)
i = ReLU

(
N∑
j=0

AijW
(k)h

(k−1)
j

di + b(k)

)

where W is the weight vector, b is the bias term,
and di represents the degree of the ith node. The
denominator in the equation denotes the normal-
ization factor to neutralize the negative impact of
node degree (Zhang et al., 2018b). The final hid-
den representation of each node after the kth layer
is the encoding of each word h(k)i in our language
universal common space, and incorporates infor-
mation about neighbors up to k hops away in the
dependency tree.

2.4 Application on Relation Extraction
The GCN encoder generates the final hidden rep-
resentation, h(k)i , for each of the n nodes. We per-
form max-pooling over these final node represen-
tations to obtain a single vector representation for
the sentence, hs. Similar to previous work (Zhang
et al., 2018b), we also use the following recipe to
obtain relation type classification results for each
mention pair in a sentence: (1) max-pooling over
the final representations of the nodes represent-
ing entity mentions, to get a single vector repre-
sentation for each mention in a pair under con-
sideration, hm1 and hm2 , (2) a concatenation of
three results of max-pooling: ([hm1 ;hs;hm2 ]) to
combine contextual sentence information with en-
tity mention information (Santoro et al., 2017;
Lee et al., 2017; Zhang et al., 2018b), (3) a lin-
ear layer to generate a combined representation of
these concatenated results, and (4) a Softmax out-
put layer for relation type classification. The ob-
jective function used here is as follows:

Lr =

N∑
i=1

Li∑
j=1

yij log(σ(U r · [hm1i ;hsij ;hm2j ]))

(1)

where U r is a weight matrix.

2.5 Application on Event Argument Role
Labeling

Event argument role labeling distinguishes argu-
ments from non-arguments and classifies argu-
ments by argument role. To label the role of an
argument candidate (xaj ) for an event trigger (xti),
we first generate the sentence representation hs,
argument candidate representation ha, and trigger
representationht by applying pooling on sentence,
argument candidate xaj and event trigger xti respec-
tively, which is the same as that for relation extrac-
tion. The mapping function from the latent space
to argument roles is composed of a concatenation
operation ([ht;hs;ha]), a linear layer (Ua) and a
Softmax output layer:

La =
N∑
i=1

Li∑
j=1

yij log(σ(Ua · [hti;hsij ;haj ])) (2)

where N is the number of event mentions, Li
is the number of argument candidates for ith event
mention and σ is the Sigmoid function.

3 Experiments

3.1 Data and Evaluation Metrics

Relation
Mentions

Event
Mentions

Event
Arguments

English 8,738 5,349 9,793
Chinese 9,317 3,333 8,032
Arabic 4,731 2,270 4,975

Table 1: Data statistics.

We choose the Automatic Content Extraction
(ACE) 2005 corpus (Walker et al., 2006) for our
experiments because it contains the most compre-
hensive gold-standard relation and event annota-
tions for three distinct languages: English, Chinese
and Arabic (see Table 1). Our target ontology in-
cludes the 7 entity types, 18 relation subtypes and
33 event subtypes defined in ACE. We randomly
choose 80% of the corpus for training, 10% for
development and 10% for blind test. We down-
sample the negative training instances by limiting
the number of negative samples to be no more than
the number of positive samples for each document.
For data preprocessing, we apply the Stanford

CoreNLP toolkit (Manning et al., 2014) for Chi-
nese word segmentation and English tokenization,
and the API provided by UDPipe (Straka and



317

Straková, 2017) for Arabic tokenization. We use
UDPipe1 for POS tagging and dependency parsing
for all three languages.
We follow the following criteria in previous

work (Ji and Grishman, 2008; Li et al., 2013; Li
and Ji, 2014) for evaluation:

• A relation mention is considered correct if its
relation type is correct, and the head offsets of
the two related entity mention arguments are
both correct.

• An event argument is correctly labeled if its
event type, offsets, and role label match any
of the reference argument mentions.

3.2 Training Details
We use themultilingual word embeddings released
by (Joulin et al., 2018) which are based on aligning
monolingual embeddings learned by fastText (Bo-
janowski et al., 2017) from Wikipedia. We adopt
the universal POS tag set (17 word categories) and
37 categories of dependency relations defined by
theUniversal Dependencies program (Nivre et al.,
2016), and seven entity types defined in ACE: per-
son, geopolitical entity, organization, facility, lo-
cation, weapon and vehicle. Table 2 shows the hy-
perparameters of our model.

Hyperparameter Value
word embedding size 300
POS embedding size 30
entity embedding size 30
dependency relation embedding size 30
hidden dimension size 200
dropout 0.5
number of layers 2
pooling function max pooling
mlp layers 2
learning rate 0.1
learning rate decay 0.9
batch size 50
optimization SGD

Table 2: GCN hyperparameters.

3.3 Overall Performance
In order to fully analyze the cross-lingual learn-
ing capabilities of our framework, we assess its

1Universal Dependencies 2.3 models: english-ewt-
ud-2.3-181115.udpipe, chinese-gsd-ud-2.3-181115.udpipe,
arabic-padt-ud-2.3-181115.udpipe

performance by applying models trained with var-
ious combinations of training and test data from
these three languages, as shown in Tables 3 and
4. We can see that the results are promising.
For both tasks, the models trained from English
are best, followed by Chinese, and then Arabic.
We find that extraction task performance degrades
as the accuracy of language-dependent tools (for
sentence segmentation, POS tagging, dependency
parsing) degrades.

Using English as training data, our cross-lingual
transfer approach achieves similar performance
on Chinese event argument role labeling (59.0%),
compared to the model trained from Chinese an-
notations (59.3%), which is much higher than the
best reported English-to-Chinese transfer result on
event argument role labeling (47.7%) (Hsi et al.,
2016).

We also show polyglot results for event argu-
ment role labeling in Table 4, by combing the train-
ing data from multiple languages. We observe
that our model benefits from the combination of
training data of multiple languages. The polyglot
transfer learning does not provide further gains to
relation extraction because the model converges
quickly on a small amount of training data.

Train
Test English Chinese Arabic

English 68.2 42.5 58.7
Chinese 62.6 69.4 54.0
Arabic 58.6 35.2 67.4

Table 3: Relation Extraction: overall performance (F-
score %) using perfect entity mentions.

Train
Test English Chinese Arabic

English 63.9 59.0 61.8
Chinese 51.6 59.3 60.6
Arabic 43.1 50.1 64.0
English + Chinese – – 63.1
English + Arabic – 60.1 –
Chinese + Arabic 51.9 – –

Table 4: Event Argument Role Labeling results (F1 %)
with perfect event triggers.



318

Feature Chinese Arabic
Multilingual Word Embedding 40.2 53.8
+ POS Embedding 40.1 54.9
+ Entity Embedding 40.8 56.9
+ Dependency Relation Embedding 42.5 58.7

Table 5: Relation Extraction: Contribution of adding
each language-universal feature consecutively for rela-
tion extraction on Arabic and Chinese test sets, using
the model trained on English (F1 scores (%)).

3.4 Ablation Study
Tables 5 and 6 show the impact of each feature cat-
egory. We can see that all categories help except
Chinese POS features for Relation Extraction and
Arabic POS features for Event Argument Role La-
beling.
Many Chinese word segmentation errors oc-

cur on tokens involved in names. For example,
“总统(NOUN, president) 萨姆(PROPN, Samuel)
·(PUNCT) 努乔马(PROPN, Nujoma)” is mis-
takenly tagged as “总(NOUN) 统萨姆(PROPN)
·(PUNCT)努乔(NOUN)马(NOUN)”.
In Arabic, sometimes one word is a combination

of Noun and Verb. For example, the single word
”الإسرائيلي“ means “Israeli conflict” in English, in-
cluding both a trigger and an argument, which are
not separated by our tokenizer. In contrast, two en-
tity mentions are unlikely to be combined into one
word in Arabic, thus Relation Extraction does not
suffer from tokenization errors and corresponding
POS features.

Feature Chinese Arabic
Multilingual Word Embedding 52.4 56.6
+ POS Embedding 57.1 37.8
+ Entity Embedding 58.3 41.4
+ Dependency Relation Embedding 59.0 61.8

Table 6: Event Extraction: Contributions of adding
each language-universal feature consecutively for event
argument role labeling on Arabic and Chinese test
datasets, using a model trained on English (F1 scores
(%)).

3.5 Comparison with Supervised Approach
We also compare the results with supervised
monolingual models trained from manual anno-
tations in the same language. Figures 3 and 4
show the learning curves of these supervised mod-
els. For event argument role labeling, we can
see that without using any annotations for the tar-
get language, our approach achieves performance
comparable to the supervised models trained from

more than 3,000 manually annotated event ar-
gument mentions, which equal to approximately
1,326 Chinese sentences and 1,141 Arabic sen-
tences based on the statistics of ACE data.
Our model performs particularly well on

relation types or argument roles that require
deep understanding of wide contexts involv-
ing complex syntactic and semantic struc-
tures, such as PART-WHOLE:Artifact, PART-
WHOLE:Geographical, GEN-AFF:Org-Location
and ORG-AFF:Employment relations, and Injure:
Victim argument role. Despite only having 14
training instances, our model achieves near
100% F-score on PART-WHOLE:Artifact rela-
tions when transferred from English to Chinese.
Our model achieves 86% F1 score on PART-
WHOLE:Geographical relations when transferred
from English to Arabic, and 73% and 79% F1
scores on GEN-AFF:Org-Location relations
when transferred from English to Chinese and
to Arabic, respectively. In an Injure event, a
Person can either be an Agent or a Victim. Surface
lexical embedding features are often not sufficient
to disambiguate them. Our model is effective
at transferring structural information such as
dependency relations between words, and obtains
72.97% F1 score on labeling Injure: Victim when
transferred from English to Chinese, and 75.43 %
from English to Arabic.
In addition, our model achieves very high per-

formance on event argument roles for which entity
type is a strong indicator. For example, a weapon
is much more likely to play as an Instrument rather
than a Target in an Attack. Our model achieves
89.9% F1 score on Attack: Instrument and 91.4%
F1 score onPersonnel: POSITION argument roles
when transferred from English to Chinese.

3.6 Using System Extracted Name Mentions

Target Language F1 score
Chinese 56.9
Arabic 60.1

Table 7: Event Argument Role Labeling results (F1 %)
on Chinese and Arabic using English as training data
(with system generated entity mentions)

Table 7 shows the results of event argument role
labeling on Chinese and Arabic entity mentions
automatically extracted by Stanford CoreNLP in-
stead of manually annotated mentions. The sys-
tem extracted entity mentions introduce noise and



319

(a) Target Language: Chinese (b) Target Language: Arabic

Figure 3: Relation Extraction: Comparison between supervised monolingual GCNmodel and cross-lingual transfer
learning.

(a) Target Language: Chinese (b) Target Language: Arabic

Figure 4: Event Argument Role Labeling: Comparison between supervised monolingual GCN model and cross-
lingual transfer learning.

thus decrease the performance of the model, but
the overall results are still promising.

4 Related Work

A large number of supervised machine learning
techniques have been used for English event ex-
traction, including traditional techniques based on
symbolic features (Ji and Grishman, 2008; Liao
and Grishman, 2011), joint inference models (Li
et al., 2014; Yang and Mitchell, 2016), and re-
cently with neural networks (Nguyen and Grish-
man, 2015a; Nguyen et al., 2016; Chen et al., 2015;
Nguyen and Grishman, 2018; Liu et al., 2018b; Lu
and Nguyen, 2018; Liu et al., 2018a; Zhang et al.,
2018a, 2019). English relation extraction in the
early days also followed supervised paradigms (Li
and Ji, 2014; Zeng et al., 2014; Nguyen and Gr-
ishman, 2015b; Miwa and Bansal, 2016; Pawar
et al., 2017; Bekoulis et al., 2018; Wang et al.,
2018b). Recent efforts have attempted to reduce
annotation costs using distant supervision (Mintz

et al., 2009; Surdeanu et al., 2012;Min et al., 2013;
Angeli et al., 2014; Zeng et al., 2015; Quirk and
Poon, 2017; Qin et al., 2018; Wang et al., 2018a)
through knowledge bases (KBs), where entities
and static relations are plentiful. Distant super-
vision is less applicable for the task of event ex-
traction because very few dynamic events are in-
cluded in KBs. These approaches, however, incor-
porate language-specific characteristics and thus
are costly in requiring substantial amount of anno-
tations to adapt to a new language (Chen and Vin-
cent, 2012; Blessing and Schütze, 2012; Li et al.,
2012; Danilova et al., 2014; Agerri et al., 2016; Hsi
et al., 2016; Feng et al., 2016).

Regardless of the recent successes in applying
cross-lingual transfer learning to sequence label-
ing tasks, such as name tagging (e.g., (Mayhew
et al., 2017; Lin et al., 2018; Huang et al., 2019)),
only limited work has explored cross-lingual rela-
tion and event structure transfer. Most previous
efforts working with cross-lingual structure trans-



320

fer rely on bilingual dictionaries (Hsi et al., 2016),
parallel data (Chen and Ji, 2009; Kim et al., 2010;
Qian et al., 2014) or machine translation (Faruqui
and Kumar, 2015; Zou et al., 2018). Recent meth-
ods (Lin et al., 2017; Wang et al., 2018b) aggre-
gate consistent patterns and complementary infor-
mation across languages to enhance Relation Ex-
traction, but they do so exploiting only distribu-
tional representations.
Event extraction shares with Semantic Role La-

beling (SRL) the task of assigning to each event ar-
gument its event role label, in the process of com-
pleting other tasks to extract the full event struc-
ture (assigning event types to predicates and more
specific roles to arguments). Cross-lingual transfer
has been very successful for SRL. Early attempts
relied onword alignment (Van der Plas et al., 2011)
or bilingual dictionaries (Kozhevnikov and Titov,
2013). Recent work incorporates universal de-
pendencies (Prazák and Konopík, 2017) or multi-
lingual word embeddings for Polyglot SRL (Mul-
caire et al., 2018). Liu et al. (2019) and Mulcaire
et al. (2019) exploit multi-lingual contextualized
word embedding for SRL and other Polyglot NLP
tasks including dependency parsing and name tag-
ging. To the best of our knowledge, our work is the
first to construct a cross-lingual structure transfer
framework that combines language-universal sym-
bolic representations and distributional represen-
tations for relation and event extraction over texts
written in a language without any training data.
GCN has been successfully applied to sev-

eral individual monolingual NLP tasks, includ-
ing relation extraction (Zhang et al., 2018b),
event detection (Nguyen and Grishman, 2018),
SRL (Marcheggiani and Titov, 2017) and sentence
classification (Yao et al., 2019). We apply GCN to
construct multi-lingual structural representations
for cross-lingual transfer learning.

5 Conclusions and Future Work

We show how cross-lingual relation and event ar-
gument structural representations may be trans-
ferred between languages without any training
data for the target language, and conclude that
language-universal symbolic and distributional
representations are complementary for cross-
lingual structure transfer. In the future we will
explore more language-universal representations
such as visual features from topically-related im-
ages and videos and external background knowl-

edge.

Acknowledgement

This research is based upon work supported in part
by U.S. DARPA LORELEI Program HR0011-
15-C-0115, the Office of the Director of Na-
tional Intelligence (ODNI), Intelligence Advanced
Research Projects Activity (IARPA), via con-
tract FA8650-17-C-9116, and ARL NS-CTA No.
W911NF-09-2-0053. The views and conclusions
contained herein are those of the authors and
should not be interpreted as necessarily represent-
ing the official policies, either expressed or im-
plied, of DARPA, ODNI, IARPA, or the U.S. Gov-
ernment. The U.S. Government is authorized to
reproduce and distribute reprints for governmen-
tal purposes notwithstanding any copyright anno-
tation therein.

References
Rodrigo Agerri, Itziar Aldabe, Egoitz Laparra, Ger-

man Rigau Claramunt, Antske Fokkens, Paul Hui-
jgen, Rubén Izquierdo Beviá, Marieke van Erp, Piek
Vossen, Anne-Lyse Minard, et al. 2016. Multilin-
gual event detection using the newsreader pipelines.
In Proceedings of International Conference on Lan-
guage Resources and Evaluation 2016.

Waleed Ammar, George Mulcaire, Yulia Tsvetkov,
Guillaume Lample, Chris Dyer, and Noah A Smith.
2016. Massively multilingual word embeddings.
arXiv preprint arXiv:1602.01925.

Gabor Angeli, Julie Tibshirani, Jean Wu, and Christo-
pher D Manning. 2014. Combining distant and par-
tial supervision for relation extraction. In Proceed-
ings of the 2014 Conference on Empirical Methods
in Natural Language Processing.

Giannis Bekoulis, Johannes Deleu, Thomas Demeester,
and Chris Develder. 2018. Adversarial training
for multi-context joint entity and relation extrac-
tion. In Proceedings of the 2018 Conference on
Empirical Methods in Natural Language Process-
ing, pages 2830–2836, Brussels, Belgium. Associ-
ation for Computational Linguistics.

Andre Blessing and Hinrich Schütze. 2012. Crosslin-
gual distant supervision for extracting relations of
different complexity. In Proceedings of the 21st
ACM international conference on Information and
knowledge management.

Piotr Bojanowski, Edouard Grave, Armand Joulin, and
Tomas Mikolov. 2017. Enriching word vectors with
subword information. Transactions of the Associa-
tion for Computational Linguistics.

https://arxiv.org/pdf/1602.01925.pdf
https://www.aclweb.org/anthology/D14-1164
https://www.aclweb.org/anthology/D14-1164
https://doi.org/10.18653/v1/D18-1307
https://doi.org/10.18653/v1/D18-1307
https://doi.org/10.18653/v1/D18-1307
https://aclweb.org/anthology/Q17-1010
https://aclweb.org/anthology/Q17-1010


321

Chen Chen and Ng Vincent. 2012. Joint modeling
for Chinese event extraction with rich linguistic fea-
tures. In Proceedings of the 24th International Con-
ference on Computational Linguistics.

Yubo Chen, Liheng Xu, Kang Liu, Daojian Zeng, and
Jun Zhao. 2015. Event extraction via dynamicmulti-
pooling convolutional neural networks. In Proceed-
ings of the 53rd Annual Meeting of the Association
for Computational Linguistics and the 7th Interna-
tional Joint Conference on Natural Language Pro-
cessing (Volume 1: Long Papers).

Zheng Chen and Heng Ji. 2009. Can one language
bootstrap the other: a case study on event extraction.
In Proceedings of the NAACL HLT 2009 Workshop
on Semi-Supervised Learning for Natural Language
Processing.

Vera Danilova, Mikhail Alexandrov, and Xavier
Blanco. 2014. A survey of multilingual event extrac-
tion from text. In Proceedings of the International
Conference on Applications of Natural Language to
Data Bases/Information Systems.

George R Doddington, Alexis Mitchell, Mark A Przy-
bocki, Lance A Ramshaw, Stephanie M Strassel, and
Ralph M Weischedel. 2004. The automatic content
extraction (ACE) program-tasks, data, and evalua-
tion. In Proceedings of the Fourth International
Conference on Language Resources and Evaluation.

Manaal Faruqui and Chris Dyer. 2014. Improving vec-
tor space word representations using multilingual
correlation. In Proceedings of the 14th Conference
of the European Chapter of the Association for Com-
putational Linguistics.

Manaal Faruqui and Shankar Kumar. 2015. Multilin-
gual open relation extraction using cross-lingual pro-
jection. In Proceedings of the 2015 Conference of
the North American Chapter of the Association for
Computational Linguistics: Human Language Tech-
nologies.

Xiaocheng Feng, Lifu Huang, Duyu Tang, Heng Ji,
Bing Qin, and Ting Liu. 2016. A language-
independent neural network for event detection. In
Proceedings of the 54th Annual Meeting of the As-
sociation for Computational Linguistics (Volume 2:
Short Papers).

Jeremy Getman, Joe Ellis, Stephanie Strassel, Zhiyi
Song, and Jennifer Tracey. 2018. Laying the ground-
work for knowledge base population: Nine years of
linguistic resources for TACKBP. In Proceedings of
the Eleventh International Conference on Language
Resources and Evaluation (LREC-2018).

Andrew Hsi, Yiming Yang, Jaime Carbonell, and
Ruochen Xu. 2016. Leveraging multilingual train-
ing for limited resource event extraction. In Pro-
ceedings of COLING 2016, the 26th International
Conference on Computational Linguistics: Techni-
cal Papers.

Lifu Huang, Heng Ji, and Jonathan May. 2019. Cross-
lingual multi-level adversarial transfer to enhance
low-resource name tagging. In Proceedings of the
2019 Conference of the North American Chapter of
the Association for Computational Linguistics: Hu-
man Language Technologies, Volume 1 (Long and
Short Papers).

Heng Ji and Ralph Grishman. 2008. Refining event ex-
traction through cross-document inference. In Pro-
ceedings of the 46th Annual Meeting of the Associ-
ation for Computational Linguistics: Human Lan-
guage Technologies.

Armand Joulin, Piotr Bojanowski, Tomas Mikolov,
Hervé Jégou, and Edouard Grave. 2018. Loss in
translation: Learning bilingual word mapping with a
retrieval criterion. In Proceedings of the 2018 Con-
ference on Empirical Methods in Natural Language
Processing.

Seokhwan Kim, Minwoo Jeong, Jonghoon Lee, and
Gary Geunbae Lee. 2010. A cross-lingual annota-
tion projection approach for relation detection. In
Proceedings of the 23rd International Conference on
Computational Linguistics.

Thomas N. Kipf and Max Welling. 2017. Semi-
supervised classification with graph convolutional
networks. In International Conference on Learning
Representations.

Mikhail Kozhevnikov and Ivan Titov. 2013. Cross-
lingual transfer of semantic role labeling models. In
Proceedings of the 51st Annual Meeting of the Asso-
ciation for Computational Linguistics.

Angeliki Lazaridou, Georgiana Dinu, and Marco Ba-
roni. 2015. Hubness and pollution: Delving into
cross-space mapping for zero-shot learning. In Pro-
ceedings of the 53rd Annual Meeting of the Associ-
ation for Computational Linguistics and the 7th In-
ternational Joint Conference on Natural Language
Processing (Volume 1: Long Papers).

Kenton Lee, Luheng He, Mike Lewis, and Luke Zettle-
moyer. 2017. End-to-end neural coreference resolu-
tion. In Proceedings of the 2017 Conference on Em-
pirical Methods in Natural Language Processing.

Peifeng Li, Guodong Zhou, Qiaoming Zhu, and Libin
Hou. 2012. Employing compositional semantics and
discourse consistency in chinese event extraction. In
Proceedings of the 2012 Joint Conference on Empir-
ical Methods in Natural Language Processing and
Computational Natural Language Learning.

Qi Li and Heng Ji. 2014. Incremental joint extraction of
entity mentions and relations. In Proceedings of the
52nd Annual Meeting of the Association for Compu-
tational Linguistics (Volume 1: Long Papers).

Qi Li, Heng Ji, and Liang Huang. 2013. Joint event
extraction via structured prediction with global fea-
tures. In Proceedings of the 51st Annual Meeting of
the Association for Computational Linguistics (Vol-
ume 1: Long Papers).

https://www.aclweb.org/anthology/C12-1033
https://www.aclweb.org/anthology/C12-1033
https://www.aclweb.org/anthology/C12-1033
https://www.aclweb.org/anthology/P15-1017
https://www.aclweb.org/anthology/P15-1017
https://www.aclweb.org/anthology/W09-2209
https://www.aclweb.org/anthology/W09-2209
http://www.lrec-conf.org/proceedings/lrec2004/pdf/5.pdf
http://www.lrec-conf.org/proceedings/lrec2004/pdf/5.pdf
http://www.lrec-conf.org/proceedings/lrec2004/pdf/5.pdf
https://www.aclweb.org/anthology/E14-1049
https://www.aclweb.org/anthology/E14-1049
https://www.aclweb.org/anthology/E14-1049
https://www.aclweb.org/anthology/N15-1151
https://www.aclweb.org/anthology/N15-1151
https://www.aclweb.org/anthology/N15-1151
https://www.aclweb.org/anthology/P16-2011
https://www.aclweb.org/anthology/P16-2011
https://www.aclweb.org/anthology/L18-1245
https://www.aclweb.org/anthology/L18-1245
https://www.aclweb.org/anthology/L18-1245
https://www.aclweb.org/anthology/C16-1114
https://www.aclweb.org/anthology/C16-1114
https://www.aclweb.org/anthology/N19-1383
https://www.aclweb.org/anthology/N19-1383
https://www.aclweb.org/anthology/N19-1383
https://www.aclweb.org/anthology/P08-1030
https://www.aclweb.org/anthology/P08-1030
https://aclweb.org/anthology/D18-1330
https://aclweb.org/anthology/D18-1330
https://aclweb.org/anthology/D18-1330
https://www.aclweb.org/anthology/C10-1064
https://www.aclweb.org/anthology/C10-1064
https://openreview.net/pdf?id=SJU4ayYgl
https://openreview.net/pdf?id=SJU4ayYgl
https://openreview.net/pdf?id=SJU4ayYgl
https://www.aclweb.org/anthology/P13-1117
https://www.aclweb.org/anthology/P13-1117
https://www.aclweb.org/anthology/P15-1027
https://www.aclweb.org/anthology/P15-1027
https://www.aclweb.org/anthology/D17-1018
https://www.aclweb.org/anthology/D17-1018
https://www.aclweb.org/anthology/D12-1092
https://www.aclweb.org/anthology/D12-1092
https://www.aclweb.org/anthology/P14-1038
https://www.aclweb.org/anthology/P14-1038
https://www.aclweb.org/anthology/P13-1008
https://www.aclweb.org/anthology/P13-1008
https://www.aclweb.org/anthology/P13-1008


322

Qi Li, Heng Ji, HONG Yu, and Sujian Li. 2014.
Constructing information networks using one single
model. In Proceedings of the 2014 Conference on
Empirical Methods in Natural Language Process-
ing.

Shasha Liao and Ralph Grishman. 2011. Acquiring
topic features to improve event extraction: in pre-
selected and balanced collections. In Proceedings
of the International Conference Recent Advances in
Natural Language Processing 2011.

Yankai Lin, Zhiyuan Liu, and Maosong Sun. 2017.
Neural relation extraction with multi-lingual atten-
tion. In Proceedings of the 55th Annual Meeting of
the Association for Computational Linguistics (Vol-
ume 1: Long Papers).

Ying Lin, Shengqi Yang, Veselin Stoyanov, and Heng
Ji. 2018. A multi-lingual multi-task architecture
for low-resource sequence labeling. In Proc. The
56th Annual Meeting of the Association for Compu-
tational Linguistics.

Jian Liu, Yubo Chen, Kang Liu, and Jun Zhao. 2018a.
Event detection via gated multilingual attention
mechanism. In Proceedings of the Thirty-Second
AAAI Conference on Artificial Intelligence.

Nelson F. Liu, Matt Gardner, Yonatan Belinkov,
Matthew E. Peters, and Noah A. Smith. 2019. Lin-
guistic knowledge and transferability of contextual
representations. In Proceedings of the Conference
of the North American Chapter of the Association for
Computational Linguistics.

Xiao Liu, Zhunchen Luo, and Heyan Huang. 2018b.
Jointlymultiple events extraction via attention-based
graph information aggregation. In Proceedings of
the 2018 Conference on Empirical Methods in Nat-
ural Language Processing.

Ang Lu, Weiran Wang, Mohit Bansal, Kevin Gimpel,
and Karen Livescu. 2015. Deep multilingual corre-
lation for improved word embeddings. In Proceed-
ings of the 2015 Conference of the North American
Chapter of the Association for Computational Lin-
guistics: Human Language Technologies.

Weiyi Lu and Thien Huu Nguyen. 2018. Similar but
not the same: Word sense disambiguation improves
event detection via neural representation matching.
In Proceedings of the 2018 Conference on Empirical
Methods in Natural Language Processing.

Christopher Manning, Mihai Surdeanu, John Bauer,
Jenny Finkel, Steven Bethard, and David McClosky.
2014. The Stanford CoreNLP natural language pro-
cessing toolkit. InProceedings of 52nd annual meet-
ing of the Association for Computational Linguis-
tics: system demonstrations.

Diego Marcheggiani and Ivan Titov. 2017. Encoding
sentences with graph convolutional networks for se-
mantic role labeling. In Proceedings of the 2017
Conference on Empirical Methods in Natural Lan-
guage Processing.

Stephen Mayhew, Chen-Tse Tsai, and Dan Roth. 2017.
Cheap translation for cross-lingual named entity
recognition. In Proceedings of the 2017 Conference
on EmpiricalMethods in Natural Language Process-
ing.

Tomas Mikolov, Quoc V Le, and Ilya Sutskever. 2013.
Exploiting similarities among languages for machine
translation. arXiv preprint arXiv:1309.4168.

Bonan Min, Ralph Grishman, Li Wan, Chang Wang,
and David Gondek. 2013. Distant supervision for
relation extraction with an incomplete knowledge
base. In Proceedings of the 2013 Conference of the
North American Chapter of the Association for Com-
putational Linguistics: Human Language Technolo-
gies.

Mike Mintz, Steven Bills, Rion Snow, and Dan Juraf-
sky. 2009. Distant supervision for relation extrac-
tion without labeled data. InProceedings of the Joint
Conference of the 47th Annual Meeting of the ACL
and the 4th International Joint Conference on Natu-
ral Language Processing of the AFNLP.

Makoto Miwa and Mohit Bansal. 2016. End-to-end re-
lation extraction using LSTMs on sequences and tree
structures. In Proceedings of the 54th Annual Meet-
ing of the Association for Computational Linguistics
(Volume 1: Long Papers).

Phoebe Mulcaire, Jungo Kasai, and Noah A. Smith.
2019. Polyglot contextual representations improve
crosslingual transfer. In Proceedings of the 2019
Conference of the North American Chapter of the
Association for Computational Linguistics: Human
Language Technologies, Volume 1 (Long and Short
Papers).

Phoebe Mulcaire, Swabha Swayamdipta, and Noah A.
Smith. 2018. Polyglot semantic role labeling. In
Proceedings of the 56th Annual Meeting of the As-
sociation for Computational Linguistics (Volume 2:
Short Papers).

Thien Huu Nguyen, Kyunghyun Cho, and Ralph Grish-
man. 2016. Joint event extraction via recurrent neu-
ral networks. In Proceedings of the 2016 Conference
of the North American Chapter of the Association for
Computational Linguistics: Human Language Tech-
nologies.

Thien Huu Nguyen and Ralph Grishman. 2015a. Event
detection and domain adaptation with convolutional
neural networks. In Proceedings of the 53rd Annual
Meeting of the Association for Computational Lin-
guistics and the 7th International Joint Conference
on Natural Language Processing (Volume 2: Short
Papers).

Thien Huu Nguyen and Ralph Grishman. 2015b. Rela-
tion extraction: Perspective from convolutional neu-
ral networks. In Proceedings of North American
Chapter of the Association for Computational Lin-
guistics: Human Language Technologies.

https://www.aclweb.org/anthology/D14-1198
https://www.aclweb.org/anthology/D14-1198
https://www.aclweb.org/anthology/R11-1002
https://www.aclweb.org/anthology/R11-1002
https://www.aclweb.org/anthology/R11-1002
https://www.aclweb.org/anthology/P17-1004
https://www.aclweb.org/anthology/P17-1004
https://www.aclweb.org/anthology/P18-1074
https://www.aclweb.org/anthology/P18-1074
https://aaai.org/ocs/index.php/AAAI/AAAI18/paper/view/16371/16017
https://aaai.org/ocs/index.php/AAAI/AAAI18/paper/view/16371/16017
https://www.aclweb.org/anthology/N19-1112
https://www.aclweb.org/anthology/N19-1112
https://www.aclweb.org/anthology/N19-1112
https://www.aclweb.org/anthology/D18-1156
https://www.aclweb.org/anthology/D18-1156
https://www.aclweb.org/anthology/N15-1028
https://www.aclweb.org/anthology/N15-1028
https://www.aclweb.org/anthology/D18-1517
https://www.aclweb.org/anthology/D18-1517
https://www.aclweb.org/anthology/D18-1517
https://nlp.stanford.edu/pubs/StanfordCoreNlp2014.pdf
https://nlp.stanford.edu/pubs/StanfordCoreNlp2014.pdf
https://www.aclweb.org/anthology/D17-1159
https://www.aclweb.org/anthology/D17-1159
https://www.aclweb.org/anthology/D17-1159
https://www.aclweb.org/anthology/D17-1269
https://www.aclweb.org/anthology/D17-1269
https://arxiv.org/pdf/1309.4168.pdf
https://arxiv.org/pdf/1309.4168.pdf
https://www.aclweb.org/anthology/N13-1095
https://www.aclweb.org/anthology/N13-1095
https://www.aclweb.org/anthology/N13-1095
https://www.aclweb.org/anthology/P09-1113
https://www.aclweb.org/anthology/P09-1113
https://www.aclweb.org/anthology/P16-1105
https://www.aclweb.org/anthology/P16-1105
https://www.aclweb.org/anthology/P16-1105
https://www.aclweb.org/anthology/N19-1392
https://www.aclweb.org/anthology/N19-1392
https://www.aclweb.org/anthology/P18-2106
https://www.aclweb.org/anthology/N16-1034
https://www.aclweb.org/anthology/N16-1034
https://www.aclweb.org/anthology/P15-2060
https://www.aclweb.org/anthology/P15-2060
https://www.aclweb.org/anthology/P15-2060
https://www.aclweb.org/anthology/W15-1506
https://www.aclweb.org/anthology/W15-1506
https://www.aclweb.org/anthology/W15-1506


323

Thien Huu Nguyen and Ralph Grishman. 2018. Graph
convolutional networks with argument-aware pool-
ing for event detection. In Thirty-Second AAAI Con-
ference on Artificial Intelligence.

Joakim Nivre, Mitchell Abrams, Željko Agić, Lars
Ahrenberg, Lene Antonsen, Katya Aplonova,
Maria Jesus Aranzabe, Gashaw Arutie, Masayuki
Asahara, Luma Ateyah, Mohammed Attia, Aitziber
Atutxa, Liesbeth Augustinus, Elena Badmaeva,
Miguel Ballesteros, Esha Banerjee, Sebastian
Bank, Verginica Barbu Mititelu, Victoria Basmov,
John Bauer, Sandra Bellato, Kepa Bengoetxea,
Yevgeni Berzak, Irshad Ahmad Bhat, Riyaz Ah-
mad Bhat, Erica Biagetti, Eckhard Bick, Rogier
Blokland, Victoria Bobicev, Carl Börstell, Cristina
Bosco, Gosse Bouma, Sam Bowman, Adriane
Boyd, Aljoscha Burchardt, Marie Candito, Bernard
Caron, Gauthier Caron, Gülşen Cebiroğlu Eryiğit,
Flavio Massimiliano Cecchini, Giuseppe G. A.
Celano, Slavomír Čéplö, Savas Cetin, Fabricio
Chalub, Jinho Choi, Yongseok Cho, Jayeol Chun,
Silvie Cinková, Aurélie Collomb, Çağrı Çöltekin,
Miriam Connor, Marine Courtin, Elizabeth David-
son, Marie-Catherine de Marneffe, Valeria de Paiva,
Arantza Diaz de Ilarraza, Carly Dickerson, Peter
Dirix, Kaja Dobrovoljc, Timothy Dozat, Kira
Droganova, Puneet Dwivedi, Marhaba Eli, Ali
Elkahky, Binyam Ephrem, Tomaž Erjavec, Aline
Etienne, Richárd Farkas, Hector Fernandez Al-
calde, Jennifer Foster, Cláudia Freitas, Katarína
Gajdošová, Daniel Galbraith, Marcos Garcia, Moa
Gärdenfors, Sebastian Garza, Kim Gerdes, Filip
Ginter, Iakes Goenaga, Koldo Gojenola, Memduh
Gökırmak, Yoav Goldberg, Xavier Gómez Guino-
vart, Berta Gonzáles Saavedra, Matias Grioni,
Normunds Grūzītis, Bruno Guillaume, Céline
Guillot-Barbance, Nizar Habash, Jan Hajič, Jan
Hajič jr., Linh Hà Mỹ, Na-Rae Han, Kim Harris,
Dag Haug, Barbora Hladká, Jaroslava Hlaváčová,
Florinel Hociung, Petter Hohle, Jena Hwang, Radu
Ion, Elena Irimia, Ọlájídé Ishola, Tomáš Jelínek,
Anders Johannsen, Fredrik Jørgensen, Hüner
Kaşıkara, Sylvain Kahane, Hiroshi Kanayama,
Jenna Kanerva, Boris Katz, Tolga Kayadelen,
Jessica Kenney, Václava Kettnerová, Jesse Kirch-
ner, Kamil Kopacewicz, Natalia Kotsyba, Simon
Krek, Sookyoung Kwak, Veronika Laippala,
Lorenzo Lambertino, Lucia Lam, Tatiana Lando,
Septina Dian Larasati, Alexei Lavrentiev, John
Lee, Phương Lê Hồng, Alessandro Lenci, Saran
Lertpradit, Herman Leung, Cheuk Ying Li, Josie
Li, Keying Li, KyungTae Lim, Nikola Ljubešić,
Olga Loginova, Olga Lyashevskaya, Teresa Lynn,
Vivien Macketanz, Aibek Makazhanov, Michael
Mandl, Christopher Manning, Ruli Manurung,
Cătălina Mărănduc, David Mareček, Katrin Marhei-
necke, Héctor Martínez Alonso, André Martins,
Jan Mašek, Yuji Matsumoto, Ryan McDonald,
Gustavo Mendonça, Niko Miekka, Margarita Misir-
pashayeva, Anna Missilä, Cătălin Mititelu, Yusuke
Miyao, Simonetta Montemagni, Amir More, Laura
Moreno Romero, Keiko Sophie Mori, Shinsuke
Mori, Bjartur Mortensen, Bohdan Moskalevskyi,

Kadri Muischnek, Yugo Murawaki, Kaili Müürisep,
Pinkey Nainwani, Juan Ignacio Navarro Horñiacek,
Anna Nedoluzhko, Gunta Nešpore-Bērzkalne,
Lương Nguyễn Thị, Huyền Nguyễn Thị Minh,
Vitaly Nikolaev, Rattima Nitisaroj, Hanna Nurmi,
Stina Ojala, Adédayọ̀ Olúòkun, Mai Omura, Petya
Osenova, Robert Östling, Lilja Øvrelid, Niko
Partanen, Elena Pascual, Marco Passarotti, Ag-
nieszka Patejuk, Guilherme Paulino-Passos, Siyao
Peng, Cenel-Augusto Perez, Guy Perrier, Slav
Petrov, Jussi Piitulainen, Emily Pitler, Barbara
Plank, Thierry Poibeau, Martin Popel, Lauma
Pretkalniņa, Sophie Prévost, Prokopis Prokopidis,
Adam Przepiórkowski, Tiina Puolakainen, Sampo
Pyysalo, Andriela Rääbis, Alexandre Rademaker,
Loganathan Ramasamy, Taraka Rama, Carlos
Ramisch, Vinit Ravishankar, Livy Real, Siva
Reddy, Georg Rehm, Michael Rießler, Larissa
Rinaldi, Laura Rituma, Luisa Rocha, Mykhailo
Romanenko, Rudolf Rosa, Davide Rovati, Valentin
Ro�ca, Olga Rudina, Jack Rueter, Shoval Sadde,
Benoît Sagot, Shadi Saleh, Tanja Samardžić,
Stephanie Samson, Manuela Sanguinetti, Baiba
Saulīte, Yanin Sawanakunanon, Nathan Schneider,
Sebastian Schuster, Djamé Seddah, Wolfgang
Seeker, Mojgan Seraji, Mo Shen, Atsuko Shimada,
Muh Shohibussirri, Dmitry Sichinava, Natalia
Silveira, Maria Simi, Radu Simionescu, Katalin
Simkó, Mária Šimková, Kiril Simov, Aaron
Smith, Isabela Soares-Bastos, Carolyn Spadine,
Antonio Stella, Milan Straka, Jana Strnadová,
Alane Suhr, Umut Sulubacak, Zsolt Szántó, Dima
Taji, Yuta Takahashi, Takaaki Tanaka, Isabelle
Tellier, Trond Trosterud, Anna Trukhina, Reut
Tsarfaty, Francis Tyers, Sumire Uematsu, Zdeňka
Urešová, Larraitz Uria, Hans Uszkoreit, Sowmya
Vajjala, Daniel van Niekerk, Gertjan van Noord,
Viktor Varga, Eric Villemonte de la Clergerie,
Veronika Vincze, Lars Wallin, Jing Xian Wang,
Jonathan North Washington, Seyi Williams, Mats
Wirén, Tsegay Woldemariam, Tak-sum Wong,
Chunxiao Yan, Marat M. Yavrumyan, Zhuoran Yu,
Zdeněk Žabokrtský, Amir Zeldes, Daniel Zeman,
Manying Zhang, and Hanzhi Zhu. 2018. Universal
dependencies 2.3. LINDAT/CLARIN digital library
at the Institute of Formal and Applied Linguistics
(ÚFAL), Faculty of Mathematics and Physics,
Charles University.

Joakim Nivre, Marie-Catherine De Marneffe, Filip
Ginter, Yoav Goldberg, Jan Hajic, Christopher D
Manning, Ryan T McDonald, Slav Petrov, Sampo
Pyysalo, Natalia Silveira, et al. 2016. Universal de-
pendencies v1: A multilingual treebank collection.
In Proceedings of the Tenth International Confer-
ence on Language Resources and Evaluation.

Xiaoman Pan, Boliang Zhang, Jonathan May, Joel
Nothman, Kevin Knight, and Heng Ji. 2017. Cross-
lingual name tagging and linking for 282 languages.
In Proceedings of the 55th Annual Meeting of the As-
sociation for Computational Linguistics (Volume 1:
Long Papers).

https://aaai.org/ocs/index.php/AAAI/AAAI18/paper/view/16329/16155
https://aaai.org/ocs/index.php/AAAI/AAAI18/paper/view/16329/16155
https://aaai.org/ocs/index.php/AAAI/AAAI18/paper/view/16329/16155
http://hdl.handle.net/11234/1-2895
http://hdl.handle.net/11234/1-2895
https://www.aclweb.org/anthology/L16-1262
https://www.aclweb.org/anthology/L16-1262
https://www.aclweb.org/anthology/P17-1178
https://www.aclweb.org/anthology/P17-1178


324

Sachin Pawar, Pushpak Bhattacharyya, and Girish K.
Palshikar. 2017. End-to-end relation extraction us-
ing neural networks and Markov logic networks. In
Proceedings of the 15th Conference of the European
Chapter of the Association for Computational Lin-
guistics.

Lonneke Van der Plas, Paola Merlo, and James Hen-
derson. 2011. Scaling up automatic cross-lingual se-
mantic role annotation. In Proceedings of the 49th
Annual Meeting of the Association for Computa-
tional Linguistics: Human Language Technologies.

Ondrej Prazák and Miloslav Konopík. 2017. Cross-
lingual SRL based upon universal dependencies. In
Proceedings of the International Conference Recent
Advances in Natural Language Processing, RANLP
2017.

Longhua Qian, Haotian Hui, Ya’nan Hu, Guodong
Zhou, and Qiaoming Zhu. 2014. Bilingual active
learning for relation classification via pseudo par-
allel corpora. In Proceedings of the 52nd Annual
Meeting of the Association for Computational Lin-
guistics (Volume 1: Long Papers).

Pengda Qin, Weiran Xu, and William Yang Wang.
2018. Dsgan: Generative adversarial training for
distant supervision relation extraction. In Proceed-
ings of the 56th Annual Meeting of the Association
for Computational Linguistics (Volume 1: Long Pa-
pers).

Chris Quirk and Hoifung Poon. 2017. Distant super-
vision for relation extraction beyond the sentence
boundary. In Proceedings of the 15th Conference of
the European Chapter of the Association for Compu-
tational Linguistics: Volume 1, Long Papers.

Sascha Rothe, Sebastian Ebert, and Hinrich Schütze.
2016. Ultradense word embeddings by orthogonal
transformation. In Proceedings of the 2016 Confer-
ence of the North American Chapter of the Associ-
ation for Computational Linguistics: Human Lan-
guage Technologies.

Adam Santoro, David Raposo, David G Barrett, Ma-
teusz Malinowski, Razvan Pascanu, Peter Battaglia,
and Timothy Lillicrap. 2017. A simple neural net-
work module for relational reasoning. In Proceed-
ings of Advances in Neural Information Processing
Systems.

Samuel L Smith, David HP Turban, Steven Hamblin,
and Nils Y Hammerla. 2017. Offline bilingual word
vectors, orthogonal transformations and the inverted
softmax. arXiv preprint arXiv:1702.03859.

Milan Straka and Jana Straková. 2017. Tokenizing,
POS tagging, lemmatizing and parsing UD 2.0 with
UDPipe. In Proceedings of the CoNLL 2017 Shared
Task: Multilingual Parsing from Raw Text to Uni-
versal Dependencies.

Mihai Surdeanu, Julie Tibshirani, Ramesh Nallapati,
and Christopher D. Manning. 2012. Multi-instance
multi-label learning for relation extraction. In Pro-
ceedings of the 2012 Joint Conference on Empirical
Methods in Natural Language Processing and Com-
putational Natural Language Learning.

Kai Sheng Tai, Richard Socher, and Christopher D.
Manning. 2015. Improved semantic representations
from tree-structured long short-term memory net-
works. In Proceedings of the 53rd Annual Meeting
of the Association for Computational Linguistics and
the 7th International Joint Conference on Natural
Language Processing (Volume 1: Long Papers).

Christopher Walker, Stephanie Strassel, Julie Medero,
and Kazuaki Maeda. 2006. Ace 2005 multilin-
gual training corpus. Linguistic Data Consortium,
Philadelphia, 57.

Guanying Wang, Wen Zhang, Ruoxu Wang, Yalin
Zhou, Xi Chen, Wei Zhang, Hai Zhu, and Huajun
Chen. 2018a. Label-free distant supervision for rela-
tion extraction via knowledge graph embedding. In
Proceedings of the 2018 Conference on Empirical
Methods in Natural Language Processing.

Xiaozhi Wang, Xu Han, Yankai Lin, Zhiyuan Liu, and
Maosong Sun. 2018b. Adversarial multi-lingual
neural relation extraction. In Proceedings of the
27th International Conference on Computational
Linguistics.

Chao Xing, DongWang, Chao Liu, and Yiye Lin. 2015.
Normalized word embedding and orthogonal trans-
form for bilingual word translation. In Proceedings
of the 2015 Conference of the North American Chap-
ter of the Association for Computational Linguistics:
Human Language Technologies.

Bishan Yang and Tom M. Mitchell. 2016. Joint extrac-
tion of events and entities within a document context.
In Proceedings of the 2016 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics: Human Language Technologies.

Liang Yao, Chengsheng Mao, and Yuan Luo. 2019.
Graph convolutional networks for text classification.
In Proceedings of the Thirty-Third AAAI Conference
on Artificial Intelligence.

Daojian Zeng, Kang Liu, Yubo Chen, and Jun Zhao.
2015. Distant supervision for relation extraction via
piecewise convolutional neural networks. In Pro-
ceedings of the 2015 Conference on Empirical Meth-
ods in Natural Language Processing.

Daojian Zeng, Kang Liu, Siwei Lai, Guangyou Zhou,
and Jun Zhao. 2014. Relation classification via con-
volutional deep neural network. In Proceedings of
the 25th International Conference on Computational
Linguistics.

Jingli Zhang, Wenxuan Zhou, Yu Hong, Jianmin Yao,
and Min Zhang. 2018a. Using entity relation to im-
prove event detection via attention mechanism. In
Proceedings of NLPCC 2018.

https://www.aclweb.org/anthology/E17-1077
https://www.aclweb.org/anthology/E17-1077
https://www.aclweb.org/anthology/P11-2052
https://www.aclweb.org/anthology/P11-2052
https://doi.org/10.26615/978-954-452-049-6_077
https://doi.org/10.26615/978-954-452-049-6_077
https://www.aclweb.org/anthology/P14-1055
https://www.aclweb.org/anthology/P14-1055
https://www.aclweb.org/anthology/P14-1055
https://www.aclweb.org/anthology/P18-1046
https://www.aclweb.org/anthology/P18-1046
https://www.aclweb.org/anthology/E17-1110
https://www.aclweb.org/anthology/E17-1110
https://www.aclweb.org/anthology/E17-1110
https://www.aclweb.org/anthology/N16-1091
https://www.aclweb.org/anthology/N16-1091
https://papers.nips.cc/paper/7082-a-simple-neural-network-module-for-relational-reasoning.pdf
https://papers.nips.cc/paper/7082-a-simple-neural-network-module-for-relational-reasoning.pdf
https://arxiv.org/pdf/1702.03859.pdf
https://arxiv.org/pdf/1702.03859.pdf
https://arxiv.org/pdf/1702.03859.pdf
https://www.aclweb.org/anthology/K17-3009
https://www.aclweb.org/anthology/K17-3009
https://www.aclweb.org/anthology/K17-3009
https://www.aclweb.org/anthology/D12-1042
https://www.aclweb.org/anthology/D12-1042
https://www.aclweb.org/anthology/P15-1150
https://www.aclweb.org/anthology/P15-1150
https://www.aclweb.org/anthology/P15-1150
https://catalog.ldc.upenn.edu/LDC2006T06
https://catalog.ldc.upenn.edu/LDC2006T06
https://www.aclweb.org/anthology/D18-1248
https://www.aclweb.org/anthology/D18-1248
https://www.aclweb.org/anthology/C18-1099
https://www.aclweb.org/anthology/C18-1099
https://www.aclweb.org/anthology/N15-1104
https://www.aclweb.org/anthology/N15-1104
https://www.aclweb.org/anthology/N16-1033
https://www.aclweb.org/anthology/N16-1033
https://www.aclweb.org/anthology/D15-1203
https://www.aclweb.org/anthology/D15-1203
https://aclweb.org/anthology/C14-1220
https://aclweb.org/anthology/C14-1220


325

Tongtao Zhang, Heng Ji, and Avirup Sil. 2019. Joint
entity and event extraction with generative adversar-
ial imitation learning. Data Intelligence Vol 1 (2).

Yuan Zhang, David Gaddy, Regina Barzilay, and
Tommi S Jaakkola. 2016. Ten pairs to tag-
multilingual pos tagging via coarse mapping be-
tween embeddings. In Proceedings of the 2016 Con-
ference of the North American Chapter of the Asso-
ciation for Computational Linguistics: Human Lan-
guage Technologies.

Yuhao Zhang, Peng Qi, and Christopher D. Manning.
2018b. Graph convolution over pruned dependency
trees improves relation extraction. In Proceedings of
the 2018 Conference on Empirical Methods in Nat-
ural Language Processing.

Bowei Zou, Zengzhuang Xu, Yu Hong, and Guodong
Zhou. 2018. Adversarial feature adaptation for
cross-lingual relation classification. In Proceedings
of the 27th International Conference on Computa-
tional Linguistics.

https://www.aclweb.org/anthology/N16-1156
https://www.aclweb.org/anthology/N16-1156
https://www.aclweb.org/anthology/N16-1156
https://aclweb.org/anthology/D18-1244
https://aclweb.org/anthology/D18-1244
https://www.aclweb.org/anthology/C18-1037
https://www.aclweb.org/anthology/C18-1037

