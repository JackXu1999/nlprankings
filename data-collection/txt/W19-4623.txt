



















































ZCU-NLP at MADAR 2019: Recognizing Arabic Dialects


Proceedings of the Fourth Arabic Natural Language Processing Workshop, pages 208–213
Florence, Italy, August 1, 2019. c©2019 Association for Computational Linguistics

208

ZCU-NLP at MADAR 2019: Recognizing Arabic Dialects

Pavel Přibáň Stephen Taylor
pribanp@kiv.zcu.cz stepheneugenetaylor@gmail.com

Department of Computer Science and Engineering,
Faculty of Applied Sciences, University of West Bohemia,

Pilsen, Czech Republic
http://nlp.kiv.zcu.cz

Abstract

In this paper, we present our systems for the
MADAR Shared Task: Arabic Fine-Grained
Dialect Identification. The shared task con-
sists of two subtasks. The goal of Subtask–
1 (S-1) is to detect an Arabic city dialect in
a given text and the goal of Subtask–2 (S-2)
is to predict the country of origin of a Twitter
user by using tweets posted by the user.

In S-1, our proposed systems are based on
language modelling. We use language mod-
els to extract features that are later used as an
input for other machine learning algorithms.
We also experiment with recurrent neural net-
works (RNN), but these experiments showed
that simpler machine learning algorithms are
more successful. Our system achieves 0.658
macro F1-score and our rank is 6th out of
19 teams in S-1 and 7th in S-2 with 0.475
macro F1-score.

1 Introduction

The Madar shared tasks (Bouamor et al., 2019)
are a follow-up to Salameh’s (Salameh et al.,
2018) work with the synthetic corpus of Bouamor
(Bouamor et al., 2014) and Salameh’s work with
tweets based on the corpus. Two corpora are pro-
vided, a six-city corpus of travel sentences ren-
dered into the dialects of five cities and MSA1, and
a 25-city + MSA corpus using a smaller number
of sentences. In the first task, test data is classi-
fied as one of the 25 cities or MSA. For the second
task, the organizers chose training, development
and test tweet-sets for download from Twitter. The
tweets are from 21 Arabic countries, and the goal
is to determine, for each tweet author, the country
of origin.

For S-1 we did not use any external data,
only data provided by the shared task organizers.

1Modern Standard Arabic

The organizers provided training and development
data2 consisting of sentences in different dialects
with a label denoting the corresponding dialect.
The training data contain 41K sentences and de-
velopment data contain 5.2K sentences. Organiz-
ers also provided additional data with Arabic sen-
tences in seven dialects.
S-2 uses a corpus of tweets. Twitter does not

permit the organizers to distribute tweets, only the
user ids and tweet ids. Every participant must ar-
range with Twitter to download the tweets them-
selves, and because tweets are subject to deletion
over time, it is possible that each participant’s ver-
sion of the corpus and test is unique.

2 Related Work

The Arabic dialects have a common written form
and unified literary tradition, so it seems most logi-
cal to distinguish dialects on the basis of acoustics,
and there is a fair amount of work there, including
Hanani et al. (2013, 2015); Ali et al. (2016).

—-
Biadsy et al. (2009) distinguish four Arabic

dialects and MSA based on (audio) phone se-
quences; the phones were obtained by phone rec-
ognizers for English, German, Japanese, Hindi,
Mandarin, Spanish, and three different MSA
phone-recognizer implementations. The dialects
were distinguished by phoneme sequences, and
the results of classifications based on each phone-
recognizer were combined using a logistic regres-
sion classifier. They train on 150 hours per dialect
of telephone recordings. They report 61% accu-
racy on 5-second segments, and 84% accuracy on
120 second segments.

Zaidan and Callison-Burch (2011) describe
building a text corpus, based on reader commen-

2The participants were not allowed to use these data for
any training purposes.



209

tary on newspaper websites, with significant di-
alect content; the goal is to provide a corpus to
improve machine translation for Arabic dialects.
They used Amazon Mechanical Turk to provide
annotation for a portion of the corpus. Zaidan and
Callison-Burch (2014) describe the same work
in greater detail, including dialect classifiers they
built using the Mechanical Turk data for classes
and origin metadata as additional features. They
say these classifiers are ‘approaching human qual-
ity.’

ElFardy and Diab (2013) classify EGY3 and
MSA sentences from the Zaidan and Callison-
Burch (2011) corpus, that is, from text. Not only
is this a binary task, but orthographic hints, in-
cluding repeated long vowels, emojis and multiple
punctuations, give strong clues of the register, and
hence whether MSA is being employed. They do
a number of experiments comparing various pre-
processing schemes and different training sizes,
ranging from 2-28 million tokens. They achieve
80% – 86% accuracy for all of their attempts.

Malmasi et al. (2015) do Arabic dialect iden-
tification from text corpora, including the Multi-
Dialect Parallel Corpus of Arabic (Bouamor
et al., 2014) and the Arabic Online Commentary
database (Zaidan and Callison-Burch, 2011).

Hanani et al. (2015) perform recognition of sev-
eral Palestinian regional accents, evaluating four
different acoustic models, achieving 81.5% accu-
racy for their best system, an I-vector framework
with 64 Gaussian components.

Ali et al. (2016) developed the corpus on which
the DSL Arabic shared task is based. Their
own dialect detection efforts depended largely on
acoustical cues.

Arabic dialect recognition appeared in the 2016
edition of the VarDial workshop’s shared task
(Malmasi et al., 2016). The shared task data was
text-only.

The best classifiers (Malmasi et al., 2016;
Ionescu and Popescu, 2016) for the shared task
performed far below the best results reported by
some of the preceding researchers, in particular
Ali et al. (2016) which used some of the same data.

Part of the reason must be that the amount of
training data for the workshop is much smaller
than that used by some of the other researchers;
the workshop data also did not include the audio
recordings on which the transcripts are based.

3Egyptian dialect

The absence of audio was remedied for the 2017
and 2018 VarDial workshops, (Zampieri et al.,
2017, 2018)

However, the five dialects plus MSA targeted
by the VarDial shared task comprise a small frac-
tion of Arabic’s dialectical variation. Salameh et
al. (Salameh et al., 2018) use a corpus (Bouamor
et al., 2018) which differentiates between twenty-
five different cities and MSA. This still doesn’t ad-
dress urban rural divides, but it begins to reflect
more realistic diversity.

3 Overview

3.1 Language Modelling

In S-1, both of our systems used for the offi-
cial submission take as an input language model
features. In our case the objective of a language
model in its simplest form is to predict probabil-
ity p(S) of sentence S which is composed from
strings (words or character n-grams) s1, s2 . . . sN ,
where N is a number of strings in the sentence.
The probability estimation of p(S) can be com-
puted as a product of conditional probabilities
p(si|hi) of its strings s1, s2 . . . sN , where hi is a
history of a string si. The probability of string
si is conditioned by history hi i.e. n − 1 preced-
ing strings si−n+1, si−n+2, . . . si−1 which can be
rewritten as si−1i−n+1. The resulting formula for the
p(S) estimation looks as follows:

p(S) =
N∏
i=1

p(si|hi) =
N∏
i=1

p(si|si−1i−n+1) (1)

The conditioned probability p(si|hi) can be
estimated with Maximum Likelihood Estimate
(MLE) which is defined as:

pMLE(si|hi) =
c(si−n+1, si−n+2 . . . si)

c(si−n+1, si−n+2 . . . si−1)
(2)

where c(si−n+1, si−n+2 . . . si) is a number of
occurrences of string si with history hi and
c(si−n+1, si−n+2 . . . si−1) is a number of occur-
rences of history hi. These counts are taken from
a training corpus.

We followed Salameh (Salameh et al., 2018)
in using the kenlm language modelling tool
(Heafield et al., 2013). kenlm doesn’t have an
option to use character n-grams instead of words,



210

so in order to get character-based language mod-
els, we prepared input files with characters sep-
arated by spaces. Instead of encoding space
as a special word, we surrounded words with a
<w></w>pair. This enables noticing strings
which occur at the beginning or end of a word
(as would a special sequence for space) but re-
duces the possible amount of inter-word infor-
mation which the language model can keep for
a given order, the parameter which indicates to
kenlm the largest n-gram to index. We used or-
der 5 for all our kenlm language models. We pre-
built models for each dialect. We prepared six di-
rectories, each containing word or character mod-
els for each dialect in one of the three corpora.

We wrote a LangModel class which quacks
like a sklearn classifier, that is, it supports
fit(), predict(), and predict proba(),
but its choices are based on a directory of lan-
guage models. predict() returns the di-
alect name whose model gives the highest score.
predict proba() provides a list of language-
model-score features, adjusted to probabilities.

4 Subtask–1 System Description

In this section we describe our models4. We sub-
mitted results for the S-1 from two systems – Tor-
tuous Classifier and Neural Network Classifier.

4.1 Tortuous Classifier
This submission uses a jumble of features and
classifiers, most from the sklearn module (Buit-
inck et al., 2013). The final classifier is a hard vot-
ing classifier with three input streams:

1. Soft voting classifier on:

(a) Multinomial naive Bayes classifier on
word 1-2grams,

(b) Multinomial naive Bayes classifier on
char 3-5grams,

(c) Language model scores adjusted to
probabilities, for word-based language
models of the corpus 26 dialects

(d) Language model scores adjusted to
probabilities, for char-based language
models of the corpus 26 dialects

(e) Multinomial naive Bayes classifier
on language-model-scores for char-
acter and language models on the

4The source code is available at https://github.
com/StephenETaylor/Madar-2019

corpus-6 language models and character
language models for the corpus-26
language models.

2. Support vector machine, svm.SVC(
gamma=’scale’, kernel =
’poly’, degree = 2) with the
same features as item 1e.

3. Multinomial naive Bayes classifier using
word and char language model features for
corpus-6 and corpus-26 features, tfid vector-
ized word 1-2grams, and tfid vectorized char
3-5grams.

The classifier did better on the development data,
suggesting that it is over-fitted, but the language
model features, which are the most predictive, also
did better on the development data.

4.2 Neural Network Classifier
We experimented with several neural networks.
Our model for the S-1 submission uses as input
26 features which correspond to one of our 26 pre-
trained dialect language models. Each feature rep-
resents the probability of a given sentence for one
language model. The probability scores measure
how close each sentence is to the dialect.

We train Multilayer Perceptron (MLP) with one
hidden (dense) layer with 400 units. The out-
put of the hidden layer is passed to a final fully-
connected softmax layer. The output of the soft-
max layer is a probability distribution over all 26
classes. The class with the highest probability is
predicted as a final output of our model. As an ac-
tivation function in the hidden layer of the MLP a
Rectified Linear Unit (ReLu) is employed.

We also tried to combine character n-gram fea-
tures with the language model features. The input
is a sequence of first 200 character n-grams of a
given text. Each sequence of character n-grams
is used as a separate input followed by a ran-
domly initialized embedding layer and then two
layers of Bidirectional LSTM (BiLSTM)(Graves
and Schmidhuber, 2005) with 64 units are em-
ployed (see Figure 1).

The output vector of the BiLSTM layers is con-
catenated with the language model features and
this concatenated vector is passed to the MLP
layer with 400 units (the same as described above).
All models were implemented by using Keras
(Chollet et al., 2015) with TensorFlow backend
(Abadi et al., 2015)

https://github.com/StephenETaylor/Madar-2019
https://github.com/StephenETaylor/Madar-2019


211

te
xt

 n
-g

ra
m

s BiLSTM BiLSTM

Character Embedings

BiLSTM BiLSTM

200 x 150 1 x (n*128+26)
Language model features

Dense
Layer

Concatenated
Vector

Softmax

Figure 1: Neural network model architecture

4.3 Neural Netwok Model Training
We tune all hyperparameters on the development
data. We train our model with Adam (Kingma and
Ba, 2014) optimizer with learning rate 0.01 and
without any dropout. The number of epochs is 800
and we do not use mini-batches or dropout regu-
larization technique. The model with these hyper-
parameters achieves the best result (0.661 macro
F1-score) on the development data and was used
for the final submission.

We also experimented with the n-gram inputs.
We tried a different number of character n-grams
and we achieve the best result (0.555 macro F1-
score) on the development data using three inputs
- character unigrams, bigrams and trigrams, with
learning rate 0.005, mini-batches of size 256 for
11 epochs and with the Adam optimizer.

5 Subtask–2 System Description

Our tortuous classifier did less well on the tweet
data, so we used a simpler classifier.

The features are the kenlm language model
scores for the 21 countries, computed for each of
the training tweets, then exponentiated and nor-
malized to sum to 1. The tweets are classified us-
ing

y_test = KNeighborsClassifier
(n_neighbors=31)
.fit(X_train,y_train)

.predict(X_test)

The users are predicted based on the plurality pre-
diction for all of their tweets, that is, the country
to which the largest number of their tweets were
assigned.

There were a significant number of tweets un-
available, about 10% in the training and develop-
ment sets, and 12% in the test set. After the sub-
missions had closed we experimented with elimi-
nating the unavailable and non-Arabic tweets from

AL
E

AL
G AL

X
AM

M
AS

W BA
G

BA
S BE

I
BE

N CA
I
DA

M
DO

H FE
S JED JER KH

A
MO

S
MS

A
MU

S
RA

B RIY SA
L

SA
N SF

X TR
I

TU
N

Predicted labels

ALE

ALG

ALX

AMM

ASW

BAG

BAS

BEI

BEN

CAI

DAM

DOH

FES

JED

JER

KHA

MOS

MSA

MUS

RAB

RIY

SAL

SAN

SFX

TRI

TUN

Tr
ue

 la
be

ls

127 0 0 12 3 4 1 7 0 0 30 1 0 0 8 0 1 0 0 0 0 3 1 0 1 1
0 160 1 1 0 1 0 0 8 0 0 1 7 1 1 4 1 4 2 2 2 1 0 1 1 1
0 3 149 4 16 1 0 0 1 10 1 1 1 2 1 8 0 0 0 0 0 0 2 0 0 0
9 0 2 118 2 1 0 6 2 8 9 7 0 2 18 3 0 0 2 0 2 7 1 0 1 0
0 0 21 4 134 1 1 0 0 19 1 4 1 3 4 2 0 0 0 0 3 1 0 0 1 0
2 0 0 5 2 127 25 1 3 1 1 3 0 1 0 4 3 3 1 0 9 2 5 2 0 0
1 1 1 2 1 29 128 1 2 0 1 6 0 1 0 0 8 2 3 0 5 2 3 2 1 0
8 1 0 7 2 1 1 129 2 3 18 0 0 1 9 3 0 1 2 1 3 5 0 0 2 1
2 5 0 6 9 3 1 1 131 2 1 8 0 4 2 0 0 0 2 0 6 1 4 1 11 0
6 4 19 3 37 2 0 0 2 100 2 1 1 5 1 9 0 0 0 0 3 0 1 0 2 2
16 0 1 16 4 4 2 17 2 3 108 4 1 2 10 0 0 0 0 0 0 8 1 0 1 0
1 1 0 4 2 2 3 1 1 1 0 150 1 8 4 5 0 1 3 1 4 2 1 1 2 1
0 7 1 1 2 0 0 0 0 2 1 4 136 3 0 2 0 1 2 31 1 1 1 3 0 1
1 1 1 6 4 0 0 2 5 3 3 14 1 123 1 6 0 1 3 0 10 5 5 1 3 1
7 2 1 26 2 1 2 7 5 1 1 6 0 2 121 2 0 0 1 0 2 9 1 0 1 0
0 2 3 8 10 1 1 1 0 1 1 3 0 4 2 147 1 12 0 0 0 0 1 0 2 0
1 0 0 0 0 3 8 0 1 0 0 0 0 1 0 0 167 1 1 0 6 4 5 0 1 1
1 7 2 2 0 1 0 0 2 1 1 0 0 0 1 9 1 158 4 0 7 1 1 0 1 0
2 3 1 3 1 3 1 1 5 1 3 15 1 8 1 10 0 32 83 1 16 4 4 0 1 0
1 7 0 3 0 0 0 0 2 0 1 1 36 1 2 1 0 0 0 139 0 1 0 3 1 1
2 0 0 3 0 2 2 3 6 5 0 13 1 18 1 1 2 9 7 0 115 2 7 0 1 0
3 3 1 12 3 2 2 5 3 2 7 10 0 5 12 2 1 3 2 0 7 109 4 1 1 0
1 1 1 2 1 6 2 1 3 1 1 7 0 9 1 3 4 3 1 0 6 4 140 0 2 0
2 6 0 1 1 2 3 0 3 1 2 0 2 0 2 0 0 0 2 1 3 1 0 146 0 22
1 3 0 2 3 0 0 2 9 2 0 4 0 2 2 4 0 0 1 1 4 1 1 4 151 3
1 9 0 1 0 3 2 1 2 2 2 2 1 0 1 0 0 0 1 2 1 2 1 38 0 128

0

30

60

90

120

150

Figure 2: Tortuous Classifier confusion matrix

training and testing and choosing Saudi Arabia
(which is the origin for the plurality of tweets at
36%) for users with no remaining tweets. This im-
proved tweet classification accuracy by about 5%,
but actually decreased user classification accuracy
on the development set.

6 Results

For the Subtask–1 we achieved 0.658 macro F1-
score on the test data, sixth among nineteen sub-
missions with the Tortuous Classifier. The Neu-
ral Network Classifier achieved a macro F1-score
of 0.648 on the test data. For the Subtask–2 we
submitted a single entry. It ranked 7th among 9
submissions with 0.475 macro F1-score.

Figure 2 shows that many of the errors are
geographically plausible. For example, ASWan
ALXandria and CAIro are all in Egypt, and each
has a sizeable chunk of mistaken identity for the
others. Similarly, DAMascus, ALEppo, AMMan,
BEIrut, JERusalem which are all ’Levantine’ and
only a few hundred miles apart.

7 Conclusion

This paper presents an automatic approach for
Arabic dialect detection in the MADAR Shared
Task. Our proposed systems for the Subtask-1
use language model features. Our experiments
showed that simpler machine learning algorithms
outperform RNN using language model features.
Subtask–2 turned out to be more challenging be-
cause Tweets, which are real-world wild data, are
more difficult to process than systematically pre-
pared texts.



212

Acknowledgments

This work has been partly supported by Grant No.
SGS-2019-018 Processing of heterogeneous data
and its specialized applications, and was partly
supported from ERDF ”Research and Develop-
ment of Intelligent Components of Advanced
Technologies for the Pilsen Metropolitan Area
(InteCom)”. Access to computing and storage fa-
cilities owned by parties and projects contributing
to the National Grid Infrastructure MetaCentrum
provided under the programme ”Projects of Large
Research, Development, and Innovations Infras-
tructures” (CESNET LM2015042), is greatly
appreciated.

References
Martı́n Abadi, Ashish Agarwal, Paul Barham, Eugene

Brevdo, Zhifeng Chen, Craig Citro, Greg S. Cor-
rado, Andy Davis, Jeffrey Dean, Matthieu Devin,
Sanjay Ghemawat, Ian Goodfellow, Andrew Harp,
Geoffrey Irving, Michael Isard, Yangqing Jia, Rafal
Jozefowicz, Lukasz Kaiser, Manjunath Kudlur, Josh
Levenberg, Dandelion Mané, Rajat Monga, Sherry
Moore, Derek Murray, Chris Olah, Mike Schus-
ter, Jonathon Shlens, Benoit Steiner, Ilya Sutskever,
Kunal Talwar, Paul Tucker, Vincent Vanhoucke,
Vijay Vasudevan, Fernanda Viégas, Oriol Vinyals,
Pete Warden, Martin Wattenberg, Martin Wicke,
Yuan Yu, and Xiaoqiang Zheng. 2015. TensorFlow:
Large-scale machine learning on heterogeneous sys-
tems. Software available from tensorflow.org.

Ahmed Ali, Najim Dehak, Patrick Cardinal, Sameer
Khurana, Sree Harsha Yella, James Glass, Peter
Bell, and Steve Renals. 2016. Automatic dialect de-
tection in Arabic broadcast speech. In Proceedings
of Interspeech 2016, pages 2934–2938.

Fadi Biadsy, Julia Hirschberg, and Nizar Habash. 2009.
Spoken arabic dialect identification using phonotac-
tic modeling. In Proceedings of the EACL Work-
shop on Computational Approaches to Semitic Lan-
guages, pages 53–61.

Houda Bouamor, Nizar Habash, and Kemal Oflazer.
2014. A multidialectal parallel corpus of Ara-
bic. In Proceedings of the Ninth International
Conference on Language Resources and Evaluation
(LREC14). European Language Resources Associa-
tion (ELRA).

Houda Bouamor, Nizar Habash, Mohammad Salameh,
Wajdi Zaghouani, Owen Rambow, Dana Abdul-
rahim, Ossama Obeid, Salam Khalifa, Fadhl Eryani,
Alexander Erdmann, and Kemal Oflazer. 2018. The
madar arabic dialect corpus and lexicon. In Pro-
ceedings of the 11th International Conference on
Language Resources and Evaluation.

Houda Bouamor, Sabit Hassan, and Nizar Habash.
2019. The MADAR Shared Task on Arabic Fine-
Grained Dialect Identification. In Proceedings of the
Fourth Arabic Natural Language Processing Work-
shop (WANLP19), Florence, Italy.

Lars Buitinck, Gilles Louppe, Mathieu Blondel, Fabian
Pedregosa, Andreas Mueller, Olivier Grisel, Vlad
Niculae, Peter Prettenhofer, Alexandre Gramfort,
Jaques Grobler, Robert Layton, Jake VanderPlas,
Arnaud Joly, Brian Holt, and Gaël Varoquaux. 2013.
API design for machine learning software: experi-
ences from the scikit-learn project. In ECML PKDD
Workshop: Languages for Data Mining and Ma-
chine Learning, pages 108–122.

François Chollet et al. 2015. Keras. https://
keras.io.

Heba ElFardy and Mona Diab. 2013. Sentence level di-
alect identification in Arabic. In Proceedings of the
51st Annual Meeting of the Association for Compu-
tational Linguistics (ACL), pages 456–461.

Alex Graves and Jürgen Schmidhuber. 2005. Frame-
wise phoneme classification with bidirectional lstm
and other neural network architectures. Neural Net-
works, 18(5-6):602–610.

Abualsoud Hanani, Hanna Basha, Yasmeen Sharaf, and
Stephen Taylor. 2015. Palestinian Arabic regional
accent recognition. In The 8th International Confer-
ence on Speech Technology and Human-Computer
Dialogue.

Abualsoud Hanani, Martin J. Russell, and Michael J.
Carey. 2013. Human and computer recognition
of regional accents and ethnic groups from British
english speech. Computer Speech and Language,
27(1):5974.

Kenneth Heafield, Ivan Pouzyrevsky, Jonathan H.
Clark, , and Philipp Koehn. 2013. Scalable modi-
fied kneser-ney language model estimation. In ACL.

Radu Tudor Ionescu and Marius Popescu. 2016.
UnibucKernel: An Approach for Arabic Dialect
Identification Based on Multiple String Kernels. In
Proceedings of the Third Workshop on NLP for Sim-
ilar Languages, Varieties and Dialects (VarDial3),
pages 135–144, Osaka, Japan.

Diederik P. Kingma and Jimmy Lei Ba. 2014.
Adam: A method for stochastic optimization.
arXiv:1412.6980v9.

Shervin Malmasi, Eshrag Refaee, and Mark Dras.
2015. Arabic dialect identification using a parallel
multidialectal corpus. In Proceedings of the 14th
Conference of the Pacific Association for Computa-
tional Linguistics (PACLING 2015), pages 209–217,
Bali, Indonesia.

Shervin Malmasi, Marcos Zampieri, Nikola Ljubei,
Preslav Nakov, Ahmed Ali, and Jrg Tiedemann.
2016. Discriminating between similar languages

https://www.tensorflow.org/
https://www.tensorflow.org/
https://www.tensorflow.org/
https://doi.org/10.21437/Interspeech.2016-1297
https://doi.org/10.21437/Interspeech.2016-1297
https://pdfs.semanticscholar.org/11ed/2bdeb34684a90442dadf53934e2ba05a6268.pdf
https://pdfs.semanticscholar.org/11ed/2bdeb34684a90442dadf53934e2ba05a6268.pdf
http://www.lrec-conf.org/proceedings/lrec2014/pdf/523_Paper.pdf
http://www.lrec-conf.org/proceedings/lrec2014/pdf/523_Paper.pdf
http://www.lrec-conf.org/proceedings/lrec2018/pdf/351.pdf
http://www.lrec-conf.org/proceedings/lrec2018/pdf/351.pdf
https://keras.io
https://keras.io
http://www.aclweb.org/anthology/P13-2081
http://www.aclweb.org/anthology/P13-2081
https://www.researchgate.net/profile/Abualsoud_Hanani2/publication/286418098_Palestinian_Arabic_regional_accent_recognition/links/5814288a08aedc7d8961f61d.pdf
https://www.researchgate.net/profile/Abualsoud_Hanani2/publication/286418098_Palestinian_Arabic_regional_accent_recognition/links/5814288a08aedc7d8961f61d.pdf
https://kheafield.com/papers/edinburgh/estimate_paper.pdf
https://kheafield.com/papers/edinburgh/estimate_paper.pdf
http://web.science.mq.edu.au/~smalmasi/vardial3/pdf/VarDial318.pdf
http://web.science.mq.edu.au/~smalmasi/vardial3/pdf/VarDial318.pdf
http://arXiv.org/abs/1412.6980
https://www.researchgate.net/publication/279201395_Arabic_Dialect_Identification_using_a_Parallel_Multidialectal_Corpus
https://www.researchgate.net/publication/279201395_Arabic_Dialect_Identification_using_a_Parallel_Multidialectal_Corpus
https://www.aclweb.org/anthology/W16-4801


213

and arabic dialect identification: A report on the
third dsl shared task. In Proceedings of the Third
Workshop on NLP for Similar Languages, Varieties
and Dialects (VarDial 2016).

Mohammad Salameh, Houda Bouamor, and Nizar
Habash. 2018. Fine-grained arabic dialect identi-
fication. In Proceedings of the International Con-
ference on Computational Linguistics (COLING),
pages 1332–1344, Santa Fe, New Mexico, USA.

Omar F. Zaidan and Chris Callison-Burch. 2011. The
Arabic Online Commentary dataset: An annotated
dataset of informal Arabic with high dialectal con-
tent. In Proceedings of ACL, pages 37–41.

Omar F. Zaidan and Chris Callison-Burch. 2014. Ara-
bic dialect identification. Computational Linguis-
tics, 40(1):171–202.

Marcos Zampieri, Shervin Malmasi, Nikola Ljubei,
Preslav Nakov, Ahmed Ali, Jrg Tiedemann, Yves
Scherrer, and Nomi Aepli. 2017. Findings of the
vardial evaluation campaign 2017. In Proceedings
of the Fourth Workshop on NLP for Similar Lan-
guages, Varieties and Dialects (VarDial 2017.

Marcos Zampieri, Shervin Malmasi, Preslav Nakov,
Ahmed Ali, Suwon Shon, James Glass, Yves Scher-
rer, Tanja Samardžić, Nikola Ljubešić, Jörg Tiede-
mann, Chris van der Lee, Stefan Grondelaers,
Nelleke Oostdijk, Dirk Speelman, Antal van den
Bosch, Ritesh Kumar, Bornini Lahiri, and Mayank
Jain. 2018. Language identification and morphosyn-
tactic tagging: The second VarDial evaluation cam-
paign. In Proceedings of the Fifth Workshop on
NLP for Similar Languages, Varieties and Dialects
(VarDial 2018), pages 1–17, Santa Fe, New Mexico,
USA. Association for Computational Linguistics.

https://www.aclweb.org/anthology/W16-4801
https://www.aclweb.org/anthology/W16-4801
http://www.aclweb.org/anthology/P11-2007
http://www.aclweb.org/anthology/P11-2007
http://www.aclweb.org/anthology/P11-2007
http://www.aclweb.org/anthology/P11-2007
http://www.mitpressjournals.org/doi/pdf/10.1162/COLI_a_00169
http://www.mitpressjournals.org/doi/pdf/10.1162/COLI_a_00169
https://pdfs.semanticscholar.org/08f0/11dd54e17d60aeede1ab0ea4e4c283e7bfe1.pdf
https://pdfs.semanticscholar.org/08f0/11dd54e17d60aeede1ab0ea4e4c283e7bfe1.pdf
https://www.aclweb.org/anthology/W18-3901
https://www.aclweb.org/anthology/W18-3901
https://www.aclweb.org/anthology/W18-3901

