



















































AFET: Automatic Fine-Grained Entity Typing by Hierarchical Partial-Label Embedding


Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 1369–1378,
Austin, Texas, November 1-5, 2016. c©2016 Association for Computational Linguistics

AFET: Automatic Fine-Grained Entity Typing by
Hierarchical Partial-Label Embedding

Xiang Ren†∗ Wenqi He†∗ Meng Qu† Lifu Huang] Heng Ji] Jiawei Han†
† University of Illinois at Urbana-Champaign, Urbana, IL, USA

] Computer Science Department, Rensselaer Polytechnic Institute, USA
†{xren7, wenqihe3, mengqu2, hanj}@illinois.edu ]{huangl7, jih}@rpi.edu

Abstract

Distant supervision has been widely used in
current systems of fine-grained entity typ-
ing to automatically assign categories (en-
tity types) to entity mentions. However, the
types so obtained from knowledge bases are
often incorrect for the entity mention’s local
context. This paper proposes a novel em-
bedding method to separately model “clean”
and “noisy” mentions, and incorporates the
given type hierarchy to induce loss functions.
We formulate a joint optimization problem
to learn embeddings for mentions and type-
paths, and develop an iterative algorithm to
solve the problem. Experiments on three pub-
lic datasets demonstrate the effectiveness and
robustness of the proposed method, with an
average 15% improvement in accuracy over
the next best compared method1.

1 Introduction

Assigning types (e.g., person, organization)
to mentions of entities in context is an important
task in natural language processing (NLP). The ex-
tracted entity type information can serve as primi-
tives for relation extraction (Mintz et al., 2009) and
event extraction (Ji and Grishman, 2008), and as-
sists a wide range of downstream applications in-
cluding knowledge base (KB) completion (Dong et
al., 2014), question answering (Lin et al., 2012) and
entity recommendation (Yu et al., 2014). While

∗Equal contribution.
1Codes and datasets used in this paper can be down-

loaded at https://github.com/shanzhenren/AFET.

Mention: “Schwarzenegger”; Context: S3;
Candidate Type Set: {person, politician, artist, 

actor, author, businessman, althete}

ID Sentence 

S1

S2

S3

...

  Governor Arnold Schwarzenegger gives a speech at
  Mission Serve's serv ice project on Veterans Day 2010.

  The fourth movie in the Predator series entitled 'The 
  Predator' may see the return of action-movie star Arnold
  Schwarzenegger to the franchise.

  Schwarzenegger’s first property investment was a block
  of six units, for which he scraped together $US27,000.

...

Entity: Arnold Schwarzenegger

Knowledge Bases

Noisy Training Examples

Candidate Type 
Set (Sub-tree)

root

product person location
organiz
ation

...

...

politician artist business
man

...

... ...

author actor singer ...

Target Type 
Hierarchy

Mention: “Arnold Schwarzenegger”; Context: S1;
Candidate Type Set: {person, politician, artist, 

actor, author, businessman, althete}

...

Mention: “Arnold Schwarzenegger”; Context: S2;
Candidate Type Set: {person, politician, artist, 

actor, author, businessman, althete}

S1

Distant 
Supervision

althete

S2

S3

Figure 1: Current systems may detect Arnold
Schwarzenegger in sentences S1-S3 and assign the same
types to all (listed within braces), when only some types
are correct for context (blue labels within braces).

traditional named entity recognition systems (Rati-
nov and Roth, 2009; Nadeau and Sekine, 2007) fo-
cus on a small set of coarse types (typically fewer
than 10), recent studies (Ling and Weld, 2012;
Yosef et al., 2012) work on a much larger set of
fine-grained types (usually over 100) which form
a tree-structured hierarchy (see the blue region of
Fig. 1). Fine-grained typing allows one mention
to have multiple types, which together constitute a
type-path (not necessarily ending in a leaf node)
in the given type hierarchy, depending on the lo-
cal context (e.g., sentence). Consider the example in
Fig. 1, “Arnold Schwarzenegger” could be labeled as
{person, businessman} in S3 (investment). But
he could also be labeled as {person, politician}
in S1 or {person, artist, actor} in S2. Such
fine-grained type representation provides more in-
formative features for other NLP tasks. For exam-

1369



ple, since relation and event extraction pipelines rely
on entity recognizer to identify possible arguments
in a sentence, fine-grained argument types help dis-
tinguish hundreds or thousands of different relations
and events (Ling and Weld, 2012).

Traditional named entity recognition systems
adopt manually annotated corpora as training
data (Nadeau and Sekine, 2007). But the process
of manually labeling a training set with large num-
bers of fine-grained types is too expensive and error-
prone (hard for annotators to distinguish over 100
types consistently). Current fine-grained typing sys-
tems annotate training corpora automatically using
knowledge bases (i.e., distant supervision) (Ling
and Weld, 2012; Ren et al., 2016a). A typical work-
flow of distant supervision is as follows (see Fig. 1):
(1) identify entity mentions in the documents; (2)
link mentions to entities in KB; and (3) assign, to
the candidate type set of each mention, all KB types
of its KB-linked entity. However, existing distant
supervision methods encounter the following limi-
tations when doing automatic fine-grained typing.
• Noisy Training Labels. Current practice of dis-
tant supervision may introduce label noise to train-
ing data since it fails to take a mention’s local con-
texts into account when assigning type labels (e.g.,
see Fig. 1). Many previous studies ignore the la-
bel noises which appear in a majority of train-
ing mentions (see Table. 1, row (1)), and assume
all types obtained by distant supervision are “cor-
rect” (Yogatama et al., 2015; Ling and Weld, 2012).
The noisy labels may mislead the trained models
and cause negative effect. A few systems try to
denoise the training corpora using simple pruning
heuristics such as deleting mentions with conflicting
types (Gillick et al., 2014). However, such strate-
gies significantly reduce the size of training set (Ta-
ble 1, rows (2a-c)) and lead to performance degrada-
tion (later shown in our experiments). The larger the
target type set, the more severe the loss.
• Type Correlation. Most existing methods (Yo-
gatama et al., 2015; Ling and Weld, 2012) treat ev-
ery type label in a training mention’s candidate type
set equally and independently when learning the
classifiers but ignore the fact that types in the given
hierarchy are semantically correlated (e.g., actor
is more relevant to singer than to politician).
As a consequence, the learned classifiers may bias

Dataset Wiki OntoNotes BBN NYT
# of target types 113 89 47 446
(1) noisy mentions (%) 27.99 25.94 22.32 51.81
(2a) sibling pruning (%) 23.92 16.09 22.32 39.26
(2b) min. pruning (%) 28.22 8.09 3.27 32.75
(2c) all pruning (%) 45.99 23.45 25.33 61.12

Table 1: A study of label noise. (1): %mentions with
multiple sibling types (e.g., actor, singer); (2a)-(2c):
%mentions deleted by the three pruning heuristics (2014)
(see Sec. 4), for three experiment datasets and New York
Times annotation corpus (2014).

toward popular types but perform poorly on infre-
quent types since training data on infrequent types is
scarce. Intuitively, one should pose smaller penalty
on types which are semantically more relevant to the
true types. For example, in Fig. 1 singer should
receive a smaller penalty than politician does,
by knowing that actor is a true type for “Arnold
Schwarzenegger” in S2. This provides classifiers
with additional information to distinguish between
two types, especially those infrequent ones.

In this paper, we approach the problem of auto-
matic fine-grained entity typing as follows: (1) Use
different objectives to model training mentions with
correct type labels and mentions with noisy labels,
respectively. (2) Design a novel partial-label loss to
model true types within the noisy candidate type set
which requires only the “best” candidate type to be
relevant to the training mention, and progressively
estimate the best type by leveraging various text fea-
tures extracted for the mention. (3) Derive type cor-
relation based on two signals: (i) the given type hier-
archy, and (ii) the shared entities between two types
in KB, and incorporate the correlation so induced by
enforcing adaptive margins between different types
for mentions in the training set. To integrate these
ideas, we develop a novel embedding-based frame-
work called AFET. First, it uses distant supervision
to obtain candidate types for each mention, and ex-
tract a variety of text features from the mentions
themselves and their local contexts. Mentions are
partitioned into a “clean” set and a “noisy” set based
on the given type hierarchy. Second, we embed
mentions and types jointly into a low-dimensional
space, where, in that space, objects (i.e., features
and types) that are semantically close to each other
also have similar representations. In the proposed
objective, an adaptive margin-based rank loss is pro-

1370



posed to model the set of clean mentions to capture
type correlation, and a partial-label rank loss is for-
mulated to model the “best” candidate type for each
noisy mention. Finally, with the learned embeddings
(i.e., mapping matrices), one can predict the type-
path for each mention in the test set in a top-down
manner, using its text features. The major contribu-
tions of this paper are as follows:

1. We propose an automatic fine-grained entity typ-
ing framework, which reduces label noise in-
troduced by distant supervision and incorporates
type correlation in a principle way.

2. A novel optimization problem is formulated to
jointly embed entity mentions and types to the
same space. It models noisy type set with a
partial-label rank loss and type correlation with
adaptive-margin rank loss.

3. We develop an iterative algorithm for solving the
joint optimization problem efficiently.

4. Experiments with three public datasets demon-
strate that AFET achieves significant improve-
ment over the state of the art.

2 Automatic Fine-Grained Entity Typing

Our task is to automatically uncover the type infor-
mation for entity mentions (i.e., token spans repre-
senting entities) in natural language sentences. The
task takes a document collection D (automatically
labeled using a KB Ψ in conjunction with a target
type hierarchy Y) as input and predicts a type-path
in Y for each mention from the test set Dt.
Type Hierarchy and Knowledge Base. Two key
factors in distant supervision are the target type hi-
erarchy and the KB. A type hierarchy, Y , is a tree
where nodes represent types of interests from Ψ.
Previous studies manually create several clean type
hierarchies using types from Freebase (Ling and
Weld, 2012) or WordNet (Yosef et al., 2012). In this
study, we adopt the existing hierarchies constructed
using Freebase types2. To obtain types for entities
EΨ in Ψ, we use the human-curated entity-type facts
in Freebase, denoted as FΨ =

{
(e, y)

}
⊂ EΨ × Y .

2We use the Freebase dump as of 2015-06-30.

Clean Training 
Mentions

Noisy Training 
Mentions

Mention: “S1_Arnold Schwarzenegger”; Context: S1;
Candidate Type Set : {person, politician, artist, actor,

author, businessman, althete}
Text Features: {HEAD_Arnold, CXT1_B:Governor, CXT1_A:gives,

POS:NN, TKN_arnold, TKN_schwarzenegger, SHAPE_Aa, ...}

Training mentions with extracted features

lc(mi)

Hierarchical Partial-label Embedding

Type Inference

root

product person location organiz
ation

...

...

politician artist
business

man
...

... ...

author actor singer
...

actor

politician

S1_Arnold 
Schwarzenegger

person

Joint 
Embedding 

Space

CXT1_B:
Governor

HEAD_arnoldMention: “S2_Arnold Schwarzenegger”; Context: S2;
Candidate Type Set : {person, politician, artist, actor,

author, businessman, althete}
Text Features: {HEAD_Arnold, CXT1_B:star, CXT2_B:action

-movie star, CXT3_A:to the franchise, POS:NN, SHAPE_Aa, ...}

...

Mention: “Ted Cruz”; Context: Sn;
Candidate Type Set : {person, politician}

Text Features: {HEAD_Ted, CXT1_B:senator, CXT1_B:told, 
CXT3_B:campaign of senator, POS:NN, SHAPE_Aa, ...}

CXT1_B:star

artistCXT3_A:play
 the role

SHAPE:Aa

CXT1_A:gives

CXT1_B:
Senator

Partition 
training 

mentions

Figure 2: Framework Overview of AFET.

Automatically Labeled Training Corpora. There
exist publicly available labeled corpora such as Wik-
ilinks (Singh et al., 2012) and ClueWeb (Gabrilovich
et al., 2013). In these corpora, entity mentions are
identified and mapped to KB entities using anchor
links. In specific domains (e.g., product reviews)
where such public corpora are unavailable, one can
utilize distant supervision to automatically label the
corpus (Ling and Weld, 2012). Specifically, an en-
tity linker will detect mentions mi and map them
to one or more entity ei in EΨ. Types of ei in KB
are then associated with mi to form its type set Yi,
i.e., Yi =

{
y | (ei, y) ∈ FΨ, y ∈ Y

}
. Formally, a

training corpus D consists of a set of extracted entity
mentionsM = {mi}Ni=1, the context (e.g., sentence,
paragraph) of each mention {ci}Ni=1, and the candi-
date type sets {Yi}Ni=1 for each mention. We repre-
sent D using a set of triples D =

{
(mi, ci,Yi)

}N
i=1

.

Problem Description. For each test mention, we
aim to predict the correct type-path in Y based on
the mention’s context. More specifically, the test set
T is defined as a set of mention-context pairs (m, c),
where mentions in T (denoted asMt) are extracted
from their sentences using existing extractors such
as named entity recognizer (Finkel et al., 2005). We
denote the gold type-path for a test mention m as
Y∗. This work focuses on learning a typing model
from the noisy training corpus D, and estimating Y∗
from Y for each test mention m (in set Mt), based
on mention m, its context c, and the learned model.

Framework Overview. At a high level, the AFET
framework (see also Fig. 2) learns low-dimensional
representations for entity types and text features, and

1371



infers type-paths for test mentions using the learned
embeddings. It consists of the following steps:

1. Extract text features for entity mentions in train-
ing set M and test set Mt using their surface
names as well as the contexts. (Sec. 3.1).

2. Partition training mentions M into a clean set
(denoted asMc) and a noisy set (denoted asMn)
based on their candidate type sets (Sec. 3.2).

3. Perform joint embedding of entity mentions
M and type hierarchy Y into the same low-
dimensional space where, in that space, close ob-
jects also share similar types (Secs. 3.3-3.6).

4. For each test mention m, estimate its type-path
Y∗ (on the hierarchy Y) in a top-down manner
using the learned embeddings (Sec. 3.6).

3 The AFET Framework

This section introduces the proposed framework and
formulates an optimization problem for learning em-
beddings of text features and entity types jointly.

3.1 Text Feature Generation

We start with a representation of entity mentions.
To capture the shallow syntax and distributional se-
mantics of a mention mi ∈ M, we extract various
features from both mi itself and its context ci. Ta-
ble 2 lists the set of text features used in this work,
which is similar to those used in (Yogatama et al.,
2015; Ling and Weld, 2012). We denote the set of
M unique features extracted from D as F = {fj}Mj=1.

3.2 Training Set Partition

A training mention mi (in setM) is considered as a
“clean” mention if its candidate type set obtained by
distant supervision (i.e., Yi) is not ambiguous, i.e.,
candidate types in Yi can form a single path in tree
Y . Otherwise, a mention is considered as “noisy”
mention if its candidate types form multiple type-
paths in Y . Following the above hypothesis, we
judge each mention mi (in set M) and place it in
either the “clean” set Mc, or the “noisy” set Mn.
Finally, we haveM =Mc ∪Mn.

3.3 The Joint Mention-Type Model

We propose to learn mappings into low-dimensional
vector space, where, both entity mentions and type

...

Example Type-Type 
Correlation Scores

Knowledge Base

(Ben Affleck, actor)
(Ben Affleck, director)
(Woody Al len, actor)

(Woody Al len, director)
(J. K. Rowling, author)
(Kobe B ryant, athlete)

...

Entity-type facts
Ben Affleck

Woody Allen

J. K. Rowling

Kobe Bryant

person

director

actor

author

athlete

Corr = 
(0.6+0.6)/2

=0.6

Corr = 
(0.25+0.55)/2

=0.4

person

politician

artist

actor

businessman

author

singer
director

athlete

coach

Adaptive Margin 

Sn_Ted Cruz

Context in Sn: “The effective 
end of Ted Cruz 's presidential 

campaign came on a call …”

politician

athlete

businessman

Score

Sn_Ted CruzScore

Score Sn_Ted Cruz

Margin = 1 / 
sim(politician, 

athlete) = 3

Margin = 1 / sim(politician, 
businessman) = 1.5

Figure 3: An illustration of KB-based type correlation
computation, and the proposed adaptive margin.

labels (in the training set) are represented, and in that
space, two objects are embedded close to each other
if and only if they share similar types. In doing so,
we later can derive the representation of a test men-
tion based on its text features and the learned map-
pings. Mapping functions for entity mentions and
entity type labels are different as they have differ-
ent representations in the raw feature space, but are
jointly learned by optimizing a global objective of
interests to handle the aforementioned challenges.

Each entity mention mi ∈ M can be represented
by aM -dimensional feature vector mi ∈ RM , where
mi,j is the number of occurrences of feature fj (in set
F) formi. Each type label yk ∈ Y is represented by a
K-dimensional binary indicator vector yk ∈ {0, 1}K ,
where yk,k = 1, and 0 otherwise.

Specifically, we aim to learn a mapping func-
tion from the mention’s feature space to a low-
dimensional vector space, i.e., ΦM(mi) : RM 7→ Rd
and a mapping function from type label space to the
same low-dimensional space, i.e., ΦY(yk) : RK 7→
Rd. In this work, we adopt linear maps, as similar to
the mapping functions used in (Weston et al., 2011).

ΦM(mi) = Umi; ΦY(yk) = Vyk, (1)

where U ∈ Rd×M and V ∈ Rd×K are the projection
matrices for mentions and type labels, respectively.

3.4 Modeling Type Correlation
In type hierarchy (tree) Y , types closer to each other
(i.e., shorter path) tend to be more related (e.g.,
actor is more related to artist than to person
in the right column of Fig. 2). In KB Ψ, types as-
signed to similar sets of entities should be more re-
lated to each other than those assigned to quite dif-
ferent entities (Jiang et al., 2015) (e.g., actor is

1372



Feature Description Example
Head Syntactic head token of the mention “HEAD Turing”
Token Tokens in the mention “Turing”, “Machine”
POS Part-of-Speech tag of tokens in the mention “NN”
Character All character trigrams in the head of the mention “:tu”, “tur”, ..., “ng:”
Word Shape Word shape of the tokens in the mention “Aa” for “Turing”
Length Number of tokens in the mention “2”
Context Unigrams/bigrams before and after the mention “CXT B:Maserati ,”, “CXT A:and the”
Brown Cluster Brown cluster ID for the head token (learned using D) “4 1100”, “8 1101111”, “12 111011111111”
Dependency Stanford syntactic dependency (Manning et al., 2014) associatedwith the head token “GOV:nn”, “GOV:turing”

Table 2: Text features used in this paper. “Turing Machine” is used as an example mention from “The band’s former drummer Jerry Fuchs—who
was also a member of Maserati, Turing Machine and The Juan MacLean—died after falling down an elevator shaft.”.

more related to director than to author in the
left column of Fig. 3). Thus, type correlation be-
tween yk and yk′ (denoted as wkk′) can be measured
either using the one over the length of shortest path
in Y , or using the normalized number of shared en-
tities in KB, which is defined as follows.

wkk′ =
(∣∣Ek ∩ Ek′

∣∣/
∣∣Ek

∣∣+
∣∣Ek ∩ Ek′

∣∣/
∣∣Ek′

∣∣
)
/2. (2)

Although a shortest path is efficient to compute,
its accuracy is limited—It is not always true that a
type (e.g., athlete) is more related to its parent
type (i.e., person) than to its sibling types (e.g.,
coach), or that all sibling types are equally re-
lated to each other (e.g., actor is more related to
director than to author). We later compare
these two methods in our experiments.

With the type correlation computed, we propose
to apply adaptive penalties on different negative
type labels (for a training mention), instead of treat-
ing all of the labels equally as in most existing
work (Weston et al., 2011). The hypothesis is intu-
itive: given the positive type labels for a mention, we
force the negative type labels which are related to the
positive type labels to receive smaller penalty. For
example, in the right column of Fig. 3, negative la-
bel businessman receives a smaller penalty (i.e.,
margin) than athele does, since businessman
is more related to politician.

Hypothesis 1 (Adaptive Margin) For a mention, if
a negative type is correlated to a positive type, the
margin between them should be smaller.

We propose an adaptive-margin rank loss to
model the set of “clean” mentions (i.e., Mc), based
on the above hypothesis. The intuition is simple: for
each mention, rank all the positive types ahead of
negative types, where the ranking score is measured
by similarity between mention and type. We denote

Types ranked w.r.t. mi 

Partial-Label Rank Loss for Noisy Mentions

Mention: mi = “Sn_Ted Cruz”

Context in Sn: “The effective end of Ted Cruz 's 
presidential campaign came on a call …”

Mention: mi’ = “S1_Arnold Schwarzenegger”

Context in S1: “ Governor Arnold Schwarzenegger gives 

a speech at Mission Serve's service project  .…”

“Full” Rank Loss for Clean Mentions

Distance between mi and types

Distance between mi and types

Person
Politician
Business

Artist
Athlete
Actor

Author
Doctor

Score(mi, yk) 

0.85
0.77
0.53
0.42
0.40
0.33
0.21
0.05

mi

person Politician

Businessman

Athlete

Artist

Actor

mi

person

Politician

Businessman

Athlete

Artist

Actor

Location

Athlete

Athlete

Politician
Coach
Chief

Doctor
Organization

Location

Politician
Person

Business
Artist

Athlete
Actor

Author

Types ranked w.r.t. mi “Best” candidate type

0.88
0.74
0.55
0.41
0.33
0.31
0.25

Positive
types

Negative
types

Coach

Noisy candidate type set

Figure 4: An illustration of the partial-label rank loss.

fk(mi) as the similarity between (mi, yk) and is de-
fined as the inner product of ΦM(mi) and ΦY(yk).

`c(mi,Yi,Yi) =
∑

yk∈Yi

∑

yk̄∈Yi

L
⌊
rankyk

(
f(mi)

)⌋
Θi,k,k̄;

Θi,k,k̄ = max
{

0, γk,k̄ − fk(mi) + fk̄(mi)
}

;

rankyk

(
f(mi)

)
=
∑

yk̄∈Yi

1

(
γk,k̄ + fk̄(mi) > fk(mi)

)
.

Here, γk,k̄ is the adaptive margin between positive
type k and negative type k̄, which is defined as γk,k̄ =
1 + 1/(wk,k̄ +α) with a smooth parameter α. L(x) =∑x

i=1
1
i transforms rank to a weight, which is then

multiplied to the max-margin loss Θi,k,k̄ to optimize
precision at x (Weston et al., 2011).

3.5 Modeling Noisy Type Labels

True type labels for noisy entity mentionsMn (i.e.,
mentions with ambiguous candidate types in the
given type hierarchy) in each sentence are not avail-
able in knowledge bases. To effectively model the
set of noisy mentions, we propose not to treat all

1373



candidate types (i.e., {Yi} as true labels. Instead, we
model the “true” label among the candidate set as
latent value, and try to infer that using text features.

Hypothesis 2 (Partial-Label Loss) For a noisy
mention, the maximum score associated with its
candidate types should be greater than the scores
associated with any other non-candidate types

We extend the partial-label loss in (Nguyen and
Caruana, 2008) (used to learn linear classifiers) to
enforce Hypothesis 2, and integrate with the adap-
tive margin to define the loss for mi (in setMn).

`n(mi,Yi,Yi) =
∑

k̄∈Yi

L
⌊
rankyk∗

(
f(mi)

)⌋
Ωi,k̄;

Ωi,k = max
{

0, γk∗,k̄ − fk∗(mi) + fk̄(mi)
}

;

rankyk∗

(
f(mi)

)
=
∑

yk̄∈Yi

1

(
γk∗,k̄ + fk̄(mi) > fk∗(mi)

)

where we define . yk∗ = argmaxyk∈Yi fk(mi) and
yk̄∗ = argmaxyk∈Yi fk(mi).

Minimizing `n encourages a large margin be-
tween the maximum scores maxyk∈Yi fyk(mi) and
maxyk̄∈Yi fyk(mi). This forces mi to be embed-
ded closer to the most “relevant” type in the noisy
candidate type set, i.e., y∗ = argmaxyk∈Yi fyk(mi),
than to any other non-candidate types (i.e., Hypoth-
esis 2). This constrasts sharply with multi-label
learning (Yosef et al., 2012), where a large margin
is enforced between all candidate types and non-
candidate types without considering noisy types.

3.6 Hierarchical Partial-Label Embedding
Our goal is to embed the heterogeneous graphG into
a d-dimensional vector space, following the three
proposed hypotheses in the section. Intuitively, one
can collectively minimize the objectives of the two
kinds of loss functions `c and `n, across all the train-
ing mentions. To achieve the goal, we formulate a
joint optimization problem as follows.

min
U, V

O =
∑

mi∈Mc
`c(mi,Yi,Yi) +

∑

mi∈Mn
`n(mi,Yi,Yi).

We use an alternative minimization algorithm based
on block-wise coordinate descent (Tseng, 2001) to
jointly optimize the objective O. One can also apply
stochastic gradient descent to do online update.

Type Inference. With the learned mention embed-
dings {ui} and type embeddings {vk}, we perform

Data sets Wiki OntoNotes BBN
#Types 113 89 47
#Documents 780,549 13,109 2,311
#Sentences 1.51M 143,709 48,899
#Training mentions 2.69M 223,342 109,090
#Ground-truth mentions 563 9,604 121,001
#Features 644,860 215,642 125,637
#Edges in graph 87M 5.9M 2.9M

Table 3: Statistics of the datasets.

top-down search in the given type hierarchy Y to
estimate the correct type-path Y∗i . Starting from the
tree’s root, we recursively find the best type among
the children types by measuring the dot product of
the corresponding mention and type embeddings,
i.e., sim(ui,vk). The search process stops when we
reach a leaf type, or the similarity score is below a
pre-defined threshold η > 0.

4 Experiments
4.1 Data Preparation

Datasets. Our experiments use three public datasets.
(1) Wiki (Ling and Weld, 2012): consists of 1.5M
sentences sampled from Wikipedia articles; (2)
OntoNotes (Weischedel et al., 2011): consists of
13,109 news documents where 77 test documents
are manually annotated (Gillick et al., 2014); (3)
BBN (Weischedel and Brunstein, 2005): consists of
2,311 Wall Street Journal articles which are man-
ually annotated using 93 types. Statistics of the
datasets are shown in Table 3.

Training Data. We followed the process in (Ling
and Weld, 2012) to generate training data for the
Wiki dataset. For the BBN and OntoNotes datasets,
we used DBpedia Spotlight3 for entity linking. We
discarded types which cannot be mapped to Free-
base types in the BBN dataset (47 of 93).

Table 2 lists the set of features used in our experi-
ments, which are similar to those used in (Yogatama
et al., 2015; Ling and Weld, 2012) except for top-
ics and ReVerb patterns. We discarded the features
which occur only once in the corpus.

4.2 Evaluation Settings

For the Wiki and OntoNotes datasets, we used the
provided test set. Since BBN corpus is fully anno-
tated, we followed a 80/20 ratio to partition it into

3http://spotlight.dbpedia.org/

1374



training/test sets. We report Accuracy (Strict-F1),
Micro-averaged F1 (Mi-F1) and Macro-averaged F1
(Ma-F1) scores commonly used in the fine-grained
type problem (Ling and Weld, 2012; Yogatama et
al., 2015). Since we use the gold mention set for
testing, the Accuracy (Acc) we reported is the same
as the Strict F1.

Baselines. We compared the proposed method
(AFET) and its variant with state-of-the-art typ-
ing methods, embedding methods and partial-label
learning methods 4: (1) FIGER (Ling and Weld,
2012); (2) HYENA (Yosef et al., 2012); (3)
FIGER/HYENA-Min (Gillick et al., 2014): re-
moves types appearing only once in the docu-
ment; (4) ClusType (Ren et al., 2015): predicts
types based on co-occurring relation phrases; (5)
HNM (Dong et al., 2015): proposes a hybrid neu-
ral model without hand-crafted features; (6) Deep-
Walk (Perozzi et al., 2014): applies Deep Walk to
a feature-mention-type graph by treating all nodes
as the same type; (7) LINE (Tang et al., 2015b):
uses a second-order LINE model on feature-type bi-
partite graph; (8) PTE (Tang et al., 2015a): ap-
plies the PTE joint training algorithm on feature-
mention and type-mention bipartite graphs. (9) WS-
ABIE (Yogatama et al., 2015): adopts WARP loss
to learn embeddings of features and types; (10) PL-
SVM (Nguyen and Caruana, 2008): uses a margin-
based loss to handle label noise. (11) CLPL (Cour
et al., 2011): uses a linear model to encourage large
average scores for candidate types.

We compare AFET and its variant: (1) AFET:
complete model with KB-induced type correlation;
(2) AFET-CoH: with hierarchy-induced correlation
(i.e., shortest path distance); (3) AFET-NoCo: with-
out type correlation (i.e., all margin are “1”) in the
objective O; and (4) AFET-NoPa: without label
partial loss in the objective O.

4.3 Performance Comparison and Analyses
Table 4 shows the results of AFET and its variants.
Comparison with the other typing methods.
AFET outperforms both FIGER and HYENA sys-
tems, demonstrating the predictive power of the

4We used the published code for FIGER, ClusType, HNM,
LINE, PTE, and DeepWalk, and implemented other baselines
which have no public code. Our implementations yield compa-
rable performance as those reported in the original papers.

learned embeddings, and the effectiveness of mod-
eling type correlation information and noisy candi-
date types. We also observe that pruning methods do
not always improve the performance, since they ag-
gressively filter out rare types in the corpus, which
may lead to low Recall. ClusType is not as good
as FIGER and HYENA because it is intended for
coarse types and only utilizes relation phrases.

Comparison with the other embedding methods.
AFET performs better than all other embedding
methods. HNM does not use any linguistic features.
None of the other embedding methods consider the
label noise issue and treat the candidate type sets as
clean. Although AFET adopts the WARP loss in
WSABIE, it uses an adaptive margin in the objec-
tive to capture the type correlation information.

Comparison with partial-label learning methods.
Compared with PL-SVM and CLPL, AFET obtains
superior performance. PL-SVM assumes that only
one candidate type is correct and does not consider
type correlation. CLPL simply averages the model
output for all candidate types, and thus may gener-
ate results biased to frequent false types. Superior
performance of AFET mainly comes from modeling
type correlation derived from KB.

Comparison with its variants. AFET always out-
performs its variant on all three datasets. It gains
performance from capturing type correlation, as well
as handling type noise in the embedding process.

4.4 Case Analyses

Example output on news articles. Table 5 shows
the types predicted by AFET, FIGER, PTE and
WSABIE on two news sentences from OntoNotes
dataset: AFET predicts fine-grained types with bet-
ter accuracy (e.g., person title) and avoids
overly-specific predictions (e.g., news company).
Figure 5 shows the types estimated by AFET,
PTE and WSABIE on a training sentence from
OntoNotes dataset. We found AFET could discover
the best type from noisy candidate types.

...	his	friend	[Travis]	would	take	a	
psychiatrist	on	a	date	to	analyze	...	
Candidate	Types:	{organization,	
music,	person,	artist}

WSABIE:					 {organization}
PTE:	 {music,	person,	artist}
AFET:	 {person}

Figure 5: Example output of AFET and the compared
methods on a training sentence from OntoNotes dataset.

1375



Typing Wiki OntoNotes BBN
Method Acc Ma-F1 Mi-F1 Acc Ma-F1 Mi-F1 Acc Ma-F1 Mi-F1
CLPL (Cour et al., 2011) 0.162 0.431 0.411 0.201 0.347 0.358 0.438 0.603 0.536
PL-SVM (Nguyen and Caruana, 2008) 0.428 0.613 0.571 0.225 0.455 0.437 0.465 0.648 0.582
FIGER (Ling and Weld, 2012) 0.474 0.692 0.655 0.369 0.578 0.516 0.467 0.672 0.612
FIGER-Min (Gillick et al., 2014) 0.453 0.691 0.631 0.373 0.570 0.509 0.444 0.671 0.613
HYENA (Yosef et al., 2012) 0.288 0.528 0.506 0.249 0.497 0.446 0.523 0.576 0.587
HYENA-Min 0.325 0.566 0.536 0.295 0.523 0.470 0.524 0.582 0.595
ClusType (Ren et al., 2015) 0.274 0.429 0.448 0.305 0.468 0.404 0.441 0.498 0.573
HNM (Dong et al., 2015) 0.237 0.409 0.417 0.122 0.288 0.272 0.551 0.591 0.606
DeepWalk (Perozzi et al., 2014) 0.414 0.563 0.511 0.479 0.669 0.611 0.586 0.638 0.628
LINE (Tang et al., 2015b) 0.181 0.480 0.499 0.436 0.634 0.578 0.576 0.687 0.690
PTE (Tang et al., 2015a) 0.405 0.575 0.526 0.436 0.630 0.572 0.604 0.684 0.695
WSABIE (Yogatama et al., 2015) 0.480 0.679 0.657 0.404 0.580 0.527 0.619 0.670 0.680
AFET-NoCo 0.526 0.693 0.654 0.486 0.652 0.594 0.655 0.711 0.716
AFET-NoPa 0.513 0.675 0.642 0.463 0.637 0.591 0.669 0.715 0.724
AFET-CoH 0.433 0.583 0.551 0.521 0.680 0.609 0.657 0.703 0.712
AFET 0.533 0.693 0.664 0.551 0.711 0.647 0.670 0.727 0.735

Table 4: Study of typing performance on the three datasets.

Text

“... going to be an im-
minent easing of mon-
etary policy, ” said
Robert Dederick , chief
economist at Northern
Trust Co. in Chicago.

...It’s terrific for adver-
tisers to know the reader
will be paying more , ”
said Michael Drexler ,
national media director
at Bozell Inc. ad agency.

Ground
Truth

organization,
company

person,
person title

FIGER organization organization

WSABIE
organization,
company,
broadcast

organization,
company,
news company

PTE organization person

AFET organization,
company

person,
person title

Table 5: Example output of AFET and the compared
methods on two news sentences from OntoNotes dataset.

Testing the effect of training set size and dimen-
sion. Experimenting with the same settings for
model learning, Fig. 6(a) shows the performance
trend on the Wiki dataset when varying the sampling
ratio (subset of mentions randomly sampled from
the training set D). Fig. 6(b) analyzes the perfor-
mance sensitivity of AFET with respect to d—the
embedding dimension on the BBN dataset. Accu-
racy of AFET improves as d becomes large but the
gain decreases when d is large enough.
Testing sensitivity of the tuning parameter.
Fig. 7(b) analyzes the sensitivity of AFET with re-
spect to α on the BBN dataset. Performance in-
creases as α becomes large. When α is large than
0.5, the performance becomes stable.
Testing at different type levels. Fig. 7(a) reports
the Ma-F1 of AFET, FIGER, PTE and WSABIE at
different levels of the target type hierarchy (e.g., per-

0 20 40 60 80 100

Sampling Ratio

0.35

0.40

0.45

0.50

0.55

0.60

M
ic

ro
-F

1

FIGER

WSABIE

AFET

(a) Varying training set size

0 50 100 150 200 250 300

Embedding Size

0.50

0.55

0.60

0.65

0.70

0.75

A
cc

u
ra

cy

PTE

WSABIE

AFET

(b) Varying d

Figure 6: Performance change with respect to (a) sam-
pling ratio of training mentions on the Wiki dataset; and
(b) embedding dimension d on the BBN dataset.

Level-1 Level-2 Level-3
0.0

0.1

0.2

0.3

0.4

0.5

0.6

0.7

0.8

A
cc

u
ra

cy

FIGER

WSABIE

PTE

AFET

(a) Test at different levels

0.0 0.2 0.4 0.6 0.8 1.0

Alpha

0.60

0.65

0.70

0.75

0.80

Accuracy

Macro-F1

Micro-F1

(b) Varying α

Figure 7: Performance change (a) at different levels of
the type hierarchy on the OntoNotes dataset; and (b) with
respect to smooth parameter α on the BBN dataset.

son and location on level-1, politician and artist on
level-2, author and actor on level-3). The results
show that it is more difficult to distinguish among
more fine-grained types. AFET always outperforms
the other two method, and achieves a 22.36% im-
provement in Ma-F1, compared to FIGER on level-3
types. The gain mainly comes from explicitly mod-
eling the noisy candidate types.

Testing for frequent/infrequent types. We also

1376



Type animal city
# of Training Mentions 1882 12421
# of Test Mentions 8 240
WSABIE 0.176 0.546
FIGER 0.167 0.648
PTE 0.222 0.677
AFET 0.400 0.766

Table 6: Example output of AFET and other methods on
frequent/infrequent type from OntoNotes dataset.

evaluate the performance on frequent and rare types
(Table 6). Note that we use a different evaluation
metric, which is introduced in (Yosef et al., 2012)
to calculate the F1 score for a type. We find AFET
can always perform better than other baselines and
it works for both frequent and rare types.

5 Related Work

There has been considerable work on named entity
recognition (NER) (Manning et al., 2014), which fo-
cuses on three types (e.g., person, location)
and cast the problem as multi-class classification fol-
lowing the type mutual exclusion assumption (i.e.,
one type per mention) (Nadeau and Sekine, 2007).

Recent work has focused on a much larger set
of fine-grained types (Yosef et al., 2012; Ling and
Weld, 2012). As the type mutual exclusion assump-
tion no longer holds, they cast the problem as multi-
label multi-class (hierarchical) classification prob-
lems (Gillick et al., 2014; Yosef et al., 2012; Ling
and Weld, 2012). Embedding techniques are also
recently applied to jointly learn feature and type rep-
resentations (Yogatama et al., 2015; Dong et al.,
2015). Del Corro et al. (2015) proposed an unsuper-
vised method to generate context-aware candidates
types, and subsequently select the most appropriate
type. Gillick et al. (2014) discuss the label noise is-
sue in fine-grained typing and propose three pruning
heuristics. However, these heuristics aggressively
delete training examples and may suffer from low
recall (see Table. 4).

In the context of distant supervision, label noise
issue has been studied for other information extrac-
tion tasks such as relation extraction (Takamatsu et
al., 2012). In relation extraction, label noise is intro-
duced by the false positive textual matches of en-
tity pairs. In entity typing, however, label noise
comes from the assignment of types to entity men-
tions without considering their contexts. The forms

of distant supervision are different in these two prob-
lems. Recently, (Ren et al., 2016b) has tackled the
problem of label noise in fine-grained entity typing,
but focused on how to generate a clean training set
instead of doing entity typing.

Partial label learning (PLL) (Zhang, 2014;
Nguyen and Caruana, 2008; Cour et al., 2011) deals
with the problem where each training example is as-
sociated with a set of candidate labels, where only
one is correct. Unlike existing PLL methods, our
method considers type hierarchy and correlation.

6 Conclusion and Future Work

In this paper, we study automatic fine-grained en-
tity typing and propose a hierarchical partial-label
embedding method, AFET, that models “clean”
and “noisy” mentions separately and incorporates a
given type hierarchy to induce loss functions. AFET
builds on a joint optimization framework, learns em-
beddings for mentions and type-paths, and itera-
tively refines the model. Experiments on three pub-
lic datasets show that AFET is effective, robust, and
outperforms other comparing methods.

As future work, it would be interesting to study
topical features as the context cues of the entity men-
tions, to leverage multi-sensing embedding to repre-
sent linguistic features with multiple senses, and to
exploits other effective modeling methods to inject
type hierarchy information. The proposed objective
function is general and can be considered to incorpo-
rate various language features, to conduct integrated
modeling of multiple sources, and to be extended to
distantly-supervised relation extraction.

7 Acknowledgments

Research was sponsored in part by the U.S. Army
Research Lab. under Cooperative Agreement
No. W911NF-09-2-0053 (NSCTA), DARPA DEFT
No. FA8750-13-2-0041, National Science Foun-
dation IIS-1017362, IIS-1320617, IIS-1354329,
and IIS-1523198, HDTRA1-10-1-0120, and grant
1U54GM114838 awarded by NIGMS through funds
provided by the trans-NIH Big Data to Knowledge
(BD2K) initiative (www.bd2k.nih.gov). The
views and conclusions contained in this paper are
those of the authors and should not be interpreted as
representing any funding agencies.

1377



References
Timothee Cour, Ben Sapp, and Ben Taskar. 2011. Learn-

ing from partial labels. JMLR, 12:1501–1536.
Luciano Del Corro, Abdalghani Abujabal, Rainer

Gemulla, and Gerhard Weikum. 2015. Finet:
Context-aware fine-grained named entity typing. In
EMNLP.

Xin Luna Dong, Thomas Strohmann, Shaohua Sun, and
Wei Zhang. 2014. Knowledge vault: A web-scale
approach to probabilistic knowledge fusion. In KDD.

Li Dong, Furu Wei, Hong Sun, Ming Zhou, and Ke Xu.
2015. A hybrid neural model for type classification of
entity mentions. In IJCAI.

Jesse Dunietz and Dan Gillick. 2014. A new en-
tity salience task with millions of training examples.
EACL.

Jenny Rose Finkel, Trond Grenager, and Christopher
Manning. 2005. Incorporating non-local information
into information extraction systems by gibbs sampling.
In ACL.

Evgeniy Gabrilovich, Michael Ringgaard, and Amarnag
Subramanya. 2013. Facc1: Freebase annotation of
clueweb corpora.

Dan Gillick, Nevena Lazic, Kuzman Ganchev, Jesse
Kirchner, and David Huynh. 2014. Context-
dependent fine-grained entity type tagging. arXiv
preprint arXiv:1412.1820.

Heng Ji and Ralph Grishman. 2008. Refining event ex-
traction through cross-document inference. In ACL.

Jyun-Yu Jiang, Chin-Yew Lin, and Pu-Jen Cheng. 2015.
Entity-driven type hierarchy construction for freebase.
In WWW.

Thomas Lin, Oren Etzioni, et al. 2012. No noun phrase
left behind: detecting and typing unlinkable entities.
In EMNLP.

Xiao Ling and Daniel S Weld. 2012. Fine-grained entity
recognition. In AAAI.

Christopher D Manning, Mihai Surdeanu, John Bauer,
Jenny Finkel, Steven J Bethard, and David McClosky.
2014. The stanford corenlp natural language process-
ing toolkit. ACL.

Mike Mintz, Steven Bills, Rion Snow, and Dan Jurafsky.
2009. Distant supervision for relation extraction with-
out labeled data. In ACL.

David Nadeau and Satoshi Sekine. 2007. A survey of
named entity recognition and classification. Lingvisti-
cae Investigationes, 30:3–26.

Nam Nguyen and Rich Caruana. 2008. Classification
with partial labels. In KDD.

Bryan Perozzi, Rami Al-Rfou, and Steven Skiena. 2014.
Deepwalk: Online learning of social representations.
In KDD.

Lev Ratinov and Dan Roth. 2009. Design challenges and
misconceptions in named entity recognition. In ACL.

Xiang Ren, Ahmed El-Kishky, Chi Wang, Fangbo Tao,
Clare R Voss, Heng Ji, and Jiawei Han. 2015.
Clustype: Effective entity recognition and typing by
relation phrase-based clustering. In KDD.

Xiang Ren, Ahmed El-Kishky, Chi Wang, and Jiawei
Han. 2016a. Automatic entity recognition and typing
in massive text corpora. In WWW.

Xiang Ren, Wenqi He, Meng Qu, Clare R Voss, Heng Ji,
and Jiawei Han. 2016b. Label noise reduction in en-
tity typing by heterogeneous partial-label embedding.
In KDD.

Sameer Singh, Amarnag Subramanya, Fernando Pereira,
and Andrew McCallum. 2012. Wikilinks: A large-
scale cross-document coreference corpus labeled via
links to wikipedia. UM-CS-2012-015.

Shingo Takamatsu, Issei Sato, and Hiroshi Nakagawa.
2012. Reducing wrong labels in distant supervision
for relation extraction. In ACL.

Jian Tang, Meng Qu, and Qiaozhu Mei. 2015a. Pte: Pre-
dictive text embedding through large-scale heteroge-
neous text networks. In KDD.

Jian Tang, Meng Qu, Mingzhe Wang, Ming Zhang, Jun
Yan, and Qiaozhu Mei. 2015b. Line: Large-scale in-
formation network embedding. In WWW.

Paul Tseng. 2001. Convergence of a block coordi-
nate descent method for nondifferentiable minimiza-
tion. JOTA, 109(3):475–494.

Ralph Weischedel and Ada Brunstein. 2005. Bbn pro-
noun coreference and entity type corpus. Linguistic
Data Consortium, 112.

Ralph Weischedel, Eduard Hovy, Mitchell Marcus,
Martha Palmer, Robert Belvin, Sameer Pradhan,
Lance Ramshaw, and Nianwen Xue. 2011.
Ontonotes: A large training corpus for enhanced pro-
cessing.

Jason Weston, Samy Bengio, and Nicolas Usunier. 2011.
Wsabie: Scaling up to large vocabulary image annota-
tion. In IJCAI.

Dani Yogatama, Dan Gillick, and Nevena Lazic. 2015.
Embedding methods for fine grained entity type clas-
sification. In ACL.

Mohamed Amir Yosef, Sandro Bauer, Johannes Hoffart,
Marc Spaniol, and Gerhard Weikum. 2012. Hyena:
Hierarchical type classification for entity names. In
COLING.

Xiao Yu, Xiang Ren, Yizhou Sun, Quanquan Gu, Bradley
Sturt, Urvashi Khandelwal, Brandon Norick, and Ji-
awei Han. 2014. Personalized entity recommenda-
tion: A heterogeneous information network approach.
In WSDM.

Min-Ling Zhang. 2014. Disambiguation-free partial la-
bel learning. In SDM.

1378


