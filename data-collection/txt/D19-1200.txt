




















































paper_v12_20191022


Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing
and the 9th International Joint Conference on Natural Language Processing, pages 1920–1930,
Hong Kong, China, November 3–7, 2019. c©2019 Association for Computational Linguistics

1920

A Semi-Supervised Stable Variational Network for Promoting
Replier-Consistency in Dialogue Generation

Jinxin Chang1,2,∗, Ruifang He1,2,3,∗†, Longbiao Wang1,2,†, Xiangyu Zhao1,2,

Ting Yang1,2, and Ruifang Wang1,2

1Tianjin Key Laboratory of Cognitive Computing and Application, Tianjin, China.
2College of Intelligence and Computing, Tianjin University, Tianjin, China.

3State Key Laboratory of Cognitive Intelligence, iFLYTEK, P.R. China.

{changjinxin,rfhe,longbiao wang}@tju.edu.cn
{zhaoxiangyu009,16622898776,wrf276224255}@163.com

Abstract

Neural sequence-to-sequence models for dia-

log systems suffer from the problem of favor-

ing uninformative and non replier-specific re-

sponses due to lacking of the global and rele-

vant information guidance. The existing meth-

ods model the generation process by leverag-

ing the neural variational network with simple

Gaussian. However, the sampled information

from latent space usually becomes useless due

to the KL divergence vanishing issue, and the

highly abstractive global variables easily di-

lute the personal features of replier, leading

to a non replier-specific response. Therefore,

a novel Semi-Supervised Stable Variational

Network (SSVN) is proposed to address these

issues. We use a unit hypersperical distribu-

tion, namely the von Mises-Fisher (vMF), as

the latent space of a semi-supervised model,

which can obtain the stable KL performance

by setting a fixed variance and hence enhance

the global information representation. Mean-

while, an unsupervised extractor is introduced

to automatically distill the replier-tailored fea-

ture which is then injected into a supervised

generator to encourage the replier-consistency.

Experimental results on two large conversa-

tion datasets show that our model outperforms

the competitive baseline models significantly,

and can generate diverse and replier-specific

responses.

1 Introduction

Dialog systems, aiming at generating relevant and

fluent responses in the replier-consistent way, have

received increasing attention due to its numerous

applications (Grosz, 2016; Chen et al., 2017a).

Recently, Seq2Seq neural networks (Sutskever

et al., 2014) have demonstrated excellent results

on open-domain conversation (Shang et al., 2015;

∗∗ Equal contribution.
†† Corresponding author.

Sordoni et al., 2015; Vinyals and V. Le, 2015;

Yao et al., 2015). However, due to lacking of the

global and relevant information guidance, they in-

herently tend to generate trivial and uninforma-

tive responses (e.g., “I don’t know”), rather than

meaningful and replier-specific ones (Serban et al.,

2016; Li et al., 2016).

The existing methods based on neural vari-

ational methods with Gaussian (Kingma and

Welling, 2014; Kingma et al., 2014), are proposed

to use a latent variable as the global information in

decoder to strengthen the generation (Serban et al.,

2017; Zhao et al., 2017; Chen et al., 2018). How-

ever, they face the problems of (1) latent space

futility and (2) replier-consistency decay.

(1) The model tends to select more gain from

a lower Kullback-Leibler (KL) divergence during

training, which encourages the approximate pos-

terior close to Gaussian prior, rendering the latent

space of the former unused. Thus, the latent vari-

ables on this space become worthless global guid-

ance for decoder. To address this issue, most pre-

vious work (Xie et al., 2017; Yang et al., 2017;

Chen et al., 2017) has suggested a weaker decoder

to match the Gaussian samples, which essentially

sacrifice the generative capacity. (2) The speak-

ers in a dyadic conversation have different lin-

guistic characteristics, sentiments and personali-

ties. However, the latent variable is learned condi-

tioned on the holistic context without any distinc-

tion between speakers, especially the replier. This

will dilute the personal features of replier and lead

to a decrease in replier-consistency. Current meth-

ods (Li et al., 2016; Zhang et al., 2018) normally

recur to artificially scheduled personal information

to promote the replier-consistency, but they cannot

be migrated to the other datasets.

Inspired by the effectiveness of vMF distribu-

tion in solving the KL-vanishing in the unsuper-

vised scene (Xu and Durrett, 2018) and the suc-



1921

cess of Variational Auto-Encoder (VAE) in cap-

turing latent feature of the real data (Davidson

et al., 2018), we propose a Semi-Supervised Stable

Variational Network (SSVN) framework to ad-

dress the above issues. It consists of an unsuper-

vised personal feature extractor (a VAE with vMF)

and a supervised information-enhanced generator

(a CVAE with vMF). To maintain the consistency

of replier features, the extractor only encodes the

previous utterances from the replier and produces

a personally tailored latent variable. On the top of

this, the generator fuses the replier-tailored latent

variable and the self vMF distributed global infor-

mation to facilitate the diverse and replier-specific

responses.

In general, our contributions are as follows:

• A semi-supervised stable variational network
is proposed to solve the latent space futility

issue and promote the replier-consistency.

• To the best of our knowledge, our model is
the first to use the vMF distribution in a semi-

supervised framework for dialogue genera-

tion, which can enhance the global informa-

tion by alleviating the KL divergence vanish-

ing problem.

• An unsupervised personal feature extractor is
designed to acquire the replier-specific fea-

tures automatically.

• The experimental results on two large con-
versation datasets validate the effectiveness

of our model.

• It is shown that the different roles of vMF
on extractor and generator. We suprisingly

find that the extractor can alleviate the KL-

vanishing to some extent.

2 Related Work

2.1 Neural Variational Network

Variational autoencoder (VAE) (Kingma and

Welling, 2014; Rezende et al., 2014) is one of the

most popular generative models. The principle

idea is to encode the data x to learn a probabil-
ity distribution z, then sample the latent variables
from z and inject them into a directed decoder net-
work to reconstruct x. The model parameters are
optimized by maximizing a reparameterized vari-

ational lower bound. Based on this process, the

conditional VAE (CVAE) (Sohn et al., 2015) can

be conditioned on certain attributes to improve di-

versity. In diaog generation task, Serban et al.

(2017) employs the CVAE to acquire a global la-

tent variable as a holistic representation in a hier-

archical setting. Zhao et al. (2017) regards the la-

tent variable as a global dialog act information and

directly feed it to the decoder to control the dia-

log act of a response. To maintain the long-term

memory of the previous utterances, Chen et al.

(2018) utilizes the higher-level abstract variable to

retrieve and update memory cells.

2.2 Latent Space Futility

As for the latent space futility issue, also called

KL-vanishing in Shen et al. (2018), most previous

work has suggested a weaker decoder to encour-

age the simple Gaussian samples to be leveraged,

such as a word drop-out technique in decoder (Xie

et al., 2017; Zhao et al., 2017) or a practice of

replacing RNN decoder with a CNN counterpart

(Yang et al., 2017; Chen et al., 2017; Semeniuta

et al., 2017). These methods are contrary to our

origiral intention due to generation capacity de-

scending. Other efforts focus on changing prior

and posterior: Rezende and Mohamed (2015) and

Kingma et al. (2016) utilize a normalizing flow

to transform the sampled variables; Shen et al.

(2018) introduces an AE module to complicate the

quondam distribution. Extending the latter direc-

tion yet without increasing the model complexity,

we only leverage the vMF distribution instead of

the simple Gaussian to strengthen the KL term.

Unlike the single vMF-based VAEs implemented

in other cases (Davidson et al., 2018; Guu et al.;

Xu and Durrett, 2018), we apply vMF into a semi-

supervised dialog model to generate diverse and

replier-specific responses.

2.3 Replier-Consistency Decay

In order to emphasize the replier-consistency, Li

et al. (2016) captures personas’ characteristics of

Twitter users by encapsulating background infor-

mation and speaking style into the distributed em-

beddings (one per user), which are used to im-

prove consistency for the same person. Zhang

et al. (2018) presents a persona-provided dialogue

dataset and trains dialog models conditioned on

their given configurable, but persistent profile in-

formation. However, the above work relies heavily

on manually scheduled persona information and

has difficulties in migrating to other common con-

versation corpora. In contrast to this, our work



1922

Output:

x2+x4: do you want to 

download ? http://releases.ub-

untu.com/precise/ and use 

torrent please

y: depends of your comp

personal feature

Input:

x1: ubuntu site doesn't work 

for me ...  i just get ' waiting 

for www. ubuntu.com ...' in 

my browser ...

x2: do you want to download ?

x3: yes . any link to some 

usable site will be appreciated 

:-) 

x4: http://releases.ubuntu.com/

precise/ and use torrent please

x5: i thought they were going 

to recommend 64-bit for 

desktops , but when I 

managed to access www.ubu-

ntu.com it preferred 32-bit . is 

32-bit preferred for desktops ?

y: depends of your comp

 y 

x2+x4

 Information-Enhanced Generator

Global Context Encoder

Utterance Encoder

Utterance Encoder

 ( , )vMF  

Recognition/Prior Network

Response

Decoder

 Personal Feature Extractor

Local Context Encoder

Utterance Encoder

 ( , )vMF  

Recognition/Prior Network

Replier

Decoder

x1 x5~

x2+x4

 y 

1

u
h

2

u
h 3

u
h 4

u
h 5

u
h

2

u
h 4

u
h

2

r
h

u

y
h

5

c
h

 

r
z

z

 

r
z

Figure 1: The SSVN framework.

focuses on automatically extracting the individ-

ual features of replier from the original conver-

sation text, to enhance the replier-consistency of

responses, without any corpus restriction.

3 Model

3.1 Task Description

Given a series of dialogue context ut-

terances (x1,x2, ...,xn), where xi =
(wi,1, wi,2, ..., wi,Ni), our task is to generate
a response y = (wy,1, wy,2, ..., wy,Ny) that not
only rely on the global information but also

consider the personally special features from

the replier. In this paper, we employ the vMF

distribution to stimulate the potential of latent

space, impelling the extractor to condense a

feature-augmented individual information and the

generator to generalize a useful global guidance.

The overview of SSVN is illustrated in Figure 1.

3.2 von Mises-Fisher

The von Mises-Fisher (vMF) places a distribution

over points on the unit hypersphere, parameterized

by a direction vector µ ∈ Rd indicating the mean
direction and a concentration parameter κ ∈ R≥0.
The PDF of the vMF distribution for the unit vec-

tor z ∈ Rd is defined as:

fd(x;µ, κ) = Cd(κ) exp(κµ
Tx) (1)

Cd(κ) =
κd/2−1

(2π)d/2Id/2−1(κ)
(2)

where ‖µ‖= 1, Cd is the normalization constant,
and Iρ stands for the modified Bessel function of
the first kind at order ρ.

3.3 Personal Feature Extractor

To enhance the replier-consistency, the personal

feature extractor, implemented by a VAE with

vMF, encodes rustically the context utterances

from replier xr = (xr1,x
r
2, ...,x

r
l ) (e.g., x

r =
(xr1,x

r
2) = (x2,x4) in Figure 1 into a ran-

dom latent variable zr, based on which the de-
coder reconstructs xr. Due to an intractable high-

dimensional integral problem over the latent vari-

able zr, we set a recognition network qφe(zr|xr)
as a variational approximation to the true poste-

rior p(zr|xr), then apply variational inference to
optimize the evidence lower bound (ELBO) as:

LE = −KL(qφe(zr|xr)‖pθe(zr))
+ Eqφe (zr|xr) log pθe(x

r|zr)

≤ log
∫

zr

pθe(zr)p(x
r|zr)dzr = log p(xr)

(3)

Utterance & Local Context Encoder Concretely,

we employ a hierarchical encoder to encode

xr: the utterance encoder based on bidirectional

RNN (Schuster and Paliwal, 1997) determiniti-

cally reads each utterance xri and output a size-

fixed real-valued hui = [
−→
hui ,
←−
hui ], which the local

context encoder takes as input to obtain the final

hidden state hrl as the summary of x
r.

Prior/Posterior Distribution Since we assume

the latent space follows vMF distribution, the prior

pθe((zr) ∼ vMF (·, κeprior = 0) and the varia-
tional posterior qφe(zr|xr) ∼ vMF (µepos, κepos)
where µepos is the output of the recognition net-



1923

work and κepos is set to a constant.

˜µepos = f
e
pos(h

r
l ) (4)

µepos = ˜µ
e
pos/‖ ˜µepos‖ (5)

where f epos(·) is a linear transformation, and ‖·‖
stands for 2-norm to ensure the normalization.

With the uniform distribution as our prior, the

KL divergence can be computed as:

KL(vMF (µepos, κ
e
pos)‖vMF (·, 0)) =

(

1− d
2

)

log 2− log Id/2−1(κepos)− log Γ
(

d

2

)

+ κepos
Id/2(κ

e
pos)

Id/2−1(κepos)
+

(

d

2
− 1

)

log κepos

(6)

Since Eq. 6 only depends on fixed constant κepos,
not on µepos, this term can resolve the latent space
futility problem by averting the KL-zeroing.

Replier Decoder During reconstruction, the de-

coder receives the concatenation of replier’s con-

text hrl and personal latent variable zr as the initial
hidden state, then generates tokens sequentially

under the following probability distribution:

pθe(x
r|zr) =

l
∏

i=1

Ni
∏

j=1

p(wi,j |xr<i, wi,<j) (7)

where l is the number of turns of the replier’s con-
text xr; Ni is the length of the i-th utterance (x

r
i )

in xr; wi,j is the j-th token in x
r
i .

3.4 Information-Enhanced Generator

Similar to the extractor, the information-enhanced

generator based on CVAE also employs a recogni-

tion network qφg(z|x,y) to approximate the true
posterior p(z|x,y), correspondingly, its ELBO
can be calculated as:

log p(y|x) = log
∫

z
p(y|x, z)p(z|x)dz

≥ −KL(qφg(z|x,y)‖pθg(z|x))
+ Eqφg (z|x,y) log pθg(y|x, z)

(8)

when considering an external personal feature zr,
the ELBO in generator would be rewritten as:

LG = −KL(qφg(z|x,y, zr)‖pθg(z|x, zr))
+ Eqφg (z|x,y,zr) log pθg(y|x, z, zr)

≤ log
∫

z
p(y|x, z, zr)p(z|x, zr)dz

= log p(y|x, zr)

(9)

Notice that zr only participates in the genera-
tion process pθg(y|x, z, zr), the approximate pos-
terior qφg(z|x,y, zr) ∼ vMF (µgpos, κgpos) is con-
ditioned on dialog context x and the correspond-

ing response y, and the prior pθg(z|x, zr) ∼
vMF (µgprior, κ

g
prior) depends on x

1.

Utterance & Golbal Context Encoder The hi-

erarchical encoder in this part utilizes the shared

utterance encoder from extractor to encode utter-

ances x1,x2, ...,xn,y into the corresponding rep-
resentations hu1 , h

u
2 , ..., h

u
n, h

u
y orderly. Thereafter,

the utterance vectors hu1 , h
u
2 , ..., h

u
n are fed to the

global context encoder to compute the represen-

tation of the whole dialog context hcn. Based on
these, the approximate posterior and prior can be

determined by the following operations:

˜µgpos = f
g
pos([h

c
n, h

u
y ]) (10)

µgpos =
˜µgpos/‖ ˜µgpos‖ (11)

˜µgprior = f
g
prior(h

c
n) (12)

µgprior =
˜µgprior/‖ ˜µ

g
prior‖ (13)

where fgpos(·) and fgprior(·) are both linear trans-
formations. κgpos and κ

g
prior in both distributions

are the constants with equal values.

Prior/Posterior Distribution Without vMF (·, 0)
as the prior, we require to recalculate the KL term

in generator as:

KL(vMF (µgpos, κ
g
pos)‖vMF (µgprior, κ

g
prior)) =

(d/2− 1) log κ
g
pos

κgprior
+ log

Id/2−1(κ
g
prior)

Id/2−1(κ
g
pos)

− κgpriorµ
g
priorµ

g
pos

−1 Id/2(κ
g
pos)

Id/2−1(κ
g
pos)

+ κgpos
Id/2(κ

g
pos)

Id/2−1(κ
g
pos)

(14)

Response Decoder We employ a RNN decoder

similar to the one in extractor, extending it to con-

dition on a personal feature zr by concatenating zr
to the input of the decoder at each time step. The

concrete generative process is as follows:

sRt = σ(s
R
t−1, [ewy,t−1 , zr]) (15)

pvocab = softmax(V s
R
t + b) (16)

1
The zr in qφg (z|x,y, zr) and pθg (z|x, zr) is introduced formally

to satisfy the consistency of the formula derivation, where, in practice, both

distributions are equivalent to qφg (z|x,y) and pθg (z|x) respectively.



1924

where σ is the sigmoid function; ewy,i is the word
embedding of the i-th word in response y; sRt de-
notes the hidden state at the time step t; V and
b are learnable parameters; pvocab stands for the
probability distribution over the vocabulary. Then

the objective function of the decoder is given by:

pθg(y|x, z, zr) =
Ny
∏

i=1

pvocab(wy,i) (17)

where pvocab(wy,i) is the probability to generate
the word wy,i; Ny is the length of the response y.

3.5 Training Objective

The entire SSVN model integrates two modules in

Figure 1, i.e., the unsupervised extractor and the

supervised generator, which can be optimized si-

multaneously in one framework. Thus, the overall

objective function of SSVN is to maximize:

L = λLG + (1− λ)LE (18)

where we have a hyperparameter λ to control the
balance between response generation (generator)

and personality reconstruction (extractor).

3.6 Sampling Techique for vMF

Similar to Xu and Durrett (2018), we utilize the re-

jection sampling scheme of Wood (1994) to sam-

ple a value w ∈ [−1, 1], then derive a random unit
vector tangent υ on the hypersphere at the mean
vector µ. Based on these, our latent variable z can
be given by z = wµ+ υ

√
1− w2.

4 Experiments

4.1 Datasets

The proposed model is evaluated on two datasets.

The first corpus is Cornell Movie Dialogs Cor-

pus2 (Danescu-Niculescu-Mizil and Lee, 2011)

that contains more than 80,000 imagined movie

conversations. To normalize the length (turns)

of the dialogs, we divide the original conversa-

tions into consecutive 3-10 utterances. Our second

dataset is Ubuntu Dialogue Corpus3 (Lowe et al.,

2015). It contains about 500,000 multi-turn dia-

logues collected from the Ubuntu Internet Relayed

Chat channel, each of which starts with a Ubuntu-

related technical problem and follows by the cor-

responding responses about solutions.

2
The dataset is available at https://www.cs.cornell.edu/˜c-

ristian/Cornell Movie-Dialogs Corpus.html.
3
We use the same train-validation-test split as in Chen et al. (2018).

In the above two datasets, the last utterance in

a conversation is regarded as the response and the

remaining ones are the input context. The detailed

statistical information is shown in Table 1.

4.2 Baselines

We compare SSVN with the following models:

S2SA: the standard Seq2Seq model with the at-

tention mechanism (Vinyals and V. Le, 2015).

HRED: a hierarchical encoder framework to

model multi-turn dialogs (Serban et al., 2016).

VHRED: a hierarchical encoder-decoder with

latent stochastic variable (Serban et al., 2017).

HVMN: an encoder-decoder network contain-

ing the hierarchical structure and the variational

memory (Chen et al., 2018).

4.3 Experimental Details

Our model is implemented using the Tensorflow

framework (Abadi et al., 2016) with the follow-

ing parameter settings: We set word embeddings

to size of 200 and initialize them randomly. The

shared utterance encoder is a 2-layer bidirectional

GRU structure with 600 hidden neurons for each

layer, while the both context encoders and the both

decoders are the unidirectional ones with hidden

size of 600. The dimensions of the latent variable

z and zr are both set to 50. We use the Adam algo-
rithm (Kingma and Ba, 2014) to update the param-

eters with an initial learning rate of 0.001. In the

training, we employ the early-stop strategy (Caru-

ana et al., 2000) to select the best models using the

variational lower-bound on the validation set.

4.4 Evaluation Metrics

We use both automatic and human evaluations to

analyze the model’s performance.

Automatic Evaluation Metrics In our exper-

iment, three embedding-based metrics (Average,

Greedy, Extreme) 4 (Liu et al., 2016) are em-

ployed to measure the semantic relevance between

generated responses and ground truths. Besides,

we also adopt Distinct-1 and Distinct-2 (Li et al.,

2016) to evaluate the diversity of responses.

Human Evaluation In order to assess how well

the models can maintain the replier’s consistency,

we conduct a human evaluation. Specifically, we

randomly sample 300 context from the test set and

apply 5 models to generate responses for each con-

text. For each response, three annotators are re-

4
We use the embeddings trained on Google News Corpus: https://

code.google.com/archive/p/word2vec/.



1925

Corpus Train Valid Test Avg. Utterances Avg. Words Vocab Coverage

Cornell 91271 871 702 5.04 16.91 10000 98.26%

Ubuntu 448833 19584 18920 4.94 23.67 20000 99.12%

Table 1: Statistical information including number of dialogs in training, validation and test sets, average number

of utterances (turns) per dialog, words per utterance, vocabulary size and its proportion in corpus.

Model
Cornell Movie Dialogs Corpus Ubuntu Dialogue Corpus

Average Greedy Extreme Distinct-1 Distinct-2 Average Greedy Extreme Distinct-1 Distinct-2

S2SA 0.1979 0.1537 0.1373 19/.0013 37/.0092 0.2156 0.1688 0.1265 1543/.0047 5342/.0260
HRED 0.4964 0.3824 0.3317 27/.0068 56/.0171 0.5415 0.4117 0.3193 1924/.0084 8549/.0409
VHRED 0.5148 0.3954 0.3446 126/.0216 329/.0642 0.5341 0.4027 0.3062 2928/.0151 13468/.0772
HVMN 0.5347 0.3876 0.3462 199/.0399 474/.1107 0.5584 0.4229 0.3220 6334/.0193 25136/.1005
SSVN 0.6417 0.4582 0.3732 591/.0535 2535/.2450 0.6089 0.4433 0.3312 9562/.0259 62539/.1955

Table 2: Automatic evaluation results on two dialogue datasets. The Distinct-* contains the number of distinct

n-grams and its ratio over all generated responses. The embedding-based metrics results of baselines on Ubuntu

are the same as Chen et al. (2018).

cruited to give a 4-graded judgement with the fol-

lowing criteria: 1: the response is ungrammati-

cal or semantically irrelevant; or inconsistent with

replier’s features (e.g., linguistic characteristics,

sentiments and personalities); or has wrong logic;

2: the response is semantically weak related, but

it is too trivial (e.g., “I don’t know”); 3: the re-

sponse is semantically relevant and informative,

but has no obvious consistency about the replier’s

personal features; 4: the response is not only se-

mantically related and informative, but also con-

sistent with the individual features of replier.

4.5 Evaluation Results

Automatic Evaluation The metric-based evalua-

tion results are shown in Table 2. From the results,

we can observe that:

(1) HRED performs better than S2SA, indicat-

ing that the hierarchical structure is benefical.

(2) VHRED outperforms HRED on all met-

rics on Cornell, which demonstrates that the latent

variables are the useful global guidance informa-

tion. Inversely, VHRED has a worse performance

than HRED in terms of three embedding-based

metrics on Ubuntu, which is consistent with Chen

et al. (2018) due to the domain specific dataset.

(3) On the top of VHRED, HVMN introduces

the memory network to enhance the long-term

memory, and obtains the best performance among

the baseline models.

(4) Compared with all the baselines, our SSVN

model achieves the highest scores in terms of all

metrics on two datasets, indicating that SSVN can

best fit the ground truth semantically and generate

more informative responses. Meanwhile, the sign

tests show that the improvements of SSVN are sta-

tistically significant (p-value<0.01).

(5) Noticeably, the models trained on Ubuntu

consistently have more distinct n-grams than the

same models trained on Cornell, while the dis-

tinct ratios do not differ much. The reason is that

Ubuntu dataset has more words averagely per ut-

terance than Cornell data (as the statistical details

shown in Table 1), which forces the models to pro-

duce longer responses.

Human Evaluation The human evaluation results

on Cornell data are shown in Table 4, in which the

score distribution values represent the percentages

of responses belonging to each grade, and Fleiss’

kappa (Fleiss and Cohen, 1973) is employed to

evaluate the inter-annotator agreement. From the

results, we have the following observations:

(1) The percentage of replier-specific responses

(i.e., the grade ‘4’) of SSVN model is 22.69%,
which is much higher than that of baselines, in-

dicating that the personal feature extractor can ef-

fectively capture the personal feature of replier.

(2) SSVN model generates much more informa-

tive responses (i.e., 71.56% labeled as ‘3+4’) and
much less generic responses (i.e., 20.28% labeled
as ‘2’) than all the baselines. The results are in line

with the above results of metric-based evaluation.

(3) Kappa scores of the models are all higher

than 0.4, demonstrating that the annotators come

to a fair agreement. Meanwhile, the sign tests

also show that the human evaluation improve-

ments of SSVN to baselines are significant on Cor-

nell dataset (p-value<0.01).



1926

Model Extractor Generator Average Greedy Extreme Distinct-1 Distinct-2

SVN – vMF 0.6108 0.4334 0.3456 403/.0374 1376/.1367
SSVNGau Gau Gau 0.5563 0.4164 0.3406 470/.0703 1859/.3109
SSVNGau−E Gau vMF 0.6164 0.4371 0.3648 472/.0439 2100/.2091
SSVNGau−G vMF Gau 0.5605 0.4195 0.3467 525/.0764 2097/.3403
SSVN vMF vMF 0.6417 0.4582 0.3732 591/.0535 2535/.2450

Table 3: Performances of model ablation on Cornell dataset. The ‘Gau’ and ‘vMF’ denote the distributions of the

latent spaces of extractor or generator, and ‘–’ means no extractor.

Model
score distribution (%)

Kappa
1 2 3 4 3+4

S2SA 29.38 45.11 20.47 5.04 25.51 0.41
HRED 26.31 39.38 26.2 8.11 34.31 0.44
VHRED 17.58 36.48 33.55 12.39 45.94 0.46
HVMN 14.03 29.79 39.47 16.71 56.18 0.50
SSVN 8.16 20.28 48.87 22.69 71.56 0.45

Table 4: Human evaluation results on Cornell dataset.

The percentages are calculated by combining the

judgements from three annotators together. ‘3+4’

stands for the sum of percentages of the grade ‘3’ and

‘4’ (i.e., the ratio of informative responses).

4.6 Discussions

Model Ablation To investigate the effect of dif-

ferent parts, we conduct a set of experiments on

Cornell by removing the extractor or modifying

the distribution of extractor and generator. From

the results listed in Table 3, we can observe that:

(1) Removing the extractor (denoted as SVN)

makes the distinct ratios and numbers drop

dramatically, while the embedding-based met-

ric scores are only slightly lower than that of

SSVN. This indicates the personal features learned

by the extractor not only maintain the replier-

consistency, but also improve the diversity of re-

sponses. In addition, SSVNGau−E (replacing the

vMF distribution with a Gaussian in extractor) has

a better performance than SVN, but a worse one

than SSVN, demonstrating the vMF-Extractor is

more effective than Gau-Extractor.

(2) As for the generator, when setting the Gaus-

sian as the latent space (denoted as SSVNGau−G),

the embedding-based performance deteriorates

dramatically whereas the distinct numbers de-

crease slightly, indicating that the vMF-Generator

is more capable of facilitating the generated re-

sponses semantically close to the ground truth

than Gau-Generator. Notably, the distinct ratios

in SSVNGau−G rise unexpectedly, which will be

investigated in (3).

(3) To figure out this special phenomenon, we

conduct an experiment on SSVNGau composed

by Gau-Extractor and Gau-Generator. We can

see that the SSVNGau−G and SSVNGau obtain

the best distinct ratios among the ablation mod-

els, but their distinct numbers are not the high-

est. The results indicate that, whatever the latent

space of extractor follows, the Gau-Generator al-

ways tends to produce informative but very short

responses. Meanwhile, their worst embedding-

based scores show that the responses generated

by Gau-Generator semantically deviate from the

ground truth significantly.

The Effect of vMF on KL Besides the metric-

based performance, we also evaluate the effective-

ness of different settings in sloving the latent space

futility problem. Figure 2 visualizes the evolu-

tion of the KL loss in both extractor and generator

parts. We can see that:

22.0
 

23.0
 

24.0
 

25.0

Ex
tra

ct
or

 K
L

��� SSVNGau SSVNGau− E SSVNGau−G ����

0 2 4 6 8 10
Epochs

0.0
0.1
0.2
0.3
0.4
0.5
0.6

74.0
 

76.0
 

78.0
 

80.0

Ge
ne

ra
to

r K
L

0 2 4 6 8 10
Epochs

2.0
 

6.0
 

10.0
 

14.0

Figure 2: KL cost of different parts in ablation models

as training progress.

(1) In Extractor KL, Gau-Extractors (i.e.,

SSVNGau and SSVNGau−E) have a KL cost close

to 0 at the begining and never recover, while

the vMF-Extractors (i.e., SSVNGau−G and SSVN)

can keep a constant KL value as evidenced by

Eq.(6). The results indicate that the vMF in ex-

tractor can mitigate KL-vanishing and capture the

more meaningful personal features.

(2) Surprisingly, in Generator KL, the KL loss

presents an upward trend in Gau-Generators (i.e.,

SSVNGau and SSVNGau−G). The reason is that,

the personal features from the extractor can effec-

tively strengthen the expressiveness of the latent

space in the generator, thus the response decoder



1927

is encouraged to exploit the latent variables and

the latent space futility problem is alleviated.

(3) Compared with the Gau-Generators in

Generator KL, the vMF-Generators (i.e., SVN,

SSVNGau−E and SSVN) have the much higher

KL values, indicating that the vMF is a better se-

lection than Gaussian to solve the KL-vanishing

problem. Meanwhile, the KL values are relatively

stable, which experimentally demonstrates the KL

cost mainly depends on the last term in Eq.(14)

and the variable term has little effect on it. Last

but not least, KL cost can explictly be changed by

setting different kappa values.

Impact of the Coefficient λ Recall that Eq.(18)

shows the capacity of SSVN in balancing response

generation and personality reconstruction. Here

we analyze the effects of different coefficient λ on
the quality of responses. Figure 3 shows the per-

formances given varying λ. Notably, the perfor-
mances of embedding-based metrics are changing

in a similar trend, as the same case in Distinct-1

and Distinct-2, thus we only consider Average and

Distinct-1 as the major analysis items.

0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0
�

0.0

0.1

0.2

0.3

0.4

0.5

0.6

0.7

0.8

0.383

0.508
0.555 0.568

0.629 0.633 0.633 0.642
0.589

0.560
0.516

0.231

0.382 0.389
0.425

0.449 0.447 0.450 0.458 0.429 0.409
0.380

0.208

0.294 0.309
0.357 0.361 0.368 0.372 0.373 0.357 0.339

0.310

0.125 0.103 0.082
0.048 0.048 0.052 0.053 0.053 0.067

0.097 0.117

0.436 0.424

0.350

0.216 0.206 0.218
0.241 0.245

0.340 0.355

0.426

generation adynamic mutual promotion reconstruction rout 

��
���
 ��

	� ����

 ���������� ����������

Figure 3: Influences of different λ on Average, Greedy,
Extreme, Distinct-1 and Distinct-2.

As we can see, the evolutions of Average and

Distinct-1 in Figure 3 can be broadly into three

stages: generation adynamic stage, mutual promo-

tion stage and reconstruction rout stage.

(1) The first stage shows that as λ increases,
the Average monotonically increases while diver-

sity decreases. This is because the lower λ gives
the model less incentive to optimize the generator,

which makes the response decoder incapable of

utilizing the higher-quality personal features, re-

sulting in the diverse but semantically inappropri-

ate responses.

(2) When λ moves to the second stage, the per-
formances of the Average and diversity improve

simultaneously, implying that the response gener-

ation and personality reconstruction achieve en ex-

pected balance.

(3) For the reconstruction rout stage, although

the model focuses on response generation, the

larger λ does not bring an improvement of Av-
erage, but instead increases diversity. The result

indicates that the unprofitable personal features in

thwarted personality reconstruction part, as a ran-

dom disturbance, can increase the diversity of the

responses, but severely bias the generation of re-

sponse decoder semantically.

Observed from the curves of all metrics, the

best performance of embedding-based metrics is

achieved at λ = 0.7, while the diversity reaches
the peak in the mutual promotion stage. Thus, we

set λ to 0.7 in all previous experiments.

Case Study Besides the quantitative analysis, we

also organize some examples (seen in Table 5)

from different models to analyze the performances

of the methods qualitatively. They are chosen ran-

domly from the responses produced by the pro-

posed model, and showed together with the corre-

sponding contexts and the outputs of the baselines.

From the case 1, we can observe that the SSVN

can extract the personal feature of the replier that

the speaker A prefers to acquire further informa-

tion from others, which guides the generator to

produce an interrogative response to promote the

replier-consistency. Meanwhile, the SSVN can

also extract the firm attitude of the replier in case

2 and the pleading tone of the replier in case 3.

By contrast, the baselines favor to produce the bad

responses, such as containing more ‘unk’.

Error Analysis To improve the performance of

SSVN in the future, we take the worse cases (i.e.,

the grade ‘1’ and ‘2’) in human judgement as an

example to analyze our errors. Specifically, we di-

vide the errors with the grade ‘1’ into grammatical

error, replier-nonconsistency and logic contradic-

tion, which occupy 19.79%, 31.49% and 48.72%,
respectively. We can find that 1) logic condradic-

tion scenes make up most of the errors as SSVN

pays little attention to this issue. 2) although con-

sidering the personal features from replier, there

still exists 31.49% replier-inconsistent cases, inci-
dating that only strengthening the VAE with the

vMF distribution may not be a perfect approach

for personal feature extraction. As for grade ‘2’,

the consistency of replier’s features improve the

of response diversity significantly, but the model

still has “safe response” problem as the baseline.



1928

ID Context Response

1

A: Where are we going?
B: We’re getting out of this place. We’ve done enough.
ground truth: Wait a minute, whoa. We can’t go now.
What’s 〈unk〉 got planned with that nerve agent?

SSVN: When we’re going out?
S2SA: We know he is on you.
HRED: I’ll 〈unk〉 there’s no 〈unk〉, or there’s no this case.
〈unk〉 〈unk〉 well, because he is the 〈unk〉.
VHRED: It motel yet .
HVMN: What kind of 〈unk〉 were you?

2

A: There.
B: I beg your pardon?
A: There wolf! There castle!
B: Why are you talking like that?
ground truth: I thought you wanted to.

SSVN: Well, I want you to get out of here!
S2SA: Me 〈unk〉.,
HRED: So how do you believe that?
VHRED: Fourth 〈unk〉 she wasn’t 〈unk〉!
HVMN: Old 〈unk〉. These 〈unk〉 is 〈unk〉 – 〈unk〉 〈unk〉.
〈unk〉 years ...

3

A: James ...
B: I have to go.
A: Then take me with you.
B: No. You’ll be safe here.
ground truth: I don’t want to be safe!

SSVN: Please, I have to go. I can’t take a walk.
S2SA: Tell you now 〈unk〉 something of 〈unk〉, doesn’t
〈unk〉!
HRED: 〈unk〉 just actually, the 〈unk〉 〈unk〉 of making
the 〈unk〉 〈unk〉. She’s dead.
VHRED: Gross ...
HVMN: No.

Table 5: Generated responses of all the models on Cornell Movie Dialogs Corpus.

The above analysis sheds light on our future di-

rections: 1) modeling the logic consistency be-

tween the context and response; 2) exploring ad-

vanced methods for extracting personal features;

3) improving the response diversity.

5 Conclusion and Future Work

In this work, we propose a semi-supervised sta-

ble variational network for addressing the latent

space futility and replier-consistency decay issues.

Different from the traditional variational dialog

models, the proposed model selects the vMF as

the prior and posterior to resolve the latent space

futility issue, and then integrates a unsupervised

extractor to obtain the replier-tailored personal

features to ensure the replier-consistency. Ex-

perimental results on two dialog datasets demon-

strate the effectiveness of our model, especially on

replier-consistency in terms of human evaluation.

However, the error analysis shows that there are

still challenges in dialogue generation, which we

would like to explore in the future.

Acknowledgments

We thank the anonymous reviewers for their valu-

able feedback. Our work is supported by National

Natural Science Foundation of China (61976154),

Tianjin Natural Science Foundation (18JCY-

BJC15500), National Natural Science Foundation

of China (61771333), Tianjin Municipal Science

and Technology Project (18ZXZNGX00330), and

the Foundation of State Key Laboratory of Cogni-

tive Intelligence, iFLYTEK(CIOS-20190001).

References

Martn Abadi, Ashish Agarwal, Paul Barham, Eu-
gene Brevdo, and Xiaoqiang Zheng. 2016. Ten-
sorflow: Large-scale machine learning on heteroge-
neous distributed systems. arXiv:1603.04467[cs],
arXiv:1603.04467.

Rich Caruana, Steve Lawrence, and C. Lee Giles. 2000.
Overfitting in neural nets: Backpropagation, conju-
gate gradient, and early stopping. In Advances in
neural information processing systems 13 (NIPS),
pages 402–408.

Hongshen Chen, Xiaorui Liu, Dawei Yin, and Jiliang
Tang. 2017a. A survey on dialogue systems: Recent
advances and new frontiers. ACM SIGKDD Explo-
rations Newsletter, 19(2):25–35.

Hongshen Chen, Zhaochun Ren, Jiliang Tang, Yi-
hong Eric Zhao, and Dawei Yin. 2018. Hierarchi-
cal variational memory network for dialogue gener-
ation. In Proceedings of the 2018 World Wide Web
Conference (WWW ’18), pages 1653–1662.

Xi Chen, Diederik P. Kingma, Tim Salimans, Yan
Duan, Prafulla Dhariwal, John Schulman, Ilya
Sutskever, and Pieter Abbeel. 2017. Variational
lossy autoencoder. In Proceedings of the Inter-
national Conference on Learning Representations
(ICLR).

Cristian Danescu-Niculescu-Mizil and Lillian Lee.
2011. Chameleons in imagined conversations: A
new approach to understanding coordination of lin-
guistic style in dialogs. In Proceedings of the 2nd
Workshop on Cognitive Modeling and Computa-
tional Linguistics (CMCL ’11), pages 76–87.

Tim R. Davidson, Luca Falorsi, Nicola De Cao,
Thomas Kipf, and Jakub M. Tomczak. 2018.
Hyperspherical variational auto-encoders.
arXiv:1804.00891[cs], arXiv:1804.00891.



1929

Joseph L Fleiss and Jacob Cohen. 1973. The equiv-
alence of weighted kappa and the intraclass corre-
lation coefficient as measures of reliability. Educa-
tional and psychological measurement, 33(3):613–
619.

Barbara J. Grosz. 2016. Ai100 report.
https://ai100.stanford. edu/2016-report.

Kelvin Guu, Tatsunori B. Hashimoto, Yonatan Oren,
and Percy Liang. Generating sentences by edit-
ing prototypes. Transactions of the Association for
Computational Linguistics (TACL), 6:437–450.

D. P. Kingma, D. J. Rezende, S. Mohamed, and
M. Welling. 2014. Semi-supervised learning with
deep generative models. In Advances in Neural
Information Processing Systems 27 (NIPS), pages
3581–3589.

Diederik Kingma and Jimmy Ba. 2014. Adam: A
method for stochastic optimization. In Proceedings
of the 3rd International Conference for Learning
Representations (ICLR).

Diederik P. Kingma, Tim Salimans, Rafal Jozefowicz,
and Xi Chen. 2016. Improved variational inference
with inverse autoregressive flow. In Advances in
Neural Information Processing Systems 29 (NIPS),
pages 4743–4751.

Diederik P Kingma and Max Welling. 2014. Auto-
encoding variational bayes. In Proceedings of the
International Conference on Learning Representa-
tions (ICLR).

Jiwei Li, Michel Galley, Chris Brockett, Jianfeng Gao,
and Bill Dolan. 2016. A diversity-promoting ob-
jective function for neural conversation models. In
Proceedings of Annual Conference of the North
American Chapter of the Association for Computa-
tional Linguistics: Human Language Technologies
(NAACL-HLT), pages 110–119.

Chia Wei Liu, Ryan Lowe, Iulian V. Serban, Michael
Noseworthy, Laurent Charlin, and Joelle Pineau.
2016. How not to evaluate your dialogue system:
An empirical study of unsupervised evaluation met-
rics for dialogue response generation. In Proceed-
ings of the 2016 Conference on Empirical Methods
in Natural Language Processing (EMNLP), pages
2122–2132.

Ryan Lowe, Nissan Pow, Iulian V. Serban, and Joelle
Pineau. 2015. The ubuntu dialogue corpus: A large
dataset for research in unstructured multi-turn dia-
logue systems. In Proceedings of the 16th Annual
Meeting of the Special Interest Group on Discourse
and Dialogue (SIGDIAL), pages 285–294.

Danilo Jimenez Rezende and Shakir Mohamed. 2015.
Variational inference with normalizing flows. In
Proceedings of the 32nd International Conference
on Machine Learning (ICML), pages 1530–1538.

Danilo Jimenez Rezende, Shakir Mohamed, and Daan
Wierstra. 2014. Stochastic backpropagation and ap-
proximate inference in deep generative models. In
Proceedings of the 31nd International Conference
on Machine Learning (ICML), pages 1278–1286.

Mike Schuster and Kuldip K Paliwal. 1997. Bidirec-
tional recurrent neural networks. IEEE Transactions
on Signal Processing, 45(11):2673–2681.

Stanislau Semeniuta, Aliaksei Severyn, and Erhardt
Barth. 2017. A hybrid convolutional variational au-
toencoder for text generation. In Proceedings of the
2017 Conference on Empirical Methods in Natural
Language Processing (EMNLP), pages 627–637.

Iulian Serban, Alessandro Sordoni, Ryan Joseph Lowe,
Laurent Charlin, Joelle Pineau, Aaron C. Courville,
and Yoshua Bengio. 2017. A hierarchical la-
tent variable encoder-decoder model for generat-
ing dialogues. In Proceedings of the Thirty-First
AAAI Conference on Artificial Intelligence (AAAI-
17), pages 3295–3301.

Iulian Vlad Serban, Alessandro Sordoni, Yoshua Ben-
gio, Aaron Courville Courville, and Joelle Pineau.
2016. Building end-to-end dialogue systems using
generative hierarchical neural network models. In
Proceedings of the Thirtieth AAAI Conference on
Artificial Intelligence (AAAI-16), pages 3776–3784.

Lifeng Shang, Zhengdong Lu, and Hang Li. 2015.
Neural responding machine for short-text conver-
sation. In Proceedings of the 53rd Annual Meet-
ing of the Association for Computational Linguistics
(ACL), pages 1577–1586.

Xiaoyu Shen, Hui Su, Shuzi Niu, and Vera Demberg.
2018. Improving variational encoder-decoders in
dialogue generation. In Proceedings of the Thirty-
Second AAAI Conference on Artificial Intelligence
(AAAI-18), pages 5456–5463.

Kihyuk Sohn, Honglak Lee, and Xinchen Yan. 2015.
Learning structured output representation using
deep conditional generative models. In Advances in
Neural Information Processing Systems 28 (NIPS),
pages 3483–3491.

Alessandro Sordoni, Michel Galley, Michael Auli,
Chris Brockett, Yangfeng Ji, Margaret Mitchell,
Jian-Yun Nie, Jianfeng Gao, and Bill Dolan. 2015.
A neural network approach to context-sensitive gen-
eration of conversational responses. In Proceed-
ings of the 2015 Conference Annual Conference of
the North American Chapter of the Association for
Computational Linguistics: Human Language Tech-
nologies (NAACL-HLT), pages 196–205.

Ilya Sutskever, Oriol Vinyals, and Quoc V Le. 2014.
Sequence to sequence learning with neural net-
works. In Advances in Neural Information Process-
ing Systems 27 (NIPS), pages 3104–3112.

Oriol Vinyals and Quoc V. Le. 2015. A neural conver-
sational model. In Proceedings of the 31st Interna-
tional Conference on Machine Learning (ICML).



1930

Andrew Wood. 1994. Simulation of the von mises
fisher distribution. Communications in Statistics
Simulation and Computation, 23(1):157–164.

Ziang Xie, Sida I. Wang, Jiwei Li, Daniel Levy, Aim-
ing Nie, Dan Jurafsky, and Andrew Y. Ng. 2017.
Data noising as smoothing in neural network lan-
guage models. In Proceedings of the International
Conference on Learning Representations (ICLR).

Jiacheng Xu and Greg Durrett. 2018. Spherical latent
spaces for stable variational autoencoders. In Pro-
ceedings of the 2018 Conference on Empirical Meth-
ods in Natural Language Processing (EMNLP),
pages 4503–4513.

Zichao Yang, Zhiting Hu, Ruslan Salakhutdinov, and
Taylor Berg-Kir. 2017. Improved variational au-
toencoders for text modeling using dilated convolu-
tions. In Proceedings of the 34th International Con-
ference on Machine Learning (ICML), pages 3881–
3890.

Kaisheng Yao, Geoffrey Zweig, and Baolin Peng.
2015. Attention with intention for a neural network
conversation model. ArXiv, abs/1510.08565.

Saizheng Zhang, Emily Dinanz, Jack Urbanekz, Arthur
Szlamz, Douwe Kielaz, and Jason Weston. 2018.
Personalizing dialogue agents: I have a dog, do you
have pets too? In Proceedings of the 56th Annual
Meeting of the Association for Computational Lin-
guistics (ACL), pages 1–10.

Tiancheng Zhao, Ran Zhao, and Maxine Eskenazi.
2017. Learning discourse-level diversity for neural
dialog models using conditional variational autoen-
coders. In Proceedings of the 55th Annual Meet-
ing of the Association for Computational Linguistics
(ACL), pages 654–664.


