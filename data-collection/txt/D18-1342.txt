



















































Breaking the Beam Search Curse: A Study of (Re-)Scoring Methods and Stopping Criteria for Neural Machine Translation


Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 3054–3059
Brussels, Belgium, October 31 - November 4, 2018. c©2018 Association for Computational Linguistics

3054

Breaking the Beam Search Curse: A Study of (Re-)Scoring Methods and
Stopping Criteria for Neural Machine Translation

Yilin Yang1 Liang Huang1,2 Mingbo Ma1,2
1Oregon State University

Corvallis, OR, USA
{yilinyang721, liang.huang.sh, cosmmb}@gmail.com

2Baidu Research
Sunnyvale, CA, USA

Abstract

Beam search is widely used in neural ma-
chine translation, and usually improves trans-
lation quality compared to greedy search. It
has been widely observed that, however, beam
sizes larger than 5 hurt translation quality. We
explain why this happens, and propose sev-
eral methods to address this problem. Fur-
thermore, we discuss the optimal stopping cri-
teria for these methods. Results show that
our hyperparameter-free methods outperform
the widely-used hyperparameter-free heuristic
of length normalization by +2.0 BLEU, and
achieve the best results among all methods on
Chinese-to-English translation.

1 Introduction

In recent years, neural machine translation (NMT)
has surpassed traditional phrase-based or syntax-
based machine translation, becoming the new state
of the art in MT (Kalchbrenner and Blunsom,
2013; Sutskever et al., 2014; Bahdanau et al.,
2014). While NMT training is typically done
in a “local” fashion which does not employ any
search (bar notable exceptions such as Ranzato
et al. (2016), Shen et al. (2016), and Wiseman
and Rush (2016)), the decoding phase of all NMT
systems universally adopts beam search, a widely
used heuristic, to improve translation quality.

Unlike phrase-based MT systems which enjoy
the benefits of very large beam sizes (in the or-
der of 100–500) (Koehn et al., 2007) , most NMT
systems choose tiny beam sizes up to 5; for exam-
ple, Google’s GNMT (Wu et al., 2016) and Face-
book’s ConvS2S (Gehring et al., 2017) use beam
sizes 3 and 5, respectively. Intuitively, the larger
the beam size is, the more candidates it explores,
and the better the translation quality should be.
While this definitely holds for phrase-based MT
systems, surprisingly, it is not the case for NMT:

many researchers observe that translation qual-
ity degrades with beam sizes beyond 5 or 10 (Tu
et al., 2017; Koehn and Knowles, 2017). We call
this phenomenon the “beam search curse”, which
is listed as one of the six biggest challenges for
NMT (Koehn and Knowles, 2017).

However, there has not been enough attention
on this problem. Huang et al. (2017) hint that
length ratio is the problem, but do not explain why
larger beam sizes cause shorter lengths and worse
BLEU. Ott et al. (2018) attribute it to two kinds
of “uncertainties” in the training data, namely the
copying of source sentence and the non-literal
translations. However, the first problem is only
found in European language datasets and the sec-
ond problem occurs in all datasets but does not
seem to bother pre-neural MT systems. Therefore,
their explanations are not satisfactory.

On the other hand, previous work adopts several
heuristics to address this problem, but with vari-
ous limitations. For example, RNNSearch (Bah-
danau et al., 2014) and ConvS2S use length nor-
malization, which (we will show in Sec. 6) seems
to somewhat alleviate the problem, but far from
being perfect. Meanwhile, He et al. (2016) and
Huang et al. (2017) use word-reward, but their re-
ward is a hyper-parameter to be tuned on dev set.

Our contributions are as follows:

• We explain why the beam search curse exists,
supported by empirical evidence (Sec. 3).

• We review existing rescoring methods, and
then propose ours to break the beam
search curse (Sec. 4). We show that our
hyperparameter-free methods outperfrom the
previous hyperparameter-free method (length
normalization) by +2.0 BLEU (Sec. 6).

• We also discuss the stopping criteria for our
rescoring methods (Sec. 5). Experiments



3055

show that with optimal stopping alone, the
translation quality of the length normaliza-
tion method improves by +0.9 BLEU.

After we finish our paper, we became aware of a
parallel work (Murray and Chiang, 2018) that also
reveals the same root cause we found for the beam
search curse: the length ratio problem.

2 Preliminaries: NMT and Beam Search

We briefly review the encoder-decoder architec-
ture with attention mechanism (Bahdanau et al.,
2014). An RNN encoder takes an input sequence
x = (x1, ..., xm), and produces a sequence of
hidden states. For each time step, the RNN de-
coder will predict the probability of next output
word given the source sequence and the previously
generated prefix. Therefore, when doing greedy
search, at time step i, the decoder will choose the
word with highest probability as yi. The decoder
will continue generating until it emits </eos>. In the
end, the generated hypothesis is y = (y1, ..., yn)
with yn=</eos>, with model score

S (x,y) =
∑|y|

i=1 log p(yi | x, y1..{i−1}) (1)
As greedy search only explores a single path,

we always use beam search to improve search
quality. Let b denote the beam size, then at step
i the beam Bi is an ordered list of size b:

B0=[〈<s>, p(<s> | x)〉]

Bi=
b

top{〈y′◦ yi, s·p(yi|x,y)〉 | 〈y′, s〉 ∈ Bi−1}

In the most naive case, after reaching the maxi-
mum length (a hard limit), we get N possible can-
didate sequences {y1, ...,yN}. The default strat-
egy chooses the one with highest model score. We
will discuss more sophistcated ways of stopping
and choosing candidates in later sections.

3 Beam Search Curse

The most popular translation quality metric,
BLEU (Papineni et al., 2002), is defined as:

BLEU = bp · exp(1/4∑4n=1 log pn) (2)
where bp = min{e1−1/lr , 1} (3)
where lr = |y|/|y∗| (4)

Here pn are the n-gram precisions, and |y| and
|y∗| denote the hypothesis and reference lengths,
while bp is the brevity penalty (penalizing short

0 20 40 60 80 100
beam size

31

32

33

34

35

36

37

BL
EU

 sc
or

e

0.70

0.75

0.80

0.85

0.90

0.95

1.00

le
ng

th
 ra

tio
 / 

br
ev

ity
 p

en
al

ty

BLEU

length ratio lr = |y||y∗|

brevity penalty bp = min{e1−1/lr , 1}

Figure 1: As beam size increases beyond 3, BLEU
score on the dev set gradually drops. All terms are cal-
culated by multi-bleu.pl.

0 50 100 150 200 250 300 350
beam size

5

10

15

20

25

30

35

40

av
er

ag
e 

</
eo

s>
 p

os
iti

on

3rd </eos>
2nd </eos>
1st </eos>

Figure 2: Searching algorithm with larger beams gen-
erates </eos> earlier. We use the average first, second
and third </eos> positions on the dev set as an example.

translations) and lr is the length ratio (Shi et al.,
2016; Koehn and Knowles, 2017), respectively.

With beam size increasing, |y| decrases, which
causes the length ratio to drop, as shown in Fig. 1.
Then the brevity penalty term, as a function of the
length ratio, decreases even more severely. Since
bp is a key factor in BLEU, this explains why the
beam search curse happens.1

The reason why |y| decreases as beam size in-
creases is actually twofold:

1. As beam size increases, the more candidates
it could explore. Therefore, it becomes eas-
ier for the search algorithm to find the </eos>
symbol. Fig. 2 shows that the </eos> indices
decrease steadily with larger beams.2

1The length ratio is not just about BLEU: if the hypoth-
esis length is only 75% of reference length, something that
should have been translated must be missing; i.e., bad ade-
quacy. Indeed, Murray and Chiang (2018) confirm the same
phenomenon with METEOR.

2Pre-neural SMT models, being probabilistic, also favor
short translations (and derivations), which is addressed by
word (and phrase) reward. The crucial difference between
SMT and NMT is that the former stops when covering the
whole input, while the latter stops on emitting </eos>.



3056

18 20 22 24 26 28 30
length

20

18

16

14

12

10

8
m

od
el

 sc
or

e

Figure 3: Candidate lengths vs. model score. This scat-
ter plot is generated from 242 finished candidates when
translated from one source sequence with beam size 80.

2. Then, as shown in Fig. 3, shorter candidates
have clear advantages w.r.t. model score.

Hence, as beam size increases, the search algo-
rithm will generate shorter candidates, and then
prefer even shorter ones among them.3

4 Rescoring Methods

We first review existing methods to counter the
length problem and then propose new ones to ad-
dress their limitations. In particular, we propose to
predict the target length from the source sentence,
in order to choose a hypothesis with proper length.

4.1 Previous Rescoring Methods

RNNSearch (Bahdanau et al., 2014) first intro-
duces the length normalization method, whose
score is simply the average model score:

Ŝlength norm(x,y) = S (x,y)/|y| (5)

This is the most widely used rescoring method
since it is hyperparameter-free.

GNMT (Wu et al., 2016) incorporates length
and coverage penalty into the length normalization
method, while also adding two hyperparameters to
adjust their influences. (please check out their pa-
per for exact formulas).

Baidu NMT (He et al., 2016) borrows the Word
Reward method from pre-neural MT, which gives
a reward r to every word generated, where r is a
hyperparameter tuned on the dev set:

ŜWR(x,y) = S (x,y) + r · |y| (6)
3Murray and Chiang (2018) attribute the fact that beam

search prefers shorter candidates to the label bias problem
(Lafferty et al., 2001) due to NMT’s local normalization.

Based on the above, Huang et al. (2017) propose
a variant called Bounded Word-Reward which
only rewards up to an “optimal” length. This
length is calculated using a fixed “generation ra-
tio” gr , which is the ratio between target and
source sequence length, namely the average num-
ber of target words generated for each source
word. It gives reward r to each word up to a
bounded length L(x,y) = min{|y|, gr · |x|}:

ŜBWR(x,y) = S (x,y) + r · L(x,y) (7)

4.2 Rescoring with Length Prediction

To remove the fixed generation ratio gr from
Bounded Word-Reward, we use a 2-layer MLP,
which takes the mean of source hidden states as
input, to predict the generation ratio gr∗(x). Then
we replace the fixed ratio gr with it, and get our
predicted length Lpred (x) = gr∗(x) · |x|.

4.2.1 Bounded Word-Reward
With predicted length, the new predicted bound
and final score would be:

L∗(x,y) = min{|y|, Lpred (x)} (8)
ŜBWR∗(x,y) = S (x,y) + r · L∗(x,y) (9)

While the predicted length is more accurate, there
is still a hyperparameter r (word reward), so we
design two methods below to remove it.

4.2.2 Bounded Adaptive-Reward
We propose Bounded Adaptive-Reward to auto-
matically calculate proper reward based on the
current beam. With beam size b, the reward for
time step t is the average negative log-probability
of the words in the current beam.

rt = −(1/b)
∑b

i=1 log p(wordi) (10)

Its score is very similar to (7):

ŜAdaR(x,y) = S (x,y) +
∑L∗

t=1 rt (11)

4.2.3 BP-Norm
Inspired by the BLEU score definition, we propose
BP-Norm method as follows:

Ŝbp(x,y) = log bp + S (x,y)/|y| (12)

bp is the same brevity penalty term as in (3). Here,
we regard our predicted length as the reference



3057

length. The beauty of this method appears when
we drop the logarithmic symbol in (12):

exp(Ŝbp(x,y))=bp ·
(∏|y|

i=1 p(yi|...)
)1/|y|

=bp ·exp
(

1

|y|
∑|y|

i=1 log p(yi|...)
)

which is in the same form of BLEU score (3).

5 Stopping Criteria

Besides rescoring methods, the stopping criteria
(when to stop beam search) is also important, for
both efficiency and accuracy.

5.1 Conventional Stopping Criteria

By default, OpenNMT-py (Klein et al., 2017)
stops when the topmost beam candidate stops, be-
cause there will not be any future candidates with
higher model scores. However, this is not the
case for other rescoring methods; e.g., the score
of length normalization (5) could still increase.

Another popular stopping criteria, used by
RNNSearch (Bahdanau et al., 2014), stops the
beam search when exactly b finished candidates
have been found. Neither method is optimal.

5.2 Optimal Stopping Criteria

For Bounded Word-Reward, Huang et al. (2017)
introduces a provably-optimal stopping criterion
that could stop both early and optimally. We also
introduce an optimal stopping criterion for BP-
Norm. Each time we generate a finished candi-
date, we update our best score Ŝ ?. Then, for the
topmost beam candidate of time step t, we have:

Ŝbp =
St,0
t

+min{1− Lpred
t

, 0} ≤ St,0
R

(13)

where R is the maximum generation length. Since
St,0 will drop after time step t, if

St,0
R ≤ Ŝ ?, we

reach optimality. This stopping criterion could
also be applied to length normalization (5).

Meawhile, for Bounded Adaptive-Reward, we
can have a similar optimal stopping criterion: If
the score of topmost beam candidate at time step
t > Lpred is lower than Ŝ ?, we reach optimality.

Proof. The first part of ŜAdaR in (11) will decrease
after time step t, while the second part stays the
same when t > Lpred . So the score in the future
will monotonically decrease.

0 5 10 15 20 25 30 35 40
beam size

34

35

36

37

38

39

40

BL
EU

 sc
or

e

Bounded Adaptive-Reward
Bounded Word-R. w/ Pred. Length
BP-Norm
Length Norm. w/ Optimal Stopping
Length Norm.
Default

0 5 10 15 20 25 30 35 40
beam size

0.84

0.86

0.88

0.90

0.92

0.94

0.96

0.98

1.00

le
ng

th
 ra

tio

Bounded Adaptive-Reward
Bounded Word-R. w/ Pred. Length
BP-Norm
Length Norm. w/ Optimal Stopping
Length Norm.
Default

Figure 4: The BLEU scores and length ratios (lr =
|y|/|y∗|) of various rescoring methods.

6 Experiments

Our experiments are on Chinese-to-English trans-
lation task, based on the OpenNMT-py codebase.4

We train our model on 2M sentences, and ap-
ply BPE (Sennrich et al., 2015) on both sides,
which reduces Chinese and English vocabulary
sizes down to 18k and 10k respectively. We then
exclude pairs with more than 50 source or target
tokens. We validate on NIST 06 and test on NIST
08 (newswire portions only for both). We report
case-insensitive, 4 reference BLEU scores.

We use 2-layers bidirectional LSTMs for the
encoder. We train the model for 15 epochs, and
choose the one with lowest perplexity on the dev
set. Batch size is 64; both word embedding and
hidden state sizes 500; and dropout 0.3. The total
parameter size is 28.5M.

6.1 Parameter Tuning and Results
We compare all rescoring methods mentioned
above. For the length normalization method, we
also show its results with optimal stopping.

For Bounded Word-Reward method with and
without our predicted length, we choose the best
r on the dev set seperately. The length normal-

4https://github.com/OpenNMT/OpenNMT-py

https://github.com/OpenNMT/OpenNMT-py


3058

0 20 40 60 80
Input sentence length

5

10

15

20

25

30

35

40

45

BL
EU

 sc
or

e

BP-Norm
Bounded Adaptive-Reward
Bounded Word-R. w/ Pred. Length
Wu et al. (2016)
Length Norm.
Default

0 20 40 60 80
Input sentence length

0.4

0.5

0.6

0.7

0.8

0.9

1.0

Le
ng

th
 ra

tio

BP-Norm
Bounded Adaptive-Reward
Bounded Word-R. w/ Pred. Length
Wu et al. (2016)
Length Norm.
Default

Figure 5: BLEU scores and length ratios on the dev
set over various input sentence lengths.

ization used by Wu et al. (2016) has two hyper-
parameters, namely α for length penalty and β for
coverage penalty. We jointly tune them on the dev
set, and choose the best config. (α=0.3, β=0.3).

Figure 4 show our results on the dev set. We
see that our proposed methods get the best per-
formance on the dev set, and continue growing
as beam size increases. We also observe that op-
timal stopping boosts the performance of length
normalization method by around +0.9 BLEU. In
our experiments, we regard our predicted length
as the maximum generation length in (13). We
further observe from Fig. 5 that our methods keep
the length ratio close to 1, and greatly improve the
quality on longer input sentences, which are noto-
riously hard for NMT (Shen et al., 2016).

Table 1 collects our results on both dev and test
sets. Without loss of generality, we show results
with both small and large beam sizes, which aver-
age over b=14,15,16 and b=39,40,41, respectively.

6.2 Discussion
From Table 1, we could observe that with our
length prediction model, Bounded word-reward
method gains consistent improvement. On the
other hand, results from length normalization
method show that optimal stopping technique

Small beam (b = 14, 15, 16)
dev test

BLEU ratio BLEU ratio
Moses (b=70) 30.14 - 29.41 -
Default (b=5) 36.45 0.87 32.88 0.87
Length Norm. 37.73 0.89 34.07 0.89
+ optimal stopping∗ 38.69 0.92 35.00 0.92
Wu et al. (2016) α=β=0.3 38.12 0.89 34.26 0.89
Bounded word-r. r=1.3 39.22 0.98 35.76 0.98
with predicted length
Bounded word-r. r=1.4∗ 39.53 0.97 35.81 0.97
Bounded adaptive-reward∗ 39.44 0.98 35.75 0.98
BP-Norm∗ 39.35 0.98 35.84 0.99

Large beam (b = 39, 40, 41)
dev test

BLEU ratio BLEU ratio
Moses (b=70) 30.14 - 29.41 -
Default (b=5) 36.45 0.87 32.88 0.87
Length Norm. 38.15 0.88 34.26 0.88
+ optimal stopping∗ 39.07 0.91 35.14 0.91
Wu et al. (2016) α=β=0.3 38.40 0.89 34.41 0.88
Bounded word-r. r=1.3 39.60 0.98 35.98 0.98
with predicted length
Bounded word-r. r=1.4∗ 40.11 0.98 36.13 0.97
Bounded adaptive-reward∗ 40.14 0.98 36.23 0.98
BP-Norm∗ 39.97 0.99 36.22 0.99

Table 1: Average BLEU scores and length ratios over
small and large beams. ? indicates our methods.

gains significant improvement by around +0.9
BLEU. While with both, our proposed methods
beat all previous methods, and gain improvement
over hyperparameter-free baseline (i.e. length nor-
malization) by +2.0 BLEU.

Among our proposed methods, Bounded word-
reward has the reward r as an hyper-parameter,
while the other two methods get rid of that.
Among them, we recommend the BP-Norm
method, because it is the simplest method, and yet
works equally well with others.

7 Conclusions

We first explain why the beam search curse exists
and then formalize all previous rescoring methods.
Beyond that, we also propose several new methods
to address this problem. Results from the Chinese-
English task show that our hyperparameter-free
methods beat the hyperparameter-free baseline
(length normalization) by +2.0 BLEU.

Acknowledgements

Kenton Lee suggested the length prediction idea.
This work was partially supported by DARPA
N66001-17-2-4030, and NSF IIS-1817231 and
IIS-1656051. We thank the anonymous reviewers
for suggestions and Juneki Hong for proofreading.



3059

References
Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben-

gio. 2014. Neural machine translation by jointly
learning to align and translate. arXiv preprint
arXiv:1409.0473.

Jonas Gehring, Michael Auli, David Grangier, Denis
Yarats, and Yann N Dauphin. 2017. Convolutional
sequence to sequence learning. In Proc. of ICML.

Wei He, Zhongjun He, Hua Wu, and Haifeng Wang.
2016. Improved neural machine translation with smt
features. In Proceedings of the Thirtieth AAAI Con-
ference on Artificial Intelligence, pages 151–157.
AAAI Press.

Liang Huang, Kai Zhao, and Mingbo Ma. 2017. When
to finish? optimal beam search for neural text gener-
ation (modulo beam size). In EMNLP.

Nal Kalchbrenner and Phil Blunsom. 2013. Recur-
rent continuous translation models. In EMNLP, vol-
ume 3, page 413.

G. Klein, Y. Kim, Y. Deng, J. Senellart, and A. M.
Rush. 2017. OpenNMT: Open-Source Toolkit for
Neural Machine Translation. ArXiv e-prints.

Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, et al. 2007. Moses: Open source
toolkit for statistical machine translation. In Pro-
ceedings of the 45th annual meeting of the ACL on
interactive poster and demonstration sessions, pages
177–180. Association for Computational Linguis-
tics.

Philipp Koehn and Rebecca Knowles. 2017. Six
challenges for neural machine translation. CoRR,
abs/1706.03872.

John Lafferty, Andrew McCallum, and Fernando
Pereira. 2001. Conditional random fields: Prob-
abilistic models for segmenting and labeling se-
quence data. In Proceedings of ICML.

Kenton Murray and David Chiang. 2018. Correcting
length bias in neural machine translation. In Pro-
ceedings of WMT 2018.

Myle Ott, Michael Auli, David Grangier, and
Marc’Aurelio Ranzato. 2018. Analyzing uncer-
tainty in neural machine translation. arXiv preprint
arXiv:1803.00047.

Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic eval-
uation of machine translation. In Proceedings of
ACL, pages 311–318, Philadephia, USA.

Marc’Aurelio Ranzato, Sumit Chopra, Michael Auli,
and Wojciech Zaremba. 2016. Sequence level train-
ing with recurrent neural networks. ICLR.

Rico Sennrich, Barry Haddow, and Alexandra Birch.
2015. Neural machine translation of rare words with
subword units. arXiv preprint arXiv:1508.07909.

Shiqi Shen, Yong Cheng, Zhongjun He, Wei He, Hua
Wu, Maosong Sun, and Yang Liu. 2016. Minimum
risk training for neural machine translation. In Pro-
ceedings of ACL.

Xing Shi, Kevin Knight, and Deniz Yuret. 2016. Why
neural translations are the right length. In Proceed-
ings of the 2016 Conference on Empirical Methods
in Natural Language Processing, pages 2278–2282.

Ilya Sutskever, Oriol Vinyals, and Quoc V Le. 2014.
Sequence to sequence learning with neural net-
works. In Advances in neural information process-
ing systems, pages 3104–3112.

Zhaopeng Tu, Yang Liu, Lifeng Shang, Xiaohua Liu,
and Hang Li. 2017. Neural machine translation with
reconstruction. In AAAI, pages 3097–3103.

Sam Wiseman and Alexander M Rush. 2016.
Sequence-to-sequence learning as beam-search op-
timization. In Proceedings of EMNLP.

Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V
Le, Mohammad Norouzi, Wolfgang Macherey,
Maxim Krikun, Yuan Cao, Qin Gao, Klaus
Macherey, et al. 2016. Google’s neural ma-
chine translation system: Bridging the gap between
human and machine translation. arXiv preprint
arXiv:1609.08144.


