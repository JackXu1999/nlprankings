



















































An End-to-End Multi-task Learning Model for Fact Checking


Proceedings of the First Workshop on Fact Extraction and VERification (FEVER), pages 138–144
Brussels, Belgium, November 1, 2018. c©2018 Association for Computational Linguistics

138

An End-to-End Multi-task Learning Model for Fact Checking

Sizhen Li and Shuai Zhao and Bo Cheng
State Key Laboratory of networking and switching technology,

Beijing university of posts and telecommunications

Hao Yang
2012 Labs, Huawei Technologies CO., LTD

Abstract

With huge amount of information generated
every day on the web, fact checking is an im-
portant and challenging task which can help
people identify the authenticity of most claims
as well as providing evidences selected from
knowledge source like Wikipedia. Here we
decompose this problem into two parts: an en-
tity linking task (retrieving relative Wikipedia
pages) and recognizing textual entailment be-
tween the claim and selected pages. In this pa-
per, we present an end-to-end multi-task learn-
ing with bi-direction attention (EMBA) model
to classify the claim as “supports”, “refutes”
or “not enough info” with respect to the pages
retrieved and detect sentences as evidence at
the same time. We conduct experiments on the
FEVER (Fact Extraction and VERification)
paper test dataset and shared task test dataset, a
new public dataset for verification against tex-
tual sources. Experimental results show that
our method achieves comparable performance
compared with the baseline system.

1 Introduction

When we got news from newspapers and TVs
which was thoroughly investigated and written by
professional journalists, most of these messages
are well-found and trustworthy. However, with the
popularity of the internet, there are 2.5 quintillion
bytes of data created each day at our current pace1.
Everyone online is a producer as well as a recip-
ient of these emerging information, and some of
them are incorrect, fabricated or even with some
evil purposes. Most time it is difficult for us to
figure out the truth of those emerging news with-
out professional background and enough investi-
gation. Fact checking, which firstly has been pro-
duced and received a lot of attention in the indus-

1https://www.forbes.com/sites/bernardmarr/2018/05/21/how-
much-data-do-we-create-every-day-the-mind-blowing-stats-
everyone-should-read/#62a15bba60ba

try of journalism, mainly verifying the speeches of
public figures, is also important for other domains,
e.g. wrong common-sense correction, rumor de-
tection, content review etc.

With the increasing demand for automatic claim
verification, several datasets for fact checking
have been produced in recent years. Vlachos
and Riedel (2014) are the first to release a pub-
lic fake news detection and fact-checking dataset
from two fact checking websites, the fact check-
ing blog of Channel 42 and the True-O-Meter from
PolitiFact3. This dataset only includes 221 state-
ments. Similarly, from PolitiFact via its API,
Wang (2017) collected LIAR dataset with 12.8K
manually labeled short statements, which per-
mits machine learning based methods used on this
dataset. Both dataset don’t include the original
justification and evidence as it was not machine-
readable. However, just verifying the claim based
on the claim itself and without referring to any ev-
idence sources is not reasonable and convincing.

In 2015, Silverman launched the Emergent
Project4, a real-time rumor tracker, part of a re-
search project with the Tow Center for Digi-
tal Journalism5 at Columbia University. Ferreira
and Vlachos (2016) firstly proposed to use the
data from Emergent Project as Emergent dataset
for rumor debunking, which contains 300 ru-
mored claims and 2,595 associated news articles.
In 2017, the Fake news challenge (Pomerleau
and Rao, 2017) consisted of 50K labeled claim-
article pairs similarly derived from the Emergent
Project. These two dataset stemmed from Emer-
gent Project alleviate the fact checking task by
detecting the relationship between claim-article
pairs. However, in more common situation, we

2http://blogs.channel4.com/factcheck/
3http://www.politifact.com/truth-o-meter/statements/
4http://www.emergent.info/
5https://towcenter.org/



139

are dealing with plenty of claims themselves on-
line without associated articles which can help to
verify the claims.

Fact Extraction and VERification (FEVER)
dataset (Thorne et al., 2018) consists of 185,445
claims manually verified against the introduc-
tory sections of Wikipedia pages and classified
as SUPPORTED, REFUTED or NOTENOUGH-
INFO. For the first two classes, the dataset pro-
vides combination of sentences forming the nec-
essary evidences supporting or refuting the claim.
Obviously, this dataset is more difficult than ex-
isting fact-checking datasets. In order to achieve
higher FEVER score, a fact-checking system is re-
quired to classify the claim correctly as well as
retrieving sentences among more than 5 million
Wikipedia pages jointed as correct evidence sup-
porting the judgement.

The baseline method of this task comprises of
three components: document retrieval, sentence-
level evidence selection and textual entailment.
For the first two retrieval components, the base-
line method uses document retrieval component of
DrQA (Chen et al., 2017) which only relies on the
unigram and bigram TF-IDF with vector similarity
and don’t understand semantics of the claim and
pages. So, we find that it extracts lots of Wikipedia
pages which are unrelated to the entities described
in claims. Besides, similarity-based method pre-
fer extracting supporting evidences than refuting
evidences. For the recognizing textual entailment
(RTE) module, on one hand, the previous retrieval
results limit the performance of the RTE model.
On the other hand, the selected sentences con-
catenated as evidences may also confuse the RTE
model due to some contradictory information.

In this paper, we introduce an end-to-end multi-
task learning with bi-direction attention (EMBA)
model for FEVER task. We utilize the multi-task
framework to jointly extract evidences and verify
the claim because these two sub-tasks can be ac-
complished at the same time. For example, after
selecting relative pages, we carefully scan these
pages to find supporting or refuting evidences. If
we find some, the claim can be labeled as SUP-
PORTS or REFUTES immediately. If not, the
claim will be classified as NOTENOUGHINFO
after we read pages completely. Our model is
trained on claim-pages pairs by using attention
mechanism in both directions, claim-to-pages and
pages-to-claim, which provides complimentary in-

formation to each other. We obtain claim-aware
sentence representation to predict the correct ev-
idence position and the pages-aware claim rep-
resentation to detect the relationship between the
claim and the pages.

2 Related Work

Natural Language Inference (NLI) or Recognizing
textual entailment (RTE) detects the relationship
between the premise-hypothesis pairs as “entail-
ment”, “contradiction” and “not related”. With
the renaissance of neural network (Krizhevsky
et al., 2012; Mikolov et al., 2010; Graves, 2012)
and attention mechanism (Xu et al., 2015; Luong
et al., 2015; Bahdanau et al., 2014), the popular
framework for the RTE is “matching-aggregation”
(Parikh et al., 2016; Wang et al., 2017). Under
this framework, words of two sentences are firstly
aligned, and then the aligning results with orig-
inal vectors are aggregated into a new represen-
tation vector to make the final decision. The at-
tention mechanism can empower this framework
to capture more interactive features between two
sentences. Compared to Fever task, RTE provides
the sentence to verify against instead of having to
retrieve it from knowledge source.

Another relative task is question answer-
ing (QA) and machine reading comprehension
(MRC), for which approaches have recently been
extended to handle large-scale resources such as
Wikipedia (Chen et al., 2017). Similar to MRC
task which needs to identify the answer span in
a passage, FEVER task requires to detect the ev-
idence sentences in Wikipedia pages. However,
MRC model tends to identify the answer span
based on the similarity and reasoning between
the question and passage, while similarity-based
method is more likely to ignore refuting evidence
in pages. For example, a claim stating “Manch-
ester by the Sea is distributed globally” can be re-
futed by retrieving “It began a limited release on
November 18, 2016” as evidence.

3 Model

The FEVER dataset is derived from the Wikipedia
pages. So, we assume each claim contains at least
one entity in Wikipedia and the evidence can be
retrieved from these relative pages. Thus, we de-
compose FEVER task into two components: (1)
entity linking which detects Wikipedia entities in
claim. We use the pages of identified entities



140

backward
last time-

step

forward
last time-

step

Dense+Softmax

Embedding layer

Context embedding layer

Attention Matching
layer

Aggregation layer

Output layer

ℎ"# ℎ$# ℎ%#… … … …ℎ"& ℎ$' ℎ('

)ℎ"# )ℎ$# )ℎ%# )ℎ"
' )ℎ$' )ℎ%'

*"# *$# *%# *"' *$' *%'

+" … …+$

…

Dense+Softmax

Sentence

Claim Classification

…

…

… …

,$-

Figure 1: An End-to-End Multi-task Learning Model for Fact Checking

as relative pages. And (2) an end-to-end multi-
task learning with bi-direction attention (EMBA)
model (in Figure 1) which classify the claim as
”supports”, ”refutes” or ”not enough info” with re-
spect to the pages retrieved and select sentences as
evidence at the same time.

3.1 Entity Liking

S-MART is a Wikipedia entity linking tool for
short and noisy text. For each claim, we use
S-MART to retrieve the top 5 entities from
Wikipedia. These entity pages are jointed together
as the source pages then passed to select correct
sentences. For a given claim, S-MART first re-
trieves all possible entities of Wikipedia by surface
matching, and then ranks them using a statistical
model, which is trained on the frequency counts
with which the surface form occurs with the en-
tity.

3.2 Sentence Extraction and Claim
Verification

We now proceed to identify the correct sentences
as evidence from relative pages and try to classify
the claim as ”supports”, ”refutes” or ”not enough
info” with respect to the pages retrieved at the
same time. Inspired by the recent success of at-
tention mechanism in NLI (Wang et al., 2017) and
MRC (Seo et al., 2016; Tan et al., 2017), we pro-
pose an end-to-end multi-task learning with bi-

direction attention (EMBA) model, which exploits
both pages-to-claim attention to verify the claim
and claim-to-pages attention to predict the evi-
dence sentence position respectively. Our model
consists of:

Embedding layer: This layer represents each
word in a fixed-size vector with two components:
a word embedding and a character-level embed-
ding. For word embedding, pre-trained word vec-
tors, Glove (Pennington et al., 2014), provides the
fixed-size embedding of each word. For character
embedding, following Kim (Kim, 2014), charac-
ters of each words are embedded into fixed-size
embedding, then fed into a Convolutional Neural
Network (CNN). The character and word embed-
ding vectors are concatenated together and passed
to a Highway Network (Srivastava et al., 2015).
The output of this layer are two sequences of word
vectors of claim and pages.

Context embedding layer: The purpose of this
layer is to incorporate contextual information into
the presentation of each word of claim and pas-
sage. We utilize a bi-directional LSTM (BiLSTM)
on the top of the embedding provided by the pre-
vious layers to encode contextual embedding for
each word.

Attention matching layer: In this layer, we
compute attention in two directions: from pages
to claim as well as from claim to pages. To ob-
tain these attention mechanisms, we first calculate



141

a shared similarity matrix between the contextual
embedding of each word of the claim hci and each
word of the pages hpj :

αij = w[h
c
i ;h

p
j ;h

c
i ◦ h

p
j ] (1)

where αij represents the attention weights on the
i-th claim word by j-th pages word, w is a train-
able weight vector, ◦ is elementwise multiplica-
tion, [;] is vector concatenation across row, and
implicit multiplication is matrix multiplication.

Claim-to-pages attention Claim-to-pages atten-
tion represents which claim words are most rele-
vant to each word of pages. To obtain attended
pages vector, we take αij as the weight of h

p
j

and weighted sum all the contextual embedding of
pages:

h̃j =
N∑
i=1

αijhi/
N∑
i=1

αij (2)

Finally, we match each contextual embedding
with its corresponding attention vector to obtain
the claim-aware representation of each word of
pages:

mj = W[hj ; h̃j ;hj ◦ h̃j ] (3)

Pages-to-claim attention Pages-to-claim atten-
tion represents which pages words are most rele-
vant to each claim word. Similar to claim-to-pages
attention, the attended claim vector and the pages-
aware representation of each pages word are cal-
culated by:

h̃i =
N∑
j=1

αijhj/
N∑
j=1

αij (4)

mi = W[hi; h̃i;hi ◦ h̃i] (5)

Aggregation layer: The input to the aggrega-
tion layer is two sequences of matching vectors,
the claim-aware pages word representation and
pages-aware claim word representation. The goal
of the modeling layer is to capture the interaction
among the pages words conditioned on the claim
as well as the claim words conditioned on the pas-
sage words. This is different from the contextual
embedding layer, which captures the interaction
among context information independent of match-
ing information.

Sentence selection layer: The FEVER task re-
quires the model to retrieve sentences of the pas-
sage as evidence to verify the claim. The sentence
representation st is obtained by concatenating vec-
tors from the last time-step of the previous layer

BiLSTM models output sequences. We calculate
the probability distribution of the evidence posi-
tion over the whole pages by:

pt = softmax(wst) (6)

For this sub-task, the objective function is to min-
imize the negative log probabilities of true evi-
dence index:

Ls = −
N∑
t=1

[yt log pt + (1− yt) log(1− pt)] (7)

where yt ∈ 0, 1 denotes a label, yt = 1 means the
t-th sentence is a correct evidence, other yt = 0.

Claim verification layer: The input of this
layer is pages-aware claim representation pro-
duced from the matching layer and the output is a
3-way classification, predicting whether the claim
is SUPPORTED, REFUTED or NOTENOUGH-
INFO by the pages. We utilize multiple convolu-
tion layers, with the output of 3 for classification.
We optimize the objective function:

Lc = −
k∑

i=1

yi log ĝi (8)

Where k is the number of claims. yt ∈ 0, 1, 2
denotes a label, meaning the i-th claim is SUP-
PORTED, REFUTED, and NOTENOUGHINFO
by the pages respectively.

Training: The model is trained by minimizing
joint objective function:

L = Ls + α ∗ Lc (9)

where α is the hyper-parameter for weights of two
loss functions.

4 Experiments

In this section, we evaluate our model on FEVER
paper test dataset and shared task test dataset.

4.1 Model Details

The model architecture used for this task is de-
picted in Figure 1. The nonlinearity function f =
tanh is employed. We use 100 1D filters for CNN
char embedding, each with a width of 5. The hid-
den state size (d) of the model is 100. We use the
Adam (Kingma and Ba, 2014) optimizer, with a
minibatch size of 32 and an initial learning rate
of 0.001. A dropout rate of 0.2 is used for the



142

EMBA Baseline
(paper test) (paper dev)

Evidence Score 30.34 32.57
Label Accuracy 45.06 -
Evidence Precision 46.12 -
Evidence Recall 42.84 -
Evidence F1 44.42 -

Table 1: our EMBA model results on the FEVER
paper test dataset, Baseline method results on the
paper dev dataset.

Evidence F1 Label Accuracy FEVER Score
39.73 45.38 29.22

Table 2: Model results on the FEVER shared task
test dataset.

CNN, all LSTM layers, and the linear transforma-
tion. The parameters are initialized by the tech-
niques described in (Glorot, 2010). The max value
used for max-norm regularization is 5. TheLc loss
weight is set to α = 0.5.

4.2 Experimental Results

We use the official evaluation script6 to compute
the evidence F1 score, label accuracy and FEVER
score. As shown in Table 1, our method achieves
comparable performance on FEVER paper test
dataset comparing with the baseline method on
FEVER paper dev dataset. The result shows that
jointly verifying a claim and retrieving evidences
at same time can be as good as pipelined model.
Our method results on the FEVER paper shared
task test dataset is showed in Table 2. Besides,
We calculate and present the confusion matrix of
claim classification results on the FEVER paper
test dataset in Table 3. Our model isn’t good
at identifying the unrelated relationship between
claim and pages retrieved. Our model sentence se-
lection performance is recorded in Table 4. We
can see that our model doesn’t perform well for
retrieving evidence. Though with low evidence
precision, our model average accuracy without re-
quirement to provide correct evidence (51.97%) is
similar to 52.09% accuracy of baseline method,
which means that claim verification module and
the sentence extraction module are relatively inde-
pendent in our model.

6https://github.com/sheffieldnlp/fever-baselines

NEI REFUTES SUPPORTS recall
NEI 1285 1432 390 41%
REFUTES 992 1937 155 62.8%
SUPPORTS 942 860 1284 41.6%
precision 39.9% 45.8% 70.2%

Table 3: confusion matrix for claim classification.
(NEI = “not enough info”)

Evidence Evidence Evidence
precision recall F1

SUPPORTS 22.07% 40.08% 28.47%
REFUTES 23.8% 45.18% 31.18%

Table 4: sentences retrieval performance.

4.3 Error Analysis

We investigate the predicted results on the paper
test dataset and show several error causes as fol-
lowings.

Document retrieval We use entity linking tool
to retrieve relative Wikipedia pages. Some entity
mentions in claims are linked incorrectly, hence
we cannot obtain the desired pages containing the
correct evidence sentences. The S-MART tool re-
turned correct entities for 70% claims of paper test
dataset. A better entity retrieval method should be
researched for the FEVER task.

Pages length After document retrieval, the rel-
ative pages are concatenated and passed through
EMBA model. However, in order to train and pre-
dict effectively, the length of the pages is limited
to 800 tokens. So, if there are many relative pages
and the position of the evidence sentence is near
the end of the page, these correct sentences would
be cut off.

Evidence composition Some claims require
composition of evidences from multiple pages.
Furthermore, the selection of second page relies
on the correct retrieval of the first page and sen-
tence. For example, claim “Deepika Padukone
has been in at least one Indian films” can be sup-
ported by combination of “She starred roles in Yeh
Jawaani Hai Deewani” and “Yeh Jawaani Hai Dee-
wani is an Indian film” from “Deepika Padukone”
and “Yeh Jawaani Hai Deewani” Wikipedia pages
respectively. The second page couldn’t be found
correctly if we don’t select the first sentence ex-
actly. 18% claims in train dataset belong to this
situation.



143

5 Conclusion

We propose a novel end-to-end multi-task learn-
ing with bi-direction attention (EMBA) model to
detect sentences as evidence and classify the claim
as “supports”, “refutes” or “not enough info” with
respect to the pages retrieved at the same time.
EMBA uses attention mechanism in both direc-
tions to capture interactive features between claim
and pages retrieved. Model obtains claim-aware
sentence representation to predict the correct ev-
idence position and the pages-aware claim rep-
resentation to detect the relationship between the
claim and the pages. Experimental results on the
FEVER paper test dataset show that our approach
achieve comparable performance comparing with
the baseline method. There are several promis-
ing directions that worth researching in the future.
For instance, in sentence selection layer, the model
just predicts whether a sentence is an evidence.
Further, we can try to instantly predict whether
a sentence is “supporting”, “refuting” or “not re-
lated with” the claim. What’s more, the hyper-
parameter α for joint loss function is fixed. A good
value for this parameter can achieve one plus one
is greater than two. We can try to learn this param-
eter value during training the model.

6 acknowledgement

This work is supported by National Natural Sci-
ence Foundation of China (Grant No. 61501048),
Beijing Natural Science Foundation (Grant No.
4182042), Fundamental Research Funds for the
Central Universities (No. 2017RC12), and
National Natural Science Foundation of China
(U1536111, U1536112).

References
Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben-

gio. 2014. Neural machine translation by jointly
learning to align and translate. Computer Science.

Danqi Chen, Adam Fisch, Jason Weston, and An-
toine Bordes. 2017. Reading wikipedia to an-
swer open-domain questions. arXiv preprint
arXiv:1704.00051.

William Ferreira and Andreas Vlachos. 2016. Emer-
gent: a novel data-set for stance classification. In
Proceedings of the 2016 conference of the North
American chapter of the association for computa-
tional linguistics: Human language technologies,
pages 1163–1168.

Yoshua Bengio Glorot, Xavier. 2010. Understanding
the difficulty of training deep feedforward neural
networks. In international conference on artificial
intelligence and statistics, pages 249–256.

Alex Graves. 2012. Long Short-Term Memory.
Springer Berlin Heidelberg.

Yoon Kim. 2014. Convolutional neural net-
works for sentence classification. arXiv preprint
arXiv:1408.5882.

Diederik P. Kingma and Jimmy Ba. 2014. Adam: A
method for stochastic optimization. Computer Sci-
ence.

Alex Krizhevsky, Ilya Sutskever, and Geoffrey E. Hin-
ton. 2012. Imagenet classification with deep con-
volutional neural networks. In International Con-
ference on Neural Information Processing Systems,
pages 1097–1105.

Minh Thang Luong, Hieu Pham, and Christopher D
Manning. 2015. Effective approaches to attention-
based neural machine translation. Computer Sci-
ence.

Tomas Mikolov, Martin Karafiát, Lukas Burget, Jan
Cernocký, and Sanjeev Khudanpur. 2010. Recur-
rent neural network based language model. In IN-
TERSPEECH 2010, Conference of the International
Speech Communication Association, Makuhari,
Chiba, Japan, September, pages 1045–1048.

Ankur P Parikh, Oscar Täckström, Dipanjan Das, and
Jakob Uszkoreit. 2016. A decomposable attention
model for natural language inference. arXiv preprint
arXiv:1606.01933.

Jeffrey Pennington, Richard Socher, and Christopher
Manning. 2014. Glove: Global vectors for word
representation. In Proceedings of the 2014 confer-
ence on empirical methods in natural language pro-
cessing (EMNLP), pages 1532–1543.

Dean Pomerleau and Delip Rao. 2017. Fake News
Challenge http://fakenewschallenge.org.

Minjoon Seo, Aniruddha Kembhavi, Ali Farhadi, and
Hannaneh Hajishirzi. 2016. Bidirectional attention
flow for machine comprehension. arXiv preprint
arXiv:1611.01603.

Rupesh Kumar Srivastava, Klaus Greff, and Jürgen
Schmidhuber. 2015. Highway networks. arXiv
preprint arXiv:1505.00387.

Chuanqi Tan, Furu Wei, Nan Yang, Bowen Du,
Weifeng Lv, and Ming Zhou. 2017. S-net: From
answer extraction to answer generation for ma-
chine reading comprehension. arXiv preprint
arXiv:1706.04815.

James Thorne, Andreas Vlachos, Christos
Christodoulopoulos, and Arpit Mittal. 2018.
Fever: a large-scale dataset for fact extraction and
verification. arXiv preprint arXiv:1803.05355.



144

Andreas Vlachos and Sebastian Riedel. 2014. Fact
checking: Task definition and dataset construction.
In Proceedings of the ACL 2014 Workshop on Lan-
guage Technologies and Computational Social Sci-
ence, pages 18–22.

William Yang Wang. 2017. ”liar, liar pants on fire”:
A new benchmark dataset for fake news detection.
pages 422–426.

Zhiguo Wang, Wael Hamza, and Radu Florian. 2017.
Bilateral multi-perspective matching for natural lan-
guage sentences.

Kelvin Xu, Jimmy Ba, Ryan Kiros, Kyunghyun Cho,
Aaron Courville, Ruslan Salakhutdinov, Richard
Zemel, and Yoshua Bengio. 2015. Show, attend and
tell: Neural image caption generation with visual at-
tention. Computer Science, pages 2048–2057.


