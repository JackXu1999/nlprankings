180

Coling 2010: Poster Volume, pages 180–188,

Beijing, August 2010

Two Methods for Extending Hierarchical Rules from the Bilingual Chart

Parsing

Martin ˇCmejrek and Bowen Zhou
IBM T. J. Watson Research Center

{martin.cmejrek, zhou}@us.ibm.com

Abstract

This paper studies two methods for train-
ing hierarchical MT rules independently
of word alignments. Bilingual chart pars-
ing and EM algorithm are used to train bi-
text correspondences. The ﬁrst method,
rule arithmetic, constructs new rules as
combinations of existing and reliable rules
used in the bilingual chart, signiﬁcantly
improving the translation accuracy on the
German-English and Farsi-English trans-
lation task. The second method is pro-
posed to construct additional rules directly
from the chart using inside and outside
probabilities to determine the span of the
rule and its non-terminals. The paper also
presents evidence that the rule arithmetic
can recover from alignment errors, and
that it can learn rules that are difﬁcult to
learn from bilingual alignments.

1 Introduction

Hierarchical phrase-based systems for machine
translation usually share the same pattern for ob-
taining rules: using heuristic approaches to ex-
tract phrase and rule pairs from word alignments.
Although these approaches are very successful
in handling local linguistic phenomena, handling
longer distance reorderings can be more difﬁcult.
To avoid the combinatorial explosion, various re-
strictions, such as limitations of the phrase length
or non-terminal span are used, that sometimes pre-
vent from extracting good rules. Another reason
is the deterministic nature of those heuristics that
does not easily recover from errors in the word
alignment.

In this work, we learn rules for hierarchical
phrase based MT systems directly from the par-
allel data, independently of bilingual word align-
ments.

Let us have an example of a German-English
sentence pair from the Europarl corpus (Koehn,
2005).

(1) GER: die herausforderung besteht darin
diese systeme zu den besten der welt zu
machen
ENG: the challenge is to make the system
the very best

The two pairs of corresponding sequences diese
systeme ... der welt—the system ... best and zu
machen—to make are swapped. We believe that
the following rule could handle long distance re-
orderings, still with a reasonably low number of
terminals, for example:

(2) X → hbesteht darin X1 zu X2, is to X2X1i,
There are 127 sentence pairs out of 300K of the
training data that contain this pattern, but this rule
was not learned using the conventional approach
(Chiang, 2007). There are three potential risks:
(1) alignment errors (the ﬁrst zu aligned to to, or
der welt (of the world) aligned to null); (2) maxi-
mum phrase length for extracting rules lower than
11 words; (3) requirement of non-terminals span-
ning at least 2 words.

The rule arithmetic (Cmejrek et al., 2009) con-
structs the new rule (2) as a combination of good
rule usages:

(3) X → hbesteht darin, is i

X → hX1 zu X2, to X2X1i

181

The approach consists of bilingual chart parsing
(BCP) of the training data, combining rules found
in the chart using a rule arithmetic to propose new
rules, and using EM to estimate rule probabilities.
In this paper, we study the behavior of the
rule arithmetic on two different language pairs:
German-English and Farsi-English. We also pro-
pose an additional method for constructing new
rules directly from the bilingual chart, and com-
pare it with the rule arithmetic.

The paper is structured as follows: In Sec. 1, we
explain our main motivation, summarize previous
work, and brieﬂy introduce the formalism of hi-
erarchical phrase-based translation. In Sec. 2, we
describe the bilingual chart parsing and the EM
algorithm. The rule arithmetic is introduced in
Sec. 3. The new method for proposing new rules
directly from the chart is described in Sec. 4. The
experimental setup is described in Sec. 5. Results
are thoroughly discussed in Sec. 6. Finally, we
conclude in Sec. 7.

1.1 Related work
Many previous works use the EM algorithm to
estimate probabilities of translation rules: Wu
(1997) uses EM to directly estimate joint word
alignment probabilities of Inversion Transduction
Grammar (ITG). Marcu and Wong (2002) use
EM to estimate joint phrasal translation model
(JPTM). Birch et al.
(2006) reduce its com-
plexity by using only concepts that match the
high-conﬁdence GIZA++ alignments. Similarly,
Cherry and Lin (2007) use ITG for pruning. May
and Knight (2007) use EM algorithm to train tree-
to-string rule probabilities, and use the Viterbi
derivations to re-align the training data. Huang
and Zhou (2009) use EM to estimate conditional
rule probabilities P (α|γ) and P (γ|α) for Syn-
chronous Context-free Grammar. Others try to
overcome the deterministic nature of using bilin-
gual alignments for rule extraction by sampling
techniques (Blunsom et al., 2009; DeNero et al.,
2008). Galley et al.
(2006) deﬁne minimal
rules for tree-to-string translation, merge them
into composed rules (similarly to the rule arith-
metic), and train weights by EM. While in their
method, word alignments are used to deﬁne all
rules, rule arithmetic proposes new rules indepen-

dently of word alignments. Similarly, Liu and
Gildea (2009) identify matching long sequences
(“big templates”) using word alignments and “lib-
erate” matching small subtrees based on chart
probabilities. Our method of proposing rules di-
rectly from the chart does not use word alignment
at all.

1.2 Formally syntax-based models
Our baseline model follows the Chiang’s hierar-
chical model (Chiang, 2007; Chiang, 2005; Zhou
et al., 2008) based on Synchronous Context-free
Grammar (SCFG). The rules have form

X → hγ, α,∼i,

(4)

where X is the only non-terminal in the gram-
mar, γ and α are source and target strings with
terminals and up to two non-terminals, ∼ is the
correspondence between the non-terminals. Cor-
responding non-terminals have to be expanded at
the same time.

2 Bilingual chart parsing and EM

algorithm

In this section, we brieﬂy overview the algorithm
for bilingual chart parsing and EM estimation of
SCFG rule features.

Let e = eM

1 and f = f N

1 of source and tar-
get sentences. For each sentence pair e, f, the ’E’
step of the EM algorithm will use the bilingual
chart parser to enumerate all possible derivations
Φ, compute inside probabilities βijkl(X) and out-
side probabilities αijkl(X), and ﬁnally calculate
expected counts c(r) how many times each rule r
produced the corpus C.

The inside probabilities can be deﬁned recur-
sively and computed dynamically during the chart
parsing:

βijkl = Xρ∈tijkl

P (ρ.r) Y(i′j′k′l′)∈ρ.bp

βi′j′k′l′,

(5)

i , f l

where tijkl represents the chart cell spanning
(ej
k), and the data structure ρ stores the rule
ρ.r. If r has non-terminals, then ρ.bp stores back-
pointers ρ.bp1 and ρ.bp2 to the cells representing
their derivations.

182

.

(9)

(12) Addition

The outside probabilities can be computed re-
cursively by iterating the chart in top-down order-
ing. We start from the root cell α1,M,1,N := 1 and
propagate the probability mass as

αρ.bp1+ = P (ρ.r)αijkl

for rules with one non-terminal, and

αρ.bp1 + = P (ρ.r)αijklβρ.bp2,
αρ.bp2 + = P (ρ.r)αijklβρ.bp1,

(6)

(7)
(8)

for rules with two non-terminals. The top-down
ordering ensures that each αijkl accumulates up-
dates from all cells higher in the chart before its
own outside probability is used.

The contributions to the rule expected counts

are computed as

c(ρ.r)+ =

P (ρ.r)αijklQρ.n

β1,M,1,N

i=1 βρ.bpi

Finally, rule probabilities P (r) are obtained by
normalizing expected counts in the ’M’ step.

To improve the grammar coverage, the rule-
set is extended by the following rules providing
“backoff” parses and scoring for the SCFG rules:

(10) hX1, X1fi, hX1, f X1i, hX1e, X1i,

heX1, X1i,

(11) hX1X2, X2X1i.
Rules (10) enable insertions and deletions, while
rule (11) allows for aligning swapped constituents
in addition to the standard glue rule.

3 Proposing new rules with rule

arithmetic

The main idea of this work is to propose new rules
independently of the bilingual word alignments.
We parse each sentence pair using the baseline
ruleset extended by the new rule types (10) and
(11). Then we select the most promising rule us-
ages and combine each two of them using the
rule arithmetic to propose new rules. We put the
new rules into a temporary pool, and parse and
compute probabilities and expected counts again,
this time we use rules from the baseline and from
the temporary pool. Finally, we dump expected

counts for proposed rules, and empty the tempo-
rary pool. This way we can try to propose many
rules for each sentence pair, and to ﬁlter them later
using accumulated expected counts from the EM.
The term most promising is purposefully vague
— to cover all possible approaches to ﬁltering rule
usages. In our implementation, we are limited by
space and time, and we have to prune the number
of rules that we can combine. We use expected
counts as the main scoring criterion. When com-
puting the contributions to expected counts from
particular rule usages as described by (9), we re-
member the n-best contributors, and use them as
candidates after the expected counts for the given
sentence pair have been estimated.

The rule arithmetic combines existing rules us-
ing addition operation to create new rules. The
idea is shown in Example 12.

5
-1

3
0
0
0

...
...
...
...

...
...
...
...

4
to
0
to

5
-2
0
-2

7
-1
-3
-3

diese
diese

12
zu
0
zu

13
-2
0
-2

6
-1
the
the

h4, 10, 6, 10, 5, 5i X → hX1 zu X2, to X2 X1i
h6, 10, 7, 10, 0, 0i X → hdiese X1, the X1i
10
11
-1
-1
-3
-3
-3
-3

h5, 13, 5, 11, 13, 13i
h5, 11, 6, 11, 0, 0i
...
4
...
0
...
0
...
0
h4, 10, 7, 10, 5, 5i X → hdiese X1 zu X2, to X2 the X1i

1:
6
2:
-1
3:
-3
4:
-3
5: h5, 13, 6, 11, 13, 13i
First, create span projections for both source
and target sides of both rules. Use symbol 0 for
all unspanned positions, copy terminal symbols as
they are, and use symbols -1, -2, -3, and -4 to tran-
scribe X1 and X2 from the ﬁrst rule, and X1 and
X2 from the second rule. Repeat the non-terminal
symbol on all spanned positions. In Example 12
line 1 shows the positions in the sentence, lines 2
and 3 show the rule span projections of the two
rules.

Second, merge source span projections (line 4),
record mappings of non-terminal symbols. We re-
quire that merged projections are continuous. We
allow substituting non-terminal symbols by termi-
nals, but we require that the whole span of the
non-terminal is fully replaced.
In other words,
shortenings of non-terminal spans are not allowed.
Third, collect new rule. The merged rule us-
ages (lines 5) are generalized into rules, so that
they are not limited to the particular span for
which they were originally proposed.

The rule arithmetic can combine all types of
rules – phrase pairs, abstract rules, glues, swaps,
insertions and deletions. However, we require that

183

at least one of the rules is either a phrase pair or
an abstract rule.

4 Proposing directly from chart

One of the issues observed while proposing new
rules with the rule arithmetic is the selection of the
best candidates. The number of all candidates that
can be combined depends on the length of the sen-
tence pair and on the number of competing pars-
ing hypotheses. Using a ﬁxed size of the n-best
can constitute a risk of selecting bad candidates
from shorter sentences. On the other hand, the
spans of the best candidates extracted from long
sentences can be far from each other, so that most
combinations are not valid rules (e.g., the combi-
nation of two discontinuous phrasal rules is not
deﬁned).

In our new approach we propose new rules di-
rectly from the bilingual chart, relying on the in-
side and outside probabilities computed after the
parsing of the sentence pair. The method has two
steps. In the ﬁrst step we identify best matching
parallel sequences; in the second step we propose
“holes” for non-terminals.

Identifying best matching sequences

4.1
To identify the best matching sequences, we score
all sequences (ej

k) by a scoring function:

i , f l

scoreijkl =

αijklβijkl
β1,M,1,N

Lex(i, j, k, l),

(13)

where the lexical score is deﬁned as:

Lex(i, j, k, l) =

NXj′=1

MYi′=0

t(fj′|ei′)δijkli′j′

(14)

The t is the lexical probability from the word-to-
word translation table, and δijkli′j′ is deﬁned as
δins if i′ ∈ hi, ji and j′ ∈ hk, li, and as δout if
i′ /∈ hi, ji and j′ /∈ hk, li, and as 0 elsewhere.
The purpose of this function is to score only the
pairs of words that are both either from within the
sequence or from outside the sequence. Usually
0 ≤ δout ≤ δins to put more weight on words
within the parallel sequence.
The scoring function is a combination of ex-
i , f l
k)

pected counts contribution of a sequence (ej

estimated from the chart with the IBM Model 1
lexical score.

Since only the sequences spanned by ﬁlled
chart cells can have non-zero expected counts,
we can select the n-best matching sequences rela-
tively efﬁciently.

4.2 Proposing non-terminal positions
Similar approach can be used to propose best po-
sitions for non-terminals. We score every com-
bination of non-terminal positions. The expected
counts can be estimated using Eq. 9. Since we are
proposing new rules, the probability P (r) used in
that equation is not deﬁned. Again, we can use
Model 1 score instead, and use the following scor-
ing function:

sijkl(bp1, bp2) =

(15)

Lex(i,j,k,l,bp1,bp2)αijklβbp1 βbp2

β1,M,1,N

,

Lex(i, j, k, l, bp1, bp2) is deﬁned as in Eq. 14.
This time using 0 ≤ δout ≤ δN T 1 = δN T 2 ≤
δterm, restricting the IBM Model 1 to score only
word pairs that both belong either to the terminals
of the proposed rule, or to the sequences spanned
by the same non-terminal, or outside of the rule
span. The scoring function for rules with one non-
terminal is just a special case of 15.

Again, the candidates can be scored efﬁciently,
taking into account only those combinations of
non-terminal spans that correspond to ﬁlled cells
in the chart.

The proposed method is again independent of
bilingual alignment, but at the same time utilizes
the information obtained from the bilingual chart
parsing.

5 Experiments

We carried out experiments on two language pairs,
German-English and Farsi-English.

The German-English data is a subset (297k
sentence pairs) of the Europarl (Koehn, 2005) cor-
pus. Since we are focused on speech-to-speech
translation, the punctuation was removed, and the
text was lowercased. The dev set and test set con-
tain each 1k sentence pairs with one reference.

The word alignments were trained by GIZA++
toolkit (Och and Ney, 2000). Phrase pairs were

184

extracted using grow-diag-ﬁnal (Koehn et al.,
2007).
The baseline ruleset was obtained as
in (Chiang, 2007). The maximum phrase length
for rule extraction was set to 10, the minimum re-
quired non-terminal span was 2.

Additional rules for insertion, deletion, and
swap were added to improve the parsability of the
data, and to help EM training and rule arithmetic.
However, these rules are not used by the decoder,
since they would degrade the performance.

New rules were proposed after the ﬁrst iteration
of EM1, either by rule arithmetic or directly from
the chart.

Only non-terminal rules proposed by the rule
arithmetic from at least two different sentence
pairs and ranked (by expected counts c(r)) in the
top 100k were used. Figure 4 presents a sample of
the new rules.

New rules were also proposed directly from the
chart, using the approach in Sec. 4. 5% of best
matching parallel sequences, and 5 best scoring
rules were selected from each parallel sequence.
Non-terminal rules from the 200k-best rank were
added to the model. Figure 5 presents a sample of
the new rules.

Finally, one more iteration of EM was used to
adjust the probabilities of the new and baseline
rules. These probabilities were used as features
in the decoding.

The performance of rule arithmetic was also
veriﬁed on Farsi-English translation. The train-
ing corpus contains conversational spoken data
from the DARPA TransTac program extended
by movie subtitles and online dictionaries down-
loaded from the web (297k sentence pairs). The
punctuation was removed, and the text was low-
ercased. The dev set is 1,420 sentence pairs held
out from the training data, with one reference. The
test set provided by NIST contains 470 sentences
with 4 references. The sentences are about 30%
longer and more difﬁcult.

The training pipeline was the same as for the
German-English experiments.
122k new non-
terminal rules were proposed using the rule arith-
metic.

1Since our initial experiments did not show any signiﬁ-
cant gain from proposing rules after additional (lengthy) it-
erations of EM.

The feature weights were tuned on the dev
set for each translation model separately. The
translation quality was measured automatically by
BLEU score (Papineni et al., 2001).

6 Discussion of results

The BLEU score results are shown in the Ta-
ble 3. The cumulative gain of rule arithmetic and
EM (RA + EM-i0) is 1 BLEU point for German-
English translation and 2 BLEU points for Farsi-
English. The cumulative gain of rules proposed
from the chart (DC + EM-i0) is 0.2 BLEU points
for German-English. For comparison of effects of
various components of our method, we also show
scores after the ﬁrst ﬁve iterations of EM (EM-i0–
EM-i4) without adding any new rules, just using
EM-trained probabilities as feature weights, and
also scores for new rules added into the baseline
without adjusting their costs by EM (RA).

The qualities of proposed rules are discussed in

this section.

6.1 German-English rules from rule

arithmetic

The Figure 4 presents a sample of new rules pro-
posed during this experiment. The table is di-
vided into three parts, presenting rules from the
top, middle, and bottom of the 100K list. The
quality of the rules is high even in the middle part
of the table, the tail part is worse.

We were surprised by seeing short rules consist-
ing of frequent words. For example hum X1, in
order X1i. When looking into word-level align-
ments, we realized that these rules following the
pattern 16 prevent the baseline approach from ex-
tracting the rule.

GER: um Obj

zu V

(16)

ENG: in

order

to V Obj

Similarly many other rules match the pattern of
beginning of a subordinated clause, such as that is
why, or insertions, such as of course, which both
have to be strictly followed by VSO construction
in German, in contrast to the SVO word order in
English.

We also studied the cases of rule arithmetic cor-
recting for systematic word alignment errors. For

185

example the new rule hX1 zu koennen, to X1i was
learned from the sentence

(17)

um die

in

kyoto

vereinbarten

senkungen

beibehalten

zu koennen

in

order

to maintain

the

reductions

agreed

in

kyoto

The English translation often uses a different
modality, thus the modal verb koennen is always
aligned with null. Since unaligned words are usu-
ally not allowed at the edges of sub-phrases gener-
alized into non-terminals (Chiang, 2007), this rule
cannot be learned by the baseline.

We observe that many new proposed rules cor-
respond to patterns with a non-terminal spanning
one word. For example hum X1 zu X2, to X2
X1i corresponds to the same pattern 16, where X2
spans one verb. The line baseline min1 in the Ta-
ble 3 shows 0.3 BLEU improvement of a model
trained without the minimum non-terminal span
requirement. However, this improvement comes
at a cost of more than four times increased model
size, as shown in Table 2. We observe that us-
ing the minimum span requirement while learning
from bitext alignments combined with rule arith-
metic that can learn the most reliable rules span-
ning one word yields better performance in speed,
memory, and precision.

We can also study the new rules quantitatively.
We want to know how the rules proposed by the
rule arithmetic are used in decoding. We traced
the translation of the 1,000 test set sentences to
mark the rules that were used to generate the best
scoring hypotheses.

The stats are presented in the Table 1. The
chance that a new rules will be used in the test set
decoding (0.86%) is more than 7 times higher than
that of all rules (0.12%). Encouraging evidence is
that while the rule arithmetic rules constitute only
1.87% of total rules, they present 9.17% of rules
used in the decoding.

The Figure 1 lists the most frequently used new
rules in the decoding. We can see many rules
with 2 non-terminals that model complex verb
forms (hwird X1 haben,will have X1i), reorder-
ing in clauses (hum X1 zu gewaehrleisten, to en-
sure X1i), or reordering of verbs from the second
position in German to SVO in English (hheute X1
wir X2, today we X1 X2i).

Sentences translated
|ALL| (all rules)
|NEW| (new rules)
|NEW|
|ALL|
|hits ALL|
|glue|
|hits ALL unique|
|hits ALL unique|
|ALL|
|hits NEW|
|hits NEW unique|
|hits NEW unique|
|NEW|
|hits NEW|
|hits ALL|
|terminals from NEW|
|terminals from NEW|

RA Ger.
1,000
5.359,751
100,000
1.87%

DC Ger.
1,000
5.459,751
200,000
3.66%

RA Farsi
417
8.532,691
121,784
1.43%

10,122
2,910
6.303
0.12%

928
858
0.86%

9.17%

7,256
271
6,433
0.12%

1,541
1,504
0.75 %

2,521
267
2,058
0.02

125
110
0.09

21.23%

4.96%

4,385
4,73

7,825
5.08

407
3.26

|hits NEW|
Table 1: Rule hits for 1,000 test set.

Model
Ger-Eng baseline
Ger-Eng baseline min1

#phrases

#rules
8.5M 5.3M
8.5M 23.M

Table 2: Model sizes.

We also studied the correlation between the
rank of the proposed rules (ranked by expected
counts) and the hit rate during the decoding. The
Figure 2 measures the hit rate for each of 1,000
best ranking rules, and should be read as follows:
the rules ranking 0 to 999 were used 70 times, the
hit rate decreases as the rank grows so that there
were no hits for rules ranking 90k and more. The
rank is a good indicator of the usefulness of new
rules.

We hypothesize that the new rules are capable
of combining partial solutions to form hypothe-
ses with better word order, or better complex verb
forms so that these hypotheses are better scored
and are parts of the winning solutions more often.

6.2 German-English rules proposed directly

from the chart

We also studied why the rules proposed directly
from the bilingual chart yield smaller improve-
ment than the rule arithmetic. The number of new
rules used in the decoding (1,541) is even higher
than that of the rule arithmetic, and it constitutes
21.23% of all cases. The two experiments were

186

#hits
5
3
3
3
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2

Ger Eng

X1 stellt X2 dar X1 is X2

X1 sowohl X2 als auch X1 both X2 and

it is X2 X1

X1 ist es X2
X1 die X2 ist X1 which is X2
wird X1 haben will have X1

wir X1 damit X2 we X1 so that X2
was X1 hat X2 what X1 has X2

was X1 betrifft so
und X1 muessen wir X2
um X1 zu gewaehrleisten
um X1 zu X2
sowohl X1 als auch
sie X1 auch X2
in erster linie X1 X1 in the ﬁrst instance

as regards X1
and X1 we must X2
to ensure X1
to X2 X1
both X1 and
they also X1 X2

in X1 an
ich X1 meine
heute X1 wir X2

in X1
i X1
today we X1 X2

herr praesident X1 und herren mr president X1 and gentlemen

es muss X1 werden

gleich X1 X1 a moment
it must be X1

Figure 1: Examples of the most frequently hit
rules during the decoding.

tuned separately, so that they used different glue
rule weights. That is why we observe the differ-
ence in the number of glues (and the number of
total rules) in the Table 1. We do not observe any
signiﬁcant correlation between the rank of the rule
and the hit rate. The Figure 3 shows that the ﬁrst
10k-ranked rules are hit several times, and then
the hit rate stays ﬂat.

We offer an explanation based on our observa-
tions of rules used for the decoding. The rules
proposed directly from the chart contain a big por-
tion of content words. These rules do not capture
any important differences between the structures
of the two languages that could not be handled
by phrasal rules as well. For example, the rule
hdie neuen vorschriften sollen X1,the new rules
are X1i is correct, but a combination of a baseline
phrasal rule and glue will produce the same result.
We also see many rules with non-terminals

spanning one word. For example, the sequence

(18) die europaeische kommission—the

european commission

will produce the rule
(19) hdie X1 kommission, the X1 commissioni.
Although the sequence and the rule are high
scored by 13 and 15, we intuitively feel that gen-

Figure 2: Usage of new rules (RA).

Figure 3: Usage of new rules (DC).

eralizing the word european is not very helpful in
this context.

The rule arithmetic could propose the rule 19 as

(20) hdie X1, the X1i + hkommission,

commissioni,

but since the candidates for combination are se-
lected as rules with the highest expected counts
(Sec. 3), the rules 20 will most likely loose to the
phrase pair 18 and will not be selected.

To conclude our comparison, we observe that
both methods produce reliable rules that are of-
ten reused in decoding. Nevertheless, since the
rule arithmetic combines the most successful rules
from each parallel parse, the resulting rules enable
structural transformations that could not be han-
dled by baseline rules.

187

Model
baseline
RA + EM-i0
DC + EM-i0
EM-i0
EM-i1
EM-i2
EM-i3
EM-i4
RA
baseline min1

German-English
test set
dev set
25.4
23.9
26.4
24.8
24.6
25.6
26.1
24.4
25.8
24.4
25.9
24.4
24.4
26.0
26.0
24.4
26.1
24.4
24.0
25.7

Farsi-English

dev set
41.1
41.8

test set
38.2
40.2

40.8
41.3
41.4
41.3
41.6
40.7

39.1
38.5
38.2
39.3
39.6
38.4

Table 3: BLEU scores

6.3 Farsi-English rules from the rule

arithmetic

Although we have only limited resources to quali-
tatively analyze the Farsi-English experiments, we
noticed that there are two major groups of new
rules.

The ﬁrst group corresponds to the fact that Farsi
does not have deﬁnite article and allows pro-drop.
We observe many new rules that could not be
learned from word alignments, since some deﬁ-
nite articles or pronouns in English were aligned
to null (and unaligned words are not allowed at the
edges of phrases). However, if the chart contains
an insertion (of the determiner or pronoun) with a
high expected count, the rule arithmetic may pro-
pose new rule by combining it with other rules.

The second group contains rules that help word
reordering. We observe rules moving verbs from
the S PP O V in Farsi into SVO in English as well
as rules reordering wh-clauses.

Most of the rules traced during the test set de-
coding belong to the second group. Figure 1
shows that the number of new rules hit during
the decoding is smaller compared to the German-
English experiments. On the other hand, the rules
have smaller number of terminals so that we as-
sume that the positive effect of these rules comes
from the reordering of non-terminals.

um X1
natuerlich X1
deshalb X1
X1 zu koennen
X1 ist
nach der tagesordnung folgt die X1

in order X1
of course X1
this is why X1
to X1
it is X1
the next item is the X1

herr X1 herr kommissar X2 mr X1 commissioner X2

die X1 der X2 X1 the X2

im gegenteil X1
nach der tagesordnung folgt X1
X1 die X2
die X1 die
ausserdem X1
daher X1

on the contrary X1
the next item is X1
the X1 the X2
the X1
in addition X1
that is why X1
wir X1 nicht X2 we X1 not X2

die X1 der X2
deshalb X1
um X1 zu X2

the X2 X1
for this reason X1
to X2 X1

X1 nicht X2 werden X1 not be X2

Figure 4: Sample rules (RA).

ausserdem X1 wir we X1 also

die X1 des kommissars
den X1 ratsvorsitz
ich hoffe dass X1

the commissioner ’s X1
the X1 presidency
i would hope that X1

X1 ist zu X2 geworden X1 has become X2

die X1 des vereinigten koenigreichs

the uk X1

X1 maij weggen X2 X1 maij weggen X2
X1 wir auf X2 sind X1 we are on X2
ich frage mich X1

i wonder X1

Figure 5: Sample rules (DC).

7 Conclusion

In this work, we studied two new methods for
learning hierarchical MT rules:
the rule arith-
metic and proposing directly from the parse for-
est. We discussed systematic patterns where the
rule arithmetic outperforms alignment-based ap-
proaches and veriﬁed its signiﬁcant improvement
on two different language pairs (German-English
and Farsi-English). We also hypothesized why the
second method – proposing rules directly from the
chart – improves the baseline less than the rule
arithmetic.

Acknowledgment

This work is partially supported by the DARPA
TRANSTAC program under the contract num-
ber NBCH2030007. Any opinions, ﬁndings, and
conclusions or recommendations expressed in this
material are those of the authors and do not nec-
essarily reﬂect the views of DARPA.

188

References
Birch, Alexandra, Chris Callison-Burch, Miles Os-
borne, and Philipp Koehn. 2006. Constraining the
phrase-based, joint probability statistical translation
model.
In Proceedings on WSMT’06, pages 154–
157.

May, Jonathan and Kevin Knight. 2007. Syntactic re-
alignment models for machine translation. In Pro-
ceedings of EMNLP-CoNLL’07, pages 360–368.

Och, F. J. and H. Ney. 2000.

Improved statistical
alignment models. In Proc. of ACL, pages 440–447,
Hong Kong, China, October.

Blunsom, Phil, Trevor Cohn, Chris Dyer, and Miles
Osborne. 2009. A gibbs sampler for phrasal syn-
chronous grammar induction.
In ACL ’09, pages
782–790.

Papineni, K., S. Roukos, T. Ward, and W. Zhu. 2001.
Bleu: a method for automatic evaluation of machine
translation. Technical Report RC22176, IBM T. J.
Watson Research Center.

Cherry, Colin. 2007. Inversion transduction grammar
for joint phrasal translation modeling. In NAACL-
HLT’07/SSST’07.

Wu, Dekai. 1997. Stochastic inversion transduction
grammars and bilingual parsing of parallel corpora.
Computational Linguistics, 23(3):377–403.

Zhou, Bowen, Bing Xiang, Xiaodan Zhu, and Yuqing
Gao. 2008. Prior derivation models for formally
syntax-based translation using linguistically syntac-
tic parsing and tree kernels. In Proceedings of the
ACL’08: HLT SSST-2, pages 19–27.

Chiang, David.

A hierarchical phrase-
based model for statistical machine translation. In
ACL’05, pages 263–270.

2005.

Chiang, David.

2007. Hierarchical phrase-based

translation. Comput. Linguist., 33(2):201–228.

Cmejrek, Martin, Bowen Zhou, and Bing Xiang. 2009.
Enriching SCFG rules directly from efﬁcient bilin-
gual chart parsing. In IWSLT’09, pages 136–143.

DeNero, John, Alexandre Bouchard-Cˆot´e, and Dan
Klein. 2008. Sampling alignment structure under
a bayesian translation model. In EMNLP ’08, pages
314–323.

Galley, Michel, Jonathan Graehl, Kevin Knight, Daniel
Marcu, Steve DeNeefe, Wei Wang, and Ignacio
Thayer. 2006. Scalable inference and training of
context-rich syntactic translation models. In Proc.
of ACL, pages 961–968.

Huang, Songfang and Bowen Zhou. 2009. An EM
algorithm for SCFG in formal syntax-based transla-
tion. In Proc. IEEE ICASSP’09, pages 4813–4816.

Koehn, Philipp, Hieu Hoang, Alexandra Birch,
Chris Callison-Burch, Marcello Federico, Nicola
Bertoldi, Brooke Cowan, Wade Shen, Christine
Moran, Richard Zens, Chris Dyer, Ondrej Bojar,
Alexandra Constantin, and Evan Herbst.
2007.
Moses: Open source toolkit for statistical machine
translation. In ACL.

Koehn, Philipp. 2005. Europarl: A parallel corpus
for statistical machine translation. In Proceedings
of MT Summit.

Liu, Ding and Daniel Gildea. 2009. Bayesian learning
of phrasal tree-to-string templates. In EMNLP ’09,
pages 1308–1317.

Marcu, Daniel and W Wong. 2002. A phrase-based,
joint probability model for statistical machine trans-
lation. In Proceedings of EMNLP’02.

