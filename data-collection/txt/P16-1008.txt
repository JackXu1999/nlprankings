



















































Modeling Coverage for Neural Machine Translation


Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 76–85,
Berlin, Germany, August 7-12, 2016. c©2016 Association for Computational Linguistics

Modeling Coverage for Neural Machine Translation

Zhaopeng Tu† Zhengdong Lu† Yang Liu‡ Xiaohua Liu† Hang Li†

†Noah’s Ark Lab, Huawei Technologies, Hong Kong
{tu.zhaopeng,lu.zhengdong,liuxiaohua3,hangli.hl}@huawei.com
‡Department of Computer Science and Technology, Tsinghua University, Beijing

liuyang2011@tsinghua.edu.cn

Abstract

Attention mechanism has enhanced state-
of-the-art Neural Machine Translation
(NMT) by jointly learning to align and
translate. It tends to ignore past alignment
information, however, which often leads
to over-translation and under-translation.
To address this problem, we propose
coverage-based NMT in this paper. We
maintain a coverage vector to keep track
of the attention history. The coverage vec-
tor is fed to the attention model to help ad-
just future attention, which lets NMT sys-
tem to consider more about untranslated
source words. Experiments show that
the proposed approach significantly im-
proves both translation quality and align-
ment quality over standard attention-based
NMT.1

1 Introduction

The past several years have witnessed the rapid
progress of end-to-end Neural Machine Transla-
tion (NMT) (Sutskever et al., 2014; Bahdanau et
al., 2015). Unlike conventional Statistical Ma-
chine Translation (SMT) (Koehn et al., 2003; Chi-
ang, 2007), NMT uses a single and large neural
network to model the entire translation process. It
enjoys the following advantages. First, the use of
distributed representations of words can alleviate
the curse of dimensionality (Bengio et al., 2003).
Second, there is no need to explicitly design fea-
tures to capture translation regularities, which is
quite difficult in SMT. Instead, NMT is capable of
learning representations directly from the training
data. Third, Long Short-Term Memory (Hochre-
iter and Schmidhuber, 1997) enables NMT to cap-

1Our code is publicly available at https://github.
com/tuzhaopeng/NMT-Coverage.

ture long-distance reordering, which is a signifi-
cant challenge in SMT.

NMT has a serious problem, however, namely
lack of coverage. In phrase-based SMT (Koehn
et al., 2003), a decoder maintains a coverage vec-
tor to indicate whether a source word is translated
or not. This is important for ensuring that each
source word is translated in decoding. The decod-
ing process is completed when all source words
are “covered” or translated. In NMT, there is no
such coverage vector and the decoding process
ends only when the end-of-sentence mark is pro-
duced. We believe that lacking coverage might
result in the following problems in conventional
NMT:

1. Over-translation: some words are unneces-
sarily translated for multiple times;

2. Under-translation: some words are mistak-
enly untranslated.

Specifically, in the state-of-the-art attention-based
NMT model (Bahdanau et al., 2015), generating a
target word heavily depends on the relevant parts
of the source sentence, and a source word is in-
volved in generation of all target words. As a
result, over-translation and under-translation in-
evitably happen because of ignoring the “cover-
age” of source words (i.e., number of times a
source word is translated to a target word). Fig-
ure 1(a) shows an example: the Chinese word
“guānbı̀” is over translated to “close(d)” twice,
while “bèipò” (means “be forced to”) is mistak-
enly untranslated.

In this work, we propose a coverage mechanism
to NMT (NMT-COVERAGE) to alleviate the over-
translation and under-translation problems. Basi-
cally, we append a coverage vector to the inter-
mediate representations of an NMT model, which
are sequentially updated after each attentive read

76



(a) Over-translation and under-translation
generated by NMT.

(b) Coverage model alleviates the problems of
over-translation and under-translation.

Figure 1: Example translations of (a) NMT without coverage, and (b) NMT with coverage. In conven-
tional NMT without coverage, the Chinese word “guānbı̀” is over translated to “close(d)” twice, while
“bèipò” (means “be forced to”) is mistakenly untranslated. Coverage model alleviates these problems by
tracking the “coverage” of source words.

during the decoding process, to keep track of the
attention history. The coverage vector, when en-
tering into attention model, can help adjust the fu-
ture attention and significantly improve the over-
all alignment between the source and target sen-
tences. This design contains many particular cases
for coverage modeling with contrasting character-
istics, which all share a clear linguistic intuition
and yet can be trained in a data driven fashion. No-
tably, we achieve significant improvement even by
simply using the sum of previous alignment prob-
abilities as coverage for each word, as a success-
ful example of incorporating linguistic knowledge
into neural network based NLP models.

Experiments show that NMT-COVERAGE sig-
nificantly outperforms conventional attention-
based NMT on both translation and alignment
tasks. Figure 1(b) shows an example, in which
NMT-COVERAGE alleviates the over-translation
and under-translation problems that NMT without
coverage suffers from.

2 Background

Our work is built on attention-based NMT (Bah-
danau et al., 2015), which simultaneously con-
ducts dynamic alignment and generation of the
target sentence, as illustrated in Figure 2. It

Figure 2: Architecture of attention-based NMT.
Whenever possible, we omit the source index j to
make the illustration less cluttered.

produces the translation by generating one target
word yi at each time step. Given an input sentence
x = {x1, . . . , xJ} and previously generated words
{y1, . . . , yi−1}, the probability of generating next
word yi is

P (yi|y<i,x) = softmax
(
g(yi−1, ti, si)

)
(1)

where g is a non-linear function, and ti is a decod-
ing state for time step i, computed by

ti = f(ti−1, yi−1, si) (2)

Here the activation function f(·) is a Gated Re-
current Unit (GRU) (Cho et al., 2014b), and si is

77



a distinct source representation for time i, calcu-
lated as a weighted sum of the source annotations:

si =
J∑
j=1

αi,j · hj (3)

where hj = [
−→
h >j ;
←−
h >j ]

>
is the annotation of

xj from a bi-directional Recurrent Neural Net-
work (RNN) (Schuster and Paliwal, 1997), and its
weight αi,j is computed by

αi,j =
exp(ei,j)∑J
k=1 exp(ei,k)

(4)

and

ei,j = a(ti−1,hj)

= v>a tanh(Wati−1 + Uahj) (5)

is an attention model that scores how well yi and
hj match. With the attention model, it avoids the
need to represent the entire source sentence with
a single vector. Instead, the decoder selects parts
of the source sentence to pay attention to, thus
exploits an expected annotation si over possible
alignments αi,j for each time step i.

However, the attention model fails to take ad-
vantage of past alignment information, which is
found useful to avoid over-translation and under-
translation problems in conventional SMT (Koehn
et al., 2003). For example, if a source word is
translated in the past, it is less likely to be trans-
lated again and should be assigned a lower align-
ment probability.

3 Coverage Model for NMT

In SMT, a coverage set is maintained to keep track
of which source words have been translated (“cov-
ered”) in the past. Let us take x = {x1, x2, x3, x4}
as an example of input sentence. The initial cov-
erage set is C = {0, 0, 0, 0} which denotes that
no source word is yet translated. When a trans-
lation rule bp = (x2x3, ymym+1) is applied, we
produce one hypothesis labelled with coverage
C = {0, 1, 1, 0}. It means that the second and third
source words are translated. The goal is to gener-
ate translation with full coverage C = {1, 1, 1, 1}.
A source word is translated when it is covered by
one translation rule, and it is not allowed to be
translated again in the future (i.e., hard coverage).
In this way, each source word is guaranteed to be
translated and only be translated once. As shown,

Figure 3: Architecture of coverage-based attention
model. A coverage vector Ci−1 is maintained to
keep track of which source words have been trans-
lated before time i. Alignment decisions αi are
made jointly taking into account past alignment
information embedded in Ci−1, which lets the at-
tention model to consider more about untranslated
source words.

coverage is essential for SMT since it avoids gaps
and overlaps in translation of source words.

Modeling coverage is also important for
attention-based NMT models, since they gener-
ally lack a mechanism to indicate whether a cer-
tain source word has been translated, and there-
fore are prone to the “coverage” mistakes: some
parts of source sentence have been translated more
than once or not translated. For NMT models, di-
rectly modeling coverage is less straightforward,
but the problem can be significantly alleviated by
keeping track of the attention signal during the de-
coding process. The most natural way for doing
that would be to append a coverage vector to the
annotation of each source word (i.e., hj), which
is initialized as a zero vector but updated after ev-
ery attentive read of the corresponding annotation.
The coverage vector is fed to the attention model
to help adjust future attention, which lets NMT
system to consider more about untranslated source
words, as illustrated in Figure 3.

3.1 Coverage Model

Since the coverage vector summarizes the atten-
tion record for hj (and therefore for a small neigh-
bor centering at the jth source word), it will
discourage further attention to it if it has been
heavily attended, and implicitly push the atten-
tion to the less attended segments of the source
sentence since the attention weights are normal-
ized to one. This can potentially solve both cover-
age mistakes mentioned above, when modeled and
learned properly.

78



Formally, the coverage model is given by

Ci,j = gupdate
(Ci−1,j , αi,j ,Φ(hj),Ψ) (6)

where

• gupdate(·) is the function that updates Ci,j af-
ter the new attention αi,j at time step i in the
decoding process;

• Ci,j is a d-dimensional coverage vector sum-
marizing the history of attention till time step
i on hj ;

• Φ(hj) is a word-specific feature with its own
parameters;

• Ψ are auxiliary inputs exploited in different
sorts of coverage models.

Equation 6 gives a rather general model, which
could take different function forms for gupdate(·)
and Φ(·), and different auxiliary inputs Ψ (e.g.,
previous decoding state ti−1). In the rest of this
section, we will give a number of representative
implementations of the coverage model, which
either leverage more linguistic information (Sec-
tion 3.1.1) or resort to the flexibility of neural net-
work approximation (Section 3.1.2).

3.1.1 Linguistic Coverage Model
We first consider at linguistically inspired model
which has a small number of parameters, as well
as clear interpretation. While the linguistically-
inspired coverage in NMT is similar to that in
SMT, there is one key difference: it indicates what
percentage of source words have been translated
(i.e., soft coverage). In NMT, each target word yi
is generated from all source words with probabil-
ity αi,j for source word xj . In other words, the
source word xj is involved in generating all tar-
get words and the probability of generating target
word yi at time step i is αi,j . Note that unlike
in SMT in which each source word is fully trans-
lated at one decoding step, the source word xj is
partially translated at each decoding step in NMT.
Therefore, the coverage at time step i denotes the
translated ratio of that each source word is trans-
lated.

We use a scalar (d = 1) to represent linguis-
tic coverage for each source word and employ
an accumulate operation for gupdate. The initial
value of linguistic coverage is zero, which de-
notes that the corresponding source word is not

translated yet. We iteratively construct linguis-
tic coverages through accumulation of alignment
probabilities generated by the attention model,
each of which is normalized by a distinct context-
dependent weight. The coverage of source word
xj at time step i is computed by

Ci,j = Ci−1,j + 1Φj αi,j =
1

Φj

i∑
k=1

αk,j (7)

where Φj is a pre-defined weight which indicates
the number of target words xj is expected to gener-
ate. The simplest way is to follow Xu et al. (2015)
in image-to-caption translation to fix Φ = 1 for all
source words, which means that we directly use
the sum of previous alignment probabilities with-
out normalization as coverage for each word, as
done in (Cohn et al., 2016).

However, in machine translation, different types
of source words may contribute differently to the
generation of target sentence. Let us take the
sentence pairs in Figure 1 as an example. The
noun in the source sentence “jı̄chǎng” is translated
into one target word “airports”, while the adjec-
tive “bèipò” is translated into three words “were
forced to”. Therefore, we need to assign a dis-
tinct Φj for each source word. Ideally, we expect
Φj =

∑I
i=1 αi,j with I being the total number

of time steps in decoding. However, such desired
value is not available before decoding, thus is not
suitable in this scenario.

Fertility To predict Φj , we introduce the con-
cept of fertility, which is firstly proposed in word-
level SMT (Brown et al., 1993). Fertility of source
word xj tells how many target words xj produces.
In SMT, the fertility is a random variable Φj ,
whose distribution p(Φj = φ) is determined by
the parameters of word alignment models (e.g.,
IBM models). In this work, we simplify and adapt
fertility from the original model and compute the
fertility Φj by2

Φj = N (xj |x) = N · σ(Ufhj) (8)

where N ∈ R is a predefined constant to denote
the maximum number of target words one source

2Fertility in SMT is a random variable with a set of fer-
tility probabilities, n(Φj |xj) = p(Φ<j ,x), which depends
on the fertilities of previous source words. To simplify the
calculation and adapt it to the attention model in NMT, we
define the fertility in NMT as a constant number, which is
independent of previous fertilities.

79



Figure 4: NN-based coverage model.

word can produce, σ(·) is a logistic sigmoid func-
tion, and Uf ∈ R1×2n is the weight matrix. Here
we use hj to denote (xj |x) since hj contains in-
formation about the whole input sentence with a
strong focus on the parts surrounding xj (Bah-
danau et al., 2015). Since Φj does not depend on
i, we can pre-compute it before decoding to mini-
mize the computational cost.

3.1.2 Neural Network Based Coverage Model
We next consider Neural Network (NN) based
coverage model. When Ci,j is a vector (d > 1) and
gupdate(·) is a neural network, we actually have
an RNN model for coverage, as illustrated in Fig-
ure 4. In this work, we take the following form:

Ci,j = f(Ci−1,j , αi,j ,hj , ti−1)
where f(·) is a nonlinear activation function and
ti−1 is the auxiliary input that encodes past trans-
lation information. Note that we leave out the
word-specific feature function Φ(·) and only take
the input annotation hj as the input to the cov-
erage RNN. It is important to emphasize that the
NN-based coverage model is able to be fed with
arbitrary inputs, such as the previous attentional
context si−1. Here we only employ Ci−1,j for past
alignment information, ti−1 for past translation in-
formation, and hj for word-specific bias.3

Gating The neural function f(·) can be either a
simple activation function tanh or a gating func-
tion that proves useful to capture long-distance

3In our preliminary experiments, considering more inputs
(e.g., current and previous attentional contexts, unnormal-
ized attention weights ei,j) does not always lead to better
translation quality. Possible reasons include: 1) the inputs
contains duplicate information, and 2) more inputs introduce
more back-propagation paths and therefore make it difficult
to train. In our experience, one principle is to only feed
the coverage model inputs that contain distinct information,
which are complementary to each other.

dependencies. In this work, we adopt GRU for
the gating activation since it is simple yet power-
ful (Chung et al., 2014). Please refer to (Cho et al.,
2014b) for more details about GRU.

Discussion Intuitively, the two types of models
summarize coverage information in “different lan-
guages”. Linguistic models summarize coverage
information in human language, which has a clear
interpretation to humans. Neural models encode
coverage information in “neural language”, which
can be “understood” by neural networks and let
them to decide how to make use of the encoded
coverage information.

3.2 Integrating Coverage into NMT

Although attention based model has the capabil-
ity of jointly making alignment and translation, it
does not take into consideration translation his-
tory. Specifically, a source word that has sig-
nificantly contributed to the generation of target
words in the past, should be assigned lower align-
ment probabilities, which may not be the case in
attention based NMT. To address this problem, we
propose to calculate the alignment probabilities by
incorporating past alignment information embed-
ded in the coverage model.

Intuitively, at each time step i in the decoding
phase, coverage from time step (i − 1) serves as
an additional input to the attention model, which
provides complementary information of that how
likely the source words are translated in the past.
We expect the coverage information would guide
the attention model to focus more on untranslated
source words (i.e., assign higher alignment prob-
abilities). In practice, we find that the coverage
model does fulfill the expectation (see Section 5).
The translated ratios of source words from lin-
guistic coverages negatively correlate to the cor-
responding alignment probabilities.

More formally, we rewrite the attention model
in Equation 5 as

ei,j = a(ti−1,hj , Ci−1,j)
= v>a tanh(Wati−1 + Uahj + VaCi−1,j)

where Ci−1,j is the coverage of source word xj be-
fore time i. Va ∈ Rn×d is the weight matrix for
coverage with n and d being the numbers of hid-
den units and coverage units, respectively.

80



4 Training

We take end-to-end learning for the NMT-
COVERAGE model, which learns not only the pa-
rameters for the “original” NMT (i.e., θ for encod-
ing RNN, decoding RNN, and attention model)
but also the parameters for coverage modeling
(i.e., η for annotation and guidance of attention) .
More specifically, we choose to maximize the like-
lihood of reference sentences as most other NMT
models (see, however (Shen et al., 2016)):

(θ∗, η∗) = arg max
θ,η

N∑
n=1

logP (yn|xn; θ, η) (9)

No auxiliary objective For the coverage model
with a clearer linguistic interpretation (Section
3.1.1), it is possible to inject an auxiliary objec-
tive function on some intermediate representation.
More specifically, we may have the following ob-
jective:

(θ∗, η∗) = arg max
θ,η

N∑
n=1

{
logP (yn|xn; θ, η)

− λ
{ J∑
j=1

(Φj −
I∑
i=1

αi,j)2; η
}}

where the term
{∑J

j=1(Φj −
∑I

i=1 αi,j)
2; η
}

pe-
nalizes the discrepancy between the sum of align-
ment probabilities and the expected fertility for
linguistic coverage. This is similar to the more
explicit training for fertility as in Xu et al. (2015),
which encourages the model to pay equal attention
to every part of the image (i.e., Φj = 1). However,
our empirical study shows that the combined ob-
jective consistently worsens the translation quality
while slightly improves the alignment quality.

Our training strategy poses less constraints on
the dependency between Φj and the attention than
a more explicit strategy taken in (Xu et al., 2015).
We let the objective associated with the transla-
tion quality (i.e., the likelihood) to drive the train-
ing, as in Equation 9. This strategy is arguably
advantageous, since the attention weight on a hid-
den state hj cannot be interpreted as the propor-
tion of the corresponding word being translated in
the target sentence. For one thing, the hidden state
hj , after the transformation from encoding RNN,
bears the contextual information from other parts
of the source sentence, and thus loses the rigid cor-
respondence with the corresponding word. There-
fore, penalizing the discrepancy between the sum

of alignment probabilities and the expected fertil-
ity does not hold in this scenario.

5 Experiments

5.1 Setup
We carry out experiments on a Chinese-English
translation task. Our training data for the trans-
lation task consists of 1.25M sentence pairs ex-
tracted from LDC corpora4 , with 27.9M Chinese
words and 34.5M English words respectively. We
choose NIST 2002 dataset as our development set,
and the NIST 2005, 2006 and 2008 datasets as our
test sets. We carry out experiments of the align-
ment task on the evaluation dataset from (Liu and
Sun, 2015), which contains 900 manually aligned
Chinese-English sentence pairs. We use the case-
insensitive 4-gram NIST BLEU score (Papineni et
al., 2002) for the translation task, and the align-
ment error rate (AER) (Och and Ney, 2003) for
the alignment task. To better estimate the qual-
ity of the soft alignment probabilities generated
by NMT, we propose a variant of AER, naming
SAER:

SAER = 1− |MA ×MS |+ |MA ×MP ||MA|+ |MS |
where A is a candidate alignment, and S and P
are the sets of sure and possible links in the ref-
erence alignment respectively (S ⊆ P ). M de-
notes alignment matrix, and for both MS and MP
we assign the elements that correspond to the ex-
isting links in S and P with probabilities 1 while
assign the other elements with probabilities 0. In
this way, we are able to better evaluate the quality
of the soft alignments produced by attention-based
NMT. We use sign-test (Collins et al., 2005) for
statistical significance test.

For efficient training of the neural networks, we
limit the source and target vocabularies to the most
frequent 30K words in Chinese and English, cov-
ering approximately 97.7% and 99.3% of the two
corpora respectively. All the out-of-vocabulary
words are mapped to a special token UNK. We set
N = 2 for the fertility model in the linguistic cov-
erages. We train each model with the sentences
of length up to 80 words in the training data. The
word embedding dimension is 620 and the size of
a hidden layer is 1000. All the other settings are
the same as in (Bahdanau et al., 2015).

4The corpora include LDC2002E18, LDC2003E07,
LDC2003E14, Hansards portion of LDC2004T07,
LDC2004T08 and LDC2005T06.

81



# System #Params MT05 MT06 MT08 Avg.
1 Moses – 31.37 30.85 23.01 28.41
2 GroundHog 84.3M 30.61 31.12 23.23 28.32
3 + Linguistic coverage w/o fertility +1K 31.26† 32.16†‡ 24.84†‡ 29.42
4 + Linguistic coverage w/ fertility +3K 32.36†‡ 32.31†‡ 24.91†‡ 29.86
5 + NN-based coverage w/o gating (d = 1) +4K 31.94†‡ 32.11†‡ 23.31 29.12
6 + NN-based coverage w/ gating (d = 1) +10K 31.94†‡ 32.16†‡ 24.67†‡ 29.59
7 + NN-based coverage w/ gating (d = 10) +100K 32.73†‡ 32.47†‡ 25.23†‡ 30.14

Table 1: Evaluation of translation quality. d denotes the dimension of NN-based coverages, and † and ‡
indicate statistically significant difference (p < 0.01) from GroundHog and Moses, respectively. “+” is
on top of the baseline system GroundHog.

We compare our method with two state-of-the-
art models of SMT and NMT5:

• Moses (Koehn et al., 2007): an open source
phrase-based translation system with default
configuration and a 4-gram language model
trained on the target portion of training data.

• GroundHog (Bahdanau et al., 2015): an
attention-based NMT system.

5.2 Translation Quality
Table 1 shows the translation performances mea-
sured in BLEU score. Clearly the proposed NMT-
COVERAGE significantly improves the translation
quality in all cases, although there are still consid-
erable differences among different variants.

Parameters Coverage model introduces few pa-
rameters. The baseline model (i.e., GroundHog)
has 84.3M parameters. The linguistic coverage
using fertility introduces 3K parameters (2K for
fertility model), and the NN-based coverage with
gating introduces 10K×d parameters (6K×d for
gating), where d is the dimension of the coverage
vector. In this work, the most complex coverage
model only introduces 0.1M additional parame-
ters, which is quite small compared to the number
of parameters in the existing model (i.e., 84.3M).

Speed Introducing the coverage model slows
down the training speed, but not significantly.
When running on a single GPU device Tesla K80,
the speed of the baseline model is 960 target words
per second. System 4 (“+Linguistic coverage with
fertility”) has a speed of 870 words per second,
while System 7 (“+NN-based coverage (d=10)”)
achieves a speed of 800 words per second.

5There are recent progress on aggregating multiple mod-
els or enlarging the vocabulary(e.g., in (Jean et al., 2015)),
but here we focus on the generic models.

Linguistic Coverages (Rows 3 and 4): Two
observations can be made. First, the simplest
linguistic coverage (Row 3) already significantly
improves translation performance by 1.1 BLEU
points, indicating that coverage information is
very important to the attention model. Second, in-
corporating fertility model boosts the performance
by better estimating the covered ratios of source
words.

NN-based Coverages (Rows 5-7): (1) Gating
(Rows 5 and 6): Both variants of NN-based cover-
ages outperform GroundHog with averaged gains
of 0.8 and 1.3 BLEU points, respectively. In-
troducing gating activation function improves the
performance of coverage models, which is consis-
tent with the results in other tasks (Chung et al.,
2014). (2) Coverage dimensions (Rows 6 and 7):
Increasing the dimension of coverage models fur-
ther improves the translation performance by 0.6
point in BLEU score, at the cost of introducing
more parameters (e.g., from 10K to 100K).6

5.3 Alignment Quality

Table 2 lists the alignment performances. We
find that coverage information improves atten-
tion model as expected by maintaining an annota-
tion summarizing attention history on each source
word. More specifically, linguistic coverage with
fertility significantly reduces alignment errors un-
der both metrics, in which fertility plays an impor-
tant role. NN-based coverages, however, does not
significantly reduce alignment errors until increas-
ing the coverage dimension from 1 to 10. It in-
dicates that NN-based models need slightly more

6In a pilot study, further increasing the coverage dimen-
sion only slightly improved the translation performance. One
possible reason is that encoding the relatively simple cover-
age information does not require too many dimensions.

82



(a) Groundhog (b) + NN cov. w/ gating (d = 10)

Figure 5: Example alignments. Using coverage mechanism, translated source words are less likely to
contribute to generation of the target words next (e.g., top-right corner for the first four Chinese words.).

System SAER AER
GroundHog 67.00 54.67
+ Ling. cov. w/o fertility 66.75 53.55
+ Ling. cov. w/ fertility 64.85 52.13
+ NN cov. w/o gating (d = 1) 67.10 54.46
+ NN cov. w/ gating (d = 1) 66.30 53.51
+ NN cov. w/ gating (d = 10) 64.25 50.50

Table 2: Evaluation of alignment quality. The
lower the score, the better the alignment quality.

dimensions to encode the coverage information.
Figure 5 shows an example. The coverage

mechanism does meet the expectation: the align-
ments are more concentrated and most impor-
tantly, translated source words are less likely to
get involved in generation of the target words next.
For example, the first four Chinese words are as-
signed lower alignment probabilities (i.e., darker
color) after the corresponding translation “roma-
nia reinforces old buildings” is produced.

5.4 Effects on Long Sentences
Following Bahdanau et al. (2015), we group sen-
tences of similar lengths together and compute
BLEU score and averaged length of translation
for each group, as shown in Figure 6. Cho et
al. (2014a) show that the performance of Ground-
hog drops rapidly when the length of input sen-
tence increases. Our results confirm these find-
ings. One main reason is that Groundhog pro-
duces much shorter translations on longer sen-
tences (e.g., > 40, see right panel in Figure 6),

and thus faces a serious under-translation prob-
lem. NMT-COVERAGE alleviates this problem by
incorporating coverage information into the atten-
tion model, which in general pushes the attention
to untranslated parts of the source sentence and
implicitly discourages early stop of decoding. It
is worthy to emphasize that both NN-based cov-
erages (with gating, d = 10) and linguistic cover-
ages (with fertility) achieve similar performances
on long sentences, reconfirming our claim that the
two variants improve the attention model in their
own ways.

As an example, consider this source sentence in
the test set:

qiáodān běn sàijı̀ pı́ngjūn défēn 24.3fēn
, tā zài sān zhōu qián jiēshòu shǒushù
, qiúduı̀ zài cı̌ qı̄jiān 4 shèng 8 fù .

Groundhog translates this sentence into:

jordan achieved an average score of
eight weeks ahead with a surgical oper-
ation three weeks ago .

in which the sub-sentence “, qiúduı̀ zài cı̌ qı̄jiān
4 shèng 8 fù” is under-translated. With the (NN-
based) coverage mechanism, NMT-COVERAGE
translates it into:

jordan ’s average score points to UNK
this year . he received surgery before
three weeks , with a team in the period
of 4 to 8 .

83



Figure 6: Performance of the generated translations with respect to the lengths of the input sentences.
Coverage models alleviate under-translation by producing longer translations on long sentences.

in which the under-translation is rectified.
The quantitative and qualitative results show

that the coverage models indeed help to allevi-
ate under-translation, especially for long sentences
consisting of several sub-sentences.

6 Related Work

Our work is inspired by recent works on im-
proving attention-based NMT with techniques that
have been successfully applied to SMT. Follow-
ing the success of Minimum Risk Training (MRT)
in SMT (Och, 2003), Shen et al. (2016) proposed
MRT for end-to-end NMT to optimize model pa-
rameters directly with respect to evaluation met-
rics. Based on the observation that attention-
based NMT only captures partial aspects of atten-
tional regularities, Cheng et al. (2016) proposed
agreement-based learning (Liang et al., 2006) to
encourage bidirectional attention models to agree
on parameterized alignment matrices. Along the
same direction, inspired by the coverage mecha-
nism in SMT, we propose a coverage-based ap-
proach to NMT to alleviate the over-translation
and under-translation problems.

Independent from our work, Cohn et al. (2016)
and Feng et al. (2016) made use of the concept of
“fertility” for the attention model, which is sim-
ilar in spirit to our method for building the lin-
guistically inspired coverage with fertility. Cohn
et al. (2016) introduced a feature-based fertility
that includes the total alignment scores for the sur-

rounding source words. In contrast, we make pre-
diction of fertility before decoding, which works
as a normalizer to better estimate the coverage ra-
tio of each source word. Feng et al. (2016) used
the previous attentional context to represent im-
plicit fertility and passed it to the attention model,
which is in essence similar to the input-feed
method proposed in (Luong et al., 2015). Compar-
atively, we predict explicit fertility for each source
word based on its encoding annotation, and incor-
porate it into the linguistic-inspired coverage for
attention model.

7 Conclusion

We have presented an approach for enhancing
NMT, which maintains and utilizes a coverage
vector to indicate whether each source word is
translated or not. By encouraging NMT to pay less
attention to translated words and more attention to
untranslated words, our approach alleviates the se-
rious over-translation and under-translation prob-
lems that traditional attention-based NMT suffers
from. We propose two variants of coverage mod-
els: linguistic coverage that leverages more lin-
guistic information and NN-based coverage that
resorts to the flexibility of neural network approx-
imation . Experimental results show that both
variants achieve significant improvements in terms
of translation quality and alignment quality over
NMT without coverage.

84



Acknowledgement

This work is supported by China National 973
project 2014CB340301. Yang Liu is supported
by the National Natural Science Foundation of
China (No. 61522204) and the 863 Program
(2015AA011808). We thank the anonymous re-
viewers for their insightful comments.

References
[Bahdanau et al.2015] Dzmitry Bahdanau, Kyunghyun

Cho, and Yoshua Bengio. 2015. Neural machine
translation by jointly learning to align and translate.
ICLR 2015.

[Bengio et al.2003] Yoshua Bengio, Réjean Ducharme,
Pascal Vincent, and Christian Janvin. 2003. A neu-
ral probabilistic language model. JMLR.

[Brown et al.1993] Peter E. Brown, Stephen A. Della
Pietra, Vincent J. Della Pietra, and Robert L. Mer-
cer. 1993. The mathematics of statistical machine
translation: Parameter estimation. Computational
Linguistics, 19(2):263–311.

[Cheng et al.2016] Yong Cheng, Shiqi Shen, Zhongjun
He, Wei He, Hua Wu, Maosong Sun, and Yang Liu.
2016. Agreement-based Joint Training for Bidirec-
tional Attention-based Neural Machine Translation.
In IJCAI 2016.

[Chiang2007] David Chiang. 2007. Hierarchical
phrase-based translation. CL.

[Cho et al.2014a] Kyunghyun Cho, Bart van Merrien-
boer, Dzmitry Bahdanau, and Yoshua Bengio.
2014a. On the properties of neural machine trans-
lation: encoder–decoder approaches. In SSST 2014.

[Cho et al.2014b] Kyunghyun Cho, Bart van Merrien-
boer, Caglar Gulcehre, Fethi Bougares, Holger
Schwenk, and Yoshua Bengio. 2014b. Learning
phrase representations using rnn encoder-decoder
for statistical machine translation. In EMNLP 2014.

[Chung et al.2014] Junyoung Chung, Caglar Gulcehre,
KyungHyun Cho, and Yoshua Bengio. 2014. Em-
pirical evaluation of gated recurrent neural networks
on sequence modeling. arXiv.

[Cohn et al.2016] Trevor Cohn, Cong Duy Vu Hoang,
Ekaterina Vylomova, Kaisheng Yao, Chris Dyer,
and Gholamreza Haffari. 2016. Incorporating
Structural Alignment Biases into an Attentional
Neural Translation Model. In NAACL 2016.

[Collins et al.2005] Michael Collins, Philipp Koehn,
and Ivona Kučerová. 2005. Clause restructuring for
statistical machine translation. In ACL 2005.

[Feng et al.2016] Shi Feng, Shujie Liu, Mu Li, and
Ming Zhou. 2016. Implicit distortion and fertil-
ity models for attention-based encoder-decoder nmt
model. arXiv.

[Hochreiter and Schmidhuber1997] Sepp Hochreiter
and Jürgen Schmidhuber. 1997. Long short-term
memory. Neural Computation.

[Jean et al.2015] Sébastien Jean, Kyunghyun Cho,
Roland Memisevic, and Yoshua Bengio. 2015. On
using very large target vocabulary for neural ma-
chine translation. In ACL 2015.

[Koehn et al.2003] Philipp Koehn, Franz Josef Och,
and Daniel Marcu. 2003. Statistical phrase-based
translation. In NAACL 2003.

[Koehn et al.2007] Philipp Koehn, Hieu Hoang,
Alexandra Birch, Chris Callison-Burch, Marcello
Federico, Nicola Bertoldi, Brooke Cowan, Wade
Shen, Christine Moran, Richard Zens, Chris Dyer,
Ondrej Bojar, Alexandra Constantin, and Evan
Herbst. 2007. Moses: open source toolkit for
statistical machine translation. In ACL 2007.

[Liang et al.2006] Percy Liang, Ben Taskar, and Dan
Klein. 2006. Alignment by agreement. In NAACL
2006.

[Liu and Sun2015] Yang Liu and Maosong Sun. 2015.
Contrastive unsupervised word alignment with non-
local features. In AAAI 2015.

[Luong et al.2015] Minh-Thang Luong, Hieu Pham,
and Christopher D Manning. 2015. Effective ap-
proaches to attention-based neural machine transla-
tion. In EMNLP 2015.

[Och and Ney2003] Franz J. Och and Hermann Ney.
2003. A systematic comparison of various statisti-
cal alignment models. Computational Linguistics,
29(1):19–51.

[Och2003] Franz Josef Och. 2003. Minimum error rate
training in statistical machine translation. In ACL
2003.

[Papineni et al.2002] Kishore Papineni, Salim Roukos,
Todd Ward, and Wei-Jing Zhu. 2002. Bleu: a
method for automatic evaluation of machine trans-
lation. In ACL 2002.

[Schuster and Paliwal1997] Mike Schuster and
Kuldip K Paliwal. 1997. Bidirectional recur-
rent neural networks. IEEE Transactions on Signal
Processing, 45(11):2673–2681.

[Shen et al.2016] Shiqi Shen, Yong Cheng, Zhongjun
He, Wei He, Hua Wu, Maosong Sun, and Yang Liu.
2016. Minimum Risk Training for Neural Machine
Translation. In ACL 2016.

[Sutskever et al.2014] Ilya Sutskever, Oriol Vinyals,
and Quoc VV Le. 2014. Sequence to sequence
learning with neural networks. In NIPS 2014.

[Xu et al.2015] Kelvin Xu, Jimmy Ba, Ryan Kiros,
Kyunghyun Cho, Aaron Courville, Ruslan Salakhut-
dinov, Richard Zemel, and Yoshua Bengio. 2015.
Show, Attend and Tell: Neural Image Caption Gen-
eration with Visual Attention. In ICML 2015.

85


