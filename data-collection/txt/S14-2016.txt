



















































Bielefeld SC: Orthonormal Topic Modelling for Grammar Induction


Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 119–122,
Dublin, Ireland, August 23-24, 2014.

Bielefeld SC: Orthonormal Topic Modelling for Grammar Induction

John P. McCrae
CITEC, Bielefeld University

Inspiration 1
Bielefeld, Germany

jmccrae@cit-ec.uni-bielefeld.de

Philipp Cimiano
CITEC, Bielefeld University

Inspiration 1
Bielefeld, Germany

cimiano@cit-ec.uni-bielefeld.de

Abstract

In this paper, we consider the application
of topic modelling to the task of induct-
ing grammar rules. In particular, we look
at the use of a recently developed method
called orthonormal explicit topic analysis,
which combines explicit and latent models
of semantics. Although, it remains unclear
how topic model may be applied to the
case of grammar induction, we show that
it is not impossible and that this may allow
the capture of subtle semantic distinctions
that are not captured by other methods.

1 Introduction

Grammar induction is the task of inducing high-
level rules for application of grammars in spoken
dialogue systems. In practice, we can extract rel-
evant rules and the task of grammar induction re-
duces to finding similar rules between two strings.
As these strings are not necessarily similar in sur-
face form, what we really wish to calculate is
the semantic similarity between these strings. As
such, we could think of applying a semantic anal-
ysis method. As such we attempt to apply topic
modelling, that is methods such as Latent Dirich-
let Allocation (Blei et al., 2003), Latent Seman-
tic Analysis (Deerwester et al., 1990) or Explicit
Semantic Analysis (Gabrilovich and Markovitch,
2007). In particular we build on the recent work
to unify latent and explicit methods by means of
orthonormal explicit topics.

In topic modelling the key choice is the docu-
ment space that will act as the corpus and hence
topic space. The standard choice is to regard all
articles from a background document collection
– Wikipedia articles are a typical choice – as the

This work is licensed under a Creative Commons At-
tribution 4.0 International Licence. Page numbers and pro-
ceedings footer are added by the organisers. Licence details:
http://creativecommons.org/licenses/by/4.0/

topic space. However, it is crucial to ensure that
these topics cover the semantic space evenly and
completely. Following McCrae et al. (McCrae et
al., 2013) we remap the semantic space defined by
the topics in such a manner that it is orthonormal.
In this way, each document is mapped to a topic
that is distinct from all other topics.

The structure of the paper is as follows: we de-
scribe our method in three parts, first the method
in section 2, followed by approximation method in
section 3, the normalization methods in section 4
and finally the application to grammar induction
in section 5, we finish with some conclusions in
section 6.

2 Orthonormal explicit topic analysis

ONETA (McCrae et al., 2013, Orthonormal ex-
plicit topic analysis) follows Explicit Semantic
Analysis in the sense that it assumes the avail-
ability of a background document collection B =
{b1, b2, ..., bN} consisting of textual representa-
tions. The mapping into the explicit topic space
is defined by a language-specific function Φ that
maps documents into RN such that the jth value in
the vector is given by some association measure
φj(d) for each background document bj . Typical
choices for this association measure φ are the sum
of the TF-IDF scores or an information retrieval
relevance scoring function such as BM-25 (Sorg
and Cimiano, 2010).

For the case of TF-IDF, the value of the j-th
element of the topic vector is given by:

φj(d) =
−−−→tf-idf(bj)T −−−→tf-idf(d)

Thus, the mapping function can be represented
as the product of a TF-IDF vector of document d
multiplied by a word-by-document (W ×N ) TF-
IDF matrix, which we denote as a X:1

1T denotes the matrix transpose as usual

119



Φ(d) =


−−−→
tf-idf(b1)T

...−−−→
tf-idf(bN )T

−−−→tf-idf(d) = XT · −−−→tf-idf(d)

For simplicity, we shall assume from this point
on that all vectors are already converted to a TF-
IDF or similar numeric vector form.

In order to compute the similarity between two
documents di and dj , typically the cosine-function
(or the normalized dot product) between the vec-
tors Φ(di) and Φ(dj) is computed as follows:

sim(di, dj) = cos(Φ(di), Φ(dj)) =
Φ(di)

TΦ(dj)

||Φ(di)||||Φ(dj)||

sim(di, dj) = cos(X
Tdi,X

Tdj) =
dTi XX

Tdj
||XTdi||||XTdj ||

The key challenge with topic modelling is
choosing a good background document collection
B = {b1, ..., bN}. A simple minimal criterion
for a good background document collection is that
each document in this collection should be maxi-
mally similar to itself and less similar to any other
document:

∀i 6= j 1 = sim(bj , bj) > sim(bi, bj) ≥ 0
As shown in McCrae et al. (2013), this property

is satisfied by the following projection:

ΦONETA(d) = (XTX)−1XTd

And hence the similarity between two docu-
ments can be calculated as:

sim(di, dj) = cos(ΦONETA(di),ΦONETA(dj))

3 Approximations

ONETA relies on the computation of a matrix in-
verse, which has a complexity that, using current
practical algorithms, is approximately cubic and
as such the time spent calculating the inverse can
grow very quickly.

We notice that X is typically very sparse and
moreover some rows of X have significantly fewer
non-zeroes than others (these rows are for terms
with low frequency). Thus, if we take the first N1
columns (documents) in X, it is possible to re-
arrange the rows of X with the result that there

is some W1 such that rows with index greater
than W1 have only zeroes in the columns up to
N1. In other words, we take a subset of N1 doc-
uments and enumerate the words in such a way
that the terms occurring in the first N1 documents
are enumerated 1, . . . ,W1. Let N2 = N − N1,
W2 = W −W1. The result of this row permuta-
tion does not affect the value of XTX and we can
write the matrix X as:

X =
(

A B
0 C

)
where A is a W1 × N1 matrix representing

term frequencies in the first N1 documents, B is a
W1×N2 matrix containing term frequencies in the
remaining documents for terms that are also found
in the first N1 documents, and C is a W2 × N2
containing the frequency of all terms not found in
the first N1 documents.

Application of the well-known divide-and-
conquer formula (Bernstein, 2005, p. 159) for ma-
trix inversion yields the following easily verifiable
matrix identity, given that we can find C′ such that
C′C = I.

(
(ATA)−1AT −(ATA)−1ATBC′

0 C′

)(
A B
0 C

)
= I

(1)

The inverse C′ is approximated by the Jacobi
Preconditioner, J, of CTC:

C′ ' JCT (2)

=

 ||c1||
−2 0

. . .
0 ||cN2 ||−2

CT

4 Normalization

A key factor in the effectiveness of topic-based
methods is the appropriate normalization of the el-
ements of the document matrix X. This is even
more relevant for orthonormal topics as the matrix
inversion procedure can be very sensitive to small
changes in the matrix. In this context, we con-
sider two forms of normalization, term and docu-
ment normalization, which can also be considered
as row/column normalizations of X.

A straightforward approach to normalization is
to normalize each column of X to obtain a matrix
as follows:

120



X′ =
(

x1
||x1|| . . .

xN
||xN ||

)

If we calculate X′TX′ = Y then we get that the
(i, j)-th element of Y is:

yij =
xTi xj
||xi||||xj ||

Thus, the diagonal of Y consists of ones only and
due to the Cauchy-Schwarz inequality we have
that |yij | ≤ 1, with the result that the matrix Y
is already close to I. Formally, we can use this
to state a bound on ||X′TX′ − I||F , but in prac-
tice it means that the orthonormalizing matrix has
more small or zero values. Previous experiments
have indicated that in general term normalization
such as TF-IDF is not as effective as using the di-
rect term frequency in ONETA, so we do not apply
term normalization.

5 Application to grammar induction

The application to grammar induction is simply
carried out by taking the rules and creating a sin-
gle ground instance. That is if we have a rule of
the form

LEAVING FROM <CITY>

We would replace the instance of <CITY> with
a known terminal for this rule, e.g.,

leaving from Berlin

This reduces the task to that of string simi-
larity which can be processed by means of any
string similarity function, for example such as the
ONETA function described above. As such the
procedure is as follows:

1. Ground the input grammar rule to an English
string d

2. Ground each candidate matching rule to an
English string di

3. Calculate for each di, the similarity
simONETA(d, di)

4. Add the rule to the grammar class with the
highest similarity

This approach has the obvious drawback that it
removes all information about the valence of the
rule, however the effect of this loss of information
remains unclear.

For application, we used 20,000 Wikipedia ar-
ticles, filtered to contain only those of over 100
words, giving us a corpus of 15.6 million tokens.
We applied ONETA using document normaliza-
tion but no term normalization and the valueN1 =
5000. These parameters were chosen based on the
best results in previous experiments.

6 Conclusions

The results show that such a naive approach is
not directly applicable to the case of grammar in-
duction, however we believe that it is possible
that the subtle semantic similarities captured by
topic modelling may yet prove useful for gram-
mar induction. However it is clear from the pre-
sented results that the use of a topic model alone
does not suffice to solve this task. We notice that
from the data many of the distinctions rely on
antonyms and stop words, especially distinctions
such as ‘to’/‘from’, which are not captured by a
topic model as topic models generally ignore stop
words, and generally consider antonyms to be in
the same topic, as they frequently occur together
in text. The question of when semantic similarity
such as provided by topic modelling is applicable
remains an open question.

References
Dennis S Bernstein. 2005. Matrix mathematics, 2nd

Edition. Princeton University Press Princeton.

David M Blei, Andrew Y Ng, and Michael I Jordan.
2003. Latent Dirichlet Allocation. Journal of Ma-
chine Learning Research, 3:993–1022.

Scott C. Deerwester, Susan T Dumais, Thomas K. Lan-
dauer, George W. Furnas, and Richard A. Harshman.
1990. Indexing by latent semantic analysis. JASIS,
41(6):391–407.

Evgeniy Gabrilovich and Shaul Markovitch. 2007.
Computing semantic relatedness using Wikipedia-
based explicit semantic analysis. In Proceedings of
the 20th International Joint Conference on Artificial
Intelligence, volume 6, page 12.

John P. McCrae, Philipp Cimiano, and Roman Klinger.
2013. Orthonormal explicit topic analysis for cross-
lingual document matching. In Proceedings of the
2013 Conference on Empirical Methods in Natural
Language Processing, pages 1732–1740.

121



Philipp Sorg and Philipp Cimiano. 2010. An experi-
mental comparison of explicit semantic analysis im-
plementations for cross-language retrieval. In Natu-
ral Language Processing and Information Systems,
pages 36–48. Springer.

122


