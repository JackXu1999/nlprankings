

















































A Novel System for Extractive Clinical Note Summarization
using EHR Data

Jennifer Liang, Ching-Huei Tsou and Ananya Poddar
IBM Research, Yorktown Heights, NY 10598

{jjliang, ctsou, poddar}@us.ibm.com

Abstract

While much data within a patient’s electronic
health record (EHR) is coded, crucial infor-
mation concerning the patient’s care and man-
agement remain buried in unstructured clinical
notes, making it difficult and time-consuming
for physicians to review during their usual
clinical workflow. In this paper, we present our
clinical note processing pipeline, which ex-
tends beyond basic medical natural language
processing (NLP) with concept recognition
and relation detection to also include compo-
nents specific to EHR data, such as structured
data associated with the encounter, sentence-
level clinical aspects, and structures of the
clinical notes. We report on the use of this
pipeline in a disease-specific extractive text
summarization task on clinical notes, focus-
ing primarily on progress notes by physicians
and nurse practitioners. We show how the
addition of EHR-specific components to the
pipeline resulted in an improvement in our
overall system performance and discuss the
potential impact of EHR-specific components
on other higher-level clinical NLP tasks.

1 Introduction

EHRs are a longitudinal record of the patient’s
health information consisting of structured (e.g.
vitals, medications, labs, procedures) and unstruc-
tured (e.g. progress notes, discharge summaries,
diagnostic test reports) information. Clinical notes
within EHRs are traditionally a rich source of
data where detailed information about the patient’s
medical history and clinical care process is docu-
mented. However, physicians at the point of care
are mostly unable to review much of this unstruc-
tured information due to the abundance of notes
within a patient EHR and the time constraint inher-
ent in the clinical setting. Also, the move from pa-
per records to EHRs have unintentionally resulted
in issues of note bloat, where use of templates and

copy-paste have introduced unnecessary or redun-
dant data into clinical notes, worsening the prob-
lem of information overload and making it more
difficult for physicians to identify key clinical data
with potentially negative consequences (Shoolin
et al., 2013; Vogel, 2013).

In the clinical care process, what is considered
key clinical data within a clinical document de-
pends greatly on the user and their task; what
is important for a physician to know while diag-
nosing a patient is different from what is impor-
tant for a social worker to know when arrang-
ing post-discharge home care. Building off the
idea of a problem-oriented medical record intro-
duced by Dr. Lawrence Weed (1968), we decided
to approach this problem of information overload
within EHRs from a disease-specific perspective.
We propose an automated summarization system
that produces an extractive summary for each note
containing only the most important information
relevant for managing a patient’s hypertension or
diabetes mellitus at the point of care.

There are multiple challenges in generating a
disease-specific extractive summary on clinical
text. First of all, the abundance of domain-specific
terminology and presence of non-standard abbre-
viations and misspellings make machine compre-
hension of clinical text a much more complex
task (Demner-Fushman et al., 2009). Secondly,
the use of temporal narratives with reference to
multiple diseases and the inherent interrelatedness
of different diseases and other clinical concepts
makes it difficult to determine what is “disease-
specific” in the context of our summary. More-
over, the heavy use of templates, copy-paste, and
imported data within clinical notes (Shoolin et al.,
2013; Vogel, 2013) suggests that medical NLP at
the concept level is insufficient for differentiation
between “important” and “unimportant” informa-
tion. Last of all, due to the regulations surround-



ing use and sharing of protected health informa-
tion, and the need for expert annotation, clinical
NLP systems typically only have access to a lim-
ited amount of labeled data. To address these con-
cerns, we leverage individual components, trained
on separate labeled datasets, that target lower level
clinical NLP tasks such as identifying note struc-
ture and specific clinical events of interest, and
chain these individual components together into
a pipeline that automatically generates disease-
specific summaries from clinical notes.

2 Related Work

EHR summarization efforts have mostly focused
on extraction of clinical variables or visualiza-
tion of structured and unstructured elements in
the EHR as a longitudinal data display (Pivovarov
and Elhadad, 2015), with the objective being to
present an overview of the entire longitudinal pa-
tient record. Savova et al. (2010) built cTAKES, an
open-source NLP system for information extrac-
tion from unstructured clinical text. Rogers et al.
(2006) developed the CLEF chronicle, which uses
a semantic network of concepts and interrelations
to represent events in a patient’s medical history,
that could serve as a building block for future sum-
marization efforts. CLAMP (Soysal et al., 2017)
allows users to more efficiently build customized
NLP pipelines and reported good performance on
named entity recognition and concept encoding in
their evaluation.

Some researchers approached EHR note sum-
marization from a problem identification perspec-
tive. Cao et al. (2004) summarized discharge sum-
maries as problem lists. Van Vleck and Elhadad
(2010) identified a list of problems relevant to a
physician seeing a new patient for a given set of
clinical notes. Our work differs from previous
published research in that our summarization sys-
tem is (1) targeted toward a single clinical en-
counter represented by a note, (2) specific to the
management of a given disease: hypertension and
diabetes mellitus, (3) generates a human readable
textual summary as opposed to a list of clinical
variables, and (4) extends beyond basic medical
NLP with concept recognition and relation detec-
tion to also include components specific to EHR
data.

3 Method

The ultimate goal of the system is to generate a co-
hesive summary of a patient, similar to a summary
written by an attending physician after reviewing
the patient’s chart. Such a system requires text
summary from individual notes, reconciliation be-
tween structured data and unstructured narratives,
temporal alignment of the clinical events, and nat-
ural language generation to produce the final ab-
stractive summary. This paper focuses entirely on
extracting informative sentences rather than cohe-
sive sentences from a single clinical note. The out-
put of the work presented here can be used as the
input for downstream components to generate co-
hesive summaries across the longitudinal patient
record.

3.1 Dataset

Our dataset consists of patient EHRs within a large
ambulatory multi-specialty medical group in the
US that contain a known diagnosis of hypertension
and/or diabetes mellitus based on their structured
encounter diagnosis list. We selected notes within
these patient EHRs authored by physicians or
nurse practitioners and manually reviewed approx-
imately half of the selected notes to ensure that
at least one of our diseases of interest, hyperten-
sion or diabetes mellitus, was addressed at the visit
documented in that note. We made the decision
to focus on physician and nurse practitioner notes
because those providers are the primary decision-
maker in the patients’ clinical care management.
Manual review was performed on approximately
half of the notes to ensure a sufficient number of
positive examples from the ground truth genera-
tion effort. The resulting corpus consisted of 3,453
outpatient clinical notes over 762 patients, with an
average length of 138 sentences per note.

The corpus was annotated by 12 internal
medicine or family medicine physicians over the
course of 6 months. Physicians were asked to
review each note and annotate information rele-
vant to the physicians’ decision-making for man-
agement of the patient’s hypertension or diabetes
mellitus, with the understanding that the anno-
tated information would be presented together as
a disease-focused summary of the note. Examples
of relevant information included in the summary
are statements about the current problem status,
any signs or symptoms experienced by the patient,
desirable and undesirable effects of current treat-



Figure 1: Sample clinical note with extractive summaries for hypertension and diabetes mellitus. Underlined
sentences together form the extractive summary for hypertension; sentences in bold together form the extractive
summary for diabetes mellitus.

ment, and any changes to current treatment plan.
Each note was independently reviewed and anno-
tated by two physicians, and then adjudicated by
a third MD. The inter-annotator agreement is re-
ported in the “Results” section. Figure 1 shows an
example of a clinical note and its extractive sum-
maries for hypertension and diabetes mellitus.

3.2 Extractive Summarization
Current approaches of document summarization in
the clinical domain have largely been extractive
rather than abstractive, so the original text as writ-
ten by the physicians are preserved.

Given a clinical note consisting of a sequence
of sentences, N = {s1, s2, ...sn} a sentence level,
single document, extractive summarization task
can be defined as to create a summary NS by se-
lecting m sentences (m ≤ n) from N . One of the
simplest approaches is to model the task as a su-
pervised binary classification problem, where we
find a model with model parameters θ that maxi-
mize the likelihood

p(Y |N, θ) =
n∏

(i=1)

p(yi|si, θ) (1)

where Y = {y1, y2, ...yn} and yi ∈ {0, 1}.
It is obvious that in (1) each sentence is clas-

sified solely on the information contained in the

sentence itself. This rudimentary approach works
reasonably well for many sentence classification
tasks, provided the majority of the sentences are
self-contained. For clinical notes, sentences are
often short and the meaning depend heavily on the
context. One way to address this is to model the
problem as a sequence labeling task, where ad-
ditional information at the document level is also
considered,

p(Y |N, θ) =
n∏

(i=1)

p(yi|si, N, θ) (2)

A simple example of sequence model is a linear-
chain CRF (Lafferty et al., 2001), which takes the
previously labeled sentence(s) into consideration
when predicting the label of the current sentence.
Recent advances in deep neural network based ap-
proaches have shown great promises in analyzing
several types of EHR data (Shickel et al., 2018),
but their application in extractive note summa-
rization is largely unexplored (Alsentzer and Kim,
2018).

In this paper, we start by creating the baseline
using the 3 approaches discussed above, i.e., a lin-
ear SVM for sentence classification, a linear chain
CRF in which each note is modeled as a sequence,
and a simple CNN-rand (Kim, 2014) for our sum-
marization task. All 3 models used only the note



Figure 2: Single clinical document extractive summa-
rization pipeline.

text as features: bag of n-grams for the SVM and
the CRF, and randomly initialized word embed-
dings (updated during training) for CNN-rand. We
report the F-scores for each of these models in the
“Results” section.

3.3 Clinical Note Processing Pipeline

Each clinical note is ingested by a basic NLP pro-
cessing layer that performs the standard NLP tasks
including tokenization, lemmatization, sentence
segmentation, POS tagging, and parsing, followed
by a medical concept recognition component,
where key medical concepts such as labs, proce-
dures, medications, signs and symptoms, and dis-
eases are identified. Our system used an English
Slot Grammar (ESG) parser (McCord, 1990) fol-
lowed by a proprietary medical concept annota-
tor that maps terms into unified medical language
system (UMLS) concepts (Bodenreider, 2004).
This entity linking pipeline is similar to MetaMap
(Aronson and Lang, 2010) but was optimized to
process clinical notes, where sentences are not al-
ways well structured and abbreviation expansion
and disambiguation plays an important role. These
foundational NLP analytics, although crucial to
the success of downstream components and re-
main an active research area, have become com-
modity in the recent years, for instance, CLAMP
(Soysal et al., 2017), Amazon Comprehend Medi-
cal, (Amazon, 2019), and IBM Natural Language
Understanding (IBM, 2019) and is not of interest

in this paper.
We separate the components in the next layer

into 3 categories based on how the analytics are
developed, namely, heuristics and rules, assertions
framework, and deep-learning framework (Figure
2). When labeled data is easier to obtain, data-
driven approaches, such as deep neural network
architectures, often outperform other methods due
to their ability to effectively learn representations
as well as model parameters. For instance, for
adverse drug events (ADEs), we use an existing
labeled dataset from the MADE1.01 NLP chal-
lenge to train a BiLSTM-CRF model with atten-
tion (Dandala et al., 2018). On the other hand,
ground truth for extractive summarization requires
human experts to read the entire note and is harder
to acquire. With limited ground truth, we have
learned that a hybrid system combining heuristics,
less expressive models (e.g. linear SVM), and out-
puts from deep-learning based components as fea-
tures generates better results than trying to train an
end-to-end pure neural network based system. In
the “Results” section, we will give one example
from each category that has significant contribu-
tion to the overall system.

3.4 Evaluation

Evaluating a text summary is challenging. Gen-
erally, the ways of evaluating the performance
of automatically generated summarizations can be
categorized into intrinsic and extrinsic evaluation
methods (Steinberger and Jezek, 2009). Intrinsic
evaluation directly compares the generated sum-
mary to the ground truth summary. For example,
co-selection measures calculate the precision, re-
call, and F-score at the sentence level; and content-
based measures such as ROUGE (Lin, 2004) com-
pares at word level using n-gram and/or longest
common subsequence. Intrinsic evaluation can
also be done qualitatively, by domain experts us-
ing a Likert-type scale. Extrinsic evaluation mea-
sures the quality of the automatic summaries indi-
rectly, for a given task. For example, how much
time a physician can save in their daily practice
with or without the help of such summarization.
In this work we present our results using intrin-
sic evaluation with co-selection measures. Stud-
ies using qualitative intrinsic measurements and
quantitative extrinsic evaluation are being planned

1bio-nlp.org/index.php/announcements/39-nlp-
challenges/



Hypertension Diabetes
Precision 0.723 0.726
Recall 0.646 0.671
F-score 0.682 0.697

Table 1: Inter-annotator agreement.

Hypertension Diabetes
SVM, linear 0.524 0.516
CRF, linear-chain 0.579 0.598
CNN 0.584 0.593

Table 2: F-scores for hypertension and diabetes melli-
tus summarization using different models.

for the future; those results will be reported at a
later time and are beyond the scope of this paper.
In this work, we evaluated the system using the
co-selection measures, i.e., calculating sentence
level F-score between system-identified span and a
physician-annotated span, using 10-fold cross val-
idation.

4 Results

4.1 Inter-Annotator Agreement
Agreement was calculated at the sentence level,
meaning that if two annotators each marked a dif-
ferent span of text within the same sentence, it
was considered a match. On average, annotators
marked 4 to 5 sentences per note to be included in
the disease-specific summary. Because the num-
ber of sentences not included in a summary is
significantly larger than the number of sentences
marked by physician annotators, we use recall,
precision, and F-score as surrogates for the typical
Cohen’s Kappa in reporting inter-annotator agree-
ment. Using one annotator’s annotations as ref-
erence, we calculate the recall, precision, and F-
score of the second annotator as a measure of the
inter-annotator agreement, as shown in Table 1. In
the “Discussion’ section, we will discuss reasons
for the observed differences between annotators.

4.2 Summarization Models
Table 2 shows the results of 3 different approaches
for our summarization task: classification with
SVM, sequence labelling with CRF, and a simple
CNN with randomly initialize word embeddings.
Note that in our training corpus, notes on aver-
age have 138 sentences and only 3% of the sen-
tences are annotated as summary. Although the F-

Hypertension Diabetes
Unigram 0.555 0.581
+N-gram 0.579 0.598
+Concept 0.590 0.608
+Section 0.630 0.637
+Context 0.642 0.655
+Plan 0.646 0.662
All 0.657 0.679

Table 3: Ablation study - F-scores of disease-specific
insights.

scores are at the fifties, the accuracies (including
true-negatives) are well above high nineties.

We can see that considering the document level
information (CRF & CNN) is important for the
task, and CNN is performing reasonably well even
with limited labeled data. In this paper, our goal
is to identify useful information from the entire
EHR, in the context of producing extractive clin-
ical narrative summarization. Those document
level and patient level features can be used in
both neural network based and non-neural network
based architectures. Our current system uses CRF
at the top level to label the sentences, and sev-
eral deep learning based models at the component
level.

4.3 Impact of EHR Components

Table 3 shows the results of an ablation study on
selected components. Here we only report the
numbers using the CRF model. We started by us-
ing only bag-of-words (unigram) as features and
introduce a new type of feature in the next row.
The last row shows the final result of using all fea-
tures, including all components shown in Figure 2,
such as ADEs, goals, and medication changes.

4.4 Heuristics and Rules - Disease Context

As our summarization is disease-specific, it is in-
tuitive that knowing the disease context of each
sentence can be a useful feature. For exam-
ple, in the sample clinical note in Figure 1,
the phrase “sub-optimally controlled” appears 3
times, each under a different disease context: “DI-
ABETES MELLITUS”, “HYPERTENSION”, and
“OBESITY/OVERWEIGHT”. Depending on the
disease context, each specific instance of “sub-
optimally controlled” may or may not be an in-
sight we want to extract for a given target disease.

In practice, this disease context can be an ex-



plicit section header (as in Figure 1), or conveyed
more implicitly, such as a disease mentioned in
the previous sentence or an encounter diagnosis
code in the structured data associated with that
note. A common example of implicit disease con-
text is in specialist notes; for example, an En-
docrinology note consulting on a patient’s newly
diagnosed diabetes mellitus. Here, the specialty
in the note metadata (“Endocrinology”) and the
reason for consult (“newly-diagnosed DM2”) both
serve as the context for the entire note.

Because the disease context can be far away
from the current sentence, especially for cases of
implicit context, heuristics are used until we have
enough labeled data to train a model with long
term memory, such as recurrent neural network
with attention mechanism. These heuristics were
developed with input from subject matter experts
familiar with how to read and interpret clinical
text. We can see from Table 3 that modeling dis-
ease context explicitly improved the overall per-
formance.

4.5 Machine Learning Framework - Note
Section

Although not required, healthcare providers of-
ten follow some common structures, for example,
SOAP (Lew and Ghassemzadeh, 2018), when doc-
umenting a patient encounter in a clinical note.
Knowing where a sentence resides with respect
to these structures or sections, will undoubtfully
help the system extract important insights from the
note.

As there are no set rules for indicating sections,
and headers and formats are not strictly enforced,
pattern matching rules using regular expression
yield mediocre results. To improve the accuracy,
we model note section classification as a super-
vised sequence labeling task using linear-chain
CRF - each sentence belongs to 1 of 14 predefined
note sections, namely, chief complaint, history of
present illness, past medical history, past surgical
history, medications, allergies, social history, fam-
ily history, review of systems, vital signs, physical
exam, diagnostic test results, assessment and plan,
and other, and both the results from regular expres-
sion (matching predefined format and headers) as
well as the words in the sentence are used as fea-
tures in the CRF model. Table 3 shows that note
section is another useful global feature in extrac-
tive summarization.

5 Discussion

5.1 Reasons for Annotator Differences
Although we placed our extractive summarization
task in the setting of a specific user (physician),
disease (hypertension and diabetes mellitus), and
task (disease management), there is still a sub-
jective component in the ground truth generation
process. Some practitioners prefer a very con-
cise summary limited to only the disease of fo-
cus, while others prefer a more informative sum-
mary that includes not only information directly
related to the disease of focus, but also to related
co-morbidities. Also, redundancy in clinical notes
means that the same information is often presented
in different ways in different parts of the same
note; for example, the patient’s presentation is de-
scribed in detail in the history of present illness
(HPI) section at the top of the note, while the same
information is summarized in a more concise way
in the assessment and plan section (AP) at the end
of the note. Some practitioners prefer the addi-
tional detail contained in the HPI section as part
of their summary, while others prefer only seeing
the more concise version in the AP section.

During the ground truth generation process, we
aligned physicians’ perspectives and preferences
the best we could through multiple discussions of
what types of information should (e.g. problem
status, home monitoring results, changes in dis-
ease management) and should not (e.g. direct im-
ports from structured data, routine labs and follow-
up instructions) be included as part of the sum-
mary. The discussions helped ensure better con-
sistency of the ground truth among different physi-
cian annotators, but still resulted in an observed
IAA of 0.682 to 0.697. This reflects the inherent
subjective nature of the task, and demonstrates the
need for a third MD to adjudicate any disagree-
ments between annotators to produce consistent
ground truth to be used by our system.

5.2 Addressing Issues of Data Scarcity
One of the challenges common in developing an-
alytics on clinical text is the limited labeled data
available for training and testing due to health in-
formation privacy concerns and the expensive cost
of expert annotations. Our automatic summariza-
tion system works around this limitation by using
individual components that can be trained using
separate ground truth. Some components, such as
note section classification, do not require annota-



tors with the same level of domain expertise and
can more easily be done in-house by appropriately
trained non-physician annotators. Other compo-
nents, such as adverse drug events, make use of
existing labeled datasets available through vari-
ous clinical NLP challenges such as MADE1.02

and TAC3. Using separately trained components
allows our system to make the most of the limited
amount of available expert-annotated data.

5.3 Limitations in Evaluation
In this study, we use an intrinsic evaluation of our
generated summary using precision, recall, and F-
score to compare against ground truth created by
physicians. However, these metrics do not fully
capture the nuances of what should or should not
be included in a clinical summary. The wide
spectrum of what could be considered “important”
to a physician means that not all false negatives
are equivalent; some information is critical to pa-
tient management and should never be missed,
while the importance and relevance of some other
pieces of information are debatable amongst dif-
ferent physicians. Similarly, not all false posi-
tives are equivalent; some false positives are com-
pletely wrong and unrelated to the disease at hand,
while others comprise of sentences that were not
included in the ground truth but still provide rele-
vant and useful information.

Adverse drug events are an example of impor-
tant information that physicians are particularly
sensitive to. ADEs have great impact on patient
safety and is considered an important insight to ex-
tract per our annotation guidelines. These are rare
yet important events for physicians to be aware of
when managing a patient’s care. We have been
actively participating in recent ADE detection re-
lated challenges and developed our component us-
ing BiLSTM-CRF (Dandala et al., 2018). A major
task for this component is to distinguish adverse
drug events (e.g. “His cough improved off lisino-
pril”) from indication for a drug (e.g. “His hyper-
tension improved on lisinopril”), i.e., to identify
the type of relation between a drug and a sign or
symptom. This is often impossible without medi-
cal knowledge, and is an example of why a generic
summarization algorithm from other domains will
not work on clinical narratives out of the box, as
the importance of a sentence depends on medical

2bio-nlp.org/index.php/announcements/39-nlp-
challenges/

3bionlp.nlm.nih.gov/tac2017adversereactions/

knowledge from outside of the document.
Because of the importance of ADEs to clinical

care, a missed ADE by the system (false nega-
tive) or an incorrectly identified ADE (false pos-
itive) both negatively impact the overall quality of
the summary significantly more compared to other
types of information. As ADEs are rare, adding
this component does not have significant impact
to the overall summarization accuracy measure.
However, we choose to discuss this component
in this paper to demonstrate the need for a qual-
itative intrinsic evaluation that weighs each sen-
tence based on its importance in order to capture
the value that rare yet important events, such as
ADEs, bring to the overall system.

Redundant information in clinical notes pose
another challenge to the evaluation of our system.
This redundancy led to observed cases where the
ground truth has one sentence annotated while the
system has annotated a different sentence contain-
ing essentially the same content. This is judged
as a false negative (because the system missed the
physician-annotated sentence) and a false positive
(because the system-annotated sentence was not in
the physician-annotated ground truth), reflecting
negatively in the overall system evaluation without
giving the system credit for the fact that the rele-
vant information is still present in the generated
summary. This demonstrates the need for a sep-
arate extrinsic evaluation of our generated sum-
maries based on its usefulness in the clinical set-
ting, which we have planned for the future.

We are planning future evaluations of our sys-
tem using qualitative intrinsic measures to capture
the importance of different information within the
clinical summary, and quantitative extrinsic mea-
sures to evaluate the usefulness of the system-
generated summaries for practicing physicians at
the point of care.

6 Conclusions

We propose an automated system for disease-
specific extractive summarization on a single clin-
ical note. We describe our clinical note process-
ing pipeline that includes a basic NLP processing
layer as well as additional EHR-specific compo-
nents such as note section classification, disease
context identification, and adverse drug events de-
tection. We show incremental improvement in
overall system performance with addition of each
component, from F-scores of 0.555 and 0.581 for



hypertension and diabetes mellitus, respectively,
when using only unigrams, to 0.657 and 0.679
when all components are included in the pipeline.
Our work demonstrates how analytics beyond con-
cept recognition is necessary for a complex and
higher-level clinical NLP task such as summariza-
tion. Also, until abundant labeled data in clin-
ical narratives becomes available, generic sum-
marization algorithms developed using non-EHR
data will benefit from using EHR-specific compo-
nents discussed here as global features.

References
Emily Alsentzer and Anne Kim. 2018. Extrac-

tive Summarization of EHR Discharge Notes.
arXiv:1810.12085.

Amazon. 2019. Amazon Comprehend Medical.

Alan R Aronson and François-Michel Lang. 2010. An
overview of MetaMap: historical perspective and re-
cent advances. Journal of the American Medical In-
formatics Association, 17(3):229–236.

Olivier Bodenreider. 2004. The Unified Medical Lan-
guage System (UMLS): integrating biomedical ter-
minology. Nucleic Acids Research, 32:D267–D270.

Hui Cao, Michael F. Chiang, James J. Cimino, Carol
Friedman, and George Hripcsak. 2004. Automatic
Summarization of Patient Discharge Summaries to
Create Problem Lists using Medical Language Pro-
cessing. Stud Health Technol Inform, 107(2):1540.

Bharath Dandala, Venkata Joopudi, and Murthy De-
varakonda. 2018. IBM Research System at MADE
2018: Detecting Adverse Drug Events from Elec-
tronic Health Records. In Proceedings of the 1st
International Workshop on Medication and Adverse
Drug Event Detection, volume 90 of Proceedings of
Machine Learning Research, pages 39–47. PMLR.

Dina Demner-Fushman, Wendy W. Chapman, and
Clement J. McDonald. 2009. What can natural lan-
guage processing do for clinical decision support?
Journal of Biomedical Informatics, 42(5):760–772.
Biomedical Natural Language Processing.

IBM. 2019. IBM Natural Language Understanding.

Yoon Kim. 2014. Convolutional Neural Networks
for Sentence Classification. In Proceedings of the
2014 Conference on Empirical Methods in Natural
Language Processing (EMNLP), pages 1746–1751,
Doha, Qatar. Association for Computational Lin-
guistics.

John D. Lafferty, Andrew McCallum, and Fernando
C. N. Pereira. 2001. Conditional Random Fields:

Probabilistic Models for Segmenting and Label-
ing Sequence Data. In Proceedings of the Eigh-
teenth International Conference on Machine Learn-
ing, ICML ’01, pages 282–289, San Francisco, CA,
USA. Morgan Kaufmann Publishers Inc.

Valerie Lew and Sassan Ghassemzadeh. 2018. SOAP
Notes. [Updated 2019 Jan 19]. In: StatPearls [In-
ternet]. Treasure Island (FL): StatPearls Publishing.

Chin-Yew Lin. 2004. ROUGE: A Package for Auto-
matic Evaluation of Summaries. In Text Summa-
rization Branches Out: Proceedings of the ACL-04
Workshop, pages 74–81, Barcelona, Spain. Associa-
tion for Computational Linguistics.

Michael C. McCord. 1990. Slot grammar. Natural lan-
guage and logic, pages 118–145.

Rimma Pivovarov and Noémie Elhadad. 2015. Auto-
mated methods for the summarization of electronic
health records. Journal of the American Medical In-
formatics Association, 22(5):938–947.

Jeremy Rogers, Colin Puleston, and Alan Rector. 2006.
The CLEF Chronicle: Patient Histories Derived
from Electronic Health Records. In 22nd Interna-
tional Conference on Data Engineering Workshops
(ICDEW’06), pages x109–x109.

Guergana K Savova, James J Masanz, Philip V Ogren,
Jiaping Zheng, Sunghwan Sohn, Karin C Kipper-
Schuler, and Christopher G Chute. 2010. Mayo clin-
ical Text Analysis and Knowledge Extraction Sys-
tem (cTAKES): architecture, component evaluation
and applications. Journal of the American Medical
Informatics Association, 17(5):507–513.

Benjamin Shickel, Patrick Tighe, Azra Bihorac, and
Parisa Rashidi. 2018. Deep EHR: A Survey of
Recent Advances in Deep Learning Techniques for
Electronic Health Record (EHR) Analysis. IEEE
Journal of Biomedical and Health Informatics,
22(5):1589–1604.

J. Shoolin, L. Ozeran, C. Hamann, and W. Bria.
2013. Association of Medical Directors of Infor-
mation Systems Consensus on Inpatient Electronic
Health Record Documentation. Appl Clin Inform,
4(2):293–303.

Ergin Soysal, Jingqi Wang, Min Jiang, Yonghui Wu,
Hua Xu, Serguei Pakhomov, and Hongfang Liu.
2017. CLAMP a toolkit for efficiently build-
ing customized clinical natural language processing
pipelines. Journal of the American Medical Infor-
matics Association, 25(3):331–336.

Josef Steinberger and Karel Jezek. 2009. Evaluation
Measures for Text Summarization. Computing and
Informatics, 28:1001–1026.

Tielman T. Van Vleck and Noémie Elhadad. 2010.
Corpus-Based Problem Selection for EHR Note
Summarization. AMIA Annu Symp Proc, pages 817–
821.

https://aws.amazon.com/comprehend/medical/
https://doi.org/10.1136/jamia.2009.002733
https://doi.org/10.1136/jamia.2009.002733
https://doi.org/10.1136/jamia.2009.002733
https://doi.org/10.1093/nar/gkh061
https://doi.org/10.1093/nar/gkh061
https://doi.org/10.1093/nar/gkh061
http://proceedings.mlr.press/v90/dandala18a.html
http://proceedings.mlr.press/v90/dandala18a.html
http://proceedings.mlr.press/v90/dandala18a.html
https://doi.org/https://doi.org/10.1016/j.jbi.2009.08.007
https://doi.org/https://doi.org/10.1016/j.jbi.2009.08.007
https://www.ibm.com/watson/services/natural-language-understanding/
https://doi.org/10.3115/v1/D14-1181
https://doi.org/10.3115/v1/D14-1181
http://dl.acm.org/citation.cfm?id=645530.655813
http://dl.acm.org/citation.cfm?id=645530.655813
http://dl.acm.org/citation.cfm?id=645530.655813
https://www.ncbi.nlm.nih.gov/books/NBK482263/
https://www.ncbi.nlm.nih.gov/books/NBK482263/
https://www.ncbi.nlm.nih.gov/books/NBK482263/
https://www.aclweb.org/anthology/W04-1013
https://www.aclweb.org/anthology/W04-1013
https://doi.org/10.1007/3-540-53082-7_20
https://doi.org/10.1093/jamia/ocv032
https://doi.org/10.1093/jamia/ocv032
https://doi.org/10.1093/jamia/ocv032
https://doi.org/10.1109/ICDEW.2006.144
https://doi.org/10.1109/ICDEW.2006.144
https://doi.org/10.1136/jamia.2009.001560
https://doi.org/10.1136/jamia.2009.001560
https://doi.org/10.1136/jamia.2009.001560
https://doi.org/10.1136/jamia.2009.001560
https://doi.org/10.1109/JBHI.2017.2767063
https://doi.org/10.1109/JBHI.2017.2767063
https://doi.org/10.1109/JBHI.2017.2767063
https://doi.org/10.4338/ACI-2013-02-R-0012
https://doi.org/10.4338/ACI-2013-02-R-0012
https://doi.org/10.4338/ACI-2013-02-R-0012
https://doi.org/10.1093/jamia/ocx132
https://doi.org/10.1093/jamia/ocx132
https://doi.org/10.1093/jamia/ocx132


Lauren Vogel. 2013. Cut-and-paste clinical notes con-
fuse care, say US internists. CMAJ, 185(18):E826–
E826.

Lawrence L. Weed. 1968. Medical records that guide
and teach. New England Journal of Medicine,
278:652657.

https://doi.org/10.1503/cmaj.109-4656
https://doi.org/10.1503/cmaj.109-4656
https://doi.org/10.1056/NEJM196803142781105
https://doi.org/10.1056/NEJM196803142781105

