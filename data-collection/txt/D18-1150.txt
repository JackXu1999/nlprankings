



















































Large Margin Neural Language Model


Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 1183–1191
Brussels, Belgium, October 31 - November 4, 2018. c©2018 Association for Computational Linguistics

1183

Large Margin Neural Language Model

Jiaji Huang1 Yi Li1 Wei Ping1 Liang Huang1,2∗
1 Baidu Research, Sunnyvale, CA, USA

2 School of EECS, Oregon State University, Corvallis, OR, USA
{huangjiaji, liyi17, pingwei01, lianghuang}@baidu.com

Abstract

We propose a large margin criterion for train-
ing neural language models. Conventionally,
neural language models are trained by mini-
mizing perplexity (PPL) on grammatical sen-
tences. However, we demonstrate that PPL
may not be the best metric to optimize in
some tasks, and further propose a large mar-
gin formulation. The proposed method aims
to enlarge the margin between the “good” and
“bad” sentences in a task-specific sense. It is
trained end-to-end and can be widely applied
to tasks that involve re-scoring of generated
text. Compared with minimum-PPL training,
our method gains up to 1.1 WER reduction for
speech recognition and 1.0 BLEU increase for
machine translation.

1 Introduction

Language models (LMs) estimate the likelihood
of a symbol sequence {xt}Tt=0, based on the joint
probability,

p(x0, . . . , xT ) = p(x0)
T∏
t=1

p(xt|x0, . . . , xt−1).

(1)

To measure the quality of an LM, a commonly
adopted metric is perplexity (PPL), defined as

PPL , exp

{
− 1
T

T∑
t=0

log p(xt|x0, . . . , xt−1)

}
,

A good language model has a small PPL, being
able to assign higher likelihoods to sentences that
are more likely to appear.

LMs are widely applied in automatic speech
recognition (ASR) (Yu and Deng, 2014) and ma-
chine translation (MT) (Koehn, 2009). Follow-
ing Koehn (2009), one may interpret the language

∗Contributions were made while at Baidu Research.

model as prior knowledge on the text to be in-
ferred, which provides information complemen-
tary to the ASR or MT system itself. In prac-
tice, there are several ways to incorporate the lan-
guage model. The simplest way may be re-scoring
an n-best list returned by the ASR or MT sys-
tem (Mikolov et al., 2010; Sundermeyer et al.,
2012). A slightly more sophisticated way is to
jointly consider the ASR/MT and language model
in a beam search decoder (Amodei et al., 2016).
Specifically, at each time step, the decoder ap-
pends every symbol in the vocabulary to each se-
quence in the current candidate set. For every
hypothesis, a score is calculated as a linear com-
bination of the log-likelihoods given by both the
ASR/MT and language models. Then, only the
top K hypotheses with the highest scores are re-
tained, as an updated candidate set. More recently,
Gulcehre et al. (2015) and Sriram et al. (2017) pro-
pose to predict the next symbol based on a fusion
of the hidden states in the ASR/MT and language
models. A gating mechanism is jointly trained to
determine how much the language model should
contribute.

The afore-discussed language models are gen-
erative in the sense that they merely model the
joint distribution of a symbol sequence (Eq. (1)).
While the research community is mostly focused
on pushing the limit of PPL (e.g., Jozefowicz et al.,
2016), very limited attention has been paid to the
discrimination power of language models when
they are applied to real tasks, such as ASR and
MT (Li and Khudanpur, 2008). By contrast, dis-
criminative language modeling aims at enhancing
the performance in downstream applications. For
example, existing works (Roark et al., 2004, 2007)
often target at improving ASR accuracy. The
key motivation underlying them is that the model
should be able to discriminate between “good” and
“bad” sentences in a task-specific sense, instead



1184

of just modeling grammatical ones. The com-
mon methodology (Dikic et al., 2013) is to build
a binary classifier upon hand-crafted features ex-
tracted from the sentences. However, it is not ob-
vious how these methods can utilize large unanno-
tated corpus, which is often easily available, and
the hand-crafted features are also ad hoc and may
result in suboptimal performance.

In this work, we study how to improve the dis-
crimination ability of a recurrent network-based
neural language model (RNNLM). The goal is to
enlarge the difference between the log-likelihoods
of “good” and “bad” sentences. In an contrast to
the existing works (Roark et al., 2004, 2007), our
method does not rely on hand-crafted features, and
is trained in end-to-end manner and able to take
advantage of large external text corpus. In fact,
it is a general training criterion that is transparent
to the network architecture of the RNNLM, and
can be applied to various text generation tasks, in-
cluding ASR and MT. Experiments on state-of-art
ASR and MT systems show its significant advan-
tage over an LM trained by minimizing PPL.

2 Background on RNNLM

We first give some background knowledge on
RNNLMs. The prototypical RNNLM (Mikolov
et al., 2010) has one layer of recurrent cell and
works as follows. Denote a sentence as x =
[x0, . . . , xt, . . . ], where the xt’s are words. Let
~xt be the embedding vector for xt. The recurrent
cell takes in the embedding and produces a hidden
state ~ht by

~ht = σ(U~xt + V~ht−1),

where σ(z) = 1
1+e−z is sigmoid activation func-

tion. ~ht−1 is the hidden state at the last timestep.
U and V are learnable parameters. The ~ht is then
passed into a multi-way classifier to produce a
probability distribution over the vocabulary (for
the next word),

~p = softmax(W~ht +~b).

The W and ~b are also trainable parameters.
The training objective is to maximize the log-
likelihood of the next word, and the parameters are
learned by back-propagation algorithm.

The vanilla recurrent cell can also be re-
placed by one or multiple layers of LSTM cells,
which produces better results (Zaremba et al.,

2014). In a more general form, the RNNLM
can be represented as a conditional probability,
pθ(x

t|x0, . . . , xt−1), parameterized by θ. In the
prototypical case, θ = [U, V,W,~b]. We could de-
fine the LM-score of a sentence x as

LM-score(x) , log pθ(x)

=
∑
t

log pθ(x
t|x0, . . . , xt−1).

The RNNLM is trained by maximizing the aver-
age LM-score over all the x’s in a corpus, or equiv-
alently, minimizing the PPL on the corpus.

3 Problem Formulation

We motivate and formulate a large margin training
criterion in this section. Suppose for every ref-
erence sentence xi, we have a collection of hy-
potheses xi,j , j = 1, . . . ,K, usually obtained as
the top-K candidates by a beam search decoder.

3.1 A Motivating Example
An RNNLM trained by minimizing PPL cannot
guarantee a higher score on the “gold” reference
than the inferior hypothesis, which is undesirable.
One example is given in Tab. 1. The reference is
taken from the text labels of dev93’ set of Wall
Street Journal (WSJ) dataset. The hypothesis is
generated by a CTC-based (Graves et al., 2006)
ASR system trained on WSJ training set. Words
in red are mistakes made by the hypothesis. We
then train an RNNLM on Common Crawl1 co-
pora by minimizing PPL. Training follows a typi-
cal setup (Jozefowicz et al., 2016) with a vocabu-
lary of 400K the most frequent words. Any out-of-
vocabulary word is replaced by an 〈UNK〉 token.
The RNNLM is then employed to score the sen-
tences. The LM-score of the erroneous hypothesis
is higher than that of the reference. In fact, this is
reasonable as “a decade as concerns” seems to be
a more common phrase. In the training corpus, we
find that “a decade as concerns” appears once, but
“its defeat is confirmed” does not appear. More-
over, “a decade as” appears 2,280 times, but “its
defeat is” appears only 24 times. However, this is
undesirable because if there is another hypothesis
that happens to be the same as reference, which
will not be ranked as the best candidate.

It would be helpful if the LM can also learn
from the imperfect hypotheses so that it can tell

1http://web-language-models.
s3-website-us-east-1.amazonaws.com/
wmt16/deduped/en-new.xz

 http://web-language-models.s3-website-us-east-1.amazonaws.com/wmt16/deduped/en-new.xz
 http://web-language-models.s3-website-us-east-1.amazonaws.com/wmt16/deduped/en-new.xz
 http://web-language-models.s3-website-us-east-1.amazonaws.com/wmt16/deduped/en-new.xz


1185

Sentence LM-score

reference

coniston declined to discuss its
plans for its defeat is confirmed

but indicated that it doesn’t
plan to simply walk away

-116.52

hypothesis

coniston declined to discuss its
plans for a decade as concerns

but indicated that it doesn’t
plan to simply walk away

-112.65

Table 1: Reference and one hypothesis, scored by
an RNNLM. Words in red are mistakes in the hy-
pothesis. The RNNLM is trained on Common
Crawl copora by minimizing PPL. We want the
reference to be higher scored than the hypothesis,
but it does not happen here.

0 2000 4000 6000 8000 10000
iteration

−2

0

2

4

6

8

tra
in
 lo

ss

1e7

(a)

0 2000 4000 6000 8000 10000
iteration

0.0

0.5

1.0

1.5

2.0

tra
in
 lo

ss

(b)

Figure 1: Training losses of (a) straightforward for-
mulation Eq. (2); and (b) large margin formulation
Eq. (3)

(a) (b)

Figure 2: Histogram of the margin log p(xi) −
log p(xi,j). The more positive, the more the dis-
crimination. (a) Straightforward formulation; (b)
LMLM compared with RNNLM (a minimum-PPL
LM trained on Common Crawl)

apart “good” and “bad” candidates. With this
motivation, we train to assign larger LM-scores
for the xi’s but smaller ones for the (imper-
fect) xi,j’s. A quantity of particular interest is
log p(xi) − log p(xi,j), the margin/difference be-
tween the LM-scores of the references and the
(imperfect) hypotheses. The intuition is that the
more positive the margin, the better the LM is at
discrimination.

3.2 Straightforward but Failed Formulation
Without loss of generality, we assume that all
the xi,j’s are imperfect and different from xi. A
straightforward way is to adopt the following ob-
jective:

min
θ

1

N

N∑
i=1

− log pθ(xi) + 1
K

K∑
j=1

log pθ(xi,j)

 .
(2)

Similar formulation is also seen in (Tachioka and
Watanabe, 2015), where they only utilize one
beam candidate, i.e., K = 1. Optimization can be
carried out by mini-batch stochastic gradient de-
scent (SGD). Each iteration, SGD randomly sam-
ples a batch of i’s and j’s, computes stochastic
gradient w.r.t. θ, and takes an update step. How-
ever, a potential problem with this formulation is
that the second term (corresponding to the infe-
rior hypotheses) may dominate the optimization.
Specifically, the training is almost always driven
by the xi,j’s, but does not effectively enhance the
discrimination. We illustrate this fact in the fol-
lowing experiment.

Using the ASR system in section 3.1, we extract
256 beam candidates for every training example in
Wall Street Journal (WSJ) dataset. Warm started
from the pre-trained RNNLM in section 3.1, we
apply SGD to minimize the loss in Eq. (2), with
a mini-batch size of 128. The training loss is
shown in Fig. 1a. We observe that the learning
dynamic is very unstable, and deceases to be neg-
ative. The unbound decreasing is due to the second
term in Eq. (2) being negative and dominating the
training process. Next, we inspect log pθ(xi) −
log pθ(xi,j), the margin between the scores of a
ground-truth and a candidate. In Fig. 2a, we his-
togram the margins for all the i, j’s in a dev set.
The distribution appears to be symmetric around
zero, which indicates poor discrimination ability.
Given these facts, we conclude that the straight-
forward formulation in Eq. (2) is not effective.

3.3 Large Margin Formulation
To effectively utilize all the imperfect beam candi-
dates, we propose the following objective,

min
θ

N∑
i=1

B∑
j=1

max
{
0, τ−(log pθ(xi)−log pθ(xi,j))

}
,

(3)
where log pθ(xi) − logθ(xi,j) is the margin be-
tween the scores of a ground-truth xi and a can-



1186

didate xi,j . The hinge loss on the margin encour-
ages the log-likelihood of the ground-truth to be at
least τ larger than that of the imperfect hypothesis.
We call an LM trained by the above formulation as
Large Margin Language Model (LMLM).

We repeat the same experiment in section 3.2,
but change the objective function to Eq. (3) and
set τ = 1. Fig. 1b shows the training loss, which
steadily decreases and approaches zero rapidly.
Compared with the learning curve of naive formu-
lation (Fig. 1a), the large margin based training is
much more stable. In Fig. 2b, we also examine
the histogram of log pθ(xi) − log pθ(xi,j), where
pθ(·) is now the LM learned by LMLM. Compared
with the histogram by the conventional RNNLM,
LMLM significantly moves the distribution to the
positive side, indicating more discrimination.

3.4 Ranking Loss Type Formulation
In most cases, all beam candidates are imperfect.
It may be beneficial to exploit the information
that some candidates are relatively better than the
others. We consider ranking them according to
some metrics w.r.t. the ground-truth sentences. For
ASR, the metric is WER, and for MT, the metric
is BLEU score. We define xi,0 , xi and assume
that the candidates {xi,j}Kj=1 are sorted such that

WER(xi,xi,j−1) < WER(xi,xi,j)

for ASR, and

BLEU(xi,xi,j−1) > BLEU(xi,xi,j)

for MT. In other words, xi,j−1 has better quality
than xi,j .

We then enforce the “better” sentences to have
a score at least τ larger than those “worse” ones.
This leads to the following formulation,

min
θ

N∑
i=1

B−1∑
j=0

B∑
k=j+1

max
{
0,

τ − (log pθ(xi,j)− logθ(xi,k)),
}
.

(4)

Compared with LMLM formulation Eq. (3), the
above introduces more comparisons among the
candidates, and hence more computational cost
during training. We call this formulation ranking-
loss-based LMLM (rLMLM).

To summarize this section, we have proposed
LMLM and rLMLM that aim at discriminating be-
tween hypotheses in a task-specific (e.g., WER or
BLEU) sense, instead of minimizing PPL.

4 Experiments on ASR

We apply the LMs trained under different criteria
to rescore the beams in various ASR systems. In
particular, we are interested in knowing which of
the two training mechanisms is better: minimizing
PPL (e.g., the RNNLM in Section 3.1), or fitting
to the WER metric by the proposed methods.

Adapting an RNNLM to a specific domain has
been of interest, especially to the speech commu-
nity (Park et al., 2010; Chen et al., 2015; Ma et al.,
2017). We adopt Ma et al. (2017) that fine-tune
the softmax layer of RNNLM by minimizing the
PPL on the text labels of training set. According
to Ma et al. (2017), the reason not to fine-tune all
the layers is due to the limited text labels in the tar-
get domain. Indeed, we also observe overfitting if
adapting all layers, but adapting only the softmax
layer effectively decreases the PPL on the text la-
bels of dev sets. We refer to this fine-tuning as
RNNLM-adapted in the following sections.

To make a fair comparison with the adapted
model, we also use the RNNLM as an initializa-
tion for our LMLM and rLMLM. In total, there
are four language models for rescoring the beams.
RNNLM and its adapted version that aim at reduc-
ing PPL; and the two proposed methods, LMLM
and rLMLM that try to fit to WER.

4.1 WSJ Dataset

The WSJ corpora consists of about 80 hours of
read speech with texts drawn from a machine-
readable corpus of Wall Street Journal news.
We use the standard configuration of train si284
dataset for training, dev93 for development and
eval92 for testing.

Our ASR model has one convolution layer, fol-
lowed by 5 bidirectional RNNs and one fully con-
nected layer, with a CTC loss on top. The text
labels of the training set are used to train a 4-gram
language model, which is employed in the ASR
decoder. The beam search decoder has a beam
width of 2000. Before beam rescoring, this ASR
system achieves a WER of 12.16 on dev93 set and
7.69 on eval92 set. To put this into perspective,
we list some previous state-of-the-art system in
Tab. 2. Compared with them, our baseline is al-
ready very competitive.

4.1.1 WERs and PPLs
The out-of-vocabulary rate of WSJ text is only
0.28%, making the RNNLM reasonable to use.



1187

We apply the RNNLM, RNNLM-adapted (Ma
et al., 2017), LMLM and rLMLM to rescore the
beams on dev and test set. The final score assigned
to a beam is a weighted sum of the ASR and lan-
guage model scores. The weight is found by min-
imizing the WER on the dev set.

Tab. 3 reports the WERs on dev93 and eval92
sets. All methods reduce the WER over the
baseline without rescoring. However, LMLM
and rLMLM are notably better than the other
two methods. Moreover, although RNNLM and
RNNLM-adapted achieve smaller PPLs on the text
labels, the advantage does not transfer to WER.

ASR Models
WER

dev93 eval92
EESEN (Miao et al., 2015) N/A 7.34

Attention (Bahdanau et al., 2016) N/A 9.30
Gram-CTC (Liu et al., 2017) N/A 6.75
5-layer Bidi-RNNs (baseline) 12.16 7.69

Table 2: Published WERs on WSJ dev93 and
eval92 set

rescoring WER PPL
language model dev93 eval92 dev93 eval92

baseline
12.16 7.69 N/A N/A

(no rescore)
RNNLM 10.71 6.59 207.43 205.00

RNNLM-adapted 10.11 6.34 159.50 157.85
LMLM 9.44 5.56 575.83 563.69
rLMLM 9.63 5.48 345.60 348.32

Table 3: Rescore 2000-best list of WSJ dev93 and
eval92 set. Digits in bold are the best and ital-
ics are the runner-ups. Lower PPL does not corre-
spond to lower WER.

4.1.2 Correlation between scores and WERs
To better understand the proposed methods, we
calculate the correlation coefficients between the
hypotheses’ WERs and their scores (by different
language models). In specific, for every utterance
in the test set, we have a set of beam candidates,
their word level accuracies (100-WER) and scores
given by an LM, from which a Pearson correla-
tion coefficient can be calculated. We calculate
the coefficients for all the utterances in the test set,
and boxplot these coefficients in Fig. 3. The cor-
relation coefficients by LMLM and rLMLM tend

RNNLM adapted LMLM rLMLM
0.0

0.2

0.4

0.6

0.8

Figure 3: Correlation coefficients between word
level accuracy (1 − WER/100) and LM-scores
by the different LMs, higher is better. Red hori-
zontal lines are medians. Green dots are means.
Whiskers are 5% and 95% quantiles. Lower and
upper box boundaries are 25% and 75% quantiles.

to be higher than RNNLM and RNNLM-adapted.
This indicates that LMLM and rLMLM are more
aligned with the goal of reducing WER.

4.1.3 Case Study
Tab. 4 posts some examples from the test set. The
first column lists the ground-truth labels, and their
corresponding best candidates as re-ranked by the
four LMs (see notes in the second column). Words
in red are mistakes made by the candidate sen-
tences. Scores of these sentences are listed in the
last four columns. We have the following observa-
tions:

1. LMLM and rLMLM give worse scores
on the ground-truth labels than RNNLM
and RNNLM-adapted, which explains their
higher PPL in Tab. 3.

2. In the first example, RNNLM and RNNLM-
adapted assign higher scores to a shorter sen-
tence. This is reasonable (though not neces-
sarily desirable) as LM-score is a summation
of log-probabilities, each of which is nega-
tive. In contrast, LMLM and rLMLM are
able to assign higher scores to longer and bet-
ter candidates.

3. In the other two examples, LMLM and
rLMLM seem to favor more sensible sen-
tences, though they are not more grammatical
than those picked by RNNLM and RNNLM-
adapted. We conjecture that since LMLM
and rLMLM utilize beam candidates in their
training, they capture and compensate for



1188

references and hypotheses reference orranked 1st by
LM-score

RNNLM RNNLM-adapted LMLM rLMLM

for such group rate coverage employers can charge the
former workers and their families the average cost of providing

the health benefits plus a two percent administrative fee

reference,
LMLM -144.74 -142.90 -162.61 -165.93

for such group rate coverage employers can charge the
former workers and their families the average cost of providing

the health benefits plus two percent administrative

RNNLM,
RNNLM-adapted -144.53 -142.85 -172.66 -168.10

for such group rate coverage employers can charge the
former workers and their families the average cost of providing

their health benefits plus a two percent administrative fee
rLMLM -146.72 -145.67 -163.73 -162.92

we’d like to see something that leads to real democracy
says jaime bonilla vice secretary general reference -105.8 -106.28 -122.93 -108.42

we’d like to see something that leads the real democracy
says jm bonier vice secretary general

RNNLM,
RNNLM-adapted -103.13 -102.84 -141.94 -118.38

we’d like to see something that leads to real democracy
says jim bone vice secretary general LMLM, rLMLM -104.52 -105.37 -125.40 -104.73

the big shoe is going to drop when we see the trade number reference -64.28 -61.04 -80.63 -82.91

the big she was going to drop in to see the trade numbers RNNLM,RNNLM-adapted -64.32 -61.68 -94.26 -86.89

the big shoe is going to drop in see the trade number LMLM, rLMLM -67.53 -64.50 -84.09 -84.65

Table 4: Some “gold” references and best hypotheses (after rescoring by different language models) for
eval92 set. In red are errors or missing word (denoted as ‘ ’).

some weakness in the ASR, which is not
achieved by RNNLM and RNNLM-adapted.

4.2 10K Speech Dataset

We further validate our methods on a larger noisy
dataset collected by Liu et al. (2017). The dataset
has about 10K hours of spontaneous speech. The
utterances are corrupted by background noise, and
a large portion of them are accented. Therefore it
is much more challenging than WSJ. We adopt the
same training-dev-test split as in Liu et al. (2017).
In specific, there are 5.4M utterances for training,
2,066 for development and 2,054 for testing.

rescoring WER PPL
language model dev test dev test

baseline
19.17 20.90 N/A N/A

(no rescore)
RNNLM 18.38 20.07 264.21 252.85

RNNLM-adapted 18.29 20.03 236.74 226.22
LMLM 18.17 19.62 2250.79 2095.39
rLMLM 17.98 19.49 1225.04 1152.63

Table 5: Rescore 2000-best list of our internal dev
and test set. Digits in bold are the best and italics
are the runner-ups.

The ASR we build has the same architecture as
in Liu et al. (2017), except that its decoder inte-
grates an in-domain 5-gram language model. This
system achieves a WER of 19.17 on dev set, bet-
ter than the reported 19.77 baseline in Liu et al.
(2017). Based on the ASR, we repeat the same
experiments in section 4.1. Tab. 5 reports WERs
and PPLs on dev and test sets. Both LMLM and
rLMLM outperform the other methods in WER,
although their PPLs are higher. This trend is simi-
lar to that in Tab. 3.

5 Experiments on NMT

In this section, we experiment the large-margin
criterion trained LM with a competitive Chinese-
to-English NMT system. The NMT model is
trained from 2M parallel sentence pairs. Follow-
ing Shen et al. (2016), we use NIST 06 newswire
portion (616 sentences) for development and NIST
08 newswire portion (691 sentences) for testing.
We use OpenNMT-py2 package with the default
configuration to train the model: batch size is 64;
word embedding size is 500; dropout rate is 0.3;
target vocabulary size is 50K; number of epochs is
20, after which a minimum dev perplexity of 7.72

2https://github.com/OpenNMT/OpenNMT-py

https: //github.com/OpenNMT/OpenNMT- py


1189

is achieved.

5.1 BLEUs and PPLs
We use a beam size of 10 for decoding, and re-
port case-insensitive 4-reference BLEU-4 scores
(by calling “multi bleu.perl”3). The NMT model
achieves 35.18 BLEU score on dev set and 31.52
on test set (see table 6). To put this into per-
spective, Shen et al. (2016) trains their models
on 2.56M pairs of sentences and reports a dev
BLEU score of 32.7 (via MOSES) or 30.7 (via
RNNsearch, beam size of 10). So our NMT model
is already very competitive.

To construct the training data for LMLM and
rLMLM, 10 beam candidates are extracted for ev-
ery sentence in the training set. We then fol-
low the same experimental steps outlined in sec-
tion 4.1, except that the ASR score is now changed
to NMT score. In addition, we also find that nor-
malizing the LM score by sentence length can
improve the re-scoring performance substantially.
Tab. 6 compares the BLEU score after re-ranking
by the different LMs. LMLM and rLMLM both
improve upon the baseline significantly, and out-
perform RNNLM and RNNLM-adapted by a no-
table margin. We also observe that the PPLs of
LMLM and rLMLM are much larger than those
of RNNLM and RNNLM-adapted, suggesting that
the PPL metric may be very poorly correlated with
BLEU.

Interestingly, RNNLM-adapted does not show
any gain in BLEU score over RNNLM. To under-
stand this, we recall that NMT is trained by min-
imizing PPL on target text. Its decoder is implic-
itly an RNNLM on target language. We conjecture
that adapting an LM to the target domain can only
duplicate the functionality of the NMT decoder,
which does not bring any additional benefit.

5.2 Correlation between scores and BLEUs
We measure the correlation between the LM
scores and BLUEs. The calculation is done on
dev06 set in the same way as Section 4.1.2, but
now we change the WERs to BLEUs. The box-
plot of the correlation coefficients are shown in
Fig. 4. Compared with the boxplot in Fig. 3,
now the correlation coefficients by all LMs are
more dispersed. Sometimes, they even take neg-
ative values. The mean correlation by LMLM

3https://github.com/OpenNMT/
OpenNMT-py/blob/master/tools/multi-bleu.
perl

rescoring BLEU PPL
language model dev06 test08 dev06 test08

baseline
35.18 31.52 N/A N/A

(no rescore)
RNNLM 36.17 32.17 129.91 137.25

RNNLM-adapted 36.17 31.97 78.20 89.27
LMLM 37.79 33.11 7.75e5 3.73e6
rLMLM 37.82 33.13 2.68e5 1.12e6

Table 6: Rescore 10-best list for dev (nist 06) and
test (nist 08) set. Digits in bold are the best and
italics are the runner-ups.

RNNLM adapted LMLM rLMLM
−1.0

−0.5

0.0

0.5

1.0

Figure 4: Correlation coefficients between BLEUs
and LM-scores by the different LMs, higher is
better. Red horizontal lines are medians. Green
dots are means. Whiskers are 5% and 95% quan-
tiles. Lower and upper box boundaries are 25%
and 75% quantiles.

and rLMLM, however, is considerably higher than
those by RNNLM and RNNLM-adapted.

6 Related Work

“Language modeling is an art of determining the
probability of a sequence of words” (Goodman,
2001). In the past decades, there has been a trend
of increasing the context that an LM can condition
on. N-gram models (Chen and Goodman, 1996)
assume that each symbol depends on the previ-
ous N − 1 symbols. Feed forward neural network
based LMs (Bengio et al., 2003) are not count
based but they inherit the restrictive assumption.
To model longer-term dependencies, RNNLMs
(Mikolov et al., 2010) are proposed. RNNLMs of-
ten achieve smaller PPLs than the N-gram coun-
terparts (Sundermeyer et al., 2012; Zaremba et al.,
2014; Jozefowicz et al., 2016). This paper focuses
on RNNLM-type architectures.

While these works all adopt PPL as the metric

 https://github.com/OpenNMT/OpenNMT-py/blob/master/tools/multi-bleu.perl
 https://github.com/OpenNMT/OpenNMT-py/blob/master/tools/multi-bleu.perl
 https://github.com/OpenNMT/OpenNMT-py/blob/master/tools/multi-bleu.perl


1190

to optimize, sometimes one may optimize a task-
specific objective. For example, Kuo et al. (2002);
Roark et al. (2007) and Dikic et al. (2013) pro-
pose discriminative LMs to improve speech recog-
nition. The common methodology therein is to
fit a probabilistic model, e.g., conditional random
field (Roark et al., 2004), to the space of text can-
didates, and maximize the probability at the de-
sired candidate. The problem is often solved by
perceptron algorithm. However, these methods all
rely on ad-hoc choice of features, e.g., counts of
n-grams where n varies in a small range (e.g.,1
to 3). Moreover, it is also not clear how these
methods would take advantage of an existing lan-
guage model (trained on large unsupervised cor-
pus). Nevertheless, the same methodology can be
extended to RNNLMs, thus avoiding the afore-
mentioned limitations. For example, Auli and Gao
(2014) train an RNNLM by favoring sentences
with high BLEU scores and integrate it into a
phrase-based MT decoder.

If we cast the problem of picking the best text
sequence as a ranking problem, the aforemen-
tioned works can be considered as “pointwise”
learning-to-rank approaches (Cossock and Zhang,
2008). In contrast, the proposed method is a “pair-
wise” approach (Liu, 2009), as it learns a neural
language model by comparison between pairs of
sentences. Earlier works in this fashion may date
back to (Collins and Koo, 2005), which improves
a semantic parser. Learning “by pairwise compari-
son” is also seen in several MT literatures. For ex-
ample, Hopkins and May (2011) propose to train
a phrase-based MT system by minimizing a pair-
wise ranking loss. Wiseman and Rush (2016) op-
timize the beam search process in a Neural Ma-
chine Translation (NMT) system. They enforce
the score of a reference to be higher than that of
its decoded k-th candidate by at least a unit mar-
gin.

Rather than optimizing the MT system itself,
this work proposes a general method of training
recurrent neural language models, which can ben-
efit various text generation tasks, including speech
recognition and machine translation.

7 Conclusions

We have proposed a large margin criterion for
training recurrent neural language models. Rather
than minimizing PPL, the proposed criterion
is based on comparison between pairs of sen-

tences. We have formulated two algorithms
that implement the training criterion. One com-
pares between references and imperfect hypothe-
ses (LMLM), the other compares between all pairs
of hypotheses (rLMLM). We applied the language
models trained by these two algorithms to speech
recognition and machine translation. Both of
them demonstrate superior performance over their
minimum-PPL counterparts. However, the perfor-
mance gain from LMLM to rLMLM is small, al-
though rLMLM is built on more pairwise compar-
isons and requires more training efforts. The ef-
ficiency with respect to the number of pairs is a
future research topic.

References
Dario Amodei, Sundaram Ananthanarayanan, Rishita

Anubhai, Jingliang Bai, Eric Battenberg, Carl Case,
and Jared Casper et al. 2016. Deep speech 2: End-
to-end speech recognition in english and mandarin.
In International Conference on Machine Learning.

Michael Auli and Jianfeng Gao. 2014. Decoder inte-
gration and expected bleu training for recurrent neu-
ral network language models. In ACL.

Dzmitry Bahdanau, Jan Chorowski, Dmitriy Serdyuk,
Philemon Brakel, and Yoshua Bengio. 2016. End-
to-end attention-based large vocabulary speech
recognition. In IEEE International Conference on
Acoustics, Speech and Signal Processing.

Yoshua Bengio, Réjean Ducharme, Pascal Vincent, and
Christian Jauvin. 2003. A neural probabilistic lan-
guage model. Journal of machine learning research,
3(Feb):1137–1155.

Stanley F. Chen and Joshua Goodman. 1996. An
empirical study of smoothing techniques for lan-
guage modeling. Computer Speech & Language,
13(4):359–394.

Xie Chen, Tian Tan, Xunying Liu, Pierre Lanchantin,
Moquan Wan, Mark J.F. Gales, and Philip C. Wood-
land. 2015. Recurrent neural network language
model adaptation for multi-genre broadcast speech
recognition. In 16th Annual Conference of the In-
ternational Speech Communication Association.

Michael Collins and Terry Koo. 2005. Discriminative
reranking for natural language parsing. Computa-
tional Linguistics, 31(1):25–70.

David Cossock and Tong Zhang. 2008. Statistical anal-
ysis of bayes optimal subset ranking. IEEE Trans-
actions on Information Theory, 54(11):5140–5154.

Erinç Dikic, Murat Semerci, Murat Saraçlar, and
Ethem Alpaydin. 2013. Classification and ranking
approaches to discriminative language modeling for



1191

asr. IEEE Transactions on Audio, Speech, and Lan-
guage Processing, 21(2):291–300.

Joshua T. Goodman. 2001. A bit of progress in lan-
guage modeling. Computer Speech & Language,
15(4):403–434.

Alex Graves, Santiago Fernández, Faustino Gomez,
and Jürgen Schmidhuber. 2006. Connectionist
temporal classification: labelling unsegmented se-
quence data with recurrent neural networks. In In
Proceedings of the 23rd international conference on
Machine learning.

Caglar Gulcehre, Orhan Firat, Kelvin Xu, Kyunghyun
Cho, Loic Barrault, Huei-Chi Lin, Fethi Bougares,
Holger Schwenk, and Yoshua Bengio. 2015. On us-
ing monolingual corpora in neural machine transla-
tion. arXiv preprint arXiv:1503.03535.

Mark Hopkins and Jonathan May. 2011. Tuning as
ranking. In EMNLP.

Rafal Jozefowicz, Oriol Vinyals, Mike Schuster, Noam
Shazeer, and Yonghui Wu. 2016. Exploring
the limits of language modeling. arXiv preprint
arXiv:1602.02410.

Philipp Koehn. 2009. Statistical machine translation.
Cambridge University Press.

Hong-Kwang Jeff Kuo, Eric Fosler-Lussier, Hui Jiang,
and Chin-Hui Lee. 2002. Discriminative training of
language models for speech recognition in acoustics.
In IEEE International Conference on Speech, and
Signal Processing (ICASSP).

Zhifei Li and Sanjeev Khudanpur. 2008. Large-scale
discriminative n-gram language models for statisti-
cal machine translation. In AMTA.

Hairong Liu, Zhenyao Zhu, Xiangang Li, and Sanjeev
Satheesh. 2017. Gram-ctc: Automatic unit selection
and target decomposition for sequence labelling. In
34th International Conference on Machine Learn-
ing.

Tie-Yan Liu. 2009. Learning to rank for information
retrieval, volume 3. Foundations and Trends R© in
Information Retrieval.

Min Ma, Michael Nirschl, Fadi Biadsy, and Shankar
Kumar. 2017. Approaches for neural-network lan-
guage model adaptation. In Proceedings of Inter-
speech.

Yajie Miao, Mohammad Gowayyed, and Florian
Metze. 2015. Eesen: End-to-end speech recogni-
tion using deep rnn models and wfst-based decod-
ing, pages 167–174. ieee, 2015. In Workshop on
Automatic Speech Recognition and Understanding
(ASRU).

Tomas Mikolov, Martin Karafiát, Lukas Burget, Jan
Černocký, and Sanjeev Khudanpur. 2010. Recurrent
neural network based language model. In 11th An-
nual Conference of the International Speech Com-
munication Association.

Junho Park, Xunying Liu, Mark J. F. Gales, and Phil C.
Woodland. 2010. Improved neural network based
language modelling and adaptation. In 11th Annual
Conference of the International Speech Communi-
cation Association.

Brian Roark, Murat Saraclar, and Michael Collins.
2007. Discriminative n-gram language modeling.
Computer Speech & Language, 21(2):373–392.

Brian Roark, Murat Saraclar, and Michael
Collins Mark Johnson. 2004. Discriminative
language modeling with conditional random
fields and the perceptron algorithm. In 42nd
Annual Meeting on Association for Computational
Linguistics.

Shiqi Shen, Yong Cheng, Zhongjun He, Wei He, Hua
Wu, Maosong Sun, and Yang Liu. 2016. Minimum
risk training for neural machine translation. In ACL.

Anuroop Sriram, Heewoo Jun, Sanjeev Satheesh, and
Adam Coates. 2017. Cold fusion: Training seq2seq
models together with language models. arXiv
preprint arXiv:1708.06426.

Martin Sundermeyer, Ralf Schlüter, and Hermann Ney.
2012. Lstm neural networks for language model-
ing. In 13th Annual Conference of the International
Speech Communication Association.

Yuuki Tachioka and Shinji Watanabe. 2015. Discrim-
inative method for recurrent neural network lan-
guage models. In IEEE International Conference on
Acoustics, Speech and Signal Processing (ICASSP).

Sam Wiseman and Alexander M. Rush. 2016.
Sequence-to-sequence learning as beam-search op-
timization. In EMNLP.

Dong Yu and Li Deng. 2014. Automatic Speech
Recognition: A Deep Learning Approach. Springer
Publish-ing Company.

Wojciech Zaremba, Ilya Sutskever, and Oriol Vinyals.
2014. Recurrent neural network regularization.
arXiv preprint arXiv:1409.2329.


