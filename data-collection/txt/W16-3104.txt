



















































Learning to Answer Biomedical Questions: OAQA at BioASQ 4B


Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 23–37,
Berlin, Germany, August 7-12, 2016. c©2016 Association for Computational Linguistics

Learning to Answer Biomedical Questions: OAQA at BioASQ 4B

Zi Yang Yue Zhou Eric Nyberg
Language Technologies Institute

School of Computer Science
Carnegie Mellon University
{ziy, ehn}@cs.cmu.edu

Abstract

This paper describes the OAQA system
evaluated in the BioASQ 4B Question
Answering track. The system extends
the Yang et al. (2015) system and inte-
grates additional biomedical and general-
purpose NLP annotators, machine learn-
ing modules for search result scoring, col-
lective answer reranking, and yes/no an-
swer prediction. We first present the over-
all architecture of the system, and then fo-
cus on describing the main extensions to
the Yang et al. (2015) approach. Before
the official evaluation, we used the devel-
opment dataset (excluding the 3B Batch
5 subset) for training. We present initial
evaluation results on a subset of the devel-
opment data set to demonstrate the effec-
tiveness of the proposed new methods, and
focus on performance analysis of yes/no
question answering.

1 Introduction

The BioASQ QA challenge (Tsatsaronis et al.,
2015) evaluates automatic question answering
technologies and systems in the biomedical do-
main. It consists of two phases: in Phase A, the
task requires to retrieve relevant document, snip-
pets, concepts, and triples given a natural lan-
guage question, and evaluates the retrieval results
in terms of mean average precision (MAP); in
Phase B, the task requires to generate ideal an-
swers for the questions, which are evaluated us-
ing accuracy and mean reciprocal rank (MRR), as
well as exact answers, which are evaluated based
on manual judgment. The OAQA team partici-
pated in Batches 3, 4, and 5 of BioASQ 4B, in
the categories of document, snippet, and concept
retrieval, factoid, list and yes/no question answer-

ing (exact answer generation). The source code of
the participating system can be downloaded from
our GitHub repository1.

We follow the same general hypothesis ex-
pressed in Ferrucci et al. (2009) and Yang et
al. (2015), specifically that informatics challenges
like BioASQ are best met through careful design
of a flexible and extensible architecture, coupled
with continuous, incremental experimentation and
optimization over various combinations of exist-
ing state-of-the-art components, rather than rely-
ing on a single “magic” component or single com-
ponent combination. This year, the number of la-
beled questions in the development set has grown
to 1,307 (up from 810 in last year’s dataset), which
allows further exploration of a) the potential of su-
pervised learning methods, and b) the effective-
ness of various biomedical NLP tools in various
phases of the system, from relevant concept and
document retrieval to snippet extraction, and from
answer text identification to answer prediction.

First, we use TmTool2 (Wei et al., 2016), in ad-
dition to MetaMap3, to identify possible biomed-
ical named entities, especially out-of-vocabulary
concepts. We also extract frequent multi-word
terms from relevant snippets (Frantzi et al., 2000)
to further improve the recall of concept and can-
didate answer text extraction. Second, we pro-
pose a supervised learning method to rerank the
answer candidates for factoid and list questions
based on the relation between each candidate an-
swer and other candidate answers, which we refer
to as collective reranking in this paper. Third, we
implement a yes/no question answering pipeline
combining various heuristics, e.g. negation words,
sentiment of the statements, the biomedical con-

1https://github.com/oaqa/bioasq
2http://www.ncbi.nlm.nih.gov/

CBBresearch/Lu/Demo/tmTools/
3http://metamap.nlm.nih.gov/

23



cepts mentioned in the relevant snippets that be-
long to the same concept type, and question in-
version (Kanayama et al., 2012). Finally, we in-
troduce a unified classification interface for judg-
ing the relevance of each retrieved concept, docu-
ment, and snippet, which can combine the relevant
scores evidenced by various sources, e.g. retrieval
scores using different queries and indexes.

This paper describes the system that was evalu-
ated in the BioASQ 4B challenge. We first review
the system architecture and the approaches used
in Yang et al. (2015) in Section 2, and then we fo-
cus on describing each individual component for
BioASQ 4B in Sections 3 to 6. Before the offi-
cial evaluation, we trained the system using the de-
velopment dataset excluding the 3B Batch 5 sub-
set; we evaluate the proposed approach using the
held-out 3B Batch 5 subset. Section 7 presents
the results, which illustrate the effectiveness of
the proposed methods, and Section 8 presents a
manual error analysis of the proposed yes/no QA
method and highlight the challenges of biomedi-
cal yes/no QA problem. We conclude and present
future work in Section 9.

2 Overview of Yang et al. (2015) System

In this section, we briefly describe the architecture
of the Yang et al. (2015) system, which provided
the baseline for the system evaluated here. Further
detail can be found in the full paper.

The Yang et al. (2015) system uses the UIMA
ECD/CSE framework4 (Garduno et al., 2013;
Yang et al., 2013) with a YAML5-based lan-
guage to support formal, declarative descriptors
for the space of system and component configu-
rations to be explored during system optimization.
The system employs a three-layered architecture.
The first layer BaseQA6 is designed for domain-
independent QA components, and includes the ba-
sic input/output definition of a QA pipeline, inter-
mediate data objects, QA evaluation components,
and data processing components. In the second
layer, we implemented biomedical resources that
can be used in any biomedical QA task (outside
the context of BioASQ). A few BioASQ-specific
components were integrated in the third design
layer; for example, GoPubMed services are only
hosted for the purpose of the BioASQ challenge.

4https://github.com/oaqa/
cse-framework/

5http://yaml.org/
6https://github.com/oaqa/baseqa/

Tools and resources that are shared by multiple
components are defined as providers, includ-
ing NLP parsers, concept identification modules,
synonym expansion modules, classifiers, etc.

Resources. The Yang et al. (2015) system uses
LingPipe and ClearNLP7 (Choi and Palmer, 2011)
to parse the questions and relevant snippets using
models applicable to generic English texts as well
as biomedical texts, e.g. the parser models trained
on the CRAFT treebank (Verspoor et al., 2012).
It uses the named entity recognition (NER) mod-
ule from LingPipe8 trained on the GENIA corpus
(Kim et al., 2003) and MetaMap annotation com-
ponent (Aronson, 2001) to identify the biomedi-
cal concepts, and further uses UMLS Terminology
Services (UTS)9 to identify concepts and retrieve
synonyms. It uses the official GoPubMed services
for concept retrieval, and a local Lucene10 index
for document retrieval and snippet retrieval. Li-
bLinear11 (Fan et al., 2008) is used to train clas-
sifiers to predict answer types to the questions
and estimate the relevance scores of candidate an-
swers.

Answer Type Prediction. To identify the gold
standard labels for the existing Q/A pairs used for
training, the Yang et al. (2015) system employs
UTS to retrieve the semantic types for each gold
standard exact answer. A number of linguistic
and semantic features are extracted from the to-
kens and concepts, including the lemma form of
each token, the semantic type of each concept in
the question, the dependency label of each token,
combination of semantic type labels and depen-
dency labels, etc., where the concepts are identi-
fied from MetaMap, LingPipe NER, and Apache
OpenNLP Chunker12 (noun phrases). A multi-
class Logistic Regression classifier is trained using
the LibLinear tool (Fan et al., 2008).

Candidate Answer Generation. Depending
on the question type (general factoid/list question,
CHOICE question, or QUANTITY question), the
Yang et al. (2015) system applies different strate-
gies to generate candidate answers. For general
factoid/list questions, it generates a candidate an-
swer using each concept identified by one of three

7http://www.clearnlp.com
8http://alias-i.com/lingpipe/
9https://uts.nlm.nih.gov/home.html

10https://lucene.apache.org/
11http://www.csie.ntu.edu.tw/˜cjlin/

liblinear/
12https://opennlp.apache.org/

24



concept identification approaches. For CHOICE
questions, it first identifies the “or” token in the
question and its head token, which is most likely
the first option in the list of candidate answers, and
then finds all the children of the first option token
in the parse tree that have a dependency relation of
conj, which are considered to be alternative op-
tions. For QUANTITY questions, it identifies all
the tokens that have a POS tag of CD in all rele-
vant snippets.

Candidate Answer Scoring and Pruning. The
Yang et al. (2015) system extends the approach
used by Weissenborn et al. (Weissenborn et al.,
2013) and defines 11 groups of features to capture
how likely each candidate answer is the true an-
swer for the question from different aspects, which
includes answer type coercion, candidate answer
occurrence count, name count, average overlap-
ping token count, stopword count, overlapping
concept count, token and concept proximity, etc.
A Logistic Regression classifier is used to learn
the scoring function, where the class is weighted
by their frequencies. A simple threshold based
pruning method is trained from the development
dataset and applied to the list questions.

Besides incorporating a larger development data
set, our OAQA system extends the Yang et al.
(2015) system by integrating additional biomedi-
cal and general-purpose NLP annotators, and in-
troducing trainable modules in more stages of
the pipeline, such as using supervised methods
in search result reranking, answer reranking, and
yes/no answer prediction, which we will detail in
the following sections. The architecture diagrams
are illustrated in Figures 1, 2, and 3 in Appendix.

3 Concept Identification

We use the MetaMap and LingPipe concept iden-
tification modules with the GENIA model from
Yang et al. (2015). However, due to the exces-
sive noise introduced from the Apache OpenNLP
Chunker based method, which extracts all noun
phrases, we discard this approach. In addition, we
integrate the TmTool biomedical concept identifi-
cation RESTful service (Wei et al., 2016) for both
the semantic type labeling of gold standard ex-
act answers and question/snippet annotation, and
also use C-Value (Frantzi et al., 2000), a frequent
phrase mining method, to extract potential out-of-
vocabulary multi-word terms.

3.1 TmTool for Annotating Questions,
Snippets, and Answer Texts

The TmTool provides a standard Web service
interface to annotate biomedical concepts us-
ing a number of state-of-the-art biomedical NLP
parsers, which includes GNormPlus/SR4GN (for
genes and species), tmChem (for chemicals),
DNorm (for diseases), and tmVar (for mutations).
Although it can only identify biomedical concepts
belonging to any of these categories, they ac-
count for a great portion of the concepts used in
the BioASQ corpus. In addition, many of these
parsers utilize morphological features to estimate
the likelihood of a term being a biomedical con-
cept, rather than relying on an existing ontology
like MetaMap, which makes it complementary to
the existing tools in Yang et al. (2015).

TmTool supports three data exchange formats:
PubTator (tab-separated), BioC (XML) and Pub-
Annotation (JSON). Since the PubTator format
does not support DNorm annotation, and BioC
format sometimes causes a single-sentence request
to timeout (no response after 20 minutes) , we
chose the robustest PubAnnotation format. We
also found that the offsets returned from the Tm-
Tool RESTful service might not align well with
original request texts, especially with tmChem
trigger, and hence we implement an escape
method to convert the texts into a TmTool com-
patible format by replacing some non-ASCII char-
acters with their normalized forms, and removing
special characters.

We use the TmTool to identify the biomedical
concepts and annotate their semantic types from
both the questions (in Phases A and B) and the
relevant snippets (in Phase B) in the same man-
ner as MetaMap. As the semantic type set of
concepts has expanded to include TmTool concept
types, the answer type prediction module should
also be able to predict these additional semantic
types. Therefore, we also use the TmTool to la-
bel the semantic types of the gold standard exact
answers. In particular, we concatenate all the ex-
act answers of each factoid or list question using
commas, and send the concatenated string to the
TmTool service, instead of each exact answer at
a time. For example, if the gold standard exact
answer is a list of strings: “NBEAL2”, “GFI1B”,
“GATA1”, then a single string “NBEAL2, GFI1B,
GATA1” will be sent.

25



3.2 C-Value for Extracting Multi-Word
Terms from Snippets

We treat the relevant snippets provided for each
question in Phase B as a corpus, and we hypoth-
esize that if a multi-word phrase is frequent in
the corpus, then it is likely a meaningful concept.
In order to extract not only high-frequency terms
but also high-quality terms, a C-Value criterion
(Frantzi et al., 2000) is introduced, which subtracts
the frequency of its super terms from a term’s
own frequency. In this way, it returns the longer
multi-word terms if two candidate terms overlap
and have the same frequency. This approach only
applies to a corpus, rather than a single sentence.
Therefore, we only use this method to extract con-
cepts from snippets. In the future, we may con-
sider to collect a corpus relevant to the question,
in order to apply the same idea to questions.

4 Collective Answer Reranking

We employ a collective answer reranking method
aiming to boost the low-ranked candidate answers
which share the same semantic type with high-
ranked candidate answers for list questions, and
use an adaptive threshold for pruning. The in-
tuition is that list questions always ask for a list
of concepts that have the same properties, which
implies that the concepts usually have the same
semantic types (e.g. all of them should be gene
names). After the answer scoring step where a
confidence score is assigned to each candidate an-
swer individually, we can imagine the top candi-
date answers might have mixed types. For exam-
ple, in a situation where the second answer is a
disease, but the rest of the top-5 answers are all
gene names, we should expect that the second an-
swer should be down-ranked.

We use the same labels used for training the
candidate answer scoring model, but incorporate
features that measure how similar each answer is
to the other top-ranked answers, which are de-
tailed in Table 1. The token distance counts the
number of intermediate tokens between the candi-
date answer tokens in the snippet text, and Leven-
shtein edit distance and shape edit distance mea-
sure the morphological similarities between the
answer texts. Common semantic type count should
“promote” the candidate answers that have a large
number of semantic types in common with the top
ranked answers.

For each candidate answer, we calculate a fea-

ture value, according to Table 1, for each other
candidate answer in the input candidate list, and
then we calculate the max/min/avg value corre-
sponding to the top-k candidate answers. We use
1, 3, 5, 10 for k, and use Logistic Regression to
train a binary classifier by down-sampling the neg-
ative instances to balance the training set. In addi-
tion to list questions, we also apply the method to
factoid questions. In Section 7, we observe if the
hypothesis also holds for factoid questions.

5 Learning to Answer Yes/No Questions

We consider the yes/no question answering prob-
lem as a binary classification problem, which al-
lows to prioritize, weight, and blend multiple
pieces of evidence from various approaches using
a supervised framework. We list the sources of ev-
idence (features) integrated into the system.

“Contradictory” concept. First, we hypothe-
size that if a statement is wrong, then the rel-
evant snippets should contain some statements
that are contradictory to the original statement,
with some mentions of “contradictory” concepts
or “antonyms”. To identify pairs of contradic-
tory concepts or antonyms is difficult given the
resources that we have. Instead, we try to iden-
tify all the different concepts in the snippets that
have the same semantic type as each concept in
the original statement. For a given concept type,
the more the unique concepts are found in both
question and relevant snippets, or the less the con-
cepts in the questions are found in the snippets, the
more likely the original statement is wrong.

Formally, for a concept type t, we calculate a
“contradictory” score as follows:∑

s∈S
∑

c∈s[type(c) = t]∑
c∈q[type(c) = t] +

∑
s∈S

∑
c∈s[type(c) = t]

where S is the set of snippets, q is the question,
c is a concept mention, and [type(c) = t] takes
1 if the concept c is type t and 0 otherwise. We
derive the aggregated contradictory score from the
concept type level scores using max/min/average
statistics. We calculate a number of similar statis-
tics to estimate how likely each snippet contradicts
the original statement.

Overlapping token count. In case the concept
identification modules fail to identify important
concepts in either the original questions or rele-
vant snippets, we also consider the difference of

26



No. Feature
1 the original score from the answer scoring prediction
2 min/max/avg token distance between each pair of candidate answer occurrences
3 min/max/avg Levenshtein edit distance between each pair of candidate answer variant names
4 min/max/avg number (and percentage) of semantic types that each pair of candidate answers

have in common
5 min/max/avg edit distance between each pair of candidate answer variant names after trans-

formed into their shape forms (i.e. upper-case letters are replaced with ‘A’, lower-case letters
are replaced with ‘a’, digits are replaced with ‘0’, and all other characters are replaced with ‘-’.)

Table 1: Collective Answer Reranking Features

No. Feature
1 “contradictory” concept count in the relevant snippets
2 overlapping token count in the relevant snippets
3 expected answer count in the relevant snippets
4 sentiment analysis via positive and negative word count of each relevant snippet
5 negation word count of each relevant snippet
6 question inversion

Table 2: Yes/No Question Answering Features

token mentions between the original question and
the relevant snippets, instead of concepts.

Expected answer count. Not all concepts and
tokens are equally important in the original ques-
tions. We find that many times the focus of a
yes/no question is the last concept mention, which
we denote as the expected answer. We count the
frequency (and the percentage) that the expected
answer is mentioned in the relevant snippets, as
well as the frequency that concepts of the same
type are mentioned.

Positive / negative / negation word count.
Sometimes, an explicit sentiment is expressed in
the relevant snippets to indicate how confident the
author believes a statement is true or false. We
use a simple dictionary 13 based method (Hu and
Liu, 2004) for sentiment analysis, and we count
whether and how many times each positive / neg-
ative word is mentioned in each snippet, then ag-
gregate across the snippets using min / max / aver-
age. We also use a list of common English nega-
tion words14 for negation detection, for simplicity.
Intuitively, a high overlapping count with a high
negative or negation count indicates that the origi-
nal statement tends to be incorrect.

Question inversion. The question inversion

13http://www.cs.uic.edu/˜liub/FBS/
opinion-lexicon-English.rar

14http://www.enchantedlearning.com/
wordlist/negativewords.shtml

method (Kanayama et al., 2012) answers a yes/no
question by first converting it to a factoid ques-
tion, then applies an existing factoid question an-
swering pipeline to generate a list of alternate can-
didate answers, and finally evidence each candi-
date answer and rank them. If the expected an-
swer in the original question is also ranked at the
top among all candidates for the factoid question,
then the statement is true.

In our system, we first assume the last con-
cept mention corresponds to the expected answer.
Therefore, its concept type(s) are also the answer
type(s) of the factoid question, and all the syn-
onyms of the concept are the answer variants. Af-
ter the token(s) and concept mention(s) covered by
the expected answer are removed from the orig-
inal question and the question type is changed
to FACTOID, we use the candidate answer gen-
eration and scoring pipeline for the factoid QA
to generate and rank a list of candidate answers.
Since annotating additional texts is computation-
ally expensive, we do not retrieve any relevant
snippets for the converted factoid questions, in-
stead we only use the relevant snippets of the origi-
nal yes/no questions (provided as part of the Phase
B input). The rank and the score of the expected
answer are used as question inversion features for
yes/no question training.

We use a number of classifiers, e.g. Logistic Re-
gression, Classification via Regression (Frank et

27



al., 1998), Simple Logistic (Landwehr et al., 2005)
using LibLinear and Weka15 tools (Hall et al.,
2009), after we down-sampled the positive (“yes”)
instances. In Section 7, we report not only the per-
formance of each method in terms of accuracy, but
also accuracy on the “yes” questions and the “no”
questions, since on an imbalanced dataset, a sim-
ple “all-yes” method is also a “strong” baseline.

6 Retrieval Result Reranking via
Relevance Classification

For relevant document, concept, and snippet re-
trieval, we first retrieve a list of 100 candidate re-
sults, then we define a set of features to estimate
the relevance of each candidate result and employ
a standardized interface to incorporate these fea-
tures to rerank the retrieval result, which is dif-
ferent from Yang et al. (2015), where each stage
employs a different retrieval / reranking strategy.

First, we replace the GoPubMed services with
local Lucene indexes as the response time is es-
timated to be at least 20 times faster, although the
task performance could be slightly worse (.2762 in
terms of MAP using the GoPubMed concept Web
service vs. .2502 using the local Lucene index in
our preliminary experiment for concept retrieval).
The concept Lucene index was created by fusion
of the same biomedical ontologies used by the
GoPubMed services, where we create 3 text fields:
concept name, synonyms, and definition, and 2
string fields: source (Gene Ontology, Disease On-
tology, etc) and URI. The document Lucene index
was created from the latest MEDLINE Baseline
corpus16 using Lucene’s StandardAnalyzer. After
a list of documents are retrieved and segmented
into sections and sentences, the snippet Lucene
index is then built in memory on-the-fly at the
sentence level. The search query is constructed
by concatenating all synonyms of identified con-
cepts (enclosed in quotes) and all tokens that are
neither covered by any concept mentions nor are
stop words, where the most 5,000 common En-
glish words17 are used as the stop list. Then, the
query searches all text fields.

The standardized search result reranking inter-
face allows each retrieval task to specify different
scoring functions (features). The features that we
used for concept, document, and snippet retrieval

15http://www.cs.waikato.ac.nz/ml/weka/
16https://mbr.nlm.nih.gov/
17http://www.wordfrequency.info/

are listed in Table 3. For example, during concept
search result reranking, we can check if each can-
didate concept is also identified in the question by
a biomedical NER. During snippet reranking, we
can also incorporate the meta information, such
as section label (title, abstract, body text, etc.),
offsets in the section, and length of each snip-
pet. In the candidate retrieval step, we have used
a query that combines all non stop words and con-
cepts identified by all biomedical concept anno-
tators, in order to guarantee high recall. How-
ever, it does not optimize the precision. For ex-
ample, some annotators/synonym expansion ser-
vices may falsely identify concepts and introduce
noisy search terms, and some search fields tend
to be less informative than others. Therefore, in
the reranking step, we employ various query for-
mulation strategies, e.g. only within certain text
fields and/or only using a subset of concept anno-
tators, and consider the search score and rank of
each candidate search result as features.

For this year’s evaluation, we use Logistic Re-
gression to learn relevance classifiers for all the
reranking tasks, after negative instances are down-
sampled to balance the training set. In the future,
we can also integrate learning-to-rank modules.

7 Results

Besides the proposed methods described in Sec-
tions 3 to 6, we also made a few minor changes to
the Yang et al. (2015) system, including

1. separating the texts in the parentheses in all
gold standard exact answers as synonyms be-
fore gold standard semantic type labeling and
answer type prediction training,

2. introducing the “null” type for the exact an-
swer texts if neither of the two concept search
providers (TmTool or UTS) can identify,

3. and adding nominal features (e.g. answer
type name, concept type name, etc.) in addi-
tion to existing numeric features (e.g. count,
distance, ratio, etc.) for candidate answer
scoring.

In this section, we first report the evaluation re-
sults using the held-out BioASQ 3B Batch 5 sub-
set, and then we conduct a manual analysis using
BioASQ 4B dataset for our yes/no question an-
swering method.

We first compare the retrieval results (Phase A)
In Table 4. We can see that the proposed retrieval

28



No. Feature
Concept

1 the original score from the concept retrieval step
2 overlapping concept count between the retrieval results and mentions annotated in the question
3 retrieval scores using various query formulation strategies

Document
1 the original score from the document retrieval step
2 retrieval scores using various query formulation strategies

Snippet
1 the score of the containing document
2 meta information, including the section label (abstract or title), binned begin/end offsets, binned

length of the snippet, etc.
3 retrieval scores using various query formulation strategies

Table 3: Retrieval Result Reranking via Relevant Classification Features

Method MAP F1 Precision Recall
Concept

LR .3216
.0297 .0154 .5504

NO .2361
Document

LR .1364
.0462 .0284 .2709

NO .1003
Snippet

NO .1073
.0147 .0079 .3015

LR .0826

Table 4: Evaluation results on BioASQ 3B Batch
5 Phase A subset. LR represents a Logistic Re-
gression based reranking method is used, and NO
means no operation is performed, i.e. original re-
trieval scores are used.

result reranking method via Logistic Regression
improves the performance of concept and doc-
ument retrieval, but not snippet retrieval, which
may be due to the fact that the input candidate
snippets have been reranked using a similar set of
features at the document reranking step, and no
further information is provided during the subse-
quent snippet reranking step.

The exact answer generation results (Phase B)
are shown in Table 5. We see that the best con-
figuration for factoid question answering in terms
of MRR is keeping the original feature set with
no collective reranking. However, if additional
features are used, then the collective reranking
method can improve the performance, and achieve
the highest lenient accuracy score.

To answer list questions, we tune the thresh-
olds (hard threshold, or TP and ratio threshold,

or RP) and report the results from the thresholds
that maximize the F1 score. Although the best
F1 score is achieved by incorporating additional
features without collective reranking, and using a
ratio-based pruning method, all other configura-
tions without collective reranking have the lowest
performance. In addition, we can see that addi-
tional features improve the performance in gen-
eral, and after carefully tuning of the threshold
and the ratio in the pruning step, we can achieve
the same level of performance. We hypothe-
size that the proposed method (CR + RP) can re-
normalize the answer scores and is thus more ro-
bust than the baseline system (NO + TP) in the
sense that the performance of the former approach
is less sensitive to the predefined threshold, al-
though the latter can sometimes outperform the
former when the threshold is carefully tuned. We
submitted two runs in BioASQ 4B Batch 5 eval-
uation: oaqa-3b-5 and oaqa-3b-5-e for the
proposed and baseline methods respectively (us-
ing the same thresholds), and initial evaluation re-
sult confirms our hypothesis.

Due to the imbalance between “yes” and “’no”
questions, we report the mean negative and posi-
tive accuracy scores in addition to the overall ac-
curacy for yes/no question answering. We can see
the performance is very sensitive to the choice of
the classifier. Using the same set of features, Clas-
sificationViaRegression achieves the highest per-
formance, with both negative and positive accu-
racy scores greatly above 0.5 (random). All other
methods tend to predict “yes”, which results in a
high positive accuracy but a low (below 0.5) neg-
ative accuracy score.

29



Factoid
Method Len.Ac. MRR Str.Ac.
OF + NO .5000 .3843 .3182
OF + CR .4545 .3791 .3182
AF + CR .5455 .3732 .2727
AF + NO .5000 .3689 .2727

List
Method F1 Precision Recall
AF + NO + RP .4291 .4449 .4593
AF + CR + RP .4246 .4045 .4864
AF + CR + TP .3969 .4100 .4267
OF + CR + TP .3704 .4231 .3645
OF + CR + RP .3629 .3654 .3874
AF + NO + TP .3463 .3840 .3677
OF + NO + RP .3460 .3188 .4431
OF + NO + TP .1461 .2639 .1183

Yes/No
Method Ac. Neg.Ac. Pos.Ac.
CVR .7143 .7778 .6842
SL .7143 .4444 .8421
AY .6786 .0000 1.0000
LR .5357 .2222 .6842

Table 5: Evaluation results on BioASQ 3B Batch
5 Phase B subset. OF and AF represent Orig-
inal or Additional features are used in training
and predicting answer scorers for factoid and list
questions. CR represents the Logistic Regression
based Collective Reranking is used. TP means
a hard Threshold is used to prune the answer,
whereas RP uses the relative Ratio to the maxi-
mum score. CVR and SL are ClassificationViaRe-
gression and SimpleLogistic classifiers from the
Weka toolkit. AY means ”All-Yes”, a simple but
strong baseline, in terms of accuracy.

8 Analysis

From Section 7, we see that, despite integration
of various sources of evidence, the current yes/no
question answering system is still unreliable. We
conducted a manual analysis of our yes/no ques-
tion answering method using BioASQ 4B dataset
based on our own judgment of yes or no, which
may not be consistent with the gold standard.

We found the BioASQ 4B dataset is more im-
balanced than the development dataset, where we
only identified six questions from all five test
batches that have a “no” answer. We applied the
proposed yes/no QA method to the six questions.
Among these questions, three are correctly pre-
dicted (namely, “Is macitentan an ET agonist?”,

“Does MVIIA and MVIIC bind to the same cal-
cium channel?”, and “Is the abnormal dosage
of ultraconserved elements disfavored in cancer
cells?”), and the answers to the other three ques-
tions are wrong. We conduct an error analysis for
the false positive predictions.

The first false positive question is “Are adenylyl
cyclases always transmembrane proteins?” The
key to this question is the recognition of the
contradictory concept pair “transmembrane” and
“soluble” or “transmembrane adenylyl cyclase
(tmAC)” and “soluble AC”. This requires first cor-
rectly identifying both terms as biomedical con-
cepts and then assigning correct semantic type la-
bels to them, where the latter can only be achieved
using MetaMap and TmTool. MetaMap correctly
identified “transmembrane proteins” in the ques-
tion and assigned a semantic label of “Amino
Acid, Peptide, or Protein”, and identified “soluble
adenylyl cyclase” in the snippet and assigned a se-
mantic label of “Gene or Genome”. Due to the
mismatch of semantic types “Amino Acid, Pep-
tide, or Protein” and “Gene or Genome”, the sys-
tem fails to recognize the contradiction.

In fact, we found that the same problem also
happened during the answer type prediction and
answer scoring steps, e.g. the question may be
predicted to ask for a “Gene or Genome”, but the
candidate answer is often labeled as “Amino Acid,
Peptide, or Protein” by MetaMap/UTS. Because
of the interchangeable use of “Amino Acid, Pep-
tide, or Protein” and “Gene or Genome” terms, we
might consider to treat them as one type. More-
over, the universal quantifier “always” also plays
an important role, in contrast to a question with an
existential quantifier such as “sometimes”, which
the current system has not captured yet. However,
this is not the main reason of the failure, since
we assume the relevant snippets will rarely men-
tion “soluble AC” if the question asks for whether
“transmembrane” exists.

The second false positive question is “Can
chronological age be predicted by measuring
telomere length?” This should be an easy one,
because we can find a negation cue “cannot” in
the snippet “telomere length measurement by real-
time quantitative PCR cannot be used to predict
age of a person”. The system integrates two types
of negation cue related features: the negation cue
count and the existence of a particular negation
cue. We found the system correctly identified

30



and counted the negation cue. Therefore, we sus-
pect the classifier did not optimize the combina-
tion of features. Furthermore, we need to observe
whether our hypothesis that the gold standard an-
swer (yes or no) is strongly correlated with the
negation word occurrence in the relevant snippets
is true using the development set.

The third false positive question is “Does the
3D structure of the genome remain stable during
cell differentiation?” The key to this question is
the word “stable”, which requires biomedical, esp.
genomics, knowledge to understand what “stable”
means in the context of genome structure. The
word “stable” is mentioned in one of the snip-
pets “the domains are stable across different cell
types”, which however does not answer the ques-
tion. Useful contradictory keywords that we find
in the relevant snippets include “reorganization”,
“alteration”, “remodelling”, etc. MetaMap/UTS
identified “stable” as a concept of semantic type
“Qualitative Concept”, whereas it labeled “reor-
ganization” as a “Idea or Concept” and missed
“alternation” and “remodelling”. It suggests that
our contradictory concept based method works the
best if the focus is factoid (entities), but the current
knowledge base can hardly support identification
of contradictory properties or behaviors.

We focus on 4B Batch 5 subset for error analy-
sis of false negative examples. In fact, the cases for
false negative questions are more diverse, which
makes it more difficult to find the causes of fail-
ures. One reason is that some snippets contain
multiple sentences or clauses, and only one is
crucial to answer the question, while others can
negatively influence the results. For example,
the snippet “OATP1B1 and OATP1B3-mediated
transport of bilirubin was confirmed and inhibi-
tion was determined for atazanavir, rifampicin, in-
dinavir, amprenavir, cyclosporine, rifamycin SV
and saquinavir.” has two clauses, but the sec-
ond one (“and inhibition...”), although is not rel-
evant to the question, introduces other chemical
names that confuse the classifier. Another prob-
lem is lack of understanding of specificity and
generality between concepts, e.g. “encephalopa-
thy” in the question is considered a different con-
cept from “Wernicke encephalopathy” mentioned
in the snippets, both belonging to the same disease
category. The classifier believed another disease
name is mentioned to contradict the statement.

We found that yes/no questions are more diffi-

cult to answer than factoid and list questions, since
there can be many different ways to support or op-
pose a statement. Although the problem can be
simply viewed as a binary classification problem,
due to the fact that a limited number of relevant
snippets are provided, simple token or phrase level
retrieval and statistics can hardly solve the prob-
lem. Instead, we believe that reliably answering
yes/no questions requires deeper linguistic and se-
mantic understanding of the questions and rele-
vant snippets, which includes leveraging semantic
networks of concepts to identify antonyms, hyper-
nyms, and hyponyms, and utilizing dependency
relations between the concepts, as well as senti-
ment analysis of the facts.

9 Conclusion

This paper describes the OAQA system evaluated
in the BioASQ 4B Question Answering track. We
first present the overall architecture of the system,
and then focus on describing the main differences
from the Yang et al. (2015) system, including two
concept identification modules: TmTool and C-
value based multi-word term extractor, collective
answer reranking, yes/no question answering ap-
proach, and a standardized retrieval result rerank-
ing method via relevant classification. We report
our initial evaluation results on 3B Batch 5 subset
show the effectiveness of the proposed new meth-
ods, and since the yes/no question answering ap-
proach is unsatisfactory, we further conduct an er-
ror analysis for yes/no questions using 4B subset.

As we mention in earlier sections, to further
improve the retrieval performance, we may use
learning-to-rank methods to rerank the retrieval
results. For exact answer generation, esp. for
yes/no questions, we believe a deeper linguistic
and semantic analysis of both questions and rel-
evant snippets are necessary. Our preliminary ex-
periment suggested that the word2vec (Mikolov et
al., 2013) based method did worse than the KB
based method in modeling the semantics of enti-
ties. We plan to study whether the former is com-
plementary to the latter in representing the seman-
tics of biomedical properties and event mentions.

Acknowledgments
We thank Ying Li, Xing Yang, Venus So, James
Cai and the other team members at Roche Innova-
tion Center New York for their support of OAQA
and biomedical question answering research and
development.

31



References
Alan R Aronson. 2001. Effective mapping of biomed-

ical text to the umls metathesaurus: the metamap
program. In Proceedings of the AMIA Symposium,
page 17. American Medical Informatics Associa-
tion.

Jinho D Choi and Martha Palmer. 2011. Getting the
most out of transition-based dependency parsing. In
Proceedings of the 49th Annual Meeting of the Asso-
ciation for Computational Linguistics: Human Lan-
guage Technologies: short papers-Volume 2, pages
687–692. Association for Computational Linguis-
tics.

Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-
Rui Wang, and Chih-Jen Lin. 2008. Liblinear: A li-
brary for large linear classification. J. Mach. Learn.
Res., 9(Aug):1871–1874.

David Ferrucci, Eric Nyberg, James Allan, Ken Barker,
Eric Brown, Jennifer Chu-Carroll, Arthur Ciccolo,
Pablo Duboue, James Fan, David Gondek, et al.
2009. Towards the open advancement of ques-
tion answering systems. Technical Report RC24789
(W0904-093), IBM Research Division.

Eibe Frank, Yong Wang, Stuart Inglis, Geoffrey
Holmes, and Ian H Witten. 1998. Using model trees
for classification. Machine Learning, 32(1):63–76.

Katerina Frantzi, Sophia Ananiadou, and Hideki
Mima. 2000. Automatic recognition of multi-word
terms:. the c-value/nc-value method. International
Journal on Digital Libraries, 3(2):115–130.

Elmer Garduno, Zi Yang, Avner Maiberg, Collin Mc-
Cormack, Yan Fang, and Eric Nyberg. 2013.
Cse framework: A uima-based distributed sys-
tem for configuration space exploration. In
UIMA@GSCL’2013, pages 14–17.

Mark Hall, Eibe Frank, Geoffrey Holmes, Bernhard
Pfahringer, Peter Reutemann, and Ian H Witten.
2009. The weka data mining software: an update.
ACM SIGKDD explorations newsletter, 11(1):10–
18.

Minqing Hu and Bing Liu. 2004. Mining and summa-
rizing customer reviews. In Proceedings of the tenth
ACM SIGKDD international conference on Knowl-
edge discovery and data mining, pages 168–177.
ACM.

Hiroshi Kanayama, Yusuke Miyao, and John Prager.
2012. Answering yes/no questions via question in-
version. In COLING, pages 1377–1392. Citeseer.

J-D Kim, Tomoko Ohta, Yuka Tateisi, and Junichi Tsu-
jii. 2003. Genia corpusa semantically annotated
corpus for bio-textmining. Bioinformatics, 19(suppl
1):i180–i182.

Niels Landwehr, Mark Hall, and Eibe Frank. 2005.
Logistic model trees. Machine Learning, 59(1-
2):161–205.

Tomas Mikolov, Kai Chen, Greg Corrado, and Jef-
frey Dean. 2013. Efficient estimation of word
representations in vector space. arXiv preprint
arXiv:1301.3781.

George Tsatsaronis, Georgios Balikas, Prodromos
Malakasiotis, Ioannis Partalas, Matthias Zschunke,
Michael R Alvers, Dirk Weissenborn, Anastasia
Krithara, Sergios Petridis, Dimitris Polychronopou-
los, et al. 2015. An overview of the bioasq
large-scale biomedical semantic indexing and ques-
tion answering competition. BMC Bioinformatics,
16(1):138.

Karin Verspoor, Kevin Bretonnel Cohen, Arrick Lan-
franchi, Colin Warner, Helen L Johnson, Christophe
Roeder, Jinho D Choi, Christopher Funk, Yuriy
Malenkiy, Miriam Eckert, et al. 2012. A corpus
of full-text journal articles is a robust evaluation tool
for revealing differences in performance of biomed-
ical natural language processing tools. BMC bioin-
formatics, 13(1):1.

Chih-Hsuan Wei, Robert Leaman, and Zhiyong Lu.
2016. Beyond accuracy: creating interoperable and
scalable text-mining web services. Bioinformatics,
pages 1–4.

Dirk Weissenborn, George Tsatsaronis, and Michael
Schroeder. 2013. Answering factoid questions in
the biomedical domain. In BioASQ’2013.

Zi Yang, Elmer Garduno, Yan Fang, Avner Maiberg,
Collin McCormack, and Eric Nyberg. 2013.
Building optimal information systems automati-
cally: Configuration space exploration for biomed-
ical information systems. In CIKM’2013, pages
1421–1430.

Zi Yang, Niloy Gupta, Xiangyu Sun, Di Xu, Chi
Zhang, and Eric Nyberg. 2015. Learning to answer
biomedical factoid & list questions: Oaqa at bioasq
3b. In CLEF’2015 (Working Note).

32



Appendix

Listing 1: ECD main descriptor for factoid and list
QA in batch 5 in Phase B

1 # execute
2 # mvn exec:exec -Dconfig=bioasq.

phase-b-test-factoid-list
3 # to test the pipeline
4
5 configuration:
6 name: phase-b-test-factoid-list
7 author: ziy
8
9 persistence-provider:

10 inherit: baseqa.persistence.
local-sqlite-persistence-provider

11
12 collection-reader:
13 inherit: baseqa.collection.json.

json-collection-reader
14 dataset: BIOASQ-QA
15 file:
16 - input/4b-5-b.json
17 type: [factoid, list]
18 persistence-provider: |
19 inherit: baseqa.persistence.

local-sqlite-persistence-provider
20
21 pipeline:
22 - inherit: ecd.phase
23 name: question-parse
24 options: |
25 - inherit: bioqa.question.parse.

clearnlp-bioinformatics
26
27 - inherit: ecd.phase
28 name: question-concept-metamap
29 options: |
30 - inherit: bioqa.question.concept.

metamap-cached
31
32 - inherit: ecd.phase
33 name: question-concept-tmtool
34 options: |
35 - inherit: bioqa.question.concept.

tmtool-cached
36
37 - inherit: ecd.phase
38 name: question-concept-lingpipe-genia
39 options: |
40 - inherit: bioqa.question.concept.

lingpipe-genia
41
42 - inherit: ecd.phase
43 name: question-focus
44 options: |
45 - inherit: baseqa.question.focus
46
47 - inherit: ecd.phase
48 name: passage-to-view
49 options: |
50 - inherit: baseqa.evidence.passage-to-view
51
52 - inherit: ecd.phase
53 name: evidence-parse
54 options: |
55 - inherit: bioqa.evidence.parse.

clearnlp-bioinformatics
56
57 - inherit: ecd.phase
58 name: evidence-concept-metamap
59 options: |
60 - inherit: bioqa.evidence.concept.

metamap-cached
61
62 - inherit: ecd.phase
63 name: evidence-concept-tmtool
64 options: |
65 - inherit: bioqa.evidence.concept.

tmtool-cached
66
67 - inherit: ecd.phase
68 name: evidence-concept-lingpipe-genia
69 options: |

70 - inherit: bioqa.evidence.concept.
lingpipe-genia

71
72 - inherit: ecd.phase
73 name: evidence-concept-frequent-phrase
74 options: |
75 - inherit: baseqa.evidence.concept.

frequent-phrase
76
77 - inherit: ecd.phase
78 name: concept-search-uts
79 options: |
80 - inherit: bioqa.evidence.concept.

search-uts-cached
81
82 - inherit: ecd.phase
83 name: concept-merge
84 options: |
85 - inherit: baseqa.evidence.concept.merge
86
87 - inherit: ecd.phase
88 name: answer-type
89 options: |
90 - inherit: bioqa.answer_type.

predict-liblinear-null
91
92 - inherit: ecd.phase
93 name: answer-generate
94 options: |
95 - inherit: bioqa.answer.generate.generate
96
97 - inherit: ecd.phase
98 name: answer-modify
99 options: |

100 - inherit: baseqa.answer.modify.modify
101
102 - inherit: ecd.phase
103 name: answer-score
104 options: |
105 - inherit: bioqa.answer.score.

predict-liblinear
106
107 - inherit: ecd.phase
108 name: answer-collective-score
109 options: |
110 - inherit: bioqa.answer.collective_score.

predict-liblinear
111 - inherit: base.noop
112
113 - inherit: ecd.phase
114 name: answer-prune
115 options: |
116 - inherit: baseqa.answer.modify.pruner
117
118 # - inherit: baseqa.cas-serialize
119
120 post-process:
121 # submission
122 - inherit: bioasq.collection.json.

json-cas-consumer

Listing 2: ECD main descriptor for yes/no QA in
batch 5 in Phase B

1 # execute
2 # mvn exec:exec -Dconfig=bioasq.

phase-b-test-yesno
3 # to test the pipeline
4
5 configuration:
6 name: phase-b-test-yesno
7 author: ziy
8
9 persistence-provider:

10 inherit: baseqa.persistence.
local-sqlite-persistence-provider

11
12 collection-reader:
13 inherit: baseqa.collection.json.

json-collection-reader
14 dataset: BIOASQ-QA
15 file:
16 - input/4b-5-b.json
17 type: [yesno]
18 persistence-provider: |

33



19 inherit: baseqa.persistence.
local-sqlite-persistence-provider

20
21 pipeline:
22 - inherit: ecd.phase
23 name: question-parse
24 options: |
25 - inherit: bioqa.question.parse.

clearnlp-bioinformatics
26
27 - inherit: ecd.phase
28 name: question-concept-metamap
29 options: |
30 - inherit: bioqa.question.concept.

metamap-cached
31
32 - inherit: ecd.phase
33 name: question-concept-tmtool
34 options: |
35 - inherit: bioqa.question.concept.

tmtool-cached
36
37 - inherit: ecd.phase
38 name: question-concept-lingpipe-genia
39 options: |
40 - inherit: bioqa.question.concept.

lingpipe-genia
41
42 - inherit: ecd.phase
43 name: passage-to-view
44 options: |
45 - inherit: baseqa.evidence.passage-to-view
46
47 - inherit: ecd.phase
48 name: evidence-parse
49 options: |
50 - inherit: bioqa.evidence.parse.

clearnlp-bioinformatics
51
52 - inherit: ecd.phase
53 name: evidence-concept-metamap
54 options: |
55 - inherit: bioqa.evidence.concept.

metamap-cached
56
57 - inherit: ecd.phase
58 name: evidence-concept-tmtool
59 options: |
60 - inherit: bioqa.evidence.concept.

tmtool-cached
61
62 - inherit: ecd.phase
63 name: evidence-concept-lingpipe-genia
64 options: |
65 - inherit: bioqa.evidence.concept.

lingpipe-genia
66
67 - inherit: ecd.phase
68 name: evidence-concept-frequent-phrase
69 options: |
70 - inherit: baseqa.evidence.concept.

frequent-phrase
71
72 - inherit: ecd.phase
73 name: concept-search-uts
74 options: |
75 - inherit: bioqa.evidence.concept.

search-uts-cached
76
77 - inherit: ecd.phase
78 name: concept-merge
79 options: |
80 - inherit: baseqa.evidence.concept.merge
81
82 - inherit: ecd.phase
83 name: answer-yesno
84 options: |
85 - inherit: bioqa.answer.yesno.

predict-weka-other
86 - inherit: baseqa.answer.yesno.all-yes
87
88 post-process:
89 # submission
90 - inherit: bioasq.collection.json.

json-cas-consumer

Listing 3: ECD main descriptor for retrieval in
batch 5 in Phase A

1 # execute
2 # mvn exec:exec -Dconfig=bioasq.phase-a-test
3 # to test the pipeline
4
5 configuration:
6 name: phase-a-test
7 author: ziy
8
9 persistence-provider:

10 inherit: baseqa.persistence.
local-sqlite-persistence-provider

11
12 collection-reader:
13 inherit: baseqa.collection.json.

json-collection-reader
14 dataset: BIOASQ-QA
15 file:
16 - input/4b-5-a.json
17 persistence-provider: |
18 inherit: baseqa.persistence.

local-sqlite-persistence-provider
19
20 pipeline:
21 - inherit: ecd.phase
22 name: question-parse
23 options: |
24 - inherit: bioqa.question.parse.

clearnlp-bioinformatics
25
26 - inherit: ecd.phase
27 name: question-concept-metamap
28 options: |
29 - inherit: bioqa.question.concept.

metamap-cached
30
31 - inherit: ecd.phase
32 name: question-concept-tmtool
33 options: |
34 - inherit: bioqa.question.concept.

tmtool-cached
35
36 - inherit: ecd.phase
37 name: question-concept-lingpipe-genia
38 options: |
39 - inherit: bioqa.question.concept.

lingpipe-genia
40
41 - inherit: ecd.phase
42 name: concept-search-uts
43 options: |
44 - inherit: bioqa.evidence.concept.

search-uts-cached
45
46 - inherit: ecd.phase
47 name: concept-merge
48 options: |
49 - inherit: baseqa.evidence.concept.merge
50
51 - inherit: ecd.phase
52 name: abstract-query-primary
53 options: |
54 - inherit: baseqa.abstract_query.token-concept
55
56 # concept
57 - inherit: ecd.phase
58 name: concept-retrieval
59 options: |
60 - inherit: bioqa.concept.retrieval.

lucene-bioconcept
61
62 - inherit: ecd.phase
63 name: concept-rerank
64 options: |
65 - inherit: bioqa.concept.rerank.

predict-liblinear
66
67 # document
68 - inherit: ecd.phase
69 name: document-retrieval
70 options: |
71 - inherit: bioqa.document.retrieval.

lucene-medline
72
73 - inherit: ecd.phase

34



74 name: document-rerank
75 options: |
76 - inherit: bioqa.document.rerank.

predict-liblinear
77
78 # snippet
79 - inherit: ecd.phase
80 name: passage-retrieval
81 options: |
82 - inherit: bioasq.passage.retrieval.

document-to-passage
83
84 - inherit: ecd.phase
85 name: passage-rerank
86 options: |
87 - inherit: bioqa.passage.rerank.

predict-liblinear
88 - inherit: base.noop
89
90 post-process:
91 # submission
92 - inherit: bioasq.collection.json.

json-cas-consumer

Listing 4: ECD component descriptor of
bioqa.answer.collective score.liblinear-predict

1 inherit: baseqa.learning_base.classifier-predict
2
3 candidate-provider: ’inherit: baseqa.answer.score.

candidate-provider’
4 scorers: |
5 - inherit: baseqa.answer.collective_score.scorers.

original
6 - inherit: baseqa.answer.collective_score.scorers.

distance
7 - inherit: baseqa.answer.collective_score.scorers.

edit-distance
8 - inherit: baseqa.answer.collective_score.scorers.

type-coercion
9 - inherit: baseqa.answer.collective_score.scorers.

shape-distance
10 classifier: ’inherit: bioqa.answer.collective_score.

liblinear-classifier’
11 feature-file: result/

answer-collective-score-predict-liblinear.tsv

Listing 5: ECD component descriptor of
bioqa.answer.yesno.predict

1 inherit: baseqa.answer.yesno.predict
2
3 scorers: |
4 - inherit: baseqa.answer.yesno.scorers.

concept-overlap
5 - inherit: bioqa.answer.yesno.scorers.

token-overlap
6 - inherit: baseqa.answer.yesno.scorers.

expected-answer-overlap
7 - inherit: baseqa.answer.yesno.scorers.sentiment
8 - inherit: baseqa.answer.yesno.scorers.negation
9 - inherit: bioqa.answer.yesno.scorers.

question-inversion

35



Input question

Question parsing

Question concept
identification

Abstract query
generation

Candidate
document retrieval

Document
reranking

Snippet extraction

Snippet reranking

Candidate
concept retrieval

Concept reranking

Relevant concepts

Relevant documents Relevant snippets

NLP providers
• ClearNLP (bioinformatics model)

Concept identification providers
• MetaMap†
• LingPipe (GE-

NIA)

• UTS†
• TmTool†

Retrieval providers
• Lucene local index and search

Search result reranking scorers
• Original retrieval score
• Overalapping concept count
• Various query formulation

strategies
• Meta information

Classifier providers
• LibLinear logistic regression

Figure 1: Retrieval (Phase A) pipeline diagram. † represents a provider that requires accessing external
Web services.

Input question

Question parsing

Question concept
identification

Focus extraction

Answer type
prediction

Relevant snippets

Snippet parsing

Snippet concept
identification

Concept retrieval

Concept merging

Candidate answer
generation

Candidate
answer merging

Answer scoring
and ranking

Collective
answer reranking Exact answer

NLP providers
• ClearNLP (bioinformatics

model)

Concept identification
providers

• MetaMap†
• LingPipe

(GENIA)

• TmTool†
• C-Value

Concept retrieval providers
• UMLS terminology service†

Classifier providers
• LibLinear logistic regression

Answer scorers
• Type coercion
• CAO count
• Name count
• Avg length
• Stopword count
• Token overlap

count
• Concept overlap

count

• Token proximity
• Concept proximity
• Focus overlap

count
• Parse proximity
• Answer type
• Focus
• Concept type

Candidate answer variant generators
• Choice question
• Quantity question
• Concept
• CAV covering concept

Collective answer reranking scorers
• Original score
• Proximity distance
• Edit distance
• Type coercion
• Shape edit distance

Figure 2: Factoid and list question answering (Phase B) pipeline diagram. † represents a provider that
requires accessing external Web services.

36



Input question

Question parsing

Question concept
identification

Relevant snippets

Snippet parsing

Snippet concept
identification

Concept retrieval

Concept merging

Yes/no answer
prediction

Exact answer

NLP providers
• ClearNLP (bioinformatics

model)

Concept identification
providers

• MetaMap†
• LingPipe

(GENIA)

• TmTool†
• C-Value

Concept retrieval providers
• UMLS terminology service†

Classifier providers
• LibLinear logistic regression
• Weka SimpleLogistic
• Weka ClassificationViaClus-

tering

Yes/no answer scorers
• Concept overlap count
• Token overlap count
• Expected overlap count
• Sentiment
• Negation
• Question inversion

Question inversion

Candidate answer
generation

Candidate
answer merging

Answer scoring
and ranking

Candidate answer variant generators
• Choice question
• Quantity question
• Concept
• CAV covering concept

Answer scorers
• Type coercion
• CAO count
• Name count
• Avg length
• Stopword count
• Token overlap

count
• Concept overlap

count

• Token proximity
• Concept proximity
• Focus overlap

count
• Parse proximity
• Answer type
• Focus
• Concept type

Figure 3: Yes/no question answering (Phase B) pipeline diagram. † represents a provider that requires
accessing external Web services.

37


