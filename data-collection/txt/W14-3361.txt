



















































Bayesian Reordering Model with Feature Selection


Proceedings of the Ninth Workshop on Statistical Machine Translation, pages 477–485,
Baltimore, Maryland USA, June 26–27, 2014. c©2014 Association for Computational Linguistics

Bayesian Reordering Model with Feature Selection

Abdullah Alrajehab and Mahesan Niranjanb
aComputer Research Institute, King Abdulaziz City for Science and Technology (KACST)

Riyadh, Saudi Arabia, asrajeh@kacst.edu.sa
bSchool of Electronics and Computer Science, University of Southampton

Southampton, United Kingdom, {asar1a10, mn}@ecs.soton.ac.uk

Abstract

In phrase-based statistical machine trans-
lation systems, variation in grammatical
structures between source and target lan-
guages can cause large movements of
phrases. Modeling such movements is cru-
cial in achieving translations of long sen-
tences that appear natural in the target lan-
guage. We explore generative learning
approach to phrase reordering in Arabic
to English. Formulating the reordering
problem as a classification problem and
using naive Bayes with feature selection,
we achieve an improvement in the BLEU
score over a lexicalized reordering model.
The proposed model is compact, fast and
scalable to a large corpus.

1 Introduction

Currently, the dominant approach to machine
translation is statistical, starting from the math-
ematical formulations and algorithms for param-
eter estimation (Brown et al., 1988), further ex-
tended in (Brown et al., 1993). These early mod-
els, widely known as the IBM models, were word-
based. Recent extensions note that a better ap-
proach is to group collections of words, or phrases,
for translation together, resulting in a significant
focus these days on phrase-based statistical ma-
chine translation systems.

To deal with the alignment problem of one-
to-many word alignments in the IBM model
formulation, whereas phrase-based models may
have many-to-many translation relationships, IBM
models are trained in both directions, source to tar-
get and target to source, and their word alignments
are combined (Och and Ney, 2004).

While phrase-based systems are a significant
improvement over word-based approaches, a par-
ticular issue that emerges is long-range reorder-
ings at the phrase level (Galley and Manning,

2008). Analogous to speech recognition systems,
translation systems relied on language models to
produce more fluent translation. While early work
penalized phrase movements without considering
reorderings arising from vastly differing grammat-
ical structures across language pairs like Arabic-
English, many researchers considered lexical re-
ordering models that attempted to learn orienta-
tion based on content (Tillmann, 2004; Kumar
and Byrne, 2005; Koehn et al., 2005). These
approaches may suffer from the data sparseness
problem since many phrase pairs occur only once
(Nguyen et al., 2009).

As an alternative way of exploiting function ap-
proximation capabilities offered by machine learn-
ing methods, there is recent interest in formulating
a learning problem that aims to predict reorder-
ing from linguistic features that capture their con-
text. An example of this is the maximum entropy
method used by (Xiang et al., 2011; Nguyen et al.,
2009; Zens and Ney, 2006; Xiong et al., 2006).

In this work we apply a naive Bayes classifier,
combined with feature selection to address the re-
ordering problem. To the best of our knowledge,
this simple model of classification has not been
used in this context previously. We present em-
pirical results comparing our work and previously
proposed lexicalized reordering model. We show
that our model is scalable to large corpora.

The remainder of this paper is organized as fol-
lows. Section 2 discusses previous work in the
field and how that is related to our paper. Section 3
gives an overview of the baseline translation sys-
tem. Section 4 introduces the Bayesian reorder-
ing model and gives details of different inference
methods, while, Section 5 describes feature selec-
tion method. Section 6 presents the experiments
and reports the results evaluated as classification
and translation problems. Finally, we end the pa-
per with a summary of our conclusions and per-
spectives.

477



Symbol Notation
f/e a source / target sentence (string)
f̄/ē a source / target phrase sequence
N the number of examples
K the number of classes
(f̄n, ēn) the n-th phrase pair in (̄f , ē)
on the orientation of (f̄n, ēn)
φ(f̄n, ēn) the feature vector of (f̄n, ēn)

Table 1: Notation used in this paper.

2 Related Work

The phrase reordering model is a crucial compo-
nent of any translation system, particularly be-
tween language pairs with different grammatical
structures (e.g. Arabic-English). Adding a lex-
icalized reordering model consistently improved
the translation quality for several language pairs
(Koehn et al., 2005). The model tries to predict the
orientation of a phrase pair with respect to the pre-
vious adjacent target words. Ideally, the reorder-
ing model would predict the right position in the
target sentence given a source phrase, which is dif-
ficult to achieve. Therefore, positions are grouped
into limited orientations or classes.The orientation
probability for a phrase pair is simply based on the
relative occurrences in the training corpus.

The lexicalized reordering model has been ex-
tended to tackle long-distance reorderings (Gal-
ley and Manning, 2008). This takes into account
the hierarchical structure of the sentence when
considering such an orientation. Certain exam-
ples are often used to motivate syntax-based sys-
tems were handled by this hierarchical model, and
this approach is shown to improve translation per-
formance for several translation tasks with small
computational cost.

Despite the fact that the lexicalized reordering
model is always biased towards the most frequent
orientation for such a phrase pair, it may suffer
from a data sparseness problem since many phrase
pairs occur only once. Moreover, the context of
a phrase might affect its orientation, which is not
considered as well.

Adopting the idea of predicting orientation
based on content, it has been proposed to represent
each phrase pair by linguistic features as reorder-
ing evidence, and then train a classifier for predic-
tion. The maximum entropy classifier is a popu-
lar choice among many researchers (Zens and Ney,
2006; Xiong et al., 2006; Nguyen et al., 2009; Xi-

ang et al., 2011). Max-margin structure classifiers
were also proposed (Ni et al., 2011). Recently,
Cherry (2013) proposed using sparse features op-
timize BLEU with the decoder instead of training
a classifier independently.

We distinguish our work from the previous ones
in the following. We propose a fast reordering
model using a naive Bayes classifier with feature
selection. In this study, we undertake a compari-
son between our work and lexicalized reordering
model.

3 Baseline System

In statistical machine translation, the most likely
translation ebest of an input sentence f can be
found by maximizing the probability p(e|f), as
follows:

ebest = arg max
e
p(e|f). (1)

A log-linear combination of different models
(features) is used for direct modeling of the poste-
rior probability p(e|f) (Papineni et al., 1998; Och
and Ney, 2002):

ebest = arg max
e

n∑
i=1

λihi(f , e) (2)

where the feature hi(f , e) is a score function
over sentence pairs. The translation model and the
language model are the main features in any sys-
tem although additional features h(.) can be inte-
grated easily (such as word penalty). State-of-the-
art systems usually have around ten features (i.e.
n = 10).

In phrase-based systems, the translation model
can capture the local meaning for each source
phrase. However, to capture the whole meaning
of a sentence, its translated phrases need to be in
the correct order. The language model, which en-
sures fluent translation, plays an important role in
reordering; however, it prefers sentences that are
grammatically correct without considering their
actual meaning. Besides that, it has a bias towards
short translations (Koehn, 2010). Therefore, de-
veloping a reordering model will improve the ac-
curacy particularly when translating between two
grammatically different languages.

3.1 Lexicalized Reordering Model

Phrase reordering modeling involves formulat-
ing phrase movements as a classification problem

478



where each phrase position considered as a class
(Tillmann, 2004). Some researchers classified
phrase movements into three categories (mono-
tone, swap, and discontinuous) but the classes can
be extended to any arbitrary number (Koehn and
Monz, 2005). In general, the distribution of phrase
orientation is:

p(ok|f̄n, ēn) = 1
Z
h(f̄n, ēn, ok) . (3)

This lexicalized reordering model is estimated
by relative frequency where each phrase pair
(f̄n, ēn) with such an orientation (ok) is counted
and then normalized to yield the probability as fol-
lows:

p(ok|f̄n, ēn) = count(f̄n, ēn, ok)∑
o count(f̄n, ēn, o)

. (4)

The orientation class of a current phrase pair is
defined with respect to the previous target word
or phrase (i.e. word-based classes or phrase-based
classes). In the case of three categories (mono-
tone, swap, and discontinuous): monotone is the
previous source phrase (or word) that is previ-
ously adjacent to the current source phrase, swap
is the previous source phrase (or word) that is next-
adjacent to the current source phrase, and discon-
tinuous is not monotone or swap.

Galley and Manning (2008) extended the lex-
icalized reordering mode to tackle long-distance
phrase reorderings. Their hierarchical model en-
ables phrase movements that are more complex
than swaps between adjacent phrases.

4 Bayesian Reordering Model

Many feature-based reordering models have been
proposed to replace the lexicalized reordering
model. The reported results showed consistent im-
provement in terms of various translation metrics.

Naive Bayes method has been a popular clas-
sification model of choice in many natural lan-
guage processing problems (e.g. text classifica-
tion). Naive Bayes is a simple classifier that ig-
nores correlation between features, but has the ap-
peal of computational simplicity. It is a generative
probabilistic model based on Bayes’ theorem as
below:

p(ok|f̄n, ēn) = p(f̄n, ēn|ok)p(ok)∑
o p(f̄n, ēn|o)p(o)

. (5)

The class prior can be estimated easily as a rel-
ative frequency (i.e. p(ok) = NkN ). The likeli-
hood distribution p(f̄n, ēn|ok) is defined based on

the type of data. The classifier will be naive if we
assume that feature variables are conditionally in-
dependent. The naive assumption simplifies our
distribution and hence reduces the parameters that
have to be estimated. In text processing, multi-
nomial is used as a class-conditional distribution
(Rogers and Girolami, 2011). The distribution is
defined as:

p(f̄n, ēn|q) = C
∏
m

qφm(f̄n,ēn)m (6)

where C is a multinomial coefficient,

C =
(
∑

m φm(f̄n, ēn))!∏
m φm(f̄n, ēn)!

, (7)

and q are a set of parameters, each of which is a
probability. Estimating these parameters for each
class by maximum likelihood,

arg max
qk

Nk∏
n

p(f̄n, ēn|qk), (8)

will result in (Rogers and Girolami, 2011):

qkm =
∑Nk

n φm(f̄n, ēn)∑M
m′
∑Nk

n φm′(f̄n, ēn)
. (9)

MAP estimate It is clear that qkm might be
zero which means the probability of a new phrase
pair with nonzero feature φm(f̄n, ēn) is always
zero because of the product in (6). Putting a prior
over q is one smoothing technique. A conjugate
prior for the multinomial likelihood is the Dirich-
let distribution and the MAP estimate for qkm is
(Rogers and Girolami, 2011):

qkm =
α− 1 +∑Nkn φm(f̄n, ēn)

M(α− 1) +∑Mm′∑Nkn φm′(f̄n, ēn)
(10)

where M is the feature vector’s length or the
feature dictionary size and α is a Dirichlet param-
eter with a value greater than one. The derivation
is in Appendix A.

Bayesian inference Instead of using a point es-
timate of q as shown previously in equation (10),
Bayesian inference is based on the whole param-
eter space in order to incorporate uncertainty into
our multinomial model. This requires a posterior

479



probability distribution over q as follows:

p(f̄n, ēn|ok) =
∫
p(f̄n, ēn|qk)p(qk|αk) dqk

=C
Γ (
∑

m αkm)∏
m Γ(αkm)

∏
m Γ(αkm + φm(f̄n, ēn))

Γ
(∑

m αkm + φm(f̄n, ēn)
) .

(11)

Here αk are new hyperparameters of the pos-
terior derived by means of Bayes theorem as fol-
lows:

p(qk|αk) = p(qk|α)
∏Nk
n p(f̄n, ēn|qk)∫

p(qk|α)
∏Nk
n p(f̄n, ēn|qk)dqk

.

(12)
The solution of (11) will result in:

αk = α +
Nk∑
n

Φ(f̄n, ēn). (13)

For completeness we give a summary of deriva-
tions of equations (11) and (13) in Appendix B,
more detailed discussions can be found in (Barber,
2012).

5 Feature Selection

In several high dimensional pattern classification
problems, there is increasing evidence that the
discriminant information may be in small sub-
spaces, motivating feature selection (Li and Niran-
jan, 2013). Having irrelevant or redundant fea-
tures could affect the classification performance
(Liu and Motoda, 1998). They might mislead the
learning algorithms or overfit them to the data and
thus have less accuracy.

The aim of feature selection is to find the op-
timal subset features which maximize the ability
of prediction, which is the main concern, or sim-
plify the learned results to be more understand-
able. There are many ways to measure the good-
ness of a feature or a subset of features; however
the criterion will be discussed is mutual informa-
tion.

5.1 Mutual Information
Information criteria are based on the concept of
entropy which is the amount of randomness. The
distribution of a fair coin, for example, is com-
pletely random so the entropy of the coin is very
high. The following equation calculates the en-
tropy of a variable X (MacKay, 2002):

H (X) = −
∑
x

p(x) log p(x). (14)

The mutual information of a feature X can be mea-
sured by calculating the difference between the
prior uncertainty of the class variable Y and the
posterior uncertainty after using the feature as fol-
lows (MacKay, 2002):

I(X;Y ) = H(Y )−H(Y |X) (15)
=
∑
x,y

p(x, y) log
p(x, y)
p(x)p(y)

.

The advantage of mutual Information over other
criteria is the ability to detect nonlinear patterns.
The disadvantage is its bias towards higher ar-
bitrary features; however this problem can be
solved by normalizing the information as follows
(Estévez et al., 2009):

Inorm(X;Y ) =
I(X;Y )

min(H(X), H(Y ))
. (16)

6 Experiments

The corpus used in our experiments is MultiUN
which is a large-scale parallel corpus extracted
from the United Nations website1 (Eisele and
Chen, 2010). We have used Arabic and English
portion of MultiUN. Table 2 shows the general
statistics.

Statistics Arabic English
Sentence Pairs 9.7 M
Running Words 255.5 M 285.7 M

Word/Line 22 25
Vocabulary Size 677 K 410 K

Table 2: General statistics of Arabic-English Mul-
tiUN (M: million, K: thousand).

We simplify the problem by classifying phrase
movements into three categories (monotone,
swap, discontinuous). To train the reordering
models, we used GIZA++ to produce word align-
ments (Och and Ney, 2000). Then, we used the
extract tool that comes with the Moses 2 toolkit
(Koehn et al., 2007) in order to extract phrase pairs
along with their orientation classes.

Each extracted phrase pair is represented by lin-
guistic features as follows:

• Aligned source and target words in a phrase
pair. Each word alignment is a feature.

1http://www.ods.un.org/ods/
2Moses is an open source toolkit for statistical machine

translation (www.statmt.org/moses/).

480



• Words within a window around the source
phrase to capture the context. We choose ad-
jacent words of the phrase boundary.

Most researchers build one reordering model
for the whole training set (Zens and Ney, 2006;
Xiong et al., 2006; Nguyen et al., 2009; Xiang
et al., 2011). Ni et al. (Ni et al., 2011) simpli-
fied the learning problem to have as many sub-
models as source phrases. Training data were di-
vided into small independent sets where samples
having the same source phrase are considered a
training set. In our experiments, we have chosen
the first method.

We compare lexicalized and Bayesian reorder-
ing models in two phases. In the classification
phase, we see the performance of the models as
a classification problem. In the translation phase,
we test the actual impact of these reordering mod-
els in a translation system.

6.1 Classification
We built naive Bayes classifier with both MAP es-
timate and Bayesian inference. We also used mu-
tual Information in order to select the most infor-
mative features for our classification task.

Table 3 reports the error rate of the reorder-
ing models compared to the lexicalized reorder-
ing model. All experiments reported here were
repeated three times to evaluate the uncertainties
in our results. The results shows that there is no
advantage to using Bayesian inference instead of
MAP estimate.

Classifier Error Rate
Lexicalized model 25.2%
Bayes-MAP estimate 19.53%
Bayes-Bayesian inference 20.13%

Table 3: Classification error rate of both lexical-
ized and Bayesian models.

The feature selection process reveals that many
features have low mutual information. Hence they
are not related to the classification task and can be
excluded from the model. Figure 1 shows the nor-
malized mutual information for all extracted fea-
tures.

A ranking threshold for selecting features based
on their mutual information is specified experi-
mentally. In Figure 2, we tried different thresh-
olds ranging from 0.001 to 0.05 and measure the
error rate after each reduction. Although there

is no much gain in terms of performance but the
Bayesian model maintains low error rate when the
proportion of selected features is low. The model
with almost half of the feature space is as good as
the one with full feature space.

Figure 1: Normalized mutual information for all
extracted features (ranked from lowest to highest).

0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1
19

19.5

20

20.5

21

21.5

22

22.5

Percentage of Feature Reductuion

E
rr

o
r 

R
a

te

Figure 2: Classification error rate of the Baysien
model with different levels of feature reduction.

6.2 Translation

6.2.1 Experimental Design
We used the Moses toolkit (Koehn et al., 2007)
with its default settings. The language model
is a 5-gram with interpolation and Kneser-Ney
smoothing (Kneser and Ney, 1995). We tuned the
system by using MERT technique (Och, 2003).

We built four Arabic-English translation sys-
tems. Three systems differ in how their reordering
models were estimated and the fourth system is a

481



baseline system without reordering model. In all
cases, orientation extraction is hierarchical-based
since it is the best approach while orientations are
monotone, swap and discontinuous. The model is
trained in Moses by specifying the configuration
string hier-msd-backward-fe.

As commonly used in statistical machine trans-
lation, we evaluated the translation performance
by BLEU score (Papineni et al., 2002). The test
sets are NIST MT06 and NIST MT08. Table 4
shows statistics of development and test sets. We
also computed statistical significance for the pro-
posed models using the paired bootstrap resam-
pling method (Koehn, 2004).

Evaluation Set Arabic English
Development sentences 696 696

words 19 K 21 K
NIST MT06 sentences 1797 7188

words 49 K 223 K
NIST MT08 sentences 813 3252

words 25 K 117 K

Table 4: Statistics of development and test sets.
The English side in NIST is larger because there
are four translations for each Arabic sentence.

6.2.2 Results
We first demonstrate in Table 5 a general com-
parison of the proposed model and the lexicalized
model in terms of disc size and average speed in a
translation system. The size of Bayesian model is
far smaller. The lexicalized model is slightly faster
than the Bayesian model because we have over-
head computational cost to extract features and
compute the orientation probabilities. However,
the disc size of our model is much smaller which
makes it more efficient practically for large-scale
tasks.

Model Size (MB) Speed (s/sent)
Lexicalized model 604 2.2
Bayesian model 18 2.6

Table 5: Disc size and average speed of the re-
ordering models in a translation system.

Table 6 shows the BLEU scores for the transla-
tion systems according to two test sets. The base-
line system has no reordering model. In the two
test sets, our Bayesian reordering model is better
than the lexicalized one with at least 95% statis-

tical significance. As we have seen in the clas-
sification section, Bayes classifier with Bayesian
inference has no advantage over MAP estimate.

Translation System MT06 MT08
Baseline 28.92 32.13
BL+ Lexicalized model 30.86 34.22
BL+ Bayes-MAP estimate 31.21* 34.72*
BL+ Bayes-Baysien inference 31.20 34.69

Table 6: BLEU scores for Arabic-English trans-
lation systems (*: better than the baseline with at
least 95% statistical significance).

7 Conclusion

In this paper, we have presented generative mod-
eling approach to phrase reordering in machine
translation. We have experimented with trans-
lation from Arabic to English and shown im-
provements over the lexicalized model of estimat-
ing probabilities as relative frequencies of phrase
movements. Our proposed Bayesian model with
feature selection is shown to be superior. The
training time of the model is as fast as the lexical-
ized model. Its storage requirement is many times
smaller which makes it more efficient practically
for large-scale tasks.

The feature selection process reveals that many
features have low mutual information. Hence they
are not related to the classification task and can be
excluded from the model. The model with almost
half of the feature space is as good as the one with
full feature space.

Previously proposed discriminative models
might achieve higher score than the reported re-
sults. However, our model is scalable to large-
scale systems since parameter estimation require
only one pass over the data with limited memory
(i.e. no iterative learning). This is a critical advan-
tage over discriminative models.

Our current work focuses on three issues. The
first is improving the translation speed of the pro-
posed model. The lexicalized model is slightly
faster. The second is using more informative fea-
tures. We plan to explore part-of-speech informa-
tion, which is more accurate in capturing content.
Finally, we will explore different feature selection
methods. In our experiments, feature reduction is
based on univariate ranking which is riskier than
multivariate ranking. This is because useless fea-
ture can be useful with others.

482



References
D. Barber. 2012. Bayesian Reasoning and Machine

Learning. Cambridge University Press.

P. Brown, J. Cocke, S. Della Pietra, V. Della Pietra,
F. Jelinek, R. Mercer, and P. Roossin. 1988. A sta-
tistical approach to language translation. In 12th In-
ternational Conference on Computational Linguis-
tics (COLING), pages 71–76.

P. Brown, V. Pietra, S. Pietra, and R. Mercer. 1993.
The mathematics of statistical machine translation:
Parameter estimation. Computational Linguistics,
19(2):263–311.

C. Cherry. 2013. Improved reordering for phrase-
based translation using sparse features. In Proceed-
ings of the 2013 Conference of the North American
Chapter of the Association for Computational Lin-
guistics: Human Language Technologies, pages 22–
31, Atlanta, Georgia, June. Association for Compu-
tational Linguistics.

A. Eisele and Y. Chen. 2010. Multiun: A multilingual
corpus from united nation documents. In Daniel
Tapias, Mike Rosner, Stelios Piperidis, Jan Odjik,
Joseph Mariani, Bente Maegaard, Khalid Choukri,
and Nicoletta Calzolari (Conference Chair), editors,
Proceedings of the Seventh conference on Interna-
tional Language Resources and Evaluation, pages
2868–2872. European Language Resources Associ-
ation (ELRA), 5.

P. Estévez, M. Tesmer, C. Perez, and J. Zurada. 2009.
Normalized mutual information feature selection.
Trans. Neur. Netw., 20(2):189–201, February.

M. Galley and C. Manning. 2008. A simple and
effective hierarchical phrase reordering model. In
Proceedings of the 2008 Conference on Empirical
Methods in Natural Language Processing, pages
848–856, Hawaii, October. Association for Compu-
tational Linguistics.

R. Kneser and H. Ney. 1995. Improved backing-off
for m-gram language modeling. IEEE International
Conference on Acoustics, Speech and Signal Pro-
cessing, pages 181–184.

P. Koehn and C. Monz. 2005. Shared task: Sta-
tistical machine translation between european lan-
guages. In Proceedings of ACL Workshop on Build-
ing and Using Parallel Texts, pages 119–124. Asso-
ciation for Computational Linguistics.

P. Koehn, A. Axelrod, A. Mayne, C. Callison-Burch,
M. Osborne, and D. Talbot. 2005. Edinburgh sys-
tem description for the 2005 IWSLT speech trans-
lation evaluation. In Proceedings of International
Workshop on Spoken Language Translation, Pitts-
burgh, PA.

P. Koehn, H. Hoang, A. Birch, C. Callison-Burch,
M. Federico, N. Bertoldi, B. Cowan, W. Shen,
C. Moran, R. Zens, C. Dyer, O. Bojar, A. Constantin,

and E. Herbst. 2007. Moses: Open source toolkit
for statistical machine translation. In Proceedings
of the ACL 2007 Demo and Poster Sessions, pages
177–180.

P. Koehn. 2004. Statistical significance tests for ma-
chine translation evaluation. In Dekang Lin and
Dekai Wu, editors, Proceedings of EMNLP 2004,
pages 388–395, Barcelona, Spain, July. Association
for Computational Linguistics.

P. Koehn. 2010. Statistical Machine Translation.
Cambridge University Press.

S. Kumar and W. Byrne. 2005. Local phrase reorder-
ing models for statistical machine translation. In
Proceedings of Human Language Technology Con-
ference and Conference on Empirical Methods in
Natural Language Processing, pages 161–168, Van-
couver, British Columbia, Canada, October. Associ-
ation for Computational Linguistics.

Hongyu Li and M. Niranjan. 2013. Discriminant sub-
spaces of some high dimensional pattern classifica-
tion problems. In IEEE International Workshop on
Machine Learning for Signal Processing (MLSP),
pages 27–32.

H. Liu and H. Motoda. 1998. Feature Selection for
Knowledge Discovery and Data Mining. Kluwer
Academic Publishers, Norwell, MA, USA.

D. MacKay. 2002. Information Theory, Inference &
Learning Algorithms. Cambridge University Press,
New York, NY, USA.

V. Nguyen, A. Shimazu, M. Nguyen, and T. Nguyen.
2009. Improving a lexicalized hierarchical reorder-
ing model using maximum entropy. In Proceed-
ings of the Twelfth Machine Translation Summit (MT
Summit XII). International Association for Machine
Translation.

Y. Ni, C. Saunders, S. Szedmak, and M. Niranjan.
2011. Exploitation of machine learning techniques
in modelling phrase movements for machine transla-
tion. Journal of Machine Learning Research, 12:1–
30, February.

F. Och and H. Ney. 2000. Improved statistical align-
ment models. In Proceedings of the 38th Annual
Meeting of the Association of Computational Lin-
guistics (ACL).

F. Och and H. Ney. 2002. Discriminative training
and maximum entropy models for statistical ma-
chine translation. In Proceedings of the 40th Annual
Meeting of the Association for Computational Lin-
guistics (ACL).

F. Och and H. Ney. 2004. The alignment template
approach to statistical machine translation. Compu-
tational Linguistics, 30(4):417–449.

483



F. Och. 2003. Minimum error rate training in statisti-
cal machine translation. In Proceedings of the 41st
Annual Meeting on Association for Computational
Linguistics - Volume 1, ACL ’03, pages 160–167,
Stroudsburg, PA, USA. Association for Computa-
tional Linguistics.

K. Papineni, S. Roukos, and T. Ward. 1998. Max-
imum likelihood and discriminative training of di-
rect translation models. In Proceedings of ICASSP,
pages 189–192.

K. Papineni, S. Roukos, T. Ward, and W. Zhu. 2002.
Bleu: a method for automatic evaluation of machine
translation. In Proceedings of the 40th Annual Meet-
ing on Association for Computational Linguistics,
pages 311–318, Stroudsburg, PA, USA. Association
for Computational Linguistics.

S. Rogers and M. Girolami. 2011. A First Course in
Machine Learning. Chapman & Hall/CRC, 1st edi-
tion.

C. Tillmann. 2004. A unigram orientation model for
statistical machine translation. In Proceedings of
HLT-NAACL: Short Papers, pages 101–104.

B. Xiang, N. Ge, and A. Ittycheriah. 2011. Improving
reordering for statistical machine translation with
smoothed priors and syntactic features. In Proceed-
ings of SSST-5, Fifth Workshop on Syntax, Semantics
and Structure in Statistical Translation, pages 61–
69, Portland, Oregon, USA. Association for Com-
putational Linguistics.

D. Xiong, Q. Liu, and S. Lin. 2006. Maximum en-
tropy based phrase reordering model for statistical
machine translation. In Proceedings of the 21st In-
ternational Conference on Computational Linguis-
tics and 44th Annual Meeting of the ACL, pages
521–528, Sydney, July. Association for Computa-
tional Linguistics.

R. Zens and H. Ney. 2006. Discriminative reorder-
ing models for statistical machine translation. In
Proceedings on the Workshop on Statistical Machine
Translation, pages 55–63, New York City, June. As-
sociation for Computational Linguistics.

A MAP Estimate Derivation

Multinomial distribution is defined as:

p(x|q) = C
∏
m

qxmm (17)

where C is a multinomial coefficient,

C =
(
∑

m xm)!∏
m xm!

, (18)

and qm is an event probability (
∑

m qm = 1).

A maximum a posteriori probability (MAP) es-
timate requires a prior over q. Dirichlet distribu-
tion is a conjugate prior and is defined as:

p(q|α) = Γ (
∑

m αm)∏
m Γ(αm)

∏
m

qαm−1m (19)

where αm is is a parameter with a positive value.
Finding the MAP estimate for q given a data is

as follows:

q∗ = arg max
q

p(q|α,X)

= arg max
q

{p(q|α)p(X|q)}

= arg max
q

{
p(q|α)

∏
n

p(xn|q)
}

= arg max
q

{∏
m

qαm−1m
∏
n,m

qxnmm

}

= arg max
q

{∑
m

log qαm−1m +
∑
n,m

log qxnmm

}
.

(20)

Since our function is subject to constraints
(
∑

m qm = 1), we introduce Lagrange multiplier
as follows:

f(q) =
∑
m

log qαm−1m +
∑
n,m

log qxnmm −λ(
∑
m

qm−1).
(21)

Now we can find q∗ by taking the partial deriva-
tive with respect to one variable qm:

∂f(q)
∂qm

=
αm − 1 +

∑
n xnm

qm
− λ

qm =
αm − 1 +

∑
n xnm

λ
. (22)

Finally, we sum both sides over M to find λ :

λ
∑
m

qm =
∑
m

(
αm − 1 +

∑
n

xnm

)
λ =

∑
m

(αm − 1) +
∑
n,m

xnm. (23)

The solution can be simplified by choosing the
same value for each αm which will result in:

qm =
α− 1 +∑n xnm

M(α− 1) +∑n,m′ xnm′ . (24)
484



B Bayesian Inference Derivation

In Appendix A, the inference is based on a single
point estimate of q that has the highest posterior
probability. However, it can be based on the whole
parameter space to incorporate uncertainty. The
probability of a new data point marginalized over
the posterior as follows:

p(x|α,X) =
∫
p(x|q)p(q|α,X) dq, (25)

p(q|α,X) = p(q|α)p(X|q)∫
p(q|α)p(X|q)dq . (26)

Since Dirichlet and Multinomial distributions
are conjugate pairs, they form the same density as
the prior. Therefore the posterior is also Dirichlet.
Now we can expand the posterior expression and
re-arrange it to look like a Dirichlet as follows:

p(q|α,X) ∝ p(q|α)
∏
n

p(xn|q)

∝
∏
m

qαm−1m
∏
n

∏
m

qxnmm

∝
∏
m

q
(αm+

∑
n xnm)−1

m . (27)

The new hyperparameters of the posterior is:

α∗m = αm +
∑
n

xnm. (28)

Finally, we expand and re-arrange Dirichlet and
multinomial distributions inside the integral in
(25) as follows:

p(x|α,X) =∫
C
∏
m

qxmm
Γ (
∑

m α
∗
m)∏

m Γ(α∗m)

∏
m

qα
∗
m−1

m dq

=C
Γ (
∑

m α
∗
m)∏

m Γ(α∗m)

∫ ∏
m

qα
∗
m+xm−1

m dq. (29)

Note that inside the integral looks a Dirichlet
without a normalizing constant. If we multiply
and divide by its normalizing constant (i.e. Beta
function), the integral is going to be one because
it is a density function, resulting in:

p(x|α,X) = C Γ (
∑

m α
∗
m)∏

m Γ(α∗m)

B(α∗ + x)
∫

1
B(α∗ + x)

∏
m

qα
∗
m+xm−1

m dqc

=C
Γ (
∑

m α
∗
m)∏

m Γ(α∗m)
B(α∗ + x)

=C
Γ (
∑

m α
∗
m)∏

m Γ(α∗m)

∏
m Γ(α

∗
m + xm)

Γ (
∑

m (α∗m + xm))
. (30)

485


