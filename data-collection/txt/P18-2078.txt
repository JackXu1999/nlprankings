



















































Simplified Abugidas


Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 491–495
Melbourne, Australia, July 15 - 20, 2018. c©2018 Association for Computational Linguistics

491

Simplified Abugidas

Chenchen Ding, Masao Utiyama, and Eiichiro Sumita
Advanced Translation Technology Laboratory,

Advanced Speech Translation Research and Development Promotion Center,
National Institute of Information and Communications Technology

3-5 Hikaridai, Seika-cho, Soraku-gun, Kyoto, 619-0289, Japan
{chenchen.ding, mutiyama, eiichiro.sumita}@nict.go.jp

Abstract

An abugida is a writing system where the
consonant letters represent syllables with
a default vowel and other vowels are de-
noted by diacritics. We investigate the fea-
sibility of recovering the original text writ-
ten in an abugida after omitting subordi-
nate diacritics and merging consonant let-
ters with similar phonetic values. This is
crucial for developing more efficient in-
put methods by reducing the complexity
in abugidas. Four abugidas in the south-
ern Brahmic family, i.e., Thai, Burmese,
Khmer, and Lao, were studied using a
newswire 20, 000-sentence dataset. We
compared the recovery performance of a
support vector machine and an LSTM-
based recurrent neural network, finding
that the abugida graphemes could be re-
covered with 94% – 97% accuracy at the
top-1 level and 98% – 99% at the top-4
level, even after omitting most diacritics
(10 – 30 types) and merging the remain-
ing 30 – 50 characters into 21 graphemes.

1 Introduction

Writing systems are used to record utterances in
a wide range of languages and can be organized
into the hierarchy shown in Fig. 1. The sym-
bols in a writing system generally represent either
speech sounds (phonograms) or semantic units
(logograms). Phonograms can be either segmen-
tal or syllabic, with segmental systems being more
phonetic because they use separate symbols (i.e.,
letters) to represent consonants and vowels. Seg-
mental systems can be further subdivided depend-
ing on their representation of vowels. Alphabets
(e.g., the Latin, Cyrillic, and Greek scripts) are the
most common and treat vowel and consonant let-

writing
system

phonogram

logogram

segmental

syllabic abugida
alphabet
abjad

Figure 1: Hierarchy of writing systems.

ណន នន

ណូន
/noon/

ណណន
/naen/

នួន
/nuən/

ននន
/nein/

vowel
diacritic
omission

consonant character
merging

N N

(a) ABUGIDA SIMPLIFICATION

…ជិតណណន…

… J T N N …

(b) RECOVERY

machine
learning
methods

Figure 2: Overview of the approach in this study.

ters equally. In contrast, abjads (e.g., the Arabic
and Hebrew scripts) do not write most vowels ex-
plicitly. The third type, abugidas, also called al-
phasyllabary, includes features from both segmen-
tal and syllabic systems. In abugidas, consonant
letters represent syllables with a default vowel,
and other vowels are denoted by diacritics. Abugi-
das thus denote vowels less explicitly than alpha-
bets but more explicitly than abjads, while being
less phonetic than alphabets, but more phonetic
than syllabaries. Since abugidas combine segmen-
tal and syllabic systems, they typically have more
symbols than conventional alphabets.

In this study, we investigate how to simplify and
recover abugidas, with the aim of developing a
more efficient method of encoding abugidas for in-
put. Alphabets generally do not have a large set of
symbols, making them easy to map to a traditional
keyboard, and logogram and syllabic systems need
specially designed input methods because of their
large variety of symbols. Traditional input meth-
ods for abugidas are similar to those for alphabets,
mapping two or three different symbols onto each
key and requiring users to type each character and
diacritic exactly. In contrast, we are able to sub-
stantially simplify inputting abugidas by encoding
them in a lossy (or “fuzzy”) way.



492

TH
ะ ั  ั   ั  ั  ั  ั  ั  ั 

ั  ั  ั  ั   ั  ั  ั  ั   ั
MY ိ  ိ  ိ  ိ  ေိ ိ  ိ  ိ   ိ KM

 ិ  ិ  ិ  ិ  ិ  ិ  ិ ើ  ិ ើិ  ើិ  ើិ ែិ ៃិ 

ើិ  ើិ   ិ ិ  ិ   ិ  ិ  ិ  ិ  ិ  ិ  ិ  ិ  ិ
LO
ະ  ិ   ិ    ិ   ិ   ិ   ិ   ិ   ិ

  ិ   ិ  ຽ  ិ   ិ   ិ   ិ   ិ   ិ
OMITTED

I II I II I II I II

MN K G U C J I Y T D N L P B M W R S H Q A E

TH กขฃ คฅฆ ง จฉ ชซฌ ญ ย ฎฏฐดตถ ฑฒทธ ณน ลฦฬ บปผฝ พฟภ ม ว รฤ ศษส หฬ อ   ๅ เ แ โ ใ ไ

MY ကခ ဂဃ င စဆ ဇဈ ဉ ည ယ ိ ဋဌတထ ဍဎဒဓ ဏန လဠ ပဖ ဗဘ မ ဝ ိ ရ ြိ သဿ ဟ ိ အ ိ  ိ  ိ ိ 

KM កខ គឃ ង ចឆ ជឈ ញ យ ដឋតថ ឌឍទធ ណន លឡ បផ ពភ ម វ រ ឝឞស ហ អ ិ  ្
LO ກຂ ຄ ງ ຈ ຊ ຍ ຢ ດຕຖ ທ ນ ລ ບປຜຝ ພຟ ມ ວ ຣ ສ ຫຮ ອ  ເ ແ ໂ ໃ ໄ

A
P

P
.

DENTALPALATE
PRE-V.

DE-V.
PLOSIVE

N
A

S.

M
E
R
G
E
D

R
-L

IK
E

S-
LI

K
E

H
-L

IK
E

LO
N

G
-A

ZE
R

O
-C

.LABIAL

PLOSIVE

N
A

S.

A
P

P
.PLOSIVE

N
A

S.

GUTTURAL

PLOSIVE

N
A

S.

A
P

P
.

Figure 3: Merging and omission for Thai (TH), Burmese (MY), Khmer (KM), and Lao (LO) scripts. The
MN row lists the mnemonics assigned to graphemes in our experiment. In this study, the mnemonics can
be assigned arbitrarily, and we selected Latin letters related to the real pronunciation wherever possible.

Fig. 2 gives an overview of this study, show-
ing examples in Khmer. We simplify abugidas by
omitting vowel diacritics and merging consonant
letters with identical or similar phonetic values,
as shown in (a). This simplification is intuitive,
both orthographically and phonetically. To resolve
the ambiguities introduced by the simplification,
we use data-driven methods to recover the origi-
nal texts, as shown in (b). We conducted experi-
ments on four southern Brahmic scripts, i.e., Thai,
Burmese, Khmer, and Lao scripts, with a unified
framework, using data from the Asian Language
Treebank (ALT) (Riza et al., 2016). The exper-
iments show that the abugidas can be recovered
satisfactorily by a recurrent neural network (RNN)
using long short-term memory (LSTM) units, even
when nearly all of the diacritics (10 – 30 types)
have been omitted and the remaining 30 – 50 char-
acters have been merged into 21 graphemes. Thai
gave the best performance, with 97% top-1 ac-
curacy for graphemes and over 99% top-4 accu-
racy. Lao, which gave the worst performance,
still achieved the top-1 and top-4 accuracies of
around 94% and 98%, respectively. The Burmese
and Khmer results, which lay in-between the other
two, were also investigated by manual evaluation.

2 Related Work

Some optimized keyboard layout have been pro-
posed for specific abugidas (Ouk et al., 2008).
Most studies on input methods have focused on
Chinese and Japanese characters, where thousands
of symbols need to be encoded and recovered. For
Chinese characters, Chen and Lee (2000) made
an early attempt to apply statistical methods to
sentence-level processing, using a hidden Markov
model. Others have examined max-entropy mod-
els, support vector machines (SVMs), conditional

random fields (CRFs), and machine translation
techniques (Wang et al., 2006; Jiang et al., 2007;
Li et al., 2009; Yang et al., 2012). Similar meth-
ods have also been developed for character con-
version in Japanese (Tokunaga et al., 2011). This
study takes a similar approach to the research on
Chinese and Japanese, transforming a less infor-
mative encoding into strings in a natural and re-
dundant writing system. Furthermore, our study
can be considered as a specific lossy compression
scheme on abugida textual data. Unlike images or
audio, the lossy text compression has received lit-
tle attention as it may cause difficulties with read-
ing (Witten et al., 1994). However, we handle this
issue within an input method framework, where
the simplified encoding is not read directly.

3 Simplified Abugidas

We designed simplification schemes for several
different scripts within a unified framework based
on phonetics and conventional usages, without
considering many language specific features. Our
primary aim was to investigate the feasibility of re-
ducing the complexity of abugidas and to establish
methods of recovering the texts. We will consider
language-specific optimization in a future work,
via both data- and user-driven studies.

The simplification scheme is shown in Fig. 3.1

Generally, the merges are based on the common
distribution of consonant phonemes in most natu-
ral languages, as well as the etymology of the char-
acters in each abugida. Specifically, three or four

1Each script also includes native punctuation marks, digit
notes, and standalone vowel characters that are not repre-
sented by diacritics. These characters were kept in the experi-
mental texts but not evaluated in the final results, as the usage
of these characters is trivial. In addition, white spaces, Latin
letters, Arabic digits, and non-native punctuation marks were
normalized into placeholders in the experiments, and were
also excluded from evaluation.



493

graphemes are preserved for the different articu-
lation locations (i.e., guttural, palate, dental, and
labial), that two for plosives, one for nasal (NAS.),
and one for approximant (APP.) if present. Addi-
tional consonants such as trills (R-LIKE), frica-
tives (S-/H-LIKE), and empty (ZERO-C.) are
also assigned their own graphemes. Although the
simplification omits most diacritics, three types
are retained, i.e., one basic mark common to
nearly all Brahmic abugidas (LONG-A), the pre-
posed vowels in Thai and Lao (PRE-V.), and
the vowel-depressors (and/or consonant-stackers)
in Burmese and Khmer (DE-V.). We assigned
graphemes to these because we found they in-
formed the spelling and were intuitive when typ-
ing. The net result was the omission of 18 types
of diacritics in Thai, 9 in Burmese, 27 in Khmer,
and 18 in Lao, and the merging of the remaining
53 types of characters in Thai, 43 in Burmese, 37
in Khmer, and 33 in Lao, into a unified set of 21
graphemes. The simplification thus substantially
reduces the number of graphemes, and represents
a straightforward benchmark for further language-
specific refinement to build on.

4 Recovery Methods

The recovery process can be formalized as a se-
quential labeling task, that takes the simplified
encoding as input, and outputs the writing units,
composed of merged and omitted character(s) in
the original abugidas, corresponding to each sim-
plified grapheme. Although structured learning
methods such as CRF (Lafferty et al., 2001) have
been widely used, we found that searching for
the label sequences in the output space was too
costly, because of the number of labels to be
recovered.2 Instead, we adopted non-structured
point-wise prediction methods using a linear SVM
(Cortes and Vapnik, 1995) and an LSTM-based
RNN (Hochreiter and Schmidhuber, 1997).

Fig. 4 shows the overall structure of the RNN.
After many experimentations, a general “shallow
and broad” configuration was adopted. Specifi-
cally, simplified grapheme bi-grams are first em-
bedded into 128-dimensional vectors3 and then
encoded in one layer of a bi-directional LSTM,

2One consonant character can be modified by multiple di-
acritics. In the ALT data used in this study, there are around
600 – 900 types of writing units in each script, and there
could be over 1, 000 on larger textual data.

3Directly embedding uni-grams (single graphemes) did
not give good performance in our preliminary experiments.

J T N N

ជិ ត ណែ ន

128-dim.

LSTM-RNN

512-dim.

input

output

linear

softmax

(256×2)

Figure 4: Structure of the RNN used in this study.

resulting in a final representation consisting of
a 512-dimensional vector that concatenates two
256-dimensional vectors from the two directions.
The number of dimensions used here is large be-
cause we found that higher-dimensional vectors
were more effective than the deeper structures for
this task, as memory capacity was more important
than classification ability. For the same reason, the
representations obtained from the LSTM layer are
transformed linearly before the softmax function
is applied, as we found that non-linear transfor-
mations, which are commonly used for final clas-
sification, did not help for this task.

5 Experiments and Evaluation

We used raw textual data from the ALT,4 compris-
ing around 20, 000 sentences translated from En-
glish. The data were divided into training, devel-
opment, and test sets as specified by the project.5

For the SVM experiments, we used the off-
the-shelf LIBLINEAR library (Fan et al., 2008)
wrapped by the KyTea toolkit.6 Table 1 gives
the recovery accuracies, demonstrating that recov-
ery is not a difficult classification task, given well
represented contextual features. In general, us-
ing up to 5-gram features before/after the sim-
plified grapheme yielded the best results for the
baseline, except with Burmese, where 7-gram fea-
tures brought a small additional improvement. Be-
cause Burmese texts use relatively more spaces
than the other three scripts, longer contexts help
more. Meanwhile, Lao produced the worst results,
possibly because the omission and merging pro-
cess was harsh: Lao is the most phonetic of the
four scripts, with the least redundant spellings.

The LSTM-based RNN was implemented using
DyNet (Neubig et al., 2017), and it was trained
using Adam (Kingma and Ba, 2014) with an initial

4http://www2.nict.go.jp/astrec-att/
member/mutiyama/ALT/

5Around 18, 000, 1, 000, and 1, 000 sentences, resp.
6http://www.phontron.com/kytea/

http://www2.nict.go.jp/astrec-att/member/mutiyama/ALT/
http://www2.nict.go.jp/astrec-att/member/mutiyama/ALT/
http://www.phontron.com/kytea/


494

learning rate of 10−3. If the accuracy decreased on
the development set, the learning rate was halved,
and learning was terminated when there was no
improvement on the development set for three it-
erations. We did not use dropout (Srivastava et al.,
2014) but instead a voting ensemble over a set of
differently initialized models trained in parallel,
which is both more effective and faster.

As shown in Table 2, the RNN outperformed
SVM on all scripts in terms of top-1 accuracy.
A more lenient evaluation, i.e., top-n accuracy,
showed a satisfactory coverage of around 98%
(Khmer and Lao) to 99% (Thai and Burmese) con-
sidering only the top four results. Fig. 5 shows the
effect of changing the size of the training dataset
by repeatedly halving it until it was one-eighth of
its original size, demonstrating that the RNN out-
performed SVM regardless of training data size.
The LSTM-based RNN should thus be a substan-
tially better solution than the SVM for this task.

We also investigated Burmese and Khmer fur-
ther using manual evaluation. The results of
RNN@1⊕16 in Table 2 were evaluated by native
speakers, who examined the output writing units
corresponding to each input simplified grapheme
and classified the errors using four levels: 0) ac-
ceptable, i.e., alternative spelling, 1) clear and
easy to identify the correct result, 2) confusing but
possible to identify the correct result, and 3) in-
comprehensible. Table 3 shows the error distribu-
tion. For Burmese, most of the errors are at lev-
els 1 and 2, and Khmer has a wider distribution.
For both scripts, around 50% of the errors are se-
rious (level 2 or 3), but the distributions suggest
that they have different characteristics. We are cur-
rently conducting a case study on these errors for
further language-specific improvements.

6 Conclusion and Future Work

In this study, a scheme was used to substantially
simplify four abugidas, omitting most diacritics
and merging the remaining characters. An SVM
and an LSTM-based RNN were then used to re-
cover the original texts, showing that the simpli-
fied abugidas could be recovered well. This illus-
trates the feasibility of encoding abugidas less re-
dundantly, which could help with the development
of more efficient input methods.

As for the future work, we are planning to in-
clude language-specific optimizations in the de-
sign of the simplification scheme and to improve

Thai Burmese Khmer Lao
Dev±3 96.1% 94.0% 94.6% 91.5%
Dev±5 97.1% 96.0% 95.7% 93.1%
Dev±7 97.0% 96.3% 95.7% 93.0%
Test 97.2% 96.1% 95.2% 93.1%
Leng. 76.0% 74.0% 77.6% 72.8%

Table 1: Top-1 recovery accuracy for the SVM.
Here, “Dev±m” represents the results for the de-
velopment set when using N -gram (N ∈ [1,m])
features within m-grapheme windows of the sim-
plified encodings, and “Test” represents the test set
results when using the feature set that gave the best
development set results. “Leng.” shows the ratio
of the number of characters in the simplified en-
codings compared with the original strings.

Thai Burmese Khmer Lao
SVM 97.2% 96.1% 95.2% 93.1%
RNN@1⊕4 97.2% 96.3%

‡ 95.5%‡ 93.3%‡

RNN@1⊕8 97.3%
† 96.4%‡ 95.6%‡ 93.6%‡

RNN@1⊕16 97.4%
‡ 96.5%‡ 95.6%‡ 93.6%‡

RNN@2⊕16 98.8% 98.4% 97.5% 96.6%
RNN@4⊕16 99.2% 98.8% 98.1% 97.7%
RNN@8⊕16 99.2% 98.9% 98.4% 97.9%

Table 2: Top-n accuracy on the test set for the
LSTM-based RNN with an m-model ensemble
(RNN@n⊕m). Here,

† and ‡ mean the RNN outper-
formed the SVM with statistical significance at
p < 10−2 and p < 10−3 level, respectively, mea-
sured by bootstrap re-sampling.

88%

90%

92%

94%

96%

98%

2.E+05 2.E+06

TH-SVM

MY-SVM

KH-SVM

LO-SVM

TH-RNN

MY-RNN

KH-RNN

LO-RNN

Figure 5: Top-1 accuracy on the test set (y-axis)
for different training data sizes (x-axis, number of
graphemes after simplification, logarithmic).

Level 0 1 2 3
Burmese 4.5% 51.0% 42.2% 2.2%
Khmer 22.5% 28.5% 16.3% 32.8%

Table 3: Recovery error distribution.

the LSTM-based RNN by integrating dictionaries
and increasing the amount of training data.



495

Acknowledgments

The experimental results of Burmese were evalu-
ated by Dr. Win Pa Pa and Ms. Aye Myat Mon
from University of Computer Studies, Yangon,
Myanmar. The experimental resutls of Khmer
were evaluated by Mr. Vichet Chea, Mr. Hour
Kaing, Mr. Kamthong Ley, Mr. Vanna Chuon,
and Mr. Saly Keo from National Institute of Posts,
Telecommunications and ICT, Cambodia. We are
grateful for their collaboration. We would like to
thank Dr. Atsushi Fujita for his helpful comments.

References
Zheng Chen and Kai-Fu Lee. 2000. A new statistical

approach to Chinese pinyin input. In Proc. of ACL.
pages 241–247.

Corinna Cortes and Vladimir Vapnik. 1995. Support-
vector networks. Machine learning 20(3):273–297.

Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-
Rui Wang, and Chih-Jen Lin. 2008. LIBLINEAR:
A library for large linear classification. Journal of
machine learning research 9(Aug):1871–1874.

Sepp Hochreiter and Jürgen Schmidhuber. 1997.
Long short-term memory. Neural computation
9(8):1735–1780.

Wei Jiang, Yi Guan, Xiaolong Wang, and Bingquan
Liu. 2007. Pinyin-to-character conversion model
based on support vector machines. Journal of Chi-
nese information processing 21(2):100–105.

Diederik Kingma and Jimmy Ba. 2014. Adam: A
method for stochastic optimization. arXiv preprint.

John Lafferty, Andrew McCallum, and Fernando C. N.
Pereira. 2001. Conditional random fields: Prob-
abilistic models for segmenting and labeling se-
quence data. In Proc. of ICML. pages 282–289.

Lu Li, Xuan Wang, Xiaolong Wang, and Yanbing Yu.
2009. A conditional random fields approach to
Chinese pinyin-to-character conversion. Journal of
Communication and Computer 6(4):25–31.

Graham Neubig, Chris Dyer, Yoav Goldberg, Austin
Matthews, Waleed Ammar, Antonios Anastasopou-
los, Miguel Ballesteros, David Chiang, Daniel
Clothiaux, Trevor Cohn, Kevin Duh, Manaal
Faruqui, Cynthia Gan, Dan Garrette, Yangfeng Ji,
Lingpeng Kong, Adhiguna Kuncoro, Gaurav Ku-
mar, Chaitanya Malaviya, Paul Michel, Yusuke
Oda, Matthew Richardson, Naomi Saphra, Swabha
Swayamdipta, and Pengcheng Yin. 2017. DyNet:
The dynamic neural network toolkit. arXiv preprint.

Phavy Ouk, Ye Kyaw Thu, Mitsuji Matsumoto, and
Yoshiyori Urano. 2008. The design of Khmer word-
based predictive non-QWERTY soft keyboard for

stylus-based devices. In Proc. of VL/HCC. pages
225–232.

Hammam Riza, Michael Purwoadi, Gunarso, Teduh
Uliniansyah, Aw Ai Ti, Sharifah Mahani Aljunied,
Luong Chi Mai, Vu Tat Thang, Nguyen Phuong
Thai, Rapid Sun, Vichet Chea, Khin Mar Soe,
Khin Thandar Nwet, Masao Utiyama, and Chenchen
Ding. 2016. Introduction of the Asian language tree-
bank. In Proc. of O-COCOSDA. pages 1–6.

Nitish Srivastava, Geoffrey E. Hinton, Alex
Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdi-
nov. 2014. Dropout: A simple way to prevent neural
networks from overfitting. Journal of machine
learning research 15(1):1929–1958.

Hiroyuki Tokunaga, Daisuke Okanohara, and Shinsuke
Mori. 2011. Discriminative method for Japanese
kana-kanji input method. In Proc. of WTIM. pages
10–18.

Xuan Wang, Lu Li, Lin Yao, and Waqas Anwar. 2006.
A maximum entropy approach to Chinese pinyin-to-
character conversion. In Proc. of SMC. pages 2956–
2959.

Ian H. Witten, Timothy C. Bell, Alistair Moffat,
Craig G. Nevill-Manning, Tony C. Smith, and
Harold Thimbleby. 1994. Semantic and generative
models for lossy text compression. The Computer
Journal 37(2):83–87.

Shaohua Yang, Hai Zhao, and Baoliang Lu. 2012. A
machine translation approach for Chinese whole-
sentence pinyin-to-character conversion. In Proc. of
PACLIC. pages 333–342.

https://dl.acm.org/citation.cfm?id=1075218.1075249
https://dl.acm.org/citation.cfm?id=1075218.1075249
https://dl.acm.org/citation.cfm?id=218929
https://dl.acm.org/citation.cfm?id=218929
https://dl.acm.org/citation.cfm?id=1442794
https://dl.acm.org/citation.cfm?id=1442794
https://dl.acm.org/citation.cfm?id=1246450
https://arxiv.org/abs/1412.6980
https://arxiv.org/abs/1412.6980
https://dl.acm.org/citation.cfm?id=655813
https://dl.acm.org/citation.cfm?id=655813
https://dl.acm.org/citation.cfm?id=655813
https://arxiv.org/abs/1701.03980
https://arxiv.org/abs/1701.03980
https://ieeexplore.ieee.org/document/4639091/
https://ieeexplore.ieee.org/document/4639091/
https://ieeexplore.ieee.org/document/4639091/
https://ieeexplore.ieee.org/document/7918974/
https://ieeexplore.ieee.org/document/7918974/
https://dl.acm.org/citation.cfm?id=2670313
https://dl.acm.org/citation.cfm?id=2670313
http://www.aclweb.org/anthology/W11-3502
http://www.aclweb.org/anthology/W11-3502
https://ieeexplore.ieee.org/document/4274331/
https://ieeexplore.ieee.org/document/4274331/
https://academic.oup.com/comjnl/article/37/2/83/491565
https://academic.oup.com/comjnl/article/37/2/83/491565
https://www.aclweb.org/anthology/Y/Y12/Y12-1.pdf
https://www.aclweb.org/anthology/Y/Y12/Y12-1.pdf
https://www.aclweb.org/anthology/Y/Y12/Y12-1.pdf

