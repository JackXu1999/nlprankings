



















































Modeling Second-Language Learning from a Psychological Perspective


Proceedings of the Thirteenth Workshop on Innovative Use of NLP for Building Educational Applications, pages 223–230
New Orleans, Louisiana, June 5, 2018. c©2018 Association for Computational Linguistics

Modeling Second-Language Learning from a Psychological Perspective

Alexander S. Rich Pamela J. Osborn Popp David J. Halpern
Anselm Rothe Todd M. Gureckis

Department of Psychology, New York University
{asr443,pamop,david.halpern,anselm,todd.gureckis}@nyu.edu

Abstract

Psychological research on learning and mem-
ory has tended to emphasize small-scale lab-
oratory studies. However, large datasets of
people using educational software provide op-
portunities to explore these issues from a
new perspective. In this paper we describe
our approach to the Duolingo Second Lan-
guage Acquisition Modeling (SLAM) com-
petition which was run in early 2018. We
used a well-known class of algorithms (gradi-
ent boosted decision trees), with features par-
tially informed by theories from the psycho-
logical literature. After detailing our mod-
eling approach and a number of supplemen-
tary simulations, we reflect on the degree to
which psychological theory aided the model,
and the potential for cognitive science and pre-
dictive modeling competitions to gain from
each other.

1 Introduction

Educational software that aims to teach people
new skills, languages, and academic subjects have
become increasingly popular. The wide-spread
deployment of these tools has created interest-
ing opportunities to study the process of learning
in large samples. The Duolingo shared task on
Second Lanugage Acquisition Modeling (SLAM)
was a competitive modeling challenge run in early
2018 (Settles et al., 2018). The challenge, orga-
nized by Duolingo1, a popular second language
learning app, was to use log data from thousands
of users completing millions of exercises to pre-
dict patterns of future translation mistakes in held-
out data. The data was divided into three sets cov-
ering Spanish speakers learning English (en es),
English speakers learning Spanish (es en), and
English speakers learning French (fr en). This
paper reports the approach used by our team,

1http://duolingo.com

which finished in third place for the en es data
set, second place for es en, and third place for
fr en.

Learning and memory has been a core focus of
psychological science for over 100 years. Most
of this work has sought to build explanatory the-
ories of human learning and memory using rela-
tively small-scale laboratory studies. Such studies
have identified a number of important and appar-
ently robust phenomena in memory including the
nature of the retention curve (Rubin and Wenzel,
1996), the advantage for spaced over massed prac-
tice (Ruth, 1928; Cepeda et al., 2006; Mozer et al.,
2009), the testing effect (Roediger and Karpicke,
2006), and retrieval-induced forgetting (Ander-
son et al., 1994). The advent of large datasets
such as the one provided in the Duolingo SLAM
challenge may offer a new perspective and ap-
proach which may prove complementary to lab-
oratory scale science (Griffiths, 2015; Goldstone
and Lupyan, 2016). First, the much larger sam-
ple sizes may help to better identify parameters
of psychological models. Second, datasets cover-
ing more naturalistic learning situations may allow
us to test the predictive accuracy of psychological
theories in a more generalizable fashion (Yarkoni
and Westfall, 2017).

Despite these promising opportunities, it re-
mains unclear how much of current psychologi-
cal theory might be important for tasks such as
the Duolingo SLAM challenge. In the field of ed-
ucation data mining, researchers trying to build
predictive models of student learning have typi-
cally relied on traditional, and interpretable, mod-
els and approaches that are rooted in cognitive
science (e.g., Atkinson, 1972b,a; Corbett and An-
derson, 1995; Pavlik and Anderson, 2008). How-
ever, a recent paper found that state-of-the-art re-
sults could be achieved using deep neural net-
works with little or no cognitive theory built in (so

223



called “deep knowledge tracing”, Piech et al.,
2015). Khajah, Lindsey, & Mozer (2016) com-
pared deep knowledge tracing (DKT) to more
standard “Bayesian knowledge tracing” (BKT)
models and showed that it was possible to equate
the performance of the BKT model by additional
features and parameters that represent core aspects
of the psychology of learning and memory such as
forgetting and individual abilities (Khajah et al.,
2016). An ongoing debate remains in this com-
munity whether using flexible models with lots of
data can improve over more heavily structured,
theory-based models (Tang et al., 2016; Xiong
et al., 2016; Zhang et al., 2017).

For our approach to the SLAM competition, we
decided to use a generic and fairly flexible model
structure that we provided with hand-coded, psy-
chologically inspired features. We therefore posi-
tioned our entry to SLAM somewhat in between
the approaches mentioned above. Specifically, we
used gradient boosting decision trees (GBDT, Ke
et al., 2017) for the model structure, which is a
powerful classification algorithm that is known to
perform well across various kinds of data sets.
Like deep learning, GBDT can extract complex
interactions among features, but it has some ad-
vantages including faster training and easier inte-
gration of diverse inputs.

We then created a number of new
psychologically-grounded features for the
SLAM dataset covering aspects such as user per-
severance, learning processes, contextual factors,
and cognate similarity. After finding a model that
provided the best held-out performance on the test
data set, we conducted a number of “lesioning”
studies where we selectively removed features
from the model and re-estimated the parameters in
order to assess the contribution of particular types
of features. We begin by describing our overall
modeling approach, and then discuss some of the
lessons learned from our analysis.

2 Task Approach

We approached the task as a binary classification
problem over instances. Each instance was a sin-
gle word within a sentence of a translation exer-
cise and the classification problem was to predict
whether a user would translate the word correctly
or not. Our approach can be divided into two
components—constructing a set of features that is
informative about whether a user will answer an

instance correctly, and designing a model that can
achieve high performance using this feature set.

2.1 Feature Engineering

We used a variety of features, including features
directly present in the training data, features con-
structed using the training data, and features that
use information external to the training data. Ex-
cept where otherwise specified, categorical vari-
ables were one-hot encoded.

2.1.1 Exercise features
We encoded the exercise number, client, session,
format, and duration (i.e., number of seconds to
complete the exercise), as well as the time since
the user started using Duolingo for the first time.

2.1.2 Word features
Using spaCy2, we lemmatized each word to pro-
duce a root word. Both the root word token and the
original token were used as categorical features.
Due to their high cardinality, these features were
not one-hot encoded but were preserved in single
columns and handled in this form by the model (as
described below).

Along with the tokens themselves we encoded
each instance word’s part of speech, morphologi-
cal features, and dependency edge label. We no-
ticed that some words in the original dataset were
paired with the wrong morphological features,
particularly near where punctuation had been re-
moved from the sentence. To fix this, we re-
processed the data using Google SyntaxNet3.

We also encoded word length and several word
characteristics gleaned from external data sources.
Research in psychology has suggested certain
word features that play a role in how difficult a
word is to process, as measured by how long read-
ers look at the word as well as people’s perfor-
mance in lexical-decision and word-identification
tasks. Two such features that have somewhat inde-
pendent effects are word frequency (i.e., how often
does the word occur in natural language; Rayner,
1998) and age-of-acquisition (i.e., the age at which
children typically exhibit the word in their vo-
cabulary; Brysbaert and Cortese, 2011; Ferrand
et al., 2011). We therefore included a feature that
encoded the frequency of each word in the lan-
guage being acquired, calculated from Speer et al.

2https://spacy.io/
3https://github.com/ljm625/syntaxnet-rest-api

224



(2017), and a feature that encoded the mean age-
of-acquisition (of the English word, in English
native speakers), derived from published age-of-
acquisition norms for 30,000 words (Kuperman
et al., 2012), which covered many of the words
present in the dataset. Additionally, words shar-
ing a common linguistic derivation (also called
“cognates”; e.g., “secretary” in English and “sec-
retario” in Spanish), are easier to learn than words
with dissimilar translations (De Groot and Keijzer,
2000). As an approximate measure of linguis-
tic similarity, we used the Levenshtein edit dis-
tance between the word tokens and their transla-
tions scaled by the length of the longer word. We
found translations using Google Translate4 and
calculated the Levenshtein distance to reflect the
letter-by-letter similarity of the word and its trans-
lation (Hyyrö, 2001).

2.1.3 User features
Just as we did for word tokens, we encoded the
user ID as a single-column, high-cardinality fea-
ture. We also calculated several other user-level
features that related to the “learning type” of a
user. In particular, we encoded features that might
be related to psychological constructs such as the
motivation and diligence of a user. These features
could help predict how users interact with old and
novel words they encounter.

As a proxy for motivation, we speculated that
more motivated users would complete more exer-
cises every time they decide to use the app. To es-
timate this, we grouped each user’s exercises into
“bursts.” Bursts were separated by at least an hour.
We used three concrete features about these bursts,
namely the mean and median number of exercises
within bursts as well as the total number of bursts
of a given user (to give the model a feature re-
lated to the uncertainty in the central tendency es-
timates).

As a proxy for diligence, we speculated that a
very diligent user might be using the app regu-
larly at the same time of day, perhaps following
a study schedule, compared to a less diligent user
whose schedule might vary more. The data set
did not provide a variable with the time of day,
which would have been an interesting feature on
its own. Instead, we were able to extract for each
exercise the time of day relative to the first time a
user had used the app, ranging from 0 to 1 (with

4https://cloud.google.com/translate/

0 indicating the same time, 0.25 indicating a rela-
tive shift by 6 hours, etc.). We then discretized this
variable into 20-minute bins and computed the en-
tropy of the empirical frequency distribution over
these bins. A lower entropy score indicated less
variability in the times of day a user started their
exercises.

The entropy score might also give an indication
for context effects on users’ memory. A user prac-
ticing exercises more regularly is more likely to be
in the same physical location when using the app,
which might result in better memory of previously
studied words (Godden and Baddeley, 1975).

2.1.4 Positional features
To account for the effects of surrounding words
on the difficulty of an instance, we created several
features related to the instance word’s context in
the exercise. These included the token of the pre-
vious word, the next word, and the instance word’s
root in the dependency tree, all stored in single
columns as with the instance token itself. We also
included the part of speech of each of these con-
text words as additional features. When there was
no previous word, next word, or dependency-tree
root word, a special None token or None part of
speech was used.

2.1.5 Temporal features
A user’s probability of succeeding on an instance
is likely related to their prior experience with that
instance. To capture this, we calculated several
features related to past experience.

First, we encoded the number of times the cur-
rent exercise’s exact sentence had been seen be-
fore by the user. This is informed by psycho-
logical research showing memory and perceptual
processing improvements for repeated contexts or
“chunks” (e.g., Chun and Phelps, 1999).

We also encoded a set of features recording
past experience with the particular instance word.
These features were encoded separately for the in-
stance token and for the instance root word created
by lemmatization. For each token (and root) we
tracked user performance through four weighted
error averages. At the user’s first encounter of the
token, each error term E starts at zero. After an
encounter with an instance of the token with label
L (0 for success, 1 for error), it is updated accord-
ing to the equation:

E ← E + α(L− E)

225



where α determines the speed of error up-
dating. The four weighted error terms use
α = {.3, .1, .03, .01}, allowing both short-run and
long-run changes in a user’s error rate with a to-
ken to be tracked. Note that in cases where a to-
ken appears multiple times in an exercise, a sin-
gle update of the error features is conducted us-
ing the mean of the token labels. Along with the
error tracking features, for each token we calcu-
lated the number of labeled, unlabeled, and total
encounters; time since last labeled encounter and
last encounter; and whether the instance is the first
encounter with the token.

In the training data, all instances are labeled as
correct or incorrect, so the label for the previous
encounter is always available. In the test data, la-
bels are unavailable, so predictions must be made
using a mix of labeled and unlabeled past encoun-
ters. In particular, for a user’s test set with n ex-
ercises, each exercise will have between zero and
n− 1 preceding unlabeled exercises.

To generate training-set features that are com-
parable to test-set features, we selectively ignored
some labels when encoding temporal features on
the training set. Specifically, for each user we first
calculated the number of exercises n in their true
test set5. Then, when encoding the features for
each training instance, we selected a random inte-
ger r in the range [0, n−1], and ignored the labels
in the prior r exercises. That is, we encoded fea-
tures for the instance as though other instances in
those prior exercises were unlabeled, and ignored
updates to the error averages from those exercises.
The result of this process is that each instance in
the training set was encoded as though it were be-
tween one and n exercises into the test set.

2.2 Modeling

After generating all of the features for the train-
ing data, we trained GBDT models to minimize
log loss. GBDT works by iteratively building re-
gression trees, each of which seeks to minimize
the residual loss from prior trees. This allows it
to capture non-linear effects and high-order inter-
actions among features. We used the LightGBM6

implementation of GBDT (Ke et al., 2017).
For continuous-valued features, GBDT can split

a leaf at any point, creating different predicted val-
5If the size of the test set were not available, it could be

estimated based on the fact that it is approximately 5% of
each participant’s data.

6http://lightgbm.readthedocs.io/

ues above and below that threshold. For categories
that are one-hot encoded, it can split a leaf on any
of the category’s features. This means that for
a category with thousands of values, potentially
thousands of tree splits would be needed to capture
its relation to the target. Fortunately, LightGBM
implements an algorithm for partitioning the val-
ues of a categorical feature into two groups based
on their relevence to the current loss, and create a
single split to divide those groups (Fisher, 1958).
Thus, as alluded to above, high-cardinality fea-
tures like token and user ID were encoded as sin-
gle columns and handled as categories by Light-
GBM.

We trained a model for each of the three lan-
guage tracks of en es, es en, and fr en, and
also trained a model on the combined data from all
three tracks, adding an additional “language” fea-
ture. Following model training, we averaged the
predictions of each single-language model with
that of the all-language model to form our fi-
nal predictions. Informal experimentation showed
that model averaging provided a modest perfor-
mance boost, and that weighted averages did not
clearly outperform a simple average.

To tune model hyper-parameters and evaluate
the usefulness of features, we first trained the mod-
els on the train data set and evaluated them on
the dev data set. Details of the datasets and the
actual files are provided on the Harvard Dataverse
(Settles, 2018). Once the model structure was fi-
nalized, we trained on the combined train and
dev data and produced predictions for the test
data. The LightGBM hyperparameters used for
each model are listed in Table 1.

2.3 Performance

The AUROC of our final predictions was .8585
on en es, .8350 on es en, and .8540 on fr en.
For reference this placed us within .01 of the win-
ning entry for each problem (.8613 on en es,
.8383 on es en, and .8570 on fr en). Also
note that the Duolingo-provided baseline model
(L2-regularized regression trained with stochastic
gradient descent weighted by frequency) obtains
.7737 on en es, .7456 on es en, and .7707 on
fr en. We did not attempt to optimize F1 score,
the competition’s secondary evaluation metric.

226



Parameter fr en en es es en all
num leaves 256 512 512 1024
learning rate .05 .05 .05 .05
min data in leaf 100 100 100 100
num boost rounds 750 650 600 750
cat smooth 200 200 200 200
feature fraction .7 .7 .7 .7
max cat threshold 32 32 32 64

Table 1: Parameters of final LightGBM models. See LightGBM documentation for more information; all other
parameters were left at their default values.

temporal

external

neighbors

word other

word ids

word all

user other

user id

user all

none

0.820 0.825 0.830 0.835 0.840 0.845

AUROC

le
si

on
ed

 fe
at

ur
es lesion type

none

user

word

other

Figure 1: Performance on dev of models trained on all train data, with different groups of lesioned features.
See main text for description of lesion groups

3 Feature Removal Experiments

To better understand which features or groups of
features were most important to our model’s pre-
dictions, we conducted a set of experiments in
which we lesioned (i.e., removed) a group of fea-
tures and re-trained the model on the train set,
evaluating performance on the dev set. For sim-
plicity, we ran each of the lesioned models on all
language data and report the average performance.
We did not run individual-language models as we
did for our primary model. The results of the le-
sion experiments are shown in Figure 1. The mod-
els are as follows.

none: All features are included.

user all: All user-level features, including the
user ID and other calculated features like en-
tropy and measures of exercise bursts, are re-
moved.

user id & user other: Only user ID or only the
calculated user features, respectively, are re-
moved.

word all: Token and token root IDs; previous,
next, and dependency-tree root word IDs; and

morphological, part of speech, and depen-
dency tree features are removed. This does
not include external features listed below.

word id & word other: Only word IDs or only
other word features, respectively, are re-
moved. word other does not include exter-
nal features listed below.

neighbors: Both word IDs and other word fea-
tures are removed, but only for the previous,
next, and dependency-tree root words. Infor-
mation about the present word is maintained.

external: External information about the word,
including corpus frequency, Levenshtein dis-
tance from translation, and age of acquisition,
are removed.

temporal: Temporal information, including num-
ber and timing of past encounters with the
word and error tracking information, is re-
moved.

Interestingly, we found that for both user-level
and word-level features, the bulk of the model’s
predictive power could be achieved using ID’s

227



alone, represented as high-cardinality categorical
features. Removing other word features, such as
morphological features and part of speech, created
only a small degradation of performance. In the
case of users, removing features such as entropy
and average exercise burst length led to a tiny in-
crease of performance. In the case of both users
and words, though, we find that in the absence of
ID features the other features are helpful and lead
to better performance than removing all features.
We also found that removing all information about
neighboring words and the dependency-parse root
word degraded performance. This confirms that
word context matters, and suggests that users com-
monly make errors in word order, subject–verb
matching and other grammatical rules.

Our external word features—Levenshtein dis-
tance to translation, frequency, and age of
acquisition—provided a slight boost to model per-
formance, showing the benefit of considering what
makes a word hard to learn from a psychological
and linguistic perspective. Adding temporal fea-
tures about past encounters and errors helped the
models, but not as much as we expected. While
not included in the final model, we had also tried
augmenting the temporal feature set with more
features related to massing and spacing of encoun-
ters with a word, but found it did not improve per-
formance. This is perhaps not surprising given
how small the benefit of the existing temporal fea-
tures are in our model.

Though not plotted above, we also ran a model
lesioning exercise-level features including client,
session type, format, and exercise duration. This
model achieved an AUROC of .787, far lower than
any other lesion. This points to the fact that the
manner in which memory is assessed often affects
observed performance (e.g., the large literature in
psychology on the difference between recall and
recognition memory, Yonelinas, 2002).

4 Discussion

When approaching the Duolingo SLAM task, we
hoped to leverage psychological insights in build-
ing our model. We found that in some cases, such
as when using the word’s age-of-acquisition, this
was helpful. In general, though, our model gained
its power not from hand-crafted features but from
applying a powerful inference technique (gradi-
ent boosted decision trees) to raw input about user
IDs, word IDs, and exercise features.

There are multiple reasons for the limited appli-
cability of psychology to this competition. First,
computational psychological models are typically
designed based on small laboratory data sets,
which might limit their suitability for generating
highly accurate predictions in big data settings.
Because they are designed not for prediction but
for explanation, they tend to use a small number of
input variables and allow those variables to inter-
act in limited ways. In contrast, gradient boosted
decision trees, as well as other cutting-edge tech-
niques like deep learning can extract high-level in-
teractions among hundreds of features. While they
are highly opaque, require a lot of data, and are
not amenable to explanation, these models excel
at prediction.

Second, it is possible that our ability to use the-
ories of learning, including ideas about massed
and spaced practice, was disrupted by the fact that
the data may have been adaptively created using
these very principles (Settles and Meeder, 2016).
If Duolingo adaptively sequenced the spacing of
trials based on past errors, then the relationship
between future errors and past spacing may have
substantially differed from that found in the psy-
chological literature (Cepeda et al., 2006).

Finally, if the task had required broader gener-
alization, psychologically inspired features might
have performed more competitively. In the SLAM
task, there is a large amount of labeled training
data for every user and for most words. This al-
lows simple ID-based features to work because
the past history of a user will likely influence
their future performance. However, with ID-based
features there is no way to generalize to newly-
encountered users or words, which have an ID
that was not in the training set. The learned ID-
based knowledge is useless here because there is
no way to generalize from one unique ID to an-
other. Theory-driven features, in contrast, can of-
ten generalize to new settings because they capture
aspects that are shared across (subsets of) users,
words, or situations of the learning task. For ex-
ample, if we were asked to generalize to a com-
pletely new language such as German, many parts
of our model would falter but word frequency, age
of acquisition, and Levenshtein distance to first-
language translation would still likely prove to be
features which have high predictive utility.

In sum, we believe that the Duolingo SLAM
dataset and challenge provide interesting oppor-

228



tunities for cognitive science and psychology.
Large-scale, predictive challenges like this one
might be used to identify features or variables that
are important for learning. Then, complementary
laboratory-scale studies can be conducted which
establish the causal status of such features through
controlled experimentation. Conversely, insights
from controlled experiments can be used to gener-
ate new features that aid predictive models on nat-
uralistic datasets (Griffiths, 2015; Goldstone and
Lupyan, 2016). This type of two-way interaction
could lead to long-run improvements in both sci-
entific explanation and real-world prediction.

5 Acknowledgments

This research was supported by NSF grant DRL-
1631436 and BCS-1255538, and the John S. Mc-
Donnell Foundation Scholar Award to TMG. We
thank Shannon Tubridy and Tal Yarkoni for help-
ful suggestions in the development of this work.

References
Michael C Anderson, Robert A Bjork, and Elizabeth L

Bjork. 1994. Remembering can cause forgetting:
Retrieval dynamics in long-term memory. Jour-
nal of Experimental Psychology: Learning, Mem-
ory, and Cognition, 20:1063–1087.

R.C. Atkinson. 1972a. Ingredients for a theory of in-
struction. American Psychologist, 27:921–931.

R.C. Atkinson. 1972b. Optimizing the learning of a
second-language vocabulary. Journal of Experimen-
tal Psychology, 96:124–129.

Marc Brysbaert and Michael J Cortese. 2011. Do
the effects of subjective frequency and age of ac-
quisition survive better word frequency norms?
Quarterly Journal of Experimental Psychology,
64(3):545–559.

N.J. Cepeda, H. Pashler, E. Vul, J.T. Wixted, and
D. Rohrer. 2006. Distributed practice in verbal re-
call tasks: A review and quantitative synthesis. Psy-
chological Bulletin, 132(3):354–380.

M.M. Chun and E.A. Phelps. 1999. Memory deficits
for implicit contextual information in amnesic sub-
jects with hippocampal damage. Nature Neuro-
science, 2(9):844–847.

AT Corbett and JR Anderson. 1995. Knowledge track-
ing: Modeling the acquisition of procedural knowl-
edge. User Modeling and User-Adapted Interac-
tion, 4:253–278.

Annette De Groot and Rineke Keijzer. 2000. What is
hard to learn is easy to forget: The roles of word

concreteness, cognate status, and word frequency
in foreign-language vocabulary learning and forget-
ting. Language Learning, 50(1):1–56.

Ludovic Ferrand, Marc Brysbaert, Emmanuel
Keuleers, Boris New, Patrick Bonin, Alain Méot,
Maria Augustinova, and Christophe Pallier. 2011.
Comparing word processing times in naming, lexi-
cal decision, and progressive demasking: Evidence
from chronolex. Frontiers in psychology, 2:306.

Walter D Fisher. 1958. On grouping for maximum ho-
mogeneity. Journal of the American statistical As-
sociation, 53(284):789–798.

Duncan R Godden and Alan D Baddeley. 1975.
Context-dependent memory in two natural environ-
ments: On land and underwater. British Journal of
psychology, 66(3):325–331.

R.L. Goldstone and G. Lupyan. 2016. Discovering
psychological principles by mining naturally occur-
ring data sets. Topics in Cognitive Science, 8:548–
568.

T.M. Griffiths. 2015. Manifesto for a new (computa-
tional) cognitive revolution. Cognition, 135:21–23.

Heikki Hyyrö. 2001. Explaining and extending the bit-
parallel approximate string matching algorithm of
myers. Technical report in Journal of the ACM, page
408.

Guolin Ke, Qi Meng, Thomas Finley, Taifeng Wang,
Wei Chen, Weidong Ma, Qiwei Ye, and Tie-Yan Liu.
2017. Lightgbm: A highly efficient gradient boost-
ing decision tree. In Advances in Neural Informa-
tion Processing Systems, pages 3149–3157.

Mohammad Khajah, Robert V. Lindsey, and Michael
Mozer. 2016. How deep is knowledge tracing? Pro-
ceedings of the Educational Data Mining (EDM).

Victor Kuperman, Hans Stadthagen-Gonzalez, and
Marc Brysbaert. 2012. Age-of-acquisition ratings
for 30,000 english words. Behavior Research Meth-
ods, 44(4):978–990.

M. C. Mozer, H. Pashler, N. Cepeda, R. Lindsey, and
E. Vul. 2009. Predicting the optimal spacing of
study: A multiscale context model of memory. In
Advances in Neural Information Processing Systems
22, pages 1321–1329, La Jolla, CA. NIPS Founda-
tion.

P.I. Pavlik and J.R. Anderson. 2008. Using a model to
compute the optimal schedule of practice. Journal
of Experimental Psychology: Applied, 14(2):101–
117.

C. Piech, J. Bassen, J. Huang, S. Ganguli, M. Sahami,
L. J. Guibas, and J. Sohl-Dickstein. 2015. Deep
knowledge tracing. In Advances in Neural Informa-
tion Processing Systems 28, pages 505–513.

229



Keith Rayner. 1998. Eye movements in reading and
information processing: 20 years of research. Psy-
chological bulletin, 124(3):372.

Henry L. Roediger and Jeffrey D. Karpicke. 2006.
Test-enhanced learning: Taking memory tests im-
proves long-term retention. Psychological Science,
17(3):249–255. PMID: 16507066.

D.C. Rubin and A.E. Wenzel. 1996. One hundred years
of forgetting: A quantitative description of retention.
Psychological Review, 103(4):734–760.

T.C. Ruth. 1928. Factors influencing the relative econ-
omy of massed and distributed practice in learning.
Psychological Review, 35:19–45.

B. Settles, C. Brust, E. Gustafson, M. Hagiwara, and
N. Madnani. 2018. Second language acquisition
modeling. In Proceedings of the NAACL-HLT Work-
shop on Innovative Use of NLP for Building Educa-
tional Applications (BEA). ACL.

Burr Settles. 2018. Data for the 2018 duolingo
shared task on second language acquisition model-
ing (slam).

Burr Settles and Brendan Meeder. 2016. A trainable
spaced repetition model for language learning. In
Proceedings of the 54th Annual Meeting of the As-
sociation for Computational Linguistics (Volume 1:
Long Papers), volume 1, pages 1848–1858.

Robert Speer, Joshua Chin, Andrew Lin, Sara Jew-
ett, and Lance Nathan. 2017. Luminosoin-
sight/wordfreq: v1.7.

Steven Tang, Joshua C. Peterson, and Zachary A. Par-
dos. 2016. Deep neural networks and how they ap-
ply to sequential education data. In Proceedings of
the Third (2016) ACM Conference on Learning @
Scale, L@S ’16, pages 321–324, New York, NY,
USA. ACM.

Xiaolu Xiong, Siyuan Zhao, Eric Van Inwegen, and
Joseph Beck. 2016. Going deeper with deep knowl-
edge tracing. In EDM, pages 545–550.

T. Yarkoni and J. Westfall. 2017. Choosing predic-
tion over explanation in psychology: Lessons from
machine learning. Persectives in Psychological Sci-
ence, 12(6):1100–1122.

Andrew P Yonelinas. 2002. The nature of recollection
and familiarity: A review of 30 years of research.
Journal of memory and language, 46(3):441–517.

Liang Zhang, Xiaolu Xiong, Siyuan Zhao, Anthony
Botelho, and Neil T Heffernan. 2017. Incorporating
rich features into deep knowledge tracing. In Pro-
ceedings of the Fourth (2017) ACM Conference on
Learning@ Scale, pages 169–172. ACM.

230


