



















































Learning Word Embeddings without Context Vectors


Proceedings of the 4th Workshop on Representation Learning for NLP (RepL4NLP-2019), pages 244–249
Florence, Italy, August 2, 2019. c©2019 Association for Computational Linguistics

244

Learning Word Embeddings without Context Vectors

Alexey Zobnin
Yandex,

National Research University
Higher School of Economics,

Moscow, Russia
azobnin@hse.ru

Evgenia Elistratova
Moscow State University,

Moscow, Russia
evg3307@yandex.ru

Abstract

Most word embedding algorithms such as
word2vec or fastText construct two sort of vec-
tors: for words and for contexts. Naive use of
vectors of only one sort leads to poor results.
We suggest using indefinite inner product in
skip-gram negative sampling algorithm. This
allows us to use only one sort of vectors with-
out loss of quality. Our “context-free” cf al-
gorithm performs on par with SGNS on word
similarity datasets.

1 Introduction

Vector representation of words are widely used in
NLP tasks. Two approaches to word embeddings
are usually contrasted: implicit (word2vec-like)
and explicit (SVD-like). Implicit models are usu-
ally faster and consume less memory than their ex-
plicit analogues.

Typically, word embedding algorithms produce
two matrices both for “words” and “contexts”.
Usually, contexts are the words themselves. It is
believed that word and context vectors cannot be
equated to each other.

In practice, however, only the vectors of one
sort are considered. For example, typical solutions
to word similarity or analogy problems use only
the inner products of word vectors.

We present a modified skip-gram negative sam-
pling algorithm that produces related word and
context vectors. One may say that some compo-
nents of our word and context vectors are equal,
while other components have different signs. An-
other point of view is to say that word and context
vectors are completely equal, but the inner prod-
uct between them is indefinite. This relation was
suggested by the properties of explicit SVD em-
beddings.

2 Preliminaries

We briefly recall the skip-gram negative sampling
(SGNS) algorithm implemented in popular pro-
grams word2vec (Mikolov et al., 2013a,b) and
fastText (Bojanowski et al., 2017). Given row
vectors of a current wordw, its context c0 and neg-
ative context samples c1, . . . , ck, SGNS algorithm
computes the loss

L = − lnσ(wcT0 )−
k∑

j=1

lnσ(−wcTj ), (1)

where σ(x) = 1
1+e−x is the sigmoid function.

Starting from random initial approximation of
word vectors, the algorithm uses stochastic gradi-
ent descent (SGD) for optimization. The intuition
is the following: the word vector should be similar
to the true context vector and dissimilar to random
negative contexts. Due to the properties of σ(x),
the gradient and update formulas look simple.

Let n be the size of the vocabulary, and d be
the dimension of embeddings. Let W and C be
n × d matrices of word (respectively, context)
vectors written in rows. The SGNS loss (1) de-
pends only on elements of WCT , and does not
depend on W or C separately. The optimal solu-
tion of SGNS is not unique: the transformation
W 7→ WS, C 7→ C

(
S−1

)T for an invertible
d×d-matrix S, gives an equivalent solution. How-
ever, such transformation could change the inner
products between word vectors dramatically if S
is not orthogonal. Nevertheless, SGNS produces a
“good” solution without any regularization. This
phenomenon is not well understood, and can be
seen as “implicit regularization” of SGD.

Levy and Goldberg showed that the implicit ma-
trix M = WCT tends to the shifted PMI matrix
M when d is sufficiently large (Levy and Gold-
berg, 2014). HereMi,j = PMI(wi, wj) − log k,



245

where

PMI(wi, wj) = log
P (wiwj)

P (wi)P (wj)

is the pointwise mutual information of the pair of
wordswi,wj , and k is the amount of negative sam-
ples. Thus, SGNS can be considered as an implicit
matrix factorization problem.

We recall that a (compact) singular value de-
composition (SVD) of a real-valued m×n-matrix
M of rank r is a decomposition M = UΣV T ,
where U and V are respectively m × r and n × r
matrices with orthogonal columns, and Σ is r × r
diagonal matrix with non-zero singular values on
diagonal. We refer to (Golub and Van Loan, 2012)
for details. SVD embeddings are obtained from
truncated SVD decomposition M ≈ UdΣdV Td ,
where only d top singular values and vectors are
kept. The common way is to take W = Ud

√
Σd

and C = Vd
√

Σd.

3 Main result

First of all, we make an observation about SVD
embeddings. We shall use it further to modify the
SGNS algorithm.

3.1 SVD of real symmetric matrices

Proposition. Let M be a real-valued symmetric
n× n-matrix.

1. There exists a decompositionM = WDW T ,
where D = diag(±1).

2. There exists a compact SVD M = UΣV T

such that V = UD.

3. If all singular values of M are different, then
all singular value decompositions of M have
this form1.

In other words, the corresponding columns of U
and V either coincide, or differ in sign.

Proof. All eigenvalues of a real symmetric ma-
trix are real. Moreover, M has an eigendecompo-
sition M = CΛCT , where Λ = diag(λi) is a di-
agonal matrix of eigenvalues, and C is an orthog-
onal matrix. Let r be the rank of M . We may re-
move columns in C and Λ that correspond to zero
eigenvalues and assume that C is n×r-matrix and

1This is also valid for a more general case when equal
singular values of M correspond to the eigenvalues of the
same sign.

Λ is r× r-matrix with non-zero eigenvalues on di-
agonal. Note that non-zero singular values σi of
M are absolute values of λi.

1. Let Σ = diag(σi) be a diagonal matrix of
non-zero singular values. Then Λ = ΣD =√

ΣD
√

Σ, where

Dii =

{
1, if λi > 0,
−1, if λi < 0.

Now take W = C
√

Σ.

2. Write M as CΣDCT and take U = C, V =
CD.

3. If all singular values are different, then SVD
is determined uniquely up to a simultaneous
change of signs in some columns of U and
V . Take the SVD constructed above and note
that the relation V = UD is preserved after
these transformations.

We denote by q the amount of −1 in D. Due to
the Sylvester’s law of inertia, it is uniquely deter-
mined by M and equals to the amount of negative
eigenvalues of M .

3.2 Negative eigenvalues of word relation
matrices

Consider an n × n-matrix M = (f(wi, wj)) de-
scribing the relation between the wordswi andwj .
For example, f(wi, wj) may be the shifted PPMI,
as suggested in (Levy and Goldberg, 2014):

f(wi, wj) = max (0, PMI(wi, wj)− log k) .

As a rule, the relation f is symmetric, and hence
M is symmetric too.

Let’s look at the SVD embeddings obtained
from this matrix. Let M = UΣV T be an SVD,
and M ≈ Md = UdΣdV Td be its truncated ap-
proximation. Then symmetric SVD embeddings
are W = Ud

√
Σd and C = Vd

√
Σd. In the fol-

lowing, we will naturally assume that top d singu-
lar values ofM are different and non-zero: all real
cases are just like that. By the proposition we have
that some columns ofW andC coincide, while the
others differ in sign. As a consequence, we ob-
tain that for such SVD embeddings word and con-
text vectors are equally good in applied problems,
because inner products (wiwj) and (cicj) are the
same. We would like to construct implicit SGNS-
like embeddings with similar properties.



246

The amount of negative eigenvalues of M mea-
sures the deviation from the positive definiteness
in some sense. To estimate it, we construct shifted
PPMI matrices for Wikipedia corpora in three dif-
ferent languages (English, French and Russian).
Each corpus contains 1M articles. We measure
the amount of negative eigenvalues among top by
magnitude eigenvalues. The results for k = 1
(pure PPMI matrix, no shift) and for k = 5 are
presented in Tables 1 and 2, respectively. We see
that the rate of negative eigenvalues is about 11–
13% for k = 1 and about 7–8% for k = 5. Sur-
prisingly, this rate actually does not depend on the
language.

corpus \ dimension 100 200 300
English Wikipedia 15 27 39
French Wikipedia 10 22 34
Russian Wikipedia 11 22 31

Table 1: Amount of negative eigenvalues in the trun-
cated SVD of PPMI matrix (k = 1, i. e., no shift).

corpus \ dimension 100 200 300
English Wikipedia 9 16 24
French Wikipedia 7 15 22
Russian Wikipedia 7 14 20

Table 2: Amount of negative eigenvalues in the trun-
cated SVD of shifted PMI matrix with k = 5.

3.3 Context-free SGNS algorithm
Recall that SGNS loss depends on the inner prod-
ucts of words and contexts. Let’s fix the matrix

D = diag(−1, . . . , −1︸ ︷︷ ︸
q

, 1, 1, . . . , 1︸ ︷︷ ︸
p=d−q

)

specifying indefinite inner product in the embed-
ding space. The amount of minus ones in this ma-
trix, q, will be a hyperparameter of our algorithm.
Next, we equate word and context vectors to each
other. This corresponds to the implicit factoriza-
tion WDW T instead of WCT .

Thus, we replace the initial SGNS loss (1) with

Lq = − lnσ(wDwT0 )−
k∑

j=1

lnσ(−wDwTj ).

The solution of this new optimization problem
is also not unique. Replacing W with WS for

any matrix S that preserves D, i. e., such that
SDST = D, yields another solution. Neverthe-
less, the solution set in our case is “smaller” in
some sense than in the pure SGNS case, where S
may be any invertible matrix.

Since we got rid of context vectors, we call this
algorithm context-free SGNS2 and denote it cf.

4 Experimental results

4.1 Training setup
We train word embeddings on the English
Wikipedia dump. We preprocess this dump us-
ing gensim.corpora.wikicorpus package3 and take
a subsample of 100K articles. Our corpus consists
of approximately 175M words with n ≈ 312000.
We use fastText4 skipgram mode with the default
values of parameters and without ngrams (-maxn
0) as a vanilla SGNS implementation. We im-
plement our model by modifying the C++ im-
plementation of fastText5. We learn cf vectors
of dimension d = 100 and 100 + q for q =
0, 5, 10, 15, 20, 25. Vectors of dimension 100 + q
were projected to 100 “positive” components.

4.2 Datasets
Datasets for word similarity evaluation consist of
pairs of words rated by humans. We use the
following well-known English similarity datasets:
MEN-3k (Bruni et al., 2014), MTurk-287 (Ha-
lawi et al., 2012), RW-STANFORD (Luong et al.,
2013), SimLex-999 (Hill et al., 2015), SimVerb-
3500 (Gerz et al., 2016), VERB-143 (Baker et al.,
2014), and WS-353 (Finkelstein et al., 2002) split-
ted into similarity and relatedness parts (Agirre
et al., 2009). We use the evaluation code (Faruqui
and Dyer, 2014) for computing Spearman rank
correlation ρ between the similarity scores and the
annotated ratings.

4.3 Results
Table 3 shows the results for word vectors of di-
mension 100 with q negative components in D.
The best results are written in bold, and the best
cf results are underlined. We see that q = 0,
i. e., the case of positive semidefinite approxima-
tion, has the worst performance. The better perfor-

2Not to be confused with context-free grammars.
3https://radimrehurek.com/gensim/

corpora/wikicorpus.html.
4https://fasttext.cc/.
5https://github.com/jen1995/fastText/

tree/sgns-loss.

https://radimrehurek.com/gensim/corpora/wikicorpus.html
https://radimrehurek.com/gensim/corpora/wikicorpus.html
https://fasttext.cc/
https://github.com/jen1995/fastText/tree/sgns-loss
https://github.com/jen1995/fastText/tree/sgns-loss


247

Dataset SGNS q = 0 q = 5 q = 10 q = 15 q = 20 q = 25
MEN-TR-3k .7314 .6790 .7234 .7185 .7165 .7124 .7092
MTurk-287 .6668 .6335 .6574 .6604 .6615 .6609 .6725
RW-STANFORD .3801 .2862 .4094 .4202 .4223 .4151 .4090
SIMLEX-999 .3323 .2520 .3141 .3194 .3126 .3098 .3226
SimVerb-3500 .2022 .1317 .1890 .1948 .1975 .1942 .1965
VERB-143 .3000 .2995 .3323 .3304 .3498 .3834 .3595
WS-353-REL .6648 .6142 .6372 .6594 .6389 .6306 .6296
WS-353-SIM .7612 .7079 .7508 .7538 .7525 .7506 .7332
average .4380 .3700 .4346 .4384 .4382 .4345 .4340

Table 3: Word similarity for d = 100.

Dataset SGNS q = 0 q = 5 q = 10 q = 15 q = 20 q = 25
MEN-TR-3k .7314 .6790 .7333 .7348 .7351 .7371 .7370
MTurk-287 .6668 .6335 .6612 .6697 .6719 .6746 .6645
RW-STANFORD .3801 .2862 .4015 .4091 .4069 .4022 .3999
SIMLEX-999 .3323 .2520 .3138 .3127 .3199 .3166 .3235
SimVerb-3500 .2022 .1317 .1926 .1917 .1894 .1905 .1914
VERB-143 .3000 .2995 .3362 .3148 .3141 .3285 .3315
WS-353-REL .6648 .6142 .6673 .6714 .6765 .6718 .6741
WS-353-SIM .7612 .7079 .7627 .7476 .7577 .7554 .7480
average .4380 .3700 .4382 .4394 .4394 .4392 .4394

Table 4: Word similarity for p = 100 (only “positive” components of p+ q-dimensional cf vectors were taken).

mance of cf is achieved at q = 10 or q = 15. This
argees with empirical results of Subsection 3.2. In
general, cf is either on par with SGNS, or slightly
loses. In the second experiment we learn cf vec-
tors of higher dimension 100 + q and projected
them to 100 “positive” components. These results
are shown in Table 4. Starting from q = 5, they
are slightly better than SGNS.

5 Related work

There were several attempts to establish a connec-
tion between word and context vectors. In (Li
et al., 2017) a PMI matrix is approximated by a
positive semidefinite matrix of the form WW T .
Dependencies between word and context vec-
tors for word2vec SGNS model were studied
in (Mimno and Thompson, 2017).

In (Allen et al., 2018) it is suggested that word
and context vectors should be conjugated in the
complex space. Our model can be reformulated in
terms of complex vectors too in the case p = q,
but we prefer to stay in reals. One of the problems
with complex embedding is that σ(z) is not holo-
morphic in the whole complex plane, and should
be replaced with σ(|z|) or σ(Re z). Complex-

valued embeddings are also discussed in (Trouil-
lon et al., 2016), where a complex decomposition
like M = ReUΛŪT is suggested.

In (Assylbekov and Takhanov, 2019) it was
conjectured that context vectors are reflections of
word vectors in half the dimensions. Our result
is similar, but we suggest using lower amount of
reflections. Perhaps this difference is due to the
fact that we consider shifted PPMI matrices, while
they consider pure PMI matrices.

Embeddings in hyperbolic space with
Minkowski metric was suggested in (Leimeister
and Wilson, 2018). In (Soleimani and Matwin,
2018) there is an erroneous statement that a
thresholded PMI matrix, as well as any symmetric
matrix, has SVD decomposition UΣUT : in fact it
is true only for positive semidefinite matrices.

Non-uniqueness of SGNS solutions was ad-
dressed in (Fonarev et al., 2017) and (Mu et al.,
2018), resulting in new models. Implicit regular-
ization of SGD in neural networks and in matrix
factorization problems was studied in (Gunasekar
et al., 2017; Neyshabur et al., 2017; Ma et al.,
2018), but SGNS loss was not considered directly
in these works.



248

6 Conclusion

We proposed cf, an alternative to SGNS algo-
rithm that do not use context vectors. Instead,
indefinite inner product between word vectors is
used. Our algorithm shows similar results com-
pared to SGNS.

The phenomenon of implicit regularization of
SGNS, as well as the problem of finding the lin-
guistic interpretation of “negative” components of
word vectors in our algorithm, deserve further in-
vestigation.

Acknowledgements

The authors thank Anna Potapenko for fruitful
discussions and anonymous referees for valuable
comments.

References
Eneko Agirre, Enrique Alfonseca, Keith Hall, Jana

Kravalova, Marius Paşca, and Aitor Soroa. 2009. A
study on similarity and relatedness using distribu-
tional and wordnet-based approaches. In Proceed-
ings of Human Language Technologies: NAACL-
2009, pages 19–27. ACL.

Carl Allen, Ivana Balažević, and Timothy Hospedales.
2018. What the vec? Towards probabilistically
grounded embeddings. arXiv:1805.12164.

Zhenisbek Assylbekov and Rustem Takhanov. 2019.
Context vectors are reflections of word vectors in
half the dimensions. arXiv:1902.09859.

Simon Baker, Roi Reichart, and Anna Korhonen. 2014.
An unsupervised model for instance level subcate-
gorization acquisition. In Proceedings of the 2014
Conference on Empirical Methods in Natural Lan-
guage Processing (EMNLP), pages 278–289.

Piotr Bojanowski, Edouard Grave, Armand Joulin, and
Tomas Mikolov. 2017. Enriching word vectors with
subword information. Transactions of the Associa-
tion for Computational Linguistics, 5:135–146.

Elia Bruni, Nam-Khanh Tran, and Marco Baroni. 2014.
Multimodal distributional semantics. Journal of Ar-
tificial Intelligence Research, 49:1–47.

Manaal Faruqui and Chris Dyer. 2014. Community
evaluation and exchange of word vectors at word-
vectors. org. In Proceedings of 52nd Annual Meet-
ing of ACL: System Demonstrations, pages 19–24.

Lev Finkelstein, Evgeniy Gabrilovich, Yossi Matias,
Ehud Rivlin, Zach Solan, Gadi Wolfman, and Ey-
tan Ruppin. 2002. Placing search in context: The
concept revisited. ACM Transactions on informa-
tion systems, 20(1):116–131.

Alexander Fonarev, Oleksii Grinchuk, Gleb Gusev,
Pavel Serdyukov, and Ivan Oseledets. 2017. Rie-
mannian optimization for skip-gram negative sam-
pling. In Proceedings of the 55th Annual Meeting
of the ACL (Volume 1: Long Papers), pages 2028–
2036.

Daniela Gerz, Ivan Vulić, Felix Hill, Roi Reichart, and
Anna Korhonen. 2016. Simverb-3500: A large-
scale evaluation set of verb similarity. In Proceed-
ings of the 2016 Conference on Empirical Methods
in Natural Language Processing, pages 2173–2182.

Gene H Golub and Charles F Van Loan. 2012. Matrix
computations, volume 3. JHU press.

Suriya Gunasekar, Blake E Woodworth, Srinadh Bho-
janapalli, Behnam Neyshabur, and Nati Srebro.
2017. Implicit regularization in matrix factorization.
In Advances in Neural Information Processing Sys-
tems, pages 6151–6159.

Guy Halawi, Gideon Dror, Evgeniy Gabrilovich, and
Yehuda Koren. 2012. Large-scale learning of word
relatedness with constraints. In Proceedings of
the 18th ACM SIGKDD international conference on
Knowledge discovery and data mining, pages 1406–
1414. ACM.

Felix Hill, Roi Reichart, and Anna Korhonen. 2015.
Simlex-999: Evaluating semantic models with (gen-
uine) similarity estimation. Computational Linguis-
tics, 41(4):665–695.

Matthias Leimeister and Benjamin J Wilson. 2018.
Skip-gram word embeddings in hyperbolic space.
arXiv:1809.01498.

Omer Levy and Yoav Goldberg. 2014. Neural word
embedding as implicit matrix factorization. In Ad-
vances in Neural Information Processing Systems,
pages 2177–2185.

Shaohua Li, Jun Zhu, and Chunyan Miao. 2017. Ps-
dvec: A toolbox for incremental and scalable word
embedding. Neurocomputing, 237:405–409.

Thang Luong, Richard Socher, and Christopher Man-
ning. 2013. Better word representations with recur-
sive neural networks for morphology. In Proceed-
ings of the 17th Conference on Computational Nat-
ural Language Learning, pages 104–113.

Cong Ma, Kaizheng Wang, Yuejie Chi, and Yuxin
Chen. 2018. Implicit regularization in nonconvex
statistical estimation: Gradient descent converges
linearly for phase retrieval and matrix completion.
In Proceedings of the 35th International Conference
on Machine Learning, volume 80.

Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey
Dean. 2013a. Efficient estimation of word represen-
tations in vector space. arXiv:1301.3781.



249

Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Cor-
rado, and Jeff Dean. 2013b. Distributed representa-
tions of words and phrases and their compositional-
ity. In Advances in neural information processing
systems, pages 3111–3119.

David Mimno and Laure Thompson. 2017. The strange
geometry of skip-gram with negative sampling. In
Proceedings of the 2017 Conference on Empirical
Methods in Natural Language Processing, pages
2873–2878.

Cun Mu, Guang Yang, and Zheng Yan. 2018. Revisit-
ing skip-gram negative sampling model with rectifi-
cation. arXiv:1804.00306.

Behnam Neyshabur, Ryota Tomioka, Ruslan Salakhut-
dinov, and Nathan Srebro. 2017. Geometry of op-
timization and implicit regularization in deep learn-
ing. arXiv:1705.03071.

Behrouz Haji Soleimani and Stan Matwin. 2018. Spec-
tral word embedding with negative sampling. In
32nd AAAI Conference on Artificial Intelligence.

Théo Trouillon, Johannes Welbl, Sebastian Riedel, Éric
Gaussier, and Guillaume Bouchard. 2016. Com-
plex embeddings for simple link prediction. In In-
ternational Conference on Machine Learning, pages
2071–2080.


