



















































Transduction Recursive Auto-Associative Memory: Learning Bilingual Compositional Distributed Vector Representations of Inversion Transduction Grammars


Proceedings of SSST-8, Eighth Workshop on Syntax, Semantics and Structure in Statistical Translation, pages 112–121,
October 25, 2014, Doha, Qatar. c©2014 Association for Computational Linguistics

Transduction Recursive Auto-Associative Memory:
Learning Bilingual Compositional Distributed Vector Representations of

Inversion Transduction Grammars
Karteek Addanki Dekai Wu

HKUST
Human Language Technology Center

Department of Computer Science and Engineering
Hong Kong University of Science and Technology

{vskaddanki|dekai}@cs.ust.hk

Abstract

We introduce TRAAM, or Transduction
RAAM, a fully bilingual generalization
of Pollack’s (1990) monolingual Recur-
sive Auto-Associative Memory neural net-
work model, in which each distributed vec-
tor represents a bilingual constituent—i.e.,
an instance of a transduction rule, which
specifies a relation between two monolin-
gual constituents and how their subcon-
stituents should be permuted. Bilingual
terminals are special cases of bilingual
constituents, where a vector represents ei-
ther (1) a bilingual token —a token-to-
token or “word-to-word” translation rule
—or (2) a bilingual segment—a segment-
to-segment or “phrase-to-phrase” transla-
tion rule. TRAAMs have properties that
appear attractive for bilingual grammar in-
duction and statistical machine translation
applications. Training of TRAAM drives
both the autoencoder weights and the vec-
tor representations to evolve, such that
similar bilingual constituents tend to have
more similar vectors.

1 Introduction

We introduce Transduction RAAM—or TRAAM
for short—a recurrent neural network model that
generalizes the monolingual RAAM model of Pol-
lack (1990) to a distributed vector representation
of compositionally structured transduction gram-
mars (Aho and Ullman, 1972) that is fully bilingual
from top to bottom. In RAAM, which stands for
Recursive Auto-Associative Memory, using fea-
ture vectors to characterize constituents at every
level of a parse tree has the advantages that (1)
the entire context of all subtrees inside the con-
stituent can be efficiently captured in the feature
vectors, (2) the learned representations generalize

well because similar feature vectors represent sim-
ilar constituents or segments, and (3) representa-
tions can be automatically learned so as to max-
imize prediction accuracy for various tasks using
semi-supervised learning. We argue that different,
but analogous, properties are desirable for bilin-
gual structured translation models.

Unlike RAAM, where each distributed vector
represents a monolingual token or constituent,
each distributed vector in TRAAM represents a
bilingual constituent or biconstituent—that is, an
instance of a transduction rule, which asserts a re-
lation between two monolingual constituents, as
well as specifying how to permute their subcon-
stituents in translation. Bilingual terminals, or
biterminals, are special cases of biconstituents
where a vector represents either (1) a bitoken—a
token-to-token or “word-to-word” translation rule
—or (2) a bisegment—a segment-to-segment or
“phrase-to-phrase” translation rule.

The properties of TRAAMs are attractive for
machine translation applications. As with RAAM,
TRAAMs can be trained via backpropagation
training, which simultaneously evolves both the
autoencoder weights and the biconstituent vector
representations. As with RAAM, the evolution
of the vector representations within the hidden
layer performs automatic feature induction, and for
many applications can obviate the need for man-
ual feature engineering. However, the result is
that similar vectors tend to represent similar bicon-
stituents, rather than monolingual constituents.

The learned vector representations thus tend to
form clusters of similar translation relations in-
stead of merely similar strings. That is, TRAAM
clusters represent soft nonterminal categories of
cross-lingual relations and translation patterns, as
opposed to soft nonterminal categories of mono-
lingual strings as in RAAM.

Also, TRAAMs inherently make full simulta-
neous use of both input and output language fea-

112



tures, recursively, in an elegant integrated fash-
ion. TRAAM does not make restrictive a pri-
ori assumptions of conditional independence be-
tween input and output language features. When
evolving the biconstituent vector representations,
generalization occurs over similar input and out-
put structural characteristics simultaneously. In
most recurrent neural network applications to ma-
chine translation to date, only input side features
or only output language features are used. Even in
the few previous cases where recurrent neural net-
works have employed both input and output lan-
guage features for machine translation, the models
have typically been factored so that their recursive
portion is applied only to either the input or output
language, but not both.

As with RAAM, the objective criteria for train-
ing can be adjusted to reflect accuracy on nu-
merous different kinds of tasks, biasing the di-
rection that vector representations evolve toward.
But again, TRAAM’s learned vector representa-
tions support making predictions that simultane-
ously make use of both input and output struc-
tural characteristics. For example, TRAAM has
the ability to take into account the structure of
both input and output subtree characteristics while
making predictions on reordering them. Similarly,
for specific cross-lingual tasks such as word align-
ment, sense disambiguation, or machine transla-
tion, classifiers can simultaneously be trained in
conjunction with evolving the vector representa-
tions to optimize task-specific accuracy (Chris-
man, 1991).

In this paper we use as examples binary bi-
parse trees consistent with transduction grammars
in a 2-normal form, which by definition are in-
version transduction grammars (Wu, 1997) since
they are binary rank. This is not a requirement
for TRAAM, which in general can be formed for
transduction grammars of any rank. Moreover,
with distributed vector representations, the notion
of nonterminal categories in TRAAM is that of soft
membership, unlike in symbolically represented
transduction grammars. We start with bracketed
training data that contains no bilingual category
labels (like training data for Bracketing ITGs or
BITGs). Training results in self-organizing clus-
ters that have been automatically induced, repre-
senting soft nonterminal categories (unlike BITGs,
which do not have differentiated nonterminal cat-
egories).

2 Related work

TRAAM builds on different aspects of a spec-
trum of previous work. A large body of work ex-
ists on various different types of self-organizing
recurrent neural network approaches to model-
ing recursive structure, but mostly in monolin-
gual modeling. Even in applications to ma-
chine translation or cross-lingual modeling, the
typical practice has been to insert neural net-
work scoring components while still maintain-
ing older SMT modeling assumptions like bags-
of-words/phrases, “shake’n’bake” translation that
relies heavily on strong monolingual language
models, and log-linear models —in contrast to
TRAAM’s fully integrated bilingual approach.
Here we survey representative work across the
spectrum.

2.1 Monolingual related work

Distributed vector representations have long
been used for n-gram language modeling; these
continuous-valued models exploit the general-
ization capabilities of neural networks, although
there is no hidden contextual or hierarchical
structure as in RAAM. Schwenk (2010) applies
one such language model within an SMT system.

In the simple recurrent neural networks (RNNs
or SRNs) of Elman (1990), hidden layer represen-
tations are fed back to the input to dynamically rep-
resent an aggregate of the immediate contextual
history. More recently, the probabilistic NNLMs
of Bengio et al. (2003) and Bengio et al. (2009)
follow in this vein.

To represent hierarchical tree structure using
vector representations, one simple family of ap-
proaches employs convolutional networks, as in
Lee et al. (2009) for example. Collobert and We-
ston (2008) use a convolution neural network layer
quite effectively to learn vector representations for
words which are then used in a host of NLP tasks
such as POS tagging, chunking, and semantic role
labeling.

RAAM approaches, and related recursive au-
toencoder approaches, can be more flexible than
convolutional networks. Like SRNs, they can be
extended in numerous ways. The URAAM (Uni-
fication RAAM) model of Stolcke and Wu (1992)
extended RAAM to demonstrate the possibility of
using neural networks to perform more sophisti-
cated operations like unification directly upon the
distributed vector representations of hierarchical

113



feature structures. Socher et al. (2011) used mono-
lingual recursive autoencoders for sentiment pre-
diction, with or without parse tree information; this
was perhaps the first use of a RAAM style ap-
proach on a large scale NLP task, albeit mono-
lingual. Scheible and Schütze (2013) automat-
ically simplified the monolingual tree structures
generated by recursive autoencoders, validated the
simplified structures via manual evaluation, and
showed that sentiment classification accuracy is
not affected.

2.2 Bilingual related work
The majority of work on learning bilingual dis-
tributed vector representations has not made use of
recursive approaches or hidden contextual or com-
positional structure, as in the bilingual word em-
bedding learning of Klementiev et al. (2012) or the
bilingual phrase embedding learning of Gao et al.
(2014). Schwenk (2012) uses a non-recursive neu-
ral network to predict phrase translation probabil-
ities in conventional phrase-based SMT.

Attempts have been made to generalize the dis-
tributed vector representations of monolingual n-
gram language models, avoiding any hidden con-
textual or hierarchical structure. Working within
the framework of n-gram translation models, Son
et al. (2012) generalize left-to-right monolingual
n-gram models to bilingual n-grams, and study
bilingual variants of class-based n-grams. How-
ever, their model does not allow tackling the chal-
lenge of modeling cross-lingual constituent order,
as TRAAM does; instead it relies on the assump-
tion that some other preprocessor has already man-
aged to accurately re-order the words of the input
sentence into exactly the order of words in the out-
put sentence.

Similarly, generalizations of monolingual SRNs
to the bilingual case have been studied. Zou
et al. (2013) generalize the monolingual recur-
rent NNLM model of Bengio et al. (2009) to
learn bilingual word embeddings using conven-
tional SMT word alignments, and demonstrate that
the resulting embeddings outperform the baselines
in word semantic similarity. They also add a sin-
gle semantic similarity feature induced with bilin-
gual embeddings to a phrase-based SMT log-linear
model, and report improvements in BLEU. Com-
pared to TRAAM, however, they only learn non-
compositional features, with distributed vectors
only representing biterminals (as opposed to bi-
constituents or bilingual subtrees), and so other

mechanisms for combining biterminal scores still
need to be used to handle hierarchical structure,
as opposed to seamlessly being integrated into
the distributed vector representation model. De-
vlin et al. (2014) obtain translation accuracy im-
provements by extending the probabilistic NNLMs
of Bengio et al. (2003), which are used for the
output language, by adding input language con-
text features. Unlike TRAAM, neither of these
approaches symmetrically models the recursive
structure of both the input and output language
sides.

For convolutional network approaches, Kalch-
brenner and Blunsom (2013) use a recurrent prob-
abilistic model to generate a representation of the
source sentence and then generate the target sen-
tence from this representation. This use of in-
put language context to bias translation choices
is in some sense a neural network analogy to
the PSD (phrase sense disambiguation) approach
for context-dependent translation probabilities of
Carpuat and Wu (2007). Unlike TRAAM, the
model does not contain structural constraints, and
permutation of phrases must still be done in con-
ventional PBSMT “shake’n’bake” style by rely-
ing mostly on a language model (in their case, a
NNLM).

A few applications of monolingual RAAM-style
recursive autoencoders to bilingual tasks have also
appeared. For cross-lingual document classifica-
tion, Hermann and Blunsom (2014) use two sep-
arate monolingual fixed vector composition net-
works, one for each language. One provides the
training signal for the other, and training is only
on the embeddings.

Li et al. (2013) described a use of monolingual
recursive autoencoders within maximum entropy
ITGs. They replace their earlier model for pre-
dicting reordering based on the first and the last
tokens in a constituent, by instead using the con-
text vector generated using the recursive autoen-
coder. Only input language context is used, unlike
TRAAM which can use the input and output lan-
guage contexts equally.

Autoencoders have also been applied to SMT in
a very different way by Zhao et al. (2014) but with-
out recursion and not for learning distributed vec-
tor representations of words; rather, they used non-
recursive autoencoders to compress very high-
dimensional bilingual sparse features down to low-
dimensional feature vectors, so that MIRA or PRO

114



could be used to optimize the log-linear model
weights.

3 Representing transduction grammars
with TRAAM

As a recurrent neural network representation of a
transduction grammar, TRAAM learns bilingual
distributed representations that parallel the struc-
tural composition of a transduction grammar. As
with transduction grammars, the learned represen-
tations are symmetric and model structured rela-
tional correlations between the input and output
languages. The induced feature vectors in effect
represent soft categories of cross-lingual relations
and translations. The TRAAM model integrates
elegantly with the transduction grammar formal-
ism and aims to model the compositional struc-
ture of the transduction grammar as opposed to
incorporating external alignment information. It
is straightforward to formulate TRAAMs for arbi-
trary syntax directed transduction grammars; here
we shall describe an example of a TRAAM model
for an inversion transduction grammar (ITG).

Formally, an ITG is a tuple ⟨N, Σ,∆, R, S⟩,
where N is a finite nonempty set of nonterminal
symbols, Σ is a finite set of terminal symbols in L0,
∆ is a finite set of terminal symbols in  L1, R is a
finite nonempty set of inversion transduction rules
and S ∈ N is a designated start symbol. A normal-
form ITG consists of rules in one of the following
four forms:

S → A, A → [BC] , A → ⟨BC⟩, A → e/f

where S ∈ N is the start symbol, A, B,C ∈
N  are nonterminal symbols and e/f  is a biter-
minal. A biterminal is a pair of symbol strings:
Σ∗ ×∆∗, where at least one of the strings have to
be nonempty. The square and angled brackets sig-
nal straight and inverted order respectively. With
straight order, both the L0 and the L1 productions
are generated left-to-right, but with inverted order,
the L1 production is generated right-to-left.

In the distributed TRAAM representation of the
ITG, we represent each bispan, using a feature vec-
tor v of dimension d that represents a fuzzy encod-
ing of all the nonterminals that could generate it.
This is in contrast to the ITG model where each
nonterminal that generates a bispan has to be enu-
merated separately. Feature vectors correspond-
ing to larger bispans are compositionally generated
from smaller bispans using a compressor network

which takes two feature vectors of dimension d,
corresponding to the smaller bispans and gener-
ates the feature vector of dimension d correspond-
ing to the larger bispan. A single bit correspond-
ing to straight or inverted order is also fed as an
input to the compressor network. The compres-
sor network in TRAAM serves a similar role as
the syntactic rules in the symbolic ITG, but keeps
the encoding fuzzy. Figure 2 shows the straight
and inverted syntactic rules and the correspond-
ing inputs to the compressor network. Modeling
of unary rules (with start symbol on the left hand
side) although similar, is beyond the scope of this
paper.

It is easy to demonstrate that TRAAM mod-
els are capable of representing any symbolic ITG
model. All the nonterminals representing a bispan
can be encoded as a bit vector in the feature vector
of the bispan. Using the universal approximation
theorem of neural networks (Hornik et al., 1989),
an encoder with a single hidden layer can represent
any set of syntactic rules. Similarly, all TRAAM
models can be represented using a symbolic ITG
by assuming a unique nonterminal label for every
feature vector. Therefore, TRAAM and ITGs rep-
resent two equivalent classes of models for repre-
senting compositional bilingual relations.

It is important to note that although both
TRAAM and ITG models might be equivalent, the
fuzzy encoding of nonterminals in TRAAM is suit-
able for modeling the generalizations in bilingual
relations without exploding the search space unlike
the symbolic models. This property of TRAAM
makes it attractive for bilingual category learning
and machine translation applications as long as ap-
propriate language bias and objective functions are
determined.

Given our objective of inducing categories of
bilingual relations in an unsupervised manner, we
bias our TRAAM model by using a simple non-
linear activation function to be our compressor,
similar to the monolingual recursive autoencoder
model proposed by Socher et al. (2011). Having a
single layer in our compressor provides the neces-
sary language bias by forcing the network to cap-
ture the generalizations while reducing the dimen-
sions of the input vectors. We use tanh as the non-
linear activation function and the compressor ac-
cepts two vectors c1 and c2 of dimension d corre-
sponding to the nonterminals of the smaller bis-
pans and a single bit o corresponding to the in-

115



Figure 1: Example of English-Telugu biparse trees where inversion depends on output language sense.

Compressor 

Compressor 

Reconstructor 

Reconstructor 

o1#=#1# c1# c2# c3#

o2#=#(1#

p1#

o1'# c1'# c2'#

p2#

o2'# p1'# c3
'#

Figure 2: Architecture of TRAAM.

version order and generates a vector p of dimen-
sion d corresponding to the larger bispan generated
by combining the two smaller bispans as shown in
Figure 2. The vector p then serves as the input for
the successive combinations of the larger bispan
with other bispans.

p = tanh(W1[o; c1; c2] + b1) (1)

where W1 and b1 are the weight matrix and the bias
vector of the encoder network.

To ensure that the computed vector p captures
the fuzzy encodings of its children and the inver-
sion order, we use a reconstructor network which
attempts to reconstruct the inversion order and the
feature vectors corresponding of its children. We
use the error in reconstruction as our objective
function and train our model to minimize the re-
construction error over all the nodes in the biparse

tree. The reconstructor network in our TRAAM
model can be replaced by any other network that
enables the computed feature vector representa-
tions to be optimized for the given task. In our
current implementation, we reconstruct the inver-
sion order o′ and the child vectors c′1 and c

′
2 using

another nonlinear activation function as follows:

[o′; c′1; c
′
2] = tanh(W2p + b2) (2)

where W2 and b2 are the weight matrix and the bias
vector of the reconstructor network.

4 Bilingual training

4.1 Initialization
The weights and the biases of the compressor and
the reconstructor networks of the TRAAM model
are randomly initialized. Bisegment embeddings

116



corresponding to the leaf nodes (biterminals in the
symbolic ITG notation) in the biparse trees are also
initialized randomly. These constitute the model
parameters and are optimized to minimize our ob-
jective function of reconstruction error. The parse
trees for providing the structural constraints are
generated by a bracketing inversion transduction
grammar (BITG) induced in a purely unsupervised
fashion, according to the algorithm in Saers et al.
(2009). Due to constraints on the training time, we
consider only the Viterbi biparse trees according
to the BITG instead of all the biparse trees in the
forest.

4.2 Computing feature vectors
We compute the feature vectors at each internal
node in the biparse tree, similar to the feedforward
pass in a neural network. We topologically sort all
the nodes in the biparse tree and set the feature vec-
tor of each node in the topologically sorted order
as follows:

• If the node is a leaf node, the feature vector is
the corresponding bisegment embedding.

• Else, the biconstituent embedding corre-
sponding to the internal node is generated us-
ing the feature vectors of the children and the
inversion order using Equation 1. We also
normalize the length of the computed fea-
ture vector so as to prevent the network from
making the biconstituent embedding arbitrar-
ily small in magnitude (Socher et al., 2011).

4.3 Feature vector optimization
We train our current implementation of TRAAM,
by optimizing the model parameters to minimize
an objective function based on the reconstruction
error over all the nodes in the biparse trees. The
objective function is defined as a linear combina-
tion of the l2 norm of the reconstruction error of
the children and the cross-entropy loss of recon-
structing the inversion order. We define the error
at each internal node n as follows:

En =
α

2
|[c1; c2]− [c′1; c′2]|2 − (1− α)

[(1− o) log(1− o′) + (1 + o) log(1 + o′)]

where c1, c2, o correspond to the left child, right
child and inversion order, c′1, c

′
2, o

′ are the respec-
tive reconstructions and α is the linear weighting
factor. The global objective function J is the sum

of the error function at all internal nodes n in the bi-
parse trees averaged over the total number of sen-
tences T in the corpus. A regularization parameter
λ is used on the norm of the model parameters θ to
avoid overfitting.

J =
1
T

ΣnEn + λ||θ||2 (3)

As the bisegment embeddings are also a part of
the model parameters, the optimization objective
is similar to a moving target training objective Ro-
hwer (1990). We use backpropagation with struc-
ture Goller and Kuchler (1996) to compute the gra-
dients efficiently. L-BFGS algorithm Liu and No-
cedal (1989) is used in order to minimize the loss
function.

5 Bilingual representation learning

We expect the TRAAM model to generate clus-
ters over cross-lingual relations similar to RAAM
models on monolingual data. We test this hypoth-
esis by bilingually training our model using a par-
allel English-Telugu blocks world dataset. The
dataset is kept simple to better understand the na-
ture of clusters. Our dataset comprises of com-
mands which involves manipulating different col-
ored objects over different shapes.

5.1 Example

Figure 1 shows the biparse trees for two English-
Telugu sentence pairs. The preposition on in En-
glish translates to ౖనున (pinunna) and ౖన(pina) re-
spectively in the first and second sentence pairs be-
cause in the first sentence block is described by its
position on the square, whereas in the second sen-
tence block is the subject and square is the object.
Since Telugu is a language with an SOV structure,
the verbs ఉంచు(vunchu) and సు (teesuko) occur at
the end for both sentences.

The sentences in 1 illustrate the importance of
modeling bilingual relations simultaneously in-
stead of focusing only on the input or output lan-
guage as the cross-lingual structural relations are
sensitive to both the input and output language
context. For example, the constituent whose input
side is block on the square, the corresponding output
language tree structure is determined by whether
or not on is translated to ౖనున (pinunna) or ౖన(pina).

In symbolic frameworks such as ITGs, such
relations are encoded using different nontermi-
nal categories. However, inducing such cate-

117



Figure 3: Clustering of biconstituents in the Telugu-English data.

gories within a symbolic framework in an un-
supervised manner creates extremely challenging
combinatorial scaling issues. TRAAM models
are a promising approach for tackling this prob-
lem, since the vector representations learned us-
ing the TRAAM model inherently yield soft syn-
tactic category membership properties, despite be-
ing trained only with the unlabeled structural con-
straints of simple BITG-style data.

5.2 Biconstituent clustering

The soft membership properties of learned dis-
tributed vector representations can be explored
via cluster analysis. To illustrate, we trained a
TRAAM network bilingually using the algorithm
in Section 4, and obtained feature vector represen-
tations for each unique biconstituent. Clustering
the obtained feature vectors reveals emergence of
fuzzy nonterminal categories, as shown in Figure
3. It is important to note that each point in the
vector space corresponds to a tree-structured bi-
constituent as opposed to merely a flat bilingual
phrase, as same surface forms with different tree
structures will have different vectors.

As the full cluster tree is too unwieldy, Figure
4 zooms in to shows an enlarged version of a por-
tion of the clustering, along with the corresponding
bracketed bilingual structures. One can observe
that the cluster represents the biconstituents that
describe the object by its position on another ob-
ject. We can deduce this from the fact that only a

single sense of on/ ౖనున (pinnuna) seems to be occur-
ing in all the biconstituents of the cluster. Manual
inspection of other clusters reveals such similari-
ties despite noise expected to be introduced by the
sparsity of our dataset.

6 Conclusion

We have introduced a fully bilingual generaliza-
tion of Pollack’s (1990) monolingual Recursive
Auto-Associative Memory neural network model,
TRAAM, in which each distributed vector repre-
sents a bilingual constituent—i.e., an instance of
a transduction rule, which specifies a relation be-
tween two monolingual constituents and how their
subconstituents should be permuted. Bilingual ter-
minals are special cases of bilingual constituents,
where a vector represents either (1) a bilingual to-
ken—a token-to-token or “word-to-word” transla-
tion rule—or (2) a bilingual segment—a segment-
to-segment or “phrase-to-phrase” translation rule.

TRAAMs can be used for arbitrary rank SDTGs
(syntax-directed transduction grammars, a.k.a.
synchronous context-free grammars). Although
our discussions in this paper have focused on bi-
parse trees from SDTGs in a 2-normal form, which
by definition are ITGs due to the binary rank,
nothing prevents TRAAMs from being applied to
higher-rank transduction grammars.

We believe TRAAMs are worth detailed ex-
ploration as their intrinsic properties address key
problems in bilingual grammar induction and sta-

118



Figure 4: Typical zoomed view into the Telugu-English biconstituent clusters from Figure 3.

tistical machine translation —their sensitivity to
both input and output language context means that
the learned vector representations tend to reflect
the similarity of bilingual rather than monolingual
constituents, which is what is needed to induce dif-
ferentiated bilingual nonterminal categories.

7 Acknowledgments

This material is based upon work supported
in part by the Defense Advanced Research
Projects Agency (DARPA) under BOLT contract
nos. HR0011-12-C-0014 and HR0011-12-C-0016,
and GALE contract nos. HR0011-06-C-0022 and
HR0011-06-C-0023; by the European Union un-
der the FP7 grant agreement no. 287658; and by
the Hong Kong Research Grants Council (RGC)
research grants GRF620811, GRF621008, and
GRF612806. Any opinions, findings and conclu-
sions or recommendations expressed in this mate-
rial are those of the authors and do not necessarily
reflect the views of DARPA, the EU, or RGC.

References

Alfred V. Aho and Jeffrey D. Ullman. The The-
ory of Parsing, Translation, and Compiling.
Prentice-Halll, Englewood Cliffs, New Jersey,
1972.

Yoshua Bengio, Réjean Ducharme, Pascal Vin-
cent, and Christian Jauvin. A neural probabilis-
tic language model. Journal of Machine Learn-
ing Research, 3:1137–1155, 2003.

Yoshua Bengio, Jérôme Louradour, Ronan Col-
lobert, and Jason Weston. Curriculum learning.
In Proceedings of the 26th annual international
conference on machine learning, pages 41–48.
ACM, 2009.

Marine Carpuat and Dekai Wu. Context-
dependent phrasal translation lexicons for sta-
tistical machine translation. In 11th Machine
Translation Summit (MT Summit XI), pages 73–
80, 2007.

Lonnie Chrisman. Learning recursive distributed
representations for holistic computation. Con-
nection Science, 3(4):345–366, 1991.

Ronan Collobert and Jason Weston. A unified
architecture for natural language processing:
Deep neural networks with multitask learning.
In Proceedings of the 25th International Con-
ference on Machine Learning, ICML ’08, pages
160–167, New York, NY, USA, 2008. ACM.

Jacob Devlin, Rabih Zbib, Zhongqiang Huang,
Thomas Lamar, Richard Schwartz, and John
Makhoul. Fast and robust neural network joint
models for statistical machine translation. In

119



52nd Annual Meeting of the Association for
Computational Linguistics, 2014.

Jeffrey L Elman. Finding structure in time. Cog-
nitive science, 14(2):179–211, 1990.

Jianfeng Gao, Xiaodong He, Wen-tau Yih, and
Li Deng. Learning continuous phrase represen-
tations for translation modeling. In 52nd Annual
Meeting of the Association for Computational
Linguistics (Short Papers), 2014.

Christoph Goller and Andreas Kuchler. Learn-
ing task-dependent distributed representations
by backpropagation through structure. In Neu-
ral Networks, 1996., IEEE International Con-
ference on, volume 1, pages 347–352. IEEE,
1996.

Karl Moritz Hermann and Phil Blunsom. Multi-
lingual models for compositional distributed se-
mantics. In 52nd Annual Meeting of the Asso-
ciation for Computational Linguistics, volume
abs/1404.4641, 2014.

Kurt Hornik, Maxwell Stinchcombe, and Hal-
bert White. Multilayer feedforward networks
are universal approximators. Neural networks,
2(5):359–366, 1989.

Nal Kalchbrenner and Phil Blunsom. Recurrent
continuous translation models. In EMNLP,
pages 1700–1709, 2013.

Alexandre Klementiev, Ivan Titov, and Binod
Bhattarai. Inducing crosslingual distributed
representations of words. In 24th Interna-
tional Conference on Computational Linguistics
(COLING 2012). Citeseer, 2012.

Honglak Lee, Roger Grosse, Rajesh Ranganath,
and Andrew Y Ng. Convolutional deep be-
lief networks for scalable unsupervised learning
of hierarchical representations. In Proceedings
of the 26th Annual International Conference
on Machine Learning, pages 609–616. ACM,
2009.

Peng Li, Yang Liu, and Maosong Sun. Recur-
sive autoencoders for itg-based translation. In
EMNLP, pages 567–577, 2013.

Dong C Liu and Jorge Nocedal. On the limited
memory bfgs method for large scale optimiza-
tion. Mathematical programming, 45(1-3):503–
528, 1989.

Jordan B Pollack. Recursive distributed represen-
tations. Artificial Intelligence, 46(1):77–105,
1990.

Richard Rohwer. The “moving targets”training al-
gorithm. In Neural Networks, pages 100–109.
Springer, 1990.

Markus Saers, Joakim Nivre, and Dekai Wu.
Learning stochastic bracketing inversion trans-
duction grammars with a cubic time biparsing
algorithm. In 11th International Conference on
Parsing Technologies (IWPT’09), pages 29–32,
Paris, France, October 2009.

Christian Scheible and Hinrich Schütze. Cutting
recursive autoencoder trees. In 1st International
Conference on Learning Representations (ICLR
2013), Scottsdale, Arizona, May 2013.

Holger Schwenk. Continuous-space language
models for statistical machine translation. In
The Prague Bulletin of Mathematical Linguis-
tics, volume 93, pages 137–146, 2010.

Holger Schwenk. Continuous space transla-
tion models for phrase-based statistical machine
translation. In Proceedings of COLING 2012:
Posters, pages 1071––1080. Citeseer, 2012.

Richard Socher, Jeffrey Pennington, Eric H
Huang, Andrew Y Ng, and Christopher D Man-
ning. Semi-supervised recursive autoencoders
for predicting sentiment distributions. In Pro-
ceedings of the Conference on Empirical Meth-
ods in Natural Language Processing, pages
151–161. Association for Computational Lin-
guistics, 2011.

Le Hai Son, Alexandre Allauzen, and François
Yvon. Continuous space translation models with
neural networks. In Proceedings of the 2012
conference of the north american chapter of the
association for computational linguistics: Hu-
man language technologies, pages 39–48. As-
sociation for Computational Linguistics, 2012.

Andreas Stolcke and Dekai Wu. Tree match-
ing with recursive distributed representations.
In AAAI 1992 Workshop on Integrating Neu-
ral and Symbolic Processes—The Cognitive Di-
mension, 1992.

Dekai Wu. Stochastic inversion transduction
grammars and bilingual parsing of parallel cor-
pora. Computational Linguistics, 23(3):377–
403, 1997.

Bing Zhao, Yik-Cheung Tam, and Jing Zheng.
An autoencoder with bilingual sparse features
for improved statistical machine translation. In

120



IEEE International Conference on Acoustic,
Speech and Signal Processing (ICASSP), 2014.

Will Y Zou, Richard Socher, Daniel M Cer, and
Christopher D Manning. Bilingual word embed-
dings for phrase-based machine translation. In
EMNLP, pages 1393–1398, 2013.

121


