



















































The Annotated Transformer


Proceedings of Workshop for NLP Open Source Software, pages 52–60
Melbourne, Australia, July 20, 2018. c©2018 Association for Computational Linguistics

52

The Annotated Transformer

Alexander M. Rush
srush@seas.harvard.edu

Harvard University

Abstract

A major aim of open-source NLP is
to quickly and accurately reproduce
the results of new work, in a manner
that the community can easily use and
modify. While most papers publish
enough detail for replication, it still
may be difficult to achieve good re-
sults in practice. This paper is an ex-
periment. In it, I consider a worked
exercise with the goal of implement-
ing the results of the recent paper.
The replication exercise aims at sim-
ple code structure that follows closely
with the original work, while achiev-
ing an efficient usable system. An im-
plicit premise of this exercise is to en-
courage researchers to consider this
method for new results.

1 Introduction

Replication of published results remains a
challenging issue in open-source NLP. When
a new paper is published with major im-
provements, it is common for many mem-
bers of the community to independently re-
produce the numbers experimentally, which
is often a struggle. Practically this makes it
difficult to improve scores, but also impor-
tantly it is a pedagogical issue if students can-
not reproduce results from scientific publica-
tions.

The recent turn towards deep learning has
exerbated this issue. New models require
extensive hyperparameter tuning and long
training times. Small mistakes can cause ma-
jor issues. Fortunately though, new toolsets
have made it possible to write simpler more
mathematically declarative code.

In this experimental paper, I propose an ex-
ercise in open-source NLP. The goal is to tran-
scribe a recent paper into a simple and under-
standable form. The document itself is pre-
sented as an annotated paper. That is the
main document (in different font) is an ex-
cerpt of the recent paper “Attention is All You
Need” (Vaswani et al., 2017). I add annota-
tion in the form of italicized comments and
include code in PyTorch directly in the paper
itself.

Note this document itself is presented as a
blog post 1 and is completely executable as a
notebook. In the spirit of reproducibility this
work itself is distilled from the same source
with images inline.

1Presented at http://nlp.seas.harvard.
edu/2018/04/03/attention.html with source
code at https://github.com/harvardnlp/
annotated-transformer

http://nlp.seas.harvard.edu/2018/04/03/attention.html
http://nlp.seas.harvard.edu/2018/04/03/attention.html
https://github.com/harvardnlp/annotated-transformer
https://github.com/harvardnlp/annotated-transformer


53

2 Background

The goal of reducing sequential computa-
tion also forms the foundation of the Extended
Neural GPU, ByteNet and ConvS2S, all of
which use convolutional neural networks as
basic building block, computing hidden rep-
resentations in parallel for all input and out-
put positions. In these models, the number
of operations required to relate signals from
two arbitrary input or output positions grows
in the distance between positions, linearly
for ConvS2S and logarithmically for ByteNet.
This makes it more difficult to learn depen-
dencies between distant positions. In the
Transformer this is reduced to a constant
number of operations, albeit at the cost of
reduced effective resolution due to averag-
ing attention-weighted positions, an effect we
counteract with Multi-Head Attention.

Self-attention, sometimes called intra-
attention is an attention mechanism relating
different positions of a single sequence in or-
der to compute a representation of the se-
quence. Self-attention has been used suc-
cessfully in a variety of tasks including read-
ing comprehension, abstractive summariza-
tion, textual entailment and learning task-
independent sentence representations. End-
to-end memory networks are based on a re-
current attention mechanism instead of se-
quencealigned recurrence and have been
shown to perform well on simple-language
question answering and language modeling
tasks.

To the best of our knowledge, however, the
Transformer is the first transduction model re-
lying entirely on self-attention to compute rep-
resentations of its input and output without
using sequence aligned RNNs or convolution.

3 Model Architecture

Most competitive neural sequence trans-
duction models have an encoder-decoder
structure (Bahdanau et al., 2014). Here, the
encoder maps an input sequence of symbol
representations (x1, ..., xn) to a sequence of
continuous representations z = (z1, ..., zn).
Given z, the decoder then generates an out-
put sequence (y1, ..., ym) of symbols one el-
ement at a time. At each step the model

is auto-regressive (Graves, 2013), consum-
ing the previously generated symbols as ad-
ditional input when generating the next.

class EncoderDecoder(nn.Module):
"""
A standard Encoder-Decoder architecture.
Base for this and many other models.
"""
def __init__(self, encoder, decoder, src_embed,

tgt_embed, generator):
super(EncoderDecoder, self).__init__()
self.encoder = encoder
self.decoder = decoder
self.src_embed = src_embed
self.tgt_embed = tgt_embed
self.generator = generator

def forward(self, src, tgt, src_mask, tgt_mask):
"Take in and process masked src and target sequences."
return self.decode(self.encode(src, src_mask),

src_mask,
tgt, tgt_mask)

def encode(self, src, src_mask):
return self.encoder(self.src_embed(src), src_mask)

def decode(self, memory, src_mask, tgt, tgt_mask):
return self.decoder(self.tgt_embed(tgt), memory,

src_mask, tgt_mask)

class Generator(nn.Module):
"Define standard linear + softmax generation step."
def __init__(self, d_model, vocab):

super(Generator, self).__init__()
self.proj = nn.Linear(d_model, vocab)

def forward(self, x):
return F.log_softmax(self.proj(x), dim=-1)

The Transformer follows this overall archi-
tecture using stacked self-attention and point-
wise, fully connected layers for both the en-
coder and decoder, shown in the left and right
halves of Figure 1, respectively.



54

3.1 Encoder and Decoder Stacks

3.1.1 Encoder

The encoder is composed of a stack of N = 6
identical layers.

def clones(module, N):
"Produce N identical layers."
return nn.ModuleList([copy.deepcopy(module)

for _ in range(N)])

class Encoder(nn.Module):
"Core encoder is a stack of N layers"
def __init__(self, layer, N):

super(Encoder, self).__init__()
self.layers = clones(layer, N)
self.norm = LayerNorm(layer.size)

def forward(self, x, mask):
"Pass the input/mask through each layer in turn."
for layer in self.layers:

x = layer(x, mask)
return self.norm(x)

We employ a residual connection (He et al.,
2016) around each of the two sub-layers, fol-
lowed by layer normalization (Ba et al., 2016).

class LayerNorm(nn.Module):
"Construct a layernorm module (See citation for details)."
def __init__(self, features, eps=1e-6):

super(LayerNorm, self).__init__()
self.a_2 = nn.Parameter(torch.ones(features))
self.b_2 = nn.Parameter(torch.zeros(features))
self.eps = eps

def forward(self, x):
mean = x.mean(-1, keepdim=True)
std = x.std(-1, keepdim=True)
return (self.a_2 * (x - mean) /

(std + self.eps) + self.b_2)

That is, the output of each sub-layer
is LayerNorm(x + Sublayer(x)), where
Sublayer(x) is the function implemented
by the sub-layer itself. We apply dropout
(Srivastava et al., 2014) to the output of each
sub-layer, before it is added to the sub-layer
input and normalized.

To facilitate these residual connections, all
sub-layers in the model, as well as the em-
bedding layers, produce outputs of dimension
dmodel = 512.

class SublayerConnection(nn.Module):
"""
A layer norm followed by a residual connection.
Note norm is not applied to residual x.
"""
def __init__(self, size, dropout):

super(SublayerConnection, self).__init__()
self.norm = LayerNorm(size)
self.dropout = nn.Dropout(dropout)

def forward(self, x, sublayer):
"Apply residual connection to sublayer fn."
return x + self.dropout(sublayer(self.norm(x)))

Each layer has two sub-layers. The first is a
multi-head self-attention mechanism, and the
second is a simple, position-wise fully con-
nected feed-forward network.

class EncoderLayer(nn.Module):
"Encoder calls self-attn and feed forward."
def __init__(self, size, self_attn,

feed_forward, dropout):
super(EncoderLayer, self).__init__()
self.self_attn = self_attn
self.feed_forward = feed_forward
sublayer = SublayerConnection(size, dropout)
self.sublayer = clones(sublayer, 2)
self.size = size

def forward(self, x, mask):
"Follow Figure 1 (left) for connections."
x = self.sublayer[0](x, lambda x:

self.self_attn(x, x, x, mask))
return self.sublayer[1](x, self.feed_forward)

3.1.2 Decoder

The decoder is also composed of a stack of
N = 6 identical layers.

class Decoder(nn.Module):
"Generic N layer decoder with masking."
def __init__(self, layer, N):

super(Decoder, self).__init__()
self.layers = clones(layer, N)
self.norm = LayerNorm(layer.size)

def forward(self, x, memory, src_mask, tgt_mask):
for layer in self.layers:

x = layer(x, memory, src_mask, tgt_mask)
return self.norm(x)

In addition to the two sub-layers in each
encoder layer, the decoder inserts a third
sub-layer, which performs multi-head atten-
tion over the output of the encoder stack.
Similar to the encoder, we employ residual
connections around each of the sub-layers,
followed by layer normalization.

class DecoderLayer(nn.Module):
"Decoder calls self-attn, src-attn, and feed forward."
def __init__(self, size, self_attn,

src_attn, feed_forward, dropout):
super(DecoderLayer, self).__init__()
self.self_attn = self_attn
self.src_attn = src_attn
self.feed_forward = feed_forward
sublayer = SublayerConnection(size, dropout)
self.sublayer = clones(sublayer, 3)
self.size = size

def forward(self, x, memory, s_mask, t_mask):
"Follow Figure 1 (right) for connections."
m = memory
x = self.sublayer[0](x, lambda x:

self.self_attn(x, x, x, t_mask))
x = self.sublayer[1](x, lambda x:

self.src_attn(x, m, m, s_mask))
return self.sublayer[2](x, self.feed_forward)

We also modify the self-attention sub-layer
in the decoder stack to prevent positions
from attending to subsequent positions. This
masking, combined with fact that the output
embeddings are offset by one position, en-
sures that the predictions for position i can
depend only on the known outputs at posi-
tions less than i.

def subsequent_mask(size):
"Mask out subsequent positions."
attn_shape = (1, size, size)
subsequent_mask = np.triu(np.ones(attn_shape), k=1)
return torch.from_numpy(

subsequent_mask.astype('uint8')) == 0



55

3.1.3 Attention
An attention function can be described as
mapping a query and a set of key-value pairs
to an output, where the query, keys, values,
and output are all vectors. The output is com-
puted as a weighted sum of the values, where
the weight assigned to each value is com-
puted by a compatibility function of the query
with the corresponding key.

We call our particular attention "Scaled
Dot-Product Attention". The input consists of
queries and keys of dimension dk, and values
of dimension dv. We compute the dot prod-
ucts of the query with all keys, divide each by√

dk, and apply a softmax function to obtain
the weights on the values.

In practice, we compute the attention func-
tion on a set of queries simultaneously,
packed together into a matrix Q. The keys
and values are also packed together into ma-
trices K and V. We compute the matrix of
outputs as:

Attention(Q, K, V) = softmax(
QKT√

dk
)V

def attention(query, key, value, mask=None, dropout=None):
"Compute 'Scaled Dot Product Attention'"
d_k = query.size(-1)
key_t = key.transpose(-2, -1)
scores = torch.matmul(query, key_t) / math.sqrt(d_k)
if mask is not None:

scores = scores.masked_fill(mask == 0, -1e9)
p_attn = F.softmax(scores, dim=-1)
if dropout is not None:

p_attn = dropout(p_attn)
return torch.matmul(p_attn, value), p_attn

The two most commonly used attention
functions are additive attention (Bahdanau
et al., 2014), and dot-product (multiplicative)
attention. Dot-product attention is identical to
our algorithm, except for the scaling factor of

1√
dk

. Additive attention computes the com-
patibility function using a feed-forward net-
work with a single hidden layer. While the

two are similar in theoretical complexity, dot-
product attention is much faster and more
space-efficient in practice, since it can be im-
plemented using highly optimized matrix mul-
tiplication code.

While for small values of dk the two mech-
anisms perform similarly, additive attention
outperforms dot product attention without
scaling for larger values of dk (Britz et al.,
2017). We suspect that for large values of
dk, the dot products grow large in magni-
tude, pushing the softmax function into re-
gions where it has extremely small gradients
(To illustrate why the dot products get large,
assume that the components of q and k are
independent random variables with mean 0
and variance 1. Then their dot product, q · k =
∑dki=1 qiki, has mean 0 and variance dk.). To
counteract this effect, we scale the dot prod-
ucts by 1√dk .

Multi-head attention allows the model to
jointly attend to information from different
representation subspaces at different posi-
tions. With a single attention head, averaging
inhibits this.

MultiHead(Q, K, V) = Concat(head1, ..., headh)WO

where headi = Attention(QW
Q
i , KW

K
i , VW

V
i )

Where the projections are parameter ma-
trices WQi ∈ Rdmodel×dk , WKi ∈ Rdmodel×dk ,
WVi ∈ Rdmodel×dv and WO ∈ Rhdv×dmodel . In
this work we employ h = 8 parallel attention
layers, or heads. For each of these we use
dk = dv = dmodel/h = 64. Due to the reduced



56

dimension of each head, the total computa-
tional cost is similar to that of single-head at-
tention with full dimensionality.

class MultiHeadedAttention(nn.Module):
def __init__(self, h, d_model, dropout=0.1):

"Take in model size and number of heads."
super(MultiHeadedAttention, self).__init__()
assert d_model % h == 0
# We assume d_v always equals d_k
self.d_k = d_model // h
self.h = h
self.linears = clones(nn.Linear(d_model, d_model), 4)
self.attn = None
self.dropout = nn.Dropout(p=dropout)

def forward(self, query, key, value, mask=None):
"Implements Figure 2"
if mask is not None:

# Same mask applied to all h heads.
mask = mask.unsqueeze(1)

nb = query.size(0)

# 1) Do all the linear projections in batch from d_model => h x d_k
query, key, value = [

l(x).view(nb, -1, self.h, self.d_k).transpose(1, 2)
for l, x in zip(self.linears, (query, key, value))]

# 2) Apply attention on all the projected vectors in batch.
x, self.attn = attention(query, key, value, mask=mask,

dropout=self.dropout)

# 3) "Concat" using a view and apply a final linear.
x = x.transpose(1, 2).contiguous().view(

nb, -1, self.h * self.d_k)
return self.linears[-1](x)

3.2 Position-wise Feed-Forward Networks

In addition to attention sub-layers, each of
the layers in our encoder and decoder con-
tains a fully connected feed-forward network,
which is applied to each position separately
and identically. This consists of two linear
transformations with a ReLU activation in be-
tween.

FFN(x) = max(0, xW1 + b1)W2 + b2

While the linear transformations are the same
across different positions, they use different
parameters from layer to layer. Another way
of describing this is as two convolutions with
kernel size 1. The dimensionality of input and
output is dmodel = 512, and the inner-layer has
dimensionality d f f = 2048.

class PositionwiseFeedForward(nn.Module):
"Implements FFN equation."
def __init__(self, d_model, d_ff, dropout=0.1):

super(PositionwiseFeedForward, self).__init__()
self.w_1 = nn.Linear(d_model, d_ff)
self.w_2 = nn.Linear(d_ff, d_model)
self.dropout = nn.Dropout(dropout)

def forward(self, x):
return self.w_2(self.dropout(F.relu(self.w_1(x))))

3.3 Embeddings and Softmax

Similarly to other sequence transduction
models, we use learned embeddings to con-
vert the input tokens and output tokens to
vectors of dimension dmodel. We also use

the usual learned linear transformation and
softmax function to convert the decoder out-
put to predicted next-token probabilities. In
our model, we share the same weight ma-
trix between the two embedding layers and
the pre-softmax linear transformation, similar
to (Press and Wolf, 2016). In the embedding
layers, we multiply those weights by

√
dmodel.

class Embeddings(nn.Module):
def __init__(self, d_model, vocab):

super(Embeddings, self).__init__()
self.lut = nn.Embedding(vocab, d_model)
self.d_model = d_model

def forward(self, x):
return self.lut(x) * math.sqrt(self.d_model)

3.4 Positional Encoding

Since our model contains no recurrence and
no convolution, in order for the model to make
use of the order of the sequence, we must in-
ject some information about the relative or ab-
solute position of the tokens in the sequence.
To this end, we add "positional encodings" to
the input embeddings at the bottoms of the
encoder and decoder stacks. The positional
encodings have the same dimension dmodel
as the embeddings, so that the two can be
summed. There are many choices of posi-
tional encodings, learned and fixed (Gehring
et al., 2017).

In this work, we use sine and cosine func-
tions of different frequencies:

PE(pos,2i) = sin(pos/10000
2i/dmodel)

PE(pos,2i+1) = cos(pos/10000
2i/dmodel)

where pos is the position and i is the dimen-
sion. That is, each dimension of the posi-
tional encoding corresponds to a sinusoid.
The wavelengths form a geometric progres-
sion from 2π to 10000 · 2π. We chose this
function because we hypothesized it would
allow the model to easily learn to attend by
relative positions, since for any fixed offset k,
PEpos+k can be represented as a linear func-
tion of PEpos.

In addition, we apply dropout to the sums of
the embeddings and the positional encodings
in both the encoder and decoder stacks. For
the base model, we use a rate of Pdrop = 0.1.



57

class PositionalEncoding(nn.Module):
"Implement the PE function."
def __init__(self, d_model, dropout, max_len=5000):

super(PositionalEncoding, self).__init__()
self.dropout = nn.Dropout(p=dropout)

# Compute the positional encodings once in log space.
pe = torch.zeros(max_len, d_model)
position = torch.arange(0, max_len).unsqueeze(1)
div_term = torch.exp(torch.arange(0, d_model, 2) *

-(math.log(10000.0) / d_model))
pe[:, 0::2] = torch.sin(position * div_term)
pe[:, 1::2] = torch.cos(position * div_term)
pe = pe.unsqueeze(0)
self.register_buffer('pe', pe)

def forward(self, x):
x = x + Variable(self.pe[:, :x.size(1)],

requires_grad=False)
return self.dropout(x)

plt.figure(figsize=(15, 5))
pe = PositionalEncoding(20, 0)
y = pe.forward(Variable(torch.zeros(1, 100, 20)))
plt.plot(np.arange(100), y[0, :, 4:8].data.numpy())
plt.legend(["dim %d" % p for p in [4, 5, 6, 7]])
None

We also experimented with using learned
positional embeddings (Gehring et al., 2017)
instead, and found that the two versions pro-
duced nearly identical results. We chose
the sinusoidal version because it may al-
low the model to extrapolate to sequence
lengths longer than the ones encountered
during training.

def make_model(src_vocab, tgt_vocab, N=6,
d_model=512, d_ff=2048, h=8, dropout=0.1):

"Helper: Construct a model from hyperparameters."
c = copy.deepcopy
attn = MultiHeadedAttention(h, d_model)
ff = PositionwiseFeedForward(d_model, d_ff, dropout)
position = PositionalEncoding(d_model, dropout)
d = d_model
model = EncoderDecoder(

Encoder(EncoderLayer(d, c(attn), c(ff), dropout), N),
Decoder(DecoderLayer(d, c(attn), c(attn),

c(ff), dropout), N),
nn.Sequential(Embeddings(d, src_vocab), c(position)),
nn.Sequential(Embeddings(d, tgt_vocab), c(position)),
Generator(d_model, tgt_vocab))

# This was important from their code.
# Initialize parameters with Glorot / fan_avg.
for p in model.parameters():

if p.dim() > 1:
nn.init.xavier_uniform(p)

return model

4 Training

This section describes the training regime for
our models.

4.1 Batches and Masking
class Batch:

"Batch of data with mask for training."
def __init__(self, src, trg=None, pad=0):

self.src = src
self.src_mask = (src != pad).unsqueeze(-2)
if trg is not None:

self.trg = trg[:, :-1]
self.trg_y = trg[:, 1:]
self.trg_mask = self.make_std_mask(self.trg, pad)
self.ntokens = (self.trg_y != pad).data.sum()

@staticmethod
def make_std_mask(tgt, pad):

"Create a mask to hide padding and future words."
tgt_mask = (tgt != pad).unsqueeze(-2)
tgt_mask = tgt_mask & Variable(

subsequent_mask(tgt.size(-1))
.type_as(tgt_mask.data))

return tgt_mask

4.2 Training Loop
def run_epoch(data_iter, model, loss_compute):

"Standard Training and Logging Function"
start = time.time()
total_tokens = 0
total_loss = 0
tokens = 0
for i, batch in enumerate(data_iter):

out = model.forward(batch.src, batch.trg,
batch.src_mask, batch.trg_mask)

loss = loss_compute(out, batch.trg_y, batch.ntokens)
total_loss += loss
total_tokens += batch.ntokens
tokens += batch.ntokens
if i % 50 == 1:

elapsed = time.time() - start
print("Epoch Step: %d Loss: %f Tokens / Sec: %f" %

(i, loss / batch.ntokens, tokens / elapsed))
start = time.time()
tokens = 0

return total_loss / total_tokens

4.3 Training Data and Batching

We trained on the standard WMT 2014
English-German dataset consisting of about
4.5 million sentence pairs. Sentences were
encoded using byte-pair encoding, which has
a shared source-target vocabulary of about
37000 tokens. For English-French, we used
the significantly larger WMT 2014 English-
French dataset consisting of 36M sentences
and split tokens into a 32000 word-piece vo-
cabulary.

Sentence pairs were batched together by
approximate sequence length. Each training
batch contained a set of sentence pairs con-
taining approximately 25000 source tokens
and 25000 target tokens.

global max_src_in_batch, max_tgt_in_batch
def batch_size_fn(new, count, sofar):

"Calculate total number of tokens + padding."
global max_src_in_batch, max_tgt_in_batch
if count == 1:

max_src_in_batch = 0
max_tgt_in_batch = 0

max_src_in_batch = max(max_src_in_batch,
len(new.src))

max_tgt_in_batch = max(max_tgt_in_batch,
len(new.trg) + 2)

src_elements = count * max_src_in_batch
tgt_elements = count * max_tgt_in_batch
return max(src_elements, tgt_elements)

4.4 Hardware and Schedule

We trained our models on one machine with 8
NVIDIA P100 GPUs. For our base models us-
ing the hyperparameters described through-



58

out the paper, each training step took about
0.4 seconds. We trained the base models for
a total of 100,000 steps or 12 hours. For our
big models, step time was 1.0 seconds. The
big models were trained for 300,000 steps
(3.5 days).

4.5 Optimizer

We used the Adam optimizer (Kingma and
Ba, 2014) with β1 = 0.9, β2 = 0.98 and
e = 10−9. We varied the learning rate over
the course of training, according to the for-
mula:

lrate = d−0.5model·

min(step_num−0.5, step_num ·warmup_steps−1.5)

This corresponds to increasing the learning
rate linearly for the first warmup_steps train-
ing steps, and decreasing it thereafter propor-
tionally to the inverse square root of the step
number. We used warmup_steps = 4000.

class NoamOpt:
"Optim wrapper that implements rate."
def __init__(self, model_size, factor,

warmup, optimizer):
self.optimizer = optimizer
self._step = 0
self.warmup = warmup
self.factor = factor
self.model_size = model_size
self._rate = 0

def step(self):
"Update parameters and rate"
self._step += 1
rate = self.rate()
for p in self.optimizer.param_groups:

p['lr'] = rate
self._rate = rate
self.optimizer.step()

def rate(self, step=None):
"Implement `lrate` above"
if step is None:

step = self._step
return self.factor * (

self.model_size ** (-0.5) *
min(step ** (-0.5), step * self.warmup ** (-1.5)))

def get_std_opt(model):
return NoamOpt(model.src_embed[0].d_model, 2, 4000,

torch.optim.Adam(model.parameters(),
lr=0,
betas=(0.9, 0.98),
eps=1e-9))

# Three settings of the lrate hyperparameters.
opts = [NoamOpt(512, 1, 4000, None),

NoamOpt(512, 1, 8000, None),
NoamOpt(256, 1, 4000, None)]

plt.plot(np.arange(1, 20000), [[opt.rate(i) for opt in opts]
for i in range(1, 20000)])

plt.legend(["512:4000", "512:8000", "256:4000"])
None

4.6 Regularization

4.6.1 Label Smoothing

During training, we employed label smooth-
ing of value els = 0.1 (Szegedy et al., 2015).
This hurts perplexity, as the model learns to
be more unsure, but improves accuracy and
BLEU score.

class LabelSmoothing(nn.Module):
"Implement label smoothing."
def __init__(self, size, padding_idx, smoothing=0.0):

super(LabelSmoothing, self).__init__()
self.criterion = nn.KLDivLoss(size_average=False)
self.padding_idx = padding_idx
self.confidence = 1.0 - smoothing
self.smoothing = smoothing
self.size = size
self.true_dist = None

def forward(self, x, target):
assert x.size(1) == self.size
true_dist = x.data.clone()
true_dist.fill_(self.smoothing / (self.size - 2))
true_dist.scatter_(1, target.data.unsqueeze(1),

self.confidence)
true_dist[:, self.padding_idx] = 0
mask = torch.nonzero(target.data == self.padding_idx)
if mask.dim() > 0:

true_dist.index_fill_(0, mask.squeeze(), 0.0)
self.true_dist = true_dist
return self.criterion(x,

Variable(true_dist,
requires_grad=False))

#Example of label smoothing.
crit = LabelSmoothing(5, 0, 0.4)
predict = torch.FloatTensor(

[[0, 0.2, 0.7, 0.1, 0],
[0, 0.2, 0.7, 0.1, 0],
[0, 0.2, 0.7, 0.1, 0]])

v = crit(Variable(predict.log()),
Variable(torch.LongTensor([2, 1, 0])))

# Show the target distributions expected by the system.
plt.imshow(crit.true_dist)
None



59

crit = LabelSmoothing(5, 0, 0.1)
def loss(x):

d = x + 3 * 1
predict = torch.FloatTensor([[0, x / d, 1 / d,

1 / d, 1 / d]])
return crit(Variable(predict.log()),

Variable(torch.LongTensor([1]))).data[0]
plt.plot(np.arange(1, 100),

[loss(x) for x in range(1, 100)])
None

4.7 Loss Computation
class SimpleLossCompute:

"A simple loss compute and train function."
def __init__(self, generator,

criterion, opt=None):
self.generator = generator
self.criterion = criterion
self.opt = opt

def __call__(self, x, y, norm):
x = self.generator(x)
loss = self.criterion(

x.contiguous().view(-1, x.size(-1)),
y.contiguous().view(-1)) / norm

loss.backward()
if self.opt is not None:

self.opt.step()
self.opt.optimizer.zero_grad()

return loss.data[0] * norm

5 Decoding and Visualization

5.1 Greedy Decoding
def greedy_decode(model, src, src_mask,

max_len, start_sym):
memory = model.encode(src, src_mask)
ys = torch.ones(1, 1).fill_(start_sym).type_as(src.data)
for i in range(max_len - 1):

out = model.decode(memory, src_mask,
Variable(ys),
Variable(

subsequent_mask(ys.size(1))
.type_as(src.data)))

prob = model.generator(out[:, -1])
_, next_word = torch.max(prob, dim=1)
next_word = next_word.data[0]
ys = torch.cat([ys,

torch.ones(1, 1)
.type_as(src.data)
.fill_(next_word)],

dim=1)
return ys

model.eval()
sent = """@@@The @@@log @@@file @@@can @@@be @@@sent @@@secret ly

@@@with @@@email @@@or @@@FTP @@@to @@@a @@@specified
@@@receiver""".split()

src = torch.LongTensor([[SRC.stoi[w] for w in sent]])
src = Variable(src)
src_mask = (src != SRC.stoi["<blank>"]).unsqueeze(-2)
out = greedy_decode(model, src, src_mask,

max_len=60,
start_symbol=TGT.stoi["<s>"])

print("Translation:", end="\t")
trans = "<s> "
for i in range(1, out.size(1)):

sym = TGT.itos[out[0, i]]
if sym == "</s>":

break
trans += sym + " "

print(trans)

5.2 Attention Visualization
tgt_sent = trans.split()
def draw(data, x, y, ax):

seaborn.heatmap(data,
xticklabels=x, square=True,
yticklabels=y, vmin=0.0, vmax=1.0,
cbar=False, ax=ax)

for layer_num in range(1, 6, 2):
fig, axs = plt.subplots(1, 4, figsize=(20, 10))
print("Encoder Layer", layer_num + 1)
layer = model.encoder.layers[layer_num]
for h in range(4):

draw(layer.self_attn.attn[0, h].data,
sent, sent if h == 0 else [], ax=axs[h])

plt.show()

for layer_num in range(1, 6, 2):
fig, axs = plt.subplots(1, 4, figsize=(20, 10))
print("Decoder Self Layer", layer_num + 1)
layer = model.decoder.layers[layer_num]
for h in range(4):

draw(layer.self_attn.attn[0, h]
.data[:len(tgt_sent), :len(tgt_sent)],
tgt_sent, tgt_sent if h == 0 else [], ax=axs[h])

plt.show()
print("Decoder Src Layer", layer_num + 1)
fig, axs = plt.subplots(1, 4, figsize=(20, 10))
for h in range(4):

draw(layer.src_attn.attn[0, h].data[
:len(tgt_sent), :len(sent)],
sent, tgt_sent if h == 0 else [], ax=axs[h])

plt.show()

6 Conclusion

This paper presents a replication exercise of
the transformer network. Consult the full on-
line version for features such as multi-gpu
training, real experiments on full translation
problems, and pointers to other extensions
such as beam search, sub-word models, and
model averaging. The goal is to explore a lit-
erate programming experiment of interleav-
ing model replication with formal writing.
While not always possible, this modality can
be useful for transmitting ideas and encour-
aging faster open-source uptake. Addition-
ally this method can be an easy way to learn
about a model alongside its implementation.



60

References
Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E

Hinton. 2016. Layer normalization. arXiv
preprint arXiv:1607.06450.

Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua
Bengio. 2014. Neural machine translation by
jointly learning to align and translate. CoRR,
abs/1409.0473.

Denny Britz, Anna Goldie, Minh-Thang Luong,
and Quoc V. Le. 2017. Massive exploration
of neural machine translation architectures.
CoRR, abs/1703.03906.

Jonas Gehring, Michael Auli, David Grangier, De-
nis Yarats, and Yann N. Dauphin. 2017. Convo-
lutional sequence to sequence learning. CoRR,
abs/1705.03122.

Alex Graves. 2013. Generating sequences with re-
current neural networks. CoRR, abs/1308.0850.

Kaiming He, Xiangyu Zhang, Shaoqing Ren, and
Jian Sun. 2016. Deep residual learning for im-
age recognition. In Proceedings of the IEEE con-
ference on computer vision and pattern recognition,
pages 770–778.

Diederik P. Kingma and Jimmy Ba. 2014. Adam:
A method for stochastic optimization. CoRR,
abs/1412.6980.

Ofir Press and Lior Wolf. 2016. Using the out-
put embedding to improve language models.
CoRR, abs/1608.05859.

Nitish Srivastava, Geoffrey Hinton, Alex
Krizhevsky, Ilya Sutskever, and Ruslan
Salakhutdinov. 2014. Dropout: A simple
way to prevent neural networks from overfit-
ting. The Journal of Machine Learning Research,
15(1):1929–1958.

Christian Szegedy, Vincent Vanhoucke, Sergey
Ioffe, Jonathon Shlens, and Zbigniew Wojna.
2015. Rethinking the inception architecture for
computer vision. CoRR, abs/1512.00567.

Ashish Vaswani, Noam Shazeer, Niki Parmar,
Jakob Uszkoreit, Llion Jones, Aidan N. Gomez,
Lukasz Kaiser, and Illia Polosukhin. 2017. At-
tention is all you need. CoRR, abs/1706.03762.

http://arxiv.org/abs/1409.0473
http://arxiv.org/abs/1409.0473
http://arxiv.org/abs/1703.03906
http://arxiv.org/abs/1703.03906
http://arxiv.org/abs/1705.03122
http://arxiv.org/abs/1705.03122
http://arxiv.org/abs/1308.0850
http://arxiv.org/abs/1308.0850
http://arxiv.org/abs/1412.6980
http://arxiv.org/abs/1412.6980
http://arxiv.org/abs/1608.05859
http://arxiv.org/abs/1608.05859
http://arxiv.org/abs/1512.00567
http://arxiv.org/abs/1512.00567
http://arxiv.org/abs/1706.03762
http://arxiv.org/abs/1706.03762

