



















































Unsupervised Learning of Discourse-Aware Text Representation for Essay Scoring


Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics: Student Research Workshop, pages 378–385
Florence, Italy, July 28 - August 2, 2019. c©2019 Association for Computational Linguistics

378

Unsupervised Learning of
Discourse-Aware Text Representation for Essay Scoring

Farjana Sultana Mim1 Naoya Inoue1,2 Paul Reisert2,1
Hiroki Ouchi2,1 Kentaro Inui1,2

1Tohoku University 2RIKEN Center for Advanced Intelligence Project (AIP)
{mim, naoya-i, inui} @ecei.tohoku.ac.jp
{paul.reisert, hiroki.ouchi} @riken.jp

Abstract

Existing document embedding approaches
mainly focus on capturing sequences of words
in documents. However, some document clas-
sification and regression tasks such as essay
scoring need to consider discourse structure of
documents. Although some prior approaches
consider this issue and utilize discourse struc-
ture of text for document classification, these
approaches are dependent on computationally
expensive parsers. In this paper, we propose
an unsupervised approach to capture discourse
structure in terms of coherence and cohesion
for document embedding that does not require
any expensive parser or annotation. Extrin-
sic evaluation results show that the document
representation obtained from our approach im-
proves the performance of essay Organization
scoring and Argument Strength scoring.

1 Introduction

Document embedding is important for many NLP
tasks such as document classification (e.g., es-
say scoring and sentiment classification) (Le and
Mikolov, 2014; Liu et al., 2017; Wu et al.,
2018; Tang et al., 2015) and summarization.
While embedding approaches can be supervised,
semi-supervised and unsupervised, recent studies
have largely focused on unsupervised and semi-
supervised approaches in order to utilize large
amounts of unlabeled text and avoid expensive an-
notation procedures.

In general, a document is a discourse where
sentences are logically connected to each other to
provide comprehensive meaning. Discourse has
two important properties: coherence and cohesion
(Halliday, 1994). Coherence refers to the seman-
tic relatedness among sentences and logical order
of concepts and meanings in a text. For example,
“I saw Jill on the street. She was going home.” is
coherent whereas “I saw Jill on the street. She has
two sisters.” is incoherent. Cohesion refers to the

use of linguistic devices that hold a text together.
Example of these linguistic devices include con-
junctions such as discourse indicators (DIs) (e.g.,
“because” and “for example”), coreference (e.g.,
“he” and “they”), substitution, ellipsis etc.

Some text classification and regression tasks
need to consider discourse structure of text in
addition to dependency relations and predicate-
argument structures. One example of such tasks
is essay scoring, where discourse structure (e.g.,
coherence and cohesion) plays a crucial role, es-
pecially when considering Organization and Argu-
ment Strength criteria, since they refer to logical-
sequence awareness in texts. Organization refers
to how good an essay structure is, where well-
structured essays logically develop arguments and
state positions by supporting them (Persing et al.,
2010). Argument Strength means how strongly an
essay argues in favor of its thesis to persuade the
readers (Persing and Ng, 2015).

An example of the relation between coherence
and an essay’s Organization is shown in Figure 1.
The high-scored essay (i.e., Organization score of
4) first states its position regarding the prompt
and then provides several reasons to strengthen the
claim. It is considered coherent because it follows
a logical order. However, the low-scored essay
is not clear on its position and what it is arguing
about. Therefore, it can be considered incoherent
since it lacks logical sequencing.

Previous studies on document embedding have
primarily focused on capturing word similarity,
word dependencies and semantic information of
documents (Le and Mikolov, 2014; Liu et al.,
2017; Wu et al., 2018; Tang et al., 2015). How-
ever, less attention has been paid to capturing dis-
course structure for document embedding in an
unsupervised manner and no prior work applies
unsupervised document representation learning to
essay scoring. In short, it has not yet been ex-
plored how some of the discourse properties can



379

Coherent Essay: Organization Score = 4 Incoherent Essay: Organization Score = 2.5
There is no doubt in the fact that we live under the full reign of
science, technology and industrialization. Our lives are
dominated by them in every aspect. …………… In other words,
what I am trying to say more figuratively is that in our world
of science, technology and industrialization there is no really
place for dreaming and imagination.

One of the reasons for the disappearing of the dreams and the
imagination from our life is one that I really regret to mention,
that is the lack of time. We are really pressed for time
nowadays …………

The world we are living in is without any doubt a modern
and civilized one. It is not like the world five hundred years
ago, it is not even like the one fifty years ago. Perhaps we -
the people who live nowadays, are happier than our
ancestors, but perhapswe are not.

The strange thing is that we judge and analyse their world
without knowing it and maybe without trying to know it.
The only thing that is certain is that the world is changing
and it is changing so fast that even we cannot notice it.
Sciece has developed to such an extent that it is difficult to
believe this can be true. …………

Prompt: Some people say that in our modern world , dominated by science, technology and 
industrialization, there is no longer a place for dreaming and imagination. What is your opinion? 

Figure 1: Example of coherent and incoherent ICLE essays with their Organization score.

be included in text embedding without an expen-
sive parser and how document embeddings affect
essay scoring tasks.

In this paper, we propose an unsupervised
method to capture discourse structure in terms
of cohesion and coherence for document embed-
ding. We train a document encoder with unla-
beled data which learns to discriminate between
coherent/cohesive and incoherent/incohesive doc-
uments. We then use the pre-trained document en-
coder to obtain feature vectors of essays for Orga-
nization and Argument Strength score prediction,
where the feature vectors are mapped to scores by
regression. The advantage of our approach is that
it is fully unsupervised and does not require any
expensive parser or annotation. Our results show
that capturing discourse structure in terms of co-
hesion and coherence for document representation
helps to improve the performance of essay Orga-
nization scoring and Argument Strength scoring.
We make our implementation publicly available.1

2 Related Work

The focus of this study is the unsupervised en-
capsulation of discourse structure (coherence and
cohesion) into document representation for es-
say scoring. A popular approach for document
representation is the use of fixed-length features
such as bag-of-words (BOW) and bag-of-ngrams
due to their simplicity and highly competitive re-
sults (Wang and Manning, 2012). However, such
approaches fail to capture the semantic similarity
of words and phrases since they treat each word or

1Our implementation is publicly available at
https://github.com/FarjanaSultanaMim/
DiscoShuffle

phrase as a discrete token.

Several methods for document representation
learning have been introduced in recent years. One
popular unsupervised method is doc2vec (Le and
Mikolov, 2014), where a document is mapped to a
unique vector and every word in the document is
also mapped to a unique vector. Then, the docu-
ment vector and and word vectors are either con-
catenated or averaged to predict the next word
in a context. Liu et al. (2017) used a convo-
lutional neural network (CNN) to capture longer
range semantic structure within a document where
the learning objective predicted the next word.
Wu et al. (2018) proposed Word Mover’s Em-
bedding (WME) utilizing Word Mover’s Distance
(WMD) that considers both word alignments and
pre-trained word vectors to learn feature represen-
tation of documents. Tang et al. (2015) proposed
a semi-supervised method called Predictive Text
Embedding (PTE) where both labeled information
and different levels of word co-occurrence were
encoded in a large-scale heterogeneous text net-
work, which was then embedded into a low di-
mensional space. Although these approaches have
been proven useful for several document classifi-
cation and regression tasks, their focus is not on
capturing the discourse structure of documents.

One exception is the study by Ji and Smith
(2017) who illustrated the role of discourse struc-
ture for document representation by implement-
ing a discourse structure (defined by RST) aware
model and showed that their model improves text
categorization performance (e.g., sentiment clas-
sification of movies and Yelp reviews, and predic-
tion of news article frames). The authors utilized
an RST-parser to obtain the discourse dependency

https://github.com/FarjanaSultanaMim/DiscoShuffle
https://github.com/FarjanaSultanaMim/DiscoShuffle


380

tree of a document and then built a recursive neu-
ral network on top of it. The issue with their ap-
proach is that texts need to be parsed by an RST
parser which is computationally expensive. Fur-
thermore, the performance of RST parsing is de-
pendent on the genre of documents (Ji and Smith,
2017).

Previous studies have modeled text coher-
ence (Li and Jurafsky, 2016; Joty et al., 2018;
Mesgar and Strube, 2018). Farag et al. (2018)
demonstrated that state-of-the-art neural auto-
mated essay scoring (AES) is not well-suited for
capturing adversarial input of grammatically cor-
rect but incoherent sequences of sentences. There-
fore, they developed a neural local coherence
model and jointly trained it with a state-of-the-art
AES model to build an adversarially robust AES
system. Mesgar and Strube (2018) used a local
coherence model to assess essay scoring perfor-
mance on a dataset of holistic scores where it is
unclear which criteria of the essay the score con-
siders.

We target Organization and Argument Strength
dimension of essays which are related to coher-
ence and cohesion. Persing et al. (2010) pro-
posed heuristic rules utilizing various DIs, words
and phrases to capture the organizational struc-
ture of texts. Persing and Ng (2015) used sev-
eral features such as part-of-speech, n-grams, se-
mantic frames, coreference, and argument com-
ponents for calculating Argument Strength in es-
says. Wachsmuth et al. (2016) achieved state-
of-the-art performance on Organization and Argu-
ment Strength scoring of essays by utilizing argu-
mentative features such as sequence of argumen-
tative discourse units (e.g., (conclusion, premise,
conclusion)). However, Wachsmuth et al. (2016)
used an expensive argument parser to obtain such
units.

3 Base Model

3.1 Overview

Our base model consists of (i) a base document
encoder, (ii) auxiliary encoders, and (iii) a scor-
ing function. The base document encoder pro-
duces a vector representation hbase by capturing
a sequence of words in each essay. The auxiliary
encoders capture additional essay-related informa-
tion that is useful for essay scoring and produce
a vector representation haux. By taking hbase and
haux as input, the scoring function outputs a score.

Specifically, these encoders first produce the
representations, hbase and haux. Then, these repre-
sentations are concatenated into one vector, which
is mapped to a feature vector z.

z = tanh(W · [hbase;haux]) , (1)

where W is a weight matrix. Finally, z is mapped
to a scalar value by the sigmoid function.

y = sigmoid(w · z+ b) ,

where w is a weight vector, b is a bias value, and
y is a score in the range of (0, 1). In the follow-
ing subsections, we describe the details of each en-
coder.

3.2 Base Document Encoder
The base document encoder produces a document
representation hbase in Equation 1. For the base
document encoder, we use the Neural Essay As-
sessor (NEA) model proposed by Taghipour and
Ng (2016). This model uses three types of layers:
an embedding layer, a Bi-directional Long Short-
Term Memory (BiLSTM) (Schuster and Paliwal,
1997) layer and a mean-over-time layer.

Given the input essay of T words w1:T =
(w1, w2, · · · , wT ), the embedding layer (Emb)
produces a sequence of word embeddings w1:T =
(w1,w2, · · · ,wT ).

w1:T = Emb(w1:T ) ,

where each word embedding is a dword dimen-
sional vector, i.e. wi ∈ Rd

word
.

Then, taking x1:T as input, the BiLSTM layer
produces a sequence of contextual representations
h1:T = (h1,h2, · · · ,hT ).

h1:T = BiLSTM(x1:T ) ,

where each representation hi is Rd
hidden

.
Finally, taking h1:T as input, the mean-over-

time layer produces a vector averaged over the se-
quence.

hmean =
1

T

T∑
t=1

ht . (2)

We use this resulting vector as the base document
representation, i.e. hbase = hmean.

3.3 Auxiliary Encoders
The auxiliary encoders produce a representation of
essay-related information haux in Equation 1. We
provide two encoders that capture different types
of essay-related information.



381

Large-scale
Unlabeled
Essays

Pre-training
(Section 4.2)

Labeled 
Essays

Auxiliary Encoder

Base Document
Encoder

Scoring Function

Binary 
Classifier

(Sentence shuffled, incoherent)

(Paragraph shuffled, incoherent)

Label = 1: original documents
(coherent/cohesive)

The majority … instead of activities.
I believe that … because the students ...

Moreover, some are … percentage.
They are only … for they pass exams.

Cross 
Entropy Loss

Cross
Entropy

Loss

Label = 0: corrupted documents 
(incoherent/incohesive)

Moreover, some are … percentage.
They are only … for they pass exams.
I believe that … because the students ...
The majority … instead of activities.

(DI shuffled, incohesive)

The majority … instead of activities.
I believe that … because the students ...

Moreover, some are … percentage.
They are only … for they pass exams.

The majority … for activities.
I believe that … Moreover the students ...

instead of, some are … percentage.
They are only … because they pass exams.

The majority … instead of activities.
I believe that … because the students ...

Moreover, some are … percentage.
They are only … for they pass exams.

Moreover, some are … percentage.
They are only … for they pass exams.

The majority … instead of activities.
I believe that … because the students ...

Figure 2: Proposed method for unsupervised learning of discourse-aware text representation utilizing coher-
ent/incoherent and cohesive/incohesive texts and use of the discourse-aware text embeddings for essay scoring.

Paragraph Function Encoder (PFE). Each
paragraph in an essay plays a different role. For
instance, the first paragraph tends to introduce the
topic of the essay, and the last paragraph tends to
sum up the whole content and make some conclu-
sions. Here, we capture such paragraph functions.

Specifically, we obtain paragraph function la-
bels of essays using Persing et al. (2010)’s heuris-
tic rules.2 Persing et al. (2010) specified four
paragraph function labels: Introduction (I), Body
(B), Rebuttal (R) and Conclusion (C). We repre-
sent these labels as vectors and incorporate them
into the base model. The paragraph function label
encoder consists of two modules, an embedding
layer and a BiLSTM layer.

We assume that an essay consists of M para-
graphs, and the i-th paragraph has already been
assigned a function label pi. Given the sequence
of paragraph function labels of an essay p1:M =
(p1, p2, ..., pM ), the embedding layer (Embpara)
produces a sequence of label embeddings, i.e.
p1:M = Embpara(p1:M ), where each embedding
pi is Rd

para
. Then, taking p1:M as input, the BiL-

STM layer produces a sequence of contextual rep-
resentations h1:M = BiLSTM(p1:M ), where hi
is RdPFE . We use the last hidden state hM as the
paragraph function label sequence representation,
i.e. haux = hM .

Prompt Encoder (PE). As shown in Figure 1,
essays are written for a given prompt, where the
prompt itself can be useful for essay scoring.

2See http://www.hlt.utdallas.edu/
˜persingq/ICLE/orgDataset.html for further
details.

Based on this intuition, we incorporate prompt in-
formation.

The prompt encoder uses an embed-
ding layer and a Long Short-Term Memory
(LSTM) (Hochreiter, Sepp and Schmidhuber,
Jürgen, 1997) layer to produce a prompt repre-
sentation. Formally, we assume that the input is a
prompt of N words, w1:N = (w1, w2, · · · , wN ).
First, the embedding layer maps the input prompt
w1:N to a sequence of word embeddings, w1:N ,
where wi is Rd

prompt
. Then, taking w1:N as input,

the LSTM layer produces a sequence of hidden
states, h1:N = (h1,h2, · · · ,hN ), where hi is
RdPE . The last hidden state is regarded as the
resulting representation, i.e. haux = hN .

4 Proposed Method

4.1 Overview

Figure 2 summarizes the proposed method. First,
we pre-train a base document encoder (Sec-
tion 3.2) in an unsupervised manner. The pretrain-
ing is motivated by the following hypotheses: (i)
artificially corrupted incoherent/incohesive docu-
ments lack logical sequencing, and (ii) training a
base document encoder to differentiate between
the original and incoherent/incohesive documents
makes the encoder logical sequence-aware.

The pre-training is done in two steps. First, we
pre-train the document encoder with large-scale
unlabeled essays. Second, we pre-train the en-
coder using only the unlabeled essays of target
corpus used for essay scoring. We expect that
this fine-tuning alleviates the domain mismatch
between the large-scale essays and target essays

http://www.hlt.utdallas.edu/~persingq/ICLE/orgDataset.html
http://www.hlt.utdallas.edu/~persingq/ICLE/orgDataset.html


382

(e.g., essay length). Finally, the pre-trained en-
coder is then re-trained on the annotations of essay
scoring tasks in a supervised manner.

4.2 Pre-training

We artificially create incoherent/incohesive docu-
ments by corrupting them with random shuffling
methods: (i) sentences, (ii) only DIs and (iii) para-
graphs. Figure 2 shows examples of original and
corrupted documents. We shuffle DIs since they
are important for representing the logical connec-
tion between sentences. For example, “Mary did
well although she was ill” is logically connected,
but “Mary did well but she was ill.” and “Mary
did well. She was ill.” lack logical sequencing be-
cause of improper and lack of DI usage, respec-
tively. Paragraph shuffling is also important since
coherent essays have sequences like Introduction-
Body-Conclusion to provide a logically consistent
meaning of the text.

Specifically, we treat the pre-training as a bi-
nary classification task where the encoder classi-
fies documents as coherent/cohesive or not.

P (y(d) = 1|d) = σ(wunsup · hmean) ,

where y is a binary function mapping from a doc-
ument d to {0, 1}, in which 1 represents the doc-
ument is coherent/cohesive and 0 represents not.
The base document representation hmean (Eq. 2)
is multiplied with a weight vector wunsup, and the
sigmoid function σ returns a probability that the
given document d is coherent/cohesive.

To train the model parameters, we minimize the
binary cross-entropy loss function,

L = −
N∑
i=1

yilog(P (y(di) = 1|di)) +

(1− yi)log(1− P (y(di) = 1|di)) ,

where yi is a gold-standard label of coher-
ence/cohesion of di and N is the total number of
documents. Note that yi is automatically assigned
in the corruption process where an original docu-
ment has a label of 1 and an artificially corrupted
document has a label of 0.

5 Experiments

5.1 Setup

We use five-fold cross-validation for evaluating
our models with the same split as Persing et al.

(2010); Persing and Ng (2015) and Wachsmuth
et al. (2016). The reported results are aver-
aged over five folds. However, our results are
not directly comparable since our training data is
smaller as we reserve a development set (100 es-
says) for model selection while they do not. We
use the mean squared error as an evaluation mea-
sure.

Data We use the International Corpus of Learner
English (ICLE) (Granger et al., 2009) for essay
scoring which contains 6,085 essays and 3.7 mil-
lion words. Most of the ICLE essays (91%) are
argumentative and vary in length, having 7.6 para-
graphs and 33.8 sentences on average (Wachsmuth
et al., 2016). Some essays have been anno-
tated with different criteria among which 1,003
essays are annotated with Organization scores
and 1,000 essays are annotated with Argument
Strength scores. Both scores range from 1 to 4
at half-point increments. For our scoring task, we
utilize the 1,003 essays.

To pre-train the document encoder, we use
35,222 essays from four datasets, (i) the Kaggle’s
Automated Student Assessment Prize (ASAP)
dataset3 (12,976) (ii) TOEFL11 (Blanchard et al.,
2013) dataset (12,100), (iii) The International Cor-
pus Network of Asian Learners of English (IC-
NALE) (Ishikawa, 2013) dataset (5,600), and (iv)
the ICLE essays not used for Organization and Ar-
gument Strength scoring (4,546).4

See Appendix A and B for further details on the
hyperparameters and preprocessing.

5.2 Results and Discussion

From two baseline models, we report the best
model for each task (Base+PFE for Organization,
Base+PE for Argument Strength).

Table 1 indicates that the proposed unsuper-
vised pre-training improves the performance of
Organization and Argument Strength scoring.
These results support our hypothesis that training
with random corruption of documents helps a doc-
ument encoder learn logical sequence-aware text
representations. In most cases, fine-tuning the en-
coder for each scoring task again helps to improve
the performance.

The results indicate that paragraph shuffling

3https://www.kaggle.com/c/asap-aes
4During pre-training with paragraph shuffled essays, we

use only 16,646 essays (TOEFL11 and ICLE essays) since
ASAP and ICNALE essays have a single paragraph.

https://www.kaggle.com/c/asap-aes


383

Model Shuffle Type Fine-tuning Mean Squared Error
Organization Argument Strength

Baseline - - 0.182 0.248

Proposed

Sentence 0.187 0.244
Sentence X 0.186 0.244*
Discourse Indicator 0.187 0.242
Discourse Indicator X 0.193 0.246
Paragraph 0.172* 0.236*

Paragraph X 0.169* 0.231*

Persing et al. (2010) 0.175 -
Persing et al. (2015) - 0.244
Wachsmuth et al. (2016) 0.164 0.226

Table 1: Performance of essay scoring. ‘*’ indicates a statistical significance (Wilcoxon signed-rank test, p < 0.05)
against the baseline model. Base+PFE and Base+PE are used in Organization and Argument Strength, respectively.

is the most effective in both scoring tasks (sta-
tistically significant by Wilcoxon’s signed rank
test, p < 0.05). This could be attributed to
the fact that paragraph sequences create a more
clear organizational and argumentative structure.
Suppose that an essay first introduces a topic,
states their position, supports their position and
then concludes. Then, the structure of the essay
would be regarded as “well-organized”. More-
over, the argument of the essay would be con-
sidered “strong” since it provides support for
their position. The results suggest that such lev-
els of abstractions (e.g., Introduction-Body-Body-
Conclusion) are well captured at a paragraph-
level, but not at a sentence-level or DI-level alone.

Furthermore, a manual inspection of DIs identi-
fied by the system suggest room for improvement
in DI shuffling. First, the identification of DIs is
not always reliable. Almost half of DIs identified
by our simple pattern matching algorithm (see Ap-
pendix B) were not actually DIs (e.g., we have sur-
vived so far only external difficulties). Second, we
also found that some DI-shuffled documents are
sometimes cohesive. This happens when original
document counterparts have two or more DIs with
the more or less same meaning (e.g., since and be-
cause). We speculate that this confuses the docu-
ment encoder in the pre-training process.

6 Conclusion and Future Work

We proposed an unsupervised strategy to capture
discourse structure (i.e., coherence and cohesion)
for document embedding. We train a document
encoder with coherent/cohesive and randomly cor-
rupted incoherent/incohesive documents to make
it logical-sequence aware. Our method does not
require any expensive annotation or parser. The

experimental results show that the proposed learn-
ing strategy improves the performance of essay
Organization and Argument Strength scoring.

Our future work includes adding more unanno-
tated data for pre-training and trying other unsu-
pervised objectives such as swapping clauses be-
fore and after DIs (e.g., A because B→ B because
A). We also intend to perform intrinsic evaluation
of the learned document embedding space. More-
over, we plan to evaluate the effectiveness of our
approach on more document regression or classi-
fication tasks.

7 Acknowledgements

This work was supported by JST CREST Grant
Number JPMJCR1513 and JSPS KAKENHI
Grant Number 19K20332. We would like to thank
the anonymous ACL reviewers for their insightful
comments. We also thank Ekaterina Kochmar for
her profound and useful feedback.

References
Daniel Blanchard, Joel Tetreault, Derrick Hig-

gins, Aoife Cahill, and Martin Chodorow. 2013.
TOEFL11: A corpus of non-native English. ETS
Research Report Series, 2013(2):i–15.

Youmna Farag, Helen Yannakoudakis, and Ted
Briscoe. 2018. Neural automated essay scoring and
coherence modeling for adversarially crafted input.
arXiv preprint arXiv:1804.06898.

Sylviane Granger, Estelle Dagneaux, Fanny Meunier,
and Magali Paquot. 2009. International corpus of
learner English.

Miochael AK Halliday. 1994. An introduction to func-
tional grammar 2nd edition. London: Arnold.



384

Hochreiter, Sepp and Schmidhuber, Jürgen. 1997.
Long short-term memory. Neural computation,
9(8):1735–1780.

S Ishikawa. 2013. ICNALE: the international corpus
network of Asian learners of English. Retrieved on
November, 21:2014.

Yangfeng Ji and Noah Smith. 2017. Neural discourse
structure for text categorization. arXiv preprint
arXiv:1702.01829.

Shafiq Joty, Muhammad Tasnim Mohiuddin, and
Dat Tien Nguyen. 2018. Coherence modeling of
asynchronous conversations: A neural entity grid
approach. In Proceedings of the 56th Annual Meet-
ing of the Association for Computational Linguistics
(Volume 1: Long Papers), volume 1, pages 558–568.

Quoc Le and Tomas Mikolov. 2014. Distributed rep-
resentations of sentences and documents. In Inter-
national Conference on Machine Learning, pages
1188–1196.

Jiwei Li and Dan Jurafsky. 2016. Neural net models for
open-domain discourse coherence. arXiv preprint
arXiv:1606.01545.

Chundi Liu, Shunan Zhao, and Maksims Volkovs.
2017. Unsupervised Document Embedding With
CNNs. arXiv preprint arXiv:1711.04168.

Mohsen Mesgar and Michael Strube. 2018. A Neu-
ral Local Coherence Model for Text Quality Assess-
ment. In Proceedings of the 2018 Conference on
EMNLP, pages 4328–4339.

Isaac Persing, Alan Davis, and Vincent Ng. 2010.
Modeling organization in student essays. In Pro-
ceedings of the 2010 Conference on EMNLP, pages
229–239. ACL.

Isaac Persing and Vincent Ng. 2015. Modeling argu-
ment strength in student essays. In Proceedings of
the 53rd Annual Meeting of the ACL the 7th Interna-
tional Joint Conference on Natural Language Pro-
cessing, volume 1, pages 543–552.

Mike Schuster and Kuldip K Paliwal. 1997. Bidirec-
tional recurrent neural networks. IEEE Transactions
on Signal Processing, 45(11):2673–2681.

Kaveh Taghipour and Hwee Tou Ng. 2016. A neu-
ral approach to automated essay scoring. In Pro-
ceedings of the 2016 Conference on EMNLP, pages
1882–1891.

Jian Tang, Meng Qu, and Qiaozhu Mei. 2015. Pte: Pre-
dictive text embedding through large-scale hetero-
geneous text networks. In Proceedings of the 21th
ACM SIGKDD International Conference on Knowl-
edge Discovery and Data Mining, pages 1165–1174.
ACM.

Henning Wachsmuth, Khalid Al Khatib, and Benno
Stein. 2016. Using argument mining to assess the
argumentation quality of essays. In Proceedings
of COLING 2016, the 26th International Confer-
ence on Computational Linguistics: Technical Pa-
pers, pages 1680–1691.

Sida Wang and Christopher D Manning. 2012. Base-
lines and bigrams: Simple, good sentiment and topic
classification. In Proceedings of the 50th Annual
Meeting of the ACL: Short Papers-Volume 2, pages
90–94. Association for Computational Linguistics.

Lingfei Wu, Ian EH Yen, Kun Xu, Fangli Xu, Avinash
Balakrishnan, Pin-Yu Chen, Pradeep Ravikumar,
and Michael J Witbrock. 2018. Word Mover’s Em-
bedding: From Word2Vec to Document Embedding.
arXiv preprint arXiv:1811.01713.

Will Y Zou, Richard Socher, Daniel Cer, and Christo-
pher D Manning. 2013. Bilingual word embed-
dings for phrase-based machine translation. In Pro-
ceedings of the 2013 Conference on EMNLP, pages
1393–1398.

A Hyperparameters

We use BiLSTM with 200 hidden units in each
layer for the base document encoder (dhidden =
200). For the paragraph function encoder, we
use a BiLSTM with hidden units of 200 in each
layer (dPFE = 200). For the prompt encoder, an
LSTM with an output dimension of 300 is used
(dPE = 300). We use the 50-dimensional pre-
trained word embeddings released by Zou et al.
(2013) in our base document encoder (dword =
50, dprompt = 50).

We use the Adam optimizer with a learning rate
of 0.001 and a batch size of 32. We use early stop-
ping with patience 15 (5 for pre-training), and train
the network for 100 epochs. The vocabulary con-
sists of the 90,000 and 15,000 most frequent words
for pre-training and essay scoring, respectively.
Out-of-vocabulary words are mapped to special
tokens. We perform hyperparameter tuning and
choose the best model. We tuned norm clipping
maximum values (3,5,7) and dropout rates (0.3,
0.5, 0.7, 0.9) for all models on the development
set.

B Preprocessing

We lowercase the tokens and specify an essay’s
paragraph boundaries with special tokens. During
sentence/DI shuffling for pre-training, paragraph
boundaries are not used. We collect 847 DIs from



385

the Web.5 We exclude the DI “and” since it is not
always used for initiating logic (e.g milk, banana
and tea). In essay scoring data, we found 176 DIs
and average DIs per essay is around 24. In the
pre-training data, the number of DIs found is 204
and the average DIs per essay is around 13. We
identified DIs by simple string-pattern matching.

5http://www.studygs.net/wrtstr6.htm,
http://home.ku.edu.tr/˜doregan/Writing/
Cohesion.html etc.

http://www.studygs.net/wrtstr6.htm
http://home.ku.edu.tr/~doregan/Writing/Cohesion.html
http://home.ku.edu.tr/~doregan/Writing/Cohesion.html

