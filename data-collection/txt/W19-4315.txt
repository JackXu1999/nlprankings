



















































Auto-Encoding Variational Neural Machine Translation


Proceedings of the 4th Workshop on Representation Learning for NLP (RepL4NLP-2019), pages 124–141
Florence, Italy, August 2, 2019. c©2019 Association for Computational Linguistics

124

Auto-Encoding Variational Neural Machine Translation

Bryan Eikema & Wilker Aziz
Institute for Logic, Language and Computation

University of Amsterdam
b.eikema@uva.nl, w.aziz@uva.nl

Abstract

We present a deep generative model of bilin-
gual sentence pairs for machine translation.
The model generates source and target sen-
tences jointly from a shared latent representa-
tion and is parameterised by neural networks.
We perform efficient training using amortised
variational inference and reparameterised gra-
dients. Additionally, we discuss the statistical
implications of joint modelling and propose
an efficient approximation to maximum a pos-
teriori decoding for fast test-time predictions.
We demonstrate the effectiveness of our model
in three machine translation scenarios: in-
domain training, mixed-domain training, and
learning from a mix of gold-standard and syn-
thetic data. Our experiments show consistently
that our joint formulation outperforms condi-
tional modelling (i.e. standard neural machine
translation) in all such scenarios.

1 Introduction

Neural machine translation (NMT) systems
(Kalchbrenner and Blunsom, 2013; Sutskever
et al., 2014; Cho et al., 2014b) require vast
amounts of labelled data, i.e. bilingual sentence
pairs, to be trained effectively. Oftentimes, the
data we use to train these systems are a byproduct
of mixing different sources of data. For example,
labelled data are sometimes obtained by putting
together corpora from different domains (Sen-
nrich et al., 2017). Even for a single domain,
parallel data often result from the combination
of documents independently translated from dif-
ferent languages by different people or agencies,
possibly following different guidelines. When
resources are scarce, it is not uncommon to
mix in some synthetic data, e.g. bilingual data
artificially obtained by having a model translate
target monolingual data to the source language
(Sennrich et al., 2016a). Translation direction,

original language, and quality of translation are
some of the many factors that we typically choose
not to control for (due to lack of information or
simply for convenience).1 All those arguably
contribute to making our labelled data a mixture
of samples from various data distributions.

Regular NMT systems do not explicitly account
for latent factors of variation, instead, given a
source sentence, NMT models a single conditional
distribution over target sentences as a fully super-
vised problem. In this work, we introduce a deep
generative model that generates source and target
sentences jointly from a shared latent representa-
tion. The model has the potential to use the la-
tent representation to capture global aspects of the
observations, such as some of the latent factors
of variation just discussed. The result is a model
that accommodates members of a more complex
class of marginal distributions. Due to the pres-
ence of latent variables, this model requires poste-
rior inference, in particular, we employ the frame-
work of amortised variational inference (Kingma
and Welling, 2014). Additionally, we propose an
efficient approximation to maximum a posteriori
(MAP) decoding for fast test-time predictions.

Contributions We introduce a deep generative
model for NMT (§3) and discuss theoretical ad-
vantages of joint modelling over conditional mod-
elling (§3.1). We also derive an efficient approx-
imation to MAP decoding that requires only a
single forward pass through the network for pre-
diction (§3.3). Finally, we show in §4 that our
proposed model improves translation performance
in at least three practical scenarios: i) in-domain

1Also note that this list is by no means exhaustive. For
example, Rabinovich et al. (2017) show influence of factors
such as personal traits and demographics in translation. An-
other clear case is presented by Johnson et al. (2017), who
combine parallel resources for multiple languages to train a
single encoder-decoder architecture.



125

training on little data, where test data are expected
to follow the training data distribution closely; ii)
mixed-domain training, where we train a single
model but test independently on each domain; and
iii) learning from large noisy synthetic data.

2 Neural Machine Translation

In machine translation our observations are
pairs of random sequences, a source sentence
x = 〈x1, . . . , xm〉 and a target sentence y =
〈y1, . . . , yn〉, whose lengthsm and nwe denote by
|x| and |y|, respectively. In NMT, the likelihood of
the target given the source

P (y|x, θ) =
|y|∏
j=1

Cat(yj |fθ(x, y<j)) (1)

factorises without Markov assumptions (Sutskever
et al., 2014; Bahdanau et al., 2015; Cho et al.,
2014a). We have a fixed parameterised function
fθ, i.e. a neural network architecture, compute cat-
egorical parameters for varying inputs, namely, the
source sentence and target prefix (denoted y<j).

Given a dataset D of i.i.d. observations, the
parameters θ of the model are point-estimated
to attain a local maximum of the log-likelihood
function, L(θ|D) =

∑
(x,y)∈D logP (y|x, θ), via

stochastic gradient-based optimisation (Robbins
and Monro, 1951; Bottou and Cun, 2004).

Predictions For a trained model, predictions are
performed by searching for the target sentence y
that maximises the conditional P (y|x), or equiva-
lently its logarithm, with a greedy algorithm

arg max
y

P (y|x, θ) ≈ greedy
y

logP (y|x, θ) (2)

such as beam-search (Sutskever et al., 2014), pos-
sibly aided by a manually tuned length penalty.
This decision rule is often referred to as MAP de-
coding (Smith, 2011).

3 Auto-Encoding Variational NMT

To account for a latent space where global fea-
tures of observations can be captured, we intro-
duce a random sentence embedding z ∈ Rd and
model the joint distribution over observations as a
marginal of p(z, x, y|θ).2 That is, (x, y) ∈ D is
assumed to be sampled from the distribution

P (x, y|θ) =
∫
p(z)P (x, y|z, θ)dz . (3)

2We use uppercase P (·) for probability mass functions
and lowercase p(·) for probability density functions.

where we impose a standard Gaussian prior on the
latent variable, i.e. Z ∼ N (0, I), and assume
X ⊥ Y |Z. That is, given a sentence embedding z,
we first generate the source conditioned on z,

P (x|z, θ) =
|x|∏
i=1

Cat(xi|gθ(z, x<i)) , (4)

then generate the target conditioned on x and z,

P (y|x, z, θ) =
|y|∏
j=1

Cat(yj |fθ(z, x, y<j)) . (5)

Note that the source sentence is generated with-
out Markov assumptions by drawing one word
at a time from a categorical distribution parame-
terised by a recurrent neural network gθ. The tar-
get sentence is generated similarly by drawing tar-
get words in context from a categorical distribu-
tion parameterised by a sequence-to-sequence ar-
chitecture fθ. This essentially combines a neural
language model (Mikolov et al., 2010) and a neu-
ral translation model (§2), each extended to condi-
tion on an additional stochastic input, namely, z.

3.1 Statistical considerations
Modelling the conditional directly, as in standard
NMT, corresponds to the statistical assumption
that the distribution over source sentences can pro-
vide no information about the distribution over
target sentences given a source. That is, condi-
tional NMT assumes independence of β determin-
ing P (y|x, β) and α determining P (x|α). Sce-
narios where this assumption is unlikely to hold
are common: where x is noisy (e.g. synthetic or
crowdsourced), poor quality x should be assigned
low probability P (x|α) which in turn should in-
form the conditional. Implications of this assump-
tion extend to parameter estimation: updates to the
conditional are not sensitive to how exotic x is.

Let us be more explicit about how we parame-
terise our model by identifying 3 sets of param-
eters θ = {θemb-x, θLM, θTM}, where θemb-x pa-
rameterises an embedding layer for the source lan-
guage. The embedding layer is shared between the
two model components

P (x, y|z, θ) =
P (x| z, θemb-x, θLM︸ ︷︷ ︸

α

)P (y|x, z, θemb-x, θTM︸ ︷︷ ︸
β

) (6)

and it is then clear by inspection that α ∩ β =
{z, θemb-x}. In words, we break the independence



126

assumption in two ways, namely, by having the
two distributions share parameters and by having
them depend on a shared latent sentence represen-
tation z. Note that while the embedding layer is
deterministic and global to all sentence pairs in the
training data, the latent representation is stochastic
and local to each sentence pair.

Now let us turn to considerations about la-
tent variable modelling. Consider a model
P (x|θemb-x, θLM)P (y|x, θemb-x, θTM) of the joint
distribution over observations that does not em-
ploy latent variables. This alternative, which we
discuss further in experiments, models each com-
ponent directly, whereas our proposed model (3)
requires marginalisation of latent embeddings
z. Marginalisation turns our directed graphical
model into an undirected one inducing further
structure in the marginal. See Appendix B, and
Figure 2 in particular, for an extended discussion.

3.2 Parameter estimation
The marginal in Equation (3) is clearly intractable,
thus precluding maximum likelihood estimation.
Instead, we resort to variational inference (Jor-
dan et al., 1999; Blei et al., 2017) and introduce
a variational approximation q(z|x, y, λ) to the in-
tractable posterior p(z|x, y, θ). We let the approx-
imate posterior be a diagonal Gaussian

Z|λ, x, y ∼ N (u, diag(s� s))
u = µλ(x, y)

s = σλ(x, y)

(7)

and predict its parameters (i.e. u ∈ Rd, s ∈ Rd>0)
with neural networks whose parameters we denote
by λ. This makes the model an instance of a varia-
tional auto-encoder (Kingma and Welling, 2014).
See Figure 1 in Appendix B for a graphical depic-
tion of the generative and inference models.

We can then jointly estimate the parameters of
both models (generative θ and inference λ) by
maximising the ELBO (Jordan et al., 1999), a
lowerbound on the marginal log-likelihood,

logP (x, y|θ) ≥ E(θ, λ|x, y) =
E�∼N (0,I) [logP (x, y|z = u + �� s, θ)]
−KL(N (z|u, diag(s� s))||N (z|0, I)) ,

(8)

where we have expressed the expectation with re-
spect to a fixed distribution—a reparameterisation
available to location-scale families such as the
Gaussian (Kingma and Welling, 2014; Rezende

et al., 2014). Due to this reparameterisation, we
can compute a Monte Carlo estimate of the gradi-
ent of the first term via back-propagation (Rumel-
hart et al., 1986; Schulman et al., 2015). The
KL term, on the other hand, is available in closed
form (Kingma and Welling, 2014, Appendix B).

3.3 Predictions
In a latent variable model, MAP decoding (9a) re-
quires searching for y that maximises the marginal
P (y|x, θ) ∝ P (x, y|θ), or equivalently its loga-
rithm. In addition to approximating exact search
with a greedy algorithm, other approximations
are necessary in order to achieve fast predic-
tion. First, rather than searching through the true
marginal, we search through the evidence lower-
bound. Second, we replace the approximate pos-
terior q(z|x, y) by an auxiliary distribution r(z|x).
As we are searching through the space of tar-
get sentences, not conditioning on y circumvents
combinatorial explosion and allows us to drop
terms that depend on x alone (9b). Finally, in-
stead of approximating the expectation via MC
sampling, we condition on the expected latent rep-
resentation and search greedily (9c).

arg max
y

logP (y|x) (9a)

≈ arg max
y

Er(z|x)[logP (y|z, x)] (9b)

≈ greedy
y

logP (y|Er(z|x)[z], x) (9c)

Together, these approximations enable prediction
with a single call to an arg max solver, in our case
a standard greedy search algorithm, which leads to
prediction times that are very close to that of the
conditional model. This strategy, and (9b) in par-
ticular, suggests that a good auxiliary distribution
r(z|x) should approximate q(z|x, y) closely.

We parameterise this prediction model using a
neural network and investigate different options to
estimate its parameters. As a first option, we re-
strict the approximate posterior to conditioning on
x alone, i.e. we approach posterior inference with
qλ(z|x) rather than qλ(z|x, y), and thus, we can
use r(z|x) = qλ(z|x) for prediction.3 As a second
option, we make rφ(z|x) a diagonal Gaussian and
estimate parameters φ to make rφ(z|x) close to the
approximate posterior qλ(z|x, y) as measured by

3Note that this does not stand in contrast to our motivation
for joint modelling, as we still tie source and target through z
in the generative model, but it does limit the context available
for posterior inference.



127

D(rφ, qλ). For as long as D(rφ, qλ) ∈ R≥0 for ev-
ery choice of φ and λ, we can estimate φ jointly
with θ and λ by maximising a modified ELBO

logP (x, y|θ) ≥ E(θ, λ|x, y)−D(rφ, qλ) (10)

which is loosened by the gap between rφ and qλ.
In experiments we investigate a few options for
D(rφ, qλ), all available in closed form for Gaus-
sians, such as KL(rφ||qλ), KL(qλ||rφ), as well as
the Jensen-Shannon (JS) divergence.

Note that rφ is used only for prediction as a de-
coding heuristic and as such need not be stochas-
tic. We can, for example, design rφ(x) to be a
point estimate of the posterior mean and optimise

E(θ, λ|x, y)−
∥∥rφ(x)− Eqλ(z|x,y)[z]∥∥22 (11)

which remains a lowerbound on log-likelihood.

4 Experiments

We investigate two translation tasks, namely,
WMT’s translation of news (Bojar et al., 2016) and
IWSLT’s translation of transcripts of TED talks
(Cettolo et al., 2014), and concentrate on transla-
tions for German (DE) and English (EN) in either
direction. In this section we aim to investigate sce-
narios where we expect observations to be repre-
sentative of various data distributions. As a san-
ity check, we start where training conditions can
be considered in-domain with respect to test con-
ditions. Though note that this does not preclude
the potential for appreciable variability in observa-
tions as various other latent factors still likely play
a role (see §1). We then mix datasets from these
two remarkably different translation tasks and in-
vestigate whether performance can be improved
across tasks with a single model. Finally, we in-
vestigate the case where we learn from synthetic
data in addition to gold-standard data. For this in-
vestigation we derive synthetic data from observa-
tions that are close to the domain of the test set in
an attempt to avoid further confounders.

Data For bilingual data we use News Commen-
tary (NC) v12 (Bojar et al., 2017) and IWSLT
2014 (Cettolo et al., 2014), where we assume NC
to be representative of the test domain of the WMT
News task. The datasets consist of 255, 591 train-
ing sentences and 153, 326 training sentences re-
spectively. In experiments with synthetic data, we
subsample 106 sentences from the News Crawl
2016 articles (Bojar et al., 2017) for either German

or English depending on the target language. For
the WMT task, we concatenate newstest2014
and newstest2015 for validation/development
(5, 172 sentence pairs) and report test results on
newstest2016 (2, 999 sentence pairs). For
IWSLT, we use the split proposed by Ranzato et al.
(2016) who separated 6, 969 training instances for
validation/development and reported test results
on a concatenation of dev2010, dev2012 and
tst2010-2012 (6, 750 sentence pairs).

Pre-processing We tokenized and truecased all
data using standard scripts from the Moses
toolkit (Koehn et al., 2007), and removed sen-
tences longer than 50 tokens. For computational
efficiency and to avoid problems with closed vo-
cabularies, we segment the data using BPE (Sen-
nrich et al., 2016b) with 32, 000 merge operations
independently for each language. For training the
truecaser and the BPEs we used a concatenation
of all the available bilingual and monolingual data
for German and all bilingual data for English.

Systems We develop all of our models on top of
Tensorflow NMT (Luong et al., 2017). Our base-
line system is a standard implementation of condi-
tional NMT (COND) (Bahdanau et al., 2015). To
illustrate the importance of latent variable mod-
elling, we also include in the comparison a sim-
pler attempt at JOINT modelling where we do not
induce a shared latent space. Instead, the model is
trained in a fully-supervised manner to maximise
what is essentially a combination of two nearly in-
dependent objectives,

L(θ|D) =
∑

(x,y)∈D

|x|∑
i=1

logP (xi|x<i, θemb-x, θLM)

+

|y|∑
j=1

logP (yj |x, y<j , θemb-x, θTM) , (12)

namely, a language model and a conditional trans-
lation model. Note that the two components of the
model share very little, i.e. an embedding layer for
the source language. Finally, we aim at investigat-
ing the effectiveness of our auto-encoding varia-
tional NMT (AEVNMT).4 Appendix A contains
a detailed description of the architectures that pa-
rameterise our systems.5

4Code available from github.com/Roxot/AEVNMT.
5In comparison to COND, AEVNMT requires additional

components: a source language model, an inference network,

github.com/Roxot/AEVNMT


128

NC IWSLT

Dropout 30% 30%
Word dropout rate 10% 20%
KL annealing steps 80, 000 80, 000

KL(q(z)||p(z)) on EN-DE 5.94 8.01

Table 1: Strategies to promote use of latent representa-
tion along with the validation KL achieved.

Hyperparameters Our recurrent cells are 256-
dimensional GRU units (Cho et al., 2014b). We
train on batches of 64 sentence pairs with Adam
(Kingma and Ba, 2015), learning rate 3 × 10−4,
for at least T updates. We then perform con-
vergence checks every 500 batches and stop af-
ter 20 checks without any improvement measured
by BLEU (Papineni et al., 2002). For in-domain
training we set T = 140, 000, and for mixed-
domain training, as well as training with synthetic
data, we set T = 280, 000. For decoding we use
a beam width of 10 and a length penalty of 1.0.
We investigate the use of dropout (Srivastava et al.,
2014) for the conditional baseline with rates from
10% to 60% in increments of 10%. Best valida-
tion performance on WMT required a rate of 40%
for EN-DE and 50% for DE-EN, while on IWSLT
it required 50% for either translation direction. To
spare resources, we also use these rates for train-
ing the simple JOINT model.

Avoiding collapsing to prior Many have no-
ticed that VAEs whose observation models are pa-
rameterised by strong generators, such as recur-
rent neural networks, learn to ignore the latent rep-
resentation (Bowman et al., 2016; Higgins et al.,
2017; Sønderby et al., 2016; Alemi et al., 2018). In
such cases, the approximate posterior “collapses”
to the prior, and where one has a fixed prior, such
as our standard Gaussian, this means that the pos-
terior becomes independent of the data, which is
obviously not desirable. Bowman et al. (2016)
proposed two techniques to counter this effect,
namely, “KL annealing”, and target word dropout.
KL annealing consists in incorporating the KL
term of Equation (8) into the objective gradually,
thus allowing the posterior to move away from the
prior more freely at early stages of training. After

and possibly a prediction network. However, this does not
add much sequential computation: the inference network can
run in parallel with the source encoder, and the source lan-
guage model runs in parallel with the target decoder.

Objective BLEU ↑

ELBOx,y−KL(rφ(z|x)||qλ(z|x, y)) 14.7
ELBOx,y−KL(qλ(z|x, y)||rφ(z|x)) 14.8
ELBOx,y− JS(rφ(z|x)||qλ(z|x, y)) 14.9
ELBOx,y−

∥∥rφ(x)− Eqλ(z|x,y)[Z]∥∥22 14.8
ELBOx 14.9

Table 2: EN-DE validation results for NC training.
ELBOx means we condition on the source alone for
posterior inference, i.e. the variational approximation
qλ(z|x) is used for training and for predictions. In
all other cases, we condition on both observations for
training, i.e. qλ(z|x, y), and train either a distribution
rφ(z|x) or a point estimate rφ(x) for predictions.

a number of annealing steps, the KL term is in-
corporated in full and training continues with the
actual ELBO. In our search we considered anneal-
ing for 20, 000 to 80, 000 training steps. Word
dropout consists in randomly masking words in
observed target prefixes at a given rate. The idea
is to harm the potential of the decoder to capi-
talise on correlations internal to the structure of
the observation in the hope that it will rely more
on the latent representation instead. We consid-
ered rates from 20% to 40% in increments of 10%.
Table 1 shows the configurations that achieve best
validation results on EN-DE. To spare resources,
we reuse these hyperparameters for DE-EN ex-
periments. With these settings, we attain a non-
negligible validation KL (see, last row of Table 1),
which indicates that the approximate posterior is
different from the prior at the end of training.

ELBO variants We investigate the effect of
conditioning on target observations for posterior
inference during training against a simpler variant
that conditions on the source alone. Table 2 sug-
gests that conditioning on x is sufficient and thus
we opt to continue with this simpler version. Do
note that when we use both observations for poste-
rior inference, i.e. qλ(z|x, y), and thus train an ap-
proximation rφ for prediction, we have additional
parameters to estimate (e.g. due to the need to en-
code y for qλ and x for rφ), thus it may be the case
that for these variants to show their potential we
need larger data and/or prolonged training.

4.1 Results

In this section we report test results in terms of
BLEU (Papineni et al., 2002) and BEER (Stano-
jević and Sima’an, 2014), but in Appendix E



129

EN-DE DE-EN

Task Model BLEU ↑ BEER ↑ BLEU ↑ BEER ↑

IWSLT14 COND 23.0 (0.1) 58.6 (0.1) 27.3 (0.2) 59.8 (0.1)
JOINT 23.2 58.7 27.5 59.8
AEVNMT 23.4 (0.1) 58.8 (0.1) 28.0 (0.1) 60.1 (0.1)

WMT16 COND 17.8 (0.2) 53.1 (0.1) 20.1 (0.1) 53.7 (0.1)
JOINT 17.9 53.4 20.1 53.7
AEVNMT 18.4 (0.2) 53.5 (0.1) 20.6 (0.2) 53.6 (0.1)

Table 3: Test results for in-domain training on IWSLT (top) and NC (bottom): we report average (1std) across 5
independent runs for COND and AEVNMT, but a single run of JOINT.

we additionally report METEOR (Denkowski and
Lavie, 2011) and TER (Snover et al., 2006). We
de-truecase and de-tokenize our system’s predic-
tions and compute BLEU scores using Sacre-
BLEU (Post, 2018).6 For BEER, METEOR and
TER, we tokenize the results and test sets using the
same tokenizer as used by SacreBLEU. We make
use of BEER 2.0, and for METEOR and TER use
MULTEVAL (Clark et al., 2011). In Appendix D
we report validation results, in this case in terms of
BLEU alone as that is what we used for model se-
lection. Finally, to give an indication of the degree
to which results are sensitive to initial conditions
(e.g. random initialisation of parameters), and to
avoid possibly misleading signifiance testing, we
report the average and standard deviation of 5 in-
dependently trained models. To spare resources
we do not report multiple runs for JOINT, but our
experience is that its performance varies similarly
to that of the conditional baseline.

We start with the case where we can reasonably
assume training data to be in-domain with respect
to test data. Table 3 shows in-domain training per-
formance. First, we remark that our conditional
baseline for the IWSLT14 task (IWSLT training)
is very close to an external baseline trained on the
same data (Bahdanau et al., 2017).7 The results
on IWSLT show benefits from joint modelling and
in particular from learning a shared latent space.
For the WMT16 task (NC training), BLEU shows
a similar trend, namely, joint modelling with a
shared latent space (AEVNMT) outperforms both
conditional modelling and the simple joint model.

6Version string: BLEU+case.mixed+numrefs.1+
smooth.exp+tok.13a+version.1.2.12

7Bahdanau et al. (2017) report 27.56 on the same test set
for DE-EN, though note that they train on words rather than
BPEs and use a different implementation of BLEU.

We now consider the scenario where we know
for a fact that observations come from two differ-
ent data distributions, which we realise by train-
ing our models on a concatenation of IWSLT and
NC. In this case, we perform model selection once
on the concatenation of both development sets and
evaluate the same model on each domain sepa-
rately. We can see in Table 4 that conditional mod-
elling is never preferred, JOINT performs reason-
ably well, especially for DE-EN, and that in every
comparison our AEVNMT outperforms the condi-
tional baseline both in terms of BLEU and BEER.

Another common scenario where two very dis-
tinct data distributions are mixed is when we capi-
talise on the abundance of monolingual data and
train on a concatenation of gold-standard bilin-
gual data (we use NC) and synthetic bilingual
data derived from target monolingual corpora via
back-translation (Sennrich et al., 2016a) (we use
News Crawl). In such a scenario the latent vari-
able might be able to inform the translation model
of the amount of noise present in the source sen-
tence. Table 5 shows results for both baselines and
AEVNMT. First, note that synthetic data greatly
improves the conditional baseline, in particular
translating into English. Once again AEVNMT
consistently outperforms conditional modelling
and joint modelling without latent variables.

By mixing different sources of data we are try-
ing to diagnose whether the generative model we
propose is robust to unknown and diverse sources
of variation mixed together in one training set (e.g.
NC + IWSLT or gold-standard + synthetic data).
However, note that a point we are certainly not try-
ing to make is that the model has been designed
to perform domain adaptation. Nonetheless, in
Appendix C we try to shed light on what hap-
pens when we use the model to translate genres



130

WMT16 IWSLT14

Training Model BLEU ↑ BEER ↑ BLEU ↑ BEER ↑

EN-DE COND 17.6 (0.4) 53.9 (0.2) 23.9 (0.3) 59.3 (0.1)
JOINT 18.1 54.3 24.2 59.5
AEVNMT 18.4 (0.2) 54.5 (0.2) 24.1 (0.3) 59.5 (0.2)

DE-EN COND 21.6 (0.2) 55.5 (0.2) 29.1 (0.2) 60.9 (0.1)
JOINT 22.3 55.6 29.2 61.2
AEVNMT 22.3 (0.1) 55.6 (0.1) 29.2 (0.1) 61.1 (0.1)

Table 4: Test results for mixed-domain training: we report average (1std) across 5 independent runs for COND and
AEVNMT, but a single run of JOINT.

it has never seen. On a dataset covering various
unseen genres, we observe that both COND and
AEVNMT perform considerably worse showing
that without taking domain adaptation seriously
both models are inadequate. In terms of BLEU,
differences range from −0.3 to 0.8 (EN-DE) and
0.3 to 0.7 (DE-EN) and are mostly in favour of
AEVNMT (17/20 comparisons).

Remarks It is intuitive to expect latent variable
modelling to be most useful in settings containing
high variability in the data, i.e. mixed-domain and
synthetic data settings, though in our experiments
AEVNMT shows larger improvements in the in-
domain setting. We speculate two reasons for this:
i) it is conceivable that variation in the mixed-
domain and synthetic data settings are too large
to be well accounted by a diagonal Gaussian; and
ii) the benefits of latent variable modelling may
diminish as the amount of available data grows.

4.2 Probing latent space

To investigate what information the latent space
encodes we explore the idea of training simple
linear probes or diagnostic classifiers (Alain and
Bengio, 2017; Hupkes et al., 2018). With simple
Bayesian logistic regression we have managed to
predict from Z ∼ q(z|x) domain indicators (i.e.
newswire vs transcripts) and gold-standard vs syn-
thetic data at performance above 90% accuracy
on development set. However, a similar perfor-
mance is achieved from the deterministic average
state of the bidirectional encoder of the conditional
baseline. We have also been able to predict from
Z ∼ q(z|x) the level of noise in back-translated
data measured on the development set at the sen-
tence level by an automatic metric, i.e. METEOR,
with performance above what can be done with

random features. Though again, the performance
is not much better than what can be done with a
conditional baseline. Still, it is worth highlight-
ing that these aspects are rather coarse, and it is
possible that the performance gains we report in
§4.1 are due to far more nuanced variations in the
data. At this point, however, we do not have a
good qualitative assessment of this conjecture.

5 Related Work

Joint modelling In similar work, Shah and Bar-
ber (2018) propose a joint generative model whose
probabilistic formulation is essentially identical to
ours. Besides some small differences in archi-
tecture, our work differs in two regards: motiva-
tion and strategy for predictions. Their goal is to
jointly learn from multiple language pairs by shar-
ing a single polyglot architecture (Johnson et al.,
2017). Their strategy for prediction is based on a
form of stochastic hill-climbing, where they sam-
ple an initial z from the standard Gaussian prior
and decode via beam search in order to obtain
a draft translation ỹ = greedyy P (y|z, x).This
translation is then iteratively refined by encoding
the pair 〈x, ỹ〉, re-sampling z, though this time
from q(z|x, ỹ), and re-decoding with beam search.
Unlike our approach, this requires multiple calls to
the inference network and to beam search. More-
over, the inference model, which is trained on
gold-standard observations, is used on noisy tar-
get sentences.

Cotterell and Kreutzer (2018) interpret back-
translation as a single iteration of a wake-sleep al-
gorithm (Hinton et al., 1995) for a joint model of
bitext P (x, y|θ) = P (y|x, θ)P?(x). They sam-
ple directly from the data distribution P?(x) and
learn two NMT models, a generative P (y|x, θ)
and an auxiliary model Q(x|y, φ), each trained



131

WMT16 EN-DE DE-EN

BLEU ↑ BEER ↑ BLEU ↑ BEER ↑

COND 17.8 (0.2) 53.1 (0.1) 20.1 (0.1) 53.7 (0.1)
+ synthetic data 22.3 (0.3) 57.0 (0.2) 26.9 (0.2) 58.5 (0.1)

JOINT + synthetic data 22.2 57.0 26.7 58.6
AEVNMT + synthetic data 22.5 (0.2) 57.0 (0.1) 27.4 (0.2) 58.8 (0.1)

Table 5: Test results for training on NC plus synthetic data (back-translated News Crawl): we report average (1std)
across 5 independent runs for COND and AEVNMT, but a single run of JOINT.

on a separate objective. Zhang et al. (2018) pro-
pose a joint model of bitext trained to incorporate
the back-translation heuristic as a trainable com-
ponent in a formulation similar to that of Cotterell
and Kreutzer (2018). In both cases, joint mod-
elling is done without a shared latent space and
without a source language model.

Multi-task learning An alternative to joint
learning is to turn to multi-task learning and ex-
plore parameter sharing across models trained on
different, though related, data with different ob-
jectives. For example, Cheng et al. (2016) incor-
porate both source and target monolingual data
by multi-tasking with a non-differentiable auto-
encoding objective. They jointly train a source-to-
target and target-to-source system that act as en-
coder and decoder respectively. Zhang and Zong
(2016) combine a source language model objec-
tive with a source-to-target conditional NMT ob-
jective and shared the source encoder in a multi-
task learning fashion.

Variational LMs and NMT Bowman et al.
(2016) first proposed to augment a neural lan-
guage model with a prior over latent space. Our
source component is an instance of their model.
More recently, Xu and Durrett (2018) proposed
to use a hyperspherical uniform prior rather than
a Gaussian and showed the former leads to bet-
ter representations. Zhang et al. (2016) proposed
the first VAE for NMT. They augment the condi-
tional with a Gaussian sentence embedding and
model observations as draws from the marginal
P (y|x, θ) =

∫
p(z|x, θ)P (y|x, z, θ)dz. Their for-

mulation is a conditional deep generative model
(Sohn et al., 2015) that does not model the source
side of the data, where, rather than a fixed standard
Gaussian, the latent model is itself parameterised
and depends on the data. Schulz et al. (2018)
extend the model of Zhang et al. (2016) with a

Markov chain of latent variables, one per timestep,
allowing the model to capture greater variability.

Latent domains In the context of statistical MT,
Cuong and Sima’an (2015) estimate a joint dis-
tribution over sentence pairs while marginalising
discrete latent domain indicators. Their model fac-
torises over word alignments and is not used di-
rectly for translation, but rather to improve word
and phrase alignments, or to perform data selec-
tion (Hoang and Sima’an, 2014), prior to train-
ing. There is a vast literature on domain adap-
tation for statistical machine translation (Cuong
and Sima’an, 2017), as well as for NMT (Chu and
Wang, 2018), but a full characterisation of this ex-
citing field is beyond the scope of this paper.

6 Discussion and Future Work

We have presented a joint generative model of
translation data that generates both observations
conditioned on a shared latent representation. Our
formulation leads to questions such as why joint
learning? and why latent variable modelling? to
which we give an answer based on statistical facts
about conditional modelling and marginalisation
as well as empirical evidence of improved perfor-
mance. Our model shows moderate but consis-
tent improvements across various settings and over
multiple independent runs.

In future work, we shall investigate datasets an-
notated with demographics and personal traits in
an attempt to assess how far we can go in cap-
turing fine grained variation. Though note that
if such factors of variation vary widely in distri-
bution, it may be naı̈ve to expect we can model
them well with a simple Gaussian prior. If that
turns out to be the case, we will investigate mixing
Gaussian components (Miao et al., 2016; Srivas-
tava and Sutton, 2017) and/or employing a hierar-
chical prior (Goyal et al., 2017).



132

Acknowledgements

This project has received fund-
ing from the Dutch Organiza-
tion for Scientific Research VICI
Grant No 277-89-002 and from

the European Union’s Horizon 2020 research
and innovation programme under grant agreement
No 825299 (GoURMET). We also thank Philip
Schulz, Khalil Sima’an, and Joost Bastings for
comments and helpful discussions. A Titan Xp
card used for this research was donated by the
NVIDIA Corporation.

References
G. Alain and Y. Bengio. 2017. Understanding interme-

diate layers using linear classifier probes. In ICLR,
2017, Toulon, France.

A. Alemi, B. Poole, I. Fischer, J. Dillon, R. A. Saurous,
and K. Murphy. 2018. Fixing a broken ELBO. In
Proceedings of ICML, 2018, pages 159–168, Stock-
holm, Sweden.

D. Bahdanau, P. Brakel, K. Xu, A. Goyal, R. Lowe,
J. Pineau, A. Courville, and Y. Bengio. 2017. An
actor-critic algorithm for sequence prediction. In
ICLR, 2017, Toulon, France.

D. Bahdanau, K. Cho, and Y. Bengio. 2015. Neural
Machine Translation by Jointly Learning to Align
and Translate. In ICLR, 2015, San Diego, USA.

D. M. Blei, A. Kucukelbir, and J. D. McAuliffe. 2017.
Variational inference: A review for statisticians.
JASA, 112(518):859–877.

O. Bojar, R. Chatterjee, C. Federmann, Y. Graham,
B. Haddow, S. Huang, M. Huck, P. Koehn, Q. Liu,
V. Logacheva, C. Monz, M. Negri, M. Post, R. Ru-
bino, L. Specia, and M. Turchi. 2017. Find-
ings of the 2017 conference on machine translation
(wmt17). In Proceedings of WMT, 2017, pages 169–
214, Copenhagen, Denmark.

O. Bojar, R. Chatterjee, C. Federmann, Y. Graham,
B. Haddow, M. Huck, A. Jimeno Yepes, P. Koehn,
V. Logacheva, C. Monz, M. Negri, A. Neveol,
M. Neves, M. Popel, M. Post, R. Rubino, C. Scarton,
L. Specia, M. Turchi, K. Verspoor, and M. Zampieri.
2016. Findings of the 2016 conference on machine
translation. In Proceedings of WMT, 2016, pages
131–198, Berlin, Germany.

L. Bottou and Y. L. Cun. 2004. Large scale online
learning. In S. Thrun, L. K. Saul, and B. Schölkopf,
editors, NIPS, 2004, pages 217–224. Vancouver,
Canada.

S. R. Bowman, L. Vilnis, O. Vinyals, A. Dai, R. Joze-
fowicz, and S. Bengio. 2016. Generating sentences

from a continuous space. In Proceedings of CoNLL,
2016, pages 10–21, Berlin, Germany.

M. Cettolo, J. Niehues, S. Stüker, L. Bentivogli, and
M. Federico. 2014. Report on the 11th iwslt eval-
uation campaign, iwslt 2014. In Proceedings of
IWSLT, 2014, Lake Tahoe, USA.

Y. Cheng, W. Xu, Z. He, W. He, H. Wu, M. Sun, and
Y. Liu. 2016. Semi-supervised learning for neural
machine translation. In Proceedings of ACL, 2016,
pages 1965–1974, Berlin, Germany.

K. Cho, B. van Merrienboer, D. Bahdanau, and Y. Ben-
gio. 2014a. On the Properties of Neural Machine
Translation: Encoder-Decoder Approaches. In Pro-
ceedings of SSST, 2014, pages 103–111, Doha,
Qatar.

K. Cho, B. van Merrienboer, C. Gulcehre, D. Bah-
danau, F. Bougares, H. Schwenk, and Y. Bengio.
2014b. Learning phrase representations using rnn
encoder–decoder for statistical machine translation.
In Proceedings of EMNLP, 2014, pages 1724–1734,
Doha, Qatar.

C. Chu and R. Wang. 2018. A survey of domain adap-
tation for neural machine translation. In Proceed-
ings of COLING, 2018, pages 1304–1319, Santa Fe,
USA.

J. H. Clark, C. Dyer, A. Lavie, and N. A. Smith.
2011. Better hypothesis testing for statistical ma-
chine translation: Controlling for optimizer instabil-
ity. In Proceedings of ACL, 2011, pages 176–181,
Portland, USA.

R. Cotterell and J. Kreutzer. 2018. Explaining and
generalizing back-translation through wake-sleep.
arXiv preprint arXiv:1806.04402.

H. Cuong and K. Sima’an. 2015. Latent domain word
alignment for heterogeneous corpora. In Proceed-
ings of NAACL-HLT, 2015, pages 398–408, Denver,
Colorado.

H. Cuong and K. Sima’an. 2017. A survey of domain
adaptation for statistical machine translation. Ma-
chine Translation, 31(4):187–224.

H. Cuong, K. Sima’an, and I. Titov. 2016. Adapting to
all domains at once: Rewarding domain invariance
in smt. TACL, 4:99–112.

M. Denkowski and A. Lavie. 2011. Meteor 1.3: Auto-
matic metric for reliable optimization and evaluation
of machine translation systems. In Proceedings of
WMT, 2011, pages 85–91, Edinburgh, Scotland.

P. Goyal, Z. Hu, X. Liang, C. Wang, E. P. Xing, and
C. Mellon. 2017. Nonparametric variational auto-
encoders for hierarchical representation learning. In
ICCV, 2017, pages 5104–5112, Venice, Italy.

http://proceedings.mlr.press/v80/alemi18a.html
http://arxiv.org/abs/1409.0473
http://arxiv.org/abs/1409.0473
http://arxiv.org/abs/1409.0473
http://www.aclweb.org/anthology/W17-4717
http://www.aclweb.org/anthology/W17-4717
http://www.aclweb.org/anthology/W17-4717
http://www.aclweb.org/anthology/W/W16/W16-2301
http://www.aclweb.org/anthology/W/W16/W16-2301
http://www.aclweb.org/anthology/P16-1185
http://www.aclweb.org/anthology/P16-1185
http://arxiv.org/abs/1409.1259
http://arxiv.org/abs/1409.1259
http://www.aclweb.org/anthology/D14-1179
http://www.aclweb.org/anthology/D14-1179
http://aclweb.org/anthology/C18-1111
http://aclweb.org/anthology/C18-1111
http://www.aclweb.org/anthology/P11-2031
http://www.aclweb.org/anthology/P11-2031
http://www.aclweb.org/anthology/P11-2031
http://www.aclweb.org/anthology/N15-1043
http://www.aclweb.org/anthology/N15-1043
https://tacl2013.cs.columbia.edu/ojs/index.php/tacl/article/view/768
https://tacl2013.cs.columbia.edu/ojs/index.php/tacl/article/view/768
https://tacl2013.cs.columbia.edu/ojs/index.php/tacl/article/view/768
http://www.aclweb.org/anthology/W11-2107
http://www.aclweb.org/anthology/W11-2107
http://www.aclweb.org/anthology/W11-2107


133

I. Higgins, L. Matthey, A. Pal, C. Burgess, X. Glo-
rot, M. Botvinick, S. Mohamed, and A. Lerchner.
2017. beta-VAE: Learning basic visual concepts
with a constrained variational framework. In ICLR,
2017, Toulon, France.

G. E. Hinton, P. Dayan, B. J. Frey, and R. M. Neal.
1995. The” wake-sleep” algorithm for unsupervised
neural networks. Science, 268(5214):1158–1161.

C. Hoang and K. Sima’an. 2014. Latent domain
translation models in mix-of-domains haystack. In
Proceedings of COLING, 2014, pages 1928–1939,
Dublin, Ireland.

D. Hupkes, S. Veldhoen, and W. Zuidema. 2018. Visu-
alisation and ‘diagnostic classifiers’ reveal how re-
current and recursive neural networks process hier-
archical structure. JAIR, 61:907–926.

M. Johnson, M. Schuster, Q. Le, M. Krikun, Y. Wu,
Z. Chen, N. Thorat, F. a. Viégas, M. Watten-
berg, G. Corrado, M. Hughes, and J. Dean. 2017.
Google’s multilingual neural machine translation
system: Enabling zero-shot translation. TACL,
5:339–351.

M. Jordan, Z. Ghahramani, T. Jaakkola, and L. Saul.
1999. An introduction to variational methods for
graphical models. Machine Learning, 37(2):183–
233.

N. Kalchbrenner and P. Blunsom. 2013. Recurrent
continuous translation models. In Proceedings of
EMNLP, 2013, pages 1700–1709, Seattle, USA.

D. P. Kingma and J. Ba. 2015. Adam: A method for
stochastic optimization. In ICLR, 2015, San Diego,
USA.

D. P. Kingma and M. Welling. 2014. Auto-encoding
variational bayes. In ICLR, 2014, Banff, Canada.

P. Koehn, H. Hoang, A. Birch, C. Callison-Burch,
M. Federico, N. Bertoldi, B. Cowan, W. Shen,
C. Moran, R. Zens, C. Dyer, O. Bojar, A. Constantin,
and E. Herbst. 2007. Moses: open source toolkit
for statistical machine translation. In Proceedings
of ACL, 2007, pages 177–180, Prague, Czech Re-
public.

D. Koller and N. Friedman. 2009. Probabilistic Graph-
ical Models. MIT Press.

A. Kucukelbir, D. Tran, R. Ranganath, A. Gelman, and
D. M. Blei. 2017. Automatic differentiation varia-
tional inference. JMLR, 18(1):430–474.

M. Luong, E. Brevdo, and R. Zhao. 2017. Neu-
ral machine translation (seq2seq) tutorial.
https://github.com/tensorflow/nmt.

Y. Miao, L. Yu, and P. Blunsom. 2016. Neural varia-
tional inference for text processing. In ICML, 2016,
pages 1727–1736, New York, USA.

T. Mikolov, M. Karafiát, L. Burget, J. Černockỳ, and
S. Khudanpur. 2010. Recurrent neural network
based language model. In ISCA, 2010, Kyoto,
Japan.

V. Nair and G. E. Hinton. 2010. Rectified linear units
improve restricted boltzmann machines. In Pro-
ceedings of ICML, 2010, Haifa, Israel.

K. Papineni, S. Roukos, T. Ward, and W.-J. Zhu. 2002.
BLEU: a method for automatic evaluation of ma-
chine translation. In Proceedings of ACL, 2002,
pages 311–318, Philadelphia, USA.

M. Post. 2018. A call for clarity in reporting BLEU
scores. In Proceedings of WMT, 2018, pages 186–
191, Brussels, Belgium.

E. Rabinovich, R. N. Patel, S. Mirkin, L. Specia, and
S. Wintner. 2017. Personalized machine translation:
Preserving original author traits. In Proceedings of
EACL, 2017, pages 1074–1084, Valencia, Spain.

M. Ranzato, S. Chopra, M. Auli, and W. Zaremba.
2016. Sequence level training with recurrent neural
networks. In ICLR, 2016, San Juan, Puerto Rico.

D. J. Rezende, S. Mohamed, and D. Wierstra. 2014.
Stochastic backpropagation and approximate infer-
ence in deep generative models. In Proceedings of
ICML, 2014, 2, pages 1278–1286, Bejing, China.

H. Robbins and S. Monro. 1951. A stochastic approxi-
mation method. Ann. Math. Statist., 22(3):400–407.

D. E. Rumelhart, G. E. Hinton, and R. J. Williams.
1986. Parallel distributed processing: Explorations
in the microstructure of cognition, vol. 1. Nature,
323.

J. Schulman, N. Heess, T. Weber, and P. Abbeel. 2015.
Gradient estimation using stochastic computation
graphs. In NIPS, 2015, pages 3528–3536. Montreal,
Canada.

P. Schulz, W. Aziz, and T. Cohn. 2018. A stochastic
decoder for neural machine translation. In Proceed-
ings of ACL, 2018, Melbourne, Australia.

R. Sennrich, A. Birch, A. Currey, U. Germann,
B. Haddow, K. Heafield, A. V. Miceli Barone, and
P. Williams. 2017. The university of edinburgh’s
neural mt systems for wmt17. In Proceedings of
WMT, 2017, pages 389–399, Copenhagen, Den-
mark.

R. Sennrich, B. Haddow, and A. Birch. 2016a. Improv-
ing neural machine translation models with mono-
lingual data. In Proceedings of ACL, 2016, pages
86–96, Berlin, Germany.

R. Sennrich, B. Haddow, and A. Birch. 2016b. Neu-
ral machine translation of rare words with subword
units. In Proceedings of ACL, 2016, pages 1715–
1725, Berlin, Germany.

http://www.aclweb.org/anthology/C14-1182
http://www.aclweb.org/anthology/C14-1182
https://transacl.org/ojs/index.php/tacl/article/view/1081
https://transacl.org/ojs/index.php/tacl/article/view/1081
http://www.aclweb.org/anthology/D13-1176
http://www.aclweb.org/anthology/D13-1176
http://dl.acm.org/citation.cfm?id=1557769.1557821
http://dl.acm.org/citation.cfm?id=1557769.1557821
http://dl.acm.org/citation.cfm?id=3122009.3122023
http://dl.acm.org/citation.cfm?id=3122009.3122023
https://doi.org/10.3115/1073083.1073135
https://doi.org/10.3115/1073083.1073135
https://aclanthology.info/papers/W18-6319/w18-6319
https://aclanthology.info/papers/W18-6319/w18-6319
http://www.aclweb.org/anthology/E17-1101
http://www.aclweb.org/anthology/E17-1101
http://proceedings.mlr.press/v32/rezende14.html
http://proceedings.mlr.press/v32/rezende14.html
https://doi.org/10.1214/aoms/1177729586
https://doi.org/10.1214/aoms/1177729586
http://www.aclweb.org/anthology/W17-4739
http://www.aclweb.org/anthology/W17-4739
http://www.aclweb.org/anthology/P16-1009
http://www.aclweb.org/anthology/P16-1009
http://www.aclweb.org/anthology/P16-1009
http://www.aclweb.org/anthology/P16-1162
http://www.aclweb.org/anthology/P16-1162
http://www.aclweb.org/anthology/P16-1162


134

H. Shah and D. Barber. 2018. Generative neural
machine translation. In S. Bengio, H. Wallach,
H. Larochelle, K. Grauman, N. Cesa-Bianchi, and
R. Garnett, editors, NIPS, 2018, pages 1352–1361.
Montreal, Canada.

N. A. Smith. 2011. Linguistic Structure Prediction.
Morgan and Claypool.

M. Snover, B. J. Dorr, R. Schwartz, L. Micciulla, and
J. Makhoul. 2006. A study of translation edit rate
with targeted human annotation. Proceedings of
AMTA, 2006, pages 223 – 231.

K. Sohn, H. Lee, and X. Yan. 2015. Learning struc-
tured output representation using deep conditional
generative models. In NIPS, 2015, pages 3483–
3491, Montreal, Canada.

C. K. Sønderby, T. Raiko, L. Maaløe, S. K. Sønderby,
and O. Winther. 2016. Ladder variational au-
toencoders. In NIPS, 2016, pages 3738–3746,
Barcelona, Spain.

A. Srivastava and C. Sutton. 2017. Autoencoding vari-
ational inference for topic models. In ICLR, 2017,
Toulon, France.

N. Srivastava, G. Hinton, A. Krizhevsky, I. Sutskever,
and R. Salakhutdinov. 2014. Dropout: A simple
way to prevent neural networks from overfitting.
JMLR, 15:1929–1958.

M. Stanojević and K. Sima’an. 2014. Fitting sentence
level translation evaluation with many dense fea-
tures. In Proceedings of EMNLP, 2014, pages 202–
206, Doha, Qatar.

I. Sutskever, O. Vinyals, and Q. V. V. Le. 2014.
Sequence to sequence learning with neural net-
works. In Z. Ghahramani, M. Welling, C. Cortes,
N. Lawrence, and K. Weinberger, editors, NIPS,
2014, pages 3104–3112. Montreal, Canada.

J. Xu and G. Durrett. 2018. Spherical latent spaces
for stable variational autoencoders. In Proceedings
of EMNLP, 2018, pages 4503–4513, Brussels, Bel-
gium.

B. Zhang, D. Xiong, j. su, H. Duan, and M. Zhang.
2016. Variational neural machine translation. In
Proceedings of EMNLP, 2016, pages 521–530,
Austin, USA.

J. Zhang and C. Zong. 2016. Exploiting source-side
monolingual data in neural machine translation. In
Proceedings of EMNLP, 2016, pages 1535–1545,
Austin, Texas.

Z. Zhang, S. Liu, M. Li, M. Zhou, and E. Chen. 2018.
Joint training for neural machine translation mod-
els with monolingual data. In Proceedings of AAAI,
2018, pages 555–562, New Orleans, USA.

http://papers.nips.cc/paper/7409-generative-neural-machine-translation.pdf
http://papers.nips.cc/paper/7409-generative-neural-machine-translation.pdf
http://jmlr.org/papers/v15/srivastava14a.html
http://jmlr.org/papers/v15/srivastava14a.html
http://www.aclweb.org/anthology/D14-1025
http://www.aclweb.org/anthology/D14-1025
http://www.aclweb.org/anthology/D14-1025
http://aclweb.org/anthology/D18-1480
http://aclweb.org/anthology/D18-1480
https://aclweb.org/anthology/D16-1050
https://aclweb.org/anthology/D16-1160
https://aclweb.org/anthology/D16-1160
https://www.aaai.org/ocs/index.php/AAAI/AAAI18/paper/view/16336
https://www.aaai.org/ocs/index.php/AAAI/AAAI18/paper/view/16336


135

A Architectures

Here we describe parameterisation of the different
models presented in §3. Rather than completely
specifying standard blocks, we use the notation
block(inputs; parameters), where we give an indi-
cation of the relevant parameter set. This makes it
easier to visually track which model a component
belongs to.

A.1 Source Language Model
The source language model consists of a sequence
of categorical draws for i = 1, . . . , |x|

Xi|z, x<i ∼ Cat(gθ(z, x<i)) (13)

parameterised by a single-layer recurrent neural
network using GRU units:

fi = emb(xi; θemb-x) (14a)

h0 = tanh(affine(z; θinit-lm)) (14b)

hi = GRU(hi−1, fi−1; θgru-lm) (14c)

gθ(z, x<i) = softmax(affine(hi; θout-x)) . (14d)

We initialise the GRU cell with a transformation
(14b) of the stochastic encoding z. For the simple
joint model baseline we initialise the GRU with a
vector of zeros as there is no stochastic encoding
we can condition on in that case.

A.2 Translation Model
The translation model consists of a sequence of
categorical draws for j = 1, . . . , |y|

Yj |z, x, y<j ∼ Cat(fθ(z, x, y<j)) (15)

parameterised by an architecture that roughly fol-
lows Bahdanau et al. (2015). The encoder is a bidi-
rectional GRU encoder (16b) that shares source
embeddings with the language model (14a) and
is initialised with its own projection of the latent
representation put through a tanh activation. The
decoder, also initialised with its own projection of
the latent representation (16d), is a single-layer re-
current neural network with GRU units (16f). At
any timestep the decoder is a function of the pre-
vious state, previous output word embedding, and
a context vector. This context vector (16e) is a
weighted average of the bidirectional source en-
codings, of which the weights are computed by a
Bahdanau-style attention mechanism. The output
of the GRU decoder is projected to the target vo-
cabulary size and mapped to the simplex using a

softmax activation (17) to obtain the categorical
parameters:

s0 = tanh(affine(z; θinit-enc)) (16a)

sm1 = BiGRU(f
m
1 , s0; θbigru-x) (16b)

ej = emb(yj ; θemb-y) (16c)

t0 = tanh(affine(z; θinit-dec)) (16d)

cj = attention(s
m
1 , tj−1; θbahd) (16e)

tj = GRU(tj−1, [cj , ej−1]; θgru-dec) , (16f)

and

fθ(z, x, y<j) = softmax(affine([tj , ej−1, cj ]; θout-y)) .
(17)

In baseline models, recurrent cells are initialised
with a vector of zeros as there is no stochastic en-
coding we can condition on.

A.3 Inference Network
The inference model q(z|x, y, λ) is a diagonal
Gaussian

Z|x, y ∼ N (u, diag(s� s)) (18)

whose parameters are computed by an inference
network. We use two bidirectional GRU encoders
to encode the source and target sentences sepa-
rately. To spare memory, we reuse embeddings
from the generative model (19a-19b), but we pre-
vent updates to those parameters based on gradi-
ents of the inference network, which we indicate
with the function detach. To obtain fixed-size rep-
resentations for the sentences, GRU encodings are
averaged (19c-19d) .

fm1 = detach(emb(x
m
1 ; θemb-x)) (19a)

en1 = detach(emb(y
n
1 ; θemb-y)) (19b)

hx = avg
(
BiGRU

(
fm1 ;λgru-x

))
(19c)

hy = avg
(
BiGRU

(
en1 ;λgru-y

))
(19d)

hxy = concat(hx,hy) (19e)

hu = ReLU(affine(hxy;λu-hid)) (19f)

hs = ReLU(affine(hxy;λs-hid) (19g)

u = affine(hu;λu-out) (19h)

s = softplus(affine(hs;λs-out)) (19i)

We use a concatenation hxy of the average source
and target encodings (19e) as inputs to compute
the parameters of the Gaussian approximate pos-
terior, namely, d-dimensional location and scale
vectors. Both transformations use ReLU hidden
activations (Nair and Hinton, 2010), but locations



136

live in Rd and therefore call for linear output acti-
vations (19h), whereas scales live in Rd>0 and call
for strictly positive outputs (19i), we follow Ku-
cukelbir et al. (2017) and use softplus. The com-
plete set of parameters used for inference is thus
λ = {λgru-x, λgru-y, λu-hid, λu-out, λs-hid, λs-out}.

A.4 Prediction Network

The prediction network parameterises our pre-
diction model r(z|x, φ), a variant of the infer-
ence model that conditions on the source sentence
alone. In §4 we explore several variants of the
ELBO using different parameterisations of rφ. In
the simplest case we do not condition on the target
sentence during training, thus we can use the same
network both for training and prediction. The net-
work is similar to the one described in A.3, except
that there is a single bidirectional GRU and we use
the average source encoding (19c) as input to the
predictors for u and s (20c-20d).

hu = ReLU(affine(hx;λu-hid)) (20a)

hs = ReLU(affine(hx;λs-hid)) (20b)

u = affine(hu;λu-out) (20c)

s = softplus(affine(hs;λs-out)) (20d)

In all other cases we use q(z|x, y, λ) parame-
terised as discussed in A.3 for training, and design
a separate network to parameterise rφ for predic-
tion. Much like the inference model, the predic-
tion model is a diagonal Gaussian

Z|x ∼ N (û, diag(̂s� ŝ)) (21)

also parameterised by d-dimensional location and
scale vectors, however in predicting û and ŝ (22d-
22e) it can only access an encoding of the source
(22a).

hx = avg
(
BiGRU

(
fm1 ;φgru-x

))
(22a)

hu = ReLU(affine(hx;φu-hid)) (22b)

hs = ReLU(affine(hx;φs-hid)) (22c)

û = affine(hu;φu-out) (22d)

ŝ = softplus(affine(hs;φs-out)) (22e)

The complete set of parameters is then φ =
{φgru-x, φu-hid, φu-out, φs-hid, φs-out}. For the deter-
ministic variant, we use û (22d) alone to approxi-
mate u (19h), i.e. the posterior mean of Z.

B Graphical models

Figure 1 is a graphical depiction of our AEVNMT
model. Circled nodes denote random variables
while uncircled nodes denote deterministic quan-
tities. Shaded random variables correspond to ob-
servations and unshaded random variables are la-
tent. The plate denotes a dataset of |D| observa-
tions.

yx

θ

z

|D|

(a) Generative model

yx

z λ

|D|

(b) Inference model

Figure 1: On the left we have AEVNMT, a generative
model parameterised by neural networks. On the right
we show an independently parameterised model used
for approximate posterior inference.

In Figure 2a, we illustrate the precise statisti-
cal assumptions of AEVNMT. Here plates iterate
over words in either the source or the target sen-
tence. Note that the arrow from xi to yj states that
the jth target word depends on all of the source
sentence, not on the ith source word alone, and
that is the case because xi is within the source
plate. In Figure 2b, we illustrate the statistical
dependencies induced in the marginal distribution
upon marginalisation of latent variables. Recall
that the marginal is the distribution which by as-
sumption produced the observed data. Now com-
pare that to the distribution modelled by the sim-
ple JOINT model (Figure 2c). Marginalisation in-
duces undirected dependencies amongst random
variables creating more structure in the marginal
distribution. In graphical models literature this
is known as moralisation (Koller and Friedman,
2009).



137

xix<i

yjy<j

z

|x|

|y|

(a) Joint distribution of AEVNMT

yjy<j y>j

xix<i x>i

|x|

|y|

(b) Marginal distribution of AEVNMT

xix<i

yjy<j

|x|

|y|

(c) Joint distribution modelled without latent variables

Figure 2: Here we zoom in into the model of Figure 1a
to show the statistical dependencies between observed
variables. In the joint distribution (top), we have the
directed dependency of a source word on all of the pre-
vious source words, and similarly, of a target word on
all of the previous target words in addition to the com-
plete source sentence. Besides, all observations depend
directly on the latent variable Z. Marginalisation of Z
(middle) ties all variables together through undirected
connections. At the bottom we show the distribution
we get if we model the data distribution directly with-
out latent variables.

C Robustness to out-of-domain data

We use our stronger models, those trained on gold-
standard NC bilingual data and synthetic News
data, to translate test sets in various unseen gen-
res. These data sets are collected and distributed
by TAUS,8 and have been used in scenarios of ada-
pation to all domains at once (Cuong et al., 2016).
Table 6 shows the performance of AEVNMT and
the conditional baseline. The first thing to note
is the remarkable drop in performance showing
that without taking domain adaptation seriously
both models are inadequate. In terms of BLEU,
differences range from −0.3 to 0.8 (EN-DE) and
0.3 to 0.7 (DE-EN) and are mostly in favour of
AEVNMT, though note the increased standard de-
viations.

8TAUS Hardware, TAUS Software, TAUS Industrial
Electronics, TAUS Professional & Business Services, and
TAUS Legal available from TAUS data cloud http://
tausdata.org/.

http://tausdata.org/
http://tausdata.org/


138

E
N

-D
E

D
E

-E
N

M
od

el
D

om
ai

n
B

L
E

U
↑

M
E

T
E

O
R
↑

T
E

R
↓

B
E

E
R
↑

B
L

E
U
↑

M
E

T
E

O
R
↑

T
E

R
↓

B
E

E
R
↑

C
O

N
D

C
om

pu
te

rH
ar

dw
ar

e
8
.7

(0
.3

)
23
.6

(0
.2

)
82
.3

(4
.0

)
44
.2

(0
.3

)
13
.0

(0
.4

)
2
0
.8

(0
.1

)
73
.5

(3
.6

)
49
.2

(0
.1

)
C

om
pu

te
rS

of
tw

ar
e

7
.9

(0
.2

)
22
.9

(0
.1

)
83
.6

(3
.2

)
44
.1

(0
.3

)
11
.8

(0
.8

)
2
0
.4

(0
.3

)
80
.8

(7
.0

)
4
8
.6

(0
.3

)
In

du
st

ri
al

E
le

ct
ro

ni
cs

7
.6

(0
.3

)
21
.8

(0
.2

)
89
.6

(5
.5

)
42
.7

(0
.3

)
10
.1

(1
.0

)
17
.6

(0
.4

)
8
5
.7

(9
.7

)
46
.0

(0
.4

)
Pr

of
es

si
on

al
&

B
us

in
es

s
Se

rv
ic

es
8
.1

(0
.4

)
2
3
.3

(0
.4

)
7
5
.6

(0
.4

)
4
3
.1

(0
.4

)
11
.1

(0
.2

)
20
.5

(0
.1

)
73
.8

(1
.4

)
44
.9

(0
.1

)
L

eg
al

12
.3

(0
.1

)
29
.7

(0
.3

)
75
.3

(0
.5

)
49
.6

(0
.1

)
1
4.

2
(0
.3

)
23
.0

(0
.3

)
6
7
.3

(2
.1

)
50
.8

(0
.3

)

A
E

V
N

M
T

C
om

pu
te

rH
ar

dw
ar

e
9
.1

(0
.3

)
2
3
.9

(0
.4

)
7
9
.4

(1
.3

)
4
4
.7

(0
.2

)
1
3
.6

(0
.3

)
2
1
.0

(0
.2

)
7
0
.6

(1
.4

)
4
9
.3

(0
.2

)
C

om
pu

te
rS

of
tw

ar
e

8
.3

(0
.1

)
2
3
.2

(0
.2

)
8
1
.4

(0
.7

)
4
4
.5

(0
.2

)
1
2
.2

(0
.5

)
2
0
.7

(0
.3

)
7
7
.0

(3
.7

)
4
8
.6

(0
.3

)
In

du
st

ri
al

E
le

ct
ro

ni
cs

8
.1

(0
.2

)
2
2
.2

(0
.3

)
8
5
.2

(1
.0

)
4
3
.1

(0
.2

)
1
0
.8

(0
.6

)
1
7
.9

(0
.3

)
7
9
.9

(4
.9

)
4
6
.3

(0
.2

)
Pr

of
es

si
on

al
&

B
us

in
es

s
Se

rv
ic

es
7
.8

(0
.7

)
23
.0

(0
.7

)
75
.7

(1
.0

)
43
.0

(0
.5

)
1
1
.5

(0
.2

)
2
0
.6

(0
.2

)
7
3
.1

(0
.7

)
4
5
.1

(0
.1

)
L

eg
al

1
3
.1

(0
.2

)
3
0
.5

(0
.3

)
7
4
.4

(0
.6

)
4
9
.9

(0
.2

)
1
4
.5

(0
.2

)
2
3
.2

(0
.2

)
6
6
.1

(1
.3

)
5
0
.9

(0
.3

)

Ta
bl

e
6:

Pe
rf

or
m

an
ce

of
m

od
el

s
tr

ai
ne

d
w

ith
N

C
an

d
ba

ck
-t

ra
ns

la
te

d
N

ew
s

on
va

ri
ou

s
TA

U
S

te
st

se
ts

:w
e

re
po

rt
av

er
ag

e
(1

st
d)

ac
ro

ss
5

in
de

pe
nd

en
tr

un
s.



139

D Validation results

WMT16 IWSLT14

EN-DE DE-EN EN-DE DE-EN

COND 14.5 (0.2) 16.9 (0.2) 25.1 (0.1) 30.8 (0.1)
JOINT 14.8 17.1 25.2 31.0
AEVNMT 14.8 (0.2) 17.4 (0.2) 25.7 (0.0) 31.4 (0.0)

Table 7: Validation results reported in BLEU for in-domain training on NC and IWSLT: we report average (1std)
across 5 independent runs for COND and AEVNMT, but a single run of JOINT.

WMT & IWSLT EN-DE DE-EN

COND 20.5 (0.1) 25.9 (0.1)
JOINT 20.7 26.1
AEVNMT 20.8 (0.1) 26.1 (0.1)

Table 8: Validation results reported in BLEU for mixed-domain training: we report average (1std) across 5 inde-
pendent runs for COND and AEVNMT, but a single run of JOINT. The validation set used is a concatenation of
the development sets from WMT and IWSLT.

WMT16 EN-DE DE-EN

COND 14.5 (0.2) 16.9 (0.2)
+ synthetic data 17.4 (0.1) 21.8 (0.1)

JOINT + synthetic data 17.3 21.8
AEVNMT + synthetic data 17.6 (0.1) 22.1 (0.1)

Table 9: Validation results reported in BLEU for training on NC plus synthetic data: we report average (1std)
across 5 independent runs for COND and AEVNMT, but a single run of JOINT.



140

E Additional Metrics

IW
SL

T
14

E
N

-D
E

D
E

-E
N

B
L

E
U
↑

M
E

T
E

O
R
↑

T
E

R
↓

B
E

E
R
↑

B
L

E
U
↑

M
E

T
E

O
R
↑

T
E

R
↓

B
E

E
R
↑

C
O

N
D

23
.0

(0
.1

)
42
.4

(0
.1

)
56
.0

(0
.1

)
58
.6

(0
.1

)
27
.3

(0
.2

)
3
0
.3

(0
.1

)
5
2
.4

(0
.5

)
59
.8

(0
.1

)
JO

IN
T

2
3
.2

4
2
.8

56
.1

58
.7

27
.5

3
0.

3
52
.7

59
.8

A
E

V
N

M
T

2
3
.4

(0
.1

)
4
2
.8

(0
.2

)
5
5
.5

(0
.3

)
5
8
.8

(0
.1

)
2
8
.0

(0
.1

)
3
0
.6

(0
.1

)
5
1
.2

(0
.6

)
6
0
.1

(0
.1

)

Ta
bl

e
10

:
Te

st
re

su
lts

fo
r

in
-d

om
ai

n
tr

ai
ni

ng
on

IW
SL

T:
w

e
re

po
rt

av
er

ag
e

(1
st

d)
ac

ro
ss

5
in

de
pe

nd
en

t
ru

ns
fo

r
C

O
N

D
an

d
A

E
V

N
M

T
,b

ut
a

si
ng

le
ru

n
of

JO
IN

T
.

W
M

T
16

E
N

-D
E

D
E

-E
N

B
L

E
U
↑

M
E

T
E

O
R
↑

T
E

R
↓

B
E

E
R
↑

B
L

E
U
↑

M
E

T
E

O
R
↑

T
E

R
↓

B
E

E
R
↑

C
O

N
D

17
.8

(0
.2

)
35
.9

(0
.2

)
65
.2

(0
.4

)
53
.1

(0
.1

)
20
.1

(0
.1

)
2
6
.0

(0
.1

)
62
.0

(0
.3

)
5
3
.7

(0
.1

)
JO

IN
T

17
.9

3
6.

2
64
.1

53
.4

20
.1

2
6
.0

62
.7

5
3
.7

A
E

V
N

M
T

1
8
.4

(0
.2

)
3
6
.6

(0
.2

)
6
4
.0

(0
.4

)
5
3
.5

(0
.1

)
2
0
.6

(0
.2

)
2
6
.0

(0
.1

)
6
0
.7

(0
.8

)
53
.6

(0
.1

)

Ta
bl

e
11

:T
es

tr
es

ul
ts

fo
ri

n-
do

m
ai

n
tr

ai
ni

ng
on

N
C

:w
e

re
po

rt
av

er
ag

e
(1

st
d)

ac
ro

ss
5

in
de

pe
nd

en
tr

un
s

fo
r

C
O

N
D

an
d

A
E

V
N

M
T

,b
ut

a
si

ng
le

ru
n

of
JO

IN
T

.

W
M

T
16

E
N

-D
E

D
E

-E
N

B
L

E
U
↑

M
E

T
E

O
R
↑

T
E

R
↓

B
E

E
R
↑

B
L

E
U
↑

M
E

T
E

O
R
↑

T
E

R
↓

B
E

E
R
↑

C
O

N
D

17
.8

(0
.2

)
35
.9

(0
.2

)
65
.2

(0
.4

)
53
.1

(0
.1

)
20
.1

(0
.1

)
2
6
.0

(0
.1

)
6
2
.0

(0
.3

)
5
3.

7
(0
.1

)
+

sy
nt

he
tic

da
ta

22
.3

(0
.3

)
40
.9

(0
.2

)
58
.5

(0
.5

)
5
7
.0

(0
.2

)
26
.9

(0
.2

)
30
.4

(0
.1

)
53
.0

(0
.5

)
58
.5

(0
.1

)
JO

IN
T

+
sy

nt
he

tic
da

ta
22
.2

40
.8

5
8
.1

5
7
.0

2
6.

7
30
.2

5
2.

1
5
8
.6

A
E

V
N

M
T

+
sy

nt
he

tic
da

ta
2
2
.5

(0
.2

)
4
1
.0

(0
.1

)
5
8
.1

(0
.2

)
5
7
.0

(0
.1

)
2
7
.4

(0
.2

)
3
0
.6

(0
.1

)
5
2
.0

(0
.1

)
5
8
.8

(0
.1

)

Ta
bl

e
12

:
Te

st
re

su
lts

fo
r

tr
ai

ni
ng

on
N

C
pl

us
sy

nt
he

tic
da

ta
(b

ac
k-

tr
an

sl
at

ed
N

ew
s

C
ra

w
l)

:
w

e
re

po
rt

av
er

ag
e

(1
st

d)
ac

ro
ss

5
in

de
pe

nd
en

tr
un

s
fo

r
C

O
N

D
an

d
A

E
V

N
M

T
,b

ut
a

si
ng

le
ru

n
of

JO
IN

T
.



141

E
N

-D
E

W
M

T
16

IW
SL

T
14

Tr
ai

ni
ng

M
od

el
B

L
E

U
↑

M
E

T
E

O
R
↑

T
E

R
↓

B
E

E
R
↑

B
L

E
U
↑

M
E

T
E

O
R
↑

T
E

R
↓

B
E

E
R
↑

E
N

-D
E

C
O

N
D

1
7.

6
(0
.4

)
3
5
.7

(0
.3

)
61
.9

(0
.7

)
53
.9

(0
.2

)
23
.9

(0
.3

)
4
3
.1

(0
.3

)
5
4
.7

(0
.2

)
59
.3

(0
.1

)
JO

IN
T

1
8.

1
36
.3

61
.4

54
.3

2
4
.2

4
3.

4
54
.5

5
9
.5

A
E

V
N

M
T

1
8
.4

(0
.2

)
3
6
.6

(0
.3

)
6
0
.9

(0
.4

)
5
4
.5

(0
.2

)
24
.1

(0
.3

)
4
3
.5

(0
.3

)
5
4
.0

(0
.3

)
5
9
.5

(0
.2

)

D
E

-E
N

C
O

N
D

2
1.

6
(0
.2

)
27
.4

(0
.1

)
59
.9

(0
.7

)
55
.5

(0
.2

)
29
.1

(0
.2

)
3
1
.5

(0
.1

)
50
.9

(0
.5

)
6
0.

9
(0
.1

)
JO

IN
T

2
2
.3

2
7.

4
58
.8

5
5
.6

2
9
.2

3
1
.5

4
9
.2

6
1
.2

A
E

V
N

M
T

2
2
.3

(0
.1

)
2
7
.5

(0
.1

)
5
7
.8

(0
.6

)
5
5
.6

(0
.1

)
2
9
.2

(0
.1

)
3
1
.5

(0
.0

)
4
9
.2

(0
.4

)
61
.1

(0
.1

)

Ta
bl

e
13

:T
es

tr
es

ul
ts

fo
rm

ix
ed

-d
om

ai
n

tr
ai

ni
ng

:w
e

re
po

rt
av

er
ag

e
(1

st
d)

ac
ro

ss
5

in
de

pe
nd

en
tr

un
s

fo
r

C
O

N
D

an
d

A
E

V
N

M
T

,b
ut

a
si

ng
le

ru
n

of
JO

IN
T

.


