



















































Implementation of a Chomsky-Schützenberger n-best parser for weighted multiple context-free grammars


Proceedings of NAACL-HLT 2019, pages 178–191
Minneapolis, Minnesota, June 2 - June 7, 2019. c©2019 Association for Computational Linguistics

178

Implementation of a Chomsky-Schützenberger n-Best Parser
for Weighted Multiple Context-Free Grammars

Thomas Ruprecht and Tobias Denkinger
Faculty of Computer Science

Technische Universität Dresden
01062 Dresden, Germany

{thomas.ruprecht,tobias.denkinger}@tu-dresden.de

Abstract

Constituent parsing has been studied ex-
tensively in the last decades. Chomsky-
Schützenberger parsing as an approach to con-
stituent parsing has only been investigated the-
oretically, yet. It uses the decomposition of
a language into a regular language, a ho-
momorphism, and a bracket language to di-
vide the parsing problem into simpler subprob-
lems. We provide the first implementation of
Chomsky-Schützenberger parsing. It employs
multiple context-free grammars and incorpo-
rates many refinements to achieve feasibility.
We compare its performance to state-of-the-art
grammar-based parsers.

1 Introduction

The description of the syntax of natural lan-
guages (such as Danish, English, and German)
with the help of formal grammars has been stud-
ied since Chomsky (1956). With a formal gram-
mar, computers can calculate a syntactic represen-
tation (called parse) of a sentence in a natural lan-
guage. Of the grammar classes in the Chomsky hi-
erarchy (Chomsky, 1959), context-free grammars
(short: CFGs) lack the expressive power necessary
to model natural languages (Shieber, 1985) and
parsing with context-sensitive grammars cannot be
done efficiently (i.e. in polynomial time). This led
to the introduction of a series of classes of mildly
context-sensitive grammars (Joshi, 1985) that al-
low parsing in polynomial time but also capture an
increasing amount of phenomena present in nat-
ural languages. Tree adjoining grammars (Joshi
et al., 1975), linear context-free string-rewriting
systems (short: LCFRSs, Vijay-Shanker et al.,
1987), and multiple CFGs (short: MCFGs, Seki
et al., 1991) are among those classes.

Chomsky-Schützenberger (short: CS) parsing
was introduced by Hulden (2011) for CFGs
and extended to MCFGs by Denkinger (2017).

It uses a classical theorem by Chomsky and
Schützenberger (1963, or the generalisation by
Yoshinaka et al., 2010), which states that the lan-
guage L(G) of a CFG (or an MCFG) G can be
represented by a regular language R, a homo-
morphism h, and a Dyck language (resp. multiple
Dyck language) D such that L(G) = h(R ∩ D).
The elements of R∩D correspond to parses in G.
For a sentence w, a CS parser calculates the ele-
ments of h−1(w)∩R∩D and transforms them into
parses. CS parsing can be viewed as a coarse-to-
fine mechanism whereR corresponds to the coarse
grammar and R ∩ D to the fine grammar. The
respective coarse-to-fine pipeline consists of (con-
ceptually) simple operations such as h−1 or the in-
tersection with R, which provides great flexibility.
The flexibility is used to provide a fallback mech-
anism in case a finer stage of the pipeline rejects
all proposals of a coarser stage. It also permits CS
parsing in a broader setting than usual (for pars-
ing) with minimal modification (see sec. 6).

We suspected that the coarse-to-fine view on CS
parsing leads to an efficient implementation. Since
initial tests revealed that the original algorithm
for MCFGs (Denkinger, 2017, alg. 3, recalled in
sec. 2) is not feasible in practice, we explore nu-
merous optimisations (sec. 4), one of which is the
use of a context-free approximation of the multiple
Dyck language D. We introduce component-wise
derivations (sec. 3) to relate this context-free ap-
proximation to D. Employing the optimisations,
we provide the first implementation of a CS parser.
In sec. 5, we compare our parser’s performance to
Grammatical Framework (Angelov and Ljunglöf,
2014), rparse (Kallmeyer and Maier, 2013), and
disco-dop (van Cranenburgh et al., 2016). We re-
strict our comparison to (discontinuous) grammar-
based parsers (excluding e.g. transition systems,
Maier, 2015, Coavoux and Crabbé, 2017) since the
principle of CS parsing requires a grammar.



179

2 Preliminaries

The sets of non-negative integers and positive in-
tegers are denoted by N and N+, respectively. We
abbreviate {1, . . . , n} by [n] for each n ∈ N.

Let A and B be sets. The powerset of A and the
set of (finite) strings over A are denoted by P(A)
and A∗, respectively. The set of possibly infinite
sequences of elements of A is denoted by Aω. A
partition of A is a set P ⊆ P(A) whose elements
(called cells) are non-empty, pairwise disjoint, and
cover A (i.e.

⋃
p∈P p = A). For each a ∈ A and

each equivalence relation ≈ on A, we denoted the
equivalence class of a w.r.t. ≈ by [a]≈. The set
of functions from A to B is denoted by A → B.
Note that (A→ B) ⊆ (A×B). The composition
of two binary relations R1 and R2 is R2 ◦ R1 =
{(a, c) | ∃b: (a, b) ∈ R1, (b, c) ∈ R2}.

Finite state automata. We assume that the
reader is familiar with finite state automata. For
details, we refer to Hopcroft and Ullman (1979).

A finite state automaton (short: FSA) is a tuple
A = (Q,∆, qi, qf, T ) where Q and ∆ are finite
sets (states and terminals, respectively), qi, qf ∈ Q
(initial and final state, respectively), and T ⊆ Q×
∆∗×Q is finite (transitions). We call q the source
and q′ the target of a transition (q, u, q′). A run
is a string θ of transitions such that the target of a
transition is the source of the next transition in θ.
The language of A is denoted by L(A).

Sorts. Sorts are a widespread concept in com-
puter science: one can think of sorts as data types
in a programming language. Let S be a set (of
sorts). An S-sorted set is a tuple (Ω, sort) where
Ω is a set and sort :Ω → S. We abbreviate
(Ω, sort) by Ω and sort−1(s) by Ωs for s ∈ S.

Now let Ω be an (S∗ × S)-sorted set. The
set of trees over Ω is the S-sorted set TΩ where
(TΩ)s = {ω(t1, . . . , tk) | s1, . . . , sk ∈ S, ω ∈
Ω(s1···sk,s), t1 ∈ (TΩ)s1 , . . . , tk ∈ (TΩ)sk} for
each s ∈ S.

Multiple context-free grammars. A rule of a
context-free grammar has the ability to concate-
nate the strings generated by its right-hand side
non-terminals. Multiple context-free grammars
extend this ability to concatenating string-tuples.
This is done with the help of composition func-
tions. Let Σ be a finite set. A composition
function w.r.t. Σ is a function c that takes tu-
ples of strings over Σ as arguments and returns

a tuple of strings over Σ (i.e. there are k ∈ N
and s1, . . . , sk, s ∈ N+ such that c: (Σ∗)s1 ×
. . . × (Σ∗)sk → (Σ∗)s), and is defined by an
equation c((x11, . . . , x

s1
1 ), . . . , (x

1
k, . . . , x

sk
k )) =

(u1, . . . , us) where u1, . . . , us are strings of x
j
i ’s

and symbols from Σ. We call c linear if each
xji occurs at most once in u1 · · ·us. We some-
times write [u1, . . . , us] instead of c. Furthermore,
setting sort(c) = (s1 · · · sk, s), the composition
functions w.r.t. Σ form a sorted set.

The following example shows how linear com-
position functions are used in the rules of a multi-
ple context-free grammar.

Example 1. Consider G = (N,Σ, S, P ) where
N = {S,A,B} and Σ = {a, b, c, d} are finite
sets (non-terminals and terminals, respectively),
S ∈ N (initial non-terminal) and P is a finite set
(rules) that contains the following five objects:

ρ1 = S → [x11x12x21x22](A,B)
ρ2 = A→ [ax11, cx21](A) ρ4 = A→ [ε, ε]()
ρ3 = B → [bx11,dx21](B) ρ5 = B → [ε, ε]().

We call G a multiple context-free grammar. Con-
sider the rule ρ1. Similar to a rule of a context-
free grammar, ρ1 rule has one left-hand side non-
terminal (S) and zero or more right-hand side
non-terminals (A and B). A derivation of G can
be build by combining rules in P to form a tree
according to their left- and right-hand side non-
terminals. If a derivation starts with the initial
non-terminal (here S), then it is called complete.
Hence, each complete derivation inG has the form
dm,n = ρ1(ρ

m
2 (ρ4), ρ

n
3 (ρ5)) for some m,n ∈ N.

If we replace each rule in a derivation by its com-
position function, we obtain a term of composition
functions which can be evaluated. We call the re-
sulting value the yield of a derivation. A derivation
dm,n has yield yd(dm,n) = ambncmdn. The set of
yields of all complete derivations is the language
of G: L(G) = {ambncmdn | m,n ∈ N}. �

The following definition formalises the notions
of ex. 1 and introduces some additional concepts.

Definition 2 (Seki et al., 1991). A multiple
context-free grammar (short: MCFG) is a tuple
G = (N,Σ, S, P ) where N is a finite N+-sorted
set (non-terminals), Σ is a finite set (terminals),
S ∈ N1 (initial non-terminal), P is a finite
(N∗×N)-sorted set of strings ρ of the form A→
c(B1, . . . , Bk) such that A,B1, . . . , Bk ∈ N , c
is a linear composition function, and sort(c) =



180

(sort(B1) · · · sort(Bk), sort(A)). The sort of ρ
is (B1 · · ·Bk, A). The left-hand side (short: lhs)
of ρ is A. The fanout of ρ is fanout(ρ) = sort(A)
and the rank of ρ is rank(ρ) = k. The elements of
P are called rules.

The set of derivations (resp. complete deriva-
tions) of G is DG = TP (resp. DcG = (TP )S). Let
w ∈ Σ∗ and d = ρ(d1, . . . , dk) ∈ DG with ρ =
A → c(B1, . . . , Bk). The yield of d is yd(d) =
c(yd(d1), . . . , yd(dk)). The set of derivations of
w inG is DcG(w) = yd

−1(w)∩DcG. The language
of A in G is L(G,A) = {yd(d) | d ∈ (TP )A}.
The language of G is L(G) = L(G,S). Any lan-
guage generated by an MCFG is called multiple
context-free (short: mcf). �

A context-free grammar (short: CFG) is an
MCFG where each non-terminal has sort 1.
Each rule of a CFG has the form A →
[u0x

1
i(1)u1 · · ·x

1
i(n)un](B1, . . . , Bk). We abbrevi-

ate this rule by A→ u0Bi(1)u1 · · ·Bi(n)un.

Weighted multiple context-free grammars. A
weighted MCFG is obtained by assigning a
weight to each rule of an (unweighted) MCFG.
In this paper, the weights will be taken from a
partially ordered commutative monoid with zero
(short: POCMOZ). A POCMOZ is an algebra
(M,�, 1, 0,�) where � is a partial order on M ;
� is associative, commutative, decreasing (i.e.
m�m′�m), and monotonous (i.e. m1 �m2 im-
pliesm1�m�m2�m); 1 is neutral w.r.t.�; and 0
is absorbing w.r.t.�. We callM factorisable if for
each m ∈ M \ {1}, there are m1,m2 ∈ M \ {1}
with m = m1 � m2. The probability algebra
Pr = ([0, 1], ·, 1, 0,≤) is a factorisable POCMOZ
where r =

√
r ·
√
r for each r ∈ [0, 1).

Example 3 (continues ex. 1). Consider the tu-
ple (G,µ) where µ:P → Pr is a function where
µ(ρ1) = 1, µ(ρ2) = µ(ρ4) = 1/2, µ(ρ3) = 1/3,
and µ(ρ5) = 2/3. We call (G,µ) a weighted
MCFG. The weight of the a derivation dm,n is ob-
tained by multiplying the weights of all rule occur-
rences in it: wt(dm,n) = 1/2m+1 · 2/3n+1. �
Definition 4. A weighted MCFG (short: wMCFG)
is a tuple (G,µ) where G = (N,Σ, S, P ) is an
MCFG (underlying MCFG), µ:P → M \ {0}
(weight assignment), and (M,�, 1, 0,�) is a fac-
torisable POCMOZ.

(G,µ) inherits all objects associated with G.
Let d = ρ(d1, . . . , dk) ∈ DG. The weight of d
is wt(d) = µ(ρ)�

⊙k
i=1 wt(di). �

For the rest of this paper, we fix a wMCFG
(G,µ) with underlying MCFG G = (N,Σ, S, P )
and weight assignment µ:P →M .

Chomsky-Schützenberger theorem. In the
Chomsky-Schützenberger theorem for CFGs (cf.
sec. 1), D contains strings of brackets where each
opening bracket is matched by the corresponding
closing bracket. This property can be described
with an equivalence relation. Let ∆ be a set (of
opening brackets) and s∆ be the set (of closing
brackets) that contains sδ for each δ ∈ ∆. We de-
fine≡∆ as the smallest equivalence relation where
u δsδ v ≡∆ uv for each δ ∈ ∆ and u, v ∈ ∆∗. The
Dyck language w.r.t. ∆ is D∆ = [ε]≡∆ .

In the Chomsky-Schützenberger representation
for MCFGs, the brackets fulfil three functions:
(i) terminal brackets JσKσ stand for a terminal
symbol σ, (ii) component brackets J`ρ and K`ρ de-
note beginning and end of substrings produced
by the `-th component of a rule ρ, and (iii) vari-
able brackets Jjρ,i and K

j
ρ,i denote beginning and

end of substrings produced by variable xji in a
rule ρ. As for CFGs, each opening bracket must
be matched by the corresponding closing bracket.
Furthermore, because applying a rule of an MCFG
produces multiple strings simultaneously, we need
to ensure that the brackets corresponding to the
same application of a rule occur simultaneously.
This is described with another equivalence rela-
tion. Let P be a partition of ∆. Intuitively, each
cell of P is a set of (opening) brackets that oc-
cur simultaneously. We define ≡P as the smallest
equivalence relation on P

(
(∆ ∪ ∆)∗

)
where for

each {δ1, . . . , δs} ∈ P with |{δ1, . . . , δs}| = s,
u0, . . . , us, v1, . . . , vs ∈ D∆, and L ⊆ (∆ ∪∆)∗:{

u0 δ1v1 sδ1 u1 · · · δsvs sδs us
}
∪ L

≡P
{
u0 · · ·us, v1 · · · vs

}
∪ L.

The multiple Dyck language w.r.t. P is mDP =⋃
(L | L ∈ [{ε}]≡P). Note that mDP ⊆ D∆.
Theorem 5 provides a representation of each

mcf language by a multiple Dyck language (see
above), a recognisable language (to ensure local
consistency), and a homomorphism (to decode the
bracket sequences into terminal strings). The cor-
responding construction is recalled in def. 6.
Theorem 5 (cf. Yoshinaka et al., 2010, thm. 3).
For every mcf language L ⊆ Σ∗ there are a homo-
morphism h: (∆ ∪ s∆)∗ → Σ∗, a regular language
R ⊆ (∆ ∪ s∆)∗, and a multiple Dyck language
mD ⊆ (∆ ∪ s∆)∗ such that L = h(R ∩mD). �



181

Definition 6 (Denkinger, 2017, def. 3.6, 4.9, 5.15).
The multiple Dyck language w.r.t. G is mDG =
mDPG where PG is the smallest set that contains
the cell

{
Jσ
}

for each σ ∈ Σ and the cells
{
J`ρ |

` ∈ [sort(A)]
}

and
{
Jjρ,i | j ∈ [sort(Bi)]

}
for

each ρ = A → c(B1, . . . , Bk) ∈ P and i ∈ [k].
Let ∆G =

⋃
p∈PG p. We denote the elements of

Ě∆G by closing brackets, e.g. sJσ = Kσ, and let
ΩG = ∆G ∪Ě∆G.

The homomorphism w.r.t.G, denoted by homG,
is the unique extension of h:ΩG → Σ ∪ {ε} to
strings where h(δ) = σ if δ is of the form Jσ and
h(δ) = ε otherwise.

The automaton w.r.t. G, denoted by AG, is the
FSA (Q,ΩG, S1,ĎS1, T ) where Q =

{
A`,ĎA` |

A ∈ N, ` ∈ [sort(A)]
}

and T is the smallest set
such that for each rule ρ ∈ P of the form

A→ [u1,0y1,1u1,1 · · · y1,n1u1,n1 , . . . ,
us,0ys,1us,1 · · · ys,nsus,ns ](B1, . . . , Bk)

where the ys are elements of X and the us
are elements of Σ∗, we have (abbreviating
Jσ1Kσ2 · · · JσkKσk by ˜σ1 · · ·σk) the following tran-
sitions in T :

(i)
(
A`, J`ρũ`,0K`ρ, sA`

)
∈ T for every ` ∈ [s]

with n` = 0,

(ii)
(
A`, J`ρũ`,0Jjρ,i, B

j
i

)
∈ T for every ` ∈ [s]

where n` 6= 0 and y`,1 is of the form xji ,

(iii)
(

sBji , K
j
ρ,iũ`,κJ

j′

ρ,i′ , B
j′

i′
)
∈ T for every ` ∈

[s] and κ ∈ [n` − 1] where y`,κ is of the form
xji and y`,κ+1 is of the form x

j′

i′ , and

(iv)
(

sBji , K
j
ρ,iũ`,n`K`ρ, sA`

)
∈ T for every ` ∈

[s] where n` 6= 0 and y`,n` is of the form x
j
i .

We abbreviate L(AG) byRG. �
Example 7 (continues ex. 1). The automaton w.r.t.
G is shown in fig. 1. An illustration of the applica-
tion of ≡PG is given in the appendix (p. 11). �

The vanilla parser. The vanilla parser (i.e.
alg. 3 from Denkinger, 2017), is shown in fig. 2
(top). Similar to the parser proposed by Hulden
(2011), we divide it in three essential phases:
(i) FSA constructions for the intersection of
hom−1G (w) and RG, (ii) an extraction of (in our
case multiple) Dyck words from the intersection,
and (iii) the conversion of words into derivations.

S1

start

A1
J1ρ1J1ρ1,1

J1ρ2 ãJ1ρ2,1

A1
J1ρ4K1ρ4

K1ρ2,1K1ρ2

B1

K1ρ1,1J1ρ1,2
J1ρ3 b̃J1ρ3,1

B1
J1ρ5K1ρ5

K1ρ3,1K1ρ3

A2
K1ρ1,2J2ρ1,1

J2ρ2 c̃J2ρ2,1

A2

J2ρ4K2ρ4

K2ρ2,1K2ρ2

B2K2ρ1,1J2ρ1,2

J2ρ3 d̃J2ρ3,1

B2J2ρ5K2ρ5

K2ρ3,1K2ρ3

S1

K2ρ1,2K1ρ1

Figure 1: Automaton w.r.t. G, cf. ex. 7.

Formally, the vanilla parser is the function
V :Σ∗ → (DcG)ω defined as

V = MAP(TODERIV) ◦ FILTER(mDG)
◦ SORT(µ′) ◦ (∩ RG) ◦ hom−1G

where hom−1G (w) ∩RG is represented by an FSA
for each w ∈ Σ∗ (phase (i)). µ′(u) is the prod-
uct of the weights of each occurrence of a bracket
of the form J`ρ or K`ρ in u. These weights are fixed
such that µ′

(
J1ρK1ρ · · · J`ρK`ρ) = µ(ρ) for each ρ ∈ P

with fanout `. SORT(µ′) brings the elements of
its argument, which is a subset of ΩG∗, in some
descending order w.r.t. µ′ and �, returning a (pos-
sibly infinite) sequence of elements ofΩG∗, which
we call candidates. Sequences are implemented as
iterators. FILTER(mDG) removes the candidates
from its argument sequence that are not in mDG
while preserving the order (cf. Denkinger, 2017,
alg. 2). (Both steps, SORT(µ′) and FILTER(mDG),
are phase (ii).) TODERIV returns the derivation in
G that corresponds to its argument (which is from
the set RG ∩ mDG), cf. Denkinger (2017, func-
tion fromBrackets, p. 20). MAP(TODERIV) ap-
plies TODERIV to each candidate in its argument
while preserving the order (phase (iii)).

Denkinger (2017, thm. 5.22) showed that
TAKE(n) ◦ V solves the n-best parsing problem.1
We omit the additional restrictions that he imposed
on the given wMCFG because they are only nec-
essary to show the termination of his algorithm.

1In the following, we will gloss over the distinction be-
tween derivations and parses.



182

hom−1G ∩ RG SORT(µ
′) FILTER(mDG) TODERIV

∈ Σ∗ ⊆ ΩG∗ ⊆ ΩG∗ ∈ (ΩG∗)ω ∈ (ΩG∗)ω ∈ (D
c
G)

ω

TOMCFGDERIVEXTRACTDYCK(G,µ′)hom−1G ∩ RG TOCOWDERIV∈ Σ∗ ∈ (ΩG∗)ω ∈ (cowDcG)ω ∈ (DcG)ω

Figure 2: Visualisation of the vanilla parser (top) and the parser with the optimisations from sec. 4 (bottom).

3 Component-Wise Derivations

In sec. 4, we will outline modifications to the
vanilla parser that make the extraction of the
elements of mDG from hom−1G (w) ∩ RG effi-
cient (items 2–4). To facilitate this, we first
decompose FILTER(mDG) into FILTER(mDG) ◦
FILTER(D∆G), which is possible because D∆G ⊇
mDG. Secondly, we implement FILTER(D∆G) ◦
SORT(µ′) with a dynamic programming algo-
rithm (cf. Hulden, 2011, alg. 1, similar to Bar-
Hillel et al., 1961, sec. 8). And lastly, we re-
place FILTER(mDG) by steps that exploit the well-
bracketing of the elements of D∆G .

The elements ofRG ∩D∆G can be represented
as trees over rules of G.2 We label the edges of
those trees to allow us to check if vertices that cor-
respond to the same application of a rule of the
MCFG G match. The resulting objects are called
component-wise derivations. The set RG ∩ D∆G
is characterised in terms a CFG Gcf.

Definition 8. Let ρ ∈ P be a rule of the form
A → [u1, . . . , us](B1, . . . , Bk), ` ∈ [s], and u`
be of the form w0x

j(1)
i(1)w1 · · ·x

j(n)
i(n)wn for some

w0, . . . , wn ∈ Σ∗. We define the rule

ρ(`) = A` →
q`
ρ
w̃0v1w̃1 · · · vnw̃n

y`
ρ

where each vκ = Jj(κ)ρ,i(κ)B
j(κ)
i(κ) K

j(κ)
ρ,i(κ). The context-

free CS approximation of G (short: CFA), de-
noted byGcf, is the CFG (Ncf, ΩG, S1, Pcf) where
Ncf = {A` | A ∈ N, ` ∈ [sort(A)]} and
Pcf = {ρ(`) | ρ ∈ P, ` ∈ [fanout(ρ)]}. �
Observation 9. D∆G ∩RG = L(Gcf). �

We introduce component-wise derivations to re-
late the derivations of Gcf with those of G.

Definition 10. Let ` ∈ N+ and t be a tree whose
vertices are labelled with elements of P and whose
edges are labelled with elements of N+ × N+.
The label at the root of t is denoted by root(t).
The set of labels of the outgoing edges from the

2Those trees correspond to the derivations of the guid-
ing grammar in the coarse-to-fine parsing approach of
Barthélemy et al. (2001, sec. 3).

root of t is denoted by out(t). A (i, j)-subtree
of t, is a sub-graph of t consisting of all the ver-
tices (and their edges) reachable from some tar-
get vertex of the outgoing edge from the root that
is labelled with (i, j). If there is a unique (i, j)-
subtree of t, then we denote it by sub(i,j)(t). Now
let root(t) = A → [u1, . . . , us](B1, . . . , Bk).
We call t an (`-)component-wise derivation, short:
(`-)cow derivation, of G if the following four re-
quirements are met: (i) out(t) contains exactly the
pairs (i, j) such that xji occurs in u`, (ii) a unique
(i, j)-subtree of t exists, (iii) root(sub(i,j)(t)) has
lhs Bi, and (iv) sub(i,j)(t) is a j-cow derivation
for each (i, j) ∈ out(t). We denote the set of cow
derivations of G whose root’s lhs is S by cowDcG.
The set of `-cow derivations whose root’s label has
lhs A is denoted by `-cowDAG. �

An example of a cow derivation is shown in
fig. 3a. The root is the top-most vertex.

Definition 11. Let ρ = A → c(B1, . . . , Bk) ∈
P , ` ∈ [fanout(ρ)], and the `-th component
of c be u0x

j(1)
i(1)u1 · · ·x

j(n)
i(n)un with u1, . . . , un ∈

Σ∗. Furthermore, for each κ ∈ [n], let tκ ∈
j(κ)-cowD

Bi(κ)
G . By ρ

〈
(i(κ), j(κ))/tκ | κ ∈ [n]

〉
,

we denote the cow derivation t such that root(t) =
ρ, out(t) = {(i(κ), j(κ)) | κ ∈ [n]}, and for each
κ ∈ [n]: sub(i(κ),j(κ))(t) = tκ. �

Lemma 12. There is a bijection toCowD between
L(Gcf) and cowDcG. �

Proof sketch. We define the partial func-
tion toCowD from ΩG∗ to cow deriva-
tions of G as follows: toCowD(u) =
ρ
〈
(i(κ), j(κ))/toCowD(vκ) | κ ∈ [n]

〉
if u

is of the form
r`
ρ
ũ0Jj(1)ρ,i(1)v1K

j(1)
ρ,i(1)ũ1 . . . J

j(n)
ρ,i(n)vnK

j(n)
ρ,i(n)ũn

z`
ρ

for some rule ρ = A → c(B1, . . . , Bk) where the
`-th component of c is u0x

j(1)
i(1)u1 · · ·x

j(n)
i(n)un with

u1, . . . , un ∈ Σ∗; otherwise, toCowD(u) is un-
defined. The partial function toCowD is a bijec-
tion between L(Gcf) and cowDcG (proven in ap-
pendix A.2). �



183

Example 13 (continues ex. 1). We construct
Gcf = ({S1, A1, A2, B1, B2}, ΩG, S1, Pcf) where
Pcf contains, among others, the following rules:

ρ
(1)
1 = S

1 →
r1
ρ1

q1
ρ1,1

A1
y1
ρ1,1

q1
ρ1,2

B1
y1
ρ1,2

q2
ρ1,1

A2
y2
ρ1,1

q2
ρ1,2

B2
y2
ρ1,2

z1
ρ1
,

ρ
(1)
3 = B

1 →
r1
ρ3

b̃
q1
ρ3,1

B1
y1
ρ3,1

z1
ρ3
,

ρ
(1)
4 = A

1 → J1ρ4K
1
ρ4 , ρ

(2)
4 = A

2 → J2ρ4K
2
ρ4 ,

ρ
(1)
5 = B

1 → J1ρ5K
1
ρ5 , . . . .

Figure 3a shows the image of the word
r1
ρ1

r1
ρ1,1

q1
ρ4

y1
ρ4

z1
ρ1,1r1

ρ1,2

q1
ρ3

b̃
q1
ρ3,1

J1ρ5K
1
ρ5

y1
ρ3,1

y1
ρ3

z1
ρ1,2r2

ρ1,1

q2
ρ4

y2
ρ4

z2
ρ1,1

r2
ρ1,2

q2
ρ5

y2
ρ5

z2
ρ1,2

z1
ρ1

in L(Gcf) under toCowD. �
In the following, we define a property called

consistency to discern those cow derivations that
correspond to derivations of the MCFG G.

Definition 14. Let s ∈ N+ and t1, . . . , ts be cow
derivations of G. We call the set {t1, . . . , ts} con-
sistent if there is a rule ρ = A→ c(B1, . . . , Bk) ∈
P such that root(t1) = . . . = root(ts) = ρ,
s = sort(A), and for each i ∈ [k]: the set
{sub(i,j)(t`) | ` ∈ [s], j ∈ [sort(Bi)]: (i, j) ∈
out(t`)} is consistent. If s = 1, then we also call
t1 consistent. �

The cow derivation shown in fig. 3a is not con-
sistent. If we consider the set of nodes that is
reachable from the root via edges labelled with a
tuple whose first component is 2 (the right dotted
box), then it is easy to see that the rules at these
nodes are not equal. A consistent cow derivation
is shown in the appendix (fig. 6).

Proposition 15. TODERIV ◦ toCowD−1 is a bi-
jection between the consistent cow derivations in
cowDcG and D

c
G. �

4 Optimisations

In this section, we describe several improvements
to the vanilla parser (cf. end of sec. 2). Since the
definitions ofAG, homG, and mDG do not depend
on the word w, we may compute appropriate rep-
resentations for these objects before the beginning
of the parsing process, and store them persistently.

S → [x11x12x21x22](A,B)

A→ [ε, ε]()
(1, 1)

A→ [ε, ε]()
(1, 2)

B → [bx11,dx21](B)

B → [ε, ε]()
(1, 1)

(2, 1) B → [ε, ε]()
(2, 2)

(a) A cow derivation. The dotted boxes show clusters of
nodes that are reachable from the root via edges labelled with
matching first components.

S → [x11x12x21x22](A,B)

A→ [ε, ε]() B → [bx11, ε](B)

B → [ε, ε]()
(1, 1)

(b) Construction of new rules for each cluster in fig. 3a.
If there were any unused nonterminals in these constructed
rules, they are removed and the indices of variables changed
accordingly. For each cluster, all reachable nodes are clus-
tered via the first component of the labels as in fig. 3a.

S → [x11x12x21x22](A,B)

A→ [ε, ε]() B → [bx11, ε](B)

B → [ε, ε]()

(c) Construction of new rules from the clusters in fig. 3b.

Figure 3: A strategy to convert a non-consistent cow
derivation into a complete derivation of an MCFG.

In the following, we briefly describe each im-
provement that we applied to the vanilla parser:

1. Let us call a rule ρ in G w-consistent if each
string of terminals that occurs in (the com-
position function of) ρ is a substring of w.
A rule is called useful w.r.t. w if it occurs in
some complete derivation ofG in which each
rule is w-consistent. In the construction of
the FSA for RG ∩ hom−1G (w), we only cal-
culate the transitions that relate to rules of G
that are useful w.r.t. w.

2. The function FILTER(mDG) is decomposed
into FILTER(mDG) ◦ FILTER(D∆G) in prepa-
ration for the next two items.

3. FILTER(D∆G) ◦ SORT(µ′) is implemented
with the algorithm EXTRACTDYCK(G,µ′)
that uses dynamic programming to extract
Dyck words from the language of the given
FSA more efficiently. For this, we extend
alg. 1 by Hulden (2011) to use weights such
that it returns the elements in descending or-
der w.r.t. µ′ and � (see appendix A.3, alg. 3).
In our implementation, we change this al-



184

Algorithm 1 reads off cow derivations from words
of the CFA of G.
Input: v ∈ L(Gcf)
Output: toCowD(v)

1: function TOCOWDERIV(v)
2: t← empty graph
3: v ← new vertex
4: pd ← ε . pushdown of vertices
5: for all σ in v from left to right do
6: if σ is of the form J`ρ then
7: add vertex v to t with label ρ
8: push vertex v on top of pd
9: else if σ is of the form Jjρ,i then

10: v′ ← top-most vertex on pd
11: v ← new vertex
12: add edge (v′ → v) to twith label (i, j)
13: else if σ is of the form Kjρ,i then
14: remove top-most vertex from pd
15: return t

gorithm even further such that items are ex-
plored in a similar fashion as in the CKY-
algorithm (Kasami, 1966; Younger, 1967;
Cocke and Schwartz, 1970).

4. For FILTER(mDG), instead of isMember ′ by
Denkinger (2017, p. 28–30), which runs in
quadratic time, we use the composition of
two algorithms that run in linear time:

• alg. 1, which reads a cow derivation off
a given word inRG ∩D∆G , and
• an algorithm that checks a given cow

derivation for consistency. (This is sim-
ilar to alg. 2; but instead of derivations,
we return Boolean values. The algo-
rithm is given explicitly in sec. A.3.)

5. Algorithm 2 computes the bijection between
cowDcG and D

c
G (see prop. 15). Analogously

to def. 14, the function TOMCFGDERIV’
checks a set of cow derivations for equiv-
alence of the root symbol and the function
COLLECTCHILDREN groups the subtrees via
the first component of the successor labels. It
is easy to see that TOMCFGDERIV(t) is only
defined if the cow derivation t is consistent
(cf. item 4). Thus, we use TOMCFGDERIV in
combination with TOCOWDERIV to replace
MAP(TODERIV) ◦ FILTER(mDG).
The time complexity of alg. 2 is linear in the

Algorithm 2 converts a consistent element of
cowDcG into a complete derivation of G.

Input: t ∈ cowDcG

Output:


TODERIV

(
toCowD−1(t)

)
if t is consistent

undefined otherwise


1: function TOMCFGDERIV(t)
2: return TOMCFGDERIV’({t})
3: function TOMCFGDERIV’(T )
4: if not

∧
t,t′∈T

(
root(t) = root(t′)

)
then

5: return undefined
6: (T1, . . . , Tk)← COLLECTCHILDREN(T )
7: for i ∈ [k] do
8: ti ← TOMCFGDERIV’(Ti)
9: if ti = undefined then

10: return undefined
11: {σ} ←

{
root(t) | t ∈ T

}
12: return σ(t1, . . . , tk)

13: function COLLECTCHILDREN(T )
14: {k} ←

{
rank(root(t)) | t ∈ T

}
15: for i ∈ [k] do
16: Ti ←

{
sub(i,j)(t)
| t ∈ T, (i, j) ∈ out(t)

}
17: return (T1, . . . , Tk)

number of vertices of the given cow deriva-
tion. This number, in turn, is linear in the
length of the processed candidate.

The parser obtained by applying items 1 to 5 to the
vanilla parser is visualised in fig. 2 (bottom). It is
sound and complete.3 The following two modifi-
cations (items 6 and 7) destroy both soundness and
completeness. Item 6 allows only the best interme-
diate results to be processed further and limits the
results to a subset of those of the vanilla parser. In
item 7, we compensate this by an approximation
we consider useful in practise.

6. EXTRACTDYCK is extended with an optional
implementation of beam search by limit-
ing the amount of items for certain groups
of state spans to a specific number (beam
width), cf. Collins (1999). In our implemen-
tation, we chose these groups of state spans
such that they correspond to equal states in

3A parser is complete if it (eventually) computes all com-
plete derivations of the given word in the given grammar. A
parser is called sound if all computed parses are complete
derivations of the given word in the given grammar.



185

the automaton for hom−1G (w). Moreover, we
introduce a variable that limits the number of
candidates that are yielded by Algorithm 3
(candidate count). Both variables are the
meta-parameters of our parser.

7. We introduce a fallback mechanism for the
case that FILTER(mDG) has input candidates
but an empty output. Usually, in that case, we
would suggest there is no derivation for w in
G, yet for robustness, it is preferable to out-
put some parse. Figure 3 illustrates a strategy
to construct a complete derivation from any
complete cow derivation with an example.

5 Evaluation and Conclusion

We implemented the parser with the modifica-
tions sketched in sec. 4 for ε-free and simple
wMCFGs,4 but no problems should arise gener-
alising this implementation to arbitrary wMCFGs.
The implementation is available as a part of Rus-
tomata,5 a framework for weighted automata with
storage written in the programming language Rust.
We used the NeGra corpus (German newspaper
articles, 20,602 sentences, 355,096 tokens; Skut
et al., 1998) to compare our parser to Grammat-
ical Framework (Angelov and Ljunglöf, 2014),
rparse (Kallmeyer and Maier, 2013), and disco-
dop (van Cranenburgh et al., 2016) with respect to
parse time and accuracy.6 Our experiments were
conducted on defoliated trees, i.e. we removed the
leaves from each tree in the corpus. Parsing was
performed on gold part-of-speech tags.

We performed a variant of ten-fold cross val-
idation (short: TFCV; cf. Mosteller and Tukey,
1968), i.e. we split the corpus into ten consec-
utive parts; each part becomes the validation set
in one iteration while the others serve as training
set. We used the first iteration to select suitable
values for our meta-parameters and the remain-
ing nine for validation. In case of Rustomata, a
binarised and markovized grammar was induced
with discodop (head-outward binarisation, v = 1,
h = 2, cf. Klein and Manning, 2003) in each iter-
ation. For all other parsers, we induced a proba-

4A wMCFG G is called ε-free and simple if each compo-
sition function that occurs in the rules of G is either of the
form [u1, . . . , us] for some non-empty strings of variables
u1, . . . , us, or of the form [t] for some terminal symbol t.

5available on https://github.com/tud-fop/
rustomata. We used commit 867a451 for evaluation.

6The evaluation scripts are available on https://
github.com/truprecht/rustomata-eval.

0 5 10 15 20 25 30

0

10

20

30

sentence length

pa
rs

e
tim

e
in

s

ddctf-lcfrs
ddctf-dop

OP
GF

ddlcfrs
rparse

(a) NeGra corpus, |w| ≤ 30 (for ddlcfrs: |w| ≤ 20)

0 5 10 15 20 25 30

0

0.5

1

1.5

sentence length
pa

rs
e

tim
e

in
s

ddctf-lcfrs
ddctf-dop

OP

(b) Lassy corpus, |w| ≤ 30

Figure 4: Median parse times

bilistic LCFRS with the respective default config-
urations (for details, cf. the evaluation scripts). Af-
ter that, we ran our parser on each sentence of the
validation set and recorded the parse time and the
computed 1-best parse. The computed parses were
evaluated against the gold parses of the validation
set w.r.t. precision, recall, and f1-score (according
to the labelled parseval measures, cf. Black et al.,
1991; Collins, 1997, we used the implementation
by van Cranenburgh et al., 2016).

Previous experiments with an implementation
of the vanilla parser already struggled with small
subsets (we used grammars extracted from 250–
1500 parse trees) of the NeGra corpus. Therefore,
we omit evaluation of the vanilla parser.

Meta-parameters. A grid search for meta-
parameters was performed on sentences of up to
20 tokens (see the appendix, tab. 2, for a detailed
listing). The results suggested to set the beam
width to 200 and the candidate count to 10,000.

Comparison to other parsers. The experiments
were performed on sentences with up to 30 to-
kens. We instructed rparse, Grammatical Frame-
work (short: GF) and Rustomata (short: OP) to
stop parsing each sentence after 30 seconds (time-
out). Disco-dop did not permit passing a timeout.
In the case of disco-dop’s LCFRS parser (short:
ddlcfrs), we limited the validation set to sentences

https://github.com/tud-fop/rustomata
https://github.com/tud-fop/rustomata
https://github.com/truprecht/rustomata-eval
https://github.com/truprecht/rustomata-eval


186

parser precision recall f1-score coverage

NeGra corpus, |w| ≤ 20
ddctf-dop 81.34 81.44 81.39 100%
ddlcfrs 74.85 73.99 74.42 100%
ddctf-lcfrs 74.78 73.89 74.33 100%
OP 74.38 74.22 74.30 99.20%
GF 70.98 72.17 71.57 98.15%
rparse 71.15 41.67 52.56 76.31%

NeGra corpus, |w| ≤ 30
ddctf-dop 77.91 78.22 78.07 100%
ddctf-lcfrs 70.32 69.70 70.01 100%
OP 68.81 69.27 69.04 99.22%
GF 66.56 68.16 67.35 98.19%
rparse 71.19 23.73 35.59 57.33%

Lassy corpus, |w| ≤ 30
ddctf-dop 73.78 73.37 73.57 100%
ddctf-lcfrs 61.23 59.73 60.52 100%
OP 50.92 51.42 51.17 100%

Table 1: Precision, recall, f1-score, and coverage

of at most 20 tokens, since ddlcfrs frequently ex-
ceeded 30 seconds of parse time for longer sen-
tences in preliminary tests. Disco-dop’s coarse-
to-fine data-oriented parser (short: ddctf-dop) and
disco-dop’s coarse-to-fine LCFRS parser (short:
ddctf-lcfrs) rarely exceeded 30 seconds of parse
time in preliminary tests and we let them run on
sentences of up to 30 tokens without the timeout.

Figure 4a shows the parse times for each sen-
tence length and parser. The parsers ddctf-dop,
ddctf-lcfrs, GF, and OP perform similar for sen-
tences of up to 20 tokens. The parse times of
rparse and ddlcfrs grow rapidly after 10 and 16 to-
kens, respectively. Rparse even exceeds the time-
out for more than half of the test sentences that are
longer than 15 tokens. For sentences with up to
30 tokens, the parse times of ddctf-dop, ddctf-lcfrs
and OP seem to remain almost constant.

Table 1 shows the accuracy (i.e. precision, re-
call, and f1-score) and the coverage (i.e. the per-
centage of sentences that could be parsed) for each
parser on the validation set. We report these scores
to assert a correct implementation of our parser
and to compare the different approximation strate-
gies (and our fallback mechanism) implemented
in the parsers. The low coverage of rparse stems
from the frequent occurrences of timeouts. They
also depress the recall for rparse. For sentences
with at most 20 tokens, ddlcfrs, ddctf-lcfrs and
OP perform very similar. These three parsers are
outperformed by ddctf-dop in all aspects. For
sentences of up to 30 tokens, the scores of all
tested parsers drop similarly. However, ddctf-

dop’s scores drop the least amount.
We repeated a part of the experiments with the

Lassy corpus (Lassy Small, various kinds of writ-
ten Dutch, 65,200 sentences, 975,055 tokens; van
Noord et al., 2013). Since it is considerably larger
than the NeGra corpus, we limited the experiments
to one iteration of TFCV, and we only investigate
OP, ddctf-lcfrs, and ddctf-dop. The results are
shown in fig. 4b (parse time) and at the bottom of
tab. 1 (accuracy). Figure 4b shows the difference
of ddctf-lcfrs, ddctf-dop and OP in terms of parse
times (which is not discernible in fig. 4a). This
plot shows that OP maintains very small parse
times – even for large copora – compared to the
state-of-the-art parser disco-dop.

All in all, our parser performs comparable to
state-of-the-art MCFG parsers (GF, rparse, ddl-
cfrs, ddctf-lcfrs) and, using the NeGra corpus, it
shows excellent results in parse time and good re-
sults in accuracy. Moreover, our parser can deal
with any ε-free and simple MCFG provided by an
external tool, making it more flexible than disco-
dop and rparse. However, we are not able to com-
pete with ddctf-dop in terms of accuracy, since dis-
continuous data-oriented parsing is a more accu-
rate formalism (van Cranenburgh and Bod, 2013).

6 Future Work

We see potential to improve the fallback mecha-
nism explained in sec. 4. For now, we only consid-
ered reporting the first cow derivation. By intro-
ducing some degree of consistency of cow deriva-
tions, we could select a cow derivation that is
closer to a derivation of G.

Since recognisable languages are closed under
inverse homomorphisms, we can use any recog-
nisable language as input for hom−1G (cf. fig. 2)
without changing the rest of the pipeline. This is
useful when the input of the parsing task is am-
biguous, as in lattice-based parsing (e.g. Goldberg
and Tsarfaty, 2008).

Moreover, since weighted recognisable lan-
guages are closed under inverse homomorphisms
and scalar product, we can even use a weighted
recognisable language as input for hom−1G , as in
the setting of Rastogi et al. (2016).

Acknowledgements

We thank our colleague Kilian Gebhardt as well as
the anonymous reviewers for their insightful com-
ments on drafts of this paper.



187

References
Krasimir Angelov and Peter Ljunglöf. 2014. Fast sta-

tistical parsing with parallel multiple context-free
grammars. In Proceedings of the 14th Conference
of the European Chapter of the Association for Com-
putational Linguistics, pages 368–376.

Yehoshua Bar-Hillel, Micha Asher Perles, and Eli
Shamir. 1961. On formal properties of simple
phrase structure grammars. Zeitschrift für Phonetik,
Sprachwissenschaft und Kommunikationsforschung,
14:143–172.

François Barthélemy, Pierre Boullier, Philippe De-
schamp, and Éric Villemonte de la Clergerie. 2001.
Guided parsing of range concatenation languages.
In Proceedings of the 39th Annual Meeting of the
Association for Computational Linguistics.

Ezra Black, Steven Abney, Dan Flickinger, Claudia
Gdaniec, Ralph Grishman, Philip Harrison, Donald
Hindle, Robert Ingria, Fred Jelinek, Judith Klavans,
Mark Liberman, and Tomek Strzalkowski. 1991. A
Procedure for Quantitatively Comparing the Syntac-
tic Coverage of English Grammars. In Proceedings
of the Workshop on Speech and Natural Language,
pages 306–311. Association for Computational Lin-
guistics.

Noam Chomsky. 1956. Three models for the descrip-
tion of language. IEEE Transactions on Information
Theory, 2(3):113–124.

Noam Chomsky. 1959. On certain formal properties of
grammars. Information and control, 2(2):137–167.

Noam Chomsky and Marcel Paul Schützenberger.
1963. The algebraic theory of context-free lan-
guages. Computer Programming and Formal Sys-
tems, Studies in Logic, pages 118–161.

Maximin Coavoux and Benoit Crabbé. 2017. Incre-
mental discontinuous phrase structure parsing with
the gap transition. In Proceedings of the 15th Con-
ference of the European Chapter of the Association
for Computational Linguistics: Volume 1, Long Pa-
pers, pages 1259–1270. Association for Computa-
tional Linguistics.

John Cocke and J. T. Schwartz. 1970. Programming
languages and their compilers: Preliminary notes.
techreport, Courant Institute of Mathematical Sci-
ences, New York University.

Michael Collins. 1997. Three generative, lexicalised
models for statistical parsing. In Proceedings of
the 35th annual meeting on Association for Com-
putational Linguistics and Eighth Conference of the
European Chapter of the Association for Compu-
tational Linguistics. Association for Computational
Linguistics.

Michael Collins. 1999. Head-driven statistical models
for natural language parsing. Ph.D. thesis.

Andreas van Cranenburgh and Rens Bod. 2013. Dis-
continuous parsing with an efficient and accurate
dop model. In Proceedings of the 13th International
Conference on Parsing Technologies.

Andreas van Cranenburgh, Remko Scha, and Rens
Bod. 2016. Data-oriented parsing with discontinu-
ous constituents and function tags. Journal of Lan-
guage Modelling, 4(1):57.

Tobias Denkinger. 2017. Chomsky-Schützenberger
parsing for weighted multiple context-free lan-
guages. Journal of Language Modelling, 5(1):3.

Yoav Goldberg and Reut Tsarfaty. 2008. A single
generative model for joint morphological segmenta-
tion and syntactic parsing. Proceedings of ACL-08:
HLT, pages 371–379.

John Edward Hopcroft and Jeffrey David Ullman.
1979. Introduction to Automata Theory, Languages
and Computation, 1st edition.

Mans Hulden. 2011. Parsing CFGs and PCFGs with
a Chomsky-Schützenberger representation. In Hu-
man Language Technology. Challenges for Com-
puter Science and Linguistics, volume 6562 of Lec-
ture Notes in Computer Science, pages 151–160.

Aravind K. Joshi. 1985. Tree adjoining grammars:
How much context-sensitivity is needed for charac-
terizing structural descriptions?, chapter 6. Camb-
drige University Press.

Aravind Krishna Joshi, Leon S. Levy, and Masako
Takahashi. 1975. Tree adjunct grammars. Journal
of Computer and System Sciences, 10(1):136–163.

Laura Kallmeyer and Wolfgang Maier. 2013. Data-
driven parsing using probabilistic linear context-
free rewriting systems. Computational Linguistics,
39(1):87–119.

T. Kasami. 1966. An efficient recognition and
syntax-analysis algorithm for context-free lan-
guages. techreport R-257, AFCRL.

Dan Klein and Christopher D. Manning. 2003. Accu-
rate unlexicalized parsing. In Proceedings of the
41st Annual Meeting on Association for Compu-
tational Linguistics. Association for Computational
Linguistics.

Wolfgang Maier. 2015. Discontinuous incremental
shift-reduce parsing. In Proceedings of the 53rd An-
nual Meeting of the Association for Computational
Linguistics and the 7th International Joint Confer-
ence on Natural Language Processing (Volume 1:
Long Papers), pages 1202–1212, Beijing, China.
Association for Computational Linguistics.

Frederick Mosteller and John Wilder Tukey. 1968.
Data analysis, including statistics. In G. Lindzey
and E. Aronson, editors, Handbook of Social Psy-
chology, volume 2. Addison-Wesley.

http://www.aclweb.org/anthology/E14-1039
http://www.aclweb.org/anthology/E14-1039
http://www.aclweb.org/anthology/E14-1039
https://doi.org/10.1524/stuf.1961.14.14.143
https://doi.org/10.1524/stuf.1961.14.14.143
http://aclweb.org/anthology/P01-1007
https://doi.org/10.3115/112405.112467
https://doi.org/10.3115/112405.112467
https://doi.org/10.3115/112405.112467
https://doi.org/10.1109/tit.1956.1056813
https://doi.org/10.1109/tit.1956.1056813
https://doi.org/10.1016/S0019-9958(59)90362-6
https://doi.org/10.1016/S0019-9958(59)90362-6
https://doi.org/10.1016/S0049-237X(09)70104-1
https://doi.org/10.1016/S0049-237X(09)70104-1
http://aclweb.org/anthology/E17-1118
http://aclweb.org/anthology/E17-1118
http://aclweb.org/anthology/E17-1118
https://doi.org/10.3115/976909.979620
https://doi.org/10.3115/976909.979620
https://doi.org/10.15398/jlm.v4i1.100
https://doi.org/10.15398/jlm.v4i1.100
https://doi.org/10.15398/jlm.v5i1.159
https://doi.org/10.15398/jlm.v5i1.159
https://doi.org/10.15398/jlm.v5i1.159
http://www.aclweb.org/anthology/P08-1043
http://www.aclweb.org/anthology/P08-1043
http://www.aclweb.org/anthology/P08-1043
https://doi.org/10.1007/978-3-642-20095-3_14
https://doi.org/10.1007/978-3-642-20095-3_14
https://doi.org/http://dx.doi.org/10.1016/S0022-0000(75)80019-5
https://doi.org/10.1162/COLI_a_00136
https://doi.org/10.1162/COLI_a_00136
https://doi.org/10.1162/COLI_a_00136
https://doi.org/10.3115/1075096.1075150
https://doi.org/10.3115/1075096.1075150
http://www.aclweb.org/anthology/P15-1116
http://www.aclweb.org/anthology/P15-1116


188

Gertjan van Noord, Gosse Bouma, Frank Van Eynde,
Daniël de Kok, Jelmer van der Linde, Ineke Schuur-
man, Erik Tjong Kim Sang, and Vincent Vandeghin-
ste. 2013. Large Scale Syntactic Annotation of Writ-
ten Dutch: Lassy, pages 147–164. Springer Berlin
Heidelberg, Berlin, Heidelberg.

Pushpendre Rastogi, Ryan Cotterell, and Jason Eisner.
2016. Weighting finite-state transductions with neu-
ral context. In Proceedings of the 2016 Conference
of the North American Chapter of the Association
for Computational Linguistics: Human Language
Technologies, pages 623–633, San Diego, Califor-
nia. Association for Computational Linguistics.

Hiroyuki Seki, Takashi Matsumura, Mamoru Fujii,
and Tadao Kasami. 1991. On multiple context-
free grammars. Theoretical Computer Science,
88(2):191–229.

Stuart M. Shieber. 1985. Evidence against the context-
freeness of natural language. Linguistics and Phi-
losophy, 8(3):333–343.

Wojciech Skut, Thorsten Brants, Brigitte Krenn, and
Hans Uszkoreit. 1998. A Linguistically Interpreted
Corpus of German Newspaper Text. In Proceed-
ings of the 10th European Summer School in Logic,
Language and Information. Workshop on Recent Ad-
vances in Corpus Annotation.

Krishnamurti Vijay-Shanker, David Jeremy Weir, and
Aravind Krishna Joshi. 1987. Characterizing struc-
tural descriptions produced by various grammati-
cal formalisms. In Proceedings of the 25th Annual
Meeting on Association for Computational Linguis-
tics, pages 104–111.

Ryo Yoshinaka, Yuichi Kaji, and Hiroyuki Seki. 2010.
Chomsky-Schützenberger-type characterization of
multiple context-free languages. In Adrian-Horia
Dediu, Henning Fernau, and Carlos Martı́n-Vide,
editors, Language and Automata Theory and Appli-
cations, pages 596–607.

Daniel H. Younger. 1967. Recognition and parsing of
context-free languages in time n3. Information and
Control, 10(2):189–208.

A Appendices

A.1 Additional Examples

Example 1 (continuing from p. 2). Figure 5
shows graphical representations of a derivation
and the corresponding term over composition
functions. �

Example 7 (continuing from p. 4). The follow-
ing calculation reduces a word of AG to ε us-
ing the equivalence relation ≡PG (abbreviated by
≡) and thereby proves that it is an element of
mDG. In each step, we point out, which cell of

PG was/were used. Note that the set obtained af-
ter two applications of ≡ has two elements.

{r1
ρ1

r1
ρ1,1

q1
ρ4

y1
ρ4

z1
ρ1,1

r1
ρ1,2

q1
ρ3
b̃
q1
ρ3,1

J1ρ5K
1
ρ5

y1
ρ3,1

y1
ρ3

z1
ρ1,2

r2
ρ1,1

q2
ρ4

y2
ρ4

z2
ρ1,1

r2
ρ1,2

q2
ρ3
d̃
q2
ρ3,1

J2ρ5K
2
ρ5

y2
ρ3,1

y2
ρ3

z2
ρ1,2

z1
ρ1

}

≡

{r1
ρ1,1

q1
ρ4

y1
ρ4

z1
ρ1,1

r1
ρ1,2

q1
ρ3
b̃
q1
ρ3,1

J1ρ5K
1
ρ5

y1
ρ3,1

y1
ρ3

z1
ρ1,2

r2
ρ1,1

q2
ρ4

y2
ρ4

z2
ρ1,1

r2
ρ1,2

q2
ρ3
d̃
q2
ρ3,1

J2ρ5K
2
ρ5

y2
ρ3,1

y2
ρ3

z2
ρ1,2

}
(because

{
J1ρ1
}
∈ PG)

≡

{
q1
ρ4

y1
ρ4

q2
ρ4

y2
ρ4
,

r1
ρ1,2

q1
ρ3
b̃
q1
ρ3,1

J1ρ5K
1
ρ5

y1
ρ3,1

y1
ρ3

z1
ρ1,2

r2
ρ1,2

q2
ρ3
d̃
q2
ρ3,1

J2ρ5K
2
ρ5

y2
ρ3,1

y2
ρ3

z2
ρ1,2

}
(because

{
J1ρ1,1, J2ρ1,1

}
∈ PG)

≡

{
ε,

r1
ρ1,2

q1
ρ3
b̃
q1
ρ3,1

J1ρ5K
1
ρ5

y1
ρ3,1

y1
ρ3

z1
ρ1,2

r2
ρ1,2

q2
ρ3
d̃
q2
ρ3,1

J2ρ5K
2
ρ5

y2
ρ3,1

y2
ρ3

z2
ρ1,2

}
(because

{
J1ρ4 , J2ρ4

}
∈ PG)

≡
{
ε,

q1
ρ3
b̃
q1
ρ3,1

J1ρ5K
1
ρ5

y1
ρ3,1

y1
ρ3

q2
ρ3
d̃
q2
ρ3,1

J2ρ5K
2
ρ5

y2
ρ3,1

y2
ρ3

}
(because

{
J1ρ1,2, J2ρ1,2

}
∈ PG)

≡
{
ε, b̃

q1
ρ3,1

J1ρ5K
1
ρ5

y1
ρ3,1

d̃
q2
ρ3,1

J2ρ5K
2
ρ5

y2
ρ3,1

}
(because

{
J1ρ3 , J2ρ3

}
∈ PG)

≡
{
ε,

q1
ρ3,1

J1ρ5K
1
ρ5

y1
ρ3,1

d̃
q2
ρ3,1

J2ρ5K
2
ρ5

y2
ρ3,1

}
(because

{
Jb
}
∈ PG and b̃ = JbKb)

≡
{
ε,

q1
ρ3,1

J1ρ5K
1
ρ5

y1
ρ3,1

q2
ρ3,1

J2ρ5K
2
ρ5

y2
ρ3,1

}
(because

{
Jd
}
∈ PG and d̃ = JdKd)

≡
{
ε, J1ρ5K

1
ρ5 J

2
ρ5K

2
ρ5

}
(because

{
J1ρ3,1, J2ρ3,1

}
∈ PG)

≡ {ε} (because
{
J1ρ5 , J2ρ5

}
∈ PG) �

A.2 Additional Proofs

Lemma 12. There is a bijection toCowD between
L(Gcf) and cowDcG.

Proof. For each ` ∈ N+, we define the
partial function f` from (∆G ∪ Ě∆G)∗ to `-
cow derivations of G as follows: f`(u) =
ρ
〈
(i(κ), j(κ))/fj(κ)(vκ) | κ ∈ [n]

〉
if u is of the

https://doi.org/10.1007/978-3-642-30910-6_9
https://doi.org/10.1007/978-3-642-30910-6_9
http://www.aclweb.org/anthology/N16-1076
http://www.aclweb.org/anthology/N16-1076
https://doi.org/10.1016/0304-3975(91)90374-B
https://doi.org/10.1016/0304-3975(91)90374-B
https://doi.org/10.1007/bf00630917
https://doi.org/10.1007/bf00630917
https://doi.org/10.3115/981175.981190
https://doi.org/10.3115/981175.981190
https://doi.org/10.3115/981175.981190
https://doi.org/10.1007/978-3-642-13089-2_50
https://doi.org/10.1007/978-3-642-13089-2_50
https://doi.org/10.1016/s0019-9958(67)80007-x
https://doi.org/10.1016/s0019-9958(67)80007-x


189

ρ1

ρ2

...

ρ2

ρ4

ρ3

...

ρ3

ρ5

m times n times

[x11x
1
2x

1
2x

2
2]

[ax11, cx
2
1]

...

[ax11, cx
2
1]

[ε, ε]

[bx11,dx
2
1]

...

[bx11,dx
2
1]

[ε, ε]

m times n times

Figure 5: A derivation dm,n (top) together with the cor-
responding term over composition functions (bottom),
cf. ex. 1.

S → [x11x12x21x22](A,B)

A→ [ε, ε]()
(1, 1)

A→ [ε, ε]()
(1, 2)

B → [bx11, dx21](B)

B → [ε, ε]()
(1, 1)

B → [ε, ε]()
(1, 2)

(2, 1)

B → [bx11,dx21](B)

B → [ε, ε]()
(1, 1)

B → [ε, ε]()
(1, 2)

(2, 2)

Figure 6: A consistent cow derivation.

form
r`
ρ
ũ0Jj(1)ρ,i(1)v1K

j(1)
ρ,i(1)ũ1 . . . J

j(n)
ρ,i(n)vnK

j(n)
ρ,i(n)ũn

z`
ρ

for some rule ρ = A → c(B1, . . . , Bk) where the
`-th component of c is u0x

j(1)
i(1)u1 · · ·x

j(n)
i(n)un with

u1, . . . , un ∈ Σ∗; otherwise, f`(u) is undefined.
Note that f1, f2, . . . are pairwise disjoint (in the
set-theoretic sense).

To prove that the function f` is bijective, we
show that it is injective and surjective.

(Injectivity) We show, for each ` ∈ N+, by in-
duction on the structure of cow derivations that
f`(v) = f`(v

′) implies v = v′ for each v, v′ in
the domain of f` (i.e. f`(v) and f`(v′) are both
defined).

Let v, v′ ∈ (∆G ∪ Ě∆G)∗ be in the domain
of f` and let t = f`(v) = f`(v′). Further-
more, let ρ = A → c(B1, . . . , Bk) = root(t),
t(i,j) = sub(i,j)(t) for each (i, j) ∈ out(t),
and let the `-th component of c be of the form
u0x

j(1)
i(1)u1 · · ·x

j(n)
i(n)un with u1, . . . , un ∈ Σ

∗.
By definition of f`, we know that u is the string

r`
ρ
ũ0Jj(1)ρ,i(1)v1K

j(1)
ρ,i(1)ũ1 . . . J

j(n)
ρ,i(n)vnK

j(n)
ρ,i(n)ũn

z`
ρ

for some v1, . . . , vn ∈ (∆G ∪Ě∆G)∗, u′ is the
string

r`
ρ
ũ0Jj(1)ρ,i(1)v

′
1K
j(1)
ρ,i(1)ũ1 . . . J

j(n)
ρ,i(n)v

′
nK
j(n)
ρ,i(n)ũn

z`
ρ

for some v′1, . . . , v
′
n ∈ (∆G ∪ Ě∆G)∗, and

fj(κ)(vκ) = fj(κ)(v
′
κ) = t(i(κ),j(κ)) for each

κ ∈ [n]. By principle of induction, we get
vκ = v

′
κ for each κ ∈ [n]. Hence v = v′.

(Surjectivity) We show by induction on the
structure of cow derivations that for each A ∈
N , ` ∈ sort(A), and t ∈ `-cowDAG, there is a
string v ∈ L(Gcf, A`) such that f`(v) = t.

Let A ∈ N , ` ∈ sort(A), and t ∈ `-cowDAG.
Furthermore, let ρ = A → c(B1, . . . , Bk) =
root(t), t(i,j) = sub(i,j)(t) for each (i, j) ∈
out(t), and let the `-th component of c be of the
form u0x

j(1)
i(1)u1 · · ·x

j(n)
i(n)un with u1, . . . , un ∈

Σ∗. By principle of induction, we know that
there are vκ ∈ L(Gcf, Bj(κ)i(κ) ) with fj(κ)(vκ) =
t(i(κ),j(κ)) for each κ ∈ [n]. Now let v be the
string

r`
ρ
ũ0Jj(1)ρ,i(1)v1K

j(1)
ρ,i(1)ũ1 . . . J

j(n)
ρ,i(n)vnK

j(n)
ρ,i(n)ũn

z`
ρ
.

By definition of the rule ρ(`) (def. 8), we know
that v ∈ L(Gcf, A`). By definition of f`, we
know that f`(v) = t. Hence, f` is surjective.

It is easy to see that toCowD =
⋃
`∈N+ f`. If

we restrict the domain of toCowD to L(Gcf), then
(since each element of L(Gcf) starts with a bracket
of the form J1ρ for some ρ ∈ P ) the resulting func-
tion is a subset of f1. Since f1 is bijective, we
know that toCowD is a bijection between L(Gcf)
and cowDcG. �

Lemma 16. Let v ∈ L(Gcf). Then

v ∈ mDG ⇐⇒ toCowD(v) is consistent.



190

Proof. Let s ∈ N+ and let t1, . . . , ts be cow
derivations in G. We abbreviate the follow-
ing property by C(t1, . . . , ts): (i) {t1, . . . , ts}
is consistent, (ii) s = fanout(root(t1)), and
(iii) t` is an `-cow derivation for each ` ∈ [s].
Now, we show by structural induction that
C(toCowD(v1), . . . , toCowD(vs)) holds iff each
permutation of v1, . . . , vs is in mDG. Let ρ =
A → [c1, . . . , cs](B1, . . . , Bk) be a rule in P . Let
vji ∈ L(G,B

j
i ) for each i ∈ [k] and j ∈ [sort(Bi)]

such that

Per(v1i , . . . , v
sort(Bi)
i ) ⊆ mDG

⇐⇒ C
(
t1i , . . . , t

sort(Bi)
i

) (IH)
for each i ∈ [k] where tji abbreviates toCowD(v

j
i )

and Per(v1i , . . . , v
sort(Bi)
i ) is the set of permuta-

tions of v1i , . . . , v
sort(Bi)
i . For each ` ∈ [s], let v`

be obtained from the right-hand side of ρ(`) by re-
placing each non-terminal of the form Bji by v

j
i .

Clearly, for each ` ∈ [s], the sequence v` is an
element of L(G,A`). From now on, we will ab-
breviate toCowD(v`) by t` for each ` ∈ [s]. Now,
letBj(`,1)i(`,1) , . . . , B

j(`,n`)
i(`,n`)

be the nonterminals on the

rhs of ρ(`), then t` is the cow derivation

ρ
〈
(i(`, κ), j(`, κ))/t

j(`,κ)
i(`,κ) | κ ∈ [n`]

〉
.

Note that ρ is the root symbol of each t1, . . . , ts

and the set of indices {(i(`, κ), j(`, κ)) | ` ∈
[s], κ ∈ [n`]} is the set {(i, j) | i ∈ [k], j ∈
[sort(Bi)]}. We derive

C(t1, . . . , ts)

⇐⇒ {t1, . . . , ts} is consistent (by def. 10)

⇐⇒ ∀i ∈ [k]: {t1i , . . . , t
sort(Bi)
i } is consistent

(by def. 14)

⇐⇒ ∀i ∈ [k]: Per(v1i , . . . , v
sort(Bi)
i ) ⊆ mDG

(by eq. (IH))

⇐⇒ Per(v1, . . . , vs) ⊆ mDG
(by defs. 6 and 8)

Since the vji s and ρ were selected arbitrarily, we
can obtain any element of L(G,A`) in that man-
ner. In particular, for each v1 ∈ L(Gcf, S1) =
L(Gcf), the cow derivation toCowD(v1) is con-
sistent iff v1 is in mDG. �

Proposition 15. TODERIV ◦ toCowD−1 is a bi-
jection between the consistent cow derivations in
cowDcG and D

c
G.

Proof. By lems. 12 and 16, there is a bijec-
tion between the consistent cow derivations in
(1 -cowDG)S and RG ∩ mDG, and there is a bi-
jection between RG ∩mDG and DcG (Denkinger,
2017, cor. 3.9). �

A.3 Additional Algorithms
Algorithm 3 is a modification of the algorithm
given by Hulden (2011, alg. 1). The changes in-
volve an introduction of weights in the algorithm;
elements of A are drawn by maximum weight in-
stead of being drawn randomly. In our implemen-
tation, we defined the weight of an item (p, v, q)
as the weight µ′(v) defined in def. 6.

Algorithm 3 extracts Dyck words from an FSA.

Input: a weight assignment wt : Q× (∆∪∆)∗×
Q → M, and an automaton A = (Q,∆ ∪
∆, qinit, qfinal, T )

Output: a sequence v1, v2, . . . of elements in
L(A) ∩ D∆ such that wt(qinit, v1, qfinal) �
wt(qinit, v2, qfinal) � . . .

1: procedure EXTRACTDYCK(wt)(A)
2: (A,C)← (∅,∅)
3: for (p, δ, q), (q, δ, r) ∈ T do
4: A← A ∪ {(p, δδ, r)}
5: for (p, v, q) ∈ argmax�,wt A do
6: if (p, q) = (qinit, qfinal) then yield v
7: A← A \ {(p, v, q)}
8: C ← C ∪ {(p, v, q)}
9: for (r, δ, p), (q, δ, s) ∈ T do

10: A← A ∪ {(r, δvδ, s)}
11: for (q, w, r) ∈ C do
12: A← A ∪ {(p, vw, r)}
13: for (o, u, p) ∈ C do
14: A← A ∪ {(o, uv, q)}

Since the function given in alg. 4 is very similar
to def. 14, we omit further discussions.

Algorithm 4 checks consistency of a set of cow
derivations.
Input: a set T of cow derivations

Output:

{
true if T is consistent

false otherwise

}
1: function ISCONSISTENT(T )
2: if not

∧
t,t′∈T

(
root(t) = root(t′)

)
then

3: return false
4: (T1, . . . , Tk)← COLLECTCHILDREN(T )
5: return

∧k
i=1 ISCONSISTENT’(Ti)



191

candidate count
102 103 104 105

be
am

w
id

th

20

median parse time 8.04 ms 8.55 ms 8.52 ms 9.61 ms
mean parse time 8.18 ms 11.48 ms 34.71 ms 256.08 ms
f1-score 67.83 % 68.44 % 68.45 % 68.80 %
coverage (no fallback) 81.20 % 87.49 % 90.40 % 92.10 %
coverage (with fallback) 97.31 % 97.31 % 97.31 % 97.31 %

50

median parse time 13.23 ms 15.26 ms 15.44 ms 15.77 ms
mean parse time 13.11 ms 17.99 ms 53.51 ms 299.63 ms
f1-score 70.54 % 71.82 % 72.17 % 72.46 %
coverage (no fallback) 78.36 % 85.88 % 90.25 % 93.47 %
coverage (with fallback) 98.31 % 98.31 % 98.31 % 98.31 %

100

median parse time 19.68 ms 23.18 ms 24.43 ms 24.67 ms
mean parse time 18.81 ms 25.11 ms 70.12 ms 374.90 ms
f1-score 70.27 % 71.43 % 71.99 % 72.86 %
coverage (no fallback) 76.82 % 84.11 % 88.72 % 92.93 %
coverage (with fallback) 98.93 % 98.93 % 98.93 % 98.93 %

200

median parse time 31.33 ms 36.04 ms 39.75ms 40.05 ms
mean parse time 30.10 ms 35.80 ms 85.94ms 412.16 ms
f1-score 70.50 % 71.50 % 72.13% 72.98 %
coverage (no fallback) 76.66 % 83.57 % 88.33% 92.79 %
coverage (with fallback) 99.00 % 99.00 % 99.00% 99.00 %

500

median parse time 53.16 ms 58.07 ms 68.22 ms 68.86 ms
mean parse time 50.21 ms 56.70 sm 105.94 ms 431.90 ms
f1-score 70.51 % 71.45 % 72.08 % 72.90 %
coverage (no fallback) 76.66 % 83.50 % 88.26 % 92.79 %
coverage (with fallback) 99.00 % 99.00 % 99.00 % 99.00 %

1000

median parse time 53.10 ms 58.59 ms 76.07 ms 77.37 ms
mean parse time 53.53 ms 59.26 ms 109.46 ms 443.56 ms
f1-score 70.51 % 71.45 % 72.08 % 72.90 %
coverage (no fallback) 76.66 % 83.50 % 88.26 % 92.79 %
coverage (with fallback) 99.00 % 99.00 % 99.00 % 99.00 %

Table 2: Results of the grid search for meta-parameters.

A.4 Results of Grid Search for
Meta-Parameters

Table 2 shows the results of the grid search for the
two introduced meta-parameters. For each com-
bination of beam width and candidate count, we
list the median and mean parse times (since medi-
ans hide outliers, those two may differ drastically)
for all sentences of length 20 and f1-score over all
test sentences. Moreover, we show the percentage
of sentences (coverage) that we were able to parse
with and without the fallback mechanism. The re-
sults for the combination of meta-parameters that
was selected for later experiments (i.e. a beam

width of 200 and a candidate count of 104) are
written in bold.


