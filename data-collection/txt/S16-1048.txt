



















































BUTknot at SemEval-2016 Task 5: Supervised Machine Learning with Term Substitution Approach in Aspect Category Detection


Proceedings of SemEval-2016, pages 301–305,
San Diego, California, June 16-17, 2016. c©2016 Association for Computational Linguistics

BUTknot at SemEval-2016 Task 5: Supervised Machine Learning with Term
Substitution Approach in Aspect Category Detection

Jakub Macháček
Brno University of Technology,

Faculty of Information Technology,
IT4Innovations Centre of Excellence

Božetěchova 2, 612 66 Brno, Czech Republic
qmachacek@stud.fit.vutbr.cz

Abstract

This paper describes an approach used to solve
Aspect Category Detection (Subtask 1, Slot 1)
of SemEval 2016 Task 5. The core of the pre-
sented system is based on Supervised machine
learning using bigram bag-of-words model.
The performance is enhanced by several pre-
processing methods, most importantly by a
term substitution technique. The system has
reached a very good performance in compari-
son with other submitted systems.

1 Introduction

As the Internet more and more becomes the means
of expressing opinions about various subjects, the
need to effectively process those opinions (subjec-
tive information) is becoming more and more impor-
tant. Many companies around the world are now in-
terested in gathering public opinion and performing
strategic moves accordingly. Thus, Sentiment Analy-
sis (also known as Opinion mining) has become an
important area of interest.

Existing systems performing sentiment analysis
usually only predict polarity of a given sentiment.
While this can be sufficient in a lot of cases, some-
times we wish to analyze opinions about different
aspects of the same entity. This task is known as
Aspect-based Sentiment Analysis.

SemEval 2016 Task 5 (Pontiki et al., 2016) con-
sists of three subtasks. The first subtask is about
sentence-level sentiment analysis and is divided into
three slots: Aspect Category Detection, Opinion Tar-
get Expression and Sentiment Polarity Detection.

There are multiple distinct domains in which par-
ticipants were given the opportunity to test their sys-
tems. Each domain was available for one of the fol-
lowing languages: Arabic, Chinese, Dutch, English,
French, Russian, Spanish and Turkish.

For all domains, there is a limited, known in ad-
vance, list of entities and aspects the system should
recognize. Each entity can be associated with only
certain aspect(s). The set of all possible entity-
aspect pairs is also limited and known in advance.

Each submitted system could run in two modes:
Constrained (using no external data sources like lex-
icons or additional training sets) and Unconstrained
(no data source restriction).

The system I propose is focused on Aspect Cat-
egory Detection only, i.e. it only predicts which as-
pects of a given entity a given sentiment has opin-
ions about. I decided to participate in the English
language, for which two domains were available:
restaurant and laptop reviews. There were 12 entity-
aspect pairs for the restaurants domain and over 80
for the laptops domain.

2 Approach

My approach was inspired by the NLANGP sys-
tem (Toh and Su, 2015) which achieved excellent
results in SemEval 2015 (Pontiki et al., 2015) with a
straightforward solution.

I model the task as a multi-label classification
with binary relevance transformation, where labels
correspond to the entity-aspect pairs. All train and
test sentences are pre-processed (see Section 2.2).
Words from each sentence are used as individual bi-
nary features of that sentence. For each entity-aspect

301



pair, all training sentences are used as positive or
negative examples of that entity-aspect pair.

Vowpal Wabbit1, a supervised machine learning
tool, is used to train the resulting binary classifiers.
More precisely, a variant of online gradient descent
algorithm is used to perform logistic regression with
squared cost function.

2.1 Model properties
It has been observed that the model offers higher
accuracy if bigrams are allowed in VW. How-
ever, additional raising the n of VW’s n-gram fea-
ture is counterproductive. Allowing skips inside
bigrams also does not help. For the RESTAU-
RANT#MISCELLANEOUS aspect category, how-
ever, setting n = 5 seemed to be a better choice.
The system has generally unsatisfying score of pre-
dicting aspect categories associated with the MIS-
CELLANEOUS attribute. Setting n > 2 did not
improve the accuracy for any other aspect category.

I have also tuned VW’s learning rate and a thresh-
old T for comparing probabilities returned from
VW’s classifiers. When it predicts that a sentiment
has opinion(s) about an aspect category C with the
probability of P , the system drops the prediction iff
P < T . Table 1 shows the tuned properties.

Domain Restaurants Laptops
Learning rate 0.41 0.38

Prediction threshold T 0.40 0.34
Table 1: Tuned learning rate and the prediction threshold

2.2 Text pre-processing
The initial text pre-processing step consists of re-
moving the punctuation from each sentence and
converting all characters to lower case. Stanford
CoreNLP2 is used to tokenize each sentence, lem-
matize all words and also extract their part of speech
(POS).

2.2.1 Filtering words
The POS tags are useful to estimate how much

important given words are in terms of aspect cate-
gory detection. The system does not introduce the
tags to the machine learning algorithm, but it uses
each tag to consider removing the corresponding

1https://github.com/JohnLangford/vowpal wabbit/wiki
2Homepage: http://stanfordnlp.github.io/CoreNLP/

Food Service Opinion Laptop
pizza staff good computer
sauce clerk average machine
entree she terrific notebook

hot dog friendly disappointing netbook
croquette attentive poor desktop

... ... ... ...
Table 2: Example term lists compilation

word from its sentence. Also, some words are re-
moved regardless of their POS.

An automated experiment over the oficial training
set has been implemented to produce a POS filter
and a list of stop words, for each domain separately.
Both features are included in the constrained mode.

The experiment showed that for the restaurants
domain, it was not demonstrably beneficial to re-
move any word based on its POS. On the other hand,
it generated a list of tags for the laptops domain.

The lists of stop words that the experiment pro-
duced were surprisingly small. It seemed there were
just few high frequency words which were irrelevant
in the learning process.

2.2.2 Term groups

As a part of the pre-processing phase, a simple
substitution system has been implemented to sup-
port machine learning. When multiple n-grams have
roughly the same meaning, or they are related in a
certain way, it is often beneficial if classifiers do not
distinguish between them. A set of n-gram (term)
lists in the fashion depicted in Table 2 has been man-
ually compiled. Presence of the listed terms is then
checked in each sentence and if found, each occur-
rence is replaced by its representative. Terms are
always compared by their lemmas. The lists have
been compiled by applying the following:

1. For each entity and attribute independently,
only those sentences from the train dataset that
contain the entity or attribute were selected.
Then all unigrams from the sentences were
sorted by number of occurrences. The most
frequent words were manually checked, one by
one, and the ones closely related to a particular
entity or attribute were added to the respective
lists.

302



2. In case of the restaurants domain, opinion tar-
gets from the train datasets were also extracted.
This resulted in much shorter lists of high pre-
cision terms. All terms could be therefore indi-
vidually checked in a reasonable time. Again,
terms closely related to an entity or attribute
were added to the respective lists. Some n-
grams were split into multiple pieces, e.g. lava
cake dessert → {lava cake, dessert}.

3. While performing the preceding two methods,
I also noticed that some terms played a cer-
tain role in aspect category detection even if
they were not associated with just a single en-
tity or attribute but rather a set of them. This,
for example, included opinion words (indicat-
ing attributes GENERAL and QUALITY) and
words describing problems (e.g. fail, problem-
atic, blue screen – indicating attributes OP-
ERATION PERFORMANCE, QUALITY and
possibly some other).

The following methods for extending the lists
were also used but they are not applied in the con-
strained mode:

4. For some specific words (e.g. adjectives ex-
pressing food taste), an online dictionary3 has
been used to search for their synonyms and the
term lists have been manually appended with
suitable words.

5. Several lists of words publicly available on the
Internet have also been included:

• Food: http://eatingatoz.com/food-list/ and
https://www.atkins.com/how-it-
works/atkins-20/phase-1/low-carb-foods;
both lists have been manually checked and
some misleading items removed (mostly
words related to drinks)

• Drinks: http://cocktails.lovetoknow.com/
List of Popular Cocktails and others
compiled manually

• Laptop manufacturers:
https://en.wikipedia.org/wiki/List of laptop
brands and manufacturers#Major brands

3Search engine available at http://www.thesaurus.com/

• Laptop series: manually extracted names of
laptop series available at
https://en.wikipedia.org/wiki/Asus#Laptops,
https://en.wikipedia.org/wiki/List of Hewlett-
Packard products#Business notebooks and
http://www.acer.com/ac/en/US/content/
models/laptops

• Processors: manually extracted names of
CPU series from Wikipedia pages
https://en.wikipedia.org/wiki/
List of AMD microprocessors and
https://en.wikipedia.org/wiki/List of Intel
microprocessors

• Operation systems: manually extracted
names of mainstream Linux distributions
from the list published at
https://en.wikipedia.org/wiki/List of
Linux distributions

• Screen resolution names:
https://en.wikipedia.org/wiki/Display
resolution#Common display resolutions

2.2.3 Other pre-processing steps
The system also tries to improve its accuracy by

replacing all numbers by the word number. Num-
bers preceded by a currency symbol ($, ¤, £) are
replaced with the word price (which then indicates
the PRICES attribute).

Words containing both alpha and numeric char-
acters are replaced with the word model as it was
observed that in most cases, such words represent
particular model names (e.g. i7, G73JH-x3, d620).
This seemed to be helpful notably in the laptops do-
main.

The system removes all words shorter than two
characters. It is important to note that this step
comes after removing all non-alphanumeric charac-
ters. This means words like w/ are eventually re-
moved.

The system also neutralizes consecutive letters
which effectively replaces words like waay or
waaaaaay by the word way.

2.3 Prediction post-processing

The presence of previously detected opinion words
indicates the QUALITY and GENERAL attributes.
The corresponding aspect categories are correctly

303



Domain Rest Lapt
Basic text pre-processing 63.9 49.6

Bigrams 65.3 50.1
Minimal word length 65.7 50.7

Lemmatization 65.8 50.7
Consecutive letters neutralization 65.8

Prices recognition 66.1 50.9
Numbers+models recognition 66.2 51.0

POS filter 52.4
Stop words 66.4 52.4

Prediction post-processing 68.0
Term groups (constrained) 71.6 53.5

Term groups (unconstrained) 72.1 53.8
Table 3: The F-measure in percentage using 4-fold cross vali-
dation over the training sets. Each row represents the system’s

accuracy when the corresponding technique and all those from

previous rows are enabled. The prediction threshold, as well as

VW’s learning rate, is always optimized. Missing values repre-

sent unused techniques.

predicted by VW only in cases in which the opinion
words are directly preceded or followed by words in-
dicating entities (i.e. they make up bigrams). When
these words are separated by one or more other
words, no aspect category is predicted. For this rea-
son, the system always looks for an entity-related
word which is closest to the opinion word and still
not farther than four skips. If such word is found,
the corresponding aspect category is additionally
predicted. The same process is repeated with the
PRICES attribute with the difference that when no
suitable entity is found, RESTAURANT#PRICES is
predicted. The system does no post-processing in
the laptops domain.

3 Results

All techniques and features described in Section 2
have been tuned separately for each domain using 4-
fold cross validation. Table 3 displays the reached
accuracy.

The tuned system has been trained using the of-
ficial training sets containing 2000 and 2500 sen-
tences in the restaurants and laptops domains respec-
tively. The test sets consisted of 676 sentences in the
restaurants domain and 808 sentences in the laptops
domain.

The system achieved very good results, especially

Domain Restaurants Laptops
Mode C U C U

1st place 71.494 73.031 47.891 51.937
2nd place 68.701 72.886 47.527 49.105
3rd place 67.817 72.396 46.728 49.076
4th place 67.350 71.537 45.629 48.396
5th place 65.563 71.494 43.754 47.891

Table 4: Rankings in ACD (slot 1) of the Subtask 1. The F-
score of the submitted systems is represented in percentage. Re-

sults of my system are highlighted in bold-faced font. C stands

for the constrained and U for the unconstrained mode.

in the restaurants domain where it ranked third in the
unconstrained mode, falling behind the winner only
by 0.635%, and first in the constrained mode. Table
4 shows the F-score of the top five systems in each
domain and mode. The best accuracy in the uncon-
strained mode has been reached in both domains by
the NLANGP team which has ranked first also in
SemEval 2015.

4 Conclusion

This paper described my approach to aspect cate-
gory detection. The presented system has ranked
as one of the most accurate in this task. As I have
never contributed to the field of aspect-based senti-
ment analysis (and SA generally) before, I find my
results more than satisfactory.

The system has an advantage of its relatively
high adaptability to work with previously unseen do-
mains. All techniques except the term groups and
the prediction post-processing can be tuned auto-
matically with no additional manual help needed.
Multilinguality is limited by the used lemmatizer(s)
but since lemmatization offers just a mild increase in
accuracy, it is possible to omit it when working with
unsupported languages.

The system does not take review context of in-
put sentences into consideration. In my future work
I would like to remove this flaw. The term groups
described in Section 2.2.2 will be significantly im-
proved by extending them and creating new groups
for other important terms.

Acknowledgments

This work was supported by the H2020 project
MixedEmotions, grant agreement No. 644632.

304



References
Everton Alvares Cherman, Maria Carolina Monard, and

Jean Metz. 2011. Multi-label problem transforma-
tion methods: a case study. CLEI Electronic Journal,
14(1).

José Saias. 2015. Sentiue: Target and Aspect based Sen-
timent Analysis in SemEval-2015 Task 12. In Pro-
ceedings of the 9th International Workshop on Seman-
tic Evaluation (SemEval 2015), Denver, Colorado.

Satarupa Guha, Aditya Joshi, and Vasudeva Varma.
2015. SIEL: Aspect Based Sentiment Analysis in Re-
views. In Proceedings of the 9th International Work-
shop on Semantic Evaluation (SemEval 2015), Denver,
Colorado.

Minqing Hu and Bing Liu. 2004. Mining and Summa-
rizing Customer Reviews. In Proceedings of the Tenth
ACM SIGKDD International Conference on Knowl-
edge Discovery and Data Mining. Seattle, Washing-
ton, USA.

John Pavlopoulos. 2014. Aspect based sentiment analy-
sis. PhD thesis, Dept. of Informatics, Athens Univer-
sity of Economics and Business, Greece.

Maria Pontiki, Dimitrios Galanis, Haris Papageorgiou,
Suresh Manandhar, and Ion Androutsopoulos. 2015.
SemEval-2015 Task 12: Aspect Based Sentiment
Analysis. In Proceedings of the 9th International
Workshop on Semantic Evaluation (SemEval 2015),
Denver, Colorado.

Maria Pontiki, Dimitrios Galanis, Haris Papageorgiou,
Ion Androutsopoulos, Suresh Manandhar, Mohammad
AL-Smadi, Mahmoud Al-Ayyoub, Yanyan Zhao, Bing
Qin, Orphée De Clercq, Véronique Hoste, Marianna
Apidianaki, Xavier Tannier, Natalia Loukachevitch,
Evgeny Kotelnikov, Nuria Bel, Salud Marı́a Jiménez-
Zafra, and Gülşen Eryiğit. 2016. SemEval-2016 task
5: Aspect based sentiment analysis. In Proceedings of
the 10th International Workshop on Semantic Evalua-
tion, SemEval ’16, San Diego, California, June. Asso-
ciation for Computational Linguistics.

Hassan Saif, Miriam Fernandez, Yulan He, and Harith
Alani. 2014. On Stopwords, Filtering and Data Spar-
sity for Sentiment Analysis of Twitter. Ninth Interna-
tional Conference on Language Resources and Evalu-
ation, pages 810–817.

Zhiqiang Toh and Jian Su. 2015. NLANGP: Supervised
Machine Learning System for Aspect Category Clas-
sification and Opinion Target Extraction. In Proceed-
ings of the 9th International Workshop on Semantic
Evaluation (SemEval 2015), Denver, Colorado.

Bo Wang and Min Liu. 2015. Deep Learning for Aspect-
Based Sentiment Analysis.

Lei Zhang and Bing Liu. 2014. Aspect and Entity Ex-
traction for Opinion Mining. In Data Mining and

Knowledge Discovery for Big Data: Methodologies,
Challenges, and Opportunities. Springer.

305


