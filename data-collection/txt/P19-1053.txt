



















































Progressive Self-Supervised Attention Learning for Aspect-Level Sentiment Analysis


Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 557–566
Florence, Italy, July 28 - August 2, 2019. c©2019 Association for Computational Linguistics

557

Progressive Self-Supervised Attention Learning for
Aspect-Level Sentiment Analysis

Jialong Tang1,2,3∗, Ziyao Lu1∗, Jinsong Su1†, Yubin Ge4, Linfeng Song5,
Le Sun2, Jiebo Luo5

1Xiamen University, Xiamen, China
2Institute of Software, Chinese Academy of Sciences, Beijing, China

3University of Chinese Academy of Sciences, Beijing, China
4University of Illinois at Urbana-Champaign, Urbana, IL 61801, USA

5Department of Computer Science, University of Rochester, Rochester NY 14627, USA
jialong2019@iscas.ac.cn, ziyaolu2018@stu.xmu.edu.cn

jssu@xmu.edu.cn

Abstract

In aspect-level sentiment classification (ASC),
it is prevalent to equip dominant neural mod-
els with attention mechanisms, for the sake of
acquiring the importance of each context word
on the given aspect. However, such a mecha-
nism tends to excessively focus on a few fre-
quent words with sentiment polarities, while
ignoring infrequent ones. In this paper, we
propose a progressive self-supervised atten-
tion learning approach for neural ASC mod-
els, which automatically mines useful atten-
tion supervision information from a training
corpus to refine attention mechanisms. Specif-
ically, we iteratively conduct sentiment predic-
tions on all training instances. Particularly, at
each iteration, the context word with the maxi-
mum attention weight is extracted as the one
with active/misleading influence on the cor-
rect/incorrect prediction of every instance, and
then the word itself is masked for subsequent
iterations. Finally, we augment the conven-
tional training objective with a regularization
term, which enables ASC models to continue
equally focusing on the extracted active con-
text words while decreasing weights of those
misleading ones. Experimental results on mul-
tiple datasets show that our proposed approach
yields better attention mechanisms, leading to
substantial improvements over the two state-
of-the-art neural ASC models. Source code
and trained models are available.1

1 Introduction

Aspect-level sentiment classification (ASC), as an
indispensable task in sentiment analysis, aims at
inferring the sentiment polarity of an input sen-
tence in a certain aspect. In this regard, pre-
∗Equal contribution
†Corresponding author
1https://github.com/DeepLearnXMU/PSSAttention

vious representative models are mostly discrim-
inative classifiers based on manual feature engi-
neering, such as Support Vector Machine (Kir-
itchenko et al., 2014; Wagner et al., 2014). Re-
cently, with the rapid development of deep learn-
ing, dominant ASC models have evolved into
neural network (NN) based models (Tang et al.,
2016b; Wang et al., 2016; Tang et al., 2016a; Ma
et al., 2017; Chen et al., 2017; Li et al., 2018;
Wang et al., 2018), which are able to automati-
cally learn the aspect-related semantic representa-
tion of an input sentence and thus exhibit better
performance. Usually, these NN-based models are
equipped with attention mechanisms to learn the
importance of each context word towards a given
aspect. It can not be denied that attention mecha-
nisms play vital roles in neural ASC models.

However, the existing attention mechanism in
ASC suffers from a major drawback. Specifically,
it is prone to overly focus on a few frequent words
with sentiment polarities and little attention is laid
upon low-frequency ones. As a result, the perfor-
mance of attentional neural ASC models is still far
from satisfaction. We speculate that this is because
there exist widely “apparent patterns” and “inap-
parent patterns”. Here, “apparent patterns” are
interpreted as high-frequency words with strong
sentiment polarities and “inapparent patterns” are
referred to as low-frequency ones in training data.
As mentioned in (Li et al., 2018; Xu et al., 2018;
Lin et al., 2017), NNs are easily affected by these
two modes: “apparent patterns” tend to be overly
learned while “inapparent patterns” often can not
be fully learned.

Here we use sentences in Table 1 to explain
this defect. In the first three training sentences,
given the fact that the context word “small” oc-
curs frequently with negative sentiment, the atten-



558

Type Sentence Ans./Pred.

Train The [place] is small and crowded but the service is quick . Neg / —

Train The [place] is a bit too small for live music . Neg / —

Train The service is decent even when this small [place] is packed . Neg / —

Test At lunch time , the [place] is crowded . Neg / Pos

Test A small area makes for quiet [place] to study alone . Pos / Neg

Table 1: The example of attention visualization for five sentences, where the first three are training instances and
the last two are test ones. The bracketed bolded words are target aspects. Ans./Pred. = ground-truth/predicted
sentiment label. Words are highlighted with different degrees according to attention weights.

tion mechanism pays more attention to it and di-
rectly relates the sentences containing it with neg-
ative sentiment. This inevitably causes another in-
formative context word “crowded” to be partially
neglected in spite of it also possesses negative sen-
timent. Consequently, a neural ASC model incor-
rectly predicts the sentiment of the last two test
sentences: in the first test sentence, the neural
ASC model fails to capture the negative sentiment
implicated by ”crowded”; while, in the second test
sentence, the attention mechanism directly focuses
on “small” though it is not related to the given
aspect. Therefore, we believe that the attention
mechanism for ASC still leaves tremendous room
for improvement.

One potential solution to the above-mentioned
issue is supervised attention, which, however,
is supposed to be manually annotated, requiring
labor-intense work. In this paper, we propose a
novel progressive self-supervised attention learn-
ing approach for neural ASC models. Our method
is able to automatically and incrementally mine
attention supervision information from a training
corpus, which can be exploited to guide the train-
ing of attention mechanisms in ASC models. The
basic idea behind our approach roots in the fol-
lowing fact: the context word with the maximum
attention weight has the greatest impact on the
sentiment prediction of an input sentence. Thus,
such a context word of a correctly predicted train-
ing instance should be taken into consideration
during the model training. In contrast, the con-
text word in an incorrectly predicted training in-
stance ought to be ignored. To this end, we itera-
tively conduct sentiment predictions on all training
instances. Particularly, at each iteration, we ex-
tract the context word with the maximum attention
weight from each training instance to form atten-
tion supervision information, which can be used

to guide the training of attention mechanism: in
the case of correct prediction, we will remain this
word to be considered; otherwise, the attention
weight of this word is expected to be decreased.
Then, we mask all extracted context words of
each training instance so far and then refollow the
above process to discover more supervision infor-
mation for attention mechanisms. Finally, we aug-
ment the standard training objective with a regu-
larizer, which enforces attention distributions of
these mined context words to be consistent with
their expected distributions.

Our main contributions are three-fold: (1)
Through in-depth analysis, we point out the exist-
ing drawback of the attention mechanism for ASC.
(2) We propose a novel incremental approach to
automatically extract attention supervision infor-
mation for neural ASC models. To the best of
our knowledge, our work is the first attempt to
explore automatic attention supervision informa-
tion mining for ASC. (3) We apply our approach to
two dominant neural ASC models: Memory Net-
work (MN) (Tang et al., 2016b; Wang et al., 2018)
and Transformation Network (TNet) (Li et al.,
2018). Experimental results on several benchmark
datasets demonstrate the effectiveness of our ap-
proach.

2 Background

In this section, we give brief introductions to
MN and TNet, which both achieve satisfy-
ing performance and thus are chosen as the
foundations of our work. Here we introduce
some notations to facilitate subsequent descrip-
tions: x= (x1, x2, ..., xN ) is the input sen-
tence, t= (t1, t2, ..., tT ) is the given target aspect,
y, yp∈{Positive, Negative, Neutral} denote the
ground-truth and the predicted sentiment, respec-
tively.



559

…

"# "$ "%

…

ℎ# ℎ%

…

'# '% ((*)'$

Attention

ℎ$

,-

…

CPT CPT CPT…

,-

"# "$ "%

Bi-LSTM

L×

ℎ#(./#) ℎ$(./#)

ℎ#(.) ℎ$
(.)

CNN/Attention

…

*# *$ *0

… … …

… …

…

*# *$ *1 ℎ2
(./#)

Bi-LSTM

Attention

((*)

Gating

Fully-connected

CPM

ℎ2
(.)

ℎ%(.)

ℎ%(./#)

…

3

3

Figure 1: The framework architecture of MN.

…

"# "$ "%

…

ℎ# ℎ%

…

'# '% ((*)'$

Attention

ℎ$

,-

…

CPT CPT CPT…

,-

"# "$ "%

Bi-LSTM

×L

ℎ#(./#) ℎ$
(./#)

ℎ#(.) ℎ$
(.)

CNN/Attention

…

*# *$ *0

… … …

… … …

…

*# *$ *1 ℎ2
(./#)

Bi-LSTM

Attention

((*)

Gating

Fully-connected

CPM

ℎ2
(.)

ℎ%(./#)

ℎ%(.)

Figure 2: The framework architecture of TNet/TNet-
ATT. Note that TNet-ATT is the variant of TNet replac-
ing CNN with an attention mechanism.

MN (Tang et al., 2016b; Wang et al., 2018). The
framework illustration of MN is given in Figure
1. We first introduce an aspect embedding matrix
converting each target aspect word tj into a vec-
tor representation, and then define the final vec-
tor representation v(t) of t as the averaged as-
pect embedding of its words. Meanwhile, an-
other embedding matrix is used to project each
context word xi to the continuous space stored in
memory, denoted by mi. Then, an internal atten-
tion mechanism is applied to generate the aspect-
related semantic representation o of the sentence
x: o =

∑
isoftmax(v

T
t Mmi)hi, where M is an at-

tention matrix and hi is the final semantic repre-
sentation of xi, induced from a context word em-
bedding matrix. Finally, we use a fully connected
output layer to conduct classification based on o
and v(t).

TNet (Li et al., 2018). Figure 2 provides the
framework illustrations of TNet, which mainly
consists of three components:

(1) The bottom layer is a Bi-LSTM that
transforms the input x into the contextualized
word representations h(0)(x)=(h(0)1 , h

(0)
2 , ..., h

(0)
N )

(i.e. hidden states of Bi-LSTM). (2) The mid-
dle part, as the core of the whole model, con-
tains L layers of Context-Preserving Transforma-
tion (CPT), where word representations are up-
dated as h(l+1)(x)=CPT(h(l)(x)). The key oper-
ation of CPT layers is Target-Specific Transfor-
mation. It contains another Bi-LSTM for gen-
erating v(t) via an attention mechanism, and
then incorporates v(t) into the word representa-
tions. Besides, CPT layers are also equipped
with a Context-Preserving Mechanism (CPM) to
preserve the context information and learn more
abstract word-level features. In the end, we
obtain the word-level semantic representations
h(x)=(h1,h2...,hN ), with hi=h

(L)
i . (3) The top-

most part is a CNN layer used to produce the
aspect-related sentence representation o for the
sentiment classification.

In this work, we consider another alternative
to the original TNet, which replaces its top-
most CNN with an attention mechanism to pro-
duce the aspect-related sentence representation as
o=Atten(h(x), v(t)). In Section 4, we will inves-
tigate the performance of the original TNet and its
variant equipped with an attention mechanism, de-
noted by TNet-ATT.

Training Objective. Both of the above-
mentioned models take the negative log-likelihood
of the gold-truth sentiment tags as their training
objectives:

J(D; θ) = −
∑

(x,t,y)∈D

J(x, t, y; θ)

=
∑

(x,t,y)∈D

d(y) · logd(x, t; θ), (1)

whereD is the training corpus, d(y) is the one-hot
vector of y, d(x, t; θ) is the model-predicted sen-
timent distribution for the pair (x,t), and · denotes
the dot product of two vectors.

3 Our Approach

In this section, we first describe the basic intuition
behind our approach and then provide its details.
Finally, we elaborate how to incorporate the mined
supervision information for attention mechanisms
into neural ASC models. It is noteworthy that our
method is only applied to the training optimization



560

of neural ASC models, without any impact on the
model testing.

3.1 Basic Intuition
The basic intuition of our approach stems from the
following fact: in attentional ASC models, the im-
portance of each context word on the given aspect
mainly depends on its attention weight. Thus, the
context word with the maximum attention weight
has the most important impact on the sentiment
prediction of the input sentence. Therefore, for a
training sentence, if the prediction of ASC model
is correct, we believe that it is reasonable to con-
tinue focusing on this context word. Conversely,
the attention weight of this context word should
be decreased.

However, as previously mentioned, the context
word with the maximum attention weight is of-
ten the one with strong sentiment polarity. It usu-
ally occurs frequently in the training corpus and
thus tends to be overly considered during model
training. This simultaneously leads to the insuf-
ficient learning of other context words, especially
low-frequency ones with sentiment polarities. To
address this problem, one intuitive and feasible
method is to first shield the influence of this most
important context word before reinvestigating ef-
fects of remaining context words of the training
instance. In that case, other low-frequency context
words with sentiment polarities can be discovered
according to their attention weights.

3.2 Details of Our Approach
Based on the above analysis, we propose a novel
incremental approach to automatically mine in-
fluential context words from training instances,
which can be then exploited as attention supervi-
sion information for neural ASC models.

As shown in Algorithm 1, we first use the ini-
tial training corpus D to conduct model training,
and then obtain the initial model parameters θ(0)

(Line 1). Then, we continue training the model
for K iterations, where influential context words
of all training instances can be iteratively extracted
(Lines 6-25). During this process, for each train-
ing instance (x, t, y), we introduce two word sets
initialized as ∅ (Lines 2-5) to record its extracted
context words: (1) sa(x) consists of context words
with active effects on the sentiment prediction of
x. Each word of sa(x) will be encouraged to re-
main considered in the refined model training, and
(2) sm(x) contains context words with misleading

Algorithm 1 : Neural ASC Model Training with
Automatically Mined Attention Supervision Infor-
mation.
Input: D: the initial training corpus;

θinit: the initial model parameters;
�α: the entropy threshold of attention weight distribution;
K: the maximum number of training iterations;

1: θ(0)← Train(D; θinit)
2: for (x, t, y) ∈ D do
3: sa(x)← ∅
4: sm(x)← ∅
5: end for
6: for k = 1, 2...,K do
7: D(k)← ∅
8: for (x, t, y) ∈ D do
9: v(t)← GenAspectRep(t, θ(k−1))

10: x′←MaskWord(x, sa(x), sm(x))
11: h(x′)← GenWordRep(x′, v(t), θ(k−1))
12: yp, α(x′)← SentiPred(h(x′), v(t), θ(k−1))
13: E(α(x′))← CalcEntropy(α(x′))
14: if E(α(x′)) < �α then
15: m← argmax1≤i≤N α(x′i)
16: if yp == y then
17: sa(x)← sa(x) ∪ {x′m}
18: else
19: sm(x)← sm(x) ∪ {x′m}
20: end if
21: end if
22: D(k)← D(k) ∪ (x′, t, y)
23: end for
24: θ(k)← Train(D(k); θ(k−1))
25: end for
26: Ds← ∅
27: for (x, t, y) ∈ D do
28: Ds← Ds ∪ (x, t, y, sa(x), sm(x))
29: end for
30: θ← Train(Ds)
Return: θ;

effects, whose attention weights are expected to
be decreased. Specifically, at the k-th training it-
eration, we adopt the following steps to deal with
(x, t, y):

In Step 1, we first apply the model parameters
θ(k−1) of the previous iteration to generate the as-
pect representation v(t) (Line 9). Importantly, ac-
cording to sa(x) and sm(x), we then mask all pre-
viously extracted context words of x to create a
new sentence x′, where each masked word is re-
placed with a special token “〈mask〉” (Line 10).
In this way, the effects of these context words will
be shielded during the sentiment prediction of x′,
and thus other context words can be potentially ex-
tracted from x′. Finally, we generate the word rep-
resentations h(x′)={h(x′i)}Ni=1 (Line 11).

In Step 2, on the basis of v(t) and h(x′), we



561

Iter Sentence Ans./Pred. E(α(x′)) x′m
1 The [place] is small and crowded but the service is quick . Neg / Neg 2.38 small

2 The [place] is 〈mask〉 and crowded but the service is quick . Neg / Neg 2.59 crowded

3 The [place] is 〈mask〉 and 〈mask〉 but the service is quick . Neg / Pos 2.66 quick

4 The [place] is 〈mask〉 and 〈mask〉 but the service is 〈mask〉 . Neg / Neg 3.07 —

Table 2: The example of mining influential context words from the first training sentence in Table 1. E(α(x′))
denotes the entropy of the attention weight distribution α(x′), �α is entropy threshold set as 3.0, and x′m indicates
the context word with the maximum attention weight. Note that all extracted words are replaced with “〈mask〉”
and their background colors are labeled as white.

leverage θ(k−1) to predict the sentiment of x′ as yp
(Line 12), where the word-level attention weight
distribution α(x′)={α(x′1), α(x′2), ..., α(x′N )}
subjecting to

∑N
i=1 α(x

′
i) = 1 is induced.

In Step 3, we use the entropy E(α(x′)) to mea-
sure the variance of α(x′) (Line 13), which con-
tributes to determine the existence of an influential
context word for the sentiment prediction of x′,

E(α(x′)) = −
N∑
i=1

α(x′i) log(α(x
′
i)). (2)

If E(α(x′)) is less than a threshold �α (Line 14),
we believe that there exists at least one context
word with great effect on the sentiment prediction
of x′. Hence, we extract the context word x′m with
the maximum attention weight (Line 15-20) that
will be exploited as attention supervision informa-
tion to refine the model training. Specifically, we
adopt two strategies to deal with x′m according to
different prediction results on x′: if the prediction
is correct, we wish to continue focusing on x′m and
add it into sa(x) (Lines 16-17); otherwise, we ex-
pect to decrease the attention weight of x′m and
thus include it into sm(x) (Lines 18-19).

In Step 4, we combine x′, t and y as a triple,
and merge it with the collected ones to form a new
training corpus D(k) (Line 22). Then, we lever-
age D(k) to continue updating model parameters
for the next iteration (Line 24). In doing so, we
make our model adaptive to discover more influ-
ential context words.

Through K iterations of the above steps, we
manage to extract influential context words of all
training instances. Table 2 illustrates the context
word mining process of the first sentence shown
in Table 1. In this example, we iteratively extract
three context words in turn: “small”, “crowded”
and “quick”. The former two words are included
in sa(x), while the last one is contained in sm(x).

Finally, the extracted context words of each train-
ing instance will be included into D, forming a
final training corpusDs with attention supervision
information (Lines 26-29), which will be used to
carry out the last model training (Line 30). The
details will be provided in the next subsection.

3.3 Model Training with Attention
Supervision Information

To exploit the above extracted context words to
refine the training of attention mechanisms for
ASC models, we propose a soft attention regular-
izer4(α(sa(x)∪sm(x)), α̂(sa(x)∪sm(x)); θ) to
jointly minimize the standard training objective,
where α(∗) and α̂(∗) denotes the model-induced
and expected attention weight distributions of
words in sa(x)∪sm(x), respectively. More specif-
ically, 4(α(∗), α̂(∗); θ) is an Euclidean Distance
style loss that penalizes the disagreement between
α(∗) and α̂(∗).

As previously analyzed, we expect to equally
continue focusing on the context words of sa(x)
during the final model training. To this end, we
set their expected attention weights to the same
value 1|sa(x)| . By doing so, the weights of words
extracted first will be reduced, and those of words
extracted later will be increased, avoiding the
over-fitting of high-frequency context words with
sentiment polarities and the under-fitting of low-
frequency ones. On the other hand, for the words
in sm(x) with misleading effects on the sentiment
prediction of x, we want to reduce their effects and
thus directly set their expected weights as 0. Back
to the sentence shown in Table 2, both “small”
and “crowded”∈sa(x) are assigned the same ex-
pected weight 0.5, and the expected weight of
“quick”∈sm(x) is 0.

Finally, our objective function on the training
corpus Ds with attention supervision information



562

Domain Dataset #Pos #Neg #Neu

LAPTOP
Train 980 858 454
Test 340 128 171

REST
Train 2159 800 632
Test 730 195 196

TWITTER
Train 1567 1563 3127
Test 174 174 346

Table 3: Datasets in our experiments. #Pos, #Neg and
#Neu denotes the number of instances with Positive,
Negative and Neutral sentiment, respectively.

becomes

Js(Ds; θ) = −
∑

(x,t,y)∈Ds

{J(x, t, y; θ)+ (3)

γ4(α(sa(x) ∪ sm(x)), α̂(sa(x) ∪ sm(x)); θ)},

where J(x, t, y; θ) is the conventional training ob-
jective defined in Equation 1, and γ>0 is a hyper-
parameter that balances the preference between
the conventional loss function and the regulariza-
tion term. In addition to the utilization of attention
supervision information, our method has a further
advantage: it is easier to address the vanishing gra-
dient problem by adding such information into the
intermediate layers of the entire network (Szegedy
et al., 2015), because the supervision of α̂(∗) is
closer to α(∗) than y.

4 Experiments

Datasets. We applied the proposed approach
into MN (Tang et al., 2016b; Wang et al., 2018)
and TNet-ATT (Li et al., 2018) (see Section 2),
and conducted experiments on three benchmark
datasets: LAPTOP, REST (Pontiki et al., 2014)
and TWITTER (Dong et al., 2014). In our
datasets, the target aspect of each sentence has
been provided. Besides, we removed a few in-
stances with conflict sentiment labels as imple-
mented in (Chen et al., 2017). The statistics of
the final datasets are listed in Table 3.

Contrast Models. We referred to our two
enhanced ASC models as MN(+AS) and TNet-
ATT(+AS), and compared them with MN, TNet,
and TNet-ATT. Note our models require addi-
tional K+1-iteration training, therefore, we also
compared them with the above models with addi-
tional K+1-iteration training, which are denoted
as MN(+KT), TNet(+KT) and TNet-ATT(+KT).
Moreover, to investigate effects of different
kinds of attention supervision information, we

0.66

0.68

0.7

0.72

0.74

0.76

0.0 1.0 2.0 3.0 4.0 5.0 6.0 7.0

F1
-s
co
re

LAPTOP

REST

TWITTER

!"

0.59

0.6

0.61

0.62

0.63

0.64

0.65

0.0 1.0 2.0 3.0 4.0 5.0 6.0 7.0

F1
-s
co
re

LAPTOP

REST

TWITTER

!"

Figure 3: Effects of �α on the validation sets using
MN(+AS).

0.66

0.68

0.7

0.72

0.74

0.76

0.0 1.0 2.0 3.0 4.0 5.0 6.0 7.0

F1
-s
co
re

LAPTOP

REST

TWITTER

!"

0.59

0.6

0.61

0.62

0.63

0.64

0.65

0.0 1.0 2.0 3.0 4.0 5.0 6.0 7.0

F1
-s
co
re

LAPTOP

REST

TWITTER

!"

Figure 4: Effects of �α on the validation sets using
TNet-ATT(+AS).

also listed the performance of MN(+ASa) and
MN(+ASm), which only leverage context words
of sa(x) and sm(x), respectively, and the same for
TNet-ATT(+ASa) and TNet-ATT(+ASm).

Training Details. We used pre-trained GloVe
vectors (Pennington et al., 2014) to initialize the
word embeddings with vector dimension 300. For
out-of-vocabulary words, we randomly sampled
their embeddings from the uniform distribution [-
0.25, 0.25], as implemented in (Kim, 2014). Be-
sides, we initialized the other model parameters
uniformly between [-0.01, 0.01]. To alleviate
overfitting, we employed dropout strategy (Hin-
ton et al., 2012) on the input word embeddings of
the LSTM and the ultimate aspect-related sentence
representation. Adam (Kingma and Ba, 2015) was
adopted as the optimizer with the learning rate
0.001.

When implementing our approach, we empir-
ically set the maximum iteration number K as
5, γ in Equation 3 as 0.1 on LAPTOP data set,
0.5 on REST data set and 0.1 on TWITTER data
set, respectively. All hyper-parameters were tuned
on 20% randomly held-out training data. Finally,
we used F1-Macro and accuracy as our evaluation



563

Model LAPTOP REST TWITTER
Macro-F1 Accuracy Macro-F1 Accuracy Macro-F1 Accuracy

MN (Wang et al., 2018) 62.89 68.90 64.34 75.30 — —
MN 63.28 68.97 65.88 77.32 66.17 67.71

MN(+KT) 63.31 68.95 65.86 77.33 66.18 67.78
MN(+ASm) 64.37 69.69 68.40 78.13 67.20 68.90
MN(+ASa) 64.61 69.95 68.59 78.23 67.47 69.17
MN(+AS) 65.24∗∗ 70.53∗∗ 69.15∗∗ 78.75∗ 67.88∗∗ 69.64∗∗

TNet (Li et al., 2018) 71.75 76.54 71.27 80.69 73.60 74.97
TNet 71.82 76.12 71.70 80.35 76.82 77.60

TNet(+KT) 71.74 76.44 71.36 80.59 76.78 77.54
TNet-ATT 71.21 76.06 71.15 80.32 76.53 77.46

TNet-ATT(+KT) 71.44 76.06 71.01 80.50 76.58 77.46
TNet-ATT(+ASm) 72.39 76.89 72.04 80.96 77.42 78.08
TNet-ATT(+ASa) 73.30 77.34 72.67 81.33 77.63 78.47
TNet-ATT(+AS) 73.84∗∗ 77.62∗∗ 72.90∗∗ 81.53∗ 77.72∗∗ 78.61∗

Table 4: Experimental results on various datasets. We directly cited the best experimental results of MN and
TNet reported in (Wang et al., 2018; Li et al., 2018). ∗∗ and ∗ means significant at p <0.01 and p <0.05 over
the baselines (MN, TNet) on each test set, respectively. Here we conducted 1,000 bootstrap tests (Koehn, 2004) to
measure the significance in metric score differences.

measures.

4.1 Effects of �α
�α is a very important hyper-parameter that con-
trols the iteration number of mining attention su-
pervision information (see Line 14 of Algorithm
1). Thus, in this group of experiments, we var-
ied �α from 1.0 to 7.0 with an increment of 1 each
time, so as to investigate its effects on the perfor-
mance of our models on the validation sets.

Figure 3 and 4 show the experimental results
of different models. Specifically, MN(+AS) with
�α=3.0 achieves the best performance, meanwhile,
the optimal performance of TNet-ATT(+AS) is
obtained when �α=4.0. We observe the increase
of �α does not lead to further improvements, which
may be due to more noisy extracted context words.
Because of these results, we set �α for MN(+AS)
and TNet-ATT(+AS) as 3.0 and 4.0 in the follow-
ing experiments, respectively.

4.2 Overall Results

Table 4 provides all the experimental results. To
enhance the persuasiveness of our experimental
results, we also displayed the previously reported
scores of MN (Wang et al., 2018) and TNet (Li
et al., 2018) on the same data set. According to
the experimental results, we can come to the fol-
lowing conclusions:

First, both of our reimplemented MN and TNet
are comparable to their original models reported in
(Wang et al., 2018; Li et al., 2018). These results
show that our reimplemented baselines are com-
petitive. When we replace the CNN of TNet with
an attention mechanism, TNet-ATT is slightly in-
ferior to TNet. Moreover, when we perform ad-
ditional K+1-iteration of training on these mod-
els, their performance has not changed signifi-
cantly, suggesting simply increasing training time
is unable to enhance the performance of the neural
ASC models.

Second, when we apply the proposed approach
into both MN and TNet-ATT, the context words
in sa(x) are more effective than those in sm(x).
This is because the proportion of correctly pre-
dicted training instances is larger than that of in-
correctly ones. Besides, the performance gap be-
tween MN(+ASa) and MN(+ASm) is larger than
that between two variants of TNet-ATT. One un-
derlying reason is that the performance of TNet-
ATT is better than MN, which enables TNet-ATT
to produce more correctly predicted training in-
stances. This in turn brings more attention super-
vision to TNet-ATT than MN.

Finally, when we use both kinds of attention
supervision information, no matter for which met-
ric, MN(+AS) remarkably outperforms MN on all
test sets. Although our TNet-ATT is slightly in-



564

Model Sentence Ans./Pred.

TNet-ATT The [folding chair] i was seated at was uncomfortable . Neg / Neu

TNet-ATT(+AS) The [folding chair] i was seated at was uncomfortable . Neg / Neg

TNet-ATT The [food] did take a few extra minutes ... the cute waiters ... Neu / Pos

TNet-ATT(+AS) The [food] did take a few extra minutes ... the cute waiters ... Neu / Neu

Table 5: Two test cases predicted by TNet-ATT and TNet-ATT(+AS).

ferior to TNet, TNet-ATT(+AS) still significantly
surpasses both TNet and TNet-ATT. These results
strongly demonstrate the effectiveness and gener-
ality of our approach.

4.3 Case Study

In order to know how our method improves neural
ASC models, we deeply analyze attention results
of TNet-ATT and TNet-ATT(+AS). It has been
found that our proposed approach can solve the
above-mentioned two issues well.

Table 5 provides two test cases. TNet-ATT in-
correctly predicts the sentiment of the first test
sentence as neutral. This is because the context
word “uncomfortable” only appears in two train-
ing instances with negative polarities, which dis-
tracts attention from it. When using our approach,
the average attention weight of “uncomfortable”
is increased to 2.6 times than that of baseline in
these two instances. Thus, TNet-ATT(+AS) is
capable of assigning a greater attention weight
(0.0056→0.2940) to this context word, leading to
the correct prediction of the first test sentence. For
the second test sentence, since the context word
“cute” occurs in training instances mostly with
positive polarity, TNet-ATT directly focuses on
this word and then incorrectly predicts the sen-
tence sentiment as positive. Adopting our method,
attention weights of “cute” in training instances
with neural or negative polarity are significantly
decreased. Specifically, in these instances, the av-
erage weight of “cute” is reduced to 0.07 times
of the original. Hence, TNet-ATT(+AS) assigns
a smaller weight (0.1090→0.0062) to “cute” and
achieves the correct sentiment prediction.

5 Related Work

Recently, neural models have been shown to be
successful on ASC. For example, due to its multi-
ple advantages, such as being simpler and faster,
MNs with attention mechanisms (Tang et al.,
2016b; Wang et al., 2018) have been widely used.

Another prevailing neural model is LSTM that
also involves an attention mechanism to explic-
itly capture the importance of each context word
(Wang et al., 2016). Overall, attention mecha-
nisms play crucial roles in all these models.

Following this trend, researchers have resorted
to more sophisticated attention mechanisms to re-
fine neural ASC models. Chen et al., (2017) pro-
posed a multiple-attention mechanism to capture
sentiment features separated by a long distance,
so that it is more robust against irrelevant infor-
mation. An interactive attention network has been
designed by Ma et al., (2017) for ASC, where two
attention networks were introduced to model the
target and context interactively. Liu et al., (2017)
proposed to leverage multiple attentions for ASC:
one obtained from the left context and the other
one acquired from the right context of a given as-
pect. Very recently, transformation-based model
has also been explored for ASC (Li et al., 2018),
and the attention mechanism is replaced by CNN.

Different from these work, our work is in line
with the studies of introducing attention super-
vision to refine the attention mechanism, which
have become hot research topics in several NN-
based NLP tasks, such as event detection (Liu
et al., 2017), machine translation (Liu et al., 2016),
and police killing detection (Nguyen and Nguyen,
2018). However, such supervised attention acqui-
sition is labor-intense. Therefore, we mainly com-
mits to automatic mining supervision information
for attention mechanisms of neural ASC models.
Theoretically, our approach is orthogonal to these
models, and we leave the adaptation of our ap-
proach into these models as future work.

Our work is inspired by two recent models: one
is (Wei et al., 2017) proposed to progressively
mine discriminative object regions using classifi-
cation networks to address the weakly-supervised
semantic segmentation problems, and the other
one is (Xu et al., 2018) where a dropout method
integrating with global information is presented to



565

encourage the model to mine inapparent features
or patterns for text classification. To the best of our
knowledge, our work is the first one to explore au-
tomatic mining of attention supervision informa-
tion for ASC.

6 Conclusion and Future Work

In this paper, we have explored how to automat-
ically mine supervision information for attention
mechanisms of neural ASC models. Through in-
depth analyses, we first point out the defect of
the attention mechanism for ASC: a few frequent
words with sentiment polarities are tend to be
over-learned, while those with low frequency of-
ten lack sufficient learning. Then, we propose a
novel approach to automatically and incrementally
mine attention supervision information for neu-
ral ASC models. These mined information can
be further used to refine the model training via
a regularization term. To verify the effectiveness
of our approach, we apply our approach into two
dominant neural ASC models, where experimental
results demonstrate our method significantly im-
proves the performance of these two models.

Our method is general for attention mecha-
nisms. Thus, we plan to extend our approach
to other neural NLP tasks with attention mech-
anisms, such as neural document classification
(Yang et al., 2016) and neural machine translation
(Zhang et al., 2018).

Acknowledgments

The authors were supported by National Natural
Science Foundation of China (Nos. 61433015,
61672440), NSF Award (No. 1704337), Bei-
jing Advanced Innovation Center for Language
Resources, the Fundamental Research Funds for
the Central Universities (Grant No. ZK1024),
Scientific Research Project of National Language
Committee of China (Grant No. YB135-49), and
Project 2019X0653 supported by XMU Training
Program of Innovation and Enterpreneurship for
Undergraduates. We also thank the reviewers for
their insightful comments.

References

Peng Chen, Zhongqian Sun, Lidong Bing, and Wei
Yang. 2017. Recurrent attention network on mem-
ory for aspect sentiment analysis. In EMNLP.

Li Dong, Furu Wei, Chuanqi Tan, Duyu Tang, Ming
Zhou, and Ke Xu. 2014. Adaptive recursive neural
network for target-dependent twitter sentiment clas-
sification. In ACL.

Geoffrey E. Hinton, Nitish Srivastava, Alex
Krizhevsky, Ilya Sutskever, and Ruslan Salakhut-
dinov. 2012. Improving neural networks by
preventing co-adaptation of feature detectors.
Computer Science.

Yoon Kim. 2014. Convolutional neural networks for
sentence classification. In EMNLP.

Diederik P. Kingma and Jimmy Ba. 2015. Adam: A
method for stochastic optimization. In ICLR.

Svetlana Kiritchenko, Xiaodan Zhu, Colin Cherry, and
Saif Mohammad. 2014. Nrc-canada-2014: Detect-
ing aspects and sentiment in customer reviews. In
SemEval.

Philipp Koehn. 2004. Statistical significance tests for
machine translation evaluation. In EMNLP.

Xin Li, Lidong Bing, Wai Lam, and Bei Shi. 2018.
Transformation networks for target-oriented senti-
ment classification. In ACL.

Tsung-Yi Lin, Priya Goyal, Ross Girshick, Kaiming
He, and Piotr Dolla r. 2017. Focal loss for dense
object detection. In ICCV.

Lemao Liu, Masao Utiyama, Andrew M. Finch, and
Eiichiro Sumita. 2016. Neural machine translation
with supervised attention. In COLING.

Shulin Liu, Yubo Chen, Kang Liu, and Jun Zhao. 2017.
Exploiting argument information to improve event
detection via supervised attention mechanisms. In
ACL.

Dehong Ma, Sujian Li, Xiaodong Zhang, and Houfeng
Wang. 2017. Interactive attention networks for
aspect-level sentiment classification. In IJCAI.

Minh Nguyen and Thien Nguyen. 2018. Who is killed
by police: Introducing supervised attention for hier-
archical lstms. In COLING.

Jeffrey Pennington, Richard Socher, and Christopher
Manning. 2014. Glove: Global vectors for word
representation. In EMNLP.

Maria Pontiki, Dimitris Galanis, John Pavlopoulos,
Harris Papageorgiou, Ion Androutsopoulos, and
Suresh Manandhar. 2014. Semeval-2014 task 4: As-
pect based sentiment analysis. In SemEval.

Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Ser-
manet, Scott E. Reed, Dragomir Anguelov, Du-
mitru Erhan, Vincent Vanhoucke, and Andrew Ra-
binovich. 2015. Going deeper with convolutions. In
CVPR.



566

Duyu Tang, Bing Qin, Xiaocheng Feng, and Ting Liu.
2016a. Effective lstms for target-dependent senti-
ment classification. In COLING.

Duyu Tang, Bing Qin, and Ting Liu. 2016b. Aspect
level sentiment classification with deep memory net-
work. In EMNLP.

Joachim Wagner, Piyush Arora, Santiago Cortes, Utsab
Barman, Dasha Bogdanova, Jennifer Foster, and
Lamia Tounsi. 2014. DCU: aspect-based polarity
classification for semeval task 4. In SemEval.

Shuai Wang, Sahisnu Mazumder, Bing Liu, Mianwei
Zhou, and Yi Chang. 2018. Target-sensitive mem-
ory networks for aspect sentiment classification. In
ACL.

Yequan Wang, Minlie Huang, Xiaoyan Zhu, and
Li Zhao. 2016. Attention-based LSTM for aspect-
level sentiment classification. In EMNLP.

Yunchao Wei, Jiashi Feng, Xiaodan Liang, Ming-Ming
Cheng, Yao Zhao, and Shuicheng Yan. 2017. Object
region mining with adversarial erasing: A simple
classification to semantic segmentation approach. In
CVPR.

Hengru Xu, Shen Li, Renfen Hu, Si Li, and Sheng Gao.
2018. From random to supervised: A novel dropout
mechanism integrated with global information. In
CONLL.

Zichao Yang, Diyi Yang, Chris Dyer, Xiaodong He,
Alex Smola, and Eduard Hovy. 2016. Hierarchical
attention networks for document classification. In
NAACL.

Biao Zhang, Deyi Xiong, and Jinsong Su. 2018. Neu-
ral machine translation with deep attention. IEEE
Transactions on Pattern Analysis and Machine In-
telligence.

Yue Zhang and Jiangming Liu. 2017. Attention mod-
eling for targeted sentiment. In EACL.


