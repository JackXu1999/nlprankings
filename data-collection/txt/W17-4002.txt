





























Transliterated Mobile Keyboard Input via

Weighted Finite-State Transducers

Lars Hellsten†, Brian Roark†, Prasoon Goyal∗, Cyril Allauzen†,

Françoise Beaufays†, Tom Ouyang†, Michael Riley† and David Rybach†

†Google, Inc. ∗New York University

{lhellsten,roark,allauzen,fsb,ouyang,riley,rybach}@google.com

prasoongoyal13@gmail.com

Abstract

We present an extension to a mobile key-

board input decoder based on finite-state

transducers that provides general translit-

eration support, and demonstrate its use

for input of South Asian languages using a

QWERTY keyboard. On-device keyboard

decoders must operate under strict latency

and memory constraints, and we present

several transducer optimizations that al-

low for high accuracy decoding under such

constraints. Our methods yield substan-

tial accuracy improvements and latency re-

ductions over an existing baseline translit-

eration keyboard approach. The resulting

system was launched for 22 languages in

Google Gboard in the first half of 2017.

1 Introduction

The usefulness of weighted finite-state transduc-

ers (WFSTs) has been well-documented for speech

recognition decoding. Large component WFSTs

representing a context-dependent phone sequence

model (C), the pronunciation lexicon (L) and the
language model (G) can be composed into a sin-
gle large transducer (C ◦ L ◦ G, or CLG for
short) mapping from context-dependent phone se-

quences on the input to word sequences on the out-

put and optimized for efficient decoding (Mohri

et al., 2002). In addition to forming the basis of

numerous commercial and research speech recog-

nition engines, this is the approach taken by the

widely-used open-source toolkit Kaldi (Povey et

al., 2011), which makes use of the OpenFst li-

brary (Allauzen et al., 2007) to represent and ma-

nipulate the WFSTs. Decoding via such an opti-

mized graph permits the efficient combination of

acoustic model scores of context-dependent phone

sequences with the scores associated with larger

(word n-gram) sequences contributed by the lan-

guage model.

Speech is not the only uncertain input sequence

modality requiring transcription to yield word

strings – others include optical character recogni-

tion (OCR) and handwriting recognition. WFST-

based methods have also recently been applied

to soft keyboard decoding (Ouyang et al., 2017),

where a sequence of taps or continuous gestures

is decoded to the most likely word or word se-

quence. Unlike typing on a standard physical key-

board (such as a QWERTY keyboard on a laptop),

ambiguity arises on a soft (on-screen) keyboard

due to the relatively small size of the keyboard

(e.g., on a smartphone) and the resulting impreci-

sion of the tap or gesture. In such an approach,

the same sort of off-line composition of weighted

FSTs provides low latency decoding with a modest

memory footprint, which is essential for on-device

keyboard functionality. The FST-based decoder

described in that paper was launched in the Google

Gboard keyboard system in early 2017.

In this paper, we present methods for extend-

ing this finite-state decoding approach for mobile

keyboard input to transliterated keyboards, where

the keyboard representation differs from the output

script. One very common scenario of this sort is

the use of a standard QWERTY-style soft keyboard

for entering text in a language with another writ-

ing system, typically because the Latin alphabet is

simpler to represent on a compact soft keyboard.

Systems for mapping from sequences of symbols

in the target script to sequences of Latin symbols

are known as romanization.

The most widely known romanization system is

Pinyin, which is a fully conventionalized system

for Chinese. For example, the word水 (water) in
Chinese is written as “shuĭ” in Pinyin. Roman-

ization, however, is quite widely used around the

world, with such writing systems as Arabic, Cyril-

10

Proceedings of the 13th International Conference on Finite State Methods and Natural Language Processing, pages 10–19,

Umeå, Sweden, 4–6 September 2017. © 2017 Association for Computational Linguistics

https://doi.org/10.18653/v1/W17-4002

https://doi.org/10.18653/v1/W17-4002


Figure 1: Screen capture of a transliteration keyboard with
a user’s gesture trace for the target word.

lic, Greek and Thai. In many cases, unlike Chi-

nese, there is no agreed upon standard romaniza-

tion system, leading to an increase in ambiguity

and noise when decoding to the target words in the

native script. In this paper, we present a general

transliteration approach applied to South Asian

languages, transliterated from romanized input to a

number of scripts, including Devanagari, Bengali

and Perso-Arabic. Figure 1 shows a screenshot of a

mobile transliteration keyboard for Hindi with the

Devanagari script, showing the trace of the user’s

input gesture. While the user inputs romanized se-

quences, suggested word completions, as well as

output strings, are in the target script.

In this context, we find that a WFST encod-

ing of transliteration models allows for several

optimizations that yield good accuracy under the

strict resource and operating constraints of the on-

device keyboard decoding. In what follows, we

first provide background and preliminaries on In-

dic languages and their various scripts, as well

as finite-state transducer terminology that will be

used. We then describe our transliteration mod-

eling approach and a number of WFST optimiza-

tions that we perform to achieve the accuracy, la-

tency and memory usage operating points. Fi-

nally we present some experiments demonstrat-

ing the impact of the optimizations and compar-

ing to an existing baseline. On Hindi and Tamil

validation sets, we demonstrate strong word er-

ror rate and latency reductions versus an exist-

ing baseline. We conclude by presenting the var-

ious languages and associated scripts for which

transliteration keyboards using this approach have

so far been launched in Google Gboard, and by dis-

cussing future directions of this work.

2 Background and preliminaries

2.1 Indic scripts

While our approach is general enough to be

broadly applicable to any transliteration pair, it is

useful to have a running example, and since we

later evaluate on South Asian languages, it makes

sense to review Indic (or Brahmic) scripts here.

We lack space to provide a full treatment of the

scripts, but will provide details sufficient to under-

stand our overall approach. We refer the interested

reader to Sproat (2003) for further details.

Brahmic scripts are a class of writing systems

that have descended from the Brahmi script and

hence share certain characteristics. Well-known

Brahmic scripts include Devanagari (the standard

script for many languages, including Hindi and

Sanskrit), Bengali, Gujarati and Tamil. Some

South Asian languages have non-Brahmic native

scripts, such as those that use the Perso-Arabic

script (e.g., Urdu) and others that use modern

scripts not descended fromBrahmi (e.g., Ol Chicki

for Santali). Still, Brahmic scripts are pervasive in

South Asian languages, hence a key focus for key-

board entry in these languages.

While the Brahmic scripts can look very dis-

tinct, they have some central shared characteris-

tics. The scripts are organized around what is

termed an orthographic syllable (akṣara), which

groups one or more consonants together with as-

sociated vowels. Each consonant has an inherent

vowel, and other vowels – or different qualities of

the vowels, such as nasalization, or also the ab-

sence of the inherent vowel – are indicated for that

consonant via a mark or diacritic on the consonant.

In most Brahmic scripts, vowels that are used in-

dependently of a consonant – e.g., word initially

– have their own symbols. Finally, sequences of

multiple consonants can be combined via ligatures.

For example, take the word “Sanskrit”, written

संस्कृत in Hindi. The initial syllable सं consists of
the consonant स (s) which comes with its inher-
ent vowel (sa), and an anusvara diacritic indicat-

ing that it is nasalized (san). This is followed by

स्कृ (skr) which consists of four pieces combined
into a single ligature: (1) a consonant स (s) with
(2) a virama diacritic canceling its inherent vowel

स्, followed by (3) another consonantक (k), which
combines into the ligature स्क (sk)1 plus (4) a vo-
calic ‘r’ diacritic, to yield the full glyph. Finally,

1Note the ligature omits the now-redundant virama.

11



the consonant त (t) completes the full word. Note
that this word is encoded as a string of seven Uni-

code codepoints: स ◌ं स ◌् क ◌ृ त, and it is up
to a rendering system to assemble these symbols

into the appropriate two dimensional glyphs. See

Sproat (2003) for more details.

2.2 Romanization and transliteration

Transliteration – converting from one writing sys-

tem to another – is a widespread sequence-to-

sequence mapping problem that arises in multiple

contexts. For example, proper names must be rep-

resented in various writing systems, so transliter-

ating names and places can be very important for

translation or querying knowledge bases. While

it is true that प्रणब मुखज is the president of India
and was born in प￸ म बगंाल, it is more useful for
those who do not read the Devanagari script to be

presented with the information that Indian presi-

dent Pranab Mukherjee was born in West Bengal.

Hence, much work in transliteration is focused on

translation and information retrieval (Knight and

Graehl, 1998; Chen et al., 1998; Virga and Khu-

danpur, 2003; Haizhou et al., 2004; Gupta et al.,

2014).

Knight and Graehl (1998) took pronunciation

as a mediating variable in mapping between

the two writing systems, and explicitly modeled

grapheme-to-phoneme and cross-lingual pronun-

ciation mapping in their model. For example, the

probability of mapping to “Sanskrit” from संस्कृत
would include probabilities for English pronunci-

ation given the written form (e.g., S AE N S K
R IH T), the Hindi pronunciation (S AH N S K
R AX T) given the English pronunciation, and the
Devanagari string given the Hindi pronunciation.

Haizhou et al. (2004) took a more direct model-

ing approach, simply modeling the observed map-

ping between one writing system representation

and the other, with strong gains in accuracy. Un-

der such an approach, transliteration is very similar

to grapheme-to-phoneme recognition (g2p), where

written representations of words (e.g., “sanskrit”)

are converted to pronunciations, represented by

phone symbols (“S AE N S K R IH T” in the
ARPAbet representation). This sort of pronuncia-

tionmodeling is also awell-studied problem, being

very important to both text-to-speech and speech-

to-text, for deriving pronunciations of items that

are out-of-vocabulary. A number of approaches

have been used to model this conversion from

one symbolic representation (letters) to another

(phones) falling in rough monotonic alignment, in-

cluding log linear models (Wu et al., 2014) and

neural sequence models (Rao et al., 2015). Ex-

plicit finite-state methods such as Bisani and Ney

(2008) and Novak et al. (2013; 2015) have been

shown to be very competitive for g2p (Wu et al.,

2014; Rao et al., 2015), and we adopt such meth-

ods here (see Section 3.1).

Romanization is the special case of mapping

from other writing systems to the Latin script, of-

ten to permit easier keyboard input. As we can

see from the Hindi example in the last section, the

number of possible symbols in Brahmic scripts is

large, due to the combinations of consonants and

vowels via diacritics, and the multi-symbol liga-

tures. While keyboard layouts do exist for these

scripts, transliteration from romanized input to the

target Brahmic script is a common form of key-

board input, especially for mobile devices. As

mentioned earlier, there are many romanization

systems for these languages (in contrast to broadly

conventionalized Pinyin systems), making this sort

of transliteration keyboard challenging for Indic

languages (Ahmed et al., 2011).

2.3 Weighted finite-state transducers

A weighted finite-state transducer T =
(Σ,∆, Q, I, F,E,K) consists of: input (Σ)
and output (∆) vocabularies; a finite set of states
Q, of which (without loss of generality) one is
the initial state I , and a subset of states F ⊆ Q
are final states; a weight semiring K; and a set of
transitions (q, σ, δ, w, q′) ∈ E, where q, q′ ∈ Q
are, respectively, the source and destination states

of the transition, σ ∈ Σ, δ ∈ ∆ and w ∈ K.
A weighted finite-state automaton is a special

case where Σ = ∆ and, for every transition
(q, σ, δ, w, q′) ∈ E, σ = δ. For the work in
this paper, we make use of the OpenFst library

(Allauzen et al., 2007) to encode and manipulate

WFSTs, and, unless otherwise stated, use the

tropical semiring for weights.

Encoding n-gram language models as WFSTs

involves the use of failure-transitions (Allauzen

et al., 2003) with a particular ‘canonical’ struc-

ture that corresponds to backoff smoothing. We

use such an encoding for building word-based lan-

guage models in the target language of the key-

board application, and also as an intermediate rep-

resentation in the training of our transliteration

12



transducer, as described in Section 3.1. For this

language model training and encoding, we make

use of the OpenGrm n-gram library (Roark et al.,

2012), which provides counting, smoothing and

pruning functions resulting in an OpenFst encoded

model.

In speech recognition, a pronunciation lexicon,

consisting of words found in the vocabulary of the

n-grammodel along with their pronunciations, can

be compiled into a WFST with input vocabulary

Σ of phones and output vocabulary ∆ of words.
When this lexicon L is composed with the n-gram
model G, various optimizations can be carried out
on the resulting transducer, to share structure and

accrue costs as early as possible (Mohri et al.,

2002). Similar optimizations are possible for key-

board models, where the input vocabularyΣ of the
lexiconL is the letters that spell the words (see sec-
tion 3.3).

2.4 WFST-based keyboard decoding

There are some differences between speech and

keyboard decoding, which are presented in detail

in Ouyang et al. (2017), and which we summa-

rize here. The need for decoding arises in key-

board due to so-called ‘fat finger’ errors, where the

actual point on the keyboard that was touched is

not within the bounding box of the intended letter.

The term for the sequence of letters correspond-

ing to actual touch points is ‘literal’. For exam-

ple, on a mobile device, with a relatively small

QWERTY keyboard, the literal string typed may

be “sabsjriy” when the intended word was “san-

skrit”. In that example, three letters (b,j,y) adja-

cent to the intended letters (n,k,t) were touched.

For possible intended keys, a spatial model pro-

vides likelihoods for touch points, via, for exam-

ple, a Gaussian for each key centered at its central

point, or perhaps a neural network learned from

real touch data. As with speech, there is contex-

tual dependency between adjacent keys, e.g., the

distribution over touch points for a given key may

depend on the previous key typed. The decoder

takes as input a sequence of touch inputs on the

device and combines the context dependent spatial

model scores (analogous to the acoustic model in

speech) with language model scores to determine

the most likely intended word.

One key difference between speech and key-

board decoding is the privileged nature of the lit-

eral string in keyboard decoding. Free entry of text

via a keyboard is by its nature an open-vocabulary

interface, i.e., the user should be able to type what-

ever they want. The keyboard decoder must de-

cide whether the most likely word according to its

models is sufficiently more likely than the literal to

warrant auto-correction. If the margin between the

score of the best scoring candidate and the literal

score falls below the margin required for correc-

tion, the literal is output, thus allowing for out-of-

vocabulary (OOV) items to be typed.

In addition to touch typing, the WFST-based

keyboard decoder also permits gesture input (Zhai

and Kristensson, 2003), which involves tracing a

path from the first letter of a word through all the

letters to the final letter before lifting the finger.

With gesture, there is no natural notion of literal

string, since multiple words may have the same

path, e.g., ‘or’ and ‘our’ on a QWERTY keyboard.

As a result, OOV processing is restricted, much as

with speech, and users must revert to touch typ-

ing to input OOVs. The same decoder, however,

is used to combine scores from a gesture spatial

model with the language model to find the most

likely word string.

We train a weighted transducer to provide likely

transliterations for unseen words and permit non-

canonical transliterations for existing words, in-

creasing uncertainty in the decoder. This is a par-

ticular challenge given the strict constraints that

come with on-device keyboard modeling: latency

can be no more than 20 msec, models must, in ag-

gregate, be on the order of 10MB in size, andmem-

ory usage during decoding is strictly constrained.

3 Methods

3.1 Pair language models

Due to the latency and memory constraints of our

keyboard application, we pursued methods that re-

sulted inmodels that could be encoded in relatively

compact WFSTs. As mentioned in section 2.2,

explicit finite-state methods used for grapheme-

to-phoneme conversion, such as Bisani and Ney

(2008) and Novak et al. (2013; 2015) are very

competitive. The starting point for all such meth-

ods is a simple per-symbol alignment of both the

input string and the output string. Thus, for ex-

ample, the word “phlegm” is pronounced F L EH
M (again using the ARPAbet representation), and
one natural alignment between the grapheme and

phoneme sequences is: p:ϵ h:F l:L e:EH g:ϵ m:M
For transliteration, we can align a Devanagari

13



script Hindi word (संस्कृत) with its romanization
(sanskrit or sanskrt), where we make use of the

Unicode symbol sequence (स◌ंस◌्क◌ृत):
s:स a:ϵ n:◌ं s:स ϵ:◌् k:क r:◌ृ i:ϵ t:त

Note that symbols on either the input or the out-

put may not directly correspond to a symbol on

the other side (such as ‘a’, ‘i’ and ◌् in the above
example), which we represent with an ϵ on the
other side of the transduction. Given a lexicon of

words and their pronunciations or transliterations

(e.g., that ‘sanskrit’ is a romanization of संस्कृत),
expectation maximization (EM) can be straightfor-

wardly used to learn effective alignments of this

sort.

Given an aligned sequence of input:output ‘pair’

symbols such as e:EH, we can build an n-gram
model to produce joint probabilities over se-

quences of such pairs. We refer to these models

as pair language models, though they are alterna-

tively called joint multi-gram models (Bisani and

Ney, 2008). By conditioning the probability of

these input:output mappings on the prior context,

the model appropriately conditions the probability

of h:F on whether the previous mapping was p:ϵ.
As stated above, results have shown these models

yield very similar performance to more complex

and compute-intensive modeling methods (Wu et

al., 2014; Rao et al., 2015), and they can be directly

encoded as WFSTs (Novak et al., 2013; Wu et al.,

2014), making them excellent candidates for low-

resource, low-latency keyboard decoder models.

One useful variant of these models are those that

reduce or eliminate input and/or output ϵ symbols
(insertions/deletions), by allowing the merger of

multiple symbols on either the input or the output

side. For example, for our pronunciation example

above, we may derive an alignment ph:F instead
of p:ϵ h:F, which can also be learned using EM
(Novak et al., 2013; Novak et al., 2015). Given

the structure ofWFST-based language models (see

section 2.3), which make use of ϵ-transitions to
encode backoff, reducing the number of deletions

and insertions is important when reducing epsilon

cycles in the WFST.

3.2 Transliteration WFST optimization

Figure 2 shows a fragment of the bigram pair lan-

guage model encoded as a WFST, with model

weights not shown for clarity reasons. It is a bi-

gram model, so the incoming arcs into any state

are labeled with the same pair symbol, e.g., state 5

1

2 5

3

4

...

6

...

...

...

...

s:स
sa:स

m:म
n:◌ं

k:◌्त
k:क

ri:◌ृ

wa:वsha:श

s:स ्

s:स

s:स ् k:क

Figure 2: Fragment of the topology of a bigram pair lan-
guage model, weights omitted for clarity.

has two incoming arcs, both labeled with k:क. The
weights on each arc are the (typically negative log)

probabilities of the pair symbols on the arcs given

the state.

This WFST is an automaton over pair symbols,

but we need a transducer mapping between Uni-

code codepoints on the input (QWERTY keyboard

letters) and output (target script). If these were sin-

gle symbol pairs, then converting to a transducer

would be trivial: just change from a pair (encoded)

symbol (k:क) to an input symbol (k) and an output
symbol (क). However, as stated earlier, we merge
symbols to reduce the number of insertions or dele-

tions2, as well as to increase the parameterization

of the model, so the conversion to transducer is

slightly more involved.

We illustrate our approach by considering the

four arcs leaving state 1 in the pair language model

automaton in Figure 2, all of which start with an

‘s’ on the input side. Two arcs leaving state 1 have

symbols consisting ofmultiple input Unicode sym-

bols (‘sa’ and ‘sha’); and one of the arcs leaving

state 1 has a symbol consisting of multiple out-

put Unicode symbols (स् which is two symbols स
and◌्). Figure 3 shows transducers that map these
four pair symbols to their corresponding input and

output strings. Figure 3a shows a transducer (call

it I) with pair language model symbols (disam-
biguated with parentheses just for ease of visual-

ization) on the output side and QWERTY key se-

quences on the input side. Figure 3b shows a trans-

ducer (call itO) with pair language model symbols
on the input side and Devanagari Unicode sym-

bols on the output side. For a pair language model

P , we can derive our transliteration transducer T

2We also restrict the transducer to one insertion or deletion

in a row, to avoid epsilon cycles.

14



s:(s:स)

s:(s:स)

a:

s:(s:स)्
s:(sha:श) h:

a:

(s:स):स

(sha:श):श

(s:स)्:स

:◌्a) b)

Figure 3: Fragments of transducers mapping a) from input
Unicode symbols to symbols in the pair language model, and

b) from symbols in the pair language model to output Uni-

code symbols. Parentheses in symbols are just to assist dis-

ambiguation in the figure.

1

2

3

4

...

s:स

a:

:◌्s:श

h:

a:

s:स
s:स

2

4

...

:◌्h:

a:श

a:स
:स

1
s: 3

a) b)

:स

Figure 4: Illustration of conversion of pair language model
to Unicode symbol transducer, showing state 1 of Figure 2: a)

shows a direct conversion into single Unicode symbol trans-

ducer; b) shows a determinized structure which allows struc-

ture sharing and proper weight pushing. Numbered states are

the same as in Figure 2 and unnumbered states are new.

by composing these together: T = I ◦ P ◦ O.
The resulting transducer looks like the fragment

shown in Figure 4a, where the original destination

states from the pair language model in Figure 2 re-

tain their numbers and new states are unnumbered.

This is a simple conversion, with no structure shar-

ing among arcs leaving the state, and the original

model weights are accrued upon leaving state 1.

An alternate construction can be used, resulting

in a transducer with structure like that shown in

Figure 4b. This involves determinizing the input

side of the transducer in Figure 3a mapping be-

tween input strings and pair language model sym-

bols. As there are two paths with “s” on the input

side (which is also a prefix of the other strings) this

is not determinizable as is, much like pronuncia-

tion lexicons are not generally determinizable. We

use a variant of the method presented in Mohri et

al. (2002) to permit determinization in these cases.

In their algorithm, they added an auxiliary phone

symbol to the end of each pronunciation that in-

dicated the word identity. In our case, as seen in

Figure 5a, we create a transducer with each input

symbol paired with ϵ, followed by an extra arc that

pairs an input ϵ with the full pair symbol. We then
encode the transducer (treating the input:output as

a a single symbol), determinize it, then decode it

again to a transducer, resulting in the transducer in

Figure 5b. We further reduce the size of this by

combining sequential arcs labeled with x:ϵ and ϵ:y
into a single arc x:y in cases where the interme-
diate state has only one incoming and one outgo-

ing arc. This results in the transducer in Figure 5c,

which we can use in lieu of the transducer in Fig-

ure 3a to produce the final transducer, as illustrated

in 4b.

While the structure sharing that results from this

weighted determinization is important, its most

important feature is the weight pushing that results.

When building the encoded automaton in Figure

5a, we put the pair language model weight from

the model on the first arc of each path, i.e., the

arcs labeled S:ϵ in this example. When that au-
tomaton is determinized, the weights get pushed

along the path, leaving only theminimum cost over

all paths for which the transition is a prefix. This

correct weight pushing of the transliteration cost

yields major benefit during decoding.

One complication to keep in mind is that the

transliteration cost provided by this model is based

on a joint probability over input/output relations,

not a conditional probability of the input given the

output, which is what is needed (see Section 3.4).

3.3 L ◦G optimization

In speech recognition, the L ◦ G transducer has
phones on the input side and words on the output

side. Unlike speech, however, in the current case

the input symbols are individual Unicode symbols

in the target script, the concatenation of which

spells out the target word. Since the words can be

inferred from the input symbols, we do not need to

store the word labels explicitly; we projectL◦G to
its input labels (with an appropriate word bound-

ary symbol to determine where such boundaries

occur), and the decoder outputs concatenations of

character label strings. Fig. 6 shows the structure

of the optimized L ◦G automaton.
The construction of L◦G is as follows. Starting

with G, replace each non-backoff arc – i.e., arcs
with word labels rather than ϵ – with a linear path
corresponding to the word’s symbols followed by

the word end symbol (here </w>). Common pre-
fixes of each such path leaving a state are merged

to obtain a trie, backoff (ϵ) arcs are retained un-

15



:(s:स)
a:

h:

a) b)

s:

:(sa:स)
s:

:(s:स)् :(sha:श) s:
s: s: :(s:स):(s:स)्

h:

a:

a:

:(sha:श)

a:

:(sa:स)

c)

s: :(s:स):(s:स)्
h:

a:(sha:श)

a:(sa:स)

Figure 5: Fragments of transducers mapping from input Unicode symbols to symbols in the pair language model: a) with
extra transition allowing for determinization; b) determinized on the encoded symbols; and c) combining sequential output and

input ϵ transitions. Parentheses are just to assist disambiguation in the figure.

य ह

◌ा

</w>

◌ं

य ह

. . .

. . .

</w>

</w>

</w>

Figure 6: Illustration of the topology of L ◦ G, for a frag-
ment beginning at the unigram state containing the words या,
या,ं and यह, with the word end delimiter </w>. Weights are
omitted.

changed, and weights are 8-bit quantized. Finally,

the automaton is encoded using a variation of the

LOUDS-based representation in Sorensen and Al-

lauzen (2011). The topology of L ◦ G does not
permit the use of the context trees from that rep-

resentation. Instead, we store explicit 32-bit next

state IDs for each backoff arc, indexed by rank in

an indicator bit vector over states. We also store

a bit vector indicating which states have a context

transition arc, which must be an outgoing arc with

the word end label. To find the destination state

for these arcs, we recursively follow backoff arcs

from the most recent word boundary state in the

L◦G traversal until we’re able to successfully find
a context match.

Note that this structure is suboptimal in state/arc

size, since common suffixes (tails) are not merged.

But the topology allows for the LOUDS-based en-

coding which is more compact than a general com-

paction method could achieve on a more minimal

but less regular topology.

3.4 Transliteration cost normalization

One key complication is the need for a conditional

(not joint) transliteration model. The overall joint

model of the words, transliterations, and touch

points must be broken down into probabilities of:

(1) the words (language modelG); (2) the translit-

eration given the words; and (3) the touch points

given the romanized symbols. Yet the translitera-

tion model T is a joint model of aligned romanized
and word strings. Thus each word requires an ad-

ditional normalization factor.

We achieve this by dividing the probability of

eachw output byL◦G by themarginalization sum:

Nw =
∑

s,t∈T |concat(t)=w

PT (s, t)

where s and t are input and output sequences in
the source and target scripts, respectively. Notice

that in the log semiring, Nw is simply the weight
of the shortest path in T that outputs an encoding
of w. Thus, we project T ◦ L to output labels, de-
terminize, invert weights, and compose the result

with L to get a “conditional lexicon” L′, such that
T ◦L′◦Gmodels the joint probability of a sequence
of words and a corresponding transliteration.

We distribute this normalization factor via

weighted determinization when building the trie

leaving each language model state, as discussed in

Section 3.3 and shown in Figure 6. Similar to the

weight pushing that we get when determinizing the

transliteration transducer, we put the negative nor-

malization cost on the first arc of each path, then

determinize, which distributes the weights cor-

rectly, and finally take the negative again to yield

the correct normalization factor, which is now ac-

crued incrementally at each character. Finally, to

achieve full weight pushing in the L ◦G, we push
the language model weights.

In addition to decoding sequences of taps and

gestures, our keyboard application implements

functionality that predicts the most likely comple-

tions and next word predictions (Ouyang et al.,

2017). These require LM probabilities, which we

can extract from L ◦ G by traversing arcs recur-
sively until an arc with the word end symbol is en-

16



countered. However, note that the L′ ◦G FST de-
scribed above has the transliteration normalization

factor built into the cost. Thus we require a further

modification to the next word prediction algorithm

to remove the normalization factors: we store the

normalization factors in a LOUDS trie (Delpratt

et al., 2006) over the lexicon, and adjust the next

word prediction costs correspondingly.

3.5 Decoder optimization

For conventional models, our keyboard decoder

(Ouyang et al., 2017) uses on-the-fly composition

of (C ◦L) (statically composed) andG using look-
ahead composition filters (Allauzen et al., 2009).

Because of the ambiguity of transliteration, a stat-

ically composedC ◦T ◦L is typically too large for
a mobile application. For example, in a conven-

tional Hindi model, C ◦ L contained fewer than
500k arcs, while incorporating T resulted in 35M
arcs, requiring over 800 MB of storage. We ad-

dressed this by refactoring the composition so that

a static C ◦ T is composed on-the-fly with a static
L ◦G. With the optimizations discussed in previ-
ous sections, the former consists of approximately

100k arcs (1.7 MB), and the latter approximately

3M arcs (6.7 MB), which both fit within our con-

straints.

While the decoder graph optimizations achieve

good model size characteristics, the overall num-

ber of states and arcs is much higher than in con-

ventional models. If we use the same decoder

meta-parameters as we do for a non-transliteration

model, the decoder will search many more states,

increasing latency and memory use. We em-

ployed the following optimizations, relative to

non-transliteratedmodels, to remain within our de-

sired constraints:

Reduced beam width. The decoder prunes low-

scoring hypotheses more aggressively.

Stricter error correction model. The decoder

supports correction of insertion, substitution, and

deletion errors (Ouyang et al., 2017). This model

is assumed to be independent from the translitera-

tion model, whereas in fact there is some overlap.

For instance, doubling or interchanging vowels is

one common source of transliteration ambiguity.

We lowered probabilities in the edit model to help

offset this.

Stricter spatial model. We used a slightly lower

Gaussian variance in the tapping model, optimized

for WER by sweeping over settings.

Latency per word (ms)

Figure 7: Sweep over beam sizes on Hindi dev set before
and after transliteration transducer T weight pushing and L ◦
G weight pushing.

4 Experiments

4.1 Data

As is typical with non-transliterated systems, we

use low order n-gram language models over a lim-

ited vocabulary. For the current study, vocabulary

was 150k words and models were pruned trigrams

with 750k n-grams.

Transliteration model training data consists of

individual word transliterations for each native

word in the vocabulary, collected from a small

number (approximately 5) of speakers drawn from

a larger overall pool of speakers. Each pair of (na-

tive word, transliteration) constituted one training

instance used by the pair LM method described in

Section 3.1. We ensured, using random sampling

where necessary, that all words in the native vo-

cabulary had the same number of training instances

to avoid introducing bias.

Performance was evaluated on blind test

datasets of tapped and gestured sentences in Hindi

and Tamil (over 3000 words total for each input

modality and language). In the data collection

study, participants tapped and gestured sentences

written in the native script on a smartphone

QWERTY keyboard that produced no output,

using their preferred transliteration. The prompts

were sentences sampled from transcribed speech

interactions. Participants were asked to type at

a natural pace and not worry about fixing any

mistakes, to collect natural errors and test the

correction capability of the decoder and robust-

ness of the transliteration modeling. We evaluate

word error rate (WER), which is the number of

errors divided by the number of reference words,

17



L ◦G size (MB)
Language standard compact LOUDS

Hindi 85.3 26.3 6.72

Tamil 126.5 37.1 9.80

Table 1: Size savings via LOUDS L ◦G construction.

presented as a percentage.

The baseline system in our experiments comes

from the Google Indic Keyboard, an existing prod-

uct with between 50 and 100 million installations

from the Google Play Store. It uses an HMM-

based decoder, with a standard n-gram language

model and a transliteration component consisting

of conditional probability distributions P (s | t)
trained using the EM algorithm over a set of sub-

word (primarily syllable) transliteration rules. The

sub-word transliteration probabilities are assumed

to be independent, in contrast to our approach. For

this study, its language model and transliteration

training data were identical to our system.

4.2 WFST optimizations

Table 1 presents the size in megabytes of the L ◦
G compiled in the standard way, with a general
method of compression (compact), and with the

specialized LOUDS format.

Figure 7 presents a sweep over beam search pa-

rameters on the Hindi dev set, for identical mod-

els compiled with and without weight pushing. As

can be seen from that plot, with both transliteration

transducer optimization (section 3.2) andL◦G op-
timization (section 3.3), very large latency reduc-

tions were achieved versus the unoptimized sys-

tem, yielding the lowest word error rate along with

the lowest latencies.3 Due to these optimizations,

low word error rates are achieved within the la-

tency constraints imposed by the application.

4.3 Blind test evaluation

After setting meta-parameters on the development

set for Hindi, we ran the decoder on blind test sets

for both Hindi and Tamil. Table 2 presents a com-

parison between the WFST-based decoder and a

baseline system.

5 Summary and future directions

We have presented a WFST-based approach to

transliteration support in keyboards on mobile de-

3Active memory usage is highly correlated with latency –

being due to decoder search – hence we do not show that plot,

though it shows a similar trend.

Hindi Tamil

System WER LPC WER LPC

Baseline HMM system 19.5 3.0 26.2 2.1

WFST-based 16.4 0.5 22.6 0.2

Table 2: Word error rate (WER) and latency per character
(LPC) results on blind test sets.

Language Script Language Script

Assamese Bengali Manipuri Manipuri

Bengali Bengali Marathi Marathi

Bodo Bengali, Nepali Nepali

Devanagari Odia Odia

Dogri Devanagari, Punjabi Gurmukhi,

Perso-Arabic Perso-Arabic

Gujurati Gujurati Sanskrit Devanagari

Hindi Devanagari Santali Ol Chiki

Kannada Kannada Sindhi Devanagari,

Kashmiri Devanagari, Perso-Arabic

Perso-Arabic Tamil Tamil

Konkani Devanagari Telugu Telugu

Maithili Devanagari Urdu Perso-Arabic

Malayalam Malayalam

Table 3: Languages and scripts launched in GBoard using
the methods presented in this paper.

vices, with specific results in South Asian lan-

guages. The presented WFST optimization meth-

ods yielded very large model size reductions, crit-

ical for on-device models. We also presented

weight pushing approaches that yielded large

speedups in decoding, with a commensurate reduc-

tion in active memory usage. The resulting system

achieves large accuracy improvements in addition

to strong latency reductions.

This approach has been used to build translitera-

tion keyboard systems for the 22 languages shown

in Table 3, some of which can be transliterated to

different scripts. These keyboards were launched

as part of Google Gboard in the first half of 2017.

We continue to investigate new methods for train-

ing such models, including jointly training target

script language models and transliteration models.

Acknowledgments

Thanks to Daan van Esch, Elnaz Sarbar and Cory

Massaro for invaluable support with data curation;

to Richard Sproat for useful guidance at the outset

of this project and helpful perspectives throughout;

and to Yuanbo Zhang for helping understand the

Indic IME approach and facilitating direct compar-

isons of the approaches.

18



References

Umair Z Ahmed, Kalika Bali, Monojit Choudhury, and

Sowmya VB. 2011. Challenges in designing in-

put method editors for Indian languages: The role

of word-origin and context. Workshop on Advances

in Text Input Methods (WTIM 2011), pages 1–9.

Cyril Allauzen, Mehryar Mohri, and Brian Roark.

2003. Generalized algorithms for constructing sta-

tistical language models. In Proceedings of the 41st

Annual Meeting on Association for Computational

Linguistics, pages 40–47.

Cyril Allauzen, Michael Riley, Johan Schalkwyk, Wo-

jciech Skut, and Mehryar Mohri. 2007. OpenFst: A

general and efficient weighted finite-state transducer

library. In Implementation and Application of Au-

tomata, pages 11–23. Springer.

Cyril Allauzen, Michael Riley, and Johan Schalkwyk.

2009. A generalized composition algorithm for

weighted finite-state transducers. In Proceedings of

Interspeech, pages 1203–1206.

Maximilian Bisani and Hermann Ney. 2008. Joint-

sequence models for grapheme-to-phoneme conver-

sion. Speech Communication, 50(5):434–451.

Hsin-Hsi Chen, Sheng-Jie Hueng, Yung-Wei Ding, and

Shih-Chung Tsai. 1998. Proper name translation in

cross-language information retrieval. In Proceed-

ings of the 36th Annual Meeting of the Associa-

tion for Computational Linguistics and 17th Inter-

national Conference on Computational Linguistics-

Volume 1, pages 232–236. Association for Compu-

tational Linguistics.

O’Neil Delpratt, Naila Rahman, and Rajeev Raman.

2006. Engineering the louds succinct tree repre-

sentation. In Carme Àlvarez and María Serna, ed-

itors, Experimental Algorithms: 5th International

Workshop, WEA 2006, Cala Galdana, Menorca,

Spain, May 24-27, 2006. Proceedings, pages 134–

145. Springer Berlin Heidelberg, Berlin, Heidelberg.

Parth Gupta, Kalika Bali, Rafael E Banchs, Monojit

Choudhury, and Paolo Rosso. 2014. Query expan-

sion for mixed-script information retrieval. In Pro-

ceedings of the 37th international ACM SIGIR con-

ference on Research & development in information

retrieval, pages 677–686. ACM.

Li Haizhou, Zhang Min, and Su Jian. 2004. A joint

source-channel model for machine transliteration. In

Proceedings of the 42nd Annual Meeting on associ-

ation for Computational Linguistics, page 159. As-

sociation for Computational Linguistics.

Kevin Knight and Jonathan Graehl. 1998. Ma-

chine transliteration. Computational Linguistics,

24(4):599–612.

Mehryar Mohri, Fernando Pereira, and Michael Ri-

ley. 2002. Weighted finite-state transducers in

speech recognition. Computer Speech & Language,

16(1):69–88.

Josef R Novak, Nobuaki Minematu, and Keikichi Hi-

rose. 2013. Failure transitions for joint n-grammod-

els and g2p conversion. In Proceedings of Inter-

speech.

Josef Robert Novak, NobuakiMinematsu, and Keikichi

Hirose. 2015. Phonetisaurus: Exploring grapheme-

to-phoneme conversion with joint n-gram models in

the WFST framework. Natural Language Engineer-

ing, pages 1–32.

Tom Ouyang, David Rybach, Françoise Beaufays, and

Michael Riley. 2017. Mobile keyboard input de-

coding with finite-state transducers. arXiv preprint

arXiv:1704.03987.

Daniel Povey, Arnab Ghoshal, Gilles Boulianne, Lukas

Burget, Ondrej Glembek, Nagendra Goel, Mirko

Hannemann, Petr Motlicek, Yanmin Qian, Petr

Schwarz, et al. 2011. The Kaldi speech recogni-

tion toolkit. In IEEE workshop on automatic speech

recognition and understanding (ASRU).

Kanishka Rao, Fuchun Peng, Haşim Sak, and Françoise

Beaufays. 2015. Grapheme-to-phoneme conver-

sion using long short-term memory recurrent neural

networks. In Proceedings of ICASSP, pages 4225–

4229.

Brian Roark, Richard Sproat, Cyril Allauzen, Michael

Riley, Jeffrey Sorensen, and Terry Tai. 2012. The

OpenGrm open-source finite-state grammar soft-

ware libraries. In Proceedings of the ACL 2012 Sys-

tem Demonstrations, pages 61–66.

Jeffrey Sorensen and Cyril Allauzen. 2011. Unary data

structures for language models. In Proceedings of

Interspeech, pages 1425–1428.

Richard Sproat. 2003. A formal computational analy-

sis of Indic scripts. In International symposium on

Indic scripts: past and future, Tokyo.

Paola Virga and Sanjeev Khudanpur. 2003. Transliter-

ation of proper names in cross-lingual information

retrieval. In Proceedings of the ACL 2003 work-

shop onMultilingual andmixed-language named en-

tity recognition-Volume 15, pages 57–64. Associa-

tion for Computational Linguistics.

Ke Wu, Cyril Allauzen, Keith Hall, Michael Riley, and

Brian Roark. 2014. Encoding linear models as

weighted finite-state transducers. In Proceedings of

Interspeech.

Shumin Zhai and Per-Ola Kristensson. 2003. Short-

hand writing on stylus keyboard. In Proceedings of

the ACM SIGCHI conference on Human factors in

computing systems, pages 97–104.

19


