



















































Joint Multilingual Supervision for Cross-lingual Entity Linking


Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 2486–2495
Brussels, Belgium, October 31 - November 4, 2018. c©2018 Association for Computational Linguistics

2486

Joint Multilingual Supervision for Cross-lingual Entity Linking

Shyam Upadhyay
University of Pennsylvania

Philadelphia, PA
shyamupa@seas.upenn.edu

Nitish Gupta
University of Pennsylvania

Philadelphia, PA
nitishg@seas.upenn.edu

Dan Roth
University of Pennsylvania

Philadelphia, PA
danroth@seas.upenn.edu

Abstract

Cross-lingual Entity Linking (XEL) aims to
ground entity mentions written in any lan-
guage to an English Knowledge Base (KB),
such as Wikipedia. XEL for most languages
is challenging, owing to limited availability of
resources as supervision. We address this chal-
lenge by developing the first XEL approach
that combines supervision from multiple lan-
guages jointly. This enables our approach to:
(a) augment the limited supervision in the tar-
get language with additional supervision from
a high-resource language (like English), and
(b) train a single entity linking model for mul-
tiple languages, improving upon individually
trained models for each language. Extensive
evaluation on three benchmark datasets across
8 languages shows that our approach signifi-
cantly improves over the current state-of-the-
art. We also provide analyses in two limited re-
source settings: (a) zero-shot setting, when no
supervision in the target language is available,
and in (b) low-resource setting, when some
supervision in the target language is available.
Our analysis provides insights into the limita-
tions of zero-shot XEL approaches in realistic
scenarios, and shows the value of joint super-
vision in low-resource settings.1

1 Introduction

Entity Linking (EL) systems ground entity men-
tions in text to entries in Knowledge Bases (KB),
such as Wikipedia (Mihalcea and Csomai, 2007).
Recently, the task of Cross-lingual Entity Linking
(XEL) has gained attention (McNamee et al., 2011;
Ji et al., 2015; Tsai and Roth, 2016) with the goal of
grounding entity mentions written in any language
to the English Wikipedia. For instance, Figure 1
shows a Tamil (a language with >70 million speak-
ers) and an English mention (shown [enclosed])

1Code at www.github.com/shyamupa/xelms

! ரே s [!வr$l] ம&'m உ*+ வே  - ளை யா23றாr.

Everton won against [Liverpool] in an FA Cup match.

Figure 1: Tamil and English mention contexts containing
[mentions] of the entity Liverpool_F.C. from the respective
Wikipedias. Tamil Wikipedia only has 9 mentions referring to
Liverpool_F.C., whereas English Wikipedia has 5303 such
mentions. Clearly, there is a need to augment the limited
contextual evidence in low-resource languages with evidence
from high-resource languages like English. Tamil sentence
translates to “Suarez plays for [Liverpool] and Uruguay.”

and their mention contexts. XEL involves ground-
ing the Tamil mention (which translates to ‘Liv-
erpool’) to the football club Liverpool_F.C., and
not the city or the university. XEL enables knowl-
edge acquisition directly from documents in any
language, without resorting to machine translation.

Training an EL model requires grounded men-
tions, i.e. mentions of entities that are grounded to
a Knowledge Base (KB), as supervision (Figure 1).
While millions of such mentions are available in
English, by virtue of hyperlinks in the English
Wikipedia, this is not the case for most languages.
This makes learning XEL models challenging, es-
pecially for languages with limited resources (e.g.,
the Tamil Wikipedia is only 1% of the English
Wikipedia in size). To overcome this challenge,
it is desirable to augment the limited contextual
evidence available in the target language with evi-
dence from high-resource languages like English.

We propose XELMS (XEL with Multilingual
Supervision) (§2), the first approach that fulfills
the above desiderata by using multilingual super-
vision to train an XEL model. XELMS represents
the mention contexts of the same entity from differ-
ent languages in the same semantic space using a
single context encoder (§2.1). Language-agnostic
entity representations are jointly learned with the
relevant mention context representations, so that an
entity and its context share similar representations.

www.github.com/shyamupa/xelms


2487

…, ! ரே s, 
உ&' வே , …

! ரே s [!வr$l] 
ம*+m உ&' வே  
- ளை யா23றாr.

Everton won against 
[Liverpool] in a FA 

Cup match.

…, Everton, 
FA_Cup, …

…, ! ரே s, 
உ&' வே , …

Everton won against 
[Liverpool] in a FA 

Cup match.

Everton won against 
[Liverpool] in a FA 

Cup match.

Everton won against 
[Liverpool] in a FA 

Cup match.

! ரே s [!வr$l] 
ம*+m உ&' வே  
- ளை யா23றாr.

t
<latexit sha1_base64="5stcGrXqsoASK8xcqONY8G/32M8=">AAAB8XicbVDLSsNAFL2pr1pfVZduBovgqiQi6LLoxmUF+8A2lMl00g6dTMLMjVBC/8KNC0Xc+jfu/BsnbRbaemDgcM69zLknSKQw6LrfTmltfWNzq7xd2dnd2z+oHh61TZxqxlsslrHuBtRwKRRvoUDJu4nmNAok7wST29zvPHFtRKwecJpwP6IjJULBKFrpsR9RHAdhhrNBtebW3TnIKvEKUoMCzUH1qz+MWRpxhUxSY3qem6CfUY2CST6r9FPDE8omdMR7lioaceNn88QzcmaVIQljbZ9CMld/b2Q0MmYaBXYyT2iWvVz8z+ulGF77mVBJilyxxUdhKgnGJD+fDIXmDOXUEsq0sFkJG1NNGdqSKrYEb/nkVdK+qHtu3bu/rDVuijrKcAKncA4eXEED7qAJLWCg4Ble4c0xzovz7nwsRktOsXMMf+B8/gD3JZEY</latexit><latexit sha1_base64="5stcGrXqsoASK8xcqONY8G/32M8=">AAAB8XicbVDLSsNAFL2pr1pfVZduBovgqiQi6LLoxmUF+8A2lMl00g6dTMLMjVBC/8KNC0Xc+jfu/BsnbRbaemDgcM69zLknSKQw6LrfTmltfWNzq7xd2dnd2z+oHh61TZxqxlsslrHuBtRwKRRvoUDJu4nmNAok7wST29zvPHFtRKwecJpwP6IjJULBKFrpsR9RHAdhhrNBtebW3TnIKvEKUoMCzUH1qz+MWRpxhUxSY3qem6CfUY2CST6r9FPDE8omdMR7lioaceNn88QzcmaVIQljbZ9CMld/b2Q0MmYaBXYyT2iWvVz8z+ulGF77mVBJilyxxUdhKgnGJD+fDIXmDOXUEsq0sFkJG1NNGdqSKrYEb/nkVdK+qHtu3bu/rDVuijrKcAKncA4eXEED7qAJLWCg4Ble4c0xzovz7nwsRktOsXMMf+B8/gD3JZEY</latexit><latexit sha1_base64="5stcGrXqsoASK8xcqONY8G/32M8=">AAAB8XicbVDLSsNAFL2pr1pfVZduBovgqiQi6LLoxmUF+8A2lMl00g6dTMLMjVBC/8KNC0Xc+jfu/BsnbRbaemDgcM69zLknSKQw6LrfTmltfWNzq7xd2dnd2z+oHh61TZxqxlsslrHuBtRwKRRvoUDJu4nmNAok7wST29zvPHFtRKwecJpwP6IjJULBKFrpsR9RHAdhhrNBtebW3TnIKvEKUoMCzUH1qz+MWRpxhUxSY3qem6CfUY2CST6r9FPDE8omdMR7lioaceNn88QzcmaVIQljbZ9CMld/b2Q0MmYaBXYyT2iWvVz8z+ulGF77mVBJilyxxUdhKgnGJD+fDIXmDOXUEsq0sFkJG1NNGdqSKrYEb/nkVdK+qHtu3bu/rDVuijrKcAKncA4eXEED7qAJLWCg4Ble4c0xzovz7nwsRktOsXMMf+B8/gD3JZEY</latexit><latexit sha1_base64="5stcGrXqsoASK8xcqONY8G/32M8=">AAAB8XicbVDLSsNAFL2pr1pfVZduBovgqiQi6LLoxmUF+8A2lMl00g6dTMLMjVBC/8KNC0Xc+jfu/BsnbRbaemDgcM69zLknSKQw6LrfTmltfWNzq7xd2dnd2z+oHh61TZxqxlsslrHuBtRwKRRvoUDJu4nmNAok7wST29zvPHFtRKwecJpwP6IjJULBKFrpsR9RHAdhhrNBtebW3TnIKvEKUoMCzUH1qz+MWRpxhUxSY3qem6CfUY2CST6r9FPDE8omdMR7lioaceNn88QzcmaVIQljbZ9CMld/b2Q0MmYaBXYyT2iWvVz8z+ulGF77mVBJilyxxUdhKgnGJD+fDIXmDOXUEsq0sFkJG1NNGdqSKrYEb/nkVdK+qHtu3bu/rDVuijrKcAKncA4eXEED7qAJLWCg4Ble4c0xzovz7nwsRktOsXMMf+B8/gD3JZEY</latexit>

e
<latexit sha1_base64="7qc84Cdo+wz8L3G9wVRAa1jLUmI=">AAAB8XicbVDLSsNAFL2pr1pfVZduBovgqiQi6LLoxmUF+8A2lMn0ph06mYSZiVBC/8KNC0Xc+jfu/BsnbRbaemDgcM69zLknSATXxnW/ndLa+sbmVnm7srO7t39QPTxq6zhVDFssFrHqBlSj4BJbhhuB3UQhjQKBnWBym/udJ1Sax/LBTBP0IzqSPOSMGis99iNqxkGY4WxQrbl1dw6ySryC1KBAc1D96g9jlkYoDRNU657nJsbPqDKcCZxV+qnGhLIJHWHPUkkj1H42TzwjZ1YZkjBW9klD5urvjYxGWk+jwE7mCfWyl4v/eb3UhNd+xmWSGpRs8VGYCmJikp9PhlwhM2JqCWWK26yEjamizNiSKrYEb/nkVdK+qHtu3bu/rDVuijrKcAKncA4eXEED7qAJLWAg4Rle4c3Rzovz7nwsRktOsXMMf+B8/gDgWpEJ</latexit><latexit sha1_base64="7qc84Cdo+wz8L3G9wVRAa1jLUmI=">AAAB8XicbVDLSsNAFL2pr1pfVZduBovgqiQi6LLoxmUF+8A2lMn0ph06mYSZiVBC/8KNC0Xc+jfu/BsnbRbaemDgcM69zLknSATXxnW/ndLa+sbmVnm7srO7t39QPTxq6zhVDFssFrHqBlSj4BJbhhuB3UQhjQKBnWBym/udJ1Sax/LBTBP0IzqSPOSMGis99iNqxkGY4WxQrbl1dw6ySryC1KBAc1D96g9jlkYoDRNU657nJsbPqDKcCZxV+qnGhLIJHWHPUkkj1H42TzwjZ1YZkjBW9klD5urvjYxGWk+jwE7mCfWyl4v/eb3UhNd+xmWSGpRs8VGYCmJikp9PhlwhM2JqCWWK26yEjamizNiSKrYEb/nkVdK+qHtu3bu/rDVuijrKcAKncA4eXEED7qAJLWAg4Rle4c3Rzovz7nwsRktOsXMMf+B8/gDgWpEJ</latexit><latexit sha1_base64="7qc84Cdo+wz8L3G9wVRAa1jLUmI=">AAAB8XicbVDLSsNAFL2pr1pfVZduBovgqiQi6LLoxmUF+8A2lMn0ph06mYSZiVBC/8KNC0Xc+jfu/BsnbRbaemDgcM69zLknSATXxnW/ndLa+sbmVnm7srO7t39QPTxq6zhVDFssFrHqBlSj4BJbhhuB3UQhjQKBnWBym/udJ1Sax/LBTBP0IzqSPOSMGis99iNqxkGY4WxQrbl1dw6ySryC1KBAc1D96g9jlkYoDRNU657nJsbPqDKcCZxV+qnGhLIJHWHPUkkj1H42TzwjZ1YZkjBW9klD5urvjYxGWk+jwE7mCfWyl4v/eb3UhNd+xmWSGpRs8VGYCmJikp9PhlwhM2JqCWWK26yEjamizNiSKrYEb/nkVdK+qHtu3bu/rDVuijrKcAKncA4eXEED7qAJLWAg4Rle4c3Rzovz7nwsRktOsXMMf+B8/gDgWpEJ</latexit><latexit sha1_base64="7qc84Cdo+wz8L3G9wVRAa1jLUmI=">AAAB8XicbVDLSsNAFL2pr1pfVZduBovgqiQi6LLoxmUF+8A2lMn0ph06mYSZiVBC/8KNC0Xc+jfu/BsnbRbaemDgcM69zLknSATXxnW/ndLa+sbmVnm7srO7t39QPTxq6zhVDFssFrHqBlSj4BJbhhuB3UQhjQKBnWBym/udJ1Sax/LBTBP0IzqSPOSMGis99iNqxkGY4WxQrbl1dw6ySryC1KBAc1D96g9jlkYoDRNU657nJsbPqDKcCZxV+qnGhLIJHWHPUkkj1H42TzwjZ1YZkjBW9klD5urvjYxGWk+jwE7mCfWyl4v/eb3UhNd+xmWSGpRs8VGYCmJikp9PhlwhM2JqCWWK26yEjamizNiSKrYEb/nkVdK+qHtu3bu/rDVuijrKcAKncA4eXEED7qAJLWAg4Rle4c3Rzovz7nwsRktOsXMMf+B8/gDgWpEJ</latexit>

g
<latexit sha1_base64="vmPs/L4DTQ+031t4ylAOlX3U35Q=">AAAB8XicbVDLSsNAFL2pr1pfVZduBovgqiQi6LLoxmUF+8A2lMl00g6dTMLMjVBC/8KNC0Xc+jfu/BsnbRbaemDgcM69zLknSKQw6LrfTmltfWNzq7xd2dnd2z+oHh61TZxqxlsslrHuBtRwKRRvoUDJu4nmNAok7wST29zvPHFtRKwecJpwP6IjJULBKFrpsR9RHAdhNpoNqjW37s5BVolXkBoUaA6qX/1hzNKIK2SSGtPz3AT9jGoUTPJZpZ8anlA2oSPes1TRiBs/myeekTOrDEkYa/sUkrn6eyOjkTHTKLCTeUKz7OXif14vxfDaz4RKUuSKLT4KU0kwJvn5ZCg0ZyinllCmhc1K2JhqytCWVLEleMsnr5L2Rd1z6979Za1xU9RRhhM4hXPw4AoacAdNaAEDBc/wCm+OcV6cd+djMVpyip1j+APn8wfjZJEL</latexit><latexit sha1_base64="vmPs/L4DTQ+031t4ylAOlX3U35Q=">AAAB8XicbVDLSsNAFL2pr1pfVZduBovgqiQi6LLoxmUF+8A2lMl00g6dTMLMjVBC/8KNC0Xc+jfu/BsnbRbaemDgcM69zLknSKQw6LrfTmltfWNzq7xd2dnd2z+oHh61TZxqxlsslrHuBtRwKRRvoUDJu4nmNAok7wST29zvPHFtRKwecJpwP6IjJULBKFrpsR9RHAdhNpoNqjW37s5BVolXkBoUaA6qX/1hzNKIK2SSGtPz3AT9jGoUTPJZpZ8anlA2oSPes1TRiBs/myeekTOrDEkYa/sUkrn6eyOjkTHTKLCTeUKz7OXif14vxfDaz4RKUuSKLT4KU0kwJvn5ZCg0ZyinllCmhc1K2JhqytCWVLEleMsnr5L2Rd1z6979Za1xU9RRhhM4hXPw4AoacAdNaAEDBc/wCm+OcV6cd+djMVpyip1j+APn8wfjZJEL</latexit><latexit sha1_base64="vmPs/L4DTQ+031t4ylAOlX3U35Q=">AAAB8XicbVDLSsNAFL2pr1pfVZduBovgqiQi6LLoxmUF+8A2lMl00g6dTMLMjVBC/8KNC0Xc+jfu/BsnbRbaemDgcM69zLknSKQw6LrfTmltfWNzq7xd2dnd2z+oHh61TZxqxlsslrHuBtRwKRRvoUDJu4nmNAok7wST29zvPHFtRKwecJpwP6IjJULBKFrpsR9RHAdhNpoNqjW37s5BVolXkBoUaA6qX/1hzNKIK2SSGtPz3AT9jGoUTPJZpZ8anlA2oSPes1TRiBs/myeekTOrDEkYa/sUkrn6eyOjkTHTKLCTeUKz7OXif14vxfDaz4RKUuSKLT4KU0kwJvn5ZCg0ZyinllCmhc1K2JhqytCWVLEleMsnr5L2Rd1z6979Za1xU9RRhhM4hXPw4AoacAdNaAEDBc/wCm+OcV6cd+djMVpyip1j+APn8wfjZJEL</latexit><latexit sha1_base64="vmPs/L4DTQ+031t4ylAOlX3U35Q=">AAAB8XicbVDLSsNAFL2pr1pfVZduBovgqiQi6LLoxmUF+8A2lMl00g6dTMLMjVBC/8KNC0Xc+jfu/BsnbRbaemDgcM69zLknSKQw6LrfTmltfWNzq7xd2dnd2z+oHh61TZxqxlsslrHuBtRwKRRvoUDJu4nmNAok7wST29zvPHFtRKwecJpwP6IjJULBKFrpsR9RHAdhNpoNqjW37s5BVolXkBoUaA6qX/1hzNKIK2SSGtPz3AT9jGoUTPJZpZ8anlA2oSPes1TRiBs/myeekTOrDEkYa/sUkrn6eyOjkTHTKLCTeUKz7OXif14vxfDaz4RKUuSKLT4KU0kwJvn5ZCg0ZyinllCmhc1K2JhqytCWVLEleMsnr5L2Rd1z6979Za1xU9RRhhM4hXPw4AoacAdNaAEDBc/wCm+OcV6cd+djMVpyip1j+APn8wfjZJEL</latexit>

Mention Context Encoder

Tamil Mention Contexts (low-resource)

English Mention Contexts (high-resource)

TE-Loss
<latexit sha1_base64="9EVnzPTNABZ8pAfSF5ND/TzHLbA=">AAAB+3icbVBNS8NAEN3Ur1q/Yj16WSyCF0sigh6LInjwUKFf0ISy2W7bpZtN2J1IS8hf8eJBEa/+EW/+G7dtDtr6YODx3gwz84JYcA2O820V1tY3NreK26Wd3b39A/uw3NJRoihr0khEqhMQzQSXrAkcBOvEipEwEKwdjG9nfvuJKc0j2YBpzPyQDCUfcErASD277AGbQOppiht35w+R1lnPrjhVZw68StycVFCOes/+8voRTUImgQqiddd1YvBTooBTwbKSl2gWEzomQ9Y1VJKQaT+d357hU6P08SBSpiTgufp7IiWh1tMwMJ0hgZFe9mbif143gcG1n3IZJ8AkXSwaJAJDhGdB4D5XjIKYGkKo4uZWTEdEEQomrpIJwV1+eZW0LqquU3UfLyu1mzyOIjpGJ+gMuegK1dA9qqMmomiCntErerMy68V6tz4WrQUrnzlCf2B9/gCut5Qx</latexit><latexit sha1_base64="9EVnzPTNABZ8pAfSF5ND/TzHLbA=">AAAB+3icbVBNS8NAEN3Ur1q/Yj16WSyCF0sigh6LInjwUKFf0ISy2W7bpZtN2J1IS8hf8eJBEa/+EW/+G7dtDtr6YODx3gwz84JYcA2O820V1tY3NreK26Wd3b39A/uw3NJRoihr0khEqhMQzQSXrAkcBOvEipEwEKwdjG9nfvuJKc0j2YBpzPyQDCUfcErASD277AGbQOppiht35w+R1lnPrjhVZw68StycVFCOes/+8voRTUImgQqiddd1YvBTooBTwbKSl2gWEzomQ9Y1VJKQaT+d357hU6P08SBSpiTgufp7IiWh1tMwMJ0hgZFe9mbif143gcG1n3IZJ8AkXSwaJAJDhGdB4D5XjIKYGkKo4uZWTEdEEQomrpIJwV1+eZW0LqquU3UfLyu1mzyOIjpGJ+gMuegK1dA9qqMmomiCntErerMy68V6tz4WrQUrnzlCf2B9/gCut5Qx</latexit><latexit sha1_base64="9EVnzPTNABZ8pAfSF5ND/TzHLbA=">AAAB+3icbVBNS8NAEN3Ur1q/Yj16WSyCF0sigh6LInjwUKFf0ISy2W7bpZtN2J1IS8hf8eJBEa/+EW/+G7dtDtr6YODx3gwz84JYcA2O820V1tY3NreK26Wd3b39A/uw3NJRoihr0khEqhMQzQSXrAkcBOvEipEwEKwdjG9nfvuJKc0j2YBpzPyQDCUfcErASD277AGbQOppiht35w+R1lnPrjhVZw68StycVFCOes/+8voRTUImgQqiddd1YvBTooBTwbKSl2gWEzomQ9Y1VJKQaT+d357hU6P08SBSpiTgufp7IiWh1tMwMJ0hgZFe9mbif143gcG1n3IZJ8AkXSwaJAJDhGdB4D5XjIKYGkKo4uZWTEdEEQomrpIJwV1+eZW0LqquU3UfLyu1mzyOIjpGJ+gMuegK1dA9qqMmomiCntErerMy68V6tz4WrQUrnzlCf2B9/gCut5Qx</latexit><latexit sha1_base64="9EVnzPTNABZ8pAfSF5ND/TzHLbA=">AAAB+3icbVBNS8NAEN3Ur1q/Yj16WSyCF0sigh6LInjwUKFf0ISy2W7bpZtN2J1IS8hf8eJBEa/+EW/+G7dtDtr6YODx3gwz84JYcA2O820V1tY3NreK26Wd3b39A/uw3NJRoihr0khEqhMQzQSXrAkcBOvEipEwEKwdjG9nfvuJKc0j2YBpzPyQDCUfcErASD277AGbQOppiht35w+R1lnPrjhVZw68StycVFCOes/+8voRTUImgQqiddd1YvBTooBTwbKSl2gWEzomQ9Y1VJKQaT+d357hU6P08SBSpiTgufp7IiWh1tMwMJ0hgZFe9mbif143gcG1n3IZJ8AkXSwaJAJDhGdB4D5XjIKYGkKo4uZWTEdEEQomrpIJwV1+eZW0LqquU3UfLyu1mzyOIjpGJ+gMuegK1dA9qqMmomiCntErerMy68V6tz4WrQUrnzlCf2B9/gCut5Qx</latexit>

TC-Loss
<latexit sha1_base64="ILdXp4IXvXZpzw/tlJ3GmKxXa3s=">AAAB+3icbVBNS8NAEN3Ur1q/Yj16WSyCF0sigh6LvXjwUKFf0ISy2W7bpZts2J1IS8hf8eJBEa/+EW/+G7dtDtr6YODx3gwz84JYcA2O820VNja3tneKu6W9/YPDI/u43NYyUZS1qBRSdQOimeARawEHwbqxYiQMBOsEk/rc7zwxpbmMmjCLmR+SUcSHnBIwUt8ue8CmkHqa4mb98kFqnfXtilN1FsDrxM1JBeVo9O0vbyBpErIIqCBa91wnBj8lCjgVLCt5iWYxoRMyYj1DIxIy7aeL2zN8bpQBHkplKgK8UH9PpCTUehYGpjMkMNar3lz8z+slMLz1Ux7FCbCILhcNE4FB4nkQeMAVoyBmhhCquLkV0zFRhIKJq2RCcFdfXiftq6rrVN3H60rtLo+jiE7RGbpALrpBNXSPGqiFKJqiZ/SK3qzMerHerY9la8HKZ07QH1ifP6ujlC8=</latexit><latexit sha1_base64="ILdXp4IXvXZpzw/tlJ3GmKxXa3s=">AAAB+3icbVBNS8NAEN3Ur1q/Yj16WSyCF0sigh6LvXjwUKFf0ISy2W7bpZts2J1IS8hf8eJBEa/+EW/+G7dtDtr6YODx3gwz84JYcA2O820VNja3tneKu6W9/YPDI/u43NYyUZS1qBRSdQOimeARawEHwbqxYiQMBOsEk/rc7zwxpbmMmjCLmR+SUcSHnBIwUt8ue8CmkHqa4mb98kFqnfXtilN1FsDrxM1JBeVo9O0vbyBpErIIqCBa91wnBj8lCjgVLCt5iWYxoRMyYj1DIxIy7aeL2zN8bpQBHkplKgK8UH9PpCTUehYGpjMkMNar3lz8z+slMLz1Ux7FCbCILhcNE4FB4nkQeMAVoyBmhhCquLkV0zFRhIKJq2RCcFdfXiftq6rrVN3H60rtLo+jiE7RGbpALrpBNXSPGqiFKJqiZ/SK3qzMerHerY9la8HKZ07QH1ifP6ujlC8=</latexit><latexit sha1_base64="ILdXp4IXvXZpzw/tlJ3GmKxXa3s=">AAAB+3icbVBNS8NAEN3Ur1q/Yj16WSyCF0sigh6LvXjwUKFf0ISy2W7bpZts2J1IS8hf8eJBEa/+EW/+G7dtDtr6YODx3gwz84JYcA2O820VNja3tneKu6W9/YPDI/u43NYyUZS1qBRSdQOimeARawEHwbqxYiQMBOsEk/rc7zwxpbmMmjCLmR+SUcSHnBIwUt8ue8CmkHqa4mb98kFqnfXtilN1FsDrxM1JBeVo9O0vbyBpErIIqCBa91wnBj8lCjgVLCt5iWYxoRMyYj1DIxIy7aeL2zN8bpQBHkplKgK8UH9PpCTUehYGpjMkMNar3lz8z+slMLz1Ux7FCbCILhcNE4FB4nkQeMAVoyBmhhCquLkV0zFRhIKJq2RCcFdfXiftq6rrVN3H60rtLo+jiE7RGbpALrpBNXSPGqiFKJqiZ/SK3qzMerHerY9la8HKZ07QH1ifP6ujlC8=</latexit><latexit sha1_base64="ILdXp4IXvXZpzw/tlJ3GmKxXa3s=">AAAB+3icbVBNS8NAEN3Ur1q/Yj16WSyCF0sigh6LvXjwUKFf0ISy2W7bpZts2J1IS8hf8eJBEa/+EW/+G7dtDtr6YODx3gwz84JYcA2O820VNja3tneKu6W9/YPDI/u43NYyUZS1qBRSdQOimeARawEHwbqxYiQMBOsEk/rc7zwxpbmMmjCLmR+SUcSHnBIwUt8ue8CmkHqa4mb98kFqnfXtilN1FsDrxM1JBeVo9O0vbyBpErIIqCBa91wnBj8lCjgVLCt5iWYxoRMyYj1DIxIy7aeL2zN8bpQBHkplKgK8UH9PpCTUehYGpjMkMNar3lz8z+slMLz1Ux7FCbCILhcNE4FB4nkQeMAVoyBmhhCquLkV0zFRhIKJq2RCcFdfXiftq6rrVN3H60rtLo+jiE7RGbpALrpBNXSPGqiFKJqiZ/SK3qzMerHerY9la8HKZ07QH1ifP6ujlC8=</latexit>

EC-Loss
<latexit sha1_base64="yWgfIQ3Qy5EO0yvNb3qfkwPc3N8=">AAAB+3icbVBNS8NAEN3Ur1q/Yj16WSyCF0sigh6LRfDgoYL9gCaUzXbbLt1swu5EWkL+ihcPinj1j3jz37htc9DWBwOP92aYmRfEgmtwnG+rsLa+sblV3C7t7O7tH9iH5ZaOEkVZk0YiUp2AaCa4ZE3gIFgnVoyEgWDtYFyf+e0npjSP5CNMY+aHZCj5gFMCRurZZQ/YBFJPU3xbP7+PtM56dsWpOnPgVeLmpIJyNHr2l9ePaBIyCVQQrbuuE4OfEgWcCpaVvESzmNAxGbKuoZKETPvp/PYMnxqljweRMiUBz9XfEykJtZ6GgekMCYz0sjcT//O6CQyu/ZTLOAEm6WLRIBEYIjwLAve5YhTE1BBCFTe3YjoiilAwcZVMCO7yy6ukdVF1nar7cFmp3eRxFNExOkFnyEVXqIbuUAM1EUUT9Ixe0ZuVWS/Wu/WxaC1Y+cwR+gPr8weUfpQg</latexit><latexit sha1_base64="yWgfIQ3Qy5EO0yvNb3qfkwPc3N8=">AAAB+3icbVBNS8NAEN3Ur1q/Yj16WSyCF0sigh6LRfDgoYL9gCaUzXbbLt1swu5EWkL+ihcPinj1j3jz37htc9DWBwOP92aYmRfEgmtwnG+rsLa+sblV3C7t7O7tH9iH5ZaOEkVZk0YiUp2AaCa4ZE3gIFgnVoyEgWDtYFyf+e0npjSP5CNMY+aHZCj5gFMCRurZZQ/YBFJPU3xbP7+PtM56dsWpOnPgVeLmpIJyNHr2l9ePaBIyCVQQrbuuE4OfEgWcCpaVvESzmNAxGbKuoZKETPvp/PYMnxqljweRMiUBz9XfEykJtZ6GgekMCYz0sjcT//O6CQyu/ZTLOAEm6WLRIBEYIjwLAve5YhTE1BBCFTe3YjoiilAwcZVMCO7yy6ukdVF1nar7cFmp3eRxFNExOkFnyEVXqIbuUAM1EUUT9Ixe0ZuVWS/Wu/WxaC1Y+cwR+gPr8weUfpQg</latexit><latexit sha1_base64="yWgfIQ3Qy5EO0yvNb3qfkwPc3N8=">AAAB+3icbVBNS8NAEN3Ur1q/Yj16WSyCF0sigh6LRfDgoYL9gCaUzXbbLt1swu5EWkL+ihcPinj1j3jz37htc9DWBwOP92aYmRfEgmtwnG+rsLa+sblV3C7t7O7tH9iH5ZaOEkVZk0YiUp2AaCa4ZE3gIFgnVoyEgWDtYFyf+e0npjSP5CNMY+aHZCj5gFMCRurZZQ/YBFJPU3xbP7+PtM56dsWpOnPgVeLmpIJyNHr2l9ePaBIyCVQQrbuuE4OfEgWcCpaVvESzmNAxGbKuoZKETPvp/PYMnxqljweRMiUBz9XfEykJtZ6GgekMCYz0sjcT//O6CQyu/ZTLOAEm6WLRIBEYIjwLAve5YhTE1BBCFTe3YjoiilAwcZVMCO7yy6ukdVF1nar7cFmp3eRxFNExOkFnyEVXqIbuUAM1EUUT9Ixe0ZuVWS/Wu/WxaC1Y+cwR+gPr8weUfpQg</latexit><latexit sha1_base64="yWgfIQ3Qy5EO0yvNb3qfkwPc3N8=">AAAB+3icbVBNS8NAEN3Ur1q/Yj16WSyCF0sigh6LRfDgoYL9gCaUzXbbLt1swu5EWkL+ihcPinj1j3jz37htc9DWBwOP92aYmRfEgmtwnG+rsLa+sblV3C7t7O7tH9iH5ZaOEkVZk0YiUp2AaCa4ZE3gIFgnVoyEgWDtYFyf+e0npjSP5CNMY+aHZCj5gFMCRurZZQ/YBFJPU3xbP7+PtM56dsWpOnPgVeLmpIJyNHr2l9ePaBIyCVQQrbuuE4OfEgWcCpaVvESzmNAxGbKuoZKETPvp/PYMnxqljweRMiUBz9XfEykJtZ6GgekMCYz0sjcT//O6CQyu/ZTLOAEm6WLRIBEYIjwLAve5YhTE1BBCFTe3YjoiilAwcZVMCO7yy6ukdVF1nar7cFmp3eRxFNExOkFnyEVXqIbuUAM1EUUT9Ixe0ZuVWS/Wu/WxaC1Y+cwR+gPr8weUfpQg</latexit>

Tr
ai

ni
ng

 m
en

tio
n 

co
nt

ex
ts

 o
rig

in
at

e 
fro

m
 tw

o 
(o

r m
or

e)
 la

ng
ua

ge
s Everton won against 

[Liverpool] in a FA 
Cup match.

! ரே s [!வr$l] 
ம*+m உ&' வே  
- ளை யா23றாr.

Liverpool F.C.
<latexit sha1_base64="TwSIEjixfw47RyeJLAjpqXQA4Pc=">AAACA3icbVDLSsNAFJ3UV62vqDvdDBbBVUhE0GWxIC5cVLAPaEKZTKft0EkmzNyIJRTc+CtuXCji1p9w5984bbPQ1gMDh3PO5c49YSK4Btf9tgpLyyura8X10sbm1vaOvbvX0DJVlNWpFFK1QqKZ4DGrAwfBWoliJAoFa4bD6sRv3jOluYzvYJSwICL9mPc4JWCkjn3gA3uAzAfAN9wEEymF37lyqs64Y5ddx50CLxIvJ2WUo9axv/yupGnEYqCCaN323ASCjCjgVLBxyU81Swgdkj5rGxqTiOkgm94wxsdG6eKeVObFgKfq74mMRFqPotAkIwIDPe9NxP+8dgq9iyDjcZICi+lsUS8VGCSeFIK7XDEKYmQIoYqbv2I6IIpQMLWVTAne/MmLpHHqeK7j3Z6VK5d5HUV0iI7QCfLQOaqga1RDdUTRI3pGr+jNerJerHfrYxYtWPnMPvoD6/MHte2XiA==</latexit><latexit sha1_base64="TwSIEjixfw47RyeJLAjpqXQA4Pc=">AAACA3icbVDLSsNAFJ3UV62vqDvdDBbBVUhE0GWxIC5cVLAPaEKZTKft0EkmzNyIJRTc+CtuXCji1p9w5984bbPQ1gMDh3PO5c49YSK4Btf9tgpLyyura8X10sbm1vaOvbvX0DJVlNWpFFK1QqKZ4DGrAwfBWoliJAoFa4bD6sRv3jOluYzvYJSwICL9mPc4JWCkjn3gA3uAzAfAN9wEEymF37lyqs64Y5ddx50CLxIvJ2WUo9axv/yupGnEYqCCaN323ASCjCjgVLBxyU81Swgdkj5rGxqTiOkgm94wxsdG6eKeVObFgKfq74mMRFqPotAkIwIDPe9NxP+8dgq9iyDjcZICi+lsUS8VGCSeFIK7XDEKYmQIoYqbv2I6IIpQMLWVTAne/MmLpHHqeK7j3Z6VK5d5HUV0iI7QCfLQOaqga1RDdUTRI3pGr+jNerJerHfrYxYtWPnMPvoD6/MHte2XiA==</latexit><latexit sha1_base64="TwSIEjixfw47RyeJLAjpqXQA4Pc=">AAACA3icbVDLSsNAFJ3UV62vqDvdDBbBVUhE0GWxIC5cVLAPaEKZTKft0EkmzNyIJRTc+CtuXCji1p9w5984bbPQ1gMDh3PO5c49YSK4Btf9tgpLyyura8X10sbm1vaOvbvX0DJVlNWpFFK1QqKZ4DGrAwfBWoliJAoFa4bD6sRv3jOluYzvYJSwICL9mPc4JWCkjn3gA3uAzAfAN9wEEymF37lyqs64Y5ddx50CLxIvJ2WUo9axv/yupGnEYqCCaN323ASCjCjgVLBxyU81Swgdkj5rGxqTiOkgm94wxsdG6eKeVObFgKfq74mMRFqPotAkIwIDPe9NxP+8dgq9iyDjcZICi+lsUS8VGCSeFIK7XDEKYmQIoYqbv2I6IIpQMLWVTAne/MmLpHHqeK7j3Z6VK5d5HUV0iI7QCfLQOaqga1RDdUTRI3pGr+jNerJerHfrYxYtWPnMPvoD6/MHte2XiA==</latexit><latexit sha1_base64="TwSIEjixfw47RyeJLAjpqXQA4Pc=">AAACA3icbVDLSsNAFJ3UV62vqDvdDBbBVUhE0GWxIC5cVLAPaEKZTKft0EkmzNyIJRTc+CtuXCji1p9w5984bbPQ1gMDh3PO5c49YSK4Btf9tgpLyyura8X10sbm1vaOvbvX0DJVlNWpFFK1QqKZ4DGrAwfBWoliJAoFa4bD6sRv3jOluYzvYJSwICL9mPc4JWCkjn3gA3uAzAfAN9wEEymF37lyqs64Y5ddx50CLxIvJ2WUo9axv/yupGnEYqCCaN323ASCjCjgVLBxyU81Swgdkj5rGxqTiOkgm94wxsdG6eKeVObFgKfq74mMRFqPotAkIwIDPe9NxP+8dgq9iyDjcZICi+lsUS8VGCSeFIK7XDEKYmQIoYqbv2I6IIpQMLWVTAne/MmLpHHqeK7j3Z6VK5d5HUV0iI7QCfLQOaqga1RDdUTRI3pGr+jNerJerHfrYxYtWPnMPvoD6/MHte2XiA==</latexit>

sports team
<latexit sha1_base64="TE+2DbEyQJlHBLUSvDdIjyd+QjM=">AAACAHicbVDLSsNAFJ3UV62vqAsXbgaL4KokIuiy6MZlBfuAJoTJdNIOnUzCzI1YQjb+ihsXirj1M9z5N07bLLT1wMDhnHu4c0+YCq7Bcb6tysrq2vpGdbO2tb2zu2fvH3R0kinK2jQRieqFRDPBJWsDB8F6qWIkDgXrhuObqd99YErzRN7DJGV+TIaSR5wSMFJgH3nAHiH3ALBOEwXaC8DEi8CuOw1nBrxM3JLUUYlWYH95g4RmMZNABdG67zop+DlRwKlgRc3LNEsJHZMh6xsqScy0n88OKPCpUQY4SpR5EvBM/Z3ISaz1JA7NZExgpBe9qfif188guvJzLtMMmKTzRVEmMCR42gYecMUoiIkhhCpu/orpiChCwXRWMyW4iycvk855w3Ua7t1FvXld1lFFx+gEnSEXXaImukUt1EYUFegZvaI368l6sd6tj/loxSozh+gPrM8ftTOXGw==</latexit><latexit sha1_base64="TE+2DbEyQJlHBLUSvDdIjyd+QjM=">AAACAHicbVDLSsNAFJ3UV62vqAsXbgaL4KokIuiy6MZlBfuAJoTJdNIOnUzCzI1YQjb+ihsXirj1M9z5N07bLLT1wMDhnHu4c0+YCq7Bcb6tysrq2vpGdbO2tb2zu2fvH3R0kinK2jQRieqFRDPBJWsDB8F6qWIkDgXrhuObqd99YErzRN7DJGV+TIaSR5wSMFJgH3nAHiH3ALBOEwXaC8DEi8CuOw1nBrxM3JLUUYlWYH95g4RmMZNABdG67zop+DlRwKlgRc3LNEsJHZMh6xsqScy0n88OKPCpUQY4SpR5EvBM/Z3ISaz1JA7NZExgpBe9qfif188guvJzLtMMmKTzRVEmMCR42gYecMUoiIkhhCpu/orpiChCwXRWMyW4iycvk855w3Ua7t1FvXld1lFFx+gEnSEXXaImukUt1EYUFegZvaI368l6sd6tj/loxSozh+gPrM8ftTOXGw==</latexit><latexit sha1_base64="TE+2DbEyQJlHBLUSvDdIjyd+QjM=">AAACAHicbVDLSsNAFJ3UV62vqAsXbgaL4KokIuiy6MZlBfuAJoTJdNIOnUzCzI1YQjb+ihsXirj1M9z5N07bLLT1wMDhnHu4c0+YCq7Bcb6tysrq2vpGdbO2tb2zu2fvH3R0kinK2jQRieqFRDPBJWsDB8F6qWIkDgXrhuObqd99YErzRN7DJGV+TIaSR5wSMFJgH3nAHiH3ALBOEwXaC8DEi8CuOw1nBrxM3JLUUYlWYH95g4RmMZNABdG67zop+DlRwKlgRc3LNEsJHZMh6xsqScy0n88OKPCpUQY4SpR5EvBM/Z3ISaz1JA7NZExgpBe9qfif188guvJzLtMMmKTzRVEmMCR42gYecMUoiIkhhCpu/orpiChCwXRWMyW4iycvk855w3Ua7t1FvXld1lFFx+gEnSEXXaImukUt1EYUFegZvaI368l6sd6tj/loxSozh+gPrM8ftTOXGw==</latexit><latexit sha1_base64="TE+2DbEyQJlHBLUSvDdIjyd+QjM=">AAACAHicbVDLSsNAFJ3UV62vqAsXbgaL4KokIuiy6MZlBfuAJoTJdNIOnUzCzI1YQjb+ihsXirj1M9z5N07bLLT1wMDhnHu4c0+YCq7Bcb6tysrq2vpGdbO2tb2zu2fvH3R0kinK2jQRieqFRDPBJWsDB8F6qWIkDgXrhuObqd99YErzRN7DJGV+TIaSR5wSMFJgH3nAHiH3ALBOEwXaC8DEi8CuOw1nBrxM3JLUUYlWYH95g4RmMZNABdG67zop+DlRwKlgRc3LNEsJHZMh6xsqScy0n88OKPCpUQY4SpR5EvBM/Z3ISaz1JA7NZExgpBe9qfif188guvJzLtMMmKTzRVEmMCR42gYecMUoiIkhhCpu/orpiChCwXRWMyW4iycvk855w3Ua7t1FvXld1lFFx+gEnSEXXaImukUt1EYUFegZvaI368l6sd6tj/loxSozh+gPrM8ftTOXGw==</latexit>

…, Everton, 
FA_Cup, …

Everton won against 
[Liverpool] in an FA 

Cup match.

local context 
document context 

…, Everton, 
FA_Cup, …
…, Everton, 
FA_Cup, …
…, Everton, 
FA_Cup, …
…, Everton, 
FA_Cup, …

…, ! ரே s, 
உ&' வே , …

(a) Overview of XELMS. Mentions are shown [enclosed].

Everton won against 
[Liverpool] in an FA Cup match.

…, Everton, 
FA_Cup, …

Left Local 
Context 
Encoder

Right Local 
Context 
Encoder

… … … …

l
<latexit sha1_base64="x65kFaBF/HZpH/Ng0a+ynMI2q6M=">AAACAnicbZDLSsNAFIZP6q3GW9SVuBksBVclEUGXRTcuK9gLNKFMppN26OTCzEQoIbjxVdy4UMStT+HOt3HSRtDWHwY+/nMOc87vJ5xJZdtfRmVldW19o7ppbm3v7O5Z+wcdGaeC0DaJeSx6PpaUs4i2FVOc9hJBcehz2vUn10W9e0+FZHF0p6YJ9UI8iljACFbaGlhHbojV2A8ynpv1H3Yp5/nAqtkNeya0DE4JNSjVGlif7jAmaUgjRTiWsu/YifIyLBQjnOamm0qaYDLBI9rXGOGQSi+bnZCjunaGKIiFfpFCM/f3RIZDKaehrzuLJeVirTD/q/VTFVx6GYuSVNGIzD8KUo5UjIo80JAJShSfasBEML0rImMsMFE6NVOH4CyevAyds4ZjN5zb81rzqoyjCsdwAqfgwAU04QZa0AYCD/AEL/BqPBrPxpvxPm+tGOXMIfyR8fENW+SXZQ==</latexit><latexit sha1_base64="x65kFaBF/HZpH/Ng0a+ynMI2q6M=">AAACAnicbZDLSsNAFIZP6q3GW9SVuBksBVclEUGXRTcuK9gLNKFMppN26OTCzEQoIbjxVdy4UMStT+HOt3HSRtDWHwY+/nMOc87vJ5xJZdtfRmVldW19o7ppbm3v7O5Z+wcdGaeC0DaJeSx6PpaUs4i2FVOc9hJBcehz2vUn10W9e0+FZHF0p6YJ9UI8iljACFbaGlhHbojV2A8ynpv1H3Yp5/nAqtkNeya0DE4JNSjVGlif7jAmaUgjRTiWsu/YifIyLBQjnOamm0qaYDLBI9rXGOGQSi+bnZCjunaGKIiFfpFCM/f3RIZDKaehrzuLJeVirTD/q/VTFVx6GYuSVNGIzD8KUo5UjIo80JAJShSfasBEML0rImMsMFE6NVOH4CyevAyds4ZjN5zb81rzqoyjCsdwAqfgwAU04QZa0AYCD/AEL/BqPBrPxpvxPm+tGOXMIfyR8fENW+SXZQ==</latexit><latexit sha1_base64="x65kFaBF/HZpH/Ng0a+ynMI2q6M=">AAACAnicbZDLSsNAFIZP6q3GW9SVuBksBVclEUGXRTcuK9gLNKFMppN26OTCzEQoIbjxVdy4UMStT+HOt3HSRtDWHwY+/nMOc87vJ5xJZdtfRmVldW19o7ppbm3v7O5Z+wcdGaeC0DaJeSx6PpaUs4i2FVOc9hJBcehz2vUn10W9e0+FZHF0p6YJ9UI8iljACFbaGlhHbojV2A8ynpv1H3Yp5/nAqtkNeya0DE4JNSjVGlif7jAmaUgjRTiWsu/YifIyLBQjnOamm0qaYDLBI9rXGOGQSi+bnZCjunaGKIiFfpFCM/f3RIZDKaehrzuLJeVirTD/q/VTFVx6GYuSVNGIzD8KUo5UjIo80JAJShSfasBEML0rImMsMFE6NVOH4CyevAyds4ZjN5zb81rzqoyjCsdwAqfgwAU04QZa0AYCD/AEL/BqPBrPxpvxPm+tGOXMIfyR8fENW+SXZQ==</latexit><latexit sha1_base64="x65kFaBF/HZpH/Ng0a+ynMI2q6M=">AAACAnicbZDLSsNAFIZP6q3GW9SVuBksBVclEUGXRTcuK9gLNKFMppN26OTCzEQoIbjxVdy4UMStT+HOt3HSRtDWHwY+/nMOc87vJ5xJZdtfRmVldW19o7ppbm3v7O5Z+wcdGaeC0DaJeSx6PpaUs4i2FVOc9hJBcehz2vUn10W9e0+FZHF0p6YJ9UI8iljACFbaGlhHbojV2A8ynpv1H3Yp5/nAqtkNeya0DE4JNSjVGlif7jAmaUgjRTiWsu/YifIyLBQjnOamm0qaYDLBI9rXGOGQSi+bnZCjunaGKIiFfpFCM/f3RIZDKaehrzuLJeVirTD/q/VTFVx6GYuSVNGIzD8KUo5UjIo80JAJShSfasBEML0rImMsMFE6NVOH4CyevAyds4ZjN5zb81rzqoyjCsdwAqfgwAU04QZa0AYCD/AEL/BqPBrPxpvxPm+tGOXMIfyR8fENW+SXZQ==</latexit>

r
<latexit sha1_base64="juw6eQUUFM3i37Bw8Ysq5tbV1Ao=">AAAB8XicbVDLSsNAFL2pr1pfVZduBovgqiQi6LLoxmUF+8A2lMl00g6dTMLMjVBC/8KNC0Xc+jfu/BsnbRbaemDgcM69zLknSKQw6LrfTmltfWNzq7xd2dnd2z+oHh61TZxqxlsslrHuBtRwKRRvoUDJu4nmNAok7wST29zvPHFtRKwecJpwP6IjJULBKFrpsR9RHAdhpmeDas2tu3OQVeIVpAYFmoPqV38YszTiCpmkxvQ8N0E/oxoFk3xW6aeGJ5RN6Ij3LFU04sbP5oln5MwqQxLG2j6FZK7+3shoZMw0CuxkntAse7n4n9dLMbz2M6GSFLlii4/CVBKMSX4+GQrNGcqpJZRpYbMSNqaaMrQlVWwJ3vLJq6R9Uffcund/WWvcFHWU4QRO4Rw8uIIG3EETWsBAwTO8wptjnBfn3flYjJacYucY/sD5/AH0G5EW</latexit><latexit sha1_base64="juw6eQUUFM3i37Bw8Ysq5tbV1Ao=">AAAB8XicbVDLSsNAFL2pr1pfVZduBovgqiQi6LLoxmUF+8A2lMl00g6dTMLMjVBC/8KNC0Xc+jfu/BsnbRbaemDgcM69zLknSKQw6LrfTmltfWNzq7xd2dnd2z+oHh61TZxqxlsslrHuBtRwKRRvoUDJu4nmNAok7wST29zvPHFtRKwecJpwP6IjJULBKFrpsR9RHAdhpmeDas2tu3OQVeIVpAYFmoPqV38YszTiCpmkxvQ8N0E/oxoFk3xW6aeGJ5RN6Ij3LFU04sbP5oln5MwqQxLG2j6FZK7+3shoZMw0CuxkntAse7n4n9dLMbz2M6GSFLlii4/CVBKMSX4+GQrNGcqpJZRpYbMSNqaaMrQlVWwJ3vLJq6R9Uffcund/WWvcFHWU4QRO4Rw8uIIG3EETWsBAwTO8wptjnBfn3flYjJacYucY/sD5/AH0G5EW</latexit><latexit sha1_base64="juw6eQUUFM3i37Bw8Ysq5tbV1Ao=">AAAB8XicbVDLSsNAFL2pr1pfVZduBovgqiQi6LLoxmUF+8A2lMl00g6dTMLMjVBC/8KNC0Xc+jfu/BsnbRbaemDgcM69zLknSKQw6LrfTmltfWNzq7xd2dnd2z+oHh61TZxqxlsslrHuBtRwKRRvoUDJu4nmNAok7wST29zvPHFtRKwecJpwP6IjJULBKFrpsR9RHAdhpmeDas2tu3OQVeIVpAYFmoPqV38YszTiCpmkxvQ8N0E/oxoFk3xW6aeGJ5RN6Ij3LFU04sbP5oln5MwqQxLG2j6FZK7+3shoZMw0CuxkntAse7n4n9dLMbz2M6GSFLlii4/CVBKMSX4+GQrNGcqpJZRpYbMSNqaaMrQlVWwJ3vLJq6R9Uffcund/WWvcFHWU4QRO4Rw8uIIG3EETWsBAwTO8wptjnBfn3flYjJacYucY/sD5/AH0G5EW</latexit><latexit sha1_base64="juw6eQUUFM3i37Bw8Ysq5tbV1Ao=">AAAB8XicbVDLSsNAFL2pr1pfVZduBovgqiQi6LLoxmUF+8A2lMl00g6dTMLMjVBC/8KNC0Xc+jfu/BsnbRbaemDgcM69zLknSKQw6LrfTmltfWNzq7xd2dnd2z+oHh61TZxqxlsslrHuBtRwKRRvoUDJu4nmNAok7wST29zvPHFtRKwecJpwP6IjJULBKFrpsR9RHAdhpmeDas2tu3OQVeIVpAYFmoPqV38YszTiCpmkxvQ8N0E/oxoFk3xW6aeGJ5RN6Ij3LFU04sbP5oln5MwqQxLG2j6FZK7+3shoZMw0CuxkntAse7n4n9dLMbz2M6GSFLlii4/CVBKMSX4+GQrNGcqpJZRpYbMSNqaaMrQlVWwJ3vLJq6R9Uffcund/WWvcFHWU4QRO4Rw8uIIG3EETWsBAwTO8wptjnBfn3flYjJacYucY/sD5/AH0G5EW</latexit>

d
<latexit sha1_base64="GGtk1qN94p3lS2+MxRDs9uIEc4U=">AAAB8XicbVDLSsNAFL2pr1pfVZduBovgqiQi6LLoxmUF+8A2lMn0ph06mYSZiVBC/8KNC0Xc+jfu/BsnbRbaemDgcM69zLknSATXxnW/ndLa+sbmVnm7srO7t39QPTxq6zhVDFssFrHqBlSj4BJbhhuB3UQhjQKBnWBym/udJ1Sax/LBTBP0IzqSPOSMGis99iNqxkGYDWeDas2tu3OQVeIVpAYFmoPqV38YszRCaZigWvc8NzF+RpXhTOCs0k81JpRN6Ah7lkoaofazeeIZObPKkISxsk8aMld/b2Q00noaBXYyT6iXvVz8z+ulJrz2My6T1KBki4/CVBATk/x8MuQKmRFTSyhT3GYlbEwVZcaWVLEleMsnr5L2Rd1z6979Za1xU9RRhhM4hXPw4AoacAdNaAEDCc/wCm+Odl6cd+djMVpyip1j+APn8wfe1ZEI</latexit><latexit sha1_base64="GGtk1qN94p3lS2+MxRDs9uIEc4U=">AAAB8XicbVDLSsNAFL2pr1pfVZduBovgqiQi6LLoxmUF+8A2lMn0ph06mYSZiVBC/8KNC0Xc+jfu/BsnbRbaemDgcM69zLknSATXxnW/ndLa+sbmVnm7srO7t39QPTxq6zhVDFssFrHqBlSj4BJbhhuB3UQhjQKBnWBym/udJ1Sax/LBTBP0IzqSPOSMGis99iNqxkGYDWeDas2tu3OQVeIVpAYFmoPqV38YszRCaZigWvc8NzF+RpXhTOCs0k81JpRN6Ah7lkoaofazeeIZObPKkISxsk8aMld/b2Q00noaBXYyT6iXvVz8z+ulJrz2My6T1KBki4/CVBATk/x8MuQKmRFTSyhT3GYlbEwVZcaWVLEleMsnr5L2Rd1z6979Za1xU9RRhhM4hXPw4AoacAdNaAEDCc/wCm+Odl6cd+djMVpyip1j+APn8wfe1ZEI</latexit><latexit sha1_base64="GGtk1qN94p3lS2+MxRDs9uIEc4U=">AAAB8XicbVDLSsNAFL2pr1pfVZduBovgqiQi6LLoxmUF+8A2lMn0ph06mYSZiVBC/8KNC0Xc+jfu/BsnbRbaemDgcM69zLknSATXxnW/ndLa+sbmVnm7srO7t39QPTxq6zhVDFssFrHqBlSj4BJbhhuB3UQhjQKBnWBym/udJ1Sax/LBTBP0IzqSPOSMGis99iNqxkGYDWeDas2tu3OQVeIVpAYFmoPqV38YszRCaZigWvc8NzF+RpXhTOCs0k81JpRN6Ah7lkoaofazeeIZObPKkISxsk8aMld/b2Q00noaBXYyT6iXvVz8z+ulJrz2My6T1KBki4/CVBATk/x8MuQKmRFTSyhT3GYlbEwVZcaWVLEleMsnr5L2Rd1z6979Za1xU9RRhhM4hXPw4AoacAdNaAEDCc/wCm+Odl6cd+djMVpyip1j+APn8wfe1ZEI</latexit><latexit sha1_base64="GGtk1qN94p3lS2+MxRDs9uIEc4U=">AAAB8XicbVDLSsNAFL2pr1pfVZduBovgqiQi6LLoxmUF+8A2lMn0ph06mYSZiVBC/8KNC0Xc+jfu/BsnbRbaemDgcM69zLknSATXxnW/ndLa+sbmVnm7srO7t39QPTxq6zhVDFssFrHqBlSj4BJbhhuB3UQhjQKBnWBym/udJ1Sax/LBTBP0IzqSPOSMGis99iNqxkGYDWeDas2tu3OQVeIVpAYFmoPqV38YszRCaZigWvc8NzF+RpXhTOCs0k81JpRN6Ah7lkoaofazeeIZObPKkISxsk8aMld/b2Q00noaBXYyT6iXvVz8z+ulJrz2My6T1KBki4/CVBATk/x8MuQKmRFTSyhT3GYlbEwVZcaWVLEleMsnr5L2Rd1z6979Za1xU9RRhhM4hXPw4AoacAdNaAEDCc/wCm+Odl6cd+djMVpyip1j+APn8wfe1ZEI</latexit>

wi
<latexit sha1_base64="UBwr2R+8MpY5RFOsjT7bXwdRlsw=">AAAB83icbVDLSsNAFL2pr1pfVZduBovgqiQi6LLoxmUF+4CmlMn0ph06mYSZiVJCf8ONC0Xc+jPu/BsnbRbaemDgcM693DMnSATXxnW/ndLa+sbmVnm7srO7t39QPTxq6zhVDFssFrHqBlSj4BJbhhuB3UQhjQKBnWBym/udR1Sax/LBTBPsR3QkecgZNVby/YiacRBmT7MBH1Rrbt2dg6wSryA1KNAcVL/8YczSCKVhgmrd89zE9DOqDGcCZxU/1ZhQNqEj7FkqaYS6n80zz8iZVYYkjJV90pC5+nsjo5HW0yiwk3lGvezl4n9eLzXhdT/jMkkNSrY4FKaCmJjkBZAhV8iMmFpCmeI2K2FjqigztqaKLcFb/vIqaV/UPbfu3V/WGjdFHWU4gVM4Bw+uoAF30IQWMEjgGV7hzUmdF+fd+ViMlpxi5xj+wPn8AX25kfc=</latexit><latexit sha1_base64="UBwr2R+8MpY5RFOsjT7bXwdRlsw=">AAAB83icbVDLSsNAFL2pr1pfVZduBovgqiQi6LLoxmUF+4CmlMn0ph06mYSZiVJCf8ONC0Xc+jPu/BsnbRbaemDgcM693DMnSATXxnW/ndLa+sbmVnm7srO7t39QPTxq6zhVDFssFrHqBlSj4BJbhhuB3UQhjQKBnWBym/udR1Sax/LBTBPsR3QkecgZNVby/YiacRBmT7MBH1Rrbt2dg6wSryA1KNAcVL/8YczSCKVhgmrd89zE9DOqDGcCZxU/1ZhQNqEj7FkqaYS6n80zz8iZVYYkjJV90pC5+nsjo5HW0yiwk3lGvezl4n9eLzXhdT/jMkkNSrY4FKaCmJjkBZAhV8iMmFpCmeI2K2FjqigztqaKLcFb/vIqaV/UPbfu3V/WGjdFHWU4gVM4Bw+uoAF30IQWMEjgGV7hzUmdF+fd+ViMlpxi5xj+wPn8AX25kfc=</latexit><latexit sha1_base64="UBwr2R+8MpY5RFOsjT7bXwdRlsw=">AAAB83icbVDLSsNAFL2pr1pfVZduBovgqiQi6LLoxmUF+4CmlMn0ph06mYSZiVJCf8ONC0Xc+jPu/BsnbRbaemDgcM693DMnSATXxnW/ndLa+sbmVnm7srO7t39QPTxq6zhVDFssFrHqBlSj4BJbhhuB3UQhjQKBnWBym/udR1Sax/LBTBPsR3QkecgZNVby/YiacRBmT7MBH1Rrbt2dg6wSryA1KNAcVL/8YczSCKVhgmrd89zE9DOqDGcCZxU/1ZhQNqEj7FkqaYS6n80zz8iZVYYkjJV90pC5+nsjo5HW0yiwk3lGvezl4n9eLzXhdT/jMkkNSrY4FKaCmJjkBZAhV8iMmFpCmeI2K2FjqigztqaKLcFb/vIqaV/UPbfu3V/WGjdFHWU4gVM4Bw+uoAF30IQWMEjgGV7hzUmdF+fd+ViMlpxi5xj+wPn8AX25kfc=</latexit><latexit sha1_base64="UBwr2R+8MpY5RFOsjT7bXwdRlsw=">AAAB83icbVDLSsNAFL2pr1pfVZduBovgqiQi6LLoxmUF+4CmlMn0ph06mYSZiVJCf8ONC0Xc+jPu/BsnbRbaemDgcM693DMnSATXxnW/ndLa+sbmVnm7srO7t39QPTxq6zhVDFssFrHqBlSj4BJbhhuB3UQhjQKBnWBym/udR1Sax/LBTBPsR3QkecgZNVby/YiacRBmT7MBH1Rrbt2dg6wSryA1KNAcVL/8YczSCKVhgmrd89zE9DOqDGcCZxU/1ZhQNqEj7FkqaYS6n80zz8iZVYYkjJV90pC5+nsjo5HW0yiwk3lGvezl4n9eLzXhdT/jMkkNSrY4FKaCmJjkBZAhV8iMmFpCmeI2K2FjqigztqaKLcFb/vIqaV/UPbfu3V/WGjdFHWU4gVM4Bw+uoAF30IQWMEjgGV7hzUmdF+fd+ViMlpxi5xj+wPn8AX25kfc=</latexit>

c
<latexit sha1_base64="+A6VEtos3HL/1r2wKYFonhC20Mo=">AAAB8XicbVDLSsNAFL2pr1pfVZduBovgqiQi6LLoxmUF+8A2lMl00g6dTMLMjVBC/8KNC0Xc+jfu/BsnbRbaemDgcM69zLknSKQw6LrfTmltfWNzq7xd2dnd2z+oHh61TZxqxlsslrHuBtRwKRRvoUDJu4nmNAok7wST29zvPHFtRKwecJpwP6IjJULBKFrpsR9RHAdhxmaDas2tu3OQVeIVpAYFmoPqV38YszTiCpmkxvQ8N0E/oxoFk3xW6aeGJ5RN6Ij3LFU04sbP5oln5MwqQxLG2j6FZK7+3shoZMw0CuxkntAse7n4n9dLMbz2M6GSFLlii4/CVBKMSX4+GQrNGcqpJZRpYbMSNqaaMrQlVWwJ3vLJq6R9Uffcund/WWvcFHWU4QRO4Rw8uIIG3EETWsBAwTO8wptjnBfn3flYjJacYucY/sD5/AHdUJEH</latexit><latexit sha1_base64="+A6VEtos3HL/1r2wKYFonhC20Mo=">AAAB8XicbVDLSsNAFL2pr1pfVZduBovgqiQi6LLoxmUF+8A2lMl00g6dTMLMjVBC/8KNC0Xc+jfu/BsnbRbaemDgcM69zLknSKQw6LrfTmltfWNzq7xd2dnd2z+oHh61TZxqxlsslrHuBtRwKRRvoUDJu4nmNAok7wST29zvPHFtRKwecJpwP6IjJULBKFrpsR9RHAdhxmaDas2tu3OQVeIVpAYFmoPqV38YszTiCpmkxvQ8N0E/oxoFk3xW6aeGJ5RN6Ij3LFU04sbP5oln5MwqQxLG2j6FZK7+3shoZMw0CuxkntAse7n4n9dLMbz2M6GSFLlii4/CVBKMSX4+GQrNGcqpJZRpYbMSNqaaMrQlVWwJ3vLJq6R9Uffcund/WWvcFHWU4QRO4Rw8uIIG3EETWsBAwTO8wptjnBfn3flYjJacYucY/sD5/AHdUJEH</latexit><latexit sha1_base64="+A6VEtos3HL/1r2wKYFonhC20Mo=">AAAB8XicbVDLSsNAFL2pr1pfVZduBovgqiQi6LLoxmUF+8A2lMl00g6dTMLMjVBC/8KNC0Xc+jfu/BsnbRbaemDgcM69zLknSKQw6LrfTmltfWNzq7xd2dnd2z+oHh61TZxqxlsslrHuBtRwKRRvoUDJu4nmNAok7wST29zvPHFtRKwecJpwP6IjJULBKFrpsR9RHAdhxmaDas2tu3OQVeIVpAYFmoPqV38YszTiCpmkxvQ8N0E/oxoFk3xW6aeGJ5RN6Ij3LFU04sbP5oln5MwqQxLG2j6FZK7+3shoZMw0CuxkntAse7n4n9dLMbz2M6GSFLlii4/CVBKMSX4+GQrNGcqpJZRpYbMSNqaaMrQlVWwJ3vLJq6R9Uffcund/WWvcFHWU4QRO4Rw8uIIG3EETWsBAwTO8wptjnBfn3flYjJacYucY/sD5/AHdUJEH</latexit><latexit sha1_base64="+A6VEtos3HL/1r2wKYFonhC20Mo=">AAAB8XicbVDLSsNAFL2pr1pfVZduBovgqiQi6LLoxmUF+8A2lMl00g6dTMLMjVBC/8KNC0Xc+jfu/BsnbRbaemDgcM69zLknSKQw6LrfTmltfWNzq7xd2dnd2z+oHh61TZxqxlsslrHuBtRwKRRvoUDJu4nmNAok7wST29zvPHFtRKwecJpwP6IjJULBKFrpsR9RHAdhxmaDas2tu3OQVeIVpAYFmoPqV38YszTiCpmkxvQ8N0E/oxoFk3xW6aeGJ5RN6Ij3LFU04sbP5oln5MwqQxLG2j6FZK7+3shoZMw0CuxkntAse7n4n9dLMbz2M6GSFLlii4/CVBKMSX4+GQrNGcqpJZRpYbMSNqaaMrQlVWwJ3vLJq6R9Uffcund/WWvcFHWU4QRO4Rw8uIIG3EETWsBAwTO8wptjnBfn3flYjJacYucY/sD5/AHdUJEH</latexit>local 

context vector document 
context vector

g
<latexit sha1_base64="vmPs/L4DTQ+031t4ylAOlX3U35Q=">AAAB8XicbVDLSsNAFL2pr1pfVZduBovgqiQi6LLoxmUF+8A2lMl00g6dTMLMjVBC/8KNC0Xc+jfu/BsnbRbaemDgcM69zLknSKQw6LrfTmltfWNzq7xd2dnd2z+oHh61TZxqxlsslrHuBtRwKRRvoUDJu4nmNAok7wST29zvPHFtRKwecJpwP6IjJULBKFrpsR9RHAdhNpoNqjW37s5BVolXkBoUaA6qX/1hzNKIK2SSGtPz3AT9jGoUTPJZpZ8anlA2oSPes1TRiBs/myeekTOrDEkYa/sUkrn6eyOjkTHTKLCTeUKz7OXif14vxfDaz4RKUuSKLT4KU0kwJvn5ZCg0ZyinllCmhc1K2JhqytCWVLEleMsnr5L2Rd1z6979Za1xU9RRhhM4hXPw4AoacAdNaAEDBc/wCm+OcV6cd+djMVpyip1j+APn8wfjZJEL</latexit><latexit sha1_base64="vmPs/L4DTQ+031t4ylAOlX3U35Q=">AAAB8XicbVDLSsNAFL2pr1pfVZduBovgqiQi6LLoxmUF+8A2lMl00g6dTMLMjVBC/8KNC0Xc+jfu/BsnbRbaemDgcM69zLknSKQw6LrfTmltfWNzq7xd2dnd2z+oHh61TZxqxlsslrHuBtRwKRRvoUDJu4nmNAok7wST29zvPHFtRKwecJpwP6IjJULBKFrpsR9RHAdhNpoNqjW37s5BVolXkBoUaA6qX/1hzNKIK2SSGtPz3AT9jGoUTPJZpZ8anlA2oSPes1TRiBs/myeekTOrDEkYa/sUkrn6eyOjkTHTKLCTeUKz7OXif14vxfDaz4RKUuSKLT4KU0kwJvn5ZCg0ZyinllCmhc1K2JhqytCWVLEleMsnr5L2Rd1z6979Za1xU9RRhhM4hXPw4AoacAdNaAEDBc/wCm+OcV6cd+djMVpyip1j+APn8wfjZJEL</latexit><latexit sha1_base64="vmPs/L4DTQ+031t4ylAOlX3U35Q=">AAAB8XicbVDLSsNAFL2pr1pfVZduBovgqiQi6LLoxmUF+8A2lMl00g6dTMLMjVBC/8KNC0Xc+jfu/BsnbRbaemDgcM69zLknSKQw6LrfTmltfWNzq7xd2dnd2z+oHh61TZxqxlsslrHuBtRwKRRvoUDJu4nmNAok7wST29zvPHFtRKwecJpwP6IjJULBKFrpsR9RHAdhNpoNqjW37s5BVolXkBoUaA6qX/1hzNKIK2SSGtPz3AT9jGoUTPJZpZ8anlA2oSPes1TRiBs/myeekTOrDEkYa/sUkrn6eyOjkTHTKLCTeUKz7OXif14vxfDaz4RKUuSKLT4KU0kwJvn5ZCg0ZyinllCmhc1K2JhqytCWVLEleMsnr5L2Rd1z6979Za1xU9RRhhM4hXPw4AoacAdNaAEDBc/wCm+OcV6cd+djMVpyip1j+APn8wfjZJEL</latexit><latexit sha1_base64="vmPs/L4DTQ+031t4ylAOlX3U35Q=">AAAB8XicbVDLSsNAFL2pr1pfVZduBovgqiQi6LLoxmUF+8A2lMl00g6dTMLMjVBC/8KNC0Xc+jfu/BsnbRbaemDgcM69zLknSKQw6LrfTmltfWNzq7xd2dnd2z+oHh61TZxqxlsslrHuBtRwKRRvoUDJu4nmNAok7wST29zvPHFtRKwecJpwP6IjJULBKFrpsR9RHAdhNpoNqjW37s5BVolXkBoUaA6qX/1hzNKIK2SSGtPz3AT9jGoUTPJZpZ8anlA2oSPes1TRiBs/myeekTOrDEkYa/sUkrn6eyOjkTHTKLCTeUKz7OXif14vxfDaz4RKUuSKLT4KU0kwJvn5ZCg0ZyinllCmhc1K2JhqytCWVLEleMsnr5L2Rd1z6979Za1xU9RRhhM4hXPw4AoacAdNaAEDBc/wCm+OcV6cd+djMVpyip1j+APn8wfjZJEL</latexit>

mention 
context vector

Document 
Context 
Encoder

Multilingual Token Embedding Lookup

local context document context 

(b) Mention Context Encoder.

Figure 2: (a) Grounded mentions from two or more languages (English and Tamil shown) can be used to supervise XELMS.
The context g, entity e and type t vectors interact through Entity-Context loss (EC-LOSS), Type-Context loss (TC-LOSS) and
Type-Entity loss (TE-LOSS). The Tamil sentence is the same as in Figure 1, and other mentions in it translate to [Suarez] and
[Uruguay]. (b) The Mention Context Encoder (§2.1) encodes the local context (neighboring words) and the document context
(surfaces of other mentions in the document) of the mention into g. Internal view of local context encoder is in Figure 3.

Additionally, by encoding freely available struc-
tured knowledge, like fine-grained entity types, the
entity and context representations can be further
improved (§2.2).

The ability to use multilingual supervision en-
ables XELMS to learn XEL models for target lan-
guages with limited resources by exploiting freely
available supervision from high resource languages
(like English). We show that XELMS outperforms
existing state-of-the-art approaches that only use
target language supervision, across 3 benchmark
datasets in 8 languages (§5.1). Moreover, while
previous XEL models (McNamee et al., 2011; Tsai
and Roth, 2016) train separate models for differ-
ent languages, XELMS can train a single model for
performing XEL in multiple languages (§5.2).

One of the goals of XEL is to enable understand-
ing of languages with limited resources. We pro-
vide experimental analyses in two such settings.
In the zero-shot setting (§6.1), where no supervi-
sion is available in the target language, we show
that the good performance of zero-shot XEL ap-
proaches (Sil et al., 2018) can be attributed to the
use of prior probabilities. These probabilities are
computed from large amount of grounded men-
tions, which are not available in realistic zero-shot
settings. In the low-resource setting (§6.2), where
some supervision is available in the target language,

we show that even when only a fraction of the avail-
able supervision in the target language is provided,
XELMS can achieve competitive performance by
exploiting supervision from English.

The contributions of our work are,
• A new XEL approach, XELMS, that learns a

XEL model for a language with limited re-
sources by exploiting additional supervision
from a high-resource language like English.
• XELMS can also train a single XEL model

for multiple languages jointly, which we show
improves on separately trained models.
• Analysis of XEL approaches in the zero-shot

and low-resource settings. Our analysis re-
veals that in realistic scenarios, zero-shot XEL
is not as effective as previously shown. We
also show that in low-resource settings jointly
training with English leads to better utilization
of target language supervision.

2 Cross-lingual EL with XELMS

Given a mention m in a document D written in any
language, XEL involves linkingm to its gold entity
e∗ in a KB, K = {e1, · · · , en}.

An overview of XELMS is shown in Figure 2a.
XELMS computes the probability, Pcontext(e | m),
of a mention m referring to entity e ∈ K using
a mention context vector g ∈ Rh representing



2488

m’s context, and an entity vector e ∈ Rh, rep-
resenting the entity e ∈ K (one vector per entity).
XELMS can also incorporate structured knowledge
like fine-grained entity types (§2.2) using a multi-
task learning approach (Caruana, 1998), by learn-
ing a type vector t ∈ Rh for each possible type
t (e.g., sports_team) associated with the entity e.
The entity vector e, context vector g and the type
vector t are jointly trained, and interact through ap-
propriately defined pairwise loss terms – an Entity-
Context loss (EC-LOSS), Type-Entity loss (TE-
LOSS) and a Type-Context loss (TC-LOSS).

The mention context vector g is generated by
a mention context encoder (§2.1), shown in Fig-
ure 2b. The mention context of m in a document D
consists of: (a) neighboring words around the men-
tion, which we refer to as its local context and, (b)
surfaces of other mentions appearing in D, which
we refer as its document context.
XELMS is trained using grounded mentions in mul-
tiple languages (English and Tamil in Figure 2a),
which can be derived from Wikipedia (§4.1).

2.1 Mention Context Representation

To learn from mention contexts in multiple lan-
guages, we generate mention context representa-
tions using a language-agnostic mention context
encoder. An overview of the mention context en-
coder is shown in Figure 2b. Below we describe
the components of the mention context encoder,
namely multilingual word embeddings and local
and document context encoders.

Multilingual Word Embeddings (Ammar et al.,
2016b; Smith et al., 2017; Duong et al., 2017)
jointly encode words in multiple (≥2) languages
in the same vector space such that semantically
similar words in the same language, and transla-
tionally equivalent words in different languages are
close (per cosine similarity). Multilingual embed-
dings generalize bilingual embeddings, which do
the same for two languages only.

We use FASTTEXT (Bojanowski et al., 2017;
Smith et al., 2017), which aligns monolingual em-
beddings of multiple languages in the same space
using a small dictionary (∼2500 pairs) from each
language to English. Both monolingual embed-
dings and the dictionary can be easily obtained for
languages with limited resources. We denote the
multilingual word embeddings for a set of tokens
{w1, w2, · · · , wn} by w1:n = {w1,w2, · · · ,wn},
where each wi ∈ Rd.

ReLU

Oi
<latexit sha1_base64="8XCJfZLXz6f3NZoZnnjJ2Skt/Dw=">AAAB83icbVDLSsNAFL2pr1pfVZduBovgqiQi6LLoxp0V7AOaUCbTSTt0MgkzN0IJ/Q03LhRx68+482+ctllo64GBwzn3cs+cMJXCoOt+O6W19Y3NrfJ2ZWd3b/+genjUNkmmGW+xRCa6G1LDpVC8hQIl76aa0ziUvBOOb2d+54lrIxL1iJOUBzEdKhEJRtFKvh9THIVRfj/ti3615tbdOcgq8QpSgwLNfvXLHyQsi7lCJqkxPc9NMcipRsEkn1b8zPCUsjEd8p6lisbcBPk885ScWWVAokTbp5DM1d8bOY2NmcShnZxlNMveTPzP62UYXQe5UGmGXLHFoSiTBBMyK4AMhOYM5cQSyrSwWQkbUU0Z2poqtgRv+curpH1R99y693BZa9wUdZThBE7hHDy4ggbcQRNawCCFZ3iFNydzXpx352MxWnKKnWP4A+fzB0Chkc8=</latexit><latexit sha1_base64="8XCJfZLXz6f3NZoZnnjJ2Skt/Dw=">AAAB83icbVDLSsNAFL2pr1pfVZduBovgqiQi6LLoxp0V7AOaUCbTSTt0MgkzN0IJ/Q03LhRx68+482+ctllo64GBwzn3cs+cMJXCoOt+O6W19Y3NrfJ2ZWd3b/+genjUNkmmGW+xRCa6G1LDpVC8hQIl76aa0ziUvBOOb2d+54lrIxL1iJOUBzEdKhEJRtFKvh9THIVRfj/ti3615tbdOcgq8QpSgwLNfvXLHyQsi7lCJqkxPc9NMcipRsEkn1b8zPCUsjEd8p6lisbcBPk885ScWWVAokTbp5DM1d8bOY2NmcShnZxlNMveTPzP62UYXQe5UGmGXLHFoSiTBBMyK4AMhOYM5cQSyrSwWQkbUU0Z2poqtgRv+curpH1R99y693BZa9wUdZThBE7hHDy4ggbcQRNawCCFZ3iFNydzXpx352MxWnKKnWP4A+fzB0Chkc8=</latexit><latexit sha1_base64="8XCJfZLXz6f3NZoZnnjJ2Skt/Dw=">AAAB83icbVDLSsNAFL2pr1pfVZduBovgqiQi6LLoxp0V7AOaUCbTSTt0MgkzN0IJ/Q03LhRx68+482+ctllo64GBwzn3cs+cMJXCoOt+O6W19Y3NrfJ2ZWd3b/+genjUNkmmGW+xRCa6G1LDpVC8hQIl76aa0ziUvBOOb2d+54lrIxL1iJOUBzEdKhEJRtFKvh9THIVRfj/ti3615tbdOcgq8QpSgwLNfvXLHyQsi7lCJqkxPc9NMcipRsEkn1b8zPCUsjEd8p6lisbcBPk885ScWWVAokTbp5DM1d8bOY2NmcShnZxlNMveTPzP62UYXQe5UGmGXLHFoSiTBBMyK4AMhOYM5cQSyrSwWQkbUU0Z2poqtgRv+curpH1R99y693BZa9wUdZThBE7hHDy4ggbcQRNawCCFZ3iFNydzXpx352MxWnKKnWP4A+fzB0Chkc8=</latexit><latexit sha1_base64="8XCJfZLXz6f3NZoZnnjJ2Skt/Dw=">AAAB83icbVDLSsNAFL2pr1pfVZduBovgqiQi6LLoxp0V7AOaUCbTSTt0MgkzN0IJ/Q03LhRx68+482+ctllo64GBwzn3cs+cMJXCoOt+O6W19Y3NrfJ2ZWd3b/+genjUNkmmGW+xRCa6G1LDpVC8hQIl76aa0ziUvBOOb2d+54lrIxL1iJOUBzEdKhEJRtFKvh9THIVRfj/ti3615tbdOcgq8QpSgwLNfvXLHyQsi7lCJqkxPc9NMcipRsEkn1b8zPCUsjEd8p6lisbcBPk885ScWWVAokTbp5DM1d8bOY2NmcShnZxlNMveTPzP62UYXQe5UGmGXLHFoSiTBBMyK4AMhOYM5cQSyrSwWQkbUU0Z2poqtgRv+curpH1R99y693BZa9wUdZThBE7hHDy4ggbcQRNawCCFZ3iFNydzXpx352MxWnKKnWP4A+fzB0Chkc8=</latexit>

r
<latexit sha1_base64="juw6eQUUFM3i37Bw8Ysq5tbV1Ao=">AAAB8XicbVDLSsNAFL2pr1pfVZduBovgqiQi6LLoxmUF+8A2lMl00g6dTMLMjVBC/8KNC0Xc+jfu/BsnbRbaemDgcM69zLknSKQw6LrfTmltfWNzq7xd2dnd2z+oHh61TZxqxlsslrHuBtRwKRRvoUDJu4nmNAok7wST29zvPHFtRKwecJpwP6IjJULBKFrpsR9RHAdhpmeDas2tu3OQVeIVpAYFmoPqV38YszTiCpmkxvQ8N0E/oxoFk3xW6aeGJ5RN6Ij3LFU04sbP5oln5MwqQxLG2j6FZK7+3shoZMw0CuxkntAse7n4n9dLMbz2M6GSFLlii4/CVBKMSX4+GQrNGcqpJZRpYbMSNqaaMrQlVWwJ3vLJq6R9Uffcund/WWvcFHWU4QRO4Rw8uIIG3EETWsBAwTO8wptjnBfn3flYjJacYucY/sD5/AH0G5EW</latexit><latexit sha1_base64="juw6eQUUFM3i37Bw8Ysq5tbV1Ao=">AAAB8XicbVDLSsNAFL2pr1pfVZduBovgqiQi6LLoxmUF+8A2lMl00g6dTMLMjVBC/8KNC0Xc+jfu/BsnbRbaemDgcM69zLknSKQw6LrfTmltfWNzq7xd2dnd2z+oHh61TZxqxlsslrHuBtRwKRRvoUDJu4nmNAok7wST29zvPHFtRKwecJpwP6IjJULBKFrpsR9RHAdhpmeDas2tu3OQVeIVpAYFmoPqV38YszTiCpmkxvQ8N0E/oxoFk3xW6aeGJ5RN6Ij3LFU04sbP5oln5MwqQxLG2j6FZK7+3shoZMw0CuxkntAse7n4n9dLMbz2M6GSFLlii4/CVBKMSX4+GQrNGcqpJZRpYbMSNqaaMrQlVWwJ3vLJq6R9Uffcund/WWvcFHWU4QRO4Rw8uIIG3EETWsBAwTO8wptjnBfn3flYjJacYucY/sD5/AH0G5EW</latexit><latexit sha1_base64="juw6eQUUFM3i37Bw8Ysq5tbV1Ao=">AAAB8XicbVDLSsNAFL2pr1pfVZduBovgqiQi6LLoxmUF+8A2lMl00g6dTMLMjVBC/8KNC0Xc+jfu/BsnbRbaemDgcM69zLknSKQw6LrfTmltfWNzq7xd2dnd2z+oHh61TZxqxlsslrHuBtRwKRRvoUDJu4nmNAok7wST29zvPHFtRKwecJpwP6IjJULBKFrpsR9RHAdhpmeDas2tu3OQVeIVpAYFmoPqV38YszTiCpmkxvQ8N0E/oxoFk3xW6aeGJ5RN6Ij3LFU04sbP5oln5MwqQxLG2j6FZK7+3shoZMw0CuxkntAse7n4n9dLMbz2M6GSFLlii4/CVBKMSX4+GQrNGcqpJZRpYbMSNqaaMrQlVWwJ3vLJq6R9Uffcund/WWvcFHWU4QRO4Rw8uIIG3EETWsBAwTO8wptjnBfn3flYjJacYucY/sD5/AH0G5EW</latexit><latexit sha1_base64="juw6eQUUFM3i37Bw8Ysq5tbV1Ao=">AAAB8XicbVDLSsNAFL2pr1pfVZduBovgqiQi6LLoxmUF+8A2lMl00g6dTMLMjVBC/8KNC0Xc+jfu/BsnbRbaemDgcM69zLknSKQw6LrfTmltfWNzq7xd2dnd2z+oHh61TZxqxlsslrHuBtRwKRRvoUDJu4nmNAok7wST29zvPHFtRKwecJpwP6IjJULBKFrpsR9RHAdhpmeDas2tu3OQVeIVpAYFmoPqV38YszTiCpmkxvQ8N0E/oxoFk3xW6aeGJ5RN6Ij3LFU04sbP5oln5MwqQxLG2j6FZK7+3shoZMw0CuxkntAse7n4n9dLMbz2M6GSFLlii4/CVBKMSX4+GQrNGcqpJZRpYbMSNqaaMrQlVWwJ3vLJq6R9Uffcund/WWvcFHWU4QRO4Rw8uIIG3EETWsBAwTO8wptjnBfn3flYjJacYucY/sD5/AH0G5EW</latexit>

Average Pooling

… …wi
<latexit sha1_base64="UBwr2R+8MpY5RFOsjT7bXwdRlsw=">AAAB83icbVDLSsNAFL2pr1pfVZduBovgqiQi6LLoxmUF+4CmlMn0ph06mYSZiVJCf8ONC0Xc+jPu/BsnbRbaemDgcM693DMnSATXxnW/ndLa+sbmVnm7srO7t39QPTxq6zhVDFssFrHqBlSj4BJbhhuB3UQhjQKBnWBym/udR1Sax/LBTBPsR3QkecgZNVby/YiacRBmT7MBH1Rrbt2dg6wSryA1KNAcVL/8YczSCKVhgmrd89zE9DOqDGcCZxU/1ZhQNqEj7FkqaYS6n80zz8iZVYYkjJV90pC5+nsjo5HW0yiwk3lGvezl4n9eLzXhdT/jMkkNSrY4FKaCmJjkBZAhV8iMmFpCmeI2K2FjqigztqaKLcFb/vIqaV/UPbfu3V/WGjdFHWU4gVM4Bw+uoAF30IQWMEjgGV7hzUmdF+fd+ViMlpxi5xj+wPn8AX25kfc=</latexit><latexit sha1_base64="UBwr2R+8MpY5RFOsjT7bXwdRlsw=">AAAB83icbVDLSsNAFL2pr1pfVZduBovgqiQi6LLoxmUF+4CmlMn0ph06mYSZiVJCf8ONC0Xc+jPu/BsnbRbaemDgcM693DMnSATXxnW/ndLa+sbmVnm7srO7t39QPTxq6zhVDFssFrHqBlSj4BJbhhuB3UQhjQKBnWBym/udR1Sax/LBTBPsR3QkecgZNVby/YiacRBmT7MBH1Rrbt2dg6wSryA1KNAcVL/8YczSCKVhgmrd89zE9DOqDGcCZxU/1ZhQNqEj7FkqaYS6n80zz8iZVYYkjJV90pC5+nsjo5HW0yiwk3lGvezl4n9eLzXhdT/jMkkNSrY4FKaCmJjkBZAhV8iMmFpCmeI2K2FjqigztqaKLcFb/vIqaV/UPbfu3V/WGjdFHWU4gVM4Bw+uoAF30IQWMEjgGV7hzUmdF+fd+ViMlpxi5xj+wPn8AX25kfc=</latexit><latexit sha1_base64="UBwr2R+8MpY5RFOsjT7bXwdRlsw=">AAAB83icbVDLSsNAFL2pr1pfVZduBovgqiQi6LLoxmUF+4CmlMn0ph06mYSZiVJCf8ONC0Xc+jPu/BsnbRbaemDgcM693DMnSATXxnW/ndLa+sbmVnm7srO7t39QPTxq6zhVDFssFrHqBlSj4BJbhhuB3UQhjQKBnWBym/udR1Sax/LBTBPsR3QkecgZNVby/YiacRBmT7MBH1Rrbt2dg6wSryA1KNAcVL/8YczSCKVhgmrd89zE9DOqDGcCZxU/1ZhQNqEj7FkqaYS6n80zz8iZVYYkjJV90pC5+nsjo5HW0yiwk3lGvezl4n9eLzXhdT/jMkkNSrY4FKaCmJjkBZAhV8iMmFpCmeI2K2FjqigztqaKLcFb/vIqaV/UPbfu3V/WGjdFHWU4gVM4Bw+uoAF30IQWMEjgGV7hzUmdF+fd+ViMlpxi5xj+wPn8AX25kfc=</latexit><latexit sha1_base64="UBwr2R+8MpY5RFOsjT7bXwdRlsw=">AAAB83icbVDLSsNAFL2pr1pfVZduBovgqiQi6LLoxmUF+4CmlMn0ph06mYSZiVJCf8ONC0Xc+jPu/BsnbRbaemDgcM693DMnSATXxnW/ndLa+sbmVnm7srO7t39QPTxq6zhVDFssFrHqBlSj4BJbhhuB3UQhjQKBnWBym/udR1Sax/LBTBPsR3QkecgZNVby/YiacRBmT7MBH1Rrbt2dg6wSryA1KNAcVL/8YczSCKVhgmrd89zE9DOqDGcCZxU/1ZhQNqEj7FkqaYS6n80zz8iZVYYkjJV90pC5+nsjo5HW0yiwk3lGvezl4n9eLzXhdT/jMkkNSrY4FKaCmJjkBZAhV8iMmFpCmeI2K2FjqigztqaKLcFb/vIqaV/UPbfu3V/WGjdFHWU4gVM4Bw+uoAF30IQWMEjgGV7hzUmdF+fd+ViMlpxi5xj+wPn8AX25kfc=</latexit>

Multilingual Token Embeddings of the Context on the Right 

…

k
<latexit sha1_base64="FN4g9GFe+3ziaTgM0DMwdUeUKp0=">AAAB6HicbVBNS8NAEJ3Ur1q/qh69LBbBU0lEqMeiF48t2A9oQ9lsJ+3azSbsboQS+gu8eFDEqz/Jm//GbZuDtj4YeLw3w8y8IBFcG9f9dgobm1vbO8Xd0t7+weFR+fikreNUMWyxWMSqG1CNgktsGW4EdhOFNAoEdoLJ3dzvPKHSPJYPZpqgH9GR5CFn1FipORmUK27VXYCsEy8nFcjRGJS/+sOYpRFKwwTVuue5ifEzqgxnAmelfqoxoWxCR9izVNIItZ8tDp2RC6sMSRgrW9KQhfp7IqOR1tMosJ0RNWO96s3F/7xeasIbP+MySQ1KtlwUpoKYmMy/JkOukBkxtYQyxe2thI2poszYbEo2BG/15XXSvqp6btVrXlfqt3kcRTiDc7gED2pQh3toQAsYIDzDK7w5j86L8+58LFsLTj5zCn/gfP4A0oWM7w==</latexit><latexit sha1_base64="FN4g9GFe+3ziaTgM0DMwdUeUKp0=">AAAB6HicbVBNS8NAEJ3Ur1q/qh69LBbBU0lEqMeiF48t2A9oQ9lsJ+3azSbsboQS+gu8eFDEqz/Jm//GbZuDtj4YeLw3w8y8IBFcG9f9dgobm1vbO8Xd0t7+weFR+fikreNUMWyxWMSqG1CNgktsGW4EdhOFNAoEdoLJ3dzvPKHSPJYPZpqgH9GR5CFn1FipORmUK27VXYCsEy8nFcjRGJS/+sOYpRFKwwTVuue5ifEzqgxnAmelfqoxoWxCR9izVNIItZ8tDp2RC6sMSRgrW9KQhfp7IqOR1tMosJ0RNWO96s3F/7xeasIbP+MySQ1KtlwUpoKYmMy/JkOukBkxtYQyxe2thI2poszYbEo2BG/15XXSvqp6btVrXlfqt3kcRTiDc7gED2pQh3toQAsYIDzDK7w5j86L8+58LFsLTj5zCn/gfP4A0oWM7w==</latexit><latexit sha1_base64="FN4g9GFe+3ziaTgM0DMwdUeUKp0=">AAAB6HicbVBNS8NAEJ3Ur1q/qh69LBbBU0lEqMeiF48t2A9oQ9lsJ+3azSbsboQS+gu8eFDEqz/Jm//GbZuDtj4YeLw3w8y8IBFcG9f9dgobm1vbO8Xd0t7+weFR+fikreNUMWyxWMSqG1CNgktsGW4EdhOFNAoEdoLJ3dzvPKHSPJYPZpqgH9GR5CFn1FipORmUK27VXYCsEy8nFcjRGJS/+sOYpRFKwwTVuue5ifEzqgxnAmelfqoxoWxCR9izVNIItZ8tDp2RC6sMSRgrW9KQhfp7IqOR1tMosJ0RNWO96s3F/7xeasIbP+MySQ1KtlwUpoKYmMy/JkOukBkxtYQyxe2thI2poszYbEo2BG/15XXSvqp6btVrXlfqt3kcRTiDc7gED2pQh3toQAsYIDzDK7w5j86L8+58LFsLTj5zCn/gfP4A0oWM7w==</latexit><latexit sha1_base64="FN4g9GFe+3ziaTgM0DMwdUeUKp0=">AAAB6HicbVBNS8NAEJ3Ur1q/qh69LBbBU0lEqMeiF48t2A9oQ9lsJ+3azSbsboQS+gu8eFDEqz/Jm//GbZuDtj4YeLw3w8y8IBFcG9f9dgobm1vbO8Xd0t7+weFR+fikreNUMWyxWMSqG1CNgktsGW4EdhOFNAoEdoLJ3dzvPKHSPJYPZpqgH9GR5CFn1FipORmUK27VXYCsEy8nFcjRGJS/+sOYpRFKwwTVuue5ifEzqgxnAmelfqoxoWxCR9izVNIItZ8tDp2RC6sMSRgrW9KQhfp7IqOR1tMosJ0RNWO96s3F/7xeasIbP+MySQ1KtlwUpoKYmMy/JkOukBkxtYQyxe2thI2poszYbEo2BG/15XXSvqp6btVrXlfqt3kcRTiDc7gED2pQh3toQAsYIDzDK7w5j86L8+58LFsLTj5zCn/gfP4A0oWM7w==</latexit>

CNN CNN 
ReLU

Figure 3: Local Context Encoder, for the right context. Fig-
ure 2b shows how it fits inside Mention Context Encoder.

Local Context Representation The local con-
text of a mention m, spanning tokens i to j, con-
sists of left context (tokens i −W to j) and right
context (tokens i to j +W ). For example, for the
mention [Liverpool] in Figure 2b, the left and right
contexts are “Everton won against Liverpool” and
“Liverpool in a FA Cup match” respectively. The lo-
cal context encoder (Figure 3) encodes the left and
the right contexts into vectors l ∈ Rh and r ∈ Rh
using a convolutional neural network (CNN). These
two vectors are then combined to generate the local
context vector c ∈ Rh (Figure 2b).

The CNN convolves continuous spans of k to-
kens using a filter matrix F ∈ Rkd×h to project
the concatenation (⊕ operator) of the token embed-
dings in the span. The resulting vector is passed
through a ReLU unit to generate convolutional out-
put Oi. The outputs {Oi} are pooled by averaging,

Oi = RELU(F
T (wi ⊕ · · · ⊕wi+k−1)) (1)

ENC(w1:n) = AVG(O1, · · · ,On−k+1) (2)

Left and right context vectors l and r are computed
using respective ENC(.) layers,

l = ENCleft(wi−W · · ·wj) (3)
r = ENCright(wi · · ·wj+W ) (4)

These vectors together generate the local context
vector c = F2h,h(l ⊕ r). Here Fdi,do : vi → vo
denotes a feed-forward layer that takes vi ∈ Rdi as
input, and outputs vo ∈ Rdo .

Document Context Representation Presence
of certain mentions in a document can help dis-
ambiguate other mentions. For example, “Suarez”,
“Everton” in a document can help disambiguate
“Liverpool”. To incorporate this, we define the



2489

document context dm of a mention m appearing
in document D to be the bag of all other men-
tions in D. We encode dm into a dense document
context vector d ∈ Rh by a feed-forward layer
d = F|V |,h(dm). Here V is the set containing all
mention surfaces seen during training. When train-
ing jointly over multiple languages, V consists of
mention surfaces seen in all languages (e.g. all
English and Tamil mention surfaces) during train-
ing. This enables parameter sharing by embedding
mention surfaces in different languages in the same
low-dimensional space.
The local and document context vectors c and d
are combined to get the mention context vector
g = F2h,h(c⊕ d).

Context Conditional Probability We compute
the probability of a mention m linking to entity e
using its context vector g and the entity vector e,

Pcontext(e | m) =
exp(gTe)∑

e′∈C(m)
exp(gTe′)

(5)

where C(m) denotes all candidate entities of the
mention m (§3.1 explains how C(m) is gener-
ated). We minimize the negative log-likelihood
of Pcontext(e | m) with respect to the gold entity e∗
against the candidate entities C(m), and call it the
Entity-Context loss (EC-LOSS),

EC-LOSS =− log Pcontext(e
∗ | m)∑

e′∈C(m)
Pcontext(e′ | m)

(6)

2.2 Including Type Information
Incorporating the fine-grained types of a mention
m can help rank entities of the appropriate type
higher than others (Ling et al., 2015; Gupta et al.,
2017; Raiman and Raiman, 2018). For instance,
knowing the correct type of mention [Liverpool]
as sports_team and constraining linking to entities
with the relevant type, encourages disambiguation
to the correct entity.

To make the mention context representation g
type-aware, we predict the set of fine-grained types
of m, T(m) = {t1, ..., t|T(m)|} using g. Each ti
belongs to a pre-defined type vocabulary Γ.2 The
probability of a type t belonging to T(m) given the
mention context is defined as P(t | m) =σ(tTg),
where σ is the sigmoid function and t is the learn-
able embedding for type t.

2We use the type vocabulary Γ from Ling and Weld (2012),
which contains 112 fine-grained types (|Γ| = 112)

We define a Type-Context loss (TC-LOSS) as,

TC-LOSS = BCE(T(m),P(t | m)) (7)

where BCE is the Binary Cross-Entropy Loss,

−
∑

t∈T(m)

log P(t | m)−
∑

t6∈T(m)

log(1− P(t | m))

We also incorporate the entity-type information
in the entity representations, and define a similar
Type-Entity loss (TE-LOSS).

To identify the gold types T(m) of a mention
m, we make the distant supervision assumption
(same as Ling et al. (2015)) and assign the types
of the gold entity e∗ to be the types of the men-
tion. Gold fine-grained types of the entities can be
acquired from resources like Freebase (Bollacker
et al., 2008) or YAGO (Hoffart et al., 2013).

3 Training and Inference

We explain how XELMS generates candidate enti-
ties, performs inference, and combines the different
training losses.

3.1 Candidate Generation
Candidate generation identifies a small number of
plausible entities for a mention m to avoid brute
force comparison with all KB entities. Given m,
candidate generation outputs a list of candidate
entities C(m) = {e1, e2, · · · , eK} of size at most
K (we use K=20), each associated with a prior
probability Pprior(ei | m) indicating the probability
of m referring to ei, given only m’s surface. Pprior
is estimated from counts over the training mentions.

We adopt Tsai and Roth (2016)’s candidate gen-
eration strategy with some minor modifications
(Appendix A). Using other approaches like Cross-
Wikis (Spitkovsky and Chang, 2012), lead to con-
sistently worse recall. We note that transliteration
based candidate generation (McNamee et al., 2011;
Pan et al., 2017; Tsai and Roth, 2018; Upadhyay
et al., 2018) can further improve recall.

3.2 Inference
We combine the context conditional entity proba-
bility Pcontext(e | m) (eq. 5) and prior probability
Pprior(e | m) by taking their union:

Pmodel(e | m) = Pprior(e | m) + Pcontext(e | m)
− Pprior(e | m)× Pcontext(e | m)

Inference for the mention m picks the entity,

ê = arg max
e∈C(m)

Pmodel(e | m) (8)



2490

3.3 Training Objective
When only training the mention context encoder
and entity vectors, we minimize the EC-LOSS av-
eraged over all training mentions. When using the
two type-aware losses, we minimize a weighted
sum of EC-LOSS, TE-LOSS, and TC-LOSS, using
the weighing scheme of Kendall et al. (2018),

EC-LOSS
2λ2EC

+
TE-LOSS

2λ2TE
+

TC-LOSS
2λ2TC

+ log λ2EC + log λ
2
TE + log λ

2
TC

(9)

Here λi are learnable scalar weighing parameters,
and the respective 1

2λ2i
and log λ2i term ensure that

λ2i does not grow unboundedly. This way, the
model learns the relative weight for each loss term.

During training, mentions from different lan-
guages are mixed using inverse-ratio mini-batch
mixing strategy. That is, if two languages have
training data sizes proportional to α : β, at any
time during training, mini-batches seen from them
are in the ratio 1α :

1
β . This strategy prevents lan-

guages with more training data from overwhelming
languages with less training data. Though simple,
we found this strategy yielded good results.

4 Experimental Setup

We briefly describe the training and evaluation
datasets, and the previous XEL approaches from
the literature used in our comparison.

4.1 Training Mentions
Following previous work, we use hyperlinks from
Wikipedia (dumps dated 05/20/2017) as our source
of grounded mentions for supervision. Wikipedias
in different languages have different pages for the
same entity, which are resolved by using inter-
language links (e.g., page 利物浦 in Chinese
Wikipedia resolves to Liverpool in English). Train-
ing mentions statistics are shown in Table 1.

We evaluate on 8 languages – German (de),
Spanish (es), Italian (it), French (fr), Chinese (zh),
Arabic (ar), Turkish (tr) and Tamil (ta), each of
which has varying amount of grounded mentions
from the respective Wikipedia (Table 1). We note
that our method is applicable to any of the 293
Wikipedia languages as a target language.

4.2 Evaluation Datasets
We evaluate XELMS on the following benchmark
datasets, spanning 8 different languages, thus pro-
viding an extensive evaluation.

Lang. # Train Mentions Size Relative to #English Mentions

German (de) 22.6M 43.7%
Spanish (es) 13.8M 26.7%
French (fr) 16.2M 31.3%
Italian (it) 11.5M 22.2%

Chinese (zh) 5.9M 11.4%
Arabic (ar) 3.1M 6.0%
Turkish (tr) 1.8M 3.5%
Tamil (ta) 473k 0.9%

Table 1: Number of train mentions (from Wikipedia) in each
language, with % size relative to English (51.7M mentions).
Train mentions from Wikipedias like Arabic, Turkish and
Tamil are <10% the size of those from the English Wikipedia.

McN-Test dataset from (McNamee et al., 2011).
The test set was collected by using parallel doc-
ument collections, and then crowd-sourcing the
ground truths. All the test mentions in this dataset
consists of person-names only.

TH-Test A subset of the dataset used in (Tsai
and Roth, 2016), derived from Wikipedia.3 The
mentions in the dataset fall in two categories – easy
and hard, where hard mentions are those for which
the most likely candidate according to the prior
probability (i.e., arg max Pprior(e | m)) is not the
correct title. Indeed, most Wikipedia mentions can
be correctly linked by selecting the most likely
candidate (Ratinov et al., 2011). We use all the
hard mentions from Tsai and Roth (2016)’s test
splits for each language, and collectively call this
subset TH-TEST.

TAC15-Test TAC 2015 (Ji et al., 2015) dataset
for Chinese and Spanish. It contains documents
from discussion forum articles and news.

We evaluate all models using linking accuracy on
gold mentions, and assume gold mentions are pro-
vided at test time. Table 2 summarizes the different
domains of the evaluation datasets.

Tuning We avoid any dataset-specific tuning, in-
stead tuning on a development set and applying
the same parameters across all datasets. All tun-
able parameters were tuned on a development set
containing the hard mentions from the train split
released by Tsai and Roth (2016). We refer the
reader to Appendix B for details on tuning.

3Pan et al. (2017) also created a dataset using Wikipedia,
but did not categorize mentions like Tsai and Roth (2016).
Preliminary experiments on their dataset showed XELMS con-
sistently beat Pan et al. (2017)’s model. We chose TH-TEST
for more controlled experiments.



2491

Dataset Lang. Source

TH-TEST de, es, fr, it, Wikipediazh, ar, tr, ta

MCN-TEST de, es, fr, it, News,zh, ar, tr Parliament Proceedings

TAC15-TEST es, zh News,Discussion Forums

Table 2: Evaluation datasets used in our experiments.

4.3 Comparative Approaches

We compare against the following state-of-the-art
(SoTA) approaches, described with the language
from which they use mention contexts in (.),

Tsai and Roth (2016) (Target Only) trains a
separate XEL model for each language using men-
tion contexts from the target language Wikipedia
only. Current SoTA on TH-TEST.

Pan et al. (2017) (English Only) uses entity co-
herence statistics from English Wikipedia and the
document context of a mention for XEL. Current
SoTA on MCN-TEST, except for Italian and Turk-
ish, for which it’s McNamee et al. (2011).

Sil et al. (2018) (English Only) uses multilin-
gual embeddings to transfer a pre-trained English
entity linking model to perform XEL for Spanish
and Chinese. Prior probabilities Pprior are used as a
feature. Current SoTA on TAC15-TEST.

5 Experiments

We show that: (a) XELMS can train a better en-
tity linking model for a target language on various
benchmark datasets by exploiting additional data
from a high resource language like English (§5.1).
(b) XELMS can train a single XEL model for multi-
ple related languages and improve upon separately
trained models (§5.2). (c) Adding additional type
information as multi-task loss to XELMS further
improves performance (§5.3).

In all tables, we report the linking accuracy of
XELMS, averaged over 5 different runs, and mark
with ∗ the statistical significance (p < 0.01) of the
best result (shown bold) against the state-of-the-art
(SoTA) using Student’s one-sample t-test.

5.1 Monolingual and Joint Models

In Table 3 and 4 we compare XELMS(mono), which
uses monolingual supervision in the target lan-
guage only, and XELMS(joint), which uses supervi-

Dataset → TH-TEST MCN-TEST

Lang ↓ SoTA XELMS SoTA XELMSmono joint mono joint

de 53.3 53.7 55.6∗ 89.7 90.9 91.5
es 54.5 54.9 56.6∗ 91.5 91.2 91.4
fr 47.5 48.5 49.9∗ 92.1 92.6 92.7
it 48.3 48.4 51.9∗ 85.9 87.0 87.8∗
zh 57.6 58.1 61.3∗ 91.2∗ 87.4 88.2
ar 62.1 62.6 63.8∗ 80.2 80.3 83.1∗
tr 60.2 61.0 61.7∗ 95.3∗ 91.0 91.9
ta 54.1 54.7 59.7∗ n/a n/a n/a

avg. 54.7 55.2 57.6 89.4 88.6 89.5

Table 3: XELMS(joint) improves upon XELMS(mono) and
the current State-of-The-Art (SoTA) on TH-TEST and MCN-
TEST, showing the benefit of using additional supervision
from English. The best score is shown bold and ∗ marks
statistical significance of best against SoTA. Refer §4.3 for
details on SoTA.

Model ↓ Lang. → es zh

(Tsai and Roth, 2016) 82.4 85.1
(Sil et al., 2018) (SoTA) 83.9 85.9

X
E

L
M

S
mono 83.3 84.4

mono+type 83.5 84.8

joint 84.1 85.5
joint+type 84.4∗ 86.0

multi 83.9 n/a
multi+type 84.4∗ n/a

Table 4: Linking accuracy on TAC15-Test. Numbers for Sil
et al. (2018) from personal communication.

sion from English in addition to the monolingual
supervision, with the state-of-the-art approaches.

We see that XELMS(mono) achieves similar or
slightly better scores than respective SoTA on all
datasets. The SoTA for MCN-TEST in Turkish
and Chinese enhances the model by using translit-
eration for candidate generation, explaining their
superior performance. XELMS(joint) performs sub-
stantially better than XELMS(mono) on all datasets
(Table 3 and 4), proving that using additional super-
vision from a high resource language like English
leads to better linking performance. In particular,
XELMS(joint) outperforms the SoTA on all lan-
guages in TH-TEST, on Spanish in TAC15-Test,
and on 4 of the 7 languages in MCN-TEST.

5.2 Multilingual Training

XELMS is the first approach that can train a single
XEL model for multiple languages. To demon-
strate this capability, we train a model, henceforth
referred as XELMS(multi), jointly on 5 related lan-
guages – Spanish, German, French, Italian and En-



2492

Dataset → TH-TEST MCN-TEST

Lang ↓ SoTA XELMS SoTA XELMSjoint multi joint multi

de 53.3 55.6∗ 55.2 89.7 91.5 91.4
es 54.5 56.6 56.8∗ 91.5 91.4 91.4
fr 47.5 49.9 51.0∗ 92.1 92.7 92.6
it 48.3 51.9 52.3∗ 85.9 87.8 87.9∗

avg. 50.9 53.5 53.8 89.8 90.8 90.8

Table 5: Linking accuracy of a single XELMS(multi) model
for four languages – German, Spanish, French and Italian.
Individually trained XELMS(joint) scores are also shown. The
best score is shown bold and ∗ marks statistical significance
of best against SoTA. Refer §4.3 for details on SoTA.

glish. We compare XELMS(multi) to the respective
XELMS(joint) model for each language.

Table 4 and 5, show that XELMS(multi) is bet-
ter (or at par) than XELMS(joint) on all datasets.
This shows that XELMS(multi) can making more
efficient use of available supervision in related lan-
guages than previous approaches which trained sep-
arate models per language.

5.3 Adding Fine-grained Type Information
To study the effect of adding fine-grained type in-
formation, in Table 4 we compare XELMS(mono)
and XELMS(joint) to XELMS(mono+type) and
XELMS(joint+type) respectively, which are versions
of XELMS(mono) and XELMS(joint) trained using
the two type-aware losses.

XELMS(mono+type) and XELMS(joint+type)
both improve compared to XELMS(mono) and
XELMS(joint) on MCN-TEST and TH-TEST
(Table 6 vs Table 3), showing the benefit of using
structured knowledge in the form of fine-grained
types. Similar trends are also seen on TAC15-
TEST (Table 4), where XELMS(joint+type) improves
on the SoTA for Spanish and Chinese.

6 Experiments with Limited Resources

The key motivation of XELMS is to exploit supervi-
sion from high-resource languages like English to
aid XEL for languages with limited resources. In
this section, we examine two such scenarios,
(a) Zero-shot setting i.e., no supervision available
in the target language. Our analysis reveals the
limitations of zero-shot XEL approaches and finds
that the prior probabilities play an important role
in achieving good performance (§6.1), which are
unavailable in realistic zero-shot scenarios.
(b) Low-resource setting i.e., some supervision
available in the target language. We show that

Dataset → TH-TEST MCN-TEST

Lang ↓ SoTA XELMS SoTA XELMSmono+type joint+type mono+type joint+type

de 53.3 54.0 55.9∗ 89.7 91.2 91.5
es 54.5 55.1 57.2∗ 91.5 91.0 91.2
fr 47.5 49.0 50.6∗ 92.1 92.6 92.7
it 48.3 49.2 52.2∗ 85.9 87.4 87.9∗
zh 57.6 58.9 61.5∗ 91.2∗ 87.6 88.4
ar 62.1 63.0 64.0∗ 80.2 81.1 84.0∗
tr 60.2 61.5 62.0∗ 95.3∗ 91.2 92.1
ta 54.1 56.0 59.9∗ n/a n/a n/a

avg. 54.7 55.8 57.9 89.4 88.9 89.7

Table 6: Adding fine-grained type information further im-
proves linking accuracy (compare to Table 3). The best score
is shown bold and ∗ marks statistical significance of best
against SoTA. Refer §4.3 for details on SoTA.

by combining supervision from a high-resource
language, like English, XELMS can achieve com-
petitive performance with a fraction of available
supervision in the target language (§6.2).

6.1 Zero-shot Setting

We first explain how XELMS can perform zero-shot
XEL, the implications of our zero-shot setting, and
how it is more realistic than previous work.

Zero-shot XEL with XELMS XELMS performs
zero-shot XEL by training a model using English
supervision and multilingual embeddings for En-
glish, and directly applying it to the test data in
another language using the respective multilingual
word embedding instead of English embeddings.

No Prior Probabilities Prior probabilities (or
prior), i.e., Pprior have been shown to be a reliable
indicator of the correct disambiguation in entity
linking (Ratinov et al., 2011; Tsai and Roth, 2016).
These probabilities are estimated from counts over
the training mentions in the target language. In the
absence of training data for the target language, as
in the zero-shot setting, these prior probabilities are
not available to an XEL model.

Comparison to Previous Work The only other
model capable of zero-shot XEL is that of Sil et al.
(2018). However, Sil et al. (2018) use prior prob-
abilities and coreference chains for the target lan-
guage in their zero-shot experiments, both of which
will not be available in a realistic zero-shot sce-
nario. Compared to Sil et al. (2018), we evaluate
the performance of zero-shot XEL in more real-
istic setting, and show it is adversely affected by
absence of prior probabilities.



2493

Dataset → TAC15-Test TH-Test McN-Test
Approach ↓ (es) (zh) (avg) (avg)

XELMS (Z-S w/ prior) 80.3 83.9 43.5 88.1
XELMS (Z-S w/o prior) 53.5 55.9 41.1 86.0

SoTA 83.9 85.9 54.7 89.4

Table 7: Linking accuracy of the zero-shot (Z-S) approach
on different datasets. Zero-shot (w/ prior) is close to SoTA for
datasets like TAC15-Test, but performance drops in the more
realistic setting of zero-shot (w/o prior) (§6.1) on all datasets,
indicating most of the performance can be attributed to the
presence of prior probabilities. The slight drop in MCN-TEST
is due to trivial mentions, which only have a single candidate.

Is zero-shot XEL really effective? To evaluate
the effectiveness of the zero-shot XEL approach,
we perform zero-shot XEL using XELMS on all
datasets. Table 7 shows zero-shot XEL results
on all datasets, both with and without using the
prior during inference. Note that zero-shot XEL
(with prior) is close to SoTA (Sil et al. (2018)) on
TAC15-TEST, which also uses the prior for zero-
shot XEL. However, for zero-shot XEL (without
prior) performance drops by more than 20% for
TAC15-Test, 2.4% for TH-Test and by 2.1% for
McN-Test. This indicates that zero-shot XEL is not
effective in a realistic zero-shot setting (i.e., when
the prior is unavailable for inference).

We found that the prior is indeed a strong indi-
cator of the correct disambiguation. For instance,
simply selecting the the most likely candidate us-
ing the prior for TAC15-TEST achieved 77.2% and
78.8% for Spanish and Chinese respectively. It is
interesting to note that both zero-shot XEL (with
or without prior) perform worse than the best pos-
sible model on TH-TEST, because TH-TEST was
constructed to ensure prior probabilities are not
strong indicators (Tsai and Roth, 2016). On MCN-
TEST, we found that an average of 75.9% mentions
have only one (the correct) candidate, making them
trivial to link, regardless of the absence of priors.

The results show that most of the XEL perfor-
mance in zero-shot settings can be attributed to
availability of prior probabilities for the candidates.
It is evident that zero-shot XEL in a realistic setting
(i.e., when prior probabilities are not available) is
still a challenging problem.

6.2 Low-resource Setting

We analyze the behavior of XELMS in a low-
resource setting, i.e. when some supervision is
available in the target language. The aim of this
setting is to estimate how much supervision from

0 0.25 0.5 0.75 1

40

45

50

55

60

# train mentions in target language (in millions)

L
in

ki
ng

A
cc

ur
ac

y

tr-mono tr-joint tr-best
zh-mono zh-joint zh-best
es-mono es-joint es-best

Figure 4: Linking accuracy vs. the number of train mentions
in the target language L (= Turkish (tr), Chinese (zh) and Span-
ish (es)). We compare both XELMS(mono) and XELMS(joint)
to the best results using all available supervision, denoted by
L-best. To discount the effect of the prior, all results above are
without it. For number of train mentions = 0, XELMS(joint) is
equivalent to zero-shot without prior. Best viewed in color.

the target language is needed to get reasonable per-
formance when using it jointly with supervision
from English. To discount the effect of prior proba-
bilities, we report all results without the prior.

Figure 4 plots results on the TH-Test dataset
when training a XELMS(joint) model by gradu-
ally increasing the number of mention contexts for
target language L (= Spanish, Chinese and Turk-
ish) that are available for supervision. Figure 4
also shows the best results achieved using all avail-
able target language supervision (denoted by L-
best). For comparison with the mono-lingually
supervised model, we also plot the performance
of XELMS(mono), which only uses the target lan-
guage supervision.

Figure 4 shows that after training on 0.75M men-
tions from Turkish and Chinese (and 1.0M men-
tions from Spanish), the XELMS(joint) model is
within 2-3% of the respective L-best model which
uses all training mentions in the target language,
indicating that XELMS(joint) can reach competi-
tive performance even with a fraction of the full
target language supervision. For comparison, a
XELMS(mono) model trained on the same number
of training mentions is 5-10% behind the respective
XELMS(joint) model, showing better utilization of
target language supervision by XELMS(joint).



2494

7 Related Work

Existing approaches have taken two main direc-
tions to obtain supervision for learning XEL mod-
els — (a) using mention contexts appearing in the
target language (McNamee et al., 2011; Tsai and
Roth, 2016), or (b) using mention contexts appear-
ing only in English (Pan et al., 2017; Sil et al.,
2018). We describe these directions and their limi-
tations below, and explain how XELMS overcomes
these limitations.

McNamee et al. (2011) use annotation projec-
tion via parallel corpora to generate mention con-
texts in the target language, while Tsai and Roth
(2016) learns separate XEL models for each lan-
guage and only use mention contexts in the target
language. Both these approach have scalability is-
sues for languages with limited resources. Another
limitation of these approaches is that they train sep-
arate models for each language, which is inefficient
when working with multiple languages. XELMS
overcomes these limitations as it can use mention
context from multiple languages simultaneously,
and train a single model.

Other approaches only use mention contexts
from English. While Pan et al. (2017) compute
entity coherence statistics from English Wikipedia,
Sil et al. (2018) perform zero-shot XEL for Chinese
and Spanish by using multilingual embeddings to
transfer a pre-trained English EL model. How-
ever, our work suggests that mention contexts in
the target language should also be used, if avail-
able. Indeed, a recent study (Lewoniewski et al.,
2017) found that for language sensitive topics, the
quality of information can be better in the relevant
language version of Wikipedia than the English ver-
sion. Our analysis also shows that zero-shot XEL
approaches like that of Sil et al. (2018) are not ef-
fective in realistic zero-shot scenarios where good
prior probabilities are unlikely to be available. In
such cases, we showed that combining supervision
available in the target language with supervision
from a high-resource language like English can
yield significant performance improvements.

The architecture of XELMS is inspired by sev-
eral monolingual entity linking systems (Francis-
Landau et al., 2016; Nguyen et al., 2016; Gupta
et al., 2017), approaches that use type informa-
tion to aid entity linking (Ling et al., 2015; Gupta
et al., 2017; Raiman and Raiman, 2018), and the re-
cent success of multilingual embeddings for several
tasks (Ammar et al., 2016a; Duong et al., 2017).

8 Conclusion

We introduced XELMS, an approach that can
combine supervision from multiple languages to
train an XEL model. We illustrate its benefits
through extensive evaluation on different bench-
marks. XELMS is also the first approach that can
train a single model for multiple languages, making
more efficient use of available supervision than pre-
vious approaches which trained separate models.

Our analysis sheds light on the poor performance
of zero-shot XEL in realistic scenarios where the
prior probabilities for candidates are unlikely to
exist, in contrast to findings in previous work that
focused on high-resource languages. We also show
how in low-resource settings, XELMS makes it
possible to achieve competitive performance even
when only a fraction of the available supervision in
the target language is provided.

Several future research directions remain open.
For all XEL approaches, the task of candidate gen-
eration is currently limited by existence of a target
language Wikipedia and remains a key challenge.
A joint inference framework which enforces co-
herent predictions (Cheng and Roth, 2013; Glober-
son et al., 2016; Ganea and Hofmann, 2017) could
also lead to further improvements for XEL. Simi-
lar techniques can be applied to other information
extraction tasks like relation extraction to extend
them to multilingual settings.

Acknowledgments

The authors would like to thank Snigdha
Chaturvedi, Anne Cocos, Stephen Mayhew, Chen-
Tse Tsai, Qiang Ning, Jordan Kodner, Dan Deutsch,
John Hewitt and the anonymous EMNLP reviewers
for their useful comments and suggestions.

This work was supported by Contract HR0011-
15-2-0025 and Agreement HR0011-15-2-0023
with the US Defense Advanced Research Projects
Agency (DARPA). Approved for Public Release,
Distribution Unlimited. The views expressed are
those of the authors and do not reflect the official
policy or position of the Department of Defense or
the U.S. Government.

References
Waleed Ammar, George Mulcaire, Miguel Ballesteros,

Chris Dyer, and Noah Smith. 2016a. Many Lan-
guages, One Parser. In Transactions of the Associ-
ation for Computational Linguistics, volume 4.



2495

Waleed Ammar, George Mulcaire, Yulia Tsvetkov,
Guillaume Lample, Chris Dyer, and Noah A Smith.
2016b. Massively Multilingual Word Embeddings.
arXiv preprint arXiv:1602.01925.

Piotr Bojanowski, Edouard Grave, Armand Joulin, and
Tomas Mikolov. 2017. Enriching Word Vectors with
Subword Information. In Transactions of the Asso-
ciation for Computational Linguistics, volume 5.

Kurt Bollacker, Colin Evans, Praveen Paritosh, Tim
Sturge, and Jamie Taylor. 2008. Freebase: A Col-
laboratively Created Graph Database for Structuring
Human Knowledge. In Proc. of ACM SIGMOD.

Rich Caruana. 1998. Multitask Learning. In Learning
to Learn, pages 95–133. Springer.

Xiao Cheng and Dan Roth. 2013. Relational Inference
for Wikification. In Proc. of EMNLP.

Long Duong, Hiroshi Kanayama, Tengfei Ma, Steven
Bird, and Trevor Cohn. 2017. Multilingual Train-
ing of Crosslingual Word Embeddings. In Proc. of
EACL.

Matthew Francis-Landau, Greg Durrett, and Dan Klein.
2016. Capturing Semantic Similarity for Entity
Linking with Convolutional Neural Networks. In
Proc. of NAACL-HLT.

Octavian-Eugen Ganea and Thomas Hofmann. 2017.
Deep Joint Entity Disambiguation with Local Neu-
ral Attention. In Proc. of EMNLP.

Amir Globerson, Nevena Lazic, Soumen Chakrabarti,
Amarnag Subramanya, Michael Ringaard, and Fer-
nando Pereira. 2016. Collective Entity Resolution
with Multi-Focal Attention. In Proc. of ACL.

Nitish Gupta, Sameer Singh, and Dan Roth. 2017. En-
tity Linking via Joint Encoding of Types, Descrip-
tions, and Context. In Proc. of EMNLP.

Johannes Hoffart, Fabian M. Suchanek, Klaus
Berberich, and Gerhard Weikum. 2013. YAGO2:
A Spatially and Temporally Enhanced Knowledge
Base from Wikipedia. In Proc. of IJCAI.

Heng Ji, Joel Nothman, Ben Hachey, and Radu Florian.
2015. Overview of TAC-KBP2015 Tri-lingual En-
tity Discovery and Linking. In Text Analysis Confer-
ence.

Alex Kendall, Yarin Gal, and Roberto Cipolla. 2018.
Multi-Task Learning Using Uncertainty to Weigh
Losses for Scene Geometry and Semantics. In Proc.
of CVPR.

Włodzimierz Lewoniewski, Krzysztof Wecel, and
Witold Abramowicz. 2017. Relative Quality and
Popularity Evaluation of Multilingual Wikipedia Ar-
ticles. In Journal of Informatics.

Xiao Ling, Sameer Singh, and Daniel S Weld. 2015.
Design Challenges for Entity Linking. In Transac-
tions of the Association for Computational Linguis-
tics, volume 3.

Xiao Ling and Daniel S Weld. 2012. Fine-grained En-
tity Recognition. In Proc. of AAAI.

Paul McNamee, James Mayfield, Dawn Lawrie, Dou-
glas W Oard, and David S Doermann. 2011. Cross-
Language Entity Linking. In Proc. of IJCNLP.

Rada Mihalcea and Andras Csomai. 2007. Wikify!:
Linking Documents to Encyclopedic Knowledge. In
Proc. of CIKM.

Thien Huu Nguyen, Nicolas Fauceglia, Mariano Ro-
driguez Muro, Oktie Hassanzadeh, Alfio Massimil-
iano Gliozzo, and Mohammad Sadoghi. 2016. Joint
Learning of Local and Global Features for Entity
Linking via Neural Networks. In Proc. of COLING.

Xiaoman Pan, Boliang Zhang, Jonathan May, Joel
Nothman, Kevin Knight, and Heng Ji. 2017. Cross-
lingual Name Tagging and Linking for 282 Lan-
guages. In Proc. of ACL.

Jonathan Raiman and Olivier Raiman. 2018. Deep-
Type: Multilingual Entity Linking by Neural Type
System Evolution. In Proc. of AAAI.

Lev Ratinov, Dan Roth, Doug Downey, and Mike An-
derson. 2011. Local and Global Algorithms for Dis-
ambiguation to Wikipedia. In Proc. of ACL-HLT.

Avirup Sil, Gourab Kundu, Radu Florian, and Wael
Hamza. 2018. Neural Cross-lingual Entity Linking.
In Proc. of AAAI.

Samuel L Smith, David HP Turban, Steven Hamblin,
and Nils Y Hammerla. 2017. Offline Bilingual Word
Vectors, Orthogonal Transformations, and the In-
verted Softmax. In Proc. of ICLR.

Valentin I. Spitkovsky and Angel X. Chang. 2012.
A Cross-Lingual Dictionary for English Wikipedia
Concepts. In Proc. of LREC.

Chen-Tse Tsai and Dan Roth. 2016. Cross-lingual
Wikification Using Multilingual Embeddings. In
Proc. of NAACL.

Chen-Tse Tsai and Dan Roth. 2018. Learning Better
Name Translation for Cross-Lingual Wikification.
In Proc. of AAAI.

Shyam Upadhyay, Jordan Kodner, and Dan Roth. 2018.
Bootstrapping Transliteration with Constrained Dis-
covery for Low-Resource Languages. In Proc. of
EMNLP.


