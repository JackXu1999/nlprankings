



















































Joint Slot Filling and Intent Detection via Capsule Neural Networks


Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 5259–5267
Florence, Italy, July 28 - August 2, 2019. c©2019 Association for Computational Linguistics

5259

Joint Slot Filling and Intent Detection via Capsule Neural Networks

Chenwei Zhang†, Yaliang Li§, Nan Du‡, Wei Fan‡, Philip S. Yu†¶
†University of Illinois at Chicago, Chicago, IL 60607 USA

§Alibaba Group, Bellevue, WA 98004 USA
‡Tencent Medical AI Lab, Palo Alto, CA 94301 USA

¶Institute for Data Science, Tsinghua University, Beijing, China
{czhang99,psyu}@uic.edu, yaliang.li@alibaba-inc.com,

nandu2048@gmail.com, davidwfan@tencent.com

Abstract
Being able to recognize words as slots and de-
tect the intent of an utterance has been a keen
issue in natural language understanding. The
existing works either treat slot filling and in-
tent detection separately in a pipeline man-
ner, or adopt joint models which sequentially
label slots while summarizing the utterance-
level intent without explicitly preserving the
hierarchical relationship among words, slots,
and intents. To exploit the semantic hierarchy
for effective modeling, we propose a capsule-
based neural network model which accom-
plishes slot filling and intent detection via a
dynamic routing-by-agreement schema. A re-
routing schema is proposed to further syner-
gize the slot filling performance using the in-
ferred intent representation. Experiments on
two real-world datasets show the effectiveness
of our model when compared with other alter-
native model architectures, as well as existing
natural language understanding services.

1 Introduction

With the ever-increasing accuracy in speech
recognition and complexity in user-generated ut-
terances, it becomes a critical issue for mobile
phones or smart speaker devices to understand the
natural language in order to give informative re-
sponses. Slot filling and intent detection play im-
portant roles in Natural Language Understanding
(NLU) systems. For example, given an utterance
from the user, the slot filling annotates the utter-
ance on a word-level, indicating the slot type men-
tioned by a certain word such as the slot artist
mentioned by the word Sungmin, while the in-
tent detection works on the utterance-level to give
categorical intent label(s) to the whole utterance.
Figure 1 illustrates this idea.

To deal with diversely expressed utterances
without additional feature engineering, deep neu-
ral network based user intent detection models (Hu

Word Put Sungmin into      my          summer playlist

Slot   O  B-artist  O B-playlist_owner B-playlist O
Intent  AddToPlaylist

Figure 1: An example of an utterance with BOI format an-
notation for slot filling, which indicates the slot of artist, play
list owner, and play list name from an utterance with an intent
AddToPlaylist.

et al., 2009; Xu and Sarikaya, 2013; Zhang et al.,
2016; Liu and Lane, 2016; Zhang et al., 2017;
Chen et al., 2016; Xia et al., 2018) are proposed
to classify user intents given their utterances in the
natural language.

Currently, the slot filling is usually treated as a
sequential labeling task. A neural network such as
a recurrent neural network (RNN) or a convolution
neural network (CNN) is used to learn context-
aware word representations, along with sequence
tagging methods such as conditional random field
(CRF) (Lafferty et al., 2001) that infer the slot type
for each word in the utterance.

Word-level slot filling and utterance-level in-
tent detection can be conducted simultaneously
to achieve a synergistic effect. The recognized
slots, which possess word-level signals, may give
clues to the utterance-level intent of an utterance.
For example, with a word Sungmin being rec-
ognized as a slot artist, the utterance is more
likely to have an intent of AddToPlayList
than other intents such as GetWeather or
BookRestaurant.

Some existing works learn to fill slots while
detecting the intent of the utterance (Xu and
Sarikaya, 2013; Hakkani-Tür et al., 2016; Liu and
Lane, 2016; Goo et al., 2018): a convolution layer
or a recurrent layer is adopted to sequentially la-
bel word with their slot types: the last hidden state
of the recurrent neural network, or an attention-



5260

put my summer playlist
Word

Slot

Intent

playlist contextmovie_type
…

play_music

…

add_to_playlist get_weather

into

… …
……

playlist
_ownerartist

Dynamic 
Routing

Dynamic 
Routing

Re-
Routing

WordCaps

SlotCaps

IntentCaps

Sungmin

Figure 2: Illustration of the proposed CAPSULE-NLU model for joint slot filling and intent detection. The model does slot
filling by learning to assign each word in the WordCaps to the most appropriate slot in SlotCaps via dynamic routing. The
weights learned via dynamic routing indicate how strong each word in WordCaps belongs to a certain slot type in SlotCaps.
The dynamic routing also learns slot representations using WordCaps and the learned weight. The learned slot representations
in SlotCaps are further aggregated to predict the utterance-level intent of the utterance. Once the intent label of the utterance is
determined, a novel re-routing process is proposed to help improve word-level slot filling by the inferred utterance-level intent
label. The solid lines indicate the dynamic-routing process and dash lines indicate the re-routing process.

weighted sum of all convolution outputs are used
to train an utterance-level classification module for
intent detection. Such approaches achieve decent
performances but do not explicitly consider the hi-
erarchical relationship between words, slots, and
intents: intents are sequentially summarized from
the word sequence. As the sequence becomes
longer, it is risky to simply rely on the gate func-
tion of RNN to compress all context information
in a single vector (Cheng et al., 2016).

In this work, we make the very first attempt to
bridge the gap between word-level slot modeling
and the utterance-level intent modeling via a hier-
archical capsule neural network structure (Hinton
et al., 2011; Sabour et al., 2017). A capsule houses
a vector representation of a group of neurons. The
capsule model learns a hierarchy of feature detec-
tors via a routing-by-agreement mechanism: cap-
sules for detecting low-level features send their
outputs to high-level capsules only when there is a
strong agreement of their predictions to high-level
capsules.

The aforementioned properties of capsule mod-
els are appealing for natural language under-
standing from a hierarchical perspective: words
such as Sungmin are routed to concept-level
slots such as artist, by learning how each
word matches the slot representation. Concept-
level slot features such as artist, playlist
owner, and playlist collectively contribute to
an utterance-level intent AddToPlaylist. The
dynamic routing-by-agreement assigns a larger
weight from a lower-level capsule to a higher-level

when the low-level feature is more predictive to
one high-level feature, than other high-level fea-
tures. Figure 2 illustrates this idea.

The inferred utterance-level intent is also help-
ful in refining the slot filling result. For exam-
ple, once an AddToPlaylist intent represen-
tation is learned in IntentCaps, the slot filling may
capitalize on the inferred intent representation and
recognize slots that are otherwise neglected previ-
ously. To achieve this, we propose a re-routing
schema for capsule neural networks, which al-
lows high-level features to be actively engaged in
the dynamic routing between WordCaps and Slot-
Caps, which improves the slot filling performance.

To summarize, the contributions of this work
are as follows:

• Encapsulating the hierarchical relationship
among word, slot, and intent in an utter-
ance by a hierarchical capsule neural network
structure.

• Proposing a dynamic routing schema with re-
routing that achieves synergistic effects for
joint slot filling and intent detection.

• Showing the effectiveness of our model on
two real-world datasets, and comparing with
existing models as well as commercial NLU
services.

2 Approach

We propose to model the hierarchical relation-
ship among each word, the slot it belongs to, and



5261

the intent label of the whole utterance by a hier-
archical capsule neural network structure called
CAPSULE-NLU. The proposed architecture con-
sists of three types of capsules: 1) WordCaps
that learn context-aware word representations, 2)
SlotCaps that categorize words by their slot types
via dynamic routing, and construct a representa-
tion for each type of slot by aggregating words
that belong to the slot, 3) IntentCaps determine
the intent label of the utterance based on the slot
representation as well as the utterance contexts.
Once the intent label has been determined by In-
tentCaps, the inferred utterance-level intent helps
re-recognizing slots from the utterance by a re-
routing schema.

2.1 WordCaps

Given an input utterance x = (w1,w2, ...,wT ) of
T words, where each word is initially represented
by a vector of dimension DW . Here we simply
trained word represenations from scratch. Vari-
ous neural network structures can be used to learn
context-aware word representations. For example,
a recurrent neural network such as a bidirectional
LSTM (Hochreiter and Schmidhuber, 1997) can
be applied to learn representations of each word in
the utterance:

~ht = LSTMfw(wt, ~ht−1),
←
ht = LSTMbw(wt,

←
ht+1).

(1)

For each word wt, we concatenate each for-
ward hidden state ~ht obtained from the forward
LSTMfw with a backward hidden state

←
ht from

LSTMbw to obtain a hidden state ht. The whole
hidden state matrix can be defined as H =
(h1,h2, ...,hT ) ∈ RT×2DH , where DH is the
number of hidden units in each LSTM. In this
work, the parameters of WordCaps are trained
with the whole model, while sophisticated pre-
trained models such as ELMo (Peters et al., 2018)
or BERT (Devlin et al., 2018) may also be inte-
grated.

2.2 SlotCaps

Traditionally, the learned hidden state ht for each
word wt is used as the logit to predict its slot tag.
When H for all words in the utterance is learned,
sequential tagging methods like the linear-chain
CRF models the tag dependencies by assigning a
transition score for each transition pattern between

adjacent tags to ensure the best tag sequence of the
utterance from all possible tag sequences.

Instead of doing slot filling via sequential label-
ing which does not directly consider the depen-
dencies among words, the SlotCaps learn to rec-
ognize slots via dynamic routing. The routing-
by-agreement explicitly models the hierarchical
relationship between capsules. For example,
the routing-by-agreement mechanism send a low-
level feature, e.g. a word representation in Word-
Caps, to high-level capsules, e.g. SlotCaps, only
when the word representation has a strong agree-
ment with a slot representation.

The agreement value on a word may vary when
being recognized as different slots. For exam-
ple, the word three may be recognized as a
party size number slot or a time slot. The
SlotCaps first convert the word representation ob-
tained in WordCaps with respect to each slot type.
We denote pk|t as the resulting prediction vector
of the t-th word when being recognized as the k-
th slot:

pk|t = σ(Wkh
T
t + bk), (2)

where k ∈ {1, 2, ...,K} denotes the slot type and
t ∈ {1, 2, ..., T}. σ is the activation function such
as tanh. Wk ∈ RDP×2DH and bk ∈ RDP×1 are
the weight and bias matrix for the k-th capsule in
SlotCaps, and DP is the dimension of the predic-
tion vector.
Slot Filling by Dynamic Routing-by-agreement
We propose to determine the slot type for each
word by dynamically route prediction vectors of
each word from WordCaps to SlotCaps. The dy-
namic routing-by-agreement learns an agreement
value ckt that determines how likely the t-th word
agrees to be routed to the k-th slot capsule. ckt is
calculated by the dynamic routing-by-agreement
algorithm (Sabour et al., 2017), which is briefly
recalled in Algorithm 1.

Algorithm 1 Dynamic routing-by-agreement
1: procedure DYNAMIC ROUTING(pk|t, iter)
2: for each WordCaps t and SlotCaps k: bkt ← 0.
3: for iter iterations do
4: for all WordCaps t: ct ← softmax(bt)
5: for all SlotCaps k: sk ← Σrcktpk|t
6: for all SlotCaps k: vk = squash(sk)
7: for all WordCaps t and SlotCaps k: bkt ←

bkt + pk|t · vk
8: end for
9: Return vk

10: end procedure

The above algorithm determines the agreement



5262

value ckt between WordCaps and SlotCaps while
learning the slot representations vk in an unsuper-
vised, iterative fashion. ct is a vector that consists
of all ckt where k ∈ K. bkt is the logit (initialized
as zero) representing the log prior probability that
the t-th word in WordCaps agrees to be routed to
the k-th slot capsule in SlotCaps (Line 2). During
each iteration (Line 3), each slot representation vk
is calculated by aggregating all the prediction vec-
tors for that slot type {pk|t|t∈T}, weighted by the
agreement values ckt obtained from bkt (Line 5-6):

sk =
T∑
t

cktpk|t, (3)

vk = squash(sk) =
‖sk‖2

1 + ‖sk‖2
sk
‖sk‖

, (4)

where a squashing function squash(·) is applied
on the weighted sum sk to get vk for each slot
type. Once we updated the slot representation vk
in the current iteration, the logit bkt becomes larger
when the dot product pk|t · vk is large. That is,
when a prediction vector pk|t is more similar to a
slot representation vk, the dot product is larger, in-
dicating that it is more likely to route this word to
the k-th slot type (Line 7). An updated, larger bkt
will lead to a larger agreement value ckt between
the t-th word and the k-th slot in the next itera-
tion. On the other hand, it assigns low ckt when
there is inconsistency between pk|t and vk. The
agreement values learned via the unsupervised, it-
erative algorithm ensures the outputs of the Word-
Caps get sent to appropriate subsequent SlotCaps
after iterslot iterations.
Cross Entropy Loss for Slot Filling
For the t-th word in an utterance, its slot type is
determined as follows:

ŷt = arg max
k∈K

(ckt). (5)

The slot filling loss is defined over the utterance as
the following cross-entropy function:

Lslot = −
∑
t

∑
k

ykt log(ŷ
k
t ), (6)

where ykt indicates the ground truth slot type for
the t-th word. ykt = 1 when the t-th word belongs
to the k-th slot type.

2.3 IntentCaps
The IntentCaps take the output vk for each slot
k ∈ {1, 2, ...,K} in SlotCaps as the input, and
determine the utterance-level intent of the whole
utterance. The IntentCaps also convert each slot
representation in SlotCaps with respect to the in-
tent type:

ql|k = σ(Wlv
T
k + bl), (7)

where l ∈ {1, 2, ..., L} and L is the number of
intents. Wl ∈ RDL×DP and bl ∈ RDL×1 are
the weight and bias matrix for the l-th capsule in
IntentCaps.

IntentCaps adopt the same dynamic routing-by-
agreement algorithm, where:

ul = DYNAMIC ROUTING(ql|k, iterintent). (8)

Max-margin Loss for Intent Detection
Based on the capsule theory, the orientation of
the activation vector ul represents intent proper-
ties while its length indicates the activation prob-
ability. The loss function considers a max-margin
loss on each labeled utterance:

Lintent =
L∑
l=1

{[[z = zl]] ·max(0,m+ − ‖ul‖)2

+ λ [[z 6= zl]] ·max(0, ‖ul‖ −m−)2},
(9)

where ‖ul‖ is the norm of ul and [[]] is an indicator
function, z is the ground truth intent label for the
utterance x. λ is the weighting coefficient, and
m+ and m− are margins.

The intent of the utterance can be easily deter-
mined by choosing the activation vector with the
largest norm ẑ = arg max

l∈{1,2,...,L}
‖ul‖.

2.4 Re-Routing
The IntentCaps not only determine the intent of
the utterance by the length of the activation vec-
tor, but also learn discriminative intent representa-
tions of the utterance by the orientations of the ac-
tivation vectors. Previously, the dynamic routing-
by-agreement shows how low-level features such
as slots help construct high-level ideas such as in-
tents. While the high-level features also work as
a guide that helps learn low-level features. For
example, the AddToPlaylist intent activation
vector in IntentCaps also helps strength the exist-
ing slots such as artist name during slot filling
on the words Sungmin in SlotCaps.



5263

Thus we propose a re-routing schema for Slot-
Caps where the dynamic routing-by-agreement is
realized by the following equation that replaces
the Line 7 in Algorithm 1:

bkt ← bkt + pk|t · vk + α · pTk|tWRRû
T
ẑ , (10)

where ûẑ is the intent activation vector with the
largest norm. WRR ∈ RDP×DL is a bi-linear
weight matrix, and α as the coefficient. The rout-
ing information for each word is updated toward
the direction where the prediction vector not only
coincides with representative slots, but also to-
wards the most-likely intent of the utterance. As
a result, the re-routing makes SlotCaps obtain up-
dated routing information as well as updated slot
representations.

3 Experiment Setup

To demonstrate the effectiveness of our pro-
posed models, we compare the proposed model
CAPSULE-NLU with existing alternatives, as well
as commercial natural language understanding
services.
Datasets For each task, we evaluate our proposed
models by applying it on two real-word datasets:
SNIPS Natural Language Understanding bench-
mark1 (SNIPS-NLU) and the Airline Travel Infor-
mation Systems (ATIS) dataset (Tur et al., 2010).
The statistical information on two datasets are
shown in Table 1.

Dataset SNIPS-NLU ATIS
Vocab Size 11,241 722
Average Sentence Length 9.05 11.28
#Intents 7 21
#Slots 72 120
#Training Samples 13,084 4,478
#Validation Samples 700 500
#Test Samples 700 893

Table 1: Dataset statistics.

SNIPS-NLU contains natural language corpus
collected in a crowdsourced fashion to benchmark
the performance of voice assistants. ATIS is a
widely used dataset in spoken language under-
standing, where audio recordings of people mak-
ing flight reservations are collected.
Baselines We compare the proposed capsule-
based model CAPSULE-NLU with other alterna-
tives: 1) CNN TriCRF (Xu and Sarikaya, 2013)

1https://github.com/snipsco/
nlu-benchmark/

introduces a Convolution Neural Network (CNN)
based sequential labeling model for slot filling.
The hidden states for each word are summed up
to predict the utterance intent. We adopt the
performance with lexical features. 2) Joint Seq.
(Hakkani-Tür et al., 2016) adopts a Recurrent
Neural Network (RNN) for slot filling and the last
hidden state of the RNN is used to predict the ut-
terance intent. 3) Attention BiRNN (Liu and Lane,
2016) further introduces a RNN based encoder-
decoder model for joint slot filling and intent de-
tection. An attention weighted sum of all encoded
hidden states is used to predict the utterance intent.
4) Slot-gated Full Atten. (Goo et al., 2018) utilizes
a slot-gated mechanism as a special gate function
in Long Short-term Memory Network (LSTM) to
improve slot filling by the learned intent context
vector. The intent context vector is used for intent
detection. 5) DR-AGG (Gong et al., 2018) aggre-
gates word-level information for text classification
via dynamic routing. The high-level capsules af-
ter routing are concatenated, followed by a multi-
layer perceptron layer that predicts the utterance
label. We used this capsule-based text classifica-
tion model for intent detection only. 6) IntentCap-
sNet (Xia et al., 2018) adopts a multi-head self-
attention to extract intermediate semantic features
from the utterances, and uses dynamic routing to
aggregate semantic features into intent represen-
tations for intent detection. We use this capsule-
based model for intent detection only.

We also compare our proposed model
CAPSULE-NLU with existing commercial
natural language understanding services, includ-
ing api.ai (Now called DialogFlow)2, Waston
Assistant3, Luis4, wit.ai5, snips.ai6, recast.ai7, and
Amazon Lex8.
Implementation Details The hyperparameters
used for experiments are shown in Table 2.

Dataset DW DH DP DL iterslot iterintent
SNIPS-NLU 1024 512 512 128 2 2
ATIS 1024 512 512 256 3 3

Table 2: Hyperparameter settings.

2https://dialogflow.com/
3https://www.ibm.com/cloud/

watson-assistant/
4https://www.luis.ai/
5https://wit.ai/
6https://snips.ai/
7https://recast.ai/
8https://aws.amazon.com/lex/

https://github.com/snipsco/nlu-benchmark/
https://github.com/snipsco/nlu-benchmark/
https://dialogflow.com/
https://www.ibm.com/cloud/watson-assistant/
https://www.ibm.com/cloud/watson-assistant/
https://www.luis.ai/
https://wit.ai/
https://snips.ai/
https://recast.ai/
https://aws.amazon.com/lex/


5264

Model SNIPS-NLU ATIS
Slot (F1) Intent (Acc) Overall (Acc) Slot (F1) Intent (Acc) Overall (Acc)

CNN TriCRF (Xu and Sarikaya, 2013) - - - 0.944 - -
Joint Seq. (Hakkani-Tür et al., 2016) 0.873 0.969 0.732 0.942 0.926 0.807
Attention BiRNN (Liu and Lane, 2016) 0.878 0.967 0.741 0.942 0.911 0.789
Slot-Gated Full Atten. (Goo et al., 2018) 0.888 0.970 0.755 0.948 0.936 0.822
DR-AGG (Gong et al., 2018) - 0.966 - - 0.914 -
IntentCapsNet (Xia et al., 2018) - 0.974 - - 0.948 -
CAPSULE-NLU 0.918 0.973 0.809 0.952 0.950 0.834
CAPSULE-NLU w/o Intent Detection 0.902 - - 0.948 - -
CAPSULE-NLU w/o Joint Training 0.902 0.977 0.804 0.948 0.847 0.743

Table 3: Slot filling and intention detection results using CAPSULE-NLU on two datasets.

AddToPlaylist BookRestaurant GetWheather PlayMusic RateBook SearchCreativeWork SearchScreeningEvent
0.93
0.94
0.95
0.96
0.97
0.98
0.99
1.00

F1

api.ai
ibm.watson
microsoft.luis
wit.ai
snips.ai
recast.ai
amazon.lex
Capsule-NLU

Figure 3: Stratified 5-fold cross validation for benchmarking with existing NLU services on SNIPS-NLU dataset. Black bars
indicate the standard deviation.

We use the validation data to choose hyperpa-
rameters. For both datasets, we randomly initial-
ize word embeddings using Xavier initializer and
let them train with the model. In the loss func-
tion, the down-weighting coefficient λ is 0.5, mar-
gins m+ and m− are set to 0.8 and 0.2 for all the
existing intents. α is set as 0.1. RMSProp opti-
mizer (Tieleman and Hinton, 2012) is used to min-
imize the loss. To alleviate over-fitting, we add the
dropout to the LSTM layer with a dropout rate of
0.2.

4 Results

Quantitative Evaluation The intent detection re-
sults on two datasets are reported in Table 3, where
the proposed capsule-based model performs con-
sistently better than current learning schemes for
joint slot filling and intent detection, as well as
capsule-based neural network models that only fo-
cuses on intent detection. These results demon-
strate the novelty of the proposed capsule-based
model CAPSULE-NLU in jointly modeling the hi-
erarchical relationships among words, slots and in-
tents via the dynamic routing between capsules.

Also, we benchmark the intent detection perfor-
mance of the proposed model with existing natu-
ral language understanding services9 in Figure 3.

9https://www.slideshare.net/KonstantinSavenkov/nlu-
intent-detection-benchmark-by-intento-august-2017

Since the original data split is not available, we
report the results with stratified 5-fold cross val-
idation. From Figure 3 we can see that the pro-
posed model CAPSULE-NLU is highly competi-
tive with off-the-shelf systems that are available to
use. Note that, our model archieves the perfor-
mance without using pre-trained word represen-
tations: the word embeddings are simply trained
from scratch.
Ablation Study To investigate the effectiveness of
CAPSULE-NLU in joint slot filling and intent de-
tection, we also report ablation test results in Ta-
ble 3. “w/o Intent Detection” is the model without
intent detection: only a dynamic routing is per-
formed between WordCaps and SlotCaps for the
slot filling task, where we minimize Lslot during
training; “w/o Joint Training” adopts a two-stage
training where the model is first trained for slot
filling by minimizing Lslot, and then use the fixed
slot representations to train for the intent detec-
tion task which minimizesLintent. From the lower
part of Table 3 we can see that by using a capsule-
based hierarchical modeling between words and
slots, the model CAPSULE-NLU w/o Intent De-
tection is already able to outperform current alter-
natives on slot filling that adopt a sequential label-
ing schema. The joint training of slot filling and
intent detection is able to give each subtask fur-
ther improvements when the model parameters are
updated jointly.



5265

Visualizing Agreement Values between Cap-
sule Layers Thanks to the dynamic routing-by-
agreement schema, the dynamically learned agree-
ment values between different capsule layers nat-
urally reflect how low-level features are collec-
tively aggregated into high-level ones for each in-
put utterance. In this section, we harness the in-
tepretability of the proposed capsule-based model
via hierarchical modeling and provide case studies
and visualizations.

Between WordCaps and SlotCaps First we
study the agreement value ckt between the t-th
word in the WordCaps and the k-th slot capsule
in SlotCaps. As shown in Figure 4, we observe
that the dynamic routing-by-agreement is able to
converge to an agreement quickly after the first it-
eration (shown in blue bars). It is able to assign
a confident probability assignment close to 0 or 1.
After the second iteration (shown in orange bars),
the model is more certain about the routing deci-
sions: probabilities are more leaning towards 0 or
1 as the model is confident about routing a word in
WordCaps to its most appropriate slot in SlotCaps.

0.0 0.2 0.4 0.6 0.8 1.0
0.0

0.2

0.4

0.6

0.8

1.0

Figure 4: The distribution of all agreement values between
WordCaps and SlotCaps on the test split of SNIPS-NLU
dataset. Blue: the distribution of values after the first iter-
ation. Yellow: the distribution after the second iteration.

However, we do find that when unseen slot
values like new object names emerge in utter-
ances like show me the movie operetta
for the theatre organ with an intent of
SearchCreativeWork, the iterative dynamic
routing process would be even more appealing.
Figure 5 shows the agreement values learned by
dynamic routing-by-agreement. Since the dy-
namic routing-by-agreement is an iterative process
controlled by the variable iterslot, we show the
agreement values after the first iteration in the left
part of Figure 5, and the values after the second
iteration in the right part.

From the left part of Figure 5, we can see that
after the first iteration, the model considers the
word operetta itself alone is likely to be an ob-

sh
ow m

e
th

e
m

ov
ie

op
er

et
ta fo
r

th
e

th
ea

tre
or

ga
n

B-object_type
I-object_type

O
B-object_name
I-object_name
B-timeRange

I-restaurant_type

sh
ow m

e
th

e
m

ov
ie

op
er

et
ta fo
r

th
e

th
ea

tre
or

ga
n

Figure 5: The learned agreement values between WordCaps
(x-axis) and SlotCaps (y-axis). A sample from the test split
of SNIPS-NLU dataset is shown (Left: after the fist routing
iteration. Right: after the second iteration). Due to space
limitations, only part of slots (7/72) are shown on the y-axis.

P
la

yM
us

ic

G
et

W
ea

th
er

B
oo

kR
es

ta
ur

an
t

R
at

eB
oo

k

S
ea

rc
hS

cr
ee

ni
ng

E
ve

nt

S
ea

rc
hC

re
at

iv
eW

or
k

A
dd

To
P

la
yl

is
t

B-object_type

O

B-object_name

I-object_name

I-playlist

P
la

yM
us

ic

G
et

W
ea

th
er

B
oo

kR
es

ta
ur

an
t

R
at

eB
oo

k

S
ea

rc
hS

cr
ee

ni
ng

E
ve

nt

S
ea

rc
hC

re
at

iv
eW

or
k

A
dd

To
P

la
yl

is
t

Figure 6: The learned agreement values between SlotCaps
(y-axis) and IntentCaps (x-axis). Left: after the first iteration.
Right: after the second iteration. The same sample utterance
used in Figure 5 is used here.

ject name, probably because the following word
for is usually a context word being annotated as
O. Thus it tends to route word for to both the
slot O and the slot I-object name. However,
from the right part of Figure 5 we can see that after
the second iteration, the dynamic routing found an
agreement and is more certain to have operetta
for the theatre organ as a whole for the
slot B-object name and I-object name.

Between SlotCaps and IntentCaps Similarly,
we visualize the agreement values between each
slot capsule in SlotCaps and each intent capsule
in IntentCaps. The left part of Figure 6 shows
that after the first iteration, since the model is
not able to correctly recognize operetta for
the theatre organ as a whole, only the
context slot O (correspond to the word show me
the) and B-object name (correspond to the
word operetta) contribute significantly to the
final intent capsule. From the right part of Figure
6, we found that with the word operetta for
the theatre organ being recognized in the
lower capsule, the slots I-object name and
B-object type contribute more to the correct
intent capsule SearchCreativeWork, when
comparing with other routing alternatives to other
intent capsules.



5266

5 Related Works

Intent Detection With recent developments in
deep neural networks, user intent detection mod-
els (Hu et al., 2009; Xu and Sarikaya, 2013; Zhang
et al., 2016; Liu and Lane, 2016; Zhang et al.,
2017; Chen et al., 2016; Xia et al., 2018) are pro-
posed to classify user intents given their diversely
expressed utterances in the natural language. As
a text classification task, the decent performance
on utterance-level intent detection usually relies
on hidden representations that are learned in the
intermediate layers via multiple non-linear trans-
formations.

Recently, various capsule based text classifi-
cation models are proposed that aggregate word-
level features for utterance-level classification via
dynamic routing-by-agreement (Gong et al., 2018;
Zhao et al., 2018; Xia et al., 2018). Among them,
Xia et al. (2018) adopts self-attention to extract in-
termediate semantic features and uses a capsule-
based neural network for intent detection. How-
ever, existing works do not study word-level su-
pervisions for the slot filling task. In this work, we
explicitly model the hierarchical relationship be-
tween words and slots on the word-level, as well as
intents on the utterance-level via dynamic routing-
by-agreement.
Slot Filling Slot filling annotates the utterance
with finer granularity: it associates certain parts
of the utterance, usually named entities, with pre-
defined slot tags. Currently, the slot filling is usu-
ally treated as a sequential labeling task. A re-
current neural network such as Gated Recurrent
Unit (GRU) or Long Short-term Memory Network
(LSTM) is used to learn context-aware word repre-
sentations, and Conditional Random Fields (CRF)
are used to annotate each word based on its slot
type. Recently, Shen et al. (2017); Tan et al. (2017)
introduce the self-attention mechanism for CRF-
free sequential labeling.
Joint Modeling via Sequence Labeling To over-
come the error propagation in the word-level slot
filling task and the utterance-level intent detection
task in a pipeline, joint models are proposed to
solve two tasks simultaneously in a unified frame-
work. Xu and Sarikaya (2013) propose a Con-
volution Neural Network (CNN) based sequential
labeling model for slot filling. The hidden states
corresponding to each word are summed up in a
classification module to predict the utterance in-
tent. A Conditional Random Field module ensures

the best slot tag sequence of the utterance from all
possible tag sequences. Hakkani-Tür et al. (2016)
adopt a Recurrent Neural Network (RNN) for slot
filling and the last hidden state of the RNN is
used to predict the utterance intent. Liu and Lane
(2016) further introduce an RNN based encoder-
decoder model for joint slot filling and intent de-
tection. An attention weighted sum of all encoded
hidden states is used to predict the utterance in-
tent. Some specific mechanisms are designed for
RNNs to explicitly encode the slot from the ut-
terance. For example, Goo et al. (2018) utilize
a slot-gated mechanism as a special gate function
in Long Short-term Memory Network (LSTM) to
improve slot filling by the learned intent context
vector. However, as the sequence becomes longer,
it is risky to simply rely on the gate function
to sequentially summarize and compress all slots
and context information in a single vector (Cheng
et al., 2016).

In this paper, we harness the capsule neural
network to learn a hierarchy of feature detectors
and explicitly model the hierarchical relationships
among word-level slots and utterance-level intent.
Also, instead of doing sequence labeling for slot
filling, we use a dynamic routing-by-agreement
schema between capsule layers to route each word
in the utterance to its most appropriate slot type.
And we further route slot representations, which
are learned dynamically from words, to the most
appropriate intent capsule for intent detection.

6 Conclusions

In this paper, a capsule-based model, namely
CAPSULE-NLU, is introduced to harness the hi-
erarchical relationships among words, slots, and
intents in the utterance for joint slot filling and in-
tent detection. Unlike treating slot filling as a se-
quential prediction problem, the proposed model
assigns each word to its most appropriate slots
in SlotCaps by a dynamic routing-by-agreement
schema. The learned word-level slot representa-
tions are futher aggregated to get the utterance-
level intent representations via dynamic routing-
by-agreement. A re-routing schema is proposed to
further synergize the slot filling performance using
the inferred intent representation. Experiments on
two real-world datasets show the effectiveness of
the proposed models when compared with other
alternatives as well as existing NLU services.



5267

7 Acknowledgments

We thank the reviewers for their valuable com-
ments. This work is supported in part by NSF
through grants IIS-1526499, IIS-1763325, and
CNS-1626432.

References
Yun-Nung Chen, Dilek Hakkani-Tür, Gökhan Tür,

Jianfeng Gao, and Li Deng. 2016. End-to-end mem-
ory networks with knowledge carryover for multi-
turn spoken language understanding. In INTER-
SPEECH, pages 3245–3249.

Jianpeng Cheng, Li Dong, and Mirella Lapata. 2016.
Long short-term memory-networks for machine
reading. In Proceedings of the 2016 Conference on
Empirical Methods in Natural Language Process-
ing, pages 551–561.

Jacob Devlin, Ming-Wei Chang, Kenton Lee, and
Kristina Toutanova. 2018. Bert: Pre-training of deep
bidirectional transformers for language understand-
ing. arXiv preprint arXiv:1810.04805.

Jingjing Gong, Xipeng Qiu, Shaojing Wang, and Xuan-
jing Huang. 2018. Information aggregation via dy-
namic routing for sequence encoding. In Proceed-
ings of the 27th International Conference on Com-
putational Linguistics, pages 2742–2752.

Chih-Wen Goo, Guang Gao, Yun-Kai Hsu, Chih-Li
Huo, Tsung-Chieh Chen, Keng-Wei Hsu, and Yun-
Nung Chen. 2018. Slot-gated modeling for joint slot
filling and intent prediction. In Proceedings of the
2018 Conference of the North American Chapter of
the Association for Computational Linguistics: Hu-
man Language Technologies, Volume 2 (Short Pa-
pers), volume 2, pages 753–757.

Dilek Hakkani-Tür, Gökhan Tür, Asli Celikyilmaz,
Yun-Nung Chen, Jianfeng Gao, Li Deng, and Ye-
Yi Wang. 2016. Multi-domain joint semantic frame
parsing using bi-directional rnn-lstm. In Inter-
speech, pages 715–719.

Geoffrey E Hinton, Alex Krizhevsky, and Sida D
Wang. 2011. Transforming auto-encoders. In Inter-
national Conference on Artificial Neural Networks,
pages 44–51. Springer.

Sepp Hochreiter and Jürgen Schmidhuber. 1997.
Long short-term memory. Neural computation,
9(8):1735–1780.

Jian Hu, Gang Wang, Fred Lochovsky, Jian-tao Sun,
and Zheng Chen. 2009. Understanding user’s query
intent with wikipedia. In WWW, pages 471–480.
ACM.

John D. Lafferty, Andrew McCallum, and Fernando
C. N. Pereira. 2001. Conditional random fields:

Probabilistic models for segmenting and labeling se-
quence data. In ICML, ICML 2001, pages 282–289.

Bing Liu and Ian Lane. 2016. Attention-based recur-
rent neural network models for joint intent detection
and slot filling. Interspeech, pages 685–689.

Matthew E Peters, Mark Neumann, Mohit Iyyer, Matt
Gardner, Christopher Clark, Kenton Lee, and Luke
Zettlemoyer. 2018. Deep contextualized word rep-
resentations. arXiv preprint arXiv:1802.05365.

Sara Sabour, Nicholas Frosst, and Geoffrey E Hinton.
2017. Dynamic routing between capsules. In NIPS,
pages 3859–3869.

Tao Shen, Tianyi Zhou, Guodong Long, Jing Jiang,
Shirui Pan, and Chengqi Zhang. 2017. Disan:
Directional self-attention network for rnn/cnn-
free language understanding. arXiv preprint
arXiv:1709.04696.

Zhixing Tan, Mingxuan Wang, Jun Xie, Yidong
Chen, and Xiaodong Shi. 2017. Deep semantic
role labeling with self-attention. arXiv preprint
arXiv:1712.01586.

Tijmen Tieleman and Geoffrey Hinton. 2012. Lecture
6.5-rmsprop: Divide the gradient by a running av-
erage of its recent magnitude. COURSERA: Neural
networks for machine learning, 4(2):26–31.

Gokhan Tur, Dilek Hakkani-Tür, and Larry Heck.
2010. What is left to be understood in atis? In Spo-
ken Language Technology Workshop (SLT), 2010
IEEE, pages 19–24. IEEE.

Congying Xia, Chenwei Zhang, Xiaohui Yan,
Yi Chang, and Philip S Yu. 2018. Zero-shot user
intent detection via capsule neural networks. In
Proceedings of the 2018 Conference on Empirical
Methods in Natural Language Processing, pages
3090–3099.

Puyang Xu and Ruhi Sarikaya. 2013. Convolutional
neural network based triangular crf for joint intent
detection and slot filling. In ASRU, pages 78–83.
IEEE.

Chenwei Zhang, Nan Du, Wei Fan, Yaliang Li, Chun-
Ta Lu, and Philip S Yu. 2017. Bringing semantic
structures to user intent detection in online medical
queries. In IEEE Big Data, pages 1019–1026.

Chenwei Zhang, Wei Fan, Nan Du, and Philip S Yu.
2016. Mining user intentions from medical queries:
A neural network based heterogeneous jointly mod-
eling approach. In WWW, pages 1373–1384.

Wei Zhao, Jianbo Ye, Min Yang, Zeyang Lei, Suofei
Zhang, and Zhou Zhao. 2018. Investigating capsule
networks with dynamic routing for text classifica-
tion. arXiv preprint arXiv:1804.00538.


