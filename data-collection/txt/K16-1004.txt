



















































Semi-supervised Clustering for Short Text via Deep Representation Learning


Proceedings of the 20th SIGNLL Conference on Computational Natural Language Learning (CoNLL), pages 31–39,
Berlin, Germany, August 7-12, 2016. c©2016 Association for Computational Linguistics

Semi-supervised Clustering for Short Text via Deep Representation
Learning

Zhiguo Wang and Haitao Mi and Abraham Ittycheriah
IBM T.J. Watson Research Center

Yorktown Heights, NY, USA
{zhigwang, hmi, abei}@us.ibm.com

Abstract

In this work, we propose a semi-
supervised method for short text clus-
tering, where we represent texts as dis-
tributed vectors with neural networks, and
use a small amount of labeled data to
specify our intention for clustering. We
design a novel objective to combine the
representation learning process and the k-
means clustering process together, and op-
timize the objective with both labeled data
and unlabeled data iteratively until conver-
gence through three steps: (1) assign each
short text to its nearest centroid based on
its representation from the current neural
networks; (2) re-estimate the cluster cen-
troids based on cluster assignments from
step (1); (3) update neural networks ac-
cording to the objective by keeping cen-
troids and cluster assignments fixed. Ex-
perimental results on four datasets show
that our method works significantly better
than several other text clustering methods.

1 Introduction

Text clustering is a fundamental problem in text
mining and information retrieval. Its task is to
group similar texts together such that texts within
a cluster are more similar to texts in other clus-
ters. Usually, a text is represented as a bag-of-
words or term frequency-inverse document fre-
quency (TF-IDF) vector, and then the k-means al-
gorithm (MacQueen, 1967) is performed to parti-
tion a set of texts into homogeneous groups.

However, when dealing with short texts, the
characteristics of short text and clustering task
raise several issues for the conventional unsuper-
vised clustering algorithms. First, the number of
uniqe words in each short text is small, as a re-

(a) What’s the color of apples?
(b) When will this apple be ripe?
(c) Do you like apples?
(d) What’s the color of oranges?
(e) When will this orange be ripe?
(f) Do you like oranges?

Table 1: Examples for short text clustering.

sult, the lexcical sparsity issue usually leads to
poor clustering quality (Dhillon and Guan, 2003).
Second, for a specific short text clustering task,
we have prior knowledge or particular intentions
before clustering, while fully unsupervised ap-
proaches may learn some classes the other way
around. Take the sentences in Table 1 for exam-
ple, those sentences can be clustered into different
partitions based on different intentions: apple {a,
b, c} and orange {d, e, f} with a fruit type inten-
tion, or what-question {a, d}, when-question {b,
e}, and yes/no-question cluster {c, f} with a ques-
tion type intension.

To address the lexical sparity issue, one direc-
tion is to enrich text representations by extracting
features and relations from Wikipedia (Banerjee et
al., 2007) or an ontology (Fodeh et al., 2011). But
this approach requires the annotated knowledge,
which is also language dependent. So the other
direction, which directly encode texts into dis-
tributed vectors with neural networks (Hinton and
Salakhutdinov, 2006; Xu et al., 2015), becomes
more interesting. To tackle the second problem,
semi-supervised approaches (e.g. (Bilenko et al.,
2004; Davidson and Basu, 2007; Bair, 2013)) have
gained significant popularity in the past decades.
Our question is can we have a unified model to in-
tegrate neural networks into the semi-supervised
framework?

In this paper, we propose a unified framework
for the short text clustering task. We employ a

31



deep neural network model to represent short sen-
tences, and integrate it into a semi-supervised al-
gorithm. Concretely, we extend the objective in
the classical unsupervised k-means algorithm by
adding a penalty term from labeled data. Thus, the
new objective covers three key groups of parame-
ters: centroids of clusters, the cluster assignment
for each text, and the parameters within deep neu-
ral networks. In the training procedure, we start
from random initialization of centroids and neu-
ral networks, and then optimize the objective iter-
atively through three steps until converge:

(1) assign each short text to its nearest centroid
based on its representation from the current
neural networks;

(2) re-estimate cluster centroids based on cluster
assignments from step (1);

(3) update neural networks according to the ob-
jective by keeping centroids and cluster as-
signments fixed.

Experimental results on four different datasets
show that our method achieves significant im-
provements over several other text clustering
methods.

In following parts, we first describe our neu-
ral network models for text representation (Sec-
tion 2). Then we introduce our semi-supervised
clustering method and the learning algorithm
(Section 3). Finally, we evaluate our method on
four different datasets (Section 4).

2 Representation Learning for Short
Texts

We represent each word with a dense vector w, so
that a short text s is first represented as a matrix
S = [w1, ..., w|s|], which is a concatenation of all
vectors of w in s, |s| is the length of s. Then we
design two different types of neural networks to
ingest the word vector sequence S: the convolu-
tional neural networks (CNN) and the long short-
term memory (LSTM). More formally, we define
the presentation function as x = f(s), where x
represents the vector of the text s. We test two
encoding functions (CNN and LSTM) in our ex-
periments.

Inspired from Kim (2014), our CNN model
views the sequence of word vectors as a matrix,
and applies two sequential operations: convolution
and max-pooling. Then, a fully connected layer is

…	  …	  

…	  …	  

convolution operation 

max-pooling operation 

fully connected layer  

Figure 1: CNN for text representation learning.

LSTM LSTM LSTM ……	  

w1 w2 wn 

Mean 

Figure 2: LSTM for text representation learning.

employed to convert the final representation vector
into a fixed size. Figure 1 gives the diagram of the
CNN model. In the convolution operation, we de-
fine a list of filters {wo}, where the shape of each
filter is d × h, d is the dimension of word vectors
and h is the window size. Each filter is applied to a
patch (a window size h of vectors) of S, and gen-
erates a feature. We apply this filter to all possible
patches in S, and produce a series of features. The
number of features depends on the shape of the
filter wo and the length of the input short text. To
deal with variable feature size, we perform a max-
pooling operation over all the features to select the
maximum value. Therefore, after the two opera-
tions, each filter generates only one feature. We
define several filters by varying the window size
and the initial values. Thus, a vector of features is
captured after the max-pooling operation, and the
feature dimension is equal to the number of filters.

Figure 2 gives the diagram of our LSTM model.
We implement the standard LSTM block de-
scribed in Graves (2012). Each word vector is
fed into the LSTM model sequentially, and the

32



mean of the hidden states over the entire sentence
is taken as the final representation vector.

3 Semi-supervised Clustering for Short
Texts

3.1 Revisiting K-means Clustering
Given a set of texts {s1, s2, ..., sN}, we repre-
sent them as a set of data points {x1, x2, ..., xN},
where xi can be a bag-of-words or TF-IDF vector
in traditional approaches, or a dense vector in Sec-
tion 2. The task of text clustering is to partition
the data set into some number K of clusters, such
that the sum of the squared distance of each data
point to its closest cluster centroid is minimized.
For each data point xn, we define a set of binary
variables rnk ∈ {0, 1}, where k ∈ {1, ...,K} de-
scribing which of the K clusters xn is assigned to.
So that if xn is assigned to cluster k, then rnk = 1,
and rnj = 0 for j 6= k. Let’s define µk as the cen-
troid of the k-th cluster. We can then formulate the
objective function as

Junsup =
N∑

n=1

K∑
k=1

rnk‖xn − µk‖2 (1)

Our goal is the find the values of {rnk} and {µk}
so as to minimize Junsup.

The k-means algorithm optimizes Junsup
through the gradient descent approach, and results
in an iterative procedure (Bishop, 2006). Each it-
eration involves two steps: E-step and M-step. In
the E-step, the algorithm minimizes Junsup with
respect to {rnk} by keeping {µk} fixed. Junsup is
a linear function for {rnk}, so we can optimize for
each data point separately by simply assigning the
n-th data point to the closest cluster centroid. In
the M-step, the algorithm minimizes Junsup with
respect to {µk} by keeping {rnk} fixed. Junsup is
a quadratic function of {µk}, and it can be mini-
mized by setting its derivative with respect to {µk}
to zero.

∂Junsup
∂µk

= 2
N∑

n=1

rnk(xn − µk) = 0 (2)

Then, we can easily solve {µk} as

µk =
∑N

n=1 rnkxn∑N
n=1 rnk

(3)

In other words, µk is equal to the mean of all the
data points assigned to cluster k.

3.2 Semi-supervised K-means with Neural
Networks

The classical k-means algorithm only uses unla-
beled data, and solves the clustering problem un-
der the unsupervised learning framework. As al-
ready mentioned, the clustering results may not be
consistent to our intention. In order to acquire use-
ful clustering results, some supervised information
should be introduced into the learning procedure.
To this end, we employ a small amount of labeled
data to guide the clustering process.

Following Section 2, we represent each text s as
a dense vector x via neural networks f(s). Instead
of training the text representation model sepa-
rately, we integrate the training process into the k-
means algorithm, so that both the labeled data and
the unlabeled data can be used for representation
learning and text clustering. Let us denote the la-
beled data set as {(s1, y1), (s2, y2), ..., (sL, yL)},
and the unlabeled data set as {sL+1, sL+2, ..., sN},
where yi is the given label for si. We then define
the objective function as:

Jsemi = α
N∑

n=1

K∑
k=1

rnk‖f(sn)− µk‖2

+ (1− α)
L∑

n=1

{‖f(sn)− µgn‖2+∑
j 6=gn

[l + ‖f(sn)− µgn‖2 − ‖f(sn)− µj‖2]+}

(4)

The objective function contains two terms. The
first term is adapted from the unsupervised k-
means algorithm in Eq. (1), and the second term is
defined to encourage labeled data being clustered
in correlation with the given labels. α ∈ [0, 1]
is used to tune the importance of unlabeled data.
The second term contains two parts. The first part
penalizes large distance between each labeled in-
stance and its correct cluster centroid, where gn =
G(yn) is the cluster ID mapped from the given la-
bel yn, and the mapping function G(·) is imple-
mented with the Hungarian algorithm (Munkres,
1957). The second part is denoted as a hinge loss
with a margin l, where [x]+ = max(x, 0). This
part incurs some loss if the distance to the correct
centroid is not shorter (by the margin l) than dis-
tances to any of incorrect cluster centroids.

There are three groups of parameters in Jsemi:
the cluster assignment of each text {rnk}, the clus-

33



1. Initialize {µk} and f(·).
2. assign cluster: Assign each text to its nearest cluster centroid.
3. estimate centroid: Estimate the cluster centroids based on the cluster assignments from step 2.
4. update parameter: Update parameters in neural networks.
5. Repeat step 2 to 4 until convergence.

Table 2: Pseudocode for semi-supervised clustering

ter centroids {µk}, and the parameters within the
neural network model f(·). Our goal is to find
the values of {rnk}, {µk} and parameters in f(·),
so as to minimize Jsemi. Inspired from the k-
means algorithm, we design an algorithm to suc-
cessively minimize Jsemi with respect to {rnk},
{µk}, and parameters in f(·). Table 2 gives the
corresponding pseudocode. First, we initialize
the cluster centroids {µk} with the k-means++
strategy (Arthur and Vassilvitskii, 2007), and ran-
domly initialize all the parameters in the neural
network model. Then, the algorithm iteratively
goes through three steps (assign cluster, esti-
mate centroid, and update parameter) until Jsemi
converges.

The assign cluster step minimizes Jsemi with
respect to {rnk} by keeping f(·) and {µk} fixed.
Its goal is to assign a cluster ID for each data point.
We can see that the second term in Eq. (4) has no
relation with {rnk}. Thus, we only need to min-
imize the first term by assigning each text to its
nearest cluster centroid, which is identical to the
E-step in the k-means algorithm. In this step, we
also calculate the mappings between the given la-
bels {yi} and the cluster IDs (with the Hungarian
algorithm) based on cluster assignments of all la-
beled data.

The estimate centroid step minimizes Jsemi
with respect to {µk} by keeping {rnk} and f(·)
fixed, which corresponds to the M-step in the k-
means algorithm. It aims to estimate the cluster
centroids {µk} based on the cluster assignments
{rnk} from the assign cluster step. The second
term in Eq. (4) makes each labeled instance in-
volved in the estimating process of cluster cen-
troids. By solving ∂Jsemi/∂µk = 0, we get

µk =
∑N

n=1 αrnkf(sn) +
∑L

n=1wnkf(sn)∑N
n=1 αrnk +

∑L
n=1wnk

(5)

wnk = (1− α)(I ′nk +
∑
j 6=gn

I
′′
nkj −

∑
j 6=gn

I
′′′
nkj)

I
′
nk = δ(k, gn)

I
′′
nkj = δ(k, j) · δ

′
nj

I
′′′
nkj = (1− δ(k, j)) · δ

′
nj

δ
′
nj = δ(l + ‖f(sn)− µgn‖2 − ‖f(sn)− µj‖2 > 0)

(6)

where δ(x1, x2)=1 if x1 is equal to x2, otherwise
δ(x1, x2)=0, and δ(x)=1 if x is true, otherwise
δ(x)=0. The first term in the numerator of Eq. (5)
is the contributions from all data points, and αrnk
is the weight of sn for µk. The second term is ac-
quired from labeled data, and wnk is the weight of
a labeled instance sn for µk.

The update parameter step minimizes Jsemi
with respect to f(·) by keeping {rnk} and {µk}
fixed, which has no counterpart in the k-means al-
gorithm. The main goal is to update parameters
for the text representation model. We take Jsemi
as the loss function, and train neural networks with
the Adam algorithm (Kingma and Ba, 2014).

4 Experiment

4.1 Experimental Setting
We evaluate our method on four short text
datasets. (1) question type is the TREC question
dataset (Li and Roth, 2002), where all the ques-
tions are classified into 6 categories: abbrevia-
tion, description, entity, human, location and nu-
meric. (2) ag news dataset contains short texts
extracted from the AG’s news corpus, where all
the texts are classified into 4 categories: World,
Sports, Business, and Sci/Tech (Zhang and Le-
Cun, 2015). (3) dbpedia is the DBpedia on-
tology dataset, which is constructed by picking
14 non-overlapping classes from DBpedia 2014
(Lehmann et al., 2014). (4) yahoo answer is the
10 topics classification dataset extracted from Ya-
hoo! Answers Comprehensive Questions and An-
swers version 1.0 dataset by Zhang and LeCun

34



dataset class# total# labeled#

question type 6 5,953 595
ag news 4 4,000 400
dbpedia 14 14,000 1,400
yahoo answer 10 10,000 1,000

Table 3: Statistics for the short text datasets

(2015). We use all the 5,952 questions for the
question type dataset. But the other three datasets
contain too many instances (e.g. 1,400,000 in-
stances in yahoo answer). Running clustering ex-
periments on such a large dataset is quite inef-
ficient. Following the same solution in (Xu et
al., 2015), we randomly choose 1,000 samples
for each classes individually for the other three
datasets. Within each dataset, we randomly sam-
ple 10% of the instances as labeled data, and eval-
uate the performance on the remaining 90% in-
stances 1. Table 3 summarizes the statistics of
these datasets.

In all experiments, we set the size of word vec-
tor dimension as d=300 2, and pre-train the word
vectors with the word2vec toolkit (Mikolov et al.,
2013) on the English Gigaword (LDC2011T07).
The number of clusters is set to be the same num-
ber of labels in the dataset. The clustering per-
formance is evaluated with two metrics: Adjusted
Mutual Information (AMI) (Vinh et al., 2009) and
accuracy (ACC) (Amigó et al., 2009). In order to
show the statistical significance, the performance
of each experiment is the average of 10 trials.

4.2 Model Properties

There are several hyper-parameters in our model,
e.g., the output dimension of the text representa-
tion models, and the α in Eq. (4). The choice of
these hyper-parameters may affect the final perfor-
mance. In this subsection, we present some exper-
iments to demonstrate the properties of our model,
and find a good configuration that we use to eval-
uate our final model. All the experiments in this

1We didn’t split dataset into train/dev/test portions which
is commonly used for classification tasks, because it is not
the convention for clustering task. First, the goal of clustering
is to group given instances into clusters, instead of applying
the trained model to new instances. Second, the evaluation
process requires the clustering result of the whole set to map
the clustering labels to the annotated labels.

2We tuned different dimensions for word vectors. When
the size is small (50 or 100), performance drops significantly.
When the size is larger (300, 500 or 1000), the curve flattens
out. To make our model more efficient, we fixed it as 300.

	  
	  
num_dim	  
	  

	  
alpha	  
	  

	  
	  

0.2 

0.25 

0.3 

0.35 

0.4 

0.45 

50 100 300 500 1000 

A
M

I 

CNN 

LSTM 

0 
0.05 
0.1 

0.15 
0.2 

0.25 
0.3 

0.35 
0.4 

0.45 
0.5 

0.00001 0.0001 0.001 0.01 0.1 

A
M

I 

CNN 

LSTM 

0 

0.05 

0.1 

0.15 

0.2 

0.25 

0.3 

0.35 

0.4 

0.45 

0.01 0.02 0.03 0.04 0.05 0.06 0.07 0.08 0.09 0.1 

A
M

I 

CNN 

LSTM 

Figure 3: Influence of the short text representation
model, where the x-axis is the output dimension of
the text representation models.

	  
	  
num_dim	  
	  

	  
alpha	  
	  

	  
	  

0.2 

0.25 

0.3 

0.35 

0.4 

0.45 

50 100 300 500 1000 

A
M

I 

CNN 

LSTM 

0 
0.05 

0.1 
0.15 

0.2 
0.25 

0.3 
0.35 

0.4 
0.45 

0.5 

0.00001 0.0001 0.001 0.01 0.1 

A
M

I 

CNN 

LSTM 

0 

0.05 

0.1 

0.15 

0.2 

0.25 

0.3 

0.35 

0.4 

0.45 

0.01 0.02 0.03 0.04 0.05 0.06 0.07 0.08 0.09 0.1 

A
M

I 

CNN 

LSTM 

Figure 4: Influence of unlabeled data, where the
x-axis is α in Eq. (4).

subsection were performed on the question type
dataset.

First, we evaluated the effectiveness of the out-
put dimension in text representation models. We
switched the dimension size among {50, 100, 300,
500, 1000}, and fixed the other options as: α =
0.5, the filter types in the CNN model includ-
ing {unigram, bigram, trigram} and 500 filters for
each type. Figure 3 presents the AMIs from both
CNN and LSTM models. We found that 100 is the
best output dimension for both CNN and LSTM
models. Therefore, we set the output dimension
as 100 in the following experiments.

Second, we studied the effect of α in Eq. (4),
which tunes the importance of unlabeled data. We
varied α among {0.00001, 0.0001, 0.001, 0.01,
0.1}, and remain the other options as the last ex-
periment. Figure 4 shows the AMIs from both
CNN and LSTM models. We found that the clus-
tering performance is not good when using a very
small α. By increasing the value of α, we ac-
quired progressive improvements, and reached to
the peak point at α=0.01. After that, the perfor-
mance dropped. Therefore, we choose α=0.01 in

35



	  
	  
num_dim	  
	  

	  
alpha	  
	  

	  
	  

0.2 

0.25 

0.3 

0.35 

0.4 

0.45 

50 100 300 500 1000 

A
M

I 

CNN 

LSTM 

0 
0.05 
0.1 

0.15 
0.2 

0.25 
0.3 

0.35 
0.4 

0.45 
0.5 

0.00001 0.0001 0.001 0.01 0.1 

A
M

I 

CNN 

LSTM 

0 

0.05 

0.1 

0.15 

0.2 

0.25 

0.3 

0.35 

0.4 

0.45 

0.01 0.02 0.03 0.04 0.05 0.06 0.07 0.08 0.09 0.1 

A
M

I 

CNN 

LSTM 

Figure 5: Influence of labeled data, where the x-
axis is the ratio of data with given labels.ratio	  

	  

	  
	  
pretrian	  

0 

0.1 

0.2 

0.3 

0.4 

0.5 

0.6 

0.7 

0.8 

AMI 

ACC 

Figure 6: Influence of the pre-training strategy.

the following experiments. This results also indi-
cate that the unlabeled data are useful for the text
representation learning process.

Third, we tested the influence of the size of la-
beled data. We tuned the ratio of labeled instances
from the whole dataset among [1%, 10%], and
kept the other configurations as the previous ex-
periment. The AMIs are shown in Figure 5. We
can see that the more labeled data we use, the bet-
ter performance we get. Therefore, the labeled
data are quite useful for the clustering process.

Fourth, we checked the effect of the pre-training
strategy for our models. We added a softmax layer
on top of our CNN and LSTM models, where the
size of the output layer is equal to the number of
labels in the dataset. We then trained the model
through the classification task using all labeled
data. After this process, we removed the top layer,
and used the remaining parameters to initialize our
CNN and LSTM models. The performance for our
models with and without pre-training strategy are
given in Figure 6. We can see that the pre-training

strategy is quite effective for our models. There-
fore, we use the pre-training strategy in the follow-
ing experiments.

4.3 Comparing with other Models

In this subsection, we compared our method with
some representative systems. We implemented a
series of clustering systems. All of these systems
are based on the k-means algorithm, but they rep-
resent short texts differently:

bow represents each text as a bag-of-words vec-
tor.

tf-idf represents each text as a TF-IDF vector.

average-vec represents each text with the average
of all word vectors within the text.

metric-learn-bow employs the metric learning
method proposed by Weinberger et al.
(2005), and learns to project a bag-of-words
vector into a 300-dimensional vector based
on labeled data.

metric-learn-idf uses the same metric learning
method, and learns to map a TF-IDF vector
into a 300-dimensional vector based on la-
beled data.

metric-learn-ave-vec also uses the metric learn-
ing method, and learns to project an averaged
word vector into a 100-dimensional vector
based on labeled data.

We designed two classifiers (cnn-classifier and
lstm-classifier) by adding a softmax layer on top
of our CNN and LSTM models. We trained these
two classifiers with labeled data, and utilized them
to predict labels for unlabeled data. We also built
two text representation models (“cnn-represent.”
and “lstm-represent.”) by setting parameters of
our CNN and LSTM models with the correspond-
ing parameters in cnn-classifier and lstm-classifier.
Then, we used them to represent short texts into
vectors, and applied the k-means algorithm for
clustering.

Table 4 summarizes the results of all systems
on each dataset, where “semi-cnn” is our semi-
supervised clustering algorithm with the CNN
model, and “semi-lstm” is our semi-supervised
clustering algorithm with the LSTM model. We

36



question type ag news dbpedia yahoo answer
AMI ACC AMI ACC AMI ACC AMI ACC

Unsup.
bow 0.028 0.257 0.029 0.311 0.578 0.546 0.019 0.140
tf-idf 0.031 0.259 0.168 0.449 0.558 0.527 0.023 0.145
average-vec 0.135 0.356 0.457 0.737 0.610 0.619 0.077 0.222

Sup.

metric-learn-bow 0.104 0.380 0.459 0.776 0.808 0.854 0.125 0.329
metric-learn-idf 0.114 0.379 0.443 0.765 0.821 0.876 0.150 0.368
metric-learn-ave-vec 0.304 0.553 0.606 0.851 0.829 0.879 0.221 0.400
cnn-classifier 0.511 0.771 0.554 0.771 0.879 0.938 0.285 0.501
cnn-represent. 0.442 0.618 0.604 0.833 0.864 0.899 0.210 0.334
lstm-classifier 0.482 0.741 0.524 0.763 0.862 0.928 0.283 0.512
lstm-represent. 0.421 0.618 0.535 0.771 0.667 0.706 0.152 0.272

Semisup.
semi-cnn 0.529 0.739 0.662 0.876 0.894 0.945 0.338 0.554
semi-lstm 0.492 0.712 0.599 0.830 0.788 0.802 0.187 0.337

Table 4: Performance of all systems on each dataset.

*

*

*

*
*

*

*

**
*
****

*

*

*

*

*

*

*

*

*

*

*

*
*

*
*

*
****

*
*

*

**
*

*

*

*

*
*

*
**

*

**

*
*

*

*

*
*

*

**
*
**

*
**

*
*

*

**

*

*

*

*

*

*

*

*

*

*

*
*

*
*

*

* *

**

*

*
*

**

*

*

*

*

*

*

*

*

*

*

*

*

*
*

* *
*

*

*

*

*

*

*

*

*

*

*

* *

*
*

*

*

*

*

* *

*
*

*

*

*

*
*

*

*

*
*

*

**

*
*

*

*
*

*

**
*

*

**

*

*

*

*

*
*

*

*
*

*

*

**

*

**

*

*

*

*

*

*

*

*

*

** **

**

*

*

*

*

*

*

*

*

*
*

*

*
*

*

*

*

*

*

*

*

*

*

*

**

*

*
*

*

*

*

*

*

*
*

*

*

*

*

*
*

*

*

*

*

*
*

*

*
*

*

*

**
**

*

*

*

*
*

*

*

*
*

*

*

*

*

*

*

*
*

*

*

*

*

*

*

*
*

*

*

*

*

*

*

*

*

*
*

*

*

*

*

*

*
*

*
*

* *
*

*

*

***

*

*

*

*

*

*

*
*

*

*

*

*

*

*

*

*

*

*
*

**

*

*

*

*

* *

*

*

*

*

*

*

*

*

*

*
*

*

*

*

*

*

*

*

*

*

*

*

*

*

*

*

*

*

*

*

*

*

*

*

**
*

*

*

*

*

*

*

*

**

*

*

*

*

*
*

*
*

*

*

*

*
*

*

*
*

*
*

*

*

*
*

*

*

*

**

**

*
*

*

* *

*

*

*

*

*

*

*

*

*

*

*

*
* *

*

*

**

*

*

*

*

**

*
*

*

*

*

*

*

*

*

*
*

*

*

*

*

*
*

*

*

*

*
*

*

*

*
* *

*
*

*
* *

*

*

*

*

*

*
*

*

*

*

*

*

*

*

*
*
* *

*

*

*

*

*

*

*

*

* **

*

*
*

*

*

*

*

**

*

*

*

*
*

*

*
*

*

*

*

* *

*
*

*

**
*

*

*

**

*

*

*

*

*

*
*

*

*

*
*

*

*

*

**
*

* *
*

* **

*

*

*

*

* *
*

*

*

*
*

*
*

*
*

*

*

*

*

*

*

*

*

*

*

*

*

*

* *

*

*

*
*

*

*

*

*

*

*
*

*

*
*

*
*

*
*

*
*

*

*

*

**

*

*
*

*

*

*

*

*
*

*
*

*

*

*

*

*

**

*

*

*

*

*

*

*
*

*

*

*

*

*
*

*
**

*
*

*

*

*

**
*

*

*

*

*
*

*
*

*

*

*

*

*
*

*

*

**

*
*

*

*

*

*

*
*

* *

*

*
*

*

*

*

*
*

*
*

*
*

** *

* *

*

*

*
*

*

*

*

**

*

*

*

*

*

*

*

*

*
*

**

*

**

*
*

*

**

*
**

*

**
*

*
*

*
*

*

** *

*

*

*
*

*

*

*

*

*
*

*
*

*

* *
*

*

*

*

*

*

*

*

*

*

*
*

* *

*

*

*

**

*

*

*
*

*

*

**

*

*

*

*

*
*

*

*

*

*

*
* **

*

*

*

*
*

*

*

**

*
*

*
*

*

*

*

*

*

*

*

*

*

*

*

*

*

*

*

*

**

*
**

*

*

*

*

*

*

**

*

*

* *

*
*

*

*

*

*

*
**

*

*

*

*

*

*

*

*

*

*

*

*

*

* *

*
*

*
*
*

*

*

**

*
*

*

*

*
* *

*

*
*

**
*

*

*
**

*

*

*

**

*

**

*

* *

*

*

*
*

*

* *

*

*

* *

*

**

**

*

*

*

*
*

*

*
*

*

*

** *

*

*

*

*

*

*

*

*

* *
*

*
*

*

*

**
*

*

**

*

*

*
*
*

*

*

*
*

*

*
*

*

*

*

*

*

*

*

*

*

*

*
*

*
*

*

*

* *

* *

*

*

*
*

*
*

*

*

*

*

*

*

*

*

*

*

*

*

*

*

*
*

*

*

*

*

*
*

**

*

*

*

*
*

* *

*

*

*

*

*

** *

*

*

**

*
*

*

*

*
*

**
*

*

*

**

*
*

*

*
*

*

*

*
*

*
* *

*

*

*

* *

*
*

*

*
*

*

*

*

*

*
*

*

**

*

**

*

*

*

*

*

*

*

*

*

*
*

*

* *

*

*

*

*

*
*

*

*
*

*

*

*

*

*

*

*

*

*

* *

* *

*

*

*

*
*

*

*
**

*
*

*

*

*

**
*

*
*

*

*

*

* *

*

*

*

*

*

*

**

*

*
*

*

*
* *

*

*

*

*

*

*

*
*

*

*

*

*

**

*

*

*

*

*

*
*

*
*

*

*
*

*
**

*

*
*

**

*

*

*

*
*

*

*

*

*

*

*
*

*

*

*

*

*

**

*

*

*

*

*

*

*
*

*

*
*

** *
*

*

*

*

*
**

*
**

*

*

*
*

*

*
*

* *

*

*

*

*

*

*

*

*
*

*

*

*

*

*
*

*

*

*

*
*

* *

*

*

*

*

*

*

*

*

*

*

*

**

*

*

*

*

**
*

*

*

*
* *

*

*
*

*

*

*

*

*

*

*
*

*

*

*

*

*

*

*

*
*

*

*

*

*

*
*

*

*

*
*

*

*

*

*
*

**

*

*

*

*
*

** *
*

*

*
*

*

*

*

*

*

*

*

*

*
*

*

*

*

*

*

* *

* *

*

* **
*

* **
*

*
*

*
*

*

*
*

* *

*

*

*

*

* *

*

*

*

*

*

*

*

*

*

*
**

*

*

*
*

*

*
*

*

*

*

*
*

*

* *

*
*

*
* **

*

*

*

*

*

*

*

*
*

*

*

*

*

*

*
*

*

*
**

**

**

*

*

*

*

*

*
*

*

*

*
**

*

* *

*

*

*

*

*

*

*
* ** **

*

*

*

*

*

*

*

*

*

*
*

*
* *

*
*

* *

*

*

*

*

*

*

*

* *

*

*

*

*

*

*

*

*
*

*
*

*

***

*

**

*

* *

*

*

*

*

*
*

*

*

*

*
*

*

*

*

**

*

*

* *

*

*

*
*

*

*

*

*

*

*

*

*
* *

*

*
*

*

*

*

*

*

*
*

*

*

*

*

*

*

*

*

**

*

*

*

*

*
*

*

* *

*

**

*

*

*

*

*
*
*

*

*

*
*

*

*

*

*
*

*

*

*

*
* *

*

*
*

*

*

*

*

*

*

*

*

*

*

*

*

*
*

*

*

*

*
*

*
*

*

*

*

*

*

*

*

* *

*

*

*
*

*

*

*

**

*
**

*

*

*

*

*
*

*

*

*

*

*

*

*

* *

*

*

** *

*

*
**

*

*

*
*

*

*
*

*
*

*

*

*

*
*

*

*

*

**

*

*

*
*

*
* *
*

*

** *

*

*
**

*

*

*

*

*

*

*
*

*

*

*

*

*

*

*

*

*
*

*
*

*

*

** *
*

*

* **
*

*

*

*
*

*

*

*

*

*
*

*
*

*

*

*

*

*

*

*
*

*

*

*

*

*

*
*

* *

*

*

*

*

**
* *

*

*

**

*

**

*

*

*

*

*

*

*

*
*

**

*
*

*

*

*

*

*
*

*

*

**

*

*

*

*

*
*

*
*

*

*

**

*

*

*

*

*

*

*

*

*

*
**

*

*
*

*

*

* *

*

*
*

*

*

*

* *
** *

**

*

*
*

*

**
* *

*

*

***

*

*

*

*

*
*

*

*
*

**** *

* *

* * *

*

*

*
*

*

*

*

**

*

*

*

*

*

*
*

*

**

*

* *
*

*

*

*

*

*

*
**

*

*

*

*

* *
*

*
*

*
*

*

*

*

*

*

*
*

*

*

* *

*

*

*
*

*
*

*
*

*

*
*

* *

*

*

*

*

* **

*

*

*

*

*

*

*

*

*

*

* *
*

*
*

*

*
*

**

*

*

*

*

*

*

*

*

*

*

*

* *

*
* **

*

**

**

*
*

*
*** *

*

*
**

*

*

*

*
*

*

*

*

*
**

*

*

*

*

*

**

* *
**
*

**

*

*
**

*
* *

*

*

*

*

*
*

*

*

*

*

*

*

*

*

*
*

*
*

*

*

*

*

*
*

*
*
*

*

*

*

**
* **

*

*

*

*

*

* *
*

*

* *

*

*

**
*

*
**

**
*

*
*

*
*

*
*

*

**

*
** *

*
*

*
**

*

*

* *
*

*

*
* *

*

*

*

*
*

*

*

**
*

*

*

*

* *
*

*
*

*

*

*
*

**

*
*

**
*

*
*

*
*

*

*
*

*

*

**

*

*

*
*
*

*

*

*
*

*
*

*

*

*

**

*

* *

*

*

*

*

*
*

*

*

*
*

*

*

*
**

*

*

*
*

*

*
**

*

*

**

*
*

*

*

* **

*
* *

*

*

*

*

*

*
*

*

*

*

*
*

*

*

*

*

*

*

*

*

*

*
*

*

*

*

**
** * *

*

**
* ** ** *

**

*

* *

*
*

*

*

*

* *
*

*

*

** *
*

*

*

*
*

*
*

*

*

*

* *
*

*
*

*

*

*

*

*

*

*

*

*

*

*

*

*

*

*

*
*

*
*

*

*

*

*
*

*

**

*

*
*

*
*

*

*

*
*

*

*

*

*

*

*

*

*

*

**

*
*

*

*
*

*
* *

*

*
*

*

*
*

**
*
* *

*

* *

*

*

*
*

**

*

*

**

*

*

*
*

*

*
*

*

*

*

**

*

*

*

*

*

*
*

*

*

*

*

*
*

*

*

*

*

*

*
*

*

*

*
*

**

*

*

*
*

*

*
*

*

**

*

*

*

*

*

*

*

*

*

*

*

*

*

*

*

*

*

*
***

*

*
*

*

* *
*

*
*

*

*

*

*

* *

*
*

*

*

*

*

*

*
*

*

*

*

*

*

*

*

*

*

*

*

*

*

*

*

*

*

*
*

*

*
*
*

*

*

*

* *

** *
*

*

*
*

*

*

*
*

**
*

*

* *

*

*

*

*

*

*

*

*

*

*

*
*

*

*

*

*

*
*

*
*

*

*

*

*
*

** *

*

*
*

**

*

*

*

**

*

**
*

*

*

*

*

*
*

*

*
**
*

*

*

*

*
**

*

* *

* *
*

*
*

*

*

*

*

*

* *

*

* *

*

* *
* *

*

*

*

*
*

* ** * *

*

*
*

*
*

*
*

*

*
*

*

*

* *

* *

*

*

*

*

*

*

**

*

*

*

*

*
*

*

*

*
*

*

*

*
*

**

*
*

*
*

*

*

*
*

*

*

*

** *
*
*

*

*

**

*

*

*
*

*

*

**

*

*
* *

*

*

*
*

*
**

*

*

*

*

* *

*
* **

*

**
*

*

*

*

*

*

* *

*
*

*

*
*

*

*

*
*

*
*

*

*

**

*

*

*

*

*

* *

*
*

*

*

**

**

*

*

*

*

*
*

*

*
*

*
*

*
*

*
*

*

*

*
*

**

*
**

*

*

*

*

* *

*

*

*

*

*

*
*

*

*

*
* **

*

*

*

*
*

*
*

**
**

*
*

*

*

*
*

*
*

**

*

*

*
*

*

*
*

*
*

* *

*

*
*

*

*
*

*

*

*

*

*

*

*
*

* *

**

**

*
*

*

* *

*

*

*

*

**
*
*

*

*

*
*
*

*

*

*
*
*

*
*

*

* **
*

***

*
*

*

*

*
*

*

*
*

*

*

*

*

*
*

*

*
*

*

*

*

*

* *

*

*
*

*

*
*

*

*

*

**
*

*

*
*

*

*

*

*

*

*

*

**

*

*

*

*

*

* *

*

*

*

*

*

*
*

*

*

*

**
*

*
*

**
*

*

*
*

*
*

*

*

*

*

*

*

*

*
*

* *

*

*

*

*

*

*
*

*

*

*

*

*

*

*
*

*

*

*

*

*

*

*

*

*
*

*
*

*
*

*
*

*

*
*

*

*

*

*

*

*

*

*

*
* *

*
**

*

*
*

*

*
*

* *

*

*

*

*

**

*

*
*
*

*
*

*
*

*

*

*

*

**
*

*

*

*

*

*

*

* *

*

*

*

*

*

* *

*

**

*

*

*

*

** *

*

* *

*

*

*

*

*

*

*
*

*
**
*

*

*

*

*
*

*

*

*

*
**

*

*

*

*

*

*

*

*

*

** *

*

*

*

*

*

*

*

*

*
*

*

* *

*

*

*

*

*

*

*
*

*
*

*

*

*

*

*
*

*
*

*

*

* *

*

*
*

*

*

*
*

*

*

*

*

*
*

**

*
*

*
*

*

*
*

**

* *

*

*

*

**

*

*
*

*

**

* **
*

*
*

*
**

*
*

*

*

*
*

*

*
*

*

*

*

*
*

*

*

*

*

*

*

*

*

***

*

*
*

*

*
*

*

*

*

*

*

*

*
*

**

*

*

**

*

*

*

*
*

*

*

*

*
*

*

*

*

*

*
**
*

*
*

*
*

*

*
*

**

**

**
*

*

*

*

*

*

** **

*

*

*

*

** *

*

**
*

* **

*

*
*

*

*

*
*

*

*

*

*

*

*

*

*

*

*

*

*

*

*

*

* *

*

*

*

* *

*
*

*
*

*
*

**

* *

* *

*
*

*

*

*

*
*

*

*

**

*

* *

*

*

*
*

*
*

*
*

*

* *

* *

*

*
*

*

*

*

*

*
*

*

*

*
*

*

*

*
*

*

*
*

*

*
**

*

*

*

*

*

*

*

**

*

*
*

*

*

*

*

*
*

*
*

*

*

*

*

*

**

*

*

*
*

*

*

* *
*

*

*
*

* *

*

*

*

*

*

*

*

*

*

*
*

*

*

*
*

*

*
*

*

*

*

*

*

*

*

*

*

*

*

*

*

*

*

*

*

*

*

*

*

*

*

*

**

*

*
*

*

*

*

** * *

*
**

*
**

*

*

*

*

*

**
***

*

**

**

*
*

**

*
*

*

*

*

*

*

*

*

*

*

*

*

*
*

*

*

**

*
*

*

*

*

*
*

*
*

*
*

*

**

*

*

*

*

*

**

* *

*
*

*

*

*

*

*

*
**
*

*

*

*

*

*

* **

*

*

*

*
*
*

*

*

*

*

*

**

*

* *

*

*

*
**

*

*
*

**

* *
*

*

*

*

*

*

*

*

*
*

**

*
*
*

*

*
*

*

*

*
* *

**

*
*

*

*

*
*

*

* **
*

*

*
*

*

*

*
*

*

*

*
*

*

*

*

*

***
*

*

*

*
*

**

**

*

*

*

*

*

* **

* *

*

*

*

*
*

*
**

*

*

*

*
*

*

*

*

*

*

*

*

*

**

*

*

**
*

*

*

*

*

*

*

*

*

*

*

*

*

*

*

**
*

*

**
*

*

*

*

*

**

*

*

*

*

*

*

**
*
* *
*

*

*

*

* *

** *
*

*

*

*

*

* *

**

*

*
*

*

*

*

*

* *
**

*

*

*

*

*
*

*

*

*

*

*

* *
*

*

*

*
**

*

*

*

*

*

*
*

*
*

*

*

*

*

*

** *
*

*

*

*
*

*

* *

**

*
**

*

*

*

* *

*

*

*

*

*
*

*

*
*

*

*

*

*

*

*

*

*

*

*

**
*

**

**

*

*

*
*

**

*
*

*

*

*

*

*

*

*
*

*

*

*

*

*
*

*

*

*

*

*

*

*

*
* *

*

*
*

*

*
*

*

*

*

*

*
*

*

*
*

*

* *

*

***
*

* **
* *

*

*

*

*

*

* *
*

**

* ** *

*

* **

*

*
*

*

*

*

*
**

*

*

*

*

**

*

*
*

*

*

*

*

*
*

*
*

*

*

*

*
*

*

* *

*

*

*
*

* **
**

*

* *
*

*

*
*

*

*

**

*

*

* *

*

*

* *

*

*

*

*

*

*

*
*

*

**
* *

*

*

*
*

* *

*

* *

*

*

*

*

*

*
*

*
*

**

*

*

*

*

**

**

*

*

* ***

*
*

*

*

*

*
*

*

*

* * *

*

**
*

*

*

*
**

* *

*

*

*
*

*
*

*

*

*

*

*

*

*

*
*

*

*
*

*

*

*
*

*
*

*

*

*

*

*
*

*

*

*

*

*
*

*

*

*

*

*
*

*
*

*

*

*

*

*

**

*

**

* *

*

*

*

*

**

**
*

*
*

*

* *

*

*
**

*

*

*
*

*
*

*
**

*

*
*

*

*
*

*

*
*

*

*
*

*

*

*

*

*

*

* *

*
**
*

*
*

*

*

*
* * *

*

*

*
*

*

*

**

*

*

*

*
*

*

*

* *
* * ***

*

*

*

*
*

**

*

*

*
*

*

*

*

*
**

* *
*

* ** *
*

*

*

*

*

*

*
*

*

*

*

*

*
*

*

*

**
*

*

*
*

*

*

*

*

*

**

*

*
*

*

**
*

*

* *

*

*

*

*

*
*

*

*

*
*

*

*

*

*

*

*

*

*

*

*

*

* **

*

*

*
*

*

***

*

*

*

*

*
**

*
*

**

***
*
**

*

*
*

*

*

*

*

*

*

**

**

*

*

*

*
*

**

*

*

*

*

*

*

*

*

*
*

*

*

*

*
*

*

* *

*

*

*
*

**

*
*

*

*

*

*

*

*

*
*

*

*

*
* *
*

*
* *
*

*

*

*

*

*

*

*

*
*

*
*

*

*

*

*

*

**

*

*
*

*

*
*

**

*

*

*

*

*

** *

*

*

*
* **

*
*

*
*
*

*

*

* *

*

*

*

*

*

**

*

*

*

*

*

*
*

*

*

*
*

*

*
*

*

*

*
*

*

*

*
*

*

*
* *

*

*
*

*

*

*

*

* *

*

*
* *

*
*

*

**

*

**

*

*

* *
*

*

**

*

*

*

* *

*

*

*

*

*

**

*

*

* *

*

*

*

*

*

*

*

*

*

*

*
*

*

*

* *

**
*
*

*

*

*
*

*

*

*
*

*

*
*

*

*

*
*

*
*

*
*

** *

*

*

*

*

*

*

*
*

*
*

*

*
*

*

**
*

*

*

*

*

**

*

*

*

*

*

*

**

*
*

*

**

*

*

*

*

*

*

*

*

*

**

*

*

*

*

*

*

*

* *

*

*

*

*

*
*

*

*

*

*

*
*

*

*

*

*

*

*

*

*
**

*

*

*

*

*

*
*

*

*

*
*

*

*

*

*

*

*
*

*
*

*

*

***
*

*

*

*

*

*

*

*

*

*
**

*
*

*
*

*

*

*

*

*

*

*

*

*

*
*

* *

*

*

*
*

*

*

*
*

*

*

**

*

*
*

*

*

*

*

*

*
*

**

**

*
*

* *
*

*

*
**

*

*
*

*

*

*

* *

*

*
*

*
*

*

*

*

*

*
*

*
*

**
*

*

*

*

*

*

**

*

*

*

*

*

*

*

*

*

*

* *

*

*
**

*

*

*

*

*
** *

*

*

*
*

*
*

*
*

*
*

*

*
**

*

*

* *

*

*
*

*

*

*

*

**
*

*

*

**

*

*

*
*

*

*

*

*

*

*

*

*

*
*

*

*

*

*

*

*
*

*

*

*

*

*

*

*

*

*

*

*

*

*

*
*

*

**

*

*

* * *
*

*

*

*
*

*

*
*

*

*

*

*
*

*

*

*

*

*

*

*

*
**

*

*

*

*

*

*

*

*

**

*
*

*

*

* *
*

*

*
*
*

*
*

*
*
*

*
*

*
*

**

*

*

*

*

*
**

*
*

*

*

*

*
*

*

*

*

*

*

*

*

* *

**

*

*

*

*

*

*
*

*

*

*

*
*

*
*

*

* * *
*

*
*

*** *
*

*

*
*

*

*

(a) metric-learn-ave-vec

*

*

*

*

**

*

**

*

***

**

*

*
*

*

*
*

**

*

*
*

*

**

*

*

* *

*

*

*

*

*

*

* *

*

*

* *

*

*

*

*

**

*

*

*

*

*

*

*

*

*

*

**

*

*

*

*
*
*

*

*

*

*

*

*

*

*
*

*

*

*
**
*

*

***

*
*

*

*

*

*

*

*
*

*

*

*

*

*

*

*

*
*

*

*

**
*

*
*

*

*

*

*
* *

*

*

*

*
*

**

*

*
*

*

*

*

*

*
* *

*

*

*

* *

*

*

* *

* *

*
*

*

*

*

*

*

**
** *

*

*

*

*

*
*

*

*

*

*

*
*

*
*

*

*

*

*
*

*

*
*

*

*

*

**

*
*

*
*

*

*
*

*

**

**

*

*

*

*

*

*

*

*
*

*
*

*

*

*
*

*

*

*
*

*

*
*

***

*

*
*

* *
*

*

*
*

*

*

*

*

*

**

*
*

*

*
*

*

*

*

*
*

*

*

*

*

*

*
* *

*

*

*

*

*

*

*
*

*

*

*

*

*

*
* *

**
*

*

*

*

*

*

*

*

*

*

*

*

*

*

*

*
*

*

*

*

*

*

*

*

*

**

*

*

*

*

*

*

*

*

*

*

*

*

*

*

*

*

* * **

*

*

*

*

*

*

*

*

*

*

*

*

*

*
*

*
*

*
*

*

* *

*

*

** *

*

*

*

**

*

*

*

*

*

**

*

*

*

*

*

*

*

*

*

*

*

*

*

*

*

* *
*

*

* *

*

*

**

*
**

*

**
*

*

*

*

*

**

*

*

*

* *
*

*

*

*

*
*

*

*

**

*
*
*

*

*

*

*

*

*

*

*

*

*

*

*

*
*

*

*

*

*

*

*

*

*

*

*

*
*

*

*

*

*

*

*

*

*

*

*

*

*

*

*

*

*
* *

*

*

*

*

*

*

*

*
*

*

*

*
*

*

*

*

*

*

*

* *
**

*

*

*

*

*

*
* *

* *
*

*

*

*

*

*

*

**

*
*

*

*
*

*

*

*

*

*

*

*

*

*
*

*

*
*

*
*

*

*

*
* *

*

*

*

*
*

*

*
*

*

*

*

*

*

*
*

*
*

*
*

*

*

*

*

*
*

*

*

*

*
*

*

*

*
*

*

*

*

*

*

*

*

*

*

*

*

*

*

*
*

*

*
*

* *

*

*

*

*

* *

*

*

*

**

*
*

*

*
*

*

*

*

*

*

*
*

*

*

*

*

*

*

*

*

*

**

*

*

*

*
*

*

*

*

*

*
*

*

*

*
*

*

*

*

*

**

*

*

*

*

*

*

*

* *

*
* **

* *

*

*

*

*

*

*

* **

*

*

*

*

*
*

*

* *

*

*
*

*

*

*
*

*
*

*

**

*

*

*
*

*
** *

*
*

* *

*

*
*

**

*

*

*

*

*

* *

*

*
*

*

*

*

*

*

*

*

**

*

*

*
**

*

*
*

*

*

*

*

*

*

*

*

*
*

*

*

*

*
**

*

*

*

*

*

*
*

*

*
*

*

*

*

**

*

*

*

*

*

*

*

*

*
*

*
*

*

*

*

*

*

*

*

*

* * *

**

*

*
*

*

**

*

*
*

*

*

*

**

*

*

*

*

*

*

*

*
*

*

*

*

**

*

*

*

*

*
*

*

*

*

*
*

**

*

*

*

*
*

*
*

*

**

*

*

*
*
*

** **

*

**
*

*

*

*

*

*

***

**

**

*

*

*

*

*

*

*

*

*

*

**
*

**

*

*

*

*

*
*

*

*

*

*

*

*

*

*

*

* *

*

*

*

*

*

*
*

*

*
* *

*

*
*

* * **

**
*

*

*

*

*

*

*
**

*

*

*

**

* *

*

*

*

*

*

*
*

*

*

*

*

*

*

*

*
*

*

**
*

**

*

*
*

*

*

*

*

*
* *

*

*
*

*

* *

**

*

*

*

*

*
*

*
*

*

*
*

*

*

**

*

*

*

*

*

*

*
*

*

*

**
*

*

*

*
*

*
*

*

*

*

*

*

**

*

*

*

*

*

*

*

*

*

*

*

*

*

*

*

*

*

*

*

*
*

* *
*

* *

*

*

*

*

* *

*

**

*
*

**

*

*

*

*

*

*

*

*

*
*

*
*

*
* *

*

*

*

**

* *
*

*
**

*

*

*

*
*

*

*
*

**

*

**

*
*

*
*

*

*

*

*

*

*

*

*
*

*

*

*

*

*
*

**

*

*

**

*

*

*

**

*

*

*

*

*

*
* * *

*
*

*

* *
*

*
*

*

*
*

*

*

*
*

*

*
*

*

*
*

*
* *
*

*

*

*

*

*

*
*

*
*

*

*

*

*

* *

*

* *
*

*

*

*

*

*
*

*

*

*

*
*

*

**

*

**

*

* *
*

*

*

*

*

*

*

*

*
*

* *

*

*

*

* *

*

**

*

* *

*

*
*

*

*

*
* *

*
*

* *
*

*

*

*
*

*

*

**

*
**

*

*
*

*

*

*

*
*

*
*

*

*

**

*

*

*

*

*

***

*

*

*

*

*

*
*

*
*

*

**

*

*

*

**

*

*

*
*

*

*

*

*

*

*
*

*

*

*

*

**

*

*

*

*

*

*

*

*

*

*

*
*

*

*

*

*
* *

*

*
*

*

* *

*

*
**

*

*

*

*

*

*

* *

*
*

*
*

*
*

*

*
*

*

*

*

* *

*

*

**

*
*

*

*

*
*

*

*

*

* *

*

* *
*

*

*

*

*

*
*

*
*

*

*

*

*** *

*

*

*

*

*

*

*

*

*

*
*

**
*

*

*

***
*

*

*
*

*

*

*

*

*

*

*
*

*

*

*

*

*
*

**

*

*

*
*

*

*
*

*

*
* **

*

*

**

*

*
**

*
*

*

*

*

*
*

**

*
*

*

*

*

*

*
*

*

*

*

*

* *
*

*

* *

*

*

*

*

*

**

*
*

* *
*

*

*

*
*

** *

*

*

*

*
*

*

*

*
*

*
*

*
*

*

*

* *

*

*

*

*
*

**

*

*

*

*

*

*
*

*
*

*

*
****

*

*

*
*

*

*

**
* *

*

*

**

*

*

*

*

*
*

*

*

*

*

*

*

*

*

* *

*

**

*

*
**

*

*

*

*

*

*
*

*
*

*

*

*

*

*

*

*

* *

* *

*

*

*
*

*

*
*

*
**

*

*

*
**

*
*

*

*

*

*

*

*

*

*

*

*

*

**

*

*

*
*

*

*

*

*
*

* *

*

*

*
*

*

*

*

*

*
*

*

*

*

**

*

*
*

*

*

*

*

*

*

* *

* *

*

**

*

*

*

*

*

*

*
*

*

*

*

*
*

*

*

*

*

*
* *

*

*

*
*

*

* *

*

*

*

*

*

*

*

*

*

* *

*

**
*

*
** *

*

*

*

*

*

*
*

*
*

*

*

*

*
*

*

*
*

*

*

*

*
*

* **

*

*

*

*

*

* **
* *

*

*
*

*
* *

*

*

*

*

** *

** *

*

*

*

**

*

*

*

*

*

*

** *

*

*

*

**
* **

* *
*

*

*

*
*

*

*

*

*

*
* *

**

*

*

*

*
*

*

*

*

*

*

*

**

*

*

*

*

*

*

*

**

*
*

*

*

* *

*

** *

*

**
*

*
*

*

*

*

*

*
*

*

* *

*

*

*

*

*

*

*

* * *

**

*

*

*

*

*

*

*
*

*

*

*

*

*

*

*
*

*

*

*

*

*

*
*

*

*

*

*

*

*

*

**

***

*

*

*

*

*

*

*

*

*

*

* *

*

*

*

*

*

*

*

*
*

*

*

*

*

* *

*

*

*

* ***

*

*
*

*

*

*
*

*
* *

*

*
*

**
*
*

*

*

**
*

*

*
*

*

*

*

**

*

*
*

*

**

*

*

*

* *

*
*

*
*

*

*

*

*

*

*
*

*

*

*

*

*

** *

*
**

*

*

*

*

*
*

*

*
*

*

*

*

*

*

*

*
*

*

*

**

*

*

*

*
*

*

*

*

**

*

*

*

**

*
*

*

*

*

*

**

**

*

*
*

*

*

***
*

*
*

*

*

*

*

*
*

*

**
*

*
*

*

**

*

* *
*

*

*

*
*

*
* *

**

*

*

*
*

*

*

*
**

*
* *

*
*

*
*

*

* **
*

**

*

**

*

*

*
*

*

*

*

*

* **

*

*

*

*

*

*

*

*

*

*
*

*
*

*
*

*

**

**

*

**

*

*

*

*

*

*

* *

*

*

*

*

*
*

**

*

*

*

*

*
*

* **
**

*

*

*

*
*

*
*

*

*
**

*

*

*
*

*
*

*
*

*
*

*

*

*

*

*
*

*

*

*

*

*
*

*

*
*

*

*

*
*

*

*

*

*

*

*

*

*

*
*

* *

*

*

*
**

*

*
*

*
*

*

*

*

*

*

*

** **

*

*

*

*

*

*

*

*
*

*

*

*

*
*

*

*

*

*

*

*
*

*

*

*

*

* *
*

*

*

*

*
**

*

*

*

*
*

* *
*

*
*

*

* *

*

*
*

*
*

**
*

*

*

**

*

*

*

*

*
*

*
*

*

**

*

*

*

*

*
*

*

*

*
*

*

*

*

*

*
*

*

*

*

*

*

*

*
* *

*
*

*

*

*

**

*

*

*

* *

*

*

*
*

* *

*

*

**
*

**

*

*
*

*
*

*

*
*

*

*
*

***

*

*
*

**
*

*

*

*

*

*

*

*
*

*
**

*

*

*
*

**

**
*

*

*

*

*

*

*

*

*

*

**

* *

*
*

*

*
*

*

*
*

*

*

*

*

*

* *

*

*
* *
*

*

*

**

*

*

*

*
*

*

*
**

*

* **

* **
**

*

*

**
*

*
*

* *

*

*

*

*

*

*

*

*

*

*
*

*

*

*

*

*

*
* *

*
** *

**

*

*
*

*

**

*
*

*
*

*

*

*

*
*

*
**

*

*

*
*

*

*

*

* *

*
*

*

*

*

* *

* *

*

*

*

*
*

*

*

*

*

*
*

*

**

*

*

*

*

*

*

*

* *

*

*

*
*

*

*

*

* *
*

*

*
**

*

*

*

*
*

*

*

*

*

*

*
* ** *

*
*

*

*
* **

*

* *
*

*

* *
*

* *
*

*

*

*

*

* *

*

*
*

**
**

*

*

* *

*
*

*

*
*

**
*

*

*

*
*

*

*

*

*

*
*

*

** * **
*

*

*
* *

*

*
**

*

**

*

*

*

*

*

*

*

*

*

*

*

**

*
*

* *

*
*

*

*

*

*

**

*

*
*

*

*

* **

*

*

* *
*

*

*
*

**

*

*
*

*

*

* **

*

*

*
**

**

**

*
*

* *

* *

*

*

**
*

*

*

*

*

**
*

*
*

*

*
**

*

*
*

*

*

*

*

* *

*

*

*

*
*

*
**

*

*

*
*

**

*
* *

*

*

*

*

*

*

*

*

* *
*

*
*

**

*

*

*
*

** *
*

*

*

*

*

*

**

*
*

*

* *

*

*

**

*
*

*

*

*

*
*

*

*
**

* **
*

* *

*

*

*

*

* *

*

*
*

*

*

*

*

*
*

*

* *
*

*

*

*

*

*
*

*

*
*

*

*

* *

*

*

*

*

*

*
*

*
**

*

*

*

*

*

*

*

*

* **

*

*
*

*

*
*

*
*

*
*

*

*
*

*
*

*

*
*

*

*

* *

*
*

*

*

*

*

*

*

*
*

*
*

*

*

*
** **

*

*
*

*
*

*

* **

*

*

**
*

*
*

*

*

*

*

*

*

*

*

*

*

*

*

* *

*

*

*

*

*

*
*

*
*

* *
*

*

*
*

*
*
*

*
*

*

*

*

* *
*

*
*

*

*

*
*

*

*

*

*

*

*

*

*

**

**

*

*

*

*
*

*
*

*

*
*

*

**

*

*

*

* *

*

*
*

*

*
*

*

*
*

*
*

*

*
* * *

* *
*

*

*

*

* *
**

*

*

*

*
*

* **

*

*
*

*
*

* **

*

*
*

*

*

*

** *
*

*

*

*
*

*
*

*

* *

*

*** *
*

**

*

*
*

*
*
*

**
*

*

*

*

*

* *

* *
*

*

*

*
*

*

*

**

* *

*

*

*

*

*
*

* *
**

*
*

*

**
*

*

*

*

*
*

*
*

*

*

*
**

*

*

*

*
*

*
* * *

*

*
*

*

*

*

*
*

*
*

*

**

*

*

*

*

*

*

*

*
*

*
*

*

*
*

*

*

*

*

*
*

*
*

*

*

* *

*
*

*

*

*

**

* *

*

*

*

*

*

*

*

**
*

*

*

*

* **
*

*

*

*

*
*

*

*

*

*

*
*

*
** *

* *
*

*

** ** *

*

**

** *
** *

*
*

*
*

*
*

*
*

*

*
*

*

*
*

*

**

*

***
*

*

*

*

*

*

* *

*

**

*

*

*
*

*
*
*

*

*

*

*

*

*

*

*

**

*

*

*
*

*

**
*

*

**

*

*

*

*
**

*
*

*

*

* *
*

*
*

*

*
*

*

*

*

*

***
**

*

*

*
*

*

*

*

*

**

* *

*

*

**

* *

*

*

*

*

*

*

*

*

*

*

*
*

*

*
*

*

*

*
*

*
*

*
*

*

*
* * *

*

*
*

*

*

*

*

*

*

*

*
*
*
*** *

*

*

*

*
*

**

*

** *

*
*

*

*
*

*

*
*

*

*
*

*

* *
*

*

* *

*

*

*

*

*

*

*

*

*
*

* **

** *
*

*

*

**
*

*

**

*

*
*

*

*

*

*

**

*

*

* *

*

*

*

*

*

*

*
*

*

*

*

*

*

*

**

*

*
*
*

*

*

*

*

*

*

*

**

*

*

**
*

*

*

*

*
*

*

*

*

*

*

*

*
*

*
*

*

*

*

*
*

*

*
*

*

*

*

*

* *
*

*

*

*
*

*

*

*

*

*

** *

**

**

**

*

*

*
*

*
***

**

*

*

*

**
*

*

*
*

*

***

*

*
*

*

*
*

**

*

*

*

*

*

*

*

*

*

*

*

**

*

*

*

*
*

**

*

**

*

*

*

*
** *

*
** *

*

*
*

*
*

*

*

*
*

*

*

*

* **

*

*

*

*

****

*
*

*
**

*

*

*

*
*

*

*
* *

*
*

*

*

*
*

*

*

*

*

*

*

*

*

*

*

*

*
*

*

**

*

*
*

**

*

*

*

*

*

*

*
*

*

*

*

*

*

*

*

* **

*

*

*

* *

*

*

*

*
**

*

*

*

*

*

*

*

*

*
*

*
*

*

*

*

*

**

*

*

*

* *
*

*
*

*

*

*

*
*

*
*

*
*

*
*

*

*

*
*

* *

*

*

*
*

*

**

*
**

*

*

*

*

*

*

* *
*

*

**
*

* *
*

*

*

*

*

*

*

*

*

*
* *

*
*

*
*

*
*

**

* *

*

*

*

**

*

*

*

*
*

*

* *

**

*

*

*
*

*

*
*

*

*

*

* *

*

*

*

*
*

*

*

*

*
*

*

*

*

*

*

*

*

*

*

*

*

*

*

*

*

*
*

*

*
*

*

*
*

*

*

* *

**

*

*

*

*

* *
*

*

*

*

*
*

**

*

*
*

*

*

*
*

*

*

*
*

*

*

* *

*
*

*

*
*

*

*

*

*

*
*

*

*

*
*

*

*

*

*

*

*

*

*

*

*

*
*
* *

*

**

**

*

*
*

*

*
*

**

*

*

*

*
*

*

*

*
*

*

*

*
*

*

*

*

*

*

*

*

*

*

*

*

*

*

*
*

**

*

*

*

*
*

*
*

*

*

*

*

*

*

*

*

**

*

*

**

*

*

*

*

*

*

*

*

*

*

*
*

**

**

*

*

*
*

*

*

*
**

*
*

**

*

**

**
*

*
*

*
*
*

*

**

*

*

*

*

*

*

**
*

*

*

*

*
*

***

*
*

*

**

*

*

*
*

*
*

* *

*

*

*

*

*

*

**
*

*
**

*
*

*

*

*

*

*
*

*

**

*

*

*

*

*
*

*

*

*

*

*

*

*

*

*

*

*
* *

*

*

**

*

*

*

*

*

*
*

*

*

*
*

*

*

*
*

*

*

*

***

*

*

*

*
**

*

*
*

*

*

*

*

*

*

*

*

*

*

**

*

*
*

*

*

*

*
*

*

*

*
*

*

*

*

*

* *

*

*

*

*

**
*

*

*

*

*

**
*

*

*

* *

*

*

* *

*

*

*

* *

*
*

*
*

*
*

*

*

*

*

*

*

*

*

*

*

*

*

*
*

*

*

*

* **

*

*

*

*

*

*

*

**

* *

*

*
*

*
*

*

*

*

*

*

*

*
*

*

*

*

*

*

*

*
*

*

*

*

*

*

*

*

*

*

* *

*

*

*

*

*

*

*
*

*

*

*

*

* *

*

*

*

*

*

* *

*
*

*

**

*
*

*

**

*

*

*

*

* *
*

*
*

*
*

*

*

*
*

*

*
*

*

*
*

*

*
*

*

*
*

*

*

*

*

*
*

*

*

*

*

*
*

*

*

*

*

*

*

*
*

*

*

*

*
*

*

*

*
*

*

* *

***

*
*

*
*

*

** *

*

*

*

*

*

*

* *

*

*
* *

*
***

*

*
*

*

*

*

*

**

*

*

*
**

* *

*

*

*

*

**
*

*

*
*

*

*

*

*

*
*

*

*

*

* *

*

**

*

***

*

*
*

*

*

*

*

*
*

*

*

*

*

*

*

*

*

*

*
*

*

*

*

*

*

*
*

*

*

*

*

*
*

*
*

*

*

* **

*
* *
*

*

*

*

*

**

**

*

*

*

*

*

**

*

*

**
*

**

*

*

*

*

*
*

*

*

* **

*

*

*

*

*

*
*

*

*

*

*

*

*

*

* *

*

*

*

*
*

* *
* *

*

*

*

*

**

*

*

*

*

*

*

*

*

*

*

*

** **

*
*

*
*

*

*

*

*
*

*

*

*

*

**

*

*

*

*

*

* *

*

**
*

*

*

*

*

*

*

*

*

*
*

*

*
*

*

*

*

*

*

**
*

*

**

**
*

*

*
*

*

*

*
*

*

*

*

*
*

*

*

*

*

*

*

*

*

*

*

*

*

*

*
*

*

*
*

*

*

*
*

*
*

*

*

*

*

*

*

*
**

*

*

*

*

*

*

*

*

*

*
*

**

*

*

*

*

*

*

*

*
*

**

*

**
**

*

*

*

*

*

*

*

*

*

*

*
*

*
* *

*

*

*

*

*

*

*

*
*

*

*

*

**
*

*

*

*
**

*

*

*
**

*

*
*

*
*

*

*

*

*

*

*

*

*

*
*

*

*

*

*

*

*

*
*

*

* *

* *
*

*

*
*

*
* *

*

*

*

*

*
*

**

*

***

** *
*

*

*

* *

*

*

*

* *

*

*

*
*

*
*

*

*

*

*
**

*

***
*

*

**

**

* *
*

*

**
*

* *

*

*

*
**

*

*

*

*

*

*

*

*

**
*

*

*
*

*

*

*

*
*

*
*

**
* *

*

*

***
*

*

* **

*

*

*

*

*
*

*
*

*

*

*

*
*

*

* *

*

** *

* *

*

*

*

*

*

*

*

*

*

*

*

*

**
*

*

*

*

*

**

*

*
*

*

*

*

*
*

*

*
*

*

**

*

**

*

*

*

*
*

*

*

*
*

*

*
*

*

*
*

*

*

**

*

* * *

*

*

*

*
*

*

*

*
*

*

*

*

*
*

*

*

*
*

**

*

*
*

(b) cnn-represent.

*

*

**

*

*
*

**

*

**

*

*
*

*

*

*

*
* *

*

*
**

*

*

*

**

*

*

*

*

*

*

*

*

*

*
*

*

*

*

*

*

*
*

**** *

*

*

*

*

*
**

*

*

*

*

*

*

**

*

***

*

*

*

*
*

*
*

*

*

*

*
**

*
*

*

**

*
*

*

**

*

* *
**

*

*

*
*

*

*

* *
* **

*

**

*

**

*
*

*

*

*

*
* *

*

**

*

*

*
*

*

*

* ***
*

*

*
*

*
*

*
*

*

*

**

*

*

*

*

*
**

** *

*
*

*

*

*

*

*

*

*

*

*

*

*

*

*

*
*
* *

*
*

*

*

* *

*
*

*
*

*

*

**

*
*

**

* *

*
*

*

*

*

*

*

*
*

* *

*

*

*

*

*

*

*

*
*

*

*
*

*

*

*

*

*

*

*

*
*

*

**
*

**

*

*
*

*

**
*

*
*

*
*

* *

*
**

*

*

*

*
*

*

*
* *

*

*

*

*

**

*

*

**
** *
*
*

*

*

**
*

*

*

*
*

*

*

*

*

*

*

*

*

*

*

*

* *
*

*

*

** *
*

*

*

*
*

*
*

* *
*

* *

*
** *

*

*

*

*
*

**

*

*
*

*

**
*

*
* *

*

*
**

*

*

*
*

*
* * *

*

**
*

*

*

*
*

* *

**

*

**
* *

*
*

**
* *

*

*

*

*

*
*

*

*

**
*

*
* **

*

*

* * *
*

**
*

*
*

**
*
*

*

**
*** *

*
*

*

*

*
*

*

*

**
*

**

*

*
*

*

** **

*
* *

*

*

*

*
*

*

*

*

*

*

*

*
*

*

*

*

*

*

*
*

*

*

*

*

**

*

*

*
*

*

*
*

*

*

*

*

*

*

* *

***

*

**

*

*

*

*

*

**

*

*

**

*

*
**

*
*

*

*
*

**

*
*

*

*

*

*
* *

*

*

*

*
*

*
*

*

* *

*

*
*

*

*

*

**

*

*

*

* *
**
*

*

*

*

*

**
*

*
*

*
**

*

*

*

*
*

* * *
*

**
*

*

*

**

**

*

* *

*

**
*

*

* **

*

*
*

*
*
*

*

*

*

*
*

*

*

*

*

*

*
*

* *
*

**
*

*
**

*

*
**

* *
* *

*
*

*

*

*

*

*

*

*
*

*

*

* *

* *
*
*

**

*
*

*

*

*

* *
*

*
* **

**

*
**

*

* *
*

**
*

*

*
*

*
*

*

*

**
*

*

*

*

*

**

*

* *

*

*

*

**

*

*
* **

*
*
**

*

*

*

*
*

*

*
*

*

*
*

*

*

*

*
*

*

* *

*

*

*

*

* *
** *

**
*

*

*
*

*

*

*
*

*

*

*
*

*
*

* *

*

*
*

*

*

*
*
*

*

*
**

**

*
*

*

*
*

*

*

*

*
*

*
*

**

*
*

*

**

*

*

*
*

*

*

*
* *

*

*

**
**

*

*

**
*

*

*
*

*

*

*
*

*
*
*

* *
*

*
*

*

**

*

*
*

*

*

*

*

*
*
* *

*

*
*

**

**
**

*

*
**

*

*

*
*

*

*
*

*

*

*

*

*

** *

*

*

*

*
*

* * *

*

*

*

*

***

*

*

**
*

*

*

*

*

*

*

***
*

*

*
*

*

*

*

*

*
*

*

* *

*

*

*
*

*

*

***

*
*

*

*

*
* *

**

*

**
*

*

*

*

*
*

*
*

**
*

*

*

*

*
***

*

*

**

*

*

*
**

**

* * * *
*

*

*

*

*

*
*

*

*

*

*

*

*

*

*
*

*
*

*

*
* *

*

*

*

*
**

*
*

*

*

*
*

*

*

*

*
*

*
* *

*
*

* *

**
*

*
*

*
*

*
*

*

**
*

*

*
*

**

* *

*

*

*
**

*

*
*

*

*

*

*

*

*

*
*
*

*

*

*

*
*

*
**

* *

*

*

**

*

*
* *

**
*

*

*
*

*

*

* *
*

*

*

* *

* * ** * *

*

*

*

*
*

*

*

*

*

*
***

*

*
*

**
*

*

**

*

**

*
* **

*

*
*

*

*

*

*

*

*
*

*

*
*

**

*

*

*

*

*
*

*

*

**
**

*

*

*

*

*

*

*

*

*

*
*

*

*

*

*

*

*

*

*

*

*
* **

*

*

*
*

*

*

*

*

*

**
*

*
*

*

*

*

*

*
*

*

*

*

*
*

*

*

*
* **

*

*

*

*
*

*

*

*

**

*

*

*

*

*

*

**
** *

*

*
*

*

*

* * * *

**

*
*

*
*

*

*

*

*
*

**

*

*

*

*

*
*

*

*

*

*
*

*

*

*

*

*

*

*

*

*

*

*

*

**

*

**
*

*
*

*

**

*

* *

* *
*

*
*

*
*

*

*
*

*

*
* *

*

*

*
*

*

*

*

*

*
*

*
*

*
*

*

*

*

* *

*

*
*

*

*

* *

*

*
**

*

*

*

*

*

*

*

*

**
*

*

*

*

*

*

*

*

*

*

*

*

*

*
*

**

**

*
*

*
**

*

* *
*

*

*

*
*

*

*

* *

*

*

*

* *
*

*

**

*
*

*
*

*

*
*

*
**

*

*

* *

*

*

*

*
*

*

*

*
*

*

*

*

*

*

*

*
* *

*

*

*

*

*

*

*
*

*

*

*

*
* *

*

*

*

*

**

**

**

*

* *
*

*
* *

*
* *

* *
*

*

*

**

*

*

*
*

**

*
*

*

*

**

*

*

*

*

*
*

*
*

* *

*

*

*

*

*
*

* **

*

*

*

*

* *

*

*
*

*

* *

*
* *

*
*

*

*
*

*

*
*

**

*

**
*

*
*

**

**

*
*

*

*

* *

* *

*

*

*
*

*
*

*

*

**

*

*
*

*
*

*

*

* *

* *

*
*

*
*

*

*

*

*
*

**

*

*
*

*

**

*

*

*

*

*

*

*

**

*
*

*

*

*

* *
* **

*

* *

*

*

*

*

***

*

*

*
*
*

*

**
*

*
* *
*

*

* *

*
*

*

*

*

*
*

*

*

*

*

*
*

*

*

*
*

**

* ** *

*

*
*

*
*

*

*
*

*

*

*

*
*

*
*

*

*

*
*

*
* **

*
*

*

*

* ***

*

*

*
*

**

***
*

*

*

**

*
*
*

*
*

*

*

*

*

*
*

*
**

*

* *

*

*

*

*

*
*

*

*

*

*

**

**

*

*

*

*

**
*

*

* *

*
*

**

*

*
*

**

*
*

*

*

*

*

*

**
*

*

*

*
*

*

*

*

*
*

*
* *

*

*
**

*

* *

*

*

*
*

*

*

*

*
*

*

*

*

* * **

*

*
**

* *

*

*

**

*

*

*
*
*

*

** *

*
*

*

*

*

*
*

* *
*

*

*

*
*

**

*

*

*

*
**

*

*

* *

*
**

*

*

*

*

*
*

*

*
* *

*

*
*

*
*

***
*

*
*
*

*

* * * **

*
*

*

*

*

*
*

*
*

*

*

*

*

*
*

*

*

*

*
*

*

*****

**

*

*

*

*

*
*

*

*

*

*

*

**

*

**

*

*

*
*

*

*
*

*

*
*

**
*

*

*

*

*

*

**

*

*

*

*

*

* *
*

*

* *

*

*

*
*

*

*

*

*
*

*
*

*

*

*

*

*
*

*

*

*

*
*

*

*

*

*

*** *

*

*

*
*
*

*

*

*

*
*

*
*

**
*

*

*

*

*

*
*

*
**

*
*

**

*

*

*

* *
*

*
*
**

*

*

*

*
*

* *

*

** *

*

*

* *

*

**
*

*

*

*

*
**

**
*

*

*

*
*

*

*

*

*

*

*

*

*

*
*

*

*

*

*

*
**

*

*

*

*
*

*
*

*

*
* *

*
*

*

**
*

*

*
*

**

*

*

*

* *

*

*

*

*

*

*
*

*
*

*

*

*

*

*

** *

**

*
*
*

*
*

* *
* **

*
**

*

**

**

*
*

**
* *

* *

*

*
** * *

*

**
*

*
*

*

*

***
*

*

*
*

*
*

*

*

*
*

*

**
*

**

*

*

*

*
* **

*
*

*

*
*

*
*

*

*
*

*

*
*

*

*
*

*

*
*

*
*

**

*

*
*

*
*

*

*

*

* *
*

*

*

*

*

*

*

*
* *

*

** **
*

*

* *

*

*
*

*
*

*
*

*

**

*
*

*

*

*
*

*
* *

*

*

*
*

* *
**

*
* *

*

**
*

*

*
**

*
* *

*

*
*

*

*

*

*

*

*

*

*
*

** ** *
*

*

*

*
**

*

**

*
*

*
*

*

*

**

*

*

*

**

*

*
*

*
*

*

**

*

*

* ** **
**

*
*

*
*

*

*

***
*

*

*

* *

**

*

** * *

*
*

*

*
*

* *

**

*

*

**

*
*

*

*
*

*

**
*

*

* *

*

*

*

*

*
*

*

**

***

*
*

*

*
** *

*

*

*
**

*
*

*

*

*

*

*
*

*

* *

*

*

*
**

* *

*

*

*
** *

*

*
*

*

*

*
*
*

**
*

*
*

*

*

*

*

*
*

*

**

*

*
*

*
*

* **

*

*

*

**
*

**
*

*

*

*

*

*
* *

*
*

*
*

* * **

**

*

*

*

*

*

*

*
*

* *

*

*

*

*
*

* *
* *

*

*
*

*
**

*
**

**

*

*

*

*
*

*

*

*

*

*

*

*

**
**

*
**

**

*

*

*

*

*
* *

*
*
*

*

*

*

* * *

*

*

*

*

**

*

***

*

*

*

*

**

*

*
**

*

*
** *

*
* *

* *

*

***

*

*

*
*

** *
*

*
* *

*

*

*
* ***

*

*

*

*
*

*

**

*

* *

***

*
*

*

*

* *
*

* *

*

*
*

*
*

*

*

*

*

*

**
*

*
*

* *
**

*
*

*

*

*

**
*
*

*
*

*

*
**

*

*
**

*

*
*

*

*

*
*

*

* *
*

*

**

*

*
*

*

* * ** *

*
**

*

*

*
**
**

*

*

*

*

*
* * ** *

*

*

**

*

* *
**

*

*
*

*

*
**

*

*

*

*
*

**
*

*
*

*

*
*

* *

*
*

*
*

*
*

*

***
**

*

*

**
**

** *
* **** **

**
*

*
*

*

* *

*

* ***
*

*
*

*
*

*
*

*

* *
* *

*
**

* *
*

**
*

*
*

*
**

*

* *

*

*

**
*

*

*
*

*

*

**

*
*

* *

*

*

* *

*

*

*
*

*
*

*
*

*

*

*

*
**

*
*

*

*
*

*
* **

*
**

*
*

* **
*

*

*
* *

*

*

*

*
*

*
*

*

*

* *

*
*

*
*

*

**
*

*
**

**

*
* *

*

* *

*

*

*
*

**
*

**

*

*

*

*

*

*
*

*

*
*

*

*
* *

*
*

** ***

*
*

*

**
*

** **

*

*

*

*
*

*

* *

*
* **

***
* *

*
*

*

*

*

**

*

*
*

*

*

*

*

*

*
*

* **
*

*

* *
*

*

*

*

*

**

*
*

* *

*

*

*

*
* ** *

*
*

*

*

*

*

*
**

* **

*

*

*

*

*
**

*
* **

*

*

*

*
**

*

*
*
*

*

*
*

* *

*

* *

*
***

*

* *
**

*
*

*

**

*
*

*

*
*

*
*

*

*

*
*

* **

*
*

*

*

*
*

*
*

*

*

*

*
*

*

* *

*

*

** **

*

*

*

*

*

*

*

* *
*

*

* *

*

*

*
*

**
*

**

**
**

*

** *

*

*

*

*
*

*

*

*

*
* *

*

*

*

*

*

*

*

*

*
**

*
*

*

*

*

*

*
*

**

***

**
*

*
*
*
**

*

*
*
*

**

*

*

*
*

*
*

*

*
*

*
*
*

*

*
**

*

*
*

*

*

*

* *

*
**

*
*

*

**
*

*

*
*

*
*

*

*
*

*

*
*

*
*

*

* **
*

*

* **

*

*

*

*

*

*

* *

*

*

* *
*
*

*

*
*

**

*

*

*
***

*
*

*** *
*

*

**
*

*
*

*
*

*

*

*
**

*
* **

*

*

*

*
**
*

*
*

*

***

*

*
**
*

*
*** **

*

*
*

*
*

*

* *

*

*

**
**

*

**
**

*
*

*
*

*

*

*

*

*
*

*

*
*

**
*

*

*

**
*

*

*

*

**

*

*

*

*

*

*

*

*

*

*
**

*

*

*
* *

*

*

*

*

*

*

** **

*

*

***

*

*

*

*

*
*

*
*

* *
*

*

*

*

*

*

*

*

*

**

*
*
* *

**
*

* **

*

*

*

*

*

*

*

*

*

*

*
*

*

* *
*

*

*

*

*

*

**
**

*

**

*
*

*
*
*

*
*
***

* *
*
*

*

*

*

*

*
*

*
*

**

*
**

*
*

*
*

*

*

*

*

*

*

*
*

*

*

*

**

*
**
* *

*
*

* *
*

*

*

*

*

*
**

*

*

**

*

**
*

*

*

*
*

*

*
*

**
*

*

*

*

*

*

*

*

*

*

*

*

*

*

****

*
*

*

*

*

*

*

*

*

*

*

*

*

* *

**
*

**
*

*

*

* *

* *

**

*

*

**
*

*

*

*
*
** *

*

* *
*

* *

*

*

*

***

*

*
*

***

*

* * ***

*

*

*
*

*

*

*
*

*

**
*

*

*

*

*

* *

*

* *

*

*

*
*

*

**
*
*

*

*

*

*

*

*

**

***

*

*
*

*
*

*

*

*

*

*

*
*

*

*

*

*

*

*

*

***

*

*
*

*

*

*

* * * *

*

*

*

*** *
*

*

*

*

*

****

*

*

*
*

*

*
* *

*
**

*

*

*

*

*
*

*

**
*

**

*
*

*
*

*
*

*
*

*

*

*

*
*

**

*

*

*

*
**

*

*

*

*

***

** *

*

*

*
***

*
*
*
**

*
*
*

**

*

*

*

*

*
* **

*

*

*
**

*
*

*
*

*
*

*

*

*

**
*

*

*

* **

*

*
*

*
*

**

*
*

*
*

*

*
*

*

*

*
**

**
***

*

*

*

*
* **

* *

**

*

*

*

*
*

*

*

*

*

*
***

*
*

*

** *
*

*

*

*
*

*

*

*
*

*

*

*

*

*

*

*

*

*

*
*

*

*

*
**

*

* *
*

**

*

*

*

*

*

*

*

* *

*

*
*

*

*
*

*
*

*

*

*
*

*

*
*

*

* *

*

****
*

*

*

*

*
* * **

** *
*

*
*

*
**

*

*

*

*
*

**
*

*

*
***
**

*

*

*

*
**

*

*

*
*

**

*
*

*

*

*

*

*
*

*
*

*
*

*
*

*

*

*

*

*
*

*

*
*

*

*

*
*

*
*

* **

*

*
*

*
*

* *

*
**

*
*

*

*

*

**
*

*

*

* *

*
**

*
*

*

*

*
*

*

*

*
**

*

*

*
**

*

*

**
*

*
*

*

**

** ***
*

*

*
**

** *
*

*

*
*

*

*

*
*
*

*

* *
**

*

**

*

*

*
*

*

*

*

*

*
* *

*

**

*
**

*

*

**
*

* *
*

*

*
* *

*
* *

*
*

*
*

*

*

*
*

*
*

*

*

*

*
*

*

*
*

*
*

**

*

*

*
*

*

*

** *

*

*

**
*

**
*

*

*

****

*

*

*

*

*

*

**
*

*
*

*

*

**
**

** *

*

*

*

* **
*

*

*

**

*

*

*
*

*

*

*

*
*

*

*

*
* *

*
*

*

*

*

*
*

***

*
*

*

*

*
*

*

*
*

*

*
*

*
**

* **
*

*

*

*

**

*

**
* *

*

*

*

*

*

*

*
* *

*

*

*
*

*
*

*

*

*

*

*

*

**

*

*

*

*

***
* * *

*

*
*

*

*
* *

*

* *

* *

**

*

*
*

*

*
**

*
*

*
*

*
*

*
*

*

*
*

**
**

*

*

*

*

*
*
*

*

* ***

*

*

*

* *** *
*

*

* *

*

*

*
* *

*

***

*
*

*

*

*

*

**

*

*

* **

*

**

*

*

* *

*

*

*
*

* ****

**

*

**

*

*
*

*

*

*

*
*

*

*

*

*
*
*

*

**

** *
*

*
*

*

*

*

*

*

*

*

* **
*

*

* *

**

* **

*
*

*

*

*

*

*

*
*

*

*

*

*
*

*

*
**

*

*

*

*

*

*
*

* *

*

*
***

*

*

*
*
* *

*
*

*
* *

**

*
*

* *

*

*
*

*

*

*
* *
*

*

*
*

*

*
*

*

*

*
* * *

* **

*

*

*
*

*

*

*

**

*
* *

*
*

*

*

*

*

* *

*
*

*

*

**

**
* *

* *

*

*

*

*
*

*

* **
*

*

*

*

*
*

*

*

*

**
* *

*

*
*

*
**

*

*

*
*
*

*
*

* * *

*

*

*

*

*
*

* *

**

*
*

*

*
*

***

*

* *

* *
*

*

*

***

*

*
* **

*

*

*
*

*

*
**

** *

*
*

*
*

*

**

*

*

*

*
*

* *

*

*
*

*

*
**

*

*

*
*

*

* *

*

*

*

*

*
*

*

*
*

*

*

* *

*

*

*
**

*
*

*

*

*

*

*
*

**

*

**
*
*

*

*

*

*

*

*

*
*

*

*

*

*

*

*
*

*

**

*

*

*

*

*

*

**
**

**
*

*
*

*

*

*

*

*

*
* **

*

*

*

*

*
*

*

*

*

*

* *

* *
* *

*
*

*

*

* *

** *
*

*
*

* *

*

* **

*
* *

* **
*

*
*

*

*

*
*

*

**
*

*

*

*

**

*

*
*

*

*

*

* *
* *

*
*
*

*

*

*
*

*

*

*

*
*

*
* *

*

*
**

*

*

*

*

*

**
**

*
* *

*

*

*

*

**

*
* *

*
*

*

*

*

*

*

*
*

*
* **

*

*
*

*
*

*

*

*

*

*
*

*
*

*

*
*

*

*

*

**

*

*

*

*

*

*
*

*

*

*
*

*

*

* *

*
*

*

*
*

*

**
*

*

**
*
* *

*

*

*

*

*

*

*

*
* *

*

*

*

*

*

*

* *

*

*

*

*

*

*

*

*

*

*

*

*

*
*

*

*

*
*

*
*

*
*

* *

*

*
*

*
**

*
*

*
***

*

** *

**

*

*

*

**

*
**

*

*

*

* *
*

*

*

* **
**

*

**
*

*

**
*

*

*

*

*

*

*

*

*

*

*

*
*

*

*
*

*
*
**

*

*

*
*

*

(c) semi-lstm

*
**

*

*

*
*

**

*

***

* *

*

*

*

**

*

* *
*

*

**

*

*

*

*

*

*

*

**

*

**

*

*

*

*

**

*

***

**

*

*

*

*
**

*

*

**
**

*

*

*
*

***

* *

** *
*

*
*

*
**

*

*

*

*

*
*

*

*

*

*
*

*

*

*

*

*

*

*

*
*

*
* *

** *
*

*

** **

*

*

*
** * **

*

**
*

*
**

*

*

*
*

*

*

*

*
*

*
*

* **

*

*

* *

*

*

*

*

*
*

**
*

*

**
*

*

*

** *
* **

*

*

*

*

*

*

*

*

*
*

*

*
**

**
*
*

** *
*

*

*

*
*

*

*

*
*

*

*

* *

* *

*
*

*

*

*

*

*

*

*
*

*
*

*

*

*

*

*
*

*

*

*

*

*

*

*
*

*

*
*
*

*

*
*
*

*

*

*

*

*

*

*

**
*

*

*

**

*

*

*

*
*

*

*

**

* *

*
*

*

*

*

**

*

*

*

*

*

*

*

*
*

*

*

*

* *
*

*
*

*
*

*

*

*

*
*

*
**

*

*

*

* **

*

*
*

*

*
*
*

* ** **
*

*

*

*

*

*
*

** **
*

*

*

*

*

*

*

*

* *

*

*
*

*

*

*

*

*
*

*

*

*

*

* *
*

*

*

*

*
*

*
*

*

**
*

*

* **
*

*

* * *
*

*
*

*
* *

*

*

**
*

**
*
*

*

*
* *

*

*
*

* *
*
* *

**

*

*
*

*

*
*

*

*

*

*

*
*

* *

*
*

*
*

*

* **

**
***

*

*

*
*
*

*

*

**

*

*

*
*

*

*

*

*
*

*
*

*
*

*

*

*
*

*

*
*

*

*

*

*

** *

*

*
*

*

*
*

*
*

*
*

*

*

*

*

*
*

**

*

*

*

**

*

*
*

*
*

*
*
*

**

*

*

*
*

*

*

*

*
*

*

*

** ** *
* *

*

*

* *

*

*
*

*
*

*
*

*

*

*

*
*

*
**
*

*

*

*

*

**

* *
* *** * *

**

**

*

*
*

*

*
*

*

*
* *

*

*

*
*

**
* *

**

*

*
*

*

*
*

*

* *
*

*

*

*

*

* *

*

*
*

*

*
*

*

*
*

*

*
*

*
*

*

*

*

**

*
*

*

*

*

*

*

*

*

* * ** *

*

*

*

*

*

**

*

*
*

**

*

**

*

**

*

*

***

*
*

*
*

*

* * **

*

*
*
*

*

*
*

*

* **
*

*

**

*

*
*

**
*

*
*
**

*

*

*

*
*

*

*

*

*

*

**

*

*
*

*

*

*
**

* * *

*

*

*

*

** *

*

*

*

*
*

*
*
*

*

*
* * *

* *
*

*

*
*

*
*

*
*

*

*

*

*

*

*
*
**

*

*

*

*
*

*
*

*

*

*
*

*

*
** * *** *

*
*

*

*** ***
*

*

*
*

*

*
* *

*
*

*
*

*
*

*

*
*

**

*

* *

*
*

*

*

*

*

*

* **

*

*
*

*
*
*

*

*

*
*

*

*

*
**

*

*

*
*

*

*

**
*

*
*

*
*

*

*

*
*

*

*
*

*
*

*

**

** *

*
*

*

*
*

*

*
*
*
*

*

* *
*

*

* *

*
*

*

*
*

***

*
*

*

*

**
*

* **

*

*

*
*

*

*

*
* *

*
*

* *
*

*

*

*
*

*
*
*

*

*

*

*

*

**

*
**

*
*

*
**

*

*
*

*

*

**

*

*
** *

*

*

*

*

* *
**

*
*

*

*
**

*

*

*
*

*

*

*

*
*

*
*

*

*
* * *

**

*

**
*

*
*

*

*

*
*

*

*
*

*

*

*

*

* *
**

*

*
** *

*

*

*

*

*
**

**
*

*

*

*

*

*

*

*
*

*

*
*
* *

*

*

*

*

*

*
*

*

*

*
*

* *
*

*

*
*

*

*
*

*

*

*

*

*

*

*
*

* *

*

*

*
*

*
*

*

* **

*
*

* *
**

**

*
* *
*

*
*

*
*

*
*

*

*

*
*

*

*

*

*

*
*

*
*

**

*

*

*

*

*

*

*
*

*
*

*

*

*

*

*

*

*

*

*

*

*

*

*

**

*
**

*

*
*

*

* **

**

*
**

*
**

*

*

*

*

*

*

*

*

*

*

*

*

**
*

* *

*

*

*
*

*

* *

*

*

*

*
*

*

*

*

*

*

*

*

*

*

*

*
*

*

**

* *

*

*
*

*

*

*

*

*

*

**
*

*

*

*

***
* *

*

*
*

*
* *

* *

*
*

*

*

*

*

*

*

*

*

*

*

*

*

*

*

*

*

*
*
*

*

*
*

*

**

*

*
*

*
*

*

*

*

*

*

*
*

*
*

*

*

*

*

*
* *

*

*

* *
*

*

*

*

*
*

*
*

*

*

*

* *

*

*

*

*

*

*

*

*

*

*
*

*

*

*

*
*

*
*

*

*

*

*

* *

*

*

*

**

*
*

*
*

*
**

*

*
*

*

*

* *

*

*

*

*

*

*

*
*

*

*

*

*

* *

*

*

*

**

*

*

*
**

*

* *

*

*

*

*
*

* *
*

* *

*

*

*

*
*

*

*

*
*

*

**

*

*

*

*

*

*
* *

*

*

*

*

*
*

*

* *
*

*

*

*

*

* *
*

* * *

*

* *
***

*
**

*

*

*

**

*

*
*

**

*

* *

*
* *

*

*

*
*

*

*

*

*

*

*

*

* *

*
*

*

*

*

*
*

*

*
*

*
*

* *
*

*

*

*
*

*

* *

*

*

*
*

*

**

*

* **
*

* *

*
*

*

*

*

* *

*
*

**
*

*

*
*

*

*

*

*

*

*
**

*

*

*

*

*
*

*
*

*

* *

*

*

* *

**

**

*

*

*

*

*

*

**

**

**
*

*

*

*

*

*

*
*

*

*

*

*

*
*

*

*

* *

*

*
*

*

*

*

*
*

*

* *

*

*

*

*

*

*

*

*
*

*

*
*

*
*

*
*

*

*

*

*
*

*

*
*

*

*

*

* *

*

*
* *

*
*

*

*

*

*

**

*

*

*

*

*

*

*

*

*

*

*

*

* *
* *

*
*

*

*
*

*

*

*

**

*

*

*

*

*

* *

*

*
* *

*
*

* *

*
*

*
*

***

*

* *
*

*

*

*
*
*

*

*

*

*

*

*
*

*

*

**
*

*

*

*

*
*

*

*

*

*

*

**

*

*

*

**

*
*

*
*

*
*

*

*

*

*

*

**

*
* *

*

*

*

*

*

*

*

*

*

**

*

*

*

*

*

*

*

*

*

*

**

*

*

*

*

**
*

*
**

*

*

*
*

*
*

*

*
*

* *

*
*

*

*

*

*

*

*

*

*
*

**

*

*
*

*

*

**

*

*
*

*

*
*

*
*

*

*

* *

*

*

**

*

*

*

*

* **

*

*

*

*

*

*
*

*

*

*
*

*
* * ***

*

*

*

*
* **

*

*

*

*

*

*
*

*
*

*

*

*
*

***

*

*

*

*

*

*
* *

**

*
*

*

** **
*

*

*

*

*

*
*

*

*

*

*

*
*

*
**

*
*

*
*

*

* *

*

*

*

*

*
*

*
*

*

*

*

*

*

*

*

*

*

*

**

*

*

*

*
*

*
*

*

*

*
* *

*

*

*

* *

*

* *

*

*
*

*

*

*
* *

*

*

*

*
*

*

*
*

*

*

*
*

** *** *
*

*

*

** *
*

*

*

*
*

*

*

**
*

*

*

* *

*

**

*

*
*

*

*

*

*
*

*

*

*
*

*

* *

*

*

*
*

*

*

* *

* *
*

*
*

*
*

*

*

*

* *

*

**

*

**

*
*

*

*
*

*

*
*

*

**
*

**

*

*

* *
*

*

*

*

*

*

*

*

*

*

*

*

*

**
*

**
*

*

*
*

**

*
*

*

**

*

**
*

*

*
* *

*
*

*

*

*

*

*
*

*

*
***

* *
**

* *

*

*

*

*
*

*

*
*

*

**
*

*
*

*
*

***

*

*

**
*

*
**

*

*

*
*

*

* **
*

* *

*

*

**

*
**

*

*

**

* *

** *
*

*

*

*

*
*

* *
*

*
*

*
**

*

*

* *

*

*
*

*

*
** *
*

*

*

* *

**

*

*

*

*
*

*

*

*
*

*
*

*
*

*
*

**

*
** *

* *
*

*

*
*

* *** *

*

*

** *

*
* **

** *

*

*
*

*

**

*

**
* *

*

**

*

*

*

* **

*

*
*

**
**

* ***

*

*

**

*

** *

* *

*

*

*

*

*

*

** *
*

*
*

*

*
*

*
**

*

*

*
*

*

*

*
*

* *

*

*

*

** **
**

*

*
*

*

*

*
*

* *
** *

*

*

*

*

*

*

*
*

*
**

*

*

*

*
*

*

*

**

*
*

*

*

* * ***
*
* *
** *

*
*

**** *
**

*
*

*
*

*

*

*
**

*

*
* *

* **

*

*

*

**

*
*

*

*

*

*
*

*

*

*

* **
*

*

**

*

**
*

*
*

** **

*

*
*

**

*
*

*

**
*

*
*

*

*

*

*
*

* **
*

*

*

** *

*

*
***

** *

*
* *

**

*

* *
*

*
* *

*

*

*
*

*

***
*

*

*

*
*

*

***

*

**

*

**
*

*

*

*
*
*

**

*

*

*

*
**

*

*
*

* *
*

*
*

*

**
**

* *
*

**

*

*

*
*

*
*

*

*
*

*

*

*

*

*

*

*

*
* *

*

*
*

*

*
*

*

*

**

* *
*

*

** *

*

*
**

*

*

*
*

*

*

* *
*

*
*

*

*

*

* *
*

*

**

*

*

*

*

*

*
*
*

*
*
*

*

*

*

*
* *

*
*

*
*

*

*

*

*

***
*

*

*

*
*

*

*
*

* *
*

*
*

*
**

*

*

*

*
*

*

*
*

*

* *
*

**
**

**
**

*
*

*

*

**
*

*
*

*

*

*

*
*

*

*
*

**
*

*

*

**
*

*

*

* *

*

*
*

*

*
**

*

**
*

*
*

**
***

***
*

*

*

* *

**
* *

* *
*

*
*

*

*
**

*
*

*

*

*
* **

*

*

*
*

* *

*

*

*
*

*
** * *

*
*

*
*

* *
*

*

**
** * *

*

*
**

* *

*
*

*
*

**
*

**
*

**

*
**

*

*
*

*

*
*

* ***

*
*

*
** *

*

*

**
*
* *
*

*
*

*

*

*

* *

*
* **

*

** *

*
*

** * *

*

*
*

*

**** *

**

*

*
*

*

*
*

*

**
* **

* **

*

*
*

*

*

*

**
*

*

*
*

*

*
*

*

*

*

* *
*

* *

*
*

*

*
*

*

*

*
*

* *

* *

*

*

*

*

*

*

* *

*
* *

*

**

*

*

* *
***

*

* *

*
* *

*
**

*

*

*

*

*

*
*

*

*
**

*

*

**
*

*

*

* **
* *

*

*

* * *
* ***

*

*

*
*

*

*

*

*

*

*

*

*

* *
*

*

* *

* *

**
*

**

*

*

*

* *
*

*

*

*

*
*

*

*
*

*
*

*

** *

*

*
*

*

*
* *
*

**

*

*

*

*
* * *

*

*

** ***
* **

*
*

*

*

*
*** *

*

*

*

*

*

*
**

*

*

*
**

*
*

*

*

** *
**

*

*
* *

*
*

*

*

*

*

*
*

**

*

*

*
*

*

*

*
*

*
*

*

*
*

*
* **

*

*

*
**

*

*

*

*

**
*

**

*
*

*

* *

*

*

*

*

*

*

*

*

*
**

*

*

*

*
*

*

*

*

* **

*

*

**

* *

*

*

*
*

*

*
*

*

*
*

*

*

* *

*

*

*
*

*

*
*

*

*

*

*

*

*

**

*

*
*

*

*

* *

*

** *

*

*

*
*

*

*

*

*

*
*

**

* * *

*

*
*

*

***

*
*

* *

*

*

*
*

**

*

*

**

*

*

** **

*

*
*

*

*

*

*

*

*

*

*

*
**

*

**

*
*

*

*

*

*

*
**

*

*

*

*

*

**

*

*

*

*

*

*
*

*
*

*

**

*

*
*

*

*

*
*

*
*

*

*

*

*

*
*

*

*

*
*

*
*

* *

*

*

* *

*

*
*

*
*

*

*

**

* *

*

*

*

*

*
*

*

*

*
*

*

*
** *

*

*

*

*

*

*

*
*

*

*

**

*

*
*

* *

*

*

*

*
*

*
*
* *

*

**

*

*
*

*

*
*

*

*

*
**

*

*
*

*

* *

*

*

*
*

*

*

*

*

*

*

*

*

*

*

*
**

*

****
* *

*

*

*
*

*

*

* *

**

*

*

*

*
*

*
*

* *

*

*

*

*

* *
*

*
*

*

* *

*

*

*
*

* *

*
**

**

*

*

*
*

*

*

**

*

*

*

*

*

***

*

*

*

*

*

*

**

*

*
*

*

*

*

* **

*
*

*** *
* *
**

***

*

*

* *
*

*

*

*
**

*

*

*
*

*

*

***

*

*

*

*

*
**

*

*

*

*

**
*

*

* **

*
*

*

*

*
*

*

* *

*

*

**

***
*

*

*

*

*

**
*

*
*

*

*

*

*

*

*
*

*
*

*

**

*

*

*

*

*
*

* *

*

*

*

* *

*
*

*
*

*

**

*

*

*
*

*

*
*

*
*

*

*

*
*

*

*

*
*

*

*

*
*

*
* * *

*
**

*
*

*

**

** **

*

* *
*

* **

*

*

*** **

*

*

*

*

*

*

*

*

**
* *

* *
* *

*

*

*
*

*
*

*
*

**
* *

*

*

*

* *
*

*
*

*

***

**

*

*
*
*

*

*

*

*
*

*

*
*

*

*

*
*

**

*

*

*

**

*

*

*
*

*

*

*
*

*
**

*

*

*

*

*

*

*

* *

*
*

*

*

*

*

*

** *
*

*

*
*

*
*

**

*

*
*

*
*

* *
*

*

*

*
*

*
*

*

*

*
*

*
*

*

*
*

*

*

*
**

*

**

*
*

*
* *

*

*

*

*

**
**

*

*

*
*

*
**

*
*

*

*

*

*
*

*

*

*

* *
***

*

*

*

*

*

*
* *

*

*

*

*

*

*

*

* *

*

*
* *

*

*

*

*
*

*

*

*

*

*

**

*

*

*

*

*

*

*

*

*

*
**

*

*

*

*
*

*

*

*

*

**
*

*

*

*
*

*

*

*

*

*

*
* *

*
**

*

*

**

*

*

*

*

*

*

*

*
*

*

*

*

*

*

*

*

*

*

*

*

*

****
*

*

*

*

**
*

*

*

*

*

*

*
**

*

*

*

*

*

*
*

*
*

*
*

*

*

*
*

* *

*
*

*

*

*

*

*

*

*

*
*
*

*

*

* *
**

*
*

*

*

*

* *

*
*
*

*

*

*

*

*

*
*

*

*
*

*
*

*

*

*

*

*

*

*

*

*

*
*

*
*

**

*

*

*

*

*

*

*
*

*

*

*

*

*

*

*

*

*

*

*

*

*
*

*

*

*

*

*

*

*

*

**
*

*

*

*

*

*
*

*

** *
*
*

**

*

*

*

* *

*

*

**

*

*

*

*

**

*

* *
*

*
*

*

*

**
*

*
*

**
*

*

*
*

*
*

*
* *

*
*

*

*
*

**
*

*

*

*

*

***

*

*

*

*
*

*

*

*

*
*

*
*

*

**

* **
*

*

*

*

*

*
*

*

*

**

* *

*

*

*

*

*
*

*

*
* *

*

*
*

*
*

*

*

*

**
*

* * *

*

*
*

*

*

* *

** *

*

*

**

*

*

**

*

*

*

**

*

*

*

***

*

*

*
*

*

*
*

*

*

*
*

*
*
* *

*
*

**

*

**
*

*

*

*

*

*

*

*

*

*

*

*

*

**

*

*

*
*
*

*

*

*
*

*

*

*
*

*

*

*

* **
*

*

*

*
*

*

*

**
*

* *
*

*

*
*

*

*

*
*

*
**

*
* *

*
*

*

*

*

**

*

*

*

*

*

*

*

*

*

* *

*

*

*

*

*

*

*

*

*

*

*

*

*

*

*

*

*
*

*
* *

*

*

*

*

*

*

*

*
*

*

*

*

*

*

*
*

*

*

*

*
**

*

*

*
*

*

*

*

*

*

*
*

* *

*
*

*

*

*

*

*

*

*

*

*

*

**

*

*

*

*

*

*

* *

**

*

*
*

*
*

*
*
*

*

*

*

*
*

**
*

**

*

*

*

*
* *

*

*
*

*

*

*

*

*

*

*
*

*
*

*

*

*

***

*

*

*

**

*

*
*

*

**

*

*

*

*
**

*

*

*
*

*

*

**

*

*
***

**
*

*

*

* **

*

*

*

*

*

*

*

**

*

*

*

*

* *
*

*

***

*

*

*

*

*

*

*

*

*

*

***
* **
*

*

**
*

*

*

*

*

*
*

*

*

*

*

*
**

*
*

*

*
*

*

*
**

*

*

*

*

*

*

*
*

*

*
*

*

*

*

*
*

*

*

*

*

**

*

* *
*

*
* **

*
*

*

*
*

*

*

*

*
*

*

*

*

*

*

*

*

**

*

*

*

*

*

*
*

*

*

*

*

*
*

*

*
*

*

*

*
** *

*

* *

*
*

*

*
*

*

*

**

*

*

**
***

*

* * *
*

*
*

**

*
*

*

* **

*
**

* *

*

*

*
*

*

*

** **

*

*

**

*

*

* *
*

*

**

* *

*

*

*

**

*
*

*

*

*

*

*

*

*

*

*
*

*

*

*
*

*

*
*

*

**
*

*

*

*

**
*

**
**

*

*

*
*

*

*

*

*

*
* *

*

*

*

*

*
*

*

*

**

*

**

*

*

* *
*

*
*

*

* *

*

*

* * *

*

*

*
*

*
*

* *

*
*

*

*

*
*

*
*

*
*

*

*

*

*
* *

**

**

**

*

*
*

*

*
*

*
*

*

*

*
*

*

*
*

*
*

*
*

*
*

*
*

*

*

*
*

*

*

*
*

*
*

*

*
*

*
*

**

*

*

*

*
*

*

*

* *

*

*

*

*

*

*

*

*

*

*

*

*
*

*

*
*

* *
*

*

*

** *

*

*
*

* *

*
*

*

*
****

**

*
*

*
*

*

*

*

*

*

*

*

*

*

*

*

*
*

*

*
**
**

*
*

* *

* *

*

*

*
*

*

*

*
*

** *

*
* **

*

*

*

*

*

*

*

*

*

* *

*

*

*

*
*

*
*

**

*

*

**
*

**

*

*

*

*

*

**

**

*

*

*
*

*
**

*

*

*

*

**

*

*
***

*

*

**

*

* **

**

*

*

*

*

*

*

*
*

*

*

*
* *

**

*

**

*

*

*

*
*

*

*
*

* *

*
* *

*

*

*

*
* *

*

*
*

*

**
** *

*

**

*

*

*

*

(d) semi-cnn

Figure 7: t-SNE visualizations of clustering results.

37



grouped all the systems into three categories: un-
supervised (Unsup.), supervised (Sup.), and semi-
supervised (Semisup.) 3. We found that the su-
pervised systems worked much better than the un-
supervised counterparts, which implies that the
small amount of labeled data is necessary for bet-
ter performance. We also noticed that within the
supervised systems, the systems using deep learn-
ing (CNN or LSTM) models worked better than
the systems using metric learning method, which
shows the power of deep learning models for short
text modeling. Our “semi-cnn” system got the best
performance on almost all the datasets.

Figure 7 visualizes clustering results on the
question type dataset from four representative sys-
tems. In Figure 7(a), clusters severely overlap with
each other. When using the CNN sentence repre-
sentation model, we can clearly identify all clus-
ters in Figure 7(b), but the boundaries between
clusters are still obscure. The clustering results
from our semi-supervised clustering algorithm are
given in Figure 7(c) and Figure 7(d). We can see
that the boundaries between clusters become much
clearer. Therefore, our algorithm is very effective
for short text clustering.

5 Related Work

Existing semi-supervised clustering methods
fall into two categories: constraint-based and
representation-based. In constraint-based meth-
ods (Davidson and Basu, 2007), some labeled
information is used to constrain the clustering
process. In representation-based methods (Bair,
2013), a representation model is first trained to
satisfy the labeled information, and all data points
are clustered based on representations from the
representation model. Bilenko et al. (2004) pro-
posed to integrate there two methods into a unified
framework, which shares the same idea of our
proposed method. However, they only employed
the metric learning model for representation
learning, which is a linear projection. Whereas,
our method utilized deep learning models to
learn representations in a more flexible non-linear
space. Xu et al. (2015) also employed deep learn-
ing models for short text clustering. However,
their method separated the representation learning

3All clustering systems are based on the same number of
instances (total# in Table 3). For the semi-supervised and su-
pervised systems, the labels for 1% of the instances are given
(labeled# in Table 3). And the evaluation was conducted only
on the unlabeled portion.

process from the clustering process, so it belongs
to the representation-based method. Whereas,
our method combined the representation learning
process and the clustering process together, and
utilized both labeled data and unlabeled data for
representation learning and clustering.

6 Conclusion

In this paper, we proposed a semi-supervised clus-
tering algorithm for short texts. We utilized deep
learning models to learn representations for short
texts, and employed a small amount of labeled
data to specify our intention for clustering. We
integrated the representation learning process and
the clustering process into a unified framework, so
that both of the two processes get some benefits
from labeled data and unlabeled data. Experimen-
tal results on four datasets show that our method is
more effective than other competitors.

References
Enrique Amigó, Julio Gonzalo, Javier Artiles, and Fe-

lisa Verdejo. 2009. A comparison of extrinsic
clustering evaluation metrics based on formal con-
straints. Information retrieval, 12(4):461–486.

David Arthur and Sergei Vassilvitskii. 2007. k-
means++: The advantages of careful seeding. In
Proceedings of the eighteenth annual ACM-SIAM
symposium on Discrete algorithms, pages 1027–
1035.

Eric Bair. 2013. Semi-supervised clustering meth-
ods. Wiley Interdisciplinary Reviews: Computa-
tional Statistics, 5(5):349–361.

Somnath Banerjee, Krishnan Ramanathan, and Ajay
Gupta. 2007. Clustering short texts using
wikipedia. In Proceedings of the 30th annual inter-
national ACM SIGIR conference on Research and
development in information retrieval, pages 787–
788.

Mikhail Bilenko, Sugato Basu, and Raymond J
Mooney. 2004. Integrating constraints and metric
learning in semi-supervised clustering. In Proceed-
ings of the twenty-first international conference on
Machine learning, page 11. ACM.

Christopher M Bishop. 2006. Pattern recognition and
machine learning. springer.

Ian Davidson and Sugato Basu. 2007. A survey
of clustering with instance level constraints. ACM
Transactions on Knowledge Discovery from Data,
1:1–41.

38



Inderjit S. Dhillon and Yuqiang Guan. 2003. Infor-
mation theoretic clustering of sparse co-occurrence
data. pages 517–520. IEEE Computer Society.

Samah Fodeh, Bill Punch, and Pang-Ning Tan. 2011.
On ontology-driven document clustering using core
semantic features. Knowledge and information sys-
tems, 28(2):395–421.

Alex Graves. 2012. Supervised sequence labelling
with recurrent neural networks, volume 385.

Geoffrey E Hinton and Ruslan R Salakhutdinov. 2006.
Reducing the dimensionality of data with neural net-
works. Science, 313(5786):504–507.

Yoon Kim. 2014. Convolutional neural networks for
sentence classification. In Proceedings of the 2014
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 1746–1751.

Diederik Kingma and Jimmy Ba. 2014. Adam: A
method for stochastic optimization. arXiv preprint
arXiv:1412.6980.

Jens Lehmann, Robert Isele, Max Jakob, Anja
Jentzsch, Dimitris Kontokostas, Pablo N Mendes,
Sebastian Hellmann, Mohamed Morsey, Patrick van
Kleef, Sören Auer, et al. 2014. Dbpedia-a large-
scale, multilingual knowledge base extracted from
wikipedia. Semantic Web Journal, 5:1–29.

Xin Li and Dan Roth. 2002. Learning question clas-
sifiers. In Proceedings of the 19th international
conference on Computational linguistics-Volume 1,
pages 1–7.

James MacQueen. 1967. Some methods for classi-
fication and analysis of multivariate observations.
In Proceedings of the fifth Berkeley symposium on
mathematical statistics and probability, volume 1,
pages 281–297. Oakland, CA, USA.

Tomas Mikolov, Kai Chen, Greg Corrado, and Jef-
frey Dean. 2013. Efficient estimation of word
representations in vector space. arXiv preprint
arXiv:1301.3781.

James Munkres. 1957. Algorithms for the assignment
and transportation problems. Journal of the Society
for Industrial and Applied Mathematics, 5(1):32–38.

Nguyen Xuan Vinh, Julien Epps, and James Bailey.
2009. Information theoretic measures for cluster-
ings comparison: is a correction for chance neces-
sary? In Proceedings of the 26th Annual Inter-
national Conference on Machine Learning, pages
1073–1080.

Kilian Q Weinberger, John Blitzer, and Lawrence K
Saul. 2005. Distance metric learning for large mar-
gin nearest neighbor classification. In Advances in
neural information processing systems, pages 1473–
1480.

Jiaming Xu, Peng Wang, Guanhua Tian, Bo Xu, Jun
Zhao, Fangyuan Wang, and Hongwei Hao. 2015.
Short text clustering via convolutional neural net-
works. In Proceedings of NAACL-HLT, pages 62–
69.

Xiang Zhang and Yann LeCun. 2015. Text understand-
ing from scratch. arXiv preprint arXiv:1502.01710.

39


