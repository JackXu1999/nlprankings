



















































Paragraph-level Neural Question Generation with Maxout Pointer and Gated Self-attention Networks


Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 3901–3910
Brussels, Belgium, October 31 - November 4, 2018. c©2018 Association for Computational Linguistics

3901

Paragraph-level Neural Question Generation
with Maxout Pointer and Gated Self-attention Networks

Yao Zhao Xiaochuan Ni Yuanyuan Ding Qifa Ke
Microsoft AI & Research

Sunnyvale, California
{yaozhao, xiaon, yuand, qke}@microsoft.com

Abstract

Question generation, the task of automatically
creating questions that can be answered by a
certain span of text within a given passage,
is important for question-answering and con-
versational systems in digital assistants such
as Alexa, Cortana, Google Assistant and Siri.
Recent sequence to sequence neural models
have outperformed previous rule-based sys-
tems. Existing models mainly focused on us-
ing one or two sentences as the input. Long
text has posed challenges for sequence to se-
quence neural models in question generation
– worse performances were reported if us-
ing the whole paragraph (with multiple sen-
tences) as the input. In reality, however, it
often requires the whole paragraph as context
in order to generate high quality questions.
In this paper, we propose a maxout pointer
mechanism with gated self-attention encoder
to address the challenges of processing long
text inputs for question generation. With
sentence-level inputs, our model outperforms
previous approaches with either sentence-level
or paragraph-level inputs. Furthermore, our
model can effectively utilize paragraphs as in-
puts, pushing the state-of-the-art result from
13.9 to 16.3 (BLEU 4).

1 Introduction

Question generation (QG), aiming at creating
questions from natural language text, e.g. a sen-
tence or paragraph, is an important area in natural
language processing (NLP). It is receiving increas-
ing interests in recent years from both industrial
and academic communities, due to the booming
of Question-and-Answer (QnA) and conversation
systems, including Alexa, Cortana, Google Assis-
tant and Siri, the advancement of QnA or machine
comprehension technologies together with the re-
leases of datasets like SQuAD (Rajpurkar et al.,
2016) and MS MARCO (Nguyen et al., 2016),

and the success of language generation technolo-
gies for tasks like machine translation (Wu et al.,
2016) and text summarization (See et al., 2017)
in NLP. A conversational system can be proactive
by asking the user questions (Shum et al., 2018),
while a QnA system can benefit from a large scale
question-answering corpus which can be created
by an automated QG system (Duan et al., 2017).
Education is another key application where QG
can help with reading comprehension (Heilman
and Smith, 2010).

In NLP, QG has been mainly tackled by two ap-
proaches: 1) rule-based approach, e.g. (Heilman
and Smith, 2010; Mazidi and Nielsen, 2014; Lab-
utov et al., 2015) 2) neural QG approach: end-to-
end training a neural network using the sequence
to sequence (also called encoder-decoder) frame-
work, e.g. (Du et al., 2017; Yuan et al., 2017; Song
et al., 2017; Zhou et al., 2017). In this paper, we
adopt the second approach.

More specifically, we focus on an answer-aware
QG problem, which takes a passage and an answer
as inputs, and generates a question that targets the
given answer. It is also assumed the answer is
comprised of certain spans of the text from the
given passage. This is the exact setting of SQuAD,
and similar problems have been addressed in, e.g.
(Zhou et al., 2017; Yuan et al., 2017; Wang et al.,
2017a).

A paragraph often contains much richer con-
text than a sentence, as shown in Figure 1. (Du
et al., 2017) pointed out that about 20% questions
in SQuAD require paragraph-level information to
be asked and using the whole paragraph can im-
prove QG performance on those questions. How-
ever, a paragraph can contain irrelevant informa-
tion w.r.t. the answer for generating the question.
The challenge is thus how to effectively utilize rel-
evant information at paragraph-level for QG. Ex-
isting neural QG works conducted on SQuAD use



3902

1 Paragraph: carolina suffered a major setback when thomas davis , an 11-
year veteran who had already overcome three acl tears in his career , went
down with a broken arm in the nfc championship game . despite this , he
insisted he would still find a way to play in the super bowl . his prediction
turned out to be accurate
Human generated: what game did thomas davis say he would play in ,
despite breaking a bone earlier on ?
Sentence-level QG: what sports game did spielberg decide to play in ?
Paragraph-level QG: what competition did thomas davis think he would
play in ?

2 Paragraph: walt disney and his brother roy contacted goldenson at the
end of 1953 for abc to agree to finance part of the disneyland project in
exchange for producing a television program for the network . walt wanted
abc to invest $ 500,000 and accrued a guarantee of $ 4.5 million in addi-
tional loans , a third of the budget intended for the park . around 1954 ,
abc agreed to finance disneyland in exchange for the right to broadcast a
new sunday night program , disneyland , which debuted on the network on
october 27 , 1954 as the first of many anthology television programs that
disney would broadcast over the course of the next 50 years
Human generated: how much did walt disney want abc to invest in dis-
neyland ?
Sentence-level QG: how much money did walt wanted to invest ?
Paragraph-level QG: how much money did walt wanted to invest in 1953
?

3 Paragraph: following the peterloo massacre of 1819 , poet percy shelley
wrote the political poem the mask of anarchy later that year , that begins
with the images of what he thought to be the unjust forms of authority of
his timeand then imagines the stirrings of a new form of social action . it is
perhaps the first modern [ vague ] statement of the principle of nonviolent
protest . a version was taken up by the author henry david thoreau in his
essay civil disobedience , and later by gandhi in his doctrine of satyagraha
. gandhi ’s satyagraha was partially influenced and inspired by shelley ’s
nonviolence in protest and political action . in particular , it is known that
gandhi would often quote shelley ’s masque of anarchy to vast audiences
during the campaign for a free india
Human generated: his poem is considered the first kind of what type of
protest ?
Sentence-level QG: what is the principle of the protest ?
Paragraph-level QG: what type of protest did percy shelley write ?

4 Paragraph: the victoria and albert museum ( often abbreviated as the
v & a ) , london , is the world ’s largest museum of decorative arts
and design , housing a permanent collection of over 4.5 million objects
. it was founded in 1852 and named after queen victoria and prince albert.
the v & a is located in the brompton district of the royal borough of kens-
ington and chelsea , in an area that has become known as “ albertopolis ”
because of its association with prince albert , the albert memorial and the
major cultural institutions with which he was associated . these include the
natural history museum , the science museum and the royal albert hall . the
museum is a non-departmental public body sponsored by the department
for culture , media and sport . like other national british museums , en-
trance to the museum has been free since 2001
Human generated: when was the victoria and albert museum founded ?
Sentence-level QG: when was prince albert and prince albert founded ?
Paragraph-level QG: when was the victoria and albert museum founded ?

Figure 1: Examples where paragraph-level infor-
mation is required to ask right questions. Sen-
tences contain answers are in italic font, while
answers are underscored. QG results are gener-
ated by model s2s-a-ct-mp-gsa.

only a sentence as context, e.g. (Du et al., 2017;
Song et al., 2017; Zhou et al., 2017); when applied
to paragraph-level context (Yuan et al., 2017) we
observed large gaps compared to state-of-the-art
results achieved by using sentence-level context.

In this paper, we extend previous sequence to
sequence attention model with a maxout pointer
mechanism and a gated self-attention encoder
which outperforms existing neural QG approaches
with either sentence or paragraph as inputs. Fur-

thermore, with paragraph-level inputs, it outper-
forms the results of previous approaches with
sentence-level inputs, improving state-of-the-art
result from 13.9 to 16.3 (BLEU 4). This is the
first model that demonstrates large improvement
with paragraph as input over sentence as input.
In addition, our model is more concise compared
to most of existing ones, e.g. (Yuan et al., 2017;
Song et al., 2017). Techniques like incorporating
rich features (Zhou et al., 2017) and policy gradi-
ent (Song et al., 2017; Yuan et al., 2017) are or-
thogonal to ours and can be leveraged for further
performance improvement in the future.

2 Our Model

2.1 Problem Definition

We use P andA to represent input passage and an-
swer respectively, and use Q to represent the gen-
erated question. ”Passage” in this section can rep-
resent either a sentence or a paragraph. The task is
to find Q̄ that:

Q̄ = argmax
Q

Prob(Q|P,A)

where passage is comprised of sequence of
words:P = {xt}Mt=1, answer A must be sub spans
of the passage. Words generated in Q = {yt}Kt=1
are either from the input passage, {xt}Mt=1, or from
a vocabulary V .

Figure 2 illustrates the end-to-end structure of
our model proposed in this paper.

2.2 Passage and Answer Encoding

Different types of encoders are designed for vari-
ous domains (Chung et al., 2014; Hochreiter and
Schmidhuber, 1997). We are agnostic to the form
of the encoder and simply use recurrent neural net-
work (RNN) to present the encoding process:

ut = RNN
E(ut−1, [et,mt]) (1)

Answer Tagging. In Eq. 1, ut represents the
RNN hidden state at time step t, et is the word
embedding representation of word xt in passage
P . mt is the meta-word representation of whether
word xt is in or outside the answer. [a,b] repre-
sents the concatenation of vector a and b. We call
this approach answer tagging which is similar to
the techniques in (Zhou et al., 2017; Yuan et al.,
2017). For applications, it is essential to be able to
generate question that is coherent to an answer.



3903

U

Û 
 

 

y
t−1

dt

rt d ̂ t
MP

Encoding Passage

Word 
Embedding 

Answer 
Tagging 

Passage-Answer 
Representation 

Self Attention 

Feature Fusion 
Gating

Final Passage-Answer Representation 
Partial Decoding

Decoding 
State 

Attention 
Score 

Decoding Attention 

Attention 
Vector 

Maxout 
Pointer 

Copy Scores Generative Scores 

softmax 

Generative Probability 
with Padding for OOV 

Copy Probability 

Final 
Distrubution 

LSTM LSTM LSTM LSTM

LSTM

ut

et

mt

st

û t

LSTM

ut

et

mt

st

û t

LSTM

ut

et

mt

st

û t

LSTM

ut

et

mt

st

û t

Figure 2: End-to-end diagram for the model with answer tagging, gated self-attention and maxout pointer
mechanism.

If RNNE is bi-directional, u is the concate-
nated representation of the forward and backward
passes: U = {[−→ut,←−ut]}Mt=1.

Gated Self-attention. Our gated self-attention
mechanism is designed to aggregate information
from the whole passage and embed intra-passage
dependency to refine the encoded passage-answer
representation at every time step. It has two steps:
1) taking encoded passage-answer representation
u as input and conducting matching against itself
to compute self matching representation; (Wang
et al., 2017b) 2) combining the input with self
matching representation using a feature fusion
gate (Gong and Bowman, 2017).

ast = softmax(U
TWsut) (2)

st = U · ast (3)

Step 1. In Eq. 2, Ws is a trainable weight ma-
trix. In Eq. 3, st is the weighted sum of all words’
encoded representation in passage based on their
corresponding matching strength to current word
at t. s = {st}Mt=1 is the final self matching repre-
sentation.

ft = tanh(W
f [ut, st]) (4)

gt = sigmoid(W
g[ut, st]) (5)

ût = gt � ft + (1− gt)� ut (6)

Step 2. The self matching representation st is
combined with original passage-answer represen-
tation ut as the new self matching enhanced rep-
resentation ft, Eq. 4. A learnable gate vector gt,
Eq. 5, chooses the information between the orig-
inal passage-answer representation and the new
self matching enhanced representation to form the
final encoded passage-answer representation ût,
Eq. 6, where� is the element-wise multiplication.

2.3 Decoding with Attention and Maxout
Pointer

In the decoding stage, the decoder is another RNN
that generates words sequentially conditioned on
the encoded input representation and the previ-
ously decoded words.

dt = RNN
D(dt−1,yt−1) (7)

p(yt|{y<t}) = softmax(WV dt) (8)



3904

In Eq. 7, dt represents the hidden state of the RNN
at time t where d0 is passed from the final hidden
state of the encoder. yt stands for the word gen-
erated at time t. The bold font yt is used to rep-
resent yt’s corresponding word embedding repre-
sentation. In Eq. 8, first an affine layer projects dt
to a space with vocabulary-size dimensions, then
a softmax layer computes a probability distribu-
tion over all words in a fixed vocabulary V . WV

is a trainable weight matrix.
Attention. Attention mechanism (Bahdanau

et al., 2014; Luong et al., 2015) has been used
to improve sequence to sequence models’ perfor-
mance and has became a default setting for many
applications.

We use Luong attention mechanism to compute
raw attention scores rt, Eq. 9. An attention layer,
Eq. 12 is applied above the concatenation of de-
coder state dt and the attention context vector ct
and its output is used as the new decoder state.

rt = Û
TWadt (9)

adt = softmax(rt) (10)

ct = Û · adt (11)
d̂t = tanh(W

b[dt, ct]) (12)

Copy/Pointer. Copy mechanism (Gu et al.,
2016) or pointer network (See et al., 2017; Vinyals
et al., 2015) was introduced to allow both copy-
ing words from input via pointing, and generating
words from a predefined vocabulary during decod-
ing.

Similar to (Gu et al., 2016), our pointer mecha-
nism directly leverages raw attention scores rt =
{rt,k}Mk=1 over the input sequence which has a vo-
cabulary of χ. Words at every time step (a pointer)
are treated as unique copy targets and the final
score on one word is calculated as the sum of all
scores pointing to the same word, Eq. 13, where
xk and yt stand for word vocabulary indices of the
kth word in input and the tth word in decoded
sequence respectively. The scores of the nonoc-
curence words are set to negative infinity which
will be masked out by the downstream softmax
function.

sccopy(yt) =


∑

k,where xk=yt

rt,k, yt ∈ χ

− inf, otherwise
(13)

We then concatenate sccopyt with the gener-
ative scores (from Eq. 8 before softmax),

[scgent , sc
copy
t ], which has dimension: |V | + |χ|.

Then we perform softmax on the concatenated
vectors and sum up the probabilities pointing to
same words. Taking softmax on the concate-
nated score vector enforces copy and generative
modes to compete with each other due to the
shared normalization denominator. Another pop-
ular solution is to do softmax independently to
the scores from each mode, and then combine their
output probabilities with a dynamic weight which
is generated by a trainable network (See et al.,
2017). We have tested both and didn’t find sig-
nificant difference in terms of accuracy on our QG
task. We choose the former copy approach mainly
because it doesn’t add extra trainable parameters.

Maxout Pointer. Despite the outstanding per-
formance of existing copy/pointer mechanisms,
we observed that repeated occurrence of words
in the input sequence tends to cause repetitions
in output sequence, especially when the input se-
quence is long, e.g. a paragraph. This issue exac-
erbates the repetition problem which has already
been commonly observed in sequence to sequence
models (Tu et al., 2016; See et al., 2017). In this
paper, we propose a new maxout pointer mecha-
nism to address this issue and improve the metrics
for QG task. Related works (Goodfellow et al.,
2013) have explored MLP maxout with dropout.

Instead of combining all the scores to calcu-
late the probability, We limit the magnitude of
scores of repeated words to their maximum value,
as shown in Eq. 14. The rest remains the same as
in the previous copy mechanism.

sccopy(yt) =

{
max

k,where xk=yt
rt,k, yt ∈ χ

− inf, otherwise
(14)

3 Experiments

In our experiments, we study the proposed model
on the QG task on SQuAD (Rajpurkar et al., 2016)
and MS MARCO (Nguyen et al., 2016) dataset,
demonstrate the performance of proposed compo-
nents on both sentence and paragraph inputs, and
compare the model with existing approaches.

3.1 Dataset
SQuAD
The SQuAD dataset contains 536 Wikipedia arti-
cles and more than 100K questions posed about
the articles by crowd-workers. Answers are also



3905

BLEU 1 BLEU 2 BLEU 3 BLEU 4 METEOR ROUGE-L

Model Sen. Par. Sen. Par. Sen. Par. Sen. Par. Sen. Par. Sen. Par.

s2s 30.41 28.49 12.68 10.43 6.33 4.70 3.44 2.38 11.98 10.69 29.93 27.32

s2s-a 34.46 31.26 18.07 14.37 11.20 8.02 7.42 4.80 14.95 12.52 34.69 30.11

s2s-a-at 40.57 40.56 24.30 24.23 16.40 16.33 11.54 11.46 18.35 18.42 40.76 40.40

s2s-a-at-cp 42.15 41.66 26.28 25.52 18.35 17.48 13.37 12.43 18.02 17.76 41.97 41.30

s2s-a-at-mcp 43.65 44.22 28.23 28.56 20.33 20.57 15.23 15.43 19.19 19.55 43.60 43.65
s2s-a-at-mcp-gsa 43.47 45.07 28.23 29.58 20.40 21.60 15.32 16.38 19.29 20.25 43.91 44.48

Table 1: Performance of our models on Split1 with both sentence-level input and paragraph-level input.
Sen. means sentence, while Par. means paragraph.

provided to the questions, which are spans of to-
kens in the articles.

Following (Du et al., 2017; Zhou et al., 2017;
Song et al., 2017), our experiments are conducted
using the accessible part of SQuAD: train and de-
velopment (dev*) sets. To be able to directly com-
pare with their works, we adopt two types of data
split: 1) Split1: similar to (Du et al., 2017), we use
dev* set as test set, and split train set into train and
dev sets randomly with ratio 90%-10%. The split
is done at article level. However, we keep all sam-
ples instead of only keeping the sentence-question
pairs that have at least one non-stop-word in com-
mon (with 6.7% pairs dropped) as in (Du et al.,
2017). This makes our dataset harder for training
and evaluation. 2) Split2: similar to (Zhou et al.,
2017), we split dev* set into dev and test sets ran-
domly with ratio 50%-50%. The split is done at
sentence level.

MS MARCO
MS MARCO datasets contains 100,000 queries
with corresponding answers and passages. All
questions are sampled from real anonymized user
queries and context passages are extracted from
real web documents. We picked a subset of MS
MARCO data where answers are sub-spans within
the passages, and use dev set as test set (7k), and
split train set with ratio 90%-10% into train (51k)
and dev (6k) sets.

3.2 Implementation Details

We used 2 layers LSTM as the RNN cell for
both encoding and decoding. For encoding, bi-
directional LSTM was used. The cell hidden size
was 600. Dropout with probability 0.3 was applied
between vertical LSTM stacks. For word em-
bedding, we used pre-trained GloVe word vectors

with 300 dimensions (Pennington et al., 2014),
and froze them during training. Dimension of an-
swer tagging meta-word embedding was 3. Both
encoder and decoder shared the same vocabulary
of the most frequent 45k GloVe words. For op-
timization, we used SGD with momentum (Qian,
1999; Nesterov, 1983). Learning rate was initially
set to 0.1 and halved since epoch 8 at every 2
epochs afterwards. Models were totally trained
with 20 epochs. The mini-batch size for param-
eter update was 64. After training, we looked at
the 4 models with lowest perplexities and selected
the one which used the most number of epochs as
final model. During decoding for prediction, we
used beam search with the beam size of 10, and
stopped decoding when every beam in the stack
generates the EOS token.

3.3 Evaluation
We conduct automatic evaluation with metrics:
BLEU 1, BLEU 2, BLEU 3, BLEU 4 (Papineni
et al., 2002), METEOR (Denkowski and Lavie,
2014) and ROUGE-L (Lin, 2004), and use eval-
uation package released by (Sharma et al., 2017)
to compute them.

4 Results and Analysis

4.1 Comparison of Techniques
Table 1 shows evaluation results for different
models on SQuAD Split1. Results with both
sentence-level and paragraph-level inputs are in-
cluded. Similar results also have been observed
on SQuAD Split2. The definitions of the models
under comparison are:
s2s: basic sequence to sequence model
s2s-a: s2s + attention mechanism
s2s-a-at: s2s-a + answer tagging
s2s-a-at-cp: s2s-a-at + copy mechanism



3906

s2s-a-at-mp: s2s-a-at + maxout pointer mechanism
s2s-a-at-mp-gsa: s2s-a-at-mp + gated self-attention

Attention Mechanism
s2s-a vs. s2s: we can see attention brings in large
improvement on both sentence and paragraph in-
puts. The lower performance on paragraph indi-
cates the challenge of encoding paragraph-level
information.

Answer Tagging
s2s-a-at vs. s2s-a: answer tagging dramatically
boosts the performance, which confirms the im-
portance of answer-aware QG: to generate good
question, we need to control/learn which part of
the context the generated question is asking about.
More importantly, answer tagging clearly reduces
the gap between sentence and paragraph inputs,
which could be explained with: by providing guid-
ance on answer words, we can make the model
learn to neglect noise when processing a long con-
text.

Copy Mechanism
s2s-a-at-cp vs. s2s-a-at: as expected, copy mecha-
nism further improves the performance on the QG
task. (Du et al., 2017) pointed out most of the
sentence-question pairs in SQuAD have over 50%
overlaps in non-stop-words. Our results prove that
sequence to sequence models with copy mecha-
nism can very well learn when to generate a word
and when to copy one from input on such QG task.
More interestingly, the performance is lower when
paragraph is given as input than sentence as input.
The gap, again, reveals the challenge of leveraging
longer context. We found that, when paragraph is
given, the model tends to generate more repetitive
words, and those words (often entities/concepts)
usually appear multiple times in the context, Fig-
ure 3. The repetition issue can also be seen for
sentence input, but it is more severe for paragraph.

Maxout Pointer
s2s-a-at-mp vs. s2s-a-at-cp: Maxout pointer is de-
signed to resolve the repetition issue brought by
the basic copy mechanism, for example Figure 3.
The maxout pointer mechanism outperforms the
basic copy mechanism in all metrics. Moreover,
the effectiveness of maxout pointer is more signif-
icant when paragraph is given as the model input,
as it reverses the performance gap between models

Paragraph: a problem is regarded as inherently difficult if its solution requires
significant resources , whatever the algorithm used . the theory formalizes
this intuition , by introducing mathematical models of computation to study
these problems and quantifying the amount of resources needed to solve them
, such as time and storage . other complexity measures are also used , such
as the amount of communication ( used in communication complexity ) , the
number of gates in a circuit ( used in circuit complexity ) and the number of
processors ( used in parallel computing ) . one of the roles of computational
complexity theory is to determine the practical limits on what computers can
and can not do
Human generated: what unit is measured to determine circuit complexity ?
Basic copy QG: what is an example of a circuit complexity in complexity
complexity ?
Maxout Pointer QG: what is another name for circuit complexity ?

Figure 3: Example for maxout pointer vs. basic
copy/pointer.

s2s s2s-a s2s-a-at s2s-a-
at-cp

s2s-a-
at-mp

s2s-a-at-
mp-gsa

4
6
8

10
12 Paragraph(left)

Sentence(right)

Figure 4: Word duplication rates of QG on Split1,
duplication rate on human generated question is
3.53.

trained with sentence and paragraph inputs, Table
1.

To demonstrate that maxout pointer reduces
repetitions in generated questions, we present the
word duplication rates in generated questions for
various models in Figure 4. Word duplication rate
was computed by counting the number of words
which appear more than once, and then taking a
ratio of them over the total word counts. As shown
in Figure 4, both the attention mechanism and the
basic copy mechanism introduce more repetitions,
although they improve overall accuracy according
to Table 1. For models trained with paragraph in-
puts, where the duplication rates are much higher,
maxout pointer reduces the duplication rates to
half of their values in the basic copy and to the
same level as model trained with sentence inputs.

Such repetition issue was also observed in other
sequence to sequence applications, e.g. (See et al.,
2017) who proposed a coverage model and cov-
erage loss to resolve this issue. We implemented
and tested their approach on our QG task. Even
though the duplication ratio dropped as expected,
we observed a slight decline in the accuracy when
coverage loss was added.



3907

ca
ro

lin
a

su
ffe

re
d a

m
aj

or
se

tb
ac

k
wh

en
th

om
as

da
vi

s , an
11

-y
ea

r
ve

te
ra

n
wh

o
ha

d
al

re
ad

y
ov

er
co

m
e

th
re

e ac
l

te
ar

s in hi
s

ca
re

er ,
we

nt
do

wn wi
th a

br
ok

en ar
m in th
e

nf
c

ch
am

pi
on

sh
ip

ga
m

e .
de

sp
ite th
is , he

in
sis

te
d he

wo
ul

d
st

ill
fin

d a
wa

y to
pl

ay in th
e

su
pe

r
bo

w
l .

hi
s

pr
ed

ict
io

n
tu

rn
ed ou

t to be
ac

cu
ra

te

carolinasufferedamajorsetbackwhenthomasdavis,an11-yearveteranwhohadalreadyovercomethreeacltearsinhiscareer,wentdownwithabrokenarminthenfcchampionshipgame.despitethis,heinsistedhewouldstillfindawaytoplayin
thesuper

bowl.hispredictionturnedouttobeaccurate

Ground-truth: what game did thomas davis say he would play in , despite breaking a bone earlier on ?
QG: what competition did thomas davis think he would play in ?

Figure 5: Self-attention alignments map: each row
represents an alignment vector of self-attention.

Gated Self-attention
s2s-a-at-mp-gsa vs. s2s-a-at-mp: the results
demonstrate the effectiveness of gated self-
attention, in particular, when working with para-
graph inputs. This is the first time, as we know,
taking paragraph as input is better than sentence
for neural QG tasks. The observation is consistent
across all metrics. Gated self-attention helps re-
fine encoded context by fusing important informa-
tion with the context’s self representation properly,
especially when the context is long.

To better understand how gated self-attention
works, we visualize the self alignment vectors at
each time step of the encoded sequence for one ex-
ample, in Figure 5. This example corresponds to
the example 1 in Figure 1. We can see the align-
ments distribution concentrates near the answer
sequence and the most relevant context: ”thomas
davis” in this example. Such alignments in turn
would help to promote the most valuable informa-
tion for decoding.

Beam Search
Beam search is commonly used for decoding for
predictions. Existing neural QG works, e.g. (Du
et al., 2017; Zhou et al., 2017), evaluated their
models with beam search decoding. However, so
far, none of them have reported the comparison be-
tween beam search decoding with greedy decod-
ing. In this paper, we give such comparison for our
best model: s2s-a-at-mp-gsa with both sentence

Beam Search Greedy

Metric Sen. Par. Sen. Par.

BLEU 1 43.47 45.07 42.25 43.48
BLEU 2 28.23 29.58 26.36 27.67
BLEU 3 20.40 21.60 18.35 19.64
BLEU 4 15.32 16.38 13.28 14.50
METEOR 19.29 20.25 18.25 19.17
ROUGE-L 43.91 44.48 42.58 43.62

Table 2: Comparison of beam search and greedy
decodings for model s2s-a-ct-mp-gsa on Split1.

and paragraph inputs in Table 2. We can clearly
see beam search decoding boosts all metrics for
both sentence and paragraph inputs. The effective-
ness of beam search has also been demonstrated
for other tasks, like neural machine translation in
(Wu et al., 2016).

4.2 Comparison with Existing Neural
Question Generation Works

On SQuAD dataset, we compare the BLEU, ME-
TEOR, ROUGE-L scores of our best model, s2s-
a-at-mp-gsa, with the numbers in the existing
works in Table 3. Comparison with (Du et al.,
2017) and (Song et al., 2017) is conducted on
SQuAD data Split1, while comparison with (Zhou
et al., 2017) and (Song et al., 2017) is conducted
on data SQuAD split2. Because (Song et al., 2017)
had results on both splits, we compare with both of
them.

On MS MARCO dataset, we compare the
BLEU 4 scores reported by (Duan et al., 2017) in
Table 4.

Our model with maxout pointer and gated self-
attention achieves the state-of-the-art results in
QG. Note that all those existing works in SQuAD
encoded only sentence-level information, the re-
sults from our model surpass them on the same
sentence input while achieving much higher num-
bers when working with paragraph.

4.3 Case Study

In Figure 1, we present some examples for
which paragraph-level information is needed to
ask good/correct questions. Generated questions
from model s2s-a-ct-mp-gsa are also presented for
both sentence and paragraph inputs. Those exam-
ples demonstrate that generated questions contain
richer information when paragraphs are provided



3908

Model BLEU 1 BLEU 2 BLEU 3 BLEU 4 METEOR ROUGE-L

Split1

(Du et al., 2017) 43.09 25.96 17.50 12.28 16.62 39.75
(Song et al., 2017) x x x 13.98 18.77 42.72

ours, sentence 43.47 28.23 20.40 15.32 19.29 43.91
ours, paragraph 45.07 29.58 21.60 16.38 20.25 44.48

Split2

(Zhou et al., 2017) x x x 13.27 x x
(Song et al., 2017) x x x 13.91 x x

ours, sentence 44.51 29.07 21.06 15.82 19.67 44.24
ours, paragraph 45.69 30.25 22.16 16.85 20.62 44.99

Table 3: Comparison of results on SQuAD dataset.

Model BLEU 4

MSMARCO

(Du et al., 2017) 10.46
(Duan et al., 2017) 11.46

ours, sentence 16.02
ours, paragraph 17.24

Table 4: Comparison of results on MSMARCO
dataset.

instead of sentences.
In example 1 and 3, name ”thomas davis” and

”percy shelley” appear in the paragraphs not the
sentences contain the answers.

In example 2, paragraph-level QG can gener-
ate richer description ”in 1953” from the para-
graph, although human generated is ”disneyland”.
Both generated questions lack one relative impor-
tant piece of information ”want abc to invest”.

In example 4, paragraph-level QG correctly
identifies ”it” is referring to the museum which is
out of the sentence.

5 Related Work

QG has been mainly tackled with two types of ap-
proaches. One is built on top of heuristic rules that
creates questions with manually constructed tem-
plate and ranks the generated results, e.g. (Heil-
man and Smith, 2010; Mazidi and Nielsen, 2014;
Labutov et al., 2015). Those approaches heavily
depend on human effort, which makes them hard
to scale up to many domains. The other one, which
is becoming increasingly popular, is to train an
end-to-end neural network from scratch by using
sequence to sequence or encoder-decoder frame-
work, e.g. (Du et al., 2017; Yuan et al., 2017; Song
et al., 2017; Zhou et al., 2017). The second one is
more related to us, so we will focus on describing
those approaches.

(Du et al., 2017) pioneered the work of auto-
matic QG using an end-to-end trainable sequence
to sequence neural model. Automatic and human
evaluation results showed that their system outper-
formed the previous rule-based systems (Heilman
and Smith, 2010; Rus et al., 2010). However, in
their study, there was no control about which part
of the passage the generated question was asking
about.

Answer-aware sequence to sequence neural QG
systems (Zhou et al., 2017; Subramanian et al.,
2017; Yuan et al., 2017) encoded answer loca-
tion information using an annotation vector cor-
responding to the answer word positions. (Zhou
et al., 2017) utilized rich features of the passage
including answer positions. (Subramanian et al.,
2017) deployed a two-stage neural model that de-
tects key phrases and subsequently generates ques-
tions conditioned on them. (Yuan et al., 2017)
combined supervised and reinforcement learning
in the training of their model using policy gradi-
ent techniques to maximize several rewards that
measure question quality. Instead of using an an-
notation vector to tag the answer locations, (Song
et al., 2017) proposed a unified framework for
QG and question answering by encoding both the
answer and the passage with a multi-perspective
matching mechanism.

(Tang et al., 2017; Wang et al., 2017a) proposed
joint models to address QG and question answer-
ing together. (Duan et al., 2017) conducted QG for
improving question answering. Due to the mixed
objectives including question answering, their ap-
proaches’ performance on QG were lower than the
state-of-the-art results.

6 Conclusion and Future Work

In this paper, we proposed a new sequence to
sequence network which contains a gated self-



3909

attention encoder and a maxout pointer decoder
to address the answer-aware QG problem for long
text input. We demonstrated the model can ef-
fectively utilize paragraph-level context, and out-
performed the results with sentence-level con-
text. The new model exceeded state-of-the-art ap-
proaches with either paragraph or sentence inputs.

We would like to discuss some potential chal-
lenges when applying the QG model in practice.
1) Answer spans aren’t provided as input. One
straight-forward method is to extract entities or
noun phrases and use them as potential answer
spans. A neural entity selection model can also be
leveraged to extract good answer candidates to im-
prove the precision as proposed in (Subramanian
et al., 2017). 2) An input passage does not contain
any eligible answers. In such case, we do not ex-
pect the model to output valid questions. We could
remove questions with low generation probability,
while a better approach could be running entity se-
lection or quality detection model before question
generation step to eliminate ineligible passages. 3)
An answer could be shared by different questions.
We could output multiple questions using beam
search. However, beam search does not guaran-
tee to output diversified candidates. We would
need to explicitly model diversity among candi-
dates during generation, for example, leveraging
the approach described in (Li and Jurafsky, 2016).

Our future work lands in the following direc-
tions: incorporate rich features, such as POS
and entity, in input passages; directly optimize
sequence-level metrics with policy gradient; re-
lax the constraint on answer to accept abstractive
answers; jointly model question generation and
question answering; ask multiple questions simul-
taneously with diverse perspectives.

References
Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben-

gio. 2014. Neural machine translation by jointly
learning to align and translate. arXiv preprint
arXiv:1409.0473.

Junyoung Chung, Caglar Gulcehre, KyungHyun Cho,
and Yoshua Bengio. 2014. Empirical evaluation of
gated recurrent neural networks on sequence model-
ing. arXiv preprint arXiv:1412.3555.

Michael Denkowski and Alon Lavie. 2014. Meteor
universal: language specific translation evaluation
for any target language. In Proceedings of the Ninth
Workshop on Statistical Machine Translation. Asso-
ciation for Computational Linguistics.

Xinya Du, Junru Shao, and Claire Cardie. 2017. Learn-
ing to ask: Neural question generation for reading
comprehension. In Proceedings of the 55th Annual
Meeting of the Association for Computational Lin-
guistics, pages 1342–1352.

Nan Duan, Duyu Tang, Peng Chen, and Ming Zhou.
2017. Question generation for question answering.
In Proceedings of the 2017 Conference on Empirical
Methods in Natural Language Processing.

Yichen Gong and Samuel R Bowman. 2017. Ruminat-
ing reader: Reasoning with gated multi-hop atten-
tion. arXiv preprint arXiv:1704.07415.

Ian J Goodfellow, David Warde-Farley, Mehdi Mirza,
Aaron Courville, and Yoshua Bengio. 2013. Maxout
networks. arXiv preprint arXiv:1302.4389.

Jiatao Gu, Zhengdong Lu, Hang Li, and Victor OK
Li. 2016. Incorporating copying mechanism in
sequence-to-sequence learning. arXiv preprint
arXiv:1603.06393.

Michael Heilman and Noah A Smith. 2010. Good
question! statistical ranking for question genera-
tion. In Human Language Technologies: The 2010
Annual Conference of the North American Chap-
ter of the Association for Computational Linguistics,
pages 609–617. Association for Computational Lin-
guistics.

Sepp Hochreiter and Jürgen Schmidhuber. 1997.
Long short-term memory. Neural computation,
9(8):1735–1780.

Igor Labutov, Sumit Basu, and Lucy Vanderwende.
2015. Deep questions without deep understand-
ing. In Proceedings of the 53rd Annual Meeting of
the Association for Computational Linguistics and
the 7th International Joint Conference on Natural
Language Processing, page 889898. Association for
Computational Linguistics.

Jiwei Li and Dan Jurafsky. 2016. Mutual information
and diverse decoding improve neural machine trans-
lation. arXiv preprint arXiv:1601.00372.

Chin-Yew Lin. 2004. Rouge: A package for auto-
matic evaluation of summaries. In Stan Szpakowicz
Marie-Francine Moens, editor, Text Summarization
Branches Out: Proceedings of the ACL-04 Work-
shop. Association for Computational Linguistics.

Minh-Thang Luong, Hieu Pham, and Christopher D
Manning. 2015. Effective approaches to attention-
based neural machine translation. arXiv preprint
arXiv:1508.04025.

Karen Mazidi and Rodney D. Nielsen. 2014. Lin-
guistic considerations in automatic question genera-
tion. In Proceedings of the 52nd Annual Meeting of
the Association for Computational Linguistics, page
321326. Association for Computational Linguistics.



3910

Yurii Nesterov. 1983. A method for unconstrained con-
vex minimization problem with the rate of conver-
gence o (1/kˆ 2). In Doklady AN USSR, volume 269,
pages 543–547.

Tri Nguyen, Mir Rosenberg, Xia Song, Jianfeng Gao,
Saurabh Tiwary, Rangan Majumder, and Li Deng.
2016. Ms marco: A human generated machine read-
ing comprehension dataset. In Proceedings of 30th
Conference on Neural Information Processing Sys-
tems (NIPS).

Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic eval-
uation of machine translation. In Proceedings of
40th Annual Meeting of the Association for Compu-
tational Linguistics. Association for Computational
Linguistics.

Jeffrey Pennington, Richard Socher, and Christopher
Manning. 2014. Glove: Global vectors for word
representation. In Proceedings of the 2014 Con-
ference on Empirical Methods in Natural Language
Processing (EMNLP), pages 1532–1543. Associa-
tion for Computational Linguistics.

Ning Qian. 1999. On the momentum term in gradi-
ent descent learning algorithms. Neural networks,
12(1):145–151.

Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and
Percy Liang. 2016. Squad: 100,000+ questions for
machine comprehension of text. In Proceedings of
the 2016 Conference on Empirical Methods in Nat-
ural Language Processing (EMNLP).

Vasile Rus, Brendan Wyse, Paul Piwek, Mihai Lin-
tean, Svetlana Stoyanchev, and Cristian Moldovan.
2010. The first question generation shared task eval-
uation challenge. In Proceedings of the 6th Inter-
national Natural Language Generation Conference,
pages 251–257. Association for Computational Lin-
guistics.

Abigail See, Peter J Liu, and Christopher D Man-
ning. 2017. Get to the point: Summarization
with pointer-generator networks. arXiv preprint
arXiv:1704.04368.

Shikhar Sharma, Layla El Asri, Hannes Schulz, and
Jeremie Zumer. 2017. Relevance of unsupervised
metrics in task-oriented dialogue for evaluating nat-
ural language generation. CoRR, abs/1706.09799.

Heung-Yeung Shum, Xiaodong He, and Di Li. 2018.
From eliza to xiaoice: challenges and oppor-
tunities with social chatbots. arXiv preprint
arXiv:1801.01957.

Linfeng Song, Zhiguo Wang, and Wael Hamza. 2017.
A unified auery-based generative model for question
generation and question answering. arXiv preprint
arXiv:1709.01058.

Sandeep Subramanian, Tong Wang, Xingdi Yuan, and
Adam Trischler. 2017. Neural models for key phrase
detection and question generation. arXiv preprint
arXiv:1706.04560.

Duyu Tang, Nan Duan, Tao Qin, Zhao Yan, and
Ming Zhou. 2017. Question answering and ques-
tion generation as dual tasks. arXiv preprint
arXiv:1706.02027.

Zhaopeng Tu, Zhengdong Lu, Yang Liu, Xiaohua Liu,
and Hang Li. 2016. Modeling coverage for neural
machine translation. In Association for computation
linguistics.

Oriol Vinyals, Meire Fortunato, and Navdeep Jaitly.
2015. Pointer networks. In Neural Information Pro-
cessing Systems.

Tong Wang, Xingdi (Eric) Yuan, and Adam Trischler.
2017a. A joint model for question answering and
question generation. In Learning to generate natu-
ral language workshop, ICML 2017.

Wenhui Wang, Nan Yang, Furu Wei, Baobao Chang,
and Ming Zhou. 2017b. Gated self-matching net-
works for reading comprehension and question an-
swering. In Proceedings of the 55th Annual Meet-
ing of the Association for Computational Linguistics
(Volume 1: Long Papers), volume 1, pages 189–198.

Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V.
Le, Mohammad Norouzi, Wolfgang Macherey,
Maxim Krikun, Yuan Cao, Qin Gao, Klaus
Macherey, Jeff Klingner, Apurva Shah, Melvin
Johnson, Xiaobing Liu, ukasz Kaiser, Stephan
Gouws, Yoshikiyo Kato, Taku Kudo, Hideto
Kazawa, Keith Stevens, George Kurian, Nishant
Patil, Wei Wang, Cliff Young, Jason Smith, Jason
Riesa, Alex Rudnick, Oriol Vinyals, Greg Corrado,
Macduff Hughes, and Jeffrey Dean. 2016. Google’s
neural machine translation system: Bridging the gap
between human and machine translation. CoRR,
abs/1609.08144.

Xingdi Yuan, Tong Wang, Caglar Gulcehre, Alessan-
dro Sordoni, Philip Bachman, Sandeep Subrama-
nian, Saizheng Zhang, and Adam Trischler. 2017.
Machine comprehension by text-to-text neural ques-
tion generation. arXiv preprint arXiv:1705.02012.

Qingyu Zhou, Nan Yang, Furu Wei, Chuanqi Tan,
Hangbo Bao, and Ming Zhou. 2017. Neural ques-
tion generation from text: A preliminary study.
In National CCF Conference on Natural Language
Processing and Chinese Computing, pages 662–671.
Springer.


