



















































WikiKreator: Improving Wikipedia Stubs Automatically


Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics
and the 7th International Joint Conference on Natural Language Processing, pages 867–877,

Beijing, China, July 26-31, 2015. c©2015 Association for Computational Linguistics

WikiKreator: Improving Wikipedia Stubs Automatically

Siddhartha Banerjee
The Pennsylvania State University

Information Sciences and Technology
University Park, PA, USA
sub253@ist.psu.edu

Prasenjit Mitra
Qatar Computing Research Institute

Hamad Bin Khalifa University
Doha, Qatar

pmitra@qf.org.qa

Abstract

Stubs on Wikipedia often lack comprehen-
sive information. The huge cost of edit-
ing Wikipedia and the presence of only a
limited number of active contributors curb
the consistent growth of Wikipedia. In this
work, we present WikiKreator, a system
that is capable of generating content au-
tomatically to improve existing stubs on
Wikipedia. The system has two compo-
nents. First, a text classifier built using
topic distribution vectors is used to as-
sign content from the web to various sec-
tions on a Wikipedia article. Second, we
propose a novel abstractive summariza-
tion technique based on an optimization
framework that generates section-specific
summaries for Wikipedia stubs. Experi-
ments show that WikiKreator is capable of
generating well-formed informative con-
tent. Further, automatically generated con-
tent from our system have been appended
to Wikipedia stubs and the content has
been retained successfully proving the ef-
fectiveness of our approach.

1 Introduction

Wikipedia provides comprehensive information
on various topics. However, a significant percent-
age of the articles are stubs1 that require exten-
sive effort in terms of adding and editing content
to transform them into complete articles. Ideally,
we would like to create an automatic Wikipedia
content generator, which can generate a compre-
hensive overview on any topic using available in-
formation from the web and append the gener-
ated content to the stubs. Addition of automati-
cally generated content can provide a useful start-

1https://en.wikipedia.org/wiki/
Wikipedia:Stub

ing point for contributors on Wikipedia, which can
be improved upon later.

Several approaches to automatically generate
Wikipedia articles have been explored (Sauper
and Barzilay, 2009; Banerjee et al., 2014; Yao
et al., 2011). To the best of our knowledge, all
the above mentioned methods identify informa-
tion sources from the web using keywords and
directly use the most relevant excerpts in the fi-
nal article. Information from the web cannot
be directly copied into Wikipedia due to copy-
right violation issues (Banerjee et al., 2014).
Further, keyword search does not always sat-
isfy information requirements (Baeza-Yates et al.,
1999). To address the above-mentioned issues,
we present WikiKreator – a system that can au-
tomatically generate content for Wikipedia stubs.
First, WikiKreator does not operate using keyword
search. Instead, we use a classifier trained using
topic distribution features to identify relevant con-
tent for the stub. Topic-distribution features are
more effective than keyword search as they can
identify relevant content based on word distribu-
tions (Song et al., 2010). Second, we propose a
novel abstractive summarization (Dalal and Malik,
2013) technique to summarize content from mul-
tiple snippets of relevant information.2

Figure 1 shows a stub that we attempt to im-
prove using WikiKreator. Generally, in stubs, only
the introductory content is available; other sec-
tions (s1, ..., sr) are absent. The stub also belongs
to several categories (C1,C2, etc. in Figure) on
Wikipedia. In this work, we address the following
research question: Given the introductory content,
the title of the stub and information on the cate-
gories - how can we transform the stub into a com-

2An example of our system’s output can be found
here – https://en.wikipedia.org/wiki/2014_
Enterovirus_D68_outbreak – content was added on
5th Jan, 2015. The sections on Epidemiology, Causes and
Prevention have been added using content automatically gen-
erated by our method.

867



����������	
���������

��������������������������������������������������������������������������������������������������� ������������

����������������������	
�������
����������

���������������
�

�

��������������������������������������������������������������������������������������

������������������������
�
�������
�
�

���������������������������

���������������
�

�

��������������������������������������������������������������������������������������

���������������������������
�
�������
�
�

������������������������

����������

•
•
•
•

�������������������� ��������	
�
�
�
���������������������������

������������
�

���
�

���
�

�����������

�� ��������	�
���������
	�������	�	���	���������	������	��
�

�� ����	���	�������
��������	�
�����	���	
��	�����
��

�� �����	������������	�
��������� ������!�
���"�	��	���	�������������

������������	
������	
����	��������
����������
���������������������������������

������������	
������	
����	��������������������������
�������������������	�	
��

��������	��������
����������
��������������������	�	
��

��
�

��
�

���������	
���	�������

Figure 1: Overview of our word-graph based generation (left) to populate Wikipedia template (right)

prehensive Wikipedia article?
Our proposed approach consists of two stages.

First, a text classifier assigns content retrieved
from the web into specific sections of the
Wikipedia article. We train the classifier using
a set of articles within the same category. Cur-
rently, we limit the system to learn and assign
content into the 10 most frequent sections in any
given category. The training set includes con-
tent from the most frequent sections as instances
and their corresponding section titles as the class
labels. We extract topic distribution vectors us-
ing Latent Dirichlet Allocation (LDA) (Blei et al.,
2003) and use the features to train a Random For-
est (RF) Classifier (Liaw and Wiener, 2002). To
gather web content relevant to the stub, we for-
mulate queries and retrieve top 20 search results
(pages) from Google. We use boilerplate detec-
tion (Kohlschütter et al., 2010) to retain the im-
portant excerpts (text elements) from the pages.
The RF classifier classifies the excerpts into one of
the most frequent classes (section titles). Second,
we develop a novel Integer Linear Programming
(ILP) based abstractive summarization technique
to generate text from the classified content. Previ-
ous work only included the most informative ex-
cerpt in the article (Sauper and Barzilay, 2009); in
contrast, our abstractive summarization approach
minimizes loss of information that should ideally
be in an Wikipedia article by fusing content from
several sentences. As shown in Figure 1, we con-
struct a word-graph (Filippova, 2010) using all the
sentences (WG1) assigned to a specific class (Epi-

demiology) by the classifier. Multiple paths (sen-
tences) between the start and end nodes in the
graph are generated (WG2). We represent the gen-
erated paths as variables in the ILP problem. The
coefficients of each variable in the objective func-
tion of the ILP problem is obtained by combin-
ing the information score and the linguistic quality
score of the path. We introduce several constraints
into our ILP model. We limit the summary for
each section to a maximum of 5 sentences. Fur-
ther, we avoid redundant sentences in the summary
that carry similar information. The solution to the
optimization problem decides the paths that are se-
lected in the final section summary. For example,
in Figure 1, the final paths determined by the ILP
solution, – 1 and 2 in WG2, are assigned to a sec-
tion (sr), where (sr) is the section title Epidemiol-
ogy.

To the best of our knowledge, this work is
the first to address the issue of generating con-
tent automatically to transform Wikipedia stubs
into comprehensive articles. Further, we address
the issue of abstractive text summarization for
Wikipedia content generation. We evaluate our
approach by generating articles in three differ-
ent categories: Diseases and Disorders3, Amer-
ican Mathematicians4 and Software companies
of the United States5. Our LDA-based classi-

3
https://en.wikipedia.org/wiki/Category:

Diseases_and_disorders
4
https://en.wikipedia.org/wiki/Category:

American_mathematicians
5
https://en.wikipedia.org/wiki/Category:

Software_companies_of_the_United_States

868



fier outperforms a TFIDF-based classifier in all
the categories. We use ROUGE (Lin, 2004) to
compare content generated by WikiKreator and
the corresponding Wikipedia articles. The re-
sults of our evaluation confirm the benefits of us-
ing abstractive summarization for content genera-
tion over approaches that do not use summariza-
tion. WikiKreator outperforms other comparable
approaches significantly in terms of content selec-
tion. On ROUGE-1 scores, WikiKreator outper-
forms the perceptron-based baseline (Sauper and
Barzilay, 2009) by ∼20%. We also analyze re-
viewer reactions, by appending content into sev-
eral stubs on Wikipedia, most of which (∼77%)
have been retained by reviewers.

2 Related Work

Wikipedia has been used to compute semantic re-
latedness (Gabrilovich and Markovitch, 2007), in-
dex topics (Medelyan et al., 2008), etc. How-
ever, the problem of enhancing the content of
a Wikipedia article has not been addressed ad-
equately. Learning structures of templates from
the Wikipedia articles have been attempted in the
past (Sauper and Barzilay, 2009; Yao et al., 2011).
Both these efforts use queries to extract excerpts
from the web and the excerpts ranked as the most
relevant are added into the article. However, as al-
ready pointed out, current standards of Wikipedia
requires rewriting of web content to avoid copy-
right violation issues.

To address the issue of copyright violation,
multi-document abstractive summarization is re-
quired. Various abstractive approaches have been
proposed till date (Nenkova et al., 2011). How-
ever, these methods suffer from severe deficien-
cies. Template-based summarization methods
work well, but, it assumes prior domain knowl-
edge (Li et al., 2013). Writing style across ar-
ticles vary widely; hence learning templates au-
tomatically is difficult. In addition, such tech-
niques require handcrafted rules for sentence re-
alization (Gerani et al., 2014). Alternatively, we
can use text-to-text generation (T2T) (Ganitke-
vitch et al., 2011) techniques. WikiKreator con-
structs a word-graph structure similar to (Filip-
pova, 2010) using all the sentences that are as-
signed to a particular section by a text classifier.
Multiple paths (sentences) from the graph are gen-
erated. WikiKreator selects few sentences from
this set of paths using an optimization problem
formulation that jointly maximizes the informa-

��������	���
	���	��
�
�

�
�

�
�

�
� �

�

�
�

�
�

�
�

�
�

�
�

�����	�
��������
����	���


��������	�����
�����
����
� ���

���������	
�������

���������	��������������

��	�����

� ������	��

��������� 
	�����!	����

"�� 
#���������

"�� 
����	����
� $�"
%����

����������	�
�

�����������	
�������


��
�
���
�����
��
�������
�

�


��
�
���
�����
��
�������
�

�


��
�
���
�����
��
�������
�

�

���������	
���

�������	
���
�
����	
���

�����������	
���

�������	
�������	�

��� ��	���

�������	

�		�����


�������	

���


��	 


��	����

�����
�!%%��&
���
��	����

Figure 2: WikiKreator System Architecture: Con-
tent Retrieval and Content Summarization

tiveness and readability of section-specific snip-
pets and generates output that is informative, well-
formed and readable.

3 Proposed Approach

Figure 2 shows the system architecture of
WikiKreator. We are required to generate content
to populate sections of the stubs (S1, S2, etc.) that
belong to category C1. Categories on Wikipedia
group together pages on similar subjects. Hence,
categories characterize Wikipedia articles surpris-
ingly well (Zesch and Gurevych, 2007). Naturally,
we leverage knowledge existing in the categories
to build our text classifier. To learn category spe-
cific templates, the system should learn from ar-
ticles contained within the same or similar cate-
gories. WikiKreator learns category-specific tem-
plates using all the articles that can be reached us-
ing a top-down approach from the particular cate-
gory. For example, in addition to C1, WikiKreator
also learns templates from articles in C2 and C3
(the subcategories of C1). As shown in the Fig-
ure 2, we deploy a two stage process to generate
content for a stub:
[i] Content Retrieval and
[ii] Content Summarization.
In the first stage, our focus is to retrieve content
that is relevant to the stub, say, S1 that belongs
to C1. We extract all the articles that belong to C1
and the subcategories, namely, C2 and C3. A train-
ing set is created with the contents in the sections
of the articles as instances and the section titles as
the corresponding classes. Topic distribution vec-
tors for each section content are generated using
LDA (Blei et al., 2003). We train a Random Forest

869



(RF) classifier using the topic distribution vectors.
As mentioned earlier, only the top 10 most fre-
quent sections are considered for the multi-class
classification task. We retrieve relevant excerpts
from the web by formulating queries. The topic
model infers the topic distribution features of each
excerpt and the RF classifier predicts the section
(s1, s2, etc.) of the excerpt. All web automation
tasks are performed using HTMLUnit6. In the sec-
ond stage, our ILP based summarization approach
synthesizes information from multiple excerpts as-
signed to a section and presents the most informa-
tive and linguistically well-formed summary as the
corresponding content for each section. A word-
graph is constructed that generates several sen-
tences; only a few of the sentences are retained
based on the ILP solution. The predicted section
is entered in the stub article along with the final
sentences selected by the ILP solution as the cor-
responding section-specific content on Wikipedia.

3.1 Content Retrieval
Article Extraction: Wikipedia provides an API7
to download articles in the XML format. Given a
category, the API is capable of extracting all the
articles under it. We recursively extract articles
by identifying all the categories in the hierarchy
that can be reached by the crawler using top-down
traversal. We use a simple python script8 to ex-
tract the section titles and the corresponding text
content from the XML dump.
Classification model: WikiKreator uses Latent
Dirichlet Allocation (LDA) to represent each doc-
ument as a vector of topic distributions. Each
topic is further represented as a vector of proba-
bilities of word distributions. Our intuition is that
the topic distribution vectors of the same sections
across different articles would be similar. Our ob-
jective is to learn these topic representations, such
that we can accurately classify any web excerpt by
inferring the topics in the text. Say C, a category
on Wikipedia, has k Wikipedia articles (W ).

(C) = {W1, W2, W3, W4, ..., Wk}
Each article Wj has several sections denoted as
sjicji where sji and cji refer to the section title and
content of the ith section in the jth article, respec-
tively. We concentrate on the 10 most frequent

6http://htmlunit.sourceforge.net/
7https://en.wikipedia.org/wiki/

Special:Export
8http://medialab.di.unipi.it/wiki/

Wikipedia_Extractor

sections in any category. Training using content
from sections that are not frequent might result in
sub-optimal classification models. In our experi-
ments, each frequent section had enough instances
to optimally train a classifier. Let us denote the 10
most frequent sections in any category as S. If
any sji from Wj exists in S, the content (cji) is
included in the training set along with the section
title (sji) as the corresponding class label. These
steps are repeated for all the articles in the cate-
gory. Each instance is then represented as:

cji = {pji(t1), pji(t2), pji(t3), . . . , pji(tm)}
where m is the number of topics. sji is the cor-
responding label for this training instance. The
set of topics are t1, t2, t3,. . ., tm while pji(tm)
refers to the probability of topic m of content cji.
Contents from the most frequent sections are each
considered as a document and LDA is applied to
generate document-topic distributions. We exper-
iment with several values of m and use the value
that generates the best classification model in each
category. The topic vectors and the correspond-
ing labels are used to train a Random Forest (RF)
classifier. As the classes might be unbalanced, we
apply resampling on the training set.
Predicting sections: In this step, we search the
web for relevant content on the stub and assign
them to their respective sections. We formulate
search queries to retrieve web pages using a search
engine. We extract multiple excerpts from the
pages and then the RF classifier predicts the class
(section label) for each excerpt.
(i) Query Generation: To search the web, we
formulate queries by combining the stub title and
keyphrases extracted from the first sentence of the
introductory content of the stub. The first sen-
tence generally contains the most important key-
words that represent the article. Focused queries
increases relevance of extraction as well as helps
in disambiguation of content. We use the topia
term extractor (Chatti et al., 2014) to extract
keyphrases. For example, the query generated for
a stub on Hereditary hyperbilirubinemia is Hered-
itary hyperbilirubinemia bilirubin metabolic dis-
order where bilirubin metabolic disorder are the
keyphrases generated from the first sentence of the
stub from Wikipedia. The query is used to identify
the top 20 URLs (search results) from Google9.
(ii) Boilerplate removal: Web content from the
search results obtained in the previous step re-

9http://www.google.com

870



quires cleaning to retain only the relevant informa-
tion. Removal of irrelevant content is done using
boilerplate detection (Kohlschütter et al., 2010).
The web pages contain several excerpts (text el-
ements) in between the HTML tags. Only the ex-
cerpts that are classified as relevant by the boiler-
plate detection technique are retained.

(iii) Classification and assignment of excerpts:
The LDA model generated earlier infers topic dis-
tribution of each excerpt based on word distribu-
tions. The RF classifier predicts the class (section
title) for each excerpt based on the topic distribu-
tion. However, predictions that do not have a high
level of confidence might lead to excerpts being
appended to inappropriate sections. Therefore, we
set the minimum confidence level at 0.5. If the
prediction confidence of the RF classifier for a par-
ticular excerpt is above the minimum confidence
level, the excerpt is assigned to the class; other-
wise, the excerpt is discarded.

In the next step, we apply summarization on the
excerpts assigned to each section.

3.2 Content Summarization

To summarize content for Wikipedia effectively,
we formulate an ILP problem to generate abstrac-
tive summaries for each section with the objective
of maximizing linguistic quality and information
content.
Word-graph: A word-graph is constructed using
all the sentences included in the excerpts assigned
to a particular section. We used the same tech-
nique to construct the word-graph as (Filippova,
2010) where the nodes represent the words (along
with parts-of-speech (POS)) and directed edges
between the nodes are added if the words are adja-
cent in the input sentences. Each sentence is con-
nected to dummy start and end nodes to mark the
beginning and ending of the sentences. The sen-
tences from the excerpts are added to the graph
in an iterative fashion. Once the first sentence
is added, words from the following sentences are
mapped onto a node in the graph provided that
they have the exact same word form and the same
POS tag. Inclusion of POS information prevents
ungrammatical mappings. The words are added to
the graph in the following order:

• Content words are added for which there are
no candidates in the existing graph;

• Content words for which multiple mappings
are possible or such words that occur more

than once in the sentence;

• Stopwords.

If multiple mappings are possible, the context of
the word is checked using word overlaps to the left
and right within a window of two words. Even-
tually, the word is mapped to that node that has
the highest context. We also changed Filippova’s
method by adding punctuations as nodes to the
graph. Figure 1 shows a simple example of the
word-graph generation technique. We do not show
POS and punctuations in the figure for the sake of
clarity. The Figure also shows that several pos-
sible paths (sentences) exist between the dummy
start and end nodes in the graph. Ideally, excerpts
for any section would contain multiple common
words as they belong to the same topic and have
been assigned the same section. The presence of
common words ensure that new sentences can be
generated from the graph by fusing original set of
sentences in the graph. Figure 1 shows an illus-
tration of our approach where the set of sentences
assigned to a particular section (WG1) are used to
create the word-graph. The word-graph generates
several possible paths between the dummy nodes;
we show only three such paths (WG2). To obtain
abstractive summaries, we remove generated paths
from the graph that are same or very similar to any
of the original sentences. If the cosine similarity
of a generated path to any of the original sentences
is greater than 0.8, we do not retain the path. We
compute cosine similarity after applying stopword
removal. However, we do not apply stemming as
our graph construction is based on words existing
in the same form in multiple sentences. Similar to
Filippova’s work, we set the minimum path length
(in words) to eight to avoid incomplete sentences.
Paths without verbs are discarded. The final set of
generated paths after discarding the ineligible ones
are used in the next step of summary generation.

3.2.1 ILP based Path Selection
Our goal is to select paths that maximize the in-
formativeness and linguistic quality of the gener-
ated summaries. To select the best multiple pos-
sible sentences, we apply an overgenerate and se-
lect (Walker et al., 2001) strategy. We formulate
an optimization problem that ‘selects’ a few of
the many generated paths in between the dummy
nodes from the word-graph. Let pi denote each
path obtained from the word-graph. We introduce
three different factors to judge the relevance of

871



a path – Local informativeness (I loc(pi)), Global
informativeness (Iglob(pi)) and Linguistic quality
(LQ(pi)). Any sentence path should be relevant
to the central topic of the article; this relevance
is tackled using Iglob(pi). I loc(pi) models the
importance of a sentence among several possible
sentences that are generated from the word-graph.
Linguistic quality (LQ(pi)) is computed using a
trigram language model (Song and Croft, 1999)
that assigns a logarithmic score of probabilities of
occurrences of three word sequences in the sen-
tences.
Local Informativeness: In principle, we can use
any existing method that computes sentence im-
portance to account for Local Informativeness. In
our model, we use TextRank scores (Mihalcea and
Tarau, 2004) to generate an importance value of
each path. TextRank creates a graph of words from
the sentences. The score of each node in the graph
is calculated as shown in Equation (1):

S(Vi) = (1− d) + d×
∑

Vj∈adj(Vi)
wji∑

Vk∈adj(Vi) wjk
S(Vi)

(1)
where Vi represents the words and adj(Vi) denotes
the adjacent nodes of Vi. Setting d to 0.80 in our
experiments provided the best content selection re-
sults. The computation convergences to return fi-
nal word importance scores. The informativeness
score of a path I loc(pi) is obtained by adding the
importance scores of the individual words in the
path.
Global Informativeness: To compute global
informativeness, we compute the relevance of a
sentence with respect to the query to assign higher
weights to sentences that explicitly mention the
main title or mention certain keywords that are
relevant to the article. We compute the cosine
similarity using TFIDF features between each
sentence and the original query that was formu-
lated during the web search stage. We define
global informativeness as follows:

Iglob(pi) = CosineSimilarity(Q, pi) (2)

where Q denotes the formulated query.
Linguistic Quality: In order to compute Linguis-
tic quality, we use a language model that assigns
probabilities to sequence of words to compute lin-
guistic quality. Suppose a path contains a se-
quence of q words {w1, w2, ..., wq}. The score
LQ(pi) assigned to each path is defined as fol-

lows:

LQ(pi) = 11−LL(w1,w2,...,wq) , (3)

where LL(w1, w2, ..., wq) is defined as:

LL(w1, . . . , wq) = 1L · log2
∏q

t=3 P (wt|wt−1wt−2).
(4)

As can be seen from Equation (4), we com-
bine the conditional probability of different sets
of 3-grams (trigrams) in the sentence and aver-
aged the value by L – the number of conditional
probabilities computed. The LL(w1, w2, . . . , wq)
scores are negative; with higher magnitude imply-
ing lower importance. Therefore, in Equation (3),
we take the reciprocal of the logarithmic value
with smoothing to compute LQ(pi). In our exper-
iments, we used a 3-gram model10 that is trained
on the English Gigaword corpus. Trigram models
have been successfully used in several text-to-text
generation tasks (Clarke and Lapata, 2006; Filip-
pova and Strube, 2008) earlier.
ILP Formulation: To select the best paths, we
combine all the above mentioned factors I loc(pi),
Iglob(pi) and linguistic quality LQ(pi) in an opti-
mization framework. We maximize the following
objective function:

F (p1, . . . , pK) =
∑K

i=1
1

T (pi)
· I loc(pi) · Iglob(pi) · LQ(pi) · pi

(5)
where K represents the total number of generated
paths. Each pi represents a binary variable, that
can be either 0 or 1, depending on whether the path
is selected in the final summary or not. In addition,
T (pi) – the number of tokens in a path, is included
in the objective function. The term 1T (pi) normal-
izes the Textrank scores by the length of the sen-
tences. First, we ensure that a maximum of Smax
sentences are selected in the summary using Equa-
tion (6).

K∑
i=1

pi ≤ Smax (6)

In our experiments, we set Smax to 5 to generate
short concise summaries in each section. Using a
length constraint enables us to only populate the
sections using the most informative content. We
introduce Equation (7) to prevent similar informa-
tion (cosine similarity≥ 0.5) from being conveyed

10The model is available here: http://www.keithv.
com/software/giga/. We used the VP 20K vocab ver-
sion.

872



Category Most Frequent Sections
American Mathematicians Awards, Awards and honors, Biography, Books, Career, Education, Life, Publications, Selected publications, Work
Diseases and Disorders Causes, Diagnosis, Early life, Epidemiology, History, Pathophysiology, Prognosis, Signs and symptoms, Symptoms, Treatment
US Software companies Awards, Criticism, Features, Games, History, Overview, Products, Reception, Services, Technology

Table 1: Data characteristics of three domains on Wikipedia

Category #Articles #Instances
American Mathematicians ∼ 2100 1493
Diseases and Disorders ∼ 7000 9098
US Software companies ∼ 3600 2478

Table 2: Dataset used for classification

by different sentences. This constraint reduces re-
dundancy. If two sentences have a high degree of
similarity, only one out of the two can be selected
in the summary.

∀i, i′ ∈ [1, K], i 6= i′,
pi + pi′ ≤ 1 if sim(pi, pi′) ≥ 0.5.

(7)

The ILP problem is solved using the Gurobi op-
timizer (2015). The solution to the problem de-
cides the paths that should be included in the final
summary. We populate the sections on Wikipedia
using the final summaries generated for each sec-
tion along with the section title. All the refer-
ences that have been used to generate the sen-
tences are appended along with the content gen-
erated on Wikipedia.

4 Experimental Results

To evaluate the effectiveness of our proposed tech-
nique, we conduct several experiments. First, we
evaluate our content generation approach by gen-
erating content for comprehensive articles that al-
ready exist on Wikipedia. Second, we analyze re-
viewer reactions on our system generated articles
by adding content to several stubs on Wikipedia.
Our experiments were designed to answer the fol-
lowing questions:
(i)What are the optimal number of topic distribu-
tion features for each category? What are the clas-
sification accuracies in each domain?
(ii)To what extent can our technique generate the
content for articles automatically?
(iii)What are the general reviewer reactions on
Wikipedia and what percentage of automatically
generated content on Wikipedia is retained?
Dataset Construction: As mentioned earlier
in Section 3.1, we crawl Wikipedia articles by
traversing the category graph. Articles that contain
at least three sections were included in the training
set; other articles having lesser number of sections

Figure 3: Performance of Classifier in the three
categories based on the number of topics.

are generally labeled as stubs and hence not used
for training. Table 1 shows the most frequent sec-
tions in each category. Further, Table 2 shows the
total number of articles retrieved from Wikipedia
in each category. The total number of instances are
also shown. The number of instances denotes the
total number of the most frequent sections in each
category. As can be seen from the table, the num-
ber of instances is higher than the number of arti-
cles only in case of the category on diseases. This
implies that there are generally more common sec-
tions in the diseases category than the other cate-
gories.

In each category, the content from only the most
frequent sections were used to generate a topic
model. The topic model is further used to in-
fer topic distribution vectors from the training in-
stances. We used the MALLET toolkit (McCal-
lum, 2002) for generating topic distribution vec-
tors and the WEKA package (Hall et al., 2009) for
the classification tasks.
Optimal number of topics: The LDA model re-
quires a pre-defined number of topics. We exper-
iment with several values of the number of top-
ics ranging from 10 to 100. The topic distribution
features of the content of the instances are used
to train a Random Forest Classifier with the cor-
responding section titles as the class labels. As
can be seen in the Figure 3, the classification per-
formance varies across domains as well as on the
number of topics. The optimal number of top-
ics based on the dataset are marked in blue cir-

873



Category LDA-RF SVM-WV
American Mathematicians 0.778 0.478
Diseases and Disorders 0.886 0.801
US Software companies 0.880 0.537

Table 3: Classification: Weighted F-Scores

cles (40, 50 and 20 topics for Diseases, Software
Companies in US and American mathematicians,
respectively) in the Figure. We classify web ex-
cerpts using the best performing classifiers trained
using the optimal number of topic features in each
category.
Classification performance: We use 10-fold
cross validation to evaluate the accuracy of our
classifier. According to the F-Scores, our classifier
(LDA-RF) performs similarly in the categories on
Diseases and US Software companies. However,
the accuracy is lower in the American Mathemati-
cians category. We also experimented with a base-
line classifier, that is trained on TFIDF features
(upto trigrams). A Support vector machine (Cortes
and Vapnik, 1995) classifier obtained the best per-
formance using the TFIDF features. The base-
line system is referred to as SVM-WV. We exper-
imented with several other combinations of classi-
fiers; however, we show only the best performing
systems using the LDA and TFIDF features. As
can be seen from the Table 3, our classifier (LDA-
RF) outperforms SVM-WV significantly in all the
domains. SVM-WV performs better in the cate-
gory on diseases than the other two categories and
the performance is comparable to (LDA-RF). The
diseases category has more uniformity in terms of
the section titles, hence specific words or phrases
characterize the sections well. In contrast, word
distributions (LDA) work significantly better than
TFIDF features in the other two categories.
Error Analysis: We performed error analysis to
understand the reason for misclassifications. As
can be seen from the Table 1, all the categories
have several overlapping sections. For example,
Awards and honors and Awards contain similar
content. Authors use various section names for
similar content in the articles within the same cat-
egory. We analyzed the confusion matrices, and
found that multiple instances in Awards were clas-
sified into the class of Awards and honors. Simi-
lar observations are made on the Books and Pub-
lications classes – which are related sections in
the context of academic biographies. In future,
we plan to use semantic measures to relate similar
classes automatically and group them in the same

Category System ROUGE-1 ROUGE-2
WikiKreator 0.522 0.311

American Mathematicians Perceptron 0.431 0.193
Extractive 0.471 0.254
WikiKreator 0.537 0.323

Diseases and Disorders Perceptron 0.411 0.197
Extractive 0.473 0.232
WikiKreator 0.521 0.321

US Software companies Perceptron 0.421 0.228
Extractive 0.484 0.257

Table 4: ROUGE-1 and 2 Recall values – Com-
paring system generated articles to model articles

class during classification.
Content Selection Evaluation: To evaluate the
effectiveness of our content generation process,
we generated the content of 500 randomly se-
lected articles that already exist on Wikipedia in
each of the categories. We compare WikiKreator’s
output against the current content of those ar-
ticles on Wikipedia using ROUGE (Lin, 2004).
ROUGE matches N-gram sequences that exist
in both the system generated articles and the
original Wikipedia articles (gold standard). We
also compare WikiKreator’s output with an ex-
isting Wikipedia generation system [Perceptron]
of Sauper and Barzilay (2009)11 that employs a
perceptron learning framework to learn topic spe-
cific extractors. Queries devised using the con-
junction of the document title and the section ti-
tle were used to obtain excerpts from the web
using a search engine, which were used in the
perceptron model. In Perceptron, the most im-
portant sections in the category was determined
using a bisectioning algorithm to identify clus-
ters of similar sections. To understand the ef-
fectiveness of our abstractive summarizer, we de-
sign a system (Extractive) that uses an extrac-
tive summarization module. In Extractive, we use
LexRank (Erkan and Radev, 2004) as the summa-
rizer instead of our ILP based abstractive summa-
rization model. We restrict the extractive sum-
maries to 5 sentences for accurate comparison of
both the systems. The same content was received
as input from the classifier by the Extractive as
well as our ILP-based system.

As can be seen from the Table 4, the ROUGE
scores obtained by WikiKreator is higher than that
of the other comparable systems in all the cat-
egories. The higher ROUGE scores imply that
WikiKreator is generally able to retrieve useful
information from the web, synthesize them and
present the important information in the article.

11The system is available here: https://github.
com/csauper/wikipedia

874



Statistics Count
Number of stubs edited 40
Number of stubs retained without any changes 21
Number of stubs that required minor editing 6
Number of stubs where edits were modified by reviewers 4
Number of stubs in which content was removed 9
Average change in size of stubs 515 bytes
Average number of edits made post content-addition ∼3

Table 5: Statistics of Wikipedia generation

However, it may also be noted that the Extractive
system outperforms the Perceptron framework.
Summarization from multiple sources generates
more informative summaries and is more effective
than ‘selection’ of the most informative excerpt,
which is often inadequate due to potential loss of
information. WikiKreator performs better than the
extractive system on all the categories. Our ILP-
based abstractive summarization system fuses and
selects content from multiple sentences, thereby
aggregating information successfully from multi-
ple sources. In contrast, LexRank ‘extracts’ the
top 5 sentences that results in some information
loss.
Analysis of Wikipedia Reviews: To compare our
method with the other techniques, it is necessary
to generate content and append to Wikipedia stubs
using all the techniques. However, recent work on
article generation (Banerjee et al., 2014) has al-
ready shown that content directly copied from web
sources cannot be used on Wikipedia. Further,
bots using copyrighted content might be banned
and real-users would have to read sub-standard ar-
ticles due to the internal tests we perform. Due to
the above mentioned reasons, we appended con-
tent generated only using our abstractive summa-
rization technique.

We published content generated by WikiKreator
on Wikipedia and appended the content to 40 ran-
domly selected stubs. As can be seen from the
Table 5, the content generated using our system
was generally accepted by the reviewers. Half of
the articles did not require any further changes;
while in 6 cases (15%) the reviewers asked us to
fix grammatical issues. In 9 stubs, the reliability of
the cited references was questioned. Information
sources on Wikipedia need to satisfy a minimum
reliability standard, which our algorithm currently
cannot determine. On an average, 3 edits were
made to the Wikipedia articles that we generated.
In general, there is an average increase in the con-
tent size of the stubs that we edited showing that
our method is capable of producing content that
generally satisfy Wikipedia criterion.

Analysis of section assignment: We manually in-
spected generated content of 20 articles in each
category. Generated summaries are both informa-
tive and precise. However, in certain cases, the
generated section title is not the same as the sec-
tion title in the original Wikipedia article. For
example, we generated content for the section
“Causes” for the article on Middle East Respira-
tory Syndrome (MERS)12:
Milk or meat may play a role in the transmission of the virus

. People should avoid drinking raw camel milk or meat that

has not been properly cooked . There is growing evidence

that contact with live camels or meat is causing MERS.

The corresponding content on the Wikipedia is in
a section labeled as “Transmission”. Section ti-
tles at the topmost level in a category might not be
relevant to all the articles. Instead of using a top-
down approach of traversing the category-graph,
we can also use a bottom-up approach where we
learn from all the categories that an article be-
longs to. For example, the article on MERS be-
longs to two categories: Viral respiratory tract in-
fection and Zoonoses. Training using all the cat-
egories will allow context-driven section identifi-
cation. Most frequent sections at a higher level in
the category graph might not always be relevant to
all the articles within a category.

5 Conclusions and Future Work

In this work, we presented WikiKreator that
can generate content automatically to improve
Wikipedia stubs. Our technique employes a topic-
model based text classifier that assigns web ex-
cerpts into various sections on an article. The
excerpts are summarized using a novel abstrac-
tive summarization technique that maximizes in-
formativeness and linguistic quality of the gen-
erated summary. Our experiments reveal that
WikiKreator is capable of generating well-formed
informative content. The summarization step en-
sures that we avoid any copyright violation issues.
The ILP based sentence generation strategy en-
sures that we generate novel content by synthesiz-
ing information from multiple sources and thereby
improve content selection. In future, we plan to
cluster related sections using semantic relatedness
measures. We also plan to estimate reliabilities of
sources to retrieve information only from reliable
sources.

12https://en.wikipedia.org/wiki/Middle_
East_respiratory_syndrome

875



References
Ricardo Baeza-Yates, Berthier Ribeiro-Neto, et al.

1999. Modern information retrieval, volume 463.
ACM press New York.

Siddhartha Banerjee, Cornelia Caragea, and Prasenjit
Mitra. 2014. Playscript classification and auto-
matic wikipedia play articles generation. In Pattern
Recognition (ICPR), 2014 22nd International Con-
ference on, pages 3630–3635, Aug.

David M Blei, Andrew Y Ng, and Michael I Jordan.
2003. Latent dirichlet allocation. the Journal of ma-
chine Learning research, 3:993–1022.

Mohamed Amine Chatti, Darko Dugoija, Hendrik
Thus, and Ulrik Schroeder. 2014. Learner mod-
eling in academic networks. In Advanced Learn-
ing Technologies (ICALT), 2014 IEEE 14th Interna-
tional Conference on, pages 117–121. IEEE.

James Clarke and Mirella Lapata. 2006. Constraint-
based sentence compression an integer program-
ming approach. In Proceedings of the COL-
ING/ACL on Main conference poster sessions, pages
144–151. Association for Computational Linguis-
tics.

Corinna Cortes and Vladimir Vapnik. 1995. Support-
vector networks. Machine learning, 20(3):273–297.

Vipul Dalal and Latesh Malik. 2013. A survey of
extractive and abstractive text summarization tech-
niques. In Emerging Trends in Engineering and
Technology (ICETET), 2013 6th International Con-
ference on, pages 109–110. IEEE.

Günes Erkan and Dragomir R Radev. 2004.
Lexrank: Graph-based lexical centrality as salience
in text summarization. J. Artif. Intell. Res.(JAIR),
22(1):457–479.

Katja Filippova and Michael Strube. 2008. Sentence
fusion via dependency graph compression. In Pro-
ceedings of the Conference on Empirical Methods in
Natural Language Processing, pages 177–185. As-
sociation for Computational Linguistics.

Katja Filippova. 2010. Multi-sentence compression:
Finding shortest paths in word graphs. In Proceed-
ings of the 23rd International Conference on Com-
putational Linguistics, pages 322–330. Association
for Computational Linguistics.

Evgeniy Gabrilovich and Shaul Markovitch. 2007.
Computing semantic relatedness using wikipedia-
based explicit semantic analysis. In IJCAI, vol-
ume 7, pages 1606–1611.

Juri Ganitkevitch, Chris Callison-Burch, Courtney
Napoles, and Benjamin Van Durme. 2011. Learn-
ing sentential paraphrases from bilingual parallel
corpora for text-to-text generation. In Proceedings
of the Conference on Empirical Methods in Natu-
ral Language Processing, pages 1168–1179. Asso-
ciation for Computational Linguistics.

Shima Gerani, Yashar Mehdad, Giuseppe Carenini,
T. Raymond Ng, and Bita Nejat. 2014. Abstractive
summarization of product reviews using discourse
structure. In Proceedings of the 2014 Conference on
Empirical Methods in Natural Language Processing
(EMNLP), pages 1602–1613. Association for Com-
putational Linguistics.

Inc. Gurobi Optimization. 2015. Gurobi optimizer ref-
erence manual.

Mark Hall, Eibe Frank, Geoffrey Holmes, Bernhard
Pfahringer, Peter Reutemann, and Ian H Witten.
2009. The weka data mining software: an update.
ACM SIGKDD explorations newsletter, 11(1):10–
18.

Christian Kohlschütter, Peter Fankhauser, and Wolf-
gang Nejdl. 2010. Boilerplate detection using shal-
low text features. In Proceedings of the third ACM
international conference on Web search and data
mining, pages 441–450. ACM.

Peng Li, Yinglin Wang, and Jing Jiang. 2013. Au-
tomatically building templates for entity summary
construction. Information Processing & Manage-
ment, 49(1):330–340.

Andy Liaw and Matthew Wiener. 2002. Classification
and regression by randomforest. R news, 2(3):18–
22.

Chin-Yew Lin. 2004. Rouge: A package for automatic
evaluation of summaries. In Text Summarization
Branches Out: Proceedings of the ACL-04 Work-
shop, pages 74–81.

Andrew K McCallum. 2002. {MALLET: A Machine
Learning for Language Toolkit}.

Olena Medelyan, Ian H Witten, and David Milne.
2008. Topic indexing with wikipedia. In Proceed-
ings of the AAAI WikiAI workshop, pages 19–24.

Rada Mihalcea and Paul Tarau. 2004. Textrank:
Bringing order into texts. Association for Compu-
tational Linguistics.

Ani Nenkova, Sameer Maskey, and Yang Liu. 2011.
Automatic summarization. In Proceedings of the
49th Annual Meeting of the Association for Com-
putational Linguistics: Tutorial Abstracts of ACL
2011, page 3. Association for Computational Lin-
guistics.

Christina Sauper and Regina Barzilay. 2009. Auto-
matically generating wikipedia articles: A structure-
aware approach. In Proceedings of the Joint Confer-
ence of the 47th Annual Meeting of the ACL and the
4th International Joint Conference on Natural Lan-
guage Processing of the AFNLP: Volume 1-Volume
1, pages 208–216. Association for Computational
Linguistics.

876



Fei Song and W Bruce Croft. 1999. A general lan-
guage model for information retrieval. In Proceed-
ings of the eighth international conference on In-
formation and knowledge management, pages 316–
321. ACM.

Wei Song, Yu Zhang, Ting Liu, and Sheng Li. 2010.
Bridging topic modeling and personalized search.
In Proceedings of the 23rd International Conference
on Computational Linguistics: Posters, pages 1167–
1175. Association for Computational Linguistics.

Marilyn A Walker, Owen Rambow, and Monica Ro-
gati. 2001. Spot: A trainable sentence planner.
In Proceedings of the second meeting of the North
American Chapter of the Association for Computa-
tional Linguistics on Language technologies, pages
1–8. Association for Computational Linguistics.

Conglei Yao, Xu Jia, Sicong Shou, Shicong Feng, Feng
Zhou, and HongYan Liu. 2011. Autopedia: auto-
matic domain-independent wikipedia article genera-
tion. In Proceedings of the 20th international con-
ference companion on World wide web, pages 161–
162. ACM.

Torsten Zesch and Iryna Gurevych. 2007. Analy-
sis of the wikipedia category graph for nlp applica-
tions. In Proceedings of the TextGraphs-2 Workshop
(NAACL-HLT 2007), pages 1–8.

877


