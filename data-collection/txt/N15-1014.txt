



















































HEADS: Headline Generation as Sequence Prediction Using an Abstract Feature-Rich Space


Human Language Technologies: The 2015 Annual Conference of the North American Chapter of the ACL, pages 133–142,
Denver, Colorado, May 31 – June 5, 2015. c©2015 Association for Computational Linguistics

HEADS: Headline Generation as Sequence Prediction Using an Abstract
Feature-Rich Space

Carlos A. Colmenares∗
Google Inc.

Brandschenkestrasse 110
8002 Zurich, Switzerland
crcarlos@google.com

Marina Litvak
Shamoon College

of Engineering
Beer Sheva, Israel
marinal@sce.ac.il

Amin Mantrach Fabrizio Silvestri
Yahoo Labs.

Avinguda Diagonal 177
08018 Barcelona, Spain

{amantrach,silvestr}@yahoo-inc.com

Abstract

Automatic headline generation is a sub-task
of document summarization with many re-
ported applications. In this study we present
a sequence-prediction technique for learning
how editors title their news stories. The intro-
duced technique models the problem as a dis-
crete optimization task in a feature-rich space.
In this space the global optimum can be found
in polynomial time by means of dynamic pro-
gramming. We train and test our model on an
extensive corpus of financial news, and com-
pare it against a number of baselines by us-
ing standard metrics from the document sum-
marization domain, as well as some new ones
proposed in this work. We also assess the
readability and informativeness of the gener-
ated titles through human evaluation. The ob-
tained results are very appealing and substan-
tiate the soundness of the approach.

1 Introduction

Document summarization, also known as text sum-
marization, is the process of automatically abridging
text documents. Although traditionally the final ob-
jective of text summarization is to produce a para-
graph or abstract that summarizes a rather large col-
lection of texts (Mani and Maybury, 1999; Das and
Martins, 2007; Nenkova and McKeown, 2012), the
task of producing a very short summary comprised
of 10–15 words has also been broadly studied. There
have been many reported practical applications for
this endeavor, most notably, efficient web browsing

∗Work done during an internship at Yahoo Labs.

on hand-held devices (Buyukkokten et al., 2001),
generation of TV captions (Linke-Ellis, 1999), dig-
itization of newspaper articles that have uninforma-
tive headlines (De Kok, 2008), and headline gener-
ation in one language based on news stories written
in another (Banko et al., 2000; Zajic et al., 2002).

In general terms, a headline of a news article can
be defined as a short statement that gives a reader
a general idea about the main contents of the story
it entitles (Borko and Bernier, 1987; Gattani, 2007).
The objective of our study is to develop a novel tech-
nique for generating informative headlines for news
articles, albeit to conduct experiments we focused
on finance articles written in English. In this work
we make a number of contributions concerning sta-
tistical models for headline generation, training of
the models, and their evaluation, specifically:

• We propose a model that learns how an edi-
tor generates headlines for news articles, where
a headline is regarded as a compression of
its article’s text. Our model significantly dif-
fers from others in the way it represents pos-
sible headlines in a feature-rich space. The
model tries to learn how humans discern be-
tween good and bad compressions. Further-
more, our model can be trained with any mono-
lingual corpus consisting of titled articles, be-
cause it does not request special conditions on
the headlines’ structure or provenance.

• We suggest a slight change of the Mar-
gin Infused Relaxed Algorithm (Crammer and
Singer, 2003) to fit our model, which yields bet-
ter empirical results.

133



• We present a simple and elegant algorithm that
runs in polynomial time and finds the global
optimum of our objective function. This rep-
resents an important advantage of our pro-
posal because many former techniques resort to
heuristic-driven search algorithms that are not
guaranteed to find the global optimum.

• With the intention of overcoming several prob-
lems suffered by traditional metrics for auto-
matically evaluating the quality of proposed
headlines, we propose two new evaluation met-
rics that correlate with ratings given by human
annotators.

2 Related work

There has been a significant amount of research
about headline generation. As noted by Gattani
(2007), it is possible to identify three main trends of
techniques broadly employed through different stud-
ies:

Rule-based approaches. These methods make
use of handcrafted linguistically-based rules for de-
tecting or compressing important parts in a docu-
ment. They are simple and lightweight, but fail at
exploring complex relationships in the text. The
most representative model for this group is the
Hedge Trimmer (Dorr et al., 2003).

Statistics-based approaches. These methods
make use of statistical models for learning correla-
tions between words in headlines and in the articles.
The models are fit under supervised learning envi-
ronments and therefore need large amounts of la-
belled data. One of the most influential works in this
category is the Naı̈ve Bayes approach presented by
Banko et al. (2000), and augmented in works such as
Jin and Hauptmann (2001; Zajic et al. (2002). The
use of statistical models for learning pruning-rules
for parse trees has also been studied, the most no-
table work on this area is presented in Knight and
Marcu (2001) and extended by Unno et al. (2006).

Summarization-based approaches. Headlines
can be regarded as very short summaries, therefore
traditional summarization methods could be adapted
for generating one-line compressions; the common
trend consists in performing multiple or combined
steps of sentence selection and compression (Ha-
jime et al., 2013; Martins and Smith, 2009). The

main problem with these approaches is that they
make use of techniques that were not initially
devised for generating compressions of less than
10% of the original content, which directly affects
the quality of the resulting summary (Banko et
al., 2000). It is noteworthy to highlight that most
of the modern summarization-based techniques
opt for generating headlines just by recycling and
reordering words present in the article, which also
raises the risk of losing or changing the contextual
meaning of the reused words (Berger and Mittal,
2000).

An area that deals with a target similar to headline
generation is multi-sentence compression, where its
objective is to produce a single short phrase that
abridges a set of sentences that conform a document.
The main difference between both practices is that
headline generation is more strict about the length of
the generated output, which should consist of about
eight tokens (Banko et al., 2000), whereas the latter
accepts longer results. One of the most recent and
competitive approaches for multi-sentence compres-
sion is described by Filippova (2010).

3 Background on sequence prediction

Sequence models have been broadly used for many
Natural Language Processing tasks, such as iden-
tification of sentence boundaries (Reynar and Rat-
naparkhi, 1997), named entity recognition (McCal-
lum and Li, 2003), part of speech tagging (Kupiec,
1992), dependency tree parsing (McDonald et al.,
2005), document summarization (Shen et al., 2007),
and single-sentence compression (McDonald, 2006;
Nomoto, 2007). These models are formalizations of
relationships between observed sequences of vari-
ables and predicted categories for each one. Math-
ematically, let X = {x1, x2, ..., xN} be a finite
set of possible atomic observations, and let Y =
{y1, y2, ..., yM} be a finite set of possible categories
that each atomic observation could belong to.

Statistical sequence models try to approximate a
probability distribution P with parameters φ capa-
ble of predicting for any sequence of n observations
x ∈ X n, and any sequence of assigned categories
per observation y ∈ Yn, the probability P (y|x;φ).
The final objective of these models is to predict the

134



most likely sequence of categories ŷ ∈ Yn for any
arbitrary observation sequence, which can be ex-
pressed as:

ŷ = arg max
y∈Yn

P (y|x;φ)

There have been many proposals for modelling the
probability distribution P . Some of the most pop-
ular proposals are Hidden Markov Models (Rabiner
and Juang, 1986), local log-linear classifiers, Max-
imum Entropy Markov Models (McCallum et al.,
2000), and Conditional Random Fields (Lafferty et
al., 2001). The following two sections will briefly
introduce the latter, together with a widely used im-
provement of the model.

3.1 Conditional Random Fields (CRF)
As presented by Lafferty et al. (2001), CRF are
sequence prediction models where no Markov as-
sumption is made on the sequence of assigned cate-
gories y, but a factorizable global feature function is
used so as to transform the problem into a log-linear
model in feature space. Formally, CRF model the
probability of a sequence in the following way:

P (y|x;φ) = exp{w · F (x,y)}
Z(x)

Where φ = {w} and w ∈ Rm is a weight vec-
tor, F : X n × Yn → Rm is a global feature func-
tion of m dimensions, and Z(x) is a normalization
function. Moreover, the global feature function is
defined in the following factored way:

F (x,y) =
n∑

i=1

f(x, i, yi-1, yi)

where f : X ∗×N+×Y×Y → Rm is a local feature
function. Due to this definition, it can be shown that
the decoding of CRF is equivalent to:

ŷ = arg max
y∈Yn

w · F (x,y)

Which is a linear classification in a feature space.
The fact that the local feature function f only de-
pends on the last two assigned categories allows the
global optimum of the model to be found by means
of a tractable algorithm, whereas otherwise it would
be necessary to explore all the |Y|n possible solu-
tions.

3.2 CRF with state sequences

Since CRF do not assume independence between as-
signed categories, it is possible to extend the local
feature function for enabling it to keep more infor-
mation about previous assigned categories and not
just the last category. These models are derived from
the work on weighted automata and transducers pre-
sented in studies such as Mohri et al. (2002). Let S
be a state space, s0 be a fixed initial empty state, and
let function g : S ×X ∗ ×N+ ×Y → S model state
transitions. Then the global feature function can be
redefined as:

F (x,y) =
n∑

i=1

f(x, i, si-1, yi), si = g(si-1,x, i, yi)

This slight change adds a lot of power to CRF be-
cause it provides the model with much more infor-
mation that it can use for learning complex relations.
Finally, the best candidate can be found by solving:

ŷ = arg max
y∈Yn

w ·
[

n∑
i=1

f(x, i, si-1, yi)

]
(1)

4 Model description: headlines as bitmaps

We model headline generation as a sequence predic-
tion task. In this manner a news article is seen as a
series of observations, where each is a possible to-
ken in the document. Furthermore, each observa-
tion can be assigned to one of two categories: in-
headline, or not in-headline. Note that this approach
allows a generated headline to be interpreted as a
bitmap over the article’s tokens.

If this set-up was used for a CRF model, the stan-
dard local feature function f(x, i, yi-1, yi) would
only be able to know whether the previous token
was taken or not, which would not be very infor-
mative. For solving the problem we integrate a state
sequence into the model, where a state s ∈ S holds
the following information:

• The last token that was chosen as part of the
headline.

• The part-of-speech tag1 of the second-to-last
word that was selected as part of the headline.

1We used the set of 45 tags from the Penn Treebank Tag-Set.

135



• The number of words already chosen to be part
of the headline, which could be zero.

Therefore, the local feature function f(x, i, si-1, yi)
will not only know about the whole text and the cur-
rent token xi, whose category yi is to be assigned,
but it will also hold information about the headline
constructed so far. In our model, the objective of the
local feature function is to return a vector that de-
scribes in an abstract euclidean space the outcome
of placing, or not placing, token xi in the headline,
provided that the words previously chosen form the
state si-1. We decide to make this feature vector con-
sist of 23 signals, which only fire if the token xi is
placed in the headline (i.e., yi = 1). The signals can
be grouped into the following five sets:

Language-model features: they assess the gram-
maticality of the headline being built. The first fea-
ture is the bigram probability of the current token,
given the last one placed on the headline. The sec-
ond feature is the trigram probability of the PoS tag
of the current token, given the tags of the two last
tokens on the headline.

Keyword features: binary features that help the
model detect whether the token under analysis is a
salient word. The document’s keywords are calcu-
lated as a preprocessing step via TF-IDF weighting,
and the features fire depending on how good or bad
the current token xi is ranked with respect to the oth-
ers on the text.

Dependency features: in charge of informing the
model about syntactical dependencies among the to-
kens placed on the headline. For this end the depen-
dency tree of all the sentences in the news article are
computed as a pre-processing step2.

Named-entity features: help the system identify
named entities3 in the text, including those that are
composed of contiguous tokens.

Headline-length features: responsible for en-
abling the model to decide whether a headline is
too short or too long. As many previous studies re-
port, an ideal headline must have from 8 to 10 tokens
(Banko et al., 2000). Thus, we include three binary
features that correspond to the following conditions:
(1) if the headline length so far is less than or equal
to seven; (2) if the headline length so far is greater

2We use the Stanford toolkit for computing parse trees.
3We only use PER, LOC, and ORG as entity annotations.

than or equal to 11; (3) if the token under analysis is
assigned to the headline, which is a bias feature.

5 Decoding the model

Decoding the model involves solving equation (1)
being given a weight vector w, whose value stays
unchanged during the process. A naive way of solv-
ing the optimization problem would be to try all pos-
sible |Y|n sequence combinations, which would lead
to an intractable procedure. In order to design a
polynomial algorithm that finds the global optimum
of the aforementioned formula, the following four
observations must be made:

(1) Our model is designed so that the local fea-
ture function only fires when a token is selected for
being part of a headline. Then, when evaluating an
arbitrary solution y, only the tokens placed on the
headline must be taken into account.

(2) When applying the local feature function to
a particular token xi (assuming yi = 1), the result
of the function will vary only depending on the pro-
vided previous state si-1; all the other parameters are
fixed. Moreover, a new state si will be generated,
which in turn will include token xi. This implies that
the entire evaluation of a solution can be completely
modeled as a sequence of state transitions; i.e., it be-
comes possible to recover a solution’s bitmap from
a sequence of state transitions and vice-versa.

(3) When analyzing the local feature function at
any token xi, the amount of different states si-1 that
can be fed to the function depend solely on the to-
kens taken before, for which there are 2i-1 different
combinations. Nevertheless, because a state only
holds three pieces of information, a better upper-
bound to the number of possible reachable states is
equal to i2 × |PoS|, which accounts to: all possible
candidates for the last token chosen before xi, times
all possible combinations of total number of tokens
taken before xi, times all possible PoS tags of the
one-before-last token taken before xi.

(4) The total amount of producible states in
the whole text is equal to

∑n
i=1 i

2 × |PoS| =
O(n3 × |PoS|). If the model is also con-
strained to produce headlines containing no more
than H tokens, the asymptotic bound drops to
O(H × n2 × |PoS|).

136



The final conclusion of these observations is as
follows: since any solution can be modelled as a
chain of state sequences, the global optimum can
be found by generating all possible states and fetch-
ing the one that, when reached from the initial state,
yields the maximum score. This task is achievable
with a number of operations linearly proportional
to the number of possible states, which at the same
time is polynomial with respect to the number of to-
kens in the document. In conclusion, the model can
be decoded in quadratic time. The pseudo-code in
algorithm 1 gives a sketch of a O(H × n2 × |PoS|)
bottom-up implementation.

6 Training the model: learning what
human-generated headlines look like

The global feature function F is responsible for tak-
ing a document and a bitmap, and producing a vec-
tor that describes the candidate headline in an ab-
stract feature space. We defined the feature function
so it only focuses on evaluating how a series of to-
kens that comprise a headline relate to each other
and to the document as a whole. This implies that
if h = {h1, h2, ..., hk} is the tokenized form of any
arbitrary headline consisting of k tokens, and we de-
fine vectors a ∈ X k+n and b ∈ Yk+n as:

a = {h1, h2, ..., hk, x1, x2, ..., xn}
b = {11, 12, ..., 1k, 01, 02, ..., 0n}

where a is the concatenation of h and x, and b is
a bitmap for only selecting the actual headline to-
kens, it follows that the feature vector that results
from calling the global feature function, which we
define as

u = F (a, b) (2)

is equivalent to a description of how headline h re-
lates to document x. This observation is the core of
our learning algorithm, because it implies that it is
possible to “insert” a human-generated headline in
the text and get its description in the abstract fea-
ture space induced by F . The objective of the learn-
ing process will consist in molding a weight vec-
tor w, such that it makes the decoding algorithm fa-
vor headlines whose descriptions in feature space re-
semble the characteristics of human-generated titles.

For training the model we follow the on-line
learning schemes presented by Collins (2002) and

Algorithm 1 Sketch of a bottom-up algorithm for find-
ing the top-scoring state s∗ that leads to the global op-
timum of our model’s objective function. It iteratively
computes two functions: π(i, l), which returns the set
of all reachable states that correspond to headlines hav-
ing token-length l and finishing with token xi, and α(s),
which returns the maximum score that can be obtained
by following a chain of state sequences that ends in the
provided state, and starts on s0.

1: //Constants.
2: H ← Max. number of allowed tokens in headlines.
3: n← Number of tokens in the document.
4: x← List of n tokens (document).
5: w ←Weight vector.
6: g ← State transition function.
7: f ← Local feature function.
8: s0 ← Init state.
9: //Variables.

10: π ← new Set<State>[n+ 1][H + 1]({})
11: α← new Float[|State|](−∞)
12: s∗ ← s0
13: //Base cases.
14: α(s0)← 0
15: for i in {0, ..., n} do
16: π(i, 0)← {s0}
17: //Bottom-up fill of π and α.
18: for l in {1, ...,H} do
19: for i in {l, ..., n} do
20: for j in {l − 1, ..., i− 1} do
21: for z in π(j, l − 1) do
22: s← g(z, x, i, 1)
23: sscore ← α(z) + w · f(x, i, z, 1)
24: π(i, l)← π(i, l) ∪ {s}
25: α(s)← max(α(s), sscore)
26: if α(s) > α(s∗) then
27: s∗ ← s

applied in studies that deal with CRF models with
state sequences, such as the dependency parsing
model of McDonald et al. (2005). The learning
framework consists of an averaged perceptron that
iteratively sifts through the training data and per-
forms the following error-correction update at each
step:

w∗ ← w + τ × (u− v̂), v̂ = F (x, ŷ)
where u is the vector defined in equation (2), ŷ is the
result of solving equation (1) with the current weight

137



vector, and τ ∈ R is a learning factor. We try three
different values for τ , which lead to the following
learning algorithms:

• Perceptron: τ = 1

• MIRA: τ = max
(
0, 1−w·(u−v̂)||u−v̂||2

)
• Forced-MIRA: τ = 1−w·(u−v̂)||u−v̂||2

The first value is a simple averaged perceptron as
presented by Collins (2002), the second value is a
Margin Infused Relaxed Algorithm (MIRA) as pre-
sented by Crammer and Singer (2003), and the third
value is a slight variation to the MIRA update. We
propose it for making the algorithm acknowledge
that the objective feature vector u cannot be pro-
duced from document x, and thus force an update
at every step. The reason for this is that if w · u >
w · v̂, then MIRA sets τ = 0, because an error was
not made (i.e. the human-generated headline got a
higher score than all of the others). Nevertheless,
we observed in our experiments that this behaviour
biases the process towards learning weights that ex-
ploit patterns that can occur in human-generated ti-
tles, but are almost never observed in the titles that
can be generated by our model, which hinders the
quality of the final headlines.

7 Automatic evaluation set-up

7.1 Evaluation metrics

For performing an automatic evaluation of the head-
lines generated by our system we follow the path
taken by related work such as Zajic et al. (2004) and
use a subset of the ROUGE metrics for comparing
candidate headlines with reference ones, which have
been proven to strongly correlate with human evalu-
ations (Lin, 2003; Lin, 2004).

We decide to use as metrics ROUGE-1, ROUGE-
2, and ROUGE-SU. We also propose as an experi-
mental new metric a weighted version of ROUGE-
SU, which we name ROUGE-WSU. The rationale of
our proposal is that ROUGE-SU gives the same im-
portance to all skip-bigrams extracted from a phrase
no matter how far apart they are. We address the
problem by weighting each shared skip-gram be-
tween phrases by the inverse of the token’s average

gap distance. Formally:

Sim(R,C) =

∑
(a,b)∈su(R)∩su(C)

2
distR(a,b)+distC(a,b)∑

(a,b)∈su(R)
1

distR(a,b)

Where function distH(a, b) returns the skip distance
between tokens “a” and “b” in headline H , and
su(H) returns all skip-bigrams in headline H .

With the objective of having a metric capable of
detecting abstract concepts in phrases and compar-
ing headlines at a semantic level, we resort to Latent
Semantic Indexing (LSI) (Deerwester et al., 1990).
We use the method for extracting latent concepts
from our training corpus so as to be able to repre-
sent text in an abstract latent space. We then com-
pute the similarity of a headline with respect to a
news article by calculating the cosine similarity of
their vector representations in latent space.

7.2 Baselines
In order to have a point of reference for interpreting
the performance of our model, we implement four
baseline models. We arbitrarily decide to make all
the baselines generate, if possible, nine-token-long
headlines, where the last token must always be a pe-
riod. This follows from the observation that good
headlines must contain about eight tokens (Banko et
al., 2000). The implemented baselines are the fol-
lowing:

Chunked first sentence: the first eight tokens
from the article, plus a period at the end.

Hidden Markov Model: as proposed by Zajic
et al. (2002), but adapted for producing eight-token
sentences, plus an ending period.

Word Graphs: as proposed by Filippova (2010).
This is a state-of-the-art multi-sentence compression
algorithm. To ensure it produces headlines as out-
put, we keep the shortest path in the graph with
length equal to or greater than eight tokens. An end-
ing period is appended if not already present. Note
that the original algorithm would produce the top-
k shortest paths and keep the one with best average
edge weight, not caring about its length.

Keywords: the top eight keywords in the article,
as ranked by TF-IDF weighting, sorted in descend-
ing order of relevance. This is not a real baseline

138



because it does not produce proper headlines, but it
is used for naively trying to maximize the achievable
value of the evaluation metrics. This is based on the
assumption that keywords are the most likely tokens
to occur in human-generated headlines.

7.3 Experiments and Results
We trained our model with a corpus consisting of
roughly 1.3 million financial news articles fetched
from the web, written in English, and published on
the second half of 2012. We decided to add three im-
portant constraints to the learning algorithm which
proved to yield positive empirical results:

(1) Large news articles are simplified by eliminat-
ing their most redundant or least informative sen-
tences. For this end, the text ranking algorithm pro-
posed by Mihalcea and Tarau (2004) is used for dis-
criminating salient sentences in the article. Further-
more, a news article is considered large if it has more
than 300 tokens, which corresponds to the average
number of words per article in our training set.

(2) Because we observed that less than 2% of the
headlines in the training set contained more than 15
tokens, we constraint the decoding algorithm to only
generate headlines consisting of 15 or fewer tokens.

(3) We restrain the decoding algorithm from
placing symbols such as commas, quotation marks,
and question marks on headlines. Nonetheless, only
headlines that end with a period are considered as
solutions; otherwise the model tends to generate
non-conclusive phrases as titles.

For automated testing purposes we use a training
set consisting of roughly 12,000 previously unseen
articles, which were randomly extracted from the
initial dataset before training. The evaluation con-
sisted in producing seven candidate headlines per ar-
ticle: one for each of the four baselines, plus one for
each of the three variations of our model (each dif-
fering solely on the scheme used to learn the weight
vector). Then each candidate is compared against
the article’s reference headline by means of the five
proposed metrics.

Table 1 summarizes the obtained results of the
models with respect to the ROUGE metrics. The
results show that our model, when trained with our
proposed forced-MIRA update, outperforms all the
other baselines on all metrics, except for ROUGE-

2, where all differences are statistically significant
when assessed via a paired t-test (p < 0.001). Also,
as initially intended, the keywords baseline does
produce better scores than all the other methods,
therefore it is considered as a naive upper-bound. It
must be highlighted that all the numbers on the table
are rather low, this occurs because, as noted by Zajic
et al. (2002), humans tend to use a very different vo-
cabulary and writing-style on headlines than on arti-
cles. The effect of this is that our methods and base-
lines are not capable of producing headlines with
wordings strongly similar to human-written ones,
which as a consequence makes it almost impossible
to obtain high ROUGE scores.

R-1 R-2 R-SU R-WSU
Perceptron 0.157 0.056 0.053 0.082
MIRA 0.172 0.042 0.057 0.084
f-MIRA 0.187 0.054 0.065 0.095
1stsent. 0.076 0.021 0.025 0.038
HMM 0.090 0.009 0.023 0.038
Word graphs 0.174 0.060 0.060 0.084
Keywords 0.313 0.021 0.112 0.148

Table 1: Result of the evaluation of our models and base-
lines with respect to ROUGE metrics.

For having a more objective assessment of our
proposal, we carried out a human evaluation of the
headlines generated by our model when trained with
the f-MIRA scheme and the word graphs approach
by Filippova (2010). For this purpose, 100 arti-
cles were randomly extracted from the test set and
their respective candidate headlines were generated.
Then different human raters were asked to evalu-
ate on a Likert scale, from 1 to 5, both the gram-
maticality and informativeness of the titles. Each
article-headline pair was annotated by three differ-
ent raters. The median of their ratings was chosen
as a final mark. As a reference, the raters were also
asked to annotate the actual human-generated head-
lines from the articles, although they were not in-
formed about the provenance of the titles. We mea-
sured inter-judge agreement by means of their Intra-
Class Correlation (ICC) (Cicchetti, 1994). The ICC
for grammaticality was 0.51 ± 0.07, which repre-
sents fair agreement, and the ICC for informative-
ness was 0.63 ± 0.05, which represents substantial
agreement.

Table 2 contains the results of the models with

139



H. Len. LSI Gram. Inf.
Perceptron 10.096 0.446 – –
MIRA 13.045 0.463 – –
f-MIRA 11.737 0.491 3.45 2.94
1stsent. 8.932 0.224 – –
HMM 9.000 0.172 – –
Word graphs 10.973 0.480 3.69 2.32
Keywords 9.000 0.701 – –
Reference 11.898 0.555 4.49 4.14

Table 2: Result of the evaluation of our models and base-
lines with LSI document similarity, grammaticality and
informativeness as assessed by human raters, and aver-
age headline length.

respect to the LSI document similarity metric, and
the human evaluations for grammaticality and in-
formativeness. For exploratory purposes the table
also contains the average length for the generated
headlines of each of the models (which also counts
the imposed final period). The results in this ta-
ble are satisfying: with respect to LSI document
similarity, our model outperforms all of the base-
lines and its value is close to the one achieved by
human-generated headlines. On the other hand, the
human evaluations are middling: the word-graphs
method produces more readable headlines, but our
model proves to be more informative because it does
better work at detecting abstract word relationships
in the text. All differences in this table are statis-
tically significant when computed as paired t-tests
(p < 0.001).

It is worth noting that the informativeness of
human-generated headlines did not get a high score.
The reason for this is the fact that editors tend to
produce rather sensationalist or partially informative
titles so as to attract the attention of readers and en-
gage them to read the whole article; human raters pe-
nalized the relevance of such headlines, which was
reflected on this final score.

Finally, table 3 contains the mutual correlation be-
tween automated and manual metrics. The first thing
to note is that none of the used metrics proved to be
good for assessing grammaticality of headlines. It is
also worth noting that our proposed metric ROUGE-
WSU performs as well as the other ROUGE metrics,
and that the proposed LSI document similarity does
not prove to be as strong a metric as the others.

R-1 R-2 R-SU R-WSU LSI-DS
Gram. -0.130 -0.084 -0.131 -0.132 -0.015
Inf. 0.561 0.535 0.557 0.542 0.370

Table 3: Spearman correlation between human-assessed
metrics and automatic ones.

8 Conclusions and Discussion

In this study we proposed a CRF model with state
transitions. The model tries to learn how humans
title their articles. The learning is performed by
means of a mapping function that, given a doc-
ument, translates headlines to an abstract feature-
rich space, where the characteristics that distinguish
human-generated titles can be discriminated. This
abstraction allows our model to be trained with any
monolingual corpus of news articles because it does
not impose conditions on the provenance of stories’
headlines –i.e, our model maps reference headlines
to a feature space and only learns what abstract prop-
erties characterize them. Furthermore, our model al-
lows defining the task of finding the best possible
producible headline as a discrete optimization prob-
lem. By doing this each candidate headline is mod-
elled as a path in a graph of state sequences, thus
allowing the best-scoring path to be found in poly-
nomial time by means of dynamic programming.
Our results, obtained through reliable automatic and
human-assessed evaluations, provide a proof of con-
cept for the soundness of our model and its capabili-
ties. Additionally, we propose a new evaluation met-
ric, ROUGE-WSU, which, as shown in table 3, cor-
relates as good as traditional ROUGE metrics with
human evaluations for informativeness of headlines.

The further work we envisage for augmenting our
research can be grouped in the following areas:

• Exploring more advanced features that manage
to detect abstract semantic relationships or dis-
course flows in the compressed article.

• Complementing our system with a separate
translation model capable of transforming to
“Headlinese” the titles generated with the lan-
guage used in the bodies of articles.

• Attempting to achieve a more objective evalua-
tion of our generated headlines, through the use
of semantic-level measures.

140



Acknowledgments

We thank Jordi Atserias, Horacio Saggion, Hora-
cio Rodrı́guez, and Xavier Carreras, who helped and
supported us during the development of this study.

References
[Banko et al.2000] Michele Banko, Vibhu O. Mittal, and

Michael J. Witbrock. 2000. Headline generation
based on statistical translation. Proceedings of the
38th Annual Meeting on Association for Computa-
tional Linguistics.

[Berger and Mittal2000] B. Berger and N. Mittal. 2000.
Discourse segmentation in aid of document summa-
rization. Proceedings of the Hawaii International
Conference on System Sciences, Minitrack on Digital
Documents Understanding.

[Borko and Bernier1987] H. Borko and C. Bernier. 1987.
Abstracting concepts and methods. New York: Aca-
demic Press.

[Buyukkokten et al.2001] Orkut Buyukkokten, Hector
Garcia-Molina, and Andreas Paepcke. 2001. Seeing
the whole in parts: text summarization for web brows-
ing on handheld devices. Proceedings of the 10th in-
ternational conference on World Wide Web. ACM.

[Cicchetti1994] Domenic V. Cicchetti. 1994. Guidelines,
criteria, and rules of thumb for evaluating normed and
standardized assessment instruments in psychology.
Psychological assessment 6.4.

[Collins2002] Michael Collins. 2002. Discriminative
training methods for hidden markov models: Theory
and experiments with perceptron algorithms. Proceed-
ings of the ACL-02 conference on Empirical methods
in natural language processing-Volume 10.

[Crammer and Singer2003] Koby Crammer and Yoram
Singer. 2003. Ultraconservative online algorithms for
multiclass problems. The Journal of Machine Learn-
ing Research 3.

[Das and Martins2007] Dipanjan Das and André FT Mar-
tins. 2007. A survey on automatic text summariza-
tion. Literature Survey for the Language and Statistics
II course at CMU 4, pages 192–195.

[De Kok2008] D. De Kok. 2008. Headline generation
for dutch newspaper articles through transformation-
based learning. Master Thesis.

[Deerwester et al.1990] Scott C. Deerwester et al. 1990.
Indexing by latent semantic analysis. JASIS 41.6.

[Dorr et al.2003] Bonnie Dorr, David Zajic, and Richard
Schwartz. 2003. Hedge trimmer: a parse-and-
trim approach to headline generation. Proceedings of
the HLT-NAACL 03 on Text summarization workshop,
pages 1–8.

[Filippova2010] Katja Filippova. 2010. Multi-sentence
compression: Finding shortest paths in word graphs.
Proceedings of the 23rd International Conference on
Computational Linguistics.

[Gattani2007] Akshay Kishore Gattani. 2007. Auto-
mated natural language headline generation using dis-
criminative machine learning models. Diss. Simon
Fraser University.

[Hajime et al.2013] Morita Hajime et al. 2013. Subtree
extractive summarization via submodular maximiza-
tion. Proceedings of 51st Annual Meeting of the As-
sociation for Computational Linguistics.

[Jin and Hauptmann2001] Rong Jin and Alexander G.
Hauptmann. 2001. Title generation using a training
corpus. CICLing ’01: Proceedings of the Second In-
ternational Conference on Computational Linguistics
and Intelligent Text Processing, pages 208–215.

[Knight and Marcu2001] Kevin Knight and Daniel
Marcu. 2001. Summarization beyond sentence
extraction: A probabilistic approach to sentence
compression. Artificial Intelligence 139.1, pages
91–107.

[Kupiec1992] Julian Kupiec. 1992. Robust part-of-
speech tagging using a hidden markov model. Com-
puter Speech and Language 6.3.

[Lafferty et al.2001] John Lafferty, Andrew McCallum,
and Fernando CN Pereira. 2001. Conditional random
fields: Probabilistic models for segmenting and label-
ing sequence data.

[Lin2003] C.Y. Lin. 2003. Cross-domain study of n-
gram co-occurrence metrics. Proceedings of the Work-
shop on Machine Translation Evaluation, New Or-
leans, USA.

[Lin2004] Chin-Yew Lin. 2004. Rouge: A package for
automatic evaluation of summaries. Text Summariza-
tion Branches Out: Proceedings of the ACL-04 Work-
shop.

[Linke-Ellis1999] N. Linke-Ellis. 1999. Closed caption-
ing in america: Looking beyond compliance. Pro-
ceedings of the TAO Workshop on TV Closed Captions
for the Hearing Impaired People, Tokyo, Japan, pages
43–59.

[Mani and Maybury1999] Inderjeet Mani and Mark
T. Maybury Maybury. 1999. Advances in automatic
text summarization, volume 293. Cambridge: MIT
press.

[Martins and Smith2009] André F.T. Martins and
Noah A. Smith. 2009. Summarization with a
joint model for sentence extraction and compres-
sion. NAACL-HLT Workshop on Integer Linear
Programming for NLP, Boulder, USA.

[McCallum and Li2003] Andrew McCallum and Wei Li.
2003. Early results for named entity recognition with

141



conditional random fields, feature induction and web-
enhanced lexicons. Proceedings of the seventh con-
ference on Natural language learning at HLT-NAACL
2003-Volume 4.

[McCallum et al.2000] Andrew McCallum, Dayne Fre-
itag, and Fernando CN Pereira. 2000. Maximum en-
tropy markov models for information extraction and
segmentation. ICML.

[McDonald et al.2005] Ryan McDonald, Koby Crammer,
and Fernando Pereira. 2005. Online large-margin
training of dependency parsers. Proceedings of the
43rd Annual Meeting on Association for Computa-
tional Linguistics.

[McDonald2006] Ryan T. McDonald. 2006. Discrimi-
native sentence compression with soft syntactic evi-
dence. EACL.

[Mihalcea and Tarau2004] Rada Mihalcea and Paul Ta-
rau. 2004. Textrank: Bringing order into texts. As-
sociation for Computational Linguistics.

[Mohri et al.2002] Mehryar Mohri, Fernando Pereira, and
Michael Richard. 2002. Weighted finite-state trans-
ducers in speech recognition. Computer Speech and
Language 16.1.

[Nenkova and McKeown2012] Ani Nenkova and Kath-
leen McKeown. 2012. A survey of text summariza-
tion techniques. Mining Text Data. Springer US, pages
43–76.

[Nomoto2007] Tadashi Nomoto. 2007. Discriminative
sentence compression with conditional random fields.
Information processing and management 43.6.

[Rabiner and Juang1986] Lawrence Rabiner and Biing-
Hwang Juang. 1986. An introduction to hidden
markov models. ASSP Magazine, IEEE 3.1, pages 4–
16.

[Reynar and Ratnaparkhi1997] Jeffrey C. Reynar and Ad-
wait Ratnaparkhi. 1997. A maximum entropy ap-
proach to identifying sentence boundaries. Proceed-
ings of the fifth conference on Applied natural lan-
guage processing.

[Shen et al.2007] Dou Shen et al. 2007. Document sum-
marization using conditional random fields. IJCAI.
Vol. 7.

[Unno et al.2006] Yuya Unno et al. 2006. Trimming
cfg parse trees for sentence compression using ma-
chine learning approaches. Proceedings of the COL-
ING/ACL on Main conference poster sessions.

[Zajic et al.2002] David Zajic, Bonnie Dorr, and Richard
Schwartz. 2002. Automatic headline generation for
newspaper stories. Workshop on Automatic Summa-
rization.

[Zajic et al.2004] David Zajic, Bonnie Dorr, and Richard
Schwartz. 2004. Bbn/umd at duc-2004: Topiary. Pro-
ceedings of the HLT-NAACL 2004 Document Under-
standing Workshop, Boston.

142


