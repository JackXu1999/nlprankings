











































Retrieval-Enhanced Adversarial Training for Neural Response Generation


Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 3763–3773
Florence, Italy, July 28 - August 2, 2019. c©2019 Association for Computational Linguistics

3763

Retrieval-Enhanced Adversarial Training for Neural Response
Generation

Qingfu Zhu], Lei Cui[, Weinan Zhang]\, Furu Wei[, Ting Liu]\⇤
]Harbin Institute of Technology, Harbin, China

[Microsoft Research Asia, Beijing, China
\Peng Cheng Laboratory, Shenzhen, China

{qfzhu, wnzhang, tliu}@ir.hit.edu.cn
{lecu, fuwei}@microsoft.com

Abstract
Dialogue systems are usually built on ei-
ther generation-based or retrieval-based ap-
proaches, yet they do not benefit from the
advantages of different models. In this pa-
per, we propose a Retrieval-Enhanced Ad-
versarial Training (REAT) method for neural
response generation. Distinct from existing
approaches, the REAT method leverages an
encoder-decoder framework in terms of an ad-
versarial training paradigm, while taking ad-
vantage of N-best response candidates from
a retrieval-based system to construct the dis-
criminator. An empirical study on a large scale
public available benchmark dataset shows that
the REAT method significantly outperforms
the vanilla Seq2Seq model as well as the con-
ventional adversarial training approach.

1 Introduction

Dialogue systems intend to converse with humans
with a coherent structure. They have been widely
used in real-world applications, including cus-
tomer service systems, personal assistants, and
chatbots. Early dialogue systems are often built
using the rule-based method (Weizenbaum, 1966)
or template-based method (Litman et al., 2000;
Schatzmann et al., 2006; Williams and Young,
2007), which are usually labor-intensive and dif-
ficult to scale up. Recently, with the rise of
social networking, conversational data have ac-
cumulated to a considerable scale. This pro-
moted the development of data-driven methods,
including retrieval-based methods (Shang et al.,
2015; Sordoni et al., 2015; Vinyals and Le, 2015;
Wen et al., 2017) and generation-based meth-
ods (Leuski et al., 2006; Ji et al., 2014; Yan et al.,
2016).

Retrieval-based methods reply to users by
searching and re-ranking response candidates

⇤Corresponding author.

MSG I made strawberry shortcake.

GT Where did you learn that, it is sweet and cheery.
RSP How did you make it? It looks delicious.
C#1 Could you tell me how this thing is cooked?
C#2 Tiramisu is my favorite dessert. It’s so delicious.

Table 1: An example of a message (MSG), a ground-
truth response (GT), a generated response (RSP) and
N-best response candidates (C#1 and C#2) during the
training process. Similar contents in the response and
candidates are in boldface.

from a pre-constructed response set. Written
mainly by humans, these responses are always
diverse and informative, but may be inappropri-
ate to input messages due to their being pre-
pared in advance and thus incapable of being
customized (Shang et al., 2015). In contrast,
generation-based methods can produce responses
tailored to the messages. The most common
method of this category in recent years is the se-
quence to sequence (Seq2Seq) model (Sutskever
et al., 2014; Shang et al., 2015; Vinyals and Le,
2015). In practice, it usually suffers from the prob-
lem of generating generic responses, such as “I
don’t know” and “Me, too” (Li et al., 2016a; Ser-
ban et al., 2016). While the contents of retrieved
responses, apart from the irrelevant parts, are of
great diversity, making it a potential resource for
tailoring appropriate and informative responses.
Therefore, it is natural to enhance the response
generation approach with retrieved responses.

Previous work has been proposed to extend the
input of a Seq2Seq model with N-best response
candidates (or their contexts) (Song et al., 2018;
Pandey et al., 2018). On one hand, these ap-
proaches are trained using MLE objective, which
correlates weakly with true quality of responses
thus limits the effectiveness of the candidates in
producing the responses. Table 1 shows an exam-



3764

ple during the training process. Related contents
of the candidates are appropriately integrated into
the response, but the model is discouraged as the
response is different from the ground-truth. On
the other hand, rather than just provide materials
for the generation, N-best response candidates also
contain references for evaluating responses. Yet
they are not efficiently utilized in the objective in
the existing training process.

In this paper, we propose a Retrieval-Enhanced
Adversarial Training (REAT) approach to make
better use of N-best response candidates. A dis-
criminator is introduced to replace the MLE ob-
jective to supervise the training process. Gen-
erated responses containing appropriate and in-
formative contents with input messages are more
likely to be seen as human-generated by the
discriminator, which encourages the generation
model to incorporate more information in candi-
dates into responses. In addition, the N-best re-
sponse candidates are also conditioned to the dis-
criminator as references to improve its classifi-
cation accuracy, which in turn benefits the gen-
eration model by adversarial training. We con-
duct extensive experiments on a public available
NTCIR corpus to verify the effectiveness of the
proposed approach, comparing it with retrieval-
based methods, generation-based methods, and
previous retrieval-enhanced response generation
approaches. The results show that the REAT ap-
proach significantly outperforms the baselines in
both automatic and human evaluations.

The contributions of this paper are summarized
as follows:

1. We propose a novel retrieval-enhanced neural
response generation model adapted from ad-
versarial training approach, which introduces
a discriminator to more efficiently utilize the
N-best response candidates.

2. Referencing to N-best response candidates,
the discriminator of our proposed approach
improves over previous discriminators on the
classification accuracy.

3. Extensive experiments show that our pro-
posed approach outperforms state-of-the-art
baselines in both automatic and human eval-
uations.

2 Related Work

Data-driven dialogue systems can be roughly di-
vided into two categories: retrieval-based and
generation-based. Retrieval-based methods re-
spond to users by selecting the response that best
matches an input message from a pre-constructed
response set. Leuski et al. (2006) match a response
with a message using a statistical language model.
Ji et al.(2014) employ information retrieval tech-
niques to rank response candidates. In addition,
the matching and ranking methods can also be im-
plemented using neural networks (Yan et al., 2016;
Qiu et al., 2017; Wu et al., 2017). Based on that,
Yang et al. (2018) propose a deep matching net-
work which could model external knowledge.

Generation-based methods can be cast as a se-
quence to sequence (Seq2Seq) process (Shang
et al., 2015; Vinyals and Le, 2015; Sordoni et al.,
2015) but suffers from generating generic re-
sponses. One way to address the problem is to
introduce new content into responses, such as key-
words (Mou et al., 2016; Serban et al., 2017a),
topic information (Xing et al., 2017) and knowl-
edge triples (Zhou et al., 2018). Another way is to
improve the Seq2Seq architecture. Li et al.(2016b)
introduce the Maximum Mutual Information as
the objective function. Serban et al.(2017b) add
a latent variable to inject variability. The train-
ing of Seq2Seq can be formulated as a reinforce-
ment learning problem (Li et al., 2016b; Zhang
et al., 2017). To avoid manually defining reward
functions, a discriminator can be introduced and
trained synchronously by adversarial learning (Li
et al., 2017). After that, Xu et al. (2018) propose a
language model based discriminator to better dis-
tinguish novel responses from repeated responses.
In a similar adversarial setting, Zhang et al. (2018)
optimize a Variational Information Maximization
Objective to improve informativeness. Our ap-
proach is also an adversarial model, the difference
is that we employ the N-best response candidates
to enhance the generation.

Taking advantages of the two methods,
retrieval-enhanced response generation ap-
proaches make use of the informative content
in retrieved results to generate new responses.
Typically, generating responses from retrieved
candidates can be seen as a text-to-text system,
which produces meaningful text from meaningful
text rather than from abstract meaning represen-
tations (Marsi and Krahmer, 2005). Barzilay



3765

Candidate#1  Could you tell me how this thing is cooked?
Candidate#2 Tiramisu is my favorite dessert. It’s so delicious.

…

Decoder

Encoder

Response  How did you make it? It looks delicious.
Generator

Message:  I made strawberry shortcake.

M
essage 
LSTM

Response
LSTM

Concatenation

MLP

Discriminator

C
andidate
 LSTM

Average

N-best Response Candidates

Zx

…
Zc

1

Index

Retrieval-based 
Method

Training Set

Policy Gradient

Zc
2

Prob ( human-generated )

Figure 1: An overview of our proposed approach. The discriminator is enhanced by the N-best response candidates
returned by a retrieval-based method. The discriminator takes as input a response and outputs the probability that
the response is human-generated. The output is then regarded as a reward to guide the generator.

and McKeown (2005) propose the sentence
fusion technique for abstractive multidocument
summarization. In the context of conversation,
Song et al.(2018) apply an encoder to every
response candidate and integrate the results into
the decoding process via the attention mecha-
nism (Bahdanau et al., 2015). Similarly, Pandey
et al.(2018) also incorporate response candidates
using the attentive encoder-decoder framework
on a proposed technical support dataset. Wu
et al.(2019) augments the decoder with an edit
vector representing lexical differences between
retrieved contexts and the message. Different
from previous work, our approach introduces a
discriminator to replace the MLE objective to
compute the loss. Besides, rather than merely
being sent to the encoder as generation materials,
response candidates in our approach are directly
utilized by the discriminator to form a discrimina-
tive signal to guide the generator. The proposed
approach is also related to Lin et al.(2017)’s work.
They propose an unconditional GAN whose
discriminator is augmented with references ran-
domly sampled from the training set for the task
of language generation. In contrast, our proposed
approach focuses on the response generation and
leverages the message as prior knowledge. In
addition, rather than sampling references from the
training set, the candidates in our approach are
retrieved according to the relevance to messages
using a retrieval-based method.

3 Method

In this section, we introduce our proposed REAT
approach. As Figure 1 shows, it consists of two

main components: a discriminator D (Sec. 3.2)
and a generator G (Sec. 3.3), both of which are
enhanced by N-best response candidates from a
retrieval-based method (Sec. 3.4). The generator
produces a response using the candidates as gen-
eration materials. While in the discriminator, the
candidates are provided as references to better dis-
tinguish a response, which in turn improves the
generator by adversarial training (Sec. 3.1).

3.1 Retrieval-Enhanced Adversarial Training
The goal of the discriminator is to distinguish
whether a response y is human-generated or
machine-generated. It computes the probabil-
ity D

�

(y|x, {c}) that the response is human-
generated given an input message x and N-best
response candidates {c} = {c1, ...ck, ..., cN},
where � denote the parameters of the discrimina-
tor. Therefore, its objective function is to mini-
mize classification error rate:

J

D

(�) =� E
y⇠ground�truth logD�(y|x, {c})

� E
y⇠G log(1�D�(y|x, {c}), (1)

We cast the retrieval-enhanced response genera-
tion as a reinforcement learning problem to back-
propagate the error computed by the discrimina-
tor to the generator via the policy gradient algo-
rithm. In this way, the generator can be seen as
an agent whose parameters ✓ define a policy ⇡.
At each time step, it takes an action a by gener-
ating a word and accordingly updates its state s,
which is defined as a tuple of the message, the
candidates and the partially generated response.
At the end of the generation of a response, the
agent observes a reward r from the discrimina-
tor, which is the probability that the response is



3766

human-generated: D
�

(y|x, {c}). Here, we do not
employ the REGS (reward for every generation
step) strategy (Li et al., 2017) as the Monte-Carlo
roll-out is quite time-consuming1 and the accuracy
of a discriminator trained on partially decoded se-
quences is not as good as that trained on complete
sequences.

The goal of the agent (the generator) is to mini-
mize the negative expected reward. With the like-
lihood ratio trick (Williams, 1992), the gradient of
✓ can be derived as:

J

G

(✓) =� E
y⇠G(D�(y|x, {c})), (2)

5J
G

(✓) =� E
y⇠G(D�(y|x, {c})

·5 logG
✓

(y|x, {c})), (3)

where G
✓

(y|x, {c}) is the probability of gener-
ating y given x and {c}. In practice, J

G

(✓)

and 5J
G

(✓) can be approximated using a single
Monte-Carlo sample from G (Rennie et al., 2017):

J

G

(✓) ⇡�D
�

(y|x, {c}), y ⇠ G, (4)
5J

G

(✓) ⇡�D
�

(y|x, {c})
·5 logG

✓

(y|x, {c}), y ⇠ G. (5)

Both the generator and the discriminator are
pre-trained before adversarial training. The gen-
erator is pre-trained on the training set with
MLE loss. The discriminator is pre-trained us-
ing human-generated responses as positive sam-
ples and machine-generated responses produced
by the pre-trained generator as negative samples.

Given the pre-trained generator and discrimi-
nator, the adversarial training is a min-max game
played between them:

min

G

max

D

J

G

(✓)� J
D

(�), (6)

where the discriminator tries to distinguish be-
tween human-generated responses and machine-
generated responses, while the generator tries to
fool the discriminator by producing human-like
responses. The overall algorithm of the retrieval-
enhanced adversarial training is summarized as
Algorithm 1.

3.2 Discriminator
The discriminator is a binary classifier. It takes
as input a response y, a message x, and N-best

1Training one epoch takes roughly 120 hours on a TITAN
Xp GPU when the roll-out number is 5.

Algorithm 1 Retrieval-Enhanced Adversarial
Training
Require:

The training set {x, y};
Ensure:

The generator parameters ✓;
The discriminator parameters �;

1: Get N-best response candidates using a
retrieval-based method;

2: Randomly initialize ✓ and �;
3: Pre-train G with MLE loss;
4: Generate responses using the pre-trained G;
5: Pre-train D using machine-generated re-

sponses as negative samples and human-
generated responses as positive samples;

6: for epoch in number of epochs do
7: for g in g-steps do
8: Update ✓ according to Equation 5;
9: end for

10: for d in d-steps do
11: Sample y from G as a negative sample;
12: Sample y from the human-generated re-

sponses as a positive sample;
13: Update � according to Equation 1;
14: end for
15: end for
16: return ✓,�;

response candidates {c}, and subsequently com-
putes a binary probability distribution to indi-
cate whether y is human-generated or machine-
generated.

First, we compute a candidate-aware response
representation zc to model the interaction between
the candidates and the response. Each candidate
is encoded by a candidate LSTM (Hochreiter and
Schmidhuber, 1997):

u

k

i

= f

c

(c

k

i

, u

k

i�1), (7)

where ck
i

is the i-th word of the k-th candidate. uk
i

denotes the hidden state of the candidate LSTM
at time step i. f

c

is the computation unit of the
candidate LSTM. The initial hidden state uk0 is set
to the zero vector and the last hidden state uk

T

(T
denotes the length of an utterence through out the
paper) can be seen as a representation of the candi-
date. Subsequently, uk

T

is used to initialize the hid-
den state of a response LSTM, which computes a
local candidate-aware response representation zck



3767

for each candidate ck:

v

k

i

=f

y

(y

i

, v

k

i�1), z
c

k
= v

k

T

, (8)

where vk
i

represents the hidden state of the re-
sponse LSTM at time step i with regard to the
k-th candidate. f

y

is the computation unit of the
response LSTM and y

i

is the i-th word of the re-
sponse. The candidate-aware response representa-
tion zc is the average of all local candidate-aware
response representations:

z

c

=

1

N

NX

k=1

z

c

k
, (9)

Second, the interaction between the message
and the response is modeled by a message-aware
response representation zx using a message LSTM
and the response LSTM introduced above in a sim-
ilar way to Equation 7 and 8.

Finally, the probability that the response is
human-generated D

�

(y|x, {c}) is computed by a
Multilayer Perception (MLP):

D

�

(y|x, {c}) = �(MLP([zx, zc])), (10)

where the bracket [·, ·] denotes concatenation. � is
the sigmoid function2.

3.3 Generator
The generator G is a multi-source Seq2Seq model,
which consists of an encoder and a decoder. The
encoder reads from a message and N-best response
candidates, summarizing them into context vec-
tors. The decoder is a language model which pro-
duces a response word by word, conditioned with
the context vectors.

The encoder first employs a bi-directional
LSTM to represent each candidate word and its
context information in a response candidate:

!
h

k

i

= g

0
c

(c

k

i

,

!
h

k

i�1),
 
h

k

i

= g

1
c

(c

k

i

,

 
h

k

i+1), (11)

where g0
c

and g1
c

denote the computation units of
a forward LSTM and a backward LSTM, respec-

tively.
!
h

k

i

and
 
h

k

i

are the i-th hidden states of the
two LSTMs. After that, hidden states in the two

directions are concatenated, i.e., hk
i

= [

!
h

k

i

,

 
h

k

i

].
2We did study more complicated relationship among x, y

,and {c} with bi-directional LSTM and attention mechanism
in the discriminator, but observed no further improvement on
the validation set.

To capture the different importance of a candi-
date word in the word-level and the sentence-level,
the encoder employs a two-level attention struc-
ture. The word-level attention models the rele-
vance of a candidate word to the decoding context
within a candidate, i.e, the word-level attention at
the j-th decoding time step is computed as:

↵

k

ij

=

exp(q(s
j�1, h

k

i

))

P
T

t=1 exp(q(sj�1, h
k

t

))

, (12)

where ↵k
ij

is the word-level weight for the i-th
word of ck. s

j�1 is the hidden state of the de-
coder, representing the decoding context at time
step j. q is a feed-forward network. Consider-
ing that different candidates are of different im-
portance, the word-level weights are then rescaled
by a sentence-level attention:

a

c

k

j

=

TX

i=1

↵

k

ij

h

k

i

, (13)

�

kj

=

exp(q(s
j�1, a

c

k

j

))

P
N

n=1 exp(q(sj�1, a
c

n

j

))

. (14)

where ack
j

can be seen as a representation of ck.
�

kj

is the sentence-level weight of ck. The can-
didate context vector ac

j

is then computed taking
into account the two-level attention weights:

a

c

j

=

NX

k=1

TX

i=1

�

kj

↵

k

ij

h

k

i

(15)

Meanwhile, the message context vector ax
j

is
computed using a message bi-directional LSTM
and a word-level attention in a similar way to
Equation 11, 12 and 13. Then, the decoder LSTM
updates its hidden state conditioned with the con-
text vectors and subsequently generates a word for
a response as a standard language model:

s

j

= g

y

([y

j�1, a
c

j

, a

x

j

], s

j�1). (16)

where g
y

is the computation unit of the decoder.

3.4 Retrieval-based Method
To get the N-best response candidates, a retrieval-
based method is built using the Lucene3 li-
brary and the state-of-the-art response ranking
model (Yang et al., 2018). First, we merge all

3https://lucene.apache.org/



3768

Corpus # of message # of response

Training 119,941 1,932,258
Validation 10,000 163,126
Test 10,000 162,230

Table 2: Some statistics of the datasets.

message-response pairs whose messages are iden-
tical into a document and subsequently build the
index for all the documents in the training set. Sec-
ond, we use each message as a query to search
for K (set to 10) documents whose messages are
similar to the query. After that, responses in the
retrieved documents are re-ranked by the ranking
model according to their matching scores to the
query. Finally, the top N (set to 2, as in Song
et al., 2018) responses are returned as the N-best
response candidates.

Note that when we collect N-best response can-
didates for a training message, the most similar
document retrieved is always the one whose mes-
sage is exactly the training message and responses
contain the ground-truth response. We thus re-
move the document from the retrieved result be-
fore re-ranking to make sure that the N-best re-
sponse candidates are different from the ground-
truth response.

4 Experiments

4.1 Data
We use the NTCIR corpus4 in our experiments.
Its data are collected from a Chinese microblog-
ging service, Sina Weibo5, where users can both
post messages and make comments (responses) on
other users’ messages. First, we tokenize each
utterance using the Language Technology Plat-
form (Che et al., 2010) and remove samples whose
responses are shorter than 5, which is helpful in
relieving the generic response problem (Li et al.,
2017). Then, we randomly select 10,000 messages
associated with responses to form a validation set
and another 10,000 messages with responses as
a test set. Table 2 shows some statistics of the
datasets.

4.2 Baselines
Rtr: The retrieval-based method searches the in-
dex for response candidates and subsequently re-

4http://research.nii.ac.jp/ntcir/data/data-en.html
5https://weibo.com

turns the one that best matches the message after
re-ranking (see Sec. 3.4 for details).
S2S: The Seq2Seq model with the attention mech-
anism (Bahdanau et al., 2015).
MS2S: The “multi sequence to sequence” (Song
et al., 2018) encodes N-best response candidates
using N encoders and subsequently incorporates
the results into the decoding process by the atten-
tion mechanism.
Edit: The prototype editing model (Wu et al.,
2019) augments the decoder with an edit vector
representing lexical differences between retrieved
contexts and the message.
AL: The adversarial learning for neural response
generation (Li et al., 2017) is also an adversarial
method but is not retrieval-enhanced. Here, we
do not employ the REGS (reward for every gen-
eration step) setting as the Monte-Carlo roll-out is
quite time-consuming and the accuracy of the dis-
criminator trained on partially decoded sequences
is not as good as that on complete sequences.

4.3 Experiment Settings

We use the published code6 for Edit and imple-
ment other approaches by an open source frame-
work: Open-NMT (Klein et al., 2017). The vo-
cabulary table consists of the most frequent 30,000
words, whose 300-dimensional word embeddings
are pre-trained on the training set by Word2Vec 7.
The number of hidden units for all LSTM in our
approach is 500. The batch size is set to 64.

The discriminator and the generator are trained
alternately, where the discriminator is optimized
for 10 batches, then switch to the generator for 20
batches. We use ADAM optimizer whose learn-
ing rate is initialized to 0.0001. In the inference
process, we generate responses using beam search
with beam size set to 5.

5 Results

5.1 Evaluation Metrics

Human Evaluation We randomly sampled 200
messages from the test set to conduct the human
evaluation as it is extremely time-consuming. Five
annotators8 are recruited to judge a response from
three aspects (Ke et al., 2018):

6https://github.com/MarkWuNLP/ResponseEdit
7https://code.google.com/archive/p/word2vec/
8All annotators are well-educated students and have

Bachelor or higher degree.



3769

Appropriateness Informativeness Grammaticality
Mean +2 +1 0  Mean +2 +1 0  Mean +2 +1 0 

Rtr 0.63 24.8 12.9 62.3 0.71 0.92 41.1 10.1 48.8 0.67 1.93 94.9 3.1 2.0 0.61
S2S 0.76 27.9 20.0 52.1 0.58 0.51 10.2 30.5 59.3 0.69 1.74 85.3 2.9 11.8 0.83
MS2S 0.85 31.9 21.5 46.6 0.63 0.62 14.1 33.8 52.1 0.73 1.74 85.5 3.2 11.3 0.82
Edit 0.85 31.4 21.9 46.7 0.66 0.67 15.9 34.9 49.2 0.68 1.92 95.2 1.5 3.3 0.63
AL 0.98 36.8 24.0 39.2 0.57 0.77 21.8 33.6 44.6 0.66 1.88 91.7 4.7 3.6 0.58
Ours 1.10 41.5 26.8 31.7 0.65 0.88 31.2 25.9 42.9 0.72 1.87 89.6 7.6 2.8 0.60

Table 3: Human evaluation results of mean score, proportions of three levels (+2, +1, and 0), and the agreements
measured by Fleiss’s Kappa in appropriateness, informativeness, and grammaticality.

AL Ours

Accuracy 94.01% 95.72%

Table 4: Classification accuracy of discriminators in
AL and our approach.

• appropriateness: a response is logical and ap-
propriate to its message.

• informativeness: a response has meaningful
information relevant to its message.

• grammaticality: a response is fluent and
grammatical.

These aspects are evaluated independently. For
each aspect, three levels are assigned to a response
with scores from 0 to +2 (Shang et al., 2015),
where 0 represents bad and +2 represents excel-
lent. The appropriateness differs from the in-
formativeness in that the former focuses on the
logical relationship between a message and a re-
sponse, while the latter evaluates the richness of
relevant content.

Automatic Evaluation We employ Dist-1 and
Dist-2 (Li et al., 2016a) to evaluate the diversity
of responses, where Dist-k is the number of dis-
tinct k-grams normalized by the total number of
words of responses. We also evaluate the Origi-
nality by computing the ratio of responses that do
not appear in the training set (Wu et al., 2019).

To validate the effectiveness of retrieved candi-
dates in enhancing the discriminator, the classifi-
cation accuracy of the discriminator in AL and our
approach is also reported. Note that the two dis-
criminators after pre-training or adversarial train-
ing cannot be compared directly because they are
trained by different negative samples produced by
different generators. We thus create a special
dataset for this metric where negative samples are
generated by a well-trained generator (otherwise,

the accuracy will easily reach nearly 100% as fixed
negative samples of low quality are too easy to be
distinguished) of AL in advance.

5.2 Analysis

The results of the classification accuracy of dif-
ferent discriminators are presented in Table 4.
Trained on an identical dataset, our discrimina-
tor achieves higher accuracy than the conventional
discriminator in AL. This indicates that the N-best
response candidates are helpful for the discrimina-
tor in distinguishing between human-generated re-
sponses and machine-generated responses, which
could in turn benefit the generator in the adversar-
ial training process (discussed later).

Table 3 shows the results of human evaluation.
Our approach has the highest mean score and the
largest proportions of +2 and +1 in appropriate-
ness. Meanwhile, it outperforms all generation-
based and retrieval-enhanced approaches in infor-
mativeness. This suggests that our approach is
able to respond more appropriately and incorpo-
rate informative content into responses at the same
time. Note that Rtr has the highest informative-
ness mean score due to its diverse human-written
content. However, it may also contain some irrel-
evant information, leading to a bad performance
in appropriateness. Besides, most responses in
Rtr are annotated as +2 or 0 in informativeness.
This is also because Rtr responses are extremely
diverse which always include new content, mak-
ing a response tend to get +2 if the content is rele-
vant, otherwise 0. In terms of grammaticality, the
mean score of our approach is higher than that
of S2S and MS2S, and is comparable with that
of AL, indicating that our approach is competi-
tive in generating fluent responses. Edit has a high
mean score mainly due to its relatively simple sen-
tence structure. As shown in Figure 2, S2S and
MS2S have similar simple sentence structure to



3770

Model # of UNI Dist-1 # of BI Dist-2 Origin

Rtr 6,837 0.113 25,863 0.428 0.000
S2S 1,247 0.023 3,122 0.060 0.288
MS2S 2,596 0.049 6,455 0.122 0.351
EDIT 1,847 0.027 5,690 0.085 0.540
AL 1,760 0.033 6,697 0.124 0.590

D+ 2,057 0.038 8,683 0.158 0.775
G+ 2,440 0.046 10,461 0.200 0.792
Ours 3,356 0.060 13,184 0.236 0.842

Table 5: Automatic evaluation results of the number
of distinct uni-grams (# of UNI) and bi-grams (# of
BI), Dist-1, Dist-2 and Originality (Origin). D+ and
G+ are two variants of our approach where candidates
are only available for the discriminator and the genera-
tor, respectively.

Edit, the reason for the relatively low mean scores
of S2S and MS2S in grammaticality is that they
have some repetitive responses, like “windows,
windows, windows”.

Agreements among different annotators are cal-
culated by Fleiss’ Kappa (Fleiss, 1971). The val-
ues of appropriateness and informativeness are all
in an interval of (0.4, 0.6] or (0.6, 0.8], which can
be seen as “Moderate agreement” and “Substan-
tial agreement”, respectively. Grammaticality has
relatively higher agreement values as it is easier to
reach an agreement on grammatical errors.

We report the results of Dist-1, Dist-2, and
Originality in Table 5. AL outperforms S2S in
all metrics, indicating that adversarial training is
helpful for generating diverse n-grams and re-
sponses. By introducing N-best response candi-
dates, our approach further increases Dist-2 by
0.112 based on AL (from 0.124 to 0.236) and the
improvement is significant (t-test, p <0.01). In
contrast, the increase of Dist-2 after combining N-
best response candidates in MLE based approach
is only 0.062, comparing MS2S with S2S. This
suggests that introducing a discriminator with ad-
versarial training is more effective than MLE ob-
jective in utilizing N-best response candidates to
generate more diverse n-grams. Note that the im-
provement after introducing candidates in Dist-1
and Originality is not as significant as that in Dist-
2. This is because responses of MLE based mod-
els (MS2S and EDIT) tend to contain informative
content with simple sentence structures, like “... is
(not) good.” (as shown in Figure 2), resulting in
high Dist-1 and Originality scores, but their Dist-2
scores are relatively lower than AL and Ours.

To understand the importance of different com-

Utterance Translation

MSG Wi-Fi I have a Wi-Fi signal at 
home, but do not have 
access to the Internet,

what’s the reason?

C#1 I guess there is a problem 
with the call between 

Telecom and Unicom.

C#2 I don't think it's your 
problem.

S2S I think so too.

MS2S My cell phone signal is 
not good.

EDIT This ad is too Telecom.

AL No signal, no signal.

Ours Let’s change to 
Unicom's mobile phone.

Figure 2: An example of a test message (MSG), can-
didates (C#1 and C#2), and responses from different
models. The last column are their translations.

ponents of our approach, we also train two vari-
ants: D+ and G+, where N-best response candi-
dates are only available for the discriminator and
the generator, respectively. Note that AL does not
utilize candidates in the generator nor the discrim-
inator, thus can be seen as a start point of D+
and G+. As shown in Table 5, there is an im-
provement in the performance of both the two vari-
ants after introducing the candidates comparing to
AL. The improvement in G+ is more significant
as its generator can directly utilize the candidates
as generation materials. While candidates’ infor-
mation in D+ is compressed into a discriminative
signal by the discriminator. Nevertheless, intro-
ducing candidates into the discriminator helps to
generate more diverse responses comparing AL
with D+, and G+ with Ours, demonstrating that the
retrieval-enhanced discriminator is able to benefit
the generator.

Figure 2 shows an example of responses of dif-
ferent models along with the input message and N-
best response candidates (C#1 and C#2). The C#1,
which best matches the message among all the
candidates, is also the response of the Rtr baseline.
We can see that it contains diverse content, such
as “Unicom” and “Telecom”(two telecommunica-
tion operators in China, providing broadband, mo-
bile communication as well as customized mobile
phones). However, it talks about “the call” be-



3771

tween the two operators, which is irrelevant to the
message. The response of S2S is a generic re-
sponse. AL has a more diverse response than S2S,
however, it does not have access to candidates,
which limits the diversity. MLE based retrieval-
enhanced models can make use of the content of
candidates, like “Telecom” in EDIT, but the way
they present the content is not as diverse as ours.

6 Conclusion and Future Work

We propose a Retrieval-Enhanced Adversarial
Training method for neural response generation
in dialogue systems. In contrast to existing ap-
proaches, our REAT method directly uses re-
sponse candidates from retrieval-based systems to
improve the discriminator in adversarial training.
Therefore, it can benefit from the advantages of
retrieval-based response candidates as well as neu-
ral responses from generation-based systems. Ex-
periments show that the REAT method signifi-
cantly improves the quality of the generated re-
sponses, which demonstrates the effectiveness of
this approach.

In future research, we will further investigate
how to better leverage larger training data to im-
prove the REAT method. In addition, we will also
explore how to integrate external knowledge in
other formats, like the knowledge graph, into ad-
versarial training so that the quality could be fur-
ther improved.

Acknowledgments

The authors would like to thank all the anonymous
reviewer for their insightful comments. The pa-
per is supported by the National Natural Science
Foundation of China (No. 61772153).

References
Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben-

gio. 2015. Neural machine translation by jointly
learning to align and translate. In Proceedings of
International Conference on Learning Representa-
tions.

Regina Barzilay and Kathleen R McKeown. 2005.
Sentence fusion for multidocument news summa-
rization. Journal of Computational Linguistics,
31(3):297–328.

Wanxiang Che, Zhenghua Li, and Ting Liu. 2010. LTP:
A Chinese language technology platform. In Col-
ing 2010: Demonstrations, pages 13–16, Beijing,
China. Coling 2010 Organizing Committee.

Joseph L Fleiss. 1971. Measuring nominal scale agree-
ment among many raters. Journal of Psychological
bulletin, 76(5):378.

Sepp Hochreiter and Jürgen Schmidhuber. 1997. Long
short-term memory. Journal of Neural computation,
9(8):1735–1780.

Zongcheng Ji, Zhengdong Lu, and Hang Li. 2014. An
information retrieval approach to short text conver-
sation. arXiv preprint arXiv:1408.6988.

Pei Ke, Jian Guan, Minlie Huang, and Xiaoyan Zhu.
2018. Generating informative responses with con-
trolled sentence function. In Proceedings of the
56th Annual Meeting of the Association for Compu-
tational Linguistics (Volume 1: Long Papers), pages
1499–1508, Melbourne, Australia. Association for
Computational Linguistics.

Guillaume Klein, Yoon Kim, Yuntian Deng, Jean
Senellart, and Alexander Rush. 2017. OpenNMT:
Open-source toolkit for neural machine translation.
In Proceedings of ACL 2017, System Demonstra-
tions, pages 67–72, Vancouver, Canada. Association
for Computational Linguistics.

Anton Leuski, Ronakkumar Patel, David Traum, and
Brandon Kennedy. 2006. Building effective ques-
tion answering characters. In Proceedings of the
7th SIGdial Workshop on Discourse and Dialogue,
pages 18–27, Sydney, Australia. Association for
Computational Linguistics.

Jiwei Li, Michel Galley, Chris Brockett, Jianfeng Gao,
and Bill Dolan. 2016a. A diversity-promoting ob-
jective function for neural conversation models. In
Proceedings of the 2016 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics: Human Language Technologies,
pages 110–119, San Diego, California. Association
for Computational Linguistics.

Jiwei Li, Will Monroe, Alan Ritter, Dan Jurafsky,
Michel Galley, and Jianfeng Gao. 2016b. Deep rein-
forcement learning for dialogue generation. In Pro-
ceedings of the 2016 Conference on Empirical Meth-
ods in Natural Language Processing, pages 1192–
1202, Austin, Texas. Association for Computational
Linguistics.

Jiwei Li, Will Monroe, Tianlin Shi, Sébastien Jean,
Alan Ritter, and Dan Jurafsky. 2017. Adversarial
learning for neural dialogue generation. In Proceed-
ings of the 2017 Conference on Empirical Methods
in Natural Language Processing, pages 2157–2169,
Copenhagen, Denmark. Association for Computa-
tional Linguistics.

Kevin Lin, Dianqi Li, Xiaodong He, Zhengyou Zhang,
and Ming-Ting Sun. 2017. Adversarial ranking for
language generation. In Proceedings of the Thirty-
First Conference on Neural Information Processing
Systems, pages 3155–3165.

https://www.aclweb.org/anthology/C10-3004
https://www.aclweb.org/anthology/C10-3004
https://www.aclweb.org/anthology/P18-1139
https://www.aclweb.org/anthology/P18-1139
https://www.aclweb.org/anthology/P17-4012
https://www.aclweb.org/anthology/P17-4012
https://www.aclweb.org/anthology/W06-1303
https://www.aclweb.org/anthology/W06-1303
https://doi.org/10.18653/v1/N16-1014
https://doi.org/10.18653/v1/N16-1014
https://doi.org/10.18653/v1/D16-1127
https://doi.org/10.18653/v1/D16-1127
https://doi.org/10.18653/v1/D17-1230
https://doi.org/10.18653/v1/D17-1230


3772

Diane Litman, Satinder Singh, Michael Kearns, and
Marilyn Walker. 2000. Njfun: a reinforcement
learning spoken dialogue system. In ANLP-NAACL
2000 Workshop: Conversational Systems, pages 17–
20.

Erwin Marsi and Emiel Krahmer. 2005. Explorations
in sentence fusion. In Proceedings of the Tenth Eu-
ropean Workshop on Natural Language Generation
(ENLG-05).

Lili Mou, Yiping Song, Rui Yan, Ge Li, Lu Zhang,
and Zhi Jin. 2016. Sequence to backward and for-
ward sequences: A content-introducing approach to
generative short-text conversation. In Proceedings
of COLING 2016, the 26th International Confer-
ence on Computational Linguistics: Technical Pa-
pers, pages 3349–3358, Osaka, Japan. The COLING
2016 Organizing Committee.

Gaurav Pandey, Danish Contractor, Vineet Kumar, and
Sachindra Joshi. 2018. Exemplar encoder-decoder
for neural conversation generation. In Proceed-
ings of the 56th Annual Meeting of the Association
for Computational Linguistics (Volume 1: Long Pa-
pers), pages 1329–1338, Melbourne, Australia. As-
sociation for Computational Linguistics.

Minghui Qiu, Feng-Lin Li, Siyu Wang, Xing Gao, Yan
Chen, Weipeng Zhao, Haiqing Chen, Jun Huang,
and Wei Chu. 2017. AliMe chat: A sequence to
sequence and rerank based chatbot engine. In Pro-
ceedings of the 55th Annual Meeting of the Associa-
tion for Computational Linguistics (Volume 2: Short
Papers), pages 498–503, Vancouver, Canada. Asso-
ciation for Computational Linguistics.

Steven J Rennie, Etienne Marcheret, Youssef Mroueh,
Jerret Ross, and Vaibhava Goel. 2017. Self-critical
sequence training for image captioning. In Proceed-
ings of the IEEE Conference on Computer Vision
and Pattern Recognition, pages 7008–7024.

Jost Schatzmann, Karl Weilhammer, Matt Stuttle, and
Steve Young. 2006. A survey of statistical user
simulation techniques for reinforcement-learning of
dialogue management strategies. Journal of The
Knowledge Engineering Review, 21(2):97–126.

Iulian V Serban, Alessandro Sordoni, Yoshua Bengio,
Aaron Courville, and Joelle Pineau. 2016. Building
end-to-end dialogue systems using generative hier-
archical neural network models. In Proceedings of
the Thirtieth AAAI Conference on Artificial Intelli-
gence.

Iulian Vlad Serban, Tim Klinger, Gerald Tesauro, Kar-
tik Talamadupula, Bowen Zhou, Yoshua Bengio,
and Aaron C Courville. 2017a. Multiresolution
recurrent neural networks: An application to dia-
logue response generation. In Proceedings of the
Thirty-First AAAI Conference on Artificial Intelli-
gence, pages 3288–3294.

Iulian Vlad Serban, Alessandro Sordoni, Ryan Lowe,
Laurent Charlin, Joelle Pineau, Aaron Courville,
and Yoshua Bengio. 2017b. A hierarchical latent
variable encoder-decoder model for generating di-
alogues. In Proceedings of the Thirty-First AAAI
Conference on Artificial Intelligence.

Lifeng Shang, Zhengdong Lu, and Hang Li. 2015.
Neural responding machine for short-text conver-
sation. In Proceedings of the 53rd Annual Meet-
ing of the Association for Computational Linguistics
and the 7th International Joint Conference on Natu-
ral Language Processing (Volume 1: Long Papers),
pages 1577–1586, Beijing, China. Association for
Computational Linguistics.

Yiping Song, Rui Yan, Cheng-Te Li, Jian-Yun Nie,
Ming Zhang, and Dongyan Zhao. 2018. An ensem-
ble of retrieval-based and generation-based human-
computer conversation systems. In Proceedings of
the 27th International Joint Conference on Artificial
Intelligence and the 23rd European Conference on
Artificial Intelligence.

Alessandro Sordoni, Michel Galley, Michael Auli,
Chris Brockett, Yangfeng Ji, Margaret Mitchell,
Jian-Yun Nie, Jianfeng Gao, and Bill Dolan. 2015.
A neural network approach to context-sensitive gen-
eration of conversational responses. In Proceed-
ings of the 2015 Conference of the North Ameri-
can Chapter of the Association for Computational
Linguistics: Human Language Technologies, pages
196–205, Denver, Colorado. Association for Com-
putational Linguistics.

Ilya Sutskever, Oriol Vinyals, and Quoc V Le. 2014.
Sequence to sequence learning with neural net-
works. In Proceedings of the Twenty-Eighth Con-
ference on Neural Information Processing Systems,
pages 3104–3112.

Oriol Vinyals and Quoc Le. 2015. A neural conversa-
tional model. arXiv preprint arXiv:1506.05869.

Joseph Weizenbaum. 1966. Eliza—a computer pro-
gram for the study of natural language communica-
tion between man and machine. Journal of Commu-
nications of the ACM, 9(1):36–45.

Tsung-Hsien Wen, David Vandyke, Nikola Mrkšić,
Milica Gasic, Lina M. Rojas Barahona, Pei-Hao Su,
Stefan Ultes, and Steve Young. 2017. A network-
based end-to-end trainable task-oriented dialogue
system. In Proceedings of the 15th Conference of
the European Chapter of the Association for Compu-
tational Linguistics: Volume 1, Long Papers, pages
438–449, Valencia, Spain. Association for Compu-
tational Linguistics.

Jason D Williams and Steve Young. 2007. Partially ob-
servable markov decision processes for spoken dia-
log systems. Journal of Computer Speech & Lan-
guage, 21(2):393–422.

https://www.aclweb.org/anthology/C16-1316
https://www.aclweb.org/anthology/C16-1316
https://www.aclweb.org/anthology/C16-1316
https://www.aclweb.org/anthology/P18-1123
https://www.aclweb.org/anthology/P18-1123
https://doi.org/10.18653/v1/P17-2079
https://doi.org/10.18653/v1/P17-2079
https://doi.org/10.3115/v1/P15-1152
https://doi.org/10.3115/v1/P15-1152
https://doi.org/10.3115/v1/N15-1020
https://doi.org/10.3115/v1/N15-1020
https://www.aclweb.org/anthology/E17-1042
https://www.aclweb.org/anthology/E17-1042
https://www.aclweb.org/anthology/E17-1042


3773

Ronald J Williams. 1992. Simple statistical gradient-
following algorithms for connectionist reinforce-
ment learning. Journal of Machine Learning,
3(8):229–256.

Yu Wu, Furu Wei, Shaohan Huang, Zhoujun Li, and
Ming Zhou. 2019. Response generation by context-
aware prototype editing. In Proceedings of the
Thirty-Third AAAI Conference on Artificial Intelli-
gence.

Yu Wu, Wei Wu, Chen Xing, Can Xu, Zhoujun Li, and
Ming Zhou. 2017. A sequential matching frame-
work for multi-turn response selection in retrieval-
based chatbots. arXiv preprint arXiv:1710.11344.

Chen Xing, Wei Wu, Yu Wu, Jie Liu, Yalou Huang,
Ming Zhou, and Wei-Ying Ma. 2017. Topic aware
neural response generation. In Proceedings of the
Thirty-First AAAI Conference on Artificial Intelli-
gence, volume 17, pages 3351–3357.

Jingjing Xu, Xuancheng Ren, Junyang Lin, and
Xu Sun. 2018. Diversity-promoting GAN: A cross-
entropy based generative adversarial network for di-
versified text generation. In Proceedings of the 2018
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 3940–3949, Brussels, Bel-
gium. Association for Computational Linguistics.

Rui Yan, Yiping Song, and Hua Wu. 2016. Learning
to respond with deep neural networks for retrieval-
based human-computer conversation system. In
Proceedings of the 39th International ACM SIGIR
Conference on Research and Development in Infor-
mation Retrieval, pages 55–64. ACM.

Liu Yang, Minghui Qiu, Chen Qu, Jiafeng Guo,
Yongfeng Zhang, W Bruce Croft, Jun Huang,
and Haiqing Chen. 2018. Response ranking with
deep matching networks and external knowledge in
information-seeking conversation systems. In Pro-
ceedings of The 41st International ACM SIGIR Con-
ference on Research and Development in Informa-
tion Retrieval, pages 245–254.

Weinan Zhang, Lingzhi Li, Dongyan Cao, and Ting
Liu. 2017. Exploring implicit feedback for open do-
main conversation generation. In Proceedings of the
Thirty-Second AAAI Conference on Artificial Intelli-
gence, pages 547–554.

Yizhe Zhang, Michel Galley, Jianfeng Gao, Zhe Gan,
and Bill Dolan. 2018. Generating informative and
diverse conversational responses via adversarial in-
formation maximization. In Proceedings of the
Thirty-Second Conference on Neural Information
Processing Systems, pages 1810–1820.

Hao Zhou, Tom Young, Minlie Huang, Haizhou Zhao,
Jingfang Xu, and Xiaoyan Zhu. 2018. Com-
monsense knowledge aware conversation genera-
tion with graph attention. In the 27th International
Joint Conference on Artificial Intelligence and the
23rd European Conference on Artificial Intelligence,
pages 4623–4629.

https://www.aclweb.org/anthology/D18-1428
https://www.aclweb.org/anthology/D18-1428
https://www.aclweb.org/anthology/D18-1428

