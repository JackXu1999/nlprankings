



















































Multiword Expression-Aware A* TAG Parsing Revisited


Proceedings of the 13th International Workshop on Tree Adjoining Grammars and Related Formalisms (TAG+13), pages 84–93,
Umeå, Sweden, September 4–6, 2017. c© 2017 Association for Computational Linguistics

Multiword Expression-Aware A? TAG Parsing Revisited

Jakub Waszczuk
LIFO

Université d’Orléans
6, rue Léonard de Vinci
45067 Orléans, France

Agata Savary
Laboratoire d’Informatique

Université François-Rabelais Tours
3, place Jean-Jaurès
41000 Blois, France

firstname.lastname@univ-{orleans|tours}.fr

Yannick Parmentier
LIFO

Université d’Orléans
6, rue Léonard de Vinci
45067 Orléans, France

Abstract

A? algorithms enable efficient parsing
within the context of large grammars
and/or complex syntactic formalisms. Be-
sides, it has been shown that promoting
multiword expressions (MWEs) is a ben-
eficial strategy in dealing with syntactic
ambiguity. The state-of-the-art A? heuris-
tic for promoting MWEs in tree-adjoining
grammar (TAG) parsing has certain draw-
backs: it is not monotonic and it com-
poses poorly with grammar compression
techniques. In this work, we propose an
enhanced version of this heuristic, which
copes with these shortcomings.

1 Introduction

In the domain of syntactic parsing, a growing in-
terest is dedicated to A? parsing strategies (Klein
and Manning, 2003) which allow to compute the
most plausible parse tree(s) without having to gen-
erate the space of all the grammar-compliant so-
lutions in advance. Such strategies enable effi-
cient parsing within the context of large gram-
mars and/or complex syntactic formalisms (An-
gelov and Ljunglöf, 2014). They were also shown
to finely combine with probabilistic supertagging,
which can be used to pre-score the individual so-
lutions and, hence, guide the parser to quickly find
the most probable one(s) (Lewis and Steedman,
2014). Once supertagging is correctly performed,
determining the corresponding syntactic structure
is greatly simplified (Bangalore and Joshi, 1999),
and A? parsing allows to backtrack and correct the
possible misestimations of the supertagger.

Lewis and Steedman (2014) showed that it
is possible to obtain a highly accurate and fast
CCG parser by (i) adopting a simple probabilis-
tic model factored on lexical category assign-

ments and (ii) using supertagging techniques to
obtain relatively reliable probability estimations in
a sentence-dependent manner. However, it seems
that this proposal, even though quite generic, is
not easy to extend to the context of a lexicalized
grammar in which various categories of multiword
expressions (MWEs) are represented as elemen-
tary grammar units, which is a standard approach
to modeling MWEs in tree-adjoining grammars,
TAGs (Abeillé and Schabes, 1989; Abeillé, 1995).
While adapting the (Lewis and Steedman, 2014)
approach to continuous MWEs, such as all of a
sudden or a hot dog, could be achieved by using
word lattices to represent ambiguous input seg-
mentations (Constant et al., 2013), non-continuous
MWEs (as in he is making no good decisions)
seem harder to account for.

At the same time, Wehrli (2014) showed that
promoting collocation-based derivations in syn-
tactic parsing – MWEs often are statistical collo-
cations – is potentially beneficial in that it helps
to deal with syntactic ambiguity. MWEs account
for up to 40% of words in a corpus (Gross and
Senellart, 1998) and, due to their lexicalized na-
ture, should be easy to spot before parsing, which
suggests the usefulness of enriching A? parsing
strategies with MWE-dedicated mechanisms.

We previously proposed such a mechanism
(Waszczuk et al., 2016b), extending the (Lewis
and Steedman, 2014) heuristic to a variant allow-
ing multi-anchored TAG elementary trees (ETs).
We showed that considerable speed-up gains can
be achieved by promoting MWE-based analyses
with virtually no loss in syntactic parsing ac-
curacy. This is in contrast to purely statistical
approaches where the idiosyncratic properties of
MWEs are hard to capture, and systematically pro-
moting MWEs may decrease the parser’s accuracy
due to the lack of control of the underlying gram-
mar which would certify the plausibility of the re-

84



Figure 1: A toy TAG grammar

sulting derivations.
However, the MWE-based A? heuristic pro-

posed in (Waszczuk et al., 2016b) has three impor-
tant drawbacks. Firstly, it lacks the proof of cor-
rectness and, therefore, it is not clear whether the
first derivation found by the parser is indeed the
most probable one, a property normally assured by
a well-defined A? heuristic. Secondly, it produces
sub-optimal estimations – the bottom-up Early-
style parser the heuristic relies on (Waszczuk et al.,
2016a) performs predictions in order to identify
the spans over which adjunction can be poten-
tially performed, but the inside probabilities re-
lated to such spans are ignored. Finally, comput-
ing the values of the heuristic is relatively easy (it
can be performed in close to constant time), but it
composes badly with grammar compression. In a
real-world context, different parsing speed-up op-
timizations should ideally combine to provide an
optimal solution. In this work, we propose an en-
hanced version of this heuristic, which copes with
the three drawbacks mentioned above.

We first describe the baseline parser and its
heuristic, and give a motivating example demon-
strating its drawbacks (Sec. 2). Then we formal-
ize a compressed grammar representation (Sec. 3)
and our new parser adapted to it (Sec. 4). We in-
troduce the enhanced heuristic and sketch a proof
of its monotonicity (Sec. 5). Finally we present
some experimental results (Sec. 6), as well as con-
clusions and future work (Sec. 7).

2 Baseline heuristic

In this section, we shortly review the heuristic pro-
posed in (Waszczuk et al., 2016b) and point out its
shortcomings. We assume the toy TAG grammar
from Fig. 1, where each node is marked with a
unique ID. This grammar contains two ETs repre-
senting two possibly discontinuous verbal MWEs

Figure 2: Traversal configurations

(making decisions and is no good). We also as-
sume an input sentence s = s1 . . . sn being a se-
quence of terminal symbols.

The parser in (Waszczuk et al., 2016b) is based
on deduction rules (Shieber et al., 1995), which
serve to infer chart items. Each chart item is a
pair 〈x, r〉, where x is a configuration and r is a
span. Each configuration x represents a position in
the traversal of the corresponding ET tx, and each
span r represents a fragment of the input sentence.
Informally, 〈x, r〉 asserts that the already traversed
part of tx can be matched against the words in r.

Fig. 2 shows three different configurations cor-
responding to the traversal of the ET rooted at
S10. For instance, Fig. 2 (a) stipulates that the
parser has already matched the nodes VP13 and
making16, and that it still needs to match the re-
maining nodes in the ET. We will refer to the
configurations by the nodes at which they are
rooted – i.e., to Fig. 2 (a), (b), and (c) by {V13},
{V13,NP14}, and {VP12}, respectively. Any
configuration x determines the split of the ter-
minals present in tx into two parts: (i) the ter-
minals already scanned by the parser, and (ii)
the remaining terminals, which the parser still
needs to match against the input words. We de-
note by inf (x) and sup(x) the multiset of ter-
minals (written {}ms) in part (i) and (ii), respec-
tively. For instance, inf ({V13}) = {making}ms,
sup({V13}) = {decisions}ms, inf ({AP37}) =
{no, good}ms, sup({AP37}) = {is}ms, etc.1

Let pos(s) = {0, . . . , n} be the set of posi-
tions between the words in s, before s1 and af-
ter sn. In TAGs, the yield of an ET can span
two non-adjacent fragments of s, hence a span
takes the form of a tuple r = 〈i, j, k, l〉, where
i, l ∈ pos(s) and j, k ∈ pos(s) ∪ {−}. 〈j, k〉,
when defined, represents the gap between the two
non-adjacent fragments 〈i, j〉 and 〈k, l〉. For in-
stance, in Fig. 3, 〈2,−,−, 6〉 corresponds to con-
tinuous making no good decisions, 〈2, 3, 5, 6〉 cor-
responds to discontinuous making decisions with

1We extend the standard set operations (sum, difference,
etc.) to multisets.

85



the gap no good, etc. We will refer to spans
of the form 〈i,−,−, l〉 as 〈i, l〉 for short. Each
span r determines the split of the terminals present
in the input sentence into: (i) the terminals in
r, and (ii) the terminals outside r. We denote
by in(r) and out(r) the multiset of terminals
in part (i) and (ii), respectively. For instance,
in(〈2, 6〉) = {making, no, good, decisions}ms,
out(〈2 , 6 〉) = {he, is}ms, in(〈2, 3, 5, 6〉) =
{making, decisions}ms, out(〈2, 3, 5, 6〉) =
{he, is, no, good}ms, etc.

Given a deduced chart item 〈x, r〉, it holds that
inf (x) ⊂ in(r). Moreover, if sup(x) 6⊂ out(r),
then the item is a dead-end – no final derivations
(i.e. the derivations covering the entire input sen-
tence) based on x can be created (the tokens re-
maining to parse do not contain all the terminals
present in the remaining part of tx’s traversal).

To each derivation constructed via the deduction
rules the parser assigns a weight which allows to
discriminate the more probable (lighter) from the
less probable (heavier) derivations. Given a chart
item η, the goal of an A? heuristic h(η) is to esti-
mate α(η), the minimal outside derivation weight
(roughly, the cost remaining to parse the entire in-
put sentence). Formally, α(η) = γ(η) − β(η),2
where γ(η) is the minimal weight of a final deriva-
tion containing η, and β(η) is the minimal weight
of an inside derivation of η (roughly, the cost of
the already parsed part of the sentence).

In (Waszczuk et al., 2016b) we assume a simple
weighting scheme in which each ET t comes with
its own weight ωt ≥ 0 and the weight of a deriva-
tion is the sum of the weights of the participating
ETs. Consider Fig. 3, where each ET has weight
1, and two items: ηgr = 〈{VP21}, 〈1, 2, 6, 6〉〉
corresponding to the derivation marked in green
(solid line), and ηbl = 〈{VP12}, 〈2, 6〉〉 corre-
sponding to the derivation marked in blue (dashed
line). Then, β(ηgr ) = 1 and β(ηbl ) = 2.

A heuristic should be admissible, i.e., never
overestimate α(η), and monotonic. Monotonic-
ity means that, when a chart item η is inferred
from another item µ contained in its lowest-weight
derivation, h(η) + β(η) should be at least as high
as h(µ) + β(µ) (Klein and Manning, 2002).

The A? heuristic presented in (Waszczuk et al.,
2016b) is based on the observation that the weight
of a particular TAG derivation tree can be refor-

2This equation holds provided that the weighting function
is monotonic in the sense of (Huang and Chiang, 2005).

Figure 3: Projecting the weights of the ETs in a
TAG derivation on the corresponding terminals.
The weights of the ETs are shown, in square
brackets, above their roots, while the projected
weights are shown, also in square brackets, below
the terminals.

mulated as follows. Firstly, the weights of the in-
dividual ETs are projected over the words in the
input sentence to which they are attached. There
are several possible ways of doing that, and the
simplest one is to evenly distribute the weight of
a given ET over its terminals, as shown in Fig. 3.
Secondly, the weight of a derivation tree is rede-
fined as the sum of the weights projected over the
words in the sentence.

When the parser considers a particular item
〈x, r〉, the weights which can be projected over
the words in r are known, but not the weights
which can be projected over the words outside r.
One can easily find the lower-bound estimates of
the latter – i.e., assume that the minimum possi-
ble weight, denoted minw(w) (e.g., minw(is) =
1
3 , minw(making) =

1
2 , etc.), will be projected

over each word w outside r. Consequently, in
(Waszczuk et al., 2016b) we define the baseline
heuristic as:

h(〈x, r〉) =





C(out(r)), if comp(x)
∞, if sup(x) 6⊂ out(r)
ωtx + C(out(r) \ sup(x)), o/w,

where C(m) is the globally minimal cost of scan-
ning all the words in the multiset m (the sum
of their minw values) and comp(x) is true iff
the traversal represented by x is complete (all the

86



nodes in tx have been parsed). For instance:

h(ηgr ) = C(out(〈1, 2, 6, 6〉))
= C({he,making, no, good, decisions}ms)

= 1 +
1

2
+

1

3
+

1

3
+

1

2
= 2 +

2

3
,

h(ηbl ) = 1 + C({he, is}ms) = 2 +
1

3
.

The heuristic does not take the weight ωtx of
the ET tx being parsed into account if x is com-
plete. This is because ωtx is already added to the
weight of the inside derivation of 〈x, r〉 in this
case. Moreover, comp(x) =⇒ sup(x) = ∅ms.

This heuristic presents some important short-
comings, foremostly non-monotonicity. Given
the predictive nature of the parser, an item
〈x, 〈i, j, k, l〉〉 such that 〈j, k〉 6= 〈−,−〉 is inferred
iff an appropriate derivation, spanning 〈j, k〉, ex-
ists. In Fig. 3, ηgr thus relies on ηbl and when
ηgr is inferred, the weights which can be projected
over the words in 〈2, 6〉 are already known. Never-
theless, h(ηgr ) assumes that the globally minimal
weights will be projected over 〈2, 6〉, thus under-
estimating the weights projected over the words no
and good. In Fig. 3, these projections are equal to
1, while minw(no) = minw(good) = 13 , since
both words belong to the MWE is no good. As a
result, h(ηgr ) + β(ηgr ) = 223 + 1 is smaller than
h(ηbl ) + β(ηbl ) = 2

1
3 + 2, which shows that the

heuristic is not monotonic.
A similar situation occurs in top-down CFG

parsing with prediction when the weight of the
premise item of the prediction rule is not trans-
ferred to the conclusion. Nederhof (2003) shows
that a simplified version of the A? algorithm (the
Knuth’s algorithm, similar to the standard Dijk-
stra’s shortest-path algorithm and relying on no
heuristic) still correctly computes the derivations
with the lowest weights in this case. While it
can be stipulated that his proof applies to the al-
gorithm used in (Waszczuk et al., 2016b), ignor-
ing the weights of the items used for adjunction-
related predictions not only makes the heuristic
non-monotonic, it also means that the already
computed inside weights, which could provide
better estimations of α(x), are sometimes ignored.

3 Grammar representation

The complexity of TAG parsing is polynomial in
the sentence length and linear in the grammar size

(O(n6 ∗ |G|)) (Gardent et al., 2014). With real-
size grammars, especially those containing explic-
itly encoded MWEs, the latter factor can be pro-
hibitive. Therefore – in order to speed up parsing –
an A? algorithm should ideally be combined with
grammar compression, as proposed below.

We assume a twofold representation of a TAG
grammar. It is first transformed into an equiva-
lent directed acyclic graph with ordered outgoing
edges for each node (GDAG). Then, traversals of
the ETs and their subtrees are represented as paths
in finite-state automata (FSAs). This is a simpli-
fied variant of the grammar encoding applied in
(Waszczuk et al., 2016a), where we showed that
grammar compression alone can greatly speed-up
TAG parsing, while using a variant of the stan-
dard, bottom-up Earley-style parser (Alonso et al.,
1999).

Formally, we define a GDAG as a tuple D =
〈VD, ED,ΣD, ND, SD, `D, footD〉 such that VD
and ED are the sets of DAG nodes and (ordered)
edges, respectively, ΣD andND are the sets of ter-
minals and non-terminals, respectively, SD ∈ ND
is the start symbol, `D : VD → ΣD∪ND is a func-
tion assigning non-terminals or terminals to D’s
nodes, and footD : VD → B tells whether a given
node is a foot node. Also, RD, LD ⊂ VD are the
sets of roots and leaves in D, respectively.

Fig. 1 is a straightforward encoding where each
ET is represented by a separate tree. Fig. 4 com-
pares two GDAG representations of three ETs
from Fig. 1: (a) the straightforward encoding, and
(b) the encoding with common subtrees shared
among ETs. Both representations are equivalent,
i.e., they entail the same TAG, which can be ob-
tained by the traversals of the sub-DAGs rooted in
the GDAG roots. For instance, the traversals start-
ing from S10 in (a) and (b) entail the same ET.

We assume that a GDAG satisfies all the rele-
vant, TAG-related well-formedness constraints –
for example, that for each r ∈ RD there exists
at most one v ∈ VD reachable from r such that
footD(v). Moreover, if such v ∈ VD exists, then
`D(r) = `D(v) ∈ ND, i.e., the root and the foot
of an auxiliary tree must be labeled with the same
non-terminal. Each v ∈ VD unambiguously deter-
mines the corresponding ET subtree (ES) rooted at
v, denoted esD(v). Note that, by definition, each
ET is also an ES.

Fig. 5 illustrates two possible FSA encodings,
without and with prefix sharing, of the GDAG

87



Figure 4: Two possible DAG encodings of three
ETs from the grammar in Fig. 1. The values of `D
are put in place of the corresponding nodes, and
the node identifiers (the values of VD) are placed
in subscript on the right. All the edges are implic-
itly oriented downwards.

from Fig. 4 (b).3 From the parsing perspective,
each path in an FSA encoding represents the left-
to-right traversal performed by the parser while
matching, against the input words, a particular ES
of height greater than 0.

Formally, we define an FSA encod-
ing of a GDAG D as a tuple M =
〈QM , δM , SM , headsM 〉 such that QM is the
set of FSA states, δM : QM × VD → QM is
the transition function consuming DAG nodes
(VD thus constitutes the set of the FSA alphabet
symbols), SM ⊂ QM is the set of start states,
and headsM : QM → 2VM is a function which
returns the final symbols outgoing from a given
state. For convenience, we represent the particular
states in the FSA encoding as dotted rules, e.g.,
S1 → NP2 • VP3 corresponds to the state
reached by following the symbol NP2 on the path
(NP2,VP3, S1). Informally, headsM represent
the left-hand-sides of such dotted rules.

Each x ∈ VD ∪ QM represents a particular
traversal configuration (cf. Sec. 2): x ∈ VD stip-
ulates that the parser has matched esD(x), while
x ∈ QM stipulates that it has matched all the
ESs on the path from one of the states in SM to
x ∈ QM . For instance, VP3 → V4NP2• stip-
ulates that the parser has matched both esD(V4)
and esD(NP2).

As mentioned in Sec. 2, when no gram-
mar compression (i.e. no subtree or prefix
sharing) occurs, each traversal configuration

3While a minimal FSA encoding, including both pre-
fix and suffix sharing, might seem optimal, we empirically
showed in (Waszczuk et al., 2016a) that it does not bring sig-
nificant improvements over the prefix-tree encoding in TAG
parsing.

Figure 5: Two possible FSA encodings of the
GDAG illustrated in Fig. 4 (b): (a) a simple FSA
encoding in which each GDAG traversal is repre-
sented by a distinct path, and (b) a prefix-tree-like
FSA encoding.

corresponds to exactly one ET tx. With gram-
mar compression, however, there can be many
ETs corresponding to a given x ∈ VD ∪ QM .
We denote their set by Tx. For instance, in
Fig. 4 (b) and 5 (b), TV4 = TVP3→V4•NP2 =
TVP12→V4•NP14 = {esD(S1), esD(S10)},
TNP7→•N8 = {esD(v) : v ∈ RD}, etc.

This brings us to the second issue with the
heuristic defined in Sec. 2. One of the first steps
of computing the value of h(〈x, r〉) is to check
whether sup(x) ⊂ out(r). This step allows to
obtain better estimations of α(〈x, r〉) – namely, to
exclude the dead-end chart items, which cannot
possibly lead to final derivations. However, with
grammar compression, sup depends not only on
the configuration x, but also on the corresponding
ET t ∈ Tx. Hence, we define a generalized ver-
sion of sup as a two-argument function sup(x, t)
which determines the multiset of words remaining
to parse on t’s traversal starting from x.

When estimating α(〈x, r〉), the parser needs to
consider all the different ET traversals contain-
ing x, leading to different ETs, and with differ-
ent multisets of the words remaining to complete
the traversal. Assuming the prefix-tree grammar
compression and that q0 is the root of the prefix-
tree FSA encoding, Tq0 is the set of all grammar
ETs. Therefore, when grammar compression is
used, computing h’s values is O(N), where N is
the number of ETs.

4 Enhancing the baseline A? TAG parser

In (Waszczuk et al., 2016b,a) we put forward a
version of the bottom-up, Earley-like parser de-
scribed in (Alonso et al., 1999) to test the idea of
promoting MWEs in A? parsing. We now formal-
ize an enhanced version of this parser, adapted to

88



AX (axiom):
(0,0):〈q0,〈i,i〉〉a

i∈pos(s)\{n}
q0∈SM

SC (scan): (w,w
′):〈q,〈i,j,k,l〉〉a

(w,w′):〈δM (q,v),〈i,j,k,l+1〉〉a
v∈LD : `D(v)=sl+1
δM (q,v) defined

DE (deactivate): (w,w
′):〈q,〈i,j,k,l〉〉a

(w+[ωesD(v)|v∈RD],w′):〈v,〈i,j,k,l〉〉p
v∈headsM (q)

PS (pseudo-subst.): (w1,w
′
1):〈q,〈i,j,k,l〉〉a (w2,w′2):〈v,〈l,j′,k′,l′〉〉p

(w1+w2,w′1+w
′
2):〈δM (q,v),〈i,j∪j′,k∪k′,l′〉〉a

δM (q,v) defined

SU (substitution): (w1,w
′
1):〈q,〈i,j,k,l〉〉a (w2,0):〈v,〈l,l′〉〉p

(w1+w2,w′1):〈δM (q,v′),〈i,j,k,l′〉〉a
v′∈LD : `D(v′)=`D(v)∧¬footD(v′)

δM (q,v
′) defined

v∈RD

FA (foot-adjoin): (w1,0):〈q,〈i,l〉〉a (w2,w
′
2):〈v,〈l,j′,k′,l′〉〉p

(w1,w2+w′2+[A(v)|v/∈RD]):〈δM (q,v′),〈i,l,l′,l′〉〉a
v′∈LD : `D(v′)=`D(v)∧footD(v′)

δM (q,v
′) defined

v∈RD =⇒ (j′,k′)=(−,−)

RA (root-adjoin): (w1,w
′
1):〈w,〈i,j,k,l〉〉p (w2,w′2):〈v,〈j,j′,k′,k〉〉p

(w1+w2,w′2):〈v,〈i,j′,k′,l〉〉p
w∈RD∧(j,k)6=(−,−)

`D(w)=`D(v)
v∈RD =⇒ (j′,k′)=(−,−)

Table 1: Weighted inference rules of the Earley-style, bottom-up parser, where: (in PS) i ∪ j is equal to
i if j = − and j otherwise, and (in DE and FA) [x|p] = x if p is true and [x|p] = 0 otherwise.

the grammar representation introduced in Sec. 3,
and to the new heuristic defined in Sec. 5.

We say that a chart item 〈x, r〉 is active if x ∈
QM , and that it is passive if x ∈ VD. We also write
〈q, r〉a or 〈v, r〉p to refer to an active or a passive
item, respectively.

Tab. 1 specifies the inference rules of the parser,
using the weighted deductive framework (Shieber
et al., 1995; Nederhof, 2003). Each of the in-
ference rules takes zero, one or two chart items
on input (premises, presented above the horizon-
tal line) and yields a new item (conclusion, pre-
sented below the line) to be added to the chart
if the conditions given on the right-hand side are
met. Besides, to each item η in each rule a
pair of weights (w,w′), given before the colon,
is assigned, thus specifying how to compute the
weights corresponding to the conclusion based on
the weights corresponding to the premises. The
meaning of w′ and A (the latter used to compute
w′s values in the FA rule), both specific to the en-
hanced heuristic, will be detailed in Sec. 5. The
value w, on the other hand, represents the weight
of η’s inside derivation. The idea of computing
pairs of weights using inference rules comes from
Nederhof (2003), who showed that such a solution
can be used to overcome the non-monotonicity is-
sue in top-down CFG parsing with prediction.

The way the inside weights are computed by
the parser corresponds to the assumption that the
probability of a derivation is the product of the
probabilities of the participating ETs, hence the

weight computed for the conclusion item is the
sum of the weights of the premise items, with two
exceptions. The DE inference rule, responsible for
matching full ESs, adds the weight esD(v), pro-
vided that esD(v) is an ET. The FA rule, used
to predict that adjunction over a particular span
r is possible, does not transfer the weight of the
premise item spanned over r. In the baseline
parser, the same behavior was precisely the rea-
son of the non-monotonicity in Fig. 3 discussed
in Sec. 2. However, now this weight is accounted
for in w′, which preserves monotonicity, as shown
below.

5 Enhanced heuristic

We now propose an enhanced version of the
heuristic described in Sec. 2, with the goal of
overcoming the issues related to non-monotonicity
and grammar compression. However, for the sake
of clarity, we start by assuming that no grammar
compression is performed.

As mentioned before, w represents the weight
of an inside derivation of the corresponding item
η. The weight w′, on the other hand, represents
(roughly) the weight of η’s witness derivation, i.e.,
the previously obtained derivation which can fill
η’s gap. Consider Fig. 6 (b) and three chart items:
µgr = 〈S8, 〈1, 2, 4, 5〉〉, µbl = 〈S3, 〈2, 2, 3, 4〉〉,
and µrd = 〈S6, 〈2, 3〉〉. Then, the derivation
delimited by the green solid line is µgr ’s inside
derivation, while the derivation delimited by the
blue dashed line is µgr ’s witness derivation. Sim-

89



Figure 6: Copy language

ilarly, the derivation delimited by the blue line is
µbl ’s inside derivation, while the derivation delim-
ited by the red dotted line is µbl ’s witness. Finally,
the derivation delimited by the red line is µrd ’s in-
side derivation, but µrd has no witness derivation,
since it is not gapped. In general, for any non-
gapped item η, w′ = 0.

Given a configuration x ∈ VD ∪QM , we define
x’s amortized weight as:

A(x) = ωtx − C(sup(x)).

For instance, in Fig. 6 (a), A(S3) = A(S10) = 12
(since minw(a) = minw(b) = 12 ), and A(S6) =
A(S1) = 1. A(x) accounts for the weight of the
ET tx, and for the fact that tx may still contain
some terminals which need to be consumed (w.r.t.
to the position of x in tx’s traversal). Thus, A(x)
can be intuitively understood as the weight of the
already parsed part of tx.

A version of h which performs no dead-end de-
tection, i.e. never takes the ∞ value unlike h in
Sec. 2, but, otherwise, provides identical estima-
tions as h, can be defined in terms of A as:

hspl (〈x, r〉) =
{
C(out(r)), if x ∈ RD
A(x) + C(out(r)), o/w.

We also define the amortized weight of a deriva-
tion δ as the sum of the amortized weights of the
ESs (more precisely, their roots) present in δ. For
instance, in Fig. 6 (b), the amortized weight of
µbl ’s inside derivation is A(S6) +A(S3) = 1 + 12 .
The weight w′, attached to each chart item η,
is precisely the amortized weight of η’s witness
derivation (if η is gapped, otherwise w′ = 0).

Then, given a chart item η = 〈x, r〉 such that
r = 〈i, j, k, l〉 and the corresponding weight w′,
we define the enhanced A? heuristic as:

hadj (η) =

{
C(rest(r)) + w′, if x ∈ RD
A(x) + C(rest(r)) + w′, o/w,

Figure 7: A fragment of an inside derivation of
〈S8, 〈1, 2, 4, 5〉〉p, represented as a hyperpath.

where rest(r) = out(r) \ in(〈j, k〉), i.e. the
multiset of words outside r but not in the gap,
whose cost is already accounted for in w′. The
advantage over the baseline heuristic is precisely
that, instead of assuming that the lowest possi-
ble weights will be projected over all words in the
gap, the weights of the existing derivations span-
ning the gap are considered. The disadvantage is
that, by not checking that the remaining part of the
sentence contains all the tokens required by tx’s
traversal, hadj does not detect the dead-end items,
which is the price to pay for better integration with
grammar compression techniques (see below).

Fig. 7 shows a fragment of the inference corre-
sponding to µgr ’s inside derivation in Fig. 6 (b).
Each node in Fig. 7 represents a deduced chart
item, and each hyperarc (which connects one tar-
get node with zero, one or two tail nodes) repre-
sents an application of an inference rule. Besides,
to each item the corresponding pair of weights
(w,w′) is attached via an undirected dashed edge.
In particular, the pair attached to 〈S8, 〈1, 2, 4, 5〉〉
is (1, 1.5), since the amortized weight of its wit-
ness derivation is 1.5. Note that the baseline
heuristic would assume that the globally minimal
weights minw(a) = 0.5 are projected over both a
in the gap and, thus, underestimate the gap’s pars-
ing cost as 1.

The enhanced heuristic composes smoothly
with grammar compression techniques. To re-
call, under compression, each configuration x ∈
VD ∪ QM can belong to several ETs and, conse-

90



quently, to several ET traversals. To ensure that
the heuristic does not overestimate, it is sufficient
to calculate the minimal amortized weight, i.e., as-
sume that the least-weight traversal will be taken
from any given x ∈ VD ∪QM :

A(x) = min{ωt − C(sup(x, t)) : t ∈ Tx}. (1)

The definition of hadj remains unchanged. The
values of A can be pre-computed on a per-
grammar basis, just as the values of C◦rest4 can be
pre-computed on a per-sentence basis,5 hence the
robustness of hadj w.r.t. grammar compression.

To show that hadj is monotonic, it is sufficient to
consider each inference rule from Tab. 1 separately
and to show that the total weight (w + hadj (η))
it computes for the conclusion item η is at least
as high as the total weight of any of its premise
items. As an example, we sketch below the proof
of monotonicity of the PS rule.

Proposition 1. Let v ∈ VD and q ∈ QM such that
δM (q, v) is defined. Then, TδM (q,v) ⊂ T (q).

Proof. Follows from the fact that any FSA path
crossing δM (q, v) must also cross v.

Observation 1. Let v ∈ VD, q ∈ QM such
that δM (q, v) defined, and t ∈ TδM (q,v). Then,
C(sup(δM (q, v), t)) = C(sup(q, t))− C(inf (v)).
Proposition 2. Let v ∈ VD and q ∈ QM such that
δM (q, v) defined. Then,

A(δM (q, v)) ≥ A(q) + C(inf (v)).

Proof. Follows from Pr. 1, Ob. 1, and Eq. 1.

Proposition 3. Let η = 〈x, r〉 be a chart item such
that x /∈ RD, r = 〈i, j, k, l〉, and (w,w′) be the
corresponding weights. Then,

w + w′ ≥ C(in(〈i, l〉))− C(inf (x)).

Proof. The LHS is the total weight projected by
η’s inside derivation over 〈i, l〉, with the exception
of the words in inf (x). The RHS is, by definition,
a lower-bound for the projected weight.

Observation 2. In the PS rule, it holds that:
C(rest(〈i, l′〉)) = C(rest(〈i, l〉))− C(in(〈l, l′〉)).
Proposition 4. The PS rule is monotonic.

4The symbol ◦ stands for function composition.
5In both cases dynamic programming techniques can be

used to speed up the process.

Proof. Let η be the conclusion and µ be the active
premise. Then:

w1 + w2 + hadj (η)− w1 − hadj (µ) = (hadj )
w2 +A(δM (q, v)) + C(rest(〈i, l′〉)) + w′1 + w′2
−A(q)− C(rest(〈i, l〉))− w′1 = (Ob. 2)
w2 +A(δM (q, v)) + w

′
2 −A(q)

− C(in(〈l, l′〉)) ≥ (Prop. 2)
w2 + C(inf (v)) + w′2
− C(in(〈l, l′〉)) ≥ 0. (Prop. 3)

It can be shown that PS is monotonic w.r.t. its pas-
sive premise item in a similar fashion.

6 Experiments

We repeated the experimental evaluation proposed
in (Waszczuk et al., 2016b) in order to compare
the new heuristic (cf. Sec. 5) with the baseline
(cf. Sec. 2). The experiment was based on the
version of the Składnica treebank (Świdziński and
Woliński, 2010) annotated with MWEs (Savary
and Waszczuk, 2017). For each sentence in the
corpus, the grammar was first reduced to only
those ETs whose terminals occurred in the sen-
tence, and compressed according to the methods
described in Sec. 3. Then, the parser (specified
in Sec. 4) was run and the search-space-size re-
ductions stemming from promoting MWEs (a be-
havior obtained by assigning the weight 1 to each
ET in the grammar) were measured. We also im-
plemented runtime verification tests which empir-
ically confirmed the monotonicity of (the imple-
mentation of) hadj .

We observed only minor differences between
the results obtained with both heuristics. On av-
erage (at most, respectively), hadj led to search-
space-size reductions of 16.8% (90.6%) vs. 18.1%
(90.7%) reductions obtained with h, a drop in per-
formance related to the lack of dead-end detection
in hadj . Recall, however, that calculating h’s val-
ues can be costly (linear w.r.t. the number of ETs)
when grammar compression is used, and can be
thus infeasible in practical parsing applications. In
our experiment, grammar compression alone de-
creased (on average) the search-space to less than
1
3 of its original size.

7 Conclusions and future work

Our previous state-of-the-art A? heuristic for pro-
moting MWEs in TAG parsing is not monotonic

91



and it composes poorly with grammar compres-
sion methods. We have presented here an en-
hanced version of this heuristic, which improves
the estimations of the outside derivation weights
by making the predictions of the weights related
to items’ gaps more accurate. The new version is
monotonic and combines with our grammar com-
pression techniques with no significant computa-
tional overhead. The price to pay is the lack of
detection of dead-end items, which partially ex-
plains the slight drop in search-space-size reduc-
tions in comparison with the baseline heuristic.
To the best of our knowledge, this is the first ap-
proach explicitly addressing interactions between
A? parsing and grammar compression.

For future work, we plan to repeat the experi-
ment with a truly weighted grammar, i.e., with the
weights corresponding to single-anchored ETs be-
ing estimated from a treebank. We believe that
such an experiment will lead to more insightful
conclusions as to the pros and cons of the new
heuristic. We also plan to optimize the imple-
mentation of the parser, in order to verify that
the search-space-size reductions transfer to pro-
portional reductions in parsing time.

Acknowledgments

This work has been supported by the French Min-
istry of Higher Education and Research via a doc-
toral grant, by the French Centre-Val de Loire Re-
gion Council via the APR-AI 2015-1850 ODIL
project, by the French National Research Agency
(ANR) via the PARSEME-FR6 project (ANR-14-
CERA-0001), and by the European Framework
Programme Horizon 2020 via the PARSEME7 Eu-
ropean COST Action (IC1207).

We are grateful to the anonymous reviewers for
their insightful comments to the first version of
this paper.

References
Anne Abeillé. 1995. The Flexibility of French Idioms:

A Representation with Lexicalised Tree Adjoining
Grammar. In M. Everaert, E-J. van der Linden,
A. Schenk, and R Schreuder, editors, Idioms: Struc-
tural and Psychological Perspectives, Lawrence Erl-
baum Associates, chapter 1.

Anne Abeillé and Yves Schabes. 1989. Pars-
ing idioms in lexicalized tags. In Proceedings

6http://parsemefr.lif.univ-mrs.fr
7http://www.parseme.eu

of the Fourth Conference on European Chap-
ter of the Association for Computational Linguis-
tics. Association for Computational Linguistics,
Stroudsburg, PA, USA, EACL ’89, pages 1–9.
https://doi.org/10.3115/976815.976816.

Miguel Alonso, David Cabrero, Eric Villemonte de la
Clergerie, and Manuel Vilares Ferro. 1999. Tabular
algorithms for TAG parsing. In EACL 1999. pages
150–157. http://acl.ldc.upenn.edu/E/E99/E99-
1020.pdf.

Krasimir Angelov and Peter Ljunglöf. 2014. Fast Sta-
tistical Parsing with Parallel Multiple Context-Free
Grammars. In EACL. volume 14, pages 368–376.

Srinivas Bangalore and Aravind K. Joshi. 1999.
Supertagging: An Approach to Almost
Parsing. Comput. Linguist. 25(2):237–265.
http://dl.acm.org/citation.cfm?id=973306.973310.

Matthieu Constant, Joseph Le Roux, and Anthony
Sigogne. 2013. Combining compound recog-
nition and PCFG-LA parsing with word lat-
tices and conditional random fields. ACM
Trans. Speech Lang. Process. 10(3):8:1–8:24.
https://doi.org/10.1145/2483969.2483970.

Claire Gardent, Yannick Parmentier, Guy Perrier, and
Sylvain Schmitz. 2014. Lexical Disambiguation
in LTAG using Left Context. In Zygmunt Vetu-
lani and Joseph Mariani, editors, Human Language
Technology. Challenges for Computer Science and
Linguistics. 5th Language and Technology Confer-
ence, LTC 2011, Poznan, Poland, November 25-27,
2011, Revised Selected Papers, Springer, volume
8387, pages 67–79. https://doi.org/10.1007/978-3-
319-08958-4_6.

Maurice Gross and Jean Senellart. 1998. Nouvelles
bases statistiques pour les mots du français. In Pro-
ceedings of JADT’98, Nice 1998. pages 335–349.

Liang Huang and David Chiang. 2005. Proceed-
ings of the Ninth International Workshop on Pars-
ing Technology, Association for Computational Lin-
guistics, chapter Better k-best Parsing, pages 53–64.
http://aclweb.org/anthology/W05-1506.

Dan Klein and Christopher D. Manning. 2002. A*
Parsing: Fast Exact Viterbi Parse Selection. Tech-
nical Report dbpubs/2002-16, Stanford University.

Dan Klein and Christopher D. Manning. 2003. A*
parsing: Fast exact viterbi parse selection. In
Proceedings of the 2003 Human Language Tech-
nology Conference of the North American Chapter
of the Association for Computational Linguistics.
http://www.aclweb.org/anthology/N03-1016.

Mike Lewis and Mark Steedman. 2014. A* CCG Pars-
ing with a Supertag-factored Model. In Proceed-
ings of the 2014 Conference on Empirical Methods
in Natural Language Processing (EMNLP). Asso-
ciation for Computational Linguistics, pages 990–
1000. http://aclweb.org/anthology/D14-1107.

92



Mark-Jan Nederhof. 2003. Weighted De-
ductive Parsing and Knuth’s Algo-
rithm. Comput. Linguist. 29(1):135–143.
https://doi.org/10.1162/089120103321337467.

Agata Savary and Jakub Waszczuk. 2017. Pro-
jecting multiword expression resources on a
polish treebank. In Proceedings of the 6th
Workshop on Balto-Slavic Natural Language
Processing. Association for Computational
Linguistics, Valencia, Spain, pages 20–26.
http://www.aclweb.org/anthology/W17-1404.

Stuart M Shieber, Yves Schabes, and Fernando CN
Pereira. 1995. Principles and implementation of de-
ductive parsing. The Journal of logic programming
24(1):3–36.

Jakub Waszczuk, Agata Savary, and Yannick Parmen-
tier. 2016a. Enhancing practical TAG parsing effi-
ciency by capturing redundancy. In 21st Interna-
tional Conference on Implementation and Applica-
tion of Automata (CIAA 2016). Séoul, South Korea,
Proceedings of the 21st International Conference
on Implementation and Application of Automata
(CIAA 2016). https://hal.archives-ouvertes.fr/hal-
01309598.

Jakub Waszczuk, Agata Savary, and Yannick Parmen-
tier. 2016b. Promoting multiword expressions in
a* tag parsing. In Proceedings of COLING 2016,
the 26th International Conference on Computational
Linguistics: Technical Papers. The COLING 2016
Organizing Committee, Osaka, Japan, pages 429–
439. http://aclweb.org/anthology/C16-1042.

Eric Wehrli. 2014. The relevance of colloca-
tions for parsing. In Proceedings of the
10th Workshop on Multiword Expressions
(MWE). Association for Computational Lin-
guistics, Gothenburg, Sweden, pages 26–32.
http://www.aclweb.org/anthology/W14-0804.

Marek Świdziński and Marcin Woliński. 2010. To-
wards a bank of constituent parse trees for Polish.
In Petr Sojka, Aleš Horák, Ivan Kopeček, and Karel
Pala, editors, Text, Speech and Dialogue: 13th In-
ternational Conference, TSD 2010, Brno, Czech Re-
public. Springer-Verlag, Heidelberg, volume 6231
of Lecture Notes in Artificial Intelligence, pages
197–204.

93


