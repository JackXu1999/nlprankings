



















































UNRAVELA Decipherment Toolkit


Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics
and the 7th International Joint Conference on Natural Language Processing (Short Papers), pages 549–553,

Beijing, China, July 26-31, 2015. c©2015 Association for Computational Linguistics

UNRAVEL—A Decipherment Toolkit

Malte Nuhn and Julian Schamper and Hermann Ney
Human Language Technology and Pattern Recognition

Computer Science Department, RWTH Aachen University, Aachen, Germany
<surname>@cs.rwth-aachen.de

Abstract

In this paper we present the UNRAVEL
toolkit: It implements many of the recently
published works on decipherment, includ-
ing decipherment for deterministic ciphers
like e.g. the ZODIAC-408 cipher and Part
two of the BEALE ciphers, as well as deci-
pherment of probabilistic ciphers and un-
supervised training for machine transla-
tion. It also includes data and example
configuration files so that the previously
published experiments are easy to repro-
duce.

1 Introduction

The idea of applying decipherment techniques to
the problem of machine translation has driven re-
search on decipherment in the recent time. Even
though the theoretical knowledge has been pub-
lished in the form of papers there has not been
any release of software until now. This made it
very difficult to follow upon the recent research
and to contribute new ideas. With this publica-
tion we want to share our implementation of two
important decipherment algorithms: Beam search
for deterministic substitution ciphers and beamed
EM training for probabilistic ciphers. It is clear
that the field of decipherment is still under heavy
research and that the true value of this release does
not lie in the current implementations themselves,
but rather in the opportunity for other researchers
to contribute their ideas to the field.

2 Overview

Enciphering a plaintext into a ciphertext can be
done using a myriad of encipherment methods.
Each of these methods needs its own customized
tools and tweaks in order to be deciphered auto-
matically. The goal of UNRAVEL is not to provide

a solver for every single encipherment method, but
rather to provide reusable tools that can be applied
to unsupervised learning for machine translation.

UNRAVEL contains two tools: DET-UNRAVEL
for decipherment of deterministic ciphers, and
EM-UNRAVEL for EM decipherment for proba-
bilistic substitution ciphers and simple machine
translation tasks. A comparison of both tools is
given in Table 1.

The code base is implemented in C++11 and
uses many publicly available libraries: The
GOOGLE-GLOG logging library is used for all log-
ging purposes, the GOOGLE-GFLAGS library is
used for providing command line flags, and the
GOOGLETEST library is used for unit testing and
consistency checks throughout the code base.

Classes for compressed I/O, access to
OpenFST (Allauzen et al., 2007), access to
KENLM(Heafield, 2011), representing mappings,
n-gram counts, vocabularies, lexicons, etc. are
shared across the code base.

For building we use the GNU build system. UN-
RAVEL can be compiled using GCC, ICC, and
CLANG on various Linux distributions and on
MacOS X. Scripts to download and compile nec-
essary libraries are also included: This makes it
easy to install UNRAVEL and its dependencies in
different computing environments.

Also, configuration- and data files (if possible
from a license point of view) for various experi-
ments (see Section 4.2 and Section 5.2) are dis-
tributed. Amongst others this includes setups for
the ZODIAC-408 and Part two of the BEALE ci-
phers (deterministic ciphers), as well as the OPUS
corpus and the VERBMOBIL corpus (probabilistic
cipher/machine translation).

3 Related Work

We list the most important publications that lead to
the implementation of UNRAVEL: Regarding DET-

549



UNRAVEL, the following literature is relevant:
Hart (1994) presents a tree search algorithm for
simple substitution ciphers with known word seg-
mentations. The idea of performing a tree search
and looking for mappings fulfilling consistency
constraints was later adopted to n-gram based de-
cipherment in an A* search approach presented
by Corlett and Penn (2010). DET-UNRAVEL im-
plements the beam search approach presented by
Nuhn et al. (2013) together with the refinements
presented in (Nuhn et al., 2014). The Bayesian
approach presented by Ravi and Knight (2011a) to
break the ZODIAC-408 cipher is not implemented,
but configuration and data to solve the ZODIAC-
408 cipher with DET-UNRAVEL is included. Also
it is worth noting that Hauer et al. (2014) provided
further work towards homophonic decipherment
that is not included in UNRAVEL.

The EM training for the decipherment of prob-
abilistic substitution ciphers, as first described by
Lee (2002) is implemented in EM-UNRAVEL to-
gether with various improvements and extensions:
The beam- and preselection search approxima-
tions presented by Nuhn and Ney (2014), the con-
text vector based candidate induction presented
by Nuhn et al. (2012), as well as training of the
simplified machine translation model presented by
Ravi and Knight (2011b).

4 Deterministic Ciphers: DET-UNRAVEL

Given an input sequence fN1 with tokens fn from
a vocabulary Vf and a language model of a tar-
get language p(eN1 ) with the target tokens from
a target vocabulary Ve, the task is to find a
mapping function φ : Vf → Ve so that the
language model probability of the decipherment
p(φ(f1)φ(f2) . . . φ(fN )) is maximized.

DET-UNRAVEL solves this optimization prob-

lem using the beam search approach presented by
Nuhn et al. (2013): The main idea is to structure
all partial φs into a search tree: If a cipher con-
tains |Vf | unique symbols, then the search tree is
of height |Vf |. At each level a decision about the
n-th symbol is made. The leaves of the tree form
full hypotheses. Instead of traversing the whole
search tree, beam search traverses the tree top to
bottom and only keeps the most promising candi-
dates at each level. Table 2 shows the important
parameters of the algorithm.

4.1 Implementation Details

During search, our implementation keeps track of
all partial hypotheses in two arraysHs andHt. We
use two different data structures for the hypothe-
ses in Hs and the hypotheses in Ht: Hs contains
the full information of the current partial mapping
φ. The candidates in the array Ht are generated
by augmenting hypotheses from the array Hs by
just one additional mapping decision f → e and
thus we use a different data structure for these hy-
potheses: They contain the current mapping deci-
sion f → e and a pointer to the parent node in
Hs. This saves memory in comparison to storing
the complete mapping at every point in time and
is faster than storing the mapping as a tree, which
would have to be traversed for every score estima-
tion.

The fact that only one additional decision is
made during the expansion process is also used
when calculating the scores for the new hypothe-
sis: Only the additional terms of the final score for
the current partial hypothesis φ are added to the
predecessor score (i.e. the scheme is scorenew =
scoreold + δ, where scoreold is independent of the
current decision f → e).

The now scored hypotheses in Ht (our imple-
mentation also includes the improved rest cost es-

Aspect Deterministic Ciphers: DET-UNRAVEL Probabilistic Ciphers: EM-UNRAVEL

Search Space Mappings φ Substitution tables {p(f |e)}
Training Beam search over all φ. The order in

which the decisions for φ(f) for each f
are made is based on the extension order.

EM-training: In the E-step use beam
search to obtain the most probable deci-
pherments eI1 for a given ciphertext se-
quence fJ1 . Update {p(f |e)} in M-step.

Decoding Apply φ to cipher text. Viterbi decoding using final {p(f |e)}.
Experiments ZODIAC-408, pt. two of BEALE ciphers OPUS, VERBMOBIL

Table 1: Comparison of DET-UNRAVEL and EM-UNRAVEL.

550



timation as described in (Nuhn et al., 2014)) are
pruned using different pruning strategies: Thresh-
old pruning—given the best hypothesis, add a
threshold score and prune the hypotheses with
scores lower than best hypothesis plus this thresh-
old score—and histogram pruning—which only
keeps the best Bhisto hypothesis at every level of
the search tree. Further, the surviving hypotheses
are checked whether they fulfill certain constraints
C(φ) like e.g. enforcing 1-to-1 mappings during
search.

Those hypotheses in Ht that survived the prun-
ing step and the constraints check are converted to
full hypotheses so that they can be stored in Hs.
Then, the search continues with the next cardinal-
ity.

The order in which decisions about the symbols
f ∈ Vf are made during search (called extension
order) can be computed using different strategies:
We implement a simple frequency sorting heuris-
tic, as well as a more advanced strategy that uses
beam search to find an improved enumeration of
f ∈ Vf , as presented in (Nuhn et al., 2014).

Our implementation expands the partial hy-
potheses in Hs in parallel: The implementation
has been tested with up to 128 threads (on a 128
core machine) with parallelization overhead of
less than 20%.

4.2 Experiments

The configurations for decoding the ZODIAC-408
cipher as well as Part two of the BEALE ciphers are
almost identical: For both setups we use an 8-gram
character language model trained on a subset of
the English Gigaword corpus (Parker et al., 2011).
We obtain n-gram counts (order 2 to 8) from the
input ciphers and pass these to DET-UNRAVEL. In
both cases we use the improved heuristic together
with the improved extension order as presented in
(Nuhn et al., 2014).

For the ZODIAC-408, using a beam sizeBhist =
26 yields 52 out of 54 correct mappings. For the
Part two of the BEALE ciphers a much larger beam
size of Bhist = 10M yields 157 correct mappings
out of 185, resulting in an error rate on the string
of 762 symbols is 5.4 %.

5 Probabilistic Ciphers: EM-UNRAVEL

For probabilistic ciphers, the goal is to find a prob-
abilistic substitution table {p(f |e)} with normal-
ization constraint ∀e

∑
f p(f |e) = 1. Learning

this table is done iteratively using the EM algo-
rithm (Dempster et al., 1977).

Each iteration consists of two steps: Hypoth-
esis generation (E-Step) and retraining the table
{p(f |e)} using the posterior probability pj(e|fJ1 )
that any translation eI1 of f

J
1 has the word e aligned

to the source word fj (M-Step).
From a higher level view, EM-UNRAVEL can be

seen as a specialized word based MT decoder that
can efficiently generate and organize all possible
translations in the E-step, and efficiently retrain
the model {p(f |e)} on all these hypotheses in the
M-step.

5.1 Implementation Details

In contrast to DET-UNRAVEL, EM-UNRAVEL pro-
cesses the input corpus sentence by sentence. For
each sentence, we build hypotheses eI1 from left to
right, one word at a time:

First, the empty hypothesis is added to a set
of currently active partial hypotheses. Then, for
each partial hypothesis, a new source word is cho-
sen such that local reordering constraints are ful-
filled. For this, a coverage vector (which encodes
the words that have already been translated) has
to be updated for each hypothesis. Once the cur-
rent source word to be translated next has been
chosen, hypotheses for all possible translations of
this source word are generated and scored. Af-
ter having processed the entire set of partial hy-
potheses, the set of newly generated hypotheses is

Name Description

Pruning
Bhist Histogram pruning. Only the best Bhist

hypotheses are kept.
Bthres Threshold pruning. Hypotheses with

scores S worse than Sbest+Bthres, where
Sbest is the score of the best hyptohesis,
are pruned.

Constraints
C(φ) Substitution constraint. Hypotheses not

fulfilling the constraintC(φ) are discarded
from search.

Extension Order
Vext Extension order. Enumeration of the vo-

cabulary Vf in which the search tree over
all φ is visited.

Bexthist Histogram Pruning for extension order
search.

W extn Weight for n−gram language model
lookahead score.

Table 2: Important parameters of DET-UNRAVEL.

551



pruned: Here, the partial hypotheses are organized
and pruned with respect to their cardinality. For
each cardinality, we keep the Bhisto best scoring
hypotheses.

Similarly to DET-UNRAVEL, the previously de-
scribed expansion and pruning step is imple-
mented using two arrays Hs and Ht. However,
in EM-UNRAVEL the partial hypotheses in Hs and
Ht use the same data structures since—in contrast
to DET-UNRAVEL—recombination of hypotheses
is possible.

In the case of large vocabularies it is not feasi-
ble to keep track of all possible substitutions for a
given source word. This step can also be approx-
imated using the preselection technique by Nuhn
and Ney (2014): Instead of adding hypotheses for
all possible target words, only a small subset of
possible successor hypotheses is generated: These
are based on the current source word that is to be
translated, as well as the current language model
state.

Once the search is completed we compute pos-
teriors on the resulting word graph and accumu-
late those across all sentences in the corpus. Hav-
ing finished one pass over the corpus, the accumu-

Name Description

Pruning
Bhist Histogram pruning. Only the best Bhist

hypotheses are kept.

Preselection Search
Blexcand Lexical candidates. Try only the best

Blexcand substitutions e for each word f
based on p(f |e)

BLMcand LM candidates. Try only the best B
LM
hist

successor words e with respect to the pre-
vious hypothesis’ LM state.

Translation Model
Wjump Jump width. Maximum jump size allowed

in local reordering.
Cjump Jump cost. Cost for non-monotonic tran-

sitions.
Cins Insertion cost. Cost for insertions of

words.
Mins Maximum number of insertions per sen-

tence.
Cdel Deletion cost. Cost for deletions of words.
Mdel Maximum number of of deletions per sen-

tence.

Other
λlex Lexical smoothing parameter.
Nctx Number of candidate translations allowed

in lexicon generation in context vector
step.

Table 3: Important parameters of EM-UNRAVEL.

lated posteriors are used to re-estimate {p(e|f)}
and the next iteration of the EM algorithm begins.
Also, with every new parameter table {p(e|f)},
the Viterbi decoding of the source corpus is com-
puted.

While full EM training is feasible and gives
good results for the OPUS corpus, Nuhn et al.
(2012) suggest to include a context vector step in
between EM iterations for large vocabulary tasks.

Using the Viterbi decoding of the source se-
quence from the last E-step and the corpus used
to train the LM, we create normalized context vec-
tors for each word e and f . The idea is that vec-
tors for words e and f that are translations of each
other are similar. For each word f ∈ Vf , a set of
candidates e ∈ Ve can be computed. These candi-
dates are used to initialize a new lexicon, which is
further refined using standard EM iterations after-
wards.

Both, EM training and the context vector step
are implemented in a parallel fashion (running in
a single process). Parallelization is done on a sen-
tence level: We successfully used our implemen-
tation with up to 128 cores.

5.2 Experiments

We briefly mention experiments on two corpora:
The OPUS corpus and the VERBMOBIL corpus.

The OPUS corpus is a subtitle corpus of roughly
100k running words. Here the vocabulary size
of the source language (Spanish) is 562 and the
target language (English) contains 411 unique
words. Using a 3-gram language model UNRAVEL
achieves 19.5 % BLEU on this task.

The VERBMOBIL corpus contains roughly 600k
running words. The target language vocabulary
size is 3, 723 (English) and the source language
vocabulary size is 5, 964 (German). Using a 3-
gram language model and the context vector ap-
proach, UNRAVEL achieves 15.5 % BLEU.

6 Download and License

UNRAVEL can be downloaded at
www.hltpr.rwth-aachen.de/unravel.
UNRAVEL is distributed under a custom open
source license. This includes free usage for
noncommercial purposes as long as any changes
made to the original software are published
under the terms of the same license. The exact
formulation is available at the download page for
UNRAVEL.

552



We have chosen to keep this paper independent
of actual implementation details such as method-
and parameter names. Please consult the README
files and comments in UNRAVEL’s source code for
implementation details.

7 Conclusion

UNRAVEL is a flexible and efficient decipherment
toolkit that is freely available to the scientific com-
munity. It implements algorithms for solving de-
terministic and probabilistic substitution ciphers.

We hope that this release sparks more interest-
ing research on decipherment and its applications
to machine translation.

References
[Allauzen et al.2007] Cyril Allauzen, Michael Riley,

Johan Schalkwyk, Wojciech Skut, and Mehryar
Mohri. 2007. Openfst: A general and efficient
weighted finite-state transducer library. In Jan
Holub and Jan Zdárek, editors, CIAA, volume 4783
of Lecture Notes in Computer Science, pages 11–23.
Springer.

[Corlett and Penn2010] Eric Corlett and Gerald Penn.
2010. An exact A* method for deciphering letter-
substitution ciphers. In Proceedings of the 48th An-
nual Meeting of the Association for Computational
Linguistics (ACL), pages 1040–1047, Uppsala, Swe-
den, July. The Association for Computer Linguis-
tics.

[Dempster et al.1977] Arthur P. Dempster, Nan M.
Laird, and Donald B. Rubin. 1977. Maximum like-
lihood from incomplete data via the EM algorithm.
Journal of the Royal Statistical Society, B, 39.

[Hart1994] George W Hart. 1994. To decode
short cryptograms. Communications of the ACM,
37(9):102–108.

[Hauer et al.2014] Bradley Hauer, Ryan Hayward, and
Grzegorz Kondrak. 2014. Solving substitution ci-
phers with combined language models. In Pro-
ceedings of COLING 2014, the 25th International
Conference on Computational Linguistics: Techni-
cal Papers, pages 2314–2325. Dublin City Univer-
sity and Association for Computational Linguistics.

[Heafield2011] Kenneth Heafield. 2011. KenLM:
Faster and Smaller Language Model Queries. In
Proceedings of the Sixth Workshop on Statistical
Machine Translation, pages 187–197, Edinburgh,
Scotland, July. Association for Computational Lin-
guistics.

[Lee2002] Dar-Shyang Lee. 2002. Substitution deci-
phering based on hmms with applications to com-
pressed document processing. Pattern Analysis

and Machine Intelligence, IEEE Transactions on,
24(12):1661–1666.

[Nuhn and Ney2014] Malte Nuhn and Hermann Ney.
2014. Em decipherment for large vocabularies. In
Annual Meeting of the Assoc. for Computational
Linguistics, pages 759–764, Baltimore, MD, USA,
June.

[Nuhn et al.2012] Malte Nuhn, Arne Mauser, and Her-
mann Ney. 2012. Deciphering foreign language by
combining language models and context vectors. In
Proceedings of the 50th Annual Meeting of the Asso-
ciation for Computational Linguistics (ACL), pages
156–164, Jeju, Republic of Korea, July. Association
for Computational Linguistics.

[Nuhn et al.2013] Malte Nuhn, Julian Schamper, and
Hermann Ney. 2013. Beam search for solving sub-
stitution ciphers. In Annual Meeting of the Assoc.
for Computational Linguistics, pages 1569–1576,
Sofia, Bulgaria, August.

[Nuhn et al.2014] Malte Nuhn, Julian Schamper, and
Hermann Ney. 2014. Improved decipherment of
homophonic ciphers. In Conference on Empirical
Methods in Natural Language Processing, Doha,
Qatar, October.

[Parker et al.2011] Robert Parker, David Graff, Junbo
Kong, Ke Chen, and Kazuaki Maeda. 2011. English
gigaword fifth edition. Linguistic Data Consortium,
Philadelphia.

[Ravi and Knight2011a] Sujith Ravi and Kevin Knight.
2011a. Bayesian inference for Zodiac and other ho-
mophonic ciphers. In Proceedings of the 49th An-
nual Meeting of the Association for Computational
Linguistics (ACL), pages 239–247, Portland, Ore-
gon, June. Association for Computational Linguis-
tics.

[Ravi and Knight2011b] Sujith Ravi and Kevin Knight.
2011b. Deciphering foreign language. In Proceed-
ings of the 49th Annual Meeting of the Association
for Computational Linguistics: Human Language
Technologies, pages 12–21, Portland, Oregon, USA,
June. Association for Computational Linguistics.

553


