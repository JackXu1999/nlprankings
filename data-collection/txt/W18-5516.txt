



















































Multi-Sentence Textual Entailment for Claim Verification


Proceedings of the First Workshop on Fact Extraction and VERification (FEVER), pages 103–108
Brussels, Belgium, November 1, 2018. c©2018 Association for Computational Linguistics

103

UKP-Athene: Multi-Sentence Textual Entailment for Claim Verification
Andreas Hanselowski†∗, Hao Zhang∗, Zile Li∗, Daniil Sorokin†∗,

Benjamin Schiller∗, Claudia Schulz†∗, Iryna Gurevych†∗

†Research Training Group AIPHES
Computer Science Department, Technische Universität Darmstadt

https://www.aiphes.tu-darmstadt.de
∗Ubiquitous Knowledge Processing Lab (UKP-TUDA)

Computer Science Department, Technische Universität Darmstadt
https://www.ukp.tu-darmstadt.de/

Abstract

The Fact Extraction and VERification
(FEVER) shared task was launched to sup-
port the development of systems able to
verify claims by extracting supporting or
refuting facts from raw text. The shared
task organizers provide a large-scale dataset
for the consecutive steps involved in claim
verification, in particular, document retrieval,
fact extraction, and claim classification. In
this paper, we present our claim verification
pipeline approach, which, according to the
preliminary results, scored third in the shared
task, out of 23 competing systems. For the
document retrieval, we implemented a new
entity linking approach. In order to be able
to rank candidate facts and classify a claim
on the basis of several selected facts, we
introduce two extensions to the Enhanced
LSTM (ESIM).

1 Introduction

In the past years, the amount of false or misleading
content on the Internet has significantly increased.
As a result, information evaluation in terms of
fact-checking has become increasingly important
as it allows to verify controversial claims stated
on the web. However, due to the large number of
fake news and hyperpartisan articles published on-
line every day, manual fact-checking is no longer
feasible. Thus, researchers as well as corporations
are exploring different techniques to automate the
fact-checking process1.

In order to advance research in this direction,
the Fact Extraction and VERification (FEVER)
shared task2 was launched. The organizers of

1https://fullfact.org/media/uploads/
full_fact-the_state_of_automated_
factchecking_aug_2016.pdf

2http://fever.ai/task.html

the FEVER shared task constructed a large-scale
dataset (Thorne et al., 2018) based on Wikipedia.
This dataset contains 185,445 claims, each of
which comes with several evidence sets. An ev-
idence set consists of facts, i.e. sentences from
Wikipedia articles that jointly support or contra-
dict the claim. On the basis of (any one of) its
evidence sets, each claim is labeled as Supported,
Refuted, or NotEnoughInfo if no decision about
the veracity of the claim can be made. Supported
by the structure of the dataset, the FEVER shared
task encompasses three sub-tasks that need to be
solved.
Document retrieval: Given a claim, find (En-
glish) Wikipedia articles containing information
about this claim.
Sentence selection: From the retrieved articles,
extract facts in the form of sentences that are rele-
vant for the verification of the claim.
Recognizing textual entailment: On the basis of
the collected sentences (facts), predict one of three
labels for the claim: Supported, Refuted, or NotE-
noughInfo. To evaluate the performance of the
competing systems, an evaluation metric was de-
vised by the FEVER organizers: a claim is con-
sidered as correctly verified if, in addition to pre-
dicting the correct label, a correct evidence set was
retrieved.
In this paper, we describe the pipeline system
that we developed to address the FEVER task.
For document retrieval, we implemented an en-
tity linking approach based on constituency pars-
ing and handcrafted rules. For sentence selection,
we developed a sentence ranking model based on
the Enhanced Sequential Inference Model (ESIM)
(Chen et al., 2016). We furthermore extended the
ESIM for recognizing textual entailment between
multiple input sentences and the claim using an at-
tention mechanism.
According to the preliminary results of the

https://www.aiphes.tu-darmstadt.de
https://www.ukp.tu-darmstadt.de/
https://fullfact.org/media/uploads/full_fact-the_state_of_automated_factchecking_aug_2016.pdf
https://fullfact.org/media/uploads/full_fact-the_state_of_automated_factchecking_aug_2016.pdf
https://fullfact.org/media/uploads/full_fact-the_state_of_automated_factchecking_aug_2016.pdf
http://fever.ai/task.html


104

FEVER shared task, our systems came third out
of 23 competing teams.

2 Background

In this section, we present underlying methods that
we adopted for the development of our system.

2.1 Entity linking

The document retrieval step requires matching a
given claim with the content of a Wikipedia arti-
cle. A claim frequently features one or multiple
entities that form the main content of the claim.

Furthermore, Wikipedia can be viewed as a
knowledge base, where each article describes a
particular entity, denoted by the article title. Thus,
the document retrieval step can be framed as an
entity linking problem (Cucerzan, 2007). That is,
identifying entity mentions in the claim and link-
ing them to the Wikipedia articles of this entity.
The linked Wikipedia articles can then be used as
the set of the retrieved documents for the subse-
quent steps.

2.2 Enhanced Sequential Inference Model

Originally developed for the SNLI task (Bow-
man et al., 2015) of determining entailment be-
tween two statements, the ESIM (Enhanced Se-
quential Inference Model) (Chen et al., 2016) cre-
ates a rich representation of statement-pairs. Since
the FEVER task requires the handling of claim-
sentence pairs, we use the ESIM as the basis
for both sentence selection and textual entailment.
The ESIM solves the entailment problem in three
consecutive steps, taking two statements as input.
Input encoding: Using a bidirectional LSTM
(BiLSTM) (Graves and Schmidhuber, 2005), rep-
resentations of the individual tokens of the two in-
put statements are computed.
Local inference modeling: Each token of one
statement is used to compute attention weights
with respect to each token in the other statement,
giving rise to an attention weight matrix. Then,
each token representation is multiplied by all of
its attention weights and weighted pooling is ap-
plied to compute a single representation for each
token. This operation gives rise to two new repre-
sentations of the two statements.
Inference composition: These two statement
representations are then fed into two BiLSTMs,
which again compute sequences of representations
for each statement. Maximum and average pool-

ing is applied to the two sequences to derive two
representations, which are then concatenated (last
hidden state of the ESIM) and fed into a multilayer
perceptron for the classification of the entailment
relation.

3 Our system for fact extraction and
claim verification

In this section, we describe the models that we de-
veloped for the three FEVER sub-tasks.

3.1 Document retrieval

As explained in Section 2.1, we propose an entity
linking approach to the document retrieval sub-
task. That is, we find entities in the claims that
match the titles of Wikipedia articles (documents).
Following the typical entity linking pipeline, we
develop a document retrieval component that has
three main steps.
Mention extraction: Named entity recognition
tools focus only on the main types of entities (Lo-
cation, Organization, Person). In order to find en-
tities of different categories, such as movie titles,
that are numerous in the shared task data set, we
employ the constituency parser from AllenNLP
(Gardner et al., 2017). After parsing the claim, we
consider every noun phrase as a potential entity
mention. However, a movie or a song title may be
an adjective or any other type of syntactic phrase.
To account for such cases, we use a heuristic that
adds all words in the claim before the main verb
as well as the whole claim itself as potential en-
tity mentions. For example, a claim “Down With
Love is a 2003 comedy film.” contains the noun
phrases ‘a 2003 comedy film’ and ‘Love’. Neither
of the noun phrases constitutes an entity mention,
but the tokens before the main verb, ‘Down With
Love’, form an entity.
Candidate article search: We use the MediaWiki
API3 to search through the titles of all Wikipedia
articles for matches with the potential entity men-
tions found in the claim. The MediaWiki API uses
the Wikipedia search engine to find matching ar-
ticles. The top match is the article whose title
has the largest overlap with the query. For each
entity mention, we store the seven highest-ranked
Wikipedia article matches.

The MediaWiki API uses the online version of
Wikipedia and since there are some discrepancies

3https://www.mediawiki.org/wiki/API:
Main_page

https://www.mediawiki.org/wiki/API:Main_page
https://www.mediawiki.org/wiki/API:Main_page


105

claim evidence
sentences

claim sampled
sentences

Input Encoding

Local Inference

Composition

Input Encoding

Local Inference

Composition

sp
sn

∑
max(0, 1 + sn − sp)

Input

ESIM

tanh +
Dropout

Ranking
score

Hinge Loss

Figure 1: Sentence selection model

between the 2017 dump used in the shared task
and the latest version, we also perform an exact
search over all Wikipedia article titles in the dump.
We add these results to the set of the retrieved ar-
ticles.
Candidate filtering: The MediaWiki API re-
trieves articles whose title overlaps with the query.
Thus, the results may contain articles with a ti-
tle longer or shorter than the entity mention used
in the query. Similarly to previous work on en-
tity linking (Sorokin and Gurevych, 2018), we re-
move results that are longer than the entity men-
tion and do not overlap with the rest of the claim.
To check this overlap, we first remove the con-
tent in parentheses from the Wikipedia article ti-
tles and stem the remaining words in the titles and
the claim. Then, we discard a Wikipedia article if
its stemmed article title is not completely included
in the stemmed claim.

We collect all retrieved Wikipedia articles for
all identified entity mentions in the claim after
filtering and supply them to the next step in the
pipeline. The evaluation of the document retrieval
system on the development data shows the effec-
tiveness of our ad-hoc entity linking approach (see
Section 4).

3.2 Sentence selection
In this step, we select candidate sentences as a po-
tential evidence set for a claim from the Wikipedia
articles retrieved in the previous step. This is
achieved by extending the ESIM to generate a
ranking score on the basis of two input statements,
instead of predicting the entailment relation be-
tween these two statements.
Architecture: The modified ESIM takes as input
a claim and a sentence. To generate the ranking
score, the last hidden state of the ESIM (see Sec-
tion 2.2) is fed into a hidden layer which is con-

nected to a single neuron for the prediction of the
ranking score. As a result, we are able to rank all
sentences of the retrieved documents according to
the computed ranking scores. In order to find a
potential evidence set, we select the five highest-
ranked sentences.
Training: Our adaptation of the ESIM is illus-
trated in Fig. 1. In the training mode, the ESIM
takes as input a claim and the concatenated sen-
tences of an evidence set. As a loss function, we
use a modified hinge loss with negative sampling:∑

max(0, 1 + sn − sp), where sp indicates the
positive ranking score and sn the negative rank-
ing score for a given claim-sentence pair. To get
sp, we feed the network a claim and the concate-
nated sentences of one of its ground truth evidence
sets. To get sn, we take all Wikipedia articles
from which the ground truth evidence sets of the
claim originate, randomly sample five sentences
(not including the sentences of the ground truth ev-
idence sets for the claim), and feed the concatena-
tion of these sentences into the same ESIM. With
our modified hinge loss function, we then try to
maximize the margin between positive and nega-
tive samples.
Testing: At testing time, we calculate the score
between a claim and each sentence in the retrieved
documents. For this purpose, we deploy an en-
semble of ten models with different random seeds.
Then, the mean score of a claim-sentence pair over
all ten models of the ensemble is calculated and
the scores for all pairs are ranked. Finally, the five
sentences of the five highest-ranked pairs are taken
as an output of the model.

3.3 Recognizing textual entailment

In order to classify the claim as Supported,
Refuted or NotEnoughInfo, we use the five
sentences retrieved by our sentence selection
model described in the previous section. For the
classification, we propose another extension to the
ESIM, which can predict the entailment relation
between multiple input sentences and the claim.
Fig. 2 gives an overview of our extended ESIM
for the FEVER textual entailment task.

As word representation for both claim and sen-
tences, we concatenate the Glove (Pennington
et al., 2014) and FastText (Bojanowski et al., 2016)
embeddings for each word. Since both types of
embeddings are pretrained on Wikipedia, they are
particularly suitable for our problem setting.



106

S1

Input Encoding

Local Inference

Composition

·

S5

Input Encoding

Local Inference

Composition

·

S2 S3 S4

. . .

. . .

. . .

claim C

⊕
max avg

Input

ESIM

Output
of ESIM

Attended
Vectors

Pooling

Attention

Classifier

Figure 2: Extended ESIM for recognizing textual en-
tailment

To process the five input sentences using the
ESIM, we combine the claim with each sentence
and feed the resulting pairs into the ESIM. The last
hidden states of the five individual claim-sentence
runs of the ESIM are compressed into one vector
using attention and pooling operations.
The attention is based on a representation of the
claim that is independent of the five sentences.
This representation is obtained by summing up the
input encodings of the claim in the five ESIM runs.
In the same way, we derive five sentence repre-
sentations, one from each of the five runs of the
ESIM, which are independent of the claim. For
each claim-sentence pair, the single sentence rep-
resentation and the claim representation are then
individually fed through a single layer perceptron.
The cosine similarity of these two vectors is then
used as an attention weight. The five output vec-
tors of all ESIMs are multiplied with their respec-
tive attention weights and we apply average and
max pooling on these vectors in order to reduce
them to two representations. Finally, the two rep-
resentations are concatenated and fed through a 3-
layer perceptron to predict one of the three classes
Supported, Refuted or NotEnoughInfo. The idea
behind the described attention mechanism is to al-
low the model to extract information from the five
sentences that is most relevant for the classifica-
tion of the claim.

4 Results

Table 1 shows the performance of our document
retrieval and sentence selection system when re-
trieving different numbers of the highest-ranked
Wikipedia articles. In contrast to the results re-
ported in Table 3, here we consider a single model
instead of an ensemble. The results show that both
systems benefit from a larger number of retrieved
articles.

#search results doc. accuracy sent. recall

3 92.60 85.37
5 93.30 86.02
7 93.55 86.24

Table 1: Performance of the retrieval systems using dif-
ferent numbers of MediaWiki search results

For the subtask of recognizing textual entail-
ment, we also experiment with different numbers
of selected sentences. The results in Table 2
demonstrate that our model performs best with all
five selected sentences.

#sentence(s) label accuracy FEVER score

1 67.94 63.64
2 68.33 64.30
3 67.82 63.72
4 67.61 63.59
5 68.49 64.74

Table 2: Performance of the textual entailment model
using different numbers of sentences

In Table 3, we compare the performance of our
three systems as well as the full pipeline to the
baseline systems and pipeline implemented by the
shared task organizers (Thorne et al., 2018) on the
development set. As the results demonstrate, we
were able to significantly improve upon the base-
line on each sub-task. The performance gains over
the whole pipeline add up to an improvement of
about 100% with respect to the baseline pipeline.

5 Error analysis

In this section, we present the error analysis for
each of the three sub-tasks, which can serve as a
basis for further improvements of the system.

5.1 Document retrieval
The typical errors encountered for the document
retrieval system can be divided into three classes.



107

Task (metric) system score (%)

Document retrieval baseline 70.20
(accuracy) our system 93.55

Sentence selection baseline 44.22
(recall) our system 87.10

Textual entailment baseline 52.09
(label accuracy) our system 68.49
Full pipeline baseline 32.27
(FEVER score) our system 64.74

Table 3: Performance comparison of our system and
the baseline system on the development set

Spelling errors: A word in the claim or in the
article title is misspelled. E.g. “Homer Hickman
wrote some historical fiction novels.” vs. “Homer
Hickam”. In this case, our document retrieval sys-
tem discards the article during the filtering phase.
Missing entity mentions: The entity mention rep-
resented by the title of the article, which needs to
be retrieved, is not related to any entity mention
in the claim. E.g. Article title: “Blue Jasmine”
Claim: “Cate Blanchett ignored the offer to act in
Cate Blanchett.”.
Search failures: Some article titles contain a cat-
egory name in parentheses for the disambiguation
of the entity. This makes it difficult to retrieve the
exact article title using the MediaWiki API. E.g.
the claim “Alex Jones is apolitical.” requires the
article “Alex Jones (radio host)”, but it is not con-
tained in the MediaWiki search results.

5.2 Sentence selection

The most frequent case, in which the sentence se-
lection model fails to retrieve a correct evidence
set, is that the entity mention in the claim does not
occur in the annotated evidence set. E.g. the only
evidence set for the claim “Daggering is nontradi-
tional.” consists of the single sentence “This dance
is not a traditional dance.”. Here, “this dance”
refers to “daggering” and cannot be resolved by
our model, since the information that “daggering”
is a dance is not mentioned in the evidence sen-
tence or in the claim. Some evidence sets contain
two sentences one of which is less related to the
claim. E.g. the claim “Herry II of France has
three cars.” has an evidence set that contains the
two sentences “Henry II died in 1559.” and “1886
is regarded as the birth year of the modern car.”.
The second sentence is not directly related to the

claim, thus, it is ranked very low by our model.

5.3 Recognizing textual entailment

A large number of claims are misclassified due to
the model’s disability to interpret numerical val-
ues. For instance, the claim “The heart beats at a
resting rate close to 22 beats per minute.” is not
classified as refuted on the basis of the evidence
sentence “The heart beats at a resting rate close
to 72 beats per minute.”. The only information re-
futing the claim is the number, but neither GloVe
nor FastText embeddings can embed numbers dis-
tinctly enough. Another problem are challeng-
ing NotEnoughInfo cases. For instance, the claim
“Terry Crews played on the Los Angeles Charg-
ers.” (annotated as NotEnoughInfo) is classified
as refuted, given the sentence “In football, Crews
played ... for the Los Angeles Rams, San Diego
Chargers and Washington Redskins, ...”. The sen-
tence is related to the claim but does not exclude
it, which makes this case difficult.

6 Conclusion

In this paper, we presented the system for fact ex-
traction and verification, which we developed in
the course of the FEVER shared task. According
to the preliminary results, our system scored third
out of 23 competing teams. The shared task was
divided into three parts: (i) Given a claim, retrieve
Wikipedia documents that contain facts about the
claim. (ii) Extract these facts from the document.
(iii) Verify the claim on the basis of the extracted
facts. To address the problem, we developed mod-
els for the three sub-tasks. We framed document
retrieval as entity linking by identifying entities in
the claim and linking them to Wikipedia articles.
To extract facts in the articles, we developed a sen-
tence ranking model by extending the ESIM. For
claim verification we proposed another extension
to the ESIM, whereby we were able to classify the
claim on the basis of multiple facts using attention.
Each of our three models, as well as the combined
pipeline, significantly outperforms the baseline on
the development set.

7 Acknowledgements

This work has been supported by the German Re-
search Foundation as part of the Research Training
Group Adaptive Preparation of Information from
Heterogeneous Sources (AIPHES) at the Technis-
che Universität Darmstadt grant No. GRK 1994/1.



108

References
Piotr Bojanowski, Edouard Grave, Armand Joulin,

and Tomas Mikolov. 2016. Enriching word vec-
tors with subword information. arXiv preprint
arXiv:1607.04606.

Samuel R Bowman, Gabor Angeli, Christopher Potts,
and Christopher D Manning. 2015. A large anno-
tated corpus for learning natural language inference.
arXiv preprint arXiv:1508.05326.

Qian Chen, Xiaodan Zhu, Zhenhua Ling, Si Wei, Hui
Jiang, and Diana Inkpen. 2016. Enhanced LSTM
for natural language inference. arXiv preprint
arXiv:1609.06038.

Silviu Cucerzan. 2007. Large-Scale Named Entity Dis-
ambiguation Based on Wikipedia Data. In Pro-
ceedings of the 2007 Joint Conference on Empirical
Methods in Natural Language Processing and Com-
putational Natural Language Learning (EMNLP-
CoNLL), pages 708–716, Prague, Czech Republic.

Matt Gardner, Joel Grus, Mark Neumann, Oyvind
Tafjord, Pradeep Dasigi, Nelson F. Liu, Matthew
Peters, Michael Schmitz, and Luke S. Zettlemoyer.
2017. AllenNLP: A deep semantic natural language
processing platform.

Alex Graves and Jürgen Schmidhuber. 2005. Frame-
wise phoneme classification with bidirectional
LSTM and other neural network architectures. Neu-
ral Networks, 18(5-6):602–610.

Jeffrey Pennington, Richard Socher, and Christopher
Manning. 2014. Glove: Global vectors for word
representation. In Proceedings of the 2014 confer-
ence on empirical methods in natural language pro-
cessing (EMNLP), pages 1532–1543.

Daniil Sorokin and Iryna Gurevych. 2018. Mixing
Context Granularities for Improved Entity Linking
on Question Answering Data across Entity Cate-
gories. In Proceedings of the 7th Joint Conference
on Lexical and Computational Semantics (*SEM
2018), pages 65–75. Association for Computational
Linguistics.

James Thorne, Andreas Vlachos, Christos
Christodoulopoulos, and Arpit Mittal. 2018.
FEVER: A large-scale dataset for fact extraction
and verification. In NAACL-HLT.


