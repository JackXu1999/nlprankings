



















































Domain Adaptive Dialog Generation via Meta Learning


Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 2639–2649
Florence, Italy, July 28 - August 2, 2019. c©2019 Association for Computational Linguistics

2639

Domain Adaptive Dialog Generation via Meta Learning

Kun Qian
Univeristy of California, Davis
kunqian@ucdavis.edu

Zhou Yu
Univeristy of California, Davis

joyu@ucdavis.edu

Abstract

Domain adaptation is an essential task in dia-
log system building because there are so many
new dialog tasks created for different needs
every day. Collecting and annotating train-
ing data for these new tasks is costly since it
involves real user interactions. We propose
a domain adaptive dialog generation method
based on meta-learning (DAML). DAML is
an end-to-end trainable dialog system model
that learns from multiple rich-resource tasks
and then adapts to new domains with mini-
mal training samples. We train a dialog sys-
tem model using multiple rich-resource single-
domain dialog data by applying the model-
agnostic meta-learning algorithm to dialog do-
main. The model is capable of learning a
competitive dialog system on a new domain
with only a few training examples in an effi-
cient manner. The two-step gradient updates
in DAML enable the model to learn general
features across multiple tasks. We evaluate
our method on a simulated dialog dataset and
achieve state-of-the-art performance, which is
generalizable to new tasks.

1 Introduction

Modern personal assistants, such as Alexa and
Siri, are composed of thousands of single-domain
task-oriented dialog systems. Every dialog task
is different, due to the specific domain knowl-
edge. An end-to-end trainable dialog system re-
quires thousands of dialogs for training. How-
ever, the availability of the training data is usu-
ally limited as real users have to be involved to
obtain the training dialogs. Therefore, adapting
existing rich-resource data to new domains with
limited resource is an essential task in dialog sys-
tem research. Transfer learning (Caruana, 1997a;
Bengio, 2012; Cohn et al., 1994; Mo et al., 2018),
few-shot learning (Salakhutdinov et al., 2012; Li
et al., 2006; Norouzi et al., 2013; Socher et al.,

2013) and meta-learning (Finn et al., 2017) are
introduced in solving such data scarcity problem
in machine learning. Because every dialog do-
main is very different from each other, general-
ize information from rich-resource domains to an-
other low resource domain is difficult. There-
fore, only a few studies have tackled domain adap-
tive end-to-end dialog training methods (Zhao and
Eskénazi, 2018). We propose DAML based on
meta-learning to combine multiple dialog tasks in
training, in order to learn general and transferable
information that is applicable to new domains.

Zhao and Eskénazi (2018) introduces action
matching, a learning framework that could re-
alize zero-shot dialog generation (ZSDG), based
on domain description, in the form of seed re-
sponse. With limited knowledge of a new do-
main, the model trained on several rich-resource
domains achieves both impressive task comple-
tion rate and natural generated response. Rather
than action matching, we propose to use model-
agnostic meta-learning (MAML) algorithm (Finn
et al., 2017) to perform dialog domain adapta-
tion. The MAML algorithm tries to build an inter-
nal representation of multiple tasks and maximize
the sensitivity of the loss function when applied
to new tasks, so that small update of parameters
could lead to large improvement of new task loss
value. This allows our dialog system to adapt to
new domain successfully not only with little target
domain data but also in a more efficient manner.

The key idea of this paper is utilizing the abun-
dant data in multiple resource domains and find-
ing an initialization that could be accurately and
quickly adapted to an unknown new domain with
little data. We use the simulated data generated
by SimDial (Zhao and Eskénazi, 2018). Specif-
ically, we use three domains: restaurant, weather,
and bus information search, as source data and test
the meta-learned parameter initialization against



2640

the target domain, movie information search. By
modifying Sequicity (Lei et al., 2018), a seq2seq
encoder-decoder network, improving it with a
two-stage CopyNet (Gu et al., 2016), we imple-
ment the MAML algorithm to achieve an optimal
initialization using dialog data from source do-
mains. Then, we fine-tune the initialization to-
wards the target domain with a minimal portion
of dialog data using normal gradient descent. Fi-
nally, we evaluate the adapted model with test-
ing data also from the target domain. We outper-
form the state-of-the-art zero-shot baseline, ZSDG
(Zhao and Eskénazi, 2018), as well as other trans-
fer learning methods (Caruana, 1997b). We pub-
lish the code on the github1.

2 Related Works

Task-oriented dialog systems are developed to
assist users to complete specific tasks, such as
booking restaurant or querying weather informa-
tion. The traditional method to build a dialog sys-
tem is to train modules separately (Chen et al.,
2017) such as: natural language understanding
(NLU) (Deng et al., 2012; Dauphin et al., 2014;
Hashemi et al.), dialog state tracker (Henderson
et al., 2014), dialog policy learning (Cuayáhuitl
et al., 2015; Young et al., 2013) and natural lan-
guage generation (NLG) (Dhingra et al., 2017;
Wen et al., 2015). Henderson et al. (2013) in-
troduces the concept of belief tracker that tracks
users’ requirements and constraints in the dialog
across turns. Recently, more and more works
combine all the modules into a seq2seq model
for the reason of easier model update. Lei
et al. (2018) has introduced a new end-to-end di-
alog system, sequicity, constructed on a two-stage
CopyNet (Gu et al., 2016): one for the belief
tracker and another one for the response genera-
tion. This model has fewer number of parameters
and trains faster than the state-of-the-art baselines
while outperforming baselines on two large-scale
datasets.

The traditional paradigm in machine learning
research is to train a model for a specific task
with plenty of annotated data. Obviously, it is
not reasonable that large amount of data is still re-
quired to train a model from scratch if we already
have models for similar tasks. Instead, we want to
quickly adapt a trained model to a new task with
a small amount of new data. Dialog adaptation

1https://github.com/qbetterk/sequicity.git

has been explored in various dimensions. Shi and
Yu (2018) introduces an end-to-end dialog system
that adapts to user sentiment. Mo et al. (2018)
and Genevay and Laroche (2016) also trains a user
adaptive dialog systems using transfer learning.
Recently, effective domain adaptation has been in-
troduced for natural language generation in dia-
log systems (Tran and Nguyen, 2018; Wen et al.,
2016). Some domain adaptation work has been
done on dialog states tracking (Mrkšić et al., 2015)
and dialog policy learning (Vlasov et al., 2018)
as well. However, there is no recent work about
domain adaptation for a seq2seq dialog system,
except ZSDG Zhao and Eskénazi (2018). ZSDG
is a zero-shot learning method that adapts action
matching to adapt models learned from multiple
source domains to a new target domain only using
its domain description. Different from ZSDG, we
propose to adapt meta-learning to achieve similar
domain adaption ability.

Meta-learning aims at learning new tasks with
few steps and little data based on well-known
tasks. One way to realize meta-learning is to learn
an optimal initialization that could be adapted
to new task accurately and quickly with little
data (Vinyals et al., 2016; Snell et al., 2017). An-
other way to learn the learning progress is to train
a meta-learner to optimize the optimizer of origi-
nal network for updating parameters (Andrychow-
icz et al., 2016; Grant et al., 2018). Meta-learning
has been applied in various circumstances such
as image classification (Santoro et al., 2016; Finn
et al., 2017), machine translation (Gu et al., 2018),
robot manipulation (Duan et al., 2016; Wang et al.,
2016), etc. We propose to apply meta-learning al-
gorithm on top of the sequicity model to achieve
dialog domain adaptation. Specifically, we chose
the recently introduced algorithm, model-agnostic
meta-learning(MAML) (Finn et al., 2017), be-
cause it generalizes across different models. This
algorithm is compatible with any model optimized
with gradient descent, such as regression, clas-
sification and even policy gradient reinforcement
learning. Moreover, this algorithm outperforms
other state-of-the-art one-shot algorithms for im-
age classification.

3 Problem Formulation

Seq2Seq-based dialog models take the dialog con-
text c as the input and generates a sentence r as the
response. Given the abundant data in the K differ-



2641

Figure 1: (a) shows the classical gradient update steps. (b) shows how we use MAML to update model with
gradient descent. The index numbers suggest the processing order of each step.

ent source domains, we have the training data in
each source domain Sk, denoted as:

DSktrain = {(c
(k)
n , r

(k)
n , Sk), n = 1...N}, k = 1...K

we also denote the data in the target domain T as:

DTtrain = {(cTn , rTn , T ), n = 1...N ′}

where N ′ << N and N ′ is only 1% of N in our
setting.

During the training process, we generate a
model

Msource : C × Sk → R

where C is the set of context and R is the set of
system responses.

For the adaptation, we fine-tune the model
Msource with target domain training data DTtrain
and obtain a new model Mtarget. Our primary
goal is to learn a model that could perform well
in the new target domain:

Mtarget : Ctarget × T → Rtarget

4 Proposed Methods

We first introduce how to combine the MAML al-
gorithm and the sequicity model. As illustrated in
the Figure 1, the typical gradient descent includes
(1) combining training data and initialized model,
(2) computing the objective loss and then (3) us-
ing the loss to update the model parameters. How-
ever, with MAML, there are two gradient update
steps. (1) We first combine the initialized model
M with training data (c(k), r(k)) from each source
domain Sk separately. (2) For each dialog domain,
we calculate the loss Lossk and them use it to up-
date every new temporary domain modelM′k. (4)

Again we use the data (c(k), r(k)) from each do-
main and its corresponding temporarily updated
domain modelM′k to calculate a new loss Loss′k
in each domain, (6) then sum all the new domain
loss to obtain the final loss. (7) Finally, we use the
final loss to update the original modelM.

In the following part, we describe the imple-
mentation details of the MAML algorithm and the
sequicity model separately. As illustrated in Algo-
rithm 1, sequicity model is used to combine nat-
ural language understanding (NLU), dialog man-
aging and response generation in a seq2seq fash-
ion, while meta-learning is a method to adjust loss
function value for better optimization. α and β in
the algorithm are the learning rate. As mentioned
in Section 3, c denotes the context and is the in-
put to the model at each turn. In order to use the
sequicity model, we format c as {Bt−1, Rt−1, Ut}
at time t, where Bt−1 is the previous belief span
at time t− 1, Rt−1 is the last system response and
Ut is the current user utterance. Sequicity model
introduces belief spans to store values of all the
informable slots and also record requestable slot
names through the history. In this way, rather than
put all the history utterances into a RNN to ex-
tract context features, we directly deal with the
slots stored in the belief span as the representation
of all history contexts. The belief span is more
accurate and simple to represent the history con-
text and needed to be updated in every turn. The
informable and requestable slots are stored in the
same span, but with different labels to avoid am-
biguity. The context at time t = 1 contains an
empty set as the former belief span B0, and an
empty string as the previous system response R0

The intuition behind the MAML algorithm is
that some internal representations are more trans-



2642

Algorithm 1 DAML

Input: dataset on source domain DStrain; α; β
Output: optimal meta-learned model
Randomly initialize model M
while not done do

for Sk ∈ Source Domain do
Sample data c(k) from DStrain
M′k =M− α∇MLSk(M, c(k))
Evaluate LSk(M′k, c(k))

end for
M←M− β∇M

∑
Sk
LSk(M′k, c(k))

end while

Function loss function L(M, c)
return cross-entropy(M(c))

Function M(c(k) = {B(k)t−1, R
(k)
t−1, U

(k)
t })

h = Encoder(B(k)t−1, R
(k)
t−1, U

(k)
t )

Bt = BspanDecoder(h)
Rt = ResponseDecoder(h,B

(k)
t ,m

(k)
t )

return Rt

ferable than others. This suggests that some in-
ternal features can be applied to multiple dialog
domains rather than a single domain.

Since MAML is compatible with any gradient
descent based model, we denote the current gener-
ative dialog model asM, which can be randomly
initialized. According to the algorithm, for each
source domain Sk, certain size of training data is
sampled. We input the training data (c(k), r(k))
into sequicity model and obtain generated system
response. We adopt cross-entropy as the loss func-
tion for all the domains:

LSk(M, c
(k), r(k)) =

|r(k)|∑
j=1

r
(k)
j · logPM(r

(k)
j )

For each source domain Sk, We use gradient de-
scent to update and get a temporary model.

M′k ←M− α∇MLSk(M, c
(k), r(k))

To be consistent with (Finn et al., 2017), we only
update the model for one step. In this way, we
have an updated model in each source domain, one
step away from M. We may consider multiple
steps of gradient update in the future work. Then,
we compute the loss based on the updated model
with the same training data in each source domain:

Loss = LSk(M
′
k, c

(k), r(k))

After this step, we have meta loss value in each do-
main. We sum up the updated loss value from all
source domains as the objective function of meta-
learning:

min
M

Meta-Loss = min
M

∑
Sk

LSk(M
′
k, c

(k), r(k))

Finally, we update the model to minimize the meta
objective function:

M←M− β∇M
∑
Sk

LSk(M
′
k, c

(k), r(k))

Unlike common gradient, in MAML, the objective
loss we use to update model is not calculated di-
rectly from the current model M′k, but from the
temporary modelM′k. The idea behind this oper-
ation is that the loss calculated from the updated
model is obviously more sensitive to the changes
in original domains, so that we learn more about
the common internal representations of all source
domains rather than the distinctive features of each
domain. Then in the adaptation step, since the ba-
sic internal representation has already been cap-
tured, the model is sensitive to the unique features
of the new domain. As a result, one or a few gra-
dient steps and minimum amount of data are re-
quired to optimize the model to the new domain.

The sequicity model is constructed based on
a single seq2seq model incorporating copying
mechanism and belief span to record dialog states.
Given a context c in the form of {Bt−1, Rt−1, Ut},
the belief span Bt at time t is extracted based on
the previous belief spanBt−1 at time t−1, the his-
tory response Rt−1 at time t− 1 and the utterance
Ut at time t:

Bt = seq2seq(Bt−1, Rt−1, Ut)

Then, we generate system response based on both
context and belief span extracted before:

Rt = seq2seq(Bt−1, Rt−1, Ut|Bt,mt)

mt is a simple label that helps generate the re-
sponse. It checks whether or not requested in-
formation is available in the database with con-
straints stored in Bt. mt has three possible val-
ues: no match, exact match and multiple match.
mt = “no match” denotes that the system cannot
find a match in the database given the constraints,
then the system would initiate restart the conver-
sation. mt = “exact match” indicates the sys-
tem successfully retrieves the requested informa-
tion and completes the task, then the system would



2643

Figure 2: Structure of dialog system

end the conversation. mt = “multiple matches”
means there are multiple items matches all the
constraints, so more constraints are needed to re-
duce the range of search in the backend database.
So the system will then output a question to elicit
more information.

The structure is illustrated in Figure 2 and it is
compatible with any seq2seq model. To have a
simple architecture, we adopt the basic encoder-
decoder structure. Both encoder and decoder em-
ploy GRU with attention mechanism. The re-
sponse is generated using belief span and utterance
at the current time. To simplify the model, we let
the belief extractor and response generator share
the same encoder. So we reformulate the equa-
tions into:

h = Encoder(Bt−1, Rt−1, Ut)

Bt = BspanDecoder(h)

Rt = ResponseDecoder(h,Bt,mt)

We also need to apply the third attention-based
GRU for the response decoding.

Because the response and the utterance usually
share some word tokens, the sequicity model also
incorporates copy-attention mechanism. Origi-
nally, to decode an encoded vector, the model uses
softmax to obtain a probability over vocabulary
P vocab(v) where v ∈ V . With copy-attention,
the decoder not only considers the word genera-
tion probability distribution over vocabulary, but
also the likelihood of copy the word from input se-
quence P copy(v) where v ∈ V ∪ Ut and Ut is the
current user utterance in the input context c. Then
the total probability of word v at ith token in the
output sequence is calculated by summing these

two probabilities (normalization is performed af-
ter the summation):

Pi(v) = (1−g)·P vocabi (v)+g·P
copy
i (v), v ∈ V ∪Ut

The copy probability is calculated similarly in Gu
et al. (2016) and is different for belief span de-
coder and response decoder.

For the belief span decoder, the copy probability
is calculated as:

P copyi (v) =
1

Z

|Ut|∑
j:uj=v

eψ(uj)

where Z is a normalization factor and uj is the jth
word tokens in the utterance Ut. We only add the
component when uj is the same as the target word
v. ψ(uj) is computed by:

ψ(uj) = σ((hencj )
TW)hdecj

where hencj is the hidden state in the encoder for
the jth word as input, hdecj is the hidden state in the
belief span decoder and W ∈ Rd×d is the copy-
attention weight.

For the response decoder, we apply the copy at-
tention on the recently generated belief span Bt
rather than utterance Ut:

P copyi (v) =
1

Z ′

|Bt|∑
j:bj=v

eψ(bj)

ψ(bj) = σ((hdecj )
TW)hdecj

where both hidden states come from belief span
decoder.

5 Experiment

We first introduce the dataset and the metrics used
to evaluate our models. Then, we describe models
evaluated in the experiments and their implemen-
tation details.

5.1 Dataset
For a fair comparison with the state-of-the-art do-
main adaptation algorithm, ZSDG (Zhao and
Eskénazi, 2018), we use the dataset, SimDial,
which first introduced to evaluate ZSDG. Please
refer to Appendix A for an example dialog. There
are in total six dialog domains in SimDial: restau-
rant, weather, bus, movie, restaurant-slot and
restaurant-style, where restaurant-slot data has the



2644

same slot type and sentence generation templates
as the restaurant task but a different slot vocabu-
lary. Similarly, restaurant-style has the same slots
but different natural language generation (NLG)
templates compared to the restaurant domain.

We choose restaurant, weather and bus as
source domains, denoted as following the exper-
iment setting of ZSDG in (Zhao and Eskénazi,
2018). For each source domain, we have 900,
100, 500 conversations for training, validation and
testing correspondingly, each of which has 9 turns
and each utterance has 13 word tokens on aver-
age. The rest three domains are for evaluation,
which are considered as target domains. The seed
response used in ZSDG is a set of system utter-
ances and corresponding labels. To achieve a fair
comparison, we use dialog data of the same size
for adaptation training. We generate 9 dialogs
(1% of source domain) for each domain’s adap-
tation training, each averagely contains about 8.4
turns. So for each target domain, we assume we
have around 76 system response, which is smaller
than the 100 seed response, ZSDG used as do-
main description. For testing, we use 500 dialogs
for each target model. Movie is chosen to be the
new target domain for evaluation. Because movie
has completely different NLG templates and dia-
log structure, sharing very few common traits with
the source domains at the surface level.

To avoid any random results in this few-shot
learning setting, we report the average of ten ran-
dom runs for all results. For further exploring the
property of the proposed method, we have also
generated one dialog for the one-shot experiment,
45 dialogs (5% of the size in source domain), 90
dialogs (10% of the size in source domain) study
the adaptation efficiency of our methods.

5.2 Metrics

There are three main metrics in our experiments:
BLEU score, entity F1 score and adapting time.
The first two are the most important and persua-
sive metrics used in Finn et al. (2017) has exhaus-
tively demonstrated the MAML’s fast adaptation
speed to new tasks. It could even achieve amaz-
ing performance with one step of gradient update
incorporating with halfcheetah and ant. We would
also like to count the number of epochs for adapta-
tion to compare the adaptation speed between our
methods and the baseline of transfer learning.

• BLEU We use BLEU score (Papineni et al.,

2002) to evaluate the quality of generated
response sentences since generating natural
language is also part of the task.

• Entity F1 Score For each dialog, we com-
pare the generated belief span and the Oracle
one. Since belief span contains all the slots
that constraints the response, this score also
checks the completeness of tasks.

• Adapting Time We count the number of
epochs during the adaptation training. We
only compare the adaptation with the data of
the same size.

5.3 Baseline Models
To evaluate the effectiveness of our model, we
compare DAML with the following two baselines:

• ZSDG (Zhao and Eskénazi, 2018) is the
state-of-the-art dialog domain adaptation
model. This model strengthens the LSTM-
based encoder-decoder with an action match-
ing mechanism. The model samples 100 la-
beled utterances as domain description seeds
for domain adaptation.

• Transfer learning is applied on the sequic-
ity model as the second baseline. We train
the basic model by simply mixing all the data
from source domains and then following Fig-
ure 1 (a) to update the model. We also enlarge
the vocabulary with the training data in tar-
get domain. Besides, we implement one-shot
learning version of this model by only using
one target domain dialog for adaptation, as a
comparison with the one-shot learning case
of DAML.

5.4 Implementation details
For all experiments, we use the pre-trained GloVe
word embedding (Pennington et al., 2014) with a
dimension of 50. We choose the one-layer GRU
networks with a hidden size of 50 to construct the
encoder and decoder. The model is optimized us-
ing Adam (Kingma and Ba, 2014) with a learn-
ing rate of 0.003. We reduce the learning rate to
half if the validation loss increases. We set the
batch (Ioffe and Szegedy, 2015) size to 32 and the
dropout (Zaremba et al., 2014) rate to 0.5.

6 Results and Analysis

Table 1 describes all the model performance.
We denote testing data from the combination of



2645

In Domain ZSDG Transfer DAML Transfer-oneshot DAML-oneshot
BLEU 70.1 51.8 51.8 51.1 53.7

Entity F1 79.9 88.5 91.4 87.6 91.2
Epoch - 2.7 1.4 2.2 1.0

Unseen Slot ZSDG Transfer DAML Transfer-oneshot DAML-oneshot
BLEU 68.5 43.3 (46.3) 41.7 (46.3) 40.8 (43.9) 40.0 (41.8)

Entity F1 74.6 78.7 (78.5) 75 (79.2) 70.1 (67.7) 72.0 (73.0)
Epoch - 2.6 (2.4) 4.8 (3.4) 3.2 (2.6) 5.0 (3.0)

Unseen NLG ZSDG Transfer DAML Transfer-oneshot DAML-oneshot
BLEU 70.1 30.6 (32.4) 21.5 (26.0) 20.0 (21.5) 19.1 (19.1)

Entity F1 72.9 82.2 (85.0) 77.5 (82.4) 82.8 (86.2) 69.0 (86.4)
Epoch - 3.2 (3.0) 3.2 (2.1) 12.3 (20.3) 4.7 (5.7)

New Domain ZSDG Transfer DAML Transfer-oneshot DAML-oneshot
BLEU 54.6 30.1 32.7 21.5 22.4

Entity F1 52.6 64.0 66.2 55.9 59.5
Epoch - 5.6 4.5 14.2 5.8

Table 1: DAML outperforms both ZSDG and transfer learning when given similar target domain data. Even the
one-shot DAML method achieves better results than ZSDG. Values in parenthesis are the results of the model
with an extra step of fine-tuning on the restaurant domain in training. “In Domain” uses all three source domains
(restaurant, weather and bus), while “New Domain” refers to the movie domain. “Unseen Slot” and “Unseen
NLG” correspond to restaurant-slot and restaurant-style separately.

restaurant, weather and bus domains as “In Do-
main” data since they are in the same domains as
what we use to train. The data from movie domain
is denoted as “New Domain” as it is unseen in
training data. “Unseen Slot” and “Unseen NLG”
represent restaurant-slot and restaurant-style do-
mains correspondingly. To keep a fair compari-
son, both Transfer and DAML use 1% of source
domain data (9 dialogs, in total 76 system re-
sponses), which is equal to the seed response that
Zhao and Eskénazi (2018) uses. We found that
both transfer learning and DAML obtain better
results than ZSDG. Especially for the “New Do-
main”, DAML achieves the entity F1 score of
66.2, 25.8% relative improvement compared with
ZSDG. As for “In Domain” testing, DAML also
obtains 14.4% improvement beyond ZSDG. How-
ever, our method does not get large improvement
in the “Unseen slot” and “Unseen NLG” domains.
We notice that these two domains are actually gen-
erated from one of the source domain (restaurant
domain). So, even though the slots or templates
are changed, they should still share some features
with the original domain data. If we could take
advantage of the original restaurant domain, the
result should be improved. Following this intu-
ition, in the “Unseen slot” domain and the “Un-
seen NLG” domain, we first fine-tune the model
obtained from DAML with the original restaurant
data in training, and then we do further fine-tune
with the adaptation data. The results are further
improved and presented in the parenthesis in Ta-
ble 1. We see that in most cases, fine-tuning on

restaurant data increases both the BLEU score and
entity F1 score on the “Unseen Slot” and “Unseen
NLG” domain.

Finn et al. (2017) emphasizes that meta-learning
obtains decent results with extremely small size
of data, even in the one-shot cases. To verify
this claim, we perform a one-shot version of the
DAML training along with one-shot transfer learn-
ing by only using one target domain dialog. The
result shows that even the one-shot case of DAML
outperforms the ZSDG baseline in all cases except
“Unseen slot” in entity F1. For the “Unseen NLG”
domain, the DAML one-shot case even obtains the
highest score. Considering DAML one-shot also
having out-standing performance when adapted to
“In Domain,” this suggests that the “Unseen NLG”
domain is relatively close to the “In Domain.” And
nearly every model achieves a similarly high score
by fine-tuning the model which is already adapted
to the “In Domain” data. Since the score of “In
Domain” is already extremely high, we assume the
model have learned the common features well. We
also mention in the Sec 4 that MAML is sensi-
tive to the new knowledge. Given that the model
already learns the common features well ,in the
one-shot setting, the model focuses on learning the
unique features of the target domain, while the set-
ting with 1% adaptation data still partially focus
on some common features.

And our method shows evident advantage not
only with better scores but also with much fewer
update steps. We observe in Table 1, DAML only
needs one epoch to find the optimum when adapt-



2646

ing to the “In Domain.” Even for the “New Do-
main,” DAML only uses 5.8 epochs on average to
converge, which is only 40% of epochs used in
transfer learning. The epoch numbers in the Ta-
ble 1 are not integers because all the results in our
experiment are the average value of results from
ten random runs, explained in Sec 5.1. Therefore,
we conclude DAML is more efficient compared
with simple transfer learning.

DAML’s success mainly comes from three pos-
sible reasons. The first is the CopyNet mecha-
nism. The copy model directly copy and output
word tokens from the context, contributing to the
high entity F1 score. The belief span also helps
to improve the performance. With the belief span,
we no longer need to extract slots from all the his-
tory utterances in each turn. Instead, we only need
the previous slots, stored in belief span, that the
copy model could directly deal with. This allows
us to simplify our framework and improve the per-
formance. Finally, the meta-learning allows our
model to learn inner features of the dialog across
different domains.

movie Transfer DAML
Entity F1 64.0 66.2
BLEU 30.1 32.7
restaurant Transfer DAML
Entity F1 80.7 82.1
BLEU 46.1 47.9
bus Transfer DAML
Entity F1 60.0 61.9
BLEU 32.0 35.9
weather Transfer DAML
Entity F1 79.1 80.4
BLEU 38.9 43.3

Table 2: Performance on different dialog domains

We also change different tasks used in source
and target data to validate the robustness of our
model. We use the leave-one-out approach to com-
pare the difference between movie, restaurant, bus
and weather domains. When we choose one of
them as the target domain, we use the other three
as the source domains. The size of the dataset (1%
target data for adaptation) and model hyperparam-
eters are keeping the same as the main experiment
described above. We observe in the table 2, the
restaurant domain achieves both the highest en-
tity F1 score and the highest BLEU score, which
means it is the easiest domain to adapt to. The
bus domain receives the lowest entity F1 score and
the movie domain holds the second lowest one, as
well as the lowest BLEU score. This demonstrates

that the movie domain is really a hard domain for
adaptation and is worth being chosen as the target
domain. Among all combinations, DAML outper-
forms the transfer learning algorithms in both En-
tity F1 and BLEU.

Figure 3: The system performance improves when the
size of the target data increases. Even the one-shot
learning setting achieves decent performance.

In addition, we investigate the impact of using
different amount of target domain data on system
performance. We use the best model trained on
restaurant, bus and weather and test on the movie
domain. The size of target data varies from one
dialog in one-shot learning to 10% of the data,
which is 90 dialogs. Figure 3 shows the system
performance positively correlates with the amount
of training data available in the target domain.
We observe that both entity F1 and BLEU scores
nearly converge when 4% of the data is used. Al-
though 4% is three times the size of the seed re-
sponse used in Zhao and Eskénazi (2018), we no-
tice that even the one-shot case of our model out-
performs ZSDG in the new domain. This demon-
strates our method’s capability to achieve good
performance with only little target data.

Although the DAML has demonstrated out-
standing performance in dialog domain adapta-
tion, it still cannot perfectly adapt to a new do-
main, especially when there is out of domain
words in new domain, denoted as unk. If unk
lies in the utterance, such as “system: Movie from
what country?” “user: Movie from unk.” System
can hardly extract the needed slot since it does not
recognize the surface form of the slot, even if we
recognize the unk as the entity. If unk appears in
the belief span, when our system uses copy model
to generate the new belief span based on the pre-
vious one, it is hard to handle the unk token.

The model also has difficulties in handling com-
plex utterances, especially when a sentence has
corrections, such as: “new request. in 2000-2010.



2647

oh no, in 70s.” In this case, our system success-
fully adds only 70s to the belief span, mainly be-
cause the adverb in suggests 70s is a year. How-
ever, the system keeps the original slot year, lead-
ing to a no match result. Moreover, in the case
“that’s wrong. i love western ones.”, our system
is confused on what the pronoun “ones” refers to.
So it does not recognize “western” is a dialog slot.

7 Conclusion and Future Work

We propose a domain adaptive dialog generation
method based on meta-learning(DAML). We also
construct an end-to-end trainable dialog system
that utilizes a two-step gradient update to obtain
models that are more sensitive to new domains.
We evaluate our model on a simulated dataset with
multiple independent domains. DAML reaches
the state-of-the-art performance in Entity F1 com-
pared with a zero-shot learning method and a
transfer learning method. DAML is an effective
and robust method for training dialog systems with
low-resources.

The DAML also provides promising potential
extension, such as applying DAML on reinforce-
ment learning-based dialog system. We also plan
to adapt DAML to multi-domain dialog tasks.

References

Marcin Andrychowicz, Misha Denil, Sergio Gomez,
Matthew W Hoffman, David Pfau, Tom Schaul,
Brendan Shillingford, and Nando De Freitas. 2016.
Learning to learn by gradient descent by gradient de-
scent. In Advances in Neural Information Process-
ing Systems, pages 3981–3989.

Yoshua Bengio. 2012. Deep learning of representa-
tions for unsupervised and transfer learning. In Pro-
ceedings of ICML Workshop on Unsupervised and
Transfer Learning, pages 17–36.

Rich Caruana. 1997a. Multitask learning. Mach.
Learn., 28(1):41–75.

Rich Caruana. 1997b. Multitask learning. Machine
learning, 28(1):41–75.

Hongshen Chen, Xiaorui Liu, Dawei Yin, and Jiliang
Tang. 2017. A survey on dialogue systems: Recent
advances and new frontiers. ACM SIGKDD Explo-
rations Newsletter, 19(2):25–35.

David Cohn, Les Atlas, and Richard Ladner. 1994. Im-
proving generalization with active learning. Ma-
chine learning, 15(2):201–221.

Heriberto Cuayáhuitl, Simon Keizer, and Oliver
Lemon. 2015. Strategic dialogue manage-
ment via deep reinforcement learning. CoRR,
abs/1511.08099.

Yann Dauphin, Gökhan Tür, Dilek Z. Hakkani-Tür, and
Larry P. Heck. 2014. Zero-shot learning and clus-
tering for semantic utterance classification. CoRR,
abs/1401.0509.

Li Deng, Gökhan Tür, Xiaodong He, and Dilek Z.
Hakkani-Tür. 2012. Use of kernel deep convex net-
works and end-to-end learning for spoken language
understanding. 2012 IEEE Spoken Language Tech-
nology Workshop (SLT), pages 210–215.

Bhuwan Dhingra, Lihong Li, Xiujun Li, Jianfeng
Gao, Yun-Nung Chen, Faisal Ahmed, and Li Deng.
2017. End-to-end reinforcement learning of dia-
logue agents for information access. In ACL.

Yan Duan, John Schulman, Xi Chen, Peter L. Bartlett,
Ilya Sutskever, and Pieter Abbeel. 2016. Rl$ˆ2$:
Fast reinforcement learning via slow reinforcement
learning. CoRR, abs/1611.02779.

Chelsea Finn, Pieter Abbeel, and Sergey Levine. 2017.
Model-agnostic meta-learning for fast adaptation of
deep networks. CoRR, abs/1703.03400.

Aude Genevay and Romain Laroche. 2016. Transfer
learning for user adaptation in spoken dialogue sys-
tems. In AAMAS.

Erin Grant, Chelsea Finn, Sergey Levine, Trevor
Darrell, and Thomas Griffiths. 2018. Recasting
gradient-based meta-learning as hierarchical bayes.
arXiv preprint arXiv:1801.08930.

Jiatao Gu, Zhengdong Lu, Hang Li, and Victor
O. K. Li. 2016. Incorporating copying mech-
anism in sequence-to-sequence learning. CoRR,
abs/1603.06393.

Jiatao Gu, Yong Wang, Yun Chen, Kyunghyun Cho,
and Victor O. K. Li. 2018. Meta-learning for
low-resource neural machine translation. CoRR,
abs/1808.08437.

Homa B Hashemi, Amir Asiaee, and Reiner Kraft.
Query intent detection using convolutional neural
networks.

Matthew Henderson, Blaise Thomson, and Jason D.
Williams. 2014. The third dialog state tracking chal-
lenge. 2014 IEEE Spoken Language Technology
Workshop (SLT), pages 324–329.

Matthew Henderson, Blaise Thomson, and Steve
Young. 2013. Deep neural network approach for the
dialog state tracking challenge. In Proceedings of
the SIGDIAL 2013 Conference, pages 467–471.

Sergey Ioffe and Christian Szegedy. 2015. Batch nor-
malization: Accelerating deep network training by
reducing internal covariate shift. arXiv preprint
arXiv:1502.03167.

https://doi.org/10.1023/A:1007379606734
http://arxiv.org/abs/1611.02779
http://arxiv.org/abs/1611.02779
http://arxiv.org/abs/1611.02779
http://arxiv.org/abs/1703.03400
http://arxiv.org/abs/1703.03400
http://arxiv.org/abs/1808.08437
http://arxiv.org/abs/1808.08437


2648

Diederik P Kingma and Jimmy Ba. 2014. Adam: A
method for stochastic optimization. arXiv preprint
arXiv:1412.6980.

Wenqiang Lei, Xisen Jin, Min-Yen Kan, Zhaochun
Ren, Xiangnan He, and Dawei Yin. 2018. Sequic-
ity: Simplifying task-oriented dialogue systems with
single sequence-to-sequence architectures. In Pro-
ceedings of the 56th Annual Meeting of the Associa-
tion for Computational Linguistics (Volume 1: Long
Papers), pages 1437–1447. Association for Compu-
tational Linguistics.

Fei-Fei Li, Rob Fergus, and Pietro Perona. 2006. One-
shot learning of object categories. IEEE transac-
tions on pattern analysis and machine intelligence,
28(4):594–611.

Kaixiang Mo, Yu Zhang, Shuangyin Li, Jiajun Li, and
Qiang Yang. 2018. Personalizing a dialogue sys-
tem with transfer reinforcement learning. In Thirty-
Second AAAI Conference on Artificial Intelligence.

Nikola Mrkšić, Diarmuid O Séaghdha, Blaise Thom-
son, Milica Gašić, Pei-Hao Su, David Vandyke,
Tsung-Hsien Wen, and Steve Young. 2015. Multi-
domain dialog state tracking using recurrent neural
networks. arXiv preprint arXiv:1506.07190.

Mohammad Norouzi, Tomas Mikolov, Samy Bengio,
Yoram Singer, Jonathon Shlens, Andrea Frome,
Greg S Corrado, and Jeffrey Dean. 2013. Zero-shot
learning by convex combination of semantic embed-
dings. arXiv preprint arXiv:1312.5650.

Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic eval-
uation of machine translation.

Jeffrey Pennington, Richard Socher, and Christo-
pher D. Manning. 2014. Glove: Global vectors for
word representation. In Empirical Methods in Nat-
ural Language Processing (EMNLP), pages 1532–
1543.

Ruslan Salakhutdinov, Joshua Tenenbaum, and Anto-
nio Torralba. 2012. One-shot learning with a hi-
erarchical nonparametric bayesian model. In Pro-
ceedings of ICML Workshop on Unsupervised and
Transfer Learning, pages 195–206.

Adam Santoro, Sergey Bartunov, Matthew Botvinick,
Daan Wierstra, and Timothy P. Lillicrap. 2016.
Meta-learning with memory-augmented neural net-
works. In ICML.

Weiyan Shi and Zhou Yu. 2018. Sentiment adaptive
end-to-end dialog systems. CoRR, abs/1804.10731.

Jake Snell, Kevin Swersky, and Richard Zemel. 2017.
Prototypical networks for few-shot learning. In Ad-
vances in Neural Information Processing Systems,
pages 4077–4087.

Richard Socher, Milind Ganjoo, Christopher D Man-
ning, and Andrew Ng. 2013. Zero-shot learning
through cross-modal transfer. In Advances in neu-
ral information processing systems, pages 935–943.

Van-Khanh Tran and Le-Minh Nguyen. 2018. Adver-
sarial domain adaptation for variational neural lan-
guage generation in dialogue systems. In COLING.

Oriol Vinyals, Charles Blundell, Timothy Lillicrap,
Daan Wierstra, et al. 2016. Matching networks for
one shot learning. In Advances in neural informa-
tion processing systems, pages 3630–3638.

Vladimir Vlasov, Akela Drissner-Schmid, and Alan
Nichol. 2018. Few-shot generalization across dia-
logue tasks. CoRR, abs/1811.11707.

Jane X. Wang, Zeb Kurth-Nelson, Dhruva Tiru-
mala, Hubert Soyer, Joel Z. Leibo, Rémi Munos,
Charles Blundell, Dharshan Kumaran, and Matthew
Botvinick. 2016. Learning to reinforcement learn.
CoRR, abs/1611.05763.

Tsung-Hsien Wen, Milica Gasic, Dongho Kim, Nikola
Mrksic, Pei hao Su, David Vandyke, and Steve J.
Young. 2015. Stochastic language generation in dia-
logue using recurrent neural networks with convolu-
tional sentence reranking. In SIGDIAL Conference.

Tsung-Hsien Wen, Milica Gasic, Nikola Mrksic,
Lina Maria Rojas-Barahona, Pei hao Su, David
Vandyke, and Steve J. Young. 2016. Multi-domain
neural network language generation for spoken dia-
logue systems. In HLT-NAACL.

Steve J. Young, Milica Gasic, Blaise Thomson, and Ja-
son D. Williams. 2013. Pomdp-based statistical spo-
ken dialog systems: A review. Proceedings of the
IEEE, 101:1160–1179.

Wojciech Zaremba, Ilya Sutskever, and Oriol Vinyals.
2014. Recurrent neural network regularization.
arXiv preprint arXiv:1409.2329.

Tiancheng Zhao and Maxine Eskénazi. 2018. Zero-
shot dialog generation with cross-domain latent ac-
tions. CoRR, abs/1805.04803.

A SimDial Dataset Example

Table 3 is an example dialog in the restaurant do-
main generated by SimDial. Both the domain and
sentence complexity can be adjusted. For each
turn, bot the informable and the requestable slots
would be recorded and stored in the belief span.

http://aclweb.org/anthology/P18-1133
http://aclweb.org/anthology/P18-1133
http://aclweb.org/anthology/P18-1133
http://www.aclweb.org/anthology/D14-1162
http://www.aclweb.org/anthology/D14-1162
http://arxiv.org/abs/1804.10731
http://arxiv.org/abs/1804.10731
http://arxiv.org/abs/1611.05763
http://arxiv.org/abs/1805.04803
http://arxiv.org/abs/1805.04803
http://arxiv.org/abs/1805.04803


2649

turn speaker utterances inform slots request slots

1 user What’s up? hmm I am looking for a restaurant.sys Which place?

2 user I uhm yeah I don’t care. Oh sorry, Philadelphia. loc,Philadelphiasys I believe you said Philadelphia.

3 user I have more requests. What kind of parking does it have? loc,Philadelphia; parkingsys The restaurant has no parking. Anything else? food,Indian

4 user I have more requests. Is hmm ... it closed? loc,Philadelphia; openingsys No, It is open right now. What else can I do? food,Indian

5 user New request. I’m interested in food uhm at Seattle. loc,Seattle;sys Do you mean Indian? food,Indian

6 user Uh-huh. loc,Seattle;sys Restaurant 56 is a good choice. What else can I do? food,Indian

7 user Not done yet. What’s the average price? loc,Seattle; pricesys The restaurant serves moderate food. food,Indian

8 user I have all I need. See you. loc,Seattle;sys See you next time. food,Indian

Table 3: An example dialog generated from SimDial


