



















































One model, two languages: training bilingual parsers with harmonized treebanks


Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 425–431,
Berlin, Germany, August 7-12, 2016. c©2016 Association for Computational Linguistics

One model, two languages: training bilingual parsers with harmonized
treebanks

David Vilares, Carlos Gómez-Rodrı́guez and Miguel A. Alonso
Grupo LyS, Departamento de Computación, Universidade da Coruña

Campus de A Coruña s/n, 15071, A Coruña, Spain
{david.vilares, carlos.gomez, miguel.alonso}@udc.es

Abstract

We introduce an approach to train lexical-
ized parsers using bilingual corpora ob-
tained by merging harmonized treebanks
of different languages, producing parsers
that can analyze sentences in either of
the learned languages, or even sentences
that mix both. We test the approach
on the Universal Dependency Treebanks,
training with MaltParser and MaltOpti-
mizer. The results show that these bilin-
gual parsers are more than competitive, as
most combinations not only preserve accu-
racy, but some even achieve significant im-
provements over the corresponding mono-
lingual parsers. Preliminary experiments
also show the approach to be promising on
texts with code-switching and when more
languages are added.

1 Introduction

The need of frameworks for analyzing content in
different languages has been discussed recently
(Dang et al., 2014), and multilingual dependency
parsing is no stranger to this challenge. Data-
driven parsing models (Nivre, 2006) can be trained
for any language, given enough annotated data.

On languages where treebanks are not available,
cross-lingual transfer can be used to train parsers
for a target language with data from one or more
source languages. Data transfer approaches (e.g.
Yarowsky et al. (2001), Tiedemann (2014)) map
linguistic annotations across languages through
parallel corpora. Instead, model transfer ap-
proaches (e.g. Naseem et al. (2012)) rely on cross-
linguistic syntactic regularities to learn aspects of
the source language that help parse an unseen lan-
guage, without parallel corpora.

Model transfer approaches have benefitted from
the development of multilingual resources that
harmonize annotations. Petrov et al. (2011)
proposed a universal tagset, and McDonald et
al. (2011) employed it to transfer delexicalized
parsers (Zeman and Resnik, 2008). More recently,
several projects have presented treebank collec-
tions of multiple languages with their annotations
standardized at the syntactic level, including Ham-
leDT (Zeman et al., 2012) and the Universal De-
pendency Treebanks (McDonald et al., 2013).

In this paper we also rely on these resources,
but with a different goal: we use universal anno-
tations to train bilingual dependency parsers that
effectively analyze unseen sentences in any of
the learned languages. Unlike delexicalized ap-
proaches for model transfer, our parsers exploit
lexical features. The results are encouraging: our
experiments show that, starting with a monolin-
gual parser, we can “teach” it an additional lan-
guage for free in terms of accuracy (i.e., without
significant accuracy loss on the original language,
in spite of learning a more complex task) in the
vast majority of cases.

2 Bilingual training

Universal Dependency Treebanks v2.0 (McDon-
ald et al., 2013) is a set of CoNLL-formatted tree-
banks for ten languages, annotated with common
criteria. They include two versions of PoS tags:
universal tags (Petrov et al., 2011) in the CPOSTAG
column, and a refined annotation with treebank-
specific information in the POSTAG column. Some
of the latter tags are not part of the core univer-
sal set, and they can denote linguistic phenomena
that are language-specific, or phenomena that not
all the corpora have annotated in the same way.

To train monolingual parsers (our baseline), we
used the official training-dev-set splits provided

425



with the corpora. For the bilingual models, for
each pair of languages L1, L2; we simply merged
their training sets into a single file acting as a train-
ing set for L1∪L2, and we did the same for the de-
velopment sets. The test sets were not merged be-
cause comparing the bilingual parsers to monolin-
gual ones requires evaluating each bilingual parser
on the two corresponding monolingual test sets.

To build the models, we relied on MaltParser
(Nivre et al., 2007). Due to the large number of
language pairs that complicates manual optimiza-
tion, and to ensure a fair comparison, we used
MaltOptimizer (Ballesteros and Nivre, 2012), an
automatic optimizer for MaltParser models. This
system works in three phases: Phase 1 and 2
choose a parsing algorithm by analyzing the train-
ing set, and performing experiments with default
features. Phase 3 tunes the feature model and
algorithm parameters. We hypothesize that the
bilingual models will learn a set of features that
fits both languages, and check this hypothesis by
evaluating on the test sets.

We propose two training configurations: (1) a
treebank-dependent tags configuration where we
include the information in the POSTAG column
and (2) a universal tags only configuration, where
we do not use this information, relying only on
the CPOSTAG column. Information that could be
present in FEATS or LEMMA columns is not used
in any case. This methodology plans to answer
two research questions: (1) can we train bilingual
parsers with good accuracy by merging harmo-
nized training sets?, and (2) is it essential that the
tagsets for both languages are the same, or can we
still get accuracy gains from fine-grained PoS tags
(as in the monolingual case) even if some of them
are treebank-specific?

All models are freely available.1

3 Evaluation

To ensure a fair comparison between monolingual
and bilingual models, we chose to optimize the
models from scratch with MaltOptimizer, expect-
ing it to choose the parsing algorithm and feature
model which is most likely to obtain good results.
We observed that the selection of a bilingual pars-
ing algorithm was not necessarily related with the
algorithms selected for the monolingual models.
The system sometimes chose an algorithm for a
bilingual model that was not selected for any of

1http://grupolys.org/software/PARSERS/

the corresponding monolingual models.
In view of this, and as it is known that different

parsing algorithms can be more or less competitive
depending on the language (Nivre, 2008), we ran
a control experiment to evaluate the models set-
ting the same parsing algorithm for all cases, exe-
cuting only phase 3 of MaltOptimizer. We chose
the arc-eager parser for this experiment, as it was
the algorithm that MaltOptimizer chose most fre-
quently for the monolingual models in the previ-
ous configuration. The aim was to compare the
accuracy of the bilingual models with respect to
the monolingual ones, when there is no variation
on the parsing algorithm between them. The re-
sults of this control experiment are not shown for
space reasons, but they were very similar to those
of the original experiment.

3.1 Results on the Universal Treebanks

Table 1 compares the accuracy of bilingual models
to that of monolingual ones, under the treebank-
dependent tags configuration. Each table cell
shows the accuracy of a model, in terms of LAS
and UAS. Cells in the diagonal correspond to
monolingual models (the baseline), with the cell
located at row i and column i representing the re-
sult obtained by training a monolingual parser on
the training set of language Li, and evaluating it
on the test set of the same language Li. Each cell
outside the diagonal (at row i and column j, with
j 6= i) shows the results of training a bilingual
model on the training set for Li∪Lj , evaluated on
the test set of Li.

As we can see, in a large majority of cases,
bilingual parsers learn to parse two languages with
no statistically significant accuracy loss with re-
spect to the corresponding monolingual parsers
(p < 0.05 with Bikel’s randomized parsing eval-
uation comparator). This happened in 74 out of
90 cases when measuring UAS, or 69 out of 90 in
terms of LAS. Therefore, in most cases where we
are applying a parser to texts in a given language,
adding a second language comes for free in terms
of accuracy.

More strikingly, there are many cases where
bilingual parsers outperform monolingual ones,
even in this evaluation on purely monolingual
datasets. In particular, there are 12 cases where
a bilingual parser obtains statistically significant
gains in LAS over the monolingual baseline, and 9
cases with significant gains in UAS. This clearly

426



de en es fr id it ja ko pt-br sv

de 78.27 78.01
− 77.82− 77.83− 77.84− 78.10− 77.86− 77.94− 78.13− 78.60+

84.03 84.08+ 83.82− 83.55−− 83.85− 84.12+ 83.88− 83.63− 83.87− 84.38+

en 89.37
+ 89.36 89.46+ 89.38+ 89.69++ 89.82++ 89.43+ 89.63++ 89.60++ 89.11−−

91.02+ 91.02 91.09+ 91.06+ 91.32++ 91.47++ 91.10+ 91.32++ 91.24+ 90.79−−

es 80.85
+ 81.08++ 80.60 80.95+ 81.16+ 80.92+ 81.41++ 81.49++ 79.96− 81.26++

85.17+ 85.27++ 84.75 85.15+ 85.00+ 85.13+ 85.52++ 85.39++ 84.70− 85.42++

fr 79.01
− 79.39+ 79.36+ 79.29 79.61+ 79.34+ 79.16− 79.36+ 79.09− 79.66+

84.17− 84.49+ 84.56+ 84.47 84.32− 84.41− 84.34− 84.72+ 83.98− 84.84+

id 75.72
−− 77.19− 77.12− 77.15− 77.69 78.29+ 77.60− 76.68−− 77.45− 77.01−−

81.73−− 82.66−− 82.72− 82.66− 83.38 84.09+ 83.18− 82.16−− 82.96− 82.59−−

it 82.62
−− 83.17−− 83.12−− 83.10−− 83.74−− 84.40 84.62+ 84.79+ 83.70− 84.55+

86.14−− 86.46−− 86.78− 86.69− 86.73−− 87.54 87.48− 87.46− 87.39− 87.23−

ja 76.53
−− 76.24−− 76.61−− 76.32−− 75.18−− 77.05− 77.46 76.89− 76.69− 76.89−

83.77− 83.89− 84.26− 84.05−− 83.08−− 83.97− 84.34 83.65− 83.97− 84.17−

ko 86.13
−− 88.30+ 87.91+ 88.49+ 85.86−− 88.72++ 87.14−− 87.83 86.75−− 88.68−

90.61−− 92.16+ 92.00− 92.35+ 90.19−− 92.55+ 91.89− 92.12 91.39−− 92.39−

pt-br 84.83
− 85.06+ 84.99+ 84.97+ 85.10+ 85.43++ 84.95+ 85.12+ 84.88 85.25++

87.18− 87.19− 87.27+ 87.17− 87.35− 87.68++ 87.13− 87.35− 87.39 87.43++

sv 81.71
−− 82.01−− 82.03− 81.92−− 82.34− 82.63+ 82.81+ 82.94++ 82.19− 82.48

86.01−− 86.39− 86.55− 86.28−− 86.69− 86.55− 86.92+ 86.83− 86.39− 86.92

Table 1: Performance on the Universal Dependency Treebanks test sets using the gold POSTAG information. For each cell,
its (row,column) pair indicates the language(s) with which the model was trained, with the row corresponding to the language
where it was evaluated. ‘ ++ ’ and ‘ + ’ indicate that the improvement in performance obtained by the bilingual model is
statistically significant or not, respectively. ‘ - - ’ and ‘ - ’ correspond to significant and not significant decreases in accuracy.

de en es fr id it ja ko pt-br sv

de 74.07 72.04
−− 74.51+ 74.44+ 73.68− 73.76− 73.90− 74.30+ 74.29+ 74.76++

79.77 77.52−− 79.95+ 79.83+ 79.24− 79.44− 79.83+ 79.76− 79.71− 80.25+

en 88.46
+ 88.35 88.65++ 88.39+ 88.61++ 88.68++ 88.65++ 88.61++ 88.65++ 88.50+

90.35+ 90.27 90.54++ 90.26− 90.47++ 90.53++ 90.49++ 90.43++ 90.55++ 90.43++

es 79.66
−− 78.78−− 80.54 79.59−− 78.98−− 79.84−− 79.59−− 79.80−− 79.74−− 79.09−−

83.81−− 82.94−− 84.35 83.26−− 82.79−− 83.79−− 83.53−− 83.57− 83.76−− 83.28−−

fr 78.43
+ 78.10− 78.63+ 78.40 77.79− 78.60+ 79.11+ 78.22− 78.56+ 78.83+

83.26− 82.77− 83.38− 83.40 82.85− 83.50+ 84.03+ 83.05− 83.45+ 83.73+

id 74.46
−− 74.65−− 77.09−− 76.23−− 78.31 77.86− 77.10−− 75.58−− 76.90−− 78.34+

80.87−− 80.21−− 82.81−− 81.78−− 83.81 83.52− 82.68−− 81.20−− 82.50−− 83.83+

it 82.27
−− 82.13−− 82.24−− 82.75−− 82.65−− 83.88 83.04−− 83.77− 83.07−− 83.47−

85.40−− 85.38−− 85.36−− 86.31−− 85.45−− 86.68 85.83−− 86.30− 86.21−− 86.33−

ja 69.41
−− 68.88−− 69.28−− 69.24−− 69.73−− 70.22−− 70.87 69.73−− 69.24−− 70.02−

79.62−− 79.21−− 79.45−− 80.11−− 79.58−− 79.58−− 81.16 80.23− 79.37−− 80.47−−

ko 84.40
−− 84.82−− 85.40−− 84.59−− 84.74−− 86.79− 86.21−− 87.52 86.29−− 86.40−−

89.61−− 90.00−− 90.77−− 89.88−− 90.00−− 91.39− 91.46−− 92.00 90.92−− 91.19−−

pt-br 83.40
− 82.76−− 83.56− 83.72− 83.08−− 83.95+ 83.80− 84.16++ 83.83 84.28++

85.78− 85.01−− 85.82− 85.85− 85.38−− 86.15+ 85.93− 86.33+ 86.11 86.41++

sv 79.65
−− 79.61−− 79.75−− 80.46− 80.94+ 81.06+ 81.19+ 81.11+ 80.89− 80.93

84.14−− 84.42−− 84.46−− 84.88− 85.14− 85.51+ 85.29− 85.14− 85.05− 85.32

Table 2: Performance on the Universal Dependency Treebanks test sets using the gold CPOSTAG information. The table is
laid out like Table 1.

surpasses the amount of significant gains to be ex-
pected by chance, and applying the Benjamini-
Hochberg procedure (Benjamini and Hochberg,
1995) to correct for multiple comparisons with a
maximum false discovery rate of 20% yields 8 sig-
nificant improvements in LAS and UAS. Therefore,
it is clear that there is synergy between datasets:
in some cases, adding annotated data in a different
language to our training set can actually improve
the accuracy that we obtain in the original lan-
guage. This opens up interesting research poten-
tial in using confidence criteria to select the data
that can help parsing in this way, akin to what
is done in self-training approaches (Chen et al.,
2008; Goutam and Ambati, 2011).

Comparing the results by language, we note that
the accuracy on the English and Spanish datasets
almost always improves when adding a second
treebank for training. Other languages that tend
to get improvements in this way are French and
Portuguese. There seems to be a rough trend to-
wards the languages with the largest training cor-
pora benefiting from adding a second language,
and those with the smallest corpora (e.g. Indone-
sian, Italian or Japanese) suffering accuracy loss,
likely because the training gets biased towards the
second language.

Training bilingual models containing a sig-
nificant number of non-overlapping treebank-
dependent tags tends to have a positive effect. En-

427



glish and Spanish are two of the clearest exam-
ples of this. As shown in Table 3, which shows a
complete report of shared PoS tags for each pair of
languages under the treebank-dependent tags con-
figuration, English only shares 1 PoS tag with the
rest of the corpora under the said configuration,
except for Swedish, with up to 5 tags in common;
and the en-sv model is the only one suffering a
significant loss on the English test set. Similar be-
havior is observed on Spanish: sv (0), en (1), ja
(10) and ko (12) are the four languages with fewest
shared PoS tags, and those are the four that ob-
tained a significant improvement on the Spanish
evaluation; while with pt-br, with 15 shared PoS
tags, we lose accuracy. The validity of this hy-
pothesis is reinforced by an experiment where we
differentiate the universal tags by language by ap-
pending a language code to them (e.g. EN NOUN
for an English noun). An overall improvement was
observed with respect to the bilingual parsers with
non-disjoint sets of features.

de en es fr id it ja ko pt-br sv
de 16 1 14 14 14 13 10 12 14 0
en 45 1 1 1 1 1 1 1 5
es 24 14 14 13 10 12 15 0
fr 14 14 13 10 12 14 0
id 14 13 10 12 14 0
it 13 10 12 13 0
ja 763 10 10 0
ko 20 12 0
pt-br 15 0
sv 25

Table 3: Shared language-specific tags between pairs of
languages

While all these experiments have been per-
formed on sentences with gold PoS tags, prelim-
inary experiments assuming predicted tags instead
show analogous results: the absolute values of
LAS and UAS are slightly smaller across the board,
but the behavior in relative terms is the same, and
the bilingual models that improved over the mono-
lingual baseline in the gold experiments keep do-
ing so under this setting.

On the other hand, Table 2 shows the perfor-
mance of the monolingual and bilingual models
under the universal tags only configuration. The
bilingual parsers are also able to keep an ac-
ceptable accuracy with respect to the monolin-
gual models, but significant losses are much more
prevalent than under the treebank-dependent tags
configuration.

Putting both tables together, our experiments

clearly suggest that not only treebank-specific tags
do not impair the training of bilingual models, but
they are even beneficial, supporting the idea that
using partially treebank-dependent tagsets helps
multilingual parsing. We hypothesize that this
may be because complementing the universal in-
formation at the syntactic level with language-
specific information at the lower levels (lexical
and morphological) may help the parser identify
specific constructions of one language that would
not benefit from the knowledge learned from the
other, preventing it from trying to exploit spuri-
ous similarities between languages. This explana-
tion is coherent with work on delexicalized parser
transfer (Lynn et al., 2014) showing that better re-
sults can be obtained using disparate languages
than closely-related languages, as long as they
have common syntactic constructions. Thus, us-
ing universal PoS tags to train multilingual parsers
can be, surprisingly, counterproductive.

3.2 Parsing code-switched sentences
Our bilingual parsers also show robustness
on texts exhibiting code-switching. Unfortu-
nately, there are no syntactically annotated code-
switching corpora, so we could not perform a for-
mal evaluation. We did perform informal tests,
by running the Spanish-English bilingual parsers
on some such sentences. We observed that they
were able to parse the English and Spanish parts
of the sentences much better than monolingual
models. This required training a bilingual tag-
ger, which we did with the free distribution of the
Stanford tagger (Toutanova and Manning, 2000);
merging the Spanish and English corpora to train
a combined bilingual tagger. Under the univer-
sal tags only configuration, the multilingual tag-
ger obtained 98.00% and 95.88% over the mono-
lingual test sets. Using treebank-dependent tags
instead, it obtained 97.19% and 93.88% over the
monolingual test sets. Figure 1 shows an interest-
ing example on how using bilingual parsers (and
taggers) affects the parsing accuracy.

Table 4 shows the performance on a tiny code-
switching treebank built on top of ten normalized
tweets.2 This confirms that monolingual pipelines
perform poorly. Using a bilingual tagger helps im-
prove the performance, thanks to accurate tags for
both languages, but a bilingual parser is needed to

2The code-switching treebank follows the Universal Tree-
bank v2.0 annotations. It can be obtained by asking any of the
authors.

428



Figure 1: Example with the en, es, en-es models. Dotted lines represent incorrectly-parsed dependencies. The corresponding
English sentence is: ‘We are working hard on putting available the best products of Spain, thank you’

Tagger Parser LAS UAS
en en 37.82 44.23
es es 27.56 41.03
en-es en 66.03 78.85
en-es es 67.95 77.56
en-es en-es 87.18 92.31

Table 4: Performance on a code-switching treebank com-
posed of 10 sentences.

push both LAS and UAS up to state-of-the-art lev-
els.

3.3 Adding more languages

To show that our approach works when more
languages are added, we created a quadrilingual
parser using the romanic languages and the fine
PoS tag set. The results (LAS/UAS) on the mono-
lingual sets were: 80.18/84.64 (es), 79.11/84.29
(fr), 82.16/86.15 (it) and 84.45/86.80 (pt). In all
cases, the performance is almost equivalent to the
monolingual parser.

Noah’s ARK group (Ammar et al., 2016) has
shown that this idea can be also adapted to univer-
sal parsing. Our models are a collection of weights
learned from mixing harmonized treebanks, that
accurately analyze sentences in any of the learned
languages and where it is possible to take advan-
tage of linguistic universals, but they are still de-
pendent on language-specific word forms. Instead,
Ammar et al. (2016) rely on multilingual word
clusters and multilingual word embeddings, learn-
ing a universal representation. They also support
incorporating language-specific information (e.g.
PoS tags) to keep learning language-specific be-
havior. To address syntactic differences between
languages (e.g. noun-adjective vs adjective-noun
structure) they can inform the parser about the in-
put language.

4 Conclusions and future work

To our knowledge, this is the first attempt to train
purely bilingual parsers to analyze sentences irre-
spective of which of the two languages they are
written in; as existing work on training a parser
on two languages (Smith and Smith, 2004) fo-
cused on using parallel corpora to transfer linguis-
tic knowledge between languages.

Our results reflect that bilingual parsers do not
lose accuracy with respect to monolingual parsers
on their corresponding language, and can even
outperform them, especially if fine-grained tags
are used. This shows that, thanks to universal de-
pendencies and shared syntactic structures across
different languages, using treebank-dependent tag
sets is not a drawback, but even an advantage.

The applications include parsing sentences of
different languages with a single model, improv-
ing the accuracy of monolingual parsing with
training sets from other languages, and success-
fully parsing sentences exhibiting code-switching.

As future work, our approach could bene-
fit from simple domain adaptation techniques
(Daumé III, 2009), to enrich the training set for
a target language by incorporating data from a
source language.

5 Acknowledgments

This research is supported by the Ministerio
de Economı́a y Competitividad (FFI2014-51978-
C2). David Vilares is funded by the Ministerio
de Educación, Cultura y Deporte (FPU13/01180).
Carlos Gómez-Rodrı́guez is funded by an Opor-
tunius program grant (Xunta de Galicia). We
thank Marcos Garcia for helping with the code-
switching treebank. We also thank the reviewers
for their comments and suggestions.

429



References
W. Ammar, G. Mulcaire, M. Ballesteros, C. Dyer, and

N. A. Smith. 2016. Many languages, one parser.
arXiv preprint arXiv:1602.01595.

M. Ballesteros and J. Nivre. 2012. MaltOptimizer:
an optimization tool for MaltParser. In Proceed-
ings of the Demonstrations at the 13th Conference
of the European Chapter of the Association for Com-
putational Linguistics, pages 58–62. Association for
Computational Linguistics.

Y. Benjamini and Y. Hochberg. 1995. Controlling
the false discovery rate: a practical and powerful
approach to multiple testing. Journal of the Royal
Statistical Society. Series B (Methodological), pages
289–300.

W. Chen, Y. Wu, and H. Isahara. 2008. Learning
reliable information for dependency parsing adap-
tation. In Proceedings of the 22Nd International
Conference on Computational Linguistics - Volume
1, COLING ’08, pages 113–120, Stroudsburg, PA,
USA. Association for Computational Linguistics.

Y. Dang, Y. Zhang, Paul J. Hu, S. A. Brown, Y. Ku,
J. Wang, and H. Chen. 2014. An integrated frame-
work for analyzing multilingual content in Web 2.0
social media. Decision Support Systems, 61:126–
135.

H. Daumé III. 2009. Frustratingly easy domain adap-
tation. arXiv preprint arXiv:0907.1815.

R. Goutam and B. R. Ambati. 2011. Exploring self
training for Hindi dependency parsing. In Proceed-
ings of 5th International Joint Conference on Natu-
ral Language Processing, pages 1452–1456, Chiang
Mai, Thailand, November. Asian Federation of Nat-
ural Language Processing.

T. Lynn, J. Foster, M. Dras, and L. Tounsi. 2014.
Cross-lingual transfer parsing for low-resourced lan-
guages: An Irish case study. In CLTW 2014. The
First Celtic Language Technology Workshop. Pro-
ceedings of the Workshop, pages 41–49, Dublin, Ire-
land.

R. McDonald, S. Petrov, and K. Hall. 2011. Multi-
source transfer of delexicalized dependency parsers.
In Proceedings of the Conference on Empirical
Methods in Natural Language Processing, pages
62–72. Association for Computational Linguistics.

R. McDonald, J. Nivre, Y. Quirmbach-brundage,
Y. Goldberg, D. Das, K. Ganchev, K. Hall, S. Petrov,
Hao Zhang, O. Täckström, C. Bedini, N. Castelló,
and J. Lee. 2013. Universal dependency annota-
tion for multilingual parsing. In Proceedings of the
51st Annual Meeting of the Association for Com-
putational Linguistics, pages 92–97. Association for
Computational Linguistics.

T. Naseem, R. Barzilay, and A. Globerson. 2012. Se-
lective sharing for multilingual dependency parsing.

In Proceedings of the 50th Annual Meeting of the As-
sociation for Computational Linguistics, pages 629–
637.

J. Nivre, J. Hall, J. Nilsson, A. Chanev, G. Eryigit,
S. Kübler, S. Marinov, and E. Marsi. 2007. Malt-
Parser: A language-independent system for data-
driven dependency parsing. Natural Language En-
gineering, 13(2):95–135.

J. Nivre. 2006. Two strategies for text parsing.
In Mickael Suominen, Antti Arppe, Anu Airola,
Orvoki Heinämäki, Matti Miestamo, Urho Määttä,
Jussi Niemi, Kari K. Pitkänen, and Kaius Sin-
nemäki, editors, A Man of Measure: Festschrift
in Honour of Fred Karlsson on his 60th Birthday,
pages 440–448. A special supplement to SKY Jour-
nal of Linguistics 19.

J. Nivre. 2008. Algorithms for deterministic incremen-
tal dependency parsing. Computational Linguistics,
34(4):513–553.

S. Petrov, D. Das, and R. McDonald. 2011. A
universal part-of-speech tagset. arXiv preprint
arXiv:1104.2086.

D. A. Smith and N. A. Smith. 2004. Bilingual Parsing
with Factored Estimation: Using English to Parse
Korean. In Dekang Lin and Dekai Wu, editors, Pro-
ceedings of EMNLP 2004, pages 49–56, Barcelona,
Spain, July. Association for Computational Linguis-
tics.

J. Tiedemann. 2014. Rediscovering annotation pro-
jection for cross-lingual parser induction. In Pro-
ceedings of COLING 2014: 25th International Con-
ference on Computational Linguistics, pages 1854–
1864.

K. Toutanova and C. D Manning. 2000. Enriching the
knowledge sources used in a maximum entropy part-
of-speech tagger. In Proceedings of the 2000 Joint
SIGDAT conference on Empirical methods in nat-
ural language processing and very large corpora:
held in conjunction with the 38th Annual Meeting
of the Association for Computational Linguistics-
Volume 13, pages 63–70.

D. Yarowsky, G. Ngai, and R. Wicentowski. 2001.
Inducing multilingual text analysis tools via robust
projection across aligned corpora. In Proceedings
of the 1st International Conference on Human Lan-
guage Technology Research, pages 1–8. Association
for Computational Linguistics.

D. Zeman and P. Resnik. 2008. Cross-language parser
adaptation between related languages. In IJCNLP
2008 Workshop on NLP for Less Privileged Lan-
guages, pages 35–42, Hyderabad, India. Interna-
tional Institute of Information Technology.

D. Zeman, D. Mareček, M. Popel, L. Ramasamy,
J. Štěpánek, Z. Žabokrtský, and J. Hajič. 2012.
HamleDT: To Parse or Not to Parse? In Nico-
letta Calzolari (Conference Chair), Khalid Choukri,

430



Thierry Declerck, Mehmet Uǧur Doǧan, Bente
Maegaard, Joseph Mariani, Asuncion Moreno, Jan
Odijk, and Stelios Piperidis, editors, Proceedings
of the Eight International Conference on Language
Resources and Evaluation (LREC’12), Istanbul,
Turkey, May. European Language Resources Asso-
ciation (ELRA).

431


