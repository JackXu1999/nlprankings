



















































Natural Language Input for In-Car Spoken Dialog Systems: How Natural is Natural?


Proceedings of the SIGDIAL 2017 Conference, pages 137–146,
Saarbrücken, Germany, 15-17 August 2017. c©2017 Association for Computational Linguistics

Natural Language Input for In-Car Spoken Dialog Systems: How Natural
is Natural?

Patricia Braunger, Wolfgang Maier, Jan Wessling, Steffen Werner
Daimler AG
Sindelfingen

Germany
{patricia.braunger, wolfgang.mw.maier}@daimler.com

{jan.wessling, steffen.s.werner}@daimler.com

Abstract

Recent spoken dialog systems are moving
away from command and control towards
a more intuitive and natural style of inter-
action. In order to choose an appropri-
ate system design which allows the sys-
tem to deal with naturally spoken user in-
put, a definition of what exactly constitutes
naturalness in user input is important. In
this paper, we examine how different user
groups naturally speak to an automotive
spoken dialog system (SDS). We conduct
a user study in which we collect freely
spoken user utterances for a wide range
of use cases in German. By means of a
comparative study of the utterances from
the study with interpersonal utterances, we
provide criteria what constitutes natural-
ness in the user input of an state-of-the-art
automotive SDS.

1 Introduction

In the automotive area, speech interfaces have con-
tinously gained importance in recent years. Cur-
rent spoken dialog systems (SDS) are expected not
to be restricted to a command-and-control-style
interaction, in which functions are invoked by the
user by speaking fixed key phrases. Instead, they
are expected to accept natural input from the user,
i.e., to understand the user without imposing re-
strictions on how he has to formulate queries.1

A definition of what exactly constitutes natural-
ness in user input is important, not only in order to
precisely understand user expectations, but also,
and especially, in order to choose an appropriate
system design which allows the system to deal
with flexible user input and spontaneous speech

1Also, it is expected, that systems answer naturally to the
user. However, a discussion of system output is beyond the
scope of this paper.

phenomena (as described by Skantze (2007)), and
to facilitate the design of meaningful system eval-
uation.

Since interpersonal interaction is the most natu-
ral form of interaction, it is often taken as a base-
line for the development of an intuitive and natural
human-machine interaction (Bonin et al., 2015).
However, earlier work shows that human speech
is strongly influenced by the assumptions that a
speaker has about his interlocutor, e.g. (Brani-
gan and Pearson, 2006), and also by individual
properties such as age, e.g. (Möller et al., 2008;
Bell, 2003). In conclusion, naturalness in user in-
put cannot simply be equated with interpersonal
speech and different user groups may have a dif-
ferent understanding of what is natural and intu-
itive. To the best of our knowledge, there are no
studies investigating what exactly constitutes nat-
uralness in user input.

In this paper, our aim is to answer the question
of which kind of utterances the natural language
understanding component of an SDS must be able
to understand from a user perspective. Thereby,
characterizing the capabilities of a dialog manage-
ment, as done by Bohlin et al. (1999) (cf. TRINDI
tick-list), is not enough – a thorough characteriza-
tion of the characteristics of natural language user
input is needed. In order to achieve this, we con-
duct a study in which we collect free user utter-
ances for an in-car SDS in German. By means
of a comparative analysis with interpersonal ut-
terances, we first show to which extent utterances
used for system interaction share properties with
interpersonal utterances. Second, we examine to
which extent different user groups speak differ-
ently in terms of naturalness.

The remainder of the paper is structured as fol-
lows. In section 2, we review previous literature
which has aimed at defining naturalness of user in-
put and describing natural language utterances re-
spectively. The following section 3 we introduce

137



our study design. Section 4 presents the evaluation
of the study, in section 5 the results are discussed
and section 6 concludes the article.

2 Towards a Definition of Naturalness

In general, natural language is human language
and therefore different from artificial languages
which are especially created for specific purposes,
e.g., computer languages. In this sense, spoken
dialog systems always make use of natural lan-
guage. This also applies to command-and-control
systems. However, the term natural is often used
as a qualifier of the abilities of the natural language
understanding (NLU) and natural language gener-
ation (NLG) modules of an SDS.

A general definition of naturalness in this sense
is given by Berg (2013), who calls SDS natural if
their language behavior is as human-like as possi-
ble. Many authors refer to this definition of nat-
ural language when they demand a more natu-
ral human-machine interaction, see, e.g., (Edlund
et al., 2008).

The literature that investigates the naturalness
of spoken user input, which is the focus of our
work, can be split into three groups.

Literature in the first group describes the users’
speaking style by means of labels like natural and
command. Hofmann et al. (2012), e.g., conduct a
web-based study to find out how users would in-
teract with internet services using speech. They
classify the observed speaking styles into natural,
command and keyword style. They state that natu-
ral reflects the way humans communicate among
each other and that the command and keyword
style is related to state-of-the-art human-machine
interaction. Berg et al. (2010) use similar la-
bels with a different meaning. They classify ut-
terances collected from a human-machine inter-
action study into commands, phrased commands
and natural language, whereas commands is used
similar to keyword style of Hofmann et al. (2012)
and natural language utterances consist of full
sentences including phrases of civility and filler
words. Similarly, in the study of Berg (2012),
speaking styles are classified into full sentences,
medium-length commands and short commands.
White et al. (2014) and Pang et al. (2011) inves-
tigate written web search queries. They classify
information seeking queries into keyword queries
and natural language questions. Natural language
questions are defined as utterances beginning with

a question indicator, such as what and do, and end-
ing with a question mark.

The second group consists of literature which
(linguistically) analyzes spoken user input style.
Braunger et al. (2016), e.g., compare crowd-
sourced natural language user input in terms of
sentence constructions. They conclude that if peo-
ple speak freely to an SDS, they mostly use an
imperative style. Winter et al. (2010) collect nat-
urally spoken utterances and quantify their com-
plexity and variety. They use context information
as a qualitative measurement for classification,
classifying the utterance content into three cate-
gories: information data, context relevant words
and non-context relevant words. They find that
users tend to repeat similar utterance patterns com-
posed from a limited set of different words.

Thirdly, we find work which concenctrates
on the differences between human-human and
human-machine communication. Guy (2016)
shows that voice queries are closer to natural lan-
guage than written queries. He builds two natu-
ral language models, one based on a corpus repre-
senting classic formal language and one based on
a corpus representing a more colloquial web lan-
guage. For measuring the similarity to a natural
language model he used perplexity. He concludes
that voice queries are still far from natural lan-
guage questions. The authors of (Hayakawa et al.,
2016) compared direct human-human dialogs to
dialogs that are mediated by a speech-to-speech
machine translation system. They found that in
machine mediated conversation speakers use less
words than in direct human-to-human communi-
cation. In (Pang and Kumar, 2011) written natural
language questions posed as web search queries
are compared to a natural language sample of
questions posted by web users on a community-
based question-answering site. Since written text
tends to be structurally complete Pang et al. (2011)
measure naturalness by means of the probability
mass of function words.

A more intuitive and natural interaction with
SDSs presupposes understanding naturally spo-
ken user utterances. In order to choose an ap-
propriate system design which allows the system
to deal with naturally spoken utterances, a defi-
nition of what exactly constitutes naturalness in
user input is necessary. Recent research in this
area only focuses on the question whether users
speak naturally or in a command-/keyword-based

138



way to a speech system, whereby naturalness is
equated with human-directed speech, e.g. (Hof-
mann et al., 2012; Pang and Kumar, 2011; Berg,
2012). The criteria mentioned for natural, human-
directed speech are full sentences, civility, filler
words and a higher number of words. Since nat-
ural is what people intuitively use, natural lan-
guage input cannot simply be equated with in-
terpersonal speaking style. Even though differ-
ent studies found that a speaker's language be-
havior is influenced by beliefs about an interlocu-
tor, cf. (Branigan and Pearson, 2006; Branigan
et al., 2010; Bell, 2003) and researchers have many
intuitions about the differences between human-
machine and human-human communication, inter-
personal speaking style is often taken as a baseline
for naturalness as can be seen from the discussed
literature and it has not been examined to which
extent the criteria mentioned for naturalness char-
acterize naturally spoken utterances towards state-
of-the-art SDS. There exist only a few empirical
studies which investigate the differences. These
research works focus either on dialog issues such
as turn-taking, e.g. (Doran et al., 2001), or on lex-
ical alignment, e.g. (Branigan et al., 2011), but not
on natural language input towards SDS in a car en-
vironment.

The way people address the system is not only
influenced by their beliefs about the system but
also by individual properties such as age or gen-
der. Work in this area of research has been done
by Bell (2003) who found that individual differ-
ences in speaker behavior are significant and by
Möller (2008) who found that younger users dif-
fer from older users in the way they speak with a
smart-home system. The observations show that
different user groups may have a different under-
standing of what is natural and intuitive. There-
fore, user profiles must be considered when defin-
ing natural language input.

3 Study Design

To the best of our knowledge, there are no data
answering the question to which extent natu-
rally spoken user input towards SDS differ from
human-directed speech and what exactly consti-
tutes naturalness in user input. We therefore con-
duct a study to examine how different user groups
would naturally speak to an actual in-car SDS and
how they would speak to their passenger.

In the following, we explain the experimental

setup and procedure of the study.

3.1 Participants
The study is targeted at younger and elder Ger-
man adults with different SDS experience and a
valid driver's license. In total, 45 subjects partici-
pated in the study. 46% of them were female and
54% were male. The average age was 39.5 years
(standard deviation SD: 13.5). 55.6% of the par-
ticipants were aged between 20-39 years, 26.6%
were 40-59 years old and 17.8% were older than
60 years. 27% were experienced in the use of spo-
ken dialog systems; 74% had little to no experi-
ence with speech-controlled devices.

3.2 Experimental Design
The study was split into two sessions and each
participant encountered both conditions (within-
subject design). In the one session the participants
had to talk to their front passenger who performed
the requested action. In the other session the par-
ticipants were asked to interact with an in-car spo-
ken dialog system. According to Möller (2008;
2005) we decided to conduct a Wizard of Oz
(WOZ) experiment. This method is less time con-
suming and less costly. In a WOZ experiment a
human operator (wizard) simulates the behavior
of an intelligent computer application whereby the
human believes to be interacting with a fully auto-
mated prototype (Dahlbaeck et al., 1993). Within
each session the participants were asked to solve
twelve tasks typically performed in a car:

1. Listen to radio station SWR3

2. Play Michael Jackson Greatest Hits

3. Navigate to Stieglitzweg 23 in Berlin

4. Call Barack Obama on his mobile phone

5. Set temperature to 23 degrees

6. Send a text message to brother

7. Weather in Berlin today

8. Date of the European Football championship
final game

9. Population of Berlin

10. Score VfB Stuttgart against FC Bayern

11. Cinema program in Berlin today

12. Next Shell gas station

The tasks consist of six non-information seeking
tasks (1-6) and six information seeking tasks (7-
12).

139



Figure 1: Task description

3.2.1 System Simulation
The system behavior was simulated with the help
of the SUEDE tool (Klemmer et al., 2000). The
system behavior was designed such as in an actual
Mercedes-Benz E-class. The system directly pro-
vided the information requested or activated the
appropriate function whereby the user input re-
sulted in a visual and acoustic system feedback.
With user input for Task 1), for example, the radio
program started playing and the screen provided
information on the current radio station.

3.2.2 Task Description
The tasks were presented by pictures in paper
form. Different studies, e.g., (Bernsen et al., 1998;
Tateishi et al., 2005), report from priming effects
when using text-based task descriptions. As pic-
tures do not bias the subjects by putting words into
their mouths, the participants were shown pictures
that describe the tasks. The tasks were pre-tested
with friendly users to find out if the desired situa-
tion was put in the user’s mind. Examples for the
task descriptions are given in Fig. 1.

3.2.3 Driving Simulation Setup
Since we want to find out how users naturally in-
teract with a spoken dialog system while driving,
we put the participants in a simulated driving situ-
ation. The participants were sitting on the driver’s
seat in a car which was placed in front of a canvas
onto which the driving simulation was projected,
such as done by Hofmann et al. (2014). They were
shown a driving simulation where they were driv-
ing behind a car. Their task was to brake if and
only if the preceding vehicle brakes. The driving
simulation setup is illustrated in Fig. 2.

3.3 Procedure

The overall procedure of the experiment was as
follows. First, the participants were informed
about the procedure. The participants were told
that they have to orally solve tasks while driv-
ing and they were shown the graphically depicted

Figure 2: Driving simulation setup

tasks. The participants had to verbally interpret
the tasks. In order to prevent wrong interpreta-
tions we gave assistance, where necessary. As for
the session with the passenger, they were told that
the passenger provided the information requested
or activated the appropriate function. As for the
system session, they were told to speak freely to
the system. They had to activate the speech recog-
nition via speaking the phrase “Hallo Auto” (eng.
“Hello Car”). Afterwards, the participants got to
know the driving simulation in a test drive last-
ing about three minutes. The instructor was sitting
on the passenger seat. The instructor showed the
task presentation pictures randomly while the par-
ticipant was driving. The tasks were permuted to
avoid order effects.

4 Evaluation and Results

In total, we collected 1.080 utterances; 540
system-directed utterances and 540 human-
directed utterances. The utterances were manually
transcribed and automatically analyzed. The
transcription exactly matched the spoken utter-
ances. The analysis included Part-of-Speech
(POS) Tagging and Parsing with SpaCy.2 The
part-of-speech-tagger uses the Google Universal
POS tag set of Petrov et al. (2011).

First, we analyze to which extent system-
directed utterances share properties with human-
directed utterances. Second, we aim at identify-
ing salient features of intuitively spoken user in-
put. Third, we analyze the impact of the users’
age and gender on their speaking style to gain
additional insights into the variability of user in-

2https://github.com/explosion/spaCy.

140



put. Therefore, system-directed utterances are
compared with human-directed utterances broken
down by the users’ age and gender. The col-
lected data are examined in terms of different lin-
guistic criteria commonly used in the literature,
e.g. (Summa et al., 2016; Johansson, 2008; Pinter
et al., 2016; Pak and Paroubek, 2010), including
those mentioned by the literature for naturalness:

• Lexical diversity

• Lexical density

• Big words

• POS tag frequencies

• Politeness

• Filler words

• Syntactic complexity

• Sentence types

• Utterance length

Only those features which occur significantly
often in system-directed speech are considered as
characteristic features of intuitively spoken user
input. In order to determine the linguistic features
that are associated with the respective criterion,
e.g., what is polite, we rely on the findings from
literature (see below).

One of the most common measures of lexical
diversity is the type-token ratio which is defined
as the ratio of the total number of individual word
types (lemmas) to the total number of occuring
word tokens, cf. (Johansson, 2008). We use the
standardized type-token tatio (STTR), firstly men-
tioned by Johnson (1944), to normalize the impact
of the size of the different corpora. Fig. 3 displays
the STTR broken down by different age, gender
and interlocutor.

The type-token ratio significantly differs be-
tween human-directed speech and system-directed
speech (p<0.01). In addition, Fig. 3 shows that
the older the users the higher the lexical diversity.
That is, older participants tend to use more indi-
vidual words than younger both in system-directed
speech and in human-directed speech. The dif-
ferences between the age groups are significant at
p<0.01. The users’ gender does not seem to have
an impact on the lexical diversity.

One of the measures of lexical density is the
content-function word ratio which is calculated
by dividing the number of content words (open
class words) by the number of function words

Figure 3: Type-token ratio broken down by user
profiles and interlocutor

(closed class words), cf. (Johansson, 2008).
This means, the higher the proportion of content
words the more information is given. In human-
directed speech people tend to use more content
words (44.68%) than in system-directed speech
(41.68%). The user profiles do not seem to have
an impact.

The big word ratio is calculated by dividing the
number of words longer than six characters (big
words) by the total number of words. We found
that people do not tend to adapt the use of big
words significantly to their interlocutor. 17.11%
of the system-directed words are big words and
16.50% of the human-directed. The user profiles
do not seem to have an impact on the use of big
words.

Next, we are interested in a difference of tag
distributions between the speech sets. Table
1 shows the seven most frequent POS tags of
both speech sets. Nouns (NOUN) and proper
nouns (PROPN) occur much more frequently in
the system-directed speech set, whereas adverbs
(ADV) and verbs (VERB) occur much more fre-
quently in the human-directed speech set. Pro-
nouns (PRON) are less frequently used in system-
directed speech (5.50%) than in human-directed
speech (10.42%). The proportion of preposi-
tions (ADP) is ranked at position seven in human-
directed speech but at position four in system-
directed speech. The proportions of determin-
ers (DET) are more or less balanced. As for
the user groups in both sets, we found differ-
ences in the occurrence of verbs between men and
women. Women tend to use more verbs than men
(in system-directed speech significant at the 0.05
level). Additionally, we found that older users
tend to use more verbs and pronouns and fewer

141



Table 1: POS tag frequencies
System-directed Human-directed

NOUN 18.93% ADV 16.73%
PROPN 17.40% NOUN 14.16%

DET 13.28% VERB 13.05%
ADP 12.65% PROPN 12.47%
ADV 12.38% DET 11.02%

VERB 9.50% PRON 10.42%
PRON 5.50% ADP 10.06%

proper nouns than younger people. These ten-
dencies hold for both system-directed speech and
human-directed speech.

Our evaluation of how polite users speak to
an SDS is based on the empirical findings of
(Danescu-Niculescu-Mizil et al., 2013). They
characterized politeness marking in requests. Out
of the 14 strategies which are perceived as being
polite the following strategies appear in our data:

• Sentence-medial please: Could you please

• Counterfactual modal: Could/Would you...

• Indicative modal: Can/Will you...

• 1st person start: I search...

• 1st person pl.: Could we find...

The distribution of utterances with politeness
indicators are shown in Fig. 4.3 The results in
Fig. 4 confirm that politeness strategies are salient
features of human-directed utterances but not of
system-directed utterances. Overall, only 19.63%
of the system-directed utterances contain polite-
ness markers, whereas 53.33% of the human-
directed utterances are polite (p<0.01). Fig. 4
shows that politeness strategies have been used
more often by women in both corpora (p<0.01).
Furthermore, younger people (20-39 years) are
far more likely to avoid politeness strategies
when speaking to the system than older people
(p<0.01).

As for the categorie filler words, we investi-
gate the number of utterances that contain dis-
fluencies such as äh and ähm (eng. “uh”) and
modal particles. We use the definition of modal

3Direct questions such as What is your native language?,
direct variants such as imperatives and sentence-initial please
are perceived as being impolite, cf. (Danescu-Niculescu-
Mizil et al., 2013). In our data, 8% of all utterances contain
an imperative with sentence-medial please. Since impera-
tives with please are perceived as not being polite we did not
count please in this morphosyntactic context.

Figure 4: Distribution of polite utterances broken
down by user profiles and interlocutor

Figure 5: Distribution of utterances containing
filler words broken down by user profiles and in-
terlocutor

particles according to Bross (2012), namely that
modal particles do not contribute to the sentence
meaning. The following modal particles occur
in our data: doch, einmal, nochmal, mal, denn,
eigentlich, vielleicht. Fig. 5 shows the percent-
age of utterances with disfluencies and modal par-
ticles. The results show that all user groups avoid
filler words when speaking to the system. Only
12.40% of the system-directed utterances contain
filler words. In contrast, 55.92% of the human-
directed utterances contain filler words. Signifi-
cant differences (p<0.01) also appear in the use of
filler words between the different age groups. 40-
59 years old people tend to use less filler words
than the younger (20-39) and older (60+) when
speaking to their passenger.

Besides lexical and pragmatic aspects we ana-
lyze our data in terms of syntactic features. One
of the measures of syntactic complexity is tree
depth. Tree depth is defined as the number of
edges in the longest path from the root node to a
leaf, cf. (Pinter et al., 2016). We have calculated
the median and mean depth of the dependency

142



Figure 6: Distribution of sentence structures bro-
ken down by interlocutor

trees. However, the differences are not significant
at p<0.05. Overall, the median tree depth of the
system-directed utterances is 3 with an interquar-
tile range of 2. The same holds for the human-
directed utterances.

Another syntactic criterion mentioned by the
literature for naturalness is the use of full sen-
tences. The criterion full sentence comprises sen-
tences containing a finite verb form. We further
subdivided the category full sentence into four cat-
egories based on sentence types. In addition, we
identified patterns without verb or just with an in-
finitive. We also found utterances composed of
two or three sentences that are categorized as sev-
eral sentences. An overview and examples of the
sentence structures we identified are given in Table
2. The frequency of the occurrence of the sentence
structures is shown in Fig. 6. Across all tasks, an
interrogative structure predominates. This is due
to the fact that the twelve tasks consist of six infor-
mation seeking tasks. As Fig. 6 implies, 95,93%
of the human-directed utterances are full sentences
but only 80,56% of the system-directed. The fre-
quency of an imperative, infinitive and verbless
construction increases significantly (p<0.05) in
system-directed speech. In human-directed speech
people tend to use more interrogative construc-
tions and several sentences to verbalize their re-
quest.

Fig. 7 displays the distribution of sentence
structures broken down by user profiles for the
system-directed utterances. Only those sentence
structures are displayed which show significantly
different distributions at the 0.05 level. Younger
people (20-39 years) and males tend to use a
lot more imperative constructions than older peo-
ple and females but less declarative constructions.

Figure 7: Distribution of sentence structures bro-
ken down by user profiles (system-directed)

Figure 8: Utterance length broken down by inter-
locutor

The group of people older than 60 years used more
often an infinitive construction than the younger
but fewer interrogative constructions. The older
participants used fewer interrogative constructions
also when speaking to the passenger. As for the
distribution of the other sentence structures oc-
cured in the human-directed set, the user groups
are more or less balanced.

In order to conclude the syntactic analysis we
compare the utterance length. Fig. 8 shows the
distribution of the number of words per utterance.
The utterances towards the system were shorter, Ø
7.01 words per utterance (SD 1.95), than the utter-
ances towards the passenger, Ø 10.22 (SD 3.64).

5 Discussion

Our comparative study shows that certain features,
e.g., full sentences or filler words, are character-
istic features of interpersonal speaking but not of
system-directed speech. We found that although
people are told to utter freely they still use syn-
tactic incomplete sentences and they are likely to
avoid politeness strategies and filler words, cf. ex-
amples given in a) and b).

143



Table 2: Sentence structures
Sentence Structure Example
Interrogative Wo ist die nächste Shell-Tankstelle?

“Where is the nearest Shell gas station?”
Imperative Spiele SWR3!

“Play SWR3!”
Declarative Ich möchte SWR3 hören.

“I would like to listen to SWR3.”
Infinitive SWR3 spielen.

No corresponding syntax existing in English
Verbless Radio SWR3

“Radio SWR3”
Several sentences Wir könnten ja heute Abend ins Kino. Was kommt denn heute in Berlin?

“We could go to the cinema this evening. What's the program in Berlin?”

a) Bitte Radiosender SWR3 einstellen.
“Please radio station SWR3 infinite verb”

b) Temperatur auf 23 Grad.
“Temperature to 23 degrees.”

Our analysis results confirm that people adapt
their speaking style depending on whom they are
talking to. According to the findings of (Levin
et al., 2013; Pearson et al., 2006; Branigan et al.,
2011) we assume that speakers are strongly influ-
enced by the assumptions that a speaker has about
his interlocutor, not only in human-machine com-
munication but also in human-human communi-
cation. Thus, people always utter in a way they
believe the system is able to understand, also if
the system behaves more human-like. We there-
fore argue that freely spoken user input should not
be considered synonymous with human-directed
speech, namely with full sentences, civility, with
the occurrence of filler words etc. The use of short
and concise phrases (such as a verbless construc-
tion) just seems to be an effect of the user adapting
to the system as conversational partner in the sense
of (Pearson et al., 2006; Branigan et al., 2011)
and is as natural (in the sense of intuitive) as us-
ing full sentences including politeness markers or
filler words. If system developers follow the as-
sumption that the linguistics of freely spoken user
input is equated with interpersonal speaking style
they hardly meet the user expectations of an intu-
itive and natural speaking. Instead, we suggest to
add incomplete syntactic structures such as verb-
less and infinite sentences to the criteria for natu-
rally spoken user input. Since 71% of the system-
directed utterances do not contain filler words or

politeness markers we also suggest not to equate
natural language input with the occurrence of filler
words and politeness indicators.

6 Conclusion

In this paper, we have contributed to the question
of how we can define naturalness in user input to-
wards a state-of-the-art SDS.

We have presented a user study in which we
have collected freely spoken user utterances for
a wide range of automotive use cases in Ger-
man. By means of a comparative study of human-
directed and system-directed utterances, we have
shown that naturalness cannot simply be equated
with human-human communication: users will
use shorter and concise phrases in order to inter-
act with the machine. We have argued that this
is an effect of the user adapting to the machine
as conversational partner in the sense of (Pearson
et al., 2006; Branigan et al., 2011). In addition,
we found that the users’ age and gender have an
impact on the way they speak to an SDS. We have
shown that women did more often make use of po-
liteness strategies and of a declarative construction
and that older users tended to use more individual
words.

Our further goal is to define evaluation criteria
which consider freely spoken user input to com-
pare different SDS. This will be subject of future
work.

144



References
Linda Bell. 2003. Linguistic Adaptions in Spoken

Human-Computer Dialogues: Empirical Studies of
User Behavior. Ph.D. thesis, KTH Royal Institute
of Technology.

Markus Berg. 2012. Survey on spoken dialogue sys-
tems: User expectations regarding style and usabil-
ity. In 14th International PhD Workshop OWD.

Markus Berg. 2013. Natürlichsprachlichkeit in di-
alogsystemen. Informatik-Spektrum 36(4):371–381.

Markus Berg, Petra Gröber, and Martina Weicht. 2010.
User study: Talking to computers. In Proceedings
of the 3rd Workshop on Inclusive eLearning.

Niels O. Bernsen, Hans Dybkjaer, and Laila Dybkjaer.
1998. Designing Interactive Speech Systems: From
First Ideas to User Testing. Springer.

Peter Bohlin, Johan Bos, Staffan Larsson, Ian Lewin,
Collin Matheson, and David Milward. 1999. Survey
of existing interactive systems. trindi project deliv-
erable d1.3. Technical report, University of Gothen-
burg.

Francesca Bonin, Ronald Böck, Nick Campbell, and
Ronald Poppe, editors. 2015. Multimodal Analyses
enabling Artificial Agents in Human-Machine Inter-
action. Springer.

Holly P. Branigan and Jamie Pearson. 2006. Align-
ment in human-computer interaction. In Proceed-
ings of the Workshop on How People Talk to Com-
puters, Robots, and Other Artificial Communication
Partners.

Holly P. Branigan, Martin J. Pickering, Jamie Pearson,
and Janet F. McLean. 2010. Linguistic alignment
between people and computers. Journal of Prag-
matics 42:2355–2368.

Holly P. Branigan, Martin J. Pickering, Jamie Pearson,
Janet F. McLean, and Ash Brown. 2011. The role of
beliefs in lexical alignment: Evidence from dialogs
with humans and computers. Cognition 121(1):41–
57.

Patricia Braunger, Hansjörg Hofmann, Steffen Werner,
and Maria Schmidt. 2016. A comparative analysis
of crowdsourced natural language corpora for spo-
ken dialog systems. In Proceedings of the 10th In-
ternational Conference on Language Resources and
Evaluation (LREC).

Fabian Bross. 2012. German modal particles and the
common ground. Helikon. A Multidisciplinary On-
line Journal 2:182–209.

Nils Dahlbaeck, Arne Joensson, and Lars Ahrenberg.
1993. Wizard of oz-studies – why and how. In Pro-
ceedings of the Workshop on Intelligent User Inter-
faces.

Cristian Danescu-Niculescu-Mizil, Moritz Sudhof,
Dan Jurafsky, Jure Leskovec, and Christopher Potts.
2013. A computational approach to politeness with
application to social factors. In Proceedings of the
51st Annual Meeting of the Association for Compu-
tational Linguistics (ACL).

Christine Doran, John Aberdeen, Laurie Damianos,
and Lynette Hirschman. 2001. Comparing several
aspects of human-computer and human-human dia-
logues. In Proceedings of the 2nd SigDial Workshop
on Discourse and Dialogue.

Jens Edlund, Joakim Gustafson, Matthias Heldner,
and Anna Hjalmarsson. 2008. Towards human-
like spoken dialog systems. Speech Communication
50:630–645.

Ido Guy. 2016. Searching by talking: Analysis of voice
queries on mobile web search. In Proceedings of the
39th International ACM SIGIR Conference on Re-
search and Development in Information Retrieval.

Akira Hayakawa, Luz Saturnino, and Nick Campbell.
2016. Talking to a system and talking to a human:
A study from a speech-to-speech, machine transla-
tion mediated map task. In Proceedings of INTER-
SPEECH.

Hansjörg Hofmann, Ute Ehrlich, André Berton, and
Wolfgang Minker. 2012. Speech interaction with the
internet - a user study. In Proceedings of the 8th In-
ternational Conference on Intelligent Environments.

Hansjörg Hofmann, Mario Hermanutz, Vanessa To-
bisch, Ute Ehrlich, André Berton, and Wolfgang
Minker. 2014. Evaluation of in-car sds notification
concepts for incoming proactive events. In Proceed-
ings of 5th International Workshop on Spoken Dia-
log Systems (IWSDS).

Victoria Johansson. 2008. The Department of Linguis-
tics and Phonetics: Working Papers 53, Lund Uni-
versity, chapter Lexical diversity and lexical density
in speech and writing: a developmental perspective,
pages 61–79.

Wendell Johnson. 1944. Studies in language behavior:
I.a program of research. Psychological Monographs
56:1–15.

Scott R. Klemmer, Anoop K. Sinha, Jack Chen,
James A. Landay, Nadeem Aboobaker, and Annie
Wang. 2000. Suede: A wizard of oz prototyping tool
of speech user interfaces. In Proceedings of the 13th
Annual ACM Symposium on User interface Software
and Technology.

Daniel T. Levin, Stephen S. Killingsworth, Megan M.
Saylor, Stephen M. Gordon, and Kazuhiko Kawa-
mura. 2013. Tests of concepts about different kinds
of minds: Predictions about the behavior of comput-
ers, robots, and people. Human-Computer Interac-
tion 28(2):161–191.

145



Sebastian Möller, Florian Gödde, and Maria Wolters.
2008. Corpus aanalysis of spoken smart-home inter-
actions with older users. In Proceeding of the 6th In-
ternational Conference on Language Resources and
Evaluation (LREC).

Alexander Pak and Patrick Paroubek. 2010. Twitter as
a corpus for sentiment analysis and opinion mining.
In Proceedings of the 7th International Conference
on Language Resources and Evaluation (LREC).

Bo Pang and Ravi Kumar. 2011. Search in the lost
sense of ”query”: Question formulation in web
search queries and its temporal changes. In Pro-
ceedings of the 49th Annual Meeting of the Asso-
ciation for Computational Linguistics: Human Lan-
guage Technologies: Short Papers - Volume 2.

Jamie Pearson, Jiang Hu, Holly P. Branigan, Martin J.
Pickering, and Clifford I. Nass. 2006. Adaptive lan-
guage behavior in hci: How expectations and beliefs
about a system affect users’ word choice. In Pro-
ceedings of the 2006 Conference on Human Factors
in Computing Systems (CHI).

Slav Petrov, Dipanjan Das, and Ryan T. McDonald.
2011. A universal part-of-speech tagset. CoRR .

Yuval Pinter, Roi Reichart, and Idan Szpektor. 2016.
Syntactic parsing of web queries with question in-
tent. In Conference of the North American Chap-
ter of the Association for Computational Linguistics:
Human Language Technologies.

Gabriel Skantze. 2007. Error Handling in Spoken Di-
alogue Systems: Managing Uncertainty, Grounding
and Miscommunication. Ph.D. thesis, KTH Stock-
holm.

Anja Summa, Bernd Resch, and Michael Strube. 2016.
Microblog emotion classification by computing sim-
ilarity in text, time, and space. In Proceedings of the
Workshop on Computational Modeling of People’s
Opinions, Personality, and Emotions in Social Me-
dia.

Masahiko Tateishi, Katsushi Asami, Ichiro Akahori,
Scott Judy, Yasunari Obuchi, Teruko Mitamura, Eric
Nyberg, and Nobuo Hataoka. 2005. A Spoken Dia-
log Corpus for Car Telematics Services, Springer,
chapter DSP for In-Vehicle and Mobile Systems,
pages 47–64.

Ryen White, Matthew Richardson, and Wen tau Yih.
2014. Questions vs. queries in informational search
tasks. Technical report, Microsoft.

Ute Winter, Tim J. Grost, and Omer Tsimhoni. 2010.
Language pattern analysis for automotive natural
language speech application. In Proceedings of the
2nd International Conference on Automotive User
Interfaces and Interactive Vehicular Applications
(AutomotiveUI).

146


