



















































The University of Maryland's Kazakh-English Neural Machine Translation System at WMT19


Proceedings of the Fourth Conference on Machine Translation (WMT), Volume 2: Shared Task Papers (Day 1) pages 134–140
Florence, Italy, August 1-2, 2019. c©2019 Association for Computational Linguistics

134

The University of Maryland’s Kazakh–English
Neural Machine Translation System at WMT19

Eleftheria Briakou and Marine Carpuat
Department of Computer Science

University of Maryland
College Park, MD 20742, USA

ebriakou@cs.umd.edu, marine@cs.umd.edu

Abstract

This paper describes the University of Mary-
land’s submission to the WMT 2019 Kazakh
to English news translation task. We study the
impact of transfer learning from another low-
resource but related language. We experiment
with different ways of encoding lexical units
to maximize lexical overlap between the two
language pairs, as well as back-translation and
ensembling. The submitted system improves
over a Kazakh–only baseline by +5.45 BLEU
on newstest2019.

1 Introduction

Neural Machine Translation (NMT) outperforms
traditional phrase-based statistical machine trans-
lation provided that large amounts of parallel data
are available (Bahdanau et al., 2014; Sennrich
et al., 2017; Vaswani et al., 2017). However,
it performs poorly under low-resource conditions
(Koehn and Knowles, 2017).

While much work addresses this problem via
semi-supervised learning from monolingual text
(Sennrich et al., 2016; He et al., 2016), we fo-
cus on transfer learning from another language
pair (Zoph et al., 2016; Nguyen and Chiang, 2017;
Lakew et al., 2018). In this setting, an NMT sys-
tem is firstly trained using auxiliary parallel data
from a so-called “parent” language pair and then
the trained model is used to initialize a “child”
model which is further trained on a low-resource
language pair. Similar approaches that support
cross-lingual transfer learning for Multi-lingual
NMT train a model on the concatenation of all data
instead of employing sequential training (Gu et al.,
2018; Zhou et al., 2018; Wang et al., 2019).

Transfer learning has been found effective in
submissions to WMT in previous years: Kocmi
et al. (2018) reported improvements of +2.4
BLEU on the low-resource Estonian→English

translation task by transfer learning from
Finnish→English. Interestingly, Kocmi and
Bojar (2018) observed that the transfer learn-
ing approach is still effective when there is no
relatedness between the “child” and “parent”
language-pairs and also hypothesize that the size
of the parent training set is the most important fac-
tor leading to translation quality improvements.
However, previous work has also empirically
validated that transfer learning benefits most
when “child”-“parent” languages belong to the
same or linguistically similar language family
(Dabre et al., 2017). Specifically, Nguyen and
Chiang (2017) showed consistent improvements
in two Turkic languages via transfering from
another related, low-resource language.

Taking those recent results into consideration,
our main focus at WMT19 is to examine transfer
learning for the Kazakh–English language pair us-
ing additional parallel data from Turkish–English.
While using distinct writing systems, both source
languages belong to the Turkic language family
and preserve many morphological and syntactic
features common for that group (Kessikbayeva
and Cicekli, 2014). As a result, they constitute a
suitable “child”-“parent” language-pair choice for
exploring transfer learning between related low-
resource languages. In this direction, we conduct
experiments to address the following questions:

• How can we represent lexical units to exploit
vocabulary overlap between languages? We
compare bilingual and monolingual byte-pair
encoding models with the recently proposed
soft decoupled encoding model.

• How can we leverage both “child” and “par-
ent” parallel data to obtain synthetic back-
translated data from monolingual resources?

mailto:ebriakou@cs.umd.edu
mailto:marine@cs.umd.edu


135

2 Approach

Our method follows a simple strategy used in
Wang et al. (2019) for multilingual training: we
directly train NMT models on the concatenation
of parallel data covering both the “child” and “par-
ent” languages with no metadata to distinguish be-
tween them.1

Within this framework, we study the impact of
(a) different lexical representations that attempt
to maximize parameter sharing across related lan-
guages, (b) romanization to increase overlap be-
tween Turkish and Kazakh which are originally
written in distinct scripts, (c) synthetic training
data obtained by back-translation.

2.1 Lexical Units

How can we define lexical units to maximize
information sharing across related source lan-
guages? We compare different configurations of
sub-word segmentations using different variants
of the standard Byte-Pair Encoding (BPE) frame-
work (Sennrich et al., 2016), and compare them
with the Soft Decoupled Encoding framework
that exploits character n-gram representations of
words instead of sub-words (Wang et al., 2019).

Joint BPEs (JBPEs) BPEs are learned jointly
from the concatenation of “child” and “parent”
parallel data. The advantage of this strategy
is that the sub-word segmentations of related
words in the two languages are encouraged to
be more aligned; thus enabling the sharing of
their representations on the source side due
to a larger vocabulary overlap. Although, the
“child” language might be “overwhelmed” by
the “parent” language when there is a significant
difference in the amount of their data (Neubig and
Hu, 2018). This could lead to over-segmentation
of the “child” language and subsequently limit the
expressive power of the NMT system.

Separate BPEs (SBPEs) BPEs are learned
separately for each language. This framework
was found to be effective in the multilingual
setting, especially for translation from extremely
low-resource languages (Neubig and Hu, 2018).
However, learning the merging operations sepa-
rately might lead to unaligned sub-units between

1We did not experiment with sequential training of the
“parent” and “child” language pairs to establish a fair com-
parison between our BPE-based models and the SDE model
that opts for joint training.

the two languages that fail to exploit relationships
between their lexical representations.

Soft Decoupled Encoding (SDE) Small dis-
crepancies in the spelling of words that share
the same semantics across the two languages
could lead to different segmented sub-units and
hinder the lexical-level sharing between them.
To take into account those spelling differences,
we further experiment with the SDE encoding
framework that is not based on any pre-processing
segmentation. Specifically, SDE represents a
word as a decomposition of two components:
a character encoding that models the language-
specific spelling of the word and a latent semantic
embedding that captures its language-agnostic
semantics. Following, we briefly summarize the
main SDE components as proposed in Wang et al.
(2019):

Lexical embedding Each word w is first decom-
posed to its bag of character n-grams (BoN(w)).
Let C be the number number of character n-grams
in the vocabulary and D be the dimension of the
corresponding character n-gram embeddings. To
acquire a lexical representation c(w), the word is
looked up to an embedding matrix Wc ∈ RC×D
as shown below:

c(w) = tanh(BoN(w) ·Wc) (1)

Language Specific Transformation Next each
word is passed through a language dependent
transformation. For each language Li a matrix
WLi ∈ RD×D is learned and the transformed em-
beddings ci(w) is computed:

ci(w) = tanh(c(w) ·WLi) (2)

Latent Semantic Embedding The shared semantic
concepts among languages are represented by a
matrix Ws ∈ RS×D, where S corresponds to the
number of semantic concepts a language can ex-
press. The latent embeddings of a word w is then
given as:

elatent(w) = Softmax(ci(w) ·W Ts ) ·Ws (3)

Finally, the SDE embedding of word w is
extracted as a combination of the language-
dependent lexical encoding and the latent embed-
ding:

eSDE(w) = elatent(w) + ci(w) (4)



136

Encoding Original Romanized

Word molekül molekula molekuel molekula
SBPEs m_ol_ek_ül mol_ek_ul_a m_ol_ek_uel mol_ek_ul_a
JBPEs mol_ek_ül mol_ek_ ul_a mol_ek_uel mol_ek_ula

Word fosfor fosfor fosfor fosfor
SBPEs f_os_for f_os_for f_os_for f_os_for
JBPEs fos_for f_os_for fos_for fos_for

Word kalamar kal~mar kalamar kalmar
SBPEs kal_am_ar k_al~_ mar kal_am_ar kalm_ar
JBPEs kal_am_ar k_al~_ mar kalam_ar kal_mar

Table 1: Examples of words sharing significant lexical overlap in Kazakh and Turkish among with their corre-
sponding sub-words segmentations.

2.2 Romanization

Given that the provided Kazakh and Turkish data
are written in the Cyrillic and Latin scripts respec-
tively, we investigate the impact of mapping text
in the two languages into a common orthography.
We transliterate both the “child” and the “parent”
data using a transliteration tool2 that applies the
same romanization rules to encourage more over-
lap between child and parent data. Table 1 illus-
trates how romanization makes shared vocabulary
and similarity between the two languages more ex-
plicit than using the original scripts.

Table 2 summarizes the statistical overlap on
the source side vocabularies between the two lan-
guages for different lexical encodings with and
without romanization. This analysis indicates that
using the original script can be seen as an attempt
to explore transfer learning when the lexical-level
sharing between the two languages is limited. On
the other hand, the vocabulary overlap between
them is significantly increased once we romanize
the data.

2.3 Synthetic Data

We further explore different ways to incorporate
target-side English monolingual data provided
by the competition into low-resource NMT.
Following the widely used back-translation ap-
proach (Sennrich et al., 2016), we create synthetic
parallel data and then train new NMT models on
the mixture of real and synthetic parallel data.

Empty source baseline The source side of each
monolingual example sentence is linked to an

2https://www.isi.edu/~ulf/uroman.html

Method Romanization # Merge op. Overlap

JBPEs

3
32K

0.44
7 0.13
3

64K
0.33

7 0.11

SBPEs

3
32K

0.18
7 0.04
3

64K
0.13

7 0.04

n-gram Overlap

SDE
3 4 0.67
3 5 0.62

Table 2: Statistical overlap results between the vocab-
ularies of the “child” and “parent” languages on the
source sides for different encoding schemes. # Merge
op. refers to the number of merge operations when
BPEs are explored. For the SDE method we compute
the overlap between the n-gram character vocabularies
(e.g., n-gram=4 corresponds to n={1,2,3,4}).

empty sentence (denoted by an artificial <null>
token).

Back-translation We create synthetic source
sentences from automatically back-translating
each target (English) sentence into the source
language (Kazakh). Within this setting, we only
use the original English-Kazakh parallel data
to train a model that translates in the opposite
direction.

Back-translation+transfer Given the data
scarcity of the Kazakh parallel data we also
attempt to incorporate both Kazakh and Turkish
data to train a model that translates in the opposite
direction. In order to produce output that is
more similar to our main language of interest, we

https://www.isi.edu/~ulf/uroman.html


137

introduce two artificial tokens (<2kk>, <2tr>)
at the beginning of the input sentence to indicate
the target language the model should translate to
(Johnson et al., 2017). After the reversed system
is trained we back-translate each target sentence
to a Kazakh synthetic sentence.3

3 Model Configuration

Our NMT systems are built upon the publicly
available code4 of Wang et al. (2019) and are
sequence-to-sequence 1-layer attentional long-
short term memory units (LSTMs) with a hidden
dimension of 512 for both the encoder and the de-
coder. The word embedding dimension is kept at
128, and all other layer dimensions are set to 512.
We use a dropout rate of 0.3 for the word embed-
ding and the output vector before the decoder Soft-
max layer. The batch size is set to be 1500 words.
We evaluate by development set BLEU score (Pa-
pineni et al., 2002) for every 2500 training batches.
For training, we use the Adam optimizer with a
learning rate of 0.001. We use learning rate de-
cay of 0.8, and stop training if the model perfor-
mance on development set doesn’t improve for 5
evaluation steps. We run each experiment with 3
different random seeds.

4 Data and Pre-processing

Parallel Data We use all the parallel data avail-
able for the Kazakh–English shared task except for
the Wikipedia Titles as they consist of very short
sentences (approximately 3 words each). Specif-
ically, the “child” training data consist of about
7.5K sentence pairs from the News Commentary
Corpus, and 98K sentence pairs from the English-
Kazakh crawled corpus5. Additionally, we used
approximately 200K Turkish–English sentence-
pairs from the Setimes2 Corpus that are provided
by the WMT18 competition.

Monolingual For the Empty source and Back-
translation methods of creating synthetic data we
used the target-side of the Turkish–English par-
allel corpus as monolingual data. For the Back-
Translation+transfer experiment we used 100K
randomly selected sentences from the News Com-
mentary corpus, excluding sentences with less
than 5 words and more than 50 words.

3Each English sentence of the monolingual corpus is aug-
mented with a <2kk> token at the beginning.

4https://github.com/cindyxinyiwang/SDE
5We didn’t filter out any sentence pairs from this corpus.

Pre-processing We process all corpora consis-
tently. We tokenize the sentences and perform
truecasing with the Moses scripts (Koehn et al.,
2007). For all the experiments we consistenly use
8K BPEs on the English target side. We exper-
iment with {32, 64}K merge operations for the
models using BPE encoding and {4, 5} n-grams
for the SDE framework. To establish a fair com-
parison between the source language representa-
tions, we consistently use the same encoding for
English words (target side) using BPEs learned on
the concatenation of all the English data.

Tuning and Testing Data The official news-
dev2019 is used as the validation set, and news-
test2019 is used as the test set.

5 Experiments

Starting from Baseline BPE-based NMT systems
trained using only the Kazakh data provided by
the competition, we conduct the following experi-
ments.

5.1 Byte Pair Encoding

Table 3 presents our results of 3 runs using
{32, 64}K merge operations in total for each ex-
periment. Generally, both Joint and Separate BPE
segmentation strategies, with and without roman-
ization improve BLEU over the Baseline. Pre-
vious empirical results on transfer learning for
extremely low-resource languages indicated that
training the BPE operations separately for the
“child” and “parent” languages has a large posi-
tive effect on the performance of the model (Wang
et al., 2019). By contrast, JBPEs and SBPEs per-
form comparably well in almost all configurations
here. This could be attributed to our less imbal-
anced setting where the ratio of “child”-“parent”
data is 1 : 2, and the child language therefore con-
tributes more to sub-word segmentation rules.

The best BLEU score is achieved using 32K
JBPEs on the romanized data which is consistent
with the configuration with the largest vocabulary
overlap, according to Table 2. However, using
{32, 64}K SBPEs on the original data only hurts
BLEU by 0.5 and 1.24, despite the lack of lexical
overlap. This suggests that most of the improve-
ment does not come from the shared encoder vo-
cabulary.

https://github.com/cindyxinyiwang/SDE


138

32K BPEs 64K BPEs

Method Original Romanized Original Romanized
Baseline 4.33± 0.16 4.49± 0.02 4.35± 0.13 4.21± 0.28
JBPEs 9.35± 0.10 9.89± 0.14 8.65± 0.27 8.77± 0.09
SBPEs 7.10± 0.26 9.70± 0.28 8.41± 0.08 8.85± 0.34

Table 3: Kazakh→ English BLEU score results on news-test2019 for different BPE configurations and versions
of data.

N-gram Lexical Latent Specific BLEU

4
3 9.12± 0.27
3 3 8.76± 0.29
3 3 3 6.57± 0.20

5
3 9.17± 0.21
3 3 8.69± 0.21
3 3 3 6.21± 0.18

Baseline-BPE 8.65± 0.27

Table 4: SDE Experiments using 64K n-grams of the
concatenated corpora. The last line refers to the best
BLEU score using 64K BPEs for comparison.

5.2 Soft-Decoupled Encoding

We compare the BPE results with different con-
figurations of the SDE model. Table 4 presents
average results of 3 runs with different random
seeds, where we use 64K character n-grams as our
vocabulary. The Language Specific Transforma-
tion consistently harms the BLEU score for both
n = 4, 5. This result validates the empirical obser-
vations of Wang et al. (2019); the separate projec-
tion does not help when the “child”-“parent” lan-
guages have a significant surface lexical overlap.
We also observe comparable BLEU results when
we use SDE embeddings or lexical embeddings
(where the latent embedding is not taken into ac-
count) to encode the semantics of words. The best
BLEU scores are achieved for the lexical encoding
using either 4-grams or 5-grams of words.

In both cases we observe that the n-gram mod-
els perform sligthly better than the best BPE
model that uses the same number of merge oper-
ations as the n-gram vocabulary size (we refer to
that model as Baseline-BPE on Table 4). However,
we do not adopt SDE in our submitted system as
the small BLEU score improvement comes with
higher computational cost when compared to the
BPE models.

5.3 Synthetic Data

Finally we experiment with back-translation of
monolingual English corpora. All experiments
used romanized text segmented with 32K BPE
merge operations. Table 5 compares 3 different
ways of using the same English data extracted
from the target side of the Turkish–English paral-
lel corpus. Each target sentence is coupled with a
synthetic Kazakh sentence (Back-translation), an
empty source sentence as a control (Empty) or a
real Turkish sentence (Transfer). The ratio of real
to additional data is kept to 1 : 2 in all cases.

NMT training does not benefit from the back-
translated data as it achieves nearly the same
BLEU as the baseline model. Suprisingly empty
source sentences yield better results than back-
translation, suggesting that the synthetic back-
translations are of low quality. Translating into
Kazakh is challenging given the small amount of
data available, especially for translating from a
morphologically poor to a morphologically rich
language. Finally, using real Turkish data on the
source side achieves the best improvement over
the baseline system (+4.4 BLEU).

Method Synthetic BLEU
Baseline 4.49
Empty 3 5.26
Back-Translation 3 4.64
Transfer 9.89

Table 5: Experiments using 200K monolingual data ex-
tracted from the target side of Turkish–English parallel
corpus. The Baseline system is trained only on Kazakh
data.

Given that in all these 3 experiments the de-
coder model was trained on the exact same English
data, these results suggest that the transfer learning
benefits both the encoder and decoder models.



139

Method Synthetic BLEU
Baseline-Transfer 9.89
Empty 3 9.17
Back-Translation 3 9.38
+ ensemble(4)? 3 9.94

Table 6: Experiments using additional 100K News
Commentary monolingual data. The Baseline system is
trained on the concatenation of Kazakh–Turkish paral-
lel data. The ? symbol denotes our primary submission
for human evaluation.

Finally, we attempt to combine Kazakh and Turk-
ish parallel data to back-translate 100K additional
monolingual data to Kazakh via training a NMT
model that has control over the output language,
as can be seen in Table 6. In this experiment our
Baseline-Transfer system refers to the best model
trained on the concatenation of “child” and “par-
ent” data. In contrast to the previous experiment
we now combine Kazakh, Turkish and synthetic
data with a ratio 1 : 2 : 1. We observe that in both
cases (Back-translation, Empty) the BLEU score
of the system trained on the augmented data fails
to outperform the Baseline-Transfer performance,
possibly due to the fact that the real Kazakh data
have been “overwhelmed” by the auxiliary ones
(Poncelas et al., 2018). However, we could as-
sume that the quality of the back-translated data
is slightly better once we utilized the Turkish data
(given that it performs better than the Empty ex-
periment).

Finally, the last row of Table 6 reports the
BLEU score of our primary submission.6 Specif-
ically, the submitted model is an ensemble ob-
tained by averaging the output distributions of
4 models trained on Kazakh, Turkish and Back-
Translated using different random seeds.

6 Conclusion

This paper presents the University of Maryland’s
NMT system for WMT 2019 Kazakh → English
news translation task. Specifically, we explored
how to improve neural machine translation of a
low-resource language by incorporating parallel
data from a related, also low-resource language.

6The Baseline-Transfer model slightly under-performed
the Baseline-Transfer+Back-Translation model on the devel-
opment set. Given that we did not have access to test data
during evaluation time, our primary submission was based on
evaluation on the development set.

Our empirical results validate that transfer learn-
ing benefits BLEU even when transfering from a
low-resource language pair. Furthermore, our re-
sults suggest that translation quality (in terms of
BLEU score) of the language-pair of focus is most
benefited when the surface-level parameter shar-
ing between the lexical representations of the two
related languages is maximized. Finally, we ob-
served that NMT training with synthetic data is
sensitive to the quality of the back-translation.

References
Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben-

gio. 2014. Neural machine translation by jointly
learning to align and translate. arXiv e-prints,
abs/1409.0473.

Raj Dabre, Tetsuji Nakagawa, and Hideto Kazawa.
2017. An empirical study of language relatedness
for transfer learning in neural machine translation.
In Proceedings of the 31st Pacific Asia Conference
on Language, Information and Computation, pages
282–286. The National University (Phillippines).

Jiatao Gu, Hany Hassan, Jacob Devlin, and Victor O.K.
Li. 2018. Universal neural machine translation for
extremely low resource languages. In Proceed-
ings of the 2018 Conference of the North Ameri-
can Chapter of the Association for Computational
Linguistics: Human Language Technologies, Vol-
ume 1 (Long Papers), pages 344–354, New Orleans,
Louisiana. Association for Computational Linguis-
tics.

Di He, Yingce Xia, Tao Qin, Liwei Wang, Nenghai Yu,
Tie-Yan Liu, and Wei-Ying Ma. 2016. Dual learn-
ing for machine translation. In Proceedings of the
30th International Conference on Neural Informa-
tion Processing Systems, pages 820–828, USA. Cur-
ran Associates Inc.

Melvin Johnson, Mike Schuster, Quoc V. Le, Maxim
Krikun, Yonghui Wu, Zhifeng Chen, Nikhil Thorat,
Fernanda Viégas, Martin Wattenberg, Greg Corrado,
Macduff Hughes, and Jeffrey Dean. 2017. Google’s
multilingual neural machine translation system: En-
abling zero-shot translation. Transactions of the As-
sociation for Computational Linguistics, 5:339–351.

Gulshat Kessikbayeva and Ilyas Cicekli. 2014. Rule
based morphological analyzer of kazakh language.
In Proceedings of the 2014 Joint Meeting of SIG-
MORPHON and SIGFSM, pages 46–54, Baltimore,
Maryland. Association for Computational Linguis-
tics.

Tom Kocmi and Ondřej Bojar. 2018. Trivial trans-
fer learning for low-resource neural machine trans-
lation. In Proceedings of the Third Conference on
Machine Translation: Research Papers, pages 244–
252, Belgium, Brussels. Association for Computa-
tional Linguistics.

https://arxiv.org/abs/1409.0473
https://arxiv.org/abs/1409.0473
https://www.aclweb.org/anthology/Y17-1038
https://www.aclweb.org/anthology/Y17-1038
https://doi.org/10.18653/v1/N18-1032
https://doi.org/10.18653/v1/N18-1032
http://dl.acm.org/citation.cfm?id=3157096.3157188
http://dl.acm.org/citation.cfm?id=3157096.3157188
https://doi.org/10.1162/tacl_a_00065
https://doi.org/10.1162/tacl_a_00065
https://doi.org/10.1162/tacl_a_00065
http://www.aclweb.org/anthology/W/W14/W14-2806
http://www.aclweb.org/anthology/W/W14/W14-2806
https://www.aclweb.org/anthology/W18-6325
https://www.aclweb.org/anthology/W18-6325
https://www.aclweb.org/anthology/W18-6325


140

Tom Kocmi, Roman Sudarikov, and Ondřej Bojar.
2018. CUNI submissions in WMT18. In Proceed-
ings of the Third Conference on Machine Trans-
lation: Shared Task Papers, pages 431–437, Bel-
gium, Brussels. Association for Computational Lin-
guistics.

Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra
Constantin, and Evan Herbst. 2007. Moses: Open
source toolkit for statistical machine translation. In
Proceedings of the 45th Annual Meeting of the As-
sociation for Computational Linguistics Companion
Volume Proceedings of the Demo and Poster Ses-
sions, pages 177–180, Prague, Czech Republic. As-
sociation for Computational Linguistics.

Philipp Koehn and Rebecca Knowles. 2017. Six chal-
lenges for neural machine translation. In Pro-
ceedings of the First Workshop on Neural Machine
Translation, pages 28–39, Vancouver. Association
for Computational Linguistics.

Surafel Melaku Lakew, Aliia Erofeeva, Matteo Negri,
Marcello Federico, and Marco Turchi. 2018. Trans-
fer learning in multilingual neural machine transla-
tion with dynamic vocabulary. In Proceedings of the
15th International Workshop on Spoken Language
Translation (IWSLT 2018), pages 54–62, Brussels,
Belgium.

Graham Neubig and Junjie Hu. 2018. Rapid adapta-
tion of neural machine translation to new languages.
In Proceedings of the 2018 Conference on Empiri-
cal Methods in Natural Language Processing, pages
875–880, Brussels, Belgium. Association for Com-
putational Linguistics.

Toan Q. Nguyen and David Chiang. 2017. Trans-
fer learning across low-resource, related languages
for neural machine translation. In Proceedings of
the Eighth International Joint Conference on Natu-
ral Language Processing (Volume 2: Short Papers),
pages 296–301, Taipei, Taiwan. Asian Federation of
Natural Language Processing.

Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: A method for automatic eval-
uation of machine translation. In Proceedings of
the 40th Annual Meeting on Association for Com-
putational Linguistics, ACL ’02, pages 311–318,
Stroudsburg, PA, USA. Association for Computa-
tional Linguistics.

Alberto Poncelas, Dimitar Shterionov, Andy Way,
Gideon Maillette de Buy Wenniger, and Peyman
Passban. 2018. Investigating backtranslation in neu-
ral machine translation. CoRR, abs/1804.06189.

Rico Sennrich, Alexandra Birch, Anna Currey, Ulrich
Germann, Barry Haddow, Kenneth Heafield, An-
tonio Valerio Miceli Barone, and Philip Williams.
2017. The university of Edinburgh’s neural MT sys-
tems for WMT17. In Proceedings of the Second

Conference on Machine Translation, pages 389–
399, Copenhagen, Denmark. Association for Com-
putational Linguistics.

Rico Sennrich, Barry Haddow, and Alexandra Birch.
2016. Improving neural machine translation mod-
els with monolingual data. In Proceedings of the
54th Annual Meeting of the Association for Compu-
tational Linguistics (Volume 1: Long Papers), pages
86–96, Berlin, Germany. Association for Computa-
tional Linguistics.

Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob
Uszkoreit, Llion Jones, Aidan N Gomez, Ł ukasz
Kaiser, and Illia Polosukhin. 2017. Attention is all
you need. In I. Guyon, U. V. Luxburg, S. Bengio,
H. Wallach, R. Fergus, S. Vishwanathan, and R. Gar-
nett, editors, Advances in Neural Information Pro-
cessing Systems 30, pages 5998–6008. Curran As-
sociates, Inc.

Xinyi Wang, Hieu Pham, Philip Arthur, and Gra-
ham Neubig. 2019. Multilingual neural machine
translation with soft decoupled encoding. CoRR,
abs/1902.03499.

Zhong Zhou, Matthias Sperber, and Alexander Waibel.
2018. Massively parallel cross-lingual learning in
low-resource target language translation. In Pro-
ceedings of the Third Conference on Machine Trans-
lation: Research Papers, pages 232–243, Belgium,
Brussels. Association for Computational Linguis-
tics.

Barret Zoph, Deniz Yuret, Jonathan May, and Kevin
Knight. 2016. Transfer learning for low-resource
neural machine translation. In Proceedings of the
2016 Conference on Empirical Methods in Natu-
ral Language Processing, pages 1568–1575, Austin,
Texas. Association for Computational Linguistics.

https://www.aclweb.org/anthology/W18-6416
https://www.aclweb.org/anthology/P07-2045
https://www.aclweb.org/anthology/P07-2045
https://doi.org/10.18653/v1/W17-3204
https://doi.org/10.18653/v1/W17-3204
https://workshop2018.iwslt.org/downloads/Proceedings_IWSLT_2018.pdf
https://workshop2018.iwslt.org/downloads/Proceedings_IWSLT_2018.pdf
https://workshop2018.iwslt.org/downloads/Proceedings_IWSLT_2018.pdf
https://www.aclweb.org/anthology/D18-1103
https://www.aclweb.org/anthology/D18-1103
https://www.aclweb.org/anthology/I17-2050
https://www.aclweb.org/anthology/I17-2050
https://www.aclweb.org/anthology/I17-2050
https://doi.org/10.3115/1073083.1073135
https://doi.org/10.3115/1073083.1073135
https://doi.org/10.18653/v1/W17-4739
https://doi.org/10.18653/v1/W17-4739
https://doi.org/10.18653/v1/P16-1009
https://doi.org/10.18653/v1/P16-1009
http://papers.nips.cc/paper/7181-attention-is-all-you-need.pdf
http://papers.nips.cc/paper/7181-attention-is-all-you-need.pdf
http://arxiv.org/abs/1902.03499
http://arxiv.org/abs/1902.03499
https://www.aclweb.org/anthology/W18-6324
https://www.aclweb.org/anthology/W18-6324
https://doi.org/10.18653/v1/D16-1163
https://doi.org/10.18653/v1/D16-1163

