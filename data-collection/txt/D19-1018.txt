



















































Justifying Recommendations using Distantly-Labeled Reviews and Fine-Grained Aspects


Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing
and the 9th International Joint Conference on Natural Language Processing, pages 188–197,
Hong Kong, China, November 3–7, 2019. c©2019 Association for Computational Linguistics

188

Justifying Recommendations using Distantly-Labeled Reviews and
Fine-Grained Aspects

Jianmo Ni, Jiacheng Li, Julian McAuley
University of California, San Diego

{jin018,j9li,jmcauley}@ucsd.edu

Abstract

Several recent works have considered the
problem of generating reviews (or ‘tips’) as a
form of explanation as to why a recommen-
dation might match a user’s interests. While
promising, we demonstrate that existing ap-
proaches struggle (in terms of both quality
and content) to generate justifications that are
relevant to users’ decision-making process.
We seek to introduce new datasets and meth-
ods to address this recommendation justifica-
tion task. In terms of data, we first pro-
pose an ‘extractive’ approach to identify re-
view segments which justify users’ intentions;
this approach is then used to distantly label
massive review corpora and construct large-
scale personalized recommendation justifica-
tion datasets. In terms of generation, we
design two personalized generation models
with this data: (1) a reference-based Seq2Seq
model with aspect-planning which can gen-
erate justifications covering different aspects,
and (2) an aspect-conditional masked language
model which can generate diverse justifica-
tions based on templates extracted from justi-
fication histories. We conduct experiments on
two real-world datasets which show that our
model is capable of generating convincing and
diverse justifications.

1 Introduction

Explaining, or justifying, recommendations to
users has the potential to increase their trans-
parency and reliability. However providing mean-
ingful interpretations remains a difficult task,
partly due to the black-box nature of many rec-
ommendation models, but also because we simply
lack ground-truth datasets specifying what ‘good’
justifications ought to look like.

Previous work has sought to learn user prefer-
ences and writing styles from crowd-sourced re-
views (Dong et al., 2017; Ni and McAuley, 2018)

Review examples:
I love this little stand! The coconut mocha chiller and
caramel macchiato are delicious.
Wow what a special find. One of the most unique and
special date nights my husband and I have had.

Tip examples:
Great food. Nice ambiance. Gnocchi were very good.
I can’t get enough of this place.

Justification examples:
The food portions were huge.
Plain cheese quesadilla is very good and very cheap.

Table 1: In contrast to reviews and tips, we seek to
automatically generate recommendation justifications
that are more concise, concrete, and helpful for deci-
sion making. Examples of justifications from reviews,
tips, and our annotated dataset are marked in bold.

to generate explanations in the form of natural lan-
guage, e.g. generating synthesized reviews simi-
lar to those that users would write about a prod-
uct. However, a large portion of review text (or
text from ‘tips’) is often of little relevance to most
users’ decision making (e.g. they describe verbose
experiences or general endorsements) and may not
be appropriate to use as explanations in terms of
content and language style. As a result, existing
models that learn directly from reviews (or tips)
may not capture crucial information that explains
users’ purchases. Table 1 shows examples of re-
views, tips and ideal justifications. More recently,
there has been work studying the task of tip gener-
ation where tips are concise summaries of reviews
(Li et al., 2017). Though tips are concise and some
subset of them might be suitable as candidates
for recommendation justifications, only a few e-
commerce systems provide tips accompanied with
reviews. Even in systems where tips are available,
the number of tips is usually far smaller than the
number of reviews. These approaches hence suffer
from generalizability issues, especially in settings



189

where user interactions are highly sparse.
On the other hand, generating diverse responses

is essential in personalized content generation sce-
narios such as justification generation. Instead
of always predicting the most popular reasons,
it’s preferable to present diverse justifications for
different users based on their personal interests.
Recent work has shown that incorporating prior
knowledge into generation frameworks can greatly
improve diversity. Prior knowledge could include
story-lines in story generation (Yao et al., 2019),
or historical responses in dialogue systems (We-
ston et al., 2018).

In this work, our goal is to generate convincing
and diverse justifications. To address the challenge
of lacking ground-truth data about ‘good’ justifi-
cations, we propose a pipeline that can identify
justifications from massive corpora of reviews or
tips. We extract fine-grained aspects from justifi-
cations and build user personas and item profiles
consisting of sets of representative aspects. To
improve generation quality and diversity, we pro-
pose two generation models (1) a reference-based
Seq2Seq model with aspect-planning, which takes
previous justifications as a reference and can pro-
duce justifications based on different aspects, and
(2) an aspect-conditional masked language model
that can generate diverse justifications from tem-
plates extracted from previous justifications.

Our contributions are threefold:

• To facilitate recommendation justification gen-
eration, we propose a pipeline to identify justi-
fication candidates and build aspect-based user
personas and item profiles from massive cor-
pora of reviews. With this approach, we are
able to build large-scale personalized justifica-
tion datasets. We use these extractive justifica-
tion segments in the task of explainable recom-
mendation and show that these are better train-
ing sources instead of whole reviews.
• We propose two models based on reference

attention, aspect-planning techniques and a
persona-conditional masked language model.
We show that adding such personalized informa-
tion enables the models to generate justifications
with high quality and diversity.
• We conduct extensive experiments on two real-

world datasets from Yelp and Amazon Clothing.
We provide an annotated dataset about ‘good’
justifications on the Yelp dataset and show that
the binary classifier trained on this dataset gener-

alizes well to the Amazon Clothing dataset. We
study different decoding strategies and compare
their effect on generation performance.

2 Dataset Generation

In this section, we introduce the pipeline to ex-
tract high quality justifications from raw user re-
views. Specifically, our goal here is to identify
review segments that can be used as justifications
and build a personalized justification dataset upon
them. Our pipeline consists of three steps:

1. Annotating a set of review segments with binary
labels, i.e., to determine whether they are ‘good’
or ‘bad’ justifications.

2. Training a classifier on the annotated subset and
applying it to distantly label all the review seg-
ments to extract ‘good’ justifications for each
user and item pair.

3. Applying fine-grained aspect extraction for the
extracted justifications, and building user per-
sonas and item profiles.

2.1 Identifying Justifications From Reviews

The first step is to extract text segments from re-
views that are appropriate to use as justifications.
Instead of a complete sentence or phrase, we de-
fine each segment as an Elementary Discourse
Unit (EDU; (Mann and Thompson, 1988)) which
corresponds to a sequence of clauses. We use the
model of Wang et al. (2018) to obtain EDUs from
reviews. Recent works have shown that EDUs can
improve the performance of document-level sum-
marization (Bhatia et al., 2015) and opinion sum-
marizaiton (Angelidis and Lapata, 2018).

After preprocessing the reviews into EDUs, we
analyzed the linguistic differences between rec-
ommendation justifications and reviews, and built
two rules to filter the segments that are unlikely
to be suitable justifications: (1) segments with
first-person or third-person pronouns, and (2) too
long or short. Next, two expert annotators were
exposed to 1,000 segments among those not fil-
tered out and asked to determine whether they are
‘good’ justifications. Labeling was performed iter-
atively, followed by feedback and discussion, un-
til the quality was aligned between the two an-
notators. At the end of the process, the inter-
annotator agreement for the binary labeling task
(good vs. bad), measured by Cohen’s kappa (Co-
hen, 1960), was 0.927 after alignment. Then, the
annotators further labeled 600 segments. Overall,



190

Method F1 Recall Precision

BOW-Xgboost 0.559 0.679 0.475
CNN 0.644 0.596 0.700
LSTM-MaxPool 0.675 0.703 0.650
BERT 0.747 0.700 0.800
BERT-SA (one epoch) 0.481 0.975 0.320
BERT-SA (three epoch) 0.491 1.000 0.325

Table 2: Performance for classifying review segments
as good or bad for recommendation justification.

24.8% of the segments were labeled good.

2.2 Automatic Classification

Our next step is to propagate labels to the complete
review corpus. Here we adopt BERT (Devlin et al.,
2019) to fine-tune on our classification task, where
a [CLS] token is added to the beginning of each
segment and the final hidden state (i.e., output of
BERT) corresponding to this token is fed into a
linear layer to obtain the binary prediction. Cross
entropy is used as the training loss.

We split the annotated dataset into Train, Dev,
and Test sets with a 0.8/0.1/0.1 ratio, fine-tune the
BERT classifier on the Train set and choose the
best model on the Dev set. After three epochs
of fine-tuning, BERT can achieve an F1-score
of 0.80 on the Test set. We compare the per-
formance of BERT with multiple baseline mod-
els: (1) a XGBoost model which uses Bags-of-
Words as sentence features (2) a convolutional
neural network (CNN) with three convolution lay-
ers and one linear layer (3) a long short-term mem-
ory (LSTM) (Hochreiter and Schmidhuber, 1997)
network with a max-pooling layer, and a linear
layer (4) a BERT sentiment classifier (BERT-SA)
trained on the complete Yelp dataset for one epoch
and three epochs. To obtain the pre-trained word
embeddings for the CNN and LSTM models, we
applied fastText (Bojanowski et al., 2016) on the
Yelp Review dataset. We set the embedding di-
mension to 200 and used default values for other
hyper-parameters.

Table 2 presents results for our binary classifica-
tion task. The BERT classifier has higher F1-score
and precision than other classifiers. The BERT-
SA model after three epochs only achieves an F1-
score of 0.491, which confirms the difference be-
tween sentiment analysis and our good/bad task,
i.e., even if the segment has positive sentiment, it
might be not suitable as a justification.

Yelp
The Tuna is pretty amazing
Appetizers and pasta are excellent here
An excellent selection of both sweet and savory crepes
It was filled with delicious food, fantastic music and
dancing

Amazon-Cloth
The quality of the material is great
Great shirt, especially for the price.
The seams and stitching are really nice
Fit the bill for a Halloween costume.

Table 3: Examples of justifications with fine-grained
aspects in our annotated dataset. The fine-grained as-
pects are italic and underlined.

2.3 Fine-grained Aspect Extraction

Finally, we extract the fine-grained aspects that
each justification covers. Fine-grained aspects
are properties of products that appear among a
user’s opinions. We adopt the method proposed
by Zhang et al. (2014) to build a sentiment lex-
icon which includes a set of fine-grained aspects
from the whole dataset. We then use simple rules
to determine which aspects appear in each justifi-
cation.1 Table 3 presents a set of examples from
our dataset. Each example consists of a justifi-
cation that a user has written about an item, and
multiple fine-grained aspects mentioned in the jus-
tification. Note that we only annotated the Yelp
dataset, trained a classifer on that and applied the
model on both Yelp and Amazon Clothing dataset.
As shown in Table 3, the trained classifier works
well on both datasets.

3 Approach

3.1 Problem Definition

For each user u (or item i), we build a justification
reference D = {d1, . . . , dlr} consisting of justifi-
cations that the user has written (or justifications
about the item) on the training set, where lr is the
maximum number of justifications. We also obtain
a user persona (or item profile) A = {a1, . . . , aK}
based on the fine-grained aspects that the user’s (or
item’s) previous justifications have covered, where
K is the maximum number of aspects.

Given a user u and an item i, as well as the their
justification reference Du and Di, and u’s persona
Au and i’s profile Ai, our target is to predict the

1For each aspect, if its singular or plural exists in the to-
kenized justification, then we consider that this aspect exists
in that justification.



191

User 
Reference

Item 
Reference

Embedding Layer

Fine-grained 
Aspect

RNN Layer

Attention Layer

User 
Representation

Item 
Representation

FA 
Representation

⨁ ⨁ ⨁

Projection Layer

ℎ1 ℎ2 ℎ3 ℎ4 ℎ5

The

food

portions were

<SOS> wereThe

food

portions

huge

Last 
Hidden

ℎ2

P(food)

ℎ2

Figure 1: Structure of the reference-based Seq2Seq model with Aspect Planning

justifications Ju,i = {w1, w2, . . . , wT } that would
explain why item i fits user u’s interests, where T
is the length of the justification.

3.2 Reference-based Seq2Seq Model

Our base model follows the structure of a stan-
dard Seq2Seq (Sutskever et al., 2014) model. Our
framework, called ‘Ref2Seq’, views the historical
justifications of users and items as references and
learns latent personalized features from them. Fig-
ure 1 shows the structure of our Reference-based
Seq2Seq Model. It includes two components: (1)
two sequence encoders that learn user and item
latent representations by taking previous justifica-
tions as references; (2) a sequence decoder incor-
porating representations from users and items to
generate personalized justifications.

Sequence Encoders. Our user encoder and se-
quence encoder share the same structure, which
includes an embedding layer, a two-layer bi-
directional GRU (Cho et al., 2014), and a projec-
tion layer. The inputs are a user (or item) reference
D consisting of a set of historical justifications.
These justifications pass a word embedding layer,
then go through the GRU and yield a sequence of
hidden states e ∈ Rls×lr×n:

E = Embedding(D), e = GRU(E) =
→
e +

←
e ,
(1)

where ls denotes the length of the sequence, n is
the hidden size of the encoder GRU, E ∈ Rls×lr×n
is the embedded sequence representation, and

→
e

and
←
e are the hidden vectors produced by a for-

ward and a backward GRU (respectively).
To combine information from different ‘refer-

ences’ (i.e. justifications), the hidden states are
then projected via a linear layer:

ê =We · e+ be, (2)

where ê ∈ Rls×n is the final output of the encoder,
and We ∈ Rlr , be ∈ R are learned parameters.

Sequence decoder. The decoder is a two-layer
GRU that predicts the target words given a start to-
ken. The hidden state of the decoder is initialized
using the sum of the last hidden state of the user
and item encoders. The hidden state at time-step t
is updated via the GRU unit based on the previous
hidden state and the input word. Specifically:

h0 = e
u
ls + e

i
ls ,ht = GRU(wt,ht−1), (3)

where euls and e
i
ls

are the last hidden states of the
user and item encoder output êu and êi.

To explore the relation between the reference
and generation, we apply an attention fusion layer
to summarize the output of each encoder. For the
user and item reference encoder, the attention vec-
tor is defined as:

a1t =

ls∑
j=1

α1tjej ,

α1tj = exp(tanh(v
1
α
>
(W 1α[ej ;ht] + b

1
α)))/Z,

(4)
where a1t ∈ Rn is an attention vector on the se-
quence encoder at time-step t, α1tj is an attention
score over the encoder hidden state ej and decoder
hidden state ht, and Z is a normalization term.

Aspect-Planning Generation. One of the chal-
lenges for generating justifications is how to im-
prove controllability, i.e., directly manipulate the
content being generated. Inspired by ‘plan-and-
write’ (Yao et al., 2019), we extend the base model
to an Aspect-Planning Ref2Seq (AP-Ref2Seq)
model where we plan a fine-grained aspect before
generation. This aspect planning can be consid-
ered as an extra form of supervision instead of
a hard constraint to make justification generation
more controllable.



192

When generating the justification for user u and
item i, we first provide a fine-grained aspect a as a
plan. The aspect a is fed into the word embedding
layer to obtain the aspect embedding Ea. Then, we
compute the scores between the embedding of the
aspect and the decoder hidden state as:

a2t = α
2
tEa,

α2t = exp(tanh(v
2
α
>
(W 2α[Ea;ht] + b

2
α)))/Z,

(5)
where a2t ∈ Rn is an attention vector and α2t is an
attention score.

The attention vectors a1ut of user u, a
1
it of item i,

and a2t of fine-grained aspect a, are concatenated
with the decoder hidden state at time-step t and
projected to obtain the output word distribution P .
The output probability for word w at time-step t is
given by:

p(wt) = tanh(W1[ht;a
1
ut;a

1
it;a

2
t ] + b1), (6)

where wt is the target word at time-step t. Given
the probability p(wt) at each time step t, the model
is trained using a cross-entropy loss compared
against the ground-truth sequence.

3.3 Aspect Conditional Masked Language
Model

Though Seq2Seq-based models can achieve high
quality output, they often fail to generate diverse
content. Recent works in natural language gener-
ation (NLG) tried to combine generation methods
with information retrieval techniques to increase
the generation diversity (Li et al., 2018; Baheti
et al., 2018). The basic idea follows the paradigm
of retrieve-and-edit—which is to first retrieve his-
torical responses as templates, and then edit the
template into new content. Since our data is anno-
tated with fine-grained aspects, it naturally fits into
this type of retrieve-and-edit paradigm. Mean-
while, masked language models have shown great
performance in language modeling. Recent work
(Wang and Cho, 2019; Mansimov et al., 2019) has
shown that by sampling from the masked language
model (e.g. BERT), it is able to generate coherent
sentences.

Inspired by this work, we want to extend such
an approach into a conditional version—we ex-
plore the use of an Aspect Conditional Masked
Language Model (ACMLM) to generate diverse
personalized justifications. Figure 2 shows the
structure of our Aspect Conditional Masked Lan-
guage Model. For a justification Ju,i that user

u wrote about item i, we adapt the pre-trained
BERT model (Devlin et al., 2019) into an encoder-
decoder network with (1) an aspect encoder which
encodes the user persona and item profile into la-
tent representations and (2) a masked language
model sequence decoder that takes in a masked
justification and predicts the masked tokens.

Aspect Encoder. Our aspect encoder shares
the same WordPiece embeddings (Wu et al., 2016)
as BERT. The encoder feeds the intersection of
fine-grained aspects from the user persona and
item profile Aui = {a1, . . . , aK′} into the em-
bedding layer and obtains the aspect embedding
Aui ∈ RK

′×n, where K ′ is the number of com-
mon fine-grained aspects and n is the dimension
of the WordPiece embeddings.

Masked Language Model Sequence Decoder.
We use the masked language model in the pre-
trained BERT model as our sequence decoder and
add attention over the aspect encoder’s output. As
shown in Figure 2, the input to the decoder is a
masked justification JMu,i = {w1, . . . , wT } with
multiple tokens be replaced as [MASK]. The de-
coder’s output T ∈ RT×n is then fed to the atten-
tion layer to calculate an attention score with the
output of the encoder:

a3t =

K′∑
j=1

α3tjAj ,

α3tj = exp(tanh(v
3
α
>
(W 3α[Aj ;Tt] + b

3
α)))/Z.

(7)
The attention vector a3t is then concatenated

with the decoder hidden state at time-step t and
sent to a linear projection layer to obtain the out-
put word distribution P . The output probability
for word w at time-step t is given by:

p(wt) = tanh(W2[Tt;a
3
t ] + b2) (8)

where wt is the target word at time-step t.
Masking Procedure. The original BERT pa-

per applies a flat rate (15%) to decide whether to
mask a token. Unlike their approach, we adopt a
higher rate to mask fine-grained aspects since they
are more important in justifications. Specifically,
if we encounter a fine-grained aspect, we will re-
place it with a [MASK] token 30% of the time;
while for other words, we will replace them with a
[MASK] token 15% of the time.

During training, the model will only predict
those masked tokens and calculate a cross-entropy
loss on them.



193

Embedding Layer

Fine-grained Aspects
𝐴𝐴𝑢𝑢𝑢𝑢 = [𝐴𝐴𝑢𝑢𝑢𝑢1 , … ,𝐴𝐴𝑢𝑢𝑢𝑢𝑘𝑘

′
]

Attention Layer

and are[MASK] [MASK] top notch

service areand top notchatmosphere

BERT

𝐸𝐸1 𝐸𝐸2 𝐸𝐸3 𝐸𝐸4 𝐸𝐸5 𝐸𝐸6𝐸𝐸𝐶𝐶𝐶𝐶𝐶𝐶

𝑇𝑇1 𝑇𝑇2 𝑇𝑇3 𝑇𝑇4 𝑇𝑇5 𝑇𝑇6𝐶𝐶

[CLS]

Projection Layer P(atmosphere) P(service)

Figure 2: Structure of the Aspect Conditional Masked Language Model

Iter 0 universe [MASK] is extremely friendly and per-
sona ##ble

Iter 5 the [MASK] is extremely friendly and persona
##ble

Iter 10 the [MASK] is extremely friendly and persona
##ble

Iter 15 the staff are extremely cool and persona ##ble
Iter 20 the staff are extra kind , persona ##ble

Table 4: Examples of the generation output of
ACMLM at different iterations.

Generation by Sampling from Masked Tem-
plates. We next discuss how to generate justifi-
cations from the trained ACMLM. We follow the
sampling strategy of Wang and Cho (2019) to gen-
erate justifications. Instead of generating from
a sequence of all [MASK] tokens, we start with
masked templates generated from historical justi-
fications about the target item. These masked tem-
plates include prior knowledge about the item and
can increase the speed of sampling convergence.

Table 4 shows an example of the generation pro-
cess. We initialize the template sequence X0 as
(universe, [MASK], . . . , ##ble) with length T . At
each iteration i, a position ti is sampled uniformly
at random from {1, . . . , T} and the token at ti
(i.e. xiti) of the current sequence X

i is replaced
by [MASK]. After that, we obtain the conditional
probability of xti as

p(xti |Xi\ti =
1

Z(Xi\ti)
exp(1h(xti)

>fθ(X
i
\ti))),

(9)
where 1h(xti) is a one-hot vector with index xti
set to 1, Xi\ti is the sequence we obtain after re-
placing the token at position ti of Xi by [MASK],
fθ(X

i
\ti) is the output after feeding X

i
\ti into

the ACMLM as in Equation (8), and Z is the
normalization term. We then sample x̃ti from
Equation (9), and construct the next sequence by

Xi+1 = (xi1, . . . , x̃ti, . . . , x
i
T ). After repeating

this procedure N times, the final output is consid-
ered as the generation output.2

4 Experiments

4.1 Datasets

With our proposed pipeline (Section 2), we con-
struct two personalized justification datasets from
existing review data—Yelp and Amazon Cloth-
ing.34 We further filter those users with fewer than
five justifications. For each user, we randomly
hold out two samples from all of their justifica-
tions to construct the Dev and Test sets. Table 5
shows the statistics of our two datasets.

4.2 Baselines

For automatic evaluation, we consider three base-
lines: Item-Rand is a baseline which randomly
chooses a justification from the item’s historical
justifications. LexRank is a strong unsupervised
baseline that is widely used in text summarization
(Erkan and Radev, 2004). Given all historical jus-
tifications about an item, LexRank can select one
justification as the summary. We then use that
as the justification for all users. Attr2Seq (Dong
et al., 2017) is a Seq2Seq baseline that uses at-
tributes (i.e. user and item identity) as input.

By default, all models use beam search dur-
ing generation. Recently, there have been works
showing that the generation output of sampling
methods is more diverse and suitable on high-
entropy tasks (Holtzman et al., 2019). To this end,
we explore another decoding strategy— ‘Top-k
sampling’ (Radford et al., 2019) in experiments

2We set N proportional to the length T of the initial
masked template to prevent the generation diverging too
much from the original template.

3https://www.yelp.com/dataset/challenge
4http://jmcauley.ucsd.edu/data/amazon



194

Dataset Train Dev Test # Users # Items # Aspects

Yelp 1,219,962 115,907 115,907 115,907 51,948 2,041
Amazon Clothing 202,528 57,947 57,947 57,947 50,240 581

Table 5: Statistics of our datasets.

Dataset Yelp Amazon Clothing

Model BLEU-3 BLEU-4 Distinct-1 Distinct-2 BLEU-3 BLEU-4 Distinct-1 Distinct-2
Item-Rand 0.440 0.150 2.766 20.151 1.620 0.680 2.400 11.853
LexRank 2.290 0.920 1.738 8.509 3.480 2.250 2.407 14.956
Attr2seq 7.890 0.000 0.049 0.095 1.720 0.560 0.076 0.352
Ref2Seq 4.380 2.450 0.188 1.163 8.780 5.670 0.141 1.240
AP-Ref2Seq 3.390 1.830 0.326 2.094 13.910 12.500 0.557 3.661
Ref2Seq (Top-k) 1.630 0.700 0.818 11.927 3.960 2.130 0.697 10.858
ACMLM 0.700 0.280 1.322 14.319 2.420 1.590 0.942 9.312

Table 6: Performance on Automatic Evaluation.

and include a variant of our model: Ref2Ref (Top-
k).5

For human evaluation, we include two base-
lines: Ref2Seq (Review) and Ref2Seq (Tip), both
of which are the same model as Ref2Seq model
but trained on the original review and tip data,
respectively. Comparisons with these two base-
lines demonstrates that training on our annotated
dataset tends to generate text more suitable as jus-
tifications.

4.3 Implementation Detail

We use PyTorch6 to implement our models.
For Req2Seq and AP-Ref2Seq, we set the hid-

den size and word embedding size as 256. We ap-
ply a dropout rate of 0.5 for the encoder and 0.2
for the decoder. The size of the justification refer-
ence lr is set to 5 and the number of fine-grained
aspects K in the user persona and item profile is
set to 30. We train the model using Adam with
learning rate 2e−4 and stop training either when
it reaches 20 epochs or the perplexity does not im-
prove (on the Dev set). For ACMLM, we build our
model based on the BERT implementation from
HuggingFace.7 We initialize our decoder using
the pre-trained ‘Bert-base’ model and set the max
sequence length to 30. We train the model for 5
epochs using Adam with learning rate 2e−5. For
models using beam search, we set the beam size
as 10. For models using ‘top-k’ sampling, we set

5At each time step, the next word is sampled from the top
k possible next tokens, according to their probabilities.

6http://pytorch.org/docs/master/index.html
7https://github.com/huggingface/pytorch-pretrained-

BERT

Model R I D

Ref2Seq (Review) 3.02 2.39 2.10
Ref2Seq (Tip) 3.25 2.35 2.34
Ref2Seq 3.87 3.13 2.96
Ref2Seq (Top-k) 3.95 3.34 3.39
ACMLM 3.23 3.29 3.42

Table 7: Performance on Human Evaluation, where
R,I,D represents Relevance, Informativeness and
Diversity, respectively.

k to 5. For ACMLM, we use a burn-in step equal
to the length of the initial sequence. Our data and
code are available online.8

4.4 Automatic Evaluation

For automatic evaluation, we use BLEU, Distinct-
1, and Distinct-2 (Li et al., 2015) to measure the
performance of our model. As shown in Table 6,
our reference-based models achieve the highest
BLEU scores on both datasets except for BLEU-
3 on Yelp. This confirms that Ref2Seq is able
to capture user and item content to generate the
most relevant content, compared with unperson-
alized models such as LexRank and personalized
models that do not leverage historical justifications
such as Attr2Seq.

On the other hand, recent works have reported
that models achieving higher diversity scores
will have lower scores on overlap-based metrics
(e.g. BLEU) for open-domain generation tasks
(Baheti et al., 2018; Gao et al., 2018). We make
a similar observation for our personalized justifi-

8https://github.com/nijianmo/recsys justification.git



195

Model Shake Shack Teharu Sushi MGM Grand Hotel

Ground Truth The burger was good The rolls are pretty great , typi-
cal rolls not that many specials

Room was very clean comfort-
able

LexRank A great burger and fries. Sushi ? Great rooms.

Ref2Seq (Review) i love trader joe ’s , i love trader
joe ’s

the food was good and the ser-
vice was great

i love this place ! the food is
always good and the service is
always great

Ref2Seq (Tip) this place is awesome love this place come here

Ref2Seq this place has some of the best
burgers

the sushi is delicious the room was nice

Ref2Seq (Top-k) the fries are amazing fresh and delicious sushi open hotel for hours

ACMLM breakfast sandwiches are over-
all very filling

overall fun experience with half
price sushi

family style dinner , long time
shopping trip to vegas, family
dining , cheap lunch

Table 8: Comparisons of the generated justifications from different models for three businesses on the Yelp dataset.

cation generation task. As shown in Table 6, both
sampling-based methods Ref2Seq (Top-k) and
ACMLM achieve higher Distinct-1 and Distinct-2,
while their BLEU scores are lower than Seq2Seq
based models using beam search. Therefore, we
also perform human evaluation to validate the gen-
eration quality of our proposed methods.

4.5 Human Evaluation

We conduct human evaluation on three aspects:
(1) Relevance measures whether the generated
output contains information relevant to an item;
(2) Informativeness measures whether the gener-
ated justification includes specific information that
is helpful to users; and (3) Diversity measures how
distinct the generated output is compared with
other justifications.

We focus on the Yelp dataset and sample 100
generated examples from each of the five mod-
els as shown in Table 7. Human annotators are
asked to give a score in the range [1,5] (lowest to
highest) for each metric. Each example is rated
by at least three annotators. The results show
that both Ref2Seq (Top-k) and ACMLM achieve
higher scores on Diversity and Informativeness
compared to other models.

4.6 Qualitative Analysis

Here we study the following two qualitative ques-
tions:
RQ1: How do training data and methods affect
generation? As Table 8 shows, models trained on
reviews and tips tend to generate generic phrases
(such as ‘i love this place’) which often do not
include information that helps users to make de-

Dataset Aspects Generated Output

Yelp

dining the dining room is nice
pastry the pastries were pretty good
chicken the chicken fried rice is the best
sandwich the pulled pork sandwich is the

best thing on the menu

product great product , fast shippong
Amazon- price design is nice , good price
Clothing leather comfortable leather sneakers .

classic
walking sturdy , great city walking shoes

Table 9: Generated justifications from AP-Ref2Seq.
The planned aspects are randomly selected from users’
personas.

cisions. Other models trained on the justifica-
tion datasets tend to mention concrete information
(e.g. different aspects). LexRank tends to generate
relevant but short content. Meanwhile, sampling-
based models are able to generate more diverse
content.
RQ2: How does aspect planning affect gener-
ation? To mitigate the trade-off between diver-
sity and relevance, one approach is to add more
constraints during generation such as constrained
Beam Search (Anderson et al., 2017). In our work,
we extend our base model Ref2Seq by incorpo-
rating aspect-planning to guide generation. As
shown in Table 9, most planned aspects are present
in the generated outputs of AP-Req2Seq.

5 Related Work

Explainable Recommendation There has been
a line of work that studies how to improve the



196

explainability of recommender systems. Cather-
ine and Cohen (2017) learn latent representations
of review text to predict ratings. These repre-
sentations are then used to find the most help-
ful reviews for given a particular user and item
pair. Another popular direction is to generate text
to justify recommendations. Dong et al. (2017)
proposed an attribute-to-sequence model to gener-
ate product reviews which utilizes categorical at-
tributes. Ni et al. (2017) developed a multi-task
learning method that considers collaborative fil-
ter and review generation. Li et al. (2019b) gen-
erated tips by considering ‘persona’ information
which can capture the language style of users and
characteristics of items. However, these works use
whole reviews or tips as training examples, which
may not be appropriate due to the quality of re-
view text. More recently, Liu et al. (2019) pro-
posed a framework to generate fine-grained expla-
nations for text classification. To achieve labels for
human-readable explanations, they constructed a
dataset from a website which provides ratings and
fine-grained summaries written by users. Unfor-
tunately, most websites do not provide such fine-
grained information. On the other hand, our work
identifies justifications from reviews, uses them as
training examples and shows these are better data
source for explainable recommendation via exten-
sive experiments.

Diversity-aware NLG Diversity is an important
aspect of NLG systems. Recent works have fo-
cused on digesting prior knowledge to improve
generation diversity. Yao et al. (2019) proposed a
method to incorporate planned story-lines in story
generation. Li et al. (2019a) developed an aspect-
aware coarse-to-fine review generation method.
They predict an aspect for each sentence in the
review to capture the content flow. Given the as-
pects, a sequence of sentence sketches is gener-
ated and a decoder will fill in the slots of each
sketch. In dialogue systems, several works have
studied frameworks to extract templates from his-
torical responses, which are then edited to form
new responses (Weston et al., 2018; Wu et al.,
2018). Similarly, the extract-and-edit paradigm
has been studied in style transfer tasks in NLG
(Li et al., 2018). Wu et al. (2019) proposed an
attribute aware masked language model for non-
parallel sentiment transfer. They first mask out the
sentimental tokens and then train a masked lan-
guage model to infill the masked positions for tar-

get sentiment. In this work, we also introduces a
conditional masked language model but considers
more fine-grained aspects.

6 Conclusion

In this work, we studied the problem of person-
alized justification generation. To build high qual-
ity justification datasets, we provided an annotated
dataset and proposed a pipeline to extract justifi-
cations from massive review corpora. To gener-
ate convincing and diverse justifications, we de-
veloped two models: (1) Ref2Seq which lever-
ages historical justifications as references dur-
ing generation; and (2) ACMLM, which is an
aspect conditional model built on a pre-trained
masked language model. Our experiments showed
that Ref2Seq achieves higher scores (in terms of
BLEU) and ACMLM achieves higher diversity
scores compared with baselines. Human evalu-
ation showed that reference-based models obtain
high relevance scores and sampling based methods
led to more diverse and informative outputs. Fi-
nally, we showed that aspect-planning is a promis-
ing way to guide generation to produce personl-
ized and relevant justifications.
Acknowledgements. This work is partly sup-
ported by NSF #1750063. We thank all the re-
viewers for their constructive suggestions.

References
Peter Anderson, Basura Fernando, Mark Johnson, and

Stephen Gould. 2017. Guided open vocabulary im-
age captioning with constrained beam search. In
EMNLP.

Stefanos Angelidis and Mirella Lapata. 2018. Summa-
rizing opinions: Aspect extraction meets sentiment
prediction and they are both weakly supervised. In
EMNLP.

Ashutosh Baheti, Alan Ritter, Jiwei Li, and William B.
Dolan. 2018. Generating more interesting responses
in neural conversation models with distributional
constraints. In EMNLP.

Parminder Bhatia, Yangfeng Ji, and Jacob Eisenstein.
2015. Better document-level sentiment analysis
from rst discourse parsing. In EMNLP.

Piotr Bojanowski, Edouard Grave, Armand Joulin, and
Tomas Mikolov. 2016. Enriching word vectors with
subword information. Transactions of the Associa-
tion for Computational Linguistics, 5:135–146.

Rose Catherine and William W. Cohen. 2017.
Transnets: Learning to transform for recommenda-
tion. In RecSys.



197

Kyunghyun Cho, Bart van Merrienboer, aglar Gülehre,
Fethi Bougares, Holger Schwenk, and Yoshua Ben-
gio. 2014. Learning phrase representations using
rnn encoder-decoder for statistical machine transla-
tion. In EMNLP.

Jacob Willem Cohen. 1960. A coefficient of agreement
for nominal scales.

Jacob Devlin, Ming-Wei Chang, Kenton Lee, and
Kristina Toutanova. 2019. Bert: Pre-training of deep
bidirectional transformers for language understand-
ing. In NAACL.

Li Dong, Shaohan Huang, Furu Wei, Mirella Lapata,
Ming Zhou, and Ke Xu. 2017. Learning to generate
product reviews from attributes. In EACL.

Günes Erkan and Dragomir R. Radev. 2004. Lexrank:
Graph-based lexical centrality as salience in text
summarization. J. Artif. Intell. Res., 22:457–479.

Jun Gao, Wei Bi, Xiaojiang Liu, Junhui Li, and Shum-
ing Shi. 2018. Generating multiple diverse re-
sponses for short-text conversation. In AAAI.

Sepp Hochreiter and Jürgen Schmidhuber. 1997. Long
short-term memory. Neural Computation, 9:1735–
1780.

Ari Holtzman, Jan Buys, Maxwell Forbes, and Yejin
Choi. 2019. The curious case of neural text degen-
eration. CoRR, abs/1904.09751.

Jiwei Li, Michel Galley, Chris Brockett, Jianfeng Gao,
and William B. Dolan. 2015. A diversity-promoting
objective function for neural conversation models.
In HLT-NAACL.

Juncen Li, Robin Jia, He He, and Percy S. Liang. 2018.
Delete, retrieve, generate: A simple approach to sen-
timent and style transfer. In NAACL-HLT.

Junyi Li, Wayne Xin Zhao, Ji-Rong Wen, and Yang
Song. 2019a. Generating long and informative re-
views with aspect-aware coarse-to-fine decoding. In
ACL.

Piji Li, Zihao Wang, Lidong Bing, and Wai Lam.
2019b. Persona-aware tips generation. In WWW.

Piji Li, Zihao Wang, Zhaochun Ren, Lidong Bing, and
Wai Lam. 2017. Neural rating regression with ab-
stractive tips generation for recommendation. In SI-
GIR.

Hui Liu, Qingyu Yin, and William Yang Wang. 2019.
Towards explainable nlp: A generative explanation
framework for text classification. In ACL.

William C. Mann and Sandra A. Thompson. 1988.
Rhetorical structure theory: toward a functional the-
ory of text.

Elman Mansimov, Alex Wang, and Kyunghyun Cho.
2019. A generalized framework of sequence genera-
tion with application to undirected sequence models.
ArXiv, abs/1905.12790.

Jianmo Ni, Zachary C. Lipton, Sharad Vikram, and Ju-
lian J. McAuley. 2017. Estimating reactions and rec-
ommending products with generative models of re-
views. In IJCNLP.

Jianmo Ni and Julian McAuley. 2018. Personalized re-
view generation by expanding phrases and attending
on aspect-aware representations. In ACL.

Alec Radford, Jeff Wu, Rewon Child, David Luan,
Dario Amodei, and Ilya Sutskever. 2019. Language
models are unsupervised multitask learners.

Ilya Sutskever, Oriol Vinyals, and Quoc V. Le. 2014.
Sequence to sequence learning with neural net-
works. In NIPS.

Alex Wang and Kyunghyun Cho. 2019. Bert has a
mouth, and it must speak: Bert as a markov random
field language model. CoRR, abs/1902.04094.

Yizhong Wang, Sujian Li, and Jingfeng Yang. 2018.
Toward fast and accurate neural discourse segmen-
tation. In EMNLP.

Jason Weston, Emily Dinan, and Alexander H. Miller.
2018. Retrieve and refine: Improved sequence gen-
eration models for dialogue. In SCAI@EMNLP.

Xing Wu, Tao Zhang, Liangjun Zang, Jizhong Han,
and Songlin Hu. 2019. Mask and infill: Applying
masked language model to sentiment transfer. In IJ-
CAI.

Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V.
Le, Mohammad Norouzi, Wolfgang Macherey,
Maxim Krikun, Yuan Cao, Qin Gao, Jeff Klingner,
Apurva Shah, Melvin Johnson, Xiaobing Liu,
Lukasz Kaiser, Stephan Gouws, Yoshikiyo Kato,
Taku Kudo, Hideto Kazawa, Keith Stevens, George
Kurian, Nishant Patil, Wei Wang, Cliff Young, Jason
Smith, Jason Riesa, Alex Rudnick, Oriol Vinyals,
Gregory S. Corrado, Macduff Hughes, and Jeffrey
Dean. 2016. Google’s neural machine translation
system: Bridging the gap between human and ma-
chine translation. CoRR, abs/1609.08144.

Yu Wu, Furu Wei, Shaohan Huang, Yunli Wang, Zhou-
jun Li, and Ming Zhou. 2018. Response generation
by context-aware prototype editing. In AAAI.

Lili Yao, Nanyun Peng, Ralph Weischedel, Kevin
Knight, Dongyan Zhao, and Rui Yan. 2019. Plan-
and-write: Towards better automatic storytelling. In
AAAI.

Yongfeng Zhang, Guokun Lai, Min Zhang, Yi Zhang,
Yiqun Liu, and Shaoping Ma. 2014. Explicit fac-
tor models for explainable recommendation based
on phrase-level sentiment analysis. In SIGIR.


