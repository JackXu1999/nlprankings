



















































Concept Pointer Network for Abstractive Summarization


Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing
and the 9th International Joint Conference on Natural Language Processing, pages 3076–3085,
Hong Kong, China, November 3–7, 2019. c©2019 Association for Computational Linguistics

3076

Concept Pointer Network for Abstractive Summarization

Wang Wenbo1, Gao Yang1,2∗ , Huang Heyan1,2 and Zhou Yuxiang1
1 School of Computer Science and Technology, Beijing Institute of Technology

2 Beijing Engineering Research Center of High Volume Language
Information Processing and Cloud Computing Applications

gyang@bit.edu.cn

Abstract

A quality abstractive summary should not only
copy salient source texts as summaries but
should also tend to generate new conceptual
words to express concrete details. Inspired
by the popular pointer generator sequence-to-
sequence model, this paper presents a concept
pointer network for improving these aspects of
abstractive summarization. The network lever-
ages knowledge-based, context-aware concep-
tualizations to derive an extended set of candi-
date concepts. The model then points to the
most appropriate choice using both the con-
cept set and original source text. This joint ap-
proach generates abstractive summaries with
higher-level semantic concepts. The train-
ing model is also optimized in a way that
adapts to different data, which is based on a
novel method of distantly-supervised learning
guided by reference summaries and testing set.
Overall, the proposed approach provides sta-
tistically significant improvements over sev-
eral state-of-the-art models on both the DUC-
2004 and Gigaword datasets. A human evalu-
ation of the model’s abstractive abilities also
supports the quality of the summaries pro-
duced within this framework.

1 Introduction

Abstractive summarization (ABS) has gained
overwhelming success owing to a tremendous
development of sequence-to-sequence (seq2seq)
model and its variants (Rush et al., 2015; Chopra
et al., 2016; Paulus et al., 2017; Guo et al., 2018;
Gao et al., 2019). In tandem with seq2seq mod-
els, pointer generator was developed by See et al.
(2017) as a solution to tackle the rare words and
out-of-vocabulary (OOV) problem associated with
generative-based models. The idea behind is to
use attention as a pointer to determine the prob-
ability of generating a word from both a vocab-

∗Corresponding author

Figure 1: “summary1” only copies keyword from the
source text, while “summary2” generates new concepts
to convey the meaning.

ulary distribution and the source text. Pointer
generator networks have also been extensively ac-
cepted by the ABS community due to their effi-
cacy with long document summaries (Chen and
Bansal, 2018; Hsu et al., 2018), title summariza-
tion (Sun et al., 2018), etc.

However, the current power of abstractive sum-
marization falls short of their potential. As the ex-
ample in Figure 1 shows, a seq2seq model with a
pointer mechanism (marked as the direct pointer)
is likely to merely copy parts of the original text to
form a summary using keywords and phrases, such
as “317 athletes”. Conversely, a more human-
like summary would be based on one’s own un-
derstanding of the detail in the words, expressed
as higher-level concepts drawn from world knowl-
edge—like using the word “group” to replace
“athletes and officials”. This indicates that a good
summary should not simply copy original mate-
rial, it should also generate new and even abstract
concepts that reflect high-level semantics.

Therefore, a pointer generator network that
solely considers the source material to generate
a summary does not adequately satisfy the needs
of high-quality abstractive summarization. We
argue that concepts have a greater ability to ex-
press deeper meanings than verbatim words. As
such, it is essential to explore the potential of us-



3077

ing concepts from world knowledge to assist with
abstractive summarization. Our developed model
not only points to informative source texts but also
leverages conceptual words from human knowl-
edge in the summaries it generates.

Hence, in this paper, we propose a novel model
based on a concept pointer generator that encour-
ages the generation of conceptual and abstract
words. As a hidden benefit, the model also alle-
viates the OOV problems. Our model uses pointer
network to capture the salient information from a
source text, and then employs another pointer to
generalize the detailed words according to their
upper level of expressions. Finally, the output
is also consistent with language model by the
seq2seq generator. Unique to our concept pointer
is a set of concept candidates particular for a word
that is drawn from a huge knowledge base. The
set of candidates adheres to a concept distribution,
where the probability of each concept being gen-
erated is linked to how strongly the candidate rep-
resents each word. Moreover, the concept distri-
bution is iteratively updated to better explain the
target word given the context of the source mate-
rial and inherent semantics in the texts. Hence, the
learned concept pointer points to the most suitable
and expressive concepts or words. The optimiza-
tion function is adaptive so as to cater for different
datasets with distantly-supervised training. The
network is then optimized end-to-end using rein-
forcement learning, with the distant-supervision
strategy as a complement to further improve the
summary.

Overall, the contributions of this paper are: 1)
a novel concept pointer generator network that
leverages context-aware conceptualization and a
concept pointer, both of which are jointly in-
tegrated into the generator to deliver informa-
tive and abstract-oriented summaries; 2) a novel
distant supervision training strategy that favors
model adaptation and generalization, which re-
sults in performance that outperforms the well-
accepted evaluation-based reinforcement learning
optimization on a test-only dataset in terms of
ROUGE metrics; 3) a statistical analysis of quan-
titative results and human evaluations from com-
parative experiments with several state-of-the-art
models that shows the proposed method provides
promising performance.

2 Related Work

Abstractive summarization supposedly digests and
understands the source content and, consequently,
the generated summaries are typically a reorgani-
zation of the wording that sometimes form new
sentences. Historically, abstractive summariza-
tion has been performed through rule-based sen-
tence selection (Dorr et al., 2003), key informa-
tion extraction (Genest and Lapalme, 2011), syn-
tactic parsing (Bing et al., 2015) and so on. How-
ever, more recently, seq2seq models with attention
have played a more dominant role in generating
abstractive summaries (Rush et al., 2015; Chopra
et al., 2016; Nallapati et al., 2016; Zhou et al.,
2017). Extensions to the seq2seq approach include
an intra-decoder attention (Paulus et al., 2017) and
coverage vectors (See et al., 2017) to decrease rep-
etition in phrasing. Copy mechanism (Gu et al.,
2016) has been integrated into these models to
tackle OOV problem. Zhou et al. (2018) went
on to propose SeqCopyNet which copies complete
sequences from an input sentence to further main-
tain the readability of the generated summary.

Pointer mechanism (Vinyals et al., 2015) has
drawn much attention in text summarization (See
et al., 2017), because this technique not only pro-
vides a potential solution for rare words and OOV
but also extends abstractive summarization in a
flexible way (Çelikyilmaz et al., 2018). Further,
pointer generator models can effectively adaptive
to both extractor and abstractor networks (Chen
and Bansal, 2018), and summaries can be gen-
erated by incorporating a pointer-generator and
multiple relevant tasks (Guo et al., 2018), such
as question or entailment generation, or multiple
source texts (Sun et al., 2018).

However, work particularly targets the problem
of the abstraction is rare. Abstract Meaning Rep-
resentation (AMR) is used to transform a sentence
into a concept graph, then merge those similar
concept nodes to form a new summary graph (Liu
et al., 2018). Concepts are also incorporated as
auxiliary features (Guo et al., 2017). Kryscinski
et al. (2018) and Weber et al. (2018) define the
number of new n-grams as the primary criteria of
abstractiveness. This makes sense in most cases.
But, we believe that abstraction means summariz-
ing detailed content with higher-level semantically
related concepts, which has motivated the devel-
opment of the model proposed in this paper.



3078

3 The Proposed Model

Neural abstractive summarization can be de-
scribed as a generation process where a sequen-
tial input is summarized into a shorter sequential
output through a neural network. Suppose that the
sequential input x = {x1, . . . , xi, . . . , xn} is a se-
quence of n number of words, and i is the index of
the input. The shorter (i.e., summarized) sequence
of output is denoted as y = {y1, . . . , yt, . . . , ym}
with number of m words, and t indicates a time
step. As Figure 2 shows, our model consists of
two sub-modules —an encoder-decoder module
and the proposed concept pointer generator mod-
ule.

3.1 Encoder-Decoder Framework

This process is formulated as an encoder-decoder
framework that consists of an encoder and an
attention-equipped decoder. We use a two-layer
bi-directional LSTM-RNN encoder and one-layer
uni-directional LSTM-RNN decoder along with
attention mechanism.

Formally, the encoder produces sequential hid-
den states as (

−→
h 1, . . . ,

−→
h n) and (

←−
h 1, . . . ,

←−
h n) in

the corresponding positions, and the bi-directional
hi = fLSTM (hi−1, xi). Each word xi in the
sequence can be represented as a concatenation
of the bi-directional hidden states, i.e., hi =
[
−→
h i,
←−
h i]. The decoder generates a target sum-

mary from a vocabulary distribution Pvocab(w),
which is based on a context vector h∗t through the
following process:

Pvocab(w) = P (yt|y<t,x; θ)
=sfm(W 2(W 1[st,h∗t ] + b1) + b2)

(1)

where st is the hidden state of the decoder at time
step t , and h∗t is the context vector at time step
t. W 1,W 2, b1, b2 are trainable parameters, and
sfm(·) is short for softmax function.

The context vector h∗t is computed by a
weighted sum of the hidden representations of the
source text, and the weight is denoted as the atten-
tion αt,i.

h∗t =
∑n

i=1
αt,ihi

αt,i = sfm(v> tanh(W hhi +W sst + b))
(2)

The softmax function normalizes the vector
of a distribution over the input position, and
v,W h,W s, b are trainable parameters.

3.2 Concept Pointer Generator
Pointer networks use attention as a pointer to se-
lect segments of the input as outputs (Vinyals
et al., 2015). As such, a pointer network is a
suitable mechanism for extracting salient informa-
tion, while remaining flexible enough to interface
with a seq2seq model for generating an abstractive
summarization (See et al., 2017). Our proposed
model is essentially an upgrade to this configura-
tion that integrates a new concept pointer network
within a unified framework.

3.2.1 Context-aware Conceptualization
“Understanding” the instances of a word requires
a taxonomic knowledge base that relates those
words to a concept space. In our model, we use
an isA taxonomy, called the Microsoft Concept
Graph1 (Wang et al., 2015), to serve this purpose
for two reasons (Wang and Wang, 2016). First,
this graph provides a huge concept space with
multi-word terms that cover concepts of worldly
facts as concepts, instances, relationships, and val-
ues2. Second, the relationships between concepts
and entities are probabilistic as a measure of how
strongly they are related. Moreover, the proba-
bilities are trustworthy given they have been de-
rived from evidence found in billions of webpages,
search log data, and other existing taxonomies.
Our model is data-driven and, therefore, is more
easily adaptable with probabilities. All these char-
acteristics make the Microsoft Concept Graph a
suitable choice for our model. More detailed ex-
amples are available in Appendix A.

The concept graph specifies the probability that
each instance x belongs to a concept c, p(c|x).
Given a word x, we have a distribution over a set
of related concepts. Yet, this raises the question of
how to identify a context-appropriate concept for a
word from the distributional set of candidate con-
cepts. For instance, apple in the context of “an ap-
ple is good for you health” tends to be associated
with the concept of fruit instead of company. For-
mally, given a word xi in a training sentence, a set
of k concept candidates, Ci = {c1i , c2i , · · · , cki }, is
linked to the word from the knowledge base, with
distributional probabilities over the concepts, i.e.,

1The Microsoft concept graph was derived from
Probase project. The public data can be downloaded
via the provided API: https://concept.research.
microsoft.com/Home/API

2The current version is mined from billions of web pages,
containing 5.3 M unique concepts, 12.5 M unique entities and
85 M isA relations.

https://concept.research.microsoft.com/Home/API
https://concept.research.microsoft.com/Home/API


3079

Figure 2: The architecture of our model. Blue bar represents the attention distribution over the inputs. Purple bar
represents the concept distribution over the inputs. Noted that, this distribution can be sparse since not every word
has its upper concept. Green bar represents the vocabulary distribution generated from seq2seq component.

P (C|xi) = {p(c1i ), p(c2i ), · · · , p(cki )}. The task
is to find the most suitable concept cji to fit the
updated context, represented by the vector h∗t in
Equation (2), at each time step t.

In the case of generating summaries given up-
dated contexts, a weighted update of the distribu-
tional concept candidates needs to be performed.
In the model, the updated weight, denoted as βji ,
is estimated by a softmax classifier that is jointly
conditioned on the hidden representation of the
word hi, the context vectors h∗t , and each of con-
cept vectors:

βji = softmax(W h′ [hi,h
∗
t , c

j
i ]) (3)

where j ∈ [1, k], W h′ is a trainable parameter,
and cji is the vector of the jth concept candidate,
which is a representation of the input embeddings.

Together with the concept probability from the
existing knowledge base p(cji ) and the updated
weights based on the contexts βji , a context-aware
conceptualized probability of jth concept for the
word xi, P ci,j , is finally estimated as

P ci,j = p(c
j
i ) + γβ

j
i (4)

where γ is a tunable parameter. Theoretically, we
will end up with a number of k relevant concepts
for each word Ci = {c1i , · · · , cki } with a proba-
bility distribution over the set, which is learned as
P ci = {P ci,1, · · · , P ci,j , · · · , P ci,k}.

3.2.2 Concept Pointer Generator
The basic pointer generator network contains two
sub-modules, one is the pointer network and the

other is the generation network. These two sub-
modules jointly determine the probabilities of the
words in the final generated summary. The gener-
ation probability pgen for the generation network
(See et al., 2017) is learned by

pgen = σ(W h∗h
∗
t +W sst +W yyt−1 + bgen)

(5)
where σ is a sigmoid function.

For the pointer network, our model consists of
a pointer to the source text and a further concept
pointer to the relevant concepts that have arisen
from the source content. These two separate point-
ers are calculated as follows. The first pointer is
taken based on the attention distribution αt,i over
the source text. The second concept pointer is op-
erated over a concept distribution of the source text
that is scaled element-wise by the attention distri-
bution.

To train the model, given the likelihood of each
concept in the current context, the updates could
be performed in two ways. In a hard assignment,
the concept that receives the highest score would
be selected for the update:

P̂ ci argmax = P
c
i,a,where a = argmax

j
(βji ) (6)

where a is the index of maximized generated
weight based on the contexts, and P ci,a is obtained
by Eqs. (4).

In random selection, each of the concept can-
didates could be trained randomly to update the
parameters:

P̂ ci random = P
c
i,j ∼ PCi (7)



3080

where j represents the selected concept index.
Considering the above baseline generation net-
work and both the pointer networks, our final out-
put distribution is

Pfinal(w) = pgenPvocab(w) + (1− pgen)

(
∑

i:wi=w
αt,i +

∑
i:wi=w

αt,i × P̂ ci )
(8)

where P̂ ci can be updated by P̂
c
i argmax, or

P̂ ci random. The difference between these two
choices is demonstrated in the Experiments sec-
tion.

3.3 Objective Learning
3.3.1 Basic MLE
The baseline objective is derived by maximiz-
ing the likelihood training for the seq2seq gen-
eration, given a reference summary y∗ =
{y∗1, y∗2, · · · , y∗m′} for document x. The train-
ing objective is to minimize the negative log-
likelihood of the target word sequence:

LMLE = −
∑m′

t=1 logP (y
∗
t |y∗1, · · · , y∗t−1,x)

(9)

3.3.2 Evaluation based Reinforcement
Learning (RL)

Similar to Paulus et al. (2017), policy gradient
methods can directly optimize discrete target eval-
uation metrics, such as ROUGE. The basic idea is
to explore new sequences and compare them to the
best greedy baseline sequence. Once the baseline
sequence ŷ, or sampled sequence ys, are gener-
ated, they are compared against the reference se-
quence y∗ to compute the rewards r(ŷ) and r(ys),
respectively. In the RL training stage, two sep-
arate output candidates at each time step are pro-
duced: ys is sampled from the probability distribu-
tion P (yst |ys1, · · · , yst−1, x) , and ŷ is the baseline
output. The training objective is then to minimize

LRL = (r(ŷ)− r(ys))
∑m′

t=1 logP (y
s
t |ys1, · · · , yst−1, x)

(10)
It is noteworthy that the samples ys are selected

from a wide range of vocabularies extended by all
the concept candidates. This strategy ensures that
the model learns to generate sequences with higher
rewards by better exploring a set of close concepts.

Thus, the combination of these two objectives
yield improved task-specific scores while cater-
ing a better language model: Lfinal = λLRL +
(1 − λ)LMLE , where λ is a soft-switch between

the two objectives. The model is pre-trained with
MLE loss, then switch to the final loss.

3.3.3 Distant Supervision (DS) for Model
Adaption

Our intuition is that, if the summary-document
pairs are dissimilar to the testing set, the model
could be retrained to adapt to weaken the influence
of the dissimilarity on the final loss. The result
would be a training model that better fits the spe-
cific testing data. The challenge is that there are
no explicit supervision labels to indicate whether
the training set is close to the testing set, so a new
training paradigm is needed. In answer to this
need and also to provide end-to-end functional-
ity in the model, we developed a simple approach
for labeling summary-document pairs by calculat-
ing the Kullback-Leibler (KL) divergence between
each training reference summary and a set of test-
ing documents. In this way, the training pairs are
distantly-labelled for training the model.

Specifically, the representations of the refer-
ence summaries and the testing set are computed
by summing all the involved word embeddings.
Given a testing document xdl , where l ∈ [1, Nd]
andNd is the size of the testing corpus, the vector-
based representation of one document is xdl =
exp(

∑n′
i=1 x

d
i ), where n

′ is the number of docu-
ment words involved. The reference summary is
represented by y∗ = exp(

∑m′
t=1 y

∗
t ). We nor-

malize these vectors through a softmax function
to cater for KL calculation. The model adaption
with the distant labels is defined as:

LDS = (π − 1Nd
∑Nd

l=1DKL(y
∗,xdl ))LMLE

(11)
where DKL(.) indicates the KL divergence be-
tween y∗ and xdl , and π is a constant parame-
ter that is tuned via adaption to the testing set.
The divergences are averaged within the testing
set, which indicates the overall distances between
testing set and each of the reference summary-
document pairs. In this way, the samples in the
training corpus are distantly annotated as either
relevant or irrelevant for model adaption, noting
that the model is pre-trained with the MLE loss
before switching to distantly-supervised training.

4 Experiments

Datasets: To evaluate the effectiveness of our
proposed model, we conducted training and test-



3081

Table 1: ROUGE F1 evaluation results on the Gigaword and ROUGE recall on DUC-2004 test set. The results with
† mark are taken from the corresponding papers. Underlined scores are the best without additional optimization.
Bold scores are the best between the two optimization strategies. ? mark indicates the improvements from the
baselines to the concept pointer are statistically significant using a two-tailed t-test (p < 0.01).

Models Gigaword DUC-2004
RG-1 RG-2 RG-L RG-1 RG-2 RG-L

ABS+† (Rush et al., 2015) 29.76 11.88 26.96 28.18 8.46 23.81
Luong-NMT† (Luong et al., 2015) 33.10 14.45 30.71 28.55 8.79 24.43
RAS-Elman† (Chopra et al., 2016) 33.78 15.97 31.15 28.97 8.26 24.06
lvt5k-lsent† (Nallapati et al., 2016) 35.30 16.64 32.62 28.61 9.42 25.24
SEASS† (Zhou et al., 2017) 36.15 17.54 33.63 29.21 9.56 25.51
Seq2seq+att ?(our impl.) 33.11 14.67 31.06 28.57 9.31 24.81
Pointer-generator ?(our impl.) (See et al., 2017) 35.98 15.99 33.33 28.28 10.04 25.69
Pointer-Cov.-Entail.-Quest.† (Guo et al., 2018) 35.98 17.76 33.63 - - -
Seq2seq-Sel.-MTL-ERAM† (Li et al., 2018) 35.33 17.27 33.19 29.33 10.24 25.24
CGU† (Lin et al., 2018) 36.3 18.0 33.8 - - -
Concept pointer 36.62 16.40 33.98 29.17 10.38 26.34
Concept pointer+RL 38.02 16.97 35.43 29.34 9.84 26.60
Concept pointer+DS 37.01 17.10 34.87 30.39 10.78 27.53

ing on two popular datasets. The first was the En-
glish Gigaword Fifth Edition corpus (Parker et al.,
2011). We replicated the pre-processing steps
in (Rush et al., 2015) to obtain the same train-
ing/testing data. After pre-processing, the corpus
contained about 3.8M sentence-summary pairs as
training set and 189K pairs as the development
set. Once pairs with empty titles were removed,
the testing set numbered 1951 pairs. The second
dataset, DUC2004, was only used for testing. This
dataset consists of 500 document-headline sum-
mary pairs, where each document is paired with
four reference summaries written by humans.

Evaluation Metrics: We used ROUGE (Lin,
2004) as the evaluation metric, which measures
the quality of a summary by computing the over-
lapping lexical elements between the candidate
summary and a reference summary. Following
previous practice, we assessed RG-1 (unigram),
RG-2 (bigram) and RG-L (longest common subse-
quence - LCS). Noted that the English Gigaword3

testing set contains references of different lengths,
while the DUC-20044 testing set fixes the sum-
mary length to 75 bytes.

Training Setups: We initialize word embed-
dings with 128-d vectors and fine-tune them dur-
ing training. Concepts share the same embeddings

3The ROUGE evaluation option is, -m -n 2 -s
4The ROUGE evaluation option is, -n 2 -m -b 75 -s

with the words. The vocabulary size was set to
150k for both the source and target text. The hid-
den state size was set to 256. The vocabulary
size is increased from around 602 to 2216 con-
cepts w.r.t the different number (k = 1, · · · , 5) of
concept candidates for each word. Note that the
generated concepts with UNKs were subsequently
deleted. Our code is available on https://
github.com/wprojectsn/codes, and the
vocabularies and candidate concepts are also in-
cluded.

We trained our models on a single GTX TI-
TAN GPU machine. We used the Adagrad opti-
mizer with a batch size of 64 to minimize the loss.
The initial learning rate and the accumulator value
were set to 0.15 and 0.1, respectively. We used
gradient clipping with a maximum gradient norm
of 2. At the time of decoding, the summaries were
produced through a beam search of size 8. The
hyper-parameter settings were λ = 0.99, γ = 0.1,
π = 2.92 on DUC-2004 and π = 1.68 on Giga-
word. We trained our concept pointer generator
for 450k iterations yielded the best performance,
then took the optimization using RL rewards for
RG-L at 95K iterations on DUC-2004 and at 50K
iterations on Gigaword. We took the distance-
supervised training at 5K iterations on DUC-2004
and at 6.5K iterations on Gigaword.

Baselines: The following state-of-the-art base-
lines were used as comparators. ABS+ (Rush

https://github.com/wprojectsn/codes
https://github.com/wprojectsn/codes


3082

Table 2: OOV problem analysis: percentages (%(NO.
UNK/NO. all generated words)) of generating UNK
w.r.t the following three models on Gigaword and
DUC-2004 datasets.

Method Gigaword DUC-2004
seq2seq+att 4.02%(570/14183) 2.08%(85/4079)
Pointer generator 1.16%(207/17859) 0.31% (16/5140)
Concept pointer 1.12%(202/17950) 0.23%(12/5230)

et al., 2015) is a tuned ABS model with addi-
tional features. Luong-NMT (Luong et al., 2015)
is a two-layer LSTM encoder-decoder. RAS-
Elman (Chopra et al., 2016) is a convolution en-
coder and an Elman RNN decoder with atten-
tion. Seq2seq+att is two-layer BiLSTM encoder
and one-layer LSTM decoder equipped with at-
tention. lvt5k-lsent (Nallapati et al., 2016) uses
temporal attention to keep track of the past atten-
tive weights of the decoder and restrains the rep-
etition in later sequences. SEASS (Zhou et al.,
2017) includes an additional selective gate to con-
trol information flow from the encoder to the de-
coder. Pointer-generator (See et al., 2017) is
an integrated pointer network and seq2seq model.
We implemented this baseline without its coverage
mechanism since this is not our focus. Baseline
models also include two pointer-generator based
extensions (Guo et al., 2018; Li et al., 2018). CGU
(Lin et al., 2018) sets a convolutional gated unit
and self-attention for global encoding.

5 Results and Analysis

The following analysis focuses on investigating
whether our model is, first, able to generate ab-
stract and new concepts, and, second, how the
overall quality performs against the baselines.

5.1 Quantitative Analysis
The results are presented in Table 1. We observe
that our model outperformed all the strong state-
of-the-art models on both datasets in all metrics
except for RG-2 on Gigaword. In terms of the
pointer generator performance, the improvements
made by our concept pointer are statistically sig-
nificant (p < 0.01) across all metrics.

OOV and Summary Length: OOV is another
major challenge for current abstractive summa-
rization models. Although generating longer sum-
maries or less UNKs is not our focus, our model
still showed improvements in this regard (Table
2). We counted the number of UNKs and all

Table 3: Abstractiveness: percentage of novel n-grams
on Gigaword dataset.

Models Novel n-gram (%)
1-gram 2-gram 3-gram

Pointer generator 14.3 41.9 63.4
Concept pointer 17.2 45.8 68.5
Reference summary 25.4 65.6 78.4

generated summary words and measured the pro-
portions in both testing sets. The OOV percent-
ages dropped from 4.02% to 1.12% on Gigaword
and from 2.08% to 0.23% on DUC-2004, which
demonstrates that our model is effective at alle-
viating OOV problems. This result also supports
the superior of the concept pointer over the base-
line pointer generator. From the statistics, we
found that the summaries generated by our con-
cept pointer averaged around 10.46 words, while
the pointer generator summaries averaged 10.28
words per summary on DUC-2004. This shows
the concept pointer is able to capture more salient
content by generating relatively longer summaries.

Abstractiveness: According to Chen and
Bansal (2018), abstractiveness scores are com-
puted as the percentage of novel n-grams in
the generated summaries that are not included
in the source documents. As shown in Table
3, compared with human-written summaries
which receive the highest novelty in terms of
abstractiveness, our concept pointer generator
achieves closest performance with human-written
summaries against the baseline. This result
demonstrates a further advantage of our model
in producing new and abstract concepts. Our
model is designed to improve semantic relevance
and promote higher abstraction. More generated
summary examples can be found in Appendix B.

5.2 Analysis on Training Strategies
To evaluate the relative impact of each training
strategy with the model, we tested different combi-
nations for comparison with each other and against
the baselines.

Context-aware Conceptualization: To investi-
gate the impact of training with both the num-
ber of concepts k and the concept update strat-
egy mentioned in Eqs.(6) and (7), we chose a dif-
ferent number of concept candidates, i.e., k =
1, 2, 3, 4, 5, to for the context-aware conceptual-
ization update strategy. Performance was fully



3083

1 2 3 4 5

K

15

20

25

30

35

ROUGE-1

ROUGE-2

ROUGE-L

(a) Gigaword: P̂Ci random

1 2 3 4 5
K

10

15

20

25

30

ROUGE-1

ROUGE-2

ROUGE-L

(b) DUC-2004: P̂Ci random

1 2 3 4 5
K

15

20

25

30

35

ROUGE-1

ROUGE-2

ROUGE-L

(c) Gigaword: P̂Ci argmax

1 2 3 4 5
K

10

15

20

25

30

ROUGE-1

ROUGE-2

ROUGE-L

(d) DUC-2004: P̂Ci argmax

Figure 3: ROUGE metrics on Gigaword and DUC-
2004 w.r.t a different number of concept candi-
dates. Updates were conducted by hard assignment
P̂Ci argmax and random selection P̂

C
i random.

evaluated with the three ROUGE metrics as shown
in Figure 3. The results only vary slightly accord-
ing to the number of concepts with the random se-
lection strategy (Eq.(7)), as shown in Figure 3(a)
and 3(b). This indicates that a random strategy is
not very sensitive to the number of extracted top-
ics. This is, in part, because the concept pointer
may or may not be able to point to the correct con-
cepts from multiple candidates. While in Figure
3(c) and 3(d), the optimum settings are clearly ap-
parent, i.e., k = 1 on Gigaword and k = 2 on
DUC-2004. Overall, the hard assignment strategy
(Eq.(6)) provided the best performance in practical
terms, while random selection (Eq.(7)) performs
stably with different settings.

Training with DS vs. RL: As shown in Ta-
ble 1, our model with either a distant supervision
strategy (concept pointer+DS) or reinforcement
learning (concept pointer+RL) were both supe-
rior to the basic concept pointer generator on both
datasets. Further, the relative improvement of the
concept pointer+DS over the concept pointer+RL
ranged from 3.5% to 9.6% on DUC-2004 but was
inferior to concept pointer+RL on Gigaword. In
comparing the results, it is clear that DS training
has a noticeable effect when the testing set is sub-
stantially semantically different from the training
set but provides less improvement than RL when

Table 4: Human evaluation: scoring of three models in
terms of abstraction and overall quality by human eval-
uators (the higher the better). The score range could be
0-20. ? indicates the improvements from the baselines
to the concept pointer are statistically significant.

Method abstraction overall quality
seq2seq+att ? 5.85 5.65
Pointer generator ? 8.95 8.10
Concept pointer 10.00 9.60

the two are close. From this analysis, we conclude
that the DS strategy is better for model adaption
with abstractive summarization.

5.3 Human Evaluations

To explore the correctness of our model using hu-
man judgment, we conducted a manual evalua-
tion with 20 post-graduate volunteers. We primar-
ily used the following criteria to assess the gener-
ated summaries: abstraction, i.e., Are the abstract
concepts contained in the summary appropriate?;
and overall quality, i.e., Is the summary readable,
informative, relevant, etc.? To conduct the eval-
uation, we randomly selected 20 examples from
the DUC 2004 testing set and asked the volun-
teers to subjectively assess the summaries. Each
example consisted of an article and three sum-
maries, i.e., a summary by the seq2seq model, the
pointer generator model, and our proposed con-
cept pointer model. The volunteers chose the best
summaries for each of the articles according to
the above criteria (can be multiple choices). Ob-
viously, the summaries were randomly shuffled,
and the model used to produce each was unknown
to prevent bias. The scores for each model were
ranked by how many times the volunteers chose a
summary w.r.t each criteria, averaged by the num-
ber of participants. The results are presented in
Table 4, which show that our model outperformed
both the seq2seq model and the pointer generator
(See et al., 2017) in both criteria.

As a last step, we manually inspect the sum-
maries generated by our model, and some exam-
ples are presented in Appendix B. We found that
the summaries were not as abstract as human-
written summary would likely be. The overarch-
ing tendency of the model is still to copy segments
of the source text and rearrange the phrases into
a summary. However, the overall approach does
produce more high-level concepts with correct re-
lations compared to the baselines, which demon-



3084

strates that our solution is a promising research di-
rection to further pursue. Additionally, the gener-
ated summaries are long, fluent, and informative.

6 Conclusion

This paper presents a novel concept pointer gener-
ator model to improve the abstractive summariza-
tion model and generate concept-oriented sum-
maries. We also propose a novel distant super-
vision strategy for model adaption to different
datasets. Both empirical and subjective experi-
ments show that our model makes a statistically
significant quality improvement over the state-of-
the-art baselines on two popular datasets.

Acknowledgement

This work was supported by National Natu-
ral Science Foundation of China (No.61602036,
61751201), partially supported by the Research
Foundation of Beijing Municipal Science & Tech-
nology Commission (No. Z181100008918002),
and partially supported by Ministry of Educa-
tion China Mobile Research Foundation (No.
MCM20170302).

References
Lidong Bing, Piji Li, Yi Liao, Wai Lam, Weiwei

Guo, and Rebecca J. Passonneau. 2015. Abstractive
multi-document summarization via phrase selection
and merging. In Proceedings of the 53rd Annual
Meeting of the Association for Computational Lin-
guistics and the 7th International Joint Conference
on Natural Language Processing of the Asian Feder-
ation of Natural Language Processing, ACL, Volume
1: Long Papers, pages 1587–1597.

Asli Çelikyilmaz, Antoine Bosselut, Xiaodong He, and
Yejin Choi. 2018. Deep communicating agents for
abstractive summarization. In Proceedings of the
2018 Conference of the North American Chapter of
the Association for Computational Linguistics: Hu-
man Language Technologies, NAACL-HLT, Volume
1 (Long Papers), pages 1662–1675.

Yen-Chun Chen and Mohit Bansal. 2018. Fast abstrac-
tive summarization with reinforce-selected sentence
rewriting. arXiv preprint arXiv:1805.11080.

Sumit Chopra, Michael Auli, and Alexander M. Rush.
2016. Abstractive sentence summarization with at-
tentive recurrent neural networks. In Conference of
the North American Chapter of the Association for
Computational Linguistics: Human Language Tech-
nologies, pages 93–98.

Bonnie Dorr, David Zajic, and Richard Schwartz.
2003. Hedge trimmer: A parse-and-trim approach

to headline generation. In Proceedings of the HLT-
NAACL 03 on Text summarization workshop-Volume
5, pages 1–8. Association for Computational Lin-
guistics.

Yang Gao, Yang Wang, Luyang Liu, Yidi Guo, and
Heyan Huang. 2019. Neural abstractive summa-
rization fusing by global generative topics. Neural
Computing and Applications.

Pierre-Etienne Genest and Guy Lapalme. 2011.
Framework for abstractive summarization using
text-to-text generation. In Proceedings of the
Workshop on Monolingual Text-To-Text Generation,
pages 64–73.

Jiatao Gu, Zhengdong Lu, Hang Li, and Victor OK
Li. 2016. Incorporating copying mechanism in
sequence-to-sequence learning. In Proceedings of
the 54th Annual Meeting of the Association for Com-
putational Linguistics (Volume 1: Long Papers),
volume 1, pages 1631–1640.

Han Guo, Ramakanth Pasunuru, and Mohit Bansal.
2018. Soft layer-specific multi-task summarization
with entailment and question generation. In Pro-
ceedings of the 56th Annual Meeting of the Associa-
tion for Computational Linguistics, ACL 2018, Vol-
ume 1: Long Papers, pages 687–697.

Yidi Guo, Heyan Huang, Yang Gao, and Chi Lu. 2017.
Conceptual multi-layer neural network model for
headline generation. In Chinese Computational Lin-
guistics and Natural Language Processing Based
on Naturally Annotated Big Data, pages 355–367.
Springer.

Wan-Ting Hsu, Chieh-Kai Lin, Ming-Ying Lee, Kerui
Min, Jing Tang, and Min Sun. 2018. A uni-
fied model for extractive and abstractive summa-
rization using inconsistency loss. arXiv preprint
arXiv:1805.06266.

Wojciech Kryscinski, Romain Paulus, Caiming Xiong,
and Richard Socher. 2018. Improving abstraction
in text summarization. In Proceedings of the 2018
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 1808–1817.

Haoran Li, Junnan Zhu, Jiajun Zhang, and Chengqing
Zong. 2018. Ensure the correctness of the sum-
mary: Incorporate entailment knowledge into ab-
stractive sentence summarization. In Proceedings of
the 27th International Conference on Computational
Linguistics, COLING 2018, pages 1430–1441.

Chin-Yew Lin. 2004. Rouge: A package for auto-
matic evaluation of summaries. Text Summarization
Branches Out.

Junyang Lin, Xu Sun, Shuming Ma, and Qi Su. 2018.
Global encoding for abstractive summarization. In
Proceedings of the 56th Annual Meeting of the As-
sociation for Computational Linguistics, ACL, 2018,
Volume 2: Short Papers, pages 163–169.



3085

Fei Liu, Jeffrey Flanigan, Sam Thomson, Norman
Sadeh, and Noah A Smith. 2018. Toward abstrac-
tive summarization using semantic representations.
arXiv preprint arXiv:1805.10399.

Thang Luong, Hieu Pham, and Christopher D. Man-
ning. 2015. Effective approaches to attention-based
neural machine translation. In Proceedings of the
2015 Conference on Empirical Methods in Natural
Language Processing, EMNLP 2015, pages 1412–
1421.

Ramesh Nallapati, Bowen Zhou, Cı́cero Nogueira dos
Santos, Çaglar Gülçehre, and Bing Xiang. 2016.
Abstractive text summarization using sequence-to-
sequence RNNs and beyond. In Proceedings of the
20th SIGNLL Conference on Computational Natural
Language Learning, CoNLL, pages 280–290.

Robert Parker, David Graff, Junbo Kong, Ke Chen, and
Kazuaki Maeda. 2011. English gigaword fifth edi-
tion ldc2011t07. dvd. Philadelphia: Linguistic Data
Consortium.

Romain Paulus, Caiming Xiong, and Richard Socher.
2017. A deep reinforced model for abstractive sum-
marization. arXiv preprint arXiv:1705.04304.

Alexander M. Rush, Sumit Chopra, and Jason Weston.
2015. A neural attention model for abstractive sen-
tence summarization. In Proceedings of the 2015
Conference on Empirical Methods in Natural Lan-
guage Processing, EMNLP, pages 379–389.

Abigail See, Peter J. Liu, and Christopher D. Man-
ning. 2017. Get to the point: Summarization with
pointer-generator networks. In Proceedings of the
55th Annual Meeting of the Association for Com-
putational Linguistics, ACL 2017, Volume 1: Long
Papers, pages 1073–1083.

Fei Sun, Peng Jiang, Hanxiao Sun, Changhua Pei,
Wenwu Ou, and Xiaobo Wang. 2018. Multi-source
pointer network for product title summarization. In
Proceedings of the 27th ACM International Confer-
ence on Information and Knowledge Management,
pages 7–16. ACM.

Oriol Vinyals, Meire Fortunato, and Navdeep Jaitly.
2015. Pointer networks. In Advances in Neural In-
formation Processing Systems, pages 2692–2700.

Zhongyuan Wang and Haixun Wang. 2016. Under-
standing short texts. In the Association for Com-
putational Linguistics (ACL) (Tutorial).

Zhongyuan Wang, Haixun Wang, Ji-Rong Wen, and
Yanghua Xiao. 2015. An inference approach to ba-
sic level of categorization. In Proceedings of the
24th acm international on conference on informa-
tion and knowledge management, pages 653–662.
ACM.

Noah Weber, Leena Shekhar, Niranjan Balasubrama-
nian, and Kyunghyun Cho. 2018. Controlling de-
coding for more abstractive summaries with copy-
based networks. arXiv preprint arXiv:1803.07038.

Qingyu Zhou, Nan Yang, Furu Wei, and Ming Zhou.
2017. Selective encoding for abstractive sentence
summarization. In Proceedings of the 55th Annual
Meeting of the Association for Computational Lin-
guistics, Volume 1: Long Papers, pages 1095–1104.

Qingyu Zhou, Nan Yang, Furu Wei, and Ming Zhou.
2018. Sequential copying networks. In Proceedings
of the Thirty-Second AAAI Conference on Artificial
Intelligence, (AAAI-18), pages 4987–4995.


