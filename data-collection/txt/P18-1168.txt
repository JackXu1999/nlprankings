



















































Weakly Supervised Semantic Parsing with Abstract Examples


Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers), pages 1809–1819
Melbourne, Australia, July 15 - 20, 2018. c©2018 Association for Computational Linguistics

1809

Weakly Supervised Semantic Parsing with Abstract Examples

Omer Goldman∗, Veronica Latcinnik∗, Udi Naveh∗, Amir Globerson, Jonathan Berant
Tel-Aviv University

{omergoldman@mail,veronical@mail,
ehudnave@mail,gamir@post,joberant@cs}.tau.ac.il

Abstract

Training semantic parsers from weak su-
pervision (denotations) rather than strong
supervision (programs) complicates train-
ing in two ways. First, a large search
space of potential programs needs to be
explored at training time to find a correct
program. Second, spurious programs that
accidentally lead to a correct denotation
add noise to training. In this work we pro-
pose that in closed worlds with clear se-
mantic types, one can substantially allevi-
ate these problems by utilizing an abstract
representation, where tokens in both the
language utterance and program are lifted
to an abstract form. We show that these
abstractions can be defined with a handful
of lexical rules and that they result in shar-
ing between different examples that alle-
viates the difficulties in training. To test
our approach, we develop the first seman-
tic parser for CNLVR, a challenging vi-
sual reasoning dataset, where the search
space is large and overcoming spurious-
ness is critical, because denotations are
either TRUE or FALSE, and thus random
programs are likely to lead to a correct
denotation. Our method substantially im-
proves performance, and reaches 82.5%
accuracy, a 14.7% absolute accuracy im-
provement compared to the best reported
accuracy so far.

1 Introduction

The goal of semantic parsing is to map language
utterances to executable programs. Early work
on statistical learning of semantic parsers utilized

∗ Authors equally contributed to this work.

I :

k :[[{y loc: ..., color: ’Black’, type: ’square’, x loc: ...
size: 20}, ...}]]

x :There is a small yellow item not touching any wall

y :True

z :Exist(Filter(ALL ITEMS, λx.And(And(IsYellow(x),
IsSmall(x)), Not(IsTouchingWall(x, Side.Any))))))

Figure 1: Overview of our visual reasoning setup for the CN-
LVR dataset. Given an image rendered from a KB k and an
utterance x, our goal is to parse x to a program z that re-
sults in the correct denotation y. Our training data includes
(x, k, y) triplets.

supervised learning, where training examples in-
cluded pairs of language utterances and programs
(Zelle and Mooney, 1996; Kate et al., 2005; Zettle-
moyer and Collins, 2005, 2007). However, col-
lecting such training examples at scale has quickly
turned out to be difficult, because expert annota-
tors who are familiar with formal languages are re-
quired. This has led to a body of work on weakly-
supervised semantic parsing (Clarke et al., 2010;
Liang et al., 2011; Krishnamurthy and Mitchell,
2012; Kwiatkowski et al., 2013; Berant et al.,
2013; Cai and Yates, 2013; Artzi and Zettlemoyer,
2013). In this setup, training examples correspond
to utterance-denotation pairs, where a denotation
is the result of executing a program against the en-
vironment (see Fig. 1). Naturally, collecting deno-
tations is much easier, because it can be performed
by non-experts.

Training semantic parsers from denotations
rather than programs complicates training in two
ways: (a) Search: The algorithm must learn to
search through the huge space of programs at
training time, in order to find the correct program.
This is a difficult search problem due to the com-
binatorial nature of the search space. (b) Spurious-



1810

ness: Incorrect programs can lead to correct deno-
tations, and thus the learner can go astray based on
these programs. Of the two mentioned problems,
spuriousness has attracted relatively less attention
(Pasupat and Liang, 2016; Guu et al., 2017).

Recently, the Cornell Natural Language for Vi-
sual Reasoning corpus (CNLVR) was released
(Suhr et al., 2017), and has presented an opportu-
nity to better investigate the problem of spurious-
ness. In this task, an image with boxes that con-
tains objects of various shapes, colors and sizes is
shown. Each image is paired with a complex nat-
ural language statement, and the goal is to deter-
mine whether the statement is true or false (Fig. 1).
The task comes in two flavors, where in one the
input is the image (pixels), and in the other it is
the knowledge-base (KB) from which the image
was synthesized. Given the KB, it is easy to view
CNLVR as a semantic parsing problem: our goal
is to translate language utterances into programs
that will be executed against the KB to determine
their correctness (Johnson et al., 2017b; Hu et al.,
2017). Because there are only two return values,
it is easy to generate programs that execute to the
right denotation, and thus spuriousness is a major
problem compared to previous datasets.

In this paper, we present the first semantic
parser for CNLVR. Semantic parsing can be
coarsely divided into a lexical task (i.e., mapping
words and phrases to program constants), and a
structural task (i.e., mapping language composi-
tion to program composition operators). Our core
insight is that in closed worlds with clear seman-
tic types, like spatial and visual reasoning, we can
manually construct a small lexicon that clusters
language tokens and program constants, and create
a partially abstract representation for utterances
and programs (Table 1) in which the lexical prob-
lem is substantially reduced. This scenario is ubiq-
uitous in many semantic parsing applications such
as calendar, restaurant reservation systems, hous-
ing applications, etc: the formal language has a
compact semantic schema and a well-defined typ-
ing system, and there are canonical ways to ex-
press many program constants.

We show that with abstract representations we
can share information across examples and bet-
ter tackle the search and spuriousness challenges.
By pulling together different examples that share
the same abstract representation, we can identify
programs that obtain high reward across multiple

examples, thus reducing the problem of spurious-
ness. This can also be done at search time, by
augmenting the search state with partial programs
that have been shown to be useful in earlier itera-
tions. Moreover, we can annotate a small number
of abstract utterance-program pairs, and automati-
cally generate training examples, that will be used
to warm-start our model to an initialization point
in which search is able to find correct programs.

We develop a formal language for visual rea-
soning, inspired by Johnson et al. (2017b), and
train a semantic parser over that language from
weak supervision, showing that abstract exam-
ples substantially improve parser accuracy. Our
parser obtains an accuracy of 82.5%, a 14.7% ab-
solute accuracy improvement compared to state-
of-the-art. All our code is publicly avail-
able at https://github.com/udiNaveh/
nlvr_tau_nlp_final_proj.

2 Setup

Problem Statement Given a training set of N
examples {(xi, ki, yi)}Ni=1, where xi is an utter-
ance, ki is a KB describing objects in an image and
yi ∈ {TRUE, FALSE} denotes whether the utter-
ance is true or false in the KB, our goal is to learn
a semantic parser that maps a new utterance x to
a program z such that when z is executed against
the corresponding KB k, it yields the correct de-
notation y (see Fig. 1).

Programming language The original KBs in
CNLVR describe an image as a set of objects,
where each object has a color, shape, size and
location in absolute coordinates. We define a
programming language over the KB that is more
amenable to spatial reasoning, inspired by work on
the CLEVR dataset (Johnson et al., 2017b). This
programming language provides access to func-
tions that allow us to check the size, shape, and
color of an object, to check whether it is touch-
ing a wall, to obtain sets of items that are above
and below a certain set of items, etc.1 More for-
mally, a program is a sequence of tokens describ-
ing a possibly recursive sequence of function ap-
plications in prefix notation. Each token is either a
function with fixed arity (all functions have either
one or two arguments), a constant, a variable or a λ
term used to define Boolean functions. Functions,
constants and variables have one of the following

1We leave the problem of learning the programming lan-
guage functions from the original KB for future work.

https://github.com/udiNaveh/nlvr_tau_nlp_final_proj
https://github.com/udiNaveh/nlvr_tau_nlp_final_proj


1811

x: “There are exactly 3 yellow squares touching the wall.”
z: Equal(3, Count(Filter(ALL ITEMS, λx. And (And (IsYellow(x), IsSquare(x), IsTouchingWall(x))))))
x̄: “There are C-QuantMod C-Num C-Color C-Shape touching the wall.”
z̄: C-QuantMod(C-Num, Count(Filter(ALL ITEMS, λx. And (And (IsC-Color(x), IsC-Shape(x), IsTouchingWall(x))))))

Table 1: An example for an utterance-program pair (x, z) and its abstract counterpart (x̄, z̄)

x: “There is a small yellow item not touching any wall.”
z: Exist(Filter(ALL ITEMS, λx.And(And(IsYellow(x), IsSmall(x)), Not(IsTouchingWall(x, Side.Any)))))
x: “One tower has a yellow base.”
z: GreaterEqual(1, Count(Filter(ALL ITEMS, λx.And(IsYellow(x), IsBottom(x)))))

Table 2: Examples for utterance-program pairs. Commas and parenthesis provided for readability only.

atomic types: Int, Bool, Item, Size, Shape,
Color, Side (sides of a box in the image); or a
composite type Set(?), and Func(?,?). Valid
programs have a return type Bool. Tables 1 and 2
provide examples for utterances and their correct
programs. The supplementary material provides a
full description of all program tokens, their argu-
ments and return types.

Unlike CLEVR, CNLVR requires substantial
set-theoretic reasoning (utterances refer to various
aspects of sets of items in one of the three boxes in
the image), which required extending the language
described by Johnson et al. (2017b) to include set
operators and lambda abstraction. We manually
sampled 100 training examples from the training
data and estimate that roughly 95% of the utter-
ances in the training data can be expressed with
this programming language.

3 Model

We base our model on the semantic parser of Guu
et al. (2017). In their work, they used an encoder-
decoder architecture (Sutskever et al., 2014) to de-
fine a distribution pθ(z | x). The utterance x is
encoded using a bi-directional LSTM (Hochreiter
and Schmidhuber, 1997) that creates a contextual-
ized representation hi for every utterance token xi,
and the decoder is a feed-forward network com-
bined with an attention mechanism over the en-
coder outputs (Bahdanau et al., 2015). The feed-
forward decoder takes as input the last K tokens
that were decoded.

More formally the probability of a program is
the product of the probability of its tokens given
the history: pθ(z | x) =

∏
t pθ(zt | x, z1:t−1),

and the probability of a decoded token is com-
puted as follows. First, a Bi-LSTM encoder con-
verts the input sequence of utterance embeddings
into a sequence of forward and backward states
h
{F,B}
1 , . . . , h

{F,B}
|x| . The utterance representation

x̂ is x̂ = [hF|x|;h
B
1 ]. Then decoding produces the

program token-by-token:

qt = ReLU(Wq[x̂; v̂; zt−K−1:t−1]),

αt,i ∝ exp(q>t Wαhi) , ct =
∑
i

αt,ihi,

pθ(zt | x, z1:t−1) ∝ exp(φ>ztWs[qt; ct]),

where φz is an embedding for program token z,
v̂ is a bag-of-words vector for the tokens in x,
zi:j = (zi, . . . , zj) is a history vector of size K,
the matrices Wq,Wα,Ws are learned parameters
(along with the LSTM parameters and embedding
matrices), and ’;’ denotes concatenation.

Search: Searching through the large space of
programs is a fundamental challenge in semantic
parsing. To combat this challenge we apply sev-
eral techniques. First, we use beam search at de-
coding time and when training from weak super-
vision (see Sec. 4), similar to prior work (Liang
et al., 2017; Guu et al., 2017). At each decoding
step we maintain a beam B of program prefixes of
length n, expand them exhaustively to programs of
length n+1 and keep the top-|B| program prefixes
with highest model probability.

Second, we utilize the semantic typing sys-
tem to only construct programs that are syntacti-
cally valid, and substantially prune the program
search space (similar to type constraints in Krish-
namurthy et al. (2017); Xiao et al. (2016); Liang
et al. (2017)). We maintain a stack that keeps
track of the expected semantic type at each de-
coding step. The stack is initialized with the type
Bool. Then, at each decoding step, only tokens
that return the semantic type at the top of the stack
are allowed, the stack is popped, and if the de-
coded token is a function, the semantic types of
its arguments are pushed to the stack. This dra-
matically reduces the search space and guarantees
that only syntactically valid programs will be pro-
duced. Fig. 2 illustrates the state of the stack when
decoding a program for an input utterance.



1812

x :One tower has a yellow base.

z : EqualInt 1 Count Filter ALL ITEMS λx And IsYellow x IsBottom x

s : Int Set Bool Item

Bool Int Int Set BoolFunc BoolFunc Bool Bool Bool Bool Item

Figure 2: An example for the state of the type stack s while decoding a program z for an utterance x.

Given the constrains on valid programs, our
model p′θ(z | x) is defined as:∏

t

pθ(zt | x, z1:t−1) · 1(zt | z1:t−1)∑
z′ pθ(z

′ | x, z1:t−1) · 1(z′ | z1:t−1)
,

where 1(zt | z1:t−1) indicates whether a certain
program token is valid given the program prefix.

Discriminative re-ranking: The above model
is a locally-normalized model that provides a dis-
tribution for every decoded token, and thus might
suffer from the label bias problem (Andor et al.,
2016; Lafferty et al., 2001). Thus, we add a
globally-normalized re-ranker pψ(z | x) that
scores all |B| programs in the final beam produced
by p′θ(z | x). Our globally-normalized model is:

pgψ(z | x) ∝ exp(sψ(x, z)),

and is normalized over all programs in the beam.
The scoring function sψ(x, z) is a neural net-
work with identical architecture to the locally-
normalized model, except that (a) it feeds the de-
coder with the candidate program z and does not
generate it. (b) the last hidden state is inserted to
a feed-forward network whose output is sψ(x, z).
Our final ranking score is p′θ(z|x)p

g
ψ(z | x).

4 Training

We now describe our basic method for training
from weak supervision, which we extend upon in
Sec. 5 using abstract examples. To use weak su-
pervision, we treat the program z as a latent vari-
able that is approximately marginalized. To de-
scribe the objective, define R(z, k, y) ∈ {0, 1} to
be one if executing program z on KB k results in
denotation y, and zero otherwise. The objective is
then to maximize p(y | x) given by:∑
z∈Z

p′θ(z | x)p(y | z, k) =
∑
z∈Z

p′θ(z | x)R(z, k, y)

≈
∑
z∈B

p′θ(z | x)R(z, k, y)

where Z is the space of all programs and B ⊂ Z
are the programs found by beam search.

In most semantic parsers there will be relatively
few z that generate the correct denotation y. How-
ever, in CNLVR, y is binary, and so spuriousness
is a central problem. To alleviate it, we utilize a
property of CNLVR: the same utterance appears
4 times with 4 different images.2 If a program is
spurious it is likely that it will yield the wrong de-
notation in one of those 4 images.

Thus, we can re-define each training example
to be (x, {(kj , yj)}4j=1), where each utterance x is
paired with 4 different KBs and the denotations of
the utterance with respect to these KBs. Then, we
maximize p({yj}4j=1 | x, ) by maximizing the ob-
jective above, except that R(z, {kj , yj}4j=1) = 1
iff the denotation of z is correct for all four KBs.
This dramatically reduces the problem of spuri-
ousness, as the chance of randomly obtaining a
correct denotation goes down from 12 to

1
16 . This

is reminiscent of Pasupat and Liang (2016), where
random permutations of Wikipedia tables were
shown to crowdsourcing workers to eliminate spu-
rious programs.

We train the discriminative ranker analogously
by maximizing the probability of programs with
correct denotation

∑
z∈B p

g
ψ(z | x)R(z, k, y).

This basic training method fails for CNLVR
(see Sec. 6), due to the difficulties of search and
spuriousness. Thus, we turn to learning from ab-
stract examples, which substantially reduce these
problems.

5 Learning from Abstract Examples

The main premise of this work is that in closed,
well-typed domains such as visual reasoning, the
main challenge is handling language composition-
ality, since questions may have a complex and
nested structure. Conversely, the problem of map-
ping lexical items to functions and constants in
the programming language can be substantially
alleviated by taking advantage of the compact
KB schema and typing system, and utilizing a

2 We used the KBs in CNLVR, for which there are 4 KBs
per utterance. When working over pixels there are 24 images
per utterance, as 6 images were generated from each KB.



1813

Utterance Program Cluster #
“yellow” IsYellow C-Color 3
“big” IsBig C-Size 3
“square” IsSquare C-Shape 4
“3” 3 C-Num 2
“exactly” EqualInt C-QuantMod 5
“top” Side.Top C-Location 2
“above” GetAbove C-SpaceRel 6

Total: 25

Table 3: Example mappings from utterance tokens to pro-
gram tokens for the seven clusters used in the abstract repre-
sentation. The rightmost column counts the number of map-
ping in each cluster, resulting in a total of 25 mappings.

small lexicon that maps prevalent lexical items
into typed program constants. Thus, if we abstract
away from the actual utterance into a partially ab-
stract representation, we can combat the search
and spuriousness challenges as we can generalize
better across examples in small datasets.

Consider the utterances:
1. “There are exactly 3 yellow squares touching

the wall.”
2. “There are at least 2 blue circles touching the

wall.”
While the surface forms of these utterances are dif-
ferent, at an abstract level they are similar and it
would be useful to leverage this similarity.

We therefore define an abstract representation
for utterances and logical forms that is suitable for
spatial reasoning. We define seven abstract clus-
ters (see Table 3) that correspond to the main se-
mantic types in our domain. Then, we associate
each cluster with a small lexicon that contains
language-program token pairs associated with this
cluster. These mappings represent the canonical
ways in which program constants are expressed in
natural language. Table 3 shows the seven clusters
we use, with an example for an utterance-program
token pair from the cluster, and the number of
mappings in each cluster. In total, 25 mappings
are used to define abstract representations.

As we show next, abstract examples can be
used to improve the process of training semantic
parsers. Specifically, in sections 5.1-5.3, we use
abstract examples in several ways, from generat-
ing new training data to improving search accu-
racy. The combined effect of these approaches is
quite dramatic, as our evaluation demonstrates.

5.1 High Coverage via Abstract Examples

We begin by demonstrating that abstraction leads
to rather effective coverage of the types of ques-
tions asked in a dataset. Namely, that many ques-

tions in the data correspond to a small set of ab-
stract examples. We created abstract representa-
tions for all 3,163 utterances in the training exam-
ples by mapping utterance tokens to their cluster
label, and then counted how many distinct abstract
utterances exist. We found that as few as 200 ab-
stract utterances cover roughly half of the training
examples in the original training set.

The above suggests that knowing how to answer
a small set of abstract questions may already yield
a reasonable baseline. To test this baseline, we
constructured a “rule-based” parser as follows. We
manually annotated 106 abstract utterances with
their corresponding abstract program (including
alignment between abstract tokens in the utterance
and program). For example, Table 1 shows the
abstract utterance and program for the utterance
“There are exactly 3 yellow squares touching the
wall”. Note that the utterance “There are at least
2 blue circles touching the wall” will be mapped
to the same abstract utterance and program.

Given this set of manual annotations, our rule-
based semantic parser operates as follows: Given
an utterance x, create its abstract representation x̄.
If it exactly matches one of the manually anno-
tated utterances, map it to its corresponding ab-
stract program z̄. Replace the abstract program to-
kens with real program tokens based on the align-
ment with the utterance tokens, and obtain a final
program z. If x̄ does not match return TRUE, the
majority label. The rule-based parser will fail for
examples not covered by the manual annotation.
However, it already provides a reasonable baseline
(see Table 4). As shown next, manual annotations
can also be used for generating new training data.

5.2 Data Augmentation

While the rule-based semantic parser has high
precision and gauges the amount of structural
variance in the data, it cannot generalize be-
yond observed examples. However, we can auto-
matically generate non-abstract utterance-program
pairs from the manually annotated abstract pairs
and train a semantic parser with strong supervi-
sion that can potentially generalize better. E.g.,
consider the utterance “There are exactly 3 yellow
squares touching the wall”, whose abstract repre-
sentation is given in Table 1. It is clear that we can
use this abstract pair to generate a program for a
new utterance “There are exactly 3 blue squares
touching the wall”. This program will be identical



1814

Algorithm 1 Decoding with an Abstract Cache
1: procedure DECODE(x, y, C,D)
2: // C is a map where the key is an abstract utterance

and the value is a pair (Z, R̂) of a list of abstract pro-
grams Z and their average rewards R̂. D is an integer.

3: x̄← Abstract utterance of x
4: A ←D programs in C[x̄] with top reward values
5: B1 ← compute beam of programs of length 1
6: for t = 2 . . . T do // Decode with cache
7: Bt ← construct beam from Bt−1
8: At = truncate(A, t)
9: Bt.add(de-abstract(At))

10: for z ∈ BT do //Update cache
11: Update rewards in C[x̄] using (z̄, R(z, y))
12: return BT ∪ de-abstract(A).

to the program of the first utterance, with IsBlue
replacing IsYellow.

More generally, we can sample any abstract ex-
ample and instantiate the abstract clusters that ap-
pear in it by sampling pairs of utterance-program
tokens for each abstract cluster. Formally, this
is equivalent to a synchronous context-free gram-
mar (Chiang, 2005) that has a rule for generat-
ing each manually-annotated abstract utterance-
program pair, and rules for synchronously gener-
ating utterance and program tokens from the seven
clusters.

We generated 6,158 (x, z) examples using this
method and trained a standard sequence to se-
quence parser by maximizing log p′θ(z|x) in the
model above. Although these are generated from
a small set of 106 abstract utterances, they can be
used to learn a model with higher coverage and ac-
curacy compared to the rule-based parser, as our
evaluation demonstrates.3

The resulting parser can be used as a standalone
semantic parser. However, it can also be used as an
initialization point for the weakly-supervised se-
mantic parser. As we observe in Sec. 6, this results
in further improvement in accuracy.

5.3 Caching Abstract Examples

We now describe a caching mechanism that uses
abstract examples to combat search and spurious-
ness when training from weak supervision. As
shown in Sec. 5.1, many utterances are identical
at the abstract level. Thus, a natural idea is to
keep track at training time of abstract utterance-
program pairs that resulted in a correct denotation,

3Training a parser directly over the 106 abstract examples
results in poor performance due to the small number of ex-
amples.

and use this information to direct the search pro-
cedure.

Concretely, we construct a cache C that maps
abstract utterances to all abstract programs that
were decoded by the model, and tracks the aver-
age reward obtained for those programs. For every
utterance x, after obtaining the final beam of pro-
grams, we add to the cache all abstract utterance-
program pairs (x̄, z̄), and update their average re-
ward (Alg. 1, line 10). To construct an abstract
example (x̄, z̄) from an utterance-program pair
(x, z) in the beam, we perform the following pro-
cedure. First, we create x̄ by replacing utterance
tokens with their cluster label, as in the rule-based
semantic parser. Then, we go over every program
token in z, and replace it with an abstract cluster
if the utterance contains a token that is mapped
to this program token according to the mappings
from Table 3. This also provides an alignment
from abstract program tokens to abstract utterance
tokens that is necessary when utilizing the cache.

We propose two variants for taking advantage
of the cache C. Both are shown in Algorithm 1.
1. Full program retrieval (Alg. 1, line 12): Given
utterance x, construct an abstract utterance x̄, re-
trieve the top D abstract programs A from the
cache, compute the de-abstracted programs Z us-
ing alignments from program tokens to utterance
tokens, and add the D programs to the final beam.
2. Program prefix retrieval (Alg. 1, line 9): Here,
we additionally consider prefixes of abstract pro-
grams to the beam, to further guide the search pro-
cess. At each step t, let Bt be the beam of de-
coded programs at step t. For every abstract pro-
gram z̄ ∈ A add the de-abstracted prefix z1:t to
Bt and expand Bt+1 accordingly. This allows the
parser to potentially construct new programs that
are not in the cache already. This approach com-
bats both spuriousness and the search challenge,
because we add promising program prefixes to the
beam that might have fallen off of it earlier. Fig. 3
visualizes the caching mechanism.

A high-level overview of our entire approach
for utilizing abstract examples at training time
for both data augmentation and model training is
given in Fig. 4.

6 Experimental Evaluation

Model and Training Parameters The Bi-
LSTM state dimension is 30. The decoder has
one hidden layer of dimension 50, that takes the



1815

Figure 3: A visualization of the caching mechanism. At each decoding step, prefixes of high-reward abstract programs are
added to the beam from the cache.

Figure 4: An overview of our approach for utilizing abstract examples for data augmentation and model training.

last 4 decoded tokens as input as well as encoder
states. Token embeddings are of dimension 12,
beam size is 40 and D = 10 programs are used
in Algorithm 1. Word embeddings are initialized
from CBOW (Mikolov et al., 2013) trained on
the training data, and are then optimized end-to-
end. In the weakly-supervised parser we encour-
age exploration with meritocratic gradient updates
with β = 0.5 (Guu et al., 2017). In the weakly-
supervised parser we warm-start the parameters
with the supervised parser, as mentioned above.
For optimization, Adam is used (Kingma and Ba,
2014)), with learning rate of 0.001, and mini-batch
size of 8.

Pre-processing Because the number of utter-
ances is relatively small for training a neural
model, we take the following steps to reduce spar-
sity. We lowercase all utterance tokens, and also

use their lemmatized form. We also use spelling
correction to replace words that contain typos. Af-
ter pre-processing we replace every word that oc-
curs less than 5 times with an UNK symbol.

Evaluation We evaluate on the public develop-
ment and test sets of CNLVR as well as on the
hidden test set. The standard evaluation metric
is accuracy, i.e., how many examples are cor-
rectly classified. In addition, we report consis-
tency, which is the proportion of utterances for
which the decoded program has the correct deno-
tation for all 4 images/KBs. It captures whether a
model consistently produces a correct answer.

Baselines We compare our models to the MA-
JORITY baseline that picks the majority class
(TRUE in our case). We also compare to the state-
of-the-art model reported by Suhr et al. (2017)



1816

Dev. Test-P Test-H
Model Acc. Con. Acc. Con. Acc. Con.
MAJORITY 55.3 - 56.2 - 55.4 -
MAXENT 68.0 - 67.7 - 67.8 -
RULE 66.0 29.2 66.3 32.7 - -
SUP. 67.7 36.7 66.9 38.3 - -
SUP.+DISC 77.7 52.4 76.6 51.8 - -
WEAKSUP. 84.3 66.3 81.7 60.1 - -
W.+DISC 85.7 67.4 84.0 65.0 82.5 63.9

Table 4: Results on the development, public test (Test-P) and
hidden test (Test-H) sets. For each model, we report both
accuracy and consistency.

when taking the KB as input, which is a maximum
entropy classifier (MAXENT). For our models, we
evaluate the following variants of our approach:
• RULE: The rule-based parser from Sec. 5.1.
• SUP.: The supervised semantic parser trained

on augmented data as in Sec. 5.2 (5, 598 exam-
ples for training and 560 for validation).
• WEAKSUP.: Our full weakly-supervised se-

mantic parser that uses abstract examples.
• +DISC: We add a discriminative re-ranker

(Sec. 3) for both SUP. and WEAKSUP.

Main results Table 4 describes our main re-
sults. Our weakly-supervised semantic parser with
re-ranking (W.+DISC) obtains 84.0 accuracy and
65.0 consistency on the public test set and 82.5
accuracy and 63.9 on the hidden one, improving
accuracy by 14.7 points compared to state-of-the-
art. The accuracy of the rule-based parser (RULE)
is less than 2 points below MAXENT, showing
that a semantic parsing approach is very suitable
for this task. The supervised parser obtains better
performance (especially in consistency), and with
re-ranking reaches 76.6 accuracy, showing that
generalizing from generated examples is better
than memorizing manually-defined patterns. Our
weakly-supervised parser significantly improves
over SUP., reaching an accuracy of 81.7 before re-
ranking, and 84.0 after re-ranking (on the public
test set). Consistency results show an even crisper
trend of improvement across the models.

6.1 Analysis

We analyze our results by running multiple abla-
tions of our best model W.+DISC on the develop-
ment set.

To examine the overall impact of our pro-
cedure, we trained a weakly-supervised parser
from scratch without pre-training a supervised
parser nor using a cache, which amounts to a
re-implementation of the RANDOMER algorithm
(Guu et al., 2017). We find that the algorithm is

Dev.
Model Acc. Con.
RANDOMER 53.2 7.1
−ABSTRACTION 58.2 17.6
−DATAAUGMENTATION 71.4 41.2
−BEAMCACHE 77.2 56.1
−EVERYSTEPBEAMCACHE 82.3 62.2
ONEEXAMPLEREWARD 58.2 11.2

Table 5: Results of ablations of our main models on the de-
velopment set. Explanation for the nature of the models is in
the body of the paper.

unable to bootstrap in this challenging setup and
obtains very low performance. Next, we exam-
ined the importance of abstract examples, by pre-
training only on examples that were manually an-
notated (utterances that match the 106 abstract pat-
terns), but with no data augmentation or use of a
cache (−ABSTRACTION). This results in perfor-
mance that is similar to the MAJORITY baseline.

To further examine the importance of abstrac-
tion, we decoupled the two contributions, train-
ing once with a cache but without data augmen-
tation for pre-training (−DATAAUGMENTATION),
and again with pre-training over the augmented
data, but without the cache (−BEAMCACHE). We
found that the former improves by a few points
over the MAXENT baseline, and the latter per-
forms comparably to the supervised parser, that is,
we are still unable to improve learning by training
from denotations.

Lastly, we use a beam cache without line 9 in
Alg. 1 (−EVERYSTEPBEAMCACHE). This al-
ready results in good performance, substantially
higher than SUP. but is still 3.4 points worse than
our best performing model on the development set.

Orthogonally, to analyze the importance of ty-
ing the reward of all four examples that share
an utterance, we trained a model without this ty-
ing, where the reward is 1 iff the denotation is
correct (ONEEXAMPLEREWARD). We find that
spuriousness becomes a major issue and weakly-
supervised learning fails.

Error Analysis We sampled 50 consistent and
50 inconsistent programs from the development
set to analyze the weaknesses of our model. By
and large, errors correspond to utterances that are
more complex syntactically and semantically. In
about half of the errors an object was described
by two or more modifying clauses: “there is a box
with a yellow circle and three blue items”; or nest-
ing occurred: “one of the gray boxes has exactly



1817

three objects one of which is a circle”. In these
cases the model either ignored one of the condi-
tions, resulting in a program equivalent to “there
is a box with three blue items” for the first case,
or applied composition operators wrongly, out-
putting an equivalent to “one of the gray boxes has
exactly three circles” for the second case. How-
ever, in some cases the parser succeeds on such
examples and we found that 12% of the sampled
utterances that were parsed correctly had a similar
complex structure. Other, less frequent reasons for
failure were problems with cardinality interpreta-
tion, i.e. ,“there are 2” parsed as “exactly 2” in-
stead of “at least 2”; applying conditions to items
rather than sets, e.g., “there are 2 boxes with a tri-
angle closely touching a corner” parsed as “there
are 2 triangles closely touching a corner”; and ut-
terances with questionable phrasing, e.g., “there is
a tower that has three the same blocks color”.

Other insights are that the algorithm tended to
give higher probability to the top ranked program
when it is correct (average probability 0.18), com-
pared to cases when it is incorrect (average proba-
bility 0.08), indicating that probabilities are corre-
lated with confidence. In addition, sentence length
is not predictive for whether the model will suc-
ceed: average sentence length of an utterance is
10.9 when the model is correct, and 11.1 when it
errs.

We also note that the model was successful
with sentences that deal with spatial relations, but
struggled with sentences that refer to the size of
shapes. This is due to the data distribution, which
includes many examples of the former case and
fewer examples of the latter.

7 Related Work

Training semantic parsers from denotations has
been one of the most popular training schemes
for scaling semantic parsers since the beginning
of the decade. Early work focused on traditional
log-linear models (Clarke et al., 2010; Liang et al.,
2011; Kwiatkowski et al., 2013), but recently de-
notations have been used to train neural semantic
parsers (Liang et al., 2017; Krishnamurthy et al.,
2017; Rabinovich et al., 2017; Cheng et al., 2017).

Visual reasoning has attracted considerable at-
tention, with datasets such as VQA (Antol et al.,
2015) and CLEVR (Johnson et al., 2017a). The
advantage of CNLVR is that language utterances
are both natural and compositional. Treating vi-

sual reasoning as an end-to-end semantic parsing
problem has been previously done on CLEVR
(Hu et al., 2017; Johnson et al., 2017b).

Our method for generating training data resem-
bles data re-combination ideas in Jia and Liang
(2016), where examples are generated automati-
cally by replacing entities with their categories.

While spuriousness is central to semantic pars-
ing when denotations are not very informative,
there has been relatively little work on explicitly
tackling it. Pasupat and Liang (2015) used man-
ual rules to prune unlikely programs on the WIK-
ITABLEQUESTIONS dataset, and then later uti-
lized crowdsourcing (Pasupat and Liang, 2016) to
eliminate spurious programs. Guu et al. (2017)
proposed RANDOMER, a method for increasing
exploration and handling spuriousness by adding
randomness to beam search and a proposing a
“meritocratic” weighting scheme for gradients. In
our work we found that random exploration during
beam search did not improve results while merito-
cratic updates slightly improved performance.

8 Discussion

In this work we presented the first semantic parser
for the CNLVR dataset, taking structured repre-
sentations as input. Our main insight is that in
closed, well-typed domains we can generate ab-
stract examples that can help combat the diffi-
culties of training a parser from delayed super-
vision. First, we use abstract examples to semi-
automatically generate utterance-program pairs
that help warm-start our parameters, thereby re-
ducing the difficult search challenge of finding
correct programs with random parameters. Sec-
ond, we focus on an abstract representation of ex-
amples, which allows us to tackle spuriousness
and alleviate search, by sharing information about
promising programs between different examples.
Our approach dramatically improves performance
on CNLVR, establishing a new state-of-the-art.

In this paper, we used a manually-built high-
precision lexicon to construct abstract examples.
This is suitable for well-typed domains, which are
ubiquitous in the virtual assistant use case. In fu-
ture work we plan to extend this work and au-
tomatically learn such a lexicon. This can re-
duce manual effort and scale to larger domains
where there is substantial variability on the lan-
guage side.



1818

References

D. Andor, C. Alberti, D. Weiss, A. Severyn, A. Presta,
K. Ganchev, S. Petrov, and M. Collins. 2016. Glob-
ally normalized transition-based neural networks.
arXiv preprint arXiv:1603.06042 .

S. Antol, A. Agrawal, J. Lu, M. Mitchell, D. Batra,
C. L. Zitnick, and D. Parikh. 2015. Vqa: Visual
question answering. In International Conference on
Computer Vision (ICCV). pages 2425–2433.

Y. Artzi and L. Zettlemoyer. 2013. Weakly supervised
learning of semantic parsers for mapping instruc-
tions to actions. Transactions of the Association for
Computational Linguistics (TACL) 1:49–62.

D. Bahdanau, K. Cho, and Y. Bengio. 2015. Neural
machine translation by jointly learning to align and
translate. In International Conference on Learning
Representations (ICLR).

J. Berant, A. Chou, R. Frostig, and P. Liang. 2013. Se-
mantic parsing on Freebase from question-answer
pairs. In Empirical Methods in Natural Language
Processing (EMNLP).

Q. Cai and A. Yates. 2013. Large-scale semantic pars-
ing via schema matching and lexicon extension. In
Association for Computational Linguistics (ACL).

J. Cheng, S. Reddy, V. Saraswat, and M. Lapata. 2017.
Learning structured natural language representations
for semantic parsing. In Association for Computa-
tional Linguistics (ACL).

D. Chiang. 2005. A hierarchical phrase-based model
for statistical machine translation. In Association for
Computational Linguistics (ACL). pages 263–270.

J. Clarke, D. Goldwasser, M. Chang, and D. Roth.
2010. Driving semantic parsing from the world’s re-
sponse. In Computational Natural Language Learn-
ing (CoNLL). pages 18–27.

K. Guu, P. Pasupat, E. Z. Liu, and P. Liang. 2017.
From language to programs: Bridging reinforce-
ment learning and maximum marginal likelihood. In
Association for Computational Linguistics (ACL).

S. Hochreiter and J. Schmidhuber. 1997. Long short-
term memory. Neural Computation 9(8):1735–
1780.

R. Hu, J. Andreas, M. Rohrbach, T. Darrell, and
K. Saenko. 2017. Learning to reason: End-to-
end module networks for visual question answer-
ing. In International Conference on Computer Vi-
sion (ICCV).

R. Jia and P. Liang. 2016. Data recombination for neu-
ral semantic parsing. In Association for Computa-
tional Linguistics (ACL).

J. Johnson, B. Hariharan, L. van der Maaten, L. Fei-Fei,
C. L. Zitnick, and R. Girshick. 2017a. Clevr: A di-
agnostic dataset for compositional language and el-
ementary visual reasoning. In Computer Vision and
Pattern Recognition (CVPR).

J. Johnson, B. Hariharan, L. van der Maaten, J. Hoff-
man, L. Fei-Fei, C. L. Zitnick, and R. Girshick.
2017b. Inferring and executing programs for visual
reasoning. In International Conference on Com-
puter Vision (ICCV).

R. J. Kate, Y. W. Wong, and R. J. Mooney. 2005.
Learning to transform natural to formal languages.
In Association for the Advancement of Artificial In-
telligence (AAAI). pages 1062–1068.

D. Kingma and J. Ba. 2014. Adam: A method
for stochastic optimization. arXiv preprint
arXiv:1412.6980 .

J. Krishnamurthy, P. Dasigi, and M. Gardner. 2017.
Neural semantic parsing with type constraints for
semi-structured tables. In Empirical Methods in
Natural Language Processing (EMNLP).

J. Krishnamurthy and T. Mitchell. 2012. Weakly
supervised training of semantic parsers. In Em-
pirical Methods in Natural Language Processing
and Computational Natural Language Learning
(EMNLP/CoNLL). pages 754–765.

T. Kwiatkowski, E. Choi, Y. Artzi, and L. Zettlemoyer.
2013. Scaling semantic parsers with on-the-fly on-
tology matching. In Empirical Methods in Natural
Language Processing (EMNLP).

J. Lafferty, A. McCallum, and F. Pereira. 2001. Con-
ditional random fields: Probabilistic models for seg-
menting and labeling data. In International Confer-
ence on Machine Learning (ICML). pages 282–289.

C. Liang, J. Berant, Q. Le, K. D. Forbus, and N. Lao.
2017. Neural symbolic machines: Learning seman-
tic parsers on Freebase with weak supervision. In
Association for Computational Linguistics (ACL).

P. Liang, M. I. Jordan, and D. Klein. 2011. Learn-
ing dependency-based compositional semantics. In
Association for Computational Linguistics (ACL).
pages 590–599.

T. Mikolov, K. Chen, G. Corrado, and Jeffrey. 2013.
Efficient estimation of word representations in vec-
tor space. arXiv .

P. Pasupat and P. Liang. 2015. Compositional semantic
parsing on semi-structured tables. In Association for
Computational Linguistics (ACL).

P. Pasupat and P. Liang. 2016. Inferring logical forms
from denotations. In Association for Computational
Linguistics (ACL).



1819

M. Rabinovich, M. Stern, and D. Klein. 2017. Abstract
syntax networks for code generation and semantic
parsing. In Association for Computational Linguis-
tics (ACL).

A. Suhr, M. Lewis, J. Yeh, and Y. Artzi. 2017. A corpus
of natural language for visual reasoning. In Associ-
ation for Computational Linguistics (ACL).

I. Sutskever, O. Vinyals, and Q. V. Le. 2014. Sequence
to sequence learning with neural networks. In Ad-
vances in Neural Information Processing Systems
(NIPS). pages 3104–3112.

C. Xiao, M. Dymetman, and C. Gardent. 2016.
Sequence-based structured prediction for semantic
parsing. In Association for Computational Linguis-
tics (ACL).

M. Zelle and R. J. Mooney. 1996. Learning to parse
database queries using inductive logic program-
ming. In Association for the Advancement of Arti-
ficial Intelligence (AAAI). pages 1050–1055.

L. S. Zettlemoyer and M. Collins. 2005. Learning to
map sentences to logical form: Structured classifica-
tion with probabilistic categorial grammars. In Un-
certainty in Artificial Intelligence (UAI). pages 658–
666.

L. S. Zettlemoyer and M. Collins. 2007. Online learn-
ing of relaxed CCG grammars for parsing to log-
ical form. In Empirical Methods in Natural Lan-
guage Processing and Computational Natural Lan-
guage Learning (EMNLP/CoNLL). pages 678–687.


