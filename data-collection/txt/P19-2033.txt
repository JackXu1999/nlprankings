



















































From Bilingual to Multilingual Neural Machine Translation by Incremental Training


Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics: Student Research Workshop, pages 236–242
Florence, Italy, July 28 - August 2, 2019. c©2019 Association for Computational Linguistics

236

From Bilingual to Multilingual Neural Machine Translation by
Incremental Training

Carlos Escolano, Marta R. Costa-jussà, José A. R. Fonollosa,
{carlos.escolano,marta.ruiz,jose.fonollosa}@upc.edu

TALP Research Center
Universitat Politècnica de Catalunya, Barcelona

Abstract

Multilingual Neural Machine Translation ap-
proaches are based on the use of task-specific
models and the addition of one more lan-
guage can only be done by retraining the
whole system. In this work, we propose a
new training schedule that allows the system
to scale to more languages without modifi-
cation of the previous components based on
joint training and language-independent en-
coder/decoder modules allowing for zero-shot
translation. This work in progress shows close
results to the state-of-the-art in the WMT task.

1 Introduction

In recent years, neural machine translation (NMT)
has had an important improvement in perfor-
mance. Among the different neural architectures,
most approaches are based in an encoder-decoder
structure and the use of attention-based mecha-
nisms (Cho et al., 2014; Bahdanau et al., 2014;
Vaswani et al., 2017). The main objective is com-
puting a representation of the source sentence that
is weighted with attention-based mechanisms to
compute the conditional probability of the tokens
of the target sentence and the previously decoded
target tokens. Same principles have been success-
fully applied to multilingual NMT, where the sys-
tem is able to translate to and from several differ-
ent languages.

Two main approaches have been proposed
for this task, language independent or shared
encoder-decoders. Language independent archi-
tectures(Firat et al., 2016a,b; Schwenk and Douze,
2017) in which each language has its own encoder
and some additional mechanism is added to pro-
duce shared representations, as averaging of the
context vectors or sharing the attention mecha-
nism. These architectures have the flexibility that
each language can be trained with its own vocab-

ulary all languages are trained in parallel. Re-
cent work (Lu et al., 2018) show how to per-
form many to many translations with indepen-
dent encoders and decoders just by sharing ad-
ditional language-specific layers that transformed
the language-specific representations into a shared
one without the need of a pivot language,

On the other hand, architectures that share pa-
rameters between all languages (Johnson et al.,
2017) by using a single encoder and decoder
trained to be able to translate from and to any
of the languages of the system. This approach
presents the advantage that no further mechanisms
are required to produced shared representation of
the languages as they all share the same vocabu-
lary and parameters, and by training all languages
without distinction they allow low resources lan-
guages to take benefit of other languages in the
system improving their performance. Even though
by sharing vocabulary between all languages the
number of required tokens grows as more lan-
guages are included in the system, especially when
languages employ different scripts in the system,
such as Chinese or Russian. Recent work pro-
poses a new approach to add new languages to a
system by adapting the vocabulary (Lakew et al.,
2018), relying on the shared tokens between the
languages to share model parameters, showing
that the amount of shared tokens between the lan-
guages had an impact in the model performance.
This could limit the capability of the system to
adapt to languages with a different script.

These approaches can be further explored into
unsupervised machine translation where the sys-
tem learns to translate between languages without
parallel data just by enforcing the generation and
representation of the tokens to be similar (Artetxe
et al., 2017; Lample et al., 2018).

Also related to our method, recent work has
explored transfer learning for NMT (Zoph et al.,



237

2016; Kim et al., 2019) to improve the perfor-
mance of new translation directions by taking ben-
efit of the information of a previous model. These
approaches are particularly useful in low resources
scenarios when a previous model trained with or-
ders of magnitude more examples is available.

This paper proposes a proof of concept of a
new multilingual NMT approach. The current ap-
proach is based on joint training without parame-
ter or vocabulary sharing by enforcing a compati-
ble representation between the jointly trained lan-
guages and using multitask learning (Dong et al.,
2015). This approach is shown to offer a scalable
strategy to new languages without retraining any
of the previous languages in the system and en-
abling zero-shot translation. Also it sets up a flex-
ible framework to future work on the usage of pre-
trained compatible modules for different tasks.

2 Definitions

Before explaining our proposed model we intro-
duce the annotation and background that will be
assumed through the paper. Languages will be re-
ferred as capital letters X,Y, Z while sentences
will be referred in lower case x, y, z given that
x ∈ X , y ∈ Y and z ∈ Z.

We consider as an encoder (ex, ey, ez) the layers
of the network that given an input sentence pro-
duce a sentence representation (h(x), h(y), h(z))
in a space. Analogously, a decoder (dx, dy, dz)
is the layers of the network that given the sen-
tence representation of the source sentence is able
to produce the tokens of the target sentence. En-
coders and decoders will be always considered
as independent modules that can be arranged and
combined individually as no parameter is shared
between them. Each language and module has
its own weights independent from all the others
present in the system.

3 Joint Training

In this section, we are going to describe the
training schedule of our language independent
decoder-encoder system. The motivation to
choose this architecture is the flexibility to add
new languages to the system without modification
of shared components and the possibility to add
new modalities in the future as the only require-
ment of the architecture is that encodings are pro-
jected in the same space. Sharing network param-
eters may seem a more efficient approach to the

task, but it would not support modality specific
modules while

Given two languages X and Y , our objective
is to train independent encoders and decoders for
each language, ex, dx and ey, dy that produce com-
patible sentence representations h(x), h(y). For
instance, given a sentence x in language X , we
can obtain a representation h(x) from that the en-
coder ex that can be used to either generate a sen-
tence reconstruction using decoder dx or a transla-
tion using decoder dy. With this objective in mind,
we propose a training schedule that combines two
tasks (auto-encoding and translation) and the two
translation directions simultaneously by optimiz-
ing the following loss:

L = LXX + LY Y + LXY + LY X + d (1)

where LXX and LY Y correspond to the recon-
struction losses of both language X and Y (de-
fined as the cross-entropy of the generated tokens
and the source sentence for each language); LXY
and LY X correspond to the translation terms of the
loss measuring token generation of each decoder
given a sentence representation generated by the
other language encoder (using the cross-entropy
between the generated tokens and the translation
reference); and d corresponds to the distance met-
ric between the representation computed by the
encoders. This last term forces the representations
to be similar without sharing parameters while
providing a measure of similarity between the gen-
erated spaces. We have tested different distance
metrics such as L1, L2 or the discriminator addi-
tion (that tried to predict from which language the
representation was generated). For all these alter-
natives, we experienced a space collapse in which
all sentences tend to be located in the same spatial
region. This closeness between the sentences of
the same languages makes them non-informative
for decoding. As a consequence, the decoder per-
forms as a language model, producing an output
only based on the information provided by the pre-
viously decoded tokens. Weighting the distance
loss term in the loss did not improve the perfor-
mance due to the fact that for the small values re-
quired to prevent the collapse the architecture did
not learn a useful representation of both languages
to work with both decoders. To prevent this col-
lapse, we propose a less restrictive measure based
on correlation distance (Chandar et al., 2016) com-
puted as in equations 2 and 3. The rationale behind
this loss is maximizing the correlation between the



238

representations produced by each language while
not enforcing the distance over the individual val-
ues of the representations.

d = 1− c(h(X), h(Y )) (2)

c(h(X), h(Y )) =∑n
i=1(h(xi − h(X)))(h(yi − h(Y )))√∑n

i (h(xi)− h(X))2
∑n

i (h(yi)− h(Y ))2
(3)

where X and Y correspond to the data sources
we are trying to represent; h(xi) and h(yi) corre-
spond to the intermediate representations learned
by the network for a given observation; and h(X)
and h(Y ) are, for a given batch, the intermediate
representation mean of X and Y , respectively.

4 Incremental training

Given the jointly trained model between languages
X and Y , the following step is to add new lan-
guages in order to use our architecture as a mul-
tilingual system. Since parameters are not shared
between the independent encoders and decoders,
our architecture enables to add new languages
without the need to retrain the current languages in
the system. Let’s say we want to add language Z.
To do so, we require to have parallel data between
Z and any language in the system. So, assuming
that we have trained X and Y , we need to have ei-
ther Z−X or Z−Y parallel data. For illustration,
let’s fix that we have Z−X parallel data. Then, we
can set up a new bilingual system with language
Z as source and language X as target. To ensure
that the representation produced by this new pair
is compatible with the previously jointly trained
system, we use the previous X decoder (dx) as
the decoder of the new ZX system and we freeze
it. During training, we optimize the cross-entropy
between the generated tokens and the language X
reference data but only updating the layers belong-
ing to the language Z encoder (ez). Doing this, we
train ez not only to produce good quality transla-
tions but also to produce similar representations to
the already trained languages. No additional dis-
tance is added during this step. The language Z
sentence representation h(z) is only enforced by
the loss of the translation to work with the already
trained module as it would be trained in a bilingual
NMT system.

Our training schedule enforces the generation of
a compatible representation, which means that the

Figure 1: Language addition and zero shoot training
scheme

newly trained encoder ez can be used as input of
the decoder dy from the jointly trained system to
produce zero-shot Z to Y translations. See Figure
1 for illustration.

The fact that the system enables zero-shot trans-
lation shows that the representations produced by
our training schedule contain useful information
and that this can be preserved and shared to new
languages just by enforcing the new modules to
train with the previous one, without any modifica-
tion of the architecture. Another important aspect
is that no pivot language is required to perform the
translation, once the added modules are trained the
zero-shot translation is performed without gener-
ating the language used for training as the sentence
representations in the shared space are compatible
with all the modules in the system.

A current limitation is the need to use the same
vocabulary for the shared language (X) in both
training steps. The use of subwords (Sennrich
et al., 2015) mitigates the impact of this constraint.

5 Data and Implementation

Experiments are conducted using data extracted
from the UN (Ziemski et al., 2016) and EPPS
datasets (Koehn, 2005) that provide 15 million
parallel sentences between English and Spanish,
German and French. newstest2012 and new-



239

System ES-EN EN-ES FR-EN DE-EN
Baseline 32.60 32.90 31.81 28.96
Joint 29.70 30.74 - -
Added lang - - 30.93 27.63

Table 1: Experiment results measured in BLEU score.
All blank positions are not tested or not viable combi-
nations with our data.

System FR-ES DE-ES
Pivot 29.09 21.74
Zero-shot 19.10 10.92

Table 2: Zero-shot results measured in BLEU score

stest2013 were used as validation and test sets,
respectively. These sets provide parallel data be-
tween the four languages that allow for zero-shot
evaluation. Preprocessing consisted of a pipeline
of punctuation normalization, tokenization, cor-
pus filtering of longer sentences than 80 words
and true-casing. These steps were performed us-
ing the scripts available from Moses (Koehn et al.,
2007). Preprocessed data is later tokenized into
BPE subwords (Sennrich et al., 2015) with a vo-
cabulary size of 32000 tokens. We ensure that the
vocabularies are independent and reusable when
new languages were added by creating vocabular-
ies monolingually, i.e. without having access to
other languages during the code generation.

6 Experiments

Our first experiment consists in comparing the per-
formance of the jointly trained system to the stan-
dard Transformer. As explained in previous sec-
tions, this joint model is trained to perform two
different tasks, auto-encoding and translation in
both directions. In our experiments, these direc-
tions are Spanish-English and English-Spanish. In
auto-encoding, both languages provide good re-
sults at 98.21 and 97.44 BLEU points for English
and Spanish, respectively. In translation, we ob-
serve a decrease in performance. Table 1 shows
that for both directions the new training performs
more than 2 BLEU points below the baseline sys-
tem. This difference suggests that even though the
encoders and decoders of the system are compati-
ble they still present some differences in the inter-
nal representation.

Note that the languages chosen for the joint
training seem relevant to the final system perfor-
mance because they are used to define the repre-
sentations of additional languages. Further exper-
imentation is required to understand such impact.

Our second experiment consists of incremen-
tally adding different languages to the system, in
this case, German and French. Note that, since we
freeze the weights while adding the new language,
the order in which we add new languages does not
have any impact on performance. Table 1 shows
that French-English performs 0.9 BLEU points be-
low the baseline and German-English performs
1.33 points below the baseline. French-English is
closer to the baseline performance and this may
be due to its similarity to Spanish, one of the lan-
guages of the initial system languages.

The added languages have better performance
than the jointly trained languages (Spanish-
English from the previous section). This may be
to the fact that the auto-encoding task may have a
negative impact on the translation task.

Finally, another relevant aspect of the proposed
architecture is enabling zero-shot translation. To
evaluate it, we compare the performance of each
of the added languages compared to a pivot sys-
tem based on cascade. Such a system consists of
translating from French (German) to English and
from English to Spanish with the standard Trans-
former. Results show that the zero shot translation
provides a consistent decrease in performance for
both cases of zero-shot translation.

7 Visualization

Our training schedule is based on training modules
to produce compatible representations, in this sec-
tion we want to analyze this similarity at the last
attention block of encoders, where we are forcing
the similarity. In order to graphically show the pre-
sentation a UMAP (McInnes et al., 2018) model
was trained to combine the representations of all
languages. Figures show 130 sentences extracted
from the test set. These sentences have been se-
lected to have a similar length to minimize the
amount of padding required.

Figure 2 (A) shows the representations of all
languages created by their encoders. Languages
are represented in clusters and no overlapping be-
tween languages occurs, similarly to what (Lu
et al., 2018) reported in their multilingual ap-
proach, the language dependent features of the
sentences have a great impact in their representa-
tions.

However, since our encoder/decoders are com-
patible and produce competitive translations, we
decided to explore the representations generated



240

Figure 2: Plot A shows the source sentence representation of each of the encoder modules(ES,EN,DE,FR). Plots B
and C show the representation of the target sentence generated by English(B) and Spanish(C) decoders given the
sentence encodings of parallel sentences generated for all four language encoder modules.

at the last attention block of the English decoder,
and are shown in Figure 2 (B). We can observe
much more similarity between English, French,
and German, (except for a small German clus-
ter) and separated clusters for Spanish. The rea-
son behind these different behaviors may be due
to the fact that French and German have directly
been trained with the frozen English decoder and
being adjusted to produce representations for this
decoder. Finally, figure 2 (C) shows the represen-
tations of the Spanish decoder. Some sentences
have the same representation for all languages,
whereas others no. Looking at the specific sen-
tences that are plotted, we found that close repre-
sentations do not correlate with better translations
or better BLEU. Sentence examples are shown in
the appendix. More research is required to ana-
lyze which layer in the decoder is responsible for
approaching languages in a common space. This
information could be used in the future to train en-
coders of new languages by wisely sharing param-
eters with the decoder as in previous works (He
et al., 2018).

8 Conclusions

This work proposes a proof of concept of a bilin-
gual system NMT which can be extended to a mul-
tilingual NMT system by incremental training. We
have analyzed how the model performs for dif-
ferent languages. Even though the model does
not outperform current bilingual systems, we show
first steps towards achieving competitive transla-
tions with a flexible architecture that enables scal-
ing to new languages (achieving multilingual and
zero-shot translation) without retraining languages
in the system.

Acknowledgments

This work is supported in part by a Google
Faculty Research Award. This work is also
supported in part by the Spanish Ministerio de
Economa y Competitividad, the European Re-
gional Development Fund and the Agencia Estatal
de Investigacin, through the postdoctoral senior
grant Ramn y Cajal, contract TEC2015-69266-P
(MINECO/FEDER,EU) and contract PCIN-2017-
079 (AEI/MINECO).

References
Mikel Artetxe, Gorka Labaka, Eneko Agirre, and

Kyunghyun Cho. 2017. Unsupervised neural ma-
chine translation. arXiv preprint arXiv:1710.11041.

Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben-
gio. 2014. Neural machine translation by jointly
learning to align and translate. arXiv preprint
arXiv:1409.0473.

Sarath Chandar, Mitesh M Khapra, Hugo Larochelle,
and Balaraman Ravindran. 2016. Correlational neu-
ral networks. Neural computation, 28(2):257–285.

Kyunghyun Cho, Bart van Merrienboer, Caglar Gul-
cehre, Dzmitry Bahdanau, Fethi Bougares, Holger
Schwenk, and Yoshua Bengio. 2014. Learning
phrase representations using RNN encoder–decoder
for statistical machine translation. In Proceedings of
the 2014 Conference on Empirical Methods in Nat-
ural Language Processing (EMNLP), pages 1724–
1734, Doha, Qatar. Association for Computational
Linguistics.

Daxiang Dong, Hua Wu, Wei He, Dianhai Yu, and
Haifeng Wang. 2015. Multi-task learning for mul-
tiple language translation. In Proceedings of the
53rd Annual Meeting of the Association for Compu-
tational Linguistics and the 7th International Joint
Conference on Natural Language Processing (Vol-
ume 1: Long Papers), volume 1, pages 1723–1732.

https://doi.org/10.3115/v1/D14-1179
https://doi.org/10.3115/v1/D14-1179
https://doi.org/10.3115/v1/D14-1179


241

Orhan Firat, Kyunghyun Cho, and Yoshua Bengio.
2016a. Multi-way, multilingual neural machine
translation with a shared attention mechanism. In
Proceedings of the 2016 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics: Human Language Technologies,
pages 866–875, San Diego, California. Association
for Computational Linguistics.

Orhan Firat, Baskaran Sankaran, Yaser Al-Onaizan,
Fatos T. Yarman Vural, and Kyunghyun Cho. 2016b.
Zero-resource translation with multi-lingual neural
machine translation. In Proceedings of the 2016
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 268–277, Austin, Texas.
Association for Computational Linguistics.

Tianyu He, Xu Tan, Yingce Xia, Di He, Tao Qin,
Zhibo Chen, and Tie-Yan Liu. 2018. Layer-wise
coordination between encoder and decoder for neu-
ral machine translation. In S. Bengio, H. Wallach,
H. Larochelle, K. Grauman, N. Cesa-Bianchi, and
R. Garnett, editors, Advances in Neural Information
Processing Systems 31, pages 7955–7965. Curran
Associates, Inc.

Melvin Johnson, Mike Schuster, Quoc V Le, Maxim
Krikun, Yonghui Wu, Zhifeng Chen, Nikhil Thorat,
Fernanda Viégas, Martin Wattenberg, Greg Corrado,
et al. 2017. Googles multilingual neural machine
translation system: Enabling zero-shot translation.
Transactions of the Association for Computational
Linguistics, 5:339–351.

Yunsu Kim, Yingbo Gao, and Hermann Ney. 2019.
Effective cross-lingual transfer of neural machine
translation models without shared vocabularies.
arXiv preprint arXiv:1905.05475.

Philipp Koehn. 2005. Europarl: A parallel corpus for
statistical machine translation. In MT summit, vol-
ume 5, pages 79–86.

Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, et al. 2007. Moses: Open source
toolkit for statistical machine translation. In Pro-
ceedings of the 45th annual meeting of the associ-
ation for computational linguistics companion vol-
ume proceedings of the demo and poster sessions,
pages 177–180.

Surafel M. Lakew, Aliia Erofeeva, Matteo Negri, Mar-
cello Federico, and Marco Turchi. 2018. Transfer
learning in multilingual neural machine translation
with dynamic vocabulary. In Proceedings of the
15th International Workshop on Spoken Language
Translation, pages 54–61, Belgium, Bruges.

Guillaume Lample, Alexis Conneau, Ludovic De-
noyer, and Marc’Aurelio Ranzato. 2018. Unsuper-
vised machine translation using monolingual cor-
pora only. In International Conference on Learning
Representations.

Yichao Lu, Phillip Keung, Faisal Ladhak, Vikas Bhard-
waj, Shaonan Zhang, and Jason Sun. 2018. A neu-
ral interlingua for multilingual machine translation.
In Proceedings of the Third Conference on Machine
Translation: Research Papers, pages 84–92, Bel-
gium, Brussels. Association for Computational Lin-
guistics.

Leland McInnes, John Healy, Nathaniel Saul, and
Lukas Grossberger. 2018. Umap: Uniform mani-
fold approximation and projection. The Journal of
Open Source Software, 3(29):861.

Holger Schwenk and Matthijs Douze. 2017. Learn-
ing joint multilingual sentence representations
with neural machine translation. arXiv preprint
arXiv:1704.04154.

Rico Sennrich, Barry Haddow, and Alexandra Birch.
2015. Neural machine translation of rare words with
subword units. arXiv preprint arXiv:1508.07909.

Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob
Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz
Kaiser, and Illia Polosukhin. 2017. Attention is all
you need. In Advances in Neural Information Pro-
cessing Systems, pages 5998–6008.

Michal Ziemski, Marcin Junczys-Dowmunt, and Bruno
Pouliquen. 2016. The united nations parallel corpus
v1. 0. In Lrec.

Barret Zoph, Deniz Yuret, Jonathan May, and Kevin
Knight. 2016. Transfer learning for low-resource
neural machine translation. In Proceedings of the
2016 Conference on Empirical Methods in Natu-
ral Language Processing, pages 1568–1575, Austin,
Texas. Association for Computational Linguistics.

A Examples

This appendix shows some examples of sentences
visualized in Figure 2 in order to further analyse
the visualization. Table 1 reports outputs produced
by the Spanish decoder given encoding represen-
tations produced by the Spanish, English, French
and German encoder. The first two sentences have
similar representations between the languages in
Figure 2 (right) (in the Spanish decoder visualiza-
tion). While the first one keeps the meaning of
the sentence, the second one produces meaning-
less translations. The third sentence produces dis-
joint representations but the meaning is preserved
in the translations. Therefore, since close repre-
sentations may imply different translation perfor-
mance, further research is required to understand
the correlation between representations and trans-
lation quality.

Table 2 shows outputs produced by the English
decoder given encoding representations produced

https://doi.org/10.18653/v1/N16-1101
https://doi.org/10.18653/v1/N16-1101
https://doi.org/10.18653/v1/D16-1026
https://doi.org/10.18653/v1/D16-1026
http://papers.nips.cc/paper/8019-layer-wise-coordination-between-encoder-and-decoder-for-neural-machine-translation.pdf
http://papers.nips.cc/paper/8019-layer-wise-coordination-between-encoder-and-decoder-for-neural-machine-translation.pdf
http://papers.nips.cc/paper/8019-layer-wise-coordination-between-encoder-and-decoder-for-neural-machine-translation.pdf
https://openreview.net/forum?id=rkYTTf-AZ
https://openreview.net/forum?id=rkYTTf-AZ
https://openreview.net/forum?id=rkYTTf-AZ
https://www.aclweb.org/anthology/W18-6309
https://www.aclweb.org/anthology/W18-6309
https://doi.org/10.18653/v1/D16-1163
https://doi.org/10.18653/v1/D16-1163


242

System Sentence
Reference ponemos todo nuestro empeo en participar en este proyecto .
ES ponemos todo nuestro empeo en participar en este proyecto .
EN participamos con esfuerzo en estos proyctos .
FR nos esfuerzos por lograr que los participantes intensivamente en estos proyectos.
DE nuestro objetivo es incorporar estas personas de manera intensiva en nuestro proyecto.
Reference Caja Libre!
ES Caja Libre—
EN Free chash points!
FR librecorrespondinte.
DE cisiguinente
Reference Cómo aplica esta definición en su vida cotidiana y en las redes sociales?
ES Cómo aplica esta definición en su vida cotidiana y en las redes sociales?
EN Cómo se aplica esta definición a su vida diaria?
FR Cómo aplicar esta definición en la vida diaria y sobre los red sociales?
DE Qué es aplicar este definición a su dadadato y las redes sociales?

Table 3: Outputs produced by the Spanish decoder given encoding representations produced by the Spanish, En-
glish, French and German encoder.

System Sentence
Reference it was a terrific season.
ES we had a strong season .
EN it was a terrific season.
FR we made a very big season .
DE we have finished the season with a very strong performance.
Reference in London and Madrid it is completely natural for people with serious handicaps to be indepen-

dently out in public, and they can use the toilets, go to the museum, or wherever ...
ES in London and Madrid , it is very normal for people with severe disability to be left to the public

and be able to serve , to the museum , where ...
EN in London and Madrid it is completely natural for people with serious handicaps to be indepen-

dently out in public, and they can use the toilets, go to the museum, or wherever ...
FR in London and Madrid, it is quite common for people with a heavy disability to travel on their

own in public spaces; they can go to the toilets, to the museum, anywhere ...
DE in London and Madrid, it is absolutely common for people with severe disabilities to be able to

move freely in public spaces, go to the museum, use lets, etc.
Reference from the Czech viewpoint, it seems they tend to put me to the left.
ES from a Czech point of view, I have the impression that people see me more than on the left.
EN from the Czech viewpoint, it seems they tend to put me to the left.
FR from a Czech point of view , I have the impression that people are putting me on the left .
DE from a Czech point of view, it seems to me that people see me rather on the left.

Table 4: Outputs produced by the English decoder given encoding representations produced by the Spanish, En-
glish, French and German encoder.

by the Spanish, English, French and German en-
coder. All examples appear to be close in Figure
2 (center) between German, French and English.
We see that the German and French outputs pre-
serve the general meaning of the sentence. Also
and differently from previous Table 1, the outputs

do not present errors in the attention, repeating
several times tokens or non unintelligible transla-
tions. There are no sentences from French that ap-
pear distant in the visualization, so again, we need
further exploration to understand the information
of this representation.


