



















































A Recurrent Neural Networks Approach for Estimating the Quality of Machine Translation Output


Proceedings of NAACL-HLT 2016, pages 494–498,
San Diego, California, June 12-17, 2016. c©2016 Association for Computational Linguistics

A Recurrent Neural Networks Approach
for Estimating the Quality of Machine Translation Output

Hyun Kim
Creative IT Engineering,

Pohang University of Science and
Technology (POSTECH),

Pohang, Republic of Korea
hkim.postech@gmail.com

Jong-Hyeok Lee
Computer Science and Engineering,
Pohang University of Science and

Technology (POSTECH),
Pohang, Republic of Korea
jhlee@postech.ac.kr

Abstract

This paper presents a novel approach using
recurrent neural networks for estimating the
quality of machine translation output. A se-
quence of vectors made by the prediction
method is used as the input of the final recur-
rent neural network. The prediction method
uses bi-directional recurrent neural network
architecture both on source and target sen-
tence to fully utilize the bi-directional quality
information from source and target sentence.
Our experiments show that the proposed re-
current neural networks approach achieves a
performance comparable to the existing state-
of-the-art models for estimating the sentence-
level quality of English-to-Spanish transla-
tion.

1 Introduction

Estimating the quality of machine translation
output, called quality estimation (QE) (Specia
et al., 2009; Blatz et al., 2004), is to pre-
dict quality scores/categories for unseen machine-
translated sentences without reference translations
at various granularity levels (sentence-level/word-
level/document-level). Quality estimation is of
growing importance in the field of machine transla-
tion (MT) since MT systems are widely used and the
quality of each machine-translated sentence is able
to vary considerably.

Previous research on QE, addressed as a re-
gression/classification problem to compute quality
scores/categories, has mainly focused on feature ex-
traction and feature selection. Feature extraction is

to find the relevant features, such as baseline fea-
tures (Specia et al., 2013) and latent semantic index-
ing (LSI) based features (Langlois, 2015), captur-
ing various aspects of quality from source and target
sentences1 and external resources. Feature selection
is to select the best features by using selection al-
gorithms, such as Gaussian processes (Shah et al.,
2015) and heuristic (González-Rubio et al., 2013),
among already extracted features. Finding desirable
features has played a key role in the QE research.

In this paper we present a recurrent neural net-
works approach for estimating the quality of ma-
chine translation output at sentence level, which
does not require manual effort for finding the best
relevant features. The remainder of this paper is or-
ganized as follows. In Section 2, we propose a re-
current neural networks approach using a sequence
of vectors made by the prediction method as input
for quality estimation. And we describe the pre-
diction method using bi-directional recurrent neural
networks architecture in Section 3. In Section 4, we
report evaluation results, and conclude our paper in
Section 5.

2 Recurrent Neural Networks Approach
for Estimating Quality Score

Because recurrent neural networks (RNNs) have the
strength for handling sequential data (Goodfellow et
al., 2015), we apply RNNs to estimate the quality
score of translation.

The input of the final RNN is a sequence of vec-
tors that have quality information about whether tar-

1In this paper, a ’target sentence’ means the machine-
translated sentence from a source sentence.

494



   POSTECH 

𝑤𝑤𝑤𝑤𝑤𝑤𝑤𝑤 𝑒𝑒𝑒𝑒𝑒𝑒𝑒𝑒𝑤𝑤𝑤𝑤𝑒𝑒𝑒𝑒𝑒𝑒𝑠𝑠 

𝑒𝑒𝑒𝑒𝑤𝑤𝑒𝑒𝑤𝑤𝑒𝑒𝑏𝑏𝑏𝑏𝑒𝑒𝑤𝑤𝑒𝑒𝑏𝑏𝑏𝑏 𝑅𝑅𝑅𝑅𝑅𝑅  
ℎ𝑒𝑒𝑤𝑤𝑤𝑤𝑒𝑒𝑒𝑒 𝑠𝑠𝑏𝑏𝑏𝑏𝑏𝑏𝑒𝑒𝑠𝑠  

[𝒊𝒊𝒊𝒊𝒊𝒊𝒊𝒊𝒊𝒊 𝟏𝟏] 

𝒗𝒗𝟏𝟏 𝒗𝒗𝟐𝟐 𝒗𝒗𝒋𝒋 𝒗𝒗𝑻𝑻𝒚𝒚  ⋯ ⋯ 

𝑸𝑸𝑸𝑸 𝒔𝒔𝒔𝒔𝒔𝒔𝒔𝒔𝒔𝒔 

𝐸𝐸𝑦𝑦𝑦𝑦1 𝐸𝐸𝑦𝑦𝑦𝑦2 𝐸𝐸𝑦𝑦𝑦𝑦𝑗𝑗 𝐸𝐸𝑦𝑦𝑦𝑦𝑇𝑇𝑦𝑦  ⋯ ⋯ 

𝒔𝒔𝟏𝟏 𝒔𝒔𝟐𝟐 𝒔𝒔𝒋𝒋 𝒔𝒔𝑻𝑻𝒚𝒚  

⋯ 

⋯ 

⋯ 

⋯ 

𝒔𝒔𝟏𝟏 𝒔𝒔𝟐𝟐 𝒔𝒔𝒋𝒋 𝒔𝒔𝑻𝑻𝒚𝒚  

𝒒𝒒𝒚𝒚𝒋𝒋 𝒒𝒒𝒚𝒚𝟏𝟏 𝒒𝒒𝒚𝒚𝟐𝟐 𝒒𝒒𝒚𝒚𝑻𝑻𝒚𝒚  ⋯ ⋯ 

𝒔𝒔𝒔𝒔𝒊𝒊𝒔𝒔𝒔𝒔𝒔𝒔 𝒔𝒔𝒔𝒔𝒊𝒊𝒊𝒊𝒔𝒔𝒊𝒊𝒔𝒔𝒔𝒔 (𝐱𝐱) 
𝑥𝑥1 𝑥𝑥2 𝑥𝑥𝑖𝑖 𝑥𝑥𝑇𝑇𝑥𝑥 ⋯ ⋯ 

𝒊𝒊𝒂𝒂𝒔𝒔𝒂𝒂𝒔𝒔𝒊𝒊 𝒔𝒔𝒔𝒔𝒊𝒊𝒊𝒊𝒔𝒔𝒊𝒊𝒔𝒔𝒔𝒔 (𝐲𝐲) 
𝑦𝑦1 𝑦𝑦2 𝑦𝑦𝑗𝑗 𝑦𝑦𝑇𝑇𝑦𝑦 ⋯ ⋯ 

𝑏𝑏𝑗𝑗 𝑏𝑏1 𝑏𝑏2 𝑏𝑏𝑇𝑇𝑦𝑦 ⋯ ⋯ 

𝑏𝑏𝑗𝑗 𝑏𝑏1 𝑏𝑏2 𝑏𝑏𝑇𝑇𝑦𝑦 ⋯ ⋯ 

𝐸𝐸𝑥𝑥𝑥𝑥1 𝐸𝐸𝑥𝑥𝑥𝑥2 𝐸𝐸𝑥𝑥𝑥𝑥𝑖𝑖 𝐸𝐸𝑥𝑥𝑥𝑥𝑇𝑇𝑥𝑥 ⋯ ⋯ 

𝒉𝒉𝟏𝟏 𝒉𝒉𝟐𝟐 𝒉𝒉𝒊𝒊 𝒉𝒉𝑻𝑻𝒙𝒙  

⋯ 

⋯ 

⋯ 

⋯ 

𝒉𝒉𝟏𝟏 𝒉𝒉𝟐𝟐 𝒉𝒉𝒊𝒊 𝒉𝒉𝑻𝑻𝒙𝒙  

𝑤𝑤𝑤𝑤𝑤𝑤𝑤𝑤 𝑒𝑒𝑒𝑒𝑒𝑒𝑒𝑒𝑤𝑤𝑤𝑤𝑒𝑒𝑒𝑒𝑒𝑒𝑠𝑠 

𝑒𝑒𝑒𝑒𝑤𝑤𝑒𝑒𝑤𝑤𝑒𝑒𝑏𝑏𝑏𝑏𝑒𝑒𝑤𝑤𝑒𝑒𝑏𝑏𝑏𝑏 𝑅𝑅𝑅𝑅𝑅𝑅  
 ℎ𝑒𝑒𝑤𝑤𝑤𝑤𝑒𝑒𝑒𝑒 𝑠𝑠𝑏𝑏𝑏𝑏𝑏𝑏𝑒𝑒𝑠𝑠 

[𝒊𝒊𝒊𝒊𝒊𝒊𝒊𝒊𝒊𝒊 𝟐𝟐] 

𝑏𝑏𝑤𝑤𝑒𝑒𝑏𝑏𝑒𝑒𝑥𝑥𝑏𝑏 𝑣𝑣𝑒𝑒𝑏𝑏𝑏𝑏𝑤𝑤𝑤𝑤𝑠𝑠 

[𝒔𝒔𝒊𝒊𝒊𝒊𝒊𝒊𝒊𝒊𝒊𝒊] 

𝑞𝑞𝑞𝑞𝑏𝑏𝑏𝑏𝑒𝑒𝑏𝑏𝑦𝑦 𝑣𝑣𝑒𝑒𝑏𝑏𝑏𝑏𝑤𝑤𝑤𝑤𝑠𝑠 

(𝑅𝑅𝑅𝑅𝑅𝑅) 
ℎ𝑒𝑒𝑤𝑤𝑤𝑤𝑒𝑒𝑒𝑒 𝑠𝑠𝑏𝑏𝑏𝑏𝑏𝑏𝑒𝑒𝑠𝑠 

𝑒𝑒𝑏𝑏𝑥𝑥𝑤𝑤𝑞𝑞𝑏𝑏 𝑞𝑞𝑒𝑒𝑒𝑒𝑏𝑏𝑠𝑠 

𝐬𝐬 𝑠𝑠𝑞𝑞𝑒𝑒𝑒𝑒𝑏𝑏𝑤𝑤𝑦𝑦 𝑞𝑞𝑒𝑒𝑒𝑒𝑏𝑏 

Figure 1: An illustration of the proposed recurrent neural networks model for quality estimation

get words in a target sentence are properly translated
from a source sentence. We will refer to this se-
quence of vectors as quality vectors (qy1 , ... , qyTy ).

Each quality vector qyj
2 has the quality information

about how well a target word yj in a target sentence
y = (y1, ... , yT y ) is translated from a source sen-
tence3 x = (x1, ... , xT x ). Quality vectors are gen-
erated from the prediction method (of Section 3).

To predict a quality estimation score (QE score)
as an HTER score (Snover et al., 2006) in [0,1] for
each target sentence, a logistic sigmoid function is
used such that

QE score(y,x)
= QE score′(qy1 , ... , qyT y )

= σ(W>
QE

s)

(1)

where s is a summary unit of the whole quality vec-
tors and WQE ∈ Rr. r is the dimensionality of sum-
mary unit.

To get the summary unit s, the hidden state vj em-
ploying p gated hidden units for the target word yj
is computed by

vj = f(qyj , vj−1) . (2)

The gated hidden unit (Cho et al., 2014) for the ac-
tivation function f is used to learn long-term depen-

21 5 j 5 Ty where Ty is the length of target sentence.
3Source(Target) sentence consists of 1-of-Kx(Ky) coded

word vectors. Kx(Ky) is the vocabulary sizes of source(target)
language.

dencies of translation qualities for target words. We
consider the QE score as the integrated/condensed
value reflecting the sequential quality information
of sequential target words. Because the last hidden
state vT y is a summary of the sequential quality vec-
tors, we fix the summary unit s to the last hidden
state vT y .

3 Prediction method using Bi-directional
RNN Architecture to Make Quality
Vectors

In this section, we detail the ways to get the quality
vectors (qy1 , ... , qyTy ) for computing QE score.

Since the training data for QE4 are not enough to
use a neural networks approach for making quality
vectors, we use an alternative based on large-scale
parallel corpora such as Europarl. We modify the
word prediction method of RNN Encoder-Decoder
(Cho et al., 2014) using parallel corpora to make the
quality vectors.

In subsection 3.1, we describe the underlying
word prediction method of RNN Encoder-Decoder.
We i) extend the prediction method to use the ad-
ditional backward RNN architecture on target sen-
tence in subsection 3.2 and ii) modify to get the qual-
ity vectors (qy1 , ... , qyTy ) in subsection 3.3.

4These data, provided in WMT Quality Estimation Shared
Task, consist of source sentences, target sentences, and quality
scores.

495



Figure 1 is the graphical illustration of the pro-
posed RNNs approach.

3.1 Word Prediction Method of RNN
Encoder-Decoder

RNN Encoder-Decoder proposed by Cho et al.
(2014) is able to predict the target word yj given
a source sentence x and all preceding target words
{y1, ..., yj−1} by using a softmax function. And it
is extended by Bahdanau et al. (2015) to use infor-
mation of relevant source words for predicting the
target word yj such that

p(yj |{y1, ..., yj−1},x)
= g(yj−1, ~sj−1, cj) .

(3)

g is a nonlinear function predicting the probabil-
ity of yj . ~sj−1 is the hidden state of the forward
RNN on target sentence and contains information
of preceding target words {y1, ... , yj−1 }. cj is the
context vector which means relevant parts of source
sentence associated with the target word yj . ~sj−1
and yj−1 are related to all preceding target words
{y1, ..., yj−1}, and cj is related to x in the word pre-
diction function of (3).

3.2 Additional Backward RNN Architecture on
Target Sentence

Bahdanau et al. (2015) introduce bi-directional
RNN architecture only on source sentence to ex-
tend RNN Encoder-Decoder. In our proposed QE
model, bi-directional RNN architecture is used both
on source and target sentence. By applying bi-
directional RNN architecture both on source and tar-
get sentence, we can fully and bi-directionally uti-
lize source and target sentence for predicting target
words, such that

p(yj |y=yj ,x)
= g([yj−1; yj+1], [~sj−1;

�
sj+1], cj)

=
exp(y>j Wo1Wo2 tj)∑Ky
k=1

exp(y>k Wo1Wo2 tj)
,

(4)

which is the extended version of (3) using the addi-
tional backward RNN architecture.5

5The additional backward RNN on target sentence use the
context vectors shared by the forward RNN on target sentence.

To reflect further all following target words
{yj+1, ... , yT y } when predicting the target word yj ,
the hidden state �sj+1 of the backward RNN and the
next target word yj+1 are added. [~sj−1;

�
sj+1] and

[yj−1; yj+1] are related to y=yj 6, and cj is related to
x in the word prediction function of (4).
Wo1 ∈ RKy×q and Wo2 ∈ Rq×l are weight ma-

trices of softmax function. Ky is the vocabulary
sizes of target language and q is the dimensionality
of quality vectors. l is the dimensionality of maxout
units such that

tj = [max{t̃j,2k−1, t̃j,2k}]>k=1,...,l , (5)

where t̃j,k is the k-th element of a vector t̃j . And

t̃j = S′o[~sj−1;
�
sj+1] + V ′o [Eyyj−1;Eyyj+1] +Cocj ,

(6)
where S′o ∈ R2l×2n, V ′o ∈ R2l×2m, and Co ∈
R2l×2n. Ey ∈ Rm×Ky is the word embedding ma-
trix on target sentence. m and n are the dimensional-
ity of word embedding and hidden states of forward
and backward RNNs. The hidden state �sj+1 of the
backward RNN and next target word yj+1 are used
in (6).7

From the extended prediction method of (4), the
probability of the target word yj is computed by us-
ing information of relevant source words in source
sentence x and all target words y=yj surrounding the
target word yj in target sentence.

3.3 Quality Vectors on Target Sentence
Word prediction method predicts the probability of
target words as a number between 0 and 1. But
we want to get quality vectors of q-dimensionality
which have the more intrinsic quality information
for target words.

To make quality vectors, we regard that the prob-
ability of the target word yj involves the quality in-
formation about whether the target word yj in target
sentence is properly translated from source sentence.
Thus, by decomposing the softmax function8 of (4),

6y=yj = {y1, ... , yj−1, yj+1, ... , yT y }
7Original t̃j (Bahdanau et al., 2015) is

t̃j = So~sj−1 + VoEyyj−1 + Cocj .
8In this softmax function, the bias term is not used for the

simplicity of deriving the quality vectors. Generally, bias terms
are visually omitted in other equations to make the equations
uncluttered.

496



Wo1 Wo2 tj








...
yj
...

Figure 2: Weight matrices (Wo1 and Wo2 ) of softmax function
and maxout unit tj for the target word yj

rowyj (Wo1 ) ◦ [Wo2 tj ]>


...
yj ◦ [ ]
...

Figure 3: The ways of computing the quality vector qy
j

( ◦ is
an element-wise multiplication)

the quality vector qyj for the target word yj is com-
puted by

qyj =
[
rowyj (Wo1 ) ◦ [Wo2 tj ]>

]>
, (7)

where ◦ is an element-wise multiplication. All of
quality information about possible Ky target words
at position j of target sentence is encoded in tj .
Thus, by decoding tj , we are able to get quality vec-
tor qyj for the target word yj ∈ RKy at position j
of target sentence. Figure 2 and 3 show the ways to
compute the quality vector qyj .

4 Experiments

The proposed RNNs approach was evaluated on the
WMT15 Quality Estimation Shared Task9 at sen-
tence level of English-Spanish.

We trained10 the proposed model through a two-
step process. First, by using English-Spanish paral-
lel corpus of Europarl v7 (Koehn, 2005), we trained
bi-directional RNNs having 1000 hidden units on
source and target sentence to make quality vectors.
Next, by using the training set of WMT15 QE task,
to predicte QE scores we trained the final RNN that

9http://www.statmt.org/wmt15/quality-
estimation-task.html

10Stochastic gradient descent (SGD) algorithm with adaptive
learning rate (Adadelta) (Zeiler, 2012) is used to train the pro-
posed model.

System ID MAE ↓ RMSE ↓
• RTM-DCU/RTM-FS+PLS-SVR 0.1325 0.1748
• LORIA/17+LSI+MT+FILTRE 0.1334 0.1735
• RTM-DCU/RTM-FS-SVR 0.1335 0.1768
• LORIA/17+LSI+MT 0.1342 0.1745

Bi-RNN 0.1359 0.1765
• UGENT-LT3/SCATE-SVM 0.1371 0.1745

Baseline SVM 0.1482 0.1913

Table 1: Proposed approach (Bi-RNN) results and official re-
sults for the scoring variant of WMT15 Quality Estimation
Shared Task at sentence level. A total of 5 tied official winning

systems are indicated by a •. Two standard metrics is used:
Mean Average Error (MAE) as a primary metric, and Root of

Mean Squared Error (RMSE) as a secondary metric (Bojar et

al., 2015).

System ID DeltaAvg ↑ Spearman’s ρ ↑
• LORIA/17+LSI+MT+FILTRE 6.51 0.36
• LORIA/17+LSI+MT 6.34 0.37
• RTM-DCU/RTM-FS+PLS-SVR 6.34 0.37
• RTM-DCU/RTM-FS-SVR 6.09 0.35

Bi-RNN 6.08 0.33
Baseline SVM 2.16 0.13

Table 2: Proposed approach (Bi-RNN) results and official re-
sults for the ranking variant of WMT15 Quality Estimation
Shared Task at sentence level. A total of 4 tied official win-

ning systems are indicated by a •. DeltaAvg metric is used as a
primary metric (Bojar et al., 2015).

use the quality vectors generated in previous step as
the input and have 100 hidden units.

Table 1 and 2 present the results of the proposed
approach (Bi-RNN) and the official results for the
scoring and ranking11 variants of the WMT15 Qual-
ity Estimation Shared Task at sentence level. At
both variants of the task, the proposed RNNs ap-
proach achieved the performance over the baseline
performance. Also our experiments showed that the
performance of the proposed RNNs approach is in-
cluded to the best performance group (at the scoring
variant of Table 1) or is close to the best performance
group (at the ranking variant of Table 2).

5 Conclusion

This paper proposed a recurrent neural networks ap-
proach using quality vectors for estimating the qual-
ity of machine translation output at sentence level.

11The ranking variant of the QE task measures how close a
proposed ranking of target translations from best to worst is to
the true ranking.

497



This approach does not require manual effort for
finding the best relevant features which the previous
QE research has mainly focused on.

To make quality vectors we used an alterna-
tive prediction method based on large-scale paral-
lel corpora, because the QE training data were not
enough. By extending the prediction method to use
bi-directional RNN architecture both on source and
target sentence, we were able to fully utilize the bi-
directional quality information from source and tar-
get sentence for quality estimation.

The proposed RNNs approach achieved a per-
formance comparable to the existing state-of-the-art
models at sentence-level QE. Our experiments have
showed that RNNs approach is a meaningful step for
QE research. Applying RNNs approach to word-
level QE and studying other ways to make quality
vectors better are remained for the future study.

Acknowledgments

This research was supported by the MSIP (Ministry
of Science, ICT and Future Planning), Korea, un-
der the ”ICT Consilience Creative Program” (IITP-
2015-R0346-15-1007) supervised by the IITP (Insti-
tute for Information & communications Technology
Promotion)

References
Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben-

gio. 2015. Neural machine translation by jointly
learning to align and translate. In ICLR 2015.

John Blatz, Erin Fitzgerald, George Foster, Simona Gan-
drabur, Cyril Goutte, Alex Kulesza, Alberto Sanchis,
and Nicola Ueffing. 2004. Confidence estimation for
machine translation. In Proceedings of the 20th in-
ternational conference on Computational Linguistics,
page 315. Association for Computational Linguistics.

Ondřej Bojar, Rajen Chatterjee, Christian Federmann,
Barry Haddow, Matthias Huck, Chris Hokamp, Philipp
Koehn, Varvara Logacheva, Christof Monz, Matteo
Negri, Matt Post, Carolina Scarton, Lucia Specia, and
Marco Turchi. 2015. Findings of the 2015 workshop
on statistical machine translation. In Proceedings of
the Tenth Workshop on Statistical Machine Transla-
tion, pages 1–46, Lisbon, Portugal, September. Asso-
ciation for Computational Linguistics.

Kyunghyun Cho, Bart van Merrienboer, Caglar Gulcehre,
Fethi Bougares, Holger Schwenk, and Yoshua Ben-
gio. 2014. Learning phrase representations using rnn

encoder-decoder for statistical machine translation. In
Conference on Empirical Methods in Natural Lan-
guage Processing (EMNLP 2014).

Jesús González-Rubio, J Ramón Navarro-Cerdán, and
Francisco Casacuberta. 2013. Dimensionality reduc-
tion methods for machine translation quality estima-
tion. Machine translation, 27(3-4):281–301.

Ian Goodfellow, Aaron Courville, and Yoshua Bengio.
2015. Deep learning. Book in preparation for MIT
Press.

Philipp Koehn. 2005. Europarl: A parallel corpus for sta-
tistical machine translation. In MT summit, volume 5,
pages 79–86. Citeseer.

David Langlois. 2015. Loria system for the wmt15
quality estimation shared task. In Proceedings of the
Tenth Workshop on Statistical Machine Translation,
pages 323–329, Lisbon, Portugal, September. Associ-
ation for Computational Linguistics.

Kashif Shah, Trevor Cohn, and Lucia Specia. 2015.
A bayesian non-linear method for feature selection
in machine translation quality estimation. Machine
Translation, pages 1–25.

Matthew Snover, Bonnie Dorr, Richard Schwartz, Lin-
nea Micciulla, and John Makhoul. 2006. A study of
translation edit rate with targeted human annotation.
In Proceedings of association for machine translation
in the Americas, pages 223–231.

Lucia Specia, Marco Turchi, Nicola Cancedda, Marc
Dymetman, and Nello Cristianini. 2009. Estimating
the sentence-level quality of machine translation sys-
tems. In 13th Conference of the European Association
for Machine Translation, pages 28–37.

Lucia Specia, Kashif Shah, José GC De Souza, and
Trevor Cohn. 2013. Quest-a translation quality es-
timation framework. In ACL (Conference System
Demonstrations), pages 79–84. Citeseer.

Matthew D Zeiler. 2012. Adadelta: An adaptive learning
rate method. arXiv preprint arXiv:1212.5701.

498


