



















































DataStories at SemEval-2017 Task 6: Siamese LSTM with Attention for Humorous Text Comparison


Proceedings of the 11th International Workshop on Semantic Evaluations (SemEval-2017), pages 390–395,
Vancouver, Canada, August 3 - 4, 2017. c©2017 Association for Computational Linguistics

DataStories at SemEval-2017 Task 6: Siamese LSTM with Attention for
Humorous Text Comparison

Christos Baziotis, Nikos Pelekis, Christos Doulkeridis
University of Piraeus - Data Science Lab

Piraeus, Greece
mpsp14057@unipi.gr, npelekis@unipi.gr, cdoulk@unipi.gr

Abstract

In this paper we present a deep-learning
system that competed at SemEval-2017
Task 6 “#HashtagWars: Learning a Sense
of Humor”. We participated in Subtask A,
in which the goal was, given two Twitter
messages, to identify which one is fun-
nier. We propose a Siamese architecture
with bidirectional Long Short-Term Mem-
ory (LSTM) networks, augmented with an
attention mechanism. Our system works
on the token-level, leveraging word em-
beddings trained on a big collection of un-
labeled Twitter messages. We ranked 2nd

in 7 teams. A post-completion improve-
ment of our model, achieves state-of-the-
art results on #HashtagWars dataset.

1 Introduction

Computational humor (Stock and Strapparava,
2003) is an area in computational linguistics and
natural language understanding. Most computa-
tional humor tasks focus on the problem of humor
detection. However SemEval-2017 Task 6 (Potash
et al., 2017) explores the subjective nature of hu-
mor, using a dataset of Twitter messages posted
in the context of the TV show “@midnight”. At
each episode during the segment “Hashtag Wars”,
a topic in the form of a hashtag is given and view-
ers of the show post funny tweets including that
hashtag. In the next episode, the show selects the
ten funniest tweets and a final winning tweet.

In the past, computational humor tasks have
been approached using hand-crafted features
(Hempelmann, 2008; Mihalcea and Strapparava,
2006; Kiddon and Brun, 2011; Yang et al., 2015).
However, these approaches require a laborious
feature-engineering process, which usually leads
to missing or redundant features, especially in the
case of humor, which is hard to define and con-

sequently hard to model. Recently, approaches us-
ing neural networks, that perform feature-learning,
have shown great results (Chen and Lee, 2017;
Potash et al., 2016; Bertero and Fung, 2016a,b)
outperforming the traditional methods.

In this paper, we present a deep-learning system
that we developed for subtask A - “Pairwise Com-
parison”. The goal of the task is, given two tweets
about the same topic, to identify which one is fun-
nier. The labels are applied using the show’s rel-
ative ranking. This is a very challenging task, be-
cause humor is subjective and the machine learn-
ing system must develop a sense of humor similar
to that of the show, in order to perform well.

We employ a Siamese neural network, which
generates a dense vector representation for each
tweet and then uses those representations as fea-
tures for classification. For modeling the Twit-
ter messages we use Long Short-Term Mem-
ory (LSTM) networks augmented with a context-
aware attention mechanism (Yang et al., 2016).
Furthermore, we perform thorough text prepro-
cessing that enables our neural network to learn
better features. Finally, our approach does not rely
on any hand-crafted features.

2 System Overview

2.1 External Data and Word Embeddings

We collected a big dataset of 330M English Twit-
ter messages, which is used (1) for calculating
word statistics needed for word segmentation and
spell correction and (2) for training word embed-
dings. Word embeddings are dense vector repre-
sentations of words (Collobert and Weston, 2008;
Mikolov et al., 2013), capturing their semantic and
syntactic information. We leverage our big Twitter
dataset to train our own word embeddings, using
GloVe (Pennington et al., 2014). The word em-
beddings are used for initializing the weights of
the first layer (embedding layer) of our network.

390



2.2 Text Preprocessing 1

For preprocessing the text we perform the follow-
ing steps: tokenization, spell correction, word nor-
malization, word segmentation (for splitting hash-
tags) and word annotation (with special tags).
Tokenizer. Our tokenizer is able to identify most
emoticons, emojis, expressions like dates (e.g.
07/11/2011, April 23rd), times (e.g. 4:30pm,
11:00 am), currencies (e.g. $10, 25mil, 50e),
acronyms, censored words (e.g. s**t), words with
emphasis (e.g. *very*) and more. This way we
keep all these expressions as one token, so later
we can normalize them, or annotate them (with
special tags) reducing the vocabulary size and en-
abling our model to learn more abstract features.
Postprocessing. After the tokenization we add
an extra postprocessing step, where we perform
spell correction, word normalization, word seg-
mentation (for splitting a hashtag to its constituent
words) and word annotation. We use the Viterbi
algorithm in order to perform spell correction (Ju-
rafsky and Martin, 2000) and word segmenta-
tion (Segaran and Hammerbacher, 2009), utiliz-
ing word statistics (unigrams and bigrams) from
our big Twitter dataset. Finally, we lowercase all
words, and replace URLs, emails and user handles
(@user), with special tags.

2.3 Recurrent Neural Networks

In computational humor tasks, the most popular
approaches that utilize neural networks involve,
Convolutional Neural Networks (CNN) (Chen and
Lee, 2017; Potash et al., 2016; Bertero and Fung,
2016a) and Recurrent Neural Networks (RNN)
(Bertero and Fung, 2016b). We model the text of
the Twitter messages using RNNs, because CNNs
have no notion of order, therefore losing the in-
formation of the word order. However, RNNs are
designed for processing sequences, where the or-
der of the elements matters. An RNN performs
the same computation, ht = fW (ht−1, xt), on ev-
ery element of a sequence, where ht is the hidden
state at time-step t, and W the weights of the net-
work. The hidden state at each time-step depends
on the previous hidden states. As a result, RNNs
utilize the information of word order and are able
to handle inputs of variable length.

RNNs are difficult to train (Pascanu et al.,
2013), because of the vanishing and exploding
gradients problem, where gradients may grow or

1github.com/cbaziotis/ekphrasis

decay exponentially over long sequences (Bengio
et al., 1994; Hochreiter et al., 2001). We overcome
this limitation by using one of the more sophisti-
cated variants of the regular RNN, the Long Short-
Term Memory (LSTM) network (Hochreiter and
Schmidhuber, 1997), which introduces a gating
mechanism, that ensures proper gradient propaga-
tion through the network.

2.3.1 Attention Mechanism
An RNN can generate a fixed representation for
inputs of variable length. It reads each element
sequentially and updates its hidden state, which
holds a summary of the processed information.
The hidden state at the last time-step, is used as
the representation of the input. In some cases, es-
pecially in long sequences, the RNN might not be
able to hold all the important information in its fi-
nal hidden state. In order to amplify the contri-
bution of important elements (i.e. words) in the
final representation, we use an attention mecha-
nism (Rocktäschel et al., 2015), that aggregates all
the intermediate hidden states using their relative
importance (Fig. 1).

𝑥1

ℎ1

𝑥2

ℎ2

𝑥3

ℎ3

𝑥𝑇

𝒉𝑻

…

(a) Regular RNN
𝑥1

𝑎1 𝑎𝑇
𝑎2 𝑎3

ℎ1

𝑥2

ℎ2

𝑥3

ℎ3

𝑥𝑇

ℎ𝑇

…

(b) Attention RNN

Figure 1: Regular RNN and RNN with attention.

3 Model Description

In our approach, we adopt a Siamese architecture
(Bromley et al., 1993), in which we create two
identical sub-networks. Each sub-network reads
a tweet and generates a fixed representation. Both
subnetworks share the same weights, in order to
project both tweets to the same vector space and
thus be able to make a meaningful comparison be-
tween them. The Siamese sub-networks involve
the Embedding layer, BiLSTM layer and Atten-
tion layer.

The network has two inputs, the sequence of
words in the first tweet X1 = (x11, x

1
2, ..., x

1
T1

),
where T1 the number of words in the first tweet,
and the sequence words of the second tweet X2 =
(x21, x

2
2, ..., x

2
T2

), where T2 the number of words of
the second tweet.

391



𝐿𝑆𝑇𝑀 𝐿𝑆𝑇𝑀 𝐿𝑆𝑇𝑀 𝐿𝑆𝑇𝑀

𝐿𝑆𝑇𝑀 𝐿𝑆𝑇𝑀 𝐿𝑆𝑇𝑀 𝐿𝑆𝑇𝑀

…

…

ℎ𝑇1
1 ℎ𝑇1

1ℎ3
1 ℎ3

1ℎ2
1 ℎ2

1ℎ1
1 ℎ2

1

B
iL

ST
M

Shared weights

…

𝑥1
1 𝑥2

1 𝑥3
1 𝑥𝑇1

1

𝐿𝑆𝑇𝑀 𝐿𝑆𝑇𝑀 𝐿𝑆𝑇𝑀 𝐿𝑆𝑇𝑀

𝐿𝑆𝑇𝑀 𝐿𝑆𝑇𝑀 𝐿𝑆𝑇𝑀 𝐿𝑆𝑇𝑀

…

…

ℎ𝑇2
2 ℎ𝑇2

2ℎ3
2 ℎ3

2ℎ2
2 ℎ2

2ℎ1
2 ℎ2

2

…

𝑥1
2 𝑥2

2 𝑥3
2 𝑥𝑇2

2

Classification

E
m

b
ed

d
in

g
A

tt
en

ti
o

n

Fully-Connected (tanh)

𝑎1
1 𝑎2

1 𝑎3
1 𝑎𝑇1

1

𝑟

𝑢ℎ 𝑎1
2 𝑎2

2 𝑎3
2 𝑎𝑇2

2
𝑢ℎ

Figure 2: Siamese Bidirectional LSTM with context-aware attention mechanism.

Embedding Layer. We use an Embedding layer
to project the words to a low-dimensional vector
space RE , where E is the size of the Embedding
layer. We initialize the weights of the Embedding
layer using our pre-trained word embeddings.
BiLSTM Layer. An LSTM takes as input the
words of a tweet and produces the word annota-
tions H = (h1, h2, ..., hT ), where hi is the hidden
state of the LSTM at time-step i, summarizing all
the information of the sentence up to xi. We use
bidirectional LSTM (BiLSTM) in order to get an-
notations for each word that summarize the infor-
mation from both directions of the message. A
bidirectional LSTM consists of a forward LSTM−→
f that reads the sentence from x1 to xT and a
backward LSTM

←−
f that reads the sentence from

xT to x1. We obtain the final annotation for each
word xi, by concatenating the annotations from
both directions,

hi =
−→
hi ‖ ←−hi , hi ∈ R2L (1)

where ‖ denotes the concatenation operation and
L the size of each LSTM.
Context-Attention Layer. An attention mecha-
nism assigns a weight ai to each word annota-
tion, which reflects its importance. We compute
the fixed representation r of the whole message as
the weighted sum of all the word annotations using
the attention weights. We use a context-aware at-
tention mechanism as in (Yang et al., 2016). This
attention mechanism introduces a context vector
uh, which can be interpreted as a fixed query, that
helps to identify the informative words and it is
randomly initialized and jointly learned with the
rest of the attention layer weights. Formally,

ei = tanh(Whhi + bh), ei ∈ [−1, 1] (2)

ai =
exp(e>i uh)∑T
t=1 exp(e

>
t uh)

,
T∑

i=1

ai = 1 (3)

r =
T∑

i=1

aihi, r ∈ R2L (4)

where Wh, bh and uh are the layer’s weights.
Fully-Connected Layer. Each Siamese subnet-
work produces a fixed representation for each
tweet, r1 and r2 respectively, that we concatenate
to produce the final representation r.

r = r1 ‖ r2, r ∈ R4L (5)
We pass the vector r, to a fully-connected feed-
forward layer with a tanh (hyperbolic tangent) ac-
tivation function. This layer learns a non-linear
function of the input vector, enabling it to perform
the complex task of humor comparison.

c = tanh(Wcr + bc) (6)

Output Layer. The output c of the comparison
layer is fed to a final single neuron layer, that
performs binary classification (logistic regression)
and identifies which tweet is funnier.

3.1 Regularization
At first we adopt the simple but effective tech-
nique of dropout (Srivastava et al., 2014), in which
we randomly turn-off a percentage of the neu-
rons of a layer in our network. Dropout pre-
vents co-adaptation of neurons and can also be
thought as a form of ensemble learning, because
for each training example a subpart of the whole

392



network is trained. Additionally, we apply dropout
to the recurrent connections of the LSTM as sug-
gested in (Gal and Ghahramani, 2016). Moreover,
we add L2 regularization penalty (weight decay)
to the loss function to discourage large weights.
Also, we stop the training of the network, after the
validation loss stops decreasing (early-stopping).
Lastly, we apply Gaussian noise and dropout at the
embedding layer. As a result, the network never
sees the exact same sentence during training, thus
making it more robust to overfitting.

3.2 Training
We train our network to minimize the cross-
entropy loss, using back-propagation with
stochastic gradient descent and mini-batches of
size 256, with the Adam optimizer (Kingma and
Ba, 2014) and we clip the gradients at unit norm.

In order to find good hyper-parameter values
in a relative short time, compared to grid or ran-
dom search, we adopt the Bayesian optimization
(Bergstra et al., 2013) approach. The size of the
embedding layer is 300, the size of LSTM lay-
ers is 50 (100 for BiLSTM) and the size of the
tanh layer is 25. We insert Gaussian noise with
σ = 0.2 and dropout of 0.3 at all layers. Moreover
we apply dropout 0.2 at the recurrent connections
of the LSTMs. Finally, we add L2 regularization
of 0.0001 at the loss function.

4 Results

Subtask A Results. The official evaluation met-
ric of Subtask A is micro-averaged accuracy. Our
team ranked 2nd in 7 teams, with score 0.632.
A post-completion bug-fix improved significantly
the performance of our model (Table 2).

training testing
hashtags 106 6
tweet pairs 109309 48285

Table 1: Dataset Statistics for Subtask A.

System Acc Micro Avg
HumorHawk 0.675
DataStories (official) 0.632
Duluth 0.627

DataStories (fixed) 0.711

Table 2: The Results of our submitted and fixed
models, evaluated on the official Semeval test set.
The updated model would have ranked 1st.

#HastagWars Dataset Results. Furthermore, we
compare the performance of our system on the
#HastagWars dataset (Potash et al., 2016). Ta-
ble 3 shows that our improved model outperforms
the other approaches. The reported results are the
average of 3 Leave-One-Out runs, in order to be
comparable with (Potash et al., 2016). Figure 3
shows the detailed results of our model on the
#HastagWars dataset, with the accuracy distribu-
tion over the hashtags.

System Acc Micro Avg
LSTM (token) (Potash et al., 2016) 0.554 (± 0.0085)
CNN (char) (Potash et al., 2016) 0.637 (± 0.0074)
DataStories (fixed) 0.696 (± 0.0075)

Table 3: Comparison on #HastagWars dataset.

0.5 0.6 0.7 0.8 0.9 1
0

20

40

accuracy

ha
sh

ta
gs

avg= 0.696

min= 0.544

max= 0.921

Figure 3: Detailed results on #HastagWars dataset.

Experimental Setup. For developing our models
we used Keras (Chollet, 2015), Theano (Theano
Dev Team, 2016) and Scikit-learn (Pedregosa
et al., 2011). We trained our neural networks on a
GTX750Ti(4GB), with each model taking approx-
imately 30 minutes to train. Our source code is
available to the research community2.

5 Conclusion

In this paper we present our submission at
SemEval-2017 Task 6 “#HashtagWars: Learning
a Sense of Humor”. We participated in Subtask A
and ranked 2nd out of 7 teams. Our neural network
uses a BiLSTM equipped with an attention mech-
anism in order to identify the most informative
words. The network operates on the word level,
leveraging word embeddings trained on a big col-
lection of tweets. Despite the good results of our
system, we believe that a character-level network
will perform even better in computational humor
tasks, as it will be able to capture the morpholog-
ical characteristics of the words and possibly to
identify word puns. We would like to explore this
approach in the future.

2https://github.com/cbaziotis/
datastories-semeval2017-task6

393



References
Yoshua Bengio, Patrice Y. Simard, and Paolo Frasconi.

1994. Learning long-term dependencies with gra-
dient descent is difficult. IEEE Trans. Neural Net-
works 5(2):157–166.

James Bergstra, Daniel Yamins, and David D. Cox.
2013. Making a Science of Model Search: Hyper-
parameter Optimization in Hundreds of Dimensions
for Vision Architectures. Proceedings of ICML
28:115–123.

Dario Bertero and Pascale Fung. 2016a. Deep learning
of audio and language features for humor prediction.
In Proceedings of LREC.

Dario Bertero and Pascale Fung. 2016b. A long short-
term memory framework for predicting humor in
dialogues. In Proceedings of NAACL-HLT . pages
130–135.

Jane Bromley, James W. Bentz, Léon Bottou, Is-
abelle Guyon, Yann LeCun, Cliff Moore, Eduard
Säckinger, and Roopak Shah. 1993. Signature Veri-
fication Using A "Siamese" Time Delay Neural Net-
work. IJPRAI 7(4):669–688.

Lei Chen and Chong Min Lee. 2017. Convolutional
Neural Network for Humor Recognition. arXiv
preprint arXiv:1702.02584 .

Fraņcois Chollet. 2015. Keras. https://github.
com/fchollet/keras.

Ronan Collobert and Jason Weston. 2008. A unified
architecture for natural language processing: Deep
neural networks with multitask learning. In Pro-
ceedings ICML. pages 160–167.

Yarin Gal and Zoubin Ghahramani. 2016. A theoret-
ically grounded application of dropout in recurrent
neural networks. In Proceedings of NIPS. pages
1019–1027.

Christian F. Hempelmann. 2008. Computational hu-
mor: Beyond the pun? The Primer of Humor Re-
search. Humor Research 8:333–360.

Sepp Hochreiter, Yoshua Bengio, Paolo Frasconi, and
Jürgen Schmidhuber. 2001. Gradient Flow in Re-
current Nets: The Difficulty of Learning Long-Term
Dependencies. A field guide to dynamical recurrent
neural networks. IEEE Press.

Sepp Hochreiter and Jürgen Schmidhuber. 1997.
Long short-term memory. Neural Computation
9(8):1735–1780.

Daniel Jurafsky and James H. Martin. 2000. Speech
and Language Processing: An Introduction to Nat-
ural Language Processing, Computational Linguis-
tics, and Speech Recognition. Prentice Hall PTR,
1st edition.

Chloe Kiddon and Yuriy Brun. 2011. That’s what she
said: Double entendre identification. In Proceedings
of ACL. pages 89–94.

Diederik Kingma and Jimmy Ba. 2014. Adam: A
method for stochastic optimization. arXiv preprint
arXiv:1412.6980 .

Rada Mihalcea and Carlo Strapparava. 2006. Learn-
ing to laugh (automatically): Computational models
for humor recognition. Computational Intelligence
22(2):126–142.

Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S. Cor-
rado, and Jeff Dean. 2013. Distributed representa-
tions of words and phrases and their compositional-
ity. In Proceedings of NIPS. pages 3111–3119.

Razvan Pascanu, Tomas Mikolov, and Yoshua Bengio.
2013. On the difficulty of training recurrent neu-
ral networks. In Proceedings of ICML. pages 1310–
1318.

Fabian Pedregosa, Gaël Varoquaux, Alexandre Gram-
fort, Vincent Michel, Bertrand Thirion, Olivier
Grisel, Mathieu Blondel, Peter Prettenhofer, Ron
Weiss, Vincent Dubourg, and others. 2011. Scikit-
learn: Machine learning in Python. Journal of Ma-
chine Learning Research 12:2825–2830.

Jeffrey Pennington, Richard Socher, and Christo-
pher D. Manning. 2014. Glove: Global Vectors for
Word Representation. In Proceedings of EMNLP.
volume 14, pages 1532–1543.

Peter Potash, Alexey Romanov, and Anna Rumshisky.
2016. # HashtagWars: Learning a Sense of Humor.
arXiv preprint arXiv:1612.03216 .

Peter Potash, Alexey Romanov, and Anna Rumshisky.
2017. SemEval-2017 Task 6: #HashtagWars:
Learning a Sense of Humor. In Proceedings of Se-
mEval.

Tim Rocktäschel, Edward Grefenstette, Karl Moritz
Hermann, Tomáš Kočiskáżş, and Phil Blunsom.
2015. Reasoning about entailment with neural at-
tention. arXiv preprint arXiv:1509.06664 .

Toby Segaran and Jeff Hammerbacher. 2009. Beautiful
Data: The Stories Behind Elegant Data Solutions.
"O’Reilly Media, Inc.".

Nitish Srivastava, Geoffrey E. Hinton, Alex
Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdi-
nov. 2014. Dropout: A simple way to prevent neural
networks from overfitting. Journal of Machine
Learning Research 15(1):1929–1958.

Oliviero Stock and Carlo Strapparava. 2003. Getting
serious about the development of computational hu-
mor. In Proceedings of IJCAI. pages 59–64.

Theano Dev Team. 2016. Theano: A Python frame-
work for fast computation of mathematical expres-
sions. arXiv e-prints abs/1605.02688.

394



Diyi Yang, Alon Lavie, Chris Dyer, and Eduard H.
Hovy. 2015. Humor Recognition and Humor An-
chor Extraction. In Proceedings of EMNLP. pages
2367–2376.

Zichao Yang, Diyi Yang, Chris Dyer, Xiaodong He,
Alex Smola, and Eduard Hovy. 2016. Hierarchical
attention networks for document classification. In
Proceedings of NAACL-HLT . pages 1480–1489.

395


