










































Extending the Entity Grid with Entity-Specific Features


Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 125–129,
Portland, Oregon, June 19-24, 2011. c©2011 Association for Computational Linguistics

Extending the Entity Grid with Entity-Speci�c Features

Micha Elsner

School of Informatics

University of Edinburgh

melsner0@gmail.com

Eugene Charniak

Department of Computer Science

Brown University, Providence, RI 02912

ec@cs.brown.edu

Abstract

We extend the popular entity grid representa-

tion for local coherence modeling. The grid

abstracts away information about the entities it

models; we add discourse prominence, named

entity type and coreference features to distin-

guish between important and unimportant en-

tities. We improve the best result for WSJ doc-

ument discrimination by 6%.

1 Introduction

A well-written document is coherent (Halliday and

Hasan, 1976) it structures information so that each

new piece of information is interpretable given the

preceding context. Models that distinguish coherent

from incoherent documents are widely used in gen-

eration, summarization and text evaluation.

Among the most popular models of coherence is

the entity grid (Barzilay and Lapata, 2008), a sta-

tistical model based on Centering Theory (Grosz et

al., 1995). The grid models the way texts focus

on important entities, assigning them repeatedly to

prominent syntactic roles. While the grid has been

successful in a variety of applications, it is still a

surprisingly unsophisticated model, and there have

been few direct improvements to its simple feature

set. We present an extension to the entity grid which

distinguishes between different types of entity, re-

sulting in signi�cant gains in performance1.

At its core, the grid model works by predicting

whether an entity will appear in the next sentence

1A public implementation is available via https://

bitbucket.org/melsner/browncoherence.

(and what syntactic role it will have) given its his-

tory of occurrences in the previous sentences. For

instance, it estimates the probability that Clinton

will be the subject of sentence 2, given that it was

the subject of sentence 1. The standard grid model

uses no information about the entity itself the prob-

ability is the same whether the entity under discus-

sion is Hillary Clinton or wheat. Plainly, this

assumption is too strong. Distinguishing important

from unimportant entity types is important in coref-

erence (Haghighi and Klein, 2010) and summariza-

tion (Nenkova et al., 2005); our model applies the

same insight to the entity grid, by adding informa-

tion from syntax, a named-entity tagger and statis-

tics from an external coreference corpus.

2 Related work

Since its initial appearance (Lapata and Barzilay,

2005; Barzilay and Lapata, 2005), the entity grid

has been used to perform wide variety of tasks. In

addition to its �rst proposed application, sentence

ordering for multidocument summarization, it has

proven useful for story generation (McIntyre and

Lapata, 2010), readability prediction (Pitler et al.,

2010; Barzilay and Lapata, 2008) and essay scor-

ing (Burstein et al., 2010). It also remains a criti-

cal component in state-of-the-art sentence ordering

models (Soricut and Marcu, 2006; Elsner and Char-

niak, 2008), which typically combine it with other

independently-trained models.

There have been few attempts to improve the en-

tity grid directly by altering its feature representa-

tion. Filippova and Strube (2007) incorporate se-

mantic relatedness, but �nd no signi�cant improve-

125



1 [Visual meteorological conditions]S prevailed for [the
personal cross country �ight for which [a VFR �ight
plan]O was �led]X .
2 [The �ight]S originated at [Nuevo Laredo , Mexico]X ,
at [approximately 1300]X.

s conditions plan flight laredo

1 S O X -

2 - - S X

Figure 1: A short text (using NP-only mention detection),

and its corresponding entity grid. The numeric token

1300 is removed in preprocessing.

ment over the original model. Cheung and Penn

(2010) adapt the grid to German, where focused con-

stituents are indicated by sentence position rather

than syntactic role. The best entity grid for English

text, however, is still the original.

3 Entity grids

The entity grid represents a document as a matrix

(Figure 1) with a row for each sentence and a column

for each entity. The entry for (sentence i, entity j),

which we write ri;j , represents the syntactic role that

entity takes on in that sentence: subject (S), object

(O), or some other role (X)2. In addition, there is a

special marker (-) for entities which do not appear at

all in a given sentence.

To construct a grid, we must �rst decide which

textual units are to be considered entities, and how

the different mentions of an entity are to be linked.

We follow the -COREFERENCE setting from Barzi-

lay and Lapata (2005) and perform heuristic coref-

erence resolution by linking mentions which share a

head noun. Although some versions of the grid use

an automatic coreference resolver, this often fails

to improve results; in Barzilay and Lapata (2005),

coreference improves results in only one of their tar-

get domains, and actually hurts for readability pre-

diction. Their results, moreover, rely on running

coreference on the document in its original order; in

a summarization task, the correct order is not known,

which will cause even more resolver errors.

To build a model based on the grid, we treat the

columns (entities) as independent, and look at lo-

cal transitions between sentences. We model the

2Roles are determined heuristically using trees produced by

the parser of (Charniak and Johnson, 2005).

transitions using the generative approach given in

Lapata and Barzilay (2005)3, in which the model

estimates the probability of an entity's role in the

next sentence, ri;j , given its history in the previ-

ous two sentences, ri�1;j ; ri�2;j . It also uses a sin-

gle entity-speci�c feature, salience, determined by

counting the total number of times the entity is men-

tioned in the document. We denote this feature vec-

tor Fi;j . For example, the vector for �ight after the

last sentence of the example would be F3;f light =

hX;S; sal = 2i. Using two sentences of context
and capping salience at 4, there are only 64 possi-

ble vectors, so we can learn an independent multino-

mial distribution for each F . However, the number

of vectors grows exponentially as we add features.

4 Experimental design

We test our model on two experimental tasks, both

testing its ability to distinguish between correct

and incorrect orderings for WSJ articles. In doc-

ument discrimination (Barzilay and Lapata, 2005),

we compare a document to a random permutation of

its sentences, scoring the system correct if it prefers

the original ordering4.

We also evaluate on the more dif�cult task of sen-

tence insertion (Chen et al., 2007; Elsner and Char-

niak, 2008). In this task, we remove each sentence

from the article and test whether the model prefers to

re-insert it at its original location. We report the av-

erage proportion of correct insertions per document.

As in Elsner and Charniak (2008), we test on sec-

tions 14-24 of the Penn Treebank, for 1004 test doc-

uments. We test signi�cance using the Wilcoxon

Sign-rank test, which detects signi�cant differences

in the medians of two distributions5.

5 Mention detection

Our main contribution is to extend the entity grid

by adding a large number of entity-speci�c features.

Before doing so, however, we add non-head nouns

to the grid. Doing so gives our feature-based model

3Barzilay and Lapata (2005) give a discriminative model,

which relies on the same feature set as discussed here.
4As in previous work, we use 20 random permutations of

each document. Since the original and permutation might tie,

we report both accuracy and balanced F-score.
5Our reported scores are means, but to test signi�cance of

differences in means, we would need to use a parametric test.

126



Disc. Acc Disc. F Ins.

Random 50.0 50.0 12.6

Grid: NPs 74.4 76.2 21.3

Grid: all nounsy 77.8 79.7 23.5

Table 1: Discrimination scores for entity grids with dif-

ferent mention detectors onWSJ development documents.
y indicates performance on both tasks is signi�cantly dif-

ferent from the previous row of the table with p=.05.

more information to work with, but is bene�cial

even to the standard entity grid.

We alter our mention detector to add all nouns

in the document to the grid6, even those which do

not head NPs. This enables the model to pick up

premodi�ers in phrases like a Bush spokesman,

which do not head NPs in the Penn Treebank. Find-

ing these is also necessary to maximize coreference

recall (Elsner and Charniak, 2010). We give non-

head mentions the role X. The results of this change

are shown in Table 1; discrimination performance

increases about 4%, from 76% to 80%.

6 Entity-speci�c features

As we mentioned earlier, the standard grid model

does not distinguish between different types of en-

tity. Given the same history and salience, the same

probabilities are assigned to occurrences of Hillary

Clinton, the airlines, or May 25th, even though

we know a priori that a document is more likely to

be about Hillary Clinton than it is to be about May

25th. This problem is exacerbated by our same-head

coreference heuristic, which sometimes creates spu-

rious entities by lumping together mentions headed

by nouns like miles or dollars. In this section,

we add features that separate important entities from

less important or spurious ones.

Proper Does the entity have a proper mention?

Named entity The majority OPENNLP Morton et

al. (2005) named entity label for the coreferen-

tial chain.

Modi�ers The total number of modi�ers in all men-

tions in the chain, bucketed by 5s.

Singular Does the entity have a singular mention?

6Barzilay and Lapata (2008) uses NPs as mentions; we are

unsure whether all other implementations do the same, but we

believe we are the �rst to make the distinction explicit.

News articles are likely to be about people and

organizations, so we expect these named entity tags,

and proper NPs in general, to be more important to

the discourse. Entities with many modi�ers through-

out the document are also likely to be important,

since this implies that the writer wishes to point

out more information about them. Finally, singular

nouns are less likely to be generic.

We also add some features to pick out entities

that are likely to be spurious or unimportant. These

features depend on in-domain coreference data, but

they do not require us to run a coreference resolver

on the target document itself. This avoids the prob-

lem that coreference resolvers do not work well for

disordered or automatically produced text such as

multidocument summary sentences, and also avoids

the computational cost associated with coreference

resolution.

Linkable Was the head word of the entity ever

marked as coreferring in MUC6?

Unlinkable Did the head word of the entity occur 5

times in MUC6 and never corefer?

Has pronouns Were there 5 or more pronouns

coreferent with the head word of the entity in

the NANC corpus? (Pronouns in NANC are

automatically resolved using an unsupervised

model (Charniak and Elsner, 2009).)

No pronouns Did the head word of the entity occur

over 50 times in NANC, and have fewer than 5

coreferent pronouns?

To learn probabilities based on these features,

we model the conditional probability p(ri;j jF ) us-
ing multilabel logistic regression. Our model has

a parameter for each combination of syntactic role

r, entity-speci�c feature h and feature vector F :

r�h�F . This allows the old and new features to in-
teract while keeping the parameter space tractable7.

In Table 2, we examine the changes in our esti-

mated probability in one particular context: an entity

with salience 3 which appeared in a non-emphatic

role in the previous sentence. The standard entity

grid estimates that such an entity will be the sub-

ject of the next sentence with a probability of about

7We train the regressor using OWLQN (Andrew and Gao,

2007), modi�ed and distributed by Mark Johnson as part of

the Charniak-Johnson parse reranker (Charniak and Johnson,

2005).

127



Context P(next role is subj)

Standard egrid .045

Head coref in MUC6 .013

...and proper noun .025

...and NE type person .037

...and 5 modi�ers overall .133

Never coref in MUC6 .006

...and NE type date .001

Table 2: Probability of an entity appearing as subject of

the next sentence, given the history - X, salience 3, and

various entity-speci�c features.

.04. For most classes of entity, we can see that this

is an overestimate; for an entity described by a com-

mon noun (such as the airline), the probability as-

signed by the extended grid model is .01. If we

suspect (based on MUC6 evidence) that the noun

is not coreferent, the probability drops to .006 (an

increase) if it is a date, it falls even further, to .001.

However, given that the entity refers to a person, and

some of its mentions are modi�ed, suggesting the ar-

ticle gives a title or description (Obama's Secretary

of State, Hillary Clinton), the chance that it will be

the subject of the next sentence more than triples.

7 Experiments

Table 3 gives results for the extended grid model

on the test set. This model is signi�cantly better

than the standard grid on discrimination (84% ver-

sus 80%) and has a higher mean score on insertion

(24% versus 21%)8.

The best WSJ results in previous work are those of

Elsner and Charniak (2008), who combine the entity

grid with models based on pronoun coreference and

discourse-new NP detection. We report their scores

in the table. This comparison is unfair, however,

because the improvements from adding non-head

nouns improve our baseline grid suf�ciently to equal

their discrimination result. State-of-the-art results

on a different corpus and task were achieved by Sori-

cut and Marcu (2006) using a log-linear mixture of

an entity grid, IBM translation models, and a word-

correspondence model based on Lapata (2003).

8For insertion using the model on its own, the median

changes less than the mean, and the change in median score is

not signi�cant. However, using the combined model, the change

is signi�cant.

Disc. Acc Disc. F Ins.

Random 50.00 50.00 12.6

Elsner+Charniak 79.6 81.0 23.0

Grid 79.5 80.9 21.4

Extended Grid 84.0y 84.5 24.2

Grid+combo 82.6 84.0 24.3

ExtEGrid+combo 86.0y 86.5 26.7y

Table 3: Extended entity grid and combination model

performance on 1004 WSJ test documents. Combination

models incorporate pronoun coreference, discourse-new

NP detection, and IBM model 1. yindicates an extended

model score better than its baseline counterpart at p=.05.

To perform a fair comparison of our extended

grid with these model-combining approaches, we

train our own combined model incorporating an en-

tity grid, pronouns, discourse-newness and the IBM

model. We combine models using a log-linear mix-

ture as in Soricut and Marcu (2006), training the

weights to maximize discrimination accuracy.

The second section of Table 3 shows these model

combination results. Notably, our extended entity

grid on its own is essentially just as good as the com-

bined model, which represents our implementation

of the previous state of the art. When we incorpo-

rate it into a combination, the performance increase

remains, and is signi�cant for both tasks (disc. 86%

versus 83%, ins. 27% versus 24%). Though the im-

provement is not perfectly additive, a good deal of

it is retained, demonstrating that our additions to the

entity grid are mostly orthogonal to previously de-

scribed models. These results are the best reported

for sentence ordering of English news articles.

8 Conclusion

We improve a widely used model of local discourse

coherence. Our extensions to the feature set involve

distinguishing simple properties of entities, such as

their named entity type, which are also useful in

coreference and summarization tasks. Although our

method uses coreference information, it does not re-

quire coreference resolution to be run on the target

documents. Given the popularity of entity grid mod-

els for practical applications, we hope our model's

improvements will transfer to summarization, gen-

eration and readability prediction.

128



Acknowledgements

We are most grateful to Regina Barzilay, Mark John-

son and three anonymous reviewers. This work was

funded by a Google Fellowship for Natural Lan-

guage Processing.

References

Galen Andrew and Jianfeng Gao. 2007. Scalable train-

ing of L1-regularized log-linear models. In ICML '07.

Regina Barzilay and Mirella Lapata. 2005. Modeling lo-

cal coherence: an entity-based approach. In Proceed-

ings of the 43rd Annual Meeting of the Association for

Computational Linguistics (ACL'05).

Regina Barzilay and Mirella Lapata. 2008. Modeling

local coherence: an entity-based approach. Computa-

tional Linguistics, 34(1):134.

Jill Burstein, Joel Tetreault, and Slava Andreyev. 2010.

Using entity-based features to model coherence in stu-

dent essays. In Human Language Technologies: The

2010 Annual Conference of the North American Chap-

ter of the Association for Computational Linguistics,

pages 681684, Los Angeles, California, June. Asso-

ciation for Computational Linguistics.

Eugene Charniak and Micha Elsner. 2009. EM works

for pronoun anaphora resolution. In Proceedings of

EACL, Athens, Greece.

Eugene Charniak and Mark Johnson. 2005. Coarse-to-

�ne n-best parsing and MaxEnt discriminative rerank-

ing. In Proc. of the 2005 Meeting of the Assoc. for

Computational Linguistics (ACL), pages 173180.

Erdong Chen, Benjamin Snyder, and Regina Barzilay.

2007. Incremental text structuring with online hier-

archical ranking. In Proceedings of EMNLP.

Jackie Chi Kit Cheung and Gerald Penn. 2010. Entity-

based local coherence modelling using topological

�elds. In Proceedings of the 48th Annual Meet-

ing of the Association for Computational Linguistics,

pages 186195, Uppsala, Sweden, July. Association

for Computational Linguistics.

Micha Elsner and Eugene Charniak. 2008. Coreference-

inspired coherence modeling. In Proceedings of ACL-

08: HLT, Short Papers, pages 4144, Columbus, Ohio,

June. Association for Computational Linguistics.

Micha Elsner and Eugene Charniak. 2010. The same-

head heuristic for coreference. In Proceedings of ACL

10, Uppsala, Sweden, July. Association for Computa-

tional Linguistics.

Katja Filippova and Michael Strube. 2007. Extend-

ing the entity-grid coherence model to semantically

related entities. In Proceedings of the Eleventh Eu-

ropean Workshop on Natural Language Generation,

pages 139142, Saarbrücken, Germany, June. DFKI

GmbH. Document D-07-01.

Barbara J. Grosz, Aravind K. Joshi, and Scott Weinstein.

1995. Centering: A framework for modeling the lo-

cal coherence of discourse. Computational Linguis-

tics, 21(2):203225.

Aria Haghighi and Dan Klein. 2010. Coreference reso-

lution in a modular, entity-centered model. In Human

Language Technologies: The 2010 Annual Conference

of the North American Chapter of the Association for

Computational Linguistics, pages 385393, Los An-

geles, California, June. Association for Computational

Linguistics.

Michael Halliday and Ruqaiya Hasan. 1976. Cohesion

in English. Longman, London.

Mirella Lapata and Regina Barzilay. 2005. Automatic

evaluation of text coherence: Models and representa-

tions. In IJCAI, pages 10851090.

Mirella Lapata. 2003. Probabilistic text structuring: Ex-

periments with sentence ordering. In Proceedings of

the annual meeting of ACL, 2003.

Neil McIntyre and Mirella Lapata. 2010. Plot induction

and evolutionary search for story generation. In Pro-

ceedings of the 48th Annual Meeting of the Associa-

tion for Computational Linguistics, pages 15621572,

Uppsala, Sweden, July. Association for Computational

Linguistics.

Thomas Morton, Joern Kottmann, Jason Baldridge, and

Gann Bierner. 2005. Opennlp: A java-based nlp

toolkit. http://opennlp.sourceforge.net.

Ani Nenkova, Advaith Siddharthan, and Kathleen McK-

eown. 2005. Automatically learning cognitive status

for multi-document summarization of newswire. In

Proceedings of Human Language Technology Confer-

ence and Conference on Empirical Methods in Nat-

ural Language Processing, pages 241248, Vancou-

ver, British Columbia, Canada, October. Association

for Computational Linguistics.

Emily Pitler, Annie Louis, and Ani Nenkova. 2010.

Automatic evaluation of linguistic quality in multi-

document summarization. In Proceedings of the 48th

Annual Meeting of the Association for Computational

Linguistics, pages 544554, Uppsala, Sweden, July.

Association for Computational Linguistics.

Radu Soricut and Daniel Marcu. 2006. Discourse gener-

ation using utility-trained coherence models. In Pro-

ceedings of the Association for Computational Lin-

guistics Conference (ACL-2006).

129


