



















































Variational Neural Machine Translation


Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 521–530,
Austin, Texas, November 1-5, 2016. c©2016 Association for Computational Linguistics

Variational Neural Machine Translation

Biao Zhang1,2, Deyi Xiong1∗, Jinsong Su2, Hong Duan2 and Min Zhang1
Provincial Key Laboratory for Computer Information Processing Technology

Soochow University, Suzhou, China 2150061

Xiamen University, Xiamen, China 3610052

zb@stu.xmu.edu.cn, {jssu,hduan}@xmu.edu.cn
{dyxiong, minzhang}@suda.edu.cn

Abstract

Models of neural machine translation are of-
ten from a discriminative family of encoder-
decoders that learn a conditional distribution
of a target sentence given a source sentence.
In this paper, we propose a variational model
to learn this conditional distribution for neu-
ral machine translation: a variational encoder-
decoder model that can be trained end-to-end.
Different from the vanilla encoder-decoder
model that generates target translations from
hidden representations of source sentences
alone, the variational model introduces a con-
tinuous latent variable to explicitly model un-
derlying semantics of source sentences and to
guide the generation of target translations. In
order to perform efficient posterior inference
and large-scale training, we build a neural
posterior approximator conditioned on both
the source and the target sides, and equip it
with a reparameterization technique to esti-
mate the variational lower bound. Experi-
ments on both Chinese-English and English-
German translation tasks show that the pro-
posed variational neural machine translation
achieves significant improvements over the
vanilla neural machine translation baselines.

1 Introduction

Neural machine translation (NMT) is an emerging
translation paradigm that builds on a single and
unified end-to-end neural network, instead of us-
ing a variety of sub-models tuned in a long training
pipeline. It requires a much smaller memory than

∗Corresponding author

phrase- or syntax-based statistical machine transla-
tion (SMT) that typically has a huge phrase/rule ta-
ble. Due to these advantages over traditional SMT
system, NMT has recently attracted growing inter-
ests from both deep learning and machine transla-
tion community (Kalchbrenner and Blunsom, 2013;
Cho et al., 2014; Sutskever et al., 2014; Bahdanau et
al., 2014; Luong et al., 2015a; Luong et al., 2015b;
Shen et al., 2015; Meng et al., 2015; Tu et al., 2016).

Current NMT models mainly take a discrimi-
native encoder-decoder framework, where a neu-
ral encoder transforms source sentence x into dis-
tributed representations, and a neural decoder gen-
erates the corresponding target sentence y according
to these representations1 (Cho et al., 2014; Sutskever
et al., 2014; Bahdanau et al., 2014). Typically, the
underlying semantic representations of source and
target sentences are learned in an implicit way in
this framework, which heavily relies on the atten-
tion mechanism (Bahdanau et al., 2014) to iden-
tify semantic alignments between source and target
words. Due to potential errors in these alignments,
the attention-based context vector may be insuffi-
cient to capture the entire meaning of a source sen-
tence, hence resulting in undesirable translation phe-
nomena (Tu et al., 2016).

Unlike the vanilla encoder-decoder framework,
we model underlying semantics of bilingual sen-
tence pairs explicitly. We assume that there exists
a continuous latent variable z from this underlying
semantic space. And this variable, together with x,

1In this paper, we use bold symbols to denote variables, and
plain symbols to denote their values. Without specific state-
ment, all variables are multivariate.

521



amssymb amsmath

x y

θφ

N

z

Figure 1: Illustration of VNMT as a directed graph.
We use solid lines to denote the generative model
pθ(z|x)pθ(y|z,x), and dashed lines to denote the varia-
tional approximation qφ(z|x) to the intractable posterior
p(z|x,y). Both variational parameters φ and generative
model parameters θ are learned jointly.

guides the translation process, i.e. p(y|z,x). With
this assumption, the original conditional probability
evolves into the following formulation:

p(y|x) =
∫

z
p(y, z|x)dz =

∫

z
p(y|z,x)p(z|x)dz

(1)
This brings in the benefits that the latent variable z
can serve as a global semantic signal that is com-
plementary to the attention-based context vector for
generating good translations when the model learns
undesirable attentions. However, although this la-
tent variable enables us to explicitly model under-
lying semantics of translation pairs, the incorpora-
tion of it into the above probabilistic model has two
challenges: 1) the posterior inference in this model
is intractable; 2) large-scale training, which lays
the ground for the data-driven NMT, is accordingly
problematic.

In order to address these issues, we propose a vari-
ational encoder-decoder model to neural machine
translation (VNMT), motivated by the recent suc-
cess of variational neural models (Rezende et al.,
2014; Kingma and Welling, 2014). Figure 1 illus-
trates the graphic representation of VNMT. As deep
neural networks are capable of learning highly non-
linear functions, we employ them to fit the latent-
variable-related distributions, i.e. the prior and pos-
terior, to make the inference tractable. The former is
modeled to be conditioned on the source side alone
pθ(z|x), because the source and target part of a sen-
tence pair usually share the same semantics so that
the source sentence should contain the prior infor-
mation for inducing the underlying semantics. The
latter, instead, is approximated from all observed
variables qφ(z|x,y), i.e. both the source and the tar-

get sides. In order to efficiently train parameters,
we apply a reparameterization technique (Rezende
et al., 2014; Kingma and Welling, 2014) on the vari-
ational lower bound. This enables us to use standard
stochastic gradient optimization for training the pro-
posed model. Specifically, there are three essential
components in VNMT (The detailed architecture is
illustrated in Figure 2):

• A variational neural encoder transforms
source/target sentence into distributed repre-
sentations, which is the same as the encoder of
NMT (Bahdanau et al., 2014) (see section 3.1).
• A variational neural inferer infers the repre-

sentation of z according to the learned source
representations (i.e. pθ(z|x)) together with the
target ones (i.e. qφ(z|x,y)), where the repa-
rameterization technique is employed (see sec-
tion 3.2).
• And a variational neural decoder integrates the

latent representation of z to guide the genera-
tion of target sentence (i.e. p(y|z,x)) together
with the attention mechanism (see section 3.3).

Augmented with the posterior approximation and
reparameterization, our VNMT can still be trained
end-to-end. This makes our model not only effi-
cient in translation, but also simple in implementa-
tion. To train our model, we employ the conven-
tional maximum likelihood estimation. Experiments
on both Chinese-English and English-German trans-
lation tasks show that VNMT achieves significant
improvements over several strong baselines.

2 Background: Variational Autoencoder

This section briefly reviews the variational autoen-
coder (VAE) (Kingma and Welling, 2014; Rezende
et al., 2014). Given an observed variable x, VAE in-
troduces a continuous latent variable z, and assumes
that x is generated from z, i.e.,

pθ(x, z) = pθ(x|z)pθ(z) (2)
where θ denotes the parameters of the model. pθ(z)
is the prior, e.g, a simple Gaussian distribution.
pθ(x|z) is the conditional distribution that models
the generation procedure, typically estimated via a
deep non-linear neural network.

Similar to our model, the integration of z in Eq.
(2) imposes challenges on the posterior inference as

522



reparameterization

hz

h′z

h′e

log σ2µ

hf

s3s2s1s0

y0 y1 y2 y3

⊕
α2,1

α2,2 α2,3 α2,4

(a) Variational Neural Encoder

(c) Variational Neural Decoder

(b) Variational Neural Inferer

mean-pooling−→
h1

←−
h1

−→
h2

←−
h2

−→
h3

←−
h3

←−
h4

−→
h4

x4x3x2x1
mean-pooling

y1 y2 y3

−→
h3

←−
h3

−→
h2

←−
h2

−→
h1

←−
h1

he

Figure 2: Neural architecture of VNMT. We use blue, gray and red color to indicate the encoder-related (x,y), under-
lying semantic (z) and decoder-related (y) representation respectively. The yellow lines show the flow of information
employed for target word prediction. The dashed red line highlights the incorporation of latent variable z into target
prediction. f and e represent the source and target language respectively.

well as large-scale learning. To tackle these prob-
lems, VAE adopts two techniques: neural approxi-
mation and reparameterization.

Neural Approximation employs deep neural net-
works to approximate the posterior inference model
qφ(z|x), where φ denotes the variational parame-
ters. For the posterior approximation, VAE regards
qφ(z|x) as a diagonal GaussianN (µ, diag(σ2)), and
parameterizes its mean µ and variance σ2 with deep
neural networks.

Reparameterization reparameterizes z as a func-
tion of µ and σ, rather than using the standard
sampling method. In practice, VAE leverages the
“location-scale” property of Gaussian distribution,
and uses the following reparameterization:

z̃ = µ+ σ � � (3)

where � is a standard Gaussian variable that plays
a role of introducing noises, and � denotes an
element-wise product.

With these two techniques, VAE tightly incor-
porates both the generative model pθ(x|z) and the
posterior inference model qφ(z|x) into an end-to-
end neural network. This facilitates its optimiza-
tion since we can apply the standard backpropaga-
tion to compute the gradient of the following varia-
tional lower bound:

LVAE(θ, φ;x) =− KL(qφ(z|x)||pθ(z))
+Eqφ(z|x)[log pθ(x|z)] ≤ log pθ(x)

(4)

KL(Q||P ) is the Kullback-Leibler divergence be-
tween Q and P . Intuitively, VAE can be considered

as a regularized version of the standard autoencoder.
It makes use of the latent variable z to capture the
variations � in the observed variable x.

3 Variational Neural Machine Translation

Different from previous work, we introduce a latent
variable z to model the underlying semantic space
as a global signal for translation. Formally, given
the definition in Eq. (1) and Eq. (4), the varia-
tional lower bound of VNMT can be formulated as
follows:

LVNMT(θ, φ;x,y) = −KL(qφ(z|x,y)||pθ(z|x))
+Eqφ(z|x,y)[log pθ(y|z,x)] (5)

where pθ(z|x) is our prior model, qφ(z|x,y) is our
posterior approximator, and pθ(y|z,x) is the de-
coder with the guidance from z. Based on this
formulation, VNMT can be decomposed into three
components, each of which is modeled by a neu-
ral network: a variational neural inferer that models
pθ(z|x) and qφ(z|x,y) (see part (b) in Figure 2), a
variational neural decoder that models pθ(y|z,x)
(see part (c) in Figure 2), and a variational neural
encoder that provides distributed representations of
a source/target sentence for the above two modules
(see part (a) in Figure 2). Following the information
flow illustrated in Figure 2, we describe part (a), (b)
and (c) successively.

3.1 Variational Neural Encoder
As shown in Figure 2 (a), the variational neural en-
coder aims at encoding an input sequence (w1, w2,

523



. . . , wT ) into continuous vectors. In this paper,
we adopt the encoder architecture proposed by Bah-
danau et al. (2014), which is a bidirectional RNN
with a forward and backward RNN. The forward
RNN reads the sequence from left to right while
the backward RNN in the opposite direction (see the
parallel arrows in Figure 2 (a)):

−→
h i = RNN(

−→
h i−1, Ewi)

←−
h i = RNN(

←−
h i+1, Ewi)

(6)

where Ewi ∈ Rdw is the embedding for word wi,
and
−→
h i,
←−
h i are hidden states generated in two direc-

tions. Following Bahdanau et al. (2014), we employ
the Gated Recurrent Unit (GRU) as our RNN unit
due to its capacity in capturing long-distance depen-
dencies.

We further concatenate each pair of hidden states
at each time step to build a set of annotation vec-
tors (h1, h2, . . . , hT ), hTi =

[−→
h Ti ;
←−
h Ti

]
. In this

way, each annotation vector hi encodes information
about the i-th word with respect to all the other sur-
rounding words in the sequence. Therefore, these
annotation vectors are desirable for the following
modeling.

We use this encoder to represent both the source
sentence {xi}Tfi=1 and the target sentence {yi}Tei=1
(see the blue color in Figure 2). Accordingly, our
encoder generates both the source annotation vec-
tors {hi}Tfi=1 ∈ R2df and the target annotation vec-
tors {h′i}Tei=1 ∈ R2de . The source vectors flow into
the inferer and decoder while the target vectors the
posterior approximator.

3.2 Variational Neural Inferer

A major challenge of variational models is how to
model the latent-variable-related distributions. In
VNMT, we employ neural networks to model both
the prior pθ(z|x) and the posterior qφ(z|x,y), and
let them subject to a multivariate Gaussian distri-
bution with a diagonal covariance structure.2 As
shown in Figure 1, these two distributions mainly
differ in their conditions.

2The reasons of choosing Gaussian distribution are twofold:
1) it is a natural choice for modeling continuous variables; 2) it
belongs to the family of “location-scale” distributions, which is
required for the following reparameterization.

3.2.1 Neural Posterior Approximator
Exactly modeling the true posterior p(z|x,y) ex-

actly usually intractable. Therefore, we adopt an
approximation method to simplify the posterior in-
ference. Conventional models typically employ the
mean-field approaches. However, a major limitation
of this approach is its inability to capture the true
posterior of z due to its oversimplification. Follow-
ing the spirit of VAE, we use neural networks for
better approximation in this paper, and assume the
approximator has the following form:

qφ(z|x,y) = N (z;µ(x,y), σ(x,y)2I) (7)

The mean µ and s.d. σ of the approximate poste-
rior are the outputs of neural networks based on the
observed variables x and y as shown in Figure 2 (b).

Starting from the variational neural encoder, we
first obtain the source- and target-side representa-
tion via a mean-pooling operation over the annota-
tion vectors, i.e. hf = 1Tf

∑Tf
i hi, he =

1
Te

∑Te
i h

′
i.

With these representations, we perform a non-linear
transformation that projects them onto our con-
cerned latent semantic space:

h′z = g(W
(1)
z [hf ;he] + b

(1)
z ) (8)

where W (1)z ∈ Rdz×2(df+de), b(1)z ∈ Rdz is the pa-
rameter matrix and bias term respectively, dz is the
dimensionality of the latent space, and g(·) is an
element-wise activation function, which we set to be
tanh(·) throughout our experiments.

In this latent space, we obtain the abovementioned
Gaussian parameters µ and log σ2 through linear re-
gression:

µ =Wµh
′
z + bµ, log σ

2 =Wσh
′
z + bσ (9)

where µ, log σ2 are both dz-dimension vectors.

3.2.2 Neural Prior Model
Different from the posterior, we model (rather

than approximate) the prior as follows:

pθ(z|x) = N (z;µ′(x), σ′(x)2I) (10)

We treat the mean µ′ and s.d. σ′ of the prior as neural
functions of source sentence x alone. This is sound
and reasonable because bilingual sentences are se-
mantically equivalent, suggesting that either y or x

524



is capable of inferring the underlying semantics of
sentence pairs, i.e., the representation of latent vari-
able z.

The neural model for the prior pθ(z|x) is the
same as that (i.e. Eq (8) and (9)) for the posterior
qφ(z|x,y), except for the absence of he. Besides,
the parameters for the prior are independent of those
for the posterior.

To obtain a representation for latent variable z, we
employ the same technique as the Eq. (3) and repa-
rameterized it as hz = µ+ σ � �, �∼N (0, I). Dur-
ing decoding, however, due to the absence of target
sentence y, we set hz to be the mean of pθ(z|x), i.e.,
µ′. Intuitively, the reparameterization bridges the
gap between the generation model pθ(y|z,x) and
the inference model qφ(z|x,y). In other words, it
connects these two neural networks. This is impor-
tant since it enables the stochastic gradient optimiza-
tion via standard backpropagation.

We further project the representation of latent
variable hz onto the target space for translation:

h′e = g(W
(2)
z hz + b

(2)
z ) (11)

where h′e ∈ Rd
′
e . The transformed h′e is then in-

tegrated into our decoder. Notice that because of
the noise from �, the representation h′e is not fixed
for the same source sentence and model parameters.
This is crucial for VNMT to learn to avoid overfit-
ting.

3.3 Variational Neural Decoder
Given the source sentence x and the latent variable
z, our decoder defines the probability over transla-
tion y as a joint probability of ordered conditionals:

p(y|z,x) =
Te∏

j=1

p(yj |y<j , z,x) (12)

where p(yj |y<j ,z,x) = g′(yj−1, sj−1, cj)
The feed forward model g′(·) (see the yellow arrows
in Figure 2) and context vector cj =

∑
i αjihi (see

the “⊕” in Figure 2) are the same as (Bahdanau et
al., 2014). The difference between our decoder and
Bahdanau et al.’s decoder (2014) lies in that in ad-
dition to the context vector, our decoder integrates
the representation of the latent variable, i.e. h′e, into
the computation of sj , which is denoted by the bold
dashed red arrow in Figure 2 (c).

Formally, the hidden state sj in our decoder is cal-
culated by3

sj = (1− uj)� sj−1 + uj � s̃j ,
s̃j = tanh(WEyj + U [rj � sj−1] + Ccj + V h′e)
uj = σ(WuEyj + Uusj−1 + Cucj + Vuh

′
e)

rj = σ(WrEyj + Ursj−1 + Crcj + Vrh
′
e)

Here, rj , uj , s̃j denotes the reset gate, update gate
and candidate activation in GRU respectively, and
Eyj ∈ Rdw is the word embedding for target word.
W, Wu, Wr ∈ Rde×dw , U, Uu, Ur ∈ Rde×de , C, Cu,
Cr ∈ Rde×2df , and V, Vu, Vr ∈ Rde×d′e are parame-
ter weights. The initial hidden state s0 is initialized
in the same way as Bahdanau et al. (2014) (see the
arrow to s0 in Figure 2).

In our model, the latent variable can affect the rep-
resentation of hidden state sj through the gate be-
tween rj and uj . This allows our model to access the
semantic information of z indirectly since the pre-
diction of yj+1 depends on sj . In addition, when the
model learns wrong attentions that lead to bad con-
text vector cj , the semantic representation he′ can
help to guide the translation process .

3.4 Model Training
We use the Monte Carlo method to approximate
the expectation over the posterior in Eq. (5), i.e.
Eqφ(z|x,y)[·] ' 1L

∑L
l=1 log pθ(y|x,h

(l)
z ), whereL is

the number of samples. The joint training objective
for a training instance (x,y) is defined as follows:

L(θ, φ) ' −KL(qφ(z|x,y)||pθ(z|x))

+
1

L

L∑

l=1

Te∑

j=1

log pθ(yj |y<j ,x,h(l)z ) (13)

where h(l)z = µ+ σ � �(l) and �(l) ∼ N (0, I)

The first term is the KL divergence between two
Gaussian distributions which can be computed and
differentiated without estimation (see (Kingma and
Welling, 2014) for details). And the second term
is the approximate expectation, which is also dif-
ferentiable. Suppose that L is 1 (which is used in
our experiments), then our second term will be de-
generated to the objective of conventional NMT. In-
tuitively, VNMT is exactly a regularized version of

3We omit the bias term for clarity.

525



System MT05 MT02 MT03 MT04 MT06 MT08 AVG
Moses 33.68 34.19 34.39 35.34 29.20 22.94 31.21

GroundHog 31.38 33.32 32.59 35.05 29.80 22.82 30.72
VNMT w/o KL 31.40 33.50 32.92 34.95 28.74 22.07 30.44

VNMT 32.25 34.50++ 33.78++ 36.72⇑++ 30.92⇑++ 24.41↑++ 32.07

Table 1: BLEU scores on the NIST Chinese-English translation task. AVG = average BLEU scores on test sets. We
highlight the best results in bold for each test set. “↑/⇑”: significantly better than Moses (p < 0.05/p < 0.01); “+/++”:
significantly better than GroundHog (p < 0.05/p < 0.01);

NMT, where the introduced noise � increases its ro-
bustness, and reduces overfitting. We verify this
point in our experiments.

Since the objective function in Eq. (13) is differ-
entiable, we can optimize the model parameter θ and
variational parameter φ jointly using standard gradi-
ent ascent techniques.

4 Experiments

4.1 Setup

To evaluate the effectiveness of the proposed
VNMT, we conducted experiments on both Chinese-
English and English-German translation tasks. Our
Chinese-English training data4 consists of 2.9M sen-
tence pairs, with 80.9M Chinese words and 86.4M
English words respectively. We used the NIST
MT05 dataset as the development set, and the NIST
MT02/03/04/06/08 datasets as the test sets for the
Chinese-English task. Our English-German train-
ing data5 consists of 4.5M sentence pairs with 116M
English words and 110M German words6. We used
the newstest2013 (3000 sentences) as the develop-
ment set, and the newstest2014 (2737 sentences)
as the test set for English-German translation. We
employed the case-insensitive BLEU-4 (Papineni et
al., 2002) metric to evaluate translation quality, and
paired bootstrap sampling (Koehn, 2004) for signif-
icance test.

We compared our model against two state-of-the-
art SMT and NMT systems:

• Moses (Koehn et al., 2007): a phrase-based
SMT system.

4This corpus consists of LDC2003E14, LDC2004T07,
LDC2005T06, LDC2005T10 and LDC2004T08 (Hong Kong
Hansards/Laws/News).

5This corpus is from the WMT’14 training data (Jean et al.,
2015; Luong et al., 2015a)

6The preprocessed data can be found and downloaded from
http://nlp.stanford.edu/projects/nmt/

• GroundHog (Bahdanau et al., 2014): an
attention-based NMT system.

Additionally, we also compared with a variant of
VNMT, which does not contain the KL part in the
objective (VNMT w/o KL). This is achieved by set-
ting hz to µ′.

For Moses, we adopted all the default settings ex-
cept for the language model. We trained a 4-gram
language model on the Xinhua section of the English
Gigaword corpus (306M words) using the SRILM7

toolkit with modified Kneser-Ney smoothing. Im-
portantly, we used all words in the vocabulary.

For GroundHog, we set the maximum length
of training sentences to be 50 words, and pre-
served the most frequent 30K (Chinese-English) and
50K (English-German) words as both the source
and target vocabulary , covering approximately
98.9%/99.2% and 97.3%/93.3% on the source and
target side of the two parallel corpora respectively .
All other words were represented by a specific to-
ken “UNK”. Following Bahdanau et al. (2014), we
set dw = 620, df = 1000, de = 1000, and M = 80.
All other settings are the same as the default config-
uration (for RNNSearch). During decoding, we used
the beam-search algorithm, and set beam size to 10.

For VNMT, we initialized its parameters with the
trained RNNSearch model. The settings of our
model are the same as that of GroundHog, except
for some parameters specific to VNMT. Following
VAE, we set the sampling number L = 1. Addi-
tionally, we set d′e = dz = 2df = 2000 according
to preliminary experiments. We used the Adadelta
algorithm for model training with ρ = 0.95. With
regard to the source and target encoders, we shared
their recurrent parameters but not word embeddings.

We implemented our VNMT based on Ground-
Hog8. Both NMT systems are trained on a Telsa K40

7http://www.speech.sri.com/projects/srilm/download.html
8Our code is publicly available at

526



System MT05 MT02 MT03 MT04 MT06 MT08
GroundHog 18.23 22.20 20.19 21.67 19.11 13.41

VNMT 21.31 26.02 23.78 25.81 21.81 15.59

Table 2: BLEU scores on the new dataset. All improvements are significant at p < 0.01.
System Architecture BLEU

Existing end-to-end NMT systems
Jean et al. (2015) RNNSearch 16.46
Jean et al. (2015) RNNSearch + unk replace 18.97
Jean et al. (2015) RNNsearch + unk replace + large vocab 19.40
Luong et al. (2015a) LSTM with 4 layers + dropout + local att. + unk replace 20.90

Our end-to-end NMT systems

this work
RNNSearch 16.40
VNMT 17.13++

VNMT + unk replace 19.58++

Table 3: BLEU scores on the English-German translation task.

5 15 25 35 45 55
20

23

26

29

32

35

Sentence Length

B
L
E
U

S
co

re
s

GroundHog
Our VNMT

Figure 3: BLEU scores on different groups of source
sentences in terms of their length.

GPU. In one hour, GroundHog processes about 1100
batches, while our VNMT processes 630 batches.

4.2 Results on Chinese-English Translation

Table 1 summarizes the BLEU scores of different
systems on the Chinese-English translation tasks.
Clearly VNMT significantly improves translation
quality in terms of BLEU on most cases, and ob-
tains the best average results that gain 0.86 and 1.35
BLEU points over Moses and GroundHog respec-
tively. Besides, without the KL objective, VNMT
w/o KL obtains even worse results than GroundHog.
These results indicate the following two points: 1)
explicitly modeling underlying semantics by a latent
variable indeed benefits neural machine translation,
and 2) the improvements of our model are not from
enlarging the network.

https://github.com/DeepLearnXMU/VNMT.

4.3 Results on Long Sentences

We further testify VNMT on long sentence transla-
tion where the vanilla NMT usually suffers from at-
tention failures (Tu et al., 2016; Bentivogli et al.,
2016). We believe that the global latent variable can
play an important role on long sentence translation.

Our first experiment is carried out on 6 disjoint
groups according to the length of source sentences in
our test sets. Figure 3 shows the BLEU scores of two
neural models. We find that the performance curve
of our VNMT model always appears to be on top of
that of GroundHog with a certain margin. Specif-
ically, on the final group with the longest source
sentences, our VNMT obtains the biggest improve-
ment (3.55 BLEU points). Overall, these obvious
improvements on all groups in terms of the length of
source sentences indicate that the global guidance
from the latent variable benefits our VNMT model.

Our second experiment is carried out on a syn-
thetic dataset where each new source sentence is
a concatenation of neighboring source sentences in
the original test sets. As a result, the average length
of source sentences in the new dataset (> 50) is
almost twice longer than the original one. Trans-
lation results is summarized in Table 2, where our
VNMT obtains significant improvements on all new
test sets. This further demonstrates the advantage of
introducing the latent variable.

4.4 Results on English-German Translation

Table 3 shows the results on English-German trans-
lation. We also provide several existing NMT sys-

527



Source

两国官员确定了今后会谈的日程和模式 ,建立起进行持续对话的机制 ,此举标
志着巴印对话进程在中断两年后重新启动 ,为两国逐步解决包括克什米尔争
端在内的所有悬而未决的问题奠定了基础 ,体现了双方可贵的和平诚意。

Reference

the officials of the two countries have established the mechanism for continued dialogue down
the road, including a confirmed schedule and model of the talks. this symbolizes the restart
of the dialogue process between pakistan and india after an interruption of two years and has
paved a foundation for the two countries to sort out gradually all the questions hanging in the
air, including the kashmir dispute. it is also a realization of their precious sincerity for peace.

Moses

officials of the two countries set the agenda for future talks , and the pattern of a continuing
dialogue mechanism . this marks a break in the process of dialogue between pakistan and india
, two years after the restart of the two countries including kashmir dispute to gradually solve
all the outstanding issues have laid the foundation of the two sides showed great sincerity in
peace .

GroundHog

the two countries have decided to set up a mechanism for conducting continuous dialogue on
the agenda and mode of the talks . this indicates that the ongoing dialogue between the two
countries has laid the foundation for the gradual settlement of all outstanding issues including
the dispute over kashmir .

VNMT

the officials of the two countries set up a mechanism for holding a continuous dialogue on
the agenda and mode of the future talks, and this indicates that the ongoing dialogue between
pakistan and india has laid a foundation for resolving all outstanding issues , including the
kashmir disputes , and this serves as a valuable and sincere peace sincerity .

Table 4: Translation examples of different systems. We highlight important parts in red color.

tems that use the same training, development and
testing data. The results show that VNMT signifi-
cantly outperforms GroundHog and achieves a sig-
nificant gain of 0.73 BLEU points (p < 0.01). With
unknown word replacement (Jean et al., 2015; Lu-
ong et al., 2015a), VNMT reaches the performance
level that is comparable to the previous state-of-the-
art NMT results.

4.5 Translation Analysis

Table 4 shows a translation example that helps un-
derstand the advantage of VNMT over NMT . As
the source sentence in this example is long (more
than 40 words), the translation generated by Moses
is relatively messy and incomprehensible. In con-
trast, translations generated by neural models (both
GroundHog and VNMT) are much more fluent and
comprehensible. However, there are essential differ-
ences between GroundHog and our VNMT. Specifi-
cally, GroundHog does not translate the phrase “官
员” at the beginning of the source sentence. The
translation of the clause “体现了双方可贵的和
平诚意。” at the end of the source sentence is com-
pletely lost. In contrast, our VNMT model does not
miss or mistake these fragments and can convey the
meaning of entire source sentence to the target side.

From these examples, we can find that although

attention networks can help NMT trace back to rel-
evant parts of source sentences for predicting tar-
get translations, capturing the semantics of entire
sentences still remains a big challenge for neural
machine translation. Since NMT implicitly models
variable-length source sentences with fixed-size hid-
den vectors, some details of source sentences (e.g.,
the red sequence of words in Table 4) may not be
encoded in these vectors at all. VNMT seems to be
able to capture these details through a latent vari-
able that explicitly model underlying semantics of
source sentences. The promising results suggest that
VNMT provides a new mechanism to deal with sen-
tence semantics.

5 Related Work

5.1 Neural Machine Translation

Neural machine translation starts from the sequence
to sequence learning, where Sutskever et al. (2014)
employ two multilayered Long Short-Term Memory
(LSTM) models that first encode a source sentence
into a single vector and then decode the translation
word by word until a special end token is gener-
ated. In order to deal with issues caused by encoding
all source-side information into a fixed-length vec-
tor, Bahdanau et al. (2014) introduce attention-based

528



NMT that aims at automatically concentrating on
relevant source parts for predicting target words dur-
ing decoding. The incorporation of attention mech-
anism allows NMT to cope better with long sen-
tences, and makes it really comparable to or even
superior to conventional SMT.

Following the success of attentional NMT, a num-
ber of approaches and models have been proposed
for NMT recently, which can be grouped into differ-
ent categories according to their motivations: deal-
ing with rare words or large vocabulary (Jean et al.,
2015; Luong et al., 2015b; Sennrich et al., 2015),
learning better attentional structures (Luong et al.,
2015a), integrating SMT techniques (Cheng et al.,
2015; Shen et al., 2015; Feng et al., 2016; Tu et al.,
2016), memory network (Meng et al., 2015), etc. All
these models are designed within the discriminative
encoder-decoder framework, leaving the explicit ex-
ploration of underlying semantics with a variational
model an open problem.

5.2 Variational Neural Model
In order to perform efficient inference and learn-
ing in directed probabilistic models on large-scale
dataset, Kingma and Welling (2014) as well as
Rezende et al. (2014) introduce variational neural
networks. Typically, these models utilize an neural
inference model to approximate the intractable pos-
terior, and optimize model parameters jointly with a
reparameterized variational lower bound using the
standard stochastic gradient technique. This ap-
proach is of growing interest due to its success in
various tasks.

Kingma et al. (2014) revisit the approach to semi-
supervised learning with generative models and fur-
ther develop new models that allow effective gen-
eralization from a small labeled dataset to a large
unlabeled dataset. Chung et al. (2015) incorporate
latent variables into the hidden state of a recurrent
neural network, while Gregor et al. (2015) combine
a novel spatial attention mechanism that mimics the
foveation of human eyes, with a sequential varia-
tional auto-encoding framework that allows the it-
erative construction of complex images. Very re-
cently, Miao et al. (2015) propose a generic varia-
tional inference framework for generative and con-
ditional models of text.

The most related work is that of Bowman et

al. (2015), where they develop a variational autoen-
coder for unsupervised generative language model-
ing. The major difference is that they focus on the
monolingual language model, while we adapt this
technique to bilingual translation. Although varia-
tional neural models have been widely used in NLP
tasks and the variational decoding has been investi-
gated for SMT (Li et al., 2009), the adaptation and
utilization of variational neural model to neural ma-
chine translation, to the best of our knowledge, has
never been investigated before.

6 Conclusion and Future Work

In this paper, we have presented a variational model
for neural machine translation that incorporates a
continuous latent variable to model the underlying
semantics of sentence pairs. We approximate the
posterior distribution with neural networks and repa-
rameterize the variational lower bound. This en-
ables our model to be an end-to-end neural network
that can be optimized through the stochastic gradi-
ent algorithms. Comparing with the conventional
attention-based NMT, our model is better at trans-
lating long sentences. It also greatly benefits from
a special regularization term brought with this la-
tent variable. Experiments on Chinese-English and
English-German translation tasks verified the effec-
tiveness of our model.

In the future, since the latent variable in our
model is at the sentence level, we want to explore
more fine-grained latent variables for neural ma-
chine translation, such as the Recurrent Latent Vari-
able Model (Chung et al., 2015). We are also inter-
ested in applying our model to other similar tasks.

Acknowledgments

The authors were supported by National Nat-
ural Science Foundation of China (Grant Nos
61303082, 61672440, 61622209 and 61403269),
Natural Science Foundation of Fujian Province
(Grant No. 2016J05161), Natural Science Founda-
tion of Jiangsu Province (Grant No. BK20140355),
and Research fund of the Provincial Key Laboratory
for Computer Information Processing Technology in
Soochow University (Grant No. KJS1520). We also
thank the anonymous reviewers for their insightful
comments.

529



References
Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben-

gio. 2014. Neural machine translation by jointly
learning to align and translate. In Proc. of ICLR.

L. Bentivogli, A. Bisazza, M. Cettolo, and M. Federico.
2016. Neural versus Phrase-Based Machine Transla-
tion Quality: a Case Study. ArXiv e-prints, August.

S. R. Bowman, L. Vilnis, O. Vinyals, A. M. Dai, R. Joze-
fowicz, and S. Bengio. 2015. Generating Sentences
from a Continuous Space. ArXiv e-prints, November.

Y. Cheng, S. Shen, Z. He, W. He, H. Wu, M. Sun, and
Y. Liu. 2015. Agreement-based Joint Training for
Bidirectional Attention-based Neural Machine Trans-
lation. ArXiv e-prints, December.

Kyunghyun Cho, Bart van Merrienboer, Caglar Gulcehre,
Dzmitry Bahdanau, Fethi Bougares, Holger Schwenk,
and Yoshua Bengio. 2014. Learning phrase represen-
tations using rnn encoder–decoder for statistical ma-
chine translation. In Proc. of EMNLP, pages 1724–
1734, October.

Junyoung Chung, Kyle Kastner, Laurent Dinh, Kratarth
Goel, Aaron C. Courville, and Yoshua Bengio. 2015.
A recurrent latent variable model for sequential data.
In Proc. of NIPS.

S. Feng, S. Liu, M. Li, and M. Zhou. 2016. Implicit
Distortion and Fertility Models for Attention-based
Encoder-Decoder NMT Model. ArXiv e-prints, Jan-
uary.

Karol Gregor, Ivo Danihelka, Alex Graves, and Daan
Wierstra. 2015. DRAW: A recurrent neural network
for image generation. CoRR, abs/1502.04623.

Sébastien Jean, Kyunghyun Cho, Roland Memisevic, and
Yoshua Bengio. 2015. On using very large target vo-
cabulary for neural machine translation. In Proc. of
ACL-IJCNLP, pages 1–10, July.

Nal Kalchbrenner and Phil Blunsom. 2013. Recurrent
continuous translation models. In Proc. of EMNLP,
pages 1700–1709, October.

Diederik P Kingma and Max Welling. 2014. Auto-
Encoding Variational Bayes. In Proc. of ICLR.

Diederik P. Kingma, Shakir Mohamed, Danilo Jimenez
Rezende, and Max Welling. 2014. Semi-supervised
learning with deep generative models. In Proc. of
NIPS, pages 3581–3589.

Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran, Richard
Zens, Chris Dyer, Ondřej Bojar, Alexandra Con-
stantin, and Evan Herbst. 2007. Moses: Open source
toolkit for statistical machine translation. In Proc. of
ACL, pages 177–180.

Philipp Koehn. 2004. Statistical significance tests for
machine translation evaluation. In Proc. of EMNLP.

Zhifei Li, Jason Eisner, and Sanjeev Khudanpur. 2009.
Variational decoding for statistical machine transla-
tion. In Proc. of ACL, pages 593–601, August.

Thang Luong, Hieu Pham, and Christopher D. Manning.
2015a. Effective approaches to attention-based neural
machine translation. In Proc. of EMNLP, pages 1412–
1421, September.

Thang Luong, Ilya Sutskever, Quoc Le, Oriol Vinyals,
and Wojciech Zaremba. 2015b. Addressing the rare
word problem in neural machine translation. In Proc.
of ACL-IJCNLP, pages 11–19, July.

F. Meng, Z. Lu, Z. Tu, H. Li, and Q. Liu. 2015.
A Deep Memory-based Architecture for Sequence-to-
Sequence Learning. ArXiv e-prints, June.

Y. Miao, L. Yu, and P. Blunsom. 2015. Neural Varia-
tional Inference for Text Processing. ArXiv e-prints,
November.

Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic eval-
uation of machine translation. In Proc. of ACL, pages
311–318.

Danilo Jimenez Rezende, Shakir Mohamed, and Daan
Wierstra. 2014. Stochastic backpropagation and ap-
proximate inference in deep generative models. In
Proc. of ICML, pages 1278–1286.

R. Sennrich, B. Haddow, and A. Birch. 2015. Neu-
ral Machine Translation of Rare Words with Subword
Units. ArXiv e-prints, August.

S. Shen, Y. Cheng, Z. He, W. He, H. Wu, M. Sun, and
Y. Liu. 2015. Minimum Risk Training for Neural Ma-
chine Translation. ArXiv e-prints, December.

Ilya Sutskever, Oriol Vinyals, and Quoc V. Le. 2014.
Sequence to sequence learning with neural networks.
CoRR, abs/1409.3215.

Zhaopeng Tu, Zhengdong Lu, Yang Liu, Xiaohua Liu,
and Hang Li. 2016. Coverage-based neural machine
translation. CoRR, abs/1601.04811.

530


