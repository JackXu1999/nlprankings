















































A Systematic Classification of Knowledge, Reasoning, and Context within the ARC Dataset


Proceedings of the Workshop on Machine Reading for Question Answering, pages 60–70
Melbourne, Australia, July 19, 2018. c©2018 Association for Computational Linguistics

60

A Systematic Classification of Knowledge, Reasoning, and Context
within the ARC Dataset

Michael Boratko, Harshit Padigela, Divyendra Mikkilineni, Pritish Yuvraj,
Rajarshi Das, Andrew McCallum

College of Information and Computer Sciences
University of Massachusetts, Amherst MA

Maria Chang, Achille Fokoue-Nkoutche, Pavan Kapanipathi, Nicholas Mattei,
Ryan Musa, Kartik Talamadupula, Michael Witbrock

IBM Research, Yorktown Heights NY

Abstract

The recent work of Clark et al. (2018)
introduces the AI2 Reasoning Challenge
(ARC) and the associated ARC dataset
that partitions open domain, complex sci-
ence questions into an Easy Set and a
Challenge Set. That paper includes an
analysis of 100 questions with respect to
the types of knowledge and reasoning re-
quired to answer them; however, it does
not include clear definitions of these types,
nor does it offer information about the
quality of the labels. We propose a com-
prehensive set of definitions of knowledge
and reasoning types necessary for answer-
ing the questions in the ARC dataset. Us-
ing ten annotators and a sophisticated an-
notation interface, we analyze the distri-
bution of labels across the Challenge Set
and statistics related to them. Addition-
ally, we demonstrate that although naive
information retrieval methods return sen-
tences that are irrelevant to answering
the query, sufficient supporting text is of-
ten present in the (ARC) corpus. Eval-
uating with human-selected relevant sen-
tences improves the performance of a neu-
ral machine comprehension model by 42
points.

1 Introduction

The recent work of Clark et al. (2018) introduces
the AI2 Reasoning Challenge (ARC)1 and the as-
sociated ARC dataset. This dataset contains sci-
ence questions from standardized tests that are
separated into an Easy Set and a Challenge Set.
The Challenge Set is comprised of questions that
are answered incorrectly by two solvers based on

1http://data.allenai.org/arc/

Pointwise Mutual Information (PMI) Information
Retrieval (IR). In addition to this division, a survey
of the various types of knowledge as well as the
types of reasoning that are required to answer var-
ious questions in the ARC dataset was presented.
This survey was based on an analysis of 100 ques-
tions chosen at random from the Challenge Set.
However, very little detail is provided about the
questions chosen, the annotations provided, or the
methodology used. These questions surround the
very core of the paper, since the main contribution
is a dataset that contains complex questions.

Additionally, while their manual analysis sug-
gests that 95% of the questions can be answered
using the ARC corpus, Clark et al. (2018) note that
the IR system (Elasticsearch2) serves as a severe
bottleneck. Our annotation process supports this
observation, but we also find that simple reformu-
lations to the query can greatly increase the quality
of the retrieved sentences.

Contributions: In this work, in order to overcome
some of the limitations of Clark et al. (2018) de-
scribed above, we present a detailed annotation
process for the ARC dataset. Specifically, we (a)
introduce a novel labeling interface that allows a
distributed set of annotators to label the knowledge
and reasoning types; and (b) improve upon the
knowledge and reasoning type categories provided
previously, in order to make the annotation more
intuitive and accurate. Following an annotation
round involving over ten people at two institutions,
we measure and report statistics such as inter-rater
agreement, and the distribution of knowledge and
reasoning type labels in the dataset. We then (c)
clarify the role of knowledge and reasoning within
the ARC dataset with a comprehensive set of an-
notations for both the questions and returned re-
sults, demonstrating the efficacy of query refine-

2https://www.elastic.co/products/elasticsearch



61

ment to improve existing QA systems. Our anno-
tators were also asked to mark whether individ-
ual retrieved sentences were relevant to answering
a given question. Our labeling interface logs the
reformulated queries issued by each annotator, as
well as their relevance annotations. To quantita-
tively demonstrate the effectiveness of the relevant
sentences, we (d) evaluate a subset of questions
and the relevant retrieval results with a pre-trained
DrQA model (Chen et al., 2017), and find that the
performance of the system increases by 42 points.

2 Related Work

To explore reading comprehension as a research
problem, Hirschman et al. (1999) manually cre-
ated a dataset of 3rd and 6th grade reading com-
prehension questions with short answers. The
techniques that were explored for this dataset in-
cluded pattern matching, rules, and logistic re-
gression. More such datasets have been created
that include natural language questions: for in-
stance, MCTest (Richardson et al., 2013). MCTest
is crowdsourced and comprises of 660 elementary-
level childrens fictional stories, which are the
source of questions and multiple choice answers.
Questions and answers were constructed with a re-
strictive vocabulary that a 7 year-old could under-
stand. Half of the questions constructed necessi-
tated the answer to be derived from two sentences,
with the motivation being to encourage research
in multi-hop (one-hop) reasoning. Recent tech-
niques such as Wang et al. (2015) and Yin et al.
(2016) have performed well on this dataset. Cur-
rently, SQuAD (Rajpurkar et al., 2016) is one of
the most popular datasets for reading comprehen-
sion: it uses Wikipedia passages as its source, and
question-answer pairs are created using crowd-
sourcing. While it is stated that SQuAD requires
logical reasoning, the complexity of reasoning re-
quired is far less than that for the AI2 standardized
tests dataset (Clark and Etzioni, 2016; Kembhavi
et al., 2017). NewsQA (Trischler et al., 2016) is
another dataset that was created using crowdsourc-
ing; it utilizes passages from 10,000 news articles
to create questions.

Most of the datasets mentioned above are closed
domain, where the answer exists in a given snippet
of text. On the other hand, in the open domain set-
ting, the question-answer datasets are constructed
to encompass the whole pipeline for question-
answering, starting with the retrieval of relevant

documents. SearchQA (Dunn et al., 2017) is an
effort to create such a dataset; it contains 140K
question-answer (QA) pairs. While the motivation
was to create an open domain dataset, SearchQA
provides text that contains ‘evidence’ (a set of an-
notated search results) and hence falls short of be-
ing a complete open-domain QA dataset. Trivi-
aQA (Joshi et al., 2017) is another reading com-
prehension dataset that contains 650K QA pairs
with evidence.

Datasets created from standardized science tests
offer some of the few existing examples of ques-
tions that require exploration of complex reason-
ing techniques to find solutions. A number of
science-question focused datasets have been re-
leased over the past few years. The AI2 Science
Questions dataset was introduced by Clark (2015)
along with the Aristo Framework, which we build
off of. This dataset contains over 1,000 multi-
ple choice questions from state and federal sci-
ence questions for elementary and middle school
students.3 A survey of the knowledge base re-
quirements for accomplishing this task was per-
formed by Clark et al. (2013), and concluded
that advanced inference methods were necessary
for many of the questions, as they could not
be answered by simple fact-based retrieval. The
SciQ Dataset (Welbl et al., 2017) contains 13,679
crowdsourced multiple choice science questions.
To construct this dataset, workers were shown a
passage and asked to construct a question along
with correct and incorrect answer options. The
dataset contains both the source passage as well
as the question and answer options.

3 ARC Dataset Annotations

In previous work (Clark et al., 2018), the stan-
dardized test questions under consideration are
split into various categories based on the kinds of
knowledge and reasoning that are needed to an-
swer those questions. The idea of classifying ques-
tions by these two types is central to the notion
of standardized testing, which endeavors to test
students on various kinds of knowledge, as well
as various problem types and solution techniques.
In accordance with this, Clark et al. provide pre-
liminary definitions for knowledge and reasoning
categories that can be employed by a QA system
to solve a given question. These categories allow

3This dataset can be downloaded at http://data.
allenai.org/ai2-science-questions/



62

Knowledge Label Instructions Example Question

Definition

A question should be labeled definition if it requires you to
know the definition of a term and use only that definition to
answer the question.

Recall is necessary to answer the question. Examples in-
clude questions that require direct recall, questions that re-
quire picking an exemplar or element from a set.

What is a worldwide increase in temperature called?
(A) greenhouse effect
(B) global warming
(C) ozone depletion
(D) solar heating

Basic Facts

A question should be labeled basic facts if you would require
basic facts or properties about a term that would not neces-
sarily capture the full textbook definition.

Examples include, how many earth rotations are in a day and
what the advantage of large family sizes are for animals.

Which element makes up most of the air we breathe?
(A) carbon
(B) nitrogen
(C) oxygen
(D) argon

Causes & Processes
A question should be labeled causes if answering it requires
recognizing an ordered process (two or more sequential, re-
lated events) described in the question.

What is the first step of the process in the formation of sedi-
mentary rocks?
(A) erosion
(B) deposition
(C) compaction
(D) cementation

Purpose
A question should be labeled purpose if answering it requires
understanding the function of one or more terms in the ques-
tion/answer.

What is the main function of the circulatory system?
(A) secrete enzymes
(B) digest proteins
(C) produce hormones
(D) transport materials

Algebraic

A question should be labeled algebraic if answering it re-
quires any kind of numerical calculation, including basic
arithmetic, solving equations (including equations retrieved
from a knowledge base, e.g. a physics equation like F =m ·a),
etc.

A 72 W navigation unit on a commercial aircraft has a 24 V
power supply and uses 3 A of electric current. What is the
electrical resistance of the navigation unit?
(A) 4 ohms
(B) 8 ohms
(C) 13 ohms
(D) 22 ohms

Experiments Questions about the Scientific Method, laboratory experi-ments, and/or experimental best practices.

Scientists perform experiments to test hypotheses. How do
scientists try to remain objective during experiments?
(A) Scientists analyze all results.
(B) Scientists use safety precautions.
(C) Scientists conduct experiments once.
(D) Scientists change at least two variables.

Physical Model
Any question that refers to a spatial / kinematic / physical
relationship between entities and likely requires a model of
the physical world in order to be answered.

What most likely happens when a cold air mass comes into
contact with a warm, moist air mass?
(A) The sky becomes clear.
(B) Rain or snow begins to fall.
(C) Cold air is pushed to high altitudes.
(D) Warm air is pushed to ground level.

Table 1: Knowledge type definitions and examples given to the annotators.

for the classification of questions, which makes it
easier to partition them into sets to measure per-
formance and improve solution strategies. In this
work, we present an interface (c.f. Section 4) and
annotation rules that seek to turn this classification
of questions into a systematic process. In this sec-
tion, we first discuss the classification types and
associated annotation rules.

3.1 Knowledge Types
In most question-answering (QA) scenarios, the
knowledge that is present with the system (or the
agent) determines whether a given question can be
answered. The full list of the revised knowledge
labels (types) – along with the instructions given
to annotators and respective exemplars from the

ARC question set – is given in Table 1. The la-
belers were given the following instructions at the
beginning of the annotation process:

You are to answer the question, “In a perfect
world given an ideal knowledge source, what
types of knowledge would you as a human need
to answer this question?” You are allowed to se-
lect multiple labels for this type which will be
recorded as an ordered list. You are to assign la-
bels in the order of importance to answering the
questions at hand.

The wording of the paragraph above is quite de-
liberate. First, we make the non-trivial point that
the kind of knowledge that is available determines
the reasoning type to be employed, and eventually
whether the given question can be answered or
not. For example, the question:



63

Reasoning Label Instructions Example Question

Question Logic

Questions which only make sense in the context of a multiple-
choice question. That is, absent the choices, the question
makes no sense; after being provided the choices, the ques-
tion contains (nearly) all necessary information to answer the
question. If taking away the answer options makes it impos-
sible to answer, then it is question logic.

Example: pick the element of a set that does not belong, pick
one of a grouping, etc.

Which item below is not made from a material grown in na-
ture?
(A) a cotton shirt
(B) a wooden chair
(C) a plastic spoon
(D) a grass basket

Linguistic Matching

Any question that requires aligning a question with retrieved
results (sentences or facts). This can be paired with multihop
and other reasoning types where the facts that are retrieved
need to be aligned with the particular questions.

This often goes with knowledge types like questions and ba-
sic facts in the case that the retrieved results do not perfectly
align with the answer choices.

Which of the following best describes a mineral?
(A) the main nutrient in all foods
(B) a type of grain found in cereals
(C) a natural substance that makes up rocks
(D) the decomposed plant matter found in soil

Causal / Explanation

Given the facts retrieved from web sentences or another rea-
sonable corpus the answer can be extrapolated from a single
fact or element. This category also includes single hop causal
processes and scenarios where the question asks in the form
“What is the most likely result of X happening?”

Example: if you need to explain this to a 10 year old, it would
require one statement of fact to explain.

Why can steam be used to cook food?
(A) Steam does work on objects.
(B) Steam is a form of water.
(C) Steam can transfer heat to cooler objects.
(D) Steam is able to move through small spaces.

Multihop Reasoning

Given the facts retrieved from web sentences or another rea-
sonable corpus it requires at least two or more distinct pieces
of evidence. This category also includes multi hop causal pro-
cesses and scenarios where the question asks in the form “if
X happens what is the result on Y?”

Example: if you need to explain this to a 10 year old, it would
require at least two distinct factual statements.

Which property of a mineral can be determined just by look-
ing at it?
(A) luster
(B) mass
(C) weight
(D) hardness

Hypothetical / Counterfactual

Any question that requires reasoning about or applying ab-
stract facts to a hypothetical/scenario situation that is de-
scribed in the question. In some cases the hypotheticals are
described in the answer options.

A hypothetical / counterfactual is an entity/scenario that may
not be not mentioned as a fact in the corpus, e.g. “..a gray
squirrel gave birth to a ...”, “When lemon juice is added to
water..”

If the Sun were larger, what would most likely also have to be
true for Earth to sustain life?
(A) Earth would have to be further from the Sun.
(B) Earth would have to be closer to the Sun.
(C) Earth would have to be smaller.
(D) Earth would have to be larger.

Comparison

Comparison questions ask to compare one or more enti-
ties/classes found in the question and/or answers along one
or more dimensions.

This includes questions like “what is the most likely place to
find water?” and ”what is the brightest star in our galaxy?”

Compared to the Sun, a red star most likely has a greater
(A) volume.
(B) rate of rotation.
(C) surface temperature.
(D) number of orbiting planets

Algebraic

A question should be labeled algebraic if answering it re-
quires any kind of numerical calculation, including basic
arithmetic, solving equations (including those retrieved from
a knowledge base, e.g. a physics equation like F = m ·a), etc.

A 72 W navigation unit on a commercial aircraft has a 24 V
power supply and uses 3 A of electric current. What is the
electrical resistance of the navigation unit?
(A) 4 ohms
(B) 8 ohms
(C) 13 ohms
(D) 22 ohms

Physical Model
Any question that refers to a physical / spatial / kinematic
relationship between entities and likely requires a model of
the physical world in order to be answered.

Where will a sidewalk feel hottest on a warm, clear day?
(A) Under a picnic table
(B) In direct sunlight
(C) Under a puddle
(D) In the shade

Analogy Is a direct statement of analogy.

Inside cells, special molecules carry messages from the mem-
brane to the nucleus. Which body system uses a similar pro-
cess?
(A) endocrine system
(B) lymphatic system
(C) excretory system
(D) integumentary system

Table 2: Reasoning type definitions and examples given to the annotators.



64

Giant redwood trees change energy from one form to
another. How is energy changed by the trees?
(A) They change chemical energy into kinetic energy.

(B) They change solar energy into chemical energy.

(C) They change wind energy into heat energy.

(D) They change mechanical energy into solar energy.

can be answered using two different kinds of
reasoning depending on the knowledge retrieved:

(1) Trees change solar energy into chemical energy: Lin-
guistic Reasoning
(2a) Solar energy is changed into chemical energy by plants;

(2b) Trees are classified as plants: Multi-hop Reasoning

In order to level the field among annotators,
we include the phrasing about an ideal knowl-
edge source. Additionally, displaying the retrieved
search results in the interface provides another
way for the annotators to share some common
ground with respect to the typical kind of knowl-
edge that is likely to be available – in this case,
from the ARC corpus.

In comparison to the knowledge types provided
by Clark et al. (2018), we make the following
changes. First – and most important – we provide
instruction-based definitions for each class, as op-
posed to the single exemplars provided previously.
We believe this greatly simplifies the annotation
task for new annotators, since they no longer need
to perform a preliminary manual analysis of the
QA set in order to understand the distinctions be-
tween the classes. Second, we completely elimi-
nate the Structure type – this is a very specific type
of knowledge, and we believe it is not represented
in any significant percentage in the current ARC
QA set. Third, we rename some of the labels to
bring them more in line with the specific proper-
ties of the knowledge that they are describing – for
example, spatial / kinematic is renamed to Physi-
cal Model in our table.

3.2 Reasoning Types
The analysis of reasoning types with an eye to-
wards annotation follows a similar pattern to the
knowledge types described in the previous section.
Table 2 shows the reasoning labels and classifica-
tion rules that we used for labeling the dataset. The
annotators were given the following instructions:

You are to answer the question, “What types
of reasoning or problem solving would a
competent student with access to Wikipedia need

to answer this question?” You are allowed to
select multiple labels for this type which will be
recorded as an ordered list. You are to assign
labels in the order of importance to answering
the questions at hand.

You may use the search results to help dif-
ferentiate between the linguistic and multi-hop
reasoning types. Any label other than these
should take precedence if they apply. For
example, a question that requires using a mathe-
matical formula along with linguistic matching
should be labeled algebraic, linguistic.

Notice that the instructions in this case refer to be-
ing able to access a specific knowledge corpus,
and allow for the selection of multiple labels in
decreasing order of applicability. We also provide
specific instructions on the order of precedence
as relates to linguistic and multi-hop reasoning
types: this is based on our empirical observation
that many questions can be classified trivially into
these reasoning categories, and we would prefer
(for downstream application use) a clean split into
as many distinct categories as possible.

4 Labeling Interface

The labeling interface is shown in Figure 1. The
text of the question is displayed at the top of the
left side, followed by the answer options. Each of
the answer options is preceded by a radio button:
each button is initially transparent, but the anno-
tator can click on a button to check whether the
corresponding option is the answer to the ques-
tion. This facility is to help annotators with extra
information if it is needed in labeling the question;
however, we leave it blank initially to avoid bias-
ing the annotations.

Clicking on a specific answer option runs a
search on the ARC corpus, with the query text set
to the last sentence of the question appended with
the entire text of the clicked answer option. The
retrieved search results are shown in the bottom
left half of the interface. Annotators have the op-
tion of labeling retrieved search results as irrele-
vant or relevant to answering the question at hand.
The query box also accepts free text, and annota-
tors who wish to craft more specific queries are
free to do so. We collect all the queries executed,
as well as the relevant/irrelevant annotations.

The right hand side of the interface deals with
the labeling process itself. There are two boxes for
annotating knowledge and reasoning types respec-
tively. The labels are populated from Table 1 and
Table 2. The annotator can also provide optional



65

Figure 1: A screenshot of the interface to our labeling system, described in Section 4.

information on the quality of the retrieved search
results if they chose to run a query. Finally, the an-
notator can use the optional field below quality to
enter additional notes about the question which are
stored and can be retrieved for subsequent discus-
sion and refinement of the labels.

5 Human Annotated Search Results

In addition to labeling the knowledge and reason-
ing types systematically, we demonstrate yet an-
other capability of our interface: given a corpus
of knowledge, we are able to retrieve and display
search results that may be relevant to the question
(and its corresponding options) at hand. This is
useful because it gives a solution technique an ad-
ditional signal as it tries to identify the correct an-
swer to a given question. In open-domain question
answering, the retriever plays as important a role
as the machine reader (Chen et al., 2017). In the
past few years, there has been a lot of effort in de-
signing sophisticated neural architectures for read-

ing a small piece of text (e.g. paragraph) (Wang
and Jiang, 2016; Xiong et al., 2016; Seo et al.,
2016; Lee et al., 2016, inter alia). However, most
work in open domain settings (Chen et al., 2017;
Clark and Gardner, 2017; Wang et al., 2018) only
uses a simple retriever (such as TF-IDF based). As
a result, there is a notable decrease in the perfor-
mance of the QA system. One roadblock for train-
ing a sophisticated retriever is the lack of available
training data which annotates the relevance of a
retrieved context with respect to the question. We
believe our annotated retrieval data can be used to
train a better ranker/retriever.

The underlying retriever in our interface is a
simple Elasticsearch, similar to the one used by
Clark et al. (2018). The interface is populated
by default with the top ranked sentences that
are retrieved with the given question as the in-
put query. However, we noticed that results thus
retrieved were often irrelevant to answering the
question. To address this, our labeling interface



66

also allows annotators to input their own custom
queries. We found that reformulating the initial
query significantly improved the quality of the re-
trieved context (results). While not the main fo-
cus of this work, we encouraged the annotators
to mark the contexts (results) that they thought
were relevant to answering the question at hand.
For example, in Figure 1, the annotator came up
with a novel query – ‘metals are solid
at room temperatures’ – and also marked
the relevant sentences which are needed to answer
this question. Note that sometimes we need to rea-
son over multiple sentences to arrive at the answer.
For example, the question in Figure 1 can be an-
swered by combining the first and third sentences
in the ‘Relevant Results’ tab.

To quantitatively measure the efficacy of the an-
notated context, we evaluated 47 questions and
their respective human-annotated relevant sen-
tences with a pretrained DrQA model (Chen et al.,
2017). We compared this to a baseline which only
returned the sentences retrieved by using the text
of the question plus given options as input queries.
Since DrQA returns a span from the input sen-
tences, we picked the multiple choice option that
maximally overlapped with the returned answer
span. Our baseline results are 7 correct out of 47
questions. With the annotated context, the perfor-
mance increased to 27 correctly answered ques-
tions - a 42% increase in accuracy. Encouraged by
these results, we posit that the community should
focus a lot of attention on improving the retrieval
portions of the various QA systems available; we
think that annotated context will certainly help in
training a better ranker.

6 Results

Each of the team members were given access to
the labeling interface (which includes the ques-
tion, answers, query search results and more infor-
mation as described above). Each annotator was
shown the questions in a random order, and was
allowed to skip or pass any question.

Statistics. We collected labels from at least 3
unique annotators (out of the possible 10) for 192
distinct questions. This labeling process produced
1.42 knowledge type labels and 1.7 reasoning type
labels per question. Figure 2 and Figure 3 shows
the distribution of annotation labels by all raters
at any position. While Basic Facts dominates the
knowledge type labels, there is no clear cut con-

ba
sic

 fa
cts

cau
ses

de
fin

itio
n

ex
pe

rim
en

ts

ph
ysi

cal

pu
rpo

se

alg
eb

rai
c

Knowledge Type

0

50

100

150

200

250

300

350

To
ta

l C
ou

nt

Figure 2: Histogram of the first (most important) knowledge
label for each question; the Y-axis refers to annotations.

qn 
log

ic

ling
uist

ic

exp
lan

atio
n

hyp
oth

etic
al

com
par

ison

mu
ltih

op
phy

sica
l

alg
ebr

aic
ana

log
y

Reasoning Type

0

50

100

150

200

Co
un

t A
ny

 P
os

iti
on

Figure 3: Histogram of the first (most important) reasoning
label for each question; the Y-axis refers to annotations.

sensus for the reasoning type. Indeed, qn logic,
linguistic, and explanation occur most frequently.

Inter-Rater Agreement. A comprehensive look
at the labels and inter-rater agreement can be
found in Table 3 and Table 4. Fleiss’ κ is of-
ten used to measure inter-rater agreement (Cohen,
1995). Informally, this measures the amount of
agreement, beyond chance, based on the number
of raters, objects and classes. κ > 0.2 is typically
taken to denote good agreement between raters,
while a negative value means that there was lit-
tle to no agreement. Since Fleiss’ κ is only de-
fined for a single set of labels, we consider only
the first (most important) label for each question
in the statistic we report.



67

In addition to Fleiss’ κ we also use the Kemeny
voting rule (Kemeny, 1959) to measure the con-
sensus by the annotators. The Kemeny voting rule
minimizes the Kendall Tau (Kendall, 1938) (flip)
distance between the output ordering and the or-
dering of all annotators. One theory of voting (ag-
gregation) is that there is a true or correct ordering
and all voters provide a noisy observation of the
ground truth. This method of thinking is largely
credited to Condorcet (de Caritat, 1785; Young,
1988) and there is recent work in characterizing
other voting rules as maximum likelihood estima-
tors (MLEs) (Conitzer et al., 2009). The Kemeny
voting rule is the MLE of the Condorcet Noise
Model, in which pairwise inversions of the pref-
erence order happen uniformly at random (Young,
1988, 1995). Hence, if we assume all annotators
make pairwise errors uniformly at random then
Kemeny is the MLE of label orders they report.

Label Appears Majority Consensus

basic facts 125 69 28
algebraic 13 5 2
definition 52 16 5

causes 78 33 15
experiments 35 19 13

purpose 30 13 0
physical 21 3 1

Fleiss’ κ = 0.342

Table 3: Pairwise inter-rater agreement along with the mean
and Fleiss’ κ for survey responses.

Label Appears Majority Consensus

linguistic 66 31 8
algebraic 15 8 3

explanation 80 22 4
hypothetical 62 21 6

multihop 45 6 0
comparison 46 13 3

qn logic 78 33 2
physical 18 3 0
analogy 4 1 1

Fleiss’ κ =−0.683

Table 4: Pairwise inter-rater agreement along with the mean
and Fleiss’ κ for survey responses.

Knowledge Labels. We achieve κ = 0.342, which
means that our raters did a good job of indepen-
dently agreeing on the types of knowledge re-
quired to answer the questions. The mean Kemeny
score of the consensus ranking for each question is
2.57, meaning on average there are less than three
flips required to get from the consensus ranking
to each of the annotators’ rankings. The most fre-
quent label in the first position was basic facts,
followed by causes. Overall, there was a reason-
able amount of consensus between the raters for
knowledge type: 64/192 questions had a consensus
amongst all the raters. Taken together, our results
on knowledge type indicate that most questions

deal with basic facts, causes, and definitions; and
that labeling can be done reliably.

Reasoning Labels. The reasoning labels tell a
very different story from the knowledge labels.
The agreement was κ = −0.683, which indicates
that raters did not agree above chance on their la-
bels. Strong evidence for this comes from the fact
that only 27/192 questions had a consensus label.
This may be due to the fact that we allow multi-
ple labels, and the annotators simply disagree on
the order of the labels. However, the score of the
consensus ranking for each question is 6.57, which
indicates that on average the ordering of the labels
is quite far apart.

Considering the histogram in Figure 3, we see
that qn logic, linguistic, and explanation are the
most frequent label types; this may indicate that
getting better at understanding the questions them-
selves could lead to a big boost for reasoners. For
Figure 4, we have merged the first and second la-
bel (if present) for all annotators. Now, the set of
all possible labels is all singletons as well as all
pairs of labels. Comparing this histogram to the
one in Figure 3, we see that while linguistic and
explanation remain somewhat unchanged, the qn
logic label becomes very spread out across the
types. This is more support for our hypothesis that
annotators may be disagreeing on the ordering of
the labels, rather than the content itself.

linguistic
explanation

hypothetical/qn logic
explanation/qn logic

comparison
comparison/qn logic

linguistic/qn logic
multihop

explanation/hypothetical
qn logic

hypothetical
explanation/linguistic
hypothetical/multihop

algebraic
multihop/qn logic

Re
as

on
in

g 
Ty

pe

0 20 40 60 80
Count 1st/2nd Combined

Figure 4: Histogram of the reasoning labels when we com-
bine the first and (if present) second label of every annotator.
The count refers to annotations.

Baseline Performance. Using the Kemeny vot-
ing rule to partition the set of questions based on
their top reasoning and knowledge labels, we eval-
uate the performance of several baseline systems:



68

Label (#) Text Search word2vec SemanticILP DecompAttn DGEM BiDAF

R
ea

so
ni

ng
Ty

pe
s

qn logic (63) 14.3 25.4 22.2 16.3 19.8 23.4
linguistic (38) 26.3 21.1 28.9 26.3 29.6 34.9

hypothetical (27) 18.5 29.6 18.5 29.4 33.3 22.0
explanation (25) 12.0 28.0 12.0 25.0 29.3 26.3

multihop (19) 21.1 15.8 15.8 22.4 32.9 25.9
comparison (11) 18.2 9.1 18.2 29.5 13.6 47.0

algebraic (8) 37.5 0.0 25.0 0.0 3.1 15.6
physical (7) 14.3 14.3 14.3 28.6 35.7 17.9
analogy (1) 0.0 0.0 100.0 0.0 0.0 0.0

K
no

w
le

dg
e

Ty
pe

s basic facts (78) 19.2 20.5 29.5 21.8 27.4 29.3
causes (39) 23.1 25.6 12.8 23.7 25.0 32.7

experiments (25) 4.0 16.0 8.0 21.8 22.0 24.1
definition (24) 16.7 29.2 16.7 26.0 34.4 16.7

purpose (21) 19.0 28.6 23.8 27.4 25.0 27.4
algebraic (6) 50.0 0.0 50.0 0.0 0.0 16.7
physical (6) 16.7 16.7 0.0 4.2 8.3 12.5

Overall acc. 18.2 21.7 20.7 21.7 24.9 26.5

Table 5: Accuracy on our subset of ARC Challenge Set questions, partitioned based on the first label from the Kemeny ordering
of the reasoning and knowledge type annotations, respectively; 1/k partial credit is given when the correct answer is in the set
of k selected answers. The number of questions assigned each primary label is indicated by (#).

word2vec similarity (Mikolov et al., 2013),
Text Search based on the scores assigned by
Elasticsearch over the Aristo-mini corpus, and
the SemanticILP system that uses constrained
search over semantically-motivated graph struc-
tures (Khashabi et al., 2018). Additionally, we also
test the three pre-trained neural baselines released
by (Clark et al., 2018): DecompAttn, the De-
composable Attention natural language inference
model (Parikh et al., 2016); DGEM, the Decompos-
able Graph Entailment Model (Khot et al., 2018);
and BiDAF, the Bi-Directional Attention Flow
reading comprehension model (Seo et al., 2016).

Results are shown in Table 5. While none of the
systems approach human performance on any of
the categories, they do illustrate some particular
difficulties of the ARC Challenge Set that moti-
vate our future work. Specifically, all techniques
perform at or below chance on questions that pri-
marily require qn logic reasoning. Unremarkable
performance on the popular basic facts and causes
knowledge types also illustrates the shortcomings
of sophisticated language processing systems that
still rely on basic text retrieval. Bridging this gap
appears to be a requirement for making progress
on this and similar datasets.

7 Conclusion & Future Work

In this paper, we introduce a novel annotation in-
terface and define annotation instructions for the
knowledge and reasoning type labels that are used
for question analysis for standardized tests. We an-

notate approximately 200 questions from the ARC
Challenge Set shared by AI2 with the types of
knowledge and reasoning required to answer the
respective questions. Each question has at least 3
annotators, with high agreement on the require-
ments for knowledge type. While standard base-
lines do not perform significantly better on any of
the different subsets of questions (partitioned by
type), we offer a preliminary demonstration that
search annotations collected through our interface
can significantly improve the performance of state
of the art systems. We will leverage the knowl-
edge and reasoning type annotations, as well as the
search annotations, to improve the performance of
QA systems. We will also release these annota-
tions to the community to complement the ARC
Dataset.

Acknowledgments
We would like to thank Salim Roukos for helpful
suggestions on the annotation process, and Daniel
Khashabi for assistance with SemanticILP.
This work is funded in part by International Busi-
ness Machines Corporation Cognitive Horizons
Network agreement number W1668553 and The
Center for Data Science.



69

References
M. J. A. N. de Caritat. 1785. Essai sur l’application

de l’analyse à la probabilité des décisions: rendues
à la pluralité des voix. Paris: L’Imprimerie Royale.

Danqi Chen, Adam Fisch, Jason Weston, and An-
toine Bordes. 2017. Reading wikipedia to an-
swer open-domain questions. arXiv preprint
arXiv:1704.00051.

Christopher Clark and Matt Gardner. 2017. Simple
and effective multi-paragraph reading comprehen-
sion. arXiv preprint arXiv:1710.10723.

P. Clark. 2015. Elementary school science and math
tests as a driver for AI: Take the Aristo Challenge!
In Proceedings of the 27th Innovative Applications
of Artificial Intelligence (IAAI), pages 4019–4021.

P. Clark, I. Cowhey, O. Etzioni, T. Khot, A. Sabhar-
wal, C. Schoenick, and O. Tafjord. 2018. Think you
have solved question answering? Try ARC, the AI2
Reasning Challenge. In ArXiv e-prints 1803.05457.

P. Clark, P. Harrison, and N. Balasubramanian. 2013. A
study of the knowledge base requirements for pass-
ing an elementary science test. In Proceedings of the
2013 Workshop on Applied Knowledge Base Con-
struction (AKBC), pages 37–42.

Peter Clark and Oren Etzioni. 2016. My computer is an
honor studentbut how intelligent is it? standardized
tests as a measure of ai. AI Magazine, 37(1):5–12.

P. R. Cohen. 1995. Empirical Methods for Artificial
Intelligence. MIT Press.

V. Conitzer, M. Rognlie, and L. Xia. 2009. Preference
functions that score rankings and maximum likeli-
hood estimation. In Proceedings of the 21st Inter-
national Joint Conference on Artificial Intelligence
(IJCAI), pages 109–115.

Matthew Dunn, Levent Sagun, Mike Higgins, Ugur
Guney, Volkan Cirik, and Kyunghyun Cho. 2017.
Searchqa: A new q&a dataset augmented with
context from a search engine. arXiv preprint
arXiv:1704.05179.

Lynette Hirschman, Marc Light, Eric Breck, and
John D Burger. 1999. Deep read: A reading compre-
hension system. In Proceedings of the 37th annual
meeting of the Association for Computational Lin-
guistics on Computational Linguistics, pages 325–
332. Association for Computational Linguistics.

Mandar Joshi, Eunsol Choi, Daniel S Weld, and Luke
Zettlemoyer. 2017. Triviaqa: A large scale distantly
supervised challenge dataset for reading comprehen-
sion. arXiv preprint arXiv:1705.03551.

A. Kembhavi, M. Seo, D. Schwenk, J. Choi,
A. Farhadi, and H. Hajishirzi. 2017. Are you
smarter than a sixth grader? textbook question an-
swering for multimodal machine comprehension. In
2017 IEEE Conference on Computer Vision and Pat-
tern Recognition (CVPR), pages 5376–5384.

J. G. Kemeny. 1959. Mathematics without numbers.
Daedalus, 88(4):577–591.

M. G. Kendall. 1938. A new measure of rank correla-
tion. Biometrika, 30(1/2):81–93.

D. Khashabi, T. Khot, A. Sabharwal, and D. Roth.
2018. Question answering as global reasoning over
semantic abstractions. In Proceedings of the 32nd
AAAI Conference on Artificial Intelligence (AAAI).

Tushar Khot, Ashish Sabharwal, and Peter Clark. 2018.
SciTail: A textual entailment dataset from science
question answering. In AAAI.

Kenton Lee, Shimi Salant, Tom Kwiatkowski, Ankur
Parikh, Dipanjan Das, and Jonathan Berant. 2016.
Learning recurrent span representations for ex-
tractive question answering. arXiv preprint
arXiv:1611.01436.

Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Cor-
rado, and Jeff Dean. 2013. Distributed representa-
tions of words and phrases and their compositional-
ity. In Advances in Neural Information Processing
Systems, pages 3111–3119.

Ankur Parikh, Oscar Täckström, Dipanjan Das, and
Jakob Uszkoreit. 2016. A decomposable attention
model for natural language inference. In Proceed-
ings of the 2016 Conference on Empirical Methods
in Natural Language Processing, pages 2249–2255,
Austin, Texas. Association for Computational Lin-
guistics.

Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and
Percy Liang. 2016. Squad: 100,000+ questions for
machine comprehension of text. In Proceedings of
the 2016 Conference on Empirical Methods in Nat-
ural Language Processing, pages 2383–2392.

Matthew Richardson, Christopher JC Burges, and Erin
Renshaw. 2013. Mctest: A challenge dataset for
the open-domain machine comprehension of text.
In Proceedings of the 2013 Conference on Empiri-
cal Methods in Natural Language Processing, pages
193–203.

Minjoon Seo, Aniruddha Kembhavi, Ali Farhadi, and
Hannaneh Hajishirzi. 2016. Bi-directional attention
flow for machine comprehension. arXiv preprint
arXiv:1611.01603.

Adam Trischler, Tong Wang, Xingdi Yuan, Justin Har-
ris, Alessandro Sordoni, Philip Bachman, and Ka-
heer Suleman. 2016. Newsqa: A machine compre-
hension dataset. arXiv preprint arXiv:1611.09830.

Hai Wang, Mohit Bansal, Kevin Gimpel, and David
McAllester. 2015. Machine comprehension with
syntax, frames, and semantics. In Proceedings of the
53rd Annual Meeting of the Association for Compu-
tational Linguistics and the 7th International Joint
Conference on Natural Language Processing (Vol-
ume 2: Short Papers), volume 2, pages 700–706.



70

Shuohang Wang and Jing Jiang. 2016. Machine com-
prehension using match-lstm and answer pointer.
arXiv preprint arXiv:1608.07905.

Shuohang Wang, Mo Yu, Xiaoxiao Guo, Zhiguo Wang,
Tim Klinger, Wei Zhang, Shiyu Chang, Gerald
Tesauro, Bowen Zhou, and Jing Jiang. 2018. R3:
Reinforced ranker-reader for open-domain question
answering.

J. Welbl, N. F. Liu, and M. Gardner. 2017. Crowd-
sourcing multiple choice science questions. In
Proceedings of the 3rd Workshop on Noisy User-
generated Text at Annual Meeting of the Association
for Computational Linguistics, pages 94–106.

Caiming Xiong, Victor Zhong, and Richard Socher.
2016. Dynamic coattention networks for question
answering. arXiv preprint arXiv:1611.01604.

Wenpeng Yin, Sebastian Ebert, and Hinrich Schütze.
2016. Attention-based convolutional neural net-
work for machine comprehension. arXiv preprint
arXiv:1602.04341.

H. P. Young. 1988. Condorcet’s theory of voting. The
American Political Science Review, 82(4):1231 –
1244.

H. P. Young. 1995. Optimal voting rules. The Journal
of Economic Perspectives, 9(1):51–64.


