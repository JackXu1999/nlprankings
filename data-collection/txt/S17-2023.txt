



















































Neobility at SemEval-2017 Task 1: An Attention-based Sentence Similarity Model


Proceedings of the 11th International Workshop on Semantic Evaluations (SemEval-2017), pages 164–169,
Vancouver, Canada, August 3 - 4, 2017. c©2017 Association for Computational Linguistics

Neobility at SemEval-2017 Task 1: An Attention-based Sentence
Similarity Model

WenLi Zhuang ∗
Shan-Si Elementary School
ChangHua County, Taiwan
bibo9901@gmail.com

Ernie Chang
Department of Linguistics
University of Washington
Seattle, WA 98195, USA
cyc025@uw.edu

Abstract

This paper describes a neural-network
model which performed competitively
(top 6) at the SemEval 2017 cross-lingual
Semantic Textual Similarity (STS) task.
Our system employs an attention-based
recurrent neural network model that op-
timizes the sentence similarity. In this
paper, we describe our participation in
the multilingual STS task which measures
similarity across English, Spanish, and
Arabic.

1 Introduction

Semantic textual similarity (STS) measures the
degree of equivalence between the meanings of
two text sequences (Agirre et al., 2016). The
similarity of the text pair can be represented as
discrete or continuous values ranging from irrel-
evance (1) to exact semantic equivalence (5). It
is widely applicable to many NLP tasks includ-
ing summarization (Wong et al., 2008; Nenkova
et al., 2011), generation and question answering
(Vo et al., 2015), paraphrase detection (Fernando
and Stevenson, 2008), and machine translation
(Corley and Mihalcea, 2005).

In this paper, we describe a system that is able
to learn context-sensitive features within the sen-
tences. Further, we encode the sequential informa-
tion with Recurrent Neural Network (RNN) and
perform attention mechanism (Bahdanau et al.,
2015) on RNN outputs for both sentences. Atten-
tion mechanism was performed to increase sen-
sitivity of the system to words of similarity sig-
nificance. We also optimize directly on the Pear-
son correlation score as part of our neural-based
approach. Moreover, we include a pair feature

∗The author is currently serving in his Alternative Mili-
tary Service of Education.

adapter module that could be used to include more
features to further improve performance. How-
ever, for this competition we include merely the
TakeLab features (Šarić et al., 2012). 1

2 Related Works

Most proposed approaches in the past adopted
a hybrid of varying text unit sizes ranging from
character-based, token-based, to knowledge-based
similarity measure (Gomaa and Fahmy, 2013).
The linguistic depths of these measures often vary
between lexical, syntactic, and semantic levels.

Most solutions include an ensemble of modules
that employs features coming from different unit
sizes and depths. More recent approaches gen-
erally include the word embedding-based similar-
ity (Liebeck et al., 2016; Brychcı́n and Svoboda,
2016) as a component of the final ensemble. The
top performing team in 2016 (Rychalska et al.,
2016) uses an ensemble of multiple modules in-
cluding recursive autoencoders with WordNet and
monolingual aligner (Sultan et al., 2016). UMD-
TTIC-UW (He et al., 2016) proposes the MPCNN
model that requires no feature engineering and
managed to perform competitively at the 6th place.
MPCNN is able to extract the hidden information
using the Convolutional Neural Network (CNN)
and adds an attention layer to extract the vital sim-
ilarity information.

3 Methods

3.1 Model

Given two sentences I1 = {w11, w12, ..., w1n1} and
I2 = {w21, w22, ..., w2n2}, where wij denote the jth
token of the ith sentence, embedded using a func-
tion φ that maps each token to a D-dimension
trainable vector. Two sentences are encoded with

1Our system and data is available at
github.com/iamalbert/semval2017task1.

164



Figure 1: Illustration of model architecture

an attentitve RNN to obtain sentence embeddings
u1, u2, respectively.

Sentence Encoder For each sentence, the RNN
firstly converts wij into x

i
j ∈ R2H , using an bidi-

rectional Gated Recurrent Unit (GRU) (Cho et al.,
2014) 2 by sequentially feeding wij into the unit,
in both forward and backward directions. The su-
perscripts of w, x, a, u, n are omitted for clear no-
tation.

xi = [xFi ;x
B
i ]

xFi = GRU(x
F
i−1, wi)

xBi = GRU(x
B
i+1, wi)

(1)

Then, we attend each word xj for different
salience aj and blend the memories x1;n into sen-
tence embedding u:

aj ∝ exp(rT tanh(Wxi))

u =
n∑

j=1

ajxj
(2)

where W ∈ R2H×2H and r ∈ R2H are trainable
parameters.

Surface Features Inspired by the simple system
described in (Šarić et al., 2012), We also extract
surface features from the sentence pair as follow-
ing:

•Ngram Overlap Similarity: These are features
drawn from external knowledge like Word-
Net (Miller, 1995) and Wikipedia. We
use both PathLen similarity (Leacock and
Chodorow, 1998) and Lin similarity (Lin
et al., 1998) to compute similarity between
pairs of words w1i and w

2
j in I1 and I2, re-

spectively. We employ the suggested pre-
processing step (Šarić et al., 2012), and add

2 We also explored Longer Short-Term Memory (LSTM),
but find it more overfitting than GRU.

both WordNet and corpus-based information
to ngram overlap scores, which is obtained
with the harmonic mean of the degree of
overlap between the sentences.

•Semantic Sentence Similarity: We also com-
pute token-based alignment overlap and vec-
tor space sentence similarity (Šarić et al.,
2012). Semantic alignment similarity is com-
puted greedily between all pairs of tokens
using both the knowledge-based and corpus-
based similarity. Scores are further enhanced
with the aligned pair information. We obtain
the weighted form of latent semantic analy-
sis vectors (Turney and Pantel, 2010) for each
word w, before computing the cosine similar-
ity. As such, sentence similarity scores are
enhanced with corpus-based information for
tokens. The features are concatenated into a
vector, denoted as m.

Scoring Let S be a descrete random variable
over {0, 1, ..., 4, 5} describing the similarity of the
given sentence pair {I1, I2}. The representation of
the given pair is the concatenation of u1, u2, and
m, which is fed into an MLP with one hidden layer
to calculate the estimated distribution of S.

p =


P (S = 0)
P (S = 1)

...
P (S = 5)


= softmax(V tanh(U

 u1u2
m

)) (3)
Therefore, the score y is the expected value of

165



S:

y = E[S] =
5∑

i=0

iP (S = i) = vT p (4)

, where v = [0, 1, 2, 3, 4, 5]T . The entire system is
shown in Figure 1.

3.2 Word Embedding
We explore initializing word embeddings ran-
domly or with pre-trained word2vec (Mikolov
et al., 2013) of dimension 50, 100, 300, respec-
tively. We found that the system works the best
with 300-dimension word2vec embeddings.

3.3 Optimization
Let pn, yn be the predicted probability density and
expected score and ŷn be the annotated gold score
of the n-th sample. Most of the previous learning-
based models are trained to minimize the follow-
ing objectives on a batch of N samples:

• Negative Log-likelihood (NLL) of p and p̂
(Aker et al., 2016). The task is viewed as a
classification problem for 6 classes.

LNLL =
N∑

n=1

− log pntn

, where tn is ŷn rounded to the nearest inte-
ger.

• Mean square error (MSE) between yn and ŷn
(Brychcı́n and Svoboda, 2016).

LMSE =
1
N

N∑
n=1

(yn − ŷn)2

• Kullback-Leibler divergence (KLD) of pn
and gold distribution p̂n estimated by ŷn:

LKLD =
N∑

n=1

(
6∑

i=1

p̂ni log
p̂ni
pni

)

where

p̂ni =


ŷn − bŷnc, if i = bŷnc+ 1
bŷnc+ 1− ŷn, if i = bŷnc
0, otherwise

(Li and Huang, 2016; Tai et al., 2015). For
each n, there exists some k such that p̂nk = 1
and ∀h 6= k, p̂nh = 0, KLD is identical to
NLL.

However, the evaluation metric of this task is
Pearson Correlation Coefficient (PCC), which is
invariant to changes in location and scale of yn but
none of the above objectives can reflect it. Here we
use an example to illustrate that MSE and KLD
can even report an inverse tendency. In Table 1,
group A has lower MSE and KLD loss than group
B, but its PCC is also lower.

To solve this problem, we train the model to
maximize PCC directly. Hence, the loss function
is given by:

LPCC = −
∑N

n=1(y
n − ȳ)(ŷn − ¯̂y)√∑N

n=1(yn − ȳ)2
√∑N

n=1(ŷn − ¯̂y)2
(5)

where ȳ = 1N
∑N

n=1 y
n and ¯̂y = 1N

∑N
n=1 ŷ

n.
Since N is fixed for every batch, LPCC is differ-
entiable with respect to yn, which means we can
apply back propagation to train the network. To
the best of our knowledge, we are the first team to
adopt this training objective.

Group A B
Gold Score 3 4 5 3 4 5
P (S = 0) 0.05 0.05 0.05 0.15 0.05 0.1
P (S = 1) 0.05 0.05 0.05 0.3 0.2 0.1
P (S = 2) 0.15 0.1 0.05 0.25 0.3 0.2
P (S = 3) 0.5 0.35 0.0 0.1 0.25 0.3
P (S = 4) 0.15 0.4 0.1 0.1 0.1 0.2
P (S = 5) 0.1 0.05 0.7 0.1 0.1 0.1
E[S] 2.95 3.15 4.2 2.0 2.45 2.7

MSE KLD PCC MSE KLD PCC
0.455 1.966 0.931 2.90 6.91 0.987

Table 1: Example of lower MSE and KLD not
indicating higher PCC.

4 Evaluation

4.1 Data

Dataset Pairs
Training 22,401

Validation 5,601

Table 2: Training and validation Data sets (STS
2012-2016 and SICK).

We gather dataset from SICK (Marelli et al.,
2014) and past STS across years 2012, 2013,
2014, 2015, and 2016 (Agirre et al., 2012, 2013,
2014, 2015, 2016) for both cross-lingual and
monolingual subtasks. We shuffle and split them
according to the ratio 80:20 into training set and

166



validation set, respectively. Table 2 indicates the
size of training set and validation set. All non-
English sentence appearing in training, validation,
and test set are translated into English with Google
Cloud Translation API.

4.2 Experiments
In the experiment, the size of output of GRU is
set to be H = 200. We use ADAM algorithm to
optimize the parameters with mini-batches of 125.
The learning rate is set to 10−4 at the beginning
and reduced by half for every 5 epochs. We trained
the network for 15 epochs.

Word embeddings In Table 3, we demonstrate
that the system performs better with pretrained
word vectors (WI) than randomly initialized (RI).

D PCC on validation set

RI
50 0.7904
300 0.8091

WI
50 0.7974
300 0.8174

Table 3: System performance with different di-
mensions of word embeddings, using either ran-
domly initialized or pre-trained word embedding.

Loss function We display performances with
systems optimized with KLD, MSE, and PCC. It
shows that when using LPCC as the training ob-
jective, our system not only performs the best but
also converges the fastest. As shown in Table 4
and Figure 2.

Loss function PCC
LKLD 0.6839
LMSE 0.7863
LPCC 0.8174

Table 4: Influence of different loss objectives on
the system performance measured using PCC on
our validation set.

4.3 Final System Results
We tune the model on validation set, and select the
set of hyper-parameters that yields the best perfor-
mance to obtain the scores of test data. We report
the official provisional results in Table 5. There
is an obvious performance drop in track4b, which
happens to all teams. We hypothesize that the sen-
tences in track4b (en es) are collected from a spe-
cial domain, due to the fact that the number of

Figure 2: Performance of different loss functions

out-of-vocabulary words in track 4b is many times
more than that in other tracks.

Track PCC mean median max
Primary 0.6171 0.66 0 28

1 0.6821 0.53 0 3
2 0.6459 0.50 0 3
3 0.7928 0.35 0 4
4a 0.7169 0.35 0 4
4b 0.0200 2.54 2 28
5 0.7927 0.36 0 4
6 0.6696 0.33 0 5

Table 5: Final system results and statistics of the
number of OOV words within a pair

5 Conclusion

We propose a simple neural-based system with a
novel means of optimization. We adopt a simple
neural network with surface features which leads
to a promising performance. We also revise sev-
eral popular training objectives and empirically
show that optimizing directly on Pearson’s corre-
lation coefficient achieved the best scores and per-
form competitively on STS-2017.

References
Eneko Agirre, Carmen Banea, Claire Cardie, Daniel

Cer, Mona Diab, Aitor Gonzalez-Agirre, Wei-
wei Guo, Inigo Lopez-Gazpio, Montse Maritxalar,
Rada Mihalcea, German Rigau, Larraitz Uria, and
Janyce Wiebe. 2015. Semeval-2015 task 2: Se-
mantic textual similarity, english, spanish and pi-
lot on interpretability. In Proceedings of the
9th International Workshop on Semantic Evalua-
tion (SemEval 2015). Association for Computa-
tional Linguistics, Denver, Colorado, pages 252–
263. http://www.aclweb.org/anthology/S15-2045.

167



Eneko Agirre, Carmen Banea, Claire Cardie, Daniel
Cer, Mona Diab, Aitor Gonzalez-Agirre, Wei-
wei Guo, Rada Mihalcea, German Rigau, and
Janyce Wiebe. 2014. Semeval-2014 task 10:
Multilingual semantic textual similarity. In Pro-
ceedings of the 8th International Workshop on
Semantic Evaluation (SemEval 2014). Associa-
tion for Computational Linguistics and Dublin
City University, Dublin, Ireland, pages 81–91.
http://www.aclweb.org/anthology/S14-2010.

Eneko Agirre, Carmen Banea, Daniel Cer, Mona
Diab, Aitor Gonzalez-Agirre, Rada Mihalcea, Ger-
man Rigau, and Janyce Wiebe. 2016. Semeval-
2016 task 1: Semantic textual similarity, mono-
lingual and cross-lingual evaluation. In Proceed-
ings of the 10th International Workshop on Se-
mantic Evaluation. Association for Computational
Linguistics, San Diego, California, pages 497–511.
http://www.aclweb.org/anthology/S16-1081.

Eneko Agirre, Daniel Cer, Mona Diab, and Aitor
Gonzalez-Agirre. 2012. Semeval-2012 task 6: A
pilot on semantic textual similarity. In *SEM
2012: The First Joint Conference on Lexical
and Computational Semantics – Volume 1: Pro-
ceedings of the main conference and the shared
task, and Volume 2: Proceedings of the Sixth
International Workshop on Semantic Evaluation
(SemEval 2012). Association for Computational
Linguistics, Montréal, Canada, pages 385–393.
http://www.aclweb.org/anthology/S12-1051.

Eneko Agirre, Daniel Cer, Mona Diab, Aitor Gonzalez-
Agirre, and Weiwei Guo. 2013. *sem 2013 shared
task: Semantic textual similarity. In Second Joint
Conference on Lexical and Computational Seman-
tics (*SEM), Volume 1: Proceedings of the Main
Conference and the Shared Task: Semantic Tex-
tual Similarity. Association for Computational Lin-
guistics, Atlanta, Georgia, USA, pages 32–43.
http://www.aclweb.org/anthology/S13-1004.

Ahmet Aker, Frederic Blain, Andres Duque, Marina
Fomicheva, Jurica Seva, Kashif Shah, and Daniel
Beck. 2016. Usfd at semeval-2016 task 1: Putting
different state-of-the-arts into a box. In Proceed-
ings of the 10th International Workshop on Semantic
Evaluation (SemEval-2016). Association for Com-
putational Linguistics, San Diego, California, pages
609–613. http://www.aclweb.org/anthology/S16-
1092.

Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben-
gio. 2015. Neural machine translation by jointly
learning to align and translate. In Proc. ICLR.

Tomáš Brychcı́n and Lukáš Svoboda. 2016. Uwb
at semeval-2016 task 1: Semantic textual sim-
ilarity using lexical, syntactic, and semantic in-
formation. In Proceedings of the 10th Interna-
tional Workshop on Semantic Evaluation (SemEval-
2016). Association for Computational Linguis-
tics, San Diego, California, pages 588–594.
http://www.aclweb.org/anthology/S16-1089.

Kyunghyun Cho, Bart van Merrienboer, Caglar Gul-
cehre, Dzmitry Bahdanau, Fethi Bougares, Holger
Schwenk, and Yoshua Bengio. 2014. Learning
phrase representations using rnn encoder–decoder
for statistical machine translation. In Proceedings of
the 2014 Conference on Empirical Methods in Nat-
ural Language Processing (EMNLP). Association
for Computational Linguistics, Doha, Qatar, pages
1724–1734. http://www.aclweb.org/anthology/D14-
1179.

Courtney Corley and Rada Mihalcea. 2005. Measur-
ing the semantic similarity of texts. In Proceedings
of the ACL workshop on empirical modeling of se-
mantic equivalence and entailment. Association for
Computational Linguistics, pages 13–18.

Samuel Fernando and Mark Stevenson. 2008. A se-
mantic similarity approach to paraphrase detection.
In Proceedings of the 11th Annual Research Collo-
quium of the UK Special Interest Group for Compu-
tational Linguistics. Citeseer, pages 45–52.

Wael H Gomaa and Aly A Fahmy. 2013. A survey of
text similarity approaches. International Journal of
Computer Applications 68(13).

Hua He, John Wieting, Kevin Gimpel, Jinfeng Rao, and
Jimmy Lin. 2016. Umd-ttic-uw at semeval-2016
task 1: Attention-based multi-perspective convolu-
tional neural networks for textual similarity mea-
surement. In Proceedings of the 10th Interna-
tional Workshop on Semantic Evaluation (SemEval-
2016). Association for Computational Linguis-
tics, San Diego, California, pages 1103–1108.
http://www.aclweb.org/anthology/S16-1170.

Claudia Leacock and Martin Chodorow. 1998. Com-
bining local context and wordnet similarity for word
sense identification. WordNet: An electronic lexical
database 49(2):265–283.

Peng Li and Heng Huang. 2016. Uta dlnlp at semeval-
2016 task 1: Semantic textual similarity: A unified
framework for semantic processing and evaluation.
In Proceedings of the 10th International Workshop
on Semantic Evaluation (SemEval-2016). Associa-
tion for Computational Linguistics, pages 584–587.
https://doi.org/10.18653/v1/S16-1088.

Matthias Liebeck, Philipp Pollack, Pashutan Modaresi,
and Stefan Conrad. 2016. Hhu at semeval-2016
task 1: Multiple approaches to measuring se-
mantic textual similarity. In Proceedings of the
10th International Workshop on Semantic Evalu-
ation (SemEval-2016). Association for Computa-
tional Linguistics, San Diego, California, pages
595–601. http://www.aclweb.org/anthology/S16-
1090.

Dekang Lin et al. 1998. An information-theoretic def-
inition of similarity. In ICML. Citeseer, volume 98,
pages 296–304.

168



Marco Marelli, Stefano Menini, Marco Baroni, Luisa
Bentivogli, Raffaella Bernardi, and Roberto Zam-
parelli. 2014. A sick cure for the evaluation of com-
positional distributional semantic models. In Nico-
letta Calzolari (Conference Chair), Khalid Choukri,
Thierry Declerck, Hrafn Loftsson, Bente Maegaard,
Joseph Mariani, Asuncion Moreno, Jan Odijk, and
Stelios Piperidis, editors, Proceedings of the Ninth
International Conference on Language Resources
and Evaluation (LREC’14). European Language Re-
sources Association (ELRA), Reykjavik, Iceland.

Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Cor-
rado, and Jeff Dean. 2013. Distributed representa-
tions of words and phrases and their compositional-
ity. In Advances in neural information processing
systems. pages 3111–3119.

George A Miller. 1995. Wordnet: a lexical database for
english. Communications of the ACM 38(11):39–
41.

Ani Nenkova, Kathleen McKeown, et al. 2011. Auto-
matic summarization. Foundations and Trends R© in
Information Retrieval 5(2–3):103–233.

Barbara Rychalska, Katarzyna Pakulska, Krystyna
Chodorowska, Wojciech Walczak, and Piotr An-
druszkiewicz. 2016. Samsung poland nlp team at
semeval-2016 task 1: Necessity for diversity; com-
bining recursive autoencoders, wordnet and ensem-
ble methods to measure semantic similarity. In
Proceedings of the 10th International Workshop
on Semantic Evaluation (SemEval-2016). Associa-
tion for Computational Linguistics, pages 602–608.
https://doi.org/10.18653/v1/S16-1091.

Frane Šarić, Goran Glavaš, Mladen Karan, Jan Šnajder,
and Bojana Dalbelo Bašić. 2012. Takelab: Systems
for measuring semantic text similarity. In *SEM
2012: The First Joint Conference on Lexical and
Computational Semantics – Volume 1: Proceedings
of the main conference and the shared task, and Vol-
ume 2: Proceedings of the Sixth International Work-
shop on Semantic Evaluation (SemEval 2012). As-
sociation for Computational Linguistics, pages 441–
448. http://aclweb.org/anthology/S12-1060.

Arafat Md Sultan, Steven Bethard, and Tamara Sum-
ner. 2016. Dls$@$cu at semeval-2016 task 1:
Supervised models of sentence similarity. In
Proceedings of the 10th International Workshop
on Semantic Evaluation (SemEval-2016). Associa-
tion for Computational Linguistics, pages 650–655.
https://doi.org/10.18653/v1/S16-1099.

Kai Sheng Tai, Richard Socher, and Christopher D.
Manning. 2015. Improved semantic representa-
tions from tree-structured long short-term mem-
ory networks. In Proceedings of the 53rd An-
nual Meeting of the Association for Computa-
tional Linguistics and the 7th International Joint
Conference on Natural Language Processing (Vol-
ume 1: Long Papers). Association for Compu-
tational Linguistics, Beijing, China, pages 1556–
1566. http://www.aclweb.org/anthology/P15-1150.

Peter D Turney and Patrick Pantel. 2010. From fre-
quency to meaning: Vector space models of se-
mantics. Journal of artificial intelligence research
37:141–188.

Ngoc Phuoc An Vo, Simone Magnolini, and Octavian
Popescu. 2015. Fbk-hlt: An application of semantic
textual similarity for answer selection in community
question answering. In Proceedings of the 9th In-
ternational Workshop on Semantic Evaluation, Se-
mEval. volume 15, pages 231–235.

Frane Šarić, Goran Glavaš, Mladen Karan, Jan Šnajder,
and Bojana Dalbelo Bašić. 2012. Takelab: Sys-
tems for measuring semantic text similarity. In
*SEM 2012: The First Joint Conference on Lexi-
cal and Computational Semantics – Volume 1: Pro-
ceedings of the main conference and the shared
task, and Volume 2: Proceedings of the Sixth
International Workshop on Semantic Evaluation
(SemEval 2012). Association for Computational
Linguistics, Montréal, Canada, pages 441–448.
http://www.aclweb.org/anthology/S12-1060.

Kam-Fai Wong, Mingli Wu, and Wenjie Li. 2008.
Extractive summarization using supervised and
semi-supervised learning. In Proceedings of the
22nd International Conference on Computational
Linguistics-Volume 1. Association for Computa-
tional Linguistics, pages 985–992.

169


