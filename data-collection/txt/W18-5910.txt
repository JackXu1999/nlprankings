



















































Classification of Medication-Related Tweets Using Stacked Bidirectional LSTMs with Context-Aware Attention


Proceedings of the 3rd Social Media Mining for Health Applications (SMM4H) Workshop & Shared Task, pages 38–42
Brussels, Belgium, October 31, 2018. c©2018 Association for Computational Linguistics

38

Classification of medication-related tweets using stacked bidirectional
LSTMs with context-aware attention

Orest Xherija
Department of Linguistics

University of Chicago
Chicago, IL, USA

orest.xherija@uchicago.edu

Abstract

This paper describes the system that team
UChicagoCompLx developed for the 2018
Social Media Mining for Health Applications
(SMM4H) Shared Task. We use a variant of
the Message-level Sentiment Analysis (MSA)
model of (Baziotis et al., 2017), a word-
level stacked bidirectional Long Short-Term
Memory (LSTM) network equipped with at-
tention, to classify medication-related tweets
in the four subtasks of the SMM4H Shared
Task. Without any subtask-specific tuning, the
model is able to achieve competitive results
across all subtasks. We make the datasets,
model weights, and code publicly available1.

1 Introduction

The Shared Task of the 2018 Social Media Min-
ing for Health Applications (SMM4H) workshop
(Weissenbacher et al., 2018) proposed four sub-
tasks in the domain of social media mining for
health monitoring and surveillance. From a Nat-
ural Language Processing (NLP) viewpoint, these
tasks present a considerable challenge since the
nature of social media posts requires dealing with
both a significant level of language variation and
a widespread presence of noise (spelling mistakes,
syntactic errors etc). Any classifier designed for
this textual domain should take into account the
above intricacies and should, furthermore, be able
to deal with with semantic complexities in the var-
ious ways people express medication-related con-
cepts and outcomes.

To address these challenges, we use a variant
of the Message-level Sentiment Analysis (MSA)
model of (Baziotis et al., 2017), originally de-
veloped for sentiment analysis of Twitter posts,
to classify tweets in all four subtasks. The

1https://github.com/orestxherija/
smm4h2018

model is a word-level stacked bidirectional LSTM
(BiLSTM) with context-aware attention that uses
word-embeddings pretrained by (Baziotis et al.,
2017) on a corpus of ≈ 330M tweets. Without ad-
ditional hyperparameter tuning or subtask-specific
modifications, the model outperforms the average
of all submitted systems in subtasks 1 and 4 and
achieves first place (by a F1-score margin of 0.234
from the next team) in subtask 2. In subtask 3 our
model was placed 6th out of 9 systems.

In the following sections, we introduce the
datasets, discuss preprocessing steps we took,
present the model and its training setup, report re-
sults, and conclude with potential avenues for fu-
ture research.

2 Datasets

In this section, we describe the datasets of each
subtask. Subtasks 1, 3 and 4 are binary classifi-
cation problems while subtask 2 is a three-class
classification problem. The data was manually an-
notated by the organizers.

Subtask 1 is about the automatic detection
of posts mentioning the name of a drug or di-
etary supplement, as defined by the United States
Food and Drug Administration (FDA). A tweet
is assigned label 1 if it contains the name of
one or more drugs or supplements and 0 other-
wise. Subtask 2 poses the challenge of auto-
matic classification of posts describing medica-
tion intake. A tweet is assigned label 1 if “the
user clearly expresses a personal medication in-
take/consumption”, 2 if the tweet suggests (with-
out certainty) that “the user may have taken the
medication”, and 3 if the tweet mentions medi-
cation names but does not indicate personal in-
take. Subtask 3 concerns the automatic classifi-
cation of posts mentioning an adverse drug reac-
tion (ADR). A tweet is assigned label 1 if it men-

https://github.com/orestxherija/smm4h2018
https://github.com/orestxherija/smm4h2018


39

1 2 3 4
training 7011 13791 21062 6956
validation 780 1533 2341 CV
evaluation 5382 5000 5000 161

Table 1: Examples per split per task. CV indicates
cross-validation, so no validation set was held out.

tions an ADR and 0 otherwise. Finally, Subtask
4 deals with the automatic detection of posts men-
tioning vaccination behavior related to influenza
vaccines. The annotators were asked the ques-
tion “Does this message indicate that someone re-
ceived, or intended to receive, a flu vaccine?” and
a tweet was assigned label 1 if the answer was af-
firmative and 0 otherwise. Subtasks 1, 3 and 4 are
evaluated using the F1-score for the positive class
while subtask 2 uses the micro-averaged F1-score
for classes 1 and 2. Subtask 1 is additionally eval-
uated on precision and recall for the positive class.

Due to Twitter privacy policies, the training set
for any subtask did not contain the actual tweet
text. To obtain said text, participants were pro-
vided with the tweet ID of each dataset example
along with a script to use for downloading the text
using this ID. The process inevitably resulted in
fewer tweets than the number of IDs contained in
the original dataset, primarily because a number
of tweets had been removed (either by the users
themselves, or by Twitter because e.g. the user
deleted his account) while others failed to down-
load (due to e.g. lag issues when requesting the
HTML of the tweet). To avoid such issues in the
evaluation datasets, the organizers decided to pro-
vide the tweet text along with the ID. Table [1]
provides a short summary of the number of tweets
that were available to our team for each subtask.

3 Pre-processing

We applied identical preprocessing to all datasets.
We replaced Twitter specific strings with appro-
priate tokens (e.g. emojis were replaced by
$EMOJI$, numbers were replaced by $NUM-
BER$, website urls by $URL$ etc) to reduce the
vocabulary size and to ameliorate the noisy nature
of the text. All non-alphanumeric characters and
all tokens that were too short (fewer than 2 char-
acters) or too long (more than 15 characters) were
removed. Finally, all text was converted to lower
case and any excess whitespace (i.e. newlines and
tabs) was removed.

4 Model description

4.1 Model architecture

We use a variant of the Message-level Sentiment
Analysis (MSA) model of (Baziotis et al., 2017).
The model consists of two stacked BiLSTMs with
a context-attention mechanism à la (Yang et al.,
2016) that identifies the maximally informative
words for each label. We describe subsequently
the individual network layers.

The input is a tweet, regarded as a sequence
of words, which is projected to a vector space
of fixed size via the Embedding Layer. The
weights of the embedding layer are initialized us-
ing pre-trained word embeddings that (Baziotis
et al., 2017) trained on a Twitter corpus of approx-
imately ≈ 330M tweets. We opt for these embed-
dings instead of the standard Word2Vec (Mikolov
et al., 2013a,b) ones since they have been trained
on a similar textual domain to the tasks at hand.

A LSTM Layer placed on top of the embed-
ding layer takes as input the embedding weights
and produces a representation {hi}Ti=1 where, hi
is the hidden state of the LSTM at time-step i, in-
tuitively corresponding to a summary of all the in-
formation of the sentence (viewed as a sequence
{wi}Ti=1 of words) up to wi. This constitutes a
forward LSTM. Since we are using a bidirectional
LSTM, we also have an LSTM that scans the se-
quence of words in the reverse direction. The final
representation of a word is produced by concate-
nating the representations from the forward and
backward LSTM:

hi =
−→
hi ||
←−
hi (1)

where || denotes the concatenation operator. We
opt for a stacked BiLSTM, and consequently we
place an additional BiLSTM layer on top of the
preceding layer. The motivation for this choice
comes from the literature on the interpretation
of hidden states of Recurrent Neural Networks
(RNNs) (Belinkov et al., 2017; Belinkov, 2018) in
which it has been claimed that deeper layers are
able to learn more abstract semantic representa-
tions of sentences, thus achieving superior perfor-
mance in downstream tasks.

To account for the fact that not all words con-
tribute equally to the assignment of a label, we
place an Attention Layer on top of the BiL-
STMs following work like (Sutskever et al., 2014)
who successfully used attention mechanisms for



40

sequence-to-sequence neural machine translation.
We use context-attention, following (Yang et al.,
2016). A context vector uh is initialized and is
governed by the following update equations:

ei = tanh(Whhi + bh) (2)

ai =
exp(e>i uh)∑T
j=1 exp(e

>
j uh)

T∑
i=1

ai = 1 (3)

r =
T∑
i=1

aihi r ∈ R2L (4)

where Wh, bh and uh are learned parameters, hi
is the concatenation of the representations of the
forward and backward LSTM, introduced in equa-
tion (1), and L is the number of cells in one LSTM
layer.

Finally, we feed the representation r produced
by the attention layer to a Dense Layer with sig-
moid activation (softmax for subtask 2) and obtain
a probability distribution over the classes. If the
probability assigned to a tweet is greater than 0.5
we assign label 1, otherwise we assign 0.

4.2 Training setup
We train the model to minimize the negative
log-likelihood loss using back-propagation with
stochastic gradient descent and mini-batch size of
50. We use the Adam optimizer (Kingma and Ba,
2015) with gradient norm clipping (Pascanu et al.,
2013) at 1. For subtasks 1, 2 and 3 we use a
90 − 10 train-validation split, while for subtask 4
we use 10−fold stratified cross-validation in con-
sideration of the very small test set. Table [1] sum-
marizes the information on train-validation splits.

4.3 Regularization
To make the model more robust to over-fitting, we
employ, following (Baziotis et al., 2017), a num-
ber of regularization techniques. We add Gaus-
sian noise at the embedding layer and use dropout
(Srivastava et al., 2014) to ignore the signal from
a set of randomly selected neurons in the net-
work. Dropout is also applied after each LSTM
layer as well as to the recurrent connections of
the LSTM (Gal and Ghahramani, 2016). L2 reg-
ularization along with class weights are applied to
the loss function to prevent overly large weights
and to account for class imbalance. Class weights
are computed as follows: assuming that #»x is the
vector of class counts, the weights are defined

as wi = max( #»x )/xi for any class i. Finally,
early-stopping (Caruana et al., 2001) is employed
to terminate training after the validation loss has
stopped decreasing.

4.4 Hyperparameter tuning

We use the similar hyperparameters to (Baziotis
et al., 2017). In particular, we use 150 as the size
of the LSTM hidden states (300 in total since we
are using a BiLSTM), the Gaussian noise param-
eter is set to σ = 0.3, dropout rate on top of the
embedding layer is set to 0.3 and dropout rate on
top of the LSTM layers is set to 0.5. Dropout at the
recurrent connections is also set to 0.3. L2 regu-
larization at the loss function is set to 0.0001. Fi-
nally, we initialize the learning rate at 0.001. De-
parting from (Baziotis et al., 2017), we use word
embeddings of dimension 100. Vocabulary size
and maximum sequence length are set to 7000 and
50 respectively for all subtasks and the patience
level for early-stopping is set to 0.001 in 5 epochs.

5 Experiments and results

5.1 Experimental setup

The model was developed using Keras2 with the
Tensorflow (Abadi et al., 2016) backend. For data
preparation and processing we use Scikit-learn
(Pedregosa et al., 2011). Given the small size of
the datasets, we do not use GPUs for training the
model. A standard 8-core CPU is sufficient. Fi-
nally, for designing the network architecture, we
use part of the code released by (Baziotis et al.,
2017)3.

5.2 Results

For subtasks 1 and 4, the organizers chose to
disclose to each team only their respective score
along with the average score of all submitted sys-
tems. These results are summarized in Table [2].
Our system performed better than the average in
both subtasks, considerably so in subtask 1.

For subtasks 2 and 3, the organizers released the
complete leaderboards, presented in Tables [3] and
[4] respectively. Our system greatly outperformed
all other systems by a significant margin in subtask
2. In subtask 3, our system ranked 6th (out of 9
participants), potentially because the other teams
developed specialized systems for the particular

2https://keras.io/
3https://github.com/cbaziotis/

datastories-semeval2017-task4

https://keras.io/ 
https://github.com/cbaziotis/datastories-semeval2017-task4
https://github.com/cbaziotis/datastories-semeval2017-task4


41

P R F1

Subtask-1
0.937 0.891 0.914

(0.890) (0.872) (0.880)

Subtask-4
0.791 0.923 0.852

(0.826) (0.858) (0.840)

Table 2: Results on the evaluation set for subtasks 1
and 4. Average score of all participating systems in
parentheses. Metric is F1-score for class 1. For sub-
task 1, precision and recall for class 1 are also used for
evaluation.

P R F1
UChicagoCompLx 0.654 0.783 0.713
Light 0.492 0.467 0.479
Tub-Oslo 0.464 0.466 0.465
IRISA team 0.434 0.501 0.465
IIT KGP 0.408 0.407 0.408
UZH 0.371 0.437 0.401
CLaC 0.402 0.366 0.383
Techno 0.327 0.432 0.372

Table 3: Subtask 2 final leaderboard. Metric is micro-
averaged F1-score for classes 1 and 2.

subtask while we opted for a general model that
can be used without modifications in all four sub-
tasks.

6 Conclusion and future directions

We demonstrated that the variant of the MSA
model of (Baziotis et al., 2017) performs compet-
itively when applied to the domain of medication-
related short text classification. Without hyperpa-
rameter tuning, major architectural modifications,
or task-specific adjustments, the model obtained
competitive results in subtasks 1 and 4 and ranked

P R F1
THU NGN 0.442 0.636 0.522
IRISA team 0.378 0.649 0.478
UZH 0.455 0.436 0.445
Tub-Oslo 0.638 0.317 0.424
Art 0.332 0.547 0.413
UChicagoCompLx 0.370 0.464 0.411
CIC-NLP 0.314 0.529 0.394
Techno 0.434 0.344 0.383
IIT KGP 0.189 0.643 0.292

Table 4: Subtask 3 final leaderboard. Metric is F1-
score for class 1.

first in subtask 3, greatly outperforming all other
models in terms of precision, recall and F1-score.
The model’s performance in this Shared Task is
further testament to the ability of attentive RNNs
to perform at state-of-the-art level in short text
classification where individual word-meaning is
essential.

In the future, we aim to investigate whether
ensembles of word- and character-level attentive
RNNs can perform even better. The benefits of en-
sembling for text classification can be seen in nu-
merous NLP tasks ranging from Natural Language
Inference (Gong et al., 2018, among many others)
to product categorization (Skinner, 2018). Word-
level models perform well in capturing aspects
of the semantics (Belinkov et al., 2017) while
character-level models succeed in capturing syn-
tactic information about the text. Ensembles of
these diverse types of models can potentially lead
to improved performance.

A second avenue to pursue would be multi-task
learning, an area of active research that has shown
promising results in text classification (Liu et al.,
2016, 2017, among others). Given that all subtasks
are nearly identical in nature (all but one of them
being binary classification problems) and share a
highly overlapping lexicon, they provide an ex-
cellent ground for testing the merits of multi-task
learning.

Acknowledgments

This work was completed in part with resources
provided by the University of Chicago Research
Computing Center, whose contribution we grate-
fully acknowledge.

References
Martı́n Abadi, Paul Barham, Jianmin Chen, Zhifeng

Chen, Andy Davis, Jeffrey Dean, Matthieu Devin,
Sanjay Ghemawat, Geoffrey Irving, Michael Isard,
Manjunath Kudlur, Josh Levenberg, Rajat Monga,
Sherry Moore, Derek G. Murray, Benoit Steiner,
Paul Tucker, Vijay Vasudevan, Pete Warden, Martin
Wicke, Yuan Yu, and Xiaoqiang Zheng. 2016. Ten-
sorFlow: A System for Large-Scale Machine Learn-
ing. In 12th USENIX Symposium on Operating Sys-
tems Design and Implementation (OSDI 16), pages
265–283, Savannah, GA, USA. USENIX Associa-
tion.

Christos Baziotis, Nikos Pelekis, and Christos Doulk-
eridis. 2017. DataStories at SemEval-2017 Task 4:
Deep LSTM with Attention for Message-level and



42

Topic-based Sentiment Analysis. In Proceedings of
the 11th International Workshop on Semantic Eval-
uation (SemEval-2017), pages 747–754, Vancouver,
Canada. Association for Computational Linguistics.

Yonatan Belinkov. 2018. On Internal Language Rep-
resentations in Deep Learning: An Analysis of Ma-
chine Translation and Speech Recognition. Ph.D.
thesis, Massachusetts Institute of Technology.

Yonatan Belinkov, Lluı́s Màrquez, Hassan Sajjad,
Nadir Durrani, Fahim Dalvi, and James Glass. 2017.
Evaluating Layers of Representation in Neural Ma-
chine Translation on Part-of-Speech and Semantic
Tagging Tasks. In Proceedings of the Eighth In-
ternational Joint Conference on Natural Language
Processing, volume 1, pages 1–10, Taipei, Taiwan.
Asian Federation of Natural Language Processing.

Rich Caruana, Steve Lawrence, and Lee Giles. 2001.
Overfitting in Neural Nets: Backpropagation, Con-
jugate Gradient, and Early Stopping. In Todd K.
Leen, Thomas G. Dietterich, and Volker Tresp, ed-
itors, Advances in Neural Information Processing
Systems, volume 13, pages 402–408. MIT Press,
Denver, CO, USA.

Yarin Gal and Zoubin Ghahramani. 2016. Dropout
As a Bayesian Approximation: Representing Model
Uncertainty in Deep Learning. In Proceedings
of The 33rd International Conference on Machine
Learning, volume 48 of Proceedings of Machine
Learning Research, pages 1050–1059, New York,
NY, USA. PMLR.

Yichen Gong, Heng Luo, and Jian Zhang. 2018. Nat-
ural Language Inference over Interaction Space. In
International Conference on Learning Representa-
tions, Vancouver, Canada.

Diederik P. Kingma and Jimmy Ba. 2015. Adam: A
Method for Stochastic Optimization. In Interna-
tional Conference on Learning Representations, San
Diego, CA, USA.

Pengfei Liu, Xipeng Qiu, and Xuanjing Huang. 2016.
Deep Multi-Task Learning with Shared Memory for
Text Classification. In Proceedings of the 2016 Con-
ference on Empirical Methods in Natural Language
Processing, pages 118–127, Austin, Texas. Associa-
tion for Computational Linguistics.

Pengfei Liu, Xipeng Qiu, and Xuanjing Huang. 2017.
Adversarial Multi-task Learning for Text Classifica-
tion. In Proceedings of the 55th Annual Meeting of
the Association for Computational Linguistics, vol-
ume 1, pages 1–10, Vancouver, Canada. Association
for Computational Linguistics.

Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey
Dean. 2013a. Efficient Estimation of Word Repre-
sentations in Vector Space. In International Confer-
ence on Learning Representations, Scottsdale, AZ,
USA.

Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg Cor-
rado, and Jeffrey Dean. 2013b. Distributed Repre-
sentations of Words and Phrases and Their Composi-
tionality. In Christopher J. C. Burges, Léon Bottou,
Max Welling, Zoubin Ghahramani, and Kilian Q.
Weinberger, editors, Advances in Neural Informa-
tion Processing Systems, volume 26, pages 3111–
3119. Curran Associates, Inc., Lake Tahoe, CA,
USA.

Razvan Pascanu, Tomas Mikolov, and Yoshua Ben-
gio. 2013. On the Difficulty of Training Re-
current Neural Networks. In Proceedings of the
30th International Conference on Machine Learn-
ing, volume 28 of Proceedings of Machine Learn-
ing Research, pages 1310–1318, Atlanta, GA, USA.
PMLR.

Fabian Pedregosa, Gaël Varoquaux, Alexandre Gram-
fort, Vincent Michel, Bertrand Thirion, Olivier
Grisel, Mathieu Blondel, Peter Prettenhofer, Ron
Weiss, Vincent Dubourg, Jake VanderPlas, Alexan-
dre Passos, David Cournapeau, Matthieu Brucher,
Matthieu Perrot, and Edouard Duchesnay. 2011.
Scikit-learn: Machine Learning in Python. Journal
of Machine Learning Research, 12:2825–2830.

Michael Skinner. 2018. Product Categorization with
LSTMs and Balanced Pooling Views. In SIGIR
2018 Workshop on eCommerce (ECOM 18), SIGIR
’18, Ann Arbor, MI, USA. ACM.

Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky,
Ilya Sutskever, and Ruslan Salakhutdinov. 2014.
Dropout: A Simple Way to Prevent Neural Networks
from Overfitting. Journal of Machine Learning Re-
search, 15:1929–1958.

Ilya Sutskever, Oriol Vinyals, and Quoc V Le. 2014.
Sequence to Sequence Learning with Neural Net-
works. In Zoubin Ghahramani, Max Welling,
Corinna Cortes, Neil D. Lawrence, and Kilian Q.
Weinberger, editors, Advances in Neural Informa-
tion Processing Systems, volume 27, pages 3104–
3112. Curran Associates, Inc., Montréal, Canada.

Davy Weissenbacher, Abeed Sarker, Michael Paul,
and Graciela Gonzalez-Hernandez. 2018. Overview
of the Third Social Media Mining for Health
(SMM4H) Shared Tasks at EMNLP 2018. In Pro-
ceedings of the 3rd Workshop Social Media Mining
for Health Applications (SMM4H), Brussels, Brus-
sels. Association for Computational Linguistics.

Zichao Yang, Diyi Yang, Chris Dyer, Xiaodong He,
Alex Smola, and Eduard Hovy. 2016. Hierarchi-
cal Attention Networks for Document Classification.
In Proceedings of the 2016 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics: Human Language Technologies,
pages 1480–1489, San Diego, CA, USA. Associa-
tion for Computational Linguistics.


