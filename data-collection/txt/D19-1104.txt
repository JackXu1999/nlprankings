











































Look-up and Adapt: A One-shot Semantic Parser


Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing
and the 9th International Joint Conference on Natural Language Processing, pages 1129–1139,
Hong Kong, China, November 3–7, 2019. c©2019 Association for Computational Linguistics

1129

Look-up and Adapt: A One-shot Semantic Parser

Zhichu Lu
⇤

Carnegie Mellon University
Pittsburgh, PA

zhichul@cs.cmu.edu

Forough Arabshahi
⇤

Carnegie Mellon University
Pittsburgh, PA

farabsha@cs.cmu.edu

Igor Labutov

LAER AI, Inc.
New York, NY

igor.labutov@laer.ai

Tom Mitchell

Carnegie Mellon University
Pittsburgh, PA

tom.mitchell@cs.cmu.edu

Abstract

Computing devices have recently become ca-
pable of interacting with their end users via
natural language. However, they can only op-
erate within a limited “supported” domain of
discourse and fail drastically when faced with
an out-of-domain utterance, mainly due to the
limitations of their semantic parser. In this pa-
per, we propose a semantic parser that gener-
alizes to out-of-domain examples by learning
a general strategy for parsing an unseen ut-
terance through adapting the logical forms of
seen utterances, instead of learning to generate
a logical form from scratch. Our parser main-
tains a memory consisting of a representative
subset of the seen utterances paired with their
logical forms. Given an unseen utterance, our
parser works by looking up a similar utterance
from the memory and adapting its logical form
until it fits the unseen utterance. Moreover,
we present a data generation strategy for con-
structing utterance-logical form pairs from dif-
ferent domains. Our results show an improve-
ment of up to 68.8% on one-shot parsing un-
der two different evaluation settings compared
to the baselines.

1 Introduction and Background

Speech recognition technologies are achieving hu-
man parity (Xiong et al., 2016). As a result, end
users can now access different functionalities of
their phones and computers through spoken in-
structions via a natural language processing inter-
face referred to as a conversational agent. Current
commercial conversational agents such as Siri,
Alexa or Google Assistant come with a fixed set
of simple functions like setting alarms and mak-
ing reminders, but are often not able to cater to the
specific phrasing of a user or the specific action a

⇤ These two authors contributed equally

user needs. However, it has recently been shown
that it is possible to add new functionalities to an
agent through natural language instruction (Azaria
et al., 2016; Labutov et al., 2018).

For example, assume that the user wants to add
a functionality for resetting an alarm based on the
weather forecast for the next day, as demonstrated
by the following utterance: “whenever it snows at
night, wake me up 30 minutes early”. The user can
instruct this task to the agent by breaking it down
into a sequence of actions that the agent already
knows.

• check the weather app,

• see if the forecast is calling for snow,

• if yes, then reset the time of the alarm to 30
minutes earlier.

This set of instructions result in a logical form or
a semantic parse for this specific new utterance.
However, this approach can be used in practice
only if the agent is capable of generalizing from
this single new utterance to similar utterances such
as “if the weather is rainy, then set an alarm for 1
hour later”. We refer to this problem as one-shot
semantic parsing.

In this paper, we address this one-shot seman-
tic parsing task and present a semantic parser that
generalizes to out-of-domain utterances by seeing
a single example from that domain. While state
of the art neural semantic parsers are flexible to
language variations, they need plenty of examples
from the new domain to be able to parse an utter-
ance from that domain, which is not possible in
our scenario. On the other hand, grammar parsers
are not robust to the flexibility of language be-
cause of their use of string matching. Therefore,
we propose a method that preserves the robust-
ness of neural semantic parsers while addressing



1130

the data sparsity of the one-shot semantic parsing
task. We present a general strategy for “adapt-
ing” logical forms rather than constructing them
from scratch by “looking up” similar sentences
that we know how to parse and changing their log-
ical forms until they fit the new utterances. These
logical forms are looked-up from a memory that
contains a representative subset of previously seen
utterance-logical form pairs. Once this general
strategy is learned, the parser can be extended to
parse an utterance from a new domain by adding
one new example of that domain to the memory.

We propose a dataset generation method that al-
lows us to evaluate the effectiveness of our model
in a one-shot setting. We show that we generate
reasonably good utterances while creating differ-
ent experimental setups and scenarios for evalua-
tion.

Summary of Results In this paper we propose a
novel neural semantic parser for the task of one-
shot semantic parsing. We design two different
experiments to evaluate the parsing accuracy. We
show that our approach improves the performance
of neural semantic parsers by a significant margin
across 6 different domains of discourse. More-
over, we present a detailed analysis of our pro-
posed model and the performance of its different
components through an oracle study.

2 Problem Definition

In this section, we introduce the basic terminology
used in the paper and formally define the task of
one-shot semantic parsing.

A semantic parser takes as input an utterance
and outputs a corresponding logical form. An ut-
terance is a sequence of words and a logical form
is an s-expression capturing the meaning of the ut-
terance.

For example, “parents of John’s friends” is
an utterance and (field parent (field
friend John)) is its logical form.

We use a synchronous context free grammar
(SCFG), which is a generalization of the context-
free grammar (CFG), to generate grammar rules
(Chiang, 2006). A rule in an SCFG has a left hand
side, which is a non-terminal category, and two
right hand sides, referred to as the source and the
target. For our semantic parsing task, the source
corresponds to an utterance, and the target cor-
responds to a logical form. We denote a gram-
mar rule with the small letter g. A set of gram-

mar rules, denoted by the capital letter G, span
a domain of utterances. In this paper, we define
domain(G) as the set of all utterance-logical form
pairs generated by a set of grammar rules G. A
sample SCFG Gsample with its domain are pro-
vided in Tables 5 of Appendix A and Table 1 be-
low, respectively.

Utterance Logical Form
John john
Mary mary
parents parent
children child
parents of John (field parent john)
parents of Mary (field parent mary)
children of John (field child john)
children of Mary (field child mary)
John ’s parents (field parent john)
Mary ’s parents (field parent mary)
John ’s children (field child john)
Mary ’s children (field child mary)

Table 1: Domain of the sample SCFG Gsample. The
rules of this SCFG are presented in the Appendix A.
The domain of the sample SCFG is the set of all
hsource, targeti pairs listed in this table.

In one-shot semantic parsing we are given as
input a subset of utterance and logical form pairs
from domain(G) and a single utterance and logi-
cal form pair from domain(G0). The goal of one-
shot semantic parsing is to parse utterances from
domain(G0) that are not in the input.

Let us provide an example to make this more
clear. Consider that we are given utterance and
logical form pairs from domain(Gsample) in Ta-
ble 1 as well as the following utterance and logical
form pair as input.

hfriends of John,(field friend john)i

The goal of one-shot semantic parsing is to parse
examples such as

hfriends of Mary, (1)
(field friend Mary)i
hparents of Mary’s friends, (2)
(field parent(field friend Mary)i

which are not in domain(Gsample).
This is a challenging task for a complex gram-

mar since the domain of a typical grammar con-
sists of hundreds of thousand of examples. There-
fore, being able to parse all variations of ut-
terances in the domain by seeing only a sin-
gle example from it does not have a trivial so-
lution. Grammar-based semantic parsers usually



1131

rely on string matching which limits their robust-
ness to natural language variation. Neural seman-
tic parsers are more robust compared to grammar-
based parsers. However, their performance signif-
icantly drops in such a data-hungry setting. In the
next section, we present our look-up adapt seman-
tic parser that addresses the challenges of neural
semantic parsers in a data-sparse scenario.

3 Look-up and Adapt

In this section, we propose a novel neural network
architecture for one-shot semantic parsing. We are
given a set of utterance-logical form pairs as input
and our goal is to output a semantic parse for an
unseen query utterance. Our model is able to gen-
erate a logical form for the utterance by “looking
up” a similar utterance from a pool of utterance-
logical form pairs and “adapting” its logical form.
This pool of utterances is maintained in a “mem-
ory” and consist of a representative subset of the
input utterance-logical form pairs. We will show
that in the data-sparse scenario of one-shot seman-
tic parsing, adapting known logical forms is eas-
ier compared to generating a logical form from
scratch. We will also discuss that it is important
what subset of the data is included in the memory.

Our model consists of two main modules,
namely look-up § 3.1 and adapt § 3.2. Figure 1
shows a sketch of our proposed model and the
main look-up adapt algorithm is given in Algo-
rithm 1. The look-up module is responsible for
retrieving utterance-logical form pairs from the
memory, using two Bidirectional LSTM encoders.
The adapt module is responsible for adapting the
retrieved logical form until it results in the correct
semantic parse. The adapt module has two sub-
modules, namely the aligner and the discriminator.
The aligner is responsible for aligning the logical
form with the query utterance and the discrimina-
tor decides which parts of the logical form should
be swapped with a new one from the memory. In
the following sections, we will define each sub-
module of our proposed model in detail and pro-
pose a loss for training the model in an end-to-end
fashion.

Before describing the model components, let us
start by introducing the notation we will use in the
following sections. Bold capital letters represent
matrices, and bold lower case letters represent vec-
tors. X denotes a distributed representation of an
utterance, where each column of X is the repre-

sentation for a word in the utterance. Y denotes a
distributed representation of a logical form, where
each column of Y is the representation for a pred-
icate in the logical form. T is a tree representation
of the logical form. w and w0 denote attention
weights that sum to at least 0 and at most 1.

Algorithm 1 LookUpAndAdapt
1: function LOOKUPANDADAPT(Memory,X 0,w0)
2: (X,Y ,T) LOOKUP(Memory,X 0,w0)
3: for all t in T.children do
4: t0  ADAPT(Memory,X 0,Y ,t,w0)
5: T.children.replace(t, t’)
6: return T

3.1 Look-up

In this section, we describe our look-up strategy
and discuss its sub-modules in detail.

Encoder Bidirectional RNNs have been suc-
cessfully used to represent sentences in many ar-
eas of natural language processing such as ques-
tion answering, neural machine translation, and
semantic parsing (Bengio and LeCun, 2015; Her-
mann et al., 2015; Dong and Lapata, 2016).
We use Bidirectional LSTMs where the forward
LSTM reads the input in the utterance word or-
der (x1, x2, ..., xT ) and the backward LSTM reads
the input in reversed order (xT , xT�1, ..., x1). The
output for each word xi is the concatenation of
the ith hidden state hfwdi of the forward LSTM
and the T � ith hidden state hbckT�i of the back-
ward LSTM. As shown in Figure 1, we use two
Bidirectional LSTMs, one to encode the utterance,
and one to encode the logical form. The utter-
ance is treated as a sequence of words, where
each word is represented as a pretrained GloVe
(Pennington et al., 2014) embedding. The log-
ical form is treated as a sequence of predicates
and parentheses, each of which is also represented
as a pretrained GloVe embedding. The motiva-
tion to use GloVe embeddings for both words and
predicates is to avoid the problem of unknown
words/predicates encountered in the one-shot ut-
terance’s domain.

Retrieving from the memory The memory
consists of a subset of the utterance-logical form
pairs given as input to the algorithm. We refer to
the number of pairs in the memory as its size and
denote it with n. We would like to note that the
algorithm’s success depends on having a memory
that captures the structure of the domain. In this



1132

Look-up Adapt

Look-up Adapt

Memory

BiLSTMX: Input Utterance 

Utterance LogicalForm

X1

X2

.

.

.

Xn Yn

.

.

.

Y1

Y2

X̂ X̂i

How similar are these?

Xi

Yi
BiLSTM

Ŷi

Aligner Discriminator

Current Node

X̂

Re-weighted representation

X̂

Should I switch?
Ŷi

Ŷi
Ŷi

Figure 1: Look-up Adapt semantic parser. Our model consists of two main modules, namely “Look-up” (left box)
and “Adapt” (right box). The look-up module is responsible for retrieving utterance-logical form pairs from the
memory that are similar to an input query utterance X . The ‘Adapt” module is further broken down into two
modules “Aligner” and “Discriminator”. Given a looked-up utterance and logical form pair, the aligner module
traverses the logical form tree and updates the utterance representation accordingly while the discriminator module
decides weather to switch the current node with another one from the memory or keep it as is.

paper, we make a simplifying assumption that the
memory is given by an oracle, and it consists of
one example per grammar rule. Other choices for
the memory are also possible but exploring them
falls out of the scope of this paper. An interested
reader is referred to Appendix B for a more de-
tailed discussion. In the future we wish to auto-
mate the acquisition of this memory.

Given an encoded query utterance X 0, we
model the retrieval as a classification over exam-
ples in the memory. We compute a fixed size query
vector x0 = X 0 ·w0 which is a weighted average
of the column vectors in X 0. We compute a fixed-
size key vector for every example in the memory
x =

PT
i=1Xi, as the mean of the column vectors

of X .
Given a query vector x0 and vectors xi from the

memory for i = 1, 2, ..., n where n is the memory
size, we model the probability of retrieving the ith
entry from memory using equation 3

P (i|x0) = exp(ei)Pn
j=1 exp(ej)

(3)

where

ei = f([x
0,xi,x

0�xi, |x0�xi|,x0TW1xi]) (4)

where � is element-wise product,[a, b] is concate-
nation of vectors a and b, and f is a Multi-Layer-
Perceptron with a single hidden layer and ReLU
activation function. This way of featurizing x0

and x was adapted from (Kumar et al., 2016).
LOOKUP greedily returns the example with the
highest probability of being selected.

3.2 Adapt

The algorithm for ADAPT is given in Algorithm 2.
The adaptation is two phase. First an alignment
from T to relevant words in the new utterance X 0

is computed. Then a decision is made on whether
this subtree fails to match the relevant words in the
utterance and needs to be replaced. If yes, a re-
cursive call to LOOKUPANDADAPT will be made
with an updated attention w to focus on words rel-
evant to the subtree, and the returned value will be
propagated and eventually used as a replacement
for the current subtree. Otherwise, recursive calls
to ADAPT will be made to check the children of
the current subtree. For an example see Table 9.

The input Memory to the algorithm is used
in case a recursive call to LookUpAndAdapt is
needed. X 0 is the representation of the new ut-
terance. Y is the representation of the current log-
ical form. T is a subtree of Y that we wish to
adapt. w0 is the attention weights of the parent of
T, to be used as a mask when the alignment of T is
computed. In the following space we describe the
aligner module for alignment and the discrimina-
tor module for scoring a match between a subtree
and part of the new utterance.

Aligner Module The aligner module produces
an attention score over the new utterance given a
subtree of a logical form. Inspired by the gating
attention mechanism in (Kumar et al., 2016), we
model attention for every word in the utterance as
the maximum attention given by any predicate in
the subtree T.



1133

Algorithm 2 Adapt
1: function ADAPT(Memory,X 0,Y ,T,w0)
2: Phase 1: aligning T to relevant words in X 0
3: w  ALIGN(X 0,Y ,T,w0)
4: x0  X 0 ·w
5: if

P
i wi > 1 then

6: x0  x0 ÷
P

i wi
7:
8: Phase 2: deciding whether to replace T and then

recurse

9: P (fail to match) DISCRIMINATE(x0,Y ,T)
10: if P (fail to match) > 0.5 then
11: return LOOKUPANDADAPT(Memory,X 0,w)
12: else
13: for all t in T.children do
14: t0  ADAPT(Memory,X 0,Y ,t,w)
15: T.children.replace(t, t’)
16: return T

wi = max
j2span(T)

�(sij) (5)

where � is the sigmoid function and

sij = g([Xi,Yj ,Yp,Xi � Yj ,Xi � Yp, (6)
|Xi � Yj |, |Xi � Yp|,XTi W2Yj ,XTi WpYp])

where� is element-wise product, [a, b] is concate-
nation of vectors a and b, Xi is the ith column of
X , Yi is the ith column of Y , and g is a learnable
linear transformation. Yp is the representation of
the parent predicate of the subtree T (e.g. the par-
ent predicate is field for subtree john of logi-
cal form (field parent john)). We found
that adding this feature helps the model learn to
align arguments of predicates better.

We also found that constraining the attention
further by requiring that the attention of a child
node w be a refinement of the attention of a parent
node w0 facilitates good alignment. This is mod-
eled by the update rule

wi  wi ·
q

w0i (7)

Observe that if the attention score of the parent
node is very low for some word, the child is not
going to be able to look at it. This encourages
the parent node to attend to words not just relevant
to itself but also the children, and encourages the
child to refine rather than drastically change from
the alignment of the parent node.

Given the output weights of the aligner module,
we compute a fixed-size representation for the rel-
evant words in the utterance to the current subtree
T by first taking the weighted average x0 = X 0 ·w
where · is matrix product. If

P
iw

0
i > 1, we will

divide x0 by
P

iw
0
i to normalize it. x

0 is then com-
pared against the representation of the subtree by
the discriminator to produce a confidence score in-
dicating whether the subtree matches the words it
aligns to in the utterance.

Discriminator Module The discriminator mod-
ule learns to tell when a subtree fails to match the
meaning of the words in its alignment. Given a
subtree T of a logical form Y , its fixed-size rep-
resentation is computed as a mean of the repre-
sentations for each predicate in the subtree using
the formula y =

P
i2span(T) Yi, where Yi is the

ith column of Y , and span(T) is the set of in-
dices corresponding to all the predicates in the
subtree T. For example, the fixed-size representa-
tion for the subtree john in logical form (field
parent john) is just Y4 (indexing from 1, and
also counting parentheses because they are also
encoded in Y ).

We model the confidence that the subtree rep-
resentation y fails to match the meaning of the
aligned words x0 as

P (fail to match|x0,y) = �(d) (8)

where � is the sigmoid function and

d = h([x0,y,x0 � y, |x0 � y|,x0TW3y]) (9)

� is element-wise product, [a, b] is concatenation
of vectors a and b, and h is a learnable linear trans-
formation. This confidence score is used by Algo-
rithm 2 to decide whether to replace the subtree or
to keep it.

4 Training

In training, we maximize the log conditional like-
lihood of the data. Due to our selection of memory
(one example for each grammar rule in G), for a
given x there is a unique sequence of retrieval and
adaptation actions that lead to the production of
the correct y. Specifically, letting ai be the ith ac-
tion in the correct sequence of l actions, we define
the probability of producing y given x as the prob-
ability of taking the sequence of actions a1, ..., al.
We decompose this joint probability into products
of conditional probabilities of taking action i given
the previous actions and the input x

P (y|x) =
lY

i=1

P (ai|a1, a2, ..., ai�1, x) (10)



1134

If ai is a retrieval action,

P (ai|a1, a2, ..., ai�1, x) = P (i|x0) (11)

where P (i|x0) is defined in Equation 3. If ai is
an adaptation decision, i.e. to replace a particular
subtree

P (ai|a1, a2, ..., ai�1, x) = P (fail to match|x0,y)
(12)

where P (fail to match|x0,y) is defined in Equa-
tion 8.

5 Dataset Generation

Using existing semantic parsing datasets to di-
rectly evaluate one-shot parsing is difficult, be-
cause evaluation of one-shot parsing requires
grouping of utterances by structural similarity to
construct domains.

Therefore, we generate our own data 1 based
on a synchronous context free grammar. We eval-
uate our approach on the generated dataset. Al-
though our dataset is synthetic, it has the proper-
ties needed to evaluate one-shot parsing:

• clear distinction of domains

• plenty of examples for rules in the old domain

• one example for each rule in the new domain

• a separate evaluation set containing varia-
tions of the new rules

It is worth noting that although datasets such as
OVERNIGHT (Wang et al., 2015) do have clear
domain distinction, they do not have the other
properties needed for a good evaluation of one-
shot parsing. In the future, we are planning to
use crowd-sourcing to rephrase the generated ut-
terances in order to make them more natural.

We have two topics of discourse in the SCFG
that we use to generate our data. The first is ac-
cessing some field of some object, and the second
is setting some field of some object to some value
or some field of another object. These are general
purpose instructions that can express many com-
mon intents, such as looking up the location of a
restaurant, getting the phone number of a contact,
and setting reminders and alarms.

We have six domains of discourse in the SCFG
that we use to generate our data – person, restau-
rant, event, course, animal, and vehicle. Table 2

1Our dataset can be accessed at
https://github.com/zhichul/
lookup-and-adapt-parser-data.

includes sample sentences and their logical form
for every domain of discourse.

Since one-shot parsing has two phases, our
dataset is slightly different from a typical dataset
consisting of a single collection of utterance-
logical form pairs. In the following space we de-
scribe the sets of data generated for each domain.
In the next section we describe how we can use
this dataset to design two different one-shot pars-
ing evaluation setups.

For each domain d, we define a Gd and ran-
domly generate examples from domain(Gd) to
construct an “old” set of examples Dold,d. We
also generate a representative subset Mold,d to be
used as the memory of our model. As described in
§ 3.1, Mold,d contains one example for each gram-
mar rule in Gd.

In addition, we define a set of new rules G0d
disjoint from Gd for each domain and generate
one example ei,d for each new rule g0i,d 2 G0d,
and store them into the memory M 0new,d. These
are the one-shot examples. The extended memory
Mnew,d = Mold,d [M 0new,d.

Finally, we generate evaluation sets Eold,d ⇢
domain(Gd) and Enew,d ⇢ domain(Gd [G0d)�
domain(Gd) where � denotes set difference. In
our experiments we split the evaluation sets ran-
domly into a development set and a test set of the
same size.

Some statistics of our generated dataset is pre-
sented in Appendix D. In the next section, we de-
scribe how we use this data to generate two differ-
ent experimental scenarios for evaluating one-shot
semantic parsing.

6 Results and Evaluation

We evaluate our approach in six domains, namely
person, restaurant, event, course, animal, and vehi-
cle, and in two one-shot parsing scenarios, namely
extension and transfer. In the next section we de-
fine these two different scenarios. We compare
the performance to a sequence-to-sequence parser
with attention which is defined in the following
paragraphs.

Baseline Our baseline is a sequence-to-
sequence parser with attention. We use a 1-layer
Bidirectional LSTM as the encoder for the ut-
terance, and a 1-layer LSTM as the decoder for
the logical form. We initialize the decoder with
a learned projection of the last hidden state of
the encoder. The inputs to the encoder are GloVe

https://github.com/zhichul/lookup-and-adapt-parser-data
https://github.com/zhichul/lookup-and-adapt-parser-data


1135

Domain Utterance & Logical Form
a1 person the hometown field of john

(field (relation hometown) (person john))
a2 person set her ’s parents with classmates

(set (field (relation parent) (person reference)) (person classmate))
b1 restaurant irish restaurant instances ’s price field

(field (relation price) (restaurant irish))
b2 restaurant set address field of all irish restaurant as indian restaurant ’s price

(set (field (relation address) (restaurant irish)) (field (relation price) (restaurant indian)))
c1 event the start time of lectures

(field (relation start) (event lecture))
c2 event set the attendants of that event to organizers field of receptions instances

(set (field (relation attendant) (event reference)) (field (relation organizer) (event reception)))
d1 course size of my history course instances

(field (relation size) (course history))
d2 course set all history course instances ’s prerequisite field as physics course

(set (field (relation prerequisite) (course history)) (course physics))
e1 animal life span of all lion instances

(field (relation span) (animal lion))
e2 animal set fish instances ’s family field with dog

(set (field (relation family) (animal fish)) (animal dog))
f1 vehicle the source field of all buses

(field (relation source) (vehicle bus))
f2 vehicle set operators of all subways as buses instances

(set (field (relation operator) (vehicle subway)) (vehicle bus))

Table 2: Sample utterance-logical form pairs from each domain of discourse. Sentences a1 ... f1 parse to com-
mands that retrieve a field of an object. Sentences a2 ... f2 parse to commands which set a field of an object to
a constant or a field of another object. The grammar we use do not constrain the type of the set command. As a
consequence, some sentences generated are not semantically correct according to a human interpreter but still they
preserve the sentence structure. Sentence b2, e2, and f2 are examples of this phenomena.

word embeddings, and the inputs to the decoder
are concatenations of embeddings of logical
predicates and context vectors. Context vectors
are attention weighted averages of projected
encoder states. The attention weights over the
utterance for each decoding step t is computed by
taking the dot product of the hidden state of the
decoder at step t and the projected encoder state
for each word in the utterance, normalized using
the softmax function. The output of a decoding
step t is a probability distribution over all the
logical predicates. The distribution is modeled as
a dot product between a projection of the overall
decoding state at time t, and the embedding
for each logical predicate, normalized using the
softmax function. The overall decoding state at
time t is the concatenation of the hidden state of
the decoder at time t and the context vector at time
t. Decoding of a logical form given an utterance
is done as a greedy search over all possible logical
forms to output.

Evaluation Metric Our evaluation metric is
parsing accuracy. We define parsing accuracy as
the ratio of the correctly parsed utterances in the

test set. An utterance is parsed correctly if its
generated logical form matches the annotated log-
ical form exactly. E.g. (field (relation
parent) (person john)) is a correct parse
of “parents of John”. We report accuracy percent-
age in Tables 3 and 4.

6.1 Discussion

We design two different scenarios for one-shot
parsing evaluation, namely the extension and the
transfer scenarios. We define and discuss the re-
sults of each scenario in the following sections.

Extension This scenario tests one-shot parsing
of new rules within the same domain that the
model is trained on. In this case, we hold out
several rules and their corresponding utterance-
logical form pairs for evaluation and train on the
remaining ones.

The results of this experiment are presented in
Table 3. Each column indicates the evaluation re-
sults on each domain. The full scores refer to the
overall percentage of correct parses on the entire
test data, the d=2 scores refer to the percentage
of correct parses on examples with logical form



1136

depth 2 and the d=3 scores refer to the percentage
of correct parses on examples with logical form
depth 3.

As it can be seen in the table, our model is able
to consistently improve the sequence-to-sequence
baseline on all the domains by a significant margin
of 68.8%.

Transfer This scenario tests one-shot parsing of
new rules in a domain different from the one that
the model is trained on. For example, in the
transfer scenario for domain person the model
is trained on examples from the other domains
restaurant, event, course, animal, and vehicle and
tested on samples from domain person. This is
harder compared to the extension setup since it re-
quires generalization to a completely new domain.

The results of this section are reported in Ta-
ble 4. As it can be seen, the performance is im-
proved by up to 36.6% compared to the baseline
model.

In order to have a better understanding of the
model and indicate why it is performing compara-
ble to the baseline on some of the domains setting,
we carry out a model analysis in the next section.

Model Analysis We add two variations of
our model ORACLE-DISCRIM and PRETRAIN-
ENC for the transfer scenario to see the perfor-
mance improvements gained from replacing dif-
ferent components of our model with an ora-
cle/near oracle. This identifies potential bottle-
necks of the model.

In ORACLE-DISCRIM, we load a trained
model from LOOKUPADAPT but replace the dis-
criminator with an oracle during evaluation. As
the numbers in row 3 of Table 4 show, using an
oracle discriminator improves the model perfor-
mance by more than 10% in the vehicle domain
and course domain, and by smaller amounts in the
other domains. On one hand, this suggests that the
discriminator has room for improvement. On the
other hand, the numbers suggest that there is still
a much larger room for improvement for the en-
coder, aligner, and look-up components. Our hy-
pothesis is that the encoder components are likely
the main bottleneck because during evaluation in
the transfer scenario they have to produce good
representations for utterances and logical forms
very different from the ones which they have seen
during training. We found evidence supporting
this hypothesis in the PRETRAIN-ENC variation.

In PRETRAIN-ENC, we pretrain our model on
all the domains, which ensures that the two en-
coders see all domains. We then use the encoder
parameters of this all-domain pretrained model to
initialize the encoders in the experiments for the
transfer scenario. We then fix the encoders and
train only the aligner, discriminator, and look-up
parameters. This results in near perfect general-
ization to the test domain by the aligner, discrimi-
nator, and look-up components, which are trained
only on the other domains. This suggests that the
encoder is the main bottleneck of our model since
using a near-oracle version boosts its performance
to perfect. The results for this variation is shown
in row 4 of Table 4.

7 Related Work

Semantic parsing is the task of mapping utterances
to a formal representation of their meaning. Re-
searchers have used grammar-based methods as
well as machine learning-based methods to ad-
dress this problem. Grammar-based parsers work
by having a set of grammar rules that are either
learned or hand-written, an algorithm for generat-
ing a set of candidate logical forms by recursive
application of the grammar rules, and a criterion
for picking the best candidate logical form within
that set (Liang and Potts, 2015; Zettlemoyer and
Collins, 2005, 2007). However, they are brittle to
the flexibility of language. To improve this lim-
itation, supervised sequence-based neural seman-
tic parsers have been proposed (Dong and Lapata,
2016). Herzig and Berant (2017) improved the
performance of neural semantic parsers by train-
ing over multiple knowledge bases and providing
the domain encoding at decoding time. In addi-
tion to supervised learning, reinforcement learning
methods for neural semantic parsing have been ex-
plored in (Zhong et al., 2017).

Retrieve-and-edit style semantic parsing is
gaining popularity. Hashimoto et al. (2018) pro-
posed a retrieve and edit framework that can
efficiently learn to embed utterances in a task-
dependent way for easy editing. Our work differs
in that we perform hierarchical retrievals and ed-
its, and that we evaluate on cross-domain data and
focus on one-shot semantic parsing. It is worth
noting that retrieve-and-edit as a general frame-
work is not limited to semantic parsing and is ap-
plicable to other areas such as sentence genera-
tion. (Guu et al., 2018) and machine translation



1137

extension

person restaurant event course animal vehicle

Sequence-To-Sequence
full 20.1 25.1 26.1 19.9 25.0 18.8
d=2 25.0 18.1 20.0 31.8 26.5 25.9
d=3 13.9 37.3 42.4 0.0 21.5 5.8

LOOKUPADAPT
full 45.9 61.7 85.5 79.2 63.6 87.6
d=2 56.7 85.4 94.5 86.7 70.1 93.6
d=3 27.8 20.0 61.7 66.7 46.5 76.5

Table 3: Test Accuracy on the extension Dataset.

transfer

person restaurant event course animal vehicle

Sequence-To-Sequence
full 5.2 29.2 5.2 19.9 35.5 12.5
d=2 6.4 41.7 4.9 27.5 44.9 15.6
d=3 3.1 8.4 5.7 6.0 13.8 6.3

LOOKUPADAPT
full 41.8 43.8 8.4 16.7 28.3 11.5
d=2 52.5 53.4 13.2 24.2 37.3 14.1
d=3 21.4 27.8 0.0 3.0 6.9 6.3

ORACLE-DISCRIM
full 46.9 45.8 12.5 29.2 33.3 25.0
d=2 55.6 56.7 18.0 41.8 44.8 29.7
d=3 30.3 27.8 2.9 5.9 6.9 15.6

PRETRAIN-ENC
full 98.0 100.0 100.0 99.0 100.0 96.9
d=2 100.0 100.0 100.0 100.0 100.0 100.0
d=3 94.0 100.0 100.0 97.1 100.0 90.1

Table 4: Test Accuracy on the transfer Dataset.

Gu et al. (2018).
Another line of research maps semantic parsing

under cross-domain setting to a domain adaptation
problem (Su and Yan, 2017). In their work, the
model is trained on a certain domain and then fine
tuned to parse data from another domain. This is in
essence different from our work in that we do not
adapt the model, rather we adapt seen samples to
form parses of new samples. Moreover, We do not
fine-tune any part of the model in the new domain
and focus on one-shot semantic parsing.

Most of these models need many data-points to
train. Therefore, there has been recent attempts
at zero-shot semantic parsing. Dadashkarimi
et al. (2018) proposed a transfer learning approach
where a domain label is predicted first and then
the parse. Ferreira et al. (2015) and Herzig and
Berant (2018) proposed slot-filling methods for
semantic parsing based on general word embed-
dings. Bapna et al. (2017) focuses on zero-shot
frame semantic parsing by leveraging the descrip-
tion of slots to be filled.

8 Conclusion and Future Work

As speech recognition technologies mature, more
computing devices support spoken instructions via
a conversational agent. However, most agents do
not adapt to the phrasing and interest of a spe-
cific end user. It has recently been shown that
new functionalities can be added to an agent from

user instruction (Azaria et al., 2016; Labutov et al.,
2018). However, the user instruction only pro-
vides one instance of a general instruction tem-
plate and the agent is challenged to generalize to
variations of the instance given during instruction.
We define the one-shot parsing task for measuring
a semantic parser’s ability to generalize to new in-
stances of user-taught commands from only one
example. We propose a new semantic parser ar-
chitecture that learns a general strategy of retriev-
ing seen utterances similar to an unseen utterance
and adapting the logical forms of seen utterances
to fit the unseen utterance. Our results show an
improvement of up to 68.8% on one-shot parsing
under two different evaluation settings compared
to the baselines. We found that the BiLSTM en-
coders are likely bottlenecks for the model. Some
future directions include exploring the effects of
contents in the memory, automating memory ex-
traction from dataset, and improving the encoder.

Acknowledgments

This work was supported by AFOSR under grant
FA95501710218, NSF under grant IIS1814472,
and a Faculty award from J. P. Morgan. The au-
thors would like to sincerely thank Bishan Yang
for the initial discussions and ideas related to
model architecture, and to Kathryn Mazaitis for
the brainstorming sessions on the limitations of the
model and future directions.



1138

References

Amos Azaria, Jayant Krishnamurthy, and Tom
Mitchell. 2016. Instructable intelligent personal
agent. In Proceedings of the Thirtieth AAAI Con-
ference on Artificial Intelligence, pages 2681–2689.

Ankur Bapna, Gokhan Tur, Dilek Hakkani-Tur, and
Larry Heck. 2017. Towards Zero-Shot Frame Se-
mantic Parsing for Domain Scaling. arXiv e-prints,
page arXiv:1707.02363.

Yoshua Bengio and Yann LeCun, editors. 2015. 3rd
International Conference on Learning Representa-
tions, ICLR 2015, San Diego, CA, USA, May 7-9,
2015, Conference Track Proceedings.

David Chiang. 2006. An introduction to synchronous
grammars.

Javid Dadashkarimi, Alexander Fabbri, Sekhar
Tatikonda, and Dragomir R. Radev. 2018. Zero-
shot Transfer Learning for Semantic Parsing. arXiv
e-prints, page arXiv:1808.09889.

Li Dong and Mirella Lapata. 2016. Language to logi-
cal form with neural attention. In Proceedings of the
54th Annual Meeting of the Association for Compu-
tational Linguistics (Volume 1: Long Papers), pages
33–43, Berlin, Germany. Association for Computa-
tional Linguistics.

Emmanuel Ferreira, Bassam Jabaian, and Fabrice
Lefèvre. 2015. Zero-shot semantic parser for spoken
language understanding. In Sixteenth Annual Con-
ference of the International Speech Communication
Association.

Jiatao Gu, Yong Wang, Kyunghyun Cho, and Vic-
tor OK Li. 2018. Search engine guided neural ma-
chine translation. In Thirty-Second AAAI Confer-
ence on Artificial Intelligence.

Kelvin Guu, Tatsunori B Hashimoto, Yonatan Oren,
and Percy Liang. 2018. Generating sentences by
editing prototypes. Transactions of the Association
for Computational Linguistics, 6:437–450.

Tatsunori B Hashimoto, Kelvin Guu, Yonatan Oren,
and Percy S Liang. 2018. A retrieve-and-edit frame-
work for predicting structured outputs. In S. Bengio,
H. Wallach, H. Larochelle, K. Grauman, N. Cesa-
Bianchi, and R. Garnett, editors, Advances in Neural
Information Processing Systems 31, pages 10052–
10062. Curran Associates, Inc.

Karl Moritz Hermann, Tomas Kocisky, Edward
Grefenstette, Lasse Espeholt, Will Kay, Mustafa Su-
leyman, and Phil Blunsom. 2015. Teaching ma-
chines to read and comprehend. In C. Cortes, N. D.
Lawrence, D. D. Lee, M. Sugiyama, and R. Garnett,
editors, Advances in Neural Information Processing
Systems 28, pages 1693–1701. Curran Associates,
Inc.

Jonathan Herzig and Jonathan Berant. 2017. Neural
semantic parsing over multiple knowledge-bases. In
Proceedings of the 55th Annual Meeting of the As-
sociation for Computational Linguistics (Volume 2:
Short Papers), pages 623–628, Vancouver, Canada.
Association for Computational Linguistics.

Jonathan Herzig and Jonathan Berant. 2018. Decou-
pling structure and lexicon for zero-shot semantic
parsing. In Proceedings of the 2018 Conference on
Empirical Methods in Natural Language Process-
ing, pages 1619–1629, Brussels, Belgium. Associ-
ation for Computational Linguistics.

Ankit Kumar, Ozan Irsoy, Peter Ondruska, Mohit
Iyyer, James Bradbury, Ishaan Gulrajani, Victor
Zhong, Romain Paulus, and Richard Socher. 2016.
Ask me anything: Dynamic memory networks for
natural language processing. In Proceedings of The
33rd International Conference on Machine Learn-
ing, volume 48 of Proceedings of Machine Learning
Research, pages 1378–1387, New York, New York,
USA. PMLR.

Igor Labutov, Shashank Srivastava, and Tom Mitchell.
2018. LIA: A natural language programmable per-
sonal assistant. In Proceedings of the 2018 Con-
ference on Empirical Methods in Natural Language
Processing: System Demonstrations, pages 145–
150, Brussels, Belgium. Association for Computa-
tional Linguistics.

Percy Liang and Christopher Potts. 2015. Bringing
machine learning and compositional semantics to-
gether. Annual Review of Linguistics, 1:355–376.

Jeffrey Pennington, Richard Socher, and Christo-
pher D. Manning. 2014. Glove: Global vectors for
word representation. In Empirical Methods in Nat-
ural Language Processing (EMNLP), pages 1532–
1543.

Yu Su and Xifeng Yan. 2017. Cross-domain se-
mantic parsing via paraphrasing. In Proceedings
of the 2017 Conference on Empirical Methods in
Natural Language Processing, pages 1235–1246,
Copenhagen, Denmark. Association for Computa-
tional Linguistics.

Yushi Wang, Jonathan Berant, and Percy Liang. 2015.
Building a semantic parser overnight. In Proceed-
ings of the 53rd Annual Meeting of the Association
for Computational Linguistics and the 7th Interna-
tional Joint Conference on Natural Language Pro-
cessing (Volume 1: Long Papers), pages 1332–1342,
Beijing, China. Association for Computational Lin-
guistics.

W. Xiong, J. Droppo, X. Huang, F. Seide, M. Seltzer,
A. Stolcke, D. Yu, and G. Zweig. 2016. Achiev-
ing Human Parity in Conversational Speech Recog-
nition. arXiv e-prints, page arXiv:1610.05256.

Luke Zettlemoyer and Michael Collins. 2007. On-
line learning of relaxed CCG grammars for parsing

https://www.aaai.org/ocs/index.php/AAAI/AAAI16/paper/view/12383
https://www.aaai.org/ocs/index.php/AAAI/AAAI16/paper/view/12383
http://arxiv.org/abs/1707.02363
http://arxiv.org/abs/1707.02363
https://iclr.cc/archive/www/doku.php%3Fid=iclr2015:accepted-main.html
https://iclr.cc/archive/www/doku.php%3Fid=iclr2015:accepted-main.html
https://iclr.cc/archive/www/doku.php%3Fid=iclr2015:accepted-main.html
https://iclr.cc/archive/www/doku.php%3Fid=iclr2015:accepted-main.html
https://www3.nd.edu/~dchiang/papers/synchtut.pdf
https://www3.nd.edu/~dchiang/papers/synchtut.pdf
http://arxiv.org/abs/1808.09889
http://arxiv.org/abs/1808.09889
https://doi.org/10.18653/v1/P16-1004
https://doi.org/10.18653/v1/P16-1004
http://papers.nips.cc/paper/8209-a-retrieve-and-edit-framework-for-predicting-structured-outputs.pdf
http://papers.nips.cc/paper/8209-a-retrieve-and-edit-framework-for-predicting-structured-outputs.pdf
http://papers.nips.cc/paper/5945-teaching-machines-to-read-and-comprehend.pdf
http://papers.nips.cc/paper/5945-teaching-machines-to-read-and-comprehend.pdf
https://doi.org/10.18653/v1/P17-2098
https://doi.org/10.18653/v1/P17-2098
https://doi.org/10.18653/v1/D18-1190
https://doi.org/10.18653/v1/D18-1190
https://doi.org/10.18653/v1/D18-1190
http://proceedings.mlr.press/v48/kumar16.html
http://proceedings.mlr.press/v48/kumar16.html
https://www.aclweb.org/anthology/D18-2025
https://www.aclweb.org/anthology/D18-2025
https://doi.org/10.1146/annurev-linguist-030514-125312
https://doi.org/10.1146/annurev-linguist-030514-125312
https://doi.org/10.1146/annurev-linguist-030514-125312
http://www.aclweb.org/anthology/D14-1162
http://www.aclweb.org/anthology/D14-1162
https://doi.org/10.18653/v1/D17-1127
https://doi.org/10.18653/v1/D17-1127
https://doi.org/10.3115/v1/P15-1129
http://arxiv.org/abs/1610.05256
http://arxiv.org/abs/1610.05256
http://arxiv.org/abs/1610.05256
https://www.aclweb.org/anthology/D07-1071
https://www.aclweb.org/anthology/D07-1071


1139

to logical form. In Proceedings of the 2007 Joint
Conference on Empirical Methods in Natural Lan-
guage Processing and Computational Natural Lan-
guage Learning (EMNLP-CoNLL), pages 678–687,
Prague, Czech Republic. Association for Computa-
tional Linguistics.

Luke S. Zettlemoyer and Michael Collins. 2005.
Learning to map sentences to logical form: Struc-
tured classification with probabilistic categorial
grammars. In Proceedings of the Twenty-First Con-
ference on Uncertainty in Artificial Intelligence,
UAI’05, pages 658–666, Arlington, Virginia, United
States. AUAI Press.

Victor Zhong, Caiming Xiong, and Richard Socher.
2017. Seq2SQL: Generating Structured Queries
from Natural Language using Reinforcement Learn-
ing. arXiv e-prints, page arXiv:1709.00103.

https://www.aclweb.org/anthology/D07-1071
http://dl.acm.org/citation.cfm?id=3020336.3020416
http://dl.acm.org/citation.cfm?id=3020336.3020416
http://dl.acm.org/citation.cfm?id=3020336.3020416
http://arxiv.org/abs/1709.00103
http://arxiv.org/abs/1709.00103
http://arxiv.org/abs/1709.00103

