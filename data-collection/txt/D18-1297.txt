



















































Towards Less Generic Responses in Neural Conversation Models: A Statistical Re-weighting Method


Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 2769–2774
Brussels, Belgium, October 31 - November 4, 2018. c©2018 Association for Computational Linguistics

2769

Towards Less Generic Responses in Neural Conversation Models:
A Statistical Re-weighting Method

Yahui Liu1∗, Victoria Bi2†, Jun Gao3, Xiaojiang Liu2, Jian Yao1, Shuming Shi2
1Wuhan University, Wuhan, China
2Tencent AI Lab, Shenzhen, China

3Soochow University, Suzhou, China
{liuyahui, jian.yao}@whu.edu.cn, imgaojun@gmail.com,
{victoriabi, kieranliu, shumingshi}@tencent.com

Abstract

Sequence-to-sequence neural generation mod-
els have achieved promising performance on
short text conversation tasks. However, they
tend to generate generic/dull responses, lead-
ing to unsatisfying dialogue experience. We
observe that in conversation tasks, each query
could have multiple responses, which forms a
1-to-n or m-to-n relationship in the view of the
total corpus. The objective function used in
standard sequence-to-sequence models will be
dominated by loss terms with generic patterns.
Inspired by this observation, we introduce a
statistical re-weighting method that assigns
different weights for the multiple responses of
the same query, and trains the standard neu-
ral generation model with the weights. Experi-
mental results on a large Chinese dialogue cor-
pus show that our method improves the accep-
tance rate of generated responses compared
with several baseline models and significantly
reduces the number of generated generic re-
sponses.

1 Introduction

Many recent works have been proposed to use
neural networks to generate responses for open-
domain dialogue systems (Shang et al., 2015; Sor-
doni et al., 2015; Vinyals and Le, 2015; Li et al.,
2016a,c; Serban et al., 2017; Shen et al., 2017;
Li et al., 2017; Yu et al., 2017; Xu et al., 2017).
These methods are inspired by the sequence-to-
sequence (Seq2Seq) framework (Sutskever et al.,
2014), which is originally applied for Neural Ma-
chine Translation (NMT). They aim at maximizing
the probability of generating a response given an
input query, and generally use the maximum likeli-
hood estimation (MLE) as their objective function.
However, various problems occur when Seq2Seq

∗This work was done while Yahui Liu was with Tencent
AI Lab.

†Corresponding author

models are used for dialogue generation tasks. One
of the most important problems is that such models
are inclined to generate generic and dull responses
(e.g., I don’t know), rather than meaningful and
specific answers (Sordoni et al., 2015; Serban et al.,
2016; Li et al., 2016a,c; Kannan et al., 2016; Li
et al., 2017; Xie, 2017; Wei et al., 2017; Mou et al.,
2017).

Until now, it has attracted increasing studies to
address the issue of generating generic response.
For example, Li et al. (2016a) used the mutual infor-
mation theory to reconstruct MLE, but this model is
easy to generate ungrammatical outputs. They fur-
ther proposed a fast diverse decoding approach (Li
et al., 2016b), which modifies the beam search to
re-rank meaningful responses into higher positions.
Similar works explore different ways to encour-
age response diversity for picking less generic re-
sponses in the decoding search (Vijayakumar et al.,
2016; Li and Jurafsky, 2016). In the reinforcement
learning framework (Li et al., 2016c), the reward
function used in the decoding considers the ease
of answering, which is measured by a distance to-
wards a set of 8 generic responses. Thus, it can
also alleviate the problem of generating generic re-
sponses to some extent. Lison and Bibauw (2017)
proposed to add a weighting model to learn the
“quality” of the query and response pair, but it re-
lies heavily on additional inputs. All these works
tried to add extra optimized terms in the encod-
ing or decoding modules in Seq2Seq, making the
training or prediction more complicated.

In this work, we consider the reason why
Seq2Seq often generates generic responses by an-
alyzing the MLE objective function directly. We
notice that multiple responses are often associated
with one single input query. As shown in Figure 1,
the relationship between queries and responses is
much looser in conversation models than that in
NMT, since the space of possible responses is much



2770

larger than the space of possible translations for a
given sentence. On one hand, the information of
these responses is only required to be relevant to
the input query but usually differs from it. On the
other hand, a query accepts large semantic diversity
among its responses. Hence, it is a 1-to-n relation-
ship between a query and its responses (Vinyals
and Le, 2015; Zhou et al., 2017). Meanwhile, we
can see there is a m-to-n relationship between all
queries and responses in the training corpus. Then,
we find that MLE, which learns a 1-to-1 mapping
in response generation, naturally puts more empha-
sis on optimizing the frequent patterns. Thus, the
converged local optimum is easy to output these
patterns or their combinations, leading to generic
responses.

Source1 Target1

Source2 Target2

Source3 Target3
(a) NMT

Query1 Response1

Query2 Response2

Query3 Response3

Response4
(b) Dialogue

Figure 1: An illustration of the differences between
NMT and dialogue generation. Response4 is the po-
tential cases that are not collected in corpus.

Inspired by this observation, we propose a statis-
tical re-weighting method which modifies MLE by
re-weighting the multiple responses for each query
such that MLE will not be dominated by the fre-
quent patterns or their combinations. The proposed
method calculates the weights of a response with
the consideration of two statistical features: simi-
larity frequency and sentence length. Our model is
simple and efficient to optimize without adding ad-
ditional terms into the original Seq2Seq objective
function. We validate the performance of our pro-
posed method on a large Chinese dialogue corpus.
Results show that it can improve the acceptance
rate of the generated responses and significantly
suppress the number of generic responses.

2 Proposed Method

Standard Seq2Seq models for NMT and dialogue
generation aim at estimating the conditional proba-
bility p(y|x) where x = (x1, . . . , xT ) is an input
sequence and y = (y1, . . . , yT ′) is its correspond-
ing output sequence whose length T

′
may differ

from T . During training, we learn all the model pa-
rameters θθθ by summing the negative log likelihood

of each sample pair (x,y) in the training corpus C:

`(x,y, θθθ) = −
T ′∑
t=1

log p(yt|x,y[t−1];θθθ), (1)

L(C, θθθ) =
∑

(x,y)∈C

`(x,y, θθθ). (2)

Recall that generic responses are those that are
safe and universal for many queries and thus fre-
quently appear in the training corpus. Hence, if we
have two responses of x in which one is generic and
the other one contains more meaningful content,
using L(C, θθθ) in Eq. 1 will put the same emphasis
on optimizing each of their loss terms. Therefore,
L(C, θθθ) contains a large amount of patterns from
the generic responses, thus it is not surprised to see
that the trained models are stuck into local opti-
mum that are inclined to generate these patterns or
their combinations.

Based on this observation, we argue that a
good loss function of Seq2Seq for dialogue gen-
eration should not be dominated by the patterns
from generic responses. Here, we propose a re-
weighting method for responses of a query x.
Specifically, `(x,y, θθθ) in Eq. 1 is modified to be:

`w(x,y, θθθ) = w(y|x)`(x,y, θθθ), (3)

where w(y|x) ∈ (0, 1] is a soft weight for a re-
sponse y of a query x. In the implementation,
we make the normalization of this loss at the
mini-batch level for better computational efficiency.
Hence, the loss of Eq. 2 for a mini-batch L(B, θθθ)
takes the form:

L(B, θθθ) =
∑

x,y∈B `w(x,y, θθθ)∑
x,y∈Bw(y|x)

. (4)

We summarize two common properties for the
responses:

• Responses with the patterns of frequently
appearing in the training corpus tend to be
generic. Here, the patterns refer to both the
whole sentence or n-grams which can be de-
scribed by similarities among responses.

• Very short and long responses should be avoid.
Owing to the MLE objective function, the
Seq2Seq frameworks are inclined to gener-
ate short responses that are universal replies.
While long responses usually contain more
specific information which may not be gener-
alized to most conversation scenarios. Hence,



2771

high-quality responses tend to be with moder-
ate length.

We propose an estimator by considering these two
properties:

w(y|x,R,C) = Φ(y)
maxr∈R{Φ(r)}

, (5)

where R denotes all collected responses of x in C.
For each response, the estimator gives a weight by:

Φ(y) = αE(y) + βF(y). (6)

Here, E(y) and F(y) correspond to the mentioned
two properties respectively:

• E(y) = e−af(y), where f(y) is a function re-
lated to the frequency of response y. It could
be formulated as

f(y) = max{0,Count(D(y,yj) ≥ τ)− b}
∀j ∈ |C|,

where D(·) refers to the similarity between
two sentences, a is a scale factor, b is bias
and τ ∈ [0, 1] is a threshold specifying the
similarity that two responses will be consid-
ered identical. For instance, it could be the
simplest strictly matching, which is used in
our experiments. Other methods like cosine
distance of TF-IDF (token or n-grams) can
also be applied, but may encounter compu-
tational issues for large corpus. A response
with a higher frequency will be assigned with
a smaller E(y).

• F(y) = e−c||y|−|ŷ||, where |y| denotes the
number of tokens in y, |ŷ| = 1|C|

∑
r∈C |r|

refers to the average length of responses in
the total training corpus, and c is a scale fac-
tor. Here, the “moderate length” is set to the
average length of responses of the total train-
ing corpus. In practice, we have tried to use
long responses (longer than average length)
to fine-tune the Seq2Seq model. Though it
slightly increases the average length of gener-
ated responses, the generated responses suffer
from more ungrammatical and influent issues.
Hence, if a response is too short or long, it
will receive a low score of F(y).

Mentioned hyper-parameters {α, β, a, b, τ, c} are
constant values in the following experiments,
which are set to {0.5, 0.5, 0.33, 3, 1.0, 0.33}. When

we performed our experiments, we tried several
hyper-parameter settings and found that our method
is not sensitive to different hyper-parameters and
achieves stable results in general. Hence, we do
not spend many efforts to specifically tune these
hyper-parameters.

Response (frequency/length) w(y|x) E(y) F(y)

一直单身 (70/2) 0.047 0.014 0.066Single till now.
求交往 (173/2) 0.039 0.000 0.066Would you like to have a date with me?
等你看到别人甜蜜就知道一个人的痛
苦了 (2/13) 0.769 1.000 0.560Once seeing the happy lovers, you will
feel sad.
应该还在哭吧 (1/5) 0.665 1.000 0.176You may be crying now.
单身狗的自我安慰 (2/4) 0.622 1.000 0.128Morale-boosting of singles.
我也是这么觉得 (30/6) 0.149 0.052 0.248I think so.
单身不好,单身23年,孤单太久了 (1/10)

1.000 1.000 0.924Given being single for 23 years, it’s not
so good.
多么心塞的领悟 (1/4) 0.623 1.000 0.128What a painful understanding!

Table 1: Weights of the responses for a query “其实单
身也挺好的 (It’s pretty good to be single)”.

To validate that our design function in Eq. 5 and
Eq. 6 are effective to weight the responses, Table 1
shows the weights of 8 responses for a query “其实
单身也挺好的 (It’s pretty good to be single)”. As
can be seen, the weights are reasonable, in which
the higher-ranked responses are more informative
ones with low similarity frequency and moderate
length.

3 Experiments

3.1 Corpus and Evaluation
We crawl conversation pairs from some popular
Chinese social media websites1, and select 7M
high-quality pairs as our training corpus. Conven-
tional metrics such as BLEU (Papineni et al., 2002)
and perplexity, are improper to be used for response
generation tasks. Following previous works (Li
et al., 2016c, 2017), we apply human annotations.
We randomly sample 500 queries (not used in train-
ing) as our test samples, and recruit 3 annotators to
evaluate each generated response from two aspects:

• Fluency: 0 (unreadable), 1 (readable but with
some grammar mistakes), 2 (fluent);

• Relevance: 0 (not relevant at all), 1 (relevant
at a distant level), 2 (relevant, including the

1Weibo: www.weibo.com, Baidu Tieba: tieba.
baidu.com, and Zhihu: www.zhihu.com

www.weibo.com
tieba.baidu.com
tieba.baidu.com
www.zhihu.com


2772

generic responses), 3 (relevant as well as in-
teresting).

Acceptance is then automatically calculated as a
metric reflecting whether the response is acceptable
to real users. A response will be assigned 1 when
it gets Fluency≥1 and Relevance≥2, otherwise it
will be assigned 0.

We implement our baseline Seq2Seq model us-
ing its standard objective function in Eq. 1 with two
LSTM layers for encoding/decoding and a standard
beam search with a beam size of 5 (the best set-
ting), termed as Seq2Seq. We also compare several
Seq2Seq variants:

• Seq2Seq-RS: training with a subset by ran-
domly sampling only one from the multiple
responses for each query;

• Seq2Seq-MMI: applying the maximum mu-
tual information (Li et al., 2016a) (only the
MMI-bidi);

• Seq2Seq-DD: applying the diverse decoding
algorithm (Li et al., 2016b);

• Ours-RW: calculating weights via our re-
weighting method proposed in Section 2.
Without applying any other tricks, we imple-
ment three versions of our method by using
E(·) only, F(·) only, a linear combination
of E(·) and F(·) in Eq 6, termed as Ours-
RW{E,F,EF}.

3.2 Results and Discussion
Human annotation results are shown in Table 2.
Several observations can be made. First, Seq2Seq-
RS performs slightly worse than the baseline model.
This means that it does not work to simply discard
a large amount of training data to construct a 1-
to-1 query-response subset for training. Second,
Seq2Seq-MMI not only provides no improvement
for the baseline but also inclines to generate generic
response. Third, Seq2Seq-DD obtains higher rel-
evance and acceptance scores than the baseline,
which shows its effectiveness by re-ranking more
meaningful responses into higher positions in beam
search. Fourth, our method achieves the best perfor-
mance on almost all metrics. When we use strictly
matched frequency of each response, Ours-RWE
does not perform better than the baseline model
because that the percentage of responses with fre-
quency higher than 3 is about 0.5% in our training
corpus. However, it still enhances the performance

in Ours-RWEF, which performs the best and in-
creases the acceptance of the baseline model from
0.42 to 0.55. This validates that the properties
about similarity frequency and sentence length play
important roles in generating better responses.

Model Evaluation MetricsFluency Relevance Acceptance

Seq2Seq 1.96±3.8e-5 1.31±5.3e-3 0.42±4.7e-4
Seq2Seq-RS 1.97±8.1e-5 1.30±2.1e-3 0.42±9.9e-4

Seq2Seq-MMI 1.94±7.2e-5 1.19±4.0e-3 0.41±1.9e-4
Seq2Seq-DD 1.86±2.8e-3 1.40±1.5e-2 0.49±2.4e-3
Ours-RWE 1.95±1.9e-4 1.30±5.1e-3 0.42±4.2e-4
Ours-RWF 1.97±1.5e-4 1.47±2.1e-3 0.51±6.3e-4
Ours-RWEF 1.96±8.3e-5 1.59±1.9e-2 0.55±4.4e-3

Table 2: Human annotation results.

Specifically, the average percentage of the gener-
ated responses that are assigned to relevance rating
2 (relevant, including the generic responses) and
3 (relevant as well as interesting) are presented in
Table 4. It shows that our method achieves higher
relevance score owing to generating more high-
quality responses with rating 3.

To validate that our method is effective to reduce
the number of generated generic responses, we
calculated the distinct-1 and distinct-2 (Li et al.,
2016a) for the compared methods respectively,
which are the number of distinct unigrams and bi-
grams divided by total number of generated words
respectively. As shown in Table 5, Ours-RWEF
achieves the best performance on the two metrics.
This indicates that our model often outputs more
meaningful and relevant responses than the other
compared methods.

We further randomly sample another 100K
queries (not used in training) and use the various
models to generate responses. We compare the
frequencies of several common generic responses
appearing in the generated results, as shown in Ta-
ble 6. It shows that our method can significantly
reduce the number of generic responses. For in-
stance, we reduce about 75% of the case “我也
不知道 (I don’t know, either.)” and 77% of the
case “我也想知道 (I want to know, too)” to be
generated.

4 Conclusion

In this paper, we propose a statistical re-weighting
method to weight multiple responses differently
and optimize the MLE objective function. The
weight of each response is calculated based on



2773

Query ResponseSeq2Seq Seq2Seq-MMI Seq2Seq-DD Ours-RWEF

我要生气了 我也生气了 生气了 我也生气了 不要生气
I’m going to be angry. So do I. I’m angry. So do I. Don’t be angry.
最近有什么好书推荐 看你喜欢什么类型的 我也想去看电影 你喜欢什么类型的 《红楼梦》
Would you like to recom-
mend me some good books?

It depends on what kind
of books you like?

I want to see the
movie, too.

What kind of books
do you like?

Dream of the Red
Chamber

做人不一定要风风光光 一定要 我也不知道 是啊。。。 做人一定要善良
It’s not necessary to be a suc-
cessful and grand man.

Must be I don’t know, ei-
ther.

Yes... Everyone should try
to be a kind man.

这几天嗓子疼 我也不知道 我也想去 我也是。。。 我这两天也是
I have a sore throat recently. I don’t know, either. I want to go, too So do I... So do I these days .
二号煤矿放几天假 假的 我也想放假 假的，假的 我放了3天
How many days did people
in No.2 coal mine take off?

False I want to have a
holiday, too

False, False I took three days off.

孟记普洱茶专营店一帖 我也想吃 我也想喝 铁观音，茶，茶，茶 我喜欢喝茶
An advertising of Mengji’s
Pu’er tea shop.

I want to eat, too. I want to drink, too. Tat-Kuan-Yin Tea,
tea, tea, tea

I like drinking tea.

Table 3: Examples of responses generated by Seq2Seq, Seq2Seq-MMI, Seq2Seq-DD and Ours-RWEF.

Model rating 2(%) rating 3(%)

Seq2Seq 31.3 10.6
Seq2Seq-MMI 35.8 4.4
Seq2Seq-DD 37.1 11.7
Ours-RWEF 36.8 18.1

Table 4: Comparisons on the average percentage of
the generated responses that are assigned to relevance
rating 2 and 3.

Model distinct-1 distinct-2

Seq2Seq 0.170 0.307
Seq2Seq-MMI 0.140 0.259
Seq2Seq-DD 0.131 0.170
Ours-RWEF 0.173 0.359

Table 5: Performances on the metrics distinct-1 and
distinct-2.

two terms according to the similarity frequency
and its length. Experiments show that our ap-
proach improves the performance over the base-
line models and reduces the number of generated
generic responses significantly. It indicates that
mismatching issue of objective function can be
alleviated through such similar re-weighting meth-
ods, by which current encoder-decoder architec-
tures can take full use of the m-to-n training corpus
and model the dialogue generation tasks better.

References
Anjuli Kannan, Karol Kurach, Sujith Ravi, Tobias

Kaufmann, Andrew Tomkins, Balint Miklos, Greg
Corrado, László Lukács, Marina Ganea, Peter
Young, et al. 2016. Smart reply: Automated re-
sponse suggestion for email. In Proceedings of the
ACM SIGKDD International Conference on Knowl-
edge Discovery and Data Mining (KDD).

Response ModelSeq2Seq MMI DD RWEF

我也不知道 3296 4404 941 847I don’t know, either.
我也想知道 3211 13764 795 738I want to know, too.
我也觉得 883 632 1505 59I think so.
我也想问 515 51 441 12I want to ask, too.
不喜欢 337 131 214 95I don’t like it.
看什么 254 52 377 87What do you look at?
不会 109 92 54 24Will/can not

Table 6: Frequencies of several generic responses.

Jiwei Li, Michel Galley, Chris Brockett, Jianfeng Gao,
and Bill Dolan. 2016a. A diversity-promoting ob-
jective function for neural conversation models. The
North American Chapter of the Association for Com-
putational Linguistics (NAACL).

Jiwei Li and Dan Jurafsky. 2016. Mutual information
and diverse decoding improve neural machine trans-
lation. arXiv preprint arXiv:1601.00372.

Jiwei Li, Will Monroe, and Dan Jurafsky. 2016b. A
simple, fast diverse decoding algorithm for neural
generation. arXiv preprint arXiv:1611.08562.

Jiwei Li, Will Monroe, Alan Ritter, Michel Galley, Jian-
feng Gao, and Dan Jurafsky. 2016c. Deep reinforce-
ment learning for dialogue generation. In Proceed-
ings of the Conference on Empirical Methods in Nat-
ural Language Processing (EMNLP).

Jiwei Li, Will Monroe, Tianlin Shi, Alan Ritter, and
Dan Jurafsky. 2017. Adversarial learning for neu-
ral dialogue generation. In Conference on Empirical
Methods in Neural Language Processing (EMNLP).

Pierre Lison and Serge Bibauw. 2017. Not all dia-
logues are created equal: instance weighing for neu-



2774

ral conversational models. In Conference on Spe-
cial Interest Group on Discourse and Dialogue (SIG-
DIAL).

Lili Mou, Yiping Song, Rui Yan, Ge Li, Lu Zhang, and
Zhi Jin. 2017. Sequence to backward and forward
sequences: A content-introducing approach to gen-
erative short-text conversation. Proceedings of the
International Conference on Computational Linguis-
tics (COLING).

Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic eval-
uation of machine translation. In Proceedings of the
Annual Meeting on Association for Computational
Linguistics (ACL).

Iulian Vlad Serban, Alessandro Sordoni, Yoshua Ben-
gio, Aaron C Courville, and Joelle Pineau. 2016.
Building end-to-end dialogue systems using gener-
ative hierarchical neural network models. In AAAI
Conference on Artificial Intelligence (AAAI).

Iulian Vlad Serban, Alessandro Sordoni, Ryan Lowe,
Laurent Charlin, Joelle Pineau, Aaron C Courville,
and Yoshua Bengio. 2017. A hierarchical latent
variable encoder-decoder model for generating di-
alogues. In AAAI Conference on Artificial Intelli-
gence (AAAI).

Lifeng Shang, Zhengdong Lu, and Hang Li. 2015. Neu-
ral responding machine for short-text conversation.
In Proceedings of the Annual Meeting of the Associ-
ation for Computational Linguistics (ACL).

Xiaoyu Shen, Hui Su, Yanran Li, Wenjie Li, Shuzi
Niu, Yang Zhao, Akiko Aizawa, and Guoping Long.
2017. A conditional variational framework for dia-
log generation. arXiv preprint arXiv:1705.00316.

Alessandro Sordoni, Michel Galley, Michael Auli,
Chris Brockett, Yangfeng Ji, Margaret Mitchell,
Jian-Yun Nie, Jianfeng Gao, and Bill Dolan. 2015.
A neural network approach to context-sensitive gen-
eration of conversational responses. In the North
American Chapter of the Association for Computa-
tional Linguistics (NAACL).

Ilya Sutskever, Oriol Vinyals, and Quoc V Le. 2014.
Sequence to sequence learning with neural networks.
In Advances in Neural Information Processing Sys-
tems (NIPS).

Ashwin K Vijayakumar, Michael Cogswell, Ram-
prasath R Selvaraju, Qing Sun, Stefan Lee, David
Crandall, and Dhruv Batra. 2016. Diverse beam
search: Decoding diverse solutions from neural se-
quence models. arXiv preprint arXiv:1610.02424.

Oriol Vinyals and Quoc Le. 2015. A neural conversa-
tional model. arXiv preprint arXiv:1506.05869.

Bolin Wei, Shuai Lu, Lili Mou, Hao Zhou, Pascal
Poupart, Ge Li, and Zhi Jin. 2017. Why do neu-
ral dialog systems generate short and meaningless
replies? a comparison between dialog and transla-
tion. arXiv preprint arXiv:1712.02250.

Ziang Xie. 2017. Neural text generation: A practical
guide. arXiv preprint arXiv:1711.09534.

Zhen Xu, Bingquan Liu, Baoxun Wang, Chengjie Sun,
Xiaolong Wang, Zhouran Wang, and Chao Qi. 2017.
Neural response generation via gan with an approxi-
mate embedding layer. In Conference on Empirical
Methods in Natural Language Processing (EMNLP).

Lantao Yu, Weinan Zhang, Jun Wang, and Yong Yu.
2017. Seqgan: Sequence generative adversarial nets
with policy gradient. In AAAI Conference on Artifi-
cial Intelligence (AAAI).

Ganbin Zhou, Ping Luo, Rongyu Cao, Fen Lin,
Bo Chen, and Qing He. 2017. Mechanism-aware
neural machine for dialogue response generation. In
AAAI Conference on Artificial Intelligence (AAAI).


