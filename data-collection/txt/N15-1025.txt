



















































Spinning Straw into Gold: Using Free Text to Train Monolingual Alignment Models for Non-factoid Question Answering


Human Language Technologies: The 2015 Annual Conference of the North American Chapter of the ACL, pages 231–237,
Denver, Colorado, May 31 – June 5, 2015. c©2015 Association for Computational Linguistics

Spinning Straw into Gold: Using Free Text to Train Monolingual Alignment
Models for Non-factoid Question Answering

Rebecca Sharp1, Peter Jansen1, Mihai Surdeanu1, and Peter Clark2
1 University of Arizona, Tucson, AZ, USA

2 Allen Institute for Artificial Intelligence, Seattle, WA, USA
{bsharp,pajansen,msurdeanu}@email.arizona.edu

peterc@allenai.org

Abstract
Monolingual alignment models have been
shown to boost the performance of question
answering systems by ”bridging the lexical
chasm” between questions and answers. The
main limitation of these approaches is that
they require semistructured training data in the
form of question-answer pairs, which is diffi-
cult to obtain in specialized domains or low-
resource languages. We propose two inex-
pensive methods for training alignment mod-
els solely using free text, by generating ar-
tificial question-answer pairs from discourse
structures. Our approach is driven by two rep-
resentations of discourse: a shallow sequen-
tial representation, and a deep one based on
Rhetorical Structure Theory. We evaluate the
proposed model on two corpora from differ-
ent genres and domains: one from Yahoo!
Answers and one from the biology domain,
and two types of non-factoid questions: man-
ner and reason. We show that these align-
ment models trained directly from discourse
structures imposed on free text improve per-
formance considerably over an information re-
trieval baseline and a neural network language
model trained on the same data.

1 Introduction
Question Answering (QA) is a challenging task that
draws upon many aspects of NLP. Unlike search
or information retrieval, answers infrequently con-
tain lexical overlap with the question (e.g. What
should we eat for breakfast? – Zoe’s Diner has
good pancakes), and require QA models to draw
upon more complex methods to bridge this ”lexical
chasm” (Berger et al., 2000). These methods range
from robust shallow models based on lexical seman-
tics, to deeper, explainably-correct, but much more
brittle inference methods based on first order logic.

Berger et al. (2000) proposed that this ”lexical
chasm” might be partially bridged by repurposing
statistical machine translation (SMT) models for
QA. Instead of translating text from one language to
another, these monolingual alignment models learn
to translate from question to answer1, learning com-
mon associations from question terms such as eat or
breakfast to answer terms like kitchen, pancakes, or
cereal.

While monolingual alignment models have en-
joyed a good deal of recent success in QA (see
related work), they have expensive training data
requirements, requiring a large set of aligned in-
domain question-answer pairs for training. For low-
resource languages or specialized domains like sci-
ence or biology, often the only option is to enlist a
domain expert to generate gold QA pairs – a process
that is both expensive and time consuming. All of
this means that only in rare cases are we accorded
the luxury of having enough high-quality QA pairs
to properly train an alignment model, and so these
models are often underutilized or left struggling for
resources.

Making use of recent advancements in discourse
parsing (Feng and Hirst, 2012), here we address this
issue, and investigate whether alignment models for
QA can be trained from artificial question-answer
pairs generated from discourse structures imposed
on free text. We evaluate our methods on two
corpora, generating alignment models for an open-
domain community QA task using Gigaword2, and
for a biology-domain QA task using a biology text-
book.

1In practice, alignment for QA is often done from answer to
question, as answers tend to be longer and provide more oppor-
tunity for association (Surdeanu et al., 2011).

2LDC catalog number LDC2012T21

231



The contributions of this work are:
1. We demonstrate that by exploiting the dis-

course structure of free text, monolingual align-
ment models can be trained to surpass the per-
formance of models built from expensive in-
domain question-answer pairs.

2. We compare two methods of discourse pars-
ing: a simple sequential model, and a deep
model based on Rhetorical Structure Theory
(RST) (Mann and Thompson, 1988). We show
that the RST-based method captures within and
across-sentence alignments and performs bet-
ter than the sequential model, but the sequential
model is an acceptable approximation when a
discourse parser is not available.

3. We evaluate the proposed methods on two cor-
pora, including a low-resource domain where
training data is expensive (biology).

4. We experimentally demonstrate that mono-
lingual alignment models trained using our
method considerably outperform state-of-the-
art neural network language models in low re-
source domains.

2 Related Work

Lexical semantic models have shown promise in
bridging Berger et al.’s (2000) ”lexical chasm.” In
general, these models can be classified into align-
ment models (Echihabi and Marcu, 2003; Soricut
and Brill, 2006; Riezler et al., 2007; Surdeanu et
al., 2011; Yao et al., 2013) which require structured
training data, and language models (Jansen et al.,
2014; Sultan et al., 2014; Yih et al., 2013), which
operate over free text. Here, we close this gap in re-
source availability by developing a method to train
an alignment model over free text by making use of
discourse structures.

Discourse has been previously applied to QA to
help identify answer candidates that contain ex-
planatory text (e.g. Verberne et al. (2007)). Jansen
et al. (2014) proposed a reranking model that used
both shallow and deep discourse features to iden-
tify answer structures in large answer collections
across different tasks and genres. Here we use dis-
course to impose structure on free text to create
inexpensive knowledge resources for monolingual
alignment. Our work is conceptually complemen-
tary to that of Jansen et al. – where they explored

He makes it 
each autumn .

He uses the apples grown in 
his orchard

Bob told
Zoey

he likes
cider

He uses
the apples

grown in
his orchard. .

He makes it 
each autumn

Bob told Zoey he 
likes cider .. .

Bob told Zoey he likes cider.  He uses the apples grown 
in his orchard.  He makes it each autumn.

Sequential Model

RST Model

elaboration

elaboration

attribution elaboration

sequentialsequential

Figure 1: An example of the alignments produced by
the two discourse models. The sequential model aligns
pairs of consecutive sentences, capturing intersentence
associations such as cider–apples, and orchard–autumn.
The RST model generates alignment pairs from par-
ticipants in all (binary) discourse relations, capturing
both intrasentence and intersentence alignments, includ-
ing apples–orchard, cider–apples, and cider–autumn.

largely unlexicalized discourse structures to identify
explanatory text, we use discourse to learn lexical-
ized models for semantic similarity.

Our work is conceptually closest to that of Hickl
et al. (2006), who created artificially aligned pairs
for textual entailment. Taking advantage of the
structure of news articles, wherein the first sentence
tends to provide a broad summary of the article’s
contents, Hickl et al. aligned the first sentence of
each article with its headline. By making use of au-
tomated discourse parsing, here we go further and
impose alignment structure over an entire text.

3 Approach
A written text is not simply a collection of sentences,
but rather a flowing narrative where sentences and
sentence elements depend on each other for meaning
– a concept known as cohesion (Halliday and Hasan,
2014). Here we examine two methods for generat-
ing alignment training data from free text that make
use of cohesion: a shallow method that uses only in-
tersentence structures, and a deep method that uses
both intrasentence and intersentence structures. We
additionally attempt to separate the contribution of
discourse from that of alignment in general by com-
paring these models against a baseline alignment
model which aligns sentences at random.

The first model, the sequential discourse model
(SEQ), considers that each sentence continues the

232



narrative of the previous one, and creates artificial
question-answer pairs from all pairs of consecutive
sentences. Thus, this model takes advantage of in-
tersentence cohesion by aligning the content words3

in each sentence with the content words in the fol-
lowing sentence. For example, in the passage in Fig-
ure 1, this model would associate cider in the first
sentence with apples and orchard in the second sen-
tence.

The second model uses RST to capture discourse
cohesion both within and across sentence bound-
aries. We extracted RST discourse structures using
an in-house parser (Surdeanu et al., 2015), which
follows the architecture introduced by Hernault et
al. (2010) and Feng and Hirst (2012). The parser
first segments text into elementary discourse units
(EDUs), which may be at sub-sentence granular-
ity, then recursively connects neighboring units with
binary discourse relations, such as Elaboration or
Contrast.4 Our parser differs from previous work
with respect to feature generation in that we imple-
ment all features that rely on syntax using solely
dependency syntax. For example, a crucial feature
used by the parser is the dominance relations of
Soricut and Marcu (2003), which capture syntactic
dominance between discourse units located in the
same sentence. While originally these dominance
relations were implemented using constituent syn-
tax, we provide an equivalent implementation that
relies on dependency syntax. The main advantage to
this approach is speed: the resulting parser performs
at least an order of magnitude faster than the parser
of Feng and Hirst (2012).

Importantly, we generate artificial alignment pairs
from this imposed structure by aligning the govern-
ing text (nucleus) with its dependent text (satellite).5

Turning again to the example in Figure 1, this RST-
based model captures additional alignments that are
both intrasentence, e.g., apples–orchard, and inter-
sentence, e.g., cider–autumn.

3In pilot experiments, we found that aligning only nouns,
verbs, adjectives, and adverbs yielded higher performance.

4The RST parser performs better on relations which occur
more frequently. We use only relations that occurred at least
1% of the time. This amounted to six relations: elaboration,
attribution, background, contrast, same-unit, and joint. Using
all relations slightly improves performance by 0.3% P@1.

5Pilot experiments showed that this direction of alignment
performed better than aligning from satellite to nucleus.

4 Models and Features

We evaluate the contribution of these align-
ment models using a standard reranking architec-
ture (Jansen et al., 2014). The initial ranking of can-
didate answers is done using a shallow candidate re-
trieval (CR) component.6 Then, these answers are
reranked using a more expressive model that incor-
porates alignment features alongside the CR score.
As a learning framework we use SVMrank, a Sup-
port Vector Machine tailored for ranking.7 We com-
pare this alignment-based reranking model against
one that uses a state-of-the-art recurrent neural net-
work language model (RNNLM) (Mikolov et al.,
2010; Mikolov et al., 2013), which has been success-
fully applied to QA previously (Yih et al., 2013).

Alignment Model: The alignment matrices were
generated with IBM Model 1 (Brown et al., 1993)
using GIZA++ (Och and Ney, 2003), and the cor-
responding models were implemented as per Sur-
deanu et al. (2011) with a global alignment prob-
ability. We extend this alignment model with fea-
tures from Fried et al. (In press) that treat each
(source) word’s probability distribution (over des-
tination words) in the alignment matrix as a dis-
tributed semantic representation, and make use the
Jensen-Shannon distance (JSD)8 between these con-
ditional distributions. A summary of all these fea-
tures is shown in Table 1.

RNNLM: We learned word embeddings using the
word2vec RNNLM of Mikolov et al. (2013),
and include the cosine similarity-based features de-
scribed in Table 1.

5 Experiments

We tested our approach in two different domains,
open-domain and cellular biology. For consistency
we use the same corpora as Jansen et al. (2014),
which are described briefly here.

Yahoo! Answers (YA): Ten thousand open-domain
how questions were randomly chosen from the Ya-

6We use the same cosine similarity between question and
answer lemmas as Jansen et al. (2014), weighted using tf.idf.

7http://www.cs.cornell.edu/people/tj/
svm_light/svm_rank.html

8Jensen-Shannon distance is based on Kullback-Liebler di-
vergence but is a distance metric (finite and symmetric).

233



Feature Group Feature Descriptions

A
lig

nm
en

tM
od

el
s

Global Alignment Probability p(Q|A) according to IBM Model 1 (Brown et al., 1993)

Jenson-Shannon Distance (JSD) Pairwise JSDs were found between the probability distribution of each
content word in the question and those in the answer. The mean, mini-
mum, and maximum JSD values were used as features. Additionally,
composite vectors were formed which represented the entire question
and the entire answer and the overall JSD between these two vectors
was also included as a feature. See Fried et. al (In press) for additional
details.

R
N

N
L

M Cosine Similarity Similar to Jansen et al. (2014), we include as features the maximum
and average pairwise cosine similarity between question and answer
words, as well as the overall similarity between the composite question
and answer vectors.

Table 1: Feature descriptions for alignment models and RNNLM baseline.

hoo! Answers9 community question answering cor-
pus and divided: 50% for training, 25% for devel-
opment, and 25% for test. Candidate answers for a
given question are selected from the corresponding
answers proposed by the community (each question
has an average of 9 answers).

Biology QA (Bio): 183 how and 193 why questions
in the cellular biology domain were hand-crafted by
a domain expert, and paired with gold answers in the
Campbell’s Biology textbook (Reece et al., 2011).
Each paragraph in the textbook was considered as a
candidate answer. As there were few questions, five
fold cross-validation was used with three folds for
training, one for development, and one for test.

Alignment Corpora: To train the alignment mod-
els we generated alignment pairs from two differ-
ent resources: Annotated Gigaword (Napoles et al.,
2012) for YA, and the textbook for Bio. Each was
discourse parsed with the RST discourse parser de-
scribed in Section 3, which is implemented in the
FastNLPProcessor toolkit10, using the MaltParser11

for syntactic analysis.

5.1 Results and Discussion

Figure 2 shows the performance of the discourse
models against the number of documents used to
train the alignment model.12 We used the standard
implementation for P@1 (Manning et al., 2008)
with the adaptations for Bio described in Jansen et
al. (2014). We address the following questions.

9http://answers.yahoo.com
10http://github.com/sistanlp/processors
11http://www.maltparser.org/
12For space reasons the graph for Bio how is not shown, but

the pattern is essentially identical to Bio why.

Bio

YA

Figure 2: Overall performance for the two discourse-based
alignment models, compared against the CR baseline,
random baselines, and a RNNLM-based reranker. The x
axis indicates the number of training documents used to
construct all models. Each point represents the average
of 10 samples of training documents.

How does the performance of the RST and SEQ
models compare? Comparing the two principal
alignment models, the RST-based model signifi-
cantly outperforms the SEQ model by about 0.5%
P@1 in both domains (p < 0.001 for Bio and p <
0.01 for YA)13. This shows that deep discourse anal-

13All reported statistics were performed at the endpoints, i.e.,
when all training data is used, using bootstrap resampling with

234



ysis (as imperfect as it is today) is beneficial.

How does the performance of the RST model
compare to a model trained on in-domain pairs?
Both the RST and SEQ results for YA are higher
than that of an alignment model trained on explicit
in-domain question-answer pairs. Fried et. al (In
press) trained an identical alignment model using
approximately 65k QA pairs from the YA corpus,
and report a performance of 27.24% P@1, or nearly
2 points lower than our model trained using 10,000
Gigaword documents. This is an encouraging re-
sult, which further demonstrates that: (a) discourse
analysis can be exploited to generate artificial semi-
structured data for alignment, and (b) the sequen-
tial model, which also outperforms Fried et. al, can
be used as a reasonable proxy for discourse when a
parser is not available.

How does the performance of the RST model
compare to previous work? Comparing our work
to Jansen et al. (2014), the most relevant prior work,
we notice two trends. First, our discourse-based
alignment models outperform their CR + RNNLM
model, which peaks at 26.6% P@1 for YA and
31.7% for Bio why. While some of this difference
can be assigned to implementation differences (e.g.,
we consider only content words for both alignment
and RNNLM, where they used all words), this re-
sult again emphasizes the value of our approach.
Second, the partially lexicalized discourse structures
used by Jansen et. al to identify explanatory text in
candidate answers perform better than our approach,
which relies solely on lexicalized alignment. How-
ever, we expect that our two approaches are comple-
mentary, because they address different aspects of
the QA task (structure vs. similarity).

How do the RST and SEQ models compare to the
non-alignment baselines? In Bio, both the RST
and SEQ alignment models significantly outperform
the RNNLM and CR baselines (p < 0.001). In YA,
the RST and SEQ models significantly outperform
the CR baseline (p < 0.001), and though they con-
siderably outperform the the RNNLM baseline for
most training document sizes, when all 10,000 doc-
uments are used for training, they do not perform
better. This shows that alignment models are more

10,000 iterations.

robust to little training data, but RNNLMs catch up
when considerable data is available.

How does the SEQ model compare to the RND
baseline? In Bio, the SEQ model significantly
outperforms the RND baseline (p < 0.001) but in
YA it does not. This is likely due to differences
in the size of the document which was randomized.
In YA, the sentences were randomized within Gi-
gaword articles, which are relatively short (averag-
ing 19 sentences), whereas in Bio the randomization
was done at the textbook level. In practice, as docu-
ment size decreases, the RND model approaches the
SEQ model.

Why does performance plateau in YA and not in
Bio? With Bio, we exploit all of the limited in-
domain training data, and continue to see perfor-
mance improvements. With YA, however, perfor-
mance asymptotes for the alignment models when
trained beyond 10,000 documents, or less than 1%
of the Gigaword corpus. Similarly, when trained
over the entirety of Gigaword (two orders of mag-
nitude more data), our RNNLM improves only
slightly, peaking at approximately 30.5% P@1 (or, a
little over 1% P@1 higher). We hypothesize that this
limitation comes from failing to take context into ac-
count. In open domains, alignments such as apple –
orchard may interfere with those from different con-
texts, e.g., apple – computer, and add noise to the
answer selection process.

6 Conclusion
We propose two inexpensive methods for training
alignment models using solely free text, by gener-
ating artificial question-answer pairs from discourse
structures. Our experiments indicate that these
methods are a viable solution for constructing state-
of-the-art QA systems for low-resource domains, or
languages where training data is expensive and/or
limited. Since alignment models have shown utility
in other tasks (e.g. textual entailment), we hypothe-
size that these methods for creating inexpensive and
highly specialized training data could be useful for
tasks other than QA.

Acknowledgments
We thank the Allen Institute for AI for funding this
work.

235



References
Adam Berger, Rich Caruana, David Cohn, Dayne Frey-

tag, and Vibhu Mittal. 2000. Bridging the lexical
chasm: Statistical approaches to answer finding. In
Proceedings of the 23rd Annual International ACM SI-
GIR Conference on Research & Development on Infor-
mation Retrieval, Athens, Greece.

Peter F. Brown, Stephen A. Della Pietra, Vincent J. Della
Pietra, and Robert L. Mercer. 1993. The mathematics
of statistical machine translation: Parameter estima-
tion. Computational Linguistics, 19(2):263–311.

Abdessamad Echihabi and Daniel Marcu. 2003. A
noisy-channel approach to question answering. In
Proceedings of the 41st Annual Meeting on Associ-
ation for Computational Linguistics-Volume 1, pages
16–23. Association for Computational Linguistics.

Vanessa Wei Feng and Graeme Hirst. 2012. Text-level
discourse parsing with rich linguistic features. In Pro-
ceedings of the Association for Computational Lin-
guistics.

Daniel Fried, Peter Jansen, Gustave Hahn-Powell, Mihai
Surdeanu, and Peter Clark. In press. Higher-order lex-
ical semantic models for non-factoid answer rerank-
ing. Transactions of the Association for Computa-
tional Linguistics.

Michael Alexander Kirkwood Halliday and Ruqaiya
Hasan. 2014. Cohesion in english. Routledge.

H. Hernault, H. Prendinger, D. duVerle, and M. Ishizuka.
2010. HILDA: A discourse parser using support vec-
tor machine classification. Dialogue and Discourse,
1(3):1–33.

Andrew Hickl, John Williams, Jeremy Bensley, Kirk
Roberts, Bryan Rink, and Ying Shi. 2006. Recogniz-
ing textual entailment with lccs groundhog system. In
Proceedings of the Second PASCAL Challenges Work-
shop.

Peter Jansen, Mihai Surdeanu, and Peter Clark. 2014.
Discourse complements lexical semantics for non-
factoid answer reranking. In Proceedings of the 52nd
Annual Meeting of the Association for Computational
Linguistics (ACL).

William C. Mann and Sandra A. Thompson. 1988.
Rhetorical structure theory: Toward a functional the-
ory of text organization. Text, 8(3):243–281.

Christopher D. Manning, Prabhakar Raghavan, and Hin-
rich Schütze. 2008. Introduction to Information Re-
trieval. Cambridge University Press.

Tomas Mikolov, Martin Karafiat, Lukas Burget, Jan Cer-
nocky, and Sanjeev Khudanpur. 2010. Recurrent neu-
ral network based language model. In Proceedings
of the 11th Annual Conference of the International
Speech Communication Association (INTERSPEECH
2010).

Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey
Dean. 2013. Efficient estimation of word represen-
tations in vector space. In Proceedings of the In-
ternational Conference on Learning Representations
(ICLR).

Courtney Napoles, Matthew Gormley, and Benjamin
Van Durme. 2012. Annotated gigaword. In Pro-
ceedings of the Joint Workshop on Automatic Knowl-
edge Base Construction and Web-scale Knowledge Ex-
traction, AKBC-WEKEX ’12, pages 95–100, Strouds-
burg, PA, USA. Association for Computational Lin-
guistics.

Franz Josef Och and Hermann Ney. 2003. A system-
atic comparison of various statistical alignment mod-
els. Computational Linguistics, 29(1):19–51.

J.B. Reece, L.A. Urry, M.L. Cain, S.A. Wasserman, and
P.V. Minorsky. 2011. Campbell Biology. Pearson
Benjamin Cummings.

Stefan Riezler, Alexander Vasserman, Ioannis Tsochan-
taridis, Vibhu Mittal, and Yi Liu. 2007. Statistical
machine translation for query expansion in answer re-
trieval. In Proceedings of the 45th Annual Meeting of
the Association for Computational Linguistics (ACL),
pages 464–471, Prague, Czech Republic.

Radu Soricut and Eric Brill. 2006. Automatic question
answering using the web: Beyond the factoid. Journal
of Information Retrieval - Special Issue on Web Infor-
mation Retrieval, 9(2):191–206.

R. Soricut and D. Marcu. 2003. Sentence level discourse
parsing using syntactic and lexical information. In
Proceedings of the Human Language Technology and
North American Association for Computational Lin-
guistics Conference.

Md. Arafat Sultan, Steven Bethard, and Tamara Sum-
ner. 2014. Back to basics for monolingual align-
ment: Exploiting word similarity and contextual evi-
dence. Transactions of the Association for Computa-
tional Linguistics, 2:219–230.

Mihai Surdeanu, Massimiliano Ciaramita, and Hugo
Zaragoza. 2011. Learning to rank answers to non-
factoid questions from web collections. Computa-
tional Linguistics, 37(2):351–383.

Mihai Surdeanu, Thomas Hicks, and Marco A.
Valenzuela-Escárcega. 2015. Two practical rhetor-
ical structure theory parsers. In Proceedings of the
North American Chapter of the Association for Com-
putational Linguistics (NAACL): Software Demonstra-
tions.

Susan Verberne, Lou Boves, Nelleke Oostdijk, Peter-
Arno Coppen, et al. 2007. Discourse-based answer-
ing of why-questions. Traitement Automatique des
Langues, Discours et document: traitements automa-
tiques, 47(2):21–41.

236



Xuchen Yao, Benjamin Van Durme, Chris Callison-
Burch, and Peter Clark. 2013. Semi-markov phrase-
based monolingual alignment. In Proceedings of
EMNLP.

Wen-tau Yih, Ming-Wei Chang, Christopher Meek, and
Andrzej Pastusiak. 2013. Question answering using
enhanced lexical semantic models. In Proceedings of
the 51st Annual Meeting of the Association for Com-
putational Linguistics (ACL).

237


