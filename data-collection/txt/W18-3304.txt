











































Convolutional Attention Networks for Multimodal Emotion Recognition from Speech and Text Data


Proceedings of the First Grand Challenge and Workshop on Human Multimodal Language (Challenge-HML), pages 28–34,
Melbourne, Australia July 20, 2018. c©2018 Association for Computational Linguistics

28

 
 
 

 

Convolutional Attention Networks for Multimodal Emotion 

Recognition from Speech and Text Data 

 

 
Chan Woo Lee1, Kyu Ye Song1, Jihoon Jeong2, Woo Yong Choi1* 

1orbis.ai Inc., Seoul, South Korea 
1{cwlee, kysong, cchoi}@orbisai.co 

2Kyung Hee Cyber University, South Korea 
2jjeong@khcu.ac.kr 

 
 

 
Abstract 

Emotion recognition has become a popular 
topic of interest, especially in the field of 
human computer interaction. Previous 
works involve unimodal analysis of emo-
tion, while recent efforts focus on multi-
modal emotion recognition from vision 
and speech. In this paper, we propose a 
new method of learning about the hidden 
representations between just speech and 
text data using convolutional attention 
networks. Compared to the shallow model 
which employs simple concatenation of 
feature vectors, the proposed attention 
model performs much better in classifying 
emotion from speech and text data con-
tained in the CMU-MOSEI dataset.  

1 Introduction 

Emotion not only is a key driver to people’s ac-
tions and thoughts, but also is a fundamental part 
of human communication. As such, emotion 
recognition technology has become growingly 
important in improving how humans interact 
with machines [1]. For instance, emotion recog-
nition has been applied to analyze people’s reac-
tions to advertisements, thus creating better neu-
romarketing campaigns [2]. It has also gained in 
popularity amongst various other domains such 
as healthcare [3], customer service, or gaming.  
 
   However, effective emotion recognition still 
remains a challenging task, due to the sheer 
complexity of generalizing human emotions. For 

example, individuals express and perceive emo-
tions differently, depending on numerous per-
sonal characteristics such as but not limited to 
age [4], gender [5] and race. Previous efforts 
have used deep learning based approaches to an-
alyze emotion from single mode of expression, 
such as facial expression [6] or speech [7]. Since 
deep learning based approaches have been prov-
en to be effective at learning and generalizing 
data with high-dimensional feature spaces like 
images, similar efforts to capture complex fea-
ture space of emotional data have also shown 
promising results with several emotion databases 
such as EmoDB [8] or IEMOCAP [9]. Unfortu-
nately, human emotion in real-life is often ex-
pressed through complex combination of multi-
ple modes of expression, and a lot of information 
is lost by employing unimodal analysis. 
 

To solve this problem, using deep learning 
based approaches for multimodal emotion recog-
nition has been researched extensively in recent 
years. Work of Tzirakis et al. uses deep residual 
networks to extract features from facial expres-
sions, convolutional neural networks to extract 
features from speech, and concatenates them to 
input into a LSTM network [10]. Work of 
Ranganathan et al. uses deep believe networks 
on facial expressions, body expressions, vocal 
expressions, and physiological signals [11].  

 
Inspired by these approaches, we suggest a new 

approach to multimodal emotion recognition 
from just speech and text data. Feature vectors 
from embedded text sequences and speech spec-
trograms are extracted using convolutional neu-
ral network based architectures. A direct way to 
learn about the relationship between these two 

------------------------------------------------ 
* Corresponding Author: cchoi@orbisai.co 



29

 
 
 

 

feature vectors would be to utilize a shallow 
model, which is a simple concatenation of two 
feature vectors. However, since the correlations 
between feature vectors from speech and text is 
highly non-linear, it is difficult for a shallow 
model to properly learn multimodal representa-
tions. Therefore, we utilize trainable attention 
mechanisms to learn nonlinear correlations be-
tween these feature vectors. Attention mecha-
nisms also help retain information in the time-
domain by forming temporal embedding be-
tween two feature vectors. Since speech features 
and context shares the same time domain, using 
attention mechanism may help to discover new 
information for emotion classification. Attention 
models have previously been successfully ap-
plied to tasks such as image caption generation 
[12], machine translation [13], and speech 
recognition [14].  
 
  To demonstrate the benefits of this new ap-
proach, we use it to classify emotions from 
speech and text data provided in the CMU-
MOSEI dataset into six classes: happy, angry, 
sad, surprised, disgusted, and fear [15]. We also 
compare this approach to the shallow model ap-
proach to show how the attention mechanism can 
improve capturing of multimodal correlations be-
tween text and speech. 

2  Model  

The attention network shown in figure 1 is 
comprised of three separate convolutional neural 
networks: one each for feature extraction from 
speech spectrogram and word embedding se-
quence, and one for emotion classifier. Outputs 
from each of the CNNs from word embedding 
and spectrogram are used to compute an atten-
tion matrix for representing word embedding’s 
correlation to the spectrogram with respect to the 
emotion labelling. This attention matrix com-
bined with the input spectrogram to be inputted 
into the CNN based classifier for emotion. 

 
Input embedded word sequences have a size of 

!"×$  (e: embedding size, L: max sequence 
length), while input spectrograms have a size of  
!%×& (f: frequency range, t: time domain after 
FT). Word embedding size is fixed at 300, and 
raw text sentence length was capped at 40 words. 
Thereby, total word embedding sequence dimen-
sion results to 300 by 40. Input spectrograms are 
derived from transforming raw audio signals 
with a sample rate of 8000 Hz in the frequency 
ranges of 0~4kHz, with a fixed size of 200 x 400. 

 
To find the attention matrix between the two 

feature vectors, 1 by 1 convolution is conducted 
before calculating the dot product. The resulting 

Figure 1 Attention Networks for multimodal representation learning between speech and text data for 
emotion classification. Separate CNNs are used to extract features from speech spectrograms and em-
bedded word sequences. An attention matrix of m x n dimension is calculated by simply taking a soft-
max of the dot products of the feature vectors. This attention matrix is then multiplied to the spectro-
gram input, and goes through a third CNN for emotion classification. 



30

 
 
 

 

attention matrix has a size of m x n, determined 
by the last feature vector after 1 by 1 convolution. 
The column of the attention matrix is the atten-
tion of word sequence with respect to the spatial 
distribution of the input spectrogram. At the ex-
tend stage, feature dimensions that are lost due to 
max pooling in the convolutional layers is recov-
ered. By broadcasting attention values by 2^P, 
where P is the number of max pooling layers ap-
plied, attention values applied to the entire width 
of the spectrogram.   

  
Attention values are calculated using the fol-

lowing equations: 
  

'(& = 	 +,-(+/	∙	12)exp	(+272=1 	∙	12)	       (1) 
 

9& = 	 '/212:2=1 	             (2) 
 
+(	stands for the word embedded latent vector, 
while  1&	stands for the spectrogram latent vector. 
By taking a dot product of +(	and 1&	and taking a 
softmax of it, we are able to calculate '(&. Since 
taking a dot product of  +(	  and 1&  essentially 
equates to calculating the similarity between to 
vectors, '(&	is the similarity distribution with re-
spect to time domain. Next, by multiplying 
'(& and 1&	 element-wise, 9&  can be obtained, 
which essentially is the input spectrogram with 
attention information added. As shown in Figure 
1, the attention matrix can be constructed with m 
x n dimensions, and when visualized looks like 
Figure 2. 
 

     

 
   After the model learns the representation of 
each features for attention, the last CNN layer 
computes the weighted sum of all the infor-
mation extracted from the attention input. The 

output vector is then fed into a fully connected 
softmax layer for classification. 

3 Data and preprocessing 

3.1 Dataset  
We use audio and text data from CMU-

Multimodal Opinion Sentiment and Emotion In-
tensity (CMU-MOSEI) dataset for all experi-
ments [15]. The videos, totaling 23,141 files, are 
chosen from YouTube speakers including vari-
ous topics and monologue, and are gender bal-
anced. 

 
Annotations consist of six emotion indexes: 

sadness (2843), angry (6794), happy (10028), 
disgust (1845), surprise (349), fear (817) with 
value ranges of [0,4.6], and sentiment label with 
a value range of [-3,3]. The dataset is organized 
by video IDs and corresponding segments with 
six emotion and sentiment labels. Video IDs are 
then further split into segments. The training set 
consists 3303 video ID and 23453 segments, 
while the validation set consists of non-
overlapping 300 video IDs and 1834 segments.  

 
Text embedding was prepared using GloVe 

word2vec method. Each word embedding is 
fixed at a length of 300. The duration of each 
word utterance is also provided by the P2FA 
forced alignment [15].  

3.2 Data preprocessing 
   Speech raw signals are converted to spectro-

grams before being input into the attention net-
work using Short Time Fourier Transform (STFT) 
after resampling with a reduced sample rate from 
44100 Hz to 8000Hz, as seen in Figure 3. Ham-
ming window is used during STFT, and the length 
of each segment is 800. The transformed spectro-
gram is then converted to log-scale to make the 
vertical axis units of dB, with a frame size of 
200x400. 

4 Experimental results 

In this section, we describe the experiment meth-
odologies and report the recognition performance 
proposed attention network architecture on the 
CMU-MOSEI dataset [15].  

 

Figure 2 Visualization of the attention matrix. 
Row means time domain matching the input 
spectrogram, column means word sequence  



31

 
 
 

 

  

 

4.1 Methods  
All models are trained with the training dataset 

provided by the ACL 2018 Multimodal Chal-
lenge. This training dataset is a subset of the en-
tire CMU-MOSEI dataset. The models are vali-
dated using the provided validation dataset, again 
as part of the Challenge. Two sets of experiments 
are conducted: First, the shallow model architec-
ture (Figure 3) is trained with the training 
set. The proposed attention network architecture 
is trained end-to-end, and validated for perfor-
mance. We then train a shallow model as out-
lined in Figure 3 to use as a baseline to track 
how much improvement the attention network 
provides in learning the correlation between 
word embedding and corresponding spectrogram 
features. 

 

 
 

Figure 4 Shallow model diagram 
 

4.2 Hyperparameters 

Stochastic gradient descent with a set learning rate 
is employed during training. For regularization, 
dropout is applied to the last hidden layer. The 
system’s hyperparameters are: 32 kernels with 3 
kernel size; a batch size of 32; a dropout rate of 
0.1; learning rate of 1e-3; a pool size of 2 and 

stride of 2; the dense layer units after final CNN 
are 1024, 512, and 128 for all configurations. 

4.3 Evaluation  
    For each experiment, we report an overall ac-
curacy (each sentence across the dataset has an 
equal weight; weighted accuracy) and a class ac-
curacy (first evaluated for each emotion and then 
averaged; unweighted accuracy). All the classifi-
cation results are listed in Tables 1-2, including 
precision, recall, and f-1 score. Confusion matri-
ces are also provided to show how well the mod-
el correctly classifies each emotion, using the 
top-1 class prediction as a metric. 

4.4 Experiment 1: shallow model 
In this section, we report the results of training 

the shallow model with the CMU-MOSEI da-
taset. Since the shallow model is a common and 
the simplest method of multimodal emotion clas-
sification, we use it as a baseline model for com-
parison.  

 
The overall validation accuracy (weighted) is 

83.11% and class validation accuracy (un-
weighted) is 77.23% as shown in Table 1. The 
multi-class confusion matrix is shown in Figure 
5, showing the highest accuracies for anger and 
happy emotions, and lowest accuracies for fear 
and surprise emotions. 
    

 

Emotion Preci-sion    Recall f-1 score 

sadness 0.82  0.65 0.73 

happy 0.93  0.88 0.91 

anger 0.75  0.90 0.82 

disgust 0.75       0.75 0.75 

surprise 0.98      0.55 0.70 

fear 0.83      0.63 0.72 

average 0.85 0.84 0.84 
class 

accuracy 77.23% 
Overall 
accuracy 83.11% 

 
Table 1 The results of shallow model 

Figure 3 Speech spectrogram after STFT (Left: 
after STFT, Right: log scale) 



32

 
 
 

 

 
 

Figure 5 Confusion matrix of shallow model 
 

4.5 Experiment 2: attention model 
In this section, we report the results of atten-

tion model to compare to the baseline results. 
 

   The overall accuracy (weighted) is 88.89% and 
class accuracy (unweighted) is 84.08 % as shown 
in Table 2 for the attention model, a significant 
improvement from the same metrics of shallow 
model.  According to the confusion matrix 
shown in Figure 6, validation accuracies have in-
creased throughout all emotion classes compared 
to the baseline. 

 
 

Emotion Preci-sion Recall 
f-1 

score 

sadness 0.88       0.86 0.87 

happy 0.92       0.92 0.92 

anger 0.85       0.92 0.88 

disgust 0.88       0.81 0.84 

surprise 0.98       0.62 0.76 

fear 0.94       0.65 0.77 

average 0.89 0.89 0.89 
class 

accuracy 84.08% 
Overall 

accuracy 88.89% 

 
Table 2 The results of attention model 

 

5 Discussion 

Comparing the attention model to the shallow 
model, shallow model utilizes a superficial fea-
ture concatenation, while attention model calcu-
lates the similarity between two feature vectors 
that can be trained with learnable weights. In the 
context of the feature space, concatenating two 
feature vectors in the shallow model essentially 
is a simple increase in dimensionality. On the 
other hand, the feature space in the attention 
model is fixed to the audio feature space. How-
ever, since the features now depend on a new 
variable called attention, the model can selective-
ly utilize different features in the audio feature 
space to different extents for better classification. 
In other words, text data now plays an important 
role in determining whether a speech feature is 
important or not in classifying certain emotions, 
an especially important benefit for training da-
tasets with limited size or data balance.  
 

 
Figure 6 Confusion matrix of attention model 

 
In addition, correlation information between 

text and speech with respect to the time domain 
can be easily lost when shallow concatenation is 
utilized. Meanwhile, calculation of the attention 
matrix requires matrix multiplication between 
embedded word and spectrogram feature for a 
given time. Hence, time series information is re-
tained in the calculated attention matrix through 
temporal embedding, and to the resulting atten-
tion applied spectrogram. Since context and its 
vocal style of delivery plays an important role in 
communicating emotion, retaining the time in-
formation provides huge benefits in classifying 
emotions from just speech and text. 

 
Furthermore, while the shallow model is merely 

an analysis of a union of text and speech infor-



33

 
 
 

 

mation, the proposed attention model aims to 
discover new meaningful methods of how two 
feature vectors intersect. In other words, shallow 
model is highly single feature dependent, while 
attention model is not. This means that if each of 
the feature vectors contain inadequate infor-
mation to begin with, shallow model will per-
form much worse than attention model. 

 
Since the attention model provides newly dis-

covered correlation between the two feature vec-
tors, this new information can be used in ensem-
ble with the original text and speech feature vec-
tors.  

 
Of course, attention models aren’t silver bullets 

in choosing the desired features and discarding 
the rest. Without careful training of the model, 
distribution of the attention values can flatten 
out. For instance, if the input data contains too 
much padding, and the network has a big bias 
causing little optimization, the feature vector 
used to calculate the attention values will ap-
proximate to 0, and subsequently attention val-
ues will also approximate to 0. One possible so-
lution is the utilize loss masking on the padding 
of the input data so that a more dynamic softmax 
distribution in the attention matrix can be ob-
tained. 
 
It is worth noting that for both experiments, f-1 
scores of select classes, namely happy and anger 
are much higher than those of other classes. This 
is mainly due to a considerable class imbalance 
of the training set, in which ~44% of the data is 
happy, and ~30% of the data is angry. 

6 Conclusion 

The attention model proposed for multimodal 
emotion recognition from speech and text data 
provides an effective method of learning about the 
correlation between the two output feature vectors 
from separate yet jointly trained CNNs. This 
method is especially effective for correlation in-
formation between speech and text, because the 
context and the way it is delivered plays a crucial 
role in affective communication, and the attention 
model retains temporal information well through-
out its model. For future work, syncing the input 
text and speech data in the temporal dimension 
may help the attention network focus on learning 
the relationship between one speech segment and 

one word, instead of the relationship between 
whole speech segment and whole text segment. 
 
References  
 
[1] Arkin, R. C.; Fujita, M.; Takagi, T.; and Haseg-
awa, R. 2003. An ethological and emotional basis 
for human–robot interaction. Robotics and Auton-
omous Systems 42(3):191–201. 
 
[2] F.Burkhardt, J.Ajmera, R.Englert, J.Stegmann, 
and W.Burleson,  “Detecting anger in automated 
voice portal dialogs,” in Proc. Annu. Conf. Int. 
Speech Commun. Assoc., 2006, pp. 1053–1056. 
 
[3] Q.Ji, Z.Zhu, and P.Lan, “Real-time non intru-
sive monitoring and prediction of driver fatigue,” 
IEEE Trans. Veh. Technol., vol. 53, no. 4, pp. 
1052– 1068, Jul. 2004. 
 
[4] A.Mill, J.Alliketal.,“Age-related differences in 
emotion recognition ability: a cross-sectional 
study.”  Emotion, vol. 9, no. 5, p.619, 2009. 
 
[5] T.Vogtand, E.Andre ́, “Improving  automatic 
emotion recognition from speech via gender differ-
entiation,” in Proc. Language Re-sources and Eval-
uation Conference (LREC 2006), Genoa, 2006. 
 
[6] Simonyan, Karen, and Andrew Zisserman. 
"Very deep convolutional networks for large-scale 
image recognition." arXiv preprint 
arXiv:1409.1556 (2014). 
 
[7] C.N.Anagnostopoulos, T.Iliou, andI. Gian-
noukos, “Features and classifiers for emotion 
recognition from speech : A survey from 2000 to 
2011,” Artif. Intell. Rev., vol. 43, no. 2, pp. 155–
177, 2015. 
 
[8] Burkhardt, F., Paeschke, A., Rolfes, M., 
Sendlmeier, W., and Weiss, B. A database of ger-
man emotional speech. In Proc. INTERSPEECH 
2005, Lissabon, Portugal (2005), pp. 1517–1520. 
 
[9] Busso, Carlos, et al. "IEMOCAP: Interactive 
emotional dyadic motion capture database." Lan-
guage resources and evaluation 42.4 (2008): 335 
 
[10] Tzirakis, P., Trigeorgis, G., Nicolaou, M. A., 
Schuller, B. W., & Zafeiriou, S. (2017). End-to-end 
multimodal emotion recognition using deep neural 
networks. IEEE Journal of Selected Topics in Sig-
nal Processing, 11(8), 1301-1309. 
 
[11] Ranganathan, H., Chakraborty, S., & Pan-
chanathan, S. (2016, March). Multimodal emotion 



34

 
 
 

 

recognition using deep learning architectures. In 
Applications of Computer Vision (WACV), 2016 
IEEE Winter Conference on (pp. 1-9). IEEE. 
 
[12] K. Xu, J. Ba, R. Kiros, K. Cho, A. C. Cour-
ville, R. Salakhutdinov,R. S. Zemel, and Y. Ben-
gio, “Show, attend and tell: Neural image caption 
generation with visual attention.” in ICML, vol. 
14,2015,pp.77–81. 
 
[13] D. Bahdanau,  K. Cho,  and Y. Bengio, “Neu-
ral machine translation by jointly learning to align 
and translate,”arXiv preprint arXiv: 1409.0473 
2014 
 
[14] J.K.Chorowski, D.Bahdanau, D.Serdyuk, 
K.Cho, and Y.Ben-gio, “Attention-based models 
for speech recognition,” in Advances in Neural In-
formation Processing Systems, 2015, pp. 577–585. 
 
[15] Zadeh, Amir, et al. "Human Multimodal Lan-
guage in the Wild: A Novel Dataset and Interpreta-
ble Dynamic Fusion Model" Association for Com-
putational Linguistics (2018) 
 
[16] Poria, Soujanya, et al. "Context-dependent 
sentiment analysis in user-generated videos” Pro-
ceedings of the 55th Annual Meeting of the Asso-
ciation for Computational Linguistics (Volume 1: 
Long Papers) pp. 873-883 (2017) 
 
[17] Zadeh, Amir, et al. "Multi-attention recurrent 
network for human communication comprehen-
sion" arXiv preprint arXiv:1802.00923 (2018) 
 
[18] Poria, S., Cambria, E., Hazarika, D., Majum-
der, N., Zadeh, A. and Morency, L.P., 2017. Con-
text-dependent sentiment analysis in user-generated 
videos. In Proceedings of the 55th Annual Meeting 
of the Association for Computational Linguistics 
(Volume 1: Long Papers) (Vol. 1, pp. 873-883).  
 
[19] Poria, S., Cambria, E., Hazarika, D., Ma-
zumder, N., Zadeh, A. and Morency, L.P., 2017, 
November. Multi-level Multiple Attentions for 
Contextual Multimodal Sentiment Analysis. In 
2017 IEEE International Conference on Data Min-
ing (ICDM) (pp. 1033-1038). IEEE. 
 
[20] Zadeh, A., Liang, P., Vanbriesen, J., Poria, S., 
Cambria, E., Chen, M., Morency, L., 2018. Multi-
modal Language Analysis in the Wild: CMU-
MOSEI Dataset and Interpretable Dynamic Fusion 
Graph. Association for Computational Linguistics. 
 
   
 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 


