



















































A Neural Probabilistic Structured-Prediction Model for Transition-Based Dependency Parsing


Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics
and the 7th International Joint Conference on Natural Language Processing, pages 1213–1222,

Beijing, China, July 26-31, 2015. c©2015 Association for Computational Linguistics

A Neural Probabilistic Structured-Prediction Model for Transition-Based
Dependency Parsing

Hao Zhou†∗ Yue Zhang‡ Shujian Huang† Jiajun Chen†
†State Key Laboratory for Novel Software Technology, Nanjing University, China

‡Singapore University of Technology and Design, Singapore
{zhouh, huangsj, chenjj}@nlp.nju.edu.cn, yue zhang@sutd.edu.sg

Abstract

Neural probabilistic parsers are attrac-
tive for their capability of automatic fea-
ture combination and small data sizes.
A transition-based greedy neural parser
has given better accuracies over its lin-
ear counterpart. We propose a neural
probabilistic structured-prediction model
for transition-based dependency parsing,
which integrates search and learning.
Beam search is used for decoding, and
contrastive learning is performed for max-
imizing the sentence-level log-likelihood.
In standard Penn Treebank experiments,
the structured neural parser achieves a
1.8% accuracy improvement upon a com-
petitive greedy neural parser baseline, giv-
ing performance comparable to the best
linear parser.

1 Introduction

Transition-based methods have given competitive
accuracies and efficiencies for dependency pars-
ing (Yamada and Matsumoto, 2003; Nivre and
Scholz, 2004; Zhang and Clark, 2008; Huang and
Sagae, 2010; Zhang and Nivre, 2011; Goldberg
and Nivre, 2013). These parsers construct depen-
dency trees by using a sequence of transition ac-
tions, such as SHIFT and REDUCE, over input sen-
tences. High accuracies are achieved by using a
linear model and millions of binary indicator fea-
tures. Recently, Chen and Manning (2014) pro-
pose an alternative dependency parser using a neu-
ral network, which represents atomic features as
dense vectors, and obtains feature combination au-
tomatically other than devising high-order features
manually.

The greedy neural parser of Chen and Man-
ning (2014) gives higher accuracies compared to

∗Work done while the first author was visiting SUTD.

the greedy linear MaltParser (Nivre and Scholz,
2004), but lags behind state-of-the-art linear sys-
tems with sparse features (Zhang and Nivre,
2011), which adopt global learning and beam
search decoding (Zhang and Nivre, 2012). The
key difference is that Chen and Manning (2014) is
a local classifier that greedily optimizes each ac-
tion. In contrast, Zhang and Nivre (2011) leverage
a structured-prediction model to optimize whole
sequences of actions, which correspond to tree
structures.

In this paper, we propose a novel framework for
structured neural probabilistic dependency pars-
ing, which maximizes the likelihood of action se-
quences instead of individual actions. Follow-
ing Zhang and Clark (2011), beam search is ap-
plied to decoding, and global structured learn-
ing is integrated with beam search using early-
update (Collins and Roark, 2004). Designing such
a framework is challenging for two main reasons:

First, applying global structured learning to
transition-based neural parsing is non-trivial. A
direct adaptation of the framework of Zhang and
Clark (2011) under the neural probabilistic model
setting does not yield good results. The main rea-
son is that the parameter space of a neural network
is much denser compared to that of a linear model
such as the structured perceptron (Collins, 2002).
Due to the dense parameter space, for neural mod-
els, the scores of actions in a sequence are rela-
tively more dependent than that in the linear mod-
els. As a result, the log probability of an action se-
quence can not be modeled just as the sum of log
probabilities of each action in the sequence, which
is the case of structured linear model. We address
the challenge by using a softmax function to di-
rectly model the distribution of action sequences.

Second, for the structured model above,
maximum-likelihood training is computationally
intractable, requiring summing over all possible
action sequences, which is difficult for transition-

1213



based parsing. To address this challenge, we take
a contrastive learning approach (Hinton, 2002; Le-
Cun and Huang, 2005; Liang and Jordan, 2008;
Vickrey et al., 2010; Liu and Sun, 2014). Using
the sum of log probabilities over the action se-
quences in the beam to approximate that over all
possible action sequences.

In standard PennTreebank (Marcus et al., 1993)
evaluations, our parser achieves a significant accu-
racy improvement (+1.8%) over the greedy neu-
ral parser of Chen and Manning (2014), and gives
the best reported accuracy by shift-reduce parsers.
The incremental neural probabilistic framework
with global contrastive learning and beam search
could be used in other structured prediction tasks.

2 Background

2.1 Arc-standard Parsing

Transition-based dependency parsers scan an in-
put sentence from left to right, and perform a se-
quence of transition actions to predict its parse
tree (Nivre, 2008). In this paper, we employ
the arc-standard system (Nivre et al., 2007),
which maintains partially-constructed outputs us-
ing a stack, and orders the incoming words in the
input sentence in a queue. Parsing starts with an
empty stack and a queue consisting of the whole
input sentence. At each step, a transition action
is taken to consume the input and construct the
output. The process repeats until the input queue
is empty and stack contains only one dependency
tree.

Formally, a parsing state is denoted as ⟨j, S, L⟩,
where S is a stack of subtrees [. . . s2, s1, s0], j
is the head of the queue (i.e. [ q0 = wj , q1 =
wj+1 · · · ]), and L is a set of dependency arcs. At
each step, the parser chooses one of the following
actions:

• SHIFT: move the front word wj from the
queue onto the stacks.

• LEFT-ARC(l): add an arc with label l between
the top two trees on the stack (s1 ← s0), and
remove s1 from the stack.

• RIGHT-ARC(l): add an arc with label l be-
tween the top two trees on the stack (s1 →
s0), and remove s0 from the stack.

The arc-standard parser can be summarized as
the deductive system in Figure 1, where k denotes

input : w0 . . . wn−1
axiom : 0 : ⟨0, ϕ, ϕ, 0⟩
goal : 2n− 1 : ⟨n, s0, L⟩

SHIFT
k : ⟨j, S, L⟩

k + 1 : ⟨j + 1, S|wj , L⟩

LEFT-ARC(l)
k : ⟨j, S|s1|s0, L⟩

k + 1 : ⟨j, S|s0, L ∪ {s1 l←− s0}⟩

RIGHT-ARC(l)
k : ⟨j, S|s1|s0, L⟩

k + 1 : ⟨j, S|s1, L ∪ {s1 l−→ s0}⟩
Figure 1: The deductive system for arc-standard
dependency parsing.

the current parsing step. For a sentence with size
n, parsing stops after performing exactly 2n − 1
actions.

MaltParser uses an SVM classifier for deter-
ministic arc-standard parsing. At each step, Malt-
Parser generates a set of successor states according
to the current state, and deterministically selects
the highest-scored one as the next state.

2.2 Global Learning and Beam Search

The drawback of deterministic parsing is error
propagation. An incorrect action will have a nega-
tive influence to its subsequent actions, leading to
an incorrect output parse tree.

To address this issue, global learning and beam
search (Zhang and Clark, 2011; Bohnet and Nivre,
2012; Choi and McCallum, 2013) are used. Given
an input x, the goal of decoding is to find the
highest-scored action sequence globally.

y = arg max
y′∈GEN(x)

score(y′) (1)

Where GEN(x) denotes all possible action se-
quences on x, which correspond to all possible
parse trees. The score of an action sequence y is:

score(y) =
∑
a∈y

θ · Φ(a) (2)

Here a is an action in the action sequence y, Φ
is a feature function for a, and θ is the parameter
vector of the linear model. The score of an action
sequence is the linear sum of the scores of each
action. During training, action sequence scores are
globally learned.

1214



The parser of Zhang and Nivre (2011) is devel-
oped using this framework. The structured percep-
tron (Collins, 2002) with early update (Collins and
Roark, 2004) is applied for training. By utilizing
rich manual features, it gives state-of-the-art accu-
racies in standard Penn Treebank evaluation. We
take this method as one baseline.

2.3 Greedy Neural Network Model
Chen and Manning (2014) build a greedy neural
arc-standard parser. The model can be regarded as
an alternative implementation of MaltParser, using
a feedforward neural network to replace the SVM
classifier for deterministic parsing.

2.3.1 Model
The greedy neural model extracts n atomic fea-
tures from a parsing state, which consists of
words, POS-tags and dependency labels from the
stack ans queue. Embeddings are used to rep-
resent word, POS and dependency label atomic
features. Each embedding is represented as a d-
dimensional vector ei ∈ R. Therefore, the full
embedding matrix is E ∈ Rd×V , where V is the
number of distinct features. A projection layer is
used to concatenate the n input embeddings into a
vector x = [e1; e2 . . . en], where x ∈ Rd·n. The
purpose of this layer is to fine-tune the embedding
features. Then x is mapped to a dh-dimensional
hidden layer by a mapping matrix W1 ∈ Rdh×d·n
and a cube activation function:

h = (W1x + b1)3 (3)

Finally, h is mapped into a softmax output layer
for modeling the probabilistic distribution of can-
didate shift-reduce actions:

p =softmax(o) (4)

where

o = W2h (5)

W2 ∈ Rdo×dh and do is the number of shift-reduce
actions.

2.3.2 Features
One advantage of Chen and Manning (2014) is
that the neural network parser achieves feature
combination automatically. Their atomic features
are defined by following Zhang and Nivre (2011).
As shown in Table 1, the features are categorized

Templates

F w
s0w, s2w, q0w, q1w, q2w, lc1(s0)w, lc2(s0)w
s1w, rc2(s0)w, lc1(s1)w, lc2(s1)w, rc2(s1)w
rc1(s0)w, rc1(s1)w, lc1(lc1(s0))w, lc1(lc1(s1))w
rc1(rc1(s1))w, rc1(rc1(s0))w

F t

s0t, q0t, q1t, q2t, rc1(s0)t, lc1(s0)t, lc2(s0)t
s1t, s2t, lc1(s1)t, lc2(s1)t, rc1(s1)t, rc2(s0)t
rc2(s1)t, lc1(lc1(s0))t, lc1(lc1(s1))t
rc1(rc1(s0))t, rc1(rc1(s1))t

F l
rc1(s0)l, lc1(s0)l, lc2(s0)l, lc1(s1)l, lc2(s1)l
rc1(s1)l, rc2(s0)l, rc2(s1)l, lc1(lc1(s0))l
lc1(lc1(s1))l, rc1(rc1(s0))l, rc1(rc1(s1))l

Table 1: Feature templates.

into three types: Fw, F t, F l, which represents
word features, POS-tag features and dependency
label features, respectively.

For example, s0w and q0w represent the
first word on the stack and queue, respectively;
lc1(s0)w and rc1(s0)w represent the leftmost and
rightmost child of s0, respectively. Similarly,
lc1(s0)t and lc1(s0)l represent the POS-tag and
dependency label of the leftmost child of s0, re-
spectively.

Chen and Manning (2014) find that the cube
activation function in Equation (3) is highly ef-
fective in capturing feature interaction, which is
a novel contribution of their work. The cube func-
tion achieves linear combination between atomic
word, POS and label features via the product of
three element combinations. Empirically, it works
better compared to a sigmoid activation function.

2.4 Training
Given a set of training examples, the training ob-
jective of the greedy neural parser is to minimize
the cross-entropy loss, plus a l2-regularization
term:

L(θ) = −
∑
i∈A

log pi +
λ

2
∥ θ ∥2 (6)

θ is the set of all parameters (i.e. W1, W2, b,
E), and A is the set of all gold actions in the train-
ing data. AdaGrad (Duchi et al., 2011) with mini-
batch is adopted for optimization. We take the
greedy neural parser of Chen and Manning (2014)
as a second baseline.

3 Structured Neural Network Model

We propose a neural structured-prediction model
that scores whole sequences of transition actions,
rather than individual actions. As shown in Ta-
ble 2, the model can be seen as a neural prob-
abilistic alternative of Zhang and Nivre (2011),

1215



local
classifier

structured
prediction

linear
sparse

Section 2.1
(Nivre

et al., 2007)

Section 2.2
(Zhang and

Nivre, 2011)

neural
dense

Section 2.3
(Chen and

Manning, 2014)
this work

Table 2: Correlation between different parsers.

or a structured-prediction alternative of Chen and
Manning (2014). It combines the advantages of
both Zhang and Nivre (2011) and Chen and Man-
ning (2014) over the greedy linear MaltParser.

3.1 Neural Probabilistic Ranking
Given the baseline system in Section 2.2, the most
intuitive structured neural dependency parser is
to replace the linear scoring model with a neu-
ral probabilistic model. Following Equation 1, the
score of an action sequence y, which corresponds
to its log probability, is sum of log probability
scores of each action in the sequence.

s(y) =
∑
a∈y

log pa (7)

where pa is defined by the baseline neural model
of Section 2.3 (Equation 4). The training objec-
tive is to maximize the score margin between the
gold action sequences (yg) and these of incorrectly
predicated action sequences (yp):

L(θ) = max(0, δ−s(yg)+s(yp))+λ2 ∥ θ ∥
2 (8)

With this ranking model, beam search and
early-update are used. Given a training instance,
the negative example is the incorrectly predicted
output with largest score (Zhang and Nivre, 2011).

However, we find that the ranking model works
poorly. One explanation is that the actions in a
sequence is probabilistically dependent on each
other, and therefore using the total log probabil-
ities of each action to compute the log probabil-
ity of an action sequence (Equation 7) is inaccu-
rate. Linear models do not suffer from this prob-
lem, because the parameter space of linear models
is much more sparse than that of neural models.
For neural networks, the dense parameter space is
shared by all the actions in a sequence. Increasing
the likelihood of a gold action may also change the

likelihood of incorrect actions through the shared
parameters. As a result, increasing the scores of a
gold action sequence and simultaneously reducing
the scores of an incorrect action sequence does not
work well for neural models.

3.2 Sentence-Level Log-Likelihood

To overcome the above limitation, we try to di-
rectly model the probabilistic distribution of whole
action sequences. Given a sentence x and neural
networks parameter θ, the probability of the action
sequence yi is given by the softmax function:

p(yi | x, θ) = e
f(x, θ)i∑

yj∈GEN(x)
ef(x, θ)j

(9)

where

f(x, θ)i =
∑

ak∈yi
o(x, yi, k, ak) (10)

Here GEN(s) is the set of all possible valid ac-
tion sequences for a sentence x; o(x, yi, k, ak)
denotes the neural network score for the action
ak given x and yi. We use the same sub net-
work as Chen and Manning (2014) to calculate
o(x, yi, k, ak) (Equation 5). The same features
in Table 1 are used.

Given the training data as (X , Y ), our train-
ing objective is to minimize the negative log-
likelihood:

L(θ) = −
∑

(xi, yi)∈(X,Y )
log p(yi | xi, θ) (11)

= −
∑

(xi, yi)∈(X,Y )
log

ef(xi,θ)i

Z(xi, θ)
(12)

=
∑

(xi, yi)∈(X,Y )
log Z(xi, θ)− f(xi, θ)i

(13)

where

Z(x, θ) =
∑

yj∈GEN(x)
ef(x, θ)j (14)

Here, Z(x, θ) is called the partition function.
Following Chen and Manning(2014), we apply l2-
regularization for training.

For optimization, we need to compute gradients
for L(θ), which includes gradients of exponential

1216



numbers of negative examples in partition func-
tion Z(x, θ). However, beam search is used for
transition-based parsing, and no efficient optimal
dynamic program is available to estimate Z(x, θ)
accurately. We adopt a novel contrastive learning
approach to approximately compute Z(x, θ).

3.3 Contrastive Learning
As an alternative to maximize the likelihood on
some observed data, contrastive learning (Hinton,
2002; LeCun and Huang, 2005; Liang and Jordan,
2008; Vickrey et al., 2010; Liu and Sun, 2014) is
an approach that assigns higher probabilities to ob-
served data and lower probabilities to noisy data.

We adopt the contrastive learning approach, as-
signing higher probabilities to the gold action se-
quence compared to incorrect action sequences in
the beam. Intuitively, this method only penalizes
incorrect action sequences with high probabilities.
Our new training objective is approximated as:

L′(θ) = −
∑

(xi, yi)∈(X,Y )
log p′(yi | xi, θ) (15)

= −
∑

(xi, yi)∈(X,Y )
log

ef(xi,θ)i

Z ′(xi, θ)
(16)

=
∑

(xi, yi)∈(X,Y )
log Z ′(xi, θ)− f(xi, θ)i

(17)

where

Z ′(x, θ) =
∑

yj∈BEAM(x)
ef(x, θ)j (18)

p′(yi | x, θ) is the relative probability of the ac-
tion sequence yi, computed over only the action
sequences in the beam. Z ′(x, θ) is the contrastive
approximation of Z(x, θ). BEAM(x) returns the
predicated action sequences in the beam and the
gold action sequence.

We assume that the probability mass concen-
trates on a relatively small number of action se-
quences, which allows the use of a limited num-
ber of probable sequences to approximate the full
set of action sequences. The concentration may be
enlarged dramatically with an exponential activa-
tion function of the neural network (i.e. a > b ⇒
ea ≫ eb ).
3.4 The Neural Probabilistic

Structured-Prediction Framework
We follow Zhang and Clark (2011) to integrate
search and learning. Our search and learning

Algorithm 1: Training Algorithm for Struc-
tured Neural Parsing
Input: training examples (X, Y)
Output: θ
θ← pretrained embedding
for i← 1 to N do

x, y = RANDOMSAMPLE(X, Y)
δ = 0
foreach xj , yj ∈ x, y do

beam = ϕ
goldState = null
terminate = false
beamGold = true
while beamGold and not terminate
do

beam = DECODE(beam, xj , yj)
goldState =
GOLDMOVE(goldState, xj , yj)
if not ISGOLD(beam) then

beamGold = false
if ITEMSCOMPLETE(beam) then

terminate = true;

δ = δ + UPDATE(goldState, beam)
θ = θ + delta

framework for dependency parsing is shown as
Algorithm 1. In every training iteration i, we
randomly sample the training instances, and per-
form online learning with early update (Collins
and Roark, 2004). In particular, given a training
example, we use beam-search to decode the sen-
tence. At any step, if the gold action sequence falls
out of the beam, we take all the incorrect action
sequences in the beam as negative examples, and
the current gold sequence as a positive example
for parameter update, using the training algorithm
of Section 3.3. AdaGrad algorithm (Duchi et al.,
2011) with mini-batch is adopted for optimization.

In this way, the distribution of ot only full ac-
tion sequences (i.e. complete parse trees), but also
partial action sequences (i.e. partial outputs) are
modeled, which makes training more challenging.
The advantage of early update is that training is
used to guide search, minimizing search errors.

1217



4 Experiments

4.1 Set-up

Our experiments are performed using the English
Penn Treebank (PTB; Marcus et al., (1993)). We
follow the standard splits of PTB3, using sections
2-21 for training, section 22 for development test-
ing and section 23 for final testing. For compar-
ison with previous work, we use Penn2Malt1 to
convert constituent trees to dependency trees. We
use the POS-tagger of Collins (2002) to assign
POS automatically. 10-fold jackknifing is per-
formed for tagging the training data.

We follow Chen and Manning (2014), and use
the set of pre-trained word embeddings2 from
Collobert et al. (2011) with a dictionary size of
13,000. The word embeddings were trained on the
entire English Wikipedia, which contains about
631 million words.

4.2 WSJ Experiments

4.2.1 Development experiments
We set the following hyper-parameters according
to the baseline greedy neural parser (Chen and
Manning, 2014): embedding size d = 50, hidden
layer size dh = 200, regularization parameter λ =
10−8, initial learning rate of Adagrad α = 0.01.
For the structured neural parser, beam size and
mini-batch size are important to the parsing per-
formance. We tune them on the development set.

Beam size. Beam search enlarges the search
space. More importantly, the larger the beam is,
the more accurate our training algorithm is. the
Contrastive learning approximates the exact prob-
abilities over exponential many action sequences
by computing the relative probabilities over action
sequences in the beam (Equation 18). Therefore,
the larger the beam is, the more accurate the rela-
tive probability is.

The first column of Table 3 shows the accura-
cies of the structured neural parser on the devel-
opment set with different beam sizes, which im-
proves as the beam size increases. We set the final
beam size as 100 according to the accuracies on
development set.

The effect of integrating search and learning.
We also conduct experiments on the parser of

1http://stp.lingfil.uu.se/ nivre/research/Penn2Malt.html
2http://ronan.collobert.com/senna/

Description UAS
Baseline 91.63

structured greedy
beam = 1 74.90 91.63
beam = 4 84.64 91.92
beam = 16 91.53 91.90
beam = 64 93.12 91.84
beam = 100 93.23 91.81

Table 3: Accuracies of structured neural parsing
and local neural classification parsing with differ-
ent beam sizes.

Description UAS
greedy neural parser 91.47
ranking model 89.08
beam contrastive learning 93.28

Table 4: Comparison between sentence-level log-
likelihood and ranking model.

Chen and Manning (2014) with beam search de-
coding. The score of a whole action sequence
is computed by the sum of log action probabili-
ties (Equation 7). As shown in the second col-
umn of Table 3, beam search can improve pars-
ing slightly. When the beam size increases beyond
16, however, accuracy improvements stop. In con-
trast, by integrating beam search and global learn-
ing, our parsing performance benefits from large
beam sizes much more significantly. With a beam
size as 16, the structured neural parser gives an
accuracy close to that of baseline greedy parser3.
When the beam size is 100, the structured neural
parser outperforms baseline by 1.6%.

Zhang and Nivre (2012) find that global learn-
ing and beam search should be used jointly for
improving parsing using a linear transition-based
model. In particular, increasing the beam size,
the accuracy of ZPar (Zhang and Nivre, 2011) in-
creases significantly, but that of MaltParser does
not. For structured neural parsing, our finding is
similar: integrating search and learning is much
more effective than using beam search only in de-
coding.

Our results in Table 3 are obtained by using
the same beam sizes for both training and testing.
Zhang and Nivre (2012) also find that for their lin-

3Our baseline accuracy is a little lower than accuracy re-
ported in baseline paper (Chen and Manning, 2014), because
we use Penn2Malt to convert the Penn Treebank, and they use
LTH Conversion.

1218



0 1000 2000 3000 4000 5000
10

20

30

40

50

60

70

80

90

100

iteration

U
A

S

 

 

batch size = 1
batch size = 10
batch size = 1000
batch size = 2000
batch size = 5000

Figure 2: Parsing performance with different
training batch sizes.

ear model, the best results are achieved by using
the same beam sizes during training and testing.
We find that this observation does not apply to our
neural parser. In our case, a large training beam al-
ways leads to better results. This is likely because
a large beam improves contrastive learning. As a
result, our training beam size is set to 100 for the
final test.

Batch size. Parsing performance using neural
networks is highly sensitive to the batch size of
training. In greedy neural parsing (Chen and Man-
ning, 2014), the accuracy on the development data
improves from 85% to 91% by setting the batch
size to 10 and 100000, respectively. In structured
neural parsing, we fix the beam size as 100 and
draw the accuracies on the development set by the
training iteration.

As shown in Figure 2, in 5000 training itera-
tions, the parsing accuracies improve as the itera-
tion grows, yet different batch sizes result in dif-
ferent convergence accuracies. With a batch size
of 5000, the parsing accuracy is about 25% higher
than with a batch size of 1 (i.e. SGD). For the re-
maining experiments, we set batch size to 5000,
which achieves the best accuracies on develop-
ment testing.

4.2.2 Sentence-level maximum likelihood vs.
ranking model

We compare parsing accuracies of the sentence-
level log-likelihood + beam contrastive learning
(Section 3.2), and the structured neural parser with
probabilistic ranking (Section 3.1). As shown
in Table 4, performance of global learning with
ranking model is weaker than the baseline greedy

System UAS LAS Speed
baseline greedy parser 91.47 90.43 0.001
Huang and Sagae (2010) 92.10 0.04
Zhang and Nivre (2011) 92.90 91.80 0.03
Choi and McCallum (2013) 92.96 91.93 0.009
Ma et al. (2014) 93.06
Bohnet and Nivre (2012)†‡ 93.67 92.68 0.4
Suzuki et al. (2009)† 93.79
Koo et al. (2008)† 93.16
Chen et al. (2014)† 93.77

beam size
training decoding

100 100 93.28 92.35 0.07
100 64 93.20 92.27 0.04
100 16 92.40 91.95 0.01

Table 5: Results on WSJ. Speed: sentences per
second. †: semi-supervised learning. ‡: joint
POS-tagging and dependency parsing models.

parser. In contrast, structured neural parsing
with sentence-level log-likelihood and contrastive
learning gives a 1.8% accuracy improvement upon
the baseline greedy parser.

As mentioned in Section 3.1, a likely reason
for the poor performance of the structured neu-
ral ranking model may be that, the likelihoods of
action sequences are highly influenced by each
other, due to the dense parameter space of neural
networks. To maximize likelihood of gold action
sequence, we need to decrease the likelihoods of
more than one incorrect action sequences.

4.2.3 Final Results

Table 5 shows the results of our final parser and
a line of transition-based parsers on the test set.
Our structured neural parser achieves an accu-
racy of 93.28%, 0.38% higher than Zhang and
Nivre (2011), which employees millions of high-
order binary indicator features in parsing. The
model size of ZPar (Zhang and Nivre, 2011) is
over 250 MB on disk. In contrast, the model size
of our structured neural parser is only 25 MB. To
our knowledge, the result is the best reported re-
sult achieved by shift-reduce parsers on this data
set.

Bohnet and Nivre (2012) obtain an accuracy of
93.67%, which is higher than our parser. How-
ever, their parser is a joint model of parsing and
POS-tagging, and they use external data in pars-
ing. We also list the result of Chen et al. (2014),
Koo et al. (2008) and Suzuki et al. (2009) in
Table 5, which make use of large-scale unanno-
tated text to improve parsing accuracies. The
input embeddings of our parser are also trained

1219



over large raw text, and in this perspective our
model is correlated with the semi-supervised mod-
els. However, because we fine-tune the word em-
beddings in supervised training, the embeddings
of in-vocabulary words become systematically dif-
ferent from these of out-of-vocabulary words af-
ter training, and the effect of pre-trained out-of-
vocabulary embeddings become uncertain. In this
sense, our model can also be regarded as an al-
most fully supervised model. The same applies to
the models of Chen and Manning (2014).

We also compare the speed of the structured
neural parser on an Intel Core i7 3.40GHz CPU
with 16GB RAM. The structured neural parser
runs about as fast as Zhang and Nivre (Zhang and
Nivre, 2011) and Huang and Sagae (Huang and
Sagae, 2010). The results show that our parser
combines the benefits of structured models and
neural probabilistic models, offering high accura-
cies, fast speed and slim model size.

5 Related Work

Parsing with neural networks. A line of work
has been proposed to explore the effect of neu-
ral network models for constituent parsing (Hen-
derson, 2004; Mayberry III and Miikkulainen,
2005; Collobert, 2011; Socher et al., 2013;
Legrand and Collobert, 2014). Performances of
most of these methods are still well below the
state-of-the-art, except for Socher et al.(2013),
who propose a neural reranker based on a PCFG
parser. For transition-based dependency parsing,
Stenetorp (2013) applies a compositional vector
method (Socher et al., 2013), and Chen and Man-
ning (2014) propose a feed-forward neural parser.
The performances of these neural parsers lag be-
hind the state-of-the-art.

More recently, Dyer et al. (2015) propose a
greedy transition-based dependency parser, using
three stack LSTMs to represent the input, the stack
of partial syntactic trees and the history of parse
actions, respectively. By modeling more history,
the parser gives significant better accuracies com-
pared to the greedy neural parser of Chen and
Manning (2014).

Structured neural models. Collobert et
al. (2011) presents a unified neural network
architecture for various natural language pro-
cessing (NLP) tasks. They propose to use
sentence-level log-likelihood to enhance a neural
probabilistic model, which inspires our model.

Sequence labeling is used for graph-based de-
coding. Using the Viterbi algorithm, they can
compute the exponential partition function in
linear time without approximation. However, with
a dynamic programming decoder, their sequence
labeling model can only extract local features. In
contrast, our integrated approximated search and
learning framework allows rich global features.

Weiss et al. (2015) also propose a structured
neural transition-based parser by adopting beam
search and early updates. Their model is close
in spirit to ours in performing structured predic-
tion using a neural network. The main difference
is that their structured neural parser uses a greedy
parsing process for pre-training, and fine-tunes
an additional perceptron layer consisting of the
pre-trained hidden and output layers using struc-
tured perceptron updates. Their structured neural
parser achieves an accuracy of 93.36% on Stan-
ford conversion of the PTB, which is significant
higher than the baseline parser of Chen and Man-
ning (2014). Their results are not directly compa-
rable with ours due to different dependency con-
versions.

6 Conclusion

We built a structured neural dependency parsing
model. Compared to the greedy neural parser of
Chen and Manning (2014), our parser integrates
beam search and global contrastive learning. In
standard PTB evaluation, our parser achieved a
1.8% accuracy improvement over the parser of
Chen and Manning (2014), which shows the effect
of combining search and learning. To our knowl-
edge, the structured neural parser is the first neural
parser that outperforms the best linear shift-reduce
dependency parsers. The structured neural proba-
bilistic framework can be used in other incremen-
tal structured prediction tasks.

7 Acknowledgments

We would like to thank the anonymous review-
ers for their insightful comments. This work
was partially founded by the Natural Science
Foundation of China (61170181, 61300158), the
Jiangsu Provincial Research Foundation for Ba-
sic Research (BK20130580), Singapore Ministra-
try of Education Tier 2 Grant T2MOE201301 and
SRGISTD2012038 from Singapore University of
Technology and Design.

1220



References
Bernd Bohnet and Joakim Nivre. 2012. A transition-

based system for joint part-of-speech tagging and la-
beled non-projective dependency parsing. In Pro-
ceedings of the 2012 Joint Conference on Empiri-
cal Methods in Natural Language Processing and
Computational Natural Language Learning, pages
1455–1465. Association for Computational Linguis-
tics.

Danqi Chen and Christopher D Manning. 2014. A fast
and accurate dependency parser using neural net-
works. In Empirical Methods in Natural Language
Processing (EMNLP).

Wenliang Chen, Yue Zhang, and Min Zhang. 2014.
Feature embedding for dependency parsing. In Pro-
ceedings of the international conference on Compu-
tational linguistics. Association for Computational
Linguistics.

Jinho D Choi and Andrew McCallum. 2013.
Transition-based dependency parsing with selec-
tional branching. In ACL (1), pages 1052–1062.

Michael Collins and Brian Roark. 2004. Incremen-
tal parsing with the perceptron algorithm. In Pro-
ceedings of the 42nd Annual Meeting on Association
for Computational Linguistics, page 111. Associa-
tion for Computational Linguistics.

Michael Collins. 2002. Discriminative training meth-
ods for hidden markov models: Theory and exper-
iments with perceptron algorithms. In Proceedings
of the ACL-02 conference on Empirical methods in
natural language processing-Volume 10, pages 1–8.
Association for Computational Linguistics.

Ronan Collobert, Jason Weston, Léon Bottou, Michael
Karlen, Koray Kavukcuoglu, and Pavel Kuksa.
2011. Natural language processing (almost) from
scratch. The Journal of Machine Learning Re-
search, 12:2493–2537.

R. Collobert. 2011. Deep learning for efficient dis-
criminative parsing. In AISTATS.

John Duchi, Elad Hazan, and Yoram Singer. 2011.
Adaptive subgradient methods for online learning
and stochastic optimization. The Journal of Ma-
chine Learning Research, 12:2121–2159.

Chris Dyer, Miguel Ballesteros, Wang Ling, Austin
Matthews, and Noah A. Smith. 2015. Transition-
based dependency parsing with stack long short-
term memory. In Proceedings of the ACL confer-
ence. Association for Computational Linguistics.

Yoav Goldberg and Joakim Nivre. 2013. Training
deterministic parsers with non-deterministic oracles.
Transactions of the Association for Computational
Linguistics, 1:403–414.

James Henderson. 2004. Discriminative training of a
neural network statistical parser. In Proceedings of

the 42nd Annual Meeting on Association for Compu-
tational Linguistics, page 95. Association for Com-
putational Linguistics.

Geoffrey Hinton. 2002. Training products of experts
by minimizing contrastive divergence. Neural com-
putation, 14(8):1771–1800.

Liang Huang and Kenji Sagae. 2010. Dynamic pro-
gramming for linear-time incremental parsing. In
Proceedings of the 48th Annual Meeting of the Asso-
ciation for Computational Linguistics, pages 1077–
1086. Association for Computational Linguistics.

Terry Koo, Xavier Carreras, and Michael Collins.
2008. Simple semi-supervised dependency parsing.

Yann LeCun and F Huang. 2005. Loss functions
for discriminative training of energybased models.
AIStats.

J. Legrand and R. Collobert. 2014. Recurrent greedy
parsing with neural networks. In Proceedings of
the European Conference on Machine Learning and
Principles and Practice of Knowledge Discovery in
Databases (ECML-PKDD).

Percy Liang and Michael I Jordan. 2008. An
asymptotic analysis of generative, discriminative,
and pseudolikelihood estimators. In Proceedings of
the 25th international conference on Machine learn-
ing, pages 584–591. ACM.

Yang Liu and Maosong Sun. 2014. Contrastive un-
supervised word alignment with non-local features.
arXiv preprint arXiv:1410.2082.

Ji Ma, Yue Zhang, and Jingbo Zhu. 2014. Punctu-
ation processing for projective dependency parsing.
In Proc. of ACL.

Mitchell P Marcus, Mary Ann Marcinkiewicz, and
Beatrice Santorini. 1993. Building a large anno-
tated corpus of english: The penn treebank. Compu-
tational linguistics, 19(2):313–330.

Marshall R Mayberry III and Risto Miikkulainen.
2005. Broad-coverage parsing with neural net-
works. Neural Processing Letters, 21(2):121–132.

Joakim Nivre and Mario Scholz. 2004. Deterministic
dependency parsing of English text. In Proceedings
of the 20th International Conference on Computa-
tional Linguistics (COLING), pages 64–70.

Joakim Nivre, Johan Hall, Jens Nilsson, Atanas
Chanev, Gülsen Eryigit, Sandra Kübler, Svetoslav
Marinov, and Erwin Marsi. 2007. Maltparser:
A language-independent system for data-driven de-
pendency parsing. Natural Language Engineering,
13(2):95–135.

Joakim Nivre. 2008. Algorithms for deterministic in-
cremental dependency parsing. Computational Lin-
guistics, 34(4):513–553.

1221



Richard Socher, John Bauer, Christopher D Manning,
and Andrew Y Ng. 2013. Parsing with composi-
tional vector grammars. In In Proceedings of the
ACL conference. Citeseer.

Pontus Stenetorp. 2013. Transition-based dependency
parsing using recursive neural networks. In NIPS
Workshop on Deep Learning.

Jun Suzuki, Hideki Isozaki, Xavier Carreras, and
Michael Collins. 2009. An empirical study of semi-
supervised structured conditional models for depen-
dency parsing. In Proceedings of the 2009 Con-
ference on Empirical Methods in Natural Language
Processing: Volume 2-Volume 2, pages 551–560.
Association for Computational Linguistics.

David Vickrey, Cliff C Lin, and Daphne Koller. 2010.
Non-local contrastive objectives. In Proceedings
of the 27th International Conference on Machine
Learning (ICML-10), pages 1103–1110.

David Weiss, Christopher Alberti, Michael Collins, and
Slav Petrov. 2015. Structured training for neural
network transition-based parsing. In Proceedings of
the ACL conference. Association for Computational
Linguistics.

Hiroyasu Yamada and Yuji Matsumoto. 2003. Statis-
tical dependency analysis with support vector ma-
chines. In Proceedings of IWPT, volume 3.

Yue Zhang and Stephen Clark. 2008. A tale of
two parsers: investigating and combining graph-
based and transition-based dependency parsing us-
ing beam-search. In Proceedings of the Conference
on Empirical Methods in Natural Language Pro-
cessing, pages 562–571. Association for Computa-
tional Linguistics.

Yue Zhang and Stephen Clark. 2011. Syntactic pro-
cessing using the generalized perceptron and beam
search. Computational Linguistics, 37(1):105–151.

Yue Zhang and Joakim Nivre. 2011. Transition-based
dependency parsing with rich non-local features. In
Proceedings of the 49th Annual Meeting of the Asso-
ciation for Computational Linguistics: Human Lan-
guage Technologies: short papers-Volume 2, pages
188–193. Association for Computational Linguis-
tics.

Yue Zhang and Joakim Nivre. 2012. Analyzing
the effect of global learning and beam-search on
transition-based dependency parsing. In COLING
(Posters), pages 1391–1400.

1222


