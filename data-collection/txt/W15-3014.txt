



















































Montreal Neural Machine Translation Systems for WMT15


Proceedings of the Tenth Workshop on Statistical Machine Translation, pages 134–140,
Lisboa, Portugal, 17-18 September 2015. c©2015 Association for Computational Linguistics.

Montreal Neural Machine Translation Systems for WMT’15
Sébastien Jean?

University of Montreal
jeasebas@iro.umontreal.ca

Orhan Firat?
Middle East Technical University, Turkey
orhan.firat@ceng.metu.edu.tr

Kyunghyun Cho
University of Montreal

Roland Memisevic
University of Montreal

firstname.lastname@umontreal.ca

Yoshua Bengio
University of Montreal
CIFAR Senior Fellow

Abstract

Neural machine translation (NMT)
systems have recently achieved re-
sults comparable to the state of the art
on a few translation tasks, including
English→French and English→German.
The main purpose of the Montreal In-
stitute for Learning Algorithms (MILA)
submission to WMT’15 is to evaluate
this new approach on a greater variety of
language pairs. Furthermore, the human
evaluation campaign may help us and the
research community to better understand
the behaviour of our systems. We use
the RNNsearch architecture, which adds
an attention mechanism to the encoder-
decoder. We also leverage some of the
recent developments in NMT, including
the use of large vocabularies, unknown
word replacement and, to a limited degree,
the inclusion of monolingual language
models.

1 Introduction

Neural machine translation (NMT) is a recently
proposed approach for machine translation that re-
lies only on neural networks. The NMT system
is trained end-to-end to maximize the conditional
probability of a correct translation given a source
sentence (Kalchbrenner and Blunsom, 2013; Cho
et al., 2014; Sutskever et al., 2014; Bahdanau et
al., 2015). Although NMT has only recently been
introduced, its performance has been found to be
comparable to the state-of-the-art statistical ma-
chine translation (SMT) systems on a number of
translation tasks (Luong et al., 2015; Jean et al.,
2015). The main purpose of our submission to
WMT’15 is to test the NMT system on a greater

? equal contribution

variety of language pairs. As such, we trained sys-
tems on Czech↔English, German↔English and
Finnish→English. Furthermore, the human evalu-
ation campaign of WMT’15 will help us better un-
derstand the quality of NMT systems which have
mainly been evaluated using the automatic evalu-
ation metric such as BLEU (Papineni et al., 2002).

Most NMT systems are based on the encoder-
decoder architecture (Cho et al., 2014; Sutskever
et al., 2014; Kalchbrenner and Blunsom, 2013).
The source sentence is first read by the encoder,
which compresses it into a real-valued vector.
From this vector representation the decoder may
then generate a translation word-by-word. One
limitation of this approach is that a source sen-
tence of any length must be encoded into a fixed-
length vector. To address this issue, our systems
for WMT’15 use the RNNsearch architecture from
(Bahdanau et al., 2015). In this case, the encoder
assigns a context-dependent vector, or annotation,
to every source word. The decoder then selectively
combines the most relevant annotations to gener-
ate each target word.

NMT systems often use a limited vocabu-
lary of approximately 30, 000 to 80, 000 target
words, which leads them to generate many out-
of-vocabulary tokens (〈UNK〉). This may easily
lead to the degraded quality of the translations.
To sidestep this problem, we employ a variant of
importance sampling to help increase the target
vocabulary size (Jean et al., 2015). Even with
a larger vocabulary, there will almost assuredly
be words in the test set that were unseen during
training. As such, we replace generated out-of-
vocaulbary tokens with the corresponding source
words with a technique similar to those proposed
by (Luong et al., 2015).

Most NMT systems rely only on parallel data,
ignoring the wealth of information found in large
monolingual corpora. On Finnish→English, we
combine our systems with a recurrent neural net-

134



work (RNN) language model by recently proposed
deep fusion (Gülçehre et al., 2015). For the other
language pairs, we tried reranking n-best lists with
5-gram language models (Chen and Goodman.,
1998).

2 System Description

In this section, we describe the RNNsearch ar-
chitecture as well as the additional techniques we
used.

Mathematical Notations Capital letters are
used for matrices, and lower-case letters for
vectors and scalars. x and y are used for a
word in source and target sentences, respectively.
We boldface them into x, y and ŷ to denote
their continuous-space representation (word em-
beddings).

2.1 Bidirectional Encoder
To encode a source sentence (x1, . . . , xTx) of
length Tx into a sequence of annotations, we use
a bidirectional recurrent neural network (Schus-
ter and Paliwal, 1997). The bidirectional recur-
rent neural network (BiRNN) consists of two re-
current neural networks (RNN) that read the sen-
tence either forward (from left to right) or back-
ward. These RNNs respectively compute the
sequences of hidden states (

−→
h 1, . . . ,

−→
h Tx) and

(
←−
h 1, . . . ,

←−
h Tx). These two sequences are con-

catenated at each time step to form the annota-
tions (h1, . . . , hTx). Each annotation hi summa-
rizes the entire sentence, albeit with more empha-
sis on word xi and the neighbouring words.

We built the BiRNN with gated recurrent
units (GRU, (Cho et al., 2014)), although long
short-term memory (LSTM) units could also be
used (Hochreiter and Schmidhuber, 1997), as in
(Sutskever et al., 2014). More precisely, for the
forward RNN, the hidden state at the i-th word is
computed as

−→
h i =

{
(1−−→z i)�−→h i−1 +−→z i �−→h i , if i > 0
0 , if i = 0

where

−→
h i = tanh

(−→
Wxi +

−→
U
[−→r i �−→h i−1]+−→b )

−→z i =σ
(−→
W zxi +

−→
U z
−→
h i−1

)
−→r i =σ

(−→
W rxi +

−→
U r
−→
h i−1

)
.

To form the new hidden state, the network first
computes a proposal

−→
h i. This is then additively

combined with the previous hidden state
−→
h i−1,

and this combination is controlled by the update
gate −→z i. Such gated units facilitate capturing
long-term dependencies.

2.2 Attentive Decoder
After computing the initial hidden state
s0 = tanh

(
Ws
←−
h 1

)
+ bs, the RNNsearch

decoder alternates between three steps: Look,
Generate and Update.

During the Look phase, the network determines
which parts of the source sentence are most rele-
vant. Given the previous hidden state si−1 of the
decoder recurrent neural network (RNN), each an-
notation hj is assigned a score eij :

eij = v>a tanh (Wasi−1 + Uahj) .

Although a more complex scoring function can
potentially learn more non-trivial alignments, we
observed that this single-hidden-layer function is
enough for most of the language pairs we consid-
ered.

These scores eij are then normalized to sum to
1:

αij =
exp (eij)∑Tx

k=1 exp (eik)
, (1)

which we call alignment weights.
The context vector ci is computed as a weighted

sum of the annotations (h1, ..., hTx) according to
the alignment weights:

ci =
Tx∑
j=1

αijhj .

This formulation allows the annotations with
higher alignment weights to be more represented
in the context vector ci.

In the Generate phase, the decoder predicts the
next target word. We first combine the previous
hidden state si−1, the previous word yi−1 and the
current context vector ci into a vector t̃i:

t̃i = Uosi−1 + Voyi−1 + Coci + bo.

We then transform t̃i into a hidden statemi with an
arbitrary feedforward network. In our submission,
we apply the maxout non-linearity (Goodfellow et
al., 2013) to t̃i, followed by an affine transforma-
tion.

135



Phase Output← Input
Look ci ← si−1, (h1, ..., hTx)
Generate yi ← si−1, yi−1, ci
Update si ← si−1, yi, ci

Table 1: Summary of RNNsearch decoder phases

For a target vocabulary V , the probability of
word yi is then

p(yi|si−1, yi−1, ci) =
exp

(
ŷ>i mi + byi

)∑
y∈V exp (ŷ>mi + by)

.

(2)
Finally, in the Update phase, the decoder com-

putes the next recurrent hidden state si from the
context ci, the generated word yi and the previ-
ous hidden state si−1. As with the encoder we use
gated recurrent units (GRU).

Table 1 summarizes this three-step procedure.
We observed that it is important to have Update to
follow Generate. Otherwise, the next step’s Look
would not be able to resolve the uncertainty em-
bedded in the previous hidden state about the pre-
viously generated word.

2.3 Very Large Target Vocabulary Extension

Training an RNNsearch model with hundreds of
thousands of target words easily becomes pro-
hibitively time-consuming due to the normaliza-
tion constant in the softmax output (see Eq. (2).)
To address this problem, we use the approach pre-
sented in (Jean et al., 2015), which is based on
importance sampling (Bengio and Sénécal, 2008).
During training, we choose a smaller vocabulary
size τ and divide the training set into partitions,
each of which contains approximately τ unique
target words. For each partition, we train the
model as if only the unique words within it existed,
leaving the embeddings of all the other words
fixed.

At test time, the corresponding subset of target
words for each source sentence is not known in
advance, yet we still want to keep computational
complexity manageable. To overcome this, we
run an existing word alignment tool on the train-
ing corpus in advance to obtain word-based con-
ditional probabilities (Brown et al., 1993). During
decoding, we start with an initial target vocabu-
lary containing the K most frequent words. Then,
reading a few sentences at once, we arbitrarily re-
place some of these initial words by the K ′ most

likely ones for each source word.1

No matter how large the target vocabulary is,
there will almost always be those words, such as
proper names or numbers, that will appear only in
the development or test set, but not during train-
ing. To handle this difficulty, we replace un-
known words in a manner similar to (Luong et
al., 2015). More precisely, for every predicted
out-of-vocabulary token (〈UNK〉), we determine
its most likely origin by choosing the source word
with the largest alignment weight αij (see Eq. (1).)
We may then replace 〈UNK〉 by either the most
likely word according to a dictionary, or simply
by the source word itself. Depending on the lan-
guage pairs, we used different heuristics according
to performance on the development set.

2.4 Integrating Language Models

Unlike some data-rich language pairs, most of
the translation tasks do not have enough paral-
lel text to train end-to-end machine translation
systems. To overcome with this issue of low-
resource language pairs, external monolingual cor-
pora is exploited by using the method of deep fu-
sion (Gülçehre et al., 2015).

In addition to the RNNsearch model, we train a
separate language model (LM) with a large mono-
lingual corpus. Then, the trained LM is plugged
into the decoder of the trained RNNsearch with
an additional controller network which modulates
the contributions from the RNNsearch and LM.
The controller network takes as input the hidden
state of the LM, and optionally RNNsearch’s hid-
den state, and outputs a scalar value in the range
[0, 1]. This value is multiplied to the LM’s hid-
den state, controlling the amount of information
coming from the LM. The combined model, the
RNNsearch, the LM and the controller network, is
jointly tuned as the final translation model for a
low-resource pair.

In our submission, we used recurrent neural net-
work language model (RNNLM). More specif-
ically, let sLMi be the hidden state of a pre-
trained RNNLM and sTMi be that of a pre-trained
RNNsearch at time i. The controller network is
defined as

gt = σ
(
V >g s

LM
t +W

>
g s

TM
t + bg

)
,

1This step differs very slightly from (Jean et al., 2015),
where the sentence-specific words were added on top of the
K common ones instead of replacing them.

136



where σ is a logistic sigmoid function, vg, wg and
bg are model parameters. The output of the con-
troller network is multiplied to the LM’s hidden
state sLMi :

pLMt = s
LM
t � gt.

The Generate phase in Sec. 2.2 is updated as,

t̃i = UTMo s
TM
i−1 +U

LM
o p

LM
t−1 + Voyi−1 +Coci + bo.

This lets the decoder fully use the signal from the
translation model, while the the signal from the
LM is modulated by the controller output.

Among all the pairs of languages in WMT’15,
Finnish↔English translation has the least amount
of parallel text, having approximately 2M aligned
sentences only. Thus, we use the deep fusion for
the Fi-En in the official submission. However, we
further experimented German→English, having
the second least parallel text, and Czech→English,
which has comparably larger data. We include
the results from these two language pairs here for
completeness.

3 Experimental Details

We now describe the settings of our experiments.
Except for minor differences, all the settings were
similar across all the considered language pairs.

3.1 Data

All the systems, except for the English→German
(En→De) system, were built using all the data
made available for WMT’15. The En→De sys-
tem, which was showcased in (Jean et al., 2015),
was built earlier than the others, using only the
data from the last year’s workshop (WMT’14.)

Each corpus was tokenized, but neither lower-
cased nor truecased. We avoided badly aligned
sentence pairs by removing any source-target sen-
tence pair with a large mismatch between their
lengths. Furthermore, we removed sentences that
were likely written in an incorrect language, ei-
ther with a simple heuristic for En→De, or with
a publicly available toolkit for the other language
pairs (Shuyo, 2010). In order to limit the memory
use during training, we only trained the systems
with sentences of length up to 50 words only. Fi-
nally, for some but not all models, we reshuffled
the data a few times and concatenated the differ-
ent segments before training.

In the case of German (De) source, we
performed compound splitting (Koehn and

Knight, 2003), as implemented in the Moses
toolkit (Koehn et al., 2007). For Finnish (Fi),
we used Morfessor 2.0 for morpheme segmenta-
tion (Virpioja et al., 2013) by using the default
parameters.

An Issue with Apostrophes In the training data,
apostrophes appear in many forms, such as a
straight vertical line (U+0027) or as a right sin-
gle quotation mark (U+0019). The use of, for
instance, the normalize-punctuation script2 could
have helped, but we did not use it in our exper-
iments. Consequently, we encountered an issue
of the tokenizer from the Moses toolkit not apply-
ing the same rule for both kinds of apostrophes.
We fixed this issue in time for Czech→English
(Cs→En), but all the other systems were affected
to some degree, in particular, the system for
De→En.

3.2 Settings

We used the RNNsearch models of size identi-
cal to those presented in (Bahdanau et al., 2015;
Jean et al., 2015). More specifically, all the
words in both target and source vocabularies were
projected into a 620-dimensional vector space.
Each recurrent neural network (RNN) had a 1000-
dimensional hidden state. The models were
trained with Adadelta (Zeiler, 2012), and the norm
of the gradient at each update was rescaled (Pas-
canu et al., 2013). For the language pairs other
than Cs→En and Fi→En, we held the word em-
beddings fixed near the end of training, as de-
scribed in (Jean et al., 2015).

With the very large target vocabulary technique
in Sec. 2.3, we used 500K source and target
words for the En→De system, while 200K source
and target words were used for the De→En and
Cs↔En systems.3 During training we set τ be-
tween 15K and 50K, depending on the hardware
availability. As for decoding, we mostly used
K = 30, 000 and K ′ = 10.

Given the small sizes of the Fi→En corpora, we
simply used a fixed vocabulary size of 40K to-
kens to avoid any adverse effect of including ev-
ery unique target word in the vocabulary. The in-
clusion of every unique word would prevent the
network from decoding out 〈UNK〉 at all, even if

2http://www.statmt.org/wmt11/
normalize-punctuation.perl

3This choice was made mainly to cope with the limited
storage availability.

137



Language pair
BLEU-c BLEU-c ranking

Human ranking
single ensemble constrained unconstrained

En→Cs 15.7 18.3 1/6 2/7 4/8
En→De 22.4 24.8 1/11 1/13 1-2/16
Cs→En 20.2 23.3 3/6 3/6 3-4/7
De→En 25.6 27.6 6/9 6/10 6-7/13
Fi→En 10.1 13.6 7/9 9/12 10/14

Table 2: Results on the official WMT’15 test sets for single models and primary ensemble submissions.
All our own systems are constrained. When ranking by BLEU, we only count one system from each
submitter. Human rankings include all primary and online systems, but exclude those used in the Cs↔En
tuning task.

out-of-vocabulary words will assuredly appear in
the test set.

For each language pair, we trained a total of four
independent models that differed in parameter ini-
tialization and data shuffling, monitoring the train-
ing progress on either newstest2012+2013, new-
stest2013 or newsdevs2015.4 Translations were
generated by beam search, with a beam width of
20, trying to find the sentence with the highest log-
probability (single model), or highest average log-
probability over all models (ensemble), divided by
the sentence length (Boulanger-Lewandowski et
al., 2013). This length normalization addresses the
tendency of the recurrent neural network to output
shorter sentences.

For Fi→En, we augmented models by deep fu-
sion with an RNN-LM. The RNN-LM, which was
built using the LSTM units, was trained on the En-
glish Gigaword corpus using the vocabulary com-
prising of the 42K most frequent words in the En-
glish side of the intersection of the parallel cor-
pora of Fi→En, De→En and Cs→En. Impor-
tantly, we use the same RNN-LM for both Fi→En,
Cs→En and De→En. In the experiments with
deep fusion, we used the randomly selected 2/3
of newsdev2015 as a validation set and the rest as
a held-out set. In the case of De→En, we used
newstest2013 for validation and newstest2014
for test.

For all language pairs except Fi→En, we also
simply built 5-gram language models, this time on
all appropriate provided data, with the exception
of the English Gigaword (Heafield, 2011). In our
contrastive submissions only, we re-ranked our 20-
best lists with the LM log-probabilities, once again
divided by sentence length. The relative weight of
the language model was manually chosen to max-

4For En→De, we created eight semi-independent models.
See (Jean et al., 2015) for more details.

imize BLEU on the development set.

4 Results

Results for single systems and primary ensem-
ble submissions are presented in Table 2.5 When
translating from English to another language, neu-
ral machine translation works particularly well,
achieving the best BLEU-c scores among all the
constrained systems. On the other hand, NMT is
generally competitive even in the case of trans-
lating to English, but it not yet as good as well
as the best SMT systems according to BLEU. If
we rather rely on human judgement instead of au-
tomated metrics, the NMT systems still perform
quite well over many language pairs, although
they are in some instances surpassed by other sta-
tistical systems that have slightly lower BLEU
scores.

In our contrastive submissions for Cs↔En and
De↔En where we re-ranked 20-best lists with a 5-
gram language model, BLEU scores went up mod-
estly by 0.1 to 0.5 BLEU, but interestingly transla-
tion error rate (TER) always worsened. One possi-
ble drawback about the manner we integrated lan-
guage models here is the lack of translation mod-
els in the reverse direction, meaning we do not
implicitely leverage the Bayes’ rule as most other
translation systems do.

In our further experiments, which are not part
of our WMT’15 submission, for single models
we observed the improvements of approximately
1.0/0.5 BLEU points for dev/test in {Cs,De}→En
tasks, when we employ deep fusion for incorporat-
ing language models.6

5Also available at http://matrix.statmt.org/
matrix/

6Improvements are for single models only. See (Gülçehre
et al., 2015) for more details.

138



5 Conclusion

We presented the MILA neural machine trans-
lation (NMT) systems for WMT’15, using the
encoder–decoder model with the attention mech-
anism (Bahdanau et al., 2015) and the recent de-
velopments in NMT (Jean et al., 2015; Gülçehre
et al., 2015). We observed that the NMT sys-
tems are now competitive against the conventional
SMT systems, ranking first by BLEU among the
constrained submission on both the En→Cs and
En→De tasks. In the future, more analysis is
needed on the influence of the source and target
languages for neural machine translation. For in-
stance, it would be interesting to better understand
why performance relative to other approaches was
somewhat weaker when translating into English,
or how the amount of reordering influences the
translation quality of neural MT systems.

Acknowledgments

The authors would like to thank the develop-
ers of Theano (Bergstra et al., 2010; Bastien
et al., 2012). We thank Kelvin Xu, Bart van
Mërrienboer, Dzmitry Bahdanau and Étienne Si-
mon for their help. We also thank the two anony-
mous reviewers and everyone who contributed to
the manual evaluation campaign. We acknowl-
edge the support of the following agencies for re-
search funding and computing support: NSERC,
Calcul Québec, Compute Canada, the Canada Re-
search Chairs, CIFAR, Samsung and TUBITAK.

References
Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben-

gio. 2015. Neural machine translation by jointly
learning to align and translate. In ICLR’2015,
arXiv:1409.0473.

Frédéric Bastien, Pascal Lamblin, Razvan Pascanu,
James Bergstra, Ian J. Goodfellow, Arnaud Berg-
eron, Nicolas Bouchard, and Yoshua Bengio. 2012.
Theano: new features and speed improvements.
Deep Learning and Unsupervised Feature Learning
NIPS 2012 Workshop.

Yoshua Bengio and Jean-Sébastien Sénécal. 2008.
Adaptive importance sampling to accelerate train-
ing of a neural probabilistic language model. IEEE
Trans. Neural Networks, 19(4):713–722.

James Bergstra, Olivier Breuleux, Frédéric Bastien,
Pascal Lamblin, Razvan Pascanu, Guillaume Des-
jardins, Joseph Turian, David Warde-Farley, and
Yoshua Bengio. 2010. Theano: a CPU and

GPU math expression compiler. In Proceedings
of the Python for Scientific Computing Conference
(SciPy), June. Oral Presentation.

Nicolas Boulanger-Lewandowski, Yoshua Bengio, and
Pascal Vincent. 2013. Audio chord recognition with
recurrent neural networks. In ISMIR.

Peter F Brown, Vincent J Della Pietra, Stephen A Della
Pietra, and Robert L Mercer. 1993. The mathemat-
ics of statistical machine translation: Parameter esti-
mation. Computational linguistics, 19(2):263–311.

Stanley F. Chen and Joshua T. Goodman. 1998. An
empirical study of smoothing techniques for lan-
guage modeling. Technical Report TR-10-98, Com-
puter Science Group, Harvard University.

Kyunghyun Cho, Bart van Merrienboer, Caglar Gul-
cehre, Fethi Bougares, Holger Schwenk, and Yoshua
Bengio. 2014. Learning phrase representations
using RNN encoder-decoder for statistical machine
translation. In Proceedings of the Empiricial
Methods in Natural Language Processing (EMNLP
2014), October.

Ian Goodfellow, David Warde-Farley, Mehdi Mirza,
Aaron Courville, and Yoshua Bengio. 2013. Max-
out networks. In Proceedings of The 30th Inter-
national Conference on Machine Learning, pages
1319–1327.

Çaglar Gülçehre, Orhan Firat, Kelvin Xu, Kyunghyun
Cho, Loı̈c Barrault, Huei-Chi Lin, Fethi Bougares,
Holger Schwenk, and Yoshua Bengio. 2015. On
using monolingual corpora in neural machine trans-
lation. CoRR, abs/1503.03535.

Kenneth Heafield. 2011. KenLM: faster and smaller
language model queries. In Proceedings of the
EMNLP 2011 Sixth Workshop on Statistical Ma-
chine Translation, pages 187–197, Edinburgh, Scot-
land, United Kingdom, July.

S. Hochreiter and J. Schmidhuber. 1997. Long short-
term memory. Neural Computation, 9(8):1735–
1780.

Sébastien Jean, Kyunghyun Cho, Roland Memisevic,
and Yoshua Bengio. 2015. On using very large tar-
get vocabulary for neural machine translation. In
Proceedings of ACL.

Nal Kalchbrenner and Phil Blunsom. 2013. Recurrent
continuous translation models. In Proceedings of
the ACL Conference on Empirical Methods in Nat-
ural Language Processing (EMNLP), pages 1700–
1709. Association for Computational Linguistics.

Philipp Koehn and Kevin Knight. 2003. Empirical
methods for compound splitting. In Proceedings of
the tenth conference on European chapter of the As-
sociation for Computational Linguistics-Volume 1,
pages 187–193. Association for Computational Lin-
guistics.

139



Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ondřej Bojar, Alexandra
Constantin, and Evan Herbst. 2007. Moses: Open
source toolkit for statistical machine translation. In
Proceedings of the 45th Annual Meeting of the ACL
on Interactive Poster and Demonstration Sessions,
ACL ’07, pages 177–180, Stroudsburg, PA, USA.
Association for Computational Linguistics.

Minh-Thang Luong, Ilya Sutskever, Quoc V. Le, Oriol
Vinyals, and Wojciech Zaremba. 2015. Addressing
the rare word problem in neural machine translation.
In Proceedings of ACL.

Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: A method for automatic
evaluation of machine translation. In Proceedings
of the 40th Annual Meeting on Association for Com-
putational Linguistics, ACL ’02, pages 311–318,
Stroudsburg, PA, USA. Association for Computa-
tional Linguistics.

Razvan Pascanu, Tomas Mikolov, and Yoshua Bengio.
2013. On the difficulty of training recurrent neural
networks.

Mike Schuster and Kuldip K Paliwal. 1997. Bidirec-
tional recurrent neural networks. Signal Processing,
IEEE Transactions on, 45(11):2673–2681.

Nakatani Shuyo. 2010. Language detection library for
java.

Ilya Sutskever, Oriol Vinyals, and Quoc V. Le. 2014.
Sequence to sequence learning with neural net-
works. In NIPS’2014.

Sami Virpioja, Peter Smit, Stig-Arne Grönroos, Mikko
Kurimo, et al. 2013. Morfessor 2.0: Python imple-
mentation and extensions for morfessor baseline.

Matthew D Zeiler. 2012. ADADELTA: An adap-
tive learning rate method. arXiv:1212.5701
[cs.LG].

140


