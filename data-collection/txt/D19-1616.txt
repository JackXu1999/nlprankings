



















































Abstract Text Summarization: A Low Resource Challenge


Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing
and the 9th International Joint Conference on Natural Language Processing, pages 5994–5998,
Hong Kong, China, November 3–7, 2019. c©2019 Association for Computational Linguistics

5994

Abstract Text Summarization: A Low Resource Challenge

Shantipriya Parida Petr Motlicek
Idiap Research Institute

Rue Marconi 19, 1920 Martigny, Switzerland
{shantipriya.parida,petr.motlicek}@idiap.ch

Abstract

Text summarization is considered as a chal-
lenging task in the NLP community. The avail-
ability of datasets for the task of multilingual
text summarization is rare, and such datasets
are difficult to construct. In this work, we
build an abstract text summarizer for the Ger-
man language text using the state-of-the-art
“Transformer” model. We propose an itera-
tive data augmentation approach which uses
synthetic data along with the real summariza-
tion data for the German language. To gener-
ate synthetic data, the Common Crawl (Ger-
man) dataset is exploited, which covers dif-
ferent domains. The synthetic data is effec-
tive for the low resource condition and is par-
ticularly helpful for our multilingual scenario
where availability of summarizing data is still
a challenging issue. The data are also useful in
deep learning scenarios where the neural mod-
els require a large amount of training data for
utilization of its capacity. The obtained sum-
marization performance is measured in terms
of ROUGE and BLEU score. We achieve an
absolute improvement of +1.5 and +16.0 in
ROUGE1 F1 (R1 F1) on the development and
test sets, respectively, compared to the system
which does not rely on data augmentation.

1 Introduction

Automatic text summarization is considered as
a challenging task because while summarizing a
piece of text, we read it entirely to develop our
understanding to prepare highlighting its main
points. Due to the lack of human knowledge and
language processing abilities in computers, auto-
matic text summarization is a major non-trivial
task (Allahyari et al., 2017).

Two major approaches for automatic summa-
rization are: extractive and abstractive. The ex-
tractive summarization approach produces sum-
maries by choosing a subset of sentences in the

original text. The abstract text summarization ap-
proach aims to shorten the long text into a human-
readable form that contains the most important
fact from the original text (Allahyari et al., 2017;
Kryściński et al., 2018).

The deep learning-based neural attention model
when applying to abstract text summarization per-
forms well compared to standard learning-based
approaches (Rush et al., 2015). Abstract text sum-
marization using the attentional encoder-decoder
recurrent neural network approach shows a state-
of-the-art performance and sets a baseline model
(Nallapati et al., 2016). Further improvements
are introduced to the baseline model by using the
pointer generator network and coverage mecha-
nism using reinforcement learning based training
procedure (See et al., 2017; Paulus et al., 2017).
There is an inherent limitation to natural lan-
guage processing tasks such as text summariza-
tion for resource-poor and morphological complex
languages owing to a shortage of quality linguis-
tic data available (Kurniawan and Louvan, 2018).
The use of synthetic data along with the real data
is one of the popular approaches followed in ma-
chine translation domain for the low resource con-
ditions to improve the translation quality (Bojar
and Tamchyna, 2011; Hoang et al., 2018; Chinea-
Rıos et al., 2017). The iterative back-translation
(e.g. training back-translation systems multiple
times) were also found effective in machine trans-
lation (Hoang et al., 2018). We explore similar
approaches in our experiments for the text sum-
marization task.

The organizations of this paper is as follows:
Section 1 describes related work on abstract text
summarization. Section 2 explains the techniques
followed in our work. Section 3 describes the
dataset used in our experiment. Section 4 explains
the experimental settings: models and their param-
eters. Section 5 provides evaluation results with



5995

analysis and discussion. Section 6 provides con-
clusion to the paper.

2 Method Description

Across all experiments performed in this paper, we
have used the Transformer model as implemented
in OpenNMT-py1 (Vaswani et al., 2018; See et al.,
2017). The Transformer model is based on en-
coder/decoder architecture. In context to summa-
rize, it takes text as input and provides its sum-
mary.

We use synthetic data as shown in Figure 1 to
increase the size of the training data.

Text Summary

SummaryText

Real

Synthetic

Real + Synthetic

Reverse System
Final System

SummaryText

Figure 1: Generation of synthetic data using a reverse
system. To generate synthetic data, first, a system in
the reverse direction (i.e. source as summary and target
as text) is trained and then used to generate text for the
given summary. Then both the real and synthetic data
acts as input to the final system.

3 Dataset Description

We use German wiki data (spread across different
domain) collected from the SwissText 20192 (real
data) and Common Crawl3 data (synthetic data) in
our experiment. The statistics of all the datasets
are shown in Table 1.

3.1 SwissText datasets used as real data

We divide the 100K SwissText dataset (down-
loaded from SwissText 2019 website) into three
subsets: train, dev, and test in 90:5:5 ratio (i.e.
90K for training, 5K for development and 5K for

1http://opennmt.net/OpenNMT-py/
Summarization.html

2https://www.swisstext.org/
3http://commoncrawl.org/

Dataset #Text #Summ
Train Real(SwissText) 90K 90K
Train RealSynth(Swiss+CC) 190K 190K
Train RealSynthRegen(Swiss+CC) 190K 190K
Dev(SwissText) 5K 5K
Test(SwissText) 5K 5K

Table 1: Statistics of the experimental data which in-
clude the number of texts and their summaries.

the test data). The experiments performed over
these datasets are described in Section 4.3 (de-
noted as S1 experimental setup).

3.2 Common Crawl dataset used as synthetic
data

The data crawled from the Internet (Common
Crawl) used to prepare synthetic data to boost the
training. The steps followed to create the synthetic
dataset as follows:

Step 1: Build vocab: We create vocabulary using
SwissText based on the occurrence of the
most frequent (top N) German words.

Step 2: Sentence selection: The sentences from
the Common Crawl data are selected with
respect to the vocabulary based on the
threshold we provide (e.g. a sentence has
10 words and the threshold is 10% (0.1)).
For a sentence to be selected, at least 1 out
of 10 words should be in the vocabulary.

Step 3: Filtering: Select random sentences (e.g.
100K) from the selected Common Crawl
data in the previous step.

Step 4: Generate summary: The 100K data ob-
tained from the previous step are used as a
summary and required to generate corre-
sponding text. We use the reverse trained
model where we provide the summary as
source and target as text. This results
in the text as well as the corresponding
summary as additional data to be utilized
along with real data (SwissText).

Eventually, the 190K dataset is created (de-
note as Train RealSynth) as a combination of 90K
SwissText train data (real) and 100K synthetic
data. This dataset is used in the experimental setup
S2 (described in details in Section 4.3).

4 Experimental Setup

This section describes our experiments conducted
for the text summarization task.

http://opennmt.net/OpenNMT-py/Summarization.html
http://opennmt.net/OpenNMT-py/Summarization.html
https://www.swisstext.org/
http://commoncrawl.org/


5996

Setting Dataset R1 F1 R2 F1 RL F1 BLEU
S1 Dev 43.9 28.5 46.3 12.6

Test 39.7 22.9 42.2 9.0
S2 Dev 45.4 29.8 47.4 14.0

Test 55.7 41.8 57.6 20.8
S3 Dev 44.3 28.5 46.4 13.1

Test 40.0 23.0 42.3 9.4

Table 2: Evaluation results of our models on development (dev) and testing (test) sets. The automatic evaluation
scores in terms of Rouge (R1 F1, R2 F1, RL F1) and BLEU for the output summaries are shown in the table.

4.1 Prepossessing

The preprocess step involves preprocessing the
dataset such that source and target are aligned and
use the same dictionary. Additionally, we trun-
cate the source length at 400 tokens and the tar-
get length at 100 tokens to expedite training (See
et al., 2017).

4.2 Model Parameters

The Transformer model is implemented in
OpenNMT-py. To train the model, we use a sin-
gle GPU. To fit the model to the GPU cluster, a
batch size equal to 4,096 is selected for training.
The validation batch size is set to 8. We use an
initial learning rate of 2, drop out of 0.2 and 8,000
warm-up steps. Decoding uses a beam size of 10
and we did not set any minimum length of output
summary.

4.3 Model Setup

We use 3 settings: (i) real data (we set this as the
baseline in our experiment), (ii) real data and syn-
thetic data, and (iii) real and regenerated synthetic
data for the summarization task, described as fol-
lows:

1. S1: Transformer model using Train Real
data

In this setup, we use the “Train Real” data for
training the Transformer model.

2. S2: Transformer Model using
Train RealSynth data

In this setup, we use the “Train RealSynth”
data for training the Transformer model. As
the balance between real and synthetic data is
an important factor, we maintain a 1:1 ratio
(e.g. 1 (real) :1 (synthetic)) for our experi-
ment (Sennrich et al., 2016).

3. S3: Transformer Model using
Train RealSynthRegen data

We propose an iterative approach to im-
prove the quality of synthetic summaries.
In this setup, after training a system with
(real+synthetic) data, it is used to regener-
ate synthetic data for the final system. As a
result, the input data to the final system is a
combination of real and regenerated synthetic
data as shown in Figure 2.

4.4 Training Procedure
The copying mechanism is applied during train-
ing. It allows the summarizer to fall back and copy
the source text when encounters < unk > tokens
by referencing to the softmax of the multiplication
between attention scores of the output with the at-
tention scores of the source (See et al., 2017). The
systems are trained for 300K iterations.

5 Evaluation and Discussion

We evaluate the results for every 10,000 iterations
on the dev and test set. The automatic evaluation
results based on the dev and test set are shown in
Table 2 with sample summaries in Table 3. To
evaluate the proposed algorithms, we use ROUGE
(Recall-Oriented Understudy for Gisting Evalu-
ation) score, which is a popular metric for text
summarization task, and has several variants like
ROUGE-N, and ROUGE-L, which measure the
overlap of n-grams between the system and refer-
ence summary (LIN, 2004). We use ROUGE 1 F1
(R1 F1), ROUGE 2 F1 (R2 F1), and ROUGE L
F1 (RL F1) for scoring the generated summary. In
addition, we also use the SacreBLEU4 evaluation
metric (Post, 2018).

Figure 3 presents the learning curves for the
models (S1 and S2) on the development set. It
can be seen that there is a variance (e.g. word

4https://github.com/mjpost/sacreBLEU

https://github.com/mjpost/sacreBLEU


5997

Summary

SummaryText

Real

Synthetic

Real + Synthetic

Reverse System1

SummaryText

SummaryText

Synthetic

SummaryText

Final System

Real + Synthetic

Text

Reverse System2

Figure 2: Regeneration of synthetic data. After training a system with real+synthetic data (Reverse System2
above), used to create synthetic summarization data for the final system.

 42.5

 43

 43.5

 44

 44.5

 45

 45.5

0 50k 100k 150k 200k 250k 300k

R
o
u
g
e1

 F
1

Iteration

S1:Real
S2:Real+Synth

Figure 3: Learning curves in terms of Rouge1 F1
(R1 F1) Score on dev set.

selection, summary length) for model S2 gener-
ated summary as compared with model S1. Dur-
ing manual verification, we found that the sum-
maries generated without a minimum length con-
straint appear better compared to summaries with
minimum length constraint. Although we do not
explicitly specify a minimum length parameter for
generating summaries for the models, the average
length of words generated by model S2 (e.g. 41.42
words) is longer than the model S1 (e.g. 39.81
words). Some data (e.g. name, year) were found
inconsistent during a comparison of the generated
summary with respect to the reference. There is
a variance in summaries generated by model S3
as compared to S2 and S1. In terms of Rouge
score model S3 outperforms model S1 but perform
worse than model S2 (see Table 2).

6 Conclusion

In this paper, we highlighted the implementation
of synthetic data for the abstract text summariza-

Ref Summary : “Das Feuerschiff Relandersgrund
war ein finnisches Feuerschiff, das von 1888 bis 1914
im Schrenmeer bei Rauma positioniert war. Heute
dient es als Restaurantschiff in Helsinki.”
Gloss: The lightship Relandersgrund was a Finnish
lightship, which was built from 1888 to 1914
Schrenmeer was positioned at Rauma.
Today serves it as a restaurant ship in Helsin
S1 Summary: :“Die “Rauma”. ist ein 1886—1888
Feuerschiff der norwegischen Reederei “Libauskij”,
Das Schiff wurde in den 1930er Jahren gebaut und in
den 2000er Jahren als Museumsschiff als”
Gloss:“The “Rauma ”. is a 1886-1888 Lightship of the
Norwegian shipping company “Libauskij”,The ship was
built in the 1930s and in the 2000s as a museum ship as
S2 Summary: :“Das Feuerschiff Relandersgrund war ein
Feuerschiff des das von 1888 bis 1914 im Einsatz war.
Heute dient es als Restaurantschiff in Kotka,”
Gloss: The lightship Relandersgrund was on Lightship
of the 1888 to 1914 was in use. Today it serves as a
restaurant ship in Kotka
S3 Summary: :“Das Kotka.” ist ein finnischer
Museumsschiff der im Zweiten Weltkrieg von der
russischen Marine als Restaurantschiff 1” eingesetzt
wurde. Im Mittelalter war das Schiff unter dem Namen
“Vuolle” 1” fr die finnische Marine 1”
Gloss: The Kotka. “Is a Finnish one Museum ship of the
World War II Russian Navy used as a restaurant
ship 1 ” has been. In the Middle Ages, the ship was
under the name “Vuolle” 1 “for the Finnish Navy 1

Table 3: Sample summaries on test set. The matching
words of generating summaries with respect to refer-
ences are shown in color blue.

tion task under low resource condition, which
helps improving the text summarization system
in terms of automatic evaluation metrics. As the
next step, we plan to investigate: i) synthetic sum-
marization data, and ii) applying transfer learning
on text summarization for the multilingual low re-
source data set with little or no ground truth sum-
maries (Keneshloo et al., 2018).



5998

Acknowledgments

The work is supported by an innovation project
(under an InnoSuisse grant) oriented to improve
the automatic speech recognition and natural lan-
guage understanding technologies for German. Ti-
tle: “SM2: Extracting Semantic Meaning from
Spoken Material” funding application no. 29814.1
IP-ICT.

References
Mehdi Allahyari, Seyedamin Pouriyeh, Mehdi Assefi,

Saeid Safaei, Elizabeth D Trippe, Juan B Gutier-
rez, and Krys Kochut. 2017. Text summariza-
tion techniques: a brief survey. arXiv preprint
arXiv:1707.02268.

Ondrej Bojar and Aleš Tamchyna. 2011. Improving
translation model by monolingual data. In Sixth
Workshop on Statistical Machine Translation, page
330.

Mara Chinea-Rıos, Alvaro Peris, and Francisco
Casacuberta. 2017. Adapting neural machine trans-
lation with parallel synthetic data. WMT 2017, page
138.

Cong Duy Vu Hoang, Philipp Koehn, Gholamreza
Haffari, and Trevor Cohn. 2018. Iterative back-
translation for neural machine translation. ACL
2018, 23(32.5):18.

Yaser Keneshloo, Naren Ramakrishnan, and Chan-
dan K Reddy. 2018. Deep transfer reinforcement
learning for text summarization. arXiv preprint
arXiv:1810.06667.

Wojciech Kryściński, Romain Paulus, Caiming Xiong,
and Richard Socher. 2018. Improving abstraction
in text summarization. In Proceedings of the 2018
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 1808–1817.

Kemal Kurniawan and Samuel Louvan. 2018. Indo-
sum: A new benchmark dataset for indonesian text
summarization. In 2018 International Conference
on Asian Language Processing (IALP), pages 215–
220. IEEE.

C-Y LIN. 2004. Rouge: A package for automatic
evaluation of summaries. In Proc. of Workshop on
Text Summarization Branches Out, Post Conference
Workshop of ACL 2004.

Ramesh Nallapati, Bowen Zhou, Cicero dos Santos,
Caglar Gulcehre, and Bing Xiang. 2016. Ab-
stractive text summarization using sequence-to-
sequence rnns and beyond. In Proceedings of The
20th SIGNLL Conference on Computational Natu-
ral Language Learning, pages 280–290.

Romain Paulus, Caiming Xiong, and Richard Socher.
2017. A deep reinforced model for abstractive sum-
marization. arXiv preprint arXiv:1705.04304.

Matt Post. 2018. A call for clarity in reporting BLEU
scores. In Proceedings of the Third Conference on
Machine Translation: Research Papers, pages 186–
191. Association for Computational Linguistics.

Alexander M Rush, Sumit Chopra, and Jason Weston.
2015. A neural attention model for abstractive sen-
tence summarization. In Proceedings of the 2015
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 379–389.

Abigail See, Peter J Liu, and Christopher D Manning.
2017. Get to the point: Summarization with pointer-
generator networks. In Proceedings of the 55th An-
nual Meeting of the Association for Computational
Linguistics (Volume 1: Long Papers), pages 1073–
1083.

Rico Sennrich, Barry Haddow, and Alexandra Birch.
2016. Improving neural machine translation mod-
els with monolingual data. In Proceedings of the
54th Annual Meeting of the Association for Compu-
tational Linguistics (Volume 1: Long Papers), vol-
ume 1, pages 86–96.

Ashish Vaswani, Samy Bengio, Eugene Brevdo, Fran-
cois Chollet, Aidan Gomez, Stephan Gouws, Llion
Jones, Łukasz Kaiser, Nal Kalchbrenner, Niki Par-
mar, Ryan Sepassi, Noam Shazeer, and Jakob
Uszkoreit. 2018. Tensor2tensor for neural machine
translation. In Proceedings of the 13th Confer-
ence of the Association for Machine Translation in
the Americas (Volume 1: Research Papers), pages
193–199. Association for Machine Translation in the
Americas.

http://aclweb.org/anthology/W18-6319
http://aclweb.org/anthology/W18-6319
http://aclweb.org/anthology/W18-1819
http://aclweb.org/anthology/W18-1819

