










































Probabilistic Databases of Universal Schema


Proc. of the Joint Workshop on Automatic Knowledge Base Construction & Web-scale Knowledge Extraction (AKBC-WEKEX), pages 116–121,
NAACL-HLT, Montréal, Canada, June 7-8, 2012. c©2012 Association for Computational Linguistics

Probabilistic Databases of Universal Schema

Limin Yao Sebastian Riedel Andrew McCallum
Department of Computer Science

University of Massachusetts, Amherst
{lmyao,riedel,mccallum}@cs.umass.edu

Abstract

In data integration we transform information
from a source into a target schema. A gen-
eral problem in this task is loss of fidelity and
coverage: the source expresses more knowl-
edge than can fit into the target schema, or
knowledge that is hard to fit into any schema
at all. This problem is taken to an extreme in
information extraction (IE) where the source
is natural language. To address this issue, one
can either automatically learn a latent schema
emergent in text (a brittle and ill-defined task),
or manually extend schemas. We propose in-
stead to store data in a probabilistic database
of universal schema. This schema is simply
the union of all source schemas, and the proba-
bilistic database learns how to predict the cells
of each source relation in this union. For ex-
ample, the database could store Freebase re-
lations and relations that correspond to natu-
ral language surface patterns. The database
would learn to predict what freebase relations
hold true based on what surface patterns ap-
pear, and vice versa. We describe an anal-
ogy between such databases and collaborative
filtering models, and use it to implement our
paradigm with probabilistic PCA, a scalable
and effective collaborative filtering method.

1 Introduction

Natural language is a highly expressive representa-
tion of knowledge. Yet, for many tasks databases
are more suitable, as they support more effective de-
cision support, question answering and data min-
ing. But given a fixed schema, any database can
only capture so much of the information natural lan-
guage can express, even if we restrict us to factual

knowledge. For example, Freebase (Bollacker et
al., 2008) captures the content of Wikipedia to some
extent, but has no criticized(Person,Person) relation
and hence cannot answer a question like “Who crit-
icized George Bush?”, even though partial answers
are expressed in Wikipedia. This makes the database
schema a major bottleneck in information extrac-
tion (IE). From a more general point of view, data in-
tegration always suffers from schema mismatch be-
tween knowledge source and knowledge target.

To overcome this problem, one could attempt to
manually extend the schema whenever needed, but
this is a time-consuming and expensive process. Al-
ternatively, in the case of IE, we can automatically
induce latent schemas from text, but this is a brit-
tle, ill-defined and error-prone task. This paper pro-
poses a third alternative: sidestep the issue of in-
complete schemas altogether, by simply combining
the relations of all knowledge sources into what we
will refer to as a universal schema. In the case of IE
this means maintaining a database with one table per
natural language surface pattern. For data integra-
tion from structured sources it simply means storing
the original tables as is. Crucially, the database will
not only store what each source table does contain,
it will also learn a probabilistic model about which
other rows each source table should correctly con-
tain.

Let us illustrate this approach in the context of
IE. First we copy tables such as profession from a
structured source (say, DBPedia). Next we create
one table per surface pattern, such as was-criticized-
by and was-attacked-by and fill these tables with the
entity pairs that appear with this pattern in some nat-
ural language corpus (say, the NYT Corpus). At
this point, our database is a simple combination of

116



a structured and an OpenIE (Etzioni et al., 2008)
knowledge representation. However, while we insert
this knowledge, we can learn a probabilistic model
which is able to predict was-criticized-by pairs based
on information from the was-attacked-by relation.
In addition, it learns that the profession relation in
Freebase can help disambiguate between physical
attacks in sports and verbal attacks in politics. At
the same time, the model learns that the natural lan-
guage relation was-criticized-by can help predict the
profession information in Freebase. Moreover, often
users of the database will not need to study a par-
ticular schema—they can use their own expressions
(say, works-at instead of profession) and still find the
right answers.

In the previous scenario we could answer more
questions than our structured sources alone, because
we learn how to predict new Freebase rows. We
could answer more questions than the text corpus
and OpenIE alone, because we learn how to pre-
dict new rows in surface pattern tables. We could
also answer more questions than in Distant Supervi-
sion (Mintz et al., 2009), because our schema is not
limited to the relations in the structured source. We
could even go further and import additional struc-
tured sources, such as Yago (Hoffart et al., 2012). In
this case the probabilistic database would have inte-
grated, and implicitly aligned, several different data
sources, in the sense that each helps predict the rows
of the other.

In this paper we present results of our first techni-
cal approach to probabilistic databases with univer-
sal schema: collaborative filtering, which has been
successful in modeling movie recommendations.
Here each entity tuple explicitly “rates” source ta-
bles as “I appear in it” or “I don’t”, and the recom-
mender system model predicts how the tuple would
“rate” other tables—this amounts to the probability
of membership in the corresponding table. Collab-
orative filtering provides us with a wide range of
scalable and effective machine learning techniques.
In particular, we are free to choose models that use
no latent representations at all (such as a graphical
model with one random variable per database cell),
or models with latent representations that do not
directly correspond to interpretable semantic con-
cepts. In this paper we explore the latter and use a
probabilistic generalization to PCA for recommen-

: A, a division of B : A's parent, B

1 ?

1 0.809

gPCA

r1 r2

 :  Pocket Books, 
Simon & Schuster
e

 :  Pocket Books, 
Simon & Schuster
e

xe,r1

xe,r1
xe,r2

xe,r2

vr1
vr2

ae

ae

vr1
vr2

σ(aT
e
vr2

)

Figure 1: gPCA re-estimates the representations of two
relations and a tuple with the arrival of an observation
r1 (e). This enables the estimation of the probability for
unseen fact r2 (e). Notice that both tuple and relation
components are re-estimated and can change with the ar-
rival of new observations.

dation.
In our experiments we integrate Freebase data

and information from the New York Times Cor-
pus (Sandhaus, 2008). We show that our prob-
abilistic database can answer questions neither of
the sources can answer, and that it uses information
from one source to improve predictions for the other.

2 Generalized PCA

In this work we concentrate on a set of binary source
relations R, consisting of surface patterns as well
as imported tables from other databases, and a set
of entity pairs E . We introduce a matrix X where
each cell xe,r is a binary random variable indicating
whether r (e) is true or not. The upper half of figure
1 shows two cells of this matrix, based on the rela-
tions r1 (“a-division-of ”) and r2 (“’s-parent,”) and
the tuple e (Pocket Books, Simon&Schuster). Gen-
erally some of the cells will be observed (such as
xe,r1) while others will be not (such as xe,r2).

We employ a probabilistic generalization of Prin-
ciple Component Analysis (gPCA) to estimate the
probabilities P (r (e)) for every non-observed fact
r (e) (Collins et al., 2001). In gPCA we learn a
k-dimensional feature vector representation vr for
each relation (column) r, and a k-dimensional fea-
ture vector representation ae for each entity pair e.

117



Figure 1 shows example vectors for both rows and
columns. Notice that these vectors do not have to be
positive nor sum up to one. Given these represen-
tations, the probability of r (e) being true is given
by the logistic function σ (θ) = 11+exp(−θ) applied

to the dot product θr,e , a
ᵀ
evr. In other words, we

represent the matrix of parameters Θ , (θr,e) using
a low-rank approximation AV where A = (ae)e∈E
and V = (vr)r∈R.

Given a set of observed cells, gPCA estimates
the tuple feature representations A and the relation
feature representations V by maximizing the log-
likelihood of the observed data. This can be done
both in batch mode or in a more incremental fashion.
In the latter we observe new facts (such as r1 (e) in
Figure 1) and then re-estimate A and V. In Figure
1 we show what this means in practice. In the upper
half we see the currently estimated representation
vr1 and vr2 of r1 and r2, and a random initializa-
tion for the representation ae of e. In the lower half
we take the observation r1 (e) into account and re-
estimate ae and vr1 . The new estimates can then be
used to calculate the probability σ (aᵀevr2) of r2 (e).

Notice that by incorporating new evidence for a
given row, both entity and relation representations
can improve, and hence beliefs across the whole ma-
trix. In this sense, gPCA performs a form of joint or
global inference. Likewise, when we observe sev-
eral active relations for a new tuple, the model will
increase the probabilistic association between theses
relations and, transitively, also previously associated
relations. This gives gPCA a never-ending-learning
quality. Also note that it is easy to incorporate entity
representations into the approach, and model selec-
tional preferences. Likewise, we can easily add pos-
terior constraints we know to hold across relations,
and learn from unlabeled data.

3 Related Work

We briefly review related work in this section. Open
IE (Etzioni et al., 2008) extracts how entities and
their relations are actually mentioned in text, but
does not predict how entities could be mentioned
otherwise and hence suffer from reduced recall.
There are approaches that learn synonym relations
between surface patterns (Yates and Etzioni, 2009;
Pantel et al., 2007; Lin and Pantel, 2001; Yao et

al., 2011) to overcome this problem. Fundamen-
tally, these methods rely on a symmetric notion of
synonymy in which certain patterns are assumed to
have the same meaning. Our approach rejects this
assumption in favor of a model which learns that cer-
tain patterns, or combinations thereof, entail others
in one direction, but not necessarily the other.

Methods that learn rules between textual pat-
terns in OpenIE aim at a similar goal as our pro-
posed gPCA algorithm (Schoenmackers et al., 2008;
Schoenmackers et al., 2010). Such methods learn
the structure of a Markov Network, and are ulti-
mately bounded by limits on tree-width and den-
sity. In contrast, the gPCA learns a latent, although
not necessarily interpretable, structure. This la-
tent structure can express models of very high tree-
width, and hence very complex rules, without loss
in efficiency. Moreover, most rule learners work in
batch mode while our method continues to learn new
associations with the arrival of new data.

4 Experiments

Our work aims to predict new rows of source tables,
where tables correspond to either surface patterns
in natural language sources, or tables in structured
sources. In this paper we concentrate on binary rela-
tions, but note that in future work we will use unary,
and generally n-ary, tables as well.

4.1 Unstructured Data

The first set of relations to integrate into our univer-
sal schema comes from the surface patterns of 20
years of New York Times articles (Sandhaus, 2008).
We preprocess the data similarly to Riedel et al.
(2010). This yields a collection of entity mention
pairs that appear in the same sentence, together with
the syntactic path between the two mentions.

For each entity pair in a sentence we extract the
following surface patterns: the dependency path
which connects the two named entities, the words
between the two named entities, and the context
words of the two named entities. Then we add the
entity pair to the set of relations to which the surface
patterns correspond. This results in approximately
350,000 entity pairs in 23,000 relations.

118



Table 1: GPCA fills in new predicates for records
Relation <-subj<-own->obj->perc.>prep->of->obj-> <-subj<-criticize->obj->

Obs. Time Inc., American Tel. and Comms. Bill Clinton, Bush Administration

New
United States, Manhattan Mr. Forbes, Mr. Bush

Campeau, Federated Department Stores Mr. Dinkins, Mr. Giuliani
Volvo, Scania A.B. Mr. Badillo, Mr. Bloomberg

4.2 Structured Data

The second set of source relations stems from Free-
base. We choose those relations that hold for entity
pairs appearing in the NYT corpus. This adds 116
relations to our universal schema. For each of the re-
lations we import only those rows which correspond
to entity tuples also found in the NYT corpus. In or-
der to link entity mentions in the text to entities in
Freebase, we follow a simple string-match heuristic.

4.3 Experimental Setup and Training

In our experiments, we hold out some of the ob-
served source rows and try to predict these based
on other observed rows. In particular, for each en-
tity pair, we traverse over all source relations. For
each relation we throw an unbiased coin to deter-
mine whether it is observed for the given pair. Then
we train a gPCA model of 50 components on the
observed rows, and use it to predict the unobserved
ones. Here a pair e is set to be in a given relation
r if P (r (e)) > 0.5 according to our model. Since
we generally do not have observed negative informa-
tion,1 we sub-sample a set of negative rows for each
relation r to create a more balanced training set.

We evaluate recall of our method by measuring
how many of the true held out rows we predict. We
could use a similar approach to measure precision
by considering each positive prediction to be a false
positive if the observed held-out data does not con-
tain the corresponding fact. However, this approach
underestimates precision since our sources are gen-
erally incomplete. To overcome this issue, we use
human annotations for the precision measure. In
particular, we randomly sample a subset of entity
pairs and ask human annotators to assess the pre-
dicted positive relations of each.

1Just because a particular e has not yet been seen in particu-
lar relation r we cannot infer that r (e) is false.

4.4 Integrating the NYT Corpus

We investigate how gPCA can help us answer ques-
tions based on only single data source: the NYT
Corpus. Table 1 presents, for two source relations
(aka surface patterns), a set of observed entity pairs
(Obs.) and the most likely inferred entity pairs
(New). The table shows that we can answer a ques-
tion like “Who owns percentages of Scania AB?”
even though the corpus does not explicitly contain
the answer. In our case, it only contains “buy-stake-
in(VOLVO,SCANIA AB).”

gPCA achieves about 49% recall, at about 67%
precision. Interestingly, the model learns more
than just paraphrasing. Instead, it captures some
notion of entailment. This can be observed in
its asymmetric beliefs. For example, the model
learned to predict “professor-at(K.BOYLE, OHIO
STATE)” based on “historian-at(KEVIN BOYLE,
OHIO STATE)” but would not make the infer-
ence “historian-at(R.FREEMAN,HARVARD)” based
on “professor-at(R.FREEMAN,HARVARD).”

4.5 Integrating Freebase

What happens if we integrate additional structured
sources into our probabilistic database? We observe
that by incorporating Freebase tables in addition to
the NYT data we can improve recall from 49% to
52% on surface patterns. The precision also in-
creases by 2%.

Table 2 sums the results and also gives an ex-
ample of how Freebase helps improve both preci-
sion and recall. Without Freebase, the gPCA pre-
dicts that Maher Arar was arrested in Syria—primarily
because he lived in Syria and the NYT often talks
about arrests of people in the city they live in2. After
learning placeOfBirth(ARAR,SYRIA) from Freebase, the
gPCA model infers wasBornIn(ARAR,SYRIA) as well as
grewUpIn(ARAR,SYRIA).

2In fact, he was arrested in US

119



Table 2: Relation predictions w/o Freebase.
without Freebase with Freebase

Prec. 0.687 0.666
Rec. 0.491 0.520
E.g. M. Arar, Syria (Freebase: placeOfBirth)

Pred.
A was arrested in B A was born in B

A appeal to B A grow up in B
A, who represent B A’s home in B

5 Conclusion

In our approach we do not design or infer new re-
lations to accommodate information from different
sources. Instead we simply combine source rela-
tions into a universal schema, and learn a proba-
bilistic model to predict what other rows the sources
could contain. This simple paradigm allows us to
perform data alignment, information extraction, and
other forms of data integration, while minimizing
both loss of information and the need for schema
maintenance.

At the heart of our approach is the hypothesis that
we should concentrate on building models to predict
source data—a relatively well defined task—as op-
posed to models of semantic equivalence that match
our intuition. Our future work will therefore investi-
gate such predictive models in more detail, and ask
how to (a) incorporate relations of different arities,
(b) employ background knowledge, (c) optimize the
choice of negative data and (d) scale up both in terms
of rows and tables.

Acknowledgments

This work was supported in part by the Center for
Intelligent Information Retrieval and the Univer-
sity of Massachusetts and in part by UPenn NSF
medium IIS-0803847. We gratefully acknowledge
the support of Defense Advanced Research Projects
Agency (DARPA) Machine Reading Program under
Air Force Research Laboratory (AFRL) prime con-
tract no. FA8750-09-C-0181. Any opinions, find-
ings, and conclusion or recommendations expressed
in this material are those of the authors and do not
necessarily reflect the view of DARPA, AFRL, or
the US government.

References

Kurt Bollacker, Colin Evans, Praveen Paritosh, Tim
Sturge, and Jamie Taylor. 2008. Freebase: a collabo-
ratively created graph database for structuring human
knowledge. In SIGMOD ’08: Proceedings of the 2008
ACM SIGMOD international conference on Manage-
ment of data, pages 1247–1250, New York, NY, USA.
ACM.

Michael Collins, Sanjoy Dasgupta, and Robert E.
Schapire. 2001. A generalization of principal com-
ponent analysis to the exponential family. In Proceed-
ings of NIPS.

Oren Etzioni, Michele Banko, Stephen Soderland, and
Daniel S. Weld. 2008. Open information extraction
from the web. Commun. ACM, 51(12):68–74.

Johannes Hoffart, Fabian M. Suchanek, Klaus Berberich,
and Gerhard Weikum. 2012. Yago2: A spatially and
temporally enhanced knowledge base from wikipedia.
Artificial Intelligence.

Dekang Lin and Patrick Pantel. 2001. DIRT - discovery
of inference rules from text. In Knowledge Discovery
and Data Mining, pages 323–328.

Mike Mintz, Steven Bills, Rion Snow, and Daniel Juraf-
sky. 2009. Distant supervision for relation extrac-
tion without labeled data. In Proceedings of the Joint
Conference of the 47th Annual Meeting of the ACL
and the 4th International Joint Conference on Natu-
ral Language Processing of the AFNLP (ACL ’09),
pages 1003–1011. Association for Computational Lin-
guistics.

Patrick Pantel, Rahul Bhagat, Bonaventura Coppola,
Timothy Chklovski, and Eduard Hovy. 2007. ISP:
Learning Inferential Selectional Preferences. In Pro-
ceedings of NAACL HLT.

Sebastian Riedel, Limin Yao, and Andrew McCallum.
2010. Modeling relations and their mentions without
labeled text. In Proceedings of the European Confer-
ence on Machine Learning and Knowledge Discovery
in Databases (ECML PKDD ’10).

Evan Sandhaus, 2008. The New York Times Annotated
Corpus. Linguistic Data Consortium, Philadelphia.

Stefan Schoenmackers, Oren Etzioni, and Daniel S.
Weld. 2008. Scaling textual inference to the web.
In EMNLP ’08: Proceedings of the Conference on
Empirical Methods in Natural Language Processing,
pages 79–88, Morristown, NJ, USA. Association for
Computational Linguistics.

Stefan Schoenmackers, Oren Etzioni, Daniel S. Weld,
and Jesse Davis. 2010. Learning first-order horn
clauses from web text. In Proceedings of the 2010
Conference on Empirical Methods in Natural Lan-
guage Processing, EMNLP ’10, pages 1088–1098,

120



Stroudsburg, PA, USA. Association for Computational
Linguistics.

Limin Yao, Aria Haghighi, Sebastian Riedel, and Andrew
McCallum. 2011. Structured relation discovery using
generative models. In Proceedings of the Conference
on Empirical methods in natural language processing
(EMNLP ’11), July.

Alexander Yates and Oren Etzioni. 2009. Unsupervised
methods for determining object and relation synonyms
on the web. Journal of Artificial Intelligence Research,
34:255–296.

121


