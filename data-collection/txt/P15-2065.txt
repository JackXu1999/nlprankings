



















































Robust Multi-Relational Clustering via 1-Norm Symmetric Nonnegative Matrix Factorization


Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics
and the 7th International Joint Conference on Natural Language Processing (Short Papers), pages 397–401,

Beijing, China, July 26-31, 2015. c©2015 Association for Computational Linguistics

Robust Multi-Relational Clustering via `1-Norm Symmetric Nonnegative
Matrix Factorization

Kai Liu
Colorado school of Mines

Department of EECS
Golden, Colorado 80401
kaliu@mines.edu

Hua Wang
Colorado school of Mines

Department of EECS
Golden, Colorado 80401

HUAWANGCS@gmail.com

Abstract

In this paper, we propose an `1-norm
Symmetric Nonnegative Matrix Tri-
Factorization (`1 S-NMTF) framework
to cluster multi-type relational data by
utilizing their interrelatedness. Due to
introducing the `1-norm distances in our
new objective function, the proposed ap-
proach is robust against noise and outliers,
which are inherent in multi-relational data.
We also derive the solution algorithm and
rigorously analyze its correctness and
convergence. The promising experimental
results of the algorithm applied to text
clustering on IMDB dataset validate the
proposed approach.

1 Introduction

Traditional clustering aims to partition data points
into several groups, such that the data points in
the same group can share some commonalities
whilst those from different groups are dissimilar.
With the recent progresses of Internet and compu-
tational technologies, data have started to appear
in much richer structures. To be more specific, in
many real-world problems a pair of object can be
related in several different ways, which inevitably
complicates the problem and calls for new clus-
tering algorithms for better understanding to the
data. To address this new challenge, Wang et. al.
(Wang et al., 2011c; Wang et al., 2011d) proposed
nonnegative matrix factorization (NMF) (Lee and
Seung, 1999) based computational algorithms that
have successfully solved the problems.

Due to its mathematical elegance and its equiv-
alence to K-means clustering and spectral clus-
tering (Ding et al., 2005), NMF (Lee and Seung,
1999) has been broadly studied in recent years and
successfully solved a variety of practical problems
in data mining and machine learning, such as those

in computer vision (Wang et al., 2011b), bioinfor-
matics (Wang et al., 2013), natural language un-
derstanding (Wang et al., 2011a), to name a few.
Compared to many traditional clustering meth-
ods, such as K-means clustering, NMF has better
mathematical interpretation, which usually lead
to improved accuracy on clustering (Ding et al.,
2010). Traditional clustering algorithms concen-
trate on dealing with homogeneous data, in which
all the data belong to one single type (Wang et al.,
2011d). To deal with the richer data structures in
modern real-world applications, symmetric Non-
negative Matrix Tri-Factorization (NMTF)(Wang
et al., 2011c) have demonstrated its effectiveness
on simultaneous clustering of multi-type relational
data by utilizing the interrelatedness among differ-
ent data types.

Traditional NMF algorithms routinely use the
least square error functions, which are notably
known to be sensitive against outliers (Kong et al.,
2011). However, at the era of big data outliers are
inevitable due to the ever increasing data sizes. As
a result, developing a more robust NMF model for
multi-relational data clustering has become more
and more important. In this paper, we further de-
velop the symmetric NMF clustering model pro-
posed in (Wang et al., 2011c) by using the `1-norm
distances, such that our new clustering model is
more robust against outliers, which is of particular
importance in multi-relational data.

2 Robust Multi-Relational Clustering via
`1-Norm Symmetric NMTF (S-NMTF)

In this section, we first introduce the backgrounds
to use symmetric NMF to cluster multi-relational
data. Then we develop our new `1-norm symmet-
ric NMF model for better robustness against outly-
ing data. The solution algorithm to our new model
will be proposed and analyzed in the next section.

Notations. In this paper, we use upper case let-
ters to denote matrices. Given a matrix M , its en-

397



try at the i-th row and j-th column is denoted as
M(ij). The Frobenius norm of a matrix M is de-

noted as ‖M‖F =
(∑

i

∑
j M

2
(ij)

)1/2
and its `1-

norm is denoted as ‖M‖1 =
∑

i

∑
j |M(ij)|.

2.1 Problem Formalization

K-type relational data set can be denoted
as χ = {χ1, χ2, . . . , χK} , where χk ={
xk1, x

k
2, . . . , x

k
nk

}
represents the data set of k-th

type. Suppose we are given a set of relationship
matrices {Rkl ∈ <nk×nl}(1≤k≤K,1≤l≤K) between
different types of data objects, then we haveRkl =
RTlk. Our goal is to simultaneously partition the
data objects in χ1, χ2, . . . , χK into c1, c2, . . . , cK
disjoint clusters respectively.

2.2 Our objective

To cluster multi-relation data, symmetric NMF has
been taken advantage that solves the following op-
timization problem (Wang et al., 2008):

min J =
∑

1≤k<l≤K
‖Rkl −GkSklGTl ‖2F ,

s.t. Gk ≥ 0, ∀ 1 ≤ k ≤ K .
(1)

It has also been shown that solving the above
equation is equivalent to solve (Long et al., 2006):

min J = ‖R−GSGT ‖2F , s.t. G ≥ 0, (2)

in which

R =


0n1×n1 Rn1×n212 · · · Rn1×nK1K
Rn2×n121 0

n2×n2 · · · Rn2×nK2K
...

...
. . .

...

RnK×n1K1 R
nK×n2
K2 · · · 0nK×nK

 ,

G =


Gn1×c11 0

n1×c2 · · · 0n1×cK
0n2×c1 Gn2×c22 · · · 0n2×cK

...
...

. . .
...

0nK×c1 0nK×c2 · · · GnK×cKK

 ,

S =


0c1×c1 Sc1×c212 · · · Sc1×cK1K
Sc2×c121 0

c2×c2 · · · Sc2×cK2K
...

...
. . .

...

ScK×c1K1 S
cK×c2
K2 · · · 0cK×cK

 ,
(3)

where Rji = RTij and Sij = S
T
ji.

Despite its successfulness of the method pro-
posed in (Wang et al., 2011c) in multi-relational
data clustering, the objectives in Equations (1—2)
use the squared `2-norm distances to measure the
matrix approximation errors, which, though, are
prone to outliers. As a result, the clustering re-
sults could be heavily dominated by outlying data
points with large approximation errors (Kong et
al., 2011; Nie et al., 2011; Wang et al., 2014). To
improve the robustness of the clustering model,
following prior works (Kong et al., 2011; Nie et
al., 2011; Wang et al., 2014) we propose to use
the following `1-norm symmetric NMTF model
for multi-relational data clustering:

min J = ‖R−GSGT ‖1 s.t. G ≥ 0, (4)

In this new formulation, the approximation errors
are measured by the `1-norm distances, which are
expected to be more insensitive to outlying data
points. As shown in Figure 1, when there ex-
ist outliers in the input data, traditional squared
Frobenius-norm NMF are inclined to cluster in-
correctly, while the `1-norm NMF are more robust
and can cluster more accurately.

Algorithm 1: Algorithm to solve `1-norm S-
NMTF

Data: Relationship matrices: {Rij}1≤i<j≤K
Result: Factor matrices: {Gk}1≤k≤K
1. Construct R,G, S
2. Initialize G as in (Ding et al., 2006).
repeat

3. Construct diagonal matrix D, where
D(i, i) =

∑ |R−GSGT |i
‖R−GSGT ‖2i

.
4. Compute
S = (GTG)−1GTRG(GTG)−1.
5. Update

G(ij) ← G(ij)
[

(RDGS)(ij)
(GSGT DGS)(ij)

] 1
4 .

until Converges

3 Algorithm to Solve `1-Norm S-NMTF
and its analysis

The computational algorithm for the proposed `1-
norm S-NMTF approach is summarized in Algo-
rithm 1 (Due to space limit, the derivation of the
algorithm is skipped and will be provided in our
journal version of the paper). Upon solution, the

398



0 20 40 60 80 100 120
0

10

20

30

40

50

60

70

80

0 20 40 60 80 100 120
0

10

20

30

40

50

60

70

80

Figure 1: Clustering data in two clusters with some outliers (represented as triangle). Left: Clustering
performance by using traditional squared Frobenius-norm NMF algorithm. Right: Clustering perfor-
mance by using the proposed `1-norm NMF algorithm.

final cluster labels are obtained from the resulted
Gk.

The following theorems guarantee the correct-
ness of Algorithm 1 (Due to space limit, the
derivation of the algorithm is skipped and will be
provided in our journal version of the paper).

Theorem 3.1 If the updating rules of G and S in
Algorithm 1 converges, the final solution satisfies
the KKT optimal condition.

This is the fixed point relationships that the so-
lution must satisfy.

The following lemmas and theorem guarantee
the convergence of Algorithm 1 (Due to space
limit, the derivation of the algorithm is skipped
and will be provided in our journal version of the
paper).

Lemma 3.2 (Lee and Seung, 1999) Z(h, h′) is
an auxiliary function of F (h) if the conditions
Z(h, h′) ≥ F (h) and Z(h, h′) = F (h) are sat-
isfied.

Lemma 3.3 (Lee and Seung, 1999) If Z is an aux-
iliary function for F , then F is non-increasing un-
der the update h(t+1) = arg minh Z(h, h′).

Theorem 3.4 Let

J(G) = tr(−2RDGSGT +GSGTDGSGT ),
(5)

then the following function

Z(G,G′) =

− 2
∑
ijkl

G′(ji)S(jk)G
′
(kl)D(ll)R(li)(1 + log

G(ji)G(kl)

G′(ij)G
′
(kl)

)

+
∑
ij

(G′SG′TDG′S)(ij)
G4(ij)

G′3(ij)
(6)

is an auxiliary function of J(G). Furthermore, it
is a convex function in G and its global minimum
is

G(ik) = G(ik)

[
(RDGS)(ik)

(GSGTDGS)(ik)

] 1
4

(7)

Based on the property of auxiliary function and
convex function, by updating G, we can always
get the optimal solution to the object function, thus
determining the final cluster label.

4 Experiments Result

In this section, We test our proposed algorithm on
IMDB dataset by using its inter-type relationship
information.

4.1 Data set
We use the dataset from ACL-IMDB provided by
(Maas et al., 2011). In this dataset, there is a sub-
training set of 25000 highly polar movie reviews,
in which positive and negative comments come up
with one half(12500) each. The dataset also in-
cludes the following two important files: the con-
tent of each comment and the corresponding URL

399



where each comment comes from. There are also
some other files but not related with the experi-
ment we conduct, thus we skip them.

4.2 Experiments settings
In our experiment, we set the multi-type data as
3 types: author, comment and word. As it is
discussed in the 3rd part, there are three rela-
tionships we need to find, which correspond to
three matrices we need to construct the multi-type
data matrix:comment-author, comment-word and
author-word. By making use of the URL of ev-
ery comment, we can find the author who posts
the corresponding comment, thus we can build the
author-comment matrix.Since each comment with
content is given by the dataset file, we could there-
fore construct the matrix of comment-word, and
the author-word matrix is the product of author-
comment matrix and comment-word matrix.

We could find the first 1500 authors who post
comments most, since the comments from the
same person are more likely to have some corre-
lations, such as similar sentence structures, same
words and etc. We also rule out the stop-words
since they may disturb the clustering and they are
meaningless to the property of comments. To
make our experiments to be more persuasive, we
also add some noise to the three relationship matri-
ces with a ratio of 25 percentage(1/5 in amplitude).
By randomly choosing 500 authors from 1500, we
could generate many sub-datasets to conduct our
experiments.

4.3 Experiments Results
We compare the performance of our proposed `1-
norm S-NMTF algorithm with other methods such
as P-NMF, Frobenius norm S-NMTF, traditional
NMF and K-means clustering. For simplicity, we
only compare the clustering accuracy of comment-
word matrix since its label (positive or negative)
is fixed(the grounding label), thus could be com-
pared with the clustering results by using the clus-
tering algorithms.

Table 1 shows that when the data is pure, in
many cases(more than the listed), `1-norm S-
NMTF approach has better performance than oth-
ers

Table 2 illustrates the situation when some noise
is added to the data, it is easy to find that `1-norm
S-NMTF algorithm is the best in terms of cluster-
ing accuracy. This meets our analysis in our Moti-
vation part.

Alg L11 L22 PMF NMF Kms
set 1 0.578 0.528 0.554 0.510 0.504
set 2 0.583 0.556 0.551 0.521 0.521
set 3 0.584 0.559 0.555 0.501 0.501
set 4 0.551 0.522 0.502 0.527 0.506
set 5 0.566 0.534 0.506 0.529 0.531
set 6 0.558 0.517 0.510 0.535 0.526

Table 1: Clustering Accuracy with Pure Data.

Alg L11 L22 PMF NMF Kms
sub 1 0.586 0.545 0.508 0.530 0.530
sub 2 0.575 0.535 0.540 0.518 0.532
sub 3 0.567 0.528 0.520 0.500 0.500
sub 4 0.574 0.533 0.525 0.500 0.500
sub 5 0.574 0.537 0.530 0.519 0.518
sub 6 0.556 0.525 0.524 0.504 0.505

Table 2: Clustering Accuracy with Noise.

Careful examination in Table 3 reveals the fact
that `1-norm S-NMTF algorithm performs more
robust than any other algorithm. Though the clus-
tering accuracy of `1-norm S-NMTF decreases
when noise exists, still it reduces the least among
the five algorithms. This result convincingly
demonstrates the robustness of our proposed `1-
norm S-NMTF method.

Alg L11 L22 PMF NMF Kms
s.1(P) 0.547 0.546 0.521 0.546 0.546
s.1(N) 0.546 0.525 0.516 0.540 0.545
s.2(P) 0.543 0.543 0.534 0.543 0.543
s.2(N) 0.543 0.539 0.531 0.513 0.531
s.3(P) 0.536 0.536 0.524 0.536 0.536
s.3(N) 0.536 0.534 0.522 0.517 0.508

Table 3: Clustering Accuracy Contrast.

5 Conclusion

In this paper, we presented an `1-norm Symmet-
ric Nonnegative Matrix Tri-Factorization Frame-
work to cluster multi-type relational data simulta-
neously. Our proposed approach clusters different
types of data, using its inter-type relationship by
transforming the original problem into a symmet-
ric NMTF problem. We also presented an auxil-
iary function and high order matrix inequality to
derive the solution algorithm. The proposed algo-
rithm not only makes use of the rich data struc-

400



ture to improve the clustering accuracy, but also
remains robust when there is noise and outliers.
Experimental results demonstrate the potential us-
age and advantage of `1-norm S-NMTF in cluster-
ing especially when there are outliers, which is in
accordance with our theory analysis.

References
C. Ding, X. He, and H.D. Simon. 2005. On the equiv-

alence of nonnegative matrix factorization and spec-
tral clustering. In SDM.

C. Ding, T. Li, W. Peng, and H. Park. 2006. Orthog-
onal nonnegative matrix t-factorizations for cluster-
ing. In SIGKDD.

C. Ding, T. Li, and M.I. Jordan. 2010. Convex
and semi-nonnegative matrix factorizations. IEEE
TPAMI, 32(1):45–55.

Deguang Kong, Chris Ding, and Heng Huang. 2011.
Robust nonnegative matrix factorization using l21-
norm. In Proceedings of the 20th ACM international
conference on Information and knowledge manage-
ment, pages 673–682. ACM.

D.D. Lee and H.S. Seung. 1999. Learning the parts
of objects by non-negative matrix factorization. Na-
ture, 401(6755):788–791.

Bo Long, Zhongfei Mark Zhang, Xiaoyun Wu, and
Philip S Yu. 2006. Spectral clustering for multi-
type relational data. In Proceedings of the 23rd in-
ternational conference on Machine learning, pages
585–592. ACM.

Andrew L. Maas, Raymond E. Daly, Peter T. Pham,
Dan Huang, Andrew Y. Ng, and Christopher Potts.
2011. Learning word vectors for sentiment analy-
sis. In Proceedings of the 49th Annual Meeting of
the Association for Computational Linguistics: Hu-
man Language Technologies, pages 142–150, Port-
land, Oregon, USA, June. Association for Computa-
tional Linguistics.

Feiping Nie, Heng Huang, Chris Ding, Dijun Luo, and
Hua Wang. 2011. Robust principal component anal-
ysis with non-greedy l1-norm maximization. In IJ-
CAI Proceedings-International Joint Conference on
Artificial Intelligence, volume 22, page 1433. Cite-
seer.

F. Wang, T. Li, and C. Zhang. 2008. Semi-supervised
clustering via matrix factorization. In SDM.

H. Wang, H. Huang, F. Nie, and C. Ding. 2011a.
Cross-language web page classification via dual
knowledge transfer using nonnegative matrix tri-
factorization. In SIGIR.

H. Wang, F. Nie, H. Huang, and C. Ding. 2011b.
Dyadic transfer learning for cross-domain image
classification. In ICCV.

Hua Wang, Heng Huang, and Chris Ding. 2011c. Si-
multaneous clustering of multi-type relational data
via symmetric nonnegative matrix tri-factorization.
In Proceedings of the 20th ACM international con-
ference on Information and knowledge management,
pages 279–284. ACM.

Hua Wang, Feiping Nie, Heng Huang, and Chris Ding.
2011d. Nonnegative matrix tri-factorization based
high-order co-clustering and its fast implementation.
In Data Mining (ICDM), 2011 IEEE 11th Interna-
tional Conference on, pages 774–783. IEEE.

Hua Wang, Heng Huang, Chris Ding, and Feiping Nie.
2013. Predicting protein–protein interactions from
multimodal biological data sources via nonnegative
matrix tri-factorization. Journal of Computational
Biology, 20(4):344–358.

Hua Wang, Feiping Nie, and Heng Huang. 2014. Ro-
bust distance metric learning via simultaneous l1-
norm minimization and maximization. In Proceed-
ings of the 31st International Conference on Ma-
chine Learning (ICML-14), pages 1836–1844.

401


