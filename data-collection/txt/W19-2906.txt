



















































A Modeling Study of the Effects of Surprisal and Entropy in Perceptual Decision Making of an Adaptive Agent


Proceedings of the Workshop on Cognitive Modeling and Computational Linguistics, pages 53–61
Minneapolis, USA, June 7, 2019. c©2019 Association for Computational Linguistics

53

A Modeling Study of the Effects of Surprisal and Entropy in Perceptual
Decision Making of an Adaptive Agent

Pyeong Whan Cho
Department of Psychology

University of Michigan
Ann Arbor, MI 48109

pyeongwc@umich.edu

Richard L. Lewis
Department of Psychology

University of Michigan
Ann Arbor, MI 48109
rickl@umich.edu

Abstract

Processing difficulty in online language com-
prehension has been explained in terms of sur-
prisal and entropy reduction. Although both
hypotheses have been supported by experi-
mental data, we do not fully understand their
relative contributions on processing difficulty.
To develop a better understanding, we propose
a mechanistic model of perceptual decision
making that interacts with a simulated task
environment with temporal dynamics. The
proposed model collects noisy bottom-up ev-
idence over multiple timesteps, integrates it
with its top-down expectation, and makes per-
ceptual decisions, producing processing time
data directly without relying on any linking
hypothesis. Temporal dynamics in the task en-
vironment was determined by a simple finite-
state grammar, which was designed to cre-
ate the situations where the surprisal and en-
tropy reduction hypotheses predict different
patterns. After the model was trained to max-
imize rewards, the model developed an adap-
tive policy and both surprisal and entropy ef-
fects were observed especially in a measure re-
flecting earlier processing.

1 Introduction

Over the past decades, computational models of
sentence comprehension have improved our un-
derstanding of processing difficulty arising in on-
line language comprehension. It has been discov-
ered that information-theoretic complexity metrics
can predict processing difficulty (for review, see
Hale, 2016).

The surprisal hypothesis (Hale, 2001; Levy,
2008) proposes processing difficulty of a word wk
in a context w1:k−1 is proportional to its surprisal,
− log p(wk|w1:k−1). Levy (2008) proved that sur-
prisal is equivalent to Kullback-Leibler divergence
between the probability distributions over parse

trees T before and after observing the word wk,
DKL(P (T |w1:k)‖P (T |w1:k−1)).

On the other hand, the entropy reduction hy-
pothesis (Hale, 2003) claims that processing dif-
ficulty is proportional to a non-negative amount
of entropy reduced after observing a word wk:
max(H(S|w1:k−1) −H(S|w1:k), 0) where S is a
random variable of sentences. It is not clear why
the language processing system works insensitive
to negative entropy changes.

Both hypotheses have been supported by ex-
perimental data (for surprisal, see Demberg and
Keller, 2008; Smith and Levy, 2013; for entropy
reduction, see Frank, 2013; Linzen and Jaeger,
2016). Some behavioral studies reported both ef-
fects of surprisal and entropy reduction (Linzen
and Jaeger, 2016; Lowder et al., 2018) and in such
cases, the surprisal effect was much stronger than
the entropy reduction effect.

However, we do not have comprehensive under-
standing of their relative contribution to process-
ing load. Empirically, the estimation of surprisal
and entropy values requires a language model, the
quality of which depends on many factors (e.g.,
the corpus size, the model type) (c.f., Goodkind
and Bicknell, 2018 argued the effect of surprisal
was robust when the measures were estimated us-
ing a wide range of language models with different
qualities). Also surprisal and entropy values tend
to be highly correlated in natural languages, which
makes it difficult to tease apart their relative roles
in online language processing.

To avoid these empirical problems, we intro-
duce a simple experimental paradigm, which com-
bines two well-established paradigms: saccade
target selection (OReilly et al., 2013) and artificial
language paradigm (Harrison et al., 2006), both
of which have been used to answer related ques-
tions. In the artificial language paradigm, we de-
sign a language such that it has some distributional



54

properties of interest. For example, we can de-
sign a grammar in which the surprisal and the en-
tropy reduction hypotheses make different predic-
tions. For example, Linzen and Jaeger (2014) used
a simple finite-state grammar to create such sit-
uation and discussed alternative accounts of pro-
cessing difficulty. In the present study, we used a
variant of their grammar (see Figure 3). Due to the
simplicity of the grammar, entropy and entropy re-
duction measures are perfectly correlated. When
we discuss the effect of those measures, we will
refer to it as the entropy effect but we are neutral
in whether it should be interpreted as the effect of
entropy or the effect of entropy reduction; we re-
serve the question for future work.

Perception module (HMM; Section 3)
(equipped with a perfect language model)

Task environment (Section 2)

Decision making module (A2C; Section 4)

Agent

Action 
(eye-movement)

Figure 1: Model architecture. The model consists of
two modules: perception module and decision making
module. Equipped with a perfect language model, the
perception module (implemented as a Hidden Markov
Model) integrates noisy inputs from environment with
its top-down expectation. The decision making module
(implemented as a neural network with the Actor Critic
architecture) makes an action based on the output of the
perception module.

To develop a better understanding, we propose
a mechanistic model of perceptual decision mak-
ing and investigate its behavior in a simulated task
environment with temporal dynamics, focusing on
the effects of surprisal and/or entropy. Figure 1
presents the architecture of the model and how
it interacts with the task environment. It con-
sists of two components: the perception module
at the bottom collects noisy bottom-up evidence
from the task environment and updates its state
(expressed in [posterior] probability distributions).
The decision making module at the top monitors
the state of the perception module and makes an
action (i.e., decision), which will update the state
of the task environment. The design of the percep-
tion module was inspired by Bicknell and Levy
(2010) that investigated a related research ques-
tion. Unlike their model, we used reinforcement

learning to let the agent develop an optimal policy.
The main contribution of the present study is

that we propose a full cognitive architecture that
performs perceptual decision making, which we
argue shares a core computational problem of un-
certainty management with online language com-
prehension tasks (e.g., self-paced reading) and in-
vestigate the optimal behavior by exploring an un-
restricted decision policy space.

In the following sections, we will present each
component in Figure 1 in detail. In Discussion, we
conclude.

2 Task Environment

+ + + + +40+ +

warning
(400ms)

warning
(400ms)

feedback
(1000ms)

target
(350ms)

target
(350ms)

wait until 
return

wait until 
return

score

trial score = C - TrialRT (ms)

visual feedback (explosion animation) 
when a mole is fixated

Figure 2: Task environment. The events occurring at
two sample trials are shown. The agent is asked to
“look at” the target (color dot) as quickly and accu-
rately as possible.

We created a simulated saccade target selection
task environment (e.g., OReilly et al., 2013) (see
Figure 2). In each trial, a target appears at one
of 7 positions and the agent is asked to “look at”
the target as quickly and accurately as possible and
look back at the center. The returning fixation ter-
minates the trial and initiates the next trial.

In the simulated task environment, each of 7
locations was represented as an angle (in radian)
in a circular space [0, 2π) and associated with a
symbol. The center location was associated with
an empty symbol �, representing the absence of
a target. A selection of a symbol was treated as
the fixation on its associated location.1 Following
OReilly et al. (2013), we measured the number of
timesteps that the agent took to select the target
(target arrival) and the number of timesteps that
the agent took to make the first “meaningful” de-
cision, by which we mean the first selection of a
non-center location which may or may not be cor-
rect (first saccade onset).2

The locations of the targets changed follow-
ing a simple finite-state grammar (see Figure 3 so

1For modeling convenience, we ignored eye-movement
details (e.g., the minimal duration of a saccade).

2The proposed model selects a symbol at every timestep.
When the model selects the symbol that it previously se-
lected, we treat it as a continuation of the previous fixation.



55

Sample space Description
V = {a, b, c, d, e, f, g} the set of input symbols
U = {�} the set of the empty symbol
W = [0, 2π) a circular space of angles
S = (V × U) ∪ (U × V) the set of states
X = V ∪ U the set of input symbols
Y =W ∪ U the set of observations

Table 1: Sample spaces

were partially predictable. We were interested in
whether the onset and arrival measures are depen-
dent on the amount of uncertainty.

3 Agent: Perception Module

For discussion, we introduce some notational con-
ventions. We use the uppercase (e.g., X), low-
ercase (e.g., x), and calligraphic font (e.g., X ) to
denote a random variable, a particular sample, and
its sample space. We use the superscript to denote
the position of a symbol in a sequence of symbols
and the subscript to denote a particular element in
a sample space.

Let S, X , and Y be a discrete random variable
of states, a discrete random variable of input sym-
bols, and a mixed random variable of observations.
A probability distribution over states P (S) will be
referred to as a “parser state”, which should be dis-
tinguished from simple states. The sample spaces
of those variables are S, X , and Y , respectively
(see Table 1).

The perception module was implemented as a
Hidden Markov Model (HMM), where the hidden
variable S(k) represents states after processing the
k-th symbol x∗(k) in a sequence of symbols, as-
suming the agent is equipped with a perfect lan-
guage model. X(k) representing symbol identities
is conditioned on S(k). Y (k) represents the obser-
vations of the input symbol (i.e., a particular loca-
tion in the task environment [see Figure 4]).

3.1 Language Model

Let us begin with the agent’s language model. We
used a Markov chain (See Figure 3) to implement
a language model but other types of models can
be used if they can emulate the environmental dy-
namics. For convenience, unique bigrams were
used as states: s(k) = x(k)x(k+1). In this sim-
ple language, each state s(k) uniquely specifies
the present input symbol x(k); p(xj |si) = 1 if
si = xjxk and 0 otherwise. An example sequence
of two sentences is c � f � | a � d �where | indicates
a hidden sentence boundary; the underlying state

�a

1/3 �b

1/3

�c
1/3

a�

b�

c�

�d

1/4

�e
1/4

�f

1/4

1/4

3/4

�g

1/4

3/4

1/4

d�

e�

f�

g�

Figure 3: Grammar G specifies probabilistic transitions
from a state si to another state sj between symbols.
The transition probability p(sj |si) is shown on an edge
from node i to node j. The edges with no labels have
the transition probability of 1.

change is as follows: c�-�f -f�-�a-a�-�d-d�-�b.
Some distributional information of the language

is given in Table 2. In the present study, we will
focus on three conditions where f is presented in
different contexts a�, b�, and c� at which surprisal
and entropy reduction hypotheses predict different
patterns.3 We will refer to the three conditions as
HiE/HiS (HighEntropy/HighSurprisal), LoE/HiS,
and LoE/LoS. Let RT(·) be a decision making
time (in onset or arrival) at a certain condition.
The surprisal hypothesis predicts: RT(LoE/LoS)
< RT(LoE/HiS) = RT(HiE/HiS). The entropy
reduction hypothesis predicts: RT(LoE/LoS) =
RT(LoE/HiS) < RT(HiE/HiS). If both surprisal
and entropy (reduction) have unique contributions
to processing load, assuming the surprisal effect
is stronger than the entropy effect, we expect:
RT(LoE/LoS) < RT(LoE/HiS) < RT(HiE/HiS).

3.2 Mapping between Symbols and
Observations

Figure 4 presents the mapping of symbols to the
locations on the task environment. The empty
symbol, representing the absence of target, is
mapped to the center location. The other symbols

3The target g in the same three contexts was designed to
be a mirror case of f and introduced (1) for counterbalancing
and (2) to increase the number of data points in the planned
human experiment. However, due to the difference in their
closest neighbors, processing f and g in the same three con-
texts can be different.



56

context target ptarg surprisal entropy
a� d, e 0.083 1.386 1.386
a� f , g 0.417 1.386 1.386
b� f 0.417 1.386 0.562
b� g 0.417 0.288 0.562
c� f 0.417 0.288 0.562
c� g 0.417 1.386 0.562

Table 2: Distributional information for unique
context-target combinations. ptarg represents the
unigram target probability. Entropy measures
the amount of uncertainty after processing context
but before receiving target: H(P (X|context)) =
−
∑

x p(x|context) log p(x|context). Surprisal and en-
tropy were calculated with e as base.

�

a

b

c
d

e

f

g

Figure 4: Screen configuration. Each of 7 symbols (a
- g) is mapped to a unique location on the ring. At the
center, there is a fixation cross, corresponding to a null
symbol �.

in V are mapped to 7 equally-distributed real num-
bers inW via a bijective function A; the image of
V is {(2π)(i/7) + j|i ∈ {0, · · · , 6}} where j can
take any arbitrary number in [0, 2π). In the fol-
lowing example, we consider a simple mapping:
A(b) = 0, A(c) = (2π)(1/7), A(d) = (2π)(2/7),
A(e) = (2π)(3/7), A(f) = (2π)(4/7), A(g) =
(2π)(5/7), A(a) = (2π)(6/7).

3.3 Noisy Input Channel

Let y∗ be the noise-free observation of the target
symbol x∗. Note that x∗ is chosen by the task envi-
ronment dynamics. We assume that the perception
module samples an observation y via a noisy input
channel at every timestep. The conditional proba-
bility of y given x, p(y|x), is presented in Table 3.
Observations y’s over multiple timesteps are as-
sumed to be independent from each other given
target x∗.4 We will use the same conditional prob-

4The likelihood of observation y is also conditioned on
the present fixation location, which is modeled as the sym-
bol chosen by the decision making module at the previous
timestep (see Section 4). The likelihood function in the target
present condition (see Table 3) assumes that the present fix-
ation is on the center, which is true at the beginning of each
target-present trial; the measure of first saccade onset is accu-

ability distribution when the module updates the
posterior probability of symbol x given noisy ob-
servation y. Parameters α and β are false positive
and false negative rates, respectively. In the false
positive case, we assume every value in the circu-
lar space W is equally likely. In the true positive
case (i.e., x∗ ∈ V , we assume p(y|x∗) is higher as
y is closer to y∗ = A(x∗). This intuition is im-
plemented by introducing a von Mises distribution
(with parameters µ and κ), which is a Gaussian-
like distribution applied to a circular space.

sample a location sample
on the ring the center
y ∈ W y ∈ U

target present
(x ∈ V) (1− β)F (y;µ = A(x), κ) β

target absent
(x ∈ U ) α/(2π) 1− α

Table 3: p(y|x), a conditional probability of a noisy
sample y given a symbol x where α and β are the rates
of false positive and false negative, F is the probabil-
ity density function of the von Mises distribution with
location and scale parameters µ and κ. As κ increases,
larger probability mass is placed near the mean µ

3.4 Noisy Memory

In addition to noisy input, we consider noise in
memory. More specifically, we assume the mem-
ory of the parser state is noisy such that a state
s can be replaced with another state s′. Noisy
memory is implemented by applying the confu-
sion matrix to the parser state P (S): P (S′) =
P (S) · P′S→S′ where P (S) is a row vector of
probabilities over possible states and P′S→S′ is the
transition probability matrix in which the (i, j)-th
component represents p(s′j |si).

We consider three types of confusion: (1: rand)
purely random noise which allows every transition
si → sj for all pairs of i and j, (2) similarity-
based interference allows transitions between two
states similar to each other. Two types of simi-
larities were considered. (2a: sim1) symbol-type
similarity; e.g., a, b, c are similar because they oc-
cur at the same position in a sequence (i.e., as the
first word of a two-word sentence) so a� can be
recalled as a�, b�, or c�. (2b: sim2) transposition

rate. However, the likelihood function would not be ideal for
modeling the belief update from noisy observations after the
first saccade to a non-target location. Although not accurate,
the measure of target arrival can still be informative because
it contains information about whether the target was chosen
at the first try or not.



57

+ symbol-type similarity; for example, a� can be
confused as �a, �b, and �c.

More specifically, we consider p(sj |si) =
(1 − ηnoise)δij + ηnoise{ηrand prand(sj |si) +
(1 − ηrand)((1 − ηtrans) psim1(sj |si) +
ηtranspsim2(sj |si))}; ptype(sj |si) (where type
∈ {rand, sim1, sim2}) was set to the reciprocal
of the number of transitions corresponding to
the type of confusion if si → sj is allowed
and 0 otherwise. We aggregate the conditional
probabilities into a transition probability matrix
P′S→S′ such that pi,j = p(sj |si). In the present
study, ηnoise = 0.001, ηrand = 0.1, ηtrans = 0.1.

3.5 Belief Update

The module updates posterior probabilities of tar-
get locations over multiple timesteps by accumu-
lating bottom-up noisy evidence (likelihood) and
integrating it with top-down expectation (prior
probabilities) p(x(k)|s(k−1,Tk−1)) where Tk is the
last timestep at the previous trial k − 1. More de-
tailed processes are presented in the below.

Step 1: Each trial begins with the instan-
taneous update of input symbol from x∗(k−1)

to x∗(k). The model uses the last parser state
P (S(k−1,Tk−1)) to set log priors forX(k) and S(k).
LPS(k) = log{P (S(k−1,Tk−1)) ·P′S→S′ ·PS→S}
(P′S→S′ adds noise to the past parser state and
PS→S [from the language model] uses the noisy
past parser state to predict the following parser
state); LPX(k) = log{P (Sk) ·PS→X}.

Step 2: At every timestep t, the mod-
ule collects a noisy observation y(k,t)

and updates log-likelihoods of X(k) and
S(k): the i-th component of a row vec-
tor LLX(k, t) is

∑t
t′=1 log p(y

(k,t′)|xi);
LLS(k, t) = PS→X exp{LLX(k, t)}>.

Step 3. Posteriors of Xk and Sk given y
(1:t)
k

are as follows: P (X(k)|y(k,1:t)) = σ(LLX(k, t) +
LPX(k)); P (Sk|y(k,1:t)) = σ(LLS(k, t)+LPS(k))
where σ is the standard softmax function.

Step 2 and Step 3 are iterated until (1) the deci-
sion making module (see the next section) selects
the target symbol x∗ correctly or (2) the maximum
number of timesteps (= 100) has passed.

3.6 The Module Behavior

We created multiple instances of the perception
module by setting some module parameters to dif-
ferent values (see Table 4) and investigated how
the posterior probabilities changed in the three

conditions of our interest. Each of 3 modules pro-
cessed 200 blocks of 8 different sentence types. In
each block, the presentation order of the sentences
was randomized. For each sentence, the model
processed each symbol over 50 timesteps.

Module memory noise ηnoise perception noise (1/κ)
M1 0.001 1
M2 0.001 1/3
M3 0.2 1

Table 4: Different module settings. ηnoise determines
the amount of memory noise while 1/κ determines the
amount of input noise. We fixed α (false negative rate)
and β (false positive rate) to 0.05 in this study.

0 10 20 30 40 50
Timestep

0.0

0.2

0.4

0.6

0.8

1.0

Ta
rg

et
 p

os
te

rio
r p

ro
ba

bi
lit

y

a#f (H/H)
b#f (L/H)
c#f (L/L)

0 10 20 30 40 50
Timestep

0.0

0.2

0.4

0.6

0.8

1.0

a#f (H/H)
b#f (L/H)
c#f (L/L)

0 10 20 30 40 50
Timestep

0.0

0.2

0.4

0.6

0.8

1.0

a#f (H/H)
b#f (L/H)
c#f (L/L)

Figure 5: Plots of target posterior probabilities in dif-
ferent conditions. Each ribbon presents mean ± one
standard error calculated from 200 trials.

Figure 5 presents the target posterior probability
change as the modules processed f in three dif-
ferent contexts a� (HiE/HiS), b� (LoE/HiS), and
c� (LoE/LoS). The effect of surprisal is clear in
all three modules. This is expected from our be-
lief update process. When a new symbol (i.e., f )
is presented, the perception module uses the last
parser state to reset log priors, which determine
different starting points before evidence integra-
tion. When the race begins, the symbol candidate
with a low surprisal value is many steps ahead of
its competitors with high surprisal values.

On the other hand, the effect of entropy (re-
duction) was weakly suggested only in Module 1
(see panel A in Figure 5). The target posterior
probability increased slightly faster in context b�
(LoE/HiS) than in context c� (HiE/HiS).

Based on the observed patterns, we chose Mod-
ule1 as the perception module of the agent.

4 Agent: Decision Making Module

Instead of searching a restricted policy space (e.g.,
static decision boundary such as maxx p(x) > .9,
or as in Bicknell and Levy, 2010), we use rein-
forcement learning to search a huge policy space



58

with no restriction to discover a (near-)optimal de-
cision policy in the task environment.

4.1 Advantageous Actor-Critic

Observation (14)
(P(S), P(X))

Shared representation (20)
(ReLU)

Value (1)
(linear)

Actions (8)
(softmax)

Figure 6: Actor-critic architecture of the decision mak-
ing module. The number in parentheses indicates the
number of units in each layer.

The decision making module has an Advanta-
geous Actor-Critic (A2C) architecture (c.f., for the
asynchronous version A3C, see Mnih et al., 2016)
(see Figure 6) in which each of 8 actions was
mapped to a unique location in the task environ-
ment. Let st be the state of the perception module
at timestep t. Let V (st) and π(ai|st) be the value
output and the probability of choosing an action ai
given input st. For the input, an action at is sam-
pled from the action probability distribution. The
advantage of the action is defined as follows:

Adv(st, at; θ) =
k−1∑
i=0

γirt+i+γ
kV (st+k; θ)−V (st; θ)

where γ(= 0.99) is the discount factor for future
rewards, rt is the acquired reward at timestep t
by making an action at, and θ is the vector of the
model parameters. The module makes actions un-
der the current policy over k(= 5) steps and uses
the rewards collected over k steps to improve the
value estimate.

4.2 Reward in the Task
We constructed 4 instances of the task environ-
ment in which the perception module (Module1,
see Table 4) was exposed to different sequences of
symbols (that were generated by the same gram-
mar). The decision making module interacted with
all four environments simultaneously to collect tu-
ples (state, action, reward, next state). This is
motivated to collect relatively independent train-
ing samples. At every step, the perception mod-

ule collects a new observation and updates its pos-
terior probabilities over symbols and over states.
The decision making module takes both distribu-
tions as input and outputs its value and an ac-
tion sampled from the action probability distri-
bution. When the action chosen at timestep t(≤
100) corresponds to the target symbol, it termi-
nates the present trial and the new target sym-
bol is presented in the task environment. In this
case, the module receives a reward (100− t)/100;
faster responses are rewarded more than slower re-
sponses. If the module selects a non-target sym-
bol (which is different from its previous selection),
the model receives a penalty (= -1). If the model
selects the same wrong symbol as in the previ-
ous timestep (i.e., at = at−1), the model is not
penalized; the reward is 0 in this case. For ex-
ample, let us suppose the decision making mod-
ule made a sequence of choices �, �, a, a, �, b, �, c
when the target symbol was c, assuming the previ-
ous trial ended at the selection of the previous tar-
get �. Then, the module would receive a sequence
of rewards 0, 0,−1, 0,−1, 0, (100 − 8)/100. If
the model fails to choose the target symbol for
100 timesteps, the task environment is updated to
present a new target symbol. Thus, the decision
making module has an option not to select any new
symbol; technically, the model can keep choos-
ing the previous target symbol over 100 timesteps.
This suboptimal policy is better than choosing a
non-target symbol; while the maximum reward per
trial is 0.99 (if the model chooses the correct tar-
get at the first timestep after the task environment
update), the model is given -1 for a single wrong
selection.

4.3 Training
Over the course of training, the model parameters
are updated to minimize the following loss func-
tion:

L(st, at; θ) =− log π(at|st, θ)Adv(st, at; θ)
− λHH(st)
+ λCAdv(st, at; θ)

2

where H(st) is the entropy of action probabili-
ties π(a|st). Hyperparameter λH determines the
strength of entropy regularization, which is in-
tended to encourage the module to explore the pol-
icy space without converging to a suboptimal pol-
icy too early. In our case, the model developed
suboptimal policies when λH was fixed at a small



59

value from the beginning; the model never chose
target symbols d and e that have lower unigram
frequencies than f and g. When d and e were pre-
sented in context a�, the module waited until the
trial ended after the deadline (100 timesteps) with-
out choosing any non-center location.

We used the ADAM optimizer (Kingma and Ba,
2014) (learning rate = 0.0003) to update the de-
cision making module’s parameters. The coeffi-
cient of value prediction cost (λC) was fixed at 0.5
but the coefficient of entropy regularization (λH )
started at 0.01 and reduced to 0.001 after 400, 000
timesteps and 0.0001 after 1, 000, 000 timesteps.
We stopped training after 1, 200, 000 timesteps af-
ter observing the performance did not improve.
Figure 7 presents the average reward acquired on
a randomly generated grammatical sequence of 10
symbols during test.5

0 200000 400000 600000 800000 1000000 1200000
Timesteps

8

6

4

2

0

Re
wa

rd
s

Figure 7: Trajectory of average reward acquired during
model evaluation.

4.4 The Model Behaviors
The model consisted of the perception module
(Module 1) and the trained decision making mod-
ule. It was given a long sequence of symbols,
a concatenation of 200 blocks of 12 sentences
(of 8 sentence types); b�g� and c�f� were three
times more frequent than other sentence types (see
Figure 3). We focus on the model’s behaviors
when f was presented in three different contexts
a� (HiE/HiS), b� (LoE/HiS), and c� (LoE/LoS).

Figure 8 presents the distributions of log(onset)
and log(arrival) as well as their means, standard
errors (thick lines), and standard deviations (thin
lines), suggesting the effects of both surprisal and
entropy. The entropy effect was more salient in
log(onset), which reflects the perception module’s
states earlier in processing.

5We trained three instances of the model with different
random seeds. Their behaviors were not identical but sim-
ilar. In the text, we report the behavior of the best model
that achieved the highest reward over 2400 four-symbol sen-
tences because we are interested in the optimal agent’s be-
havior. When the trials with a trivial target � were excluded,
the best model achieved average reward of 0.591. Other two
models acquired average rewards of 0.566 and 0.485.

0 1 2 3 4
log(onset)

0.0

0.2

0.4

0.6

0.8

pr
ob

ab
ilit

y 
de

ns
ity

HiE/HiS
LoE/HiS
LoE/LoS

(a) log(onset)

0 1 2 3 4
log(arrival)

0.0

0.2

0.4

0.6

0.8

pr
ob

ab
ilit

y 
de

ns
ity

HiE/HiS
LoE/HiS
LoE/LoS

(b) log(arrival)

Figure 8: Histograms of (a) log onset time and (b) log
arrival time in timesteps. The mean ± one standard
error (thick line) or one standard deviation (thin line)
in each condition was presented at the top.

0 20 40 60 80 100
Timestep

0.0

0.2

0.4

0.6

0.8

1.0

Ta
rg

et
 se

le
ct

io
n 

pr
op

or
tio

n
a#f (HiE/HiS)
b#f (LoE/HiS)
c#f (LoE/LoS)

Figure 9: Timecourses of target selection proportions.

Figure 9 presents the proportion of target selec-
tion as a function of timesteps and contexts.6 Both
surprisal and entropy effects are clear.

0.0 0.2 0.4 0.6 0.8 1.0 1.2
H(X) at onset

0.0

0.5

1.0

1.5

2.0

2.5

3.0

pr
ob

ab
ilit

y 
de

ns
ity

HiE/HiS
LoE/HiS
LoE/LoS

(a) H(X) at onset

0.0 0.2 0.4 0.6 0.8 1.0
H(X) at arrival

0.0

0.5

1.0

1.5

2.0

2.5

3.0

pr
ob

ab
ilit

y 
de

ns
ity

HiE/HiS
LoE/HiS
LoE/LoS

(b) H(X) at arrival

Figure 10: Histograms of entropy values of posterior
probability distributions at (a) onset and (b) arrival. The
mean ± one standard error (thick line) or one standard
deviation (thin line) is presented for each condition at
the top.

Figure 10 presents the distribution of the en-
tropy values of X in the perception module when
the decision making module chose the first non-
center location (onset) and the target location (ar-

6Different trials ended at different timesteps, typically
much earlier than the maximum timesteps (= 100). For the
purpose of calculating the proportion, we extended the final
choice to the maximum timestep; for example, if the last ac-
tion (i.e., selection of symbol f ) was made at timestep 30 in
a trial, we treated the module chose the same symbol for the
next 70 timesteps.



60

rival). Distributions in the conditions HiE/HiS and
LoE/HiS are largely overlapped but can be distin-
guished. Note that in all three conditions, the ideal
target posterior probability is 1 and the entropy is
0. However, the decision making module made
decisions before the perception module developed
its belief on the target to the ideal level. This was
true especially in HiE/HiS and LoE/HiS condi-
tions. This is because the target posterior probabil-
ity increased slowly either because the target has
many competitors (HiE/HiS) or because the target
has a very strong competitor (LoE/HiS). Instead
of waiting until the target posterior probability in-
creased enough, the module seemed to take a more
risky approach (i.e., making a choice in a more un-
certain situation) to obtain more rewards. It makes
sense that the model took a safer approach for the
target f in the LoS context c� given that symbol f
in the LoS context was three times more frequent
than f in each HiS context. Developing a risky
policy for such case will be harmful.

5 Discussion

In this study, we introduced a simple task that
combines the saccade target selection task (e.g.,
OReilly et al., 2013) with the artificial language
paradigm (e.g., Harrison et al., 2006), both of
which have been used to investigate how the hu-
man cognitive system deals with uncertainty. In-
spired by Linzen and Jaeger (2014), we designed
a simple artificial language in which the surprisal
hypothesis and the entropy reduction hypothesis
predict different patterns. When a perceptual de-
cision making model was trained to maximize re-
wards in the simulated task environment, both sur-
prisal and entropy effects were observed in the
model’s behavior; consistent with the literature
(Linzen and Jaeger, 2016; Lowder et al., 2018),
the surprisal effect was stronger than the entropy
effect.

The model developed a flexible decision pol-
icy such that it made more risky decisions in
the HiE/HiS and LoE/HiS conditions than in the
LoE/LoS condition. It was interpreted as the
model pursuing a good balance between speed and
accuracy because the model could obtain higher
rewards from faster responses. The investigation
of decision policy reveals the adaptive nature of
the system which is not clear from pure rational
models.

Our modeling study was intended to explore

design-related issues and predict results in human
eye-tracking experiments that we plan to run. In
human experiments, participants need to learn the
grammar hidden in a sequence of symbols. To
make learning easier, we chose a simple gram-
mar which made it hard to interpret the effect of
entropy; it could be the effect of entropy or the
effect of entropy reduction. However, the pro-
posed model is general enough to cover more com-
plex grammars and diverse situations (e.g., self-
paced reading). We chose the Hidden Markov
Model and the A2C architecture for the perception
the decision making modules mainly for model-
ing convenience. The HMM can be replaced with
a more elaborated neural language model when
dealing with more complex grammars. The em-
phasis should be given to our architectual choice.
The addition of the decision making module that
has the ability to develop a policy on its own pro-
vides the system to control the amount of uncer-
tainty flexibly in response to the task situations.

Bicknell and Levy (2010) took the same ap-
proach similar to explain reading eye movement
patterns, which influenced our work. Our work
is different from theirs in that (1) we considered
noisy memory more directly and (2) we used re-
inforcement learning to let the model discover a
good decision policy; we believe both additions
can lead us to interesting research questions.

References
Klinton Bicknell and Roger Levy. 2010. A rational

model of eye movement control in reading. In Pro-
ceedings of the 48th annual meeting of the Associ-
ation for Computational Linguistics, pages 1168–
1178. Association for Computational Linguistics.

Vera Demberg and Frank Keller. 2008. Data from eye-
tracking corpora as evidence for theories of syntactic
processing complexity. Cognition, 109(2):193–210.

Stefan L. Frank. 2013. Uncertainty reduction as a mea-
sure of cognitive load in sentence comprehension.
Topics in Cognitive Science, 5(3):475–494.

Adam Goodkind and Klinton Bicknell. 2018. Predic-
tive power of word surprisal for reading times is a
linear function of language model quality. Proceed-
ings of the 8th Workshop on Cognitive Modeling and
Computational Linguistics (CMCL 2018), pages 10–
18.

John Hale. 2001. A probabilistic Earley parser as
a psycholinguistic model. In Proceedings of the
Second Meeting of the North American Chapter of
the Association for Computational Linguistics on

https://doi.org/10.1016/j.cognition.2008.07.008
https://doi.org/10.1016/j.cognition.2008.07.008
https://doi.org/10.1016/j.cognition.2008.07.008
https://doi.org/10.1111/tops.12025
https://doi.org/10.1111/tops.12025
https://aclanthology.info/papers/W18-0102/w18-0102
https://aclanthology.info/papers/W18-0102/w18-0102
https://aclanthology.info/papers/W18-0102/w18-0102
https://doi.org/10.3115/1073336.1073357
https://doi.org/10.3115/1073336.1073357


61

Language Technologies, NAACL ’01, pages 1–8,
Stroudsburg, PA, USA. Association for Computa-
tional Linguistics.

John Hale. 2003. The information conveyed by words
in sentences. Journal of Psycholinguistic Research,
32(2):101–123.

John Hale. 2016. Information-theoretical complex-
ity metrics. Language and Linguistics Compass,
10(9):397–412.

L. M. Harrison, A. Duggins, and K. J. Friston. 2006.
Encoding uncertainty in the hippocampus. Neural
Networks, 19(5):535–546.

Diederik P. Kingma and Jimmy Ba. 2014.
Adam: A method for stochastic optimization.
arXiv:1412.6980 [cs]. ArXiv: 1412.6980.

Roger Levy. 2008. Expectation-based syntactic com-
prehension. Cognition, 106(3):1126–1177.

Tal Linzen and T. Florian Jaeger. 2014. Investigating
the role of entropy in sentence processing. In Pro-
ceedings of the Fifth Workshop on Cognitive Model-
ing and Computational Linguistics, pages 10–18.

Tal Linzen and T. Florian Jaeger. 2016. Uncertainty
and expectation in sentence processing: Evidence
from subcategorization distributions. Cognitive Sci-
ence, 40(6):1382–1411.

Matthew W. Lowder, Wonil Choi, Fernanda Ferreira,
and John M. Henderson. 2018. Lexical predictabil-
ity during natural reading: Effects of surprisal and
entropy reduction. Cognitive Science, 42(S4):1166–
1183.

Volodymyr Mnih, Adri Puigdomnech Badia, Mehdi
Mirza, Alex Graves, Timothy P. Lillicrap, Tim
Harley, David Silver, and Koray Kavukcuoglu.
2016. Asynchronous methods for deep reinforce-
ment learning. arXiv:1602.01783 [cs]. ArXiv:
1602.01783.

Jill X. OReilly, Urs Schffelgen, Steven F. Cuell, Tim-
othy E. J. Behrens, Rogier B. Mars, and Matthew
F. S. Rushworth. 2013. Dissociable effects of sur-
prise and model update in parietal and anterior cin-
gulate cortex. Proceedings of the National Academy
of Sciences, 110(38):E3660–E3669.

Nathaniel J. Smith and Roger Levy. 2013. The effect of
word predictability on reading time is logarithmic.
Cognition, 128(3):302–319.

https://doi.org/10.1023/A:1022492123056
https://doi.org/10.1023/A:1022492123056
https://doi.org/10.1111/lnc3.12196
https://doi.org/10.1111/lnc3.12196
https://doi.org/10.1016/j.neunet.2005.11.002
http://arxiv.org/abs/1412.6980
https://doi.org/10.1016/j.cognition.2007.05.006
https://doi.org/10.1016/j.cognition.2007.05.006
http://www.aclweb.org/website/old_anthology/W/W14/W14-20.pdf#page=20
http://www.aclweb.org/website/old_anthology/W/W14/W14-20.pdf#page=20
https://doi.org/10.1111/cogs.12274
https://doi.org/10.1111/cogs.12274
https://doi.org/10.1111/cogs.12274
https://doi.org/10.1111/cogs.12597
https://doi.org/10.1111/cogs.12597
https://doi.org/10.1111/cogs.12597
http://arxiv.org/abs/1602.01783
http://arxiv.org/abs/1602.01783
https://doi.org/10.1073/pnas.1305373110
https://doi.org/10.1073/pnas.1305373110
https://doi.org/10.1073/pnas.1305373110
https://doi.org/10.1016/j.cognition.2013.02.013
https://doi.org/10.1016/j.cognition.2013.02.013

