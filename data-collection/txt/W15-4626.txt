



















































Memory-Based Acquisition of Argument Structures and its Application to Implicit Role Detection


Proceedings of the SIGDIAL 2015 Conference, pages 178–187,
Prague, Czech Republic, 2-4 September 2015. c©2015 Association for Computational Linguistics

Memory-Based Acquisition of Argument Structures and its Application to
Implicit Role Detection

Christian Chiarcos and Niko Schenk
Applied Computational Linguistics Lab

Goethe University Frankfurt am Main, Germany
{chiarcos,n.schenk}@em.uni-frankfurt.de

Abstract

We propose a generic, memory-based ap-
proach for the detection of implicit se-
mantic roles. While state-of-the-art meth-
ods for this task combine hand-crafted
rules with specialized and costly lexical
resources, our models use large corpora
with automated annotations for explicit se-
mantic roles only to capture the distri-
bution of predicates and their associated
roles. We show that memory-based learn-
ing can increase the recognition rate of im-
plicit roles beyond the state-of-the-art.

1 Introduction

Automated implicit semantic role labeling (iSRL)
has emerged as a novel area of interest in the re-
cent years. In contrast to traditional SRL, which
aims to detect events (e.g., verbal or nominal
predicates) together with their associated seman-
tic roles (agent, theme, recipient, etc.) as overtly
realized in the current sentence, iSRL extends this
analysis with locally unexpressed linguistic items.
Hence, iSRL requires to broaden the scope beyond
isolated sentences to the surrounding discourse.
As an illustration, consider the following example
from Roth and Frank (2013):

El Salvador is now the only Latin Ameri-
can country which still has troops in [Iraq].
Nicaragua, Honduras and the Dominican Repub-
lic have withdrawn their troops [∅].

In the second sentence, a standard SRL parser
would ideally identify withdraw as the main ver-
bal predicate. In its thematic relation to the other
words within the same sentence, all countries
serve as the overtly expressed (explicit) agents,
and are thus labeled as arguments A0.1 Seman-
tically, they are the action performers, whereas

1For details on all PropBank labels used in our study, see
Palmer et al. (2005).

troops would carry the patient role A1 as the entity
which undergoes the action of being withdrawn.
However, given these explicit role annotations for
A0 and A1 in the second sentence, the standard
system would definitely fail to infer the underly-
ing, linguistically unexpressed, i.e., non-overt re-
alization of an implicit argument of withdraw (de-
noted by [∅]) about source information. Its corre-
sponding realization is associated with Iraq in the
preceding sentence, which is outside of the scope
of any standard SRL parser. The resulting implicit
role has the label A2.

Many role realizations are suppressed on the
surface level. The automated detection of such im-
plicit roles and their fillers, which are also called
null instantiations (NIs) (Fillmore, 1986; Ruppen-
hofer, 2005), is a challenging task. Yet, if un-
covered, NIs provide highly beneficial ‘supple-
mentary’ information which in turn can be incor-
porated into practical, downstream NLU applica-
tions, like automated text summarization, recog-
nizing textual entailment or question answering.

Current issues in iSRL Corpus data with man-
ually annotated implicit roles is extremely sparse
and hard to obtain, and annotation efforts have
emerged only recently; cf. Ruppenhofer et al.
(2010), Gerber and Chai (2012), and also Feiz-
abadi and Padó (2015) for an attempt to enlarge
the number of annotation instances by combina-
tion of scarce resources. As a result, most state-of-
the-art iSRL systems cannot be trained in a super-
vised setting and thus integrate custom, rule-based
components to detect NIs (we elaborate on related
work in Section 2). To this end, a predicate’s overt
roles are matched against a predefined predicate-
specific template. Informally, all roles found in the
template but not in the text are regarded as null in-
stantiations. Such pattern-based methods perform
satisfactorily, yet there are drawbacks:
(1) They are inflexible and absolute according to

178



their type, in that they assume that all candidate
NIs are equally likely to be missing, which is unre-
alistic given the variety of different linguistic con-
texts in which predicates co-occur with their se-
mantic roles.
(2) They are expensive in that they require hand-
crafted, idiosyncratic rules (Ruppenhofer et al.,
2011) and rich background knowledge in the form
of language-specific lexical resources, such as
FrameNet (Baker et al., 1998), PropBank (Palmer
et al., 2005) or NomBank (Meyers et al., 2004).
Dictionaries providing information about each
predicate and status of the individual roles (e.g.,
whether they can serve as implicit elements or not)
are costly, and for most other languages not avail-
able to the same extent as for English.
(3) Most earlier studies heuristically restrict im-
plicit arguments to core roles2 only (Tonelli and
Delmonte, 2010; Silberer and Frank, 2012), but
this is problematic as it ignores the fact that im-
plicit non-core roles also provide valid and valu-
able information. Our approach remains agnostic
regarding the role inventory, and can address both
core and non-core arguments. Yet, in accordance
with the limited evaluation data and in line with
earlier literature, we had to restrict ourselves to
evaluate NI predictions for core arguments only.

Our contribution We propose a novel, generic
approach to infer information about implicit roles
which does not rely on the availability of manually
annotated gold data. Our focus is exclusively on
NI role identification, i.e., per-predicate detection
of the missing implicit semantic role(s) given their
overtly expressed explicit role(s) (without finding
filler elements) as we believe that it serves as a
crucial preprocessing step and still bears great po-
tential for improvement. We treat NI identification
separately from the resolution of their fillers, also
because not all NIs are resolvable from the con-
text. In order to facilitate a more flexible mech-
anism, we propose to condition on the presence
of other roles, and primarily argue that NI de-
tection should be probabilistic instead of rule-
based. More specifically, we predict implicit ar-
guments using large corpora from which we build
a background knowledge base of predicates, co-
occurring (explicit) roles and their probabilities.
With such a memory-based approach, we gener-

2Core roles are obligatory arguments of a predicate. Infor-
mally, non-core roles are optional arguments often realized as
adjuncts or modifiers.

alize over large quantities of explicit roles to find
evidence for implicit information in a mildly su-
pervised manner. Our proposed models are largely
domain independent, include a sense distinction
for predicates, and are not bound to a specific re-
lease of a hand-maintained dictionary. Our ap-
proach is portable across languages in that train-
ing data can be created using projected SRL anno-
tations. Unlike most earlier approaches, we em-
ploy a generic role set which is based on Prop-
Bank/NomBank rather than FrameNet: The Prop-
Bank format comprises a relatively small role in-
ventory which is better suited to obtain statisti-
cal generalizations than the great variety of highly
specific FrameNet roles. While FrameNet roles
seem to be more fine-grained, their greater num-
ber arises mostly from predicate-specific semantic
roles, whose specific semantics can be recovered
from PropBank annotations by pairing semantic
roles with the predicate.

Yet another motivation of our work is related
to the recent development of AMR parsing (Ba-
narescu et al., 2013, Abstract Meaning Represen-
tation) which aims at modeling the semantic rep-
resentation of a sentence while abstracting from
syntactic idiosyncrasies. This particular appraoch
makes extensive use of the PropBank-style frame-
sets, as well, and would greatly benefit from the
integration of information on implicit roles.

The paper is structured as follows: Section 2
outlines related work in which we exclusively fo-
cus on how previous research has handled the
sole identification of NIs. Sect. 3 describes our
approach to probabilistic NI detection; Sect. 4
presents two experiments and their evaluation;
Sect. 5 concludes our work.

2 Related Work

In the context of the 2010 SemEval Shared Task
on Linking Events and Their Participants in Dis-
course3 on implicit argument resolution, Ruppen-
hofer et al. (2010) have released a data set of fic-
tion novels with manual NI role annotations for
diverse predicates. The data has been referred to
by various researchers in the community for di-
rect or indirect evaluation of their results. The
NIs in the data set are further subdivided into two
categories: Definite NIs (DNIs) are locally unex-
pressed arguments which can be resolved to ele-
ments in the proceeding or following discourse;

3http://semeval2.fbk.eu/semeval2.php

179



Indefinite NIs (INIs) are elements for which no an-
tecedent can be identified in the surrounding con-
text.4 Also, the evaluation data comes in two fla-
vors: a base format which is compliant with the
FrameNet paradigm and a CoNLL-based Prop-
Bank format. Previous research has exclusively
focused on the former.

Chen et al. (2010) present an extension of an ex-
isting FrameNet-style parser (SEMAFOR) to han-
dle implicit elements in text. The identification of
NIs is guided by the assumption that, whenever the
traditional SRL parser returns the default label in-
volved in a non-saturated analysis for a sentence,
an implicit role has to be found in the context in-
stead. Additional FrameNet-specific heuristics are
employed in which, e.g., the presence of one par-
ticular role in a frame makes the identification of
another implicit role redundant.5

Tonelli and Delmonte (2010, VENSES++)
present a deep semantic approach to NI resolu-
tion whose system-specific output is mapped to
FrameNet valency patterns. For the detection of
NIs, they assume that these are always core ar-
guments, i.e., non-omissible roles in the interac-
tion with a specific predicate. It is unclear how
different predicate senses are handled by their ap-
proach. Moreover, not all types of NIs can be de-
tected, resulting in a low overall recall of identi-
fied NIs, also having drawbacks for nouns. Again
using FrameNet-specific modeling assumptions,
their work has been significantly refined in Tonelli
and Delmonte (2011).

Despite their good performance in the overall
task, Silberer and Frank (2012, S&F) give a rather
vague explanation regarding NI identification in
text. Using a FrameNet API, the authors restrict
their analysis only to the core roles by exclud-
ing “conceptually redundant” roles without further
elaboration.

Laparra and Rigau (2013) propose a determinis-
tic algorithm to detect NIs on grounds of discourse
coherence: It predicts an NI for a predicate if the
corresponding role has been explicitly realized for
the same predicate in the preceding discourse but
is currently unfilled. Their approach is promising
but ignorant of INIs.

Earlier, Laparra and Rigau (2012, L&R) intro-
duce a statistical approach to identifying NIs sim-
ilar to ours in that they rely on frequencies from

4The average F-score annotator agreement for frame as-
signments is about .75 (Ruppenhofer et al., 2010).

5Cf. CoreSet and Exludes relationship in FrameNet.

overt arguments to predict implicit arguments. For
each predicate template (frame), their algorithm
computes all Frame Element patterns, i.e., all co-
occurring overt roles and their frequencies. For
NI identification a given predicate and its overtly
expressed roles are matched against the most fre-
quent pattern not violated by the explicit argu-
ments. Roles of the pattern which are not overtly
expressed in the text are predicted as missing NIs.
Even though their approach outperforms all pre-
vious results in terms of NI detection, Laparra
and Rigau (2012) only estimate the raw frequen-
cies from a very limited training corpus, raising
the question whether all patterns are actually suf-
ficiently robust. Also, the authors disregard all the
valuable less frequent patterns and limit their anal-
ysis to only a subtype of NI instances which are
resolvable from the context.

Finally, Gerber and Chai (2012) describe a su-
pervised model for implicit argument resolution
on the NomBank corpus which—unlike the pre-
vious literature—follows the PropBank annotation
format. However, NI detection is still done by dic-
tionary lookup, and the analysis is limited to only
a small set of predicates with only one unambigu-
ous sense. Again limiting NIs to only core roles,
the authors empirically demonstrate that this sim-
plification accounts for 8% of the overall error rate
of their system.

3 Experimental Setup

3.1 Memory-Based Learning

Memory-based learning for NLP (Daelemans and
van den Bosch, 2009) is a lazy learning technique
which keeps a record of training instances in the
form of a background knowledge base (BKB).
Classification compares new items directly to the
stored items in the BKB via a distance metric. In
semantics, the method has been applied by, e.g.,
Peñas and Hovy (2010) for semantic enrichment,
and Chiarcos (2012) to infer (implicit markers for)
discourse relations. Here, we adopt its methodol-
ogy to identify null-instantiated argument roles in
text. More precisely, we setup a BKB of proba-
blistic predicate-role co-occurrences and estimate
thresholds which serve as a trigger for the predic-
tion of an implicit role (a slight modification of the
distance metric). We elaborate on this methodol-
ogy in Section 4.

180



3.2 Data & Preprocessing

We train our model on a subset of the
WaCkypedia EN6 corpus (Baroni et al., 2009).
The data set provides a 2008 Wikipedia dump
from which we extracted the tokens and sentences.
We have further divided the dump into pieces of
growing size (cumulatively by 100 sentences) and
applied MATE7 (Björkelund et al., 2009) for the
automatic detection of semantic roles to the vary-
ing portions and annotated them with SRL infor-
mation. For each sentence, MATE identifies the
predicates and all of its associated core and non-
core arguments.8 MATE has been used in previ-
ous research on implicit elements in text (Roth and
Frank, 2013) and provides semantic roles with a
sense disambiguation for both verbal and nominal
predicates. The resulting output is based on the
PropBank format.

3.3 Model Generation

We build a probablistic model from annotated
predicate-role co-occurrences as follows:
1. For every sentence, record all distinct predicate

instances and their associated roles.
2. For every predicate instance, sort the role labels

lexicographically (not the role fillers), disre-
garding their sequential order. (We thus obtain
a normalized template of role co-occurrences
for each frame instantiation.)

3. Compute the frequencies for all templates asso-
ciated with the same predicate.

4. By relative frequency estimation, derive all
conditional probabilities of the form:

P (r|R, PREDICATE)

with R being the role inventory of the SRL
parser, R ⊆ R a (sub)set of explicitly realized
semantic roles, and r ∈ R \ R an arbitrary se-
mantic role. When we try to gather information
on null instantiated roles, r is typically an un-
realized role label. The PREDICATE consists of
the lemma of the corresponding verb or noun,
optionally followed by sense number (if pred-
icates are sense-disambiguated) and its part of
speech (V/N), e.g., PLAY.01.N.

6http://wacky.sslmit.unibo.it/doku.php?id=corpora
7http://code.google.com/p/mate-tools/
8In order to minimize the noise in the data, we attempted

to resplit unrealistically long sentences (> 90 tokens) by
means of the Stanford Core NLP module (Manning et al.,
2014). All resulting splits > 70 tokens were rejected.

Paradigm #Roles #Overt
Overt DNI INI #DNI+#INI

Train FrameNet 2,526 303 277 4.36
PropBank 1,027 125 101 4.52

Test FrameNet 3,141 349 361 4.42
PropBank 1,332 167 85 5.28

Table 1: Label distribution of the SemEval 2010 data set for
overt and null instantiated arguments for both the FrameNet
(all roles and parts of speech) and the PropBank version (only
core roles for nouns and verbs).

We build models from SRL data in PropBank for-
mat, both manually and automatically annotated.
We experiment with models for two different
styles of predicates: Sense-ignorant or SI models
represent predicates by lemma and part of speech
(PLAY.N), sense-disambiguated or SD models rep-
resent predicates by lemma, sense number and part
of speech (PLAY.01.N, PLAY.02.N, etc.).

3.4 Annotated Data

In accordance with previous iSRL studies, we
evaluate our model on the SemEval data set (Rup-
penhofer et al., 2010). However, to the best of our
knowledge, this is the first study to focus on the
PropBank version of this data set. It has been de-
rived semi-automatically from the FrameNet base
format using hand-crafted mapping rules (as part
of the data set) for both verbs and nouns. For
example, a conversion for the predicate fear in
FrameNet’s EXPERIENCER FOCUS frame is de-
fined as fear.01 (its first sense) with the roles EX-
PERIENCER and CONTENT mapped to PropBank
labels A0 and A1, respectively. In accordance
with the mapping patterns, the resulting distribu-
tion of NIs varies slightly from the base format.
Table 1 shows the label distribution of overt roles,
DNIs, INIs for both the FrameNet and PropBank
versions, respectively. Some information is lost
while the general proportions remain similar to the
base format. This is also due to the fact that for
some parts of speech (e.g., for adjectives) no map-
pings are defined, even though some of them are
annotated with NI information in the FrameNet
version. Moreover, mapping rules exist only for
core roles A0-A4 (agent, patient, . . . ). As a con-
sequence, we restrict our analysis to these five
(unique) roles, even though our models described
in this work incorporate probabilistic information
for all possible roles in R, i.e., A0-A4, but also
for non-core (modifier) roles, such as AM-TEMP
(temporal), AM-LOC (location), etc.

181



Role Verbs Nouns
Overt NIs Overt NIs

A0 40 45 24 23
A1 83 39 29 33
A2 3 11 10 6
A3 - 7 - 1
A4 - 24 - -

totals: 126 126 63 63

Table 2: Label distributions of all roles in both data sets
from Experiment 1; majority NI classes in bold.

4 Experiments

4.1 Experiment 1

To evaluate the general usefulness of our memory-
based approach to detect implicit roles, we set up
a simplified framework for predicates with exactly
one overt argument and one NI annotated in the
SemEval data (for all verbs and all nouns and from
both the train and test files to obtain a reasonably
large sample; no differentiation of DNIs and INIs).
This pattern accounts for 189 instances—roughly
9% of the data samples in the SemEval set. We di-
vided the instances into two subsets based on the
predicate’s part of speech. The label distributions
over overt and null instantiated roles for both ver-
bal and nominal predicates are given in Table 2.

4.1.1 Task Description
Predict the role of the single missing NI (A0–A4)
for each given predicate instance.

4.1.2 Predicting Null Instantiations
We trained one sense-disambiguated (SD) gold
model for verbs (PB) and one for nouns (NB) ac-
cording to Sect. 3.3 on the complete PropBank
and the complete NomBank, respectively. This
was compared with 30 separate SD and SI models
on varying portions of the automatically annotated
WaCkypedia EN dump: These were trained on the
first k sentences each, in order to make their pre-
diction quality comparable, while k ranges from
50 sentences for the smallest model to k = 10
million for the largest model (≈ 15 of the whole
corpus). For NI role prediction, we return ni, i.e.,
the maximally probable unrealized semantic role
given the overt argument oj plus the predicate:

ni = arg max
n∈R\R

P (n|oj , PREDICATE),

where R = {oj}, the predicate’s single explicit
role andR = {A0..A4} ⊃ R, the role inventory.

4.1.3 Results & Evaluation
The prediction accuracies for verbal and nominal
predicates are illustrated in Figure 1. Although the
number of instances in the data sets is small, some
general trends are clearly visible. Our major find-
ings are:

By increasing the number of training sentences
the performance of the SD and the SI-based clas-
sification models steadily increases as well. The
trend is the same for both verbs and for nouns,
even though training in the nominal domain re-
quires more data to obtain similarly good results.
More precisely, models trained on only 50k sen-
tences already have an adequate performance on
test data for verbs (≈76% with the SD model). To
reach a similar performance on nouns, we need to
increase the training size roughly by a factor of 5.

Likewise, the performance of the SD models is
better in general than the one of the SI models
throughout all models analyzing verbal predicates,
but only marginally better for nouns.

Both the SD and the SI models outperform the
majority class baseline for both parts of speech.9

Also, with 800k sentences for nouns and only
50k sentences for verbs, both SD model types
reach accuracies equal to or greater than the super-
vised PB and NB (gold) models which have been
trained on the complete PropBank and NomBank
corpus including sense distinctions, respectively.

The classification accuracies for the SD models
reach their saturated maxima for verbs at around
91.27% (115/126) with 6 million training sen-
tences and 85.71% (54/63) with 2.85 million sen-
tences for nouns. For verbs, a χ2 test confirms
a significant (p < .01) improvement of our best
model over the PB gold model. On the sparse eval-
uation data for nouns, the improvement over the
NB gold model is, however, not significant.

Taken together, the improvements confirm that
memory-based learning over mass data of auto-
matically annotated (explicit) semantic roles can
actually outperform gold models constructed from
corpora with manual SRL annotations, even if the
tools for automated mass annotation were trained
on the very same corpora used to build the gold
models (PropBank, NomBank). Also, the exper-
iment demonstrated the feasibility of predicting
implicit roles solely using information about the
distribution of explicit roles. For the artificially

935.71% with only 1k training sentences (verbs), 52.38%
with 50k sentences (nouns).

182



0.0

0.1

0.2

0.3

0.4

0.5

0.6

0.7

0.8

0.9

     0.1k       1k 1,000k10k 100k
# training sentences

a
cc

u
ra

cy

classifier
MC

PB

SD

SI

MC

PB
SI

SD

NB

a
cc

u
ra

cy

SD
SI

MC

# training sentences

0.1k 1k 1,000k10k 100k

0.0

0.1

0.2

0.3

0.4

0.5

0.6

0.7

0.8

Figure 1: Prediction accuracies for verbal (left figure) and nominal predicates (right figure) from Experiment 1. Majority
class (MC) baselines in red, PropBank (PB) and NomBank (NB) gold models in green. The log-scaled x-axis only refers to the
SD and SI models and indicates first k sentences used for training.

simplified NI patterns in Experiment 1, already
small portions of automatically annotated SRL
data are sufficient to yield adequate results for both
types (DNIs and INIs). Sense disambiguation of
predicates generally increases the performance.10

4.2 Experiment 2

The setup from the previous experiment is by far
too simplistic compared to a real linguistic sce-
nario. Usually, a predicate can have an arbitrary
number of overt arguments, and similarly the num-
ber of missing NIs varies. To tackle this problem,
we take the original train and test split (744 vs.
929 unrestricted frame instances of the form: any
combination of overt roles vs. any combination of
NI roles per predicate). Again, we do not draw a
distinction between DNIs and INIs, but treat them
generally as NIs. Table 3 shows the distribution of
the different NI role patterns in the test data.

4.2.1 Task Description

Given a predicate and its overtly expressed argu-
ments (ranging from any combination of A0 to A4
or none), predict the correct set of null instantia-
tions (which can also be empty or contain up to
five different implicit elements).

10A simple error analysis of the misclassified noun in-
stances revealed that classification on the test data suffers
from sparsity issues: In the portions of the WaCkypedia EN
that we used for model building, three predicates were not
attested (twice murder.01 and once murderer.01). This has a
considerable impact on test results.

NI Pattern Freq NI Pattern Freq
- 706 A0 A2 7

A1 86 A1 A2 6
A0 51 A3 5
A2 35 A1 A4 3
A4 18 A0 A1 A2 1

A0 A1 11

Table 3: The 929 NI role patterns from the test set sorted
by their number of occurrence. Most of the predicates are
saturated and do not seek an implicit argument. Only one
predicate instance has three implicit roles.

4.2.2 Predicting Null Instantiations
We distinguish two main types of classifiers: su-
pervised classifiers are directly obtained from NI
annotations in the SemEval training data, mildly
supervised classifiers instead use only information
about (automatically obtained) explicitly realized
semantic roles in a given corpus, hybrid classifiers
combine both sources of information. We esti-
mated all parameters optimizing F-measure on the
train section of the SemEval data set. Their perfor-
mance is evaluated on its test section. We aim to
demonstrate that mildly supervised classifiers are
capable of predicting implicit roles, and to study
whether NI annotations can be used to improve
their performance.
Baseline: Given the diversity of possible patterns,
it is hard to decide how a suitable and competitive
baseline should be defined: predicting the majority
class means not to predict anything. So, instead,
we predict implicit argument roles randomly, but
in a way that emulates their frequency distribu-
tion in the SemEval data (cf. Tab. 3), i.e., predict

183



Classifier A B1 B2 C0 C1 C2 C3 C4 C4n,v C4n,v,B1 C4n,v,B2
Precision 0.149 0.848 0.853 0.368 0.378 0.398 0.400 0.400 0.423 0.561 0.582
Recall 0.075 0.155 0.206 0.861 0.851 0.837 0.837 0.837 0.782 0.615 0.814
F1 Score 0.100 0.262 0.332 0.516 0.523 0.540 0.541 0.541 0.549 0.589 0.679

Table 4: Precision, recall and F1 scores for all classifiers introduced in Experiment 2. Scores are compared row-wise to the
best-performing classifier C4n,v,B2 . A significant improvement over a cell entry with p < .05 is indicated in italics.

no NIs with a probability of 76.0% (706/929), A1
with 38.6% (86/929), etc. The baseline scores are
averaged over 100 runs of this random ‘classifier’,
further referred to as A.
Supervised classifier: Supervised classifiers, as
understood here, are classifiers that use the in-
formation obtained from manual NI annotations.
We set up two predictors B1 and B2 tuned on the
SemEval training set: B1 is obtained by count-
ing for each predicate its most frequent NI role
pattern. For instance, for seem.02—once anno-
tated with implicit A1, but twice without implicit
arguments—B1 would predict an empty set of
NIs. B2 is similar toB1 but conditions NI role pat-
terns not only on the predicate, but also on its ex-
plicit arguments.11 For prediction, these classifiers
consult the most frequent NI pattern observed for
a predicate (B2: plus its overt arguments). If a test
predicate is unknown (i.e., not present in the train-
ing data), we predict the majority class (empty set)
for NI.
Mildly supervised classifier: Mildly supervised
classifiers do not take any NI annotation into ac-
count. Instead, they rely on explicitly realized
semantic roles observed in a corpus, but use ex-
plicit NI annotations only to estimate prediction
thresholds. We describe an extension of our pre-
diction method from Exp. 1 and present eight
parameter-based classification algorithms for our
best-performing SD model from Exp. 1, trained
on 6 million sentences.

We define prediction for classifier C0 as fol-
lows: Given a predicate PREDICATE, the role in-
ventory R = {A0..A4}, its (possibly empty) set
of overt roles R ⊆ R and a fixed, predicate-
independent threshold t0. We start by optimiz-
ing threshold t0 on all predicate instances with no
given overt argument. If there is no overt role and
an unrealized role ni ∈ R for which it is true that

11Specifically, we extract finer-grained patterns, e.g.,
evening.01[A1] → {}=2, {A2}=3, where a predicate is as-
sociated with its overt role(s) (left side of the arrow). The
corresponding implicit role patterns and their number of oc-
currence is shown to the right.

P (ni|PREDICATE) > t0, then predict ni as an im-
plicit role. If there is an overt role oj ∈ R and an
unrealized role ni ∈ R\R for which it is true that
P (ni|oj ,PREDICATE) > t0, then predict ni as an
implicit role. Note that C0 requires that this condi-
tion to hold for one oj , not all explicit arguments
of the predicate instance (logical disjunction).

We refine this classifier by introducing an
additional parameter that accounts for the
group of overtly realized frames with exactly
one overt argument, i.e., C1 predicts ni if
P (ni|oj ,PREDICATE) > t1; for all other configu-
rations the procedure is the same as in C0, i.e., the
threshold t0 is applied.

Classifiers C2, C3 and C4 extend C1 ac-
cordingly and introduce additional thresholds t2,
t3, t4 for the respective number of overt ar-
guments. For example, C3 predicts ni if
P (ni|oj1 , oj2 , oj3 ,PREDICATE) > t3, for config-
urations with less arguments, it relies on C2, etc.
Our general intuition here is to see whether the in-
creasing number of specialized parameters for in-
creasingly marginal groups of frames is justified
by the improvements we achieve in this way.

A final classifier C4n,v extends C4 by distin-
guishing verbal and nominal predicates, yielding
a total of ten parameters t0n ..t4n , t0v ..t0n .
Hybrid classifier: To explore to what extent ex-
plicit NI annotations improve the classification re-
sults, we combine the best-performing and most
elaborate mildly supervised classifier C4n,v with
the supervised classifiers B1 and B2: For pred-
icates encountered in the training data, C4n,v,B1
(resp., C4n,v,B2 ) uses B1 (resp., B2) to predict the
most frequent pattern observed for the predicate;
for unknown predicates, apply the threshold-based
procedure of C4n,v .

4.2.3 Results & Evaluation

Table 4 contains the evaluation scores for the in-
dividual parameter-based classifiers. All classi-
fiers demonstrate significant improvements over
the random baseline. Also the mildly supervised

184



classifiers outperform the supervised algorithms in
terms of F1 score and recall. However, detecting
NIs by the supervised classifiers is very accurate in
terms of high precision. Classifier B2 outperforms
B1 as a result of directly incorporating additional
information about the overt arguments.

Concerning our parameter-based classifiers, the
main observations are: First, the overall perfor-
mance (F1 score) increases from C0 to C4 (yet
not significantly). Secondly, with more param-
eters, recall decreases while precision increases.
We can observe, however, that improvements from
C2 to C4 are marginal, at best, due to the spar-
sity of predicates with two or more overt argu-
ments. Similar problems related to data sparsity
have been reported in Chen et al. (2010). Results
for C3 and C4 are identical, as no predicate with
more than three overt arguments occurred in the
test data. Encoding the distinction between ver-
bal and nominal predicates into the classifier again
slightly increases the performance.

A combination of the high-precision supervised
classifiers and the best performing mildly super-
vised algorithm yields a significant boost in per-
formance (Tab. 4, last two columns). The optimal
parameter values for all classifiers C4n,v estimated
on the train section of the SemEval data set are
given in Table 5.

Noun thresholds tC0n tC1n tC2n tC3n tC4n
Values 0.35 0.10 0.20 0.35 0.45

Verb thresholds tC0v tC1v tC2v tC3v tC4v
Values 0.05 0.25 0.25 0.30 0.20

Table 5: Optimal parameter values for the thresholds in
all C4n,v classifiers estimated on the train section of the
SemEval data set.

In Table 6, we report the performance of our
best classifier C4n,v,B2 with detailed label scores.
Its overall NI recognition rate of 0.81 (recall) out-
performs the state-of-the-art in implicit role identi-
fication: cf. L&P (0.66), SEMAFOR (0.63), S&F
(0.58), T&D (0.54), VENSES++ (0.08).12

Summarizing our results, Exp. 2 has shown
that combining supervised and mildly supervised
strategies to NI detection achieves the best re-
sults on the SemEval test set. Concerning the
mildly supervised, parameter-based classifiers, it

12Note that only an indirect comparison of these scores is
possible due to the aforementioned difference between data
formats and also because none of the other systems report
precision scores for their pattern-based NI detection systems.

Roles A0 A1 A2 A3 A4
# Labels 70 107 49 5 21

Precision 0.675 0.578 0.432 0.400 0.791

Recall 0.800 0.897 0.653 0.400 0.905

F1 Score 0.732 0.703 0.520 0.400 0.844

Table 6: Evaluation of C4n,v,B2 for all 252 implicit roles.

has proven beneficial to incorporate a maximum
of available information on overtly expressed ar-
guments in order to determine implicit roles. Our
best-performing classifier achieves NI recognition
rate beyond state-of-the-art.

Interestingly, memory-based learning offers the
capability to detect both DNIs (resolvable from
context), as well as INIs (not resolvable from con-
text), simply by learning patterns from local ex-
plicit role realizations. Subsequent experiments
should extend this approach to distinguish be-
tween the two types, as well, which we have
treated equivalently in our settings. First promis-
ing experiments in this direction are being con-
ducted in Chiarcos and Schenk (2015).

5 Summary and Outlook

We have presented a novel, statistical method to
infer evidence for implicit roles from their explicit
realizations in large amounts of automatically an-
notated SRL data. We conclude that—especially
when annotated training data is sparse—memory-
based approaches to implicit role detection seem
highly promising. With a much greater degree
of flexibility, they offer an alternative solution to
static rule-/template-based methods.

Despite its simplicity, we demonstrated the suit-
ability of our approach: It is competitive with
state-of-the-art systems in terms of the overall
recognition rate, however, still suffers in preci-
sion of the respective null instantiated arguments.
Thus, directions for future research should con-
sider integrating additional contextual features,
and would benefit from the complete role inven-
tory of our models (including non-core roles). In
this extended setting, we would like to experiment
with other machine learning approaches to assess
whether the accuracy of the detected NIs can be
increased. Also, we plan to apply the memory-
based strategy described in this paper to NI reso-
lution (on top their detection), and in this context,
examine more closely the characteristic (possibly
contrastive) distributions of DNIs and INIs.

185



References

Collin F. Baker, Charles J. Fillmore, and John B. Lowe.
1998. The Berkeley FrameNet Project. In Pro-
ceedings of the 36th Annual Meeting of the Associ-
ation for Computational Linguistics and 17th Inter-
national Conference on Computational Linguistics -
Volume 1, ACL ’98, pages 86–90, Stroudsburg, PA,
USA. Association for Computational Linguistics.

Laura Banarescu, Claire Bonial, Shu Cai, Madalina
Georgescu, Kira Griffitt, Ulf Hermjakob, Kevin
Knight, Philipp Koehn, Martha Palmer, and Nathan
Schneider. 2013. Abstract Meaning Representation
for Sembanking. Proc. Linguistic Annotation Work-
shop.

Marco Baroni, Silvia Bernardini, Adriano Ferraresi,
and Eros Zanchetta. 2009. The WaCky Wide Web:
A Collection of Very Large Linguistically Processed
Web-Crawled Corpora. Language Resources and
Evaluation, 43(3):209–226.

Anders Björkelund, Love Hafdell, and Pierre Nugues.
2009. Multilingual Semantic Role Labeling. In Pro-
ceedings of the Thirteenth Conference on Computa-
tional Natural Language Learning (CoNLL 2009):
Shared Task, pages 43–48, Boulder, Colorado, June.
Association for Computational Linguistics.

Desai Chen, Nathan Schneider, Dipanjan Das, and
Noah A. Smith. 2010. SEMAFOR: Frame Argu-
ment Resolution with Log-linear Models. In Pro-
ceedings of the 5th International Workshop on Se-
mantic Evaluation, SemEval ’10, pages 264–267,
Stroudsburg, PA, USA. Association for Computa-
tional Linguistics.

Christian Chiarcos and Niko Schenk. 2015. (accepted)
Towards the Unsupervised Acquisition of Implicit
Semantic Roles. In Recent Advances in Natu-
ral Language Processing, RANLP 2015, September,
2015, Hissar, Bulgaria.

Christian Chiarcos. 2012. Towards the Unsupervised
Acquisition of Discourse Relations. In Proceedings
of the 50th Annual Meeting of the Association for
Computational Linguistics: Short Papers - Volume
2, ACL ’12, pages 213–217, Stroudsburg, PA, USA.
Association for Computational Linguistics.

Walter Daelemans and Antal van den Bosch. 2009.
Memory-Based Language Processing. Cambridge
University Press, New York, NY, USA, 1st edition.

Parvin Sadat Feizabadi and Sebastian Padó. 2015.
Combining Seemingly Incompatible Corpora for
Implicit Semantic Role Labeling. In Proceedings of
STARSEM, pages 40–50, Denver, CO.

Charles J. Fillmore. 1986. Pragmatically Controlled
Zero Anaphora. In Proceedings of Berkeley Linguis-
tics Society, pages 95–107, Berkeley, CA.

Matthew Gerber and Joyce Chai. 2012. Semantic Role
Labeling of Implicit Arguments for Nominal Pred-
icates. Comput. Linguist., 38(4):755–798, Decem-
ber.

Egoitz Laparra and German Rigau. 2012. Exploiting
Explicit Annotations and Semantic Types for Im-
plicit Argument Resolution. In Sixth IEEE Inter-
national Conference on Semantic Computing, ICSC
2012., Palermo, Italy. IEEE Computer Society.

Egoitz Laparra and German Rigau. 2013. ImpAr: A
Deterministic Algorithm for Implicit Semantic Role
Labelling. In Proceedings of the 51st Annual Meet-
ing of the Association for Computational Linguistics
(Volume 1: Long Papers), pages 1180–1189. Asso-
ciation for Computational Linguistics.

Christopher D. Manning, Mihai Surdeanu, John Bauer,
Jenny Finkel, Steven J. Bethard, and David Mc-
Closky. 2014. The Stanford CoreNLP natural lan-
guage processing toolkit. In Proceedings of 52nd
Annual Meeting of the Association for Computa-
tional Linguistics: System Demonstrations, pages
55–60.

Adam Meyers, Ruth Reeves, Catherine Macleod,
Rachel Szekely, Veronika Zielinska, Brian Young,
and Ralph Grishman. 2004. The NomBank Project:
An Interim Report. In A. Meyers, editor, HLT-
NAACL 2004 Workshop: Frontiers in Corpus Anno-
tation, pages 24–31, Boston, Massachusetts, USA,
May 2 - May 7. Association for Computational Lin-
guistics.

Martha Palmer, Daniel Gildea, and Paul Kingsbury.
2005. The Proposition Bank: An Annotated Corpus
of Semantic Roles. Comput. Linguist., 31(1):71–
106, March.

Anselmo Peñas and Eduard Hovy. 2010. Semantic
Enrichment of Text with Background Knowledge.
In Proceedings of the NAACL HLT 2010 First Inter-
national Workshop on Formalisms and Methodology
for Learning by Reading, FAM-LbR ’10, pages 15–
23, Stroudsburg, PA, USA. Association for Compu-
tational Linguistics.

Michael Roth and Anette Frank. 2013. Automati-
cally Identifying Implicit Arguments to Improve Ar-
gument Linking and Coherence Modeling. In Pro-
ceedings of the Second Joint Conference on Lexical
and Computational Semantics (*SEM), pages 306–
316, Atlanta, Georgia, USA, June. Association for
Computational Linguistics.

Josef Ruppenhofer, Caroline Sporleder, Roser
Morante, Collin Baker, and Martha Palmer. 2010.
SemEval-2010 Task 10: Linking Events and Their
Participants in Discourse. In Proceedings of the 5th
International Workshop on Semantic Evaluation,
SemEval ’10, pages 45–50, Stroudsburg, PA, USA.
Association for Computational Linguistics.

186



Josef Ruppenhofer, Philip Gorinski, and Caroline
Sporleder. 2011. In Search of Missing Arguments:
A Linguistic Approach. In Galia Angelova, Kalina
Bontcheva, Ruslan Mitkov, and Nicolas Nicolov, ed-
itors, RANLP, pages 331–338. RANLP 2011 Organ-
ising Committee.

Josef Ruppenhofer. 2005. Regularities in Null Instan-
tiation. Ms, University of Colorado.

Carina Silberer and Anette Frank. 2012. Casting Im-
plicit Role Linking as an Anaphora Resolution Task.
In Proceedings of the First Joint Conference on Lex-
ical and Computational Semantics - Volume 1: Pro-
ceedings of the Main Conference and the Shared
Task, and Volume 2: Proceedings of the Sixth In-
ternational Workshop on Semantic Evaluation, Se-
mEval ’12, pages 1–10, Stroudsburg, PA, USA. As-
sociation for Computational Linguistics.

Sara Tonelli and Rodolfo Delmonte. 2010.
VENSES++: Adapting a Deep Semantic Processing
System to the Identification of Null Instantiations.
In Proceedings of the 5th International Workshop on
Semantic Evaluation, pages 296–299. Association
for Computational Linguistics.

Sara Tonelli and Rodolfo Delmonte. 2011. Desper-
ately Seeking Implicit Arguments in Text. In Pro-
ceedings of the ACL 2011 Workshop on Relational
Models of Semantics, pages 54–62. Association for
Computational Linguistics.

187


