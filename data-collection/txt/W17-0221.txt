



















































Evaluation of language identification methods using 285 languages


Proceedings of the 21st Nordic Conference of Computational Linguistics, pages 183–191,
Gothenburg, Sweden, 23-24 May 2017. c©2017 Linköping University Electronic Press

Evaluation of language identification methods using 285 languages

Tommi Jauhiainen
University of Helsinki
@helsinki.fi

Krister Lindén
University of Helsinki
@helsinki.fi

Heidi Jauhiainen
University of Helsinki
@helsinki.fi

Abstract

Language identification is the task of giv-
ing a language label to a text. It is an im-
portant preprocessing step in many auto-
matic systems operating with written text.
In this paper, we present the evaluation of
seven language identification methods that
was done in tests between 285 languages
with an out-of-domain test set. The evalu-
ated methods are, furthermore, described
using unified notation. We show that a
method performing well with a small num-
ber of languages does not necessarily scale
to a large number of languages. The HeLI
method performs best on test lengths of
over 25 characters, obtaining an F1-score
of 99.5 already at 60 characters.

1 Introduction

Automatic language identification of text has been
researched since the 1960s. Language identifica-
tion is an important preprocessing step in many au-
tomatic systems operating with written text. State
of the art language identifiers obtain high rates
in both recall and precision. However, even the
best language identifiers do not give perfect results
when dealing with a large number of languages,
out-of-domain texts, or short texts. In this paper
seven language identification methods are evalu-
ated in tests incorporating all three of these hard
contexts. The evaluations were done as part of the
Finno-Ugric Languages and The Internet project
(Jauhiainen et al., 2015) funded by the Kone Foun-
dation Language Programme (Kone Foundation,
2012). One of the major goals of the project is
creating text corpora for the minority languages
within the Uralic group.

In Section 2, we describe the methods chosen
for this evaluation. In Section 3, we present the
corpora used for training and testing the methods

and in Section 4 we discuss and present the results
of the evaluations of the methods using these cor-
pora.

2 Previous work

There are not many previously published arti-
cles which provide language identification results
for more than 100 languages. Results for such
evaluations were provided by King and Dehdari
(2008), Jauhiainen (2010), Vatanen et al. (2010),
Rodrigues (2012), and Brown (2012). King and
Dehdari (2008) achieved 99% accuracy with 500
bytes of input for over 300 languages. Vatanen et
al. (2010) created a language identifier which in-
cluded 281 languages and obtained an in-domain
identification accuracy of 62.8% for extremely
short samples (5-9 characters). Rodrigues (2012)
presents a boosting method using the method of
Vatanen et al. (2010) for language identification.
His method could possibly also be used with other
language identification methods and we leave the
evaluation of the boosting method to future work.
The language identifier created by Brown (2012),
”whatlang”, obtains 99.2% classification accuracy
with smoothing for 65 character test strings when
distinguishing between 1,100 languages (Brown,
2013; Brown, 2014).

The HeLI method described in Jauhiainen et
al. (2016) was used successfully with 103 lan-
guages by Jauhiainen (2010). Some of the more
detailed results concerning the Uralic languages
for the evaluations presented in this paper were
previously published by Jauhiainen et al. (2015).

In this section, we also include the original
method of Cavnar and Trenkle (1994), as it is
the most frequently used baseline in the language
identification literature. As baselines, we have
also included the methods presented by Tromp and
Pechenizkiy (2011) and Vogel and Tresner-Kirsch
(2012), which provided promising results when
used with 6 languages.

183



2.1 On notation (Jauhiainen et al., 2016)

A corpus C consists of individual tokens u which
may be words or characters. A corpus C is a finite
sequence of individual tokens, u1, ...,ul . The total
count of all individual tokens u in the corpus C is
denoted by lC. A set of unique tokens in a corpus
C is denoted by U(C). The number of unique to-
kens is referred to as |U(C)|. A feature f is some
countable characteristic of the corpus C. When re-
ferring to all features F in a corpus C, we use CF

and the count of all features is denoted by lCF . The
count of a feature f in the corpus C is referred to as
c(C, f ). An n-gram is a feature which consists of a
sequence of n individual tokens. An n-gram start-
ing at position i in a corpus is denoted ui,...,i−1+n.
If n = 1, u is an individual token. When referring
to all n-grams of length n in a corpus C, we use
Cn and the count of all such n-grams is denoted
by lCn . The count of an n-gram u in a corpus C is
referred to as c(C,u) and is defined by Equation 1.

c(C,u) =
lC+1−n

∑
i=1

{
1 , if u = ui,...,i−1+n
0 , otherwise (1)

The set of languages is G, and lG denotes the
number of languages. A corpus C in language g
is denoted by Cg. A language model O based on
Cg is denoted by O(Cg). The features given values
by the model O(Cg) are the domain dom(O(Cg))
of the model. In a language model, a value v for
the feature f is denoted by vCg( f ). A corpus in an
unknown language is referred to as a mystery text
M. For each potential language g of a corpus M, a
resulting score R(g,M) is calculated.

2.2 N-Gram-Based Text Categorization

The method of Cavnar and Trenkle (1994) uses
overlapping character n-grams of varying size cal-
culated from words. The language models are cre-
ated by tokenizing the training texts for each lan-
guage g into words and then padding each word
with spaces, one before and four after. Each
padded word is then divided into overlapping char-
acter n-grams of sizes from 1 to 5 and the counts of
every unique n-gram are calculated over the whole
corpus. The n-grams are ordered by frequency and
k of the most frequent n-grams, u1, ...,uk, are used
as the domain of the language model O(Cg) for
the language g. The rank of an n-gram u in lan-
guage g is determined by the n-gram frequency in
the training corpus Cg and denoted rankCg(u).

During language identification, the mystery text
is treated in a similar way and a corresponding
model O(M) of the k most frequent n-grams is
created. Then a distance score is calculated be-
tween the model of the mystery text and each of
the language models. The value vCg(u) is calcu-
lated as the difference in ranks between rankM(u)
and rankCg(u) of the n-gram u in the domain
dom(O(M)) of the model of the mystery text. If an
n-gram is not found in a language model, a special
penalty value p is added to the total score of the
language for each missing n-gram. The penalty
value should be higher than the maximum possi-
ble distance between ranks. We use p = k+ 1, as
the penalty value.

vCg(u) =
{
|rankM(u)− rankCg(u)|, if u ∈ dom(O(Cg))
p, if u /∈ dom(O(Cg))

(2)

The score Rsum(g,M) for each language g is the
sum of values as in Equation 3.

Rsum(g,M) =
lMF

∑
i=1

vCg( fi) (3)

The language having the lowest score
Rsum(g,M) is selected as the identified lan-
guage.

2.3 LIGA-algorithm
The graph-based n-gram approach called LIGA
was first described in (Tromp, 2011). The method
is here reproduced as explained in (Vogel and
Tresner-Kirsch, 2012). The language models con-
sist of relative frequencies of character trigrams
and the relative frequencies of two consecutive
overlapping trigrams. The frequency of two con-
secutive overlapping trigrams is exactly the same
as the 4-gram starting from the beginning of the
first trigram. So the language models consist of
the relative frequencies vCg(u) of 3- and 4-grams
as in Equation 4.

vCg(u) =
c(Cg,u)

lCng
(4)

where c(Cg,u), is the number of 3- or 4-grams u
and lCng , is the total number of 3- and 4-grams in
the training corpus.

The mystery text M is scanned for the 3- and
4-grams u. For each 3- and 4-gram found in the
model of a language g, the relative frequencies are
added to the score Rsum(g,M) of the language g, as
in Equation 3. The winner is the language with the

184



highest score as opposed to the lowest score with
the previous method.

In the logLIGA variation of the method, intro-
duced by Vogel and Tresner-Kirsch (2012), the
natural logarithm of the frequencies is used when
calculating the relative frequencies, as in Equa-
tion 5.

vCg(u) =
ln(c(Cg,u))

ln(lCng )
(5)

Otherwise the method is identical to the original
LIGA algorithm.

2.4 The method of King and Dehdari (2008)
King and Dehdari (2008) tested the use of the rela-
tive frequencies of byte n-grams with Laplace and
Lidstone smoothings in distinguishing between
312 languages. They separately tested overlap-
ping 2-, 3-, and 4-grams with both smoothing tech-
niques. They used the Universal Declaration of
Human Rights corpus, which is accessible using
NLTK (Bird, 2006), separating the testing mate-
rial before training. The values for each n-gram
are calculated as in Equation 6,

vCg(u) =
c(Cg,u)+λ

lCng + |U(Cng)|λ
(6)

where vCg(u) is the probability estimate of n-gram
u in the model and c(Cg,u) its frequency in the
training corpus. lCng is the total number of n-
grams of length n and |U(Cng)| the number of dis-
tinct n-grams in the training corpus. λ is the Lid-
stone smoothing parameter. When using Laplace
smoothing, the λ is equal to 1 and with Lidstone
smoothing, the λ is usually set between 0 and
1. King and Dehdari (2008) found that Laplace
smoothing with the bigram model turned out to
be the most accurate on two of their longer test
sets and that Lidstone smoothing (with λ set to
0.5) was better with the shortest test set. King
and Dehdari (2008) used the entropy(model, text)
function of NLTK, which evaluates the entropy be-
tween text and model by summing up the log prob-
abilities of words found in the text.1

2.5 ”Whatlang” program (Brown, 2013)
The ”Whatlang” program uses variable length byte
n-grams from 3 to 12 bytes as its language model.
K of the most frequent n-grams are extracted from

1Jon Dehdari recently uploaded the instruc-
tions to: https://github.com/jonsafari/
witch-language

training corpora for each language and their rel-
ative frequencies are calculated. In the tests re-
ported in (Brown, 2013), K varied from 200 to
3,500 n-grams. After the initial models are gen-
erated, n-grams, which are substrings of longer n-
grams in the same model, are filtered out, if the
frequency of the longer n-gram is at least 62% of
the shorter n-grams frequency. The value vCg(u)
of an n-gram u in the model of the corpus Cg is
calculated as in Equation 7

vCg(u) =
(c(Cg,u)

lCng

)0.27
n0.09 (7)

where c(Cg,u) is the frequency of the n-gram u
and lCng is the number of all n-grams of the length
n in the training corpus Cg. The weights in the
model are calculated so that the longer n-grams
have greater weights than short ones with the
same relative frequency. Baseline language mod-
els Obase(Cg) are formed for each language g using
the values vCg(u).

For each language model Obase(Cg), the cosine
similarity between it and every other language
model is calculated. A union of n-grams is formed
by taking all of the models for which the sim-
ilarity is higher than an empirically determined
threshold. The corpus Cg is scanned for occur-
rences of the n-grams in the union. If some of
the n-grams are not found at all, these n-grams
are then appended with negative weights to the
base model. The negative weight used for an n-
gram u in the model O(Cg) is the maximum co-
sine similarity between Obase(Cg) and the models
containing an n-gram u times the maximum vC(u)
within those models. These negative weighted n-
grams are called stop-grams. If the size of the
training corpus for a certain model is less than 2
million bytes, the weights of the stop-grams are
discounted as a function of the corpus size.

The score Rwhatlang(g,M) for the language g is
calculated as in Equation 8

Rwhatlang(g,M) =
∑i vCg(ui)

lM1
(8)

where ui are the n-grams found in the mystery text
M. The score is also normalized by dividing it
with the length (in characters) of the mystery text
lM1 . The language with the highest score is identi-
fied as the language of the mystery text.

Brown (2013) tested ”Whatlang” with 1,100
languages as well as a smaller subset of 184 lan-
guages. The reported average of classification ac-

185



curacy with 1,100 languages for lines up to 65
characters is 98.68%, which is extremely good.

2.6 VariKN toolkit (Vatanen et al., 2010)

The problem with short text samples was consid-
ered by Vatanen et al. (2010). Several smooth-
ing techniques with a naive Bayes classifier were
compared in tests of 281 languages. Absolute dis-
counting (Ney et al., 1994) smoothing with a max-
imum n-gram length of 5 turned out to be their best
method. When calculating the Markovian prob-
abilities in absolute discounting, a constant D is
subtracted from the counts c(C,uni−n+1) of all ob-
served n-grams uni−n+1 and the left-out probability
mass is distributed between the unseen n-grams in
relation to the probabilities of lower order n-grams
Pg(ui|un−1i−n+2), as in Equation 9.

Pg(ui|un−1i−n+1) =
c(C,uni−n+1)−D

c(C,un−1i−n+1)
+λun−1i−n+1 Pg(ui|u

n−1
i−n+2)

(9)

The language identification is performed us-
ing the ”perplexity” program provided with the
toolkit.2 Perplexity is calculated from the Marko-
vian probability P(M|Cg) = ∏i Pg(ui|un−1i−n+1) for
the mystery text M given the training data Cg as
in Equations 10 and 11.

Hg(M) =−
1

c(M,u) ∏i
log2Pg(ui|un−1i−n+1) (10)

Rperplexity(g,M) = 2
Hg(M) (11)

2.7 The HeLI method

The HeLI3 method (Jauhiainen, 2010) is described
in Jauhiainen et al. (2016) using the same notation
as in this article. In the method, each language is
represented by several different language models
only one of which is used for every word found in
the mystery text. The language models for each
language are: a model based on words and one or
more models based on character n-grams from one
to nmax. When a word not included in the model
based on words is encountered in the mystery text
M, the method backs off to using the n-grams of
the size nmax. If it is not possible to apply the n-
grams of the size nmax, the method backs off to
lower order n-grams and continues backing off un-
til character unigrams, if needed. A development

2https://github.com/vsiivola/variKN
3https://github.com/tosaja/HeLI

set is used for finding the best values for the pa-
rameters of the method. The three parameters are
the maximum length of the used character n-grams
(nmax), the maximum number of features to be in-
cluded in the language models (cut-off c), and the
penalty value for those languages where the fea-
tures being used are absent (penalty p). Because of
the large differences between the sizes of the train-
ing corpora, we used a slightly modified imple-
mentation of the method, where we used relative
frequencies as cut-offs c. The values in the mod-
els are 10-based logarithms of the relative frequen-
cies of the features u, calculated using only the fre-
quencies of the retained features, as in Equation 12

vCg(u) =

{
− log10

(
c(Cg,u)

lCg

)
, if c(Cg,u)> 0

p , if c(Cg,u) = 0
(12)

where c(Cg,u) is the number of features u and lCg
is the total number of all features in language g. If
c(Cg,u) is zero, then vCg(u) gets the penalty value
p.

A score vg(t) is calculated for each word t in
the mystery text for each language g, as shown in
Equation 13.

vg(t) =
{

vCg(t) , if t ∈ dom(O(Cg))
vg(t,min(nmax, lt +2)) , if t /∈ dom(O(Cg))

(13)

The whole mystery text M gets the score
RHeLI(g,M) equal to the average of the scores of
the words vg(t) for each language g, as in Equa-
tion 14

RHeLI(g,M) =
∑

lT (M)
i=1 vg(ti)

lT (M)
(14)

where T (M) is the sequence of words and lT (M) is
the number of words in the mystery text M. The
language having the lowest score is assigned to the
mystery text.

3 Test setting

In addition to the Uralic languages relevant to the
project (Jauhiainen et al., 2015), the languages for
the evaluation of the language identification meth-
ods were chosen so that we were able to train and
test with texts from different sources, preferably
also from different domains. We were able to
gather suitable corpora for a set of 285 languages.4

4The list of all the languages and most of the sources
can be found at http://suki.ling.helsinki.fi/
LILanguages.html.

186



In our project we are interested in gathering as
much of the very rare Uralic texts as possible, so
we need a high recall. On the other hand, if our
precision is bad, we end up with a high percent-
age of incorrect language labels for the rare lan-
guages. For these reasons we use the F1-score
as the main performance measure when evalu-
ating the language identifiers. We calculate the
language-level averages of recall, precision and
the F1-score. Language-level averages are referred
to as macro-averages by Lui et al. (Lui et al.,
2014). As the number of mystery texts for each
language were identical, the macro-averaged re-
call equals the commonly used classification accu-
racy5. The Fβ -score is based on the effectiveness
measure introduced by van Rijsbergen (1979) and
is calculated from the precision p and recall r, as
in Equation 15

Fβ = (1+β 2)
(

pr
(β 2 p)+ r

)
(15)

where β = 1 gives equal weight to precision and
recall.

3.1 Training Corpora

The biggest bulk of the training corpora is formed
from various Wikipedias.6 The collection sizes
range from a few articles for the very small lan-
guages to over a million articles in the English,
German, French, Dutch and Italian collections.
The sheer amount of linguistic material contained
in the article collections makes using them as
text corpora an appealing thought. The article
collections had to be cleaned as they contained
lots of non-lingual metadata and links as well as
text in non-native languages. In addition to the
text from Wikipedia, there is material from bible
translations7, other religious texts8, the Leipzig
Corpora Collection (Quasthoff et al., 2006), the
AKU project9, Sámi giellatekno10, and generic
web pages. Even with these additions, the amount

5The evaluation results for all the languages and identi-
fiers can be found at http://suki.ling.helsinki.
fi/NodaEvalResults.xlsx.

6http://www.wikipedia.org
7http://gospelgo.com/biblespage.html,

http://worldbibles.org, http://ibt.org.ru,
http://gochristianhelps.com

8http://www.christusrex.org, https:
//www.lds.org

9http://www.ling.helsinki.fi/˜rueter/
aku-index.shtml

10http://giellatekno.uit.no

of training material differs greatly between lan-
guages.

Each language has one file including all the
training texts for that language. Some of the texts
are copyrighted, so they cannot be published as
such. The amount of training material differs dras-
tically between languages: they span from 2,710
words of Tahitian to 29 million words of En-
glish. Some of the corpora were manually exam-
ined to remove text obviously written in foreign
languages. Even after all the cleaning, the training
corpora must be considered rather unclean.

3.2 Testing Corpora

The test corpora are mostly derived from the
translations of the universal declaration of human
rights.11 However, the test set includes languages
for which no translation of the declaration is avail-
able and for these languages texts were collected
from some of the same sources as for the training
corpora, but also from Tatoeba.12 Most of the test
texts have been examined manually for purity, so
that obvious inclusions of foreign languages were
removed.

The aim was to have the mystery texts from dif-
ferent domains than the training texts. Wikipedia
refers to the declaration of human rights in several
languages and in many places. In order to deal
with possible inclusion of test material in train-
ing corpora, every test corpus was divided into
30 character chunks and any lines including these
chunks in the corresponding training corpus were
removed. Also, if long sequences of numbers were
noticed, they were removed from both corpora.
There are still numbers in the test set and for ex-
ample some of the 5 character or even 10 character
sequences in the test set consist only or mostly of
numbers.

The test set has been randomly generated from
the test corpora. A test sample always begins at the
beginning of a word, but it might end anywhere,
including in the middle of a word. An extra blank
was inserted in the beginning of each line when
testing those language identifiers, which did not
automatically expect the text to begin with a word.
The test samples are of 19 different sizes ranging
from 5 to 150 characters. Each language and size
pair has 1,000 random (some can be identical) test
samples. The full test set comprises of around 5.4

11http://www.unicode.org/udhr/
12http://tatoeba.org/eng/

187



30

40

50

60

70

80

90

100

5 10 15 20 25 30 35 40 45 50 55 60 65 70

F
-s

c
o

re

Test length in characters

LIGA LogLIGA Cavnar & Trenkle

Figure 1: Comparison between the F1-scores of
the method of Cavnar and Trenkle (1994), LIGA,
and logLIGA.

million samples to be identified.

4 The results and discussion

After training the aforementioned language identi-
fiers with our own training corpora, we tested them
against all the languages in our test suite.

4.1 The baselines

Cavnar and Trenkle (1994) included the 300 most
frequent n-grams in the language models. In our
tests the best results were attained using 20,000
n-grams with their method. From the LIGA vari-
ations introduced by Vogel and Tresner-Kirsch
(2012), we chose to test the logLIGA as it per-
formed the best in their evaluations in addition
to the original LIGA algorithm. For these three
methods, the averaged results of the evaluations
for 285 languages can be seen in Figure 1.

The results of both the LIGA and logLIGA al-
gorithms are clearly outperformed by the method
of Cavnar and Trenkle (1994). Especially the
poor results of logLIGA were surprising, as it
was clearly better than the original LIGA algo-
rithm in the tests presented by Vogel and Tresner-
Kirsch (2012). To verify the performance of our
implementations, we tested them with the same
set of languages which were tested in (Vogel and
Tresner-Kirsch, 2012), where the baseline LIGA
had an average recall of 97.9% and logLIGA
99.8% over 6 languages. The tweets in their
dataset average around 80 characters. The results

55

60

65

70

75

80

85

90

95

100

5 10 15 20 25 30 35 40 45 50 55 60 65 70

R
e
c
a
ll

Test length in characters

LIGA LogLIGA

Figure 2: The recall of LIGA and logLIGA algo-
rithms with 6 languages.

of our tests can be seen in Figure 2. The logLIGA
clearly outperforms the LIGA algorithm and ob-
tains 99.8% recall already at 50 characters even
for our cross-domain test set. From these results
we believe that especially the logLIGA algorithm
does not scale to a situation with a large number
of languages.

4.2 The evaluations

For the evaluation of the method of King and
Dehdari (2008) we created Laplace and Lidstone
smoothed language models from our training cor-
pora and programmed a language identifier, which
used the sum of log probabilities (we did not use
NLTK) to measure the distance between the mod-
els and the mystery text. We tested n-grams from
1 to 6 with several different values of λ . King and
Dehdari (2008) used byte n-grams, but as our cor-
pus is completely UTF-8 encoded, we use n-grams
of characters instead.

The best results (Figure 3) in our tests were
achieved with 5-grams and a λ of 0.00000001.
These findings are not exactly in line with those
of King and Dehdari (2008). The number of lan-
guages used in both language identifiers is compa-
rable, but the amount of training data in our cor-
pus varies considerably between languages when
compared with the corpus used by King and De-
hdari (2008), where each language had about the
same amount of material. The smallest test set
they used was 2%, which corresponds to around
100 - 200 characters, which is comparable to the

188



Figure 3: The F1-scores of the six best evaluated
methods.

longest test sequences used in this article. We be-
lieve that these two dissimilarities in test setting
could be the reason for the differing results, but
we decided that investigating this further was not
within the scope of this article.

In the evaluation of the method of Brown
(2013), we used the ”mklangid” program provided
with the Brown’s package13 to create new lan-
guage models for the 285 languages of our test
suite. The best results with the ”whatlang” were
obtained using up to 10-byte n-grams, 40,000 n-
grams in the models, and 160 million bytes of
training data as well as stop-grams. Stop-grams
were calculated for languages with a similarity
score of 0.4 or higher. The average recall obtained
for 65 character samples was 98.9% with an F1-
score of 99.0%. Brown’s method clearly outper-
forms the results of the algorithm of Cavnar and
Trenkle (1994), as can be seen in Figure 3. One
thing to note is also the running time. Running the
tests using the algorithm of Cavnar and Trenkle
(1994) with 20,000 n-grams took over two days,
as opposed to the less than an hour with Brown’s
”Whatlang” program.

In order to evaluate the method used by Vatanen
et al. (2010), we utilized the VariKN toolkit (Si-
ivola et al., 2007) to create language models from
our training data with the same settings: absolute
discounting smoothing with a character n-gram
length of 5. When compared with the Browns

13https://sourceforge.net/projects/
la-strings/

Figure 4: The F1-scores of the HeLI method com-
pared with the methods of Brown (2012) and Vata-
nen et al. (2010).

identifier the results are clearly in favor of the
VariKN toolkit for short test lengths and almost
equal at test lengths of 70 characters, after which
Brown’s language identifier performs better.

For the evaluation of the HeLI method we used
a slightly modified Python based implementation
of the method. In our implementation, we used
relative frequencies as cut-offs c instead of just
the frequencies. In order to find the best possi-
ble parameters using the training corpora, we ap-
plied a simple form of the greedy algorithm us-
ing the last 10% of the training corpus for each
language as a development set. We started with
the same n-gram length nmax and the penalty value
p, which were found to provide the best results
in (Jauhiainen, 2010). Then we proceeded using
the greedy algorithm and found at least a local op-
timum with the values nmax = 6, c = 0.0000005,
and p = 7. The HeLI method obtains high recall
and precision clearly sooner than the methods of
Brown (2013) or Vatanen et al. (2010). The F1-
score of 99.5 is achieved at 60 characters, while
Brown’s method achieved it at 90 characters and
the method of Vatanen et al. (2010) at more than
100 characters, which can be seen in Figure 4. The
method of Vatanen et al. (2010) performs better
than the HeLI method when the length of the mys-
tery text is 20 characters or less.

The HeLI method was also tested without us-
ing the language models composed of words. It
was found that in addition to obtaining slightly

189



Lang. ISO HL LG LL VK WL CT KG
N.-Aram. aii 5 5 5 5 5 10 80
Amharic amh 5 5 20 5 10 5 -
Tibetan bod 5 10 10 5 5 25 20
Cherokee chr 5 10 15 5 5 20 -
Greek ell 5 5 5 5 5 10 40
Gujarati guj 5 5 5 5 5 10 15
Armenian hye 5 5 5 5 5 10 65
Inuktitut iku 5 10 15 5 5 10 -
Kannada kan 5 5 5 10 5 10 30
Korean kor 5 5 5 5 5 15 70
Malayal. mal 5 5 5 5 5 15 15
Thai tha 5 5 5 20 5 15 25

Table 1: Some of the easiest languages to iden-
tify showing how many characters were needed for
100.0% recall by each method.

Lang. ISO HL LG LL VK WL CT KG
Achinese ace 120 - - - - - -
Bislama bis 100 - - - - - -
Chayah. cbt - 70 - - 80 90 90
Danish dan 150 - - - - 100 -
T. Enets enh 150 80 - 70 - - 45
Evenki evn 150 - - - - - 150
Erzya myv - - - - - - -
Newari new - - - 90 - - -
Tumbuka tum - - - 90 150 150 -
Votic vot - - - 150 - 100 -

Table 2: Some of the most difficult languages
to identify showing how many characters were
needed for 100.0% recall by each method.

lower F1-scores, the language identifier was also
much slower when the words were not used. We
also tested using Lidstone smoothing instead of
the penalty values. The best results were acquired
with the Lidstone value of 0.0001, almost reach-
ing the same F1-scores as the language identifier
with the penalty value p of 7. The largest differ-
ences in F1-scores were at the lower mid-range of
test lengths, being 0.5 with 25-character samples
from the development set.

Some of the languages in the test set had such
unique writing systems that their average recall
was 100% already at 5 characters by many of the
methods as can be seen in Table 1. Some of the
most difficult languages can be seen in Table 2.
In both of the Tables HL stands for HeLI, LG for
LIGA, LL for LogLIGA, VK for VariKN, WL for
Whatlang, CT for Cavnar and Trenkle, and KG for
King and Dehdari.

5 Conclusions and Future Work

The purpose of the research was to test methods
capable of producing good identification results
in a general domain with a large number of lan-
guages. The methods of Vatanen et al. (2010)
and Brown (2012) outperformed the other meth-
ods, even though the original method of Cavnar
and Trenkle (1994) also obtained very good re-

sults. The recently published HeLI method outper-
forms previous methods and considerably reduces
the identification error rate for texts over 60 char-
acters in length.

There still exists several interesting language
identification methods and implementations that
we have not evaluated using the test setting de-
scribed in this article. These methods and imple-
mentations include, for example, those of Lui and
Baldwin (2012), Majli[Pleaseinsertintopreamble]
(2012), and Zampieri and Gebre (2014).

Acknowledgments

We would like to thank Kimmo Koskenniemi for
many valuable discussions and comments. This
research was made possible by funding from the
Kone Foundation Language Programme.

References
Steven Bird. 2006. Nltk: the natural language

toolkit. In COLING-ACL ’06 Proceedings of the
COLING/ACL on Interactive presentation sessions,
pages 69–72, Sydney.

Ralf D. Brown. 2012. Finding and identifying text in
900+ languages. Digital Investigation, 9:S34–S43.

Ralf D. Brown. 2013. Selecting and weighting n-
grams to identify 1100 languages. In Text, Speech,
and Dialogue 16th International Conference, TSD
2013 Pilsen, Czech Republic, September 2013 Pro-
ceedings, pages 475–483, Pilsen.

Ralf D. Brown. 2014. Non-linear mapping for
improved identification of 1300+ languages. In
Proceedings of the 2014 Conference on Empirical
Methods in Natural Language Processing (EMNLP
2014), pages 627–632, Doha, Qatar.

William B. Cavnar and John M. Trenkle. 1994. N-
gram-based text categorization. In Proceedings of
SDAIR-94, 3rd Annual Symposium on Document
Analysis and Information Retrieval, pages 161–175,
Las Vegas.

Heidi Jauhiainen, Tommi Jauhiainen, and Krister
Lindén. 2015. The finno-ugric languages and
the internet project. Septentrio Conference Series,
0(2):87–98.

Tommi Jauhiainen, Krister Lindén, and Heidi Jauhi-
ainen. 2016. Heli, a word-based backoff method for
language identification. In Proceedings of the 3rd
Workshop on Language Technology for Closely Re-
lated Languages, Varieties and Dialects (VarDial),
pages 153–162, Osaka, Japan.

Tommi Jauhiainen. 2010. Tekstin kielen automaatti-
nen tunnistaminen. Master’s thesis, University of
Helsinki, Helsinki.

190



Josh King and Jon Dehdari. 2008. An n-gram based
language identification system. The Ohio State Uni-
versity.

Kone Foundation. 2012. The language programme
2012-2016. http://www.koneensaatio.fi/en.

Marco Lui and Timothy Baldwin. 2012. langid.py:
an off-the-shelf language identification tool. In Pro-
ceedings of the 50th Annual Meeting of the Associ-
ation for Computational Linguistics, pages 25–30,
Jeju.

Marco Lui, Jey Han Lau, and Timothy Baldwin. 2014.
Automatic detection and language identification of
multilingual documents. Transactions of the Asso-
ciation for Computational Linguistics, 2:27–40.

Martin Majliš. 2012. Yet another language identi-
fier. In Proceedings of the Student Research Work-
shop at the 13th Conference of the European Chap-
ter of the Association for Computational Linguistics,
pages 46–54, Avignon.

Hermann Ney, Ute Essen, and Reinhard Kneser. 1994.
On structuring probabilistic dependences in stochas-
tic language modelling. Computer Speech and Lan-
guage, 8(1):1–38.

Uwe Quasthoff, Matthias Richter, and Christian Bie-
mann. 2006. Corpus portal for search in monolin-
gual corpora. In Proceedings of the fifth interna-
tional conference on Language Resources and Eval-
uation, LREC 2006, pages 1799–1802, Genoa.

Paul Rodrigues. 2012. Processing Highly Variant Lan-
guage Using Incremental Model Selection. Ph.D.
thesis, Indiana University.

Vesa Siivola, Teemu Hirsimäki, and Sami Virpi-
oja. 2007. On growing and pruning kneser-
ney smoothed n-gram models. IEEE Transac-
tions on Audio, Speech and Language Processing,
15(5):1617–1624.

Erik Tromp and Mykola Pechenizkiy. 2011. Graph-
based n-gram language identification on short texts.
In Benelearn 2011 - Proceedings of the Twentieth
Belgian Dutch Conference on Machine Learning,
pages 27–34, The Hague.

Erik Tromp. 2011. Multilingual sentiment analysis on
social media. Master’s thesis, Eindhoven University
of Technology, Eindhoven.

C. J. van Rijsbergen. 1979. Information Retrieval.
Butterworths.

Tommi Vatanen, Jaakko J. Väyrynen, and Sami Virpi-
oja. 2010. Language identification of short text seg-
ments with n-gram models. In LREC 2010, Seventh
International Conference on Language Resources
and Evaluation, pages 3423–3430, Malta.

John Vogel and David Tresner-Kirsch. 2012. Ro-
bust language identification in short, noisy texts:
Improvements to liga. In The Third International
Workshop on Mining Ubiquitous and Social Envi-
ronments, pages 43–50, Bristol.

Marcos Zampieri and Binyam Gebrekidan Gebre.
2014. Varclass: An open source language identifi-
cation tool for language varieties. In Proceedings of
Language Resources and Evaluation (LREC.

191


