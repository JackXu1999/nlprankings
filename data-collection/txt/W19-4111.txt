



















































Improving Long Distance Slot Carryover in Spoken Dialogue Systems


Proceedings of the 1st Workshop on NLP for Conversational AI, pages 96–105
Florence, Italy, August 1, 2019. c©2019 Association for Computational Linguistics

96

Improving Long Distance Slot Carryover in Spoken Dialogue Systems

Tongfei Chen∗ Chetan Naik† Hua He†
Pushpendre Rastogi† Lambert Mathias†

∗ Johns Hopkins University
† Amazon.com, Inc.

tongfei@jhu.edu, {chetnaik,huhe,prastogi,mathiasl}@amazon.com

Abstract

Tracking the state of the conversation is a cen-
tral component in task-oriented spoken dia-
logue systems. One such approach for track-
ing the dialogue state is slot carryover, where
a model makes a binary decision if a slot from
the context is relevant to the current turn. Pre-
vious work on the slot carryover task used
models that made independent decisions for
each slot. A close analysis of the results show
that this approach results in poor performance
over longer context dialogues. In this paper,
we propose to jointly model the slots. We
propose two neural network architectures, one
based on pointer networks that incorporate slot
ordering information, and the other based on
transformer networks that uses self attention
mechanism to model the slot interdependen-
cies. Our experiments on an internal dialogue
benchmark dataset and on the public DSTC2
dataset demonstrate that our proposed models
are able to resolve longer distance slot refer-
ences and are able to achieve competitive per-
formance.

1 Introduction

In task-oriented spoken dialogue systems, the user
and the system are engaged in interactions that can
span multiple turns. A key challenge here is that
the user can reference entities introduced in previ-
ous dialogue turns. For example, if a user request
for what’s the weather in arlington is followed by
how about tomorrow, the dialogue system has to
keep track of the entity arlington being referenced.

In slot-based spoken dialogue systems, tracking
the entities in context can be cast as slot carryover
task – only the relevant slots from the dialogue
context are carried over to the current turn. Re-
cent work by Naik et al. (2018) describes a scal-
able multi-domain neural network architecture to
address the task in a diverse schema setting. How-
ever, this approach treats every slot as indepen-

LOCATION
La taqueria

PLACE
La taqueria

TEMPERATURE
57 F

Mexican restaurants
PLACETYPE

CITY
San Francisco

What's the weather in San 
Francisco?

In San Francisco, CA
it's 57 F …

La taqueria is a mile away

Thanks, send directions to my 
phone

San Francisco

TOWN
San Francisco

SEARCH PLACE INTENT

GET WEATHER INTENT

GET DIRECTIONS INTENT

Any good Mexican 
restaurants there? STATECA

WEATHERCITY
San Francisco

WEATHERSTATE
CA

WEATHERCITY

Figure 1: An example of a conversation session. Slots
are listed on the right. Related slots often co-occur,
such as (1) [WEATHERCITY: San Francisco] and
[WEATHERSTATE: CA], and should be carried over to-
gether due to their interdependencies (2) PLACE slot is
often seen to occur along with TOWN.

dent. Consequently, as shown in our experiments,
this results in lower performance when the con-
textual slot being referenced is associated with di-
alogue turns that are further away from the cur-
rent turn. We posit that modeling slots jointly
is essential for improving the accuracy over long
distances, particularly when slots are correlated.
We motivate this with an example conversation
in Figure 1. In this example, the slots WEATH-
ERCITY/WEATHERSTATE, need to be carried over
together from dialogue history as they are corre-
lated. However, the model in Naik et al. (2018)
has no information about this slot interdependence
and may choose to carryover only one of the slots.
In this work, we alleviate this issue by propos-



97

ing two novel neural network architectures – one
based on pointer networks (Vinyals et al., 2015)
and another based on self-attention with trans-
formers (Vaswani et al., 2017) – that can learn to
jointly predict jointly whether a subset of related
slots should be carried over from dialogue history.

To validate our approach, we conduct thorough
evaluations on both the publicly available DSTC2
task (Henderson et al., 2014), as well as our in-
ternal dialogue dataset collected from a commer-
cial digital assistant. In Section 4.3, we show that
our proposed approach improve slot carryover ac-
curacy over the baseline systems over longer dia-
logue contexts. A detailed error analysis reveals
that our proposed models are more likely to utilize
“anchor” slots – slots tagged in the current utter-
ance – to carry over long-distance slots from con-
text.

To summarize we make the following contribu-
tions in this work:

1. We improve upon the slot carryover model ar-
chitecture in Naik et al. (2018) by introduc-
ing approaches for modeling slot interdepen-
dencies. We propose two neural network mod-
els based on pointer networks and transformer
networks that can make joint predictions over
slots.

2. We provide a detailed analysis of the proposed
models both on an internal benchmark and pub-
lic dataset. We show that contextual encoding
of slots and modeling slot interdependencies is
essential for improving performance of slot car-
ryover over longer dialogue contexts. Trans-
former architectures with self attention provide
the best performance overall.

2 Problem Formulation

A dialogue H is formulated as a sequence of utter-
ances, alternatively uttered by a user (U) and the
system agent (A):

H =
(
h{U ,A}
d

, · · · , hU2 , hA1 , hU0
)
, (1)

where each element h is an utterance. A subscript
d denotes the utterance distance which measures
the offset from the most recent user utterance (hU0 ).
The i-th token of an utterance with distance d is
denoted as hd[i].

A slot x = (d, k, l,r) in a dialogue is defined as a
key-value pair that contains an entity information,
e.g. [CITY:San Francisco]. Each slot can be de-
termined by the utterance distance d, slot key k,

and a span [l : r] over the tokens of the utterance
with slot value represented as hd[l : r].

Given a dialogue history H and a set of can-
didate slots X , the context carryover task is ad-
dressed by deciding which slots should be carried
over. The previous work (Naik et al., 2018) ad-
dressed the task as a binary classification problem
and each slot x ⊆ X is classified independently. In
contrast, our proposed models can explicitly cap-
ture slot interactions and make joint predictions
of all slots. We show formulations of both model
types below,

Fbinary(x,H) ∈ (0,1) ∀x ∈ X (2)
Fjoint(X,H) ⊆ X (3)

where Fbinary(x,H) denotes a binary classification
model (Naik et al., 2018), Fjoint(X,H) denotes our
joint prediction models.

3 Models

3.1 General architecture

Candidate Generation We follow the approach
in Naik et al. (2018), where, given a dialogue H,
we construct a candidate set of slots X from the
context by leveraging the slot key embeddings to
find the nearest slot keys that are associated with
the current turn.

Slot Encoder A model, given a candidate slot
(a slot key, a span in the history and a distance),
results in a fixed-length vector representation of a
slot: x = FS(x,H) ∈ RDS , where x is the slot, H is
the full history.

Dialogue Encoder We serialize the utterances
in the dialogue and use BiLSTM to en-
code the context as a fixed-length vector
c = BiLSTM(H) ∈ RDC .

Intent Encoder The intent I of the most recent
utterance determined by an NLU module is also
encoded as a fixed-length vector i ∈ RDI by av-
eraging the tokens in the intent. We average the
word embeddings of the tokens associated with the
intent to get the intent embedding.

Decoder Given the encoded vector representa-
tions {x1, · · · ,xn} of the slots, the context vector c,
the intent vector i, produce a subset of the slot ids:

FD(x1:n,c, i) ⊆ {1, · · · ,N} (4)



98

x1

x2

x3

x4

I i c

✓

✗

✗

✓

Decoder 

Intent 
Encoder

Dialogue Encoder (bi-LSTM)

Serialized
Dialogue

Token
Embeddings

Contextual
Embeddings

hA1
<latexit sha1_base64="hqyxwmIPyDky4HnJD7Fbg9tZGQo=">AAACAnicbVDLTgIxFL2DL8QX6tJNIzFxRWaMiS5RNy4xcYAERtIpHWhoO5O2YySEnXu3+gvujFt/xD/wM+zALAQ8SZOTc+6rJ0w408Z1v53Cyura+kZxs7S1vbO7V94/aOg4VYT6JOaxaoVYU84k9Q0znLYSRbEIOW2Gw5vMbz5SpVks780ooYHAfckiRrCxkj/oeg9X3XLFrbpToGXi5aQCOerd8k+nF5NUUGkIx1q3PTcxwRgrwwink1In1TTBZIj7tG2pxILqYDw9doJOrNJDUazskwZN1b8dYyy0HonQVgpsBnrRy8T/vHZqostgzGSSGirJbFGUcmRilP0c9ZiixPCRJZgoZm9FZIAVJsbmM7clm50Y8TQp2Wi8xSCWSeOs6ll+d16pXechFeEIjuEUPLiAGtxCHXwgwOAFXuHNeXbenQ/nc1ZacPKeQ5iD8/UL6hmXsQ==</latexit><latexit sha1_base64="hqyxwmIPyDky4HnJD7Fbg9tZGQo=">AAACAnicbVDLTgIxFL2DL8QX6tJNIzFxRWaMiS5RNy4xcYAERtIpHWhoO5O2YySEnXu3+gvujFt/xD/wM+zALAQ8SZOTc+6rJ0w408Z1v53Cyura+kZxs7S1vbO7V94/aOg4VYT6JOaxaoVYU84k9Q0znLYSRbEIOW2Gw5vMbz5SpVks780ooYHAfckiRrCxkj/oeg9X3XLFrbpToGXi5aQCOerd8k+nF5NUUGkIx1q3PTcxwRgrwwink1In1TTBZIj7tG2pxILqYDw9doJOrNJDUazskwZN1b8dYyy0HonQVgpsBnrRy8T/vHZqostgzGSSGirJbFGUcmRilP0c9ZiixPCRJZgoZm9FZIAVJsbmM7clm50Y8TQp2Wi8xSCWSeOs6ll+d16pXechFeEIjuEUPLiAGtxCHXwgwOAFXuHNeXbenQ/nc1ZacPKeQ5iD8/UL6hmXsQ==</latexit><latexit sha1_base64="hqyxwmIPyDky4HnJD7Fbg9tZGQo=">AAACAnicbVDLTgIxFL2DL8QX6tJNIzFxRWaMiS5RNy4xcYAERtIpHWhoO5O2YySEnXu3+gvujFt/xD/wM+zALAQ8SZOTc+6rJ0w408Z1v53Cyura+kZxs7S1vbO7V94/aOg4VYT6JOaxaoVYU84k9Q0znLYSRbEIOW2Gw5vMbz5SpVks780ooYHAfckiRrCxkj/oeg9X3XLFrbpToGXi5aQCOerd8k+nF5NUUGkIx1q3PTcxwRgrwwink1In1TTBZIj7tG2pxILqYDw9doJOrNJDUazskwZN1b8dYyy0HonQVgpsBnrRy8T/vHZqostgzGSSGirJbFGUcmRilP0c9ZiixPCRJZgoZm9FZIAVJsbmM7clm50Y8TQp2Wi8xSCWSeOs6ll+d16pXechFeEIjuEUPLiAGtxCHXwgwOAFXuHNeXbenQ/nc1ZacPKeQ5iD8/UL6hmXsQ==</latexit><latexit sha1_base64="hqyxwmIPyDky4HnJD7Fbg9tZGQo=">AAACAnicbVDLTgIxFL2DL8QX6tJNIzFxRWaMiS5RNy4xcYAERtIpHWhoO5O2YySEnXu3+gvujFt/xD/wM+zALAQ8SZOTc+6rJ0w408Z1v53Cyura+kZxs7S1vbO7V94/aOg4VYT6JOaxaoVYU84k9Q0znLYSRbEIOW2Gw5vMbz5SpVks780ooYHAfckiRrCxkj/oeg9X3XLFrbpToGXi5aQCOerd8k+nF5NUUGkIx1q3PTcxwRgrwwink1In1TTBZIj7tG2pxILqYDw9doJOrNJDUazskwZN1b8dYyy0HonQVgpsBnrRy8T/vHZqostgzGSSGirJbFGUcmRilP0c9ZiixPCRJZgoZm9FZIAVJsbmM7clm50Y8TQp2Wi8xSCWSeOs6ll+d16pXechFeEIjuEUPLiAGtxCHXwgwOAFXuHNeXbenQ/nc1ZacPKeQ5iD8/UL6hmXsQ==</latexit>

hU0
<latexit sha1_base64="CJEIf6+FZBDy3tGAOMwywx49DIo=">AAACFXicbVBNS8NAEN34WetX1aOXxSJ4KokIeix68VjBtIU2ls120y7d3YTdiVhCf4NH9cd4E6+e/S1e3LQ52NYHA4/3ZpiZFyaCG3Ddb2dldW19Y7O0Vd7e2d3brxwcNk2casp8GotYt0NimOCK+cBBsHaiGZGhYK1wdJP7rUemDY/VPYwTFkgyUDzilICV/GHPffB7lapbc6fAy8QrSBUVaPQqP91+TFPJFFBBjOl4bgJBRjRwKtik3E0NSwgdkQHrWKqIZCbIpsdO8KlV+jiKtS0FeKr+nciINGYsQ9spCQzNopeL/3mdFKKrIOMqSYEpOlsUpQJDjPPPcZ9rRkGMLSFUc3srpkOiCQWbz9wW4PbguTeyfF0C8mlStml5i9ksk+Z5zbP87qJavy5yK6FjdILOkIcuUR3dogbyEUUcPaNX9Oa8OO/Oh/M5a11xipkjNAfn6xcwLp/P</latexit><latexit sha1_base64="CJEIf6+FZBDy3tGAOMwywx49DIo=">AAACFXicbVBNS8NAEN34WetX1aOXxSJ4KokIeix68VjBtIU2ls120y7d3YTdiVhCf4NH9cd4E6+e/S1e3LQ52NYHA4/3ZpiZFyaCG3Ddb2dldW19Y7O0Vd7e2d3brxwcNk2casp8GotYt0NimOCK+cBBsHaiGZGhYK1wdJP7rUemDY/VPYwTFkgyUDzilICV/GHPffB7lapbc6fAy8QrSBUVaPQqP91+TFPJFFBBjOl4bgJBRjRwKtik3E0NSwgdkQHrWKqIZCbIpsdO8KlV+jiKtS0FeKr+nciINGYsQ9spCQzNopeL/3mdFKKrIOMqSYEpOlsUpQJDjPPPcZ9rRkGMLSFUc3srpkOiCQWbz9wW4PbguTeyfF0C8mlStml5i9ksk+Z5zbP87qJavy5yK6FjdILOkIcuUR3dogbyEUUcPaNX9Oa8OO/Oh/M5a11xipkjNAfn6xcwLp/P</latexit><latexit sha1_base64="CJEIf6+FZBDy3tGAOMwywx49DIo=">AAACFXicbVBNS8NAEN34WetX1aOXxSJ4KokIeix68VjBtIU2ls120y7d3YTdiVhCf4NH9cd4E6+e/S1e3LQ52NYHA4/3ZpiZFyaCG3Ddb2dldW19Y7O0Vd7e2d3brxwcNk2casp8GotYt0NimOCK+cBBsHaiGZGhYK1wdJP7rUemDY/VPYwTFkgyUDzilICV/GHPffB7lapbc6fAy8QrSBUVaPQqP91+TFPJFFBBjOl4bgJBRjRwKtik3E0NSwgdkQHrWKqIZCbIpsdO8KlV+jiKtS0FeKr+nciINGYsQ9spCQzNopeL/3mdFKKrIOMqSYEpOlsUpQJDjPPPcZ9rRkGMLSFUc3srpkOiCQWbz9wW4PbguTeyfF0C8mlStml5i9ksk+Z5zbP87qJavy5yK6FjdILOkIcuUR3dogbyEUUcPaNX9Oa8OO/Oh/M5a11xipkjNAfn6xcwLp/P</latexit><latexit sha1_base64="3KNN29br2o9T8I1ER0ONam5p20I=">AAACAnicbVDLSgMxFL1TX3WsWtdugkVwVWbc6FJw47KCfUBbSiaTaUOTzJDcEcvQH3Anfow78Tf8Fjdm2i5s64ULh3MSzrknyqSwGATfXmVnd2//oHroH9X845PTeq1j09ww3mapTE0vopZLoXkbBUreywynKpK8G03vS737zI0VqX7CWcaHio61SASj6KjWqN4ImsFiyDYIV6ABqxnVfwZxynLFNTJJre2HQYbDghoUTPK5P8gtzyib0jHvO6ip4nZYLGLOyaVjYpKkxq1GsmD//iiosnamIvdSUZzYTa0k/9P6OSa3w0LoLEeu2dIoySXBlJQ3k1gYzlDOHKDMCJeVsAk1lKFrZs0FhQu8dkZR2mWoXua+KyvcrGYbdK6bocOPAVThHC7gCkK4gTt4gBa0gUEMr/DuvXkf3uey1Iq3avcM1sb7+gXoqJuI</latexit><latexit sha1_base64="VG0dOYrcwSL9c11hxMHBICj7BiQ=">AAACCnicbVBNTwIxFHzrJyIqevXSSEw8kV0vejTx4hETF0gASbd0oaHtbtq3RrLhN3hUf4w343/wt3ixCxwEfMlLJjNtZt5EqRQWff/b29jc2t7ZLe2V9ysHh0fV40rTJplhPGSJTEw7opZLoXmIAiVvp4ZTFUneisa3hd564saKRD/gJOU9RYdaxIJRdFQ46vuPYb9a8+v+bMg6CBagBotp9Ks/3UHCMsU1Mkmt7QR+ir2cGhRM8mm5m1meUjamQ95xUFPFbS+fhZ2Sc8cMSJwYtxrJjP37I6fK2omK3EtFcWRXtYL8T+tkGF/3cqHTDLlmc6M4kwQTUlxOBsJwhnLiAGVGuKyEjaihDF0/Sy4oXOClM/LCLkX1PC27toLVbtZB87IeOHzvQwlO4QwuIIAruIE7aEAIDAS8wBu8e6/eh/c573XDWxR8Akvjff0CTrqeTw==</latexit><latexit sha1_base64="VG0dOYrcwSL9c11hxMHBICj7BiQ=">AAACCnicbVBNTwIxFHzrJyIqevXSSEw8kV0vejTx4hETF0gASbd0oaHtbtq3RrLhN3hUf4w343/wt3ixCxwEfMlLJjNtZt5EqRQWff/b29jc2t7ZLe2V9ysHh0fV40rTJplhPGSJTEw7opZLoXmIAiVvp4ZTFUneisa3hd564saKRD/gJOU9RYdaxIJRdFQ46vuPYb9a8+v+bMg6CBagBotp9Ks/3UHCMsU1Mkmt7QR+ir2cGhRM8mm5m1meUjamQ95xUFPFbS+fhZ2Sc8cMSJwYtxrJjP37I6fK2omK3EtFcWRXtYL8T+tkGF/3cqHTDLlmc6M4kwQTUlxOBsJwhnLiAGVGuKyEjaihDF0/Sy4oXOClM/LCLkX1PC27toLVbtZB87IeOHzvQwlO4QwuIIAruIE7aEAIDAS8wBu8e6/eh/c573XDWxR8Akvjff0CTrqeTw==</latexit><latexit sha1_base64="6uBsst/raNscIASO937iRr3ls2c=">AAACFXicbVA9T8MwEHXKVylfBUYWiwqJqUpYYKxgYSwSaSu1oXJcp7VqO5F9QVRRfwMj8GPYECszv4UFp81AW5500tN7d7q7FyaCG3Ddb6e0tr6xuVXeruzs7u0fVA+PWiZONWU+jUWsOyExTHDFfOAgWCfRjMhQsHY4vsn99iPThsfqHiYJCyQZKh5xSsBK/qjvPvj9as2tuzPgVeIVpIYKNPvVn94gpqlkCqggxnQ9N4EgIxo4FWxa6aWGJYSOyZB1LVVEMhNks2On+MwqAxzF2pYCPFP/TmREGjORoe2UBEZm2cvF/7xuCtFVkHGVpMAUnS+KUoEhxvnneMA1oyAmlhCqub0V0xHRhILNZ2ELcHvwwhtZvi4B+TSt2LS85WxWSeui7ll+59Ya10VuZXSCTtE58tAlaqBb1EQ+ooijZ/SK3pwX5935cD7nrSWnmDlGC3C+fgEu7p/L</latexit><latexit sha1_base64="CJEIf6+FZBDy3tGAOMwywx49DIo=">AAACFXicbVBNS8NAEN34WetX1aOXxSJ4KokIeix68VjBtIU2ls120y7d3YTdiVhCf4NH9cd4E6+e/S1e3LQ52NYHA4/3ZpiZFyaCG3Ddb2dldW19Y7O0Vd7e2d3brxwcNk2casp8GotYt0NimOCK+cBBsHaiGZGhYK1wdJP7rUemDY/VPYwTFkgyUDzilICV/GHPffB7lapbc6fAy8QrSBUVaPQqP91+TFPJFFBBjOl4bgJBRjRwKtik3E0NSwgdkQHrWKqIZCbIpsdO8KlV+jiKtS0FeKr+nciINGYsQ9spCQzNopeL/3mdFKKrIOMqSYEpOlsUpQJDjPPPcZ9rRkGMLSFUc3srpkOiCQWbz9wW4PbguTeyfF0C8mlStml5i9ksk+Z5zbP87qJavy5yK6FjdILOkIcuUR3dogbyEUUcPaNX9Oa8OO/Oh/M5a11xipkjNAfn6xcwLp/P</latexit><latexit sha1_base64="CJEIf6+FZBDy3tGAOMwywx49DIo=">AAACFXicbVBNS8NAEN34WetX1aOXxSJ4KokIeix68VjBtIU2ls120y7d3YTdiVhCf4NH9cd4E6+e/S1e3LQ52NYHA4/3ZpiZFyaCG3Ddb2dldW19Y7O0Vd7e2d3brxwcNk2casp8GotYt0NimOCK+cBBsHaiGZGhYK1wdJP7rUemDY/VPYwTFkgyUDzilICV/GHPffB7lapbc6fAy8QrSBUVaPQqP91+TFPJFFBBjOl4bgJBRjRwKtik3E0NSwgdkQHrWKqIZCbIpsdO8KlV+jiKtS0FeKr+nciINGYsQ9spCQzNopeL/3mdFKKrIOMqSYEpOlsUpQJDjPPPcZ9rRkGMLSFUc3srpkOiCQWbz9wW4PbguTeyfF0C8mlStml5i9ksk+Z5zbP87qJavy5yK6FjdILOkIcuUR3dogbyEUUcPaNX9Oa8OO/Oh/M5a11xipkjNAfn6xcwLp/P</latexit><latexit sha1_base64="CJEIf6+FZBDy3tGAOMwywx49DIo=">AAACFXicbVBNS8NAEN34WetX1aOXxSJ4KokIeix68VjBtIU2ls120y7d3YTdiVhCf4NH9cd4E6+e/S1e3LQ52NYHA4/3ZpiZFyaCG3Ddb2dldW19Y7O0Vd7e2d3brxwcNk2casp8GotYt0NimOCK+cBBsHaiGZGhYK1wdJP7rUemDY/VPYwTFkgyUDzilICV/GHPffB7lapbc6fAy8QrSBUVaPQqP91+TFPJFFBBjOl4bgJBRjRwKtik3E0NSwgdkQHrWKqIZCbIpsdO8KlV+jiKtS0FeKr+nciINGYsQ9spCQzNopeL/3mdFKKrIOMqSYEpOlsUpQJDjPPPcZ9rRkGMLSFUc3srpkOiCQWbz9wW4PbguTeyfF0C8mlStml5i9ksk+Z5zbP87qJavy5yK6FjdILOkIcuUR3dogbyEUUcPaNX9Oa8OO/Oh/M5a11xipkjNAfn6xcwLp/P</latexit><latexit sha1_base64="CJEIf6+FZBDy3tGAOMwywx49DIo=">AAACFXicbVBNS8NAEN34WetX1aOXxSJ4KokIeix68VjBtIU2ls120y7d3YTdiVhCf4NH9cd4E6+e/S1e3LQ52NYHA4/3ZpiZFyaCG3Ddb2dldW19Y7O0Vd7e2d3brxwcNk2casp8GotYt0NimOCK+cBBsHaiGZGhYK1wdJP7rUemDY/VPYwTFkgyUDzilICV/GHPffB7lapbc6fAy8QrSBUVaPQqP91+TFPJFFBBjOl4bgJBRjRwKtik3E0NSwgdkQHrWKqIZCbIpsdO8KlV+jiKtS0FeKr+nciINGYsQ9spCQzNopeL/3mdFKKrIOMqSYEpOlsUpQJDjPPPcZ9rRkGMLSFUc3srpkOiCQWbz9wW4PbguTeyfF0C8mlStml5i9ksk+Z5zbP87qJavy5yK6FjdILOkIcuUR3dogbyEUUcPaNX9Oa8OO/Oh/M5a11xipkjNAfn6xcwLp/P</latexit><latexit sha1_base64="CJEIf6+FZBDy3tGAOMwywx49DIo=">AAACFXicbVBNS8NAEN34WetX1aOXxSJ4KokIeix68VjBtIU2ls120y7d3YTdiVhCf4NH9cd4E6+e/S1e3LQ52NYHA4/3ZpiZFyaCG3Ddb2dldW19Y7O0Vd7e2d3brxwcNk2casp8GotYt0NimOCK+cBBsHaiGZGhYK1wdJP7rUemDY/VPYwTFkgyUDzilICV/GHPffB7lapbc6fAy8QrSBUVaPQqP91+TFPJFFBBjOl4bgJBRjRwKtik3E0NSwgdkQHrWKqIZCbIpsdO8KlV+jiKtS0FeKr+nciINGYsQ9spCQzNopeL/3mdFKKrIOMqSYEpOlsUpQJDjPPPcZ9rRkGMLSFUc3srpkOiCQWbz9wW4PbguTeyfF0C8mlStml5i9ksk+Z5zbP87qJavy5yK6FjdILOkIcuUR3dogbyEUUcPaNX9Oa8OO/Oh/M5a11xipkjNAfn6xcwLp/P</latexit><latexit sha1_base64="CJEIf6+FZBDy3tGAOMwywx49DIo=">AAACFXicbVBNS8NAEN34WetX1aOXxSJ4KokIeix68VjBtIU2ls120y7d3YTdiVhCf4NH9cd4E6+e/S1e3LQ52NYHA4/3ZpiZFyaCG3Ddb2dldW19Y7O0Vd7e2d3brxwcNk2casp8GotYt0NimOCK+cBBsHaiGZGhYK1wdJP7rUemDY/VPYwTFkgyUDzilICV/GHPffB7lapbc6fAy8QrSBUVaPQqP91+TFPJFFBBjOl4bgJBRjRwKtik3E0NSwgdkQHrWKqIZCbIpsdO8KlV+jiKtS0FeKr+nciINGYsQ9spCQzNopeL/3mdFKKrIOMqSYEpOlsUpQJDjPPPcZ9rRkGMLSFUc3srpkOiCQWbz9wW4PbguTeyfF0C8mlStml5i9ksk+Z5zbP87qJavy5yK6FjdILOkIcuUR3dogbyEUUcPaNX9Oa8OO/Oh/M5a11xipkjNAfn6xcwLp/P</latexit>

h{U, A}d
<latexit sha1_base64="Oemh46EfhhvMKk620BM9eEFVCGg=">AAACDnicbZDNSsNAFIUn9a/Wn0ZduhksggspiQi6rLpxWcG0hSaGyWTSDp1MwsxELCHv4N6tvoI7cesr+AY+hpO2C9t6YODj3HvnXk6QMiqVZX0blZXVtfWN6mZta3tnt27u7XdkkglMHJywRPQCJAmjnDiKKkZ6qSAoDhjpBqObst59JELShN+rcUq8GA04jShGSlu+WR/64UPu5s6pC6/covDNhtW0JoLLYM+gAWZq++aPGyY4iwlXmCEp+7aVKi9HQlHMSFFzM0lShEdoQPoaOYqJ9PLJ4QU81k4Io0ToxxWcuH8nchRLOY4D3RkjNZSLtdL8r9bPVHTp5ZSnmSIcTxdFGYMqgWUKMKSCYMXGGhAWVN8K8RAJhJXOam5L+Xeq4qeipqOxF4NYhs5Z09Z8d95oXc9CqoJDcAROgA0uQAvcgjZwAAYZeAGv4M14Nt6ND+Nz2loxZjMHYE7G1y8inpwe</latexit><latexit sha1_base64="Oemh46EfhhvMKk620BM9eEFVCGg=">AAACDnicbZDNSsNAFIUn9a/Wn0ZduhksggspiQi6rLpxWcG0hSaGyWTSDp1MwsxELCHv4N6tvoI7cesr+AY+hpO2C9t6YODj3HvnXk6QMiqVZX0blZXVtfWN6mZta3tnt27u7XdkkglMHJywRPQCJAmjnDiKKkZ6qSAoDhjpBqObst59JELShN+rcUq8GA04jShGSlu+WR/64UPu5s6pC6/covDNhtW0JoLLYM+gAWZq++aPGyY4iwlXmCEp+7aVKi9HQlHMSFFzM0lShEdoQPoaOYqJ9PLJ4QU81k4Io0ToxxWcuH8nchRLOY4D3RkjNZSLtdL8r9bPVHTp5ZSnmSIcTxdFGYMqgWUKMKSCYMXGGhAWVN8K8RAJhJXOam5L+Xeq4qeipqOxF4NYhs5Z09Z8d95oXc9CqoJDcAROgA0uQAvcgjZwAAYZeAGv4M14Nt6ND+Nz2loxZjMHYE7G1y8inpwe</latexit><latexit sha1_base64="Oemh46EfhhvMKk620BM9eEFVCGg=">AAACDnicbZDNSsNAFIUn9a/Wn0ZduhksggspiQi6rLpxWcG0hSaGyWTSDp1MwsxELCHv4N6tvoI7cesr+AY+hpO2C9t6YODj3HvnXk6QMiqVZX0blZXVtfWN6mZta3tnt27u7XdkkglMHJywRPQCJAmjnDiKKkZ6qSAoDhjpBqObst59JELShN+rcUq8GA04jShGSlu+WR/64UPu5s6pC6/covDNhtW0JoLLYM+gAWZq++aPGyY4iwlXmCEp+7aVKi9HQlHMSFFzM0lShEdoQPoaOYqJ9PLJ4QU81k4Io0ToxxWcuH8nchRLOY4D3RkjNZSLtdL8r9bPVHTp5ZSnmSIcTxdFGYMqgWUKMKSCYMXGGhAWVN8K8RAJhJXOam5L+Xeq4qeipqOxF4NYhs5Z09Z8d95oXc9CqoJDcAROgA0uQAvcgjZwAAYZeAGv4M14Nt6ND+Nz2loxZjMHYE7G1y8inpwe</latexit><latexit sha1_base64="Oemh46EfhhvMKk620BM9eEFVCGg=">AAACDnicbZDNSsNAFIUn9a/Wn0ZduhksggspiQi6rLpxWcG0hSaGyWTSDp1MwsxELCHv4N6tvoI7cesr+AY+hpO2C9t6YODj3HvnXk6QMiqVZX0blZXVtfWN6mZta3tnt27u7XdkkglMHJywRPQCJAmjnDiKKkZ6qSAoDhjpBqObst59JELShN+rcUq8GA04jShGSlu+WR/64UPu5s6pC6/covDNhtW0JoLLYM+gAWZq++aPGyY4iwlXmCEp+7aVKi9HQlHMSFFzM0lShEdoQPoaOYqJ9PLJ4QU81k4Io0ToxxWcuH8nchRLOY4D3RkjNZSLtdL8r9bPVHTp5ZSnmSIcTxdFGYMqgWUKMKSCYMXGGhAWVN8K8RAJhJXOam5L+Xeq4qeipqOxF4NYhs5Z09Z8d95oXc9CqoJDcAROgA0uQAvcgjZwAAYZeAGv4M14Nt6ND+Nz2loxZjMHYE7G1y8inpwe</latexit>

Candidate
Generation 

… EOS in san francisco CA … EOS any good mexican restaurants there EOS

… …

… …ṽfrancisco
<latexit sha1_base64="rYhX3kZ2cSi3v4aSHCy1zDPh1GQ=">AAACWnicZZDbSsNAEIa38dx6qIc7EYKl4FVJquCt4I1XomK10JSy2U7q4h5CdqqRJU/g03irTyL4MG5rL0w7sPDPt/8MMxOnghsMgu+Kt7S8srq2vlGtbW5t79R39x6MHmcMOkwLnXVjakBwBR3kKKCbZkBlLOAxfr6c/D++QGa4Vvf4lkJf0pHiCWcUHRrUmxFyMQQbSYpPcWJfimJgI4QcbZJRxbhh2qF6I2gF0/AXRTgTDTKLm8Fu5SgaajaWoJAJakwvDFLsW5ohZwKKajQ2kFL2TEfQc1JRCaZvp/sUftORoZ/ozD2F/pT+r7DInb3cxEgqBGQ6dW0UvGI+WahksVSaRRjLcp7wXGDehjkqNMUyMnysOOZzPjevSYE5agAl5WpC7P1kXv8aXv07Lakqqu6g4fz5FsVDuxWettq3Z42L7uy06+SQHJMTEpJzckGuyA3pEEbeyQf5JF+VH8/zNrzan9WrzGr2SSm8g1/fS7pu</latexit>

ṽsan
<latexit sha1_base64="UFbjKD4aajDMax/wp0TA6/s+0Dc=">AAACVHicZZBNS8NAEIY3qR+1flU9ihgsgqeSVMGr4MWTVGm1YErZbKe6uB8hO62RpUd/jVf9K4L/xYPb2oNpBxbeeXZmmHmTVHCDYfjt+aWl5ZXV8lplfWNza7u6s3tn9DBj0GZa6KyTUAOCK2gjRwGdNAMqEwH3yfPl5P9+BJnhWrXwNYWupI+KDzij6FCvehgjF32wsaT4lAzsaDzu2RghR2uockm1FtbDaQSLIpqJGplFs7fjHcR9zYYSFDJBjXmIwhS7lmbImYBxJR4aSCl7po/w4KSiEkzXTi8ZB8eO9IOBztxTGEzp/w6L3JUXhxhJhYBMp26MghfMJ6cUSiyVZhEmspgPeC4wb8AcFZpiERk+VBzzuTq3r0mBOWoAJeVqQmxrsm9wDS/BrZbO0IozNJq3b1HcNerRab1xc1a76MysLZN9ckROSETOyQW5Ik3SJoy8kXfyQT69L+/HL/nLf6W+N+vZI4Xwt34BV5u4uw==</latexit>

ṽCA
<latexit sha1_base64="XNIQdwJlglCr3RkE0r6Jz3rpamM=">AAACU3icZZBNS8NAEIY38avWr6pHUYJF8FSSKnhVevEkKlYLppTNdqqL+xGyU40suflrvOpf8eBv8eK29mDagYV3nn1nmJkkFdxgGH57/tz8wuJSZbm6srq2vlHb3Lo1epgxaDMtdNZJqAHBFbSRo4BOmgGViYC75Kk1+r97hsxwrW7wNYWupA+KDzij6FCvthcjF32wsaT4mAzsc1H0bIyQo22dOV2rh41wHMGsiCaiTiZx2dv0duO+ZkMJCpmgxtxHYYpdSzPkTEBRjYcGUsqe6APcO6moBNO140WK4MCRfjDQmXsKgzH9X2GRO3u5iZFUCMh06tooeMF8tEnJYqk0szCR5XzAc4F5E6ao0BTLyPCh4phP+dy8JgXmqAGUlKsRsTejeYMLeAmutaSqqLqDRtPnmxW3zUZ01GheHddPO5PTVsgO2SeHJCIn5JSck0vSJoy8kXfyQT69L+/H9/35P6vvTWq2SSn8tV/L/Lfz</latexit>

ṽany
<latexit sha1_base64="O4JMuzxzi78Qfw2p6tyqnafKZQY=">AAACVHicZZBNS8NAEIY3qZ/1q9WjiMEieCpJFbwKXjyJitWCKWWznerifoTstEaWHP01XvWvCP4XD25rD6YdWHjn2XeGmUlSwQ2G4bfnVxYWl5ZXVqtr6xubW7X69p3Rw4xBm2mhs05CDQiuoI0cBXTSDKhMBNwnz+fj//sRZIZrdYuvKXQlfVR8wBlFh3q1/Ri56IONJcWnZGBHRdGzMUKOlqpXl9QaYTOcRDAvoqlokGlc9ereXtzXbChBIRPUmIcoTLFraYacCSiq8dBAStkzfYQHJxWVYLp2skkRHDrSDwY6c09hMKH/KyxyZy83MZIKAZlOXRsFL5iPVylZLJVmHiaynA94LjBvwQwVmmIZGT5UHPMZn5vXpMAcNYCScjUm9nY8b3AJL8GNllQVVXfQaPZ88+Ku1YyOm63rk8ZZZ3raFbJLDsgRicgpOSMX5Iq0CSNv5J18kE/vy/vxK/7in9X3pjU7pBT+5i9icLjB</latexit>

ṽgood
<latexit sha1_base64="QVKpGDoPqhq+lFjgaWlWqTFatD0=">AAACVXicZZBNS8NAEIY38avWj1Y9ihAsgqeSVMGr4MWTVLFaMKVsttO6uB8hO9XIkqu/xqv+FfHHCG5rD6YdWHjn2XeGmUlSwQ2G4bfnLy2vrK5V1qsbm1vbtfrO7p3R44xBh2mhs25CDQiuoIMcBXTTDKhMBNwnTxeT//tnyAzX6hZfU+hJOlJ8yBlFh/r1IEYuBmBjSfExGdrnoujbGCFHO9J64LJ6I2yG0wgWRTQTDTKLdn/HO4gHmo0lKGSCGvMQhSn2LM2QMwFFNR4bSCl7oiN4cFJRCaZnp6sUwZEjg2CoM/cUBlP6v8Iid/ZyEyOpEJDp1LVR8IL5ZJeSxVJpFmEiy/mQ5wLzFsxRoSmWkeFjxTGf87l5TQrMUQMoKVcTYm8n8wZX8BLcaElVUXUHjebPtyjuWs3opNm6Pm2cd2enrZB9ckiOSUTOyDm5JG3SIYy8kXfyQT69L+/HX/ZX/6y+N6vZI6Xwa79FBrks</latexit>

ṽmexican
<latexit sha1_base64="13mYBdTunzVMEy3PhXIUbcTm1rU=">AAACWHicZZDLSgNBEEUr4zu+oi5FGAyCqzATBbeCG1eiYjTghNDTqWhjP4bpihlp8gF+jVv9FP0aOzELJylouH36VlF900wKS1H0XQkWFpeWV1bXqusbm1vbtZ3de2sGOccWN9Lk7ZRZlEJjiwRJbGc5MpVKfEhfLsbvD6+YW2H0Hb1l2FHsSYu+4Iw86tbqCQnZQ5coRs9p372ORl2XEBbkFBbepj2o1aNGNKlwXsRTUYdpXXd3KgdJz/CBQk1cMmsf4yijjmM5CS5xVE0GFjPGX9gTPnqpmULbcZPfjMIjT3ph3+T+aAon9H+HI+Ht5SFWMSkxN5kfo3FIxfg7JYtjys7DVJXvfVFIKpo4Q6VhVEZWDLSgYsbn97UZck8tkmJCj4m7G+8bXuEwvDXKB1r1gcaz8c2L+2YjPmk0b07r5+1ptKuwD4dwDDGcwTlcwjW0gMM7fMAnfFV+AghWgrU/a1CZ9uxBqYLdX/e+uYc=</latexit>

ṽrestaurants
<latexit sha1_base64="23/vUWpuSi0NGldxkEEtGpyFROc=">AAACXHicZZDPattAEMbXipumTpM6CfRSCqKmNCcjuYVcA730VJJiJ4bImNF6nCzZP2J3lCgseoQ+Ta/Jg/SSZ8na9aGyBxa++e03w8zkhRSOkuRvK9pqv9p+vfOms/t2b/9d9+DwwpnSchxxI40d5+BQCo0jEiRxXFgElUu8zG+/L/4v79A6YfSQHgqcKLjWYi44UEDT7peMhJyhzxTQTT73d3U99RlhRd6iIygtaHIBdntJP1lGvCnSleixVZxND1ofs5nhpUJNXIJzV2lS0MSDJcEl1p2sdFgAv4VrvApSg0I38cuN6vhzILN4bmx4muIl/b/Ckwj2ZhOnQEq0pghtNN5TtVipYfGg3CbMVTOfi0pSNcA1Kg1QEzlRakHVmi/M6wrkgTokBUIviB8u5o1/4n38yyjQdSccNF0/36a4GPTTr/3B+bfe6Xh12h32gX1ixyxlJ+yU/WBnbMQ4+83+sEf21HqO2tFutPfPGrVWNUesEdH7FyLlu4Y=</latexit>

ṽthere
<latexit sha1_base64="QSoRCkf87IlNc9Gq/G0kaVBeXAQ=">AAACVnicZZBNS8NAEIY38aNav1o9ihAtgqeSVMFrwYsnUbFaMKVstlNd3I+QnWpkydlf41X/iv4ZcVt7MO3AwjvPvjPMTJIKbjAMvz1/YXFpubKyWl1b39jcqtW3b40eZQw6TAuddRNqQHAFHeQooJtmQGUi4C55Ohv/3z1DZrhWN/iaQk/SB8WHnFF0qF/bj5GLAdhYUnxMhva5KPo2RsjR4iNk4NJaI2yGkwjmRTQVDTKNy37d24sHmo0kKGSCGnMfhSn2LM2QMwFFNR4ZSCl7og9w76SiEkzPTnYpgkNHBsFQZ+4pDCb0f4VF7uzlJkZSISDTqWuj4AXz8TIli6XSzMNElvMhzwXmLZihQlMsI8NHimM+43PzmhSYowZQUq7GxN6M5w0u4CW41pKqouoOGs2eb17ctprRcbN1ddJod6enXSG75IAckYickjY5J5ekQxh5I+/kg3x6X96Pv+RX/qy+N63ZIaXwa79Birml</latexit>

ṽEOS
<latexit sha1_base64="skflkg9J7EjOY2CqSOBfqOJd/GE=">AAACVHicZZBLS8NAEMc38V1fVY8iBovgqSRV8CqI4Ml3tWBK2WynuriPkJ1qZMnRT+NVv4rgd/HgtvZg2oGF//z2P8PMJKngBsPw2/Onpmdm5+YXKotLyyur1bX1W6P7GYMm00JnrYQaEFxBEzkKaKUZUJkIuEuejgf/d8+QGa7VDb6m0Jb0QfEeZxQd6lS3Y+SiCzaWFB+Tnn0uio6NEXK0J+fXLqnWwno4jGBSRCNRI6O46Kx5W3FXs74EhUxQY+6jMMW2pRlyJqCoxH0DKWVP9AHunVRUgmnb4SZFsOtIN+jpzD2FwZD+r7DInb3cxEgqBGQ6dW0UvGA+WKVksVSaSZjIct7jucC8AWNUaIplZHhfcczHfG5ekwJz1ABKytWA2JvBvMEZvARXWlJVVNxBo/HzTYrbRj3arzcuD2pHrdFp58km2SF7JCKH5IickgvSJIy8kXfyQT69L+/Hn/Jn/qy+N6rZIKXwV34BsXi4YA==</latexit>

ṽEOS
<latexit sha1_base64="skflkg9J7EjOY2CqSOBfqOJd/GE=">AAACVHicZZBLS8NAEMc38V1fVY8iBovgqSRV8CqI4Ml3tWBK2WynuriPkJ1qZMnRT+NVv4rgd/HgtvZg2oGF//z2P8PMJKngBsPw2/Onpmdm5+YXKotLyyur1bX1W6P7GYMm00JnrYQaEFxBEzkKaKUZUJkIuEuejgf/d8+QGa7VDb6m0Jb0QfEeZxQd6lS3Y+SiCzaWFB+Tnn0uio6NEXK0J+fXLqnWwno4jGBSRCNRI6O46Kx5W3FXs74EhUxQY+6jMMW2pRlyJqCoxH0DKWVP9AHunVRUgmnb4SZFsOtIN+jpzD2FwZD+r7DInb3cxEgqBGQ6dW0UvGA+WKVksVSaSZjIct7jucC8AWNUaIplZHhfcczHfG5ekwJz1ABKytWA2JvBvMEZvARXWlJVVNxBo/HzTYrbRj3arzcuD2pHrdFp58km2SF7JCKH5IickgvSJIy8kXfyQT69L+/Hn/Jn/qy+N6rZIKXwV34BsXi4YA==</latexit>

vEOS
<latexit sha1_base64="tRUSF6nHRRRB4fkdw5SiXEXYY7Q=">AAACTHicZZBNS8NAEIY39bt+VT14ECFYBE8lqYLXggietGqrBVvKZp3q0v0I2UkbCfk1XvWvePd/eBPBbe3BtAML7zz7zjAzQSi4Qc/7dApz8wuLS8srxdW19Y3N0tb2ndFxxKDJtNBRK6AGBFfQRI4CWmEEVAYC7oP+2ej/fgCR4Vo18CWEjqRPivc4o2hRt7TblhSfg146yLppGyHB9PzqNsu6pbJX8cbhzgp/IspkEvXulrPfftQslqCQCWrMg++F2ElphJwJyIrt2EBIWZ8+wYOVikownXS8QeYeWvLo9nRkn0J3TP9XpMitPd/ESCoERDq0bRQMMRltkrOkVJpZGMh83uOJwKQKU1RoinlkeKw4JlM+O68JgVlqACXlakTSxmhe9xKG7o2WVGVFe1B/+nyz4q5a8Y8r1euTcq01Oe0y2SMH5Ij45JTUyAWpkyZhJCOv5I28Ox/Ol/Pt/PxZC86kZofkorD4C0brtqg=</latexit>

vin
<latexit sha1_base64="dUI8NYgVw0Et9jufmvT8yldJ+bA=">AAACS3icZZDNSsNAEMc39avWr1bBiwjBIngqSRW8Frx4kiptLZhSNutUF/cjZKcaiXkZr/oqPoDP4U08uK05mHZg4T+//c8wM2EkuEHP+3RKC4tLyyvl1cra+sbmVrW23TN6HDPoMi103A+pAcEVdJGjgH4UA5WhgOvw4Wzyf/0IseFadfA5goGkd4qPOKNo0bC6G0iK9+EofcyGaYCQYMpVlg2rda/hTcOdF34u6iSP9rDm7Ae3mo0lKGSCGnPjexEOUhojZwKySjA2EFH2QO/gxkpFJZhBOl0gcw8tuXVHOrZPoTul/ytS5NZebGIkFQJiHdk2Cp4wmSxSsKRUmnkYymI+4onApAkzVGiKRWT4WHFMZnx2XhMBs9QASsrVhKSdybzuBTy5V1pSlVXsQf3Z882LXrPhHzealyf1Vj8/bZnskQNyRHxySlrknLRJlzDyQl7JG3l3Ppwv59v5+bOWnLxmhxSitPQL/CG2jg==</latexit>

vsan
<latexit sha1_base64="O8ZEjVNPDYs9jd+9z0P8DLIezZk=">AAACTHicZZDLSsNAFIYn9VbrrdWFCxGCRXBVkiq4Fdy4kiqtFpoSJtNTHZxLyJzWSMjTuNVXce97uBPBae3CtAcG/vnmP4czfxQLbtDzPp3S0vLK6lp5vbKxubW9U63t3hk9Shh0mBY66UbUgOAKOshRQDdOgMpIwH30dDl5vx9DYrhWbXyJoS/pg+JDzihaFFb3A0nxMRpm4zzMAoQUM0NVnofVutfwpuUuCn8m6mRWrbDmHAYDzUYSFDJBjen5Xoz9jCbImYC8EowMxJQ90QfoWamoBNPPpj/I3WNLBu5QJ/YodKf0f0eG3NqLQ4ykQkCiYztGwTOmk58ULBmVZhFGsngf8lRg2oQ5KjTFIjJ8pDimcz67r4mBWWoAJeVqQrL2ZF/3Gp7dWy1toBUbqD8f36K4azb800bz5qx+0Z1FWyYH5IicEJ+ckwtyRVqkQxjJySt5I+/Oh/PlfDs/f9aSM+vZI4Uqrf4C7P+3Aw==</latexit>

vfrancisco
<latexit sha1_base64="xTjSwpUUGrcyEFkfyiBsC91uCTE=">AAACUnicZZJNS8NAEIY39avWr1aPUggWwVNJquBV8OJJqrS2YErYrBNd3I+QndZIyMlf41X/ihf/iie3tQfTDiy8++w7w8ywUSK4Qc/7diorq2vrG9XN2tb2zu5evbF/Z/Q4ZdBnWuh0GFEDgivoI0cBwyQFKiMBg+j5cvo+mEBquFY9fE1gJOmj4jFnFC0K681AUnyK4nxShHmAkGEep1QxbpguirDe8treLNxl4c9Fi8yjGzacZvCg2ViCQiaoMfe+l+AopylyJqCoBWMDCWXP9BHurVRUghnlszkK99iSBzfWqT0K3Rn9n5Ejt/ZyESOpEJDqxJZR8ILZdJ6SJafSLMNIlu8xzwRmHVigQlMsI8PHimO24LP9mgSYpQZQUq6mJO9N+3Wv4cW91ZKqomYX6i+ub1ncddr+abtzc9a6GM5XWyWH5IicEJ+ckwtyRbqkTxh5I+/kg3w6X85Pxf6SP2vFmecckFJUtn8BQRq3tw==</latexit>

vCA
<latexit sha1_base64="R5nueB94Bot8fZ8CK0/4vpS/df4=">AAACS3icZZDNSsNAEMc31fpRv1oFLyIEi+CpJFXwWunFk6hYLdhSNttpXbofITvVSMzLeNVX8QF8Dm/iwU3twbQDC//57X+GmQlCwQ163qdTWFgsLi2vrJbW1jc2t8qV7VujxxGDFtNCR+2AGhBcQQs5CmiHEVAZCLgLRs3s/+4RIsO1usHnELqSDhUfcEbRol55tyMpPgSD5DHtJR2EGJPmWZr2ylWv5k3CnRf+VFTJNC57FWe/09dsLEEhE9SYe98LsZvQCDkTkJY6YwMhZSM6hHsrFZVguslkgdQ9tKTvDnRkn0J3Qv9XJMitPd/ESCoERDq0bRQ8YZwtkrMkVJp5GMh8PuCxwLgOM1Roinlk+FhxjGd8dl4TArPUAErKVUaSm2xe9wKe3GstqUpL9qD+7PnmxW295h/X6lcn1UZ7etoVskcOyBHxySlpkHNySVqEkRfySt7Iu/PhfDnfzs+fteBMa3ZILgrFX2Tltjs=</latexit>

vany
<latexit sha1_base64="6Lb3wZGOmz8Ospfm+NcFdQCMzK8=">AAACTHicZZBNS8NAEIY39avWr1YPHkQIFsFTSargVfDiSaq0ttCUslmnurgfITutkZBf41X/inf/hzcR3NYcTDuw8M6z7wwzE0aCG/S8T6e0tLyyulZer2xsbm3vVGu7d0aPYwYdpoWOeyE1ILiCDnIU0ItioDIU0A2fLqf/3QnEhmvVxpcIBpI+KD7ijKJFw+p+ICk+hqN0kg3TACHBlKqXLBtW617Dm4W7KPxc1EkerWHNOQzuNRtLUMgENabvexEOUhojZwKySjA2EFH2RB+gb6WiEswgnW2QuceW3LsjHdun0J3R/xUpcmsvNjGSCgGxjmwbBc+YTDcpWFIqzSIMZTEf8URg0oQ5KjTFIjJ8rDgmcz47r4mAWWoAJeVqStL2dF73Gp7dWy2pyir2oP78+RbFXbPhnzaaN2f1i15+2jI5IEfkhPjknFyQK9IiHcJIRl7JG3l3Ppwv59v5+bOWnLxmjxSitPoL99S3CQ==</latexit>

vgood
<latexit sha1_base64="9SXYQglSje8++XyePlujPeQnIDY=">AAACTXicZZBNS8NAEIY39bt+Vb0IIgSL4KkkVfBa8OJJqrRaMCVsttO6uB8hO7WREH+NV/0rnv0h3kTc1h5MO7DwzrPvDDMTxYIb9LxPp7SwuLS8srpWXt/Y3Nqu7OzeGj1MGLSZFjrpRNSA4ArayFFAJ06AykjAXfR4Mf6/e4LEcK1a+BxDV9KB4n3OKFoUVvYDSfEh6mdPeZgFCClmA617eR5Wql7Nm4Q7L/ypqJJpNMMd5zDoaTaUoJAJasy978XYzWiCnAnIy8HQQEzZIx3AvZWKSjDdbLJC7h5b0nP7OrFPoTuh/ysy5NZebGIkFQISHds2CkaYjlcpWDIqzTyMZDHv81RgWocZKjTFIjJ8qDimMz47r4mBWWoAJeVqTLLWeF73CkbujZZU5WV7UH/2fPPitl7zT2v167NqozM97So5IEfkhPjknDTIJWmSNmHkhbySN/LufDhfzrfz82ctOdOaPVKI0sov1vS3dA==</latexit>

vmexican
<latexit sha1_base64="ATlLNUJV9F2m9cvf/2cpSOiy5vE=">AAACUHicZZBNS8NAEIYn9bt+VT2KECyCp5JUwavgxZNUabVgS9msU13cj5Cd1kjIwV/jVf+KN/+JN93WCqYdWPbdZ94dZiaKpbAUBJ9eaW5+YXFpeaW8ura+sVnZ2r62ZpBwbHEjTdKOmEUpNLZIkMR2nCBTkcSb6PFslL8ZYmKF0U16jrGr2L0WfcEZOdSr7HYUo4eonw3zXtYhTClTmLq0zvNepRrUgnH4syKciCpMotHb8vY6d4YPFGrikll7GwYxdTOWkOAS83JnYDFm/JHd462Tmim03Ww8Re4fOHLn903ijiZ/TP//yEg4e7GIVUxKTEzsymh8onQ0TcGSMWVnYaSK775IJaV1nKLSMCoiKwZaUDrlc/3aGLmjFkkxoUcka4769S/wyb8yyi207BYaTq9vVlzXa+FRrX55XD1tT1a7DLuwD4cQwgmcwjk0oAUcXuAV3uDd+/C+vO+S92v9u2EHClEq/wCDVbfP</latexit>

vrestaurants
<latexit sha1_base64="cWEROB2nrv/v3ZHxtq5veIZ7n80=">AAACVHicZZBNT9tAEIbXpnw0QAjtsapqESFxiuxQqVckLj0hqAiJRKJovExglf2wdsfEyPKRX9Mr/StI/BcO3aQ51MlIK73z7DujmUkzKRzF8VsQbnzY3Nre+djY3dtvHrQOP904k1uOPW6ksYMUHEqhsUeCJA4yi6BSif10ej7/7z+idcLoa3rKcKTgXouJ4EAejVvfhgroIZ2Uj9W4HBIWVFp0BLkFTa6qxq123IkXEa2LZCnabBmX48Pg6/DO8FyhJi7BudskzmhUgiXBJVaNYe4wAz6Fe7z1UoNCNyoXm1TRsSd30cRY/zRFC/p/RUnC2+tNnAIp0ZrMt9E4o2K+Uc1SgnLrMFX1fCIKSUUXV6g0QHXkRK4FFSs+P6/LkHvqkBQIPSfl9Xze6AJn0S+jQFcNf9Bk9Xzr4qbbSU473avv7bPB8rQ77As7YicsYT/YGfvJLlmPcfbMfrMX9id4Dd7DjXDznzUMljWfWS3C5l99ubjP</latexit>

vthere
<latexit sha1_base64="xGGszJWh+UL39SAfaiMzXfuZiCY=">AAACTnicZZBNS8NAEIY39avWr6onESFYBE8lqYLXghdPomJtoS1ls522S/cjZKdtJAR/jVf9K179I95Et7UH0w4svPPsO8PMBKHgBj3v08mtrK6tb+Q3C1vbO7t7xf2DJ6NHEYMa00JHjYAaEFxBDTkKaIQRUBkIqAfD6+l/fQyR4Vo94nMIbUn7ivc4o2hRp3jUkhQHQS8Zp52khRBjggOIIE07xZJX9mbhLgt/LkpkHnedfeek1dVsJEEhE9SYpu+F2E5ohJwJSAutkYGQsiHtQ9NKRSWYdjLbIXXPLOm6PR3Zp9Cd0f8VCXJrzzYxkgoBkQ5tGwUTjKe7ZCwJlWYZBjKb93gsMK7AAhWaYhYZPlIc4wWfndeEwCw1gJJyNSXJ43Re9xYm7oOWVKUFe1B/8XzL4qlS9i/KlfvLUrUxP22eHJNTck58ckWq5IbckRph5IW8kjfy7nw4X8638/NnzTnzmkOSiVz+F9ACt+0=</latexit>

vEOS
<latexit sha1_base64="tRUSF6nHRRRB4fkdw5SiXEXYY7Q=">AAACTHicZZBNS8NAEIY39bt+VT14ECFYBE8lqYLXggietGqrBVvKZp3q0v0I2UkbCfk1XvWvePd/eBPBbe3BtAML7zz7zjAzQSi4Qc/7dApz8wuLS8srxdW19Y3N0tb2ndFxxKDJtNBRK6AGBFfQRI4CWmEEVAYC7oP+2ej/fgCR4Vo18CWEjqRPivc4o2hRt7TblhSfg146yLppGyHB9PzqNsu6pbJX8cbhzgp/IspkEvXulrPfftQslqCQCWrMg++F2ElphJwJyIrt2EBIWZ8+wYOVikownXS8QeYeWvLo9nRkn0J3TP9XpMitPd/ESCoERDq0bRQMMRltkrOkVJpZGMh83uOJwKQKU1RoinlkeKw4JlM+O68JgVlqACXlakTSxmhe9xKG7o2WVGVFe1B/+nyz4q5a8Y8r1euTcq01Oe0y2SMH5Ij45JTUyAWpkyZhJCOv5I28Ox/Ol/Pt/PxZC86kZofkorD4C0brtqg=</latexit>

vEOS
<latexit sha1_base64="tRUSF6nHRRRB4fkdw5SiXEXYY7Q=">AAACTHicZZBNS8NAEIY39bt+VT14ECFYBE8lqYLXggietGqrBVvKZp3q0v0I2UkbCfk1XvWvePd/eBPBbe3BtAML7zz7zjAzQSi4Qc/7dApz8wuLS8srxdW19Y3N0tb2ndFxxKDJtNBRK6AGBFfQRI4CWmEEVAYC7oP+2ej/fgCR4Vo18CWEjqRPivc4o2hRt7TblhSfg146yLppGyHB9PzqNsu6pbJX8cbhzgp/IspkEvXulrPfftQslqCQCWrMg++F2ElphJwJyIrt2EBIWZ8+wYOVikownXS8QeYeWvLo9nRkn0J3TP9XpMitPd/ESCoERDq0bRQMMRltkrOkVJpZGMh83uOJwKQKU1RoinlkeKw4JlM+O68JgVlqACXlakTSxmhe9xKG7o2WVGVFe1B/+nyz4q5a8Y8r1euTcq01Oe0y2SMH5Ij45JTUyAWpkyZhJCOv5I28Ox/Ol/Pt/PxZC86kZofkorD4C0brtqg=</latexit>

ṽEOS
<latexit sha1_base64="skflkg9J7EjOY2CqSOBfqOJd/GE=">AAACVHicZZBLS8NAEMc38V1fVY8iBovgqSRV8CqI4Ml3tWBK2WynuriPkJ1qZMnRT+NVv4rgd/HgtvZg2oGF//z2P8PMJKngBsPw2/Onpmdm5+YXKotLyyur1bX1W6P7GYMm00JnrYQaEFxBEzkKaKUZUJkIuEuejgf/d8+QGa7VDb6m0Jb0QfEeZxQd6lS3Y+SiCzaWFB+Tnn0uio6NEXK0J+fXLqnWwno4jGBSRCNRI6O46Kx5W3FXs74EhUxQY+6jMMW2pRlyJqCoxH0DKWVP9AHunVRUgmnb4SZFsOtIN+jpzD2FwZD+r7DInb3cxEgqBGQ6dW0UvGA+WKVksVSaSZjIct7jucC8AWNUaIplZHhfcczHfG5ekwJz1ABKytWA2JvBvMEZvARXWlJVVNxBo/HzTYrbRj3arzcuD2pHrdFp58km2SF7JCKH5IickgvSJIy8kXfyQT69L+/Hn/Jn/qy+N6rZIKXwV34BsXi4YA==</latexit>

ṽin
<latexit sha1_base64="qV3G5eW4tWAE3+el428gyaUxAqk=">AAACU3icZZBNS8NAEIY38avWr1aPogSL4KkkVfAqePEkKlYLppTNdqqL+xGy0xpZcvPXeNW/4sHf4sVt7cG0AwvvPPvOMDNJKrjBMPz2/IXFpeWVymp1bX1jc6tW374zepgxaDMtdNZJqAHBFbSRo4BOmgGViYD75Pl8/H8/gsxwrW7xNYWupI+KDzij6FCvth8jF32wsaT4lAzsqCh6NkbI0XLldK0RNsNJBPMimooGmcZVr+7txX3NhhIUMkGNeYjCFLuWZsiZgKIaDw2klD3TR3hwUlEJpmsnixTBoSP9YKAz9xQGE/q/wiJ39nITI6kQkOnUtVHwgvl4k5LFUmnmYSLL+YDnAvMWzFChKZaR4UPFMZ/xuXlNCsxRAygpV2Nib8fzBpfwEtxoSVVRdQeNZs83L+5azei42bo+aZx1pqetkF1yQI5IRE7JGbkgV6RNGHkj7+SDfHpf3o/v+4t/Vt+b1uyQUvgbv2NHuEY=</latexit>

h1[1 : 2]
<latexit sha1_base64="EvCzU7zwdNe7ZyYhD6Y4G1C/G4o=">AAACGHicdVDLSsNAFJ34rPVVdelmsAiuQlIVxVXBjcsKtg2koUymk3boTBJmbsQS+hMu1Y9xJ27d+S1unLQVrI8DA4dz7517zwlTwTU4zru1sLi0vLJaWiuvb2xubVd2dls6yRRlTZqIRHkh0UzwmDWBg2BeqhiRoWDtcHhZ1Nu3TGmexDcwSlkgST/mEacEjOQNuq7vXtSCbqXq2LVTxwD/Jq7tTFBFMzS6lY9OL6GZZDFQQbT2XSeFICcKOBVsXO5kmqWEDkmf+YbGRDId5JN7x/jQKD0cJcq8GPBE/T6RE6n1SIamUxIY6J+1Qvyr5mcQnQc5j9MMWEyni6JMYEhwYR73uGIUxMgQQhU3t2I6IIpQMBHNbSn+TkHezTnJgRsb47JJ6ysS/D9p1Wz32K5dn1Tr3iy3EtpHB+gIuegM1dEVaqAmokige/SInqwH69l6sV6nrQvWbGYPzcF6+wTh46C/</latexit>

Sl
ot

 E
nc

od
er

 

Current
Intent

h1[3 : 3]
<latexit sha1_base64="RGRXn/67wyxj7ASeU3EFq5FbAjo=">AAACO3icdZDLSsNAFIYnXmu9tboUIVgEVyVpFcFVwY0rqdJLoA1lMj1th84lZCa2EvoSbvVVfBDX7sSte6e1gmnxh4FzvvnP4ecEIaNKO86btbK6tr6xmdnKbu/s7u3n8gcNJeOIQJ1IJiMvwAoYFVDXVDPwwggwDxg0g+H19L/5AJGiUtT0Ywg+x31Be5RgbZA36Lit8lXZ7+QKTrF04RjZy4VbdGYqoLmqnbx13O5KEnMQmjCsVMt1Qu0nONKUMJhk27GCEJMh7kPLlAJzUH4yCzyxTw3p2j0ZmSe0PaN/JxJNjT29RHHMGEQyNGsEjPSYYz1IWRLM1TIMeLrv0THT4xIsUCaxTiNFY0H1eMFn8qoQiKEKNMdUTElSm+a1b2Fk30uOxSRrDvp7Nfv/olEquuVi6e68UPHmp82gI3SCzpCLLlEF3aAqqiOCGHpCz+jFerXerQ/r88e6Ys1nDlFK1tc3MFSvsg==</latexit>

h0[2 : 3]
<latexit sha1_base64="Qibxjv4yQRFoPo26/Tym01potgE=">AAACO3icdZDLSsNAFIYnXmu9tboUIVgEVyVNFcFVwY0rqdJLoA1lMj1th84lZCa2EvoSbvVVfBDX7sSte6e1gmnxh4FzvvnP4ecEIaNKO86btbK6tr6xmdnKbu/s7u3n8gcNJeOIQJ1IJiMvwAoYFVDXVDPwwggwDxg0g+H19L/5AJGiUtT0Ywg+x31Be5RgbZA36Dgt96rsd3IFp+heOEb2clEqOjMV0FzVTt46bncliTkITRhWqlVyQu0nONKUMJhk27GCEJMh7kPLlAJzUH4yCzyxTw3p2j0ZmSe0PaN/JxJNjT29RHHMGEQyNGsEjPSYYz1IWRLM1TIMeLrv0THTYxcWKJNYp5GisaB6vOAzeVUIxFAFmmMqpiSpTfPatzCy7yXHYpI1B/29mv1/0XCLpXLRvTsvVLz5aTPoCJ2gM1RCl6iCblAV1RFBDD2hZ/RivVrv1of1+WNdseYzhygl6+sbLKyvsA==</latexit>

Candidate Slots

Dialogue

Encoding

Figure 2: General architecture of the proposed contextual carryover model. Bi-LSTM is used to encode the
utterances in dialogue into a fixed length dialogue representation and also get contextual slot value embeddings.
Slot encoder uses the slot key, value and distance to create a fixed length slot embedding for each of the candidate
slots. Given the encoded slots, intent and dialogue context, decoder selects the subset of slots that are relevant for
the current user request.

The overall architecture of the model is shown
in Figure 2. We elaborate on the specific designs
of these components under this general architec-
ture.

3.2 Slot Encoder Variants
In this section, we describe the different encoding
methods that we use to encode slots.

We average the word embeddings of the tokens
in the slot key as the slot key encoding:

xkey =
1
K

K∑
i=1

v(ki) . (5)

where v(w) is the embedding vector of token w.
For the slot value (the tokens hd[l : r]), we pro-

pose following encoding approaches.

CTXavg The first is to average the token embed-
dings of the tokens in the slot value:

xval =
1

r − l + 1
r∑
i=l

v(hd[i]) ; (6)

CTXLSTM To get improved contextualized rep-
resentation of the slot value in dialogue, we also
use neural network models to encode slots. We ex-
perimented with bidirectional LSTM (Hochreiter
and Schmidhuber, 1997) model for slot encoding.
LSTMs are equipped with feedback loops in their
recurrent layer, which helps store contextual infor-
mation over a long history. We encode all dialogue
utterances with BiLSTM to obtain contextualized
vector representations ṽ(w) for each token w, then
average the output hidden states of the tokens in
the span [l : r] to get the slot value encoding.

xval =
1

r − l + 1
r∑
i=l

ṽ(hd[i]); (7)

Additionally, distance may contain important
signals. This integer, being odd or even, provides
information on whether this utterance is uttered by
a user or the system. The smaller it is, the closer
a slot is to the current utterance, hence implic-
itly more probable to be carried over. Building on
these intuitions, we encode the distance as a small



99

vector (xdist, 4 dimensions) and append it to the
overall slot encoding:

x =
[
xkey ; xval ; xdist

]
. (8)

3.3 Decoder Variants

x1 x2 x3 x4EOS

e1 e2 e3 e4eEOS

Bi-LSTM 

d1 d2 d3

BOS

q1

x4

q2

q3

x1

Figure 3: Architecture of the pointer network decoder.
In this case, the pointer network selects x4, x1 succes-
sively and stops after selecting EOS.

Pointer network decoder We adopt the archi-
tecture of the pointer network (Vinyals et al.,
2015) as a method to perform joint prediction of
the slots to be carried over. Pointer networks,
a variant of Seq2Seq (Bahdanau et al., 2015;
Sutskever et al., 2014; Luong et al., 2015) model,
instead of transducing the input sequence into an-
other output sequence, yields a succession of soft
pointers (attention vectors) to the input sequence,
hence producing an ordering of the elements of a
variable-length input sequence.

We use a pointer network to select a subset of
the slots from the input slot set. The input slot en-
codings are ordered as a sequence, then fed into
a bidirectional LSTM encoder to yield a sequence
of encoded hidden states. We experiment with dif-
ferent slot orderings as described in section 4.

e0:n = BiLSTM([xEOS , x1:n]) (9)

Here a special sentinel token EOS is appended to
the beginning of the input to the pointer network –
when decoding, once the output pointer points to
this EOS token, the decoding process stops.

Given the hidden states, e0:n, the decoding pro-
cess at every time step i is computed and updated
as shown in Algorithm 1.

Contrary to normal attention-based models
which directly uses the decoder state (di) as the
query, we incorporate the context vector (c) and
the intent vector (i) into the attention query. The

Algorithm 1 Pointer network decoding
1: procedure PTRNETDEC(x0:n,e0:n,d0,c, i)
2: i ← 0
3: y0 ← BOS . special BOS token
4: m0:n ← TRUE . every slot is available
5: repeat
6: i ← i + 1
7: di ← LSTM(di−1,xyi−1) . update state
8: qi ← FQ(di,c, i) . constructs query
9: ai j ← FA(qi,ej) . attention scores

10: pi j ←
exp ai j∑

m j=TRUE

exp ai j
. soft pointer

11: ŷi ← arg max
m j=TRUE

pi j . predicted output

12: if at inference time then
13: yi ← ŷi . no gold output
14: end if
15: myi ← FALSE . update mask
16: until yi = 0 . index of EOS is 0
17: return ŷ1:i−1 . return all generated ŷ’s
18: end procedure

query vector is a concatenation of the three com-
ponents:

qi = FQ(di,c, i) = [ di ; c ; i ] . (10)

We use the general Luong attention (Luong
et al., 2015) scoring function (bilinear form):

ai j = FA(qi,ej) = qTi Wej . (11)

As a subset output is desired, the output ŷi
should be distinct at each step i. To this end, we
utilize a dynamic mask in the decoding process:
for every input slot encoding xj a Boolean mask
variable mj is set to TRUE. Once a specific slot
is generated, it is crossed out – its correspond-
ing mask is set to FALSE, and further pointers will
never attend to this slot again. Hence distinctness
of the output sequence is ensured.

Self-attention decoder The pointer network as
introduced previously yields a succession of point-
ers that select slots based on attention scores,
which allows the model to look back and forth
over entire slot sequence for slot dependency mod-
eling. Similar to the pointer network, the self-
attention mechanism is also capable of modeling
relationships between all slots in the dialogue, re-
gardless of their respective positions. To com-
pute the representation of any given slot, the self-
attention model compares it to every other slot in



100

the dialogue. The result of these comparisons is
attention scores which determine how much each
of the other slots should contribute to the repre-
sentation of the given slot. In this section, we also
propose to use the self-attention mechanism with
the neural transformer networks (Vaswani et al.,
2017) to model slot interdependencies for the task.

One major component in the transformer is
the multi-head self-attention unit. Rather than
only computing the attention once, the multi-head
mechanism runs through the scaled dot-product
attention multiple times and allows the model to
jointly attend to information from different per-
spectives at different positions, which is empiri-
cally shown to be more powerful than a single at-
tention head (Vaswani et al., 2017). In our con-
figurations, we increase the number of heads Z ,
as described in section 4. The independent atten-
tion head g outputs are simply concatenated and
linearly transformed into the expected output.

Given the input slot encodings x1:n, we compute
the self-attention as follows:

qzi =W
z
QFQ(xi) (12)

kzi =W
z
Kxi (13)

azi j = FA(q
z
i ,k

z
j ) (14)

pzi j =
exp azi j∑
j

exp azi j
(15)

ozi =
∑
j

pzi jk
z
j (16)

x̃i =WO
[
o0i ; · · · ; oZ−1i

]
+ bO (17)

where the superscript 0 ≤ z < Z is the head
number. We model the query construction, Equa-
tion 12, and the attention score, Equation 14, in
the same way as their counterparts (Equation 10
and Equation 11) in the previous pointer network
model. The self-attended representation of slot i,
x̃i, is a representation of slot i with the relations to
all other slots taken into account.

We derive the final decision over whether to
carry over a slot as a 2-layer feedforward neural
network atop the features xi, x̃i, context vector (c)
and the intent vector (i):

yi = σ(W2 ·ReLU(W1[ xi ; x̃i ; c ; i ]+b1)+b2) .

This creates a highway network connection (Sri-
vastava et al., 2015) that connects the input and
the self-attention transformed encodings.

Split Slot distance
0 1 2 ≥3

Train Positive 183K 48K 6.7K 591Total 183K 327K 111K 108K

Dev Positive 22K 6.0K 785 66Total 22K 40K 13K 13K

Test Positive 23K 6.1K 807 85Total 23K 41K 13K 14K

Table 1: Internal Dataset breakdown showing the
number of carryover candidate slots at different dis-
tances. ‘Total’ shows the total number of candidate
slots and ‘Positive’ shows the number of candidate slots
that are relevant for the current turn.

Split Slot distance
0 2 4 ≥6

Train Positive 4.6K 3.8K 3.7K 9.6KTotal 5.2K 4.9K 4.7K 14.5K

Dev Positive 1.4K 1.2K 1.1K 3.0KTotal 1.7K 1.6K 1.5K 5.0K

Test Positive 4.1K 3.2K 3.0K 9.4KTotal 4.8K 4.2K 3.9K 15.2K

Table 2: DSTC2 Dataset breakdown showing the num-
ber of carryover candidate slots at different distances.
‘Total’ shows the total number of candidate slots and
‘Positive’ shows the number of candidate slots that rep-
resent the user goal at the current turn.

4 Experiments

4.1 Datasets

We evaluate our approaches on both internal and
external datasets. The internal dataset contains di-
alogues collected specifically for reference resolu-
tion, while the external dataset was collected for
dialogue state tracking.

Internal This dataset is made up of a subset
of user-initiated dialogue data collected from a
commercial voice-based digital assistant. This
dataset has 156K dialogues from 7 domains –
Music, Q&A, Video, Weather, Local Businesses
and Home Automation. Each domain has its own
schema. There are ∼13 distinct slot keys per do-
main and only 20% of these keys are reused in
more than one domain. To handle dialogue data
belonging to a diverse schema, slots in dialogue
are converted into candidate slots in the schema
associated with the current domain. We follow the



101

same slot candidate generation recipe by leverag-
ing slot key embedding similarities as in Naik et al.
(2018). These candidates are then presented to the
models for selecting a subset of relevant candidate
slots. Statistics for the candidate slots in the train,
development, and test sets broken down by slot
distances are shown in Table 1.

DSTC2 The DSTC2 dataset (Henderson et al.,
2014) contains system-initiated dialogues between
human and dialogue systems in restaurant booking
domain. We use top ASR hypothesis as the user
utterance and use all the slots from n-best SLU
with score > 0.1 as candidate slots. These candi-
dates are then presented to the models for select-
ing a subset of candidate slots which represent the
user goal. Statistics for the candidate slots in the
train, development, and test sets broken down by
slot distances are shown in Table 2. Since only the
user mentioned slots contribute to the user-goal,
there are no candidates with odd-numbered slot
distances.

4.2 Experimental setup

For all the models, we initialize the word embed-
dings using fastText embeddings (Lample et al.,
2018). The models are trained using mini-batch
SGD with Adam optimizer (Kingma and Ba,
2015) with a learning rate of 0.001 to minimize the
negative log-likelihood loss. We set the dropout
rate of 0.3 for our models during training. In
our experiments, we use 300 dimensions for the
LSTM hidden states in the pointer network en-
coder and decoder. Our transformer decoder has 1
layer, Z = 80 heads, dk = dv = 64 for the projec-
tion size of keys and values in the attention heads.
We do not use positional encoding for the trans-
former decoder. All pointer network model setups
are trained for 40 epochs, our transformer models
are trained for 200 epochs. For evaluation on the
test set, we pick the best model based on perfor-
mance on dev set. We use standard definitions of
precision, recall, and F1 by comparing the refer-
ence slots with the model hypothesis slots.

4.3 Results and discussion

We compare our models against the baseline
model – encoder-decoder with word attention ar-
chitecture described by Naik et al. (2018). Table 3
shows the performance of the models for slots at
different distances on Internal dataset.

Impact of slot ordering Using pointer network
model, we experiment with the following slot or-
derings to measure the impact of the order on car-
ryover performance. no order – slots are ordered
completely randomly. turn-only order – slots are
ordered based on their slot distance, but the slots
with the same distance (i.e., candidates generated
from the same contextual turn) are ordered ran-
domly. temporal order – slots are ordered based
on the order in which they occur in the dialogue.

Partial ordering slots across turns i.e., turn-only
order significantly improves the carryover perfor-
mance as compared to using no order. Further, en-
forcing within distance order using temporal order
improves the overall performance slightly, but we
see drop in F1 by 7 points for slots at distance ≥3.
indicating that a strict ordering might hurt model
accuracy.

Impact of slot encoding Here, we compare slot
value representations obtained by averaging pre-
trained embeddings (CTXavg) with contextualized
slot value representation obtained from BiLSTM
over complete dialogue(CTXLSTM). The results in
Table 3, show that contextualized slot value rep-
resentation substantially improves model perfor-
mance compared to the non-contextual represen-
tation. This is aligned with the observations on
other tasks using contextual word vectors (Peters
et al., 2018a; Howard and Ruder, 2018; Devlin
et al., 2019).

Impact of decoder Compared to the baseline
model, both the pointer network model and the
transformer model are able to carry over longer
dialogue context due to being able to model the
slot interdependence. With the transformer net-
work, we completely forgo ordering information.
Though the slot embedding includes distance fea-
ture xdist, the actual order in which the slots are
arranged does not matter. We see improvement
in carryover performance for slots at all distances.
While the pointer network seems to deal with
longer context better, the transformer architecture
still gives us the best overall performance.

For completeness, Table 4 shows the perfor-
mance on DSTC2 public dataset, where similar
conclusions hold.

4.4 Error Analysis
To gain deeper insight into the ability of the mod-
els to learn and utilize slot co-occurrence patterns,
we measure the models’ performance on buckets



102

Decoder Slot
Encoder

Slot
Ordering

Slot distance

1 2 ≥3 ≥1

Baseline (Naik et al., 2018) 0.8818 0.6551 0.0000 0.8506

Pointer Network
Decoder

CTXLSTM no order 0.8155 0.5571 0.1290 0.7817
CTXLSTM turn-only order 0.8466 0.6154 0.4095 0.8157
CTXavg temporal order 0.7565 0.4716 0.0225 0.7166
CTXLSTM temporal order 0.8631 0.6623 0.3350 0.8318

Transformer Decoder CTXLSTM 0.8771 0.7035 0.3803 0.8533

Table 3: Carryover performance (F1) of different models for slots at different distances on Internal dataset. The
rightmost column contains the aggregate scores for all slots with distance greater than or equal to 1.

Decoder Slot
Encoder

Slot
Ordering

Slot distance

0 2 4 ≥6

Baseline (Naik et al., 2018) 0.9242 0.9111 0.9134 0.8799

Pointer Network
Decoder

CTXLSTM no order 0.8316 0.8199 0.8183 0.7641
CTXLSTM turn-only order 0.9049 0.8993 0.9145 0.8892
CTXLSTM temporal order 0.9270 0.9204 0.9290 0.9139

Transformer Decoder CTXLSTM 0.9300 0.9269 0.9280 0.8949

Table 4: Carryover performance (F1) of different models for slots at different distances on DSTC2 dataset.

1 2

1 437

2 2011 89

1360 2931 210

<latexit sha1_base64="TctOJNBCSGX/E6A9rWGmK3MKuuI=">AAAChnicbVHLbhMxFHWGAmF4pbBkYxFVYpXMIFARq6hIFcuikrZSEkUe587ExC/ZdxCRNf/YLT/SLTiTWdCWI9k6Ouc+rOPCSuExy373kgcHDx897j9Jnz57/uLl4PDVhTe14zDlRhp3VTAPUmiYokAJV9YBU4WEy2LzZedf/gTnhdHfcWthoVilRSk4wygtBz/m3GiM0yToCtchG2U5qOaInismJTjq2ErUPp0j/MLQ3kUZzpuW+brw3AkbjW5KKCTjm2Zf6Hk4FZrJJiJN0+VgGKe3oPdJ3pEh6XC2POwdzFeG1wo0csm8n+WZxUVgDgWX0KTz2oON+1gFs0g1U+AXoQ2loUdRWdHSuHg00lb9tyMw5f1WFbFSMVz7u95O/J83q7H8tAhC2xpB8/2ispYUDd0lTFfCAUe5jYTFdOJbKV8zxzjGf7i1pQstpr1GtJ/HY45Mj4yrxnZTjTt3l1t+N6X75OL9KI/824fh5KRLsE/ekLfkHcnJMZmQr+SMTAkn1+SG/OmRpJ+Mko/J8b406XU9r8ktJJO//jjHsw==</latexit><latexit sha1_base64="TctOJNBCSGX/E6A9rWGmK3MKuuI=">AAAChnicbVHLbhMxFHWGAmF4pbBkYxFVYpXMIFARq6hIFcuikrZSEkUe587ExC/ZdxCRNf/YLT/SLTiTWdCWI9k6Ouc+rOPCSuExy373kgcHDx897j9Jnz57/uLl4PDVhTe14zDlRhp3VTAPUmiYokAJV9YBU4WEy2LzZedf/gTnhdHfcWthoVilRSk4wygtBz/m3GiM0yToCtchG2U5qOaInismJTjq2ErUPp0j/MLQ3kUZzpuW+brw3AkbjW5KKCTjm2Zf6Hk4FZrJJiJN0+VgGKe3oPdJ3pEh6XC2POwdzFeG1wo0csm8n+WZxUVgDgWX0KTz2oON+1gFs0g1U+AXoQ2loUdRWdHSuHg00lb9tyMw5f1WFbFSMVz7u95O/J83q7H8tAhC2xpB8/2ispYUDd0lTFfCAUe5jYTFdOJbKV8zxzjGf7i1pQstpr1GtJ/HY45Mj4yrxnZTjTt3l1t+N6X75OL9KI/824fh5KRLsE/ekLfkHcnJMZmQr+SMTAkn1+SG/OmRpJ+Mko/J8b406XU9r8ktJJO//jjHsw==</latexit><latexit sha1_base64="TctOJNBCSGX/E6A9rWGmK3MKuuI=">AAAChnicbVHLbhMxFHWGAmF4pbBkYxFVYpXMIFARq6hIFcuikrZSEkUe587ExC/ZdxCRNf/YLT/SLTiTWdCWI9k6Ouc+rOPCSuExy373kgcHDx897j9Jnz57/uLl4PDVhTe14zDlRhp3VTAPUmiYokAJV9YBU4WEy2LzZedf/gTnhdHfcWthoVilRSk4wygtBz/m3GiM0yToCtchG2U5qOaInismJTjq2ErUPp0j/MLQ3kUZzpuW+brw3AkbjW5KKCTjm2Zf6Hk4FZrJJiJN0+VgGKe3oPdJ3pEh6XC2POwdzFeG1wo0csm8n+WZxUVgDgWX0KTz2oON+1gFs0g1U+AXoQ2loUdRWdHSuHg00lb9tyMw5f1WFbFSMVz7u95O/J83q7H8tAhC2xpB8/2ispYUDd0lTFfCAUe5jYTFdOJbKV8zxzjGf7i1pQstpr1GtJ/HY45Mj4yrxnZTjTt3l1t+N6X75OL9KI/824fh5KRLsE/ekLfkHcnJMZmQr+SMTAkn1+SG/OmRpJ+Mko/J8b406XU9r8ktJJO//jjHsw==</latexit><latexit sha1_base64="TctOJNBCSGX/E6A9rWGmK3MKuuI=">AAAChnicbVHLbhMxFHWGAmF4pbBkYxFVYpXMIFARq6hIFcuikrZSEkUe587ExC/ZdxCRNf/YLT/SLTiTWdCWI9k6Ouc+rOPCSuExy373kgcHDx897j9Jnz57/uLl4PDVhTe14zDlRhp3VTAPUmiYokAJV9YBU4WEy2LzZedf/gTnhdHfcWthoVilRSk4wygtBz/m3GiM0yToCtchG2U5qOaInismJTjq2ErUPp0j/MLQ3kUZzpuW+brw3AkbjW5KKCTjm2Zf6Hk4FZrJJiJN0+VgGKe3oPdJ3pEh6XC2POwdzFeG1wo0csm8n+WZxUVgDgWX0KTz2oON+1gFs0g1U+AXoQ2loUdRWdHSuHg00lb9tyMw5f1WFbFSMVz7u95O/J83q7H8tAhC2xpB8/2ispYUDd0lTFfCAUe5jYTFdOJbKV8zxzjGf7i1pQstpr1GtJ/HY45Mj4yrxnZTjTt3l1t+N6X75OL9KI/824fh5KRLsE/ekLfkHcnJMZmQr+SMTAkn1+SG/OmRpJ+Mko/J8b406XU9r8ktJJO//jjHsw==</latexit>

<latexit sha1_base64="gBMf7Nj3k4DO8zz48qWnIvIsnMQ=">AAAChXicbVHLbhMxFHWmQNvhlcKyG4uoEhuSGVQBYkNFN10WlbSVkijyOHcmbvySfQcRWfOPbPkRttSZzIK2HMnW0Tn3YR0XVgqPWfa7l+w8evxkd28/ffrs+YuX/YNXl97UjsOYG2ncdcE8SKFhjAIlXFsHTBUSrorV6ca/+gHOC6O/49rCTLFKi1JwhlGa92+m3GiM0yToCpchG2Y5qOaIXigmJTjq2ELUPp0i/MTQ3kUZLpqW+brw3AkbjW5KKCTjq2Zb6Hk4Zc6tm4g0Tef9QZzegj4keUcGpMP5/KC3M10YXivQyCXzfpJnFmeBORRcQpNOaw827mMVTCLVTIGfhTaUhh5FZUFL4+LRSFv1347AlPdrVcRKxXDp73sb8X/epMby0ywIbWsEzbeLylpSNHSTMF0IBxzlOhIW04lvpXzJHOMY/+HOli60mPYS0X4ejTgyPTSuGtlVNercGFt+P6SH5PL9MI/82/Hg5GsX4B45JG/IW5KTj+SEnJFzMiac/CJ/yN8eSXaTd8lx8mFbmvS6ntfkDpIvt2cxx7o=</latexit><latexit sha1_base64="gBMf7Nj3k4DO8zz48qWnIvIsnMQ=">AAAChXicbVHLbhMxFHWmQNvhlcKyG4uoEhuSGVQBYkNFN10WlbSVkijyOHcmbvySfQcRWfOPbPkRttSZzIK2HMnW0Tn3YR0XVgqPWfa7l+w8evxkd28/ffrs+YuX/YNXl97UjsOYG2ncdcE8SKFhjAIlXFsHTBUSrorV6ca/+gHOC6O/49rCTLFKi1JwhlGa92+m3GiM0yToCpchG2Y5qOaIXigmJTjq2ELUPp0i/MTQ3kUZLpqW+brw3AkbjW5KKCTjq2Zb6Hk4Zc6tm4g0Tef9QZzegj4keUcGpMP5/KC3M10YXivQyCXzfpJnFmeBORRcQpNOaw827mMVTCLVTIGfhTaUhh5FZUFL4+LRSFv1347AlPdrVcRKxXDp73sb8X/epMby0ywIbWsEzbeLylpSNHSTMF0IBxzlOhIW04lvpXzJHOMY/+HOli60mPYS0X4ejTgyPTSuGtlVNercGFt+P6SH5PL9MI/82/Hg5GsX4B45JG/IW5KTj+SEnJFzMiac/CJ/yN8eSXaTd8lx8mFbmvS6ntfkDpIvt2cxx7o=</latexit><latexit sha1_base64="gBMf7Nj3k4DO8zz48qWnIvIsnMQ=">AAAChXicbVHLbhMxFHWmQNvhlcKyG4uoEhuSGVQBYkNFN10WlbSVkijyOHcmbvySfQcRWfOPbPkRttSZzIK2HMnW0Tn3YR0XVgqPWfa7l+w8evxkd28/ffrs+YuX/YNXl97UjsOYG2ncdcE8SKFhjAIlXFsHTBUSrorV6ca/+gHOC6O/49rCTLFKi1JwhlGa92+m3GiM0yToCpchG2Y5qOaIXigmJTjq2ELUPp0i/MTQ3kUZLpqW+brw3AkbjW5KKCTjq2Zb6Hk4Zc6tm4g0Tef9QZzegj4keUcGpMP5/KC3M10YXivQyCXzfpJnFmeBORRcQpNOaw827mMVTCLVTIGfhTaUhh5FZUFL4+LRSFv1347AlPdrVcRKxXDp73sb8X/epMby0ywIbWsEzbeLylpSNHSTMF0IBxzlOhIW04lvpXzJHOMY/+HOli60mPYS0X4ejTgyPTSuGtlVNercGFt+P6SH5PL9MI/82/Hg5GsX4B45JG/IW5KTj+SEnJFzMiac/CJ/yN8eSXaTd8lx8mFbmvS6ntfkDpIvt2cxx7o=</latexit><latexit sha1_base64="gBMf7Nj3k4DO8zz48qWnIvIsnMQ=">AAAChXicbVHLbhMxFHWmQNvhlcKyG4uoEhuSGVQBYkNFN10WlbSVkijyOHcmbvySfQcRWfOPbPkRttSZzIK2HMnW0Tn3YR0XVgqPWfa7l+w8evxkd28/ffrs+YuX/YNXl97UjsOYG2ncdcE8SKFhjAIlXFsHTBUSrorV6ca/+gHOC6O/49rCTLFKi1JwhlGa92+m3GiM0yToCpchG2Y5qOaIXigmJTjq2ELUPp0i/MTQ3kUZLpqW+brw3AkbjW5KKCTjq2Zb6Hk4Zc6tm4g0Tef9QZzegj4keUcGpMP5/KC3M10YXivQyCXzfpJnFmeBORRcQpNOaw827mMVTCLVTIGfhTaUhh5FZUFL4+LRSFv1347AlPdrVcRKxXDp73sb8X/epMby0ywIbWsEzbeLylpSNHSTMF0IBxzlOhIW04lvpXzJHOMY/+HOli60mPYS0X4ejTgyPTSuGtlVNercGFt+P6SH5PL9MI/82/Hg5GsX4B45JG/IW5KTj+SEnJFzMiac/CJ/yN8eSXaTd8lx8mFbmvS6ntfkDpIvt2cxx7o=</latexit>

≥3

≥3

(a) Number of positive in-
stances in the dataset

1 2 ≥3

1 0.883

2 0.922 0.792

0.894 0.939 0.872

<latexit sha1_base64="TctOJNBCSGX/E6A9rWGmK3MKuuI=">AAAChnicbVHLbhMxFHWGAmF4pbBkYxFVYpXMIFARq6hIFcuikrZSEkUe587ExC/ZdxCRNf/YLT/SLTiTWdCWI9k6Ouc+rOPCSuExy373kgcHDx897j9Jnz57/uLl4PDVhTe14zDlRhp3VTAPUmiYokAJV9YBU4WEy2LzZedf/gTnhdHfcWthoVilRSk4wygtBz/m3GiM0yToCtchG2U5qOaInismJTjq2ErUPp0j/MLQ3kUZzpuW+brw3AkbjW5KKCTjm2Zf6Hk4FZrJJiJN0+VgGKe3oPdJ3pEh6XC2POwdzFeG1wo0csm8n+WZxUVgDgWX0KTz2oON+1gFs0g1U+AXoQ2loUdRWdHSuHg00lb9tyMw5f1WFbFSMVz7u95O/J83q7H8tAhC2xpB8/2ispYUDd0lTFfCAUe5jYTFdOJbKV8zxzjGf7i1pQstpr1GtJ/HY45Mj4yrxnZTjTt3l1t+N6X75OL9KI/824fh5KRLsE/ekLfkHcnJMZmQr+SMTAkn1+SG/OmRpJ+Mko/J8b406XU9r8ktJJO//jjHsw==</latexit><latexit sha1_base64="TctOJNBCSGX/E6A9rWGmK3MKuuI=">AAAChnicbVHLbhMxFHWGAmF4pbBkYxFVYpXMIFARq6hIFcuikrZSEkUe587ExC/ZdxCRNf/YLT/SLTiTWdCWI9k6Ouc+rOPCSuExy373kgcHDx897j9Jnz57/uLl4PDVhTe14zDlRhp3VTAPUmiYokAJV9YBU4WEy2LzZedf/gTnhdHfcWthoVilRSk4wygtBz/m3GiM0yToCtchG2U5qOaInismJTjq2ErUPp0j/MLQ3kUZzpuW+brw3AkbjW5KKCTjm2Zf6Hk4FZrJJiJN0+VgGKe3oPdJ3pEh6XC2POwdzFeG1wo0csm8n+WZxUVgDgWX0KTz2oON+1gFs0g1U+AXoQ2loUdRWdHSuHg00lb9tyMw5f1WFbFSMVz7u95O/J83q7H8tAhC2xpB8/2ispYUDd0lTFfCAUe5jYTFdOJbKV8zxzjGf7i1pQstpr1GtJ/HY45Mj4yrxnZTjTt3l1t+N6X75OL9KI/824fh5KRLsE/ekLfkHcnJMZmQr+SMTAkn1+SG/OmRpJ+Mko/J8b406XU9r8ktJJO//jjHsw==</latexit><latexit sha1_base64="TctOJNBCSGX/E6A9rWGmK3MKuuI=">AAAChnicbVHLbhMxFHWGAmF4pbBkYxFVYpXMIFARq6hIFcuikrZSEkUe587ExC/ZdxCRNf/YLT/SLTiTWdCWI9k6Ouc+rOPCSuExy373kgcHDx897j9Jnz57/uLl4PDVhTe14zDlRhp3VTAPUmiYokAJV9YBU4WEy2LzZedf/gTnhdHfcWthoVilRSk4wygtBz/m3GiM0yToCtchG2U5qOaInismJTjq2ErUPp0j/MLQ3kUZzpuW+brw3AkbjW5KKCTjm2Zf6Hk4FZrJJiJN0+VgGKe3oPdJ3pEh6XC2POwdzFeG1wo0csm8n+WZxUVgDgWX0KTz2oON+1gFs0g1U+AXoQ2loUdRWdHSuHg00lb9tyMw5f1WFbFSMVz7u95O/J83q7H8tAhC2xpB8/2ispYUDd0lTFfCAUe5jYTFdOJbKV8zxzjGf7i1pQstpr1GtJ/HY45Mj4yrxnZTjTt3l1t+N6X75OL9KI/824fh5KRLsE/ekLfkHcnJMZmQr+SMTAkn1+SG/OmRpJ+Mko/J8b406XU9r8ktJJO//jjHsw==</latexit><latexit sha1_base64="TctOJNBCSGX/E6A9rWGmK3MKuuI=">AAAChnicbVHLbhMxFHWGAmF4pbBkYxFVYpXMIFARq6hIFcuikrZSEkUe587ExC/ZdxCRNf/YLT/SLTiTWdCWI9k6Ouc+rOPCSuExy373kgcHDx897j9Jnz57/uLl4PDVhTe14zDlRhp3VTAPUmiYokAJV9YBU4WEy2LzZedf/gTnhdHfcWthoVilRSk4wygtBz/m3GiM0yToCtchG2U5qOaInismJTjq2ErUPp0j/MLQ3kUZzpuW+brw3AkbjW5KKCTjm2Zf6Hk4FZrJJiJN0+VgGKe3oPdJ3pEh6XC2POwdzFeG1wo0csm8n+WZxUVgDgWX0KTz2oON+1gFs0g1U+AXoQ2loUdRWdHSuHg00lb9tyMw5f1WFbFSMVz7u95O/J83q7H8tAhC2xpB8/2ispYUDd0lTFfCAUe5jYTFdOJbKV8zxzjGf7i1pQstpr1GtJ/HY45Mj4yrxnZTjTt3l1t+N6X75OL9KI/824fh5KRLsE/ekLfkHcnJMZmQr+SMTAkn1+SG/OmRpJ+Mko/J8b406XU9r8ktJJO//jjHsw==</latexit>

<latexit sha1_base64="gBMf7Nj3k4DO8zz48qWnIvIsnMQ=">AAAChXicbVHLbhMxFHWmQNvhlcKyG4uoEhuSGVQBYkNFN10WlbSVkijyOHcmbvySfQcRWfOPbPkRttSZzIK2HMnW0Tn3YR0XVgqPWfa7l+w8evxkd28/ffrs+YuX/YNXl97UjsOYG2ncdcE8SKFhjAIlXFsHTBUSrorV6ca/+gHOC6O/49rCTLFKi1JwhlGa92+m3GiM0yToCpchG2Y5qOaIXigmJTjq2ELUPp0i/MTQ3kUZLpqW+brw3AkbjW5KKCTjq2Zb6Hk4Zc6tm4g0Tef9QZzegj4keUcGpMP5/KC3M10YXivQyCXzfpJnFmeBORRcQpNOaw827mMVTCLVTIGfhTaUhh5FZUFL4+LRSFv1347AlPdrVcRKxXDp73sb8X/epMby0ywIbWsEzbeLylpSNHSTMF0IBxzlOhIW04lvpXzJHOMY/+HOli60mPYS0X4ejTgyPTSuGtlVNercGFt+P6SH5PL9MI/82/Hg5GsX4B45JG/IW5KTj+SEnJFzMiac/CJ/yN8eSXaTd8lx8mFbmvS6ntfkDpIvt2cxx7o=</latexit><latexit sha1_base64="gBMf7Nj3k4DO8zz48qWnIvIsnMQ=">AAAChXicbVHLbhMxFHWmQNvhlcKyG4uoEhuSGVQBYkNFN10WlbSVkijyOHcmbvySfQcRWfOPbPkRttSZzIK2HMnW0Tn3YR0XVgqPWfa7l+w8evxkd28/ffrs+YuX/YNXl97UjsOYG2ncdcE8SKFhjAIlXFsHTBUSrorV6ca/+gHOC6O/49rCTLFKi1JwhlGa92+m3GiM0yToCpchG2Y5qOaIXigmJTjq2ELUPp0i/MTQ3kUZLpqW+brw3AkbjW5KKCTjq2Zb6Hk4Zc6tm4g0Tef9QZzegj4keUcGpMP5/KC3M10YXivQyCXzfpJnFmeBORRcQpNOaw827mMVTCLVTIGfhTaUhh5FZUFL4+LRSFv1347AlPdrVcRKxXDp73sb8X/epMby0ywIbWsEzbeLylpSNHSTMF0IBxzlOhIW04lvpXzJHOMY/+HOli60mPYS0X4ejTgyPTSuGtlVNercGFt+P6SH5PL9MI/82/Hg5GsX4B45JG/IW5KTj+SEnJFzMiac/CJ/yN8eSXaTd8lx8mFbmvS6ntfkDpIvt2cxx7o=</latexit><latexit sha1_base64="gBMf7Nj3k4DO8zz48qWnIvIsnMQ=">AAAChXicbVHLbhMxFHWmQNvhlcKyG4uoEhuSGVQBYkNFN10WlbSVkijyOHcmbvySfQcRWfOPbPkRttSZzIK2HMnW0Tn3YR0XVgqPWfa7l+w8evxkd28/ffrs+YuX/YNXl97UjsOYG2ncdcE8SKFhjAIlXFsHTBUSrorV6ca/+gHOC6O/49rCTLFKi1JwhlGa92+m3GiM0yToCpchG2Y5qOaIXigmJTjq2ELUPp0i/MTQ3kUZLpqW+brw3AkbjW5KKCTjq2Zb6Hk4Zc6tm4g0Tef9QZzegj4keUcGpMP5/KC3M10YXivQyCXzfpJnFmeBORRcQpNOaw827mMVTCLVTIGfhTaUhh5FZUFL4+LRSFv1347AlPdrVcRKxXDp73sb8X/epMby0ywIbWsEzbeLylpSNHSTMF0IBxzlOhIW04lvpXzJHOMY/+HOli60mPYS0X4ejTgyPTSuGtlVNercGFt+P6SH5PL9MI/82/Hg5GsX4B45JG/IW5KTj+SEnJFzMiac/CJ/yN8eSXaTd8lx8mFbmvS6ntfkDpIvt2cxx7o=</latexit><latexit sha1_base64="gBMf7Nj3k4DO8zz48qWnIvIsnMQ=">AAAChXicbVHLbhMxFHWmQNvhlcKyG4uoEhuSGVQBYkNFN10WlbSVkijyOHcmbvySfQcRWfOPbPkRttSZzIK2HMnW0Tn3YR0XVgqPWfa7l+w8evxkd28/ffrs+YuX/YNXl97UjsOYG2ncdcE8SKFhjAIlXFsHTBUSrorV6ca/+gHOC6O/49rCTLFKi1JwhlGa92+m3GiM0yToCpchG2Y5qOaIXigmJTjq2ELUPp0i/MTQ3kUZLpqW+brw3AkbjW5KKCTjq2Zb6Hk4Zc6tm4g0Tef9QZzegj4keUcGpMP5/KC3M10YXivQyCXzfpJnFmeBORRcQpNOaw827mMVTCLVTIGfhTaUhh5FZUFL4+LRSFv1347AlPdrVcRKxXDp73sb8X/epMby0ywIbWsEzbeLylpSNHSTMF0IBxzlOhIW04lvpXzJHOMY/+HOli60mPYS0X4ejTgyPTSuGtlVNercGFt+P6SH5PL9MI/82/Hg5GsX4B45JG/IW5KTj+SEnJFzMiac/CJ/yN8eSXaTd8lx8mFbmvS6ntfkDpIvt2cxx7o=</latexit>

≥3

(b) Baseline model perfor-
mance

1 2

1 0.822

2 0.92 0.689

0.908 0.944 0.8

<latexit sha1_base64="TctOJNBCSGX/E6A9rWGmK3MKuuI=">AAAChnicbVHLbhMxFHWGAmF4pbBkYxFVYpXMIFARq6hIFcuikrZSEkUe587ExC/ZdxCRNf/YLT/SLTiTWdCWI9k6Ouc+rOPCSuExy373kgcHDx897j9Jnz57/uLl4PDVhTe14zDlRhp3VTAPUmiYokAJV9YBU4WEy2LzZedf/gTnhdHfcWthoVilRSk4wygtBz/m3GiM0yToCtchG2U5qOaInismJTjq2ErUPp0j/MLQ3kUZzpuW+brw3AkbjW5KKCTjm2Zf6Hk4FZrJJiJN0+VgGKe3oPdJ3pEh6XC2POwdzFeG1wo0csm8n+WZxUVgDgWX0KTz2oON+1gFs0g1U+AXoQ2loUdRWdHSuHg00lb9tyMw5f1WFbFSMVz7u95O/J83q7H8tAhC2xpB8/2ispYUDd0lTFfCAUe5jYTFdOJbKV8zxzjGf7i1pQstpr1GtJ/HY45Mj4yrxnZTjTt3l1t+N6X75OL9KI/824fh5KRLsE/ekLfkHcnJMZmQr+SMTAkn1+SG/OmRpJ+Mko/J8b406XU9r8ktJJO//jjHsw==</latexit><latexit sha1_base64="TctOJNBCSGX/E6A9rWGmK3MKuuI=">AAAChnicbVHLbhMxFHWGAmF4pbBkYxFVYpXMIFARq6hIFcuikrZSEkUe587ExC/ZdxCRNf/YLT/SLTiTWdCWI9k6Ouc+rOPCSuExy373kgcHDx897j9Jnz57/uLl4PDVhTe14zDlRhp3VTAPUmiYokAJV9YBU4WEy2LzZedf/gTnhdHfcWthoVilRSk4wygtBz/m3GiM0yToCtchG2U5qOaInismJTjq2ErUPp0j/MLQ3kUZzpuW+brw3AkbjW5KKCTjm2Zf6Hk4FZrJJiJN0+VgGKe3oPdJ3pEh6XC2POwdzFeG1wo0csm8n+WZxUVgDgWX0KTz2oON+1gFs0g1U+AXoQ2loUdRWdHSuHg00lb9tyMw5f1WFbFSMVz7u95O/J83q7H8tAhC2xpB8/2ispYUDd0lTFfCAUe5jYTFdOJbKV8zxzjGf7i1pQstpr1GtJ/HY45Mj4yrxnZTjTt3l1t+N6X75OL9KI/824fh5KRLsE/ekLfkHcnJMZmQr+SMTAkn1+SG/OmRpJ+Mko/J8b406XU9r8ktJJO//jjHsw==</latexit><latexit sha1_base64="TctOJNBCSGX/E6A9rWGmK3MKuuI=">AAAChnicbVHLbhMxFHWGAmF4pbBkYxFVYpXMIFARq6hIFcuikrZSEkUe587ExC/ZdxCRNf/YLT/SLTiTWdCWI9k6Ouc+rOPCSuExy373kgcHDx897j9Jnz57/uLl4PDVhTe14zDlRhp3VTAPUmiYokAJV9YBU4WEy2LzZedf/gTnhdHfcWthoVilRSk4wygtBz/m3GiM0yToCtchG2U5qOaInismJTjq2ErUPp0j/MLQ3kUZzpuW+brw3AkbjW5KKCTjm2Zf6Hk4FZrJJiJN0+VgGKe3oPdJ3pEh6XC2POwdzFeG1wo0csm8n+WZxUVgDgWX0KTz2oON+1gFs0g1U+AXoQ2loUdRWdHSuHg00lb9tyMw5f1WFbFSMVz7u95O/J83q7H8tAhC2xpB8/2ispYUDd0lTFfCAUe5jYTFdOJbKV8zxzjGf7i1pQstpr1GtJ/HY45Mj4yrxnZTjTt3l1t+N6X75OL9KI/824fh5KRLsE/ekLfkHcnJMZmQr+SMTAkn1+SG/OmRpJ+Mko/J8b406XU9r8ktJJO//jjHsw==</latexit><latexit sha1_base64="TctOJNBCSGX/E6A9rWGmK3MKuuI=">AAAChnicbVHLbhMxFHWGAmF4pbBkYxFVYpXMIFARq6hIFcuikrZSEkUe587ExC/ZdxCRNf/YLT/SLTiTWdCWI9k6Ouc+rOPCSuExy373kgcHDx897j9Jnz57/uLl4PDVhTe14zDlRhp3VTAPUmiYokAJV9YBU4WEy2LzZedf/gTnhdHfcWthoVilRSk4wygtBz/m3GiM0yToCtchG2U5qOaInismJTjq2ErUPp0j/MLQ3kUZzpuW+brw3AkbjW5KKCTjm2Zf6Hk4FZrJJiJN0+VgGKe3oPdJ3pEh6XC2POwdzFeG1wo0csm8n+WZxUVgDgWX0KTz2oON+1gFs0g1U+AXoQ2loUdRWdHSuHg00lb9tyMw5f1WFbFSMVz7u95O/J83q7H8tAhC2xpB8/2ispYUDd0lTFfCAUe5jYTFdOJbKV8zxzjGf7i1pQstpr1GtJ/HY45Mj4yrxnZTjTt3l1t+N6X75OL9KI/824fh5KRLsE/ekLfkHcnJMZmQr+SMTAkn1+SG/OmRpJ+Mko/J8b406XU9r8ktJJO//jjHsw==</latexit>

<latexit sha1_base64="gBMf7Nj3k4DO8zz48qWnIvIsnMQ=">AAAChXicbVHLbhMxFHWmQNvhlcKyG4uoEhuSGVQBYkNFN10WlbSVkijyOHcmbvySfQcRWfOPbPkRttSZzIK2HMnW0Tn3YR0XVgqPWfa7l+w8evxkd28/ffrs+YuX/YNXl97UjsOYG2ncdcE8SKFhjAIlXFsHTBUSrorV6ca/+gHOC6O/49rCTLFKi1JwhlGa92+m3GiM0yToCpchG2Y5qOaIXigmJTjq2ELUPp0i/MTQ3kUZLpqW+brw3AkbjW5KKCTjq2Zb6Hk4Zc6tm4g0Tef9QZzegj4keUcGpMP5/KC3M10YXivQyCXzfpJnFmeBORRcQpNOaw827mMVTCLVTIGfhTaUhh5FZUFL4+LRSFv1347AlPdrVcRKxXDp73sb8X/epMby0ywIbWsEzbeLylpSNHSTMF0IBxzlOhIW04lvpXzJHOMY/+HOli60mPYS0X4ejTgyPTSuGtlVNercGFt+P6SH5PL9MI/82/Hg5GsX4B45JG/IW5KTj+SEnJFzMiac/CJ/yN8eSXaTd8lx8mFbmvS6ntfkDpIvt2cxx7o=</latexit><latexit sha1_base64="gBMf7Nj3k4DO8zz48qWnIvIsnMQ=">AAAChXicbVHLbhMxFHWmQNvhlcKyG4uoEhuSGVQBYkNFN10WlbSVkijyOHcmbvySfQcRWfOPbPkRttSZzIK2HMnW0Tn3YR0XVgqPWfa7l+w8evxkd28/ffrs+YuX/YNXl97UjsOYG2ncdcE8SKFhjAIlXFsHTBUSrorV6ca/+gHOC6O/49rCTLFKi1JwhlGa92+m3GiM0yToCpchG2Y5qOaIXigmJTjq2ELUPp0i/MTQ3kUZLpqW+brw3AkbjW5KKCTjq2Zb6Hk4Zc6tm4g0Tef9QZzegj4keUcGpMP5/KC3M10YXivQyCXzfpJnFmeBORRcQpNOaw827mMVTCLVTIGfhTaUhh5FZUFL4+LRSFv1347AlPdrVcRKxXDp73sb8X/epMby0ywIbWsEzbeLylpSNHSTMF0IBxzlOhIW04lvpXzJHOMY/+HOli60mPYS0X4ejTgyPTSuGtlVNercGFt+P6SH5PL9MI/82/Hg5GsX4B45JG/IW5KTj+SEnJFzMiac/CJ/yN8eSXaTd8lx8mFbmvS6ntfkDpIvt2cxx7o=</latexit><latexit sha1_base64="gBMf7Nj3k4DO8zz48qWnIvIsnMQ=">AAAChXicbVHLbhMxFHWmQNvhlcKyG4uoEhuSGVQBYkNFN10WlbSVkijyOHcmbvySfQcRWfOPbPkRttSZzIK2HMnW0Tn3YR0XVgqPWfa7l+w8evxkd28/ffrs+YuX/YNXl97UjsOYG2ncdcE8SKFhjAIlXFsHTBUSrorV6ca/+gHOC6O/49rCTLFKi1JwhlGa92+m3GiM0yToCpchG2Y5qOaIXigmJTjq2ELUPp0i/MTQ3kUZLpqW+brw3AkbjW5KKCTjq2Zb6Hk4Zc6tm4g0Tef9QZzegj4keUcGpMP5/KC3M10YXivQyCXzfpJnFmeBORRcQpNOaw827mMVTCLVTIGfhTaUhh5FZUFL4+LRSFv1347AlPdrVcRKxXDp73sb8X/epMby0ywIbWsEzbeLylpSNHSTMF0IBxzlOhIW04lvpXzJHOMY/+HOli60mPYS0X4ejTgyPTSuGtlVNercGFt+P6SH5PL9MI/82/Hg5GsX4B45JG/IW5KTj+SEnJFzMiac/CJ/yN8eSXaTd8lx8mFbmvS6ntfkDpIvt2cxx7o=</latexit><latexit sha1_base64="gBMf7Nj3k4DO8zz48qWnIvIsnMQ=">AAAChXicbVHLbhMxFHWmQNvhlcKyG4uoEhuSGVQBYkNFN10WlbSVkijyOHcmbvySfQcRWfOPbPkRttSZzIK2HMnW0Tn3YR0XVgqPWfa7l+w8evxkd28/ffrs+YuX/YNXl97UjsOYG2ncdcE8SKFhjAIlXFsHTBUSrorV6ca/+gHOC6O/49rCTLFKi1JwhlGa92+m3GiM0yToCpchG2Y5qOaIXigmJTjq2ELUPp0i/MTQ3kUZLpqW+brw3AkbjW5KKCTjq2Zb6Hk4Zc6tm4g0Tef9QZzegj4keUcGpMP5/KC3M10YXivQyCXzfpJnFmeBORRcQpNOaw827mMVTCLVTIGfhTaUhh5FZUFL4+LRSFv1347AlPdrVcRKxXDp73sb8X/epMby0ywIbWsEzbeLylpSNHSTMF0IBxzlOhIW04lvpXzJHOMY/+HOli60mPYS0X4ejTgyPTSuGtlVNercGFt+P6SH5PL9MI/82/Hg5GsX4B45JG/IW5KTj+SEnJFzMiac/CJ/yN8eSXaTd8lx8mFbmvS6ntfkDpIvt2cxx7o=</latexit>

≥3

≥3

(c) Pointer network perfor-
mance

1 2

1 0.809

2 0.93 0.753

0.911 0.96 0.87

<latexit sha1_base64="TctOJNBCSGX/E6A9rWGmK3MKuuI=">AAAChnicbVHLbhMxFHWGAmF4pbBkYxFVYpXMIFARq6hIFcuikrZSEkUe587ExC/ZdxCRNf/YLT/SLTiTWdCWI9k6Ouc+rOPCSuExy373kgcHDx897j9Jnz57/uLl4PDVhTe14zDlRhp3VTAPUmiYokAJV9YBU4WEy2LzZedf/gTnhdHfcWthoVilRSk4wygtBz/m3GiM0yToCtchG2U5qOaInismJTjq2ErUPp0j/MLQ3kUZzpuW+brw3AkbjW5KKCTjm2Zf6Hk4FZrJJiJN0+VgGKe3oPdJ3pEh6XC2POwdzFeG1wo0csm8n+WZxUVgDgWX0KTz2oON+1gFs0g1U+AXoQ2loUdRWdHSuHg00lb9tyMw5f1WFbFSMVz7u95O/J83q7H8tAhC2xpB8/2ispYUDd0lTFfCAUe5jYTFdOJbKV8zxzjGf7i1pQstpr1GtJ/HY45Mj4yrxnZTjTt3l1t+N6X75OL9KI/824fh5KRLsE/ekLfkHcnJMZmQr+SMTAkn1+SG/OmRpJ+Mko/J8b406XU9r8ktJJO//jjHsw==</latexit><latexit sha1_base64="TctOJNBCSGX/E6A9rWGmK3MKuuI=">AAAChnicbVHLbhMxFHWGAmF4pbBkYxFVYpXMIFARq6hIFcuikrZSEkUe587ExC/ZdxCRNf/YLT/SLTiTWdCWI9k6Ouc+rOPCSuExy373kgcHDx897j9Jnz57/uLl4PDVhTe14zDlRhp3VTAPUmiYokAJV9YBU4WEy2LzZedf/gTnhdHfcWthoVilRSk4wygtBz/m3GiM0yToCtchG2U5qOaInismJTjq2ErUPp0j/MLQ3kUZzpuW+brw3AkbjW5KKCTjm2Zf6Hk4FZrJJiJN0+VgGKe3oPdJ3pEh6XC2POwdzFeG1wo0csm8n+WZxUVgDgWX0KTz2oON+1gFs0g1U+AXoQ2loUdRWdHSuHg00lb9tyMw5f1WFbFSMVz7u95O/J83q7H8tAhC2xpB8/2ispYUDd0lTFfCAUe5jYTFdOJbKV8zxzjGf7i1pQstpr1GtJ/HY45Mj4yrxnZTjTt3l1t+N6X75OL9KI/824fh5KRLsE/ekLfkHcnJMZmQr+SMTAkn1+SG/OmRpJ+Mko/J8b406XU9r8ktJJO//jjHsw==</latexit><latexit sha1_base64="TctOJNBCSGX/E6A9rWGmK3MKuuI=">AAAChnicbVHLbhMxFHWGAmF4pbBkYxFVYpXMIFARq6hIFcuikrZSEkUe587ExC/ZdxCRNf/YLT/SLTiTWdCWI9k6Ouc+rOPCSuExy373kgcHDx897j9Jnz57/uLl4PDVhTe14zDlRhp3VTAPUmiYokAJV9YBU4WEy2LzZedf/gTnhdHfcWthoVilRSk4wygtBz/m3GiM0yToCtchG2U5qOaInismJTjq2ErUPp0j/MLQ3kUZzpuW+brw3AkbjW5KKCTjm2Zf6Hk4FZrJJiJN0+VgGKe3oPdJ3pEh6XC2POwdzFeG1wo0csm8n+WZxUVgDgWX0KTz2oON+1gFs0g1U+AXoQ2loUdRWdHSuHg00lb9tyMw5f1WFbFSMVz7u95O/J83q7H8tAhC2xpB8/2ispYUDd0lTFfCAUe5jYTFdOJbKV8zxzjGf7i1pQstpr1GtJ/HY45Mj4yrxnZTjTt3l1t+N6X75OL9KI/824fh5KRLsE/ekLfkHcnJMZmQr+SMTAkn1+SG/OmRpJ+Mko/J8b406XU9r8ktJJO//jjHsw==</latexit><latexit sha1_base64="TctOJNBCSGX/E6A9rWGmK3MKuuI=">AAAChnicbVHLbhMxFHWGAmF4pbBkYxFVYpXMIFARq6hIFcuikrZSEkUe587ExC/ZdxCRNf/YLT/SLTiTWdCWI9k6Ouc+rOPCSuExy373kgcHDx897j9Jnz57/uLl4PDVhTe14zDlRhp3VTAPUmiYokAJV9YBU4WEy2LzZedf/gTnhdHfcWthoVilRSk4wygtBz/m3GiM0yToCtchG2U5qOaInismJTjq2ErUPp0j/MLQ3kUZzpuW+brw3AkbjW5KKCTjm2Zf6Hk4FZrJJiJN0+VgGKe3oPdJ3pEh6XC2POwdzFeG1wo0csm8n+WZxUVgDgWX0KTz2oON+1gFs0g1U+AXoQ2loUdRWdHSuHg00lb9tyMw5f1WFbFSMVz7u95O/J83q7H8tAhC2xpB8/2ispYUDd0lTFfCAUe5jYTFdOJbKV8zxzjGf7i1pQstpr1GtJ/HY45Mj4yrxnZTjTt3l1t+N6X75OL9KI/824fh5KRLsE/ekLfkHcnJMZmQr+SMTAkn1+SG/OmRpJ+Mko/J8b406XU9r8ktJJO//jjHsw==</latexit>

<latexit sha1_base64="gBMf7Nj3k4DO8zz48qWnIvIsnMQ=">AAAChXicbVHLbhMxFHWmQNvhlcKyG4uoEhuSGVQBYkNFN10WlbSVkijyOHcmbvySfQcRWfOPbPkRttSZzIK2HMnW0Tn3YR0XVgqPWfa7l+w8evxkd28/ffrs+YuX/YNXl97UjsOYG2ncdcE8SKFhjAIlXFsHTBUSrorV6ca/+gHOC6O/49rCTLFKi1JwhlGa92+m3GiM0yToCpchG2Y5qOaIXigmJTjq2ELUPp0i/MTQ3kUZLpqW+brw3AkbjW5KKCTjq2Zb6Hk4Zc6tm4g0Tef9QZzegj4keUcGpMP5/KC3M10YXivQyCXzfpJnFmeBORRcQpNOaw827mMVTCLVTIGfhTaUhh5FZUFL4+LRSFv1347AlPdrVcRKxXDp73sb8X/epMby0ywIbWsEzbeLylpSNHSTMF0IBxzlOhIW04lvpXzJHOMY/+HOli60mPYS0X4ejTgyPTSuGtlVNercGFt+P6SH5PL9MI/82/Hg5GsX4B45JG/IW5KTj+SEnJFzMiac/CJ/yN8eSXaTd8lx8mFbmvS6ntfkDpIvt2cxx7o=</latexit><latexit sha1_base64="gBMf7Nj3k4DO8zz48qWnIvIsnMQ=">AAAChXicbVHLbhMxFHWmQNvhlcKyG4uoEhuSGVQBYkNFN10WlbSVkijyOHcmbvySfQcRWfOPbPkRttSZzIK2HMnW0Tn3YR0XVgqPWfa7l+w8evxkd28/ffrs+YuX/YNXl97UjsOYG2ncdcE8SKFhjAIlXFsHTBUSrorV6ca/+gHOC6O/49rCTLFKi1JwhlGa92+m3GiM0yToCpchG2Y5qOaIXigmJTjq2ELUPp0i/MTQ3kUZLpqW+brw3AkbjW5KKCTjq2Zb6Hk4Zc6tm4g0Tef9QZzegj4keUcGpMP5/KC3M10YXivQyCXzfpJnFmeBORRcQpNOaw827mMVTCLVTIGfhTaUhh5FZUFL4+LRSFv1347AlPdrVcRKxXDp73sb8X/epMby0ywIbWsEzbeLylpSNHSTMF0IBxzlOhIW04lvpXzJHOMY/+HOli60mPYS0X4ejTgyPTSuGtlVNercGFt+P6SH5PL9MI/82/Hg5GsX4B45JG/IW5KTj+SEnJFzMiac/CJ/yN8eSXaTd8lx8mFbmvS6ntfkDpIvt2cxx7o=</latexit><latexit sha1_base64="gBMf7Nj3k4DO8zz48qWnIvIsnMQ=">AAAChXicbVHLbhMxFHWmQNvhlcKyG4uoEhuSGVQBYkNFN10WlbSVkijyOHcmbvySfQcRWfOPbPkRttSZzIK2HMnW0Tn3YR0XVgqPWfa7l+w8evxkd28/ffrs+YuX/YNXl97UjsOYG2ncdcE8SKFhjAIlXFsHTBUSrorV6ca/+gHOC6O/49rCTLFKi1JwhlGa92+m3GiM0yToCpchG2Y5qOaIXigmJTjq2ELUPp0i/MTQ3kUZLpqW+brw3AkbjW5KKCTjq2Zb6Hk4Zc6tm4g0Tef9QZzegj4keUcGpMP5/KC3M10YXivQyCXzfpJnFmeBORRcQpNOaw827mMVTCLVTIGfhTaUhh5FZUFL4+LRSFv1347AlPdrVcRKxXDp73sb8X/epMby0ywIbWsEzbeLylpSNHSTMF0IBxzlOhIW04lvpXzJHOMY/+HOli60mPYS0X4ejTgyPTSuGtlVNercGFt+P6SH5PL9MI/82/Hg5GsX4B45JG/IW5KTj+SEnJFzMiac/CJ/yN8eSXaTd8lx8mFbmvS6ntfkDpIvt2cxx7o=</latexit><latexit sha1_base64="gBMf7Nj3k4DO8zz48qWnIvIsnMQ=">AAAChXicbVHLbhMxFHWmQNvhlcKyG4uoEhuSGVQBYkNFN10WlbSVkijyOHcmbvySfQcRWfOPbPkRttSZzIK2HMnW0Tn3YR0XVgqPWfa7l+w8evxkd28/ffrs+YuX/YNXl97UjsOYG2ncdcE8SKFhjAIlXFsHTBUSrorV6ca/+gHOC6O/49rCTLFKi1JwhlGa92+m3GiM0yToCpchG2Y5qOaIXigmJTjq2ELUPp0i/MTQ3kUZLpqW+brw3AkbjW5KKCTjq2Zb6Hk4Zc6tm4g0Tef9QZzegj4keUcGpMP5/KC3M10YXivQyCXzfpJnFmeBORRcQpNOaw827mMVTCLVTIGfhTaUhh5FZUFL4+LRSFv1347AlPdrVcRKxXDp73sb8X/epMby0ywIbWsEzbeLylpSNHSTMF0IBxzlOhIW04lvpXzJHOMY/+HOli60mPYS0X4ejTgyPTSuGtlVNercGFt+P6SH5PL9MI/82/Hg5GsX4B45JG/IW5KTj+SEnJFzMiac/CJ/yN8eSXaTd8lx8mFbmvS6ntfkDpIvt2cxx7o=</latexit>

≥3

≥3

(d) Transformer network per-
formance

Figure 4: On internal dataset, plots comparing the performance (F1) of the models across different subsets of
candidates separated based on the number of final slots after resolution (y-axis) and the number of slots that are
carried over as part of reference resolution (x-axis)

obtained by slicing the data using SFINAL – total
number of slots after resolution (i.e after context
carryover) and SCARRY – total number of slots car-
ried from context. For example, in a dialogue, if
the current turn utterance has 2 slots, and after ref-
erence resolution if we carry 3 slots from context,
the values for SFINAL and SCARRY would be 5 and
3 respectively. Figure 4 shows the number of in-
stances in each of these buckets and performance
of the baseline model, the best pointer network and
transformer models on the internal dataset. We no-
tice that the baseline model performs better than
the proposed models for instances in the table di-

agonal (SFINAL = SCARRY). These are the instances
where the current turn has no slots, and all the nec-
essary slots for the turn have to be carried from
historical context. Proposed models perform bet-
ter in off-diagonal buckets. We hypothesize that
the proposed models use anchor slots (slots in cur-
rent utterance having slot distance 0 which are al-
ways positive) and learn slot co-occurrence of can-
didate slots from context with these anchor slots
to improve resolution (i.e., carryover) from longer
distances.



103

Domain
Natural Language 

Understanding
(NLU)

Slot Carryover

Dialog Manager
Natural Language 

Generation
(NLG)

Context
Store

User
Query

System
Response

Figure 5: Spoken dialogue system architecture: the ref-
erence resolver/context carryover component is used to
resolve references in a conversation.

5 Related Work

Figure 5 shows a typical pipelined approach to
spoken dialogue (Tur and De Mori, 2011), and
where the context carryover system fits into the
overall architecture. The context carryover system
takes as input, an interpretation output by NLU –
typically represented as intents and slots (Wang
et al., 2011) – and outputs another interpretation
that contains slots from the dialogue context that
are relevant to the current turn. The output from
context carryover is then fed to the dialogue man-
ager to take the next action. Resolving references
to slots in the dialogue plays a vital role in track-
ing conversation states across turns (Çelikyilmaz
et al., 2014). Previous work, e.g., Bhargava et al.
(2013); Xu and Sarikaya (2014); Bapna et al.
(2017), focus on better leveraging dialogue con-
texts to improve SLU performance. However, in
commercial systems like Siri, Google Assistant,
and Alexa, the NLU component is a diverse collec-
tion of services spanning rules and statistical mod-
els. Typical end-to-end approaches (Bapna et al.,
2017) which require back-propagation through the
NLU sub-systems are not feasible in this setting.

Dialogue state tracking Dialogue state tracking
(DST) focuses on tracking conversational states
as well. Traditional DST models rely on hand-
crafted semantic delexicalization to achieve gen-
eralization (Henderson et al., 2014; Zilka and
Jurcı́cek, 2015; Mrksic et al., 2015). Mrksic et al.
(2017) utilize representation learning for states
rather than using hand-crafted features. These
approaches only operate on fixed ontology and
do not generalize well to unknown slot key-value
pairs. Rastogi et al. (2017) address this by us-
ing sophisticated candidate generation and scor-
ing mechanism while Xu and Hu (2018) use a
pointer network to handle unknown slot values.
Zhong et al. (2018) share global parameters be-
tween estimates for each slot to address extraction

of rare slot-value pairs and achieve state-of-the-
art on DST. In context carryover, our state track-
ing does not rely on the definition of user goals
and is instead focused on resolving slot references
across turns. This approach scales when dealing
with multiple spoken language systems, as we do
not track the belief states explicitly.

Coreference resolution Our problem is closely
related to coreference resolution, where mentions
in the current utterance are to be detected and
linked to previously mentioned entities. Previ-
ous work on coreference resolution have relied
on clustering (Bagga and Baldwin, 1998; Stoy-
anov and Eisner, 2012) or comparing mention
pairs (Durrett and Klein, 2013; Wiseman et al.,
2015; Sankepally et al., 2018). This has two prob-
lems. (1) most traditional methods for corefer-
ence resolution follows a pipeline approach, with
rich linguistic features, making the system cum-
bersome and prone to cascading errors; (2) Zero
pronouns, intent references and other phenomena
in spoken dialogue are hard to capture with this
approach (Rao et al., 2015). These problems are
circumvented in our approach for slot carryover.

6 Conclusions

In this work, we proposed an improvement to the
slot carryover task as defined in Naik et al. (2018).
Instead of independent decisions across slots, we
proposed two architectures to leverage the slot
interdependence – a pointer network architecture
and a self-attention and transformer based archi-
tecture. Our experiments show that both proposed
models are good at carrying over slots over longer
dialogue context. The transformer model with its
self attention mechanism gives us the best overall
performance. Furthermore, our experiments show
that temporal ordering of slots in the dialogue mat-
ter, since recent slots are more likely to be referred
to by users in a spoken dialogue system. More-
over, contextualized encoding of slots is also im-
portant, which follows the trend of contextualized
embeddings (Peters et al., 2018b).

For future work, we plan to improve these mod-
els by encoding the actual dialogue timing infor-
mation into the contextualized slot embeddings
as additional signals. We also plan on exploring
the impact of pre-trained representations (Devlin
et al., 2019) trained specifically over large-scale
dialogues as another way to get improved contex-
tualized slot embeddings.



104

References
Amit Bagga and Breck Baldwin. 1998. Entity-

based cross-document coreferencing using the vec-
tor space model. In 36th Annual Meeting of the As-
sociation for Computational Linguistics and 17th In-
ternational Conference on Computational Linguis-
tics, COLING-ACL ’98, August 10-14, 1998, Uni-
versité de Montréal, Montréal, Quebec, Canada.
Proceedings of the Conference., pages 79–85.

Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben-
gio. 2015. Neural machine translation by jointly
learning to align and translate. In 3rd Inter-
national Conference on Learning Representations,
ICLR 2015, San Diego, CA, USA, May 7-9, 2015,
Conference Track Proceedings.

Ankur Bapna, Gökhan Tür, Dilek Z. Hakkani-Tür, and
Larry P. Heck. 2017. Sequential dialogue context
modeling for spoken language understanding. In
Proceedings of the 18th Annual SIGdial Meeting
on Discourse and Dialogue, Saarbrücken, Germany,
August 15-17, 2017, pages 103–114.

A. Bhargava, Asli Çelikyilmaz, Dilek Hakkani-Tür,
and Ruhi Sarikaya. 2013. Easy contextual intent
prediction and slot detection. In IEEE Interna-
tional Conference on Acoustics, Speech and Signal
Processing, ICASSP 2013, Vancouver, BC, Canada,
May 26-31, 2013, pages 8337–8341.

Asli Çelikyilmaz, Zhaleh Feizollahi, Dilek Z. Hakkani-
Tür, and Ruhi Sarikaya. 2014. Resolving refer-
ring expressions in conversational dialogs for nat-
ural user interfaces. In Proceedings of the 2014
Conference on Empirical Methods in Natural Lan-
guage Processing, EMNLP 2014, October 25-29,
2014, Doha, Qatar, A meeting of SIGDAT, a Special
Interest Group of the ACL, pages 2094–2104.

Jacob Devlin, Ming-Wei Chang, Kenton Lee, and
Kristina Toutanova. 2019. BERT: Pre-training of
deep bidirectional transformers for language under-
standing. In Proceedings of the 2019 Conference
of the North American Chapter of the Association
for Computational Linguistics: Human Language
Technologies, Volume 1 (Long and Short Papers),
pages 4171–4186, Minneapolis, Minnesota. Associ-
ation for Computational Linguistics.

Greg Durrett and Dan Klein. 2013. Easy victories and
uphill battles in coreference resolution. In Proceed-
ings of the 2013 Conference on Empirical Methods
in Natural Language Processing, pages 1971–1982,
Seattle, Washington, USA. Association for Compu-
tational Linguistics.

Matthew Henderson, Blaise Thomson, and Steve J.
Young. 2014. Word-based dialog state tracking with
recurrent neural networks. In Proceedings of the
SIGDIAL 2014 Conference, The 15th Annual Meet-
ing of the Special Interest Group on Discourse and
Dialogue, 18-20 June 2014, Philadelphia, PA, USA,
pages 292–299.

Sepp Hochreiter and Jürgen Schmidhuber. 1997.
Long short-term memory. Neural Computation,
9(8):1735–1780.

Jeremy Howard and Sebastian Ruder. 2018. Universal
language model fine-tuning for text classification. In
Proceedings of the 56th Annual Meeting of the As-
sociation for Computational Linguistics, ACL 2018,
Melbourne, Australia, July 15-20, 2018, Volume 1:
Long Papers, pages 328–339.

Diederik P. Kingma and Jimmy Ba. 2015. Adam: A
method for stochastic optimization. In 3rd Inter-
national Conference on Learning Representations,
ICLR 2015, San Diego, CA, USA, May 7-9, 2015,
Conference Track Proceedings.

Guillaume Lample, Alexis Conneau, Ludovic De-
noyer, and Marc’Aurelio Ranzato. 2018. Unsuper-
vised machine translation using monolingual cor-
pora only. In 6th International Conference on
Learning Representations, ICLR 2018, Vancouver,
BC, Canada, April 30 - May 3, 2018, Conference
Track Proceedings.

Thang Luong, Hieu Pham, and Christopher D. Man-
ning. 2015. Effective approaches to attention-based
neural machine translation. In Proceedings of the
2015 Conference on Empirical Methods in Natu-
ral Language Processing, pages 1412–1421, Lis-
bon, Portugal. Association for Computational Lin-
guistics.

Nikola Mrksic, Diarmuid Ó Séaghdha, Blaise Thom-
son, Milica Gasic, Pei-hao Su, David Vandyke,
Tsung-Hsien Wen, and Steve J. Young. 2015. Multi-
domain dialog state tracking using recurrent neural
networks. In Proceedings of the 53rd Annual Meet-
ing of the Association for Computational Linguistics
and the 7th International Joint Conference on Natu-
ral Language Processing of the Asian Federation of
Natural Language Processing, ACL 2015, July 26-
31, 2015, Beijing, China, Volume 2: Short Papers,
pages 794–799.

Nikola Mrksic, Diarmuid Ó Séaghdha, Tsung-Hsien
Wen, Blaise Thomson, and Steve J. Young. 2017.
Neural belief tracker: Data-driven dialogue state
tracking. In Proceedings of the 55th Annual Meet-
ing of the Association for Computational Linguis-
tics, ACL 2017, Vancouver, Canada, July 30 - August
4, Volume 1: Long Papers, pages 1777–1788.

Chetan Naik, Arpit Gupta, Hancheng Ge, Lambert
Mathias, and Ruhi Sarikaya. 2018. Contextual slot
carryover for disparate schemas. In Interspeech
2018, 19th Annual Conference of the International
Speech Communication Association, Hyderabad, In-
dia, 2-6 September 2018., pages 596–600.

Matthew E. Peters, Mark Neumann, Mohit Iyyer, Matt
Gardner, Christopher Clark, Kenton Lee, and Luke
Zettlemoyer. 2018a. Deep contextualized word rep-
resentations. In Proceedings of the 2018 Confer-
ence of the North American Chapter of the Associ-

http://aclweb.org/anthology/P/P98/P98-1012.pdf
http://aclweb.org/anthology/P/P98/P98-1012.pdf
http://aclweb.org/anthology/P/P98/P98-1012.pdf
http://arxiv.org/abs/1409.0473
http://arxiv.org/abs/1409.0473
https://aclanthology.info/papers/W17-5514/w17-5514
https://aclanthology.info/papers/W17-5514/w17-5514
https://doi.org/10.1109/ICASSP.2013.6639291
https://doi.org/10.1109/ICASSP.2013.6639291
http://aclweb.org/anthology/D/D14/D14-1223.pdf
http://aclweb.org/anthology/D/D14/D14-1223.pdf
http://aclweb.org/anthology/D/D14/D14-1223.pdf
https://www.aclweb.org/anthology/papers/N/N19/N19-1423
https://www.aclweb.org/anthology/papers/N/N19/N19-1423
https://www.aclweb.org/anthology/papers/N/N19/N19-1423
https://aclweb.org/anthology/papers/D/D13/D13-1203
https://aclweb.org/anthology/papers/D/D13/D13-1203
http://aclweb.org/anthology/W/W14/W14-4340.pdf
http://aclweb.org/anthology/W/W14/W14-4340.pdf
https://doi.org/10.1162/neco.1997.9.8.1735
https://aclanthology.info/papers/P18-1031/p18-1031
https://aclanthology.info/papers/P18-1031/p18-1031
http://arxiv.org/abs/1412.6980
http://arxiv.org/abs/1412.6980
https://openreview.net/forum?id=rkYTTf-AZ
https://openreview.net/forum?id=rkYTTf-AZ
https://openreview.net/forum?id=rkYTTf-AZ
https://doi.org/10.18653/v1/D15-1166
https://doi.org/10.18653/v1/D15-1166
http://aclweb.org/anthology/P/P15/P15-2130.pdf
http://aclweb.org/anthology/P/P15/P15-2130.pdf
http://aclweb.org/anthology/P/P15/P15-2130.pdf
https://doi.org/10.18653/v1/P17-1163
https://doi.org/10.18653/v1/P17-1163
https://doi.org/10.21437/Interspeech.2018-1035
https://doi.org/10.21437/Interspeech.2018-1035
https://aclanthology.info/papers/N18-1202/n18-1202
https://aclanthology.info/papers/N18-1202/n18-1202


105

ation for Computational Linguistics: Human Lan-
guage Technologies, NAACL-HLT 2018, New Or-
leans, Louisiana, USA, June 1-6, 2018, Volume 1
(Long Papers), pages 2227–2237.

Matthew E. Peters, Mark Neumann, Mohit Iyyer, Matt
Gardner, Christopher Clark, Kenton Lee, and Luke
Zettlemoyer. 2018b. Deep contextualized word rep-
resentations. In Proceedings of the 2018 Confer-
ence of the North American Chapter of the Associ-
ation for Computational Linguistics: Human Lan-
guage Technologies, Volume 1 (Long Papers), pages
2227–2237, New Orleans, Louisiana. Association
for Computational Linguistics.

Sudha Rao, Allyson Ettinger, Hal Daumé III, and
Philip Resnik. 2015. Dialogue focus tracking for
zero pronoun resolution. In Proceedings of the
2015 Conference of the North American Chapter of
the Association for Computational Linguistics: Hu-
man Language Technologies, pages 494–503, Den-
ver, Colorado. Association for Computational Lin-
guistics.

Abhinav Rastogi, Dilek Hakkani-Tür, and Larry P.
Heck. 2017. Scalable multi-domain dialogue state
tracking. In 2017 IEEE Automatic Speech Recogni-
tion and Understanding Workshop, ASRU 2017, Ok-
inawa, Japan, December 16-20, 2017, pages 561–
568.

Rashmi Sankepally, Tongfei Chen, Benjamin Van
Durme, and Douglas W. Oard. 2018. A test collec-
tion for coreferent mention retrieval. In The 41st In-
ternational ACM SIGIR Conference on Research &
Development in Information Retrieval, SIGIR 2018,
Ann Arbor, MI, USA, July 08-12, 2018, pages 1209–
1212.

Rupesh Kumar Srivastava, Klaus Greff, and Jürgen
Schmidhuber. 2015. Highway networks. Comput-
ing Research Repository, arXiv:1505.00387.

Veselin Stoyanov and Jason Eisner. 2012. Easy-first
coreference resolution. In COLING 2012, 24th In-
ternational Conference on Computational Linguis-
tics, Proceedings of the Conference: Technical Pa-
pers, 8-15 December 2012, Mumbai, India, pages
2519–2534.

Ilya Sutskever, Oriol Vinyals, and Quoc V. Le. 2014.
Sequence to sequence learning with neural net-
works. In Advances in Neural Information Process-
ing Systems 27: Annual Conference on Neural In-
formation Processing Systems 2014, December 8-
13 2014, Montreal, Quebec, Canada, pages 3104–
3112.

Gokhan Tur and Renato De Mori. 2011. Spoken Lan-
guage Understanding: Systems for Extracting Se-
mantic Information from Speech. John Wiley and
Sons.

Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob
Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz

Kaiser, and Illia Polosukhin. 2017. Attention is all
you need. In Advances in Neural Information Pro-
cessing Systems 30: Annual Conference on Neural
Information Processing Systems 2017, 4-9 Decem-
ber 2017, Long Beach, CA, USA, pages 6000–6010.

Oriol Vinyals, Meire Fortunato, and Navdeep Jaitly.
2015. Pointer networks. In Advances in Neural
Information Processing Systems 28: Annual Con-
ference on Neural Information Processing Systems
2015, December 7-12, 2015, Montreal, Quebec,
Canada, pages 2692–2700.

Ye-Yi Wang, Li Deng, and Alex Acero. 2011. Seman-
tic Frame-Based Spoken Language Understanding,
pages 35–80. Wiley.

Sam Wiseman, Alexander M. Rush, Stuart M. Shieber,
and Jason Weston. 2015. Learning anaphoricity
and antecedent ranking features for coreference res-
olution. In Proceedings of the 53rd Annual Meet-
ing of the Association for Computational Linguistics
and the 7th International Joint Conference on Natu-
ral Language Processing of the Asian Federation of
Natural Language Processing, ACL 2015, July 26-
31, 2015, Beijing, China, Volume 1: Long Papers,
pages 1416–1426.

Puyang Xu and Qi Hu. 2018. An end-to-end approach
for handling unknown slot values in dialogue state
tracking. In Proceedings of the 56th Annual Meet-
ing of the Association for Computational Linguis-
tics, ACL 2018, Melbourne, Australia, July 15-20,
2018, Volume 1: Long Papers, pages 1448–1457.

Puyang Xu and Ruhi Sarikaya. 2014. Contextual do-
main classification in spoken language understand-
ing systems using recurrent neural network. In IEEE
International Conference on Acoustics, Speech and
Signal Processing, ICASSP 2014, Florence, Italy,
May 4-9, 2014, pages 136–140.

Victor Zhong, Caiming Xiong, and Richard Socher.
2018. Global-locally self-attentive encoder for dia-
logue state tracking. In Proceedings of the 56th An-
nual Meeting of the Association for Computational
Linguistics, ACL 2018, Melbourne, Australia, July
15-20, 2018, Volume 1: Long Papers, pages 1458–
1467.

Lukás Zilka and Filip Jurcı́cek. 2015. Incremental
lstm-based dialog state tracker. In 2015 IEEE Work-
shop on Automatic Speech Recognition and Under-
standing, ASRU 2015, Scottsdale, AZ, USA, Decem-
ber 13-17, 2015, pages 757–762.

https://doi.org/10.18653/v1/N18-1202
https://doi.org/10.18653/v1/N18-1202
https://doi.org/10.3115/v1/N15-1052
https://doi.org/10.3115/v1/N15-1052
https://doi.org/10.1109/ASRU.2017.8268986
https://doi.org/10.1109/ASRU.2017.8268986
https://doi.org/10.1145/3209978.3210139
https://doi.org/10.1145/3209978.3210139
http://arxiv.org/abs/1505.00387
http://aclweb.org/anthology/C/C12/C12-1154.pdf
http://aclweb.org/anthology/C/C12/C12-1154.pdf
http://papers.nips.cc/paper/5346-sequence-to-sequence-learning-with-neural-networks
http://papers.nips.cc/paper/5346-sequence-to-sequence-learning-with-neural-networks
https://www.microsoft.com/en-us/research/publication/spoken-language-understanding-systems-for-extracting-semantic-information-from-speech/
https://www.microsoft.com/en-us/research/publication/spoken-language-understanding-systems-for-extracting-semantic-information-from-speech/
https://www.microsoft.com/en-us/research/publication/spoken-language-understanding-systems-for-extracting-semantic-information-from-speech/
http://papers.nips.cc/paper/7181-attention-is-all-you-need
http://papers.nips.cc/paper/7181-attention-is-all-you-need
http://papers.nips.cc/paper/5866-pointer-networks
https://www.microsoft.com/en-us/research/publication/semantic-frame-based-spoken-language-understanding/
https://www.microsoft.com/en-us/research/publication/semantic-frame-based-spoken-language-understanding/
http://aclweb.org/anthology/P/P15/P15-1137.pdf
http://aclweb.org/anthology/P/P15/P15-1137.pdf
http://aclweb.org/anthology/P/P15/P15-1137.pdf
https://aclanthology.info/papers/P18-1134/p18-1134
https://aclanthology.info/papers/P18-1134/p18-1134
https://aclanthology.info/papers/P18-1134/p18-1134
https://doi.org/10.1109/ICASSP.2014.6853573
https://doi.org/10.1109/ICASSP.2014.6853573
https://doi.org/10.1109/ICASSP.2014.6853573
https://aclanthology.info/papers/P18-1135/p18-1135
https://aclanthology.info/papers/P18-1135/p18-1135
https://doi.org/10.1109/ASRU.2015.7404864
https://doi.org/10.1109/ASRU.2015.7404864

