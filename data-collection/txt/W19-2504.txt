



















































Are Fictional Voices Distinguishable? Classifying Character Voices in Modern Drama


Proc. of the 3rd Joint SIGHUM Workshop on Computational Linguistics for Cultural Heritage, Social Sciences, Humanities and Literature, pp. 29–34
Minneapolis, MN, USA, June 7, 2019. c©2019 Association for Computational Linguistics

29

Are Fictional Voices Distinguishable?
Classifying Character Voices in Modern Drama

Krishnapriya Vishnubhotla
Department of Computer Science

University of Toronto
Toronto, Canada

vkpriya@cs.toronto.edu

Adam Hammond
Department of English
University of Toronto

Toronto, Canada

adam.hammond@utoronto.ca

Graeme Hirst
Department of Computer Science

University of Toronto
Toronto, Canada

gh@cs.toronto.edu

Abstract

According to the literary theory of Mikhail
Bakhtin, a dialogic novel is one in which char-
acters speak in their own distinct voices, rather
than serving as mouthpieces for their authors.
We use text classification to determine which
authors best achieve dialogism, looking at a
corpus of plays from the late nineteenth and
early twentieth centuries. We find that the
SAGE model of text generation, which high-
lights deviations from a background lexical
distribution, is an effective method of weight-
ing the words of characters’ utterances. Our
results show that it is indeed possible to dis-
tinguish characters by their speech in the plays
of canonical writers such as George Bernard
Shaw, whereas characters are clustered more
closely in the works of lesser-known play-
wrights.

1 Introduction

The concept of dialogism has been a notable fo-
cus in recent computational literary scholarship
(Brooke et al., 2017; Hammond and Brooke, 2016;
Muzny et al., 2017). As theorized by Russian
literary critic Mikhail Bakhtin (2013), a dialogic
novel is one in which characters present “a plural-
ity of independent and unmerged voices and con-
sciousnesses, a genuine polyphony of fully valid
voices”. Bakhtin presents Dostoevsky as the pre-
eminent dialogic author, arguing that his novels
are “multi-accented and contradictory in [their]
values”, whereas the works of other novelists like
Tolstoy are monologic or homogeneous in their
style, with characters reflecting the prejudices as
well as the distinctive mannerisms of their authors.

While previous computational studies of dialo-
gism take this definition of dialogism for granted
and seek to model it, here we take a step back to
pose a series of fundamental questions: Can the
voices of characters be distinguished in fictional

texts? Which computational techniques are most
effective in making these distinctions? Are certain
authors better than others at creating characters
with distinctive voices and do these authors tend
to be more canonical? Focusing, for pragmatic
purposes, on plays rather than novels, we argue
here that character voices can, in the work of cer-
tain authors, be readily distinguished; that SAGE
(Sparse Additive Generative) models (Eisenstein
et al., 2011) are especially powerful in making
these distinctions; and that canonical authors are,
in our small sample, more successful in creating
distinctive character voices than are less canonical
authors.

2 Related Work

Computational approaches to the authorship attri-
bution problem involve using certain textual fea-
tures, called style markers, to build a represen-
tation of an author’s texts, which is then passed
to a classification algorithm. Stop-word frequen-
cies, part-of-speech trigrams, and structural fea-
tures such as sentence lengths have been shown to
be good indicators of author identity (Stamatatos,
2009). The earliest work in authorship attribution
focused on discovering the stylistic markers that
would reveal the identity of the author or authors
of disputed works (Mosteller and Wallace, 1963),
and the bulk of contemporary work in authorship
attribution continues in this vein (Rybicki, 2018).
Our work draws on an alternative tradition that
uses the techniques of authorship attribution to in-
vestigate what J. F. Burrows, in a study of the
novels of Jane Austen, calls idiolects, the distinc-
tive stylistic patterns of individual speakers within
texts (Burrows, 1987). Whereas Burrows’s ap-
proach focuses on very common words and relies
on statistical methods whose results are not easily
interpretable, our particular application requires us



30

to employ methods that are sensitive to rare and in-
frequent words, and whose results allow us to dis-
tinguish between stylistic and topical phenomena.

Recently, machine learning methods have been
applied in computational stylometry for author-
ship attribution tasks, and also in the context of
style transfer for texts. Bagnall (2015) uses a re-
current neural network (RNN) based model for
the author identification task. Since neural archi-
tectures massively overfit the training set unless
used with large datasets, the authors propose a
shared recurrrent layer, with only the final soft-
max layer being author-specific. Shrestha et al.
(2017) use convolutional neural networks (CNNs)
over character n-grams for authorship attribution,
which proves to be more interpretable than the for-
mer in identifying important features.

3 Corpus

Our corpus consists of plays published in the late
19th and early 20th centuries by George Bernard
Shaw, Oscar Wilde, Cale Young Rice, Sydney
Grundy, Somerset Maugham, Arthur Wing Pinero,
and Hermann Sudermann (whose plays are trans-
lated from German) — giving a total of 63 plays.
We would ideally have examined character dia-
logue in novels, Bakhtin’s preferred genre, but the
problem of sufficiently reliable quote attribution
for novels remains unsolved. However, in plays,
each utterance is explicitly labeled with the name
of the character who speaks it. We use GutenTag
(Brooke et al., 2015) to extract all plays from the
specified authors, restricting the year of publica-
tion to 1880–1920 to roughly capture the literary
period from which Bakhtin developed his theory
of dialogism.

4 Methodology

Our primary method of measuring the distin-
guishability of character voices is classification.
Our task is to build a classifier able to correctly
discriminate between the speech of different char-
acters. We perform experiments using several fea-
ture sets, in order to capture stylistic aspects that
are syntactic as well as lexical. These include sur-
face, syntactic, and generative topic-modeling in-
duced features. Generative models that we used
include latent Dirichlet allocation (Blei et al.,
2003), naive Bayes, and SAGE models (Eisenstein
et al., 2011). Accuracy of classification is mea-
sured using the F1 score, which strikes a balance

between precision and recall. We experiment with
both support vector machine (SVM) and logistic
regression classifiers.

In addition, we experiment with vector repre-
sentations of words as features. We use distributed
word vectors trained on the Wikipedia corpus us-
ing the word2vec algorithm (Mikolov et al., 2013).
Each dialogue is represented as a weighted av-
erage of the individual word vectors, where the
weights are TF-IDF weights, or obtained from the
SAGE algorithm.

We also look at representations obtained from
lexicons that score words across a discrete set of
stylistic dimensions. Brooke and Hirst (2013) pick
three dimensions to rate words along, the opposing
polarities of which give us six styles: colloquial
vs. literary, concrete vs. abstract, and subjective
vs. objective. We also use the NRC Emotion Inten-
sity Lexicon (EmoLex) (Mohammad, 2018b) and
the NRC Valence, Arousal, and Dominance Lex-
icon (VAD Lexicon) (Mohammad, 2018a). The
former provides real-valued intensity scores for
four basic emotions — anger, fear, sadness, and
joy, and the latter for the three primary dimensions
of word meaning — valence, arousal, and domi-
nance. The scores along each dimension are nor-
malized to give us a set of values ranging from 0
to 1. Principal component analysis (PCA) of these
vectors gives us an insight into which authors are
the most successful at creating characters whose
style is highly mutually distinguishable.

We repeat these experiments for “artificial
plays” constructed by sampling a random subset
of characters either across plays (strategy 1) or
across authors (strategy 2). Intuitively, we expect
the character speech in these artificial plays to be
more readily distinguishable than in actual plays,
because the characters are likely to discuss a wider
variety of topics and to come from a wider variety
of classes, professional milieus, and dialect com-
munities than a group of characters in any actual
play (strategies 1 and 2), and because the char-
acters are the creations of different authors, each
with their own distinct stylistic fingerprints (strat-
egy 2).

5 Classification Models

In this section, we describe the two main mod-
els of classification that we employed. All hyper-
parameters in both models are tuned using grid-
search, along with 5-fold cross validation.



31

5.1 Lexical and Syntactic features

Our first feature set consists of lexical, syntac-
tic and structural features. These include aver-
age sentence and word lengths, type-token ratio,
and proportion of function words in each sen-
tence. We also use n-gram frequencies of word
and part-of-speech tags, where n ∈ {1,2,3}, and
dependency triples of the form (head-PoS, child-
PoS, DepRel) from the dependency parse of each
sentence, where child-PoS and head-PoS are the
parts-of-speech of the current word and its parent
node, and DepRel is the dependency relation be-
tween them. All proper nouns in our sentences are
masked, as they often serve as indicative clues as
to who the speaker is or is not.

Because word and PoS n-grams are very sparse
features, the resulting feature vector has a rela-
tively high dimensionality. We therefore pass it
through a feature selection pipeline before classifi-
cation. Two main feature selection algorithms are
used: variance threshold and k-best selection. The
former removes all features with a zero variance
across samples — i.e, features that have the same
value at each datapoint. The k-best selection al-
gorithm then picks the top-k features according to
some correlation measure. Here, we use the chi-
squared statistic, which gets rid of the features that
are the most likely to be independent of class and
therefore irrelevant for classification. We pass this
feature vector through a support vector machine
(SVM) classifier.

5.2 Sentence Vectors with SAGE

Since we are dealing with a dataset that can con-
tain very few samples per class, we need a model
that is sensitive to low-frequency word features.
We use the Sparse Additive Generative (SAGE)
model of text, proposed by Eisenstein et al. (2011),
which models the word distribution of each class
as a vector of log-frequency deviations from a
background distribution. We take the background
distribution to be the average of the word frequen-
cies across all classes. An alternative to the naive
Bayes and LDA-like models of text generation, the
SAGE model enforces a sparse prior on its param-
eters, which biases it towards rare and infrequent
terms in the text.

We use the SAGE model to derive weights for
each sentence (i.e, each quote) in our dataset. Sen-
tence vectors are obtained by averaging the vector
representation of each word in the sentence with

Author #Plays Baseline Avg F1
Shaw 29 .153 .400
Wilde 6 .116 .376
Maugham 8 .137 .318
Grundy 4 .107 .283
Pinero 5 .090 .272
Sudermann 5 .084 .253
Rice 6 .151 .234
Weighted Avg. .133 .342

Table 1: F1 scores for classification of individual char-
acters, by author, using lexical and syntactic features.
Baseline is random classification with the class distri-
bution of the training data. The final row reports the
weighted average of the scores for each author, where
the weights are proportional to the number of their
plays in our dataset.

Author Baseline Avg F1
Shaw .148 .573
Wilde .194 .376
Maugham .182 .318
Grundy .184 .283
Pinero .140 .272
Sudermann .119 .253
Rice .186 .234
Average .165 .329

Table 2: F1 scores for classification, using lexical and
syntactic features, of characters by each author in arti-
ficial plays generated by sampling characters from the
all plays of that author. Baseline is computed in the
same manner as in Table 1.

its corresponding SAGE weight. Classification is
performed by passing these sentence vectors to a
logistic regression classifier.

6 Results

We first present results for classification of indi-
vidual characters with our lexical and syntactic
features in Tables 1 and 2. We compare scores
with a baseline that randomly generates predic-
tions that respect the class distributions of the
training data.

The classification scores are above the baseline
for almost all the plays, though the absolute num-
bers themselves are not very high. Table 1 shows
the average scores across all plays for each author,
while Table 2 contains the average scores for the
artificial plays. Shaw achieves the highest average
score.

As expected, the scores for artificial plays are,



32

Average F1
Original Artificial

Author plays plays
Wilde .641 .669
Shaw .635 .630
Maugham .662 .645
Sudermann .538 .574
Grundy .517 .517
Pinero .458 .543
Rice .181 .208
Weighted Avg. .561 .540

Table 3: F1 scores for classification of characters in
original artificial plays using out SAGE classification
model.

on average, higher than those of actual plays. We
generate a maximum of 50 artificial plays for each
author by sampling 7 characters from the complete
set of characters, without repetition.

We achieve the best classification results, how-
ever, using the SAGE+word2vec classification al-
gorithm described in Section 5.2. Table 3 shows
the author-wise average F1 scores for both origi-
nal and artificial (strategy 1) plays. The average
F1 is higher still, at .605, for strategy 2 artificial
plays (not presented in the table).

As an additional test, we performed PCA on
vectors constructed using the style lexicons from
Section 4. To construct our vectors, we replace our
word2vec embeddings with a concatenated vector
of the scores for each word along each of the 14
dimensions. Missing dimensions for words are as-
signed a score of zero. All the vectors are nor-
malized along each dimension to account for vari-
ations in scale.

The results are shown in Figure 1, which plots
the first two principal components. The two com-
ponents combined account for 74.7% of the vari-
ance of the data. Each dot corresponds to a
character in an actual play, and wider spacing
between them indicates a wider range of styles
and emotions. Even taking into account the fact
that Shaw has significantly more plays, and thus
more characters, than the other playwrights, he
is nonetheless evidently the most successful, fol-
lowed by Maugham, at creating characters with a
wide range across all of the dimensions.

Figure 1: Plot of first two components of PCA on the
lexical style vectors of each character of each author.

7 Discussion

Our work presents insights into a series of funda-
mental questions related to the phenomenon of lit-
erary dialogism and its tractability for computa-
tional analysis. The most fundamental is whether
the voices of individual characters can be distin-
guished at all in literary texts. In a provocative ar-
gument in Enumerations, Andrew Piper uses com-
putational methods to argue that “character-text”
(the words used to describe characters) is — con-
trary to the intuitions of many literary scholars —
relatively uniform within and across novels (Piper,
2018). Our work suggests that the same cannot be
said of “dialogue-text” (the words that characters
say). In a finding more in line with the intuitions of
critics and the theories of Bakhtin, our experiment
shows that the voices of characters can indeed be
distinguished from one another, sometimes with
quite high precision.

As to the question of whether certain authors are
better able to distinguish their characters’ voices
than others, our results suggest that this is clearly
the case. Although we approach the classifica-
tion task from a variety of methodological per-
spectives, each of these reveals a continuum along
which some playwrights are able to create distinc-
tive character voices (e.g., Shaw) and some are not
(e.g, Rice). That this continuum separates well-
known playwrights like Shaw and Wilde from
mostly forgotten playwrights like Pinero and Rice
suggests that the ability to distinguish voices may
be a property of more canonical — and, per-



33

haps, more talented — writers.1 A larger sam-
ple size would be necessary to draw such con-
clusions definitively, however, as would an inves-
tigation of the effect of genre on the distinctive-
ness of character speech — for instance, whether
comedy, which tends to put characters of different
classes (and class dialects) in conversation, pro-
duces higher distinctiveness scores.

Our experiments with different feature sets also
provide insights into how these characters are dis-
tinguishable from one another. SAGE, as an alter-
native to TF-IDF and naive Bayes measures of vo-
cabulary usage, proves to be a very good indicator
of which words are most distinctive for a particu-
lar character. At the character level, looking at the
top features from the SAGE algorithm provides
insights into the easiest types of stylistic distinc-
tion one can make while creating characters. Ser-
vants and butlers are easily recognizable by their
use of words such as ‘sir’, ‘yes’, and ‘please’, and
achieve a high classification score despite having
relatively fewer quotes. In Shaw’s Pygmalion, the
character of The Flower Girl is distinguished by
her unique vocabulary of words like ‘ow’, ‘ai’, ‘–’,
‘ ’m’, ‘ah’, ‘oo’, etc. These kinds of lexical, di-
alectal features seem to be the most popular way
of creating unique character voices.

The semantic and syntactic information cap-
tured by word2vec vectors forms the other key
component of our analysis. While these dense vec-
tors are not directly interpretable, we did attempt
an initial clustering experiment with the word em-
beddings, which resulted in some insightful clus-
ters. Proper nouns were grouped into one, another
had words associated with tragedy (sad, dreadful,
miserable, awful, horrible, terrible, unfortunate),
and yet another cluster had duty, servants, rank,
ideals. These are indicative of some stylistic as-
pect of words being captured by the embeddings
which, when combined with the SAGE weights,
boosts our classification performance. However,
we reiterate that quantifying this is a hard-to-solve
problem. Our analysis with lexicon-based vectors
more concretely illustrates some of the stylistic di-
mensions along which characters and authors dif-
fer.

1Nonetheless, we acknowledge the alternative viewpoint
expressed by one of the reviewers: “It could be that the char-
acters from Rice are so rich and diverse that they cannot be
classified and that Shaw’s or Wilde’s are so exaggerated or
archetypal that even simple classification mechanisms can
recognize them.”

An interesting observation we make is that
the artificial plays do not achieve a significantly
higher score when compared to the original ones,
despite the intuition that they must deal with more
disparate topics. The number of sources of vari-
ance in creating these plays makes it hard to inter-
pret this; performing more controlled experiments
in the future might provide a better explanation.

8 Conclusion

We propose new techniques for classifying char-
acter speech in the works of seven modern drama-
tists. We show that SAGE models achieve the
highest classification scores. Our results suggest
that, in many dramatic works, characters are dis-
tinguishable with relatively high precision; that
certain playwrights are better able to create dis-
tinctive character voices; and that these play-
wrights tend to be more canonical. Given the small
size and restricted domain of our dataset, we treat
these results are preliminary. Further investigation
with a wider range of authors and genres, includ-
ing novels, would aid us in drawing more decisive
conclusions.

Acknowledgements

This work was supported financially by the Natu-
ral Sciences and Engineering Research Council of
Canada. We are grateful to the anonymous review-
ers for their helpful comments.

References
Douglas Bagnall. 2015. Author identification us-

ing multi-headed recurrent neural networks. arXiv
preprint arXiv:1506.04891.

Mikhail Bakhtin. 2013. Problems of Dostoevsky’s Po-
etics. University of Minnesota Press.

David M Blei, Andrew Y Ng, and Michael I Jordan.
2003. Latent Dirichlet allocation. Journal of Ma-
chine Learning Research, 3(Jan):993–1022.

Julian Brooke, Adam Hammond, and Graeme Hirst.
2015. GutenTag: an NLP-driven tool for digital hu-
manities research in the Project Gutenberg corpus.
In Proceedings of the Fourth Workshop on Compu-
tational Linguistics for Literature, pages 42–47.

Julian Brooke, Adam Hammond, and Graeme Hirst.
2017. Using models of lexical style to quantify
free indirect discourse in modernist fiction. Digital
Scholarship in the Humanities, 32(2):234–250.



34

Julian Brooke and Graeme Hirst. 2013. A multi-
dimensional Bayesian approach to lexical style. In
Proceedings of the 2013 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics: Human Language Technologies,
pages 673–679.

J. F. Burrows. 1987. Computation into Criticism: A
Study of Jane Austen’s Novels and an Experiment in
Method. Oxford University Press.

Jacob Eisenstein, Amr Ahmed, and Eric P Xing. 2011.
Sparse additive generative models of text. In Pro-
ceedings of the 28th International Conference on In-
ternational Conference on Machine Learning, pages
1041–1048. Omnipress.

Adam Hammond and Julian Brooke. 2016. Project Di-
alogism: Toward a computational history of vocal
diversity in English-language literature. In Digital
Humanities, pages 543–544, Kraków.

Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Cor-
rado, and Jeff Dean. 2013. Distributed representa-
tions of words and phrases and their compositional-
ity. In Advances in Neural Information Processing
Systems, pages 3111–3119.

Saif Mohammad. 2018a. Obtaining reliable human rat-
ings of valence, arousal, and dominance for 20,000
english words. In Proceedings of the 56th Annual
Meeting of the Association for Computational Lin-
guistics (Volume 1: Long Papers), volume 1, pages
174–184.

Saif Mohammad. 2018b. Word affect intensities.
In Proceedings of the Eleventh International Con-
ference on Language Resources and Evaluation
(LREC-2018).

Frederick Mosteller and David L. Wallace. 1963. Infer-
ence in an authorship problem. Journal of the Amer-
ican Statistical Association, 58(302):275–309.

Grace Muzny, Mark Algee-Hewitt, and Dan Juraf-
sky. 2017. Dialogism in the novel: A computa-
tional model of the dialogic nature of narration and
quotations. Digital Scholarship in the Humanities,
32(supplement 2):ii31–ii52.

Andrew Piper. 2018. Enumerations: Data and Literary
Study. University of Chicago Press.

Jan Rybicki. 2018. Partners in life, partners in crime?
In Arjuna Tuzzi and Michele A. Cortelazzo, editors,
Drawing Elena Ferrante’s Profile, pages 109–119.
Padova University Press.

Prasha Shrestha, Sebastian Sierra, Fabio Gonzalez,
Manuel Montes, Paolo Rosso, and Thamar Solorio.
2017. Convolutional neural networks for authorship
attribution of short texts. In Proceedings of the 15th
Conference of the European Chapter of the Associa-
tion for Computational Linguistics: Volume 2, Short
Papers, volume 2, pages 669–674.

Efstathios Stamatatos. 2009. A survey of modern au-
thorship attribution methods. Journal of the Ameri-
can Society for Information Science and Technology,
60(3):538–556.


