










































CoNLL-2011 Shared Task: Modeling Unrestricted Coreference in OntoNotes


Proceedings of the 15th Conference on Computational Natural Language Learning: Shared Task, pages 1–27,
Portland, Oregon, 23-24 June 2011. c©2011 Association for Computational Linguistics

CoNLL-2011 Shared Task:
Modeling Unrestricted Coreference in OntoNotes

Sameer Pradhan
BBN Technologies,

Cambridge, MA 02138
pradhan@bbn.com

Lance Ramshaw
BBN Technologies,

Cambridge, MA 02138
lramshaw@bbn.com

Mitchell Marcus
University of Pennsylvania,

Philadelphia, 19104
mitch@linc.cis.upenn.edu

Martha Palmer
University of Colorado,

Boulder, CO 80309
martha.palmer@colorado.edu

Ralph Weischedel
BBN Technologies,

Cambridge, MA 02138
weischedel@bbn.com

Nianwen Xue
Brandeis University,
Waltham, MA 02453
xuen@cs.brandeis.edu

Abstract

The CoNLL-2011 shared task involved pre-
dicting coreference using OntoNotes data. Re-
sources in this field have tended to be lim-
ited to noun phrase coreference, often on a
restricted set of entities, such as ACE enti-
ties. OntoNotes provides a large-scale corpus
of general anaphoric coreference not restricted
to noun phrases or to a specified set of en-
tity types. OntoNotes also provides additional
layers of integrated annotation, capturing ad-
ditional shallow semantic structure. This pa-
per briefly describes the OntoNotes annota-
tion (coreference and other layers) and then
describes the parameters of the shared task
including the format, pre-processing informa-
tion, and evaluation criteria, and presents and
discusses the results achieved by the partic-
ipating systems. Having a standard test set
and evaluation parameters, all based on a new
resource that provides multiple integrated an-
notation layers (parses, semantic roles, word
senses, named entities and coreference) that
could support joint models, should help to en-
ergize ongoing research in the task of entity
and event coreference.

1 Introduction
The importance of coreference resolution for the
entity/event detection task, namely identifying all
mentions of entities and events in text and clustering
them into equivalence classes, has been well recog-
nized in the natural language processing community.
Automatic identification of coreferring entities and
events in text has been an uphill battle for several
decades, partly because it can require world knowl-
edge which is not well-defined and partly owing to
the lack of substantial annotated data. Early work
on corpus-based coreference resolution dates back

to the mid-90s by McCarthy and Lenhert (1995)
where they experimented with using decision trees
and hand-written rules. A systematic study was
then conducted using decision trees by Soon et al.
(2001). Significant improvements have been made
in the field of language processing in general, and
improved learning techniques have been developed
to push the state of the art in coreference resolu-
tion forward (Morton, 2000; Harabagiu et al., 2001;
McCallum and Wellner, 2004; Culotta et al., 2007;
Denis and Baldridge, 2007; Rahman and Ng, 2009;
Haghighi and Klein, 2010). Various different knowl-
edge sources from shallow semantics to encyclo-
pedic knowledge are being exploited (Ponzetto and
Strube, 2005; Ponzetto and Strube, 2006; Versley,
2007; Ng, 2007). Researchers continued finding
novel ways of exploiting ontologies such as Word-
Net. Given that WordNet is a static ontology and
as such has limitation on coverage, more recently,
there have been successful attempts to utilize in-
formation from much larger, collaboratively built
resources such as Wikipedia (Ponzetto and Strube,
2006). In spite of all the progress, current techniques
still rely primarily on surface level features such as
string match, proximity, and edit distance; syntac-
tic features such as apposition; and shallow seman-
tic features such as number, gender, named entities,
semantic class, Hobbs’ distance, etc. A better idea
of the progress in the field can be obtained by read-
ing recent survey articles (Ng, 2010) and tutorials
(Ponzetto and Poesio, 2009) dedicated to this sub-
ject.

Corpora to support supervised learning of this
task date back to the Message Understanding Con-
ferences (MUC). These corpora were tagged with
coreferring entities identified by noun phrases in the
text. The de facto standard datasets for current coref-
erence studies are the MUC (Hirschman and Chin-

1



chor, 1997; Chinchor, 2001; Chinchor and Sund-
heim, 2003) and the ACE1 (G. Doddington et al.,
2000) corpora. The MUC corpora cover all noun
phrases in text, but represent small training and test
sets. The ACE corpora, on the other hand, have much
more annotation, but are restricted to a small subset
of entities. They are also less consistent, in terms of
inter-annotator agreement (ITA) (Hirschman et al.,
1998). This lessens the reliability of statistical ev-
idence in the form of lexical coverage and seman-
tic relatedness that could be derived from the data
and used by a classifier to generate better predic-
tive models. The importance of a well-defined tag-
ging scheme and consistent ITA has been well rec-
ognized and studied in the past (Poesio, 2004; Poe-
sio and Artstein, 2005; Passonneau, 2004). There
is a growing consensus that in order for these to be
most useful for language understanding applications
such as question answering or distillation – both of
which seek to take information access technology
to the next level – we need more consistent anno-
tation of larger amounts of broad coverage data for
training better automatic techniques for entity and
event identification. Identification and encoding of
richer knowledge – possibly linked to knowledge
sources – and development of learning algorithms
that would effectively incorporate them is a neces-
sary next step towards improving the current state
of the art. The computational learning community,
in general, is also witnessing a move towards eval-
uations based on joint inference, with the two pre-
vious CoNLL tasks (Surdeanu et al., 2008; Hajič et
al., 2009) devoted to joint learning of syntactic and
semantic dependencies. A principle ingredient for
joint learning is the presence of multiple layers of
semantic information.

One fundamental question still remains, and that
is – what would it take to improve the state of the art
in coreference resolution that has not been attempted
so far? Many different algorithms have been tried in
the past 15 years, but one thing that is still lacking
is a corpus comprehensively tagged on a large scale
with consistent, multiple layers of semantic infor-
mation. One of the many goals of the OntoNotes
project2 (Hovy et al., 2006; Weischedel et al., 2011)
is to explore whether it can fill this void and help
push the progress further – not only in coreference,
but with the various layers of semantics that it tries
to capture. As one of its layers, it has created a
corpus for general anaphoric coreference that cov-

1http://projects.ldc.upenn.edu/ace/data/
2http://www.bbn.com/nlp/ontonotes

ers entities and events not limited to noun phrases
or a limited set of entity types. A small portion of
this corpus from the newswire and broadcast news
genres (∼120k) was recently used for a SEMEVAL
task (Recasens et al., 2010). As mentioned earlier,
the coreference layer in OntoNotes constitutes just
one part of a multi-layered, integrated annotation of
shallow semantic structure in text with high inter-
annotator agreement, which also provides a unique
opportunity for performing joint inference over a
substantial body of data.

The remainder of this paper is organized as
follows. Section 2 presents an overview of the
OntoNotes corpus. Section 3 describes the coref-
erence annotation in OntoNotes. Section 4 then de-
scribes the shared task, including the data provided
and the evaluation criteria. Sections 5 and 6 then de-
scribe the participating system results and analyze
the approaches, and Section 7 concludes.

2 The OntoNotes Corpus

The OntoNotes project has created a corpus of large-
scale, accurate, and integrated annotation of multi-
ple levels of the shallow semantic structure in text.
The idea is that this rich, integrated annotation cov-
ering many layers will allow for richer, cross-layer
models enabling significantly better automatic se-
mantic analysis. In addition to coreference, this
data is also tagged with syntactic trees, high cov-
erage verb and some noun propositions, partial verb
and noun word senses, and 18 named entity types.
However, such multi-layer annotations, with com-
plex, cross-layer dependencies, demands a robust,
efficient, scalable mechanism for storing them while
providing efficient, convenient, integrated access to
the the underlying structure. To this effect, it uses a
relational database representation that captures both
the inter- and intra-layer dependencies and also pro-
vides an object-oriented API for efficient, multi-
tiered access to this data (Pradhan et al., 2007a).
This should facilitate the creation of cross-layer fea-
tures in integrated predictive models that will make
use of these annotations.

Although OntoNotes is a multi-lingual resource
with all layers of annotation covering three lan-
guages: English, Chinese and Arabic, for the scope
of this paper, we will just look at the English por-
tion. Over the years of the development of this cor-
pus, there were various priorities that came into play,
and therefore not all the data in the English portion is
annotated with all the different layers of annotation.
There is a core portion, however, which is roughly

2



1.3M words which has been annotated with all the
layers. It comprises ∼450k words from newswire,
∼150k from magazine articles, ∼200k from broad-
cast news, ∼200k from broadcast conversations and
∼200k web data.

OntoNotes comprises the following layers of an-
notation:

• Syntax – A syntactic layer representing a re-
vised Penn Treebank (Marcus et al., 1993;
Babko-Malaya et al., 2006).

• Propositions – The proposition structure of
verbs in the form of a revised PropBank(Palmer
et al., 2005; Babko-Malaya et al., 2006).

• Word Sense – Coarse grained word senses
are tagged for the most frequent polysemous
verbs and nouns, in order to maximize cov-
erage. The word sense granularity is tailored
to achieve 90% inter-annotator agreement as
demonstrated by Palmer et al. (2007). These
senses are defined in the sense inventory files
and each individual sense has been connected
to multiple WordNet senses. This provides a
direct access to the WordNet semantic struc-
ture for users to make use of. There is also a
mapping from the word senses to the PropBank
frames and to VerbNet (Kipper et al., 2000) and
FrameNet (Fillmore et al., 2003).

• Named Entities – The corpus was tagged with
a set of 18 proper named entity types that
were well-defined and well-tested for inter-
annotator agreement by Weischedel and Burn-
stein (2005).

• Coreference – This layer captures general
anaphoric coreference that covers entities and
events not limited to noun phrases or a limited
set of entity types (Pradhan et al., 2007b). We
will take a look at this in detail in the next sec-
tion.

3 Coreference in OntoNotes

General anaphoric coreference that spans a rich set
of entities and events – not restricted to a few types,
as has been characteristic of most coreference data
available until now – has been tagged with a high
degree of consistency. Attributive coreference is
tagged separately from the more common identity
coreference.

Two different types of coreference are distin-
guished in the OntoNotes data: Identical (IDENT),

and Appositive (APPOS). Appositives are treated
separately because they function as attributions, as
described further below. The IDENT type is used
for anaphoric coreference, meaning links between
pronominal, nominal, and named mentions of spe-
cific referents. It does not include mentions of
generic, underspecified, or abstract entities.

Coreference is annotated for all specific entities
and events. There is no limit on the semantic types
of NP entities that can be considered for coreference,
and in particular, coreference is not limited to ACE
types.

The mentions over which IDENT coreference ap-
plies are typically pronominal, named, or definite
nominal. The annotation process begins by auto-
matically extracting all of the NP mentions from the
Penn Treebank, though the annotators can also add
additional mentions when appropriate. In the fol-
lowing two examples (and later ones), the phrases
notated in bold form the links of an IDENT chain.

(1) She had a good suggestion and it was unani-
mously accepted by all.

(2) Elco Industries Inc. said it expects net income
in the year ending June 30, 1990, to fall below a
recent analyst’s estimate of $ 1.65 a share. The
Rockford, Ill. maker of fasteners also said it
expects to post sales in the current fiscal year
that are “slightly above” fiscal 1989 sales of $
155 million.

3.1 Verbs

Verbs are added as single-word spans if they can
be coreferenced with a noun phrase or with an-
other verb. The intent is to annotate the VP, but we
mark the single-word head for convenience. This in-
cludes morphologically related nominalizations (3)
and noun phrases that refer to the same event, even
if they are lexically distinct from the verb (4). In the
following two examples, only the chains related to
the growth event are shown.

(3) Sales of passenger cars grew 22%. The strong
growth followed year-to-year increases.

(4) Japan’s domestic sales of cars, trucks and buses
in October rose 18% from a year earlier to
500,004 units, a record for the month, the Japan
Automobile Dealers’ Association said. The
strong growth followed year-to-year increases
of 21% in August and 12% in September.

3



3.2 Pronouns
All pronouns and demonstratives are linked to any-
thing that they refer to, and pronouns in quoted
speech are also marked. Expletive or pleonastic pro-
nouns (it, there) are not considered for tagging, and
generic you is not marked. In the following exam-
ple, the pronoun you and it would not be marked. (In
this and following examples, an asterisk (*) before a
boldface phrase identifies entity/event mentions that
would not be tagged as coreferent.)

(5) Senate majority leader Bill Frist likes to tell a
story from his days as a pioneering heart sur-
geon back in Tennessee. A lot of times, Frist re-
calls, *you’d have a critical patient lying there
waiting for a new heart, and *you’d want to
cut, but *you couldn’t start unless *you knew
that the replacement heart would make *it to
the operating room.

3.3 Generic mentions
Generic nominal mentions can be linked with refer-
ring pronouns and other definite mentions, but are
not linked to other generic nominal mentions. This
would allow linking of the bracketed mentions in (6)
and (7), but not (8).

(6) Officials said they are tired of making the same
statements.

(7) Meetings are most productive when they are
held in the morning. Those meetings, however,
generally have the worst attendance.

(8) Allergan Inc. said it received approval to
sell the PhacoFlex intraocular lens, the first
foldable silicone lens available for *cataract
surgery. The lens’ foldability enables it to be
inserted in smaller incisions than are now pos-
sible for *cataract surgery.

Bare plurals, as in (6) and (7), are always consid-
ered generic. In example (9) below, there are two
generic instances of parents. These are marked as
distinct IDENT chains (with separate chains distin-
guished by subscripts X, Y and Z), each containing
a generic and the related referring pronouns.

(9) ParentsX should be involved with theirX chil-
dren’s education at home, not in school. TheyX
should see to it that theirX kids don’t play tru-
ant; theyX should make certain that the children
spend enough time doing homework; theyX
should scrutinize the report card. ParentsY are

too likely to blame schools for the educational
limitations of theirY children. If parentsZ are
dissatisfied with a school, theyZ should have
the option of switching to another.

In (10) below, the verb “halve” cannot be linked
to “a reduction of 50%”, since “a reduction” is in-
definite.

(10) Argentina said it will ask creditor banks to
*halve its foreign debt of $64 billion – the
third-highest in the developing world . Ar-
gentina aspires to reach *a reduction of 50%
in the value of its external debt.

3.4 Pre-modifiers
Proper pre-modifiers can be coreferenced, but
proper nouns that are in a morphologically adjecti-
val form are treated as adjectives, and not corefer-
enced. For example, adjectival forms of GPEs such
as Chinese in “the Chinese leader”, would not be
linked. Thus we could coreference United States in
“the United States policy” with another referent, but
not American “the American policy.” GPEs and Na-
tionality acronyms (e.g. U.S.S.R. or U.S.). are also
considered adjectival. Pre-modifier acronyms can be
coreferenced unless they refer to a nationality. Thus
in the examples below, FBI can be coreferenced to
other mentions, but U.S. cannot.

(11) FBI spokesman

(12) *U.S. spokesman

Dates and monetary amounts can be considered
part of a coreference chain even when they occur as
pre-modifiers.

(13) The current account deficit on France’s balance
of payments narrowed to 1.48 billion French
francs ($236.8 million) in August from a re-
vised 2.1 billion francs in July, the Finance
Ministry said. Previously, the July figure was
estimated at a deficit of 613 million francs.

(14) The company’s $150 offer was unexpected.
The firm balked at the price.

3.5 Copular verbs
Attributes signaled by copular structures are not
marked; these are attributes of the referent they mod-
ify, and their relationship to that referent will be
captured through word sense and propositional ar-
gument tagging.

4



(15) JohnX is a linguist. PeopleY are nervous
around JohnX, because heX always corrects
theirY grammar.

Copular (or ’linking’) verbs are those verbs that
function as a copula and are followed by a sub-
ject complement. Some common copular verbs are:
be, appear, feel, look, seem, remain, stay, become,
end up, get. Subject complements following such
verbs are considered attributes, and not linked. Since
Called is copular, neither IDENT nor APPOS corefer-
ence is marked in the following case.

(16) Called Otto’s Original Oat Bran Beer, the brew
costs about $12.75 a case.

3.6 Small clauses
Like copulas, small clause constructions are not
marked. The following example is treated as if the
copula were present (“John considers Fred to be an
idiot”):

(17) John considers *Fred *an idiot.
3.7 Temporal expressions
Temporal expressions such as the following are
linked:

(18) John spent three years in jail. In that time...
Deictic expressions such as now, then, today, to-

morrow, yesterday, etc. can be linked, as well as
other temporal expressions that are relative to the
time of the writing of the article, and which may
therefore require knowledge of the time of the writ-
ing to resolve the coreference. Annotators were al-
lowed to use knowledge from outside the text in re-
solving these cases. In the following example, the
end of this period and that time can be coreferenced,
as can this period and from three years to seven
years.

(19) The limit could range from three years to
seven yearsX, depending on the composition
of the management team and the nature of its
strategic plan. At (the end of (this period)X)Y,
the poison pill would be eliminated automati-
cally, unless a new poison pill were approved
by the then-current shareholders, who would
have an opportunity to evaluate the corpora-
tion’s strategy and management team at that
timeY.

In multi-date temporal expressions, embedded
dates are not separately connected to to other men-
tions of that date. For example in Nov. 2, 1999, Nov.
would not be linked to another instance of November
later in the text.

3.8 Appositives
Because they logically represent attributions, appos-
itives are tagged separately from Identity corefer-
ence. They consist of a head, or referent (a noun
phrase that points to a specific object/concept in the
world), and one or more attributes of that referent.
An appositive construction contains a noun phrase
that modifies an immediately-adjacent noun phrase
(separated only by a comma, colon, dash, or paren-
thesis). It often serves to rename or further define
the first mention. Marking appositive constructions
allows us to capture the attributed property even
though there is no explicit copula.

(20) Johnhead, a linguistattribute

The head of each appositive construction is distin-
guished from the attribute according to the following
heuristic specificity scale, in a decreasing order from
top to bottom:

Type Example

Proper noun John
Pronoun He
Definite NP the man
Indefinite specific NP a man I know
Non-specific NP man

This leads to the following cases:

(21) Johnhead, a linguistattribute

(22) A famous linguistattribute, hehead studied at ...

(23) a principal of the firmattribute, J. Smithhead

In cases where the two members of the appositive
are equivalent in specificity, the left-most member of
the appositive is marked as the head/referent. Defi-
nite NPs include NPs with a definite marker (the) as
well as NPs with a possessive adjective (his). Thus
the first element is the head in all of the following
cases:

(24) The chairman, the man who never gives up

(25) The sheriff, his friend

(26) His friend, the sheriff

In the specificity scale, specific names of diseases
and technologies are classified as proper names,
whether they are capitalized or not.

(27) A dangerous bacteria, bacillium, is found

5



Type Description

Annotator Error An annotator error. This is a catch-all category for cases of errors that do not fit in the other
categories.

Genuine Ambiguity This is just genuinely ambiguous. Often the case with pronouns that have no clear an-
tecedent (especially this & that)

Generics One person thought this was a generic mention, and the other person didn’t
Guidelines The guidelines need to be clear about this example
Callisto Layout Something to do with the usage/design of Callisto
Referents Each annotator thought this was referring to two completely different things
Possessives One person did not mark this possessive
Verb One person did not mark this verb
Pre Modifiers One person did not mark this Pre Modifier
Appositive One person did not mark this appositive
Extent Both people marked the same entity, but one person’s mention was longer
Copula Disagreement arose because this mention is part of a copular structure

a) Either each annotator marked a different half of the copula
b) Or one annotator unnecessarily marked both

Figure 1: Description of various disagreement types

Figure 1: The distribution of disagreements across the various types in Table 2

Sheet1

Page 1

Copulae 2%
Appositives 3%
Pre Modifiers 3%
Verbs 3%
Possessives 4%
Referents 7%
Callisto Layout 8%
Guidelines 8%
Generics 11%
Genuine Ambiguity 25%
Annotator Error 26%

Copulae

Appositives

Pre Modifiers

Verbs

Possessives

Referents

Callisto Layout

Guidelines

Generics

Genuine Ambiguity

Annotator Error

0% 5% 10% 15% 20% 25% 30%

Figure 2: The distribution of disagreements across the various types in Table 1

When the entity to which an appositive refers is
also mentioned elsewhere, only the single span con-
taining the entire appositive construction is included
in the larger IDENT chain. None of the nested NP
spans are linked. In the example below, the en-
tire span can be linked to later mentions to Richard
Godown. The sub-spans are not included separately
in the IDENT chain.

(28) Richard Godown, president of the Indus-
trial Biotechnology Association

Ages are tagged as attributes (as if they were el-
lipses of, for example, a 42-year-old):

(29) Mr.Smithhead, 42attribute,

3.9 Special Issues
In addition to the ones above, there are some special
cases such as:

• No coreference is marked between an organi-
zation and its members.

Genre ANN1-ANN2 ANN1-ADJ ANN2-ADJ

Newswire 80.9 85.2 88.3
Broadcast News 78.6 83.5 89.4
Broadcast Conversation 86.7 91.6 93.7
Magazine 78.4 83.2 88.8
Web 85.9 92.2 91.2

Table 1: Inter Annotator and Adjudicator agreement for
the Coreference Layer in OntoNotes measured in terms
of the MUC score.

• GPEs are linked to references to their govern-
ments, even when the references are nested
NPs, or the modifier and head of a single NP.

3.10 Annotator Agreement and Analysis
Table 1 shows the inter-annotator and annotator-
adjudicator agreement on all the genres of
OntoNotes. We also analyzed about 15K dis-
agreements in various parts of the data, and grouped
them into one of the categories shown in Figure 1.
Figure 2 shows the distribution of these different
types that were found in that sample. It can be

6



seen that genuine ambiguity and annotator error
are the biggest contributors – the latter of which is
usually captured during adjudication, thus showing
the increased agreement between the adjudicated
version and the individual annotator version.

4 CoNLL-2011 Coreference Task

This section describes the CoNLL-2011 Corefer-
ence task, including its closed and open track ver-
sions, and characterizes the data used for the task
and how it was prepared.

4.1 Why a Coreference Task?
Despite close to a two-decade history of evaluations
on coreference tasks, variation in the evaluation cri-
teria and in the training data used have made it dif-
ficult for researchers to be clear about the state of
the art or to determine which particular areas require
further attention. There are many different parame-
ters involved in defining a coreference task. Looking
at various numbers reported in literature can greatly
affect the perceived difficulty of the task. It can seem
to be a very hard problem (Soon et al., 2001) or one
that is somewhat easier (Culotta et al., 2007). Given
the space constraints, we refer the reader to Stoy-
anov et al. (2009) for a detailed treatment of the
issue.

Limitations in the size and scope of the available
datasets have also constrained research progress.
The MUC and ACE corpora are the two that have
been used most for reporting comparative results,
but they differ in the types of entities and corefer-
ence annotated. The ACE corpus is also one that
evolved over a period of almost five years, with dif-
ferent incarnations of the task definition and dif-
ferent corpus cross-sections on which performance
numbers have been reported, making it hard to un-
tangle and interpret the results.

The availability of the OntoNotes data offered an
opportunity to define a coreference task based on a
larger, more broad-coverage corpus. We have tried
to design the task so that it not only can support the
current evaluation, but also can provide an ongoing
resource for comparing different coreference algo-
rithms and approaches.

4.2 Task Description
The CoNLL-2011 shared task was based on the En-
glish portion of the OntoNotes 4.0 data. The task
was to automatically identify mentions of entities
and events in text and to link the coreferring men-
tions together to form entity/event chains. The target

coreference decisions could be made using automat-
ically predicted information on the other structural
layers including the parses, semantic roles, word
senses, and named entities.

As is customary for CoNLL tasks, there were two
tracks, closed and open. For the closed track, sys-
tems were limited to using the distributed resources,
in order to allow a fair comparison of algorithm per-
formance, while the open track allowed for almost
unrestricted use of external resources in addition to
the provided data.

4.2.1 Closed Track
In the closed track, systems were limited to the pro-
vided data, plus the use of two pre-specified external
resources: i) WordNet and ii) a pre-computed num-
ber and gender table by Bergsma and Lin (2006).

For the training and test data, in addition to the
underlying text, predicted versions of all the supple-
mentary layers of annotation were provided, where
those predictions were derived using off-the-shelf
tools (parsers, semantic role labelers, named entity
taggers, etc.) as described in Section 4.4.2. For the
training data, however, in addition to predicted val-
ues for the other layers, we also provided manual
gold-standard annotations for all the layers. Partici-
pants were allowed to use either the gold-standard or
predicted annotation for training their systems. They
were also free to use the gold-standard data to train
their own models for the various layers of annota-
tion, if they judged that those would either provide
more accurate predictions or alternative predictions
for use as multiple views, or wished to use a lattice
of predictions.

More so than previous CoNLL tasks, corefer-
ence predictions depend on world knowledge, and
many state-of-the-art systems use information from
external resources such as WordNet, which can
add a layer that helps the system to recognize se-
mantic connections between the various lexical-
ized mentions in the text. Therefore, the use of
WordNet was allowed, even for the closed track.
Since word senses in OntoNotes are predominantly3
coarse-grained groupings of WordNet senses, sys-
tems could also map from the predicted or gold-
standard word senses provided to the sets of under-
lying WordNet senses. Another significant piece of
knowledge that is particularly useful for coreference
but that is not available in the layers of OntoNotes is
that of number and gender. There are many different

3There are a few instances of novel senses introduced in
OntoNotes which were not present in WordNet, and so lack a
mapping back to the WordNet senses

7



ways of predicting these values, with differing accu-
racies, so in order to ensure that participants in the
closed track were working from the same data, thus
allowing clearer algorithmic comparisons, we spec-
ified a particular table of number and gender predic-
tions generated by Bergsma and Lin (2006), for use
during both training and testing.

Following the recent CoNLL tradition, partici-
pants were allowed to use both the training and the
development data for training the final model.

4.2.2 Open Track
In addition to resources available in the closed track,
the open track, systems were allowed to use external
resources such as Wikipedia, gazetteers etc. This
track is mainly to get an idea of a performance ceil-
ing on the task at the cost of not getting a compar-
ison across all systems. Another advantage of the
open track is that it might reduce the barriers to par-
ticipation by allowing participants to field existing
research systems that already depend on external re-
sources – especially if there were hard dependen-
cies on these resources. They can participate in the
task with minimal or no modification to their exist-
ing system.

4.3 Coreference Task Data
Since there are no previously reported numbers on
the full version of OntoNotes, we had to create
a train/development/test partition. The only por-
tion of OntoNotes that has a previously determined,
widely used, standard split is the WSJ portion of the
newswire data. For that subcorpus, we maintained
the same partition. For all the other portions we cre-
ated stratified training, development and test parti-
tions over all the sources in OntoNotes using the pro-
cedure shown in Algorithm 1. The list of training,
development and test document IDs can be found on
the task webpage.4

4.4 Data Preparation
This section gives details of the different annota-
tion layers including the automatic models that were
used to predict them, and describes the formats in
which the data were provided to the participants.

4.4.1 Manual Annotation Gold Layers
We will take a look at the manually annotated, or
gold layers of information that were made available
for the training data.

4http://conll.bbn.com/download/conll-train.id
http://conll.bbn.com/download/conll-dev.id
http://conll.bbn.com/download/conll-test.id

Algorithm 1 Procedure used to create OntoNotes
training, development and test partitions.
Procedure: GENERATE PARTITIONS(ONTONOTES) returns TRAIN,
DEV, TEST
1: TRAIN← ∅
2: DEV← ∅
3: TEST← ∅
4: for all SOURCE ∈ ONTONOTES do
5: if SOURCE = WALL STREET JOURNAL then
6: TRAIN← TRAIN ∪ SECTIONS 02 – 21
7: DEV← DEV ∪ SECTIONS 00, 01, 22, 24
8: TEST← TEST ∪ SECTION 23
9: else

10: if Number of files in SOURCE ≥ 10 then
11: TRAIN← TRAIN ∪ FILE IDS ending in 1 – 8
12: DEV← DEV ∪ FILE IDS ending in 0
13: TEST← TEST ∪ FILE IDS ending in 9
14: else
15: DEV← DEV ∪ FILE IDS ending in 0
16: TEST← TEST ∪ FILE ID ending in the highest number
17: TRAIN← TRAIN ∪ Remaining FILE IDS for the

SOURCE
18: end if
19: end if
20: end for
21: return TRAIN, DEV, TEST

Coreference The manual coreference annotation
is stored as chains of linked mentions connecting
multiple mentions of the same entity. Coreference is
the only document-level phenomenon in OntoNotes,
and the complexity of annotation increases non-
linearly with the length of a document. Unfortu-
nately, some of the documents – especially ones in
the broadcast conversation, weblogs, and telephone
conversation genre – are very long which prohib-
ited us from efficiently annotating them in entirety.
These had to be split into smaller parts. We con-
ducted a few passes to join some adjacent parts, but
since some documents had as many as 17 parts, there
are still multi-part documents in the corpus. Since
the coreference chains are coherent only within each
of these document parts, for this task, each such part
is treated as a separate document. Another thing
to note is that there were some cases of sub-token
annotation in the corpus owing to the fact that to-
kens were not split at hyphens. Cases such as pro-
WalMart had the sub-span WalMart linked with another
instance of the same. The recent Treebank revision
which split tokens at most hyphens, made a majority
of these sub-token annotations go away. There were
still some residual sub-token annotations. Since
subtoken annotations cannot be represented in the
CoNLL format, and they were a very small quantity
– much less than even half a percent – we decided to
ignore them.

For various reasons, not all the documents in
OntoNotes have been annotated with all the differ-

8



Corpora Words Documents
Total Train Dev Test Total Train Dev Test

MUC-6 25K 12K 13K 60 30 30
MUC-7 40K 19K 21K 67 30 37
ACE (2000-2004) 1M 775K 235K - - -
OntoNotes5 1.3M 1M 136K 142K 2,083

(2,999)
1,674

(2,374)
202

(303)
207

(322)

Table 2: Number of documents in the OntoNotes data, and some comparison with the MUC and ACE data sets. The
numbers in parenthesis for the OntoNotes corpus indicate the total number of parts that correspond to the documents.
Each part was considered a separate document for evaluation purposes.

Syntactic category Train Development Test
Count % Count % Count %

NP 60,345 59.71 8,463 59.31 8,629 53.09
PRP 25,472 25.21 3,535 24.78 5,012 30.84
PRP$ 8,889 8.80 1,208 8.47 1,466 9.02
NNP 2,643 2.62 468 3.28 475 2.92
NML 900 0.89 151 1.06 118 0.73
Vx 1,915 1.89 317 2.22 314 1.93
Other 893 0.88 126 0.88 239 1.47
Overall 101,057 100.00 14,268 100.00 16,253 100.00

Table 3: Distribution of mentions in the data by their syn-
tactic category.

Train Development Test

Entities/Chains 26,612 3,752 3,926
Links 74,652 10,539 12,365
Mentions 101,264 14,291 16,291

Table 4: Number of entities, links and mentions in the
OntoNotes 4.0 data.

ent layers of annotation, with full coverage.6 There
is a core portion, however, which is roughly 1.3M
words which has been annotated with all the layers.
This is the portion that we used for the shared task.

The number of documents in the corpus for this
task, for each of the different genres, are shown in
Table 2. Tables 3 and 4 shows the distribution of
mentions by the syntactic categories, and the counts
of entities, links and mentions in the corpus respec-
tively. All of this data has been Treebanked and
PropBanked either as part of the OntoNotes effort
or some preceding effort.

For comparison purposes, Table 2 also lists the
number of documents in the MUC-6, MUC-7, and
ACE (2000-2004) corpora. The MUC-6 data was
taken from the Wall Street Journal, whereas the
MUC-7 data was from the New York Times. The
ACE data spanned many different genres similar to

6Given the nature of word sense annotation, and changes in
project priorities, we could not annotate all the low frequency
verbs and nouns in the corpus. Furthermore, PropBank annota-
tion currently only covers verb predicates.

the ones in OntoNotes.

Parse Trees This represents the syntactic layer
that is a revised version of the Penn Treebank. For
purposes of this task, traces were removed from the
syntactic trees, since the CoNLL-style data format,
being indexed by tokens, does not provide any good
means of conveying that information. Function tags
were also removed, since the parsers that we used
for the predicted syntax layer did not provide them.
One thing that needs to be dealt with in conversa-
tional data is the presence of disfluencies (restarts,
etc.). In the original OntoNotes parses, these are
marked using a special EDITED7 phrase tag – as was
the case for the Switchboard Treebank. Given the
frequency of disfluencies and the performance with
which one can identify them automatically,8 a prob-
able processing pipeline would filter them out be-
fore parsing. Since we did not have a readily avail-
able tagger for tagging disfluencies, we decided to
remove them using oracle information available in
the Treebank.

Propositions The propositions in OntoNotes con-
stitute PropBank semantic roles. Most of the verb
predicates in the corpus have been annotated with
their arguments. Recent enhancements to the Prop-
Bank to make it synchronize better with the Tree-
bank (Babko-Malaya et al., 2006) have enhanced
the information in the proposition by the addition of
two types of LINKs that represent pragmatic corefer-
ence (LINK-PCR) and selectional preferences (LINK-
SLC). More details can be found in the addendum to
the PropBank guidelines9 in the OntoNotes 4.0 re-

7There is another phrase type – EMBED in the telephone con-
versation genre which is similar to the EDITED phrase type, and
sometimes identifies insertions, but sometimes contains logical
continuation of phrases, so we decided not to remove that from
the data.

8A study by Charniak and Johnson (2001) shows that one
can identify and remove edits from transcribed conversational
speech with an F-score of about 78, with roughly 95 Precision
and 67 recall.

9doc/propbank/english-propbank.pdf

9



lease. Since the community is not used to this rep-
resentation which relies heavily on the trace struc-
ture in the Treebank which we are excluding, we de-
cided to unfold the LINKs back to their original rep-
resentation as in the Release 1.0 of the Proposition
Bank. This functionality is part of the OntoNotes
DB Tool.10

Word Sense Gold word sense annotation was
supplied using sense numbers as specified in
the OntoNotes list of senses for each lemma.11
The sense inventories that were provided in the
OntoNotes 4.0 release were not all mapped to the lat-
est version 3.0 of WordNet, so we provided a revised
version of the sense inventories, containing mapping
to WordNet 3.0, on the task page for the participants.

Named Entities Named Entities in OntoNotes
data are specified using a catalog of 18 Name types.

Other Layers Discourse plays a vital role in
coreference resolution. In the case of broadcast con-
versation, or telephone conversation data, it partially
manifests in the form of speakers of a given utter-
ance, whereas in weblogs or newsgroups it does so
as the writer, or commenter of a particular article
or thread. This information provides an important
clue for correctly linking anaphoric pronouns with
the right antecedents. This information could be au-
tomatically deduced, but since it would add addi-
tional complexity to the already complex task, we
decided to provide oracle information of this meta-
data both during training and testing. In other words,
speaker and author identification was not treated
as an annotation layer that needed to be predicted.
This information was provided in the form of an-
other column in the .conll table. There were some
cases of interruptions and interjections that ideally
would associate parts of a sentence to two different
speakers, but since the frequency of this was quite
small, we decided to make an assumption of one
speaker/writer per sentence.

4.4.2 Predicted Annotation Layers
The predicted annotation layers were derived using
automatic models trained using cross-validation on
other portions of OntoNotes data. As mentioned ear-
lier, there are some portions of the OntoNotes corpus
that have not been annotated for coreference but that
have been annotated for other layers. For training

10http://cemantix.org/ontonotes.html
11It should be noted that word sense annotation in OntoNotes

is note complete, so only some of the verbs and nouns have
word sense tags specified.

Senses Lemmas

1 1,506
2 1,046

> 2 1,016

Table 6: Word sense polysemy over verb and noun lem-
mas in OntoNotes

models for each of the layers, where feasible, we
used all the data that we could for that layer from
the training portion of the entire OntoNotes release.

Parse Trees Predicted parse trees were produced
using the Charniak parser (Charniak and Johnson,
2005).12 Some additional tag types used in the
OntoNotes trees were added to the parser’s tagset,
including the NML tag that has recently been added
to capture internal NP structure, and the rules used to
determine head words were appropriately extended.
The parser was then re-trained on the training por-
tion of the release 4.0 data using 10-fold cross-
validation. Table 5 shows the performance of the
re-trained Charniak parser on the CoNLL-2011 test
set. We did not get a chance to re-train the re-ranker,
and since the stock re-ranker crashes when run on n-
best parses containing NMLs, because it has not seen
that tag in training, we could not make use of it.

Word Sense We trained a word sense tagger us-
ing a SVM classifier and contextual word and part
of speech features on all the training portion of the
OntoNotes data. The OntoNotes 4.0 corpus com-
prises a total of 14,662 sense definitions across 4877
verb and noun lemmas13. The distribution of senses
per lemma is as shown in Table 6. Table 7 shows
the performance of this classifier over both the verbs
and nouns in the CoNLL-2011 test set. Again this
performance is not directly comparable to any re-
ported in the literature before, and it seems lower
then performances reported on previous versions
of OntoNotes because this is over all the genres
of OntoNotes, and aggregated over both verbs and
nouns in the CoNLL-2011 test set.

Propositions To predict propositional structure,
ASSERT14 (Pradhan et al., 2005) was used, re-
trained also on all the training portion of the release

12http://bllip.cs.brown.edu/download/reranking-
parserAug06.tar.gz

13The number of lemmas in Table 6 do not add up to this
number because not all of them have examples in the training
data, where the total number of instantiated senses amounts to
7933.

14http://cemantix.org/assert.html

10



All Sentences Sentence len < 40
N POS R P F N R P F

Broadcast Conversation (BC) 2,194 95.93 84.30 84.46 84.38 2124 85.83 85.97 85.90
Broadcast News (BN) 1,344 96.50 84.19 84.28 84.24 1278 85.93 86.04 85.98
Magazine (MZ) 780 95.14 87.11 87.46 87.28 736 87.71 88.04 87.87
Newswire (NW) 2,273 96.95 87.05 87.45 87.25 2082 88.95 89.27 89.11
Telephone Conversation (TC) 1,366 93.52 79.73 80.83 80.28 1359 79.88 80.98 80.43
Weblogs and Newsgroups (WB) 1,658 94.67 83.32 83.20 83.26 1566 85.14 85.07 85.11

Overall 9,615 96.03 85.25 85.43 85.34 9145 86.86 87.02 86.94

Table 5: Parser performance on the CoNLL-2011 test set

Frameset Total Total % Perfect Argument ID + Class
Accuracy Sentences Propositions Propositions P R F

Broadcast Conversation (BC) 0.92 2,037 5,021 52.18 82.55 64.84 72.63
Broadcast News (BN) 0.91 1,252 3,310 53.66 81.64 64.46 72.04
Magazine (MZ) 0.89 780 2,373 47.16 79.98 61.66 69.64
Newswire (NW) 0.93 1,898 4,758 39.72 80.53 62.68 70.49
Weblogs and Newsgroups (WB) 0.92 929 2,174 39.19 81.01 60.65 69.37

Overall 0.91 6,896 17,636 46.82 81.28 63.17 71.09

Table 8: Performance on the propositions and framesets in the CoNLL-2011 test set.

Accuracy

Broadcast Conversation (BC) 0.70
Broadcast News (BN) 0.68
Magazine (MZ) 0.60
Newswire (NW) 0.62
Weblogs and Newsgroups (WB) 0.63

Overall 0.65

Table 7: Word sense performance over both verbs and
nouns in the CoNLL-2011 test set

4.0 data. Given time constraints, we had to per-
form two modifications: i) Instead of a single model
that predicts all arguments including NULL argu-
ments, we had to use the two-stage mode where the
NULL arguments are first filtered out and the remain-
ing NON-NULL arguments are classified into one of
the argument types, and ii) The argument identifi-
cation module used an ensemble of ten classifiers
– each trained on a tenth of the training data and
performed an unweighted voting among them. This
should still give a close to state of the art perfor-
mance given that the argument identification perfor-
mance tends to start to be asymptotic around 10k
training instances. At first glance, the performance
on the newswire genre is much lower than what has
been reported for WSJ Section 23. This could be
attributed to two factors: i) the fact that we had to
compromise on the training method, but more im-
portantly because ii) the newswire in OntoNotes not
only contains WSJ data, but also Xinhua news. One

could try to verify using just the WSJ portion of the
data, but it would be hard as it is not only a sub-
set of the documents that the performance has been
reported on previously, but also the annotation has
been significantly revised; it includes propositions
for be verbs missing from the original PropBank,
and the training data is a subset of the original data
as well. Table 8 shows the detailed performance
numbers.

In addition to automatically predicting the argu-
ments, we also trained a classifier to tag PropBank
frameset IDs in the data using the same word sense
module as mentioned earlier. OntoNotes 4.0 con-
tains a total of 7337 framesets across 5433 verb
lemmas.15 An overwhelming number of them are
monosemous, but the more frequent verbs tend to be
polysemous. Table 9 gives the distribution of num-
ber of framesets per lemma in the PropBank layer of
the OntoNotes 4.0 data.

During automatic processing of the data, we
tagged all the tokens that were tagged with a part
of speech VBx. This means that there would be cases
where the wrong token would be tagged with propo-
sitions. The CoNLL-2005 scorer was used to gener-
ate the scores.

Named Entities BBN’s IdentiFinderTMsystem
was used to predict the named entities. Given the

15The number of lemmas in Table 9 do not add up to this
number because not all of them have examples in the training
data, where the total number of instantiated senses amounts to
4229.

11



Framesets Lemmas

1 2,722
2 321

> 2 181

Table 9: Frameset polysemy across lemmas

Overall BC BN MZ NW TC WB
F F F F F F F

ALL Named Entities 71.8 64.8 72.2 61.5 84.3 39.5 55.2

Cardinal 68.7 51.8 71.1 66.1 82.8 34.0 68.7
Date 76.1 63.7 77.9 66.7 83.7 60.5 56.0
Event 27.6 00.0 34.8 30.8 47.6 - 13.3
Facility 41.9 55.0 16.7 23.1 66.7 00.0 22.9
GPE 87.9 87.5 90.3 73.7 92.9 65.9 88.7
Language 41.2 - 50.0 50.0 00.0 20.0 75.0
Law 63.0 00.0 85.7 00.0 67.9 00.0 50.0
Location 58.4 59.1 59.6 53.3 68.0 00.0 23.5
Money 74.6 16.7 66.7 73.2 79.4 30.8 61.5
NORP 00.0 00.0 00.0 00.0 00.0 00.0 00.0
Ordinal 73.4 73.8 73.4 78.1 78.4 88.9 37.0
Organization 71.0 57.8 67.1 52.9 86.9 21.2 32.1
Percent 71.2 88.9 76.9 69.6 92.1 01.2 71.6
Person 79.6 78.9 87.7 66.7 91.6 65.1 64.8
Product 46.9 00.0 43.8 00.0 81.8 00.0 00.0
Quantity 47.5 25.3 58.3 61.1 71.9 00.0 22.2
Time 58.6 56.9 64.1 42.9 80.0 23.8 51.7
Work of Art 41.9 26.9 37.1 16.0 77.9 00.0 05.6

Table 10: Named Entity performance on the CoNLL-
2011 test set

time constraints, we could not re-train it on the
OntoNotes data and so an existing, pre-trained
model was used, therefore the results are not a
good indicator of the model’s best performance.
The pre-trained model had also used a somewhat
different catalog of name types, which did not
include the OntoNotes NORP type (for nationalities,
organizations, religions, and political parties),
so that category was never predicted. Table 10
shows the overall performance of the tagger on the
CoNLL-2011 test set, as well as the performance
broken down by individual name types. IdentiFinder
performance has been reported to be in the low 90’s
on WSJ test set.

Other Layers As noted above, systems were al-
lowed to make use of gender and number predic-
tions for NPs using the table from Bergsma and Lin
(Bergsma and Lin, 2006).

4.4.3 Data Format
In order to organize the multiple, rich layers of anno-
tation, the OntoNotes project has created a database
representation for the raw annotation layers along
with a Python API to manipulate them (Pradhan et
al., 2007a). In the OntoNotes distribution the data is

organized as one file per layer, per document. The
API requires a certain hierarchical structure with
documents at the leaves inside a hierarchy of lan-
guage, genre, source and section. It comes with var-
ious ways of cleanly querying and manipulating the
data and allows convenient access to the sense in-
ventory and propbank frame files instead of having
to interpret the raw .xml versions. However, main-
taining format consistency with earlier CoNLL tasks
was deemed convenient for sites that already had
tools configured to deal with that format. Therefore,
in order to distribute the data so that one could make
the best of both worlds, we created a new file type
called .conll which logically served as another layer
in addition to the .parse, .prop, .name and .coref
layers. Each .conll file contained a merged repre-
sentation of all the OntoNotes layers in the CoNLL-
style tabular format with one line per token, and with
multiple columns for each token specifying the input
annotation layers relevant to that token, with the fi-
nal column specifying the target coreference layer.
Because OntoNotes is not authorized to distribute
the underlying text, and many of the layers contain
inline annotation, we had to provide a skeletal form
(.skel of the .conll file which was essentially the
.conll file, but with the word column replaced with
a dummy string. We provided an assembly script
that participants could use to create a .conll file tak-
ing as input the .skel file and the top-level directory
of the OntoNotes distribution that they had sepa-
rately downloaded from the LDC16 Once the .conll
file is created, it can be used to create the individual
layers such as .parse, .name, .coref etc. using an-
other set of scripts. Since the propositions and word
sense layers are inherently standoff annotation, they
were provided as is, and did not require that extra
merging step. One thing thing that made this data
creation process a bit tricky was the fact that we had
dissected some of the trees for the conversation data
to remove the EDITED phrases. Table 11 describes
the data provided in each of the column of the .conll
format. Figure 3 shows a sample from a .conll file.

4.5 Evaluation
This section describes the evaluation criteria used.
Unlike for propositions, word sense and named en-
tities, where it is simply a matter of counting the
correct answers, or for parsing, where there are sev-
eral established metrics, evaluating the accuracy of
coreference continues to be contentious. Various al-

16OntoNotes is deeply grateful to the Linguistic Data Con-
sortium for making the source data freely available to the task
participants.

12



Column Type Description

1 Document ID This is a variation on the document filename
2 Part number Some files are divided into multiple parts numbered as 000, 001, 002, ... etc.
3 Word number This is the word index in the sentence
4 Word The word itself
5 Part of Speech Part of Speech of the word
6 Parse bit This is the bracketed structure broken before the first open parenthesis in the parse, and the

word/part-of-speech leaf replaced with a *. The full parse can be created by substituting
the asterix with the ([pos] [word]) string (or leaf) and concatenating the items in the
rows of that column.

7 Predicate lemma The predicate lemma is mentioned for the rows for which we have semantic role informa-
tion. All other rows are marked with a -

8 Predicate Frameset ID This is the PropBank frameset ID of the predicate in Column 7.
9 Word sense This is the word sense of the word in Column 3.
10 Speaker/Author This is the speaker or author name where available. Mostly in Broadcast Conversation and

Web Log data.
11 Named Entities These columns identifies the spans representing various named entities.
12:N Predicate Arguments There is one column each of predicate argument structure information for the predicate

mentioned in Column 7.
N Coreference Coreference chain information encoded in a parenthesis structure.

Table 11: Format of the .conll file used on the shared task

#begin document (nw/wsj/07/wsj_0771); part 000
...
...
nw/wsj/07/wsj_0771 0 0 ‘‘ ‘‘ (TOP(S(S* - - - - * * (ARG1* * * -
nw/wsj/07/wsj_0771 0 1 Vandenberg NNP (NP* - - - - (PERSON) (ARG1* * * * (8|(0)
nw/wsj/07/wsj_0771 0 2 and CC * - - - - * * * * * -
nw/wsj/07/wsj_0771 0 3 Rayburn NNP *) - - - - (PERSON) *) * * *(23)|8)
nw/wsj/07/wsj_0771 0 4 are VBP (VP* be 01 1 - * (V*) * * * -
nw/wsj/07/wsj_0771 0 5 heroes NNS (NP(NP*) - - - - * (ARG2* * * * -
nw/wsj/07/wsj_0771 0 6 of IN (PP* - - - - * * * * * -
nw/wsj/07/wsj_0771 0 7 mine NN (NP*)))) - - 5 - * *) * * * (15)
nw/wsj/07/wsj_0771 0 8 , , * - - - - * * * * * -
nw/wsj/07/wsj_0771 0 9 ’’ ’’ *) - - - - * * *) * * -
nw/wsj/07/wsj_0771 0 10 Mr. NNP (NP* - - - - * * (ARG0* (ARG0* * (15
nw/wsj/07/wsj_0771 0 11 Boren NNP *) - - - - (PERSON) * *) *) * 15)
nw/wsj/07/wsj_0771 0 12 says VBZ (VP* say 01 1 - * * (V*) * * -
nw/wsj/07/wsj_0771 0 13 , , * - - - - * * * * * -
nw/wsj/07/wsj_0771 0 14 referring VBG (S(VP* refer 01 2 - * * (ARGM-ADV* (V*) * -
nw/wsj/07/wsj_0771 0 15 as RB (ADVP* - - - - * * * (ARGM-DIS* * -
nw/wsj/07/wsj_0771 0 16 well RB *) - - - - * * * *) * -
nw/wsj/07/wsj_0771 0 17 to IN (PP* - - - - * * * (ARG1* * -
nw/wsj/07/wsj_0771 0 18 Sam NNP (NP(NP* - - - - (PERSON* * * * * (23
nw/wsj/07/wsj_0771 0 19 Rayburn NNP *) - - - - *) * * * * -
nw/wsj/07/wsj_0771 0 20 , , * - - - - * * * * * -
nw/wsj/07/wsj_0771 0 21 the DT (NP(NP* - - - - * * * * (ARG0* -
nw/wsj/07/wsj_0771 0 22 Democratic JJ * - - - - (NORP) * * * * -
nw/wsj/07/wsj_0771 0 23 House NNP * - - - - (ORG) * * * * -
nw/wsj/07/wsj_0771 0 24 speaker NN *) - - - - * * * * *) -
nw/wsj/07/wsj_0771 0 25 who WP (SBAR(WHNP*) - - - - * * * * (R-ARG0*) -
nw/wsj/07/wsj_0771 0 26 cooperated VBD (S(VP* cooperate 01 1 - * * * * (V*) -
nw/wsj/07/wsj_0771 0 27 with IN (PP* - - - - * * * * (ARG1* -
nw/wsj/07/wsj_0771 0 28 President NNP (NP* - - - - * * * * * -
nw/wsj/07/wsj_0771 0 29 Eisenhower NNP *))))))))))) - - - - (PERSON) * *) *) *) 23)
nw/wsj/07/wsj_0771 0 30 . . *)) - - - - * * * * * -

nw/wsj/07/wsj_0771 0 0 ‘‘ ‘‘ (TOP(S* - - - - * * * -
nw/wsj/07/wsj_0771 0 1 They PRP (NP*) - - - - * (ARG0*) * (8)
nw/wsj/07/wsj_0771 0 2 allowed VBD (VP* allow 01 1 - * (V*) * -
nw/wsj/07/wsj_0771 0 3 this DT (S(NP* - - - - * (ARG1* (ARG1* (6
nw/wsj/07/wsj_0771 0 4 country NN *) - - 3 - * * *) 6)
nw/wsj/07/wsj_0771 0 5 to TO (VP* - - - - * * * -
nw/wsj/07/wsj_0771 0 6 be VB (VP* be 01 1 - * * (V*) (16)
nw/wsj/07/wsj_0771 0 7 credible JJ (ADJP*))))) - - - - * *) (ARG2*) -
nw/wsj/07/wsj_0771 0 8 . . *)) - - - - * * * -

#end document

Figure 3: Sample portion of the .conll file.

13



ternative metrics have been proposed, as mentioned
below, which weight different features of a proposed
coreference pattern differently. The choice is not
clear in part because the value of a particular set of
coreference predictions is integrally tied to the con-
suming application.

A further issue in defining a coreference metric
concerns the granularity of the mentions, and how
closely the predicted mentions are required to match
those in the gold standard for a coreference predic-
tion to be counted as correct.

Our evaluation criterion was in part driven by the
OntoNotes data structures. OntoNotes coreference
distinguishes between identity coreference and ap-
positive coreference, treating the latter separately
because it is already captured explicitly by other lay-
ers of the OntoNotes annotation. Thus we evaluated
systems only on the identity coreference task, which
links all categories of entities and events together
into equivalent classes.

The situation with mentions for OntoNotes is also
different than it was for MUC or ACE. OntoNotes
data does not explicitly identify the minimum ex-
tents of an entity mention, but it does include hand-
tagged syntactic parses. Thus for the official evalua-
tion, we decided to use the exact spans of mentions
for determining correctness. The NP boundaries
for the test data were pre-extracted from the hand-
tagged Treebank for annotation, and events trig-
gered by verb phrases were tagged using the verbs
themselves. This choice means that scores for the
CoNLL-2011 coreference task are likely to be lower
than for coref evaluations based on MUC, where the
mention spans are specified in the input,17 or those
based on ACE data, where an approximate match is
often allowed based on the specified head of the NP
mention.

4.5.1 Metrics
As noted above, the choice of an evaluation met-
ric for coreference has been a tricky issue and there
does not appear to be any silver bullet approach that
addresses all the concerns. Three metrics have been
proposed for evaluating coreference performance
over an unrestricted set of entity types: i) The link
based MUC metric (Vilain et al., 1995), ii) The men-
tion based B-CUBED metric (Bagga and Baldwin,
1998) and iii) The entity based CEAF (Constrained
Entity Aligned F-measure) metric (Luo, 2005). Very
recently BLANC (BiLateral Assessment of Noun-
Phrase Coreference) measure (Recasens and Hovy,

17as is the case in this evaluation with Gold Mentions

2011) has been proposed as well. Each of the met-
ric tries to address the shortcomings or biases of the
earlier metrics. Given a set of key entities K, and
a set of response entities R, with each entity com-
prising one or more mentions, each metric generates
its variation of a precision and recall measure. The
MUC measure if the oldest and most widely used. It
focuses on the links (or, pairs of mentions) in the
data.18 The number of common links between en-
tities in K and R divided by the number of links
in K represents the recall, whereas, precision is the
number of common links between entities in K and
R divided by the number of links in R. This met-
ric prefers systems that have more mentions per en-
tity – a system that creates a single entity of all
the mentions will get a 100% recall without signifi-
cant degradation in its precision. And, it ignores re-
call for singleton entities, or entities with only one
mention. The B-CUBED metric tries to addresses
MUCS’s shortcomings, by focusing on the mentions
and computes recall and precision scores for each
mention. If K is the key entity containing mention M,
and R is the response entity containing mention M,
then recall for the mention M is computed as |K∩R||K|
and precision for the same is is computed as |K∩R||R| .
Overall recall and precision are the average of the
individual mention scores. CEAF aligns every re-
sponse entity with at most one key entity by finding
the best one-to-one mapping between the entities us-
ing an entity similarity metric. This is a maximum
bipartite matching problem and can be solved by
the Kuhn-Munkres algorithm. This is thus a entity
based measure. Depending on the similarity, there
are two variations – entity based CEAF – CEAFe and
a mention based CEAF – CEAFe. Recall is the total
similarity divided by the number of mentions in K,
and precision is the total similarity divided by the
number of mentions in R. Finally, BLANC uses a
variation on the Rand index (Rand, 1971) suitable
for evaluating coreference. There are a few other
measures – one being the ACE value, but since this
is specific to a restricted set of entities (ACE types),
we did not consider it.

4.5.2 Official Evaluation Metric
In order to determine the best performing system
in the shared task, we needed to associate a single
number with each system. This could have been
one of the metrics above, or some combination of
more than one of them. The choice was not sim-
ple, and while we consulted various researchers in

18The MUC corpora did not tag single mention entities.

14



the field, hoping for a strong consensus, their con-
clusion seemed to be that each metric had its pros
and cons. We settled on the MELA metric by Denis
and Baldridge (2009), which takes a weighted av-
erage of three metrics: MUC, B-CUBED, and CEAF.
The rationale for the combination is that each of the
three metrics represents a different important dimen-
sion, the MUC measure being based on links, the
B-CUBED based on mentions, and the CEAF based
on entities. For a given task, a weighted average
of the three might be optimal, but since we don’t
have an end task in mind, we decided to use the un-
weighted mean of the three metrics as the score on
which the winning system was judged. We decided
to use CEAFe instead of CEAFm.

4.5.3 Scoring Metrics Implementation
We used the same core scorer implementation19 that
was used for the SEMEVAL-2010 task, and which
implemented all the different metrics. There were a
couple of modifications done to this scorer after it
was used for the SEMEVAL-2010 task.

1. Only exact matches were considered cor-
rect. Previously, for SEMEVAL-2010 non-exact
matches were judged partially correct with a
0.5 score if the heads were the same and the
mention extent did not exceed the gold men-
tion.

2. The modifications suggested by Cai and Strube
(2010) were incorporated in the scorer.

Since there are differences in the version used for
CoNLL and the one available on the download site,
and it is possible that the latter would be revised in
the future, we have archived the version of the scorer
on the CoNLL-2011 task webpage.20

5 Systems and Results

About 65 different groups demonstrated interest in
the shared task by registering on the task webpage.
Of these, 23 groups submitted system outputs on the
test set during the evaluation week. 18 groups sub-
mitted only closed track results, 3 groups only open
track results, and 2 groups submitted both closed and
open track results. 2 participants in the closed track,
did not write system papers, so we don’t use their re-
sults in the discussion. Their results will be reported
on the task webpage.

19http://www.lsi.upc.edu/ esapena/downloads/index.php?id=3
20http://conll.bbn.com/download/scorer.v4.tar.gz

The official results for the 18 systems that submit-
ted closed track outputs are shown in Table 12, with
those for the 5 systems that submitted open track
results in Table 13. The official ranking score, the
arithmetic mean of the F-scores of MUC, B-CUBED
and CEAFe, is shown in the rightmost column. For
convenience, systems will be referred to here using
the first portion of the full name, which is unique
within each table.

For completeness, the tables include the raw pre-
cision and recall scores from which the F-scores
were derived. The tables also include two additional
scores (BLANC and CEAFm) that did not factor into
the official ranking score. Useful further analysis
may be possible based on these results beyond the
preliminary results presented here.

As discussed previously in the task description,
we will consider three different test input conditions:
i) Predicted only (Official), ii) Predicted plus gold
mention boundaries, and iii) Predicted plus gold
mentions

5.1 Predicted only (Official)
For the official test, beyond the raw source text,
coreference systems were provided only with the
predictions from automatic engines as to the other
annotation layers (parses, semantic roles, word
senses, and named entities).

In this evaluation it is important to note that the
mention detection score cannot be considered in iso-
lation of the coreference task as has usually been the
case. This is mainly owing to the fact that there are
no singleton entities in the OntoNotes data. Most
systems removed singletons from the response as a
post-processing step, so not only will they not get
credit for the singleton entities that they correctly re-
moved from the data, but they will be penalized for
the ones that they accidentally linked with another
mention. What this number does indicate is the ceil-
ing on recall that a system would have got in absence
of being penalized for making mistakes in corefer-
ence resolution. A close look at the Table 12 indi-
cates a possible outlier in case of the sapena system.
The recall for this system is very high, and precision
way lower than any other system. Further investi-
gations uncovered that the reason for this aberrant
behavior was that fact that this system opted to keep
singletons in the response. By design, the scorer re-
moves singletons that might be still present in the
system, but it does so after the mention detection
accuracy is computed.

The official scores top out in the high 50’s. While
this is lower than the figures cited in previous coref-

15



Sy
st

em
M

en
tio

n
D

et
ec

tio
n

M
U

C
B

-C
U

B
E

D
C

E
A

F m
C

E
A

F e
B

L
A

N
C

O
ffi

ci
al

R
P

F
R

P
F1

R
P

F2
R

P
F

R
P

F3
R

P
F

F
1
+

F
2
+

F
3

3
le

e
75

.0
7

66
.8

1
70

.7
0

61
.7

6
57

.5
3

59
.5

7
68

.4
0

68
.2

3
68

.3
1

56
.3

7
56

.3
7

56
.3

7
43

.4
1

47
.7

5
45

.4
8

70
.6

3
76

.2
1

73
.0

2
57

.7
9

sa
pe

na
92

.3
9

28
.1

9
43

.2
0

56
.3

2
63

.1
6

59
.5

5
62

.7
5

72
.0

8
67

.0
9

53
.5

1
53

.5
1

53
.5

1
44

.7
5

38
.3

8
41

.3
2

69
.5

0
73

.0
7

71
.1

0
55

.9
9

ch
an

g
68

.0
8

61
.9

6
64

.8
8

57
.1

5
57

.1
5

57
.1

5
67

.1
4

70
.5

3
68

.7
9

54
.4

0
54

.4
0

54
.4

0
41

.9
4

41
.9

4
41

.9
4

71
.1

9
77

.0
9

73
.7

1
55

.9
6

nu
gu

es
69

.8
7

68
.0

8
68

.9
6

60
.2

0
57

.1
0

58
.6

1
66

.7
4

64
.2

3
65

.4
6

51
.4

5
51

.4
5

51
.4

5
38

.0
9

41
.0

6
39

.5
2

71
.9

9
70

.3
1

71
.1

1
54

.5
3

sa
nt

os
67

.8
0

63
.2

5
65

.4
5

59
.2

1
54

.3
0

56
.6

5
68

.7
9

62
.8

1
65

.6
6

49
.5

4
49

.5
4

49
.5

4
35

.8
6

40
.2

1
37

.9
1

73
.3

7
66

.9
1

69
.4

6
53

.4
1

so
ng

57
.8

1
80

.4
1

67
.2

6
53

.7
3

67
.7

9
59

.9
5

60
.6

5
66

.0
5

63
.2

3
46

.2
9

46
.2

9
46

.2
9

43
.3

7
30

.7
1

35
.9

6
69

.4
9

59
.7

1
61

.4
7

53
.0

5
st

oy
an

ov
70

.8
4

64
.9

8
67

.7
8

63
.6

1
54

.0
4

58
.4

3
72

.5
8

53
.2

7
61

.4
4

46
.0

8
46

.0
8

46
.0

8
32

.0
0

40
.8

2
35

.8
8

73
.2

1
58

.9
3

60
.8

8
51

.9
2

so
bh

a
67

.8
2

62
.0

9
64

.8
3

51
.0

8
49

.8
8

50
.4

8
62

.6
3

65
.4

3
64

.0
0

49
.4

8
49

.4
8

49
.4

8
40

.6
5

41
.8

2
41

.2
3

61
.4

0
68

.3
5

63
.8

8
51

.9
0

ko
bd

an
i

62
.0

6
60

.0
4

61
.0

3
55

.6
4

51
.5

0
53

.4
9

69
.6

6
62

.4
3

65
.8

5
42

.7
0

42
.7

0
42

.7
0

32
.3

3
35

.4
0

33
.7

9
61

.8
6

63
.5

1
62

.6
1

51
.0

4
zh

ou
61

.0
8

63
.5

9
62

.3
1

45
.6

5
52

.7
9

48
.9

6
57

.1
4

72
.9

1
64

.0
7

47
.5

3
47

.5
3

47
.5

3
43

.1
9

36
.7

9
39

.7
4

61
.1

0
73

.9
4

64
.7

2
50

.9
2

ch
ar

to
n

65
.9

0
62

.7
7

64
.3

0
55

.0
9

50
.0

5
52

.4
5

66
.2

6
58

.4
4

62
.1

0
46

.8
2

46
.8

2
46

.8
2

34
.3

3
39

.0
5

36
.5

4
69

.9
4

62
.2

3
64

.8
0

50
.3

6
ya

ng
71

.9
2

57
.5

3
63

.9
3

59
.9

1
46

.4
3

52
.3

1
71

.6
4

55
.1

4
62

.3
2

46
.5

5
46

.5
5

46
.5

5
30

.2
8

42
.3

9
35

.3
3

71
.1

1
61

.7
5

64
.6

3
49

.9
9

ha
o

64
.5

0
64

.1
1

64
.3

0
57

.8
9

51
.4

2
54

.4
7

67
.8

3
55

.4
3

61
.0

1
45

.0
7

45
.0

7
45

.0
7

30
.0

8
35

.7
6

32
.6

7
72

.6
1

62
.3

7
65

.3
5

49
.3

8
xi

nx
in

65
.4

9
58

.7
1

61
.9

2
48

.5
4

44
.8

5
46

.6
2

61
.5

9
62

.2
8

61
.9

3
44

.7
5

44
.7

5
44

.7
5

35
.1

9
38

.6
2

36
.8

3
63

.0
4

65
.8

3
64

.2
7

48
.4

6
zh

an
g

55
.3

5
68

.2
5

61
.1

3
42

.0
3

55
.6

2
47

.8
8

52
.5

7
73

.0
5

61
.1

4
44

.4
6

44
.4

6
44

.4
6

42
.0

0
30

.2
8

35
.1

9
62

.8
4

69
.2

2
65

.2
1

48
.0

7
ku

m
m

er
fe

ld
69

.7
7

56
.9

7
62

.7
2

46
.3

9
39

.5
6

42
.7

0
63

.6
0

57
.3

0
60

.2
9

45
.3

5
45

.3
5

45
.3

5
35

.0
5

42
.2

6
38

.3
2

58
.7

4
61

.5
8

59
.9

1
47

.1
0

zh
ek

ov
a

67
.4

9
37

.6
0

48
.2

9
28

.8
7

20
.6

6
24

.0
8

67
.1

4
56

.6
7

61
.4

6
40

.4
3

40
.4

3
40

.4
3

31
.5

7
41

.2
1

35
.7

5
52

.7
7

57
.0

5
53

.7
7

40
.4

3
ir

w
in

17
.0

6
61

.0
9

26
.6

7
12

.4
5

50
.6

0
19

.9
8

35
.0

7
89

.9
0

50
.4

6
31

.6
8

31
.6

8
31

.6
8

45
.8

4
17

.3
8

25
.2

1
51

.4
8

56
.8

3
51

.1
2

31
.8

8

Ta
bl

e
12

:P
er

fo
rm

an
ce

of
sy

st
em

s
in

th
e

of
fic

ia
l,

cl
os

ed
tr

ac
k

us
in

g
al

lp
re

di
ct

ed
in

fo
rm

at
io

n

Sy
st

em
M

en
tio

n
D

et
ec

tio
n

M
U

C
B

-C
U

B
E

D
C

E
A

F m
C

E
A

F e
B

L
A

N
C

O
ffi

ci
al

R
P

F
R

P
F1

R
P

F2
R

P
F

R
P

F3
R

P
F

F
1
+

F
2
+

F
3

3
le

e
74

.3
1

67
.8

7
70

.9
4

62
.8

3
59

.3
4

61
.0

3
68

.8
5

69
.0

1
68

.9
3

56
.7

0
56

.7
0

56
.7

0
43

.2
9

46
.8

0
44

.9
8

71
.9

0
76

.5
5

73
.9

6
58

.3
1

ca
i

67
.1

5
67

.6
4

67
.4

0
56

.7
3

58
.9

0
57

.8
0

64
.6

0
71

.0
3

67
.6

6
53

.3
7

53
.3

7
53

.3
7

42
.7

1
40

.6
8

41
.6

7
69

.7
7

73
.9

6
71

.6
2

55
.7

1
ur

yu
pi

na
70

.6
0

66
.3

1
68

.3
9

59
.7

0
55

.7
0

57
.6

3
66

.2
9

64
.1

2
65

.1
8

51
.4

2
51

.4
2

51
.4

2
38

.3
4

42
.1

7
40

.1
6

69
.2

3
68

.5
4

68
.8

8
54

.3
2

kl
en

ne
r

64
.4

1
60

.2
8

62
.2

8
49

.0
4

50
.7

1
49

.8
6

61
.7

0
68

.6
1

64
.9

7
50

.0
3

50
.0

3
50

.0
3

41
.2

8
39

.7
0

40
.4

8
66

.0
5

73
.9

0
69

.0
5

51
.7

7
ir

w
in

24
.6

0
62

.2
7

35
.2

7
18

.5
6

51
.0

1
27

.2
1

38
.9

7
85

.5
7

53
.5

5
33

.8
6

33
.8

6
33

.8
6

43
.3

3
19

.3
6

26
.7

6
51

.6
2

52
.9

1
51

.7
6

35
.8

4

Ta
bl

e
13

:P
er

fo
rm

an
ce

of
sy

st
em

s
in

th
e

of
fic

ia
l,

op
en

tr
ac

k
us

in
g

al
lp

re
di

ct
ed

in
fo

rm
at

io
n

Sy
st

em
M

en
tio

n
D

et
ec

tio
n

M
U

C
B

-C
U

B
E

D
C

E
A

F m
C

E
A

F e
B

L
A

N
C

O
ffi

ci
al

R
P

F
R

P
F1

R
P

F2
R

P
F

R
P

F3
R

P
F

F
1
+

F
2
+

F
3

3
le

e
79

.5
2

71
.2

5
75

.1
6

65
.8

7
62

.0
5

63
.9

0
69

.5
2

70
.5

5
70

.0
3

59
.2

6
59

.2
6

59
.2

6
46

.2
9

50
.4

8
48

.3
0

72
.0

0
78

.5
5

74
.7

7
60

.7
4

nu
gu

es
74

.1
8

70
.7

4
72

.4
2

64
.3

3
60

.0
5

62
.1

2
68

.2
6

65
.1

7
66

.6
8

53
.8

4
53

.8
4

53
.8

4
39

.8
6

44
.2

3
41

.9
3

72
.5

3
71

.0
4

71
.7

5
56

.9
1

ch
an

g
63

.3
7

73
.1

8
67

.9
2

55
.0

0
65

.5
0

59
.7

9
62

.1
6

76
.6

5
68

.6
5

54
.9

5
54

.9
5

54
.9

5
46

.7
7

37
.1

7
41

.4
2

70
.9

7
79

.3
0

74
.2

9
56

.6
2

sa
nt

os
65

.8
2

69
.9

0
67

.8
0

57
.7

6
61

.3
9

59
.5

2
64

.4
9

70
.2

7
67

.2
6

51
.8

7
51

.8
7

51
.8

7
41

.4
2

38
.1

6
39

.7
2

72
.7

2
71

.9
7

72
.3

4
55

.5
0

ko
bd

an
i

67
.1

1
65

.0
9

66
.0

8
62

.6
3

56
.8

0
59

.5
7

73
.2

0
62

.2
2

67
.2

7
44

.4
9

44
.4

9
44

.4
9

32
.8

7
37

.2
5

34
.9

2
64

.0
7

64
.1

3
64

.1
0

53
.9

2
st

oy
an

ov
76

.9
0

64
.7

3
70

.2
9

69
.8

1
55

.0
1

61
.5

4
77

.0
7

52
.5

4
62

.4
8

48
.0

8
48

.0
8

48
.0

8
30

.9
7

44
.8

4
36

.6
4

76
.5

7
60

.3
3

62
.9

6
53

.5
5

zh
an

g
59

.6
2

71
.1

9
64

.8
9

46
.0

6
58

.7
5

51
.6

4
53

.8
9

73
.4

1
62

.1
6

46
.6

2
46

.6
2

46
.6

2
43

.4
9

32
.1

1
36

.9
5

64
.1

1
70

.4
7

66
.5

4
50

.2
5

so
ng

58
.4

3
77

.6
4

66
.6

8
46

.6
6

68
.4

0
55

.4
8

54
.4

0
70

.1
9

61
.2

9
43

.6
2

43
.6

2
43

.6
2

43
.7

7
25

.8
8

32
.5

3
66

.2
9

58
.7

6
60

.2
2

49
.7

7
zh

ek
ov

a
69

.1
9

57
.2

7
62

.6
7

33
.4

8
37

.1
5

35
.2

2
55

.4
7

68
.2

3
61

.2
0

41
.3

1
41

.3
1

41
.3

1
38

.2
9

34
.6

5
36

.3
8

53
.4

5
63

.3
3

54
.7

9
44

.2
7

Ta
bl

e
14

:P
er

fo
rm

an
ce

of
sy

st
em

s
in

th
e

su
pp

le
m

en
ta

ry
cl

os
ed

tr
ac

k
us

in
g

pr
ed

ic
te

d
in

fo
rm

at
io

n
pl

us
go

ld
bo

un
da

ri
es

Sy
st

em
M

en
tio

n
D

et
ec

tio
n

M
U

C
B

-C
U

B
E

D
C

E
A

F m
C

E
A

F e
B

L
A

N
C

O
ffi

ci
al

R
P

F
R

P
F1

R
P

F2
R

P
F

R
P

F3
R

P
F

F
1
+

F
2
+

F
3

3
le

e
78

.7
1

72
.3

3
75

.3
9

66
.9

3
63

.9
1

65
.3

9
70

.0
9

71
.4

9
70

.7
8

59
.7

8
59

.7
8

59
.7

8
46

.3
4

49
.6

2
47

.9
2

73
.3

8
79

.0
0

75
.8

3
61

.3
6

Ta
bl

e
15

:P
er

fo
rm

an
ce

of
sy

st
em

s
in

th
e

su
pp

le
m

en
ta

ry
op

en
tr

ac
k

us
in

g
pr

ed
ic

te
d

in
fo

rm
at

io
n

pl
us

go
ld

bo
un

da
ri

es

16



Sy
st

em
M

en
tio

n
D

et
ec

tio
n

M
U

C
B

-C
U

B
E

D
C

E
A

F m
C

E
A

F e
B

L
A

N
C

O
ffi

ci
al

R
P

F
R

P
F1

R
P

F2
R

P
F

R
P

F3
R

P
F

F
1
+

F
2
+

F
3

3
ch

an
g

10
0

10
0

10
0

80
.4

6
84

.7
5

82
.5

5
72

.8
4

74
.5

7
73

.7
0

69
.7

1
69

.7
1

69
.7

1
70

.4
5

60
.7

5
65

.2
4

78
.0

1
76

.5
7

77
.2

6
73

.8
3

Ta
bl

e
16

:P
er

fo
rm

an
ce

of
sy

st
em

s
in

th
e

su
pp

le
m

en
ta

ry
,c

lo
se

d
tr

ac
k

us
in

g
pr

ed
ic

te
d

in
fo

rm
at

io
n

pl
us

go
ld

m
en

tio
ns

Sy
st

em
M

en
tio

n
D

et
ec

tio
n

M
U

C
B

-C
U

B
E

D
C

E
A

F m
C

E
A

F e
B

L
A

N
C

O
ffi

ci
al

R
P

F
R

P
F1

R
P

F2
R

P
F

R
P

F3
R

P
F

F
1
+

F
2
+

F
3

3
le

e
83

.3
7

10
0

90
.9

3
74

.7
9

89
.6

8
81

.5
6

67
.4

6
86

.8
8

75
.9

5
70

.7
3

70
.7

3
70

.7
3

77
.7

5
51

.0
5

61
.6

4
76

.6
5

85
.8

5
80

.3
5

73
.0

5

Ta
bl

e
17

:P
er

fo
rm

an
ce

of
sy

st
em

s
in

th
e

su
pp

le
m

en
ta

ry
,o

pe
n

tr
ac

k
us

in
g

pr
ed

ic
te

d
in

fo
rm

at
io

n
pl

us
go

ld
m

en
tio

ns

Sy
st

em
M

en
tio

n
D

et
ec

tio
n

M
U

C
B

-C
U

B
E

D
C

E
A

F m
C

E
A

F e
B

L
A

N
C

O
ffi

ci
al

R
P

F
R

P
F1

R
P

F2
R

P
F

R
P

F3
R

P
F

F
1
+

F
2
+

F
3

3
le

e
76

.7
9

68
.3

4
72

.3
2

63
.2

9
58

.9
6

61
.0

5
68

.8
4

68
.7

2
68

.7
8

57
.2

8
57

.2
8

57
.2

8
44

.1
9

48
.7

5
46

.3
6

70
.9

3
76

.5
8

73
.3

6
58

.7
3

sa
pe

na
95

.2
7

29
.0

7
44

.5
5

56
.9

9
63

.9
1

60
.2

5
62

.8
9

72
.3

1
67

.2
7

53
.9

0
53

.9
0

53
.9

0
45

.2
2

38
.7

0
41

.7
1

69
.7

1
73

.3
2

71
.3

2
56

.4
1

ch
an

g
69

.8
8

63
.6

1
66

.6
0

58
.4

8
58

.4
8

58
.4

8
67

.4
2

70
.9

1
69

.1
2

55
.2

1
55

.2
1

55
.2

1
42

.6
6

42
.6

6
42

.6
6

71
.4

2
77

.3
6

73
.9

6
56

.7
5

nu
gu

es
72

.9
6

71
.0

8
72

.0
1

62
.6

8
59

.4
6

61
.0

3
67

.2
4

64
.8

9
66

.0
4

52
.8

2
52

.8
2

52
.8

2
39

.2
5

42
.5

0
40

.8
1

72
.5

7
70

.8
6

71
.6

8
55

.9
6

sa
nt

os
70

.3
9

65
.6

7
67

.9
5

61
.2

8
56

.2
0

58
.6

3
69

.2
5

63
.1

6
66

.0
7

50
.4

7
50

.4
7

50
.4

7
36

.5
1

41
.1

5
38

.6
9

73
.9

2
67

.3
2

69
.9

3
54

.4
6

so
ng

59
.2

4
82

.3
9

68
.9

2
54

.9
2

69
.2

9
61

.2
7

60
.8

9
66

.2
7

63
.4

6
46

.9
7

46
.9

7
46

.9
7

44
.4

9
31

.1
5

36
.6

5
69

.7
3

59
.8

7
61

.6
1

53
.7

9
st

oy
an

ov
74

.4
3

68
.2

8
71

.2
2

67
.1

8
57

.0
8

61
.7

2
74

.0
6

53
.4

5
62

.0
9

47
.4

0
47

.4
0

47
.4

0
32

.7
8

42
.5

2
37

.0
2

74
.1

0
59

.3
4

61
.3

1
53

.6
1

so
bh

a
71

.0
6

65
.0

6
67

.9
3

53
.9

1
52

.6
4

53
.2

7
63

.1
7

66
.1

4
64

.6
2

50
.8

0
50

.8
0

50
.8

0
41

.7
7

43
.0

3
42

.3
9

61
.9

1
69

.1
5

64
.4

9
53

.4
3

ko
bd

an
i

65
.9

8
63

.8
3

64
.8

9
59

.2
2

54
.8

1
56

.9
3

70
.4

9
63

.1
2

66
.6

0
44

.1
7

44
.1

4
44

.1
5

33
.1

9
36

.5
0

34
.7

7
62

.5
2

64
.2

5
63

.3
2

52
.7

7
zh

ou
64

.1
1

66
.7

4
65

.4
0

48
.0

0
55

.5
1

51
.4

8
57

.1
8

73
.7

1
64

.4
0

48
.4

0
48

.4
0

48
.4

0
44

.1
8

37
.3

5
40

.4
8

61
.5

4
74

.8
6

65
.3

0
52

.1
2

ch
ar

to
n

71
.0

1
67

.6
4

69
.2

8
59

.2
4

53
.8

2
56

.4
0

67
.1

0
59

.0
2

62
.8

0
48

.9
1

48
.9

1
48

.9
1

35
.9

6
41

.3
9

38
.4

8
70

.6
5

62
.7

1
65

.3
4

52
.5

6
ya

ng
73

.7
3

58
.9

7
65

.5
3

61
.2

3
47

.4
5

53
.4

7
71

.8
8

55
.1

3
62

.4
0

47
.0

5
47

.0
5

47
.0

5
30

.5
4

43
.1

6
35

.7
7

71
.3

9
61

.9
2

64
.8

3
50

.5
5

ha
o

66
.7

9
66

.3
8

66
.5

9
59

.5
5

52
.8

9
56

.0
2

68
.2

7
55

.4
6

61
.2

0
45

.9
5

45
.9

5
45

.9
5

30
.7

6
36

.8
1

33
.5

1
73

.2
2

62
.7

3
65

.7
8

50
.2

4
xi

nx
in

69
.0

5
61

.9
1

65
.2

8
50

.9
9

47
.1

1
48

.9
7

61
.5

9
62

.7
0

62
.1

4
45

.6
4

45
.6

4
45

.6
4

35
.8

6
39

.5
7

37
.6

2
63

.4
2

66
.2

9
64

.6
8

49
.5

8
zh

an
g

57
.4

1
70

.7
8

63
.4

0
43

.4
8

57
.5

3
49

.5
3

52
.4

4
73

.6
0

61
.2

4
44

.9
7

44
.9

7
44

.9
7

42
.7

1
30

.4
4

35
.5

5
63

.1
2

69
.6

3
65

.5
3

48
.7

7
ku

m
m

er
fe

ld
71

.0
5

58
.0

1
63

.8
7

47
.4

2
40

.4
4

43
.6

5
63

.7
3

57
.3

9
60

.3
9

45
.7

6
45

.7
6

45
.7

6
35

.3
0

42
.7

2
38

.6
6

58
.8

9
61

.7
7

60
.0

7
47

.5
7

zh
ek

ov
a

72
.6

5
40

.4
8

51
.9

9
31

.7
3

22
.7

0
26

.4
6

66
.9

2
56

.6
8

61
.3

7
41

.0
4

41
.0

4
41

.0
4

31
.9

3
42

.1
7

36
.3

4
53

.0
9

57
.8

6
54

.2
2

41
.3

9
ir

w
in

17
.5

8
62

.9
6

27
.4

9
12

.6
9

51
.5

9
20

.3
7

34
.8

8
89

.9
8

50
.2

7
31

.7
1

31
.7

1
31

.7
1

46
.1

3
17

.3
3

25
.2

0
51

.5
1

56
.9

3
51

.1
4

31
.9

5

Ta
bl

e
18

:H
ea

d
w

or
d

ba
se

d
pe

rf
or

m
an

ce
of

sy
st

em
s

in
th

e
of

fic
ia

l,
cl

os
ed

tr
ac

k
us

in
g

al
lp

re
di

ct
ed

in
fo

rm
at

io
n

Sy
st

em
M

en
tio

n
D

et
ec

tio
n

M
U

C
B

-C
U

B
E

D
C

E
A

F m
C

E
A

F e
B

L
A

N
C

O
ffi

ci
al

R
P

F
R

P
F1

R
P

F2
R

P
F

R
P

F3
R

P
F

F
1
+

F
2
+

F
3

3
le

e
76

.0
1

69
.4

3
72

.5
7

64
.4

0
60

.8
3

62
.5

7
69

.3
4

69
.5

7
69

.4
5

57
.6

8
57

.6
8

57
.6

8
44

.1
5

47
.8

5
45

.9
2

72
.2

3
76

.9
4

74
.3

2
59

.3
1

ca
i

69
.3

2
69

.8
2

69
.5

7
58

.3
9

60
.6

3
59

.4
9

64
.8

8
71

.5
3

68
.0

4
54

.3
6

54
.3

6
54

.3
6

43
.7

4
41

.5
8

42
.6

4
70

.1
3

74
.3

9
72

.0
1

56
.7

2
ur

yu
pi

na
72

.1
0

67
.7

2
69

.8
4

60
.7

4
56

.6
8

58
.6

4
66

.4
3

64
.2

5
65

.3
2

52
.0

0
52

.0
0

52
.0

0
38

.8
7

42
.8

5
40

.7
6

69
.4

3
68

.7
3

69
.0

7
54

.9
1

kl
en

ne
r

71
.7

3
67

.1
4

69
.3

6
55

.1
7

57
.0

4
56

.0
9

62
.6

7
70

.6
9

66
.4

4
53

.2
5

53
.2

5
53

.2
5

44
.2

7
42

.3
9

43
.3

1
67

.4
5

75
.9

2
70

.6
8

55
.2

8
ir

w
in

25
.2

4
63

.8
7

36
.1

8
18

.9
0

51
.9

4
27

.7
1

38
.7

9
85

.6
4

53
.4

0
33

.8
9

33
.8

9
33

.8
9

43
.5

9
19

.3
1

26
.7

6
51

.6
6

52
.9

8
51

.8
0

35
.9

6

Ta
bl

e
19

:H
ea

d
w

or
d

ba
se

d
pe

rf
or

m
an

ce
of

sy
st

em
s

in
th

e
of

fic
ia

l,
op

en
tr

ac
k

us
in

g
al

lp
re

di
ct

ed
in

fo
rm

at
io

n

17



erence evaluations, that is as expected, given that the
task here includes predicting the underlying men-
tions and mention boundaries, the insistence on ex-
act match, and given that the relatively easier appos-
itive coreference cases are not included in this mea-
sure. The top-performing system (lee) had a score
of 57.79 which is about 1.8 points higher than that
of the second (sapena) and third (chang) ranking
systems, which scored 55.99 and 55.96 respectively.
Another 1.5 points separates them from the fourth
best score of 54.53 (nugues). Thus the performance
differences between the better-scoring systems were
not large, with only about three points separating the
top four systems.

This becomes even clearer if we merge in the re-
sults of systems that participated only in the open
track but that made relatively limited use of outside
resources.21 Comparing that way, the cai system
scores in the same ball park as the second rank sys-
tems (sapena and chang). The uryupina system sim-
ilarly scores very close to nugues’s 54.53

Given that our choice of the official metric was
somewhat arbitrary, if is also useful to look at
the individual metrics, including the mention-based
CEAFm and BLANC metrics that were not part of
the official metric. The lee system which scored
the best using the official metric does slightly worse
than song on the MUC metric, and also does slightly
worse than chang on the B-CUBED and BLANC met-
rics. However, it does much better than every other
group on the entity-based CEAFe, and this is the pri-
mary reason for its 1.8 point advantage in the offi-
cial score. If the CEAFe measure does indicate the
accuracy of entities in the response, this suggests
that the lee system is doing better on getting coher-
ent entities than any other system. This could be
partly due to the fact that that system is primarily
a precision-based system that would tend to create
purer entities. The CEAFe measure also seems to pe-
nalize other systems more harshly than do the other
measures.

We cannot compare these results to the ones ob-
tained in the SEMEVAL-2010 coreference task using
a small portion of OntoNotes data because it was
only using nominal entities, and had heuristically
added singleton mentions to the OntoNotes data22

21The cai system specifically mentions that, and the only re-
source that the uryupina system used outside of the closed track
setting was the Stanford named entity tagger.

22The documentation that comes with the SEMEVAL data
package from LDC (LDC2011T01) states: “Only nominal
mentions and identical (IDENT) types were taken from the
OntoNotes coreference annotation, thus excluding coreference

5.2 Predicted plus gold mention boundaries

We also explored performance when the systems
were provided with the gold mention boundaries,
that is, with the exact spans (expressed in terms of
token offsets) for all of the NP constituents in the
human-annotated parse trees for the test data. Sys-
tems could use this additional data to ensure that the
output mention spans in their entity chains would not
clash with those in the answer set. Since this was
a secondary evaluation, it was an optional element,
and not all participants ran their systems on this task
variation. The results for those systems that did par-
ticipate in this optional task are shown in Tables 14
(closed track) and 15 (open track).

Most of the better scoring systems did supply
these results. While all systems did slightly better
here in terms of raw scores, the performance was
not much different from the official task, indicating
that mention boundary errors resulting from prob-
lems in parsing do not contribute significantly to the
final output.23

One side benefit of performing this supplemental
evaluation was that it revealed a subtle bug in the
automatic scoring routine that we were using that
could double-count duplicate correct mentions in a
given entity chain. These can occur, for example, if
the system considers a unit-production NP-PRP com-
bination as two mentions that identify the exact same
token in the text, and reports them as separate men-
tions. Most systems had a filter in their processing
that selected only one of these duplicate mentions,
but the kobdani system considered both as potential
mentions, and its developers tuned their algorithm
using that flawed version of the scorer.

When we fixed the scorer and re-evaluated all of
the systems, the kobdani system was the only one
whose score was affected significantly, dropping by
about 8 points, which lowered that system’s rank
from second to ninth. It is not clear how much of
this was owing to the fact that the system’s param-

relations with verbs and appositives. Since OntoNotes is only
annotated with multi-mention entities, singleton referential ele-
ments were identified heuristically: all NPs and possessive de-
terminers were annotated as singletons excluding those func-
tioning as appositives or as pre-modifiers but for NPs in the
possessive case. In coordinated NPs, single constituents as well
as the entire NPs were considered to be mentions. There is no
reliable heuristic to automatically detect English expletive pro-
nouns, thus they were (although inaccurately) also annotated as
singletons.”

23It would be interesting to measure the overlap between the
entity clusters for these two cases, to see whether there was
any substantial difference in the mention chains, besides the ex-
pected differences in boundaries for individual mentions.

18



eters had been tuned using the scorer with the bug,
which double-credited duplicate mentions. To find
out for sure, one would have to re-tune the system
using the modified scorer.

One difficulty with this supplementary evaluation
using gold mention boundaries is that those bound-
aries alone provide only very partial information.
For the roughly 10% of mentions that the automatic
parser did not correctly identify, while the systems
knew the correct boundaries, they had no hierarchi-
cal parser or semantic role label information, and
they also had to further approximate the already
heuristic head word identification. This incomplete
data complicated the systems’ task and also compli-
cates interpretation of the results.

5.3 Predicted plus gold mentions
The final supplementary condition that we explored
was if the systems were supplied with the manually-
annotated spans for exactly those mentions that did
participate in the gold standard coreference chains.
This supplies significantly more information than
the previous case, where exact spans were supplied
for all NPs, since the gold mentions list here will also
include verb headwords that are linked to event NPs,
but will not include singleton mentions, which do
not end up as part of any chain. The latter constraint
makes this test seem somewhat artificial, since it di-
rectly reveals part of what the systems are designed
to determine, but it still has some value in quanti-
fying the impact that mention detection has on the
overall task and what the results are if the mention
detection is perfect.

Since this was a logical extension of the task and
since the data was available to the participants for
the development set, a few of the sites did run ex-
periments of this type. Therefore we decided to pro-
vide the gold mentions data to a few sites who had
reported these scores, so that we could compute the
performance on the test set. The results of these ex-
periments are shown in Tables 16 and 17. The results
show that performance does go up significantly, in-
dicating that it is markedly easier for the systems
to generate better entities given gold mentions. Al-
though, ideally, one would expect a perfect mention
detection score, it is the case that one of the two sys-
tems – lee – did not get a 100% Recall. This could
possibly be owing to unlinked singletons that were
removed in post-processing.

The lee system developers also ran a further ex-
periment where both gold mentions for the elements
of the coreference chains and also gold annota-
tions for all the other layers were available to the

system. Surprisingly, the improvement in corefer-
ence performance from having gold annotation of
the other layers was almost negligible. This sug-
gests that either: i) the automatic models are pre-
dicting those layers well enough that switching to
gold doesn’t make much difference; ii) information
from the other layers does not provide much lever-
age for coreference resolution; or iii) current coref-
erence models are not capable of utilizing the infor-
mation from these other layers effectively. Given
the performance numbers on the individual layers
cited earlier, (i) seems unlikely, and we hope that
further research in how best to leverage these lay-
ers will result in models that can benefit from them
more definitively.

5.4 Head word based scoring
In order to check how stringent the official, exact
match scoring is, we also performed a relaxed scor-
ing. Unlike ACE and MUC, the OntoNotes data does
not have manually annotated minimum spans that
a mention must contain to be considered correct.
However, OntoNotes does have manual syntactic
analysis in the form of the Treebank. Therefore, we
decided to approximate the minimum spans by using
the head words of the mentions using the gold stan-
dard syntax tree. If the response mention contained
the head word and did not exceed the true mention
boundary, then it was considered correct – both from
the point of view of mention detection, and corefer-
ence resolution. The scores using this relaxed strat-
egy for the open and closed track submissions using
predicted data are shown in Tables 18 and 19. It
can be observed that the relaxed, head word based,
scoring does not improve performance very much.
The only exception was the klenner system whose
performance increased from 51.77 to 55.28. Over-
all, the ranking remained quite stable, though it did
change for some adjacent systems which had very
close exact match scores.

5.5 Genre variation
In order to check how the systems did on various
genres, we scored their performance per genre as
well. Tables 20 and 21 summarize genre based per-
formance for the closed and open track participants
respectively. System performance does not seem
to vary as much across the different genres as is
normally the case with language processing tasks,
which could suggest that coreference is relatively
genre insensitive, or it is possible that scores are
two low for the difference to be apparent. Compar-
isons are difficult, however, because the spoken gen-

19



MD MUC BCUB Cm Ce BLANC O MD MUC BCUB Cm Ce BLANC O
F F F F F F F F F F F F F F

lee GENRE zhou GENRE
BC 72.2 60.0 66.2 53.9 43.7 71.7 56.7 BC 64.1 49.5 62.1 45.3 38.8 61.8 50.1
BN 72.0 59.0 68.7 57.6 48.7 68.8 58.8 BN 60.8 45.9 64.4 49.5 41.2 66.8 50.5
MZ 70.1 58.0 72.2 61.6 50.9 75.0 60.4 MZ 58.8 44.4 66.9 50.1 41.8 64.6 51.0
NW 65.4 54.3 69.4 56.5 45.5 70.4 56.4 NW 57.7 44.8 65.7 48.7 40.3 63.1 50.2
TC 75.9 66.8 69.5 59.3 41.3 81.6 59.2 TC 69.2 58.1 60.8 43.1 35.7 62.6 51.5

WB 73.0 63.9 65.7 54.2 42.7 73.4 57.5 WB 67.4 55.4 62.8 47.9 39.2 69.1 52.5
sapena charton

BC 48.7 58.8 64.6 50.8 39.4 70.4 54.3 BC 65.8 53.1 59.1 44.6 35.2 64.4 49.1
BN 47.1 60.0 69.1 57.4 45.0 74.3 58.0 BN 65.5 52.0 64.0 50.0 39.6 65.9 51.9
MZ 35.3 59.2 72.3 60.4 48.2 75.0 59.9 MZ 61.7 46.3 64.6 49.7 39.9 64.1 50.3
NW 35.2 57.9 69.7 55.3 41.9 73.8 56.5 NW 57.6 44.6 64.5 48.2 37.7 67.0 48.9
TC 60.4 64.3 63.3 48.3 35.1 68.8 54.2 TC 73.1 66.8 56.2 42.8 29.9 58.1 51.0

WB 46.3 60.1 62.5 49.1 37.4 67.4 53.3 WB 67.6 57.6 59.3 45.1 33.3 66.6 50.0
chang yang

BC 65.5 56.4 67.1 51.5 39.8 71.6 54.4 BC 65.7 53.8 62.3 46.8 35.0 67.5 50.3
BN 66.6 57.4 69.1 56.0 45.6 70.5 57.4 BN 66.0 53.1 63.8 49.1 40.0 63.1 52.3
MZ 61.6 52.7 71.3 57.6 46.4 72.9 56.8 MZ 58.8 43.9 59.7 42.6 32.8 55.5 45.5
NW 61.0 53.3 69.1 54.1 42.1 71.9 54.8 NW 57.2 44.7 62.9 45.3 35.0 62.7 47.6
TC 72.2 68.5 71.4 59.6 37.7 81.7 59.2 TC 74.2 66.8 66.3 55.3 36.0 76.1 56.4

WB 66.4 59.7 66.7 52.7 39.4 74.7 55.3 WB 67.6 57.6 57.0 42.6 32.1 60.1 48.9
nugues hao

BC 71.4 59.2 62.4 48.2 37.2 68.4 52.9 BC 68.9 58.7 58.9 44.8 31.7 64.9 49.8
BN 70.0 58.5 67.4 54.5 43.1 73.1 56.3 BN 62.0 51.1 63.0 46.2 35.5 64.1 49.9
MZ 65.4 53.6 68.6 54.2 42.2 70.1 54.8 MZ 60.3 46.7 61.5 46.3 34.3 61.9 47.5
NW 61.8 51.9 67.0 51.3 39.2 69.4 52.7 NW 57.2 47.7 63.3 45.5 32.9 66.0 48.0
TC 77.2 69.2 63.9 53.0 37.9 72.2 57.0 TC 67.9 60.4 58.8 44.7 30.3 68.3 49.8

WB 72.9 64.2 63.4 51.1 38.5 74.3 55.4 WB 71.4 61.8 55.7 42.6 30.0 64.4 49.2
santos xinxin

BC 66.6 57.2 64.8 48.5 37.2 68.6 53.0 BC 64.8 47.8 60.2 43.9 35.5 65.1 47.9
BN 66.9 57.3 66.9 52.3 41.0 71.8 55.1 BN 61.5 44.7 63.2 47.0 38.9 65.8 48.9
MZ 62.7 51.0 65.9 48.9 37.8 64.5 51.6 MZ 54.6 35.5 64.5 45.7 37.7 61.0 45.9
NW 58.4 49.5 66.2 48.1 37.4 66.9 51.0 NW 54.3 39.5 64.0 45.0 37.5 61.1 47.0
TC 74.2 66.9 65.9 52.5 35.5 72.5 56.1 TC 74.2 62.0 57.9 45.4 33.4 66.5 51.1

WB 70.4 63.2 63.4 49.5 38.2 70.3 55.0 WB 66.9 52.6 58.5 42.2 35.9 63.4 49.0
song zhang

BC 68.9 61.4 61.0 44.1 34.3 59.5 52.2 BC 65.8 50.6 61.1 45.3 35.5 67.3 49.1
BN 66.2 58.4 64.8 49.0 38.2 65.2 53.8 BN 56.3 43.9 61.0 45.8 35.8 66.8 46.9
MZ 63.7 53.4 65.5 49.9 39.0 63.4 52.6 MZ 57.1 35.1 62.2 44.4 36.1 59.4 44.5
NW 62.4 53.6 64.3 48.0 37.2 62.7 51.7 NW 49.9 37.8 61.8 43.2 35.2 59.8 44.9
TC 76.9 74.4 62.0 43.3 33.2 58.1 56.5 TC 75.4 65.9 60.2 46.0 32.1 67.1 52.7

WB 70.0 63.0 60.1 43.3 31.8 60.8 51.6 WB 69.2 55.4 57.4 42.5 34.6 64.7 49.1
stoyanov kummerfield

BC 69.5 59.1 57.6 43.5 34.0 58.7 50.2 BC 66.4 41.5 55.6 41.7 36.2 57.9 44.4
BN 69.2 59.1 65.4 50.4 40.0 65.5 54.8 BN 68.3 48.2 63.4 51.7 44.7 61.6 52.1
MZ 66.7 55.1 65.5 51.0 39.9 63.7 53.5 MZ 58.0 39.9 65.8 51.0 43.4 64.1 49.7
NW 61.8 52.0 63.3 46.2 36.1 62.0 50.5 NW 55.2 41.3 64.7 46.8 37.0 63.5 47.6
TC 72.6 66.6 57.6 42.3 31.0 57.6 51.7 TC 61.8 34.5 51.5 34.7 30.0 54.1 38.7

WB 71.5 63.9 58.3 44.8 33.1 61.1 51.8 WB 68.2 48.1 56.0 44.4 38.6 59.6 47.6
sobha zhekova

BC 68.3 51.7 61.4 47.8 40.4 62.9 51.2 BC 50.5 23.8 60.6 39.4 35.1 53.4 39.8
BN 66.5 51.9 66.5 53.7 45.5 66.3 54.6 BN 51.2 26.0 62.4 42.5 37.5 54.3 42.0
MZ 68.8 54.9 70.3 58.9 49.3 69.8 58.1 MZ 44.0 22.6 63.4 43.3 37.3 56.0 41.1
NW 55.1 43.1 65.8 48.6 39.0 64.9 49.3 NW 39.7 19.4 62.8 41.0 35.8 53.7 39.3
TC 71.5 55.1 57.5 44.2 36.7 60.5 49.7 TC 59.4 31.6 58.2 37.7 33.6 54.1 41.1

WB 70.5 55.7 59.2 46.6 39.8 62.6 51.6 WB 54.1 27.8 58.7 38.5 34.7 53.0 40.4
kobdani irwin

BC 63.2 56.3 65.8 40.6 32.4 61.9 51.5 BC 23.5 16.1 46.0 29.4 23.6 49.8 28.6
BN 63.5 55.7 68.5 46.9 37.5 64.6 53.9 BN 24.9 20.0 49.7 34.2 27.1 52.9 32.3
MZ 57.5 52.2 69.8 45.7 36.4 61.7 52.8 MZ 23.2 17.9 55.9 36.2 28.5 53.0 34.1
NW 52.2 41.7 64.4 43.2 33.7 62.6 46.6 NW 27.5 21.6 56.4 33.9 27.3 52.6 35.1
TC 67.7 60.2 65.3 36.6 28.5 57.6 51.3 TC 28.0 19.3 38.2 24.5 18.7 49.0 25.4

WB 68.7 62.8 62.4 42.5 32.9 64.0 52.7 WB 33.6 24.8 47.6 29.7 23.0 50.2 31.8

Table 20: Detailed look at the performance per genre for the official, closed track using automatic performance. MD
represents MENTION DETECTION; BCUB represents B-CUBED; Cm represents CEAFm; Ce represents CEAFe and O
represents the OFFICIAL score.

20



res were treated here with perfect speech recognition
accuracy and perfect speaker turn information. Un-
der more realistic application conditions, the spread
in performance between genres might be greater.

MD MUC BCUB Cm Ce BLANC O
F F F F F F F

lee GENRE
BC 72.7 61.7 67.0 54.5 43.6 72.7 57.4
BN 72.0 60.6 69.4 57.9 48.1 70.3 59.3
MZ 69.9 58.4 72.1 61.2 50.1 75.2 60.2
NW 65.3 55.8 70.0 56.7 44.9 71.7 56.9
TC 76.6 68.4 70.4 59.6 40.8 82.1 59.9

WB 73.8 65.5 66.2 54.5 42.1 74.2 57.9
cai

BC 69.7 59.1 66.0 50.5 39.9 69.2 55.0
BN 68.6 57.6 67.8 55.4 45.5 68.2 56.9
MZ 64.0 51.1 69.5 55.9 45.6 71.2 55.4
NW 60.3 49.9 67.8 52.7 41.2 69.1 53.0
TC 75.6 70.5 72.2 59.6 38.0 80.3 60.2

WB 71.7 63.9 65.0 51.8 39.8 72.8 56.2
uryupina

BC 70.2 58.3 62.7 48.7 38.0 68.7 53.0
BN 69.0 57.6 66.8 53.6 43.1 69.2 55.8
MZ 65.7 52.4 68.3 54.3 43.6 68.8 54.8
NW 62.6 52.1 68.3 53.2 41.2 71.3 53.9
TC 75.7 67.1 61.0 50.7 34.6 67.1 54.2

WB 72.0 61.7 60.9 48.8 38.3 67.6 53.6
klenner

BC 63.2 50.3 63.4 48.2 38.9 66.8 50.8
BN 63.1 48.6 65.0 51.0 42.6 66.0 52.1
MZ 59.1 43.7 67.1 52.9 45.3 65.0 52.0
NW 55.3 41.3 65.0 48.0 39.6 64.5 48.7
TC 73.9 64.9 67.9 56.4 39.0 78.0 57.3

WB 66.8 58.1 64.0 50.1 39.6 72.7 53.9
irwin

BC 36.6 27.6 50.9 32.0 25.5 50.2 34.7
BN 30.8 24.6 51.9 36.4 28.6 54.8 35.0
MZ 26.1 20.0 57.3 37.6 29.4 54.3 35.6
NW 32.3 24.7 58.4 34.7 27.9 51.1 37.0
TC 46.4 34.3 44.6 29.4 21.9 51.7 33.6

WB 41.7 32.9 50.5 32.9 25.1 53.2 36.2

Table 21: Detailed look at the performance per genre for
the official, open track using predicted information. MD
represents MENTION DETECTION; BCUB represents B-
CUBED; Cm represents CEAFm; Ce represents CEAFe and
O represents the OFFICIAL score.

6 Approaches

Tables 22 and 23 summarize the approaches of the
participating systems along with some of the impor-
tant dimensions.

Most of the systems broke the problem into two
phases, first identifying the potential mentions in the
text and then linking the mentions to form corefer-
ence chains. Most participants also used rule-based
approaches for mention detection, though two did
use trained models. While trained morels seem able
to better balance precision and recall, and thus to
achieve a higher F-score on the mention task itself,
their recall tends to be quite a bit lower than that

achievable by rule-based systems designed to fa-
vor recall. This impacts coreference scores because
the full coreference system has no way to recover
if the mention detection stage misses a potentially
anaphoric mention.

Only one of the participating systems cai at-
tempted to do joint mention detection and corefer-
ence resolution. While it did not happen to be among
the top-performing systems, the difference in perfor-
mance could be due to the richer features used by
other systems rather than to the use of a joint model.

Most systems represented the markable mentions
internally in terms of the parse tree NP constituent
span, but some systems used shared attribute mod-
els, where the attributes of the merged entity are
determined collectively by heuristically merging the
attribute types and values of the different constituent
mentions.

Various types of trained models were used for pre-
dicting coreference. It is interesting to note that
some of the systems, including the best-performing
one, used a completely rule-based approach even for
this component.

Most participants appear not to have focused
much on eventive coreference, those coreference
chains that build off verbs in the data. This usu-
ally meant that mentions that should have linked to
the eventive verb were instead linked in with some
other entity. Participants may have chosen not to fo-
cus on events because they pose unique challenges
while making up only a small portion of the data.
Roughly 91% of mentions in the data are NPs and
pronouns.

In the systems that used trained models, many
systems used the approach described in Soon et al.
(2001) for selecting the positive and negative train-
ing examples, while others used some of the al-
ternative approaches that have been introduced in
the research literature more recently. Many of the
trained systems also were able to improve their per-
formance by using feature selection, though things
varied some depending on the example selection
strategy and the classifier used. Almost half of the
trained systems used the feature selection strategy
from Soon et al. (2001) and found it beneficial. It is
not clear whether the other systems did not explore
this path, or whether it just did not prove as useful in
their case.

7 Conclusions

In this paper we described the anaphoric coreference
information and other layers of annotation in the

21



Ta
sk

Sy
nt

ax
L

ea
rn

in
g

Fr
am

ew
or

k
M

ar
ka

bl
e

Id
en

tifi
ca

tio
n

M
ar

ka
bl

e
V

er
b

Fe
at

ur
e

Se
le

ct
io

n
#

Fe
at

ur
es

Tr
ai

ni
ng

le
e

C
+O

P
R

ul
e-

ba
se

d
R

ul
es

to
ex

cl
ud

e
C

op
ul

ar
co

ns
tr

uc
tio

n,
A

pp
os

iti
ve

s,
Pl

eo
na

st
ic

it,
et

c.
Fe

at
ur

e
de

pe
nd

en
t

w
ith

sh
ar

ed
at

tr
ib

ut
es

×
×

—

sa
pe

na
C

P
D

ec
is

io
n

Tr
ee

+
R

el
ax

at
io

n
L

ab
el

in
g

N
P

(m
ax

im
al

sp
an

)+
P

R
P

+
N

E
+

C
ap

ita
liz

ed
no

un
he

ur
is

tic
Fu

ll
ph

ra
se

×
×

Tr
ai

n
+

D
ev

ch
an

g
C

P
L

ea
rn

in
g

B
as

ed
Ja

va
N

P,
N

E
,P

R
P,

P
R

P$
Fu

ll
ph

ra
se

×
×

Tr
ai

n
+

D
ev

ca
i

O
P

C
om

pu
te

hy
pe

re
dg

e
w

ei
gh

ts
on

30
%

of
tr

ai
ni

ng
da

ta
N

P,
P

R
P,

P
R

P$
,B

as
e

ph
ra

se
ch

un
ks

,
Pl

eo
na

st
ic

it
fil

te
r

Fu
ll

ph
ra

se
×

×
—

nu
gu

es
C

D
L

og
is

tic
R

eg
re

ss
io

n
(L

IB
L

IN
E

A
R

)
N

P,
P

R
P$

an
d

se
qu

en
ce

of
N

N
P(

s)
in

po
st

pr
oc

es
si

ng
us

in
g

A
L

IA
S

an
d

S
T

R
IN

G
M

A
T

C
H

H
ea

d
w

or
d

×
Fo

rw
ar

d
+

B
ac

kw
ar

d
st

ar
tin

g
fr

om
So

on
fe

at
ur

e
se

t
24

Tr
ai

n
+

D
ev

ur
yu

pi
na

O
P

D
ec

is
io

n
Tr

ee
.D

iff
er

en
t

cl
as

si
fie

rs
fo

rP
ro

no
m

in
al

an
d

no
n-

Pr
on

om
in

al
m

en
tio

ns

N
P,

N
E

,P
R

P,
P

R
P$

,a
nd

ru
le

s
to

ex
cl

ud
e

so
m

e
sp

ec
ifi

c
ca

se
s

Fu
ll

ph
ra

se
×

M
ul

ti-
O

bj
ec

tiv
e

O
pt

im
iz

at
io

n
on

th
re

e
sp

lit
s.

N
S

G
A

-I
I

46
Tr

ai
n

+
D

ev

sa
nt

os
C

P

E
T

L
(E

nt
ro

py
gu

id
ed

Tr
an

sf
or

m
at

io
na

lL
ea

rn
in

g)
co

m
m

itt
ee

an
d

R
an

do
m

Fo
re

st
(W

E
K

A
)

A
ll

N
P

an
d

al
lp

ro
no

un
s

an
d

P
E

R
,O

R
G

,G
P

E
in

N
P

Fu
ll

ph
ra

se
×

In
he

re
nt

to
th

e
cl

as
si

fie
rs

Tr
ai

n
+

D
ev

so
ng

C
P

M
ax

E
nt

(O
pe

nN
L

P)
M

en
tio

n
de

te
ct

io
n

cl
as

si
fie

r
Fu

ll
ph

ra
se

×
Sa

m
e

fe
at

ur
e

se
t,

bu
t

pe
rc

la
ss

ifi
er

40
Tr

ai
n

st
oy

an
ov

C
P

A
ve

ra
ge

d
pe

rc
ep

tr
on

N
E

an
d

po
ss

es
si

ve
s

in
ad

di
tio

n
to

A
C

E
ba

se
d

sy
st

em
Fu

ll
ph

ra
se

×
×

76
—

so
bh

a
C

P
C

R
F

fo
rn

on
-p

ro
no

m
in

al
an

d
sa

lie
nc

e
fa

ct
or

fo
rp

ro
no

m
in

al
re

so
lu

tio
n

M
ac

hi
ne

le
ar

ne
d

pl
eo

na
st

ic
it,

pl
us

N
P,

P
R

P,
P

R
P$

an
d

N
E

M
in

im
al

(C
hu

nk
/N

E
)

an
d

M
ax

im
um

sp
an

×
×

Tr
ai

n

kl
en

ne
r

O
D

R
ul

e-
ba

se
d.

Sa
lie

nc
e

m
ea

su
re

us
in

g
de

pe
nd

en
ci

es
ge

ne
ra

te
d

fr
om

tr
ai

ni
ng

da
ta

N
P,

N
E

,P
R

P,
P

R
P$

Sh
ar

ed
at

tr
ib

ut
ed

/tr
an

si
tiv

ity
by

us
in

g
a

vi
rt

ua
l

pr
ot

ot
yp

e

×
×

—

ko
bd

an
i

C
P

D
ec

is
io

n
Tr

ee
N

P
(n

o
m

en
tio

n
of

P
R

P$
)

St
ar

tw
or

d,
E

nd
w

or
d

an
d

H
ea

d
of

N
P

×
In

fo
rm

at
io

n
ga

in
ra

tio
Tr

ai
n

zh
ou

C
P

S
V

M
tr

ee
ke

rn
el

us
in

g
B

C
po

rt
io

n
of

th
e

da
ta

R
ul

e-
ba

se
d;

Fi
ve

ru
le

s:
P

R
P$

,P
R

P,
N

E
,

sm
al

le
st

N
P

su
bs

um
in

g
N

E
an

d
D

E
T

+N
P

Fu
ll

ph
ra

se
×

×
17

Tr
ai

n
+

D
ev

ch
ar

to
n

C
P

M
ul

ti-
la

ye
rp

er
ce

pt
ro

n
R

ul
es

ba
se

d
on

P
O

S,
N

E
an

d
fil

te
ro

ut
pl

eo
na

st
ic

it
us

in
g

ru
le

-b
as

ed
fil

te
r

Fu
ll

ph
ra

se
×

×
22

Tr
ai

n

ya
ng

C
P

M
ax

E
nt

(M
A

L
L

E
T

)
N

P,
P

R
P,

P
R

P$
,p

re
-m

od
ifi

er
s

an
d

ve
rb

s
Fu

ll
ph

ra
se

√
×

40
Tr

ai
n

+
D

ev
ha

o
C

P
M

ax
E

nt
N

P,
P

R
P,

P
R

P$
,V

B
D

fu
ll

ph
ra

se
√

×
Tr

ai
n

+
D

ev
xi

nx
in

C
P

IL
P/

In
fo

rm
at

io
n

ga
in

N
P,

P
R

P,
P

R
P$

Fu
ll

ph
ra

se
×

In
fo

rm
at

io
n

ga
in

ra
tio

65
—

zh
an

g
C

P
S

V
M

IO
B

cl
as

si
fic

at
io

n
Fu

ll
ph

ra
se

×
×

—
ku

m
m

er
fie

ld
C

P
U

ns
up

er
vi

se
d

ge
ne

ra
tiv

e
m

od
el

N
P,

P
R

P,
P

R
P$

w
ith

m
ax

im
al

sp
an

Fu
ll

ph
ra

se
×

×
—

zh
ek

ov
a

C
P

T
IM

B
L

m
em

or
y

ba
se

d
le

ar
ne

r
N

P,
Pr

op
er

no
un

s,
P

R
P,

P
R

P$
,p

lu
s

ve
rb

w
ith

pr
ed

ic
at

e
le

m
m

a
H

ea
d

w
or

d
√

×
Tr

ai
n

+
D

ev

ir
w

in
C

+O
P

C
la

ss
ifi

ca
tio

n-
ba

se
d

ra
nk

er
N

P,
P

R
P,

P
R

P$
Sh

ar
ed

at
tr

ib
ut

es
×

×
—

Ta
bl

e
22

:
Pa

rt
ic

ip
at

in
g

sy
st

em
pr

ofi
le

s
–

Pa
rt

I.
In

th
e

Ta
sk

co
lu

m
n,

C
/O

re
pr

es
en

ts
w

he
th

er
th

e
sy

st
em

pa
rt

ic
ip

at
ed

in
th

e
cl

os
ed

,
op

en
or

bo
th

tr
ac

ks
.

In
th

e
Sy

nt
ax

co
lu

m
n,

a
P

re
pr

es
en

ts
th

at
th

e
sy

st
em

s
us

ed
a

ph
ra

se
st

ru
ct

ur
e

gr
am

m
ar

re
pr

es
en

ta
tio

n
of

sy
nt

ax
,w

he
re

as
a

D
re

pr
es

en
ts

th
at

th
ey

us
ed

a
de

pe
nd

en
cy

re
pr

es
en

ta
tio

n.

22



Po
si

tiv
e

Tr
ai

ni
ng

E
xa

m
pl

es
N

eg
at

iv
e

Tr
ai

ni
ng

E
xa

m
pl

es
D

ec
od

in
g

Pa
rs

e
C

on
fig

ur
at

io
n

le
e

—
—

M
ul

ti-
pa

ss
Si

ev
es

sa
pe

na
A

ll
m

en
tio

n
pa

ir
s

an
d

lo
ng

er
of

ne
st

ed
m

en
tio

ns
w

ith
co

m
m

on
he

ad
ke

pt

M
en

tio
n

pa
ir

s
w

ith
le

ss
th

an
th

re
sh

ol
d

(5
)

nu
m

be
ro

fd
iff

er
en

ta
ttr

ib
ut

e
va

lu
es

ar
e

co
ns

id
er

ed
(2

2%
ou

to
f9

9%
or

ig
in

al
ar

e
di

sc
ar

de
d)

It
er

at
iv

e
1-

be
st

ch
an

g
C

lo
se

st
an

te
ce

de
nt

A
ll

pr
ec

ed
in

g
m

en
tio

ns
in

a
un

io
n

of
of

go
ld

an
d

pr
ed

ic
te

d
m

en
tio

ns
.M

en
tio

ns
w

he
re

th
e

fir
st

is
pr

on
ou

n
an

d
ot

he
rn

ot
ar

e
no

t
co

ns
id

er
ed

B
es

tl
in

k
an

d
A

ll
lin

ks
st

ra
te

gy
;w

ith
an

d
w

ith
ou

tc
on

st
ra

in
ts

–
B

es
tl

in
k

w
ith

ou
t

co
ns

tr
ai

nt
s

w
as

se
le

ct
ed

fo
rt

he
of

fic
ia

lr
un

ca
i

W
ei

gh
ts

ar
e

tr
ai

ne
d

on
pa

rt
of

th
e

tr
ai

ni
ng

da
ta

R
ec

ur
si

ve
2-

w
ay

Sp
ec

tr
al

cl
us

te
ri

ng
(A

ga
rw

al
,2

00
5)

nu
gu

es
C

lo
se

st
A

nt
ec

ed
en

t(
So

on
,2

00
1)

N
eg

at
iv

e
ex

am
pl

es
in

be
tw

ee
n

an
ap

ho
ra

nd
cl

os
es

ta
nt

ec
ed

en
t(

So
on

,2
00

1)
C

lo
se

st
-fi

rs
tc

lu
st

er
in

g
fo

rp
ro

no
un

s
an

d
B

es
t-

fir
st

cl
us

te
ri

ng
fo

rn
on

-p
ro

no
un

s
1-

be
st

ur
yu

pi
na

C
lo

se
st

an
te

ce
de

nt
(S

oo
n,

20
01

)
N

eg
at

iv
e

ex
am

pl
es

in
be

tw
ee

n
an

ap
ho

ra
nd

cl
os

es
ta

nt
ec

ed
en

t(
So

on
,2

00
1)

m
en

tio
n

pa
ir

m
od

el
w

ith
ou

tr
an

ki
ng

as
in

So
on

20
01

sa
nt

os
E

xt
en

de
d

ve
rs

io
n

of
So

on
(2

00
1)

w
he

re
in

ad
di

tio
n

to
th

ei
rs

tr
at

eg
y,

po
si

tiv
e

an
d

ne
ga

tiv
e

ex
am

pl
es

fr
om

m
en

tio
ns

in
th

e
se

nt
en

ce
of

th
e

cl
os

es
tp

re
ce

di
ng

an
te

ce
de

nt
ar

e
co

ns
id

er
ed

L
im

ite
d

nu
m

be
ro

fp
re

ce
di

ng
m

en
tio

ns
60

fo
ra

ut
om

at
ic

an
d

40
gi

ve
n

go
ld

bo
un

da
ri

es
;

A
gg

re
ss

iv
e-

m
er

ge
cl

us
te

ri
ng

(M
cc

ar
th

y
an

d
L

en
he

rt
,1

99
5)

so
ng

Pr
e-

cl
us

te
rp

ai
rm

od
el

s
se

pa
ra

te
fo

re
ac

h
pa

ir
N

P
-N

P,
N

P
-P

R
P

an
d

P
R

P
-P

R
P

Pr
e-

cl
us

te
rs

,w
ith

si
ng

le
to

n
pr

on
ou

n
pr

e-
cl

us
te

rs
,a

nd
us

e
cl

os
es

t-
fir

st
cl

us
te

ri
ng

.
D

iff
er

en
tl

in
k

m
od

el
s

ba
se

d
on

th
e

ty
pe

of
lin

ki
ng

m
en

tio
ns

–
N

P
-P

R
P,

P
R

P
-P

R
P

an
d

N
P
-N

P

st
oy

an
ov

Sm
ar

tP
ai

rG
en

er
at

io
n

(S
m

ar
tP

G
)w

he
re

th
e

ty
pe

of
an

te
ce

de
nt

is
de

te
rm

in
ed

by
th

e
ty

pe
of

an
ap

ho
ru

si
ng

a
se

to
fr

ul
es

Si
ng

le
-l

in
k

cl
us

te
ri

ng
by

co
m

pu
tin

g
tr

an
si

tiv
e

cl
os

ur
e

be
tw

ee
n

pa
ir

w
is

e
po

si
tiv

es
.

so
bh

a
C

lo
se

st
an

te
ce

de
nt

(S
oo

n,
20

01
)

N
eg

at
iv

e
ex

am
pl

es
in

be
tw

ee
n

an
ap

ho
ra

nd
cl

os
es

ta
nt

ec
ed

en
t(

So
on

,2
00

1)
Pr

on
om

in
al

:a
ll

pr
ec

ed
in

g
N

Ps
in

th
e

se
nt

en
ce

an
d

pr
ec

ed
in

g
4

se
nt

en
ce

s
kl

en
ne

r
—

—
In

cr
em

en
ta

le
nt

ity
cr

ea
tio

n

ko
bd

an
i

C
lo

se
st

an
te

ce
de

nt
(S

oo
n,

20
01

)
N

eg
at

iv
e

ex
am

pl
es

in
be

tw
ee

n
an

ap
ho

ra
nd

cl
os

es
ta

nt
ec

ed
en

t(
So

on
,2

00
1)

B
es

t-
fir

st
cl

us
te

ri
ng

.T
hr

es
ho

ld
of

10
0

w
or

ds
us

ed
fo

rl
on

g
do

cu
m

en
ts

1-
be

st

zh
ou

C
lo

se
st

an
te

ce
de

nt
(S

oo
n,

20
01

)
N

eg
at

iv
e

ex
am

pl
es

in
be

tw
ee

n
an

ap
ho

ra
nd

cl
os

es
ta

nt
ec

ed
en

t(
So

on
,2

00
1)

—

ch
ar

to
n

Fr
om

th
e

en
d

of
th

e
do

cu
m

en
t,

un
til

an
an

te
ce

de
nt

is
fo

un
d,

or
10

m
en

tio
ns

N
eg

at
iv

e
ex

am
pl

es
in

be
tw

ee
n

an
ap

ho
ra

nd
cl

os
es

ta
nt

ec
ed

en
t

M
L

P
w

ith
sc

or
e

of
0.

5
us

ed
fo

rl
in

ki
ng

an
d

10
m

en
tio

ns

ya
ng

C
lo

se
st

an
te

ce
de

nt
(S

oo
n,

20
01

)
N

eg
at

iv
e

ex
am

pl
es

in
be

tw
ee

n
an

ap
ho

ra
nd

cl
os

es
ta

nt
ec

ed
en

t(
So

on
,2

00
1)

M
ax

im
um

23
se

nt
en

ce
s

to
th

e
le

ft
;

C
on

st
ra

in
ed

cl
us

te
ri

ng

ha
o

C
lo

se
st

an
te

ce
de

nt
(S

oo
n,

20
01

)
N

eg
at

iv
e

ex
am

pl
es

in
be

tw
ee

n
an

ap
ho

ra
nd

cl
os

es
ta

nt
ec

ed
en

t(
So

on
,2

00
1)

B
ea

m
se

ar
ch

(L
uo

,2
00

4)
Pa

ck
ed

fo
re

st

xi
nx

in
C

lo
se

st
an

te
ce

de
nt

(S
oo

n,
20

01
)

N
eg

at
iv

e
ex

am
pl

es
in

be
tw

ee
n

an
ap

ho
ra

nd
cl

os
es

ta
nt

ec
ed

en
t(

So
on

,2
00

1)
B

es
t-

fir
st

cl
us

te
ri

ng
fo

llo
w

ed
by

IL
P

op
tim

iz
at

io
n

zh
an

g
C

lo
se

st
an

te
ce

de
nt

(S
oo

n,
20

01
)

N
eg

at
iv

e
ex

am
pl

es
in

be
tw

ee
n

an
ap

ho
ra

nd
cl

os
es

ta
nt

ec
ed

en
t(

So
on

,2
00

1)
W

in
do

w
of

10
0

m
ar

ka
bl

es

ku
m

m
er

fie
ld

—
—

Pr
e-

an
d

po
st

-r
es

ol
ut

io
n

fil
te

rs
G

iv
en

+
B

er
ke

le
y

pa
rs

er
pa

rs
es

;p
ar

se
s

w
ith

ou
tN

M
L

s
im

pr
ov

ed
pe

rf
or

m
an

ce
sl

ig
ht

ly
;r

e-
tr

ai
ne

d
B

er
ke

le
y

pa
rs

er
zh

ek
ov

a
E

xa
m

pl
es

in
th

e
pa

st
th

re
e

se
nt

en
ce

s
Fr

om
la

st
po

ss
ib

le
m

en
tio

n
in

do
cu

m
en

t
ir

w
in

C
lu

st
er

qu
er

y
w

ith
N

U
L

L
cl

us
te

rf
or

di
sc

ou
rs

e
ne

w
m

en
tio

ns
C

lu
st

er
-r

an
ki

ng
ap

pr
oa

ch
(r

ah
m

an
,2

00
9)

Ta
bl

e
23

:P
ar

tic
ip

at
in

g
sy

st
em

pr
ofi

le
s

–
Pa

rt
II

.T
hi

s
fo

cu
se

s
on

th
e

w
ay

po
si

tiv
e

an
d

ne
ga

tiv
e

ex
am

pl
es

w
er

e
ge

ne
ra

te
d

an
d

th
e

de
co

di
ng

st
ra

te
gy

us
ed

.

23



OntoNotes corpus, and presented the results from an
evaluation on learning such unrestricted entities and
events in text. The following represent our conclu-
sions on reviewing the results:

• Perhaps the most surprising finding was that the
best-performing system (lee) was completely
rule-based, rather than trained. This suggests
that their rule-based approach was able to do
a more effective job of combining the multiple
sources of evidence than the trained systems.
The features for coreference prediction are cer-
tainly more complex than for many other lan-
guage processing tasks, which makes it more
challenging to generate effective feature com-
binations. The rule-based approach used by
the best-performing system seemed to benefit
from a heuristic that captured the most con-
fident links before considering less confident
ones, and also made use of the information in
the guidelines in a slightly more refined man-
ner than other systems. They also included ap-
positives and copular constructions in their cal-
culations. Although OntoNotes does not count
those as instances of IDENT coreference, using
that information may have helped their system
discover additional useful links.

• It is interesting to note that the developers of
the lee system also did the experiment of run-
ning their system using gold standard informa-
tion on the individual layers, rather than auto-
matic model predictions. The somewhat sur-
prising result was that using perfect informa-
tion for the other layers did not end up improv-
ing coreference performance much, if at all. It
is not clear whether this means that: i) Auto-
matic predictors for the individual layers are
accurate enough already; ii) Information cap-
tured by those supplementary layers actually
does not provide much leverage for resolving
coreference; or iii) researchers have yet have
found an effective way of capturing and utiliz-
ing the extra information provided by these lay-
ers.

• It does seem that collecting information about
an entity by merging information across the
various attributes of the mentions that comprise
it can be useful, though not all systems that at-
tempted this achieved a benefit.

• System performance did not seem to vary as
much across the different genres as is nor-
mally the case with language processing tasks,

which could suggest that coreference is rela-
tively genre insensitive, or it is possible that
scores are two low for the difference to be ap-
parent. Comparisons are difficult, however, be-
cause the spoken genres were treated here with
perfect speech recognition accuracy and perfect
speaker turn information. Under more realis-
tic application conditions, the spread in perfor-
mance between genres might be greater.

• It is noteworthy that systems did not seem to
attempt the kind of joint inference that could
make use of the full potential of various layers
available in OntoNotes, but this could well have
been owing to the limited time available for the
shared task.

• We had expected to see more attention paid to
event coreference, which is a novel feature in
this data, but again, given the time constraints
and given that events represent only a small
portion of the total, it is not surprising that most
systems chose not to focus on it.

• Scoring coreference seems to remain a signif-
icant challenge. There does not seem to be an
objective way to establish one metric in prefer-
ence to another in the absence of a specific ap-
plication. On the other hand, the system rank-
ings do not seem terribly sensitive to the par-
ticular metric chosen. It is interesting that both
versions of the CEAF metric – which tries to
capture the goodness of the entities in the out-
put – seem much lower than the other metric,
though it is not clear whether that means that
our systems are doing a poor job of creating
coherent entities or whether that metric is just
especially harsh.

Finally, it is interesting to note that the problem of
coreference does not seem to be following the same
kind of learning curve that we are used to with other
problems of this sort. While performance has im-
proved somewhat, it is not clear how far we will be
able to go given the strategies at hand, or whether
new techniques will be needed to capture additional
information from the texts or from world knowl-
edge. We hope that this corpus and task will provide
a useful resource for continued experimentation to
help resolve this issue.

Acknowledgments
We gratefully acknowledge the support of the
Defense Advanced Research Projects Agency

24



(DARPA/IPTO) under the GALE program,
DARPA/CMO Contract No. HR0011-06-C-0022.
We would like to thank all the participants. Without
their hard work, patience and perseverance this eval-
uation would not have been a success. We would
also like to thank the Linguistic Data Consortium
for making the OntoNotes 4.0 corpus freely and
timely available to the participants. Emili Sapena,
who graciously allowed the use of his scorer
implementation, and made available enhancements
and immediately fixed issues that were uncovered
during the evaluation. Finally, we offer our special
thanks to Lluı́s Màrquez and Joakim Nivre for their
wonderful support and guidance without which this
task would not have been successful.

References

Olga Babko-Malaya, Ann Bies, Ann Taylor, Szuting Yi,
Martha Palmer, Mitch Marcus, Seth Kulick, and Li-
bin Shen. 2006. Issues in synchronizing the English
treebank and propbank. In Workshop on Frontiers in
Linguistically Annotated Corpora 2006, July.

Amit Bagga and Breck Baldwin. 1998. Algorithms for
Scoring Coreference Chains. In The First Interna-
tional Conference on Language Resources and Eval-
uation Workshop on Linguistics Coreference, pages
563–566.

Shane Bergsma and Dekang Lin. 2006. Bootstrapping
path-based pronoun resolution. In Proceedings of the
21st International Conference on Computational Lin-
guistics and 44th Annual Meeting of the Association
for Computational Linguistics, pages 33–40, Sydney,
Australia, July.

Jie Cai and Michael Strube. 2010. Evaluation metrics
for end-to-end coreference resolution systems. In Pro-
ceedings of the 11th Annual Meeting of the Special In-
terest Group on Discourse and Dialogue, SIGDIAL
’10, pages 28–36.

Eugene Charniak and Mark Johnson. 2001. Edit detec-
tion and parsing for transcribed speech. In Proceed-
ings of the Second Meeting of North American Chapter
of the Association of Computational Linguistics, June.

Eugene Charniak and Mark Johnson. 2005. Coarse-to-
fine n-best parsing and maxent discriminative rerank-
ing. In Proceedings of the 43rd Annual Meeting of the
Association for Computational Linguistics (ACL), Ann
Arbor, MI, June.

Nancy Chinchor and Beth Sundheim. 2003. Message
understanding conference (MUC) 6. In LDC2003T13.

Nancy Chinchor. 2001. Message understanding confer-
ence (MUC) 7. In LDC2001T02.

Aron Culotta, Michael Wick, Robert Hall, and Andrew
McCallum. 2007. First-order probabilistic models for
coreference resolution. In HLT/NAACL, pages 81–88.

Pascal Denis and Jason Baldridge. 2007. Joint de-
termination of anaphoricity and coreference resolu-
tion using integer programming. In Proceedings of
HLT/NAACL.

Pascal Denis and Jason Baldridge. 2009. Global joint
models for coreference resolution and named entity
classification. Procesamiento del Lenguaje Natural,
(42):87–96.

Charles Fillmore, Christopher Johnson, and Miriam R. L.
Petruck. 2003. Background to framenet. Interna-
tional Journal of Lexicography, 16(3).

G. G. Doddington, A. Mitchell, M. Przybocki,
L. Ramshaw, S. Strassell, and R. Weischedel.
2000. The automatic content extraction (ACE)
program-tasks, data, and evaluation. In Proceedings
of LREC.

Aria Haghighi and Dan Klein. 2010. Coreference reso-
lution in a modular, entity-centered model. In Human
Language Technologies: The 2010 Annual Conference
of the North American Chapter of the Association for
Computational Linguistics, pages 385–393, Los An-
geles, California, June.

Jan Hajič, Massimiliano Ciaramita, Richard Johans-
son, Daisuke Kawahara, Maria Antònia Martı́, Lluı́s
Màrquez, Adam Meyers, Joakim Nivre, Sebastian
Padó, Jan Štěpánek, Pavel Straňák, Mihai Surdeanu,
Nianwen Xue, and Yi Zhang. 2009. The CoNLL-
2009 shared task: Syntactic and semantic dependen-
cies in multiple languages. In Proceedings of the Thir-
teenth Conference on Computational Natural Lan-
guage Learning (CoNLL 2009): Shared Task, pages
1–18, Boulder, Colorado, June.

Sanda M. Harabagiu, Razvan C. Bunescu, and Steven J.
Maiorano. 2001. Text and knowledge mining for
coreference resolution. In NAACL.

L. Hirschman and N. Chinchor. 1997. Coreference task
definition (v3.0, 13 jul 97). In Proceedings of the Sev-
enth Message Understanding Conference.

Eduard Hovy, Mitchell Marcus, Martha Palmer, Lance
Ramshaw, and Ralph Weischedel. 2006. OntoNotes:
The 90% solution. In Proceedings of HLT/NAACL,
pages 57–60, New York City, USA, June. Association
for Computational Linguistics.

Karin Kipper, Anna Korhonen, Neville Ryant, and
Martha Palmer. 2000. A large-scale classification of
english verbs. Language Resources and Evaluation,
42(1):21 – 40.

Xiaoqiang Luo. 2005. On coreference resolution perfor-
mance metrics. In Proceedings of Human Language
Technology Conference and Conference on Empirical

25



Methods in Natural Language Processing, pages 25–
32, Vancouver, British Columbia, Canada, October.

Mitchell P. Marcus, Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1993. Building a large annotated cor-
pus of English: The Penn treebank. Computational
Linguistics, 19(2):313–330, June.

Andrew McCallum and Ben Wellner. 2004. Conditional
models of identity uncertainty with application to noun
coreference. In Advances in Neural Information Pro-
cessing Systems (NIPS).

Joseph McCarthy and Wendy Lehnert. 1995. Using de-
cision trees for coreference resolution. In Proceedings
of the Fourteenth International Conference on Artifi-
cial Intelligence, pages 1050–1055.

Thomas S. Morton. 2000. Coreference for nlp applica-
tions. In Proceedings of the 38th Annual Meeting of
the Association for Computational Linguistics, Octo-
ber.

Vincent Ng. 2007. Shallow semantics for coreference
resolution. In Proceedings of the IJCAI.

Vincent Ng. 2010. Supervised noun phrase coreference
research: The first fifteen years. In Proceedings of the
48th Annual Meeting of the Association for Compu-
tational Linguistics, pages 1396–1411, Uppsala, Swe-
den, July.

Martha Palmer, Daniel Gildea, and Paul Kingsbury.
2005. The Proposition Bank: An annotated corpus of
semantic roles. Computational Linguistics, 31(1):71–
106.

Martha Palmer, Hoa Trang Dang, and Christiane Fell-
baum. 2007. Making fine-grained and coarse-grained
sense distinctions, both manually and automatically.

R. Passonneau. 2004. Computing reliability for corefer-
ence annotation. In Proceedings of LREC.

Massimo Poesio and Ron Artstein. 2005. The reliability
of anaphoric annotation, reconsidered: Taking ambi-
guity into account. In Proceedings of the Workshop on
Frontiers in Corpus Annotations II: Pie in the Sky.

Massimo Poesio. 2004. The mate/gnome scheme for
anaphoric annotation, revisited. In Proceedings of
SIGDIAL.

Simone Paolo Ponzetto and Massimo Poesio. 2009.
State-of-the-art nlp approaches to coreference resolu-
tion: Theory and practical recipes. In Tutorial Ab-
stracts of ACL-IJCNLP 2009, page 6, Suntec, Singa-
pore, August.

Simone Paolo Ponzetto and Michael Strube. 2005. Se-
mantic role labeling for coreference resolution. In
Companion Volume of the Proceedings of the 11th
Meeting of the European Chapter of the Associa-
tion for Computational Linguistics, pages 143–146,
Trento, Italy, April.

Simone Paolo Ponzetto and Michael Strube. 2006.
Exploiting semantic role labeling, WordNet and
Wikipedia for coreference resolution. In Proceedings
of the HLT/NAACL, pages 192–199, New York City,
N.Y., June.

Sameer Pradhan, Kadri Hacioglu, Valerie Krugler,
Wayne Ward, James Martin, and Dan Jurafsky. 2005.
Support vector learning for semantic argument classi-
fication. Machine Learning Journal, 60(1):11–39.

Sameer Pradhan, Eduard Hovy, Mitchell Marcus, Martha
Palmer, Lance Ramshaw, and Ralph Weischedel.
2007a. OntoNotes: A Unified Relational Semantic
Representation. International Journal of Semantic
Computing, 1(4):405–419.

Sameer Pradhan, Lance Ramshaw, Ralph Weischedel,
Jessica MacBride, and Linnea Micciulla. 2007b.
Unrestricted Coreference: Indentifying Entities and
Events in OntoNotes. In in Proceedings of the
IEEE International Conference on Semantic Comput-
ing (ICSC), September 17-19.

Altaf Rahman and Vincent Ng. 2009. Supervised mod-
els for coreference resolution. In Proceedings of the
2009 Conference on Empirical Methods in Natural
Language Processing, pages 968–977, Singapore, Au-
gust. Association for Computational Linguistics.

W. M. Rand. 1971. Objective criteria for the evaluation
of clustering methods. Journal of the American Statis-
tical Association, 66(336).

Marta Recasens and Eduard Hovy. 2011. Blanc: Im-
plementing the rand index for coreference evaluation.
Natural Language Engineering.

Marta Recasens, Lluı́s Màrquez, Emili Sapena,
M. Antònia Martı́, Mariona Taulé, Véronique
Hoste, Massimo Poesio, and Yannick Versley. 2010.
Semeval-2010 task 1: Coreference resolution in
multiple languages. In Proceedings of the 5th Interna-
tional Workshop on Semantic Evaluation, pages 1–8,
Uppsala, Sweden, July.

W. Soon, H. Ng, and D. Lim. 2001. A machine learn-
ing approach to coreference resolution of noun phrase.
Computational Linguistics, 27(4):521–544.

Veselin Stoyanov, Nathan Gilbert, Claire Cardie, and
Ellen Riloff. 2009. Conundrums in noun phrase coref-
erence resolution: Making sense of the state-of-the-
art. In Proceedings of the Joint Conference of the 47th
Annual Meeting of the ACL and the 4th International
Joint Conference on Natural Language Processing of
the AFNLP, pages 656–664, Suntec, Singapore, Au-
gust. Association for Computational Linguistics.

Mihai Surdeanu, Richard Johansson, Adam Meyers,
Lluı́s Màrquez, and Joakim Nivre. 2008. The CoNLL
2008 shared task on joint parsing of syntactic and se-
mantic dependencies. In CoNLL 2008: Proceedings

26



of the Twelfth Conference on Computational Natu-
ral Language Learning, pages 159–177, Manchester,
England, August.

Yannick Versley. 2007. Antecedent selection techniques
for high-recall coreference resolution. In Proceedings
of the 2007 Joint Conference on Empirical Methods
in Natural Language Processing and Computational
Natural Language Learning (EMNLP-CoNLL).

M. Vilain, J. Burger, J. Aberdeen, D. Connolly, and
L. Hirschman. 1995. A model theoretic coreference
scoring scheme. In Proceedings of the Sixth Message
Undersatnding Conference (MUC-6), pages 45–52.

Ralph Weischedel and Ada Brunstein. 2005. BBN pro-
noun coreference and entity type corpus LDC catalog
no.: LDC2005T33. BBN Technologies.

Ralph Weischedel, Eduard Hovy, Martha Palmer, Mitch
Marcus, Robert Belvin, Sameer Pradhan, Lance
Ramshaw, and Nianwen Xue. 2011. OntoNotes: A
Large Training Corpus for Enhanced Processing. In
Joseph Olive, Caitlin Christianson, and John McCary,
editors, Handbook of Natural Language Processing
and Machine Translation. Springer.

27


