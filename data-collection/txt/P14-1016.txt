



















































Weakly Supervised User Profile Extraction from Twitter


Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 165–174,
Baltimore, Maryland, USA, June 23-25 2014. c©2014 Association for Computational Linguistics

Weakly Supervised User Profile Extraction from Twitter

Jiwei Li1, Alan Ritter2, Eduard Hovy1
1Language Technology Institute, 2Machine Learning Department

Carnegie Mellon University, Pittsburgh, PA 15213, USA
bdlijiwei@gmail.com, rittera@cs.cmu.edu, ehovy@andrew.cmu.edu

Abstract

While user attribute extraction on social
media has received considerable attention,
existing approaches, mostly supervised,
encounter great difficulty in obtaining gold
standard data and are therefore limited
to predicting unary predicates (e.g., gen-
der). In this paper, we present a weakly-
supervised approach to user profile extrac-
tion from Twitter. Users’ profiles from so-
cial media websites such as Facebook or
Google Plus are used as a distant source
of supervision for extraction of their at-
tributes from user-generated text. In addi-
tion to traditional linguistic features used
in distant supervision for information ex-
traction, our approach also takes into ac-
count network information, a unique op-
portunity offered by social media. We test
our algorithm on three attribute domains:
spouse, education and job; experimental
results demonstrate our approach is able
to make accurate predictions for users’ at-
tributes based on their tweets.1

1 Introduction

The overwhelming popularity of online social me-
dia creates an opportunity to display given as-
pects of oneself. Users’ profile information in
social networking websites such as Facebook2 or
Google Plus3 provides a rich repository personal
information in a structured data format, making it
amenable to automatic processing. This includes,
for example, users’ jobs and education, and pro-
vides a useful source of information for applica-
tions such as search4, friend recommendation, on-

1Both code and data are available at http://aclweb.
org/aclwiki/index.php?title=Profile_data

2https://www.facebook.com/
3https://plus.google.com/
4https://www.facebook.com/about/

graphsearch

@[shanenicholson] has taken all the kids today so
I can go shopping-CHILD FREE! #iloveyoushano
#iloveyoucreditcard
Tamworth promo day with my handsome classy husband
@[shanenicholson]

Spouse: shanenicholson

I got accepted to be part of the UofM engineering safety
pilot program in [FSU]
Here in class. (@ [Florida State University] - Williams
Building)
Don’t worry , guys ! Our beloved [FSU] will always con-
tinue to rise ” to the top !

Education: Florida State University (FSU)

first day of work at [HuffPo], a sports bar woo come visit
me yo..
start to think we should just add a couple desks to the
[HuffPo] newsroom for Business Insider writers
just back from [HuffPo], what a hell !

Job: HuffPo

Table 1: Examples of Twitter message clues for
user profile inference.

line advertising, computational social science and
more.

Although profiles exist in an easy-to-use, struc-
tured data format, they are often sparsely popu-
lated; users rarely fully complete their online pro-
files. Additionally, some social networking ser-
vices such as Twitter don’t support this type of
structured profile data. It is therefore difficult to
obtain a reasonably comprehensive profile of a
user, or a reasonably complete facet of information
(say, education level) for a class of users. While
many users do not explicitly list all their personal
information in their online profile, their user gen-
erated content often contains strong evidence to
suggest many types of user attributes, for example
education, spouse, and employment (See Table 1).
Can one use such information to infer more de-
tails? In particular, can one exploit indirect clues
from an unstructured data source like Twitter to
obtain rich, structured user profiles?

In this paper we demonstrate that it is feasi-
ble to automatically extract Facebook-style pro-

165



files directly from users’ tweets, thus making
user profile data available in a structured format
for upstream applications. We view user profile
inference as a structured prediction task where
both text and network information are incorpo-
rated. Concretely, we cast user profile predic-
tion as binary relation extraction (Brin, 1999),
e.g., SPOUSE(Useri, Userj), EDUCATION(Useri,
Entityj) and EMPLOYER(Useri, Entityj). Inspired
by the concept of distant supervision, we collect
training tweets by matching attribute ground truth
from an outside “knowledge base” such as Face-
book or Google Plus.

One contribution of the work presented here is
the creation of the first large-scale dataset on three
general Twitter user profile domains (i.e., EDUCA-
TION, JOB, SPOUSE). Experiments demonstrate
that by simultaneously harnessing both text fea-
tures and network information, our approach is
able to make accurate user profile predictions. We
are optimistic that our approach can easily be ap-
plied to further user attributes such as HOBBIES
and INTERESTS (MOVIES, BOOKS, SPORTS or
STARS), RELIGION, HOMETOWN, LIVING LOCA-
TION, FAMILY MEMBERS and so on, where train-
ing data can be obtained by matching ground truth
retrieved from multiple types of online social me-
dia such as Facebook, Google Plus, or LinkedIn.
Our contributions are as follows:

• We cast user profile prediction as an informa-
tion extraction task.

• We present a large-scale dataset for this task
gathered from various structured and unstruc-
tured social media sources.

• We demonstrate the benefit of jointly rea-
soning about users’ social network structure
when extracting their profiles from text.

• We experimentally demonstrate the effective-
ness of our approach on 3 relations: SPOUSE,
JOB and EDUCATION.

The remainder of this paper is organized as fol-
lows: We summarize related work in Section 2.
The creation of our dataset is described in Section
3. The details of our model are presented in Sec-
tion 4. We present experimental results in Section
5 and conclude in Section 6.

2 Related Work

While user profile inference from social media has
received considerable attention (Al Zamal et al.,
2012; Rao and Yarowsky, 2010; Rao et al., 2010;
Rao et al., 2011), most previous work has treated
this as a classification task where the goal is to pre-
dict unary predicates describing attributes of the
user. Examples include gender (Ciot et al., 2013;
Liu and Ruths, 2013; Liu et al., 2012), age (Rao et
al., 2010), or political polarity (Pennacchiotti and
Popescu, 2011; Conover et al., 2011).

A significant challenge that has limited previous
efforts in this area is the lack of available training
data. For example, researchers obtain training data
by employing workers from Amazon Mechanical
Turk to manually identify users’ gender from pro-
file pictures (Ciot et al., 2013). This approach is
appropriate for attributes such as gender with a
small numbers of possible values (e.g., male or fe-
male), for which the values can be directly iden-
tified. However for attributes such as spouse or
education there are many possible values, making
it impossible to manually search for gold standard
answers within a large number of tweets which
may or may not contain sufficient evidence.

Also related is the Twitter user timeline extrac-
tion algorithm of Li and Cardie (2013). This work
is not focused on user attribute extraction, how-
ever.

Distant Supervision Distant supervision, also
known as weak supervision, is a method for learn-
ing to extract relations from text using ground
truth from an existing database as a source of
supervision. Rather than relying on mention-
level annotations, which are expensive and time
consuming to generate, distant supervision lever-
ages readily available structured data sources as
a weak source of supervision for relation ex-
traction from related text corpora (Craven et
al., 1999). For example, suppose r(e1, e2) =
IsIn(Paris, France) is a ground tuple in the
database and s =“Paris is the capital of France”
contains synonyms for both “Paris” and “France”,
then we assume that s may express the fact
r(e1, e2) in some way and can be used as pos-
itive training examples. In addition to the wide
use in text entity relation extraction (Mintz et al.,
2009; Ritter et al., 2013; Hoffmann et al., 2011;
Surdeanu et al., 2012; Takamatsu et al., 2012),
distant supervision has been applied to multiple

166



Figure 1: Illustration of Goolge Plus “knowledge
base”.

fields such as protein relation extraction (Craven
et al., 1999; Ravikumar et al., 2012), event extrac-
tion from Twitter (Benson et al., 2011), sentiment
analysis (Go et al., 2009) and Wikipedia infobox
generation (Wu and Weld, 2007).

Homophily Online social media offers a rich
source of network information. McPherson et
al. (2001) discovered that people sharing more
attributes such as background or hobby have
a higher chance of becoming friends in social
media. This property, known as HOMOPHILY
(summarized by the proverb “birds of a feather
flock together”) (Al Zamal et al., 2012) has been
widely applied to community detection (Yang and
Leskovec, 2013) and friend recommendation (Guy
et al., 2010) on social media. In the user attribute
extraction literature, researchers have considered
neighborhood context to boost inference accuracy
(Pennacchiotti and Popescu, 2011; Al Zamal et al.,
2012), where information about the degree of their
connectivity to their pre-labeled users is included
in the feature vectors. A related algorithm by Mis-
love et al. (2010) crawled Facebook profiles of
4,000 Rice University students and alumni and in-
ferred attributes such as major and year of ma-
triculation purely based on network information.
Mislove’s work does not consider the users’ text
stream, however. As we demonstrate below, rely-
ing solely on network information is not enough to
enable inference about attributes.

3 Dataset Creation

We now describe the generation of our distantly
supervised training dataset in detail. We make
use of Google Plus and Freebase to obtain ground
facts and extract positive/negative bags of post-
ings from users’ twitter streams according to the
ground facts.

Figure 2: Example of fetching tweets containing
entity USC mention from Miranda Cosgrove (an
American actress and singer-songwriter)’s twitter
stream.

Education/Job We first used the Google Plus
API5 (shown in Figure 1) to obtain a seed set
of users whose profiles contain both their educa-
tion/job status and a link to their twitter account.6

Then, we fetched tweets containing the mention of
the education/job entity from each correspondent
user’s twitter stream using Twitter’s search API7

(shown in Figure 2) and used them to construct
positive bags of tweets expressing the associated
attribute, namely EDUCATION(Useri, Entityj), or
EMPLOYER(Useri, Entityj). The Freebase API

8

is employed for alias recognition, to match terms
such as “Harvard University”, “Harvard”, “Har-
vard U” to a single The remainder of each corre-
sponding user’s entire Twitter feed is used as neg-
ative training data.9

We expanded our dataset from the seed users
according to network information provided by
Google Plus and Twitter. Concretely, we crawled
circle information of users in the seed set from
both their Twitter and Google Plus accounts and
performed a matching to pick out shared users
between one’s Twitter follower list and Google
Plus Circle. This process assures friend identity
and avoids the problem of name ambiguity when
matching accounts across websites. Among candi-
date users, those who explicitly display Job or Ed-
ucation information on Google Plus are preserved.
We then gathered positive and negative data as de-
scribed above.

Dataset statistics are presented in Table 2. Our

5https://developers.google.com/+/api/
6An unambiguous twitter account link is needed here be-

cause of the common phenomenon of name duplication.
7https://twitter.com/search
8http://wiki.freebase.com/wiki/

Freebase_API
9Due to Twitter user timeline limit, we crawled at most

3200 tweets for each user.

167



education dataset contains 7,208 users, 6,295 of
which are connected to others in the network. The
positive training set for the EDUCATION is com-
prised of 134,060 tweets.

Spouse Facebook is the only type of social me-
dia where spouse information is commonly dis-
played. However, only a tiny amount of individ-
ual information is publicly accessible from Face-
book Graph API10. To obtain ground truth for the
spouse relation at large scale, we turned to Free-
base11, a large, open-domain database, and gath-
ered instances of the /PEOPLE/PERSON/SPOUSE
relation. Positive/negative training tweets are ob-
tained in the same way as was previously de-
scribed for EDUCATION and JOB. It is worth
noting that our Spouse dataset is not perfect, as
individuals retrieved from Freebase are mostly
celebrities, and thus it’s not clear whether this
group of people are representative of the general
population.

SPOUSE is an exception to the “ho-
mophily” effect. But it exhibits another
unique property, known as, REFLEXIVITY: fact
IsSpouseOf(e1, e2) and IsSpouseOf(e2, e1)
will hold or not hold at the same time. Given train-
ing data expressing the tuple IsSpouseOf(e1, e2)
from user e1’s twitter stream, we also gather user
e2’s tweet collection, and fetch tweets with the
mention of e1. We augment negative training
data from e2 as in the case of Education and Job.
Our Spouse dataset contains 1,636 users, where
there are 554 couples (1108 users). Note that
the number of positive entities (3,121) is greater
than the number of users as (1) one user can have
multiple spouses at different periods of time (2)
multiple entities may point to the same individual,
e.g., BarackObama, Barack Obama and Barack.

4 Model

We now describe our approach to predicting user
profile attributes.

4.1 Notation
Message X: Each user i ∈ [1, I] is associ-
ated with his Twitter ID and his tweet corpus
Xi. Xi is comprised of a collection of tweets
Xi = {xi,j}j=Nij=1 , where Ni denotes the number
of tweets user i published.

10https://developers.facebook.com/docs/
graph-api/

11http://www.freebase.com/

Education Job Spouse
#Users 7,208 1,806 1,636
#Users Con-
nected

6,295 1,407 1,108

#Edges 11,167 3,565 554
#Pos Entities 451 380 3121
#Pos Tweets 124,801 65,031 135,466
#Aver Pos
Tweets per User

17.3 36.6 82.8

#Neg Entity 6,987,186 4,405,530 8,840,722
#Neg Tweets 16,150,600 10,687,403 12,872,695

Table 2: Statistics for our Dataset

Tweet Collection Lei : Lei denotes the collection
of postings containing the mention of entity e from
user i. Lei ⊂ Xi.
Entity attribute indicator zki,e and zki,x: For
each entity e ∈ Xi, there is a boolean variable zki,e,
indicating whether entity e expresses attribute k of
user i. Each posting x ∈ Lei is associated with at-
tribute indicator zki,x indicating whether posting x
expresses attribute k of user i. zki,e and z

k
i,x are

observed during training and latent during testing.

Neighbor set F ki : F ki denotes the neighbor set
of user i. For Education (k = 0) and Job (k = 1),
F ki denotes the group of users within the network
that are in friend relation with user i. For Spouse
attribute, F ki denote current user’s spouse.

4.2 Model
The distant supervision assumes that if entity e
corresponds to an attribute for user i, at least one
posting from user i’s Twitter stream containing a
mention of emight express that attribute. For user-
level attribute prediction, we adopt the following
two strategies:

(1) GLOBAL directly makes aggregate (entity)
level prediction for zki,e, where features for all
tweets from Lei are aggregated to one vector for
training and testing, following Mintz et al. (2009).

(2) LOCAL makes local tweet-level predictions
for each tweet zei,x, x ∈ Lki in the first place, mak-
ing the stronger assumption that all mentions of an
entity in the users’ profile are expressing the asso-
ciated attribute. An aggregate-level decision zki,e is
then made from the deterministic OR operators.

zei,x =

{
1 ∃x ∈ Lei , s.t.zki,x = 1
0 Otherwise

(1)

The rest of this paper describes GLOBAL in de-
tail. The model and parameters with LOCAL are
identical to those in GLOBAL except that LOCAL

168



encode a tweet-level feature vector rather than an
aggregate one. They are therefore excluded for
brevity. For each attribute k, we use a model that
factorizes the joint distribution as product of two
distributions that separately characterize text fea-
tures and network information as follows:

Ψ(zki,e, Xi, F
k
i : Θ) ∝

Ψtext(zki,e, Xi)ΨNeigh(z
k
i,e, F

k
i )

(2)

Text Factor We use Ψtext(zke , Xi) to capture the
text related features which offer attribute clues:

Ψtext(zke , , Xi) = exp[(Θ
k
text)

T · ψtext(zki,e, Xi)]
(3)

The feature vector ψtext(zki,e, Xi) encodes the fol-
lowing standard general features:
• Entity-level: whether begins with capital let-

ter, length of entity.
• Token-level: for each token t ∈ e, word iden-

tity, word shape, part of speech tags, name
entity tags.
• Conjunctive features for a window of k

(k=1,2) words and part of speech tags.
• Tweet-level: All tokens in the correspondent

tweet.
In addition to general features, we employ

attribute-specific features, such as whether the en-
tity matches a bag of words observed in the list
of universities, colleges and high schools for Edu-
cation attribute, whether it matches terms in a list
of companies for Job attribute12. Lists of universi-
ties and companies are taken from knowledge base
NELL13.

Neighbor Factor For Job and Education, we
bias friends to have a larger possibility to share
the same attribute. ΨNeigh(zki,e, F

k
i ) captures such

influence from friends within the network:

ΨNeigh(zki,e, F
k
i ) =

∏
j∈F ki

ΦNeigh(zke , Xj)

ΦNeigh(zki,e, Xj)

= exp[(ΘkNeigh)
T · ψNeigh(zki,e, Xj)]

(4)
Features we explore include the whether entity e
is also the correspondent attribute with neighbor
user j, i.e., I(zej,k = 0) and I(z

e
j,k = 1).

12Freebase is employed for alias recognition.
13http://rtw.ml.cmu.edu/rtw/kbbrowser/

Input: Tweet Collection {Xi}, Neighbor set
{F ki }
Initialization:
• for each user i:

for each candidate entity e ∈ Xi
zki,e = argmaxz′ Ψ(z

′, Xi) from text
features
End Initialization
while not convergence:
• for each user i:

update attribute values for j ∈ F ki
for each candidate entity e ∈ Xi
zki,e = argmaxz′ Ψ(z

′, Xi, F ki )
end while:

Figure 3: Inference for NEIGH-LATENT setting.

For Spouse, we set F spousei = {e} and the
neighbor factor can be rewritten as:

ΨNeigh(zki,e, Xj) = ΨNeigh(Ci, Xe) (5)

It characterizes whether current user Ci to be the
spouse of user e (if e corresponds to a Twitter
user). We expect clues about whether Ci being en-
tity e’s spouse from e’s Twitter corpus will in turn
facilitate the spouse inference procedure of user i.
ψNeigh(Ci, Xe) encodes I(Ci ∈ Se), I(Ci 6∈ Se).
Features we explore also include whether Ci’s
twitter ID appears in e’s corpus.

4.3 Training

We separately trained three classifiers regarding
the three attributes. All variables are observed
during training; we therefore take a feature-based
approach to learning structure prediction models
inspired by structure compilation (Liang et al.,
2008). In our setting, a subset of the features
(those based on network information) are com-
puted based on predictions that will need to be
made at test time, but are observed during train-
ing. This simplified approach to learning avoids
expensive inference; at test time, however, we still
need to jointly predict the best attribute values for
friends as is described in section 4.4.

4.4 Inference

Job and Education Our inference algorithm
for Job/Education is performed on two settings,
depending on whether neighbor information is

169



observed (NEIGH-OBSERVED) or latent (NEIGH-
LATENT). Real world applications, where network
information can be partly retrieved from all types
of social networks, can always falls in between.

Inference in the NEIGH-OBSERVED setting is
trivial; for each entity e ∈ Gi, we simply predict
it’s candidate attribute values using Equ.6.

zki,e = argmax
z′

Ψ(z′, Xi, F ki ) (6)

For NEIGH-LATENT setting, attributes for each
node along the network are treated latent and user
attribute prediction depends on attributes of his
neighbors. The objective function for joint infer-
ence would be difficult to optimize exactly, and
algorithms for doing so would be unlikely to scale
to network of the size we consider. Instead, we use
a sieve-based greedy search approach to inference
(shown in Figure 3) inspired by recent work on
coreference resolution (Raghunathan et al., 2010).
Attributes are initialized using only text features,
maximizing Ψtext(e,Xi), and ignoring network
information. Then for each user we iteratively re-
estimate their profile given both their text features
and network features (computed based on the cur-
rent predictions made for their friends) which pro-
vide additional evidence.

In this way, highly confident predictions will be
made strictly from text in the first round, then the
network can either support or contradict low con-
fidence predictions as more decisions are made.
This process continues until no changes are made
at which point the algorithm terminates. We em-
pirically found it to work well in practice. We ex-
pect that NEIGH-OBSERVED performs better than
NEIGH-LATENT since the former benefits from
gold network information.

Spouse For Spouse inference, if candidate entity
e has no correspondent twitter account, we directly
determine zki,e = argmaxz′ Ψ(z

′, Xi) from text
features. Otherwise, the inference of zki,e depends
on the zke,Ci . Similarly, we initialize z

k
i,e and z

k
e,Ci

by maximizing text factor, as we did for Educa-
tion and Job. Then we iteratively update zk given
by the rest variables until convergence.

5 Experiments

In this Section, we present our experimental re-
sults in detail.

Education Job
AFFINITY 74.3 14.5

Table 3: Affinity values for Education and Job.

5.1 Preprocessing and Experiment Setup
Each tweet posting is tokenized using Twitter NLP
tool introduced by Noah’s Ark14 with # and @
separated following tokens. We assume that at-
tribute values should be either name entities or
terms following @ and #. Name entities are ex-
tracted using Ritter et al.’s NER system (2011).
Consecutive tokens with the same named entity
tag are chunked (Mintz et al., 2009). Part-of-
speech tags are assigned based on Owoputi et al’s
tweet POS system (Owoputi et al., 2013).

Data is divided in halves. The first is used as
training data and the other as testing data.

5.2 Friends with Same Attribute
Our network intuition is that users are much more
likely to be friends with other users who share at-
tributes, when compared to users who have no at-
tributes in common. In order to statistically show
this, we report the value of AFFINITY defined by
Mislove et al (2010), which is used to quantita-
tively evaluate the degree of HOMOPHILY in the
network. AFFINITY is the ratio of the fraction of
links between attribute (k)-sharing users (Sk), rel-
ative to what is expected if attributes are randomly
assigned in the network (Ek).

Sk =

∑
i

∑
j∈F ki I(P

k
i = P

k
j )∑

i

∑
j∈F ki I

Ek =
∑

m T
k
m(T

k
m − 1)

Uk(Uk − 1)

(7)

where T km denotes the number of users with m
value for attribute k and Uk =

∑
m T

k
m. Table 3

shows the affinity value of the Education and Job.
As we can see, the property of HOMOPHILY in-
deed exists among users in the social network with
respect to Education and Job attribute, as signifi-
cant affinity is observed. In particular, the affinity
value for Education is 74.3, implying that users
connected by a link in the network are 74.3 times
more likely affiliated in the same school than as
expected if education attributes are randomly as-
signed. It is interesting to note that Education ex-
hibits a much stronger HOMOPHILY property than

14https://code.google.com/p/
ark-tweet-nlp/downloads/list

170



Job. Such affinity demonstrates that our approach
that tries to take advantage of network information
for attribute prediction of holds promise.

5.3 Evaluation and Discussion

We evaluate settings described in Section 4.2 i.e.,
GLOBAL setting, where user-level attribute is pre-
dicted directly from jointly feature space and LO-
CAL setting where user-level prediction is made
based on tweet-level prediction along with differ-
ent inference approaches described in Section 4.4,
i.e. NEIGH-OBSERVED and NEIGH-LATENT, re-
garding whether neighbor information is observed
or latent.

Baselines We implement the following base-
lines for comparison and use identical processing
techniques for each approach for fairness.

• Only-Text: A simplified version of our algo-
rithm where network/neighbor influence is ig-
nored. Classifier is trained and tested only based
on text features.
• NELL: For Job and Education, candidate is se-

lected as attribute value once it matches bag of
words in the list of universities or companies
borrowed from NELL. For Education, the list is
extended by alias identification based on Free-
base. For Job, we also fetch the name abbrevia-
tions15. NELL is only implemented for Educa-
tion and Job attribute.

For each setting from each approach, we report
the (P)recision, (R)ecall and (F)1-score. For LO-
CAL setting, we report the performance for both
entity-level prediction (Entity) and posting-level
prediction (Tweet). Results for Education, Job and
Spouse from different approaches appear in Table
4, 5 and 6 respectively.

Local or Global For horizontal comparison, we
observe that GLOBAL obtains a higher Precision
score but a lower Recall than LOCAL(ENTITY).
This can be explained by the fact that LOCAL(U)
sets zki,e = 1 once one posting x ∈ Lei is identified
as attribute related, while GLOBAL tend to be more
meticulous by considering the conjunctive feature
space from all postings.

Homophile effect In agreement with our ex-
pectation, NEIGH-OBSERVED performs better than
NEIGH-LATENT since erroneous predictions in

15http://www.abbreviations.com/

NEIGH-LATENT setting will have negative in-
fluence on further prediction during the greedy
search process. Both NEIGH-OBSERVED and
NEIGH-LATENT where network information is
harnessed, perform better than Only-Text, which
the prediction is made independently on user’s text
features. The improvement of NEIGH-OBSERVED
over Only-Text is 22.7% and 6.4% regarding F-
1 score for Education and Job respectively, which
further illustrate the usefulness of making use of
Homophile effect for attribute inference on online
social media. It is also interesting to note the im-
provement much more significant in Education in-
ference than Job inference. This is in accord with
what we find in Section 5.2, where education net-
work exhibits stronger HOMOPHILE property than
Job network, enabling a significant benefit for ed-
ucation inference, but limited for job inference.

Spouse prediction also benefits from neighbor-
ing effect and the improvement is about 12% for
LOCAL(ENTITY) setting. Unlike Education and
Job prediction, for which in NEIGH-OBSERVED
setting all neighboring variables are observed, net-
work variables are hidden during spouse predic-
tion. By considering network information, the
model benefits from evident clues offered by tweet
corpus of user e’s spouse when making prediction
for e, but also suffers when erroneous decision are
made and then used for downstream predictions.

NELL Baseline Notably, NELL achieves high-
est Recall score for Education inference. It is
also worth noting that most of education men-
tions that NELL fails to retrieve are those in-
volve irregular spellings, such as HarvardUniv and
Cornell U, which means Recall score for NELL
baseline would be even higher if these irregular
spellings are recognized in a more sophisticated
system. The reason for such high recall is that as
our ground truths are obtained from Google plus,
the users from which are mostly affiliated with de-
cent schools found in NELL dictionary. However,
the high recall from NELL is sacrificed at preci-
sion, as users can mention school entities in many
of situations, such as paying a visit or reporting
some relevant news. NELL will erroneously clas-
sify these cases as attribute mentions.

NELL does not work out for Job, with a fairly
poor 0.0156 F1 score for LOCAL(ENTITY) and
0.163 for LOCAL(TWEET). Poor precision is ex-
pected for as users can mention firm entity in a
great many of situations. The recall score for

171



GLOBAL LOCAL(ENTITY) LOCAL(TWEET)
P R F P R F P R F

Our approach NEIGH-OBSERVED 0.804 0.515 0.628 0.524 0.780 0.627 0.889 0.729 0.801
NEIGH-LATENT 0.755 0.440 0.556 0.420 0.741 0.536 0.854 0.724 0.783

Only-Text —- 0.735 0.393 0.512 0.345 0.725 0.467 0.809 0.724 0.764
NELL —- —- —- —- 0.170 0.798 0.280 0.616 0.848 0.713

Table 4: Results for Education Prediction

GLOBAL LOCAL(ENTITY) LOCAL(TWEET)
P R F P R F P R F

Our approach NEIGH-OBSERVED 0.643 0.330 0.430 0.374 0.620 0.467 0.891 0.698 0.783
NEIGH-LATENT 0.617 0.320 0.421 0.226 0.544 0.319 0.804 0.572 0.668

Only-Text —- 0.602 0.304 0.404 0.155 0.501 0.237 0.764 0.471 0.583
NELL —- —- —- —- 0.0079 0.509 0.0156 0.094 0.604 0.163

Table 5: Results for Job Prediction

GLOBAL LOCAL(ENTITY) LOCAL(TWEET)
P R F P R F P R F

Our approach —- 0.870 0.560 0.681 0.593 0.857 0.701 0.904 0.782 0.839
Only-Text —- 0.852 0.448 0.587 0.521 0.781 0.625 0.890 0.729 0.801

Table 6: Results for Spouse Prediction

NELL in job inference is also quite low as job
related entities exhibit a greater diversity of men-
tions, many of which are not covered by the NELL
dictionary.

Vertical Comparison: Education, Job and
Spouse Job prediction turned out to be much
more difficult than Education, as shown in Ta-
bles 4 and 5. Explanations are as follows: (1)
Job contains a much greater diversity of mentions
than Education. Education inference can benefit a
lot from the dictionary relevant feature which Job
may not. (2) Education mentions are usually asso-
ciated with clear evidence such as homework, ex-
ams, studies, cafeteria or books, while situations
are much more complicated for job as vocabular-
ies are usually specific for different types of jobs.
(3) The boundary between a user working in and
a fun for a specific operation is usually ambigu-
ous. For example, a Google engineer may con-
stantly update information about outcome prod-
ucts of Google, so does a big fun. If the aforemen-
tioned engineer barely tweets about working con-
ditions or colleagues (which might still be ambigu-
ous), his tweet collection, which contains many of
mentions about outcomes of Google product, will
be significantly similar to tweets published by a
Google fun. Such nuisance can be partly solved
by the consideration of network information, but
not totally.

The relatively high F1 score for spouse predic-
tion is largely caused by the great many of non-

individual related entities in the dataset, the iden-
tification of which would be relatively simpler. A
deeper look at the result shows that the classifier
frequently makes wrong decisions for entities such
as userID and name entities. Significant as some
spouse relevant features are, such as love, hus-
band, child, in most circumstances, spouse men-
tions are extremely hard to recognize. For exam-
ple, in tweets “Check this out, @alancross, it’s
awesome bit.ly/1bnjYHh.” or “Happy Birth-
day @alancross !”. alancross can reasonably be
any option among current user’s friend, colleague,
parents, child or spouse. Repeated mentions add
no confidence. Although we can identify alan-
cross as spouse attribute once it jointly appear
with other strong spouse indicators, they are still
many cases where they never co-appear. How to
integrate more useful side information for spouse
recognition constitutes our future work.

6 Conclusion and Future Work

In this paper, we propose a framework for user at-
tribute inference on Twitter. We construct the pub-
licly available dataset based on distant supervision
and experiment our model on three useful user
profile attributes, i.e., Education, Job and Spouse.
Our model takes advantage of network informa-
tion on social network. We will keep updating the
dataset as more data is collected.

One direction of our future work involves ex-
ploring more general categories of user profile at-

172



tributes, such as interested books, movies, home-
town, religion and so on. Facebook would an
ideal ground truth knowledge base. Another direc-
tion involves incorporating richer feature space for
better inference performance, such as multi-media
sources (i.e. pictures and video).

7 Acknowledgments

A special thanks is owned to Dr. Julian McAuley
and Prof. Jure Leskovec from Stanford University
for the Google+ circle/network crawler, without
which the network analysis would not have been
conducted. This work was supported in part by
DARPA under award FA8750-13-2-0005.

References
Faiyaz Zamal, Wendy Liu, and Derek Ruths. 2012.

Homophily and latent attribute inference: Inferring
latent attributes of twitter users from neighbors. In
ICWSM.

Edward Benson, Aria Haghighi, and Regina Barzilay.
2011. Event discovery in social media feeds. In
Proceedings of the 49th Annual Meeting of the Asso-
ciation for Computational Linguistics: Human Lan-
guage Technologies-Volume 1, pages 389–398. As-
sociation for Computational Linguistics.

Sergey Brin. 1999. Extracting patterns and relations
from the world wide web. In The World Wide Web
and Databases.

Morgane Ciot, Morgan Sonderegger, and Derek Ruths.
2013. Gender inference of twitter users in non-
english contexts. In Proceedings of the 2013 Con-
ference on Empirical Methods in Natural Language
Processing, Seattle, Wash, pages 18–21.

Michael Conover, Jacob Ratkiewicz, Matthew Fran-
cisco, Bruno Gonçalves, Filippo Menczer, and
Alessandro Flammini. 2011. Political polarization
on twitter. In ICWSM.

Mark Craven and Johan Kumlien 1999. Construct-
ing biological knowledge bases by extracting infor-
mation from text sources. In ISMB, volume 1999,
pages 77–86.

Alec Go, Richa Bhayani, and Lei Huang. 2009. Twit-
ter sentiment classification using distant supervision.
CS224N Project Report, Stanford, pages 1–12.

Ido Guy, Naama Zwerdling, Inbal Ronen, David
Carmel, and Erel Uziel. 2010. Social media recom-
mendation based on people and tags. In Proceedings
of the 33rd international ACM SIGIR conference on
Research and development in information retrieval,
pages 194–201. ACM.

Raphael Hoffmann, Congle Zhang, Xiao Ling, Luke S
Zettlemoyer, and Daniel S Weld. 2011. Knowledge-
based weak supervision for information extraction
of overlapping relations. In ACL, pages 541–550.

Jiwei Li and Claire Cardie. 2013. Timeline generation:
Tracking individuals on twitter. Proceedings of the
23rd international conference on World wide web.

Percy Liang, Hal Daumé III, and Dan Klein. 2008.
Structure compilation: trading structure for features.
In Proceedings of the 25th international conference
on Machine learning.

Wendy Liu and Derek Ruths. 2013. Whats in a name?
using first names as features for gender inference in
twitter. In 2013 AAAI Spring Symposium Series.

Wendy Liu, Faiyaz Zamal, and Derek Ruths. 2012.
Using social media to infer gender composition of
commuter populations. In Proceedings of the When
the City Meets the Citizen Workshop, the Interna-
tional Conference on Weblogs and Social Media.

Miller McPherson, Lynn Smith-Lovin, and James M
Cook. 2001. Birds of a feather: Homophily in social
networks. Annual review of sociology, pages 415–
444.

Mike Mintz, Steven Bills, Rion Snow, and Dan Ju-
rafsky. 2009. Distant supervision for relation ex-
traction without labeled data. In Proceedings of the
Joint Conference of the 47th Annual Meeting of the
ACL and the 4th International Joint Conference on
Natural Language Processing of the AFNLP: Vol-
ume 2-Volume 2, pages 1003–1011. Association for
Computational Linguistics.

Alan Mislove, Bimal Viswanath, Krishna Gummadi,
and Peter Druschel. 2010. You are who you know:
inferring user profiles in online social networks. In
Proceedings of the third ACM international confer-
ence on Web search and data mining, pages 251–
260. ACM.

Olutobi Owoputi, Brendan OConnor, Chris Dyer,
Kevin Gimpel, Nathan Schneider, and Noah A
Smith. 2013. Improved part-of-speech tagging for
online conversational text with word clusters. In
Proceedings of NAACL-HLT, pages 380–390.

Marco Pennacchiotti and Ana Popescu. 2011. A ma-
chine learning approach to twitter user classification.
In ICWSM.

Karthik Raghunathan, Heeyoung Lee, Sudarshan Ran-
garajan, Nathanael Chambers, Mihai Surdeanu, Dan
Jurafsky, and Christopher Manning. 2010. A multi-
pass sieve for coreference resolution. In Proceed-
ings of the 2010 Conference on Empirical Methods
in Natural Language Processing.

Delip Rao and David Yarowsky. 2010. Detecting latent
user properties in social media. In Proc. of the NIPS
MLSN Workshop.

173



Delip Rao, David Yarowsky, Abhishek Shreevats, and
Manaswi Gupta. 2010. Classifying latent user at-
tributes in twitter. In Proceedings of the 2nd in-
ternational workshop on Search and mining user-
generated contents, pages 37–44. ACM.

Delip Rao, Michael Paul, Clayton Fink, David
Yarowsky, Timothy Oates, and Glen Coppersmith.
2011. Hierarchical bayesian models for latent at-
tribute detection in social media. In ICWSM.

Haibin Liu, Michael Wall, Karin Verspoor, et al. 2012.
Literature mining of protein-residue associations
with graph rules learned through distant supervision.
Journal of biomedical semantics, 3(Suppl 3):S2.

Alan Ritter, Sam Clark, Mausam, Oren Etzioni, et al.
2011. Named entity recognition in tweets: an ex-
perimental study. In Proceedings of the Conference
on Empirical Methods in Natural Language Pro-
cessing, pages 1524–1534. Association for Compu-
tational Linguistics.

Alan Ritter, Luke Zettlemoyer, Mausam, and Oren Et-
zioni. 2013. Modeling missing data in distant su-
pervision for information extraction.

Mihai Surdeanu, Julie Tibshirani, Ramesh Nallapati,
and Christopher Manning. 2012. Multi-instance
multi-label learning for relation extraction. In Pro-
ceedings of the 2012 Joint Conference on Empirical
Methods in Natural Language Processing and Com-
putational Natural Language Learning, pages 455–
465. Association for Computational Linguistics.

Shingo Takamatsu, Issei Sato, and Hiroshi Nakagawa.
2012. Reducing wrong labels in distant supervi-
sion for relation extraction. In Proceedings of the
50th Annual Meeting of the Association for Compu-
tational Linguistics: Long Papers-Volume 1, pages
721–729. Association for Computational Linguis-
tics.

Fei Wu and Daniel S Weld. 2007. Autonomously se-
mantifying wikipedia. In Proceedings of the six-
teenth ACM conference on Conference on infor-
mation and knowledge management, pages 41–50.
ACM.

Jaewon Yang and Jure Leskovec. 2013. Overlapping
community detection at scale: A nonnegative matrix
factorization approach. In Proceedings of the sixth
ACM international conference on Web search and
data mining, pages 587–596. ACM.

174


