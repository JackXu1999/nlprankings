



















































Extracting Aspect Specific Opinion Expressions


Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 627–637,
Austin, Texas, November 1-5, 2016. c©2016 Association for Computational Linguistics

Extracting Aspect Specific Opinion Expressions

Abhishek Laddha
IIT Delhi ∗

New Delhi, India, 110016
laddhaabhishek11@gmail.com

Arjun Mukherjee
Department of Computer Science,
University of Houston, TX, USA

arjun@cs.uh.edu

Abstract

Opinionated expression extraction is a cen-
tral problem in fine-grained sentiment anal-
ysis. Most existing works focus on either
generic subjective expression or aspect ex-
pression extraction. However, in opinion
mining, it is often desirable to mine the as-
pect specific opinion expressions (or aspect-
sentiment phrases) containing both the as-
pect and the opinion. This paper proposes
a hybrid generative-discriminative framework
for extracting such expressions. The hybrid
model consists of (i) an unsupervised gener-
ative component for modeling the semantic
coherence of terms (words/phrases) based on
their collocations across different documents,
and (ii) a supervised discriminative sequence
modeling component for opinion phrase ex-
traction. Experimental results using Ama-
zon.com reviews demonstrate the effective-
ness of the approach that significantly outper-
forms several state-of-the-art baselines.

1 Introduction

Aspect based sentiment analysis is one of the main
frameworks in opinion mining (Liu and Zhang,
2012). Most of the websites only display the ag-
gregated ratings of products but people are more in-
terested in fine-grained opinions that capture aspect
specific properties in reviews. Therefore, it is desir-
able to have a holistic approach to mine aspect spe-
cific opinion expressions containing both aspect and

∗Research performed during author’s internship at Univer-
sity of Houston

opinion terms within the sentence context as a com-
posite aspect-sentiment phrase (e.g., “had to flash
firmware everyday”, “clear directions in voice” etc.)
and further group them under coherent aspect cate-
gories. Apart from knowing the key issues in prod-
ucts that are often expressed via aspect-sentiment
phrases, they are also useful in applications such
as comparing similar products and summarizing
their important features where it is more conve-
nient to have the aspect-sentiment phrases rather
than generic aspect/sentiment words lacking the nat-
ural aspect opinion correspondence in the right con-
text. They can also be applied to the various tasks
such as sentiment classification, comparative aspect
evaluations, aspect rating prediction, etc.

The thread of research in (Brody and Elhadad,
2010; Titov and McDonald, 2008; Zhao et al., 2010;
Mei et al., 2007; Jo and Oh, 2011) focus on extract-
ing and grouping aspect and opinion words via gen-
erative models but lack the natural aspect opinion
correspondence (e.g., in the manner they appear in
sentences). (Wang et al., 2016; Fei et al., 2016) can
discover aspect specific opinion unigrams but does
not focus on phrases. The thread on fine grained
opinion expressions (Wiebe et al., 2005; Choi et al.,
2006; Breck et al., 2007) focus on subjective expres-
sion extraction which are generic instead of aspect
specific. Formally, the task can be stated as follows:

Given a set of reviews, for each sentence,
s = (w1, . . . wn), with the head aspect
(HA), wHA=i, i ∈ [1, n], discover a sub-sequence
(wp, . . . wq) where p ≤ i ≤ q that best describes
the aspect-sentiment phrase containing the head
aspect. We refer head aspect to the word describing

627



fine-grained property of product. Further, group
these phrases under relevant aspect categories. The
examples below show labeled aspect-sentiment
phrases within [[ ]] with the head aspect (HA)
italicized:

• I’ve been very happy with it so far done a
[[firmware update without a hitch]].
• After less than two years, the [[signal became

spotty]].

In this paper, we propose a novel hybrid model
to solve the problem. We call this Phrase Senti-
ment Model (PSM). PSM is capable of extracting
a myriad of expression types covering: verb phrases
(“screen has poor viewability”), noun, adjective or
adverbial phrase (“recurrent black screen of death”,
“quite stable and fast connection”), implied positive
(“voice activated directions”), implied negative (“re-
quires reboot every few hours”) etc. The hybrid
framework facilitates holistic modeling that caters
for varied expression types (leveraging its discrim-
inative sequence model) and also grouping them un-
der relevant aspect categories with context (exploit-
ing its generative framework). Our approach is also
context and polarity independent facilitating generic
aspect-sentiment phrase extraction in any domain.

Further, we propose a novel sampling scheme
based on Generalized Pólya urn models that opti-
mizes phrasal collocations to improve coherence. To
the best of our knowledge, a hybrid framework has
not been attempted before for opinion phrase ex-
traction. Additionally, the paper produced a labeled
dataset of aspect specific opinion phrases across 4
domains containing more than 5200 sentences coded
with phrase boundaries across both positive and neg-
ative polarities which will be released to serve as a
language resource. Experimental evaluation shows
that our approach outperformed the baselines by a
large margin.

2 Related Work

Subjective expression extraction (Choi et al., 2005)
has traditionally used sequence models (e.g., CRFs).
Various parsing, syntactic, lexical and dictionary
based features (Kim and Hovy, 2006; Jakob and
Gurevych, 2010; Kobayashi et al., 2007) have been
used for subjective expression extraction. In (Yang

and Cardie, 2013; Johansson and Moschitti, 2011)
dependency relations were also used for opinion ex-
pression extraction. Sauper et al., (2011) employs
an HMM over words and model the latent topics as
states in an HMM to discover the product properties
(often aspects) and its associated attributes (pos/neg)
polarities separately. In Yang and Cardie, (2012)
a semi-CRF based approach is used which allow
sequence labeling at segment level and (Yang and
Cardie, 2014) employed semi-CRF for opinion ex-
pression intensity and polarity classification. How-
ever, all the above works focus on generic subjective
expressions as opposed to aspect specific opinion-
sentiment phrases.

In (Choi et al., 2006; Yang and Cardie, 2013)
joint models were proposed for identifying opin-
ion holders and expression, relations among them in
news articles. In (Johansson and Moschitti, 2011)
a re-ranking approach was used on the output of
a sequence model to improve opinion expression
extraction. In (Li et al., 2015; Mukherjee, 2016)
subjective expressions implying a negative opin-
ion were discovered using sequence models and
markov networks; while in (Berend, 2011) super-
vised keyphrase extraction was used for phrase ex-
traction. These works mostly relied on word level
features under the first-order Markov assumption.
Above works are tailored for only expression extrac-
tion and do not group coherent phrases under rele-
vant aspect categories.

Another thread of research involves topic phrase
mining. Wang et al., (2007) proposes a Topical n-
gram model (TNG) that mines phrases based on sta-
tistical collocation. Lindsey et al., (2012) employ
hierarchical Pitman-Yor process to model phrases.
In Fei et al., (2014), Generalized Pólya urn model
(LDA-P-GPU) was used to group the candidate noun
phrases. In (El-Kishky et al., 2014; Liu et al., 2015),
frequency based information were used for mining
phrases that are good for generic phrases but can-
not model relevant yet longer phrases due to their
infrequency. Thus, they are unable to capture long
phrases containing both aspect and opinion. The
models TNG and LDA-P-GPU are closest to our
task as they can discover relevant aspect expressions
that can contain opinions and are considered as base-
lines.

Next there are works that generate phrasal

628



Figure 1: Plate Notation of PSM

datasets. In SemEval 2015, Aspect based Sentiment
Analysis Task (Pontiki et al., 2015), a dataset was
produced that had annotations for aspect phrases.
The focus was on aspect phrases as opposed to
aspect-sentiment phrases. The MPQA 2.0 corpus
(Wiebe et al., 2005) has some labeled opinion ex-
pressions, but they are generic subjective expres-
sions as opposed to aspect-sentiment phrases (see
Section 1) we find in reviews.

Zhao et al., (2011) extracts topical phrase in
tweets using relevance and interestingness. Wu et
al., (2009) proposed a phrase dependency parsing
approach to extract product feature (aspect expres-
sion) and opinion expression and the relation be-
tween them. They considered all noun and verb
phrases (NPs, VPs) as product features and its sur-
rounding dictionary opinion words as opinions. Fea-
tures were constructed using phrase dependency tree
to extract relation among all product features and
opinions that were later used in aspect and opinion
expression extraction. Although, Wu et al., (2009)
doesn’t discover aspect specific opinion phrases, its
use of NPs in extracting candidate opinion phrases
is similar to Fei et al., (2014) which is considered as
a baseline.

3 Phrase Sentiment Model (PSM)

PSM is a hybrid between generative and discrimina-
tive modeling that combines the best of both worlds.
Its generative modeling lays the foundation for emis-
sion of aspects and aspect specific opinion phrases in
documents, while its discriminative sequence mod-
eling component (via an embedded CRF) facilitates
aspect specific opinion phrase extraction.

As noted in Titov and McDonald (2008), model-
ing entire reviews as documents tend to correspond

to the global properties of a product (e.g., brand,
name, etc.) resulting in rather overlapping aspects.
To avoid this, we perform sentence level model-
ing that helps improve aspect sharpness. A review
sentence sd of N words, is denoted as wd,s,j =
{wd,s,1, wd,s,2 . . . wd,s,N} where each wd,s,j is one
of the V words in the vocabulary. There can be ex-
ponential number of phrase sequence possible for
each sentence sd i.e. 〈wd,s,j〉j=st+len−1j=st of arbi-
trary length len(len = 1 for words; len > 1 for
phrases) starting at an index st ∈ [0, |sd|]. We
observed that most of opinion expression are cen-
tered around the head aspect thereby causing the
space of potential opinion expressions to be quite
sparse. We took advantage of following observa-
tion and trained a sequence model (e.g., CRF) for
phrase sequence tagging as described in the next
subsection. We generated M = 5 best sequence
labelings of each sentence sd(sm∈1...Md ) via for-
ward Viterbi and backward A∗ search1. Hence, our
vocabulary is the union of unigrams and n-grams
discovered by CRF over M best sequence label-
ing, i.e., the model’s vocabulary, V = {wd,s,j} ∪
{〈wd,smd ,j〉

j=st+len−1
j=st } ∀d, s, j, len, st,m.

In PSM, for each aspect a, we model its aspect
specific terms (words/phrases) distributions and as-
pect background word distributions using multino-
mials ϕAa and ϕ

B
a , drawn from Dir(β) over the vo-

cabulary v1...V . For each domain d, we first draw a
domain specific aspect distribution θd ∼ Dir(α).
Next, for each review sentence (document), sd of
a domain, d we draw an aspect, zd,s ∼ Mult(θd).
We assume that each sentence evaluates one as-
pect which mostly holds true in the review do-
main. We associate a latent switch variable for
each word rd,s,j and each phrase 〈rd,smd ,j〉

j=st+len−1
j=st

of vocabulary where each switch variable r ∈
{0, 1}. To generate each term 〈wd,smd ,j〉

j=st+len−1
j=st

of the labeled sequence sm∈{1...M}d , we first set
the switch variables for the sentence, sd via the
discriminative CRF model, i.e. 〈rd,smd ,j〉

j=|sd|
j=1 ←

1
Z exp

(∑
j

∑
k λkfk (rj−1, rj , wj)

)
by fitting a

previously trained CRF model. The switch vari-
ables, r ∈ {1, 0} for a particular tagging smd of

1Value of M was tuned via pilot experiments using the
CRF++ toolkit (Kudo, 2009)

629



sentence sd span over all its words and take val-
ues r = 1 for words being part of an aspect spe-
cific opinion phrase or r = 0 for aspect background
words, upon observing all words in sd. Finally, de-
pending upon the aspect, zd,s and the switch vari-
able rd,s,j , we emit (unigram) terms in the sentence
as follows:

wd,s,j ∼
{
Mult(ϕAzd,s) if rd,s,j = 1

Mult(ϕBzd,s) if rd,s,j = 0
(1)

and for phrasal terms (i.e., when
〈rd,smd ,j〉

j=st+len−1
j=1 = 1∀ valid st and len),

we emit〈wd,smd ,j〉
j=st+len−1
j=st ∼Mult(ϕAzd,s).

3.1 Inference

We employ MCMC Gibbs sampling for posterior in-
ference. As latent variables z and r belong to differ-
ent levels, we hierarchically sample z and then r for
each sweep of a Gibbs iteration as follows:
p(zd,s = a|Z¬d,s, R¬d,s,W¬d,s) ∝

(nsd,a)¬d,s+α
(ns
d,(·))¬d,s+Aα

×
[(
∏V
v=1

Γ(nAa,v+β)

Γ((nAa,v)¬d,s+β)

)
/

(
Γ(nA

a,(·)+V β)

Γ((nA
a,(·))¬d,s+V β)

)]
×

[(
∏V
v=1

Γ(nBa,v+β)

Γ((nBa,v)¬d,s+β)

)
/

(
Γ(nB

a,(·)+V β)

Γ((nB
a,(·))¬d,s+V β)

)]
(2)

Samplers for r consist of three cases: (i) individual
aspect-specific opinion words, (ii) individual back-
ground words, and (iii) phrasal opinion:
p(rd,s,j = 1|zd,s = a,wd,s,j = v, . . .) ∝

(nAa,v)¬d,s,j+β)
(nA
a,(·))¬d,s,j+V β)

× pCRF (rd,s,j−1, rd,s,j = 1|v) (3)
p(rd,s,j = 0|zd,s = a,wd,s,j = v, . . .) ∝

(nBa,v)¬d,s,j+β)
(nB
a,(·))¬d,s,j+V β)

× pCRF (rd,s,j−1, rd,s,j = 0|v) (4)
p(〈rd,s,j = 1〉j=st+len−1j=st |zd,s = a, 〈wd,s,j = v

′〉j=st+len−1j=st , . . .) ∝
(nAa,v)¬d,s,j+β)

(nA
a,(·))¬d,s,j+V β)

× pCRF (rd,s,j=st−1, rd,s,j=st = 1,

rd,s,j=st+1 = 1, . . . rd,s,j=st+len−1 = 1|v
′
) (5)

where nsd,a denotes the # of sentences in domain d
assigned to aspect a, nAa,v, n

B
a,v denote the # of times

term v was assigned to aspect a in the aspect specific
opinion, and aspect specific background language
models respectively. A count variable with subscript
(·) signifies the marginalized sum over the latter in-
dex and ¬ denotes the discounted counts. The sam-
pler in (5) computes the likelihood of a sequence
of contiguous terms, 〈rd,s,j = 1〉j=st+len−1j=st form-
ing an aspect (zd,s = a) specific opinion phrase, v

′

starting at index j = st and of length len, where
v
′

= 〈wd,s,j〉j=st+len−1j=st . The sequence probabili-
ties, pCRF in equations (3, 4, 5) can be obtained as
follows.

Let w = (wt=1 . . . wt=T ) denote the sequence of
observed words in a sentence, and let each obser-
vation wt have a label yt ∈ Y indicating whether
wt is part of a aspect specific opinion phrase, where
Y = {1, 0}. We consider a first order Markov
linear-chain CRF in our hybrid model. We define
the Markovian transition and forward-backward
variables of our embedded CRF as follows:
ψt(j, i, w) = p(yt = j|yt−1 = i)p(wt = w|yt = j) (6)
αt(j) =

∑
i∈Y ψt(j, i, wt)αt−1(i) (7)

βt(j) =
∑

j∈Y ψt+1(j, i, wt+1)βt+1(j) (8)
where α1(j) = ψ1(j, y0, w1) and βT (i) = 1.
This lays the foundation for expressing the sequence
probabilities, pCRF in closed form as follows:
pCRF (yt−1, yt|w) ∝ αt−1(yt−1)ψt(yt, yt−1, wt)βt(yt) (9)
pCRF (yt−2, yt−1, yt|w) ∝

αt−2(yt−2)ψt−1(yt−1, yt−2, wt−1)ψt(yt, yt−1, wt)βt(yt) (10)
Eq. (9) is used for computing the sequence proba-
bilities for individual opinion/background words for
samplers in eq. (3, 4) while eq. (10) and its exten-
sions are used for computing the sequence probabil-
ities in the phrase samplers in eq. (5). The values
for ψt, αt, βt are obtained from a previously trained
CRF model upon fitting to the current sentence sd
for which sampling is being performed.

3.2 Embedded CRF Training

We employ linear-chain CRFs (Lafferty et al., 2001)
for modeling phrases. While word (W) and Part-Of-
Speech (POS) tag features are effective in various
sequence modeling tasks (Yang and Cardie, 2014;
Yang and Cardie, 2012), in our problem context,
(W+POS) features are insufficient as they do not
consider the head aspect (HA) and its relevant posi-
tional/contextual features, i.e., how do different POS
tags, syntactic units (chunks), polar sentiments ap-
pear in proximity to the head aspect? Hence, center-
ing on the HA, we propose a set of pivot features to
model context.
Pivot Features: We consider five feature families:
POS Tags (T): DT, IN, JJ, MD, NN, RB, VB, etc.
Phrase Chunks (C): ADJP, ADVP, NP, PP, VP, etc.
Prefixes (P): anti, in, mis, non, pre, sub, un, etc.

630



Category Feature Template Example of feature apperaing in a sentence
1st order features
Wi+j ;−4 ≤ j ≤ 4
W ∈ {T,C, P, S, SP}

SPi+j SPi−1 = NEG; previous term of HA is of NEG
polarity,. . . have this terrible voice on the . . .

Si+j Si−2 = ing; suffix of 2nd previous term of head aspect
is “ing”, . . . kept dropping the signal . . .

. . . . . .
2nd order features
Wi+j , Yi+j ;−4 ≤ j ≤ 4
W,Y ∈ {T,C, P, S, SP}

Ti+j , Ti+j′ Ti−2 = JJ, Ti−1 = V BZ, · · · frequently drops
connection. . .

Ti+j , Ci+j′ Ti+2 = RB,Ci+3 = ADJP ; . . . screen clarity is
good. . .

. . . . . .
3rd order features
Wi+j , Yi+j , Zi+j ;−4 ≤
j ≤ 4
W,Y,Z ∈ {T,C, P, S, SP}

Ti+j , Si+j′ , Ti+j′′ Ti+2 = JJ, Pi+4 = un, Ti+4 = JJ ; . . . screen is blank
and unresponsive. . .

. . . . . .

Table 1: Pivot Templates: Subscript i denotes the index of the head aspect, HA (italicized). Subscript j denotes the
index relative to i

Suffixes (S): able, est, ful, ic, ing, ive, ness etc.
Word Sentiment Polarity (SP): POS, NEG, NEU

Pivoting on the head aspect, we look forward and
backward to generate a family of binary features de-
fined by a specific template (see Table 1). Each tem-
plate generates several features that capture various
positional context around the HA. Additionally, we
consider up to 3rd order pivot features allowing us to
model various dependencies as features. For polar-
ity, we used the opinion lexicon2 derived from (Hu
and Liu, 2004).
Feature Templates: Table 1 details the templates
for features pivoting on the head aspect. Various fea-
tures from these templates coupled with the value
of the current sequence tag at yt or a combination
of current and previous labels yt, yt−1 serve as our
linear chain features (LCF), f(yi−1, yi, w). Further,
the index i for LCF can refer to any word in the sen-
tence and not necessarily the head aspect, yielding
us a very rich feature space.
Learning the CRF λs: Given a set of training ex-
amples {wi, yt} where yt are the correct sequence
tags, we estimate the CRF Λ = {λk}parameters by
minimizing the negative log-likelihood (NLL),
Λ = argminΛ(C

∑
i log(p(yt|wi,Λ)) +

∑
k λ

2
k) (11)

Where p(yt|w,Λ) ∝ exp(
∑

k

∑
t λkfk(yt−1, yt, w)), C is

the soft-margin parameter, and the term
∑

k λ
2
k indi-

cates L2 regularization on the feature weights, λk.

2http://www.cs.uic.edu/∼liub/FBS/opinion-lexicon-
English.rar

The training set for learning the embedded CRF
models λs is detailed in Table 3 (col 1, 2).

4 Optimizing Phrasal Collocations

Topic models can be described in terms of a simple
Pólya urn (SPU) sampling schemes in the sense that
when a particular term (word or phrase) is drawn
from a topic, count of that term is incremented in
that topic. This enforces the topic distribution to
tend towards these terms over time as frequency of
them increases. Therefore, the posterior of gen-
erative topic models often favors terms with high
frequency e.g., unigrams, while phrasal terms are
ranked lower due to their lower frequencies. This
is undesirable for phrase extraction.

In contrast, Generalized Pőlya urn (GPU) model
differs from SPU in its sampling process. When
a certain term is drawn, the count of that term in-
creases as well as it also increases the count of terms
which are similar to drawn word/phrases via pseudo-
counts for promotion. Thus, GPU caters for promo-
tion of others terms in a principled manner. It has
been previously used for unigram topic modeling in
(Mimno et al., 2011). In this work we leverage it for
phrases.

4.1 Proposed PSM-GPU model

We optimize the collocations of relevant aspect
words and phrases in the GPU framework in two
ways:

631



Word to phrase: Intuitively, if an aspect word is as-
signed to a topic then that topic should represent that
aspect and to all other phrases in that aspect’s phrase
set (i.e., phrases containing that aspect) should be-
long to the same topic. Thus, when an aspect word
is assigned to a topic then each phrase in its aspect
set is promoted with a small count in that topic.
Phrase to word: When a phrase 〈wd,s,j〉j=st+len−1j=st
is assigned to a topic, each component word wd,s,j
where j ∈ [st, st+len−1] in it is also promoted with
a certain small count, i.e., each word of that phrase
is also assigned to that topic by a certain amount.

We now define the term promotion matrix, A for
the GPU framework. Every element of A,Aw,w′
refers to the promotion pseudocount, i.e., whenever
a w was seen in an urn, we increment the count by
Aw,w′ of w

′
. w,w

′
can be word or phrases.

Aw,w′ =





1 if w = w
′

σ if w is an aspect word,

w
′
is a phrase ∈ Phrase set of w

δ w
′
is a word ∈ Phrase w

0 otherwise

(12)

To improve the ranking of phrases, the value of σ
is kept greater than δ. Empirically values are given
section 5.1.

4.2 PSM-GPU Inference

Accounting the GPU process above, the approxi-
mate Gibbs samplers for z and r take the following
form:

p(zd,s = a|Z¬d,s, R¬d,s,W¬d,s) ∝
[

(nsd,a)¬d,s+α)
(ns
d,(·))¬d,s+Aα)

]
×


∏V

v=1

Γ(
∑V
w
′
=1

(A
v,w
′ ∗nA

a,w
′ )+β)

Γ(
∑V
w
′
=1

(A
v,w
′ ∗nA

a,w
′ )¬d,s+β)







Γ(
∑V
v=1

∑V
w
′
=1

(A
v,w
′ ∗nA

a,w
′ )+V β)

Γ(
∑V
v=1

∑V
w
′
=1

(A
v,w
′ ∗nA

a,w
′ )¬d,s+V β)



×

[(∏V
v=1

Γ(nBa,v+β)

Γ((nBa,v)¬d,s+β)

)
/

(
Γ(nB

a,(·)+V β)

Γ((nB
a,(·))¬d,s+V β)

)]
(13)

p(rd,s,j = 1|zd,s = a,wd,s,j = v, . . .) ∝
∑V
w
′
=1

(A
v,w
′ ∗nA

a,w
′ )¬d,s,j+β)

∑V
v=1

∑V
w
′
=1

(A
v,w
′ ∗nA

a,w
′ )¬d,s,j+V β

×

pCRF (rd,s,j−1, rd,s,j = 1|v) (14)
Similarly, phrasal opinion switch variable can be de-
rived. The sampler for individual background words
remains unchanged.

5 Experimental Evaluation

In this section, we evaluate our proposed models.
We first detail our dataset, followed by baselines and
results.

5.1 Dataset and Parameter Settings

Domain
Pos. LabeledNeg. Labeled

Positive Negative Total
Phrase Phrase

Router 414 1256 1937 5291 7228
GPS 948 672 2473 2231 4704

Mouse 376 477 1421 2591 4012
Keyboard 398 660 912 1539 2451

Table 3: Statistics of dataset of four domain

Dataset Statistics: For CRF training, we created
a phrase labeled dataset of aspect opinion phrases
using product reviews from Amazon across 4 do-
mains each spanning 4 head aspects. In this work,
head aspects for a domain are known a priori ei-
ther directly using unsupervised topic induction or
guided by domain knowledge (e.g. using aspect
models such as (Zhao et al., 2010; Chen et al., 2013;
Mukherjee and Liu, 2012)). Our focus is on phrase
extraction and grouping. We labeled the positive and
negative opinion phrases spans (Table 3; col 2, 3)
in the reviews following the annotation schemes in
(Wilson et al., 2005) for embedded CRF training in
PSM. Table 3 details our labeled data for CRF train-
ing. This phrase boundary labeled dataset (Table 3;
col 2, 3) is “orthogonal” or disjoint from the data
where the PSM model was fit and evaluated (Table
3, col 4, 5). This avoids overfitting and makes a fair
case for all the experiments of PSM.
Preprocessing and Parameter Setting: We re-
moved the stopwords, punctuation, special charac-
ters and words appearing less than 5 times in each
domain. For all models, posterior estimates of la-
tent variables were taken with a sampling lag of 50
iterations post burn-in phase (of 200 iterations) with
2,000 iterations in total. Dirichlet priors were set to
α = 50/K, where K is the number of topics (em-
pirically set to 10 via pilot) and β = 0.1. The CRF
parameters C = 1 and GPU parameters σ = 0.05
and δ = 0.01 were estimated using cross validation.

5.2 Baselines
We consider the following relevant phrase extraction
models as our baselines:

632



PSM-GPU sMC-GPU LDA-P-GPU
Router→Connection:“updating
firmware secure connection”,
“dropping connection”, “con-
nection excellent”, “instability
wireless connection”, “crashes
entire connection”, “updating”,
“kills current connection in-
cluding downloads”, “affected
plugged connection”,“cable radio
frequency connection”, “halfway”

Router→Connection:“connection
big time”, “signal weak time
connection”, “connection time sta-
ble”, “lose connection”, “internet
connection speed dropped”, “stop
working lot connection”, “started
dropping internet connection”,
“started dropping connection”,
“drop wireless connection drops
wired connection”, “connection
dropping problems”

Router→Connection:“drops”,
“times day internet connection”,
“internet connection multiple
times”, “internet connection
times”, “internet connection
minutes”, “dropping internet
connection”, “broadband internet
connection”, “extremely slow
internet connection”, “lost internet
connection”,“internet connection
couple”

GPS→Screen:“poor screen con-
trast daylight”, “direction screen
missing poor screen contrast”,
“slow screen size makes useless”,
“screen turned”, “turn”, “nothing
screen”, “night screen bit bright”,
“screen unpredictable directions”,
“lacks faster screen refresh rate”,
“smoother screen refresh”

GPS→Screen:“bright”, “excellent
nice touch screen”, “touch screen
n’t”, “nice big touch screen”,
”touch screen big size”, “smaller”,
“touch screen but nice”, “screen
accurate spoken direction”, “touch
screen nice”, “nice slim touch
screen”

GPS→Screen:“smaller screen but
dont”, “wide screen”, “nothing but
screen”, “ok but screen”, “screen
but normal”, “screen size”, “large
screen”, “better traffic features cons
touch screen”, “screen real estate
than”, “than years screen”

Table 2: Example aspect specific opinion phrases (comma delimited in order) discovered by PSM-GPU, sMC-GPU,
LDA-P-GPU. Errors are italicized and marked in red.

LDA with phrases (LDA-P): As aspect-sentiment
phrases are often noun phrases, a basic approach
is to include the noun phrases (extracted using a
parser) as separate terms in the corpus.
Topical N-gram (TNG): The TNG model in (Wang
et al., 2007) extends LDA to model n-grams of arbi-
trary length. As aspects often appear close to their
opinion in the sentence, topical n-grams for each as-
pect form a natural baseline. We used the authors
original implementation in the MALLET toolbox.
LDA-P with GPU (LDA-P-GPU): This model is
due to (Fei et al., 2014) and is tailored for phrase
extraction in opinion mining. It employs LDA with
noun phrases in the GPU framework to rank the as-
pect phrases higher in their topics. Our implemen-
tations of LDA-P and LDA-P-GPU use the noun
phrases discovered by the Stanford Parser.
semi-Markov CRF with GPU(sMC-GPU): This
model builds over the model of (Yang and Cardie,
2012) that used dependency tree features and semi-
CRF to model the arbitrarily long expressions. We
used these expression spans as multiword in vocab-
ulary. Then we employ GPU based sampling with
LDA proposed by (Fei et al., 2014) to collocate opin-
ion expressions.

5.3 Qualitative Analysis

To assess the quality of extracted expressions, we la-
beled the topics following instructions in (Mimno et
al., 2011). First, each topic was labeled as coher-
ent or incoherent and an aspect name was given if
the topic was coherent. Each topic was presented as
a list of top 45 terms in descending order of their
probabilities under that topic. A topic was consid-
ered coherent if the terms in the topic were semanti-
cally related to each other.

Next, for coherent topics, their terms were labeled
as correct (if the terms semantics was relevant to the
topic) or incorrect (otherwise). Two human judges
were used in the annotation. Agreements being high
(κ > 0.78), disagreements were resolved upon con-
sensus among judges.

Table 2 reports the top 10 terms(words/phrases)
for aspect ’connection’ (Router domain) and aspect
’screen’(GPS domain) across PSM-GPU, sMC-GPU
and LDA-P-GPU (the two closest competitor). We
note that PSM-GPUs phrases are more expressive
compared to sMC-GPU because sMC-GPU is prone
to have longer phrases due to segment features but
PSM’s switch variable captured more relevant aspect
specific opinion expressions. sMC-GPU has better

633



Figure 2: Charts from left to right are Topical words Precision@15, Precision@30, Precision@45 of coherent topics
of each model and last one is number of coherent topics of each model.

Domain PSM-GPU PSM sMC-GPU LDA-P-GPU TNG LDA-PP R F Ac. P R F Ac. P R F Ac. P R F Ac. P R F Ac. P R F Ac.
Router 71.867.869.768.8 69.067.368.167.7 69.667.768.668.2 65.267.966.567.1 65.266.866.065.9 65.568.066.767.4
GPS 69.369.269.269.3 66.767.767.267.4 65.368.065.765.5 63.668.465.966.8 64.168.366.158.8 66.069.767.868.6

Mouse 87.481.484.087.3 82.980.481.382.4 83.481.582.682.3 81.378.379.480.9 82.977.980.082.5 77.977.377.177.7
Keyboard 92.572.981.284.1 90.270.178.481.6 88.470.377.681.0 86.769.376.479.9 88.866.675.879.1 83.663.471.475.0

Avg. 80.372.876.077.4 77.271.473.874.8 76.771.973.674.3 74.271.072.173.7 75.369.972.071.6 73.369.670.872.2

Table 4: Sentiment classification: Precision, Recall, F1 and accuracy from top to down for each domain and each
model

phrases compared to LDA-P-GPU because the lat-
ter only considers noun phrases which may not al-
ways be semantically coherent under an aspect. The
qualitative results of other baselines TNG and LDA-
P were worse than that of LDA-P-GPU and hence
omitted due to space constraints. However, the sub-
sequent experiments compare all models quantita-
tively.

5.4 Quantitative Analysis

We consider the following metrics and tasks:
Average Precision: Figure 2 shows the average
Precision@n (p@n) for n = 15, 30, 45 of all coher-
ent topics for each model in each domain. We note
that PSM-GPU achieves the highest precision for all
domains significantly (p < 0.01) outperforming its
closest competitor sMC-GPU. sMC-GPU tends to
discover longer phrases due to segment features in
semi-CRF and combined with GPU gains the max-
imum strength among other baselines. Next in or-
der are LDA-P-GPU, PSM, and TNG. We have not
shown the result of LDA-P as its top terms didn’t
contain enough phrases and its precision scores were
quite lower compared to other models. But it is
worthwhile to note that PSM outperforms LDA-P-
GPU (2nd best competitor) at lower ranks which is
more important (e.g., in majority domains for p@15)

and shows its effectiveness. It is a bit unfair to
compare PSM with sMC-GPU because PSM is lack-
ing phrase rank optimization whereas sMC-GPU en-
forces it, and the p@n metric uses rank position as
its goodness criterion. However, we will see that in
an actual application task, both PSM and PSM-GPU
does better than sMC-GPU. Also, we observed that
p@45 is higher compare to p@15 or p@30. The
reason is even though we are promoting the phrases
using GPU it is not able to remove some aspect opin-
ion words from top 15 terms due to their high occur-
rence in phrases. For e.g. Table 2 has opinion words
like “updating”, “turn” which are considered incor-
rect because of non-phrasal terms.
# of coherent topics: Figure 2 (rightmost chart)
shows the number of coherent topics produced by
each model. A model that can discover more co-
herent topics is better. We find that PSM-GPU can
discover more coherent topics with phrases than its
baselines across all domains. The trends of other
models are similar to p@n and can be analogously
explained.
We note that the Topic Coherence (TC) metric in
(Mimno et al., 2011) which is often used to approx-
imate coherence in unigram topic models as it cor-
relates with human notions of coherence, uses co-
document frequency of individual words in topics.

634



However, in our problem as phrases are sparse, their
co-document frequency is far lower than words.
Hence, the TC metric is not directly applicable. Our
measure of coherence is based on human judgment
(achieves high agreements, κ > 0.78 see Section
5.3) and from Table 2 we can see the discovered
phrases do reflect coherence. Hence, to evaluate the
phrases quantitatively, we employ an actual senti-
ment classification task that uses the posterior of our
models (top phrases) as features. This is reasonable
because the estimated topics (when used as features)
improve sentiment classification, it shows that they
are meaningful and capable of capturing latent sen-
timent that govern polarities.
Sentiment Classification: For this task, instead of
using all the words as features, we used the poste-
rior on ϕA (top 50 terms of ϕA) as features. For all
models, all possible n-grams of top 50 terms are also
considered as features. We trained SVMs3 (using
the SVMLight toolkit) with the features described
above. Evaluation for this task employed 5-fold
cross validation on the data in Table 3 (col 4, 5). For
each test fold, the features were induced upon fitting
the aspect extraction models on the training data of
that fold. From Table 4 we note that both PSM-GPU
and PSM outperform all competitors on average F1
across all domains. More specifically, we note that
PSM alone that uses no rank optimization performs
better than sMC-GPU employing phrasal rank op-
timization under GPU scheme. We believe this is
due to PSM’s switching component that can dis-
cover correct aspect/sentiment terms (sufficient for
polarity classification) and rank it higher based on
frequency even though the expressive aspect specific
phrases remain ranked lower. sMC-GPU tends to
have longer phrases so it does well, however, under
GPU, longer phrases may not be promoted well as
they lack anchor aspect terms under a relevant topic.
LDA-P-GPU uses standard (Noun Phrases) NPs for
phrases with rank optimization and hence is the next
in performance order as NPs may not capture opin-
ion well. TNG does not perform as well as it relies
on multiword collocation as opposed to NP/VP for
phrase extraction. LDA-P’s performance is lowest
as it cannot rank the relevant NPs high. PSM-GPU

3Using an RBF kernel (C = 10, g = 0.01) which per-
formed best upon tuning various SVM parameters via cross val-
idation.

has the right balance of phrase boundary span and
phrasal rank optimization via GPU that makes it sig-
nificantly outperform (p < 0.01) all competitors.

Domain Precision Recall F1 Acc.
Router 69.68 67.3 68.4 67.9
GPS 66.47 68.6 67.5 68.00

Mouse 87.24 80.5 83.4 86.9
Keyboard 85.45 70.9 77.1 81.1

Table 5: Domain ablation result on polarity classification

Sequence Model Sensitivity: To assess the robust-
ness of the hybrid framework, we evaluate the sensi-
tivity of the embedded CRF model via domain abla-
tion. We choose the best performer PSM-GPU and
ablate each domain during its CRF training. We re-
peat the previous experiment on sentiment classifi-
cation using the ablated model. From the results in
Table 5, we note that the reduction in precision is
relatively more than that of recall. However, the F1
score does not drop significantly (compared to Table
4) for any domain showing the robustness of the hy-
brid framework. We note that even with some skew-
ness in the labeled data (Table 3), CRF is not over-
fitting here and the proposed pivot features (Table
1) are powerful enough to learn the phrasal structure
across domain.

6 Conclusion

This paper presented a novel hybrid framework for
aspect specific opinion expression extraction. Two
models PSM and PSM-GPU were proposed that
employ CRF discriminative sequence modeling for
phrase boundary extraction and generative modeling
for grouping relevant terms under a topic. PSM-
GPU further optimized the aspect coherence using
the generalized Pólya urn sampling scheme. Ex-
perimental results showed that the proposed hybrid
models can extract more coherent aspect specific
opinion expressions significantly outperforming all
competitors across all domains and are robust in
cross-domain knowledge transfer.

Acknowledgment

This work was supported in part by NSF 1527364.

635



References
Gábor Berend. 2011. Opinion expression mining by

exploiting keyphrase extraction. In IJCNLP, pages
1162–1170. Citeseer.

Eric Breck, Yejin Choi, and Claire Cardie. 2007. Iden-
tifying expressions of opinion in context. In IJCAI,
volume 7, pages 2683–2688.

Samuel Brody and Noemie Elhadad. 2010. An unsu-
pervised aspect-sentiment model for online reviews.
In Human Language Technologies: The 2010 Annual
Conference of the North American Chapter of the As-
sociation for Computational Linguistics, pages 804–
812. Association for Computational Linguistics.

Zhiyuan Chen, Arjun Mukherjee, Bing Liu, Meichun
Hsu, Malu Castellanos, and Riddhiman Ghosh. 2013.
Exploiting domain knowledge in aspect extraction. In
EMNLP, pages 1655–1667.

Yejin Choi, Claire Cardie, Ellen Riloff, and Siddharth
Patwardhan. 2005. Identifying sources of opinions
with conditional random fields and extraction patterns.
In Proceedings of the conference on Human Language
Technology and Empirical Methods in Natural Lan-
guage Processing, pages 355–362. Association for
Computational Linguistics.

Yejin Choi, Eric Breck, and Claire Cardie. 2006. Joint
extraction of entities and relations for opinion recog-
nition. In Proceedings of the 2006 Conference on
Empirical Methods in Natural Language Processing,
pages 431–439. Association for Computational Lin-
guistics.

Ahmed El-Kishky, Yanglei Song, Chi Wang, Clare R
Voss, and Jiawei Han. 2014. Scalable topical phrase
mining from text corpora. Proceedings of the VLDB
Endowment, 8(3):305–316.

Geli Fei, Zhiyuan Chen, and Bing Liu. 2014. Re-
view topic discovery with phrases using the pólya urn
model. In COLING, pages 667–676.

Geli Fei, Zhiyuan Brett Chen, Arjun Mukherjee, and
Bing Liu. 2016. Discovering correspondence of senti-
ment words and aspects. In In proceedings of the 17th
International Conference on Intelligent Text Process-
ing and Computational Linguistics.

Minqing Hu and Bing Liu. 2004. Mining and summa-
rizing customer reviews. In Proceedings of the tenth
ACM SIGKDD international conference on Knowl-
edge discovery and data mining, pages 168–177.
ACM.

Niklas Jakob and Iryna Gurevych. 2010. Extracting
opinion targets in a single-and cross-domain setting
with conditional random fields. In Proceedings of the
2010 conference on empirical methods in natural lan-
guage processing, pages 1035–1045. Association for
Computational Linguistics.

Yohan Jo and Alice H Oh. 2011. Aspect and sentiment
unification model for online review analysis. In Pro-
ceedings of the fourth ACM international conference
on Web search and data mining, pages 815–824. ACM.

Richard Johansson and Alessandro Moschitti. 2011. Ex-
tracting opinion expressions and their polarities: ex-
ploration of pipelines and joint models. In Proceed-
ings of the 49th Annual Meeting of the Association for
Computational Linguistics: Human Language Tech-
nologies: short papers-Volume 2, pages 101–106. As-
sociation for Computational Linguistics.

Soo-Min Kim and Eduard Hovy. 2006. Extracting opin-
ions, opinion holders, and topics expressed in online
news media text. In Proceedings of the Workshop on
Sentiment and Subjectivity in Text, pages 1–8. Associ-
ation for Computational Linguistics.

Nozomi Kobayashi, Kentaro Inui, and Yuji Matsumoto.
2007. Extracting aspect-evaluation and aspect-of re-
lations in opinion mining. In EMNLP-CoNLL, vol-
ume 7, pages 1065–1074. Citeseer.

T Kudo. 2009. Crf++: Yet another crf toolkit [ol].
John Lafferty, Andrew McCallum, and Fernando CN

Pereira. 2001. Conditional random fields: Probabilis-
tic models for segmenting and labeling sequence data.

Huayi Li, Arjun Mukherjee, Jianfeng Si, and Bing Liu.
2015. Extracting verb expressions implying negative
opinions. In AAAI, pages 2411–2417.

Robert V Lindsey, William P Headden III, and Michael J
Stipicevic. 2012. A phrase-discovering topic model
using hierarchical pitman-yor processes. In Proceed-
ings of the 2012 Joint Conference on Empirical Meth-
ods in Natural Language Processing and Computa-
tional Natural Language Learning, pages 214–222.
Association for Computational Linguistics.

Bing Liu and Lei Zhang. 2012. A survey of opinion
mining and sentiment analysis. In Mining text data,
pages 415–463. Springer.

Jialu Liu, Jingbo Shang, Chi Wang, Xiang Ren, and Ji-
awei Han. 2015. Mining quality phrases from mas-
sive text corpora. In Proceedings of the 2015 ACM
SIGMOD International Conference on Management of
Data, pages 1729–1744. ACM.

Qiaozhu Mei, Xu Ling, Matthew Wondra, Hang Su, and
ChengXiang Zhai. 2007. Topic sentiment mixture:
modeling facets and opinions in weblogs. In Pro-
ceedings of the 16th international conference on World
Wide Web, pages 171–180. ACM.

David Mimno, Hanna M Wallach, Edmund Talley,
Miriam Leenders, and Andrew McCallum. 2011. Op-
timizing semantic coherence in topic models. In Pro-
ceedings of the Conference on Empirical Methods in
Natural Language Processing, pages 262–272. Asso-
ciation for Computational Linguistics.

636



Arjun Mukherjee and Bing Liu. 2012. Aspect extraction
through semi-supervised modeling. In Proceedings of
the 50th Annual Meeting of the Association for Com-
putational Linguistics: Long Papers-Volume 1, pages
339–348. Association for Computational Linguistics.

Arjun Mukherjee. 2016. Extracting aspect specific sen-
timent expres-sions implying negative opinions. In
In proceedings of the 17th International Conference
on Intelligent Text Processing and Computational Lin-
guistics.

Maria Pontiki, Dimitrios Galanis, Haris Papageorgiou,
Suresh Manandhar, and Ion Androutsopoulos. 2015.
Semeval-2015 task 12: Aspect based sentiment anal-
ysis. In Proceedings of the 9th International Work-
shop on Semantic Evaluation (SemEval 2015), Asso-
ciation for Computational Linguistics, Denver, Col-
orado, pages 486–495.

Christina Sauper, Aria Haghighi, and Regina Barzi-
lay. 2011. Content models with attitude. In Pro-
ceedings of the 49th Annual Meeting of the Associa-
tion for Computational Linguistics: Human Language
Technologies-Volume 1, pages 350–358. Association
for Computational Linguistics.

Ivan Titov and Ryan McDonald. 2008. Modeling on-
line reviews with multi-grain topic models. In Pro-
ceedings of the 17th international conference on World
Wide Web, pages 111–120. ACM.

Xuerui Wang, Andrew McCallum, and Xing Wei. 2007.
Topical n-grams: Phrase and topic discovery, with an
application to information retrieval. In Data Mining,
2007. ICDM 2007. Seventh IEEE International Con-
ference on, pages 697–702. IEEE.

Shuai Wang, Zhiyuan Chen, and Bing Liu. 2016. Mining
aspect-specific opinion using a holistic lifelong topic
model. In Proceedings of the 25th International Con-
ference on World Wide Web, pages 167–176. Interna-
tional World Wide Web Conferences Steering Com-
mittee.

Janyce Wiebe, Theresa Wilson, and Claire Cardie. 2005.
Annotating expressions of opinions and emotions in
language. Language resources and evaluation, 39(2-
3):165–210.

Theresa Wilson, Janyce Wiebe, and Paul Hoffmann.
2005. Recognizing contextual polarity in phrase-level
sentiment analysis. In Proceedings of the conference
on human language technology and empirical methods
in natural language processing, pages 347–354. Asso-
ciation for Computational Linguistics.

Yuanbin Wu, Qi Zhang, Xuanjing Huang, and Lide Wu.
2009. Phrase dependency parsing for opinion mining.
In Proceedings of the 2009 Conference on Empirical
Methods in Natural Language Processing: Volume 3-
Volume 3, pages 1533–1541. Association for Compu-
tational Linguistics.

Bishan Yang and Claire Cardie. 2012. Extracting opin-
ion expressions with semi-markov conditional random
fields. In Proceedings of the 2012 Joint Conference
on Empirical Methods in Natural Language Process-
ing and Computational Natural Language Learning,
pages 1335–1345. Association for Computational Lin-
guistics.

Bishan Yang and Claire Cardie. 2013. Joint inference
for fine-grained opinion extraction. In ACL (1), pages
1640–1649.

Bishan Yang and Claire Cardie. 2014. Joint modeling
of opinion expression extraction and attribute classifi-
cation. Transactions of the Association for Computa-
tional Linguistics, 2:505–516.

Wayne Xin Zhao, Jing Jiang, Hongfei Yan, and Xiaoming
Li. 2010. Jointly modeling aspects and opinions with
a maxent-lda hybrid. In Proceedings of the 2010 Con-
ference on Empirical Methods in Natural Language
Processing, pages 56–65. Association for Computa-
tional Linguistics.

Wayne Xin Zhao, Jing Jiang, Jing He, Yang Song,
Palakorn Achananuparp, Ee-Peng Lim, and Xiaoming
Li. 2011. Topical keyphrase extraction from twitter.
In Proceedings of the 49th Annual Meeting of the As-
sociation for Computational Linguistics: Human Lan-
guage Technologies-Volume 1, pages 379–388. Asso-
ciation for Computational Linguistics.

637


