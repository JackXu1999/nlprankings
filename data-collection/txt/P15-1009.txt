



















































Semantically Smooth Knowledge Graph Embedding


Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics
and the 7th International Joint Conference on Natural Language Processing, pages 84–94,

Beijing, China, July 26-31, 2015. c©2015 Association for Computational Linguistics

Semantically Smooth Knowledge Graph Embedding

Shu Guo†, Quan Wang†∗, Bin Wang†, Lihong Wang‡, Li Guo†
†Institute of Information Engineering, Chinese Academy of Sciences, Beijing 100093, China

{guoshu,wangquan,wangbin,guoli}@iie.ac.cn
‡National Computer Network Emergency Response Technical Team

Coordination Center of China, Beijing 100029, China
wlh@isc.org.cn

Abstract

This paper considers the problem of em-
bedding Knowledge Graphs (KGs) con-
sisting of entities and relations into low-
dimensional vector spaces. Most of the
existing methods perform this task based
solely on observed facts. The only re-
quirement is that the learned embeddings
should be compatible within each individ-
ual fact. In this paper, aiming at further
discovering the intrinsic geometric struc-
ture of the embedding space, we propose
Semantically Smooth Embedding (SSE).
The key idea of SSE is to take full ad-
vantage of additional semantic informa-
tion and enforce the embedding space to
be semantically smooth, i.e., entities be-
longing to the same semantic category will
lie close to each other in the embedding s-
pace. Two manifold learning algorithms
Laplacian Eigenmaps and Locally Linear
Embedding are used to model the smooth-
ness assumption. Both are formulated as
geometrically based regularization terms
to constrain the embedding task. We em-
pirically evaluate SSE in two benchmark
tasks of link prediction and triple classi-
fication, and achieve significant and con-
sistent improvements over state-of-the-art
methods. Furthermore, SSE is a general
framework. The smoothness assumption
can be imposed to a wide variety of em-
bedding models, and it can also be con-
structed using other information besides
entities’ semantic categories.

1 Introduction

Knowledge Graphs (KGs) like WordNet (Miller,
1995), Freebase (Bollacker et al., 2008), and DB-

∗Corresponding author: Quan Wang.

pedia (Lehmann et al., 2014) have become ex-
tremely useful resources for many NLP relat-
ed applications, such as word sense disambigua-
tion (Agirre et al., 2014), named entity recogni-
tion (Magnini et al., 2002), and information ex-
traction (Hoffmann et al., 2011). A KG is a multi-
relational directed graph composed of entities as
nodes and relations as edges. Each edge is repre-
sented as a triple of fact ⟨ei, rk, e j⟩, indicating that
head entity ei and tail entity e j are connected by re-
lation rk. Although powerful in representing struc-
tured data, the underlying symbolic nature makes
KGs hard to manipulate.

Recently a new research direction called knowl-
edge graph embedding has attracted much atten-
tion (Socher et al., 2013; Bordes et al., 2013; Bor-
des et al., 2014; Lin et al., 2015). It attempts to
embed components of a KG into continuous vector
spaces, so as to simplify the manipulation while
preserving the inherent structure of the original
graph. Specifically, given a KG, entities and re-
lations are first represented in a low-dimensional
vector space, and for each triple, a scoring func-
tion is defined to measure its plausibility in that
space. Then the representations of entities and re-
lations (i.e. embeddings) are learned by maximiz-
ing the total plausibility of observed triples. The
learned embeddings can further be used to benefit
all kinds of tasks, such as KG completion (Socher
et al., 2013; Bordes et al., 2013), relation extrac-
tion (Riedel et al., 2013; Weston et al., 2013), and
entity resolution (Bordes et al., 2014).

To our knowledge, most of existing KG embed-
ding methods perform the embedding task based
solely on observed facts. The only requiremen-
t is that the learned embeddings should be com-
patible within each individual fact. In this pa-
per we propose Semantically Smooth Embedding
(SSE), a new approach which further imposes con-
straints on the geometric structure of the embed-
ding space. The key idea of SSE is to make ful-

84



l use of additional semantic information (i.e. se-
mantic categories of entities) and enforce the em-
bedding space to be semantically smooth—entities
belonging to the same semantic category should
lie close to each other in the embedding space.
This smoothness assumption is closely related to
the local invariance assumption exploited in mani-
fold learning theory, which requires nearby points
to have similar embeddings or labels (Belkin and
Niyogi, 2001). Thus we employ two manifold
learning algorithms Laplacian Eigenmaps (Belkin
and Niyogi, 2001) and Locally Linear Embed-
ding (Roweis and Saul, 2000) to model the s-
moothness assumption. The former requires an
entity to lie close to every other entity in the same
category, while the latter represents that entity as
a linear combination of its nearest neighbors (i.e.
entities within the same category). Both are for-
mulated as manifold regularization terms to con-
strain the KG embedding objective function. As
such, SSE obtains an embedding space which is
semantically smooth and at the same time com-
patible with observed facts.

The advantages of SSE are two-fold: 1) By im-
posing the smoothness assumption, SSE success-
fully captures the semantic correlation between
entities, which exists intrinsically but is over-
looked in previous work on KG embedding. 2)
KGs are typically very sparse, containing a rela-
tively small number of facts compared to the large
number of entities and relations. SSE can effec-
tively deal with data sparsity by leveraging ad-
ditional semantic information. Both aspects lead
to more accurate embeddings in SSE. Moreover,
our approach is quite general. The smoothness as-
sumption can actually be imposed to a wide va-
riety of KG embedding models. Besides seman-
tic categories, other information (e.g. entity sim-
ilarities specified by users or derived from auxil-
iary data sources) can also be used to construc-
t the manifold regularization terms. And besides
KG embedding, similar smoothness assumptions
can also be applied in other embedding tasks (e.g.
word embedding and sentence embedding).

Our main contributions can be summarized as
follows. First, we devise a novel KG embedding
framework that naturally requires the embedding
space to be semantically smooth. As far as we
know, it is the first work that imposes constraints
on the geometric structure of the embedding space
during KG embedding. By leveraging addition-

al semantic information, our approach can also
deal with the data sparsity issue that commonly
exists in typical KGs. Second, we evaluate our
approach in two benchmark tasks of link predic-
tion and triple classification, and achieve signif-
icant and consistent improvements over state-of-
the-art models.

In the remainder of this paper, we first provide
a brief review of existing KG embedding model-
s in Section 2, and then detail the proposed SSE
framework in Section 3. Experiments and results
are reported in Section 4. Then in Section 5 we
discuss related work, followed by the conclusion
and future work in Section 6.

2 A Brief Review of KG Embedding

KG embedding aims to embed entities and rela-
tions into a continuous vector space and model the
plausibility of each fact in that space. In general, it
consists of three steps: 1) representing entities and
relations, 2) specifying a scoring function, and 3)
learning the latent representations. In the first step,
given a KG, entities are represented as points (i.e.
vectors) in a continuous vector space, and relation-
s as operators in that space, which can be charac-
terized by vectors (Bordes et al., 2013; Bordes et
al., 2014; Wang et al., 2014b), matrices (Bordes et
al., 2011; Jenatton et al., 2012), or tensors (Socher
et al., 2013). In the second step, for each candi-
date fact ⟨ei, rk, e j⟩, an energy function f (ei, rk, e j)
is further defined to measure its plausibility, with
the corresponding entity and relation representa-
tions as variables. Plausible triples are assumed to
have low energies. Then in the third step, to obtain
the entity and relation representations, a margin-
based ranking loss, i.e.,

L=
∑
t+∈O

∑
t−∈Nt+

[
γ+ f (ei, rk, e j)− f (e′i , rk, e′j)

]
+
, (1)

is minimized. Here, O is the set of observed (i.e.
positive) triples, and t+ = ⟨ei, rk, e j⟩ ∈ O; Nt+ de-
notes the set of negative triples constructed by re-
placing entities in t+, and t− = ⟨e′i , rk, e′j⟩ ∈ Nt+ ;
γ > 0 is a margin separating positive and nega-
tive triples; and [x]+ = max(0, x). The ranking
loss favors lower energies for positive triples than
for negative ones. Stochastic gradient descent (in
mini-batch mode) is adopted to solve the mini-
mization problem. For details please refer to (Bor-
des et al., 2013) and references therein.

Different embedding models differ in the first t-
wo steps: entity/relation representation and energy

85



Method Entity/Relation embeddings Energy function

TransE (Bordes et al., 2013) e, r ∈ Rd f (ei, rk, e j) = ∥ei + rk − e j∥ℓ1/ℓ2
SME (lin) (Bordes et al., 2014) e, r ∈ Rd f (ei, rk, e j) = (Wu1rk +Wu2ei + bu)T

(
Wv1rk +Wv2e j + bv

)
SME (bilin) (Bordes et al., 2014) e, r ∈ Rd f (ei, rk, e j) =

((
Wu×̄3rk

)
ei + bu

)T ((
Wv×̄3rk

)
e j + bv

)
SE (Bordes et al., 2011) e ∈ Rd, Ru,Rv ∈ Rd×d f (ei, rk, e j) = ∥Rukei − Rvke j∥ℓ1

Table 1: Existing KG embedding models.

function definition. Three state-of-the-art embed-
ding models, namely TransE (Bordes et al., 2013),
SME (Bordes et al., 2014), and SE (Bordes et al.,
2011), are detailed below. Please refer to (Jenat-
ton et al., 2012; Socher et al., 2013; Wang et al.,
2014b; Lin et al., 2015) for other methods.

TransE (Bordes et al., 2013) represents both en-
tities and relations as vectors in the embedding s-
pace. For a given triple ⟨ei, rk, e j⟩, the relation is
interpreted as a translation vector rk so that the
embedded entities ei and e j can be connected by
rk with low error. The energy function is defined
as f (ei, rk, e j) = ∥ei + rk − e j∥ℓ1/ℓ2 , where ∥·∥ℓ1/ℓ2
denotes the ℓ1-norm or ℓ2-norm.

SME (Bordes et al., 2014) also represents enti-
ties and relations as vectors, but models triples in
a more expressive way. Given a triple ⟨ei, rk, e j⟩,
it first employs a function gu (·, ·) to combine rk
and ei, and gv (·, ·) to combine rk and e j. Then,
the energy function is defined as matching gu (·, ·)
and gv (·, ·) by their dot product, i.e., f (ei, rk, e j) =
gu(rk, ei)T gv(rk, e j). There are two versions of
SME, linear and bilinear (denoted as SME (lin)
and SME (bilin) respectively), obtained by defin-
ing different gu (·, ·) and gv (·, ·).

SE (Bordes et al., 2011) represents entities as
vectors but relations as matrices. Each relation is
modeled by a left matrix Ruk and a right matrix R

v
k,

acting as independent projections to head and tail
entities respectively. If a triple ⟨ei, rk, e j⟩ holds,
Rukei and R

v
ke j should be close to each other. The

energy function is f (ei, rk, e j) = ∥Rukei − Rvke j∥ℓ1 .
Table 1 summarizes the entity/relation representa-
tions and energy functions used in these models.

3 Semantically Smooth Embedding

The methods introduced above perform the em-
bedding task based solely on observed facts. The
only requirement is that the learned embeddings
should be compatible within each individual fact.
However, they fail to discover the intrinsic geo-
metric structure of the embedding space. To deal
with this limitation, we introduce Semantically S-

mooth Embedding (SSE) which constrains the em-
bedding task by incorporating geometrically based
regularization terms, constructed by using addi-
tional semantic categories of entities.

3.1 Problem Formulation

Suppose we are given a KG consisting of n entities
and m relations. The facts observed are stored as
a set of triples O =

{
⟨ei, rk, e j⟩

}
. A triple ⟨ei, rk, e j⟩

indicates that entity ei and entity e j are connected
by relation rk. In addition, the entities are classi-
fied into multiple semantic categories. Each entity
e is associated with a label ce indicating the cate-
gory to which it belongs. SSE aims to embed the
entities and relations into a continuous vector s-
pace which is compatible with the observed facts,
and at the same time semantically smooth.

To make the embedding space compatible with
the observed facts, we make use of the triple set O
and follow the same strategy adopted in previous
methods. That is, we define an energy function
on each candidate triple (e.g. the energy functions
listed in Table 1), and require observed triples to
have lower energies than unobserved ones (i.e. the
margin-based ranking loss defined in Eq. (1)).

To make the embedding space semantically s-
mooth, we further leverage the entity category in-
formation {ce}, and assume that entities within the
same semantic category should lie close to each
other in the embedding space. This smoothness
assumption is similar to the local invariance as-
sumption exploited in manifold learning theory
(i.e. nearby points are likely to have similar em-
beddings or labels). So we employ two manifold
learning algorithms Laplacian Eigenmaps (Belkin
and Niyogi, 2001) and Locally Linear Embed-
ding (Roweis and Saul, 2000) to model such se-
mantic smoothness, termed as LE and LLE for
short respectively.

3.2 Modeling Semantic Smoothness by LE

Laplacian Eigenmaps (LE) is a manifold learning
algorithm that preserves local invariance between

86



each two data points (Belkin and Niyogi, 2001).
We borrow the idea of LE and enforce semantic
smoothness by assuming:
Smoothness Assumption 1 If two entities ei and
e j belong to the same semantic category, they will
have embeddings ei and e j close to each other.
To encode the semantic information, we construct
an adjacency matrix W1 ∈ Rn×n among the enti-
ties, with the i j-th entry defined as:

w(1)i j =

1, if cei = ce j ,0, otherwise,
where cei /ce j is the category label of entity ei/e j.
Then, we use the following term to measure the
smoothness of the embedding space:

R1 = 12
n∑

i=1

n∑
j=1

∥ei − e j∥22w(1)i j ,

where ei and e j are the embeddings of entities ei
and e j respectively. By minimizing R1, we expect
Smoothness Assumption 1: if two entities ei and e j
belong to the same semantic category (i.e. w(1)i j =
1), the distance between ei and e j (i.e. ∥ei − e j∥22)
should be small.

We further incorporate R1 as a regularization
term into the margin-based ranking loss (i.e. Eq.
(1)) adopted in previous KG embedding methods,
and propose our first SSE model. The new mod-
el performs the embedding task by minimizing the
following objective function:

L1= 1N
∑
t+∈O

∑
t−∈Nt+
ℓ
(
t+, t−

)
+
λ1
2

n∑
i=1

n∑
j=1

∥ei− e j∥22w(1)i j ,

where ℓ
(
t+, t−

)
=
[
γ+ f (ei, rk, e j)− f (e′i , rk, e′j)

]
+

is
the ranking loss on the positive-negative triple pair(
t+, t−

)
, and N is the total number of such triple

pairs. The first term in L1 enforces the resultant
embedding space compatible with all the observed
triples, and the second term further requires that
space to be semantically smooth. Hyperparameter
λ1 makes a trade-off between the two cases.

The minimization is carried out by stochastic
gradient descent. Given a randomly sampled posi-
tive triple t+ = ⟨ei, rk, e j⟩ and the associated nega-
tive triple t− = ⟨e′i , rk, e′j⟩,1 the stochastic gradient
w.r.t. es (s ∈ {i, j, i′, j′}) can be calculated as:
∇esL1 = ∇esℓ

(
t+, t−

)
+ 2λ1E (D −W1) 1s,

1The negative triple is constructed by replacing one of the
entities in the positive triple.

where E = [e1, e2, · · · , en] ∈ Rd×n is a matrix con-
sisting of entity embeddings; D ∈ Rn×n is a di-
agonal matrix with the i-th entry on the diagonal
being dii =

∑n
j=1 w

(1)
i j ; and 1s ∈ Rn is a column

vector where the s-th entry is 1 and the others are
0. Other parameters are not included in R1, and
their gradients remain the same as defined in pre-
vious work.

3.3 Modeling Semantic Smoothness by LLE

As opposed to LE which preserves local invari-
ance within data pairs, Locally Linear Embedding
(LLE) expects each data point to be roughly re-
constructed by a linear combination of its nearest
neighbors (Roweis and Saul, 2000). We borrow
the idea of LLE and enforce semantic smoothness
by assuming:

Smoothness Assumption 2 Each entity ei can be
roughly reconstructed by a linear combination of
its nearest neighbors in the embedding space, i.e.,
ei ≈ ∑e j∈N(ei) α je j. Here nearest neighbors refer
to entities belonging to the same semantic catego-
ry with ei.

To model this assumption, for each entity ei, we
randomly sample K entities uniformly from the
category to which ei belongs, denoted as the n-
earest neighbor set N (ei). We construct a weight
matrix W2 ∈ Rn×n by defining:

w(2)i j =

1, if e j ∈ N (ei) ,0, otherwise,
and normalize the rows so that

∑n
j=1 w

(2)
i j = 1 for

each row i. Note that W2 is no longer a symmetric
matrix. The smoothness of the embedding space
can be measured by the reconstruction error:

R2 =
n∑

i=1

∥∥∥∥∥∥∥ei − ∑e j∈N(ei)w(2)i j e j
∥∥∥∥∥∥∥

2

2

.

Minimizing R2 results in Smoothness Assump-
tion 2: each entity can be linearly reconstructed
from its nearest neighbors with low error.

By incorporating R2 as a regularization term in-
to the margin-based ranking loss defined in Eq.
(1), we obtain our second SSE model, which per-
forms the embedding task by minimizing:

L2= 1N
∑
t+∈O

∑
t−∈Nt+
ℓ
(
t+, t−

)
+λ2

n∑
i=1

∥∥∥∥∥∥∥ei − ∑e j∈N(ei)w(2)i j e j
∥∥∥∥∥∥∥

2

2

.

87



The resultant embedding space is also semanti-
cally smooth and compatible with the observed
triples. Hyperparameter λ2 makes a trade-off be-
tween the two cases.

Similar to the first model, stochastic gradien-
t descent is used to solve the minimization prob-
lem. Given a positive triple t+ = ⟨ei, rk, e j⟩ and
the associated negative triple t− = ⟨e′i , rk, e′j⟩, the
gradient w.r.t. es (s ∈ {i, j, i′, j′}) is calculated as:
∇esL2 = ∇esℓ

(
t+, t−

)
+2λ2E (I −W2)T (I −W2) 1s,

where I ∈ Rn×n is the identity matrix. Other pa-
rameters are not included in R2, and their gradi-
ents remain the same as defined in previous work.
To better capture the cohesion within each cate-
gory, during each stochastic step we resample the
nearest neighbors for each entity, uniformly from
the category to which it belongs.

3.4 Advantages and Extensions
The advantages of our approach can be summa-
rized as follows: 1) By incorporating geometri-
cally based regularization terms, the SSE mod-
els are able to capture the semantic correlation
between entities, which exists intrinsically but is
overlooked in previous work. 2) By leveraging ad-
ditional entity category information, the SSE mod-
els can deal with the data sparsity issue that com-
monly exists in most KGs. Both aspects lead to
more accurate embeddings.

Entity category information has also been inves-
tigated in (Nickel et al., 2012; Chang et al., 2014;
Wang et al., 2015), but in different manners. Nick-
el et al. (2012) take categories as pseudo entities
and introduce a specific relation to link entities
to categories. Chang et al. (2014) and Wang et
al. (2015) use entity categories to specify relation-
s’ argument expectations, removing invalid triples
during training and reasoning respectively. None
of them considers the intrinsic geometric structure
of the embedding space.

Actually, our approach is quite general. 1) The
smoothness assumptions can be imposed to a wide
variety of KG embedding models, not only the
ones introduced in Section 2, but also those based
on matrix/tensor factorization (Nickel et al., 2011;
Chang et al., 2013). 2) Besides semantic cate-
gories, other information (e.g. entity similarities
specified by users or derived from auxiliary data
sources) can also be used to construct the mani-
fold regularization terms. 3) Besides KG embed-
ding, similar smoothness assumptions can also be

Location Sport

CityCapitalOfCountry AthleteLedSportTeam
CityLocatedInCountry AthletePlaysForTeam
CityLocatedInGeopoliticallocation AthletePlaysInLeague
CityLocatedInState AthletePlaysSport
CountryLocatedInGeopoliticallocation CoachesInLeague
StateHasCapital CoachesTeam
StateLocatedInCountry TeamPlaysInLeague
StateLocatedInGeopoliticallocation TeamPlaysSport

Table 2: Relations in Location and Sport.

applied in other embedding tasks (e.g. word em-
bedding and sentence embedding).

4 Experiments

We empirically evaluate the proposed SSE models
in two tasks: link prediction (Bordes et al., 2013)
and triple classification (Socher et al., 2013).

4.1 Data Sets
We create three data sets with different sizes using
NELL (Carlson et al., 2010): Location, Sport, and
Nell186. Location and Sport are two small-scale
data sets, both containing 8 relations on the topics
of “location” and “sport” respectively. The corre-
sponding relations are listed in Table 2. Nell186 is
a larger data set containing the most frequent 186
relations. On all the data sets, entities appearing
only once are removed. We extract the entity cat-
egory information from a specific relation called
Generalization, and keep non-overlapping cat-
egories.2 Categories containing less than 5 entities
on Location and Sport as well as categories con-
taining less than 50 entities on Nell186 are fur-
ther removed. Table 3 gives some statistics of the
three data sets, where # Rel./# Ent./# Trip./# Cat.
denotes the number of relations/entities/observed
triples/categories respectively, and # c-Ent. de-
notes the number of entities that have category la-
bels. Note that our SSE models do not require ev-
ery entity to have a category label. From the statis-
tics, we can see that all the three data sets suffer
from the data sparsity issue, containing a relative-
ly small number of observed triples compared to
the number of entities.

On the two small-scale data sets Location and
Sport, triples are split into training/validation/test
sets, with the ratio of 3:1:1. The first set is used
for modeling training, the second for hyperparam-
eter tuning, and the third for evaluation. All ex-
periments are repeated 5 times by drawing new

2If two categories overlap, the smaller one is discarded.

88



# Rel. # Ent. # Trip. # Cat. # c-Ent.

Location 8 380 718 5 358
Sport 8 1,520 3,826 4 1,506
Nell186 186 14,463 41,134 35 8,590

Table 3: Statistics of data sets.

training/validation/test splits, and results averaged
over the 5 rounds are reported. On Nell186 ex-
periments are conducted only once, using a train-
ing/validation/test split with 31,134/5,000/5,000
triples respectively. We will release the data up-
on request.

4.2 Link Prediction
This task is to complete a triple ⟨ei, rk, e j⟩ with ei
or e j missing, i.e., predict ei given (rk, e j) or pre-
dict e j given (ei, rk).

Baseline methods. We take TransE, SME (lin),
SME (bilin), and SE as our baselines. We then in-
corporate manifold regularization terms into these
methods to obtain the SSE models. A model
with the LE/LLE regularization term is denoted
as TransE-LE/TransE-LLE for example. We fur-
ther compare our SSE models with the setting pro-
posed by Nickel et al. (2012), which also takes in-
to account the entity category information, but in
a more direct manner. That is, given an entity e
with its category label ce, we create a new triple
⟨e, Generalization, ce⟩ and add it into the train-
ing set. Such a method is denoted as TransE-Cat
for example.

Evaluation protocol. For evaluation, we adopt
the same ranking procedure proposed by Bordes et
al. (2013). For each test triple ⟨ei, rk, e j⟩, the head
entity ei is replaced by every entity e′i in the KG,
and the energy is calculated for the corrupted triple
⟨e′i , rk, e j⟩. Ranking the energies in ascending or-
der, we get the rank of the correct entity ei. Sim-
ilarly, we can get another rank by corrupting the
tail entity e j. Aggregated over all test triples, we
report three metrics: 1) the averaged rank, denoted
as Mean (the smaller, the better); 2) the median of
the ranks, denoted as Median (the smaller, the bet-
ter); and 3) the proportion of ranks no larger than
10, denoted as Hits@10 (the higher, the better).

Implementation details. We implement the
methods based on the code provided by Bordes et
al. (2013)3. For all the methods, we create 100
mini-batches on each data set. On Location and
Sport, the dimension of the embedding space d is

3https://github.com/glorotxa/SME

set in the range of {10, 20, 50, 100}, the margin γ
is set in the range of {1, 2, 5, 10}, and the learning
rate is fixed to 0.1. On Nell186, the hyperparame-
ters d and γ are fixed to 50 and 1 respectively, and
the learning rate is fixed to 10. In LE and LLE,
the regularization hyperparameters λ1 and λ2 are
tuned in {10−4, 10−5, 10−6, 10−7, 10−8}. And the
number of nearest neighbors K in LLE is tuned in
{5, 10, 15, 20}. The best model is selected by ear-
ly stopping on the validation sets (by monitoring
Mean), with a total of at most 1000 iterations over
the training sets.

Results. Table 4 reports the results on the test
sets of Location, Sport, and Nell186. From the
results, we can see that: 1) SSE (regularized vi-
a either LE or LLE) outperforms all the baselines
on all the data sets and with all the metrics. The
improvements are usually quite significant. The
metric Mean drops by about 10% to 65%, Medi-
an drops by about 5% to 75%, and Hits@10 rises
by about 5% to 190%. This observation demon-
strates the superiority and generality of our ap-
proach. 2) Even if encoded in a direct way (e.g.
TransE-Cat), the entity category information can
still help the baseline methods in the link predic-
tion task. This observation indicates that leverag-
ing additional information is indeed useful in deal-
ing with the data sparsity issue and hence leads to
better performance. 3) Compared to the strategy
which incorporates the entity category information
directly, formulating such information as manifold
regularization terms results in better and more sta-
ble results. The *-Cat models sometimes perfor-
m even worse than the baselines (e.g. TransE-Cat
on Sport data), while the SSE models consistent-
ly achieve better results. This observation further
demonstrates the superiority of constraining the
geometric structure of the embedding space.

We further visualize and compare the geometric
structures of the embedding spaces learned by tra-
ditional embedding and semantically smooth em-
bedding. We select the 10 largest semantic cate-
gories in Nell186 (specified in Figure 1) and the
5,740 entities therein. We take the embeddings
of these entities learned by TransE, TransE-Cat,
TransE-LE, and TransE-LLE, with the optimal hy-
perparameter settings determined in the link pre-
diction task. Then we create 2D plots using t-
SNE (Van der Maaten and Hinton, 2008)4. The
results are shown in Figure 1, where a different

4http://lvdmaaten.github.io/tsne/

89



Location Sport Nell186
Mean Median Hits@10 (%) Mean Median Hits@10 (%) Mean Median Hits@10 (%)

TransE 30.94 10.70 50.56 362.66 62.90 43.86 924.37 94.00 16.95
TransE-Cat 28.48 8.90 52.43 320.30 86.40 37.46 657.53 80.50 19.14
TransE-LE 28.59 8.90 53.06 183.10 23.20 45.83 573.55 79.00 20.26
TransE-LLE 28.03 9.20 52.36 231.67 52.40 43.18 535.32 95.00 20.02
SME (lin) 63.01 24.10 40.90 266.50 87.10 32.34 427.86 26.00 35.97
SME (lin)-Cat 41.12 18.30 42.43 263.88 70.80 35.03 309.60 25.00 36.22
SME (lin)-LE 36.19 16.10 43.75 237.38 50.80 38.35 276.94 25.00 37.14
SME (lin)-LLE 38.22 15.60 43.96 241.70 63.70 36.54 252.87 25.00 37.14
SME (bilin) 47.66 20.90 37.85 314.49 124.00 33.83 848.39 28.00 35.71
SME (bilin)-Cat 40.75 16.20 42.71 298.09 103.80 35.86 560.76 24.00 37.83
SME (bilin)-LE 33.41 14.00 44.24 297.90 116.10 38.95 448.31 24.00 37.80
SME (bilin)-LLE 32.84 13.60 46.25 286.63 110.10 35.67 452.43 28.00 36.51
SE 108.15 69.90 14.72 426.70 242.60 24.72 904.84 44.00 27.81
SE-Cat 88.36 48.20 20.76 435.44 231.00 35.39 529.38 40.00 28.68
SE-LE 36.43 16.00 42.92 252.30 90.50 37.19 456.20 43.00 30.89
SE-LLE 38.47 17.50 42.08 235.44 105.40 37.83 447.05 37.00 31.55

Table 4: Link prediction results on the test sets of Location, Sport, and Nell186.

Athlete Politicianus Chemical City Clothing Country Sportsteam Journalist Televisionstation Room

 

 

(a) TransE.
 

 

(b) TransE-Cat.
 

 

(c) TransE-LE.
 

 

(d) TransE-LLE.

Figure 1: Embeddings of entities belonging to the 10 largest categories in Nell186 (best viewed in color).

color is used for each category. It is easy to see
that imposing the semantic smoothness assump-
tions helps in capturing the semantic correlation
between entities in the embedding space. Entities
within the same category lie closer to each oth-
er, while entities belonging to different categories
are easily distinguished (see Figure 1(c) and Fig-
ure 1(d)). Incorporating the entity category infor-
mation directly could also helps. But it fails on
some “hard” entities (i.e., those belonging to d-
ifferent categories but mixed together in the cen-
ter of Figure 1(b)). We have conducted the same
experiments with the other methods and observed
similar phenomena.

4.3 Triple Classification

This task is to verify whether a given triple
⟨ei, rk, e j⟩ is correct or not. We test our SSE mod-
els in this task, with the same comparison settings
as used in the link prediction task.

Evaluation protocol. We follow the same eval-
uation protocol used in (Socher et al., 2013; Wang
et al., 2014b). To create labeled data for classifica-

tion, for each triple in the test and validation sets,
we construct a negative triple for it by randomly
corrupting the entities. To corrupt a position (head
or tail), only entities that have appeared in that po-
sition are allowed. During triple classification, a
triple is predicted as positive if the energy is be-
low a relation-specific threshold δr; otherwise as
negative. We report two metrics on the test sets:
micro-averaged accuracy and macro-averaged ac-
curacy, denoted as Micro-ACC and Macro-ACC
respectively. The former is a per-triple average,
while the latter is a per-relation average.

Implementation details. We use the same hy-
perparameter settings as in the link prediction task.
The relation-specific threshold δr is determined by
maximizing Micro-ACC on the validation sets. A-
gain, training is limited to at most 1000 iterations,
and the best model is selected by early stopping on
the validation sets (by monitoring Micro-ACC).

Results. Table 5 reports the results on the test
sets of Location, Sport, and Nell186. The results
indicate that: 1) SSE (regularized via either LE or
LLE) performs consistently better than the base-

90



Location Sport Nell186
Micro-ACC Macro-ACC Micro-ACC Macro-ACC Micro-ACC Macro-ACC

TransE 86.11 81.66 72.52 73.78 84.21 77.86
TransE-Cat 82.50 77.81 75.09 74.23 87.34 81.27
TransE-LE 86.39 81.50 79.88 77.34 90.32 84.61
TransE-LLE 87.01 83.03 80.29 77.71 90.08 84.50
SME (lin) 75.90 71.82 72.61 71.24 88.54 84.17
SME (lin)-Cat 83.33 80.90 73.52 72.28 91.00 86.20
SME (lin)-LE 84.65 79.33 79.25 74.95 92.44 88.07
SME (lin)-LLE 84.58 79.60 79.45 75.61 92.99 88.68
SME (bilin) 73.06 67.26 71.33 67.78 88.78 84.79
SME (bilin)-Cat 79.38 74.35 75.12 72.41 91.67 86.48
SME (bilin)-LE 83.75 79.66 79.23 76.18 93.37 89.29
SME (bilin)-LLE 83.54 80.36 79.33 75.35 93.64 89.39
SE 65.14 60.01 68.61 63.71 90.18 83.93
SE-Cat 68.61 62.82 67.62 62.17 92.87 87.72
SE-LE 81.67 77.52 81.46 74.72 93.94 88.62
SE-LLE 82.01 77.45 80.25 76.07 93.95 88.54

Table 5: Triple classification results (%) on the test sets of Location, Sport, and Nell186.

line methods on all the data sets in both metric-
s. The improvements are usually quite substantial.
The metric Micro-ACC rises by about 1% to 25%,
and Macro-ACC by about 2% to 30%. 2) Incorpo-
rating the entity category information directly can
also improve the baselines in the triple classifica-
tion task, again demonstrating the effectiveness of
leveraging additional information to deal with the
data sparsity issue. 3) It is a better choice to in-
corporate the entity category information as man-
ifold regularization terms as opposed to encoding
it directly. The *-Cat models sometimes perfor-
m even worse than the baselines (e.g. TransE-
Cat on Location data and SE-Cat on Sport data),
while the SSE models consistently achieve better
results. The observations are similar to those ob-
served during the link prediction task, and further
demonstrate the superiority and generality of our
approach.

5 Related Work

This section reviews two lines of related work: KG
embedding and manifold learning.

KG embedding aims to embed a KG composed
of entities and relations into a low-dimensional
vector space, and model the plausibility of each
fact in that space. Yang et al. (2014) categorized
the literature into three major groups: 1) method-
s based on neural networks, 2) methods based on
matrix/tensor factorization, and 3) methods based
on Bayesian clustering. The first group perform-
s the embedding task using neural network archi-
tectures (Bordes et al., 2013; Bordes et al., 2014;
Socher et al., 2013). Several state-of-the-art neural

network-based embedding models have been in-
troduced in Section 2. For other work please refer
to (Jenatton et al., 2012; Wang et al., 2014b; Lin et
al., 2015). In the second group, KGs are represent-
ed as tensors, and embedding is performed via ten-
sor factorization or collective matrix factorization
techniques (Singh and Gordon, 2008; Nickel et al.,
2011; Chang et al., 2014). The third group embeds
factorized representations of entities and relations
into a nonparametric Bayesian clustering frame-
work, so as to obtain more interpretable embed-
dings (Kemp et al., 2006; Sutskever et al., 2009).
Our work falls into the first group, but differs in
that it further imposes constraints on the geomet-
ric structure of the embedding space, which exists
intrinsically but is overlooked in previous work.
Although this paper focuses on incorporating ge-
ometrically based regularization terms into neural
network architectures, it can be easily extended to
matrix/tensor factorization techniques.

Manifold learning is a geometrically motivat-
ed framework for machine learning, enforcing the
learning model to be smooth w.r.t. the geometric
structure of data (Belkin et al., 2006). Within this
framework, various manifold learning algorithm-
s have been proposed, such as ISOMAP (Tenen-
baum et al., 2000), Laplacian Eigenmaps (Belkin
and Niyogi, 2001), and Locally Linear Embed-
ding (Roweis and Saul, 2000). All these algo-
rithms are based on the so-called local invariance
assumption, i.e., nearby points are likely to have
similar embeddings or labels. Manifold learning
has been widely applied in many different areas,
from dimensionality reduction (Belkin and Niyo-

91



gi, 2001; Cai et al., 2008) and semi-supervised
learning (Zhou et al., 2004; Zhu and Niyogi,
2005) to recommender systems (Ma et al., 2011)
and community question answering (Wang et al.,
2014a). This paper employs manifold learning al-
gorithms to model the semantic smoothness as-
sumptions in KG embedding.

6 Conclusion and Future Work

In this paper, we have proposed a novel approach
to KG embedding, referred to as Semantically S-
mooth Embedding (SSE). The key idea of SSE is
to impose constraints on the geometric structure of
the embedding space and enforce it to be semanti-
cally smooth. The semantic smoothness assump-
tions are constructed by using entities’ category
information, and then formulated as geometrical-
ly based regularization terms to constrain the em-
bedding task. The embeddings learned in this way
are capable of capturing the semantic correlation
between entities. By leveraging additional infor-
mation besides observed triples, SSE can also deal
with the data sparsity issue that commonly exists
in most KGs. We empirically evaluate SSE in two
benchmark tasks of link prediction and triple clas-
sification. Experimental results show that by in-
corporating the semantic smoothness assumption-
s, SSE significantly and consistently outperforms
state-of-the-art embedding methods, demonstrat-
ing the superiority of our approach. In addition,
our approach is quite general. The smoothness as-
sumptions can actually be imposed to a wide vari-
ety of embedding models, and it can also be con-
structed using other information besides entities’
semantic categories.

As future work, we would like to: 1) Construct
the manifold regularization terms using other da-
ta sources. The only information required to con-
struct the manifold regularization terms is the sim-
ilarity between entities (used to define the adja-
cency matrix in LE and to select nearest neigh-
bors for each entity in LLE). We would try entity
similarities derived in different ways, e.g., spec-
ified by users or calculated from entities’ textual
descriptions. 2) Enhance the efficiency and scala-
bility of SSE. Processing the manifold regulariza-
tion terms can be time- and space-consuming (e-
specially the one induced by the LE algorithm).
We would investigate how to address this prob-
lem, e.g., via the efficient iterative algorithms in-
troduced in (Saul and Roweis, 2003) or via paral-

lel/distributed computing. 3) Impose the seman-
tic smoothness assumptions on other KG embed-
ding methods (e.g. those based on matrix/tensor
factorization or Bayesian clustering), and even on
other embedding tasks (e.g. word embedding or
sentence embedding).

Acknowledgments

We would like to thank the anonymous reviewers
for their valuable comments and suggestions. This
work is supported by the National Natural Science
Foundation of China (grant No. 61402465), the S-
trategic Priority Research Program of the Chinese
Academy of Sciences (grant No. XDA06030200),
and the National Key Technology R&D Program
(grant No. 2012BAH46B03).

References
Eneko Agirre, Oier Lopez de Lacalle, and Aitor Soroa.

2014. Random walks for knowledge-based word
sense disambiguation. Computational Linguistics,
40(1):57–84.

Mikhail Belkin and Partha Niyogi. 2001. Laplacian
eigenmaps and spectral techniques for embedding
and clustering. In Advances in Neural Information
Processing Systems, pages 585–591.

Mikhail Belkin, Partha Niyogi, and Vikas Sindhwani.
2006. Manifold regularization: A geometric frame-
work for learning from labeled and unlabeled ex-
amples. Journal of Machine Learning Research,
7:2399–2434.

Kurt Bollacker, Colin Evans, Praveen Paritosh, Tim S-
turge, and Jamie Taylor. 2008. Freebase: A collab-
oratively created graph database for structuring hu-
man knowledge. In Proceedings of the 2008 ACM
SIGMOD International Conference on Management
of Data, pages 1247–1250.

Antoine Bordes, Jason Weston, Ronan Collobert, and
Yoshua Bengio. 2011. Learning structured em-
beddings of knowledge bases. In Proceedings of
the 25th AAAI Conference on Artificial Intelligence,
pages 301–306.

Antoine Bordes, Nicolas Usunier, Alberto Garcia-
Durán, Jason Weston, and Oksana Yakhnenko.
2013. Translating embeddings for modeling multi-
relational data. In Advances in Neural Information
Processing Systems, pages 2787–2795.

Antoine Bordes, Xavier Glorot, Jason Weston, and
Yoshua Bengio. 2014. A semantic matching en-
ergy function for learning with multi-relational data.
Machine Learning, 94(2):233–259.

92



Deng Cai, Xiaofei He, Xiaoyun Wu, and Jiawei Han.
2008. Non-negative matrix factorization on mani-
fold. In Proceedings of the 8th IEEE International
Conference on Data Mining, pages 63–72.

Andrew Carlson, Justin Betteridge, Bryan Kisiel, Bur-
r Settles, Estevam R. Hruschka Jr, and Tom M.
Mitchell. 2010. Toward an architecture for never-
ending language learning. In Proceedings of the
24th AAAI Conference on Artificial Intelligence,
pages 1306–1313.

Kai-Wei Chang, Wen-tau Yih, and Christopher Meek.
2013. Multi-relational latent semantic analysis. In
Proceedings of the 2013 Conference on Empirical
Methods in Natural Language Processing, pages
1602–1612.

Kai-Wei Chang, Wen-tau Yih, Bishan Yang, and
Christopher Meek. 2014. Typed tensor decom-
position of knowledge bases for relation extraction.
In Proceedings of the 2014 Conference on Empiri-
cal Methods in Natural Language Processing, pages
1568–1579.

Raphael Hoffmann, Congle Zhang, Xiao Ling,
Luke Zettlemoyer, and Daniel S. Weld. 2011.
Knowledge-based weak supervision for information
extraction of overlapping relations. In Proceedings
of the 49th Annual Meeting of the Association for
Computational Linguistics: Human Language Tech-
nologies, pages 541–550.

Rodolphe Jenatton, Nicolas L. Roux, Antoine Bordes,
and Guillaume R. Obozinski. 2012. A latent fac-
tor model for highly multi-relational data. In Ad-
vances in Neural Information Processing Systems,
pages 3167–3175.

Charles Kemp, Joshua B. Tenenbaum, Thomas L. Grif-
fiths, Takeshi Yamada, and Naonori Ueda. 2006.
Learning systems of concepts with an infinite rela-
tional model. In Proceedings of the 21st AAAI Con-
ference on Artificial Intelligence, pages 381–388.

Jens Lehmann, Robert Isele, Max Jakob, Anja
Jentzsch, Dimitris Kontokostas, Pablo N. Mendes,
Sebastian Hellmann, Mohamed Morsey, Patrick van
Kleef, Sören Auer, et al. 2014. Dbpedia: A large-
scale, multilingual knowledge base extracted from
wikipedia. Semantic Web Journal.

Yankai Lin, Zhiyuan Liu, Maosong Sun, Yang Liu, and
Xuan Zhu. 2015. Learning entity and relation em-
beddings for knowledge graph completion. In Pro-
ceedings of the 29th AAAI Conference on Artificial
Intelligence, pages 2181–2187.

Hao Ma, Dengyong Zhou, Chao Liu, Michael R. Lyu,
and Irwin King. 2011. Recommender systems with
social regularization. In Proceedings of the 4th ACM
International Conference on Web Search and Data
Mining, pages 287–296.

Bernardo Magnini, Matteo Negri, Roberto Prevete, and
Hristo Tanev. 2002. A wordnet-based approach
to named entities recognition. In Proceedings of
the 2002 Workshop on Building and Using Seman-
tic Networks, pages 1–7.

George A. Miller. 1995. Wordnet: A lexical
database for english. Communications of the ACM,
38(11):39–41.

Maximilian Nickel, Volker Tresp, and Hans-Peter
Kriegel. 2011. A three-way model for collective
learning on multi-relational data. In Proceedings
of the 28th International Conference on Machine
Learning, pages 809–816.

Maximilian Nickel, Volker Tresp, and Hans-Peter
Kriegel. 2012. Factorizing yago: Scalable machine
learning for linked data. In Proceedings of the 21st
International Conference on World Wide Web, pages
271–280.

Sebastian Riedel, Limin Yao, Andrew McCallum, and
Benjamin M. Marlin. 2013. Relation extraction
with matrix factorization and universal schemas. In
Proceedings of the 2013 Conference on North Amer-
ican Chapter of the Association for Computational
Linguistics: Human Language Technologies, pages
74–84.

Sam T. Roweis and Lawrence K. Saul. 2000. Nonlin-
ear dimensionality reduction by locally linear em-
bedding. Science, 290(5500):2323–2326.

Lawrence K. Saul and Sam T. Roweis. 2003. Think
globally, fit locally: Unsupervised learning of low
dimensional manifolds. Journal of Machine Learn-
ing Research, 4:119–155.

Geoffrey J. Singh and Ajit P. Gordon. 2008. Relational
learning via collective matrix factorization. In Pro-
ceedings of the 14th ACM SIGKDD International
Conference on Knowledge Discovery and Data Min-
ing, pages 650–658.

Richard Socher, Danqi Chen, Christopher D. Manning,
and Andrew Y. Ng. 2013. Reasoning with neural
tensor networks for knowledge base completion. In
Advances in Neural Information Processing System-
s, pages 926–934.

Ilya Sutskever, Joshua B. Tenenbaum, and Ruslan R.
Salakhutdinov. 2009. Modelling relational data us-
ing bayesian clustered tensor factorization. In Ad-
vances in Neural Information Processing Systems,
pages 1821–1828.

Joshua B. Tenenbaum, Vin De Silva, and John C.
Langford. 2000. A global geometric framework
for nonlinear dimensionality reduction. Science,
290(5500):2319–2323.

Laurens Van der Maaten and Geoffrey Hinton. 2008.
Visualizing data using t-sne. Journal of Machine
Learning Research, 9(85):2579–2605.

93



Quan Wang, Jing Liu, Bin Wang, and Li Guo. 2014a.
A regularized competition model for question diffi-
culty estimation in community question answering
services. In Proceedings of the 2014 Conference on
Empirical Methods in Natural Language Process-
ing, pages 1115–1126.

Zhen Wang, Jianwen Zhang, Jianlin Feng, and Zheng
Chen. 2014b. Knowledge graph embedding by
translating on hyperplanes. In Proceedings of the
28th AAAI Conference on Artificial Intelligence,
pages 1112–1119.

Quan Wang, Bin Wang, and Li Guo. 2015. Knowl-
edge base completion using embeddings and rules.
In Proceedings of the 24th International Joint Con-
ference on Artificial Intelligence.

Jason Weston, Antoine Bordes, Oksana Yakhnenko,
and Nicolas Usunier. 2013. Connecting language
and knowledge bases with embedding models for re-
lation extraction. In Proceedings of the 2013 Con-
ference on Empirical Methods in Natural Language
Processing, pages 1366–1371.

Bishan Yang, Wen-tau Yih, Xiaodong He, Jianfeng
Gao, and Li Deng. 2014. Learning multi-relational
semantics using neural-embedding models. arXiv
preprint arXiv:1411.4072.

Dengyong Zhou, Olivier Bousquet, Thomas Navin
Lal, Jason Weston, and Bernhard Schölkopf. 2004.
Learning with local and global consistency. In Ad-
vances in Neural Information Processing Systems,
pages 321–328.

Xiaojin Zhu and Partha Niyogi. 2005. Harmonic mix-
tures: combining mixture models and graph-based
methods for inductive and scalable semi-supervised
learning. In Proceedings of the 22nd Internation-
al Conference on Machine Learning, pages 1052–
1059.

94


