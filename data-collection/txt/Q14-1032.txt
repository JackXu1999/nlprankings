








































A New Parsing Algorithm for Combinatory Categorial Grammar

Marco Kuhlmann
Department of

Computer and Information Science
Linköping University, Sweden
marco.kuhlmann@liu.se

Giorgio Satta
Department of

Information Engineering
University of Padua, Italy
satta@dei.unipd.it

Abstract

We present a polynomial-time parsing algo-
rithm for CCG, based on a new decomposition
of derivations into small, shareable parts. Our
algorithm has the same asymptotic complex-
ity, O.n6/, as a previous algorithm by Vijay-
Shanker and Weir (1993), but is easier to un-
derstand, implement, and prove correct.

1 Introduction

Combinatory Categorial Grammar (CCG; Steedman
and Baldridge (2011)) is a lexicalized grammar for-
malism that belongs to the class of so-called mildly
context-sensitive formalisms, as characterized by
Joshi (1985). CCG has been successfully used for a
wide range of practical tasks including data-driven
parsing (Clark and Curran, 2007), wide-coverage se-
mantic construction (Bos et al., 2004; Kwiatkowski
et al., 2010; Lewis and Steedman, 2013) and machine
translation (Weese et al., 2012).

Several parsing algorithms for CCG have been
presented in the literature. Earlier proposals show
running time exponential in the length of the input
string (Pareschi and Steedman, 1987; Tomita, 1988).
A breakthrough came with the work of Vijay-Shanker
and Weir (1990) and Vijay-Shanker and Weir (1993)
who report the first polynomial-time algorithm for
CCG parsing. Until this day, this algorithm, which
we shall refer to as the V&W algorithm, remains the
only published polynomial-time parsing algorithm
for CCG. However, we are not aware of any practical
parser for CCG that actually uses it. We speculate
that this has two main reasons: First, some authors

have argued that linguistic resources available for
CCG can be covered with context-free fragments of
the formalism (Fowler and Penn, 2010), for which
more efficient parsing algorithms can be given. Sec-
ond, the V&W algorithm is considerably more com-
plex than parsing algorithms for equivalent mildly
context-sensitive formalisms, such as Tree-Adjoin-
ing Grammar (Joshi and Schabes, 1997), and is quite
hard to understand, implement, and prove correct.

The V&W algorithm is based on a special decom-
position of CCG derivations into smaller parts that
can then be shared among different derivations. This
sharing is the key to the polynomial runtime. In
this article we build on the same idea, but develop
an alternative polynomial-time algorithm for CCG
parsing. The new algorithm is based on a different
decomposition of CCG derivations, and is arguably
simpler than the V&W algorithm in at least two re-
spects: First, the new algorithm uses only three basic
steps, against the nine basic steps of the V&W parser.
Second, the correctness proof of the new algorithm is
simpler than the one reported by Vijay-Shanker and
Weir (1993). The new algorithm runs in time O.n6/
where n is the length of the input string, the same as
the V&W parser.

We organize our presentation as follows. In Sec-
tion 2 we introduce CCG and the central notion of
derivation trees. In Section 3 we start with a simple
but exponential-time parser for CCG, from which
we derive our polynomial-time parser in Section 4.
Section 5 further simplifies the algorithm and proves
its correctness. We then provide a discussion of our
algorithm and possible extensions in Section 6. Sec-
tion 7 concludes the article.

405

Transactions of the Association for Computational Linguistics, 2 (2014) 405–418. Action Editor: Mark Steedman.
Submitted 4/2014; Revised 8/2014; Published 10/2014. c©2014 Association for Computational Linguistics.



2 Combinatory Categorial Grammar

We assume basic familiarity with CCG in general and
the formalism of Weir and Joshi (1988) in particular.
In this section we set up our terminology and notation.
A CCG has two main parts: a lexicon that associates
words with categories, and rules that specify how
categories can be combined into other categories.
Together, these components give rise to derivations
such as the one shown in Figure 1.

2.1 Lexicon

The CCG lexicon is a finite set of word–category
pairs w WD X .1 Categories are built from a finite
set of atomic categories and two binary operators:
forward slash (=) and backward slash ( =). Atomic
categories represent the syntactic types of complete
constituents; they include a distinguished category S
for complete sentences. A constituent with the com-
plex category X=Y represents a function that seeks a
constituent of category Y immediately to its right and
returns a constituent of category X ; similarly, X =Y
represents a function that seeks a Y to its left. We
treat slashes as left-associative operators and omit
unnecessary parentheses. By this convention, every
category X can be written as

X D AjmXm � � � j1X1

where m � 0, A is an atomic category called the
target of X and the jiXi are slash–category pairs
called the arguments of X . We view these arguments
as being arranged in a stack with j1X1 at the top and
jmXm at the bottom. Thus another way of writing
the category X above is as X D A˛, where ˛ is a
(possibly empty) stack of m arguments. The number
m is called the arity of X ; we denote it by ar.X/.

2.2 Rules

The rules of CCG are directed versions of (general-
ized) functional composition. There are two forms,
forward rules and backward rules:

X=Y Y jdYd � � � j1Y1 ) X jdYd � � � j1Y1 .>
d /

Y jdYd � � � j1Y1 X =Y ) X jdYd � � � j1Y1 .<
d /

1The formalism of Weir and Joshi (1988) also allows lexicon
entries for the empty string, a feature that we ignore here.

Louise
..........

NP

might
....

S =NP=.S =NP/

marry
....

S =NP=NP
S =NP=NP >

1

Harry
.......

NP
S =NP >

0

S <
0

Figure 1: A sample derivation tree.

Every rule is obtained by choosing a specific degree
d � 0 and specific directions (forward or backward)
for each of the slashes ji , while X , Y and the Yi
are variables ranging over categories. Thus for every
degree d � 0 there are 2d forward rules and 2d

backward rules. The rules of degree 0 are called
application rules. In contexts where we refer to both
application and composition, we use the latter term
for “proper” composition rules of degree d > 0.
Note that in most of this article we ignore additional
rules required for linguistic analysis with CCG, in
particular type-raising and substitution. We briefly
discuss these rules in Section 6.

Every CCG grammar restricts itself to a finite set
of rules, but each such rule may give rise to infinitely
many rule instances. A rule is instantiated by sub-
stituting concrete categories for the variables. For
example, the derivation in Figure 1 contains the fol-
lowing instance of forward composition (>1):

S =NP=.S =NP/ S =NP=NP) S =NP=NP

Note that we overload the double arrow to denote not
only rules but also rule instances. Given a rule in-
stance, the category that instantiates the pattern X=Y
(forward) or X =Y (backward) is called the primary
input, and the category that instantiates the pattern
Y jdYd � � � j1Y1 is called the secondary input. Adopt-
ing our stack-based view, each rule can be understood
as an operation on argument stacks: pop jY off the
stack of the primary input; pop the jiYi off the stack
of the secondary input and push them to the stack of
the primary input (preserving their order).

The formalism of Weir and Joshi (1988) allows to
restrict the valid instances of individual rules. Similar
to our treatment of additional combinatory rules, in
most of this article we ignore these rule restrictions;
but see the discussion in Section 6.

406



w
....
X

iff w WD X

X=Y Yˇ

Xˇ

t1 t2

iff X=Y Yˇ) Xˇ

Yˇ X =Y

Xˇ

t2 t1

iff Yˇ X =Y ) Xˇ

Figure 2: Recursive definition of derivation trees. Nodes labeled with primary input categories are shaded.

2.3 Derivation Trees
The set of derivation trees of a CCG can be formally
defined as in Figure 2. There and in the remainder
of this article we use ˇ and other symbols from the
beginning of the Greek alphabet to denote a (pos-
sibly empty) stack of arguments. Derivation trees
consist of unary branchings and binary branchings:
unary branchings (drawn with dotted lines) corre-
spond to lexicon entries; binary branchings corre-
spond to (valid instances of) composition rules. The
yield of a derivation tree is the left-to-right sequence
of its leaves. The type of a derivation tree is the
category at its root.

3 CKY-Style Parsing Algorithm

As the point of departure for our own work, we now
introduce a straightforward, CKY-style parsing algo-
rithm for CCGs. It is a simple generalization of the
algorithm presented by Shieber et al. (1995), which
is restricted to grammars with rules of degree 0 or 1.
As in that article, we specify our algorithm in terms
of a grammatical deduction system.

3.1 Deduction System
We are given a CCG and a stringw D w1 � � �wn to be
parsed, where eachwi is a lexical token. As a general
notation, for integers i; j with 0 � i � j � n
we write wŒi; j  to denote the substring wiC1 � � �wj
of w. As usual, we take wŒi; i  to be the empty string.

Items The CKY-style algorithm uses a logic with
items of the form ŒX; i; j  where X is a category
and i; j are fencepost positions in w. The intended
interpretation of such an item is to assert that we
can build a derivation tree with yield wŒi; j  and
type X . The goal of the algorithm is the construction
of the item ŒS; 0; n, which asserts the existence of a
derivation tree for the entire input string. (Recall that
S is the distinguished category for sentences.)

Axioms and Inference Rules The steps of the al-
gorithm are specified by means of inference rules
over items. These rules implement the recursive defi-
nition of derivation trees given in Figure 2. The con-
struction starts with axioms of the form ŒX; i; i C 1
where wiC1 WD X is a lexicon entry; these items
assert the existence of a unary-branching derivation
tree of the form shown in the left of Figure 2 for each
lexical token wiC1. There is one inference rule for
every forward rule (application or composition):

ŒX=Y; i; j  ŒYˇ; j; k

ŒXˇ; i; k
X=Y Yˇ) Xˇ (1)

A symmetrical rule is used for backward application
and composition. However, here and in the remainder
of the article we only specify the forward version of
each rule and leave the backward version implicit.

3.2 Correctness and Runtime
The soundness and completeness of the CKY-style
algorithm can be proved by induction on the number
of inferences and the number of nodes in a derivation
tree, respectively.

It is not hard to see that, in the general case, the
algorithm uses an amount of time and space exponen-
tial with respect to the length of the input string, n.
This is because rule (1) may be used to grow the
arity of primary input categories up to some linear
function of n, resulting in exponentially many cate-
gories.2 Note that this is only possible if there are
rules with degree 2 or more. For grammars restricted
to rules with degree 0 or 1, such as those considered
by Shieber et al. (1995), the runtime of the algorithm
is cubic in n. This restricted class of grammars only
holds context-free generative power, while the power
of general CCG is beyond that of context-free gram-
mars (Vijay-Shanker and Weir, 1994).

2Categories whose arity is not bounded by a linear func-
tion of n are not useful, in the sense that they cannot occur in
complete derivations.

407



wŒi 0; j 0

t 0

wŒi; i 0 wŒj 0; j 

X=Y

c

Xˇ

(a) tree t

wŒi 00; i 0 wŒj 0; j 00

X j1Y

c1

wŒi; i 00 wŒj 00; j 

Xˇj2Z

c2

Xˇ

(b) context c

Figure 3: Decomposition of derivations.

4 A Polynomial-Time Algorithm

We now introduce our polynomial-time algorithm.
This algorithm uses the same axioms and the same
goal item as the CKY-style algorithm, but features
new items and new inference rules.

4.1 New Items
In a first step, in order to avoid runtime exponential
in n we restrict our item set to categories whose arity
is bounded by some grammar constant cG :

ŒX; i; j  where ar.X/ � cG

The exact choice of the constant will be discussed
in Section 5. With the restricted item set, the new
algorithm behaves like the old one as long as the arity
of categories does not exceed cG . However, rule (1)
alone is no longer complete: Derivations with cate-
gories whose arity exceeds cG cannot be simulated
anymore. To remedy this deficiency, we introduce a
new type of item to implement a specific decomposi-
tion of long derivations into smaller pieces.

Consider a derivation t of the form shown in Fig-
ure 3(a). Note that the yield of t is wŒi; j . The
derivation consists of two parts, named t 0 and c; these
share a common node with a category of the form
X=Y . Now assume that c has the special property
that none of the combinatory rules that it uses pops
the argument stack of the category X . This means
that c, after popping the argument =Y , may push
new arguments and pop them again, but may never
“touch” X . We call a fragment with this special prop-
erty a derivation context. (A formal definition will
be given in Section 5.2.)

The special property of c is useful because it im-
plies that c can be carried out for any choice of X .
To be more specific, let us write ˇ for the (possibly
empty) sequence of arguments that c pushes to the
argument stack ofX in place of =Y . We shall refer to
=Y as the bridging argument and to the sequence ˇ
as the excess of c. Suppose now that we replace t 0

by a derivation tree with the same yield but with a
type X 0=Y where X 0 ¤ X . Then because c does not
touch X 0 we obtain another valid derivation tree with
the same yield as t ; the type of this tree will be X 0ˇ.

For the combination with c, the internal structure
of t 0 is of no importance; the only important informa-
tion is the extent of the yield of t 0 and the identity of
the bridging argument =Y . In terms of our deduction
system, this can be expressed as follows: The deriva-
tion context c can be combined with any tree t 0 that
is associated with an item of the form ŒX=Y; i 0; j 0,
where X is any category. Similarly, the internal struc-
ture of c is of no importance either, as long as the
argument stack of the category X remains untouched.
It suffices to record the following:

� the extent of the yield of t , specified in terms of
the positions i and j ;

� the extent of the yield of t 0, specified in terms
of the positions i 0 and j 0;

� the bridging argument =Y ; and

� the excess ˇ.

We represent these pieces of information in a new
type of item of the form Œ=Y; ˇ; i; i 0; j 0; j . The in-
tended interpretation of these items is to assert that,
for any choice of X , if we can build a derivation
tree t 0 with yield wŒi 0; j 0 and type X=Y , then we
can also build a derivation tree t 0 with yield wŒi; j 
and type Xˇ. We also use items Œ =Y; ˇ; i; i 0; j 0; j 
with a backward slash, with a similar meaning. Like
items that represent derivation trees, our items for
derivation contexts are arity-restricted:

ŒjY; ˇ; i; i 0; j 0; j  where ar.Yˇ/ � cG

As we will see in Section 5, these restricted items
suffice to simulate all derivations of a CCG. Further-
more, this can be done in time polynomial in n, be-
cause our encoding allows sharing of the same items
among several derivations.

408



ŒA; 0; 1

ŒS=H =A=F; 2; 5

ŒB; 1; 2

ŒC =A=F; 2; 3

ŒS=E; 3; 4 ŒE=H =C; 4; 5

ŒS=H =C; 3; 5
(1)

ŒS=H =A=F; 2; 5
(1)

ŒF =G =B; 5; 6

Œ=F; =G =B; 2; 2; 5; 6
(2) 

Œ=F; =G; 1; 2; 5; 6
(4)

ŒG; 6; 7

Œ=F; "; 1; 2; 5; 7
(4)

ŒS=H =A; 1; 7
(3) 

ŒS=H; 0; 7
(1)

ŒH; 7; 8

ŒS; 0; 8
(1)

Figure 4: A sample derivation of the grammatical deduction system of Section 4. Inference  triggers a new
context item from a tree item; inference  reuses the tree item (as indicated by the arrow), recombining it
with the (modified) context item.

4.2 New Inference Rules
In our parsing algorithm, context items are introduced
whenever the composition of two categories whose
arities are bounded by cG would result in a category
whose arity exceeds this bound:

ŒX=Y; i; j  ŒYˇ; j; k

Œ=Y; ˇ; i; i; j; k

8<:X=Y Yˇ) Xˇar.Xˇ/ > cG (2)
The new rule has the same antecedents as rule (1), but
rather than extending the derivation asserted by the
first antecedent ŒX=Y; i; j , which is not possible be-
cause of the arity bound, it triggers a new derivation
context, asserted by the item Œ=Y; ˇ; i; i; j; k. Fur-
ther applications and compositions will extend the
new context, and only when the excess of this context
has become sufficiently small will it be recombined
with the derivation that originally triggered it. This
is done by the following rule:

ŒX jY; i 0; j 0 ŒjY; ˇ; i; i 0; j 0; j 

ŒXˇ; i; j 
(3)

Note that this rule (like all rules in the deduction
system) is only defined on valid items; in particular it
only fires if the arity of the category Xˇ is bounded
by cG .

The remaining rules of the algorithm parallel the
three rules that we have introduced so far but take
items that represent derivation contexts rather than
derivation trees as their first antecedents. First out,
rule (4) extends a derivation context in the same way
as rule (1) extends a derivation tree.

ŒjY; ˇ=Z; i; i 0; j 0; j  ŒZ; j; k

ŒjY; ˇ; i; i 0; j 0; k

8<:X=Z Z ) Xar.Yˇ/ � cG (4)

Rule (5) is the obvious correspondent of rule (2): It
triggers a new context when the antecedent context
cannot be extended because of the arity bound.

ŒjY; ˇ=Z; i; i 0; j 0; j  ŒZ; j; k

Œ=Z; ; i; i; j; k

8<:X=Z Z ) Xar.Yˇ/ > cG (5)
Finally, and parallel to rule (3), we need a rule to
recombine a context with the context that originally
triggered it. As it will turn out, we only need this in
cases where the triggered context has no excess.

Œj1Y; ˇj2Z; i
00; i 0; j 0; j 00 Œj2Z; "; i; i

00; j 00; j 

Œj1Y; ˇ; i; i
0; j 0; j 

(6)

4.3 Sample Derivation

We now illustrate our algorithm on a toy grammar.
The grammar has the following lexicon:

w1 WD A w5 WD E=H =C

w2 WD B w6 WD F=G =B

w3 WD C =A=F w7 WD G

w4 WD S=E w8 WD H

The start symbol is S . The grammar allows all in-
stances of application and all instances of composi-
tion with degree bounded by 2. We let cG D 3 (as
explained later in Section 5.2).

A derivation of our deduction system on the in-
put string w1 � � �w8 is given in Figure 4. We start
by applying rule (1) twice (once forward, once back-
ward) to obtain the item ŒS=H =A=F; 2; 5. Com-
bining this item with the axiom ŒF =G =B; 5; 6 is
not possible using rule (1), as this would result in

409



a category with arity 4, exceeding the arity bound.
We therefore use rule (2) to trigger the context
item Œ=F; =G =B; 2; 2; 5; 6 (). Successively, we use
rule (4) twice to obtain the item Œ=F; "; 1; 2; 5; 7. At
this point we use rule (3) (with ˇ D ") to recombine
the context item with the tree item that originally
triggered it (); this yields the item ŒS=H =A; 1; 7.
Note that the recombination effectively retrieves the
portion of the stack that was below the argument =F
when the context item was triggered in . Double ap-
plication of rule (1) produces the goal item ŒS; 0; 8.

4.4 Runtime Analysis
We now turn to an analysis of the runtime complexity
of our algorithm. We first consider runtime complex-
ity with respect to the length of the input string, n.
The runtime is dominated by the number of instantia-
tions of rule (6) which involves two context items as
antecedents. By inspection of this rule, we see that
the number of possible instantiations is bounded by
n6. Therefore we conclude that the algorithm runs in
time O.n6/.

We now consider runtime complexity with respect
to the size of the input grammar. Here the runtime is
dominated by the number of instantiations of rules
(1)–(5). For example, rule (5) combines items

ŒjY; ˇ=Z; i; i 0; j 0; j  and ŒZ; j; k :

By our restrictions on items, both the arity of Yˇ=Z
and the arity of Z are upper-bounded by the con-
stant cG . Now recall that every category X can be
written as X D A˛ for some atomic category A and
stack of arguments ˛. Let A be the set of atomic
categories in the input grammar, and let L be the
set of all arguments occurring in any category of
the lexicon. By a result of Vijay-Shanker and Weir
(1994, Lemma 3.1), every argument that may occur
in a derived category occurs in L. Then the number
of possible instantiations of rule (5) as well as rules
(1)–(4), and hence the runtime of the algorithm, is in

O.jAj jLjcG � jLjcG / D O.jAj jLj2cG / :
Note that both A and L may grow with the grammar
size. As we will see in Section 5.2, the constant cG
also depends on the grammar size. This means that
the worst-case runtime complexity of our parser is
exponential in the size of the input grammar. We will
return to this point in Section 7.

5 Correctness

In this section we prove the correctness of our parsing
algorithm. In order to simplify the proofs, we start
by simplifying our algorithm, at the cost of making
it less efficient:

� We remove the rules for extending trees and
contexts, rule (1) and rule (4).

� We conflate the rules for triggering contexts,
rule (2) and rule (5), into the single rule

ŒYˇ; j; k

Œ=Y; ˇ; i; i; j; k
X=Y Yˇ) Xˇ (7)

This rule blindly guesses the extension of the
triggering tree or context (specified by posi-
tions i and j ), rather than waiting for a cor-
responding item to be derived.

The simplified algorithm is specified in Figure 5.
We now argue that this algorithm and the algorithm
from Section 4 parse exactly the same derivation
trees, although they use different parsing strategies.
First, we observe that rule (1) in the old algorithm
can be simulated by a combination of other rules in
the simplified algorithm, as follows:

ŒX=Y; i; j 

ŒYˇ; j; k

Œ=Y; ˇ; i; i; j; k
(7)

ŒXˇ; i; k
(3)

Furthermore, the simplified algorithm does no longer
need rule (4), whose role is now taken over by
rule (7). To see this, recall that rule (4) extends an
existing context whenever the composition of two
categories results in a new category whose arity does
not exceed cG . In contrast, rule (7) always triggers a
new context c, even if the result of the composition
of c with some existing context satisfies the above
arity restriction. Despite the difference in the adopted
strategy, these two rules are equivalent in terms of
stack content, leading to the same derivation trees.

5.1 Definitions
We introduce some additional terminology and nota-
tion that we will use in the proofs. For a derivation
tree t and a node u of t , we write t Œu to denote the
category at u, and we write t ju to denote the sub-
tree of t at u. Formally, tju is the restriction of t to
node u and all of its descendants. Each subtree of a
derivation tree is another derivation tree.

410



Items of type 1: ŒX; i; j , ar.X/ � cG Items of type 2: ŒjY; ˇ; i; i 0; j 0; j , ar.Yˇ/ � cG

Axioms: ŒX; i; i C 1, wiC1 WD X Goal: ŒS; 0; n

Inference rules:
ŒYˇ; j; k

Œ=Y; ˇ; i; i; j; k
X=Y Yˇ) Xˇ (7)

ŒX=Y; i 0; j 0 Œ=Y; ˇ; i; i 0; j 0; j 

ŒXˇ; i; j 
(3)

Œj1Y; ˇj2Z; i
00; i 0; j 0; j 00 Œj2Z; "; i; i

00; j 00; j 

Œj1Y; ˇ; i; i
0; j 0; j 

(6)

Figure 5: Items and inference rules of the simplified algorithm for an input string w1 � � �wn.

Definition 1 Let t be a derivation tree with root r .
Then t has signature ŒX; i; j  if

1. the yield of t is wŒi; j  and

2. the type of t is X , that is, t Œr D X .

Note that while we use the same notation for sig-
natures as for items, the signature of a derivation tree
is a purely structural concept, whereas an item is an
object in the algorithm.

A central concept in our proof is the notion of
spine. Recall that a derivation tree consists of unary
branchings and binary branchings. In each binary
branching, we refer to the two children of the branch-
ing’s root node as the primary child and the sec-
ondary child, depending on which of the two is la-
beled with the primary and secondary input category
of the corresponding rule instance. In Figure 2, the
primary children of the root node are shaded.

Definition 2 For a derivation tree t , the spine of t is
the unique path that starts at the root node of t and at
each node u continues to the primary child of u.

The spine of a derivation tree always ends at a
node that is labeled with a category from the lexicon.

Definition 3 Let t be a derivation tree with root r .
A derivation context c is obtained by removing all
proper descendants of some node f ¤ r on the spine
of t , under the restriction that ar.t Œu/ > ar.t Œr/ for
every node u on the spine properly between f and r .

The node f is called the foot node of c. The yield
of c is the pair whose first component is the yield
of t to the left of f and whose second component
is the yield of t to the right of f . For a derivation
context c and a node u of c, we write cŒu to denote
the category at u.

Definition 3 formalizes the concept of derivation
contexts that we introduced in Section 4.1. First,
because f is on the spine and f ¤ r , the cate-
gory cŒf  takes the form X=Y . The arity restric-
tion implies that the category of every node u on the
spine properly between f and r takes the form Xˇu,
jˇuj > 0, and that the category at the root takes the
form Xˇ, jˇj � 0. Thus the category X is never
exposed in c, except perhaps at r . As we will see
in Section 5.4, this property, together with a careful
selection of “split nodes”, will allow us to decompose
derivations into smaller, shareable parts. The basic
idea is the same as in the tabulation of pushdown
automata (Lang, 1974; Nederhof and Satta, 2004),
where the pushdown in our case is the argument stack
of the primary input categories along a spine.

The concepts of signature and spine are general-
ized to derivation contexts as follows:

Definition 4 Let c be a derivation context with root
node r and foot node f . Then c has signature
ŒjY; ˇ; i; i 0; j 0; j  if

1. the yield of c is .wŒi; i 0; wŒj 0; j /;

2. for some X , cŒf  D X jY and cŒr D Xˇ.

Definition 5 For a derivation context c, the spine
of c is the path from its root node to its foot node.

5.2 Grammar Constant
Before we start with the proof as such, we turn to
the choice of the grammar constant cG , which was
left pending in previous sections. Recall that we
are using cG as a bound on the arity of X in type 1
items ŒX; i; j . Since these items are produced by
our axioms from the set of categories in the lexicon,
cG must not be smaller than the maximum arity ` of
a category in this finite set.

411



We also use cG as a bound on the arity of the cat-
egory Yˇ in type 2 items ŒjY; ˇ; i; i 0; j 0; j . These
items are produced by inference rule (7) to simulate
instances of composition of the form X=Y Yˇ )
Xˇ. Here the length of ˇ is bounded by the maxi-
mum degree d of a composition rule in the grammar,
and ar.Y / is bounded by the maximum arity a of an
argument from the (finite) set L of arguments in the
lexicon (recall Section 4.4). Therefore cG cannot be
smaller than aC d . Putting everything together, we
obtain the condition

cG � maxf`; aC dg: (8)

The next lemma will be used in several places later.

Lemma 1 Let c be a derivation context with signa-
ture ŒjY; ˇ; i; i 0; j 0; j . Then ar.Yˇ/ � cG .

Proof. Let r and f be the root and the foot node,
respectively, of c. From the definition of signature,
there must be some X such that cŒr D Xˇ and
cŒf  D X jY . Now let p be the parent node of f ,
and assume that the rule used at p is instantiated as
X=Y Yˇ0 ) Xˇ0, so that cŒp D Xˇ0. If p D r
then ˇ0 D ˇ; otherwise, because of the arity restric-
tion in the definition of derivation contexts (Defini-
tion 3) we have jˇ0j > jˇj. Then

ar.Yˇ/ � ar.Yˇ0/ � cG ;

where the right inequality follows from the assump-
tion that X=Y Yˇ0 ) Xˇ0 is a rule instance of the
grammar, and from inequality (8).

5.3 Soundness

We start the correctness proof by arguing for the
soundness of the deduction system in Figure 5. More
specifically, we show that for every item of type 1
there exists a derivation tree with the same signa-
ture, and that for every item of type 2 there exists a
derivation context with the same signature.

The soundness of the axioms is obvious.
Rule (7) states that, if we have built a derivation

tree t with signature ŒYˇ; j; k then we can build a
derivation context c with signature Œ=Y; ˇ; i; i; j; k.
Under the condition that the grammar admits the rule
instance X=Y Yˇ ) Xˇ, this inference is sound;
the context can be built as shown in Figure 6.

wŒj; k

YˇX=Y

Xˇ

t

Figure 6: Soundness of rule (7).

Rule (3) states that, if we have built a derivation
tree t 0 with signature ŒX=Y; i 0; j 0 and a context c
with signature Œ=Y; ˇ; i; i 0; j 0; j , then we can build
a new tree t with signature ŒXˇ; i; j . We obtain t by
substituting t 0 for the foot node of c (see Figure 3(a)).

Rule (6) states that, if we have built a derivation
context c1 with signature Œj1Y; ˇj2Z; i 00; i 0; j 0; j 00
and another context c2 with signature
Œj2Z; "; i; i

00; j 00; j , then we can build a derivation
context c with signature Œj1Y; ˇ; i; i 0; j 0; j . We
obtain c by substituting c1 for the foot node of c2;
this is illustrated by Figure 3(b) if we assume  D ".

5.4 Completeness

In the final part of our correctness proof we now
prove the completeness of the deduction system in
Figure 5. Specifically we show the following stronger
statement: For every derivation tree and for every
derivation context with a signature I satisfying the
arity bounds for items of Figure 5, the deduction
system infers the corresponding item I . From this
statement we can immediately conclude that the sys-
tem constructs the goal item whenever there exists
a derivation tree whose yield is the complete input
string and whose type is the distinguished category S .

Our proof is by induction on a measure that we
call rank. The rank of a derivation tree or context
is the number of its non-leaf nodes. Note that this
definition implies that the foot node of a context is
not counted against its rank. The rank of a tree or
context is always at least 1, with rank 1 only realized
for derivation trees consisting of a single node.

5.4.1 Base Case
Consider a derivation tree t with signature ŒX; i; j 

and rank.t/ D 1. The tree t takes the form shown
in the left of Figure 2, and we have j D i C 1 and
wj WD X . The item ŒX; i; j  is then produced by one
of the axioms of our deduction system.

412



5.4.2 Inductive Case
The general idea underlying the inductive case can

be stated as follows. We consider a derivation tree
or context ' with signature I satisfying the bounds
stated in Figure 5 for items of type 1 or 2. We then
identify a special node s in '’s spine, which we call
the split node. We use s to “split” ' into two parts
that are either derivation trees or contexts, that both
satisfy the bounds for items of type 1 or 2, and that
both have rank smaller than the rank of '. We then
apply the induction hypothesis to obtain two items
that can be combined by one of the inference rules of
our algorithm, resulting in the desired item I for '.
We first consider the case in which ' is a tree, and
later the case in which ' is a context.

5.4.3 Splitting Trees
Consider a derivation tree t with signature

ŒX 0; i; j , root node r , and rank.t/ > 1. Then the
spine of t consists of at least 2 nodes. Now assume
that ar.X 0/ � cG , that is, ŒX 0; i; j  is a valid item.

Choose the split node s to be the highest (closest
to the root) non-root node on the spine for which
ar.t Œs/ � cG . Node s always exists, as the arity
constraint is satisfied at least for the lowest (farthest
from the root) node on the spine, which is labeled
with a category from the lexicon.

Consider the subtree t 0 D t js; thus s is the root
node of t 0. Because s is a primary node in t , the
category at s has at least one argument. We deal
here with the case where this category takes the
form t Œs D X=Y ; the case t Œs D X =Y is sym-
metrical. Thus the signature of t 0 takes the form
ŒX=Y; i 0; j 0 where i 0; j 0 are integers with i � i 0 <
j 0 � j . By our choice of s, ar.X=Y / � cG , and
therefore ŒX=Y; i 0; j 0 is a valid item. Furthermore,
rank.tjs/ < rank.t/, as r does not belong to tjs . We
may then use the induction hypothesis to deduce that
our algorithm constructs the item ŒX=Y; i 0; j 0.

Now consider the context c that is obtained from t
by removing all proper descendants of s; thus r is the
root node of c and s is its foot node. To see that c is
well-defined, note that our choice of s guarantees that
ar.t Œu/ > ar.t Œr/ for every node u that is properly
between s and r : If there was a node u such that
ar.t Œu/ � ar.t Œr/ then because of t Œr D X 0 and
our assumption that ar.X 0/ � cG we would have
chosen u instead of s.

Now let ˇ be the excess of c; then X 0 D t Œr D
t Œsˇ D Xˇ. Thus the signature of c takes the form
Œ=Y; ˇ; i; i 0; j 0; j . Applying Lemma 1 to c, we get
ar.Yˇ/ � cG , and therefore Œ=Y; ˇ; i; i 0; j 0; j  is
also a valid item. Furthermore, rank.c/ < rank.t/,
since node s is counted in rank.t/ but not in rank.c/.
By the induction hypothesis, we conclude that our
algorithm constructs the item Œ=Y; ˛; i; i 0; j 0; j .

Finally, we apply the inference rule (3) to
the previously constructed items ŒX=Y; i 0; j 0 and
Œ=Y; ˇ; i; i 0; j 0; j . This yields the item ŒXˇ; i; j  D
ŒX 0; i; j  for t , as desired.

5.4.4 Splitting Contexts
Consider a derivation context c with signature

Œ=Y; ˇ; i; i 0; j 0; j , root r , and foot f . (The case
where we have =Y instead of =Y can be covered
with a symmetrical argument.) From Definition 4 we
know that there is a category X such that cŒf  D
X=Y and cŒr D Xˇ, and from the definition of
context we know that for every spinal node u that is
properly between f and r it holds that ar.cŒu/ >
ar.cŒr/. Now assume that ar.Yˇ/ � cG , that is,
Œ=Y; ˇ; i; i 0; j 0; j  is a valid item. We distinguish two
cases below.

Case 1 Suppose that the spine of c consists of ex-
actly 2 nodes. In this case the foot f is the left child
of the root r and i D i 0. Let f 0 be the right sibling
of f and consider the subtree t 0 D t jf 0 ; thus f 0

is the root node of t 0. The signature of t 0 takes the
form ŒYˇ; j 0; j . By our assumption, ar.Yˇ/ � cG ,
and then ŒYˇ; j 0; j  is a valid item. Furthermore,
rank.t 0/ < rank.c/ since the root node r is counted
in rank.c/ but not in rank.t 0/. Then, by the induction
hypothesis, the item ŒYˇ; j 0; j  is constructed by our
algorithm. We now apply inference rule (7) to this
item; this yields the item Œ=Y; ˇ; i; i 0; j 0; j  for c, as
required.

Case 2 Suppose that the spine of c consists of more
than 2 nodes. This means that there is at least one
spinal node that is properly between f and r .

Choose the split node s to be the deepest (farthest
from the root) node properly between f and r for
which ar.cŒs/ D ar.cŒr/ C 1. Node s always ex-
ists, as the arity constraint is satisfied at least for the
primary child of r . This is because the definition
of context states that ar.cŒu/ > ar.cŒr/ for every

413



node u in the spine, and at the same time, no combi-
natory rule can reduce the arity of its primary input
category by more than one unit.

Consider the context c1 that is obtained by restrict-
ing c to node s and all of its descendants; thus s
is the root node of c1 and f is the foot node. To
see that c1 is well-defined, note that our choice of s
guarantees that ar.cŒu/ > ar.cŒs/ for every node
u that is properly between f and s. To see this,
suppose that there was a node u ¤ s such that
ar.cŒu/ � ar.cŒs/. Since ar.cŒs/ D ar.cŒr/C 1
and ar.cŒu/ ¤ ar.cŒs/, by our definition of s, we
would have ar.cŒu/ � ar.cŒr/, which cannot be
because in c, every node u properly between f and
s has arity ar.cŒu/ > ar.cŒr/.

Because f is a primary node in c, the category at
f has at least one argument; call it j1Y . The node
s is a primary node in c as well, so the excess of
c1 takes the form ˇj2Z, where j2Z is the topmost
argument of the category at s. Thus the signature
of c1 takes the form Œj1Y; ˇj2Z; i 00; i 0; j 0; j 00 where
i 00; j 00 are integers with i � i 00 � i 0 and j 0 � j 00 �
j . Applying Lemma 1 to c1, we get ar.Yˇj2Z/ �
cG , and therefore Œj1Y; ˇj2Z; i 00; i 0; j 0; j 00 is a valid
item. Finally, we note that rank.c1/ < rank.c/, since
the root node r is counted in c but not in c1. By the
induction hypothesis we conclude that our algorithm
constructs the item Œj1Y; ˇj2Z; i 00; i 0; j 0; j 00.

Now consider the context c2 that is obtained from
c by removing all proper descendants of the node s;
thus r is the root node of c2 and s is the foot node.
To see that c2 is well-defined, note that ar.cŒu/ >
ar.cŒr/ for every node u that is properly between s
and r simply because every such node is also properly
between f and r . The excess of c2 is the empty
stack " by our choice of s. Thus the signature of
c2 is Œj2Z; "; i; i 00; j 00; j . We apply Lemma 1 once
more, this time to c2, to show that ar.Z/ � cG , and
conclude that Œj2Z; "; i; i 00; j 00; j  is also a valid item.
Finally we note that rank.c2/ < rank.c/, as the node
s is counted in c but not in c2. By the induction
hypothesis we conclude that our algorithm constructs
the item Œj2Z; "; i; i 00; j 00; j .

We now apply inference rule (6) to the pre-
viously constructed items Œj1Y; ˇj2Z; i 00; i 0; j 0; j 00
and Œj2Z; "; i; i 00; j 00; j . This yields the item
Œj1Y; ˇ; i; i

0; j 0; j  as desired.

6 Discussion

We round off the article with a discussion of our
algorithm and possible extensions.

6.1 Support for Rule Restrictions

As we mentioned in Section 2, the CCG formalism of
Weir and Joshi (1988) allows a grammar to impose
certain restrictions on valid rule instances. More
specifically, for every rule a grammar may restrict
(a) the target of the primary input category and/or
(b) parts of or the entire secondary input category.3

The algorithm in Figure 5 can be extended to sup-
port such rule restrictions. Note that already in its
present form, the algorithm only allows inferences
that are licensed by valid instances of a given rule.
Supporting restrictions on the secondary input cat-
egory (restrictions of type b) is straightforward—
assuming that these restrictions can be efficiently
tested. To also support restrictions on the target of
the primary input category (restrictions of type a) the
items can be extended by an additional component
that keeps track of that target category for the cor-
responding derivation subtree or context. With this
information, rule (7) can perform a check against the
restrictions specified for the composition rule, and
rules (3) and (6) merely need to test whether the tar-
get categories of their two antecedents match, and
propagate the common target category to the conclu-
sion. This is essentially the same solution as the one
adopted in the V&W algorithm.

6.2 Support for Multi-Modal CCG

The modern version of CCG has abandoned rule
restrictions in favor of a new, lexicalized control
mechanism in the form of modalities or slash types
(Steedman and Baldridge, 2011). However, as shown
by Baldridge and Kruijff (2003), every multi-modal
CCG can be translated into an equivalent CCG with
rule restrictions. The basic idea is to specialize the
target of each category and argument for a slash type,
and to reformulate the multi-modal rules as rules with
restrictions that reference this information. With this
simulation, our parsing algorithm can also be used as
a parsing algorithm for multi-modal CCG.

3Such restrictions can be used, for example, to impose the
linguistically relevant distinction between harmonic and crossed
forms of composition.

414



6.3 Comparison with the V&W Algorithm

As already mentioned in Section 1, apart from the
algorithm presented in this article, the only other
algorithm known to run in polynomial time in the
length of the input string is the one presented by
Vijay-Shanker and Weir (1993). At an abstract level,
the two algorithms are based on the same basic idea
of decomposing CCG derivations into pieces of two
different types, one of which spans a portion of the
input string that includes a gap. This idea actually
underlies several parsing algorithms for equivalent
mildly context-sensitive formalisms, such as Tree-
Adjoining Grammar (Joshi and Schabes, 1997).

The main difference between the V&W algorithm
and the one presented in this article is the use of
different decompositions for CCG derivations. In
our algorithm we allow the excess of a derivation
context to be the empty list of arguments, something
that is not possible in the V&W algorithm. There,
when an application operation empties the excess
ˇ of some context, one is forced to retrieve, in the
same elementary step, the portion of the stack placed
right below ˇ. This requires the distinction of several
possible cases, resulting in four different realizations
of the application rule (Vijay-Shanker and Weir, 1993,
p. 616). As a consequence, the V&W algorithm uses
nine (forward) inference rules, against our algorithm
in Figure 5 which uses only three. Furthermore, some
of the inference rules in the V&W algorithm use three
antecedent items, while our use a maximum of two.
This results in a runtime complexity of O.n7/ for
the V&W algorithm, n the length of the input string;
however, Vijay-Shanker and Weir (1993) show how
their algorithm can be implemented in time O.n6/ at
the cost of some extra bookkeeping. In contrast, our
algorithm directly runs in time O.n6/.

The relative proliferation of inference rules, com-
bined with the increase in their complexity, makes,
in our own opinion, the specification of the V&W
parser more difficult to understand and implement,
and calls for a more articulated correctness proof.

6.4 Support for Additional Types of Rules

Like the V&W algorithm, our algorithm currently
only supports (generalized) composition but no other
combinatory rules required for linguistic analysis, in
particular type-raising and substitution.

Type-raising is a unary rule of the (forward) form
X ) T =.T =X/ where T is a variable over cate-
gories. Under the standard assumption that T =X is
limited to a finite set of categories (Steedman, 2000),
this rule can be implemented in our algorithm by
introducing a new unary inference rule and choos-
ing the constant cG large enough to accomodate all
instances of T =X .

Substitution is a binary rule of the (forward) form
X=Y jZ Y jZ ) X=Z. This rule is easy to imple-
ment if both =Y and jZ are stored in the same item.
Otherwise, we need to pass jZ to any item storing
the =Y . This can be done by changing the second
antecedent of rule (6) to allow a single argument
jZ instead of the empty excess ". The price of this
change is spurious ambiguity in the derivations of the
grammatical deduction system.

7 Conclusion

Recently, there has been a surge of interest in
the mathematical properties of CCG; see for in-
stance Hockenmaier and Young (2008), Koller and
Kuhlmann (2009), Fowler and Penn (2010) and
Kuhlmann et al. (2010). Following this line, this
article has revisited the parsing problem for CCG.

Our work, like the polynomial-time parsing algo-
rithm previously discovered by Vijay-Shanker and
Weir (1993), is based on the idea of decomposing
large CCG derivations into smaller, shareable pieces.
Here we have proposed a derivation decomposition
different from the one adopted by Vijay-Shanker and
Weir (1993). This results in an algorithm which, in
our own opinion, is simpler and easier to understand.

Although we have specified only a recognition
version of the algorithm, standard techniques can be
applied to obtain a derivation forest from our parsing
table. This consists in saving backpointers at each
inference rule, linking newly inferred items to their
antecedent items.

As observed in Section 4.4, the worst case run-
time of our algorithm is exponential in the size of the
grammar. The same holds true for the algorithm of
Vijay-Shanker and Weir (1993). We are not aware of
any published discussion on this issue, and we there-
fore leave as an open problem the question whether
CCG parsing can be done in polynomial time when
the grammar is considered as part of the input.

415



Acknowledgments

We thank the Action Editor and the reviewers for their
detailed and insightful feedback on the first version
of this article. GS has been partially supported by
MIUR under project PRIN No. 2010LYA9RH_006.

References
Jason Baldridge and Geert-Jan M. Kruijff. 2003. Multi-

modal combinatory categorial grammar. In Tenth Con-
ference of the European Chapter of the Association
for Computational Linguistics (EACL), pages 211–218,
Budapest, Hungary.

Johan Bos, Stephen Clark, Mark Steedman, James R. Cur-
ran, and Julia Hockenmaier. 2004. Wide-coverage
semantic representations from a CCG parser. In Pro-
ceedings of the 20th International Conference on Com-
putational Linguistics (COLING), pages 1240–1246,
Geneva, Switzerland.

Stephen Clark and James R. Curran. 2007. Wide-
coverage efficient statistical parsing with CCG and log-
linear models. Computational Linguistics, 33(4):493–
552.

Timothy Fowler and Gerald Penn. 2010. Accurate
context-free parsing with combinatory categorial gram-
mar. In Proceedings of the 48th Annual Meeting of
the Association for Computational Linguistics (ACL),
pages 335–344, Uppsala, Sweden.

Julia Hockenmaier and Peter Young. 2008. Non-local
scrambling: The equivalence of TAG and CCG revis-
ited. In Ninth International Workshop on Tree Ad-
joining Grammars and Related Formalisms (TAG+),
Tübingen, Germany.

Aravind K. Joshi and Yves Schabes. 1997. Tree-
Adjoining Grammars. In Grzegorz Rozenberg and Arto
Salomaa, editors, Handbook of Formal Languages, vol-
ume 3, pages 69–123. Springer.

Aravind K. Joshi. 1985. Tree Adjoining Grammars: How
much context-sensitivity is required to provide reason-
able structural descriptions? In David R. Dowty, Lauri
Karttunen, and Arnold M. Zwicky, editors, Natural Lan-
guage Parsing, pages 206–250. Cambridge University
Press.

Alexander Koller and Marco Kuhlmann. 2009. Depen-
dency trees and the strong generative capacity of CCG.
In Proceedings of the 12th Conference of the European
Chapter of the Association for Computational Linguis-
tics (EACL), pages 460–468, Athens, Greece.

Marco Kuhlmann, Alexander Koller, and Giorgio Satta.
2010. The importance of rule restrictions in CCG. In
Proceedings of the 48th Annual Meeting of the Asso-
ciation for Computational Linguistics (ACL), pages
534–543, Uppsala, Sweden.

Tom Kwiatkowski, Luke Zettlemoyer, Sharon Goldwater,
and Mark Steedman. 2010. Inducing probabilistic
CCG grammars from logical form with higher-order
unification. In Proceedings of the 2010 Conference on
Empirical Methods in Natural Language Processing
(EMNLP), pages 1223–1233, Cambridge, MA, USA.

Bernard Lang. 1974. Deterministic techniques for ef-
ficient non-deterministic parsers. In Jacques Loecx,
editor, Automata, Languages and Programming, 2nd
Colloquium, University of Saarbrücken, July 29–August
2, 1974, number 14 in Lecture Notes in Computer Sci-
ence, pages 255–269. Springer.

Mike Lewis and Mark Steedman. 2013. Combined distri-
butional and logical semantics. Transactions of the As-
sociation for Computational Linguistics, 1(May):179–
192.

Mark-Jan Nederhof and Giorgio Satta. 2004. Tabular
parsing. In Carlos Martín-Vide, Victor Mitrana, and
Gheorghe Păun, editors, Formal Languages and Appli-
cations, volume 148 of Studies in Fuzziness and Soft
Computing, pages 529–549. Springer.

Remo Pareschi and Mark Steedman. 1987. A lazy way to
chart-parse with categorial grammars. In Proceedings
of the 25th Annual Meeting of the Association for Com-
putational Linguistics (ACL), pages 81–88, Stanford,
CA, USA.

Stuart M. Shieber, Yves Schabes, and Fernando Pereira.
1995. Principles and implementation of deductive pars-
ing. Journal of Logic Programming, 24(1–2):3–36.

Mark Steedman and Jason Baldridge. 2011. Combinatory
categorial grammar. In Robert D. Borsley and Kersti
Börjars, editors, Non-Transformational Syntax: Formal
and Explicit Models of Grammar, chapter 5, pages 181–
224. Blackwell.

Mark Steedman. 2000. The Syntactic Process. MIT
Press.

Masaru Tomita. 1988. Graph-structured stack and natural
language parsing. In Proceedings of the 26th Annual
Meeting of the Association for Computational Linguis-
tics (ACL), pages 249–257, Buffalo, USA.

K. Vijay-Shanker and David J. Weir. 1990. Polynomial
time parsing of combinatory categorial grammars. In
Proceedings of the 28th Annual Meeting of the Associa-
tion for Computational Linguistics (ACL), pages 1–8,
Pittsburgh, USA.

K. Vijay-Shanker and David J. Weir. 1993. Parsing some
constrained grammar formalisms. Computational Lin-
guistics, 19(4):591–636.

K. Vijay-Shanker and David J. Weir. 1994. The equiv-
alence of four extensions of context-free grammars.
Mathematical Systems Theory, 27(6):511–546.

Jonathan Weese, Chris Callison-Burch, and Adam Lopez.
2012. Using categorial grammar to label translation

416



rules. In Proceedings of the Seventh Workshop on Sta-
tistical Machine Translation, pages 222–231, Montréal,
Canada.

David J. Weir and Aravind K. Joshi. 1988. Combinatory
categorial grammars: Generative power and relation-
ship to linear context-free rewriting systems. In Pro-
ceedings of the 26th Annual Meeting of the Association
for Computational Linguistics (ACL), pages 278–285,
Buffalo, USA.

417



418


