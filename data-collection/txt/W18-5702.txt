



















































Autonomous Sub-domain Modeling for Dialogue Policy with Hierarchical Deep Reinforcement Learning


Proceedings of the 2018 EMNLP Workshop SCAI: The 2nd Int’l Workshop on Search-Oriented Conversational AI, pages 9–16
Brussels, Belgium, October 31, 2018. c©2018 Association for Computational Linguistics

ISBN 978-1-948087-75-9

9

Autonomous Sub-domain Modeling for Dialogue Policy with Hierarchical
Deep Reinforcement Learning

Giovanni Yoko Kristianto1, Huiwen Zhang2, 3, Bin Tong1,
Makoto Iwayama1, Yoshiyuki Kobayashi1

1Hitachi Central Research Laboratory, Tokyo, Japan
2Shenyang Institute of Automation, Chinese Academy of Sciences, Shenyang, China

3University of Chinese Academy of Sciences, Beijing, China
{yokogiovanni.kristianto.oq, bin.tong.hh, makoto.iwayama.nw, yoshiyuki.kobayashi.gp}@hitachi.com

zhanghuiwen@sia.cn

Abstract

Solving composites tasks, which consist of
several inherent sub-tasks, remains a challenge
in the research area of dialogue. Current stud-
ies have tackled this issue by manually de-
composing the composite tasks into several
sub-domains. However, much human effort
is inevitable. This paper proposes a dialogue
framework that autonomously models mean-
ingful sub-domains and learns the policy over
them. Our experiments show that our frame-
work outperforms the baseline without sub-
domains by 11% in terms of success rate, and
is competitive with that with manually defined
sub-domains.

1 Introduction

Modeling a composite dialogue (Peng et al.,
2017), which consists of several inherent sub-
tasks, is in high demand due to the complexity
of human conversation. For instance, a compos-
ite dialogue of making a hotel reservation involves
several sub-tasks, such as looking for a hotel that
meets the user’s constraints, booking the room,
and paying for the room. The completion of a
composite dialogue requires the fulfillment of all
involved sub-tasks. In this paper, we focus on the
development of a dialogue agent that can discover
inherent sub-tasks autonomously from a compos-
ite domain, learn a policy to fulfill each sub-task,
and learn a policy among these sub-tasks to solve
the composite task. Composite dialogues are dif-
ferent from multi-domain dialogues. In multi-
domain dialogue systems (Cuayáhuitl et al., 2016;
Gasic et al., 2016), each dialogue typically in-
volves one domain, and consequently, its fulfill-
ment does not need policy across domains.

To develop a dialogue agent that can handle a
composite task, using standard flat reinforcement
learning (RL), which are often used for dialogues
with a simple task (Young et al., 2013; Gašić and

Young, 2014; Williams et al., 2017; Casanueva
et al., 2017; Li et al., 2017), might be inappro-
priate. Flat RL methods, such as DQN (Mnih
et al., 2015), could suffer from the curse of di-
mensionality, that is the number of parameters to
be learned grows exponentially with the size of
any compact encoding of system state. There-
fore, flat RL is unable to learn reliable value func-
tions (Kulkarni et al., 2016) for a composite task.
A composite task has a larger state space and ac-
tion set, longer trajectory, and more sparse rewards
than a simple task. Hierarchical reinforcement
learning (HRL) (Dietterich, 2000; Parr and Rus-
sell, 1997) is a technique to model complex di-
alogues (Cuayáhuitl, 2009). Peng et al. (2017)
and Budzianowski et al. (2017) used the options
framework (Sutton et al., 1999) to solve the above
problems in composite dialogues and showed its
superiority over flat RL. In their work, however,
each option (i.e. sub-task) and its property (e.g.
starting and terminating conditions, and valid ac-
tion set) had to be manually defined. Such hand-
crafted options ease the policy learning in a com-
posite task, but much human effort is inevitable.

To solve the above problems, we propose to
model sub-domains autonomously without any
human intervention. The modeled sub-domains
imitate the intentions to fulfill sub-tasks in a dia-
logue, which consequently can be reused by simi-
lar yet different domains. Challenges to achieve
such autonomous sub-domain modeling include
(i) how to discover meaningful sub-domains and
their properties (i.e. starting conditions, termi-
nating conditions, and the policies), and (ii) how
to have a coherent interaction among these sub-
domains so that the dialogue agent can accom-
plish a dialogue goal efficiently. To tackle these
challenges, we propose a unified framework that
integrates option discovery (Bacon et al., 2017;
Machado et al., 2017) with HRL to learn the opti-



10

mal policies over options. With an evaluation in-
volving a task of reserving hotel room, we confirm
that our framework achieves a significant improve-
ment over flat RL by 11% in terms of success rate,
and is competitive with the framework with man-
ually defined options (Budzianowski et al., 2017).

2 Hierarchical Policy Management

A composite task can be decomposed into a se-
quence of sub-domains, which are also called op-
tions. The composite task is accomplished when
all these sub-domains are fulfilled. Following the
options framework (Sutton et al., 1999), our dia-
logue agent handles the composite task by design-
ing two levels of policies in a hierarchical struc-
ture, as shown in Figure 1.

Top-level policy (Ê·)
option ñ1è:ñ�O;

Low-level policy (ÊÓ)
dialogue action =1è :=�O;

Internal 

Evaluator

intrinsic reward NÜ

=

=

extrinsic reward NØ
dialogue states O

User

Dialogue Policy

dialogue action

Oá NØ ñ

Figure 1: Overview of our dialogue policy.

In this hierarchical policy framework, S denotes
the dialogue state space, Ω the option space, andA
the action set. For a dialogue state s ∈ S, the top-
level policy πΩ determines which option ω ∈ Ω
should be chosen. Then, the policy πω determines
which primitive action a ∈ A should be chosen
in option ω for s. As shown by the example in
Figure 2, a primitive action is an action lasting for
one time step, while an option is an action last-
ing several time steps. For each s, a dialogue ac-
tion, which is a primitive action, is returned to the
user. The dialogue system will receive an extrin-
sic reward re and a new belief state s′. An opti-
mal policy π∗ maximizes the expected discounted
return Gt = Eπ,P [

∑∞
k=0 γ

kre,t+k+1|st] at every
time step t, where P is a transition probability ker-
nel, γ ∈ [0, 1] is a discount factor, and re,t′ is the
extrinsic reward obtained at step t′.

Figure 2 shows an example of the execution of
our hierarchical dialogue policy in a dialogue do-
main about hotel room reservation. This domain
comprises two sub-domains, i.e., searching for a
hotel and booking a hotel room. In this exam-
ple, we assume that the dialogue system has prior
knowledge regarding these sub-domains. In this
paper, we propose a dialogue framework that can
autonomously discover such sub-domains.

3 Autonomous Sub-Domain Modeling

An option is defined as 3-tuple ω = 〈Iω, πω, βω〉,
where Iω ⊆ S is the initiation set of states where
ω can be chosen, πω : S × A → [0, 1] is the
policy of ω, and β : S → [0, 1] is the termina-
tion condition of ω. To autonomously discover
options and learn their policies, we proposed to in-
tegrate option-critic (OC) (Bacon et al., 2017) and
proto-value functions (PVFs) (Mahadevan, 2007;
Machado et al., 2017) into a unified framework.

3.1 Option-Critic Architecture
OC is a gradient-based approach for simultane-
ously learning intra-option policies πω and termi-
nation functions βω. It learns options gradually
from its interactions with environment. It uses op-
tion value function QΩ(s, ω) defined as follows.

QΩ(s, ω) =
∑
a

πω(a|s)QU (s, ω, a)

QU (s, ω, a) = r(s, a) + γ
∑
s′

P (s′|s, a)U(ω, s′)

U(ω, s′) = (1− βω(s′))QΩ(s′, ω) + βω(s′)VΩ(s′)

QU (s, ω, a) is the value of executing an action in
the context of a state-option pair, and U(ω, s′) is
the utility from s′ onwards, given that we arrive in
s′ using ω. We parameterize πω by θ and βω by ϑ.
The learning algorithm of OC involves two steps:

• options evaluation: updating QΩ and QU
with temporal difference errors; and

• options improvement: updating θ with ∂QΩ∂θ
and ϑ with ∂QΩ∂ϑ .

To obtain policy πΩ over options, we combine OC
with intra-option Q-learning (Sutton et al., 1999).
Hereinafter, this combination is denoted as HRL-
OC.

HRL-OC optimizes the options and their poli-
cies for maximizing the cumulative extrinsic re-
ward. It is focused less on discovering meaning-
ful options (Bacon et al., 2017), which may result
in unnatural sub-domains in a successful conver-
sation. To tackle this issue, we use PVFs, which
are capable of capturing the geometry of the state
space, to discover meaningful sub-domains.

3.2 Proto-Value Functions as Options
Proto-value functions (PVFs) are learned repre-
sentations that approximate state-value function in
RL (Mahadevan, 2007). Machado et al. (2017)



11

O4

O4 O5 O6 OÞ

OÞ

OÞ OÞ>5 Oá

Oá

Top-level policy: ñ1è:ñ�O;

Low-level policy: =1è :=�O;

ñ4: hotel_searching ñ5: hotel_booking

=4

(request_

area)

=5

(request_

price)

YYXX

=Þ: terminate

=Þ

(request_

duration)

YYXX

=á: terminate

Figure 2: An example of the execution of our hierarchical dialogue policy in hotel reservation domain. At
time t = 0 and t = k, given the belief state st, top-level policy πΩ takes options ω0 and ω1, respectively.
ω0 lasts for k turns until its policy πωi takes terminate action, while ω1 lasts for n− k turns.

further demonstrated that PVFs implicitly define
options. PVF-based option discovery extracts op-
tions from the topology structure of the state space
and is capable of providing dense intrinsic rewards
for each option. The discovery process is given be-
low.

Given a set of sampled state transitions, we con-
struct an adjacency matrixW between belief states
using Gaussian kernel. Then, we apply eigen-
decomposition to the combinatorial graph Lapla-
cian of W . Each eigenvector (i.e. PVF) eω cor-
responds to an option with intrinsic reward func-
tion rωi (s, s

′) = eω[s
′] − eω[s] for a state transi-

tion from s to s′. Since our dialogue system has
continuous belief states, we interpolate the value
of eigenvectors to novel states using Nyström ap-
proximation (Mahadevan, 2007). The number of
generated intrinsic reward functions is equal to
the number of dialogue states in W , but we used
intrinsic reward functions from eigenvectors with
the smallest eigenvalues.

An option ω, which corresponds to an eigen-
vector eω, can be interpreted as a desire to reach
a belief state s that has the highest value of
eω[s] (Machado et al., 2017). In our experiment,
such a state usually represents a dialogue goal or
a state where user’s inherent sub-domain changes
(e.g. user starts the booking sub-domain once she
finds the hotel satisfying her requirements).

3.3 Policy Learning with Intrinsic Rewards

To realize a dialogue framework that can dis-
cover effective and meaningful sub-domains, we
feed PVFs into HRL-OC, then follows HRL-OC’s
learning procedure. Here, PVFs act as an inter-
nal evaluator of the dialogue policy. We formu-
late the r(s, a) in QU to be r(s, a) = αrωi +
(1 − α)re. Hereinafter, this model is denoted as

HRL-OC PVF. We can regard HRL-OC as HRL-
OC PVF with α = 0.

We also introduce alternative dialogue frame-
works by applying the intrinsic rewards from
PVFs directly to HRL algorithms. We train each
policy πω in HRL using a specific intrinsic re-
ward function rωi . We implemented the hier-
archical deep Q-networks (HRL-DQN; Kulkarni
et al. (2016)), and policy gradient-DQN (HRL-
PG DQN), i.e., REINFORCE (Williams, 1992) as
the top-level policy and DQN low-level policy.
This assesses whether using only general-purpose
intrinsic rewards, which are designed for explo-
ration, is good for maximizing extrinsic rewards.

4 Experimental Setup

We conducted three evaluations on (i) the effec-
tiveness of our autonomous sub-domain model-
ing compared to the manual sub-domain model-
ing, (ii) the performance difference between flat
RL (i.e. without modeling) and the HRL with au-
tonomous modeling, and (iii) the impact of using
PVFs in discovering meaningful sub-domains.

4.1 Dialogue Domain

Following the setting in Budzianowski et al.
(2017), we evaluated our proposed framework in
the task of reserving a hotel room, which involves
three sub-domains: searching for a hotel, book-
ing, and payment. This domain has 13 constraint
slots, that is 5 slots in hotel searching (price, kind,
area, stars, hasparking), 5 slots in booking (day,
hour, duration, peopleno, surname), and 3 slots in
payment sub-domain (address, cardno, surname).
Dialogue management over this dialogue domain
is cast as a Markov Decision Process (MDP) with
the following specification.



12

• State: the belief state s ∈ S with 239 dimen-
sions that captures distribution over user’s in-
tents and requestable slots

• Action set A: 44 dialogue actions, which
consists of 8 slot-independent actions and 36
slot-dependent actions.

• Reward: -1 at each turn, and 0 or 20 (failed
or success dialogue) at the end of dialogue

• Discount factor γ: 0.95

• Maximum number of turns: 30

4.2 User Simulator

We used an agenda-based user simulator (Schatz-
mann et al., 2007) with which the belief states per-
fectly capture the user intent. At the start of each
dialogue, the simulated user randomly sets its goal
that consists of searching for a hotel and either
booking it or paying for it. User will proceed to the
booking or payment sub-domain only after achiev-
ing the goal of the hotel searching sub-domain.

At the beginning of each sub-domain execution,
the user’s goal for that sub-domain is randomly
generated using database. The agenda is popu-
lated by converting all goal constraints into in-
form acts, and all goal requests into request acts.
For instance, inform(price=moderate) indicates a
user requirement, and request(address) indicates
the user asking for the address of the hotel returned
by the system. Furthermore, in different dialogue
episodes, the simulated user might convey its re-
quirements (i.e. slot values) within a sub-domain
to the dialogue system in different orders.

4.3 Dialogue Frameworks

Implementation As the benchmarks without
sub-domain, we used flat RL algorithms (i.e.
DQN, and PG with REINFORCE). For the bench-
mark with manual modeling, we used the frame-
work introduced by Budzianowski et al. (2017),
which utilized hierarchical Gaussian Process RL
(HRL-GP).

All deep (flat and hierarchical) RL agents con-
sist of 2 hidden layers (150 units in layer 1, and 75
(70 for PG) in layer 2). We used Adam optimizer,
a mini-batch size of 32, and �-greedy strategy for
exploration. In HRL-DQN and HRL-PG DQN
agents, top-level and low-level policies have sepa-
rate policy networks, each of which has 2 hidden
layers as specified above. In these agents, the low-
level policies share the same policy network. Dur-

ing execution, we pass the information of the op-
tion taken by the top-level policy to the low-level
policy network. In HRL-OC and HRL-OC PVF
agents, the policy, the critic QΩ, and the termina-
tion networks share the same 2 hidden layers, but
each of them has its own output layer.

For discovering PVFs, we generated state tran-
sition samples using hand-crafted rules (Ultes
et al., 2017). We sub-sampled 1,000 unique states
using trajectory sampling, and builtW from them.

Prior Knowledge In the manual sub-domain
modeling, the agent has two types of prior knowl-
edge as follows.

• sub-domains comprising a dialogue (i.e. ho-
tels, booking, payment).

• a valid action set for each sub-domain.
All sub-domains share the same 8 slot-
independent actions, but each of them has its
own slot-dependent actions.

To assess the impact of each type of prior knowl-
edge, we implemented an HRL-GP framework
that uses both types of knowledge and its vari-
ant HRL-GP2 that uses only sub-domain infor-
mation. Both frameworks have separated policies
to handle each sub-domain, but HRL-GP2 deals
with a more complex situation since it has to se-
lect an action from the union of actions sets from
all sub-domains, that is 44 dialogue actions in to-
tal. Unlike HRL-GP and HRL-GP2, our frame-
works with autonomous modeling (HRL-DQN,
HRL-PG DQN, HRL-OC, HRL-OC PVF) cannot
access any prior knowledge. They initially per-
ceive dialogues as a single domain problem and
attempt to discover the meaningful sub-domains.

Evaluation We trained each policy in the frame-
works for 30 iterations, each of which consists of
200 episodes. In the end of each iteration, we eval-
uated the performance of the models using 200
episodes. The metric we used for evaluation is the
average success rate (SR) of dialogues.

Benchmark SR(%) Our framework SR(%)
FlatRL (DQN) 66.9 HRL-DQN 49.6
FlatRL (PG) 62.0 HRL-PG DQN 51.0
HRL-GP 84.8 HRL-OC 73.4
HRL-GP2 75.9 HRL-OC PVF 72.1

Table 1: Highest SR of dialogue agents.



13

(a) SR of different frameworks (b) HRL-OC PVF w/ different # of options (c) HRL-OC PVF with different α

Figure 3: Learning curves of different dialogue frameworks

5 Experimental Results

5.1 Success Rate
The experimental results are shown in Table 1 and
Figure 3. First, the flat RL, which is a DQN,
achieved an SR of up to 66.9%, but it was unsta-
ble. The more stable flat framework, PG, obtained
62%. Our frameworks with autonomous modeling
(HRL-OC and HRL-OC PVF) outperformed flat
RL significantly. However, HRL-DQN and HRL-
PG DQN performed worse than flat RL. This sug-
gests that using only intrinsic rewards from PVFs
is not adequate for constructing sub-domains that
are effective in accumulating extrinsic rewards.
Since HRL-OC optimizes its options for maxi-
mizing the accumulated extrinsic reward, it has
a better SR compared to HRL-DQN and HRL-
PG DQN, which did not use any extrinsic rewards.

The frameworks with manual modeling, i.e.
HRL-GP and HRL-GP2, reached an SR of 84.8
and 75.9%, respectively. One of the frame-
works with autonomous modeling (i.e. HRL-OC)
achieved up to 73.4%. Note that, in HRL-OC, all
primitive actions are used for each option, which
is the same as HRL-GP2. Although HRL-OC does
not have any prior knowledge about sub-domains
in a dialogue, it is competitive with the framework
with strong supervision on sub-domains. This in-
dicates that HRL-OC is able to learn effective sub-
goals in a composite-task dialogue.

As shown in Figure 3, learning curves of dif-
ferent dialogue frameworks are examined. Fig-
ure 3(a) shows that HRL-OC and HRL-OC PVF
have steeper learning curves than HRL-GP in
the first 1000 episodes, which indicates that our
frameworks can shorten learning time. Figure 3(b)
reports that the use of 2 or 3 options is optimal.
Using too many options is harmful because the
agent will require more episodes to learn the opti-
mal policy over options. Figure 3(c) shows the ef-

fect of the interpolation ratio α for combining both
extrinsic and intrinsic rewards on the SR. How-
ever, PVFs seem ineffective with respect to SR.
To have α > 0 reduces the SR of HRL-OC PVF
with 3 options.

5.2 Discovered Sub-domains

According to our observation, the HRL-OC PVF
with α = 0.2, however, discovered more mean-
ingful sub-domains than HRL-OC. To assess the
meaningfulness of the discovered sub-domains,
we examined how similar these sub-domains to
those inherent in the user’s agenda. We judge the
similarity using the average dialogue turn distance
between the turn when the user simulator enters
a sub-domain and that when the agent switches
sub-domains. The ideal case is to have a turn dis-
tance of 1, i.e., once a user enters a sub-domain,
the agent responds by switching the active option
in the next turn. Table 2 shows that compared to
HRL-OC, the integration of PVFs results in sub-
domains whose boundaries are similar to those of
the user’s sub-domains.

Table 3 shows that the integration of PVFs into
HRL-OC makes the agent capable of changing the
active sub-domain soon after the user enters a sub-
domain. This indicates that PVFs can detect in-
teresting belief states. In our further examination,
PVFs successfully discover states that indicate di-
alogue goal, sub-task switching, and request of al-
ternatives from the sampled transitions.

Agents i
th iteration

1 2 3 4 5
HRL-OC 2.8 2.8 4.0 3.9 5.3
HRL-OC PVF (α = 0.2) 2.6 2.5 3.0 2.8 2.8
HRL-OC PVF (α = 0.4) 2.9 3.0 3.4 3.2 3.9

Table 2: Average turn distance between activation
of user’s sub-domains and agent’s



14

User Goal
slots values slots values

hotel searching payment
price don’t care address bristol
kind don’t care cardNo 600
area west surname wilson
stars 3
hasparking don’t care

requestable slots
name of the hotel

hasinternet
postcode

S: How may I help you?
U: I’m looking for a hotel in west.
S: A hotel in the west area right?
U: Yes, and 3 stars.
S: Would you like a hotel in the cheap, moderate,

or expensive price range?
U: any price
S: Alpha-Milton guest house is a nice hotel.
U: Does it have internet connection?
S: It doesn’t have internet.
U: post code
S: Their postcode is cb4 1xa.
U: I’ll pay with CC by Wilson. [user enters payment]
S: How long is your stay?
U: I don’t care, my surname is Wilson.
S: Your name is Wilson right? [system is in payment]
U: Yes, and my address is Bristol.

(...21 dialogue turns to CC alternatives...)
S: I will pay the room with CC 600 by Matt Wilson.
U: Goodbye.

Table 3: Sample dialogue by HRL-OC PVF (S:
agent, U: user). Different font styles and colors
indicate different sub-domains activated by agent.

5.3 Discussion

Our experiments show that our proposed frame-
work outperforms the baseline, and is competi-
tive with with the framework with manually de-
fined sub-domains. Even though the experiments
are done using a simulator, the simulated user pro-
duces dialogue behavior realistic enough for train-
ing and testing. As mentioned in Section 4.2, the
simulated user specifies its requirements within a
sub-domain to the dialogue system in a random or-
der. In addition, the simulator may also not spec-
ify several slot values. Such a behavior simulates a
situation in which a human user forgets to specify
some goal constraints.

In the experiments, the simulator has a con-
straint, that is it executes the inherent sub-domains
in a fixed order. The fixed order of sub-domains,
i.e. hotel search and then followed by either book-
ing or payment, can still simulate the real world
conversational data, since an activity of reserv-
ing a hotel room is commonly accomplished in

such order. In other tasks, however, a fixed or-
der of inherent sub-domains may not simulate
the real conversation well. Nevertheless, even
when the order of the inherent sub-domains are
not fixed, we suggest that our proposed frame-
work could still discover options that imitate the
inherent sub-domains. This holds when the in-
herent sub-domains are executed sequentially, and
the environment dynamics within each inherent
sub-domain is invariant to the execution order of
the sub-domains. Another challenging situation is
when the inherent sub-domains are executed in an
interleaved manner. This simulates a scenario in
which a user frequently switches the active sub-
domain before the current sub-domain is fulfilled.
A further investigation is required to examine the
options discovered in such a situation.

6 Conclusion

We proposed a framework that autonomously dis-
covers sub-domains for a composite-task dialogue.
Experimental results shows that our framework
with autonomous modeling is competitive with the
framework with manually defined sub-domains.
Analysis also showed that the integration of PVFs
leads to meaningful sub-domains.

For future work, we consider the adjustment of
the PVFs construction, such as the distance met-
ric between states, the construction of the adja-
cency matrix, and the use of successor representa-
tion (Dayan, 1993; Barreto et al., 2017). We may
also need to further examine the discovered op-
tions when the inherent sub-domains are executed
in several different manners and orders. Finally, it
is also interesting to investigate the effectiveness
of reusing the learned options in other related dia-
logue domains.

References
Pierre-Luc Bacon, Jean Harb, and Doina Precup. 2017.

The Option-Critic Architecture. In Proceedings of
the 31st AAAI Conference on Artificial Intelligence,
pages 1726–1734, San Francisco, California, USA.

André Barreto, Will Dabney, Rémi Munos, Jonathan J.
Hunt, Tom Schaul, David Silver, and Hado P. van
Hasselt. 2017. Successor features for transfer in re-
inforcement learning. In Advances in Neural Infor-
mation Processing Systems 30, pages 4058–4068,
California, USA.

Paweł Budzianowski, Stefan Ultes, Pei-Hao Su, Nikola
Mrkšić, Tsung-Hsien Wen, Iñigo Casanueva, Lina

http://arxiv.org/abs/1609.05140
http://papers.nips.cc/paper/6994-successor-features-for-transfer-in-reinforcement-learning
http://papers.nips.cc/paper/6994-successor-features-for-transfer-in-reinforcement-learning


15

Rojas-Barahona, and Milica Gašić. 2017. Sub-
domain Modelling for Dialogue Management with
Hierarchical Reinforcement Learning. In Proceed-
ings of the 18th Annual SIGDIAL Meeting on Dis-
course and Dialogue, pages 86–92, Saarbrücken,
Germany.

Iñigo Casanueva, Paweł Budzianowski, Pei-Hao Su,
Nikola Mrkšić, Tsung-Hsien Wen, Stefan Ultes,
Lina Rojas-Barahona, Steve Young, and Milica
Gašić. 2017. A Benchmarking Environment for
Reinforcement Learning Based Task Oriented Dia-
logue Management. CoRR, abs/1711.1.

Heriberto Cuayáhuitl. 2009. Hierarchical Reinforce-
ment Learning for Spoken Dialogue Systems. Ph.D.
thesis, University of Edinburgh.

Heriberto Cuayáhuitl, Seunghak Yu, Ashley
Williamson, and Jacob Carse. 2016. Deep Re-
inforcement Learning for Multi-Domain Dialogue
Systems. In Advances in Neural Information
Processing Systems 29 Workshop on Deep Rein-
forcement Learning, Barcelona, Spain.

Peter Dayan. 1993. Improving generalization for tem-
poral difference learning: The successor representa-
tion. Neural Computation, 5(4):613–624.

Thomas G. Dietterich. 2000. Hierarchical Reinforce-
ment Learning with the MAXQ Value Function De-
composition. Journal of Artificial Intelligence Re-
search, 13:227–303.

M. Gasic, N. Mrksic, P. H. Su, D. Vandyke, T. H.
Wen, and S. Young. 2016. Policy committee for
adaptation in multi-domain spoken dialogue sys-
tems. 2015 IEEE Workshop on Automatic Speech
Recognition and Understanding, ASRU 2015 - Pro-
ceedings, pages 806–812.

Milica Gašić and Steve Young. 2014. Gaussian pro-
cesses for POMDP-based dialogue manager opti-
mization. IEEE Transactions on Audio, Speech and
Language Processing, 22(1):28–40.

Tejas D. Kulkarni, Karthik R. Narasimhan, Ardavan
Saeedi, and Joshua B. Tenenbaum. 2016. Hierarchi-
cal Deep Reinforcement Learning: Integrating Tem-
poral Abstraction and Intrinsic Motivation. In Ad-
vances in Neural Information Processing Systems
29, pages 3675–3683, Barcelona, Spain.

Xiujun Li, Yun-Nung Chen, Lihong Li, Jianfeng Gao,
and Asli Celikyilmaz. 2017. End-to-End Task-
Completion Neural Dialogue Systems. In Proceed-
ings of the Eighth International Joint Conference on
Natural Language Processing, IJCNLP, pages 733–
743, Taipei, Taiwan.

Marlos C. Machado, Marc G. Bellemare, and Michael
Bowling. 2017. A Laplacian Framework for Option
Discovery in Reinforcement Learning. In Proceed-
ings of the 34th International Conference on Ma-
chine Learning, ICML 2017, pages 2295–2304, Syd-
ney, NSW, Australia.

Sridhar Mahadevan. 2007. Proto-value Functions : A
Laplacian Framework for Learning Representation
and Control in Markov Decision Processes. Journal
of Machine Learning Research, 8:2169–2231.

Volodymyr Mnih, Koray Kavukcuoglu, David Silver,
Andrei A. Rusu, Joel Veness, Marc G. Bellemare,
Alex Graves, Martin Riedmiller, Andreas K. Fidje-
land, Georg Ostrovski, Stig Petersen, Charles Beat-
tie, Amir Sadik, Ioannis Antonoglou, Helen King,
Dharshan Kumaran, Daan Wierstra, Shane Legg,
and Demis Hassabis. 2015. Human-level con-
trol through deep reinforcement learning. Nature,
518(7540):529–533.

Ronald Parr and Stuart Russell. 1997. Reinforcement
learning with hierarchies of machines. In Advances
in Neural Information Processing Systems 10, pages
1043–1049, Denver, Colorado, USA.

Baolin Peng, Xiujun Li, Lihong Li, Jianfeng Gao,
Asli Celikyilmaz, Sungjin Lee, and Kam-Fai Wong.
2017. Composite Task-Completion Dialogue Pol-
icy Learning via Hierarchical Deep Reinforcement
Learning. In Proceedings of the 2017 Conference on
Empirical Methods in Natural Language Process-
ing, EMNLP 2017, pages 2231–2240, Copenhagen,
Denmark.

Jost Schatzmann, Blaise Thomson, Karl Weilhammer,
Hui Ye, and Steve Young. 2007. Agenda-based user
simulation for bootstrapping a pomdp dialogue sys-
tem. In Human Language Technologies 2007: The
Conference of the North American Chapter of the
Association for Computational Linguistics; Com-
panion Volume, Short Papers, pages 149–152. As-
sociation for Computational Linguistics.

Richard S. Sutton, Doina Precup, and Satinder Singh.
1999. Between MDPs and semi-MDPs: A frame-
work for temporal abstraction in reinforcement
learning. Artificial Intelligence, 112(1–2):181–211.

Stefan Ultes, Lina Rojas-barahona, Pei-Hao Su, David
Vandyke, Dongho Kim, Paweł Budzianowski, and
Nikola Mrksic. 2017. PyDial : A Multi-domain Sta-
tistical Dialogue System Toolkit. In Proceedings
of the 55th Annual Meeting of the Association for
Computational Linguistics, ACL 2017, pages 73–78,
Vancouver, Canada.

Jason D. Williams, Kavosh Asadi, and Geoffrey Zweig.
2017. Hybrid Code Networks: practical and effi-
cient end-to-end dialog control with supervised and
reinforcement learning. In Proceedings of the 55th
Annual Meeting of the Association for Computa-
tional Linguistics, ACL 2017, pages 665–677, Van-
couver, Canada.

R. J. Williams. 1992. Simple statistical gradient-
following algorithms for connectionist reinforce-
ment learning. Machine Learning, 8:229–256.

Steve Young, Milica Gašić, Blaise Thomson, and Ja-
son D. Williams. 2013. POMDP-based statistical

http://arxiv.org/abs/1706.06210
http://arxiv.org/abs/1706.06210
http://arxiv.org/abs/1706.06210
http://arxiv.org/abs/1711.11023
http://arxiv.org/abs/1711.11023
http://arxiv.org/abs/1711.11023
http://arxiv.org/abs/1611.08675
http://arxiv.org/abs/1611.08675
http://arxiv.org/abs/1611.08675
https://doi.org/10.1162/neco.1993.5.4.613
https://doi.org/10.1162/neco.1993.5.4.613
https://doi.org/10.1162/neco.1993.5.4.613
https://doi.org/10.1613/jair.639
https://doi.org/10.1613/jair.639
https://doi.org/10.1613/jair.639
https://doi.org/10.1109/ASRU.2015.7404871
https://doi.org/10.1109/ASRU.2015.7404871
https://doi.org/10.1109/ASRU.2015.7404871
https://doi.org/10.1109/TASL.2013.2282190
https://doi.org/10.1109/TASL.2013.2282190
https://doi.org/10.1109/TASL.2013.2282190
https://doi.org/10.1023/A:1025696116075
https://doi.org/10.1023/A:1025696116075
https://doi.org/10.1023/A:1025696116075
http://arxiv.org/abs/1703.01008
http://arxiv.org/abs/1703.01008
http://arxiv.org/abs/1703.00956
http://arxiv.org/abs/1703.00956
https://doi.org/10.1145/1102351.1102421
https://doi.org/10.1145/1102351.1102421
https://doi.org/10.1145/1102351.1102421
http://dx.doi.org/10.1038/nature14236
http://dx.doi.org/10.1038/nature14236
http://papers.nips.cc/paper/1384-reinforcement-learning-with-hierarchies-of-machines
http://papers.nips.cc/paper/1384-reinforcement-learning-with-hierarchies-of-machines
http://arxiv.org/abs/1704.03084
http://arxiv.org/abs/1704.03084
http://arxiv.org/abs/1704.03084
http://www.aclweb.org/anthology/N07-2038
http://www.aclweb.org/anthology/N07-2038
http://www.aclweb.org/anthology/N07-2038
https://doi.org/10.1016/S0004-3702(99)00052-1
https://doi.org/10.1016/S0004-3702(99)00052-1
https://doi.org/10.1016/S0004-3702(99)00052-1
https://doi.org/10.18653/v1/P17-4013
https://doi.org/10.18653/v1/P17-4013
https://doi.org/10.18653/v1/P17-1062
https://doi.org/10.18653/v1/P17-1062
https://doi.org/10.18653/v1/P17-1062
https://doi.org/10.1109/JPROC.2012.2225812


16

spoken dialog systems: A review. Proceedings of
the IEEE, 101(5):1160–1179.

https://doi.org/10.1109/JPROC.2012.2225812

