



















































Stochastic Wasserstein Autoencoder for Probabilistic Sentence Generation


Proceedings of NAACL-HLT 2019, pages 4068–4076
Minneapolis, Minnesota, June 2 - June 7, 2019. c©2019 Association for Computational Linguistics

4068

Stochastic Wasserstein Autoencoder for Probabilistic Sentence Generation

Hareesh Bahuleyan,† Lili Mou,† Hao Zhou,‡ Olga Vechtomova†
†University of Waterloo ‡ByteDance AI Lab

hpallika@uwaterloo.ca doublepower.mou@gmail.com
zhouhao.nlp@bytedance.com ovechtomova@uwaterloo.ca

Abstract

The variational autoencoder (VAE) imposes
a probabilistic distribution (typically Gaus-
sian) on the latent space and penalizes the
Kullback–Leibler (KL) divergence between
the posterior and prior. In NLP, VAEs are ex-
tremely difficult to train due to the problem of
KL collapsing to zero. One has to implement
various heuristics such as KL weight anneal-
ing and word dropout in a carefully engineered
manner to successfully train a VAE for text.
In this paper, we propose to use the Wasser-
stein autoencoder (WAE) for probabilistic sen-
tence generation, where the encoder could be
either stochastic or deterministic. We show
theoretically and empirically that, in the orig-
inal WAE, the stochastically encoded Gaus-
sian distribution tends to become a Dirac-delta
function, and we propose a variant of WAE
that encourages the stochasticity of the en-
coder. Experimental results show that the la-
tent space learned by WAE exhibits proper-
ties of continuity and smoothness as in VAEs,
while simultaneously achieving much higher
BLEU scores for sentence reconstruction.1

1 Introduction

Natural language sentence generation in the deep
learning regime typically uses a recurrent neural
network (RNN) to predict the most probable next
word given previous words (Mikolov et al., 2010).
Such RNN architecture can be further conditioned
on some source information, for example, an in-
put sentence, resulting in a sequence-to-sequence
(Seq2Seq) model.

Traditionally, sentence generation is accom-
plished in a deterministic fashion, i.e., the model
uses a deterministic neural network to encode an

1Our code is availabe at https://github.com/
HareeshBahuleyan/probabilistic_nlg
A preliminary version of this paper was preprinted at
https://arxiv.org/abs/1806.08462

input sentence to some hidden representations,
from which it then decodes an output sentence us-
ing another deterministic neural network.

Bowman et al. (2016) propose to use the vari-
ational autoencoder (VAE, Kingma and Welling,
2014) to map an input sentence to a probabilistic
continuous latent space. VAE makes it possible
to generate sentences from a distribution, which
is desired in various applications. For example,
in an open-domain dialog system, the informa-
tion of an utterance and its response is not nec-
essarily a one-to-one mapping, and multiple plau-
sible responses could be suitable for a given input.
Probabilistic sentence generation makes the dia-
log system more diversified and more meaning-
ful (Serban et al., 2017; Bahuleyan et al., 2018).
Besides, probabilistic modeling of the hidden rep-
resentations serves as a way of posterior regular-
ization (Zhang et al., 2016), facilitating interpola-
tion (Bowman et al., 2016) and manipulation of
the latent representation (Hu et al., 2017).

However, training VAEs in NLP is more diffi-
cult than the image domain (Kingma and Welling,
2014). The VAE training involves a reconstruction
loss and a Kullback–Leibler (KL) divergence be-
tween the posterior and prior of the latent space.
In NLP, the KL term tends to vanish to zero dur-
ing training, leading to an ineffective latent space.
Previous work has proposed various engineering
tricks to alleviate this problem, including KL an-
nealing and word dropout (Bowman et al., 2016).

In this paper, we address the difficulty of train-
ing VAE sentence generators by using a Wasser-
stein autoencoder (WAE, Tolstikhin et al., 2018).
WAE modifies VAE in that it requires the integra-
tion of the posterior to be close to its prior, where
the closeness is measured with empirical samples
drawn from the distributions. In this way, the en-
coder could be either stochastic or deterministic,
but the model still retains probabilistic properties.

hpallika@uwaterloo.ca
doublepower.mou@gmail.com
zhouhao.nlp@bytedance.com
ovechtomova@uwaterloo.ca
https://github.com/HareeshBahuleyan/probabilistic_nlg
https://github.com/HareeshBahuleyan/probabilistic_nlg
https://arxiv.org/abs/1806.08462


4069

(a)		DAE																(b)	VAE														(c)	WAE								(d)	WAE	+	aux	loss

Figure 1: The latent space of the deterministic autoen-
coder (DAE), variational autoencoder (VAE), Wasser-
stein autoencoder (WAE), as well as WAE with by our
KL penalty. Blue circles: Posterior or aggregated pos-
terior distributions of data in the latent space. Red cir-
cles: Regularizations of the posterior.

Moreover, we show both theoretically and em-
pirically that the stochastic Gaussian encoder in
the original form tends to be a Dirac-delta func-
tion. We thus propose a WAE variant that encour-
ages the encoder’s stochasticity by penalizing an
auxiliary KL term.

Experiments show that the sentences gener-
ated by WAE exhibit properties of continuity and
smoothness as in VAE, while achieving a much
higher reconstruction performance. Our proposed
variant further encourages the stochasticity of the
encoder. More importantly, WAE is robust to hy-
perparameters and much easier to train, without
the need for KL annealing or word dropout as
in VAE. In a dialog system, we demonstrate that
WAEs are capable of generating better quality and
more diverse sentences than VAE.

2 Probabilistic Sentence Generation

Base Model: Deterministic Autoencoder
(DAE). DAE encodes an input sentence with a
recurrent neural network (RNN) and then decodes
the same sentence through another RNN.

For the encoder, the hidden state of the last word
is represented as the latent space of the input sen-
tence x. The latent representation is denoted as
z. We feed z to the decoder RNN, which predicts
one word at a time using a softmax layer, given by
p(xt|z,x<t).The training objective for DAE is the
sequence-aggregated cross-entropy loss, given by

J = −
N∑
n=1

|x(n)|∑
t=1

log p(x
(n)
t |z(n),x

(n)
<t ) (1)

where superscript (n) indicates the nth data point
among 1, · · · , N .

In DAE, the latent space is encoded and then de-
coded in a deterministic way, i.e., there is no prob-
abilistic modeling of the hidden space. The hidden

representations of data may be located on an arbi-
trary manifold (Figure 1a), which is not suitable
for probabilistic generation.

Variational Autoencoder (VAE). VAE extends
DAE by imposing a prior distribution p(z) on the
latent variable z, which is typically set to the stan-
dard normalN (0, I) (Kingma and Welling, 2014).
Given an input sentence x, we would like to model
the posterior of z by another normal distribution,
q(z|x) = N (µpost, diagσ2post), where µpost and
σ2post are the outputs of the encoder.

In the training of VAE, z is sampled from
q(z|x), and the training objective is to maximize a
variational lower bound of the likelihood of data.
This is equivalent to minimizing the (expected) re-
construction loss similar to (1), while being regu-
larized by the KL divergence between q(z|x) and
p(z), given by

J =
N∑
n=1

[
− E

z(n)∼q

|x(n)|∑
t=1

log p(x
(n)
t |z(n),x

(n)
<t )

+ λVAE ·KL(q(z(n)|x(n))‖p(z))
]

(2)

where in the expectation z(n) is sampled from
q(z|x(n)) and λVAE is a hyperparameter balancing
the two terms.

Since VAE penalizes the divergence of z’s pos-
terior from its prior, it serves as a way of posterior
regularization, making it possible to generate sen-
tences from the continuous latent space.

However, the two objectives in (2) are contra-
dictory to each other, as argued by Tolstikhin et al.
(2018). VAE pushes the posterior of z, given any
input x(n), to be close to its prior, i.e., every blue
ellipse in Figure 1b should be close to the red one.
This makes perfect reconstruction impossible.

Further, VAE is difficult to train in NLP due to
the problem of KL collapse, where the KL term
tends to be zero, meaning that the encoder captures
no information and the decoder learns an uncondi-
tioned language model. This phenomenon is ob-
served in variational auto-regressive decoders us-
ing RNN. To alleviate this problem, existing tricks
include KL annealing and word dropout (Bowman
et al., 2016), but both require extensive engineer-
ing.

Wasserstein Autoencoder (WAE). An alterna-
tive way of posterior regularization is to impose
a constraint that the aggregated posterior of z
should be the same as its prior (Tolstikhin et al.,



4070

2018), i.e., q(z) def=
∑

x q(z|x)pD(x)
set
= p(z),

where pD is the data distribution. This is also
demonstrated in Figure 1c. By contrast, VAE re-
quires that q(z|x) should be close to p(z) for ev-
ery input sentence x.

For computational purposes, Tolstikhin et al.
(2018) relax the above constraint by penalizing the
Wasserstein distance between q(z) and p(z). In
particular, it is computed by the Maximum Mean
Discrepancy (MMD), defined as

MMD =

∥∥∥∥∫ k(z, ·) dP (z)− ∫ k(z, ·) dQ(z)∥∥∥∥
Hk

where P (z) and Q(z) are cumulative density
functions. Hk refers to the reproducing kernel
Hilbert space defined by the kernel k, which is
often chosen as the inverse multiquadratic kernel
k(x, y) = C

C+‖x−y‖22
for high-dimensional Gaus-

sians.
One advantage of the Wasserstein distance is

that it can be estimated by empirical samples as

M̂MD =
1

N(N − 1)
∑
n6=m

k(z(n),z(m)) (3)

+
1

N(N − 1)
∑
n 6=m

k(z̃(n), z̃(m))− 1
N2

∑
n,m

k(z(n), z̃(m))

where z̃(n) is a sample from the prior p(z), and
z(n) is a sample from the aggregated posterior
q(z), which is obtained by sampling x(n) from
the data distribution and then sampling z(n) from
q(z|x(n)). In summary, the training objective of
WAE is

JWAE = −
N∑

n=1

|x(n)|∑
t=1

log p(x
(n)
t |z

(n),x
(n)
<t ) + λWAE · M̂MD

(4)

where λWAE balances the MMD penalty and the
reconstruction loss.

Alternatively, the dual form (adversarial loss)
can also be used for WAE (Zhao et al., 2018).
In our preliminary experiments, we found MMD
similar to but slightly better than the adversarial
loss. The difference between our work and Zhao
et al. (2018)—who extend the original WAE to se-
quence generation—is that we address the KL an-
nealing problem of VAE and further analyze the
stochasticity of WAE from a theoretical perspec-
tive, as follows.

WAE with Auxiliary Loss. In WAE, the aggre-
gated posterior q(z) involves an integration of data
distribution, which allows using a deterministic
function to encode z as z = fencode(x) as sug-
gested by Tolstikhin et al. (2018). This would
largely alleviate the training difficulties as in VAE,
because backpropagating gradient into the encoder
no longer involves a stochastic layer.

The stochasticity of the encoder, however, is
still a desired property in some applications, for
example, generating diverse responses in a dialog
system. We show both theoretically and empiri-
cally that a dangling Gaussian stochastic encoder
could possibly degrade to a deterministic one.

Theorem 1. Suppose we have a Gaussian family
N (µ,diagσ2), where µ and σ are parameters.
The covariance is diagonal, meaning that the vari-
ables are independent. If the gradient of σ com-
pletely comes from sample gradient and σ is small
at the beginning of training, then the Gaussian
converges to a Dirac delta function with stochas-
tic gradient descent, i.e., σ → 0. (See Appendix A
for the proof.)

To alleviate this problem, we propose a sim-
ple heuristic that encourages the stochasticity of
the encoder. In particular, we penalize, for ev-
ery data point, a KL term between the predicted
posterior q(z|x) = N (µpost, diagσ2post) and a
Gaussian with covariance I centered at the pre-
dicted mean, i.e., N (µpost, I). This is shown in
Figure 1d, where each posterior is encouraged to
stretch with covariance I. Formally, the loss is

J = Jrec + λWAE · M̂MD

+ λKL
∑

n
KL
(
N (µ

(n)

post , diag(σ
(n)

post )
2)
∥∥N (µ(n)post , I))

(5)

While our approach appears heuristic, the next the-
orem shows its theoretical justification.

Theorem 2. Objective (5) is a relaxed optimiza-
tion of the WAE loss (4) with a constraint on σpost.
(See Appendix B for the proof.)

We will show empirically that such auxiliary
loss enables us to generate smoother and more di-
verse sentences in WAE. It, however, does not suf-
fer from KL collapse as in VAEs. The auxiliary
KL loss that we define for stochastic WAE is com-
puted against a target distribution N (µ(n)post, I) for
each data sample x(n). Here, the predicted poste-
rior mean itself is used in the target distribution.



4071

BLEU↑ PPL↓ UniKL↓ Entropy AvgLen
Corpus - - - → 5.65 → 9.6
DAE 86.35 146.2 0.178 6.23 11.0
VAE (KL-annealed) 43.18 79.4 0.081 5.04 8.8
WAE-D λWAE=3 86.03 113.8 0.071 5.59 10.0
WAE-D λWAE=10 84.29 104.9 0.073 5.57 9.9
WAE-S λKL = 0.0 75.66 115.2 0.069 5.61 9.9
WAE-S λKL = 0.01 82.01 84.9 0.058 5.26 9.4
WAE-S λKL = 0.1 47.63 62.5 0.150 4.65 8.7

Table 1: Results of SNLI-style sentence generation,
where WAE is compared with DAE and VAE. D and
S refer to the deterministic and stochastic encoders, re-
spectively. ↑/↓The larger/lower, the better. For En-
tropy and AvgLen, the closer to corpus statistics, the
better (indicated by the→ arrow).

As a result, this KL term does not force the model
to learn the same posterior for all data samples (as
in VAE), and thus, the decoder does not degrade
to an unconditioned language model.

3 Experiments

We evaluate WAE in sentence generation on
the Stanford Natural Language Inference (SNLI)
dataset (Bowman et al., 2015) as well as dialog
response generation. All models use single-layer
RNN with long short term memory (LSTM) units
for both the encoder and decoder. Appendix C de-
tails our experimental settings.

VAE training. VAE is notoriously difficult to
train in the RNN setting. While different re-
searchers have their own practice of training VAE,
we follow our previous experience (Bahuleyan
et al., 2018) and adopt the following tricks to
stabilize the training: (1) λVAE was annealed in
a sigmoid manner. We monitored the value of
λ ·KL and stop annealing once it reached its peak
value, known as peaking annealing. (2) For word
dropout, we started with no dropout, and gradually
increased the dropout rate by 0.05 every epoch un-
til it reached a value of 0.5. The effect of KL an-
nealing is further analyzed in Appendix D.

3.1 SNLI Generation

The SNLI sentences are written by crowd-
sourcing human workers in an image captioning
task. It is a massive corpus but with compara-
tively simple sentences (examples shown in Ta-
ble 4). This task could be thought of as domain-
specific sentence generation, analogous to hand
written digit generation in computer vision.

In Table 1, we compare all methods in two as-
pects. (1) We evaluate by BLEU (Papineni et al.,

2002) how an autoencoder preserves input infor-
mation in a reconstruction task. (2) We also evalu-
ate the quality of probabilistic sentence generation
from the latent space. Although there is no proba-
bilistic modeling of the latent space in DAE, we
nevertheless draw samples from N (0, I), which
could serve as a non-informative prior. Perplex-
ity (PPL) evaluates how fluent the generated sen-
tences are. This is given by a third-party n-gram
language model trained on the Wikipedia dataset.
The unigram-KL (UniKL) evaluates if the word
distribution of the generated sentences is close to
that of the training corpus. Other surface metrics
(entropy of the word distribution and average sen-
tence length) also measure the similarity of the la-
tent space generated sentence set to that of the cor-
pus.

We see that DAE achieves the best BLEU score,
which is not surprising because DAE directly opti-
mizes the maximum likelihood of data as a surro-
gate of word prediction accuracy. Consequently,
DAE performs poorly for probabilistic sentence
generation as indicated by the other metrics.

VAE and WAE have additional penalties that
depart from the goal of reconstruction. However,
we see that WAEs, when trained with appropri-
ate hyperparameters (λWAE, λKL), achieve close
performance to DAE, outperforming VAE by 40
BLEU points. This is because VAE encodes each
input’s posterior to be close to the prior, from
which it is impossible to perfectly reconstruct the
data.

Comparing the deterministic and stochastic en-
coders in WAE, we observe the same trade-off be-
tween reconstruction and sampling. However, our
proposed stochastic encoder, with λKL = 0.1 for
WAE, consistently outperforms VAE in the con-
tradictory metrics BLEU and PPL. The hyperpa-
rameters λWAE = 10.0 and λKL = 0.01 appear to
have the best balance between reconstruction, sen-
tence fluency, as well as similarity to the original
corpus.

Moreover, all our WAEs are trained without an-
nealing or word dropout. It is significantly simpler
than training a VAE, whose KL annealing typi-
cally involves a number of engineering tricks, such
as the time step when KL is included, the slope of
annealing, and the stopping criterion for anneal-
ing.



4072

BLEU-2 BLEU-4 Entropy Dist-1 Dist-2
Test Set - - 6.15 0.077 0.414
DED 3.96 0.85 5.55 0.044 0.275
VED 3.26 0.59 5.45 0.053 0.204
WED-D 4.05 0.98 5.53 0.042 0.272
WED-S 3.72 0.69 5.59 0.066 0.309

Table 2: Results on dialog generation, where
VED/WED hyperparameters for each model were cho-
sen by Table 1.

3.2 Dialog Generation
We extend WAE to an encoder-decoder framework
(denoted by WED) and evaluate it on the DailyDi-
alog corpus (Li et al., 2017).2 We follow Bahu-
leyan et al. (2018), using the encoder to capture an
utterance and the decoder to generate a reply.

Table 2 shows that WED with a deterministic
encoder (WED-D) is better than the variational
encoder-decoder (VED) in BLEU scores, but the
generated sentences lack variety, which is mea-
sured by output entropy and the percentage of
distinct unigrams and bigrams (Dist-1/Dist-2, Li
et al., 2016), evaluated on the generated test set
responses.

We then applied our stochastic encoder for
WED and see that, equipped with our KL-
penalized stochastic encoder, WED-S outperforms
DED, VED, and WED-D in all diversity measures.
WED-S also outperforms VED in generation qual-
ity, consistent with the results in Table 1.

4 Conclusion

In this paper, we address the difficulty of training
VAE by using a Wasserstein autoencoder (WAE)
for probabilistic sentence generation. WAE im-
plementation can be carried out with either a de-
terministic encoder or a stochastic one. The deter-
ministic version achieves high reconstruction per-
formance, but lacks diversity for generation. The
stochastic encoder in the original form may col-
lapse to a Dirac delta function, shown by both a
theorem and empirical results. We thus propose to
encourage stochasticity by penalizing a heuristic

2In our pilot experiment, we obtained a BLEU-4 score of
6 by training a pure Seq2Seq model with LSTM units for 200
epochs, whereas Li et al. (2017) report 0.009 BLEU-4 and
Luo et al. (2018) report 2.84 BLEU-4. Due to our unrea-
sonably high performance, we investigated this in depth and
found that the training and test sets of the DailyDialog corpus
have overlaps. For the results reported in our paper, we have
removed duplicate data in the test set, which is also available
on our website (Footnote 1). To the best of our knowledge,
we are the first to figure out the problem, which, unfortu-
nately, makes comparison with previous work impossible.

KL loss for WAE, which turns out to be a relaxed
optimization of the Wasserstein distance with a
constraint on the posterior family.

We evaluated our model on both SNLI sentence
generation and dialog systems. We see that WAE
achieves high reconstruction performance as DAE,
while retaining the probabilistic property as VAE.
Our KL-penalty further improves the stochasticity
of WAE, as we achieve the highest performance in
all diversity measures.

Acknowledgments

We would like to acknowledge Yiping Song and
Zhiliang Tian for their independent investigation
on the DailyDialog corpus. We also thank Yan-
ran Li, one of the authors who released DailyDia-
log, for discussion on this issue. This work was
supported in part by the NSERC grant RGPIN-
261439-2013 and an Amazon Research Award.

References
Hareesh Bahuleyan, Lili Mou, Olga Vechtomova, and

Pascal Poupart. 2018. Variational attention for
sequence-to-sequence models. In Proceedings of
the 27th International Conference on Computational
Linguistics, pages 1672–1682.

Samuel R. Bowman, Gabor Angeli, Christopher Potts,
and Christopher D. Manning. 2015. A large anno-
tated corpus for learning natural language inference.
In Proceedings of the 2015 Conference on Empiri-
cal Methods in Natural Language Processing, pages
632–642.

Samuel R Bowman, Luke Vilnis, Oriol Vinyals, An-
drew M Dai, Rafal Jozefowicz, and Samy Ben-
gio. 2016. Generating sentences from a continuous
space. In Proceedings of the 20th SIGNLL Confer-
ence on Computational Natural Language Learning,
pages 10–21.

Christopher P Burgess, Irina Higgins, Arka Pal, Loic
Matthey, Nick Watters, Guillaume Desjardins, and
Alexander Lerchner. 2017. Understanding disentan-
gling in beta-VAE. In Proceedings of the Workshop
on Learning Disentangled Representations: From
Perception to Control.

Irina Higgins, Loic Matthey, Arka Pal, Christopher
Burgess, Xavier Glorot, Matthew Botvinick, Shakir
Mohamed, and Alexander Lerchner. 2017. beta-vae:
Learning basic visual concepts with a constrained
variational framework. In Proceedings of the Inter-
national Conference on Learning Representations.

Zhiting Hu, Zichao Yang, Xiaodan Liang, Ruslan
Salakhutdinov, and Eric P. Xing. 2017. Toward con-
trolled generation of text. In Proceedings of the

http://aclweb.org/anthology/C18-1142
http://aclweb.org/anthology/C18-1142
https://doi.org/10.18653/v1/D15-1075
https://doi.org/10.18653/v1/D15-1075
http://aclweb.org/anthology/K/K16/K16-1002.pdf
http://aclweb.org/anthology/K/K16/K16-1002.pdf
https://arxiv.org/abs/1804.03599
https://arxiv.org/abs/1804.03599
http://www.matthey.me/pdf/betavae_iclr_2017.pdf
http://www.matthey.me/pdf/betavae_iclr_2017.pdf
http://www.matthey.me/pdf/betavae_iclr_2017.pdf
http://proceedings.mlr.press/v70/hu17e/hu17e.pdf
http://proceedings.mlr.press/v70/hu17e/hu17e.pdf


4073

34th International Conference on Machine Learn-
ing, pages 1587–1596.

Diederik P Kingma and Jimmy Ba. 2015. Adam: A
method for stochastic optimization. In Proceedings
of the International Conference on Learning Repre-
sentations.

Diederik P Kingma and Max Welling. 2014. Auto-
encoding variational Bayes. In Proceedings of Inter-
national Conference on Learning Representations,
pages 10–21.

Jiwei Li, Michel Galley, Chris Brockett, Jianfeng Gao,
and Bill Dolan. 2016. A diversity-promoting objec-
tive function for neural conversation models. In Pro-
ceedings of the 2016 Conference of the North Amer-
ican Chapter of the Association for Computational
Linguistics: Human Language Technologies, pages
110–119.

Yanran Li, Hui Su, Xiaoyu Shen, Wenjie Li, Ziqiang
Cao, and Shuzi Niu. 2017. DailyDialog: a manually
labelled multi-turn dialogue dataset. In Proceedings
of the 8th International Joint Conference on Natural
Language Processing, pages 986–995.

Liangchen Luo, Jingjing Xu, Junyang Lin, Qi Zeng,
and Xu Sun. 2018. An auto-encoder matching
model for learning utterance-level semantic depen-
dency in dialogue generation. In Proceedings of the
2018 Conference on Empirical Methods in Natural
Language Processing, pages 702–707.

Tomáš Mikolov, Martin Karafiát, Lukáš Burget, Jan
Černockỳ, and Sanjeev Khudanpur. 2010. Recur-
rent neural network based language model. In
Eleventh Annual Conference of the International
Speech Communication Association, pages 1045–
1048.

Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: a method for automatic
evaluation of machine translation. In Proceedings
of the 40th Annual Meeting of the Association for
Computational Linguistics, pages 311–318.

Iulian Vlad Serban, Alessandro Sordoni, Ryan Lowe,
Laurent Charlin, Joelle Pineau, Aaron C Courville,
and Yoshua Bengio. 2017. A hierarchical latent
variable encoder-decoder model for generating di-
alogues. In Proceedings of the Thirty-First AAAI
Conference on Artificial Intelligence, pages 3295–
3301.

Ilya Tolstikhin, Olivier Bousquet, Sylvain Gelly, and
Bernhard Scholkopf. 2018. Wasserstein auto-
encoders. In Proceedings of International Confer-
ence on Learning Representations.

Jiacheng Xu and Greg Durrett. 2018. Spherical latent
spaces for stable variational autoencoders. In Pro-
ceedings of the 2018 Conference on Empirical Meth-
ods in Natural Language Processing, pages 4503–
4513.

Zichao Yang, Zhiting Hu, Ruslan Salakhutdinov, and
Taylor Berg-Kirkpatrick. 2017. Improved vari-
ational autoencoders for text modeling using di-
lated convolutions. In Proceedings of the Inter-
national Conference on Machine Learning, pages
3881–3890.

Biao Zhang, Deyi Xiong, jinsong su, Hong Duan, and
Min Zhang. 2016. Variational neural machine trans-
lation. In Proceedings of the 2016 Conference on
Empirical Methods in Natural Language Process-
ing, pages 521–530.

Junbo Zhao, Yoon Kim, Kelly Zhang, Alexander Rush,
and Yann LeCun. 2018. Adversarially regularized
autoencoders. In Proceedings of the 35th Inter-
national Conference on Machine Learning, pages
5902–5911.

A Proof of Theorem 1

Theorem 1. Suppose we have a Gaussian family
N (µ,diagσ2), where µ and σ are parameters.
The covariance is diagonal, meaning that the vari-
ables are independent. If the gradient of σ com-
pletely comes from sample gradient and σ is small
at the beginning of training, then the Gaussian
converges to a Dirac delta function with stochastic
gradient descent, i.e., σ → 0.

Proof. For the predicted posterior N (µ, diagσ2)
where all dimensions are independent, we con-
sider a certain dimension, where the sample is
zi ∼ N (µi, σ2i ).

We denote the gradient of J wrt to zi at zi = µi
by gi

∆
= ∂J∂zi

∣∣
zi=µi

. At a particular sample z(j)i
around µi, the gradient g

(j)
i is

g
(j)
i

∆
=
∂J

∂zi

∣∣∣∣
zi=z

(j)
i

(6)

≈ gi +
∂2J

∂µ2i
(z

(j)
i − µi) (7)

∆
= gi + k(z

(j)
i − µi) (8)

= gi + k(µi + �
(j)σi − µi) (9)

= gi + kσi�
(j) (10)

where (7) is due to Taylor series approximation, if
we assume σ2i is small and thus z

(j)
i is near µi. k

denotes ∂
2J
∂µ2i

.
We compute the expected gradient wrt to σi for

� ∼ N (0, 1). The assumption of this theorem is
that the gradient of µi and σi completely comes
from the sample zi. By the chain rule, we have

https://arxiv.org/pdf/1412.6980.pdf
https://arxiv.org/pdf/1412.6980.pdf
https://arxiv.org/pdf/1312.6114.pdf
https://arxiv.org/pdf/1312.6114.pdf
https://doi.org/10.18653/v1/N16-1014
https://doi.org/10.18653/v1/N16-1014
http://www.aclweb.org/anthology/I17-1099
http://www.aclweb.org/anthology/I17-1099
http://aclweb.org/anthology/D18-1075
http://aclweb.org/anthology/D18-1075
http://aclweb.org/anthology/D18-1075
https://www.isca-speech.org/archive/archive_papers/interspeech_2010/i10_1045.pdf
https://www.isca-speech.org/archive/archive_papers/interspeech_2010/i10_1045.pdf
http://aaai.org/ocs/index.php/AAAI/AAAI17/paper/view/14567
http://aaai.org/ocs/index.php/AAAI/AAAI17/paper/view/14567
http://aaai.org/ocs/index.php/AAAI/AAAI17/paper/view/14567
http://arxiv.org/abs/1711.01558
http://arxiv.org/abs/1711.01558
http://aclweb.org/anthology/D18-1480
http://aclweb.org/anthology/D18-1480
http://proceedings.mlr.press/v70/yang17d.html
http://proceedings.mlr.press/v70/yang17d.html
http://proceedings.mlr.press/v70/yang17d.html
https://doi.org/10.18653/v1/D16-1050
https://doi.org/10.18653/v1/D16-1050
http://proceedings.mlr.press/v80/zhao18b.html
http://proceedings.mlr.press/v80/zhao18b.html


4074

E
�(j)∼N (0,1)

[
∂J

∂σi

]
(11)

= E
�(j)∼N (0,1)

[
∂J

∂z
(j)
i

·
∂z

(j)
i

∂σi

]
(12)

= E
�(j)∼N (0,1)

[
(gi + kσi�

(j)) · �(j)
]

(13)

= E
�(j)∼N (0,1)

[
gi�

(j)
i + kσi(�

(j))2
]

(14)

=kσi (15)

Notice that k > 0 if we are near a local optimum
(locally convex).

In other words, the expected gradient of σi is
proportional to σi. According to stochastic gradi-
ent descent (SGD), σi will converge to zero.

The theorem assumes σ2 is small, compared
with how J changes in the latent space. In prac-
tice, the encoded vectors of different samples may
vary a lot, whereas if we sample different vec-
tors from a certain predicted multi-variate Gaus-
sian, we would generally obtain the same sen-
tence. Therefore, J is kind of smooth in the la-
tent space. The phenomenon can also be verified
empirically by plotting the histogram of σ in WAE
with a stochastic Gaussian encoder (Figure 2). We
see that if the KL coefficient λKL is 0, meaning
that the gradient of σ comes only from the sam-
ples, then most σ’s collapse to 0.

Notice, however, that the theorem does not sug-
gest a stochastic WAE and a deterministic WAE
will yield exactly the same result, as their trajecto-
ries may be different.

B Proof of Theorem 2

Theorem 2. Objective (5) is a relaxed optimiza-
tion of the WAE loss (4) with a constraint on σpost.

Proof. Objective (5) optimizes

J = Jrec + λWAE · M̂MD

+ λKL
∑

n
KL
(
N (µ

(n)

post , diag(σ
(n)

post )
2)
∥∥N (µ(n)post , I))

The first two terms are the WAE loss, whereas
the last penalty relaxes the following optimization
problem

(a) λKL = 0

(b) λKL = 0.01

(c) λKL = 0.1

Figure 2: The histograms of σ in the posterior of the
WAE in the SNLI experiment. In the plot, there are
200 buckets in the range of (0, 1).

minimize Jrec + λWAE · M̂MD
subject to∑

n
KL
(
N (µ

(n)

post , diag(σ
(n)

post
2))
∥∥N (µ(n)post , I)) < C

(16)

for some constant C.

As known, the KL divergence between two



4075

SNLI Experiment
LSTM Hidden Dimension 100d, single layer
Word Embeddings 300d, pretrained on SNLI Corpus
Latent Dimension 100d
Epochs 20
Learning Rate Fixed rate of 0.001
Batch Size 128
Max Sequence Length 20
Vocab Size 30000

Dialog Experiment
LSTM Hidden Dimension 500d, single layer
Word Embeddings 300d, pretrained on DialyDialog Cor-

pus
Latent Dimension 300d
Epochs 200
Learning Rate Initial rate of 0.001, multiplicative de-

cay of 0.98 until a minimum of 0.00001
Batch Size 128
Max Sequence Length 20
Vocab Size 20000

Table 3: Experimental settings.

(univariant) Gaussian distributions is

KL(N (µ1, σ1)‖(N (µ2, σ2)))

= log
σ2
σ1

+
σ21 + (µ1 − µ2)2

2σ22
− 1

2
(17)

The constraint in (16) is equivalent to∑
n

∑
i

[
− log σ(n)i +

1

2
(σ

(n)
i )

2

]
< C (18)

In other words, our seemingly heuristic KL
penalty optimizes the Wasserstein loss, while re-
stricting the posterior family.

C Implementation Details

All models were trained with the Adam optimizer
(Kingma and Ba, 2015) with β1 = 0.9 and β2 =
0.999. In all our experiments, we feed the sampled
latent vector z to each time step of the decoder.
Task-specific settings are listed in Table 3.

D VAE Training Difficulties

It is a common practice that training VAEs in-
volves KL annealing and word dropout, which
further consists of hacks for tuning hyperparame-
ters. We conducted an experiment of training VAE
without KL annealing. In Figure 3, we present the
KL loss (weighted by λVAE) during the training
process for different values of λVAE. The KL loss
is believed to be an important diagnostic measure
to indicate if the latent space is “variational” (Yang
et al., 2017; Higgins et al., 2017; Burgess et al.,
2017). We see that if the penalty is too large, KL

Figure 3: Learning curves of the KL term in the VAE
loss function (λ ·KL) for different values of λ, and the
variant where λ is annealed.

simply collapses to zero ignoring the entire input,
in the case of which, the decoder becomes an un-
conditioned language model. On the other hand, if
the KL penalty is too small, the model tends to be-
come more deterministic and the KL term does not
play a role in the training. This is expected since in
the limit of λVAE to 0, the model would probably
ignore the KL term and becomes a deterministic
autoencoder (shown also by Theorem 1).

The VAE with collapsed KL does not exhibit in-
teresting properties such as random sampling for
probabilistic sequence generation (Bowman et al.,
2016). As seen in Table 4, the generated sen-
tences by VAE without annealing are very close
to each other. This is because VAE’s encoder
does not capture useful information in the latent
space, which is simply ignored during the decod-
ing phase. By sampling the latent space, we do not
obtain varying sentences. The empirical evidence
verifies our intuition.

A recent study (Xu and Durrett, 2018) propose
to get rid of KL annealing by using the von Mises–
Fisher (vMF) family of posterior and prior. In
particular, they set the prior to the uniform distri-
bution on a unit hypersphere, whereas the poste-
rior family is normal distribution on the surface of
the same sphere. They fix the standard deviation
(parametrized by κ) of the posterior, so that their
KL is a constant and annealing is not required.
This, unfortunately, loses the privilege of learning
uncertainty in the probabilistic modeling. Exam-
ples in Table 4 show that, while we have repro-
duced the reconstruction negative log-likelihood
with vMF-VAE (the metric used in their paper),
the generated sentences are of poor quality. As
also suggested by Xu and Durrett (2018), if the
posterior uncertainty in vMF is made learnable, it
re-introduces the KL collapse problem, in which



4076

Training Samples
a mother and her child are outdoors.
the people are opening presents.
the girls are looking toward the water.
a small boy walks down a wooden path in the woods.
a person in a green jacket it surfing while holding on to a line.
DAE
two families walking in a towel down alaska sands a cot .
a blade is rolling its nose furiously paper .
a woman in blue shirts is passing by a some beach
transporting his child are wearing overalls .
a guys are blowing on professional thinks the horse .
VAE without Annealing
a man is playing a guitar .
a man is playing with a dog .
a man is playing with a dog .
a man is playing a guitar .
a man is playing with a dog .
VAE with Annealing
the band is sitting on the main street .
couple dance on stage in a crowded room .
two people run alone in an empty field .
the group of people have gathered in a picture .
a cruise ship is docking a boat ship .
VAE vMF (κ fixed)
a car is a and and a blue shirt top is .
two children are playing on the group in are the the . the
a child and a adult and
the young is playing for a picture a are playing to
a little is playing a background . .
WAE-D (λWAE = 10)
the lone man is working .
the group of men is using ice at the sunset .
a family is outside in the background .
two women are standing on a busy street outside a fair
a tourists is having fun on a sunny day
WAE-S (λWAE = 10, λKL = 0.01)
an asian man is dancing in a highland house .
a person wearing a purple snowsuit jumps over the tree .
the vocalist is at the music and dancing with a microphone .
a young man is dressed in a white shirt cleaning clothes .
three children lie together and a woman falls in a plane .

Table 4: Sentences generated by randomly sampling
from the prior for different models.

case, the KL annealing is still needed.
By contrast, WAEs for sequence-to-sequence

models are trained without any additional opti-
mization strategies such as annealing. Even in our
stochastic encoder, the KL penalty does not make
WAE an unconditioned language model, because
it does not force the encoded posterior to be the
same for different input sentences.

E Qualitative Samples

Table 4 shows sentences generated by randomly
sampling points in the latent space for different
models, along with sample sentences from the
training set. They provide a qualitative under-
standing of each model’s performance.


