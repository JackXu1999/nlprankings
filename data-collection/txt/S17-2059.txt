



















































QU-BIGIR at SemEval 2017 Task 3: Using Similarity Features for Arabic Community Question Answering Forums


Proceedings of the 11th International Workshop on Semantic Evaluations (SemEval-2017), pages 360–364,
Vancouver, Canada, August 3 - 4, 2017. c©2017 Association for Computational Linguistics

QU-BIGIR at SemEval 2017 Task 3: Using Similarity Features for
Arabic Community Question Answering Forums

Marwan Torki and Maram Hasanain and Tamer Elsayed
Qatar University

Department of Computer Science & Engineering
Doha, Qatar

{mtorki,maram.hasanain,telsayed}@qu.edu.qa

Abstract

In this paper, we describe our QU-BIGIR
system for the Arabic subtask D of the Se-
mEval 2017 Task 3. Our approach builds
on our participation in the past version of
the same subtask. This year, our system
uses different similarity features that en-
codes lexical and semantic pairwise sim-
ilarity of text pairs. In addition to well-
known similarity measures such as cosine
similarity, we use other measures based on
the summary statistics of word embedding
representation for a given text. To rank a
list of candidate question-answer pairs for
a given question, we train a linear SVM
classifier over our similarity features. Our
best resulting run came second in subtask
D with a very competitive performance to
the first-ranking system.

1 Introduction

The ubiquitous presence of community question
answering (CQA) websites has motivated research
on building automatic question answering (QA)
systems that can benefit from previously-answered
questions to answer newly-posed ones (Shtok
et al., 2012). A core functionality of such sys-
tems is their ability to effectively rank previously-
suggested answers with respect to their de-
gree/probability of relevance to a posted question.
Ranking is vital to push away irrelevant and low
quality answers, which are commonplace in CQA
as they are generally open with no restrictions on
who can post or answer questions.

To this effect, SemEval 2017 Task 3 “Com-
munity Question Answering” has emphasized the
ranking component in the main task of the chal-
lenge. We have participated in Task 3-Subtask D
(Arabic Subtask) which is confined to the main

 

 

   

 

Question:                                                     اسبابھا ھي وما الدوالي ھي ما   
 

Candidate question-answer pairs (QApairs): 

 Q: تزويدي منكم اطلب ولكن القدم في دوالي معي يوجد اعراض القدم دواليل ھل 
  يسببھا التي والمشاكل الدوالي باعراض

A  :الساق واجھة على منتفخة األوردة ورظھ ھو شيوعا األكثر العرض 
 السبب وفق سيكون والعالج المباشر الطبي الفحص خالل من يكون واالتشخيص

Q  :منھا الوقايه يتم وكيف اسبابھا وما الثالثه الدرجه دوالى ماھى  
A  :ولھا المصابه للمنطقه العلويه االورده في توسع عن عباره ھي الدوالي 

 منھا الوقايه جدا ويتم طويله لمده والجلوس والسمنه كالحمل االسباب من الكثير
 جوارب ولبس اكبر بشكل الرايضه وممارسة طويله لمده الوقوف عدم طريق عن

 التفاقم لمنع الحاالت لھذه مخصصه
Q  :الدوالي وھل الدوالي من تعتبر القدم في واضح بشكل الملتويه العروق ھل 
  عالجھا يتم لم اذا اخطار لھا
A :دموية وترسبات كدمات إلى وتؤدى الجلد تحت الدوالي تنفجر أن يمكن 

 الساقين دوالي عالج يشمل قد. تقرحات إلى تؤدي أن ويمكن الجلد تحت وتصبغات
 تطورھا من الحد أو األعراض حدة من التخفيف الى تھدف التي التدابير من عدد

Q  :فكيف الساقين في األوردة دوالي من من وأعاني سنة٣٧ عمري شاب انا 
 أزيلھا

 A: اختصاصي راجع ومزعجة كبيرة كانت إذا الستئصال بحاجة أنه الغالب 
الحالة لتقييم الدموية األوعية جراحة أو عامة جراحة

Figure 1: A question and 4 of its given 30 candidate QA-
pairs

task of ranking answers; given a new question and
a set of 30 question-answer pairs (QApairs) re-
trieved by a search engine, re-rank those QApairs
by their degree/probability of relevance to the new
question. Figure 1 shows an example of a question
and four of its 30 given candidate question-answer
pairs. Further details about SemEval 2017 Task 3
can be found in (Nakov et al., 2017).

In this paper, we describe the system we devel-
oped to participate in that task. The system lever-
ages a supervised learning approach over similar-
ity features. We utilize two types of similarity fea-
tures. First, we employ similarity features based
on term representation for a given pairs of text.
Second, we utilize word2vec to build text repre-
sentation following the same approach as in our
last year’s submission for the same subtask (Mal-
has et al., 2016). We used similarity features based
on that text representation to encode the semantic
similarity for pairs of texts.

The rest of the paper is organized as follows;
the approach and description of features are in-

360



troduced in section 2; the experimental setup fol-
lowed in our submitted runs and the results are
presented in section 3. Finally we conclude our
study with final remarks in section 4.

2 Approach

We tackled the answer ranking task with a su-
pervised learning approach that uses linear SVM
models. The features used in classification are de-
signed to capture both lexical and semantic infor-
mation of pairs of texts.

2.1 Data Setup

We are given a set of questions Q; each is asso-
ciated with P question-answer pairs. To compute
our features, we define a text pair < T1, T2 > ac-
cording to three setups:

• QQA: We consider T1 to be the original
question q and the concatenation of one pair
p of its associated question-answer pairs as
T2.

• QA: We consider T1 to be the original ques-
tion q and one answer of its associated
question-answer pairs as T2.

• QQ: We consider T1 to be the original ques-
tion q and one question of its associated
question-answer pairs as T2.

2.2 Term-based Similarity Features

A recent study has showed that simple features
like MK features (Metzler and Kanungo, 2008)
can be very effective for re-ranking candidate
question answer pairs (Yang et al., 2016). We
specifically use the following features described
by Yang et al. (Yang et al., 2016). For all features,
we assume the input to be two pieces of text: T1
and T2 as defined by any of the setups illustrated
in section 2.1.

• SynonymsOverlap: Before computing this
feature, we first apply light text normaliza-
tion to T1 and T2 including special charac-
ter (e.g., ‘,’, ‘.’, etc.) and diacritics removal.
The feature is then computed as the portion
of T1 terms that have a synonym or the origi-
nal term in T2. Synonyms are extracted from
the Arabic WordNet.1

1Described here: http://bit.ly/2mzfc7X

To compute the remaining features, we nor-
malize T1 and T2 following the same ap-
proach when computing SynonymsOverlap
feature. We also apply preprocessing steps
including stemming and stopwords removal.

• LMScore: The language model score is
computed as the Dirichlet-smoothed log-
likelihood score of generating T1 given T2.
The score is computed using the following
equation:

LMScore(T1, T2) =∑
w∈T1

tfw,T1 log
tfw,T2 + µP (w|C)

|T2|+ µ
(1)

where tfw,T1 and tfw,T2 is the frequency of
term w in T1 and T2 respectively. P (w|C)
is the background language model com-
puted using the maximum likelihood estimate
with term statistics extracted from a recent
large-scale crawl of the Arabic Web called
ArabicWeb16 (Suwaileh et al., 2016). We
set µ to 2000 as this is the default value
used in Lucene’s language modeling retrieval
model.2

• CosineSimialirty. This feature computes the
cosine similarity between T1 and T2 as fol-
lows.

CS(T1, T2) =
~T1 · ~T2

|| ~T1|| || ~T2||
(2)

where ~T1 and ~T2 is the vector representation
of T1 and T2 respectively and || ~T1|| and || ~T2||
is the Euclidean lengths of vectors ~T1 and ~T2.
We represent texts as vectors using TF-IDF
representation where term statistics are ex-
tracted from ArabicWeb16 (Suwaileh et al.,
2016).

• JaccardSimialirty. This feature computes
the Jaccard similarity between T1 and T2 as
follows.

JS(T1, T2) =
| ~T1 ∩ ~T2|
| ~T1 ∪ ~T2|

(3)

• JaccardSimialirtyV1. This is a variant of
Jaccard similarity computed as follows.

JS1(T1, T2) =
| ~T1 ∩ ~T2|
| ~T1|

(4)

2http://bit.ly/2lOOdqw

361



• JaccardSimialirtyV2. This is a second vari-
ant of Jaccard similarity computed as fol-
lows.

JS2(T1, T2) =
| ~T1 ∩ ~T2|
| ~T2|

(5)

2.3 Semantic word2vec Similarity Features

Every text snippet T has a set of words. Each
word has a fixed-length word embedding represen-
tation, w ∈ Rd, where d is the dimensionality of
the word embedding. Thus for a text snippet T we
define T = {w1, · · · , wk}, where k is the number
of words in T . The word embedding representa-
tion is computed offline following Mikolov et al.
approach (Mikolov et al., 2013).

To compute similarity scores, we represent each
text snippet by a feature vector; different alterna-
tives for feature representations are adopted as de-
scribed next.

2.3.1 Average Word Embedding Similarity
For a text snippet T that has k words, we compute
the average vector as follows:

Tµ =
∑k

i=1(wi)
k

(6)

Notice that Tµ =∈ Rd. This leads to the following
cosine similarity feature.

CSµ(T1, T2) =
~Tµ1 · ~Tµ2

|| ~Tµ1 || || ~Tµ2 ||
(7)

2.3.2 Covariance Word Embedding
Similarity

Instead of computing the average vector, we can
compute a covariance matrix C ∈ Rd×d. The co-
variance matrix C is computed by treating each
dimension as a random variable and every entry
in Cu,v is the covariance between the pair of vari-
ables (u, v). The covariance between two random
variables u and v is computed as in eq. 8, where k
is the number of observations (words).

Cu,v =
∑k

i=1(ui − ū)(vi − v̄)
k − 1 (8)

The matrix C ∈ Rd×d is a symmetric matrix. We
compute a vectorized representation of the matrix
C as the stacking of the lower triangular part of

matrix C as in eq. 9. This process produces a vec-
tor TCov ∈ Rd×(d+1)/2

TCov=vect(C)={Cu,v :u∈{1,··· ,d},v∈{u,··· ,d}} (9)

This leads to the following cosine similarity fea-
ture.

CSCov(T1, T2) =
~TCov1 · ~TCov2

|| ~TCov1 || || ~TCov2 ||
(10)

2.4 Ranking Using SVM

Although Subtask D is a re-ranking task, it has
also a classification task where answers need to
be ranked and labeled with either true or false; the
former designates a Direct or Relevant answer to
the new question, and the latter designates an Irrel-
evant answer. In our last year’s submission (Mal-
has et al., 2016) we used learning-to-rank module
for re-ranking pairs, but we used a simple heuris-
tic to give labels to the candidate question-answer
pairs. This year we use SVM to give a label for
every candidate pair using the SVM model. In ad-
dition to labeling pairs, we use the decision scores
from the SVM model for re-ranking the candidate
question-answer pairs.

3 Experimental Evaluation

In this section we present the experimental setup
and results of our primary, contrastive-1 and
contrastive-2 submissions.

3.1 Experimental Setup

We used the Arabic collection of questions and
their potentially related question-answer pairs pro-
vided by Task 3 organizers to train our word em-
bedding model. The Gensim3 tool was used to
generate the word2vec model from training data4,
setting d = 100. We used the learned model to
compute our features as described in section 2.
Features were generated for the three data setups
described in section 2.1.

3.2 Submissions and Results

The differences among our submitted runs is based
on the selection of the features. In all cases we use
linear SVM for classifying and ranking question-
answer pairs. Details on our official submissions

3
http://radimrehurek.com/gensim/

4Testing data are held out during the computation of the
word2vec model.

362



MAP AvgRec MRR P R F1 Acc
QU-BigIR Contrastive 2 59.48 83.83 64.56 55.35 70.95 62.19 66.15
QU-BigIR Contrastive 1 59.13 83.56 64.68 49.37 85.41 62.57 59.91
QU-BigIR Primary 56.69 81.89 61.83 41.59 70.16 52.22 49.64
Baseline 1 (IR) 60.55 85.06 66.08 - - - -
Baseline 2 (Random) 48.48 73.89 53.27 39.04 66.43 49.18 46.13
Baseline 3 (all true) - - - 39.23 100 56.36 39.23
Baseline 4 (all false) - - - - - - 60.77

Table 1: The official scores attained by our primary and contrastive submissions to SemEval 2017 Task 3-SubTask D

are summarized next. Table 1 presents the official
results of our submissions.
Contrastive-1. We use the set of term-based sim-
ilarity features defined in section 2.2. We compute
these features for all data setups defined in sec-
tion 2.1. This results in 18 features in total. Six
features for every data setup (QQ, QA and QQA).
We tuned the parameter C for the linear SVM on
the development set.
Contrastive-2. In addition to the features used
in Contrastive-1 submission, we use the set of se-
mantic word2vec based similarity features defined
in section 2.3. This results in 24 features in total;
eight features for every data setup (QQ, QA and
QQA). This submission produced our best MAP
results.
Primary. We use the full set of similarity fea-
tures defined in section 2.2 and section 2.3. In
addition, we performed a weighted score fusion
with an SVM model based on fixed length repre-
sentation using Covariance word embedding. The
feature vectors we used are computed using equa-
tion 9. We tuned the model weights using the de-
velopment set. Table 2 shows that this setup gets
the best MAP results on the development set.

MAP
QU-BigIR Contrastive 1 42.54
QU-BigIR Contrastive 2 42.87
QU-BigIR Primary 43.41
Baseline (Random) 29.79

Table 2: The development set MAP scores obtained by our
primary and contrastive submissions.

3.3 Discussion

• Our best official submission is Contrastive-
2 using both term-based similarity features
and semantic word2vec similarity features.
This indicates that the two similarity features
types are complementing each other.

• Our results justify the usage of SVM model
for labeling and re-ranking question-answer

pairs. This is clear in the P, R, F1 and Acc
scores reported across all other baselines. We
report very competitive MAP scores to the
best performing ranking systems which are
not using any form of labeling such as IR-
baseline.

• Score fusion in our primary run did not
achieve best results on the official test set
while it was the best run in our experiments
on the development set. We believe that this
happened due to the difference in the source
of question-answer pairs in the development
set compared to the the official test set where
the test set contains only medical questions.

4 Conclusion

This paper describes the system we developed to
participate in SemEval-2017 Task 3 on Commu-
nity Question Answering. Our system has focused
on the Arabic Subtask D which is confined to An-
swer Selection in Community Question Answer-
ing, i.e., finding good answers for a given new
question.

We have adopted a supervised learning ap-
proach where linear SVM models were trained
over similarity features. In our best submission,
term-based similarity features and word2vec sim-
ilarity features were both used; our system ranked
second among the other participating teams.

Acknowledgments

This work was made possible by NPRP grant#
NPRP 6-1377-1-257 from the Qatar National Re-
search Fund (a member of Qatar Foundation). The
statements made herein are solely the responsibil-
ity of the authors.

References
Rana Malhas, Marwan Torki, and Tamer Elsayed.

2016. QU-IR at SemEval 2016 Task 3: Learning

363



to Rank on Arabic Community Question Answer-
ing Forums with Word Embedding. In Proceed-
ings of the 10th International Workshop on Semantic
Evaluation (SemEval-2016). Association for Com-
putational Linguistics, San Diego, California, pages
866–871.

Donald Metzler and Tapas Kanungo. 2008. Ma-
chine learned sentence selection strategies for query-
biased summarization. In SIGIR Learning to Rank
Workshop. pages 40–47.

Tomas Mikolov, Kai Chen, Greg Corrado, and Jef-
frey Dean. 2013. Efficient estimation of word
representations in vector space. arXiv preprint
arXiv:1301.3781 .

Preslav Nakov, Doris Hoogeveen, Lluı́s Màrquez,
Alessandro Moschitti, Hamdy Mubarak, Timothy
Baldwin, and Karin Verspoor. 2017. SemEval-2017
task 3: Community question answering. In Proceed-
ings of the 11th International Workshop on Semantic
Evaluation. Association for Computational Linguis-
tics, Vancouver, Canada, SemEval ’17.

Anna Shtok, Gideon Dror, Yoelle Maarek, and Idan
Szpektor. 2012. Learning from the past: answering
new questions with past answers. In Proceedings
of the 21st international conference on World Wide
Web. ACM, pages 759–768.

Reem Suwaileh, Mucahid Kutlu, Nihal Fathima, Tamer
Elsayed, and Matthew Lease. 2016. Arabicweb16:
A new crawl for today’s arabic web. In Proceed-
ings of the 39th International ACM SIGIR Confer-
ence on Research and Development in Information
Retrieval. SIGIR ’16, pages 673–676.

Liu Yang, Qingyao Ai, Damiano Spina, Ruey-Cheng
Chen, Liang Pang, W. Bruce Croft, Jiafeng Guo,
and Falk Scholer. 2016. Beyond factoid qa: Ef-
fective methods for non-factoid answer sentence
retrieval. In Advances in Information Retrieval:
38th European Conference on IR Research, ECIR
2016, Padua, Italy, March 20–23, 2016. Proceed-
ings, Springer International Publishing, pages 115–
128.

364


