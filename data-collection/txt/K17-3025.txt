



















































CoNLL-2017 Shared Task


Proceedings of the CoNLL 2017 Shared Task: Multilingual Parsing from Raw Text to Universal Dependencies, pages 237–242,
Vancouver, Canada, August 3-4, 2017. c© 2017 Association for Computational Linguistics

A Fast and Lightweight System for Multilingual Dependency Parsing

Tao Ji, Yuanbin Wu, Man Lan
Department of Computer Science and Technology

East China Normal University
10132130251@stu.ecnu.edu.cn
{ybwu, mlan}@cs.ecnu.edu.cn

Abstract

Following Kiperwasser and Goldberg
(2016), we present a multilingual de-
pendency parser with a bidirectional-
LSTM (BiLSTM) feature extractor and
a multi-layer perceptron (MLP) classifier.
We trained our transition-based projective
parser in UD version 2.0 datasets without
any additional data. The parser is fast,
lightweight and effective on big treebanks.

In the CoNLL 2017 Shared Task: Multi-
lingual Parsing from Raw Text to Univer-
sal Dependencies, the official results show
that the macro-averaged LAS F1 score of
our system Mengest is 61.33%.

1 Introduction

Developing tools that can process multiple lan-
guages has always been an important goal in
NLP. Ten years ago, CoNLL 2006 (Buchholz and
Marsi, 2006) and CoNLL 2007 (Nivre et al., 2007)
Shared Task were a major milestone for multilin-
gual dependency parsing. The CoNLL 2017 UD
Shared Task (Zeman et al., 2017) is an extension
of the tasks addressed in previous years. Unlike
CoNLL 2006 and CoNLL 2007, the focus of the
CoNLL 2017 UD Shared Task is learning syntactic
dependency parsers on a universal syntactic anno-
tation standard. This shared task requires partici-
pants to parse raw texts from different languages,
which vary both in typology and training set size.

The CoNLL 2017 UD Shared Task provided
universal dependencies description from LREC
2016 (Nivre et al., 2016), two datasets, which are
UD version 2.0 datasets (Nivre et al., 2017b) and
this task test datasets (Nivre et al., 2017a), two
baseline models, which are UDPipe (Straka et al.,
2016) and SyntaxNet (Weiss et al., 2015), and the
evaluation platform TIRA (Potthast et al., 2014).

In this paper, We present our multilingual de-
pendency parsing system Mengest for CoNLL
2017 UD Shared Task. The system contains a
BiLSTM feature extractor for feature representa-
tion and a MLP classifier for the transition system.
The inputs of our system are word form (lemma or
stem, which depending on the particular treebank)
and part of speech (POS) tags (coarse-grained and
fine-grained) for each token. Based on this input,
the system finds a governor for each token, and as-
signs a universal dependency relation label to each
syntactic dependency. Our official submission ob-
tains 61.33% macro-averaged LAS F1 score on all
treebanks.

The rest of this paper is organized as follows.
Section 2 discusses the transition-based model
(Kiperwasser and Goldberg, 2016) and our im-
plementation. Section 3 explains how our system
deals with parallel sets and surprise languages. Fi-
nally, we present experimental and official results
in Section 4.

2 System Description

We implement a transition-based projective parser
following Kiperwasser and Goldberg (2016). The
system consists of a BiLSTM feature extractor and
an MLP classifier. We describe their model and
our implementation in the following sections in
detail.

2.1 Arc-Hybrid System
In this work, we use the arc-hybrid transition sys-
tem (Kuhlmann et al., 2011). In the arc-hybrid
system, a configuration c = (α, β,A) consists of a
stack α, a buffer β, and a set of dependency arcs A.
Given n words sentence s = w1, · · · , wn, the ini-
tial configuration c = (∅, {1, 2, · · · , n, root}, ∅)
with an empty stack, an empty arc set, and a full
buffer β = 1, 2, · · · , n, root, where root is the
special root index. The terminal configuration set

237



contains configurations with an empty stack, an
arc set and a buffer containing only root.

For each configuration c = (σ|s1|s0, b0|β, A),
the arc-hybrid system has 3 kinds of transitions,
T = {SHIFT, LEFTl, RIGHTl}:

SHIFT(c) = (σ|s1|s0|b0, β, A)
s.t. |β| > 0

LEFTl(c) =
(
σ|s1, b0|β, A ∪ {(b0, s0, l)}

)
s.t. |β| > 0, |σ| > 0

RIGHTl(c) =
(
σ|s1, b0|β, A ∪ {(s1, s0, l)}

)
s.t. |σ| > 0, s0 ̸= root

The SHIFT transition moves the first item of
the buffer (b0) to the stack. The LEFTl transition
removes the first item on top of the stack (s0) and
attaches it as a modifier to b0 with label l, adding
the arc (b0, s0, l) to arc set A. The RIGHTl transi-
tion removes s0 from the stack and attaches it as a
modifier to the next item on the stack (s1), adding
the arc (s1, s0, l) to arc set A.

We apply a classifier to determine the best ac-
tion for a configuration. Following Chen and Man-
ning (2014), we use a MLP with one hidden layer.
The score of the transition t ∈ T is defined as:

MLPθ
(
ϕ(c)

)
= W 2 · tanh(W 1 · ϕ(c) + b1) + b2

SCOREθ
(
ϕ(c), t

)
= MLPθ

(
ϕ(c)

)
[t]

where θ = {W 1,W 2, b1, b2} are the model pa-
rameters, ϕ(c) is the feature representation of the
configuration c. MLPθ

(
ϕ(c)

)
[t] denotes an in-

dexing operation taking the output element which
is the class of transition t.

2.2 The Feature Representation
We consider two types of feature repersentations
ϕ(c) of a configuration: simple and extended.

Simple: For an input sequence s =
w1, · · · , wn, we associate each word wi with a
vector xi:

xi = e(wi) ◦ e(pi) ◦ e(qi)

where e(wi) is the embedding vector of word
wi, e(pi) is the embedding vector of POS tag
pi, e(qi) is the embedding vector of coarse-
grained POS (CPOS) tag qi. The embeddings
e(wi), e(pi), e(qi) are randomly initialized (with-
out pre-training) and jointly trained with the pars-
ing model. Then, in order to encode context fea-
tures, we use a 2-layer sentence level BiLSTM on
top of x1:n:

h⃗t = LSTM (⃗ht−1, xi, θ⃗)
⃗ht = LSTM( ⃗ht+1, xi, ⃗θ)

vi = h⃗i ◦ ⃗hi

θ⃗ are the model parameters of the forward hidden
sequence h⃗. ⃗θ are the model parameters of the
backward hidden sequence ⃗h. The vector vi is our
final vector representation of ith token in s, which
has took into account both the entire history h⃗i and
the entire future ⃗hi by concatenating the matching
Long Short-Term Memory Network (LSTM).

For ϕ(c), our simple feature function is the con-
catenated BiLSTM vectors of the top 3 items on
the stack and the first item on the buffer. A config-
uration c is represented by:

ϕ(c) = vs2 ◦ vs1 ◦ vs0 ◦ vb0

Extended: We add the feature vectors corre-
sponding to the right-most and left-most modifiers
of s0, s1 and s2, as well as the left-most modifier
of b0, reaching a total of 11 BiLSTM vectors as
extended feature representation. As we will see in
experimental sections, using the extended set does
indeed improves parsing accuracies.

2.3 Training Details

The training objective is to make the score of cor-
rect transitions always above the scores of incor-
rect transitions. We use a margin-based criteria.
Assume Tgold is the set of gold transitions at the
current configuration c. At each time stamp, the
objective function tries to maximize the margin
between Tgold and T − Tgold. The hinge loss of
a configuration c is defined as:

Lossθ(c) =
(
1 − max

to∈Tgold
SCOREθ(ϕ(c), to)

+ max
tp∈(T−Tgold)

SCOREθ(ϕ(c), tp)
)
+

Our system use the backpropagation algorithm to
calculate the gradients of the entire network (in-
cluding the MLP and the BiLSTM).

Since our parser can only deal with projective
dependency trees, we exclude all training exam-
ples with non-projective dependencies. This ap-
proach undoubtedly downgrades the performance
of our system, we plan to use pseudo-projective
approach to improve it in the future work.

238



3 Multilingual Dependency Parsing

There are 81 treebanks in the CoNLL 2017 UD
Shared Task, including 55 big treebanks, 14 PUD
treebanks (additional parallel test sets), 8 small
treebanks and 4 surprise language treebanks. For
each language treebank of UD version 2.0 training
sets, we train a parser only using its monolingual
training set (no cross-lingual features). In total, we
trained 61 models, 55 on big treebanks and 6 on
small treebanks1. Our system reads the CoNLL-U
files predicted by UDPipe, and uses morphology
(lemmas, UPOS, XPOS) predicted by UDPipe.

3.1 Dealing with Parallel Test Sets
There are 14 additional parallel test sets. Our sys-
tem simply selects one trained model when we en-
counter a parallel test set where multiple training
treebanks exist. For example, although we don’t
have English-PUD training set but we have En-
glish, English-LinES and English-ParTUT train-
ing set. So we only use the model trained on En-
glish training set to predict English-PUD test set.

3.2 Dealing with Surprise Languages
There are 4 surprise languages in the CoNLL 2017
UD Shared Task. Our system simply use the
model trained on English to predict 4 surprise lan-
guages, without looking at the input words.

4 Results

We trained our system based on a MacBook Air
with a Intel Core i5 1.6 GHz CPU and 4G memory.
We used the official TIRA (Potthast et al., 2014)
to evaluate the system. We used Dynet neural net-
work library to build our system (Neubig et al.,
2017).

The hyper-parameters of the final system used
for all the reported experiments are detailed in Ta-
ble 1.

4.1 Token Representation
We compare two constructions of xi:

• lemma and POS tag (wi ◦ pi).
• lemma, POS tag and CPOS tag (wi ◦ pi ◦ qi).
The performance of different token representa-

tions on 4 example languages are given in Table 2.
The results show that the CPOS tag improves the
LAS measure between 0.5% and 0.72%.

1In UD version 2.0 datasets, Kazakh and Uyghur only contain develop-
ment set, no training set.

Word embedding dimension 100
POS tag embedding dimension 25

CPOS tag embedding dimension 10
Label embedding dimension 25

Hidden units in MLP 100
BiLSTM layers 2

BiLSTM hidden layer dimensions 125
BiLSTM output layer dimensions 125

α (for word dropout) 0.25
Learning rate 0.1

Optimization algorithm Adam

Table 1: Hyper-parameter values used in shared
task.

Language wi ◦ pi wi ◦ pi ◦ qi
Bulgarian(bg) 83.78 84.28
Catalan(ca) 85.67 86.26
German(de) 70.77 71.49
English(en) 75.91 76.42

Table 2: The LAS score of two different token
representations on the 4 treebanks: Bulgarian(bg),
Catalan(ca), German(de), English(en).

4.2 BiLSTM Feature Representation
Performances of simple feature representation and
extended feature representation are given in Ta-
ble 3. The results show that the extended feature
representation slightly increases the performance
of our system. while the simple feature represen-
tation can significantly speed up the system.

Simple Feature Extended Feature
LAS train(sec) test(sec) LAS train(sec) test(sec)

bg 84.24 205.6 22.4 84.28 287.9 29.1
ca 85.74 663.5 37.2 86.26 878.5 48.8
de 71.15 416.8 29.8 71.49 733.2 37.4
en 76.18 375.6 24.5 76.42 524.8 31.1

Table 3: Comparison of Simple and Extended fea-
ture representations, we report LAS score, offline
training time, and TIRA testing time.

4.3 Overall Performances
In our final submitted system to the shared task,
we used lemmas, POS tags and CPOS tags in to-
ken representation and selected extended feature
representation.

The macro-average LAS of the 55 big treebanks
is 68.37% and the results for each language are

239



Language LAS(max) Language LAS(max) Language LAS(max) Language LAS(max)

ar 65.65(72.90) bg 84.28(89.81) ca 86.26(90.70) cs 83.85(90.17)
cs cac 83.22(90.43) cs cltt 68.42(85.82) cu 48.95(76.84) da 72.78(82.97)

de 71.49(80.71) el 78.72(87.38) en 76.42(82.23) en lines 72.66(82.09)
en partut 73.74(84.46) es 75.41(87.29) es ancora 78.64(89.99) et 55.40(71.65)

eu 62.89(81.44) fa 61.43(86.31) fi 69.86(85.64) fi ftb 75.13(86.81)
fr 80.07(85.51) fr sequoia 79.00(87.31) gl 79.28(83.23) got 57.02(71.36)

grc 49.30(73.19) grc proiel 60.61(75.28) he 58.10(68.16) hi 86.76(91.59)
hr 76.59(85.25) hu 57.85(77.56) id 74.40(79.19) it 86.14(90.68)
ja 73.00(91.13) ko 63.21(82.49) la ittb 74.37(87.02) la proiel 54.07(71.55)
lv 59.50(74.01) nl 68.84(80.48) nl lassysmall 71.53(87.71) no bokmaal 75.96(89.88)

no nynorsk 70.97(88.81) pl 67.63(90.32) pt 62.85(87.65) pt br 79.71(91.36)
ro 64.38(85.92) ru 56.56(83.65) ru syntagrus 82.42(92.60) sk 60.48(86.04)
sl 61.28(91.51 ) sv 61.43(85.87) sv lines 61.09(82.89) tr 49.11(62.79)
ur 61.77(82.28) vi 31.67(47.51) zh 58.03(68.56)

Table 4: The LAS F1 score of our system and best system on the 55 big treebanks: ar, bg, ca, cs, cs cac,
cs cltt, cu, da, de, el, en, en lines, en partut, es, es ancora, et, eu, fa, fi, fi ftb, fr, fr sequoia, gl, got, grc,
grc proiel, he, hi, hr, hu, id, it, ja, ko, la ittb, la proiel, lv, nl, nl lassysmall, no bokmaal, no nynorsk, pl,
pt, pt br, ro, ru, ru syntagrus, sk, sl, sv, sv lines, tr, ur, vi, zh.

Language LAS(max) Language LAS(max) Language LAS(max) Language LAS(max)

fr partut 72.40(88.13) ga 55.07(70.06) gl treegal 61.17(74.34) kk (29.22)
la 38.00(63.37) sl sst 23.77(59.07) ug (43.51) uk 20.61(75.33)

Table 5: The LAS F1 score of our system and best system on the 8 small treebanks: fr partut, ga,
gl treegal, kk, la, sl sst, ug, uk.

Language LAS(max) Language LAS(max) Language LAS(max) Language LAS(max)

ar pud 43.70(49.94) cs pud 80.44(84.42) de pud 69.13(74.86) en pud 79.02(85.51)
es pud 72.61(81.05) fi pud 71.77(88.47) fr pud 73.92(78.81) hi pud 51.07(54.49)
it pud 83.79(88.14) ja pud 76.66(83.75) pt pud 59.32(78.48) ru pud 52.73(75.71)
sv pud 54.83(78.49) tr pud 22.52(38.22)

Table 6: The LAS F1 score of our system and best system on the 14 PUD treebanks (additional parallel
test sets): ar pud, cs pud, de pud, en pud, es pud, fi pud, fr pud, hi pud, it pud, ja pud, pt pud, ru pud,
sv pud, tr pud.

Language LAS(max) Language LAS(max) Language LAS(max) Language LAS(max)

bxr 12.44(32.24) hsb 14.19(61.70) kmr 8.62(47.53) sme 10.00(48.96)

Table 7: The LAS F1 score of our system and best system on the 4 surprise language treebanks: bxr, hsb,
kmr, sme.

240



shown in Table 4. The macro-average LAS of the
8 small treebanks is 33.88% and the results for
each language are shown in Table 5. The macro-
average LAS of the 14 PUD treebanks is 63.68%
and the results for each language are shown in Ta-
ble 6. The macro-average LAS of the 4 surprise
language treebanks is 11.31% and the results for
each language are shown in Table 7. The macro-
averaged LAS F1 score of our system on all tree-
banks is 61.33%.

4.4 Computational Efficiencies

The parser is fast. Offline training time is about
300 words/sec. Prediction time on the official
TIRA is about 400 words/sec without asking for
more resources.

Memory requirements are lower than 512M for
each language.

5 Conclusions

In this paper, we present a fast and lightweight
multilingual dependency parsing system for the
CoNLL 2017 UD Shared Task, which composed
of a BiLSTMs feature extractor and a MLP classi-
fier. Our system only uses UD version 2.0 datasets
(without any additional data). The parser makes
a good ranking at some of the big treebanks. The
results suggests that the simple BiLSTM extrac-
tor is a reasonable baseline for multilingual depen-
dency parsing. We will continue to improve our
system and add cross-lingual techniques in our fu-
ture work.

Acknowledgments

We would like to thank the CoNLL 2017 UD
Shared Task organizers (Jan Hajič, Daniel Zeman,
Joakim Nivre, Filip Ginter, Slav Petrov, Milan
Straka , Martin Popel, Eduard Bejček, Martin Pot-
thast et al.).

This research is supported by
NSFC(61402175).

References

Sabine Buchholz and Erwin Marsi. 2006. Conll-x
shared task on multilingual dependency parsing. In
Proceedings of the Tenth Conference on Computa-
tional Natural Language Learning, CoNLL 2006,
New York City, USA, June 8-9, 2006. pages 149–
164. http://aclweb.org/anthology/W/W06/W06-
2920.pdf.

Danqi Chen and Christopher D Manning. 2014. A fast
and accurate dependency parser using neural net-
works. In Empirical Methods in Natural Language
Processing (EMNLP).

Eliyahu Kiperwasser and Yoav Goldberg.
2016. Simple and accurate dependency
parsing using bidirectional LSTM fea-
ture representations. TACL 4:313–327.
https://transacl.org/ojs/index.php/tacl/article/view/
885.

Marco Kuhlmann, Carlos Gómez-Rodrı́guez, and Gior-
gio Satta. 2011. Dynamic programming algo-
rithms for transition-based dependency parsers. In
The 49th Annual Meeting of the Association for
Computational Linguistics: Human Language Tech-
nologies, Proceedings of the Conference, 19-24
June, 2011, Portland, Oregon, USA. pages 673–682.
http://www.aclweb.org/anthology/P11-1068.

Graham Neubig, Chris Dyer, Yoav Goldberg, Austin
Matthews, Waleed Ammar, Antonios Anastasopou-
los, Miguel Ballesteros, David Chiang, Daniel
Clothiaux, Trevor Cohn, Kevin Duh, Manaal
Faruqui, Cynthia Gan, Dan Garrette, Yangfeng Ji,
Lingpeng Kong, Adhiguna Kuncoro, Gaurav Ku-
mar, Chaitanya Malaviya, Paul Michel, Yusuke
Oda, Matthew Richardson, Naomi Saphra, Swabha
Swayamdipta, and Pengcheng Yin. 2017. Dynet:
The dynamic neural network toolkit. arXiv preprint
arXiv:1701.03980 .

Joakim Nivre, Željko Agić, Lars Ahrenberg, et al.
2017a. Universal dependencies 2.0 CoNLL 2017
shared task development and test data. LIN-
DAT/CLARIN digital library at the Institute of For-
mal and Applied Linguistics, Charles University.
http://hdl.handle.net/11234/1-2184.

Joakim Nivre, Marie-Catherine de Marneffe, Filip Gin-
ter, Yoav Goldberg, Jan Hajič, Christopher Man-
ning, Ryan McDonald, Slav Petrov, Sampo Pyysalo,
Natalia Silveira, Reut Tsarfaty, and Daniel Zeman.
2016. Universal Dependencies v1: A multilingual
treebank collection. In Proceedings of the 10th In-
ternational Conference on Language Resources and
Evaluation (LREC 2016). European Language Re-
sources Association, Portoro, Slovenia, pages 1659–
1666.

Joakim Nivre, Johan Hall, Sandra Kübler, Ryan T. Mc-
Donald, Jens Nilsson, Sebastian Riedel, and Deniz
Yuret. 2007. The conll 2007 shared task on de-
pendency parsing. In EMNLP-CoNLL 2007, Pro-
ceedings of the 2007 Joint Conference on Empiri-
cal Methods in Natural Language Processing and
Computational Natural Language Learning, June
28-30, 2007, Prague, Czech Republic. pages 915–
932. http://www.aclweb.org/anthology/D07-1096.

Joakim Nivre et al. 2017b. Universal Dependencies
2.0. LINDAT/CLARIN digital library at the Insti-
tute of Formal and Applied Linguistics, Charles Uni-
versity, Prague, http://hdl.handle.net/

241



11234/1-1983. http://hdl.handle.net/11234/1-
1983.

Martin Potthast, Tim Gollub, Francisco Rangel, Paolo
Rosso, Efstathios Stamatatos, and Benno Stein.
2014. Improving the reproducibility of PAN’s
shared tasks: Plagiarism detection, author iden-
tification, and author profiling. In Evangelos
Kanoulas, Mihai Lupu, Paul Clough, Mark Sander-
son, Mark Hall, Allan Hanbury, and Elaine Toms,
editors, Information Access Evaluation meets Mul-
tilinguality, Multimodality, and Visualization. 5th
International Conference of the CLEF Initiative
(CLEF 14). Springer, Berlin Heidelberg New York,
pages 268–299. https://doi.org/10.1007/978-3-319-
11382-1 22.

Milan Straka, Jan Hajič, and Jana Straková. 2016. UD-
Pipe: trainable pipeline for processing CoNLL-U
files performing tokenization, morphological anal-
ysis, POS tagging and parsing. In Proceedings
of the 10th International Conference on Language
Resources and Evaluation (LREC 2016). European
Language Resources Association, Portoro, Slovenia.

David Weiss, Chris Alberti, Michael Collins, and Slav
Petrov. 2015. Structured training for neural network
transition-based parsing. CoRR abs/1506.06158.
http://arxiv.org/abs/1506.06158.

Daniel Zeman, Martin Popel, Milan Straka, Jan
Hajič, Joakim Nivre, Filip Ginter, Juhani Luotolahti,
Sampo Pyysalo, Slav Petrov, Martin Potthast, Fran-
cis Tyers, Elena Badmaeva, Memduh Gökırmak,
Anna Nedoluzhko, Silvie Cinková, Jan Hajič jr.,
Jaroslava Hlaváčová, Václava Kettnerová, Zdeňka
Urešová, Jenna Kanerva, Stina Ojala, Anna Mis-
silä, Christopher Manning, Sebastian Schuster, Siva
Reddy, Dima Taji, Nizar Habash, Herman Leung,
Marie-Catherine de Marneffe, Manuela Sanguinetti,
Maria Simi, Hiroshi Kanayama, Valeria de Paiva,
Kira Droganova, Hěctor Martı́nez Alonso, Hans
Uszkoreit, Vivien Macketanz, Aljoscha Burchardt,
Kim Harris, Katrin Marheinecke, Georg Rehm,
Tolga Kayadelen, Mohammed Attia, Ali Elkahky,
Zhuoran Yu, Emily Pitler, Saran Lertpradit, Michael
Mandl, Jesse Kirchner, Hector Fernandez Alcalde,
Jana Strnadova, Esha Banerjee, Ruli Manurung, An-
tonio Stella, Atsuko Shimada, Sookyoung Kwak,
Gustavo Mendonça, Tatiana Lando, Rattima Nitis-
aroj, and Josie Li. 2017. CoNLL 2017 Shared Task:
Multilingual Parsing from Raw Text to Universal
Dependencies. In Proceedings of the CoNLL 2017
Shared Task: Multilingual Parsing from Raw Text to
Universal Dependencies. Association for Computa-
tional Linguistics.

242


