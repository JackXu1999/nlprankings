




































Coarse-Grained Sense Annotation of Danish across Textual Domains

Sussi Olsen Bolette S. Pedersen Héctor Martı́nez Alonso Anders Johannsen

University of Copenhagen, Njalsgade 140, Copenhagen (Denmark)

{saolsen, bspedersen, alonso, ajohannsen}@hum.ku.dk

Abstract

We present the results of a coarse-grained

sense annotation task on verbs, nouns and

adjectives across six textual domains in

Danish. We present the domain-wise dif-

ferences in intercoder agreement and dis-

cuss how the applicability and validity of

the sense inventory vary depending on do-

main. We find that domain-wise agree-

ment is not higher in very canonical or

edited text. In fact, newswire text and

parliament speeches have lower agreement

than blogs and chats, probably because the

language of these text types is more com-

plex and uses more abstract concepts. We

further observe that domains differ in their

sense distribution. For instance, newswire

and magazines stand out as having a high

focus on persons, and discussion fora typi-

cally include a restricted number of senses

dependent on specialized topics. We antic-

ipate that these findings can be exploited

in automatic sense tagging when dealing

with domain shift.

1 Introduction

It is commonly observed that word meanings vary

substantially across textual domains, so that an

appropriate sense inventory for one domain may

be inappropriate or insufficient for another (Gale

et al., 1992). This essential quality of the lex-

icon poses a huge challenge to natural language

processing and underlines the need for developing

systems that are generally less sensitive to domain

shifts. The present work is framed within a project

that deals with sense inventories of different gran-

ularity and across textual domains.

The overall goal is to discover what sense in-

ventories and algorithms are manageable for an-

notation purposes and useful for automatic sense

tagging. In this paper we experiment with coarse-

grained annotations, and we analyze how reliable

the annotations are and how much they vary over

textual domains.

In Section 2 we present the backbone of our

scalable sense inventory based on a monolingual

dictionary of Danish. In Sections 3 and 4 we

present the data, describing the different corpora,

as well as the coarse-grained sense inventory. In

Section 5 we present the differences in inter-coder

agreement across the textual domains and discuss

how the applicability and validity of the sense in-

ventory vary depending on the kind of textual do-

main. Section 6 is devoted to comparisons of the

relative frequency of selected supersenses across

the six domains, and Section 7 describes the rela-

tion between specific senses via pointwise mutual

information. Section 8 provides the conclusion for

the article.

2 Scalable sense inventory

We operate with a sense inventory derived from

the Danish wordnet (DanNet), which bases its

sense inventory on a medium-sized Danish dictio-

nary, Den Danske Ordbog (DDO). This is a prag-

matic decision that leaves the more theoretical dis-

cussion aside of whether it is at all possible to de-

fine where one word sense starts and another be-

gins (Kilgarriff, 2006). The ontological labels en-

coded in DanNet, based on the EuroWordNet top

ontology as described in Pedersen et al. (2009)

and Vossen (1998), have enabled us to automat-

ically the word senses defined for the Danish vo-

cabulary onto the cross-lingual supersenses. These

are based on the Princeton Wordnet lexicographi-

cal classes1 and have become a popular choice for

coarse-grained sense tagging with the advantage

of being applicable across languages.

1https://wordnet.princeton.edu/man/
lexnames.5WN.html

Sussi Olsen, Bolette S. Pedersen, Héctor Martínez Alonso and Anders Johannsen 2015. Coarse-grained sense

annotation of Danish across textual domains. Proceedings of the workshop on Semantic resources and seman-

tic annotation for Natural Language Processing and the Digital Humanities at NODALIDA 2015. NEALT

Proceedings Series 27 / Linköping Electronic Conference Proceedings 112: 36–43.

36



Figure 1: Scales of sense granularity.

All corpora have been automatically pre-

annotated on the basis of this mapping, allowing

the annotator to choose the appropriate supersense

in context.

Figure 1 shows three points on the continuum

of word sense granularity applied in the project,

spanning from the supersense annotation experi-

ment presented in this paper over clusters of DDO

senses, to the highly fine-grained full sense inven-

tory of DDO applied to lexical samples experi-

ments (Pedersen et al., 2015).

3 Corpora across domains

In this paper we use the term domain (or textual

domain) for text type or genre, and not for sub-

ject domain; i.e. our domains are categories like

BLOG, CHAT, and MAGAZINE, instead of Politics,

Geography or Literature. The texts for annota-

tion have been selected from the Danish CLARIN

Reference Corpus (Asmussen and Halskov, 2012),

which is a general-language corpus of 45M words

spanning several text types or domains, although

with a predominance of newswire texts (48%) .

We have taken care to include a broad range of

domains in our annotation data set.

Table 1 lists the domains and text sources that

have been selected for manual annotation from

each domain. The rightmost column shows the

names of the domains in this paper.

3.1 Corpus characteristics

We have characterized aspects of language use in

the different textual domains with regard to aver-

age sentence length and the token/type ratio. The

results of the analysis can be seen in Table 2.

Average sentence length is considerably larger

for PARLIAMENT. These texts are originally

speeches, written down by professional secretary

staff, and long sentences are common in this genre.

Apart from this, differences in sentence length

Domain Av. sent.length token
type

# sentences

BLOG 19.83 3.88 600

FORUM 22.22 3.22 300

CHAT 18.66 3.83 600

MAGAZINE 20.58 2.90 600

PARLIAMENT 32.49 5.07 600

NEWSWIRE 19.47 2.66 600

Table 2: Language characteristics of the textual

domains.

between the textual domains are small. We ini-

tially expected the texts produced by profession-

als (NEWSWIRE and MAGAZINE) to have longer

sentences than user-generated texts (BLOG, CHAT

and FORUM), but found that for the user-generated

content domains the language was similar to spo-

ken language, and punctuation was less used,

which may account for the longer sentences.

The token/type ratio measures the variety of the

vocabulary, or more precisely the average number

of repetitions of each type. A higher token/type

ratio thus means a less varied choice of vocabu-

lary. PARLIAMENT is the domain with the highest

token/type ratio. The domains BLOG and CHAT

also have a rather high token/type ratio, which fits

well with the annotators’ impression that the lan-

guage in these textual domains was homogenous

with lots of repetitions. We find the highest lexical

variation in the newswire domain.

3.2 Annotation process

The texts in our analyses were manually anno-

tated by trained students. Our students annotate

using WebAnno, a web-based annotation tool de-

veloped by Technische Universität Darmstadt for

the CLARIN community (Yimam et al., 2013).

Using WebAnno allows monitoring and measur-

ing the progress and the quality of the annotation

projects in terms of inter-annotator agreement.

More than half of the sentences have been anno-

tated by two or more annotators in order to mea-

sure inter-annotator agreement, and most of these

sentences have been adjudicated by a trained lin-

guist. The remaining sentences have only been

annotated by one annotator. Three annotators

worked on the newswire texts, and two of them

did the annotations on the remaining texts. Al-

though these two annotators are skilled and, as

demonstrated by the adjudication process, adhered

closely to the instruction guidelines, the low num-

Proceedings of the workshop on Semantic resources and semantic annotation for Natural Language Processing and the Digital Humanities at NODALIDA 2015

37



Source Description Domain

Bentes blog A blog written by a woman in her forties BLOG

Selvhenter A chat forum mostly used by young people CHAT

Se og Hør A celebrity gossip magazine MAGAZINE

Folketingstaler Speeches from the Danish Parliament written down by professionals PARLIAMENT

Mangamania A chat forum for persons who love manga FORUM

Politiken A large Danish newspaper NEWSWIRE

Table 1: The domains and texts included in the annotation data set.

ber of annotators may have adversely affected the

results, leading to slightly biased data (see Section

5).

4 The extended supersense inventory

Basing the supersense inventory on the Princeton

Wordnet lexicographical classes has the advantage

of being inter-lingually comparable and interoper-

able, because wordnets for a wide range of lan-

guages are linked to Princeton Wordnet.

However, the supersense classes were not orig-

inally designed for sense annotation. During

the annotation process, we discovered that some

senses are needlessly coarse and in fact con-

found important distinctions. Therefore, we re-

fine the Princeton supersense inventory with ad-

ditional senses in cases where these cover large

groups of easily detectable word senses in Dan-

Net, such as diseases, body parts, institutions and

vehicles. Because this is a process of refinement,

we maintain compatibility with Princeton Word-

net. A new sense is introduced by subdividing an

original sense and can thus always be unambigu-

ously mapped back to the original sense.The full

set of supersenses with the extensions can be seen

in Table 3.

In total, the standard supersense set has been ex-

tended with seven noun categories and two verb

categories. For adjectives, which only have a

catch-all sense in Princeton Wordnet, we have

added four high-level categories covering mental,

social, physical and time-related property senses.

The inspiration for the new adjective senses came

from the four major sense groupings from the

Danish wordnet. Finally, three tags for verbal

satellites have been introduced to account for col-

locations, particles, and reflexive pronouns. While

these satellite tags seemingly do not carry se-

mantic meaning but are more grammatical in na-

ture, they obtain a semantic interpretation in con-

Figure 2: Annotation of phrasal verbs.

junction with a verb. In particular they ensure

that a certain particle, pronoun or element of

a collocation is understood as a lexical unit in

conjunction with its preceding verb. To exem-

plify, Figure 2 shows how the phrasal verb sige

fra (lit. say from, ‘cancel’) receives the super-

sense verb.communication, while the particle fra

received the particle tag.

Ide and Wilks (2006), Brown et al. (2010) and

more recently Melo et al. (2012) discuss coarse-

grained sense distinctions for natural language

processing, and Ciaramita and Johnson (2003)

provide one of the first to use lexicographical

classes as sense inventory for an automatic predic-

tion task.

5 Inter-annotator agreement across

domains

Over 50% of our data, 1900 sentences in total,

has been doubly annotated with the aim of mea-

suring and controlling annotator consistency. The

disagreements inform on the validity of the sense

inventory in general as well as for the different do-

mains. They also provide hints about problematic,

document-specific issues. Such issues were found

for BLOG, for instance, which includes a frequent

number of meta remarks where a certain feed can

be found, as in:

Dette indlæg blev udgivet den tirsdag,

21. september 2010 kl. 10:14 og er

gemt i Min have. Du kan følge alle

svar til dette indlæg via RSS 2.0-feedet.

(Bentes Blog)

Proceedings of the workshop on Semantic resources and semantic annotation for Natural Language Processing and the Digital Humanities at NODALIDA 2015

38



ADJ.ALL NOUN.FOOD SAT.PARTICLE
ADJ.MENTAL NOUN.GROUP SAT.RELFPRON
ADJ.PHYS NOUN.INSTITUTION VERB.ACT
ADJ.SOCIAL NOUN.LOCATION VERB.ASPECTUAL
ADJ.TIME NOUN.MOTIVE VERB.BODY
NOUN.TOPS NOUN.OBJECT VERB.CHANGE
NOUN.ABSTRACT NOUN.PERSON VERB.COGNITION
NOUN.ACT NOUN.PHENOMENON VERB.COMMUNICATION
NOUN.ANIMAL NOUN.PLANT VERB.COMPETITION
NOUN.ARTIFACT NOUN.POSSESSION VERB.CONSUMPTION
NOUN.ATTRIBUTE NOUN.PROCESS VERB.CONTACT
NOUN.BODY NOUN.QUANTITY VERB.CREATION
NOUN.BUILDING NOUN.RELATION VERB.EMOTION
NOUN.COGNITION NOUN.SHAPE VERB.MOTION
NOUN.COMMUNICATION NOUN.STATE VERB.PERCEPTION
NOUN.CONTAINER NOUN.SUBSTANCE VERB.PHENOMENON
NOUN.DISEASE NOUN.TIME VERB.POSSESSION
NOUN.DOMAIN NOUN.VEHICLE VERB.SOCIAL
NOUN.FEELING SAT.COLL VERB.STATIVE

Table 3: The standard supersense inventory with the added senses/satellite types in bold.

Domain κ-agreement % double annotated

BLOG 0.66 50 %

FORUM 0.54 66 %

CHAT 0.68 66 %

MAGAZINE 0.61 33 %

PARLIAMENT 0.59 33 %

NEWSWIRE 0.59 100 %

All domains 0.63

Table 4: Inter-annotator agreement κ across do-

mains together with the percentage of double an-

notated files.

This feed was published Tuesday

September 21 at 10:00 and is saved

under My garden. You can follow all

comments to this feed via the RSS 2.0

feed.

In such cases the annotators reached a consen-

sus on how to tag the blog-specific metadata.

Table 4 shows that even if agreement results are

generally good for the task (Artstein and Poesio,

2008), not all textual domains are equally easy

to annotate. NEWSWIRE and PARLIAMENT show

the lowest agreement, which is a somewhat sur-

prising finding, because these texts are the most

canonical and elaborate and thus arguably easier

to understand and annotate. FORUM has 300 sen-

tences, unlike the other domains, which have dou-

ble the amount. This difference has an impact in

the chance-correction measure of the κ coefficient,

making the chance-adjustment more severe. How-

ever, NEWSWIRE has more semantic types than

n
.s
ta
te

n
.a
b
st
ra
ct

n
.v
e
h
ic
le

n
.m

o
ti
v
e

n
.d
o
m
a
in

n
.q
u
a
n
ti
ty

n
.l
o
ca

ti
o
n

n
.r
e
la
ti
o
n

n
.f
e
e
lin

g
n
.f
o
o
d

n
.p
o
ss
e
ss
io
n

n
.e
v
e
n
t

n
.a
n
im

a
l

n
.g
ro
u
p

n
.p
h
e
n
o
m
e
n
o
n

n
.c
o
g
n
it
io
n

n
.b
u
ild

in
g

n
.a
ct

n
.a
rt
if
a
ct

n
.a
tt
ri
b
u
te

n
.c
o
m
m
u
n
ic
a
ti
o
n

n
.p
e
rs
o
n

n
.t
im

e
n
.i
n
st
it
u
ti
o
n

n
.b
o
d
y

n.state
n.abstract
n.vehicle
n.motive
n.domain
n.quantity
n.location
n.relation
n.feeling

n.food
n.possession

n.event
n.animal
n.group

n.phenomenon
n.cognition
n.building

n.act
n.artifact

n.attribute
n.communication

n.person
n.time

n.institution
n.body

Figure 3: Disagreement for noun senses in

NEWSWIRE.

e.g. BLOG (see Figure 3, 4 and 5), and the more

varied the text, the more difficult it will be to an-

notate and achieve high agreement. Furthermore,

PARLIAMENT texts are in a higher register than

texts from BLOG or CHAT and include more ab-

stract words (verb.cognition, noun.abstract).

Figures 3 to 5 illustrate the patterns of dis-

agreement between annotators. The matrix is con-

structed by first gathering all of the words tagged

by at least one annotator as, say, noun.abstract, ob-

serving what the other annotators tagged the same

words as. Each cell in the plotted matrix mea-

sures the number of times two annotators tagged

Proceedings of the workshop on Semantic resources and semantic annotation for Natural Language Processing and the Digital Humanities at NODALIDA 2015

39



n
.d
o
m
a
in

n
.a
tt
ri
b
u
te

n
.a
b
st
ra
ct

n
.a
ct

n
.i
n
st
it
u
ti
o
n

n
.c
o
g
n
it
io
n

n
.c
o
m
m
u
n
ic
a
ti
o
n

n
.f
e
e
lin

g
n
.a
rt
if
a
ct

n
.e
v
e
n
t

n
.p
h
e
n
o
m
e
n
o
n

n
.p
e
rs
o
n

n
.q
u
a
n
ti
ty

n
.b
u
ild

in
g

n
.l
o
ca

ti
o
n

n
.t
im

e

n.domain
n.attribute
n.abstract

n.act
n.institution
n.cognition

n.communication
n.feeling
n.artifact
n.event

n.phenomenon
n.person

n.quantity
n.building
n.location

n.time

Figure 4: Disagreement for noun senses in BLOG.

n
.d
o
m
a
in

n
.g
ro
u
p

n
.m

o
ti
v
e

n
.r
e
la
ti
o
n

n
.a
tt
ri
b
u
te

n
.s
ta
te

n
.i
n
st
it
u
ti
o
n

n
.p
o
ss
e
ss
io
n

n
.q
u
a
n
ti
ty

n
.a
b
st
ra
ct

n
.a
rt
if
a
ct

n
.e
v
e
n
t

n
.c
o
g
n
it
io
n

n
.o
b
je
ct

n
.b
o
d
y

n
.c
o
m
m
u
n
ic
a
ti
o
n

n
.p
h
e
n
o
m
e
n
o
n

n
.l
o
ca

ti
o
n

n
.f
e
e
lin

g
n
.p
e
rs
o
n

n
.a
ct

n
.b
u
ild

in
g

n
.t
im

e
n
.d
is
e
a
se

n.domain
n.group

n.motive
n.relation

n.attribute
n.state

n.institution
n.possession

n.quantity
n.abstract
n.artifact
n.event

n.cognition
n.object
n.body

n.communication
n.phenomenon

n.location
n.feeling
n.person

n.act
n.building

n.time
n.disease

Figure 5: Disagreement for noun senses in PAR-

LIAMENT.

Word Conflicting annotation

musik (music) noun.communication

dans (dancing) noun.act

natradio (night radio) noun.communication

design (design) noun.attribute

kultur (culture) noun.cognition

Table 5: Examples of disagreement between

noun.domain and another supersense.

a word with a given combination of tags (e.g. one

annotator chose noun.abstract and another chose

noun.body). Large entries on the diagonal indicate

agreement, while off-diagonal entries mean that

two senses are confused. Furthermore, the matrix

is normalized by row, and rows are sorted after the

size of the diagonal value. Thus the senses with

the worst disagreement appear first while the best

senses are located near the bottom of the matrix.

For instance, the sense noun.group has a smaller

value in the diagonal than in the column for

noun.quantity. This difference indicates that an-

notators often disagree about these senses, and

that there is little agreement on when to as-

sign the sense noun.group. Other senses like

noun.food have perfect or near-perfect agree-

ment. In all three disagreement plots, covering the

NEWSWIRE, BLOG and PARLIAMENT, we find that

the supersense noun.domain is problematic to the

annotators. This supersense has a smaller value in

the diagonal than in the column for communica-

tion and cognition.

Table 5 shows some examples of this disagree-

ment, where nouns have been annotated with

noun.domain and some other sense respectively.

As a consequence this supersense should either be

better explained and exemplified in the annotator

guidelines, or it should be discarded from the ex-

tended list altogether.

We also observe that some of the very fre-

quently used types are easier to annotate in

NEWSWIRE than in BLOG and PARLIAMENT de-

bates. This is true for supersenses such as

noun.institution and noun.communication (for su-

persense frequency see Section 6) where the

number of off-diagonal boxes are lower for

NEWSWIRE than for the other textual domains.

More metaphorical language in political speeches,

which is generally harder to annotate, could ex-

plain this difference, as well as frequent reference

Proceedings of the workshop on Semantic resources and semantic annotation for Natural Language Processing and the Digital Humanities at NODALIDA 2015

40



0 1000 2000 3000 4000 5000

noun.phenomenon
noun.event

verb.emotion
verb.change

adj.mental
noun.quantity
noun.building

verb.motion
verb.particle

verb.possession
adj.social

noun.cognition
adj.time

verb.phenomenon
adj.phys

noun.abstract
noun.act

noun.location
verb.coll

noun.artifact
verb.communication

noun.institution
verb.cognition

noun.time
verb.act

adj.all
noun.communication

verb.stative
noun.person

Most common

0 50 100 150 200 250 300

verb.top
b.communication

n.act
adj.top

verb.body
noun.top

verb.contact
noun.shape

verb.competition
verb.consumption

noun.container
noun.group
noun.plant

noun.disease
noun.substance

noun.animal
noun.motive
noun.object

noun.possession
noun.state
verb.social

verb.creation
verb.aspectual

noun.vehicle
noun.relation
noun.feeling
verb.reflpron

noun.body
verb.perception

noun.domain
noun.food

noun.attribute
Least common

Figure 6: Most and least frequent supersenses in

the complete annotated corpus.

to institutions of a very different status.

6 Sense distributions

We now analyze the variation across domains for

the top 15 supersenses. Figure 7 provides a pic-

ture of which senses are the dominating in each

selected domain compared to the sense distribu-

tion in the complete annotated corpus in Figure 6.

We observe that noun.person is by far the most

frequent tag in MAGAZINE and NEWSWIRE where

references to people make up a large portion of

the text. The MAGAZINE domain is mostly tabloid

content in which the life of famous people is dis-

cussed. In contrast, the annotated blogs refer only

sparingly to people but focus on personal reflec-

tions on life. The tag noun.communication is fre-

quent in BLOG, partly influenced by the meta com-

ments exemplified in Section 5.

The CHAT domain is the only one where the first

most frequent sense is not a nominal sense, but in-

stead the verb.stative supersense (mainly forms of

the verb være, to be). In this domain pronomi-

nal subjects are about three times as common as

in the NEWSWIRE domain, and many of the syn-

tactic slots (e.g. subject) that would otherwise be

satisfied by noun.person in other types of text are

satisfied by pronouns in this domain. This explains

why noun.person is only the fourth most frequent

sense in this domain.

In FORUM, noun.artifact is the second most

frequent sense, because the members of the fo-

rum discuss things: publications, computer parts,

and collectible card games. More abstract con-

cepts like movies or games are often referred to

Sense 1 Sense 2 PMI

verb.consumption noun.food 2.71

verb.contact noun.body 2.26

noun.food noun.container 2.04

verb.body noun.body 1.39

noun.disease noun.body 1.29

verb.competition noun.event 1.13

verb.motion verb.contact 1.10

verb.contact noun.artifact 1.08

noun.substance noun.object 1.07

noun.shape noun.body 1.06

noun.vehicle noun.substance 0.79

verb.competition noun.relation 0.75

Table 6: Mutual information for supersenses.

in their physical incarnation. The high frequency

of noun.artifact is a result of the specialized topic

of the forum.

The PARLIAMENT texts are special in several

ways, which we see reflected in the annotations.

Abstract concepts and verbal states are frequent

for this text type, which is not the case for the

other text types. Moreover, this text type has more

words per sentence and the highest token/type ra-

tio (as seen in Table 2) and thus the least varied

language.

7 Relation between senses

This section offers an overview on how super-

senses co-occur. To give account for relevant as-

sociations between senses, we use PMI (pointwise

mutual information), which is an information-

theoretical measure of association between vari-

ables. Higher PMI values indicate stronger associ-

ation, i.e. variable A is more predictable from vari-

able B.

Table 6 shows the twelve pairs of supersense

with the highest pointwise mutual information cal-

culated across sentences. We observe that some

of the associations are prototypical selectional re-

strictions like verb.comsumption + noun.food as

in:

Hvad drikker I af sodavand , hvis I gør?

What kind of soda (noun.food) do you

drink(verb.consumption), if you do?

Other associations are topical, regardless

of parts of speech, like verb.competition and

noun.event:

Proceedings of the workshop on Semantic resources and semantic annotation for Natural Language Processing and the Digital Humanities at NODALIDA 2015

41



Figure 7: Variation across domains in the top 15 supersenses.

FCK har vundet pokalfinalen.

FCK has won(verb.competition) the cup

final(noun.event).

Finally, some of the associations appear for

the same part of speech, like noun.disease and

noun.body, or noun.food and noun.container. In

these associations, one sense is a strong indicator

for the other at the topic level (diseases are bodily;

food is kept somewhere, etc).

8 Conclusion

We observe that domain-wise agreement is not

linked to factors such as how canonical the text

is, and whether the text is professionally edited or

not. The NEWSWIRE and PARLIAMENT domains,

which contain the most thoroughly edited text in

the corpus, have the lowest agreement, which is

somewhat unexpected. Here we suggest that cer-

tain words and sense variations are intrinsically

more difficult, e.g. abstract senses. In compari-

son, FORUM has a clear topic, constraining the dis-

course elements and their semantic type and thus

making annotation easier.

The annotation task yields good agreement for

supersense annotations across a number of do-

mains, matching or exceeding the level of agree-

ment found in previous, comparable studies. How-

ever, a few supersenses are hard to apply uni-

formly across all domains, calling for further anal-

ysis and perhaps an adjustment of the sense inven-

tory. Abstract noun supersenses as well as verb

supersenses related to cognition were generally

harder to annotate consistently than more concrete

supersenses.

By examining the top 15 supersenses of each

domain, we have also shown how textual domains

differ in their sense distribution. These observa-

tions can later be exploited in automatic sense tag-

ging when dealing with domain shift. One way to

do this is pre-estimating the most-frequent sense

of the target domain using a lexical knowledge

base like DanNet.

For experiments with automatic tagging of

Danish data based on the annotations, we re-

fer to Martı́nez Alonso et al. (2015a) and

Martı́nez Alonso et al. (2015b).

Proceedings of the workshop on Semantic resources and semantic annotation for Natural Language Processing and the Digital Humanities at NODALIDA 2015

42



Acknowledgements

We wish to thank the anonymous reviewers for

their valuable comments. Likewise, we thank all

the project staff, as well as our team of annotators.

The research resulting in this publication has

been funded by the Danish Research Coun-

cil under the Semantic Processing across Do-

mains project: http://cst.ku.dk/english/

projekter/semantikprojekt/.

References

Ron Artstein and Massimo Poesio. 2008. Inter-coder
agreement for computational linguistics. Computa-
tional Linguistics, 34(4):555–596.

Jørg Asmussen and Jakob Halskov. 2012. The
CLARIN DK Reference Corpus. In Sprogteknolo-
gisk Workshop.

Susan Windisch Brown, Travis Rood, and Martha
Palmer. 2010. Number or nuance: Which factors
restrict reliable word sense annotation? In LREC.

Massimiliano Ciaramita and Mark Johnson. 2003. Su-
persense tagging of unknown nouns in wordnet. In
Proceedings of the 2003 conference on Empirical
methods in natural language processing, pages 168–
175. Association for Computational Linguistics.

Gerard De Melo, Collin F Baker, Nancy Ide, Rebecca J
Passonneau, and Christiane Fellbaum. 2012. Em-
pirical comparisons of masc word sense annotations.
In LREC, pages 3036–3043.

William A Gale, Kenneth W Church, and David
Yarowsky. 1992. One sense per discourse. In Pro-
ceedings of the workshop on Speech and Natural
Language, pages 233–237. Association for Compu-
tational Linguistics.

Nancy Ide and Yorick Wilks. 2006. Making sense
about sense. In Word sense disambiguation, pages
47–73. Springer.

Adam Kilgarriff. 2006. Word senses. In Eneko Agirre
and Philip Edmonds, editors, Word Sense Disam-
biguation, pages 29–46. Springer.

Héctor Martı́nez Alonso, Anders Johannsen, An-
ders Søgaard, Sussi Olsen, Anna Braasch, Sanni
Nimb, Nicolai Hartvig Sørensen, and Bolette Sand-
ford Pedersen. 2015a. Supersense tagging for dan-
ish. In Nodalida.

Héctor Martı́nez Alonso, Barbara Plank, Anders Jo-
hannsen, and Søgaard. 2015b. Active learning for
sense annotation. In Nodalida.

Bolette Sandford Pedersen, Sanni Nimb, Jørg As-
mussen, Nicolai Hartvig Sørensen, Lars Trap-
Jensen, and Henrik Lorentzen. 2009. Dannet:

the challenge of compiling a wordnet for danish
by reusing a monolingual dictionary. Language re-
sources and evaluation, 43(3):269–299.

Bolette Pedersen, Anna Braasch, Sanni Nimb, and
Sussi Olsen. 2015. Betydningsinventar - i ordbøger
og i løbende tekst, forthcoming. In Presentation at
the 13th Conference on Lexicography in the Nordic
Countries.

Piek Vossen. 1998. EuroWordNet: A multilingual
database with lexical semantic networks. Springer.

Seid Muhie Yimam, Iryna Gurevych, Richard Eckart
de Castilho, and Chris Biemann. 2013. Webanno:
A flexible, web-based and visually supported sys-
tem for distributed annotations. In ACL (Conference
System Demonstrations), pages 1–6.

Proceedings of the workshop on Semantic resources and semantic annotation for Natural Language Processing and the Digital Humanities at NODALIDA 2015

43


