



















































Pre-training of Hidden-Unit CRFs


Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics
and the 7th International Joint Conference on Natural Language Processing (Short Papers), pages 192–198,

Beijing, China, July 26-31, 2015. c©2015 Association for Computational Linguistics

Pre-training of Hidden-Unit CRFs

Young-Bum Kim† Karl Stratos‡ Ruhi Sarikaya†

†Microsoft Corporation, Redmond, WA
‡Columbia University, New York, NY

{ybkim, ruhi.sarikaya}@microsoft.com
stratos@cs.columbia.edu

Abstract

In this paper, we apply the concept of pre-
training to hidden-unit conditional ran-
dom fields (HUCRFs) to enable learning
on unlabeled data. We present a simple
yet effective pre-training technique that
learns to associate words with their clus-
ters, which are obtained in an unsuper-
vised manner. The learned parameters are
then used to initialize the supervised learn-
ing process. We also propose a word clus-
tering technique based on canonical corre-
lation analysis (CCA) that is sensitive to
multiple word senses, to further improve
the accuracy within the proposed frame-
work. We report consistent gains over
standard conditional random fields (CRFs)
and HUCRFs without pre-training in se-
mantic tagging, named entity recognition
(NER), and part-of-speech (POS) tagging
tasks, which could indicate the task inde-
pendent nature of the proposed technique.

1 Introduction

Despite the recent accuracy gains of the deep
learning techniques for sequence tagging prob-
lems (Collobert and Weston, 2008; Collobert et
al., 2011; Mohamed et al., 2010; Deoras et al.,
2012; Xu and Sarikaya, 2013; Yao et al., 2013;
Mesnil et al., 2013; Wang and Manning, 2013;
Devlin et al., 2014), conditional random fields
(CRFs) (Lafferty et al., 2001; Sutton and McCal-
lum, 2006) still have been widely used in many
research and production systems for the problems
due to the effectiveness and simplicity of train-
ing, which does not involve task specific param-
eter tuning (Collins, 2002; McCallum and Li,
2003; Sha and Pereira, 2003; Turian et al., 2010;
Kim and Snyder, 2012; Celikyilmaz et al., 2013;
Sarikaya et al., 2014; Anastasakos et al., 2014;

Kim et al., 2014; Kim et al., 2015a; Kim et al.,
2015c; Kim et al., 2015b). The objective function
for CRF training operates globally over sequence
structures and can incorporate arbitrary features.
Furthermore, this objective is convex and can be
optimized relatively efficiently using dynamic pro-
gramming.

Pre-training has been widely used in deep learn-
ing (Hinton et al., 2006) and is one of the distin-
guishing advantages of deep learning models. The
best results obtained across a wide range of tasks
involve unsupervised pre-training phase followed
by the supervised training phase. The empirical
results (Erhan et al., 2010) suggest that unsuper-
vised pre-training has the regularization effect on
the learning process and also results in a model
parameter configuration that places the model near
the basins of attraction of minima that support bet-
ter generalization.

While pre-training became a standard steps in
many deep learning model training recipes, it has
not been applied to the family of CRFs. There
were several reasons for that; (i) the shallow and
linear nature of basic CRF model topology, which
limits their expressiveness to the inner product be-
tween data and model parameters, and (ii) Lack
of a training criterion and configuration to employ
pre-training on unlabeled data in a task indepen-
dent way.

Hidden-unit CRFs (HUCRFs) of Maaten et al.
(2011) provide a deeper model topology and im-
prove the expressive power of the CRFs but it
does not address how to train them in a task inde-
pendent way using unlabeled data. In this paper,
we present an effective technique for pre-training
of HUCRFs that can potentially lead to accuracy
gains over HUCRF and basic linear chain CRF
models. We cluster words in the text and treat clus-
ters as pseudo-labels to train an HUCRF. Then we
transfer the parameters corresponding to observa-
tions to initialize the training process on labeled

192



Figure 1: Graphical representation of hidden unit
CRFs.

data. The intuition behind this is that words that
are clustered together tend to assume the same la-
bels. Therefore, learning the model parameters to
assign the correct cluster ID to each word should
accrue to assigning the correct task specific label
during supervised learning.

This pre-training step significantly reduces the
challenges in training a high-performance HUCRF
by (i) acquiring a broad feature coverage from un-
labeled data and thus improving the generalization
of the model to unseen events, (ii) finding a good a
initialization point for the model parameters, and
(iii) regularizing the parameter learning by min-
imizing variance and introducing a bias towards
configurations of the parameter space that are use-
ful for unsupervised learning.

We also propose a word clustering technique
based on canonical correlation analysis (CCA)
that is sensitive to multiple word senses. For ex-
ample, the resulting clusters can differentiate the
instance of “bank” in the sense of financial insti-
tutions and the land alongside the river. This is an
important point as different senses of a word are
likely to have a different task specific tag. Putting
them in different clusters would enable the HU-
CRF model to learn the distinction in terms of la-
bel assignment.

2 Model

2.1 HUCRF definition

A HUCRF incorporates a layer of binary-valued
hidden units z = z1 . . . zn ∈ {0, 1} for each pair
of observation sequence x = x1 . . . xn and label
sequence y = y1 . . . yn. It is parameterized by

Figure 2: Illustration of a pre-training scheme for
HUCRFs.

θ ∈ Rd and γ ∈ Rd′ and defines a joint probability
of y and z conditioned on x as follows:

pθ,γ(y, z|x) =
exp(θ>Φ(x, z) + γ>Ψ(z, y))∑

z′∈{0,1}n
y′∈Y(x,z′)

exp(θ>Φ(x, z′) + γ>Ψ(z′, y′))

where Y(x, z) is the set of all possible label
sequences for x and z, and Φ(x, z) ∈ Rd
and Ψ(z, y) ∈ Rd′ are global feature func-
tions that decompose into local feature
functions: Φ(x, z) =

∑n
j=1 φ(x, j, zj) and

Ψ(z, y) =
∑n

j=1 ψ(zj , yj−1, yj).

HUCRF forces the interaction between the ob-
servations and the labels at each position j to go
through a latent variable zj : see Figure 1 for illus-
tration. Then the probability of labels y is given
by marginalizing over the hidden units,

pθ,γ(y|x) =
∑

z∈{0,1}n
pθ,γ(y, z|x)

As in restricted Boltzmann machines (Larochelle
and Bengio, 2008), hidden units are conditionally
independent given observations and labels. This
allows for efficient inference with HUCRFs de-
spite their richness (see Maaten et al. (2011) for
details). We use a perceptron-style algorithm of
Maaten et al. (2011) for training HUCRFs.

2.2 Pre-training HUCRFs
How parameters are initialized for training is im-
portant for HUCRFs because the objective func-
tion is non-convex. Instead of random initializa-
tion, we use a simple and effective initialization
scheme (in a similar spirit to the pre-training meth-
ods in neural networks) that can leverage a large

193



body of unlabeled data. This scheme is a simple
two-step approach.

In the first step, we cluster observed tokens in
M unlabeled sequences and treat the clusters as la-
bels to train an intermediate HUCRF. Let C(u(i))
be the “cluster sequence” of the i-th unlabeled se-
quence u(i). We compute:

(θ1, γ1) ≈ arg max
θ,γ

M∑
i=1

log pθ,γ(C(u(i))|u(i)))

In the second step, we train a final model on the
labeled data {(x(i), y(i))}Ni=1 using θ1 as an ini-
tialization point:

(θ2, γ2) ≈ arg max
θ,γ:

init(θ,θ1)

N∑
i=1

log pθ,γ(y(i)|x(i))

While we can use γ1 for initialization as well, we
choose to only use θ1 since the label space is task-
specific. This process is illustrated in Figure 2.

In summary, the first step is used to find
generic parameters between observations and hid-
den states; the second step is used to specialize the
parameters to a particular task. Note that the first
step also generates additional feature types absent
in the labeled data which can be useful at test time.

3 Multi-Sense Clustering via CCA

The proposed pre-training method requires assign-
ing a cluster to each word in unlabeled text. Since
it learns to associate the words to their clusters, the
quality of clusters becomes important. A straight-
forward approach would be to perform Brown
clustering (Brown et al., 1992), which has been
very effective in a variety of NLP tasks (Miller et
al., 2004; Koo et al., 2008).

However, Brown clustering has some undesir-
able aspects for our purpose. First, it assigns a
single cluster to each word type. Thus a word that
can be used very differently depending on its con-
text (e.g., “bank”) is treated the same across the
corpus. Second, the Brown model uses only un-
igram and bigram statistics; this can be an issue
if we wish to capture semantics in larger contexts.
Finally, the algorithm is rather slow in practice for
large vocabulary size.

To mitigate these limitations, we propose multi-
sense clustering via canonical correlation analy-
sis (CCA). While there are previous work on in-
ducing multi-sense representations (Reisinger and

CCA-PROJ
Input: samples (x(1), y(1)) . . . (x(n), y(n)) ∈ {0, 1}d ×
{0, 1}d′ , dimension k
Output: projections A ∈ Rd×k and B ∈ Rd′×k

• Calculate B ∈ Rd×d′ , u ∈ Rd, and v ∈ Rd′ :

Bi,j =

n∑
l=1

[[x
(l)
i = 1]][[y

(l)
j = 1]]

ui =

n∑
l=1

[[x
(l)
i = 1]] vi =

n∑
l=1

[[y
(l)
i = 1]]

• Define Ω̂ = diag(u)−1/2Bdiag(v)−1/2.

• Calculate rank-k SVD Ω̂. Let U ∈ Rd×k (V ∈ Rd′×k)
be a matrix of the left (right) singular vector corre-
sponding to the largest k singular values.

• Let A = diag(u)−1/2U and B = diag(v)−1/2V .

Figure 3: Algorithm for deriving CCA projections
from samples of two variables.

Mooney, 2010; Huang et al., 2012; Neelakantan et
al., 2014), our proposed method is simpler and is
shown to perform better in experiments.

3.1 Review of CCA
CCA is a general technique that operates on a
pair of multi-dimensional variables. CCA finds
k dimensions (k is a parameter to be specified)
in which these variables are maximally correlated.
Let x(1) . . . x(n) ∈ Rd and y(1) . . . y(n) ∈ Rd′ be
n samples of the two variables. For simplicity, as-
sume that these variables have zero mean. Then
CCA computes the following for i = 1 . . . k:

arg max
ai∈Rd, bi∈Rd′ :
a>i ai′=0 ∀i′<i
b>i bi′=0 ∀i′<i

∑n
l=1(a

>
i x

(l))(b>i y
(l))√∑n

l=1(a
>
i x

(l))2
√∑n

l=1(b
>
i y

(l))2

In other words, each (ai, bi) is a pair of pro-
jection vectors such that the correlation between
the projected variables a>i x

(l) and b>i y
(l) (now

scalars) is maximized, under the constraint that
this projection is uncorrelated with the previous
i − 1 projections. A method based on singu-
lar value decomposition (SVD) provides an effi-
cient and exact solution to this problem (Hotelling,
1936). The resulting solution A ∈ Rd×k (whose
i-th column is ai) and B ∈ Rd′×k (whose i-th col-
umn is bi) can be used to project the variables from

194



Input: word-context pairs from a corpus of length n:
D = {(w(l), c(l))}nl=1, dimension k

Output: cluster C(l) ≤ k for l = 1 . . . n
• Use the algorithm in Figure 3 to compute projection

matrices (ΠW , ΠC) = CCA-PROJ(D, k).

• For each word type w, perform k-means clustering on
Cw = {Π>Cc(l) ∈ Rk : w(l) = w} to partition occur-
rences of w in the corpus into at most k clusters.

• Label each word w(l) with the cluster obtained from
the previous step. Let D̄ = {(w̄(l), c̄(l))}nl=1 denote
this new dataset.

• (ΠW̄ , ΠC̄) = CCA-PROJ(D̄, k)

• Perform k-means clustering on {Π>̄W w̄(l) ∈ Rk}.

• Let C(l) be the cluster corresponding to Pi>̄W v(l).

Figure 4: Algorithm for clustering of words in a
corpus sensitive to multiple word senses.

the original d- and d′-dimensional spaces to a k-
dimensional space:

x ∈ Rd −→ A>x ∈ Rk
y ∈ Rd′ −→ B>y ∈ Rk

The new k-dimensional representation of each
variable now contains information about the other
variable. The value of k is usually selected to be
much smaller than d or d′, so the representation
is typically also low-dimensional. The CCA algo-
rithm is given in Figure 3: we assume that samples
are 0-1 indicator vectors. In practice, calculating
the CCA projections is fast since there are many
efficient SVD implantations available. Also, CCA
can incorporate arbitrary context definitions unlike
the Brown algorithm.

3.2 Multi-sense clustering
CCA projections can be used to obtain vector
representations for both words and contexts. If
we wished for only single-sense clusters (akin
to Brown clusters), we could simply perform k-
means on word embeddings.

However, we can exploit context embeddings to
infer word senses. For each word type, we create
a set of context embeddings corresponding to all
occurrences of that word type. Then we cluster
these embeddings; we use an implementation of
k-means which automatically determines the num-
ber of clusters upper bounded by k. The number

of word senses, k, is set to be the number of la-
bel types occurring in labeled data (for each task-
specific training set).

We use the resulting context clusters to deter-
mine the sense of each occurrence of that word
type. For instance, an occurrence of “bank” might
be labeled as “bank1” near “financial” or “Chase”
and “bank2” near “shore” or “edge”.

This step is for disambiguating word senses, but
what we need for our pre-training method is the
partition of words in the corpus. Thus we perform
a second round of CCA on these disambiguated
words to obtain corresponding word embeddings.
As a final step, we perform k-means clustering on
the disambiguated word embeddings to obtain the
partition of words in the corpus. The algorithm is
shown in Table 4.

4 Experiments

To validate the effectiveness of our pre-training
method, we experiment on three sequence label-
ing tasks: semantic tagging, named entity recogni-
tion (NER), and part-of-speech (POS) tagging. We
used L-BFGS for training CRFs 1 and the averaged
perceptron for training HUCRFs. The number of
hidden variables was set to 500.

4.1 Semantic tagging

The goal of semantic tagging is to assign the cor-
rect semantic tag to a words in a given utter-
ance. We use a training set of 50-100k queries
across domains and the test set of 5-10k queries.
For pre-training, we collected 100-200k unlabeled
text from search log data and performed a stan-
dard preprocessing step. We use n-gram features
up to n = 3, regular expression features, do-
main specific lexicon features and Brown clus-
ters. We present the results for various config-
urations in Table 1. HUCRF with random ini-
tialization from Gaussian distribution (HUCRFG)
boosts the average performance up to 90.52%
(from 90.39% of CRF). HUCRF with pre-training
with Brown clusters (HUCRFB) and CCA-based
clusters (HUCRFC) further improves performance
to 91.36% and 91.37%, respectively.

Finally, when we use multi-sense cluster
(HUCRFC+), we obtain an F1-score of 92.01%.
We also compare other alternative pre-training
methods. HUCRF with pre-training RBM

1For CRFs, we found that L-BFGS had higher perfor-
mance than SGD and the average percetpron.

195



alarm calendar comm. note ondevice places reminder weather home avg
CRF 92.8 89.59 92.13 88.02 88.21 89.64 87.72 96.93 88.51 90.39
HUCRFG 91.79 89.56 92.08 88.42 88.64 90.99 89.21 96.38 87.63 90.52
HUCRFR 91.64 89.6 91.77 88.64 87.43 88.54 88.83 95.88 88.17 90.06
HUCRFB 92.86 90.58 92.8 88.72 89.37 91.14 90.05 97.63 89.08 91.36
HUCRFC 92.82 90.61 92.84 88.69 88.94 91.45 90.31 97.62 89.04 91.37
HUCRFS 91.2 90.53 92.43 88.7 88.09 90.91 89.54 97.24 88.91 90.84
HUCRFNS 90.8 89.88 91.54 87.83 88.15 91.02 88.2 96.77 89.02 90.36
HUCRFC+ 92.86 91.94 93.72 89.18 89.97 93.22 91.51 97.95 89.66 92.22

Table 1: Comparison of slot F1 scores on nine personal assistant domains. The numbers in boldface
are the best performing method. Subscripts mean the following: G = random initialization from a
Gaussian distribution with variance 10−4, R = pre-training with Restricted Boltzmann Machine (RBM)
using contrastive divergence of (Hinton, 2002), C = pre-training with CCA-based clusters, B = pre-
training with Brown clusters, S = pre-training with skip-ngram multi-sense clusters with fixed cluster
size 5, NS = pre-training with non-parametric skip-ngram multi-sense clusters, C+ = pre-training with
CCA-based multi-sense clusters.

(HUCRFR) does not perform better than with
random initialization. The skip-gram clusters
(HUCRFS , HUCRFSN ) do not perform well ei-
ther. Some examples of disambiguated word oc-
currences are shown below, demonstrating that the
algorithm in Figure 3 yields intuitive clusters.

NER POS
Test-A Test-B Test-A Test-B

CRF 90.75 86.37 95.51 94.99
HUCRFG 89.99 86.72 95.14 95.08
HUCRFR 90.12 86.43 95.42 94.14
HUCRFB 90.27 87.24 95.55 95.33
HUCRFC 90.9 86.89 95.67 95.23
HUCRFS 90.18 86.84 95.48 95.07
HUCRFNS 90.14 85.66 95.35 94.82
HUCRFC+ 92.04 88.41 95.88 95.48

Table 2: F1 Score for NER task and Accuracy for
POS task.

word context

Book

a book(1) store within 5 miles of my address
find comic book(1) stores in novi michigan

book(2) restaurant for tomorrow
book(2) taxi to pizza hut

look for book(3) chang dong tofu house in pocono
find book(3) bindery seattle

High

restaurant nearby with high(1) ratings
show me high(1) credit restaurant nearby

the address for shelley high(2) school
directions to leota junior high(2) school

what’s the distance to kilburn high(3) road
domino’s pizza in high(3) ridge missouri

Table 3: Examples of disambiguated word occur-
rences.

4.2 NER & POS tagging
We use CoNLL 2003 dataset for NER and POS
with the standard train/dev/test split. For pre-

training, we used the Reuters-RCV1 corpus. It
contains 205 millions tokens with 1.6 million
types. We follow same preprocessing steps as in
semantic tagging. Also, we use the NER features
used in Turian et al. (2010) and POS features used
in Maaten et al. (2011).

We present the results for both tasks in Table 2.
In both tasks, the HUCRFC+ yields the best per-
formance, achieving error reduction of 20% (Test-
A) and 13% (Test-B) for NER as well as 15%
(Test-A) and 8% (Test-B) for POS over HUCRFR.
Note that HUCRF does not always perform bet-
ter than CRF when initialized randomly. How-
ever, However, HUCRF consistently outperforms
CRF with the pre-training methods proposed in
this work.

5 Conclusion

We presented an effective technique for pre-
training HUCRFs. Our method transfers observa-
tion parameters trained on clustered text to initial-
ize the training process. We also proposed a word
clustering scheme based on CCA that is sensitive
to multiple word senses. Using our pre-training
method, we reported significant improvement over
several baselines in three sequence labeling tasks.

References

Tasos Anastasakos, Young-Bum Kim, and Anoop Deo-
ras. 2014. Task specific continuous word represen-
tations for mono and multi-lingual spoken language
understanding. In ICASSP, pages 3246–3250. IEEE.

Peter F Brown, Peter V Desouza, Robert L Mercer,
Vincent J Della Pietra, and Jenifer C Lai. 1992.

196



Class-based n-gram models of natural language.
Computational linguistics, 18(4):467–479.

Asli Celikyilmaz, Dilek Z Hakkani-Tür, Gökhan Tür,
and Ruhi Sarikaya. 2013. Semi-supervised seman-
tic tagging of conversational understanding using
markov topic regression. In ACL, pages 914–923.
Association for Computational Linguistics.

Michael Collins. 2002. Discriminative training meth-
ods for hidden markov models: Theory and exper-
iments with perceptron algorithms. In Proceedings
of the ACL-02 conference on Empirical methods in
natural language processing-Volume 10, pages 1–8.
Association for Computational Linguistics.

Ronan Collobert and Jason Weston. 2008. A unified
architecture for natural language processing: Deep
neural networks with multitask learning. In ICML,
pages 160–167. ACM.

Ronan Collobert, Jason Weston, Léon Bottou, Michael
Karlen, Koray Kavukcuoglu, and Pavel Kuksa.
2011. Natural language processing (almost) from
scratch. The Journal of Machine Learning Re-
search, 12:2493–2537.

Anoop Deoras, Ruhi Sarikaya, Gökhan Tür, and
Dilek Z Hakkani-Tür. 2012. Joint decoding for
speech recognition and semantic tagging. In INTER-
SPEECH.

Jacob Devlin, Rabih Zbib, Zhongqiang Huang, Thomas
Lamar, Richard Schwartz, and John Makhoul. 2014.
Fast and robust neural network joint models for sta-
tistical machine translation. In ACL, volume 1,
pages 1370–1380.

Dumitru Erhan, Yoshua Bengio, Aaron Courville,
Pierre-Antoine Manzagol, Pascal Vincent, and Samy
Bengio. 2010. Why does unsupervised pre-training
help deep learning? The Journal of Machine Learn-
ing Research, 11:625–660.

Geoffrey Hinton, Simon Osindero, and Yee-Whye Teh.
2006. A fast learning algorithm for deep belief nets.
Neural computation, 18(7):1527–1554.

Geoffrey Hinton. 2002. Training products of experts
by minimizing contrastive divergence. Neural com-
putation, 14(8):1771–1800.

Harold Hotelling. 1936. Relations between two sets of
variates. Biometrika, 28(3/4):321–377.

Eric H. Huang, Richard Socher, Christopher D. Man-
ning, and Andrew Y. Ng. 2012. Improving Word
Representations via Global Context and Multiple
Word Prototypes. In ACL. Association for Compu-
tational Linguistics.

Young-Bum Kim and Benjamin Snyder. 2012. Univer-
sal grapheme-to-phoneme prediction over latin al-
phabets. In EMNLP, pages 332–343. Association
for Computational Linguistics.

Young-Bum Kim, Heemoon Chae, Benjamin Snyder,
and Yu-Seop Kim. 2014. Training a korean srl
system with rich morphological features. In ACL,
pages 637–642. Association for Computational Lin-
guistics.

Young-Bum Kim, Minwoo Jeong, Karl Stratos, and
Ruhi Sarikaya. 2015a. Weakly supervised slot
tagging with partially labeled sequences from web
search click logs. In HLT-NAACL, pages 84–92. As-
sociation for Computational Linguistics.

Young-Bum Kim, Karl Stratos, Xiaohu Liu, and Ruhi
Sarikaya. 2015b. Compact lexicon selection with
spectral methods. In ACL. Association for Compu-
tational Linguistics.

Young-Bum Kim, Karl Stratos, Ruhi Sarikaya, and
Minwoo Jeong. 2015c. New transfer learning tech-
niques for disparate label sets. In ACL. Association
for Computational Linguistics.

Terry Koo, Xavier Carreras, and Michael Collins.
2008. Simple semi-supervised dependency parsing.

John Lafferty, Andrew McCallum, and Fernando CN
Pereira. 2001. Conditional random fields: Prob-
abilistic models for segmenting and labeling se-
quence data. In ICML, pages 282–289.

Hugo Larochelle and Yoshua Bengio. 2008. Classifi-
cation using discriminative restricted boltzmann ma-
chines. In ICML.

Laurens van der Maaten, Max Welling, and
Lawrence K Saul. 2011. Hidden-unit condi-
tional random fields. In AISTAT.

Andrew McCallum and Wei Li. 2003. Early results
for named entity recognition with conditional ran-
dom fields, feature induction and web-enhanced lex-
icons. In HLT-NAACL, pages 188–191. Association
for Computational Linguistics.

Grégoire Mesnil, Xiaodong He, Li Deng, and Yoshua
Bengio. 2013. Investigation of recurrent-neural-
network architectures and learning methods for spo-
ken language understanding. In INTERSPEECH,
pages 3771–3775.

Scott Miller, Jethran Guinness, and Alex Zamanian.
2004. Name tagging with word clusters and discrim-
inative training. In HLT-NAACL, volume 4, pages
337–342. Citeseer.

Abdel-rahman Mohamed, Dong Yu, and Li Deng.
2010. Investigation of full-sequence training of deep
belief networks for speech recognition. In INTER-
SPEECH, pages 2846–2849.

Arvind Neelakantan, Jeevan Shankar, Alexandre Pas-
sos, and Andrew McCallum. 2014. Efficient non-
parametric estimation of multiple embeddings per
word in vector space. In EMNLP. Association for
Computational Linguistics.

197



Joseph Reisinger and Raymond J Mooney. 2010.
Multi-prototype vector-space models of word mean-
ing. In Human Language Technologies: The 2010
Annual Conference of the North American Chap-
ter of the Association for Computational Linguistics,
pages 109–117. Association for Computational Lin-
guistics.

Ruhi Sarikaya, Asli Celikyilmaz, Anoop Deoras, and
Minwoo Jeong. 2014. Shrinkage based features for
slot tagging with conditional random fields. In Proc.
of Interspeech.

Fei Sha and Fernando Pereira. 2003. Shallow parsing
with conditional random fields. In Proceedings of
the 2003 Conference of the North American Chapter
of the Association for Computational Linguistics on
Human Language Technology-Volume 1, pages 134–
141. Association for Computational Linguistics.

Charles Sutton and Andrew McCallum. 2006. An in-
troduction to conditional random fields for relational
learning. Introduction to statistical relational learn-
ing, pages 93–128.

Joseph Turian, Lev Ratinov, and Yoshua Bengio. 2010.
Word representations: a simple and general method
for semi-supervised learning. In Proceedings of the
48th annual meeting of the association for compu-
tational linguistics, pages 384–394. Association for
Computational Linguistics.

Mengqiu Wang and Christopher D Manning. 2013. Ef-
fect of non-linear deep architecture in sequence la-
beling. In ICML Workshop on Deep Learning for
Audio, Speech and Language Processing.

Puyang Xu and Ruhi Sarikaya. 2013. Convolutional
neural network based triangular crf for joint intent
detection and slot filling. In IEEE Workshop on
Automatic Speech Recognition and Understanding
(ASRU), pages 78–83. IEEE.

Kaisheng Yao, Geoffrey Zweig, Mei-Yuh Hwang,
Yangyang Shi, and Dong Yu. 2013. Recurrent neu-
ral networks for language understanding. In INTER-
SPEECH, pages 2524–2528.

198


