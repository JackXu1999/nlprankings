



















































A statistical approach for Non-Sentential Utterance Resolution for Interactive QA System


Proceedings of the SIGDIAL 2015 Conference, pages 335–343,
Prague, Czech Republic, 2-4 September 2015. c©2015 Association for Computational Linguistics

A Statistical Approach for Non-Sentential Utterance Resolution for
Interactive QA System

Dinesh Raghu ∗
IBM Watson

diraghu1@in.ibm.com

Sathish Indurthi ∗
IBM Watson

saindurt@in.ibm.com

Jitendra Ajmera
IBM Watson

jajmera1@in.ibm.com

Sachindra Joshi
IBM Watson

jsachind@in.ibm.com

Abstract

Non-Sentential Utterances (NSUs) are
short utterances that do not have the form
of a full sentence but nevertheless convey a
complete sentential meaning in the context
of a conversation. NSUs are frequently
used to ask follow up questions during in-
teractions with question answer (QA) sys-
tems resulting into in-correct answers be-
ing presented to their users. Most of the
current methods for resolving such NSUs
have adopted rule or grammar based ap-
proach and have limited applicability.

In this paper, we present a data driven sta-
tistical method for resolving such NSUs.
Our method is based on the observation
that humans identify keyword appearing
in an NSU and place them in the context
of conversation to construct a meaningful
sentence. We adapt the keyword to ques-
tion (K2Q) framework to generate natu-
ral language questions using keywords ap-
pearing in an NSU and its context. The
resulting questions are ranked using differ-
ent scoring methods in a statistical frame-
work. Our evaluation on a data-set col-
lected using mTurk shows that the pro-
posed method perform significantly bet-
ter than the previous work that has largely
been rule based.

1 Introduction

Recently Question Answering (QA) systems have
been built with high accuracies [Ferrucci, 2012].
The obvious next step for them is to assist peo-
ple by improving their experience in seeking day
to day information needs like product support and
troubleshooting. For QA systems to be effective

∗D. Raghu and S. Indurthi contributed equally to this work

and usable they need to evolve into conversational
systems. One extra challenge that conversational
systems throw is that users tend to form succes-
sive queries that allude to the entities and concepts
made in the past utterances. Therefore, among
other things, such systems need to be equipped
with the ability to understand what are called Non-
Sentential Utterances (NSUs) [Fernández et al.,
2005, Fernández, 2006].

NSUs are utterances that do not have the form
of a full sentence, according to the most tradi-
tional grammars, but nevertheless convey a com-
plete sentential meaning. Consider for example,
the conversation between a sales staff of a mobile
store (S) and one of their customers (C), where C:2
and C:3 are examples of NSUs.

S:1 Hi, How may I help you

C:1 How much does an Apple iPhone 6 cost ?

S:2 $ . . .

C:2 What about 6S ?

S:3 $ . . .

C:3 with 64 GB ?

S:4 $ . . .

Humans have the ability to understand these
NSUs in a conversation based on the context de-
rived so far. The conversation context could in-
clude topic(s) under discussion, the past history
between the participants or even their geograph-
ical location.

In the example above, the sales staff, based on
her domain knowledge, knows that iPhone 6 and
iPhone 6S are different models of iPhone and all
phones have a cost feature associated with them.
Therefore an utterance What about 6S, in the con-
text of utterance How much does an Apple iPhone
6 cost, would mean How much does an Apple
iPhone 6S cost. Similarly, 64 GB is an attribute
of iPhone 6S and therefore the utterance with 64

335



GB in the context of utterance How much does an
Apple iPhone 6S cost would mean How much does
an Apple iPhone 6s with 64 GB cost.

In fact, studies have suggested that users of in-
teractive systems prefer on being as terse as pos-
sible and thus give rise to NSUs frequently. Cog-
nizant of this limitation, some systems explicitly
ask the users to avoid usage of pronouns and in-
complete sentences [Carbonell, 1983]. The cur-
rent state of the QA systems would not be able to
handle such NSUs and would result into inappro-
priate answers.

In this paper we propose a novel approach for
handling such NSUs arising when users are trying
to seek information using QA systems. Resolving
NSUs is the process of recovering a full clausal
meaningful question for an NSU utterance, by uti-
lizing the context of previous utterances.

The occurrence and resolution of NSUs in a
conversation have been studied in the literature
and is an active area of research. However, most of
the proposed approaches in the past have adopted a
rule or grammar based approach [Carbonell, 1983,
Fernández et al., 2005, Giuliani et al., 2014]. The
design of the rules or grammars in these works
were motivated by the frequent patterns observed
empirically which may not scale well for unseen
or domain specific scenarios.

Also, note that while the NSU resolution task
can be quite broad in scope and cover many as-
pects including ellipsis [Giuliani et al., 2014], we
limit the investigation in this paper to only the
Question aspect of NSU, i.e. resolving C:2 and
C:3 in the example above. More specifically, we
would not be trying to resolve the system (S:2, S:3,
S:4) and other non-question utterances (e.g. OK,
Ohh! I see). This focus and choice is primarily
driven by our motivation of facilitating a QA sys-
tem.

We propose a statistical approach to NSU reso-
lution which is not restricted by limited number of
patterns. Our approach is motivated by the obser-
vation that humans try to identify the keywords ap-
pearing in the NSU and place them in the context
to construct a complete sentential form. For con-
structing a meaningful and relevant sentence from
keywords, we adapt the techniques proposed for
generating questions from keywords, also known
as keyword-to-question (K2Q).

The K2Q [Zhao et al., 2011, Zheng et al., 2011,
Liu et al., 2012] is a recently investigated prob-

lem with the motivation to convert succinct web
queries to natural language (NL) questions to di-
rect users to cQA (community QA) websites. As
an example, the query ticket Broadway New York
could be converted to a NL question Where do I
buy tickets for the Broadway show in New York ?.
We leverage the core idea for the question genera-
tion module from these approaches.

The main contributions of this paper are as fol-
lows:

1. We propose a statistical approach for NSU
resolution which is not limited by a set of pre-
defined patterns. To the best of our knowl-
edge, statistical approaches have not been in-
vestigated for the purpose of NSU resolution.

2. We also propose a formulation that uses syn-
tactic, semantic and lexical evidences to iden-
tify the most likely clausal meaningful ques-
tion from a given NSU.

In Section 2 we present the related work. We
describe the a simple rule based approach in sec-
tion 3. In section 4 we present the details of the
proposed NSU resolution system. In Section 5,
we report experimental results on dataset collected
through mTurk and finally conclude our work and
discuss future work in section 6.

2 Related Work

A taxonomy of different types of NSUs used in
conversations was proposed by [Fernández et al.,
2005]. According to their taxonomy the replies
from the sales staff (S:2, S:3 and S:4) are NSUs
of type Short Answers. However, the utterances
C:2 and C:3 which are the focus of this paper and
referred to as Question NSU, are not a good fit
in any of the proposed types. One possible rea-
son why the authors in [Fernández et al., 2005]
did not consider them, may be because of the type
of dialog transcripts used in the study. The tax-
onomy was constructed by performing a corpus
study on the dialogue transcripts of the British Na-
tional Corpus (BNC) [Burnard, 2000]. Most of the
used transcripts were from meetings, seminars and
interviews.

Some authors have also referred to this phe-
nomenon as Ellipsis because of the elliptical form
of the NSU [Carbonell, 1983, Fernández et al.,
2004, Dalrymple et al., 1991, Nielsen, 2004, Giu-
liani et al., 2014]. While the statistical approaches

336



have been investigated for the purpose of ellipsis
detection [Fernández et al., 2004, Nielsen, 2004,
Giuliani et al., 2014], it has been a common prac-
tice to use rules – syntactic or semantic – for the
purpose of Ellipsis resolution [Carbonell, 1983,
Dalrymple et al., 1991, Giuliani et al., 2014].

A special class of ellipsis, verb phrase ellipsis
(VPE) was investigated in [Nielsen, 2004] in a do-
main independent manner. The authors have taken
the approach of first finding the modal verb which
can be then used as a substitute for the verb phrase.
For example, in the utterance “Bill loves his wife.
John does too”, the modal verb does can be re-
placed by the verb phrase loves his wife to result
in the resolved utterance “John loves his wife too”.
Authors used a number of syntactical features such
as part-of-speech (POS) tags and auxiliary verbs,
derived from the automatic parsed text to detect
the ellipsis.

Another important class of NSUs referred to as
Sluice was investigated in [Fernández et al., 2004].
Sluices are those situations where a follow-up bare
wh-phrase exhibits a sentential meaning. For ex-
ample:

Sue You were getting a real panic then.

Angela When?

Authors in [Fernández et al., 2004] extract a set
of heuristic principles from a corpus-based sample
and formulate them as probabilistic Horn clauses.
The predicates of such clauses are used to create
a set of domain independent features to annotate
an input dataset, and run machine learning algo-
rithms. Authors achieved a success rate of 90% in
identifying sluices.

Most of the previous work, as discussed here,
have used statistical approaches for detection of
ellipsis. However, the task of resolving these in-
complete utterances – NSU resolution – has been
largely based on rules. For example, a semantic
space was defined based on “CaseFrames” in [Car-
bonell, 1983]. The notion of these frames is sim-
ilar to a SQL query where conditions or rules can
be defined for different attributes and their values.
In contrast to this, we present a statistical approach
for NSU resolution in this paper with the motiva-
tion of scaling the coverage of the overall solu-
tion.

3 Rule Based Approach

As a baseline, we built a rule based approach sim-
ilar to the one proposed in [Carbonell, 1983]. The

rules capture frequent discourse patterns in which
NSUs are used by users of a question answering
system.

As a first step, let us consider the following con-
versation involving an NSU:

• Utt1: Who is the president of USA?

• Ans1: Barack Obama

• Utt2: and India?

We use the following two rules for NSU
resolution.

Rule 1: if ∃s|s ∈ phrase(Utt1) ∧ s.type =
PUtt2.type then create an utterance by substituting
s with PUtt2 in the utterance Utt1.

Rule 2: if whUtt2 is the only wh−word in Utt2
and whUtt2 6= whUtt1 then create an utterance by
substituting whUtt1 by whUtt2 in Utt1.

Here phrase(Utt1) denotes the set of all the
phrases in Utt1 and PUtt2 denotes the key phrase
that occurs in utterance Utt2. s.type denotes the
named entity type associated with the phrase s
whS1 and whS2 denote the wh word used in the
Utt1 and Utt2 respectively.

This rule based approach suffers from two main
problems. One, it is only as good as the named
entity recognizer (NER). For example, if antonym
? occurs in context of What is the synonym of neb-
ulous ?, it is not likely for the NER to detect syn-
onym and antonym are of the same type. Two, the
approach has a very limited scope. For example, if
with 64 GB ? occurs in context of What is the cost
of iPhone 6?, the approach will fail as the resolu-
tion cannot be modeled with a simple substitution.

4 Proposed NSU Resolution Approach

In this section, we explain the proposed approach
used to resolve NSUs. In the context of the
running example above, the proposed approach
should result in a resolved utterance “Who is the
president of India?”. As mentioned above, intu-
itively the resolved utterance should contain all the
keywords from Utt2, and these keywords should
be placed in an appropriate structure created by
the context of Utt1. One possible approach to-
wards this would be to identify all the keywords
from Utt1 and Utt2 and then forming a meaning-
ful question using an appropriate subset of these
keywords. Accordingly, the proposed approach

337



consists of the following three steps as shown in
Figure 1.

• Candidate Keyword Set Generation
• Keyword to Question Generation (K2Q)
• Learning to Rank Generated Questions

These three steps are explained in the following
subsections.

4.1 Candidate Keyword Set Generation
Given Utt1, Ans1 and Utt2 as outlined in the previ-
ous section, the first step is to remove all the non-
essential words (stop words) from these and gen-
erate different combinations of the essential words
(keywords).

Let U2 = {U2i, i ∈ 1 . . . N} be the set of
keywords in Utt2 and U1 = {U1i, i ∈ 1 . . . M}
be the set of keywords in Utt1. For the exam-
ple above, U2 would be {India} and U1 would
be {president, USA}. Let ΦU1,U2 represent the
power set resulting from the union of U1 and U2.
Now, we use the following constraints to further
rule out some invalid combinations:

• Filter out all the sets that do not contain all
the keywords in U2.

• Filter out all the sets that do not contain at
least one keyword from U1.

The basis for these constraints is coming from the
observation that the NSU resolution is about inter-
preting the current utterance in the context of the
conversation so far. Therefore it should contain
all the keywords from the current utterance and at
least one keyword from the context.

The valid keyword sets that satisfy these con-
straint are now used to form a meaningful question
as explained in the following section.

4.2 Keyword to Question Generation
Keyword-to-question (K2Q) generation is the pro-
cess of generating a meaningful and relevant ques-
tion from a given set of keywords. For each key-
word set K ∈ ΦU1,U2 resulting from the previ-
ous step, we use the following template based ap-
proach to generate a set of candidate questions.

4.2.1 Template Based Approach for K2Q
In this section, we summarize the template based
approach proposed by [Zhao et al., 2011] that was
adopted for this work. It consists of the following
three steps:

• Template Generation: This step takes as in-
put a corpus of reference questions. This
corpus should contain a large number of ex-
ample meaningful questions, relevant for the
task or domain at hand. The keyword terms
(all non-stop words) in each question are re-
placed by variable slots to induce templates.
For example, questions “what is the price of
laptop?” and “what is the capital of India”
would induce a template “what is the T1 of
T2?”. In the following discussion, we would
denote these associated questions as Qref .
Subsequently, the rare templates that occur
less than a pre-defined threshold are filtered
out.

This step is performed once in an offline man-
ner. The result of this step is a database of
templates associated with a set of questions
{Qref} that induced them.
• Template Selection: Given a set of keywords

K, this step selects templates that meet the
following criteria:

– The template has the same number of
slots as the number of query keywords.

– At least one question Qref associated
with the template has one user keyword
in exact same position.

For example, given a query “price phone”,
the template “what is the T1 of T2” would
be selected, if there is a question “what is the
price of laptop” associated with this template
that has price keyword at the first position.

• Question Generation: For each of the tem-
plates selected in the previous step, a ques-
tion Q is hypothesized by substituting the slot
variables by the keywords in K. For exam-
ple, if the keywords are president, India and
the template is “who is the T1 of T2”, then
the resulting question would be “ who is the
president of India”.

4.3 Learning to Rank Generated Questions
The previous step of question generation results
in a set of questions {Q} given a set of keywords
{K}. To rank these questions, we transform each
question’s candidate into a feature vector. These
features capture various semantic and syntactic as-
pects of the candidate question as well as the con-
text. In this section we explain the different fea-

338



Query
Generator

Query
Generator

Previous
Utterance

Keywords
Combinations

Current
Utterance

What is the language
of Jamica? Of Mexico?

language mexico
mexico language
Jamica mexico
Jamica mexcio language 
           . . .

K2QK2Q Questions

Feature Extractor
(BLEU, LM, Similarity 

Scores, ...)

Feature Extractor
(BLEU, LM, Similarity 

Scores, ...)
ReRanker
(SVMRank)

ReRanker
(SVMRank)

ReRanked
Questions

What language does Mexico?
What does Mexico do on language?
What is the language of Mexico?
What language is Mexico?
          . . . 

What is the language of 
Mexico?
What is the language in Mexcio? 
           . . . 

Figure 1: Architecture of NSU Resolution System

tures and ranking algorithm used to rank the gen-
erated questions.

• Semantic Similarity Score: A semantic
similarity score is computed between the key-
word set K and each example question Qref
associated with the template from which Q
was generated. The computation is based on
the semantic similarity of the keywords in-
volved in Q and Qref .

Sim(Q, Qref ) = ΠNi Sim(Ki, Qref,i)
1
N

(1)
where the similarity between the keywords
involved Sim(K., Qref,.) is computed as the
cosine similarity of their word2vec represen-
tations [Mikolov et al., 2013].

• Language Model Score: To evaluate the
syntactic correctness of the generated candi-
date question Q, we compute the language
model score LM(Q). A statistical language
model assigns a probability to a sequence of
n words (n-gram) by means of a probability
distribution. The LM score represents how
well a given sequence of n words is likely to
be generated by this probability distribution.
The distribution for the work presented in
this paper is learned from the question corpus
used in the template generation step above.

• BLEU Score: Intuitively, the intended sen-
tential form of the resolved NSU should be
similar to the preceding sentential form (Utt1
in the example above). A similar require-
ment arises in evaluation of machine trans-
lation (MT) systems and BLEU score is the

most commonly used metric for MT evalua-
tion [Papineni et al., 2002]. We compute it
as the amount of n-gram overlap between the
generated question Q and the preceding ut-
terance Utt1.

• Rule Based Score: Intuitively, the candidate
question from K2Q should be similar to the
resolved question generated by the rule based
system (iff rules apply). As discussed in Sec-
tion 3, we assign 1 to this feature when a rule
fires, otherwise assign 0.

We use a learning to rank model for scoring
each question Q ∈ {Q}, in the candidate pool for
a given keyword set K: w.Ψ(Q), where w is a
model weight vector and Ψ(Q) is the feature vec-
tor of question Q. The weights are trained using
SV M rank [Joachims, 2006] algorithm. To train
it, for a given K, we assign higher rank to the cor-
rect candidate questions and all other candidates
are ranked below.

5 Experiments

In this section, we present the datasets, evalua-
tion approaches and results. We also present the
comparative analysis of the performance obtained
when we employ a rule-based baseline approach
(Section 3) for this task.

5.1 Data
We organize the discussion around the data used
for our evaluation in two parts. In the first part, we
explain the dataset used for the purpose of setting
up the template based K2Q approach described in
Section 4.2. In the second part, we explain the
dataset used for evaluating the performance of the
NSU resolution.

339



Question Answer Q2e Q2r
What does the golden marmoset eat? flowers and tiger? What do tigers eat?

What is the average life span of Indian men? 65 And women Average life span of women in India, is?
Who is the highest paid athlete today? Tiger Woods And in the 1990? Who was the highest paid athlete in 1990?

Does a solid or liquid absorb more heat? Liquid What about gas or liquid? Does a gas or a liquid absorb more heat?

Table 1: Examples of collected data entries from Amazon Mechanical Turk

5.1.1 Dataset for the K2Q Step

In section 4.2 we noted that the template genera-
tion step involves a large corpus of reference ques-
tions. One such large collection of open-domain
questions is provided by the WikiAnswers∗

dataset.
The WikiAnswers corpus contains clusters of

questions tagged by WikiAnswers users as para-
phrases. Each cluster optionally contains an an-
swer provided by WikiAnswers users. Since the
scope of this work was limited to forming tem-
plates for the K2Q system, we use only the ques-
tions from this corpus. The corpus is split into 40
gzip-compressed files. The total compressed file
size is 8GB. We use only the first two parts (out of
40) for the purpose of our experiments. After re-
placing the keywords by slot variables as required
for template induction, this results into a total of
≈ 8M unique question-keyword-template tuples.
Further, we filter out those templates which have
less than five associated reference questions and
this results into a total of ≈ 74K templates and
corresponding≈ 3.7M associated reference ques-
tions.

5.1.2 Dataset for NSU Resolution

In this section, we describe the data that we use
for evaluating the performance of the proposed
method for NSU resolution.

We used a subset of the data that was collected
using Amazon Mechanical Turk. For collecting
this data a question answer pair (Q,A) was pre-
sented to an mTurk worker and who was then
asked to conceive another question Q2 related to
the pair (Q, A). The Q2 was to be given in two
different versions, an elliptical version Q2e and a
fully resolved version Q2r. The original data con-
tains 7400 such entries and contains examples for
NSUs as well as anaphora in Q2. We selected a
subset of 500 entries from this dataset for our eval-
uation. Table 1 presents some examples entries
from this data.

∗Available at http://knowitall.cs.
washington.edu/oqa/data/wikianswers/

5.2 Evaluations

We present our evaluations based on the following
three different configurations to investigate the im-
portance of various scoring and ranking modules.
The configurations used are,

1. Rule Based: This configuration is used as
a baseline system, as described in section 3.
As rule based methodologies are dominant in
the field of NSU resolutions, we compare to
clearly illustrate the limitations of just using
rules.

2. Semantic Similarity: We investigate how
well the semantic similarity score as de-
scribed in Section 4.3 works when we sort the
candidate questions generated based on this
feature alone.

3. SVM Rank: In this configuration, we use all
the scores as described in Section 4.3 in an
SVM Rank formulation.

5.2.1 Evaluation Methodology
Given the input conversation {Utt1, Ans1, Utt2},
system generated resolved utterance Q (corre-
sponding to NSU Utt2) and the intended utterance
Qr, the goal of the evaluation metric is to judge
how similar Q is to Qr. We use BLEU score and
human judgments for the purpose of this evalua-
tion.

BLEU score is often used for evaluation of ma-
chine translation systems to judge the goodness of
the translated text with the reference text. Please
note that we also used the BLEU score as one of
the features as mentioned in Section 4.3. There,
it was computed between the generated question
Q and the preceding utterance Utt1. Whereas,
for evaluation purposes, this score is computed be-
tween the generated question Q and the intended
question provided by the ground truth Qr.

To account for the paraphrasing errors, as the
same utterance can be said in several different
ways, we also use human judgment for the eval-
uation.

340



Rule Based Sem. Similarity SVM Rank
0.0

0.1

0.2

0.3

0.4

0.5

A
v
g
. 
B
le
u
 S
co
re

Figure 2: Average BLEU score for different configurations

Method Recall@1
Rule Based 0.17
SVM Rank 0.21

Table 2: Comparing Recall@1 using Human Judgments

0 1 2 3 4 5 6
N Best List

0.0

0.2

0.4

0.6

R
e
ca

ll

Figure 3: Recall@N obtained using human judgments

We use Recall@N to present the evalua-
tion results when human judgments are used.
Our test set comprises only of those utterances
({Utt2}) which require a resolution and there-
fore Recall@N captures how many of these NSUs
were correctly resolved if candidates only up to
top N are to be considered.

5.2.2 BLEU Score Evaluation
We compute the BLEU score between the can-
didate resolution Q and the ground truth utter-
ance Qr and compare it across the three config-
urations. Figure 2 shows the comparison of the
average BLEU score at position 1. A low score
for the rule based approach is expected as it re-
solves only those cases in which rules fire. The
semantic similarity configuration gains over the
rule based approach as it is able to utilize the tem-
plate database generated using the WikiAnswers
corpus. Finally, the SVM Rank uses various other

scores (LM, BLEU score) on top of rule-based and
semantic similarity score and therefore achieves
higher BLEU Score.

5.2.3 Human Judgments Evaluation
Finally, to account for the paraphrasing artifacts
manifested in human language, we use human
judgments to make a true comparison between the
rule based approach and the SVM Rank configu-
ration.

For human judgments, we presented just the re-
solved Q and the ground truth Qr. For all the 200
data points in the test set, top 5 candidates were
presented to human annotators who were asked to
decide if it was a correct resolution or not. We
choose just the top 5 just to analyze the quality
of the candidates generated at various positions by
the system.

Table 2 shows the Recall@1 for the the two
configurations. A better recall for the proposed

341



SVM configuration signifies the better coverage of
the proposed approach beyond a pre-defined set of
rules. The Recall@1 was used for this comparison
since the rule-based approach can only yield a sin-
gle candidate. To further see the behavior of the
proposed approach as more candidates are consid-
ered, Recall@N is presented in Figure 3. The fig-
ure shows that a recall of 42.5% can be achieved
when results up to top 5 are considered. The ob-
jective of this experiment is to study the quality of
top (1-5) ranked generated questions. This exper-
iment helps us conclude that improving the rank-
ing module has the potential to improve the overall
performance of the system.

5.3 Discussion

We discuss two types of scenarios where our SVM
rank based approach works better than the baseline
rule based approach. One of the rules to generate
resolved utterance is to replace a phrase in Utt1
with a phrase of the same semantic type in Utt2.
Such an approach is limited by the availability of
an exhaustive list of semantic types which is in
general difficult to capture. In the following exam-
ple, the phrases antidote and symptoms belong to
the entity type disease attribute. However it may
not be obvious to include disease attribute as a se-
mantic type unless the context is specified. Our
approach aims at capturing such semantic types
automatically using the semantic similarity score.

Utt1 What is the antidote of streptokinase?

Utt2 What are the symptoms?

Resolved what are the symptoms of streptokinase

The baseline approach fails to handle cases
where the resolved utterance cannot be generated
by merely replacing a phrase in Utt1 with a phrase
in Utt2. While our approach can handle cases
which requires sentence transformations such as
the one shown below.

Utt1 Is cat scratch disease a viral or bacterial disease?

Utt2 What’s the difference?

Resolved what’s the difference between a viral and bacterial
disease

One of the scenarios where our approach fails
is when there are no keywords in Utt2. This is be-
cause the K2Q module tries to generate questions
without any keywords (information) from Utt2. A
few examples are given below.

Utt1 (a) Kansas sport teams?

Utt2 (a) What others?

Utt1 (b) Cell that forms in fertilization?

Utt2 (b) And ones that don’t are called what?

6 Conclusion and Future Work

In this paper we presented a statistical ap-
proach for resolving questions appearing as non-
sentential utterances (NSU) in an interactive ques-
tion answering session. We adapted a keyword-
to-question approach to generate a set of candi-
date questions and used various scoring methods
to generate scores for the generated questions. We
then used a learning to rank framework to select
the best generated question. Our results show that
the proposed approach has significantly better per-
formance than a rule based method. The results
also show that for many of the cases where the cor-
rect resolved question does not appear at the top,
a correct candidate exists in the top 5 candidates.
Thus it is possible that by employing more fea-
tures and better ranking methods we can get fur-
ther performance boost. We plan to explore this
further and extend this method to cover other types
of NSUs in our future work.

Acknowledgments

We thank Martin Schmid, IBM Watson Prague and
Adam J Sporka, Pavel Slavik, Czech Techincal
University Prague for providing us with the cor-
pus of dialog ellipsis (dataset for NSU resolution)
without which training and evaluation of our sys-
tem would not have been possible.

References

Lou Burnard. Reference guide for the british na-
tional corpus. Oxford University Computing
Services, 2000.

Jaime G. Carbonell. Discourse pragmatics and
ellipsis resolution in task-oriented natural lan-
guage interfaces. In Proceedings of the 21st An-
nual Meeting on Association for Computational
Linguistics, pages 164–168, 1983.

Mary Dalrymple, Stuart M. Shieber, and Fernando
C. N. Pereira. Ellipsis and higher-order unifica-
tion. Linguistics and Philosophy, 14:399–452,
1991.

342



Fernández, Raquel, Jonathan Ginzburg, and
Shalom Lappin. Classifying ellipsis in dia-
logue: A machine learning approach. In Pro-
ceedings of the 20th International Conference
on Computational Linguistics, 2004.

Raquel Fernández. Non-sentential utterances in
dialogue: classification, resolution and use.
PhD thesis, University of London, 2006.

Raquel Fernández, Jonathan Ginzburg, and
Shalom Lappin. Using machine learning for
non-sentential utterance classification. pages
77–86, 2005.

David A Ferrucci. Introduction to this is watson.
IBM Journal of Research and Development, 56
(3.4), 2012.

Manuel Giuliani, Thomas Marschall, and Amy Is-
ard. Using ellipsis detection and word similar-
ity for transformation of spoken language into
grammatically valid sentences. In Proceedings
of the 15th Annual Meeting of the Special In-
terest Group on Discourse and Dialogue, pages
243–250, 2014.

Thorsten Joachims. Training linear svms in linear
time. In Proceedings of the 12th ACM SIGKDD
International Conference on Knowledge Dis-
covery and Data Mining, pages 217–226, 2006.

Qiaoling Liu, Eugene Agichtein, Gideon Dror,
Yoelle Maarek, and Idan Szpektor. When web
search fails, searchers become askers: Under-
standing the transition. In Proceedings of the
35th International ACM SIGIR Conference on
Research and Development in Information Re-
trieval, pages 801–810, 2012.

Tomas Mikolov, Kai Chen, Greg Corrado, and Jef-
frey Dean. Efficient estimation of word rep-
resentations in vector space. In Proceedings
of Workshop at International Conference on
Learning Representations, 2013.

Leif Arda Nielsen. Robust vpe detection using au-
tomatically parsed text. In Proceedings of the
ACL Workshop on Student Research, 2004.

Kishore Papineni, Salim Roukos, Todd Ward, and
Wei-Jing Zhu. Bleu: a method for automatic
evaluation of machine translation. In Proceed-
ings of the 40th Annual Meeting on Associa-
tion for Computational Linguistics, pages 311–
318. Association for Computational Linguis-
tics, 2002.

Shiqi Zhao, Haifeng Wang, Chao Li, Ting Liu,
and Yi Guan. Automatically generating ques-
tions from queries for communitybased ques-
tion answering. In Proceedings of 5th Inter-
national Joint Conference on Natural Language
Processing, pages 929–937, 2011.

Zhicheng Zheng, Xiance Si, Edward Y. Chang,
and Xiaoyan Zhu. K2q: Generating natural lan-
guage questions from keywords with user re-
finements. In Proceedings of the 5th Interna-
tional Joint Conference on Natural Language
Processing, pages 947–955, 2011.

343


