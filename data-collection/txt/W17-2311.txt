



















































Creation and evaluation of a dictionary-based tagger for virus species and proteins


Proceedings of the BioNLP 2017 workshop, pages 91–98,
Vancouver, Canada, August 4, 2017. c©2017 Association for Computational Linguistics

Creation and evaluation of a dictionary-based tagger
for virus species and proteins

Helen Victoria Cook
Rūdolfs Bērziņš

Cristina Leal Rodrı́guez
Novo Nordisk Foundation

Center for Protein Research,

Faculty of Health and Medical Sciences,

University of Copenhagen, Denmark

helen.cook@cpr.ku.dk

Juan Miguel Cejuela
TUM, Department of Informatics,

Bioinformatics & Computational Biology,

i12, Boltzmannstr. 3, 85748

Garching/Munich, Germany

Lars Juhl Jensen
Novo Nordisk Foundation

Center for Protein Research,

Faculty of Health and Medical Sciences,

University of Copenhagen, Denmark

lars.juhl.jensen@cpr.ku.dk

Abstract

Text mining automatically extracts infor-
mation from the literature with the goal
of making it available for further analy-
sis, for example by incorporating it into
biomedical databases. A key first step
towards this goal is to identify and nor-
malize the named entities, such as pro-
teins and species, which are mentioned in
text. Despite the large detrimental im-
pact that viruses have on human and agri-
cultural health, very little previous text-
mining work has focused on identifying
virus species and proteins in the literature.
Here, we present an improved dictionary-
based system for viral species and the
first dictionary for viral proteins, which
we benchmark on a new corpus of 300
manually annotated abstracts. We achieve
81.0% precision and 72.7% recall at the
task of recognizing and normalizing viral
species and 76.2% precision and 34.9% re-
call on viral proteins. These results are
achieved despite the many challenges in-
volved with the names of viral species and,
especially, proteins. This work provides a
foundation that can be used to extract more
complicated relations about viruses from
the literature.

1 Introduction

Viruses are major human and agricultural
pathogens. Influenza A in the US alone costs
billions of dollars each year in lost wages and
medical expenses (Molinari et al., 2007). World-
wide, Influenza, Human papilloma virus and
Hepatitis C virus are each responsible for at least
a quarter of a million deaths each year (WHO,
2014). At the same time, viruses such as Zika

virus are emerging as global health threats as
the habitats of their vectors are expanding due
to climate change (Mills et al., 2010; Fauci and
Morens, 2016). Such arboviruses are previously
neglected diseases, and as such vaccines and
antiviral drugs are not available for them, posing
a large health risk.

The impact of outbreaks in livestock can also be
immense. The 2001 Foot and mouth disease virus
outbreak in the UK cost an estimated 8 billion
(Knight-Jones and Rushton, 2013) and still today
much remains unknown about the virus, includ-
ing the mechanisms for persistent infection (Paul
et al., 2010), and the virus’ interactions with the
immune system that may aid cross serotype vac-
cine production (Paton and Taylor, 2011).

The study of viruses is aided by bioinformat-
ics resources such as protein–protein interaction
databases. Having a comprehensive picture of a
virus protein’s interaction partners is crucial to the
understanding of the viral lifecycle and aids in the
search for vaccines and antiviral drugs (Shah et al.,
2015). However, manually creating and maintain-
ing such resources is a cost, time and labour in-
tensive endeavour (Attwood et al., 2015). Text
mining provides a means to automatically iden-
tify relevant publications and the entities of inter-
est that are mentioned in them quickly and at low
cost. A first step towards building these resources
for viruses is the identification of viral species and
their proteins in text.

1.1 Background

Text mining for viruses presents several chal-
lenges over text mining for species of cellular or-
ganisms. Viruses are often known by many differ-
ent names, either because a virus was identified
in different countries and given different names
(e.g. Bovine pestivirus and Bovine viral diarrhoea
virus), or because the taxonomy has changed (e.g.

91



polyomaviruses). Another source of synonyms is
the use of the disease that the virus causes in place
of the virus name.

Viral proteins are even more challenging to text
mine, as they are often referred to by one-letter
names such as E or M. Further, even if their names
are longer, they can be written in many different
orthographic variants e.g. U(S)11, Us11, or US11.
Viral proteins may also have many synonyms re-
lated to the gene name, position on a segment,
or may be referred to by their function e.g. “the
polymerase”. Some RNA viruses have polypro-
teins which complicates their analysis. Their vi-
ral mRNA codes for a single open reading frame
that is translated to a polypeptide product, which
is then post translationally cleaved into functional
protein products. Bioinformatics databases such
as UniProt (The UniProt Consortium, 2014) of-
ten give first class identifiers to the polyprotein but
not to the cleavage products, thus complicating the
process of referring to the functional protein prod-
uct.

These challenges can be mitigated by using a
dictionary approach to text mining. In such an ap-
proach, comprehensive dictionaries are created to
contain all the alternative names that are likely to
be referred to in a corpus. In this work, we have
chosen to use a dictionary based method based on
the success of this approach to identify bacteria
species and biotopes (Cook et al., 2016). We have
chosen to use curated databases (NCBI taxonomy
and UniProt) to populate the dictionary instead of
other approaches such as unsupervised methods to
learn which items are named entities (Neelakan-
tan and Collins, 2014), as the data available in
these databases is high quality and openly avail-
able. Furthermore, starting with a resource dra-
matically reduces the difficulty of normalization
of recognized entities.

Previous work in this field includes LINNAEUS
(Gerner et al., 2010), a dictionary-based system
that is also designed to recognize species in ab-
stracts. The SPECIES tagger (Pafilis et al., 2013)
is a newer and faster dictionary system that aims
to identify names of any species in biomedical ab-
stracts. SPECIES has achieved good performance
when tagging names of viruses species in abstracts
from virology journals. A more recent and spe-
cialized effort used the dictionary and template-
based ANDSystem to text mine the HCV interac-
tome (Saik et al., 2015).

Here, we improve on the SPECIES dictionary
for all virus species, and additionally tag names
of virus proteins for those proteins that have ref-
erence proteomes in UniProt. We have created
viral species and protein dictionaries, and a gold-
standard corpus that has been annotated by 4 hu-
man annotators.

2 Availability

The version of the dictionaries used
in this publication are available at
http://figshare.com/articles/
virus_entities_tsv/4721287 and
the most recent version will be available at
http://download.jensenlab.org/.
The V300 corpus and annotator guide-
lines is publicly available at http:
//www.tagtog.net/-corpora. The eval-
uation code is available at http://github.
com/bitmask/textmining-stats.
The tagger software used for this work is
available at http://bitbucket.org/
larsjuhljensen/tagger.

3 Methods

3.1 Dictionary creation and tagging
Virus names were taken from NCBI Taxonomy
(Sayers et al., 2009) and included all synonyms at
all taxonomic levels under the viruses superking-
dom. Disease names were taken from the Disease
Ontology (Kibbe et al., 2015) and were manually
mapped onto the correct virus taxid, giving an ad-
ditional 387 names for 102 species that are hu-
man pathogens. This resulted in a total of 173,367
names for 150,885 virus tax levels.

Virus species name acronyms were taken from
the ninth ICTV report on virus taxonomy (King
et al., 2012) by text mining the document and ex-
tracting any text in parentheses that appears to be
an acronym and that follows a match for a virus
name. This way we found 778 acronyms, more
than 500 of which were not found in the previous
sources, for 662 virus species.

Virus protein names were taken from UniProt
reference proteomes (The UniProt Consortium,
2014) as of Aug 31, 2015. Viruses that did not
have complete proteomes were not included in the
protein dictionary, although they are included in
the species dictionary. Protein names and syn-
onyms were taken from all fields in the UniProt
record, including the protein name, short name,

92



gene, and chain entries if the protein is a polypro-
tein. Additionally, many variants of the protein
names were generated following a set of rules to
cover orthographic variation, such as “X protein”
is expanded to “protein X” and “X”. For a com-
plete list of rules, refer to the code. This resulted
in 16,580 proteins with 112,013 names from 397
virus species.

Stopwords were adapted from the text mining
done for the text-mining channel of the STRING
database (Szklarczyk et al., 2015). Additional
stopwords were found by running the dictionary
over all documents in PubMed and inspecting the
100 most frequent matches to determine if they
should be stopworded. Although normally consid-
ered to be stopwords by the tagging system, spe-
cific one and two letter names from the dictionary
were permitted to be matches to enable finding
very short protein names.

Automated tagging used the dictionaries de-
scribed above and the tagger text-mining system
developed for the SPECIES resource (Pafilis et al.,
2013).

3.2 Corpus creation and gold standard
creation

300 abstracts were selected randomly by filtering
abstracts mentioned in reviewed UniProt entries
for viral proteins for top virology journals as deter-
mined by impact factor. Documents were divided
among four annotators such that each pair of an-
notators shared 10 documents, implying that 20%
of the documents were annotated by two annota-
tors. These overlapping documents were used to
calculate inter-annotator agreement (IAA), and the
annotators were blind to which documents were in
this set throughout the project.

Annotation guidelines were agreed upon fol-
lowing the annotation of 10 documents in a pilot
set, which were not used in the evaluation of IAA
or to assess the performance of the tagger. All ab-
stracts were manually annotated using tagtog (Ce-
juela et al., 2014), an online system for text min-
ing. Species names were normalized to NCBI tax-
onomic identifiers. Protein names were normal-
ized to UniProt entry names, unless they were the
cleavage product of a polyprotein, in which case
they were normalized to their chain name.

3.3 Evaluation
The IAA among the human annotators was deter-
mined separately for viral species and proteins by

determining the number of annotations that over-
lap and contain the same normalization. Bound-
aries of annotations were considered to match if
the annotations overlapped.

Species normalizations were considered to
match if one was a parent of the other and if both
were at or below species level, or if both were be-
low species level and had a common parent. For
example, both of the following pairs were con-
sidered matches: “Influenza A” and “Influenza A
H1N1”, and “Influenza A H1N1” and “Influenza
A H7N9”. This allowed for an annotation to not
be penalized if the strain was annotated instead of
the species, or if two different strains of the same
species were annotated. Protein normalizations
were considered to match if they were within 90%
identity according to BLAST (Zhang et al., 2000).

IAA was measured by F-score, however since
we allow boundaries to overlap, this measure may
not be symmetric. If one annotator has annotated
“long form (short form)” as one annotation, and
another annotator has annotated it as two annota-
tions, then this will count as one true positive when
comparing the first annotator to the second, but as
two true positives when comparing the second an-
notator to the first. To avoid this asymmetry, we
counted all the true positives, false negatives and
false positives across both annotators.

The guidelines specify that if a span refers to
multiple entities, then it should be normalized to
each of them. Each normalization was treated as
contributing separately to the number of true or
false positives. A special case was established
for Adenovirus, which is a large genus containing
very many species of viruses that have a highly
conserved set of proteins. Adenovirus proteins are
often referred to in general in the literature, with-
out specifying a specific species. Manual anno-
tation of Adenovirus proteins required that only
one representative protein from one species be
tagged, thus effectively treating this genus as a sin-
gle species.

The recall and precision of the tagger was cal-
culated against the consensus of the human an-
notations. The consensus was determined as fol-
lows. If only one annotator annotated the doc-
ument, their annotations were taken as the gold
standard. The annotations were similarly accepted
as the gold standard if two annotators agreed on
position and normalization. However, if there was
a disagreement, then a third annotator was asked to

93



resolve it. For positions that overlapped, the union
of the spans was used as the consensus.

The precision and recall were calculated in three
different ways. The first method required that
the boundaries and normalizations of the con-
sensus and tagger annotations match. The sec-
ond method, “boundaries only”, required only the
boundaries of the annotations to match. The last
method, “document level normalization”, com-
pared the lists of unique normalizations found in
the document, regardless of position and number
of occurrences.

4 Results and Discussion

4.1 Corpus and Inter-annotator agreement

The corpus consisted of 300 documents with 1,826
species and 2,540 protein annotations. There was
overall good agreement between annotators for
both species and proteins. The mean IAA F-score
for species was 87.3%, and considering bound-
aries only was 90.0%. For proteins, the mean IAA
F-score was 76.5%, which rose to 86.9% when
considering boundaries only. Detailed results are
shown in figure 1.

There was substantial agreement between anno-
tators regarding the location of species and protein
annotations, and there was also good agreement on
the normalization of species. However, there was
less agreement among protein normalizations than
those for species. 26% of these disagreements in-
volve one annotator normalizing a protein name to
a UniProt entry, and the second annotator report-
ing the normalization as unknown. An additional
20% of the disagreement is due to an annotator
normalizing a span to multiple entities and another
annotator normalizing it to fewer entities. Such
cases, in which an abstract discusses a protein
in one virus and compares it to a closely related
protein, can be ambiguous and refer to the pro-
tein without being completely clear about which
species is being referred to.

However, the largest part this disagreement
comes from instances in which annotators have
normalized to different proteins that are different
enough to not pass the 90% identity BLAST cri-
terion. Manual inspection of these proteins indi-
cate that the majority are correct, but that fast vi-
ral evolution has caused the protein sequences of
similar isolates to diverge. The set of documents
randomly chosen to calculate IAA was unlucky to
contain a few documents containing proteins that

0.82

0.86

0.86

0.93

0.85

0.9

0.820.77

0.73

0.8

0.62

0.85

0.95

0.92

0.94

0.9

0.83

0.85

0.920.9

0.94

0.85

0.84

0.78

proteinsspecies

Cristina Helen Juanmi Rudolfs Cristina Helen Juanmi Rudolfs

Cristina

Helen

Juanmi

Rudolfs

0.7

0.8

0.9

fscore

Figure 1: Inter-annotator agreement for viral
proteins and species. Above the diagonal both
normalization and boundaries are required to be
correct, below the diagonal only identification of
boundaries are required to be correct.

are quite divergent, but this is not representative of
the whole corpus. This can be seen by dropping
the BLAST identity criterion to 50%, which then
accounts for 29% of the difference between anno-
tators, but increases the tagger precision and recall
by only 1%.

4.2 Tagger performance for species
The automatic tagger achieved 81.5% precision
and 73.3% recall for the combined task of rec-
ognizing and normalizing viral species. When
requiring only the boundaries to be correct, i.e.
recognition but not normalization, the precision
and recall were 93.1% and 79.8% respectively. At
the document level, the normalization precision
was 74.9% and the recall was 85.4%. Results are
summarized in table 1. Combined, this shows that
if the tagger identifies a viral species, it is very
likely that a viral species is mentioned at the re-
ported position, and it is also likely that the tagger
has normalized it correctly. Also, the tagger cor-
rectly identifies most of the species that are men-
tioned in a document.

In 43% of the cases of incorrect species nor-
malization, the tagger has identified both the cor-
rect species normalization and additional normal-
izations with the same abbreviation. For example,
the tagger normalized SV40 to “Simian virus 40”,
which is correct, but also to “Polyomavirus sp.”
under unclassified Polyomaviridae because both
taxa have SV40 as an abbreviation in the NCBI
taxonomy. The abbreviation SV40 will thus count
as both a true positive and a false positive with re-
spect to normalization. If instead such partially
correct normalizations were counted only as true
positives, the precision would rise from 81.5% to
85.8%.

94



The tagger does not attempt to correctly identify
all referenced entities in sentence constructs such
as “HSV types 1 and 2” although such normal-
izations are obvious to human annotators. More
ambiguously, papers that discuss Influenza pro-
teins or Adenovirus proteins, without specifying
the species (such as Influenza A, or Adenovirus
type 1) are not clear about what exactly is being
referred to.

In an additional 32% of the cases of incorrect
species normalization, an annotator identified the
virus as unclassified in which case it and the taxa
identified by the tagger joined the taxonomic tree
above the species level, and so was not considered
to be a match by the matching code. If the match is
relaxed to genus level, then the precision will rise
from 81.5% to 85.0% and to 86.3% if accepting
also partially correct normalizations as described
above.

Despite efforts to be comprehensive, some ab-
breviations are missing from the virus dictionary,
for example the abbreviations Ad2 and Ad5 for
Adenovirus type 2 and 5 respectively were not
included in the dictionary. The tagger does con-
tain logic to identify and expand acronyms on the
fly, but has very strict matching criteria to prevent
false positives (Pafilis et al., 2013). Further, syn-
onyms that are not present in NCBI taxonomy will
not be identified. For example “Blackberry yel-
low vein disease” was not identified as as synonym
for “Blackberry yellow vein virus” and so was not
found by the tagger. This could be improved with
more comprehensive synonym generation.

The tagger will tag all instances of entries in its
dictionary, even in contexts that are not appropri-
ate. The annotation guidelines state that viruses
that are used as vectors should not be tagged, since
the scientific work they are mentioned in is not pri-
marily about the virus. However, this is a matter
of opinion and the opposite case could also be ar-
gued. Regardless, the tagger cannot distinguish
the context in which viruses are mentioned, and
will blindly tag all occurrences of the virus name.

4.3 Tagger performance for proteins

For combined recognition and normalization of vi-
ral proteins, the precision and recall of the tagger
were 76.2% and 34.9% respectively. Observing
boundaries only, the precision and recall rose to
87.4% and 40.0% respectively. At the document
level, the normalization precision was 76.2% and

Precision Recall
Normalisation 81.5% 73.3%

Boundaries only 93.1% 79.8%
Doc level normalisation 74.9% 85.4%
Partially correct norm 85.8% 73.3%
Match at genus level 85.0% 73.5%
Previous two criteria 86.3% 73.5%

Table 1: Summary of species precision and re-
call for different evaluation criteria: Normaliza-
tion and recognition, recognition of boundaries
only, normalization at the document level, treating
entities that have been normalized to multiple en-
tities as correct if one of the normalizations is cor-
rect, relaxing the matching criterion to the genus
level, and finally allowing both of the previous two
criteria.

the recall was 38.1%. Results are summarized in
table 2.

Since viral protein names are so short and not
unique to one species, the tagger will only tag pro-
tein names for species that have already been iden-
tified. This means that the theoretical upper bound
for tagging proteins is equivalent to the species
document level normalization recall (85.4%) as-
suming that all the proteins are present in the dic-
tionary. However, the dictionary only contains
protein names for species that are contained in re-
viewed UniProt proteomes, a total of 348 species
and 88.1% of the proteins mentioned in the corpus.
This gives a maximum possible recall of 75.2% for
proteins. Conversely, since the tagger detects pro-
teins only after the species has been detected, the
normalization of the viral proteins that are found
is quite accurate.

Considering only annotation of the proteins in
the dictionary, the precision was 86.0% and the
recall 35.5%. Recall does not change significantly
from considering all proteins because there are 10
times more false negatives due to not locating the
protein compared to false negatives due to incor-
rectly normalizing the protein. At the document
level, the normalization precision of proteins that
were present in the dictionary is 77.1% and the re-
call is 50.7%.

Viral proteins are very hard for the tagger to
identify due to the diversity of names that are used
to refer to them. For example, the tagger has
missed 97% of names in which the protein is re-
ferred to by its molecular weight (e.g. “the 33K

95



protein”). Including these synonyms would in-
crease the recall by 4 percentage points. Similarly,
the tagger has tagged only 10% of the cases in
which the viral protein is referred to by its func-
tion (e.g. “the helicase”). Including these syn-
onyms would increase the recall by 6 percentage
points. As observed for species, the tagger does
not recognize novel abbreviations, such as “sGP”
for the Ebola virus nonstructural small glycopro-
tein, and such constructs are used quite frequently
in the literature. Better on-the-fly acronym identi-
fication in the tagger may help increase this recall
rate.

Another source of error is the ambiguity of
terms used in the text to refer to parts of the virus
that are also names of proteins such as “capsid”.
Although the frequently-named capsid protein is
the main constituent of the viral capsid, references
in the text to “capsid” are often ambiguous as to
whether they refer to the protein or to the assem-
bled virus part. The annotation guidelines state
that such terms should only be tagged if they re-
fer to the protein and should not be tagged if they
refer to part of the virus, but these cases are often
difficult to distinguish in practice.

The tagger identifies false positives at a much
lower rate than false negatives. Since very short
protein names are present in the dictionary, it is
much more likely for these names to appear in
places that are not in the context of a protein. For
example, Coronavirus infectious bronchitis virus
has a spike protein abbreviated S, however discus-
sion of the polyprotein cleavage site before a ser-
ine residue will be false positively tagged as serine
is also abbreviated S.

Normalization of protein names to multiple en-
tities can also be incorrect in instances where an
abstract discusses both a specific protein in one
species, and the same protein in many species. The
tagger will tag all instances of the protein name
with all species and will not be able to distinguish
the instances that refer only to the protein in a spe-
cific species, whereas human annotators are more
easily able to distinguish these cases.

4.4 Results in other corpora

Compared to the S800 virus corpus (Pafilis et al.,
2013), the improved dictionary finds over 100
more mentions, including new abbreviations, but
does not tag more general terms such as “infec-
tious virus” and “avian viruses” which refer to

Precision Recall
Normalisation 77.5% 35.5%

Boundaries only 87.4% 40.0%
Doc level normalisation 77.1% 50.7%
Theoretical max recall - 75.2%

Table 2: Summary of results for protein detec-
tion for different evaluation criteria: normalization
and recognition, recognition of boundaries only
and document level normalization. The theoretical
maximum recall based on requiring the species to
be recognized and present in the dictionary is also
listed.

more than one species. Measured against the S800
gold standard for only virus annotations in the
virus subset of the corpus, the improved tagger
has a precision and recall of 63.3% and 57.0%
respectively, compared to the initial results from
SPECIES of 63.2% and 53.0% respectively.

Running the tagger over all of Medline finds
over 53 million mentions of 8063 viral species
in more than 1.5M articles. Of these, we have
protein level detail for 348 species, and find over
10M mentions of 4668 unique proteins. The most
commonly mentioned species is HIV-1, making up
over 3% of species mentions.

5 Conclusions and Perspectives

As the biomedical literature continues to grow at
an exponential rate (Lu, 2011), automated tools,
such as text mining, are necessary to enable ex-
tracting information from the literature in a timely
and efficient manner. Text mining is a means to
automatically extract information from the litera-
ture without requiring manual curation of a large
number of documents. It can be used success-
fully to extract virus species and proteins from ab-
stracts that pertain to viruses with good precision
and also, in the case of species, good recall. There
is still much room to improve the recall of pro-
teins due to the abundance of alternative names
that are used to refer to them. Further, the tag-
ger does not recognize disjoint entities, and since
there has recently been progress in this field (Tang
et al., 2013), this could also be an area for future
improvement of the tagger.

These results can be used in future work to ex-
tract co-occurrences of virus and host proteins,
which could imply an interaction between these
proteins. Integrating virus-host protein-protein in-

96



teractions into the larger host interaction network
may provide insight into viral mechanisms of dis-
ease. Work done specifically on EBV, HPV, and
Hepatitis C virus (Gulbahce et al., 2012; Mosca
et al., 2014) revealed that host proteins local to vi-
ral targets form network modules that are related
to the diseases caused by these viruses. With the
virus-agnostic tools presented here, such work can
be scaled up to easily enable investigation of all
viruses for which there is sufficient data.

The work presented here could also be used as
a foundation to identify viruses that are understud-
ied compared to their impact, and may reveal fu-
ture directions that are promising to study. The
interrelationship of proteins and diseases has been
explored recently using text mining to assess both
the strength of an interaction between a protein
and a disease, and also the scarceness of publica-
tions about a given protein target (Cannon et al.,
2017). This gives researchers an overview of un-
derstudied proteins that could be relevant for dis-
ease etiology. A similar approach could be taken
to reveal new directions in virus research.

Acknowledgements

The authors would like to thank Jorge Campos for
his work on the interface of tagtog to support this
project.

References
Teresa Attwood, Bora Agit, and Lynda Ellis. 2015.

Longevity of Biological Databases. EMBnet.journal
21(0).

Daniel C Cannon, Jeremy J Yang, Stephen L Math-
ias, Oleg Ursu, Subramani Mani, Anna Waller,
Stephan C Schurer, Lars Juhl Jensen, Larry A Sklar,
Cristian G Bologa, and Tudor I Oprea. 2017. TIN-
X: Target Importance and Novelty Explorer. Bioin-
formatics pages 1–3.

Juan Miguel Cejuela, Peter McQuilton, Laura Ponting,
S. J. Marygold, Raymund Stefancsik, Gillian H.
Millburn, and Burkhard Rost. 2014. Tagtog: Inter-
active and text-mining-assisted annotation of gene
mentions in PLOS full-text articles. Database
2014:1–8. https://doi.org/10.1093/database/bau033.

Helen Cook, Evangelos Pafilis, and Lars Jensen.
2016. A dictionary- and rule-based sys-
tem for identification of bacteria and habi-
tats in text. In Proceedings of the 4th
BioNLP Shared Task Workshop. pages 50–55.
http://www.aclweb.org/anthology/W/W16/W16-
30.pdf#page=60.

Anthony S Fauci and David M Morens.
2016. Zika Virus in the Americas Yet
Another Arbovirus Threat. The New Eng-
land journal of medicine 374:601–604.
https://doi.org/10.1056/NEJMp1600297.

Martin Gerner, Goran Nenadic, and Casey M Bergman.
2010. LINNAEUS: a species name identification
system for biomedical literature. BMC bioinformat-
ics 11(1):85. https://doi.org/10.1186/1471-2105-11-
85.

Natali Gulbahce, Han Yan, Amélie Dricot, Megha
Padi, Danielle Byrdsong, Rachel Franchi, Deok-
Sun Lee, Orit Rozenblatt-Rosen, Jessica C. Mar,
Michael A. Calderwood, Amy Baldwin, Bo Zhao,
Balaji Santhanam, Pascal Braun, Nicolas Simo-
nis, Kyung-Won Huh, Karin Hellner, Miranda
Grace, Alyce Chen, Renee Rubio, Jarrod A. Marto,
Nicholas A. Christakis, Elliott Kieff, Frederick P.
Roth, Jennifer Roecklein-Canfield, James A. De-
Caprio, Michael E. Cusick, John Quackenbush,
David E. Hill, Karl Münger, Marc Vidal, and
Albert-László Barabási. 2012. Viral Perturba-
tions of Host Networks Reflect Disease Etiol-
ogy. PLoS Computational Biology 8(6):e1002531.
https://doi.org/10.1371/journal.pcbi.1002531.

Warren A. Kibbe, Cesar Arze, Victor Felix, Elvira
Mitraka, Evan Bolton, Gang Fu, Christopher J.
Mungall, Janos X. Binder, James Malone, Drashtti
Vasant, Helen Parkinson, and Lynn M. Schriml.
2015. Disease Ontology 2015 update: An ex-
panded and updated database of Human diseases
for linking biomedical knowledge through disease
data. Nucleic Acids Research 43(D1):D1071–
D1078. https://doi.org/10.1093/nar/gku1011.

Andrew M.Q. King, Michael J. Adams, Eric B.
Carstens, and Elliot J. Lefkowitz, editors. 2012.
Virus Taxonomy: Classification and Nomenclature
of Viruses: Ninth Report of the International Com-
mittee on Taxonomy of Viruses. Elsevier Academic
Press. https://doi.org/10.1016/B978-0-12-384684-
6.X0001-8.

T. J D Knight-Jones and J. Rushton. 2013. The eco-
nomic impacts of foot and mouth disease - What are
they, how big are they and where do they occur?
Preventive Veterinary Medicine 112(3-4):162–173.
https://doi.org/10.1016/j.prevetmed.2013.07.013.

Zhiyong Lu. 2011. PubMed and beyond: A
survey of web tools for searching biomed-
ical literature. Database 2011:1–13.
https://doi.org/10.1093/database/baq036.

James N. Mills, Kenneth L. Gage, and Ali S.
Khan. 2010. Potential influence of climate
change on vector-borne and zoonotic diseases:
A review and proposed research plan. Envi-
ronmental Health Perspectives 118(11):1507–1514.
https://doi.org/10.1289/ehp.0901389.

97



Noelle Angelique M. Molinari, Ismael R. Ortega-
Sanchez, Mark L. Messonnier, William W. Thomp-
son, Pascale M. Wortley, Eric Weintraub, and Car-
olyn B. Bridges. 2007. The annual impact of
seasonal influenza in the US: Measuring disease
burden and costs. Vaccine 25(27):5086–5096.
https://doi.org/10.1016/j.vaccine.2007.03.046.

Ettore Mosca, Roberta Alfieri, and Luciano Milanesi.
2014. Diffusion of Information throughout the
Host Interactome Reveals Gene Expression Varia-
tions in Network Proximity to Target Proteins of
Hepatitis C Virus. PLoS ONE 9(12):e113660.
https://doi.org/10.1371/journal.pone.0113660.

Arvind Neelakantan and Michael Collins. 2014.
Learning Dictionaries for Named Entity Recogni-
tion using Minimal Supervision. Proceedings of the
14th Conference of the European Chapter of the As-
sociation for Computational Linguistics pages 452–
461. http://www.aclweb.org/anthology/E14-1048.

Evangelos Pafilis, Sune P. Frankild, Lucia Fanini,
Sarah Faulwetter, Christina Pavloudi, Aikaterini
Vasileiadou, Christos Arvanitidis, and Lars Juhl
Jensen. 2013. The SPECIES and ORGANISMS
Resources for Fast and Accurate Identification of
Taxonomic Names in Text. PLoS ONE 8(6):2–7.
https://doi.org/10.1371/journal.pone.0065390.

David J Paton and Geraldine Taylor. 2011. Devel-
oping vaccines against foot-and-mouth disease and
some other exotic viral diseases of livestock. Philo-
sophical transactions of the Royal Society of Lon-
don. Series B, Biological sciences 366(1579):2774–
81. https://doi.org/10.1098/rstb.2011.0107.

William E Paul, Michael Mcheyzer-williams,
and David Barthold, Stephen. Bowen, R.
Hedrick, Ronald. Knowles, Donald. Lair-
more, Michael. Parrish, Colin. Saif, Linda.
Swayne. 2010. Fenner’S Veterinary Vi-
rology. Elsevier 5th editio(August):43–51.
https://doi.org/10.1016/B978-0-12-375158-
4.X0001-6.

Olga V. Saik, Timofey V. Ivanisenko, Pavel S. De-
menkov, and Vladimir A. Ivanisenko. 2015. In-
teractome of the hepatitis C virus: Literature min-
ing with ANDSystem. Virus Research 218:40–48.
https://doi.org/10.1016/j.virusres.2015.12.003.

Eric W Sayers, Tanya Barrett, Dennis A Benson,
Stephen H Bryant, Kathi Canese, Vyacheslav
Chetvernin, Deanna M Church, Michael DiCuc-
cio, Ron Edgar, Scott Federhen, Michael Feolo,
Lewis Y Geer, Wolfgang Helmberg, Yuri Kapustin,
David Landsman, David J Lipman, Thomas L
Madden, Donna R Maglott, Vadim Miller, Ilene
Mizrachi, James Ostell, Kim D Pruitt, Gregory D
Schuler, Edwin Sequeira, Stephen T Sherry, Martin
Shumway, Karl Sirotkin, Alexandre Souvorov, Grig-
ory Starchenko, Tatiana A Tatusova, Lukas Wagner,
Eugene Yaschenko, and Jian Ye. 2009. Database
resources of the National Center for Biotechnology

Information. Nucleic Acids Research 37:D5–15.
https://doi.org/10.1093/nar/gkn741.

Priya S. Shah, Jason A. Wojcechowskyj, Manon Eck-
hardt, and Nevan J. Krogan. 2015. Comparative
mapping of host-pathogen protein-protein interac-
tions. Current Opinion in Microbiology 27:62–68.
https://doi.org/10.1016/j.mib.2015.07.008.

Damian Szklarczyk, Andrea Franceschini, Stefan
Wyder, Kristoffer Forslund, Davide Heller, Jaime
Huerta-Cepas, Milan Simonovic, Alexander Roth,
Alberto Santos, Kalliopi P. Tsafou, Michael Kuhn,
Peer Bork, Lars J. Jensen, and Christian Von
Mering. 2015. STRING v10: Protein-protein
interaction networks, integrated over the tree of
life. Nucleic Acids Research 43(D1):D447–D452.
https://doi.org/10.1093/nar/gku1003.

Buzhou Tang, Yonghui Wu, Min Jiang, Joshua C.
Denny, and Hua Xu. 2013. Recognizing and Encod-
ing Discorder Concepts in Clinical Text using Ma-
chine Learning and Vector Space. Proceedings of
the ShARe/CLEF Evaluation Lab http://www.clef-
initiative.eu/documents/71612/d596ae25-c4b3-
4a9a-be4a-648a77712aaf.

The UniProt Consortium. 2014. UniProt:
a hub for protein information. Nu-
cleic Acids Research 43(D1):D204–212.
https://doi.org/10.1093/nar/gku989.

WHO. 2014. WHO Fact Sheets: Influenza, HCV,
HPV. http://www.who.int/mediacentre/factsheets/.

Zheng Zhang, Scott Schwartz, Lukas Wagner,
and Webb Miller. 2000. A Greedy Algo-
rithm for Aligning DNA Sequences. Jour-
nal of Computational Biology 7(12):203–214.
https://doi.org/10.1089/10665270050081478.

98


