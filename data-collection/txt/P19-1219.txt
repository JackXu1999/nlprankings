



















































Explicit Utilization of General Knowledge in Machine Reading Comprehension


Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 2263–2272
Florence, Italy, July 28 - August 2, 2019. c©2019 Association for Computational Linguistics

2263

Explicit Utilization of General Knowledge
in Machine Reading Comprehension

Chao Wang and Hui Jiang
Department of Electrical Engineering and Computer Science

Lassonde School of Engineering, York University
4700 Keele Street, Toronto, Ontario, Canada
{chwang, hj}@eecs.yorku.ca

Abstract

To bridge the gap between Machine Reading
Comprehension (MRC) models and human
beings, which is mainly reflected in the hunger
for data and the robustness to noise, in this
paper, we explore how to integrate the neu-
ral networks of MRC models with the general
knowledge of human beings. On the one hand,
we propose a data enrichment method, which
uses WordNet to extract inter-word semantic
connections as general knowledge from each
given passage-question pair. On the other
hand, we propose an end-to-end MRC model
named as Knowledge Aided Reader (KAR),
which explicitly uses the above extracted gen-
eral knowledge to assist its attention mecha-
nisms. Based on the data enrichment method,
KAR is comparable in performance with the
state-of-the-art MRC models, and significantly
more robust to noise than them. When only
a subset (20%–80%) of the training examples
are available, KAR outperforms the state-of-
the-art MRC models by a large margin, and is
still reasonably robust to noise.

1 Introduction

Machine Reading Comprehension (MRC), as the
name suggests, requires a machine to read a pas-
sage and answer its relevant questions. Since the
answer to each question is supposed to stem from
the corresponding passage, a common MRC so-
lution is to develop a neural-network-based MRC
model that predicts an answer span (i.e. the an-
swer start position and the answer end position)
from the passage of each given passage-question
pair. To facilitate the explorations and innovations
in this area, many MRC datasets have been estab-
lished, such as SQuAD (Rajpurkar et al., 2016),
MS MARCO (Nguyen et al., 2016), and Trivi-
aQA (Joshi et al., 2017). Consequently, many pi-
oneering MRC models have been proposed, such
as BiDAF (Seo et al., 2016), R-NET (Wang et al.,
2017), and QANet (Yu et al., 2018). According

to the leader board of SQuAD, the state-of-the-art
MRC models have achieved the same performance
as human beings. However, does this imply that
they have possessed the same reading comprehen-
sion ability as human beings?
OF COURSE NOT. There is a huge gap between
MRC models and human beings, which is mainly
reflected in the hunger for data and the robust-
ness to noise. On the one hand, developing MRC
models requires a large amount of training exam-
ples (i.e. the passage-question pairs labeled with
answer spans), while human beings can achieve
good performance on evaluation examples (i.e. the
passage-question pairs to address) without training
examples. On the other hand, Jia and Liang (2017)
revealed that intentionally injected noise (e.g. mis-
leading sentences) in evaluation examples causes
the performance of MRC models to drop signif-
icantly, while human beings are far less likely to
suffer from this. The reason for these phenomena,
we believe, is that MRC models can only utilize
the knowledge contained in each given passage-
question pair, but in addition to this, human beings
can also utilize general knowledge. A typical cate-
gory of general knowledge is inter-word semantic
connections. As shown in Table 1, such general
knowledge is essential to the reading comprehen-
sion ability of human beings.
A promising strategy to bridge the gap mentioned
above is to integrate the neural networks of MRC
models with the general knowledge of human be-
ings. To this end, it is necessary to solve two prob-
lems: extracting general knowledge from passage-
question pairs and utilizing the extracted gen-
eral knowledge in the prediction of answer spans.
The first problem can be solved with knowledge
bases, which store general knowledge in struc-
tured forms. A broad variety of knowledge bases
are available, such as WordNet (Fellbaum, 1998)
storing semantic knowledge, ConceptNet (Speer
et al., 2017) storing commonsense knowledge, and



2264

Passage Question Answer
Teachers may use a lesson plan to facilitate student
learning, providing a course of study which is called
the curriculum.

What can a teacher use to help
students learn?

lesson plan

Manufacturing accounts for a significant but declin-
ing share of employment, although the city’s gar-
ment industry is showing a resurgence in Brooklyn.

In what borough is the gar-
ment business prominent?

Brooklyn

Table 1: Two examples about the importance of inter-word semantic connections to the reading comprehension
ability of human beings: in the first one, we can find the answer because we know “facilitate” is a synonym of
“help”; in the second one, we can find the answer because we know “Brooklyn” is a hyponym of “borough”.

Freebase (Bollacker et al., 2008) storing factoid
knowledge. In this paper, we limit the scope of
general knowledge to inter-word semantic con-
nections, and thus use WordNet as our knowl-
edge base. The existing way to solve the second
problem is to encode general knowledge in vector
space so that the encoding results can be used to
enhance the lexical or contextual representations
of words (Weissenborn et al., 2017; Mihaylov and
Frank, 2018). However, this is an implicit way
to utilize general knowledge, since in this way we
can neither understand nor control the functioning
of general knowledge. In this paper, we discard
the existing implicit way and instead explore an
explicit (i.e. understandable and controllable) way
to utilize general knowledge.
The contribution of this paper is two-fold. On the
one hand, we propose a data enrichment method,
which uses WordNet to extract inter-word seman-
tic connections as general knowledge from each
given passage-question pair. On the other hand,
we propose an end-to-end MRC model named as
Knowledge Aided Reader (KAR), which explic-
itly uses the above extracted general knowledge to
assist its attention mechanisms. Based on the data
enrichment method, KAR is comparable in per-
formance with the state-of-the-art MRC models,
and significantly more robust to noise than them.
When only a subset (20%–80%) of the training ex-
amples are available, KAR outperforms the state-
of-the-art MRC models by a large margin, and is
still reasonably robust to noise.

2 Data Enrichment Method

In this section, we elaborate a WordNet-based data
enrichment method, which is aimed at extract-
ing inter-word semantic connections from each
passage-question pair in our MRC dataset. The
extraction is performed in a controllable manner,

and the extracted results are provided as general
knowledge to our MRC model.

2.1 Semantic Relation Chain
WordNet is a lexical database of English, where
words are organized into synsets according to their
senses. A synset is a set of words expressing the
same sense so that a word having multiple senses
belongs to multiple synsets, with each synset cor-
responding to a sense. Synsets are further related
to each other through semantic relations. Accord-
ing to the WordNet interface provided by NLTK
(Bird and Loper, 2004), there are totally sixteen
types of semantic relations (e.g. hypernyms, hy-
ponyms, holonyms, meronyms, attributes, etc.).
Based on synset and semantic relation, we define a
new concept: semantic relation chain. A semantic
relation chain is a concatenated sequence of se-
mantic relations, which links a synset to another
synset. For example, the synset “keratin.n.01”
is related to the synset “feather.n.01” through
the semantic relation “substance holonym”, the
synset “feather.n.01” is related to the synset
“bird.n.01” through the semantic relation “part
holonym”, and the synset “bird.n.01” is related
to the synset “parrot.n.01” through the semantic
relation “hyponym”, thus “substance holonym →
part holonym→ hyponym” is a semantic relation
chain, which links the synset “keratin.n.01” to the
synset “parrot.n.01”. We name each semantic re-
lation in a semantic relation chain as a hop, there-
fore the above semantic relation chain is a 3-hop
chain. By the way, each single semantic relation is
equivalent to a 1-hop chain.

2.2 Inter-word Semantic Connection
The key problem in the data enrichment method is
determining whether a word is semantically con-
nected to another word. If so, we say that there
exists an inter-word semantic connection between



2265

them. To solve this problem, we define another
new concept: the extended synsets of a word.
Given a word w, whose synsets are represented as
a set Sw, we use another set S∗w to represent its
extended synsets, which includes all the synsets
that are in Sw or that can be linked to from Sw
through semantic relation chains. Theoretically, if
there is no limitation on semantic relation chains,
S∗w will include all the synsets in WordNet, which
is meaningless in most situations. Therefore, we
use a hyper-parameter κ ∈ N to represent the per-
mitted maximum hop count of semantic relation
chains. That is to say, only the chains having no
more than κ hops can be used to construct S∗w so
that S∗w becomes a function of κ: S

∗
w(κ) (if κ = 0,

we will have S∗w(0) = Sw). Based on the above
statements, we formulate a heuristic rule for deter-
mining inter-word semantic connections: a word
w1 is semantically connected to another word w2
if and only if S∗w1(κ) ∩ Sw2 6= ∅.

2.3 General Knowledge Extraction

Given a passage-question pair, the inter-word se-
mantic connections that connect any word to any
passage word are regarded as the general knowl-
edge we need to extract. Considering the require-
ments of our MRC model, we only extract the
positional information of such inter-word seman-
tic connections. Specifically, for each word w,
we extract a set Ew, which includes the positions
of the passage words that w is semantically con-
nected to (if w itself is a passage word, we will
exclude its own position from Ew). We can con-
trol the amount of the extracted results by setting
the hyper-parameter κ: if we set κ to 0, inter-word
semantic connections will only exist between syn-
onyms; if we increase κ, inter-word semantic con-
nections will exist between more words. That is to
say, by increasing κ within a certain range, we can
usually extract more inter-word semantic connec-
tions from a passage-question pair, and thus can
provide the MRC model with more general knowl-
edge. However, due to the complexity and diver-
sity of natural languages, only a part of the ex-
tracted results can serve as useful general knowl-
edge, while the rest of them are useless for the
prediction of answer spans, and the proportion of
the useless part always rises when κ is set larger.
Therefore we set κ through cross validation (i.e.
according to the performance of the MRC model
on the development examples).

3 Knowledge Aided Reader

In this section, we elaborate our MRC model:
Knowledge Aided Reader (KAR). The key com-
ponents of most existing MRC models are their
attention mechanisms (Bahdanau et al., 2014),
which are aimed at fusing the associated represen-
tations of each given passage-question pair. These
attention mechanisms generally fall into two cat-
egories: the first one, which we name as mutual
attention, is aimed at fusing the question repre-
sentations into the passage representations so as
to obtain the question-aware passage representa-
tions; the second one, which we name as self at-
tention, is aimed at fusing the question-aware pas-
sage representations into themselves so as to ob-
tain the final passage representations. Although
KAR is equipped with both categories, its most re-
markable feature is that it explicitly uses the gen-
eral knowledge extracted by the data enrichment
method to assist its attention mechanisms. There-
fore we separately name the attention mechanisms
of KAR as knowledge aided mutual attention and
knowledge aided self attention.

3.1 Task Definition

Given a passage P = {p1, . . . , pn} and a relevant
question Q = {q1, . . . , qm}, the task is to predict
an answer span [as, ae], where 1 ≤ as ≤ ae ≤ n,
so that the resulting subsequence {pas , . . . , pae}
from P is an answer to Q.

3.2 Overall Architecture

As shown in Figure 1, KAR is an end-to-end MRC
model consisting of five layers:
Lexicon Embedding Layer. This layer maps the
words to the lexicon embeddings. The lexicon em-
bedding of each word is composed of its word em-
bedding and character embedding. For each word,
we use the pre-trained GloVe (Pennington et al.,
2014) word vector as its word embedding, and ob-
tain its character embedding with a Convolutional
Neural Network (CNN) (Kim, 2014). For both
the passage and the question, we pass the con-
catenation of the word embeddings and the charac-
ter embeddings through a shared dense layer with
ReLU activation, whose output dimensionality is
d. Therefore we obtain the passage lexicon em-
beddings LP ∈ Rd×n and the question lexicon
embeddings LQ ∈ Rd×m.
Context Embedding Layer. This layer maps the
lexicon embeddings to the context embeddings.



2266

Figure 1: An end-to-end MRC model: Knowledge Aided Reader (KAR)

For both the passage and the question, we process
the lexicon embeddings (i.e. LP for the passage
and LQ for the question) with a shared bidirec-
tional LSTM (BiLSTM) (Hochreiter and Schmid-
huber, 1997), whose hidden state dimensionality
is 12d. By concatenating the forward LSTM out-
puts and the backward LSTM outputs, we obtain
the passage context embeddings CP ∈ Rd×n and
the question context embeddings CQ ∈ Rd×m.
Coarse Memory Layer. This layer maps the con-
text embeddings to the coarse memories. First we
use knowledge aided mutual attention (introduced
later) to fuse CQ into CP , the outputs of which are
represented as G̃ ∈ Rd×n. Then we process G̃
with a BiLSTM, whose hidden state dimension-
ality is 12d. By concatenating the forward LSTM
outputs and the backward LSTM outputs, we ob-
tain the coarse memories G ∈ Rd×n, which are
the question-aware passage representations.
Refined Memory Layer. This layer maps the
coarse memories to the refined memories. First
we use knowledge aided self attention (introduced
later) to fuse G into themselves, the outputs of
which are represented as H̃ ∈ Rd×n. Then we
process H̃ with a BiLSTM, whose hidden state di-
mensionality is 12d. By concatenating the forward
LSTM outputs and the backward LSTM outputs,
we obtain the refined memoriesH ∈ Rd×n, which
are the final passage representations.
Answer Span Prediction Layer. This layer pre-

dicts the answer start position and the answer end
position based on the above layers. First we obtain
the answer start position distribution os:

ti = v
>
s tanh(Wshpi + UsrQ) ∈ R

os = softmax({t1, . . . , tn}) ∈ Rn

where vs, Ws, and Us are trainable parameters;
hpi represents the refined memory of each passage
word pi (i.e. the i-th column in H); rQ represents
the question summary obtained by performing an
attention pooling over CQ. Then we obtain the an-
swer end position distribution oe:

ti = v
>
e tanh(Wehpi + Ue[rQ;Hos]) ∈ R

oe = softmax({t1, . . . , tn}) ∈ Rn

where ve, We, and Ue are trainable parameters;
[; ] represents vector concatenation. Finally we
construct an answer span prediction matrix O =
uptri(oso

>
e ) ∈ Rn×n, where uptri(X) represents

the upper triangular matrix of a matrix X . There-
fore, for the training, we minimize −log(Oas,ae)
on each training example whose labeled answer
span is [as, ae]; for the inference, we separately
take the row index and column index of the maxi-
mum element in O as as and ae.

3.3 Knowledge Aided Mutual Attention
As a part of the coarse memory layer, knowledge
aided mutual attention is aimed at fusing the ques-



2267

tion context embeddings CQ into the passage con-
text embeddings CP , where the key problem is to
calculate the similarity between each passage con-
text embedding cpi (i.e. the i-th column in CP )
and each question context embedding cqj (i.e. the
j-th column in CQ). To solve this problem, Seo
et al. (2016) proposed a similarity function:

f(cpi , cqj ) = v
>
f [cpi ; cqj ; cpi � cqj ] ∈ R

where vf is a trainable parameter; � represents
element-wise multiplication. This similarity func-
tion has also been adopted by several other works
(Clark and Gardner, 2017; Yu et al., 2018). How-
ever, since context embeddings contain high-level
information, we believe that introducing the pre-
extracted general knowledge into the calculation
of such similarities will make the results more rea-
sonable. Therefore we modify the above similarity
function to the following form:

f∗(cpi , cqj ) = v
>
f [c
∗
pi ; c

∗
qj ; c

∗
pi � c

∗
qj ] ∈ R

where c∗x represents the enhanced context embed-
ding of a word x. We use the pre-extracted gen-
eral knowledge to construct the enhanced context
embeddings. Specifically, for each word w, whose
context embedding is cw, to construct its enhanced
context embedding c∗w, first recall that we have
extracted a set Ew, which includes the positions
of the passage words that w is semantically con-
nected to, thus by gathering the columns in CP
whose indexes are given by Ew, we obtain the
matching context embeddings Z ∈ Rd×|Ew|. Then
by constructing a cw-attended summary of Z, we
obtain the matching vector c+w (if Ew = ∅, which
makes Z = {}, we will set c+w = 0):

ti = v
>
c tanh(Wczi + Uccw) ∈ R

c+w = Z softmax({t1, . . . , t|Ew|}) ∈ R
d

where vc, Wc, and Uc are trainable parameters; zi
represents the i-th column in Z. Finally we pass
the concatenation of cw and c+w through a dense
layer with ReLU activation, whose output dimen-
sionality is d. Therefore we obtain the enhanced
context embedding c∗w ∈ Rd.
Based on the modified similarity function and
the enhanced context embeddings, to perform
knowledge aided mutual attention, first we con-
struct a knowledge aided similarity matrix A ∈
Rn×m, where each element Ai,j = f∗(cpi , cqj ).
Then following Yu et al. (2018), we construct the

passage-attended question summaries RQ and the
question-attended passage summaries RP :

RQ = CQ softmax
>
r (A) ∈ Rd×n

RP = CP softmaxc(A) softmax
>
r (A) ∈ Rd×n

where softmaxr represents softmax along the row
dimension and softmaxc along the column dimen-
sion. Finally following Clark and Gardner (2017),
we pass the concatenation of CP , RQ, CP � RQ,
andRP�RQ through a dense layer with ReLU ac-
tivation, whose output dimensionality is d. There-
fore we obtain the outputs G̃ ∈ Rd×n.

3.4 Knowledge Aided Self Attention

As a part of the refined memory layer, knowledge
aided self attention is aimed at fusing the coarse
memories G into themselves. If we simply fol-
low the self attentions of other works (Wang et al.,
2017; Huang et al., 2017; Liu et al., 2017b; Clark
and Gardner, 2017), then for each passage word
pi, we should fuse its coarse memory gpi (i.e. the
i-th column in G) with the coarse memories of
all the other passage words. However, we believe
that this is both unnecessary and distracting, since
each passage word has nothing to do with many
of the other passage words. Thus we use the pre-
extracted general knowledge to guarantee that the
fusion of coarse memories for each passage word
will only involve a precise subset of the other pas-
sage words. Specifically, for each passage word
pi, whose coarse memory is gpi , to perform the fu-
sion of coarse memories, first recall that we have
extracted a set Epi , which includes the positions
of the other passage words that pi is semantically
connected to, thus by gathering the columns in G
whose indexes are given by Epi , we obtain the
matching coarse memories Z ∈ Rd×|Epi |. Then
by constructing a gpi-attended summary of Z, we
obtain the matching vector g+pi (if Epi = ∅, which
makes Z = {}, we will set g+pi = 0):

ti = v
>
g tanh(Wgzi + Uggpi) ∈ R

g+pi = Z softmax({t1, . . . , t|Epi |}) ∈ R
d

where vg, Wg, and Ug are trainable parameters.
Finally we pass the concatenation of gpi and
g+pi through a dense layer with ReLU activation,
whose output dimensionality is d. Therefore we
obtain the fusion result h̃pi ∈ Rd, and further the
outputs H̃ = {h̃p1 , . . . , h̃pn} ∈ Rd×n.



2268

4 Related Works

Attention Mechanisms. Besides those mentioned
above, other interesting attention mechanisms in-
clude performing multi-round alignment to avoid
the problems of attention redundancy and atten-
tion deficiency (Hu et al., 2017), and using mutual
attention as a skip-connector to densely connect
pairwise layers (Tay et al., 2018).
Data Augmentation. It is proved that properly
augmenting training examples can improve the
performance of MRC models. For example, Yang
et al. (2017) trained a generative model to generate
questions based on unlabeled text, which substan-
tially boosted their performance; Yu et al. (2018)
trained a back-and-forth translation model to para-
phrase training examples, which brought them a
significant performance gain.
Multi-step Reasoning. Inspired by the fact that
human beings are capable of understanding com-
plex documents by reading them over and over
again, multi-step reasoning was proposed to bet-
ter deal with difficult MRC tasks. For example,
Shen et al. (2017) used reinforcement learning to
dynamically determine the number of reasoning
steps; Liu et al. (2017b) fixed the number of rea-
soning steps, but used stochastic dropout in the
output layer to avoid step bias.
Linguistic Embeddings. It is both easy and effec-
tive to incorporate linguistic embeddings into the
input layer of MRC models. For example, Chen
et al. (2017) and Liu et al. (2017b) used POS em-
beddings and NER embeddings to construct their
input embeddings; Liu et al. (2017a) used struc-
tural embeddings based on parsing trees to con-
structed their input embeddings.
Transfer Learning. Several recent breakthroughs
in MRC benefit from feature-based transfer learn-
ing (McCann et al., 2017; Peters et al., 2018) and
fine-tuning-based transfer learning (Radford et al.,
2018; Devlin et al., 2018), which are based on
certain word-level or sentence-level models pre-
trained on large external corpora in certain super-
vised or unsupervised manners.

5 Experiments

5.1 Experimental Settings

MRC Dataset. The MRC dataset used in this pa-
per is SQuAD 1.1, which contains over 100, 000
passage-question pairs and has been randomly par-
titioned into three parts: a training set (80%),

a development set (10%), and a test set (10%).
Besides, we also use two of its adversarial sets,
namely AddSent and AddOneSent (Jia and Liang,
2017), to evaluate the robustness to noise of MRC
models. The passages in the adversarial sets con-
tain misleading sentences, which are aimed at dis-
tracting MRC models. Specifically, each passage
in AddSent contains several sentences that are
similar to the question but not contradictory to the
answer, while each passage in AddOneSent con-
tains a human-approved random sentence that may
be unrelated to the passage.
Implementation Details. We tokenize the MRC
dataset with spaCy 2.0.13 (Honnibal and Mon-
tani, 2017), manipulate WordNet 3.0 with NLTK
3.3, and implement KAR with TensorFlow 1.11.0
(Abadi et al., 2016). For the data enrichment
method, we set the hyper-parameter κ to 3. For the
dense layers and the BiLSTMs, we set the dimen-
sionality unit d to 600. For model optimization,
we apply the Adam (Kingma and Ba, 2014) opti-
mizer with a learning rate of 0.0005 and a mini-
batch size of 32. For model evaluation, we use
Exact Match (EM) and F1 score as evaluation met-
rics. To avoid overfitting, we apply dropout (Sri-
vastava et al., 2014) to the dense layers and the
BiLSTMs with a dropout rate of 0.3. To boost the
performance, we apply exponential moving aver-
age with a decay rate of 0.999.

5.2 Model Comparison in both Performance
and the Robustness to Noise

We compare KAR with other MRC models in both
performance and the robustness to noise. Specif-
ically, we not only evaluate the performance of
KAR on the development set and the test set, but
also do this on the adversarial sets. As for the com-
parative objects, we only consider the single MRC
models that rank in the top 20 on the SQuAD 1.1
leader board and have reported their performance
on the adversarial sets. There are totally five such
comparative objects, which can be considered as
representatives of the state-of-the-art MRC mod-
els. As shown in Table 2, on the development set
and the test set, the performance of KAR is on par
with that of the state-of-the-art MRC models; on
the adversarial sets, KAR outperforms the state-
of-the-art MRC models by a large margin. That
is to say, KAR is comparable in performance with
the state-of-the-art MRC models, and significantly
more robust to noise than them.



2269

Single MRC model Dev set(EM / F1)
Test set
(EM / F1)

AddSent
(F1)

AddOneSent
(F1)

FusionNet (Huang et al., 2017) 75.3 / 83.6 76.0 / 83.9 51.4 60.7
RaSoR+TR+LM (Salant and Be-
rant, 2017)

77.0 / 84.0 77.6 / 84.2 47.0 57.0

SAN (Liu et al., 2017b) 76.2 / 84.1 76.8 / 84.4 46.6 56.5
R.M-Reader (Hu et al., 2017) 78.9 / 86.3 79.5 / 86.6 58.5 67.0
QANet (with data augmentation)
(Yu et al., 2018)

75.1 / 83.8 82.5 / 89.3 45.2 55.7

KAR (ours) 76.7 / 84.9 76.1 / 83.5 60.1 72.3

Table 2: Model comparison based on SQuAD 1.1 and two of its adversarial sets: AddSent and AddOneSent. All
the numbers are up to date as of October 18, 2018. Note that SQuAD 2.0 (Rajpurkar et al., 2018) is not involved in
this paper, because it requires MRC models to deal with the problem of answer triggering, but this paper is aimed
at improving the hunger for data and robustness to noise of MRC models.

To verify the effectiveness of general knowledge,
we first study the relationship between the amount
of general knowledge and the performance of
KAR. As shown in Table 3, by increasing κ from
0 to 5 in the data enrichment method, the amount
of general knowledge rises monotonically, but the
performance of KAR first rises until κ reaches 3
and then drops down. Then we conduct an ablation
study by replacing the knowledge aided attention
mechanisms with the mutual attention proposed
by Seo et al. (2016) and the self attention proposed
by Wang et al. (2017) separately, and find that the
F1 score of KAR drops by 4.2 on the development
set, 7.8 on AddSent, and 9.1 on AddOneSent. Fi-
nally we find that after only one epoch of train-
ing, KAR already achieves an EM of 71.9 and an
F1 score of 80.8 on the development set, which is
even better than the final performance of several
strong baselines, such as DCN (EM / F1: 65.4 /
75.6) (Xiong et al., 2016) and BiDAF (EM / F1:
67.7 / 77.3) (Seo et al., 2016). The above empiri-
cal findings imply that general knowledge indeed
plays an effective role in KAR.
To demonstrate the advantage of our explicit way
to utilize general knowledge over the existing im-
plicit way, we compare the performance of KAR
with that reported by Weissenborn et al. (2017),
which used an encoding-based method to utilize
the general knowledge dynamically retrieved from
Wikipedia and ConceptNet. Since their best model
only achieved an EM of 69.5 and an F1 score of
79.7 on the development set, which is much lower
than the performance of KAR, we have good rea-
son to believe that our explicit way works better
than the existing implicit way.

κ
Average number of inter-
word semantic connections
per word

Dev set
(EM / F1)

0 0.39 74.2 / 82.8
1 0.63 74.6 / 83.1
2 1.24 75.1 / 83.5
3 2.21 76.7 / 84.9
4 3.68 75.9 / 84.3
5 5.58 75.3 / 83.8

Table 3: With κ set to different values in the data en-
richment method, we calculate the average number of
inter-word semantic connections per word as an estima-
tion of the amount of general knowledge, and evaluate
the performance of KAR on the development set.

5.3 Model Comparison in the Hunger for
Data

We compare KAR with other MRC models in the
hunger for data. Specifically, instead of using all
the training examples, we produce several train-
ing subsets (i.e. subsets of the training examples)
so as to study the relationship between the pro-
portion of the available training examples and the
performance. We produce each training subset by
sampling a specific number of questions from all
the questions relevant to each passage. By sepa-
rately sampling 1, 2, 3, and 4 questions on each
passage, we obtain four training subsets, which
separately contain 20%, 40%, 60%, and 80% of
the training examples. As shown in Figure 2, with
KAR, SAN (re-implemented), and QANet (re-
implemented without data augmentation) trained
on these training subsets, we evaluate their perfor-
mance on the development set, and find that KAR



2270

Figure 2: With KAR, SAN, and QANet (without data
augmentation) trained on the training subsets, we eval-
uate their performance on the development set.

Figure 3: With KAR, SAN, and QANet (without data
augmentation) trained on the training subsets, we eval-
uate their performance on AddSent.

performs much better than SAN and QANet. As
shown in Figure 3 and Figure 4, with the above
KAR, SAN, and QANet trained on the same train-
ing subsets, we also evaluate their performance on
the adversarial sets, and still find that KAR per-
forms much better than SAN and QANet. That is
to say, when only a subset of the training exam-
ples are available, KAR outperforms the state-of-
the-art MRC models by a large margin, and is still
reasonably robust to noise.

6 Analysis

According to the experimental results, KAR is not
only comparable in performance with the state-of-
the-art MRC models, but also superior to them in
terms of both the hunger for data and the robust-

Figure 4: With KAR, SAN, and QANet (without data
augmentation) trained on the training subsets, we eval-
uate their performance on AddOneSent.

ness to noise. The reasons for these achievements,
we believe, are as follows:

• KAR is designed to utilize the pre-extracted
inter-word semantic connections from the
data enrichment method. Some inter-word
semantic connections, especially those ob-
tained through multi-hop semantic relation
chains, are very helpful for the prediction of
answer spans, but they will be too covert to
capture if we simply leverage recurrent neu-
ral networks (e.g. BiLSTM) and pre-trained
word vectors (e.g. GloVe).

• An inter-word semantic connection extracted
from a passage-question pair usually also ap-
pears in many other passage-question pairs,
therefore it is very likely that the inter-word
semantic connections extracted from a small
amount of training examples actually cover
a much larger amount of training examples.
That is to say, we are actually using much
more training examples for model optimiza-
tion than the available ones.

• Some inter-word semantic connections are
distracting for the prediction of answer spans.
For example, the inter-word semantic con-
nection between “bank” and “waterside”
makes no sense given the context “the bank
manager is walking along the waterside”. It
is the knowledge aided attention mechanisms
that enable KAR to ignore such distracting
inter-word semantic connections so that only
the important ones are used.



2271

7 Conclusion

In this paper, we innovatively integrate the neural
networks of MRC models with the general knowl-
edge of human beings. Specifically, inter-word se-
mantic connections are first extracted from each
given passage-question pair by a WordNet-based
data enrichment method, and then provided as
general knowledge to an end-to-end MRC model
named as Knowledge Aided Reader (KAR), which
explicitly uses the general knowledge to assist its
attention mechanisms. Experimental results show
that KAR is not only comparable in performance
with the state-of-the-art MRC models, but also su-
perior to them in terms of both the hunger for data
and the robustness to noise. In the future, we plan
to use some larger knowledge bases, such as Con-
ceptNet and Freebase, to improve the quality and
scope of the general knowledge.

Acknowledgments

This work is partially supported by a research do-
nation from iFLYTEK Co., Ltd., Hefei, China, and
a discovery grant from Natural Sciences and Engi-
neering Research Council (NSERC) of Canada.

References
Martı́n Abadi, Paul Barham, Jianmin Chen, Zhifeng

Chen, Andy Davis, Jeffrey Dean, Matthieu Devin,
Sanjay Ghemawat, Geoffrey Irving, Michael Isard,
et al. 2016. Tensorflow: A system for large-scale
machine learning. In OSDI, volume 16, pages 265–
283.

Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben-
gio. 2014. Neural machine translation by jointly
learning to align and translate. arXiv preprint
arXiv:1409.0473.

Steven Bird and Edward Loper. 2004. Nltk: the nat-
ural language toolkit. In Proceedings of the ACL
2004 on Interactive poster and demonstration ses-
sions, page 31. Association for Computational Lin-
guistics.

Kurt Bollacker, Colin Evans, Praveen Paritosh, Tim
Sturge, and Jamie Taylor. 2008. Freebase: a collab-
oratively created graph database for structuring hu-
man knowledge. In Proceedings of the 2008 ACM
SIGMOD international conference on Management
of data, pages 1247–1250. ACM.

Danqi Chen, Adam Fisch, Jason Weston, and An-
toine Bordes. 2017. Reading wikipedia to an-
swer open-domain questions. arXiv preprint
arXiv:1704.00051.

Christopher Clark and Matt Gardner. 2017. Simple
and effective multi-paragraph reading comprehen-
sion. arXiv preprint arXiv:1710.10723.

Jacob Devlin, Ming-Wei Chang, Kenton Lee, and
Kristina Toutanova. 2018. Bert: Pre-training of deep
bidirectional transformers for language understand-
ing. arXiv preprint arXiv:1810.04805.

Christiane Fellbaum. 1998. WordNet. Wiley Online
Library.

Sepp Hochreiter and Jürgen Schmidhuber. 1997.
Long short-term memory. Neural computation,
9(8):1735–1780.

Matthew Honnibal and Ines Montani. 2017. spacy 2:
Natural language understanding with bloom embed-
dings, convolutional neural networks and incremen-
tal parsing. To appear.

Minghao Hu, Yuxing Peng, Zhen Huang, Xipeng Qiu,
Furu Wei, and Ming Zhou. 2017. Reinforced
mnemonic reader for machine reading comprehen-
sion. arXiv preprint arXiv:1705.02798.

Hsin-Yuan Huang, Chenguang Zhu, Yelong Shen, and
Weizhu Chen. 2017. Fusionnet: Fusing via fully-
aware attention with application to machine compre-
hension. arXiv preprint arXiv:1711.07341.

Robin Jia and Percy Liang. 2017. Adversarial exam-
ples for evaluating reading comprehension systems.
arXiv preprint arXiv:1707.07328.

Mandar Joshi, Eunsol Choi, Daniel S Weld, and Luke
Zettlemoyer. 2017. Triviaqa: A large scale distantly
supervised challenge dataset for reading comprehen-
sion. arXiv preprint arXiv:1705.03551.

Yoon Kim. 2014. Convolutional neural net-
works for sentence classification. arXiv preprint
arXiv:1408.5882.

Diederik P Kingma and Jimmy Ba. 2014. Adam: A
method for stochastic optimization. arXiv preprint
arXiv:1412.6980.

Rui Liu, Junjie Hu, Wei Wei, Zi Yang, and Eric Ny-
berg. 2017a. Structural embedding of syntactic
trees for machine comprehension. arXiv preprint
arXiv:1703.00572.

Xiaodong Liu, Yelong Shen, Kevin Duh, and Jian-
feng Gao. 2017b. Stochastic answer networks for
machine reading comprehension. arXiv preprint
arXiv:1712.03556.

Bryan McCann, James Bradbury, Caiming Xiong, and
Richard Socher. 2017. Learned in translation: Con-
textualized word vectors. In Advances in Neural In-
formation Processing Systems, pages 6294–6305.

Todor Mihaylov and Anette Frank. 2018. Knowledge-
able reader: Enhancing cloze-style reading com-
prehension with external commonsense knowledge.
arXiv preprint arXiv:1805.07858.



2272

Tri Nguyen, Mir Rosenberg, Xia Song, Jianfeng Gao,
Saurabh Tiwary, Rangan Majumder, and Li Deng.
2016. Ms marco: A human generated machine
reading comprehension dataset. arXiv preprint
arXiv:1611.09268.

Jeffrey Pennington, Richard Socher, and Christopher
Manning. 2014. Glove: Global vectors for word
representation. In Proceedings of the 2014 confer-
ence on empirical methods in natural language pro-
cessing (EMNLP), pages 1532–1543.

Matthew E Peters, Mark Neumann, Mohit Iyyer, Matt
Gardner, Christopher Clark, Kenton Lee, and Luke
Zettlemoyer. 2018. Deep contextualized word rep-
resentations. arXiv preprint arXiv:1802.05365.

Alec Radford, Karthik Narasimhan, Tim Salimans, and
Ilya Sutskever. 2018. Improving language under-
standing by generative pre-training. URL https://s3-
us-west-2. amazonaws. com/openai-assets/research-
covers/languageunsupervised/language under-
standing paper. pdf.

Pranav Rajpurkar, Robin Jia, and Percy Liang. 2018.
Know what you don’t know: Unanswerable ques-
tions for squad. arXiv preprint arXiv:1806.03822.

Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and
Percy Liang. 2016. Squad: 100,000+ questions
for machine comprehension of text. arXiv preprint
arXiv:1606.05250.

Shimi Salant and Jonathan Berant. 2017. Contextu-
alized word representations for reading comprehen-
sion. arXiv preprint arXiv:1712.03609.

Minjoon Seo, Aniruddha Kembhavi, Ali Farhadi, and
Hannaneh Hajishirzi. 2016. Bidirectional attention
flow for machine comprehension. arXiv preprint
arXiv:1611.01603.

Yelong Shen, Po-Sen Huang, Jianfeng Gao, and
Weizhu Chen. 2017. Reasonet: Learning to stop
reading in machine comprehension. In Proceedings
of the 23rd ACM SIGKDD International Conference
on Knowledge Discovery and Data Mining, pages
1047–1055. ACM.

Robert Speer, Joshua Chin, and Catherine Havasi.
2017. Conceptnet 5.5: An open multilingual graph
of general knowledge. In Thirty-First AAAI Confer-
ence on Artificial Intelligence.

Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky,
Ilya Sutskever, and Ruslan Salakhutdinov. 2014.
Dropout: A simple way to prevent neural networks
from overfitting. The Journal of Machine Learning
Research, 15(1):1929–1958.

Yi Tay, Anh Tuan Luu, Siu Cheung Hui, and Jian Su.
2018. Densely connected attention propagation for
reading comprehension. In Advances in Neural In-
formation Processing Systems, pages 4906–4917.

Wenhui Wang, Nan Yang, Furu Wei, Baobao Chang,
and Ming Zhou. 2017. Gated self-matching net-
works for reading comprehension and question an-
swering. In Proceedings of the 55th Annual Meet-
ing of the Association for Computational Linguistics
(Volume 1: Long Papers), volume 1, pages 189–198.

Dirk Weissenborn, Tomáš Kočiskỳ, and Chris Dyer.
2017. Dynamic integration of background knowl-
edge in neural nlu systems. arXiv preprint
arXiv:1706.02596.

Caiming Xiong, Victor Zhong, and Richard Socher.
2016. Dynamic coattention networks for question
answering. arXiv preprint arXiv:1611.01604.

Zhilin Yang, Junjie Hu, Ruslan Salakhutdinov, and
William W Cohen. 2017. Semi-supervised qa with
generative domain-adaptive nets. arXiv preprint
arXiv:1702.02206.

Adams Wei Yu, David Dohan, Minh-Thang Luong, Rui
Zhao, Kai Chen, Mohammad Norouzi, and Quoc V
Le. 2018. Qanet: Combining local convolution
with global self-attention for reading comprehen-
sion. arXiv preprint arXiv:1804.09541.


