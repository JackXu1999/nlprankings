



















































Noisy-context surprisal as a human sentence processing cost model


Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 1, Long Papers, pages 688–698,
Valencia, Spain, April 3-7, 2017. c©2017 Association for Computational Linguistics

Noisy-context surprisal as a human sentence processing cost model

Richard Futrell and Roger Levy
Department of Brain and Cognitive Sciences

Massachusetts Institute of Technology
{futrell, rplevy}@mit.edu

Abstract

We use the noisy-channel theory of hu-
man sentence comprehension to develop
an incremental processing cost model
that unifies and extends key features
of expectation-based and memory-based
models. In this model, which we call
noisy-context surprisal, the processing
cost of a word is the surprisal of the
word given a noisy representation of
the preceding context. We show that
this model accounts for an outstand-
ing puzzle in sentence comprehension,
language-dependent structural forgetting
effects (Gibson and Thomas, 1999; Va-
sishth et al., 2010; Frank et al., 2016),
which are previously not well modeled by
either expectation-based or memory-based
approaches. Additionally, we show that
this model derives and generalizes local-
ity effects (Gibson, 1998; Demberg and
Keller, 2008), a signature prediction of
memory-based models. We give corpus-
based evidence for a key assumption in
this derivation.

1 Introduction

Models of human sentence processing difficulty
can be divided into two kinds, expectation-based
and memory-based. Expectation-based models
predict the processing difficulty of a word from
the word’s surprisal given previous material in the
sentence (Hale, 2001; Levy, 2008a). These models
have good coverage: they can account for effects
of syntactic construction frequency and resolution
of ambiguity on incremental processing difficulty.
Memory-based models, on the other hand, explain
difficulty resulting from working memory limita-
tions during incremental parsing (Gibson, 1998;

Lewis and Vasishth, 2005); a major prediction of
these models is locality effects, where process-
ing a word is difficult when it is far from other
words with which it must be syntactically inte-
grated. Expectation-based models do not intrin-
sically capture this difficulty.

Integrating these two approaches at a high level
has proven challenging. A major hurdle is that
the theories are typically stated at different lev-
els of analysis: expectation-based theories are
computational-level theories (Marr, 1982) specify-
ing what computational problem the human sen-
tence processing system is solving—the problem
of how update one’s belief about a sentence given
a new word—without specifying implementation
details. Memory-based theories such as Lewis
and Vasishth (2005) are for the most part based in
mechanistic algorithmic-level theories describing
the actions of a specific incremental parser.

Previous theories that capture both surprisal and
locality effects have typically done so by aug-
menting parsing models with a special prediction-
verification operation to capture surprisal effects
(Demberg and Keller, 2009; Demberg et al.,
2013), or by combining surprisal and memory-
based cost derived from a parsing model as sep-
arate factors in a linear model (Shain et al., 2016).
These models capture surprisal and locality effects
at the same time, but they do not clearly capture
phenomena involving the interaction of memory
and probabilistic expectations such as language-
dependent structural forgetting (see Section 3).

Here we develop a computational-level model
capturing both memory and expectation effects
from a single set of principles, without reference
to a specific parsing algorithm. In our model,
the processing cost of a word is a function of its
surprisal given a noisy representation of previous
context (Section 2). We show that the model can
reproduce structural forgetting effects, including

688



the difference between English and German (Sec-
tion 3), a phenomenon not previously captured
by memory-based or expectation-based models in
isolation. We also give a derivation of the exis-
tence of locality effects in the model; these effects
were previously accounted for only in mechanistic
memory-based models (Section 4). The derivation
yields a generalization of classic locality effects
which we call information locality: sentences
are predicted to be easier to process when words
with high mutual information are close. We give
corpus-based evidence that words in syntactic de-
pendencies have high mutual information, mean-
ing that classical dependency locality effects can
be seen as a subset of information locality effects.

2 Noisy-Context Surprisal

In surprisal theory, the processing cost of a word is
asserted to be proportional to extent to which one
must change one’s beliefs given that word (Hale,
2001; Smith and Levy, 2013). So the cost of a
word is (up to proportionality):

Csurprisal(wi|w1:i−1) = − log pL(wi|w1:i−1), (1)
where pL(·|·) is the conditional probability of a
word in context in a probabilistic language L.

Standard surprisal assumes that the compre-
hender has perfect access to a representation of
wi’s full context, including the words preceding it
in the sentence (w1:i−1) and also extra-sentential
context (which we leave implicit). But given that
human working memory is limited, the assump-
tion of perfect access is unrealistic. We propose
that processing cost at a word is better modeled
as the cost of belief updates given a noisy rep-
resentation of the previous input. The probabil-
ity of a word given a noisy context is modeled as
the noisy channel probability of the word, assum-
ing that people do noisy channel inference on their
context representation (Levy, 2008b; Gibson et al.,
2013). Given this model, the expected processing
cost of a word is its expected surprisal over the
possible noisy representations of its context.

The noisy-context surprisal processing cost
function is thus:1

C(wi|w1:i−1) = E
V |w1:i−1

[− log pNCL (wi|V )] (2)

= −
∑
V

pN (V |w1:i−1) log pNCL (wi|V ) (3)

1Neglecting the implicit proportionality term in Equa-
tion 1.

where V is the noisy representation of the previous
materialw1:i−1, the noise distribution pN charac-
terizes how memory of previous material may be
corrupted, and pNCL (·|·) is the noisy-channel prob-
ability of a word given a noisy context, computed
via marginalization:

pNCL (wi|V ) =
∑

w1:i−1

pL(wi|w1:i−1)pNC(w1:i−1|V )

with pNC(w1:i−1|V ) computed via Bayes Rule:

pNC(w1:i−1|V ) ∝ pN (V |w1:i−1)pL(w1:i−1).

Note here that wi’s cost is computed using its true
identity but a noisy representation of the context:
from the incremental perspective, wi is observed
now, but context is stored and retrieved in a po-
tentially noisy storage medium. This asymmetry
between noise levels for proximal versus distal in-
put differs from the noisy-channel surprisal model
of Levy (2011), and is crucial to the derivation of
information locality we present in Section 4.

Here we use two types of noise distributions for
pN : erasure noise and deletion noise. In erasure
noise, a symbol in the context is probabilistically
erased and replaced with a special symbol E with
probability e. In deletion noise, a symbol is erased
from the sequence completely, leaving no trace.
Given deletion noise, a comprehender does not
know how many symbols were in the original con-
text; with erasure noise, the comprehender knows
exactly which symbols were affected by noise. In
both cases, we assume that the application or non-
application of noise is probabilistically indepen-
dent among elements in the context. We use these
concrete noise distributions for convenience, but
we believe our results should generalize to larger
classes of noise distributions.

3 Structural Forgetting Effects

Here we show that noisy-context surprisal as
a processing cost model can reproduce effects
that were not previously well-explained by either
expectation-based or memory-based theories. In
particular, we take up the puzzle of structural
forgetting effects, where comprehenders seem
to forget structural elements of a sentence prefix
when predicting the rest of the sentence. The re-
sult is that some ungrammatical sentences have
lower processing cost and higher acceptability
than some complex grammatical sentences: with

689



doubly nested relative clauses, for instance, sub-
jects rate ungrammatical sentence (1) as more ac-
ceptable than sentence (2), forgetting about the
VP predicted by the second noun (Gibson and
Thomas, 1999).

(1) *The apartment1 that the maid2 who the
cleaning service3 had3 sent over was1 well-
decorated.

(2) The apartment1 that the maid2 who the
cleaning service3 had3 sent over was2 cleaning ev-
ery week was1 well-decorated.
Vasishth et al. (2010) show this same effect in
reading times at the last verb: in English native
speakers are more surprised to encounter a third
VP than not to. However, this effect is language-
specific: the same authors find that in German, na-
tive speakers are more surprised when a third VP is
missing than when it is present. Frank et al. (2016)
show further that native speakers do not show the
effect in Dutch, but Dutch-native L2 speakers of
English do show the effect in English. The result
shows that the memory resources taxed by these
structures are themselves meaningfully shaped by
the distributional statistics of the language.

The verb forgetting effect is a challenge for
both expectation-based and memory-based mod-
els. Pure expectation-based models cannot repro-
duce the effect: they have no mechanism for for-
getting an established VP prediction and thus they
assign small or zero probability to ungrammati-
cal sentences. On the other hand, memory-based
models will have to account for why the same
structures are forgotten in English but not in Ger-
man. Here we show that noisy-context surprisal
provides the first purely computational-level ac-
count for the language-dependent verb forgetting
effect. The essential mechanism is that when verb-
final nested structures are more probable in a lan-
guage, then they will be better preserved in a noisy
memory representation.

3.1 Model of Verb Forgetting

Table 1 presents a toy probabilistic context-free
grammar for the constructions involved in verb
forgetting. The grammar generates strings over the
alphabet of N (noun), V (verb), C (complemen-
tizer), P (preposition). We apply deletion noise
with by-symbol deletion probability d. So for ex-
ample, given a prefix NCNCNVV, the prefix can
be corrupted to NCNNVV with probability pro-
portional to d, representing one deletion. In that

Rule Probability
S→ NP V 1
NP→ N 1−m
NP→ N RC mr
NP→ N PP m(1− r)
PP→ P NP 1
RC→ C V NP s
RC→ C NP V 1− s

Table 1: Toy grammar used to demonstrate verb
forgetting. Nouns are postmodified with proba-
bility m; a postmodifier is a relative clause with
probability r, and a relative clause is V-initial with
probability s. For practical reasons we bound non-
terminal rewrites of NP at 2.

case a noisy-channel comprehender might incor-
rectly infer that the original prefix was in fact NC-
NPNVV, and thus fail to predict a third verb.

To illustrate that noisy surprisal can account
for language-dependent verb forgetting, we show
in Figure 1 the differences between noisy sur-
prisal values for grammatical (V) and ungrammat-
ical (end-of-sentence) continuations of prefixes
NCNCNVV under parameter settings reflecting
the difference between English and German, and
compare these differences with self-paced read-
ing times observed after the final verb by Vasishth
et al. (2010). Noisy surprisal qualitatively re-
produces language-dependent verb forgetting: in
English the ungrammatical continuation is higher
surprisal, but in German the grammatical contin-
uation is higher surprisal. The English–German
difference in the model is entirely accounted for
by the parameter s, which determines the propor-
tion of relative clauses that are verb-initial. In En-
glish, most relative clauses are subject-extracted
and those are verb-initial, so for English s ≈ .8
(Roland et al., 2007). German, in contrast, has
s ≈ 0, since its relative clauses are obligatorily
verb-final. When verb-final relative clauses have
higher prior probability, a doubly-nested RC pre-
fix NCNCVV is more likely to be preserved by a
rational noisy-channel comprehender.

The results of Figure 1 do not speak, how-
ever, to the generality of the model’s predictions
regarding verb forgetting. To explore this mat-
ter, we partition the model’s four-dimensional pa-
rameter space into regions distinguishing whether
noisy-context surprisal is lower for (G) grammat-
ical continuations or (U) ungrammatical contin-

690



−500

0

500

1000

English German

(U
ng

ra
m

m
at

ic
al

 −
 G

ra
m

m
at

ic
al

) 
R

T
 (

m
s)

−1

0

1

2

English German

(U
ng

ra
m

m
at

ic
al

 −
 G

ra
m

m
at

ic
al

) 
su

rp
ris

al
 (

bi
ts

)

Figure 1: Differences in reaction times for
ungrammatical continuations minus grammatical
continuations, compared to noisy surprisal differ-
ences. RT data comes from self-paced reading ex-
periments in Vasishth et al. (2010) in the post-VP
region. The noisy surprisal predictions are pro-
duced with d = .2, m = .5, r = .5 fixed, and
s = .8 for English and s = 0 for German.

uations for (1) singly-embedded NCNV and (2)
doubly-embedded NCNCNVV contexts. Figure 2
shows this partition for a range of r, s, m, and
d. In the blue region, grammatical continuations
are lower-cost than ungrammatical continuations
for both singly and doubly embedded contexts, as
in German (G1G2); in the red region, the ungram-
matical continuation is lower-cost for both con-
texts (U1U2). In the green region, the grammatical
continuation is lower cost for single embedding,
but higher cost for double embedding, as in En-
glish (G1U2). No combination of parameter values
instantiates U1G2 (for either the depicted or other
possible values ofm and d). Thus both the English
and German behavioral patterns are quite gener-
ally predicted by the model. Furthermore, each
language’s statistics place it in a region of parame-
ter space plausibly corresponding to its behavioral
pattern: the English-type forgetting effect is pre-
dicted mostly for high s, the German-type for low
s.

The only previous formalized account of
language-specific verb forgetting, Frank et al.
(2016), showed that Simple Recurrent Networks
(SRNs) trained on English and Dutch data partly
reproduce the verb forgetting effect in the sur-
prisals they assign to the final verb. Our model
provides an explanation of the SRN’s behavior.
When an SRN predicts words, it effectively uses

0.0

0.2

0.4

0.6

0.8

1.0

r

m = 0.1 m = 0.25
d = 0.1

m = 0.5

0.0 0.2 0.4 0.6 0.8 1.0
s

0.0

0.2

0.4

0.6

0.8

1.0

r

0.0 0.2 0.4 0.6 0.8 1.0
s

0.0 0.2 0.4 0.6 0.8 1.0
s

d = 0.2

Figure 2: Regions of different model behavior
with respect to parameters r, s, m, and d (see Ta-
ble 1). Blue: G1G2; red: U1U2; green: G1U2 (see
text).

a lossily compressed representation of the previ-
ous words. This lossy compression is analogous
to the noisy representation posited here.

4 Information Locality

Here we show how, given an appropriate noise dis-
tribution, noisy surprisal gives rise to locality ef-
fects. Standard locality effects are related to syn-
tactic dependencies: the claim is that processing
is difficult when the parser must make a syntac-
tic connection with an element that has been in
memory for a long time. In Section 4.1, we de-
rive a more general prediction: that processing is
difficult when any elements with high mutual in-
formation are far from one another. The effect
arises under noisy surprisal because context ele-
ments that would have been helpful for predicting
a word might have been forgotten. We call this
principle information locality. In Section 4.3, we
argue that words in syntactic dependencies have
higher mutual information than other word pairs,
which leads to a view of dependency locality ef-
fects as a special case of information locality ef-
fects.

4.1 Derivation of Information Locality

Viewing processing cost as a function of word or-
der, noisy surprisal gives rise to the generalization
that cost is minimized when elements with high
mutual information are close. We show this by
decomposing the noisy surprisal cost of a word
into many terms of higher-order mutual informa-
tion with the context, then showing that applying a
certain kind of erasure noise to the context causes

691



these terms to be downweighted based on their dis-
tance to the word. Thus the best word order puts
the words that have high mutual information with
a word close to that word.

4.1.1 Noise Distribution
Noisy surprisal gives rise to information local-
ity under a family of noise distributions which
we call progressive erasure noise, which is any
noise function that erases discrete elements of a
sequence with increasing probability the earlier
those elements are in the sequence. Formally, in
progressive erasure noise, the ith element in a se-
quence X with length |X| is erased with probabil-
ity proportional to some monotonically increasing
function of how far left that element is in the se-
quence: f(|X| − i). As a concrete example of
progressive erasure noise, consider an exponential
decay function, such that the probability that an el-
ement i in X remains unerased is (1− e)|X|−i for
some probability e. The exponential decay func-
tion corresponds to a noise model where the con-
text sequence is hit with erasure noise successively
as each word is processed. Any progressive era-
sure noise distribution suffices for the derivation
here to go through.

4.1.2 Decomposing Surprisal Cost
In noisy surprisal theory, the cost of a word wi in
context w1:i−1 is:

C(wi|w1:i−1) = E
V |w1:i−1

[− log p(wi|V )]

= E
V |w1:i−1

[h(wi)− pmi(wi;V )]

= h(wi)− E
V |w1:i−1

[pmi(wi;V )], (4)

where h(·) is surprisal (here unconditional, equiv-
alent to log inverse-frequency) and pmi(·; ·) is
pointwise mutual information between two val-
ues under a joint distribution:

pmi(x; y) = h(x) + h(y)− h(x, y). (5)

Essentially, each word has an inherent cost de-
termined by its log inverse probability, mitigated
to the extent that it is predictable from context
(pmi(wi;w1:i−1)).

Now define the interaction information be-
tween a sequence of m values {a} drawn from
a sequence of m random variables {α} (McGill,

1955; Bell, 2003) as:2

i(a1; ...; am) =

m∑
n=1

∑
I∈(1:mn )

(−1)m−n−1h(aI1 , ..., aIn),

where the notation
(
1:m
n

)
means all cardinality-n

subsets of the set of integers 1 through m. The
equation amounts to alternately adding and sub-
tracting the joint surprisals of all subsets of values.
For m = 2, expanding the equation reveals that
mutual information is a special case of interaction
information.

Supposing that the noisy representation of con-
text V is the result of running the veridical context
w1:i−1 through progressive erasure noise, we can
see V as a sequence of values v1:i−1, where each
vi is equal to either wi or the erasure symbol E.
Rewriting pmi(wi;V ) as pmi(wi; v1:i−1), we can
decompose it into interaction informations as fol-
lows:

pmi(wi; v1:i−1) =
i−1∑
n=1

∑
I∈(1:i−1n )

i(wi; vI1 ; ...; vIn). (6)

The equation expresses a sum of interaction infor-
mations between the current word wi and all sub-
sets of the context values.3

2Higher-order information terms are typically defined us-
ing a different sign convention and referred to as coinforma-
tion or multivariate mutual information (Bell, 2003). For
even orders, interaction information is equal to coinforma-
tion. For odd orders, interaction information is equal to neg-
ative coinformation. We adopt our particular sign convention
to make the generalization of information locality easier to
express.

3To see that this is true, first note that we can express joint
surprisal in terms of interaction information:

h(a1, ..., am) = −
m∑

n=1

∑
I∈(1:mn )

i(aI1 ; ...; aIn).

Now consider the pmi of a value ai with a sequence a1:i−1.
Using the decomposition of joint surprisal to expand the def-
inition of pmi in Equation 5, we get:

pmi(ai; a1:i−1) = h(ai) + h(a1:i−1)− h(ai, a1:i−1)
= h(ai) + h(a1:i−1)− h(a1:i)

= h(ai)−
i−1∑
n=1

∑
I∈(1:i−1n )

i(aI1 ; ...; aIn)

+

i∑
n=1

∑
I∈(1:in )

i(aI1 ; ...; aIn)

In the final expression, all the terms that do not contain ai

692



Now combining Equations 4 and 6, we get:

C(wi|w1:i−1) = h(wi)−

E
v|w1:i−1

 i−1∑
n=1

∑
I∈(1:i−1n )

i(wi; vI1 ; ...; vIn)


= h(wi)−

i−1∑
n=1

∑
I∈(1:i−1n )

∑
v

pN (v|wi:i−1)i(wi; vI1 ; ...; vIn).

Now if any element of an interaction information
term is E, then that whole interaction informa-
tion term is equal to 0. This happens because
the probability that an element is erased is inde-
pendent of the identity of other elements in the
sequence, and thus E has no interaction informa-
tion with any subset of those elements. That is,
i(wi; vI1 ; ...; vIn) = 0 unless vIj = wIj for all j.
This allows us to write:

C(wi|w1:i−1) = h(wi)−
i−1∑
n=1

∑
I∈(1:i−1n )

i(wi;wI1 ; ...;wIn)
∑

m∈{0,1}i−1
pN (m)mI

where the variable m ranges over bit-masks of
length i − 1, and mI is equal to 1 when all in-
dices I in m are equal to 1, and 0 otherwise.
Now

∑
m∈{0,1}i−1 pN (m)mI is the total probabil-

ity that all of a set of indices I survives erasure.
Thus, informally:

C(wi|w1:i−1) = h(wi)−
i−1∑
n=1

∑
I∈(1:i−1n )

pN (I survives)i(wi;wI1 ; ...;wIn).

(7)

That is, the cost of a word is its inherent cost minus
its interaction informations with context, which
are weighted by the probability that all elements
of those interactions survive erasure.
cancel out, leaving:

pmi(ai; a1:i−1) = h(ai) +
i−1∑
n=0

∑
I∈(1:i−1n )

i(ai; aI1 ; ...; aIn)

= h(ai) +

i−1∑
n=1

∑
I∈(1:i−1n )

i(ai; aI1 ; ...; aIn)− h(ai)

=

i−1∑
n=1

∑
I∈(1:i−1n )

i(ai; aI1 ; ...; aIn),

which gives Equation 6 when applied to wi and v1:i−1.

Under progressive erasure noise, the probabil-
ity that a subset of variables is erased increases
the farther left those variables are in the context.
Therefore, Equation 7 expresses information lo-
cality: context elements which are predictive of
wi will only get to mitigate the cost of processing
wi if they are close to it. The surprisal-mitigating
effect of a context element on a word wi decreases
as that element gets farther from wi.

4.2 Noisy Surprisal and Dependency Locality
Memory-based models of sentence processing ac-
count for apparent dependency locality effects,
which is processing cost apparently arising from
two words linked in a syntactic dependency ap-
pearing far from one another (Gibson, 1998). De-
pendency length has been proposed as a rough
measure of comprehension and production diffi-
culty, and studied as a predictor of reaction times
(Grodner and Gibson, 2005; Demberg and Keller,
2008; Mitchell et al., 2010; Shain et al., 2016), and
also as a theory of production preferences and lin-
guistic typology, under the assumption that people
prefer to produce sentences with short dependen-
cies (dependency length minimization) (Hawkins,
1994; Gildea and Temperley, 2010; Futrell et al.,
2015; Rajkumar et al., 2016).

Dependency locality follows from information
locality if words linked in a syntactic dependency
have particularly high mutual information. To see
this, consider only the lowest-order interaction in-
formation terms in Equation 7, truncating the sum-
mation over n at 1. We can write

C(wi|w1:i−1) = h(wi)−
i−1∑
j=1

f(i− j)pmi(wi;wj) +R,

where R collects all the interaction information
terms of order greater than 2, and f(d) is the
monotonically decreasing survival probability of
a d-back word, described in Section 4.1.1. The ef-
fects of R are bounded because higher-order mu-
tual information terms are more penalized by era-
sure noise than lower-order terms, simply because
large sets of context items are more likely to expe-
rience at least one erasure.

If the effects ofR are negligible, then the cost of
a whole utterance w as a function of word order is
determined only by pairwise information locality:

C(w) ≈
|w|∑
i=1

h(wi)−
|w|∑
i=2

i−1∑
j=1

f(i− j)pmi(wi;wj).

693



If words linked in a dependency have higher
mutual information than words that are not, then
the processing cost as a function of word order is a
monotonically increasing function of dependency
length. Under this assumption, for which we pro-
vide evidence below, dependency locality effects
can be seen as a special case of information local-
ity effects. As a theory of production preferences
or typology, processing cost as a monotonically in-
creasing function of dependency length suffices to
derive the predictions of dependency length mini-
mization (Ferrer i Cancho, 2015).

4.3 Mutual Information and Syntactic
Dependency

We have shown that noisy-context surprisal de-
rives information locality, and argued that depen-
dency locality can be seen as a special case of in-
formation locality. However, deriving dependency
locality requires a crucial assumption that words
linked in a dependency have higher mutual infor-
mation than those words that are not.

To test this assumption, we calculated mutual
information between wordforms in various depen-
dency relations in the Google Syntactic n-gram
corpus (Goldberg and Orwant, 2013). We com-
pared the mutual information of content words in
a direct dependency relationship to content words
in grandparent–grandchild and sister–sister depen-
dency relationships. Mutual information was esti-
mated using maximum likelihood estimation from
frequencies, treating the corpus as samples from a
distribution over (head, dependent) pairs. In order
to exclude nonlinguistic forms, we only included
wordforms if they were among the top 10000 most
frequent wordforms in the corpus. The direct
head–dependent frequencies were calculated from
the same corpus as the grandparent-grandchild fre-
quencies, so that all mutual information estimates
are affected by the same frequency cutoff. The re-
sults are shown in Table 2: direct head–dependent
pairs indeed have the highest mutual information.

To test the crosslinguistic validity of this gen-
eralization about syntactic dependency and mu-
tual information, we calculated mutual informa-
tion between the distributions over POS tags for
dependency pairs of 43 languages in the Univer-
sal Dependencies corpus (Nivre et al., 2016). For
this calculation, we used mutual information over
POS tags rather than wordforms to avoid data spar-
sity issues. The results are shown in Figure 3.

Relation MI (bits)
Head–dependent 1.79

Grandparent–dependent 1.34
Sister–sister 1.19

Table 2: Mutual information over wordforms in
different dependency relations in the Syntactic n-
gram corpus. The pairwise comparison of head–
dependent and grandparent–dependent MI is sig-
nificant at p < 0.005 by Monte Carlo permutation
tests over n-grams with 500 samples. The com-
parison of head–dependent and sister–sister MI is
not significant.

Again, we find that mutual information is high-
est for direct head–dependency pairs, and falls off
for more distant relations. These results show that
two words in a syntactic dependency relationship
are more predictive of each other than two words
in some other kinds of relationship.

We also compared the mutual information of
word pairs in and out of dependency relationships
while controlling for distance. This test has a dual
purpose. First, it allows us to control for distance
when claiming that words in dependency relation-
ships have high mutual information. Second, it
allows us to test a simple prediction of informa-
tion locality as applied to language production:
that words with high mutual information should
be close together. For pairs of words (wi, wi+k),
we calculated the pmi values among POS tags of
the words. Figure 4 shows the average pmi of
all words at each distance compared with the av-
erage pmi of the subset of words in a direct de-
pendency relationship at that distance. In all lan-
guages, we find that words in a dependency rela-
tionship have higher pmi than the baseline, espe-
cially at close distances. Furthermore, we find that
words at close distances tend to have higher pmi,
regardless of whether they are in a dependency re-
lationship.

4.4 Discussion

Information locality can be seen as a decay in
the effectiveness of contextual cues for predicting
words. Precisely such a decay in cue effectiveness
was found to be effective for predicting entropy
distributions across sentences in Qian and Jaeger
(2012), although that work did not distinguish be-
tween an inherent, noise-based decay in cue effec-
tiveness or optimized placement of cues.

694



Ancient Greek Arabic Basque Bulgarian Catalan Chinese Church Slavonic Croatian

Czech Danish Dutch English Estonian Finnish French Galician

German Gothic Hebrew Hindi Hungarian Indonesian Irish Italian

Japanese Kazakh Latin Latvian Modern Greek Norwegian (B) Persian Polish

Portuguese Romanian Russian Slovak Slovenian Spanish Swedish Tamil

Turkish Uyghur Vietnamese

0.0

0.2

0.4

0.0

0.2

0.4

0.0

0.2

0.4

0.0

0.2

0.4

0.0

0.2

0.4

0.0

0.2

0.4

M
ut

ua
l i

nf
or

m
at

io
n 

(b
its

)

Relation
Head−Dependent

Sister−Sister

Grandparent−Dependent

Figure 3: Mutual information over POS tags for dependency relations in the Universal Dependencies 1.4
corpus, for languages with over 500 sentences. All pairwise MI comparisons are significant at p < 0.005
by Monte Carlo permutation tests over dependency observations with 500 samples.

* * * * ** * * * * * * * ** * * * * * * ** * * * * * * * ** * * * * * * * * ** * * * * * * * * ** * * * * * * * * ** * * * * * * * * ** * * * *

* * * * ** * * * * * * * * ** * * * * * * * * ** * * * * * * * * ** * * * * * * * * ** * * * * * * * ** * * * * * * ** * * * * * * * ** * * * *

* * * ** * * * * * * * ** * * * * * * * ** * * * * * * * ** * * * * * * * ** * * * * * * ** * * * * * ** * * * * * * ** * * * *

* * * * ** * * * * * * ** * * * * * * ** * * * * * * * * ** * * * * * * ** * * * * * * ** * * * * * * * ** * * * * * * ** * * *

* * * * ** * * * * * * * ** * * * * * * ** * * * * * * * ** * * * * * * * * ** * * * * * * * * ** * * * * * * * ** * * * * * ** * *

* * * * ** * * * * * * * * ** * * * * * * ** * *

Ancient Greek Arabic Basque Bulgarian Catalan Chinese Church Slavonic Croatian

Czech Danish Dutch English Estonian Finnish French Galician

German Gothic Hebrew Hindi Hungarian Indonesian Irish Italian

Japanese Kazakh Latin Latvian Modern Greek Norwegian (B) Persian Polish

Portuguese Romanian Russian Slovak Slovenian Spanish Swedish Tamil

Turkish Uyghur Vietnamese

0.0
0.2
0.4
0.6
0.8

0.0
0.2
0.4
0.6
0.8

0.0
0.2
0.4
0.6
0.8

0.0
0.2
0.4
0.6
0.8

0.0
0.2
0.4
0.6
0.8

0.0
0.2
0.4
0.6
0.8

1 2 3 4 5 1 2 3 4 5 1 2 3 4 5
k (Distance in words)

pm
i(w

i; 
w

i+
k)

Head−Dependent

Baseline

Figure 4: Average pointwise mutual information over POS tags for word pairs with k words intervening,
for all words (baseline) and for words in a direct dependency relationship. Asterisks mark distances
where the difference between the baseline and words in a dependency relationship is significant at p <
0.005 by Monte Carlo permutation tests over word pair observations with 500 samples.

695



The result of Gildea and Jaeger (2015), which
shows that word orders in languages are optimized
to minimize trigram surprisal of words, can be
taken to show maximization of information local-
ity under the noise distribution where context is
truncated deterministically at length 2. Whereas
Gildea and Jaeger (2015) treat dependency length
minimization and trigram surprisal minimization
as separate factors, under the view in this paper
these two phenomena emerge as two aspects of in-
formation locality. In general, the mutual informa-
tion of linguistic elements has been found to de-
crease with distance (Li, 1989; Lin and Tegmark,
2016), although this claim has only been tested for
letters, not for larger linguistic units such as mor-
phemes. The fact that linguistic units that are close
typically have high mutual information could re-
sult from optimization of word order for informa-
tion locality.

The idea that syntactically dependent words
have high mutual information is also ubiquitously
implicit in probabilistic models of language and in
practical NLP models. For example, it is implied
by head-outward generative models (Eisner, 1996;
Eisner, 1997; Klein and Manning, 2004), the first
successful models for grammar induction. Mutual
information has been used directly for unsuper-
vised discovery of syntactic dependencies (Yuret,
1998) and evaluation of dependency parses (de
Paiva Alves, 1996), as well as commonly for col-
location detection (Church and Hanks, 1990). In
addition to providing evidence for a crucial as-
sumption in the derivation of information locality,
our results also give evidence backing up the the-
oretical validity of such models and methods.

The derivation of information locality given
here assumed progressive erasure noise for con-
creteness, but we believe it should be possible
to derive this generalization for a large family of
noise distributions.

5 Conclusion

We have introduced a computational-level model
of incremental sentence processing difficulty
based on the principle that comprehenders have
uncertainty about the previous input and act ratio-
nally on that uncertainty. Noisy-context surprisal
accounts for key effects predicted by expectation-
based and memory-based models, in addition
to providing the first computational-level expla-
nation of language-specific structural forgetting,

which involves subtle interactions between mem-
ory and probabilistic expectations. Noisy-context
surprisal also leads to a general principle of in-
formation locality offering a new interpretation of
syntactic locality effects, and leading to broader
and potentially different predictions than purely
memory-based models.

Here we have used qualitative arguments and
have used different specific noise distributions to
make different points. Our aim has been to ar-
gue for the theoretical viability of noisy-context
surprisal, without committing the theory to a par-
ticular noise distribution. We believe our predic-
tions will be derivable under very general classes
of noise distributions, and we plan to pursue these
more general derivations in future work.

A more psychologically accurate model will
likely use a more nuanced noise distribution than
the simple decay functions in this paper, which
do not capture the subtleties of human memory.
In particular, simple decay functions to not cap-
ture memory retrieval effects of the kind described
in Anderson and Schooler (1991), where different
items in a sequence have different propensities to
be forgotten, in accordance with rational alloca-
tion of resources for retrieval. Seen as a noise
distribution, this memory model implies that the
erasure probability of a word is a function of the
word’s identity, and not only the word’s position in
the sequence as in Section 4.1.1. Including such
noise distributions in the noisy-context surprisal
model could provide a rich set of predictions to
test the model more extensively.

Acknowledgments

We would like to thank members of Tedlab and
the Computational Psycholinguistics Lab at MIT
for helpful comments. R.F. was supported by NSF
grant #1551543.

References
John R. Anderson and Lael J. Schooler. 1991. Reflec-

tions of the environment in memory. Psychological
Science, 2(6):396.

Anthony J. Bell. 2003. The co-information lattice. In
Proceedings of the Fifth International Workshop on
Independent Component Analysis and Blind Signal
Separation, pages 921–926.

Kenneth Ward Church and Patrick Hanks. 1990. Word
association norms, mutual information, and lexicog-
raphy. Computational linguistics, 16(1):22–29.

696



Eduardo de Paiva Alves. 1996. The selection of the
most probable dependency structure in Japanese us-
ing mutual information. In Proceedings of the 34th
Annual Meeting of the Association for Computa-
tional Linguistics, pages 372–374.

Vera Demberg and Frank Keller. 2008. Data from eye-
tracking corpora as evidence for theories of syntactic
processing complexity. Cognition, 109(2):193–210.

Vera Demberg and Frank Keller. 2009. In Proceed-
ings of the 31st Annual Meeting of the Cognitive Sci-
ence Society, Amsterdam, The Netherlands. Cogni-
tive Science Society.

Vera Demberg, Frank Keller, and Alexander Koller.
2013. Incremental, predictive parsing with psy-
cholinguistically motivated tree-adjoining grammar.
Computational Linguistics, 39(4):1025–1066.

Jason M. Eisner. 1996. Three new probabilistic mod-
els for dependency parsing: An exploration. In Pro-
ceedings of the 16th Conference on Computational
Linguistics, pages 340–345.

Jason M. Eisner. 1997. An empirical compari-
son of probability models for dependency grammar.
Technical report, IRCS Report 96–11, University of
Pennsylvania.

Ramon Ferrer i Cancho. 2015. The placement of the
head that minimizes online memory: a complex sys-
tems approach. Language Dynamics and Change,
5(1):114–137.

Stefan L. Frank, Thijs Trompenaars, Richard L. Lewis,
and Shravan Vasishth. 2016. Cross-linguistic
differences in processing double-embedded relative
clauses: Working-memory constraints or language
statistics? Cognitive Science, 40:554–578.

Richard Futrell, Kyle Mahowald, and Edward Gibson.
2015. Large-scale evidence of dependency length
minimization in 37 languages. Proceedings of
the National Academy of Sciences, 112(33):10336–
10341.

Edward Gibson and James Thomas. 1999. Memory
limitations and structural forgetting: The perception
of complex ungrammatical sentences as grammati-
cal. Language and Cognitive Processes, 14(3):225–
248.

Edward Gibson, Steven T. Piantadosi, Kimberly Brink,
Leon Bergen, Eunice Lim, and Rebecca Saxe. 2013.
A noisy-channel account of crosslinguistic word-
order variation. Psychological science, 24(7):1079–
1088.

Edward Gibson. 1998. Linguistic complexity: Local-
ity of syntactic dependencies. Cognition, 68(1):1–
76.

Daniel Gildea and T. Florian Jaeger. 2015. Hu-
man languages order information efficiently. arXiv,
abs/1510.02823.

Daniel Gildea and David Temperley. 2010. Do gram-
mars minimize dependency length? Cognitive Sci-
ence, 34(2):286–310.

Yoav Goldberg and Jon Orwant. 2013. A dataset of
syntactic-ngrams over time from a very large cor-
pus of english books. In Second Joint Conference
on Lexical and Computational Semantics, volume 1,
pages 241–247.

Daniel Grodner and Edward Gibson. 2005. Con-
sequences of the serial nature of linguistic in-
put for sentential complexity. Cognitive Science,
29(2):261–290.

John Hale. 2001. A probabilistic Earley parser as a
psycholinguistic model. In Proceedings of the Sec-
ond Meeting of the North American Chapter of the
Association for Computational Linguistics and Lan-
guage Technologies, pages 1–8.

John A. Hawkins. 1994. A performance theory of or-
der and constituency. Cambridge University Press,
Cambridge.

Dan Klein and Christopher D. Manning. 2004.
Corpus-based induction of syntactic structure: Mod-
els of dependency and constituency. In Proceedings
of the 42nd Annual Meeting of the Association for
Computational Linguistics, page 478.

Roger Levy. 2008a. Expectation-based syntactic com-
prehension. Cognition, 106(3):1126–1177.

Roger Levy. 2008b. A noisy-channel model of ratio-
nal human sentence comprehension under uncertain
input. In Proceedings of the Conference on Empiri-
cal Methods in Natural Language Processing, pages
234–243.

Roger Levy. 2011. Integrating surprisal and uncertain-
input models in online sentence comprehension: for-
mal techniques and empirical results. In ACL, pages
1055–1065.

Richard L. Lewis and Shravan Vasishth. 2005.
An activation-based model of sentence processing
as skilled memory retrieval. Cognitive Science,
29(3):375–419.

Wentian Li. 1989. Mutual information functions of
natural language texts. Technical report, Santa Fe
Institute Working Paper #1989-10-008.

Henry W. Lin and Max Tegmark. 2016. Critical be-
havior from deep dynamics: A hidden dimension in
natural language. arXiv, abs/1606.06737.

David Marr. 1982. Vision: A Computational Investiga-
tion into the Human Representation and Processing
of Visual Information. W.H. Freeman & Company.

William J. McGill. 1955. Multivariate information
transmission. IEEE Transactions on Information
Theory, 4(4):93–111.

697



Jeff Mitchell, Mirella Lapata, Vera Demberg, and
Frank Keller. 2010. Syntactic and semantic factors
in processing difficulty: An integrated measure. In
Proceedings of the 48th Annual Meeting of the As-
sociation for Computational Linguistics, pages 196–
206.

Joakim Nivre, Željko Agić, Lars Ahrenberg, Maria Je-
sus Aranzabe, Masayuki Asahara, Aitziber Atutxa,
Miguel Ballesteros, John Bauer, Kepa Ben-
goetxea, Yevgeni Berzak, Riyaz Ahmad Bhat, Eck-
hard Bick, Carl Börstell, Cristina Bosco, Gosse
Bouma, Sam Bowman, Gülşen Cebirolu Eryiit,
Giuseppe G. A. Celano, Fabricio Chalub, Çar
Çöltekin, Miriam Connor, Elizabeth Davidson,
Marie-Catherine de Marneffe, Arantza Diaz de
Ilarraza, Kaja Dobrovoljc, Timothy Dozat, Kira
Droganova, Puneet Dwivedi, Marhaba Eli, Tomaž
Erjavec, Richárd Farkas, Jennifer Foster, Claudia
Freitas, Katarı́na Gajdošová, Daniel Galbraith, Mar-
cos Garcia, Moa Gärdenfors, Sebastian Garza, Filip
Ginter, Iakes Goenaga, Koldo Gojenola, Memduh
Gökrmak, Yoav Goldberg, Xavier Gómez Guino-
vart, Berta Gonzáles Saavedra, Matias Grioni, Nor-
munds Grūzītis, Bruno Guillaume, Jan Hajič, Linh
Hà M, Dag Haug, Barbora Hladká, Radu Ion,
Elena Irimia, Anders Johannsen, Fredrik Jørgensen,
Hüner Kaşkara, Hiroshi Kanayama, Jenna Kanerva,
Boris Katz, Jessica Kenney, Natalia Kotsyba, Si-
mon Krek, Veronika Laippala, Lucia Lam, Phng
Lê Hng, Alessandro Lenci, Nikola Ljubešić, Olga
Lyashevskaya, Teresa Lynn, Aibek Makazhanov,
Christopher Manning, Cătălina Mărănduc, David
Mareček, Héctor Martı́nez Alonso, André Martins,
Jan Mašek, Yuji Matsumoto, Ryan McDonald, Anna
Missilä, Verginica Mititelu, Yusuke Miyao, Simon-
etta Montemagni, Keiko Sophie Mori, Shunsuke
Mori, Bohdan Moskalevskyi, Kadri Muischnek,
Nina Mustafina, Kaili Müürisep, Lng Nguyn Th,
Huyn Nguyn Th Minh, Vitaly Nikolaev, Hanna
Nurmi, Petya Osenova, Robert Östling, Lilja Øvre-
lid, Valeria Paiva, Elena Pascual, Marco Passarotti,
Cenel-Augusto Perez, Slav Petrov, Jussi Piitulainen,
Barbara Plank, Martin Popel, Lauma Pretkalnia,
Prokopis Prokopidis, Tiina Puolakainen, Sampo
Pyysalo, Alexandre Rademaker, Loganathan Ra-
masamy, Livy Real, Laura Rituma, Rudolf Rosa,
Shadi Saleh, Baiba Saulīte, Sebastian Schuster,
Wolfgang Seeker, Mojgan Seraji, Lena Shakurova,
Mo Shen, Natalia Silveira, Maria Simi, Radu
Simionescu, Katalin Simkó, Mária Šimková, Kiril
Simov, Aaron Smith, Carolyn Spadine, Alane Suhr,
Umut Sulubacak, Zsolt Szántó, Takaaki Tanaka,
Reut Tsarfaty, Francis Tyers, Sumire Uematsu,
Larraitz Uria, Gertjan van Noord, Viktor Varga,
Veronika Vincze, Lars Wallin, Jing Xian Wang,
Jonathan North Washington, Mats Wirén, Zdeněk
Žabokrtský, Amir Zeldes, Daniel Zeman, and
Hanzhi Zhu. 2016. Universal dependencies 1.4.
LINDAT/CLARIN digital library at the Institute of
Formal and Applied Linguistics, Charles University
in Prague.

Ting Qian and T. Florian Jaeger. 2012. Cue effective-

ness in communicatively efficient discourse produc-
tion. Cognitive Science, 36:1312–1336.

Rajakrishnan Rajkumar, Marten van Schijndel,
Michael White, and William Schuler. 2016. In-
vestigating locality effects and surprisal in written
English syntactic choice phenomena. Cognition,
155:204–232.

Douglas Roland, Frederic Dick, and Jeffrey L. El-
man. 2007. Frequency of basic English grammat-
ical structures: A corpus analysis. Journal of Mem-
ory and Language, 57(3):348–379.

Cory Shain, Marten van Schijndel, Richard Futrell, Ed-
ward Gibson, and William Schuler. 2016. Mem-
ory access during incremental sentence processing
causes reading time latency. In Proceedings of
the Workshop on Computational Linguistics for Lin-
guistic Complexity (CL4LC), pages 49–58, Osaka,
Japan.

Nathaniel J. Smith and Roger Levy. 2013. The effect
of word predictability on reading time is logarith-
mic. Cognition, 128(3):302–319.

Shravan Vasishth, Katja Suckow, Richard L. Lewis,
and Sabine Kern. 2010. Short-term forgetting in
sentence comprehension: Crosslinguistic evidence
from verb-final structures. Language and Cognitive
Processes, 25(4):533–567.

Deniz Yuret. 1998. Discovery of linguistic rela-
tions using lexical attraction. arXiv preprint cmp-
lg/9805009.

698


