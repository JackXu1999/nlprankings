Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 206–214,

Beijing, August 2010

206

Simplicity is Better: Revisiting Single Kernel PPI Extraction 

Sung-Pil Choi 

Sung-Hyon Myaeng 

Information Technology Laboratory 

Department of Computer Science 

Korea Institute of Science and Technol-

Korea Advanced Institute of Science and 

ogy Information 

Technology 

spchoi@kisti.re.kr 

myaeng@kaist.ac.kr 

 

Abstract 

It has been known that a combination of 
multiple kernels and addition of various 
resources  are  the  best  options  for  im-
proving  effectiveness  of  kernel-based 
PPI  extraction  methods.  These  supple-
ments,  however,  involve  extensive  ker-
nel  adaptation  and  feature  selection 
processes,  which  attenuate  the  original 
benefits of the kernel methods. This pa-
per  shows  that  we  are  able  to  achieve 
the  best  performance  among  the  state-
of-the-art  methods  by  using  only  a  sin-
gle kernel, convolution parse tree kernel. 
In-depth  analyses  of  the  kernel  reveal 
that the keys to the improvement are the 
tree  pruning  method  and  consideration 
of  tree  kernel  decay  factors.  It  is  note-
worthy  that  we  obtained  the  perfor-
mance  without  having  to  use  any  addi-
tional features, kernels or corpora. 

1 

Introduction 

Protein-Protein  Interaction  (PPI)  Extraction 
refers to an automatic extraction of the interac-
tions between multiple protein names from nat-
ural language sentences using linguistic features 
such as lexical clues and syntactic structures. A 
sentence  may  contain  multiple  protein  names 
and  relations,  i.e.,  multiple  PPIs.  For  example, 
the sentence in Fig.1 contains a total of six pro-
tein  names  of  varying  word  lengths  and  three 
explicit  interactions  (relations).  The  interaction 
type  between  phosphoprotein  and  the  acronym 
P in the parentheses is “EQUAL.” A longer pro-
tein name phosphoprotein of vesicular stomati-
tis  virus  is  related  to  nucleocapsid  protein  via 
“INTERACT”  relation.  Like  the  first  PPI,  nuc-

leocapsid  protein  is  equivalent  to  the  abbre-
viated term N.  

It is not straightforward to extract PPIs from 
a  sentence  or  textual  segment.  There  may  be 
multiple  protein  names  and  their  relationships, 
which are intertwined in a sentence. An interac-
tion type may be expressed in a number of dif-
ferent ways.  

 

Figure 1. An example sentence containing mul-
tiple PPIs involving different  names of varying 
scopes and relations1  
 

A  significant  amount  of  efforts  have  been 
devoted  to  kernel-based  approaches  to  PPI  ex-
tractions (PPIE) as well as relation extractions2 
(Zhang  et  al.,  2006;  Pyysalo  et al.,  2008;  Guo-
Dong et al., 2007; Zhang et al., 2008; Airola et 
al.,  2008;  Miwa  et  al.,  2009).  They  include 
word  feature  kernels,  parse  tree  kernels,  and 
graph  kernels.  One  of  the  benefits  of  using  a 
kernel  method  is  that  it  can  keep  the  original 

                                                 
1 BioInfer, Sentence ID:BioInfer.d10.s0 
2 Relation extraction has been studied massively with the 
help of the ACE (www.nist.gov/tac) competition work-
shop and its corpora. The ACE corpora contain valuable 
information showing the traits of target entities (e.g., ent-
ity types, roles) for relation extraction in single sentences. 
Since all target entities are of the same type, protein 
name, in PPIE, however, we cannot use relational infor-
mation that exists among entity types. This makes PPIE 
more challenging.  

207

formation  of  target  objects  such  as  parse  trees, 
not  requiring  extensive  feature  engineering  for 
learning algorithms (Zelenko et al., 2003).  

In  an  effort  to  improve  the  performance  of 
PPIE, researchers have developed not only new 
kernels  but  also  methods  for  combining  them 
(GuoDong et al., 2007; Zhang et al., 2008; Air-
ola et al., 2008; Miwa et al., 2009a; Miwa et al., 
2009b).  While  the  intricate  ways  of  combing 
various  kernels  and  using  extra  resources  have 
played  the  role  of  establishing  strong  baseline 
performance  for  PPIE,  however, 
they  are 
viewed  as  another  form  of  engineering  efforts. 
After all, one of the reasons the kernel methods 
have become popular is to avoid such engineer-
ing efforts. 

Instead, we focus on a state-of-the-art kernel 
and  investigate  how  it  can  be  best  utilized  for 
enhanced performance. We show that even with 
a single kernel, convolution parse tree kernel in 
this case, we can achieve superior performance 
in PPIE by devising an appropriate preprocess-
ing  and  factor  adjustment  method.  The  keys  to 
the improvement are tree pruning and consider-
ation  of  a  tree  kernel  decay  factor,  which  are 
independent  of  the  machine  learning  model 
used in this paper. The main contribution of our 
work  is  the  extension  and  application  of  the 
particular  convolution  tree  kernel  method  for 
PPIE, which gives a lesson that a deep analysis 
and a subsequent extension of a kernel for max-
imal  performance  can  override  the  gains  ob-
tained  from  engineering  additional  features  or 
combining other kernels. 

The remaining part of the paper is organized 
as follows. In section 2, we survey the existing 
approaches.  Section  3 introduces  the  parse tree 
kernel  model  and  its  algorithm.  Section  4  ex-
plains  the  performance  improving  factors  ap-
plied  to  the  parse  tree  kernel.  The  architecture 
of our system is introduced in section 5. Section 
6  shows  the  improvements  in  effectiveness  in 
multiple  PPI  corpora  and  finally  we  conclude 
our work in section 7. 

2  Related Work 

In  recent  years,  numerous  studies  have  at-
tempted to extract PPI automatically from  text. 
Zhou  and  He  (2008)  classified  various  PPIE 
approaches  into  three  categories:  linguistic, 

rule-based  and  machine  learning  and  statistical 
methods. 

Linguistic  approaches  involve  constructing 
special  grammars  capable  of  syntactically  ex-
pressing  the  interactions  in  sentences  and  then 
applying them to the language analyzers such as 
part-of-speech  taggers,  chunkers  and  parsers to 
extract  PPIs.  Based  on  the  level  of  linguistic 
analyses, we can divide the linguistic approach-
es  into  two  categories:  shallow  parsing  (Seki-
mizu  et  al.,  1998;  Gondy  et  al.,  2003)  and  full 
parsing  methods  (Temkin  &  Gilder,  2003;  Ni-
kolai et al., 2004). 

Rule-based approaches use manually defined 
sets  of  lexical  patterns  and  find  text  segments 
that  match  the  patterns.  Blaschke  et  al.  (1996) 
built a  set of lexical rules based  on clue  words 
denoting interactions. Ono et al. (2001) defined 
a group of lexical and syntactic interaction pat-
terns,  embracing  negative  expressions,  and  ap-
plied  them  to  extract  PPIs  from  documents 
about “Saccharomyces cerevisiae” and “Esche-
richia coli”. Recently, Fundel et al. (2007) pro-
posed  a  PPI  extraction  model  based  on  more 
systematic rules using a dependency parser.  

Machine  learning  and  statistical  approaches 
have been around for a while but have recently 
become a dominant approach for PPI extraction. 
These  methods  involve  building  supervised  or 
semi-supervised  models  based  on  training  sets 
and  various  feature  extraction  methods  (An-
drade  & Valencia, 1998;  Marcotte et  al.,  2001; 
Craven  &  Kumlien,  1999).  Among  them,  ker-
nel-based methods have been studied extensive-
ly in recent years. Airola et al. (2008) attempted 
to extract PPIs using a graph kernel by convert-
ing dependency parse trees into the correspond-
ing dependency graphs.  

Miwa et al. (2009a) utilized multiple kernels 
such as word feature kernels, parse tree kernels, 
and even graph kernels in order to improve the 
performance  of  PPI  extraction.  Their  experi-
ments  based  on  five  PPI  corpora,  however, 
showed  that  combining  multiple  kernels  gave 
only  minor  improvements  compared  to  other 
methods.  To  further  improve  the  performance 
of  the  multiple  kernel  system,  the  same  group 
combined multiple corpora to exploit additional 
features  for  a  modified  SVM  model  (Miwa  et 
al.,  2009b).  While  they  achieved  the  best  per-
formance in PPI extraction, it was possible only 

208

with additional kernels and corpora from which 
additional features were extracted.  

parse trees rooted by n1 and n2. Figure 2 shows 
the algorithm. 

Unlike the aforementioned approaches trying 
to  use  all  possible  resources  for  performance 
enhancement, this paper aims at maximizing the 
performance of PPIE using only a single kernel 
without  any  additional  resources.  Without  lo-
wering  the  performance,  we  attempt to  stick  to 
the  initial  benefits  of  the  kernel  methods:  sim-
plicity  and  modularity  (Shawe-Taylor  &  Cris-
tianini, 2004).  

3  Convolution  Parse  Tree  Kernel 

Model for PPIE 

The  main  idea  of  a  convolution  parse  tree  ker-
nel is to sever a parse tree into its sub-trees and 
transfer it as a point in a vector space in which 
each  axis  denotes  a  particular  sub-tree  in  the 
entire  set  of  parse  trees.  If  this  set  contains  M 
unique  sub-trees,  the  vector  space  becomes  M-
dimensional.  The  similarity  between  two  parse 
trees  can  be  obtained  by  computing  the  inner 
product  of  the  two  corresponding  vectors, 
which is the output of the parse tree kernel. 

There  are  two  types  of  parse  tree  kernels  of 
different  forms  of  sub-trees:  one  is  SubTree 
Kernel  (STK)  proposed  by  Vishwanathan  and 
Smola  (2003),  and  the  other  is  SubSet  Tree 
Kernel (SSTK) developed by Collins and Duffy 
(2001). In STK, each sub-tree should be a com-
plete tree rooted by a specific node in the entire 
tree and ended with leaf nodes. All the sub-trees 
must obey the production rules of the syntactic 
grammar.  Meanwhile,  SSTK  can  have  any 
forms of sub-trees in the entire parse tree given 
that  they  should  obey  the  production  rules.  It 
was  shown  that  SSTK  is  much  superior  to  STK 
in many tasks (Moschitti, 2006). He also intro-
duced  a  fast  algorithm  for  computing  a  parse 
tree kernel and showed its beneficial effects on 
the semantic role labeling problem.  

A  parse  tree  kernel  can  be  computed  by  the 

 

      

               

following equation: 
                 
                  
      
where Ti is ith parse tree and n1 and n2 are nodes 
in  NT,  the  set  of  the  entire  nodes  of  T.  λ 
represents a tree kernel decay factor, which will 
be  explained  later,  and  σ  decides  the  way  the 
tree is severed. Finally Δ(n1, n2, λ, σ) counts the 
number  of  the  common  sub-trees  of  the  two 

   (1) 

In  this  algorithm,  the  get_children_number 
function  returns  the  number  of  the  direct  child 
nodes of the current node in a tree. The function 
named  get_node_value  gives  the  value  of  a 
node  such  as  part-of-speeches,  phrase  tags  and 
words.  The  get_production_rule  function  finds 
the grammatical rule of the current node and its 
children by inspecting their relationship. 

 
1 
2 
3 
4 
5 
6 
7 
8 
9 
10 
11 
12 
13 
14 
15 
16 
17 
18 
19 
20 
21 
22 
23 
24 
25 
26 
27 
28 
29 

FUNCTION delta(TreeNode n1, TreeNode n2, λ, σ) 
n1 = one node of T1;  n2 = one node of T2; 
λ = tree kernel decay factor;  σ = tree division me-
thod; 
BEGIN 

nc1 = get_children_number(n1);   
nc2 = get_children_number(n2); 
IF nc1 EQUAL 0 AND nc2 EQUAL 0 THEN     

nv1 = get_node_value(n1);   
nv2 = get_node_value(n2);  
IF nv1 EQUAL nv2 THEN RETURN 1; 

ENDIF 
np1 = get_production_rule(n1);   
np2 = get_production_rule(n2); 
IF np1 NOT EQUAL np2 THEN RETURN 0; 

 

IF np1 EQUAL np2 AND nc1 EQUAL 1  

AND nc2 EQUAL 1 THEN 

        RETURN λ; 

END IF 

 

mult_delta = 1; 
FOR I = 1 TO nc1 

nch1 = Ith child of n1;   nch2 = Ith child of n2; 
mult_delta = mult_delta ×  

(σ + delta(nch1, nch2, λ, σ)); 

END FOR 
RETURN λ × mult_delta; 

END 

Figure 2. Δ (n1, n2, λ, σ) algorithm 

4  Performance Improving Factors 

4.1  Tree Pruning Methods 

Tree  pruning  for  relation  extraction  was  firstly 
introduced  by  Zhang  et  al.  (2006)  and  also  re-
ferred  to  as  “tree  shrinking  task”  for  removing 
less related contexts. They suggested five types 
of  the  pruning  methods  and  later  invented  two 
more  in  Zhang  et  al.  (2008).  Among  them,  the 
path-enclosed  tree  (PT)  method  was  shown  to 
give  the  best  result  in  the  relation  extraction 
task  based  on  ACE  corpus.  We  opted  for  this 
pruning method in our work.  

209

Figure 4. The effect of decaying in comparing two trees. n(·) denotes #unique subtrees in a tree. 

Figure 3 shows how the PT method prunes a 
tree.  To  focus  on  the  pivotal  context,  it  pre-
serves  only  the  syntactic  structure  encompass-
ing  the  two  proteins  at  hand  and  the  words  in 
between  them  (the  part  enclosed  by  the  dotted 
lines). Without pruning, all the words like addi-
tion,  increased  and  activity  would  intricately 
participate  in  deciding  the  interaction  type  of 
this sentence. 

 

Figure 3. Path-enclosed Tree (PT) Method 
 
Another  important  effect  of  the  tree  pruning 
is  its  ability  to  separate  features  when  two  or 
more interactions exist in a sentence. As in Fig-
ure 1, each interaction involves its unique con-
text  even  though  a  sentence  has  multiple  inte-
ractions. With tree pruning, it is likely to extract 
context-sensitive  features  by  ignoring  external 
features. 

4.2  Tree Kernel Decay Factor 

Collins  and  Duffy  (2001)  addressed  two  prob-
lems  of  the  parse  tree  kernel.  The  first  one  is 
that  its  kernel  value  tends  to  be  largely  domi-
nated by the size of two input trees. If they are 
large in size, it is highly probable for the kernel 
to  accumulate  a  large  number  of  overlapping 
counts  in  computing  their  similarity.  Secondly, 
the kernel value of two identical parse trees can 

become overly large while the value of two dif-
ferent parse trees is much tiny in general. These 
two  aspects  can  cause  a  trouble  during  a  train-
ing phase because pairs of large parse trees that 
are  similar  to  each  other  are  disproportionately 
dominant.  Consequently,  the  resulting  models 
could act like nearest neighbor models (Collins 
and Duffy, 2001). 

To alleviate the problems, Collins and Duffy 
(2001) introduced a scalability parameter called 
decay factor, 0 < λ ≤ 1 which scales the relative 
importance of tree fragments with their sizes as 
in  line  33  of  Fig.  2.  Based  on  the  algorithm,  a 
decay  factor  decreases  the  degree  of  contribu-
tion  of  a  large  sub-tree  exponentially  in  kernel 
computation. Figure 4 illustrates both the way a 
tree  kernel  is  computed  and  the  effect  of  a  de-
cay  factor.  In  the  figure,  T1  and  T2  share  four 
common sub-trees (S1, S2, S3, S5). Let us assume 
that there are only two trees in a training set and 
only five unique sub-trees exist. Then each tree 
can  be  expressed  by  a  vector  whose  elements 
are  the  number  of  particular  sub-trees.  Kernel 
value is obtained by computing the inner prod-
uct of the two vectors. As shown in the figure, 
S1 is a large sub-sub-trees, S1, S2 S3, and S4, two 
of which (S2, and S3) are duplicated in the inner 
product  computation.  It  is  highly  probable  for 
large  sub-trees  to  contain  many  smaller  sub-
trees, which lead to an over-estimated similarity 
value  between  two  parse  trees.  As  mentioned 
above,  therefore,  it  is  necessary  to  rein  those 
large  sub-trees  with  respect  to  their  sizes  in 
computing kernel values by using decay factors. 
In this paper, we treat the decay factor as one of 
the important optimization parameters for a PPI 
extraction task. 

210

5  Experimental Results 

In  order  to  show  the  superiority  of  the  simple 
kernel based method using the two factors used 
in this paper, compared to the resent results for 
PPIE using additional resources, we ran a series 
of  experiments  using  the  same  PPI  corpora 
cited in the literature. In addition, we show that 
the method is robust especially for cross-corpus 
experiments  where  a  classifier  is  trained  and 
tested with entirely different corpora.  

5.1  Evaluation Corpora 

To  evaluate  our  approach  for  PPIE,  we  used 
“Five PPI Corpora3” organized by Pyysalo et al. 
(2008).  It  contains  five  different  PPI  corpora: 
AImed,  BioInfer,  HPRD50,  IEPA  and  LLL. 
They  have  been  combined  in  a  unified  XML 
format  and  “binarized”  in  case  of  involving 
multiple interaction types.  

AIMed  BioInfer  HPRD50 

IEPA  LLL 

 

#Sentence  1,955 

1,100 

#Positive  

1,000 

2,534 

#Negative   4,834 

7,132 

145 

163 

270 

486 

335 

482 

77 

164 

166 

Table 1. Five PPI Corpora 

 
Table  1  shows  the  size  of  each  corpus  in 
“Five  PPI  Corpora.”  As  mentioned  before,  a 
sentence  can  have  multiple  interactions,  which 
results  in  the  gaps  between  the number  of sen-
tences and the sum of the number of instances. 
Negative  instances  have  been  automatically 
generated  by  enumerating  sentences  with  mul-
tiple  proteins  but  not  having  interactions  be-
tween them (Pyysalo et al., 2008).  

5.2  Evaluation Settings 

In order to parse each sentence, we used  Char-
niak  Parser4.  For  kernel-based  learning,  we ex-
panded the original libsvm 2.895 (Chang & Lin, 
2001)  so  that  it  has  two  additional  kernels  in-
cluding parse tree kernel and composite kernel6 
along with four built-in kernels7 

Our  experiment  uses  both  macro-averaged 
and  micro-averaged  F-scores. Macro-averaging 
                                                 
3 http://mars.cs.utu.fi/PPICorpora/eval-standard.html 
4 http://www.cs.brown.edu/people/ec/#software 
5 http://www.csie.ntu.edu.tw/~cjlin/libsvm/ 
6 A kernel combining built-in kernels and parse tree kernel 
7 Linear, polynomial, radial basis function, sigmoid ker-
nels 

computes  F-scores  for  all  the  classes  indivi-
dually  and  takes  average  of  the  scores.  On  the 
other  hand,  micro-averaging  enumerates  both 
positive  results  and  negative  results  on  the 
whole  without  considering  the  score  of  each 
class and computes total F-score.  

In  10-fold  cross  validation,  we  apply  the 
same split used in Airola et al., (2008), Miwa et 
al.,  (2009a)  and  Miwa  et  al.,  (2009b)  for  com-
parisons.  Also,  we  empirically  estimate  the  re-
gularization  parameters  of  SVM  (C-values)  by 
conducting  10-fold  cross  validation  on  each 
training  data.  We  do  not  adjust  the  SVM  thre-
sholds  to  the  optimal  value  as  in  Airola  et  al., 
(2008) and Miwa et al., (2009a).  

5.3  PPI Extraction Performance 

Table  2  shows  the  best  scores  of  our  system. 
The optimal  decay  factor varies  with  each  cor-
pus.  In  LLL,  the  optimal  decay  factor  is  0.28 
indicating  that  the  shortage  of  data  has  forced 
our system to normalize parse trees more inten-
sively with a strong decay factor in kernel com-
putation  in  order  to  cover  various  syntactic 
structures.  

DF 

AC 

ma-P 

ma-R 

ma-F 

 

 

A 

B 

0.6 

83.6 

0.5 

79.8 

H 

0.7 

74.5 

I 

L 

0.6 

74.2 

0.2 

82.2 

72.8 
(55.0) 
74.5 
(65.7) 
75.3 
(68.5) 
74.1 
(67.5) 
83.2 
(77.6) 

62.1 
(68.8) 
70.9 
(71.1) 
71.0 
(76.1) 
72.2 
(78.6) 
81.2 
(86.0) 

67.0 
(60.8) 
72.6 
(68.1) 
73.1 
(70.9) 
73.1 
(71.7) 
82.1 
(80.1) 

σma-F 

4.5 
(6.6) 
2.7 
(3.2) 
10.2 
(10.3) 

6.0 
(7.8) 
10.4 
(14.1) 

 
Table  2.  The  highest  results  of  the  proposed 
system  w.r.t.  decay  factors.  DF:  Decay  Factor, 
AC:  accuracy,  ma-F:  macro-averaged  F1,  σma-F: 
standard deviation of F-scores in CV. A:AIMed, 
B:BioInfer,  H:HPRD50,  I:IEPA,  L:LLL.  The 
numbers  in  parentheses  refer  to  the  scores  of 
Miwa et al., (2009a).  

 
Our  system  outperforms  the  previous  results 
as  in  Table  2.  Even  using  rich  feature  vectors 
including Bag-Of-Words and shortest path trees 

                                                 
8 It was determined by increasing it by 0.1 progressively 
through 10-fold cross validation. 

211

generated  from  multiple  corpora,  Miwa  et  al., 
(2009b)  reported  64.0%  and  66.7%  in  AIMed 
and BioInfer, respectively. Our system, howev-
er,  produced  67.0%  in  AIMed  and  72.6%  in 
BioInfer with a single parse tree kernel. We did 
not  have  to  perform  any  intensive  feature  gen-
eration  tasks  using  various  linguistic  analyzers 
and more importantly, did not use any addition-
al  corpora  for  training  as  done  in  Miwa  et  al., 
(2009b). While the performance differences are 
not  very  big,  we  argue  that  obtaining  higher 
performance  values  is  significant  because  the 
proposed  system  did  not  use  any  of  the  addi-
tional efforts and resources.  

To  investigate  the  effect  of  the  scaling  para-
meter of the parse tree kernel in PPI extraction, 
we  measure  how  the  performance  changes  as 
the decay factor varies (Figure 5). It is obvious 
that the decay factor influences the overall per-
formance  of  PPI  extraction.  Especially,  the  F-
scores  of  the  small-scale  corpora  such  as 
HPRD50 and LLL are  influenced by the decay 
factor.  The  gaps  between  the  best  and  worst 
scores  in  LLL  and  HPRD50  are  19.1%  and 
5.2%,  respectively.  The  fluctuation  in  F-scores 
of  the  large-scale  corpora  (AIMed,  BioInfer, 
IEPA)  is  not  so  extreme,  which  seems  to  stem 
from  the  abundance  in  syntactic  and  lexical 
forms  that  reduce the  normalizing  effect  of the 
decay  factor.  The  increase  in  the  decay  factor 
leads  to  the  increase  in  the  precision  values  of 
all  the  corpora  except  for  LLL.  The  phenome-
non  is  fairly  plausible  because  the  decreased 
normalization power causes the system to com-
pute  the  tree  similarities  more  intensively  and 
therefore  it  classifies  each  instance  in  a  strict 
and detailed manner. On the contrary, the recall 
values  slightly  decrease  with  respect to  the  de-
cay factor, which indicates that the tree pruning 
(PT)  has  already  conducted  the  normalization 
process  to  reduce  the  sparseness  problem  in 
each corpus. 

Most  importantly,  along  with  tree  pruning, 
decay factor could boost the performance of our 
system by controlling the rigidness of the parse 
tree kernel in PPI extraction. 

Table 3 shows the results of the cross-corpus 
evaluation to measure the generalization power 
of  our  system  as  conducted  in  Airola  et  al., 
(2008)  and  Miwa  et  al.,  (2009a).  Miwa  et  al., 
(2009b)  executed  a  set  of  combinatorial  expe-
riments  by  mixing  multiple  corpora  and  pre-

sented their results. Therefore, it is not reasona-
ble to compare our results with them due to the 
size  discrepancy  between 
training  corpora. 
Nevertheless, we will compare our results with 
their  approaches  in  later  based  on  AIMed  cor-
pus. 

As  seen  in  Table  3,  our  system  outperforms 
the  existing  approaches  in  almost  all  pairs  of 
corpora.  In  particular,  in  the  multiple  corpora-
based  evaluations  aimed  at  AIMed  which  has 
been  frequently  used  as  a  standard  set  in  PPI 
extraction,  our  approach  shows  prominent  re-
sults  compared  with  others.  While  other  ap-
proaches showed the performance ranging from 
33.3%  to  60.8%,  our  approach  achieved  much 
higher scores between 55.9% and 67.0%. More 
specific observations are: 
(1) Our PPIE method trained on any corpus ex-
cept for IEPA outperforms the other approaches 
regardless  of  the  test  corpus  only  with  a  few 
exceptions with IEPA and LLL. 
(2)  Even  when  using  LLL  or  HPRD50,  two 
smallest  corpora,  as  training  sets,  our  system 
performs well with every other corpus for test-
ing.  It  indicates  that  our  approach is  much  less 
vulnerable to  the  sizes  of training  corpora  than 
other methods. 
(3)  The  degree  of  score  fluctuation  of  our  sys-
tem  across  different  testing  corpora  is  much 
smaller than other regardless of the training da-
ta  set.  When  trained  on  LLL,  for  example,  the 
range for our system (55.9% ~ 82.1%) is small-
er than the others (38.6% ~ 83.2% and 33.3% ~ 
76.8%). 
(4) The cross-corpus evaluation reveals that our 
method  outperforms  the  others  significantly. 
This is more visibly shown especially when the 
large-scale  corpora  (AIMed  and  BioInfer)  are 
used.  
(5)  PPI  extraction  model  trained  on  AIMed 
shows  lower  scores  in  IEPA  and  LLL  as  com-
pared  with  other  methods,  which  could  trigger 
further investigation. 

In order to convince ourselves further the su-
periority  of  the  proposed  method,  we  compare 
it  with  other  previously  reported  approaches.  
Table  4  lists  the  macro-averaged  precision,  re-
call and  F-scores of the nine approaches tested 
on AIMed. While the experimental settings are 
different  as  reported  in  the  literature,  they  are 
quite  close  in  terms  of  the  numbers  of  positive 
and negative documents. 

212

Figure 5. Performance variation with respect to decay factor in Five PPI Corpora. Macro-
averaged F1 (left), Precision (middle), Recall (right) evaluated by 10-fold CV 

   

 

Training 
corpora 

AIMed 

BioInfer 

HPRD50 

IEPA 

LLL 

Systems 

Our System 
(Miwa et al., 2009a) 
(Airola et al., 2008) 
Our System 
(Miwa et al., 2009a) 
(Airola et al., 2008) 
Our System 
(Miwa et al., 2009a) 
(Airola et al., 2008) 
Our System 
(Miwa et al., 2009a) 
(Airola et al., 2008) 
Our System 
(Miwa et al., 2009a) 
(Airola et al., 2008) 

F-Scores in the test corpora 

AIMed 
67.0  
60.8  
56.4  
65.2  
49.6  
47.2  
63.1  
43.9  
42.2  
57.8  
40.4  
39.1  
55.9  
38.6  
33.3  

BioInfer 

HPRD50 

64.2  
53.1  
47.1  
72.6  
68.1  
61.3  
65.5  
48.6  
42.5  
66.1  
55.8  
51.7  
64.4  
48.9  
42.5  

72.9  
68.3  
69.0  
71.9  
68.3  
63.9  
73.1  
70.9  
63.4  
66.3  
66.5  
67.5  
69.4  
64.0  
59.8  

IEPA 
59.0  
68.1  
67.4  
72.9  
71.4  
68.0  
69.3  
67.8  
65.1  
73.1  
71.7  
75.1  
71.4  
65.6  
64.9  

LLL 
62.7  
73.5  
74.5  
78.4  
76.9  
78.0  
73.7  
72.2  
67.9  
78.4  
83.2  
77.6  
82.1  
83.2  
76.8  

Table  3. Macro-averaged  F1  scores in  cross-corpora evaluation.  Rows and  columns  correspond to 
the  training  and  test  corpora,  respectively.  We  parallel  our  results  with  other  recently  reported  re-
sults. All the split methods in 10-fold CV are the same for fair comparisons. 

As seen in the table, the proposed method is 
superior  to  all  the  others  in  F-scores.  The  im-
provement in precision (12.8%) is most signifi-
cant, especially in comparison with the work of 
Miwa et  al., (2009b),  which  used  multiple  cor-
pora  (AIMed  +  IEPA)  for  training  and  com-
bined  various  kernels  such  as  bag-of-words, 
parse trees  and  graphs.  It  is  natural  that  the  re-
call  value  is  lower  since  a  less  number  of  pat-
terns (features) must have been learned. What’s 
important  is  that  the  proposed  method  has  a 
higher  or  at  least  comparable  overall  perfor-
mance without additional resources.  

Our approach is significantly better than that 
of  Airola  et  al.,  (2008),  which  employed  two 
different forms of graph kernels to improve the 
initial  model.  Since  they  did  not  use  multiple 
corpora  for  training,  the  comparison  shows  the 
direct benefit of using the extension of the ker-
nel. 

6  Conclusion and Future Works 

To  improve  the  performance  of  PPIE,  recent 
research  activities  have  had  a  tendency  of  in-
creasing the complexity of the systems by com-
bining  various  methods  and  resources.  In  this 
paper, however, we argue that by paying more  

213

   
Our System 
(Miwa et al., 2009b) 
(Miwa et al., 2009a) 
(Miwa et al., 2008) 
(Miyao et al., 2008) 
(Giuliano et al., 2006) 
(Airola et al., 2008) 
(Sæ tre et al., 2007) 
(Erkan et al., 2007) 
(Bunescu & Mooney, 2005) 

POS 
1,000 
1,000 
1,000 
1,005 
1,059 

- 

1,000 
1,068 
951 

- 

NEG 
4,834 
4,834 
4,834 
4,643 
4,589 

- 

4,834 
4,563 
4,020 

- 

ma-P 
72.8 
60.0 
58.7 
60.4 
54.9 
60.9 
52.9 
64.3 
59.6 
65.0 

ma-R 
62.1 
71.9 
66.1 
69.3 
65.5 
57.2 
61.8 
44.1 
60.7 
46.4 

ma-F 
67.0 
65.2 
61.9 
61.5 
59.5 
59.0 
56.4 
52.0 
60.0 
54.2 

σF 
4.5 

 

7.4 

 
 
 

5.0 

 
 
 

Table  4.  Comparative  results  in  AIMed.  The  number  of  positive  instances  (POS)  and  negative  in-
stances (NEG) and macro-averaged precision (ma-P), recall (ma-R) and F1-score (ma-F) are shown.  

attention  to  a  single  model  and  adjusting  para-
meters  more  carefully,  we  can  obtain  at  least 
comparable performance if not better. 

This  paper  indicates  that  a  well-tuned  parse 
tree  kernel  based  on  decay  factor  can  achieve 
the  superior  performance  in  PPIE  when  it  is 
preprocessed  by  the  path-enclosed tree pruning 
method. It was shown in a series of experiments 
that our system produced the best scores in sin-
gle  corpus  evaluation  as  well  as  cross-corpora 
validation  in  comparison  with  other  state-of-
the-art methods. Contribution points of this  pa-
per are as follows: 
(1)  We  have  shown  that  the  benefits  of  using 
additional  resources  including  richer  features 
can  be  obtained  by  tuning  a  single  tree  kernel 
method with tree pruning and decaying factors. 
(2) We have newly found that the decay factor 
influences  precision  enhancement  of  PPIE  and 
hence its overall performance as well. 
(3)  We  have  also  revealed  that  the  parse  tree 
kernel  method  equipped  with  decay  factors 
shows superior  generalization  power  even  with 
small  corpora  while  presenting  significant  per-
formance  increase  on  cross-corpora  experi-
ments. 

As a future study, we leave experiments with 
training the classifier with multiple corpora and 
deeper  analysis  of  what  aspects  of  the  corpora 
gave different magnitudes of the improvements. 

Acknowledgment 

We  want  to  thank  the  anonymous  reviewers 
for  their  valuable  comments.  This  work  has 
been supported in part by KRCF Grant, the Ko-
rean government. 

Reference 

Airola,  A.,  Pyysalo,  S.,  Bjorne,  J.,  Pahikkala,  T., 
Ginter, F. & Salakoski, T. (2008). All-paths graph 
kernel  for  protein-protein  interaction  extraction 
with  evaluation  of  cross-corpus  learning.  BMC 
Bioinformatics,  9(S2),  doi:10.1186/1471-2105-9-
S11-S2. 

Andrade,  M.  A.  &  Valencia,  A.  (1998).  Automatic 
extraction of keywords from scientific text: appli-
cation  to  the  knowledge  domain  of  protein  fami-
lies. Bioinformatics, 14(7), 600-607. 

Blaschke, C., Andrade, M., Ouzounis, C. & Valencia, 

A.  (1999).  Automatic  extraction  of  biological  in-
formation  from scientific  text: protein-protein in-
teractions. Proc. Int. Conf. Intell. Syst. Mol. Biol., 
(pp. 60-67). 

Bunescu, R., Ge, R., Kate, R., Marcotte, E., Mooney, 
R., Ramani, A. & Wong, Y. (2005).  Comparative 
Experiments  on  Learning  Information  Extractors 
for  Proteins  and  their  Interactions.  Artif.  Intell. 
Med., Summarization and Information Extraction 
from Medical Documents, 33, 139-155 

Collins,  M.  &  Duffy,  N.  (2001).  Convolution  Ker-
nels  for Natural Language. NIPS-2001, (pp. 625-
632). 

Craven, M. & Kumlien, J. (1999). Constructing bio-
logical  knowledge  bases  by  extracting  informa-
tion from text sources. Proceedings of the 7th In-
ternational  conference  on  intelligent  systems  for 
molecular  biology,  (pp.77-86),  Heidelberg,  Ger-
many. 

Ding,  J.,  Berleant,  D.,  Nettleton,  D.  &  Wurtele,  E. 
(2002).  Mining  MEDLINE:  abstracts,  sentences, 
or phrases?. Proceedings of PSB'02, (pp. 326-337) 

Erkan, G., Ozgur, A., & Radev, D. R., (2007). Semi-
supervised classification for extracting protein in-

214

teraction  sentences  using  dependency  parsing.  In 
EMNLP 2007. 

Fundel, K., Küffner, R. & Zimmer, R. (2007). RelEx 
–  Relation  extraction  using  dependency  parse 
trees. Bioinformatics, 23, 365-371. 

Giuliano,  C.,  Lavelli,  A.,  Romano,  L.,  (2006).  Ex-
ploiting Shallow Linguistic Information for Rela-
tion  Extraction  From  Biomedical  Literature.  Pro-
ceedings of  the 11th Conference of the European 
Chapter  of  the  Association  for  Computational 
Linguistics. 

Gondy, L., Hsinchun C. & Martinez Jesse D. (2003). 
A  shallow  parser  based  on  closed-class  words  to 
capture  relations  in  biomedical  text.  J.  Biomed. 
Informatics. 36(3), 145-158. 

GuoDong, Z., Min, Z., Dong, H. J. & QiaoMing, Z. 
(2007).  Tree  Kernel-based  Relation  Extraction 
with  Context-Sensitive  Structured  Parse  Tree  In-
formation.  Proceedings  of  the  2007  Joint  Confe-
rence on Empirical Methods in Natural Language 
Processing  and  Computational  Natural  Language 
Learning, Prague, (pp. 728–736) 

Nikolai,  D.,  Anton,  Y.,  Sergei,  E.,  Svetalana,  N., 
Alexander,  N.  &  llya,  M.  (2004).  Extracting  hu-
man protein interactions from MEDLINE using a 
full-sentence  parser.    Bioinformatics,  20(5),  604-
611. 

Ono,  T.,  Hishigaki,  H.,  Tanigam,  A.  &  Takagi,  T. 
(2001).  Automated  extraction  of  information  on 
protein-protein interactions from the biological li-
terature. Bioinformatics, 17(2), 155-161. 

Pyysalo,  S.,  Airola,  A.,  Heimonen,  J.,  Björne,  J., 
Ginter,  F.  &  Salakoski,  T.  (2008).    Comparative 
analysis of five protein-protein interaction corpo-
ra. 
9(S6), 
doi:10.1186/1471-2105-9-S3-S6. 

Bioinformatics, 

BMC 

Sæ tre,  R.,  Sagae,  K.,  &  Tsujii,  J.  (2007).  Syntactic 
features for protein-protein interaction extraction. 
In LBM 2007 short papers. 

Sekimizu, T., Park H. S. & Tsujii J. (1998). Identify-
ing the interaction between genes and gene prod-
ucts based on frequently seen verbs in MEDLINE 
abstracts.  Workshop  on  genome  informatics,  vol. 
9, (pp. 62-71). 

Marcotte, E. M., Xenarios, I. & Eisenberg D. (2001). 
Mining  literature  for  protein-protein  interactions. 
Bioinformatics, 17(4), 359-363. 

Shawe-Taylor,  J.,  Cristianini,  N.,  (2004).  Kernel 
Methods for Pattern Analysis, Cambridge Univer-
sity Press. 

Miwa, M., Sæ tre, R., Miyao, Y. & Tsujii J. (2009a). 
Protein-protein  interaction  extraction  by  leverag-
ing  multiple  kernels  and  parsers.  International 
Journal of Medical Informatics, 78(12), e39-e46. 

Temkin, J. M. & Gilder, M. R. (2003). Extraction of 
protein interaction information  from  unstructured 
text using a context-free grammar. Bioinformatics, 
19(16), 2046-2053. 

Miwa, M., Sæ tre, R., Miyao, Y. & Tsujii J. (2009b). 
A  Rich  Feature  Vector  for  Protein-Protein  Inte-
raction  Extraction  from  Multiple  Corpora.  Pro-
ceedings  of  the  2009  Conference  on  Empirical 
Methods  in  Natural  Language  Processing,  (pp. 
121-130) 

Miwa, M., Sæ tre, R., Miyao, Y., Ohta,  T., & Tsujii, 
J. (2008). Combining multiple layers of syntactic 
information for protein-protein interaction extrac-
tion.  In  Proceedings  of  the  Third  International 
Symposium  on  Semantic  Mining  in  Biomedicine 
(SMBM 2008), (pp. 101–108) 

Miyao,  Y.,  Sæ tre,  R.,  Sagae,  K.,  Matsuzaki,  T.,  & 
Tsujii, J. (2008). Task-oriented evaluation of syn-
tactic  parsers  and  their  representations.  Proceed-
ings  of  the  45th  Meeting  of  the  Association  for 
Computational Linguistics (ACL’08:HLT). 

Moschitti,  A.  (2006).  Making  tree  kernels  practical 
for  natural  language  learning.    Proceedings  of 
EACL’06, Trento, Italy. 

 

Vishwanathan,  S.  V.  N.,  Smola,  A.  J.  (2003).  Fast 
Kernels for String and Tree Matching.  Advances 
in  Neural  Information  Processing  Systems,  15, 
569-576, MIT Press. 

Zhang, M., GuoDong, Z. & Aiti, A. (2008). Explor-
ing  syntactic  structured  features  over  parse  trees 
for  relation  extraction  using  kernel  methods.  In-
formation  Processing  and  Management,  44,  687-
701 

Zhang,  M.,  Zhang,  J.,  Su,  J.  &  Zhou,  G.  (2006).  A 
Composite  Kernel  to  Extract  Relations  between 
Entities  with  both  Flat  and  Structured  Features. 
21st  International  Conference  on  Computational 
Linguistics and 44th Annual Meeting of the ACL, 
(pp.825-832). 

Zhou,  D.  &  He,  Y.  (2008).  Extracting  interactions 
between  proteins  from  the  literature.  Journal  of 
Biomedical Informatics, 41, 393-407. 

