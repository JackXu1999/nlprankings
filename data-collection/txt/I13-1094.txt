International Joint Conference on Natural Language Processing, pages 781–787,

Nagoya, Japan, 14-18 October 2013.

781

What information is helpful

for dependency based Semantic Role Labeling

Yanyan Luo

Kevin Duh

Yuji Matsumoto

Computational Linguistics, Nara Institute of Science and Technology

Takayama, Ikoma, Nara 630-0192, Japan

{yanyan-l; kevinduh; matsu}@is.naist.jp

Abstract

Semantic Role Labeling (SRL) is an im-
portant task since it beneﬁts a wide range
of natural language processing applica-
tions. Given a sentence,
the task of
SRL is to identify arguments for a pred-
icate (target verb or noun) and assign
semantically meaningful labels to them.
Dependency parsing based methods have
achieved much success in SRL. How-
ever, due to errors in dependency pars-
ing, there remains a large performance gap
between SRL based on oracle parses and
SRL based on automatic parses in prac-
tice.
In light of this, this paper investi-
gates what additional information is nec-
essary to close this gap. Is it worthwhile to
introduce additional dependency informa-
tion in the form of N-best parse features, or
is it better to incorporate orthogonal non-
dependency information (base chunk con-
stituents)? We compare the above features
in a SRL system that achieves state-of-the-
art results on the CoNLL 2009 Chinese
task corpus. Our ﬁndings suggest that or-
thogonal information in the form of con-
stituents is much more helpful in improv-
ing dependency based SRL in practice.

1 Introduction

In recent years, SRL has become an impor-
tant component in many kinds of deep natural
language processing applications, such as ques-
tion answering (Narayanan and Harabagiu, 2004),
event extraction (Riedel and McCallum, 2011),
document categorization (Persson et al., 2009).
SRL aims at identifying the semantic relations be-
tween predicates in a sentence and their associ-
ated arguments, with these relations drawn from
a pre-speciﬁc list of possible semantic roles for

corresponding predicates. Syntax information is
essential in SRL systems. To date, both con-
stituent parsing and dependency parsing based
SRL have been investigated (Xue, 2008; Johans-
son and Nugues, 2008), with dependency based
systems giving superior results in CoNLL 2008
(Surdeanu et al., 2008) and CoNLL 2009 shared
tasks ( Hajiˇc et al., 2009).

However,

the performance gap is still quite
large between SRL systems using oracle ”perfect”
dependency parses and SRL systems using auto-
matic dependency parses. We observe as much
as 10% F-score difference in our experiments.
Clearly, errors in the 1-best dependency parse af-
fects SRL prediction. This leaves an open ques-
tion: in order to improve dependency based SRL,
is it more worthwhile to incorporate more depen-
dency information (in the form of N-best parse), or
to incorporate an entirely separate source of infor-
mation, such as base phrase chunks? We perform
such an analysis in this paper, using a state-of-the-
art Chinese SRL system.

Our ﬁndings suggest that constituent informa-
tion such as chunking nicely complements depen-
dency based SRL, achieving more improvements
compared to N-best dependency information. Fi-
nally, we also report the best results to date on the
CoNLL 2009 Chinese shared task.

2 Related Work
The bulk of previous work on automatic SRL has
primarily focused on using full constituent parse
of sentences to deﬁne argument boundaries and
to extract relevant information for training clas-
siﬁers. However, there have been some attempts
at relaxing the necessity of using syntactic infor-
mation derived from full parse trees. Sun et. al
(2009) and Hacioglu et. al (2004) addressed the
SRL problem on the basis of shallow syntactic in-
formation at the level of phrase chunks. In their
approach, SRL is formulated as a sequence label-

782

ing problem, performing IOB2 decisions on the
syntactic chunks of a sentence. However, this
method ignores the full syntactic parsing informa-
tion entirely, and we believe that even the accu-
racy of full syntactic parsing is not ideal, it is still
helpful for SRL. Moreover, their method is inap-
plicable to dependency based SRL since a chunk
usually consists of successive words.

A substantial amount of research has focused on
dependency-based SRL (Meza-Ruiz and Riedel,
2009; Luo et al., 2012) since the CoNLL-2009
shared task and rich linguistic features (Zhao et
al., 2009) are applied. For dependency related
features, most studies focused on extracting them
from the best dependency result. Johansson and
Nugues (2008) tried to use N-best dependency
parsing results. In their work, they applied 16-best
dependency trees to generate predicate-argument
structures and applied both syntactic trees and
predicate-argument structures to a linear model.
This model reranks the predicate-argument struc-
tures and the top 16 dependency trees at the same
time. Though their work suggests that N-best de-
pendency parsing can enhance the SRL, little is
known about how the N-best dependency parsing
related features perform on SRL.

3 Dependency based SRL Model

First, we deﬁne an instance as a predicate word
and its corresponding argument words.
If there
are m predicates in a sentence, then there will
be m instances.
Given an instance X =
{x1, . . . , xp, . . . , xn} with the predicate position
p, we want to ﬁnd the corresponding sequence
of argument
labels and predicate sense S =
a1, ap−1, P, ap+1, . . . , an = ⟨P, A⟩. Each ai for
the i-th word in the instance X is drawn from a set
of tags T (A) which contains all the semantic role
labels in the corpus and which follows the deﬁni-
tion criteria in Chinese PropBank. In addition, the
special label N ON E is added to T (A). Words, la-
beled as N ON E, are not arguments for the pred-
icate. As for P , this is a member of a sense set
T (xp) which contains all possible senses of pred-
icate word xp. We propose two sorts of label as-
signment models P rlocal and P rglobal. The for-
mer can incorporate local features only; the latter
can incorporate also global features. We use three
types of local feature sets: FP , FA, FP A and one
global feature set FG. These type deﬁnitions are
the same as those in Watanabe et. al (2010).

∑

3.1 Predicate Sense Disambiguation and SRL

with a Local Model

Since the predicate cannot be an argument of itself
for Chinese, we deﬁne the following local prob-
abilistic model for argument classiﬁcation and
predicate sense disambiguation.

P rlocal(S|X) =

P r(ai|P, X, i, p) · P r(P|X, p)

where P r(ai|P, X, i, p) and P r(P|X, p) are esti-
mated according to the following equation:

(1)

n∏

i=1(i̸=p)

P r(ai|P, X, i, p) =

Z A(X)

1

∑

exp{

+

∈FP A

fP Ak

P r(P|X, p) =

1

Z P (X)

exp{

λfAj

fAj (ai, X)

∈FA
fP Ak (ai, X, p, P )},

fPl (X, p, P )},

λfPl

fAj

λfP Ak

∑

∈FP

fPl

where ZA and ZP are normalization functions,
i.e.,

∑

∑
∑
∑

fP Ak

ai∈T (A)

ZA =

+

ZP =

P∈T (xp)

exp{

fAj

λfAj

fAj (ai, X)
∈FA
fP Ak (ai, X, p, P )};
∑

λfP Ak

fPl(X, p, P )};

λfPl

∈FP

fPl

∈FP A
exp{

f are the features with associated weight λ learned
via training.

3.2 Predicate Sense Disambiguation and SRL

with the Global Model

Global information is known to be useful in SRL
(Nakagawa, 2007). We propose a global proba-
bilistic model P rglobal here for SRL as follows:

P rglobal(S|X) =

P rlocal(S|X) · exp

9=;

λfGm

fGm (S, X)

(2)

1
Z

8<: ∑

∈FG

fGm

where Z is a normalizing factor over all candi-
date sequences S(X, p) (set of possible conﬁgura-
tions of semantic tags and predicate senses given
X and predicate location p). To get the whole se-
quence of S, we need to perform a computation-
ally expensive search. As done in previous work
(Watanabe et al., 2010), we use a simple approach,

783

Type %Error
49.4%
88.62%
80.71%

C
G
O

#Error/#Occurrence

7,162/14,497

109/123

3,175/3,934

Table 1: The distribution of SRL errors on devel-
opment corpus by the joint model.

n-best relaxation. Unlike the P rlocal(S|X), the
product of probability distributions of each word,
the probability distribution P rglobal(S|X) is cal-
culated by feature functions fG deﬁned on an in-
stance X with assignment S. Thereby, we can use
any information in an instance without the inde-
pendence assumption for assignments of words in
it.

3.3 Error Analysis for Dependency-based

SRL

Using the gold parse of dependency relations be-
tween a predicate and its arguments and accord-
ing to these relations, we classiﬁed SRL errors into
following three types.

• C: children of a predicate should be argu-
ments but they are tagged incorrectly.
• G: grand children of a predicate should be ar-
guments but they are tagged incorrectly.
• O: others
Table 1 shows the distribution of three errors
observed in the development corpus after tagging
by our joint model. For example, there are a total
of 14,497 arguments that are children of predicates
and among them, and 7,162(49.4%) are errors.

4 Results and Discussion
4.1 Experimental Setting
We used the Chinese dataset provided by CoNLL-
2009 shared task for experiments. For compari-
son, two kinds of dependency parsing results are
provided, the ﬁrst is from MALT parser, the sec-
ond is from second-order MST parser.

As for chunking information, we used the chunk
deﬁnition presented in (Chen et al., 2006) to ex-
tract chunks from Chinese Tree Bank as training
corpus. The line CH in Figure 1 shows the deﬁ-
nition of chunks. In this example, ” 金融工作”(ﬁ-
nance work) is a noun phrase and is composed by
two nouns.

With the Inside/Outside representation for
proper chunks and the following feature templates,
where x0 is the current word, a CRF++1 is trained
for Chinese chunking task.

• Uni-gram word/POS tag features: x−2, x−1,
x0, x+1 and x+2.
• Bi-gram word/POS tag features: x−2x−1,
x−1x0, x0x+1 and x+1x+2.

4.2 Features
Most of features templates are ”standard” which
have been widely used in previous dependency-
based SRL research
(Johansson and Nugues,
2008; Luo et al., 2012). We do not explain ”stan-
dard” features, however, we give a detailed de-
scription of the features used in this work.
4.2.1 Base Phrase Chunking Related

Features

In Figure 1, obviously, words in chunks do not
have equal importance for SRL. Headwords rep-
resent the main meaning of the chunks. The base
phrase chunking related features shown in Ta-
ble 2 are only applied to these headwords. For
other words in chunks, only lemma and POS in-
formation is used. The rules described in Sun
and Jurafsky (2004) are used to extract head-
words. Verb class in Table 2 is represented sim-
ilarly as V erb.C1C2, which means this verb has
two senses. For its ﬁrst sense, it has one core ar-
gument and for its second sense, it has two core
arguments. These verb classes are extracted from
Chinese PropBank (Xue, 2008).

4.2.2 Features from N-best Dependency

Parsing

According to the statistics of development corpus,
it is found that about 78.13% arguments are chil-
dren of predicates. Even if its error percentage
shown in Table 1 is less than 10%, the total er-
ror number is also considerable. If we can reduce
the head errors for dependents, the C errors caused
by dependency parsing errors should be decreased,
and SRL tagging results would be improved. Un-
der this hypothesis, we simply extracted the fol-
lowing features from every parse tree in the N-
best list which are generated using second-order
MST parser. These features are also included in
the ”standard” feature set when N = 1.
1http://crfpp.sourceforge.net/

784

Feature Name Description

Figure 1: Chunking information for a predicate-argument structure.

chunk tag of headword with IB representation (e.g. B − N P )
chunk tag of the chunk where the headword belongs to
the number of words in a chunk
the POS sequence of words in a chunk, for example, ”金融工作” (ﬁnance
work) is ”NN NN”
the position of the chunk with respect to the predicate(Position). There are
three possible values: ”before”, ”after” and ”here”.
the conjunctions of Position and headword, predicate and verb class
the conjunctions of Position and POS of headword, predicate and verb class
lemma/POS of one word immediately before/after of the chunk
a chain of chunk types between the headword and the predicate.
the length of the chunk chain between the headword and the predicate
For example, chain of chunk types between headword ”工作” and predicate ”
取得” is ”NP-VP” and the length of the chunk chain is 2.

Chunk features

Path features

Table 2: Chunking related feature template for experiments.

′

heads:

Arguments

lemma and
pos; dependency label; whether they are predi-
cates.

lemma/pos;

P osition: position of the argument candidates
with respect to the predicate positions in the tree;
position of the heads of the argument candidates
with respect to the predicate position in the sen-
tence.

Chain:

the left-to-right chain of the depen-

dency labels of the predicate’s dependents.

4.3 SRL Performance
The overall performance of SRL is calculated us-
ing the semantic evaluation metric of the CoNLL-
2009 shared task scorer2.
Table 3 gives the
comparison of SRL performance before and after
adding the proposed base phrase chunking related
features on the test data. Lines with −/+ show
the SRL performance without/with base phrase
chunking related features. As seen in this table,
without gold dependency parse, the best SRL is up
to 80.52 in F1 score. To the best of author’s knowl-
edge, there are few Chinese SRL results more than

2http://ufal.mff.cuni.cz/conll2009-st/

eval09.pl

Gold parsing − 88.68
Gold parsing + 90.03
MALT −
82.64
84.17
MALT +
MST-2 −
83.01
84.49
MST-2 +

P(%) R(%) F1(%)
87.47
88.86
77.34
79.13
79.02
80.52

86.30
87.71
72.68
74.67
75.39
76.92

Table 3: SRL results without/with base phrase
chunking information.

80%.

Although comparing the lines with −, it shows
dependency parsing play the central role in Chi-
nese SRL as expected. Comparing their corre-
sponding lines with +, Chinese SRL can still ben-
eﬁt a lot from shallow parsing information. An ex-
ample from the corpus is shown in Figure 2. Fig-
ure 2a shows the gold dependency parsing result
and the gold predicate argument structure; Fig-
ure 2b shows the dependency parsing result from
MALT parser and the predicate argument struc-
ture as a result of the predicted parse; Figure 2c
shows the predicate argument structure which is
predicted after adding base phrase chunking re-

WORD	 去年	 西藏	

金融	

工作	

取得	

显著	

成绩	

POS	

CH	

TAG	

SRL	

NN	

NR	

NN	

NN	

[         NP        ]	

[NP]	

[NP]	
B-NP	
TMP	 NONE	 NONE	

B-NP	

B-NP	

VV	

[VP]	

JJ	

[ADJP]	

NN	

[NP]	

I-NP	

B-VP	

B-ADJP	

B-NP	

A0	

取得.01	

NONE	

A1	

785

(a) Gold SRL result with gold dependency tree

(b) Wrong SRL result brought by MALT parse errors

(c) SRL result corrected by introducing base phrase chunking related features

Figure 2: An example that the argument prediction error brought by MALT parse errors is corrected by
introducing base phrase chunking related features.

MST-2−
MST-2−
MST-2−
MST-2−

N-best P(%) R (%) F1(%)
79.02
79.75
79.82
79.62

83.01
82.52
82.74
82.44

75.39
77.16
77.10
76.98

1
3
5
10

N-best Correct (#) Error(#) Noise(#)

1
3
5
10

18,428
19,071
19,392
19,738

3,176
2,533
2,212
1,866

-

4,636
5,667
7,699

Table 4:
parsing related features.

SRL results with N-best dependency

Table 5: Dependency accuracy and the noise
changes with different N.

lated features. In Figure 2c, the subscripts stand
for chunk types. From Figure 2b, it can be seen
that the argument A1 is not identiﬁed by the de-
pendency based SRL because of dependency er-
rors. Comparing Figure 2b and 2c, we can see
that after adding the base phrase chunking related
features, this SRL error brought by dependency
parsing errors is corrected.
Line MALT+ and line MST-2− show that even
the dependency parsing result from MALT is not
better than that from second order MST, with the
aid of chunking related features, Chinese SRL can
still get comparable results.

Table 4 shows the Chinese SRL results after
adding the N-best dependency parsing related fea-
tures. It is not surprising that SRL can get better
performance when N > 1, because the larger N, a
more accurate dependency parsing results can be

likely obtained. When N = 5, SRL gets the best
performance 79.82 in F1 with 0.8 point improve-
ment.

However, the improvement declines when N =
10. A larger N may result in adding more accurate
dependency parsing, however, it can also result in
including more noises. For the MST parser us-
ing second order algorithm, Table 5 shows how
the choice of the value of N affects the depen-
dency parsing. The Correct(#) column represents
the number of cases where the correct parent of
an argument is predicted within the N-best. For
example, in 3-best, it counts the number of argu-
ments where their parents are correctly predicted
in at least one of the 3 predictions.
In the case
where the parent is not predicted in any tree, they
are counted as an error, as listed in the second col-
umn. The third column (Noise), is deﬁned under

中国	 已 累计	 批准  	外商    	投资	 项目      	近	 二十五点九万	 个	

A0	

ADV	

ADV	

A1	

EXT	

 批准.01  	

中国	 已	 累计	 批准	 外商    	投资	 项目	 近	 二十五点九万	 个	

A0	

ADV	

ADV	

 批准.01  	

EXT	

中国	 【已	 累计	 批准】VP  	【外商    	投资	 项目】NP      	近	 【二十五点九万	 个】QP	

A0	

ADV	

ADV	

A1	

EXT	

 批准.01  	

China has approved nearly 259,000 foreign investment projects.	

786

[Bj¨orkelund,
2009]
[Meza-Ruiz,
2009]
[Zhao, 2009]
MST-2 +
MST-2 +
MST-2 +

N-best P(%) R (%) F1(%)
78.60

82.42

75.12

-

-

-
1
3
5

82.66

73.36

77.73

80.42
84.49
83.81
83.71

75.20
76.92
78.51
78.40

77.72
80.52
81.07
80.97

Table 6: SRL results with base phrase chunking
information and N-best parsing related features.

a hypothesis: correct dependency relations gener-
ate correct SRL results, wrong dependency rela-
tions generate incorrect SRL results. It represents
the number of wrong dependency relations in Cor-
rect case which can cause bad inﬂuence on SRL
results. For example, if 3 best heads for an ar-
gument are top-1, top-2, top-3 respectively, and
top-1 is the correct one, then this case is a Cor-
rect case and the number of noise are 2; if none
of the three results are correct, then this case is an
Error case, and no noise. From this table, it ob-
viously indicates that the beneﬁt for dependency
parsing brought by a larger N is less than the noise
brought by the N.

With Tables 3 and 4, it can be seen that SRL
beneﬁts more from chunking related features than
from N-best parse related features.

Table 6 shows the the results of Chinese SRL
after adding base phrase chunking information and
N-best parsing related features and gives the com-
parison with the previous work. From Tables 4
and 6 we can see that after adding the chunking
related features, the impact of N-best parsing re-
lated features is a little reduced.

4.4 Discussion
In Section 4.3, we see that both chunking and N-
best parsing related features are helpful for Chi-
nese SRL to some extent. In order to understand
how they affect SRL, we analyze the results from
three types of errors introduced in Section 3.3. Ta-
ble 7 shows the error changes when different fea-
tures are added.

Since accurate dependency information is not
always available,
the three types of errors
should become larger when automatic dependency
parsers are used. From Tables 1 and 7, the C
and O errors increased as expected, while G de-

MST-2−
MST-2−
MST-2−
MST-2+
MST-2+
MST-2+

N-best C(%) G(%) O(%)
59.06
56.84
57.36
54.70
53.05
53.38

25.37
22.83
22.83
23.93
21.5
21.66

86.93
78.43
78.43
86.93
76.47
76.47

1
3
5
1
3
5

Table 7: SRL error changes with different features

creased. The main reason is that arguments, that
are grandchildren of predicates, are relocated in
the dependency trees because of dependency er-
rors, and these locations make them easier to be
tagged. From the ﬁrst and fourth rows, they sug-
gest that shallow parsing information are helpful
to reduce the C and O errors. Comparing the
fourth line with second and third rows, they ex-
plain why SRL achieves more improvements from
chunking than from N-best dependency. When N
changed from 1 to 3, the errors decreased obvi-
ously, however, when the N = 5, there are no
obviously different changes.

5 Conclusions and Future Work
In this paper, we introduce additional informa-
tion: base phrase chunking and N-best depen-
dency parsing related features to a dependency
based SRL system and investigate the beneﬁt that
our Chinese SRL model can get from them. Eval-
uations on the CoNLL 2009 Chinese corpus show
that chunking information well complements de-
pendency based SRL, achieving more improve-
ments compared to N-best dependency informa-
tion. With those additional features, our depen-
dency based SRL achieves the best result on the
same Chinese corpus to our knowledge. Further-
more, while all our experiments are for Chinese,
it is possible to design experiments for other lan-
guages with our models.

Our experiment results show that we are not
limited to increase SRL performance via more ac-
curate syntactic parsing, but that we can explore
other information, which is easier to get and is
helpful for SRL. This also guides our future work.
In our future work, we would like to explore more
features and their inﬂuence on SRL.

787

Lance A. Ramshaw and Mitchell P. Marcus. 1995.
Text Chunking using Transformation-Based Learn-
ing. Proceedings of the 3rd Workshop on Very Large
Corpora , pp. 88-94.

Sebastian Riedel and Andrew McCallum. 2011. Fast
and Robust Joint Models for Biomedical Event Ex-
Proceeding of the 2011 Joint Confer-
traction.
ence on Empirical Methods in Natural Language
Processing and Computational Natural Languages
Learning , pp. 1-12.

Honglin Sun and Daniel Jurafsky.

2004. Shallow
Semantic Parsing of Chinese. Proceedings of Hu-
man Language Technology Conference/North Amer-
ican Chapter of the Association for Computational
Linguistics(HLT/NAACL-2004) , pp. 1249-256.

Weiwei Sun, Zhifang Sui, Meng Wang and Xin Wang.
2009. Chinese Semantic Role Labeling with Shal-
low Parsing. Proceedings of the 2009 Conference on
Empirical Methods in Natural Language Processing
, pp. 1475-1483.

Mihai Surdeanu, Richard Johansson, Adam Meyers,
Llu´ıs M`arquez and Joakim Nivre.
The
CoNLL-2008 Shared Task on Joint Parsing of Syn-
tactic and Semantic dependencies. Proceedings of
the 12th Conference on Computational Natural Lan-
guage Learning , pp. 157-177.

2008.

Kristina Toutanova, Aria Haghighi and Christoper D.
Manning. 2008. A Global Joint Model for Semantic
Role Labeling. Computational Linguistics , 34(2):
pp.161-191.

Yotaro Watanabe, Masayuki Asahara and Yuji Mat-
sumoto. 2010. A Structured Model for Joint Learn-
ing of Argument Roles and Predicate Senses. PPro-
ceedings of the ACL 2010 Conference Short Papers
, pp. 98-102.

Nianwen Xue. 2008. Labeling Chinese Predicates
with Semantic Roles. Computational Linguistics ,
34(2): pp. 225-255.

Hai Zhao, Wenliang Chen, Chunyu Kit and Guodong
Zhou.
2009. Multilingual Dependency Learn-
ing:A Huge Feature Engineering Method to Seman-
tic Dependency Parsing. Proceedings of the Thir-
teenth Conference on Computational Natural Lan-
guage Learning (CoNLL): Shared Task , pp. 55-60.

References
Anders Bj¨orkelund, Love Hafdell and Pierre Nugues.
2009. Multilingual Semantic Role Labeling. Pro-
ceedings of the Thirteenth Conference on Computa-
tional Natural Language Learning(CoNLL), pp. 43-
48.

Wenliang Chen, Yujie Zhang and Hitoshi Isahara.
2006. An Empirical Study of Chinese Chunking.
Proceedings of the COLING/ACL on Main confer-
ence poster sessions , pp. 97-104.

Kadri Hacioglu, Sameer Pradhan, Wayne Ward, James
H.Martin and Daniel Jurafsky. 2004. Semantic Role
Labeling by Tagging Syntactic Chunks. Proceed-
ings of the 8th Conference on CoNLL-2004, Shared
Task .

Jan Hajiˇc, Massimiliano Ciaramita, Richard Johans-
son, Daisuke Kawahara, Maria Ant`oia Mart´ı, Llu´ıs
M`arquez, Adam Meyers, Joakim Nivre, Sebastian
Pad´o, Jan ˇStˇep´anek, Pavel Straˇn´ak, Mihai Surdeanu,
Nianwen Xue and Yi Zhang. 2009. The CoNLL-
2009 Shared Task: Syntactic and Semantic Depen-
dencies in Multiple Languages. Proceedings of the
Thirteenth Conference on Computational Natural
Language Learning , pp. 1-18.

Richard Johansson and Pierre Nugues.

2008.
Dependency-based Semantic Role Labeling of Prop-
Bank. In Proceedings of the Conference on Empir-
ical Methods in Natural Language Processing, pp.
69-78.

Luo Yanyan, Asahara Masayuki and Matsumoto Yuji.
Robust Integrated Models for Chinese
China

2012.
Predicate-Argument Structure Analysis.
Communications , 9(3): pp. 10-18.

Ryan McDonald, Koby Crammer and Fernando
Pereira. 2005. Online Large-margin Training of De-
pendency Parsers. Proceeding of the 43th Annual
Meeting on Association for Computational Linguis-
tics, pp.91-98.

Ivan Meza-Ruiz and Sebastian Riedel. 2009. Jointly
Identifying Predicates, Arguments and Senses Us-
Proceedings of Human
ing Markov Logic.
Language Technology Conference/North American
Chapter of
the Association for Computational
Linguistics(HLT/NAACL-2009), pp. 155-163.

Tetsuji Nakagawa. 2007. Multilingual Dependency
Proceedings of
Parsing Using Global Features.
the CoNLL Shared Task Session of EMNLP-CoNLL
2007 , 34(2): pp. 952-956.

Srini Narayanan and Sanda Harabagiu. 2004. Ques-
tion Answering Based on Semantic Structures. Pro-
ceeding of the 20th International Conference on
Computer Linguistics, pp. 693-701.

Jacob Persson, Richard Johansson and Pierre Nugues.
2009. Fast and Robust Joint Models for Biomedi-
cal Event Extraction. NODALIDA 2009 Conference
Proceedings , pp.142-149.

