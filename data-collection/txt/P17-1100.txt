



















































Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics


Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, pages 1084–1094
Vancouver, Canada, July 30 - August 4, 2017. c©2017 Association for Computational Linguistics

https://doi.org/10.18653/v1/P17-1100

Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, pages 1084–1094
Vancouver, Canada, July 30 - August 4, 2017. c©2017 Association for Computational Linguistics

https://doi.org/10.18653/v1/P17-1100

Supervised Learning of Automatic Pyramid
for Optimization-Based Multi-Document Summarization

Maxime Peyrard and Judith Eckle-Kohler
Research Training Group AIPHES and UKP Lab

Computer Science Department, Technische Universität Darmstadt
www.aiphes.tu-darmstadt.de, www.ukp.tu-darmstadt.de

Abstract

We present a new supervised framework
that learns to estimate automatic Pyramid
scores and uses them for optimization-
based extractive multi-document summa-
rization. For learning automatic Pyramid
scores, we developed a method for au-
tomatic training data generation which is
based on a genetic algorithm using auto-
matic Pyramid as the fitness function. Our
experimental evaluation shows that our
new framework significantly outperforms
strong baselines regarding automatic Pyra-
mid, and that there is much room for im-
provement in comparison with the upper-
bound for automatic Pyramid.

1 Introduction

We consider extractive text summarization, the
task of condensing a textual source, e.g., a set of
source documents in multi-document summariza-
tion (MDS), into a short summary text. The qual-
ity of an automatic system summary is tradition-
ally evaluated by comparing it against one or more
reference summaries written by humans. This
comparison is performed by means of an evalua-
tion metric measuring indicators of summary qual-
ity and combining them into an aggregated score.

Many state-of-the-art summarization systems
cast extractive summarization as an optimization
problem and maximize an objective function in or-
der to create good, i.e., high-scoring summaries.
To this end, optimization-based systems com-
monly use an objective function which encodes
exactly those quality indicators which are mea-
sured by the particular evaluation metric being
used. Some systems even employ an approxima-
tion of the evaluation metric as objective function.

Consider as an example the ROUGE metric
which has become a de-facto standard for sum-
mary evaluation (Lin, 2004). ROUGE computes
the n-gram overlap between a system summary
and a pool of reference summaries. There are sev-
eral previous approaches which have used an ap-
proximation of ROUGE as the optimization objec-
tive (e.g., Sipos et al. (2012); Peyrard and Eckle-
Kohler (2016a)).

However, ROUGE has been widely criticized
for being too simplistic and not suitable for captur-
ing important quality aspects we are interested in.
In particular, ROUGE does not capture sentences
which are semantically equivalent but expressed
with different words (Nenkova et al., 2007).

Ideally, we would like to evaluate our sum-
maries based on human judgments. A well-known
example of such a human evaluation method is the
so-called Pyramid method (Nenkova et al., 2007):
it evaluates the particular quality aspect of content
selection and is based on a manual comparison of
Summary Content Units (SCUs) in reference sum-
maries against SCUs in system summaries. While
the resulting Pyramid score is much more mean-
ingful and informative than ROUGE, it is very ex-
pensive to obtain, and – worse – not reproducible.

These issues have been addressed by a line of
research aimed at automating the Pyramid evalua-
tion (Harnly et al., 2005; Passonneau et al., 2013).
Recently, Yang et al. (2016) developed a freely
available off-the-shelf system for automatic Pyra-
mid scoring called PEAK, which uses open Infor-
mation Extraction (open IE) propositions as SCUs
and relies on proposition comparison. Automatic
Pyramid (AP) scores are reproducible, and unlike
ROUGE, they are based on semantically motivated
content units (SCUs) rather than word n-grams.
Moreover, they correlate better with human judg-
ments than ROUGE (Yang et al., 2016).

Given these recent advances in the automatic

1084

https://doi.org/10.18653/v1/P17-1100
https://doi.org/10.18653/v1/P17-1100


evaluation of summaries regarding content selec-
tion, we believe that research in optimization-
based summarization should move away from
ROUGE towards AP as a more meaningful eval-
uation metric to approximate and to optimize.

In our work, we are the first to explore this
new direction and to systematically investigate the
use of AP in optimization-based extractive sum-
marization. We make the following contributions:

• We compute an upper-bound for AP with a
Genetic Algorithm (GA), and compare it to
the ROUGE upper-bound.

• We develop a new extractive MDS system
specifically optimizing for an approximation
of AP. Our system uses a supervised learning
setup to learn an approximation of AP from
automatically generated training data. We
constrain the learned approximation of AP to
be linear so that we can extract summaries
efficiently via Integer Linear Programming
(ILP). Our experimental evaluation shows
that our approach significantly outperforms
strong baselines on the AP metric.

The code both for the new upper-bound and for
our ILP is available at github.com/UKPLab/
acl2017-optimize_pyramid.

2 Background

In this section, we summarize the Pyramid method
and the PEAK system, the automated version of
Pyramid we consider in this work.

Pyramid The Pyramid method (Nenkova et al.,
2007) is a manual evaluation method which deter-
mines to what extent a system summary covers the
content expressed in a set of reference summaries.
The comparison of system summary content to
reference summary content is performed on the
basis of SCUs which correspond to semantically
motivated, subsentential units, such as phrases or
clauses.

The Pyramid method consists of two steps: the
creation of a Pyramid set from reference sum-
maries, and second, Pyramid scoring of system
summaries based on the Pyramid set. In the first
step, humans annotate phrasal content units in the
reference summaries and group them into clusters
of semantically equivalent phrases. The resulting
clusters are called SCUs and the annotators as-
sign an SCU label to each cluster, which is a sen-
tence describing the cluster content in their own

words. The final set of SCUs forms the Pyramid
set. Each SCU has a weight corresponding to the
number of reference summaries in which the SCU
appears. Since each SCU must not appear more
than once in each reference summary, the maxi-
mal weight of an SCU is the total number of refer-
ence summaries. In the second step, humans anno-
tate phrasal content units in a system summary and
align them to the corresponding SCUs in the Pyra-
mid set. The Pyramid score of a system summary
is then calculated as the sum of the SCU weights
for all Pyramid set SCUs being aligned to anno-
tated system summary phrases.

PEAK The AP system PEAK by Yang et al.
(2016) uses clauses as the content expressing units
and represents them as propositions in the open IE
paradigm. An open IE proposition is a triple of
subject, predicate and object phrases. PEAK uses
the state-of-the-art system clausIE (Del Corro and
Gemulla, 2013) for proposition extraction.

While PEAK includes the automatic creation of
Pyramid sets from reference summaries, as well as
automatic Pyramid scoring of system summaries,
in this work, we use PEAK for automatic scoring
only. As for the Pyramid sets, we can assume that
these have already been created, either via PEAK
or by humans (e.g., using the TAC 2009 data1).

Since automatic scoring with PEAK requires
that the Pyramid sets consist of representative
open IE propositions which constitute the auto-
mated counterparts of the SCUs, we first need to
represent the manually constructed SCUs as open
IE propositions, too. To this end, we use clausIE
to extract an open IE proposition from each SCU
label – a sentence describing the cluster content.
As a result, each pyramid set is represented as a
list of propositions {pj} with a weight taken from
the underlying SCU.

For scoring, PEAK processes a system sum-
mary with clausIE, converting it from a list of
sentences to a list of propositions {si}. A bi-
partite graph G is constructed, where the two
sets of nodes are the summary propositions {si}
and the pyramid propositions {pj}. An edge is
drawn between si and pj if the similarity is above
a given threshold. PEAK computes the similar-
ity with the ADW system (Align, Disambiguate
and Walk), a system for computing text similar-
ity based on WordNet, which reaches state-of-the-

1http://tac.nist.gov/2009/
Summarization

1085



art performance but is slow (Pilehvar et al., 2013).
Since each system summary unit can be aligned
to at most one SCU, the alignment of the sum-
mary propositions {si} and the pyramid propo-
sitions {pj} is equivalent to finding a maximum
weight matching, which PEAK solves using the
Munkres-Kuhn bipartite graph algorithm. From
the matched pyramid propositions {pj} the final
pyramid score is computed.

3 Approach

3.1 Upper-bound for Automatic Pyramid
We start by computing upper-bound summaries
according to AP in order to gain a better under-
standing of the metric.

Notations Let D = {si} be a document collec-
tion considered as a set of sentences. A summary
S is simply a subset of D. We use ppyr to de-
note the set of propositions in the Pyramid sets ex-
tracted from the SCU labels using clausIE.

The upper-bound is the set of sentences S∗ with
the best AP score.

Method The task is to extract the set of sen-
tences which contains the propositions matching
most of the highest-weighted SCUs, thus resulting
in the best matching of propositions, i.e., the high-
est AP score possible. Formally, we have to solve
the following optimization problem:

S∗ = argmax
S

AutoPyr(S) (1)

Unfortunately, it cannot be solved directly via
ILP because of the Munkres-Kuhn bipartite graph
algorithm within AP. While Munkres-Kuhn is an
ILP, we solve a different problem. In our problem,
Munkres-Kuhn would act as constraint because we
are looking for the best matching among all valid
matchings. Munkres-Kuhn only yields the valid
matching for one particular set of sentences. One
global ILP can be written down by enumerating
all possible matchings in the constraints but it will
have a completely unrealistic runtime.

Instead, we have to rely on search-based algo-
rithms and compute summaries close to the upper-
bound. We search for such an approximate so-
lution by employing a meta-heuristic solver in-
troduced recently for extractive MDS by Peyrard
and Eckle-Kohler (2016a). Specifically, we use
the tool published with their paper.2 Their meta-

2https://github.com/UKPLab/
coling2016-genetic-swarm-MDS

heuristic solver implements a Genetic Algorithm
(GA) to create and iteratively optimize summaries
over time.

In this implementation, the individuals of the
population are the candidate solutions which are
valid extractive summaries. Valid means that the
summary meets the length constraint. Each sum-
mary is represented by a binary vector indicating
for each sentence in the source document whether
it is included in the summary or not. The size
of the population is a hyper-parameter that we set
to 100. Two evolutionary operators are applied:
the mutation and the reproduction. The mutation
happens to several randomly chosen summaries
by randomly removing one of its sentences and
adding a new one that does not violate the length
constraint. The reproduction is performed by ran-
domly extracting a valid summary from the union
of sentences of randomly selected parent sum-
maries. Both operators are controlled by hyper-
parameters which we set to their default values.

In our scenario, the fitness function is the AP
metric, which takes a summary S as input and
outputs its AP score. S is converted into a list
of propositions pS by looking-up the propositions
of each sentence in S from a pre-computed hash-
map. For all sentences in the document collection
D, the hash-map stores the corresponding propo-
sitions. Then the Munkres-Kuhn algorithm is ap-
plied to pS and ppyr in order to find matching
propositions, and finally the scores of their corre-
sponding SCUs are used to evaluate the fitness of
the summary.

The runtime might become an issue, because
the similarity computation between propositions
via ADW is slow. However, all the necessary in-
formation is present in the similarity matrix A de-
fined by:

Aij = ADW (p
D
i , p

pyr
j ) (2)

Here Aij is the semantic similarity between the
proposition pDi from the source document i and
the proposition pPj from the Pyramid set j. A has
dimensions m × n if m is the number of proposi-
tions in the document collection and n the number
of propositions in the Pyramid set. We keep the
runtime low by pre-computing the similarity ma-
trix A.

With a population of 100 summaries in the GA,
the algorithm converges in less than a minute to
high scoring summaries, which we can expect to
be close to the real upper-bound.

1086



3.2 Supervised Setup to Learn an
Approximation of AP

We denote the true AP scoring function by π∗.
π∗ scores summaries by matching the summary
propositions to the Pyramid propositions in Ppyr
as described before. In this work, we aim to learn
a function π, which approximates π∗ without hav-
ing access to Ppyr, but only to the document col-
lection D.

Formally, it means that over all document col-
lections D and all summaries S, we look for π
which minimizes the following loss:

L(π) =
∑

D∈D

∑

S∈S
‖π(D,S)− π∗(Ppyr, S)‖2 (3)

This states that the learned π minimizes the
squared distance from π∗ over the available train-
ing data.

Model Note that we simply denote π(D,S) by
π(S) as it is not ambiguous which document col-
lection is used when S is a summary of D.

In order to be able to use an exact and efficient
solver like ILP, we constrain π to be a linear func-
tion. Therefore, we look for π of the following
form:

π(S) =
∑

s∈S
fθ(s)−

∑

i>j

gγ(si ∩ sj) (4)

Two functions are jointly learned: fθ is a function
scoring individual sentences, and gγ is a function
scoring the intersection of sentences. θ ∪ γ is the
set of learned paramaters.

We can interpret this learning scenario as jointly
learning the sentence importance and the redun-
dancy to get π as close as possible to the true AP
π∗. fθ represents the notion of importance learned
in the context of AP, while gγ contains notions of
coherence and redundancy by scoring sentence in-
tersections. This scenario is intuitive and inspired
by previous work on summarization (McDonald,
2007).

Now, we explain how to learn these two func-
tions while enforcing π to be linear. Suppose each
sentence is represented by a feature set φ and each
sentence intersections is represented by φ∩, then
the set of features for a summary S is:

Φ(S) = {
⋃

s∈S
φ(s) ∪

⋃

i>j

φ∩(si ∩ sj)} (5)

It is clear that the number of features is vari-
able and depends on the number m of sentences

in S. In order to deal with a variable number of
sentences as input, one could use recurrent neural
networks, but at the cost of loosing linearity.

Instead, to keep the linearity and to cope with
variable sized inputs, we employ linear models for
both fθ and gγ :

π(S) =
∑

s∈S
θ · φ(s)−

∑

i>j

γ · φ∩(si ∩ sj) (6)

By leveraging the properties of linear models
we end-up with the following formulation:

π(S) = θ ·
∑

s∈S
φ(s)− γ ·

∑

i≥j
φ∩(si ∩ sj) (7)

Because of the linear models, we can sum fea-
tures over sentences and over sentence intersec-
tions to obtain a fixed size feature set:

Φ
∑

(S) = {φ
∑

(S) ∪ φ
∑
∩ (S)} (8)

where we introduced the following notations:

φ
∑

(S) =
∑
s∈S

φ(s)

φ
∑
∩ (S) =

∑
i>j

φ(si ∩ sj)

Suppose φ is composed of k features and φ∩ of
n features. Then φ

∑
(S) is a vector of dimension

k, and similarly φ
∑
∩ (S) is of dimension n. Finally,

Φ∑ is a fixed size feature set of dimension k + n.
The function π as defined in equation 6 is still

linear with respect to sentence and sentence inter-
section features, which is convenient for the sub-
sequent summary extraction stage.

Features While any feature set for sentences φ
and for sentence intersections φ∩ could be used,
we focused on simple ones in this work.

For a sentence s, φ(s) consists of the following
features:

• Sentence length in number of words.

• Sentence position as an integer number start-
ing from 0.

• Word overlap with title: Jaccard similarity
between the unigrams in the title t and a sen-
tence s:

Jaccard(s, t) =
|t ∩ s|
|t ∪ s| (9)

• Sum of frequency of unigrams and bigrams
in the sentence.

1087



• Sum of TF*IDF of unigrams and bigrams in
the sentence. The idf of unigrams and bi-
grams is trained on a background corpus of
DBpedia articles.3

• Centrality of the sentence computed via
PageRank: A similarity matrix is built be-
tween sentences in the document collection
based on their TF*IDF vector similarity.
Then a power method is applied on the sim-
ilarity matrix to get PageRank scores of in-
dividual sentences. It is similar to the clas-
sic LexRank algorithm (Erkan and Radev,
2004).

• Propositions centrality: We also use the
centrality feature for propositions. Each sen-
tence is scored by the sum of the centrality of
its propositions. As PEAK is based on propo-
sitions, we expect proposition-level features
to provide a useful signal.

Finally, φ∩(si ∩ sj) consists of the unigram, bi-
gram and trigram overlap between the two sen-
tences si and sj .

Training The model is trained with a stan-
dard linear least squares regression using pairs of
(Φ(S), π∗(S)) as training examples. Because our
approach relies on an automatic metric, an arbi-
trarily large number of summaries and their corre-
sponding scores can be generated. In contrast, get-
ting manual Pyramid annotations for a large num-
ber of summaries would be expensive and time-
consuming.

As training examples we take the population of
scored summaries created by the same GA we use
for computing upper-bound summaries. It is im-
portant to note that this GA is also a perfect gen-
erator of training instances: the summaries in its
population are already scored because the fitness
function is the AP metric. Indeed, for each topic,
an arbitrarily large amount of scored summaries
can be generated by adjusting the size of the popu-
lation. Moreover, the summaries in the population
are very diverse and have a wide range of scores,
from almost upper-bound to completely random.

Optimization-based Summary Extraction
Since the function π is constrained to be linear,
we can extract the best scoring summary by
solving an ILP.

3http://wiki.dbpedia.org/
nif-abstract-datasets

Let x be a binary vector indicating whether sen-
tence i is in the summary or not. Similarly, let
α be a binary matrix indicating whether both sen-
tence i and j are in the summary. Finally, let K
be the length constraint. With these notations, the
best summary is extracted by solving the follwo-
gin ILP:

argmax
S

∑
si∈S

xi∗θ ·φ(si)−
∑
i≥j

αi,j ∗γ ·φ∩(si∩sj)
m∑
i=1

xi ∗ len(si) ≤ K
∀(i, j), αi,j − xi ≤ 0
∀(i, j), αi,j − xj ≤ 0

∀(i, j), xi + xj − αi,j ≤ 1
Which is the ILP directly corresponding to maxi-
mizing π as defined by equation 6. Note that · is
the dot product while ∗ is the scalar multiplication
in R.

4 Experiments

4.1 Setup
Dataset We perform our experiments on a multi-
document summarization dataset from the Text
Analysis Conference (TAC) shared task in 2009,
TAC-2009.4 TAC-2009 contains 44 topics, each
consisting of 10 news articles to be summarized in
a maximum of 100 words. In our experiments, we
use only the so-called initial summaries (A sum-
maries), but not the update summaries. For each
topic, there are 4 human reference summaries and
a manually created Pyramid set. As described in
section 2, we pre-processed these Pyramid sets
with clausIE in order to make them compatible
with PEAK.

Metrics We primarily evaluate our system via
automatic Pyramid scoring from PEAK, after pre-
processing the summaries with clausIE. PEAK has
a parameter twhich is the minimal similarity value
required for matching a summary proposition and
a Pyramid proposition. We use two different val-
ues: t = 0.6 (AP-60) and t = 0.7 (AP-70).

For completeness, we also report the ROUGE
scores identified by Owczarzak et al. (2012a) as
strongly correlating with human evaluation meth-
ods: ROUGE-1 (R-1) and ROUGE-2 (R-2) recall
with stemming and stopwords not removed.

Finally, we perform significance testing with t-
test to compare differences between two means.5

4http://tac.nist.gov/2009/
Summarization/

5The symbol * indicates that the difference compared to

1088



4.2 Automatic Evalution

Upper-bound Comparison We compute the set
of upper-bound summaries for both ROUGE-2 (R-
UB) and for AP (AP-UB).6 Both sets of upper-
bound summaries are evaluated with ROUGE and
AP, and the results are reported in Table 1.

R-1 R-2 AP-60 AP-70

R-UB 0.4722* 0.2062* 0.5088 0.3074
AP-UB 0.3598 0.1057 0.5789* 0.3790*

Table 1: Upper bound comparison between
ROUGE and Automatic Pyramid (AP).

Interestingly, we observe significant differences
between the two upper-bounds. While it is ob-
vious that each set of upper-bound summaries
reaches the best score on the metric it maximizes,
the same summary set scores much worse when
evaluated with the other metric. This observation
empirically confirms that the two metrics measure
different properties of system summaries.

Moreover, the upper-bound for AP gives us in-
formation about the room for improvement that
summarization systems have with respect to AP.
This is relevant in the next paragraph, where we
compare systems in an end-to-end evaluation.

End-to-end Evaluation We evaluate the qual-
ity of the summaries extracted by the summarizer
π − ILP in a standard end-to-end evaluation sce-
nario. π − ILP is the system composed of the
learned function π and the ILP defined in the pre-
vious section.

Learning π Using our GA data generation
method, we produce 100 scored summaries for
each of the 44 topics in TAC2009 while comput-
ing the upper-bound. We use the threshold value
of 0.65 as a compromise between AP-60 and AP-
70. The data generated have scores ranging from
0. to 0.4627 with an average of 0.1615. The data is
well distributed because the standard deviation is
0.1449. A highly diverse set of summaries is pro-
duced, because on average two summaries in the
training set only have 1.5% sentences in common,
and most of the sentences of the source documents
are contained in at least one summary.

The model is then trained in a leave-one-out
cross-validation setup. The parameters θ and γ are

the previous best baseline is significant with p ≤ 0.05.
6We use the parameter t = 0.6 during the upper-bound

computation of AP-UB.

R-1 R-2 AP-60 AP-70

TF*IDF 0.3251 0.0626 0.2857 0.1053
LexRank 0.3539 0.0900 0.3969 0.1854
ICSI 0.3670 0.1030 0.3520 0.1568
JS-Gen 0.3381 0.0868 0.3745 0.1463

π-ILP 0.3498 0.0867 0.4402* 0.2109*

Table 2: End-to-end evaluation of our approach on
TAC-2009.

trained on all topics but one. The trained model is
used to extract a high-scoring summary on the re-
maining topic by solving the ILP defined above.

Our framework is compared to the following
baselines:

TF*IDF weighting A simple heuristic intro-
duced by Luhn (1958) where each sentence re-
ceives a score from the TF*IDF of its terms.
The best sentences are greedily extracted until the
length constraint is met. We use the implementa-
tion available in the sumy package.7

LexRank (Erkan and Radev, 2004) is a pop-
ular graph-based approach. A similarity graph
G(V,E) is constructed where V is the set of sen-
tences and an edge eij is drawn between sentences
vi and vj if and only if the cosine similarity be-
tween them is above a given threshold. Sentences
are scored according to their PageRank score inG.
It is also available in the sumy package.

ICSI (Gillick and Favre, 2009) is a recent sys-
tem that has been identified as one of the state-
of-the-art systems by Hong et al. (2014). It is an
ILP framework that extracts a summary by solv-
ing a maximum coverage problem considering the
most frequent bigrams in the source documents.
We use the Python implementation released by
Boudin et al. (2015).

JS-Gen (Peyrard and Eckle-Kohler, 2016a) is
a recent approach which uses a GA to minimize
the Jensen-Shannon (JS) divergence between the
extracted summary and the source documents. JS
divergence measures the difference between prob-
ability distributions of words in the source docu-
ments and in the summary.

Results We report the performance of π− ILP
in comparison to the baselines in Table 2.

The results confirm an expected behavior. Our
supervised framework which aims at approximat-
ing and maximizing AP, easily and significantly
outperforms all the other baselines when evaluated

7https://github.com/miso-belica/sumy

1089



with AP for both values of the threshold. While
the system is not designed with ROUGE in mind,
it still performs reasonably well in the ROUGE
evaluation, even though it does not outperform
previous works.

In general, the two metrics ROUGE and AP do
not produce the same rankings of systems. This is
another piece of empirical evidence that they mea-
sure different properties of summaries.

When we compare the system performances to
the upper-bound scores reported in Table 1, we see
that there is still a large room for improvements.
We take a closer look at this performance gap in
the next paragraph where we evaluate the learning
component of our approach.

Evaluation of Learned π In this paragraph, we
evaluate the learning of π as an approximation of
π∗. We do so by measuring the correlation be-
tween π and the true AP π∗.

We report three correlation metrics to evaluate
and compare the ranking of summaries induced by
π and π∗: Pearson’s r, Spearman’s ρ and NDCG.
Pearson’s r is a value correlation metric which de-
picts linear relationship between the scores pro-
duced by two ranking lists.

Spearman’s ρ is a rank correlation metric which
compares the ordering of systems induced by the
two ranking lists.

NDCG is a metric from information retrieval
which compares ranked lists and puts a special
emphasis on the top elements by applying loga-
rithm decay weighting for elements further down
in the list. Intuitively, it describes how well the
π function is able to recognize the best scoring
summaries. In our case, it is particularly desir-
able to have a high NDCG score, because the op-
timizer extracts summaries with high π scores; we
want to confirm that top scoring summaries are
also among top scoring summaries according to
the true π∗.

For comparison, we report how well our base-
lines correlate with π∗. For this, we consider
the scoring function for summaries which is part
of all our baselines, and which they explicitly or
implicitly optimize: TF*IDF greedily maximizes
fTF∗IDF , the sum of the frequency of the words in
the summary. ICSI maximizes the sum of the doc-
ument frequency of bigrams (fICSI ). LexRank
maximizes fLexRank, the sum of the PageRank of
sentences in the summary, and fJS is the JS diver-
gence between the summary and the source docu-

Pearson’s r Spearman’s ρ NDCG

fTF∗IDF 0.1246 0.0765 0.8869
fLexRank 0.1733 0.0879 0.8774
fICSI 0.3742 0.3295 0.8520
fJS 0.4074 0.3833 0.8803

π 0.4929* 0.4667* 0.9429*

Table 3: Performance of the supervised learn-
ing of π on TAC-2009 in a leave-one-out cross-
validation.

ments optimized by JS-Gen.
For our supervised learning of π, the training

procedure is the same as described in the previous
section. The correlation scores are averaged over
topics and reported in Table 3.

We observe that π is able to approximate AP
significantly better than any baseline for all met-
rics. This explains why optimizing π with ILP out-
performs the baseline systems in the end-to-end
evaluation (Table 2).

The learned π achieves a high NDCG, indicat-
ing that optimizing π produces summaries very
likely to have high π∗ scores. This means that π
is capable of accurately identifying high-scoring
summaries, which again explains the strong per-
formance of π − ILP . The fact that the overall
correlations are lower for every system shows that
it is difficult to predict π for poor and average qual-
ity summaries.

It is interesting to observe that features such as
unigram and bigram frequency, which are known
to be strong features to approximate ROUGE, are
less useful to approximate the more complex AP.

Feature Weights The advantage of linear mod-
els is their interpretability. One can investigate the
contribution of each feature by looking at its corre-
sponding weight learned during training. The sign
of the weight indicates whether the feature corre-
lates positively or negatively with the results, and
its amplitude determines the importance of this
feature in the final estimation.

We observe that the most useful feature is the
proposition centrality, which confirms our expec-
tation that proposition-based features are useful
for approximating PEAK. The bigram coverage
has also a high weight explaining the strong per-
formance of ICSI. The least useful feature is the
sentence position, even if it still contains some
useful signal.

Interestingly, the analysis of features from the

1090



Pearson’s r Spearman’s ρ NDCG

ROUGE − 1 0.3292 0.3187 0.7195
ROUGE − 2 0.3292 0.2936 0.7259

Table 4: Correlation between ROUGE-1 and
ROUGE-2 with AP on the automatically generated
training data for TAC-2009.

sentence intersection reveals a slightly positive
correlation for the unigram and bigram overlap,
but a negative correlation for trigram overlap. Our
interpretation is that the model learns that good
summaries tend to have repeated unigrams and
bigrams to ensure some coherence, while the re-
peated trigrams are more indicative of undesired
redundancy.

Agreement between ROUGE and AP In the
previous paragraphs, we already saw that differ-
ent metrics produce different rankings of systems.
We want to investigate this further and understand
to what extent ROUGE and AP disagree. To that
end, we use the summaries automatically gener-
ated by the genetic algorithm during the upper-
bound computation. Remember that for each topic
of TAC-2009 it produces 100 summaries with a
wide range of AP scores. We then score these
summaries with both ROUGE-1 and ROUGE-2
and compare how ROUGE metrics correlate with
AP. In order to get a meaningful picture, we use
the same three correlation metrics as above: Pear-
son’s, Spearman’s ρ and NDCG. The results are
presented in Table 4.

We observe a low correlation between ROUGE
metrics and AP in terms of both rank correlation
(Spearman’s ρ) and value correlation (Pearson’s
r). Even though the NDCG numbers are better, the
correlation is also relatively low given that higher
numbers are usually expected for NDCG (also ob-
served in Table 3).

This analysis confirms the initial claim that
ROUGE and AP behave quite differently and mea-
sure different aspects of summary quality. There-
fore, we believe systems developed and trained for
AP are worth studying because they necessarily
capture different aspects of summarization.

5 Related Work

We discuss (i) related work in extractive summa-
rization where an approximation of an automatic
evaluation metric was optimized, and (ii) work re-

lated to AP specifically.
As ROUGE is the metric predominantly

used for evaluation of extractive summarization,
there are several previous optimization-based ap-
proaches which included an approximation of
ROUGE in the objective function to maximize.
For example, Takamura and Okumura (2010) and
Sipos et al. (2012) performed structured out-
put learning (using pairs of summaries and their
ROUGE scores available in benchmark datasets as
training examples) and thereby learned to maxi-
mize the ROUGE scores of the system summaries.
Peyrard and Eckle-Kohler (2016b) on the other
hand, learned an approximation of ROUGE scores
for individual sentences in a supervised setup, and
subsequently employed these estimated sentence
scores in an ILP formulation to extract summaries.

There is also recent work on considering fully
automatic evaluation metrics (not relying on hu-
man reference summaries), such as the JS di-
vergence as optimization objective. Peyrard and
Eckle-Kohler (2016a) used metaheuristics to min-
imize JS divergence in a multi-document summa-
rization approach and showed that the resulting ex-
tractive summaries also scored competitively us-
ing ROUGE.

Regarding AP, there is not much prior work
apart from the papers where the different variants
of AP have been presented (Harnly et al., 2005;
Passonneau et al., 2013; Yang et al., 2016). Espe-
cially, there is no prior work in optimization-based
extractive summarization which has developed an
approximation of AP and used it in an objective
function.

However, AP as an evaluation metric is becom-
ing ever more important in the context of abstrac-
tive summarization, a research topic which has
been gaining momentum in the last few years. For
example Li (2015) and Bing et al. (2015) use an
earlier version of AP based on distributional se-
mantics (Passonneau et al., 2013) to evaluate ab-
stractive multi-document summarization.

6 Discussion and Future Work

We presented a supervised framework that learns
automatic Pyramid scores and uses them for
optimization-based summary extraction. Us-
ing the TAC-2009 multi-document summarization
dataset, we performed an upper-bound analysis
for AP, and we evaluated the summaries extracted
with our framework in an end-to-end evaluation

1091



using automatic evaluation metrics. We observed
that the summaries extracted with our framework
achieve significantly better AP scores than several
strong baselines, but compared to the upper-bound
for AP, there is still a large room for improvement.

We show that AP and ROUGE catch differ-
ent aspects of summary quality, but further work
would be needed in order to substantiate the
claim that AP is indeed better than ROUGE.
One way of doing so would be to perform a
human evaluation of high-scoring summaries ac-
cording to ROUGE and AP. In general, ROUGE-
1 and ROUGE-2 were considered as the base-
lines for validating the performance of AP be-
cause these variants strongly correlate with human
evaluation methods (Owczarzak et al., 2012a,b).
However, the comparison could be repeated with
ROUGE-3, ROUGE-4 and ROUGE-BE, which
have been found to predict manual Pyramid bet-
ter than ROUGE-1 and ROUGE-2 (Rankel et al.,
2013).

More generally, we see two main directions for
future research: (i) the more specific question on
how to improve the approximation of AP and (ii)
the general need for more research on AP.

There are several possible ways how to improve
the approximation of AP. First, more semantically-
oriented features could be developed, e.g., fea-
tures based on propositions rather than sentences
or n-grams, or word embedding features encoding
a large amount of distributional semantic knowl-
edge (Mikolov et al., 2013). Second, the linear-
ity constraint we used for efficiency reasons could
be relaxed. Modeling AP as a non-linear func-
tion will presumably enhance the approximation.
For the extraction of summaries based on a non-
linear function, greedy algorithms or search-based
strategies could be used, e.g., the GA we used in
this work for the upper-bound computation.

We see a general need for more research on AP,
because the way AP measures the quality aspect
of content selection is not only more meaningful
than ROUGE, but also applicable to the growing
field of abstractive summarization.

An important direction would be the improve-
ment of AP itself, both in terms of methods used
to compute AP, and in terms of tools: while the
current off-the-shelf system PEAK is a promising
start, it is very slow and therefore difficult to apply
in practice.

In this context, we would like to stress that our

GA-based method to create training data for learn-
ing a model of AP can easily be adapted to any au-
tomatic scoring metric, and specifically to other or
future AP variants.

Finally, we hope to encourage the community
to move away from ROUGE and instead consider
AP as the main summary evaluation metric. This
would be especially interesting for optimization-
based approaches, since the quality of the sum-
maries created by such approaches depends on the
quality of the underlying scoring metric.

7 Conclusion

We presented the first work on AP in optimization-
based extractive summarization. We computed
an upper-bound for AP and developed a super-
vised framework which learns an approximation
of AP based on automatically generated training
instances. We could access a large number of
high-quality training data by using the population
of a genetic algorithm. Our end-to-end evaluation
showed that of our framework significantly outper-
forms strong baselines on the AP metric, but also
revealed a large room for improvement in compar-
ison to the upper-bound, which motivates future
work on developing systems with better perfor-
mance on the semantically motivated AP metric.

Acknowledgments

This work has been supported by the German Re-
search Foundation (DFG) as part of the Research
Training Group “Adaptive Preparation of Informa-
tion from Heterogeneous Sources” (AIPHES) un-
der grant No. GRK 1994/1, and via the German-
Israeli Project Cooperation (DIP, grant No. GU
798/17-1).

References
Lidong Bing, Piji Li, Yi Liao, Wai Lam, Weiwei Guo,

and Rebecca Passonneau. 2015. Abstractive Multi-
Document Summarization via Phrase Selection and
Merging. In Proceedings of the 53rd Annual Meet-
ing of the Association for Computational Linguistics
and the 7th International Joint Conference on Natu-
ral Language Processing (Volume 1: Long Papers).
Association for Computational Linguistics, Beijing,
China, pages 1587–1597.

Florian Boudin, Hugo Mougard, and Benoit Favre.
2015. Concept-based Summarization using Inte-
ger Linear Programming: From Concept Pruning
to Multiple Optimal Solutions. In Proceedings of

1092



the 2015 Conference on Empirical Methods in Nat-
ural Language Processing. Association for Compu-
tational Linguistics, Lisbon, Portugal, pages 1914–
1918.

Luciano Del Corro and Rainer Gemulla. 2013.
ClausIE: Clause-based Open Information Extrac-
tion. In Proceedings of the 22Nd International Con-
ference on World Wide Web. ACM, Rio de Janeiro,
Brazil, pages 355–366.

Günes Erkan and Dragomir R. Radev. 2004. LexRank:
Graph-based Lexical Centrality As Salience in Text
Summarization. Journal of Artificial Intelligence
Research pages 457–479.

Dan Gillick and Benoit Favre. 2009. A Scalable Global
Model for Summarization. In Proceedings of the
Workshop on Integer Linear Programming for Natu-
ral Language Processing. Association for Computa-
tional Linguistics, Boulder, Colorado, pages 10–18.

Aaron Harnly, Rebecca Passonneau, and Owen Ram-
bow. 2005. Automation of Summary Evaluation
by the Pyramid Method. In Proceedings of the In-
ternational Conference Recent Advances in Natural
Language Processing (RANLP). Borovets, Bulgaria,
pages 226–232.

Kai Hong, John Conroy, benoit Favre, Alex Kulesza,
Hui Lin, and Ani Nenkova. 2014. A Repository
of State of the Art and Competitive Baseline Sum-
maries for Generic News Summarization. In Pro-
ceedings of the Ninth International Conference on
Language Resources and Evaluation (LREC’14).
Reykjavik, Iceland, pages 1608–1616.

Wei Li. 2015. Abstractive Multi-document Summa-
rization with Semantic Information Extraction. In
Proceedings of the 2015 Conference on Empirical
Methods in Natural Language Processing. Associ-
ation for Computational Linguistics, Lisbon, Portu-
gal, pages 1908–1913.

Chin-Yew Lin. 2004. ROUGE: A Package for Auto-
matic Evaluation of Summaries. In Text Summa-
rization Branches Out: Proceedings of the ACL-04
Workshop. Association for Computational Linguis-
tics, Barcelona, Spain, pages 74–81.

Hans Peter Luhn. 1958. The Automatic Creation of
Literature Abstracts. IBM Journal of Research De-
velopment 2:159–165.

Ryan McDonald. 2007. A study of global inference al-
gorithms in multi-document summarization. In Pro-
ceedings of the 29th European Conference on IR
Research. Springer-Verlag, Rome, Italy, ECIR’07,
pages 557–564.

Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Cor-
rado, and Jeff Dean. 2013. Distributed Representa-
tions of Words and Phrases and their Composition-
ality. In Advances in Neural Information Processing
Systems 26, Curran Associates, Inc., pages 3111–
3119.

Ani Nenkova, Rebecca Passonneau, and Kathleen
McKeown. 2007. The Pyramid Method: Incorporat-
ing Human Content Selection Variation in Summa-
rization Evaluation. ACM Transactions on Speech
and Language Processing (TSLP) 4(2).

Karolina Owczarzak, John M. Conroy, Hoa Trang
Dang, and Ani Nenkova. 2012a. An Assessment of
the Accuracy of Automatic Evaluation in Summa-
rization. In Proceedings of Workshop on Evaluation
Metrics and System Comparison for Automatic Sum-
marization. Association for Computational Linguis-
tics, Montréal, Canada, pages 1–9.

Karolina Owczarzak, Peter A. Rankel, Hoa Trang
Dang, and John M. Conroy. 2012b. Assessing the
Effect of Inconsistent Assessors on Summarization
Evaluation. In Proceedings of the 50th Annual
Meeting of the Association for Computational Lin-
guistics. Association for Computational Linguistics,
Jeju Island, Korea, pages 359–362.

Rebecca Passonneau, Emily Chen, Weiwei Guo, and
Dolores Perin. 2013. Automated Pyramid Scoring
of Summaries using Distributional Semantics. In
Proceedings of the 51st Annual Meeting of the As-
sociation for Computational Linguistics. Associa-
tion for Computational Linguistics, Sofia, Bulgaria,
pages 143–147.

Maxime Peyrard and Judith Eckle-Kohler. 2016a.
A General Optimization Framework for Multi-
Document Summarization Using Genetic Algo-
rithms and Swarm Intelligence. In Proceedings of
the 26th International Conference on Computational
Linguistics (COLING 2016). The COLING 2016 Or-
ganizing Committee, Osaka, Japan, pages 247 – 257.

Maxime Peyrard and Judith Eckle-Kohler. 2016b. Op-
timizing an Approximation of ROUGE - a Problem-
Reduction Approach to Extractive Multi-Document
Summarization. In Proceedings of the 54th Annual
Meeting of the Association for Computational Lin-
guistics (Volume 1: Long Papers). Association for
Computational Linguistics, Berlin, Germany, pages
1825–1836.

Mohammad Taher Pilehvar, David Jurgens, and
Roberto Navigli. 2013. Align, Disambiguate and
Walk: A Unified Approach for Measuring Seman-
tic Similarity. In Proceedings of the 51st Annual
Meeting of the Association for Computational Lin-
guistics. Association for Computational Linguistics,
Sofia, Bulgaria, pages 1341–1351.

Peter A. Rankel, John M. Conroy, Hoa Trang Dang,
and Ani Nenkova. 2013. A Decade of Automatic
Content Evaluation of News Summaries: Reassess-
ing the State of the Art. In Proceedings of the
51st Annual Meeting of the Association for Compu-
tational Linguistics. Association for Computational
Linguistics, Sofia, Bulgaria, pages 131–136.

Ruben Sipos, Pannaga Shivaswamy, and Thorsten
Joachims. 2012. Large-margin Learning of Sub-
modular Summarization Models. In Proceedings

1093



of the 13th Conference of the European Chapter of
the Association for Computational Linguistics. As-
sociation for Computational Linguistics, Avignon,
France, pages 224–233.

Hiroya Takamura and Manabu Okumura. 2010. Learn-
ing to Generate Summary as Structured Output. In
Proceedings of the 19th ACM international Confer-
ence on Information and Knowledge Management.
Association for Computing Machinery, Toronto ,
ON, Canada, pages 1437–1440.

Qian Yang, Rebecca Passonneau, and Gerard de Melo.
2016. PEAK: Pyramid Evaluation via Automated
Knowledge Extraction. In Proceedings of the 30th
AAAI Conference on Artificial Intelligence (AAAI
2016). AAAI Press, Phoenix, AZ, USA.

1094


	Supervised Learning of Automatic Pyramid for Optimization-Based Multi-Document Summarization

