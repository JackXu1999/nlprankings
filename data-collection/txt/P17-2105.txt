



















































Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics


Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 666–671
Vancouver, Canada, July 30 - August 4, 2017. c©2017 Association for Computational Linguistics

https://doi.org/10.18653/v1/P17-2105

Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 666–671
Vancouver, Canada, July 30 - August 4, 2017. c©2017 Association for Computational Linguistics

https://doi.org/10.18653/v1/P17-2105

Character-Aware Neural Morphological Disambiguation

Alymzhan Toleu Gulmira Tolegen
National Laboratory Astana, Nazarbayev University

53 Kabanbay batyr ave., Astana, Kazakhstan
alymzhan.toleu@gmail.com, gulmira.tolegen.cs@gmail.com,

aibek.makazhanov@nu.edu.kz

Aibek Makazhanov

Abstract

We develop a language-independent, deep
learning-based approach to the task of
morphological disambiguation. Guided
by the intuition that the correct analysis
should be “most similar” to the context,
we propose dense representations for mor-
phological analyses and surface context
and a simple yet effective way of com-
bining the two to perform disambiguation.
Our approach improves on the language-
dependent state of the art for two agglu-
tinative languages (Turkish and Kazakh)
and can be potentially applied to other
morphologically complex languages.

1 Introduction

Morphological disambiguation (MD) is a long
standing problem in processing morphologically
complex languages (MCL). POS tagging is a
somewhat related problem, however in MD, in
addition to POS tags, one typically has to pre-
dict lemmata (roots hereinafter) that surface forms
stem from and morphemes1 they bear. For
example, depending on the context, a Turkish
word adam can be analyzed as: (i) a man –
[adam]1+[Noun]2+[A3sg+Pnon+Nom]3 or (ii) my
island – [ada]1+[Noun]2+[A3sg+P1sg +Nom]3
(Hakkani-Tür et al., 2002). Thus, if one counts
analyses as tags, MD can be cast as a tagging prob-
lem with an extremely large tagset. This fact dis-
courages direct application of the state of the art
approaches designed for small fixed tagsets.

To develop a language independent dense repre-
sentation of the analyses, we segment2 an analysis

1We use the term morpheme for its universal recognition
within the community. A more appropriate term might be
grammeme, i.e. a value of grammatical category.

2Such a segmentation is denoted by the squared brackets
numbered in the respective order (cf. Turkish example).

into (i) the root, (ii) its POS and (iii) the morpheme
chain (MC). We then proceed to jointly learn the
embeddigns for the root and the POS segments and
to combine them and the MC segment representa-
tion into a single dense representation. MC seg-
ments are represented as binary vectors that, for
a given analysis, encode presence or absence of
each morpheme found in the train set. This en-
sures language independence and contrasts previ-
ous work (at least on Turkish and Kazakh), where
only certain morphemes are chosen as features de-
pending on their position (Assylbekov et al., 2016;
Hakkani-Tür et al., 2002) or presence (Makham-
betov et al., 2015) in an analysis, or the authors’
intuition (Yildiz et al., 2016; Tolegen et al., 2016;
Sak et al., 2007).

Apart from the sparseness of analyses distribu-
tion MCL notoriously raise free word order and
long dependency issues. Thus, decoding analysis
sequences using only the leftmost context may not
be enough. To address this we leverage the right-
most context as well. We model the left- and right-
most surface context in two ways: using (i) BiL-
STM (Greff et al., 2015) with a character-based
sub-layer (Ling et al., 2015) and (ii) with a feed
forward network on word embeddings. We then
entertain the idea that given a word with multiple
analyses and its surface context, the correct analy-
sis might be “closer” to the context. Following our
intuition, we have tried computing the distance be-
tween the analysis and the context representations,
and a simple dot product (as in unnormalized co-
sine similarity) has yielded the best performance.

We evaluate our approach on Turkish and
Kazakh data sets, using several baselines (includ-
ing the state of the art methods for both languages)
and a variety of settings and metrics. In terms
of general accuracy our approach has achieved a
nearly 1% improvement over the state of the art for
Turkish and a marginal improvement for Kazakh.

666

https://doi.org/10.18653/v1/P17-2105
https://doi.org/10.18653/v1/P17-2105


Our contribution amounts to the following: (i) a
general MD framework for MCL that can be ana-
lyzed in <root, POS, MC> triplets; (ii) improve-
ment on language-dependent state of the art for
Turkish and Kazakh.

2 Models

In this section we describe our approach to encod-
ing morphological analyses and the context into
the embeddings and combining them to perform
morphological disambiguation.

2.1 Morphological Representation
We treat a morphological analysis as a combina-
tion of three main constituents: the root, its POS
and the morpheme chain. These constituents are
represented as dr, dp, and dm-dimensional vectors
respectively. The former two vectors correspond
to dense word embeddings (Collobert et al., 2011),
and the latter is a binary vector which encodes the
presence of a certain morpheme in the chain. The
size of the binary vector, dm, is equal to the size of
the morpheme dictionary obtained from the data.

Given a sentence and the j-th surface word form
withN analyses, we represent the k-th analysis as:

Akj = tanh(W
rrk + Wppk + Wmmk) (1)

where Aj ∈ Rdh×|N |, dh is the dimension of
each analysis embedding, rk ∈ Rdr×1, pk ∈
Rdp×1,mk ∈ {0, 1}dp×1 are constituent vectors
of the k-th analysis, and Wr ∈ Rdh×dr , Wp ∈
Rdh×dp ,Wm ∈ Rdh×dm are the model parame-
ters. The bias term was left out for clarity. This
representation is shown on Figure 1 (bottom).

2.2 Recurrent Neural Disambiguation
The model architecture is shown on Figure 1. It
consists of two main blocks that learn the surface
context (top) and the morphological analyses rep-
resentations (bottom).

When it comes to modeling context via word
embeddings for morphologically complex lan-
guages, it is impractical to actually store vectors
for all words, since majority of words in such
languages has a large number of surface realiza-
tions. Our solution to this problem is to con-
struct a surface word representation from charac-
ters that not only reduces data sparseness, but also
help in dealing with the out-of-vocabulary (OOV)
words (Ling et al., 2015). We represent each char-
acter of each word as a vector xi ∈ Rdc and the

Hidden layer:
Aj

k
 = tanh(W

rrk+ W
ppk+W

mmk)

LSTM

hidden layer

C a t

Cat sat on the mat

...

... ...

root pos MC

Characters

Context

Input layer: [rk , pk , mk]

Morphological representation

...
LSTMLSTM

LSTMLSTMLSTM

LSTMLSTMLSTM

LSTMLSTMLSTM

LSTMLSTM

LSTMLSTM

...
......

Sj = tanh(Dchc(j)+ DaLj+bj) Similarity

hw = [h
f,hb]

hc (j)

Figure 1: Model architecture

entire embedding matrix as Ec ∈ Rdc×|C|, where
C is the character vocabulary extracted from the
training set including alphanumeric characters and
other possible symbols. Given an input surface
word wi with its character embeddings x1, ..., xn,
the hidden state ht at the time step t can be com-
puted via the following Vanilla LSTM calcula-
tions:

it = σ(Wixt + Uiht−1 + bi) (2)

ft = σ(Wfxt + Ufht−1 + bf ) (3)
ot = σ(Woxt + Uoht−1 + bo) (4)
zt = tanh(Wzxt + Uzht−1 + bz) (5)
ct = ft � ct−1 + it � zt (6)
ht = ot � tanh(ct) (7)

where σ(·) and tanh(·) are the non-linear func-
tions. it, ft, ot are referred to three gates: input,
forget, output that control the information flow of
inputs. Parameters of the LSTM are W ∗, U∗, b∗,
where ∗ can be any of {i, f, o, g}. The peephole
connections were left out for clarity.

We use both forward and backward LSTM to
learn word representations obtained by concatena-
tion of the last states in both direction hf and hb,
e.g. hw =

[
hf , hb

]
. Character-based word em-

beddings obtained in this manner do not yet con-

667



tain the context information on a sentence level.
Thus, we adopt another LSTM to learn context-
sensitive information for each word in both direc-
tions. We denote the concatenation of the em-
beddings learned from the forward and backward
LSTM states as hc(j) ∈ R2hs×1 (where hs is the
output size for the j-th word), and represent sur-
face context as:

Sj = tanh(Dchc(j) + bj) (8)

where Sj ∈ Rdh×1 is a hidden layer output,
hc(j) ∈ R2hs×1is a context vector of the j-th
word, and bj ∈ Rdh×1 is a bias term.

For the final prediction, we score each analysis
by computing the inner product between its repre-
sentation and the context’s representations:

P kj = Sj · Akj (9)

where Sj and Akj are computed as equations (8)
and (1) respectively. We normalize the obtained
scores using softmax and choose the analysis with
the maximum score as the correct one. In what
follows we refer to this model as BiLSTM.

Finally, in a separate setting, in addition to the
surface context in the hidden layer we also incor-
porate the immediate (left and right) morphologi-
cal context in the form of the average of the anal-
yses representations:

S∗j = tanh(Dchc(j) + DaLj + bj) (10)

where Lj ∈ R2dh×1 is concatenation of averaged
representations of the leftmost and rightmost anal-
yses, and Dc ∈ Rdh×2hs and Da ∈ Rdh×2dh are
the model parameters. This advanced variation is
referred to as BiLSTM*.

2.3 Alternative Context Representation
We also experiment with an alternative context
model that uses a feed-forward NN architecture
(Collobert et al., 2011; Zheng et al., 2013). In this
model word embeddings of fixed window size are
fed to the hidden layer, and the output represents
the context. The remaining parts of the architec-
ture stay the same: we use the same morpholog-
ical representation and choose the correct analy-
sis exactly as we did for BiLSTM model. As in
the case with BiLSTM, we leverage morphologi-
cal context, by performing a Viterbi decoding con-
ditioned on the leftmost analysis. We refer to this

Lang. Train Test OOV ∆AT

Kazakh 16,624 2,324 43.9% 2.85

Turkish 752,332 20,536 10.24% 1.76

Table 1: Corpora statistics: ∆AT denotes the av-
erage number of analysis per token.

model as DNN (Deep NN), an advanced variation
of which uses the averaged rightmost morpholog-
ical context as well, and is referred to as DNN*.

2.4 Training
In all models, the top layer of the networks has a
softmax that computes the normalized scores over
morphological candidates given the input word.
The networks are trained to minimize the cross
entropy of the predicted and true morphological
analyses. Back-propagation is employed to com-
pute the gradient of the corresponding object func-
tion with respect to the model parameters.

3 Experiments and Evaluation

3.1 Data Sets
We conduct our experiments on Kazakh (Assyl-
bekov et al., 2016) and Turkish (Yuret and Türe,
2006) data sets3. Table 1 shows the corpora statis-
tics. Kazakh data set is almost 50 times smaller
than that of Turkish, with four times the OOV rate
and almost twice as many analyses per word on
average. Given such a drastic difference in the
resources it would be interesting to see how our
models perform on otherwise similar languages
(both Turkic). Lastly, while the corpora provide
train and test splits, there are no tuning sets, so we
withdraw small portions from the training sets for
tuning hyper-parameters4.

3.2 Baselines
We compare our models to three other approaches.
For Kazakh we use an HMM based tagger and
its version extended with the rule-based constraint
grammar (Assylbekov et al., 2016), which is con-
sidered the state of the art for the language. We

3For Turkish, we used a test set that was manually re-
annotated by Yildiz et al. (2016).

4The following hyper-parameters are used in all the ex-
periments: character embedding size dc = 35, character and
context LSTM states are 50, root and POS embedding sizes
are all set to 50, hidden layer size dh = 200, learning rate
is set to 0.01. The window size of DNN is set to 5. For
regularization we use dropout (Srivastava et al., 2014) with
probability 0.5 on the hidden layers. We further constrain the
norm of gradient to be below 2 by using gradient clipping.

668



Models Kazakh Turkish
tok.
acc.

tok.
amb.acc.

OOV
acc.

OOV
amb.acc.

sen.
acc.

tok.
acc.

tok.
amb.acc.

OOV
acc.

OOV
amb.acc.

sen.
acc.

HMM 83.82 74.98 80.90 73.89 32.41 - - - - -
MANN 85.56 77.66 81.68 74.96 32.80 91.35 82.32 86.72 74.09 40.38
Voted Perceptron 88.41 82.07 86.19 81.12 40.31 91.89 83.47 87.98 76.87 41.52
DNN 86.33 78.86 84.13 78.31 40.71 92.24 84.14 87.05 74.74 41.16
DNN* 87.25 80.28 85.99 80.85 40.31 92.22 84.12 87.24 75.11 40.38
DNN*‡ 88.26 81.85 86.77 81.92 39.52 92.32 84.32 87.95 76.50 41.55
BiLSTM 87.49 80.65 85.21 79.78 39.52 91.37 82.39 86.91 74.46 38.13
BiLSTM* 90.92 85.95 88.73 84.60 50.19 92.03 83.73 88.01 76.60 40.54
BiLSTM*‡ 91.06 86.40 88.93 85.27 50.98 92.16 84.01 88.24 77.06 41.01
HMMCG 90.39 85.88 88.83 86.07 53.75 - - - - -
BiLSTM*‡+CG 91.74 87.45 90.00 86.74 54.54 - - - - -

Table 2: Results: here, tok. acc. and tok. amb. acc. denote the accuracy over all and ambiguous tokens
respectively. Same goes for OOV acc. and OOV amb. acc.. Sentence accuracy is denoted as sen. acc..

refer to these baselines as HMM and HMMCG.
Another baseline is a voted perceptron (Collins,
2002) based tagger. We use our implementation
of this baseline for Kazakh and the model devel-
oped by Sak et al. (2007) for Turkish. Lastly, we
use a neural network model proposed by Yildiz
et al. (2016), which is considered state of the art
for Turkish. For this baseline too we use our own
implementation (for both languages) and refer to
it as MANN5.

3.3 Experimental Setup

As described in the previous section, each of our
models has two settings: the one that does not in-
corporate surrounding morphological context and
the one that does (the starred one). In addition to
that we use pre-trained embeddings, by training
word2vec (Mikolov et al., 2013) skip-gram model
on Wikipedia texts. This setting is denoted by a
double dagger (‡).

We perform a single run evaluation in terms of
token- and sentence- based accuracy. We consider
four types of tokens: (i) all tokens; (ii) ambiguous
tokens (the ones with at least two analyses); (iii)
OOV tokens; (iv) ambiguous OOV tokens. Thus,
we use a total of five metrics. In terms of strictness
we deem correct only the predictions that match
the golden truth completely, i.e. in root, POS and
MC (up to a single morpheme tag).

5Note that all of the baselines are language dependent
to a certain degree, with MANN being the least dependent
and HMMCG the most. The latter baseline employs hand-
engineered constraint grammar rules to perform initial disam-
biguation, followed by application of the HMM tagger, which
cherry-picks the most informative grammatical features.

3.4 Results and Discussion

The results are given in Table 2. Unless stated oth-
erwise we refer to the general (all tokens) accuracy
when comparing model performances.

For Kazakh, DNN conditioned on the leftmost
analysis yields 86.33% accuracy. DNN* that in
addition uses the rightmost analysis embeddings,
improves almost 1% over that result (87.25%).
On the other hand BiLSTM, whose context repre-
sentation uses surface forms only, performs even
better (87.49%). When this model incorporates
immediate morphological context, it (BiLSTM*)
performs at 90.92% and beats the HMMCG base-
line. However, the latter being a very strong lan-
guage dependent baseline still outperforms our
model in ambiguous OOV and sentence accuracy.
When we evaluate our model under equal condi-
tions (BiLSTM*‡+CG) it beats HMMCG on all of
the metrics. We separate this comparison from the
rest because of a language-dependent set up.

In contrast, for Turkish DNN models outper-
form BiLSTM on seen tokens and yield an al-
most equal 92.2% accuracy regardless of using
the rightmost morphological context. This perfor-
mance is also higher than that of all baselines, in-
cluding the state of the art MANN. However BiL-
STM* is still better than DNN* in OOV token ac-
curacy, both overall and ambiguous.

As it can be seen, pre-training boosts the per-
formance of DNN* and BiLSTM* across all met-
rics. For Kazakh pre-training results in .14%
improvement in general token accuracy for BiL-
STM*, which amounts to .67% improvement over
the state of the art. For Turkish this results in an

669



almost 1% net improvement in overall token accu-
racy over MANN, the state of the art6.

A cross-linguistic comparison reveals that al-
though Kazakh data set is much smaller than that
of Turkish and has more analyses per word on
average and higher OOV rate, on certain met-
rics the models perform on par or even better for
Kazakh7. To investigate this further we have made
data sets comparable in size by randomly choos-
ing 20.6K+ and 3.4K from Turkish training and
test sets. On this data BiLSTM*‡ yields 91.18,
82.0% general and ambiguous token accuracy and
respective scores for OOV are 87.0, 74.6%. This
result follows the pattern, where for Turkish only
the general accuracy is higher than that of Kazakh.
It turns out that Turkish data contains many unam-
biguous tokens: 49% and 48% for full and small
data sets (train + test average), against 36% for
Kazakh. This suggests that the higher general ac-
curacy on Turkish data can be explained by the
higher rate of the unambiguous tokens. Also Turk-
ish has a more complex derivational morphology,
which “lengthens” the analyses, e.g. an average
number of morphemes per analysis is higher for
Turkish (5.25) than for Kazakh (4.6). This adds
sparseness to the morpheme chains and certainly
further complicates disambiguation, especially in
an OOV scenario.

We also observe that BiLSTM*‡ works best on
all metrics for Kazakh, but for Turkish it beats
DNN*‡ only on the OOV part. Due to BiLSTM*‡
being computationally prohibitive we ran it with
significantly less number of epochs than DNN,
and it also being a character-based model, we
speculate that it was able to learn character aware
context embeddings hence better at OOV.

4 Related Work

A morphology-aware NN (MANN) for MD was
proposed by Yildiz et al. (2016), and has been
reported to achieve ambiguous token accuracies
of 84.12, 88.35 and 93.78% for Turkish, Finish
and Hungarian respectively. This approach dif-
fers from ours in a number of ways. (i) Our

6For the un-pretrained model original work reports
84.12% accuracy on ambiguous tokens (Yildiz et al., 2016),
which is lower than 84.14% that un-pretrained DNN achieves
on this metric.

7 For instance, BiLSTM*‡ applied to Kazakh performs
better than any other model for Turkish in terms of sentence,
ambiguous and OOV token accuracy. Moreover all of the
models (including the baselines) perform better on Kazakh in
terms of ambiguous OOV accuracy.

analysis representation treats morpheme tags in a
language-independent manner considering every
tag found in the training set, whereas in MANN
certain tags are chosen with a specific language
in mind. (ii) MANN is a feed-forward NN that,
unlike our approach, does not account for the sur-
face context. (iii) As we understood, at the de-
coding step MANN makes use of the golden truth,
whereas our models have no need for that.

Although several statistical models have
been proposed for Kazakh MD, such as
HMM- (Makazhanov et al., 2014; Makham-
betov et al., 2015; Assylbekov et al., 2016),
voted perceptron- (Tolegen et al., 2016) and
transformation-based (Kessikbayeva and Cicekli,
2016) taggers, to our knowledge ours is the first
deep learning-based approach to the problem that
is also purely language independent.

It is becoming increasingly popular to use richer
architectures to learn better embeddings from
characters/words (Yessenbayev and Makazhanov,
2016; Ling et al., 2015; Wieting et al., 2016). Ling
et al. (2015) used a BiLSTM to learn word vectors,
showing strong performance on language model-
ing and POS tagging. Melamud et al. (2016)
proposed context2vec, a BiLSTM based model
to learn context embedding of target words and
achieved state-of-the-art results on sentence com-
pletion and word sense disambiguation.

5 Conclusion

We have proposed a general MD framework for
MCL that can be analyzed in <root, POS, MC>
triplets. We have showed that the surface context
can be useful to MD, especially if combined with
morphological context. Our next step would be to
assess our claims on a larger number of typologi-
cally distant languages.

Acknowledgments

This work has been conducted under the tar-
geted program O.0743 (0115PK02473) of the
Committee of Science of the Ministry of Educa-
tion and Science of the Republic of Kazakhstan,
and the research grant 129-2017/022-2017 of the
Nazarbayev University.

The authors would like to thank Xiaoqing
Zheng for tremendously helpful discussions, as
well as Eray Yildiz and Zhenisbek Assylbekov for
the data sets used in this study and prompt replies
to all questions regarding those.

670



References

Zhenisbek Assylbekov, Jonathan Washington, Fran-
cis Tyers, Assulan Nurkas, Aida Sundetova, Aidana
Karibayeva, Balzhan Abduali, and Dina Amirova.
2016. A free/open-source hybrid morphological dis-
ambiguation tool for Kazakh. In TurCLing 2016.
pages 18–26.

Michael Collins. 2002. Discriminative Training Meth-
ods for Hidden Markov Models: Theory and Exper-
iments with Perceptron Algorithms. In Proceedings
of the ACL-02 Conference on Empirical Methods in
Natural Language Processing - Volume 10. Asso-
ciation for Computational Linguistics, Stroudsburg,
PA, USA, EMNLP ’02, pages 1–8.

Ronan Collobert, Jason Weston, Léon Bottou, Michael
Karlen, Koray Kavukcuoglu, and Pavel Kuksa.
2011. Natural Language Processing (Almost) from
Scratch. J. Mach. Learn. Res. 12:2493–2537.

Klaus Greff, Rupesh Kumar Srivastava, Jan Koutnı́k,
Bas R. Steunebrink, and Jürgen Schmidhuber.
2015. LSTM: A Search Space Odyssey. CoRR
abs/1503.04069.

Dilek Z. Hakkani-Tür, Kemal Oflazer, and Gökhan Tür.
2002. Statistical Morphological Disambiguation for
Agglutinative Languages. Computers and the Hu-
manities 36(4):381–410.

Gulshat Kessikbayeva and Ilyas Cicekli. 2016. A Rule
Based Morphological Analyzer and a Morphologi-
cal Disambiguator for Kazakh Language. Linguis-
tics and Literature Studies 4(4):96–104.

Wang Ling, Chris Dyer, Alan W. Black, Isabel Tran-
coso, Ramon Fermandez, Silvio Amir, Lus Marujo,
and Tiago Lus. 2015. Finding Function in Form:
Compositional Character Models for Open Vocabu-
lary Word Representation. In EMNLP. The Asso-
ciation for Computational Linguistics, pages 1520–
1530.

Aibek Makazhanov, Zhandos Yessenbayev, Islam
Sabyrgaliyev, Anuar Sharafudinov, and Olzhas
Makhambetov. 2014. On certain aspects of Kazakh
part-of-speech tagging. In Application of Informa-
tion and Communication Technologies (AICT), 2014
IEEE 8th International Conference on. pages 1–4.

Olzhas Makhambetov, Aibek Makazhanov, Islam
Sabyrgaliyev, and Zhandos Yessenbayev. 2015.
Data-Driven Morphological Analysis and Disam-
biguation for Kazakh. In Computational Linguistics
and Intelligent Text Processing - 16th International
Conference, CICLing 2015, Cairo, Egypt, April 14-
20, 2015, Proceedings Part I. pages 151–163.

Oren Melamud, Jacob Goldberger, and Ido Dagan.
2016. context2vec: Learning Generic Context Em-
bedding with Bidirectional LSTM. In CoNLL.

Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Cor-
rado, and Jeff Dean. 2013. Distributed Representa-
tions of Words and Phrases and their Composition-
ality. In C. J. C. Burges, L. Bottou, M. Welling,
Z. Ghahramani, and K. Q. Weinberger, editors, Ad-
vances in Neural Information Processing Systems
26, Curran Associates, Inc., pages 3111–3119.

Haşim Sak, Tunga Güngör, and Murat Saraçlar. 2007.
Morphological Disambiguation of Turkish Text with
Perceptron Algorithm. In Proceedings of CICLing
2007. volume LNCS 4394, pages 107–118.

Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky,
Ilya Sutskever, and Ruslan Salakhutdinov. 2014.
Dropout: A Simple Way to Prevent Neural Networks
from Overfitting. Journal of Machine Learning Re-
search 15:1929–1958.

Gulmira Tolegen, Alymzhan Toleu, and Zheng Xiao-
qing. 2016. Named Entity Recognition for Kazakh
Using Conditional Random Fields. In Proceedings
of the 4-th International Conference on Computer
Processing of Turkic Languages TurkLang 2016.
Izvestija KGTU im.I.Razzakova. TurkLang 2016,
pages 122–129.

John Wieting, Mohit Bansal, Kevin Gimpel, and Karen
Livescu. 2016. Charagram: Embedding Words and
Sentences via Character n-grams. In Proceedings of
the 2016 Conference on Empirical Methods in Natu-
ral Language Processing. Association for Computa-
tional Linguistics, Austin, Texas, pages 1504–1515.

Zhandos Yessenbayev and Aibek Makazhanov. 2016.
Character-based Feature Extraction with LSTM
Networks for POS-tagging Task. In Application
of Information and Communication Technologies
(AICT), 2016 IEEE 10th International Conference
on. pages 62–66.

Eray Yildiz, Caglar Tirkaz, H. Bahadir Sahin,
Mustafa Tolga Eren, and Omer Ozan Sonmez. 2016.
A Morphology-Aware Network for Morphological
Disambiguation. In Proceedings of AAAI. AAAI
Press, pages 2863–2869.

Deniz Yuret and Ferhan Türe. 2006. Learning Morpho-
logical Disambiguation Rules for Turkish. In Pro-
ceedings of the Main Conference on Human Lan-
guage Technology Conference of the North Amer-
ican Chapter of the Association of Computational
Linguistics. Association for Computational Linguis-
tics, Stroudsburg, PA, USA, HLT-NAACL ’06,
pages 328–334.

Xiaoqing Zheng, Hanyang Chen, and Tianyu Xu. 2013.
Deep learning for Chinese word segmentation and
POS tagging. In Proceedings of the 2013 Confer-
ence on Empirical Methods in Natural Language
Processing. Association for Computational Linguis-
tics, Seattle, Washington, USA, pages 647–657.

671


	Character-Aware Neural Morphological Disambiguation

