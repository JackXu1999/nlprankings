Pretraining Sentiment Classiﬁers with Unlabeled Dialog Data

Toru Shimizu1, Hayato Kobayashi1,2, and Nobuyuki Shimizu1

1Yahoo Japan Corporation

ftoshimiz,hakobaya,nobushimg@yahoo-corp.jp

2Riken AIP

Abstract

The huge cost of creating labeled train-
ing data is a common problem for su-
pervised learning tasks such as sentiment
classiﬁcation. Recent studies showed that
pretraining with unlabeled data via a lan-
guage model can improve the performance
of classiﬁcation models. In this paper, we
take the concept a step further by using
a conditional language model, instead of
a language model. Speciﬁcally, we ad-
dress a sentiment classiﬁcation task for a
tweet analysis service as a case study and
propose a pretraining strategy with unla-
beled dialog data (tweet-reply pairs) via
an encoder-decoder model. Experimen-
tal results show that our strategy can im-
prove the performance of sentiment clas-
siﬁers and outperform several state-of-the-
art strategies including language model
pretraining.

Introduction

1
Sentiment classiﬁcation is a task to predict a sen-
timent label, such as positive/negative, for a given
text and has been applied to many domains such
as movie/product reviews, customer surveys, news
comments, and social media. A common prob-
lem of this task is the lack of labeled training data
due to costly annotation work, especially for social
media without explicit sentiment feedback such as
review scores.

To overcome this problem, Dai and Le (2015)
recently proposed a semi-supervised sequence
learning framework, where a sentiment classiﬁer
based on recurrent neural networks (RNNs) is
trained with labeled data after initializing it with
the parameters of an RNN-based language model
pretrained with a large amount of unlabeled data.

The concept of their framework is simple but ef-
fective, and their work yielded many related stud-
ies of semi-supervised training based on sequence
modeling, as described in Section 4.

In this paper, we take their concept a step further
by using a conditional language model with unla-
beled dialog data (i.e., tweet-reply pairs) instead of
a language model with unpaired data1. An impor-
tant observation of the dialog data that underpins
our strategy is that the sentiment or mood in a mes-
sage often affects messages in reply to it. People
tend to write angry responses to angry messages,
empathetic replies to sad remarks, or congratula-
tory phrases to good news.

Our contributions are listed as follows.
(cid:15) We propose a pretraining strategy with unla-
beled dialog data (tweet-reply pairs) via an
encoder-decoder model for sentiment classiﬁers
(Section 2). To the best of our knowledge, our
proposal is the ﬁrst such proposal, as clariﬁed
in Section 4.
(cid:15) We report on a case study based on a costly la-
beled sentiment dataset of 99.5K items and a
large-scale unlabeled dialog dataset of 22.3M,
which were provided from a tweet analysis ser-
vice (Section 3.1).
(cid:15) Experimental results of sentiment classiﬁcation
show that our method outperforms the current
semi-supervised methods based on a language
model, autoencoder, and distant supervision, as
well as linear classiﬁers (Section 3.4).

2 Proposed Method
Our pretraining strategy simply consists of the fol-
lowing two steps:

1We use the term “conditional language model” in a nar-
row sense only for a model trained with explicit source-target
pairs, although both RNN-based language and autoencoder
models can generate a text from a real-valued context vector.

Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 764–770

Melbourne, Australia, July 15 - 20, 2018. c(cid:13)2018 Association for Computational Linguistics

764

1. Training a dialog (encoder-decoder) model us-
ing unlabeled dialog data (tweet-reply pairs) as
pretraining.

2. Training a

classiﬁer

sentiment

(encoder-
labeler) model using labeled sentiment data
(tweet-label pairs) after
initializing its en-
coder part with the encoder parameters of the
encoder-decoder model.

The encoder-decoder model is a conditional lan-
guage model that predicts a correct output se-
quence from an input sequence (Sutskever et al.,
2014). This model consists of two RNNs: an en-
coder and decoder. The encoder extracts a context
of the input sequence as a real-valued vector, and
the decoder predicts the output sequences from the
context individually.

Our classiﬁer forms an encoder-labeler struc-
ture, which consists of the above encoder and a la-
beler that predicts a sentiment label from the con-
text. Note that the encoder of the classiﬁer is ﬁne-
tuned with labeled data, as in (Dai and Le, 2015).
The main difference between their approach and
ours is that we examine paired (dialog) data for
pretraining, while they only showed the usefulness
of pretraining with unpaired data.

3 Experiments

3.1 Datasets
We used two datasets, a dialog dataset for pre-
training the encoder-decoder model and a sen-
timent dataset for training (ﬁne-tuning) the sen-
timent classiﬁer, as shown in Table 1. Those
datasets were provided by Yahoo! JAPAN, which
is the largest portal site in Japan.

The dialog dataset contains about 22.3 million
tweet-reply pairs extracted from Twitter Firehose
data.
In its preprocessing, we ﬁltered out spam
and bot posts by using user-level signals such as
the follower count, the friend count, the favorite
count, and whether a proﬁle image is set or not.
Also, we replaced all the URLs in the text with
“[u]” and all the user mentions with “[m]”, consid-
ering them as noise. The rest of the text was used

Dialog
Sentiment

Train Valid

Test
22,300,000 10,000 50,000
4,000 15,000

80,591

Table 1: Details of dialog and sentiment datasets

The sentiment dataset

as it was. On average, source and target (or re-
ply) tweets after preprocessing were 31.5 and 27.8
characters long, respectively. While redistribution
of tweets is prohibited, we are planning to publi-
cize tweet IDs of this dataset for reproducibility.2
includes about 100K
tweets with manually annotated three-class sen-
timent
labels: positive, negative, and
neutral.
The breakdown of positive,
negative, and neutral in the training set was
15.0, 18.6, and 66.4%, respectively. Note that the
tweets were sampled separately from those of the
dialog dataset. The procedure for text preprocess-
ing was the same with that of the dialog dataset.
The average length of the tweets after preprocess-
ing was 17 characters. Each tweet was judged
by a majority vote of three experienced editors in
the company providing the sentiment-analysis ser-
vice. The inter-annotator agreement ratio assessed
with Fleiss’ (cid:20) was 0.495. The overall annotation
work took roughly 300 person-days. This means
that the cost is at least 24K dollars, 8 hours (cid:2)
300 days (cid:2) legal minimum wage in Japan 10 dol-
lars/hour. Considering that the in-house annota-
tors are well-educated, skilled proper employees,
the actual cost would be much higher than this
rough estimate and much more costly than collect-
ing unlabeled dialog data. In addition, the annota-
tors had gone through a few days of training to
become able to appropriately judge the sentiment
before they got down to actual annotation work,
but the number, 300 person-days, does not include
the time for this training.

3.2 Model and Training
the dialog (encoder-decoder)
The settings of
model are as follows.
In both the encoder and
decoder, the size of the word-embedding layer is
256 and that of the LSTM-RNN hidden layer is
1024. The size of the output layer is 4000, which
is the same as the (character-based) vocabulary
size.3. The encoder and decoder share these hyper-
parameters as well as the parameters themselves
(that is, with regard to the embedding layer and

2The tweet IDs will be provided from https://research-

lab.yahoo.co.jp/en/software/ .

3We used a character-based model since it performed
better than word-based models in our preliminary experi-
ments. Existing morphological analyzers needed for word-
based models have usually been trained by formal text such
as that of newspapers and seem not suitable to highly collo-
quial text seen in tweets, which often includes emoticons and
emoji.

765

recurrent layer). The total number of parameters
is 8.9 million.

the

The

sentiment

settings of

classiﬁer
(encoder-labeler) model are as follows.
The
encoder part has the same structure and hyper-
parameters as that of the dialog model, making
them compatible for transferring learned param-
eters. We reused the dialog model’s dictionaries
in the classiﬁer model so that the two models
could process tweet texts consistently. The labeler
consists of a fully connected layer and soft max
nonlinearity.

The models were trained with ADADELTA
(Zeiler, 2012) with a mini-batch size of 64. The
dialog model was trained in ﬁve epochs, and the
classiﬁer model was tuned with the early-stopping
strategy, which stops training when the validation
accuracy drops. For ADADELTA’s parameters,
we ﬁxed the learning rate to 1.0, decay rate (cid:26) to
(cid:0)6 for all
0.95, and smoothing constant ϵ to 10
training sessions. We evaluated validation costs
ten times per epoch and selected the model with
the lowest validation cost. The training took 15.9
days on 1 GPU with 7 TFLOPS computational
power.

3.3 Compared Models
We compared the following eight models: non-
pretrained (Default), proposed dialog pretrain-
ing (Dial), current pretraining with unpaired data
(Lang, SeqAE) and pseudo labeled data (Emo2M,
Emo6M), and classical linear learners (LogReg,
LinSVM). The details of these models are given
below.
(cid:15) Default: Trained without pretraining by exe-
cuting only Step 2 in Section 2.
(cid:15) Dial: Pretrained with the dialog model de-
scribed in Section 2.
(cid:15) Lang, SeqAE: Pretrained with the language
model and autoencoder model proposed in (Dai
and Le, 2015). The language model is the de-
coder part of the encoder-decoder model using
a zero vector as the initial hidden layer value,
and the autoencoder model is the same structure
of the encoder-decoder model, where input and
output are the same. To make the comparison
as fair as possible, we used the reply-side of the
dialog dataset for pretraining Lang and SeqAE
so that the same supervision information on the

basis of the same tweet-reply pairs would be ap-
plied to Lang, SeqAE, and Dial. The num-
ber of their pretraining epochs was also equal to
that of Dial.
(cid:15) Emo2M, Emo6M: Pretrained with pseudo la-
beled data (2M, 6M) based on manually col-
lected emoticons, which consist of 120 posi-
tive emoticons and 116 negative ones. This
technique is also known as distant-supervision.
These pseudo labels were annotated by extract-
ing tweets including one of those emoticons
from our dialog data and another 92M tweets.
Pretraining was conducted via a two-class sen-
timent classiﬁer, which is a similar model
to Default, since uncertain tweets without
emoticons are not always neutral. We con-
ﬁrmed that this two-class classiﬁer can reach
more than 90% test accuracy on the emoticon-
based test dataset. After pretraining, the param-
eters of the encoder part were transfered to the
ﬁnal classiﬁer model.
(cid:15) LogReg, LinSVM: Logistic regression and
linear support vector machine (SVM) models
of LIBLINEAR (Fan et al., 2008) with bag-of-
words features, which consist of 50K unigrams
(w/o stopwords), 50K bigrams, and 233 emoti-
cons. These features are based on a state-of-
the-art system (Mohammad et al., 2013) that
performed best in the SEMEVAL competition
(Nakov et al., 2013) and was actually used in
the tweet analysis service of the data-providing
company. The best parameters were found
through a grid-search on the validation set.

3.4 Results
Table 2 shows the macro-average F-measure re-
sults of the compared models in Section 3.3 on
the sentiment classiﬁcation task when varying data
size (5K to 80K). Each value is the average of
ﬁve trials with different random seeds for each set-
ting, and a value of a trial is the macro-average
of F-measure values of three sentiment classes.
The ﬁrst row (Default) shows the default sen-
timent classiﬁer model without pretraining. The
second row block (Dial to Emo6M) shows the re-
sults of the same training as Default after pre-
training via different models, while the third block
shows those of linear classiﬁers (non-RNN mod-
els). The supplemental materials also include the
results measured by accuracy.

766

5K

40K

20K

10K

80K
Default 0.517 0.590 0.623 0.653 0.673
0.665y 0.685y 0.702y 0.717y 0.738y
Dial
0.653 0.674 0.692 0.707 0.726
Lang
0.568 0.598 0.626 0.649 0.677
SeqAE
0.482 0.532 0.579 0.626 0.664
Emo2M
0.484 0.517 0.565 0.613 0.650
Emo6M
0.577 0.609 0.631 0.648 0.675
LogReg
0.582 0.610 0.627 0.637 0.648
LinSVM

Table 2: Macro-average F-measure of sentiment
classiﬁcation of each model versus labeled data
size. Dial is our proposed method, and y in
its row indicates statistically signiﬁcant difference
from the corresponding value of Lang (p <
0:05).

Comparing Dial with the other models, we
can see that our pretraining strategy with dialog
data consistently outperformed all the other mod-
els: state-of-the-art pretraining strategies with un-
paired unlabeled data (Lang, SeqAE) and pseudo
labeled data (Emo2M, Emo6M), as well as linear
learners (LogReg, LinSVM). This indicates that
unlabeled dialog data (tweet-reply pairs) have use-
ful information for sentiment classiﬁers, as ex-
pected in Section 1. In fact, we observed that the
pretrained encoder-decoder model seems to gener-
ate an appropriate reply, on which the sentiment on
the input tweet is well reﬂected. For example, the
reply “:(” was generated for the input tweet “I’m
sorry to hear that” (see supplementary material for
more examples).

Lang also outperformed well but did not over-
take Dial. The differences between Dial and
Lang are statistically signiﬁcant4 for all ﬁve train-
ing dataset sizes.
Interestingly, SeqAE was not
so effective like Dial, despite their model struc-
tures are basically the same. This implies that it is
practically important to ﬁnd appropriate data for
pretraining, such as dialog data for sentiment clas-
siﬁcation.

As for the results of distant supervision with
emoticons, both Emo2M and Emo6M performed
worse than Default, and increasing the dataset
size did not change the situation. The reason why
these models did not perform as well as other
pretraining-based models is considered to be noisy
labels, especially in negative ones. We illustrate
two instances in the Emo2M training data that in-
clude an emoticon that is usually negative emoti-
4Under the signiﬁcance level of 0.05 with two-tailed t-test

assuming unequal variances.

con but can be considered positive:
(cid:15) 美人すぎるよ可愛い（; ;）, “She is so beautiful,
cute (crying emoticon)”
(cid:15) うらやましいです。おめでとうございます
orz, “I envy you. Congratulations (bow-the-
knee emoticon)”

Comparing Default with LogReg and
LinSVM, we can see that the linear models per-
formed better than the default RNN model without
pretraining, when the labeled data size is less than
or equal to 20K. However, looking at the results of
Dial, our method improved Default even for
these cases (5K to 20K), and Dial clearly out-
performed the linear models. This means that pre-
training is useful especially on the situation where
the labeled data size is limited.

4 Related Work
After Dai and Le (2015) proposed the framework
of semi-supervised sequence learning, there have
been several attempts to extend sequence learn-
ing models for different tasks to semi-supervised
settings. Cheng et al. (2016) and Ramachandran
et al. (2017) studied semi-supervised training of
machine translation models via an autoencoder
model and language model, respectively. They
also used paired data (parallel corpora), but un-
supervised training was conducted with reason-
able monolingual corpora to compensate for costly
parallel corpora, which is opposite to our set-
ting. Zhou et al. (2016a,b) proposed to use par-
allel corpora for adapting the sentiment resources
in a resource-rich language to a resource-poor lan-
guage. Their purpose was completely different
from ours, since making parallel corpora is also
costly. The other studies include semi-supervised
extensions for predicting the property values of
Wikipedia (Hewlett et al., 2017), detecting medi-
cal conditions from heart rate data (Ballinger et al.,
2018), and morphological reinﬂection of inﬂected
words (e.g., “playing” to “played”). They did not
use paired-text data to leverage their tasks.

Our method can be regarded as a general ver-
sion of distant supervision since we assume that
a reply includes the label information of the cor-
responding tweet. There have been many studies
about distant supervision for sentiment analysis
(Read, 2005; Go et al., 2009; Davidov et al., 2010;
Purver and Battersby, 2012; Mohammad et al.,
2013; Tang et al., 2014; dos Santos and Gatti,

767

2014; Severyn and Moschitti, 2015; Deriu et al.,
2016; M¨uller et al., 2017), but they basically fo-
cused on how to use emoticons and hashtags to
leverage performance. One exception is the study
by (Pool and Nissim, 2016), in which Facebook
reactions were used for distant supervision. Their
approach is similar to ours using tweet-reply pairs,
but our method is more general since they only
used six reply categories (i.e., like, love, haha,
wow, sad, and angry), not text replies.

There have been a few studies on sentiment
classiﬁcation in dialogue data (Bertero and Fung,
2016; Bertero et al., 2016). These studies involved
sentiment classiﬁcation based on dialog contexts,
which means that they used labeled dialog data,
while we used unlabeled dialog data. For tweet
data, several studies used reply-features for senti-
ment classiﬁcation of tweets (Barbosa and Feng,
2010; Jiang et al., 2011; Vanzo et al., 2014; Bam-
man and Smith, 2015; Ren et al., 2016; Castellucci
et al., 2016). However, they used replies as la-
beled data for sentiment classiﬁcation, not unla-
beled data for pretraining.

5 Conclusion
We proposed a pretraining strategy with dialog
data for sentiment classiﬁers. The experimental
results showed that our strategy clearly outper-
formed the existing pretraining with unpaired un-
labeled data via language modeling and pseudo
labeled data via distant supervision, as well as
linear classiﬁers.
In the future, we will investi-
gate whether or not we can use other paired data
for pretraining of classiﬁcation tasks. For exam-
ple, we expect that news article-comment pairs are
useful for predicting fake news detection and that
question-answer pairs of Q&A sites are useful for
recommending questions for answering.

References
Brandon Ballinger,

Johnson Hsieh, Avesh Singh,
Nimit Sohoni, Jack Wang, Geoffrey H. Tison, Gre-
gory M. Marcus, Jose M. Sanchez, Carol Maguire,
Jeffrey E. Olgin, and Mark J. Pletcher. 2018.
DeepHeart: Semi-Supervised Sequence Learning
In Proceed-
for Cardiovascular Risk Prediction.
the Thirty-Second AAAI Conference on
ings of
Artiﬁcial Intelligence (AAAI 2018).
To appear.
https://arxiv.org/abs/1802.02511.

David Bamman and Noah A. Smith. 2015. Contex-
In Pro-
tualized Sarcasm Detection on Twitter.
ceedings of the Ninth International Conference on

Web and Social Media (ICWSM 2015). pages 574–
577.
http://www.aaai.org/ocs/index.php/ICWSM/
ICWSM15/paper/view/10538.

Luciano Barbosa and Junlan Feng. 2010. Robust Sen-
timent Detection on Twitter from Biased and Noisy
In Proceedings of the 21st International
Data.
Conference on Computational Linguistics (COLING
2010). Coling 2010 Organizing Committee, pages
36–44.
http://www.aclweb.org/anthology/C10-
2005.

Dario Bertero and Pascale Fung. 2016. A Long Short-
Term Memory Framework for Predicting Humor in
In Proceedings of the 2016 Confer-
Dialogues.
ence of the North American Chapter of the Associ-
ation for Computational Linguistics: Human Lan-
guage Technologies (NAACL-HLT 2016). Associa-
tion for Computational Linguistics, pages 130–135.
http://www.aclweb.org/anthology/N16-1016.

Dario Bertero, Farhad Bin Siddique, Chien-Sheng
Wu, Yan Wan, Ricky Ho Yin Chan, and Pas-
cale Fung. 2016.
Real-Time Speech Emotion
and Sentiment Recognition for Interactive Dia-
In Proceedings of the 2016 Con-
logue Systems.
ference on Empirical Methods in Natural Lan-
guage Processing (EMNLP 2016). Association
for Computational Linguistics, pages 1042–1047.
https://aclweb.org/anthology/D16-1110.

Giuseppe Castellucci, Danilo Croce, and Roberto
Basili. 2016. Context-aware Convolutional Neu-
ral Networks for Twitter Sentiment Analysis in
In Proceedings of Third Italian Confer-
Italian.
ence on Computational Linguistics (CLiC-it 2016)
& Fifth Evaluation Campaign of Natural Language
Processing and Speech Tools for Italian. Final
Workshop (EVALITA 2016). http://ceur-ws.org/Vol-
1749/paper 029.pdf.

Yong Cheng, Wei Xu, Zhongjun He, Wei He,
Hua Wu, Maosong Sun, and Yang Liu. 2016.
Semi-Supervised Learning for Neural Machine
the 54th An-
Translation.
nual Meeting of
the Association for Computa-
tional Linguistics (ACL 2016). Association for
Computational Linguistics,
pages 1965–1974.
http://www.aclweb.org/anthology/P16-1185.

In Proceedings of

Andrew M Dai and Quoc V Le. 2015.

Semi-
In Advances
supervised Sequence Learning.
Information Processing Systems 28
in Neural
(NIPS 2015), Curran Associates,
Inc., pages
3079–3087. http://papers.nips.cc/paper/5949-semi-
supervised-sequence-learning.pdf.

Dmitry Davidov, Oren Tsur, and Ari Rappoport.
Enhanced Sentiment Learning Using
2010.
In Proceed-
Twitter Hashtags and Smileys.
International Conference on
ings of
Computational Linguistics (COLING 2010). Col-
ing 2010 Organizing Committee, pages 241–249.
http://www.aclweb.org/anthology/C10-2028.

the 21st

768

Jan Deriu, Maurice Gonzenbach, Fatih Uzdilli, Au-
relien Lucchi, Valeria De Luca, and Martin Jaggi.
2016. SwissCheese at SemEval-2016 Task 4: Sen-
timent Classiﬁcation Using an Ensemble of Convo-
lutional Neural Networks with Distant Supervision.
In Proceedings of the 10th International Workshop
on Semantic Evaluation (SemEval 2016). Associ-
ation for Computational Linguistics, pages 1124–
1128. http://www.aclweb.org/anthology/S16-1173.

Cicero dos Santos and Maira Gatti. 2014. Deep Con-
volutional Neural Networks for Sentiment Analysis
In Proceedings of the 25th Inter-
of Short Texts.
national Conference on Computational Linguistics
(COLING 2014). Dublin City University and Asso-
ciation for Computational Linguistics, pages 69–78.
http://www.aclweb.org/anthology/C14-1008.

Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-
Rui Wang, and Chih-Jen Lin. 2008. LIBLINEAR:
A Library for Large Linear Classiﬁcation. Journal
of Machine Learning Research 9:1871–1874.

Sentiment Classiﬁcation

Alec Go, Richa Bhayani, and Lei Huang. 2009.
using Dis-
Stan-
Project.

Twitter
tant Supervision.
ford Digital
https://cs.stanford.edu/people/alecmgo/papers/
TwitterDistantSupervision09.pdf.

Technologies

Technical

Library

report,

Daniel Hewlett, Llion Jones, Alexandre Lacoste,
and izzeddin gur. 2017.
Accurate Supervised
and Semi-Supervised Machine Reading for Long
the 2017 Con-
Documents.
ference on Empirical Methods in Natural Lan-
guage Processing (EMNLP 2017). Association
for Computational Linguistics, pages 2011–2020.
https://www.aclweb.org/anthology/D17-1214.

In Proceedings of

the 49th Annual Meeting of

Long Jiang, Mo Yu, Ming Zhou, Xiaohua Liu,
and Tiejun Zhao. 2011. Target-dependent Twit-
In Proceedings
ter Sentiment Classiﬁcation.
of
the Associa-
tion for Computational Linguistics: Human Lan-
guage Technologies (ACL-HLT 2011). Associa-
tion for Computational Linguistics, pages 151–160.
http://www.aclweb.org/anthology/P11-1016.

Saif Mohammad, Svetlana Kiritchenko, and Xiaodan
Zhu. 2013. NRC-Canada: Building the State-
of-the-Art in Sentiment Analysis of Tweets.
In
Proceedings of the Seventh International Workshop
on Semantic Evaluation (SemEval 2013). Associa-
tion for Computational Linguistics, pages 321–327.
http://www.aclweb.org/anthology/S13-2053.

Simon M¨uller, Tobias Huonder, Jan Deriu, and Mark
Cieliebak. 2017. TopicThunder at SemEval-2017
Task 4: Sentiment Classiﬁcation Using a Convo-
lutional Neural Network with Distant Supervision.
In Proceedings of the 11th International Workshop
on Semantic Evaluation (SemEval 2017). Associa-
tion for Computational Linguistics, pages 766–770.
http://www.aclweb.org/anthology/S17-2129.

Preslav Nakov, Sara Rosenthal, Zornitsa Kozareva,
Veselin Stoyanov, Alan Ritter, and Theresa Wilson.
2013. SemEval-2013 Task 2: Sentiment Analysis in
Twitter. In Proceedings of the Seventh International
Workshop on Semantic Evaluation (SemEval 2013).
Association for Computational Linguistics, pages
312–320.
http://www.aclweb.org/anthology/S13-
2052.

Chris Pool and Malvina Nissim. 2016. Distant super-
vision for emotion detection using facebook reac-
tions. In Proceedings of the Workshop on Compu-
tational Modeling of People’s Opinions, Personal-
ity, and Emotions in Social Media (PEOPLES). The
COLING 2016 Organizing Committee, pages 30–
39. http://aclweb.org/anthology/W16-4304.

Matthew Purver and Stuart Battersby. 2012.

Ex-
perimenting with Distant Supervision for Emotion
In Proceedings of the 13th Confer-
Classiﬁcation.
ence of the European Chapter of the Association
for Computational Linguistics (ACL 2012). Associa-
tion for Computational Linguistics, pages 482–491.
http://www.aclweb.org/anthology/E12-1049.

Prajit Ramachandran, Peter Liu,

and Quoc Le.
Unsupervised Pretraining for Sequence
2017.
the
to Sequence Learning.
2017 Conference on Empirical Methods in Natu-
ral Language Processing (EMNLP 2017). Associa-
tion for Computational Linguistics, pages 383–391.
https://www.aclweb.org/anthology/D17-1039.

In Proceedings of

Jonathon Read. 2005.

Using Emoticons to Re-
duce Dependency in Machine Learning Techniques
In Proceedings of
for Sentiment Classiﬁcation.
the ACL Student Research Workshop. Associa-
tion for Computational Linguistics, pages 43–48.
http://www.aclweb.org/anthology/P/P05/P05-2008.

Yafeng Ren, Yue Zhang, Meishan Zhang,

and
Context-sensitive Twit-
Donghong Ji. 2016.
Sentiment Classiﬁcation Using Neural
ter
the Thirtieth
Network.
Intelligence
AAAI Conference
(AAAI 2016). AAAI Press,
pages 215–221.
http://dl.acm.org/citation.cfm?id=3015812.3015844.

In Proceedings of
on Artiﬁcial

Aliaksei Severyn and Alessandro Moschitti. 2015.
UNITN: Training Deep Convolutional Neural Net-
work for Twitter Sentiment Classiﬁcation.
In
Proceedings of
the 9th International Workshop
on Semantic Evaluation (SemEval 2015). Associa-
tion for Computational Linguistics, pages 464–469.
http://www.aclweb.org/anthology/S15-2079.

Ilya Sutskever, Oriol Vinyals, and Quoc V Le.
2014.
Sequence to sequence learning with
In-
neural networks.
formation Processing Systems 27 (NIPS 2014),
Curran Associates,
3104–3112.
http://papers.nips.cc/paper/5346-sequence-to-
sequence-learning-with-neural-networks.pdf.

In Advances in Neural

pages

Inc.,

Duyu Tang, Furu Wei, Nan Yang, Ming Zhou,
Learning

and Bing Qin. 2014.

Ting Liu,

769

Sentiment-Speciﬁc Word Embedding for Twitter
In Proceedings of the
Sentiment Classiﬁcation.
52nd Annual Meeting of the Association for Com-
putational Linguistics (ACL 2014). Association
for Computational Linguistics, pages 1555–1565.
http://www.aclweb.org/anthology/P14-1146.

Andrea Vanzo, Danilo Croce, and Roberto Basili. 2014.
A context-based model for Sentiment Analysis in
In Proceedings of the 25th International
Twitter.
Conference on Computational Linguistics (COL-
ING 2014). Dublin City University and Association
for Computational Linguistics, pages 2345–2354.
http://www.aclweb.org/anthology/C14-1221.

Matthew D. Zeiler. 2012. ADADELTA: An Adaptive

Learning Rate Method. CoRR abs/1212.5701.

Xinjie Zhou, Xiaojun Wan, and Jianguo Xiao. 2016a.
Attention-based LSTM Network for Cross-Lingual
In Proceedings of the
Sentiment Classiﬁcation.
2016 Conference on Empirical Methods in Natural
Language Processing (EMNLP 2016). Association
for Computational Linguistics, Austin, Texas, pages
247–256. https://aclweb.org/anthology/D16-1024.

Xinjie Zhou, Xiaojun Wan, and Jianguo Xiao. 2016b.
Cross-Lingual Sentiment Classiﬁcation with Bilin-
In Pro-
gual Document Representation Learning.
ceedings of the 54th Annual Meeting of the Asso-
ciation for Computational Linguistics (ACL 2016).
Association for Computational Linguistics, pages
1403–1412. http://www.aclweb.org/anthology/P16-
1133.

770

