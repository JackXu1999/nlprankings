



















































Investigating LSTMs for Joint Extraction of Opinion Entities and Relations


Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 919–929,
Berlin, Germany, August 7-12, 2016. c©2016 Association for Computational Linguistics

Investigating LSTMs for Joint Extraction
of Opinion Entities and Relations

Arzoo Katiyar and Claire Cardie
Department of Computer Science

Cornell University
Ithaca, NY, 14853, USA

arzoo, cardie@cs.cornell.edu

Abstract
We investigate the use of deep bi-
directional LSTMs for joint extraction of
opinion entities and the IS-FROM and IS-
ABOUT relations that connect them — the
first such attempt using a deep learning
approach. Perhaps surprisingly, we find
that standard LSTMs are not competitive
with a state-of-the-art CRF+ILP joint in-
ference approach (Yang and Cardie, 2013)
to opinion entities extraction, perform-
ing below even the standalone sequence-
tagging CRF. Incorporating sentence-level
and a novel relation-level optimization,
however, allows the LSTM to identify
opinion relations and to perform within 1–
3% of the state-of-the-art joint model for
opinion entities and the IS-FROM relation;
and to perform as well as the state-of-the-
art for the IS-ABOUT relation — all with-
out access to opinion lexicons, parsers and
other preprocessing components required
for the feature-rich CRF+ILP approach.

1 Introduction

There has been much research in recent years in
the area of fine-grained opinion analysis where
the goal is to identify subjective expressions in
text along with their associated sources and tar-
gets. More specifically, fine-grained opinion anal-
ysis aims to identify three types of opinion enti-
ties:

• opinion expressions, O, which are direct
subjective expressions (i.e., explicit mentions
of otherwise private states or speech events
expressing private states (Wiebe and Cardie,
2005));

• opinion targets, T , which are the entities or
topics that the opinion is about; and

• opinion holders, H , which are the entities
expressing the opinion.

In addition, the task involves identifying the IS-
FROM and IS-ABOUT relations between an opinion
expression and its holder and target, respectively.
In the sample sentences, numerical subscripts in-
dicate an IS-FROM or IS-ABOUT relation.

S1 [The sale]T1 [infuriated]O1 [Beijing]H1,2
which [regards]O2 [Taiwan]T2 an integral
part of its territory awaiting reunification, by
force if necessary.

S2 “[Our agency]T1,H2 [seriously needs]O2
[equipment for detecting drugs]T2 ,” [he]H1
[said]O1 .

In S1, for example, “infuriated” indicates that
there is an (negative) opinion from “Beijing” re-
garding “the sale.”1

Traditionally, the task of extracting opinion
entities and opinion relations was handled in a
pipelined manner, i.e., extracting the opinion ex-
pressions first and then extracting opinion tar-
gets and opinion holders based on their syntac-
tic and semantic associations with the opinion ex-
pressions (Kim and Hovy, 2006; Kobayashi et al.,
2007). More recently, methods that jointly in-
fer the opinion entity and relation extraction tasks
(e.g., using Integer Linear Programming (ILP))
have been introduced (Choi et al., 2006; Yang and
Cardie, 2013) and show that the existence of opin-
ion relations provides clues for the identification
of opinion entities and vice-versa, and thus results
in better performance than a pipelined approach.
However, the success of these methods depends
critically on the availability of opinion lexicons,
dependency parsers, named-entity taggers, etc.

1This paper does not attempt to determine the sentiment,
i.e., the positive or negative polarity, of an opinion.

919



Alternatively, neural network-based methods
have been employed. In these approaches, the
required latent features are automatically learned
as dense vectors of the hidden layers. Liu et al.
(2015), for example, compare several variations
of recurrent neural network methods and find that
long short-term memory networks (LSTMs) per-
form the best in identifying opinion expressions
and opinion targets for the specific case of prod-
uct/service reviews.

Motivated by the recent success of LSTMs on
this and other problems in NLP, we investigate
here the use of deep bi-directional LSTMs for joint
extraction of opinion expressions, holders, targets
and the relations that connect them. This is the
first attempt to handle the full opinion entity and
relation extraction task using a deep learning ap-
proach.

In experiments on the MPQA dataset for opin-
ion entities (Wiebe and Cardie, 2005; Wilson,
2008), we find that standard LSTMs are not com-
petitive with the state-of-the-art CRF+ILP joint
inference approach of Yang and Cardie (2013),
performing below even the standalone sequence-
tagging CRF. Inspired by Huang et al. (2015),
we show that incorporating sentence-level, and
our newly proposed relation-level optimization,
allows the LSTM to perform within 1–3% of the
ILP joint model for all three opinion entity types
and to do so without access to opinion lexicons,
parsers or other preprocessing components.

For the primary task of identifying opinion en-
tities together with their IS-FROM and IS-ABOUT
relations, we show that the LSTM with sentence-
and relation-level optimizations outperforms an
LSTM baseline that does not employ joint infer-
ence. When compared to the CRF+ILP-based
joint inference approach, the optimized LSTM
performs slightly better for the IS-ABOUT2 rela-
tion and within 3% for the IS-FROM relation.

In the sections that follow, we describe: related
work (Section 2) and the multi-layer bi-directional
LSTM (Section 3); the LSTM extensions (Sec-
tion 4); the experiments on the MPQA corpus
(Sections 5 and 6) and error analysis (Section 7).

2Target and IS-ABOUT relation identification is one im-
portant aspect of opinion analysis that hasn’t been much ad-
dressed in previous work and has proven to be difficult for
existing methods.

2 Related Work

LSTM-RNNs (Hochreiter and Schmidhuber,
1997) have recently been applied to many se-
quential modeling and prediction tasks, such
as machine translation (Bahdanau et al., 2014;
Sutskever et al., 2014), speech recognition
(Graves et al., 2013), NER (Hammerton, 2003).
The bi-directional variant of RNNs has been
found to perform better as it incorporates infor-
mation from both the past and the future (Schuster
and Paliwal, 1997; Graves et al., 2013). Deep
RNNs (stacked RNNs) (Schmidhuber, 1992; Hihi
and Bengio, 1996) capture more abstract and
higher-level representation in different layers
and benefit sequence modeling tasks (İrsoy and
Cardie, 2014). Collobert et al. (2011) found
that adding dependencies between the tags in
the output layer improves the performance of
Semantic Role Labeling task. Later, Huang et al.
(2015) also found that adding a CRF layer on top
of bi-directional LSTMs to capture these depen-
dencies can produce state-of-the-art performance
on part-of-speech (POS), chunking and NER.

For fine-grained opinion extraction, earlier
work (Wilson et al., 2005; Breck et al., 2007; Yang
and Cardie, 2012) focused on extracting subjective
phrases using a CRF-based approach from open-
domain text such as news articles. Choi et al.
(2005) extended the task to jointly extract opin-
ion holders and these subjective expressions. Yang
and Cardie (2013) proposed a ILP-based joint-
inference model to jointly extract the opinion en-
tities and opinion relations, which performed bet-
ter than the pipelined based approaches (Kim and
Hovy, 2006).

In the neural network domain, İrsoy and Cardie
(2014) proposed a deep bi-directional recurrent
neural network for identifying subjective expres-
sions, outperforming the previous CRF-based
models. Irsoy and Cardie (2013) additionally pro-
posed a bi-directional recursive neural network
over a binary parse tree to jointly identify opinion
entities, but performed significantly worse than
the feature-rich CRF+ILP approach of Yang and
Cardie (2013). Liu et al. (2015) used several vari-
ants of recurrent neural networks for joint opin-
ion expression and aspect/target identification on
customer reviews for restaurants and laptops, out-
performing the feature-rich CRF based baseline.
In the product reviews domain, however, the opin-
ion holder is generally the reviewer and the task

920



does not involve identification of relations be-
tween opinion entities. Hence, standard LSTMs
are applicable in this domain. None of the above
neural network based models can jointly model
opinion entities and opinion relations.

In the relation extraction domain, several neu-
ral networks have been proposed for relation clas-
sification, such as RNN-based models (Socher et
al., 2012) and LSTM-based models (Xu et al.,
2015). These models depend on constituent or
dependency tree structures for relation classifica-
tion, and also do not model entities jointly. Re-
cently, Miwa and Bansal (2016) proposed a model
to jointly represent both entities and relations with
shared parameters, but it is not a joint-inference
framework.

3 Methodology

For our task, we propose the use of multi-layer
bi-directional LSTMs, a type of recurrent neural
network. Recurrent neural networks have recently
been used for modeling sequential tasks. They
are capable of modeling sequences of arbitrary
length by repetitive application of a recurrent unit
along the tokens in the sequence. However, re-
current neural networks are known to have sev-
eral disadvantages like the problem of vanishing
and exploding gradients. Because of these prob-
lems, it has been found that recurrent neural net-
works are not sufficient for modeling long term de-
pendencies. Hochreiter and Schmidhuber (1997),
thus proposed long short term memory (LSTMs),
a variant of recurrent neural networks.

3.1 Long Short Term Memory (LSTM)
Long short term memory networks are capable of
learning long-term dependencies. The recurrent
unit is replaced by a memory block. The mem-
ory block contains two cell states – memory cell
Ct and hidden state ht; and three multiplicative
gates – input gate it, forget gate ft and output gate
ot. These gates regulate the addition or removal of
information to the cell state thus overcoming van-
ishing and exploding gradients.

ft = σ(Wfxt + Ufht−1 + bf )
it = σ(Wixt + Uiht−1 + bi)

The forget gate ft and input gate it above decides
what part of the information we are going to throw
away from the cell state and what new information
we are going to store in the cell state. The sigmoid

outputs a number between 0 and 1 where 0 im-
plies that the information is completely lost and 1
means that the information is completely retained.

C̃t = tanh(Wcxt + Ucht−1 + bc)

Ct = it ∗ C̃t + ft ∗ Ct−1
Thus, the intermediate cell state C̃t and previous
cell stateCt−1 are used to update the new cell state
Ct.

ot = σ(Woxt + Uoht−1 + VoCt + bo)
ht = ot ∗ tanh(Ct)

Next, we update the hidden state ht based on the
output gate ot and the cell state Ct. We pass both
the cell state Ct and the hidden state ht to the next
time step.

3.2 Multi-layer Bi-directional LSTM
In sequence tagging problems, it has been found
that only using past information for computing the
hidden state ht may not be sufficient. Hence, pre-
vious works (Graves et al., 2013; İrsoy and Cardie,
2014) proposed the use of bi-directional recurrent
neural networks for speech and NLP tasks, respec-
tively. The idea is to also process the sequence in
the backward direction. Hence, we can compute
the hidden state

−→
ht in the forward direction and

←−
ht

in the backward direction for every token.
Also, in more traditional feed-forward net-

works, deep networks have been found to learn
abstract and hierarchical representations of the in-
put in different layers (Bengio, 2009). The multi-
layer LSTMs have been proposed (Hermans and
Schrauwen, 2013) to capture long-term dependen-
cies of the input sequences in different layers.

For the first hidden layer, the computation pro-
ceeds similar to that described in Section 3.1.
However, for higher hidden layers i the input to
the memory block is the hidden state and memory
cell from the previous layer i − 1 instead of the
input vector representation.

For this paper, we only use the hidden state from
the last layer L to compute the output state yt.

zt =
−→
V
−→
ht

(L) +
←−
V
←−
ht

(L) + c
yt = g(zt)

4 Network Training

For our problem, we wish to predict a label y from
a discrete set of classes Y for every word in a sen-
tence. As is the norm, we train the network by

921



maximizing the log-likelihood∑
(x,y)∈T

log p(y|x, θ)

over the training data T, with respect to the pa-
rameters θ, where x is the input sentence and y is
the corresponding tag sequence. We propose three
alternatives for the log-likelihood computation.

4.1 Word-Level Log-Likelihood (WLL)
We first formulate a word-level log-likelihood
(WLL) (adapted from Collobert et al. (2011))
that considers all words in a sentence indepen-
dently. We interpret the score zt corresponding
to the ith tag [zt]i as a conditional tag probability
log p(i|x, θ) by applying a softmax operation.

p(i|x, θ) = softmax(zit)

=
ez

i
t∑

j e
zjt

For the tag sequence y given the input sentence x
the log-likelihood is :

log p(y|x, θ) = zy − logadd
j

zj

4.2 Sentence-Level Log-Likelihood (SLL)
In the word-level approach above, we discard the
dependencies between the tags in a tag sequence.
In our sentence-level log-likelihood (SLL) formu-
lation (also adapted from Collobert et al. (2011))
we incorporate these dependencies: we introduce
a transition score [A]i,j for jumping from tag i to
tag j of adjacent words in the tag sequence to the
set of parameters θ̃. These transition scores are
going to be trained.

We use both the transition scores [A] and the
output scores z to compute the sentence score
s(x|Tt=1, y|Tt=1, θ̃).

s(x, y, θ̃) =
T∑

t=1

(
[A]yt−1,yt + z

yt
t

)
We normalize this sentence score over all possible
paths of tag sequences ỹ to get the log conditional
probability as below :

log psent(y|x, θ̃) = s(x, y, θ̃)− logadd
ỹ

s(x, ỹ, θ̃)

Even though the number of tag sequences grows
exponentially with the length of the sentence, we

can compute the normalization factor in linear
time (Collobert et al., 2011).

At inference time, we find the best tag sequence

argmax
ỹ

s(x, ỹ, θ̃)

for an input sentence x using Viterbi decoding. In
this case, we basically maximize the same likeli-
hood as in a CRF except that a CRF is a linear
model.

The above sentence-level log-likelihood is use-
ful for sequential tagging, but it cannot be directly
used for modeling relations between non-adjacent
words in the sentence. In the next subsection, we
extend the above idea to also model relations be-
tween non-adjacent words.

4.3 Relation-Level Log-Likelihood (RLL)
For every word xt in the sentence x, we output the
tag yt and a distance dt. If a word at position t is
related to a word at position k and k < t, then dt =
(t − k). If word t is not related to any other word
to its left, then dt = 0. Let DLeft be the maximum
distance we model for such left-relations 3.

zt =
−→
Vr
−→
ht

(L) +
←−
Vr
←−
ht

(L) + cr

We let
−→
Vr ∈ R(DLeft+1)×Y×dh (where dh is the

dimensionality of hidden units) such that the out-
put state zt ∈ R(DLeft+1)×Y as compared to zt ∈
R(1)×Y in case of sentence-level log-likelihood.

In order to add dependencies between tags
and relations, we introduce a transition score
[A]i,j,d′ ,d” for jumping from tag i and relation dis-
tance d

′
to tag j and relation distance d” of adja-

cent words in the tag sequence, to the set of pa-
rameters θ

′
. These transition scores are also go-

ing to be trained similar to the transition scores in
sentence-level log-likelihood.

The sentence score s(x|Tt=1, y|Tt=1, d|Tt=1, θ
′
) is:

s(x, y, d, θ
′
) =

T∑
t=1

(
[A]yt−1,yt,dt−1,dt + z

yt,dt
t

)
We normalize this sentence score over all possi-

ble paths of tag ỹ and relation sequences d̃ to get
the log conditional probability as below :

log prel,Left(y, d|x, θ̃) =s(x, y, d, θ′)
− logadd

ỹ,d̃

s(x, ỹ, d̃, θ
′
)

3Later in this section, we will also add a similar likeli-
hood in the objective function for right-relations, i.e., for each
word the related words are in its right context.

922



The sale infuriated Beijing which regards Taiwan an integral part ...
Entity tags B T I T B O B H O B O B T O O O ...

Left Rel (dleft) 0 0 0 0 0 2 1 0 0 0 ...
Right Rel (dright) 2 1 1 0 0 0 0 0 0 0 ...

IS-ABOUT IS-FROM
IS-FROM

IS-ABOUT

Figure 1: Gold standard annotation for an example sentence from MPQA dataset. O represents the
‘Other’ tag in the BIO scheme.

We can still compute the normalization fac-
tor in linear time similar to sentence-level log-
likelihood.

At inference time, we jointly find the best tag
and relation sequence

argmax
ỹ,d̃

s(x, ỹ, d̃, θ
′
)

for an input sentence x using Viterbi decoding.
For our task of joint extraction of opinion en-

tities and relations, we train our model to pre-
dict tag y and relation distance d for every word
in the sentence by maximizing the log-likelihood
(SLL+RLL) below using Adadelta (Zeiler, 2012).∑
(x,y)∈T

log psent(y|x, θ′)+ log prel,Left(y, d|x, θ′)

+ log prel,Right(y, d|x, θ′)

5 Experiments

5.1 Data
We use the MPQA 2.0 corpus (Wiebe and Cardie,
2005; Wilson, 2008). It contains news articles
and editorials from a wide variety of news sources.
There are a total of 482 documents in our dataset
containing 9471 sentences with phrase-level anno-
tations. We set aside 132 documents as a devel-
opment set and use the remaining 350 documents
as the evaluation set. We report the results us-
ing 10-fold cross validation at the document level
to mimic the methodology of Yang and Cardie
(2013).

The dataset contains gold-standard annotations
for opinion entities — expressions, targets, hold-
ers. We use only the direct subjective/opinion ex-
pressions. There are also annotations for opin-
ion relations – IS-FROM between opinion holders
and opinion expressions; and IS-ABOUT between
opinion targets and opinion expressions. These re-
lations can overlap but we discard all relations that

contain sub-relations similar to Yang and Cardie
(2013). We also leave identification of overlap-
ping relations for future work.

Figure 1 gives an example of an annotated sen-
tence from the dataset: boxes denote opinion enti-
ties and opinion relations are shown by arcs. We
interpret these relations arcs as directed — from
an opinion expression towards an opinion holder,
and from an opinion target towards an opinion ex-
pression.

In order to use the RLL formulation as de-
fined in Section 4.3, we pre-process these relation
arcs to obtain the left-relation distances (dleft) and
right-relation distances (dright) as shown in Fig-
ure 1. For each word in an entity, we find its
distance to the nearest word in the related entity.
These distances become our relation tags. The en-
tity tags are interpreted using the BIO scheme, also
shown in the figure. Our RLL model jointly mod-
els the entity tags and relation tags. At inference
time, these entity tags and relation tags are used to-
gether to determine IS-FROM and IS-ABOUT rela-
tions. We use a simple majority vote to determine
the final entity tag from SLL+RLL model.

5.2 Evaluation Metrics
We use precision, recall and F-measure (as in Yang
and Cardie (2013)) as evaluation metrics. Since
the identification of exact boundaries for opin-
ion entities is hard even for humans (Wiebe and
Cardie, 2005), soft evaluation methods such as
Binary Overlap and Proportional Overlap are re-
ported. Binary Overlap counts every overlapping
predicted and gold entity as correct, while Propor-
tional Overlap assigns a partial score proportional
to the ratio of overlap span and the correct span
(Recall) or the ratio of overlap span and the pre-
dicted span (Precision).

For the case of opinion relations, we report pre-
cision, recall and F-measure according to the Bi-
nary Overlap. It considers a relation correct if
there is an overlap between the predicted opin-

923



Opinion Expression Opinion Target Opinion Holder
Method P R F1 P R F1 P R F1

CRF 84.423.24 61.613.20 71.172.66 80.382.72 46.804.41 59.104.06 73.374.09 49.713.46 59.213.49

CRF+ILP 73.533.90 74.892.51 74.112.49 77.273.49 56.943.94 65.403.07 67.003.17 67.223.50 67.222.54

LSTM+WLL 67.884.49 66.133.20 66.872.66 58.714.87 54.923.23 56.501.51 60.334.54 63.342.33 61.652.37

LSTM+SLL 70.455.12 66.653.46 68.373.14 63.024.61 56.773.98 59.653.61 61.853.82 63.123.59 62.352.46

LSTM+SLL+RLL 71.735.35 70.923.96 71.112.71 64.525.52 65.944.74 64.841.44 62.753.75 67.174.37 64.712.23

CRF 80.783.27 57.623.24 67.192.63 71.813.22 42.363.78 53.233.69 71.563.54 48.613.51 57.863.43

CRF+ILP 71.034.03 69.722.37 70.222.44 71.943.25 49.833.24 58.722.80 65.703.07 65.913.63 65.682.61

LSTM+WLL 64.474.79 59.453.52 61.672.26 52.725.01 44.212.54 47.851.41 58.414.72 59.722.52 52.452.23

LSTM+SLL 65.975.46 61.763.69 63.603.05 54.464.49 50.164.38 52.013.05 59.803.29 61.273.75 60.402.26

LSTM+SLL+RLL 65.484.92 65.543.65 65.562.71 52.756.81 60.544.78 55.811.96 59.443.56 65.514.22 62.182.50

Table 1: Performance on opinion entity extraction. Top table shows Binary Overlap performance; bottom
table shows Proportional Overlap performance. Superscripts designate one standard deviation.

ion expression and the gold opinion expression as
well as an overlap between the predicted entity
(holder/target) and the gold entity (holder/target).

5.3 Baselines
CRF+ILP. We use the ILP-based joint inference
model (Yang and Cardie, 2013) as baseline for
both the entity and relation extraction tasks. It rep-
resents the state-of-the-art for fine-grained opin-
ion extraction. Their method first identifies opin-
ion entities using CRFs (an additional baseline)
with a variety of features such as words, POS
tags, and lexicon features (the subjectivity strength
of the word in the Subjectivity Lexicon). They
also train a relation classifier (logistic regression)
by over-generating candidates from the CRFs (50-
best paths) using local features such as word, POS
tags, subjectivity lexicons as well as semantic and
syntactic features such as semantic frames, depen-
dency paths, WordNet hypernyms, etc. Finally,
they use ILP for joint-inference to find the opti-
mal prediction for both opinion entity and opinion
relation extraction.

LSTM+SLL+Softmax. As an additional base-
line for relation extraction, we train a softmax
classifier on top of our SLL framework. We
jointly learn the relation classifier and SLL model.
For every entity pair [x]ji , [x]

l
k, we first sum the

start and end word output representation [zt] and
then concatenate them to learn softmax weightW ′

where W ′ ∈ R3×2dh .

yrel = softmax(W
′
[
[zt]i + [zt]j
[zt]k + [zt]l

]
)

The inference is pipelined in this case. At the time
of inference, we first predict the entity spans and
then use these spans for relation classification.

5.4 Hyperparameter and Training Details
We use multi-layer bi-directional LSTMs for all
the experiments such that the number of hidden
layers is 3 and the dimensionality of hidden units
(dh) is 50. We use Adadelta for training. We
initialize our word representation using publicly
available word2vec (Mikolov et al., 2013) trained
on Google News dataset and keep them fixed dur-
ing training. For RLL, we keep DLeft and DRight
as 15. All the weights in the network are initial-
ized from small random uniform noise. We train
all our models for 200 epochs. We do not pre-
train our network. We regularize our network us-
ing dropout (Srivastava et al., 2014) with the drop-
out rate tuned using the development set. We se-
lect the final model based on development-set per-
formance (average of Proportional Overlap for en-
tities and Binary Overlap for relations).

6 Results

6.1 Opinion Entities
Table 1 shows the performance of opinion entity
identification using the Binary Overlap and Pro-
portional Overlap evaluation metrics. We discuss
specific results in the paragraphs below.
WLL vs. SLL. SLL performs better than WLL
on all entity types, particularly with respect to Pro-
portional Overlap on opinion holder and target en-
tities. A similar trend can be seen for the exam-
ple sentences in Table 3. In S1, SLL extracts “has
been in doubt” as the opinion expression whereas
WLL only identifies “has”. Similarly in S2, WLL
annotates “Saudi Arabia’s request on a case-by-
case” as the target while SLL correctly includes
“basis” in its annotation. Thus, we find that mod-
eling the transitions between adjacent tags enables

924



IS-ABOUT IS-FROM
Method P R F1 P R F1

CRF+ILP 61.574.56 47.653.12 54.392.49 64.043.08 58.794.42 61.173.02

LSTM+SLL+Softmax 36.235.10 36.127.75 35.403.35 36.445.26 40.196.13 37.603.42

LSTM+SLL+RLL 62.483.87 49.802.84 54.982.54 64.193.81 53.756.00 58.223.01

Table 2: Performance on opinion relation extraction using Binary Overlap on the opinion entities. Su-
perscripts designate one standard deviation.

SLL to find entire opinion entity phrases better
than WLL, leading to better Proportional Overlap
scores.

SLL vs. SLL+RLL. From Table 1, we see that
the joint-extraction model (SLL+RLL) performs
better than SLL as expected. More specifically,
SLL+RLL model has better recall for all opinion
entity types. The example sentences from Table 3
corroborate these results. In S1, SLL+RLL identi-
fies “announced” as an opinion expression, which
was missing in both WLL and SLL. In S3, neither
the WLL nor the SLL model can annotate opin-
ion holder (H1) or the target (T1), but SLL+RLL
correctly identifies the opinion entities because of
modeling the relations between the opinion ex-
pression “will decide” and the holder/target enti-
ties.

CRF vs. LSTM-based Models. From the anal-
ysis of the performance in Table 1, we find that
our WLL and SLL models perform worse while
our best SLL+RLL model can only match the per-
formance of the CRF baseline on opinion expres-
sions. Even though the recall of all our LSTM-
based models is higher than the recall of the CRF-
baseline for opinion expressions, we cannot match
the precision of CRF baseline. We suspect that
the reason for such high precision on the part
of the CRF is its access to carefully prepared
subjectivity-lexicons4. Our LSTM-based models
do not rely on such features except via the word-
vectors. With respect to holders and targets, we
find that our SLL model performs similar to the
CRF baseline. However, the SLL+RLL model
outperforms CRF baseline.

CRF+ILP vs. SLL+RLL. Even though we find
that our LSTM-based joint-model (SLL+RLL)
outperforms our LSTM-based only-entity extrac-
tion model (SLL), the performance is still below
the ILP-based joint-model (CRF+ILP). However,
we perform comparably with respect to target en-

4http://mpqa.cs.pitt.edu/lexicons/ subj lexicon/

tities (Binary Overlap). Also, our recall on tar-
gets is much better than all other models whereas
the recall on holders is very similar to CRF+ILP.
Our SLL+RLL model can identify targets such as
“Australia’s involvement in Kyoto” which the ILP-
based model cannot, as observed for S1 in Ta-
ble 3. In S3, the ILP-based model also erroneously
divides the target “consider Saudi Arabia’s re-
quest on a case-by-case basis” into a holder “Saudi
Arabia’s” and opinion expression “request”, while
SLL+RLL model can correctly identify it. We will
compare the two models in detail in Section 7.

6.2 Opinion Relations

The extraction of opinion relations is our primary
task. Table 25 shows the performance on opinion
relation extraction task using Binary Overlap.

SLL+Softmax vs. SLL+RLL. The opinion en-
tities and relations are jointly modeled in both the
models, but we see a significant improvement in
performance by adding relation level dependen-
cies to the model vs. learning a classifier on top
of sentence-level dependencies to learn the rela-
tion between entities. LSTM+SLL+RLL performs
much better in terms of both precision and recall
on both IS-FROM and IS-ABOUT relations.

CRF+ILP vs. SLL+RLL. We find that our
SLL+RLL model performs comparably and even
slightly better on IS-ABOUT relations. Such
performance is encouraging because our LSTM-
based model does not rely on features such as
dependency paths, semantic frames or subjectiv-
ity lexicons for our model. Our sequential LSTM
model is able to learn these relations thus validat-
ing that LSTMs can model long-term dependen-
cies. However, for IS-FROM relations, we find that
our recall is lower than the ILP-based joint model.

5Yang and Cardie (2013) omitted a subset of targets and
IS-ABOUT relations. We fixed this and re-ran their models
on the updated dataset, obtaining the lower F-score 54.39 for
IS-ABOUT relations.

925



S1 :
[Australia’s involvement in Kyoto]T1 [has been in doubt ]O1 ever since [ the US President, George Bush]H2 ,

[ announced]O2 last year that [ ratifying the protocol ]T2 would hurt the US economy.

CRF+ILP
Australia’s involvement in Kyoto [has been in doubt ]O1 ever since the US President, George Bush, announced
last year that [ ratifying the protocol ]T1 would hurt the US economy.

WLL [Australia’s involvement in Kyoto]T [has]O been in doubt ever since the US [President ]H, [George Bush]H,
announced last year that ratifying the protocol would hurt the US economy.

SLL [Australia’s involvement in Kyoto]T [has been in doubt ]O ever since the US President, George Bush, an-
nounced last year that ratifying the protocol would hurt the US economy.

SLL+RLL
[Australia’s involvement in Kyoto]T [has been in doubt ]O ever since the US President, [George Bush]H2 ,

[ announced]O2 last year that [ ratifying the protocol ]T2 would hurt the US economy.

S2 :
Bush said last week [he]H1,2 [was willing]O1 [ to consider ]O2 [Saudi Arabia’s request on a case-by-case basis ]T2
but [U.S. officials ]H3 [doubted]O3 [ it would happen any time soon]T3 .

CRF+ILP
[Bush]H1 [ said]O1 last week [he]H2 [was willing to consider ]O2 [Saudi Arabia’s ]H3 [ request ]O3 on a

case-by-case basis but [U.S. officials ]H4 [doubted]O4 [ it ]T4 would happen any time soon.

WLL
Bush said last week [he]H [was willing]O to [consider ]O [Saudi Arabia’s request on a case-by-case]T basis

but [U.S. officials ]H [doubted]O [ it ]T would [happen any time soon]T.

SLL
Bush said last week [he]H [was willing]O to [consider Saudi Arabia’s request on a case-by-case basis ]T but

[U.S. officials ]H [doubted]O [ it ]T would happen any time soon.

SLL+RLL
Bush said last week [he]H1 [was willing to consider ]O1 [Saudi Arabia’s request on a case-by-case basis ]T1
but [U.S. officials ]H2 [doubted]O2 [ it would happen any time soon]T2 .

S3 :
Hence, [ the Organization of Petroleum Exporting Countries (OPEC)]H1 , [will decide]O1 at its meeting on

Wednesday [whether or not to cut its worldwide crude production in an effort to shore up energy prices ]T1 .

CRF+ILP
Hence, the Organization of Petroleum Exporting Countries (OPEC), [will decide]O1 at its meeting on Wednes-
day whether [or not to cut its worldwide crude production in an effort to shore up energy prices ]T1 .

WLL Hence, the Organization of Petroleum Exporting Countries (OPEC), will [decide]O at its meeting on Wednes-
day whether or not to cut its worldwide crude production in an effort to shore up energy prices.

SLL Hence, the Organization of Petroleum Exporting Countries (OPEC), [will decide]O at its meeting on Wednes-
day whether or not to cut its worldwide crude production in an effort to shore up energy prices.

SLL+RLL
Hence, [ the Organization of Petroleum Exporting Countries (OPEC)]H1 , [will decide]O1 at its meeting on

Wednesday whether [or not to cut its worldwide crude production in an effort to shore up energy prices ]T1 .

Table 3: Output from different models. The first row for each example is the gold standard.

7 Discussion

In this section, we discuss the various advan-
tages and disadvantages of the LSTM-based
SLL+RLL model as compared to the joint-
inference (CRF+ILP) model. We provide exam-
ples from the dataset in Table 4.

From Table 2, we find that SLL+RLL model
performs worse with respect to the opinion ex-
pression entities and opinion holder entities. On
careful analysis of the output, we found cases
such as S1 in Table 4. For such sentences
SLL+RLL model prefers to annotate the opinion
target (T3) “US requests for more oil exports”,
whereas the ILP model annotates the embedded
opinion holder (H4) “US” and opinion expression
(T4) “requests”. Both models are valid with re-
spect to the gold-standard. In order to simplify

our problem, we discard these embedded rela-
tions during training similar to Yang and Cardie
(2013). However, for future work we would like
to model these overlapping relations which could
potentially improve our performance on opinion
holders and opinion expressions.

We also found several cases such as S2, where
the SLL+RLL model fails to annotate “said” as an
opinion expression. The gold standard opinion ex-
pressions include speech events like “said” or “a
statement”, but not all occurrences of these speech
events are opinion expressions, some are merely
objective events. In S2, “was martyred” is an indi-
cation of an opinion being expressed, so “said” is
annotated as an opinion expression. From our ob-
servation, the ILP model is more relaxed in anno-
tating most of these speech events as opinion ex-
pressions and thus likely to identify corresponding

926



S1 :
However, [Chavez]T1 who [ is known for ]O1 [his ]H2 [ala Fidel Castro left-leaning anti-American philosophy]O2

had on a number of occasions [ rebuffed]O3 [ [US]H4 [ requests ]O4 for [more oil exports ]T4 ]T3 .

CRF+ILP
However, [Chavez]H1 who [ is known]O for [his ala Fidel Castro]H2 [ left-leaning anti-American

philosophy]O2 had on a number of occasions [ rebuffed]O1 [US]H3 [ requests ]O3 for more oil exports.

SLL+RLL
However, Chavez who [ is known]O for his ala Fidel Castro left-leaning anti-American [philosophy]O had on

a number of occasions [ rebuffed]O1 [US requests for more oil exports ]T1 .

S2 : A short while ago, [our correspondent in Bethlehem]H1 [ said]O1 that [Ra’fat al-Bajjali ]T1 was martyred of
wounds sustained in the explosion.

CRF+ILP A short while ago, [our correspondent ]H1 in Bethlehem [said]O1 that [Ra’fat al-Bajjali ]T1 was martyred of
wounds sustained in the explosion.

SLL+RLL A short while ago, our correspondent in Bethlehem said that Ra’fat al-Bajjali was martyred of wounds sustainedin the explosion.
S3 : This is no criticism, and is widely known and appreciated.
CRF+ILP This is no criticism, and is widely known and appreciated.
SLL+RLL [This ]T1 [ is no criticism]O1 , and is widely [known and appreciated]O.

S4 : From the fact that mothers care for their young, we can not deduce that they ought to do so, Hume argued.

CRF+ILP
From the fact that [mothers ]H1 [care]O1 for their young, we can not deduce that they ought to do so,
[Hume]H2 [argued]O2 .

SLL+RLL
From the fact that mothers care for their young, [we]H1 [can not deduce]O1 that [ they]T1 ought to do so,

[Hume]H2 [argued]O2 .

Table 4: Examples from the dataset with label annotations from CRF+ILP and SLL+RLL models for
comparison. The first row for each example is the gold standard.

opinion holders and opinion targets as compared
to SLL+RLL model.

There were also instances such as S3 and S4 in
Table 4 for which the gold standard does not have
an annotation but the SLL+RLL output looks rea-
sonable with respect to our task. In S3, SLL+RLL
identifies “is no criticism” as an opinion expres-
sion for the target “This”. However, it fails
to identify the relation-link between “known and
appreciated” and the target “This”. Similarly,
SLL+RLL also identifies reasonable opinion enti-
ties in S4, whereas the ILP model erroneously an-
notates “mothers” as the opinion holder and “care”
as the opinion expression.

We handle the task of joint-extraction of opin-
ion entities and opinion relations as a sequence
labeling task in this paper and report the perfor-
mance of the 1-best path at the time of Viterbi in-
ference. However, there are approaches such as
discriminative reranking (Collins and Koo, 2005)
to rerank the output of an existing system that of-
fer a means for further improving the performance
of our SLL+RLL model. In particular, the oracle
performance using the top-10 Viterbi paths from
our SLL+RLL model has an F-score of 82.11 for
opinion expressions, 76.77 for targets and 78.10
for holders. Similarly, IS-ABOUT relations have

an F-score of 65.99 and IS-FROM relations, an F-
score of 70.80. These scores are on average 10
points better than the performance of the current
SLL+RLL model, indicating that substantial gains
might be attained via reranking.

8 Conclusion

In this paper, we explored LSTM-based models
for the joint extraction of opinion entities and re-
lations. Experimentally, we found that adding
sentence-level and relation-level dependencies on
the output layer improves the performance on
opinion entity extraction, obtaining results within
1-3% of the ILP-based joint model on opinion en-
tities, within 3% for IS-FROM relation and compa-
rable for IS-ABOUT relation.

In future work, we plan to explore the effects
of pre-training (Bengio et al., 2009) and sched-
uled sampling (Bengio et al., 2015) for training
our LSTM network. We would also like to explore
re-ranking methods for our problem. With respect
to the fine-grained opinion mining task, a poten-
tial future direction to be able to model overlap-
ping and embedded entities and relations and also
to extend this model to handle cross-sentential re-
lations.

927



References
Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua

Bengio. 2014. Neural machine translation by
jointly learning to align and translate. CoRR,
abs/1409.0473.

Yoshua Bengio, Jérôme Louradour, Ronan Collobert,
and Jason Weston. 2009. Curriculum learning. In
Proceedings of the 26th Annual International Con-
ference on Machine Learning, ICML ’09, pages 41–
48, New York, NY, USA. ACM.

Samy Bengio, Oriol Vinyals, Navdeep Jaitly, and
Noam Shazeer. 2015. Scheduled sampling for se-
quence prediction with recurrent neural networks.
CoRR, abs/1506.03099.

Yoshua Bengio. 2009. Learning deep architectures for
ai. Found. Trends Mach. Learn., 2(1):1–127, Jan-
uary.

Eric Breck, Yejin Choi, and Claire Cardie. 2007. Iden-
tifying expressions of opinion in context. In Pro-
ceedings of the 20th International Joint Conference
on Artifical Intelligence, IJCAI’07, pages 2683–
2688, San Francisco, CA, USA. Morgan Kaufmann
Publishers Inc.

Yejin Choi, Claire Cardie, Ellen Riloff, and Siddharth
Patwardhan. 2005. Identifying sources of opin-
ions with conditional random fields and extraction
patterns. In Proceedings of the Conference on Hu-
man Language Technology and Empirical Methods
in Natural Language Processing, HLT ’05, pages
355–362, Stroudsburg, PA, USA. Association for
Computational Linguistics.

Yejin Choi, Eric Breck, and Claire Cardie. 2006. Joint
extraction of entities and relations for opinion recog-
nition. In Proceedings of the 2006 Conference on
Empirical Methods in Natural Language Process-
ing, EMNLP ’06, pages 431–439, Stroudsburg, PA,
USA. Association for Computational Linguistics.

Michael Collins and Terry Koo. 2005. Discrimina-
tive reranking for natural language parsing. Comput.
Linguist., 31(1):25–70, March.

Ronan Collobert, Jason Weston, Léon Bottou, Michael
Karlen, Koray Kavukcuoglu, and Pavel Kuksa.
2011. Natural language processing (almost) from
scratch. J. Mach. Learn. Res., 12:2493–2537,
November.

Alex Graves, Navdeep Jaitly, and Abdel-rahman Mo-
hamed. 2013. Hybrid speech recognition with deep
bidirectional LSTM. In 2013 IEEE Workshop on
Automatic Speech Recognition and Understanding,
Olomouc, Czech Republic, December 8-12, 2013,
pages 273–278.

James Hammerton. 2003. Named entity recognition
with long short-term memory. In Proceedings of the
Seventh Conference on Natural Language Learning
at HLT-NAACL 2003 - Volume 4, CONLL ’03, pages

172–175, Stroudsburg, PA, USA. Association for
Computational Linguistics.

Michiel Hermans and Benjamin Schrauwen. 2013.
Training and analysing deep recurrent neural net-
works. In Advances in Neural Information Process-
ing Systems 26: 27th Annual Conference on Neural
Information Processing Systems 2013. Proceedings
of a meeting held December 5-8, 2013, Lake Tahoe,
Nevada, United States., pages 190–198.

Salah El Hihi and Yoshua Bengio. 1996. Hierarchical
recurrent neural networks for long-term dependen-
cies.

Sepp Hochreiter and Jürgen Schmidhuber. 1997. Long
short-term memory. Neural Comput., 9(8):1735–
1780, November.

Zhiheng Huang, Wei Xu, and Kai Yu. 2015. Bidi-
rectional LSTM-CRF models for sequence tagging.
CoRR, abs/1508.01991.

Ozan Irsoy and Claire Cardie. 2013. Bidirectional re-
cursive neural networks for token-level labeling with
structure. arXiv preprint arXiv:1312.0493.

Ozan İrsoy and Claire Cardie. 2014. Opinion mining
with deep recurrent neural networks. In Proceedings
of the Conference on Empirical Methods in Natural
Language Processing, pages 720–728.

Soo-Min Kim and Eduard Hovy. 2006. Extracting
opinions, opinion holders, and topics expressed in
online news media text. In Proceedings of the Work-
shop on Sentiment and Subjectivity in Text, SST ’06,
pages 1–8, Stroudsburg, PA, USA. Association for
Computational Linguistics.

Nozomi Kobayashi, Kentaro Inui, and Yuji Matsumoto.
2007. Extracting aspect-evaluation and aspect-of re-
lations in opinion mining. In Proceedings of the
2007 Joint Conference on Empirical Methods in
Natural Language Processing and Computational
Natural Language Learning (EMNLP-CoNLL.

Pengfei Liu, Shafiq Joty, and Helen Meng. 2015. Fine-
grained opinion mining with recurrent neural net-
works and word embeddings. In Proceedings of
the 2015 Conference on Empirical Methods in Natu-
ral Language Processing, pages 1433–1443, Lisbon,
Portugal, September. Association for Computational
Linguistics.

Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Cor-
rado, and Jeff Dean. 2013. Distributed representa-
tions of words and phrases and their composition-
ality. In C.J.C. Burges, L. Bottou, M. Welling,
Z. Ghahramani, and K.Q. Weinberger, editors, Ad-
vances in Neural Information Processing Systems
26, pages 3111–3119. Curran Associates, Inc.

Makoto Miwa and Mohit Bansal. 2016. End-to-end
relation extraction using lstms on sequences and tree
structures. CoRR, abs/1601.00770.

928



Jürgen Schmidhuber. 1992. Learning complex, ex-
tended sequences using the principle of history com-
pression. Neural Comput., 4(2):234–242, March.

M. Schuster and K.K. Paliwal. 1997. Bidirec-
tional recurrent neural networks. Trans. Sig. Proc.,
45(11):2673–2681, November.

Richard Socher, Brody Huval, Christopher D. Man-
ning, and Andrew Y. Ng. 2012. Semantic com-
positionality through recursive matrix-vector spaces.
In Proceedings of the 2012 Joint Conference on
Empirical Methods in Natural Language Process-
ing and Computational Natural Language Learning,
EMNLP-CoNLL ’12, pages 1201–1211, Strouds-
burg, PA, USA. Association for Computational Lin-
guistics.

Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky,
Ilya Sutskever, and Ruslan Salakhutdinov. 2014.
Dropout: A simple way to prevent neural networks
from overfitting. Journal of Machine Learning Re-
search, 15:1929–1958.

Ilya Sutskever, Oriol Vinyals, and Quoc V Le. 2014.
Sequence to sequence learning with neural net-
works. In Z. Ghahramani, M. Welling, C. Cortes,
N. D. Lawrence, and K. Q. Weinberger, editors, Ad-
vances in Neural Information Processing Systems
27, pages 3104–3112. Curran Associates, Inc.

Janyce Wiebe and Claire Cardie. 2005. Annotating
expressions of opinions and emotions in language.
language resources and evaluation. In Language
Resources and Evaluation (formerly Computers and
the Humanities, page 2005.

Theresa Wilson, Janyce Wiebe, and Paul Hoffmann.
2005. Recognizing contextual polarity in phrase-
level sentiment analysis. In Proceedings of the Con-
ference on Human Language Technology and Em-
pirical Methods in Natural Language Processing,
HLT ’05, pages 347–354, Stroudsburg, PA, USA.
Association for Computational Linguistics.

Theresa Ann Wilson. 2008. Fine-grained Subjectivity
and Sentiment Analysis: Recognizing the intensity,
polarity, and attitudes of private states. Ph.D. thesis,
The University of Pittsburgh, June.

Yan Xu, Lili Mou, Ge Li, Yunchuan Chen, Hao Peng,
and Zhi Jin. 2015. Classifying relations via long
short term memory networks along shortest depen-
dency paths. In In Proceedings of Conference on
Empirical Methods in Natural Language Process-
ing.

Bishan Yang and Claire Cardie. 2012. Extracting
opinion expressions with semi-markov conditional
random fields. In Proceedings of the 2012 Joint
Conference on Empirical Methods in Natural Lan-
guage Processing and Computational Natural Lan-
guage Learning, EMNLP-CoNLL ’12, pages 1335–
1345, Stroudsburg, PA, USA. Association for Com-
putational Linguistics.

Bishan Yang and Claire Cardie. 2013. Joint infer-
ence for fine-grained opinion extraction. In Pro-
ceedings of the 51st Annual Meeting of the Associa-
tion for Computational Linguistics (Volume 1: Long
Papers), pages 1640–1649, Sofia, Bulgaria, August.
Association for Computational Linguistics.

Matthew D. Zeiler. 2012. ADADELTA: an adaptive
learning rate method. CoRR, abs/1212.5701.

929


