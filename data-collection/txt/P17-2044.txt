Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 281–286
Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 281–286

Vancouver, Canada, July 30 - August 4, 2017. c(cid:13)2017 Association for Computational Linguistics
Vancouver, Canada, July 30 - August 4, 2017. c(cid:13)2017 Association for Computational Linguistics

https://doi.org/10.18653/v1/P17-2044
https://doi.org/10.18653/v1/P17-2044

281

Japanese Sentence Compression with a Large Training Dataset

Shun Hasegawa

Tokyo Institute of Technology, Japan

hasegawa.s@lr.pi.titech.ac.jp

Yuta Kikuchi

Preferred Networks, Inc., Japan
kikuchi@preferred.jp

Hiroya Takamura

Tokyo Institute of Technology, Japan
takamura@pi.titech.ac.jp

Manabu Okumura

Tokyo Institute of Technology, Japan

oku@pi.titech.ac.jp

Abstract

In English, high-quality sentence com-
pression models by deleting words have
been trained on automatically created
large training datasets. We work on
Japanese sentence compression by a sim-
ilar approach. To create a large Japanese
training dataset, a method of creating En-
glish training dataset is modiﬁed based
on the characteristics of the Japanese lan-
guage. The created dataset is used to
train Japanese sentence compression mod-
els based on the recurrent neural network.

1 Introduction
Sentence compression is the task of shorten-
ing a sentence while preserving its important
information and grammaticality.
Robust sen-
tence compression systems are useful by them-
selves and also as a module in an extractive sum-
marization system (Berg-Kirkpatrick et al., 2011;
Thadani and McKeown, 2013). In this paper, we
work on Japanese sentence compression by delet-
ing words. One advantage of compression by
deleting words as opposed to abstractive compres-
sion lies in the small search space. Another one is
that the compressed sentence is more likely to be
free from incorrect information not mentioned in
the source sentence.

for

Japanese

for English

(Knight and Marcu,

There are many sentence compression mod-
els
(Harashima and Kurohashi,
2012; Hirao et al., 2009; Hori and Furui, 2004)
and
2000;
Turner and Charniak, 2005; Clarke and Lapata,
2006).
In recent years, a high-quality En-
glish sentence compression model by deleting
words was trained on a large training dataset
(Filippova and Altun,
Filippova et al.,
2015). While it is impractical to create a large

2013;

training dataset by hand, one can be created auto-
matically from news articles (Filippova and Altun,
2013). The procedure is as follows (where S, H,
and C respectively denote the ﬁrst sentence of an
article, the headline, and the created compressed
sentence of S). Firstly,
to restrict the training
data to grammatical and informative sentences,
only news articles satisfying certain conditions
are used. Then, nouns, verbs, adjectives, and
adverbs (i.e., content words) shared by S and H
are identiﬁed by matching word lemmas, and a
rooted dependency subtree that contains all the
shared content words is regarded as C.

However, their method is designed for English,
and cannot be applied to Japanese as it is. Thus,
in this study, their method is modiﬁed based on
the following three characteristics of the Japanese
language: (a) Abbreviation of nouns and nominal-
ization of verbs frequently occur in Japanese. (b)
Words that are not verbs can also be the root node
especially in headlines. (c) Subjects and objects
that can be easily estimated from the context are
often omitted.

The created training dataset is used to train three
models. The ﬁrst model is the original Filippova et
al.’s model, an encoder-decoder model with a long
short-term memory (LSTM), which we extend in
this paper to make the other two models that can
control the output length (Kikuchi et al., 2016),
because controlling the output length makes a
compressed sentence more informative under the
desired length.

2 Creating training dataset for Japanese

Filippova et al.’s method of creating training data
consists of the conditions imposed on news arti-
cles and the following three steps: (1) identiﬁca-
tion of shared content words, (2) transformation of
a dependency tree, and (3) extraction of the min-

282

Figure 1: Dependency tree of S1. Squares and arrows respectively indicate chunks and dependency
relations. Bold arrows mean that chunks are merged into a single node. Bold squares are extracted as C.

Figure 2: A sequence of chunks of H1

imum rooted subtree. We modiﬁed their method
based on the characteristics of the Japanese lan-
guage as follows. To explain our method, a de-
pendency tree of S and a sequence of bunsetsu
chunks of H in Japanese are shown in Figures 1
and 2. Note that nodes of dependency trees in
Japanese are bunsetsu chunks each consisting of
content words followed by function words.

2.1 Identiﬁcation of shared content words
Content words shared by S and H are identiﬁed by
matching lemmas and pronominal anaphora reso-
lution in Filippova et al.’s method. Abbreviation of
nouns and nominalization of verbs frequently oc-
cur in Japanese (characteristic (a) in Introduction),
and it is difﬁcult to identify these transformations
simply by matching lemmas. Thus, after the iden-
tiﬁcation by matching lemmas, two identiﬁcation
methods (described below) using character-level
information are applied. Note that pronominal
anaphora resolution is not used, because pronouns
are often omitted in Japanese.
Abbreviation of nouns: There are two types of
abbreviations of nouns in Japanese. One is the
abbreviation of a proper noun, which shortens the
form by deleting characters (e.g., the pair with “+”
in Figures 1 and 2). The other is the abbreviation
of consecutive nouns, which deletes nouns that be-
have as adjectives (e.g., the pair with “-”). To deal
with such cases, if the character sequence of the
noun sequence in a chunk in H is identical to a
subsequence of characters composing the noun se-
quence in a chunk in S, the noun sequences in H
and S are regarded as shared.
Nominalization of verbs:
in
Japanese have corresponding nouns with similar
meanings (e.g., the pair with # in Figures 1 and

Many

verbs

1 Words in red are regarded as shared through lemma
matching, while words in blue (also underlined) are through
other ways.

2).
Such pairs often share the same Chinese
character, kanji. Kanji is an ideogram and is
more informative than the other types of Japanese
letters. Thus, if a noun in H and a verb in S2 share
one kanji, the noun and the verb are regarded as
shared.

2.2 Transformation of a dependency tree
Some edges in dependency trees cannot be cut
without changing the meaning or losing the gram-
maticality of the sentence.
In Filippova et al.’s
method, the nodes linked by such an edge are
merged into a single node before extraction of the
subtree. The method is adapted to Japanese as fol-
lows. If the function word of a chunk is one of
the speciﬁc particles3, which often make obliga-
tory cases, the chunk and its modiﬁee are merged
into a single node. In Figure 1, the chunks at the
start and end of a bold arrow are merged into a
single node.

2.3 Extraction of the minimum rooted

subtree

In Filippova et al.’s method, the minimum rooted
subtree that contains all the shared content words
is extracted from the transformed tree. We modify
their method to take into account the characteris-
tics (a) and (b).
Deleting the global root: In English, only verbs
can be the root node of a subtree. However, in
Japanese, words with other parts-of-speech can
also be the root node in headlines (characteristics
(b)). Therefore, the global root, which is the root
node of S, and the chunks including a word that
can be located at the end of a sentence4, are the
candidates for the root node of a subtree. Then,
if the root node is not the global root, words suc-
ceeding the word that can be located at the end are
removed from the root node. In Figure 1, among
the two words with “*” that can be located at the
2Candidates are restricted to verbs consisting of one kanji

and some following hiragana.

3“を (wo)”, “に (ni)”, “が (ga)” and “は (ha)”
4An auxiliary verb, a noun, or a verb of which next word

is not a noun, an auxiliary verb, or “の (no)”

283

end, the latter is extracted as the root, and the suc-
ceeding word is removed from the chunk.
Reﬂecting abbreviated forms: Abbreviation of
nouns frequently occurs in Japanese (characteris-
tic (a)). Thus, in C, original forms are replaced
by their abbreviated forms obtained as explained
in Section 2.1 (e.g., the pair with “-” in Figures
1 and 2). However, we do not allow the head of
a chunk to be deleted to keep the grammaticality.
We also restrict here ourselves to word-level dele-
tion and do not allow character-level deletion, be-
cause our purpose is to construct a training dataset
for compression by word deletion. In the example
of Figure 1, chunks in bold squares are extracted
as C.

2.4 Conditions imposed on news articles
Filippova et al.’s method imposed eight conditions
on news articles to restrict the training data to
grammatical and informative sentences.
In our
method, these conditions are modiﬁed to adapt to
Japanese. Firstly, the condition “S should include
content words of H in the same order” is removed,
because word order in Japanese is relatively free.
Secondly, the condition “S should include all con-
tent words of H” is relaxed to “the ratio of shared
content words to content words in H is larger than
threshold θ” because in Japanese, subjects and ob-
jects that can be easily estimated from the context
are often omitted (characteristics (c)). In addition,
two conditions “H should have a verb” and “H
must not begin with a verb” are removed, leaving
four conditions5.

3 Sentence compression with LSTM
Three models are used for sentence compres-
sion.
Each model predicts a label sequence,
where the label for each word is either “retain”
or “delete” (Filippova et al., 2015).
The ﬁrst
is an encoder-decoder model with LSTM (lstm)
(Sutskever et al., 2014). Given an input sequence
x = (x1, x2, . . . , xn), yt in label sequence y =
(y1, y2, . . . , yn) is computed as follows:

s0 = encoder(x)

(st, mt) = decoder(st−1, mt−1, e1(xt)⊕el(yt−1))

yt = sof tmax(W ∗ st + b),

5“H is not a question”, “both H and S are not less than
four words.”, “S is more than 1.5 times longer than H”, and
“S is more than 1.5 times longer than C”.

threshold θ 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0
size(10k) 202 189 172 154 134 106 81 59 35 27
vocab(k) 105 102 98 94 90 81 72 62 48 42
ROUGE-2 54.6 54.4 54.0 54.3 55.3 54.0 55.1 54.4 53.0 53.3

Table 1: Created training data with each θ.
ROUGE-2 is the score of compressed sentence
generated by the model trained on each training
data to target.

where ⊕ indicates concatenation, st and mt are re-
spectively a hidden state and a memory cell at time
t, and e1(word ) is embedding of word. If label is
“retain”, el(label) is (1; 0); otherwise, (0; 1). m0
is a zero vector.

to control

As the second and the third models, we
extend the ﬁrst model
the output
length (Kikuchi et al., 2016). The second model,
lstm+leninit, initializes the memory cell of the de-
coder as follows: m0 = tarlen∗blen where tarlen
is the desired output length, and blen is a train-
able parameter. The third model, lstm+lenemb,
uses the embedding of the potential desired length
e2(length) as an additional input.
In this case,
e1(xt) ⊕ el(yt−1) ⊕ e2(lt) is used as the input of
the decoder where lt is the potential desired length
at time t.

4 Experiments
The created training datasets were used to train
three models for sentence compression. To see the
effect of the modiﬁed subtree extraction method
(Section 2.3), two training datasets were tested:
rooted and multi-root+. rooted includes only dele-
tion of the leafs in a dependency tree. In contrast,
multi-root+ includes deleting the global root and
reﬂecting abbreviated forms besides it.
Setting: Training datasets were created from
seven million, 35-years’ worth of news articles
from the Mainichi, Nikkei, and Yomiuri news-
papers, from which duplicate sentences and sen-
tences in test data were ﬁltered out. Gold-standard
data were composed of the ﬁrst sentences of the
1,000 news articles from Mainichi, each of which
has 5 compressed sentences separately created
by ﬁve human annotators. 100 sentences of the
gold-standard data were used as development data,
while the other sentences were used as test data.
The three models were trained on datasets created
with each value of threshold θ6, which is the pa-
rameter used in the condition (introduced in Sec-
6In Table 2, the thresholds for our models are 0.7, 0.5, 0.3,

0.6, 0.4, and 0.2, respectively, from the top.

284

dataset

model

prop-w-dpnd
tree-base
lstm
lstm+leninit
lstm+lenemb

rooted
rooted
rooted
multi-root+ lstm
multi-root+ lstm+leninit
multi-root+ lstm+lenemb

R-1

73.8
68.6
66.3
70.8
71.0
60.8
71.7
72.6

ROUGE-2

R(R-2)
56.5
52.6
51.5
55.2
55.2
50.6
56.0
56.2

P
51.5
50.4
59.3
57.1
56.7
60.5
57.8
56.4

F
53.7
51.4
54.6
55.9
55.7
54.1
56.7
56.1

R-L

32.7
34.4
36.1
36.0
35.9
33.7
36.3
35.8

CR

60.5
59.1
51.8
56.8
57.8
47.9
57.8
59.4

Table 2: Automatic evaluation. R-1, R-2, and R-L are ROUGE-1, ROUGE-2, and ROUGE-L score. R,
P and F are recall, precision and F-measure.

tion 2.4). The properties of the dataset in relation
to θ are shown in Table 1. θ is tuned for ROUGE-
2 score on the development data. In Table 1, we
show the tendency of the ROUGE-2 scores when
lstm+leninit was trained on created rooted with
each θ. From Table 1, we chose 0.5 as θ. All
models are three stacked LSTM layers7. Words
with frequency lower than ﬁve are regarded as un-
known words. ADAM8 was used as the optimiza-
tion method. The desired length was set to the
bytes of a compressed sentence randomly chosen
from the ﬁve human-generated sentences. In the
test step, beam-search was used (beam size: 20)
and candidates exceeding the desired length were
truncated.
tree-base and prop-w-dpnd: Existing methods
for Japanese sentence compression are not based
on the training on a large dataset. Therefore, the
proposed method is compared with two methods,
tree-base, (Filippova and Strube, 2008) and prop-
w-dpnd (Harashima and Kurohashi, 2012), which
are not based on supervised learning.
tree-base
is implemented as an integer linear programming
problem that ﬁnds a subtree of a dependency tree.
prop-w-dpnd is also implemented as an integer lin-
ear programming problem, but modiﬁed based on
characteristics of the Japanese language. prop-w-
dpnd allows the deletion inside the chunks.

4.1 Automatic evaluation
Table 2 shows the ROUGE score of each model.
We used ROUGE-1, ROUGE-2, and ROUGE-
L (R-1, R-2, and R-L) for evaluation.
In addi-
tion, we also show the precision and F-measure
of ROUGE-2 because the output length of each
model is not same. Compression ratio (CR) is
the ratio of the bytes of the compressed sentence
to the bytes of the source sentence. Average
7Dimension of word embedding and hidden layer:256,

dropout:20%

8α:0.001, β1:0.9, β2:0.999, eps:1e-8, batchsize:180

CR of the test data is approximately 61.0%. We
think we should focus on F-measure of ROUGE-
2 because of the different CR of the models.
The models trained on a large training dataset
achieved higher F-measure than the unsupervised
models. Moreover, F-measure of lstm+leninit and
lstm+lenemb, which were trained on either multi-
root+ or rooted, is signiﬁcantly better than prop-
w-dpnd (p < 0.001). lstm achieved lower R-1 and
R-2 than the other models trained on a large train-
ing dataset, probably because it tends to gener-
ate too short sentences, as indicated by low CR.
It is also noteworthy that CRs of lstm+leninit
and lstm+lenemb are mostly closer to the aver-
age CR of the test data than lstm. Furthermore,
lstm+leninit and lstm+lenemb trained on multi-
root+ instead of rooted worked better in terms
of F-measure. We consider it is because vari-
ous types of deletion make the compression model
more ﬂexible, as indicated by closer CR of the
model trained on multi-root+ instead of rooted to
the average CR of the test data.

between

difference

lstm+leninit

4.2 Human evaluation
The
and
lstm+lenemb trained on multi-root+ was in-
vestigated ﬁrst. With lstm+leninit, 2 out of 100
sentences, chosen randomly, ended with a word
that cannot be located at the end of a sentence. In
contrast, with lstm+lenemb, 24 sentences ended
with such words and therefore are ungrammatical,
although lenemb has shown to be effective in ab-
stractive sentence summarization (Kikuchi et al.,
2016). This result suggests that lstm+lenemb is
excessively affected by the desired length because
lenemb receives the potential desired length at
each time of decoding. In fact, 21 out of the 24
sentences are as long as the desired length.

Then, lstm+leninit trained on multi-root+ was
evaluated by crowdsourcing in comparison with
the gold-standard and tree-base. Each crowd-

285

model
gold-standard
lstm+leninit
prop-w-dpnd

read
4.22
3.99
3.55

info
3.76
3.09
2.81

Table 3: Human absolute evaluation

model
lstm+leninit
lstm
tie

read info
108
78
47
47
125
95

training data
multi-root+
rooted
tie

read info
85
55
33
57
138
132

Table 4: Human relative evaluation

Next,

sourcing worker reads each source sentence and
a set of the compressed sentences, reordered ran-
domly, and gives a score from 1 to 5 (where 5
is best) to each compressed sentence in terms of
informativeness (info) and readability (read). As
shown in Table 3, in terms of both read and info,
lstm+leninit archived higher scores than prop-w-
dpnd, and the difference is signiﬁcant (p < 0.001).
lstm+leninit was compared with lstm
(both are trained on multi-root+), and multi-root+
was compared with rooted (lstm+leninit was used
as the model), by human relative evaluation.
In
this evaluation, each worker votes for one of
lstm+leninit, lstm, or tie, and for one of multi-
root+, rooted, or tie. 250 votes in total were re-
ceived (50 sentences × 5 votes). The results are
shown in Table 4. lstm+leninit is better than lstm
in terms of read, which is not directly related to
the output length, as well as of info. It is also clear
that multi-root+ achieves much higher info with a
negligible reduction in read than rooted.

5 Conclusion

Filippova et al.’s method of creating training data
for English sentence compression was modiﬁed
to create a training dataset for Japanese sentence
compression. The effectiveness of the created
Japanese training dataset was veriﬁed by auto-
matic and human evaluation. Our method can re-
ﬁne the created dataset by giving more ﬂexibil-
ity in compression and achieved better informa-
tiveness with negligible reduction in readability.
Furthermore, it has been shown that controlling
the output length improves the performance of the
sentence compression models.

Acknowledgment

This work was
supported by Google Re-
search Award, JSPS KAKENHI Grant Number
JP26280080, and JPMJPR1655.

References
Taylor Berg-Kirkpatrick, Dan Gillick, and Dan Klein.
2011. Jointly learning to extract and compress. In
Proceedings of the 49th Annual Meeting of the Asso-
ciation for Computational Linguistics: Human Lan-
guage Technologies - Volume 1. pages 481–490.

James Clarke and Mirella Lapata. 2006. Models for
sentence compression: A comparison across do-
mains, training requirements and evaluation mea-
sures. In Proceedings of the 21st International Con-
ference on Computational Linguistics and the 44th
Annual Meeting of the Association for Computa-
tional Linguistics. pages 377–384.

Katja Filippova, Enrique Alfonseca, Carlos A. Col-
menares, Lukasz Kaiser, and Oriol Vinyals. 2015.
Sentence compression by deletion with lstms.
In
Proceedings of the 2015 Conference on Empirical
Methods in Natural Language Processing. pages
360–368.

Katja Filippova and Yasemin Altun. 2013. Overcom-
ing the lack of parallel data in sentence compression.
In Proceedings of the 2013 Conference on Empiri-
cal Methods in Natural Language Processing. pages
1481–1491.

Katja Filippova and Michael Strube. 2008. Depen-
dency tree based sentence compression. In Proceed-
ings of the Fifth International Natural Language
Generation Conference. pages 25–32.

Jun Harashima and Sadao Kurohashi. 2012. Flexible
Japanese sentence compression by relaxing unit con-
straints.
In Proceedings of COLING 2012. pages
1097–1112.

Tsutomu Hirao, Jun Suzuki, and Hideki Isozaki. 2009.
A syntax-free approach to japanese sentence com-
pression. In Proceedings of the Joint Conference of
the 47th Annual Meeting of the ACL and the 4th In-
ternational Joint Conference on Natural Language
Processing of the AFNLP. pages 826–833.

Chiori Hori and Sadaoki Furui. 2004. Speech summa-
rization : An approach through word extraction and
a method for evaluation (¡special section¿the 2002
ieice excellent paper award). IEICE transactions on
information and systems pages 15–25.

Yuta Kikuchi, Graham Neubig, Ryohei Sasano, Hi-
roya Takamura, and Manabu Okumura. 2016. Con-
trolling output length in neural encoder-decoders.
In Proceedings of the 2016 Conference on Empiri-
cal Methods in Natural Language Processing. pages
1328–1338.

Kevin Knight and Daniel Marcu. 2000.

Statistics-
based summarization - step one: Sentence compres-
sion.
In Proceedings of the Seventeenth National
Conference on Artiﬁcial Intelligence and Twelfth
Conference on Innovative Applications of Artiﬁcial
Intelligence. pages 703–710.

286

Ilya Sutskever, Oriol Vinyals, and Quoc V. Le. 2014.
Sequence to sequence learning with neural net-
works.
In Proceedings of the 27th International
Conference on Neural Information Processing Sys-
tems. pages 3104–3112.

Kapil Thadani and Kathleen McKeown. 2013. Sen-
tence compression with joint structural inference. In
Proceedings of the Seventeenth Conference on Com-
putational Natural Language Learning. pages 65–
74.

Jenine Turner and Eugene Charniak. 2005. Super-
vised and unsupervised learning for sentence com-
pression. In Proceedings of the 43rd Annual Meet-
ing on Association for Computational Linguistics.
pages 290–297.

