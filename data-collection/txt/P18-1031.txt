



















































Universal Language Model Fine-tuning for Text Classification


Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers), pages 328–339
Melbourne, Australia, July 15 - 20, 2018. c©2018 Association for Computational Linguistics

328

Universal Language Model Fine-tuning for Text Classification

Jeremy Howard∗
fast.ai

University of San Francisco
j@fast.ai

Sebastian Ruder∗
Insight Centre, NUI Galway

Aylien Ltd., Dublin
sebastian@ruder.io

Abstract

Inductive transfer learning has greatly im-
pacted computer vision, but existing ap-
proaches in NLP still require task-specific
modifications and training from scratch.
We propose Universal Language Model
Fine-tuning (ULMFiT), an effective trans-
fer learning method that can be applied to
any task in NLP, and introduce techniques
that are key for fine-tuning a language
model. Our method significantly outper-
forms the state-of-the-art on six text clas-
sification tasks, reducing the error by 18-
24% on the majority of datasets. Further-
more, with only 100 labeled examples, it
matches the performance of training from
scratch on 100× more data. We open-
source our pretrained models and code1.

1 Introduction

Inductive transfer learning has had a large impact
on computer vision (CV). Applied CV models (in-
cluding object detection, classification, and seg-
mentation) are rarely trained from scratch, but in-
stead are fine-tuned from models that have been
pretrained on ImageNet, MS-COCO, and other
datasets (Sharif Razavian et al., 2014; Long et al.,
2015a; He et al., 2016; Huang et al., 2017).

Text classification is a category of Natural Lan-
guage Processing (NLP) tasks with real-world ap-
plications such as spam, fraud, and bot detection
(Jindal and Liu, 2007; Ngai et al., 2011; Chu et al.,
2012), emergency response (Caragea et al., 2011),
and commercial document classification, such as
for legal discovery (Roitblat et al., 2010).

1http://nlp.fast.ai/ulmfit.
?Equal contribution. Jeremy focused on the algorithm de-

velopment and implementation, Sebastian focused on the ex-
periments and writing.

While Deep Learning models have achieved
state-of-the-art on many NLP tasks, these models
are trained from scratch, requiring large datasets,
and days to converge. Research in NLP focused
mostly on transductive transfer (Blitzer et al.,
2007). For inductive transfer, fine-tuning pre-
trained word embeddings (Mikolov et al., 2013),
a simple transfer technique that only targets a
model’s first layer, has had a large impact in prac-
tice and is used in most state-of-the-art models.
Recent approaches that concatenate embeddings
derived from other tasks with the input at different
layers (Peters et al., 2017; McCann et al., 2017;
Peters et al., 2018) still train the main task model
from scratch and treat pretrained embeddings as
fixed parameters, limiting their usefulness.

In light of the benefits of pretraining (Erhan
et al., 2010), we should be able to do better than
randomly initializing the remaining parameters of
our models. However, inductive transfer via fine-
tuning has been unsuccessful for NLP (Mou et al.,
2016). Dai and Le (2015) first proposed fine-
tuning a language model (LM) but require millions
of in-domain documents to achieve good perfor-
mance, which severely limits its applicability.

We show that not the idea of LM fine-tuning but
our lack of knowledge of how to train them ef-
fectively has been hindering wider adoption. LMs
overfit to small datasets and suffered catastrophic
forgetting when fine-tuned with a classifier. Com-
pared to CV, NLP models are typically more shal-
low and thus require different fine-tuning methods.

We propose a new method, Universal Language
Model Fine-tuning (ULMFiT) that addresses these
issues and enables robust inductive transfer learn-
ing for any NLP task, akin to fine-tuning ImageNet
models: The same 3-layer LSTM architecture—
with the same hyperparameters and no addi-
tions other than tuned dropout hyperparameters—
outperforms highly engineered models and trans-

http://nlp.fast.ai/ulmfit


329

fer learning approaches on six widely studied text
classification tasks. On IMDb, with 100 labeled
examples, ULMFiT matches the performance of
training from scratch with 10× and—given 50k
unlabeled examples—with 100× more data.

Contributions Our contributions are the follow-
ing: 1) We propose Universal Language Model
Fine-tuning (ULMFiT), a method that can be used
to achieve CV-like transfer learning for any task
for NLP. 2) We propose discriminative fine-tuning,
slanted triangular learning rates, and gradual
unfreezing, novel techniques to retain previous
knowledge and avoid catastrophic forgetting dur-
ing fine-tuning. 3) We significantly outperform the
state-of-the-art on six representative text classifi-
cation datasets, with an error reduction of 18-24%
on the majority of datasets. 4) We show that our
method enables extremely sample-efficient trans-
fer learning and perform an extensive ablation
analysis. 5) We make the pretrained models and
our code available to enable wider adoption.

2 Related work

Transfer learning in CV Features in deep neu-
ral networks in CV have been observed to tran-
sition from task-specific to general from the first
to the last layer (Yosinski et al., 2014). For this
reason, most work in CV focuses on transferring
the last layers of the model (Long et al., 2015b).
Sharif Razavian et al. (2014) achieve state-of-the-
art results using features of an ImageNet model as
input to a simple classifier. In recent years, this
approach has been superseded by fine-tuning ei-
ther the last (Donahue et al., 2014) or several of
the last layers of a pretrained model and leaving
the remaining layers frozen (Long et al., 2015a).

Hypercolumns In NLP, only recently have
methods been proposed that go beyond transfer-
ring word embeddings. The prevailing approach
is to pretrain embeddings that capture additional
context via other tasks. Embeddings at different
levels are then used as features, concatenated ei-
ther with the word embeddings or with the in-
puts at intermediate layers. This method is known
as hypercolumns (Hariharan et al., 2015) in CV2

and is used by Peters et al. (2017), Peters et al.
(2018), Wieting and Gimpel (2017), Conneau

2A hypercolumn at a pixel in CV is the vector of activa-
tions of all CNN units above that pixel. In analogy, a hyper-
column for a word or sentence in NLP is the concatenation of
embeddings at different layers in a pretrained model.

et al. (2017), and McCann et al. (2017) who use
language modeling, paraphrasing, entailment, and
Machine Translation (MT) respectively for pre-
training. Specifically, Peters et al. (2018) require
engineered custom architectures, while we show
state-of-the-art performance with the same basic
architecture across a range of tasks. In CV, hyper-
columns have been nearly entirely superseded by
end-to-end fine-tuning (Long et al., 2015a).

Multi-task learning A related direction is
multi-task learning (MTL) (Caruana, 1993). This
is the approach taken by Rei (2017) and Liu et al.
(2018) who add a language modeling objective
to the model that is trained jointly with the main
task model. MTL requires the tasks to be trained
from scratch every time, which makes it inefficient
and often requires careful weighting of the task-
specific objective functions (Chen et al., 2017).

Fine-tuning Fine-tuning has been used success-
fully to transfer between similar tasks, e.g. in QA
(Min et al., 2017), for distantly supervised senti-
ment analysis (Severyn and Moschitti, 2015), or
MT domains (Sennrich et al., 2015) but has been
shown to fail between unrelated ones (Mou et al.,
2016). Dai and Le (2015) also fine-tune a lan-
guage model, but overfit with 10k labeled exam-
ples and require millions of in-domain documents
for good performance. In contrast, ULMFiT lever-
ages general-domain pretraining and novel fine-
tuning techniques to prevent overfitting even with
only 100 labeled examples and achieves state-of-
the-art results also on small datasets.

3 Universal Language Model Fine-tuning

We are interested in the most general inductive
transfer learning setting for NLP (Pan and Yang,
2010): Given a static source task TS and any tar-
get task TT with TS 6= TT , we would like to im-
prove performance on TT . Language modeling
can be seen as the ideal source task and a counter-
part of ImageNet for NLP: It captures many facets
of language relevant for downstream tasks, such as
long-term dependencies (Linzen et al., 2016), hi-
erarchical relations (Gulordava et al., 2018), and
sentiment (Radford et al., 2017). In contrast to
tasks like MT (McCann et al., 2017) and entail-
ment (Conneau et al., 2017), it provides data in
near-unlimited quantities for most domains and
languages. Additionally, a pretrained LM can be
easily adapted to the idiosyncrasies of a target



330

13/02/2018 ulmfit_pretraining.html

1/1

dollarThe gold or

Embedding
layer

Layer 1

Layer 2

Layer 3

Softmax
layer

gold

(a) LM pre-training

13/02/2018 ulmfit_lm_fine-tuning.html

1/1

sceneThe best ever

Embedding
layer

Layer 1

Layer 2

Layer 3

Softmax
layer

(b) LM fine-tuning

13/02/2018 ulmfit_clas_fine-tuning.html

1/1

sceneThe best ever

Embedding
layer

Layer 1

Layer 2

Layer 3

Softmax
layer

(c) Classifier fine-tuning

Figure 1: ULMFiT consists of three stages: a) The LM is trained on a general-domain corpus to capture
general features of the language in different layers. b) The full LM is fine-tuned on target task data using
discriminative fine-tuning (‘Discr’) and slanted triangular learning rates (STLR) to learn task-specific
features. c) The classifier is fine-tuned on the target task using gradual unfreezing, ‘Discr’, and STLR to
preserve low-level representations and adapt high-level ones (shaded: unfreezing stages; black: frozen).

task, which we show significantly improves per-
formance (see Section 5). Moreover, language
modeling already is a key component of existing
tasks such as MT and dialogue modeling. For-
mally, language modeling induces a hypothesis
spaceH that should be useful for many other NLP
tasks (Vapnik and Kotz, 1982; Baxter, 2000).

We propose Universal Language Model Fine-
tuning (ULMFiT), which pretrains a language
model (LM) on a large general-domain corpus and
fine-tunes it on the target task using novel tech-
niques. The method is universal in the sense that
it meets these practical criteria: 1) It works across
tasks varying in document size, number, and label
type; 2) it uses a single architecture and training
process; 3) it requires no custom feature engineer-
ing or preprocessing; and 4) it does not require ad-
ditional in-domain documents or labels.

In our experiments, we use the state-of-the-
art language model AWD-LSTM (Merity et al.,
2017a), a regular LSTM (with no attention,
short-cut connections, or other sophisticated ad-
ditions) with various tuned dropout hyperparame-
ters. Analogous to CV, we expect that downstream
performance can be improved by using higher-
performance language models in the future.

ULMFiT consists of the following steps, which
we show in Figure 1: a) General-domain LM
pretraining (§3.1); b) target task LM fine-tuning
(§3.2); and c) target task classifier fine-tuning
(§3.3). We discuss these in the following sections.

3.1 General-domain LM pretraining
An ImageNet-like corpus for language should be
large and capture general properties of language.
We pretrain the language model on Wikitext-103
(Merity et al., 2017b) consisting of 28,595 prepro-
cessed Wikipedia articles and 103 million words.
Pretraining is most beneficial for tasks with small
datasets and enables generalization even with 100
labeled examples. We leave the exploration of
more diverse pretraining corpora to future work,
but expect that they would boost performance.
While this stage is the most expensive, it only
needs to be performed once and improves perfor-
mance and convergence of downstream models.

3.2 Target task LM fine-tuning
No matter how diverse the general-domain data
used for pretraining is, the data of the target task
will likely come from a different distribution. We
thus fine-tune the LM on data of the target task.
Given a pretrained general-domain LM, this stage
converges faster as it only needs to adapt to the id-
iosyncrasies of the target data, and it allows us to
train a robust LM even for small datasets. We pro-
pose discriminative fine-tuning and slanted trian-
gular learning rates for fine-tuning the LM, which
we introduce in the following.

Discriminative fine-tuning As different layers
capture different types of information (Yosinski
et al., 2014), they should be fine-tuned to differ-
ent extents. To this end, we propose a novel fine-



331

tuning method, discriminative fine-tuning3.
Instead of using the same learning rate for all

layers of the model, discriminative fine-tuning al-
lows us to tune each layer with different learning
rates. For context, the regular stochastic gradient
descent (SGD) update of a model’s parameters θ at
time step t looks like the following (Ruder, 2016):

θt = θt−1 − η · ∇θJ(θ) (1)

where η is the learning rate and∇θJ(θ) is the gra-
dient with regard to the model’s objective func-
tion. For discriminative fine-tuning, we split the
parameters θ into {θ1, . . . , θL} where θl contains
the parameters of the model at the l-th layer and
L is the number of layers of the model. Similarly,
we obtain {η1, . . . , ηL} where ηl is the learning
rate of the l-th layer.

The SGD update with discriminative fine-
tuning is then the following:

θlt = θ
l
t−1 − ηl · ∇θlJ(θ) (2)

We empirically found it to work well to first
choose the learning rate ηL of the last layer by
fine-tuning only the last layer and using ηl−1 =
ηl/2.6 as the learning rate for lower layers.

Slanted triangular learning rates For adapting
its parameters to task-specific features, we would
like the model to quickly converge to a suitable
region of the parameter space in the beginning
of training and then refine its parameters. Using
the same learning rate (LR) or an annealed learn-
ing rate throughout training is not the best way
to achieve this behaviour. Instead, we propose
slanted triangular learning rates (STLR), which
first linearly increases the learning rate and then
linearly decays it according to the following up-
date schedule, which can be seen in Figure 2:

cut = bT · cut fracc

p =

{
t/cut, if t < cut
1− t−cutcut·(ratio−1) , otherwise

ηt = ηmax ·
1 + p · (ratio− 1)

ratio

(3)

where T is the number of training iterations4,
cut frac is the fraction of iterations we increase

3 An unrelated method of the same name exists for deep
Boltzmann machines (Salakhutdinov and Hinton, 2009).

4In other words, the number of epochs times the number
of updates per epoch.

the LR, cut is the iteration when we switch from
increasing to decreasing the LR, p is the fraction of
the number of iterations we have increased or will
decrease the LR respectively, ratio specifies how
much smaller the lowest LR is from the maximum
LR ηmax, and ηt is the learning rate at iteration t.
We generally use cut frac = 0.1, ratio = 32 and
ηmax = 0.01.

STLR modifies triangular learning rates (Smith,
2017) with a short increase and a long decay pe-
riod, which we found key for good performance.5

In Section 5, we compare against aggressive co-
sine annealing, a similar schedule that has recently
been used to achieve state-of-the-art performance
in CV (Loshchilov and Hutter, 2017).6

Figure 2: The slanted triangular learning rate
schedule used for ULMFiT as a function of the
number of training iterations.

3.3 Target task classifier fine-tuning
Finally, for fine-tuning the classifier, we augment
the pretrained language model with two additional
linear blocks. Following standard practice for
CV classifiers, each block uses batch normaliza-
tion (Ioffe and Szegedy, 2015) and dropout, with
ReLU activations for the intermediate layer and a
softmax activation that outputs a probability dis-
tribution over target classes at the last layer. Note
that the parameters in these task-specific classi-
fier layers are the only ones that are learned from
scratch. The first linear layer takes as the input the
pooled last hidden layer states.

Concat pooling The signal in text classification
tasks is often contained in a few words, which may

5We also credit personal communication with the author.
6While Loshchilov and Hutter (2017) use multiple anneal-

ing cycles, we generally found one cycle to work best.



332

occur anywhere in the document. As input docu-
ments can consist of hundreds of words, informa-
tion may get lost if we only consider the last hid-
den state of the model. For this reason, we con-
catenate the hidden state at the last time step hT
of the document with both the max-pooled and the
mean-pooled representation of the hidden states
over as many time steps as fit in GPU memory
H = {h1, . . . ,hT }:

hc = [hT , maxpool(H), meanpool(H)] (4)

where [] is concatenation.
Fine-tuning the target classifier is the most crit-

ical part of the transfer learning method. Overly
aggressive fine-tuning will cause catastrophic for-
getting, eliminating the benefit of the information
captured through language modeling; too cautious
fine-tuning will lead to slow convergence (and re-
sultant overfitting). Besides discriminative fine-
tuning and triangular learning rates, we propose
gradual unfreezing for fine-tuning the classifier.

Gradual unfreezing Rather than fine-tuning all
layers at once, which risks catastrophic forgetting,
we propose to gradually unfreeze the model start-
ing from the last layer as this contains the least
general knowledge (Yosinski et al., 2014): We
first unfreeze the last layer and fine-tune all un-
frozen layers for one epoch. We then unfreeze the
next lower frozen layer and repeat, until we fine-
tune all layers until convergence at the last itera-
tion. This is similar to ‘chain-thaw’ (Felbo et al.,
2017), except that we add a layer at a time to the
set of ‘thawed’ layers, rather than only training a
single layer at a time.

While discriminative fine-tuning, slanted trian-
gular learning rates, and gradual unfreezing all
are beneficial on their own, we show in Section
5 that they complement each other and enable our
method to perform well across diverse datasets.

BPTT for Text Classification (BPT3C) Lan-
guage models are trained with backpropagation
through time (BPTT) to enable gradient propa-
gation for large input sequences. In order to
make fine-tuning a classifier for large documents
feasible, we propose BPTT for Text Classifica-
tion (BPT3C): We divide the document into fixed-
length batches of size b. At the beginning of each
batch, the model is initialized with the final state
of the previous batch; we keep track of the hid-
den states for mean and max-pooling; gradients

Dataset Type # classes # examples

TREC-6 Question 6 5.5k
IMDb Sentiment 2 25k
Yelp-bi Sentiment 2 560k
Yelp-full Sentiment 5 650k
AG Topic 4 120k
DBpedia Topic 14 560k

Table 1: Text classification datasets and tasks with
number of classes and training examples.

are back-propagated to the batches whose hidden
states contributed to the final prediction. In prac-
tice, we use variable length backpropagation se-
quences (Merity et al., 2017a).

Bidirectional language model Similar to exist-
ing work (Peters et al., 2017, 2018), we are not
limited to fine-tuning a unidirectional language
model. For all our experiments, we pretrain both a
forward and a backward LM. We fine-tune a clas-
sifier for each LM independently using BPT3C
and average the classifier predictions.

4 Experiments

While our approach is equally applicable to se-
quence labeling tasks, we focus on text classifica-
tion tasks in this work due to their important real-
world applications.

4.1 Experimental setup
Datasets and tasks We evaluate our method on
six widely-studied datasets, with varying numbers
of documents and varying document length, used
by state-of-the-art text classification and transfer
learning approaches (Johnson and Zhang, 2017;
McCann et al., 2017) as instances of three com-
mon text classification tasks: sentiment analy-
sis, question classification, and topic classifica-
tion. We show the statistics for each dataset and
task in Table 1.

Sentiment Analysis For sentiment analysis, we
evaluate our approach on the binary movie review
IMDb dataset (Maas et al., 2011) and on the binary
and five-class version of the Yelp review dataset
compiled by Zhang et al. (2015).

Question Classification We use the six-class
version of the small TREC dataset (Voorhees and
Tice, 1999) dataset of open-domain, fact-based
questions divided into broad semantic categories.



333

Model Test Model Test

IM
D

b
CoVe (McCann et al., 2017) 8.2

T
R

E
C

-6

CoVe (McCann et al., 2017) 4.2
oh-LSTM (Johnson and Zhang, 2016) 5.9 TBCNN (Mou et al., 2015) 4.0
Virtual (Miyato et al., 2016) 5.9 LSTM-CNN (Zhou et al., 2016) 3.9
ULMFiT (ours) 4.6 ULMFiT (ours) 3.6

Table 2: Test error rates (%) on two text classification datasets used by McCann et al. (2017).

AG DBpedia Yelp-bi Yelp-full

Char-level CNN (Zhang et al., 2015) 9.51 1.55 4.88 37.95
CNN (Johnson and Zhang, 2016) 6.57 0.84 2.90 32.39
DPCNN (Johnson and Zhang, 2017) 6.87 0.88 2.64 30.58
ULMFiT (ours) 5.01 0.80 2.16 29.98

Table 3: Test error rates (%) on text classification datasets used by Johnson and Zhang (2017).

Topic classification For topic classification, we
evaluate on the large-scale AG news and DBpedia
ontology datasets created by Zhang et al. (2015).

Pre-processing We use the same pre-processing
as in earlier work (Johnson and Zhang, 2017; Mc-
Cann et al., 2017). In addition, to allow the lan-
guage model to capture aspects that might be rel-
evant for classification, we add special tokens for
upper-case words, elongation, and repetition.

Hyperparameters We are interested in a model
that performs robustly across a diverse set of tasks.
To this end, if not mentioned otherwise, we use the
same set of hyperparameters across tasks, which
we tune on the IMDb validation set. We use
the AWD-LSTM language model (Merity et al.,
2017a) with an embedding size of 400, 3 layers,
1150 hidden activations per layer, and a BPTT
batch size of 70. We apply dropout of 0.4 to
layers, 0.3 to RNN layers, 0.4 to input embed-
ding layers, 0.05 to embedding layers, and weight
dropout of 0.5 to the RNN hidden-to-hidden ma-
trix. The classifier has a hidden layer of size 50.
We use Adam with β1 = 0.7 instead of the de-
fault β1 = 0.9 and β2 = 0.99, similar to (Dozat
and Manning, 2017). We use a batch size of 64,
a base learning rate of 0.004 and 0.01 for fine-
tuning the LM and the classifier respectively, and
tune the number of epochs on the validation set of
each task7. We otherwise use the same practices

7On small datasets such as TREC-6, we fine-tune the LM
only for 15 epochs without overfitting, while we can fine-tune
longer on larger datasets. We found 50 epochs to be a good
default for fine-tuning the classifier.

used in (Merity et al., 2017a).

Baselines and comparison models For each
task, we compare against the current state-of-the-
art. For the IMDb and TREC-6 datasets, we com-
pare against CoVe (McCann et al., 2017), a state-
of-the-art transfer learning method for NLP. For
the AG, Yelp, and DBpedia datasets, we com-
pare against the state-of-the-art text categorization
method by Johnson and Zhang (2017).

4.2 Results

For consistency, we report all results as error rates
(lower is better). We show the test error rates
on the IMDb and TREC-6 datasets used by Mc-
Cann et al. (2017) in Table 2. Our method outper-
forms both CoVe, a state-of-the-art transfer learn-
ing method based on hypercolumns, as well as the
state-of-the-art on both datasets. On IMDb, we
reduce the error dramatically by 43.9% and 22%
with regard to CoVe and the state-of-the-art re-
spectively. This is promising as the existing state-
of-the-art requires complex architectures (Peters
et al., 2018), multiple forms of attention (McCann
et al., 2017) and sophisticated embedding schemes
(Johnson and Zhang, 2016), while our method em-
ploys a regular LSTM with dropout. We note
that the language model fine-tuning approach of
Dai and Le (2015) only achieves an error of 7.64
vs. 4.6 for our method on IMDb, demonstrating
the benefit of transferring knowledge from a large
ImageNet-like corpus using our fine-tuning tech-
niques. IMDb in particular is reflective of real-
world datasets: Its documents are generally a few



334

Figure 3: Validation error rates for supervised and semi-supervised ULMFiT vs. training from scratch
with different numbers of training examples on IMDb, TREC-6, and AG (from left to right).

paragraphs long—similar to emails (e.g for legal
discovery) and online comments (e.g for commu-
nity management); and sentiment analysis is simi-
lar to many commercial applications, e.g. product
response tracking and support email routing.

On TREC-6, our improvement—similar as the
improvements of state-of-the-art approaches—is
not statistically significant, due to the small size of
the 500-examples test set. Nevertheless, the com-
petitive performance on TREC-6 demonstrates
that our model performs well across different
dataset sizes and can deal with examples that range
from single sentences—in the case of TREC-6—
to several paragraphs for IMDb. Note that despite
pretraining on more than two orders of magnitude
less data than the 7 million sentence pairs used by
McCann et al. (2017), we consistently outperform
their approach on both datasets.

We show the test error rates on the larger AG,
DBpedia, Yelp-bi, and Yelp-full datasets in Table
3. Our method again outperforms the state-of-
the-art significantly. On AG, we observe a simi-
larly dramatic error reduction by 23.7% compared
to the state-of-the-art. On DBpedia, Yelp-bi, and
Yelp-full, we reduce the error by 4.8%, 18.2%,
2.0% respectively.

5 Analysis

In order to assess the impact of each contribution,
we perform a series of analyses and ablations. We
run experiments on three corpora, IMDb, TREC-
6, and AG that are representative of different tasks,
genres, and sizes. For all experiments, we split off
10% of the training set and report error rates on
this validation set with unidirectional LMs. We
fine-tune the classifier for 50 epochs and train all
methods but ULMFiT with early stopping.

Low-shot learning One of the main benefits of
transfer learning is being able to train a model for

Pretraining IMDb TREC-6 AG

Without pretraining 5.63 10.67 5.52
With pretraining 5.00 5.69 5.38

Table 4: Validation error rates for ULMFiT with
and without pretraining.

a task with a small number of labels. We evalu-
ate ULMFiT on different numbers of labeled ex-
amples in two settings: only labeled examples are
used for LM fine-tuning (‘supervised’); and all
task data is available and can be used to fine-tune
the LM (‘semi-supervised’). We compare ULM-
FiT to training from scratch—which is necessary
for hypercolumn-based approaches. We split off
balanced fractions of the training data, keep the
validation set fixed, and use the same hyperparam-
eters as before. We show the results in Figure 3.

On IMDb and AG, supervised ULMFiT with
only 100 labeled examples matches the perfor-
mance of training from scratch with 10× and 20×
more data respectively, clearly demonstrating the
benefit of general-domain LM pretraining. If we
allow ULMFiT to also utilize unlabeled exam-
ples (50k for IMDb, 100k for AG), at 100 labeled
examples, we match the performance of training
from scratch with 50× and 100×more data on AG
and IMDb respectively. On TREC-6, ULMFiT
significantly improves upon training from scratch;
as examples are shorter and fewer, supervised and
semi-supervised ULMFiT achieve similar results.

Impact of pretraining We compare using no
pretraining with pretraining on WikiText-103
(Merity et al., 2017b) in Table 4. Pretraining is
most useful for small and medium-sized datasets,
which are most common in commercial applica-
tions. However, even for large datasets, pretrain-
ing improves performance.



335

LM IMDb TREC-6 AG

Vanilla LM 5.98 7.41 5.76
AWD-LSTM LM 5.00 5.69 5.38

Table 5: Validation error rates for ULMFiT with a
vanilla LM and the AWD-LSTM LM.

LM fine-tuning IMDb TREC-6 AG

No LM fine-tuning 6.99 6.38 6.09
Full 5.86 6.54 5.61
Full + discr 5.55 6.36 5.47
Full + discr + stlr 5.00 5.69 5.38

Table 6: Validation error rates for ULMFiT with
different variations of LM fine-tuning.

Impact of LM quality In order to gauge the im-
portance of choosing an appropriate LM, we com-
pare a vanilla LM with the same hyperparame-
ters without any dropout8 with the AWD-LSTM
LM with tuned dropout parameters in Table 5.
Using our fine-tuning techniques, even a regular
LM reaches surprisingly good performance on the
larger datasets. On the smaller TREC-6, a vanilla
LM without dropout runs the risk of overfitting,
which decreases performance.

Impact of LM fine-tuning We compare no fine-
tuning against fine-tuning the full model (Erhan
et al., 2010) (‘Full’), the most commonly used
fine-tuning method, with and without discrimi-
native fine-tuning (‘Discr’) and slanted triangular
learning rates (‘Stlr’) in Table 6. Fine-tuning the
LM is most beneficial for larger datasets. ‘Discr’
and ‘Stlr’ improve performance across all three
datasets and are necessary on the smaller TREC-6,
where regular fine-tuning is not beneficial.

Impact of classifier fine-tuning We compare
training from scratch, fine-tuning the full model
(‘Full’), only fine-tuning the last layer (‘Last’)
(Donahue et al., 2014), ‘Chain-thaw’ (Felbo et al.,
2017), and gradual unfreezing (‘Freez’). We fur-
thermore assess the importance of discriminative
fine-tuning (‘Discr’) and slanted triangular learn-
ing rates (‘Stlr’). We compare the latter to an
alternative, aggressive cosine annealing schedule
(‘Cos’) (Loshchilov and Hutter, 2017). We use a
learning rate ηL = 0.01 for ‘Discr’, learning rates

8To avoid overfitting, we only train the vanilla LM classi-
fier for 5 epochs and keep dropout of 0.4 in the classifier.

Classifier fine-tuning IMDb TREC-6 AG

From scratch 9.93 13.36 6.81
Full 6.87 6.86 5.81
Full + discr 4.57 6.21 5.62
Last 6.49 16.09 8.38
Chain-thaw 5.39 6.71 5.90
Freez 6.37 6.86 5.81
Freez + discr 5.39 5.86 6.04
Freez + stlr 5.04 6.02 5.35
Freez + cos 5.70 6.38 5.29
Freez + discr + stlr 5.00 5.69 5.38

Table 7: Validation error rates for ULMFiT with
different methods to fine-tune the classifier.

of 0.001 and 0.0001 for the last and all other layers
respectively for ‘Chain-thaw’ as in (Felbo et al.,
2017), and a learning rate of 0.001 otherwise. We
show the results in Table 7.

Fine-tuning the classifier significantly improves
over training from scratch, particularly on the
small TREC-6. ‘Last’, the standard fine-tuning
method in CV, severely underfits and is never
able to lower the training error to 0. ‘Chain-
thaw’ achieves competitive performance on the
smaller datasets, but is outperformed significantly
on the large AG. ‘Freez’ provides similar per-
formance as ‘Full’. ‘Discr’ consistently boosts
the performance of ‘Full’ and ‘Freez’, except
for the large AG. Cosine annealing is competi-
tive with slanted triangular learning rates on large
data, but under-performs on smaller datasets. Fi-
nally, full ULMFiT classifier fine-tuning (bottom
row) achieves the best performance on IMDB and
TREC-6 and competitive performance on AG. Im-
portantly, ULMFiT is the only method that shows
excellent performance across the board—and is
therefore the only universal method.

Classifier fine-tuning behavior While our re-
sults demonstrate that how we fine-tune the clas-
sifier makes a significant difference, fine-tuning
for inductive transfer is currently under-explored
in NLP as it mostly has been thought to be un-
helpful (Mou et al., 2016). To better understand
the fine-tuning behavior of our model, we compare
the validation error of the classifier fine-tuned with
ULMFiT and ‘Full’ during training in Figure 4.

On all datasets, fine-tuning the full model leads
to the lowest error comparatively early in train-
ing, e.g. already after the first epoch on IMDb.



336

Figure 4: Validation error rate curves for fine-
tuning the classifier with ULMFiT and ‘Full’ on
IMDb, TREC-6, and AG (top to bottom).

The error then increases as the model starts to
overfit and knowledge captured through pretrain-
ing is lost. In contrast, ULMFiT is more sta-
ble and suffers from no such catastrophic forget-
ting; performance remains similar or improves un-
til late epochs, which shows the positive effect of
the learning rate schedule.

Impact of bidirectionality At the cost of train-
ing a second model, ensembling the predictions of
a forward and backwards LM-classifier brings a
performance boost of around 0.5–0.7. On IMDb
we lower the test error from 5.30 of a single model
to 4.58 for the bidirectional model.

6 Discussion and future directions

While we have shown that ULMFiT can achieve
state-of-the-art performance on widely used text
classification tasks, we believe that language
model fine-tuning will be particularly useful in the
following settings compared to existing transfer
learning approaches (Conneau et al., 2017; Mc-
Cann et al., 2017; Peters et al., 2018): a) NLP for
non-English languages, where training data for su-
pervised pretraining tasks is scarce; b) new NLP
tasks where no state-of-the-art architecture exists;
and c) tasks with limited amounts of labeled data
(and some amounts of unlabeled data).

Given that transfer learning and particularly
fine-tuning for NLP is under-explored, many fu-
ture directions are possible. One possible direc-
tion is to improve language model pretraining and
fine-tuning and make them more scalable: for
ImageNet, predicting far fewer classes only in-
curs a small performance drop (Huh et al., 2016),
while recent work shows that an alignment be-
tween source and target task label sets is impor-
tant (Mahajan et al., 2018)—focusing on predict-
ing a subset of words such as the most frequent
ones might retain most of the performance while
speeding up training. Language modeling can also
be augmented with additional tasks in a multi-task
learning fashion (Caruana, 1993) or enriched with
additional supervision, e.g. syntax-sensitive de-
pendencies (Linzen et al., 2016) to create a model
that is more general or better suited for certain
downstream tasks, ideally in a weakly-supervised
manner to retain its universal properties.

Another direction is to apply the method to
novel tasks and models. While an extension to
sequence labeling is straightforward, other tasks
with more complex interactions such as entailment
or question answering may require novel ways to
pretrain and fine-tune. Finally, while we have
provided a series of analyses and ablations, more
studies are required to better understand what
knowledge a pretrained language model captures,
how this changes during fine-tuning, and what in-
formation different tasks require.

7 Conclusion

We have proposed ULMFiT, an effective and ex-
tremely sample-efficient transfer learning method
that can be applied to any NLP task. We have also
proposed several novel fine-tuning techniques that
in conjunction prevent catastrophic forgetting and
enable robust learning across a diverse range of
tasks. Our method significantly outperformed ex-
isting transfer learning techniques and the state-
of-the-art on six representative text classification
tasks. We hope that our results will catalyze new
developments in transfer learning for NLP.

Acknowledgments

We thank the anonymous reviewers for their valu-
able feedback. Sebastian is supported by Irish
Research Council Grant Number EBPPG/2014/30
and Science Foundation Ireland Grant Number
SFI/12/RC/2289.



337

References
Jonathan Baxter. 2000. A Model of Inductive Bias

Learning. Journal of Artificial Intelligence Research
12:149–198.

John Blitzer, Mark Dredze, and Fernando Pereira.
2007. Biographies, bollywood, boom-boxes
and blenders: Domain adaptation for sentiment
classification. Annual Meeting-Association
for Computational Linguistics 45(1):440.
https://doi.org/10.1109/IRPS.2011.5784441.

Cornelia Caragea, Nathan McNeese, Anuj Jaiswal,
Greg Traylor, Hyun-Woo Kim, Prasenjit Mitra,
Dinghao Wu, Andrea H Tapia, Lee Giles, Bernard J
Jansen, et al. 2011. Classifying text messages for the
haiti earthquake. In Proceedings of the 8th interna-
tional conference on information systems for crisis
response and management (ISCRAM2011). Citeseer.

Rich Caruana. 1993. Multitask learning: A
knowledge-based source of inductive bias. In Pro-
ceedings of the Tenth International Conference on
Machine Learning.

Zhao Chen, Vijay Badrinarayanan, Chen-Yu Lee, and
Andrew Rabinovich. 2017. GradNorm: Gradient
Normalization for Adaptive Loss Balancing in Deep
Multitask Networks pages 1–10.

Zi Chu, Steven Gianvecchio, Haining Wang, and Sushil
Jajodia. 2012. Detecting automation of twitter ac-
counts: Are you a human, bot, or cyborg? IEEE
Transactions on Dependable and Secure Computing
9(6):811–824.

Alexis Conneau, Douwe Kiela, Holger Schwenk, Loı̈c
Barrault, and Antoine Bordes. 2017. Supervised
Learning of Universal Sentence Representations
from Natural Language Inference Data. In Proceed-
ings of the 2017 Conference on Empirical Methods
in Natural Language Processing.

Andrew M. Dai and Quoc V. Le. 2015. Semi-
supervised Sequence Learning. Advances in Neu-
ral Information Processing Systems (NIPS ’15)
http://arxiv.org/abs/1511.01432.

Jeff Donahue, Yangqing Jia, Oriol Vinyals, Judy Hoff-
man, Ning Zhang, Eric Tzeng, and Trevor Darrell.
2014. Decaf: A deep convolutional activation fea-
ture for generic visual recognition. In International
conference on machine learning. pages 647–655.

Timothy Dozat and Christopher D. Manning. 2017.
Deep Biaffine Attention for Neural Dependency
Parsing. In Proceedings of ICLR 2017.

Dumitru Erhan, Yoshua Bengio, Aaron Courville,
Pierre-Antoine Manzagol, Pascal Vincent, and Samy
Bengio. 2010. Why does unsupervised pre-training
help deep learning? Journal of Machine Learning
Research 11(Feb):625–660.

Bjarke Felbo, Alan Mislove, Anders Søgaard, Iyad
Rahwan, and Sune Lehmann. 2017. Using millions
of emoji occurrences to learn any-domain represen-
tations for detecting sentiment, emotion and sar-
casm. In Proceedings of the 2017 Conference on
Empirical Methods in Natural Language Process-
ing.

Kristina Gulordava, Piotr Bojanowski, Edouard Grave,
Tal Linzen, and Marco Baroni. 2018. Colorless
green recurrent networks dream hierarchically. In
Proceedings of NAACL-HLT 2018.

Bharath Hariharan, Pablo Arbeláez, Ross Girshick, and
Jitendra Malik. 2015. Hypercolumns for object seg-
mentation and fine-grained localization. In Proceed-
ings of the IEEE Conference on Computer Vision
and Pattern Recognition. pages 447–456.

Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian
Sun. 2016. Deep Residual Learning for Image
Recognition. In Proceedings of the IEEE Confer-
ence on Computer Vision and Pattern Recognition.

Gao Huang, Zhuang Liu, Kilian Q. Weinberger, and
Laurens van der Maaten. 2017. Densely Connected
Convolutional Networks. In Proceedings of CVPR
2017.

Minyoung Huh, Pulkit Agrawal, and Alexei A Efros.
2016. What makes ImageNet good for transfer
learning? arXiv preprint arXiv:1608.08614 .

Sergey Ioffe and Christian Szegedy. 2015. Batch nor-
malization: Accelerating deep network training by
reducing internal covariate shift. In International
Conference on Machine Learning. pages 448–456.

Nitin Jindal and Bing Liu. 2007. Review spam de-
tection. In Proceedings of the 16th international
conference on World Wide Web. ACM, pages 1189–
1190.

Rie Johnson and Tong Zhang. 2016. Supervised and
semi-supervised text categorization using lstm for
region embeddings. In International Conference on
Machine Learning. pages 526–534.

Rie Johnson and Tong Zhang. 2017. Deep pyramid
convolutional neural networks for text categoriza-
tion. In Proceedings of the 55th Annual Meeting of
the Association for Computational Linguistics (Vol-
ume 1: Long Papers). volume 1, pages 562–570.

Tal Linzen, Emmanuel Dupoux, and Yoav Gold-
berg. 2016. Assessing the ability of lstms to
learn syntax-sensitive dependencies. arXiv preprint
arXiv:1611.01368 .

Liyuan Liu, Jingbo Shang, Frank Xu, Xiang Ren, Huan
Gui, Jian Peng, and Jiawei Han. 2018. Empower
sequence labeling with task-aware neural language
model. In Proceedings of AAAI 2018.

https://doi.org/10.1109/IRPS.2011.5784441
https://doi.org/10.1109/IRPS.2011.5784441
https://doi.org/10.1109/IRPS.2011.5784441
https://doi.org/10.1109/IRPS.2011.5784441
http://arxiv.org/abs/1511.01432
http://arxiv.org/abs/1511.01432
http://arxiv.org/abs/1511.01432


338

Jonathan Long, Evan Shelhamer, and Trevor Darrell.
2015a. Fully convolutional networks for semantic
segmentation. In Proceedings of the IEEE Confer-
ence on Computer Vision and Pattern Recognition.
pages 3431–3440.

Mingsheng Long, Yue Cao, Jianmin Wang, and
Michael I. Jordan. 2015b. Learning Transferable
Features with Deep Adaptation Networks. In Pro-
ceedings of the 32nd International Conference on
Machine learning (ICML ’15). volume 37.

Ilya Loshchilov and Frank Hutter. 2017. SGDR:
Stochastic Gradient Descent with Warm Restarts. In
Proceedings of the Internal Conference on Learning
Representations 2017.

Andrew L Maas, Raymond E Daly, Peter T Pham, Dan
Huang, Andrew Y Ng, and Christopher Potts. 2011.
Learning word vectors for sentiment analysis. In
Proceedings of the 49th Annual Meeting of the Asso-
ciation for Computational Linguistics: Human Lan-
guage Technologies-Volume 1. Association for Com-
putational Linguistics, pages 142–150.

Dhruv Mahajan, Ross Girshick, Vignesh Ramanathan,
Kaiming He, Manohar Paluri, Yixuan Li, Ashwin
Bharambe, and Laurens van der Maaten. 2018. Ex-
ploring the Limits of Weakly Supervised Pretraining
.

Bryan McCann, James Bradbury, Caiming Xiong, and
Richard Socher. 2017. Learned in Translation: Con-
textualized Word Vectors. In Advances in Neural
Information Processing Systems.

Stephen Merity, Nitish Shirish Keskar, and Richard
Socher. 2017a. Regularizing and Optimiz-
ing LSTM Language Models. arXiv preprint
arXiv:1708.02182 .

Stephen Merity, Caiming Xiong, James Bradbury, and
Richard Socher. 2017b. Pointer Sentinel Mixture
Models. In Proceedings of the International Con-
ference on Learning Representations 2017.

Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey
Dean. 2013. Distributed Representations of Words
and Phrases and their Compositionality. In Ad-
vances in Neural Information Processing Systems.

Sewon Min, Minjoon Seo, and Hannaneh Hajishirzi.
2017. Question Answering through Transfer Learn-
ing from Large Fine-grained Supervision Data. In
Proceedings of the 55th Annual Meeting of the As-
sociation for Computational Linguistics (Short Pa-
pers).

Takeru Miyato, Andrew M Dai, and Ian Good-
fellow. 2016. Adversarial training methods for
semi-supervised text classification. arXiv preprint
arXiv:1605.07725 .

Lili Mou, Zhao Meng, Rui Yan, Ge Li, Yan Xu,
Lu Zhang, and Zhi Jin. 2016. How Transferable are

Neural Networks in NLP Applications? Proceed-
ings of 2016 Conference on Empirical Methods in
Natural Language Processing .

Lili Mou, Hao Peng, Ge Li, Yan Xu, Lu Zhang, and
Zhi Jin. 2015. Discriminative neural sentence mod-
eling by tree-based convolution. In Proceedings of
the 2015 Conference on Empirical Methods in Nat-
ural Language Processing.

EWT Ngai, Yong Hu, YH Wong, Yijun Chen, and Xin
Sun. 2011. The application of data mining tech-
niques in financial fraud detection: A classifica-
tion framework and an academic review of literature.
Decision Support Systems 50(3):559–569.

Sinno Jialin Pan and Qiang Yang. 2010. A survey on
transfer learning. IEEE Transactions on Knowledge
and Data Engineering 22(10):1345–1359.

Matthew E Peters, Waleed Ammar, Chandra Bhagavat-
ula, and Russell Power. 2017. Semi-supervised se-
quence tagging with bidirectional language models.
In Proceedings of ACL 2017.

Matthew E Peters, Mark Neumann, Mohit Iyyer, Matt
Gardner, Christopher Clark, Kenton Lee, and Luke
Zettlemoyer. 2018. Deep contextualized word rep-
resentations. In Proceedings of NAACL 2018.

Alec Radford, Rafal Jozefowicz, and Ilya Sutskever.
2017. Learning to generate reviews and discovering
sentiment. arXiv preprint arXiv:1704.01444 .

Marek Rei. 2017. Semi-supervised multitask learning
for sequence labeling. In Proceedings of ACL 2017.

Herbert L Roitblat, Anne Kershaw, and Patrick Oot.
2010. Document categorization in legal electronic
discovery: computer classification vs. manual re-
view. Journal of the Association for Information
Science and Technology 61(1):70–80.

Sebastian Ruder. 2016. An overview of gradient
descent optimization algorithms. arXiv preprint
arXiv:1609.04747 .

Ruslan Salakhutdinov and Geoffrey Hinton. 2009.
Deep boltzmann machines. In Artificial Intelligence
and Statistics. pages 448–455.

Rico Sennrich, Barry Haddow, and Alexandra Birch.
2015. Improving neural machine translation
models with monolingual data. arXiv preprint
arXiv:1511.06709 .

Aliaksei Severyn and Alessandro Moschitti. 2015.
UNITN: Training Deep Convolutional Neural Net-
work for Twitter Sentiment Classification. Proceed-
ings of the 9th International Workshop on Semantic
Evaluation (SemEval 2015) pages 464–469.

Ali Sharif Razavian, Hossein Azizpour, Josephine Sul-
livan, and Stefan Carlsson. 2014. Cnn features off-
the-shelf: an astounding baseline for recognition. In
Proceedings of the IEEE conference on computer vi-
sion and pattern recognition. pages 806–813.



339

Leslie N Smith. 2017. Cyclical learning rates for train-
ing neural networks. In Applications of Computer
Vision (WACV), 2017 IEEE Winter Conference on.
IEEE, pages 464–472.

Vladimir Naumovich Vapnik and Samuel Kotz. 1982.
Estimation of dependences based on empirical data,
volume 40. Springer-Verlag New York.

Ellen M Voorhees and Dawn M Tice. 1999. The trec-8
question answering track evaluation. In TREC. vol-
ume 1999, page 82.

John Wieting and Kevin Gimpel. 2017. Revisiting Re-
current Networks for Paraphrastic Sentence Embed-
dings. In Proceedings of the 55th Annual Meeting of
the Association for Computational Linguistics (ACL
2017).

Jason Yosinski, Jeff Clune, Yoshua Bengio, and Hod
Lipson. 2014. How transferable are features in deep
neural networks? In Advances in neural information
processing systems. pages 3320–3328.

Xiang Zhang, Junbo Zhao, and Yann LeCun. 2015.
Character-level convolutional networks for text clas-
sification. In Advances in neural information pro-
cessing systems. pages 649–657.

Peng Zhou, Zhenyu Qi, Suncong Zheng, Jiaming Xu,
Hongyun Bao, and Bo Xu. 2016. Text classification
improved by integrating bidirectional lstm with two-
dimensional max pooling. In Proceedings of COL-
ING 2016.


