



















































Neural Word Segmentation Learning for Chinese


Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 409–420,
Berlin, Germany, August 7-12, 2016. c©2016 Association for Computational Linguistics

Neural Word Segmentation Learning for Chinese

Deng Cai and Hai Zhao∗
Department of Computer Science and Engineering

Key Lab of Shanghai Education Commission
for Intelligent Interaction and Cognitive Engineering

Shanghai Jiao Tong University, Shanghai, China
thisisjcykcd@gmail.com, zhaohai@cs.sjtu.edu.cn

Abstract

Most previous approaches to Chinese
word segmentation formalize this prob-
lem as a character-based sequence label-
ing task so that only contextual informa-
tion within fixed sized local windows and
simple interactions between adjacent tags
can be captured. In this paper, we pro-
pose a novel neural framework which thor-
oughly eliminates context windows and
can utilize complete segmentation history.
Our model employs a gated combination
neural network over characters to produce
distributed representations of word candi-
dates, which are then given to a long short-
term memory (LSTM) language scoring
model. Experiments on the benchmark
datasets show that without the help of
feature engineering as most existing ap-
proaches, our models achieve competitive
or better performances with previous state-
of-the-art methods.

1 Introduction

Most east Asian languages including Chinese are
written without explicit word delimiters, therefore,
word segmentation is a preliminary step for pro-
cessing those languages. Since Xue (2003), most
methods formalize the Chinese word segmentation
(CWS) as a sequence labeling problem with char-
acter position tags, which can be handled with su-

∗Corresponding author. This paper was partially sup-
ported by Cai Yuanpei Program (CSC No. 201304490199
and No. 201304490171), National Natural Science Founda-
tion of China (No. 61170114 and No. 61272248), National
Basic Research Program of China (No. 2013CB329401),
Major Basic Research Program of Shanghai Science and
Technology Committee (No. 15JC1400103), Art and Sci-
ence Interdisciplinary Funds of Shanghai Jiao Tong Univer-
sity (No. 14JCRZ04), and Key Project of National Society
Science Foundation of China (No. 15-ZDA041).

pervised learning methods such as Maximum En-
tropy (Berger et al., 1996; Low et al., 2005) and
Conditional Random Fields (Lafferty et al., 2001;
Peng et al., 2004; Zhao et al., 2006a). However,
those methods heavily depend on the choice of
handcrafted features.

Recently, neural models have been widely used
for NLP tasks for their ability to minimize the ef-
fort in feature engineering. For the task of CWS,
Zheng et al. (2013) adapted the general neural
network architecture for sequence labeling pro-
posed in (Collobert et al., 2011), and used char-
acter embeddings as input to a two-layer network.
Pei et al. (2014) improved upon (Zheng et al.,
2013) by explicitly modeling the interactions be-
tween local context and previous tag. Chen et al.
(2015a) proposed a gated recursive neural network
to model the feature combinations of context char-
acters. Chen et al. (2015b) used an LSTM archi-
tecture to capture potential long-distance depen-
dencies, which alleviates the limitation of the size
of context window but introduced another window
for hidden states.

Despite the differences, all these models are de-
signed to solve CWS by assigning labels to the
characters in the sequence one by one. At each
time step of inference, these models compute the
tag scores of character based on (i) context fea-
tures within a fixed sized local window and (ii)
tagging history of previous one.

Nevertheless, the tag-tag transition is insuffi-
cient to model the complicated influence from
previous segmentation decisions, though it could
sometimes be a crucial clue to later segmentation
decisions. The fixed context window size, which
is broadly adopted by these methods for feature
engineering, also restricts the flexibility of model-
ing diverse distances. Moreover, word-level infor-
mation, which is being the greater granularity unit
as suggested in (Huang and Zhao, 2006), remains

409



Models Characters Words Tags

character based
(Zheng et al., 2013), . . . ci−2, ci−1, ci, ci+1, ci+2 - ti−1ti

(Chen et al., 2015b) c0, c1, . . . , ci, ci+1, ci+2 - ti−1ti

word based
(Zhang and Clark, 2007), . . . c in wj−1, wj , wj+1 wj−1, wj , wj+1 -

Ours c0, c1, . . . , ci w0, w1, . . . , wj -

Table 1: Feature windows of different models. i(j) indexes the current character(word) that is under
scoring.

unemployed.
To alleviate the drawbacks inside previous

methods and release those inconvenient constrains
such as the fixed sized context window, this pa-
per makes a latest attempt to re-formalize CWS
as a direct segmentation learning task. Our
method does not make tagging decisions on in-
dividual characters, but directly evaluates the rel-
ative likelihood of different segmented sentences
and then search for a segmentation with the high-
est score. To feature a segmented sentence, a
series of distributed vector representations (Ben-
gio et al., 2003) are generated to characterize the
corresponding word candidates. Such a repre-
sentation setting makes the decoding quite dif-
ferent from previous methods and indeed much
more challenging, however, more discriminative
features can be captured.

Though the vector building is word centered,
our proposed scoring model covers all three pro-
cessing levels from character, word until sen-
tence. First, the distributed representation starts
from character embedding, as in the context of
word segmentation, the n-gram data sparsity is-
sue makes it impractical to use word vectors im-
mediately. Second, as the word candidate rep-
resentation is derived from its characters, the in-
side character structure will also be encoded, thus
it can be used to determine the word likelihood
of its own. Third, to evaluate how a segmented
sentence makes sense through word interacting,
an LSTM (Hochreiter and Schmidhuber, 1997) is
used to chain together word candidates incremen-
tally and construct the representation of partially
segmented sentence at each decoding step, so that
the coherence between next word candidate and
previous segmentation history can be depicted.

To our best knowledge, our proposed approach
to CWS is the first attempt which explicitly mod-
els the entire contents of the segmenter’s state,
including the complete history of both segmenta-
tion decisions and input characters. The compar-

 Neural Network
  Scoring Model

           Decoder

···

··· ···

··· ···

Max-Margin 
   Training

自然/语言/处理

自 +1.5 自然语 -1.5

然语 -1.5 然语言 +0.7

言处理 -2.1 处理 +1.5

处理 +1.5

+1.2 +0.8

+2.0+2.3 +3.2

+0.3 +1.2

自然语言处理(input sequence) 自
/然语

言/处
理

(out
put 

sent
ence

)

(golden sentence)

Figure 1: Our framework.

isons of feature windows used in different mod-
els are shown in Table 1. Compared to both se-
quence labeling schemes and word-based models
in the past, our model thoroughly eliminates con-
text windows and can capture the complete history
of segmentation decisions, which offers more pos-
sibilities to effectively and accurately model seg-
mentation context.

2 Overview

We formulate the CWS problem as finding a map-
ping from an input character sequence x to a word
sequence y, and the output sentence y∗ satisfies:

y∗ = arg max
y∈GEN(x)

(
n∑
i=1

score(yi|y1, · · · , yi−1))

where n is the number of word candidates in y,
and GEN(x) denotes the set of possible segmenta-
tions for an input sequence x. Unlike all previous
works, our scoring function is sensitive to the com-
plete contents of partially segmented sentence.

As shown in Figure 1, to solve CWS in this
way, a neural network scoring model is designed
to evaluate the likelihood of a segmented sentence.
Based on the proposed model, a decoder is de-
veloped to find the segmented sentence with the
highest score. Meanwhile, a max-margin method
is utilized to perform the training by comparing

410



segmented sentence

Lookup Table

GCNN Unit

LSTM Unit

Predicting

Scoring

c1 c2 c3 c4 c5 c6 c7 c8

y1 y2 y3 y4

p1 p2 p3 p4

u

Figure 2: Architecture of our proposed neural network scoring model, where ci denotes the i-th input
character, yj denotes the learned representation of the j-th word candidate, pk denotes the prediction
for the (k + 1)-th word candidate and u is the trainable parameter vector for scoring the likelihood of
individual word candidates.

the structured difference of decoder output and the
golden segmentation. The following sections will
introduce each of these components in detail.

3 Neural Network Scoring Model

The score for a segmented sentence is computed
by first mapping it into a sequence of word candi-
date vectors, then the scoring model takes the vec-
tor sequence as input, scoring on each word can-
didate from two perspectives: (1) how likely the
word candidate itself can be recognized as a legal
word; (2) how reasonable the link is for the word
candidate to follow previous segmentation history
immediately. After that, the word candidate is ap-
pended to the segmentation history, updating the
state of the scoring system for subsequent judge-
ments. Figure 2 illustrates the entire scoring neu-
ral network.

3.1 Word Score

Character Embedding. While the scores are
decided at the word-level, using word embedding
(Bengio et al., 2003; Wang et al., 2016) imme-
diately will lead to a remarkable issue that rare
words and out-of-vocabulary words will be poorly
estimated (Kim et al., 2015). In addition, the
character-level information inside an n-gram can
be helpful to judge whether it is a true word.
Therefore, a lookup table of character embeddings
is used as the bottom layer.

Formally, we have a character dictionary D of

size |D |. Then each character c ∈ D is repre-
sented as a real-valued vector (character embed-
ding) c ∈ Rd, where d is the dimensionality of the
vector space. The character embeddings are then
stacked into an embedding matrix M ∈ Rd×|D|.
For a character c ∈ D , its character embedding
c ∈ Rd is retrieved by the embedding layer ac-
cording to its index.

Gated Combination Neural Network. In order
to obtain word representation through its charac-
ters, in the simplest strategy, character vectors are
integrated into their word representation using a
weight matrix W(L) that is shared across all words
with the same length L, followed by a non-linear
function g(·). Specifically, ci (1 ≤ i ≤ L) are
d-dimensional character vector representations re-
spectively, the corresponding word vector w will
be d-dimensional as well:

w = g(W(L)

 c1...
cL

) (1)
where W(L) ∈ Rd×Ld and g is a non-linear func-
tion as mentioned above.

Although the mechanism above seems to work
well, it can not sufficiently model the complicated
combination features in practice, yet.

Gated structure in neural network can be useful
for hybrid feature extraction according to (Chen et
al., 2015a; Chung et al., 2014; Cho et al., 2014),

411



c1

cL

ŵ w

r1

rL

zN

z1

zL

Figure 3: Gated combination neural network.

we therefore propose a gated combination neu-
ral network (GCNN) especially for character com-
positionality which contains two types of gates,
namely reset gate and update gate. Intuitively, the
reset gates decide which part of the character vec-
tors should be mixed while the update gates decide
what to preserve when combining the characters
information. Concretely, for words with length L,
the word vector w ∈ Rd is computed as follows:

w = zN � ŵ +
L∑
i=1

zi � ci

where zN , zi (1 ≤ i ≤ L) are update gates for new
activation ŵ and governed characters respectively,
and � indicates element-wise multiplication.

The new activation ŵ is computed as:

ŵ = tanh(W(L)

 r1 � c1...
rL � cL

)
where W(L) ∈ Rd×Ld and ri ∈ Rd (1 ≤ i ≤ L)
are the reset gates for governed characters respec-
tively, which can be formalized as: r1...

rL

 = σ(R(L)
 c1...

cL

)
where R(L) ∈ RLd×Ld is the coefficient matrix of
reset gates and σ denotes the sigmoid function.

The update gates can be formalized as:
zN
z1
...

zL

 = exp(U(L)


ŵ
c1
...

cL

)�


1/Z
1/Z

...
1/Z


where U(L) ∈ R(L+1)d×(L+1)d is the coefficient
matrix of update gates, and Z ∈ Rd is the normal-

ization vector,

Zk =
L∑
i=1

[exp(U(L)


ŵ
c1
...

cL

)]d×i+k
where 0 ≤ k < d.

According to the normalization condition, the
update gates are constrained by:

zN +
L∑
i=1

zi = 1

The gated mechanism is capable of capturing
both character and character interaction character-
istics to give an efficient word representation (See
Section 6.3).

Word Score. Denote the learned vector rep-
resentations for a segmented sentence y with
[y1,y2, · · · ,yn], where n is the number of word
candidates in the sentence. word score will be
computed by the dot products of vector yi(1 ≤
i ≤ n) and a trainable parameter vector u ∈ Rd.

Word Score(yi) = u · yi (2)

It indicates how likely a word candidate by itself
is to be a true word.

3.2 Link Score

Inspired by the recurrent neural network language
model (RNN-LM) (Mikolov et al., 2010; Sunder-
meyer et al., 2012), we utilize an LSTM system to
capture the coherence in a segmented sentence.

Long Short-Term Memory Networks. The
LSTM neural network (Hochreiter and Schmid-
huber, 1997) is an extension of the recurrent neu-
ral network (RNN), which is an effective tool for
sequence modeling tasks using its hidden states
for history information preservation. At each time
step t, an RNN takes the input xt and updates its
recurrent hidden state ht by

ht = g(Uht−1 + Wxt + b)

where g is a non-linear function.
Although RNN is capable, in principle, to pro-

cess arbitrary-length sequences, it can be difficult
to train an RNN to learn long-range dependencies
due to the vanishing gradients. LSTM addresses

412



yt−1 pt yt pt+1 yt+1 pt+2

ht−1 ht ht+1

Figure 4: Link scores (dashed lines).

this problem by introducing a memory cell to pre-
serve states over long periods of time, and con-
trols the update of hidden state and memory cell
by three types of gates, namely input gate, for-
get gate and output gate. Concretely, each step
of LSTM takes input xt,ht−1, ct−1 and produces
ht, ct via the following calculations:

it = σ(Wixt + Uiht−1 + bi)

ft = σ(Wfxt + Ufht−1 + bf )
ot = σ(Woxt + Uoht−1 + bo)
ĉt = tanh(Wcxt + Ucht−1 + bc)
ct = ft � ct−1 + it � ĉt
ht = ot � tanh(ct)

where σ,� are respectively the element-wise sig-
moid function and multiplication, it, ft,ot, ct are
respectively the input gate, forget gate, output gate
and memory cell activation vector at time t, all of
which have the same size as hidden state vector
ht ∈ RH .
Link Score. LSTMs have been shown to outper-
form RNNs on many NLP tasks, notably language
modeling (Sundermeyer et al., 2012).

In our model, LSTM is utilized to chain to-
gether word candidates in a left-to-right, incre-
mental manner. At time step t, a prediction pt+1 ∈
Rd about next word yt+1 is made based on the hid-
den state ht:

pt+1 = tanh(Wpht + bp)

link score for next word yt+1 is then computed as:

Link Score(yt+1) = pt+1 · yt+1 (3)

Due to the structure of LSTM, the prediction
vector pt+1 carries useful information detected
from the entire segmentation history, including
previous segmentation decisions. In this way, our
model gains the ability of sequence-level discrim-
ination rather than local optimization.

3.3 Sentence score
Sentence score for a segmented sentence y with n
word candidates is computed by summing up word
scores (2) and link scores (3) as follow:

s(y[1:n], θ) =
n∑
t=1

(u · yt + pt · yt) (4)

where θ is the parameter set used in our model.

4 Decoding

The total number of possible segmented sentences
grows exponentially with the length of character
sequence, which makes it impractical to compute
the scores of every possible segmentation. In order
to get exact inference, most sequence-labeling sys-
tems address this problem with a Viterbi search,
which takes the advantage of their hypothesis
that the tag interactions only exist within adjacent
characters (Markov assumption). However, since
our model is intended to capture complete his-
tory of segmentation decisions, such dynamic pro-
gramming algorithms can not be adopted in this
situation.

Algorithm 1 Beam Search.
Input: model parameters θ

beam size k
maximum word length w
input character sequence c[1 : n]

Output: Approx. k best segmentations
1: π[0]← {(score = 0,h = h0, c = c0)}
2: for i = 1 to n do
3: . Generate Candidate Word Vectors
4: X ← ∅
5: for j = max(1, i− w) to i do
6: w = GCNN-Procedure(c[j : i])
7: X.add((index = j − 1, word = w))
8: end for
9: . Join Segmentation

10: Y ← { y.append(x) | y ∈ π[x.index] and
x ∈ X}

11: . Filter k-Max
12: π[i]← k- arg max

y∈Y
y.score

13: end for
14: return π[n]

To make our model efficient in practical use, we
propose a beam-search algorithm with dynamic
programming motivations as shown in Algorithm
1. The main idea is that any segmentation of the

413



first i characters can be separated as two parts, the
first part consists of characters with indexes from
0 to j that is denoted as y, the rest part is the word
composed by c[j+1 : i]. The influence from previ-
ous segmentation y can be represented as a triple
(y.score, y.h, y.c), where y.score, y.h, y.c in-
dicate the current score, current hidden state vec-
tor and current memory cell vector respectively.
Beam search ensures that the total time for seg-
menting a sentence of n characters is w × k × n,
where w, k are maximum word length and beam
size respectively.

5 Training

We use the max-margin criterion (Taskar et al.,
2005) to train our model. As reported in (Kum-
merfeld et al., 2015), the margin methods gen-
erally outperform both likelihood and perception
methods. For a given character sequence x(i), de-
note the correct segmented sentence for x(i) as
y(i). We define a structured margin loss ∆(y(i), ŷ)
for predicting a segmented sentence ŷ:

∆(y(i), ŷ) =
m∑
t=1

µ1{y(i),t 6= ŷt}

wherem is the length of sequence x(i) and µ is the
discount parameter. The calculation of margin loss
could be regarded as to count the number of in-
correctly segmented characters and then multiple
it with a fixed discount parameter for smoothing.
Therefore, the loss is proportional to the number
of incorrectly segmented characters.

Given a set of training setΩ, the regularized ob-
jective function is the loss function J(θ) including
an `2 norm term:

J(θ) =
1
|Ω|

∑
(x(i),y(i))∈Ω

li(θ) +
λ

2
||θ||22

li(θ) = max
ŷ∈GEN(x(i))

(s(ŷ, θ) + ∆(y(i), ŷ)− s(y(i), θ))

where the function s(·) is the sentence score de-
fined in equation (4).

Due to the hinge loss, the objective function is
not differentiable, we use a subgradient method
(Ratliff et al., 2007) which computes a gradient-
like direction. Following (Socher et al., 2013), we
use the diagonal variant of AdaGrad (Duchi et al.,
2011) with minibatchs to minimize the objective.

Character embedding size d = 50
Hidden unit number H = 50
Initial learning rate α = 0.2
Margin loss discount µ = 0.2
Regularization λ = 10−6

Dropout rate on input layer p = 0.2
Maximum word length w = 4

Table 2: Hyper-parameter settings.

The update for the i-th parameter at time step t is
as follows:

θt,i = θt−1,i − α√∑t
τ=1 g

2
τ,i

gt,i

where α is the initial learning rate and gτ,i ∈ R|θi|
is the subgradient at time step τ for parameter θi.

6 Experiments

6.1 Datasets
To evaluate the proposed segmenter, we use two
popular datasets, PKU and MSR, from the second
International Chinese Word Segmentation Bakeoff
(Emerson, 2005). These datasets are commonly
used by previous state-of-the-art models and neu-
ral network models.

Both datasets are preprocessed by replacing the
continuous English characters and digits with a
unique token. All experiments are conducted
with standard Bakeoff scoring program1 calculat-
ing precision, recall, and F1-score.

6.2 Hyper-parameters
Hyper-parameters of neural network model signif-
icantly impact on its performance. To determine
a set of suitable hyper-parameters, we divide the
training data into two sets, the first 90% sentences
as training set and the rest 10% sentences as de-
velopment set. We choose the hyper-parameters
as shown in Table 2.

We found that the character embedding size has
a limited impact on the performance as long as it
is large enough. The size 50 is chosen as a good
trade-off between speed and performance. The
number of hidden units is set to be the same as
the character embedding. Maximum word length
determines the number of parameters in GCNN
part and the time consuming of beam search, since
the words with a length l > 4 are relatively rare,

1http://www.sighan.org/bakeoff2003/score

414



92

93

94

95

96

0 10 20 30 40

beam size=2
beam size=4
beam size=8

epochs

F
1-

sc
or

e(
%

)

Figure 5: Performances of different beam sizes on
PKU dataset.

92

93

94

95

96

0 10 20 30 40

only word score
only link score
both

epochs

F
1-

sc
or

e(
%

)

Figure 6: Performances of different score strate-
gies on PKU dataset.

0.29% in PKU training data and 1.25% in MSR
training data, we set the maximum word length to
4 in our experiments.2

Dropout is a popular technique for improving
the performance of neural networks by reducing
overfitting (Srivastava et al., 2014). We also drop
the input layer of our model with dropout rate 20%
to avoid overfitting.

6.3 Model Analysis

Beam Size. We first investigated the impact of
beam size over segmentation performance. Fig-
ure 5 shows that a segmenter with beam size 4 is
enough to get the best performance, which makes
our model find a good balance between accuracy
and efficiency.

GCNN. We then studied the role of GCNN in
our model. To reveal the impact of GCNN, we
re-implemented a simplified version of our model,

2This 4-character limitation is just for consistence for both
datasets. We are aware that it is a too strict setting, especially
which makes additional performance loss in a dataset with
larger average word length, i.e., MSR.

models P R F
Single layer (d = 50) 94.3 93.7 94.0
GCNN (d = 50) 95.8 95.2 95.5
Single layer (d = 100) 94.9 94.4 94.7

Table 3: Performances of different models on
PKU dataset.

PKU MSR
+Dictionary ours theirs ours theirs

(Chen et al., 2015a) 94.9 95.9 95.8 96.2
(Chen et al., 2015b) 94.6 95.7 95.7 96.4

This work 95.7 - 96.4 -

Table 4: Comparison of using different Chinese
idiom dictionaries.3

which replaces the GCNN part with a single non-
linear layer as in equation (1). The results are
listed in Table 3, which demonstrate that the per-
formance is significantly boosted by exploiting the
GCNN architecture (94.0% to 95.5% on F1-score),
while the best performance that the simplified ver-
sion can achieve is 94.7%, but using a much larger
character embedding size.

Link Score & Word Score. We conducted sev-
eral experiments to investigate the individual ef-
fect of link score and word score, since these
two types of scores are intended to estimate the
sentence likelihood from two different perspec-
tives: the semantic coherence between words and
the existence of individual words. The learning
curves of models with different scoring strategies
are shown in Figure 6.

The model with only word score can be re-
garded as the situation that the segmentation de-
cisions are made only based on local window in-
formation. The comparisons show that such a
model gives moderate performance. By contrast,
the model with only link score gives a much bet-
ter performance close to the joint model, which
demonstrates that the complete segmentation his-
tory, which can not be effectively modeled in pre-
vious schemes, possesses huge appliance value for
word segmentation.

6.4 Results

3The dictionary used in (Chen et al., 2015a; Chen et al.,
2015b) is neither publicly released nor specified the exact
source until now. We have to re-run their code using our se-
lected dictionary to make a fair comparison. Our dictionary
has been submitted along with this submission.

415



Models
PKU MSR

P R F P R F
(Zheng et al., 2013) 92.8 92.0 92.4 92.9 93.6 93.3

(Pei et al., 2014) 93.7 93.4 93.5 94.6 94.2 94.4
(Chen et al., 2015a)* 94.6 94.2 94.4 94.6 95.6 95.1
(Chen et al., 2015b) * 94.6 94.0 94.3 94.5 95.5 95.0

This work 95.5 94.9 95.2 96.1 96.7 96.4
+Pre-trained character embedding

(Zheng et al., 2013) 93.5 92.2 92.8 94.2 93.7 93.9
(Pei et al., 2014) 94.4 93.6 94.0 95.2 94.6 94.9

(Chen et al., 2015a)* 94.8 94.1 94.5 94.9 95.9 95.4
(Chen et al., 2015b)* 95.1 94.4 94.8 95.1 96.2 95.6

This work 95.8 95.2 95.5 96.3 96.8 96.5

Table 5: Comparison with previous neural network models. Results with * are from our runs on their
released implementations.5

Models PKU MSR PKU MSR
(Tseng et al., 2005) 95.0 96.4 - -

(Zhang and Clark, 2007) 94.5 97.2 - -
(Zhao and Kit, 2008b) 95.4 97.6 - -

(Sun et al., 2009) 95.2 97.3 - -
(Sun et al., 2012) 95.4 97.4 - -

(Zhang et al., 2013) - - 96.1* 97.4*
(Chen et al., 2015a) 94.5 95.4 96.4* 97.6*
(Chen et al., 2015b) 94.8 95.6 96.5* 97.4*

This work 95.5 96.5 - -

Table 6: Comparison with previous state-of-the-art models. Results with * used external dictionary or
corpus.

We first compare our model with the latest neural
network methods as shown in Table 4. However,
(Chen et al., 2015a; Chen et al., 2015b) used an
extra preprocess to filter out Chinese idioms ac-
cording to an external dictionary.4 Table 4 lists
the results (F1-scores) with different dictionaries,
which show that our models perform better when
under the same settings.

Table 5 gives comparisons among previous neu-
ral network models. In the first block of Table 5,
the character embedding matrix M is randomly
initialized. The results show that our proposed
novel model outperforms previous neural network

4In detail, when a dictionary is used, a preprocess is per-
formed before training and test, which scans original text to
find out Chinese idioms included in the dictionary and replace
them with a unique token. This treatment does not strictly fol-
low the convention of closed-set setting defined by SIGHAN
Bakeoff, as no linguistic resources, either dictionary or cor-
pus, other than the training corpus, should be adopted.

5To make comparisons fair, we re-run their code
(https://github.com/dalstonChen) without their unspecified
Chinese idiom dictionary.

methods.
Previous works have found that the perfor-

mance can be improved by pre-training the char-
acter embeddings on large unlabeled data. There-
fore, we use word2vec (Mikolov et al., 2013)
toolkit6 to pre-train the character embeddings on
the Chinese Wikipedia corpus and use them for
initialization. Table 5 also shows the results
with additional pre-trained character embeddings.
Again, our model achieves better performance
than previous neural network models do.

Table 6 compares our models with previous
state-of-the-art systems. Recent systems such as
(Zhang et al., 2013), (Chen et al., 2015b) and
(Chen et al., 2015a) rely on both extensive feature
engineering and external corpora to boost perfor-
mance. Such systems are not directly compara-
ble with our models. In the closed-set setting, our
models can achieve state-of-the-art performance

6http://code.google.com/p/word2vec/

416



Max. word length F1 score Time (Days)
4 96.5 4
5 96.7 5
6 96.8 6

Table 7: Results on MSR dataset with different
maximum decoding word length settings.

on PKU dataset but a competitive result on MSR
dataset, which can attribute to too strict maximum
word length setting for consistence as it is well
known that MSR corpus has a much longer aver-
age word length (Zhao et al., 2010).

Table 7 demonstrates the results on MSR corpus
with different maximum decoding word lengths,
in which both F1 scores and training time are
given. The results show that the segmentation
performance can indeed further be improved by
allowing longer words during decoding, though
longer training time are also needed. As 6-
character words are allowed, F1 score on MSR can
be furthermore improved 0.3%.

For the running cost, we roughly report the cur-
rent computation consuming on PKU dataset.7 It
takes about two days to finish 50 training epochs
(for results in Figure 6 and the last row of Ta-
ble 6) only with two cores of an Intel i7-5960X
CPU. The requirement for RAM during training is
less than 800MB. The trained model can be saved
within 4MB on the hard disk.

7 Related Work

Neural Network Models. Most modern CWS
methods followed (Xue, 2003) treated CWS as a
sequence labeling problems (Zhao et al., 2006b).
Recently, researchers have tended to explore neu-
ral network based approaches (Collobert et al.,
2011) to reduce efforts of feature engineering
(Zheng et al., 2013; Qi et al., 2014; Chen et al.,
2015a; Chen et al., 2015b). They modeled CWS
as tagging problem as well, scoring tags on indi-
vidual characters. In those models, tag scores are
decided by context information within local win-
dows and the sentence-level score is obtained via
context-independently tag transitions. Pei et al.
(2014) introduced the tag embedding as input to
capture the combinations of context and tag his-
tory. However, in previous works, only the tag of
previous one character was taken into considera-
tion though theoretically the complete history of

7Our code is released at https://github.com/jcyk/CWS.

actions taken by the segmenter should be consid-
ered.

Alternatives to Sequence Labeling. Besides
sequence labeling schemes, Zhang and Clark
(2007) proposed a word-based perceptron method.
Zhang et al. (2012) used a linear-time incremental
model which can also benefits from various kinds
of features including word-based features. But
both of them rely heavily on massive handcrafted
features. Contemporary to this work, some neural
models (Zhang et al., 2016a; Liu et al., 2016) also
leverage word-level information. Specifically, Liu
et al. (2016) use a semi-CRF taking segment-level
embeddings as input and Zhang et al. (2016a) use
a transition-based framework.

Another notable exception is (Ma and Hinrichs,
2015), which is also an embedding-based model,
but models CWS as configuration-action match-
ing. However, again, this method only uses the
context information within limited sized windows.

Other Techniques. The proposed model might
furthermore benefit from some techniques in
recent state-of-the-art systems, such as semi-
supervised learning (Zhao and Kit, 2008b; Zhao
and Kit, 2008a; Sun and Xu, 2011; Zhao and Kit,
2011; Zeng et al., 2013; Zhang et al., 2013), incor-
porating global information (Zhao and Kit, 2007;
Zhang et al., 2016b), and joint models (Qian and
Liu, 2012; Li and Zhou, 2012).

8 Conclusion

This paper presents a novel neural framework for
the task of Chinese word segmentation, which
contains three main components: (1) a factory to
produce word representation when given its gov-
erned characters; (2) a sentence-level likelihood
evaluation system for segmented sentence; (3) an
efficient and effective algorithm to find the best
segmentation.

The proposed framework makes a latest attempt
to formalize word segmentation as a direct struc-
tured learning procedure in terms of the recent dis-
tributed representation framework.

Though our system outputs results that are bet-
ter than the latest neural network segmenters but
comparable to all previous state-of-the-art sys-
tems, the framework remains a great of potential
that can be further investigated and improved in
the future.

417



References
Yoshua Bengio, Réjean Ducharme, Pascal Vincent, and

Christian Janvin. 2003. A neural probabilistic lan-
guage model. The Journal of Machine Learning Re-
search, 3:1137–1155.

Adam L Berger, Vincent J Della Pietra, and Stephen
A Della Pietra. 1996. A maximum entropy ap-
proach to natural language processing. Computa-
tional linguistics, 22(1):39–71.

Xinchi Chen, Xipeng Qiu, Chenxi Zhu, and Xuanjing
Huang. 2015a. Gated recursive neural network for
chinese word segmentation. In Proceedings of the
53rd Annual Meeting of the Association for Compu-
tational Linguistics and the 7th International Joint
Conference on Natural Language Processing, pages
1744–1753.

Xinchi Chen, Xipeng Qiu, Chenxi Zhu, Pengfei Liu,
and Xuanjing Huang. 2015b. Long short-term
memory neural networks for chinese word segmen-
tation. In Proceedings of the 2015 Conference on
Empirical Methods in Natural Language Process-
ing, pages 1197–1206.

Kyunghyun Cho, Bart van Merrienboer, Caglar Gul-
cehre, Dzmitry Bahdanau, Fethi Bougares, Holger
Schwenk, and Yoshua Bengio. 2014. Learning
phrase representations using rnn encoder–decoder
for statistical machine translation. In Proceedings of
the 2014 Conference on Empirical Methods in Nat-
ural Language Processing, pages 1724–1734.

Junyoung Chung, Caglar Gulcehre, KyungHyun Cho,
and Yoshua Bengio. 2014. Empirical evaluation of
gated recurrent neural networks on sequence model-
ing. arXiv preprint arXiv:1412.3555.

Ronan Collobert, Jason Weston, Léon Bottou, Michael
Karlen, Koray Kavukcuoglu, and Pavel Kuksa.
2011. Natural language processing (almost) from
scratch. The Journal of Machine Learning Re-
search, 12:2493–2537.

John Duchi, Elad Hazan, and Yoram Singer. 2011.
Adaptive subgradient methods for online learning
and stochastic optimization. The Journal of Ma-
chine Learning Research, 12:2121–2159.

Thomas Emerson. 2005. The second international chi-
nese word segmentation bakeoff. In Proceedings of
the fourth SIGHAN workshop on Chinese language
Processing, volume 133.

Sepp Hochreiter and Jürgen Schmidhuber. 1997.
Long short-term memory. Neural computation,
9(8):1735–1780.

Chang-Ning Huang and Hai Zhao. 2006. Which is
essential for chinese word segmentation: Character
versus word. In The 20th Pacific Asia Conference
on Language, Information and Computation, pages
1–12.

Yoon Kim, Yacine Jernite, David Sontag, and Alexan-
der M Rush. 2015. Character-aware neural lan-
guage models. arXiv preprint arXiv:1508.06615.

Jonathan K. Kummerfeld, Taylor Berg-Kirkpatrick,
and Dan Klein. 2015. An empirical analysis of opti-
mization for max-margin nlp. In Proceedings of the
2015 Conference on Empirical Methods in Natural
Language Processing, pages 273–279.

John Lafferty, Andrew McCallum, and Fernando CN
Pereira. 2001. Conditional random fields: Prob-
abilistic models for segmenting and labeling se-
quence data. In Proceedings of the Eighteenth In-
terntional Conference on Machine Learning.

Zhongguo Li and Guodong Zhou. 2012. Unified de-
pendency parsing of chinese morphological and syn-
tactic structures. In Proceedings of the 2012 Joint
Conference on Empirical Methods in Natural Lan-
guage Processing and Computational Natural Lan-
guage Learning, pages 1445–1454.

Yijia Liu, Wanxiang Che, Jiang Guo, Bing Qin, and
Ting Liu. 2016. Exploring segment representations
for neural segmentation models. arXiv preprint
arXiv:1604.05499.

Jin Kiat Low, Hwee Tou Ng, and Wenyuan Guo. 2005.
A maximum entropy approach to chinese word seg-
mentation. In Proceedings of the Fourth SIGHAN
Workshop on Chinese Language Processing, volume
1612164, pages 448–455.

Jianqiang Ma and Erhard Hinrichs. 2015. Accurate
linear-time chinese word segmentation via embed-
ding matching. In Proceedings of the 53rd Annual
Meeting of the Association for Computational Lin-
guistics and the 7th International Joint Conference
on Natural Language Processing, pages 1733–1743.

Tomas Mikolov, Martin Karafiát, Lukas Burget, Jan
Cernockỳ, and Sanjeev Khudanpur. 2010. Re-
current neural network based language model. In
11th Annual Conference of the International Speech
Communication Association, pages 1045–1048.

Tomas Mikolov, Kai Chen, Greg Corrado, and Jef-
frey Dean. 2013. Efficient estimation of word
representations in vector space. arXiv preprint
arXiv:1301.3781.

Wenzhe Pei, Tao Ge, and Baobao Chang. 2014. Max-
margin tensor neural network for chinese word seg-
mentation. In Proceedings of the 52nd Annual Meet-
ing of the Association for Computational Linguis-
tics, pages 293–303.

Fuchun Peng, Fangfang Feng, and Andrew McCallum.
2004. Chinese segmentation and new word detec-
tion using conditional random fields. In Proceed-
ings of the 20th international conference on Compu-
tational Linguistics, page 562.

418



Yanjun Qi, Sujatha G Das, Ronan Collobert, and Jason
Weston. 2014. Deep learning for character-based
information extraction. In Advances in Information
Retrieval, pages 668–674.

Xian Qian and Yang Liu. 2012. Joint chinese word
segmentation, pos tagging and parsing. In Pro-
ceedings of the 2012 Joint Conference on Empirical
Methods in Natural Language Processing and Com-
putational Natural Language Learning, pages 501–
511.

Nathan D Ratliff, J Andrew Bagnell, and Martin Zinke-
vich. 2007. (approximate) subgradient methods for
structured prediction. In International Conference
on Artificial Intelligence and Statistics, pages 380–
387.

Richard Socher, John Bauer, Christopher D. Manning,
and Ng Andrew Y. 2013. Parsing with composi-
tional vector grammars. In Proceedings of the 51st
Annual Meeting of the Association for Computa-
tional Linguistics, pages 455–465.

Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky,
Ilya Sutskever, and Ruslan Salakhutdinov. 2014.
Dropout: A simple way to prevent neural networks
from overfitting. The Journal of Machine Learning
Research, 15(1):1929–1958.

Weiwei Sun and Jia Xu. 2011. Enhancing chinese
word segmentation using unlabeled data. In Pro-
ceedings of the Conference on Empirical Methods
in Natural Language Processing, pages 970–979.

Xu Sun, Yaozhong Zhang, Takuya Matsuzaki, Yoshi-
masa Tsuruoka, and Jun’ichi Tsujii. 2009. A dis-
criminative latent variable chinese segmenter with
hybrid word/character information. In Proceedings
of Human Language Technologies: The 2009 An-
nual Conference of the North American Chapter
of the Association for Computational Linguistics,
pages 56–64.

Xu Sun, Houfeng Wang, and Wenjie Li. 2012. Fast on-
line training with frequency-adaptive learning rates
for chinese word segmentation and new word de-
tection. In Proceedings of the 50th Annual Meet-
ing of the Association for Computational Linguis-
tics, pages 253–262.

Martin Sundermeyer, Ralf Schlüter, and Hermann Ney.
2012. Lstm neural networks for language model-
ing. In 13th Annual Conference of the International
Speech Communication Association.

Ben Taskar, Vassil Chatalbashev, Daphne Koller, and
Carlos Guestrin. 2005. Learning structured predic-
tion models: A large margin approach. In Proceed-
ings of the 22nd international conference on Ma-
chine learning, pages 896–903.

Huihsin Tseng, Pichuan Chang, Galen Andrew, Daniel
Jurafsky, and Christopher Manning. 2005. A condi-
tional random field word segmenter for sighan bake-
off 2005. In Proceedings of the fourth SIGHAN

workshop on Chinese language Processing, volume
171.

Peilu Wang, Yao Qian, Hai Zhao, Frank K. Soong, Lei
He, and Ke Wu. 2016. Learning distributed word
representations for bidirectional lstm recurrent neu-
ral network. In Proceedings of the 2016 Conference
of the North American Chapter of the Association
for Computational Linguistics: Human Language
Technologies.

Nianwen Xue. 2003. Chinese word segmentation as
character tagging. Computational Linguistics and
Chinese Language Processing, 8(1):29–48.

Xiaodong Zeng, Derek F. Wong, Lidia S. Chao, and Is-
abel Trancoso. 2013. Graph-based semi-supervised
model for joint chinese word segmentation and part-
of-speech tagging. In Proceedings of the 51st An-
nual Meeting of the Association for Computational
Linguistics, pages 770–779.

Yue Zhang and Stephen Clark. 2007. Chinese segmen-
tation with a word-based perceptron algorithm. In
Proceedings of the 45th Annual Meeting of the As-
sociation of Computational Linguistics, pages 840–
847.

Kaixu Zhang, Maosong Sun, and Changle Zhou. 2012.
Word segmentation on chinese mirco-blog data with
a linear-time incremental model. In Second CIPS-
SIGHAN Joint Conference on Chinese Language
Processing, pages 41–46.

Longkai Zhang, Houfeng Wang, Xu Sun, and Mairgup
Mansur. 2013. Exploring representations from un-
labeled data with co-training for Chinese word seg-
mentation. In Proceedings of the 2013 Conference
on Empirical Methods in Natural Language Pro-
cessing, pages 311–321.

Meishan Zhang, Yue Zhang, and Guohong Fu. 2016a.
Transition-based neural word segmentation. In Pro-
ceedings of the 54nd Annual Meeting of the Associ-
ation for Computational Linguistics.

Zhiong Zhang, Hai Zhao, and Lianhui Qin. 2016b.
Probabilistic graph-based dependency parsing with
convolutional neural network. In Proceedings of the
54nd Annual Meeting of the Association for Compu-
tational Linguistics.

Hai Zhao and Chunyu Kit. 2007. Incorporating
global information into supervised learning for chi-
nese word segmentation. In Proceedings of the 10th
Conference of the Pacific Association for Computa-
tional Linguistics, pages 66–74.

Hai Zhao and Chunyu Kit. 2008a. Exploiting unla-
beled text with different unsupervised segmentation
criteria for chinese word segmentation. Research in
Computing Science, 33:93–104.

419



Hai Zhao and Chunyu Kit. 2008b. Unsupervised
segmentation helps supervised learning of charac-
ter tagging for word segmentation and named entity
recognition. In Proceedings of the Third Interna-
tional Joint Conference on Natural Language Pro-
cessing, pages 106–111.

Hai Zhao and Chunyu Kit. 2011. Integrating unsu-
pervised and supervised word segmentation: The
role of goodness measures. Information Sciences,
181(1):163–183.

Hai Zhao, Chang-Ning Huang, and Mu Li. 2006a. An
improved chinese word segmentation system with
conditional random field. In Proceedings of the Fifth
SIGHAN Workshop on Chinese Language Process-
ing, volume 1082117.

Hai Zhao, Chang-Ning Huang, Mu Li, and Bao-Liang

Lu. 2006b. Effective tag set selection in chinese
word segmentation via conditional random field
modeling. In Proceedings of the 9th Pacific Asso-
ciation for Computational Linguistics, volume 20,
pages 87–94.

Hai Zhao, Chang-Ning Huang, Mu Li, and Bao-Liang
Lu. 2010. A unified character-based tagging frame-
work for chinese word segmentation. ACM Trans-
actions on Asian Language Information Processing,
9(2):5.

Xiaoqing Zheng, Hanyang Chen, and Tianyu Xu.
2013. Deep learning for Chinese word segmentation
and POS tagging. In Proceedings of the 2013 Con-
ference on Empirical Methods in Natural Language
Processing, pages 647–657.

420


