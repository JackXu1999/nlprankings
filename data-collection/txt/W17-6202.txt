



















































Parsing Minimalist Languages with Interpreted Regular Tree Grammars


Proceedings of the 13th International Workshop on Tree Adjoining Grammars and Related Formalisms (TAG+13), pages 11–20,
Umeå, Sweden, September 4–6, 2017. c© 2017 Association for Computational Linguistics

Parsing Minimalist Languages with Interpreted Regular Tree Grammars

Meaghan Fowlie
Saarland University

mfowlie@coli.uni-saarland.de

Alexander Koller
Saarland University

koller@coli.uni-saarland.de

1 Introduction

Minimalist Grammars (MGs) (Stabler, 1997) are
a formalisation of Chomsky’s minimalist pro-
gram (Chomsky, 1995), which currently domi-
nates much of mainstream syntax. MGs are sim-
ple and intuitive to work with, and are mildly
context sensitive (Michaelis, 1998), putting them
in the right general class for human language
(Joshi, 1985).1 Minimalist Grammars are known
to be more succinct than their Multiple Context-
Free equivalents (Stabler, 2013), to have regular
derivation tree languages (Kobele et al., 2007), and
to be recognisable in polynomial time (Harkema,
2001) with a bottom-up CKY-like parser. How-
ever, the polynomial is large, O(n4k+4) where k
is a grammar constant. By approaching minimal-
ist grammars from the perspective of Interpreted
Regular Tree Grammars, we show that standard
chart-based parsing is substantially computation-
ally cheaper than previously thought at O(n2k+3 ·
2k).

1.1 Notation

We treat functions as sets of pairs. For 〈a, b〉 ∈ f
we write a 7→ b. For a partial function f : A B,
the domain of f , Dom(f) = {a ∈ A | f(a) is
defined}. The set of all such functions is [A B].

For partial functions f, g : A B, let f ⊕ g =
f ∪ g if Dom(f) ∩ Dom(g) = ∅, and undefined
otherwise. For a ∈ A, let f−a = f−{(a, f(a))}

2 Minimalist Grammars

We begin with a brief overview of Minimalist
Grammars. Readers familiar with MGs should
note that we encode movers with a partial func-
tion from licensing features to movers, otherwise

1Or slightly below, if unbounded phrasal copying is re-
quired: see for example (Kobele, 2006) on Yoruba clefting.

Section 3 should be familiar. Minimalist Gram-
mars are a family of formal grammars in which
parts of sentences are put together with operations
merge and move. MGs are feature-driven, which
means that lexical items come with a stack of fea-
tures which determine whether operations apply.

Features encode properties of lexical items,
such as syntactic categories (noun, verb, etc), cat-
egories of arguments of the word (such as a verb
that selects a noun), as well as agreement or dis-
placement, such as person, case, quantifier rais-
ing, or wh-movement. For example, lexical item
〈Loki, D〉 has string part Loki and feature stack D,
meaning it has category D, while 〈laughed, =DV〉
has features =D and V, meaning that it requires
something of category D and is itself of category
V.

These requirements of the lexical items are ful-
filled by applications of Merge and Move opera-
tions. For example, laughed’s requirement of a D
is fulfilled by Merging it with Loki or who, for ex-
ample:

merge

〈laughed, =DV〉 〈Loki, D〉

Figure 1: Derivation tree of 〈laughed Loki, V〉

The diagram in 1 above illustrates a deriva-
tion tree, a term over 〈merge(2),move(1),Lex(0)〉
which describes the application of merge and
move to expressions.

An operation’s applicability is determined by
the features on the top of the feature stacks – the
head features – and the application deletes those
features; here the =D and D features are deleted,
leaving us with V. Notice that deleting D left Loki
without features; when features remain, something

11



different happens.
The lexical item 〈who, D-wh〉 has a D like Loki,

but also has a -wh feature which can ultimately
cause it to be pronounced in a different place in the
string than it would have if it had had only feature
D; this is movement. For instance, if instead we
applied Merge to laughed and who, deleting the
head features leaves -wh on who. This means that
although who is selected by laughed as an argu-
ment, its final position in the string will be deter-
mined by something else: the operation licensed
by its -wh feature. Because the final position is at
this point unknown, instead of trying to add it to
the string laughed, we store it for later insertion.

Our storage mechanism is a partial function
from features to moving items. When laughed se-
lects who and the D features are deleted, 〈who, �〉
is stored as the image of feature wh, as follows:

merge

〈〈laughed, =DV〉, ∅〉 〈who, D-wh〉

Figure 2: Derivation of 〈〈laughed, V〉,
{wh7→〈who, �〉 }〉

We call the partial function the storage and the
〈string, feature stack〉 pair the workspace. To-
gether they form an expression. Moving items,
or movers, are taken out of storage when their
head feature matches the head feature of the
workspace. For example, suppose our expression
〈〈laughed, V〉, {wh7→〈who,�〉 }〉 is selected by a
silent complementiser that triggers wh-movement,
〈�, =V +wh C〉. The string parts will remain un-
changed, and the storage untouched, but the head
feature in the workspace becomes +wh.

merge

〈�,=V +wh C〉 merge

〈laughed, =DV〉 〈who, D-wh〉

Figure 3: Derivation of 〈〈laughed, +wh C〉,
{wh7→〈who,�〉 }〉

The +wh feature triggers Move. We look in stor-
age for wh, find 〈who, �〉, delete the +wh feature,
and concatenate who with laughed:

move

merge

〈�, =V+whC〉 merge

〈laughed, =DV〉 〈who, D-wh〉

Figure 4: Derivation of 〈〈who laughed, C〉,∅〉

The item 〈who, �〉 had only its -wh feature left,
as represented by its place in storage and the �
where its features had been. If it had features left,
it would go back into storage after Move, as the
image of its new head feature. For example, if who
also had nominative case – 〈who, -nom-wh〉–, af-
ter moving for case it would go back into storage
under wh. Such a derivation would also require a
locus of case assignment; we add in a silent Tense
head, 〈�,=V+nomT〉. We illustrate this derivation
in Figure 5 with a derivation tree annotated by the
expression generated at each step.

move
〈〈laughed, V〉, {wh 7→〈who, �〉 }〉

merge
〈〈laughed, +nomT〉, {nom7→〈who, -wh〉 }〉

〈�, =V+nomT〉 merge
〈〈laughed, V〉, {nom7→〈who, -wh〉 }〉

〈laughed, =DV〉 〈who, D-nom-wh〉

Figure 5: Annotated derivation tree of non-final
Move: 〈who, -wh〉 has a feature remaining, so af-
ter Move applies it goes back into storage.

Intuitively, it is as though who started out beside
laughed because it is an argument of the verb. But
because it needed Case, it moved up to beside the
Tense. Next, because it is a wh-word, it will move
up to the front of the sentence.

2.1 Formal defintion
Formally, following Stabler and Keenan (2003),
we define a (string-generating) Minimalist Gram-
mar over expressions, with two finite, disjoint sets
of bare features. Selectional features sel , drive
Merge, and licensing features, lic drive Move.
Each of sel and lic has a positive and negative
polarity. A feature pairs a polarity with a bare
feature. Merge and Move apply when head fea-
tures of two items have the same bare feature,

12



but with opposite polarities. The features are
F={+f,-f,=X,X | f∈ lic , X ∈ sel }. Let Σ be a
finite alphabet. The lexicon Lex ⊂ Σ∗ × F ∗ is a
finite set of string-feature stack pairs.

An expression is a string-feature stack pair,
paired with a partial function from licensing fea-
tures to string-feature stack pairs; that is, expres-
sions are Expr = (Σ∗ × F ∗)× [lic  Σ∗ × F ∗]

MGs have one constraint: for a given nega-
tive feature -f, only one pair whose head fea-
ture is -f may be in storage. This is the short-
est move constraint (SMC), and we implement it
by defining storage as a partial function from lic
to 〈string, feature stack〉 pairs, and by defin-
ing storage parts of merge and move with ⊕ as
defined in section 1.1 above.

We define four partial functions,
merge1,merge2,move1, and move2, as fol-
lows.2 They have ⊕ as subfunctions, and are only
defined if their subfunctions are defined.
Merge merge1,merge2 : Expr × Expr Expr

Let α, β ∈ F ∗, let X∈ sel , and let f∈ lic .

merge1(〈(s, =Xα),S〉, 〈(t, X),T〉) = 〈〈st, α〉,S⊕T〉

merge2(〈〈s=Xα〉, Ss〉, 〈〈t, X-fβ〉, St〉) =
〈〈s, α〉, {f 7→〈t,β〉 }⊕Ss ⊕ St〉
Move move1,move2 : Expr Expr

Let α, β, γ ∈ F ∗, let f,g ∈ lic , and suppose
S(f) = 〈t,β〉.
move1〈〈s, +fα〉, S〉 = 〈〈ts, α〉, S− f〉 if β = �
move2(〈〈s, +fα〉, S〉)=
〈〈s,α〉, {g7→〈t, γ〉 }⊕(S−f)〉 if β=-gγ

For example, in the derivation in Figure 5,
the lowest merge node is an instance of merge2.
merge applies because the head feature of laughed
is =X and that of who is D. It is merge2 specif-
ically because, in the language of the definition
above, β = -nom-wh. The next merge node is
merge1 because the feature stack of laughed is just
V. The move node is an instance of move2 since
β = -wh 6= �.

An MG is a 6-tuple

g = 〈Σ, sel , lic,M,Lex, S〉

where Σ is a finite alphabet,
2The domains of Merge 1 and 2, and those of Move 1 and

2 being disjoint, the operations can alternatively be defined as
just Merge and Move with 2 cases each. We choose this vari-
ant for parallelism with the minimalist string algebra defined
in section 2.3 below.

M = {merge1,merge2,move1,move2},
Lex ⊂ Σ∗×{+f,-f,=X,X | f∈lic ,X∈sel}∗, and
S ⊆ sel is a designated set of start cate-
gories. From our two examples above, we
can define an MG g where Σ = {Loki,
laughed, who}, sel ={D,V,T,C}, lic ={nom,wh},
Lex={〈who, D-wh〉, 〈who, D-nom-wh〉, 〈Loki, D〉,
〈laughed, =D V〉, 〈�, =V +wh C〉, 〈�, =V +nom T〉,
〈�, =T +wh C〉 }, and S={T,C}.

Let Expr(〈s, fs〉) = 〈〈s, fs〉, ∅〉 make an ex-
pression with empty storage from a lexical item.
CL(Lex,M), the closure of the lexicon under the
operations M , is the closure of Expr(Lex) un-
der the operations. Then for a Minimalist Gram-
mar g with lexicon Lex and start categories S ⊆
sel , the language generated by g is L(g) =
{s | 〈〈s, S〉, ∅〉 ∈ CL(Lex,M) and S ∈ S}; that
is, for an expression with empty storage and ex-
actly one feature, where that feature is a start cate-
gory, the string part of that expression is in the lan-
guage of g. In our example, L(g) = {who laughed,
Loki laughed}.

If merge applies because the head feature of the
first expression is =X, we say that application of
merge is triggered by X. Similarly, if move ap-
plies because the head feature of the expression is
+f, we say the application of move is triggered by
f. merge2 and move2 always add a mover to stor-
age; if that mover is the image of feature f,then we
say it is an f-storing operation.

2.2 Expressive capacity

MGs are mildly context sensitive; in particular
they are are weakly equivalent to multiple context
free grammars (MCFGs) and Linear Context-Free
Rewrite Systems (LCFRSs) (Michaelis, 1998),
(Michaelis, 2001), and multicomponent tree-
adjoining grammars (MC-TAGs), which are more
expressive than TAGs. Every MG with k licensing
features has a strongly equivalent (k+1)-MCFG(2)
– an MCFG with at most binary rules and at most
k + 1-tuples –, where the category names are the
features of an expression and the strings behave
very much like the string tuple algebra in sec-
tion 2.3 below.3 An MG can be exponentially
more succinct than its equivalent MCFG (Stabler,
2013); similarly the IRTGs defined here can be ex-

3More preceisely, the licensing features are given an or-
der, and the MCFG category names, rather than having a par-
tial function from licensing features to feature stacks just has
the feature stacks in the right order, and similarly for the order
of elements in the tuples.

13



ponentially larger than the MGs they describe.

2.3 Minimalist String Algebra

Kobele et al. (2007) define an algebra of tree tu-
ples, which handles how the Minimalist Grammar
builds trees. We define a similar algebra which
builds strings, and convert the algebra into our no-
tation of a partial function from licensing features
to strings. These functions are just the string parts
of the MG operations, separated out from the fea-
ture calculus.

The values of the algebra are strings paired with
a partial function from lic to strings, i.e.
Σ∗ × [lic  Σ∗], which we call minimalist string
tuples. We define |lic| + 1 Merge operations
and |lic|2 + |lic| + 1 Move operations as follows,
∀f,g ∈ lic . merge1 and move1 are for final
merge/move, so the string of the merging or mov-
ing element concatenates (·) with the main string,
on the right for Merge and on the left for Move.
merge2f is for f-storing Merge, and move2g-f is
for f-storing Move triggered by g.

merge1(〈s, S〉, 〈t, T〉) = 〈s · t, S⊕ T〉
merge2f(〈s, S〉, 〈t, T〉) = 〈s, S⊕ T⊕ {f 7→ t}〉

move1f(〈s, S〉) = 〈S(f) · s, S− f〉
move2f-g(〈s, S〉) = 〈s, (S− f)

⊕ {g 7→ S(f)}〉
For an MG 〈Σ, sel ∪ lic,M,Lex〉, the signa-

ture of a tuple-feature algebra includes each ele-
ment s(0) of the alphabet Σ (evaluates to 〈s, ∅〉),
merge

(2)
1 (evaluates to merge1), merge

(2)
2f for each

f ∈ lic (evaluates to merge2f), move(1)1f for each
f ∈ lic (evaluates to move1f), and move(1)2f-g for
each pair f,g ∈ lic (evaluates to move2f-g).

If t = m(d0, . . . , dn) is a term of the signature
of the algebra, t evaluates to the function m evalu-
ates to, applied to what d0, . . . , dn evaluate to. We
write JtK = JmK(Jd0K, . . . , JdnK).

3 Interpreted Regular Tree Grammar

Minimalist Grammars lend themselves readily to
so-called “two-step” approaches in which the fea-
ture calculus is separated from the algebra of the
derived forms (strings, trees, etc). For instance,
Kobele et al. (2007) show that for a given MG, the
language of valid derivation trees is regular, and
that a derived tree can be generated by a multi-
bottom up transduction from the derivation tree.
Graf (2012) adds MSO-definable constraints on
the transduction to constrain movement and de-

fine different movement types (sidewards, lower-
ing, covert, etc).

Michaelis et al. (2000), Morawietz (2003), and
Mönnich (2006), etc. take a related approach, gen-
erating derived trees by Monadic Second-Order
(MSO)-definable transduction not from the deriva-
tion tree but rather from the equivalent MCGF,
translated into a regular tree grammar. (Kobele
et al. (2007) note that this second approach can
generate transductions that theirs cannot.)

In this tradition, we define an interpreted regular
tree grammar for Minimalist Grammars. IRTGs
are a generalisation of, among other things, the
synchronous grammars of Shieber (1994), 2004,
2006 that form the basis for the tree homomor-
phisms of Kobele et al. (2007).

3.1 IRTGs
An interpreted regular tree grammar
(IRTG) (Koller and Kuhlmann, 2011)
G = 〈G, (h1,A1), . . . , (hn,An)〉 derives n-
tuples of objects, such as strings or trees from
derivation trees in G. A given t ∈ G is interpreted
into the n algebras A1, . . .An by means of
the n tree homomorphisms h1, . . . , hn. For a
given i ≤ n, hi(t) is a term of the signature of
algebra Ai, which is in turn evaluated (J·KAi)
into an object of the algebra. For example,
suppose we have a minimalist string algebra
A as defined in Section 2.3, and suppose we
have a derivation tree as in the first tree in
Table 3, call it t. A tree homomorphism h that
includes the rules {mv1 7→ merge1nom,mg13 7→
merge1, lex11 7→ �,mg2 7→ merge2nom, lex9 7→
laughed, lex3 7→ Loki} yields the second tree
in Table 3, call it u. Then we write h(t) = u
and JuKA = 〈〈Loki laughed, T〉, ∅〉. The lan-
guage of the grammar L(G) is the set of tuples
{〈Jh1(t)KA1 , . . . , Jhn(t)KAn〉 | t ∈ L(G)}.

An IRTG is regular in that G is a regular tree
language, meaning it is a set of trees that can be
generated by a finite set of production rules of the
form NT0 → t or NT0 → t(NT1, . . . , NTn) for
nonterminals NTi and terminals t. The terminals
are elements of the signature of the tree language.
An example is given in Table 3.

3.2 IRTG for Minimalist Grammars
We use the regular tree language of derivation
trees defined in Kobele et al. (2007) and define a
homomorphism from the derivation trees to terms
of the minimalist string algebra (with notation

14



modified to match ours), explicitly defining it as
an IRTG. Finally, we calculate the parsing com-
plexity.

For a Minimalist Grammar g with lexicon Lex,
the production rules of its regular tree grammar,
RTG(g), have as their nonterminal symbols the
featural configurations of expressions defined by
the grammar. Let
f : [lic  Σ∗ × F ∗] → [lic  F ∗] strip away
the string parts of a storage function, leaving only
the features. Then the nonterminals of RTG(g)
are {〈fs, f(S)〉|〈〈s, fs〉, S〉 ∈ CL(Expr(Lex))}.
Since lexical items have finite feature stacks, the
SMC limits the size of the storage, and each appli-
cation of merge or move deletes features, there are
a finite number of nonterminals for a given finite
lexicon. Therefore each possible application of
merge or move to expressions of g belong to a fi-
nite set of instances; these are the non-lexical rules
of the RTG. Each lexical item 〈s, fs〉 has associ-
ated with it a rule with left hand side 〈fs, ∅〉. We
give each rule a name from {mgi,mvi, lexi | i ∈
N} by choosing a new i ∈ N for each rule: in an
IRTG, each rule has its own name. The rules are
named according to Table 1. The start categories
are {〈S, ∅〉 |S ∈ S}.
Example 3.1.

Let sel = {T,V,D,C}, lic = {nom,wh}. Let S =
{T, C} be the start categories Let Lex be defined
according to Table 2; for example, 〈Thor, D-nom〉
∈ Lex.

Table 3 lists the RTG production rules and con-
tains an example tree and its interprtation in the
minimalist string algebra defined in section 2.3
above. Rules that are greyed out are rules that can
never be used in a complete derivation; the RTG
could also be defined to leave them out.

3.3 Interpretation

The derivation trees – the terms over
{mg(2)i ,mv

(1)
i , lex

(0)
i | i ∈ N } – are inter-

preted in algebras, meaning for each algebra we
want to interpret into, we define a tree homo-
morphism from derivation trees to terms of the
algebra. In our case, we want to interpret into
the minimalist string algebra as follows. The
examples are from the grammar in Table 3.
Merge 1 Merge of a non-mover is interpreted as

merge1. e.g.:
h(mgi(t1, t2)) = merge1(h(t1), h(t2)) for
i ∈ {1, 3, 5, 7, 8, 12, 13, 14, 15, 16, 17}

Merge 2 f-storing Merge is interpreted
as merge2f. e.g.: h(mgi(t1, t2)) =
merge2nom(h(t1), h(t2)) for i ∈ {2, 6, 9, 11}
or merge2wh(h(t1), h(t2)) for i ∈ {4, 10}

Move 1 Final move triggered by f is interpreted
as move2f. e.g.:
h(mvi(t)) = move1nom(h(t)) for i ∈ {1, 2}

Move 2 g-storing Move triggered by f is inter-
preted as move2f-g. e.g.:
h(mv3(t)) = move2nom-wh(h(t))

Lex for a production rule 〈fs, ∅〉 → lexi, each
h(lexi) = s for some 〈s, fs〉 ∈ Lex. e.g.:
h(lex1) = h(lex3) = Loki

For example, the derivation tree in Table 3 is
interpreted by the homomorphism h as a term of
the string-feature tuple algebra, which evaluates to
the minimalist string tuple 〈Loki laughed, ∅〉.

4 IRTG-based parsing for minimalist
grammars

Given a minimalist grammar g, we can ask
whether a given string w is grammatical accord-
ing to g, i.e. if w ∈ L(g). This parsing problem
has been addressed in a substantial amount of lit-
erature (Harkema, 2001), (Stabler, 2013), (Stano-
jević, 2016). The best known upper bound for a
complete parser from this literature is O(n4k+4)
(Harkema, 2001). This is based on a relatively
coarse estimation, by which there are O(n2k+2)
different parse items, and binary rules such as
those for Merge could combine these arbitrarily.
Alternatively, by encoding g into an (k + 1)-
MCFG, we can apply standard parsing algorithms
for MCFGs, which yields a parsing complexity of
O(n3k+3) (Seki et al., 1991). The more efficient
MCFG parsing algorithm for well-nested MCFGs
of Gómez-Rodrı́guez et al. (2010), which would
yield a parsing complexity of O(n2k+4), is not ap-
plicable because the MCFGs that are produced by
the MG-to-MCFG encoding are not well-nested
(Boston et al., 2010).

Here we present a parsing algorithm for min-
imalist grammars that is based on the MG-to-
IRTG encoding. This algorithm has a runtime of
O(n2k+3), a substantial improvement over previ-
ously published upper bounds. It is worth noting
that we achieve this improved upper bound not
through a particularly clever parsing algorithm –
indeed, the basic idea of the algorithm presented
here is the same as in Harkema (2001) –, but
through a more careful analysis of the algorithm’s

15



MG rule application RTG production rule
merge1|2(〈〈s, fs〉, S〉, 〈〈t, ft〉,T〉) = 〈〈s′, fs′〉,S′〉 〈fs′, f (S′)〉 →mgx(〈fs, f (S)〉,〈ft, f (T)〉)
move1|2(〈〈s, fs〉, S〉) = 〈〈s′, fs′〉,S′〉 〈fs′, f (S′)〉 →mgx(〈fs, f (S)〉)
〈s, fs〉 ∈ Lex 〈fs, ∅〉 →lexx

Table 1: RTG(g) rule template

Types strings feature stacks
Nominals Loki, Thor D-nom | D
wh-words who D-nom-wh | D-wh
Intransitive verbs laughed, cried =D V
Transitive verbs slew, tricked =D =D V
Tense � =V +nom T
Complementiser � =T +wh C

Table 2: Sample lexicon

runtime. The primary advantage we obtain from
using the standard IRTG parsing algorithm is that
it separates the parts that depend on the string
length very cleanly from those that depend on the
grammar, which makes it a bit easier to see the
exact runtime complexity.

We will make a Java implementation of our
parsing algorithm available open-source upon
publication.

We first sketch the general approach to parsing
with IRTGs (Koller and Kuhlmann, 2011). The
objective of IRTG parsing is to compute, given
an input object w and an IRTG grammar G =
(G, (h,A)), a compact representation of the lan-
guage parses(w) = {t ∈ L(G) | Jh(t)K = w} –
i.e., of those derivation trees that are both gram-
matically correct and that are interpreted to w.
This is done by first computing a decomposition
grammar Dw, that is, an RTG such that L(Dw)
consists of all terms that evaluate to w in the al-
gebra. Then we can exploit closure properties of
regular tree languages to compute a parse chart
– that is, an RTG C such that L(C) = L(G) ∩
h−1(L(Dw)) –, by intersecting G with an RTG
that generates all trees which h maps to a term in
L(Dw). By construction, we have that L(C) =
parses(w).

Most pieces of this parsing algorithm are com-
pletely generic, and do not depend on the algebra
that is being used. Thus, when one applies IRTGs
to a new algebra, all that is required to obtain a
complete parser is to specify how decomposition
grammarsDw are computed for arbitrary elements
w of the algebra. We now explain how to obtain
decomposition grammars for the minimalist string
algebra.

Let w ∈ Σ∗ be a string that we want to parse
with an IRTG G over the minimalist string alge-
bra. The decomposition grammar Dw will de-
scribe a language of terms over this algebra, such
as the term in the lower left of Table 3. Let Sp be
the set of all spans in w, i.e. of all pairs (i, j) of
string positions with 1 ≤ i ≤ j ≤ n + 1. Then
the nonterminals of Dw will be pairs [s, S] where
s ∈ Sp and S : lic  Sp is a partial function that
assigns spans to licensing features. We assume
that s and the spans for all features are pairwise
disjoint. The start symbol is [(1, n+ 1), ∅].

Now consider first the constants c of the mini-
malist string algebra. These derive a span of length
one or, if c = �, of length zero. Thus we obtain the
following rules for Dw:

[(i, i+ 1), ∅] → c if wi = c ∈ Σ
[(i, i), ∅] → � for all 1 ≤ i ≤ n+ 1

Furthermore, terms can be combined into larger
terms using the merge and move operations. The
grammar Dw contains rules which essentially
evaluate these operations as defined in Section 2.3,
in terms of the spans represented by each sub-
string. Concretely, the rules look as in figure 6.

Rules in which ⊕ would yield undefined results
(because a feature would appear twice in a partial
function) do not exist in the grammar.

4.1 Parsing Complexity

Asymptotic parsing complexity is determined by
the time it takes to compute the rules of Dw; the
rest of the IRTG parsing algorithm is linear in the
size of Dw. The most costly rule of Dw, in terms
of parsing complexity, is that for merge1. In this
rule there are O(n3) values for the string positions
i, j, p. Within S ⊕ T there are spans for at most
k spans, each of which has O(n2) possible values.
These spans are distributed over the two child non-
terminals. This can be done in 2k different ways.
Thus, in total, there are areO(n2k+3 ·2k) instances
of this rule, which can be enumerated asymptoti-
cally in that time.

16



Lexical
〈D, ∅〉 → lex1 | lex2
〈D-nom, ∅〉 → lex3 | lex4
〈D-nom-wh, ∅〉 → lex5
〈D-wh, ∅〉 → lex6
〈=D=DV, ∅〉 → lex7 | lex8
〈=DV, ∅〉 → lex9 | lex10
〈=V+nomT, ∅〉 → lex11
〈=T+whC, ∅〉 → lex12
Derivation tree

mv1

mg13

lex11 mg2

lex9 lex3
→h Term of minimalist algebra

move1nom

merge1

� merge2nom

laughed Loki
→J·K 〈Loki laughed, ∅〉

Phrasal
merge of subject

〈V, ∅〉 → mg1( 〈=DV, ∅〉, 〈D, ∅〉 )
〈V, {nom7→∅}〉 → mg2( 〈=DV, ∅〉, 〈D-nom, ∅〉 ) |

mg3( 〈=DV, {nom7→∅}〉, 〈D, ∅〉 )
〈V, {wh7→∅}〉 → mg4( 〈=DV, ∅〉, 〈D-wh, ∅〉 ) |

mg5( 〈=DV, {wh 7→∅}〉, 〈D, ∅〉 )
〈V, {nom7→-wh}〉 → mg6( 〈=DV, ∅〉, 〈D-nom-wh, ∅〉 ) |

mg7( 〈=DV, {nom7→-wh}〉, 〈D, ∅〉 )
merge of object

〈=DV, ∅〉 → mg8( 〈=D=DV, ∅〉, 〈D, ∅〉 )
〈=DV, {nom7→∅}〉 → mg9( 〈=D=DV, ∅〉, 〈D-nom, ∅〉 )
〈=DV, {wh 7→∅}〉 → mg10( 〈=D=DV, ∅〉, 〈D-wh, ∅〉 )
〈=DV, {nom7→-wh}〉 → mg11( 〈=D=DV, ∅〉, 〈D-nom-wh, ∅〉 )

Tense selects VP
〈+nomT, ∅〉 → mg12(〈=V+nomT, ∅〉, 〈V, ∅〉)
〈+nomT, {nom7→∅}〉 → mg13(〈=V+nomT, ∅〉, 〈=DV, {nom7→∅}〉)
〈+nomT, {wh 7→∅}〉 → mg14(〈=V+nomT, ∅〉, 〈V, {wh7→∅}〉)
〈+nomT, {nom7→-wh}〉 → mg15(〈=V+nomT, ∅〉, 〈=DV, {nom7→-wh}〉)

Subject moves to spec-TP
〈T, ∅〉! → mv1(〈+nom T, {nom7→∅}〉)
〈T, {wh7→∅}〉 → mv2(〈+nom T, {nom7→-wh}〉)

C selects TP
〈+wh C, ∅〉 → mg16(〈=T +wh C, ∅〉 〈T, ∅〉)
〈+wh C, {wh 7→∅}〉 → mg17(〈=T +wh C, ∅〉 〈T, {wh 7→∅}〉)

wh-word moves to spec-CP
〈C, ∅〉! → mv3(〈+wh C, {wh 7→∅}〉)

Table 3: Example IRTG rules and an example derivation of Loki laughed

[(i, p), S⊕ T] → merge1([(i, j), S], [(j, p),T])
[(i, j), S⊕ T⊕ {f 7→ (p, l)}] → merge2f([(i, j),S], [(p, l),T])

[(i, p), S− f] → move1f([(j, p), S] if S(f) = (i, j)
[(i, j), (S− f)⊕ {g 7→ S(f)}] → move2f-g([(i, j),S]),

Figure 6: Decomposition rules

5 Comparison with other Mildly Context
Sensitive grammars

Mildly context sensitive grammars (Joshi, 1985)
frequently come with constants that limit the
number of pieces being manipulated by the
grammar. In Multiple Context-Free Grammars
(MCFGs) (Seki et al., 1991) and Linear Context-
Free Rewrite Systems (LCFRSs) (Vijay-Shanker
et al., 1987) these are the rank – the maximum
number of daughters/arguments a rule can have,
and the fanout – the maximum number of elements
in a tuple. In Minimalist Grammars it is the num-
ber of licensing features k, which limits the num-
ber of movers via the SMC. The maximum num-
ber of elements in a minimalist item is therefore
k + 1 – all possible movers plus the workspace.
Transforming an MG into an MCFG yields a
grammar with rank 2 and fanout k+ 1 (Michaelis,
1998). Our O(n2k+3 · 2k) expressed in terms of

fanout f = k + 1 is therefore O(n2f+1 · 2f−1),
which is less than the parsing complexity for an
arbitrary binary MCFG with fanout f : O(n3f ).

It is difficult to compare parsing complexities
across grammars, as moving from one grammar
to another can change the fanout. While MGs,
MCFGs, and LCFRSs with finite fanout generate
the same languages, an arbitrary binary MCFG of
fanout f may not have a weakly equivalent MG
with f − 1 licensing features; indeed Michaelis
(2001) shows that an LCFRS with fanout f has a
weakly equivalent MG with 3f licensing features.

In terms of the string algebra, the difference be-
tween an MCFG and an MG is that an MCFG rule
is unrestricted in how it concatenates strings; in
an MG, only the workspace can be made by con-
catenation; the movers are simply pooled into one
function, never concatenated with one another. In
this sense, MG equivalents of MCFGs are a sub-
class of general MCFGs of the same fanout, one

17



which has a lower parsing complexity. To trans-
form an MG into an MCFG we take as categories
the RTG categories, choose an (arbitrary) order on
the licensing features, and interpret the mover stor-
age partial function as tuples in the chosen order.
We call the class of MCFGs with string concatena-
tion rules restricted to the rules of the Minimalist
String Algebra MCFGMG.

Another subclass of MCFGs with lowered pars-
ing complexity is well-nested (Kuhlmann, 2007)
MCFGs (MCFGwn) in which no rule involves the
interleaving of elements from two daughters (no
abab rules). The parsing complexity of a binary
MCFGwn with fanout f is O(n2f+2), due to the
fact that there is a normal form in which all deduc-
tion rules are either concatenation rules or wrap-
ping rules, which have complexity O(n2f+1) and
O(n2f+2) respectively (Gómez-Rodrı́guez et al.,
2010). In a concatenation rule, we take one ele-
ment of each tuple and concatenate them, and the
rest are kept as they are; in a well-nested MCFG
the last element of the first daughter is concate-
nated with the first element of the second daughter,
which maintains the well-nestedness.

Interestingly, although the MCFG equivalent
of MGs is not well-nested, the argument for the
parsing complexity of merge1 is closely related
to that for MCFGwn. The well-nested concate-
nation rules have the same number of indices
as merge1. Therefore the complexity of merge1
(O(n2f+1 · 2f−1)) and concatenation rules for
parsing a MCFGwn (O(n2f+1)) have the same
polynomial degree, 2f + 1. This is perhaps
counter-intuitive, since well-nested MCFLs are a
proper subset of MCFLs/MLs (Gómez-Rodrı́guez
et al., 2010). However, as noted above, trans-
forming between grammars will often change the
fanout.

A proper subclass of well-nested MCFGs is
monadic-branching MCFGs (MCFGmb), which
are binary MCFGs in which only the right daugh-
ter may have fanout greater than 1. MGs with the
specifier island condition (SpIC), in which nothing
can move out of a specifier, are weakly equivalent
to monadic-branching MCFGs (Kanazawa et al.,
2011). These grammars have three kinds of Merge
rules: merge1, which merges a lexical item with
its complement; merge2, which merges a non-
lexical item with its specifier, and merge3, which
merges a mover. Move is restricted to prevent a
certain kind of movement from within a mover,

and Merge is restricted to prevent movement from
within a specifier. The result is grammar that never
has to combine mover lists. merge1 can’t have
movers in the selector, since lexical items never
carry movers, and merge2 is constrained by the
SpIC not to have movers in the selected item. Our
string-tuple analysis of minimalist parsing makes
it clear that SpIC-MGs have a parsing complexity
of O(n2k+3). The most complex rules are merge1
and merge2, which still have at most 3 indices for
the workspace and 2 for each mover. The only dif-
ference is that in the standard MG case, the movers
could have come from either daughter, but for a
SpIC-MG they could only have come from one
daughter. For SpIC-MGs the parsing complexity
is therefore reduced to O(n2k+3). For our parser
the difference is not necessarily huge since 2k is
a constant, but for some, like Stabler (2013)’s top-
down beam parser, the SpIC can greatly reduce the
search space.

Figure 7 shows the grammars described above.4

We don’t have a linguistic characterization of the
“?”-node, which stands for the intersection be-
tween the two higher nodes. These would be well-
nested MCFGs that only have concatenation in the
first element of the tuple. We speculate that this
is a linguistically uninteresting class, as the non-
well-nestedness of the rules is a reflection of the
arbitrarily-chosen order on the licensing features,
and has no special linguistic significance.5

6 Conclusion

Approaching Minimalist Grammars as interpreted
regular tree grammars makes clear the parsing
complexity of traditional chart-based parsing, and
the options available for interpretation of a deriva-
tion as a string. We found that the commonly-cited
upper bound of O(n4k+4) was in fact too conser-
vative, and MGs can be parsed in the much smaller
polynomial time of O(n2k+3 · 2k). MGs with the
specifier island constraint have a parsing complex-
ity of O(n2k+3).

4Note that the inclusion refers to the string algebra restric-
tions in the grammars themselves, and not necessarily to the
languages they generate. The left side of the diagram in fact
is reflected in the languages – for a given fanout and rank,
MCFLmb ( MCFLwn ( MCFL. We don’t make any claims
about the weak generative capacity on the right side.

5Also missing from the lattice is the class of MGs with
a looser SpIC where only Move is restricted by the SpIC.
This restriction leaves the asymptotic parsing complexity un-
changed as Merge is still the most complex rule and is un-
changed.

18



MCFG
O(n3f )

MCFGwn
O(n2f+2)

MCFGMG
O(n2f+1 · 2f−1)

?

MCFGmb = MCFGMG-SpIC
O(n2f+1)

Figure 7: Subclasses of binary MCFGs with
fanout f and their parsing complexities

References
Marisa Ferrara Boston, John Hale, and Marco

Kuhlmann. 2010. Dependency structures derived
from minimalist grammars. In Christian Ebert, Ger-
hard Jäger, and Jens Michaelis, editors, The Mathe-
matics of Language. 10th and 11th Biennial Confer-
ence, MOL 10, Revised Selected Papers. Springer,
volume 6149 of Lecture Notes in Computer Science,
pages 1–12.

Noam Chomsky. 1995. The Minimalist Program. MIT
Press, Cambridge, MA.

Carlos Gómez-Rodrı́guez, Marco Kuhlmann, and Gior-
gio Satta. 2010. Efficient parsing of well-nested lin-
ear context-free rewriting systems. In Human Lan-
guage Technologies: The 2010 Annual Conference
of the North American Chapter of the Association
for Computational Linguistics (NAACL 2010). As-
sociation for Computational Linguistics, pages 276–
284. http://www.aclweb.org/anthology/N10-1035.

Thomas Graf. 2012. Movement-generalized minimal-
ist grammars. In Denis Béchet and Alexander J.
Dikovsky, editors, LACL 2012. volume 7351 of Lec-
ture Notes in Computer Science, pages 58–73.

Henk Harkema. 2001. Parsing minimalist languages.
Ph.D. thesis, University of California Los Angeles.

Aravind Joshi. 1985. How much context-sensitivity is
necessary for characterizing structural descriptions.
In D. Dowty, L. Karttunen, and A. Zwicky, editors,
Natural Language Processing: Theoretical, Compu-
tational and Psychological Perspectives, Cambridge
University Press, New York, pages 206–250.

Makoto Kanazawa, Jens Michaelis, Sylvain Salvati,
and Ryo Yoshinaka. 2011. Well-nestedness prop-
erly subsumes strict derivational minimalism. In In-
ternational Conference on Logical Aspects of Com-
putational Linguistics. Springer, pages 112–128.

Greg Kobele. 2006. Generating copies. Ph.D. thesis,
UCLA.

Gregory M. Kobele, Christian Retoré, and Sylvain Sal-
vati. 2007. An automata-theoretic approach to min-
imalism. In J. Rogers and S. Kepser, editors, Model
Theoretic Syntax at ESSLLI ’07. ESSLLI.

Alexander Koller and Marco Kuhlmann. 2011. A gen-
eralized view on parsing and translation. In Pro-
ceedings of the 12th International Conference on
Parsing Technologies (IWPT). Dublin.

Marco Kuhlmann. 2007. Dependency Structures and
Lexicalized Grammars. Doctoral Dissertation, Saar-
land University, Saarbrücken, Germany.

Jens Michaelis. 1998. Derivational minimalism is
mildly context-sensitive. In LACL. Springer, vol-
ume 98, pages 179–198.

Jens Michaelis. 2001. Transforming linear context-
free rewriting systems into minimalist grammars.
In Philippe de Groote, Glyn Morrill, and Christian
Retoré, editors, Logical Aspects of Computational
Linguistics: 4th International Conference, LACL
2001 Le Croisic, France, June 27–29, 2001 Pro-
ceedings, Springer Berlin Heidelberg, Berlin, Hei-
delberg, pages 228–244. https://doi.org/10.1007/3-
540-48199-0 14.

Jens Michaelis, Uwe Mönnich, and Frank Morawietz.
2000. Derivational minimalism in two regular and
logical steps. In Proceedings of the 5th international
workshop on tree adjoining grammars and related
formalisms (tag+ 5).

Uwe Mönnich. 2006. Grammar morphisms. Ms. Uni-
versity of Tübingen .

Frank Morawietz. 2003. Two-Step Approaches to Nat-
ural Language Formalism, volume 64. Walter de
Gruyter.

H. Seki, T. Matsumura, M. Fujii, and T. Kasami. 1991.
On Multiple Context-Free Grammars. Theoretical
Computer Science 88(2):191–229.

Stuart Shieber. 2004. Synchronous grammars as tree
transducers. In Proceedings of the Seventh Inter-
national Workshop on Tree Adjoining Grammar and
Related Formalisms (TAG+ 7).

Stuart Shieber. 2006. Unifying synchronous tree-
adjoining grammars and tree transducers via bimor-
phisms. In Proceedings of the 11th Conference of
the European Chapter of the Association for Compu-
tational Linguistics (EACL-2006). Association for
Computational Linguistics.

Stuart M Shieber. 1994. Restricting the weak-
generative capacity of synchronous tree-adjoining
grammars. Computational Intelligence 10(4):371–
385.

Edward Stabler. 1997. Derivational minimalism. Logi-
cal Aspects of Computational Linguistics pages 68–
95.

19



Edward P Stabler. 2013. Two models of minimalist,
incremental syntactic analysis. Topics in Cognitive
Science 5(3):611–633.

Edward P Stabler and Edward L Keenan. 2003. Struc-
tural similarity within and among languages. Theo-
retical Computer Science 293(2):345–363.

Miloš Stanojević. 2016. Minimalist grammar
transition-based parsing. In Logical Aspects of
Computational Linguistics. Celebrating 20 Years of
LACL (1996–2016) 9th International Conference,
LACL 2016, Nancy, France, December 5-7, 2016,
Proceedings 9. Springer, pages 273–290.

Krishnamurti Vijay-Shanker, David J Weir, and Ar-
avind K Joshi. 1987. Characterizing structural de-
scriptions produced by various grammatical for-
malisms. In Proceedings of the 25th annual meeting
on Association for Computational Linguistics. As-
sociation for Computational Linguistics, pages 104–
111.

20


