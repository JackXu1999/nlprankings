



















































Multi-Level Matching and Aggregation Network for Few-Shot Relation Classification


Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 2872–2881
Florence, Italy, July 28 - August 2, 2019. c©2019 Association for Computational Linguistics

2872

Multi-Level Matching and Aggregation Network
for Few-Shot Relation Classification

Zhi-Xiu Ye, Zhen-Hua Ling∗
National Engineering Laboratory for Speech and Language Information Processing,

University of Science and Technology of China
zxye@mail.ustc.edu.cn, zhling@ustc.edu.cn

Abstract

This paper presents a multi-level matching and
aggregation network (MLMAN) for few-shot
relation classification. Previous studies on this
topic adopt prototypical networks, which cal-
culate the embedding vector of a query in-
stance and the prototype vector of each sup-
port set independently. In contrast, our pro-
posed MLMAN model encodes the query in-
stance and each support set in an interactive
way by considering their matching informa-
tion at both local and instance levels. The fi-
nal class prototype for each support set is ob-
tained by attentive aggregation over the rep-
resentations of its support instances, where
the weights are calculated using the query in-
stance. Experimental results demonstrate the
effectiveness of our proposed methods, which
achieve a new state-of-the-art performance on
the FewRel dataset1.

1 Introduction

Relation classification (RC) is a fundamental task
in natural language processing (NLP), which aims
to identify the semantic relation between two enti-
ties in text. For example, the instance “[London]e1
is the capital of [the UK]e2” expresses the relation
capital of between the two entities London and
the UK.

Some conventional relation classification meth-
ods (Bethard and Martin, 2007; Zelenko et al.,
2002) adopted supervised training and suffered
from the lack of large-scale manually labeled
data. To address this issue, the distant supervision
method (Mintz et al., 2009) was proposed which
annotated training data by heuristically aligning
knowledge bases (KBs) and texts. However, the
long-tail problem in KBs (Xiong et al., 2018; Han

∗Corresponding author: Zhen-Hua Ling.
1The code is available at https://github.com/

ZhixiuYe/MLMAN.

Support Set

class A: mother
instance #1 The Queen Consort [Jetsun Pema]e2 gave
birth to a son on 5 February 2016 , [Jigme Namgyel
Wangchuck]e1.
instance #2 He married the American actress [Cindy
Robbins]e2 and was stepfather to her daughter , [Kim-
berly Beck]e1.
instance #3 Edgar married actress [Moyna Macgill]e2
and became the father of [Angela Lansbury]e1.
instance #4 In 1845 , [Cemile Sultan]e1 ’s mother , Em-
press [Dzdidil Kadn]e2, died.
instance #5 Bo ’s wife [Gu Kailai]e2 traveled with their
son [Bo Guagua]e1 to Britain.

class B: member of ...

class C: father ...

class D: sport ...

class E: voice type ...

Query Instance

He was married to [Eva Funck]e2 and they have a son
[Gustav]e1 .

Table 1: A data example of 5-way-5-shot relation clas-
sification in FewRel development set. The correct re-
lation class for the query instance is class A: mother.
The instances for other relation classes are omitted for
saving space.

et al., 2018) still exists and makes it hard to clas-
sify the relations with very few training samples.

This paper focuses on the few-shot relation clas-
sification task, which was designed to address the
long-tail problem. In this task, only few (e.g., 1 or
5) support instances are given for each relation, as
shown by an example in Table 1.

The few-shot learning problem has been studied
extensively in computer vision (CV) field. Some
methods adopt meta-learning architectures (San-
toro et al., 2016; Ravi and Larochelle, 2016; Finn
et al., 2017; Munkhdalai and Yu, 2017), which
learn fast-learning abilities from previous expe-
riences (e.g., training set) and then rapidly gen-

https://github.com/ZhixiuYe/MLMAN
https://github.com/ZhixiuYe/MLMAN


2873

eralize to new concepts (e.g., test set). Some
other methods use metric learning based networks
(Koch et al., 2015; Vinyals et al., 2016; Snell
et al., 2017), which learn the distance distributions
among classes. A simple and effective metric-
based few-shot learning method is prototypical
network (Snell et al., 2017). In a prototype net-
work, query and support instances are encoded
into an embedding space independently. Then, a
prototype vector for each class candidate is de-
rived as the mean of its support instances in the
embedding space. Finally, classification is per-
formed by calculating the distances between the
embedding vector of the query and all class pro-
totypes. This prototype network method has also
been applied to few-shot relation classification re-
cently (Han et al., 2018).

This paper proposes a multi-level matching and
aggregation network (MLMAN) for few-shot re-
lation classification. Different from prototypi-
cal networks, which represent support sets with-
out dependency on query instances, our proposed
MLMAN model encodes each query instance and
each support set in an interactive way by consid-
ering their matching information at both local and
instance levels. At local level, the local context
representations of a query instance and a support
set are softly matched toward each other follow-
ing the sentence matching framework (Chen et al.,
2017). Then, the matched local representations
are aggregated into an embedding vector for each
query and each support instance using max and av-
erage pooling. At instance level, the matching de-
gree between the query instance and each of the
support instances is calculated via a multi-layer
perceptron (MLP). Taking the matching degrees as
weights, the instances in a support set are aggre-
gated to form the class prototype for final classifi-
cation. All these matching and aggregation layers
in the MLMAN model are estimated jointly us-
ing training data. Since the representations of the
support instances in each class are expected to be
close with each other, an auxiliary loss function
is further designed to measure the inconsistency
among all support representations in each class.

In summary, our contributions in this paper are
three-fold. First, a multi-level matching and ag-
gregation network is proposed to encode query in-
stances and class prototypes in an interactive fash-
ion. Second, an auxiliary loss function measur-
ing the consistency among support instances is de-

signed. Third, our method achieves a new state-of-
the-art performance on FewRel, a public few-shot
relation classification dataset.

2 Related Work

2.1 Relation Classification

Relation classification is to identify the semantic
relation between two entities in one sentence. In
recently years, neural networks have been widely
applied to deal with this task. Zeng et al. (2014)
employed position features and convolutional neu-
ral networks (CNNs) to capture the structure and
contextual information respectively. Then, a max
pooling operation was adopted to determine the
most useful features. Wang et al. (2016) pro-
posed multi-level attention CNNs, which captured
both entity-specific attention and relation-specific
pooling attention in order to better discern pat-
terns in heterogeneous contexts. Zhou et al. (2016)
proposed attention-based bidirectional long short-
term memory networks (AttBLSTMs) to capture
the most important semantic information in a sen-
tence. All of these methods require a large amount
of training data and can’t quickly adapt to a new
class that has never been seen.

2.2 Metric Based Few-Shot Learning

In few-shot learning paradigm, a classifier is re-
quired to generalize to new classes with only a
small number of training samples. The metric
based approach aims to learn a set of projec-
tion functions that take support and query sam-
ples from the target problem and classify them
in a feed forward manner. This approach has
lower complexity and is easier for implementa-
tion than meta-learner based approach (Ravi and
Larochelle, 2016; Finn et al., 2017; Santoro et al.,
2016; Munkhdalai and Yu, 2017).

Some metric based few-shot learning methods
have been developed for computer vision (CV)
tasks, and all these methods encoded each support
or query image to a vector independently for clas-
sification. Koch et al. (2015) proposed a method
for learning siamese neural networks, which em-
ployed an unique structure to encode both sup-
port and query samples respectively and one more
layer computing the induced distance metric be-
tween the pair. Vinyals et al. (2016) proposed to
learn a matching network augmented with atten-
tion and external memories. And also, an episode-
based training procedure was proposed, which was



2874

based on a principle that test and training condi-
tions must match and has been adopted by many
following studies. Snell et al. (2017) proposed
prototypical networks that learn a metric space in
which classification can be performed by comput-
ing distances to prototype representations of all
classes, and the prototype representation of each
class was the mean of all its support samples. Gar-
cia and Bruna (2017) defined a graph neural net-
work architecture to assimilate generic message-
passing inference algorithms, which generalized
above three models.

Regarding with few-shot relation classification,
Han et al. (2018) adopted prototypical networks to
build baseline models on the FewRel dataset. Gao
et al. (2019) proposed hybrid attention-based pro-
totypical networks to handle noisy training sam-
ples in few-shot learning. In this paper, we im-
prove the conventional prototypical networks for
few-shot relation classification by encoding the
query instance and class prototype interactively
through multi-level matching and aggregation.

2.3 Sentence Matching

Sentence matching is essential for many NLP
tasks, such as natural language inference (NLI)
(Bowman et al., 2015) and response selection
(Lowe et al., 2015). Some sentence match-
ing methods mainly rely on sentence encoding
(Mueller and Thyagarajan, 2016; Conneau et al.,
2017; Chen et al., 2018), which encode a pair sen-
tences independently and then transmit their em-
beddings into a classifier, such as a neural net-
work, to decide the relationship between them.
Some other methods are based on joint models
(Chen et al., 2017; Gong et al., 2017; Kim et al.,
2018), which use cross-features to represent the
local (i.e., word-level and phrase-level) alignments
for better performance. In this paper, we follow
the joint models to achieve the local matching be-
tween a query instance and the support set for a
class. The difference between our task and the
other sentence matching tasks mentioned above is
that, our goal is to match a sentence to a set of sen-
tences, instead of to another sentence (Bowman
et al., 2015) or to a sequence of sentences (Lowe
et al., 2015).

3 Task Definition

In few-shot relation classification, we are given
two datasets, Dmeta−train and Dmeta−test. Each

dataset consists of a set of samples (x, p, r), where
x is a sentence composed of T words and the t-
th word is wt, p = (p1, p2) indicate the posi-
tions of two entities, and r is the relation label
of the instance (x, p). These two datasets have
their own relation label spaces that are disjoint
with each other. Under few-shot configuration,
Dmeta−test is splited into two parts, Dtest−support
and Dtest−query. If Dtest−support contains K la-
beled samples for each of N relation classes, this
target few-shot problem is named N -way-K-shot.
Dtest−query contains test samples, each labeled
with one of the N classes. Assuming that we only
have Dtest−support and Dtest−query, we can train a
model using Dtest−support and evaluate its perfor-
mance on Dtest−query. But limited by the number
of support samples (i.e,.,N×K), it is hard to train
a good model from scratch.

Although Dmeta−train and Dmeta−test have dis-
joint relation label spaces, Dmeta−train can also
been utilized to help the few-shot relation clas-
sification on Dmeta−test. One approach is the
paradigm proposed by Vinyals et al. (2016), which
obey an important machine learning principle that
test and train conditions must match. That’s to
say, we also split Dmeta−train into two parts,
Dtrain−support and Dtrain−query, and mimic the
few-shot learning settings at training stage. In
each training iteration, N classes are randomly
selected from Dtrain−support, and K support in-
stances are randomly selected from each class.
In this way, we construct the train-support set
S = {sik; i = 1, ..., N, k = 1, ...,K}, where sik
is the k-th instance in class i. And also, we ran-
domly select R samples from the remaining sam-
ples of those N classes and construct the train-
query set Q = {(qj , lj); j = 1, ..., R}, where
lj ∈ {1, ..., N} is the label of instance qj .

Just like conventional prototypical networks,
we expect to minimize the following objective
function at training time

Jmatch = −
1

R

∑
(q,l)∈Q

P(l|S, q), (1)

and P(l|S, q) is defined as

P(l|S, q) =
exp(f({slk}Kk=1, q))∑N
i=1 exp(f({sik}Kk=1, q))

. (2)

The function f({sik}Kk=1, q) is to calculate the
matching degree between the query instance q and



2875

𝑠1  Encoder

𝑠𝐾  Encoder

... ... ...

𝐶  

𝑞  Encoder 𝑄  

Local Matching 
and Aggregation

𝑠 1  ...

𝑠 𝐾  

...

Class 
Matching

𝛾  𝑠2  
Encoder

𝑆1  

𝑆2  

𝑆𝐾  
𝑞  

Instance Matching 
and Aggregation 𝑠  

Figure 1: The framework of our proposed MLMAN model.

the set of support instances {sik}Kk=1. How to de-
sign this function is the focus of this paper.

4 Methodology

In this section, we will introduce our proposed
multi-level matching and aggregation network
(MLMAN) for modeling f({sik}Kk=1, q). For sim-
plicity, we will discard the superscript i of sik from
Section 4.1 to Section 4.4. The framework of our
proposed MLMAN model is shown in Fig. 1,
which has four main modules.

• Context Encoder. Given a sentence and the
positions of two entities within this sentence,
CNNs (Zeng et al., 2014) are adopted to de-
rive the local context representations of each
word in the sentence.

• Local Matching and Aggregation. Similar
to (Chen et al., 2017), given the local repre-
sentation of a query instance and the local
representations of K support instances, the
attention method is employed to collect local
matching information between them. Then,
the matched local representations are aggre-
gated to represent each instance as an embed-
ding vector.

• Instance Matching and Aggregation. The
matching information between a query in-
stance and each of the K support instances
are calculated using an MLP. Then, we take
the matching degrees as weights to sum the
representations of support instances in order
to get the class prototype.

• Class Matching. An MLP is built to cal-
culate the matching score between the repre-
sentations of the query instance and the class
prototype.

More details of these four modules will be in-
troduced in the following subsections.

4.1 Context Encoder
For a query or support instance, each word
wt in the sentence x is first mapped into a
dw-dimensional word embedding et (Pennington
et al., 2014). In order to describe the position in-
formation of the two entities in this instance, the
position features (PFs) proposed by Zeng et al.
(2014) are also adopted in our work. Here, PFs de-
scribe the relative distances between current word
and the two entities, and are further mapped into
two vectors p1t and p2t of dp dimensions. Fi-
nally, these three vectors are concatenated to get
the word representation wt = [et;p1t;p2t] of
dw+2dp dimensions, and the instance can be writ-
ten as W ∈ RT×(dw+2dp).

The most popular models for local context en-
coding are recurrent neural networks (RNNs) with
long short-term memories (LSTMs) (Hochreiter
and Schmidhuber, 1997) and convolutional neural
networks (CNNs) (Kim, 2014). In this paper, we
employ CNNs to build the context encoder. For an
input instance W ∈ RT×(dw+2dp), we input it into
a CNN with dc filters. The output from the CNN
is a matrix with T × dc dimensions. In this way,
the context representations of the query instance
Q ∈ RTq×dc and the context representations of
support instances {Sk ∈ RTk×dc ; k = 1, ...,K}
are obtained, where Tq and Tk are the sentence
lengths of the query sentence and the k-th support
sentence respectively.

4.2 Local Matching and Aggregation
In order to get the matching information between
Q and {Sk; k = 1, ...,K}, we first concatenate the
K support instance representations into one matrix
as follow

C = concat({Sk}Kk=1), (3)

where C ∈ RTs×dc with Ts =
∑K

k=1 Tk. Then,
we collect the matching information between Q



2876

and C and calculate their matched representations
Q̃ and S̃ as follows

αmn = q
>
mcn, (4)

q̃m =

Ts∑
n=1

exp(αmn)∑Ts
n′=1 exp(αmn′)

cn, (5)

c̃n =

Tq∑
m=1

exp(αmn)∑Tq
m′=1 exp(αm′n)

qm, (6)

where m ∈ {1, ..., Tq} in Eq. (5), n ∈ {1, ..., Ts}
in Eq. (6), qm and q̃m are the m-th rows of Q and
Q̃ respectively, and cn and c̃n are the n-th rows of
C and C̃ respectively.

Next, the original representations and the
matched representations are fused utilizing a
ReLU layer as follows,

Q̄ = ReLU([Q; Q̃; |Q− Q̃|;Q� Q̃]W1), (7)

C̄ = ReLU([C; C̃; |C− C̃|;C� C̃]W1), (8)

where � is the element-wise product and W1 ∈
R4dc×dh is the weight matrix at this layer for re-
ducing dimensionality. C̄ is further split into K
representations {S̄k}Kk=1 corresponding to the K
support instances where S̄k ∈ RTk×dh . All S̄k
and Q̄ are fed into a single-layer Bi-directional
LSTM (BLSTM) with dh hidden units along each
direction to obtain the final local matching results
Ŝk ∈ RTk×2dh and Q̂ ∈ RTq×2dh .

Local aggregation aims to convert the results of
local matching into a single vector for each query
and each support instance. In this paper, we em-
ploy a max pooling together with an average pool-
ing, and concatenate their results into one vector̂sk
or q̂. The calculations are as follows,

ŝk =[max(Ŝk); ave(Ŝk)],∀k ∈ {1, ...,K}, (9)

q̂ =[max(Q̂); ave(Q̂)], (10)

where {ŝk, q̂} ∈ R4dh .

4.3 Instance Matching and Aggregation
Similar to conventional prototypical networks
(Snell et al., 2017), our proposed method calcu-
lates class prototype ŝ via the representations of all
support instances in this class, i.e., {ŝk}Kk=1. How-
ever, instead of using a naive mean operation, we
aggregate instance-level representations via atten-
tion over {ŝk}Kk=1, where each weight is derived

from the instance matching score between ŝk and
q̂. The matching function is as follow,

βk = v
>(ReLU(W2 [̂sk; q̂])), (11)

where W2 ∈ Rdh×8dh and v ∈ Rdh . βk describes
the instance-level matching degree between the
query instance q and the support instance sk.
Then, all {ŝk}Kk=1 are aggregated into one vector ŝ
as

ŝ =
K∑
k=1

exp(βk)∑K
k′=1 exp(β

′
k)
ŝk, (12)

and ŝ is the class prototype.

4.4 Class Matching
After the class prototype ŝ and the embedding vec-
tor of the query instance q̂ have been determined,
the class-level matching function f({sk}Kk=1, q) in
Eq. (2) is defined as

f({sk}Kk=1, q) = v>(ReLU(W2 [̂s; q̂])). (13)

Eq. (11) and (13) have the same form. In our
experiments, sharing the weights W2 and v in
these two equations, i.e., employing the exactly
same function for both instance-level and class-
level matching in each training iteration, lead to
better performance.

4.5 Joint Training with Inconsistency
Measurement

If the representations of all support instances in a
class are far away from each other, it could become
difficult for the derived class prototype to cap-
ture the common characteristics of all support in-
stances. Therefore, a function which measures the
inconsistency among the set of support instances
is designed. In order to avoid the high complexity
of directly comparing every two support instances
in a class, we calculate the inconsistency measure-
ment as the average Euclidean distance between
the support instances and the class prototype as

Jincon =
1

NK

N∑
i=1

K∑
k=1

||̂sik − ŝi||22, (14)

where i is the class index and || · ||2 calculates the
2-norm of a vector.

By combining Eqs. (1) and (14), the final ob-
jective function for training the whole model is de-
fined as

J = Jmatch + λJincon, (15)

where λ is a hyper-parameter and was set as 1 in
our experiments without any tuning.



2877

Model 5 Way 1 Shot 5 Way 5 Shot 10 Way 1 Shot 10 Way 5 Shot

Meta Network (Han et al., 2018) 64.46± 0.54 80.57± 0.48 53.96± 0.56 69.23± 0.52
GNN (Han et al., 2018) 66.23± 0.75 81.28± 0.62 46.27± 0.80 64.02± 0.77
SNAIL (Han et al., 2018) 67.29± 0.26 79.40± 0.22 53.28± 0.27 68.33± 0.25
Prototypical Network (Han et al., 2018) 69.20± 0.20 84.79± 0.16 56.44± 0.22 75.55± 0.19
Proto-HATT (Gao et al., 2019) - - 90.12± 0.04 - - 83.05± 0.05

MLMAN 82.98± 0.20 92.66± 0.09 75.59± 0.27 87.29± 0.15

Table 2: Accuracies (%) of different models on FewRel test set.

5 Experiments

5.1 Dataset and Evaluation Metrics

The few-shot relation classification dataset
FewRel2 was adopted in our experiments. This
dataset was first generated by distant supervision
and then filtered by crowdsourcing to remove
noisy annotations. The final FewRel dataset
consists of 100 relations, each has 700 instances.
The average number of tokens in each sentence
is 24.99, and there are 124,577 unique tokens in
total. The 100 relations are split into 64, 16 and
20 for training, validation and test respectively.

Our experiments investigated four few-shot
learning configurations, 5 way 1 shot, 5 way 5
shot, 10 way 1 shot, and 10 way 5 shot, which
were the same as Han et al. (2018). According to
the official evaluation scripts3, all results given by
our experiments were the mean and standard de-
viation values of 10 training repetitions, and were
tested using 20,000 independent samples.

5.2 Training Details and Hyperparameters

All of the hyperparameters used in our experi-
ments are listed in Table 3. The 50-dimensional
Glove word embeddings released by Pennington
et al. (2014) 4 were adopted in the context encoder
and were fixed during training. For the unknown
words, we just replaced them with an unique spe-
cial token <UNK> and fixed its embedding as
a zero vector. Previous study (Munkhdalai and
Yu, 2017) found that the models trained on harder
tasks may achieve better performances than using
the same configurations at both training and test
stages. Therefore, we set N = 20 to construct
the train-support sets for 5-way and 10-way tasks.
In our experiments, grid searches among dc ∈
{100, 150, 200, 250}, dh ∈ {100, 150, 200, 250}

2https://thunlp.github.io/fewrel.html.
3https://thunlp.github.io/fewrel.html.
4https://nlp.stanford.edu/projects/

glove/.

Component Parameter Value
word embedding dimension 50

position feature max relative distance ±40dimension 5

CNN window size 3filter number dc 200
dropout dropout rate 0.2

unidirectional LSTM hidden size dh 100

optimization

strategy SGD
learning rate 0.1

size of query set R 5
Ntrain 20
λ 1

Table 3: Hyper-parameters of the models built in our
experiments.

and R ∈ {5, 10, 15} were conducted to deter-
mine their optimal values. For optimization, we
employed mini-batch stochastic gradient descent
(SGD) with the initial learning rate of 0.1. The
learning rate was decayed to one tenth every
20,000 steps. And also, dropout layers (Hinton
et al., 2012) were inserted before CNN and LSTM
layers and the drop rate was set as 0.2.

5.3 Comparison with Previous Work

Table 2 shows the results of different models
tested on FewRel test set. The results of the first
four models, Meta Network (Munkhdalai and Yu,
2017), GNN (Garcia and Bruna, 2017), SNAIL
(Mishra et al., 2018), Prorotypical Network (Snell
et al., 2017), were reported by Han et al. (2018).
These models were initially proposed for image
classification. Han et al. (2018) just replaced their
image encoding module with an instance encoding
module and kept other modules unchanged. Proto-
HATT (Gao et al., 2019) added hybrid attention
mechanism to prototypical networks, mainly fo-
cusing on improving the performance on few-shot
relation classification with N > 1. From Table
2, we can see that our proposed MLMAN model
outperforms all other models by a large margin,
which shows the effectiveness of considering the

https://thunlp.github.io/fewrel.html
https://thunlp.github.io/fewrel.html
https://nlp.stanford.edu/projects/glove/
https://nlp.stanford.edu/projects/glove/


2878

Model No. 5 Way 1 Shot 5 Way 5 Shot 10 Way 1 Shot 10 Way 5 Shot

MLMAN 1 79.01± 0.20 88.86± 0.20 67.37± 0.19 80.07± 0.18

-Jincon 2 79.01± 0.20 88.33± 0.15 67.37± 0.19 79.38± 0.22
IM(shared→ untied) 3 79.01± 0.20 86.77± 0.19 67.37± 0.19 77.66± 0.09
IA(att. → max.) 4 79.01± 0.20 87.84± 0.13 67.37± 0.19 78.86± 0.15
IA(att. → ave.) 5 79.01± 0.20 87.48± 0.17 67.37± 0.19 78.58± 0.23

-Jincon 6 79.01± 0.20 86.23± 0.22 67.37± 0.19 77.36± 0.26
LM(-concatenation) 7 79.01± 0.20 85.48± 0.28 67.37± 0.19 74.56± 0.36
CM(MLP→ ED) 8 76.52± 0.23 81.91± 0.13 62.89± 0.13 69.41± 0.15
-LM 9 74.13± 0.16 82.73± 0.16 59.71± 0.22 70.23± 0.23

CM(MLP→ ED) 10 75.42± 0.23 82.36± 0.07 62.54± 0.26 70.45± 0.11

Table 4: Accuracies (%) of different models on FewRel development set. Here, IM stands for instance matching,
IA stands for instance aggregation, LM stands for the local matching, CM stands for the class matching, MLP
stands for multi-layer perceptrons and ED stands for Euclidean distance.

interactions between query instance and support
set at multiple levels.

5.4 Ablation Study

In order to evaluate the contributions of individ-
ual model components, ablation studies were con-
ducted. Table 4 shows the performance of our
model and its ablations on the development set of
FewRel. Considering that the first 6 ablations only
affected the few-shot learning tasks with N > 1,
model 2 to model 7 achieved exactly the same per-
formance as the complete model (i.e., model 1) un-
der 5 way 1 shot and 10 way 1 shot configurations.

5.4.1 Instance Matching and Aggregation
First, the attention-based instance aggregation in-
troduced in Section 4.3 was replaced with a max
pooling (model 4) or an average pooling (model
5). We can see that the model with instance-level
attentive aggregation (model 1) outperformed the
ones using a max pooling (model 4) or an aver-
age pooling (model 5) on 5-shot tasks. Their dif-
ference were significantly at 1% significance level
in t-test. The advantage of attentive pooling is
that the weights of integrating all support instances
can be determined dynamically according to the
query. For example, when conducting instance
matching and aggregation between the query in-
stance and the support set in Table 1, the weights
of the 5 instances in class A were 0.03, 0.46, 0.25,
0.08 and 0.18 respectively. Instance #2 achieved
the highest weight because it had the best similar-
ity with the query instance and was considered as
the most helpful one when matching the query in-
stance with class A.

Then, the effectiveness of sharing the weight
parameters in Eqs. (11) and (13) was evaluated

by untying them (model 3). The performance of
model 3 was much worse than the complete model
(model 1) as shown in Table 4, which demon-
strates the need of sharing the weights for calcu-
lating matching scores at both instance and class
levels.

5.4.2 Inconsistency Measurement
As introduced in Section 4.5, Jincon is designed to
measure the inconsistency among the representa-
tions of all support instances in a class. After re-
moving Jincon, model 2 was optimized only using
the objective function Jmatch. We can see that it
performed much worse than the complete model.
Furthermore, we calculated the mean of the Eu-
clidean distances between every support instance
pair (ŝik, ŝ

i
k′) in the same class using model 1 and

model 2 respectively. For each support set, the cal-
culation can be written as

D =
2

NK(K − 1)

N∑
i=1

K∑
k=1

K∑
k′=k+1

||̂sik − ŝik′ ||22.

(16)
We sampled 20,000 support sets under the 5-way
5-shot configuration and calculated the mean of
them. The results were 0.0199 and 0.0346 for
model 1 and model 2 respectively, which means
that Jincon was effective at forcing the representa-
tions of the support instances in the same class to
be close with each other.

Jincon was further removed from model 5 and
model 6 was obtained. It can be found that the ac-
curacy degradation from model 5 to model 6 was
larger than the one from model 1 to model 2. This
implies that the Jincon objective function also ben-
efited from the attentive aggregation over support
instances.



2879

5.4.3 Local Matching
First, the concatenation operation in local match-
ing was removed from model 6 in this ablation
study. That’s to say, instead of concatenating the
representations of all support instances {Sk}Kk=1
into one single matrix as Eq. (3), local match-
ing was conducted between the query instance and
each support instance separately to get their vector
representations {(ŝk, q̂k); k = 1, ...,K} (model
7). It should be noticed that this led to K differ-
ent representations of a query instance according
to each support class. Then, the mean over k for
ŝk and q̂k were calculated to get the representa-
tions of the support set ŝ and the query instance
q̂. Comparing model 6 and model 7, we can see
that the concatenation operation plays an impor-
tant role in our model. One possible reason is that
the concatenation operation can help local match-
ing to restrain the support instances with low sim-
ilarity to the query.

Second, the whole local matching module to-
gether with the concatenation and attentive ag-
gregation operation were removed from model 6,
which led to model 9. Model 9 is similar to the one
proposed by Snell et al. (2017) that encoded the
support and query instances independently. The
difference was that model 9 was equipped with
more components, including an LSTM layer, two
pooling operations, and a learnable class matching
function. Comparing the performance of model 6
and model 9 in Table 4, we can see that the local
matching operation significantly improves the per-
formance in few-shot relation classification. Fig.
2 shows the attention weight matrix calculated be-
tween the query instance and the support instance
#2 of class A in Table 1. From this figure, we
can see that the attention-based local matching is
able to capture some matching relations of local
contexts, such as the head entities Eva Funck and
Cindy Robbins, the tail entities Gustav and Kim-
berly Beck, the key phrases son and daughter, the
same keyword “married”, and so on.

5.4.4 Class Matching
In this experiment, we compared two class match-
ing functions, (1) Euclidean distance (ED) (Snell
et al., 2017) and (2) a learnable MLP function as
shown by Eq. (13). In order to ignore the influence
of the instance-level attentive aggregation, these
two matching functions were compared based on
model 6 and model 9. After converting the MLP
function in model 6 and model 9 to Euclidean dis-

He m
ar

rie
d

th
e

Am
er

ica
n

ac
tre

ss
Ci

nd
y

Ro
bb

in
s

an
d

wa
s

st
ep

fa
th

er
to he

r
da

ug
ht

er
, Ki

m
be

rly
Be

ck
.

support instance

He
was

married
to

Eva
Funck

and
they
have

a
son

Gustav
.

qu
er

y 
in

st
an

ce

Figure 2: The attention weight matrix calculated be-
tween the query instance and the support instance #2 of
class A in Table 1. The darker units have larger value.
The summation of one column in the matrix is one.

tance, model 8 and model 10 were obtained. Com-
paring the performance of these models in Table
4, we have two findings. (1) When local matching
was adopted, the learnable MLP for class match-
ing (model 6) outperformed the ED metric (model
8) by a large margin. (2) After removing local
matching, the learnable MLP for class matching
(model 9) performed not as good as the ED met-
ric (model 10). One possible reason is that the
local matching process enhances the interaction
between a query instance and a support set when
calculating ŝ and q̂. Thus, simple Euclidean dis-
tance between them may not be able to describe
the complex correlation and dependency between
them. On the other hand, MLP mapping is more
powerful than calculating Euclidean distance, and
can be more appropriate for class matching when
local matching is also adopted.

6 Conclusions

In this paper, a neural network with multi-level
matching and aggregation has been proposed for
few-shot relation classification. First, the query
and support instances are encoded interactively via
local matching and aggregation. Then, the support
instances in a class are further aggregated to form
the class prototype and the weights are calculated
by attention-based instance matching. Finally, a
learnable MLP matching function is employed to
calculate the class matching score between the
query instance and each candidate class. Further-
more, an additional objective function is designed
to improve the consistency among the vector rep-



2880

resentations of all support instances in a class. Ex-
periments have demonstrated the effectiveness of
our proposed model, which achieves state-of-the-
art performance on the FewRel dataset. Studying
few-shot relation classification with data gener-
ated by distant supervision and extending our ML-
MAN model to zero-shot learning will be the tasks
of our future work.

Acknowledgments

We thank the anonymous reviewers for their valu-
able comments. This work was partially funded by
the National Nature Science Foundation of China
(Grant No. U1636201, 61871358).

References

Steven Bethard and James H. Martin. 2007. Cu-
tmp: Temporal relation classification using syntac-
tic and semantic features. In Proceedings of the
Fourth International Workshop on Semantic Evalu-
ations (SemEval-2007), pages 129–132. Association
for Computational Linguistics.

Samuel R. Bowman, Gabor Angeli, Christopher Potts,
and Christopher D. Manning. 2015. A large anno-
tated corpus for learning natural language inference.
In Proceedings of the 2015 Conference on Empiri-
cal Methods in Natural Language Processing, pages
632–642. Association for Computational Linguis-
tics.

Qian Chen, Zhen-Hua Ling, and Xiaodan Zhu. 2018.
Enhancing sentence embedding with generalized
pooling. In Proceedings of the 27th International
Conference on Computational Linguistics, pages
1815–1826. Association for Computational Linguis-
tics.

Qian Chen, Xiaodan Zhu, Zhen-Hua Ling, Si Wei, Hui
Jiang, and Diana Inkpen. 2017. Enhanced lstm for
natural language inference. In Proceedings of the
55th Annual Meeting of the Association for Compu-
tational Linguistics (Volume 1: Long Papers), pages
1657–1668. Association for Computational Linguis-
tics.

Alexis Conneau, Douwe Kiela, Holger Schwenk, Loı̈c
Barrault, and Antoine Bordes. 2017. Supervised
learning of universal sentence representations from
natural language inference data. In Proceedings of
the 2017 Conference on Empirical Methods in Nat-
ural Language Processing, pages 670–680. Associ-
ation for Computational Linguistics.

Chelsea Finn, Pieter Abbeel, and Sergey Levine. 2017.
Model-agnostic meta-learning for fast adaptation of
deep networks. arXiv preprint arXiv:1703.03400.

Tianyu Gao, Xu Han, Zhiyuan Liu, and Maosong Sun.
2019. Hybrid attention-based prototypical networks
for noisy few-shot relation classification.

Victor Garcia and Joan Bruna. 2017. Few-shot learn-
ing with graph neural networks. arXiv preprint
arXiv:1711.04043.

Yichen Gong, Heng Luo, and Jian Zhang. 2017. Natu-
ral language inference over interaction space. arXiv
preprint arXiv:1709.04348.

Xu Han, Hao Zhu, Pengfei Yu, Ziyun Wang, Yuan Yao,
Zhiyuan Liu, and Maosong Sun. 2018. Fewrel: A
large-scale supervised few-shot relation classifica-
tion dataset with state-of-the-art evaluation. In Pro-
ceedings of the 2018 Conference on Empirical Meth-
ods in Natural Language Processing, pages 4803–
4809. Association for Computational Linguistics.

Geoffrey E Hinton, Nitish Srivastava, Alex Krizhevsky,
Ilya Sutskever, and Ruslan R Salakhutdinov. 2012.
Improving neural networks by preventing co-
adaptation of feature detectors. arXiv preprint
arXiv:1207.0580.

S Hochreiter and J Schmidhuber. 1997. Long short-
term memory. Neural Computation, 9(8):1735–
1780.

Seonhoon Kim, Jin-Hyuk Hong, Inho Kang, and No-
jun Kwak. 2018. Semantic sentence matching with
densely-connected recurrent and co-attentive infor-
mation. arXiv preprint arXiv:1805.11360.

Yoon Kim. 2014. Convolutional neural networks for
sentence classification. Eprint Arxiv.

Gregory Koch, Richard Zemel, and Ruslan Salakhut-
dinov. 2015. Siamese neural networks for one-shot
image recognition. In ICML Deep Learning Work-
shop, volume 2.

Ryan Lowe, Nissan Pow, Iulian Serban, and Joelle
Pineau. 2015. The ubuntu dialogue corpus: A large
dataset for research in unstructured multi-turn dia-
logue systems. In Proceedings of the 16th Annual
Meeting of the Special Interest Group on Discourse
and Dialogue, pages 285–294. Association for Com-
putational Linguistics.

Mike Mintz, Steven Bills, Rion Snow, and Dan Juraf-
sky. 2009. Distant supervision for relation extrac-
tion without labeled data. In Proceedings of the
Joint Conference of the 47th Annual Meeting of the
ACL and the 4th International Joint Conference on
Natural Language Processing of the AFNLP: Vol-
ume 2-Volume 2, pages 1003–1011. Association for
Computational Linguistics.

Nikhil Mishra, Mostafa Rohaninejad, Xi Chen, and
Pieter Abbeel. 2018. A simple neural attentive meta-
learner.

Jonas Mueller and Aditya Thyagarajan. 2016. Siamese
recurrent architectures for learning sentence similar-
ity. In AAAI, volume 16, pages 2786–2792.

http://aclweb.org/anthology/S07-1025
http://aclweb.org/anthology/S07-1025
http://aclweb.org/anthology/S07-1025
https://doi.org/10.18653/v1/D15-1075
https://doi.org/10.18653/v1/D15-1075
http://aclweb.org/anthology/C18-1154
http://aclweb.org/anthology/C18-1154
https://doi.org/10.18653/v1/P17-1152
https://doi.org/10.18653/v1/P17-1152
https://doi.org/10.18653/v1/D17-1070
https://doi.org/10.18653/v1/D17-1070
https://doi.org/10.18653/v1/D17-1070
http://aclweb.org/anthology/D18-1514
http://aclweb.org/anthology/D18-1514
http://aclweb.org/anthology/D18-1514
https://doi.org/10.18653/v1/W15-4640
https://doi.org/10.18653/v1/W15-4640
https://doi.org/10.18653/v1/W15-4640


2881

Tsendsuren Munkhdalai and Hong Yu. 2017. Meta net-
works. arXiv preprint arXiv:1703.00837.

Jeffrey Pennington, Richard Socher, and Christopher
Manning. 2014. Glove: Global vectors for word
representation. In Proceedings of the 2014 Con-
ference on Empirical Methods in Natural Language
Processing (EMNLP), pages 1532–1543. Associa-
tion for Computational Linguistics.

Sachin Ravi and Hugo Larochelle. 2016. Optimization
as a model for few-shot learning.

Adam Santoro, Sergey Bartunov, Matthew Botvinick,
Daan Wierstra, and Timothy Lillicrap. 2016. Meta-
learning with memory-augmented neural networks.
In International conference on machine learning,
pages 1842–1850.

Jake Snell, Kevin Swersky, and Richard Zemel. 2017.
Prototypical networks for few-shot learning. In Ad-
vances in Neural Information Processing Systems,
pages 4077–4087.

Oriol Vinyals, Charles Blundell, Timothy Lillicrap,
Daan Wierstra, et al. 2016. Matching networks for
one shot learning. In Advances in neural informa-
tion processing systems, pages 3630–3638.

Linlin Wang, Zhu Cao, Gerard de Melo, and Zhiyuan
Liu. 2016. Relation classification via multi-level
attention cnns. In Proceedings of the 54th An-
nual Meeting of the Association for Computational
Linguistics (Volume 1: Long Papers), pages 1298–
1307. Association for Computational Linguistics.

Wenhan Xiong, Mo Yu, Shiyu Chang, Xiaoxiao Guo,
and William Yang Wang. 2018. One-shot relational
learning for knowledge graphs. In Proceedings of
the 2018 Conference on Empirical Methods in Nat-
ural Language Processing, pages 1980–1990. Asso-
ciation for Computational Linguistics.

Dmitry Zelenko, Chinatsu Aone, and Anthony
Richardella. 2002. Kernel methods for relation ex-
traction. In Proceedings of the 2002 Conference on
Empirical Methods in Natural Language Processing
(EMNLP 2002).

Daojian Zeng, Kang Liu, Siwei Lai, Guangyou Zhou,
and Jun Zhao. 2014. Relation classification via con-
volutional deep neural network. In Proceedings of
COLING 2014, the 25th International Conference
on Computational Linguistics: Technical Papers,
pages 2335–2344. Dublin City University and As-
sociation for Computational Linguistics.

Peng Zhou, Wei Shi, Jun Tian, Zhenyu Qi, Bingchen
Li, Hongwei Hao, and Bo Xu. 2016. Attention-
based bidirectional long short-term memory net-
works for relation classification. In Proceedings of
the 54th Annual Meeting of the Association for Com-
putational Linguistics (Volume 2: Short Papers),
pages 207–212. Association for Computational Lin-
guistics.

https://doi.org/10.3115/v1/D14-1162
https://doi.org/10.3115/v1/D14-1162
https://doi.org/10.18653/v1/P16-1123
https://doi.org/10.18653/v1/P16-1123
http://aclweb.org/anthology/D18-1223
http://aclweb.org/anthology/D18-1223
http://aclweb.org/anthology/W02-1010
http://aclweb.org/anthology/W02-1010
http://aclweb.org/anthology/C14-1220
http://aclweb.org/anthology/C14-1220
https://doi.org/10.18653/v1/P16-2034
https://doi.org/10.18653/v1/P16-2034
https://doi.org/10.18653/v1/P16-2034

