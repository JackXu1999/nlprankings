



















































Conversational Memory Network for Emotion Recognition in Dyadic Dialogue Videos


Proceedings of NAACL-HLT 2018, pages 2122–2132
New Orleans, Louisiana, June 1 - 6, 2018. c©2018 Association for Computational Linguistics

Conversational Memory Network
for Emotion Recognition in Dyadic Dialogue Videos

Devamanyu Hazarika
School of Computing,

National University of Singapore
hazarika@comp.nus.edu.sg

Soujanya Poria
Artificial Intelligence Initiative,

A*STAR, Singapore
sporia@ihpc.a-star.edu.sg

Amir Zadeh
Language Technologies
Institute, CMU, USA
abagherz@cs.cmu.edu

Erik Cambria
School of Computer Science and

Engineering, NTU, Singapore
cambria@ntu.edu.sg

Louis-Philippe Morency
Language Technologies
Institute, CMU, USA
morency@cs.cmu.edu

Roger Zimmermann
School of Computing,

National University of Singapore
rogerz@comp.nus.edu.sg

Abstract
Emotion recognition in conversations is cru-
cial for the development of empathetic ma-
chines. Present methods mostly ignore the role
of inter-speaker dependency relations while
classifying emotions in conversations. In this
paper, we address recognizing utterance-level
emotions in dyadic conversational videos. We
propose a deep neural framework, termed con-
versational memory network, which leverages
contextual information from the conversation
history. The framework takes a multimodal
approach comprising audio, visual and textual
features with gated recurrent units to model
past utterances of each speaker into memo-
ries. Such memories are then merged using
attention-based hops to capture inter-speaker
dependencies. Experiments show an accuracy
improvement of 3−4% over the state of the art.

1 Introduction

Development of machines with emotional intelli-
gence has been a long-standing goal of AI. With
the increasing infusion of interactive systems in
our lives, the need for empathetic machines with
emotional understanding is paramount. Previous
research in affective computing has looked at di-
alogues as an essential basis to learn emotional
dynamics (Sidnell and Stivers, 2012; Poria et al.,
2017a; Zhou et al., 2017).

Since the advent of Web 2.0, dialogue videos
have proliferated across the internet through plat-
forms like movies, webinars, and video chats. Emo-
tion detection from such resources can benefit
numerous fields like counseling (De Choudhury
et al., 2013), public opinion mining (Cambria et al.,
2017), financial forecasting (Xing et al., 2018), and
intelligent systems such as smart homes and chat-
bots (Young et al., 2018).

In this paper, we analyze emotion detection in
videos of dyadic conversations. A dyadic conver-
sation is a form of a dialogue between two enti-
ties. We propose a conversational memory net-
work (CMN), which uses a multimodal approach
for emotion detection in utterances (a unit of speech
bound by breathes or pauses) of such conversa-
tional videos.

Emotional dynamics in a conversation is known
to be driven by two prime factors: self and inter-
speaker emotional influence (Morris and Keltner,
2000; Liu and Maitlis, 2014). Self-influence re-
lates to the concept of emotional inertia, i.e., the
degree to which a person’s feelings carry over from
one moment to another (Koval and Kuppens, 2012).
Inter-speaker emotional influence is another trait
where the other person acts as an influencer in
the speaker’s emotional state. Conversely, speak-
ers also tend to mirror emotions of their counter-
parts (Navarretta et al., 2016). Figure 1 provides
an example from the dataset showing the presence
of these two traits in a dialogue.

Existing works in the literature do not capitalize
on these two factors. Context-free systems infer
emotions based only on the current utterance in
the conversation (Bertero et al., 2016). Whereas,
state-of-the-art context-based networks like Poria
et al., 2017b, use long short-term memory (LSTM)
networks to model speaker-based context that suf-
fers from incapability of long-range summarization
and unweighted influence from context, leading to
model bias.

Our proposed CMN incorporates these factors
by using emotional context information present
in the conversation history. It improves speaker-
based emotion modeling by using memory net-
works which are efficient in capturing long-term

2122



dependencies and summarizing task-specific de-
tails using attention models (Weston et al., 2014;
Graves et al., 2014; Young et al., 2017).

Specifically, the memory cells of CMN are con-
tinuous vectors that store the context information
found in the utterance histories. CMN also mod-
els interplay of these memories to capture inter-
speaker dependencies.

CMN first extracts multimodal features (audio,
visual, and text) for all utterances in a video. In
order to detect the emotion of a particular utterance,
say ui, it gathers its histories by collecting previous
utterances within a context window. Separate histo-
ries are created for both speakers. These histories
are then modeled into memory cells using gated
recurrent units (GRUs).

After that, CMN reads both the speaker’s memo-
ries and employs attention mechanism on them, in
order to find the most useful historical utterances
to classify ui. The memories are then merged with
ui using an addition operation weighted by the at-
tention scores. This is done to model inter-speaker
influences and dynamics. The whole cycle is re-
peated for multiple hops and finally, this merged
representation of utterance ui is used to classify its
emotion category.
The contributions of this paper can be summarized
as follows:

1. We propose an architecture, termed CMN, for
emotion detection in a dyadic conversation that
considers utterance histories of both the speaker
to model emotional dynamics. The architecture
is extensible to multi-speaker conversations in
formats such as textual dialogues or conversa-
tional videos.

2. When applied to videos, we adopt a multimodal
approach to extract diverse features from utter-
ances. It also makes our model robust to missing
information.

3. CMN provides a significant increase in accu-
racy of 3 − 4% over previous state-of-the-art
networks. One variant called CMNself which
does not consider the inter-speaker relation in
emotion detection also outperforms the state of
the art by a significant margin.

The remainder of the paper is organized as fol-
lows: Section 2 provides a brief literature review;
Section 3 formalizes the problem statement; Sec-
tion 4 describes the proposed method in detail; ex-

So you're leaving tomorrow. [sad]
Yeah, they just called. [sad]

I don't know what to say. I don't want to go but I 
don't have a choice. [sad]

I am afraid when you leave you won't come back. [sad]

I have to do this. Do you think I want to miss seeing her (their 
daughter) grow? [sad]

You don’t have to do this. Its not gonna work out. We're 
not a complete family without you being here. [sad]

Well I ll come back, what are you not willing to wait for me? [ang]

Thanks that helps. I feel much better now. [ang]

We are not a complete family here, you don’t understand. [ang]

Person BPerson A

Ti
me

Figure 1: An abridged dialogue from the dataset. Per-
son A (wife) is leaving B (husband) for a work assign-
ment. Initially both A and B are emotionally driven by
their own emotional inertia. In the end, emotional in-
fluence can be seen when B, despite being sad, reacts
angrily to A’s angry statement.

perimental results are covered in Section 5; finally,
Section 6 provides concluding remarks.

2 Related Works

Over the years, emotion recognition as an area of
research has seen contributions from researchers
across varied fields like signal processing, machine
learning, cognitive and social psychology, natu-
ral language processing, etc. (Picard, 2010). Ek-
man, 1993, provided initial findings that related
facial expressions as universal indicators of emo-
tions. Datcu and Rothkrantz, 2008, 2011, showed
the importance of acoustic cues in affect modeling.

A large section of researchers approaches emo-
tion recognition from a multimodal learning per-
spective. Hence, many works used visual and audio
features together for detecting affect (Busso et al.,
2004; Castellano et al., 2008; Ranganathan et al.,
2016). An in-depth review of the literature in these
systems is provided by D’mello and Kory, 2015.
Our work, which performs context-sensitive recog-
nition (Wöllmer et al., 2010) uses three modalities:
audio, visual and text. Recently, this combination
of modalities has provided the best performance
in affect recognition systems (Poria et al., 2017b;
Wang et al., 2017; Tzirakis et al., 2017), thus moti-
vating the use of a multimodal approach.

Previous works have focused on conversations
as a resourceful event for emotion analysis. Ru-
usuvuori, 2013, provides an in-depth analysis on
how emotions affect social interactions and con-
versations. In fact, significant works have at-
tributed emotional dynamics as an interactive phe-

2123



nomenon, rather than being within-person and
one-directional (Richards et al., 2003; Hareli and
Rafaeli, 2008). Such emotional dynamics are mod-
eled by observing transition properties. Yang et al.,
2011, study patterns for emotion transitions and
show the evidence of emotional inertia. Xiaolan
et al., 2013, use finite state machines to model tran-
sitions using stimuli and personality characteristics.
Our work also tries to model emotional transitions
using multimodal features. Unlike these works,
however, we use memory networks to achieve the
same.

The use of memory networks have been instru-
mental in the progress of multiple research prob-
lems, e.g., question-answering (Weston et al., 2014;
Sukhbaatar et al., 2015; Kumar et al., 2016), ma-
chine translation (Bahdanau et al., 2014), speech
recognition (Graves et al., 2014), and common-
sense reasoning (Cambria et al., 2018). The re-
peated read and write to their memory cells is often
coupled with attention modules, thus allowing it to
filter only relevant memories.

Our model is loosely inspired from Sukhbaatar
et al., 2015. Unlike their model, which directly en-
codes sentences into memories, we perform tempo-
ral sequence processing on our utterance histories
using GRUs. We also extend their architecture to
handle two speakers while keeping the possibility
to add more. Finally, our model is different in the
fact that we use multimodal features for input and
processing.

3 Task Definition

Our goal is to infer the emotion of utterances
present in a dyadic conversation. Let us define
a dyadic conversation to be an asynchronous ex-
change of utterances between two persons Pa and
Pb. Both the speakers speak a sequence of ut-
terances Ua and Ub, respectively. Here, Uλ =(s1λ, s2λ, ..., slλλ ) is ordered temporally, where siλ
is the ith utterance by Pλ and lλ is the total num-
ber of utterances spoken by person Pλ, λ ∈ {a, b}.
Overall, the utterances by both speakers can be
linearly ordered based on temporal occurrence as(u1, u2, ...ula+lb) , where, uj ∈ Ua or Ub.

Our model takes as input an utterance ui whose
emotion category (Section 5.1) needs to be classi-
fied. To get its history, preceding K utterances of
each person are separately collected as hista and
histb. Here, K serves as the length of the context

window for history of ui. Thus, for λ ∈ {a, b}:
histλ = {uj ∣ uj ∈ Uλ, j < i} , ∣ histλ ∣≤K (1)

histλ is also ordered temporally. At the beginning
of the conversation, histories would have lesser
than K utterances, i.e., ∣ histλ ∣<K.

In the remaining sections, for brevity, we ex-
plain the processes using a subscript λ which can
instantiate to either a or b, i.e., λ ∈ {a, b}.
4 Approach

We start by detailing the multimodal feature ex-
traction scheme for all utterances followed by the
mechanism to model emotional context using mem-
ory networks.

4.1 Multimodal Feature Extraction

The first phase of CMN is to extract multimodal
features of all utterances in the conversations. The
dyadic conversations are present in the form of
videos. Each utterance of a particular conversation
is thus a small segment of the full video. For each
utterance, we extract features for the modes: audio,
visual and text. The process of feature extraction
for each mode is described below.

4.1.1 Textual Features Extraction
We extract features from the transcript of an ut-
terance video using convolutional neural networks
(CNNs). CNNs are effective in learning high level
abstract representations of sentences from constitut-
ing words or n-grams (Kalchbrenner et al., 2014).
To get our sentence representation, we use a sim-
ple CNN with one convolutional layer followed by
max-pooling (Kim, 2014; Poria et al., 2016).

Specifically, the convolution layer consists fil-
ters of sizes 3,4 and 5 with 50 feature maps each.
Max-pooling is employed on these feature maps
with a pooling window of size 2. Finally, a fully
connected layer is used with 100 neurons. The
activations of this layer form our sentence repre-
sentation tu.

4.1.2 Audio Feature Extraction
To extract audio features we use openSMILE (Ey-
ben et al., 2010). It is an open-source software
which provides high dimensional audio vectors.
These vectors comprise of features like loudness,
Mel-spectra, MFCC, pitch, etc. Audio features play
a significant role in providing information on the
emotional state of a speaker (Song et al., 2004).

2124



In fact, the literature shows that there exists
a high correlation between many statistical mea-
sures of speech with speakers’ emotion. For ex-
ample, high pitch and fast speaking rate often de-
note anger while sadness associates low standard
deviation of pitch and slow speech rate (Dellaert
et al., 1996; Amir, 1998). In this work, we use
the IS13 ComParE1 config file which extracts a
total of 6373 features for each utterance video. Z-
standardization is performed for voice normaliza-
tion and dimension of the audio vector is reduced
to 100 using a fully-connected neural layer. This
provides the final audio feature vector au.

4.1.3 Visual Feature Extraction
Facial expressions and visual surrounding provide
rich emotional indicators. We use a 3D-CNN
to capture these details from the utterance video.
Apart from the benefits of extracting relevant fea-
tures from each image frame, 3D-CNN also ex-
tracts spatiotemporal features across frames (Tran
et al., 2015). This leads to the identification of
emotional expressions like a smile or frown.

The working of a 3D-CNN is identical to its 2D
counterpart with an input being a video v of di-
mension: (3, f, h,w). Here, 3 represents the RGB
channels and f, h,w are the number of frames,
height, and width of each frame, respectively. For
the convolution operation, a 3D filter fl of dimen-
sion (fm,3, fd, fh, fw) is used where, f[m/d/h/f]
represents number of feature maps, depth, height
and width of the filter, respectively. Max-pooling
is applied to the output of this convolution across a
3D sliding window of dimension (mp,mp,mp).

In our model, we use 128 feature maps for 3D
filters of size 5. For pooling, we set mp to be
3 whose output is fed to a fully connected layer
with 100 neurons. All the values are decided using
hyperparameter tuning (see Section 5). For the
input utterance, the activations of this layer form
the video representation vu.

Fusion: We perform feature level fusion to map
the individual modalities to a joint space. This is
done through a simple feature concatenation. Thus,
the extracted features tu, au and vu are joined to
form the utterance representation u = [tu;au; vu]
of dimension din = 300. This multimodal represen-
tation is generated for all utterances in a conversa-
tion.

1http://audeering.com/technology/
opensmile

Literature consists of numerous fusion tech-
niques for multimodal data (Atrey et al., 2010;
Zadeh et al., 2017; Poria et al., 2017c). Explor-
ing these on CMN, however, is beyond the scope
of this paper and left as a future work.

4.2 Conversational Memory Network
For classifying the emotion of an utterance ui, its
corresponding histories (hista and histb) are taken.
Each history histλ contains the preceding K utter-
ances by person Pλ (see Section 3). Here, both ui
and utterances in the histories are represented using
their multimodal feature vectors of dimensionRdin
(Figure 2).

The histories are first modeled into memory cells
using GRUs. This provides the memories with
context information summarized by the GRU. We
call this step as memory representation. Follow-
ing cognitive evidence of self-emotional dynam-
ics, we model separate memory cells for each per-
son. Thus, identical but separate computations are
performed on both histories. From these memo-
ries, content relevant to utterance ui is then filtered
out using attention mechanism over multiple in-
put/output hops. At each hop, both memories are
accumulated and merged with ui to model inter-
speaker emotional dynamics. First, we describe
our model as a single layer memory network which
runs one hop operation on the memories.
4.2.1 Single Layer
Here, we explain the representation scheme of the
memories for both histories and the input/output
operations on them along with attention mecha-
nism. The memory representation for each history
is generated using a GRU for modeling emotion
transitions. First, we define the GRU cell.
Gated Recurrent Unit: GRUs are a gating
mechanism in recurrent neural networks introduced
by (Cho et al., 2014). Similar to an LSTM (Hochre-
iter and Schmidhuber, 1997), GRU provides a sim-
pler computation with similar performance. At any
timestep t, it utilizes two gates rt (reset gate) and
zt (update gate) to control the combination criteria
with current input utterance ut and previous hidden
state st−1.

The new state st is computed as:

zt = σ(V z.ut +W z.st−1 + bz) (2)
rt = σ(V r.ut +W r.st−1 + br) (3)

ht = tanh(V h.ut +W h.(st−1 ⊗ rt) + bh) (4)
st = (1 − zt)⊗ ht + zt ⊗ st−1 (5)

2125



Memory Output

Conversation :

Pe
rs

on
 B

Pe
rs

on
 A

Multimodal  
Feature Extractor

Do
 y

ou
 h

av
e 

yo
ur

 fo
rm

?

Ex
cu

se
 M

e.

Le
t m

e 
se

e 
th

em
.

Is 
th

er
e 

a 
pr

ob
lem

?

W
ho

 to
ld

 y
ou

 to
 

ge
t i

n 
th

is 
lin

e?

Yo
u 

di
d.

Ok
ay

. B
ut

 I 
di

dn
't 

te
ll y

ou
 to

 
ge

t i
n 

th
is 

lin
e 

if 
yo

u 
ar

e 
fill

in
g 

ou
t 

th
is 

pa
rti

cu
lar

 
fo

rm
.

W
ha

t?
 I 

am
 

ge
tti

ng
 a

n 
ID

. 
Th

is 
is 

wh
y 

I 
ca

m
e 

he
re

.

Ye
ah

.

GRU GRU GRU GRU

GRU GRU GRU GRU

Happy
Sad
Neutral 
Angry

A V T A V T A V T A V T

A V T A V T A V T A V T

A
V
T

: Audio
: Video
: Text

A 
V 
T

softmax

q1i

o1b
weighted sum

∑
q2i

Memory Input

Memory I/O for a single hop

Memory  
Input for Pa

Memory 
Output for Pa

R hops

M′�b
1M1b

Figure 2: Overall architecture of proposed model: CMN. First, multimodal representations are extracted for each
utterance and then the previous K = 4 utterances by both persons are used to model the histories using GRUs. For
each person, R + 1 different GRUs are used to represent M (r)λ for all R hops. Then, attention based filtering using
multiple memory hops is performed. Finally, Person A’s utterance ui is classified to predict its emotion category.

Here, V,W and b are parameter matrices and
vector and ⊗ represents element-wise multiplica-
tion. The above equations can be summarized as:
st = GRUλ(st−1, ut).
Memory Representation: For each λ ∈ {a, b},
a memory representation Mλ = [m1λ, ...,mKλ ] for
histλ is generated using a GRU. To grasp the tem-
poral context, the K utterances in histλ are framed
as a sequence (starting from the oldest one) and
fed to the GRUλ. At each timestep t ∈ [1,K], the
GRUλ’s internal state st (equation 5) forms the tth

memory cell mtλ of memory representation Mλ.

Memory Input: This step takes the memory rep-
resentation Mλ and performs an attention mecha-
nism on it, resulting in an attention vector pλ ∈RK .
First, the current utterance ui is embedded into a
vector qi of dimension Rd using a projection ma-
trix B ∈ Rd×din . To find the relevance of each
memory mtλ’s context with qi, a match between
both is computed.
We do this by taking an inner product as follows:

qi = B.ui (6)
ptλ = softmax(qTi .mtλ) (7)

Here, softmax(xi) = exi/∑j exj and attention
vector pλ = {ptλ} is a probability distribution over
the input memories Mλ = {mtλ} for t ∈ [1,K].

Memory Output: First a new set of memories
are created using another GRU

′
λ to get new memory

representationM
′
λ = {(mtλ)′}. An output represen-

tation oλ ∈Rd is then generated using the weighted
sum of attention vector pλ and new memory M

′
λ as

follows:

oλ =∑
t

ptλ.(mtλ)′ =M ′λ.pλ (8)
Thus, the output representation oλ contains

weighted contextual summary accumulated from
the memory.

Final Prediction: To generate the predictions for
the current utterance ui, we combine the output rep-
resentations of both persons: oa and ob with ui’s
representation qi and perform an affine transforma-
tion using matrix Wo. Softmax is applied to this
final vector to get the emotion predictions,

ŷ = softmax(Wo.(qi + oa + ob)) (9)
Categorical cross-entropy is used as the loss:

Loss = −1
N

N∑
i=1

C∑
j=1yi,j log2(ŷi,j) (10)

Here, N denotes total utterances across all videos
andC is the number of emotion categories. yi is the
one-hot vector ground truth of ith utterance from
the training set and ŷi,j is its predicted probability
of belonging to class j.

2126



4.2.2 Multiple Layers
Many recent works on memory networks adopt
a multiple hop scheme in their network. This re-
peated input and output cycle on the memories
along with a soft attention module, leads to a re-
fined representation of the memories (Sukhbaatar
et al., 2015; Kumar et al., 2016). Motivated by
these works, we extend our model to perform R
hops on the memories. This is done by stacking the
single hop layers (Section 4.2.1) as follows:

• At a particular hop r, the output memory of the

previous hop M
′
λ

(r−1)
is used as the input mem-

ory of the current hop M (r)λ . Output memory of
current rth hop is generated using a newGRU (r)λ .
This constraint of sharing parameters adjacently
between layers is added for reduction in total
parameters and ease of training.

• At every hop, the query utterance ui’s represen-
tation qi is updated as:

q
(r+1)
i = q(r)i + o(r)a + o(r)b (11)

o
(r)
λ is calculated as per equation 8 using M

′
λ

(r)
.

• After R hops, the final prediction is done using
equation 9 as: ŷ = softmax(Wo.(q(R+1)i )). Al-
gorithm 1 summarizes the overall CMN network.

Algorithm 1 Conversational Memory Network
1: procedure CMN(ui, hista, histb,K,R) ▷ predict the

emotion of ui
2: q(1)i ← B.ui
3: M

′
λ

(0) ← GRU (0)λ (histλ)
4: for r in [1,R] do ▷ Multi-hop memory I/O
5: M (r)λ ←M ′λ(r−1)
6: M

′
λ

(r) ← GRU (r)λ (histλ)
7: pλ ← softmax( (qri )T .M (r)λ ) ▷ Memory in
8: o(r)λ ←M ′λ(r).pλ ▷ Memory out
9: q(r+1)i ← q(r)i + o(r)a + o(r)b ▷ Query update

10: return ŷ ← softmax(Wo.(q(R+1)i )) ▷ Prediction

5 Experiments

5.1 Dataset

We perform experiments on the IEMOCAP
dataset2 (Busso et al., 2008). It is a multimodal
database of 10 speakers (5 male and 5 female) in-
volved in two-way dyadic conversations. A pair

2http://sail.usc.edu/iemocap/

Figure 3: Each block represents an utterance and the
blocks are ordered as per temporal occurrence. Color
scheme identifies their corresponding emotions. The
arrows denote emotional influence directions.

of speakers is given multiple conversation scenar-
ios which are grouped in a single session. All the
conversations are segmented into utterances. Each
utterance is annotated using the following emo-
tion categories: anger, happiness, sadness, neu-
tral, excitement, frustration, fear, surprise, and
other. However, in our experiments, we consider
the first four categories. This is done to compare
our method with state-of-the-art frameworks (Po-
ria et al., 2017b; Rozgic et al., 2012). The dataset
provides rich video and audio samples for all the
utterances along with transcriptions.

Apart from these emotional states, we also in-
vestigate the valence and arousal degrees of each
utterance. IEMOCAP provides labels for both
these attributes on a 5-point Likert scale. Follow-
ing Aldeneh et al., 2017, we convert the attributes
into 3 categories, namely, low (≤ 2), medium
(> 2 and < 4) and high (≥ 4). The dataset configu-
ration for the experiments is obtained from Poria
et al. (2017b). The first 8 speakers (Session 1 - 4)
compose the training fold while the last session is
used as the testing fold. Overall, the training and
testing set comprises of 4290 utterances (120 con-
versational videos) and 1208 utterances (31 conver-
sational videos), respectively. There is no speaker
overlap in the training and testing set to make the
model person-independent.

5.2 Emotional Influence Patterns

In this section, we perform dataset exploration to
check the existence of emotional influences. Figure
3a) presents the emotion sequence of two videos

2127



sampled from the dataset. Both videos show the
presence of self and inter-speaker emotional influ-
ences. Visual exploration of videos from the dataset
reveal significant existence of such instances in the
conversations. To provide quantitative evidence of
the emotional influence patterns, we curate a non
exhaustive list of possible cases of influence. For
all utterances in the dataset, we sample their histo-
ries by setting K = 5, i.e., five previous utterances
(as per availability) from both speakers.

Cases 1 and 2 (Figure 3) represent scenarios
when the emotion of current utterance is influenced
by self or the other person respectively. In case 3,
the utterance has relevant content in the histories
that do not precede immediately. An effective atten-
tion mechanism provides the capability to capture
this pattern. Finally case 4 presents the situation
when the utterance is independent of the history.
Such situations are indicative from the content of
the utterance which often deviates from the previ-
ous topic of discussion or introduces a new infor-
mation. Table 1 presents a statistical summary of
these cases present in the dataset. From the table
it can be seen that a large section of the dataset
demonstrate these influence patterns. This pro-
vides motivation to explicitly model these patterns.
We thus hypothesize that models that are able to
capture these cases would have superior emotion
inference capabilities.

This passive exploration is a label-based analysis
which is performed as a sanity check. Needless to
say, existence of some false positive patterns at the
label level is imminent. On the other hand, our
model CMN is content-based which enables it to
mine intricate patterns from the utterance histories.

5.2.1 Training Details
We use 10% of the training set as a held-out valida-
tion set for hyperparameter tuning. To optimize the
parameters, we use Stochastic Gradient Descent
(SGD) optimizer, starting with an initial learning

Case 1 Case 2 Case 3 Case 4
Percentage 63.77 40.44 30.97 16.24

Table 1: Percentage of occurrence of different cases in the
dataset as mentioned in Section 5.2. All cases are analyzed
with K = 5. Utterances whose history has atleast 3 similar
emotion labels in either own history or the history of the other
person, is counted in case 1 or 2, respectively. Case 3 is
considered when the utterance’s emotion is found in atleast
3 utterances which occur before the second past-utterance of
each history. Case 4 is considered when no history has the
emotion label of the current utterance.

rate (lr) of 0.01. An annealing approach halves
the lr every 20 epochs and termination is decided
using an early-stop measure with a patience of 12
by monitoring the validation loss. Gradient clip-
ping is used for regularization with a norm set to
40. Hyperparameters are decided using a Random
Search (Bergstra and Bengio, 2012). Based on val-
idation performance, context window length K is
set to be 40 and the number of hops R is fixed at
3 hops. If K previous utterances are unavailable,
then null utterances are added at the beginning of
the history sequence. The dimension size of the
memory cells d is set as 50.

5.2.2 Baselines
We compare CMN with the following baselines:

SVM-ensemble: A strong context-free bench-
mark model which uses similar multimodal
approach on an ensemble of trees. Each
node represents binary support vector machines
(SVM) (Rozgic et al., 2012).

bc-LSTM: A bi-directional LSTM equipped
with hierarchical fusion, proposed by Poria et al.,
2017b. It is the present state-of-the-art method.
The model uses context features from unimodal
LSTMs and its concatenation is fed to a final LSTM
for classification. For fair comparison in an end-to-
end learning paradigm, we remove the penultimate
SVM of this model. The model doesn’t accommo-
date inter-speaker dependencies.

Memn2n: The original memory network as pro-
posed by Sukhbaatar et al., 2015. Contrasting to
CMN, the model generates the memory represen-
tations for each historical utterance using an em-
bedding matrix B as used in equation 7, without
sequential modeling. Thus for utterance ui, both
memories are created as Mλ using {mtλ = B.ut ∣
ut ∈ histλ and t ∈ [1,K]} for λ ∈ {a, b}.
CMNSelf : In this baseline, we use only self his-
tory for classifying emotion of utterance ui. Thus,
if ui is spoken by person Pa, then only hista is
considered. Clearly, this variant is also incapable
of modeling inter-speaker dependencies.

CMNNA: Single layer variant of the CMN with
no attention module. Thus, its output oλ (equa-
tion 8) is generated using a uniform probability
distribution pλ, i.e., {ptλ = 1K }Kt=1.
5.3 Results
Table 2 presents the performances of CMN and
its variants along with the state-of-the-art mod-

2128



Models Emotion Categories Valence Arousalhops history Happiness Sadness Neutral Anger WAA WAA UAR WAA UAR
SVM-ensemble1 - single 72.40 61.90 58.10 73.10 69.50 - - - -
bc-LSTM2 - single 74.21 76.50 66.31 75.68 74.31 64.3 62.3 70.1 45.0

Memn2n 1 dual 72.36 76.16 66.93 80.23 74.17 - - - -3 dual 75.03 76.36 66.45 81.59 75.08 65.3 64.0 71.5 45.6
CMNSelf 3 single 77.14† 76.99 66.99 87.26† 76.54† 65.5 64.0 72.1 47.1
CMNNA 1 dual 74.33 76.93 66.49 86.29† 75.77 65.6 64.2 71.6 46.3
CMN 3 dual 81.75† 77.73 67.32 89.88† 77.62† 66.1 64.3 72.2 47.6
1(Rozgic et al., 2012), 2(Poria et al., 2017b). †: significantly better than bc-LSTM1

Table 2: Comparison of CMN and its variants with state-of-the-art models (Section 5.2.2). All results use multi-
modal features. We report scores using weighted accuracy (WAA) and unweighted recall (UAR). UAR is a popular
metric that is used when dealing with imbalanced classes (Rosenberg, 2012). Results are an average of 10 runs
with varied weight initializations. We assert significance when p < 0.05 under McNemar’s test.
els. CMN succeeds over both neural (Poria et al.,
2017b) and SVM-based (Rozgic et al., 2012) meth-
ods by 3.3% and 8.12%, respectively. Improvement
in performance is seen for all emotions over the
ensemble-SVM based method. A similar trend is
seen with bc-LSTM (Poria et al., 2017b), where
our model does explicitly well for the active emo-
tions happiness and anger. This trend suggests
that CMN is capable of capturing inter-speaker
emotional influences which are often seen in the
presence of such active emotions.

The importance of sequential processing of
the histories using a recurrent neural network (in
our case, a GRU) is evidenced by the poorer
performance of Memn2n with respect to CMN.
This suggests that gathering contexts temporally
through sequential processing is indeed a superior
method over non-temporal memory representations.
CMNself which uses only single history channel
also provides lesser performance when compared
to CMN. This signifies the role of inter-speaker
influences that often moderate the emotions of the
current utterance. Overall, predictions on valence
and arousal levels also show similar results which
reinforce our hypothesis of CMN’s ability to model
emotional dynamics.

Models unimodal unimodal unimodal trimodalaudio visual text
SVM-ensemble 60.8 51.5 48.5 69.5‡

bc-LSTM 62.2 56.1 72.5 74.3‡

Memn2n 63.0 61.8 72.6 75.0‡

CMNself 63.1 62.5 73.0 76.5‡

CMNNA 62.4 60.9 74.1 75.7
CMN 65.3 64.2 74.2 77.6‡

‡: significantly better than unimodals (p < 0.05)
Table 3: Comparison of CMN to all the baselines in
different modalities. Weighted accuracy is used as the
metric.

Hyperparameters: Figure 4 provides a sum-
mary of the performance trend of our model for
different values of the hyperparameters K (context
window length) and Q (number of hops). In the
first graph, as K increases, more past-utterances
are provided to the model as memories. The per-
formance maintains a positive correlation with K.
This trend supplements our intuition that the histor-
ical context acts as an essential resource to model
emotional dynamics. Given enough history, the
performance saturates. The second graph shows
that multiple hops on the histories indeed lead to an
improvement in performance. The attention-based
filtering in each hop provides a refined context rep-
resentation of the histories. Models with hops in
the range of 3−10 outperform the single layer vari-
ant. However, each added hop contributes a new set
of parameters for memory representation, leading
to an increase in total parameters of the model and
making it susceptible to overfitting. This effect is
evidenced in the figure where higher hops lead to a
dip in performance.

Multimodality: Table 3 summarizes the perfor-
mance of unimodal and multimodal variants of the
baselines along with CMN. As seen in the table,
text modality performes best out of the three. This
is in contrast to Rozgic et al. 2012 where audio pro-
vides the best performance. A possible reason for

tes
t a

cc
ur

ac
y %

K: history window length Q: number of hops

Figure 4: Performance trends of our model with differ-
ent values of K (history length) and Q (number of hops).
While K is varied, Q is set to be 3. Similarly, K = 20
when Q varies.

2129



       You mustn't be serious my dear 
one, that's just what they want.  [hap]

Pe
rs

on
 A

Pe
rs

on
 B

Whose they? [hap]

With the most perfect poise .  [hap]

H
is

to
rie

s

Person A: All the futile mortals 
who try to make life 

unbearable. [?]

To classify:

     Hello? What? ... 
Wrong Number  [hap]

Oh , it sent shivers up 
my spine. [hap]

2.

7.

1.

6.

Behave exquisitely [hap]
4.

    Oh, what shall we 
do if they suddenly 

walk in on us?  [hap]

3.

5.

(a) Correct label: happiness

Pe
rs

on
 A

Pe
rs

on
 B

Well I’ll come back, what, are you not 
willing to wait for me? [ang]

You don’t have to do this. Its not 
gonna work out. We're not a 

complete family without you being 
here. [sad]I am afraid when you leave you won't come back. [sad] 

H
is

to
rie

s

Person A:  We are not a 
complete family here, you 

don’t understand. [?]

To classify:

So you're leaving tomorrow. [sad]

I have to do this. Do you 
think I want to miss seeing 

her grow? [sad]

    I don't know what to say. I 
don't want to go but I don't 

have a choice. [sad] 

2. 4. 6.

1.

3.

5.Past

In
cr

ea
si

ng
 a

tte
nt

io
n

Thanks that helps. I feel much better now. [ang]
7.

(b) Correct label: anger

Figure 5: Average attention vectors across 3 hops for both memories for a given test utterance.

this shift is the improved representational scheme
of the textual modality. Text tends to have lesser
noisy signals as opposed to audio-visual sources,
thus providing better features in the joint represen-
tation. Overall, multimodal systems outperform the
unimodal variants justifying the design of CMN as
a multimodal system.

Table 3 also showcases the superiority of CMN
and its variants over bc-LSTM. The proposed
model achieves better performance over the state
of the art in all the unimodal and multimodal seg-
ments. This asserts the importance of the memory-
network framework and its ability to effectively
store context information.

Role of Attention: Attention module plays a vi-
tal role in memory refinement. This is also ob-
served in Table 2, where CMNNA provides inferior
performance over CMN. With the uniform weight,
all the memory cells in both memories Ma and
Mb equally contribute to the output representation.
This incorporates irrelevant information from the
perspective of emotional context.

Case Study: We perform qualitative visualiza-
tion of the attention module by applying it on
the testing set. Figure 5a represents a conversa-
tion where both the speakers are in an excited and
jolly mood. Person A, in particular, drives the dia-
logue with less influence from Person B. To clas-
sify the test utterance of A, the attention module of
CMN successfully focuses on the utterances 1,3,5
which had triggered the speaker’s positive mood in
the video. This shows CMN’s capacity to model
speaker-based emotions. Also, at the textual level,
utterances 3 and 6 do not seem to depict a happy
mood. However, audio and visual sources provide
contrasting evidence which helps CMN to correctly
model them as utterances spoken with happiness.
This shows the advantage of a multimodal system.

In Figure 5b we reiterate through the dialogue

presented in Figure 1. As shown, Person A con-
verses in a sad mood (utterances 1,3,5 in Fig 5b),
bounded by the grief of his wife’s departure. But
when he expresses his inhibitions, his wife B re-
acts in an angry and sarcastic manner (utterance
7). This ignites an emotional shift for A who then
replies angrily. In this example, CMN is able to fo-
cus on utterance 7 spoken by B to anticipate A’s test
utterance to be an angry statement, thus showing
its ability to model inter-speaker influences. How-
ever, there are cases where our model fails, e.g., in
the absence of historical utterances as this forces
attention to focus on null memories.

6 Conclusion

In this paper, we presented a deep neural frame-
work that identifies emotions for utterances in
dyadic conversational videos. Our results suggest
that leveraging context information from utterance
histories and representing them as memories indeed
helps to better recognize emotions. Performing
speaker-specific modeling and considering inter-
speaker influences also helps in capturing emo-
tional dynamics.

This work also showed the importance of at-
tention mechanism in filtering relevant contextual
information from utterance histories and, hence,
paved the path to the development of more efficient
and human-like dialogue systems.

Acknowledgement

This research was supported in part by the National
Natural Science Foundation of China under Grant
no. 61472266 and by the National University of
Singapore (Suzhou) Research Institute, 377 Lin
Quan Street, Suzhou Industrial Park, Jiang Su, Peo-
ple’s Republic of China, 215123.

2130



References
Zakaria Aldeneh, Soheil Khorram, Dimitrios Dimitri-

adis, and Emily Mower Provost. 2017. Pooling
acoustic and lexical features for the prediction of va-
lence.

Noam Amir. 1998. Towards an automatic classification
of emotions in speech. ICSLP, pages 699–702.

Pradeep K Atrey, M Anwar Hossain, Abdulmotaleb
El Saddik, and Mohan S Kankanhalli. 2010. Mul-
timodal fusion for multimedia analysis: a survey.
Multimedia systems, 16(6):345–379.

Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben-
gio. 2014. Neural machine translation by jointly
learning to align and translate. arXiv preprint
arXiv:1409.0473.

James Bergstra and Yoshua Bengio. 2012. Random
search for hyper-parameter optimization. Journal of
Machine Learning Research, 13(Feb):281–305.

Dario Bertero, Farhad Bin Siddique, Chien-Sheng Wu,
Yan Wan, Ricky Ho Yin Chan, and Pascale Fung.
2016. Real-time speech emotion and sentiment
recognition for interactive dialogue systems. In
EMNLP, pages 1042–1047.

Carlos Busso, Murtaza Bulut, Chi-Chun Lee, Abe
Kazemzadeh, Emily Mower, Samuel Kim, Jean-
nette N Chang, Sungbok Lee, and Shrikanth S
Narayanan. 2008. Iemocap: Interactive emotional
dyadic motion capture database. Language re-
sources and evaluation, 42(4):335.

Carlos Busso, Zhigang Deng, Serdar Yildirim, Murtaza
Bulut, Chul Min Lee, Abe Kazemzadeh, Sungbok
Lee, Ulrich Neumann, and Shrikanth Narayanan.
2004. Analysis of emotion recognition using facial
expressions, speech and multimodal information. In
ICMI, pages 205–211.

Erik Cambria, Soujanya Poria, Alexander Gelbukh,
and Mike Thelwall. 2017. Sentiment analysis is a
big suitcase. IEEE Intelligent Systems, 32(6):74–80.

Erik Cambria, Soujanya Poria, Devamanyu Hazarika,
and Kenneth Kwok. 2018. SenticNet 5: Discover-
ing conceptual primitives for sentiment analysis by
means of context embeddings. In AAAI.

Ginevra Castellano, Loic Kessous, and George Cari-
dakis. 2008. Emotion recognition through multiple
modalities: face, body gesture, speech. Affect and
emotion in human-computer interaction, pages 92–
103.

Kyunghyun Cho, Bart Van Merriënboer, Caglar Gul-
cehre, Dzmitry Bahdanau, Fethi Bougares, Holger
Schwenk, and Yoshua Bengio. 2014. Learning
phrase representations using rnn encoder-decoder
for statistical machine translation. arXiv preprint
arXiv:1406.1078.

Dragos Datcu and L Rothkrantz. 2008. Semantic audio-
visual data fusion for automatic emotion recognition.
Euromedia’2008.

Dragoş Datcu and Léon JM Rothkrantz. 2011. Emo-
tion recognition using bimodal data fusion. In Inter-
national Conference on Computer Systems and Tech-
nologies, pages 122–128. ACM.

Munmun De Choudhury, Michael Gamon, Scott
Counts, and Eric Horvitz. 2013. Predicting depres-
sion via social media. ICWSM, 13:1–10.

Frank Dellaert, Thomas Polzin, and Alex Waibel. 1996.
Recognizing emotion in speech. In ICSLP, vol-
ume 3, pages 1970–1973.

Sidney K D’mello and Jacqueline Kory. 2015. A re-
view and meta-analysis of multimodal affect detec-
tion systems. ACM Computing Surveys (CSUR),
47(3):43.

Paul Ekman. 1993. Facial expression and emotion.
American psychologist, 48(4):384.

Florian Eyben, Martin Wöllmer, and Björn Schuller.
2010. Opensmile: the munich versatile and fast
open-source audio feature extractor. In Interna-
tional Conference on Multimedia, pages 1459–1462.
ACM.

Alex Graves, Greg Wayne, and Ivo Danihelka.
2014. Neural turing machines. arXiv preprint
arXiv:1410.5401.

Shlomo Hareli and Anat Rafaeli. 2008. Emotion cy-
cles: On the social influence of emotion in organiza-
tions. Research in organizational behavior, 28:35–
59.

Sepp Hochreiter and Jürgen Schmidhuber. 1997.
Long short-term memory. Neural computation,
9(8):1735–1780.

Nal Kalchbrenner, Edward Grefenstette, and Phil Blun-
som. 2014. A convolutional neural network for mod-
elling sentences. In ACL 2014, volume 1, pages
655–665.

Yoon Kim. 2014. Convolutional neural networks for
sentence classification. In EMNLP 2014, pages
1746–1751.

Peter Koval and Peter Kuppens. 2012. Changing emo-
tion dynamics: individual differences in the effect of
anticipatory social stress on emotional inertia. Emo-
tion, 12(2):256.

Ankit Kumar, Ozan Irsoy, Peter Ondruska, Mohit Iyyer,
James Bradbury, Ishaan Gulrajani, Victor Zhong,
Romain Paulus, and Richard Socher. 2016. Ask me
anything: Dynamic memory networks for natural
language processing. In ICML, pages 1378–1387.

Feng Liu and Sally Maitlis. 2014. Emotional dynam-
ics and strategizing processes: A study of strategic
conversations in top team meetings. Journal of Man-
agement Studies, 51(2):202–234.

2131



Michael W Morris and Dacher Keltner. 2000. How
emotions work: The social functions of emotional
expression in negotiations. Research in organiza-
tional behavior, 22:1–50.

Costanza Navarretta, K Choukri, T Declerck, S Goggi,
M Grobelnik, and B Maegaard. 2016. Mirroring
facial expressions and emotions in dyadic conversa-
tions. In LREC.

Rosalind W Picard. 2010. Affective computing: from
laughter to ieee. IEEE Transactions on Affective
Computing, 1(1):11–17.

Soujanya Poria, Erik Cambria, Rajiv Bajpai, and Amir
Hussain. 2017a. A review of affective computing:
From unimodal analysis to multimodal fusion. In-
formation Fusion, 37:98–125.

Soujanya Poria, Erik Cambria, Devamanyu Hazarika,
Navonil Majumder, Amir Zadeh, and Louis-Philippe
Morency. 2017b. Context-dependent sentiment anal-
ysis in user-generated videos. In ACL 2017, vol-
ume 1, pages 873–883.

Soujanya Poria, Erik Cambria, Devamanyu Haz-
arika, Navonil Mazumder, Amir Zadeh, and Louis-
Philippe Morency. 2017c. Multi-level multiple at-
tentions for contextual multimodal sentiment analy-
sis. In ICDM 2017, pages 1033–1038. IEEE.

Soujanya Poria, Erik Cambria, Devamanyu Hazarika,
and Prateek Vij. 2016. A deeper look into sarcastic
tweets using deep convolutional neural networks. In
COLING 2016, pages 1601–1612.

Hiranmayi Ranganathan, Shayok Chakraborty, and
Sethuraman Panchanathan. 2016. Multimodal emo-
tion recognition using deep learning architectures.
In WACV, pages 1–9. IEEE.

Jane M Richards, Emily A Butler, and James J Gross.
2003. Emotion regulation in romantic relation-
ships: The cognitive consequences of concealing
feelings. Journal of Social and Personal Relation-
ships, 20(5):599–620.

Andrew Rosenberg. 2012. Classifying skewed data:
Importance weighting to optimize average recall. In
INTERSPEECH 2012.

Viktor Rozgic, Sankaranarayanan Ananthakrishnan,
Shirin Saleem, Rohit Kumar, and Rohit Prasad.
2012. Ensemble of svm trees for multimodal emo-
tion recognition. In APSIPA ASC, pages 1–4.

Johanna Ruusuvuori. 2013. Emotion, affect and con-
versation. The handbook of conversation analysis,
pages 330–349.

Jack Sidnell and Tanya Stivers. 2012. The handbook of
conversation analysis, volume 121. John Wiley &
Sons.

Mingli Song, Jiajun Bu, Chun Chen, and Nan Li. 2004.
Audio-visual based emotion recognition-a new ap-
proach. In CVPR, volume 2.

Sainbayar Sukhbaatar, Jason Weston, Rob Fergus, et al.
2015. End-to-end memory networks. In NIPS 2015,
pages 2440–2448.

Du Tran, Lubomir Bourdev, Rob Fergus, Lorenzo Tor-
resani, and Manohar Paluri. 2015. Learning spa-
tiotemporal features with 3d convolutional networks.
In ICCV 2015, pages 4489–4497.

Panagiotis Tzirakis, George Trigeorgis, Mihalis A
Nicolaou, Björn W Schuller, and Stefanos Zafeiriou.
2017. End-to-end multimodal emotion recogni-
tion using deep neural networks. IEEE JSTSP,
11(8):1301–1309.

Haohan Wang, Aaksha Meghawat, Louis-Philippe
Morency, and Eric P Xing. 2017. Select-additive
learning: Improving generalization in multimodal
sentiment analysis. In ICME, pages 949–954.

Jason Weston, Sumit Chopra, and Antoine Bor-
des. 2014. Memory networks. arXiv preprint
arXiv:1410.3916.

Martin Wöllmer, Angeliki Metallinou, Florian Eyben,
Björn Schuller, and Shrikanth S Narayanan. 2010.
Context-sensitive multimodal emotion recognition
from speech and facial expression using bidirec-
tional lstm modeling. In INTERSPEECH 2010.

Peng Xiaolan, Xie Lun, Liu Xin, and Wang Zhil-
iang. 2013. Emotional state transition model based
on stimulus and personality characteristics. China
Communications, 10(6):146–155.

Frank Xing, Erik Cambria, and Roy Welsch. 2018. Nat-
ural language based financial forecasting: A survey.
Artificial Intelligence Review.

Liang Yang, Hong-fei LIN, and Wei GUO. 2011. Text-
based emotion transformation analysis. Computer
Engineering & Science, 9:026.

Tom Young, Erik Cambria, Iti Chaturvedi, Hao Zhou,
Subham Biswas, and Minlie Huang. 2018. Aug-
menting end-to-end dialog systems with common-
sense knowledge. In AAAI.

Tom Young, Devamanyu Hazarika, Soujanya Poria,
and Erik Cambria. 2017. Recent trends in deep
learning based natural language processing. arXiv
preprint arXiv:1708.02709.

Amir Zadeh, Minghai Chen, Soujanya Poria, Erik Cam-
bria, and Louis-Philippe Morency. 2017. Tensor fu-
sion network for multimodal sentiment analysis. In
EMNLP 2017, pages 1103–1114.

Hao Zhou, Minlie Huang, Tianyang Zhang, Xiaoyan
Zhu, and Bing Liu. 2017. Emotional chatting ma-
chine: Emotional conversation generation with inter-
nal and external memory. arXiv:1704.01074.

2132


