



















































Building compositional semantics and higher-order inference system for a wide-coverage Japanese CCG parser


Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 2236–2242,
Austin, Texas, November 1-5, 2016. c©2016 Association for Computational Linguistics

Building compositional semantics and higher-order inference system
for a wide-coverage Japanese CCG parser

Koji Mineshima1
mineshima.koji@ocha.ac.jp

Ribeka Tanaka1
tanaka.ribeka@is.ocha.ac.jp

Pascual Martı́nez-Gómez2
pascual.mg@aist.go.jp

Yusuke Miyao3
yusuke@nii.ac.jp

Daisuke Bekki1
bekki@is.ocha.ac.jp

1Ochanomizu University
Tokyo, Japan

2AIST
Tokyo, Japan

3National Institute of Informatics
The Graduate University for Advanced Studies

Tokyo, Japan

Abstract

This paper presents a system that compo-
sitionally maps outputs of a wide-coverage
Japanese CCG parser onto semantic represen-
tations and performs automated inference in
higher-order logic. The system is evaluated
on a textual entailment dataset. It is shown
that the system solves inference problems that
focus on a variety of complex linguistic phe-
nomena, including those that are difficult to
represent in the standard first-order logic.

1 Introduction

Logic-based semantic representations have played
an important role in the study of semantic parsing
and inference. For English, several methods have
been proposed to map outputs of parsers based on
syntactic theories like CCG (Steedman, 2000) onto
logical formulas (Bos, 2015). Output formulas have
been used in various tasks, including Question An-
swering (Lewis and Steedman, 2013) and Recog-
nizing Textual Entailment (RTE) (Bos and Markert,
2005; Beltagy et al., 2013; Bjerva et al., 2014).

Syntactic and semantic parsing for Japanese, by
contrast, has been dominated by chunk-based de-
pendency parsing and semantic role labelling (Kudo
and Matsumoto, 2002; Kawahara and Kurohashi,
2011; Hayashibe et al., 2011). Recently, the method
of inducing wide-coverage CCG resources for En-
glish (Hockenmaier and Steedman, 2007) has been
applied to Japanese and a robust CCG parser based
on it has been developed (Uematsu et al., 2015).
However, building a method to map CCG trees in
Japanese onto logical formulas is not a trivial task,

mainly due to the differences in syntactic structures
between English and Japanese (Section 3).

There are two primary contributions of this pa-
per. First, based on an in-depth analysis of the
syntax-semantics interface in Japanese, we present
the first system that compositionally derives seman-
tic representations for a wide-coverage Japanese
CCG parser. Output representations are formulas
in higher-order logic (HOL) combined with Neo-
Davidsonian Event Semantics (Parsons, 1990). Sec-
ond, we demonstrate the capacity of HOL for textual
entailment. We evaluate the system on a Japanese
textual entailment dataset (Kawazoe et al., 2015), a
dataset constructed in a similar way to the FraCaS
dataset for English (Cooper et al., 1994; MacCartney
and Manning, 2007). Although it is usually thought
that HOL is unfeasible for practical applications, the
results show that the entire system is able to perform
efficient logical inference on complex linguistic phe-
nomena such as generalized quantifiers and inten-
sional modifiers — phenomena that pose challenges
to the standard first-order-logic-based approaches.

2 Background and system overview

This section provides a brief overview of the en-
tire system as applied to RTE, a task of determin-
ing whether a given text (T ) entails, contradicts, or
is just consistent with, a given hypothesis (H). In
logic-based approaches, the meanings of T and H
are represented by logical formulas; whether the en-
tailment relation holds is typically determined by
checking whether T → H is a theorem in a logical
system with the help of a knowledge base.

Currently, first-order logic (FOL) is the most pop-

2236



ular logical system used for RTE (Bos and Mark-
ert, 2005; Lewis and Steedman, 2013; Bjerva et al.,
2014). One advantage of systems based on FOL is
that practical general-purpose theorem provers and
model-builders are available. However, a drawback
is that there are linguistic phenomena that cannot be
represented in the standard FOL; a typical example
is a generalized quantifier such as most (Barwise and
Cooper, 1981). Accordingly, it has been standard in
formal semantics of natural language to use HOL
as a representation language (Montague, 1974). Al-
though HOL does not have general-purpose theorem
provers, there is room for developing an automated
reasoning system specialized for natural language
inference. In general, a higher-order representation
makes the logical structure of a sentence more ex-
plicit than a first-order encoding does and hence can
simplify the process of proof search (Miller and Na-
dathur, 1986). Recently, based on the evaluation on
the FraCaS dataset (Cooper et al., 1994), Mineshima
et al. (2015) showed that a higher-order inference
system outperformed the Boxer/Nutcracker’s first-
order system (Bos, 2008) in both speed and ac-
curacy. Likewise, Abzianidze (2015) developed a
higher-order prover based on natural logic tableau
system and showed that it achieved high accuracy
comparable to state-of-the-art results on the SICK
dataset (Marelli et al., 2014).

There are three main steps in our pipeline. The
focus of this paper is on the last two components.
1. Syntactic parsing Input sentences are mapped
onto CCG trees. We use a Japanese CCG parser
Jigg (Noji and Miyao, 2016)1, a statistical parser
based on Japanese CCGbank (Uematsu et al., 2015).
2. Semantic parsing CCG derivation trees are
compositionally mapped onto semantic representa-
tions in HOL. The compositional mapping is imple-
mented via simply typed λ-calculus in the standard
way (Bos, 2008; Martı́nez-Gómez et al., 2016).
3. Logical inference Theorem proving in HOL
is performed to check for entailment and contra-
diction. Axioms and proof-search procedures are
largely language-independent, so we use the higher-
order inference system of Mineshima et al. (2015)2

and adapt it for our purpose.

1https://github.com/mynlp/jigg
2https://github.com/mynlp/ccg2lambda

Syntactic category Semantic representation
NP λNF.∃x(N(Base, x) ∧ F (x))
S\NPga λQK.Q(λI.I, λx.∃v(K(Base, v) ∧ (Nom(v) = x)))
S\NPga\NPo λQ2Q1K.Q1(λI.I, λx1.Q2(λI.I, λx2.∃v(K(Base, v)

∧ (Nom(v) = x1) ∧ (Acc(v) = x2))))
S/S λSK.S(λJv.K(λv′.(J(v′) ∧ Base(v′)), v))
NP/NP λQNF.Q(λGx.N(λy.(Base(y) ∧ G(y)), x), F )

Table 1: Examples of semantic templates. Base is the position
in which the base form of a word appears.

3 Compositional Semantics and HOL

3.1 CCG and semantic lexicon
Combinatory Categorial Grammar (CCG) (Steed-
man, 2000) is a lexicalized grammar formalism
suitable for implementing a compositional mapping
from syntax to semantics. A syntactic category of
CCG is either a basic category such as S and NP
or a functional category of the form X/Y or X\Y.
The meaning of a sentence is computed from a small
number of combinatory rules and the meanings of
constituent words. In addition to standard combi-
natory rules, the Japanese CCG parser uses a small
number of unary type-shifting rules (e.g., the rel-
ativization rule that changes the category S\NP to
NP/NP), to which suitable meaning composition
rules are given.

We follow the standard method of building a
semantic lexicon in CCG-based logical seman-
tics (Bos, 2008). There are two kinds of lexical en-
tries: (1) semantic templates that are schematic en-
tries assigned to syntactic categories, possibly with
syntactic features and (2) lexical entries directly as-
signed to a limited number of logical and functional
expressions. Lexical entries can be sensitive to a
POS tag, a surface form, and other information con-
tained in the parser output. Table 1 shows semantic
templates for main syntactic categories. More de-
tails will be provided in Section 3.2 and 3.3.

We use a language of standard higher-order logic
(simple type theory) (Carpenter, 1997) as a represen-
tation language. Expressions in HOL are assigned
semantic types. We use three basic types: E (En-
tity), Ev (Event), and Prop (Proposition). Thus, the
semantic types of expressions in our system are de-
fined by the rule

T ::= E | Ev | Prop | T1 ⇒ T2
where T1 ⇒ T2 is a function type.

First-order language can be taken as a fragment
of this system; apart from logical connectives and

2237



NP • = ((E⇒Prop)⇒E⇒Prop)⇒(E⇒Prop)⇒Prop
S• = ((Ev⇒Prop)⇒Ev⇒Prop)⇒Prop
(C1/C2)• = (C1\C2)• = C2• ⇒C1•

Figure 1: The mapping from syntactic categories to semantic
types. ⇒ is right-associative.

quantifiers, all primitive expressions in first-order
logic are confined to constant symbols of type E and
predicates of type E ⇒ Prop, E ⇒ E ⇒ Prop, and
so on. Thus, adopting higher-order language does
not lead to the loss of the expressive power of first-
order language.

The Japanese CCG parser simplifies the standard
CCG and uses two basic categories, S and NP. Ac-
cordingly, a mapping (·)• from syntactic categories
to semantic types can be defined as in Figure 1.
Keeping the correspondence between syntactic cat-
egories and semantic types in the semantic lexicon
guarantees that a well-formed formula is compo-
sitionally derived from the meaning assignment to
each leaf of a CCG derivation tree.

3.2 Semantic composition for VPs
To model a semantics for VPs in Japanese, we adopt
Neo-Davidsonian Event Semantics (Parsons, 1990;
Jurafsky and Martin, 2009), which is widely used
in the NLP field. For instance, the sentence (1) is
analyzed as having the logical form in (2):

(1) ジョン
John

が
NOM

ゆっくり
slowly

歩い
walk

た。
PAST

‘John walked slowly’

(2) ∃v(walk(v) ∧ (Nom(v)= john) ∧ slow(v) ∧ Past(v))

In this approach, verbs are analyzed as 1-place pred-
icates over events; arguments and adjuncts of VPs
are also analyzed as event predicates. This seman-
tic uniformity is suitable to handling Japanese syn-
tactic structures in which the arguments of a VP
is often implicit and thus the argument-adjunct dis-
tinction is less transparent than languages like En-
glish (Pietroski, 2005). As is seen in (2), we adopt
the unique-role requirement for case markers (Carl-
son, 1984); for instance, the nominative case marker
does not denote the relation Nom(v, x), as in the
event semantics in Boxer (Bos, 2008), but the func-
tion Nom(v)=x. This treatment allows us to make
use of logical properties of equality and hence is
more suited to theorem-proving in our setting.

To derive a semantic representation in event se-
mantics compositionally, we adopt the composi-
tional semantics of VPs in Champollion (2015) and
analyze VPs themselves as introducing existential
quantification over events. To derive the correct
meaning for VP modifiers, the semantic type of a
verb is raised so that the verb takes a modifier as
argument but not vice versa. Figures 2 and 3 give
example derivations.

VP modifiers such as slowly license an inference
from John walked slowly to John walked, an infer-
ence correctly captured by the formula in (2). In En-
glish and Japanese, however, there are intensional
VP modifiers that do not license this inference pat-
tern. Thus, the sentence John almost walked does
not entail John walked (Dowty, 1979). While it is
not easy to provide a desirable analysis in first-order
language (Hobbs, 1985), HOL gives a perspicuous
representation:

(3) ∃v(almost(walk, v)∧ (Nom(v)= john)∧Past(v))

Here, almost is a higher-order predicate having the
semantic type (Ev ⇒ Prop) ⇒ Ev ⇒ Prop. The
meaning assignment to VP modifiers of category
S/S in Table 1 is for extensional modifiers; an
intensional modifier is assigned the representation
λSK.S(λJv.K(Base(J), v)) in the lexical entry,
which results in a representation as in (3).

3.3 Semantic composition for NPs

The quantificational structure of an NP plays a cru-
cial role in capturing basic entailment patterns such
as monotonicity inference. In the case of English,
quantificational structures are specified by the type
of determiners (e.g. a, the, every, some, no); to-
gether with the category distinction between N and
NP, which is supported in English CCGbank (Hock-
enmaier and Steedman, 2007), one can provide a
correct representation for NPs.

By contrast, Japanese is a classifier language,
where NPs freely occur without determiners in ar-
gument position (Chierchia, 1998). For example, the
subject in (4) appears in argument position without
accompanying any determiner.

(4) 小さな
small

犬
dog
が
NOM

吠え
bark

た。
PAST

‘A small dog barked’

2238



小さな (small)
NP/NP

λQNF.Q(λGx.N(λy.(small(y) ∧ G(y)), x), F )

犬 (dog)
NP

λNF.∃x(N(dog, x) ∧ F (x))
NP

λNF.∃x.(N(λy.(small(y) ∧ dog(y)), x) ∧ F (x))

> が (NOM)
NPga\NP

λQ.Q

NPga
λNF.∃x(N(λy.(small(y) ∧ dog(y)), x) ∧ F (x))

<

吠え (bark)
S\NPga

λQK.Q(λI.I, λx.∃v(K(bark, v)
∧ (Nom(v) = x)))

た (PAST)
S\S

λSK.S(λJv.K(λv′.(J(v′)
∧ Past(v′)), v))

S\NPga
λQK.Q(λI.I, λx.∃v(K(λv′.(bark(v′)

∧ Past(v′)), v) ∧ (Nom(v) = x)))

<B

S
λK.∃x(small(x) ∧ dog(x) ∧ ∃v(K(λv′.(bark(v′) ∧ Past(v′)), v) ∧ (Nom(v) = x)))

< PERIOD
S\S

λV.V (λI.I)

S
∃x(small(x) ∧ dog(x) ∧ ∃v(bark(v) ∧ Past(v) ∧ (Nom(v) = x)))

<

Figure 2: A CCG derivation tree for the sentence “A small dog barked”.

ほとんどの (Most)
NP/NP

λQNF.Most(λx.Q(λG.N(λy.(G(y)
∧ y = x)), λx.⊤), F )

車 (car)
NP

λNF.∃z(N(car, z)
∧ F (z))

NP
λNF.Most(λx.∃z(N(λy.(car(y)

∧ y = x)), z) ∧ ⊤), F )

> が (NOM)
NPga\NP

λQ.Q

NPga
λNF.Most(λx.∃z(N(λy.(car(y)

∧ y = x)), z) ∧ ⊤), F )

<

ゆっくり (slowly)
S/S

λSK.S(λJv.K(λv′.J(v′)
∧ slow(v′), v))

動い (move)
S\NPga

λQK.Q(λI.I, λx.∃v(K(move, v)
∧ (Nom(v) = x)))

た (PAST)
S\S

λSK.S(λJv.K(λv′.(J(v′)
∧ Past(v′)), v))

S\NPga
λQK.Q(λI.I, λx.∃v(K(λv′.(move(v′)

∧ Past(v′)), v) ∧ (Nom(v) = x)))

<B

S\NPga
λQK.Q(λI.I, λx.∃v(K(λv′.(move(v′) ∧ Past(v′) ∧ slow(v′)), v) ∧ (Nom(v) = x)))

>Bx

S
λK.Most(λx.∃z(car(z) ∧ z = x ∧ ⊤), λx.∃v(K(λv′.(move(v′) ∧ Past(v′) ∧ slow(v′)), v) ∧ (Nom(v) = x)))

< PERIOD
S\S

λV.V (λI.I)

S
Most(λx.∃z(car(z) ∧ z = x ∧ ⊤), λx.∃v(move(v) ∧ Past(v) ∧ slow(v) ∧ (Nom(v) = x)))

<

Figure 3: A CCG derivation tree for the sentence “Most cars moved slowly”. ⊤ denotes the tautology.

Bekki (2010) provides a comprehensive CCG gram-
mar for Japanese that adopts the N-NP distinction
and analyzes Japanese bare NPs as accompanying
the null determiner. The Japanese CCGbank, by
contrast, simplifies Bekki’s (2010) grammar and
avoids the use of the null determiner; it does not
use the category N and takes all NPs in Japanese to
have the syntactic category NP. This discrepancy in
NP-structure between English and Japanese poses a
challenge to the standard approach to building com-
positional semantics.

To provide a compositional semantics adapted for
the Japanese CCG, we take NPs themselves as in-
troducing quantification over individuals, along the
same lines as the semantics for VPs. The semantic
type of NPs needs to be raised so that they take NP-
modifiers as argument (cf. the template for NP in Ta-
ble 1). Figure 2 shows a derivation for the sentence
in (4), where the adjective small modifies the NP dog
to form a bare NP small dog. It should be noted
that the predicate small(x) is correctly inserted in-
side the scope of the existential quantification intro-
duced by the NP dog. The so-called privative adjec-
tives (e.g. fake and former) are analyzed in the same
way as intensional VP modifiers.

Following the analysis in Mineshima et al. (2015),
we analyze non-first-order generalized quantifier
most as having the higher-order logical form

Most(F,G), where Most has the type of general-
ized quantifier (E ⇒ Prop) ⇒ (E ⇒ Prop) ⇒ Prop.
Figure 3 shows an example derivation for a sentence
containing a generalized quantifier most. Our sys-
tem also handles floating quantifiers in Japanese.

4 Experiments

We evaluate our system3 on Japanese Semantics test
suite (JSeM)4 (Kawazoe et al., 2015), a Japanese
dataset for textual entailment designed in a simi-
lar way to the FraCaS dataset for English. These
datasets focus on the types of logical inferences that
do not require world knowledge. JSeM has Japanese
translations of FraCaS problems and an extended set
of problems focusing on Japanese syntax and se-
mantics. Each problem has one or more premises,
followed by a hypothesis. There are three types of
answer: yes (entailment), no (contradiction), and un-
known (neutral). Each problem is annotated with the
types of inference (logical entailment, presupposi-
tion, etc.) and of linguistic phenomena.

We evaluate the system on 523 problems in the
dataset. We focus on problems tagged with one
of the five phenomena: generalized quantifier, plu-

3The system will be available at https://github.
com/mynlp/ccg2lambda.

4http://researchmap.jp/community-inf/
JSeM/?lang=english

2239



Section #Problem Gold System SLC
Quantifier 337 92.3 78.0 88.4
Plural 41 68.3 56.1 51.2
Adjective 65 67.7 63.1 44.6
Verb 36 77.8 75.0 55.5
Attitude 44 88.6 86.4 75.0
Total 523 86.0 75.0 76.7

Table 2: Accuracy on each section of JSeM.

Acc. Prec. Rec. Time
Gold parses 86.0 94.9 81.3 3.30s

w/o HOL axioms 69.8 93.3 56.5 2.47s
System parses 75.0 92.7 65.4 3.58s
SLC 76.7 77.5 79.3 n/a
Most common class (yes) 56.8 56.8 85.6 n/a

Table 3: Accuracy, precision, recall, and average proof time.

ral, adjective, verb, and attitude. We use problems
whose inference type is logical entailment, exclud-
ing anaphora and presupposition. We use Kuromoji5

for morphological analysis. To focus on the evalua-
tion of semantic parsing and inference, we use gold
syntactic parses, which show an upper bound on the
performance of the semantic component. Gold syn-
tactic parses are manually selected from n-best out-
puts of the CCG parser. For the higher-order infer-
ence system, we use the axioms presented in Mi-
neshima et al. (2015) adapted with the necessary
modification for our event semantics.

Given premises P1, ... , Pn and a hypothesis H, the
system outputs yes (P1 ∧· · ·∧Pn →H is proved), no
(P1 ∧· · ·∧Pn →¬H is proved), or unknown (neither
is proved in a fixed proof-search space).6 We set a
30 seconds timeout for each inference run; the sys-
tem outputs unknown after it. The current semantic
lexicon has 36 templates and 113 lexical entries.

Table 2 and 3 show the results. The system with
gold syntactic parses achieved 86% accuracy on the
total 523 problems, with high precision and reason-
able speed. There was no timeout.7 The accuracy
dropped to 70% when ablating HOL axioms (Table
3). SLC refers to the performance of a supervised
learning classifier8 based on 5-fold cross-validation
for each section. Although direct comparison is not

5http://www.atilika.org/
6Note that natural-logic-based systems (MacCartney and

Manning, 2008) do not handle multi-premised problems.
7Our pipeline was run single-threaded on Ubuntu Linux 64

bits with a CPU at 2.67GHz.
8We used NTCIR RITE baseline tools (http://www.cl.

ecei.tohoku.ac.jp/rite2/doku.php).

Section #Problem Gold System M15
Quantifier 335 92.5 78.2 78.4
Plural 38 65.8 52.6 66.7
Adjective 21 57.1 47.6 68.2
Verb 9 66.7 66.7 62.5
Attitude 14 78.6 78.6 76.9
Total 417 87.3 74.1 73.3

Table 4: The results on a subset of JSeM that is a translation of
FraCaS. M15 refers to the accuracy of Mineshima et al. (2015)

on the corresponding sections of FraCaS.

possible, our system with gold parses outperforms it
for all sections.

Out of the 523 problems, 417 are Japanese trans-
lations of the FraCaS problems. Table 4 shows a
comparison between the performance of our system
on this subset of the JSeM problems and the perfor-
mance of the RTE system for English in Mineshima
et al. (2015) on the corresponding problems in the
FraCaS dataset. Mineshima et al. (2015) used sys-
tem parses of the English C&C parser (Clark and
Curran, 2007). The total accuracy of our system is
comparable to that of Mineshima et al. (2015).

Most errors we found are due to syntactic parse
errors caused by the CCG parser, where no cor-
rect syntactic parses were found in n-best responses.
Comparison between gold parses and system parses
shows that correct syntactic disambiguation im-
proves performance.

5 Conclusion

To our knowledge, this study provides the first se-
mantic parsing system based on CCG that compo-
sitionally maps real texts in Japanese onto logical
forms. We have also demonstrated the capacity of
HOL for textual entailment. The evaluation on JSeM
showed that our system performs efficient logical in-
ference on various semantic phenomena, including
those that challenge the standard FOL. The attrac-
tiveness of a logic-based system is that it is highly
modular and can be extended with other components
such as a robust knowledge base (Lewis and Steed-
man, 2013; Beltagy et al., 2013; Bjerva et al., 2014).
Such an extension will be a focus of future work.

Acknowledgments We are grateful to the three
anonymous reviewers for their helpful comments
and suggestions. This research has been supported
by the JST CREST program.

2240



References
Lasha Abzianidze. 2015. A tableau prover for natural

logic and language. In Proceedings of the 2015 Con-
ference on Empirical Methods in Natural Language
Processing, pages 2492–2502.

Jon Barwise and Robin Cooper. 1981. Generalized quan-
tifiers and natural language. Linguistics and Philoso-
phy, 4(2):159–219.

Daisuke Bekki. 2010. A Formal Theory of Japanese
Grammar: The Conjugation System, Syntactic Struc-
tures, and Semantic Composition. Kuroshio. (In
Japanese).

Islam Beltagy, Cuong Chau, Gemma Boleda, Dan Gar-
rette, Katrin Erk, and Raymond Mooney. 2013. Mon-
tague meets Markov: Deep semantics with probabilis-
tic logical form. In 2nd Joint Conference on Lexi-
cal and Computational Semantics: Proceeding of the
Main Conference and the Shared Task, pages 11–21.

Johannes Bjerva, Johan Bos, Rob van der Goot, and
Malvina Nissim. 2014. The Meaning Factory: Formal
semantics for recognizing textual entailment and deter-
mining semantic similarity. In Proceedings of the 8th
International Workshop on Semantic Evaluation (Se-
mEval 2014), pages 642–646.

Johan Bos and Katja Markert. 2005. Recognising textual
entailment with logical inference. In Proceedings of
the conference on Human Language Technology and
Empirical Methods in Natural Language Processing,
pages 628–635.

Johan Bos. 2008. Wide-coverage semantic analysis with
Boxer. In Proceedings of the 2008 Conference on Se-
mantics in Text Processing, pages 277–286.

Johan Bos. 2015. Open-domain semantic parsing with
Boxer. In Proceedings of the 20th Nordic Conference
of Computational Linguistics, pages 301–304.

Greg Carlson. 1984. Thematic roles and their role in
semantic interpretation. Linguistics, 22(3):259–280.

Bob Carpenter. 1997. Type-Logical Semantics. MIT
press.

Lucas Champollion. 2015. The interaction of composi-
tional semantics and event semantics. Linguistics and
Philosophy, 38(1):31–66.

Gennaro Chierchia. 1998. Reference to kinds across lan-
guage. Natural Language Semantics, 6(4):339–405.

Stephen Clark and James R. Curran. 2007. Wide-
coverage efficient statistical parsing with CCG and
log-linear models. Computational Linguistics,
33(4):493–552.

Robin Cooper, Richard Crouch, Jan van Eijck, Chris Fox,
Josef van Genabith, Jan Jaspers, Hans Kamp, Manfred
Pinkal, Massimo Poesio, Stephen Pulman, et al. 1994.
FraCaS — a framework for computational semantics.
Deliverable, D16.

David Dowty. 1979. Word Meaning and Montague
Grammar. Springer.

Yuta Hayashibe, Mamoru Komachi, and Yuji Matsumoto.
2011. Japanese predicate argument structure analysis
exploiting argument position and type. In Proceedings
of IJCNLP 2011, pages 201–209.

Jerry R. Hobbs. 1985. Ontological promiscuity. In Pro-
ceedings of the 23rd annual meeting on Association
for Computational Linguistics, pages 60–69.

Julia Hockenmaier and Mark Steedman. 2007. CCG-
bank: a corpus of CCG derivations and dependency
structures extracted from the Penn Treebank. Compu-
tational Linguistics, 33(3):355–396.

Daniel Jurafsky and James H. Martin. 2009. Speech and
Language Processing. Prentice-Hall, Inc.

Daisuke Kawahara and Sadao Kurohashi. 2011. Genera-
tive modeling of coordination by factoring parallelism
and selectional preferences. In Proceedings of IJC-
NLP 2011, pages 456–464.

Ai Kawazoe, Ribeka Tanaka, Koji Mineshima, and
Daisuke Bekki. 2015. An inference problem set for
evaluating semantic theories and semantic processing
systems for Japanese. In Proceedings of LENLS12,
pages 67–73.

Taku Kudo and Yuji Matsumoto. 2002. Japanese de-
pendency analysis using cascaded chunking. In Pro-
ceedings of the 6th Conference on Natural Language
Learning, pages 63–69.

Mike Lewis and Mark Steedman. 2013. Combining
distributional and logical semantics. Transactions of
the Association for Computational Linguistics, 1:179–
192.

Bill MacCartney and Christopher D. Manning. 2007.
Natural logic for textual inference. In Proceedings
of the ACL-PASCAL Workshop on Textual Entailment
and Paraphrasing, pages 193–200.

Bill MacCartney and Christopher D. Manning. 2008.
Modeling semantic containment and exclusion in nat-
ural language inference. In Proceedings of the 22nd
International Conference on Computational Linguis-
tics, pages 521–528.

Marco Marelli, Stefano Menini, Marco Baroni, Luisa
Bentivogli, Raffaella Bernardi, and Roberto Zampar-
elli. 2014. A SICK cure for the evaluation of composi-
tional distributional semantic models. In Proceedings
of LREC2014, pages 216–223.

Pascual Martı́nez-Gómez, Koji Mineshima, Yusuke
Miyao, and Daisuke Bekki. 2016. ccg2lambda: a
compositional semantics system. In Proceedings of
ACL 2016 System Demonstrations, pages 85–90.

Dale Miller and Gopalan Nadathur. 1986. Some uses
of higher-order logic in computational linguistics. In
Proceedings of the 24th annual meeting on Associa-
tion for Computational Linguistics, pages 247–256.

2241



Koji Mineshima, Pascual Martı́nez-Gómez, Yusuke
Miyao, and Daisuke Bekki. 2015. Higher-order log-
ical inference with compositional semantics. In Pro-
ceedings of the 2015 Conference on Empirical Meth-
ods in Natural Language Processing, pages 2055–
2061.

Richard Montague. 1974. Formal Philosophy: Selected
Papers. Yale University Press New Haven.

Hiroshi Noji and Yusuke Miyao. 2016. Jigg: a frame-
work for an easy natural language processing pipeline.
In Proceedings of ACL 2016 System Demonstrations,
pages 103–108.

Terence Parsons. 1990. Events in the Semantics of En-
glish. MIT Press.

Paul Pietroski. 2005. Events and Semantic Architecture.
Oxford University Press.

Mark Steedman. 2000. The Syntactic Process. MIT
Press.

Sumire Uematsu, Takuya Matsuzaki, Hiroki Hanaoka,
Yusuke Miyao, and Hideki Mima. 2015. Integrat-
ing multiple dependency corpora for inducing wide-
coverage Japanese CCG resources. ACM Transac-
tions on Asian and Low-Resource Language Informa-
tion Processing, 14(1):1–24.

2242


