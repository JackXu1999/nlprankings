










































Reducing Wrong Labels in Distant Supervision for Relation Extraction


Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 721–729,
Jeju, Republic of Korea, 8-14 July 2012. c©2012 Association for Computational Linguistics

Reducing Wrong Labels in Distant Supervision for Relation Extraction

Shingo Takamatsu

System Technologies Laboratories

Sony Corporation

5-1-12 Kitashinagawa, Shinagawa-ku, Tokyo

Shingo.Takamatsu@jp.sony.com

Issei Sato and Hiroshi Nakagawa

Information Technology Center

The University of Tokyo

7-3-1 Hongo, Bunkyo-ku, Tokyo

{sato@r., n3@}dl.itc.u-tokyo.ac.jp

Abstract

In relation extraction, distant supervision

seeks to extract relations between entities

from text by using a knowledge base, such as

Freebase, as a source of supervision. When

a sentence and a knowledge base refer to the

same entity pair, this approach heuristically la-

bels the sentence with the corresponding re-

lation in the knowledge base. However, this

heuristic can fail with the result that some sen-

tences are labeled wrongly. This noisy labeled

data causes poor extraction performance. In

this paper, we propose a method to reduce

the number of wrong labels. We present a

novel generative model that directly models

the heuristic labeling process of distant super-

vision. The model predicts whether assigned

labels are correct or wrong via its hidden vari-

ables. Our experimental results show that this

model detected wrong labels with higher per-

formance than baseline methods. In the ex-

periment, we also found that our wrong label

reduction boosted the performance of relation

extraction.

1 Introduction

Machine learning approaches have been developed

to address relation extraction, which is the task of

extracting semantic relations between entities ex-

pressed in text. Supervised approaches are limited in

scalability because labeled data is expensive to pro-

duce. A particularly attractive approach, called dis-

tant supervision (DS), creates labeled data by heuris-

tically aligning entities in text with those in a knowl-

edge base, such as Freebase (Mintz et al., 2009).

�������� ������	 ������


������������ ������� ������� ����
��� ��� ���

��������� ����

������ !��"#$% &'( )*+, -, .�/0 1

������ !��"#$% 2*345 6+*2 .�/0 1

��7�

89':4;*6;)-+<=

89':4;*6;)-+<=

>?@AB>@CD E>FGECHI

Figure 1: Automatic labeling by distant supervision. Up-

per sentence: correct labeling; lower sentence: incorrect

labeling.

With DS it is assumed that if a sentence contains

an entity pair in a knowledge base, such a sentence

actually expresses the corresponding relation in the

knowledge base.

However, the DS assumption can fail, which re-

sults in noisy labeled data and this causes poor ex-

traction performance. An entity pair in a target text

generally expresses more than one relation while

a knowledge base stores a subset of the relations.

The assumption ignores this possibility. For in-

stance, consider the place of birth relation between

Michael Jackson and Gary in Figure 1. The upper

sentence indeed expresses the place of birth relation

between the two entities. In DS place of birth is as-

signed to the sentence, and it becomes a useful train-

ing example. On the other hand, the lower sentence

does not express this relation between the two enti-

ties, but the DS heuristic wrongly labels the sentence

as expressing it.

Riedel et al. (2010) relax the DS assumption as

at least one sentence containing an entity pair ex-

721



pressing the corresponding relation in the knowl-

edge base. They cast the relaxed assumption as

multi-instance learning. However, even the relaxed

assumption can fail. The relaxation is equivalent to

the DS assumption when a labeled pair of entities

is mentioned once in a target corpus (Riedel et al.,

2010). In fact, 91.7% of entity pairs appear only

once in Wikipedia articles (see Section 7).

In this paper, we propose a method to reduce the

number of wrong labels generated by DS without

using either of these assumptions. Given the labeled

corpus created with the DS assumption, we first pre-

dict whether each pattern, which frequently appears

in text to express a relation (see Section 4), expresses

a target relation. Patterns that are predicted not to ex-

press the relation are used to form a negative pattern

list for removing wrong labels of the relation.

The main contributions of this paper are as fol-

lows:

• To make the pattern prediction, we propose a
generative model that directly models the pro-

cess of automatic labeling in DS. Without any

strong assumptions like Riedel et al. (2010)’s,

the model predicts whether each pattern ex-

presses each relation via hidden variables (see

Section 5).

• Our variational inference for our generative
model lets us automatically calibrate parame-

ters for each relation, which are sensitive to the

performance (see Section 6).

• We applied our method to Wikipedia articles
using Freebase as a knowledge base and found

that (i) our model identified patterns express-

ing a given relation more accurately than base-

line methods and (ii) our method led to bet-

ter extraction performance than the original DS

(Mintz et al., 2009) and MultiR (Hoffmann et

al., 2011), which is a state-of-the-art multi-

instance learning system for relation extraction

(see Section 7).

2 Related Work

The increasingly popular approach, called distant

supervision (DS), or weak supervision, utilizes a

knowledge base to heuristically label a corpus (Wu

and Weld, 2007; Bellare and McCallum, 2007; Pal

et al., 2007). Our work was inspired by Mintz et al.

(2009) who used Freebase as a knowledge base by

making the DS assumption and trained relation ex-

tractors on Wikipedia. Previous works (Hoffmann

et al., 2010; Yao et al., 2010) have pointed out that

the DS assumption generates noisy labeled data, but

did not directly address the problem. Wang et al.

(2011) applied a rule-based method to the problem

by using popular entity types and keywords for each

relation. In (Bellare and McCallum, 2007; Riedel et

al., 2010; Hoffmann et al., 2011), they used multi-

instance learning, which deals with uncertainty of

labels, to relax the DS assumption. However, the re-

laxed assumption can fail when a labeled entity pair

is mentioned only once in a corpus (Riedel et al.,

2010). Our approach relies on neither of these as-

sumptions.

Bootstrapping for relation extraction (Riloff and

Jones, 1999; Pantel and Pennacchiotti, 2006; Carl-

son et al., 2010) is related to our method. In boot-

strapping, seed entity pairs of the target relation are

given in order to select reliable patterns, which are

used to extract new entity pairs. To avoid the selec-

tion of unreliable patterns, bootstrapping introduces

scoring functions for each pattern candidate. This

can be applied to our approach, which seeks to re-

duce the number of unreliable patterns by using a set

of given entity pairs. However, the bootstrapping-

like approach suffers from sensitive parameters that

are critical to its performance. Ideally, the parame-

ters such as a threshold for scoring function should

be determined for each relation, but there are no

principled methods (Komachi et al., 2008). In our

approach, parameters are calibrated for each rela-

tion by maximizing the likelihood of our generative

model.

3 Knowledge-based Distant Supervision

In this section, we describe DS for relation extrac-

tion. We use the term relation as the relation be-

tween two entities. A relation instance is a tuple

consisting of two entities and relation r. For exam-
ple, place of birth(Michael Jackson, Gary) in Fig-

ure 1 is a relation instance.

Relation extraction seeks to extract relation in-

stances from text. An entity is mentioned as a named

entity in text. We extract a relation instance from a

722



single sentence. For example, from the upper sen-

tence in Figure 1 we extract place of birth(Michael

Jackson, Gary). Since two entities mentioned in a

sentence do not always have a relation, we select en-

tity pairs from a corpus when: (i) the path of the de-

pendency parse tree between the corresponding two

named entities in the sentence is no longer than 4

and (ii) the path does not contain a sentence-like

boundary, such as a relative clause1 (Banko et al.,

2007; Banko and Etzioni, 2008). Banko and Et-

zioni (2008) found that a set of eight lexico-syntactic

forms covers nearly 95% of relation phrases in their

corpus. (Fader et al. (2011) found that this set covers

69% of their corpus). Our rule is designed to cover

at least the eight lexico-syntactic forms. We use the

entity pairs extracted by this rule.

DS uses a knowledge base to create labeled data

for relation extraction by heuristically matching en-

tity pairs. A knowledge base is a set of relation

instances about predefined relations. For each sen-

tence in the corpus, we extract all of its entity pairs.

Then, for each entity pair, we try to retrieve the rela-

tion instances about the entity pair from the knowl-

edge base. If we found such a relation instance, then

the set of its relation, the entity pair, and the sentence

is stored as a positive example. If not, then the set of

the entity pair and the sentence is stored as a nega-

tive example. Features of an entity pair are extracted

from the sentence containing the entity pair.

As mentioned in Section 1, the assumption of DS

can fail, resulting in wrong assignments of a relation

to sentences that do not express the relation. We call

such assignments wrong labels. An example of a

wrong label is place of birth assigned to the lower

sentence in Figure 1.

4 Wrong Label Reduction

We define a pattern as the entity types of an entity

pair2 as well as the sequence of words on the path

of the dependency parse tree from the first entity to

the second one. For example, from “Michael Jack-

son was born in Gary” in Figure 1, the pattern “[Per-

son] born in [Location]” is extracted. We use entity

1We reject sentence-like dependencies such as ccomp, com-

plm and mark
2If we use a standard named entity tagger, the entity types

are Person, Location, and Organization.

Algorithm 1 Wrong Label Reduction

labeled data generated by DS: LD
negative patterns for relation r: NegPat(r)
for each entry (r, Pair, Sentence) in LD do

pattern Pat← the pattern from (Pair, Sentence)
if Pat ∈ NegPat(r) then

remove (r, Pair, Sentence) from LD
end if

end for

return LD

types to distinguish the sentences that express differ-

ent relations with the same dependency path, such

as “ABBA was formed in Stockholm.” and “ABBA

was formed in 1970.”

Our aim is to remove wrong labels assigned to

frequent patterns, which cause poor precision. In-

deed, in our Wikipedia corpus, more than 6% of the

sentences containing the pattern “[Person] moved to

[Location]”, which does not express place of death,

are labeled as place of death, and the labels as-

signed to these sentences hurt extraction perfor-

mance (see Section 7.3.3). We would like to remove

place of death from the sentences that contain this

pattern.

In our method, we reduce the number of wrong

labels as follows: (i) given a labeled corpus with the

DS assumption, we first predict whether a pattern

expresses a relation and then (ii) remove wrong la-

bels using the negative pattern list, which is defined

as patterns that are predicted not to express the rela-

tion. In the first step, we introduce the novel gener-

ative model that directly models DS’s labeling pro-

cess and make the prediction (see Section 5). The

second step is formally described in Algorithm 1.

For relation extraction, we train a classifier for en-

tity pairs using the resultant labeled data.

5 Generative Model

We now describe our generative model, which pre-

dicts whether a pattern expresses relation r or not
via hidden variables. In this section, we consider re-

lation r since parameters are conditionally indepen-
dent if relation r and the hyperparameter are given.

An observation of our model is whether entity

pair i appearing with pattern s in the corpus is la-
beled with relation r or not. Our binary observa-
tions are written as Xr = {(xrsi)|s = 1, . . . , S, i =

723



JKLM

NO PQ RS

TUV
WXY

Z

[ \
]^

_

`` a

Figure 2: Graphical model representation of our model.

R indicates the number of relations. S is the number of
patterns. Ns is the number of entity pairs that appear
with pattern s in the corpus. xrsi is the observed vari-
ables. The circled variables except xrsi are parameters
or hidden variables. λ is the hyperparameter and mst is
constant. The boxes are “plates” representing replicates.

1, . . . , Ns},
3 where we define S to be the number of

patterns and Ns to be the number of entity pairs ap-
pearing with pattern s. Note that we count an entity
pair for given pattern s once even if the entity pair
is mentioned with pattern s more than once in the
corpus, because DS assigns the same relation to all

mentions of the entity pair.

Given relation r, our model assumes the follow-
ing generative process:

1. For each pattern s
Choose whether s expresses relation r or not

zrs ∼ Be(θr)
2. For each entity pair i appearing with pattern s

Choose whether i is labeled or not
xrsi ∼ P (xrsi|Zr, ar, dr, λ,M),

where Be(θr) is a Bernoulli distribution with pa-
rameter θr, zrs is a binary hidden variable that is 1
if pattern s expresses relation r and 0 otherwise, and
Zr = {(zrs)|s = 1, . . . , S}. Given a value of zrs,
we model two kinds of probabilities: one for pat-

terns that actually express relation r, i.e., P (xrsi =
1|zrs = 1), and one for patterns that do not express
r, i.e., P (xrsi = 1|zrs = 0). The former is simply
parameterized as 0 ≤ ar ≤ 1. We express the lat-
ter as brs = P (xrsi = 1|Zr, ar, dr, λ,M), which is
a function of Zr, ar, dr, λ and M; we explain its
modeling in the following two subsections.

3Since a set of entity pairs appearing with pattern s is differ-

ent, i should be written as is. For simplicity, however, we use i

for each pattern.

b bb

c defghifij ieklkm nglo p q
c fghifij ieklkm nglo p q

r
s

t

t

t t t

uv pngkkioewq ux pngkkioeyq

z{|}~


Figure 3: Venn diagram-like description. E1 and E2 are
sets of entity pairs. E1/E2 has 6/4 entity pairs because
the 6/4 entity pairs appear with pattern 1/2 in the target

corpus. Pattern 1 expresses relation r and pattern 2 does
not. Elements in E1 are labeled with probability ar =
3/6 = 0.5. Those in E2 are labeled with probability
br2 = ar(|E1 ∩ E2|/|E2|) = 0.5(2/4) = 0.25.

The graphical model of our model is shown in

Figure 2.

5.1 Example of Wrong Labeling

Using a simple example, we describe how we model

brs, the probability with which DS assigns relation r
to pattern s via entity pairs when pattern s does not
express relation r.

Consider two patterns: pattern 1 that expresses re-
lation r and pattern 2 that does not (i.e., zr1 = 1 and
zr2 = 0). We also assume that there are entity pairs
that appear with pattern 1 as well as with pattern 2 in
different places in the corpus (for example, Michael

Jackson and Gary in Figure 1). When such entity

pairs are labeled, relation r is assigned to pattern 1
and at the same time to wrong pattern 2. Such entity
pairs are observed as elements in the intersection of

the two sets of entity pairs, E1 and E2. Here, Es is
the set of entity pairs that appear with pattern s in
the corpus. This situation is described in Figure 3.

We model probability br2 as follows. In E1, an
entity pair is labeled with probability ar. We as-
sume that entity pairs in the intersection, E1 ∩ E2,
are also labeled with ar. From the viewpoint of E2,
entity pairs in its subset, E1 ∩ E2, are labeled with
ar. Therefore, br2 is modeled as

br2 = ar
|E1 ∩ E2|

|E2|
,

where |E| denotes the number of elements in set E.
An example of this calculation is shown in Figure 3.

724



We generalize the example in the next subsection.

5.2 Modeling of Probability brs

We model brs so that it is proportional to the number
of entity pairs that are shared with correct patterns

whose zrs = 1, i.e.,

brs = ar

∣

∣

∣

(

⋂

{t|zrt=1,t 6=s}
Et

)

∩ Es

∣

∣

∣

|Es|
, (1)

where
⋂

indicates set intersections. However, the

enumeration in Eq.1 requires O(SN2s ) computa-
tional cost and a huge amount of memory to store

all of the entity pairs. We approximate the right-

hand side of Eq.1 as

brs ≈ ar



1−
S

∏

t=1,t 6=s

(

1−
|Et ∩ Es|

|Es|

)zrt



 .

This approximation is made, given the sizes of all

Ess and those of all intersections of two Ess. This
has a lower computational cost of O(S) and let us
use less memory. We define S×S matrix M whose
elements are mst = |Et ∩ Es|/|Es|.

In reality, factors other than the process described

in the previous subsection can cause wrong labeling

(for example, errors in the knowledge base). We in-

troduce a parameter 0 ≤ dr ≤ 1 that covers such
factors. Finally, we define brs as

brs≡ar



λ



1−
S

∏

t=1,t 6=s

(1−mst)
zrt



+(1−λ) dr



, (2)

where 0 ≤ λ ≤ 1 is the hyperparameter that con-
trols how strongly brs is affected by the main label-
ing process explained in the previous subsection.

5.3 Likelihood

Given observation Xr, the likelihood of our model

is

P (Xr|θr, ar, dr, λ,M)

=
∑

Zr

P (Zr|θr) P (Xr|Zr, ar, dr, λ,M) ,

where

P (Zr|θr) =
S

∏

s=1

θzrsr (1− θr)
1−zrs .

For each pattern s, we define nrs as the number
of entity pairs to which relation r is assigned (i.e.,
nrs =

∑

i xrsi).

p (Xr|Zr, ar, dr, λ,M) =
S

∏

s=1

{

anrsr (1− ar)
Ns−nrs

}zrs

{

bnrsrs (1− brs)
Ns−nrs

}

1−zrs
, (3)

where brs is in Eq.2.

6 Learning

We learn parameters ar, θr, and dr and infer hidden
variables Zr by maximizing the log likelihood given

Xr. Estimated Zr is used to predict which patterns

express relation r.
To infer zrs, we would like to calculate the pos-

terior probability of zrs. However, this calculation
is intractable because each zrs depends on the oth-
ers, {(zrt)|t 6= s}, as shown in Eqs.2 and 3. This
prevents us from using the EM algorithm. Instead,

we apply variational approximation to the posterior

distribution by using the following trial distribution:

Q (Zr|Φr) =
S

∏

s=1

φzrsrs (1− φrs)
1−zrs ,

where 0 ≤ φrs ≤ 1 is a parameter for the trial dis-
tribution.

The following function Fr is a lower bound of the
log likelihood, and maximizing this function with

respect to Φr is equivalent to minimizing the KL di-
vergence between the trial distribution and the pos-

terior distribution of Zr.

Fr = EQ[log P (Zr,Xr|θr, ar, dr, λ,M)]

− EQ [log Q (Zr|Φr)] . (4)

EQ[•] represents the expectation over trial distribu-
tion Q. We maximize function Fr with respect to
the parameters instead of the log likelihood.

However, we need further approximation for two

terms on expanding Eq.4. Both of the terms are ex-

pressed as EQ[log(f(Zr))], where f(Zr) is a func-
tion of Zr. We apply the following approximation

(Asuncion et al., 2009).

EQ [log(f(Zr))] ≈ log (EQ [f(Zr)]) .

725



This is based on the Taylor series of log at
EQ[f(Zr)]. In our problem, since the second deriva-
tive is sufficiently small, we use the zeroth-order ap-

proximation.4

Our learning algorithm is derived by calculating

the stationary condition of the resultant evaluation

function with respect to each parameter. We have the

exact solution for θr. For each φrs and dr, we derive
a fixed point iteration. We update ar by using the
steepest ascent. We update each parameter in turn

while keeping the other parameters fixed. Parameter

updating proceeds until a termination condition is

met.

After learning, we have φrs for each pair of rela-
tion r and pattern s. The greater the value of φrs is,
the more likely it is that pattern s expresses relation
r. We set a threshold and determine zrs = 0 when
φrs is less than the threshold.

7 Experiments

We performed two sets of experiments.

Experiment 1 aimed to evaluate the performance of

our generative model itself, which predicts whether

a pattern expresses a relation, given a labeled corpus

created with the DS assumption.

Experiment 2 aimed to evaluate how much our

wrong label reduction in Section 4 improved the per-

formance of relation extraction. In our method, we

trained a classifier with a labeled corpus cleaned by

Algorithm 1 using the negative pattern list predicted

by the generative model.

7.1 Dataset

Following Mintz et al. (2009), we carried out our

experiments using Wikipedia as the target corpus

and Freebase (September, 2009, (Google, 2009)) as

the knowledge base. We used more than 1,300,000

Wikipedia articles in the wex dump data (September,

2009, (Metaweb Technologies, 2009)). The proper-

ties of our data are shown in Table 1.

In Wikipedia articles, named entities were iden-

tified by anchor text linking to another article and

starting with a capital letter (Yan et al., 2009). We

applied Open NLP POS tagger5 and MaltParser

(Nivre et al., 2007) to sentences containing more

4The first-order information becomes zero in this case.
5http://opennlp.sourceforge.net/

Table 1: Properties of Wikipedia dataset

documents 1,303,000

entity pairs 2,017,000

(matched to Freebase) 129,000

(with entity types) 913,000

frequent patterns 3,084

relations 24

than one named entity. We then extracted sentences

containing related entity pairs with the method ex-

plained in Section 3. To match entity pairs, we used

ID mapping between the dump data and Freebase.

We used the most frequent 24 relations.

7.2 Experiment 1: Pattern Prediction

We compared our model with baseline methods in

terms of ability to predict patterns that express a

given relation.

The input of this task was Xrs, which expresses

whether or not each entity pair appearing with each

pattern is labeled with relation r, as explained in
Section 5. In Experiment 1, since we needed entity

types for patterns, we restricted ourselves to entities

matched with Freebase, which also provides entity

types for entities. We used patterns that appear more

than 20 times in the corpus.

7.2.1 Evaluation

We split the data into training data and test data.

The training data was Xrs for 12 relations and the

test data was that for the remaining 12 relations. The

training data was used to calibrate parameters (see

the following subsection for details). The test data

was used for evaluation. We randomly split the data

five times and took the average of the following eval-

uation values.

We evaluated the performance by precision, re-

call, and F value. They were calculated using gold

standard data, which was constructed by hand. We

manually selected patterns that actually express a

target relation as positive patterns for the relation. 6

We averaged the evaluation values in terms of macro

average over relations before averaging over the data

splits.

6Patterns that ambiguously express the relation, for instance

“[Person] in [Location]” for place of birth, were not selected as

positive patterns.

726



Table 2: Averages of precision, recall, and F value in Ex-

periment 1. The averages of threshold of RS(rank) and

RS(value) were 6.2 ± 3.2 and 0.10 ± 0.06, respectively.
The averages of hyperparameters of PROP were 0.84 ±
0.05 for λ and 0.85 ± 0.10 for the threshold.

Precision Recall F value

Baseline 0.339 1.000 0.458

RS(rank) 0.749 0.549 0.467

RS(value) 0.601 0.647 0.545

PROP 0.782 0.688 0.667

7.2.2 Methods

We compared the following methods:

Baseline: This method assigns relation r to a pat-
tern when the pattern is mentioned with at least one

entity pair corresponding to relation r in Freebase.
This method is based on the DS assumption.

Ratio-based Selection (RS): Given relation r and
pattern s, this method calculates nrs/Ns, which is
the ratio of the number of labeled entity pairs ap-

pearing with pattern s to the number of entity pairs
including unlabeled ones. RS then selects the top

n patterns (RS(rank)). We also tested a version us-
ing a real-valued threshold (RS(value)). In train-

ing, we selected the threshold that maximized the

F value. Some bootstrapping approaches (Carlson et

al., 2010) use a rank-based threshold like RS(rank).

Proposed Model (PROP): Using the training data,

we determined the two hyperparameters, λ and the
threshold to round φrs to 1 or 0, so that they max-
imized the F value. When φrs is greater than the
threshold, we select pattern s as one expressing re-
lation r.

7.2.3 Result and Discussion

The results of Experiment 1 are shown in Table 2.

Our model achieved the best precision, recall, and F

value. RS(value) had the second best F value, but it

completely removed more than one infrequent rela-

tion on average in test sets. This is problematic for

real situations. RS(rank) achieved the second high-

est precision. However, its recall, which is also im-

portant in our task, was the lowest and its F value

was almost the same as naive Baseline.

The thresholds of RS, which directly affect their

performance, should be calibrated for each relation,

but it is hard to do this in advance. On the other

Table 3: Example of estimated φrs for r =
place of birth . Entity types are omitted in patterns.

nrs/Ns is the ratio of the number of labeled entity pairs
to the number of entity pairs appearing with pattern s.

pattern s nrs/Ns φrs expresses r?
born in 0.512 0.999 true

actor from 0.480 0.999 true

elected Mayor of 0.384 0.855 false

family moved from 0.344 0.055 false

native of 0.327 0.999 true

grew in 0.162 0.000 false

hand, our model learns parameters such as ar for
each relation and thus the hyperparameter of our

model does not directly affect its performance. This

results in a high prediction performance.

Examples of estimated φrs, the probability with
which pattern s expresses relation r, are shown in
Table 3. The pattern, “[Person] family moved from

[Location]”, which does not express place of birth,

had low φrs in spite of having higher nrs/Ns than
the valid pattern “[Person] native of [Location]”.

The former pattern had higher brs, the probability
with which relation r is wrongly assigned to pat-
tern s via entity pairs, because there were more en-
tity pairs that appeared not only with this pattern

but also with patterns that was predicted to express

place of birth.

7.3 Experiment 2: Relation Extraction

We investigated the performance of relation extrac-

tion using our wrong label reduction, which uses the

results of the pattern prediction.

Following Mintz et al. (2009), we performed an

automatic held-out evaluation and a manual evalu-

ation. In both cases, we used 400,000 articles for

testing and the remaining 903,000 for training.

7.3.1 Configuration of Classifiers

Following Mintz et al. (2009), we used a multi-

class logistic classifier optimized using L-BFGS

with Gaussian regularization to classify entity pairs

to the predefined 24 relations and NONE. In order to

train the NONE class, we randomly picked 100,000

examples that did not match to Freebase as pairs.

(Several entities in the examples matched and had

entity types of Freebase.) In this experiment, we

727





















      









 

¡

¢£¤¥¦

Figure 4: Precision-recall curves in held-out evaluation.

Precision is reported at recall levels from 5 to 50,000.

used not only entity pairs matched to Freebase but

also ones not matched to Freebase (i.e., entity pairs

that do not have entity types). We used syntactic

features (i.e., features obtained from the dependency

parse tree of a sentence) and lexical features, and en-

tity types, which essentially correspond to the ones

developed by Mintz et al. (2009).

We compared the following methods: logistic re-

gression with the labeled data cleaned by the pro-

posed method (PROP), logistic regression with the

standard DS labeled data (LR), and MultiR proposed

in (Hoffmann et al., 2011) as a state-of-the-art multi-

instance learning system.7 For logistic regression,

when more than one relation is assigned to a sen-

tence, we simply copied the feature vector and cre-

ated a training example for each relation. In PROP,

we used training articles for pattern prediction.8

7.3.2 Held-out Evaluation

In the held-out evaluation, relation instances dis-

covered from testing articles were automatically

compared with those in Freebase. This let us calcu-

late the precision of each method for the best n re-
lation instances. The precisions are underestimated

because this evaluation suffers from false negatives

due to the incompleteness of Freebase. We changed

n from 5 to 50,000 and measured precision and re-
call. Precision-recall curves for the held-out data are

7For MultiR, we used the authors’ implementation from

http://www.cs.washington.edu/homes/raphaelh/mr/
8In Experiment 2 we set λ = 0.85 and the threshold at 0.95.

Table 4: Averages of precisions at 50 for the most fre-

quent 15 relations as well as example relations.

PROP MultiR LR

place of birth 1.0 1.0 0.56

place of death 1.0 0.7 0.84

average 0.89±0.14 0.83±0.21 0.82±0.23

shown in Figure 4.

PROP achieved comparable or higher precision at

most recall levels compared with LR and MultiR. Its

performance at n = 50,000 is much higher than that
of the others. While our generative model does not

use unlabeled examples as negative ones in detecting

wrong labels, classifier-based approaches including

MultiR do, suffering from false negatives.

7.3.3 Manual Evaluation

For manual evaluation, we picked the top ranked

50 relation instances for the most frequent 15 rela-

tions. The manually evaluated precisions averaged

over the 15 relations are shown in table 4.

PROP achieved the best average precision. For

place of birth, LR wrongly extracted entity pairs

with “[Person] played with club [Location]”, which

does not express the relation. PROP and MultiR

avoided this mistake. For place of death, LR and

MultiR wrongly extracted entity pairs with “[Per-

son] moved to [Location]”. Multi-instance learning

does not work for wrong labels assigned to entity

pairs that appear only once in a corpus. In fact, 72%

of entity pairs that appeared with this pattern and

were wrongly labeled as place of death appeared

only once in the corpus. Only PROP avoided mis-

takes of this kind because our method works in such

situations.

8 Conclusion

We proposed a method that reduces the number of

wrong labels created with the DS assumption, which

is widely applied. Our generative model directly

models the labeling process of DS and predicts pat-

terns that are wrongly labeled with a relation. The

predicted patterns are used for wrong label reduc-

tion. The experimental results show that this method

successfully reduced the number of wrong labels

and boosted the performance of relation extraction.

728



References

Arthur Asuncion, Max Welling, Padhraic Smyth, and

Yee W. Teh. 2009. On smoothing and inference

for topic models. In Proceedings of the 25th Con-

ference on Uncertainty in Artificial Intelligence (UAI

’09), pages 27–34.

Michele Banko and Oren Etzioni. 2008. The tradeoffs

between open and traditional relation extraction. In

Proceedings of the 46th Annual Meeting of the Asso-

ciation for Computational Linguistics: Human Lan-

guage Technologies (ACL-HLT ’08), pages 28–36.

Michele Banko, Michael J Cafarella, Stephen Soderl,

Matt Broadhead, and Oren Etzioni. 2007. Open in-

formation extraction from the web. In Proceedings of

the International Joint Conferences on Artificial Intel-

ligence (IJCAI ’07), pages 2670–2676.

Kedar Bellare and Andrew McCallum. 2007. Learn-

ing Extractors from Unlabeled Text using Relevant

Databases. In Sixth International Workshop on Infor-

mation Integration on the Web (IIWeb ’07).

Andrew Carlson, Justin Betteridge, Richard C. Wang, Es-

tevam R. Hruschka Jr., and Tom M. Mitchell. 2010.

Coupled semi-supervised learning for information ex-

traction. In Proceedings of the 3rd ACM International

Conference on Web Search and Data Mining (WSDM

’10), pages 101–110.

Anthony Fader, Stephen Soderland, and Oren Etzioni.

2011. Identifying relations for open information ex-

traction. In Proceedings of the 2011 Conference on

Empirical Methods in Natural Language Processing

(EMNLP ’11), pages 1535–1545.

Google. 2009. Freebase data dumps. http://

download.freebase.com/datadumps/.

Raphael Hoffmann, Congle Zhang, and Daniel S. Weld.

2010. Learning 5000 relational extractors. In Pro-

ceedings of the 48th Annual Meeting of the Associa-

tion for Computational Linguistics (ACL ’10), pages

286–295.

Raphael Hoffmann, Congle Zhang, Xiao Ling, Luke

Zettlemoyer, and Daniel S. Weld. 2011. Knowledge-

based weak supervision for information extraction of

overlapping relations. In Proceedings of the 49th An-

nual Meeting of the Association for Computational

Linguistics: Human Language Technologies (ACL-

HLT ’11), pages 541–550.

Mamoru Komachi, Taku Kudo, Masashi Shimbo, and

Yuji Matsumoto. 2008. Graph-based analysis of se-

mantic drift in Espresso-like bootstrapping algorithms.

In Proceedings of the 2008 Conference on Empirical

Methods in Natural Language Processing (EMNLP

’08), pages 1011–1020.

Metaweb Technologies. 2009. Freebase wikipedia ex-

traction (wex). http://download.freebase.

com/wex/.

Mike Mintz, Steven Bills, Rion Snow, and Daniel Juraf-

sky. 2009. Distant supervision for relation extraction

without labeled data. In Proceedings of the Joint Con-

ference of the 47th Annual Meeting of the ACL and

the 4th International Joint Conference on Natural Lan-

guage Processing of the AFNLP (ACL-IJCNLP ’09),

pages 1003–1011.

Joakim Nivre, Johan Hall, and Jens Nilsson. 2007.

Maltparser: A language-independent system for data-

driven dependency parsing. Natural Language Engi-

neering, 37:95–135.

Chris Pal, Gideon Mann, and Richard Minerich. 2007.

Putting semantic information extraction on the map:

Noisy label models for fact extraction. In Sixth Inter-

national Workshop on Information Integration on the

Web (IIWeb ’07).

Patrick Pantel and Marco Pennacchiotti. 2006. Espresso:

Leveraging generic patterns for automatically harvest-

ing semantic relations. In Proceedings of the 21st

International Conference on Computational Linguis-

tics and 44th Annual Meeting of the Association for

Computational Linguistics (COLING-ACL ’06), pages

113–120.

Sebastian Riedel, Limin Yao, and Andrew McCallum.

2010. Modeling relations and their mentions without

labeled text. In In Proceedings of the European Con-

ference on Machine Learning and Knowledge Discov-

ery in Databases (ECML-PKDD ’10), pages 148–163.

Ellen Riloff and Rosie Jones. 1999. Learning dictionar-

ies for information extraction by multi-level bootstrap-

ping. In AAAI/IAAI, pages 474–479.

Chang Wang, James Fan, Aditya Kalyanpur, and David

Gondek. 2011. Relation extraction with relation

topics. In Proceedings of the 2011 Conference on

Empirical Methods in Natural Language Processing

(EMNLP ’11), pages 1426–1436.

Fei Wu and Daniel S. Weld. 2007. Autonomously se-

mantifying wikipedia. In Proceedings of the 16th

ACM Conference on Conference on Information and

Knowledge Management (CIKM ’07), pages 41–50.

Yulan Yan, Naoaki Okazaki, Yutaka Matsuo, Zhenglu

Yang, and Mitsuru Ishizuka. 2009. Unsupervised re-

lation extraction by mining wikipedia texts using in-

formation from the web. In Proceedings of the Joint

Conference of the 47th Annual Meeting of the ACL

and the 4th International Joint Conference on Natu-

ral Language Processing of the AFNLP (ACL-IJCNLP

’09), pages 1021–1029.

Limin Yao, Sebastian Riedel, and Andrew McCallum.

2010. Collective cross-document relation extraction

without labelled data. In Proceedings of the 2010 Con-

ference on Empirical Methods in Natural Language

Processing (EMNLP ’10), pages 1013–1023.

729


