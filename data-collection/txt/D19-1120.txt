



















































A Multilingual Topic Model for Learning Weighted Topic Links Across Corpora with Low Comparability


Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing
and the 9th International Joint Conference on Natural Language Processing, pages 1243–1248,
Hong Kong, China, November 3–7, 2019. c©2019 Association for Computational Linguistics

1243

A Multilingual Topic Model for
Learning Weighted Topic Links Across Corpora with Low Comparability

Weiwei Yang∗
Computer Science

University of Maryland
wwyang@cs.umd.edu

Jordan Boyd-Graber†
Computer Science, iSchool,
Language Science, UMIACS

University of Maryland
jbg@umiacs.umd.edu

Philip Resnik
Linguistics and UMIACS

University of Maryland
resnik@umd.edu

Abstract

Multilingual topic models (MTMs) learn top-
ics on documents in multiple languages. Past
models align topics across languages by im-
plicitly assuming the documents in different
languages are highly comparable, often a false
assumption. We introduce a new model that
does not rely on this assumption, particularly
useful in important low-resource language sce-
narios. Our MTM learns weighted topic links
and connects cross-lingual topics only when
the dominant words defining them are sim-
ilar, outperforming LDA and previous MTMs
in classification tasks using documents’ topic
posteriors as features. It also learns coherent
topics on documents with low comparability.

1 Introduction

Topic models explain document collections at a
high level (Boyd-Graber et al., 2017). Multi-
lingual topic models (MTMs) uncover latent top-
ics across languages and reveal commonalities
and differences across languages and cultures (Ni
et al., 2009; Shi et al., 2016; Gutiérrez et al.,
2016). Existing models extend latent Dirichlet al-
location (Blei et al., 2003, LDA) and learn aligned
topics across languages (Mimno et al., 2009).

Prior models work well because they implicitly
assume—even if not part of the model—parallel or
highly comparable data with well-aligned topics.
However, this assumption does not always com-
port with reality. Even documents from the same
place and time can discuss very different things
across languages: in multicultural London, Hindi
tweets focus on a Bollywood actor’s BBC appear-
ance, French blogs fret about Brexit, and English
articles focus on Tottenham’s lineup. Generally,
corpora have a range of “nonparallelness” (Fung,
2000). In less comparable settings, while some

∗Now at Facebook
† Now at Google AI Zürich

EN-1 EN-2 EN-3 EN-4

ZH-1 ZH-2 ZH-3 ZH-4

universities

schools

students

research

science

economics

dollars

million

invest

income

technology

information

computers

smart

system

sports

match

referee

tournament

champion

°_(technology)

�(science)

ÑÇj(computer)

ª-(smart)

+�(system)

#(music)

CÁ(album)

|{(singer)

ñ(works)

DaJ(concert)

�Ø(sports)

�(match)

ñT(referee)

V7(tournament)

ÐË(champion)

ÿ~(economics)

¾s(dollars)

®7(million)

Åt(invest)

f(income)

Figure 1: Topic pairs with many word translation pairs
have high link weights, e.g., (EN-1, ZH-3) and (EN-2,
ZH-4); topic pairs with partial overlap receive lower
weights, e.g., (EN-4, ZH-1); a topic is unlinked if there
is no corresponding topic in the other language (ZH-2).

topics are shared, languages’ emphasis may di-
verge and some topics may lack analogs.

We therefore introduce a new multilingual topic
model that assumes each language has its own
topic sets and jointly learns all topics, but does not
force one-to-one alignment across languages. In-
stead, our MTM learns weighted topic links across
languages and only assigns a high link weight to
a topic pair whose top words have many direct
translation pairs (Figure 1). Moreover, it allows
unlinked topics if there is no matching topic in
the other language. This makes the model robust
for (more common) less-comparable data with
topic misalignment. Joint inference also allows
insights from high-resource languages to uncover
low-resource language patterns. It is particularly
useful in scenarios that involve modeling topics
on low-resource languages in humanitarian assis-
tance, peacekeeping, and/or infectious disease re-
sponse, while limiting the additional cost to other
steps that will also need to be taken, such as find-
ing or creating a word translation dictionary.

We validate the MTM in two classification tasks
using inferred topic posteriors as features. Our



1244

MTM has higher F1 than other models in both
intra- and cross-lingual evaluations, while discov-
ering coherent topics and meaningful topic links.

2 Multilingual Topic Model for
Connecting Cross-Lingual Topics

Yang et al. (2015) present a flexible framework for
adding regularization to topic models. We extend
this model to the multilingual setting by adding
a potential function that links topics across lan-
guages. For simplicity of exposition, we focus on
the bilingual case with languages S and T .

Unlike Yang et al. (2015) that encode monolin-
gual information only, our potential function en-
codes multilingual knowledge parameterized by
two matrices, ρS→T and ρT→S , that transform
topics between the two languages. Cells’ values
are between 0 and 1 and a cell ρS→T,kT ,kS close to
one is a strong connection of topics kT and kS in
language T and S. Transformations ρ are learned
from translation pairs’ topic distributions.

These topic distributions come from the assign-
ments of Gibbs sampling (Griffiths and Steyvers,
2004). Fortunately adding the potential function is
equivalent to adding an additional term to Gibbs
sampling for topic models (Yang et al., 2015).
During sampling, each token is assigned to a topic,
so we can compute a post hoc word distribution
over topics. The probability of a topic k given
a word w is Pr (k |w) ≡ Ωw,k ≡ Nk,w/Nw,
where Nk,w is the number of times that word w is
assigned to topic k and Nw is w’s term frequency.

To find good topic links ρS→T , we use a
dictionary. For instance, given the translation
pair of “sports” and “运动 (yùn dòng)”, they
should have similar topic distributions, so we
want ρEN→ZHΩsports to be close to Ω运动 and vice
versa. Moreover, the transformations should be
symmetric: ρS→TΩwS close to ΩwT , and vice
versa. We encode this cross-lingual knowledge of
topic transformations into the potential function Ψ
which measures the difference of translation pairs’
topic distributions after transformation:(

C∏
c=1

‖ΩS,c − ρT→SΩT,c‖ηc2 ‖ρS→TΩS,c −ΩT,c‖
ηc
2

)−1
,

(1)

where ηc is the statistical importance of the c-th
translation pair to the corpus (Figure 2, full details
in the Supplement).

While Yang et al. (2015) provide a blueprint for
Gibbs sampling with potential functions without

T
DSD

T
M

S
M

S
E

dT
N

,

T
D

T
KSK

dS
N

,

S
D

dT ,
T

ndT
z

,,

ndT
w

,,

dS ,
T

ndS
z

,,

ndS
w

,,

T
E<

SToU

TSoU

Figure 2: The graphical model of our multilingual topic
model. The topic links ρ, as instantiated by the func-
tion Ψ, encourage topics to encourage word transla-
tions to have consistent topics.

additional parameters, our model has additional
parameters of ρS→T and ρT→S so we need to op-
timize them. Thus, we use stochastic EM (Celeux,
1985). The E-step updates tokens’ topic assign-
ments using Gibbs sampling, while holding the pa-
rameters of the topic link weight matrices ρ fixed.
The M-step optimizes ρ while holding the topic
assignments fixed. We optimize Ψ in log space
using the objective function J(ρS→T ) as

C∑
c=1

ηc log ‖ΩT,c − ρS→T,iT ΩS,c‖2 , (2)

which is minimized by using L-BFGS (Liu and No-
cedal, 1989), with the partial derivatives with re-
spect to ρS→T,kT ,kS

−
C∑
c=1

ηcΩS,c,kS (ΩT,c,kT − ρS→T,kT ΩS,c)
‖ΩT,c − ρS→T,iT ΩS,c‖

2
2

. (3)

3 Experiments

We evaluate our model extrinsically on classifica-
tion tasks, followed by intrinsic topic coherence.

3.1 Classification with Topic Posteriors
We use two datasets for classification: Wikipedia
documents in English (EN) and Chinese
(ZH) (Yuan et al., 2018) and an English-Sinhalese
(SI) disaster response dataset (Strassel and Tracey,
2016).1 Each dataset provides labeled documents
and a dictionary. Yuan et al. (2018) extract the
EN-ZH dictionary from MDBG, while Strassel and
Tracey (2016) construct the EN-SI dictionary from
online resources and manual annotation.2 Each

1More dataset details in the Supplement.
2MDBG: https://www.mdbg.net/chinese/

dictionary?page=cc-cedict

https://www.mdbg.net/chinese/dictionary?page=cc-cedict
https://www.mdbg.net/chinese/dictionary?page=cc-cedict


1245

MTM+TF-IDF+TOP
MTM+TF-IDF

MTM+TOP
MTM

ptLDA
LDA

MTAnchor
MCTA

26.7
26.7

42.9
42.9

12.8
27.8

20.8
13.0
Intra-Lingual

14.5
14.5

35.3
22.2

16.0
22.9
24.5

4.1
Cross-Lingual

English

0 20 40 60

MTM+TF-IDF+TOP
MTM+TF-IDF

MTM+TOP
MTM

ptLDA
LDA

MTAnchor
MCTA

38.1
38.1

23.1
23.1

18.2
24.0

32.6
26.5

0 20 40 60
11.4
15.1

33.3
26.7

15.1
21.1
24.7

15.6

Sinhalese

MTM+TF-IDF+TOP
MTM+TF-IDF

MTM+TOP
MTM

ptLDA
LDA

MTAnchor
MCTA

94.1
94.1
93.0
93.0
91.6
92.1

80.7
51.6

Intra-Lingual

63.2
57.3

78.1
74.7

2.9
16.5

57.6
23.2
Cross-Lingual

English

0 25 50 75 100

MTM+TF-IDF+TOP
MTM+TF-IDF

MTM+TOP
MTM

ptLDA
LDA

MTAnchor
MCTA

85.6
85.6
86.5
86.5
83.3
83.4

75.3
33.4

0 25 50 75 100
59.6

55.1
83.1

64.5
21.0

10.5
54.5

39.8

Chinese

Figure 3: The F1 scores on disaster response (upper)
and Wikipedia (lower) datasets. Our MTM outperforms
all the baselines in intra- and cross-lingual evaluations.

Wikipedia document is labeled with one of the
topics of film, music, animals, politics, religion,
and food. A portion of the disaster response
documents are labeled with one of eight types of
needed rescue resources: evacuation, food supply,
search/rescue, utilities, infrastructure, medical
assistance, shelter, and water supply.

We follow Yuan et al. (2018) for preprocess-
ing (such as lemmatization for English and seg-
mentation for Chinese) and use a linear SVM for
classification. For the Wikipedia dataset, we re-
port micro-F1 scores on a six-way classification.
For the disaster response dataset, our goal is bi-
nary classification of the need for evacuation ver-
sus other assistance. The classification uses fea-
tures of topic posteriors: Pr (k | d) ≡ Nd,k/Nd

which is the proportion of the tokens assigned to
topic k in document d.

The baselines include polylingual tree LDA (Hu
et al., 2014, ptLDA) which encodes the dictio-
nary as a tree prior (Andrzejewski et al., 2009),
Multilingual Topic Anchoring (Yuan et al., 2018,
MTAnchor), and Multilingual Cultural-common
Topic Analysis (Shi et al., 2016, MCTA). We
also include LDA, which runs monolingually in
each language. We use 20 topics and set hyper-
parameters α = 0.1 and β = 0.01 (if applicable).

Our evaluations are both intra- and cross-
lingual. The intra-lingual evaluation trains and
tests classifiers on the same language, while the
cross-lingual evaluation trains classifiers on one
language and tests on another. In cross-lingual
evaluations, MTAnchor, MCTA, and ptLDA align
topic spaces, so topic posterior transformation is
not necessary. LDA cannot transform topic spaces,
so we do not apply any transformation. For our
MTM, we explore two transformation methods
with ρ. The first multiplies ρ with a language’s
document topic distributions, i.e., ρZH→ENθZH and
vice versa. The second (TOP), transfers each doc-
ument topic’s probability mass to the topic in the
other language with the highest link weight.3

Our MTM has higher F1 both intra- and cross-
lingually (Figure 3). TF-IDF weighting on trans-
lation pairs sometimes improves the intra-lingual
F1, although it hurts the cross-lingual F1. Con-
necting the top linked topics (TOP) is better than
directly using the topic link weight matrices. This
indicates that ρ’s values have some noise.

3.2 Looking at Learned Topics
Past MTMs align topics across languages but our
MTM does not, so we compare the topics across
models to see how they differ. We look at the
Movies topics from the Wikipedia dataset (Ta-
ble 1). For the Chinese MTM topics, we show the
three English topics with the highest link weights.

The topics are about Movies, but the MCTA and
MTAnchor topics do not rank “movie” or “电影
(diàn yı̌ng)” at the top. The ptLDA topics, al-
though aligned well, incorrectly align some Chi-
nese words. “胶片 (jiāo piàn)” means “photo-
graphic film”, while “释放 (shı̀ fàng)” means re-
lease as in “let something go”, not movie distri-
bution. ptLDA links words based on translations

3An example of TOP is available in the Supplement.
4In Tables 1 and 2, “[Q]” denotes the Chinese word is a

counter for the following English word.



1246

Lang. Words
MCTA

ZH
主演 (starring),改编 (adapt),本 (this),
小说 (novel),拍摄 (shoot),角色 (role)

EN dog, san, movie, mexican, fighter, novel
MTAnchor

ZH
主演 (starring),改编 (adapt),饰演 (act),
本片 (the movie),演员 (actor),编剧 (playwright)

EN kong, hong, movie, official, martial, box
LDA

ZH
电影 (movie),部 ([Q] movie),4 美国 (USA),
上映 (release),英语 (English),剧情 (plot)

EN film, star, direct, release, action, plot
ptLDA

ZH
电影 (movie),胶片 (film),星 (star),
动作 (action),释放 (release),影片 (movie)

EN film, star, direct, action, release, plot
MTM

ZH
电影 (movie),部 ([Q] movie),上映 (release),
动画 (animation),故事 (story),作品 (works),

EN (.20) film, direct, star, release, action, plot
EN (.12) kill, find, death, attack, escape, return
EN (.11) shrine, japanese, temple, japan, shinto, kami

MTM + TF-IDF

ZH
电影 (movie),部 ([Q] movie),上映 (release),
美国 (USA),英语 (English),导演 (director)

EN (.32) film, direct, star, action, release, plot
EN (.24) film, kill, find, escape, attack, return
EN (.09) character, series, star, game, trek, create

Table 1: The Movies topics given by models. For the
Chinese (ZH) topics given by MTM, the top three En-
glish (EN) topics and their link weights are also given.

without looking at the context, which causes prob-
lems with multiple-sense words. The LDA and
MTM topics are generally coherent.

The MTM’s unique joint modeling of weighted
topic links also recovers additional topical struc-
ture: after linking respective EN-ZH Movies top-
ics, the next linked topics are Action Movies
(“kill”, “death”, “attack”, and “escape”). Fur-
ther, the models capture a degree of connection be-
tween Movies and Computer Games (MTM + TF-
IDF) and Japanese Animations (MTM).

3.3 Looking at Learned Topic Links

We give more examples of weighted MTM topic
links in Table 2. High-weighted Biology (ZH-0,
EN-12, and EN-19) and Music topics (EN-10, ZH-
9, and ZH-17) are characterized by cross-lingual
words in common. The model can also infer topic
links beyond words, linking topics when the top-
ical words have few direct translations but are re-
lated in senses. For instance, ZH-14 is about the
“campaigns” against “government”. Only “gov-
ernment” overlaps with EN-16 and EN-11, but

Lang. Words

ZH-0 学名 (scientific name),它们 (they),呈 (show),
白色 (white),长 (long),黑色 (black)

EN-12 (.57) specie, bird, eagle, genus, white, owl
EN-19 (.13) breed, chicken, white, goose, bird, black

ZH-14
主义 (-ism),组织 (organization),
美国 (USA),革命 (revolution),
运动 (campaign),政府 (government)

EN-16 (.32) sex, law, act, sexual, marriage, court
EN-11 (.17) traffic, victim, government, trafficking, child, force
EN-10 album, release, record, music, song, single

ZH-9 (.30) 专辑 (album),张 ([Q] album),发行 (release),
音乐 (music),首 ([Q] song),唱片 (record)

ZH-17 (.20) 音乐 (music),乐团 (musical group),艺术 (art),
创作 (create),奖 (award),演出 (perform)

Table 2: Topics are linked because they have overlap in
topical words. Our MTM can also infer the topic rela-
tions beyond words, e.g., ZH-14 and EN-16.

1.3
1.4
1.5

To
pi

c 
Co

he
re

nc
e

INCO
LDA ptLDA MTM MTM-TFIDF

PACO

Arabic

1.2
1.3
1.4
1.5 Chinese

1.3
1.4
1.5
1.6
1.7

Farsi

0.8
1

1.2 Russian

25 50 75 100
1.1
1.2
1.3
1.4

25 50 75 100
Top Words

Spanish

Figure 4: Topic coherence on INCO and PACO datasets
with the number of top words in each topic.

MTM identifies the two English topics as the top
linked topics for ZH-14: EN-16 is about the “cam-
paign” in Sexual Rights, while EN-11 is about the
Crime of human trafficking. This shows that our
MTM can incorporate word translations and infer
more cross-lingual word and topic relationships.

3.4 Evaluating Topic Coherence

We intrinsically evaluate models’ topic coherence
on two sets of preprocessed bilingual Wikipedia
corpora (Hao and Paul, 2018) that vary in “non-



1247

parallelness”. Both pair English with Arabic,
Chinese, Spanish, Farsi, and Russian. In PACO,
30% of documents have direct translations across
languages, and in INCO none has direct trans-
lations. Dictionaries are extracted fromn Wik-
tionary.5 Standard preprocessing has already been
applied to the datasets, including stemming, stop
word removal, and high-frequency word removal.

We use an intra-lingual metric to evaluate topic
coherence (Lau et al., 2014): for every topic,
we compute its top N words’ average pairwise
PMI score on a disjoint subset of Wikipedia doc-
uments (Hao and Paul, 2018). We report average
coherence with N from 10 to 100 with a step size
of 10 (five-fold cross-validation). We use the same
translation pair weighting options as in our classi-
fication tasks and also compare against monolin-
gual LDA and ptLDA (Hu et al., 2014).

MTM is no worse than LDA and sometimes
slightly better (Figure 4). TF-IDF weighting on
translation pairs sometimes further improves co-
herence (e.g., Arabic, Farsi, Russian, and Spanish
on INCO) but occasionally hurts (e.g., Chinese).
ptLDA mostly works poorly, except on Farsi with
a high number of top words. ptLDA aligns topic
spaces, which is hard for low-comparability data,
thus sacrificing coherence for alignment; in con-
trast MTM only connects topics when they align
well in senses.

3.5 Topic Coherence vs. Target Language
Corpora Sizes

We next vary the size of target language (non-
English languages in PACO and INCO) corpora:
how much can MTM help topic coherence for low-
resource languages? We start from 10% of the
randomly-selected documents in target languages
and incrementally add more target language docu-
ments at a step size of 10% until it reaches 100%.
Meanwhile, we always use 100% of the English
documents. We train monolingual LDA, ptLDA,
and MTMs with and without TF-IDF weighting on
translation pairs on each setting and evaluate the
topic coherence on the same reference corpora us-
ing the top thirty words of each topic (Figure 5).

In most cases, the topic coherence improves
with larger target corpora, except Arabic and Rus-
sian on PACO. This confirms our intuition that
more data yield a better topic model. MTM is help-

5https://dumps.wikimedia.org/
enwiktionary/

1.30
1.35
1.40
1.45
1.50

To
pi

c 
Co

he
re

nc
e

INCO
LDA tLDA MTM MTM-TFIDF

PACO
Arabic

1.2
1.3
1.4

Chinese

1.3
1.4
1.5
1.6

Farsi

0.8
0.9

1
1.1
1.2 Russian

25 50 75 100
1.16
1.20
1.24
1.28
1.32

25 50 75 100
Target Language Corpora Size Ratios

Spanish

Figure 5: The models’ topic coherence on INCO and
PACO datasets when the sizes of target language cor-
pora grow from 10% to 100%, with a step size of 10%.

ful in cases when the target language corpora sizes
are small, e.g., Chinese and Russian with 10% or
20% of the corpora. TF-IDF weighting is not con-
sistently better or worse than equal weights.

The ptLDA with tree priors based on dictionaries
performs poorly in topic coherence, except Farsi
in INCO. In most cases, its topic coherence is sub-
stantially below others’ and improves little when
the target corpora grow.

4 Conclusions and Future Work

We introduce a novel multilingual topic model
(MTM) that learns weighted topic links across lan-
guages by minimizing the Euclidean distances of
translation pairs’ (transformed) topic distributions,
where translation pairs can be weighted, e.g., by
TF-IDF. This connects topics in different lan-
guages only when necessary and is more robust
on low-comparability corpora. The MTM outper-
forms baselines substantially in both intra- and
cross-lingual classification tasks, while achieving
no worse or slightly better topic coherence than
monolingual LDA on low-comparability data.

We plan to explore weighting methods to better
evaluate the importance of translation pairs. We
will also study how to improve topic transforma-
tion with the topic link weight matrices.

https://dumps.wikimedia.org/enwiktionary/
https://dumps.wikimedia.org/enwiktionary/


1248

Acknowledgements

We thank Shudong Hao and Michelle Yuan for
providing their datasets. We thank the anony-
mous reviewers for their insightful and construc-
tive comments. This research has been sup-
ported under subcontract to Raytheon BBN Tech-
nologies, by DARPA award HR0011-15-C-0113.
Boyd-Graber is also supported by NSF grant IIS-
1409287. Any opinions, findings, conclusions, or
recommendations expressed here are those of the
authors and do not necessarily reflect the view of
the sponsors.

References
David Andrzejewski, Xiaojin Zhu, and Mark Craven.

2009. Incorporating domain knowledge into topic
modeling via Dirichlet forest priors. In Proceedings
of the International Conference of Machine Learn-
ing.

David M. Blei, Andrew Y. Ng, and Michael I. Jordan.
2003. Latent Dirichlet allocation. Journal of Ma-
chine Learning Research, pages 993–1022.

Jordan Boyd-Graber, Yuening Hu, and David Mimno.
2017. Applications of Topic Models, volume 11 of
Foundations and Trends in Information Retrieval.
NOW Publishers.

Gilles Celeux. 1985. The SEM algorithm: A proba-
bilistic teacher algorithm derived from the EM al-
gorithm for the mixture problem. Computational
Statistics Quarterly, pages 73–82.

Pascale Fung. 2000. A statistical view on bilingual lex-
icon extraction. In Parallel Text Processing, pages
219–236.

Thomas L. Griffiths and Mark Steyvers. 2004. Find-
ing scientific topics. Proceedings of the National
Academy of Sciences, pages 5228–5235.

E. Dario Gutiérrez, Ekaterina Shutova, Patricia Licht-
enstein, Gerard de Melo, and Luca Gilardi. 2016.
Detecting cross-cultural differences using a multi-
lingual topic model. Transactions of the Association
for Computational Linguistics, pages 47–60.

Shudong Hao and Michael J. Paul. 2018. Learning
multilingual topics from incomparable corpora. In
Proceedings of International Conference on Compu-
tational Linguistics.

Yuening Hu, Ke Zhai, Vlad Eidelman, and Jordan
Boyd-Graber. 2014. Polylingual tree-based topic
models for translation domain adaptation. In Pro-
ceedings of the Association for Computational Lin-
guistics.

Jey Han Lau, David Newman, and Timothy Baldwin.
2014. Machine reading tea leaves: Automatically
evaluating topic coherence and topic model quality.
In Proceedings of the Association for Computational
Linguistics.

Dong C. Liu and Jorge Nocedal. 1989. On the limited
memory BFGS method for large scale optimization.
Mathematical Programming, pages 503–528.

David Mimno, Hanna M. Wallach, Jason Naradowsky,
David A. Smith, and Andrew McCallum. 2009.
Polylingual topic models. In Proceedings of Empir-
ical Methods in Natural Language Processing.

Xiaochuan Ni, Jian-Tao Sun, Jian Hu, and Zheng Chen.
2009. Mining multilingual topics from Wikipedia.
In Proceedings of the World Wide Web Conference.

Bei Shi, Wai Lam, Lidong Bing, and Yinqing Xu. 2016.
Detecting common discussion topics across culture
from news reader comments. In Proceedings of the
Association for Computational Linguistics.

Stephanie M. Strassel and Jennifer Tracey. 2016.
LORELEI language packs: Data, tools, and re-
sources for technology development in low resource
languages. In Proceedings of the Language Re-
sources and Evaluation Conference.

Yi Yang, Doug Downey, and Jordan Boyd-Graber.
2015. Efficient methods for incorporating knowl-
edge into topic models. In Proceedings of Empirical
Methods in Natural Language Processing.

Michelle Yuan, Benjamin Van Durme, and Jordan
Boyd-Graber. 2018. Multilingual anchoring: In-
teractive topic modeling and alignment across lan-
guages. In Proceedings of Advances in Neural In-
formation Processing Systems.

http://www.nowpublishers.com/article/Details/INR-030

