



















































BREAKING! Presenting Fake News Corpus for Automated Fact Checking


Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics: Student Research Workshop, pages 357–362
Florence, Italy, July 28 - August 2, 2019. c©2019 Association for Computational Linguistics

357

BREAKING! Presenting Fake News Corpus For Automated Fact
Checking

Archita Pathak
University at Buffalo (SUNY)

New York, USA
architap@buffalo.edu

Rohini K. Srihari
University at Buffalo (SUNY)

New York, USA
rohini@buffalo.edu

Abstract

Popular fake news articles spread faster than
mainstream articles on the same topic which
renders manual fact checking inefficient. At
the same time, creating tools for automatic de-
tection is as challenging due to lack of dataset
containing articles which present fake or ma-
nipulated stories as compelling facts. In this
paper, we introduce manually verified corpus
of compelling fake and questionable news ar-
ticles on the USA politics, containing around
700 articles from Aug-Nov, 2016. We present
various analyses on this corpus and finally im-
plement classification model based on linguis-
tic features. This work is still in progress as
we plan to extend the dataset in the future and
use it for our approach towards automated fake
news detection.

1 Introduction

Fake news is a widespread menace which has re-
sulted into protests and violence around the globe.
A study published by Vosoughi et al. of MIT states
that falsehood diffuses significantly farther, faster,
deeper and more broadly than the truth in all cate-
gories of information. They also stated that effects
were more pronounced for false political news
than for false news about terrorism, natural dis-
aster, science, urban legends or financial informa-
tion. According to Pew Research Center’s survey
1 of 2017, 64% of US adults said to have great deal
of confusion about the facts in the current events.
Major events around the globe saw sudden jump in
deceitful stories on internet during sensitive events
because of which social media organizations and
government institutions have scrambled together
to tackle this problem as soon as possible. How-
ever, fake news detection has its own challenges.

1http://www.journalism.org/2016/12/15/many-
americans-believe-fake-news-is-sowing-confusion/

Figure 1: Conceptual framework for examining infor-
mation disorder presented by Wardle and Derakhshan

First and foremost is the consensus on the defini-
tion of fake news which is a topic of discussion
in several countries. Considering the complexity
of defining fake news, Wardle and Derakhshan in-
stead created a conceptual framework for exam-
ining information disorder as shown in Figure -
1. Based on this model, EU commission in 2018
categorized fake news as disinformation with the
characteristics being verifiably false or misleading
information that is created, presented and dissem-
inated for economic gain or to intentionally de-
ceive the public, and in any event to cause public
harm. Since our work focuses on false information
written intentionally to deceive people in order to
cause social unrest, we will be following the def-
inition defined by EU to categorize fake articles
into various categories as defined in Section 3 of
this paper.

The second challenge is the lack of structured
and clean dataset which is a bottleneck for fake
opinion mining, automated fact checking and cre-
ating computationally-intensive learning models



358

for feature learning. In the following section, we
elaborate on this challenge more, specify related
works and how our work is different from others.

2 Related Works

There have been several datasets released previ-
ously for fake news, most notably Buzzfeed2 and
Stanford (Allcott and Gentzkow, 2017) datasets
containing list of popular fake news articles from
2016. However, these datasets only contain web-
page links of these articles and most of them don’t
exist anymore. Fake news challenge, 20173 re-
leased a fake news dataset for stance detection,
i.e, identifying whether a particular news head-
line represents the content of news article, but al-
most 80% of the dataset does not meet the defi-
nition that we are following in our work. Many
of the articles were satire or personal opinions
which cannot be flagged as intentionally deceiv-
ing the public. (Wang, 2017) released a bench-
mark dataset containing manually labelled 12, 836
short statements, however it contains short state-
ments by famous personalities and not malicious
false stories about certain events that we are inter-
ested in. Finally, the dataset created by using BS
detector4 contains articles annotated by news ve-
racity detector tool and hence, cannot be trusted,
as the labels are not verified manually and there
are many anomalies like movie or food reviews
being flagged as fake.

In this paper, we overcome these issues by man-
ually selecting popular fake and questionable sto-
ries about US politics during Aug-Nov, 2016. The
corpus has been designed in such a way that the
writing style matches mainstream articles. In the
following sections, we define our motivation for
creating such corpus, provide details on it and
present a classification model trained on this cor-
pus to segregate articles based on writing style.
We also discuss properties of fake news articles
that we observed while developing this corpus.

Other notable works like FEVER dataset
(Thorne et al., 2018), TwoWingOS (Yin and Roth,
2018) etc. are focused on claim extraction and ver-
ification which is currently out of scope of this pa-
per. In this work, we are solely focused on creat-
ing a clean corpus for fake news articles and per-
forming classification of articles into “question-

2https://www.buzzfeednews.com/article/craigsilverman/these-
are-50-of-the-biggest-fake-news-hits-on-facebook-in

3http://www.fakenewschallenge.org/
4https://www.kaggle.com/mrisdal/fake-news/data/

able” and “mainstream” based on writing style.
Fact checking, fake news opinion mining and fake
claim extraction and verification fall under future
work of this paper. For classification task, previ-
ous works include Potthast et al. who used stylo-
metric approach on Buzzfeed dataset and achieved
an F1 score of 0.41; Pérez-Rosas et al. who
used Amazon Mechanical Turk to manually gen-
erate fake news based on real news content and
achieved an F1 score of 0.76 to detect fake news,
and Ruchansky et al. who used Twitter and Weibo
datasets and achieved F1 scores of 0.89 and 0.95
on respective datasets. In our work, we were
able to achieve an F1 score of 0.97 on the corpus
we have created, hence setting benchmark for the
writing-style based classification task on this cor-
pus.

3 Motivation

Fake news detection is a complicated topic fraught
with many difficulties such as freedom of expres-
sion, confirmation bias and different types of dis-
semination techniques. In addition to these, there
are three more ambiguities that one needs to over-
come to create an unbiased automated fake news
detector - 1. harmless teenagers writing false sto-
ries for monetary gains5; 2. AI tools generat-
ing believable articles6 and; 3. mainstream chan-
nels publishing unverifiable stories7. Considering
these elements, our motivation for this problem is
to design a system with the focus on understand-
ing the inference of the assertions, automatically
fact check and explain the users why a particular
article was tagged as questionable by the system.
In order to do so, our first priority was to create an
efficient corpus for this task. As we will see in the
following sections, we have manually selected the
articles that contain malicious assertions about a
particular topic (2016 US elections), written with
an intent of inciting hatred towards a particular en-
tity of the society. The uniqueness of the articles
in this corpus lies in the fact that a reader might be-
lieve them if not specifically informed about them
being fake, hence confusing them whether the arti-
cle was written by mainstream media or fake news

5https://www.wired.com/2017/02/veles-macedonia-fake-
news/

6https://www.technologyreview.com/s/612960/an-ai-
tool-auto-generates-fake-news-bogus-tweets-and-plenty-of-
gibberish/

7https://indianexpress.com/article/cities/delhi/retract-
reports-that-claim-najeeb-is-isis-sympathiser-delhi-hc-to-
media-5396414/



359

channel. This uniqueness will ultimately help in
creating a detector which is not biased towards
mainstream news articles on the same topic (2016
elections in this case.)

4 Corpus

4.1 Creation and Verification

The final corpus consists of 26 known fake arti-
cles from Stanford’s dataset along with 679 ques-
tionable articles taken from BS detector’s kag-
gle dataset. All the articles are in English and
talk about 2016 US presidential elections. We,
first, went through the Stanford’s list of fake ar-
ticles links and found the webpages that still ex-
ist. We then picked the first 50-70 assertive sen-
tences from these articles. After that, we read the
articles from BS detector’s kaggle dataset and se-
lected those articles which are written in a very
compelling way and succeeded in making us be-
lieve its information. We then manually verified
the claims made by these articles by doing simple
Google searches and categorized them as shown in
Table - 1. Based on the type of assertions, we have
come up with two types of labels:

Primary Label: based on the assertions they
make, articles are divided into 3 categories: 1.
False but compelling (innovated lies having jour-
nalistic style of writing); 2. Half baked informa-
tion i.e, partial truth (manipulating true events to
suit agenda); and 3. Opinions/commentary pre-
sented as facts (written in a third person narrative
with no disclaimer of the story being a personal
opinion).

Secondary Label: is of 2 types: 1. articles ex-
tracted from Stanford’s dataset have been tagged
as fake as per their own description and; 2. articles
taken from kaggle dataset are tagged as question-
able. We believe tagging something fake, when
they do contain some elements of truth, will be an
extreme measure and we leave this task to the ex-
perts.

Finally, the corpus was cleaned by removing ar-
ticles with first person narrative; removing images
and video links and keeping only textual content;
removing tweets, hyperlinks and other gibberish
like Read additional information here, [MORE],
[CLICK HERE] etc.

4.2 Analysis

Features: We used NLTK package to explore ba-
sic features of the content of news articles in terms

Figure 2: Top 20 most common keywords

of sentence count, word count etc. Number of
assertions ranges from 2 to 124 with word count
varying from 74-1430. Maximum number of stop
words in the longest article is 707. Since keywords
form an important representation of the dataset,
we extracted top 20 most common keywords from
the corpus as shown in Figure - 2.

Comparison with mainstream article: Main-
stream articles from publishers like New York
Times were extracted from all the news8 kaggle
dataset, which contains news articles from 15 US
mainstream publishers. We selected articles from
Oct - Nov, 2016 covering US politics during the
time of election. There were total 6679 articles re-
trieved from this dataset which were then catego-
rized into two labels as per our corpus’s schema.
Table - 2 compares characteristics of top 3 sen-
tences from mainstream news articles with our
corpus. We can observe that there are not many
dissimilarities except the information presented in
both the articles.

Observed Characteristics: Although articles
in our corpus have many similarities with main-
stream articles, there are some underlying patterns
that can be noticed by reading all of them together
at once. Following are the observations that we
made while verifying the claims presented in these
articles.

1. All of the news articles were trying to create
sympathy for a particular entity by manipu-
lating real stories. They were either trying
to sympathize with Donald Trump by men-
tioning media rhetoric against him, or with
Hillary Clinton by mentioning Trump’s past.

2. Similarly, they also made false claims against
above mentioned entities, by referring leaked

8https://www.kaggle.com/snapcrack/all-the-news



360

URL Authors Content Headline Primary Label Secondary
Label

URL
of
article

Can contain
anonymous
writers

Collection of
assertions

Headline of
the article

1. False
2. Partial truth
3. Opinions

1. Fake
2. Questionable

Table 1: Corpus schema. No cleaning has been performed on headlines as they are intentionally made catchy for
attention seeking.

Attributes Mainstream Questionable
Word count range (min to max) 20-100 21-100
Character count range (min to
max)

89-700 109-691

Uppercase words count range
(min to max)

0-14 0-8

Mainstream Example WASHINGTON - An exhausted Iraqi Army faces
daunting obstacles on the battlefield that will most
likely delay for months a major offensive on the
Islamic State stronghold of Mosul, American and
allied officials say. The delay is expected despite
American efforts to keep Iraqs creaky war machine
on track. Although President Obama vowed to end
the United States role in the war in Iraq, in the
last two years the American military has increas-
ingly provided logistics to prop up the Iraqi military,
which has struggled to move basics like food, water
and ammunition to its troops.

Questionable Example WASHINGTON - Hillary Clinton is being accused
of knowingly allowing American weapons into the
hands of ISIS terrorists. Weapons that Hillary Clin-
ton sent off to Qatar ostensibly designed to give to
the rebels in Libya eventually made their way to
Syria to assist the overthrow of the Assad regime.
The folks fighting against Assad were ISIS and al-
Qaeda jihadists.

Table 2: Comparing basic features of top 3 sentences from mainstream articles and questionable articles.

documents from WikiLeaks or other similar
sources. In our verification, we found that
in most of these articles, only few claims
matched the leaked story and rest were in-
vented.

3. Articles first tell false stories against a cer-
tain entity and then asks the reader ques-
tions such as “Do you think mainstream me-
dia is conspiring against you by hiding this
story?”, “Do you think Google is making al-
gorithms to create an illusion of reality?” Af-
ter asking such questions, they either leave

the reader hanging in contemplation or ask
them to “share [the article] if you believe so.”

4. The above point then leads to what (Nyhan
and Reifler, 2010) describes as backfire effect
in which correcting readers actually makes
them believe false stories.

5. Fake news writers have learnt/are learning to
use mainstream writing style such as, men-
tioning city name before starting the article,
mentioning abbreviations like (SC) for Secu-
rity Council, (AI) for Amnesty Investor, us-
ing elaborate vocabulary etc.



361

Split
Random K-Fold

Questionable (1) Mainstream (0) Questionable (1) Mainstream (0)

Train 406 5334 396 5343
Test 90 1345 100 1336

Table 3: To meet the real world scenario, train data and test data have been split with an approximate ratio of 1:10.

Figure 3: Bi-directional architecture

5 Classification Task

5.1 Model

After creating this corpus, we trained a classifi-
cation model on the content to see if it can learn
any writing patterns which are not visible to hu-
man eye and create benchmark results. Our model,
as shown in Figure - 3, is inspired by the works of
(Anand et al., 2016) and uses bi-directional LSTM
and character embedding to classify articles into
questionable (1) and mainstream (0) by learning
writing style. We have not performed any pre-
processing like removing punctuation, stop words
etc. to make sure the model learns every context
of the content. 1-D CNN has been used to cre-
ate character embedding which has been proven to
be very efficient in learning orthographic and mor-
phological features of text (Zhang et al., 2015).
This layer takes one-hot vector of each character
and uses filter size of [196, 196, 300]. We have
used 3 layers of 1-D CNN with pool size 2 and
kernel stride of 3. Finally, we have performed
maxpooling across the sequence to identify fea-
tures that produces strong sentiments. This layer
is then connected to bi-directional LSTM which
contains one forward and one backward layer with
128 units each.

This model was trained with top 3 sentences,
containing 20-100 words, and retrieved from total

7175 articles. As per the example shown in Ta-
ble - 2, it can be assumed that top 3 sentences are
enough for a normal reader to understand what the
article is talking about. Dataset splitting for train-
ing and testing was inspired by the findings in the
study conducted by (Guess et al., 2019) of Prince-
ton and NYU earlier this year which stated that
Americans who shared false stories during 2016
Presidential elections were far more likely to be
over 65. Current US age demographic suggests
that 15% of the population are over 65 9. There-
fore, we have decided to have the ratio of question-
able to mainstream news articles approximately
1:10, as shown in Table - 3.

5.2 Training and Results

We first trained our model by randomly splitting
our dataset into training, validation and test set of
sizes 4592, 1148 and 1435 respectively. However,
since the dataset is unbalanced, stratified k-fold
cross-validation mechanism is also implemented
with 5 folds. (Kohavi, 1995) states that stratifica-
tion, which ensures that each fold represents the
entire dataset, is generally a better scheme both
in terms of bias and variance. The model was
trained on an average number of 2, 296, 000 char-
acters. We have evaluated our models on various
metrics as shown in Figure - 4. For this prob-
lem, we will be majorly focused on ROC and F1
scores. In both the cases, stratified k-fold out-
performs random sampling significantly. Training
with only random sampling resulted into under-
representation of data points leading to many false
positives. This can also be because writing style
of questionable and mainstream articles are very
similar. On the other hand, 5-fold cross validation
performed significantly well in learning the pat-
terns and avoiding the problems of false positives.

9https://www.census.gov/quickfacts/fact/table/US/PST045217



362

Figure 4: Evaluation results of our model over various
metrics. Performance of model using Stratified K-Fold
is exceptionally good in terms of ROC score and F1
score.

6 Conclusion

In this paper, we have introduced a novel corpus of
articles containing false assertions which are writ-
ten in a very compelling way. We explain how
this corpus is different from previously published
datasets and explore the characteristics of the cor-
pus. Finally, we use a deep learning classification
model to learn the invisible contextual patterns of
the content and produce benchmark results on this
dataset. Our best model was able to achieve state
of the art ROC score of 97%. This is a work in
progress and we are planning to extend this cor-
pus by adding more topics and metadata. Future
work also involves claim extraction and verifica-
tion which can be further used to design an un-
biased automated detector of intentionally written
false stories.

7 Acknowledgement

I would like to extend my gratitude to my doctor-
ate advisor, Dr. Rohini K. Srihari, for taking in-
terest into my work and guiding me into the right
direction. I would also like to thank my men-
tor assigned by ACL-SRW, Arkaitz Zubiaga, and
anonymous reviewers for their invaluable sugges-
tions and comments on this paper.

References
Hunt Allcott and Matthew Gentzkow. 2017. Social me-

dia and fake news in the 2016 election. Journal of
economic perspectives, 31(2):211–36.

Ankesh Anand, Tanmoy Chakraborty, and Noseong
Park. 2016. We used neural networks to detect

clickbaits: You won’t believe what happened next!
CoRR, abs/1612.01340.

Andrew Guess, Jonathan Nagler, and Joshua Tucker.
2019. Less than you think: Prevalence and predic-
tors of fake news dissemination on facebook. Sci-
ence Advances, 5(1).

Ron Kohavi. 1995. A study of cross-validation and
bootstrap for accuracy estimation and model selec-
tion. In Proceedings of the 14th International Joint
Conference on Artificial Intelligence - Volume 2,
IJCAI’95, pages 1137–1143, San Francisco, CA,
USA. Morgan Kaufmann Publishers Inc.

Brendan Nyhan and Jason Reifler. 2010. When cor-
rections fail: The persistence of political mispercep-
tions. Political Behavior, 32(2):303–330.

Verónica Pérez-Rosas, Bennett Kleinberg, Alexandra
Lefevre, and Rada Mihalcea. 2017. Automatic de-
tection of fake news. CoRR, abs/1708.07104.

Martin Potthast, Johannes Kiesel, Kevin Reinartz,
Janek Bevendorff, and Benno Stein. 2017. A sty-
lometric inquiry into hyperpartisan and fake news.
CoRR, abs/1702.05638.

Natali Ruchansky, Sungyong Seo, and Yan Liu. 2017.
CSI: A hybrid deep model for fake news. CoRR,
abs/1703.06959.

James Thorne, Andreas Vlachos, Christos
Christodoulopoulos, and Arpit Mittal. 2018.
FEVER: a large-scale dataset for fact extraction and
verification. CoRR, abs/1803.05355.

Soroush Vosoughi, Deb Roy, and Sinan Aral. 2018.
The spread of true and false news online. Science,
359(6380):1146–1151.

William Yang Wang. 2017. ”liar, liar pants on fire”:
A new benchmark dataset for fake news detection.
CoRR, abs/1705.00648.

Claire Wardle and Hossein Derakhshan. 2017. Infor-
mation disorder: Toward an interdisciplinary frame-
work for research and policy making. Council of
Europe report, DGI (2017), 9.

Wenpeng Yin and Dan Roth. 2018. Twowingos: A
two-wing optimization strategy for evidential claim
verification. CoRR, abs/1808.03465.

Xiang Zhang, Junbo Zhao, and Yann LeCun. 2015.
Character-level convolutional networks for text clas-
sification. In C. Cortes, N. D. Lawrence, D. D. Lee,
M. Sugiyama, and R. Garnett, editors, Advances in
Neural Information Processing Systems 28, pages
649–657. Curran Associates, Inc.

http://arxiv.org/abs/1612.01340
http://arxiv.org/abs/1612.01340
https://doi.org/10.1126/sciadv.aau4586
https://doi.org/10.1126/sciadv.aau4586
http://dl.acm.org/citation.cfm?id=1643031.1643047
http://dl.acm.org/citation.cfm?id=1643031.1643047
http://dl.acm.org/citation.cfm?id=1643031.1643047
https://doi.org/10.1007/s11109-010-9112-2
https://doi.org/10.1007/s11109-010-9112-2
https://doi.org/10.1007/s11109-010-9112-2
http://arxiv.org/abs/1708.07104
http://arxiv.org/abs/1708.07104
http://arxiv.org/abs/1702.05638
http://arxiv.org/abs/1702.05638
http://arxiv.org/abs/1703.06959
http://arxiv.org/abs/1803.05355
http://arxiv.org/abs/1803.05355
https://doi.org/10.1126/science.aap9559
http://arxiv.org/abs/1705.00648
http://arxiv.org/abs/1705.00648
http://arxiv.org/abs/1808.03465
http://arxiv.org/abs/1808.03465
http://arxiv.org/abs/1808.03465
http://papers.nips.cc/paper/5782-character-level-convolutional-networks-for-text-classification.pdf
http://papers.nips.cc/paper/5782-character-level-convolutional-networks-for-text-classification.pdf

