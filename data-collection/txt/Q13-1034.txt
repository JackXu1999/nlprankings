








































Joint Morphological and Syntactic Analysis for Richly Inflected Languages

Bernd Bohnet∗ Joakim Nivre? Igor Boguslavsky•◦ Richárd Farkas� Filip Ginter† Jan Hajič‡

∗University of Birmingham, School of Computer Science
?Uppsala University, Department of Linguistics and Philology

•Universidad Politécnica de Madrid, Departamento de Inteligencia Artificial
◦Russian Academy of Sciences, Institute for Information Transmission Problems

�University of Szeged, Institute of Informatics
†University of Turku, Department of Information Technology

‡Charles University in Prague, Institute of Formal and Applied Linguistics

Abstract

Joint morphological and syntactic analysis has
been proposed as a way of improving parsing
accuracy for richly inflected languages. Start-
ing from a transition-based model for joint
part-of-speech tagging and dependency pars-
ing, we explore different ways of integrating
morphological features into the model. We
also investigate the use of rule-based mor-
phological analyzers to provide hard or soft
lexical constraints and the use of word clus-
ters to tackle the sparsity of lexical features.
Evaluation on five morphologically rich lan-
guages (Czech, Finnish, German, Hungarian,
and Russian) shows consistent improvements
in both morphological and syntactic accuracy
for joint prediction over a pipeline model, with
further improvements thanks to lexical con-
straints and word clusters. The final results
improve the state of the art in dependency
parsing for all languages.

1 Introduction

Syntactic parsing of natural language has witnessed
a tremendous development during the last twenty
years, especially through the use of statistical mod-
els for robust and accurate broad-coverage parsing.
However, as statistical parsing techniques have been
applied to more and more languages, it has also
been observed that typological differences between
languages lead to new challenges. In particular, it
has been found over and over again that languages
exhibiting rich morphological structure, often to-
gether with a relatively free word order, usually ob-
tain lower parsing accuracy, especially in compar-

ison to English. One striking demonstration of this
tendency can be found in the CoNLL shared tasks on
multilingual dependency parsing, organized in 2006
and 2007, where richly inflected languages clustered
at the lower end of the scale with respect to pars-
ing accuracy (Buchholz and Marsi, 2006; Nivre et
al., 2007). These and similar observations have led
to an increased interest in the special challenges
posed by parsing morphologically rich languages,
as evidenced most clearly by a new series of work-
shops devoted to this topic (Tsarfaty et al., 2010), as
well as a special issue in Computational Linguistics
(Tsarfaty et al., 2013) and a shared task on parsing
morphologically rich languages.1

One hypothesized explanation for the lower pars-
ing accuracy observed for richly inflected languages
is the strict separation of morphological and syn-
tactic analysis assumed in many parsing frame-
works (Tsarfaty et al., 2010; Tsarfaty et al., 2013).
This is true in particular for data-driven dependency
parsers, which tend to assume that all morphological
disambiguation has been performed before syntactic
analysis begins. However, as argued by Lee et al.
(2011), in morphologically rich languages there is
often considerable interaction between morphology
and syntax, such that neither can be disambiguated
without the other. Lee et al. (2011) go on to show
that a discriminative model for joint morphological
disambiguation and dependency parsing gives con-
sistent improvements in morphological and syntac-
tic accuracy, compared to a pipeline model, for An-
cient Greek, Czech, Hungarian and Latin. Simi-
larly, Bohnet and Nivre (2012) propose a model for

1See https://sites.google.com/site/spmrl2013/home/sharedtask.

415

Transactions of the Association for Computational Linguistics, 1 (2013) 415–428. Action Editor: Brian Roark.
Submitted 7/2013; Revised 9/2013; Published 10/2013. c©2013 Association for Computational Linguistics.



joint part-of-speech tagging and dependency parsing
and report improved accuracy for Czech and Ger-
man (but also for Chinese and English), although in
this case the joint model is limited to basic part-of-
speech tags and does not involve the full complex
of morphological features. An integrated approach
to morphological and syntactic analysis can also
be found in grammar-based dependency parsers,
such as the ETAP-3 linguistic processor (Apresian
et al., 2003), where morphological disambiguation
is mostly carried out together with syntactic anal-
ysis. Finally, it is worth noting that joint models
of morphology and syntax have been more popu-
lar in constituency-based statistical parsing (Cowan
and Collins, 2005; Tsarfaty, 2006; Cohen and Smith,
2007; Goldberg and Tsarfaty, 2008).

Another hypothesis from the literature is that the
high type-token ratio resulting from large morpho-
logical paradigms leads to data sparseness when es-
timating the parameters of a statistical parsing model
(Tsarfaty et al., 2010; Tsarfaty et al., 2013). In par-
ticular, for many words in the language, only a sub-
set of its morphological forms will be observed at
training time. This suggests that using rule-based
morphological analyzers or other lexical resources
may be a viable strategy to improve coverage and
performance. Thus, Goldberg and Elhadad (2013)
show that integrating an external wide-coverage lex-
icon with a treebank-trained PCFG parser improves
parsing accuracy for Modern Hebrew, which is in
line with earlier studies of part-of-speech tagging for
morphologically rich languages (Hajič, 2000). The
sparsity of lexical features can also be tackled by the
use of distributional word clusters as pioneered by
Koo et al. (2008).

In this paper, we present a transition-based model
that jointly predicts complex morphological repre-
sentations and dependency relations, generalizing
the approach of Bohnet and Nivre (2012) to include
the full range of morphological information. We
start by investigating different ways of integrating
morphological features into the model, go on to ex-
amine the effect of using rule-based morphological
analyzers to derive hard or soft constraints on the
morphological analysis, and finally add word clus-
ter features to combat lexical sparsity. We evalu-
ate our methods on data from Czech, Finnish, Ger-
man, Hungarian, and Russian, five morphologically

rich languages representing three different language
groups. The experiments show that joint prediction
of morphology and syntax, rule-based morpholog-
ical analyzers, and word clusters all contribute to
improved parsing accuracy, leading to new state-of-
the-art results for all languages.

2 Method

In this section, we define target representations and
evaluation metrics (2.1), and describe our transition-
based parsing framework, consisting of an abstract
transition system (2.2), a feature-based scoring func-
tion (2.3), and algorithms for decoding (2.4) and
learning (2.5).

2.1 Representations and Metrics

We take an unlabeled dependency tree for a sentence
x = w1, . . . , wn to be a directed tree T = (Vx, A),
where Vx = {0, 1, ..., n}, A ⊆ Vx × V +x , and 0 is
the root of the tree (Kübler et al., 2009). The set Vx
of nodes is the set of positive integers up to and in-
cluding n, each corresponding to the linear position
of a word in the sentence, plus an extra artificial root
node 0. We use V +x to denote Vx−{0}. The setA of
arcs is a set of pairs (i, j), where i is the head node
and j is the dependent node.

To this basic representation of syntactic structure
we add four labeling functions for part-of-speech
tags, morphological features, lemmas, and depen-
dency relations. The function π : V +x → P maps
each node in V +x to a part-of-speech tag in the set
P ; the function µ : V +x → M maps each node to a
morphological description in the setM ; the function
λ : V +x → Z∗ maps each node in V +x to a lemma
(a string over some character set Z); and the func-
tion δ : A → D maps each arc to a dependency
label in the set D. The exact nature of P , M and D
depends on the data sets used, but normally P and
D only contain atomic labels while the members of
M are sets of atomic features encoding properties
like number, case, tense, etc. For lemmas, we do not
assume that there is a fixed lexicon but allow any
character string as a legal value.

We define our target representation for a sentence
x = w1, . . . , wn as a quintuple Γ = (A, π, µ, λ, δ)
such that (Vx, A) is an unlabeled dependency tree;
π, µ and λ label the nodes with part-of-speech tags,

416



Transition Condition
LEFT-ARCd ([σ|i, j], B,Γ)⇒ ([σ|j], B,Γ[(j, i)∈A, δ(j, i)=d]) i 6= 0
RIGHT-ARCd ([σ|i, j], B,Γ)⇒ ([σ|i], B,Γ[(i, j)∈A, δ(i, j)=d])
SHIFTp,m,l (σ, [i|β],Γ)⇒ ([σ|i], β,Γ[π(i)=p, µ(i)=m,λ(i)= l])
SWAP ([σ|i, j], β,Γ)⇒ ([σ|j], [i|β],Γ) 0 < i < j

Figure 1: Transitions for joint morphological and syntactic analysis. The stack Σ is represented as a list with its head
to the right (and tail σ) and the buffer B as a list with its head to the left (and tail β). The notation Γ[q1, . . . , qm] is
used to denote an MS-parse that is exactly like Γ except that q1, . . . , qm hold true.

morphological features and lemmas; and δ labels the
arcs with dependency relations. For convenience,
we refer to this type of structure as a morphosyn-
tactic parse (or MS-parse, for short). The follow-
ing evaluation metrics are used to score an MS-parse
with respect to a gold standard:

1. POS: The percentage of nodes in V +x that have
the correct part-of-speech tag.

2. MOR: The percentage of nodes in V +x that have
the correct morphological description; if the
description is set-valued, all members of the set
must match exactly.

3. LEM: The percentage of nodes in V +x that have
the correct lemma.

4. UAS: The percentage of nodes in V +x that have
the correct incoming arc.

5. LAS: The percentage of nodes in V +x that have
the correct incoming arc with the correct label.

6. PM: The percentage of nodes in V +x that have
the correct part-of-speech tag and the correct
morphological description.

7. PMD: The percentage of nodes in V +x that have
the correct part-of-speech tag, the correct mor-
phological description, and the correct incom-
ing arc with the correct label.

The POS, UAS and LAS metrics are standard in the
dependency parsing literature; the additional met-
rics will provide us with a more fine-grained pic-
ture of the (joint) morphological and syntactic accu-
racy. All evaluation scores are computed over all to-
kens, including punctuation. We test statistical sig-
nificance primarily for the PMD metric, using a two-
tailed paired t-test.

2.2 Transition System

A transition system for dependency parsing is a
quadruple S = (C, T, cs, Ct), where C is a set
of configurations, T is a set of transitions, each of
which is a (partial) function t : C → C, cs is an ini-
tialization function, mapping a sentence x to a con-
figuration c ∈ C, and Ct ⊆ C is a set of terminal
configurations. A transition sequence for a sentence
x in S is a sequence of configuration-transition pairs
C0,m = [(c0, t0), (c1, t1), . . . , (cm, tm)] where c0 =
cs(x), tm(cm) ∈ Ct, and ti(ci) = ci+1(0 ≤ i < m).

In our model for joint prediction of part-of-speech
tags, morphological features and dependency trees,
the set C of configurations consists of all triples
c = (Σ, B,Γ) such that Σ (the stack) and B
(the buffer) are disjoint sublists of the nodes Vx
of some sentence x, and Γ = (A, π, µ, λ, δ) is
an MS-parse for x. We take the initial config-
uration for a sentence x = w1, . . . , wn to be
cs(x) = ([0], [1, . . . , n], (∅,⊥,⊥,⊥,⊥)), where
⊥ is the function that is undefined for all argu-
ments, and we take the set Ct of terminal config-
urations to be the set of all configurations of the
form c = ([0], [ ],Γ) (for any Γ). The MS-parse de-
fined for x by c = (Σ, B, (A, π, µ, λ, δ)) is Γc =
(A, π, µ, λ, δ), and the MS-parse defined for x by a
complete transition sequence C0,m is Γtm(cm).

The set T of transitions is shown in Figure 1. It
is based on the system of Nivre (2009), where a de-
pendency tree is built by repeated applications of the
LEFT-ARCd and RIGHT-ARCd transitions, which
add an arc (with some label d ∈ D) between the
two topmost nodes on the stack (with the leftmost
or rightmost node as the dependent, respectively).
The SHIFT transition is used to move nodes from the
buffer to the stack, and the SWAP transition is used

417



to permute nodes in order to allow non-projective
dependencies. Bohnet and Nivre (2012) modified
this system by replacing the simple SHIFT transition
by SHIFTp, which not only moves a node from the
buffer to the stack but also assigns it a part-of-speech
tag p, turning it into a system for joint part-of-speech
tagging and dependency parsing.2 Here we add two
additional parameters m and l to the SHIFT transi-
tion, so that a node moved from the buffer to the
stack is assigned not only a tag p but also a morpho-
logical description m and a lemma l. In this way,
we get a joint model for the prediction of part-of-
speech tags, morphological features, lemmas, and
dependency trees.

2.3 Scoring

In transition-based parsing, we score parses in an in-
direct fashion by scoring transition sequences. In
general, we assume that the score function s factors
by configuration-transition pairs:

s(x,C0,m) =
m∑

i=0

s(x, ci, ti) (1)

Moreover, when using structured learning, as first
proposed for transition-based parsing by Zhang and
Clark (2008), we assume that the score is given by a
linear model whose feature representations decom-
pose in the same way:

s(x,C0,m) = f(x,C0,m) ·w

=

m∑

i=0

f(x, ci, ti) ·w

(2)

Here, f(x, c, t) is a high-dimensional feature vec-
tor, where each component fi(x, c, t) is a nonneg-
ative numerical feature (usually binary), and w is
a weight vector of the same dimensionality, where
each component wi is the real-valued weight of the
feature fi(x, c, t). The choice of features to include
in f(x, c, t) is discussed separately for each instanti-
ation of the model in Sections 4–6.

2Hatori et al. (2011) previously made the same modifica-
tion to the arc-standard system (Nivre, 2004), without the SWAP
transition. Similarly, Titov and Henderson (2007) added a word
parameter to the SHIFT transition to get a joint model of word
strings and dependency trees. A similar model was considered
but finally not used by Gesmundo et al. (2009).

2.4 Decoding

Exact decoding for transition-based parsing is hard
in general.3 Early transition-based parsers mostly
relied on greedy, deterministic decoding, which
makes for very efficient parsing (Yamada and Mat-
sumoto, 2003; Nivre, 2003), but research has shown
that accuracy can be improved by using beam search
instead (Zhang and Clark, 2008; Zhang and Nivre,
2012). While still not exact, beam search decoders
explore a larger part of the search space than greedy
parsers, which is likely to be especially important
for joint models, where the search space is larger
than for plain dependency parsing without morphol-
ogy (even more so with the SWAP transition for non-
projectivity). Figure 2 outlines the beam search al-
gorithm used for decoding with our model. Differ-
ent instantiations of the model will require slightly
different implementations of the permissibility con-
dition invoked in line 8, which can be used to filter
out labels that are improbable or incompatible with
an external lexicon, and the pruning step performed
in line 13, where there may be a need to balance the
amount of morphological and syntactic variation in
the beam. Both these aspects will be discussed in
depth in Sections 4–6.

Although the worst-case running time with con-
stant beam size is quadratic in sentence length, the
observed running time is linear for natural language
data sets, due to the sparsity of non-projective de-
pendencies (Nivre, 2009). The running time is also
linear in |D|+ |P ×M |, which means that joint pre-
diction only gives a linear increase in running time,
often quite marginal because |D| > |P ×M |. This
assumes that the lemma is predicted deterministi-
cally given a tag and a morphological description, an
assumption that is enforced in all our experiments.

2.5 Learning

In order to learn a weight vector w from a training
set of sentences with gold parses, we use a variant
of the structured perceptron, introduced by Collins
(2002) and first used for transition-based parsing by
Zhang and Clark (2008). We initialize all weights

3While there exist exact dynamic programming algorithms
for projective transition systems (Huang and Sagae, 2010;
Kuhlmann et al., 2011) and even for restricted non-projective
systems (Cohen et al., 2011), parsing is intractable for systems
like ours that permit arbitrary non-projective trees.

418



PARSE(x,w)
1 h0.c← cs(x)
2 h0.s← 0.0
3 h0.f← {0.0}dim(w)
4 BEAM ← [h0]
5 while ∃h ∈ BEAM : h.c 6∈ Ct
6 TMP ← [ ]
7 foreach h ∈ BEAM
8 foreach t ∈ T : PERMISSIBLE(h.c, t)
9 h.f← h.f + f(x, h.c, t)

10 h.s← h.s+ f(x, h.c, t) · w
11 h.c← t(h.c)
12 TMP ← INSERT(h, TMP)
13 BEAM← PRUNE(TMP)
14 h∗ ← TOP(BEAM)
15 return Γh∗c

Figure 2: Beam search algorithm for finding the best MS-
parse for input sentence x with weight vector w. The
symbols h.c, h.s and h.f denote, respectively, the con-
figuration, score and feature vector of a hypothesis h; Γc
denotes the MS-parse defined by c.

to 0.0, make N iterations over the training data and
update the weight vector for every sentence x where
the transition sequence C0,m corresponding to the
gold parse is different from the highest scoring tran-
sition sequence C∗0,m′ .

4 More precisely, we use the
passive-aggressive update of Crammer et al. (2006).
We also use the early update strategy found benefi-
cial for parsing in several previous studies (Collins
and Roark, 2004; Zhang and Clark, 2008; Huang
and Sagae, 2010). This means that, at learning
time, we terminate the beam search as soon as the
hypothesis corresponding to the gold parse is pruned
from the beam and then update with respect to the
partial transition sequences constructed up to that
point. Finally, we use the standard technique of av-
eraging over all weight vectors seen in training, as
originally proposed by Collins (2002).

4Note that there may be more than one transition sequence
corresponding to the gold parse, in which case we pick the
canonical transition sequence that processes all left-dependents
before right-dependents and applies the lazy swapping strategy
of Nivre et al. (2009).

3 Data Sets and Resources

Throughout the paper, we experiment with data from
five languages: Czech, Finnish, German, Hungarian,
and Russian. For each language, we use a morpho-
logically and syntactically annotated corpus (tree-
bank), divided into a training set, a development set
and a test set. In addition, we use a lexicon gen-
erated by a rule-based morphological analyzer, and
distributional word clusters derived from a large un-
labeled corpus. Below we describe the specific re-
sources used for each language. Table 1 provides
descriptive statistics about the resources.

Czech For training and test we use the Prague De-
pendency Treebank (Hajič et al., 2001; Böhmová et
al., 2003), Version 2.5, converted to the format used
in the CoNLL 2009 shared task (Hajič et al., 2009).
The morphological lexicon comes from Hajič and
Hladká (1998),5 and word clusters are derived from
a large web corpus (Spoustová and Spousta, 2012).

Finnish The training set is from the Turku Depen-
dency Treebank (Haverinen et al., 2013), and the test
set is the hidden test set maintained by the treebank
developers. It is worth noting that, while the entire
treebank has manually validated syntactic annota-
tion, the morphological annotation is automatic ex-
cept for a subset of 1204 tokens in the test set, which
will be used to estimate the POS, MOR, LEM, PM
and PMD scores. The estimated accuracy of the
automatic annotation is 97.3% POS and 94.8% PM
(Haverinen et al., 2013). Also, because of the lim-
ited amount of data, we do not use a development
set for Finnish but instead use cross-validation on
the training set when tuning parameters. We use the
open-source morphological analyzer OMorFi (Piri-
nen, 2011) and word clusters derived from the entire
Finnish Wikipedia.6

German Training and test sets are from the Tiger
Treebank (Brants et al., 2002) in the improved de-
pendency conversion by Seeker et al. (2010). We
use the SMOR morphological analyzer (Schmid et
al., 2004), but because the tags and morphological
features in the lexicon are not the same as in the

5Downloaded from the http://lindat.cz repository as resource
PID http://hdl.handle.net/11858/00-097C-0000-0015-A780-9.

6Downloaded in March 2012.

419



Treebank Morphology Clusters
Train Dev Test P M D Forms Lemmas Tokens Types

Czech 652,544 87,988 70,348 12 1851 49 98,360 42,058 628,332,859 477,185
Finnish 183,118 – 21,211 12 1917 47 57,127 25,280 50,207,300 257,984
German 648,296 32,065 31,692 54 257 43 76,729 55,220 1,327,701,182 1,621,083
Hungarian 1,101,871 210,068 171,466 22 1105 33 151,971 71,263 200,249,814 538,138
Russian 575,400 72,893 71,664 14 454 78 97,905 35,039 195,897,041 639,446

Table 1: Statistics about data sets and resources used in the experiments. Treebank: number of tokens in data sets;
number of labels in label sets. Morphology: number of word forms and lemmas in treebank covered by morphological
analyzer. Clusters: number of tokens and types in unlabeled corpus.

treebank annotation we have to rely on a heuristic
mapping between the two. Word clusters are derived
from the so-called Huge German Corpus.7

Hungarian For training and test we use the
Szeged Dependency Treebank (Farkas et al., 2012).
We use a finite-state morphological analyzer con-
structed from the morphdb.hu lexical resource (Trón
et al., 2006), and word clusters come from the Hun-
garian National Corpus (Váradi, 2002).

Russian Parsers are trained and tested on data
from the SynTagRus Treebank (Boguslavsky et al.,
2000; Boguslavsky et al., 2002). The morpholog-
ical analyzer is a module of the ETAP-3 linguistic
processor (Apresian et al., 2003) with a dictionary
comprising more than 130,000 lexemes (Iomdin and
Sizov, 2008). Word clusters have been produced on
the basis of an unlabeled corpus of Russian com-
piled by the Russian Language Institute of the Rus-
sian Academy of Sciences and tokenized by the
ETAP-3 analyzer.

4 Joint Morphology and Syntax

We start by exploring different ways of integrating
morphology and syntax in a data-driven setting, that
is, where our only knowledge source is the anno-
tated training corpus. At both learning and parsing
time, we preprocess sentences using a tagger that as-
signs (up to) kp part-of-speech tags and km morpho-
logical descriptions and a lemmatizer that assigns
a single best lemma to each word. Complex mor-
phological descriptions consisting of several atomic
features are predicted as a whole, both in prepro-
cessing and in parsing. Although it would be pos-

7See http://www.ims.uni-stuttgart.de/forschung/ressourcen/
korpora/hgc.html.

sible to predict each atomic morphological feature
separately, we believe this would increase the risk
of creating inconsistent morphological descriptions.
As preprocessors, we use the tagger and lemmatizer
included in the MATE tools8 trained on the same
annotated training set, using 10-fold jack-knifing to
get predictions for the training set itself. The tag-
ger is a greedy left-to-right tagger trained with the
same passive-aggressive online learning as the pars-
ing system, which is run twice over the input to make
more effective use of contextual features. The tagger
scores are not properly normalized but tend to be in
the [0,1] range for both part-of-speech tags and mor-
phological descriptions. In this setting, we consider
four different models for deriving a full MS-parse:

1. In the PIPELINE model, we set kp = km = 1,
which means that the SHIFT transition always
selects the 1-best tag, morphological descrip-
tion and lemma for each word. We use a beam
size of 40 and prune by simply keeping the 40
highest scoring hypotheses at each step. As the
name suggests, this is equivalent to a standard
pipeline with no joint prediction.

2. The SIMPLETAG model replicates the model of
Bohnet and Nivre (2012) with kp = 2, km = 1,
and a score threshold for tags of 0.25, meaning
that the second best tag is only considered if
its score is less than 0.25 below that of the best
tag. We use two-step beam pruning, where we
first extract the 40 highest scoring hypotheses
with distinct dependency trees and then add the
8 highest scoring remaining hypotheses (nor-
mally morphological variants of hypotheses al-
ready included) for a total beam size of 48. This

8Available at https://code.google.com/p/mate-tools/.

420



model performs joint tagging and parsing but
relies on 1-best morphological features.

3. The COMPLEXTAG model is like SIMPLETAG
except that we let tags represent the concate-
nation of ordinary tags and morphological de-
scriptions (and retrain the preprocessing tagger
on this representation). This model performs
joint morphological and syntactic analysis as
joint tagging and parsing with a fine-grained
tag set.

4. The JOINT model has kp = km = 2, meaning
that the tag and the morphological description
can be selected independently by the parser.
For morphological descriptions, we use a score
threshold of 0.1. For beam pruning, we gener-
alize the previous method by first extracting the
40 highest-scoring hypotheses with distinct de-
pendency trees. For each of these, we then find
the highest-scoring hypothesis with the same
dependency tree but different tags or morpho-
logical features, storing these in two temporary
lists TMPp, for hypotheses that differ with re-
spect to tags, and TMPm, for hypotheses that
differ only with respect to morphological fea-
tures. Finally, we extract the 8 highest-scoring
hypotheses from each of TMPp and TMPm and
add them to the beam for a total beam size
of 56. This model performs joint prediction
of part-of-speech tags, morphological descrip-
tions and dependency relations (but still relies
on 1-best lemmas, like all the other models.)

The procedures for beam pruning may appear both
complex and ad hoc, especially for the JOINT model,
but are motivated by the need to achieve a balance
between morphological and syntactic ambiguity in
the set of hypotheses maintained. As explained by
Bohnet and Nivre (2012), just maintaining a single
beam does not give enough variety in the beam. The
method used for the JOINT model is one way of gen-
eralizing this technique to a fully joint model, but
other strategies are certainly conceivable.

Another point that may be surprising is the choice
to keep kp and km as low as 2, which is fairly close to
a pipeline model. Bohnet and Nivre (2012) experi-
mented with higher values for the tag threshold but
found no improvement in accuracy, and our own pre-

liminary experiments confirmed this trend for mor-
phological descriptions. In Section 7, we present an
empirical analysis that gives further support for this
choice, at least for the languages considered in this
paper. Note also that the choice is not motivated by
efficiency concerns, since increasing the values of kp
and km has only a marginal effect on running time,
as explained in Section 2.4. Finally, the choice not
to consider k-best lemmas is dictated by the fact that
our lemmatizer only provides a 1-best analysis.

For the first three models, we use the same fea-
ture representations as Bohnet and Nivre (2012),9

consisting of their adaptation of the features used
by Zhang and Nivre (2011), the graph completion
features of Bohnet and Kuhn (2012), and the spe-
cial features over k-best tags introduced specifi-
cally for joint tagging and parsing by Bohnet and
Nivre (2012). For the JOINT model, we simply add
features over the k-best morphological descriptions
analogous to the features over k-best tags.10

Experimental results for these four models can be
found in Table 2. From the PIPELINE results, we
see that the 1-best accuracy of the preprocessing tag-
ger ranges from 95.0 (Finnish) to 99.2 (Czech) for
POS, and from 89.4 (Finnish) to 96.5 (Hungarian)
for MOR. The lemmatizer does a good job for four
of the languages (93.9–97.9) but has really poor per-
formance on Finnish (73.7). With respect to syn-
tactic accuracy, the PIPELINE system achieves LAS
ranging from 79.9 (Finnish) to 91.8 (German) and
UAS ranging from 84.4 to 93.7. It is interesting
to note that the highest PMD score, which requires
both morphology and syntax to be completely cor-
rect, is observed for Hungarian (86.2).

Turning to the results for SIMPLETAG, we note
that our results are consistent with those reported
by Bohnet and Nivre (2012), with small but con-
sistent improvements in POS and UAS/LAS (and
in the compound metrics PM and PMD) for most
languages. However, the improvement in the PMD
score is statistically significant only for Hungarian
and Russian (p < 0.01). By contrast, the results for
COMPLEXTAG confirm our hypothesis that merging
tags and morphological descriptions into a single tag
is not an effective way to do joint morphological and

9See http://stp.lingfil.uu.se/∼nivre/exp/emnlp12.html.
10A complete description of our feature representations is

available at http://stp.lingfil.uu.se/∼nivre/exp/tacl13.html.

421



Czech POS MOR LEM UAS LAS PM PMD
PIPELINE 99.2 93.2 95.5 88.5 83.1 93.0 78.4
SIMPLETAG 99.2 93.2 95.5 88.5 83.2 93.1 78.4
COMPLEXTAG 98.8 93.3 95.5 87.1 81.2 93.3 77.3
JOINT 99.2 93.7 95.5 88.7 83.3 93.7 79.2
LEXHARD 99.3 93.0 94.3 88.7 83.4 92.9 78.3
LEXSOFT 99.4 94.5 95.9 88.8 83.5 94.4 79.8
CLUSTER 99.4 94.6 96.0 89.0 83.7 94.5 80.0
ORACLE 99.8 97.0 – 92.7 89.9 94.1 84.7
GOLD – – – 89.3 84.5 – –
Finnish POS MOR LEM UAS LAS PM PMD
PIPELINE 95.0 89.4 73.7 84.4 79.9 88.8 71.5
SIMPLETAG 95.6 89.4 73.7 84.8 80.5 89.0 73.0
COMPLEXTAG 93.0 84.9 73.7 80.1 74.5 84.7 65.3
JOINT 95.4 89.2 73.7 84.8 80.6 89.1 72.6
LEXHARD 95.8 91.6 93.4 86.1 82.5 91.1 75.9
LEXSOFT 95.5 91.9 93.0 86.0 82.3 91.6 75.7
CLUSTER 95.7 92.0 94.4 86.6 83.1 91.4 75.8
ORACLE 98.0 94.8 – 91.3 89.4 91.8 83.1
German POS MOR LEM UAS LAS PM PMD
PIPELINE 97.6 90.0 97.9 93.7 91.8 89.1 82.9
SIMPLETAG 98.0 90.0 97.9 93.8 91.9 89.1 83.0
COMPLEXTAG 97.3 87.6 97.9 92.3 90.1 86.9 79.7
JOINT 98.1 90.8 97.9 93.9 92.0 90.0 83.9
LEXHARD 97.0 65.6 97.9 93.7 92.0 64.6 61.3
LEXSOFT 98.4 91.9 97.9 94.0 92.1 91.2 85.1
CLUSTER 98.4 92.5 97.9 94.1 92.4 91.7 85.9
ORACLE 99.6 96.3 – 96.3 95.9 91.9 88.8
GOLD – – – 94.2 92.7 – –
Hungarian POS MOR LEM UAS LAS PM PMD
PIPELINE 97.6 96.5 93.9 91.0 88.4 96.1 86.2
SIMPLETAG 97.8 96.5 93.9 91.3 88.8 96.1 86.6
COMPLEXTAG 97.5 90.9 93.9 90.6 87.7 90.9 81.2
JOINT 97.8 96.4 93.7 91.3 88.9 96.2 86.7
LEXHARD 98.5 97.3 99.0 91.5 89.1 97.1 87.4
LEXSOFT 98.5 97.6 99.0 91.4 89.1 97.4 87.7
CLUSTER 98.5 97.6 99.0 91.7 89.3 97.4 88.0
ORACLE 99.7 99.3 – 94.6 93.3 97.6 91.7
GOLD – – – 91.9 89.8 – –
Russian POS MOR LEM UAS LAS PM PMD
PIPELINE 98.4 94.0 96.1 92.6 87.4 92.6 82.7
SIMPLETAG 98.5 94.0 96.1 92.6 87.5 92.6 82.9
COMPLEXTAG 97.9 91.4 96.1 91.2 85.1 90.8 79.0
JOINT 98.5 94.4 96.1 92.8 87.6 92.8 83.5
LEXHARD 98.9 95.1 94.0 93.0 88.0 94.5 84.1
LEXSOFT 98.8 95.7 96.5 92.9 87.7 95.1 84.5
CLUSTER 98.8 95.7 96.6 93.0 87.9 95.7 84.7
ORACLE 99.9 98.6 – 95.5 92.9 95.2 89.0
GOLD – – – 94.0 89.1 – –

Table 2: Test set results for all models. ORACLE = ora-
cle scores for LEXSOFT; GOLD = accuracy for PIPELINE
with gold POS, MOR, LEM. Bold marks best result per
column and language (excluding ORACLE and GOLD).

syntactic analysis. Here, we see a significant drop
in most scores for all languages, but in particular in
the accuracy of morphological descriptions (MOR),
where the score drops by 5.6 percentage points for
Hungarian, 4.5 for Finnish, 2.6 for Russian, and 2.4
for German. The only exception is Czech, where
MOR and PM actually improve slightly, but this
comes at the expense of a substantial drop in depen-
dency accuracy. In any case, the decrease in PMD is
highly significant for all languages (p < 0.01).

Finally, we see that the JOINT model, where
tags and morphological descriptions are predicted
separately during the parsing process, gives sig-
nificant improvements in MOR accuracy compared
to the PIPELINE and SIMPLETAG models for Ger-
man (+0.8), Czech (+0.5), and Russian (+0.4), with
marginal improvements also in the syntactic UAS
and LAS scores. For Finnish and Hungarian, on
the other hand, there is actually a small drop in ac-
curacy (and for Finnish also a drop in POS accu-
racy compared to SIMPLETAG). Interestingly, how-
ever, for both these languages there is nevertheless
a small improvement in the joint PM score, indicat-
ing that the JOINT model in general does a better
job at selecting a valid complete morphological de-
scription than the SIMPLETAG model. Since Finnish
and Hungarian are the most morphologically com-
plex languages, it is likely that the lack of a strong
positive effect is due in part to sparse data, espe-
cially for Finnish where the training set is small. As
we shall see in the next section, this problem can be
partly overcome through the use of external lexical
resources. Still, the improvement in the PMD score
over the other three models is highly significant for
all languages except Finnish (p < 0.01).

5 Lexical Constraints

Our starting point in this section is the JOINT model,
which gave the best overall accuracy score (PMD)
for all languages except Finnish. To this model we
now add constraints derived from a morphological
lexicon that maps each word form to a set of pos-
sible tags, morphological descriptions and lemmas.
We explore two different ways of integrating these
constraints:

1. In the LEXHARD model, we use the lexicon to
derive hard constraints and filter out tags and

422



morphological descriptions that are not in the
lexicon. More precisely, for word forms that
are covered by the lexicon, we let the prepro-
cessing tagger select the kp best tags and km
best morphological descriptions that are in the
lexicon. We do this both during training and
parsing, and we use exactly the same features
and beam handling as for the JOINT model in
the previous section.

2. In the LEXSOFT model, we instead use soft
lexical constraints by adding features that en-
code whether a tag or morphological descrip-
tion is in the lexicon or not. Again, we add
these features both to the preprocessing tagger
and to the joint parser, which otherwise remain
exactly as before.

One additional modification that we make for both
the LEXHARD and the LEXSOFT model is to com-
pletely rely on the external lexicon for the predic-
tion of lemmas. After the parser has selected a tag
and morphological description for a word, we sim-
ply predict the corresponding lemma from the lexi-
con, breaking ties arbitrarily in the very few cases
where the word form, tag and morphological de-
scription do not determine a unique lemma, and
leaving the lemma empty for word forms that are not
contained in the lexicon. This means that, in con-
trast to the purely data-driven models, the lexicon-
enriched models predict the complete morphological
analysis jointly with parsing (with the lemma being
derived deterministically from the tag and the mor-
phological description). We make an exception only
for German, where the lexicon provides lemmas that
would require further disambiguation and where we
therefore continue to use the data-driven lemmatizer.

As can be seen in Table 2, the results for the LEX-
HARD model are somewhat mixed. For Finnish, we
see a dramatic improvement of the LEM score (from
73.7 to 93.4), indicating that the rule-based morpho-
logical analyzer is vastly superior to the data-driven
lemmatizer for Finnish. There is also a very nice
boost to the MOR score (+2.2) and a smaller im-
provement on POS (+0.4). These improvements also
lead to higher syntactic accuracy, with LAS increas-
ing from 80.6 to 82.5 and UAS from 84.8 to 86.1.
For Hungarian, we have nice improvements of the
LEM score (+5.3), the MOR score (+0.9) and the

POS score (+0.7), but only small improvements in
LAS/UAS. For Russian, we observe improvements
in POS and MOR, a small drop in LEM, and again
minor improvements in UAS/LAS. For Czech and
German, finally, we see a drop in MOR (and in LEM
for Czech and POS for German), while UAS/LAS is
largely unaffected. For German, this result can prob-
ably be explained largely by the fact that the mor-
phological descriptions in the lexicon are not fully
compatible with those in the treebank, as explained
in Section 3. Similarly, for Czech, we think the drop
in the LEM score is due to discrepancies caused by
updates in the dictionary version released in 2013,
deviating from the previously published treebank.

In general, the LEXSOFT model performs consid-
erably better, achieving the best results so far for
most languages and metrics. The only clear ex-
ception is Finnish, where it performs slightly worse
than LEXHARD (but better than all the other mod-
els). In addition, there is a marginal drop in POS
and LAS/UAS for Russian and in UAS for Hungar-
ian (but again only compared to LEXHARD). The
results are particularly striking for German, where
the soft lexical constraints are clearly beneficial (es-
pecially for the MOR score) despite not being quite
compatible with the morphological descriptions in
the training set. In terms of statistical signifance,
LEXSOFT outperforms the JOINT model with re-
spect to the PMD score for all languages (p < 0.01).
It is also significantly better than LEXHARD for all
languages except Finnish (p < 0.01).

6 Word Clusters

Finally, we add word cluster features to the best
model for each language (LEXHARD for Finnish,
LEXSOFT for the others).11 We use Brown clus-
ters (Brown et al., 1992), with 800 clusters for all
languages, and we use the same feature represen-
tation as Bohnet and Nivre (2012). The results in
Table 2 show small but consistent improvements
in almost all metrics for all languages, confirming
the benefit of cluster features for morphologically
rich languages. It is worth noting that we see the
biggest improvement for Finnish, the language with
the smallest training set and therefore most likely to

11The best model was selected according to results on the dev
set (cross-validation on the training set for Finnish).

423



suffer from sparse data, where the syntactic accu-
racy improves substantially (LAS +0.6, UAS +0.5)
and lemmatization even more (LEM +1.0). We also
see a nice improvement in morphological accuracy
for German (MOR +0.6, PM +0.5), which may be
related to the lack of a compatible morphological
analyzer for this language or simply to the fact that
the clusters are derived from a much larger corpus
for German than for the other languages. The PMD
improvement is statistically significant for all lan-
guages except Finnish (p < 0.01).

7 Discussion

The experimental results generally support the con-
clusion that joint prediction of morphology and syn-
tax, where morphology includes rich morphologi-
cal features as well as basic part-of-speech tags, im-
proves both morphological and syntactic accuracy.
The effect is especially clear on the joint evalua-
tion metrics PM and PMD, which indicates that the
joint model produces more internally consistent rep-
resentations. However, we also see evidence that
the joint model may suffer from data sparsity, as in
the case of Finnish, where a model that only pre-
dicts part-of-speech tags jointly with dependency
relations achieve better accuracy on some metrics.
However, even in this case, the joint model has the
best results on the joint evaluation metrics.

The second conclusion that can be drawn from the
experiments is that the use of an external lexicon is
an effective way of mitigating the sparse data prob-
lem and thereby improving accuracy. In general,
however, it is more effective to add the lexical con-
straints in the form of features, or soft constraints,
than to apply them as hard constraints and discard all
analyses that are not licensed by the lexicon. In par-
ticular, this is a useful strategy when the lexical re-
source is not completely compatible with the annota-
tion in the training set, as seen in the case of German
and (to a lesser extent) Czech. The only exception to
this generalization is again Finnish, where the hard
constraint model works marginally better (except for
the MOR and PM metrics), which may again indi-
cate that the training set is too small to make opti-
mal use of the additional features. Still, the soft con-
straint model improves substantially over the models
without lexical resources also for Finnish.

Finally, our experiments confirm that features
based on distributional word clusters have a positive
impact on syntactic accuracy, but little or no impact
on morphological accuracy. This is consistent with
previous findings in the literature, mainly from En-
glish (Koo et al., 2008; Sagae and Gordon, 2009),
and it is interesting to see that it holds also for richly
inflected languages and when added on top of fea-
tures derived from external lexical resources.

One issue worth discussing is the choice to allow
the joint model to consider at most 2 tags and 2 mor-
phological descriptions per word, which may seem
overly restrictive and very close to a pipeline model.
As already mentioned, this was motivated by the re-
sults of Bohnet and Nivre (2012), which explored
higher values without seeing any improvements, as
well as by our own preliminary experiments. In an
attempt to shed further light on this issue, we com-
puted oracle scores for the LEXSOFT model, which
uses soft lexical constraints but no cluster features.
The oracle scores for POS and MOR tell us how of-
ten the correct analysis is actually included in the
input to the joint model, while the oracle scores for
UAS and LAS reports the score of the best depen-
dency tree present in the beam at termination. The
results, reported in Table 2, show that the oracle
scores are very high, especially for part-of-speech
tags (98.0–99.9) but also for morphological descrip-
tions (94.8–99.3). Hence, very few correct analyses
are pruned away when setting the kp and km param-
eters to 2, and increasing the search space further is
therefore unlikely to improve accuracy.

For further analysis, Table 2 reports the UAS/LAS
scores of the PIPELINE system when given gold
standard tags, morphological descriptions and lem-
mas as input.12 Viewing this as an upper bound
on improvements in parsing accuracy for the joint
models, and comparing with the LEXSOFT model,
which like PIPELINE does not use cluster features,
we see that joint prediction with (soft) lexical con-
straints gives an average error reduction of about
40% for UAS and about 32% for LAS, which is
substantial especially given that the error reduction
in the PM score (compared to the perfect morphol-
ogy underlyling the GOLD scores) is only about

12Finnish had to be excluded because gold standard morpho-
logical annotation exists only for a small subset of the treebank.

424



27.5%. It is also worth pointing out that these im-
provements come at a very modest cost in computa-
tional efficiency, as the run times for the LEXSOFT
model are on average only 15% higher than for the
PIPELINE model, despite having a 40% larger beam
size.13 Interestingly, however, for all languages the
LAS/UAS scores are actually higher for ORACLE
than for GOLD, indicating that the LEXSOFT model
has in its final beam dependency trees that are better
than the 1-best trees predicted with perfect morpho-
logical input and suggesting that there is room for
further improvement of the scoring model.

The final results obtained with joint prediction of
morphology and syntax, external lexical constraints,
and cluster features represent a new state of the art
for syntactic dependency parsing for all five lan-
guages. For Czech, the best previous UAS on the
standard train-test split of the PDT is 87.32, re-
ported by Koo et al. (2010) with a parser using
non-projective head automata and dual decomposi-
tion, while the best LAS is 78.82 LAS from Nilsson
et al. (2006), using a greedy arc-eager transition-
based system with pseudo-projective parsing. Our
best results are 1.7 percentage points better for UAS
(89.0) and almost 5 percentage points better for LAS
(83.7).14 For Finnish, the only previous results are
from Haverinen et al. (2013), who achieve 81.01
LAS and 84.97 UAS with the graph-based parser
of Bohnet (2010). We get substantial improvements
with 83.1 LAS and 86.6 UAS. We also improve
slightly over their best POS score, obtained with the
HunPos tagger (Halácsy et al., 2007) together with
the OMorFi analyzer (95.7 vs. 95.4). For German,
the best previous results on the same train-test split
are from Seeker and Kuhn (2012), using the graph-
based parser of Bohnet (2010) in a pipeline archi-
tecture. With the same evaluation setup as in this
paper, they achieve 91.50 LAS and 93.48 UAS –

13LEXSOFT averages 0.132 ms per sentence on an Intel i7-
3930K processor with 6 cores, against 0.112 ms for PIPELINE.

14It is worth noting that there are a number of more recent
parsing results for Czech, but they all use a different test set (and
often a different training set), usually from one of the CoNLL
shared tasks in 2006 (Buchholz and Marsi, 2006), 2007 (Nivre
et al., 2007) and 2009 (Hajič et al., 2009). For the 2009 data set,
the best results are 83.73 LAS and 88.82 UAS from Bohnet and
Nivre (2012), who use the SIMPLETAG model but with a beam
size of 80. In our setup, we outperform this model by 0.5 points
in both LAS and UAS.

in the original paper, they only report results with-
out punctuation – to be compared with 92.4 LAS
and 94.1 UAS for our best model.15 In addition,
our POS score of 98.4 is the highest reported for a
tagger trained only on the Tiger Treebank, outper-
forming the previous best from Bohnet and Nivre
(2012) by 0.3 percentage points. The only previ-
ous results on Hungarian using the same version of
the treebank are from Farkas et al. (2012), who re-
port 87.2 LAS and 90.1 UAS for the graph-based
parser of Bohnet (2010). Our best results improve
labeled accuracy by 2.1 points (89.3 LAS) and un-
labeled accuracy by 1.6 points (91.7 UAS), which is
again quite substantial. For Russian, Boguslavsky et
al. (2011) report 86.0 LAS and 90.0 UAS using the
rule-based ETAP-3 parser with an added statistical
model and joint morphological and syntactic disam-
biguation. The scores are not strictly comparable,
because we use a more recent version of the Syn-
TagRus treebank (May 2013 vs. April 2011), but our
results nevertheless show substantial improvements,
in particular for UAS (93.0) but also for LAS (88.0).

8 Concluding Remarks

We have presented the first system that performs
full morphological disambiguation and labeled non-
projective dependency parsing in a joint model, and
we have demonstrated its usefulness for parsing
richly inflected languages. A thorough empirical
investigation of joint prediction models, rule-based
lexical constraints, and distributional word clusters
has shown substantial improvements in accuracy for
five languages. In the future, we hope to conduct a
detailed error analysis for all languages, which may
give us more insight about the benefits of different
components and hopefully pave the way for further
improvements.

Acknowledgments
Work partly funded by the projects LM2010013 and
LH12093 of the MEYS of the Czech Republic and the
National Excellence Program of the State of Hungary
(TÁMOP 4.2.4. A/2-11-1-2012-0001).

15As in the case of Czech, there are many recent results for
German based on the CoNLL 2009 data sets, but the previous
best is with the SIMPLETAG model of Bohnet and Nivre (2012),
which we outperform by 0.5/0.3 points in LAS/UAS.

425



References
Ju. Apresian, I. Boguslavsky, L. Iomdin, A. Lazursky,

V. Sannikov, V. Sizov, and L. Tsinman. 2003. ETAP-3
linguistic processor: A full-fledged NLP implementa-
tion of the MTT. In Proceedings of the First Inter-
national Conference on Meaning-Text Theory, pages
279–288.

Igor Boguslavsky, Svetlana Grigorieva, Nikolai Grig-
oriev, Leonid Kreidlin, and Nadezhda Frid. 2000.
Dependency treebank for Russian: Concept, tools,
types of information. In Proceedings of the 18th In-
ternational Conference on Computational Linguistics
(COLING), pages 987–991.

Igor Boguslavsky, Ivan Chardin, Svetlana Grigorieva,
Nikolai Grigoriev, Leonid Iomdin, Leonid Kreidlin,
and Nadezhda Frid. 2002. Development of a depen-
dency treebank for Russian and its possible applica-
tions in NLP. In Proceedings of the 3rd International
Conference on Language Resources and Evaluation
(LREC), pages 852–856.

Igor Boguslavsky, Leonid Iomdin, Victor Sizov, Leonid
Tsinman, and Vadim Petrochenkov. 2011. Rule-based
dependency parser refined by empirical and corpus
statistics. In Proceedings of the International Confer-
ence on Dependency Linguistics, pages 318–327.

Alena Böhmová, Jan Hajič, Eva Hajičová, and Barbora
Hladká. 2003. The Prague Dependency Treebank: A
three-level annotation scenario. In Anne Abeillé, ed-
itor, Treebanks: Building and Using Parsed Corpora,
pages 103–127. Kluwer.

Bernd Bohnet and Jonas Kuhn. 2012. The best of
both worlds – a graph-based completion model for
transition-based parsers. In Proceedings of the 13th
Conference of the European Chpater of the Associa-
tion for Computational Linguistics (EACL), pages 77–
87.

Bernd Bohnet and Joakim Nivre. 2012. A transition-
based system for joint part-of-speech tagging and la-
beled non-projective dependency parsing. In Proceed-
ings of the 2012 Joint Conference on Empirical Meth-
ods in Natural Language Processing and Computa-
tional Natural Language Learning (EMNLP-CoNLL),
pages 1455–1465.

Bernd Bohnet. 2010. Top accuracy and fast dependency
parsing is not a contradiction. In Proceedings of the
23rd International Conference on Computational Lin-
guistics (COLING), pages 89–97.

Sabine Brants, Stefanie Dipper, Silvia Hansen, Wolfgang
Lezius, and George Smith. 2002. TIGER treebank.
In Proceedings of the 1st Workshop on Treebanks and
Linguistic Theories (TLT), pages 24–42.

Peter F. Brown, Vincent J. Della Pietra, Peter V. deSouza,
Jennifer C. Lai, and Robert L. Mercer. 1992. Class-

based n-gram models of natural language. Computa-
tional Linguistics, 18:467–479.

Sabine Buchholz and Erwin Marsi. 2006. CoNLL-X
shared task on multilingual dependency parsing. In
Proceedings of the 10th Conference on Computational
Natural Language Learning (CoNLL), pages 149–164.

Shay B. Cohen and Noah A. Smith. 2007. Joint morpho-
logical and syntactic disambiguation. In Proceedings
of the 2007 Joint Conference on Empirical Methods
in Natural Language Processing and Computational
Natural Language Learning (EMNLP-CoNLL), pages
208–217.

Shay B. Cohen, Carlos Gómez-Rodrı́guez, and Giorgio
Satta. 2011. Exact inference for generative probabilis-
tic non-projective dependency parsing. In Proceedings
of the 2011 Conference on Empirical Methods in Nat-
ural Language Processing, pages 1234–1245.

Michael Collins and Brian Roark. 2004. Incremental
parsing with the perceptron algorithm. In Proceed-
ings of the 42nd Annual Meeting of the Association for
Computational Linguistics (ACL), pages 112–119.

Michael Collins. 2002. Discriminative training meth-
ods for hidden markov models: Theory and experi-
ments with perceptron algorithms. In Proceedings of
the Conference on Empirical Methods in Natural Lan-
guage Processing (EMNLP), pages 1–8.

Brooke Cowan and Michael Collins. 2005. Morphology
and reranking for the statistical parsing of spanish. In
Proceedings of the Human Language Technology Con-
ference and the Conference on Empirical Methods in
Natural Language Processing (HLT/EMNLP), pages
795–802.

Koby Crammer, Ofer Dekel, Joseph Keshet, Shai Shalev-
Shwartz, and Yoram Singer. 2006. Online passive-
aggressive algorithms. Journal of Machine Learning
Research, 7:551–585.

Richárd Farkas, Veronika Vincze, and Helmut Schmid.
2012. Dependency parsing of hungarian: Baseline re-
sults and challenges. In Proceedings of the 13th Con-
ference of the European Chpater of the Association for
Computational Linguistics (EACL), pages 55–65.

Andrea Gesmundo, James Henderson, Paola Merlo, and
Ivan Titov. 2009. A latent variable model of syn-
chronous syntactic-semantic parsing for multiple lan-
guages. In Proceedings of the Thirteenth Confer-
ence on Computational Natural Language Learning
(CoNLL 2009): Shared Task, pages 37–42.

Yoav Goldberg and Michael Elhadad. 2013. Word seg-
mentation, unknown-word resolution, and morpholog-
ical agreement in a hebrew parsing system. Computa-
tional Linguistics, 39:121–160.

Yoav Goldberg and Reut Tsarfaty. 2008. A single gener-
ative model for joint morphological segmentation and

426



syntactic parsing. In Proceedings of the 46th Annual
Meeting of the Association for Computational Linguis-
tics (ACL), pages 371–379.

Jan Hajič and Barbora Hladká. 1998. Tagging Inflective
Languages: Prediction of Morphological Categories
for a Rich, Structured Tagset. In Proceedings of the
36th Annual Meeting of the Association for Compu-
tational Linguistics (ACL) and the 17th International
Conference on Computational Linguistics (COLING),
pages 483–490.

Jan Hajič, Barbora Vidova Hladka, Jarmila Panevová,
Eva Hajičová, Petr Sgall, and Petr Pajas. 2001. Prague
Dependency Treebank 1.0. LDC, 2001T10.

Jan Hajič, Massimiliano Ciaramita, Richard Johans-
son, Daisuke Kawahara, Maria Antònia Martı́, Lluı́s
Màrquez, Adam Meyers, Joakim Nivre, Sebastian
Padó, Jan Štěpánek, Pavel Straňák, Mihai Surdeanu,
Nianwen Xue, and Yi Zhang. 2009. The conll-
2009 shared task: Syntactic and semantic dependen-
cies in multiple languages. In Proceedings of the Thir-
teenth Conference on Computational Natural Lan-
guage Learning (CoNLL): Shared Task, pages 1–18.

Jan Hajič. 2000. Morphological tagging: Data vs. dic-
tionaries. In Proceedings of the First Meeting of the
North American Chapter of the Association for Com-
putational Linguistics (NAACL), pages 94–101.

Péter Halácsy, András Kornai, and Csaba Oravecz. 2007.
HunPos – an open source trigram tagger. In Proceed-
ings of the 45th Annual Meeting of the Association for
Computational Linguistics: Companion Volume Pro-
ceedings of the Demo and Poster Sessions, pages 209–
212.

Jun Hatori, Takuya Matsuzaki, Yusuke Miyao, and
Jun’ichi Tsujii. 2011. Incremental joint pos tagging
and dependency parsing in chinese. In Proceedings
of 5th International Joint Conference on Natural Lan-
guage Processing, pages 1216–1224.

Katri Haverinen, Jenna Nyblom, Timo Viljanen,
Veronika Laippala, Samuel Kohonen, Anna Missilä,
Stina Ojala, Tapio Salakoski, and Filip Ginter. 2013.
Building the essential resources for Finnish: the Turku
Dependency Treebank. Language Resources and
Evaluation.

Liang Huang and Kenji Sagae. 2010. Dynamic program-
ming for linear-time incremental parsing. In Proceed-
ings of the 48th Annual Meeting of the Association for
Computational Linguistics (ACL), pages 1077–1086.

Leonid Iomdin and Viktor Sizov. 2008. Lexicographer’s
companion: A user-friendly software system for en-
larging and updating high-profile computerized bilin-
gual dictionaries. In Lexicographic Tools and Tech-
niques. MONDILEX First Open Workshop, pages 42–
54.

Terry Koo, Xavier Carreras, and Michael Collins. 2008.
Simple semi-supervised dependency parsing. In Pro-
ceedings of the 46th Annual Meeting of the Association
for Computational Linguistics (ACL), pages 595–603.

Terry Koo, Alexander M. Rush, Michael Collins, Tommi
Jaakkola, and David Sontag. 2010. Dual decompo-
sition for parsing with non-projective head automata.
In Proceedings of the 2010 Conference on Empiri-
cal Methods in Natural Language Processing, pages
1288–1298.

Sandra Kübler, Ryan McDonald, and Joakim Nivre.
2009. Dependency Parsing. Morgan and Claypool.

Marco Kuhlmann, Carlos Gómez-Rodrı́guez, and Gior-
gio Satta. 2011. Dynamic programming algorithms
for transition-based dependency parsers. In Proceed-
ings of the 49th Annual Meeting of the Association for
Computational Linguistics (ACL), pages 673–682.

John Lee, Jason Naradowsky, and David A. Smith. 2011.
A discriminative model for joint morphological disam-
biguation and dependency parsing. In Proceedings of
the 49th Annual Meeting of the Association for Com-
putational Linguistics (ACL), pages 885–894.

Jens Nilsson, Joakim Nivre, and Johan Hall. 2006.
Graph transformations in data-driven dependency
parsing. In Proceedings of the 21st International Con-
ference on Computational Linguistics and the 44th An-
nual Meeting of the Association for Computational
Linguistics, pages 257–264.

Joakim Nivre, Johan Hall, Sandra Kübler, Ryan McDon-
ald, Jens Nilsson, Sebastian Riedel, and Deniz Yuret.
2007. The CoNLL 2007 shared task on dependency
parsing. In Proceedings of the CoNLL Shared Task of
EMNLP-CoNLL 2007, pages 915–932.

Joakim Nivre, Marco Kuhlmann, and Johan Hall. 2009.
An improved oracle for dependency parsing with
online reordering. In Proceedings of the 11th
International Conference on Parsing Technologies
(IWPT’09), pages 73–76.

Joakim Nivre. 2003. An efficient algorithm for pro-
jective dependency parsing. In Proceedings of the
8th International Workshop on Parsing Technologies
(IWPT), pages 149–160.

Joakim Nivre. 2004. Incrementality in deterministic de-
pendency parsing. In Proceedings of the Workshop on
Incremental Parsing: Bringing Engineering and Cog-
nition Together (ACL), pages 50–57.

Joakim Nivre. 2009. Non-projective dependency parsing
in expected linear time. In Proceedings of the Joint
Conference of the 47th Annual Meeting of the ACL and
the 4th International Joint Conference on Natural Lan-
guage Processing of the AFNLP (ACL-IJCNLP), pages
351–359.

427



Tommi A. Pirinen. 2011. Modularisation of finnish
finite-state language description – towards wide col-
laboration in open source development of a morpho-
logical analyser. In Proceedings of the 18th Nordic
Conference of Computational Linguistics (NODAL-
IDA), pages 299–302.

Kenji Sagae and Andrew S. Gordon. 2009. Clustering
words by syntactic similarity improves dependency
parsing of predicate-argument structures. In Proceed-
ings of the 11th International Conference on Parsing
Technologies (IWPT), pages 192–201.

Helmut Schmid, Arne Fitschen, and Ulrich Heid. 2004.
SMOR: A German computational morphology cover-
ing derivation, composition and inflection. In Pro-
ceedings of the 4th International Conference on Lan-
guage Resources and Evaluation (LREC), pages 1263–
1266.

Wolfgang Seeker and Jonas Kuhn. 2012. Making el-
lipses explicit in dependency conversion for a ger-
man treebank. In Proceedings of the 8th International
Conference on Language Resources and Evaluation
(LREC), pages 3132–3139.

Wolfgang Seeker, Bernd Bohnet, Lilja Øvrelid, and Jonas
Kuhn. 2010. Informed ways of improving data-driven
dependency parsing for german. In Coling 2010:
Posters, pages 1122–1130.

Johanka Spoustová and Miroslav Spousta. 2012. A high-
quality web corpus of czech. In Proceedings of the 8th
International Conference on Language Resources and
Evaluation (LREC 2012), pages 311–315.

Ivan Titov and James Henderson. 2007. A latent variable
model for generative dependency parsing. In Proceed-
ings of the 10th International Conference on Parsing
Technologies (IWPT), pages 144–155.

Viktor Trón, Péter Halácsy, Péter Rebrus, András Rung,
Eszter Simon, and Péter Vajda. 2006. Morphdb.hu:
Hungarian lexical database and morphological gram-
mar. In Proceedings of the 5th International Confer-
ence on Language Resources and Evaluation (LREC),
pages 1670–1673.

Reut Tsarfaty, Djamé Seddah, Yoav Goldberg, San-
dra Kuebler, Yannick Versley, Marie Candito, Jen-
nifer Foster, Ines Rehbein, and Lamia Tounsi. 2010.
Statistical parsing of morphologically rich languages
(spmrl) what, how and whither. In Proceedings of the
NAACL HLT 2010 First Workshop on Statistical Pars-
ing of Morphologically-Rich Languages, pages 1–12.

Reut Tsarfaty, Djamé Seddah, Sandra Kübler, and Joakim
Nivre. 2013. Parsing morphologicall rich languages:
Introduction to the special issue. Computational Lin-
guistics, 39:15–22.

Reut Tsarfaty. 2006. Integrated morphological and syn-
tactic disambiguation for modern hebrew. In Pro-

ceedings of the COLING/ACL 2006 Student Research
Workshop, pages 49–54.

Tamás Váradi. 2002. The hungarian national corpus.
In Proceedings of the 3rd International Conference on
Language Resources and Evaluation (LREC), pages
385–389.

Hiroyasu Yamada and Yuji Matsumoto. 2003. Statisti-
cal dependency analysis with support vector machines.
In Proceedings of the 8th International Workshop on
Parsing Technologies (IWPT), pages 195–206.

Yue Zhang and Stephen Clark. 2008. A tale of two
parsers: Investigating and combining graph-based and
transition-based dependency parsing. In Proceedings
of the Conference on Empirical Methods in Natural
Language Processing (EMNLP), pages 562–571.

Yue Zhang and Joakim Nivre. 2011. Transition-based
parsing with rich non-local features. In Proceedings of
the 49th Annual Meeting of the Association for Com-
putational Linguistics (ACL).

Yue Zhang and Joakim Nivre. 2012. Analyzing the ef-
fect of global learning and beam-search on transition-
based dependency parsing. In Proceedings of COL-
ING 2012: Posters, pages 1391–1400.

428


