



















































Automatic Correction of Arabic Text: a Cascaded Approach


Proceedings of the EMNLP 2014 Workshop on Arabic Natural Langauge Processing (ANLP), pages 132–136,
October 25, 2014, Doha, Qatar. c©2014 Association for Computational Linguistics

Automatic Correction of Arabic Text: a Cascaded Approach

Hamdy Mubarak, Kareem Darwish
Qatar Computing Research Institute

Qatar Foundation
{hmubarak,kdarwish}@qf.org.qa

Abstract

This paper describes the error correction model that
we used for the Automatic Correction of Arabic Text
shared task. We employed two correction mod-
els, namely a character-level model and a case-
specific model, and two punctuation recovery mod-
els, namely a simple statistical model and a CRF
model. Our results on the development set suggest
that using a cascaded correction model yields the
best results.

1 Introduction

In This paper, we describe our system for auto-
matic Arabic error correction shared task (QALB-
2014 Shared Task on Automatic Correction of Ara-
bic) as part of the Arabic NLP workshop (Mohit
et al., 2014). Our system is composed of two main
steps. The first involves correcting word level er-
rors, and the second pertains to performing punctu-
ation recovery. For word level correction, we used
two approaches, namely: 1) a statistical character
level transformation model that is aided by a lan-
guage model (LM) to handle letter insertions, dele-
tions, and substitutions and word merges; and 2) a
case-specific system that is aided by a LM to han-
dle specific error types such as dialectal word sub-
stitutions and word splits. For punctuation recovery,
we used two approaches, namely a simple statistical
word-based system, and a conditional random fields
(CRF) sequence labeler (Lafferty et al., 2001) that
attempts to recover punctuation based on POS and
word sequences. We performed all experiments on
the QALB dataset (Zaghouani et al., 2014).

2 Word Error Correction

In this section we describe two approaches for word
correction. The first approach involves using a char-
acter level model, and the second handles specific
correction cases.

2.1 Character-level Correction Model
For the character level model, we treated correction
as a Transliteration Mining (TM) task. In TM, a
sequence in a source alphabet is used to find the
most similar sequence in a lexicon that is written
in a target alphabet. TM has been fairly well stud-
ied with multiple evaluation campaigns such as the
Named Entities Workshop (NEWS) (Zhang et al.,
2011; Zhang et al., 2012). In our work, we adopted
a TM system to find corrections appearing in a large
Arabic corpus. The system involved learning char-
acter (or character-sequence) level mappings be-
tween erroneous words and their correct counter-
parts. Given the character mappings between the
erroneous and correct words, we used a generative
model that attempts to generate all possible map-
pings of a source word while restricting the out-
put to words in the target language (El-Kahki et
al., 2011; Noeman and Madkour, 2010). Specifi-
cally, we used the baseline system of El-Kahky et
al. (2011). To train character-level mappings, we
extracted all the parallel word-pairs in the original
(uncorrected) and corrected versions in the training
set. If a word in the original version of the training
set was actually correct, the word would be mapped
to itself. We then aligned the parallel word pairs at
character level using GIZA++ (Och and Ney, 2003),
and symmetrized the alignments using grow-diag-

132



final-and heuristic (Koehn et al., 2007). In all, we
aligned a little over one million word pairs. As in the
baseline of El-Kahki et al. (2011), given a possibly
misspelled word worg, we produced all its possible
segmentations along with their associated mappings
that we learned during alignment. Valid target se-
quences were retained and sorted by the product of
the constituent mapping probabilities. The top n (we
picked n = 10) candidates, wtrg1..n with the highest
probability were generated. Using Bayes rule, we
computed:

argmax
wtrgi∈1..n

p(wtrgi |worg) = p(worg|wtrgi)p(wtrgi)
(1)

where p(worg|wtrgi) is the posterior probability of
mapping, which is computed as the product of the
mappings required to generate worg from wtrgi ,
and p(wtrgi) is the prior probability of the word.
Then we used a trigram LM to pick the most likely
candidate in context. We used a linear combination
of the the character-level transformation probability
and the LM probability using the following formula:
score = λlog(ProbLM ) + (1− λ)log(Probchar)
We built the lexicon from a set of 234,638 Aljazeera
articles1 that span 10 years and all of Arabic
Wikipedia. We also built a trigram language
model on the same corpus. The combined corpus
contains 576 million tokens including 1.6 million
unique ones. Spelling mistakes in Aljazeera arti-
cles (Mubarak et al., 2010) and Wikipedia were
infrequent.

We varied the value of λ between 0 and 1 with in-
crements of 0.1 and found that the values 0.6 and 0.7
yielded the best results. This indicates that LM prob-
ability is more important than character-mapping
probability.

2.2 Case-specific Correction

In this method we attempted to address specific
types of errors that are potentially difficult for the
character-based model to handle. Some of these er-
rors include dialectal words and words that were er-
roneously split. Before applying any correction, we
consulted a bigram LM that was trained the afore-
mentioned set of Aljazeera articles. The following

1http://www.aljazeera.net

cases are handled (in order):

• Switching from English punctuations to Arabic
ones, namely changing: “?” → “?” and “,”→ “,”.
• Handling common dialectal words and common

word-level mistakes. An example dialectal word is
ú
ÎË@ (Ally)

2 (meaning “this” or “that”) which could

be mapped to ø

	Y Ë@ (Al∗y) , ú


�æ Ë@ (Alty) or 	áK

	Y Ë @

(Al∗yn). An example of a common mistake is ZA � 	� @
éÊË @ (An$A’ Allh) (meaning “God willing”) which is
corrected to é Ê Ë @ Z A � 	à@ (>n $A’ Allh). The sen-
tence is scored with and without the word replace-
ment, and the replacement is done if it yields higher
LM probability.

•Handling errors pertaining to the different forms
of alef, alef maqsoura and ya, and ta marbouta
and ha (Nizar Habash, 2010). We reimplemented
the baseline system in (Moussa et al., 2012) where
words are normalized and the different possible de-
normalized forms are scored in context using the
LM. We also added the following cases, namely at-
tempting to replace: ð' (&) with ð ð' (&w) or ñ K'
(}w); and ø' (}) with Zø
 (y’) or vice versa (ex:
 ðQÓ (mr&s)→ ð ðQÓ (mr&ws)).
• Handling merges and splits. Often words are

concatenated erroneously. Thus, we attempted to
split all words that were at least 5 letters long af-
ter letters that don’t change their shapes when they
are connected to the letters following them, namely
different alef forms, X (d), 	X (*), P (r), 	P (z), ð (w), �è
(p), and ø (Y) (ex: A 	JK. PAK
 (yArbnA)→ A 	JK. P AK
 (yA
rbnA)). If the bigram was observed in the LM and
the LM score was higher (in context) than when they
were concatenated, then the word was split. Con-
versely, some words were split in the middle. We
attempted to merge every two words in sequence.
If the LM score was higher (in context) after the
merge, then the two words would be merged (ex:

2Buckwalter transiteration

133



�H@ PA�J 	K @ (AntSAr At)→ �H@PA�J 	K @ (AntSArAt)).

• Removing repeated letters. Often people repeat
letters, particularly long vowels, for emphasis as in
@ @ @Q�
 J
 J
 J
 	k


@ (>xyyyyrAAA) (meaning “at last”). We

corrected for elongation in a manner similar to that
of Darwish et al. (Darwish et al., 2012). When a
long vowel are repeated, we replaced it with a either
the vowel (ex. @Q�
 	g


@) (>xyrA) or the vowel with one

repetition (ex. @Q�
 J
 	k

@) (>xyyrA) and scored using

the LM. If a repeated alef appeared in the beginning
of the word, we attempted to replace it with alef lam
(ex. �èPA 	 k@@ (AAHDArp) → �èPA 	 mÌ'@ (AlHDArp)
(meaning “civilization”)). A trailing alef-hamza-
alef sequence was replaced by alef-hamza (ex. @ Z AÖÞ
(smA’A)→ ZAÖÞ (smA’) (meaning “sky”)).

• Correcting out-of-vocabulary words. For words
that were not observed in the LM, we attempted the
following corrections: 1) replacing phonetically or
visually confusable letters, namely 	 (D) and 	 
(Z), X (d) and 	X (*), and 	X (*) and 	P; (z) (ex: ¡�. A 	£
(ZAbT) → ¡ �. A 	 (DAbT)) 2) removing the letters
H. (b) and X (d) that are added to verbs in present
tense in some dialects (ex: I. �JºJ
K. (byktb)→ I. �JºK

(yktb)); 3) replacing the letters h (H) and è (h),
which are added in some dialects to indicate future
tense, with  (s) (ex: H. Qå�J
k (Hy$rb)→ H. Qå�J

(sy$rb)); and 4) replacing a leading ÈAë (hAl) with
either È@ @ 	Y ë (h*A Al) or È@ è 	Y ë (h*h Al) (ex.
H. A�JºËAë (hAlktAb)→ H. A�JºË@ @

	Yë (h*A AlktAb))
and the leading ÈA« (EAl) with È@ úÎ« (ElY Al) (ex.

	P

BA « (EAl>rD) → 	P


B@ úÎ « (ElY Al>rD)).

After replacement, the LM was always consulted.

2.3 Correction Results

Table 1 reports on the results of performing both cor-
rection methods on the development set. Also, since

Method F-measure
Character-level 0.574
Case-specific 0.587
Character-level→ Case-specific 0.615
Case-specific→ Character-level 0.603

Table 1: The correction results using the character-level
model, case-specific correction, or their cascades.

the case-specific corrections handle cases that were
not handled by the character-level model, we at-
tempted to cascade both methods together. It seems
that when applying the character-level model first
followed by the case-specific correction yielded the
best results.

3 Punctuation Recovery

In this section, we describe two methods for punc-
tuation recovery. The first is a simple word-based
model and the other is a CRF based model.

3.1 Simple Statistical Model

In this approach, we identified words that were pre-
ceded or followed by punctuations in the training
set. If a word was preceded or followed by a par-
ticular punctuation mark more than 40% of the time,
then we automatically placed the punctuation before
or after the word in the dev set. Also, if a sentence
did not have a period at the end of it, we added a
period.

3.2 CRF Model

In this approach we trained a CRF sequence labeling
to attempt to recover punctuation. CRF combines
state and transition level features making it a pos-
sibly better choice than an HMM or a simple clas-
sifier. We used the CRF++ implementation3 of the
sequence labeler. We trained the labeler on the train-
ing part of the QALB dataset. We used the following
features:
Word features: the current word, the previous and
next words, and the two previous and two next
words.
Part-of-speech (POS) tags: the POS of the current

3 http://crfpp.googlecode.com/svn/trunk/
doc/index.html

134



Method Precision Recall F-measure
Stat model 0.306 0.153 0.204
CRF model 0.373 0.141 0.204

Table 2: The punctuation recovery results using the sim-
ple statistical model and the CRF model.

Method F-measure
Stat model (before correction) 0.593
Stat model (after correction) 0.614
CRF model (before correction) 0.607
CRF model (after correction) 0.615

Table 3: Cascaded correction (Character-level → Case-
specific) combined with punctuation recovery.

word and the POS of the two previous and two fol-
lowing words.

3.3 Punctuation Recovery Results
Table 2 reports on the results of using the two differ-
ent methods for punctuation recovery. Note that no
other correction is applied.

4 Combining Correction with Punctuation
Recovery

Given that cascading both correction models yielded
the best results, we attempted to combine the cas-
caded correction model with the two punctuation re-
covery methods. We tried to put punctuation recov-
ery before and after correction. Table 3 summarizes
the results. As the results suggest, combining cor-
rection with punctuation recovery had a negative ef-
fect on overall F-measure. This requires further in-
vestigation.

5 Official Shared Task Experiments and
Results

For the official submissions to the shared task, we
submitted 3 runs as follows:

1. QCRI-1: character-level correction, then case-
based correction.

2. QCRI-2: case-based correction, then statistical
punctuation recovery

3. QCRI-3: exactly like 2, but preceded also by
statistical punctuation recovery

Run Precision Recall F-measure
QCRI-1 0.717 0.5686 0.6343
QCRI-2 0.6286 0.6032 0.6157
QCRI-3 0.6066 0.5928 0.5996

Table 4: Official Results.

Table 4 reports on the officially submitted results
against the test set. It seems that our attempts to add
punctuation recovery worsened results.

6 Conclusion

In this paper, we presented automatic approaches
for correcting Arabic text and punctuation recovery.
Our results on the development set shows that using
a cascaded approach that involves a character-level
model and another model that handles specific errors
yields the best results. Incorporating punctuation re-
covery did not improve correction.

References
Kareem Darwish, Walid Magdy, and Ahmed Mourad.

2012. Language processing for arabic microblog re-
trieval. Proceedings of the 21st ACM international
conference on Information and knowledge manage-
ment. ACM, 2012.

Ali El-Kahky, Kareem Darwish, Ahmed Saad Aldein,
Mohamed Abd El-Wahab, Ahmed Hefny, and Waleed
Ammar. 2001. Improved transliteration mining using
graph reinforcement. In Proceedings of the Conference
on Empirical Methods in Natural Language Process-
ing, pp. 1384-1393, 2011.

Nizar Habash. 2010. Introduction to Arabic natural lan-
guage processing. Synthesis Lectures on Human Lan-
guage Technologies 3.1 (2010): 1-187

Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran, Richard
Zens, Chris Dyer, Ondrej Bojar, Alexandra Con-
stantin, Evan Herbst, Moses: Open Source Toolkit
for Statistical Machine Translation, Annual Meeting of
the Association for Computational Linguistics (ACL),
demonstration session, Prague, Czech Republic, June
2007.

J. Lafferty, A. McCallum, and F. Pereira. 2001. Con-
ditional random fields: Probabilistic models for seg-
menting and labeling sequence data, In Proc. of ICML,
pp.282-289, 2001.

Behrang Mohit, Alla Rozovskaya, Nizar Habash, Wajdi
Zaghouani, and Ossama Obeid, 2014. The First QALB

135



Shared Task on Automatic Text Correction for Arabic.
In Proceedings of EMNLP workshop on Arabic Natu-
ral Language Processing. Doha, Qatar.

Mohammed Moussa, Mohamed Waleed Fakhr, and Ka-
reem Darwish. 2012. Statistical denormalization for
Arabic Text. In Empirical Methods in Natural Lan-
guage Processing, pp. 228. 2012.

Hamdy Mubarak, Ahmed Metwali, Mostafa Ramadan.
2010. Spelling Mistakes in Arabic Newspapers. Arabic
Language and Scientific Researches conference, Fac-
ulty of Arts, Ain Shams University, Cairo, Egypt

Sara Noeman and Amgad Madkour. 2010. Language In-
dependent Transliteration Mining System Using Finite
State Automata Framework. ACL NEWS workshop
2010.

Franz J. Och and Hermann Ney. 2003. A Systematic
Comparison of Various Statistical Alignment Models.
Computational Linguistics, Vol. 1(29), 2003.

Wajdi Zaghouani, Behrang Mohit, Nizar Habash, Os-
sama Obeid, Nadi Tomeh, Alla Rozovskaya, Noura
Farra, Sarah Alkuhlani, and Kemal Oflazer. 2014.
Large Scale Arabic Error Annotation: Guidelines and
Framework. In Proceedings of the Ninth International
Conference on Language Resources and Evaluation
(LREC14), Reykjavik, Iceland.

Min Zhang, A Kumaran, Haizhou Li. 2011. Whitepaper
of NEWS 2012 Shared Task on Machine Translitera-
tion. IJCNLP-2011 NEWS workshop.

Min Zhang, Haizhou Li, Ming Liu, A Kumaran. 2012.
Whitepaper of NEWS 2012 Shared Task on Machine
Transliteration. ACL-2012 NEWS workshop.

136


