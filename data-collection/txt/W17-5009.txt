



















































Predicting Audience's Laughter During Presentations Using Convolutional Neural Network


Proceedings of the 12th Workshop on Innovative Use of NLP for Building Educational Applications, pages 86–90
Copenhagen, Denmark, September 8, 2017. c©2017 Association for Computational Linguistics

Predicting Audience’s Laughter During Presentations Using
Convolutional Neural Network

Lei Chen
Educational Testing Service (ETS)

Princeton, NJ USA
LChen@ets.org

Chong Min Lee
Educational Testing Service (ETS)

Princeton, NJ USA
CLee001@ets.org

Abstract

Public speakings play important roles in
schools and work places and properly us-
ing humor contributes to effective pre-
sentations. For the purpose of automat-
ically evaluating speakers’ humor usage,
we build a presentation corpus containing
humorous utterances based on TED talks.
Compared to previous data resources sup-
porting humor recognition research, ours
has several advantages, including (a) both
positive and negative instances coming
from a homogeneous data set, (b) con-
taining a large number of speakers, and
(c) being open. Focusing on using lexical
cues for humor recognition, we systemati-
cally compare a newly emerging text clas-
sification method based on Convolutional
Neural Networks (CNNs) with a well-
established conventional method using lin-
guistic knowledge. The advantages of the
CNN method are both getting higher de-
tection accuracies and being able to learn
essential features automatically.

1 Introduction

The ability to make effective presentations has
been found to be linked with success at school and
in the workplace (Hill and Storey, 2003; Stevens,
2005). Humor plays an important role in success-
ful public speaking, e.g., helping to reduce pub-
lic speaking anxiety often regarded as the most
prevalent type of social phobia, generating shared
amusement to boost persuasive power, and serv-
ing as a means to attract attention and reduce ten-
sion (Xu, 2016).

Automatically simulating an audience’s reac-
tions to humor will not only be useful for presenta-
tion training, but also improve conversational sys-

tems by giving machines more empathetic power.
The present study reports our efforts in recogniz-
ing utterances that cause laughter in presentations.
These include building a corpus from TED talks
and using Convolutional Neural Networks (CNNs)
in the recognition.

The remainder of the paper is organized as fol-
lows: Section 2 briefly reviews the previous re-
lated research; Section 3 describes the corpus we
collected from TED talks; Section 4 describes the
text classification methods; Section 5 reports on
our experiments; finally, Section 6 discusses the
findings of our study and plans for future work.

2 Previous Research

Humor recognition refers to the task of deciding
whether a sentence/spoken-utterance expresses a
certain degree of humor. In most of the previous
studies (Mihalcea and Strapparava, 2005; Puran-
dare and Litman, 2006; Yang et al., 2015), humor
recognition was modeled as a binary classification
task.

In the seminal work (Mihalcea and Strappar-
ava, 2005), a corpus of 16,000 “one-liners” was
created using daily joke websites to collect hu-
morous instances while using formal writing re-
sources (e.g., news titles) to obtain non-humorous
instances. Three humor-specific stylistic features,
including alliteration, antonymy, and adult slang
were utilized together with content-based features
to build classifiers. In a recent work (Yang et al.,
2015), a new corpus was constructed from the Pun
of the Day website. Yang et al. (2015) explained
and computed stylistic features based on the fol-
lowing four aspects: (a) Incongruity, (b) Ambi-
guity, (c) Interpersonal Effect, and (d) Phonetic
Style. In addition, Word2Vec (Mikolov et al.,
2013) distributed representations were utilized in
the model building.

86



Beyond lexical cues from text inputs, other
research has also utilized speakers’ acoustic
cues (Purandare and Litman, 2006; Bertero and
Fung, 2016b). These studies have typically used
audio tracks from TV shows and their corre-
sponding captions in order to categorize charac-
ters’ speaking turns as humorous or non-humorous
based on canned laughter.

Convolutional Neural Networks (CNNs) have
recently been successfully used in several text
categorization tasks (e.g., review rating, senti-
ment recognition, and question type recognition).
Kim (2014); Johnson and Zhang (2015); Zhang
and Wallace (2015) suggested that using a simple
CNN setup, which entails one layer of convolu-
tion on top of word embedding vectors, achieves
excellent results on multiple tasks. Deep learning
recently has been applied to computational humor
research (Bertero and Fung, 2016b,a). In Bertero
and Fung (2016b), CNN was found to be the best
model that uses both acoustic and lexical cues for
humor recognition. However, it did not outper-
form the Logistical Regression (LR) model when
using text inputs exclusively. Beyond treating hu-
mor detection as a binary classification task, Bert-
ero and Fung (2016a) formulated the recognition
to be a sequential labeling task and utilized Re-
current Neural Networks (RNNs) (Hochreiter and
Schmidhuber, 1997) on top of CNN models (serv-
ing as feature extractors) to utilize context infor-
mation among utterances.

From the brief review, it is clear that there is
a great need for an open corpus that can sup-
port investigating humor in presentations.1 CNN-
based text categorization methods have been ap-
plied to humor recognition (e.g., in (Bertero and
Fung, 2016b)) but with limitations: (a) a rigorous
comparison with the state-of-the-art conventional
method examined in Yang et al. (2015) is missing;
(b) CNN’s performance in the previous research
is not quite clear; and (c) some important tech-
niques that can improve CNN performance (e.g.,
using varied-sized filters and dropout regulariza-
tion (Hinton et al., 2012)) were not applied. There-
fore, the present study is meant to address these
limitations.

1While we were working on this paper, we found a recent
Master’s thesis (Acosta, 2016) that also conducted research
on detecting laughter on the TED transcriptions. However,
that study only explored conventional text classification ap-
proaches.

3 TED Talk Data

TED Talks2 are recordings from TED conferences
and other special TED programs. Many effects in
a presentation can cause audience laugh, such as
speaking content, presenters’ nonverbal behaviors,
and so on. In the present study, we focused on
the transcripts of the talks. Most transcripts of the
talks contain the markup ‘(Laughter)’, which rep-
resents where audiences laughed aloud during the
talks. This special markup was used to determine
utterance labels.

We collected 1,192 TED Talk transcripts3. An
example transcription is given in Figure 1. The
collected transcripts were split into sentences us-
ing the Stanford CoreNLP tool (Manning et al.,
2014). In this study, sentences containing or im-
mediately followed by ‘(Laughter)’ were used as
‘Laughter’ sentences, as shown in Figure 1; all
other sentences were defined as ‘No-Laughter’
sentences. Following Mihalcea and Strapparava
(2005) and Yang et al. (2015), we selected the
same numbers (n = 4726) of ‘Laughter’ and ‘No-
Laughter’ sentences. To minimize possible topic
shifts between positive and negative instances, for
each positive instance, we randomly picked one
negative instance nearby (the context window was
7 sentences in this study). For example, in Fig-
ure 1, a negative instance (corresponding to ‘sent-
2’) was selected from the nearby sentences rang-
ing from ‘sent-7’ to ‘sent+7’. More details about
this data set can refer to Lee et al. (2016). The
TED data set can be obtained by contacting the
authors.

4 Methods
4.1 Conventional Model

Following Yang et al. (2015), we applied Random
Forest (Breiman, 2001) to perform humor recog-
nition by using the following two groups of fea-
tures. The first group are humor-specific stylistic
features covering the following 4 categories4: In-
congruity (2), Ambiguity (6), Interpersonal Effect
(4), and Phonetic Pattern (4). The second group
are semantic distance features, including the hu-
mor label classes from 5 sentences in the training
set that are closest to the sentence being evalu-
ated (found by using a k-Nearest Neighbors (kNN)

2http://www.ted.com
3The transcripts were collected on 7/9/2015.
4The number in parenthesis indicates how many features

are in that category.

87



sent-7 . . .
. . . . . .
No-Laughter He has no memory of the past, no knowledge of the future, and he only cares about two

things: easy and fun.
sent-1 Now, in the animal world, that works fine.
Laughter If you’re a dog and you spend your whole life doing nothing other than easy and fun things,

you’re a huge success! (Laughter)
sent+1 And to the Monkey, humans are just another animal species.
. . . . . .
sent+7 . . .

Figure 1: An excerpt from TED talk “Tim Urban: Inside the mind of a master procrastinator” (http:
//bit.ly/2l1P3RJ)

method), and each sentence’s averaged Word2Vec
representations (n = 300). More details can be
found in Yang et al. (2015).

4.2 CNN model

Our CNN-based text classification’s setup follows
Kim (2014). Figure 2 depicts the model’s details.
From the left side’s input texts to the right side’s
prediction labels, different shapes of tensors flow
through the entire network for solving the classifi-
cation task in an end-to-end mode.

Firstly, tokenized text strings were converted to
a 2D tensor with shape (L × d), where L rep-
resents sentences’ maximum length while d rep-
resents the word-embedding dimension. In this
study, we utilized the Word2Vec (Mikolov et al.,
2013) embedding vectors (d = 300) that were
trained on 100 billion words of Google News.
Next, the embedding matrix was fed into a 1D
convolution network with multiple filters. To
cover varied reception fields, we used filters of
sizes of fw − 1, fw, and fw + 1. For each fil-
ter size, nf filters were utilized. Then, max pool-
ing, which stands for finding the largest value from
a vector, was applied to each feature map (to-
tal 3 × nf feature maps) output by the 1D con-
volution. Finally, maximum values from all of
3× nf filters were formed as a flattened vector to
go through a fully connected (FC) layer to predict
two possible labels (Laughter vs. No-Laughter).
Note that for 1D convolution and FC layer’s in-
put, we applied ‘dropout’ (Hinton et al., 2012)
regularization, which entails randomly setting a
proportion of network weights to be zero during
model training, to overcome over-fitting. By using
cross-entropy as the learning metric, the whole se-
quential network (all weights and bias) could be

optimized by using any SGD optimization, e.g.,
Adam (Kingma and Ba, 2014), Adadelta (Zeiler,
2012), and so on.

5 Experiments
We used two corpora: the TED Talk corpus (de-
noted as TED) and the Pun of the Day corpus5

(denoted as Pun). Note that we normalized words
in the Pun data to lowercase to avoid a possibly
elevated result caused by a special pattern: in the
original format, all negative instances started with
capital letters. The Pun data allows us to verify
that our implementation of the conventional model
is consistent with the work reported in Yang et al.
(2015).

In our experiment, we firstly divided each cor-
pus into two parts. The smaller part (the Dev
set) was used for setting various hyper-parameters
used in text classifiers. The larger portion (the
CV set) was then formulated as a 10-fold cross-
validation setup for obtaining a stable and com-
prehensive model evaluation result. For the PUN
data, the Dev contains 482 sentences, while the
CV set contains 4344 sentences. For the TED data,
the Dev set contains 1046 utterances, while the
CV set contains 8406 utterances. Note that, with
a goal of building a speaker-independent humor
detector, when partitioning our TED data set, we
always kept all utterances of a single talk within
the same partition.

When building conventional models, we de-
veloped our own feature extraction scripts and
used the SKLL6 python package for building Ran-
dom Forest models. When implementing CNN,

5The authors of Yang et al. (2015) kindly shared their data
with us. We would like to thank them for their generosity.

6https://github.com/
EducationalTestingService/skll

88



Figure 2: CNN network architecture

Acc. (%) F1 Precision Recall
Pun set

Chance 50.2 .498 .506 .497
Base 78.3 .795 .757 .839
CNN 86.1 .857 .864 .864

TED set
Chance 51.0 .506 .510 .503

Base 52.0 .595 .515 .705
CNN 58.9 .606 .582 .632

Table 1: Humor recognition on both Pun and TED
data sets by using (a) random prediction (Chance),
conventional method (Base) and CNN method

we used the Keras Python package7. Regarding
hyper-parameter tweaking, we utilized the Tree
Parzen Estimation (TPE) method as detailed in
Bergstra et al. (2013). After running 200 itera-
tions of tweaking, we ended up with the follow-
ing selection: fw is 6 (entailing that the vari-
ous filter sizes are (5, 6, 7)), nf is 100, dropout1
is 0.7 and dropout2 is 0.35, optimization uses
Adam (Kingma and Ba, 2014). When training the
CNN model, we randomly selected 10% of the
training data as the validation set for using early
stopping to avoid over-fitting.

On the Pun data, the CNN model shows consis-
tent improved performance over the conventional
model, as suggested in Yang et al. (2015). In par-
ticular, precision has been greatly increased from
0.762 to 0.864. On the TED data, we also ob-
served that the CNN model helps to increase pre-
cision (from 0.515 to 0.582) and accuracy (from
52.0% to 58.9%). The empirical evaluation results
suggest that the CNN-based model has an advan-
tage on the humor recognition task. In addition,
focusing on the system development time, gener-

7Our code implementation was based on
https://github.com/shagunsodhani/
CNN-Sentence-Classifier

ating and implementing those features in the con-
ventional model would take days or even weeks.
However, the CNN model automatically learns
its optimal feature representation and can adjust
the features automatically across data sets. This
makes the CNN model quite versatile for support-
ing different tasks and data domains. Compared
with the humor recognition results on the Pun data,
the results on the TED data are still quite low, and
more research is needed to fully handle humor in
authentic presentations.

6 Discussion

For the purpose of monitoring how well speak-
ers can use humor during their presentations, we
have created a corpus from TED talks. Com-
pared to the existing corpora, ours has the fol-
lowing advantages: (a) it was collected from au-
thentic talks, rather than from TV shows per-
formed by professional actors based on scripts; (b)
it contains about 100 times more speakers com-
pared to the limited number of actors in exist-
ing corpora. We compared two types of lead-
ing text-based humor recognition methods: a con-
ventional classifier (e.g., Random Forest) based
on human-engineered features vs. an end-to-end
CNN method, which relies on its inherent rep-
resentation learning. We found that the CNN
method has better performance. More importantly,
the representation learning of the CNN method
makes it very efficient when facing new data sets.

Stemming from the present study, we envision
that more research is worth pursuing: (a) for pre-
sentations, cues from other modalities such as au-
dio or video will be included, similar to Bertero
and Fung (2016b); (b) context information from
multiple utterances will be modeled by using se-
quential modeling methods.

89



References
Andrew D. Acosta. 2016. Laff-O-Tron: Laugh Predic-

tion in TED Talks. Master’s thesis, California Poly-
technic State University, San Luis Obispo, CA.

James Bergstra, Daniel Yamins, and David D Cox.
2013. Making a science of model search: Hyper-
parameter optimization in hundreds of dimensions
for vision architectures. ICML (1) 28:115–123.

D Bertero and P Fung. 2016a. A long short-term mem-
ory framework for predicting humor in dialogues. In
Proceedings of NAACL-HLT . San Diego, CA.

D Bertero and P Fung. 2016b. Deep learning of audio
and language features for humor prediction. In In-
ternational Conference on Language Resources and
Evaluation (LREC). Portoroz, Slovenia.

L Breiman. 2001. Random forests. Machine learning
45(1):5–32.

Monica Hill and Anne Storey. 2003. Speakeasy: on-
line support for oral presentation skills. ELT Journal
57(4):370–376.

GE Hinton, N Srivastava, and A Krizhevsky. 2012.
Improving neural networks by preventing co-
adaptation of feature detectors. arXiv preprint
arXiv:1207.0580 .

Sepp Hochreiter and Jürgen Schmidhuber. 1997.
Long Short-Term Memory. Neural Computation
9(8):1735–1780.

R Johnson and T Zhang. 2015. Semi-supervised con-
volutional neural networks for text categorization
via region embedding. Advances in neural informa-
tion processing .

Y Kim. 2014. Convolutional neural networks for sen-
tence classification. In Proc. EMNLP. Doha, Qatar,
pages 1746–1751.

D Kingma and J Ba. 2014. Adam: A method
for stochastic optimization. arXiv preprint
arXiv:1412.6980 .

Chong Min Lee, Su-Youn Yoon, and Lei Chen. 2016.
Can we make computers laugh at talks? In Proceed-
ings of the Workshop on Computational Modeling
of People’s Opinions, Personality, and Emotions in
Social Media (PEOPLES). The COLING 2016 Or-
ganizing Committee, Osaka, Japan, pages 173–181.

Christopher D. Manning, Mihai Surdeanu, John Bauer,
Jenny Finkel, Steven J. Bethard, and David Mc-
Closky. 2014. The Stanford CoreNLP natural lan-
guage processing toolkit. In Association for Compu-
tational Linguistics (ACL) System Demonstrations.
pages 55–60.

Rada Mihalcea and Carlo Strapparava. 2005. Making
computers laugh: Investigations in automatic humor
recognition. In Proceedings of Human Language

Technology Conference and Conference on Empir-
ical Methods in Natural Language Processing. As-
sociation for Computational Linguistics, Vancouver,
British Columbia, Canada, pages 531–538.

T Mikolov, I Sutskever, and K Chen. 2013. Distributed
representations of words and phrases and their com-
positionality. Advances in neural information pro-
cessing systems .

Amruta Purandare and Diane J. Litman. 2006. Hu-
mor: Prosody analysis and automatic recognition for
F*R*I*E*N*D*S*. In EMNLP. Sydney, Australia.

Betsy Stevens. 2005. What communication skills do
employers want? silicon valley recruiters respond.
Journal of Employment Counseling 42(1):2.

Z Xu. 2016. Laughing Matters: Humor Strategies in
Public Speaking. Asian Social Science 12(1):117.

Diyi Yang, Alon Lavie, Chris Dyer, and Eduard Hovy.
2015. Humor recognition and humor anchor extrac-
tion. In Proceedings of the 2015 Conference on Em-
pirical Methods in Natural Language Processing.
Association for Computational Linguistics, Lisbon,
Portugal, pages 2367–2376.

MD Zeiler. 2012. ADADELTA: an adaptive learning
rate method. arXiv preprint arXiv:1212.5701 .

Y Zhang and B Wallace. 2015. A sensitivity anal-
ysis of (and practitioners’ guide to) convolutional
neural networks for sentence classification. In
arXiv:1510.03820.

90


