



















































Morphological Modeling for Machine Translation of English-Iraqi Arabic Spoken Dialogs


Human Language Technologies: The 2015 Annual Conference of the North American Chapter of the ACL, pages 995–1000,
Denver, Colorado, May 31 – June 5, 2015. c©2015 Association for Computational Linguistics

Morphological Modeling for Machine Translation of English-Iraqi Arabic
Spoken Dialogs

Katrin Kirchhoff
Department of

Electrical Engineering
University of Washington

Seattle, WA, USA
kk2@u.washington.edu

Wilson Tam
Microsoft Corporation

wilson.yctam@gmail.com

Colleen Richey, Wen Wang
SRI International

Menlo Park, CA, USA
colleen@speech.sri.com
wwang@speech.sri.com

Abstract

This paper addresses the problem of mor-
phological modeling in statistical speech-to-
speech translation for English to Iraqi Ara-
bic. An analysis of user data from a real-time
MT-based dialog system showed that generat-
ing correct verbal inflections is a key problem
for this language pair. We approach this prob-
lem by enriching the training data with mor-
phological information derived from source-
side dependency parses. We analyze the per-
formance of several parsers as well as the ef-
fect on different types of translation models.
Our method achieves an improvement of more
than a full BLEU point and a significant in-
crease in verbal inflection accuracy; at the
same time, it is computationally inexpensive
and does not rely on target-language linguistic
tools.

1 Introduction

SMT from a morphologically poor language like En-
glish into a language with richer morphology con-
tinues to be a problem, in particular when training
data is sparse and/or the SMT system has insufficient
modeling capabilities for morphological variation
in the target language. Most previous approaches
to this problem have utilized a translate-and-inflect
method, where a first-pass SMT system is trained
on lemmatized forms, and the correct inflection for
every word is predicted in a second pass by statis-
tical classifiers trained on a combination of source
and target language features. This paper looks at
morphological modeling from a different perspec-
tive, namely to improve SMT in a real-time speech-

to-speech translation system. Our focus is on resolv-
ing those morphological translation errors that are
most likely to cause confusions and misunderstand-
ings in machine-translation mediated human-human
dialogs. Due to the constraints imposed by a real-
time system, previous approaches that rely on elabo-
rate feature sets and multi-pass processing strategies
are unsuitable for this problem. The language pair
of interest in this study is English and Iraqi Arabic
(IA). The latter is a spoken dialect of Arabic with
few existing linguistic resources. We therefore de-
velop a low-resource approach that relies on source-
side dependency parses only. We analyze its perfor-
mance in combination with different types of parsers
and different translation models. Results show a sig-
nificant improvement in translation performance in
both automatic and manual evaluations. Moreover,
the proposed method is sufficiently fast for a real-
time system.

2 Prior Work

Much work in SMT has addressed the issue of
translating from morphologically-rich languages by
preprocessing the source and/or target data by
e.g., stemming and morphological decomposition
(Popovic and Ney, 2004; Goldwater and McClosky,
2005), compound splitting (Koehn and Knight,
2003), or various forms of tokenization (Lee, 2004;
Habash and Sadat, 2006). In (Minkov et al., 2007;
Toutanova et al., 2008) morphological generation
was applied as a postprocessing step for translation
into morphologically-rich languages. A maximum-
entropy Markov model was trained to predict the
correct inflection for every stemmed word in the

995



machine translation output from a first-pass sys-
tem, conditioned on a set of lexical, morphological
and syntactic features. More recently, (Chahuneau
et al., 2013) applied a similar translate-and-inflect
approach, utilizing unsupervised in addition to su-
pervised morphological analyses. Inflection gen-
eration models were also used by (Fraser et al.,
2012; Weller et al., 2013) for translation into Ger-
man, and by (El Kholy and Habash, 2012) for Mod-
ern Standard Arabic. (Sultan, 2011) added both
syntactic information on the source side that was
used in filtering the phrase table, plus postprocess-
ing on the target side for English-Arabic translation.
Still other approaches enrich the translation system
with morphology-aware feature functions or specific
agreement models (Koehn and Hoang, 2007; Green
and DeNero, 2012; Williams and Koehn, 2011).

In contrast to the above studies, which have con-
centrated on text translation, this paper focuses
on spoken language translation within a bilingual
human-human dialog system. Thus, our main goal
is not to predict the correct morphological form of
every word, but to prevent communication errors re-
sulting from the mishandling of morphology. The
intended use in a real-time dialog system imposes
additional constraints on morphological modeling:
any proposed approach should not add a signifi-
cant computational burden to the overall system that
might result in delays in translation or response gen-
eration. Our goal is also complicated by the fact that
our target language is a spoken dialect of Arabic, for
which few linguistic resources (training data, lexi-
cons, morphological analyzers) exist. Lastly, Arabic
written forms are morphologically highly ambigu-
ous due to the lack of short vowel markers that signal
grammatical categories.

3 Dialog System and Analysis

The first step in the dialog system used for this study
consists of an automatic speech recognition (ASR)
component that produces ASR hypotheses for the
user’s speech input. Several error detection modules
then identify likely out-of-vocabulary and misrecog-
nized words. This information is used by a clarifi-
cation module that asks the user to rephrase these
error segments; another module then combines the
user’s answers into a merged, corrected representa-

1	  

LanguageA	  
ASR	  

ASR	  Error	  
Detec1on	  

OOV/Name	  
Detec1on	  

Dialog	  Manager	  	  

LanguageB	  
TTS	  

LanguageA	  
TTS	  

Answer	  
Merging	  	  

Error	  
Processing	  

Clarify	  

User	  A	  

User	  B	  

Needs	  
Clarifica-­‐
1on?	  

Play	  Transla2on	  

SLU	  

Sense	  
Detec1on	   MT	  

MT	  Error	  
Detec1on	  

Parser	  

Figure 1: Dialog system used in this work.

tion before sending it to the translation engine. A
machine translation error detection module analyzes
the translation to check for errors, such as unknown
words. If an error is found, another clarification sub-
dialog is initiated; otherwise, the translation is sent
to a text-to-speech engine to produce the acoustic
output in the other language. A schematic represen-
tation is shown in Figure 1. More details about the
system can be found in (et al., 2013). The system
was evaluated in live mode with native IA speakers
as part of the DARPA BOLT Phase-II benchmark
evaluations. The predefined scenarios included mil-
itary and humanitarian assistance/disaster relief sce-
narios as well as general topics. All system interac-
tions were logged and evaluated by bilingual human
assessors.

During debriefing sessions with the users, some
users voiced dissatisfaction with the translation
quality, and a subsequent detailed error analysis was
conducted on the logs of 30 interactions. Similar
to previous studies (Condon et al., 2010) we found
that a frequently recurring problem was wrong mor-
phological verb forms in the IA output. Some ex-
amples are shown in Table 1. In Example 1, to
make sure should be translated by a first-person plu-
ral verb but it is translated by a second-person plural
form, changing the meaning to (you (pl.) make sure).
The desired verb form would be ntAkd. Similarly, in
Example 2 the translation of transport should agree
with the translations of someone and the preceding

996



1 you need to tell the locals to evacuate the area so we can secure the area to make sure no one gets hurt
lAzm tqwl Alhm AhAly AlmnTqp bAlAxlA’ AlmnTqp HtY nqdr nwmn AlmnTqp Elmwd ttAkdwn Anh mHd ytAY

2 do you have someone that can transport you to the nearest american base
Endk wAHd yqdr nqlk lAqrb qAEdp Amrykyp

Table 1: Examples of mistranslated morphology: English ASR hypotheses and IA translation hypotheses.

auxiliary verb can (yqdr). The correct form would
be yqlk (he/she transports you) instead of nqlk (we
transport you). Such translation errors are confus-
ing to users as they affect the understanding of ba-
sic semantic roles. They tend to occur when trans-
lating English infinitival constructions (to+verb) or
other syntactic constructions where English base
verb forms need to be translated by a finite verb in
IA. In these cases, explicit morphological features
like person and number are required in Arabic but
they are lacking in the English input.

4 Approach

An analysis of the SMT component showed that
morphological translation errors primarily occur
when a head word and its dependent (such as a ver-
bal head and its subject noun dependent) are trans-
lated as part of different phrases or rules. In that
case, insufficient context is available to produce the
correct translation. Our approach is to annotate syn-
tactic dependencies on the source side using a sta-
tistical parser. Based on the resulting dependency
structures the source-side data is then tagged with
explicit morphological verbal features using deter-
ministic rules (e.g., subject nouns assign their per-
son/number features to their verbal heads), and a
new translation model is trained on this data. Our
assumption is that words tagged with explicit mor-
phological features will be aligned with their cor-
rect translations during training and will thus pro-
duce correctly inflected forms during testing even
when the syntactic context is not available in the
same phrase/rule. For instance, the input sentence
in Example 1 in Table 1 would be annotated as:
you need-2sg to tell-2sg the locals to evacuate-3pl
the area so we can-1pl secure-1pl the area to make-
1pl sure no one gets-3sg hurt.
This approach avoids the costly extraction of multi-
ple features, subsequent statistical classification, and
inflection generation during run time; moreover, it

does not require target-side annotation tools, an ad-
vantage when dealing with under-resourced spoken
dialects. There are, however, several potential issues
with this approach. First, introducing tags fragments
the training data: the same word may receive multi-
ple different tags, either due to genuine ambiguity or
because of parser errors. As a result, word alignment
and phrase extraction may suffer from data spar-
sity. Second, new word-tag combinations in the test
data that were not observed in the training data will
not have an existing translation. Third, the perfor-
mance of the model is highly dependent on the accu-
racy of the parser. Finally, we make the assumption
that the expression of person and number categories
are matched across source and target language – in
practice, we have indeed seen very few mismatched
cases where e.g., a singular noun phrase in English is
translated by a plural noun phrase in IA (see Section
6 below).

To address the first point the morph-tagged trans-
lation model can be used in a backoff procedure
rather than as an alternative model. In this case the
baseline model is used by default, and the morph-
tagged model is only used whenever heads and de-
pendents are translated as part of different phrases.
Unseen translations for particular word-tag com-
binations in the test set could in principle be ad-
dressed by using a morphological analyzer to gen-
erate novel word forms with the desired inflections.
However, this would require identifying the correct
stem for the word in question, generating all pos-
sible morphological forms, and either selecting one
or providing all options to the SMT system, which
again increases system load. We analyzed unseen
word-tag combination in the test data but found that
their percentage was very small (< 1%). Thus, for
these forms we back off to the untagged counterparts
rather than generating new inflected forms. To ob-
tain better insight into the effect of parsing accuracy
we compared the performance of two parsers in our

997



annotation pipeline: the Stanford parser (de Marn-
effe et al., 2006) (version 3.3.1) and the Macaon
parser (Nasr et al., 2014). The latter is an im-
plementation of graph-based parsing (McDonald et
al., 2005) where a projective dependency tree max-
imizing a score function is sought in the graph of
all possible trees using dynamic programming. It
uses a 1st-order decoder, which is more robust to
speech input as well as out-of-domain training data.
The features implemented reflect those of (Bohnet,
2010) (based on lexemes and part-of-speech tags).
The parser was trained on Penn-Treebank data trans-
formed to match speech (lower-cased, no punctu-
ation), with one iteration of self-training on the
Transtac training set. We also use the combination
of both parsers, where source words are only tagged
if the tags derived independently from each parser
agree with each other.

5 Data and Baseline Systems

Development experiments were carried out on the
Transtac corpus of dialogs in the military and medi-
cal domain. The number of sentence pairs is 762k
for the training set, 6.9k for the dev set, 2.8k for
eval set 1, and 1.8k for eval set 2. Eval set 1 has
one reference per sentence, eval set 2 has four ref-
erences. For the development experiments we used
a phrase-based Moses SMT system with a hierarchi-
cal reordering model, tested on Eval set 1. The lan-
guage model was a backoff 6-gram model trained
using Kneser-Ney discounting and interpolation of
higher- and lower-order n-grams. In addition to au-
tomatic evaluation we performed manual analyses of
the accuracy of verbal features in the IA translations
on a subset of 65 sentences (containing 143 verb
forms) from the live evaluations described above.
This analysis counts a verb form as correct if its mor-
phological features for person and number are cor-
rect, although it may have the wrong lemma (e.g.,
wrong word sense). The development experiments
were designed to identify the setup that produces the
highest verbal inflection accuracy. For final testing
we used a more advanced SMT engine on Eval set
2.This system is the one used in the real-time dialog
system; it contains a hierarchical phrase-based trans-
lation model, sparse features, and a neural network
joint model (NNJM) (Devlin et al., 2014).

BLEU Acc (%)
Parser std bo std bo

Baseline 16.8 N/A 37.1 N/A
Stanford 16.9 17.0 60.1 59.4
Macaon 17.0 17.1 67.1 62.9

Combined 17.1 17.1 59.4 57.3

Table 2: BLEU scores on Transtac eval set 1 and accuracy
of verbal morphological features on manual eval set. std
= standard, bo = backed-off system.

6 Experiments and Results

Results in Table 2 show the comparison between the
baseline, different parsers, and the combined sys-
tem. We see that verbal inflection accuracy increases
substantially from the baseline performance and is
best for the Macaon parser. Improvements over
the baseline system without morphology are statisti-
cally significant; differences between the individual
parsers are not (not, however, that the sample size
for manual evaluation was quite small).

BLEU is not affected negatively but even in-
creases slightly - thus, data fragmentation does not
seem to be a problem overall. This may be due
to the nature of the task and domain, which is re-
sults in fairly short, simple sentence constructions
that can be adequately translated by a concatena-
tion of shorter phrases rather than requiring longer
phrases. Back-off systems (indicated by bo) and
the combined system improve BLEU only trivially
while decreasing verbal inflection accuracy by vary-
ing amounts. For testing within the dialog system
we thus choose the Macaon parser and utilize a stan-
dard translation model rather than a backoff model.
An added benefit is that the Macaon parser is already
used in other components in the dialog system. Us-
ing this setup we ran two experiments with dialog
system’s SMT engine: first, we re-extracted phrases
and rules based on the morph-tagged data and re-
optimized the feature weights. In the second ex-
periment, we additionally applied the NNJM to the
morph-tagged source text. To this end we include
all the morphological variants of the original vocab-
ulary that was used for the NNJM in the untagged
baseline system. Table 3 shows the results. The
morph-tagged data improves the BLEU score un-
der both conditions: in Experiment 1, the improve-

998



ment is almost a full BLEU point (0.91); in Experi-
ment 2 the improvement is even larger (1.13), even
though the baseline performance is stronger. Both
results are statistically significant at p = 0.05, using
a paired bootstrap resampling test. The combina-
tion of morph-tagged data and the more advanced
modeling options (sparse features, NNJM) in this
system seem to be beneficial. Improved translation
performance may also be captured by the four ref-
erence translations as opposed to one in Eval set
1. In order to assess the added computation cost

System no NNJM with NNJM
Baseline 34.38 36.17

Morph tags 35.29 37.30

Table 3: BLEU on Eval set 2 using dialog system’s SMT
engine.

of our procedure we computed the decoding speed
of the MT component in the dialog system for both
the baseline and the morpho-tag systems. In the
baseline MT system (with NNJM) without morpho-
tags, decoding takes 0.01572 seconds per word or
0.15408 seconds per sentence – these numbers were
obtained on a Dell Precision M4800 Laptop with a
quad-core Intel i7-4930MX Processor and 32GB of
RAM. Morpho-tagging only adds 0.00031 seconds
per word or 0.0024 seconds per sentence. Thus, our
procedure is extremely efficient.

An analysis of the remaining morphological
translation errors not captured by our approach
showed that in about 34% of all cases these were due
to part-of-speech tagging or parser errors, i.e. verbs
were mistagged as nouns rather than verbs and thus
did not receive any morphological tags, or the parser
hypothesized wrong dependency relations. In 53%
of the cases the problem is the lack of more extensive
discourse or contextual knowledge. This includes
constructions where there is no overt subject for a
verb in the current utterance, and the appropriate un-
derlying subject must be inferred from the preceding
discourse or from knowledge of the situational con-
text. This is an instance of the more general problem
of control (see e.g.,(Landau, 2013) for an overview
of research in this area). It is exemplified by cases
such as the following:
1. The first step is to make sure that all personnel

are in your debrief.
Here, the underlying subject of “to make sure” could
be a range of different candidates (I, you, we, etc.)
and must be inferred from context.
2. I can provide up to one platoon to help you guys
cordon off the area.
In this case the statistical parser identified I as the
subject of help, but platoon is more likely to be the
controller and was in fact identified as the underly-
ing subject by the annotator. Such cases could po-
tentially be resolved during the parsing step by in-
tegrating semantic information, e.g. as in (Bansal et
al., 2014). However, initial investigations with se-
mantic features in the Macaon parser resulted in a
significant slow-down of the parser. In other cases,
more sophisticated modeling of the entities and their
relationships in the situational context will be re-
quired. This clearly is an area for future study.

Finally, in 13% of the cases, mistranslations are
caused by a mismatch of number features across lan-
guages (e.g. number features for nouns such as fam-
ily or people).

7 Conclusion

We have shown that significant gains in BLEU and
verbal inflection accuracy in speech-to-speech trans-
lation for English-IA can be achieved by incor-
porating morphological tags derived from depen-
dency parse information in the source language.
The proposed method is fast, low-resource, and can
easily be incorporated into a real-time dialog sys-
tem. It adds negligible computational cost and does
not require any target-language specific annotation
tools. Possible areas for future study include the
use of discourse or and other contextual information
to determine morphological agreement, application
to other languages pairs/morphological agreement
types, and learning the annotation rules from data.

Acknowledgments
This study was funded by the Defense Advanced
Research Projects Agency (DARPA) under contract
HR0011-12-C-0016 - subcontract 19-000234.

References

M. Bansal, K. Gimpel, and K. Livescu. 2014. Tailoring
continuous word representations for dependency pars-
ing. In Proc. of ACL.

B. Bohnet. 2010. Very high accuracy and fast depen-

999



dency parsing is not a contradiction. In Proceedings
of COLING, pages 89–97.

V. Chahuneau, E. Schlinger, N. Smith, and C. Dyer.
2013. Translating into morphologically rich languages
with synthetic phrases. In Proceedings of EMNLP.

S. Condon, D. Parvaz, J. Aberdeen, C. Doran, A. Free-
man, and M. Awad. 2010. Evaluation of machine
translation errors in English and Iraqi Arabic. In Pro-
ceedings of LREC.

M.-C. de Marneffe, B. MacCartney, and C.D. Man-
ning. 2006. Generating typed dependency parses from
phrase structure parses. In Proceedings of LREC.

J. Devlin et al. 2014. Fast and robust neural network joint
models for statistical machine translation. In Proceed-
ings of ACL, pages 1370–1380.

A. El Kholy and N. Habash. 2012. Translate, predict
or generate: modeling rich morphology in statistical
machine translation. In Proceedings of EAMT.

N.F. Ayan et al. 2013. Can you give me another word
for hyperbaric? - improving speech translation us-
ing targeted clarification questions. In Proceedings of
ICASSP.

A. Fraser, M. Weller, A. Cahill, and F. Cap. 2012. Mod-
eling inflection and word-formation in SMT. In Pro-
ceedings of EACL, pages 664–674.

S. Goldwater and D. McClosky. 2005. Improving statis-
tical MT through morphological analysis. In Proceed-
ings of EMNLP, pages 676–683.

S. Green and J. DeNero. 2012. A class-based agreement
model for generating accuractely inflected translation.
In Proceedings of ACL, pages 146–155.

N. Habash and F. Sadat. 2006. Arabic preprocessing
schemes for statistical machine translation. In Pro-
ceedings of NAACL.

P. Koehn and H. Hoang. 2007. Factored translation mod-
els. In Proceedings of EMNLP, pages 868–876.

P. Koehn and K. Knight. 2003. Empirical methods for
compound splitting. In Proceedings of EACL.

I. Landau. 2013. Control in Generative Grammar: A
Research Companion. Cambridge University Press,
Cambridge, UK.

Y.S. Lee. 2004. Morphological analysis for statistical
machine translation. In Proceedings of HLT-NAACL.

R. McDonald, F. Pereira, K. Ribarov, and J. Hajič.
2005. Non-projective dependency parsing using span-
ning tree algorithms. In Proceedings of HLT/EMNLP,
pages 523–530.

E. Minkov, K. Toutanova, and H. Suzuki. 2007. Gener-
ating complex morphology for machine translation. In
Proceedings of ACL, pages 128–135.

A. Nasr, F. Bechet, B. Favre, T. Bazillon, J. Deulofeu, and
A. Valli. 2014. Automatically enriching spoken cor-
pora with syntactic information for linguistic studies.
In Proceedings of LREC.

M. Popovic and H. Ney. 2004. Towards the use of word
stems and suffixes for statistical machine translation.
In Proceedings of LREC.

S. Sultan. 2011. Applying Morphology to English-
Arabic Statistical Machine Translation. Ph.D. thesis,
Department of Computer Science, ETH Zürich.

K. Toutanova, H. Suzuki, and A. Ruopp. 2008. Applying
morphology generation models to machine translation.
In Proceedings of ACL, pages 514–522.

M. Weller, A. Fraser, and S. Schulte im Walde. 2013. Us-
ing subcategorization knowledge to improve case pre-
diction for translation to German. In Proceedings of
ACL, pages 593–603.

P. Williams and P. Koehn. 2011. Agreement constraints
for statistical machine translation into German. In
Proceedings of WMT.

1000


