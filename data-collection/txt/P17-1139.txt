



















































Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics


Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, pages 1514–1523
Vancouver, Canada, July 30 - August 4, 2017. c©2017 Association for Computational Linguistics

https://doi.org/10.18653/v1/P17-1139

Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, pages 1514–1523
Vancouver, Canada, July 30 - August 4, 2017. c©2017 Association for Computational Linguistics

https://doi.org/10.18653/v1/P17-1139

Prior Knowledge Integration for Neural Machine Translation
using Posterior Regularization

Jiacheng Zhang†, Yang Liu†‡∗, Huanbo Luan†, Jingfang Xu# and Maosong Sun†‡
†State Key Laboratory of Intelligent Technology and Systems

Tsinghua National Laboratory for Information Science and Technology
Department of Computer Science and Technology, Tsinghua University, Beijing, China
‡Jiangsu Collaborative Innovation Center for Language Competence, Jiangsu, China

#Sogou Inc., Beijing, China

Abstract

Although neural machine translation has
made significant progress recently, how
to integrate multiple overlapping, arbitrary
prior knowledge sources remains a chal-
lenge. In this work, we propose to use pos-
terior regularization to provide a general
framework for integrating prior knowl-
edge into neural machine translation. We
represent prior knowledge sources as fea-
tures in a log-linear model, which guides
the learning process of the neural trans-
lation model. Experiments on Chinese-
English translation show that our approach
leads to significant improvements.

1 Introduction

The past several years have witnessed the rapid de-
velopment of neural machine translation (NMT)
(Sutskever et al., 2014; Bahdanau et al., 2015),
which aims to model the translation process using
neural networks in an end-to-end manner. With
the capability of capturing long-distance depen-
dencies due to the gating (Hochreiter and Schmid-
huber, 1997; Cho et al., 2014) and attention (Bah-
danau et al., 2015) mechanisms, NMT has shown
remarkable superiority over conventional statisti-
cal machine translation (SMT) across a variety
of natural languages (Junczys-Dowmunt et al.,
2016).

Despite the apparent success, NMT still suffers
from one significant drawback: it is difficult to in-
tegrate prior knowledge into neural networks. On
one hand, neural networks use continuous real-
valued vectors to represent all language structures
involved in the translation process. While these
vector representations prove to be capable of cap-
turing translation regularities implicitly (Sutskever

∗Corresponding author: Yang Liu.

et al., 2014), it is hard to interpret each hidden state
in neural networks from a linguistic perspective.
On the other hand, prior knowledge in machine
translation is usually represented in discrete sym-
bolic forms such as dictionaries and rules (Niren-
burg, 1989) that explicitly encode translation regu-
larities. It is difficult to transform prior knowledge
represented in discrete forms to continuous repre-
sentations required by neural networks.

Therefore, a number of authors have endeav-
ored to integrate prior knowledge into NMT in re-
cent years, either by modifying model architec-
tures (Tu et al., 2016; Cohn et al., 2016; Tang
et al., 2016; Feng et al., 2016) or by modifying
training objectives (Cohn et al., 2016; Feng et al.,
2016; Cheng et al., 2016). For example, to address
the over-translation and under-translation prob-
lems widely observed in NMT, Tu et al. (2016)
directly extend standard NMT to model the cover-
age constraint that each source phrase should be
translated into exactly one target phrase (Koehn
et al., 2003). Alternatively, Cohn et al. (2016) and
Feng et al. (2016) propose to control the fertilities
of source words by appending additional additive
terms to training objectives.

Although these approaches have demonstrated
clear benefits of incorporating prior knowledge
into NMT, how to combine multiple overlapping,
arbitrary prior knowledge sources still remains a
major challenge. It is difficult to achieve this end
by directly modifying model architectures because
neural networks usually impose strong indepen-
dence assumptions between hidden states. As a
result, extending a neural model requires that the
interdependence of information sources be mod-
eled explicitly (Tu et al., 2016; Tang et al., 2016),
making it hard to extend. While this drawback can
be partly alleviated by appending additional addi-
tive terms to training objectives (Cohn et al., 2016;
Feng et al., 2016), these terms are restricted to a

1514

https://doi.org/10.18653/v1/P17-1139
https://doi.org/10.18653/v1/P17-1139


limited number of simple constraints.
In this work, we propose a general frame-

work for integrating multiple overlapping, arbi-
trary prior knowledge sources into NMT using
posterior regularization (Ganchev et al., 2010).
Our framework is capable of incorporating in-
direct supervision via posterior distributions of
neural translation models. To represent prior
knowledge sources as arbitrary real-valued fea-
tures, we define the posterior distribution as a log-
linear model instead of a constrained posterior set
(Ganchev et al., 2010). This treatment not only
leads to a simpler and more efficient training al-
gorithm but also achieves better translation per-
formance. Experiments show that our approach
is able to incorporate a variety of features and
achieves significant improvements over posterior
regularization using constrained posterior sets on
NIST Chinese-English datasets.

2 Background

2.1 Neural Machine Translation

Given a source sentence x = x1, . . . , xi, . . . , xI
and a target sentence y = y1, . . . , yj , . . . , yJ , a
neural translation model (Sutskever et al., 2014;
Bahdanau et al., 2015) is usually factorized as a
product of word-level translation probabilities:

P (y|x;θ) =
J∏

j=1

P (yj |x,y<j ;θ), (1)

where θ is a set of model parameters and y<j =
y1, . . . , yj−1 denotes a partial translation.

The word-level translation probability is de-
fined using a softmax function:

P (yj |x,y<j ;θ) ∝ exp
(
f(vyj ,vx,vy<j ,θ)

)
, (2)

where f(·) is a non-linear function, vyj is a vector
representation of the j-th target word yj , vx is a
vector representation of the source sentence x that
encodes the context on the source side, and vy<j
is a vector representation of the partial translation
y<j that encodes the context on the target side.

Given a training set {〈x(n),y(n)〉}Nn=1, the stan-
dard training objective is to maximize the log-
likelihood of the training set:

θ̂MLE = argmax
θ

{
L(θ)

}
, (3)

where

L(θ) =
N∑

n=1

logP (y(n)|x(n);θ). (4)

Although the introduction of vector represen-
tations into machine translation has resulted in
substantial improvements in terms of translation
quality (Junczys-Dowmunt et al., 2016), it is dif-
ficult to incorporate prior knowledge represented
in discrete symbolic forms into NMT. For ex-
ample, given a Chinese-English dictionary con-
taining ground-truth translational equivalents such
as 〈baigong, the White House〉, it is non-trivial
to leverage the dictionary to guide the learning
process of NMT. To address this problem, Tang
et al. (2016) propose a new architecture called
phraseNet on top of RNNsearch (Bahdanau et al.,
2015) that equips standard NMT with an external
memory storing phrase tables.

Another important prior knowledge source is
the coverage constraint (Koehn et al., 2003): each
source phrase should be translated into exactly
one target phrase. To encode this linguistic in-
tuition into NMT, Tu et al. (2016) extend standard
NMT with a coverage vector to keep track of the
attention history.

While these approaches are capable of incor-
porating individual prior knowledge sources sep-
arately, how to combine multiple overlapping, ar-
bitrary knowledge sources still remains a major
challenge. This can be hardly addressed by mod-
ifying model architectures because of the lack of
interpretability in NMT and the incapability of
neural networks in modeling arbitrary knowledge
sources. Although modifying training objectives
to include additional knowledge sources as ad-
ditive terms can partially alleviate this problem,
these terms have been restricted to a limited num-
ber of simple constraints (Cheng et al., 2016; Cohn
et al., 2016; Feng et al., 2016) and incapable of
combining arbitrary knowledge sources.

Therefore, it is important to develop a new
framework for integrating arbitrary prior knowl-
edge sources into NMT.

2.2 Posterior Regularization
Ganchev et al. (2010) propose posterior regular-
ization for incorporating indirect supervision via
constraints on posterior distributions of structured
latent-variable models. The basic idea is to penal-
ize the log-likelihood of a neural translation model

1515



with the KL divergence between a desired distri-
bution that incorporates prior knowledge and the
model posteriors. The posterior regularized likeli-
hood is defined as

F (θ, q)

= λ1L(θ)−

λ2

N∑

n=1

min
q∈Q

KL
(
q(y)

∣∣∣
∣∣∣P (y|x(n);θ),

)
(5)

where λ1 and λ2 are hyper-parameters to balance
the preference between likelihood and posterior
regularization,Q is a set of constrained posteriors:

Q = {q(y) : Eq[φ(x,y)] ≤ b}, (6)
where φ(x,y) is constraint feature and b is the

bound of constraint feature expectations. Ganchev
et al. (2010) use constraint features to encode
structural bias and define the set of valid distribu-
tions with respect to the expectations of constraint
features to facilitate inference.

As maximizing F (θ, q) involves minimizing
the KL divergence, Ganchev et al. (2010) present a
minorization-maximization algorithm akin to EM
at sentence level:

E : q(t+1) = argmin
q

KL
(
q(y)

∣∣∣
∣∣∣P (y|x(n);θ(t))

)

M : θ(t+1) = argmax
θ

Eq(t+1)
[
logP (y|x(n);θ)

]

However, directly applying posterior regulariza-
tion to neural machine translation faces a major
difficulty: it is hard to specify the hyper-parameter
b to effectively bound the expectation of features,
which are usually real-valued in translation (Och
and Ney, 2002; Koehn et al., 2003; Chiang, 2005).
For example, the coverage penalty constraint (Wu
et al., 2016) proves to be an essential feature for
controlling the length of a translation in NMT. As
the value of coverage penalty varies significantly
over different sentences, it is difficult to set an ap-
propriate bound for all sentences on the training
data. In addition, the minorization-maximization
algorithm involves an additional step to find q(t+1)

as compared with standard NMT, which increases
training time significantly.

3 Posterior Regularization for Neural
Machine Translation

3.1 Modeling
In this work, we propose to adapt posterior reg-
ularization (Ganchev et al., 2010) to neural ma-

chine translation. The major difference is that we
represent the desired distribution as a log-linear
model (Och and Ney, 2002) rather than a con-
strained posterior set as described in (Ganchev
et al., 2010):

J (θ,γ)
= λ1L(θ)−

λ2

N∑

n=1

KL
(
Q(y|x(n);γ)

∣∣∣
∣∣∣P (y|x(n);θ)

)
, (7)

where the desired distribution that encodes prior
knowledge is defined as: 1

Q(y|x;γ) =
exp

(
γ · φ(x,y)

)

∑
y′ exp

(
γ · φ(x,y′)

) . (8)

As compared to previous work on integrating
prior knowledge into NMT (Tu et al., 2016; Cohn
et al., 2016; Tang et al., 2016), our approach pro-
vides a general framework for combining arbi-
trary knowledge sources. This is due to log-linear
models that offer sufficient flexibility to repre-
sent arbitrary prior knowledge sources as features.
We tackle the representation discrepancy prob-
lem by associating the Q distribution that encodes
discrete representations of prior knowledge with
neural models using continuous representations
learned from data in the KL divergence. Another
advantage of our approach is the transparency to
model architectures. In principle, our approach
can be applied to any neural models for natural
language processing.

Our approach also differs from the original ver-
sion of posterior regularization (Ganchev et al.,
2010) in the definition of desired distribution. We
resort to log-linear models (Och and Ney, 2002) to
incorporate features that have proven effective in
SMT. Another benefit of using log-linear models
is the differentiability of our training objective (see
Eq. (7)). It is easy to leverage standard stochastic
gradient descent algorithms to optimize model pa-
rameters (Section 3.3).

3.2 Feature Design
In this section, we introduce how to design fea-
tures to encode prior knowledge in the desired dis-

1Ideally, the desired distribution Q should be fixed to
guide the learning process of P . However, it is hard to man-
ually specify the feature weights γ. Therefore, we propose
to train both θ and λ jointly (see Section 3.3). We find that
joint training results in significant improvements in practice
(see Table 1).

1516



tribution.
Note that not all features in SMT can be adopted

to our framework. This is because features in SMT
are defined on latent structures such as phrase
pairs and synchronous CFG rules, which are not
accessible to the decoding process of NMT. For-
tunately, we can still leverage internal information
in neural models that is linguistically meaningful
such as the attention matrix a (Bahdanau et al.,
2015).

We will introduce a number of features used in
our experiments as follows.

3.2.1 Bilingual Dictionary
It is natural to leverage a bilingual dictionary D to
improve neural machine translation. Arthur et al.
(2016) propose to incorporate discrete translation
lexicons into NMT by using the attention vector to
select lexical probabilities on which to be focused.

In our work, for each entry 〈x, y〉 ∈ D in the
dictionary, a bilingual dictionary (BD) feature is
defined at the sentence level:

φBD〈x,y〉(x,y) =

{
1 if x ∈ x ∧ y ∈ y
0 otherwise

. (9)

Note that number of bilingual dictionary features
depends on the vocabulary of the neural transla-
tion model. Entries containing out-of-vocabulary
words has to be discarded.

3.2.2 Phrase Table
Phrases, which are sequences of consecutive
words, are capable of memorizing local context to
deal with word ordering within phrases and trans-
lation of short idioms, word insertions or deletions
(Koehn et al., 2003; Chiang, 2005). As a result,
phrase tables that specify phrase-level correspon-
dences between the source and target languages
also prove to be an effective knowledge source in
NMT (Tang et al., 2016).

Similar to the bilingual dictionary features, we
define a phrase table (PT) feature for each entry
〈x̃, ỹ〉 in a phrase table P:

φPT〈x̃,ỹ〉(x,y) =

{
1 if x̃ ∈ x ∧ ỹ ∈ y
0 otherwise

. (10)

The number of phrase table features also depends
on the vocabulary of the neural translation model.

3.2.3 Coverage Penalty
To overcome the over-translation and under-
translation problems widely observed in NMT, a

number of authors have proposed to model the fer-
tility (Brown et al., 1993) and converge constraint
(Koehn et al., 2003) to improve the adequacy of
translation (Tu et al., 2016; Cohn et al., 2016; Feng
et al., 2016; Wu et al., 2016; Mi et al., 2016).

We follow Wu et al. (2016) to define a coverage
penalty (CP) feature to penalize source words with
lower sum of attention weights: 2

φCP(x,y) =

|x|∑

i=1

log

(
min

( |y|∑

j=1

ai,j , 1.0
))

, (11)

where ai,j is the attention probability of the j-th
target word on the i-th source word. Note that
the value of coverage penalty feature varies sig-
nificantly over sentences of different lengths.

3.2.4 Length Ratio
Controlling the length of translations is very im-
portant in NMT as neural models tend to gener-
ate short translations for long sentences, which
deteriorates the translation performance of NMT
for long sentences as compared with SMT (Shen
et al., 2016).

Therefore, we define the length ratio (LR) fea-
ture to encourage the length of a translation to fall
in a reasonable range:

φLR(x,y) =

{
(β|x|)/|y| if β|x| < |y|
|y|/(β|x|) otherwise , (12)

where β is a hyper-parameter for penalizing too
long or too short translations.

For example, to convey the same meaning, an
English sentence is usually about 1.2 times longer
than a Chinese sentence. As a result, we can set
β = 1.2. If the length of a Chinese sentence |x| is
10 and the length of an English sentence |y| is 12,
then, φLR(x,y) = 1. If the translation is too long
(e.g., |y| = 100), then the feature value is 0.12.
If the translation is too short (e.g., |y| = 6), the
feature value is 0.5.

3.3 Training

In training, our goal is to find a set of model pa-
rameters that maximizes the posterior regularized
likelihood:

θ̂, γ̂ = argmax
θ,γ

{
J (θ,γ)

}
. (13)

2For simplicity, we omit the attention matrix a in the input
of the coverage feature function.

1517



Note that unlike the original version of poste-
rior regularization (Ganchev et al., 2010) that re-
lies on a minorization-maximization algorithm to
optimize model parameters, our training objective
is differentiable with respect to model parameters.
Therefore, it is easy to use standard stochastic gra-
dient descent algorithms to train our model.

However, a major difficulty in calculating gra-
dients is that the algorithm needs to sum over
all candidate translations in an exponential search
space for KL divergence. For example, the partial
derivative of J (θ,γ) with respect to γ is given by

∂J (θ,γ)
∂γ

= −λ2 ×
N∑

n=1

∂

∂γ
KL
(
Q(y|x(n);γ)

∣∣∣
∣∣∣P (y|x(n);θ)

)
. (14)

The KL divergence is defined as

KL
(
Q(y|x(n);γ)

∣∣∣
∣∣∣P (y|x(n);θ)

)

=
∑

y∈Y(x(n))
Q(y|x(n);γ) log Q(y|x

(n);γ)

P (y|x(n);θ) , (15)

where Y(x(n)) is a set of all possible candidate
translations for the source sentence x(n).

To alleviate this problem, we follow Shen
et al. (2016) to approximate the full search
space Y(x(n)) with a sampled sub-space S(x(n)).
Therefore, the KL divergence can be approxi-
mated as

KL
(
Q(y|x(n);γ)

∣∣∣
∣∣∣P (y|x(n);θ)

)

≈
∑

y∈S(x(n))
Q̃(y|x(n);γ) log Q̃(y|x

(n);γ)

P̃ (y|x(n);θ)
. (16)

Note that the Q distribution is also approxi-
mated on the sub-space:

Q̃(y|x(n);γ)

=
exp(γ · φ(x(n),y))∑

y′∈S(x(n)) exp(γ · φ(x(n),y′))
. (17)

We follow Shen et al. (2016) to control the
sharpness of approximated neural translation dis-
tribution normalized on the sampled sub-space:

P̃ (y|x(n);θ) = P (y|x
(n);θ)α∑

y′∈S(x(n)) P (y
′|x(n);θ)α . (18)

3.4 Search
Given learned model parameters θ̂ and γ̂, the deci-
sion rule for translating an unseen source sentence
x is given by

ŷ = argmax
Y(x)

{
P (y|x; θ̂)

}
. (19)

The search process can be factorized at the word
level:

ŷj = argmax
y∈Vy

{
P (y|x, ŷ<j ; θ̂)

}
, (20)

where Vy is the target language vocabulary.
Although this decision rule shares the same ef-

ficiency and simplicity with standard NMT (Bah-
danau et al., 2015), it does not involve prior knowl-
edge in decoding. Previous studies reveal that in-
corporating prior knowledge in decoding also sig-
nificantly boosts translation performance (Arthur
et al., 2016; He et al., 2016; Wang et al., 2016).

As directly incorporating prior knowledge into
the decoding process of NMT depends on both
model structure and the locality of features, we re-
sort to a coarse-to-fine approach instead to keep
the architecture transparency of our approach.
Given a source sentence x in the test set, we first
use the neural translation model P (y|x; θ̂) to gen-
erate a k-best list of candidate translation C(x).
Then, the algorithm decides on the most probable
candidate translation using the following decision
rule:

ŷ = argmax
y∈C(x)

{
logP (y|x; θ̂) + γ̂ · φ(x,y)

}
. (21)

4 Experiments

4.1 Setup
We evaluate our approach on Chinese-English
translation. The evaluation metric is case-
insensitive BLEU calculated by the multi-
bleu.perl script. Our training set3 consists of
1.25M sentence pairs with 27.9M Chinese words
and 34.5M English words. We use the NIST 2002
dataset as validation set and the NIST 2003, 2004,
2005, 2006, 2008 datasets as test sets.

In the experiments, we compare our approach
with the following two baseline approaches:

3The training set includes LDC2002E18, LDC2003E07,
LDC2003E14, part of LDC2004T07, LDC2004T08 and
LDC2005T06.

1518



Method Feature MT02 MT03 MT04 MT05 MT06 MT08 All
RNNSEARCH N/A 33.45 30.93 32.57 29.86 29.03 21.85 29.11

CPR N/A 33.84 31.18 33.26 30.67 29.63 22.38 29.72

POSTREG

BD 34.65 31.53 33.82 30.66 29.81 22.55 29.97
PT 34.56 31.32 33.89 30.70 29.84 22.62 29.99
LR 34.39 31.41 34.19 30.80 29.82 22.85 30.14
BD+PT 34.66 32.05 34.54 31.22 30.70 22.84 30.60
BD+PT+LR 34.37 31.42 34.18 30.99 29.90 22.87 30.20

this work

BD 36.61 33.47 36.04 32.96 32.46 24.78 32.27
PT 35.07 32.11 34.73 31.84 30.82 23.23 30.86
CP 34.68 31.99 34.67 31.37 30.80 23.34 30.76
LR 34.57 31.89 34.95 31.80 31.43 23.75 31.12
BD+PT 36.30 33.83 36.02 32.98 32.53 24.54 32.29
BD+PT+CP 36.11 33.64 36.36 33.11 32.53 24.57 32.39
BD+PT+CP+LR 36.10 33.64 36.48 33.08 32.90 24.63 32.51

Table 1: Comparison of BLEU scores on the Chinese-English datasets. RNNSEARCH is an attention-
based neural machine translation model (Bahdanau et al., 2015) that does not incorporate prior knowl-
edge. CPR extends RNNSEARCH by introducing coverage penalty refinement (Eq. (11)) in decoding.
POSTREG extends RNNSEARCH with posterior regularization (Ganchev et al., 2010), which uses con-
straint features to represent prior knowledge and a constrained posterior set to denote the desired distri-
bution. Note that POSTREG cannot use the CP feature (Section 3.2.3) because it is hard to bound the
feature value appropriately. On top of RNNSEARCH, our approach also exploits posterior regularization
to incorporate prior knowledge but uses a log-linear model to denote the desired distribution. All results
of this work are significantly better than RNNSEARCH (p < 0.01).

1. RNNSEARCH (Bahdanau et al., 2015):
a standard attention-based neural machine
translation model,

2. CPR (Wu et al., 2016): extending
RNNSEARCH by introducing coverage
penalty refinement (Eq. (11)) in decoding,

3. POSTREG (Ganchev et al., 2010): extend-
ing RNNSEARCH with posterior regulariza-
tion using constrained posterior set.

For RNNSEARCH, we use an in-house
attention-based NMT system that achieves com-
parable translation performance with GROUND-
HOG (Bahdanau et al., 2015), which serves as a
baseline approach in our experiments. We limit
vocabulary size to 30K for both languages. The
word embedding dimension is set to 620. The
dimension of hidden layer is set to 1,000. In
training, the batch size is set to 80. We use the
AdaDelta algorithm (Zeiler, 2012) for optimizing
model parameters. In decoding, the beam size is
set to 10.

For CPR, we simply follow Wu et al. (2016)
to incorporate the coverage penalty into the beam

search algorithm of RNNSEARCH.
For POSTREG, we adapt the original version

of posterior regularization (Ganchev et al., 2010)
to NMT on top of RNNSEARCH. Following
Ganchev et al. (2010), we use a ten-step pro-
jected gradient descent algorithm to search for an
approximate desired distribution in the E step and
a one-step gradient descent for the M step.

Our approach extends RNNSEARCH by incor-
porating prior knowledge. For each source sen-
tence, we sample 80 candidate translations to ap-
proximate the P̃ and Q̃ distributions. The hyper-
parameter α is set to 0.2. The batch size is 1. The
hyper-parameters λ1 and λ2 are set to 8×10−5 and
2.5 × 10−4. Note that they not only balance the
preference between likelihood and posterior regu-
larization, but also control the values of gradients
to fall in a reasonable range for optimization.

We construct bilingual dictionary and phrase ta-
ble in an automatic way. First, we run the statis-
tical machine translation system MOSES (Koehn
and Hoang, 2007) to obtain probabilistic bilin-
gual dictionary and phrase table. For the bilin-
gual dictionary, we retain entries with probabili-
ties higher than 0.1 in both source-to-target and

1519



Feature Rerank MT02 MT03 MT04 MT05 MT06 MT08 All

BD
w/o 36.06 32.99 35.62 32.59 32.13 24.36 31.87
w/ 36.61 33.47 36.04 32.96 32.46 24.78 32.27

PT
w/o 34.98 32.01 34.71 31.77 30.77 23.20 30.81
w/ 35.07 32.11 34.73 31.84 30.82 23.23 30.86

CP
w/o 34.68 31.99 34.67 31.37 30.80 23.34 30.76
w/ 34.68 31.99 34.67 31.37 30.80 23.34 30.76

LR
w/o 34.60 31.89 34.79 31.72 31.39 23.63 31.03
w/ 34.57 31.89 34.95 31.80 31.43 23.75 31.12

BD+PT
w/o 35.76 33.27 35.64 32.47 32.03 24.17 31.83
w/ 36.30 33.83 36.02 32.98 32.53 24.54 32.29

BD+PT+CP
w/o 35.71 33.15 35.81 32.52 32.16 24.11 31.89
w/ 36.11 33.64 36.36 33.11 32.53 24.57 32.39

BD+PT+CP+LR
w/o 36.06 33.01 35.86 32.70 32.24 24.27 31.96
w/ 36.10 33.64 36.48 33.08 32.90 24.63 32.51

Table 2: Effect of reranking on translation quality.

target-to-source directions. For phrase table, we
first remove phrase pairs that occur less than 10
times and then retain entries with probabilities
higher than 0.5 in both directions. As a result, both
bilingual dictionary and phrase table contain high-
quality translation correspondences. We estimate
the length ratio on Chinese-English data and set
the hyper-parameter β to 1.236.

By default, both POSTREG and our approach
use reranking to search for the most probable
translations (Section 3.4).

4.2 Main Results

Table 1 shows the BLEU scores obtained by
RNNSEARCH, POSTREG, and our approach on
the Chinese-English datasets.

We find POSTREG achieves significant im-
provements over RNNSEARCH by adding features
that encode prior knowledge. The most effective
single feature for POSTREG seems to be the length
ratio (LR) feature, suggesting that it is important
for NMT to control the length of translation to im-
prove translation quality. Note that POSTREG is
unable to include the coverage penalty (CP) fea-
ture because the feature value varies significantly
over different sentences. It is hard to specify an
appropriate bound b for constraining the expected
feature value. We observe that a loose bound often
makes the training process very unstable and fail
to converge. Combining features obtains further
modest improvements.

Our approach outperforms both RNNSEARCH
and POSTREG significantly. The bilingual dictio-

nary (BD) feature turns out to make the most con-
tribution. Compared with CPR that imposes cov-
erage penalty during decoding, our approach that
using a single CP feature obtains a significant im-
provement (i.e., 30.76 over 29.72), suggesting that
incorporating prior knowledge sources in model-
ing might be more beneficial than in decoding.

We find that combining features only results in
modest improvements for our approach. One pos-
sible reason is that the bilingual dictionary and
phrase table features overlap on single word pairs.

4.3 Effect of Reranking

Table 2 shows the effect of reranking on trans-
lation quality. We find that using prior knowl-
edge features to rescore the k-best list produced
by the neural translation model usually leads to
improvements. This finding confirms that adding
prior knowledge is beneficial for NMT, either in
the training or decoding process.

4.4 Training Speed

Initialized with the best RNNSEARCH model
trained for 300K iterations, our model converges
after about 100K iterations. For each iteration, our
approach is 1.5 times slower than RNNSEARCH.
On a single GPU device Tesla M40, it takes four
days to train the RNNSEARCH model and three
extra days to train our model.

4.5 Example Translations

Table 3 gives four examples to demonstrate the
benefits of adding features.

1520



Source lijing liang tian yu bingxue de fenzhan , 31ri shenye 23 shi 50 fen , shanghai
jichang jituan yuangong yinglai le 2004nian de zuihou yige hangban .

Reference after fighting with ice and snow for two days , staff members of shanghai
airport group welcomed the last flight of 2004 at 23 : 50pm on the 31st .

RNNSEARCH after a two - day and two - day journey , the team of shanghai ’s airport in
shanghai has ushered in the last flight in 2004 .

+ BD after two days and nights fighting with ice and snow , the shanghai airport
group ’s staff welcomed the last flight in 2004 .

Source suiran tonghuopengzhang weilai ji ge yue reng jiang weizhi zai baifenzhier
yishang , buguo niandi zhiqian keneng jiangdi .

Reference although inflation will remain above 2 % for the coming few months , it
may decline by the end of the year .

RNNSEARCH although inflation has been maintained for more than two months from the
year before the end of the year , it may be lower .

+ PT although inflation will remain at more than 2 percent in the next few
months , it may be lowered before the end of the year .

Source qian ji tian ta ganggang chuyuan , jintian jianchi lai yu lao pengyou daobie
.

Reference just discharged from the hospital a few days ago , he insisted on coming
to say farewell to his old friend today .

RNNSEARCH during the previous few days , he had just been given treatment to the old
friends .

+ CP during the previous few days , he had just been discharged from the hos-
pital , and he insisted on goodbye to his old friend today .

Source ( guoji ) yiselie fuzongli fouren jihua kuojian gelan gaodi dingjudian
Reference ( international ) israeli deputy prime minister denied plans to expand golan

heights settlements
RNNSEARCH ( world ) israeli deputy prime minister denies the plan to expand the golan

heights in the golan heights
+ LR ( international ) israeli deputy prime minister denies planning to expand

golan heights

Table 3: Example translations that demonstrate the effect of adding features.

In the first example, source words “fen-
zhan” (fighting), “yuangong” (staff), and “yinglai”
(welcomed) are untranslated in the output of
RNNSEARCH. Adding the bilingual dictionary
(BD) feature encourages the model to translate
these words if they occur in the dictionary.

In the second example, while RNNSEARCH
fails to capture phrase cohesion, adding the phrase
table (PT) feature is beneficial for translating short
idioms, word insertions or deletions that are sensi-
tive to local context.

In the third example, RNNSEARCH tends
to omit many source content words such as
“chuyuan” (discharged from the hospital),
“jianchi” (insisted on), and “daobie” (say
farewell). The coverage penalty (CP) feature

helps to alleviate the word omission problem.
In the fourth example, the translation produced

by RNNSEARCH is too long and “the golan
heights” occurs twice. The length ratio (LR) fea-
ture is capable of controlling the sentence length
in a reasonable range.

5 Related Work

Our work is directly inspired by posterior regu-
larization (Ganchev et al., 2010). The major dif-
ference is that we use a log-linear model to rep-
resent the desired distribution rather than a con-
strained posterior set. Using log-linear models
not only enables our approach to incorporate ar-
bitrary knowledge sources as real-valued features,
but also is differentiable to be jointly trained with

1521



neural translation models efficiently.
Our work is closely related to recent work on in-

jecting prior knowledge into NMT (Arthur et al.,
2016; Tu et al., 2016; Cohn et al., 2016; Tang et al.,
2016; Feng et al., 2016; Wang et al., 2016). The
major difference is that our approach aims to pro-
vide a general framework for incorporating arbi-
trary prior knowledge sources while keeping the
neural translation model unchanged.

He et al. (2016) also propose to combine the
strengths of neural networks on learning represen-
tations and log-linear models on encoding prior
knowledge. But they treat neural translation mod-
els as a feature in the log-linear model. In contrast,
we connect the two models via KL divergence to
keep the transparency of our approach to model ar-
chitectures. This enables our approach to be easily
applied to other neural models in NLP.

6 Conclusion

We have presented a general framework for incor-
porating prior knowledge into end-to-end neural
machine translation based on posterior regulariza-
tion (Ganchev et al., 2010). The basic idea is to
guide NMT models towards desired behavior us-
ing a log-linear model that encodes prior knowl-
edge. Experiments show that incorporating prior
knowledge leads to significant improvements over
both standard NMT and posterior regularization
using constrained posterior sets.

Acknowledgments

We thank Shiqi Shen for useful discussions and
anonymous reviewers for insightful comments.
This work is supported by the National Natu-
ral Science Foundation of China (No.61432013),
the 973 Program (2014CB340501), and the
National Natural Science Foundation of China
(No.61522204). This research is also supported by
Sogou Inc. and the Singapore National Research
Foundation under its International Research Cen-
tre@Singapore Funding Initiative and adminis-
tered by the IDM Programme.

References
Philip Arthur, Graham Neubug, and Satoshi Naka-

mura. 2016. Incorporating discrete transla-
tion lexicons into neural machine translation.
arXiv:1606.02006v2.

Dzmitry Bahdanau, KyungHyun Cho, and Yoshua
Bengio. 2015. Neural machine translation by jointly

learning to align and translate. In Proceedings of
ICLR.

Peter F. Brown, Stephen A. Della Pietra, Vincent J.
Della Pietra, and Robert L. Mercer. 1993. The math-
ematics of statistical machine translation: Parameter
estimation. Computational Linguistics .

Yong Cheng, Shiqi Shen, Zhongjun He, Wei He,
Hua Wu, Maosong Sun, and Yang Liu. 2016.
Agreement-based learning of parallel lexicons and
phrases from non-parallel corpora. In Proceedings
of IJCAI.

David Chiang. 2005. A hirarchical phrase-based model
for statistical machine translation. In Proceedings of
ACL.

Kyunghyun Cho, Bart van Merrienboer, Caglar Gul-
cehre, Dzmitry Bahdanau, Fethi Bougares, Holger
Schwenk, and Yoshua Bengio. 2014. Learning
phrase representations using rnn encoder–decoder
for statistical machine translation. In Proceedings
of EMNLP.

Trevor Cohn, Cong Duy Vu Hoang, Ekaterina Vy-
molova, Kaisheng Yao, Chris Dyer, and Gholamreza
Haffari. 2016. Incorporating structural alignment bi-
ases into an attentional neural translation model. In
Proceedings of NAACL.

Shi Feng, Shujie Liu, Nan Yang, Mu Li, Ming Zhou,
and Kenny Q. Zhu. 2016. Improving attention mod-
eling with implicit distortion and fertility for ma-
chine translation. In Proceedings of COLING.

Kuzman Ganchev, João Graça, Jennifer Gillenwater,
and Ben Taskar. 2010. Posterior regularization for
structured latent variable models. Journal of Ma-
chine Learning Research .

Wei He, Zhongjun He, Hua Wu, and Haifeng Wang.
2016. Improved nerual machine translation with
SMT features. In Proceedings of AAAI.

Sepp Hochreiter and Jürgen Schmidhuber. 1997. Long
short-term memory. Neural Computation .

Marcin Junczys-Dowmunt, Tomasz Dwojak, and Hieu
Hoang. 2016. Is neural machine translation ready
for deployment? a case study on 30 translation di-
rections. arXiv:1610.01108v2.

Philipp Koehn and Hieu Hoang. 2007. Factored trans-
lation models. In Proceedings of EMNLP.

Philipp Koehn, Franz J. Och, and Daniel Marcu. 2003.
Statistical phrase-based translation. In Proceedings
of NAACL.

Haitao Mi, Baskaran Sankaran, Zhiguo Wang, and
Abe Ittycheriah. 2016. Coverage embedding mod-
els for neural machine translation. In Proceedings
of EMNLP.

Sergei Nirenburg. 1989. Knowledge-based machine
translation. Machine Translation .

1522



Franz J. Och and Hermann Ney. 2002. Discriminative
training and maximum entropy models for statistical
machine translation. In Proceedings of ACL.

Shiqi Shen, Yong Cheng, Zhongjun He, Wei He, Hua
Wu, Maosong Sun, and Yang Liu. 2016. Minimum
risk training for neural machine translation. In Pro-
ceedings of ACL.

Ilya Sutskever, Oriol Vinyals, and Quoc V. Le. 2014.
Sequence to sequence learning with neural net-
works. In Proceedings of NIPS.

Yaohua Tang, Fandong Meng, Zhengdong Lu, Hang
Li, and Philip L. H. Yu. 2016. Neural ma-
chine translation with external phrase memory.
arXiv:1606.01792v1.

Zhaopeng Tu, Zhengdong Lu, Yang Liu, Xiaohua Liu,
and Hang Li. 2016. Modeling coverage for neural
machine translation. In Proceedings of ACL.

Xing Wang, Zhengdong Lu, Zhaopeng Tu, Hang Li,
Deyi Xiong, and Min Zhang. 2016. Neural machine
translation advised by statistical machine transla-
tion. arXiv:1610.05150.

Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V.
Le, Mohammad Norouzi, Wolfgang Macherey,
Maxim Krikun, Yuan Cao, Qin Gao, Klaus
Macherey, Jeff Klingner, Apurva Shah, Melvin
Johnson, Xiaobing Liu, Lukasz Kaiser, Stephan
Gouws, Yoshikiyo Kato, Taku Kudo, Hideto
Kazawa, Keith Stevens, George Kurian, Nishant
Patil, Wei Wang, Cliff Young, Jason Smith, Ja-
son Riesa, Alex Rudnick, Oriol Vinyals, Greg Cor-
rado, Macduff Hughes, and Jeffrey Dean. 2016.
Google’s neural machine translation system: Bridg-
ing the gap between human and machine translation.
arXiv:1609.08144v2.

Matthew D. Zeiler. 2012. Adadelta: an adaptive learn-
ing rate method. arXiv:1212.5701.

1523


	Prior Knowledge Integration for Neural Machine Translation using Posterior Regularization

