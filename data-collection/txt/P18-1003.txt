



















































Unsupervised Learning of Distributional Relation Vectors


Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers), pages 23–33
Melbourne, Australia, July 15 - 20, 2018. c©2018 Association for Computational Linguistics

23

Unsupervised Learning of Distributional Relation Vectors

Shoaib Jameel
School of Computing

Medway Campus
University of Kent, UK
M.S.Jameel@kent.ac.uk

Zied Bouraoui
CRIL CNRS and
Artois University

France
bouraoui@cril.fr

Steven Schockaert
School of Computer Science

and Informatics
Cardiff University, UK

schockaerts1@cardiff.ac.uk

Abstract

Word embedding models such as GloVe
rely on co-occurrence statistics to learn
vector representations of word meaning.
While we may similarly expect that co-
occurrence statistics can be used to cap-
ture rich information about the relation-
ships between different words, existing ap-
proaches for modeling such relationships
are based on manipulating pre-trained
word vectors. In this paper, we introduce
a novel method which directly learns re-
lation vectors from co-occurrence statis-
tics. To this end, we first introduce a vari-
ant of GloVe, in which there is an explicit
connection between word vectors and PMI
weighted co-occurrence vectors. We then
show how relation vectors can be naturally
embedded into the resulting vector space.

1 Introduction

Word embeddings are vector space representations
of word meaning (Mikolov et al., 2013b; Penning-
ton et al., 2014). A remarkable property of these
models is that they capture various lexical rela-
tionships, beyond mere similarity. For example,
(Mikolov et al., 2013b) found that analogy ques-
tions of the form “a is to b what c is to ?” can
often be answered by finding the word d that max-
imizes cos(wb−wa+wc, wd), where we write wx
for the vector representation of a word x.

Intuitively, the word vector wa represents a in
terms of its most salient features. For example,
wparis implicitly encodes that Paris is located in
France and that it is a capital city, which is intu-
itively why the ‘capital of’ relation can be mod-
eled in terms of a vector difference. Other rela-
tionships, however, such as the fact that Macron
succeeded Hollande as president of France, are un-

likely to be captured by word embeddings. Rela-
tion extraction methods can discover such infor-
mation by analyzing sentences that contain both of
the words or entities involved (Mintz et al., 2009;
Riedel et al., 2010; dos Santos et al., 2015), but
they typically need a large number of training ex-
amples to be effective.

A third alternative, which we consider in this
paper, is to characterize the relatedness between
two words s and t by learning a relation vector
rst in an unsupervised way from corpus statistics.
Among others, such vectors can be used to find
word pairs that are similar to a given word pair
(i.e. finding analogies), or to find the most pro-
totypical examples among a given set of relation
instances. They can also be used as an alternative
to the aforementioned relation extraction methods,
by subsequently training a classifier that uses the
relation vectors as input, which might be particu-
larly effective in cases where only limited amounts
of training data are available (with the case of anal-
ogy finding from a single instance being an ex-
treme example).

The most common unsupervised approach for
learning relation vectors consists of averaging the
embeddings of the words that occur in between s
and t, in sentences that contain both (Weston et al.,
2013; Fan et al., 2015; Hashimoto et al., 2015).
While this strategy is often surprisingly effective
(Hill et al., 2016), it is sub-optimal for two rea-
sons. First, many of the words co-occurring with
s and t will be semantically related to s or to t, but
will not actually be descriptive for the relationship
between s and t; e.g. the vector describing the re-
lation between Paris and France should not be af-
fected by words such as eiffel (which only relates
to Paris). Second, it gives too much weight to stop-
words, which cannot be addressed in a straightfor-
ward way as some stop-words are actually crucial
for modeling relationships (e.g. prepositions such



24

as ‘in’ or ‘of’ or Hearst patterns (Indurkhya and
Damerau, 2010)).

In this paper, we propose a method for learn-
ing relation vectors directly from co-occurrence
statistics. We first introduce a variant of GloVe, in
which word vectors can be directly interpreted as
smoothed PMI-weighted bag-of-words represen-
tations. We then represent relationships between
words as weighted bag-of-words representations,
using generalizations of PMI to three arguments,
and learn vectors that correspond to smoothed ver-
sions of these representations.

As far as the possible applications of our
methodology is concerned, we imagine that rela-
tion vectors can be used in various ways to enrich
the input to neural network models. As a sim-
ple example, in a question answering system, we
could “annotate” mentions of entities with relation
vectors encoding their relationship to the differ-
ent words from the question. As another exam-
ple, we could consider a recommendation system
which takes advantage of vectors expressing the
relationship between items that have been bought
(or viewed) by a customer and other items from
the catalogue. Finally, relation vectors should also
be useful for knowledge completion, especially
in cases where few training examples per relation
type are given (meaning that neural network mod-
els could not be used) and where relations cannot
be predicted from the already available knowledge
(meaning that knowledge graph embedding meth-
ods could not be used, or are at least not sufficient).

2 Related Work

The problem of characterizing the relationship be-
tween two words has been studied in various set-
tings. From a learning point of view, the most
straightforward setting is where we are given la-
beled training sentences, with each label explic-
itly indicating what relationship is expressed in
the sentence. This fully supervised setting has
been the focus of several evaluation campaigns, in-
cluding as part of ACE (Doddington et al., 2004)
and at SemEval 2010 (Hendrickx et al., 2010). A
key problem with this setting, however, is that la-
beled training data is hard to obtain. A popular
alternative is to use known instances of the rela-
tions of interest as a form of distant supervision
(Mintz et al., 2009; Riedel et al., 2010). Some au-
thors have also considered unsupervised relation
extraction methods (Shinyama and Sekine, 2006;

Banko et al., 2007), in which case the aim is es-
sentially to find clusters of patterns that express
similar relationships, although these relationships
may not correspond to the ones that are needed for
the considered application. Finally, several sys-
tems have also used bootstrapping strategies (Brin,
1998; Agichtein and Gravano, 2000; Carlson et al.,
2010), where a small set of instances are used to
find extraction patterns, which are used to find
more instances, which can in turn be used to find
better extraction patterns, etc.

Traditionally, relation extraction systems have
relied on a variety of linguistic features, such as
lexical patterns, part-of-speech tags and depen-
dency parsers. More recently, several neural net-
work architectures have been proposed for the re-
lation extraction problem. These architectures rely
on word embeddings to represent the words in the
input sentence, and manipulate these word vectors
to construct a relation vector. Some approaches
simply represent the sentence (or the phrase con-
necting the entities whose relationship we want to
determine) as a sequence of words, and use e.g.
convolutional networks to aggregate the vectors of
the words in this sequence (Zeng et al., 2014; dos
Santos et al., 2015). Another possibility, explored
in (Socher et al., 2012), is to use parse trees to cap-
ture the structure of the sentence, and to use re-
cursive neural networks (RNNs) to aggregate the
word vectors in a way which respects this struc-
ture. A similar approach is taken in (Xu et al.,
2015), where LSTMs are applied to the shortest
path between the two target words in a depen-
dency parser. A straightforward baseline method
is to simply take the average of the word vec-
tors (Mitchell and Lapata, 2010). While conceptu-
ally much simpler, variants of this approach have
obtained state-of-the-art performance for relation
classification (Hashimoto et al., 2015) and a va-
riety of tasks that require sentences to be repre-
sented as a vector (Hill et al., 2016).

Given the effectiveness of word vector averag-
ing, in (Kenter et al., 2016) a model was proposed
that explicitly tries to learn word vectors that gen-
eralize well when being averaged. Similarly, the
model proposed in (Hashimoto et al., 2015) aims
to produce word vectors that perform well for the
specific task of relation classification. The Para-
graphVector method from (Le and Mikolov, 2014)
is related to the aformentioned approaches, but it
explicitly learns a vector representation for each



25

paragraph along with the word embeddings. How-
ever, this method is computationally expensive,
and often fails to outperform simpler approaches
(Hill et al., 2016).

To the best of our knowledge, existing methods
for learning relation vectors are all based on ma-
nipulating pre-trained word vectors. In contrast,
we will directly learn relation vectors from cor-
pus statistics, which will have the important ad-
vantage that we can focus on words that describe
the interaction between the two words s and t, i.e.
words that commonly occur in sentences that con-
tain both s and t, but are comparatively rare in sen-
tences that only contain s or only contain t.

Finally, note that our work is fundamentally dif-
ferent from Knowledge Graph Embedding (KGE)
(Wang et al., 2014b), (Wang et al., 2014a), (Bor-
des et al., 2011) in at least two ways: (i) KGE
models start from a structured knowledge graph
whereas we only take a text corpus as input, and
(ii) KGE models represent relations as geometric
objects in the “entity embedding” itself (e.g. as
translations, linear maps, combinations of projec-
tions and translations, etc), whereas we represent
words and relations in different vector spaces.

3 Word Vectors as PMI Encodings

Our approach to relation embedding is based on
a variant of the GloVe word embedding model
(Pennington et al., 2014). In this section, we first
briefly recall the GloVe model itself, after which
we discuss our proposed variant. A key advantage
of this variant is that it allows us to directly inter-
pret word vectors in terms of the Pointwise Mu-
tual Information (PMI), which will be central to
the way in which we learn relation vectors.

3.1 Background

The GloVe model (Pennington et al., 2014) learns
a vector wi for each word i in the vocabulary,
based on a matrix of co-occurrence counts, en-
coding how often two words appear within a given
window. Let us write xij for the number of times
word j appears in the context of word i in some
text corpus. More precisely, assume that there are
m sentences in the corpus, and letP li ⊆ {1, ..., nl}
be the set of positions from the lth sentence where
the word i can be found (with nl the length of the

sentence). Then xij is defined as follows:

m∑
l=1

∑
p∈Pli

∑
q∈Plj

weight(p, q)

where weight(p, q) = 1|p−q| if 0 < |p − q| ≤ W ,
and weight(p, q) = 0 otherwise, where the win-
dow size W is usually set to 5 or 10.

The GloVe model learns for each word i two
vectors wi and w̃i by optimizing the following ob-
jective:∑

i

∑
j:xij 6=0

f(xij)(wi·w̃j + bi + b̃j − log xij)2

where f is a weighting function, aimed at re-
ducing the impact of rare terms, and bi and b̃j
are bias terms. The GloVe model is closely re-
lated to the notion of pointwise mutual informa-
tion (PMI), which is defined for two words i and j
as PMI(i, j) = log

( P (i,j)
P (i)P (j)

)
, where P (i, j) is the

probability of seeing the words i and j if we ran-
domly pick a word position from the corpus and a
second word position within distance W from the
first position. The PMI between i and j is usually
estimated as follows:

PMIX(i, j) = log
(
xijx∗∗
xi∗x∗j

)
where xi∗ =

∑
j xij , x∗j =

∑
i xij and x∗∗ =∑

i

∑
j xij . In particular, it is straightforward to

see that after the reparameterization given by bi 7→
bi + log xi∗ − log x∗∗ and bj 7→ bj + log x∗j , the
GloVe model is equivalent to∑
i

∑
j

xij 6=0

f(xij)(wi·w̃j + bi + b̃j − PMIX(i, j))2

(1)

3.2 A Variant of GloVe
In this paper, we will use the following variant of
the formulation in (1):∑

i

∑
j∈Ji

1

σ2j
(wi·w̃j + b̃j − PMIS(i, j))2 (2)

Despite its similarity, this formulation differs from
the GloVe model in a number of important ways.
First, we use smoothed frequency counts instead
of the observed frequency counts xij . In particu-
lar, the PMI between words i and j is given as:

PMIS(i, j) = log
(

P (i, j)

P (i)P (j)

)



26

where the probabilities are estimated as follows:

P (i) =
xi∗ + α

x∗∗ + nα
P (j) =

x∗j + α

x∗∗ + nα

P (i, j) =
xij + α

x∗∗ + n2α

where α ≥ 0 is a parameter controlling the amount
of smoothing and n is the size of the vocabulary.
This ensures that the estimation of PMI(i, j) is
well-defined even in cases where xij = 0, mean-
ing that we no longer have to restrict the inner
summation to those j for which xij > 0. For
efficiency reasons, in practice, we only consider
a small subset of all context words j for which
xij = 0, which is similar in spirit to the use of
negative sampling in Skip-gram (Mikolov et al.,
2013b). In particular, the set Ji contains each j
such that xij > 0 as well as M uniformly1 sam-
pled context words j for which xij = 0, where we
choose M = 2 · |{j : xij > 0}|.

Second, following (Jameel and Schockaert,
2016), the weighting function f(xij) has been re-
placed by 1

σ2j
, where σ2j is the residual variance of

the regression problem for context word j, esti-
mated follows:

σ2j =
1

|J−1j |

∑
i∈J−1j

(wi · w̃j + b̃j − PMIS(i, j))2

with J−1j = {i : j ∈ Ji}. Since we need the word
vectors to estimate this residual variance, we re-
estimate σ2j after every five iterations of the SGD
optimization. For the first 5 iterations, where no
estimation for σ2j is available, we use the GloVe
weighting function.

The use of smoothed frequency counts and
residual variance based weighting make the word
embedding model more robust for rare words. For
instance, if w only co-occurs with a handful of
other terms, it is important to prioritize the most
informative context words, which is exactly what
the use of the residual variance achieves, i.e. σ2j
is small for informative terms and large for stop
words; see (Jameel and Schockaert, 2016). This
will be important for modeling relations, as the re-
lation vectors will often have to be estimated from
very sparse co-occurrence counts.

1While the negative sampling method used in Skip-gram
favors more frequent words, initial experiments suggested
that deviating from a uniform distribution almost had no im-
pact in our setting.

Finally, the bias term bi has been omitted from
the model in (2). We have empirically found that
omitting this bias term does not affect the perfor-
mance of the model, while it allows us to have a
more direct connection between the vector wi and
the corresponding PMI scores.

3.3 Word Vectors and PMI
Let us define PMIW as follows:

PMIW (i, j) = wi·w̃j + b̃j

Clearly, when the word vectors are trained accord-
ing to (2), it holds that PMIW (i, j) ≈ PMIS(i, j).
In other words, we can think of the word vector
wi as a low-dimensional encoding of the vector
(PMIS(i, 1), ...,PMIS(i, n)), with n the number
of words in the vocabulary. This view allows us to
assign a natural interpretation to some word vec-
tor operations. In particular, the vector difference
wi−wk is commonly used as a model for the rela-
tionship between words i and k. For a given con-
text word j, we have

(wi − wk) · w̃j = PMIW (i, j)− PMIW (k, j)

The latter is an estimation of log
(

P (i,j)
P (i)P (j)

)
−

log
(

P (k,j)
P (k)P (j)

)
= log

(
P (j|i)
P (j|k)

)
. In other words,

the vector translation wi − wk encodes for each
context word j the (log) ratio of the probability of
seeing j in the context of i and in the context of k,
which is in line with the original motivation under-
lying the GloVe model (Pennington et al., 2014).
In the following section, we will propose a num-
ber of alternative vector representations for the re-
lationship between two words, based on general-
izations of PMI to three arguments.

4 Learning Global Relation Vectors

We now turn to the problem of learning a vector
rik that encodes how the source word i and tar-
get word k are related. The main underlying idea
is that rik will capture which context words j are
most closely associated with the word pair (i, k).
Whereas the GloVe model is based on statistics
about (main word, context word) pairs, here we
will need statistics on (source word, context word,
target word) triples. First, we discuss how co-
occurrence statistics among three words can be ex-
pressed using generalizations of PMI to three ar-
guments. Then we explain how this can be used to
learn relation vectors in natural way.



27

4.1 Co-occurrence Statistics for Triples
Let P li ⊆ {1, ..., nl} again be the set of positions
from the lth sentence corresponding to word i. We
define:

yijk =

m∑
l=1

∑
p∈Pli

∑
q∈Plj

∑
r∈Plk

weight(p, q, r)

where weight(p, q, r) = max( 1q−p ,
1
r−q ) if p <

q < r and r−p ≤W , and weight(p, q, r) = 0 oth-
erwise. In other words, yijk reflects the (weighted)
number of times word j appears between words i
and k in a sentence in which i and k occur suffi-
ciently close to each other, in that order. Note that
by taking word order into account in this way, we
will be able to model asymmetric relationships.

To model how strongly a context word j is asso-
ciated with the word pair (i, k), we will consider
the following two well-known generalizations of
PMI to three arguments (Van de Cruys, 2011):

SI1(i, j, k) = log
(

P (i, j)P (i, k)P (j, k)

P (i)P (j)P (k)P (i, j, k)

)
SI2(i, j, k) = log

(
P (i, j, k)

P (i)P (j)P (k)

)
where P (i, j, k) is the probability of seeing the
word triple (i, j, k) when randomly choosing a
sentence and three (ordered) word positions in that
sentence within a window size of W . In addition
we will also consider two ways in which PMI can
be used more directly:

SI3(i, j, k) = log
(

P (i, j, k)

P (i, k)P (j)

)
SI4(i, j, k) = log

(
P (i, k|j)

P (i|j)P (k|j)

)
Note that SI3(i, j, k) corresponds to the PMI be-
tween (i, k) and j, whereas SI4(i, j, k) is the PMI
between i and k conditioned on the fact that j oc-
curs. The measures SI3 and SI4 are closely related
to SI1 and SI2 respectively2. In particular, the fol-
lowing identities are easy to show:

PMI(i, j) + PMI(j, k)− SI1(i, j, k) = SI3(i, j, k)
SI2(i, j, k)− PMI(i, j)− PMI(j, k) = SI4(i, j, k)

2Note that probabilities of the form P (i, j) or P (i) here
refer to marginal probabilities over ordered triples. In con-
trast, the PMI scores from the word embedding model are
based on probabilities over unordered word pairs, as is com-
mon for word embeddings.

Using smoothed versions of the counts yijk, we
can use the following probability estimates for
SI1(i, j, k)–SI4(i, j, k):

P (i, j, k) =
yijk + α

y∗∗∗ + n3α
P (i, j) =

yij∗ + α

y∗∗∗ + n2α

P (i, k) =
yi∗k + α

y∗∗∗ + n2α
P (j, k) =

y∗jk + α

y∗∗∗ + n2α

P (i) =
yi∗∗ + α

y∗∗∗ + nα
P (j) =

y∗j∗ + α

y∗∗∗ + nα

P (k) =
y∗∗k + α

y∗∗∗ + nα

where yij∗ =
∑

k yijk, and similar for the other
counts. For efficiency reasons, the counts of the
form yij∗, yi∗k and y∗jk are pre-computed for all
word pairs, which can be done efficiently due to
the sparsity of co-occurrence counts (i.e. these
counts will be 0 for most pairs of words), sim-
ilarly to how to the counts xij are computed in
GloVe. From these counts, we can also efficiently
pre-compute the counts yi∗∗, y∗j∗, y∗∗k and y∗∗∗.
On the other hand, the counts yijk cannot be pre-
computed, since the total number of triples for
which yijk 6= 0 is prohibitively high in a typi-
cal corpus. However, using an inverted index, we
can efficiently retrieve the sentences that contain
the words i and k, and since this number of sen-
tences is typically small, we can efficiently obtain
the counts yijk corresponding to a given pair (i, k)
whenever they are needed.

4.2 Relation Vectors
Our aim is to learn a vector rik that models the
relationship between i and k. Computing such a
vector for each pair of words (which co-occur at
least once) is not feasible, given the number of
triples (i, j, k) that would need to be considered.
Instead, we first learn a word embedding, by op-
timizing (2). Then, fixing the context vectors w̃j
and bias terms bj , we learn a vector representation
for a given pair (i, k) of interest by solving the fol-
lowing objective:∑

j∈Ji,k

(rik·w̃j + b̃j − SI(i, j, k))2 (3)

where SI refers to one of SI1S , SI
2
S , SI

3
S , SI

4
S . Note

that (3) is essentially the counterpart of (1), where
we have replaced the role of the PMI measure by
SI. In this way, we can exploit the representations
of the context words from the word embedding
model for learning relation vectors. Note that the



28

factor 1
σ2j

has been omitted. This is because words

j that are normally relatively uninformative (e.g.
stop words), for which σ2j would be high, can actu-
ally be very important for characterizing the rela-
tionship between i and k. For instance, the phrase
“X such as Y ” clearly suggests a hyponomy re-
lationship between X and Y , but both ‘such’ and
‘as’ would be associated with a high residual vari-
ance σ2j . The set Ji,k contains every j for which
yijk > 0 as well as a random sample of m words
for which yijk = 0, where m = 2 · |{j : yijk > 0|.
Note that because w̃j is now fixed, (3) is a lin-
ear least squares regression problem, which can be
solved exactly and efficiently.

The vector rik is based on words that appear
between i and k. In the same way, we can learn
a vector sik based on the words that appear be-
fore i and a vector tik based on the words that
appear after k, in sentences where i occurs be-
fore k. Furthermore, we also learn vectors rki, ski
and tki from the sentences where k occurs before
i. As the final representation Rik of the relation-
ship between i and k, we concatenate the vectors
rik, rki, sik, ski, tik, tki as well as the word vectors
wi and wk. We write Rlik to denote the vector that
results from using measure SIl (l ∈ {1, 2, 3, 4}).

5 Experimental Results

In our experiments, we have used the Wikipedia
dump from November 2nd, 2015, which consists
of 1,335,766,618 tokens. We have removed punc-
tuations and HTML/XML tags, and we have low-
ercased all tokens. Words with fewer than 10
occurrences have been removed from the corpus.
To detect sentence boundaries, we have used the
Apache sentence segmentation tool. In all our
experiments, we have set the number of dimen-
sions to 300, which was found to be a good choice
in previous work, e.g. (Pennington et al., 2014).
We use a context window size W of 10 words.
The number of iterations for SGD was set to
50. For our model, we have tuned the smooth-
ing parameter α based on held-out tuning data,
considering values from {0.1, 0.01, 0.001, 0.0001,
0.00001, 0.000001}. We have noticed that in most
of the cases the value of α was automatically se-
lected as 0.00001. To efficiently compute the
triples, we have used the Zettair3 retrieval engine.

As our main baselines, we use three popular un-
supervised methods for constructing relation vec-

3http://www.seg.rmit.edu.au/zettair/

Table 1: Results for the relation induction task.

Google Analogy
Diff Conc Avg R1ik R

2
ik R

3
ik R

4
ik

Acc 90.0 89.0 89.9 90.0 92.3 90.9 90.4
Pre 81.6 78.7 80.8 79.9 87.1 83.2 81.1
Rec 82.6 83.9 83.9 86.0 84.8 84.8 85.5
F1 82.1 81.2 82.3 82.8 85.9 84.0 83.3

DiffVec
Diff Conc Avg R1ik R

2
ik R

3
ik R

4
ik

Acc 29.5 28.9 29.7 29.7 31.3 30.4 30.1
Pre 19.6 18.7 20.4 21.5 22.9 21.9 22.3
Rec 23.8 22.9 23.7 24.5 25.7 25.3 22.9
F1 21.5 20.6 21.9 22.4 24.2 23.5 22.6

tors. First, Diff uses the vector difference wk−wi,
following the common strategy of modeling rela-
tions as vector differences, as e.g. in (Vylomova
et al., 2016). Second, Conc uses the concatenation
ofwi andwk. This model is more general than Diff
but it uses twice as many dimensions, which may
make it harder to learn a good classifier from few
examples. The use of concatenations is popular
e.g. in the context of hypernym detection (Baroni
et al., 2012). Finally, Avg averages the vector rep-
resentations of the words occurring in sentences
that Diff, contain i and k. In particular, let ravgik be
obtained by averaging the word vectors of the con-
text words appearing between i and k for each sen-
tence containing i and k (in that order), and then
averaging the vectors obtained from each of these
sentences. Let savgik and t

avg
ik be similarly obtained

from the words occurring before i and the words
occurring after k respectively. The considered re-
lation vector is then defined as the concatenation
of ravgik , r

avg
ki , s

avg
ik , s

avg
ki , t

avg
ik , t

avg
ki , wi and wk. The

Avg will allow us to directly compare how much
we can improve relation vectors by deviating from
the common strategy of averaging word vectors.

5.1 Relation Induction

In the relation induction task, we are given word
pairs (s1, t1), ..., (sk, tk) that are related in some
way, and the task is to decide for a number of test
examples (s, t) whether they also have this rela-
tionship. Among others, this task was considered
in (Vylomova et al., 2016), and a ranking version
of this task was studied in (Drozd et al., 2016).
As test sets we use the Google Analogy Test Set
(Mikolov et al., 2013a), which contains instances
of 14 different types of relations, and the DiffVec
dataset, which was introduced in (Vylomova et al.,
2016). This dataset contains instances of 36 dif-

http://www.seg.rmit.edu.au/zettair/


29

Table 2: Results for the relation induction task using alternative word embedding models.

GloVe SkipGram CBOW
Google DiffVec Google DiffVec Google DiffVec

Acc F1 Acc F1 Acc F1 Acc F1 Acc F1 Acc F1
Diff 90.0 81.9 21.2 13.9 89.8 81.9 21.7 14.5 89.9 82.1 17.4 9.7
Conc 88.9 80.4 20.2 11.9 89.2 81.6 20.5 12.0 89.1 81.1 16.4 7.7
Avg 89.8 82.1 21.4 13.9 90.2 82.4 21.8 14.4 89.8 82.2 17.5 10.0
R1ik 89.7 81.7 20.9 12.5 89.4 81.2 21.1 12.3 89.8 81.9 17.2 9.2
R2ik 90.0 82.8 21.2 13.4 89.1 81.3 21.1 12.9 90.2 82.4 17.7 10.0
R3ik 90.0 82.3 20.0 11.2 89.5 81.1 20.5 12.3 89.5 81.1 17.2 9.6
R4ik 90.0 82.5 20.0 11.4 88.9 80.8 20.6 12.1 90.5 82.2 17.1 8.4

ferent types of relations4. Note that both datasets
contain a mix of semantic and syntactic relations.

In our evaluation, we have used 10-fold cross-
validation (or leave-one-out for relations with
fewer than 10 instances). In the experiments, we
consider for each relation in the test set a separate
binary classification task, which was found to be
considerably more challenging than a multi-class
classification setting in (Vylomova et al., 2016).
To generate negative examples in the training data
(resp. test data), we have used three strategies, fol-
lowing (Vylomova et al., 2016). First, for a given
positive example (s, t) of the considered relation,
we add (t, s) as a negative example. Second, for
each positive example (s, t), we generate two neg-
ative examples (s, t1) and (s, t2) by randomly se-
lecting two tail words t1, t2 from the other training
(resp. test) examples of the same relation. Finally,
for each positive example, we also generate a neg-
ative example by randomly selecting two words
from the vocabulary. For each relation, we then
train a linear SVM classifier. To set the parameters
of the SVM, we initially use 25% of the training
data for tuning, and then retrain the SVM with the
optimal parameters on the full training data.

The results are summarized in Table 1 in terms
of accuracy and (macro-averaged) precision, recall
and F1 score. As can be observed, our model out-
performs the baselines on both datasets, with the
R2ik variant outperforming the others.

To analyze the benefit of our proposed word
embedding variant, Table 2 shows the results that
were obtained when we use standard word embed-
ding models. In particular, we show results for the
standard GloVe model, SkipGram and the Contin-
uous Bag of Words (CBOW) model. As can be
observed, our variant leads to better results than
the original GloVe model, even for the baselines.

4Note that in contrast to (Vylomova et al., 2016) we use
all 36 relations from this dataset, including those with very
few instances.

Table 3: Relation induction without position
weighting (left) and without the relation vectors
sik and tik (right).

Google DiffVec
Acc F1 Acc F1

R1ik 89.7 82.4 30.2 22.2
R2ik 91.0 83.4 30.8 24.1
R3ik 90.4 83.2 30.1 22.3
R4ik 90.2 82.9 29.1 21.2

Google DiffVec
Acc F1 Acc F1

R1ik 90.0 82.5 29.9 22.3
R2ik 92.3 85.8 31.2 24.2
R3ik 90.5 83.2 30.2 23.0
R4ik 90.3 83.1 29.8 22.3

The difference is particularly noticeable for Diff-
Vec. The difference is also larger for our relation
vectors than for the baselines, which is expected as
our method is based on the assumption that con-
text word vectors can be interpreted in terms of
PMI scores, which is only true for our variant.

Similar as in the GloVe model, the context
words in our model are weighted based on their
distance to the nearest target word. Table 3 shows
the results of our model without this weighting, for
the relation induction task. Comparing these re-
sults with those in Table 1 shows that the weight-
ing scheme indeed leads to a small improvement
(except for the accuracy of R1ik for DiffVec). Sim-
ilarly, in Table 3, we show what happens if the re-
lation vectors sik, ski, tik and tki are omitted. In
other words, for the results in Table 3, we only
use context words that appear between the two
target words. Again, the results are worse than
those in Table 1 (with the accuracy ofR1ik for Diff-
Vec again being an exception), although the dif-
ferences are very small in this case. While includ-
ing the vectors sik, ski, tik, tki should be helpful,
it also significantly increases the dimensionality
of the vectors Rlik. Given that the number of in-
stances per relation is typically quite small for this



30

Table 4: Results for measuring degrees of proto-
typicality (Spearman ρ× 100).

Diff Conc Avg R1ik R
2
ik R

3
ik R

4
ik

17.3 16.7 21.1 22.7 23.9 21.8 22.2

task, this can also make it harder to learn a suitable
classifier.

5.2 Measuring Degrees of Prototypicality

Instances of relations can often have different de-
grees of prototypicality. For example, for the rela-
tion “X characteristically makes the sound Y ”, the
pair (dog,bark) should be considered more proto-
typical than the pair (floor,squeak), even though
both pairs might be considered to be instances
of the relation (Jurgens et al., 2012). A suit-
able relation vector should allow us to rank word
pairs according to how prototypical they are as
instances of that relation. We evaluate this abil-
ity using a dataset that was produced in the after-
math of SemEval 2012 Task 2. In particular, we
have used the “Phase2AnswerScaled” data from
the platinum rankings dataset, which is available
from the SemEval 2012 Task 2 website5. In this
dataset, 79 ranked list of word pairs are provided,
each of which corresponds to a particular relation.
For each relation, we first split the associated rank-
ing into 60% training, 20% tuning, and 20% test-
ing (i.e. we randomly select 60% of the word pairs
and use their ranking as training data, and similar
for tuning and test data). We then train a linear
SVM regression model on the ranked word pairs.
Note that this task slightly differs from the task
that was considered at SemEval 2012, to allow us
to use an SVM based model for consistency with
the rest of the paper.

We report results using Spearman’s ρ in Table
4. Our model again outperforms the baselines,
with R2ik again being the best variant. Interest-
ingly, in this case, the Avg baseline is consider-
ably stronger than Diff and Conc. Intuitively, we
might indeed expect that this ranking problem re-
quires a more fine-grained representation than the
relation induction setting. Note that the Diff repre-
sentations were found to achieve near state-of-the-
art performance on a closely related task in (Zhila
et al., 2013). The only model that was found to
perform (slightly) better was a hybrid model, com-
bining Diff representations with linguistic patterns

5https://sites.google.com/site/semeval2012task2/download

0 0.1 0.2 0.3 0.4
0

0.2

0.4

0.6

0.8

1

Recall

Pr
ec

is
io

n

R1ik
R2ik

R2ik(Quadratic)
R3ik
R4ik
Avg
Diff
Conc

Figure 1: Results for the relation extraction from
the NYT corpus: comparison with the main base-
lines.

(inspired by (Rink and Harabagiu, 2012)) and lex-
ical databases, among others.

5.3 Relation Extraction

Finally, we consider the problem of relation ex-
traction from a text corpus. Specifically, we con-
sider the task proposed in (Riedel et al., 2010),
which is to extract (subject,predicate,object)
triples from the New York Times (NYT) corpus.
Rather than having labelled sentences as training
data, the task requires using the existing triples
from Freebase as a form of distant supervision, i.e.
for some pairs of entities we know some of the
relations that hold between them, but not which
sentences assert these relationships (if any). To be
consistent with published results for this task, we
have used a word embedding that was trained from
the NYT corpus6, rather than Wikipedia (using the
same preprocessing and set-up). We have used the
training and test data that was shared publicly for
this task7, which consist of sentences from arti-
cles published in 2005-2006 and in 2007, respec-
tively. Each of these sentences contains two en-
tities, which are already linked to Freebase. We
learn relation vectors from the sentences in the
training and test sets, and learn a linear SVM clas-
sifier based on the Freebase triples that are avail-
able in the training set. Initially, we split the train-
ing data into 75% training and 25% tuning to find
the optimal parameters of the linear SVM model.
We tuned the parameters for each test fold sepa-

6https://catalog.ldc.upenn.edu/LDC2008T19
7http://iesl.cs.umass.edu/riedel/ecml/



31

0 0.1 0.2 0.3 0.4
0

0.2

0.4

0.6

0.8

1

Recall

Pr
ec

is
io

n
R2ik(Quadratic)

CNN+ATT
Hoffmann

PCNN+ATT
MIMLRE

Mintz

Figure 2: Results for the relation extraction from
the NYT corpus: comparison with state-of-the-art
neural network models.

rately. For each test fold, we used 25% of the 9
training folds as tuning data. After the optimal
parameters have been determined, we retrain the
model on the full training data, and apply it on
the test fold. We used this approach (rather than
e.g. fixing a train/tune/test split) because the to-
tal number of examples for some of the relations
is very small. After tuning, we re-train the SVM
models on the full training data. As the number
of training examples is larger for this task, we also
consider SVMs with a quadratic kernel.

Following earlier work on this task, we re-
port our results on the test set as a precision-
recall graph in Figure 1. This shows that the
best performance is again achieved by R2ik, espe-
cially for larger recall values. Furthermore, us-
ing a quadratic kernel (only shown for R2ik) out-
performs the linear SVM models. Note that the
differences between the baselines are more pro-
nounced in this task, with Avg being clearly bet-
ter than Diff, which is in turn better than Conc.
For this relation extraction task, a large number
of methods have already been proposed in the lit-
erature, with variants of convolutional neural net-
work models with attention mechanisms achiev-
ing state-of-the-art performance8. A comparison
with these models9 is shown in Figure 2. The per-
formance of R2ik is comparable with the state-of-

8Note that such models would not be suitable for the eval-
uation tasks in Sections 5.1 and 5.2, due to the very limited
number of training examples.

9Results for the neural network models have been
obtained from https://github.com/thunlp/
TensorFlow-NRE/tree/master/data.

the-art PCNN+ATT model (Lin et al., 2016), out-
performing it for larger recall values. This is re-
markable, as our model is conceptually much sim-
pler, and has not been specifically tuned for this
task. For instance, it could easily be improved by
incorporating the attention mechanism from the
PCNN+ATT model to focus the relation vectors
on the considered task. Similarly, we could con-
sider a supervised variant of (3), in which a learned
relation-specific weight is added to each term.

6 Conclusions

We have proposed an unsupervised method which
uses co-occurrences statistics to represent the re-
lationship between a given pair of words as a vec-
tor. In contrast to neural network models for rela-
tion extraction, our model learns relation vectors
in an unsupervised way, which means that it can
be used for measuring relational similarities and
related tasks. Moreover, even in (distantly) super-
vised tasks (where we need to learn a classifier
on top of the unsupervised relation vectors), our
model has proven competitive with state-of-the-art
neural network models. Compared to approaches
that rely on averaging word vectors, our method is
able to learn more faithful representations by fo-
cusing on the words that are most strongly related
to the considered relationship.

Acknowledgments

This work was supported by ERC Starting Grant
637277. Experiments in this work were performed
using the computational facilities of the Advanced
Research Computing at Cardiff (ARCCA) Divi-
sion, Cardiff University and the ICARUS compu-
tational facility from Information Services, at the
University of Kent.

References
Eugene Agichtein and Luis Gravano. 2000. Snow-

ball: Extracting relations from large plain-text col-
lections. In Proceedings of the Fifth ACM Confer-
ence on Digital libraries. pages 85–94.

Michele Banko, Michael J Cafarella, Stephen Soder-
land, Matthew Broadhead, and Oren Etzioni. 2007.
Open information extraction from the web. In Proc.
IJCAI. pages 2670–2676.

Marco Baroni, Raffaella Bernardi, Ngoc-Quynh Do,
and Chung-chieh Shan. 2012. Entailment above
the word level in distributional semantics. In Proc.
EACL. pages 23–32.

https://github.com/thunlp/TensorFlow-NRE/tree/master/data
https://github.com/thunlp/TensorFlow-NRE/tree/master/data


32

A. Bordes, J. Weston, R. Collobert, and Y. Bengio.
2011. Learning structured embeddings of knowl-
edge bases. In AAAI.

Sergey Brin. 1998. Extracting patterns and relations
from the world wide web. In International Work-
shop on The World Wide Web and Databases. pages
172–183.

Andrew Carlson, Justin Betteridge, Bryan Kisiel, Burr
Settles, Estevam .R. Hruschka Jr., and Tom M.
Mitchell. 2010. Toward an architecture for never-
ending language learning. In Proc. AAAI. pages
1306–1313.

George R. Doddington, Alexis Mitchell, Mark A. Przy-
bocki, Lance A. Ramshaw, Stephanie Strassel, and
Ralph M. Weischedel. 2004. The automatic content
extraction (ACE) program - tasks, data, and evalua-
tion. In Proc. LREC.

Cı́cero Nogueira dos Santos, Bing Xiang, and Bowen
Zhou. 2015. Classifying relations by ranking with
convolutional neural networks. In Proc. ACL. pages
626–634.

Aleksandr Drozd, Anna Gladkova, and Satoshi Mat-
suoka. 2016. Word embeddings, analogies, and ma-
chine learning: Beyond king - man + woman =
queen. In Proc. COLING. pages 3519–3530.

Miao Fan, Kai Cao, Yifan He, and Ralph Grishman.
2015. Jointly embedding relations and mentions for
knowledge population. In Proc. RANLP. pages 186–
191.

Kazuma Hashimoto, Pontus Stenetorp, Makoto Miwa,
and Yoshimasa Tsuruoka. 2015. Task-oriented
learning of word embeddings for semantic relation
classification. In Proc. CoNLL. pages 268–278.

Iris Hendrickx, Su Nam Kim, Zornitsa Kozareva,
Preslav Nakov, Diarmuid Ó Séaghdha, Sebastian
Padó, Marco Pennacchiotti, Lorenza Romano, and
Stan Szpakowicz. 2010. Semeval-2010 task 8:
Multi-way classification of semantic relations be-
tween pairs of nominals. In Proc. SemEval. pages
33–38.

Felix Hill, Kyunghyun Cho, and Anna Korhonen. 2016.
Learning distributed representations of sentences
from unlabelled data. In Proc. NAACL-HLT . pages
1367–1377.

Nitin Indurkhya and Fred J Damerau. 2010. Handbook
of natural language processing, volume 2. CRC
Press.

Shoaib Jameel and Steven Schockaert. 2016. D-GloVe:
A feasible least squares model for estimating word
embedding densities. In Proc. COLING. pages
1849–1860.

David A Jurgens, Peter D Turney, Saif M Mohammad,
and Keith J Holyoak. 2012. Semeval-2012 task 2:
Measuring degrees of relational similarity. In Proc.
*SEM. pages 356–364.

Tom Kenter, Alexey Borisov, and Maarten de Rijke.
2016. Siamese CBOW: optimizing word embed-
dings for sentence representations. In Proc. ACL.

Quoc Le and Tomas Mikolov. 2014. Distributed rep-
resentations of sentences and documents. In Proc.
ICML. pages 1188–1196.

Yankai Lin, Shiqi Shen, Zhiyuan Liu, Huanbo Luan,
and Maosong Sun. 2016. Neural relation extrac-
tion with selective attention over instances. In Proc.
ACL.

Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey
Dean. 2013a. Efficient estimation of word represen-
tations in vector space. In Proc. ICLR.

Tomas Mikolov, Ilya Sutskever, Kai Chen, Gregory S.
Corrado, and Jeffrey Dean. 2013b. Distributed rep-
resentations of words and phrases and their compo-
sitionality. In Proc. NIPS. pages 3111–3119.

Mike Mintz, Steven Bills, Rion Snow, and Dan Ju-
rafsky. 2009. Distant supervision for relation ex-
traction without labeled data. In Proc. ACL. pages
1003–1011.

Jeff Mitchell and Mirella Lapata. 2010. Composition
in distributional models of semantics. Cognitive Sci-
ence 34(8):1388–1429.

Jeffrey Pennington, Richard Socher, and Christo-
pher D. Manning. 2014. GloVe: Global vectors for
word representation. In Proc. EMNLP. pages 1532–
1543.

Sebastian Riedel, Limin Yao, and Andrew McCallum.
2010. Modeling relations and their mentions with-
out labeled text. In Proc. ECML/PKDD. pages 148–
163.

Bryan Rink and Sanda Harabagiu. 2012. UTD: De-
termining relational similarity using lexical patterns.
In Proceedings of the First Joint Conference on Lex-
ical and Computational Semantics. pages 413–418.

Yusuke Shinyama and Satoshi Sekine. 2006. Preemp-
tive information extraction using unrestricted rela-
tion discovery. In Proc. NAACL-HLT . pages 304–
311.

Richard Socher, Brody Huval, Christopher D Manning,
and Andrew Y Ng. 2012. Semantic compositional-
ity through recursive matrix-vector spaces. In Proc.
EMNLP. pages 1201–1211.

Tim Van de Cruys. 2011. Two multivariate general-
izations of pointwise mutual information. In Pro-
ceedings of the Workshop on Distributional Seman-
tics and Compositionality. pages 16–20.

Ekaterina Vylomova, Laura Rimell, Trevor Cohn, and
Timothy Baldwin. 2016. Take and took, gaggle and
goose, book and read: Evaluating the utility of vec-
tor differences for lexical relation learning. In Proc.
ACL.



33

Z. Wang, J. Zhang, J. Feng, and Z. Chen. 2014a.
Knowledge graph and text jointly embedding. In
EMNLP. pages 1591–1601.

Zhen Wang, Jianwen Zhang, Jianlin Feng, and Zheng
Chen. 2014b. Knowledge graph embedding by
translating on hyperplanes. In AAAI. pages 1112–
1119.

Jason Weston, Antoine Bordes, Oksana Yakhnenko,
and Nicolas Usunier. 2013. Connecting language
and knowledge bases with embedding models for
relation extraction. In Proc. EMNLP. pages 1366–
1371.

Yan Xu, Lili Mou, Ge Li, Yunchuan Chen, Hao Peng,
and Zhi Jin. 2015. Classifying relations via long
short term memory networks along shortest depen-
dency paths. In Proc. EMNLP. pages 1785–1794.

Daojian Zeng, Kang Liu, Siwei Lai, Guangyou Zhou,
Jun Zhao, et al. 2014. Relation classification via
convolutional deep neural network. In Proc. COL-
ING. pages 2335–2344.

Alisa Zhila, Wen-tau Yih, Christopher Meek, Geof-
frey Zweig, and Tomas Mikolov. 2013. Combining
heterogeneous models for measuring relational sim-
ilarity. In Proceedings of the 2013 Conference of
the North American Chapter of the Association for
Computational Linguistics: Human Language Tech-
nologies. pages 1000–1009.


