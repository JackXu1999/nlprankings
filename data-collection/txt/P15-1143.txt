



















































Graph parsing with s-graph grammars


Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics
and the 7th International Joint Conference on Natural Language Processing, pages 1481–1490,

Beijing, China, July 26-31, 2015. c©2015 Association for Computational Linguistics

Graph parsing with s-graph grammars

Jonas Groschwitz and Alexander Koller and Christoph Teichmann
Department of Linguistics

University of Potsdam
firstname.lastname@uni-potsdam.de

Abstract

A key problem in semantic parsing with
graph-based semantic representations is
graph parsing, i.e. computing all pos-
sible analyses of a given graph accord-
ing to a grammar. This problem arises
in training synchronous string-to-graph
grammars, and when generating strings
from them. We present two algorithms for
graph parsing (bottom-up and top-down)
with s-graph grammars. On the related
problem of graph parsing with hyperedge
replacement grammars, our implementa-
tions outperform the best previous system
by several orders of magnitude.

1 Introduction

The recent years have seen an increased interest
in semantic parsing, the problem of deriving a se-
mantic representation for natural-language expres-
sions with data-driven methods. With the recent
availability of graph-based meaning banks (Ba-
narescu et al., 2013; Oepen et al., 2014), much
work has focused on computing graph-based se-
mantic representations from strings (Jones et al.,
2012; Flanigan et al., 2014; Martins and Almeida,
2014).

One major approach to graph-based semantic
parsing is to learn an explicit synchronous gram-
mar which relates strings with graphs. One can
then apply methods from statistical parsing to
parse the string and read off the graph. Chiang et
al. (2013) and Quernheim and Knight (2012) rep-
resent this mapping of a (latent) syntactic struc-
ture to a graph with a grammar formalism called
hyperedge replacement grammar (HRG; (Drewes
et al., 1997)). As an alternative to HRG, Koller
(2015) introduced s-graph grammars and showed
that they support linguistically reasonable gram-
mars for graph-based semantics construction.

One problem that is only partially understood
in the context of semantic parsing with explicit
grammars is graph parsing, i.e. the computation
of the possible analyses the grammar assigns to
an input graph (as opposed to string). This prob-
lem arises whenever one tries to generate a string
from a graph (e.g., on the generation side of an MT
system), but also in the context of extracting and
training a synchronous grammar, e.g. in EM train-
ing. The state of the art is defined by the bottom-
up graph parsing algorithm for HRG by Chiang et
al. (2013), implemented in the Bolinas tool (An-
dreas et al., 2013).

We present two graph parsing algorithms (top-
down and bottom-up) for s-graph grammars. S-
graph grammars are equivalent to HRGs, but em-
ploy a more fine-grained perspective on graph-
combining operations. This simplifies the parsing
algorithms, and facilitates reasoning about them.
Our bottom-up algorithm is similar to Chiang et
al.’s, and derives the same asymptotic number of
rule instances. The top-down algorithm is novel,
and achieves the same asymptotic runtime as the
bottom-up algorithm by reasoning about the bi-
connected components of the graph. Our eval-
uation on the “Little Prince” graph-bank shows
that our implementations of both algorithms out-
perform Bolinas by several orders of magnitude.
Furthermore, the top-down algorithm can be more
memory-efficient in practice.

2 Related work

The AMR-Bank (Banarescu et al., 2013) annotates
sentences with abstract meaning representations
(AMRs), like the one shown in Fig. 1(a). These
are graphs that represent the predicate-argument
structure of a sentence; notably, phenomena such
as control are represented by reentrancies in the
graph. Another major graph-bank is the SemEval-
2014 shared task on semantic dependency parsing
dataset (Oepen et al., 2014).

1481



(a)

A

AA

(b)

R

OS

(f)

R

OS

(c)

R

S

(d)

O

S

(e)

R

OS

arg2arg1

arg1

arg2arg1

arg1

arg2arg1 arg1 arg1 arg2arg1

arg1

want

sleepboy

want

sleepboy

want

boy

sleep sleep want

sleepboy

Figure 1: AMR (a) for ‘The boy wants to sleep’, and s-graphs. We call (b) SGwant and (c) SGsleep.

The primary grammar formalism currently in
use for synchronous graph grammars is hyper-
edge replacement grammar (HRG) (Drewes et al.,
1997), which we sketch in Section 4.3. An alterna-
tive is offered by Koller (2015), who introduced s-
graph grammars and showed that they lend them-
selves to manually written grammars for semantic
construction. In this paper, we show the equiv-
alence of HRG and s-graph grammars and work
out graph parsing for s-graph grammars.

The first polynomial graph parsing algorithm
for HRGs on graphs with limited connectivity was
presented by Lautemann (1988). Lautemann’s
original algorithm is a top-down parser, which is
presented at a rather abstract level that does not
directly support implementation or detailed com-
plexity analysis. We extend Lautemann’s work
by showing how new parse items can be repre-
sented and constructed efficiently. Finally, Chiang
et al. (2013) presented a bottom-up graph parser
for HRGs, in which the representation and con-
struction of items was worked out for the first time.
It produces O((n · 3d)k+1) instances of the rules
in a parsing schema, where n is the number of
nodes of the graph, d is the maximum degree of
any node, and k is a quantity called the tree-width
of the grammar.

3 An algebra of graphs

We start by introducing the exact type of graphs
that our grammars and parsers manipulate, and by
developing some theory.

Throughout this paper, we define a graph G =
(V,E) as a directed graph with edge labels from
some label alphabet L. The graph consists of a
finite set V of nodes and a finite setE ⊆ V ×V ×L
of edges e = (u, v, l), where u and v are the nodes
connected by e, and l ∈ L is the edge label. We
say that e is incident to both u and v, and call the
number of edges incident to a node its degree. We
write u e↔ v if either e = (u, v, l) or e = (v, u, l)
for some l; we drop the e if the identity of the edge
is irrelevant. Edges with u = v are called loops;
we use them here to encode node labels. Given a

graph G, we write n = |V |, m = |E|, and d for
the maximum degree of any node in V .

If f : A  B and g : A  B are partial func-
tions, we let the partial function f ∪ g be defined
if for all a ∈ A with both f(a) and g(a) defined,
we have f(a) = g(a). We then let (f ∪ g)(a) be
f(a) if f(a) is defined; g(a) if g(a) is defined; and
undefined otherwise.

3.1 The HR algebra of graphs with sources

Our grammars describe how to build graphs from
smaller pieces. They do this by accessing nodes
(called source nodes) which are assigned “public
names”. We define an s-graph (Courcelle and En-
gelfriet, 2012) as a pair SG = (G,φ) of a graph
G and a source assignment, i.e. a partial, injective
function φ : S  V that maps some source names
from a finite set S to the nodes of G. We call the
nodes in φ(S) the source nodes or sources of SG;
all other nodes are internal nodes. If φ is defined
on the source name σ, we call φ(σ) the σ-source
of SG. Throughout, we let s = |S|.

Examples of s-graphs are given in Fig. 1. We
use numbers as node names and lowercase strings
for edge names (except in the concrete graphs of
Fig. 1, where the edges are marked with edge la-
bels instead). Source nodes are drawn in black,
with source names drawn on the inside. Fig. 1(b)
shows an s-graph SGwant with three nodes and
four edges. The three nodes are marked as the R-,
S-, and O-source, respectively. Likewise, the s-
graph SGsleep in (c) has two nodes (one of which
is an R-source and the other an S-source) and two
edges.

We can now apply operations to these graphs.
First, we can rename the R-source of (c) to an O-
source. The result, denoted SGd = SGsleep[R →
O], is shown in (d). Next, we can merge SGd
with SGwant. This copies the edges and nodes
of SGd and SGwant into a new s-graph; but cru-
cially, for every source name σ the two s-graphs
have in common, the σ-sources of the graphs are
fused into a single node (and become a σ-source of
the result). We write || for the merge operation;

1482



thus we obtain SGe = SGd || SGwant, shown
in (e). Finally, we can forget source names. The
graph SGf = fS(fO(SGe)), in which we forgot S
and O, is shown in (f). We refer to Courcelle and
Engelfriet (2012) for technical details.1

We can take the set of all s-graphs, together with
these operations, as an algebra of s-graphs. In ad-
dition to the binary merge operation and the unary
operations for forget and rename, we fix some fi-
nite set of atomic s-graphs and take them as con-
stants of the algebra which evaluate to themselves.
Following Courcelle and Engelfriet, we call this
algebra the HR algebra. We can evaluate any term
τ consisting of these operation symbols into an s-
graph JτK as usual. For instance, the following
term encodes the merge, forget, and rename oper-
ations from the example above, and evaluates to
the s-graph in Fig. 1(f).

(1) fS(fO(SGwant || SGsleep[R→ O]))

The set of s-graphs that can be represented as
the value JτK of some term τ over the HR alge-
bra depends on the source set S and on the con-
stants. For simplicity, we assume here that we
have a constant for each s-graph consisting of a
single labeled edge (or loop), and that the values
of all other constants can be expressed by combin-
ing these using merge, rename, and forget.

3.2 S-components

A central question in graph parsing is how some
s-graph that is a subgraph of a larger s-graph SG
(a sub-s-graph) can be represented as the merge
of two smaller sub-s-graphs of SG. In general,
SG1 || SG2 is defined for any two s-graphs SG1
and SG2. However, if we see SG1 and SG2 as
subgraphs of SG, SG1 || SG2 may no longer be
a subgraph of SG. For instance, we cannot merge
the s-graphs (b) and (c) in Fig. 2 as part of the
graph (a): The startpoints of the edges a and d are
both A-sources and would thus become the same
node (unlike in (a)), and furthermore the edge d
would have to be duplicated. In graph parsing, we
already know the identity of all nodes and edges
in sub-s-graphs (as nodes and edges in SG), and
must thus pay attention that merge operations do
not accidentally fuse or duplicate them. In partic-

1 Note that the rename operation of Courcelle and En-
gelfriet (2012) allows for swapping source assignments and
making multiple renames in one step. We simplify the pre-
sentation here, but all of our techniques extend easily.

{d}2{a,b,c}

3 {e}

{f} {g} {h}

(a)

(b) (c)

(d) (e)

A1 A 2

A 4B3

C 6B5

A1 A 2

A 4B3

A 2B1

A 4

A1 A 4dA

2

A 4eB3

A5f

A5h A 6

A5g A 4g

a

db c d

a

b c d

e

f g

h

a

b c

d

f

e

h

g

{d}{a,b,c,f,e}

{a,b,c,d}

{e}

{f}

Figure 2: (a) An s-graph with (b,c) some sub-s-
graphs, (d) its BCCs, and (e) its block-cutpoint
graph.

ular, two sub-s-graphs cannot be merged if they
have edges in common.

We call a sub-s-graph SG1 of SG extensible if
there is another sub-s-graph SG2 of SG such that
SG1 || SG2 contains the same edges as SG. An
example of a sub-s-graph that is not extensible is
the sub-s-graph (b) of the s-graph in (a) in Fig. 2.
Because sources can only be renamed or forgotten
by the algebra operations, but never introduced,
we can never attach the missing edge a: this can
only happen when 1 and 2 are sources. As a gen-
eral rule, a sub-s-graph can only be extensible if
it contains all edges that are adjacent to all of its
internal nodes in SG. Obviously, a graph parser
need only concern itself with sub-s-graphs that are
extensible.

We can further clarify the structure of extensible
sub-s-graphs by looking at the s-components of a
graph. Let U ⊆ V be some set of nodes. This
set splits the edges of G into equivalence classes
that are separated by U . We say that two edges
e, f ∈ E are equivalent with respect toU , e ∼U f ,
if there is a sequence v1

e↔ v2 ↔ . . . vk−1 f↔ vk
with v2, . . . , vk−1 /∈ U , i.e. if we can reach f
from an endpoint of ewithout visiting a node in U .
We call the equivalence classes of E with respect
to ∼U the s-components of G and denote the s-
component that contains an edge e with [e]. In
Fig. 2(a), the edges a and f are equivalent with
respect to U = {4, 5}, but a and h are not. The s-
components are [a] = {a, b, c, d, e, f}, [g] = {g},
and [h] = {h}.

It can be shown that for any s-graph SG =

1483



(G,φ), a sub-s-graph SH with source nodes U
is extensible iff its edge set is the union of a set
of s-components of G with respect to U . We let
an s-component representation C = (C, φ) in the
s-graph SG = (G,φ′) consist of a source assign-
ment φ : S  V and a set C of s-components of
G with respect to the set VSC = φ(S) ⊆ V of
source nodes of φ. Then we can represent every
extensible sub-s-graph SH = (H,φ) of SG by
the s-component representation C = (C, φ) where
C is the set of s-components of which SH con-
sists. Conversely, we write T (C) for the unique
extensible sub-s-graph of SG represented by the
s-component representation C.

The utility of s-component representations de-
rives from the fact that merge can be evaluated on
these representations alone, as follows.

Lemma 1. Let C = (C, φ), C1 = (C1, φ1), C2 =
(C2, φ2) be s-component representations in the s-
graph SG. Then T (C) = T (C1) || T (C2) iff C =
C1]C2 (i.e., disjoint union) and φ1∪φ2 is defined,
injective, and equal to φ.

3.3 Boundary representations

If there is no C such that all conditions of Lemma 1
are satisfied, then T (C1) || T (C2) is not defined.
In order to check this efficiently in the bottom-up
parser, it will be useful to represent s-components
explicitly via their boundary.

Consider an s-component representation C =
(C, φ) in SG and let E be the set of all edges that
are adjacent to a source node in VSC and contained
in an s-component in C. Then we let the bound-
ary representation (BR) β of C in the s-graph SG
be the pair β = (E, φ). That is, β represents the
s-components through the in-boundary edges, i.e.
those edges inside the s-components (and thus the
sub-s-graph) which are adjacent to a source. The
BR β specifies C uniquely if the base graph SG
is connected, so we write T (β) for T (C) and VSβ
for VSC .

In Fig. 2(a), the bold sub-s-graph is represented
by β = 〈{d, e, f, g}, {A:4,B:5}〉, indicating that
it contains the A-source 4 and the B-source 5; and
further, that the edge set of the sub-s-graph is [d]∪
[e] ∪ [f ] ∪ [g] = {a, b, c, d, e, f, g}. The edge h
(which is also incident to 5) is not specified, and
therefore not in the sub-s-graph.

The following lemma can be shown about com-
puting merge on boundary representations. Intu-
itively, the conditions (b) and (c) guarantee that

the component sets are disjoint; the lemma then
follows from Lemma 1.

Lemma 2. Let SG be an s-graph, and let β1 =
(E1, φ1), β2 = (E2, φ2) be two boundary repre-
sentations in SG. Then T (β1) || T (β2) is defined
within SG iff the following conditions hold:

(a) φ1 ∪ φ2 is defined and injective;
(b) the two BRs have no in-boundary edges in

common, i.e. E1 ∩ E2 = ∅;
(c) for every source node v of β1, the last edge

on the path in SG from v to the closest source
node of β2 is not an in-boundary edge of β2,
and vice versa.

Furthermore, if these conditions hold, we have
T (β1 || β2) = T (β1) || T (β2), where we define
β1 || β2 = (E1 ∪ E2, φ1 ∪ φ2).
4 S-graph grammars

We are now ready to define s-graph grammars,
which describe languages of s-graphs. We also
introduce graph parsing and relate s-graph gram-
mars to HRGs.

4.1 Grammars for languages of s-graphs
We use interpreted regular tree grammars (IRTGs;
Koller and Kuhlmann (2011)) to describe lan-
guages of s-graphs. IRTGs are a very general
mechanism for describing languages over and re-
lations between arbitrary algebras. They sepa-
rate conceptually the generation of a grammatical
derivation from its interpretation as a string, tree,
graph, or some other object.

Consider, as an example, the tiny grammar in
Fig. 3; see Koller (2015) for linguistically mean-
ingful grammars. The left column consists of a
regular tree grammar G (RTG; see e.g. Comon et
al. (2008)) with two rules. This RTG describes a
regular language L(G) of derivation trees (in gen-
eral, it may be infinite). In the example, we can
derive S ⇒ r1(VP) ⇒ r1(r2), therefore we have
t = r1(r2) ∈ L(G).

We then use a tree homomorphism h to rewrite
the derivation trees into terms over an algebra; in
this case the HR algebra. In the example, the val-
ues h(r1) and h(r2) are specified in the second col-
umn of Fig. 3. We compute h(t) by substituting
the variable x1 in h(r1) with h(r2). The term h(t)
is thus the one shown in (1). It evaluates to the
s-graph SGf in Fig. 1(f).

1484



Rule of RTG G homomorphism h
S→ r1(VP) fS(fO(SGwant || x1[R→ O]))
VP→ r2 SGsleep

Figure 3: An example s-graph grammar.

In general, the IRTG G = (G, h,A) generates
the language L(G) = {Jh(t)K | t ∈ L(G)}, whereJ·K is evaluation in the algebra A. Thus, in the
example, we have L(G) = {SGf}.

In this paper, we focus on IRTGs that describe
languages L(G) ⊆ A of objects in an algebra;
specifically, of s-graphs in the HR algebra. How-
ever, IRTGs extend naturally to a synchronous
grammar formalism by adding more homomor-
phisms and algebras. For instance, the grammars
in Koller (2015) map each derivation tree simulta-
neously to a string and an s-graph, and therefore
describe a binary relation between strings and s-
graphs. We call IRTGs where at least one algebra
is the HR algebra, s-graph grammars.

4.2 Parsing with s-graph grammars

In this paper, we are concerned with the pars-
ing problem of s-graph grammars. In the context
of IRTGs, parsing means that we are looking for
those derivation trees t that are (a) grammatically
correct, i.e. t ∈ L(G), and (b) match some given
input object a, i.e. h(t) evaluates to a in the al-
gebra. Because the set P of such derivation trees
may be large or infinite, we aim to compute an
RTG Ga such that L(Ga) = P . This RTG plays
the role of a parse chart, which represents the pos-
sible derivation trees compactly.

In order to compute Ga, we need to solve two
problems. First, we need to determine all the pos-
sible ways in which a can be represented by terms
τ over the algebra A. This is familiar from string
parsing, where a CKY parse chart spells out all
the ways in which larger substrings can be decom-
posed into smaller parts by concatenation. Sec-
ond, we need to identify all those derivation trees
t ∈ L(G) that map to such a decomposition τ ,
i.e. for which h(t) evaluates to a. In string pars-
ing, this corresponds to retaining only such de-
compositions into substrings that are justified by
the grammar rules.

While any parsing algorithm must address both
of these issues, they are usually conflated, in that
parse items combine information about the de-
composition of a (such as a string span) with infor-
mation about grammaticality (such as nonterminal

symbols). In IRTG parsing, we take a different,
more generic approach. We assume that the set
D of all decompositions τ , i.e. of all terms τ that
evaluate to a in the algebra, can be represented
as the language D = L(Da) of a decomposition
grammar Da. Da is an RTG over the signature of
the algebra. Crucially, Da only depends on the al-
gebra and a itself, and not on G or h, because D
contains all terms that evaluate to a and not just
those that are licensed by the grammar. However,
we can compute Ga fromDa efficiently by exploit-
ing the closure of regular tree languages under in-
tersection and inverse homomorphism; see Koller
and Kuhlmann (2011) for details.

In practice, this means that whenever we want
to apply IRTGs to a new algebra (as, in this pa-
per, to the HR algebra), we can obtain a parsing
algorithm by specifying how to compute decom-
position grammars over this algebra. This is the
topic of Section 5.

4.3 Relationship to HRG

We close our exposition of s-graph grammars by
relating them to HRGs. It is known that the graph
languages that can be described with s-graph
grammars are the same as the HRG languages
(Courcelle and Engelfriet, 2012, Prop. 4.27). Here
we establish a more precise equivalence result, so
we can compare our asymptotic runtimes directly
to those of HRG parsers.

An HRG rule, such as the one shown in Fig. 4,
rewrites a nonterminal symbol into a graph. The
example rule constructs a graph for the nontermi-
nal S by combining the graph Gr in the middle
(with nodes 1, 2, 3 and edges e, f ) with graphsGX
and GY that are recursively derived from the non-
terminals X and Y . The combination happens by
merging the external nodes of GX and GY with
nodes of Gr: the squiggly lines indicate that the
external node I of GX should be 1, and the ex-
ternal node II should be 2. Similarly the external
nodes of GY are unified with 1 and 3. Finally, the
external nodes I and II of the HRG rule for S itself,
shaded gray, are 1 and 3.

The fundamental idea of the HRG-to-IRTG
translation is to encode external nodes as sources,
and to use rename and merge to unify the nodes of
the different graphs. In the example, we might say
that the external nodes of GX and GY are repre-
sented using the source names I and II, and extend
Gr to an s-graph by saying that the nodes 1, 2, and

1485



3 are its I-source, III-source, and II-source respec-
tively. This results in the expression

(2) fIII(〈I〉 e→ 〈III〉 || x1[II→ III]
|| 〈I〉 f→ 〈II〉 || x2)

where we write “〈I〉 e→ 〈III〉” for the s-graph con-
sisting of the edge e, with node 1 as I-source and
2 as III-source.

However, this requires the use of three source
names (I, II, and III). The following encoding of
the rule uses the sources more economically:

(3) fII(〈I〉 e→ 〈II〉 || x1) || 〈I〉 f→ 〈II〉 || x2
This term uses only two source names. It forgets

II as soon as we are finished with the node 2, and
frees the name up for reuse for 3. The complete
encoding of the HRG rule consists of the RTG rule
S→ r(X,Y) with h(r) = (3).

In the general case, one can “read off” possible
term encodings of a HRG rule from its tree decom-
positions; see Chiang et al. (2013) or Def. 2.80 of
Courcelle and Engelfriet (2012) for details. A tree
decomposition is a tree, each of whose nodes π is
labeled with a subset Vπ of the nodes in the HRG
rule. We can construct a term encoding from a tree
decomposition bottom-up. Leaves map to vari-
ables or constants; binary nodes introduce merge
operations; and we use rename and forget oper-
ations to ensure that the subterm for the node π
evaluates to an s-graph in which exactly the nodes
in Vπ are source nodes.2 In the example, we obtain
(3) from the tree decomposition in Fig. 4 like this.

The tree-width k of an HRG rule is measured
by finding the tree decomposition of the rule for
which the node sets have the lowest maximum size
s and setting k = s− 1. It is a crucial measure be-
cause Chiang et al.’s parsing algorithm is exponen-
tial in k. The translation we just sketched uses s
source names. Thus we see that a HRG with rules
of tree-width ≤ k can be encoded into an s-graph
grammar with k + 1 source names. (The converse
is also true.)

5 Graph parsing with s-graph grammars

Now we show how to compute decomposition
grammars for the s-graph algebra. As we ex-
plained in Section 4.2, we can then obtain a com-
plete parser for s-graph grammars through generic
methods.

2This uses the swap operations mentioned in Footnote 1.

I

1

II

3

A

2

X Y
S→

Gr

fe

I

II

I

II e : 1,2 f : 1,3X : 1,2

1,2

Y : 1,3

1,3

1,3

Figure 4: An HRG rule (left) with one of its tree
decompositions (right).

Given an s-graph SG, the language of the de-
composition grammar DSG is the set of all terms
over the HR algebra that evaluate to SG. For ex-
ample, the decomposition grammar for the graph
SG in Fig. 1(a) contains – among many others –
the following two rules:

(4) SG→ fR(SGf )
(5) SGe → || (SGb, SGd),

where SGf , SGe, SGb, and SGd are the graphs
from Fig. 1 (see Section 3.1). In other words,DSG
keeps track of sub-s-graphs in the nonterminals,
and the rules spell out how “larger” sub-s-graphs
can be constructed from “smaller” sub-s-graphs
using the operations of the HR algebra. The al-
gorithms below represent sub-s-graphs compactly
using s-component and boundary representations.

Because the decomposition grammars in the s-
graph algebra can be very large (see Section 6),
we will not usually compute the entire decompo-
sition grammar explicitly. Instead, it is sufficient
to maintain a lazy representation of DSG, which
allows us to answer queries to the decomposition
grammar efficiently. During parsing, such queries
will be generated by the generic part of the pars-
ing algorithm. Specifically, we will show how to
answer the following types of query:

• Top-down: given an s-component represen-
tation C of some s-graph and an algebra
operation o, enumerate all the rules C →
o(C1, . . . , Ck) inDSG. This asks how a larger
sub-s-graph can be derived from other sub-s-
graphs using the operation o. In the example
above, a query for SG and fR(·) should yield,
among others, the rule in (4).

• Bottom-up: given boundary representations
β1, . . . , βk and an algebra operation o, enu-
merate all the rules β → o(β1, . . . , βk) in
DSG. This asks how smaller sub-s-graphs
can be combined into a bigger one using the

1486



forget rename merge
bottom-up O(d + s) O(s) O(ds)
top-down O(ds) O(s) O(ds)

I = # rules O(ns2ds) O(ns2ds) O(ns3ds)

Table 1: Amortized per-rule runtimes T for the
different rule types.

operation o. In the example above, a merge
query for SGb and SGd should yield the rule
in (5). Unlike in the top-down case, every
bottom-up query returns at most one rule.

The runtime of the complete parsing algorithm
is bounded by the number I of different queries
to DSG that we receive, multiplied by the per-
rule runtime T that we need to answer each query.
The factor I is analogous to the number of rule
instances in schema-based parsing (Shieber et al.,
1995). The factor T is often ignored in the anal-
ysis of parsing algorithms, because in parsing
schemata for strings, we typically have T = O(1).
This need not be the case for graph parsers. In the
HRG parsing schema of Chiang et al. (2013), we
have I = O(nk+13d(k+1)), where k is the tree-
width of the HRG. In addition, each of their rule
instances takes time T = O(d(k + 1)) to actually
calculate the new item.

Below, we show how we can efficiently answer
both bottom-up and top-down queries toDSG. Ev-
ery s-graph grammar has an equivalent normal
form where every constant describes an s-graph
with a single edge. Assuming that the grammar
is in this normal form, queries of the form β → g
(resp. C → g), where g is a constant of the HR-
algebra, are trivial and we will not consider them
further. Table 1 summarizes our results.

5.1 Bottom-up decomposition

Forget and rename. Given a boundary repre-
sentation β′ = (E′, φ′), answering the bottom-up
forget query β → fA(β′) amounts to verifying that
all edges incident to φ′(A) are in-boundary in β′,
since otherwise the result would not be extensible.
This takes time O(d). We then let β = (E, φ),
where φ is like φ′ but undefined on A, and E is the
set of edges in E′ that are still incident to a source
in φ. Computing β thus takes time O(d+ s).

The rename operation works similarly, but since
the edge set remains unmodified, the per-rule run-
time is O(s).

A BR is fully determined by specifying the node
and in-boundary edges for each source name, so

there are at most O
((
n2d

)s) different BRs. Since
the result of a forget or rename rule is determined
by the child β′, this is an upper bound for the num-
ber I of rule instances of forget or rename.

Merge. Now consider the bottom-up merge
query for the boundary representations β1 and β2.
As we saw in Section 3.3, T (β1) || T (β2) is not
always defined. But if it is, we can answer the
query with the rule (β1 || β2)→ || (β1, β2), with
β1 || β2 defined as in Section 3.3. Computing this
BR takes time O(ds).

We can check whether T (β1) || T (β2) is de-
fined by going through the conditions of Lemma 2.
The only nontrivial condition is (c). In order to
check it efficiently, we precompute a data struc-
ture which contains, for any two nodes u, v ∈ V ,
the length k of the shortest undirected path u =
v1 ↔ . . . e↔ vk = v and the last edge e on this
path. This can be done in time O(n3) using the
Floyd-Warshall algorithm. Checking (c) for every
source pair then takes time O(s2) per rule, but be-
cause sources that are common to both β1 and β2
automatically satisfy (c) due to (a), one can show
that the total runtime of checking (c) for all merge
rules of DSG is O(ns3dss).

Observe finally that there are I = O(ns3ds)
instances of the merge rule, because each of the
O(ds) edges that are incident to a source node can
be either in β1, in β2, or in neither. Therefore
the runtime for checking (c) amortizes to O(s) per
rule. The Floyd-Warshall step amortizes to O(1)
per rule for s ≥ 3; for s ≤ 2 the node table can
be computed in amortized O(1) using more spe-
cialized algorithms. This yields a total amortized
per-rule runtime T for bottom-up merge ofO(ds).

5.2 Top-down decomposition

For the top-down queries, we specify sub-s-graphs
in terms of their s-component representations. The
number I of instances of each rule type is the same
as in the bottom-up case because of the one-to-
one correspondence of s-component and bound-
ary representations. We focus on merge and forget
queries; rename is as above.

Merge. Given an s-component representation
C = (C, φ), a top-down merge query asks us to
enumerate the rules C → || (C1, C2) such that
T (C1) || T (C2) = T (C). By Lemma 1, we
can do this by using every distribution of the s-
components in C over C1 and C2 and restricting φ

1487



accordingly. This brings the per-rule time of top-
down merge to O(ds), the maximum number of
s-components in C.

Block-cutpoint graphs. The challenging query
to answer top-down is forget. We will first de-
scribe the problem and introduce a data structure
that supports efficient top-down forget queries.

Consider top-down forget queries on
the sub-s-graph SG1 drawn in bold in
Fig. 2(a); its s-component representation is
〈{[a], [g]}, {A:4,B:5}〉. A top-down forget might
promote the node 3 to a C-source, yielding a
sub-s-graph SG2 (that is, fC(SG2) is the orig-
inal s-graph SG1). In SG2, a, e, and f are
no longer equivalent; its s-component repre-
sentation is 〈{[a], [e], [f ], [g]}, {A:4,B:5,C:3}〉.
Thus promoting 3 to a source splits the original
s-component into smaller parts.

By contrast, the same top-down forget might
also promote the node 1 to a C-source, yield-
ing a sub-s-graph SG3; fC(SG3) is also SG1.
However, all edges in [a] are still equiva-
lent in SG3; its s-component representation is
〈{[a], [g]}, {A:4,B:5,C:1}〉.

An algorithm for top-down forget must be able
to determine whether promotion of a node splits
an s-component or not. To do this, let G be the in-
put graph. We create an undirected auxiliary graph
GU from G and a set U of (source) nodes. GU

contains all nodes in V \U , and for each edge e
that is incident to a node u ∈ U , it contains a node
(u, e). Furthermore, GU contains undirected ver-
sions of all edges inG; if an edge e ∈ E is incident
to a node u ∈ U , it becomes incident to (u, e) in
GU instead. The auxiliary graph G{4,5} for our
example graph is shown in Fig. 2(d).

Two edges are connected in GU if and only if
they are equivalent with respect to U in G. There-
fore, promotion of u splits s-components iff u is a
cutpoint in GU , i.e. a node whose removal discon-
nects the graph. Cutpoints can be characterized
as those nodes that belong to multiple biconnected
components (BCCs) of GU , i.e. the maximal sub-
graphs such that any node can be removed without
disconnecting a graph segment. In Fig. 2(d), the
BCCs are indicated by the dotted boxes. Observe
that 3 is a cutpoint and 1 is not.

For any given U , we can represent the structure
of the BCCs of GU in its block-cutpoint graph.
This is a bipartite graph whose nodes are the cut-
points and BCCs of GU , and a BCC is connected

to all of its cutpoints; see Fig. 2(e) for the block-
cutpoint graph of the example. Block-cutpoint
graphs are always forests, with the individual trees
representing the s-components of G. Promoting
a cutpoint u splits the s-component into smaller
parts, each corresponding to an incident edge of u.
We annotate each edge with that part.

Forget. We can now answer a top-down forget
query C → fA(C′) efficiently from the block-
cutpoint graph for the sources of C = (C, φ). We
iterate over all components c ∈ C, and then over
all internal nodes u of c. If u is not a cutpoint,
we simply let C′ = (C ′, φ′) by making u an A-
source and letting C ′ = C. Otherwise, we also
remove c from C and add the new s-components
on the edges adjacent to u in the block-cutpoint
graph. The query returns rules for all C′ that can
be constructed like this.

The per-rule runtime of top-down forget is
O(ds), the time needed to compute C ′ in the cut-
point case. We furthermore precompute the block-
cutpoint graphs for the input graph with respect to
all sets U ⊆ V of nodes with |U | ≤ s − 1. For
each U , we can compute the block-cutpoint graph
and annotate its edges in time O(nd2s). Thus the
total time for the precomputation is O(ns · d2s),
which amortizes to O(1) per rule.

6 Evaluation

We evaluate the performance of our algorithms on
the “Little Prince” AMR-Bank version 1.4, avail-
able from amr.isi.edu. This graph-bank con-
sists of 1562 sentences manually annotated with
AMRs. We implemented our algorithms in Java
as part of the Alto parser for IRTGs (Alto Devel-
opers, 2015), and compared them to the Bolinas
HRG parser (Andreas et al., 2013). We measured
runtimes using Java 8 (for Alto) and Pypy 2.5.0
(for Bolinas) on an Intel Xeon E7-8857 CPU at 3
GHz, after warming up the JIT compilers.

As there are no freely available grammars for
this dataset, we created our own for the evalua-
tion, using Bayesian grammar induction roughly
along the lines of Cohn et al. (2010). We pro-
vide the grammars as supplementary material.
Around 64% of the AMRs in the graph-bank have
treewidth 1 and can thus be parsed using s = 2
source names. 98% have treewidth 1 or 2, corre-
sponding to s = 3 source names. All experiments
evaluated parser times on the same AMRs from
which the grammar was sampled.

1488



Top-down versus bottom-up. Fig. 5 compares
the performance of the top-down and the bottom-
up algorithm, on a grammar with three source
names sampled from all 1261 graphs with up to
10 nodes. Each point in the figure is the geometric
mean of runtimes for all graphs with a given num-
ber of nodes; note the log-scale. We aborted the
top-down parser after its runtimes grew too large.

We observe that the bottom-up algorithm out-
performs the top-down algorithm, and yields prac-
tical runtimes even for nontrivial graphs. One pos-
sible explanation for the difference is that the top-
down algorithm spends more time analyzing un-
grammatical s-graphs, particularly subgraphs that
are not connected.

Comparison to Bolinas. We also compare our
implementations to Bolinas. Because Bolinas is
much slower than Alto, we restrict ourselves to
two source names (= treewidth 1) and sampled the
grammar from 30 randomly chosen AMRs each of
size 2 to 8, plus the 21 AMRs of size one.

Fig. 6 shows the runtimes. Our parsers are
generally much faster than in Fig. 5, due to the
decreased number of sources and grammar size.
They are also both much faster than Bolinas. Mea-
suring the total time for parsing all 231 AMRs,
our bottom-up algorithm outperforms Bolinas by a
factor of 6722. The top-down algorithm is slower,
but still outperforms Bolinas by a factor of 340.

Further analysis. In practice, memory use can
be a serious issue. For example, the decomposi-
tion grammar for s=3 for AMR #194 in the corpus
has over 300 million rules. However, many uses
of decomposition grammars, such as sampling for
grammar induction, can be phrased purely in terms
of top-down queries. The top-down algorithm can
answer these without computing the entire gram-
mar, alleviating the memory problem.

Finally, we analyzed the asymptotic runtimes in
Table 1 in terms of the maximum number d · s of
in-boundary edges. However, the top-down parser
does not manipulate individual edges, but entire
s-components. The maximum number Ds of s-
components into which a set of s sources can split
a graph is called the s-separability of G by Laute-
mann (1990). We can analyze the runtime of the
top-down parser more carefully as O(ns3Dsds);
as the dotted line in Fig. 5 shows, this predicts
the runtime well. Interestingly, Ds is much lower
in practice than its theoretical maximum. In the

●

●

●

●

●

●

●

●
● ●

●

O(n3 ⋅ 3D3 ⋅ 3 ⋅ d)
Bottom−up
Top−down

1 2 3 4 5 6 7 8 9 10

1
10

10
0

10
00

0
1e

+0
6

Node count

[m
s]

Figure 5: Runtimes of our parsers with s = 3.

●

●

●

● ●
●

●
●

● Bottom−up
Top−down
Bolinas

1 2 3 4 5 6 7 8

1
10

10
0

10
00

1e
+0

5

Node count
[m

s]

Figure 6: Runtimes of our parsers and Bolinas
with s = 2.

“Little Prince” AMR-Bank, the mean ofD3 is 6.0,
whereas the mean of 3 · d is 12.7. Thus exploit-
ing the s-component structure of the graph can im-
prove parsing times.

7 Conclusion

We presented two new graph parsing algorithms
for s-graph grammars. These were framed in
terms of top-down and bottom-up queries to a de-
composition grammar for the HR algebra. Our
implementations outperform Bolinas, the previ-
ously best system, by several orders of magnitude.
We have made them available as part of the Alto
parser.

A challenge for grammar-based semantic pars-
ing is grammar induction from data. We will ex-
plore this problem in future work. Furthermore,
we will investigate methods for speeding up graph
parsing further, e.g. with different heuristics.

Acknowledgments. We thank the anonymous
reviewers for their comments, and Daniel Bauer
for his help with Bolinas. We received valuable
feedback at the 2015 Dagstuhl seminar on graph
grammars and the 2014 Johns Hopkins workshop
in Prague. This work was supported by the DFG
grant KO 2916/2-1.

1489



References
Alto Developers. 2015. Alto: algebraic language

toolkit for parsing and decoding with IRTGs. Avail-
able at https://bitbucket.org/tclup/
alto.

Jacob Andreas, Daniel Bauer, Karl Moritz Hermann,
Bevan Jones, Kevin Knight, and David Chiang.
2013. Bolinas graph processing package. Available
at http://www.isi.edu/publications/
licensed-sw/bolinas/. Downloaded in Jan-
uary 2015.

Laura Banarescu, Claire Bonial, Shu Cai, Madalina
Georgescu, Kira Griffitt, Ulf Hermjakob, Kevin
Knight, Philipp Koehn, Martha Palmer, and Nathan
Schneider. 2013. Abstract meaning representation
for sembanking. In Proceedings of the 7th Lin-
guistic Annotation Workshop & Interoperability with
Discourse, pages 178–186.

David Chiang, Jacob Andreas, Daniel Bauer,
Karl Moritz Hermann, Bevan Jones, and Kevin
Knight. 2013. Parsing graphs with hyperedge
replacement grammars. In Proceedings of the 51st
Annual Meeting of the Association for Computa-
tional Linguistics, pages 924–932.

Trevor Cohn, Phil Blunsom, and Sharon Goldwater.
2010. Inducing tree-substitution grammars. Journal
of Machine Learning Research (JMLR), 11:3053–
3096.

Hubert Comon, Max Dauchet, Rémi Gilleron, Flo-
rent Jacquemard, Denis Lugiez, Christof Löding,
Sophie Tison, and Marc Tommasi. 2008. Tree
automata techniques and applications. http://
tata.gforge.inria.fr/.

Bruno Courcelle and Joost Engelfriet. 2012. Graph
Structure and Monadic Second-Order Logic, vol-
ume 138 of Encyclopedia of Mathematics and its
Applications. Cambridge University Press.

Frank Drewes, Hans-Jörg Kreowski, and Annegret Ha-
bel. 1997. Hyperedge replacement graph gram-
mars. In Grzegorz Rozenberg, editor, Handbook of
Graph Grammars and Computing by Graph Trans-
formation, pages 95–162. World Scientific Publish-
ing Co., Inc.

Jeffrey Flanigan, Sam Thomson, Jamie Carbonell,
Chris Dyer, and Noah A. Smith. 2014. A discrim-
inative graph-based parser for the abstract meaning
representation. In Proceedings of the 52nd Annual
Meeting of the Association for Computational Lin-
guistics, pages 1426–1436, Baltimore, Maryland.

Bevan K. Jones, Jacob Andreas, Daniel Bauer,
Karl Moritz Hermann, and Kevin Knight. 2012. Se-
mantics – Based machine translation with hyperedge
replacement grammars. In Proceedings of COLING
2012: Technical Papers, pages 1359–1376.

Alexander Koller and Marco Kuhlmann. 2011. A gen-
eralized view on parsing and translation. In Pro-
ceedings of the 12th International Conference on
Parsing Technologies, pages 2–13.

Alexander Koller. 2015. Semantics construction with
graph grammars. In Proceedings of the 11th Inter-
national Conference on Computational Semantics
(IWCS), pages 228–238.

Clemens Lautemann. 1988. Decomposition trees:
Structured graph representation and efficient algo-
rithms. In Max Dauchet and Maurice Nivat, editors,
13th Colloquium on Trees in Algebra and Program-
ming, volume 299 of Lecture Notes in Computer Sci-
ence, pages 28–39. Springer Berlin Heidelberg.

Clemens Lautemann. 1990. The complexity of
graph languages generated by hyperedge replace-
ment. Acta Informatica, 27:399–421.

André F. T. Martins and Mariana S. C. Almeida. 2014.
Priberam: A turbo semantic parser with second or-
der features. In Proceedings of the 8th International
Workshop on Semantic Evaluation (SemEval 2014),
pages 471–476.

Stephan Oepen, Marco Kuhlmann, Yusuke Miyao,
Daniel Zeman, Dan Flickinger, Jan Hajic, Angelina
Ivanova, and Yi Zhang. 2014. SemEval 2014 Task
8: Broad-coverage semantic dependency parsing. In
Proceedings of the 8th International Workshop on
Semantic Evaluation (SemEval 2014), pages 63–72.

Daniel Quernheim and Kevin Knight. 2012. DAG-
GER: A toolkit for automata on directed acyclic
graphs. In Proceedings of the 10th International
Workshop on Finite State Methods and Natural Lan-
guage Processing, pages 40–44.

Stuart M. Shieber, Yves Schabes, and Fernando C. N.
Pereira. 1995. Principles and implementation of
deductive parsing. Journal of Logic Programming,
24(1–2):3–36.

1490


