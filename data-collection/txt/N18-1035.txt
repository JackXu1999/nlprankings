



















































Microblog Conversation Recommendation via Joint Modeling of Topics and Discourse


Proceedings of NAACL-HLT 2018, pages 375–385
New Orleans, Louisiana, June 1 - 6, 2018. c©2018 Association for Computational Linguistics

Microblog Conversation Recommendation
via Joint Modeling of Topics and Discourse

Xingshan Zeng1,3, Jing Li2, Lu Wang4,
Nicholas Beauchamp5,6, Sarah Shugars6, Kam-Fai Wong1,3

1The Chinese University of Hong Kong, 2Tencent AI Lab
3MoE Key Laboratory of High Confidence Software Technologies, China

4College of Computer and Information Science, Northeastern University, US
5Department of Political Science, Northeastern University, US

6Network Science Institute, Northeastern University, US
1,3{xszeng,kfwong}@se.cuhk.edu.hk

2ameliajli@tencent.com, 4luwang@ccs.neu.edu
5n.beauchamp@northeastern.edu, 6shugars.s@husky.neu.edu

Abstract
Millions of conversations are generated ev-
ery day on social media platforms. With lim-
ited attention, it is challenging for users to se-
lect which discussions they would like to par-
ticipate in. Here we propose a new method
for microblog conversation recommendation.
While much prior work has focused on post-
level recommendation, we exploit both the
conversational context, and user content and
behavior preferences. We propose a statistical
model that jointly captures: (1) topics for rep-
resenting user interests and conversation con-
tent, and (2) discourse modes for describing
user replying behavior and conversation dy-
namics. Experimental results on two Twitter
datasets demonstrate that our system outper-
forms methods that only model content with-
out considering discourse.

1 Introduction

Online platforms have revolutionized the way in-
dividuals collect and share information (O’Connor
et al., 2010; Lee and Ma, 2012; Bakshy et al.,
2015), but the vast bulk of online content is ir-
relevant or unpalatable to any given individual.
A user interested in political discussion, for in-
stance, might prefer content concerning a specific
candidate or issue, and only then if discussed in
a positive light without controversy (Adamic and
Glance, 2005; Bakshy et al., 2015).

How do individuals facing such large quanti-
ties of superfluous material select which conver-
sations to engage in, and how might we better al-
gorithmically recommend conversations suited to
individual users? We approach this problem from
a microblog conversation recommendation frame-
work. Where prior work has focused on the con-
tent of individual posts for recommendation (Chen

Conversation 1
...
[U1]: The sheer cognitive dissonance required for a “lib-
eral” to say Clinton is as bad as Trump is just staggering.
[U2]: Hillarists, Troll; they insult Liberals trying to dis-
tract from Hillary’s Conseratism.
[U3]: I still prefer Hillarist b/c it describes their Cultish
and ideological aspects.
——————————
...
Conversation 2
...
[U4]: I do not like trump at all, but Comey left her in
place knowing Bernie is much stronger.
[U1]: If you’re going to actively start rooting against the
Democrats, get off my mentions. I have enough GOP
doing that.
[U5]: Your tweets are an example of why open primaries
are stupid. You’re not a Dem, you’re just for one guy.
——————————
[U1]: No offense, but you’ve been wrong about pretty
much everything so far. Why would I trust your prognos-
tication now?
...

Figure 1: Two snippets of conversations on Twitter.
[Ui]: The message is posted by user Ui. “—” is the
dividing line between training history and test part. U1
did not reengage in Conversation 1 but reengaged in
Conversation 2.

et al., 2012; Yan et al., 2012; Vosecky et al., 2014;
He and Tan, 2015), we examine the entire history
and context of a conversation, including both top-
ical content and discourse modes such as agree-
ment, question-asking, argument and other dia-
logue acts (Ritter et al., 2010).1 And where Back-
strom et al. (2013) leveraged conversation reply
structure (such as previous user engagement), their
model is unable to predict first entry into new con-
versations, while ours is able to predict both new

1In this paper, discourse mode refers to a certain type of
dialogue act, e.g., agreement or argument. The discourse
structure of a conversation means some combination (or a
probability distribution) of discourse modes.

375



and repeated entry into conversations based on a
combination of topical and discourse features.

To illustrate the interplay between topics and
discourse, Figure 1 displays two snippets of con-
versations on Twitter collected during the 2016
United States presidential election. User U1 par-
ticipates in both conversations. The first conver-
sation is centered around Clinton, and U1, who is
more typically involved with conversations about
candidate Sanders, does not return. In the second
conversation, however, U1 is involved in a heated
back-and-forth debate, and thus is drawn back to
a conversation that they may otherwise have aban-
doned but for their enjoyment of adversarial dis-
course.

Effective conversation prediction and recom-
mendation requires an understanding of both user
interests and discourse behaviors, such as agree-
ment, disagreement, inquiry, backchanneling, and
emotional reactions. However, acquiring manual
labels for both is a time-consuming process and
hard to scale for new datasets. We instead propose
a unified statistical learning framework for conver-
sation recommendation, which jointly learns (1)
hidden factors that reflect user interests based on
conversation history, and (2) topics and discourse
modes in ongoing conversations, as discovered by
a novel probabilistic latent variable model. Our
model is built on the success of collaborative fil-
tering (CF) in recommendation systems, where la-
tent dimensions of product ratings or movie re-
views are extracted to better capture user pref-
erences (Linden et al., 2003; Salakhutdinov and
Mnih, 2008; Wang and Blei, 2011; McAuley and
Leskovec, 2013). To the best of our knowledge,
we are the first to model both topics and discourse
modes as part of a CF framework and apply it to
microblog conversation recommendation.2

Experimental results on two Twitter conversa-
tion datasets show that our proposed model yields
significantly better performance than state-of-the-
art post-level recommendation systems. For ex-
ample, by leveraging both topical content and
discourse structure, our model achieves a mean
average precision (MAP) of 0.76 on conversa-
tions about the U.S. presidential election, com-
pared with 0.70 by McAuley and Leskovec (2013),
which only considers topics. We further con-

2To ensure the general applicability of our approach to
domains lacking such information, we do not utilize external
features such as network structure, but it may certainly be
added in future, more narrowly targeted applications.

ducted detailed analysis on the latent topics and
discourse modes and find that our model can dis-
cover reasonable topic and discourse representa-
tions, which play an important role in characteriz-
ing reply behaviors. Finally, we also provide a pi-
lot study on recommendation for first time replies,
which shows that our model outperforms compa-
rable recommendation systems.

The rest of this paper is structured as follows.
The related work is discussed in Section 2. We
then present our microblog conversation recom-
mendation model in Section 3. The experimental
setup and results are described in Sections 4 and 5.
Finally, we conclude in Section 6.

2 Related Work

Social media has attracted increasing attention in
digital communication research (Agichtein et al.,
2008; Kwak et al., 2010; Wu et al., 2011). The
problem studied here is closely related to work on
recommendation and response prediction in mi-
croblogs (Artzi et al., 2012; Hong et al., 2013),
where the goal is to predict whether a user will
share or reply to a given post. Existing methods
focus on measuring features that reflect personal-
ized user interests, including topics (Hong et al.,
2013) and network structures (Pan et al., 2013; He
and Tan, 2015). These features have been investi-
gated under a learning to rank framework (Duan
et al., 2010; Artzi et al., 2012), graph ranking
models (Yan et al., 2012; Feng and Wang, 2013;
Alawad et al., 2016), and neural network-based
representation learning methods (Yu et al., 2016).

Distinguishing from prior work that focuses on
post-level recommendation, we tackle the chal-
lenges of predicting user reply behaviors at the
conversation-level. In addition, our model not
only captures latent factors such as the topical in-
terests of users, but also leverages the automat-
ically learned discourse structure. Much of the
previous work on discourse structure and dialogue
acts has relied on labeled data (Jurafsky et al.,
1997; Stolcke et al., 2000), while unsupervised
approaches have not been applied to the problem
of conversation recommendation (Woszczyna and
Waibel, 1994; Crook et al., 2009; Ritter et al.,
2010; Joty et al., 2011).

Our work is also in line with conversation mod-
eling for social media discussions (Ritter et al.,
2010; Budak and Agrawal, 2013; Louis and Co-
hen, 2015; Cheng et al., 2017). Topic modeling

376



has been employed to identify conversation con-
tent on Twitter (Ritter et al., 2010). In this work,
we propose a probabilistic model to capture both
topics and discourse modes as latent variables. A
further line of work studies the reposting and reply
structure of conversations (Gómez et al., 2011; La-
niado et al., 2011; Backstrom et al., 2013; Budak
and Agrawal, 2013). But none of this work dis-
tinguishes the rich discourse functions of replies,
which is modeled and exploited in our work.

3 The Joint Model of Topic and
Discourse for Recommendation

Our proposed microblog conversation recommen-
dation framework is based on collaborative fil-
tering and a novel probabilistic graphical model.
Concretely, our objective function takes the form:

minL+ µ ·NLL(C |Θ) (1)

This function encodes two types of information.
First, L models user reply preference in a similar
fashion to collaborative filtering (CF) (Hu et al.,
2008; Pan et al., 2008). It captures topics of inter-
ests and discourse structures users are commonly
involved (e.g., argumentation), and takes the form
of mean square error (MSE) based on user reply
history. This part is detailed in Section 3.1.

The second term, NLL(C |Θ), denotes the
negative log-likelihood of a set of conversations C,
with Θ containing all parameters. A probabilistic
model is described in Section 3.2 that shows how
the topical content and discourse structures of con-
versations are captured by these latent variables.

The hyperparameter µ controls the trade-off be-
tween the two effects. `2 regularization is also
added for parameters to avoid model overfitting.

For the rest of this section, we first present the
construction of L andNLL(C |Θ) in Sections 3.1
and 3.2. We then discuss how these two compo-
nents can be mutually informed by each other in
Section 3.3. Finally, the generative process and
parameter learning are described in Section 3.4.

3.1 Reply Preference (L)
Our user reply preference modeling is built on
the success of collaborative filtering (CF) for
product ratings. However, classic CF problems,
such as product recommendation, generally rely
on explicit user feedback. Unlike user ratings
on products, our input lacks explicit feedback
from users about negative preferences and non-
response. Therefore, we follow one-class Collab-
orative Filtering (Hu et al., 2008; Pan et al., 2008),

which weights positive instances higher during
training and is thus suited to our data. Formally,
for user u and conversation c, we measure reply
preference based on the MSE between predicted
preference score pu,c and reply history ru,c. ru,c
equals 1 if u is in the conversation history; other-
wise, it is 0. The first term of objective (Eq. 1)
takes the following form:

L =
|U|∑

u=1

|C|∑

c=1

fu,c · (pu,c − ru,c)2 (2)

where U consists of users {u} and C is a set of
conversations {c} in a dataset. fu,c is the corre-
sponding weight for a conversation c and a target
user u. Intuitively, it has a large value if positive
feedback (user replied) is observed. Therefore, we
adapt the formulation from Pan et al. (2008):

fu,c =

{
s if ru,c = 1 (i.e., user replied)
1 if ru,c = 0

(3)

where s > 1, an integer hyperparameter to be
tuned.

Inspired by prior models (Koren et al., 2009;
McAuley and Leskovec, 2013), we propose the
following latent factor model to describe pu,c:

pu,c = λ · γUu · γCc + (1− λ) · δUu · δCc + bu + bc + a (4)

γUu and γ
C
c are K-dimensional latent vectors that

encode topic-specific information (where K is the
number of latent topics) for users and conversa-
tions. Specifically, γUu reflects the topical interests
of u, with higher value γUu,k indicating greater in-
terest by u in topic k. γCc captures the extents that
topics are discussed in conversation c.

Similarly, D-dimensional vectors δUu and δ
C
c

capture discourse structures in shaping reply be-
haviors (where D is the number of discourse clus-
ters). δUu reflects the discourse behaviors u prefers,
such as u1 often enjoys arguments as in the sec-
ond conversation of Figure 1, while δCc captures
the discourse modes used throughout conversation
c. By multiplying user and conversation factors,
we can measure the corresponding similarity. The
predicted score pu,c thereby reflects the tendency
for a user u to be involved in conversation c.

As pointed out by McAuley and Leskovec
(2013), these latent vectors often encode hidden
factors that are hard to interpret under a CF frame-
work. Therefore, in Section 3.2, we present a
novel probabilistic model which can extract in-
terpretable topics and discourse modes as word

377



distributions. We then describe how they can be
aligned with the latent vectors of γC and δU .

Parameter a is an offset parameter, bu and bc
are user and conversation biases, and λ ∈ [0, 1]
serves as the weight for trading offs of topic and
discourse factors in reply preference modeling.

3.2 Corpus Likelihood NLL(C |Θ)
Here we present a novel probabilistic model that
learns coherent word distributions for latent top-
ics and discourse modes of conversations. For-
mally, we assume that each conversation c ∈ C
contains Mc messages, and each message m has
Nc,m words. We distinguish three latent compo-
nents – discourse, topic, and background – un-
derlying conversations, each with their own type
of word distribution. At the corpus level, there
are K topics represented by word distribution φTk
(k = 1, 2, ...,K), while φDd (d = 1, 2, ..., D) rep-
resents the D discourse modes embedded in cor-
pus. In addition, we add a background word dis-
tribution φB to capture general information (e.g.,
common words), which do not indicate either dis-
course or topic information. φDd , φ

T
k , and φ

B are
all multinomial word distributions over vocabu-
lary size V . Below describes more details.

Message-level Modeling. Our model assigns
two types of message-level multinomial variables
to each message: zc,m reflects its latent topic and
dc,m represents its discourse mode.

Topic assignments. Due to the short nature of
microblog posts, we assume each message m in
conversation c contains only one topic, indexed as
zc,m. This strategy has been proven useful to alle-
viate data sparsity for topic inference (Quan et al.,
2015). We further assume messages in the same
conversation would focus on similar topics. We
thus draw topic zc,m ∼ θc, where θc denotes the
fractions of topics discussed in conversation c.

Discourse assignments. To capture discourse
behaviors of u, distribution πu is used to repre-
sent the discourse modes in messages posted by u.
The discourse mode dc,m for message m is then
generated from πuc,m , where uc,m is the author of
m in c.

Word-level Modeling. We aim to separate dis-
course, topic, and background information for
conversations. Therefore, for each word wc,m,n
of message m, a ternary switcher xc,m,n ∈
{DISC, TOPIC,BACK} controls word wc,m,n to

fall into one of the three types: discourse, topic,
and background.

Discourse words (DISC) are indicative of the
discourse modes of messages. When xc,m,n =
DISC (i.e., wc,m,n is assigned as a discourse
word), word wc,m,n is generated from the dis-
course word distribution φDdc,m where dc,m is dis-
course assignment to message m.

Topic words (TOPIC) describe the topical fo-
cus of a conversation. When xc,m,n = TOPIC,
wc,m,n is assigned as a topic word and generated
from φTzc,m – word distribution given topic of m.

Background words (BACK) capture the gen-
eral information that is not related to discourse or
topic. When word wc,m,n is assigned as a back-
ground word (xc,m,n = BACK), it is drawn from
background distribution φB .

Switching among Topic, Discourse, and Back-
ground. We further assume the word type switcher
xc,m,n is sampled from a multinomial distribu-
tion which depends on the current discourse mode
dc,m. The intuition is that messages of different
discourse modes may show different distributions
of the three word types. For instance, a state-
ment message may contain more content words
than a rhetorical question. Specifically, xc,m,n ∼
Multi(τdc,m), where τd is a 3-dimension stochas-
tic vector that expresses the appearing probabil-
ities of three kinds of words (DISC, TOPIC,
BACK), when the discourse assignment is d. Stop
words and punctuations are forced to be labeled
as discourse or background. By explicitly distin-
guishing different types of words with switcher
xc,m,n, we can thus separate word distributions
that reflect discourse, topic, and background infor-
mation.

Likelihood. Based on the message-level and the
word-level generation process, the probability of
observing words in the given corpus is:

Pr(C |θ,π,φ, τ , z,d,x)

=

C∏

c=1

Mc∏

m=1

θc,zc,mπuc,m,dc,m

×
∏

xc,m,n=BACK

τdc,m,BACKφ
B
wc,m,n

×
∏

xc,m,n=DISC

τdc,m,DISCφ
D
dc,m,wc,m,n

×
∏

xc,m,n=TOPIC

τdc,m,TOPICφ
T
zc,m,wc,m,n

(5)

And we use negative log likelihood to model cor-
pus likelihood effect in Eq. 1, i.e., NLL(C |Θ) =

378



− log(Pr(C |Θ), where parameters set Θ =
{θ,π,φ, τ , z,d,x}.

3.3 Mutually Informed User Preference and
Latent Variables

As mentioned above, the hidden factors discov-
ered in Section 3.1 lack interpretability, which can
be boosted by the learned latent topics and dis-
course modes in Section 3.2. However, it is non-
trivial to link the topic-related parameters of γCc to
the conversation topic distributions of θc, since the
former takes real values from −∞ to +∞ while
the latter is a stochastic vector. Therefore, we
follow the strategy from McAuley and Leskovec
(2013) to apply a softmax function over γCc :

θc,k =
exp(κT γCc,k)∑K

k′=1 exp(κ
T γCc,k′)

(6)

We further assume that the discourse mode pref-
erence by users, δUu , can also be informed by the
discourse mode distribution captured by πu, i.e., a
user who enjoys arguments may be willing to par-
ticipate another. So similarly, we define:

πu,d =
exp(κDδUu,d)∑D

d′=1 exp(κ
DδUu,d′)

(7)

where κT and κD are learnable parameters that
control the “peakiness” of the transformation. For
example, a larger κT indicates a more focused
conversation, while a smaller κT means users dis-
cuss diverse topics.

Finally, softmax transformation is also applied
to φTk , φ

D
d , φ

B , and τd, as done in McAuley and
Leskovec (2013), with additional parameters ψTk ,
ψDd , ψ

B , and χd (as shown in Figure 2). This is to
ensure that the distributions φ∗∗ and τd are stochas-
tic vectors. In doing so, these distributions can be
learned via optimizing ψ∗∗ and χd, which take any
value and thus ensure that the cost function in Eq.
1 is optimized without considering any parameter
constraints.

3.4 Generative Process and Model Learning
Our word generation process is displayed in Fig-
ure 2 and described as follows:

• Compute topic distribution θc by Eq. 6
• For message m = 1 to Mc:

– Compute discourse distribution πuc,m by Eq. 7
– Draw topic assignment zc,m ∼Multi(θc)
– Draw discourse mode dc,m ∼Multi(πuc,m)
– For word index n = 1 to Nc,m:
∗ Draw word type xc,m,n ∼Multi(τd)

𝑈
𝜋#

𝜃%

𝐶
𝑀%

𝑧%,*

𝑁%,*	

𝑤%,*,.

𝑑%,*

𝑢%,*

𝑥%,*,.

𝐷
𝜙45

𝐾

𝜙78

𝜙9

𝐷

𝜏4

𝛾%<

𝛿#>

𝜅8

𝜅5𝜒A

𝜓78

𝜓45

𝜓9

Figure 2: Generative process of our joint model of topic
and discourse. u represents users. c represents conver-
sations. Dotted arrows represent the softmax linkings,
while solid arrows mean conditional priors.

∗ if xc,m,n == BACK:
Draw word wc,m,n ∼Multi(φB)

∗ if xc,m,n == DISC:
Draw word wc,m,n ∼Multi(φDdc,m)

∗ if xc,m,n == TOPIC:
Draw word wc,m,n ∼Multi(φTzc,m)

Parameter Learning. For learning, we ran-
domly initialize all learnable parameters and then
alternate between the following two steps:
Step 1. Fix topic and discourse assignments z and
d, and word type switcher x, then optimize the
remaining parameters in Eq. 1 by L-BFGS (No-
cedal, 1980):

Update a, b, γ∗, δ∗, κ∗, ψ∗, χ =
argminL+ µ ·NLL(C |Θ) (8)

Step 2. Sample topic and discourse assignments z
and d at the message level and word type switcher
x at the word level, using the distributions, com-
puted according to parameters optimized in step 1:

Sample zc,m, dc,m, xc,m,n with probabilities
p(zc,m = k) = θc,k

p(dc,m = d) = πuc,m,d

p(xc,m,n = BACK) = φ
B
wc,m,nτdc,m,BACK

p(xc,m,n = DISC) = φ
D
dc,m,wc,m,nτdc,m,DISC

p(xc,m,n = TOPIC) = φ
T
zc,m,wc,m,nτdc,m,TOPIC

(9)

Step 2 is analogous to Gibbs Sampling (Grif-
fiths, 2002) in probabilistic graphical models, such
as LDA (Blei et al., 2003). However, distinguish-
ing from previous models, the multinomial distri-
butions in our models are not drawn from a Dirich-
let prior. Instead, they are computed based on the
parameters learned in Step 1.

Our learning process stops when the change of
parameters is small (i.e., below a pre-specified

379



Dataset # of # of # of Avg msg Avg convuser conv msg per user per user
US Election 4,300 2,013 22,092 5.14 1.23
TREC 10,122 7,500 38,999 3.85 1.71

Table 1: Statistics of two datasets.

0
1
2
3
4
5
6
7
8

1 4 7 10 13 16 19 22 25 28 >30

# 
of

 u
se

rs

# of  conversations a user participated

US Election

TREC

(unit: 1,000)

Figure 3: Horizontal axis: number of conversations that
a user is involved. Vertical axis: number of users fall
in the category (unit: 1,000). Notice that most of users
(about 98%) participate in less than 10 conversations.

threshold). Multiple restarts are tried, and similar
results are achieved.

4 Experimental Setup

Datasets. We collected two microblog conversa-
tion datasets from Twitter for experiments3: one
contains discussions about the U.S. presidential
election (henceforth US Election), the other gath-
ers conversations of diverse topics based on the
tweets released by TREC 2011 microblog track
(henceforth TREC)4. US Election was collected
from January to June of 2016 using Twitter’s
Streaming API5 with a small set of political key-
words.6 To recover conversations, Tweet Search
API7 was used to retrieve messages with the “in-
reply-to” relations to collect tweets in a recursive
way until full conversations were recovered.

Statistics of the datasets are shown in Table 1.
Figure 3 displays the number of conversations in-
dividual users participated in. As can be seen,
most users are involved in only a few conversa-
tions. Simply leveraging personal chat history will
not produce good performance for conversation

3The datasets are available at http://www.ccs.
neu.edu/home/luwang/

4 http://trec.nist.gov/data/tweets/
5https://developer.twitter.com/

en/docs/tweets/filter-realtime/
api-reference/post-statuses-filter.html

6Keyword list: “trump”, “hillary”, “clinton”, “president”,
“politics”, and “election.”

7https://developer.twitter.com/en/
docs/tweets/search/api-reference/
get-saved_searches-show-id

recommendation.
In our experiments, we predict whether a user

will engage in a conversation given the previous
messages in that conversation and past conversa-
tions the user is involved. For model training and
testing, we divide conversations into three ordered
segments, corresponding to training, development,
and test sets at 75%, 12.5%, and 12.5%.8

Preprocessing and Hyperparameter Tuning.
For preprocessing, links, mentions (i.e., @user-
name), and hashtags in tweets were replaced
with generic tags of “URL”, “MENTION”, and
“HASHTAG”. We then utilized the Twitter NLP
tool9 (Gimpel et al., 2011; Owoputi et al., 2013)
for tokenization and non-alphabetic token re-
moval. We removed stop words and punctuations
for all comparisons to ensure comparable perfor-
mance. We maintain a vocabulary with the 5,000
most frequent words.

Our model parameters are tuned on the devel-
opment set based on grid search, i.e. the param-
eters that give the lowest value for our objective
are selected. Specifically, the number of discourse
modes (D) and topics (K) are tuned to be 10. The
trade-off parameter µ between user preference and
corpus negative log-likelihood takes value of 0.1,
and λ, the parameter for balancing topic and dis-
course, is set to 0.5. Finally, the confidence param-
eter s takes a value of 200 to give higher weight for
positive instances, i.e., a user replied to a conver-
sation.

Evaluation Metrics. Following prior work on
social media post recommendation (Chen et al.,
2012; Yan et al., 2012), we treat our task on con-
versation recommendation as a ranking problem.
Therefore, popular information retrieval evalua-
tion metrics, including precision at K (P@K),
mean average precision (MAP) (Manning et al.,
2008), and normalized Discounted Cumulative
Gain at K (nDCG@K) (Järvelin and Kekäläinen,
2002) are reported. The metrics are computed per
user in the dataset and then averaged over all users.
The values range from 0.0 to 1.0, with higher val-
ues indicating better performance.

Baselines and Comparisons. For comparison,
we first consider three baselines: 1) ranking

8At least one turn per conversation is retained for training.
It is possible that one user only replies in either development
set or test set, but it is rather infrequent.

9http://www.cs.cmu.edu/˜ark/TweetNLP/

380



Models US Election TRECMAP P@1 nDCG@5 MAP P@1 nDCG@5
Baselines
RANDOM 0.018 0.004 0.009 0.006 0.001 0.002
LENGTH 0.025 0.002 0.003 0.013 0.002 0.004
POPULARITY 0.050 0.010 0.025 0.023 0.005 0.010
Comparisons
OCCF 0.637 0.589 0.649 0.410 0.385 0.425
RSVM 0.687 0.680 0.690 0.554 0.575 0.559
CTR 0.673 0.649 0.678 0.475 0.431 0.495
ADAPTED HFT 0.698 0.652 0.706 0.487 0.447 0.504
Our model 0.762 0.750 0.757 0.591 0.591 0.600

Table 2: Conversation recommendation results on US
Election and TREC. The best result for each column is
highlighted in bold. Our model performs significantly
better than all the comparisons (p < 0.01, paired t-
test).

conversations randomly (RANDOM); 2) longer
conversations (i.e., more words) ranked higher
(LENGTH); 3) conversations with more distinct
users ranked higher (POPULARITY).

We further compare results with three estab-
lished recommendation models:
• OCCF: one-class Collaborative Filtering (Pan
et al., 2008), which only considers users’ reply his-
tory without modeling content in conversations.
• RSVM: ranking SVM (Joachims, 2002), which
ranks conversations for each user with the content
and Twitter features as in Duan et al. (2010).
• CTR: messages in one conversation are aggre-
gated into one post and a state-of-the art Collabo-
rative Filtering-based post recommendation model
is applied (Chen et al., 2012).

Finally, we also adapt the “hidden factors as
topics” (HFT) model proposed in McAuley and
Leskovec (2013) (henceforth ADAPTED HFT).
Because the original model leverages the ratings
for all product reviews and does not handle im-
plicit user feedback well, we replace their user
preference objective function with ours (Eq. 2).

5 Experimental Results

In this section, we first discuss our main evaluation
in Section 5.1. A case study and corresponding
discussion are provided in Section 5.2 to provide
further insights, which is followed by an analysis
of the topics and discourse modes discovered by
our model (Section 5.3). We also examine our per-
formance on first time replies (Section 5.4).

5.1 Conversation Recommendation Results

Experimental results are displayed in Table 2,
where our model yields statistically significantly
better results than baselines and comparisons

0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8

25% 50% 75%

M
A

P

OCCF RSVM CTR adapted HFT Our Model

(a) US Election dataset

0
0.1
0.2
0.3
0.4
0.5
0.6
0.7

25% 50% 75%
M

A
P

OCCF RSVM CTR adapted HFT Our Model

(b) TREC dataset

Figure 4: MAP scores for models trained on 25%,
50%, and 75% of conversation history. For each quan-
tile, from left to right shows the result of OCCF,
RSVM, CTR, ADAPTED HFT, and our model. In gen-
eral, longer conversation history leads to better perfor-
mance, and our model outperforms compared systems
in all settings.

(paired t-tests, p < 0.01). For P@K, we only re-
port P@1, because a significant amount of users
participate only in 1 or 2 conversations. For
nDCG@K, different K values are experimented,
which results in similar trend, so only nDCG@5
is reported.

We find that the baselines that rank conversa-
tions with simple features (e.g., length or popu-
larity) perform poorly. This implies that generic
algorithms that do not consider conversation con-
tent or user preference cannot produce reasonable
recommendations.

Although some non-baseline systems capture
content in one way or another, only ADAPTED
HFT and our model exploit latent topic models to
better represent content in tweets, and outperform
other methods.

Compared to ADAPTED HFT, which only con-
siders latent topics under a collaborative filtering
framework, our model extracts both topics and dis-
course modes as latent variables, and shows supe-
rior performance on both datasets. Our discourse
variables go beyond topical content to capture so-
cial behaviors that affect user engagement, such as

381



0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9

1 2 3 >3

M
A

P
OCCF RSVM CTR adapted HFT Our Model

Figure 5: MAP scores of models for users involved
in varying number of conversations on TREC dataset.
Horizontal axis: degree of data sparsity indicated by
the number of conversations a user involved in training
data. Vertical axis: MAP scores. For each degree level,
from left to right shows the results of OCCF, RSVM,
CTR, ADPATED HFT, and our model.

arguments, question-asking, agreement, and other
discourse modes.

Training with Varying Conversation History.
To test the model performance based different lev-
els of user engagement history, we further experi-
ment with varying the length of conversations for
training. Specifically, in addition to using 75% of
conversation history, we also extract the first 25%
and 50% of history as training. The rest of a con-
versation is separated equally for development and
test. Figure 4 shows the MAP scores for US Elec-
tion and TREC datasets. The increasing MAP for
all methods as the training history increases in-
dicates that generally, conversation history is es-
sential for recommendation. Our model performs
consistently better over different lengths of con-
versation histories.

Results for Varying Degree of Data Sparsity.
From Table 1 and Figure 3, we observe that most
users in our datasets are involved in only a few
conversations. In order to study the effects of data
sparsity on recommendation models, we examine
in Figure 5 the MAP scores for users engaged in a
varying number of conversations, as measured on
the TREC dataset. The results on the US Election
dataset have similar distributions. As we see, the
prediction results become worse for users involved
in fewer conversations. This indicates that data
sparsity serves as a challenge for all recommenda-
tion models. We also observe that our model per-
forms consistently better than other models over
different degrees of sparsity. This implies that ef-
fectively capturing discourse structure in conver-
sation context is useful to mitigating the effects of

Models Conv 1 (c1) Conv 2 (c2)
OCCF 0.941 0.922
ADAPTED HFT 0.923 0.954
Our model 0.924 0.961

Table 3: Predicted recommendation scores by different
models of U1 for conversations c1 and c2 in Figure 1.
U1 later replies to c2 but not c1, where our model pre-
dicts scores of 0.961 for c2 (higher than 0.924 for c1).

Latent Dim. User U1 Conv 1 (c1) Conv 2 (c2)
Topic 1 (Sanders) 0.92 (γUu1,1) 0.10 (γ

C
c1,1) 0.63 (γ

C
c2,1)

Topic 2 (Clinton) 0.14 (γUu1,2) 0.84 (γ
C
c1,2) 0.12 (γ

C
c2,2)

Disc 1 (argument) 0.46 (δUu1,1) 0.28 (δ
C
c1,1) 0.38 (δ

C
c2,1)

Disc 2 (statement) -0.24 (δUu1,2) 0.98 (δ
C
c1,2) -0.09 (δ

C
c2,2)

Table 4: Sample latent dimensions of topics (γUu1 for
user, and γCc∗ for conversations) and discourse modes
(δUu1 for user, and δ

C
c∗ for conversations). User U1

shows interest in topic 1 (about Sanders), which is also
a dominating topic in conversation c2, but is not inter-
ested in topic 2 (about Clinton). U1 shows a preference
for discourse mode 1 (argument) over mode 2 (state-
ment).

data sparsity on conversation recommendation.

5.2 Case Study and Discussion
Here we present a case study based on the sample
conversations in Figure 1. Recall that user U1 is
interested in conversations about Sanders, and also
prefers more argumentative discourse, and thus re-
turns in conversation c2 but not c1.

Table 3 shows the predicted scores for the two
conversations from OCCF, ADAPTED HFT, and
our model (as in Eq. 2). Both ADAPTED HFT and
our model more accurately recommend c2 over c1,
with our model producing a slightly higher recom-
mendation score for c2.

Table 4 shows the latent dimension values for
the learned topics and discourse modes for this
user and these two conversations. Based on human
inspection, topic 1 appears to contain words about
Sanders, which is the main topic in conversation
c2. Topic 2 is about Clinton, which is a dominat-
ing topic in conversation c1. Our model also picks
up user interest in topic 1 (Sanders), and thus as-
signs γUu1,1 a high value. For discourse modes, our
model also generates a high score for “argument”
discourse (labeled via human inspection) for both
the user and c2.

5.3 Further Analysis of Topic and Discourse
Ablation Study. We have shown that joint mod-
eling of topical content and discourse modes pro-
duces the superior performance for our model.

382



Here we provide an ablation study to examine the
relative contributions of those two aspects by set-
ting the trade-off parameter λ to 1.0 (topic only) or
0.0 (discourse only). Table 5 shows that topics or
discourse individually improve slightly upon the
comparison ADAPTED HFT, but only jointly do
they improve significantly upon it.

Models US Election TREC
ADAPTED HFT 0.698 0.487
Our model (topic only) 0.711 0.491
Our model (discourse only) 0.705 0.483
Our model (full) 0.762 0.591

Table 5: MAP of different variants of our model. Best
results in each column is in bold.

Topic Coherence. To examine the quality of
topics found by our model, we use the CV topic
coherence score measured via the open-source
toolkit Palmetto10, which has been shown to pro-
duce evaluation performance comparable to hu-
man judgment (Röder et al., 2015). Our model
achieves topic coherence scores of 0.343 and
0.376 on TREC and US Election datasets, com-
pared to 0.338 and 0.371 for the topics from
ADAPTED HFT.

Discourse Top 10 TermsUS Election TREC

Question ? it if ... so all howbecause when any
? : HASHTAG or
too with MENTION
and ... what

Reaction you like any good !please no ˜ lol what
all EMOTICON &
!!! right ok u ;)
thank haha

Statement
’s do think the .
should they from
and have

i , a what all you be
how then ...

Argument
but that all fuck
without against out
though ! anything

do would up that too
even always never
anything much

Reference
be i about that
MENTION it “ you
-lrb- ?

MENTION ... ! :
what rt it you URL :)

Table 6: Top 10 representative terms for sample dis-
course modes discovered by our model in two datasets.
Names of discourse modes are our interpretations ac-
cording to the word distributions generated by our
model.

Sample Discourse Modes. While our topic
word distributions are relatively unsurprising, of
greater interest are the discourse mode word dis-
tributions. Table 6 shows a sample of discourse
modes as labeled by human. Although this is
merely a qualitative human judgment at this point,
there does appear to be a notable overlap in
discourse modes between the two datasets even
though they were learned separately.

10https://github.com/AKSW/Palmetto/

5.4 First Time Reply Results
From a recommendation perspective, users may
be interested in joining new conversations. We
thus compare each recommendation system for
first time replies. For each user, we only evaluate
for conversations where they are newcomers. Ta-
ble 7 shows that, unsurprisingly, all systems per-
form poorly on this task, though our model per-
forms slightly better. This suggests that other fea-
tures, e.g., network structures or other discussion
thread features, could usefully be included in fu-
ture studies that target new conversations.

Models US Election TREC
OCCF 0.035 0.033
RSVM 0.023 0.002
CTR 0.029 0.016
ADAPTED HFT 0.054 0.058
Our model 0.083 0.090

Table 7: MAP of models considering only first time
replies. Best results in each column is in bold.

6 Conclusion

This paper has presented a framework for mi-
croblog conversation recommendation via jointly
modeling topics and discourse modes. Experi-
mental results show that our method can outper-
form competitive approaches that omit user dis-
course behaviors. Qualitative analysis shows that
our joint model yields meaningful topics and dis-
course representations.

Acknowledgements

This work is partly supported by Innova-
tion and Technology Fund (ITF) Project No.
6904333, General Research Fund (GRF) Project
No. 14232816 (12183516), and National Science
Foundation Grant IIS-1566382. We thank Shum-
ing Shi, Yan Song, and the three anonymous re-
viewers for the insightful suggestions on various
aspects of this work.

References
Lada A. Adamic and Natalie Glance. 2005. The politi-

cal blogosphere and the 2004 U.S. election: Divided
they blog. In Proceedings of the 3rd International
Workshop on Link Discovery. ACM, LinkKDD ’05,
pages 36–43.

Eugene Agichtein, Carlos Castillo, Debora Donato,
Aristides Gionis, and Gilad Mishne. 2008. Finding
high-quality content in social media. In Proceedings
of the 2008 International Conference on Web Search
and Data Mining. ACM, pages 183–194.

383



Noor Aldeen Alawad, Aris Anagnostopoulos, Stefano
Leonardi, Ida Mele, and Fabrizio Silvestri. 2016.
Network-aware recommendations of novel tweets.
In Proceedings of the 39th International ACM SI-
GIR Conference on Research and Development in
Information Retrieval. ACM, pages 913–916.

Yoav Artzi, Patrick Pantel, and Michael Gamon. 2012.
Predicting responses to microblog posts. In Pro-
ceedings of the 2012 Conference of the North Amer-
ican Chapter of the Association for Computational
Linguistics: Human Language Technologies. Asso-
ciation for Computational Linguistics, pages 602–
606.

Lars Backstrom, Jon Kleinberg, Lillian Lee, and Cris-
tian Danescu-Niculescu-Mizil. 2013. Characteriz-
ing and curating conversation threads: expansion,
focus, volume, re-entry. In Proceedings of the sixth
ACM International Conference on Web Search and
Data Mining. ACM, pages 13–22.

Eytan Bakshy, Solomon Messing, and Lada A Adamic.
2015. Exposure to ideologically diverse news and
opinion on Facebook. Science 348(6239):1130–
1132.

David M Blei, Andrew Y Ng, and Michael I Jordan.
2003. Latent Dirichlet Allocation. Journal of ma-
chine Learning research 3(Jan):993–1022.

Ceren Budak and Rakesh Agrawal. 2013. On participa-
tion in group chats on Twitter. In Proceedings of the
22nd International Conference on World Wide Web.
ACM, pages 165–176.

Kailong Chen, Tianqi Chen, Guoqing Zheng, Ou Jin,
Enpeng Yao, and Yong Yu. 2012. Collaborative per-
sonalized tweet recommendation. In Proceedings of
the 35th international ACM SIGIR Conference on
Research and development in information retrieval.
ACM, pages 661–670.

Justin Cheng, Michael Bernstein, Cristian Danescu-
Niculescu-Mizil, and Jure Leskovec. 2017. Any-
one can become a troll: Causes of trolling behavior
in online discussions. In Proceedings of the 2017
ACM Conference on Computer Supported Coopera-
tive Work and Social Computing. ACM, CSCW ’17,
pages 1217–1230.

Nigel Crook, Ramón Granell, and Stephen G. Pulman.
2009. Unsupervised classification of dialogue acts
using a Dirichlet process mixture model. In Pro-
ceedings of The 10th Annual Meeting of the Spe-
cial Interest Group on Discourse and Dialogue, SIG-
DIAL 2009. pages 341–348.

Yajuan Duan, Long Jiang, Tao Qin, Ming Zhou, and
Heung-Yeung Shum. 2010. An empirical study on
learning to rank of tweets. In Proceedings of the
23rd International Conference on Computational
Linguistics. Association for Computational Linguis-
tics, pages 295–303.

Wei Feng and Jianyong Wang. 2013. Retweet or not?:
personalized tweet re-ranking. In Proceedings of the
sixth ACM International Conference on Web Search
and Data Mining. ACM, pages 577–586.

Kevin Gimpel, Nathan Schneider, Brendan O’Connor,
Dipanjan Das, Daniel Mills, Jacob Eisenstein,
Michael Heilman, Dani Yogatama, Jeffrey Flanigan,
and Noah A. Smith. 2011. Part-of-speech tagging
for Twitter: Annotation, features, and experiments.
In The 49th Annual Meeting of the Association for
Computational Linguistics: Human Language Tech-
nologies, Proceedings of the Conference. pages 42–
47.

Vicenç Gómez, Hilbert J Kappen, and Andreas
Kaltenbrunner. 2011. Modeling the structure and
evolution of discussion cascades. In Proceedings of
the 22nd ACM Conference on Hypertext and hyper-
media. ACM, pages 181–190.

Tom Griffiths. 2002. Gibbs sampling in the generative
model of Latent Dirichlet Allocation .

Yue He and Jinxiu Tan. 2015. Study on sina micro-
blog personalized recommendation based on se-
mantic network. Expert Systems with Applications
42(10):4797–4804.

Liangjie Hong, Aziz S Doumith, and Brian D Davison.
2013. Co-factorization machines: modeling user in-
terests and predicting individual decisions in Twit-
ter. In Proceedings of the sixth ACM International
Conference on Web Search and Data Mining. ACM,
pages 557–566.

Yifan Hu, Yehuda Koren, and Chris Volinsky.
2008. Collaborative filtering for implicit feedback
datasets. In Data Mining, 2008. ICDM’08. Eighth
IEEE International Conference on. IEEE, pages
263–272.

Kalervo Järvelin and Jaana Kekäläinen. 2002. Cumu-
lated gain-based evaluation of IR techniques. ACM
Trans. Inf. Syst. 20(4):422–446.

Thorsten Joachims. 2002. Optimizing search engines
using clickthrough data. In Proceedings of the
Eighth ACM SIGKDD International Conference on
Knowledge Discovery and Data Mining. pages 133–
142.

Shafiq R. Joty, Giuseppe Carenini, and Chin-Yew Lin.
2011. Unsupervised modeling of dialog acts in
asynchronous conversations. In Proceedings of the
22nd International Joint Conference on Artificial In-
telligence, IJCAI 2011. pages 1807–1813.

D. Jurafsky, E. Shriberg, and D. Biasca. 1997. Switch-
board SWBD-DAMSL shallow-discourse-function
annotation coders manual. Technical Report Draft
13, University of Colorado, Institute of Cognitive
Science.

Yehuda Koren, Robert M. Bell, and Chris Volinsky.
2009. Matrix factorization techniques for recom-
mender systems. IEEE Computer 42(8):30–37.

384



Haewoon Kwak, Changhyun Lee, Hosung Park, and
Sue Moon. 2010. What is Twitter, a social network
or a news media? In Proceedings of the 19th In-
ternational Conference on World wide Web. ACM,
pages 591–600.

David Laniado, Riccardo Tasso, Yana Volkovich, and
Andreas Kaltenbrunner. 2011. When the wikipedi-
ans talk: Network and tree structure of wikipedia
discussion pages. In ICWSM.

Chei Sian Lee and Long Ma. 2012. News sharing
in social media: The effect of gratifications and
prior experience. Computers in Human Behavior
28(2):331–339.

Greg Linden, Brent Smith, and Jeremy York. 2003.
Amazon. com recommendations: Item-to-item col-
laborative filtering. IEEE Internet computing
7(1):76–80.

Annie Louis and Shay B. Cohen. 2015. Conversa-
tion trees: A grammar model for topic structure in
forums. In Proceedings of the 2015 Conference
on Empirical Methods in Natural Language Pro-
cessing. Association for Computational Linguistics,
pages 1543–1553.

Christopher D. Manning, Prabhakar Raghavan, and
Hinrich Schütze. 2008. Introduction to information
retrieval. Cambridge University Press.

Julian McAuley and Jure Leskovec. 2013. Hidden fac-
tors and hidden topics: understanding rating dimen-
sions with review text. In Proceedings of the 7th
ACM Conference on Recommender Systems. ACM,
pages 165–172.

Jorge Nocedal. 1980. Updating quasi-newton matrices
with limited storage. Mathematics of computation
35(151):773–782.

Brendan O’Connor, Ramnath Balasubramanyan,
Bryan R Routledge, and Noah A Smith. 2010. From
tweets to polls: Linking text sentiment to public
opinion time series. ICWSM 11(122-129):1–2.

Olutobi Owoputi, Brendan O’Connor, Chris Dyer,
Kevin Gimpel, Nathan Schneider, and Noah A.
Smith. 2013. Improved part-of-speech tagging for
online conversational text with word clusters. In
Human Language Technologies: Conference of the
North American Chapter of the Association of Com-
putational Linguistics. pages 380–390.

Rong Pan, Yunhong Zhou, Bin Cao, Nathan Nan Liu,
Rajan M. Lukose, Martin Scholz, and Qiang Yang.
2008. One-class collaborative filtering. In Proceed-
ings of the 8th IEEE International Conference on
Data Mining. pages 502–511.

Ye Pan, Feng Cong, Kailong Chen, and Yong Yu. 2013.
Diffusion-aware personalized social update recom-
mendation. In Proceedings of the 7th ACM Confer-
ence on Recommender Systems. ACM, pages 69–76.

Xiaojun Quan, Chunyu Kit, Yong Ge, and Sinno Jialin
Pan. 2015. Short and sparse text topic modeling
via self-aggregation. In Proceedings of the Twenty-
Fourth International Joint Conference on Artificial
Intelligence. pages 2270–2276.

Alan Ritter, Colin Cherry, and Bill Dolan. 2010. Un-
supervised modeling of Twitter conversations. In
Human Language Technologies: Conference of the
North American Chapter of the Association of Com-
putational Linguistics. pages 172–180.

Michael Röder, Andreas Both, and Alexander Hinneb-
urg. 2015. Exploring the space of topic coherence
measures. In Proceedings of the Eighth ACM Inter-
national Conference on Web Search and Data Min-
ing. pages 399–408.

Ruslan Salakhutdinov and Andriy Mnih. 2008.
Bayesian probabilistic matrix factorization using
markov chain monte carlo. In Proceedings of the
25th International Conference on Machine learning.
ACM, pages 880–887.

Andreas Stolcke, Klaus Ries, Noah Coccaro, Eliza-
beth Shriberg, Rebecca Bates, Daniel Jurafsky, Paul
Taylor, Rachel Martin, Carol Van Ess-Dykema, and
Marie Meteer. 2000. Dialogue act modeling for au-
tomatic tagging and recognition of conversational
speech. Computational Linguistics 26(3):339–373.

Jan Vosecky, Kenneth Wai-Ting Leung, and Wilfred
Ng. 2014. Collaborative personalized Twitter search
with topic-language models. In Proceedings of
the 37th international ACM SIGIR Conference on
Research & Development in Information Retrieval.
ACM, pages 53–62.

Chong Wang and David M Blei. 2011. Collaborative
topic modeling for recommending scientific articles.
In Proceedings of the 17th ACM SIGKDD Inter-
national Conference on Knowledge Discovery and
Data Mining. ACM, pages 448–456.

M Woszczyna and A Waibel. 1994. Inferring linguistic
structure in spoken language. In Proceedings of IC-
SLP. IC-SLP.

Shaomei Wu, Jake M Hofman, Winter A Mason, and
Duncan J Watts. 2011. Who says what to whom on
Twitter. In Proceedings of the 20th International
Conference on World Wide Web. ACM, pages 705–
714.

Rui Yan, Mirella Lapata, and Xiaoming Li. 2012.
Tweet recommendation with graph co-ranking. In
Proceedings of the 50th Annual Meeting of the
Association for Computational Linguistics: Long
Papers-Volume 1. Association for Computational
Linguistics, pages 516–525.

Yang Yu, Xiaojun Wan, and Xinjie Zhou. 2016. User
embedding for scholarly microblog recommenda-
tion. In Proceedings of the 54th Annual Meeting of
the Association for Computational Linguistics. vol-
ume 2, pages 449–453.

385


