















































A Discriminative Approach to Japanese Zero Anaphora Resolution with Large-scale Lexicalized Case Frames


Proceedings of the 5th International Joint Conference on Natural Language Processing, pages 758–766,
Chiang Mai, Thailand, November 8 – 13, 2011. c©2011 AFNLP

A Discriminative Approach to Japanese Zero Anaphora Resolution
with Large-scale Lexicalized Case Frames

Ryohei Sasano
Precision and Intelligence Laboratory

Tokyo Institute of Technology
sasano@pi.titech.ac.jp

Sadao Kurohashi
Graduate School of Informatics

Kyoto University
kuro@i.kyoto-u.ac.jp

Abstract

We present a discriminative model for
Japanese zero anaphora resolution that si-
multaneously determines an appropriate
case frame for a given predicate and its
predicate-argument structure. Our model
is based on a log linear framework, and
exploits lexical features obtained from a
large raw corpus, as well as non-lexical
features obtained from a relatively small
annotated corpus. We report the results
of zero anaphora resolution on Web text
and demonstrate the effectiveness of our
approach. In addition, we also investigate
the relative importance of each feature for
resolving zero anaphora in Web text.

1 Introduction

Zero anaphora resolution is the task of detect-
ing and identifying the omitted arguments of a
predicate. Since arguments are often omitted in
Japanese, zero anaphora resolution plays an im-
portant role in the analysis of Japanese predicate-
argument structures. For example, in the follow-
ing text:

(i) Musuko-wa itazura-ga sukide
son-TOP mischief-NOM like

watashi-mo (φ-ni) te-wo yaiteiru.
I φ-DAT hands-ACC burn

(have difficulty)

(My son likes mischief, so I have difficulty with φ.)

the dative argument of the predicate ‘yaku1 (burn)’
has been omitted. The omitted element is called
a zero pronoun, and in this example it refers to
‘musuko (son).’ Although most previous work has
focused on zero anaphora in newspaper articles,

1‘Yaku’ is the original form of ‘yaiteiru.’

this paper aims to resolve zero anaphora in Web
text, since this involves a wider range of writing
styles and is thus considered to be a more practical
setting.

Reference resolution systems generally require
a variety of information sources ranging from
syntactic and discourse preferences to semantic
preferences (Ng and Cardie, 2002; Haghighi and
Klein, 2010). Since syntactic and discourse pref-
erences are not word-specific, they can be learned
from a relatively small annotated corpus. Seman-
tic preferences, on the other hand, represent highly
lexicalized knowledge, and hence it is difficult to
learn these from a small annotated corpus. In some
cases, knowledge of the relations between a predi-
cate and its particular argument is insufficient, par-
ticularly for zero anaphora resolution. For exam-
ple, although the dative argument of the predicate
‘yaku (bake/burn)’ is generally filled by a disk,
such as a CD or DVD, it is often filled by a per-
son, such as ‘musuko (son),’ if the accusative ar-
gument is filled by ‘te (hands)’ as in the example
(i).2 Thus, knowledge of relations among a predi-
cate and its multiple arguments is required to take
such preferences into account.

Sasano et al. (2008) exploited large-scale case
frames that were automatically constructed from
1.6 billion Web sentences as such a lexical re-
source, and proposed a probabilistic model for
Japanese zero anaphora resolution. Their model
demonstrated moderate performance, but it could
not easily introduce new features, especially over-
lapping ones, nor take into consideration the im-
portance of each feature, due to the assumption of
independence in estimating probability. However,
we think a variety of clues can be useful for zero
anaphora resolution, where it is important to ex-
ploit overlapping features and consider the impor-
tance of each feature.

2‘Te-wo yaku’ (literally ‘burn hands’) is a Japanese idiom,
which means ‘have difficulty’ in English.

758



Therefore, in this paper, we extend Sasano et
al. (2008)’s model by incorporating it into a log-
linear framework, and introduce overlapping fea-
tures such as lexical features with different gran-
ularities. In addition, we also investigate the rela-
tive importance of each feature for resolving zero
anaphora in Web text.

2 Related Work

Several approaches to Japanese zero anaphora res-
olution have been proposed. Seki et al. (2002) pro-
posed a probabilistic model for zero pronoun de-
tection and resolution that used hand-crafted case
frames. Kawahara and Kurohashi (2004) intro-
duced wide-coverage case frames that were auto-
matically constructed from a large corpus to alle-
viate the sparseness of hand-crafted case frames.
They used the case frames as selectional restric-
tions for zero pronoun resolution. Iida et al. (2006)
explored a machine learning method using rich
syntactic pattern features that represented the syn-
tactic relations between a zero-pronoun and its
candidate antecedent.

Since predicate-argument structure analysis and
zero anaphora resolution are closely related, sev-
eral approaches have simultaneously solved these
two tasks. Sasano et al. (2008) proposed a lex-
icalized probabilistic model for zero anaphora
resolution, which adopted an entity-mention
model and simultaneously resolved predicate-
argument structures and zero anaphora. Taira
et al. (2008) proposed a model for analyzing
predicate-argument structures by using decision
lists, which integrated the tasks of semantic role
labeling and zero-pronoun identification. Ima-
mura et al. (2009) proposed a discriminative
model for analyzing predicate-argument structures
that simultaneously conducted zero anaphora res-
olution.

For languages other than Japanese, Ferrandez
and Peral (2000) proposed a hand-engineered rule-
based method for both determining anaphoricity
and identifying antecedents in Spanish zero pro-
noun resolution. Zhao and Ng (2007) proposed
feature-based methods to Chinese zero anaphora
resolution. Kong and Zhou (2010) proposed a tree
kernel-based unified framework for Chinese zero
anaphora resolution, which dealt with three sub-
tasks: zero anaphora detection, anaphoricity de-
termination, and antecedent identification.

3 Case Frames

3.1 Lexicalized case frames
Our model exploits lexicalized case frames that
are automatically constructed from 1.6 billion
Web sentences by using Kawahara and Kuro-
hashi (2002)’s method. Case frames are con-
structed for each predicate like PropBank frames
(Palmer et al., 2005), and for each meaning of
the predicate like FrameNet frames (Fillmore et
al., 2003). However, neither pseudo-semantic role
labels such as Arg1 in PropBank nor information
about frames defined in FrameNet are included in
the case frames. Each case frame describes surface
cases that each predicate has and examples that
can fill a case slot, which is fully-lexicalized like
the subcategorization lexicon VALEX (Korhonen
et al., 2006).

Note that case frames offer not only knowledge
of the relations between a predicate and its par-
ticular case slot, but also knowledge of the re-
lations among a predicate and its multiple case
slots. Table 1 shows examples of constructed case
frames.3 A different case frame is constructed for
each meaning of ‘yaku (bake/burn),’ such as ‘bake
foods,’ ‘have difficulty,’ and ‘burn on a disk.’

3.2 Generalization of examples
The data sparseness problem is alleviated to some
extent but not eliminated by using case frames that
are automatically constructed from a large corpus.
For instance, there are thousands of named en-
tities (NEs) that cannot intrinsically be covered.
Sasano et al. (2008) generalized examples of case
slots based on 22 common noun categories defined
in the Japanese morphological analyzer JUMAN,4

and 8 NE classes defined by the IREX Commit-
tee (1999) to deal with this problem.

In addition, we generalized case slot examples
based on automatically acquired multi-word noun
clusters. Kazama and Torisawa (2008) proposed
the parallelization of EM-based clustering with the
aim of enabling large-scale clustering and using
the resulting clusters in NE recognition. We used
the resulting 2,000 clusters acquired from 1 mil-
lion unique multi-word nouns.

Table 2 lists examples of the resulting 2,000
clusters.3 As well as common noun categories and
NE classes, we calculated the average of the prob-
abilities that each case slot example belonged to

3We use English examples for the sake of readability.
4http://nlp.kuee.kyoto-u.ac.jp/nl-resource/juman-e.html

759



Case slot Examples:# Generalized examples:%
yaku (1) ga I:39, owner:26, daughter:22, [CT:PERSON]:0.620, [NE:PERSON]:0.116,
(bake) nominative mother:19, · · · , Asako:2, · · · [CL:887]:0.070, · · ·

wo bakery:9265, cake:4495, [CT:FOOD]:0.711, [CL:883]:0.221,
accusative meat:4057, fish:2002, · · · [CT:BODY PART]:0.105, · · ·

ni snack:35, breakfast:33, [CT:ABSTRACTION]:0.233, [CT:FOOD]:0.171,
dative birthday:29, gift:21, · · · [CL:624]:0.076, · · ·

de frying pan:894, [CT:ARTIFACT]:0.356,
tools/methods moderate heat:425, · · · [CL:291]:0.252, · · ·

...

yaku (2) ga who:7, teacher:7, everyone:5, [CT:PERSON]:0.372, [NE:PERSON]:0.128,
(have difficulty) nominative family:4, government:3, · · · [CT:ORGANIZATION]:0.128, · · ·

wo hand:6864 [CT:BODY PART]:1.000accusative
ni child:52, attack:43, treatment:40, [CT:ABSTRACTION]:0.432, [CT:PERSON]:0.172,

dative provision:32, daughter:30, · · · [NE:PERSON]:0.060, [CL:32]:0.016, · · ·

...

yaku (3) ga I:1, husband:1, [CT:PERSON]:1.000
(burn) nominative

wo file:20, tune:14, music:9, [CT:ABSTRACTION]:0.645,
accusative image:8, video:4, · · · [CT:ARTIFACT]:0.273, · · ·

ni CD:3106, DVD:2066, · · · [CL:70]:0.829, · · ·
dative

de machine:21, writing soft:10, [CT:ABSTRACTION]:0.294,
tools PC:5, iTunes:4, · · · [CT:ARTIFACT]:0.191, [NE:ARTIFACT]:0.054, · · ·

...
...

Table 1: Example case frames for ‘yaku (bake / have difficulty / burn).’

Cluster Nouns
CL:32 child (0.974), infant (0.738), kid (0.727), babies and infant (0.436), · · ·
CL:70 CD (0.896), DVD (0.837), CD-ROM (0.603), cassette (0.512), · · ·
CL:291 low heat (0.720), slow fire (0.715), moderate heat (0.681), distant fire (0.678), · · ·
CL:624 dinner (0.926), supper (0.925), lunch (0.882), breakfast (0.868), · · ·
CL:883 Chinese noodles (0.860), noodles (0.801), curry (0.793), cake (0.749), · · ·
CL:887 mother (0.909), parents (0.875), mom (0.838), husband (0.775), father (0.774), · · ·

Table 2: Examples of resulting 2,000 clusters (Kazama and Torisawa, 2008). Nouns that have high
probabilities of belonging to target clusters are shown with probabilities.

the target cluster, and added it to the case slot.
For example, if examples of a case slot include
‘CD:3106,’ ‘DVD:2066,’ and 271 other examples
that have no probability of belonging to cluster 70,
the average for cluster 70 is calculated as:

0.896 × 3106 + 0.837 × 2066
3106 + 2066 + 271

≈ 0.829.

This type of generalized example represents more
fine-grained semantic categories compared with
examples that are generalized by using common
noun categories and NE classes. The generalized
examples are also included in Table 1, such as
“[CL:70]:0.829” in the dative case slot of ‘yaku
(3).’ CT, NE, and CL in generalized examples de-
note common noun category, named entity class,
and multi-word noun cluster, respectively.

4 A Discriminative Model for Zero
Anaphora Resolution

4.1 Overview
Our model basically follows that of Sasano et
al. (2008), except for the method of estimat-
ing possible combinations of case frames and
predicate-argument structures. We also limited the
target cases for zero anaphora resolution to ‘ga’
(nominative), ‘wo’ (accusative), and ‘ni’ (dative)
cases. The outline of our model is as follows:

1. Parse an input text and recognize NEs.

2. Resolve coreference and link each mention to
a discourse entity or create a new entity.

3. From the end of each sentence, analyze the
predicate-argument structure for each verb or
adjective using the following steps:

760



1. < cf =‘yaku’(1), s =[NOM:‘watashi’ (I), ACC:‘te’ (hands), DAT:‘musuko’ (son)]>
2. < cf =‘yaku’(1), s =[NOM:‘watashi’ (I), ACC:‘te’ (hands), DAT:NULL’]>
3. < cf =‘yaku’(1), s =[NOM:NULL, ACC:‘te’ (hands), DAT:‘watashi’ (I)]>
4. < cf =‘yaku’(1), s =[NOM:‘musuko’ (son), ACC:‘te’ (hands), DAT:NULL>
5. <cf =‘yaku’(2),s= [NOM:‘watashi’ (I), ACC:‘te’ (hands), DAT:‘musuko’ (son)]>
6. < cf =‘yaku’(2), s =[NOM:‘watashi’ (I), ACC:‘te’ (hands), DAT:NULL’]>

...

Table 3: Examples of possible combinations of case frame cf and predicate-argument structure s for
the predicate ‘yaku’ in the example (i) in Section 1. Bold font indicates the proper combination for this
example.

(a) Select a case frame temporarily.
(b) List possible predicate-argument struc-

tures including omitted arguments.
(c) Estimate possible combinations of case

frames and predicate-argument struc-
tures, and select the one with the highest
estimate.

In 3-(b), we first consider only the overt argu-
ments and prune away improbable structures to
reduce the search space. We apply a log-linear
framework to estimating a combination of a case
frame and a predicate-argument structure to intro-
duce overlapping features and take into considera-
tion the relative importance of each feature.

Note that the estimation is not separately con-
ducted for each argument, but for all arguments
including overt and omitted arguments. For exam-
ples, when we analyze the predicate ‘yaku’ in the
example (i) in Section 1, we consider the various
combinations as listed in Table 3, and choose the
combination with the highest estimate.

4.2 Log-linear model
When text t and target predicate p are given, we
choose the combination of case frame cf and
predicate-argument structure s with the highest
conditional probability,

(cfbest, sbest) = arg max
cf,s

P (cf, s|p, t).

We model the conditional probability, using a
log-linear framework:

P (cf, s|p, t; Λ)= 1
Z(p, t)

exp{Λ · F (cf, s, p, t)},

Z(p, t) =
∑

{cf,s}∈C(p,t)
exp{Λ · F (cf, s, p, t)},

where F = (f1, . . . , fK) is a feature vector whose
elements represent K feature functions, Λ =
(λ1, . . . , λK) denotes a weight vector (parameter

vector) for the feature functions, and C(p, t) yields
a set of possible combinations of case frame cf
and predicate-argument structure s for given pred-
icate p and text t.

4.3 Parameter estimation
We now describe how parameter vector Λ is
estimated from a set of training data. When
the training set consisting of N instances {(s(1),
p(1), t(1)), (s(2), p(2), t(2)), . . . , (s(N), p(N), t(N))}
is given, we choose the combination of CF and Λ
that maximize the posterior probability:

max
CF,Λ

{
N∑

n=1

log P (cf (n), s(n)|p(n), t(n); Λ)−α||Λ||2
}

,

where α is a regularization parameter for the
L2 norm, and CF = (cf (1), cf (2), . . . , cf (N))
is a combination of possible case frames, i.e.
cf (n) is a candidate case frame for the given in-
stance (s(n), p(n), t(n)). Since the appropriate case
frames are not annotated in the training set, we
choose an appropriate case frame in estimating pa-
rameters with the following algorithm:

1. Initialize parameter Λ to a random value in
the range [0,1].

2. For each training instance, update cf (n) that
maximizes P (cf, s|p, t; Λ) with current pa-
rameter Λ:

ĉf
(n)

= arg max
cf (n)

P (cf (n), s(n)|p(n), t(n); Λ)

If CF = (cf (1), cf (2), . . . , cf (N)) is not up-
dated, we determine current parameter Λ as
the resulting parameter.

3. If CF is updated, we renew parameter Λ
that maximizes the posterior probability with
N training instances {(cf (1), s(1), p(1), t(1)),
. . . , (cf (N), s(N), p(N), t(N))},

761



LΛ=
N∑

n=1

logP (cf (n), s(n)|p(n), t(n); Λ)−α||Λ||2,

and go back to step 2. To avoid overfit-
ting, we include an L2-regularization term
in the objective.5 LΛ is maximized by the
Limited memory BFGS (L-BFGS) algorithm
(Nocedal, 1980).6

In both steps 2 and 3, the log-likelihood in-
creases monotonically, and this algorithm thus al-
ways converges to an optimal solution but does not
ensure the global maximum parameter will be as-
signed. That is, we can obtain convergence to dif-
ferent local optima. Therefore, we test several ini-
tial values and adopt the resulting parameter that
maximizes posterior probability.

5 Features

5.1 Lexical features
We exploit six types of lexical features: word PMI
(Pointwise Mutual Information), cluster PMI, cat-
egory PMI, NE PMI, occupancy of a case slot,
and overt argument assignment score. Their val-
ues are real values that are calculated by using case
frames. Since our model is based on the entity-
mention model that assigns zero pronouns not to a
certain mention but to an entity, several values can
be calculated for a certain lexical feature by tak-
ing coreferential mentions into consideration. In
such cases, we choose the highest value as a cor-
responding lexical feature.

Word PMI Each case slot of a case frame has
typical words that are often assigned to the slot.
We use the PMI features between a slot of a case
frame and its antecedent candidate to reflect such
preferences:

e.g.
• log{P (child|cf=yaku(2),case=ni)/P (child)}

As well as most other features, this type of fea-
tures is distinguished by the case of zero pronouns:
‘ga,’ ‘wo,’ and ‘ni,’ respectively.

Cluster, Category, and NE PMI We also use
generalized example versions of word PMI to al-
leviate the data sparseness problem in word PMI:

5We set α to 1.0 in all our experiments.
6We used libLBFGS 1.9:

http://www.chokkan.org/software/liblbfgs/.

e.g.
• log{P ([CL:32] |yaku(2), ni)/P ([CL:32])}

• log{P ([CT:PERSON]|yaku(2), ni)/P ([CT:PERSON])}

• log{P ([NE:PERSON]|yaku(2), ni)/P ([NE:PERSON])}

After this, we will generically call these three fea-
tures GE PMI. All the features described above
overlap, and only their granularities are different.

Occupancy of case slot We believe that there is
a relation between the occupancy of a case slot and
its generativity of zero pronouns. For example,
since the ni (dative) case of ‘yaku (3)’ often ap-
pears, we assume this slot is just omitted as a zero
pronoun even if there is no overt argument. How-
ever, since the ni (dative) case of ‘yaku (1)’ rarely
appears, we assume there is no case slot if there is
no overt argument.

Therefore, we use the log occupancy of the case
slot, whose value is the same as the log of the
generative probability of a case slot in Kawahara
and Kurohashi (2006)’s model and estimated from
case structure analysis of a large raw corpus:

e.g.

• logP (A(cs) = 1|cf=yaku(2),case=ni),
where A(cs) = 1 denotes that the target case
slot is occupied by an overt argument.

Overt argument assignment score Our model
not only takes into account the correspondence be-
tween a case slot and its omitted argument, but
also the correspondence between a case slot and
its overt argument. The score for overt argument
assignment reflects the likelihood of an overt argu-
ment assignment, and this is the same as the score
for the predicate-argument structure in Kawahara
and Kurohashi (2006)’s model, which does not
take into consideration zero anaphoric relations.

e.g.

• logP (cf=yaku(2), gaov:watashi, woov:te|yaku)

Only this feature is not separately calculated for
each case, but for the whole overt predicate-
argument structure. Note that, this feature also in-
cludes non-lexical preferences in the analysis of a
predicate-argument structure.

5.2 Non-lexical features
We also exploit three types of non-lexical features,
which are binary features to reflect syntactic and

762



Intra sentential (64 categories)
Itopic: Mentioned with topic marker
IP-self: Mentioned at parent node
IC-self: Mentioned at child node
IGP-self: Mentioned at grand-parent node
IGC-self: Mentioned at grand-child node

... ...

IB-self: Mentioned at a preceding node in the
sentence except above (before)

IA-self: Mentioned at a following node in the
sentence except above (after)

IP-ga-ov: Overt nominative argument of a predicate
in parent node

IP-ga-om: Omitted nominative argument of a
predicate in parent node

IP-wo-ov: Overt accusative argument of a predicate
in parent node

IP-wo-om: Omitted accusative argument of a
predicate in parent node

... ...

IGP-ga-ov: Overt nominative argument of a predicate
in grand-parent node

... ...

Inter sentential (21 categories)
B1: Mentioned in the adjacent sentence
B1-ga-ov: Overt nominative argument of a predicate

in the adjacent sentence
B1-ga-om: Omitted nominative argument of a

predicate in the adjacent sentence
B1-wo-ov: Overt accusative argument of a predicate

in the adjacent sentence

... ...

B2: Mentioned in two sentences before
B2-ga-ov: Overt nominative argument of a predicate

in the two sentence before

... ...

B3: Mentioned in more than two sentences
before

... ...

Table 4: Examples of case/location categories.

discourse preferences. Since Web text slightly ad-
heres to formal grammar and thus rich syntactic
preferences are not considered very important for
resolution of zero anaphora in Web text, we only
introduce simple features as non-lexical features.

Case/location We use case/location features to
reflect syntactic, functional, and locational prefer-
ences. We considered 85 case/location categories,
examples of which are summarized in Table 4.
If an antecedent candidate appears in a certain
case/location category, the corresponding feature
value is 1; otherwise 0. These features are made
for each case, respectively, i.e. there are a total of
255 case/location features.

Salience Previous work has reported the useful-
ness of salience in anaphora resolution (Lappin
and Leass, 1994; Mitkov et al., 2002; Sasano et
al., 2008). We introduce salience features to take

into account the salience of each discourse entity.
First, we apply the following simple rules to esti-
mating the salience of each entity, and we then set
salience features, whose value is 1 if the salience
of an antecedent candidate is no less than 1.0; oth-
erwise 0.

• +2: mentioned with topical marker “wa.”
• +1: mentioned without topical marker “wa.”
• ×0.5: beginning of each sentence.

Case assigned features We introduce case as-
signed features for each case type. The value is 1
if the corresponding zero pronoun is assigned to
an antecedent; otherwise 0.

If the weights for these features become larger,
corresponding zero pronouns are assigned to an-
tecedents more often. Thus, the weights for these
features are regarded as parameters to control the
recall/precision trade-off. Although we mainly
evaluate our system by using the F-measure, the
algorithm mentioned in Section 4.3 does not select
a parameter that maximizes the F-measure. Thus,
after parameters are estimated by the algorithm,
we adjust the weights for these features to maxi-
mize the F-measure by using training or develop-
ment data.

6 Experiments

6.1 Setting
We used the same data set as described in (Sasano
et al., 2009). This data set consisted of 186 Web
documents (979 sentences, 19,677 morphemes), in
which all predicate-argument relations were man-
ually annotated. There were 2,137 predicates in
this corpus, and 683 zero anaphoric relations were
annotated. We call this data set a Web Corpus
after this. We performed 6-fold cross-validation.
We used correct morphemes, named entities, de-
pendency structures, and coreference relations that
were manually annotated to concentrate on zero
anaphora resolution. We tested 10 initial values
as parameter Λ. Since the parameters converged
to almost the same value for each initial value,
we considered that our model achieved the global
maximum parameters in most cases.

We applied two baseline models for compari-
son. The first was the model proposed by Sasano
et al. (2008), which did not use a log-linear frame-
work but exploited almost the same clues. In
addition, we also conducted an experiment with
merged case frames to verify the usefulness of the

763



Recall Precision F-measure
Sasano et al. (2008)’s 0.341 0.306 0.322
model (233/683) (233/762)
Proposed model with 0.334 0.412 0.369
merged case frames (228/683) (228/553)
Proposed model 0.379 0.403 0.391

(259/683) (259/642)

Table 5: Experimental results of zero anaphora
resolution on Web Corpus.

Case Type Recall Precision F-measure
NOM Intra- 0.504 0.460 0.481

sentential (120/238) (120/261)
Inter- 0.460 0.387 0.420
sentential (104/226) (104/269)

ACC Intra- 0.250 0.447 0.321
sentential (17/68) (17/38)
Inter- 0.163 0.194 0.177
sentential (7/43) (7/36)

DAT Intra- 0.105 0.316 0.158
sentential (6/57) (6/19)
Inter- 0.098 0.263 0.143
sentential (5/51) (5/19)

Table 6: Detailed experimental results for the pro-
posed model on the Web Corpus.

case frames that had been constructed for each
meaning of each verb/adjective. We merged all
case frames of a certain verb/adjective in this ex-
periment, and thus each predicate only had one
case frame. As a result, this model only took into
account knowledge of the relations between two
terms, i.e. a predicate and its particular argument.
For example, while the model with the case frames
in Table 1 considers the dative case of ‘te-wo yaku’
would be filled by a person, the model with the
merged case frame considers the case would be
filled by a disk with high probability.

6.2 Experimental Results

Table 5 summarizes the experimental results of
zero anaphora resolution on the Web Corpus.
The results indicate that our proposed model out-
performed both Sasano et al. (2008)’s model7

and the model with merged case frames, which
demonstrates the effectiveness of the log-linear
framework and the usefulness of the case frames
that were constructed for each meaning of each
verb/adjective.

Table 6 shows the performance of the proposed
7For the same 20 articles that were used for testing in

(Sasano et al., 2008), our model achieved a recall of 0.525
(64/122), a precision of 0.615 (64/104), and an F-measure of
0.566, while they obtained a recall of 0.410 (50/122), a pre-
cision of 0.373 (50/134), and an F-measure of 0.391.

Removed feature type Recall Precision F-measure
None (Use all features) 0.379 0.403 0.391

(259/683) (259/642)
(Lexical features)
− Word PMI 0.363 0.378 0.370

(248/683) (248/656)
− Cluster PMI 0.375 0.401 0.387

(256/683) (256/638)
− Category PMI 0.367 0.423 0.393

(251/683) (251/593)
− NE PMI 0.381 0.389 0.385

(260/683) (260/668)
− GW PMI 0.350 0.413 0.379

(Clust. + Cat. + NE) (239/683) (239/579)
− All PMI 0.325 0.391 0.355

(Word + GW) (222/683) (222/567)
− Occupancy of 0.365 0.404 0.383

case slot (249/683) (249/617)
− Overt argument 0.411 0.350 0.378

assignment score (281/683) (281/804)
(Non-lexical features)
− Case/location 0.264 0.346 0.299

(180/683) (180/520)
− Salience 0.376 0.411 0.393

(257/683) (257/626)
− All non-lexical 0.250 0.312 0.279

(Case/loc. + Salience) (171/683) (171/545)

Table 7: Performance by removing one feature
type at a time from feature sets. Bold font indi-
cates higher F-measures than 0.390 and italics in-
dicate F-measures lower than 0.380.

model for each case and for each of intra- and
inter-sentential zero anaphoric relations. Since the
Web Corpus consists of relatively short sentences,
there are many inter-sentential zero anaphora.
Compared with previous work (Taira et al., 2008;
Imamura et al., 2009), our model can resolve inter-
sentential zero anaphora in the Web Corpus with
comparatively good performance.

6.3 Contribution of features

We eliminated feature types one by one to in-
vestigate the contribution each made. Table 7
presents the eliminated feature types and the per-
formance without each type. This table indicates
the importance of word PMI and case/location fea-
tures, since we obtained 0.021 and 0.092 lower F-
measures without these features, respectively.

On the other hand, generalized example ver-
sions of word PMI did not affect performance
much. However, when all generalized exam-
ple PMIs were eliminated, performance wors-
ened. Therefore, we considered that cluster PMI,
category PMI, and NE PMI could be clues for
zero anaphora resolution, and confirmed that zero
anaphora resolution could benefit from overlap-

764



Feature Weight
ga wo ni

nominative accusative dative
Occupancy of 0.292 0.531 0.723case slot
Word PMI 0.154 0.299 0.211
Cluster PMI 0.005 0.347 0.058
Category PMI 0.844 0.617 0.391
NE PMI 0.563 -0.119 -0.444

Table 8: Weights of lexical features. The Bold font
indicates that value of weight is larger than aver-
age of weights in the same row.

ping features. Salience features did not contribute
to performance when using case/location features.
We think this is because case/location features in-
volve salience information.

Table 8 lists the weights of the lexical features
in the proposed model. Since the variance in each
type of feature is different, it is not very meaning-
ful to compare different feature types. However,
we can find a tendency for each case type by com-
paring the weights of the same feature types. In
fact, we have obtained several interesting findings.

The weights of the case slot occupancy features,
which denote how often the target slot is occupied
by an overt argument, are large for the wo (ac-
cusative) and ni (dative) cases, but small for the ga
(nominative) case. This means that there are tight
relations between occupancy of the case slot and
the generativity of the slot in the accusative and
dative cases, but not in the case of the nominative.

We also found differences between the nomina-
tive and the accusative cases in the PMI features.
The weights of course-grained lexical knowledge,
such as category and NE PMIs, are relatively large
in the nominative case. However, the weights of
fine-grained lexical knowledge are relatively large
in the accusative case. This means that the lexical
preference for the nominative is coarser than that
for the accusative case.

6.4 Comparison with previous work
We also conducted experiments on the NAIST
Text Corpus version 1.4β (Iida et al., 2007) to
compare our results with those from previous
work. We used articles from January 1st to 11th
and editorials from January to August for training,
articles on January 12th and 13th and the Septem-
ber editorials for development, and articles from
January 14th to 17th and editorials from October
to December for testing. While the NAIST Text
Corpus has the predicate-argument structure of the

Case Type Imamura et al. (2009) Our modelR P F R P F

NOM Intra 0.434 0.588 0.500 0.400 0.390 0.395
Inter 0.076 0.475 0.131 0.221 0.273 0.244

ACC Intra 0.216 0.537 0.308 0.169 0.181 0.175
Inter 0.004 0.250 0.007 0.050 0.101 0.066

DAT Intra 0.000 0.000 0.000 0.098 0.082 0.089
Inter 0.000 0.000 0.000 0.030 0.023 0.026

Table 9: Experimental results on the NAIST Text
Corpus. R, P, and F denote recall, precision, and
F-measure, respectively.

original form annotated even for predicates that
appear in passive or causative voice, our model
outputs the surface predicate-argument structure.
Therefore, we excluded such predicates. Table 9
summarizes the performance of our model on the
NAIST Text Corpus with the performance of Ima-
mura et al. (2009)’s model. Although these exper-
iments did not use the exact same data, we can see
that our model achieved comparable performance
even for newspaper articles.

Compared with Iida et al. (2006)’s model,
which exploited rich syntactic patterns, our model
did not seem to perform as well for the NAIST
Text Corpus.8 We conjectured that this was be-
cause the NAIST Text Corpus consisted of news-
paper articles that included relatively long sen-
tences with formal grammar, and thus rich syn-
tactic patterns were quite effective. Our model
also took into account syntactic clues by using
case/location features. However, since we basi-
cally focused on the zero anaphora in Web text
that only adhered slightly to formal grammar, we
did not give priority to exploring effective syntac-
tic patterns.

7 Conclusion

This paper presented a discriminative model for
Japanese zero anaphora resolution that can ex-
ploit large-scale lexicalized case frames, as well
as non-lexical features obtained from a relatively
small annotated corpus. Experimental results on
a Web text revealed that our model could effec-
tively resolve zero anaphora. We plan to investi-
gate new features for zero anaphora resolution in-
cluding richer syntactic patterns and global con-
straints in future work.

8Note that, their experiment was conducted on a small and
relatively reliable subset of the NAIST Text Corpus, and thus
we could not directly compare the results.

765



References
A. Ferrandez and J. Peral. 2000. A computational

approach to zero-pronouns in spanish. In Proc. of
ACL’00, pages 166–172.

C. J. Fillmore, C. R. Johnson, and M. R.L. Petruck.
2003. Background to framenet. International Jour-
nal of Lexicography, 16(3):235–250.

Aria Haghighi and Dan Klein. 2010. Coreference reso-
lution in a modular, entity-centered model. In Proc.
of NAACL-HLT’10, pages 385–393.

Ryu Iida, Kentaro Inui, and Yuji Matsumoto. 2006.
Exploiting syntactic patterns as clues in zero-
anaphora resolution. In Proc. of COLING-ACL’06,
pages 625–632.

Ryu Iida, Mamoru Komachi, Kentaro Inui, and Yuji
Matsumoto. 2007. Annotating a japanese text cor-
pus with predicate-argument and coreference rela-
tions. In Proc. of ACL’07 Workshop: Linguistic An-
notation Workshop, pages 132–139.

Kenji Imamura, Kuniko Saito, and Tomoko Izumi.
2009. Discriminative approach to predicate-
argument structure analysis with zero-anaphora res-
olution. In Proc. of ACL-IJCNLP’09, pages 85–88.

IREX Committee, editor. 1999. Proc. of the IREX
Workshop.

Daisuke Kawahara and Sadao Kurohashi. 2002. Fertil-
ization of case frame dictionary for robust Japanese
case analysis. In Proc. of COLING’02, pages 425–
431.

Daisuke Kawahara and Sadao Kurohashi. 2004. Zero
pronoun resolution based on automatically con-
structed case frames and structural preference of an-
tecedents. In Proc. of IJCNLP’04, pages 334–341.

Daisuke Kawahara and Sadao Kurohashi. 2006. A
fully-lexicalized probabilistic model for Japanese
syntactic and case structure analysis. In Proc. of
HLT-NAACL’06, pages 176–183.

Jun’ichi Kazama and Kentaro Torisawa. 2008. Induc-
ing gazetteers for named entity recognition by large-
scale clustering of dependency relations. In Proc. of
ACL-HLT’08, pages 407–415.

Fang Kong and Guodong Zhou. 2010. A tree kernel-
based unified framework for chinese zero anaphora
resolution. In Proc. of EMNLP’10, pages 882–891.

Anna Korhonen, Yuval Krymolowski, and Ted Briscoe.
2006. A large subcategorization lexicon for nat-
ural language processing applications. In Proc.of
LREC’06, pages 3000–3006.

Shalom Lappin and Herbert J. Leass. 1994. An algo-
rithm for pronominal anaphora resolution. Compu-
tational Linguistics, 20(4):535–562.

Ruslan Mitkov, Richard Evans, and Constantin Orăsan.
2002. A new, fully automatic version of Mitkov’s
knowledge-poor pronoun resolution method. In
Proc. of CICLing’02, pages 168–186.

Vincent Ng and Claire Cardie. 2002. Improving ma-
chine learning approaches to coreference resolution.
In Proc. of ACL’02, pages 104–111.

Jorge Nocedal. 1980. Updating quasi-newton matrices
with limited storage. Mathematics of Computation,
35(151):773–782.

Martha Palmer, Dan Gildea, and Paul Kingsbury. 2005.
The proposition bank: A corpus annotated with se-
mantic roles. Computational Linguistics, 31(1):71–
105.

Ryohei Sasano, Daisuke Kawahara, and Sadao Kuro-
hashi. 2008. A fully-lexicalized probabilistic model
for japanese zero anaphora resolution. In Proc. of
COLING’08, pages 769–776.

Ryohei Sasano, Daisuke Kawahara, and Sadao Kuro-
hashi. 2009. The effect of corpus size on case
frame acquisition for discourse analysis. In Proc.
of NAACL-HLT’09, pages 521–529.

Kazuhiro Seki, Atsushi Fujii, and Tetsuya Ishikawa.
2002. A probabilistic method for analyzing
Japanese anaphora integrating zero pronoun detec-
tion and resolution. In Proc. of COLING’02, pages
911–917.

Hirotoshi Taira, Sanae Fujita, and Masaaki Nagata.
2008. A japanese predicate argument structure anal-
ysis using decision lists. In Proc. of EMNLP’08,
pages 523–532.

Shanheng Zhao and Hwee Tou Ng. 2007. Identifi-
cation and resolution of Chinese zero pronouns: A
machine learning approach. In Proc. of EMNLP-
CoNLL’07, pages 541–550.

766


