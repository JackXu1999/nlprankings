




































NORMA: Neighborhood Sensitive Maps for Multilingual Word Embeddings


Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 512–522
Brussels, Belgium, October 31 - November 4, 2018. c©2018 Association for Computational Linguistics

512

NORMA:
Neighborhood Sensitive Maps for Multilingual Word Embeddings

Ndapa Nakashole
Computer Science and Engineering
University of California, San Diego

La Jolla, CA 92093
nnakashole@eng.ucsd.edu

Abstract

Inducing multilingual word embeddings by
learning a linear map between embedding
spaces of different languages achieves remark-
able accuracy on related languages. How-
ever, accuracy drops substantially when trans-
lating between distant languages. Given
that languages exhibit differences in vocabu-
lary, grammar, written form, or syntax, one
would expect that embedding spaces of dif-
ferent languages have different structures es-
pecially for distant languages. With the goal
of capturing such differences, we propose a
method for learning neighborhood sensitive
maps, NORMA. Our experiments show that
NORMA outperforms current state-of-the-art
methods for word translation between distant
languages.

1 Introduction

The success of monolingual word embeddings has
sparked interest in multilingual word embeddings.
The goal is to learn word vectors where similar
words have similar vector representations regard-
less of their language. Multilingual word embed-
dings are playing an increasingly prominent role
in machine translation (Zou et al., 2013; Lam-
ple et al., 2018; Artetxe et al., 2018b). In addi-
tion, they are a promising avenue for cross-lingual
model transfer (Guo et al., 2015; Täckström et al.,
2012).

A prominent approach to learning multilingual
word embeddings is to induce a mapping function
between embedding spaces of different languages.
However, there is a key assumption behind learn-
ing such a mapping function: that the embedding
spaces of different languages exhibit similar struc-
tures (Mikolov et al., 2013a). Evidence that this
assumption holds has mostly been through extrin-
sic evaluation metrics such as word translation ac-
curacy. A notable exception is (Mikolov et al.,

Figure 1: Bottom: By learning a linear map between
embedding spaces of related languages, e.g., en-es, cur-
rent methods achieve high accuracy on word transla-
tion. Top: For distant language pairs, e.g., en-ru, where
differences are larger, word translation accuracy sub-
stantially degrades.

2013a), who showed empirical evidence on ani-
mals and numbers. Embeddings corresponding to
a few numbers and animals in English and Span-
ish were projected down to two dimensions us-
ing PCA, and then manually rotated to accentuate
similarity. Despite showing only these two con-
cepts for two related languages, this work con-
cluded that embedding spaces of different lan-
guages exhibit similar geometric arrangements.
Additionally, work in this line of inquiry has con-
tinued to develop methods based on this assump-
tion (Artetxe et al., 2018a; Conneau et al., 2018).
Given that languages differ along dimensions such
as vocabulary, grammar, written form, and syntax,
one would expect that embedding spaces of differ-
ent languages exhibit different structures. Indeed,
recent work showed that assumptions of isomor-
phism and linearity do not hold (Søgaard et al.,
2018; Nakashole and Flauger, 2018)

While these assumptions do not substantially
affect accuracy when translating between related



513

languages, this is not the case for distant lan-
guages, see Figure 1. There is no established
quantitative metric for measuring distances be-
tween languages. Language trees trace the evo-
lution of languages but do not provide similar-
ity scores. (Chiswick and Miller, 2005) learned
similarity scores of 43 different languages to En-
glish by measuring how well Americans could
learn a given language in a fixed period of time.
Low scores on a standardized proficiency test were
taken to indicate a large distance between the lan-
guage and English. According to their scores,
Japanese and Chinese are the most distant from
English, Russian has a middle score, and French,
Portuguese, Dutch, as expected, have some of the
highest scores.

Additionally, linguists and psychologists have
long studied the question of how language af-
fects the way we think (Birner, 1999; Boroditsky,
2011). This influence would arise due to different
languages organizing concepts differently.

We would like to model some aspects of the
structural differences of languages when learning
mapping functions between embedding spaces. To
this end, we propose to learn neighborhood sensi-
tive maps. We can, in principle, achieve neighbor-
hood sensitive maps by training non-linear func-
tions. However, training non-linear functions, in
particular deep neural networks for this problem
is difficult to optimize for this zero-shot (Lazari-
dou et al., 2015) learning problem, as we show in
our experiments. Prior work alludes to similar ob-
servations(Mikolov et al., 2013a). For example,
(Conneau et al., 2018) found that using non-linear
mapping functions made training unstable1.

In summary, our contributions are as follows:

• We propose a method for learning neighbor-
hood sensitive maps, NORMA, which learns
a single mapping function but in a departure
from prior work, it discovers neighborhoods.
NORMA avoids learning multiple mapping
functions, thus enabling parameter sharing
among neighborhoods. This is a more effi-
cient use of training data than if we were to
train multiple mapping functions for differ-
ent neighborhoods as is done in (Zou et al.,
2013).

• The neighborhoods are learned jointly while
learning to translate, and we show that they

1https://openreview.net/forum?id=H196sainb

are interpretable.

• Our experiments show that for word trans-
lation between distant languages, NORMA
substantially outperforms methods that
achieve the best performance when translat-
ing between related languages.

• Additionally, in the related language setting,
we show that on rare words NORMA sub-
stantially outperforms state-of-the-art meth-
ods.

2 Related Work

The common approach to learning cross embed-
ding space mapping functions is: first monolingual
word embeddings for each language are trained in-
dependently; and second, a mapping function is
learned, using supervised or unsupervised meth-
ods. The resulting mapping function enables
translating words from the source to the target lan-
guage.

Map Induction Methods. The earliest and sim-
plest approach is to use a regularized least squares
loss to induce a linear map M as follows:
M̂ = argminM ||MX − Y||F + λ||M||,
here X and Y are matrices that contain word em-
bedding vectors for the source and target language
(Mikolov et al., 2013a; Dinu et al., 2014; Vulic
and Korhonen, 2016). Improved results were ob-
tained by imposing an orthogonality constraint on
M (Xing et al., 2015; Smith et al., 2017). Another
loss function used in prior work is the max-margin
loss, which has been shown to significantly outper-
form the least squares loss (Lazaridou et al., 2015;
Nakashole and Flauger, 2017).

Another approach is to use canonical correla-
tion analysis (CCA) to map two languages to a
shared embedding space (Haghighi et al., 2008;
Faruqui and Dyer, 2014; Lu et al., 2015; Ammar
et al., 2016).

Most of the prior methods can be characterized
as a series of linear transformations. In particular,
(Artetxe et al., 2018a) propose a framework to dif-
ferentiate prior methods in terms of which trans-
formations they perform: embedding normaliza-
tion, whitening, re-weighting, de-whitening, and
dimensionality reduction.

Work on phrase translation proposed to in-
duce many local maps that are individually trained
(Zhao et al., 2015) on local neighborhoods. In



514

contrast, our approach trains a single function
while taking into account neighborhood sensitiv-
ity. Our underlying motivation of neighborhood
sensitivity is similar in spirit to the use of lo-
cally linear embeddings for nonlinear dimension-
ality reduction (Roweis and Saul, 2000).

Forms of Supervision. The methods we have
described so far fall under supervised learning.
In the supervised setting, a seed dictionary (5k
word pairs is a typical size) is used to induce
the mapping function. In (Artetxe et al., 2017)
a semi-supervised approach is explored, whereby
the method alternates between learning the map
and generating an increasingly large dictionary.
Completely unsupervised methods have recently
been proposed using adversarial training (Barone,
2016; Zhang et al., 2017; Conneau et al., 2018).
However, the underlying methods for learning the
mapping function are similar to prior work such as
(Xing et al., 2015). The limitations and strengths
of unsupervised methods are detailed in (Søgaard
et al., 2018)

Although in our our experiments we work in the
supervised setting, NORMA can work with any
form of supervision.

Translation Retrieval Methods. The most
commonly used way to obtain a translation t of
a source language word s is nearest neighbor re-
trieval, given by: t = argmaxt cos(Mxs, yt).
Alternative retrieval methods have been pro-
posed, such as the inverted nearest neighbor re-
trieval(Dinu et al., 2014), inverted softmax (Smith
et al., 2017) and Cross-Domain Similarity Local
Scaling (CSLS) (Conneau et al., 2018). Since we
are interested in evaluating the quality of mapping
functions, our experiments use standard nearest
neighbor retrieval for all methods.

3 Local Maps in Embedding Space

Is it useful for maps to be neighborhood sensitive?
To study this question we carried out experiments
comparing performance of neighborhood-specific
maps to global maps. A thorough analysis of this
kind was carried out in our prior work (Nakashole
and Flauger, 2018)

We created neighborhoods by first selecting
the embeddings of a few words associated with
specific topics such as diseases, or cities. We then
added all nearby words, which are words whose
cosine similarity to any of the selected words is

>=0.5 2. We used three language pairs for local
vs global map translation experiments: English to
German, English to Portuguese, and English to
Swedish. The neighborhoods and their train/test
splits are:
en − de: medication(3,415/500),
cities(2,083/500), and animals(990/500);
en − pt: diseases(1,670/300), chemi-
cals(1,279/300), and names(1,986/300);
en − sv: flowers(1,537/200), insects(1,271/200),
and names(1,416/200). The training and test data
was obtained from subsets of Facebook AI MUSE
lexicons3

For each of the neighborhoods, we evaluated
translation accuracy both when using a locally
trained map and when using a globally trained
map. The difference is that the locally trained map
is only trained using training data from the neigh-
borhood, whereas the global map is trained using
training data from the neighborhood but also from
all other neighborhoods and more (˜10000 word
pairs). That is, the training data for global maps is
a superset of the local training data.

We trained all maps using linear transforma-
tions. As we will show in our experiments, opti-
mizing neural network mapping functions for this
problem fails. This is a similar observation to
prior work (Mikolov et al., 2013a; Conneau et al.,
2018)1. More details on models and experimental
settings are described in Sections 4 and 5.

Figure 2 shows that for various neighbor-
hoods, translation accuracy is higher when we
train neighborhood-specific maps than one single
global map. These results are similar to (Zou et al.,
2013) who then trained many local maps. While
we could also proceed to train many local maps,
this requires identifying optimal neighborhoods.
It also requires gathering sufficient training data
for each of the neighborhoods independently. In
our proposed method, NORMA, we avoid learn-
ing multiple maps, creating a single map, while
modeling neighborhood information and promot-
ing parameter sharing.

Overall, the results in Figure 2 are an indicator
neighborhood sensitivity in maps is useful. This
would particularly be useful for distant languages

2We found a 0.5 cutoff to be a good compromise between
neighborhood purity, and size. However, our final method
(Section 4) on which all our comparison experiments were
based, automatically discovers neighborhoods based on ideas
from sparse coding.

3https://github.com/facebookresearch/
MUSE

https://github.com/facebookresearch/MUSE
https://github.com/facebookresearch/MUSE


515

where a single global map that is linear might not
suffice since the underlying embedding structure
for distant languages might differ more than those
of related languages as depicted in Figure 1.

4 Model

In this section we introduce our model for learn-
ing neighborhood sensitive maps, NORMA. Our
approach jointly discovers neighborhoods while
learning to translate.

4.1 Reconstructive Neighborhood Discovery

Inspired by work on sparse coding (Lee et al.,
2007), we discover neighborhoods by learning a
reconstructive dictionary. We would like to learn
a dictionary of neighborhoods on the source lan-
guage side. To learn this dictionary, we set up a re-
construction objective, where for any given word
embedding xi ∈ Rd, where d is the dimension-
ality of the word embeddings, we want to recon-
struct xi using a linear combination of K neigh-
borhoods. Let D ∈ RK×d be the neighborhood
matrix, each row of D represents a d-dimensional
vector which can be interpreted as representing the
center of the neighborhood. Let X ∈ RN×d be
a set of N embedding vectors corresponding to
words in the source language vocabulary4. We can
learn a reconstructive dictionary of K neighbor-
hoods with the following objective:

D,V = argmin
D,V

||X−VD||22 (1)

D ∈ RK×d is the learned dictionary of neighbor-
hoods, K > d and thus the dictionary is over-
complete; V ∈ RN×K are the learned neighbor-
hood membership weights for X. While we use
the squared loss, other loss functions can be used
(Lee et al., 2007). To encourage neighborhoods to
be different from each other, one can impose an
orthogonality constraint : ||DDT − I|| where I is
the identity matrix. The reconstruction error with
an orthogonality penalty is:

R(θ) = ||X−VD||22 + λ||DDT − I|| (2)

Where λ is a hyperparameter which controls the
contribution of the orthogonality constraint to the
reconstruction error.

4Since the vocabulary size can be very large, in our exper-
iments, we work in batches of N=50

4.2 Joint Neighborhood Discovery and
Translation

Our approach ties neighborhood discovery to the
word translation task. First, we obtain neighbor-
hood ‘factorized’ representations by multiplying
the input vector X by the dictionary of neighbor-
hoods:

XN = XD
T ,

where XN ∈ RN×K . Here again N refers to
words in the source language vocabulary, English
in the case of en − de translation. And K is the
number of neighborhoods.

Second, we obtain an intermediate representa-
tion of the input, which contains both the original
input X and the neighborhood ‘factorized’ repre-
sentations of the input XN , through vector con-
catenation as follows:

XI = [XN ;X],

where XI ∈ RN×(K+d).
To get the final representation of the input, we

project XI into a low-dimensional vector of the
same size as the original input:

XF = XIWf ,

where Wf ∈ R(K+d)×d is a set of learned pa-
rameters. And XF ∈ RN×d is the resulting final
representation.

We use these neighborhood sensitive represen-
tation XF as the input for learning the mapping
function W, instead of the original X. We ex-
plore different ways for learning the mapping W:
first a linear mapping, and second, a single layer
neural network with a leaky rectified linear unit
(leaky ReLU5) non-linearity and a highway layer
(Srivastava et al., 2015). As we will show in our
experiments, training neural networks with more
layers fails on this zero-shot learning problem.

For the linear map, the translation ŷi is given
by:

ŷlineari = WxFi (3)

where xFi ∈ XF is the neighborhood sensitive
representation of xi.

For the neural network map, using a single layer
neural network, and a highway layer, the transla-

5It outperformed other non-linearities such as tanh in our
initial experiments.



516

medication cities animals0.0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9

Tr
an

sla
tio

n 
ac

cu
ra

cy

en-de

global local

diseases chemicals names0.0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9 en-pt

flowers insects names0.0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9 en-sv

Figure 2: Accuracy of globally vs locally trained mapping functions for various neighborhoods on en−de, en−pt,
and en− sv translation.

tion ŷi is given by:

hi = σ1(xFiW)

ti = σ2(xFiW
t)

ŷnni = ti × hi + (1.0− ti)× xFi (4)

where σ1 is a non-linearity. We use a leaky-ReLU
non-linearity. σ2 is the sigmoid function. Wt is
another set of parameters in addition to W.

4.3 Objective Function

We use the max-margin loss function to learn the
parameters of the model:

L(θ) =
m∑
i=1

k∑
j ̸=i

max
(
0, γ + d(yi, ŷ

g
i )−

d(yj , ŷ
g
i )
)
, (5)

Where yi is the true label; ŷ
g
i is the prediction,

which is either ŷlineari or ŷ
nn
i . The goal of the

max-margin loss function is to rank correct train-
ing data pairs (xi, yi) higher than incorrect pairs
(xi, yj) with a margin of at least γ. The margin
γ is a hyper-parameter and the incorrect labels,
yj are selected randomly such that j ̸= i. k is
the number of incorrect examples per training in-
stance, and d(x, y) = (x−y)2 is the distance mea-
sure.

The joint neighborhood discovery and word
translation objective is given by:

J(θ) = L(θ) +R(θ) (6)

The neighborhood discovery part of the objec-
tive, R(θ), does not depend on availability of su-
pervised data and only requires monolingual data

on the source language side. Thus, we can dis-
cover neighborhoods in an unsupervised manner
on a large set of monolingual word embeddings,
then initialize using this pre-trained D which is
then jointly optimized with the translation part of
the objective L(θ). Importantly, this also means
that our method can work with unsupervised meth-
ods for learning mapping functions such as those
using adversarial training (Barone, 2016; Conneau
et al., 2018).

5 Experimental Evaluation

In this section, we study the following questions:
How does NORMA compare to state-of-the-art
methods for learning mapping functions between
embedding spaces of different languages? We
study this question in three settings: when trans-
lating between distant languages, when translat-
ing between related languages, and lastly, when
translating between related languages but on rare
words. Additionally, we ask the following ques-
tion: are the neighborhoods learned by NORMA
meaningful?

To study these questions, we carried out experi-
ments on word translation from English to two dis-
tant languages, a Slavic language (Russian), and
a Sino-Tibetan language (Chinese). In addition,
we carried out experiments on word translation be-
tween related languages (English, French, German
and Portuguese).

Data and Experimental Setup. The Facebook
AI MUSE3 project (Conneau et al., 2018) pro-
vides train/test data for bilingual dictionaries of
various language pairs, we use this data in our
experiments. The MUSE dictionaries consist of



517

Method
Slavic & Sino-Tibetan

en-ru en-zh en-de en-es en-fr

NORMA-Linear 50.33 43.27 68.50 77.47 76.10
NORMA-Highway-NN 49.27 33.10 67.33 77.65 75.50

1 layer-NN 49.13 30.66 66.80 77.60 75.53

2 layer-NN 0 0 0 0 0

1 layer-Highway-NN 49.50 30.91 67.00 77.50 75.60

2 layer-Highway-NN 0 0 0 0 0

Artetxe et al . 2018 47.93 20.4 70.13 79.6 79.30
Conneau et al. 2018 37.30 30.90 71.30 79.10 78.10
Smith et al. 2017 46.33 39.60 69.20 78.80 78.13

Xing et al. 2015 44.50 41.0 67.07 77.33 75.47

Lazaridou et al. 2015 48.27 29.60 68.20 77.60 75.86

Faruqui and Dyer (2014) 35.47 32.20 55.67 72.33 69.27

Mikolov et al. 2013 42.47 19.80 60.07 74.20 71.60

Table 1: Precision at 1 comparison of NORMA to previously proposed mapping functions. We used FAIR/MUSE
word translation lexicons train/test splits.

en-ru en-zh en-de en-es en-fr
NOUN 42% /55.1 42% /42.1 39% /74.6 40% /82.3 42% /80.0
VERB 41% /47.3 39% /47.6 38% /64.4 40% /71.6 41% /70.0
ADJECTIVE 10% /34.4 11% /38.7 10% /56.1 9% /76.9 10% /71.3

Table 2: Part-of-Speech (POS) distributions of the MUSE test sets. Listed are the top 3 parts of speech, which
account for ˜90% of the test data for all language pairs. X% /Y means the POS tag makes up X% of the test set,
with accuracy Y.

5,000/1,500 word pairs for train/test data. Unless
specified, we use the train/test split provided by
MUSE. Development sets: the MUSE dictionar-
ies that we used are very large. They contain over
100,000 entries for most language pairs, we tuned
our models on data that was not part of the train
and test sets.

We obtained pre-trained word embeddings from
FastText (Bojanowski et al., 2017). In Equation 2,
we did not find it helpful to encourage neighbor-
hoods to be different, thus we set λ = 0. We set
the margin γ in Equation 5 to be γ = 0.4. For the
dictionary of neighborhoods D in Equation 1, we
set the number of neighborhoods K = 2, 0006.
We use N = 50 batch size. We estimate model

6We carried out experiments using different neighborhood
sizes, and consistently found K ≈ 2000 to outperform other
choices.

parameters using stochastic gradient descent.

Methods Under Comparison. We compare
variations of NORMA to several previously pro-
posed methods for generating mapping functions.
The methods compared are: (Artetxe et al., 2018a;
Conneau et al., 2018; Smith et al., 2017; Xing
et al., 2015; Lazaridou et al., 2015; Faruqui and
Dyer, 2014; Mikolov et al., 2013a). More detailed
descriptions of these prior methods can be found
in the related work section.

Our primary goal is to evaluate the quality of
maps produced. While a number of prior work
proposed various approaches for retrieval, which
have been shown to improve accuracy by a few
points, we compare all methods using the same re-
trieval method, nearest neighbor. Thus, for (Con-
neau et al., 2018), we report the results for the vari-
ant of their method called: adv - Refine - NN.



518

5.1 English to Slavic and Sino Tibetan

State-of-the-art methods have mostly focused
word translation evaluation on English to Latin
languages or other nearby languages. (Artetxe
et al., 2018a) performed experiments on en-es, en-
de, en-it and en-fi, where concepts might still be
organized in a relatively similar way. In (Con-
neau et al., 2018), the adversarial training method
proposed was evaluated on Chinese, Russian, and
Esperanto, but thorough comparison experiments
to prior work on word translation were only per-
formed on English to Italian.

We carried out en-ru and en-zh comparison ex-
periments, and present the results in the second
and third columns of Table 1. The two state-
of-of-the art methods (Artetxe et al., 2018a) and
(Conneau et al., 2018) are significantly outper-
formed by NORMA-Linear. On English to Rus-
sian, NORMA-Linear achieves 50.33 precision 1,
outperforming both (Artetxe et al., 2018a) (Con-
neau et al., 2018), as well as other methods.
On English to Chinese, NORMA-Linear achieves
43.37 precision 1, again ahead of other meth-
ods. The best performing variant of our method is
NORMA-Linear. The neural networks with more
than a single layer prove difficult to optimize for
this problem, and produce accuracy of 0. This
could be because the problem of cross-embedding
space mapping is a zero-shot learning problem,
which is much more difficult to train than a super-
vised problem, the setting in which deep learning
methods have thrived so far.

5.2 English to Related Languages

We show experiments on English to related lan-
guages in the last three columns of Table 1. On
these languages, indeed the most recently pro-
posed methods (Artetxe et al., 2018a; Conneau
et al., 2018) produce the best performing maps.
However, NORMA-Linear is only 2-3 points be-
hind these methods. This in contrast to English
to Chinese where both (Artetxe et al., 2018a) and
(Conneau et al., 2018) are behind NORMA - Lin-
ear, by more than 10 points.

A promising line of future work is to get
NORMA-Linear to bridge the 2-3 point gap on
related languages by exploring a best of both
worlds approach, combining neighborhood sensi-
tivity with the methods that achieve superior per-
formance on nearby languages.

en-pt
RARE MUSE

NORMA-Linear 57.67 72.60
NORMA-Highway-NN 49.33 71.73
1 layer-NN 48.67 72.13
1 layer-Highway-NN 49.33 72.10
Artetxe et al . 2018 47.00 77.73
Lazaridou et al 2015 48.00 72.27

Table 3: Performance for en-pt on rare words (RARE),
and the en-pt MUSE dataset, which as shown in Figure
3 contains a lot of frequent words.

0 10000 20000 30000 40000 50000
Word Frequency

0

5

10

15

20

25

30

35

To
ta

l W
or

ds

en-pt: MUSE: Test

0 10000 20000 30000 40000 50000
Word Frequency

0
5

10
15
20
25
30
35
40

To
ta

l W
or

ds

en-pt: MUSE: Train

0 10000 20000 30000 40000 50000
Word Frequency

0

500

1000

1500

2000

To
ta

l W
or

ds

en-pt: RARE - Test

0 10000 20000 30000 40000 50000
Word Frequency

0
1000
2000
3000
4000
5000
6000
7000
8000

To
ta

l W
or

ds

en-pt: RARE:  Train

Figure 3: Top: Frequency distribution of MUSE dictio-
nary test and train sets for en-pt. Bottom: Frequency
distribution of the RARE words dataset.

5.3 Accuracy by Part-of-Speech

We assigned each word its majority part-of-speech
by tagging the ClueWeb7 corpus, which contains
over 500 million webpages. We then evaluated
translation precision of NORMA-Linear stratified
by part-of-speech. The results are shown in Ta-
ble 5 We found that, nouns and verbs make up
about 80 percent of the MUSE test dictionaries,
followed by adjectives (˜10%). We found that
while nouns and verbs make up a large chunk
of the test data, nouns are translated with much
higher accuracy than verbs, except for English to
Chinese. This finding will serve as a guide for fu-
ture improvements to our method.

5.4 English to Languages: Rare Words

We analyzed the frequency distribution of the
MUSE dictionaries. To get word frequency infor-

7https://www.lemurproject.org/
clueweb09.php/

https://www.lemurproject.org/clueweb09.php/
https://www.lemurproject.org/clueweb09.php/


519

Neighborhood
51 134 162 7
drugs criminally chuanyao khoisan
zonisamide judicature chuanyan bantu
cocaine prosecutory zhiang sepedi
ritalin derogation thanong otjiherero
hospitalized restitutionary qiangbing ndebeles
pheniprazine derogative pengpeng hereros
overdose jailable nguyan otjinene
disorientation extradition yuning shona
focusyn sodomy liheng hutu
alfaxalone crimes thanong witotoan

Table 4: Sample neighborhoods discovered by NORMA during en-de translation: 51 appears to represent drugs,
132: justice and crime; 162: Asian names, 7 : African names.

mation, we processed documents in the ClueWeb7

corpus and recorded word occurrence frequency.
We discovered that the MUSE dictionaries contain
a lot of frequent words. The top half of Figure 3
shows frequency counts of the en-pt MUSE test
dictionary. For readability we only show bins up
to occurrence frequency of 50,000. We see that
only about 50/1500 in the MUSE en-pt test data
are infrequent, the rest are frequent words, occur-
ring more than 10,000 times in the ClueWeb cor-
pus.

We therefore created another test set for en-pt
from the rest of the MUSE data which is not part
of the train or test data, with the goal of creating a
train/test of rare words. The bottom half of Figure
3 is a plot of frequency counts of train and test data
for these rare words.

We then compared variations of NORMA to
the best performing method on English to related
languages, which is (Artetxe et al., 2018a). The
comparison was done both on the regular MUSE
test dataset for en-pt and the rare word dataset for
en-pt. Since our method uses a max-margin loss
much like (Lazaridou et al., 2015), we also com-
pare to (Lazaridou et al., 2015).

Table 3 shows that NORMA-Linear outper-
forms (Artetxe et al., 2018a) by over 10 points on
the RARE words dataset. On the regular MUSE
dictionary, (Artetxe et al., 2018a) is ahead by
about 5 points. On RARE, (Lazaridou et al., 2015)
is behind NORMA-Linear by 9 points, whereas
on the MUSE dictionary performance of (Lazari-
dou et al., 2015) and NORMA-Linear is about the
same.

5.5 Neighborhood Interpretability

NORMA jointly discovers neighborhoods while
learning to translate words. We now ask if the dis-
covered neighborhoods semantically make sense.
We can answer this question since each neighbor-
hood vector can be seen as a “center” vector rep-
resenting the words in the neighborhood. Thus
we can consider words whose cosine similarity
to the neighborhood vector is greater than some
threshold, to be members of that neighborhood.
As we mentioned, we found that setting the to-
tal number of neighborhoods to be discovered to
K = 2, 000 provided the best results. Of these
2,000 we show some of them in Table 4 obtained
when training en − de. For each neighborhood,
we show 10 words that appear among the top 100
words of that neighborhood. It can be seen that the
neighborhoods represent some kind of “topics”.
For example, neighborhood number 51 appears to
represent drugs, and drug-related concepts; num-
ber 132 contains justice and crime-related con-
cepts; number 162 contains mostly Asian concepts
and names, number 7 contains mostly African and
names. We can see that the granularity of neigh-
borhoods and their specificity varies.

6 Conclusions

We propose neighborhood sensitive maps
for learning multilingual word embeddings,
NORMA. Our method is motivated by the fact
that languages differ along dimensions such as
vocabulary, grammar, written form, and syntax,
and therefore one would expect that embedding
spaces of different languages exhibit differ-
ent structures especially for distant languages.



520

Our method jointly discovers neighborhoods
while learning to translate words. Experimental
evaluation showed that NORMA substantially
outperforms state-of-the-art (SOTA) methods on
distant languages, while only being a few points
behind on related languages. A promising line of
future work is to explore a best of both worlds ap-
proach, combining neighborhood sensitivity with
the methods that achieve superior performance on
nearby languages.

Acknowledgments

We thank Raphael Flauger for useful discussions,
and the anonymous reviewers for their construc-
tive comments.

References
Hanan Aldarmaki, Mahesh Mohan, and Mona Diab.

2018. Unsupervised word mapping using structural
similarities in monolingual embeddings. Transac-
tions of the Association of Computational Linguis-
tics, 6:185–196.

Waleed Ammar, George Mulcaire, Yulia Tsvetkov,
Guillaume Lample, Chris Dyer, and Noah A. Smith.
2016. Massively multilingual word embeddings.
CoRR, abs/1602.01925.

Mikel Artetxe, Gorka Labaka, and Eneko Agirre. 2016.
Learning principled bilingual mappings of word em-
beddings while preserving monolingual invariance.
In Proceedings of the 2016 Conference on Empiri-
cal Methods in Natural Language Processing, pages
2289–2294.

Mikel Artetxe, Gorka Labaka, and Eneko Agirre. 2017.
Learning bilingual word embeddings with (almost)
no bilingual data. In Proceedings of the 55th Annual
Meeting of the Association for Computational Lin-
guistics (Volume 1: Long Papers), volume 1, pages
451–462.

Mikel Artetxe, Gorka Labaka, and Eneko Agirre.
2018a. Generalizing and improving bilingual word
embedding mappings with a multi-step framework
of linear transformations. In AAAI.

Mikel Artetxe, Gorka Labaka, Eneko Agirre, and
Kyunghyun Cho. 2018b. Unsupervised neural ma-
chine translation. In ICLR.

Antonio Valerio Miceli Barone. 2016. Towards
crosslingual distributed representations without par-
allel text trained with adversarial autoencoders. In
1st Workshop on Representation Learning for NLP.

Betty Birner. 1999. Does the language i speak influ-
ence the way i think?.

Phil Blunsom and Karl Moritz Hermann. 2014. Mul-
tilingual distributed representations without word
alignment. In ICLR.

Piotr Bojanowski, Edouard Grave, Armand Joulin, and
Tomas Mikolov. 2017. Enriching word vectors with
subword information. TACL.

Lera Boroditsky. 2011. How language shapes thought.
Scientific American, 304(2):62–65.

A. P. Sarath Chandar, Stanislas Lauly, Hugo
Larochelle, Mitesh M. Khapra, Balaraman Ravin-
dran, Vikas C. Raykar, and Amrita Saha. 2014. An
autoencoder approach to learning bilingual word
representations. In NIPS, pages 1853–1861.

Barry R Chiswick and Paul W Miller. 2005. Linguistic
distance: A quantitative measure of the distance be-
tween english and other languages. Journal of Mul-
tilingual and Multicultural Development, 26(1):1–
11.

Alexis Conneau, Guillaume Lample, Marc’Aurelio
Ranzato, Ludovic Denoyer, and Hervé Jégou. 2018.
Word translation without parallel data.

Georgiana Dinu, Angeliki Lazaridou, and Marco Ba-
roni. 2014. Improving zero-shot learning by
mitigating the hubness problem. arXiv preprint
arXiv:1412.6568.

Manaal Faruqui and Chris Dyer. 2014. Improving vec-
tor space word representations using multilingual
correlation. In EACL, pages 462–471.

Stephan Gouws, Yoshua Bengio, and Greg Corrado.
2015. Bilbowa: Fast bilingual distributed represen-
tations without word alignments. In ICML, pages
748–756.

Stephan Gouws and Anders Søgaard. 2015. Sim-
ple task-specific bilingual word embeddings. In
NAACL, pages 1386–1390.

Jiang Guo, Wanxiang Che, David Yarowsky, Haifeng
Wang, and Ting Liu. 2015. Cross-lingual depen-
dency parsing based on distributed representations.
In ACL, pages 1234–1244.

Aria Haghighi, Percy Liang, Taylor Berg-Kirkpatrick,
and Dan Klein. 2008. Learning bilingual lexicons
from monolingual corpora. In ACL, pages 771–779.

Alexandre Klementiev, Ivan Titov, and Binod Bhat-
tarai. 2012. Inducing crosslingual distributed repre-
sentations of words. In COLING, pages 1459–1474.

Tomáš Kočiskỳ, Karl Moritz Hermann, and Phil Blun-
som. 2014. Learning bilingual word representa-
tions by marginalizing alignments. arXiv preprint
arXiv:1405.0947.

Philipp Koehn and Kevin Knight. 2002. Learning a
translation lexicon from monolingual corpora. In
ACL Workshop on Unsupervised Lexical Acquisi-
tion.



521

Guillaume Lample, Ludovic Denoyer, and
Marc’Aurelio Ranzato. 2018. Unsupervised
machine translation using monolingual corpora
only. In ICLR.

Angeliki Lazaridou, Georgiana Dinu, and Marco Ba-
roni. 2015. Hubness and pollution: Delving into
cross-space mapping for zero-shot learning. In ACL,
pages 270–280.

Honglak Lee, Alexis Battle, Rajat Raina, and An-
drew Y Ng. 2007. Efficient sparse coding algo-
rithms. In NIPS, pages 801–808.

Jinyu Li, Rui Zhao, Jui-Ting Huang, and Yifan
Gong. 2014. Learning small-size dnn with output-
distribution-based criteria. In INTERSPEECH,
pages 1910–1914.

Ang Lu, Weiran Wang, Mohit Bansal, Kevin Gimpel,
and Karen Livescu. 2015. Deep multilingual cor-
relation for improved word embeddings. In Pro-
ceedings of the 2015 Conference of the North Amer-
ican Chapter of the Association for Computational
Linguistics: Human Language Technologies, pages
250–256.

Tomas Mikolov, Quoc V. Le, and Ilya Sutskever. 2013a.
Exploiting similarities among languages for ma-
chine translation. CoRR, abs/1309.4168.

Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Cor-
rado, and Jeff Dean. 2013b. Distributed representa-
tions of words and phrases and their compositional-
ity. In NIPS, pages 3111–3119.

Tom M Mitchell, Svetlana V Shinkareva, Andrew Carl-
son, Kai-Min Chang, Vicente L Malave, Robert A
Mason, and Marcel Adam Just. 2008. Predicting
human brain activity associated with the meanings
of nouns. science, 320(5880):1191–1195.

Ndapa Nakashole and Raphael Flauger. 2018. Char-
acterizing departures from linearity in word transla-
tion. In ACL.

Ndapandula Nakashole and Raphael Flauger. 2017.
Knowledge distillation for bilingual dictionary in-
duction. In EMNLP, pages 2487–2496.

Jeffrey Pennington, Richard Socher, and Christopher
Manning. 2014. Glove: Global vectors for word
representation. In EMNLP, pages 1532–1543.

Reinhard Rapp. 1999. Automatic identification of
word translations from unrelated english and german
corpora. In ACL.

Sam T Roweis and Lawrence K Saul. 2000. Nonlin-
ear dimensionality reduction by locally linear em-
bedding. science, 290(5500):2323–2326.

Tianze Shi, Zhiyuan Liu, Yang Liu, and Maosong Sun.
2015. Learning cross-lingual word embeddings via
matrix co-factorization. In Proceedings of the 53rd

Annual Meeting of the Association for Computa-
tional Linguistics and the 7th International Joint
Conference on Natural Language Processing (Vol-
ume 2: Short Papers), volume 2, pages 567–572.

Samuel L Smith, David HP Turban, Steven Hamblin,
and Nils Y Hammerla. 2017. Offline bilingual word
vectors, orthogonal transformations and the inverted
softmax. In ICLR.

Anders Søgaard, Zeljko Agic, Héctor Martı́nez Alonso,
Barbara Plank, Bernd Bohnet, and Anders Jo-
hannsen. 2015. Inverted indexing for cross-lingual
NLP. In ACL, pages 1713–1722.

Anders Søgaard, Sebastian Ruder, and Ivan Vulić.
2018. On the limitations of unsupervised bilingual
dictionary induction.

Rupesh Kumar Srivastava, Klaus Greff, and Jürgen
Schmidhuber. 2015. Highway networks. arXiv
preprint arXiv:1505.00387.

Oscar Täckström, Ryan T. McDonald, and Jakob
Uszkoreit. 2012. Cross-lingual word clusters for di-
rect transfer of linguistic structure. In NAACL, pages
477–487.

Peter D. Turney and Patrick Pantel. 2010. From fre-
quency to meaning: Vector space models of seman-
tics. J. Artif. Intell. Res. (JAIR), 37:141–188.

Shyam Upadhyay, Manaal Faruqui, Chris Dyer, and
Dan Roth. 2016. Cross-lingual models of word em-
beddings: An empirical comparison. In ACL.

Ivan Vulic and Anna Korhonen. 2016. On the role
of seed lexicons in learning bilingual word embed-
dings. ACL.

Ivan Vulic and Marie-Francine Moens. 2015. Bilin-
gual word embeddings from non-parallel document-
aligned data applied to bilingual lexicon induction.
In ACL, pages 719–725.

Derry Tanti Wijaya, Brendan Callahan, John Hewitt,
Jie Gao, Xiao Ling, Marianna Apidianaki, and Chris
Callison-Burch. 2017. Learning translations via ma-
trix completion. In EMNLP, pages 1452–1463.

Chao Xing, Dong Wang, Chao Liu, and Yiye Lin. 2015.
Normalized word embedding and orthogonal trans-
form for bilingual word translation. In HLT-NAACL,
pages 1006–1011.

Meng Zhang, Yang Liu, Huanbo Luan, and Maosong
Sun. 2017. Adversarial training for unsupervised
bilingual lexicon induction. In Proceedings of the
55th Annual Meeting of the Association for Compu-
tational Linguistics (Volume 1: Long Papers), vol-
ume 1, pages 1959–1970.

Kai Zhao, Hany Hassan, and Michael Auli. 2015.
Learning translation models from monolingual con-
tinuous representations. In NAACL, pages 1527–
1536.



522

Will Y Zou, Richard Socher, Daniel Cer, and Christo-
pher D Manning. 2013. Bilingual word embeddings
for phrase-based machine translation. In EMNLP,
pages 1393–1398.


