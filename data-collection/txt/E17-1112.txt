



















































Analyzing Semantic Change in Japanese Loanwords


Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 1, Long Papers, pages 1195–1204,
Valencia, Spain, April 3-7, 2017. c©2017 Association for Computational Linguistics

Analyzing Semantic Changes in Japanese Loanwords

Hiroya Takamura
Tokyo Institute of Technology

takamura@pi.titech.ac.jp

Ryo Nagata
Konan University

nagata-acl@hyogo-u.ac.jp

Yoshifumi Kawasaki
Sophia University

kyossii@gmail.com

Abstract

We analyze semantic changes in loan-
words from English that are used in
Japanese (Japanese loanwords). Specifi-
cally, we create word embeddings of En-
glish and Japanese and map the Japanese
embeddings into the English space so that
we can calculate the similarity of each
Japanese word and each English word. We
then attempt to find loanwords that are
semantically different from their original,
see if known meaning changes are cor-
rectly captured, and show the possibility
of using our methodology in language ed-
ucation.

1 Introduction

We often come across advertisements that have ex-
travagant images. In Japan, such images are usu-
ally accompanied by the following sentence1:

This sentence sounds like a nonsense tautology,
but actually means this image is only for illustra-
tive purposes and may differ from the actual prod-
uct. Both gazō and imēji are Japanese words, each
meaning image. However, the latter is a loanword
from English, i.e., image2. In the sentence above,
imēji, the loanword for image, is closer in mean-
ing to the word impression, and it makes the sen-
tence roughly mean this image is just an impres-

1TOP and COP respectively mean a topic marker and a
copula in interlinear glossed text (IGT) representation. The
last line is a literal translation of the Japanese sentence.

2Note that although gazō is also from ancient Chinese, we
focus on loanwords from English, which are usually written
in katakana letters in Japanese.

sion that you might have on this product. What
happens in this seeming tautology is that the loan-
word changes meaning; i.e., the sense of the loan-
word deviates from the sense of its original word.

Loanwords from English occupy an important
place in the Japanese language. It is reported that
approximately 8% of the vocabulary of contem-
porary Japanese consists of loanwords from En-
glish (Barrs, 2013). One noteworthy characteris-
tics of loanwords in Japanese is that their mean-
ings are often different from their original words,
as in the above example.Indeed, the meanings of
loanwords in any language are not generally the
same as those in the language, but according to
Kay (1995), Japanese has particularly a strong ten-
dency of changing the meanings of loanwords;
Kay argued that in Japan there is no deep cul-
tural motivation to protect their original meaning.
Daulton (2009) also argued that Japanese loan-
words are malleable in terms of meanings. Thus,
Japanese loanwords would be an interesting sub-
ject to work on in the study of meaning change.

Japanese loanwords from English are also im-
portant in language education (Barrs, 2013).
Japanese learners of English often make mistakes
in using English words that have corresponding
loanwords in Japanese but with very different
meanings. By contrast, learners are able to make
better use of a loanword in conversation if they
know that its meaning is the same as that of the
original. It is thus important to know which loan-
words are semantically different from their origi-
nal and which are not.

With this background in mind, we work on
Japanese loanwords derived from English. Since
the word embedding vectors (or simply, embed-
dings), which have become very popular recently,
are powerful tools for dealing with word mean-
ings, we use them to analyze Japanese loanwords.
Specifically, we create word embeddings of En-

1195



glish and Japanese, and map the Japanese embed-
dings into the English space so that we can cal-
culate the similarity of each Japanese word and
each English word. We then attempt to find loan-
words that are semantically different from their
original, see if known meaning changes are cor-
rectly captured, and show the possibility of using
our methodology in language education.

In this paper, we use the term semantic change
or meaning change in a broad sense. Some loan-
words are semantically different from the origi-
nal words because the loanwords or the original
words semantically changed after they were intro-
duced into Japanese or because only one of the
multiple senses of the original words were intro-
duced. Moreover, some loanwords did not come
directly from English, but from words in other lan-
guages, which later became English words. Thus,
in this paper, the terms semantic change or mean-
ing change cover all of these semantic differences.

2 Related Work

Japanese loanwords have attracted much interest
from researchers. Many interesting aspects of
Japanese loanwords are summarized in a book
written by Irwin (2011). In the field of natu-
ral language processing, there have been a num-
ber of efforts to capture the behavior of Japanese
loanwords including the phonology (Blair and In-
gram, 1998; Mao and Hulden, 2016) and seg-
mentation of multi-word loanwords (Breen et al.,
2012). The rest of this section explains the compu-
tational approaches to semantic changes or varia-
tions of words. In particular, there are mainly two
different phenomena, namely diachronic change
and geographical variation.

Jatowt and Duh (2014) used conventional
distributional representations of words, i.e.,
bag-of-context-words, calculated from Google
Book (Michel et al., 2011)3 to analyze the di-
achronic meaning changes of words. They also
attempted to capture the change in sentiment of
words across time. Kulkarni et al. (2014) used dis-
tributed representations of words (or word embed-
dings), instead of the bag-of-context-words used
by Jatowt and Duh, to capture meaning changes
of words and in addition used the change point
detection technique to find the point on the time-
line where the meaning change occurred. Hamil-

3https://books.google.com/ngrams/
datasets

ton et al. (2016) also used distributed representa-
tions for the same purpose and attempted to re-
veal the statistical laws of meaning change. They
compared the following three methods for creat-
ing word embedding: positive pointwise mutual
information (PPMI), low-dimensional approxima-
tion of PPMI obtained through singular value de-
composition, and skip-gram with negative sam-
pling. They suggested that the skip-gram with
negative sampling is a reasonable choice for study-
ing meaning changes of words. We decided to fol-
low their work and use the skip-gram with nega-
tive sampling to create word embeddings.

Bamman et al. (2014) used a similar technique
to study differences in word meanings ascribed to
geographical factors. They succeeded in correctly
recognizing some dialects of English within the
United States. Kulkarni et al. (2016) also worked
on geographic variations in languages.

With some modification, the methods used in
the literature (Kulkarni et al., 2014; Hamilton et
al., 2016) can be applied to loanword analysis.

3 Methodology

We use word embeddings to analyze the seman-
tic changes in Japanese loanwords from the corre-
sponding English. Among the methods of anal-
ysis, we chose to use the skip-gram with nega-
tive sampling for the reason discussed in Section 2
with reference to Hamilton et al.’s work (2016).

First, we create word embeddings for two lan-
guages. We then calculate the similarity or dis-
similarity between the embedding (or vector) of a
word in a language (say, Japanese) and the embed-
ding of a word in another language (say, English).
For this purpose, words in the two languages need
to be represented in the same vector space with the
same coordinates. There are a number of meth-
ods for this purpose (Gouws et al., 2015; Zou et
al., 2013; Faruqui and Dyer, 2014; Mikolov et al.,
2013a). Among them, we choose the simplest and
most computationally efficient one proposed by
Mikolov et al. (2013a), where it is assumed that
embeddings in one language can be mapped into
the vector space of another language by means of
a linear transformation represented by W . Sup-
pose we are given trained word embeddings of the
two languages and a set of seed pairs of embed-
ding vectors {(xi, zi)|1 ≤ i ≤ n}, each of which
is a pair of a vector in one language and a vector
in the other language that are translation equiva-

1196



lents of each other. The transformation matrix W
is obtained by solving the following minimization
problem :

min
W

n∑
i=1

||Wxi − zi||2, (1)

where, in our case, xi is the embedding of a
Japanese seed word and zi is the embedding of its
English counterpart. Thus, the Japanese word em-
beddings are mapped into the English vector space
so that the embeddings of the words in each seed
pair should be as close to each other as possible.
Although Hamilton et al. (2016) preserved cosine
similarities between embedding vectors by adding
the orthogonality constraint (i.e., W T W = I ,
where I is the identity matrix) when they aligned
English word embeddings of different time pe-
riods, we do not adopt this constraint for two
reasons. The first reason is that since we need
an inter-language mapping instead of across-time
mappings of the same language, the orthogonality
constraint would degrade the quality of the map-
ping; the two spaces might be so different that
even the best rotation represented by an orthogo-
nal matrix would leave much error between corre-
sponding words. The second reason is that we do
not need to preserve cosine similarities between
words in mapping embedding vectors, because we
do not use the cosine similarities between mapped
embedding vectors of Japanese words.

After mapping the Japanese word embeddings
to the English vector space, we calculate the co-
sine similarity between each Japanese loanword
and its original English word. If the cosine sim-
ilarity is low for a pair of words, the meaning of
the Japanese loanword is different from that of its
original English word.

4 Empirical Evaluations

Since it is generally difficult to evaluate methods
for capturing semantic changes in words, we con-
duct a number of quantitative and qualitative eval-
uations from different viewpoints.

4.1 Data and Experimental Settings
The word embeddings of English and Japanese
were obtained via the skip-gram with negative
sampling (Mikolov et al., 2013b)4 with differ-
ent dimensions as shown in the result. The data

4https://code.google.com/archive/p/
word2vec/ with options “-window 5 -sample 1e-4
-negative 5 -hs 0 -cbow 0 -iter 3”

used for this calculation was taken from Wikipedia
dumps5 as of June 2016 for each language; the
text was extracted by using wp2txt (Hasebe,
2006)6, non-alphabetical symbols were removed,
and noisy lines such as the ones corresponding
to the infobox were filtered out7. We performed
word segmentation on the Japanese Wikipedia
data by using the Japanese morphological ana-
lyzer MeCab (Kudo et al., 2004)8 with the neolo-
gism dictionary, NEologd9, so that named entities
would be recognized correctly.

The list of Japanese loanwords was obtained
from Wiktionary10. Only one-word entries were
used and some errors were corrected, resulting in
1,347 loanwords from English11.

We extracted seed word pairs from an English-
Japanese dictionary, edict (Breen, 2000)12; these
were used in the minimization problem expressed
by Equation (1). Specifically, we extracted one-
word English entries that were represented as a
single Japanese word. We then excluded the 1,347
loanwords obtained above from the word pairs,
which resulted in 41,366 seed word pairs.

4.2 Evaluation through Correlation
To see if the differences in word embeddings
are related to the meaning changes of loanwords,
we calculate an evaluation measure indicating the
global trend. We first extracted one-to-one trans-
lation sentence pairs from Japanese-English News
Article Alignment Data (JENAAD) (Utiyama and
Isahara, 2003). We then use this set of sentence
pairs to calculate the Dice coefficient for each pair
of a loanword wjpn and its original English word
weng, which is defined as

2× P (wjpn, weng)
P (wjpn) + P (weng)

, (2)

5https://dumps.wikimedia.org/enwiki/
https://dumps.wikimedia.org/jawiki/

6https://github.com/yohasebe/wp2txt
7https://en.wikipedia.org/wiki/Help:

Infobox
8http://taku910.github.io/mecab/
9https://github.com/neologd/

mecab-ipadic-neologd
10https://ja.wiktionary.org/wiki/%E3%82

%AB%E3%83%86%E3%82%B4%E3%83%AA%3A%E6%97%
A5%E6%9C%AC%E8%AA%9E %E5%A4%96%E6%9D%A5%E
8%AA%9E

11Some of these loanwords may have been introduced into
Japanese via other languages. However, in this paper, we
regard them as from English as long as they are also used in
English.

12http://www.edrdg.org/jmdict/edict.
html

1197



dimension correlation coefficient
dimjpn dimeng Pearson Spearman

100 100 0.363 0.443
200 100 0.386 0.471
200 200 0.402 0.474
400 200 0.404 0.487
300 300 0.422 0.492
600 300 0.432 0.506

Table 1: Correlation coefficients between the
Dice coefficient and the cosine similarity. dimjpn
and dimeng are respectively the dimensions of
the Japanese and English word embeddings; i.e.,
the dimjpn-dimensional space is mapped to the
dimeng-dimensional space. All the coefficients are
statistically significant (significance level 0.01).

where P (wjpn, weng) is the probability that this
word pair appears in the same sentence pair, and
P (wjpn) and P (weng) are the generative probabil-
ities of wjpn and weng. All the probabilities were
obtained using the maximum likelihood estima-
tion. The Dice coefficient is a measure of coocur-
rence and can be used to extract translate equiv-
alents (Smadja et al., 1996). If the Dice coeffi-
cient of a word pair is low, the words in the pair
are unlikely to be translation equivalents of each
other. Therefore, if the meaning of a loanword has
changed from the original English word, its Dice
coefficient should be low. In other words, the co-
sine similarity should be correlated to the Dice co-
efficient if the cosine similarity is a good indicator
of meaning change. We thus calculate the Pear-
son’s correlation coefficient between the two. In
addition, we calculate the Spearman’s rank-order
correlation coefficient to examine the relation of
the orders given by the Dice coefficient and the
cosine similarity.

Note that although we use a parallel corpus for
evaluation, it does not mean that we can simply
use a parallel corpus for finding meaning changes
in loanwords. Parallel corpora are usually much
smaller than monolingual corpora and can cover
only a small portion of the entire set of loanwords.
With the model described in Section 3, we will be
able to find meaning changes in loanwords that do
not appear in a parallel corpus.

The results for different Japanese and English
dimensions, dimjpn and dimeng, are shown in
Table 1. Pearson’s correlation coefficients sug-
gest that the cosine similarity is moderately cor-

 0

 0.2

 0.4

 0.6

 0.8

 1

 0  0.2  0.4  0.6  0.8  1

C
o
si

n
e
 S

im
ila

ri
ty

Dice Coefficient

Figure 1: Dice coefficient vs. cosine similarity.
Dice coefficients are extracted from a parallel cor-
pus. Cosine similarities are for the embedding
vectors of the Japanese loanwords and their En-
glish counterparts. The line in the figure is ob-
tained by linear regression.

related with the Dice coefficient except for the
case dimeng=100, which shows weak correla-
tion. Spearman’s rank-order correlation coeffi-
cients also suggest that these two are moderately
correlated with each other. The result depends on
the dimensions of the word embeddings. Basi-
cally, larger dimensions tend to have higher cor-
relation coefficients. In addition, when the di-
mension is decreased (e.g., dimjpn = 600 to
dimeng = 300), the correlation coefficients tend
to be higher, compared with the case where the
dimension remains the same (e.g., dimjpn = 300
to dimeng = 300). This result is consistent with
the report by Mikolov et al. (2013a) that the word
vectors trained on the source language should be
several times (around 2x-4x) larger than the word
vectors trained on the target language.

To examine the relation between the Dice coef-
ficient and the cosine similarity in more detail, we
plot these values for the bottom row in Table 1,
i.e., where the dimensions for Japanese number
600 and the dimensions for English number 300.
The scatter plot that we obtained is shown in Fig-
ure 1. The line obtained by linear regression is
also drawn in the figure.

4.3 Detailed Evaluation on Known Change

Here, we conduct a detailed evaluation on mean-
ing changes that are already known. We se-
lected the ten Japanese loanwords shown in Ta-

1198



cos(weng, wa) cos(wjpn, wb)
weng wjpn wa wb − cos(wjpn, wa) − cos(weng, wb)

image imēji photo impression 0.097 0.274
corner cōnā crossroad section 0.099 0.115
digest daijesuto dissolve summary 0.047 0.291
bug bagu insect glitch 0.092 0.200
idol aidoru deity popstar 0.127 0.086
icon aikon deity illustration −0.035 0.145

cunning kanningu shrewd cheating 0.259 0.273
pension penshon annuity hotel 0.368 0.445
nature neichā characteristics magazine 0.106 0.202
driver doraibā chauffeur screwdriver −0.063 0.158

Table 2: Differences in cosine similarity. The Japanese loanword from corner can mean a small section in
a larger building or space. The Japanese loanword from bug usually means a bug in a computer program.
The Japanese loanword from cunning usually means cheating on an exam. The Japanese loanword from
nature is often used to indicate the scientific journal Nature. The Japanese loanword from driver can
mean both a vehicle driver and a screwdriver (the latter meaning was not one of the original word).

ble 2 that are supposed to have different mean-
ings compared with the original English words.
Some of these words were taken from a book
about loanwords written by Kojima (1988). The
others were collected by the authors. We also
added two pivot words wa and wb for each word13.
For the first nine words, the meaning of pivot
word wa is supposed to be closer to the English
word weng than to the Japanese loanword wjpn,
and the meaning of the pivot word wb is sup-
posed to be closer to the Japanese loanword wjpn
than to the English word weng. It is thus ex-
pected that cos(weng, wa) − cos(wjpn, wa) > 0
and cos(wjpn, wb)− cos(weng, wb) > 0 hold true.
The last Japanese loanword in Table 2 is used as
both pivot words wa and wb, but the original En-
glish word is not used as wb. It is thus expected
that cos(wjpn, wb)−cos(weng, wb) > 0 holds true,
but cos(weng, wa)− cos(wjpn, wa) > 0 might not
be necessarily true. The differences in cosine sim-
ilarities are shown in Table 2. As expected, al-
most all the differences are positive, which sug-
gests that the difference of the word embeddings
captures the meaning change. However, there was
one exception:

cos(icon, deity)− cos(iconjpn, deity) = −0.035.
13Pivot words are not necessarily synonyms of the corre-

sponding English words. They are the words that we think
are useful for capturing how the meanings of the loanwords
and the original English are different. We also made sure that
pivot words themselves are unambiguous.

The cosine similarity between icon and deity was
0.266, which is smaller than expected. We ran-
domly sampled 100 lines containing icon from En-
glish Wikipedia text, which we used for calculat-
ing word embeddings, and found that the domi-
nant sense of icon in Wikipedia is not a religious
painting or figure, but a representative person or
thing’ as in the Wikipedia page of a football super-
star David Beckham14 :

Beckham became known as a fashion
icon, and together with Victoria, the
couple became · · ·

Thus, the reason of icon’s anomalous behavior is
that the distribution over senses in Wikipedia was
a lot different from the expected one.

4.4 Nearest Neighbors

We show in Table 3 the English nearest neighbors
of the English word weng and the Japanese loan-
word wjpn in the 300-dimensional space of En-
glish. Japanese loanwords are mapped from the
600-dimensional space of Japanese into the 300-
dimensional space of English. The English word
image is close in meaning to the word picture, as
suggested by jpeg and close-up, while its loanword
seems to have a more abstract meaning such as
idealizing. The nearest neighbors of the English
word digest are influenced by an American fam-

14https://en.wikipedia.org/wiki/David_
Beckham

1199



weng nearest neighbor of weng nearest neighbors of wjpn
image file 0.774 idealizing 0.671

jpeg 0.748 stylization 0.665
jpg 0.724 inescapably 0.665

closeup 0.694 evoking 0.664
close-up 0.658 englishness 0.664

corner corners 0.727 recapped 0.666
tiltons 0.646 cliff-hanger 0.644

goerkes 0.643 “blank” 0.642
uphams 0.629 announcer’s 0.641

intersection’s 0.627 sports-themed 0.632
digest digests 0.609 recaps 0.717

digest’s 0.594 wrap-up 0.697
reader’s 0.591 recapped 0.695

wallace-reader’s 0.573 preview 0.693
wallace/reader’s 0.556 recap 0.690

bug bugs 0.672 heartbleed 0.714
leaf-footed 0.605 workaround 0.695
motherhead 0.590 workarounds 0.686

harpactorinae 0.582 glitches 0.684
thread-legged 0.579 copy-on-write 0.684

icon icons 0.750 swoosh 0.701
iverskaya 0.580 viewport 0.694
nicopeia 0.579 crosshair 0.691
eleusa 0.570 upper-left 0.684

derzhavnaya 0.569 wireframe 0.680
nature teiči 0.649 phytogeography 0.684

søraust-svalbard 0.643 ethological 0.679
naturans 0.627 life-history 0.676
naturata 0.623 paleoclimatology 0.671
naturing 0.623 archaeoastronomy 0.670

driver drivers 0.837 driver 0.762
driver’s 0.703 race-car 0.689

car 0.685 mechanic 0.649
co-drivers 0.655 harvick’s 0.645

owner-driver 0.653 andretti’s 0.642

Table 3: English words that are nearest weng and wjpn. wjpn is a Japanese loanword and weng is the
original English word. wjpn is mapped into the English vector space. Only words that appear more than
100 times in the Wikipedia corpus are considered as candidates of the nearest neighbors. The value next
to each word is the cosine similarity between the nearest neighbor word and weng or wjpn.

1200



ily magazine Reader’s Digest15 by Wallaces, but
the terms related to summary do not appear in the
top-5 list, except for digest itself. In contrast, its
loanword seems to mean wrap-up. We now re-
turn to the English word icon that was mentioned
as an exception in Section 4.3. Besides icons, the
nearest neighbors of icon are iverskaya, nicopeia,
eleusa, and derzhavnaya. These four words are all
related to religious paintings or figures, but they
have low cosine similarities. The other parts of
the table are also mostly interpretable. The nearest
neighbors of weng nature look uninterpretable at a
first glance, but they are influenced by the Søraust-
Svalbard Nature Reserve in Norway, and Natura
naturans, which is a term associated with the phi-
losophy of Baruch Spinoza.

4.5 Ranking of Word Pairs According to
Similarity

Here, we investigate the possibility of whether the
similarity calculated in the mapped space can be
used to detect the loanwords that are very differ-
ent from or close to the original English words.
We show the 20 words with the lowest cosine sim-
ilarities and the 20 words with highest cosine sim-
ilarities in Table 4. First, let us take a look at the
words on the right, which have high similarities.
Most of them are technical terms (e.g., hexade-
cane and propylene), and domain-specific terms
such as musical instruments (e.g., piano and vi-
olin) and computer-related terms (computer and
software). This result is consistent with the fact
that the meanings of technical terms tend not to
change, at least for Japanese (Nishiyama, 1995).
Next, let us take a look at the words on the left,
which have low similarities. Many of them are ac-
tually ambiguous, and this ambiguity is often due
to the Japanese phonetic system. For example,
lighter and writer are assigned to the same loan-
word in Japanese, because the Japanese language
does not distinguish the consonants l and r. The
words clause, close and clothe are also assigned
to the same loanword also because of the Japanese
phonetic system. Other words are used as parts of
named entities, also resulting in low similarity. For
example, the Japanese loanword for refer is more
often used as Rifaa, the name of a city in Bahrain,
but hardly as refer. The loanword for irregular is
often used as part of a video game title Irregular
Hunter. However, we can also find words with sig-

15http://rd.com

dissimilar pair similar pair
weng cosine weng cosine
lac 0.225 piano 0.886

refer 0.245 violin 0.881
police 0.247 cello 0.881
spread 0.251 hexadecane 0.864
mof 0.261 propylene 0.857
pond 0.270 keyboard 0.855
inn 0.274 clarinet 0.851
ism 0.279 cheese 0.849

lighter 0.280 mayonnaise 0.848
root 0.281 software 0.847
tabu 0.284 methanol 0.843
gnu 0.293 hotel 0.843

thyme 0.296 chocolate 0.841
clause 0.310 computer 0.840
board 0.315 engine 0.840

present 0.319 globalization 0.835
coordinate 0.337 tomato 0.833
expanded 0.341 trombone 0.832
irregular 0.342 recipe 0.831
measure 0.346 antimony 0.829

Table 4: Twenty words with the lowest similarities
and twenty words with the highest similarities.

nificant changes in meaning, such as present16 and
coordinate17. Therefore, the result suggests that
the similarity calculated by our method has the ca-
pability of detecting changes in the meanings of
loanwords, but we need to filter out the words that
are ambiguous in the Japanese phonetic system.

We manually evaluated the 100 words that have
the lowest similarities to the corresponding loan-
words including the 20 words shown in Table 4.
Among the 100 words, 21 words are influenced
by ambiguity, and 19 are influenced by named
entities. Among the remaining 60 words, 57
are judged to be semantically different from their
loanwords. For the other three words, the embed-
dings would not be quite accurate probably due
to their infrequency in either the English or the
Japanese corpora used for training.

4.6 Evaluation for Educational Use

To see if the obtained word embeddings of En-
glish and Japanese can assist in language learn-

16In Japanese, present usually means a gift, or to give a
gift, but hardly to show or introduce.

17In Japanese, this word usually means to match one’s
clothes attractively.

1201



ing, for purposes such as lexical-choice error cor-
rection, we evaluate their usefulness by using the
writings of Japanese learners of English. Specif-
ically, we use the Lang-8 English data set (Mizu-
moto et al., 2011)18 to calculate the Dice coeffi-
cient instead of JENAAD. This dataset consists of
sentences originally written by learners, some of
which have been corrected by (presumably) native
speakers of English. Because we target embed-
dings of English and Japanese, we only use En-
glish sentences written by Japanese among other
learners of English. Of those, approximately one
million sentences have corresponding corrections.
With these sentence pairs, we calculate the Dice
coefficient just as in Section 4.2. The coefficient
measures how often a word co-occurs in the orig-
inal sentences and corresponding corrections. If a
word is often corrected to another, it tends to ap-
pear only in the original sentences and not in the
corresponding corrections, and thus, its Dice co-
efficient becomes small, and vice versa. In other
words, the Dice coefficient roughly measures how
often a word is corrected in the Lang-8 English
data. Considering this, we compare the cosine
similarity based on the proposed method with the
Dice coefficient by means of the Pearson’s corre-
lation to evaluate how effective the cosine similar-
ity is in predicting words in which lexical-choice
errors likely occur19; the higher the correlation is,
the better the cosine similarity is as an indicator of
lexical-choice errors. Note that we apply lemma-
tization to all words both in the original sentences
and in the corresponding corrections when calcu-
lating the Dice coefficient in order to focus only
on lexical-choice errors20.

It turns out that the value of the Pear-
son’s correlation coefficient shows a milder cor-
relation (ρ=0.302; significant at the signifi-
cance level α=0.01) in this dataset than in
JENAAD. Some loanwords having the almost
equivalent senses in English have high values
both for the cosine similarity and the Dice
coefficient; examples are musical instruments

18http://cl.naist.jp/nldata/lang-8/
19Some of the words in the loanword list are too infrequent

to calculate the Dice coefficient in the Lang-8 data set. Ac-
cordingly, we excluded those appearing fewer than 30 times
in it when calculating the Pearson’s correlation.

20Other grammatical errors including errors in number and
inflection often appear in the Lang-8 English data, which are
mistakenly included in lexical-choice errors in the calculation
of the Dice coefficient. Lemmatization reduces their influ-
ences to some extent.

such as piano (cos=0.886, Dice=0.951) and vi-
olin (cos=0.881, Dice=0.914); computer terms
computer (cos=0.840, Dice=0.865) and software
(cos=0.847, Dice=0.880) as has discussed in Sec-
tion 4.5. Moreover, some that do not have equiva-
lent senses show mild correspondences (e.g., sen-
tence (cos=0.493, Dice=0.346); note (cos=0.470,
Dice=0.352)).

By contrast, most of the others show less cor-
respondence. One possible reason is that in
the Lang-8 English data, corrections are applied
to grammatical errors other than lexical choices,
which undesirably decreases the Dice coefficient.
Typical examples are errors in number (singu-
lar countable nouns are often corrected as corre-
sponding plural nouns; e.g., book → books) and in
inflection (e.g., book → booked). Therefore, loan-
words whose corresponding English words un-
dergo word-form changes less often tend to show
strong correspondences as can been seen in the
above examples (i.e., software and piano). This
can be regarded as noise in the use of the Lang-
8 data set. As mentioned above, we applied
lemmatization to reduce the influences by noise.
More sophisticated methods such as word align-
ment might improve the accuracy of the evalua-
tion.

5 Conclusions

We computationally analyzed semantic changes
of Japanese loanwords. We used the word em-
beddings of Japanese and English, and mapped
the Japanese embeddings to the space of English,
where we calculated the cosine similarity of a
Japanese loanword and its original English word.
We regarded this value as an indicator of semantic
change. We evaluated our methodology in a num-
ber of ways.

To detect semantic changes accurately, we have
to filter out the words that are ambiguous in the
Japanese phonetic system. Such words tend to
have low cosine similarities. One direction for fu-
ture work is application of the technique to sim-
ilar tasks. For example, we can use our method
to analyze semantic changes of cognates. There
are also a number of ways to investigate seman-
tic changes of loanwords in more detail. For ex-
ample, we can examine the relation between the
semantic change of a loanword and the time at
which the word was introduced in the target lan-
guage. Hamilton et al. (2016) reported that they

1202



used word embeddings to show the relation be-
tween semantic changes and polysemy. It would
be interesting to see if similar results are obtained
for loanwords.

Acknowledgments

This work was partly supported by Grant-in-
Aid for Young Scientists (B) Grant Number
JP26750091. Yoshifumi Kawasaki is supported by
JSPS KAKENHI Grant Number 15J04335.

References
David Bamman, Chris Dyer, and Noah A. Smith. 2014.

Distributed representations of geographically situ-
ated language. In Proceedings of the 52nd Annual
Meeting of the Association for Computational Lin-
guistics (Volume 2: Short Papers), pages 828–834.

Keith Barrs. 2013. Assimilation of English vocabulary
into the Japanese language. Studies in Linguistics
and Language Teaching, 24:1–12.

Alan D. Blair and John Ingram. 1998. Loanword for-
mation: a neural network approach. In Proceed-
ings of SIGPHON Workshop on the Computation of
Phonological Constraints, pages 45–54.

James Breen, Timothy Baldwin, and Francis Bond.
2012. Segmentation and translation of Japanese
multi-word loanwords. In Proceedings of Aus-
tralasian Language Technology Association Work-
shop, pages 61–69.

Jim W. Breen. 2000. A WWW Japanese dictionary.
Japanese Studies, pages 313–317.

Frank E. Daulton. 2009. A sociolinguist expla-
nation of Japan’s prolific borrowing of English.
The Ryukokou Journal of Humanities and Sciences,
pages 29–36.

Manaal Faruqui and Chris Dyer. 2014. Improving
vector space word representations using multilin-
gual correlation. In Proceedings of the European
Chapter of Association for Computational Linguis-
tics, pages 462–471.

Stephan Gouws, Yoshua Bengio, and Greg Corrado.
2015. Bilbowa: Fast bilingual distributed represen-
tations without word alignments. In David Blei and
Francis Bach, editors, Proceedings of the 32nd Inter-
national Conference on Machine Learning (ICML-
15), pages 748–756.

William L. Hamilton, Jure Leskovec, and Dan Juraf-
sky. 2016. Diachronic word embeddings reveal sta-
tistical laws of semantic change. In Proceedings of
the Annual Meeting of the Association for Compu-
tational Linguistics (ACL2016), pages 1489–1501.
Association for Computational Linguistics.

Yoichiro Hasebe. 2006. Method for using Wikipedia
as Japanese corpus (in Japanese). Doshisha studies
in language and culture, 9(2):373–403.

Mark Irwin. 2011. Loanwords in Japanese. John Ben-
jamins Publishing Company.

Adam Jatowt and Kevin Duh. 2014. A framework for
analyzing semantic change of words across time. In
Proceedings of the 14th ACM/IEEE-CS Joint Con-
ference on Digital Libraries, pages 229–238. IEEE
Press.

Gillian Kay. 1995. English loanwords in Japanese.
World Englishes, pages 67–76.

Yoshiro Kojima, editor. 1988. Nihongo no imi eigo
no imi (meanings in Japanese, meanings in English
(translated by the authors of this paper)). Nan’un-
do.

Taku Kudo, Kaoru Yamamoto, and Yuji Matsumoto.
2004. Applying conditional random fields to
Japanese morphological analysis. In Proceedings of
the 2004 Conference on Empirical Methods in Nat-
ural Language Processing (EMNLP-2004), pages
230–237.

Vivek Kulkarni, Rami Al-Rfou, Bryan Perozzi, and
Steven Skiena. 2014. Statistically significant detec-
tion of linguistic change. In Proceedings of the 24th
World Wide Web Conference (WWW), pages 625–
635.

Vivek Kulkarni, Bryan Perozzi, and Steven Skiena.
2016. Freshman or fresher? quantifying the geo-
graphic variation of language in online social media.
In Proceedings of the Tenth International AAAI Con-
ference on Web and Social Media (ICWSM 2016),
pages 615–618.

Lingshuang Jack Mao and Mans Hulden. 2016. How
regular is Japanese loanword adaptation? A com-
putational study. In Proceedings of the 26th Inter-
national Conference on Computational Linguistics
(COLING), pages 847–856.

Jean-Baptiste Michel, Yuan Kui Shen, Aviva Presser
Aiden, Adrian Veres, Matthew K. Gray, The
Google Books Team, Joseph P. Pickett, Dale
Hoiberg, Dan Clancy, Peter Norvig, Jon Orwant,
Steven Pinker, Martin A. Nowak, and Erez Lieber-
man Aiden. 2011. Quantitative analysis of cul-
ture using millions of digitized books. Science,
331(6014):176–182.

Tomas Mikolov, Quoc V. Le, and Ilya Sutskever.
2013a. Exploiting similarities among languages for
machine translation. CoRR, abs/1309.4168.

Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg Cor-
rado, and Jeffrey Dean. 2013b. Distributed repre-
sentations of words and phrases and their composi-
tionality. In Proceedings of the Advances in Neu-
ral Information Processing Systems (NIPS), pages
3111–3119.

1203



Tomoya Mizumoto, Mamoru Komachi, Masaaki Na-
gata, and Yuji Matsumoto. 2011. Mining revi-
sion log of language learning SNS for automated
Japanese error correction of second language learn-
ers. In Proceedings of the 5th International Joint
Conference on Natural Language Processing (IJC-
NLP), pages 147–155.

Sen Nishiyama. 1995. Speaking English with a
Japanese mind. World Englishes, pages 1–6.

Frank Smadja, Kathleen R. McKeown, and Vasileios
Hatzivassiloglou. 1996. Translating collocations for
bilingual lexicons: a statistical approach. Computa-
tional Linguistics, pages 1–38.

Masao Utiyama and Hitoshi Isahara. 2003. Reliable
measures for aligning Japanese-English news arti-
cles and sentences. In Proceedings of the Annual
Meeting of the Association for Computational Lin-
guistics (ACL), pages 72–79.

Will Zou, Richard Socher, Daniel Cer, and Christopher
Manning. 2013. Bilingual word embeddings for
phrase-based machine translation. In Proceedings
of the Conference on Empirical Methods in Natural
Language Processing (EMNLP), pages 1393–1398.

1204


