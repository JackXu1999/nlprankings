



















































Further Investigation into Reference Bias in Monolingual Evaluation of Machine Translation


Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 2476–2485
Copenhagen, Denmark, September 7–11, 2017. c©2017 Association for Computational Linguistics

Further Investigation into Reference Bias in Monolingual Evaluation of
Machine Translation

Qingsong Ma∗† Yvette Graham† Timothy Baldwin‡ Qun Liu†

∗Institute of Computing Technology
Chinese Academy of Sciences

maqingsong@ict.ac.cn

†ADAPT Centre
Dublin City University

firstname.surname@dcu.ie

‡Computing and Info Systems
University of Melbourne

tb@ldwin.net

Abstract

Monolingual evaluation of Machine
Translation (MT) aims to simplify human
assessment by requiring assessors to com-
pare the meaning of the MT output with a
reference translation, opening up the task
to a much larger pool of genuinely qual-
ified evaluators. Monolingual evaluation
runs the risk, however, of bias in favour
of MT systems that happen to produce
translations superficially similar to the ref-
erence and, consistent with this intuition,
previous investigations have concluded
monolingual assessment to be strongly
biased in this respect. On re-examination
of past analyses, we identify a series of
potential analytical errors that force some
important questions to be raised about the
reliability of past conclusions, however.
We subsequently carry out further investi-
gation into reference bias via direct human
assessment of MT adequacy via quality
controlled crowd-sourcing. Contrary to
both intuition and past conclusions, results
show no significant evidence of reference
bias in monolingual evaluation of MT.

1 Introduction

Despite it being known for some time now that au-
tomatic metrics, such as BLEU (Papineni et al.,
2002), provide a less than perfect substitute for
human assessment (Callison-Burch et al., 2006),
evaluation in MT more often than not still com-
prises BLEU scores. Besides increased time and
resources required by the alternative, human eval-
uation of systems, human assessment of MT faces
additional challenges, in particular the fact that
human assessors of translation quality tend to be
highly inconsistent. In recent Conference on Ma-

chine Translation (WMT) shared tasks, for exam-
ple, manual evaluators complete a relative ranking
(RR) of the output of five alternate MT systems,
where they must rank the quality of competing
translations from best to worst. Within this set-up,
when presented with the same pair of MT output
translations, human assessors often disagree with
one another’s preference, and even their own pre-
vious judgment about which translation is better
(Callison-Burch et al., 2007; Bojar et al., 2016).
Low levels of inter-annotator agreement in human
evaluation of MT not only cause problems with re-
spect to the reliability of MT system evaluations,
but unfortunately have an additional knock-on ef-
fect with respect to the meta-evaluation of metrics,
in providing an unstable gold standard. As such,
provision of a fair and reliable human evaluation
of MT remains a high priority for empirical evalu-
ation.

Direct assessment (DA) (Graham et al., 2013,
2014, 2016) is a relatively new human evaluation
approach that overcomes previous challenges with
respect to lack of reliability of human judges. DA
collects assessments of translations separately in
the form of both fluency and adequacy on a 0–100
rating scale, and, by combination of repeat judg-
ments for translations, produces scores that have
been shown to be highly reliable in self-replication
experiments (Graham et al., 2015). The main com-
ponent of DA used to provide a primary ranking
of systems is adequacy, where the MT output is
assessed via a monolingual similarity of meaning
assessment. A reference translation is displayed to
the human assessor (rendered in gray) and below
it the MT output (in black), with the human judge
asked to state the degree to which they agree that
The black text adequately expresses the meaning
of the gray text in English.1 The motivation behind

1Instructions are translated into a given target language.

2476



constructing DA as a monolingual MT evaluation
are as follows:

• Monolingual assessment of MT opens up the
annotation task to a larger pool of genuinely
qualified human assessors;

• Crowd-sourced workers are unlikely to make
use of information that is not entirely nec-
essary for completing a given task; and are
therefore unlikely to use the source language
input if the reference is also displayed or to
make use of the source input inconsistently;

• Displaying only the source without a refer-
ence greatly increases both the difficulty of
the task and the time required to complete
each annotation, which is too serious a trade-
off when we wish to carry out human assess-
ment on a very large scale;

• Varying levels of proficiency in the source
language across different human assessors
could contribute to inconsistency in bilingual
MT evaluations.

Although DA has been shown to overcome the
long-standing challenge of lack of reliability in
human evaluation of MT, the possibility still ex-
ists that, although scores collected with DA have
been shown to be almost perfectly reliable in self-
replication experiments, both sets of scores, al-
though consistent with each other, could in fact
both be biased in the same way. Graham et al.
(2013) include in the design of DA a number of
criteria aimed at minimizing such bias: (i) assess-
ment of individual translations in isolation from
others to avoid a given system being scored un-
fairly low due to its translations being assessed
more frequently alongside high quality transla-
tions (Bojar et al., 2011); (ii) elicit assessment
scores via a Likert-style question without inter-
mediate labeling, motivated by medical research
showing patients’ ratings of their own health to be
highly dependent on the exact wording of descrip-
tors (Seymour et al., 1985); (iii) accurate qual-
ity control by assessing the consistency of judges
with reference only to their own rating distribu-
tions, to accurately remove inconsistent crowd-
sourced data while avoiding removal of data that
legitimately diverges from the scoring strategy of a
given expert judge; and (iv) score standardization
to avoid bias introduced by legitimate variations in
scoring strategies.

Despite efforts to avoid bias in Graham et al.
(2013), since DA is a monolingual evaluation of
MT that operates via comparison of MT output
with a reference translation, it is therefore still pos-
sible, while avoiding other sources of bias, that
DA incurs reference bias where the level of su-
perficial similarity of translations with reference
translations results in an unfair gain, or indeed an
unfair disadvantage for systems that yield trans-
lations that legitimately deviate from the surface
form of reference translations. Following this in-
tuition, Fomicheva and Specia (2016) carry out
an investigation into bias in monolingual evalua-
tion of MT and conclude that in a monolingual
setting, human assessors of MT are strongly bi-
ased by the reference translation. In this paper,
we provide further analysis of experiments orig-
inally provided in Fomicheva and Specia (2016),
in addition to further investigation into the degree
to which the intuition about reference bias can be
supported.

2 Background

Fomicheva and Specia (2016) provide an investi-
gation into reference bias in monolingual evalu-
ation of MT. 100 Chinese to English MT output
translations are assessed by 25 human judges on
a five-point scale, in the form of their response
(None, Little, Much, Most, or All) to the following
question: how much of the meaning of the human
translation is also expressed in the machine trans-
lation?. Precisely the same 100 translations were
assessed by all 25 judges. Human judges were di-
vided into five groups of five: Group 1 (G1) was
shown the source language input and the MT out-
put only and carried out a bilingual assessment,
while Groups 2–5 (G2–G5) were not shown the
source input but instead compared the MT out-
put to a human-generated reference translation. A
distinct set of reference translations was assigned
to each group G2–G5. Inter-annotator agreement
(IAA) was measured for pairs of judges as follows
(the total number of judge pairs resulting from
each setting is provided in parentheses):
• SOURCE: a given pair of judges assessed

translations in a bilingual setting (all possible
pairs within G1 =

(
5
2

)
= 10 pairs);

• SAME: a given pair of judges assessed trans-
lations in a monolingual setting by compari-
son with precisely the same reference trans-
lation (the sum of all possible pairs result-

2477



DIFF SAME SOURCE

0.163 ± 0.01 0.197 ±0.01 0.190 ± 0.02

Table 1: Average Kappa coefficients and 99% con-
fidence intervals reported in Fomicheva and Spe-
cia (2016)

ing from each individual group G2–G5 =(
5
2

)
+

(
5
2

)
+

(
5
2

)
+

(
5
2

)
= 40 pairs);

• DIFF: a given pair of judges assessed trans-
lations in a monolingual setting by com-
parison with a distinct reference translation
(cross product of judges belonging to the four
groups G2–G5 = G2×G3×G4×G5 = 150
pairs).

Reference bias is investigated by comparison
of levels of IAA, via Cohen’s Kappa (κ) and
weighted Kappa coefficients. The hypothesis, al-
though not explicitly stated, is that if agreement of
human assessors of MT in SAME is higher than
that of assessors in DIFF, then the likely cause
is reference bias in human assessment scores.
Agreement in terms of Cohen’s Kappa reported in
Fomicheva and Specia (2016) are reproduced here
in Table 1, where a small increase of 0.034 in av-
erage Kappa is shown for pairs of human asses-
sors in SAME over that of DIFF. To avoid draw-
ing conclusions from a difference that is likely to
have occurred simply by chance, confidence inter-
vals (CIs) are provided and the non-overlapping
CIs for SAME and DIFF shown in Table 1 pro-
vide the basis for the conclusion that IAA is sig-
nificantly higher for SAME compared to DIFF and
subsequently that monolingual evaluation of MT
is strongly biased by the reference translation. On
examination of the analysis that led to the con-
clusion of strong reference bias, we unfortunately
discover a series of methodological issues with re-
spect to confidence interval estimation, however,
that raise doubt about the reliability of this con-
clusion.2

A clear indication of the precise approach to
CI estimation attempted in Fomicheva and Specia
(2016) is unfortunately not explicitly stated but out
of the range of methods that exist the approach
that is applied most resembles bootstrap resam-
pling. Conventionally speaking, bootstrap resam-

2We provide a re-analysis of experiment data specifically
with respect to Cohen’s Kappa. All errors outlined for Co-
hen’s Kappa also lead to the same inaccuracies for weighted
Kappa in Fomicheva and Specia (2016), however.

pling can be applied to CI estimation of a point
estimate for a sample, D, of size N , by simulat-
ing the variance in the population sampling dis-
tribution (Efron and Tibshirani, 1993). A stan-
dard method of estimating CIs via bootstrap re-
sampling is to generate a bootstrap distribution for
the statistic of interest made up of M repeat com-
putations of it, each time drawing a random sam-
ple of size N from D with replacement. Although
most similar to bootstrap resampling, the applica-
tion in Fomicheva and Specia (2016) to CI estima-
tion of Kappa coefficients diverges in some impor-
tant ways from a standard application, however.
We therefore provide a comparison of the analy-
sis drawn in Fomicheva and Specia (2016) with a
standard bootstrap implementation.

Figure 1(a) shows SAME and DIFF bootstrap
distributions, reproduced from code released with
Fomicheva and Specia (2016), originally yielding
non-overlapping CIs that led to the conclusion of
strong reference bias.3 Although the level of sta-
tistical significance is reported to be 99%, CIs in
Figure 1(a) show that the proportion of each boot-
strap distribution was substantially underestimated
leading to overly narrow CI limits for both SAME
and DIFF. In contrast, Figure 1(b) shows CIs re-
sulting from an accurately computed proportion
of 95% of the same bootstrap distribution, where
even at the lower level of 95% significance (as op-
posed to 99%) CIs for SAME and DIFF now over-
lap, reversing the conclusion of strong reference
bias.

In addition, CI estimation diverges from boot-
strap resampling with respect to the number of
bootstrap samples employed. Since there are a to-
tal of NN possible distinct bootstrap samples for
a given sample D (taking order into account), in
a conventional bootstrap implementation a Monte
Carlo approximation of size M is employed, and
the larger M is, the closer the distribution ap-
proaches the true bootstrap distribution (Chernick
and LaBudde, 2014). In Fomicheva and Specia
(2016), CIs are computed via only 50 bootstrap
samples, however.4 Figure 1(c) shows the change
in location of CIs for a typical M =1,000, as op-
posed to M =50 (Figure 1(b)).

3CIs correspond closely to those of the original (Table 1)
but differ by a tiny amount due to the randomness involved in
regeneration from the code.

4We note that M is described as 100 in the publication,
but 50 in the released code. Our question raised about the
methodology also stands for M =100.

2478



(a) Inacc. BD, M=50, n=20, R=no (b) Acc. BD, M=50, n=20, R=no (c) Acc. BD, M=1000, n=20, R=no

0.12 0.16 0.20 0.24

0
10

20
30

40
50

Average Kappa

D
en

si
ty

Diff K
Same K
Diff CI
Same CI

0.12 0.16 0.20 0.24

0
10

20
30

40
50

Average Kappa

D
en

si
ty

Diff K
Same K
Diff CI
Same CI

0.12 0.16 0.20 0.24

0
10

20
30

40
50

Average Kappa

D
en

si
ty

Diff K
Same K
Diff CI
Same CI

(d) Acc. BD, M=1000, n=20, R=yes (e) Acc. BD, M=50, n=N , R=yes (f) Acc. BD, M=1000, n=N , R=yes

0.12 0.16 0.20 0.24

0
10

20
30

40
50

Average Kappa

D
en

si
ty

Diff K
Same K
Diff CI
Same CI

0.12 0.16 0.20 0.24

0
10

20
30

40
50

Average Kappa

D
en

si
ty

Diff K
Same K
Diff CI
Same CI

0.12 0.16 0.20 0.24

0
10

20
30

40
50

Average Kappa
D

en
si

ty

Diff K
Same K
Diff CI
Same CI

Figure 1: (a) Original bootstrap distribution (BD) and confidence intervals (CI) for average Kappa co-
efficients when human annotators employ the same reference translation (Same K) or a different ref-
erence translation (Diff K) in Fomicheva and Specia (2016) (“Inacc. BD”=inaccurate BD proportion;
“Acc. BD”=accurate BD proportion; “M”=number of bootstrap samples; “n”=bootstrap sample size;
“R=yes”: sampled with replacement; “R=no”: sampled without replacement); (b) is (a) with accurate
BD proportion; (c) is (b) with conventional M ; (d) is (c) with R=yes; (f) is (d) with N=n (N is the full
sample size); (e) is (f) with M=50; (f) corresponds to correct BD with all CI errors corrected.

Furthermore, bootstrap distributions in
Fomicheva and Specia (2016) are computed
by random sampling without replacement, and the
size of each bootstrap does not equal the original
sample size N .5 Figure 1(d) shows bootstrap
distributions of Figure 1(c) when the sampling
without replacement error is corrected, and Figure
1(f) shows bootstrap distributions of Figure 1(d)
when the sample size error is corrected.

In summary, Figure 1(f) shows all errors with
respect to CI estimate in Fomicheva and Specia
(2016) corrected, and subsequently CIs for a stan-
dard implementation of bootstrap, which can be
contrasted to those that led to the original conclu-
sion of strong reference bias in Figure 1(a). CIs

5A variant of bootstrap does exist where N is intentionally
lowered to appropriately reduce the variance estimate but is
only applicable when that of standard bootstrap is known to
be over-estimated (Chernick and LaBudde, 2014).

in Figure 1(f) for SAME and DIFF now overlap re-
vealing that experiments in Fomicheva and Specia
(2016), thus far do not show any evidence of ref-
erence bias.

2.1 Measures of Central Tendency

Even if the correct implementation of bootstrap re-
sampling, shown in Figure 1(f), had shown non-
overlapping confidence intervals, it would still un-
fortunately not have been appropriate to draw a
conclusion from this of reference bias, however,
due to the fact that significant differences are not
investigated for the statistic of interest, the Kappa
coefficient, but only for a measure of central ten-
dency of two Kappa coefficient distributions, the
average Kappa of each Kappa distribution. One
reason for avoiding a comparison based on signif-
icant differences in average Kappa, as opposed to
the Kappa point estimates themselves, is that it is

2479



possible for the average of two distributions to be
equal, or indeed have a small but non-significant
difference, while the underlying distributions dif-
fering considerably in several other respects.

Figure 2 shows Kappa coefficient distributions
for all pairs of judges in SAME (40 pairs), DIFF
(150 pairs) and SOURCE (10 pairs), revealing all
distributions to have very similar Kappa coeffi-
cient distributions, with the one exception arising
for SOURCE, where two of the human annotator
pairs had an unusually high agreement level.6

A more informative comparison about levels of
agreement in SAME and DIFF examines signifi-
cant differences in Kappa point estimates, as op-
posed to comparison based on a measure of cen-
tral tendency. For this reason, despite there be-
ing no significant difference in average Kappa for
SAME and DIFF, we also examine the proportion
of Kappa point estimates of judge pairs in SAME
that are significantly different from agreement lev-
els of judge pairs in DIFF, which will provide gen-
uine insight into differences in levels of agreement
between the two groups.

Table 2 shows proportions of all judge pairs
with significant differences in Kappa point esti-
mates (non-overlapping confidence intervals) for
each combination of settings (Revelle, 2014).7

The number of significant differences in Kappa
point estimates for pairs of judges in SAME and
DIFF is only 13%, or, in other words, 87% of judge
pairs across SAME and DIFF have no significant
difference in agreement levels. Table 2 also in-
cludes proportions of significant differences for
Kappa point estimates resulting from judges be-
longing to a single setting (significance testing all
Kappa of SAME with respect to all other Kappa be-
longing to SAME, for example), revealing that the
proportion of significant differences within SAME
(12%) to be very similar to that of SAME × DIFF
(13%), and similarly for DIFF (12%), with only a
single percentage point difference in both cases
in proportions of significant differences. Sub-
sequently, even after correcting the measure of
central tendency error in Fomicheva and Specia
(2016), evidence of reference bias can still not be
concluded.

6The difference in distributions for SOURCE is exagger-
ated to some degree due to the total number of annotator pairs
in SOURCE being substantially lower than the other two set-
tings (only 10 pairs).

7Our re-analysis code is available at https://
github.com/qingsongma/percentage-refBias

−0.4 0.0 0.4 0.8

0.
0

1.
0

2.
0

3.
0

Kappas

D
en

si
ty

Same K
Diff K
Src K

Figure 2: Distribution of Kappa coefficients for
translations assessed with the same reference
translation (“Same K”), different reference trans-
lations (“Diff K”) and source sentences (“Src K”)
(Fomicheva and Specia (2016) data set).

SOURCE SAME DIFF

SOURCE 47% (45) 29% (400) 27% (1,500)
SAME − 12% (780) 13% (6,000)
DIFF − − 12% (11,175)

Table 2: Percentage of human annotator pairs
in Fomicheva and Specia (2016) with signifi-
cant differences in Kappa coefficients for pairs of
annotators shown the same reference translation
(SAME), different reference translations (DIFF) or
the source language input only (SOURCE), total
numbers of annotator comparisons in each case are
provided within parentheses, numbers of annota-
tor pairs was 10 for SOURCE, 40 for SAME and
150 for DIFF.

2.2 Differences in Ratings

The effect that reference bias may or may not have
on actual 1–5 ratings attributed to translations, is
again only reported in terms of a measure of cen-
tral tendency, i.e. average ratings, in Fomicheva
and Specia (2016). The average rating of each
group shown a distinct reference translation is re-
ported, showing distinct average scores for asses-
sors employing a distinct set of reference transla-
tions. Due to the fact that each group had a dis-
tinct average rating, the conclusion is drawn that
MT quality is perceived differently depending on
the human translation used as gold-standard. It
is however, entirely possible that, the difference in
average ratings is in part or even fully caused by

2480



(a) DIFF (b) SAME (c) Source vs Ref. (d) SOURCE
70

60

50

40

30

20

10

0

P
er

ce
nt

ag
e 

(%
)

1 2 3 4 5

1

2

3

4

5

R
at

in
g

(R
ef

B
)

1 2 3 4 5

1

2

3

4

5

R
at

in
g

(R
ef

A
)

1 2 3 4 5

1

2

3

4

5

R
at

in
g

(S
ou

rc
e)

1 2 3 4 5

1

2

3

4

5

R
at

in
g

(S
ou

rc
e)

Rating (Ref A) Rating (Ref A) Rating (Ref) Rating (Source)

Figure 4: Proportions of 1–5 ratings (1=lowest; 5=highest) for translations when human assessors are
shown different reference translations (DIFF), the same reference translation (SAME), the source input
versus a reference translation (Source vs Ref.) or the source input (SOURCE) for data in Fomicheva and
Specia (2016).

1.5 2.0 2.5 3.0 3.5

Average Ratings

0.97

0.91

0.76

0.66

0.55

0.41

SAME(G5)

SAME(G4)

DIFF(G2 − G5)

SAME(G3)

SAME(G2)

SOURCE(G1)

Figure 3: Average rating of human assessors
shown the source input (SOURCE), the same ref-
erence translation (SAME), or a distinct reference
translation (DIFF); range of average ratings pro-
vided adjacent to each setting.

the known lack of consistency across human an-
notators in general.

Quite a substantial leap is made therefore be-
tween the difference in average ratings and the
cause of that difference. To investigate this fur-
ther, we reproduce the average ratings for asses-
sors shown a distinct reference translation, each
represented by a green square along the line la-
beled “DIFF(G2–G5)” in Figure 3, where the over-
all range in average ratings is 0.76. The extrem-
ity of this range is better put into context by com-
parison with the average rating of human asses-
sors shown the same reference translation, each
labeled SAME in Figure 3, where the range of av-
erage ratings attributed to human assessors shown
the same reference can be as large as 0.97 (G5).
Thus, it cannot be concluded from a difference in

average ratings for annotators shown distinct ref-
erence translations that the cause of this difference
is the reference translation.

However, comparison of ratings based only on
averages, again hides detail that an analysis could
otherwise benefit from. We therefore examine
the distribution of individual ratings attributed to
translations, and how well ratings for the same
translation correspond when pairs of annotators
employ the same or distinct reference translation
(or indeed the source input) in Figure 4.8 The
rating pattern in Figure 4 (a) of judge pairs em-
ploying a distinct reference translation compared
to those in Figure 4 (b), where assessors employ
the same reference translation, shows agreement
at the level of individual ratings to be almost in-
distinguishable, showing no evidence of reference
bias.

3 Alternate Reference Bias Investigation

Although we can now say that experiments in
Fomicheva and Specia (2016) showed no evidence
of reference bias, a further issue lies in the fact that
low IAA was incurred throughout the study, and
low IAA unfortunately provides no assurance with
respect to the reliability of conclusions, even when
corrected for analytical errors. In addition, the fact
that IAA was itself the measure by which bias was
investigated is also likely to exacerbate any prob-
lems with respect to reliability of conclusions. We
therefore provide our own additional investigation
into reference bias in monolingual evaluation of
MT. Instead of investigating via IAA, we explore
the degree to which unfairly high or low ratings
might be assigned to translations with respect to

8The sum of percentages in a given row equals 100% in
each heat map.

2481



++

+

+

+

+
+

+

+ +

+

+

+

+
+

+
+

+

+

+

+

+

+

+
+

+

+

+

+

+

+

+

+

+

+

+

+

+

+

+

+

+

+

+

+

+

+

+

+

+

+

+

+

+

+

+

+

+

+

+

+

++

+ +

+

+

+

+

+

+

+

+

+

+

+

+

+

+

+
+

+

+

+

+

+

+

+

+

+

+

+
+

+ +

+

+++

+

−1.5 −1.0 −0.5 0.0 0.5 1.0 1.5

−
1.

5
−

1.
0

−
0.

5
0.

0
0.

5
1.

0
1.

5

DA Postedit

D
A

 G
en

−
R

ef

r = 0.885

+

+

+

+
+

+

+

+

+

+

+

+

+
+

+

+
+

+

+

+

+

+

+

+

+

+

+

+

+

+ +

+

+

+

+

+

+

+

+

+

+

+

+

+

+

+

+

+

+

+ +
+

+
+

+ +

+

++
++ +

+

+

+

+

+

+

+

+

+

+

+

+

+
+

+

+

+

+

+

+

++
+

+

+

++

+ +

+

+

+

+

+

+

+

+

+

−1.5 −1.0 −0.5 0.0 0.5 1.0 1.5

−
1

0
1

2
3

4

DA Postedit

B
LE

U

r = 0.399

(a) (b)

Figure 5: (a) Scatter plot of direct assessment (DA) scores for 100 Chinese to English translations carried
out by comparison with a generic reference translation (DA Gen-Ref) or DA with the reference replaced
by a human post-edit of the MT output (DA Postedit); (b) sentence-level (smoothed) BLEU scores for
the same translations also plotted against DA POST-EDIT; translations and references of (a) and (b) data
set of Fomicheva and Specia (2016); post-edits provided by professional translators with access to the
source and MT output only. BLEU and DA scores are standardized for ease of comparison in all plots.

surface similarity or dissimilarity with the refer-
ence translation.

Reference-similarity bias is the attribution of
unfairly high scores to translations due to high
surface-similarity with the reference translation
even though the translation is not high quality.
A converse kind of reference bias can also oc-
cur, which we call reference-dissimilarity bias,
where unfairly low scores are attributed to transla-
tions that are superficially dissimilar to the refer-
ence translation but are in fact high quality trans-
lations. The challenge in investigating reference
bias lies in the ability to accurately distinguish be-
tween translations that receive unfair scores due to
surface-similarity or dissimilarity from those that
achieved a fair score due to the translation being
genuinely high or low quality.

To separate genuine high quality translations
from those that score unfairly high, we carry out
two separate assessments of the same set of trans-
lations. Firstly, we carry out a standard monolin-
gual MT evaluation that employs a generic ref-
erence translation (GEN-REF setting), the scores
that potentially encounter reference bias. Sec-
ondly, we carry out an additional human evalu-
ation of the same translations, where, instead of
the generic reference, the human assessor com-
pares the MT output with a human post-edit of it
(POST-EDIT setting). The latter human assessment

is highly unlikely to encounter any form of refer-
ence bias because the assessment employs a post-
edit of the MT output, which itself will only differ
the MT output with respect to the parts of it that
are genuinely incorrect. Translations encounter-
ing reference-similarity bias can then be identified
by a high GEN-REF score combined with a low
POST-EDIT score, and vice-versa for reference-
dissimilarity, a low GEN-REF score combined with
a high POST-EDIT score.

3.1 Reference Bias Experiments

Experiments were carried out using the original
100 Chinese to English translations released by
Fomicheva and Specia (2016), in addition to 70
English to Spanish MT translations (WMT-13
Quality Estimation Task 1.1).9 Professional trans-
lators, entirely blind to the purpose of the study,
were employed to post-edit the MT outputs used in
the POST-EDIT setting, and were shown the source
input document and the MT output document only
(no reference translations).10

Once post-edits had been created, DA was em-
ployed in two separate runs on Amazon Mechani-

9A single generic reference translation was chosen at ran-
dom from the Chinese to English data set; only a single refer-
ence is available for each translation in the English to Spanish
data set.

10Post-editors were paid at the standard rate.

2482



+

+

+

+

+

+

+

+

+

+

+++

+

+

+

+

+

+

+

+

+

+

+

+
+

+
++

+

+

+

+

++

++

+

+

+

+

+
+

+

+

+

+

+

+

+

+

+

+

+

+

+

+

+

+

+

+
+

+

+

+
+ +

+

+
+

−1.5 −1.0 −0.5 0.0 0.5 1.0 1.5

−
1.

5
−

1.
0

−
0.

5
0.

0
0.

5
1.

0
1.

5

DA Postedit

D
A

 G
en

−
R

ef

r = 0.854

+

+

+

++

+

+

+

+

+
+

+

+

+

+

+

+
+

+

+ +

+ +

+

+

+

+ +

+

+

+

+

+

+
+

+

+

+

+
+ +

+

+

+ +
+

+

+ +
+

+

+

+ +

+

+

++

+

+

+

+

+

+
+

+
+

+

+

+

−1.5 −1.0 −0.5 0.0 0.5 1.0 1.5

−
1

0
1

2
3

4

DA Postedit

B
LE

U

r = 0.588

(a) (b)

Figure 6: (a) Scatter plot of direct assessment (DA) scores for 70 English to Spanish translations carried
out by comparison with a generic reference translation (DA Gen-Ref) or DA with the reference replaced
by a human post-edit of the MT output (DA Postedit); (b) sentence-level (smoothed) BLEU scores for the
same translations also plotted against DA POST-EDIT; translations and generic references for (a) and (b)
WMT-13 Quality Estimation Task 1.1 (Bojar et al., 2013) data set; post-edits provided by professional
translators with access to the source and MT output only. DA and BLEU scores are standardized for
ease of comparison in all plots.

cal Turk,11 once for GEN-REF and once for POST-
EDIT. Besides employing distinct reference trans-
lations in the assessment, all other set-up crite-
ria were identical for both evaluation settings, in-
cluding the conventional segment-level DA set-
ting, where a minimum of 15 human assessments
are combined into a mean DA score for a given
translation, after strict quality control measures
and score standardization have been applied.

3.2 Results and Discussion

Figure 5(a) shows a scatter-plot of DA scores at-
tributed to translations for GEN-REF compared to
POST-EDIT in the Chinese to English experiment.
Translations that encounter reference-dissimilarity
bias are expected to appear in the lower-right
quadrant of Figure 5(a), receiving an unfairly low
GEN-REF score combined with a high POST-EDIT
score. As can be seen from Figure 5(a) only a very
small number of translations fall into this quad-
rant, all of which are very closely located to adja-
cent upper-right and lower-left quadrants. A single
translation in Figure 5(a) is an outlier in this re-
spect, receiving a high POST-EDIT score in combi-
nation with a lower than average GEN-REF score,

11https://www.mturk.com

possibly indicating reference bias. On closer in-
spection, however, the score combination is in fact
the result of a mistake in the reference translation.
Although the low GEN-REF score was the result of
an error in the reference translation, a single trans-
lation having this score combination is not suffi-
cient evidence to conclude strong reference bias.
In future work we would like to investigate the fre-
quency of erroneous reference translations in ex-
isting MT test sets, although we expect them to be
few, accurate statistics would provide a better indi-
cation of the degree to which they could negatively
impact the accuracy of DA evaluations.

Figure 5(a) is also void of evidence of
reference-similarity bias, as only a small number
of translations lie in the upper-left quadrant and
are all very close to the origin and/or adjacent
quadrants.

Contrasting Figure 5(a), the correspondence of
GEN-REF scores to POST-EDIT scores, with Fig-
ure 5(b), the correspondence of known reference-
biased BLEU scores, in contrast a large number
of BLEU scores for translations do encounter ref-
erence bias, as seen by the spread of translations
appearing across both the bottom-right and upper-
left quadrants.

2483



Similarly for English to Spanish, the correspon-
dence between GEN-REF and POST-EDIT scores
for translations are shown in Figure 6(a), where,
again, only a small number of translations ap-
pear in the bottom-right and upper-left quadrants,
all lying very close to adjacent quadrants, again,
showing no significant indication of reference
bias. A single translation appears to break the
trend again, however, receiving a low GEN-REF
score combined with a high POST-EDIT score, lo-
cated in the lower-right quadrant of Figure 6(a).
On closer inspection, the low GEN-REF score is
the result of something unexpected, as the MT out-
put is in fact an accurate translation while at the
same time the generic reference is also correct,
but unusually the meaning of the two diverge from
each other.12 Again, a single translation receiving
this score combination is not sufficient evidence
to conclude reference bias to be a significant prob-
lem for monolingual evaluation. The lack of ref-
erence bias in Figure 6(a) can again be contrasted
to known reference-biased BLEU scores in Figure
6(b) for English to Spanish.

4 Conclusions

In this paper, we provided an investigation into ref-
erence bias in monolingual evaluation of MT. Our
review of past investigations reveals potential an-
alytical errors and raises questions about the relia-
bility of past conclusions of strong reference bias.
This motivates our further investigation for Chi-
nese to English and English to Spanish MT em-
ploying direct human assessment in a monolingual
MT evaluation setting. Results showed no signif-
icant evidence of reference bias, contrary to prior
reports and intuition.

Acknowledgments

This project has received funding from NSFC
Grant No. 61379086 and the European Union
Horizon 2020 research and innovation pro-
gramme under grant agreement 645452 (QT21)
and Science Foundation Ireland in the ADAPT
Centre for Digital Content Technology (www.
adaptcentre.ie) at Dublin City University
funded under the SFI Research Centres Pro-
gramme (Grant 13/RC/2106) co-funded under the
European Regional Development Fund.

12Source: A straightforward man; MT: Un hombre sen-
cillo; Reference: Un hombre sincero

References
Ondrej Bojar, Miloš Ercegovčevic, Martin Popel, and

Omar F. Zaidan. 2011. A grain of salt for the
WMT manual evaluation. In Proceedings of the 6th
Workshop on Statistical Machine Translation. As-
sociation for Computational Linguistics, Edinburgh,
Scotland, pages 1–11.

Ondřej Bojar, Christian Buck, Chris Callison-Burch,
Christian Federmann, Barry Haddow, Philipp
Koehn, Christof Monz, Matt Post, Radu Soricut, and
Lucia Specia. 2013. Findings of the 2013 Work-
shop on Statistical Machine Translation. In Pro-
ceedings of the Eighth Workshop on Statistical Ma-
chine Translation. Association for Computational
Linguistics, Sofia, Bulgaria, pages 1–44.

Ondřej Bojar, Rajen Chatterjee, Christian Federmann,
Yvette Graham, Barry Haddow, Matthias Huck,
Antonio Jimeno Yepes, Philipp Koehn, Varvara
Logacheva, Christof Monz, Matteo Negri, Aure-
lie Neveol, Mariana Neves, Martin Popel, Matt
Post, Raphael Rubino, Carolina Scarton, Lucia Spe-
cia, Marco Turchi, Karin Verspoor, and Marcos
Zampieri. 2016. Findings of the 2016 conference
on machine translation. In Proceedings of the First
Conference on Machine Translation. Association for
Computational Linguistics, Berlin, Germany, pages
131–198.

Chris Callison-Burch, Cameron Fordyce, Philipp
Koehn, Christof Monz, and Josh Schroeder. 2007.
(meta-) evaluation of machine translation. In Pro-
ceedings of the Second Workshop on Statistical Ma-
chine Translation. Association for Computational
Linguistics, Prague, Czech Republic, pages 136–
158.

Chris Callison-Burch, Miles Osborne, and Philipp
Koehn. 2006. Re-evaluating the role of BLEU in
machine translation research. In Proceedings of the
11th Conference European Chapter of the Associa-
tion for Computational Linguistics. Association for
Computational Linguistics, Trento, Italy, pages 249–
256.

Michael R Chernick and Robert A LaBudde. 2014. An
introduction to bootstrap methods with applications
to R. John Wiley & Sons.

Bradley Efron and Robert J. Tibshirani. 1993. An In-
troduction to the Bootstrap. Chapman & Hall, New
York City, NY.

Marina Fomicheva and Lucia Specia. 2016. Reference
bias in monolingual machine translation evaluation.
In Proceedings of the 54th Annual Meeting of the
Association for Computational Linguistics. Associ-
ation for Computational Linguistics, Berlin, Ger-
many, pages 77–82.

Yvette Graham, Timothy Baldwin, Alistair Moffat, and
Justin Zobel. 2013. Continuous measurement scales
in human evaluation of machine translation. In Pro-
ceedings of the 7th Linguistic Annotation Workshop

2484



and Interoperability with Discourse. Association for
Computational Linguistics, Sofia, Bulgaria, pages
33–41.

Yvette Graham, Timothy Baldwin, Alistair Moffat, and
Justin Zobel. 2014. Is machine translation getting
better over time? In Proceedings of the 14th Confer-
ence of the European Chapter of the Association for
Computational Linguistics. Association for Com-
putational Linguistics, Gothenburg, Sweden, pages
443–451.

Yvette Graham, Timothy Baldwin, Alistair Mof-
fat, and Justin Zobel. 2016. Can machine
translation systems be evaluated by the crowd
alone? Natural Language Engineering pages 1–28.
https://doi.org/10.1017/S1351324915000339.

Yvette Graham, Nitika Mathur, and Timothy Bald-
win. 2015. Accurate evaluation of segment-level
machine translation metrics. In Proceedings of
the 2015 Conference of the North American Chap-
ter of the Association for Computational Linguis-
tics Human Language Technologies. Association for
Computational Linguistics, Denver, Colorado, pages
1183–1191.

Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. A method for automatic evalua-
tion of machine translation. In Proceedings of the
40th Annual Meeting of the Association for Compu-
tational Linguistics. Association for Computational
Linguistics, Philadelphia, PA, pages 311–318.

William Revelle. 2014. psych: Procedures for person-
ality and psychological research. Northwestern Uni-
versity, Evanston. R package version 1(1).

Robin A. Seymour, Judy. M. Simpson, J. Ed Charl-
ton, and Michael E. Phillips. 1985. An evaluation
of length and end-phrase of visual analogue scales
in dental pain. Pain 21:177–185.

2485


