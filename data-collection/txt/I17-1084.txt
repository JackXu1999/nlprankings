



















































Selective Decoding for Cross-lingual Open Information Extraction


Proceedings of the The 8th International Joint Conference on Natural Language Processing, pages 832–842,
Taipei, Taiwan, November 27 – December 1, 2017 c©2017 AFNLP

Selective Decoding for Cross-lingual Open Information Extraction

Sheng Zhang
Johns Hopkins University
zsheng2@jhu.edu

Kevin Duh
Johns Hopkins University
kevinduh@cs.jhu.edu

Benjamin Van Durme
Johns Hopkins University
vandurme@cs.jhu.edu

Abstract

Cross-lingual open information extrac-
tion is the task of distilling facts from
the source language into representations
in the target language. We propose
a novel encoder-decoder model for this
problem. It employs a novel selective de-
coding mechanism, which explicitly mod-
els the sequence labeling process as well
as the sequence generation process on
the decoder side. Compared to a stan-
dard encoder-decoder model, selective de-
coding significantly increases the perfor-
mance on a Chinese-English cross-lingual
open IE dataset by 3.87-4.49 BLEU and
1.91-5.92 F1. We also extend our ap-
proach to low-resource scenarios, and gain
promising improvement.

1 Introduction

Cross-lingual open information extraction is de-
fined as the task of extracting facts from the source
language (e.g., Chinese text in Fig. 1(a)) and rep-
resenting them in the target language (e.g. English
predicate-argument information in Fig. 1(c))1. It is
a challenging task and of great importance to solve
the cross-lingual portability issues of various NLP
systems which are in the support of open informa-
tion extraction (Sudo et al., 2004). Additionally,
there is often a great demand for rapid access to
information across languages, especially when a
large-scale incident occurs (Lu et al., 2016).

Conventional solutions decompose the task as
a pipeline of machine translation followed by

1The predicate-argument information is normally repre-
sented by relation tuples. Here, we use a richer representation
(i.e., a tree structure) adopted by PredPatt (White et al., 2016),
a lightweight tool for identifying predicate-argument infor-
mation, available at https://github.com/hltcoe/
PredPatt.

P
[ Soldiers

A
started
P

mortars
A

]
PP

[ Soldiers
A

firing
P

]
P

[(Soldiers:ah) started:ph firing:ph (mortars:ah )]][(Soldiers:ah)

.
(a)

(d)

(e)

started

firingSoldiers

Soldiers mortars

ARG ARG

ARG ARG

SOMETHING

(c)

 
(b)

Figure 1: Example of cross-lingual open IE:
Chinese input text (a), English translation (b),
English predicate-argument information (c), lin-
earized PredPatt output (d) and output with sep-
arated predicate and argument labels (e).

open information extraction (or vice versa), which
causes a deviation since machine translation at-
taches equal importance to adequacy and fluency
of the intermediate translation results (Snover
et al., 2009), whereas the final goal focuses more
on extracting correct predicates and arguments.

Recently Zhang et al. (2017a) proposes an end-
to-end solution that outperforms the conventional
pipeline solutions. They recast cross-lingual open
IE as a sequence-to-sequence learning problem
by converting the target facts in the tree struc-
ture (Fig. 1(c)) into a linear form called lin-
earized PredPatt2 (Fig. 1(d)), and employ a stan-

2Linearized PredPatt is converted from the PredPatt tree
structure by taking an in-order traversal of every node in the
tree. See Zhang et al. (2017a) for details.

832



dard encoder-decoder model to address the prob-
lem (from (a) to (d) in Fig. 1). In the linearized
PredPatt (Fig. 1(d)), special labels are appended
to tokens as type indications. Brackets and paren-
theses have to be inserted to delimit predicate and
argument spans. Such a workaround inevitably ex-
pands the vocabulary space and increases the bur-
dens on the decoder, which is not ideal for sparse
data scenarios and limits the overall performance.

In this paper, we reformulate cross-lingual open
IE as a sequence generation and labeling problem
(from (a) to (e) in Fig. 1) by separating the predi-
cate and argument labels from the target linearized
PredPatt, and removing unnecessary parentheses.
We propose a novel encoder-decoder model which
employs a selective decoding mechanism to ex-
plicitly model the sequence labeling as well as the
sequence generation process. The new model sub-
stantially reduces the vocabulary space, eases the
burden on the decoder, and leads to a significant
gain of performance. And the natural of the selec-
tive decoding mechanism enables a joint training
strategy that optimizes sequence generation and
labeling simultaneously. In addition, we introduce
an adapted beam search algorithm to further im-
prove the prediction quality.

Experimental results demonstrate that our
model employing the selective decoding mecha-
nism significantly outperforms the previous end-
to-end solution not only on a Chinese-English
dataset, but also in low-resource cross-lingual
open IE scenarios.

2 Problem Formulation

Our goal is to learn a model which directly maps
a sentence input X in the source language into a
sentence Y in the target language, and simultane-
ously labels each token in Y with type information
T . For cross-lingual open IE, the types are pred-
icate and argument. It is important to label types
because they are used to annotate predicates and
arguments in the generated tokens. Formally, we
regard the input as a sequence X = x1, · · · , x|X|,
and the output as two sequences (Y, T ): (1) the
sentence in target language Y = y1, · · · , y|Y |, and
(2) the type information T = t1, · · · , t|Y |, where
ti ∈ T is the label for the token yi, and |X| and
|Y | are the length of the sequence X and Y re-
spectively. Our model maps X into (Y, T ) using a
conditional probability which is decomposed as:

Soldiers

started

Soldiers

firing

mortars

<eos>

s0

<bos> Soldiers started Soldiers firing mortars

Predicate
Decoder

Argument
Decoder

PA A P PA

Selector

Figure 2: Selective decoding process.
(Brackets and attention layers are omitted.)

P (Y, T | X) =
∏|Y |

i=1
P (yi, ti | y<i, t<i, X)

=
|Y |∏
i=1

P (yi | y<i, t≤i, X)P (ti | y<i, t<i, X) (1)

where y<i = y1 · · · yi−1 and t≤i = t1 · · · ti.
Equation (1) can be interpreted as at each de-

coding time step, the model first decides which
type (label) of tokens to generate, and then gen-
erates a token for that type.

3 Proposed Model

To learn the factored conditional probabilities
as shown in Equation (1), we propose a novel
encoder-decoder model with the selective decod-
ing mechanism: on the encoder side, an input sen-
tence X is encoded into vector representations;
on the decoder side, the selective decoding mech-
anism employs multiple decoders each of which
learns the conditional probability of decoding a
specific type of token (i.e., P (yi | y<i, t≤i, X)),
and a selector learning to decide which type of de-
coder to use (i.e., P (ti | y<i, t<i, X)) at each de-
coding time step.

Fig. 2 illustrates the selective decoding process
for the example shown in Fig. 1(e). s0 is the ini-
tial decoder hidden state initialized by the last hid-
den state of the encoder. Special tokens 〈bos〉 and
〈eos〉 are added to the beginning and the end of
the sequence to indicate the start and the finish of
decoding. The connections at each decoding time
step are dynamically changing according to the de-
cision of the selector. Specifically, at the decoding
time step i, firstly the selector on the top decides to
use which type of decoder Dti ∈ {DP, DA}3, and

3DP stands for the predicate decoder, and DA for the ar-
gument decoder.

833



then the decoder Dti decodes the token yi which
is naturally given the label ti.

In addition to distinguish between labels, the
multiple decoders used by the selective decoding
mechanism has two prominent advantages over
the single standard RNN decoder: (1) Multiple de-
coders learn different conditional probability dis-
tributions for predicate and argument generation
respectively. For instance, given the same input to-
ken ”wanted”, the predicate decoder would like to
next generate tokens such as ”to” and ”by” which
starts a prepositional phrase, whereas the argu-
ment decoder would be in favor of tokens such
as ”a” and ”him” which starts a direct object. (2)
Multiple decoders reduce the decoder vocabulary
size, which eases the burden of sequence genera-
tion. Moreover, we propose an efficient architec-
ture that supports batch training of the model. The
details of the architecture are described in the De-
coder with Selective Decoding section.

3.1 Encoder
The encoder employs a bi-directional recurrent
neural network (Schuster and Paliwal, 1997) to en-
code the input sequence X = x1, · · · , x|X| into a
sequence of hidden states h = h1, · · · , h|X|. Each
hidden state hi in h is a concatenation of a left-
to-right hidden state

−→
hi ∈ Rn and a right-to-left

hidden state
←−
hi ∈ Rn,

hi =

[←−
h i
−→
h i

]
=

[←−
f (xi,

←−
h i+1)

−→
f (xi,

−→
h i−1)

]
,

where
←−
f and

−→
f are two L-layer stacked

LSTMs units (Hochreiter and Schmidhuber,
1997).

3.2 Decoder with Selective Decoding
Unlike the single standard RNN decoder (Bah-
danau et al., 2014), which recurrently uses the
same decoder to generate tokens, our model dy-
namically selects different decoders at each decod-
ing time step to generate tokens (Fig. 2). How-
ever, since the decoding path may be different for
each input sequence X , directly running the selec-
tive decoding process suffers from a key technical
issue: it does not support batched computation,
which makes them slow and unwieldy for large-
scale NLP tasks (Bowman et al., 2016).

To address this issue, we introduce a general de-
coding architecture that is applicable to all selec-
tive decoding processes. The detailed connection

si-1

mi

yi-1

yi-1

yi
mi[P]

mi[A]

mi[P]

mi[A]

si

Predicate LSTMs

Argument LSTMs

Selector

Vector Transfer

Copy

Concatenate

Pointwise Add

Legend

Figure 3: Detailed connection at a decoding step.
(Attention layers are ommited.)

in the architecture is shown in Fig. 3. At each de-
coding time step, the model feeds the input token
and the previous hidden state to all types of de-
coders, and use a mask vector created by the selec-
tor to select the decoder output to generate tokens
and update the hidden state.

Formally, let si ∈ Rn denote the hidden state at
decoding time step i. The last left-to-right hidden
state

−→
h |X| from the encoder is used to initialize

the first hidden state s0 in the decoder.
Selector: At the decoding time step i, given the
sequence of encoder hidden states h and the pre-
vious decoder hidden state si−1, the selector com-
putes the conditional probability of ti (i.e., the type
of decoder to use) as:

P (ti | y<i, t<i, X) = g(ti, si−1, h)
=softmax(Uosi−1 + Coci−1 + bo)[ti], (2)

where Uo ∈ R|T |×n, Co ∈ R|T |×n and bo ∈ R|T |
are weight matrices and bias.4 [ti] indexes the ele-
ment of a vector that corresponds to the type ti.
Attention: The context vector ci−1 captures
the attention to the encoder side (Bahdanau
et al., 2014; Luong et al., 2015), computed as a
weighted sum of encoder hidden states: ci−1 =∑|X|

j a(i−1)jhj . The weight a(i−1)j is computed
by:

a(i−1)j =
exp (score(si−1, hj))∑|X|

j′=1 exp (score(si−1, hj′))
, (3)

where score(si−1, hj) = s
ᵀ
i−1Wahj , and Wa ∈

Rn×2n is a transform matrix.
Hidden State Update: According to the condi-
tional probability P (ti | y<i, t<i, X), a mask vec-
tor mi is created, which is used to mask out the

4|T | is the number of token types. In the example shown
in Fig. 3, T = {P,A}, and |T | = 2.

834



decoders’ hidden states,

mi[ti] =


1, if ti =argmax

t′i∈T
P (t′i |y<i,t<i,X)

0, otherwise

Then the hidden state si for the decoding time
step i is computed by:

si =
∑
ti∈T

mi[ti]fti(yi−1, si−1, ci)

where ci is the context vector capturing the atten-
tion, computed in the same way as Equation (3).
fti is L-layer stacked LSTMs for the type ti.
In Fig. 3, there is an L-layer stacked LSTMs for
generating predicate tokens fP, and another L-
layer stacked LSTMs for generating argument to-
kens fA. They have untied parameters.
Token Generation: The conditional probability
of the token yi with the type ti is defined as:

P (yi | y<i, t≤i, X) = g′(yi−1, si−1, h, mi)
=softmax(U ′osi + C

′
oci + b

′
o)[yi], (4)

where U ′o ∈ R|V|×n, C ′o ∈ R|V|×n and bo ∈ R|V|
are weight matrices and bias.5

3.3 Training
In the training procedure, our optimization objec-
tive is to minimize the negative log-likelihood of
the sequence Y and its type information T given
the input sequence X over the training data, de-
fined as:

minimize−
∑

(X,Y,T )∈D
log P (Y, T | X)

According to Equation (1), the log-likelihood
log P (Y, T | X) can be decomposed as:
|Y |∑
i=1

[logP (yi |y<i,t≤i,X)+logP (ti |y<i,t<i,X)],

where P (yi |y<i,t≤i,X) models the sequence gen-
eration process, and P (ti | y<i,t<i,X) models the
sequence labeling process. They are computed by
Equations (4) and (2) respectively. The decom-
position of the log-likelihood into these two parts
enables a joint optimization for the sequence gen-
eration and labeling process simultaneously.

We use the Adam optimizer (Kingma and Ba,
2014) and mini-batch gradient to solve this opti-
mization problem. To prevent overfitting, we ap-
ply dropout operators (Srivastava et al., 2014) to
non-recurrent connections between LSTM layers.

5|V| is the vocabulary size of the target language.

3.4 Inference

In the inference procedure, we predict the se-
quence Y and its type information T for an input
sequence X according to:

(Ŷ , T̂ ) = argmax
(Y ′,T ′)∈V|Y ′|×T |Y ′|

P (Y ′, T ′ | X)

V |Y ′| × T |Y ′| is the set of all possible (Y ′, T ′)
pairs. And (Ŷ , T̂ ) can be directly converted to the
form of linearized PredPatt which is used for eval-
uation.

However, it is impractical to iterate over all
these (Y ′, T ′) pairs during inference: here, we
use beam search to generate tokens and labels as
shown in Algorithm 1.

Algorithm 1 Beam search for selective decoding
Input: X - sequence in the source language
Output: (Y, T ) - sequence in the target language

and its type information
1: step← 0
2: b← beam size
3: l← max decoder length
4: fw hid← Encoder(X)
5: TERMINATED STATES.init(∅)
6: start state← State(〈bos〉, fw hid)
7: BEAM.init({start state})
8: while BEAM 6= ∅ and step < l do
9: step← step + 1

10: ACTIVE STATES.init(∅)
11: for state in BEAM do
12: . Select the decoder.
13: t← Selector(state)
14: . Generate the next token.
15: states← Decodert(state)
16: . Update the active states.
17: ACTIVE STATES.update(states)
18: BEAM.clean()
19: . Get the top-k candidates.
20: for state in top(b, ACTIVE STATES) do
21: if state.decoded token = 〈eos〉 then
22: b← b− 1
23: . Move out the terminated states.
24: TERMINATED STATES.add(state)
25: else
26: . Update the beam.
27: BEAM.add(state)
28: TERMINATED STATES.update(BEAM)
29: state← top(1, TERMINATED STATES)
30: return (state.Y, state.T )

835



The beam is used to increase the search space
for the sequence Y in the target language. At each
decoding time step, we first greedily select the
type of decoder, and then generate candidate to-
kens from the selected decoder to update the beam.
When the special token 〈eos〉 is generated, we re-
move the candidate sequence from the beam.

4 Related Work

The model we propose in this paper is adapted
from the RNN encoder-decoder architectures
which have been successfully applied to a wide
range of NLP tasks such as machine trans-
lation (Kalchbrenner and Blunsom, 2013; Cho
et al., 2014; Bahdanau et al., 2014), image de-
scription generation (Karpathy and Fei-Fei, 2015;
Vinyals et al., 2015b), syntactic parsing (Vinyals
et al., 2015a), question answering (Hermann et al.,
2015), summarization (Rush et al., 2015), and se-
mantic parsing (Dong and Lapata, 2016).

As a novel variation of the encoder-decoder ar-
chitecture, our model provides a general solution
to tasks involving translation and labeling. cross-
lingual open IE is an example of this kind of task.
The end-to-end solution proposed by Zhang et al.
(2017a) used a vanilla attention-based encoder-
decoder model to achieve results which outper-
form the traditional pipeline solutions. Compared
to the vanilla encoder-decoder model, our model
splits the joint task into two concurrent tasks (i.e.,
labeling and translating), which are jointly learnt
by a selector and multiple decoders. This eases
the burden of the decoder by shifting the labeling
task to the selector. As a result, our model requires
a smaller vocabulary for the target language.

The selective decoding mechanism can be
viewed as having different types of decoders
stacking together and adding a hard gate to the
RNN unit, through which the bit of information
will be either totally kept or dropped. It may
seem redundant since the RNN gated unit already
has the sophisticated gating mechanism such as
the GRU unit (Cho et al., 2014) and the LSTM
unit (Hochreiter and Schmidhuber, 1997). How-
ever, we think that the selective decoding mecha-
nism is a complement to the gated unit: rather than
having a soft pointwise control, the selective de-
coding mechanism adopts a hard vectorwise con-
trol to explicitly select a certain type of informa-
tion which corresponds to the predicate or the ar-
gument by keeping one and dropping the others,

whereas the GRU/LSTM gated unit itself learns
to memorize long short-term dependencies. Simi-
lar mechanisms have been used in neural machine
translating (Tu et al., 2016) and image caption
generation (Xu et al., 2015) to explicitly control
the influence from source or target contexts. The
experiments in § 5 also confirms our point: our
model using the selective decoding mechanism
significantly improves the performance, compared
to the standard encoder-decoder model.

Regarding to open IE systems for generat-
ing training data, PredPatt has shown promis-
ing performance on large-scale open IE bench-
marks (Zhang et al., 2017c). Compared to other
existing open IE systems (Banko et al., 2007;
Fader et al., 2011; Angeli et al., 2015), PredPatt
uses manual language-agnostic patterns on UD,
which makes it a well-founded component across
languages. Additionally, the underlying structure
constructed by PredPatt has been shown to be
a well-formed syntax-semantics interface (Zhang
et al., 2017b).

5 Experiments

We describe the hyper-parameters setting for ex-
periments, evaluate our approach in two kinds of
scenarios, and compare the results of our approach
and the other comparing approaches.

5.1 Hyper-parameters

On the encoder side, both the forward RNN and
the backward RNN have 2-layer stacked LSTMs
with 500 hidden units. On the decoder side, all
types of decoders are 2-layer stacked LSTMs with
500 hidden units. All LSTM parameters are sam-
pled from U(−0.1, 0.1). The dropout rate is set
to 0.3. The word embedding size is 300 for input
tokens on both the encoder side and the decoder
side. We use open-source GloVe vectors (Pen-
nington et al., 2014) trained on Common Crawl
840B with 300 dimensions6 to initialize the word
embeddings on the decoder side. The mini-batch
size is set to 64 and the step size set to 50. Gra-
dients are clipped when their norms are greater
than 5 (Pascanu et al., 2013). For simplicity, we
use vanilla softmax over the decoder vocabulary
as opposed to more efficient alternatives such as
sampled softmax (Jean et al., 2015). The vocabu-
lary size is set to 40,000. The number of epochs

6https://nlp.stanford.edu/projects/
glove/

836



is 20. Early stopping is used to avoid overfitting.
The beam size is 5. Before feeding into the en-
coder, we reverse the input sentences (Sutskever
et al., 2014).

5.2 Chinese-English

5.2.1 Dataset

We first evaluate our approach on the Chinese-
English dataset (Zhang et al., 2017a), which con-
tains pairs of Chinese sentences and English lin-
earized PredPatt. Table 1 shows the number of
data for training, validation and test.

#Train #Valid #Test

941,040 10,000 39,626

Table 1: Number of data used for Chinese-English
cross-lingual open IE.

5.2.2 Comparisons

Our approach (Selective Decoding) is compared
against four other approaches: (1) Joint Seq2Seq,
which trains a standard encoder-decoder model on
the Chinese-English dataset described in Table 1;
(2) Joint Moses, which trains a phrase-based ma-
chine translation system, Moses (Koehn et al.,
2007), directly on the same data; (3) Pipeline-S
which consists of a Moses system that translates
Chinese sentence to English sentence, followed
by SyntaxNet Parser (Andor et al., 2016) for Uni-
versal Dependency parsing on English, and Pred-
Patt (White et al., 2016) for predicate-argument
identification; and (4) Pipeline-N is the same as
Pipeline-S except that the Moses system is re-
placed by OpenNMT (Klein et al., 2017), a neural
machine translation system.

5.2.3 Evaluation Metrics

For evaluation, we directly convert the output by
our approach (e.g. Fig. 1(e)) to the form of lin-
earized PredPatt (e.g., Fig. 1(d)), and follow the
same manner in Zhang et al. (2017a), using the
cased BLEU score and the token-level F1 score to
evaluate the results, since the generation of lin-
earized PredPatt involves translation and informa-
tion extraction. We also compute the recoverabil-
ity: the number of outputs can not be recovered to
the tree structure (e.g., Fig. 1(c)).

5.2.4 Evaluation using BLEU
Table 2 shows the cased BLEU scores of linearized
PredPatt and linearized predicates7 on the test set.
Selective Decoding significantly improves the per-
formance on both of them. Compared to the pre-
vious best approach (Joint Seq2Seq), Selective
Decoding improves the BLEU score of linearized
PredPatt to 23.88, and the score of linearized pred-
icates to 25.42.

Approach LinearizedPredPatt
Linearized
Predicate

Pipeline-S 17.19 17.24
Pipeline-N 18.03 18.59
Joint Moses 18.34 16.43
Joint Seq2Seq 18.94 21.55

Selective Decoding 23.88 24.81
- pretrained embeddings 23.67 25.42
- beam search 22.07 23.94

Table 2: Evaluation results (BLEU) of linearized
PredPatt and linearized predicates on the test set.

We also report two ablation variants of Selective
Decoding, i.e., without the pretrained word em-
beddings for parameter initialization (-pretrained
embeddings), and without beam search, only using
greedy search during inference (-beam search). As
shown in Table 2, while the pretrained word em-
beddings moderately improve the BLEU score of
linearized PredPatt, they have slightly negative im-
pact on linearized predicates. Beam search helps
improve the BLEU score of both.

Selective Decoding explicitly models sequence
generation and sequence labeling, which enables
a standalone evaluation of the sequence generation
process (i.e., the final output without predicate and
argument labels). To make a baseline comparison,
we train an OpenNMT system (Klein et al., 2017)
directly on the same data ignoring the labels.

OpenNMT Selective Decoding
24.92 25.16

Table 3: Evaluation results (BLEU) of sequence
generation on the test set.

Table 3 shows the BLEU score of sequence
generation on the test set. Selective Decoding
achieves higher BLEU than OpenNMT. It demon-
strates that the selective decoding mechanism

7In linearized predicates, arguments are replaced by
placeholders. For example, the linearized PredPatt in
Fig. 1(d) becomes “[ ?arg started:ph Sth:= [ ?arg firing:ph
?arg ] ]” after replacement.

837



learning with extra labels helps improve the qual-
ity of sequence generation. We also notice that
the BLEU score (25.16 in Table 2) of the final lin-
earized PredPatt from Selective Decoding is even
higher than OpenNMT (24.92 in Table 3). Hence,
we can draw a conclusion that simply placing a
labeler atop the OpenNMT system to tackle the
cross-lingual open IE problem will not narrow the
gap in BLEU between itself and our Selective De-
coding approach.

5.2.5 Evaluation using F1

Approach Predicate Argument

Pipeline-S 24.24 33.54
Pipeline-N 24.41 33.51
Joint Moses 25.11 38.90
Joint Seq2Seq 25.79 34.44

Selective Decoding 31.71 40.81
- pretrained embeddings 31.56 40.81
- beam search 30.06 39.06

Table 4: Evaluation results (F1) of predicates and
arguments on the test set.

We compute the token-level F1 score (Liu et al.,
2015) of predicates and arguments. As shown
in Table 4, Selective Decoding substantially im-
proves the F1 score of both predicates and argu-
ments. In the ablation test, pretrained word em-
beddings slightly improve F1 of predicates, but
have no improvement on F1 of arguments. Beam
search helps improve the score of both.

5.2.6 Recoverability
We compute the number of the linearized PredPatt
outputs from which the tree structure representa-
tion can not be recovered, including the empty out-
puts and the outputs which have unmatched brack-
ets, or have zero or multiple heads for an argument
or a predicate. As shown in Table 5, compared to
the previous best Joint Seq2Seq approach, Selec-
tive Decoding further reduces the number of unre-
coverable outputs by one order of magnitude.

Pipeline-
S

Pipeline-
N

Joint
Moses

Joint
Seq2Seq

Selective
Decoding

5,965 6,014 33,178 557 53

Table 5: Number of unrecoverable outputs.

5.2.7 Analysis
To analyze the difference between Selective De-
coding and the previous best approach Joint

Seq2Seq, we plot the BLEU scores of the lin-
earized PredPatt on the test set with respect to
the lengths of the reference. As shown in Fig. 4,
when the reference length is greater than 20, the
linearized PredPatt generated by Selective Decod-
ing gets notably better BLEU scores, especially for
the reference length around 30. However, when
the reference length is shorter than 11, the per-
formance of Selective Decoding drops below the
Joint Seq2Seq approach.

10 15 20 25 30 35 40 45 50
Sequence Length

0

5

10

15

20

25

30

B
L

E
U

Selective Decoding

Joint Seq2Seq

Figure 4: BLEU scores of the linearized PredPatt
on the test set w.r.t. the lengths of the references.

To explain this performance drop, we randomly
sample an example from the test set, where the
reference length is shorter than 11, and the BLEU
score of the linearized PredPatt generated by Joint
Seq2Seq is higher than Selective Decoding. The
example is shown in Table 6.

Input sentence and its English translation:

我哪怕有千分之一的希望呢 ,我死
活都要给他做最后的
(Even if there was only a one thousandth
of a hope , er , live or die I would give
him my all.)

Reference8:

(1) (I) would give (him) (my all)

Selective Decoding:

(1) Even if (we) have (a UNK per cent hope)

(2) uh , (I) would have SOMETHING9

(3) SOMETHING := (I) give (him) (the final thing)

Joint Seq2Seq:

(1) (I) wish (everyone) (last hope)

Table 6: Example outputs with the reference
length shorter than 11.

838



In this example (Table 6), the Chinese input sen-
tence has a grammatical error: the object modified
by “最后 的” is missing. Additionally, the ref-
erence linearized PredPatt output in this example
is incomplete: the fact related to the concessive
clause is missing. Although here Joint Seq2Seq
gets the better BLEU score against the incomplete
reference, Selective Decoding is able to better gen-
erlize over the train data: the facts it generates
are much closer to the original input sentence, and
even better than the reference.

Input sentence and its English translation:
结果 ,民主党失去了列举布什 “罪状 ”的良
机 ,
(As a result, the Democratic party lost a good
opportunity to list the ‘ charges ’ against Bush.

Reference:

(1) As (a result) , (the Democratic party) lost (a good
opportunity)

(2) (a good opportunity) list (the ‘ charges ’ against Bush)

Selective Decoding:

(1) As (a result) , (the Democratic Party) lost (the good
opportunity)

(2) (the good opportunity) cite (Bush)

Joint Seq2Seq:

(1) (The result) is SOMETHING

(2) SOMETHING := (the Democratic Party) lost (his
opportunity)

(3) (his opportunity) give (him) (good opportunity)

Table 7: Example outputs with the reference
length longer than 20.

Another example where the reference length
is greater than 20 is shown in Table 7. In this
example, Selective Decoding generates the same
number of facts as the reference, and the mean-
ing of the facts is closer to the reference than
Joint Seq2Seq: though not perfect, Selective De-
coding captures “列举 布什 ‘ 罪状 ’” (“list
the ‘charges’ against Bush”) by generating “cite
(Bush)”, whereas Joint Seq2Seq fails to generate
any thing related.

8The predicate tokens are colored blue, and the argument
tokens are colored purple. Head tokens are underlined in
bold. Token labels and brackets are omitted.

9“SOMETHING” is a special argument used to indicate that
the argument is a proposition.

5.3 Low-resource Scenarios
One of the goals of cross-lingual open IE is to
extract facts from languages for which few NLP
resources and tools are available, and represent
the facts in the language for which plenties of re-
sources and tools can be used. Therefore, we ex-
tend the experiments to cross-lingual open IE from
5 languages to English in a low-resource setting.

5.3.1 Datasets

Task #Train #Valid #Test

uzb-eng 31,581 1,373 1,373

tur-eng 20,774 903 903

amh-eng 12,140 527 527

som-eng 10,702 465 465

yor-eng 5,787 251 251

Table 8: Number of data used for cross-lingual
open IE in low-resource scenarios.

To prepare the experiment datasets, we first
collect bitexts from DARPA LORELEI language
packs (Strassel and Tracey, 2016). The source lan-
guages of the bitexts are Uzbek, Turkish, Amharic,
Somali, and Yoruba.10

We then run a process similar to Zhang et al.
(2017a) to generate pairs of source-language sen-
tences and English linearized PredPatt: first, we
employ SyntaxNet Parser (Andor et al., 2016)
to generate Universal Dependency parses for the
English sentences, and then run PredPatt (White
et al., 2016) to generate English linearized Pred-
Patt from the Universal Dependency parses. We
remove empty sequences and very long sequences
(length>50) in the pairs, and randomly split them
into training, validation and test sets in the ratio
of 23:1:1. The detailed number of pairs for each
experiment is shown in Table 8.

5.3.2 Baseline
To compare with Selective Decoding, we imple-
ment the Joint Seq2Seq approach which uses a
standard encoder-decoder model as the baseline.

5.3.3 Evaluation Results
We train the models of Selective Decoding and
Joint Seq2Seq respectively using the same hyper-

10These bitexts are from LDC2016E29 (uzb-eng);
LDC2014E115 (tur-eng); LDC2016E86 and LDC2016E87
(amh-eng); LDC2016E90 and LDC2016E91 (som-eng);
LDC2016E104 and LDC2016E105 (yor-eng).

839



parameters setting described in § 5.1, and evalu-
ate the results on the test set by computing the
cased BLEU score of the linearized PredPatt, and
the token-level F1 scores.

Task JointS2S
Selective
Decoding

uzb-eng 8.66 10.76
tur-eng 7.18 7.47

amh-eng 7.18 8.37
som-eng 10.61 13.06
yor-eng 11.31 12.19

Table 9: Evaluation results in low-resource cross-
lingual open IE scenarios: BLEU of linearized
PredPatt.

Table 9 shows the evaluation results using
BBLEU. Selective Decoding outperforms the Joint
Seq2Seq approach by 0.29-2.45, which is ex-
pected since Selective Decoding employs the de-
coder solely for sequence generation and the se-
lector solely for sequence labeling.

Task Predicate ArgumentJoint
S2S

Selective
Decoding

Joint
S2S

Selective
Decoding

uzb-eng 12.50 12.46 19.57 24.08
tur-eng 9.89 6.49 17.39 17.76

amh-eng 8.44 8.82 17.31 18.58
som-eng 13.64 13.91 22.81 25.38
yor-eng 11.97 10.74 22.61 25.57

Table 10: Evaluation results in low-resource cross-
lingual open IE scenarios: the token-level F1 of
predicates and arguments.

The F1 score measure is shown in Table 10
where both the F1 scores of predicates and argu-
ments are computed seperately. For predicates,
Selective Decoding shows no advantage to the
Joint Seq2Seq approach. In the tur-eng task, its F1
score is obviously worse than the baseline. How-
ever, Selective Decoding in general shows promis-
ing results in the argument F1 scores.

6 Conclusions

In this paper, we recast cross-lingual open IE as
a more general problem, which involves sequence
generation and sequence labeling. We propose a
novel encoder-decoder model which employs the
selective decoding mechanism to explicitly model
the sequence generation and sequence labeling
process. Experimental results show our approach
achieves consistent and significant improvements
in a variety of cross-lingual open IE scenarios.

Since the selective decoding mechanism is not
limited to cross-lingual open IE, we believe that
it will also benefit other NLP tasks which can be
generalized as jointly doing sequence generation
and sequence labeling. In the future, we plan to
investigate its effectiveness to tasks such as cross-
lingual information retrieval.

Acknowledgments

Thank you to the anonymous reviewers for their
feedback. This work was supported in part by
the JHU Human Language Technology Center of
Excellence (HLTCOE), and DARPA LORELEI.
The U.S. Government is authorized to reproduce
and distribute reprints for Governmental purposes.
The views and conclusions contained in this pub-
lication are those of the authors and should not be
interpreted as representing official policies or en-
dorsements of DARPA or the U.S. Government.

References
Daniel Andor, Chris Alberti, David Weiss, Aliaksei

Severyn, Alessandro Presta, Kuzman Ganchev, Slav
Petrov, and Michael Collins. 2016. Globally nor-
malized transition-based neural networks. In Pro-
ceedings of the 54th Annual Meeting of the Associa-
tion for Computational Linguistics (Volume 1: Long
Papers), pages 2442–2452, Berlin, Germany. Asso-
ciation for Computational Linguistics.

Gabor Angeli, Melvin Jose Johnson Premkumar, and
Christopher D. Manning. 2015. Leveraging linguis-
tic structure for open domain information extraction.
In Proceedings of the 53rd Annual Meeting of the
Association for Computational Linguistics and the
7th International Joint Conference on Natural Lan-
guage Processing (Volume 1: Long Papers), pages
344–354, Beijing, China. Association for Computa-
tional Linguistics.

Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben-
gio. 2014. Neural machine translation by jointly
learning to align and translate. arXiv preprint
arXiv:1409.0473.

Michele Banko, Michael J. Cafarella, Stephen Soder-
land, Matt Broadhead, and Oren Etzioni. 2007.
Open information extraction from the web. In Pro-
ceedings of the 20th International Joint Conference
on Artifical Intelligence, IJCAI’07, pages 2670–
2676, San Francisco, CA, USA. Morgan Kaufmann
Publishers Inc.

Samuel R. Bowman, Jon Gauthier, Abhinav Ras-
togi, Raghav Gupta, Christopher D. Manning, and
Christopher Potts. 2016. A fast unified model for
parsing and sentence understanding. In Proceed-
ings of the 54th Annual Meeting of the Association

840



for Computational Linguistics (Volume 1: Long Pa-
pers), pages 1466–1477, Berlin, Germany. Associa-
tion for Computational Linguistics.

Kyunghyun Cho, Bart van Merrienboer, Caglar Gul-
cehre, Dzmitry Bahdanau, Fethi Bougares, Holger
Schwenk, and Yoshua Bengio. 2014. Learning
phrase representations using rnn encoder–decoder
for statistical machine translation. In Proceedings of
the 2014 Conference on Empirical Methods in Nat-
ural Language Processing (EMNLP), pages 1724–
1734, Doha, Qatar. Association for Computational
Linguistics.

Li Dong and Mirella Lapata. 2016. Language to logi-
cal form with neural attention. In Proceedings of the
54th Annual Meeting of the Association for Compu-
tational Linguistics (Volume 1: Long Papers), pages
33–43, Berlin, Germany. Association for Computa-
tional Linguistics.

Anthony Fader, Stephen Soderland, and Oren Etzioni.
2011. Identifying relations for open information ex-
traction. In Proceedings of the 2011 Conference on
Empirical Methods in Natural Language Process-
ing, pages 1535–1545, Edinburgh, Scotland, UK.
Association for Computational Linguistics.

Karl Moritz Hermann, Tomas Kocisky, Edward
Grefenstette, Lasse Espeholt, Will Kay, Mustafa Su-
leyman, and Phil Blunsom. 2015. Teaching ma-
chines to read and comprehend. In C. Cortes, N. D.
Lawrence, D. D. Lee, M. Sugiyama, and R. Garnett,
editors, Advances in Neural Information Processing
Systems 28, pages 1693–1701. Curran Associates,
Inc.

Sepp Hochreiter and Jürgen Schmidhuber. 1997.
Long short-term memory. Neural computation,
9(8):1735–1780.

Sébastien Jean, Kyunghyun Cho, Roland Memisevic,
and Yoshua Bengio. 2015. On using very large
target vocabulary for neural machine translation.
In Proceedings of the 53rd Annual Meeting of the
Association for Computational Linguistics and the
7th International Joint Conference on Natural Lan-
guage Processing (Volume 1: Long Papers), pages
1–10, Beijing, China. Association for Computa-
tional Linguistics.

Nal Kalchbrenner and Phil Blunsom. 2013. Recurrent
continuous translation models. In Proceedings of
the 2013 Conference on Empirical Methods in Natu-
ral Language Processing, pages 1700–1709, Seattle,
Washington, USA. Association for Computational
Linguistics.

Andrej Karpathy and Li Fei-Fei. 2015. Deep visual-
semantic alignments for generating image descrip-
tions. In Proceedings of the IEEE Conference on
Computer Vision and Pattern Recognition, pages
3128–3137.

Diederik Kingma and Jimmy Ba. 2014. Adam: A
method for stochastic optimization. arXiv preprint
arXiv:1412.6980.

G. Klein, Y. Kim, Y. Deng, J. Senellart, and A. M.
Rush. 2017. OpenNMT: Open-Source Toolkit for
Neural Machine Translation. ArXiv e-prints.

Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, et al. 2007. Moses: Open source
toolkit for statistical machine translation. In Pro-
ceedings of the 45th annual meeting of the ACL on
interactive poster and demonstration sessions, pages
177–180. Association for Computational Linguis-
tics.

Zhengzhong Liu, Teruko Mitamura, and Eduard Hovy.
2015. Evaluation algorithms for event nugget detec-
tion: A pilot study. In Proceedings of the 3rd Work-
shop on EVENTS at the NAACL-HLT, pages 53–57.

Di Lu, Xiaoman Pan, Nima Pourdamghani, Shih-Fu
Chang, Heng Ji, and Kevin Knight. 2016. A multi-
media approach to cross-lingual entity knowledge
transfer. In Proceedings of the 54th Annual Meet-
ing of the Association for Computational Linguistics
(Volume 1: Long Papers), pages 54–65, Berlin, Ger-
many. Association for Computational Linguistics.

Minh-Thang Luong, Hieu Pham, and Christopher D.
Manning. 2015. Effective approaches to attention-
based neural machine translation. In Proceedings of
the 2015 Conference on Empirical Methods in Nat-
ural Language Processing, pages 1412–1421, Lis-
bon, Portugal. Association for Computational Lin-
guistics.

Razvan Pascanu, Tomas Mikolov, and Yoshua Ben-
gio. 2013. On the difficulty of training recurrent
neural networks. In Proceedings of The 30th In-
ternational Conference on Machine Learning, pages
1310–1318.

Jeffrey Pennington, Richard Socher, and Christo-
pher D. Manning. 2014. Glove: Global vectors for
word representation. In Empirical Methods in Nat-
ural Language Processing (EMNLP), pages 1532–
1543.

Alexander M. Rush, Sumit Chopra, and Jason Weston.
2015. A neural attention model for abstractive sen-
tence summarization. In Proceedings of the 2015
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 379–389, Lisbon, Portugal.
Association for Computational Linguistics.

Mike Schuster and Kuldip K Paliwal. 1997. Bidirec-
tional recurrent neural networks. IEEE Transactions
on Signal Processing, 45(11):2673–2681.

Matthew Snover, Nitin Madnani, Bonnie J Dorr, and
Richard Schwartz. 2009. Fluency, adequacy, or
hter?: exploring different human judgments with a
tunable mt metric. In Proceedings of the Fourth

841



Workshop on Statistical Machine Translation, pages
259–268. Association for Computational Linguis-
tics.

Nitish Srivastava, Geoffrey E Hinton, Alex Krizhevsky,
Ilya Sutskever, and Ruslan Salakhutdinov. 2014.
Dropout: a simple way to prevent neural networks
from overfitting. Journal of Machine Learning Re-
search, 15(1):1929–1958.

Stephanie Strassel and Jennifer Tracey. 2016. Lorelei
language packs: Data, tools, and resources for
technology development in low resource languages.
In Proceedings of the Tenth International Confer-
ence on Language Resources and Evaluation (LREC
2016), Paris, France. European Language Resources
Association (ELRA).

Kiyoshi Sudo, Satoshi Sekine, and Ralph Grishman.
2004. Cross-lingual information extraction sys-
tem evaluation. In Proceedings of the 20th inter-
national Conference on Computational Linguistics,
page 882. Association for Computational Linguis-
tics.

Ilya Sutskever, Oriol Vinyals, and Quoc V Le. 2014.
Sequence to sequence learning with neural net-
works. In Advances in neural information process-
ing systems, pages 3104–3112.

Zhaopeng Tu, Yang Liu, Zhengdong Lu, Xiaohua Liu,
and Hang Li. 2016. Context gates for neural ma-
chine translation. arXiv preprint arXiv:1608.06043.

Oriol Vinyals, Łukasz Kaiser, Terry Koo, Slav Petrov,
Ilya Sutskever, and Geoffrey Hinton. 2015a. Gram-
mar as a foreign language. In Advances in Neural
Information Processing Systems, pages 2773–2781.

Oriol Vinyals, Alexander Toshev, Samy Bengio, and
Dumitru Erhan. 2015b. Show and tell: A neural im-
age caption generator. In Proceedings of the IEEE
Conference on Computer Vision and Pattern Recog-
nition, pages 3156–3164.

Aaron Steven White, Drew Reisinger, Keisuke Sak-
aguchi, Tim Vieira, Sheng Zhang, Rachel Rudinger,
Kyle Rawlins, and Benjamin Van Durme. 2016.
Universal decompositional semantics on universal
dependencies. In Proceedings of the 2016 Confer-
ence on Empirical Methods in Natural Language
Processing, pages 1713–1723, Austin, Texas. Asso-
ciation for Computational Linguistics.

Kelvin Xu, Jimmy Ba, Ryan Kiros, Kyunghyun Cho,
Aaron C Courville, Ruslan Salakhutdinov, Richard S
Zemel, and Yoshua Bengio. 2015. Show, attend and
tell: Neural image caption generation with visual at-
tention. In ICML, volume 14, pages 77–81.

Sheng Zhang, Kevin Duh, and Benjamin Van Durme.
2017a. Mt/ie: Cross-lingual open information ex-
traction with neural sequence-to-sequence models.
In Proceedings of the 15th Conference of the Euro-
pean Chapter of the Association for Computational
Linguistics: Volume 2, Short Papers, pages 64–70,

Valencia, Spain. Association for Computational Lin-
guistics.

Sheng Zhang, Rachel Rudinger, Kevin Duh, and Ben
Van Durme. 2017b. Ordinal common-sense infer-
ence. Transactions of the Association for Computa-
tional Linguistics.

Sheng Zhang, Rachel Rudinger, and Ben Van Durme.
2017c. An evaluation of predpatt and open ie via
stage 1 semantic role labeling. In Proceedings of
the 12th International Conference on Computational
Semantics (IWCS), Montpellier, France.

842


