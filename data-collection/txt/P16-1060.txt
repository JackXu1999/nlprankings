



















































Which Coreference Evaluation Metric Do You Trust? A Proposal for a Link-based Entity Aware Metric


Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 632–642,
Berlin, Germany, August 7-12, 2016. c©2016 Association for Computational Linguistics

Which Coreference Evaluation Metric Do You Trust?
A Proposal for a Link-based Entity Aware Metric

Nafise Sadat Moosavi and Michael Strube
Heidelberg Institute for Theoretical Studies gGmbH

Schloss-Wolfsbrunnenweg 35
69118 Heidelberg, Germany

{nafise.moosavi|michael.strube}@h-its.org

Abstract

Interpretability and discriminative power
are the two most basic requirements for
an evaluation metric. In this paper, we re-
port the mention identification effect in the
B3, CEAF, and BLANC coreference eval-
uation metrics that makes it impossible to
interpret their results properly. The only
metric which is insensitive to this flaw is
MUC, which, however, is known to be the
least discriminative metric. It is a known
fact that none of the current metrics are
reliable. The common practice for rank-
ing coreference resolvers is to use the av-
erage of three different metrics. However,
one cannot expect to obtain a reliable score
by averaging three unreliable metrics. We
propose LEA, a Link-based Entity-Aware
evaluation metric that is designed to over-
come the shortcomings of the current eval-
uation metrics. LEA is available as branch
LEA-scorer in the reference implemen-
tation of the official CoNLL scorer.

1 Introduction

There exists a variety of models (e.g. pairwise,
entity-based, and ranking) and feature sets (e.g.
string match, lexical, syntactic, and semantic) to
be used in coreference resolution. There is no
known formal way to prove which coreference
model is superior to the others and which set of
features is more beneficial/less useful in corefer-
ence resolution. The only way to compare differ-
ent models, features or implementations of coref-
erence resolvers is to compare the values of the
existing coreference resolution evaluation metrics.
By comparing the evaluation scores, we determine
which system performs best, which model suits
coreference resolution better, and which feature

set is useful for improving the recall or precision
of a coreference resolver. Therefore, evaluation
metrics play an important role in the advancement
of the underlying technology. It is imperative for
the evaluation metrics to be reliable. However, it
is not a trivial task to score output entities with
various kinds of coreference errors.

Several evaluation metrics have been introduced
for coreference resolution (Vilain et al., 1995;
Bagga and Baldwin, 1998; Luo, 2005; Recasens
and Hovy, 2011; Tuggener, 2014). Metrics that
are being used widely are MUC (Vilain et al.,
1995), B3 (Bagga and Baldwin, 1998), CEAF
(Luo, 2005), and BLANC (Recasens and Hovy,
2011). There are known flaws for each of these
metrics. Besides, the agreement between all these
metrics is relatively low (Holen, 2013), and it is
not clear which metric is the most reliable. The
CoNLL-2011/2012 shared tasks (Pradhan et al.,
2011; Pradhan et al., 2012) ranked participating
systems using an average of three metrics, i.e.
MUC, B3, and CEAF, following a proposal by
(Denis and Baldridge, 2009a). Averaging three
unreliable scores does not result in a reliable one.
Besides, when an average score is used for com-
parisons, it is not possible to analyse recall and
precision to determine which output is more pre-
cise and which one covers more coreference infor-
mation. This is indeed a requirement for corefer-
ence resolvers to be used in end-tasks. Therefore,
averaging individual metrics is nothing but a com-
promise.

As mentioned by Luo (2005), interpretability
and discriminative power are two basic require-
ments for a reasonable evaluation metric. In regard
to the interpretability requirement a high score
should indicate that the vast majority of corefer-
ence relations and entities are detected correctly.
Similarly, a system that resolves none of the coref-
erence relations or entities should get a zero score.

632



MUC B3 CEAFe BLANC
R P F1 R P F1 R P F1 R P F1

Base 69.31 76.23 72.60 55.83 66.07 60.52 54.88 59.41 57.05 57.46 65.77 61.31
More precise 69.31 82.29 75.24 53.94 69.32 60.67 50.92 55.85 53.27 53.60 66.45 59.25
Less precisea 69.31 69.46 69.38 60.53 63.98 62.21 64.82 53.06 58.35 68.68 67.02 67.68
Less preciseb 69.31 74.70 71.90 60.61 69.14 64.60 69.50 47.61 56.51 68.74 67.37 67.87

Table 1: Counterintuitive values of B3, CEAF and BLANC recall and precision.

An evaluation metric should also be discrimina-
tive. It should be able to discriminate between
good and bad coreference decisions. In this pa-
per, we report on a drawback for B3, CEAF, and
BLANC which violates the interpretability require-
ment. We also show that this flaw invalidates
the recall/precision analysis of coreference out-
puts based on these three metrics. We then review
the current evaluation metrics with their known
flaws to explain why we cannot trust them and
need a new reliable one. Finally, we propose LEA,
a Link-based Entity Aware evaluation metric that
is designed to overcome problems of the existing
metrics. We have begun the process of integrating
the LEA metric in the official CoNLL scorer1 so
as to continue the progress made in recent years to
produce replicable evaluation metrics. In order to
use the LEA metric, there is no additional require-
ment than that of the CoNLL scorer v8.01 2.

2 The Mention Identification Effect

All the proposed evaluation metrics for corefer-
ence resolution use recall, precision and F1 for re-
porting the performance of a coreference resolver.
Recall is an indicator of the fraction of correct
coreference information, i.e. coreference links or
entities, that is resolved. Precision is an indicator
of the fraction of resolved coreference information
that is correct. F1 is the weighted harmonic mean
of recall and precision.

While we usually use F1 for comparing coref-
erence resolution systems, it is also important for
the corresponding recall and precision values to
be interpretable and discriminative. Coreference
resolution is not an end-task itself but it is an im-
portant step toward text understanding. Depend-
ing on the task, recall or precision may be more
important. For example, as Stuckhardt (2003) ar-
gues, a coreference resolver needs high precision

1Currently available as branch LEA-scorer
in https://github.com/conll/
reference-coreference-scorers.

2LEA scores will be obtained by running the command
perl scorer.pl lea goldFile systemFile.

to meet the specific requirements of text summa-
rization and question answering.

In this section, we show that the recall and pre-
cision of the B3, CEAF and BLANC metrics are
neither interpretable nor reliable. We choose the
output of the state-of-the-art coreference resolver
of Wiseman et al. (2015) on the CoNLL 2012 En-
glish test set as the base output. The CoNLL 2012
English test set contains 222 documents (compris-
ing 348 partially annotated sections). This test set
contains 19,764 coreferring mentions that belong
to 4,532 different entities.

In Table 1, Base represents the scores of (Wise-
man et al., 2015) on the CoNLL 2012 test set.
All reported scores in this paper are computed by
the official CoNLL scorer v8.01 (Pradhan et al.,
2014).

Assume Mk,r is the set of mentions that exists
in both key and response entities. Let Lk(m) and
Lr(m) be the set of coreference links of mention
m in the key and response entities, respectively.
Mention m is an incorrectly resolved mention if
m ∈ Mk,r and Lk(m) ∩ Lr(m) = ∅. There-
fore, m is a coreferent mention that has at least
one coreference link in the response entities. How-
ever, none of its detected coreference links in the
response entities are correct.

By removing the incorrectly resolved mentions,
the response entities will become more precise.
The precision improves because the wrong links
that are related to the incorrectly resolved men-
tions have been removed. Besides, the recall will
not change because no correct coreference rela-
tions or entities have been added or removed.

We make the Base output more precise by re-
moving all 1075 incorrectly resolved mentions
from the response entities. The score for this more
precise output is shown as More precise in Table 1.
As can be seen, (1) recall changes for all the met-
rics except for MUC; (2) both CEAFe recall and
precision significantly decrease; and (3) BLANC
recall notably decreases so that F1 drops signifi-
cantly in comparison to Base.

On the other hand, adding completely incorrect

633



entities to the response entities should not affect
the recall and it should decrease the precision.

Assume Md,k,r̄ is the set of mentions of docu-
ment d that exists in the key entities but is missing
from the response entities. We can add completely
incorrect entities to the Base output as follows: (1)
By linking m1 ∈ Md,k,r̄ to mention m2 ∈ Md,k,r̄
that is non-coreferent with m1. All the new wrong
entities are of size two (Less precisea). (2) By
linking m1 ∈Md,k,r̄ to all mentions of Md,k,r̄ that
are non-coreferent with m1. In this case the new
entities are larger but their number is smaller (Less
preciseb). The number of new entities is 1350 and
283 for the first and second case, respectively. As
can be seen from the results of Table 1, (1) re-
call changes for all metrics except for MUC; and
(2) the B3, CEAF and BLANC scores improve sig-
nificantly over those of Base when the output is
doubtlessly worse.

These experiments show that B3, CEAF and
BLANC are not reliable for recall-precision anal-
ysis. We refer to the problem that is causing these
contradictory results as the mention identifica-
tion effect.

3 Reasons for the Unreliable Results

In this section, we briefly give an overview of the
common evaluation metrics for coreference reso-
lution. We also discuss the shortcomings of each
metric, including the mention identification ef-
fect, that may lead to counterintuitive and unreli-
able results. In all metrics, K is the key entity set
and R is the response entity set.

3.1 MUC

MUC is the earliest systematic coreference eval-
uation metric and is introduced by Vilain et al.
(1995). MUC is a link-based metric. It computes
recall based on the minimum number of missing
links in the response entities in comparison to the
key entities. MUC recall is defined as:

Recall =

∑
ki∈K(|ki| − |p(ki)|)∑

ki∈K(|ki| − 1)

where p(ki) is the set of partitions that is created
by intersecting ki with the corresponding response
entities. MUC precision is computed by switching
the role of the key and response entities.

It is not trivial to determine which evaluation
metric discriminates coreference responses best.

However, MUC is known to be the least discrim-
inative coreference resolution metric (Bagga and
Baldwin, 1998; Luo, 2005; Recasens and Hovy,
2011). The MUC evaluation is only based on the
minimum number of missing/extra links in the re-
sponse compared to the key entities. For instance,
MUC does not differentiate whether an extra link
merges two singletons or the two most prominent
entities of the text. However, the latter error does
more damage than the first one.

Another major problem with MUC is that it has
an incorrect preference in ranking coreference
outputs. MUC favors the outputs in which entities
are over-merged (Luo, 2005). For instance, if we
link all the key mentions of the CoNLL 2012 test
set into a single response entity, the correspond-
ing MUC scores, i.e. Recall=100, Precision=78.44
and F1=87.91, will be all higher than those of the
state-of-the-art system (Base in Table 1).

3.2 BCUBED
The B3 score is introduced by Bagga and Bald-
win (1998). B3 is a mention-based metric, i.e., the
overall recall/precision is computed based on the
recall/precision of the individual mentions. For
each mention m in the key entities, B3 recall con-
siders the fraction of the correct mentions that are
included in the response entity of m. B3 recall is
computed as follows:

Recall =

∑
ki∈K

∑
rj∈R

|ki∩rj |2
|ki|∑

ki∈K |ki|

Similar to MUC, B3 precision is computed by
switching the role of the key and response entities.

The mention identification effect arises in B3,
because B3 uses mentions instead of coreference
relations to evaluate the response entities. There-
fore, if a mention exists in a response entity, it
is considered as a resolved mention regardless of
whether it has a correct coreference relation in the
response entity.

Luo (2005) argues that B3 leads to counter-
intuitive results for boundary cases: (1) con-
sider a system that makes no decision and leaves
every key mention as a singleton. B3 precision
for this system is 100%. However, not all of the
recognized system entities (i.e. singletons), or the
detected coreference relations (i.e. every mention
only coreferent with itself) are correct; (2) con-
sider a system that merges all key mentions into a
single entity. B3 recall for this system is 100%.

634



Luo (2005) interprets this recall as counterintu-
itive because the key entities have not been found
in the response. The intuitiveness or counterintu-
itiveness of this recall value depends on the eval-
uator’s point of view. From one point of view, all
of the key mentions, that are supposed to be in the
same entity, are indeed in the same entity.

Finally, as discussed by Luo and Pradhan
(2016), B3 cannot properly handle repeated men-
tions in the response entities. If a gold mention
is repeated in several response entities, B3 receives
credit for all the repetitions. The repeated response
mentions issue is not an imaginary problem (Luo
and Pradhan, 2016). It can happen if system men-
tions are read from a parse tree where an NP node
has a single child, a pronoun, and where both the
nodes are considered as candidate mentions.

3.3 CEAF
The CEAF metric is introduced by Luo (2005).
CEAF’s main assumption is that each key entity
should only be mapped to one reference entity, and
vice versa. CEAF uses a similarity measure (φ) to
evaluate the similarity of two entities. It uses the
Kuhn-Munkres algorithm to find the best one-to-
one mapping of the key to the response entities
(g∗) using the given similarity measure. Assum-
ing K∗ is the set of key entities that is included in
the optimal mapping, recall is computed as:

Recall =

∑
ki∈K∗ φ(ki, g

∗(ki))∑
ki∈K φ(ki, ki)

(1)

For computing CEAF precision, the denomina-
tor of Equation 1 is changed to

∑
Ri∈R φ(ri, ri).

Based on φ, there are two variants of CEAF: (1)
mention-based CEAF (CEAFm), which computes
the similarity as the number of common mentions
between two entities, i.e. φ(ki, rj) = |ki ∩ rj |;
and (2) entity-based CEAF (CEAFe), in which
φ(ki, rj) =

2×|ki∩rj |
|ki|+|rj | . The denominator of Equa-

tion 1 for CEAFe is the number of key entities.
Similar to B3, the mention identification ef-

fect of CEAF is caused by both similarity mea-
sures of CEAF using the number of common men-
tions between two entities, i.e. |ki ∩ rj |. In this
way, even if the two mapped entities (ki and rj)
have only one mention in common, CEAFm re-
wards recall and precision by 1∑

ki
|ki| and

1∑
rj
|rj | ,

respectively. CEAFe rewards recall and precision
by 2(|ki|+|rj |)×|K| and

2
(|ki|+|rj |)×|R| , respectively.

If instead of the number of common mentions,

[The American administration](1) committed a fatal
mistake when [it1](1) [executed](2) [this man](3), in a
way for which [it2](1) will pay a hefty price in the
near future. [[His1](3) survival](4) would have bene-
fited [it3](1) much more than [[his2](3) execution](2) if
[they1](1) understood politics as [they2](1) should, be-
cause [[his3](3) survival](4) could have been a card to
threaten [the sectarians](5) and keep [them1](5) as ser-
vants to [them1](1) and [their](1) schemes.

Figure 1: Sample text from CoNLL 2012.

Response entities

cr1
r1={the American administration, it1, it2, it3} ,
r2={they1, they2, them, their}

cr2 r1={the American administration, it1, it2, it3}

Table 2: Different system outputs for Figure 1.

we would use the number of common coreference
links between two entities in both CEAFm and
CEAFe similarity measures, this problem would
be solved. However, even if we handle the men-
tion identification effect by using coreference re-
lations rather than mentions in the similarity mea-
sures, CEAF may still result in counterintuitive
results. As mentioned by Denis and Baldridge
(2009b), CEAF ignores all correct decisions of
unaligned response entities that may lead to un-
reliable results. In order to illustrate this, we use
a sample text from the CoNLL 2012 development
set as an example (Figure 1). Gold mentions are
enclosed in square brackets. Mentions with the
same text are marked with different indices. The
indices in parentheses denote to which key entity
the mentions belong Consider cr1 and cr2 in Ta-
ble 2, which are different responses for entity (1)
of Figure 1. cr1 resolves many coreference rela-
tions of entity (1). However, it misses that they1
could refer to an entity which is already referred
to by ’it’. Therefore cr1 produces two entities in-
stead of one because of this missing relation. On
the other hand, cr2 only recognizes half of the cor-
rect coreference relations of entity (1).

As can be seen from Table 3, CEAF prefers cr2
over cr1 even though cr1 makes more correct deci-
sions. CEAF only selects one of the output entities
of cr1 for giving credit to the correct decisions.

MUC B3 CEAFm CEAFe BLANC
cr1 92.30 66.66 50.00 44.44 60.00
cr2 60.00 40.00 66.66 66.66 32.29

Table 3: F1 scores for Table 2’s response entities.

635



The other response entity is only used for penal-
izing the precision of cr1. This counterintuitive
result is only because of the stringent constraint of
CEAF that the mapping of key to response entities
should be one-to-one.

Another problem with CEAFe, mentioned by
Stoyanov et al. (2009), is that it weights entities
equally regardless of their sizes. The system that
does not detect entity (1), the most prominent en-
tity of Figure 1, gets the same score as that of a
system which does not detect entity (4) of size 2.

3.4 BLANC
BLANC (Recasens and Hovy, 2011; Luo et al.,
2014) is a link-based metric that adapts the Rand
index (Rand, 1971) to coreference resolution eval-
uation. Let Ck and Cr be the sets of coreference
links in the key and response entities, respectively.
AssumeNk andNr are the sets of non-coreference
links in the key and response entities, respectively.
Recall and precision of coreference links are com-
puted as:

Rc =
|Ck ∩ Cr|
|Ck| , Pc =

|Ck ∩ Cr|
|Cr|

Recall and precision of non-coreference links are
computed as:

Rn =
|Nk ∩Nr|
|Nk| , Pn =

|Nk ∩Nr|
|Nr|

BLANC recall and precision are computed by av-
eraging the recall and precision of coreference and
non-coreference links, e.g. Recall= Rc+Rn2 .

The BLANC measure is the newest but the
least popular metric for evaluating coreference re-
solvers. Because of considering non-coreferent
relations, the mention identification effect af-
fects BLANC most strongly. When the number
of gold mentions that exist in the response entities
is larger, the number of detected non-coreference
links will also get larger. Therefore, it results in
higher values for BLANC recall and precision ig-
noring whether those gold mentions are resolved.

4 LEA

In this section, we present our new evaluation met-
ric, namely the Link-Based Entity-Aware metric
(LEA). LEA is designed to overcome the shortcom-
ings of the current evaluation metrics.

For each entity, LEA considers how important
the entity is and how well it is resolved. Therefore,

LEA evaluates a set of entities as follows:∑
ei∈E(importance(ei)× resolution-score(ei))∑

ek∈E importance(ek)

We consider the size of an entity as a measure of
importance, i.e. importance(e) = |e|. There-
fore, the more prominent entities of the text get
higher importance values. However, according to
the end-task or domain used, one can choose other
importance measures based on factors besides ei’s
size, e.g. ei’s entity type or ei’s mention types. For
example, as suggested by Holen (2013), each men-
tion carries different information values, and con-
sidering this information could benefit the quan-
titative evaluation of coreference resolution. The
importance measure of LEA is the appropriate
place to incorporate this kind of information.

Entity e with n mentions has link(e) = n ×
(n−1)/2 unique coreference links. The resolution
score of key entity ki is computed as the fraction
of correctly resolved coreference links of ki:

resolution-score(ki) =
∑
rj∈R

link(ki ∩ rj)
link(ki)

For each ki, LEA checks all the response entities
to see whether they are partial matches for ki. rj
is a partial match for ki, if it contains at least one
of the coreference links of ki. Thus, if a response
entity only contains one mention of ki, it is not a
partial mapping of ki.

Having the definitions of importance and
resolution-score, LEA recall is computed as:

Recall =

∑
ki∈K(|ki| ×

∑
rj∈R

link(ki∩rj)
link(ki)

)∑
kz∈K |kz|

LEA precision is computed by switching the role
of the key and response entities:

Precision =

∑
ri∈R(|ri| ×

∑
kj∈K

link(ri∩kj)
link(ri)

)∑
rz∈R |rz|

LEA handles singletons by self-links. A self-link
is a link connecting a mention to itself. Self-links
indicate that a mention is only coreferent with it-
self and not with other mentions. By considering
self-links, the number of links in a singleton is one.
If entity ki is a singleton, link(ki∩ rj) is one only
if rj is a singleton and contains the same mention
as ki.

In summary, LEA is a link-based metric with the
following properties:

636



– LEA takes into account all coreference links
instead of only extra/missing links. Therefore,
it has more discriminative power than MUC.

– LEA evaluates resolved coreference relations
instead of resolved mentions. LEA also does
not rely on non-coreferent links in order to de-
tect entity structures or singletons. Therefore,
the mention identification effect does not ap-
ply to LEA recall and precision. As a result,
one can trust LEA recall or precision.

– LEA allows one-to-many mappings of entities.
Unlike CEAF, all correct coreference relations
are rewarded by LEA. More splits (or simi-
larly merges) in entity ki result in a smaller∑

rj∈R link(ki ∩ rj). Therefore, splitting
(merging) of an entity in several entities will
be penalized implicitly in resolution-score.

– LEA takes the importance of missing/extra en-
tities into account. Therefore, unlike CEAFe,
it differentiates between the outputs missing
the most prominent and the smallest entities.

– LEA considers resolved coreference relations
instead of resolved mentions. Therefore, the
existence of repeated mentions in different re-
sponse entities is not troublesome for LEA.

5 An Illustrative Example

In this section, we use the example from Pradhan
et al. (2014) to show the process of computing
the LEA scores. In this example, K = {k1 =
{a, b, c}, k2 = {d, e, f, g}} is the set of key en-
tities and R = {r1 = {a, b}, r2 = {c, d}, r3 =
{f, g, h, i}} is the set of response entities.

Here we assume that importance corresponds
to entity size. Hence, importance(k1) = 3
and importance(k2) = 4. The sets of coref-
erence links in k1 and k2 are {ab, ac, bc} and
{de, df, dg, ef, eg, fg}, respectively. Therefore,
link(k1) = 3 and link(k2) = 6. ab is the only
common link between k1 and r1. There are no
common links between k1 and the two other re-
sponse entities. Similarly, k2 has one common
link with r3 and it has no common links with r1
or r2. Therefore, resolution-score(k1) = 1+0+03
and resolution-score(k2) = 0+0+16 . As a result
LEA recall is computed as:∑

importance(ki)× resolution-score(ki)∑
importance(kj)

=
3× 13 + 4× 16

3 + 4
≈ 0.24

By changing the roles of key and response entities,
LEA precision is computed as:

2× 1+01 + 2× 0+01 + 4× 0+16
2 + 2 + 4

≈ 0.33

6 Evaluation on Real Data

Table 4 shows the scores of the state-of-the-art
coreference resolvers developed by Wiseman et al.
(2015), Martschat and Strube (2015), and Peng et
al. (2015). Clark and Manning (2015)’s resolver
is also among the state-of-the-art systems but we
did not have access to their output. Consider-
ing the average score of MUC, B3, and CEAFe,
Martschat, and Peng perform equally. However,
according to LEA, Martschat performs signifi-
cantly better based on an approximate randomiza-
tion test (Noreen, 1989). CEAFe also agrees with
LEA for this ranking. However, CEAFe recall and
precision are similar for Peng while based on LEA,
Peng’s precision is marginally better than recall.

In addition to the state-of-the-art systems, we
report the scores of boundary cases in the CoNLL
2012 test set in Table 4: (1) sys-sing: all system
mentions as singletons; and (2) sys-1ent: all sys-
tem mentions in a single entity.

Table 5 presents the evaluations of the parti-
cipating systems in the CoNLL 2012 shared task
(closed task with predicted mentions). The rank-
ings are specified in parentheses. For the LEA
rankings we also perform a significance test. The
systems without significant differences have the
same ranking. The main difference between the
rankings of avg. and LEA is the rank of xu. Based
on LEA, xu is significantly better than chen and
chunyuang, while avg. ranks these two above xu.
The recall values of chen and chunyuang for men-
tion identification are 75.08 and 75.23, which are
higher than those of the best performing systems,
i.e 72.75 for fernandes, and 74.23 for martschat.
chen and chunyuang include 1850 and 1735 gold
mentions in their outputs that have not a single cor-
rect coreference link. On the other hand, the num-
ber of these gold mentions in xu is 757. Therefore,
these different rankings could be a direct result of
the mention identification effect.

Overall, using one reliable metric instead of an
average score benefits us in two additional ways:
(1) we can perform a significance test to check
whether there is a meaningful difference, and (2)
the recall and precision values are meaningful.

637



MUC B3 CEAFe CoNLL LEA
R P F1 R P F1 R P F1 Avg. F1 R P F1

Wiseman 69.31 76.23 72.60 55.83 66.07 60.52 54.88 59.41 57.05 63.39 51.78 62.12 56.48
Martschat 68.55 77.22 72.63 54.64 66.78 60.11 52.85 60.30 56.33 63.02 50.64 62.87 56.10
Peng 69.54 75.80 72.53 56.91 65.40 60.86 55.49 55.98 55.73 63.04 51.91 58.97 55.21
sys-sing 0.00 0.00 0.00 19.72 39.05 26.20 50.32 4.99 9.08 11.76 0.00 0.00 0.00
sys-1ent 88.01 29.58 44.28 84.87 2.53 4.91 1.50 19.63 2.80 17.33 82.31 2.27 4.43

Table 4: Results on the CoNLL 2012 test set.

MUC B3 CEAFm CEAFe BLANC CoNLL avg. LEA
fernandes 70.51 (1) 57.58 (1) 61.42 53.86 (1) 58.75 60.65 (1) 53.28 (1)
martschat 66.97 (3) 54.62 (2) 58.77 51.46 (2) 55.04 57.68 (2) 49.99 (2)
bjorkelund 67.58 (2) 54.47 (3) 58.19 50.21(3) 55.42 57.42 (3) 49.98 (2)
chang 66.38 (4) 52.99 (4) 57.10 48.94 (4) 53.86 56.10 (4) 48.50 (4)
chen 63.71 (7) 51.76 (5) 55.77 48.10 (5) 52.87 54.52 (5) 46.24 (6)
chunyuang 63.82 (6) 51.21 (6) 55.10 47.58 (6) 52.65 54.20 (6) 45.84 (6)
shou 62.91 (8) 49.44 (9) 53.16 46.66 (7) 50.44 53.00 (7) 43.97 (8)
yuan 62.55 (9) 50.11 (8) 54.53 45.99 (8) 52.10 52.88 (8) 44.76 (8)
xu 66.18 (5) 50.30 (7) 51.31 41.25 (11) 46.47 52.58 (9) 46.83 (5)
uryupina 60.89 (10) 46.24 (10) 49.31 42.93 (9) 46.04 50.02 (10) 41.15 (10)
songyang 59.83 (12) 45.90 (11) 49.58 42.36 (10) 45.10 49.36 (11) 41.25 (10)
zhekova 53.52 (13) 35.66 (13) 39.66 32.16 (12) 34.80 40.45 (12) 29.98 (12)
xinxin 48.27 (14) 35.73 (12) 37.99 31.90 (13) 36.54 38.63 (13) 29.22 (12)
li 50.84 (11) 32.29 (14) 36.28 25.21 (14) 31.85 36.11 (14) 27.32 (14)

Table 5: The results of the CoNLL 2012 shared task.

0% 20% 40% 60% 80% 100%
0

20

40

60

80

100

F 1

MUC B3 CEAFm

CEAFe BLANC LEA

Figure 2: Resolved coreference links ratio without
incorrect links.

7 Analysis

In this section we analyze the behavior of the eval-
uation metrics based on various coreference reso-
lution errors. The set of key entities in all experi-
ments contains: one entity of size 20, two entities
of size 10, three entities of size 5, one entity of size
4, and ten entities of size 2.

7.1 Correct Links

We analyze different metrics based on the ratio
of correctly resolved coreference links: (1) with-
out wrong coreference links (Figure 2), and (2)
with wrong coreference links (Figure 3). In the

0% 20% 40% 60% 80% 100%
0

20

40

60

80

100

F 1

MUC B3 CEAFm

CEAFe BLANC LEA

Figure 3: Resolved coreference links ratio in the
presence of incorrect links.

experiments of Figure 2, only mentions that are
correctly resolved exist in the response. In Fig-
ure 3, apart from the mentions that are resolved
correctly, other mentions are linked to at least one
non-coreferent mention. Therefore, mention de-
tection F1 is always 100%.

The following observations can be drawn from
these experiments: (1) MUC and LEA are the only
measures which give a zero score to the response
that contains no correct coreference relations; (2)
in our experiments, CEAFe shows an unreason-
able drop when the correct link ratio changes from
0% to 20%; and (3), in Figure 2, the BLANC

638



0% 20% 40% 60% 80% 100%
0

20

40

60

80

100

F 1

MUC B3 CEAFm CEAFe
BLANC LEA %correct links

Figure 4: Resolving entities in decreasing order.
F1 of B3, CEAF, and LEA are the same.

F1 values are less than or equal to those of B3

and LEA. However, in Figure 3 that contains both
coreferent and non-coreferent links, BLANC F1 is
at least 20% higher than that of other metrics.

7.2 Correct Entities
Apart from the correctly resolved links, a coref-
erence metric should also take into account the
resolved entities. In this section, we analyze the
coreference resolution metrics based on the num-
ber and the size of the correctly resolved entities.
In these experiments, each entity is either resolved
completely, or all of its mentions are absent from
the response. In Figure 4, the key entities are
added to the response in decreasing order of their
size. Figure 5 shows the experiments in which the
entities are resolved in increasing order. The ra-
tio of the correctly resolved coreference links is
shown in both figures.

We can observe the following points from Fig-
ure 4 and Figure 5: (1) CEAFe results in the same
F1 values regardless of the size of entities that are
resolved or are missing; (2) B3, CEAFm and LEA
result in the same F1 values; and (3) BLANC is
very sensitive to the total number of links.

7.3 Splitting/Merging Entities
The effect of splitting a single entity into two or
more entities is studied in Figure 6. The overall
effect of merging entities would be similar to that
of splitting if the roles of the key and response en-
tities change. In each experiment, only one key en-
tity is split in a way that no singletons are created.
For example, 18-2 in the horizontal axis indicates

0% 20% 40% 60% 80% 100%
0

20

40

60

80

100

F 1

MUC B3 CEAFm CEAFe
BLANC LEA %correct links

Figure 5: Resolving entities in increasing order.
F1 of B3, CEAF, and LEA are the same.

18-
2 3-2 2-2 16-

4
10-

10
5-3

-2
9-9

-2
9-5

-6
85

90

95

100

F 1

MUC B3 CEAFm CEAFe BLANC LEA

Figure 6: Effect of splitting entities.

that an entity of size 20 is split into two entities of
size 18 and 2.

The following observations can be drawn from
Figure 6: (1) MUC only recognizes the number of
splits regardless of the size of entities; (2) CEAFe
does not differentiate 2-2 from 10-10, and 9-9-
2 from 9-5-6; and (3) the highest disagreement
is for ranking different numbers of splits in enti-
ties with different sizes, i.e., B3: 18-2>5-3-2>16-
4, BLANC: 5-3-2>18-2>16-4, CEAF: 18-2>16-
4>5-3-2, and LEA: 18-2>16-4>5-3-2. These are
the cases that are even for humans hard to rank.

7.4 Extra/Missing Mentions

Figure 7 shows the effect of extra mentions, i.e.
mentions that are not included in any key entity. If
we change the roles of the key and response enti-

639



1-2 1-10 2-0 2-2 2-10 3-0 3-2 3-10

96

98

100

F 1

MUC B3 CEAFm CEAFe BLANC LEA

Figure 7: Effect of extra mentions.

ties, the overall effect of missing mentions would
be similar. In the horizontal axis, the first number
shows the number of extra mentions. The second
number shows the size of the entity to which extra
mentions are added. A zero entity size indicates
the extra mentions are linked together.

The following points are worth noting from the
results of Figure 7: (1) MUC and CEAFm are the
least discriminative metrics when the system out-
put includes extra mentions; (2) except for CEAFe,
other metrics rank 3-10 as the worst output;(3)
CEAFe recognizes both 2-0 and 3-0 as the worst
outputs. However, in these outputs the extra men-
tions are linked together and therefore no incor-
rect information is added to the correctly resolved
entities; and (4) LEA is the only metric that rec-
ognizes error 2-0 is less harmful than 1-2 or 1-10.
However, LEA does not discriminate the different
outputs in which only one extra mention is added
to an entity. If k extra mentions are added to an
entity of size n, the corresponding resolution error
multiplied by the importance of the response en-
tity is (n + k) × (1 − n×(n−1)(n+k)×(n+k−1)) . If k = 1,
this equation is 2 regardless of n’s value.

7.5 Mention Identification

The mention identification effect is shown in
Figure 8. In all experiments, the number of cor-
rect coreference links is zero. The horizontal axis
shows the mention identification accuracy in the
system output. The F1 of B3, CEAF and BLANC
in these experiments clearly contrast the inter-
pretability requirement. A coreference resolver
with a non-zero score should have resolved some
of the coreference relations.

0% 20% 40% 60% 80% 100%
0

20

40

F 1

MUC B3 CEAFm

CEAFe BLANC LEA

Figure 8: Effect of mention identification.

8 Conclusions

Current coreference resolution evaluation metrics
have flaws which make them unreliable for com-
paring coreference resolvers. There is also a low
agreement between the rankings of different met-
rics. The current solution is to use an average
value of different metrics for comparisons. Aver-
aging unreliable scores does not result in a reliable
one. Indeed, recall and precision comparisons of
coreference resolvers are not possible based on an
average score. We first report the mention iden-
tification effect on B3, CEAF and BLANC which
causes these metrics to report misleading values.
The only metric that is resistant to the mention
identification effect is the least discriminative one,
i.e. MUC. We introduce LEA, the Link-based
Entity-Aware metric, as a new evaluation metric
for coreference resolution. LEA is a simple intu-
itive metric that overcomes the drawbacks of the
current metrics. It can be easily adapted for entity
evaluation in different domains or applications in
which entities with various attributes are of differ-
ent importance.

Acknowledgments

The authors would like to thank Sameer Prad-
han, Mark-Christoph Müller, Mohsen Mesgar and
Sebastian Martschat for their helpful comments.
We would also like to thank Sam Wiseman and
Haoruo Peng for providing us with their corefer-
ence system outputs. This work has been funded
by the Klaus Tschira Foundation, Heidelberg, Ger-
many. The first author has been supported by a
Heidelberg Institute for Theoretical Studies PhD.
scholarship.

640



References
Amit Bagga and Breck Baldwin. 1998. Algorithms

for scoring coreference chains. In Proceedings
of the 1st International Conference on Language
Resources and Evaluation, Granada, Spain, 28–30
May 1998, pages 563–566.

Kevin Clark and Christopher D. Manning. 2015.
Entity-centric coreference resolution with model
stacking. In Proceedings of the 53rd Annual Meet-
ing of the Association for Computational Linguis-
tics (Volume 1: Long Papers), Beijing, China, 26–31
July 2015, pages 1405–1415.

Pascal Denis and Jason Baldridge. 2009a. Global joint
models for coreference resolution and named entity
classification. Procesamiento del Lenguaje Natural,
(42):87–96.

Pascal Denis and Jason Baldridge. 2009b. Global joint
models for coreference resolution and named entity
classification. Procesamiento del Lenguaje Natural,
42:87–96, March.

Gordana Ilic Holen. 2013. Critical reflections on eval-
uation practices in coreference resolution. In Pro-
ceedings of the 2013 NAACL HLT Student Research
Workshop, Atlanta, Georgia, 9-14 June 2013, pages
1–7.

Xiaoqiang Luo and Sameer Pradhan. 2016. Evaluation
metrics. In M. Poesio, R. Stuckardt, and Y. Vers-
ley, editors, Anaphora Resolution: Algorithms, Re-
sources, and Applications. Springer. To appear.

Xiaoqiang Luo, Sameer Pradhan, Marta Recasens, and
Eduard Hovy. 2014. An extension of BLANC to
system mentions. In Proceedings of the 52nd An-
nual Meeting of the Association for Computational
Linguistics (Volume 2: Short Papers), pages 24–29,
Baltimore, Maryland, June. Association for Compu-
tational Linguistics.

Xiaoqiang Luo. 2005. On coreference resolution
performance metrics. In Proceedings of the Hu-
man Language Technology Conference and the 2005
Conference on Empirical Methods in Natural Lan-
guage Processing, Vancouver, B.C., Canada, 6–8
October 2005, pages 25–32.

Sebastian Martschat and Michael Strube. 2015. La-
tent structures for coreference resolution. Transac-
tions of the Association for Computational Linguis-
tics, 3:405–418.

Eric W. Noreen. 1989. Computer Intensive Meth-
ods for Hypothesis Testing: An Introduction. Wiley,
New York, N.Y.

Haoruo Peng, Kai-Wei Chang, and Dan Roth. 2015. A
joint framework for coreference resolution and men-
tion head detection. In Proceedings of the 19th Con-
ference on Computational Natural Language Learn-
ing, Beijing, China, 30–31 July 2015, pages 12–21.

Sameer Pradhan, Lance Ramshaw, Mitchell Marcus,
Martha Palmer, Ralph Weischedel, and Nianwen
Xue. 2011. CoNLL-2011 Shared Task: Modeling
unrestricted coreference in OntoNotes. In Proceed-
ings of the Shared Task of the 15th Conference on
Computational Natural Language Learning, Port-
land, Oreg., 23–24 June 2011, pages 1–27.

Sameer Pradhan, Alessandro Moschitti, Nianwen Xue,
Olga Uryupina, and Yuchen Zhang. 2012. CoNLL-
2012 Shared Task: Modeling multilingual unre-
stricted coreference in OntoNotes. In Proceedings
of the Shared Task of the 16th Conference on Com-
putational Natural Language Learning, Jeju Island,
Korea, 12–14 July 2012, pages 1–40.

Sameer Pradhan, Xiaoqiang Luo, Marta Recasens, Ed-
uard Hovy, Vincent Ng, and Michael Strube. 2014.
Scoring coreference partitions of predicted men-
tions: A reference implementation. In Proceed-
ings of the 52nd Annual Meeting of the Association
for Computational Linguistics (Volume 2: Short Pa-
pers), Baltimore, Md., 22–27 June 2014, pages 30–
35.

William R. Rand. 1971. Objective criteria for the eval-
uation of clustering methods. Journal of the Ameri-
can Statistical Association, 66(336):846–850.

Marta Recasens and Eduard Hovy. 2011. BLANC:
Implementing the Rand index for coreference eval-
uation. Natural Language Engineering, 17(4):485–
510.

Veselin Stoyanov, Nathan Gilbert, Claire Cardie, and
Ellen Riloff. 2009. Conundrums in noun phrase
coreference resolution: Making sense of the state-
of-the-art. In Proceedings of the Joint Conference
of the 47th Annual Meeting of the Association for
Computational Linguistics and the 4th International
Joint Conference on Natural Language Processing,
Singapore, 2–7 August 2009, pages 656–664.

Roland Stuckhardt. 2003. Coreference-based summa-
rization and question answering: A case for high
precision anaphor resolution. In Proceedings of the
2003 International Symposium on Reference Reso-
lution and Its Applications to Question Answering
and Summarization, Venice, Italy, 23–24 June 2003,
pages 33–42.

Don Tuggener. 2014. Coreference resolution evalua-
tion for higher level applications. In Proceedings of
the 14th Conference of the European Chapter of the
Association for Computational Linguistics, Volume
2: Short Papers, Gothenburg, Sweden, 26–30 April
2014, pages 231–235.

Marc Vilain, John Burger, John Aberdeen, Dennis Con-
nolly, and Lynette Hirschman. 1995. A model-
theoretic coreference scoring scheme. In Proceed-
ings of the 6th Message Understanding Conference
(MUC-6), pages 45–52, San Mateo, Cal. Morgan
Kaufmann.

641



Sam Wiseman, Alexander M. Rush, Stuart Shieber, and
Jason Weston. 2015. Learning anaphoricity and an-
tecedent ranking features for coreference resolution.
In Proceedings of the 53rd Annual Meeting of the
Association for Computational Linguistics (Volume
1: Long Papers), Beijing, China, 26–31 July 2015,
pages 1416–1426.

642


