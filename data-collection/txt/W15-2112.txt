



















































Quantifying Word Order Freedom in Dependency Corpora


Proceedings of the Third International Conference on Dependency Linguistics (Depling 2015), pages 91–100,
Uppsala, Sweden, August 24–26 2015.

Quantifying Word Order Freedom in Dependency Corpora

Richard Futrell, Kyle Mahowald, and Edward Gibson
Department of Brain and Cognitive Sciences

Massachusetts Institute of Technology
{futrell, kylemaho, egibson}@mit.edu

Abstract

Using recently available dependency cor-
pora, we present novel measures of a key
quantitative property of language, word
order freedom: the extent to which word
order in a sentence is free to vary while
conveying the same meaning. We discuss
two topics. First, we discuss linguistic
and statistical issues associated with our
measures and with the annotation styles of
available corpora. We find that we can
measure reliable upper bounds on word
order freedom in head direction and the
ordering of certain sisters, but that more
general measures of word order freedom
are not currently feasible. Second, we
present results of our measures in 34 lan-
guages and demonstrate a correlation be-
tween quantitative word order freedom of
subjects and objects and the presence of
nominative-accusative case marking. To
our knowledge this is the first large-scale
quantitative test of the hypothesis that lan-
guages with more word order freedom
have more case marking (Sapir, 1921;
Kiparsky, 1997).

1 Introduction

Comparative cross-linguistic research on the
quantitative properties of natural languages has
typically focused on measures that can be ex-
tracted from unannotated or shallowly annotated
text. For example, probably the most inten-
sively studied quantitative properties of language
are Zipf’s findings about the power law distribu-
tion of word frequencies (Zipf, 1949). However,
the properties of languages that can be quantified
from raw text are relatively shallow, and are not
straightforwardly related to higher-level properties
of languages such as their morphology and syntax.

As a result, there has been relatively little large-
scale comparative work on quantitative properties
of natural language syntax.

In recent years it has become possible to bridge
that gap thanks to the availability of large depen-
dency treebanks for many languages and the de-
velopment of standardized annotation schemes (de
Marneffe et al., 2014; Nivre, 2015; Nivre et al.,
2015). These resources make it possible to per-
form direct comparisons of quantitative proper-
ties of dependency trees. Previous work using de-
pendency corpora to study crosslinguistic syntac-
tic phenomena includes Liu (2010), who quanti-
fies the frequency of right- and left-branching in
dependency corpora, and Kuhlmann (2013), who
quantifies the frequency with which natural lan-
guage dependency trees deviate from projectiv-
ity. Other work has studied graph-theoretic prop-
erties of dependency trees in the context of lan-
guage classification (Liu and Li, 2010; Abramov
and Mehler, 2011).

Here we study a particular quantitative property
of language syntax: word order freedom. We fo-
cus on developing linguistically interpretable mea-
sures, as close as possible to an intuitive, relatively
theory-neutral idea of what word order freedom
means. In doing so, a number of methodological
issues and questions arise. What quantitative mea-
sures map most cleanly onto the concept of word
order freedom? Is it feasible to estimate the pro-
posed measure given limited corpus size? Which
corpus annotation style—e.g., content-head de-
pendencies or dependencies where function words
are heads—best facilitates crosslinguistic compar-
ison? In this work, we argue for a set of method-
ological decisions which we believe balance the
interests of linguistic interpretability, stability with
respect to corpus size, and comparability across
languages.

We also present results of our measures as ap-
plied to 34 languages and discuss their linguis-

91



tic significance. In particular, we find that lan-
guages with quantitatively large freedom in their
ordering of subject and object all have nomina-
tive/accusative case marking, but that languages
with such case marking do not necessarily have
much word order freedom. This asymmetric rela-
tionship has been suggested in the typological lit-
erature (Kiparsky, 1997), but this is the first work
to verify it quantitatively. We also discuss some of
the exceptions to this generalization in the light of
recent work on information-theoretic properties of
different word orders (Gibson et al., 2013).

2 Word Order and the Notion of
Dependency

We define word order freedom as the extent to
which the same word or constituent in the same
form can appear in multiple positions while retain-
ing the same propositional meaning and preserv-
ing grammaticality. For example, the sentence pair
(1a-b) provides an example of word order free-
dom in German, while sentence pair (2a-b) pro-
vides an example of a lack of word order freedom
in English. However, the sentences (2a) and (2c)
do not provide an instance of word order freedom
in English by our definition, since the agent and
patient appear in different syntactic forms in (2c)
compared to (2a). We provide dependency syntax
analyses of these sentences below.

(1a)

Hans sah den Mann
Hans saw the-ACC man

nsubj

dobj

det

Meaning: “Hans saw the man.”
(1b)

den Mann sah Hans
the-ACC man saw Hans

dobjdet nsubj

Meaning: “Hans saw the man.”
(2a)

John saw the man.

nsubj

dobj

det

(2b)

*The man saw John.

dobjdet nsubj

Cannot mean: “John saw the man.”

(2c)

The man was seen by John.

det

nsubjpass

aux

nmod

case

In the typological literature, this phenomenon
has also been called word order flexibility, prag-
matic word order, and a lack of word order rigid-
ity. These last two terms reflect the fact that word
order freedom does not mean that that word order
is random. When word order is “free”, speakers
might order words to convey non-propositional as-
pects of their intent. For example, a speaker might
place certain words earlier in a sentence in order
to convey that those words refer to old informa-
tion (Ferreira and Yoshita, 2003); a speaker might
order words according to how accessible they are
psycholinguistically (Chang, 2009); etc. Word or-
der may be predictable given these goals, but here
we are interested only in the extent to which word
order is conditioned on the syntactic and composi-
tional semantic properties of an utterance.

In a dependency grammar framework, we can
conceptualize word order freedom as variability in
the linear order of words given an unordered de-
pendency graph with labelled edges. For example,
both sentences (1a) and (1b) are linearizations of
this unordered dependency graph:

sah

Hans Mann

den

nsubj dobj

det

The dependency formalism also gives us a
framework for a functional perspective on why
word order freedom exists and under what con-
ditions it might arise. In general, the task of un-
derstanding the propositional meaning of a sen-
tence requires identifying which words are linked
to other words, and what the relation types of those
links are. The dependency formalism directly en-
codes a subset of these links, with the additional
assumption that links are always between exactly
two explicit words. Therefore, we can roughly
view an utterance as an attempt by a language pro-
ducer to serialize a dependency graph such that a
comprehender can recover it. The producer will
want to choose a serialization which is efficient to

92



produce and which will allow the comprehender
to recover the structure robustly. That is, the ut-
terance must be informative about which pairs of
words are linked in a dependency, and what the
relation types of those links are.

Here we focus on the communication of rela-
tion types. In the English and German examples
above, the relation types to be conveyed are nsubj
and dobj in the notation of the Universal Depen-
dencies project (Nivre et al., 2015). For the task of
communicating the relation type between a head
and dependent, natural languages seem to adopt
two non-exclusive solutions: either the order of
the head, the dependent, and the dependent’s sis-
ters is informative about relation type (a word or-
der code), or the wordform of the head or depen-
dent is informative about relation type (Nichols,
1986) (a case-marking code). Considerations of
robustness and efficiency lead to a prediction of
a tradeoff between these options. If a language
uses case-marking to convey relation type, then
word order can be repurposed to efficiently con-
vey other, potentially non-propositional aspects of
meaning. On the other hand, if a language uses in-
flexible word order to convey relation type, then it
would be inefficient to also include case marking.
However, some word order codes are less robust
to noise than others (Gibson et al., 2013; Futrell et
al., 2015), so certain rigid word orders might still
require case-marking to maintain robustness. Sim-
ilarly, some case-marking systems might be more
or less robust, and so require rigid word order.

The idea that word order freedom is related to
the prevalence of morphological marking is an old
one (Sapir, 1921). A persistent generalization in
the typological literature is that while word order
freedom implies the existence of morphological
marking, morphological marking does not imply
the existence of word order freedom (Kiparsky,
1997; McFadden, 2003). These generalizations
have been made primarily on the basis of native
speaker intuitions and analyses of small datasets.
Such data is problematic for measures such as
word order freedom, since languages may vary
quantitatively in how much variability they have,
and it is not clear where to discretize this variabil-
ity in order to form the categories “free word or-
der” and “fixed word order”. In order to test the
reality of these generalizations, and to explore ex-
planatory hypotheses for crosslinguistic variation,
it is necessary to quantify the degree of word order

freedom in a language.

3 Entropy Measures

Our basic idea is to measure the extent to which
the linear order of words is determined by the un-
ordered dependency graph of a sentence. A natural
way to quantify this is conditional entropy:

H(X|C) =
∑

c∈C
pC(c)

∑

x∈X
pX|C(x|c)logpX|C(x|c), (1)

which is the expected conditional uncertainty
about a discrete random variable X , which we
call the dependent variable, conditioned on an-
other discrete random variable C, which we call
the conditioning variable. In our case, the “per-
fect” measure of word order freedom would be the
conditional entropy of sequences of words given
unordered dependency graphs. Directly measur-
ing this quantity is impractical for a number of rea-
sons, so we will explore a number of entropy mea-
sures over partial information about dependency
trees.

Using a conditional entropy measure with de-
pendency corpora requires us to decide on three
parameters: (1) the method of estimating entropy
from observed joint counts of X and C, (2) the in-
formation contained in the dependent variable X ,
and (3) the information contained in the condition-
ing variable C. The two major factors in deciding
these parameters are avoiding data sparsity and re-
taining linguistic interpretability. In this section
we discuss the detailed considerations that must
go into these decisions.

3.1 Estimating Entropy
The simplest way to estimate entropy given joint
counts is through maximum likelihood estimation.
However, maximum likelihood estimates of en-
tropy are known to be biased and highly sensi-
tive to sample size (Miller, 1955). The bias is-
sues arise because the entropy of a distribution
is highly sensitive to the shape of its tail, and it
is difficult to estimate the tail of a distribution
given a small sample size. As a result, entropy
is systematically underestimated. These issues are
exacerbated when applying entropy measures to
natural language data, because of the especially
long-tailed frequency distribution of sentences and
words.

The bias issue is especially acute when doing
crosslinguistic comparison with dependency cor-
pora because the corpora available vary hugely in

93



their sample size, from 1017 sentences of Irish to
82,451 sentences of Czech. An entropy difference
between one language and another might be the
result of sample size differences, rather than a real
linguistic difference.

We address this issue in two ways: first, we
estimate entropy using the bootstrap estimator of
DeDeo et al. (2013), and apply the estimator to
equally sized subcorpora across languages1. Sec-
ond, we choose dependent and conditioning vari-
ables to minimize data sparsity and avoid long
tails. In particular, we avoid entropy measures
where the conditioning variable involves word-
forms or lemmas. We evaluate the effects of data
sparsity on our measures in Section 4.

3.2 Local Subtrees

In order to cope with data sparsity and long-tailed
distributions, the dependent and conditioning vari-
ables must have manageable numbers of possible
values. This means that we cannot compute some-
thing like the entropy over full sentences given full
dependency graphs, as these joint counts would be
incredibly sparse, even if we include only part of
speech information about words.

We suggest computing conditional entropy only
on local subtrees: just subtrees consisting of a
head and its immediate dependents. We conjec-
ture that most word order and morphological rules
can be stated in terms of heads and their depen-
dents, or in terms of sisters of the same head. For
example, almost all agreement phenomena in nat-
ural language involve heads and their immediate
dependents (Corbett, 2006). Prominent and suc-
cessful generative models of dependency struc-
ture such as the Dependency Model with Valence
(Klein and Manning, 2004) assume that depen-
dency trees are generated recursively by generat-
ing these local subtrees.

There are two shortcomings to working only
with local subtrees; here we discuss how to deal
with them.

First, there are certain word order phenom-
ena which appear variable given only local sub-
tree structure, but which are in fact determinis-
tic given dependency structure beyond local sub-
trees. The extent to which this is true depends

1At a high level, the bootstrap algorithm works by mea-
suring entropy in the whole sample and in subsamples and
uses these estimates to attempt to correct bias in the whole
sample. We refer the reader to DeDeo et al. (2013) for de-
tails.

on the specifics of the dependency formalism. For
example, in German, the position of the verb de-
pends on clause type. In a subordinate clause with
a complementizer, the verb must appear after all
of its dependents (V-final order). Otherwise, the
verb must appear after exactly one of its depen-
dents (V2 order). If we analyze complementiz-
ers as heading their verbs, as in (3a), then the lo-
cal subtree of the verb sah does not include infor-
mation about whether the verb is in a subordinate
clause or not.

(3a)

Hans sah den Mann
Hans saw the-ACC man

nsubj

dobj

det

(3b)

Ich weiß, dass Hans den Mann sah
I know that Hans the man saw

nsubj dobj dobjdet

nsubj

As a result, if we measure the entropy of the or-
der of verbal dependents conditioned on the local
subtree structure, then we will erroneously con-
clude that German is highly variable, since the or-
der is either V2 or V-final and there is nothing in
the local subtree to predict which one is appropri-
ate. However, if we analyze complementizers as
the dependent of their verb (as in the Universal
Dependencies style, (3c)), then the conditional en-
tropy of the verb position given local subtree struc-
ture is small. This is because the position of the
verb is fully predicted by the presence in the lo-
cal subtree of a mark relation whose dependent is
dass, weil, etc.

(3c)
sah

dass Hans Mann

den

mark nsubj dobj

det

94



Ich weiß, dass Hans den Mann sah

I know that Hans the-ACC man saw

nsubj

dobj

mark

dobjdet

nsubj

We deal with this issue by preferring annotation
styles under which the determinants of the order
of a local subtree are present in that subtree. This
often means using the content-head dependency
style, as in this example.

The second issue with looking only at local sub-
trees is that we miss certain word order variabil-
ity associated with nonprojectivity, such as scram-
bling. Due to space constraints, we do not address
this issue here.

When we condition on the local subtree struc-
ture and find the conditional entropy of word or-
ders, we call this measure Relation Order En-
tropy, since we are getting the order with which
relation types are expressed in a local subtree.

3.3 Dependency Direction

Another option for dealing with data sparsity is to
get conditional entropy measures over even less
dependency structure. In particular we consider
the case of entropy measures conditioned only on
a dependent, its head, and the relation type to
its head, where the dependent measure is simply
whether the head is to the left or right of the depen-
dent. This measure potentially suffers much less
from data sparsity issues, since the set of possible
heads and dependents in a corpus is much smaller
than the set of possible local subtrees. But in re-
stricting our attention only to head direction, we
miss the ability to measure any word order free-
dom among sister dependents. This measure also
has the disadvantage that it can miss the kind of
conditioning information present in local subtrees,
as described in Section 3.2.

When we condition only on simple dependen-
cies, we call this measure Head Direction En-
tropy.

3.4 Conditioning Variables

So far we have discussed our decision to use con-
ditional entropy measures over local subtrees or
single dependencies. In this setting, the condition-
ing variable is the unordered local subtree or de-
pendency, and the dependent variable is the linear
order of words. We now turn to the question of

what information should be contained in the con-
ditioning variable: whether it should be the full
unordered tree, or just the structure of the tree, or
the structure of the tree plus part-of-speech (POS)
tags and relation types, etc.

In Section 3.1 we argued that we should not
condition on the wordforms or lemmas due to
sparsity issues. The remaining kinds of informa-
tion available in corpora are the tree topology, POS
tags, and relation types. Many corpora also in-
clude annotation for morphological features, but
this is not reliably present.

Without conditioning on relation types, our en-
tropy measures become much less linguistically
useful. For example, if we did not condition on de-
pendency relation types, it would be impossible to
identify verbal subjects and objects or to quantify
how informative word order is about these rela-
tions crosslinguistically. So we always include de-
pendency relation type in conditioning variables.

The remaining questions are whether to include
the POS tags of heads and of each dependent.
Some annotation decisions in the Universal De-
pendencies and Stanford Dependencies argue for
including POS information of heads. For example,
the Universal Dependencies annotation for copu-
lar sentences has the predicate noun as the head,
with the subject noun as a dependent of type nsubj,
as in example (4):

(4)

Bob is a criminal

nsubj

cop
det

This has the effect that the linguistic meaning
of the nsubj relation encodes one syntactic relation
when its head is a verb, and another syntactic rela-
tion when its head is a noun. So we should include
POS information about heads when possible.

There are also linguistic reasons for including
the POS of dependents in the conditioning vari-
able. Word order often depends on part of speech;
for example, in Romance languages, the standard
order in the main clause is Subject-Verb-Object if
the object is a noun but Subject-Object-Verb if the
object is a pronoun. Not including POS tags in
the conditioning variable would lead to mislead-
ingly high word order freedom numbers for these
clauses in these languages.

Therefore, when possible, our conditioning
variables include the POS tags of heads and de-
pendents in addition to dependency relation types.

95



3.5 Annotation style and crosslinguistic
comparability

We have discussed issues involving entropy esti-
mation and the choice of conditioning and depen-
dent variables. Here we discuss another dimension
of choices: what dependency annotation scheme
to use.

Since the informativity of dependency trees
about syntax and semantics affects our word order
freedom measures, it is important to ensure that
dependency trees across different corpora convey
the same information. Certain annotation styles
might allow unordered local subtrees to convey
more information in one language than in another.
To ensure comparability, we should use those an-
notation styles which are most consistent across
languages regarding how much information they
give about words in local subtrees, even if this
means choosing annotation schemes which are
less informative overall. We give examples below.

In many cases, dependency annotation schemes
where function words are heads provide more in-
formation about syntactic and semantic relations,
so such annotation schemes lead to lower esti-
mates of word order freedom. For example, con-
sider the ordering of German verbal adjuncts. The
usual order is time adjuncts followed by place ad-
juncts. Time is often expressed by a bare noun
such as gestern “yesterday”, while place is often
expressed with an adpositional phrase.

We will consider how our measures will behave
for these constructions given function-word-head
dependencies, and given content-head dependen-
cies. Given function-word-head dependencies as
in (5a), these two adjuncts will appear with rela-
tions nmod and adpmod in the local subtree rooted
by the verb tanzte; their order will be highly pre-
dictable given these relation types inasmuch as
time adjuncts are usually expressed as bare nouns
and place adjuncts are usually expressed as adpo-
sitional phrases. On the other hand, given content-
head dependencies as in (5b), the adjuncts will ap-
pear in the local subtree as nmod and nmod, and
their order will appear free.

(5a)

Ich tanzte gestern in der Stadt
I danced yesterday in the city

nsubj
nmod

adpmod
pobj

det

(5b)

Ich tanzte gestern in der Stadt
I danced yesterday in the city

nsubj
nmod

nmod

case
det

However, function-word-head dependencies do
not provide the same amount of information from
language to language, because languages differ
in how often they use adpositions as opposed to
case marking. In the German example, function-
word-head dependencies allowed us to distinguish
time adjuncts from place adjuncts because place
adjuncts usually appear as adpositional phrases
while time adjuncts often appear as noun phrases.
But in a language which uses case-marked noun
phrases for such adjuncts, such as Finnish, the
function-word-head dependencies would not pro-
vide this information. Therefore, even if (say)
Finnish and German had the same degree of free-
dom in their ordering of place adjuncts and time
adjuncts, we would estimate more word order
freedom in Finnish and less in German. However,
using content-head dependencies, we get the same
amount of information in both languages. There-
fore, we prefer content-head dependencies for our
measures.

Following similar reasoning, we decide to use
only the universal POS tags and relation types
in our corpora, and not finer-grained language-
specific tags.

Using content-head dependencies while condi-
tioning only on local subtrees overestimates word
order freedom compared to function-word-head
dependencies. At first glance, the content-head
dependency annotation seems inappropriate for a
typological study, because it clashes with standard
linguistic analyses where function words such as
adpositions and complementizers (and, in some
analyses, even determiners (Abney, 1987)) are
heads, rather than dependents. However, content-
head dependencies provide more consistent mea-
sures across languages. Therefore we present re-
sults from our measures applied to content-head
dependencies.

3.6 Summary of Parameters of Entropy
Measures

We have discussed a number of parameters which
go into the construction of a conditional entropy

96



measure of word order freedom. They are:
1. Annotation style: function words as heads or

content words as heads.
2. Whether we measure entropy of lineariza-

tions of local subtrees (Relation Order En-
tropy) or of simple dependencies (Head Di-
rection Entropy).

3. What information we include in the condi-
tioning variable: relation types, head and
dependent POS, head and dependent word-
forms, etc.

4. Whether to measure entropy over all depen-
dents, or only over some subset of interest,
such as subjects or objects.

The decisions for these parameters are dic-
tated by balancing data sparsity and linguistic in-
terpretability. We have argued that we should
use content-head dependencies, and never include
wordforms or lemmas in the conditioning vari-
ables. Furthermore, we have argued that it is gen-
erally better to include part-of-speech information
in the conditioning variable, but that this may have
to be relaxed to cope with data sparsity. The deci-
sions about whether to condition on local subtrees
or on simple dependencies, and whether to restrict
attention to a particular subset of dependencies,
depends on the particular question of interest.

3.7 Entropy Measures as Upper Bounds on
Word Order Freedom

We initially defined an ideal measure, the entropy
of word orders given full unordered dependency
trees. We argued that we would have to back away
from this measure by looking only at the con-
ditional entropy of orders of local subtrees, and
furthermore that we should only condition on the
parts of speech and relation types in the local sub-
tree. Here we argue that these steps away from
the ideal measure mean that the resulting measures
can only be interpreted as upper bounds on word
order freedom.

With each step away from the ideal measure,
we also move the interpretation of the measures
away from the idealized notion of word order free-
dom. With each kind of information we remove
from the independent variable, we allow instances
where the word order of a phrase might in fact be
fully deterministic given that missing information,
but where we will erroneously measure high word
order freedom. For example, in German, the or-
der of verbal adjuncts is usually time before place.

However, in a dependency treebank, these rela-
tions are all nmod. By considering only the or-
dering of dependents with respect to their relation
types and parts of speech, we miss the extent to
which these dependents do have a deterministic or-
der determined by their semantics. Thus, we tend
to overestimate true word order freedom.

On the other hand, the conditional entropy ap-
proach do not in principle underestimate word or-
der freedom as we have defined it. The condition-
ing information present in a dependency tree rep-
resents only semantic and syntactic relations, and
we are explicitly interested in word order variabil-
ity beyond what can be explained by these factors.
Therefore, our word order freedom measures con-
stitute upper bounds on the true word order free-
dom in a language.

Underestimation can arise due to data sparsity
issues and bias issues in entropy estimators. For
this reason, it is important to ensure that our mea-
sures are stable with respect to sample size, lest
our upper bound become a lower bound on an up-
per bound.

The tightness of the upper bound on word order
freedom depends on the informativity of the rela-
tion types and parts of speech included in a mea-
sure. For example, if we use a system of relation
types which subdivides nmod relations into cate-
gories like nmod:tmod for time phrases, then we
would not overestimate the word order freedom
of German verbal adjuncts. As another example,
to achieve a tighter bound for a limited aspect of
word order freedom at the cost of empirical cov-
erage, we might restrict ourselves to relation types
such as nsubj and dobj, which are highly informa-
tive about their meanings.

4 Applying the Measures

Here we give the results of applying some of the
measures discussed in Section 3 to dependency
corpora. We use the dependency corpora of the
HamleDT 2.0 (Zeman et al., 2012; Rosa et al.,
2014) and Universal Dependencies 1.0 (Nivre et
al., 2015). All punctuation and dependencies with
relation type punct are removed. We only examine
sentences with a single root. Annotation was nor-
malized to content-head format when necessary.
Combined this gives us dependency corpora of 34
languages in a fairly standardized format.

In order to evaluate the stability of our measures
with respect to sample size, we measure all en-

97



●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

Ancient Greek
Latin

Basque
Croatian

Slovak
German
Estonian
Russian

Slovenian
Danish

Hungarian
Persian
Czech

Hebrew
Finnish

Dutch
Spanish

Modern Greek
Catalan

Bulgarian
Romanian

Swedish
Bengali

Portuguese
Italian

French
Japanese

Arabic
Turkish
English

Hindi
Irish

Telugu
Tamil

0.0 0.2 0.4 0.6
Head Direction Entropy

Mostly head−final

●

●

FALSE

TRUE

Figure 1: Head direction entropy in 34 languages.
The bar represents the average magnitude of head
direction entropy estimated from subcorpora of
1000 sentences; the red dot represents head direc-
tion entropy estimated from the whole corpus.

tropies using the bootstrap estimator of DeDeo et
al. (2013). We report the mean results from apply-
ing our measures to subcorpora of 1000 sentences
for each corpus. We also report results from apply-
ing measures to the full corpus, so that the differ-
ence between the full corpus and the subcorpora
can be compared, and the effect of data sparsity
evaluated.

4.1 Head Direction Entropy

Head direction entropy, defined and motivated in
Section 3.3, is the conditional entropy of whether
a head is to the right or left of a dependent, condi-
tioned on relation type and part of speech of head
and dependent. This measure can reflect either
consistency in head direction conditioned on rela-
tion type, or consistency in head direction overall.
Results from this measure are shown in Figure 1.
As can be seen, the measure gives similar results
when applied to subcorpora as when applied to full
corpora, indicating that this is measure is not un-
duly affected by differences in sample size.

We find considerable variability in word order
freedom with respect to head direction. In lan-
guages such as Korean, Telugu, Irish, and English,
we find that head direction is nearly determinis-
tic. On the other hand, in Slavic languages and
in Latin and Ancient Greek we find great variabil-
ity. The fact that entropy measures on subcorpora

of 1000 sentences do not diverge greatly from en-
tropy measures on full corpora indicates that this
measure is stable with respect to sample size.

We find a potential relationship between pre-
dominant head direction and word order freedom
in head direction. Figure 1 is coded according to
whether languages have more than 50% head-final
dependencies or not. The results suggest that lan-
guages which have highly predictable head direc-
tion might tend to be mostly head-final languages.

The results here also have bearing on appro-
priate generative models for grammar induction.
Common generative models, such as DMV, use
separate multinomial models for left and right de-
pendents of a head. Our results suggest that for
some languages there should be some sharing be-
tween these distributions.

4.2 Relation Order Entropy
Relation order entropy (Section 3.2) is the con-
ditional entropy of the order of words in a local
subtree, conditioned on the tree structure, relation
types, and parts of speech. Figure 2 shows relation
order entropy for our corpora. As can be seen, this
measure is highly sensitive to sample size: for cor-
pora with a medium sample size, such as English
(16535 sentences), there is a moderate difference
between the results from subcorpora and the re-
sults from the full corpus. For other languages
with comparable size, such as Spanish (15906 sen-
tences), there is a larger difference. In the case
of languages with small corpora such as Bengali
(1114 sentences), their true relation order entropy
is almost certainly higher than measured.

While relation order entropy is the most easily
interpretable and general measure of word order
freedom, it does not seem to be workable given
current corpora and methods. In further experi-
ments, we found that removing POS tags from the
conditioning variable does not reduce the instabil-
ity of this measure.

4.3 Relation Order Entropy of Subjects and
Objects

We can alleviate the data sparsity issues of relation
order entropy by restricting our attention to a few
relations of interest. For example, the position of
subject and object in the main clause has long been
of interest to typologists (Greenberg, 1963), (cf.
(Dryer, 1992)). In Figure 3 we present relation or-
der entropy of subject and object for local subtrees
containing relations of type nsubj and dobj (obj in

98



●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

Latin
Basque

Ancient Greek
Estonian

Telugu
Russian

Bulgarian
Persian

Romanian
Bengali
Turkish
Hebrew

Croatian
German

Japanese
Finnish

Spanish
Slovak
Czech

Danish
Modern Greek

Arabic
Catalan

Tamil
Portuguese

Slovenian
Hungarian

Italian
Dutch

French
Hindi

Swedish
Irish

English

0.0 0.1 0.2 0.3
Relation Order Entropy

Figure 2: Relation order entropy in 34 languages.
The bar represents the average magnitude of rela-
tion order entropy estimated from subcorpora of
1000 sentences; the red dot represents relation or-
der entropy estimated from the whole corpus.

the case of HamleDT corpora), conditioned on the
parts of speech for these dependents.

The languages Figure 3 are colored accord-
ing to their nominative-accusative2 case marking
on nouns. We consider a language to have full
case marking if it makes a consistent morpho-
logical distinction between subject and object in
at least one paradigm. If the distinction is only
present conditional on animacy or definiteness, we
mark the language as DOM for Differential Object
Marking (Aissen, 2003).

The figure reveals a relationship between mor-
phology and this particular aspect of word order
freedom. Languages with relation order entropy
above .625 all have relevant case marking, so it
seems word order freedom in this domain im-
plies the presence of case marking. However, case
marking does not imply rigid word order; sev-
eral languages in the sample have rigid word or-
der while still having case marking. Our result is
a quantitative sharpening of the pattern claimed in
Kiparsky (1997).

Interestingly, many of the exceptional
languages—those with case marking and rigid
word order—are languages with verb-final or
verb-initial orders. In our sample, Persian, Hindi,

2Or ergative-absolutive in the case of Basque and the
Hindi past tense.

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

Tamil
Latin

Hungarian
Russian

Slovak
Basque
Telugu
Czech

Slovenian
Ancient Greek

Bengali
Bulgarian
Japanese

Catalan
German
Croatian
Spanish

Dutch
Finnish

Romanian
Turkish

Modern Greek
Hindi

Portuguese
Swedish
Estonian

Italian
Danish

Hebrew
English
Arabic

French
Persian

Irish

0.00 0.25 0.50 0.75
SO Relation Order Entropy

case

●

●

●

none

full

dom

Figure 3: Relation order entropy for subject and
object in 34 languages. Language names are an-
notated with corpus size in number of sentences.
Bars are colored depending on the nominative-
accusative case marking system type for each lan-
guage. “Full” means fully present case marking in
at least one paradigm. “dom” means Differential
Object Marking.

and Turkish are case-marking verb-final languages
where we measure low levels of freedom in the
order of subject and object. Modern Standard
Arabic is (partly) verb-initial and case-marking
(although case marking is rarely pronounced or
explicitly written in modern Arabic). This finding
is in line with recent work (Gibson et al., 2013;
Futrell et al., 2015) which has suggested that
verb-final and verb-initial orders without case
marking do not allow robust communication in a
noisy channel, and so should be dispreferred.

5 Conclusion

We have presented a set of interrelated method-
ological and linguistic issues that arise as part of
quantifying word order freedom in dependency
corpora. We have shown that conditional entropy
measures can be used to get reliable estimates of
variability in head direction and in ordering rela-
tions for certain restricted relation types. We have
argued that such measures constitute upper bounds
on word order freedom. Further, we have demon-
strated a simple relationship between morpholog-
ical case marking and word order freedom in the
domain of subjects and objects, providing to our

99



knowledge the first large-scale quantitative valida-
tion of the old intuition that languages with free
word order must have case marking.

Acknowledgments

K.M. was supported by the Department of Defense
through the National Defense Science & Engineer-
ing Graduate Fellowship program.

References
Steven Paul Abney. 1987. The English noun phrase

in its sentential aspect. Ph.D. thesis, Massachusetts
Institute of Technology.

Olga Abramov and Alexander Mehler. 2011. Auto-
matic language classification by means of syntactic
dependency networks. Journal of Quantitative Lin-
guistics, 18(4):291–336.

Judith Aissen. 2003. Differential object marking:
Iconicity vs. economy. Natural Language & Lin-
guistic Theory, 21(3):435–483.

Franklin Chang. 2009. Learning to order words: A
connectionist model of Heavy NP Shift and acces-
sibility effects in Japanese and English. Journal of
Memory and Language, 61:374–397.

Greville G Corbett. 2006. Agreement. Cambridge
University Press.

Marie-Catherine de Marneffe, Timothy Dozat, Na-
talia Silveira, Katri Haverinen, Filip Ginter, Joakim
Nivre, and Christopher D. Manning. 2014. Univer-
sal Stanford Dependencies: A cross-linguistic typol-
ogy. In Proceedings LREC’14, Reykjavı́k, Iceland.

Simon DeDeo, Robert X. D. Hawkins, Sara Klin-
genstein, and Tim Hitchcock. 2013. Bootstrap
methods for the empirical study of decision-making
and information flows in social systems. Entropy,
15(6):2246–2276.

Matthew S Dryer. 1992. The Greenbergian word order
correlations. Language, 68(1):81–138.

Victor S Ferreira and Hiromi Yoshita. 2003. Given-
new ordering effects on the production of scrambled
sentences in Japanese. Journal of psycholinguistic
research, 32(6):669–692.

Richard Futrell, Tina Hickey, Aldrin Lee, Eunice Lim,
Elena Luchkina, and Edward Gibson. 2015. Cross-
linguistic gestures reflect typological universals: A
subject-initial, verb-final bias in speakers of diverse
languages. Cognition, 136:215–221.

Edward Gibson, Steven T Piantadosi, Kimberly Brink,
Leon Bergen, Eunice Lim, and Rebecca Saxe. 2013.
A noisy-channel account of crosslinguistic word-
order variation. Psychological science, 24(7):1079–
1088.

Joseph Greenberg. 1963. Some universals of grammar
with particular reference to the order of meaningful
elements. In Joseph Greenberg, editor, Universals
of Language, pages 73–113. MIT Press, Cambridge,
MA.

Paul Kiparsky. 1997. The rise of positional licensing.
In Ans von Kemenade and Nigel Vincent, editors,
Parameters of morphosyntactic change, pages 460–
494. Cambridge University Press.

Dan Klein and Christopher D Manning. 2004. Corpus-
based induction of syntactic structure: Models of de-
pendency and constituency. In Proceedings of the
ACL, page 478. Association for Computational Lin-
guistics.

Marco Kuhlmann. 2013. Mildly non-projective de-
pendency grammar. Computational Linguistics,
39(2):355–387.

Haitao Liu and Wenwen Li. 2010. Language clusters
based on linguistic complex networks. Chinese Sci-
ence Bulletin, 55(30):3458–3465.

Haitao Liu. 2010. Dependency direction as a means
of word-order typology: A method based on depen-
dency treebanks. Lingua, 120(6):1567–1578.

Thomas McFadden. 2003. On morphological case and
word-order freedom. In Proceedings of the Berkeley
Linguistics Society.

George Miller. 1955. Note on the bias of informa-
tion estimates. In Information Theory in Psychol-
ogy: Problems and Methods, pages 95–100.

Johanna Nichols. 1986. Head-marking and dependent-
marking grammar. Language, 62.

Joakim Nivre et al.2015. Universal Dependencies 1.0.
Universal Dependencies Consortium.

Joakim Nivre. 2015. Towards a universal grammar
for natural language processing. In Computational
Linguistics and Intelligent Text Processing, pages 3–
16. Springer.

Rudolf Rosa, Jan Mašek, David Mareček, Martin
Popel, Daniel Zeman, and Zdeněk Žabokrtský.
2014. HamleDT 2.0: Thirty dependency treebanks
Stanfordized. In Proceedings LREC’14, Reykjavik,
Iceland.

E Sapir. 1921. Language, an introduction to the study
of speech. Harcourt, Brace and Co., New York.

Daniel Zeman, David Marecek, Martin Popel,
Loganathan Ramasamy, Jan Stepánek, Zdenek
Zabokrtský, and Jan Hajič. 2012. HamleDT: To
parse or not to parse? In Proceedings LREC’12,
pages 2735–2741.

George Kingsley Zipf. 1949. Human behavior and
the principle of least effort. Addison-Wesley Press,
Oxford, UK.

100


