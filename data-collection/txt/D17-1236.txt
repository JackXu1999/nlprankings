



















































Bootstrapping incremental dialogue systems from minimal data: the generalisation power of dialogue grammars


Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 2220–2230
Copenhagen, Denmark, September 7–11, 2017. c©2017 Association for Computational Linguistics

Bootstrapping incremental dialogue systems from minimal data: the
generalisation power of dialogue grammars

Arash Eshghi
Interaction Lab

Heriot-Watt University
a.eshghi@hw.ac.uk

Igor Shalyminov
Interaction Lab

Heriot-Watt University
is33@hw.ac.uk

Oliver Lemon
Interaction Lab

Heriot-Watt University
o.lemon@hw.ac.uk

Abstract

We investigate an end-to-end method for
automatically inducing task-based dia-
logue systems from small amounts of
unannotated dialogue data. It combines an
incremental semantic grammar - Dynamic
Syntax and Type Theory with Records
(DS-TTR) - with Reinforcement Learn-
ing (RL), where language generation and
dialogue management are a joint deci-
sion problem. The systems thus pro-
duced are incremental: dialogues are pro-
cessed word-by-word, shown previously
to be essential in supporting natural, spon-
taneous dialogue. We hypothesised that
the rich linguistic knowledge within the
grammar should enable a combinatorially
large number of dialogue variations to be
processed, even when trained on very few
dialogues. Our experiments show that our
model can process 74% of the Facebook
AI bAbI dataset even when trained on only
0.13% of the data (5 dialogues). It can
in addition process 65% of bAbI+, a cor-
pus1 we created by systematically adding
incremental dialogue phenomena such as
restarts and self-corrections to bAbI. We
compare our model with a state-of-the-
art retrieval model, memn2n (Bordes et al.,
2017). We find that, in terms of semantic
accuracy, memn2n shows very poor robust-
ness to the bAbI+ transformations even
when trained on the full bAbI dataset.

1 Introduction

There are currently several key problems for the
practical data-driven (rather than hand-crafted)
development of task-oriented dialogue systems,

1Dataset available at https://bit.ly/babi_plus

among them: (1) large amounts of dialogue data
are needed, i.e. thousands of examples in a do-
main; (2) this data is usually required to be anno-
tated with task-specific semantic/pragmatic infor-
mation for the domain (e.g. various dialogue act
schemes); and (3) the resulting systems are gen-
erally turn-based, and so do not support natural
spontaneous dialogue which is processed incre-
mentally, word-by-word, with many characteristic
phenomena that arise from this incrementality.

In overcoming issue (2), a recent advance made
in research on (non-task) chat dialogues has been
the development of so-called “end-to-end" sys-
tems, in which all components are trained from
textual dialogue examples, e.g. (Sordoni et al.,
2015; Vinyals and Le, 2015). However, as Bordes
and Weston (2017) argued, these end-to-end meth-
ods may not transfer well to task-based settings
(where the user is trying to achieve a domain goal,
such as booking a flight or finding a restaurant, re-
sulting in an API call). Bordes and Weston (2017)
then presented an end-to-end method using Mem-
ory Networks (memn2ns), which achieves 100%
performance on a test-set of 1000 dialogues, af-
ter being trained on 1000 dialogues. This method
processes dialogues turn-by-turn, and so does not
have the advantages of more natural incremen-
tal systems (Aist et al., 2007; Skantze and Hjal-
marsson, 2010); nor does it really perform lan-
guage generation, rather it’s based on a retrieval
model that selects from a set of candidate system
responses seen in the data.

This paper investigates an approach to these
challenges - dubbed babble - using an incremental,
semantic parser and generator for dialogue (Es-
hghi et al., 2011; Eshghi, 2015), based around the
Dynamic Syntax grammar formalism (DS, Kemp-
son et al. (2001); Cann et al. (2005)).

Our advance in this paper, for end-to-end sys-
tems, is therefore twofold: (a) the babble method

2220



overcomes the requirement for large amounts of
dialogue data (i.e. 1000s of dialogues in a do-
main); (b) resulting systems are word-by-word in-
cremental, in both parsing, generation and dia-
logue management. We show that using only 5 ex-
ample dialogues from the bAbI, Task 1 dataset (i.e.
0.13% of the training data used by (Bordes et al.,
2017)) babble can automatically induce dialogue
systems which process 74% of the bAbI testset
in an incremental manner. We then introduce an
extended incremental version of the bAbI dataset,
which we call bAbI+ (see section 4.1), which
adds some characteristic incremental phenomena
- such as mid-utterance self-corrections - to the
bAbI dialogues (this new dataset is freely avail-
able). Using this, we demonstrate that the bab-
ble system can in addition generalise to, and pro-
cess 65% of the bAbI+ dataset, still when trained
only on 5 dialogues from bAbI. We compare this
method to (Bordes et al., 2017)’s memn2n, which,
in terms of semantic accuracy (reflected in how
well api-calls are predicted at the end of bAbI
Task 1), shows very poor robustness to the bAbI+
transformations, even when it is trained on the full
bAbI dataset.

This overall method is portable to other task-
based domains. Furthermore, as we use a seman-
tic parser, the semantic/contextual representations
of the dialogue can be used directly for large-scale
inference, required in more complex tasks (e.g. in-
teractive QA and search).

1.1 Dimensions of Pragmatic Synonymy

There are two important dimensions along which
dialogues can vary, but nevertheless, lead to iden-
tical contexts: interactional, and lexical. Inter-
actional synonymy is analogous to syntactic syn-
onymy - when two distinct sentences are parsed to
identical logical forms - except that it occurs not
only at the level of a single sentence, but at the di-
alogue or discourse level. Fig. 1 shows examples
of interactional variants that lead to very similar
final contexts, in this case, that the user wants to
buy an LG phone. These dialogues can be said to
be pragmatically synonymous for this domain. Ar-
guably, a good computational model of dialogue
processing, and interactional dynamics should be
able to capture this synonymy.

Lexical synonymy relations, on the other hand,
hold among utterances, or dialogues, when differ-
ent words (or sequences of words) express mean-

ings that are sufficiently similar in a particular do-
main. What is striking about lexical synonymy re-
lations is that unlike syntactic/interactional ones,
they can often break down when one moves to an-
other domain: lexical synonymy relations are do-
main specific.

Eshghi & Lemon (2014) developed a method
similar in spirit to Kwiatkowski et al. (2013) for
capturing lexical synonymy relations by creating
clusters of semantic representations based on ob-
servations that they give rise to similar or identi-
cal extra-linguistic actions observed within a do-
main (e.g. a data-base query, a flight booking, or
any API call). Distributional methods could also
be used for this purpose (see e.g. Lewis & Steed-
man (2013)). In general, this kind of clustering is
achieved when the domain-general semantics re-
sulting from semantic parsing is grounded in a par-
ticular domain.

We note that while interactional synonymy re-
lations in dialogue should be accounted for by se-
mantic grammars or formal models of dialogue
structure (such as DS-TTR (Eshghi et al., 2012),
or KoS (Ginzburg, 2012)), lexical synonymy rela-
tions have to be learned from data.

2 Why a grammar-based approach?

Recent end-to-end data-driven machine learn-
ing approaches treat dialogue as a sequence-to-
sequence generation problem, and train their mod-
els from large datasets e.g. (Sordoni et al., 2015;
Wen et al., 2016b,a; Vinyals and Le, 2015). The
systems resulting from these types of approach are
in principle able to handle variations/patterns that
they have encountered (sufficiently often) in the
training data, but not beyond.

This large-data constraint is problematic for de-
velopers but is also strange when we consider the
structural knowledge that we have about language
and dialogue that can be encoded in grammars and
computational models of interaction. Indeed, it
is often stated that for humans to learn how to
perform adequately in a domain, one example is
enough from which to learn (e.g. Li et. al (2006)).

Furthermore, as these systems do not parse to
logical forms, they do not allow for explicit infer-
ence, which further limits their application.

We therefore develop a method combining
learning from data with an incremental seman-
tic grammar of dialogue that is able to generalise
from small number of observations in a domain –

2221



In
te

ra
ct

io
na

lV
ar

ia
tio

n
USR: I would like an LG laptop

sorry uhm phone
SYS: okay.

USR: I would like a
phone by LG.

SYS: sorry a what?
USR: a phone by LG.
SYS: okay.

SYS: what would you like?
USR: an LG phone
SYS: okay.

SYS: what would you like?
USR: a phone
SYS: by which brand?
USR: LG
SYS: okay

SYS: you’d like a ...?
USR: a phone
SYS: by what brand?
USR: LG.
SYS: okay

SYS: so would you like a computer?
USR: no, a phone.
SYS: okay. by which brand?
USR: LG.
SYS: okay.

Figure 1: Some Interactional Variations in a Shopping Domain

in fact even from just a few examples of success-
ful dialogues – to a large range of interactional and
syntactic variations, including everyday natural in-
cremental phenomena.

3 Inducing Dialogue Systems

Our overall method involves incrementally pars-
ing dialogues, and encoding the resulting seman-
tics as state vectors in a Markov Decision Pro-
cess (MDP), which is then used for Reinforcement
Learning (RL) of word-level actions for system
output (i.e. a combined incremental DM and NLG
module for the resulting dialogue system).

3.1 Dynamic Syntax and Type Theory with
Records (DS-TTR)

Dynamic Syntax (DS) is an action-based, word-
by-word incremental and semantic grammar for-
malism (Kempson et al., 2001; Cann et al., 2005),
especially suited to the highly fragmentary and
context-dependent nature of dialogue. In DS,
words are conditional actions - semantic updates;
and dialogue is modelled as the interactive and in-
cremental construction of contextual and semantic
representations (Eshghi et al., 2015) - see Fig. 2.
The contextual representations afforded by DS
are of the fine-grained semantic content that is
jointly negotiated/agreed upon by the interlocu-
tors, as a result of processing questions and an-
swers, clarification interaction, acceptances, self-
/other-corrections, restarts, and other characteris-
tic incremental phenomena in dialogue - see 3 for a
sketch of how self-corrections and restarts are pro-
cessed via a backtrack and search mechanism over
the parse search graph (see Hough (2011); Hough
and Purver (2014); Eshghi et al. (2015) for details
of the model, and how this parse search graph is
effectively the context of the conversation). Gen-
eration/linearisation in DS is defined using trial-
and-error parsing (see Section 3.2, with the pro-
vision of a generation goal, viz. the semantics

of the utterance to be generated. Generation thus
proceeds, just as with parsing, on a word-by-word
basis (see Purver et al. (2014); Hough (2015) for
details). The upshot of this is that using DS, we
can not only track the semantic content of some
current turn as it is being constructed (parsed or
generated) word-by-word, but also the context of
the conversation as whole, with the latter also en-
coding the grounded/agreed content of the con-
versation (see e.g. Fig. 4, and see Eshghi et al.
(2015); Purver et al. (2010) for details). Crucially
for our model below, the inherent incrementality
of DS-TTR together with the word-level, as well
as cross-turn, parsing constraints it provides, en-
ables the word-by-word exploration of the space
of grammatical dialogues, and the semantic and
contextual representations that result from them.

Type Theory with Records (TTR) is an exten-
sion of standard type theory shown to be useful in
semantics and dialogue modelling (Cooper, 2005;
Ginzburg, 2012). To accommodate dialogue pro-
cessing, and allow for richer representations of the
dialogue context recent work has integrated DS
and the TTR framework to replace the logical for-
malism in which meanings are expressed (Purver
et al., 2010, 2011; Eshghi et al., 2012). In TTR,
logical forms are specified as record types (RTs),
sequences of fields of the form [ l : T ] contain-
ing a label l and a type T . RTs can be witnessed
(i.e. judged as true) by records of that type, where
a record is a sequence of label-value pairs [ l = v ],
and [ l = v ] is of type [ l : T ] just in case v is of
type T (see Fig. 2 for example record types).

Importantly for us here, the standard subtype re-
lation ⊑ can be defined for record types: R1 ⊑ R2
if for all fields [ l : T2 ] in R2, R1 contains [ l : T1 ]
where T1 ⊑ T2. A record type can thus be in-
definitely extended, and is therefore always under-
specified by definition. This allows for incremen-
tally growing meanings to be expressed in a natu-
ral way as more words are parsed or generated in

2222



[
event : es
p1=today(event) : t

]
7→


event=arrive : es
p1=today(event) : t
p2=pres(event) : t
x=robin : e
p3=sub j(event,x) : t

 7→


event=arrive : es
p1=today(event) : t
p2=pres(event) : t
x=robin : e
p3=sub j(event,x) : t
x1 : e
p3= f rom(event,x1) : t


7→



event=arrive : es
p1=today(event) : t
p2=pres(event) : t
x=robin : e
p=sub j(event,x) : t
x1=S weden : e
p3= f rom(event,x1) : t


“A: Today” 7→ “..Robin arrives” 7→ “B: from?” 7→ “A: Sweden”

Figure 2: Incremental parsing using DS-TTR

Figure 3: DS-TTR: Incremental Parsing of self-corrections and restarts

turn. In addition, as will become clear below, this
subtype checking operation is the key mechanism
used in our system below for feature checking.

3.2 Overall Method: babble

In this section we describe our method for combin-
ing incremental dialogue parsing with Reinforce-
ment Learning for Dialogue Management (DM)
and Natural Language Generation (NLG) where
these are treated as a joint decision/optimisation
problem.

We start with two resources: a) a DS-TTR
parser DS (either learned from data (Eshghi et al.,
2013a), or constructed by hand), for incremental
language processing, but also, more generally, for
tracking the context of the dialogue using Eshghi
et al.’s model of feedback (Eshghi et al., 2015; Es-
hghi, 2015; Eshghi et al., 2011); b) a set D of tran-
scribed successful dialogues in the target domain.

We perform the following steps overall to in-
duce a fully incremental dialogue system from D:

1. Automatically induce the MDP state space,
S , and the dialogue goal, GD, from D;

2. Automatically define the state encoding func-
tion F : C → S ; where s ∈ S is a (binary)
state vector, designed to extract from the cur-
rent context of the dialogue, the semantic fea-
tures observed in the example dialogues D;
and c ∈ C is a DS context, viz. a pair of TTR
Record Types: ⟨cp, cg⟩, where cp is the con-
tent of the current, PENDING clause as it is
being constructed, but not necessarily fully
grounded yet; and cg is the content already

jointly built and GROUNDED by the inter-
locutors (loosely following the DGB model
of (Ginzburg, 2012)).

3. Define the MDP action set as the DS lexicon
L (i.e. actions are words);

4. Define the reward function R as reaching GD,
while minimising dialogue length.

We then solve the generated MDP using
Reinforcement Learning, with a standard Q-
learning method, implemented using BURLAP
(McGlashan, 2016): train a policy π : S → L,
where L is the DS Lexicon, and S the state space
induced using F. The system is trained in inter-
action with a (semantic) simulated user, also au-
tomatically built from the dialogue data and de-
scribed in the next section.

The state encoding function, F As shown in
figure 4 the MDP state is a binary vector of size
2 × |Φ|, i.e. twice the number of the RT fea-
tures. The first half of the state vector contains the
grounded features (i.e. agreed by the participants)
ϕi, while the second half contains the current se-
mantics being incrementally built in the current di-
alogue utterance. Formally:
s = ⟨F1(cp), . . . , Fm(cp), F1(cg), . . . , Fm(cg)⟩;
where Fi(c) = 1 if c ⊑ ϕi, and 0 otherwise. (Recall
that ⊑ is the RT subtype relation).
3.2.1 Semantic User Simulation
The simulator is in charge of two key tasks dur-
ing training: (1) generating user turns in the right
dialogue contexts; and (2) word-by-word monitor-
ing of the utterance so far generated by the sys-

2223



Grounded Semantics Current Turn Semantics Dialogue so far



x2 : e
e2=like : es
x1=US R : e
p2=pres(e2) : t
p5=sub j(e2,x1) : t
p4=ob j(e2,x2) : t
p11=phone(x2) : t





x2 : e
e2=like : es
x1=US R : e
p2=pres(e2) : t
p5=sub j(e2,x1) : t
p4=ob j(e2,x2) : t
p11=phone(x2) : t
x3 : e
p10=by(x2,x3) : t
p9=brand(x3) : t
p10=question(x3) : t



SYS: What would you like?
USR: a phone
SYS: by which brand?

RT Feature:
[

x10 : e
p15=brand(x10) : t

][
e3=like : es
p2=pres(e3) : t

] x10 : ex8 : ep14=by(x8,x10) : t

 e3=like : esx5=usr : ep7=sub j(e3,x5) : t


 x8 : ee3=like : esp6=ob j(e3,x8) : t


F1 ↓ F2 ↓ F3 ↓ F4 ↓ F5 ↓

State:
⟨ Current Turn: 1, 1, 1, 1, 1, ⟩

Grounded: 0, 1, 0, 1, 1

Figure 4: Semantics to MDP state encoding with RT features

tem during exploration (i.e. babbling grammati-
cal word sequences) by the system. To exploit
(and evaluate) the full generalisation properties of
the DS dialogue model, both (1) and (2) use the
full machinery of the DS parser, as well the state
encoding function F, described above. They are
thus performed based on the semantic context of
the dialogue so far, as tracked by DS (rather than,
e.g. being based on string or template matching).
Since this includes not just the semantic features
of the current turn, but also of the history of the
conversation, our simulator respects the turn or-
derings encountered in the data, i.e. it is sensitive
to the order in which information is gathered from
the user.

The rules required for (1) & (2) are extracted
automatically from the raw dialogue data, D, us-
ing DS and F. The dialogues in D are parsed and
encoded using F incrementally. For (1), all the
states that trigger the user into action, si = F(c)
– where c is a DS context – immediately prior to
any user turn are recorded, and mapped to what the
user ends up saying in those contexts - for more
than one training dialogue there may be more than
one candidate (in the same context/state). The
rules thus extracted will be of the form:
strig → {u1, . . . , un}, where ui are user turns.

Now note that the si’s prior to the user turns also
immediately follow system turns. And thus to per-
form (2), i.e. to monitor the system’s behaviour
during training, we only need to check further that
the current state resulting from processing a word
generated by the system, subsumes - is extendible
to - one of the si. We perform this through a sim-

ple bitmask operation (recall that the states are bi-
nary). The simulation can thus semantically iden-
tify erroneous/out-of-domain actions (words) by
the system. It would then terminate the learning
episode and penalise the system immediately, aid-
ing speed of training significantly.

4 Evaluation

We have so far induced two prototype dialogue
systems, one in an ‘electronics shopping’ domain
(see Kalatzis et al. (2016) and Fig. 1) and another
in a ‘restaurant-search’ domain, showing that fully
incremental dialogue systems can be automati-
cally induced from small amounts of unannotated
dialogue transcripts (Kalatzis et al., 2016; Eshghi
et al., 2017) - in this case both systems were boot-
strapped from a single successful example dia-
logue. We are in the process of evaluating these
systems with real users.

In this paper, however, our focus is not on build-
ing dialogue systems per se, but on: (1) study-
ing and quantifying the interactional and struc-
tural generalisation power of the DS-TTR gram-
mar formalism (see Section 2), and that of sym-
bolic, grammar-based approaches to language pro-
cessing more generally. We focus here on spe-
cific dialogue phenomena, such as mid-sentence
self-corrections, hesitations, and restarts (see be-
low); (2) doing the same for Bordes and We-
ston’s (2017) state-of-the-art, bottom up response
retrieval model, without use of linguistic knowl-
edge of any form; and (3) comparing (1) and (2).

In order to test and quantify the interactional

2224



and structural generalisation power/robustness of
the two models, babble and memn2n, we need
contrasting dialogue data-sets that control for in-
teractional vs. lexical variations in the input dia-
logues. Furthermore, to make our results compa-
rable to the existing approach of Bordes and We-
ston (2017), we need to use the same dataset that
they have used. We therefore use Facebook AI
Research’s bAbI dialogue tasks dataset (Bordes
et al., 2017). These are goal-oriented dialogues
in the domain of restaurant search. Here we tackle
Task 1, where in each dialogue the system asks the
user about their preferences for the properties of a
restaurant, and each dialogue results in an API call
which contains values of each slot obtained. Other
than the explicit API call notation, there are no an-
notations in the data whatsoever.

4.1 The bAbI+ dataset

While containing some lexical variation, the origi-
nal bAbI dialogues significantly lack interactional
variation vital for natural real-life dialogue. In
order to obtain such variation while holding lex-
ical variation constant, we created the bAbI+
dataset by systematically transforming the bAbI
dialogues.

bAbI+ is an extension of the original bAbI Task
1 dialogues with everyday incremental dialogue
phenomena (hesitations, restarts, and corrections
– see below). While the original bAbI tasks 2—7
increase the user’s goal complexity, modifications
introduced in bAbI+ can be thought of as orthog-
onal to this: we instead increase the complexity of
surface forms of dialogue utterances, while keep-
ing every other aspect of the task fixed.

The variations introduced in bAbI+ are: 1.
Hesitations, e.g. as in “we will be uhm eight”;
2. Restarts, e.g. “can you make a restau-
rant uhm yeah can you make a restaurant
reservation for four people with french cuisine in a
moderate price range”; and 3. Corrections affect-
ing task-specific information - both short-distance
ones correcting one token, e.g. “with french oh no
spanish food”, and long-distance NP/PP-level
corrections, e.g. “with french food uhm sorry
with spanish food”.

The phenomena above are mixed in probabilis-
tically from the fixed sets of templates to the origi-
nal data2. The modifications affect a total of 11336

2See https://github.com/ishalyminov/babi_
tools

utterances in the 3998 dialogues. Around 21%
of user turns contain corrections, 40% hesitations,
and 5% restarts (they are not mutually exclusive,
so that an utterance can contain up to 3 modifi-
cations). Our modifications, with respect to cor-
rections in particular, are more conservative than
those observed in real-world data: Hough (2015)
reports that self-corrections appear in 20% of all
turns of natural conversations from the British Na-
tional Corpus, and in 40% of turns in the Map
Task, a corpus of human-human goal-oriented di-
alogues. Here’s part of an example dialogue in the
bAbI+ corpus:

sys: hello what can I help you with today?
usr: I’d like to book a uhm yeah I’d like to book a

table in a expensive price range
sys: I’m on it. Any preference on a type of cuisine?
usr: with indian food no sorry with spanish food

please

4.2 Memory Network setup
In all the experiments we describe below, we fol-
low Bordes and Weston’s setup by using a memn2n
(we took an open source Tensorflow implementa-
tion for bAbI QA tasks and modified it3 accord-
ing to their setup – see details below). In or-
der to adapt the data for the memn2n, we trans-
form the dialogues into <story, question, answer>
triplets. The number of triplets for a single dia-
logue is equal to the number of the system’s turns,
and in each triplet, the answer is the current sys-
tem’s turn, the question is the user’s turn preced-
ing it, and the story is a list of all the previous turns
among both sides.

The memn2n hyperparameters are set as follows:
1 hop, and 128 as the size of embeddings; we train
it for 100 epochs with a learning rate of 0.01 and
a batch size of 8 – in this we follow the best bAbI
Task 1 setup reported by (Bordes et al., 2017).

4.3 Testing the DS-TTR parser
Dynamic Syntax (DS) lexicons are learnable from
data (Eshghi et al., 2013a,b). But since the lexicon
was induced from a corpus of child-directed utter-
ances in this prior work, there were some construc-
tions as well as individual words that it did not
include4. One of the authors therefore extended
this induced grammar manually to cover the bAbI
dataset, which, despite not being very diverse,

3See https://github.com/ishalyminov/memn2n
4We are currently looking into applying Eshghi et al.’s

(2013a) model to induce DS grammars from larger seman-
tic corpora such as the Groningen Meaning Bank, leading to
much more wide-coverage lexicons

2225



contains a wide range of complex grammatical
constructions, such as long sequences of preposi-
tional phrases, adjuncts, short answers to yes/no
and wh-questions, appositions of NPs, causative
verbs etc.

We parsed all dialogues in the bAbI train and
test sets, as well as on the bAbI+ corpus word-by-
word, including both user and system utterances,
in context. The grammar parses 100% of the dia-
logues, i.e. it does not fail on any word in any of
the dialogues. We assess the semantic accuracy of
the parser on bAbI & bAbI+ using the dialogue-
final api-calls in section 4.5 below.

4.4 Experiment 1: Generalisation from small
data

We have now set out all we need to perform the
first experiment. Our aim here is to assess the
generalisation power that results from the gram-
mar and our state encoding method (section 3.1)
- we dub our overall model babble - and compare
this to the state of the art results of Bordes et al.
(2017). The method in Bordes et al. (2017) is not
generative, rather it is based on retrieval of sys-
tem responses, based on the history of the dialogue
up to that point. Therefore, for direct comparison,
and for simplicity of exposition, we do the same
here: we apply the method described for creating
a user simulation (section 3.2.1), this time for the
system side, resulting in a ‘system simulation’. We
then use this to predict a system response, by pars-
ing and encoding the containing test dialogue up
to the point immediately prior to the system turn.
This results in a triggering state, strig, which is
then used as the key to look up the system’s re-
sponse from the rules constructed as per section
3.2.1. The returned response is then parsed word-
by-word as normal, and this same process con-
tinues for the rest of the dialogue. This method
uses the full machinery of DS-TTR & our state-
encoding method - the babble model - and will
thus reflect the generalisation properties that we
are interested in.

Cross-Validation Since we are here interested
in data efficiency and generalisation we use all the
bAbI and bAbI+ data - the train, dev, and test sets
- as follows: we train Bordes & Weston’s memn2n
and babble from 1-5 examples selected at random
from the longest dialogues in bAbI – note bAbI+
data is never used for training in these experi-
ments. This process is repeated across 10 folds.

The models are then tested on sets of 1000 ex-
amples selected at random, in each fold. Both
the training and test sets constructed in this way
are kept constant in each fold across the babble &
memn2n models. The test sets are selected either
exclusively from bAbI or exclusively from bAbI+.

4.4.1 Results: Predicting system turns

Table 1 shows per utterance accuracies for the bab-
ble & memn2n models. Per utterance accuracy is
the percentage of all system turns in the test di-
alogues that were correctly predicted. The table
shows that babble can generalise to a remarkable
74% of bAbI and 65% of bAbI+ with only 5 input
dialogues from bAbI. It also shows that memn2ns
can also generalise remarkably well. Although as
discussed below, this result is misleading on its
own as the memn2ns are very poor at generating
the final api-calls correctly on both the bAbI &
bAbI+ data, and are thus making too many seman-
tic mistakes.

4.5 Experiment 2: Semantic Accuracy

The results from Experiment 1 on their own can
be misleading, as correct prediction of system re-
sponses does not in general tell us enough about
how well the models are interpreting the dia-
logues, or whether they are doing this with a suf-
ficient level of granularity. To assess this, in this
second experiment, we measure the semantic ac-
curacy of each model by looking exclusively at
how accurately they predict the final api-calls
in the bAbI & bAbI+ datasets. For the memn2n
model, we follow the same overall procedure as in
the previous experiment: train on bAbI data, and
test on bAbI+.

4.5.1 Results: Prediction of api-calls

BABBLE Mere successful parsing of all the di-
alogues in the bAbI and bAbI+ datasets as shown
above doesn’t mean that the semantic representa-
tions compiled for the dialogues were in fact cor-
rect. To measure the semantic accuracy of the DS-
TTR parser we programmatically checked that the
correct slot values – those in the api-call anno-
tations – were in fact present in the semantic rep-
resentations produced by the parser for each dia-
logue (see Fig. 2 for example semantic representa-
tions). We further checked that there is no other in-
correct slot value present in these representations.
The results showed that the parser has 100% se-

2226



# of training dialogues: 1 2 3 4 5
babble on bAbI 67.12 73.36 72.63 73.32 74.08
memn2n on bAbI 2.77 59.15 70.94 71.68 72.6
babble on bAbI+ 59.42 65.27 63.45 64.34 65.2
memn2n on bAbI+ 0.22 56.75 68.65 71.84 73.2

Table 1: Mean per utterance accuracies (%) for memn2n & babble models across the bAbI & bAbI+
datasets (10 folds)

mantic accuracy on both bAbI and bAbI+5. This
result is not surprising, given that DS-TTR is a
general model of incremental language process-
ing, including phenomena such as self-corrections
and restarts (see Hough (2015) for details of the
model).

MEMN2N Given just 1 to 5 training instances
from bAbI as in the previous experiment, the mean
api-call prediction accuracy of the memn2n
model is nearly 0 on both bAbI and bAbI+.
This is not at all unexpected, since prediction of
the api-calls is a generative process, unlike the
prediction of system turns which can be done on a
retrieval/look-up basis alone. For this, the model
needs to observe the different word sequences that
might determine each parameter (slot) value, and
observe them with sufficient frequency and vari-
ation. This is unlike a semantic parser like DS-
TTR, that produces semantic representations for
the dialogues as a result of the structural, linguis-
tic knowledge that it embodies.

Nevertheless, we were also interested in the
general semantic robustness of the memn2nmodel,
to the transformations in bAbI+, i.e. how well
does the memn2nmodel interpret bAbI+ dialogues,
when trained on the full bAbI dataset? Does it
then learn to generalise to (process) the bAbI+ di-
alogues with sufficient semantic accuracy?

Table 2 shows that we can fully replicate the re-
sults reported in Bordes et al. (2017): the memn2n
model can predict the api-calls with 100% ac-
curacy, when trained on the bAbI train-set and
tested on the bAbI test-set. But when this same
model is tested on bAbI+, the accuracy drops to a

5A helpful reviewer points out that the DS-TTR setup is
a carefully tuned rule-based system, thus perhaps rendering
these results trivial. But we note that the results here are not
due to ad-hoc constructions of rules/lexicons, but due to the
generality of the grammar model, and its attendant incremen-
tal, left-to-right properties; and that the same parser can be
used in other domains. Furthermore, the ability to process
self-corrections, restarts, etc. “comes for free”, without the
need to add or posit new machinery

testing configuration accuracy
memn2n on bAbI 100
memn2n on bAbI+ 28

Table 2: api-call prediction accuracies (%) for
the memn2n model trained on the bAbI trainset

very poor 28%, making any dialogue system built
using this model unusable in the face of natural,
spontaneous dialogue data. This is further dis-
cussed below.

5 Discussion
5.1 babble

The method described above has the following
advantages over previous approaches to dialogue
system development:

– incremental (and thus more natural) language
understanding, dialogue management, and gener-
ation;

– “end-to-end" method for task-based systems:
no Dialogue Act annotations are required (i.e. re-
duced development time and effort);

– a complete dialogue system for a new task can
be automatically induced, using only ‘raw’ data –
i.e. successful dialogue transcripts;

– the MDP state and action spaces are automat-
ically induced, rather than having to be designed
by hand (as in prior work);

– wide-coverage, task-based dialogue systems
can be built from much smaller amounts of data as
shown in section 4 .

This final point bears further examination. As
an empirically adequate model of incremental lan-
guage processing in dialogue, the DS-TTR gram-
mar is required to capture interactional variants
such as question-answer pairs, over- and under-
answering, self- and other-corrections, clarifica-
tion, split-utterances, and ellipsis more generally.
As we showed in section 4, even if most of these
structures are not present in the training exam-

2227



ple(s), the resulting trained system is able to han-
dle them, thus resulting in a very significant gen-
eralisation around the original data.

We also note that since we were in this instance
interested in a direct comparison with memn2ns
over the bAbI & bAbI+ datasets, we didn’t ex-
ploit the power of Reinforcement Learning and
exploration as we described above - as we have
done before with other systems (Kalatzis et al.,
2016). Therefore the generalisation results we
report above for babble follow entirely from the
knowledge present within the grammar as a com-
putational model of dialogue processing and con-
textual update, rather than this having been learned
from data. Applying the full RL method described
above would have meant that the system would
actually discover many interactional and syntac-
tic variations that are not present in bAbI, nor in
bAbI+.

5.2 memn2n

Even when trained on very few training instances,
the memn2n model was able to predict system re-
sponses remarkably well. But results from Exper-
iment 2 above showed that this was misleading:
the memn2ns were making a drastic number of se-
mantic mistakes when interpreting the dialogues,
both in the bAbI and bAbI+ datasets. Even when
trained on the full bAbI data-set, the model per-
formed badly on bAbI+ in terms of semantic ac-
curacy. We diagnose these results as follows:
Problem complexity: The first thing to notice is
that in bAbI dialogue Task 1, the responses are
highly predictable and stay constant regardless
of the actual task details (slot values) up to the
point of the final api-calls; and further, that the
prediction of api-calls is a generative process,
unlike the prediction of the system turns, which
is retrieval-based. This, in our view, explains
the very large difference in memn2n performance
across the two prediction tasks.

Model robustness to the bAbI+ transforma-
tions:. The variations introduced in bAbI+ are
repetitions of both content and non-content words,
as well additional incorrect slot values. The model
was working in the same setup as babble, there-
fore none of those variations could be treated as
unknown tokens for either system. Although in
the case of memn2n, some of the mixed-in words
never appeared in the training data, and bAbI+ ut-
terances were augmented significantly with those

words – so it was interesting to see how such un-
trained embeddings would affect the latent mem-
ory representations inside memn2n. The resulting
performance suggests that there was no signifi-
cant impact on memn2n from these variations as
far as the predicting system responses was con-
cerned. But the incorrect slot values introduced
in self-corrections affect the system’s task com-
pletion performance significantly, only appearing
at the point of api-call predictions.

We note also that none of our experiments in
this paper involved training memn2n on bAbI+
data. There is a very interesting question here: is
the memn2nmodel in principle able to learn to pro-
cess the bAbI+ structures if it is in fact trained on
it? And how much bAbI+ data would it require to
do so? These issues are address in detail in Sha-
lyminov et al. (2017).

6 Conclusions

Our main advances are in a) training end-to-end
dialogue systems from small amounts of data,
b) incremental processing for wider coverage of
more natural everyday dialogues (e.g. containing
self-repairs).

We compared our grammar-based approach
to dialogue processing (DS-TTR) with a state-
of-the-art, end-to-end response retrieval model
(memn2ns) (Bordes et al., 2017), when training on
small amounts of dialogue data.

Our experiments show that our model can pro-
cess 74% of the Facebook AI bAbI dataset even
when trained on only 0.13% of the data (5 dia-
logues). It can in addition process 65% of bAbI+,
a corpus we created by systematically adding in-
cremental dialogue phenomena such as restarts
and self-corrections to bAbI. We find on the other
hand that the memn2n model is not robust to the
structures we introduced in bAbI+, even when
trained on the full bAbI dataset.

Acknowledgements

This research is supported by the EPSRC, un-
der grant number EP/M01553X/1 (BABBLE
project6).

6https://sites.google.com/site/
hwinteractionlab/babble

2228



References
Gregory Aist, James Allen, Ellen Campana, Car-

los Gomez Gallo, Scott Stoness, Mary Swift, and
Michael K Tanenhaus. 2007. Incremental dialogue
system faster than and preferred to its nonincremen-
tal counterpart. In Annual Conference of the Congi-
tive Science Society.

Antoine Bordes, Y-Lan Boureau, and Jason Weston.
2017. Learning end-to-end goal-oriented dialog.
ICLR https://openreview.net/pdf?id=S1Bb3D5gg.

Ronnie Cann, Ruth Kempson, and Lutz Marten. 2005.
The Dynamics of Language. Elsevier, Oxford.

Robin Cooper. 2005. Records and record types in se-
mantic theory. Journal of Logic and Computation
15(2):99–112.

A. Eshghi, C. Howes, E. Gregoromichelaki, J. Hough,
and M. Purver. 2015. Feedback in conversation
as incremental semantic update. In Proceedings of
the 11th International Conference on Computational
Semantics (IWCS 2015). Association for Computa-
tional Linguisitics, London, UK.

A. Eshghi, M. Purver, and Julian Hough. 2011. Dylan:
Parser for dynamic syntax. Technical report, Queen
Mary University of London.

Arash Eshghi. 2015. DS-TTR: An incremental, seman-
tic, contextual parser for dialogue. In Proceedings
of Semdial 2015 (goDial), the 19th workshop on the
semantics and pragmatics of dialogue.

Arash Eshghi, Julian Hough, and Matthew Purver.
2013a. Incremental grammar induction from child-
directed dialogue utterances. In Proceedings of the
4th Annual Workshop on Cognitive Modeling and
Computational Linguistics (CMCL). Association for
Computational Linguistics, Sofia, Bulgaria, pages
94–103.

Arash Eshghi, Julian Hough, Matthew Purver,
Ruth Kempson, and Eleni Gregoromichelaki.
2012. Conversational interactions: Capturing
dialogue dynamics. In S. Larsson and L. Borin,
editors, From Quantification to Conversation:
Festschrift for Robin Cooper on the occasion
of his 65th birthday, College Publications, Lon-
don, volume 19 of Tributes, pages 325–349.
http://www.eecs.qmul.ac.uk/mpurver/papers/eshghi-
et-al12cooper.pdf.

Arash Eshghi and Oliver Lemon. 2014. How domain-
general can we be? Learning incremental dialogue
systems without dialogue acts. In Proceedings of
Semdial 2014 (DialWatt).

Arash Eshghi, Matthew Purver, Julian Hough, and
Yo Sato. 2013b. Probabilistic grammar induction in
an incremental semantic framework. In CSLP, Lec-
ture Notes in Computer Science, Springer.

Arash Eshghi, Igor Shalyminov, and Oliver Lemon.
2017. Interactional Dynamics and the Emergence
of Language Games. In Proceedings of the ESSLLI
2017 workshop on Formal approaches to the Dy-
namics of Linguistic Interaction. Barcelona.

Jonathan Ginzburg. 2012. The Interactive Stance:
Meaning for Conversation. Oxford University
Press.

Julian Hough. 2011. Incremental semantics driven nat-
ural language generation with self-repairing capabil-
ity. In Recent Advances in Natural Language Pro-
cessing (RANLP). Hissar, Bulgaria, pages 79–84.

Julian Hough. 2015. Modelling Incremental Self-
Repair Processing in Dialogue. Ph.D. thesis, Queen
Mary University of London.

Julian Hough and Matthew Purver. 2014. Proba-
bilistic type theory for incremental dialogue pro-
cessing. In Proceedings of the EACL 2014 Work-
shop on Type Theory and Natural Language Se-
mantics (TTNLS). Association for Computational
Linguistics, Gothenburg, Sweden, pages 80–88.
http://www.aclweb.org/anthology/W14-1410.

Dimitrios Kalatzis, Arash Eshghi, and Oliver Lemon.
2016. Bootstrapping incremental dialogue systems:
using linguistic knowledge to learn from minimal
data. In Proceedings of the NIPS 2016 workshop
on Learning Methods for Dialogue. Barcelona.

Ruth Kempson, Wilfried Meyer-Viol, and Dov Gabbay.
2001. Dynamic Syntax: The Flow of Language Un-
derstanding. Blackwell.

Tom Kwiatkowski, E. Choi, Y. Artzi, and L. Zettle-
moyer. 2013. Scaling semantic parsers with on-the-
fly ontology matching. In Proceedings of the Con-
ference on Empirical Methods in Natural Language
Processing (EMNLP).

Mike Lewis and Mark Steedman. 2013. Com-
bined distributional and logical seman-
tics. Transactions of the Association
for Computational Linguistics 1:179–192.
http://aclweb.org/anthology//Q/Q13/Q13-1015.pdf.

Fei-Fei Li, Robert Fergus, and Pietro Perona. 2006.
One-shot learning of object categories. IEEE
Trans. Pattern Anal. Mach. Intell. 28(4):594–611.
https://doi.org/10.1109/TPAMI.2006.79.

James McGlashan. 2016. BURLAP: Brown-
UMBC Reinforcement Learning and Planning.
http://burlap.cs.brown.edu/ .

Matthew Purver, Arash Eshghi, and Julian Hough.
2011. Incremental semantic construction in a di-
alogue system. In J. Bos and S. Pulman, editors,
Proceedings of the 9th International Conference on
Computational Semantics. Oxford, UK, pages 365–
369.

2229



Matthew Purver, Eleni Gregoromichelaki, Wilfried
Meyer-Viol, and Ronnie Cann. 2010. Splitting the
‘I’s and crossing the ‘You’s: Context, speech acts
and grammar. In P. Łupkowski and M. Purver, ed-
itors, Aspects of Semantics and Pragmatics of Dia-
logue. SemDial 2010, 14th Workshop on the Seman-
tics and Pragmatics of Dialogue. Polish Society for
Cognitive Science, Poznań, pages 43–50.

Matthew Purver, Julian Hough, and Eleni Gre-
goromichelaki. 2014. Dialogue and compound
contributions. In S. Bangalore and A. Stent, edi-
tors, Natural Language Generation in Interactive
Systems, Cambridge University Press, pages 63–92.
http://www.cambridge.org/us/academic/subjects/engineering/communications-
and-signal-processing/natural-language-generation-
interactive-systems.

Igor Shalyminov, Arash Eshghi, and Oliver Lemon.
2017. Challenging Neural Dialogue Models with
Natural Data: Memory Networks Fail on Incremen-
tal Phenomena. In Proceedings of the 21st Work-
shop on the Semantics and Pragmatics of Dialogue
(SemDial 2017 - SaarDial).

Gabriel Skantze and Anna Hjalmarsson. 2010. To-
wards incremental speech generation in dialogue
systems. In Proceedings of the SIGDIAL 2010 Con-
ference. Association for Computational Linguistics,
Tokyo, Japan, pages 1–8.

Alessandro Sordoni, Michel Galley, Michael Auli,
Chris Brockett, Yangfeng Ji, Margaret Mitchell,
Jian-Yun Nie, Jianfeng Gao, and Bill Dolan. 2015.
A neural network approach to context-sensitive
generation of conversational responses. arXiv
(1506.06714).

Oriol Vinyals and Quoc Le. 2015. A neural conversa-
tional model. arXiv (1506.05869v3).

Tsung-Hsien Wen, Milica Gašić, Nikola Mrkšić, Lina
M. Rojas-Barahona, Pei-Hao Su, David Vandyke,
and Steve Young. 2016a. Multi-domain neural
network language generation for spoken dialogue
systems. In Proceedings of the 2016 Conference
on North American Chapter of the Association for
Computational Linguistics (NAACL). Association
for Computational Linguistics.

Tsung-Hsien Wen, David Vandyke, Nikola Mrkšić,
Milica Gašić, Lina M. Rojas-Barahona, Pei-Hao Su,
Stefan Ultes, and Steve Young. 2016b. A network-
based end-to-end trainable task-oriented dialogue
system. arXiv preprint: 1604.04562 .

2230


