



















































Auditing Deep Learning processes through Kernel-based Explanatory Models


Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing
and the 9th International Joint Conference on Natural Language Processing, pages 4037–4046,
Hong Kong, China, November 3–7, 2019. c©2019 Association for Computational Linguistics

4037

Auditing Deep Learning processes through
Kernel-based Explanatory Models

Danilo Croce and Daniele Rossini and Roberto Basili
Department of Enterprise Engineering

University of Roma, Tor Vergata
{croce,basili}@info.uniroma2.it

rossini.danie@gmail.com

Abstract

While NLP systems become more pervasive,
their accountability gains value as a focal point
of effort. Epistemological opaqueness of non-
linear learning methods, such as deep learn-
ing models, can be a major drawback for their
adoptions. In this paper, we discuss the ap-
plication of Layerwise Relevance Propagation
over a linguistically motivated neural architec-
ture, the Kernel-based Deep Architecture, in
order to trace back connections between lin-
guistic properties of input instances and sys-
tem decisions. Such connections then guide
the construction of argumentations on the net-
work’s inferences, i.e., explanations based on
real examples that are semantically related to
the input. We also propose here a method-
ology to evaluate the transparency and co-
herence of analogy-based explanations model-
ing an audit stage for the system. Quantita-
tive analysis on two semantic tasks, i.e., ques-
tion classification and semantic role labeling,
shows that the explanatory capabilities (native
in KDAs) are effective and they pave the way
to more complex argumentation methods.

1 Introduction

AI systems are currently used in a wide variety of
applications, with several levels of societal impact,
and are expected to be soon deployed in safety-
critical fields, e.g., autonomous driving. The def-
inition of codes of conduct in the development of
AI applications to ensure their ethical sustainabil-
ity across dimensions, such as fairness, reliability
and beneficialness (Kroll et al., 2016; Garfinkel
et al., 2017; Dignum, 2017) is becoming a cru-
cial issue. Hence, a natural need for the ethical
accountability of such systems is gaining impor-
tance.

A central issue lies in designing systems whose
decisions are transparent (Ribeiro et al., 2016;

Doshi-Velez et al., 2017), i.e., they must be eas-
ily interpretable by humans, as users must be able
to suitably weight and trust the assistance of such
systems.

Deep neural networks are clearly problematic
in this regard: their high non-linearity, despite al-
lowing for state-of-the-art performances in several
challenging problems, amplifies the epistemologi-
cal opaqueness of the decision-flow and limits its
interpretability. The concept of transparency of
a machine learning model spans multiple defini-
tions, focusing on different aspects, from the sim-
plicity of the model, e.g., the number of nodes in a
decision tree, to the intuitiveness of its parameters
and computations (Chakraborty et al., 2017).

In this context, an important capability of an
AI system is the ability to provide post-hoc ex-
planations in terms of evidences supporting the
produced decisions: although they usually do not
formally elucidate how a model works, they of-
ten have the property of being quite intuitive, con-
veying useful information also to end-users with-
out any AI or machine learning expertise (Lipton,
2018). In semantic inference tasks (e.g., text clas-
sification), an explanation model generating post-
hoc explanations should hence be able to trace
back connections between the output categories
and the semantic and syntactic properties of the in-
put texts. Such models should have three desired
properties: semantic transparency, informative-
ness w.r.t. the system decision and effectiveness
in enabling auditing processes against the system.

In this work we focus on a specific post-hoc
mechanism, which is to provide, along with the
prediction, a comparison with one or more other
examples, namely landmarks, that share task-
relevant linguistic properties with the input. From
an argument theory perspective, this corresponds
to supporting decisions through an “argument by
analogy” schema (Walton et al., 2008): a user ex-



4038

posed to such a kind of argument will endow a
different level of trust into the machine decision
according to the linguistic plausibility of the anal-
ogy. He will implicitly gauge the evidence from
the linguistic properties shared between the input
sentence (or its parts) and the one(s) used for com-
parison as well their importance with respect to
the output decision. Let us consider, for exam-
ple, the following prediction in a question classi-
fication (QC) task (Li and Roth, 2006): “What is
the capital of Zimbabwe?” refers to a Location.
We would like the system to motivate its decision
with an argument such as: ...since it recalls me of
“What is the capital of California?” which also
refers to a Location. Notice that explanation
of a decision is quite different from sentence or
document ranking in Information Retrieval so that
semantic similarity plays only a minor role: clear
and trustful analogies may exist with semantically
different training examples that imply similar re-
lationships between the input and the decision.

Recent work has been inspired by efforts in im-
proving model’s interpretability in image process-
ing tasks, in particular by the Layerwise Relevance
Propagation (LRP) (Bach et al., 2015). In LRP the
classification decision of a deep neural network is
decomposed backward across the network layers
and evidence about the contribution to the final de-
cision brought by individual input fragments (i.e.,
pixels of the input image) is gathered.

In this paper, we propose to extend the LRP ap-
plication to the linguistically motivated network
architectures, known as Kernel-Based Deep Ar-
chitectures (KDAs) (Croce et al., 2017), which
frames semantic information captured by linguis-
tic Tree Kernel methods (Collins and Duffy, 2001)
within the neural-based learning paradigm. The
result is a mechanism that, for each system’s pre-
diction such as in question classification, gener-
ates an argument-by-analogy explanation based on
real training examples, not necessarily similar to
the input.

We also propose a novel approach to eval-
uate numerically the interpretability of any
explanation-enriched model applied in semantic
inference tasks. By defining a specific audit pro-
cess, we derive a synthetic metric, i.e. Audit-
ing Accuracy, that takes into account the prop-
erties of transparency, informativeness and effec-
tiveness. The evaluation of the proposed method-
ology shows the meaningful impact of LRP-based

explanation models: users faced with explanations
are systematically oriented to accept (or reject) the
system decisions, so that post hoc judgments may
even improve the overall application accuracy.

In the rest of the paper, section 2 reports re-
lated works, while Section 3 describes the LRP
and its extension to KDAs. In Section 4, we
propose three explanation models and illustrate
a novel evaluation methodology, commenting on
the audit process and deriving quantitative notions
such as the Auditing Accuracy measure. Section
5 presents and discusses the system effectiveness
against two semantic tasks, i.e., question classifi-
cation and frame-based argument classification in
a semantic role labeling chain. Finally, in Section
6 conclusions are derived.

2 Related Work

In recent years, research communities showed
great interest in improving neural models’ inter-
pretability, as testified by the effort of defining
the concept of interpretability itself and the de-
velopment of a variety of approaches to the prob-
lem. In (Chakraborty et al., 2017) and (Lipton,
2018), the authors examine the different notions of
interpretability found in literature and categorize
techniques according to the transparency proper-
ties they confer to decision models. Common ap-
proaches to improve the readability of a neural
model in image-related tasks are based on back-
ward algorithms that reuse arc weights to propa-
gate the prediction down to the input (Erhan et al.,
2010; Zeiler and Fergus, 2013), thus leading to
the re-creation of meaningful patterns in the in-
put space. Typical examples are deconvolution
heatmaps, used to approximate through Taylor se-
ries the partial derivatives at each layer (Simonyan
et al., 2013), or the so-called Layer-wise Rele-
vance Propagation (LRP), that redistributes back
positive and negative evidence across the layers
(Bach et al., 2015).

Local explanation approaches focus on high-
lighting a handful of crucial features (Baehrens
et al., 2010) or deriving simpler, more readable
models from a complex one, e.g., a binary decision
tree (Frosst and Hinton, 2017), or by local approx-
imation with linear models (Ribeiro et al., 2016).
However, although they can explicitly show the
representations learned in the specific hidden neu-
rons (Frosst and Hinton, 2017), these approaches
base their effectiveness on the user ability to study



4039

the quality of the reasoning and of the account-
ability as a side effect of the quality and the co-
herence of the features selection: this can be very
hard in tasks where boundaries between classes
are not well defined. Another strategy is pair-
ing the decision model with a generative model
to produce verbose explanations (Krening et al.,
2017). Sometimes explanations are associated to
vector representations as in (Ribeiro et al., 2016),
i.e., bag-of-words in case of text classification, that
are clearly weak at capturing significant linguistic
abstractions, such as the involved syntactic rela-
tions. In this work, we systematically extend the
model presented in (Croce et al., 2018) which al-
lows to provide explanations that are easily inter-
pretable even by non-expert users, as they are ex-
pressed in natural language. Moreover, the inves-
tigated approach is is computationally affordable,
as it roughly corresponds to a forward pass across
the network. In addition, we also provide a sys-
tematic way to evaluate the provided explanations
with a methodology able to support the audit of
the targeted AI systems.

3 Layer-wise Relevance Propagation in
Kernel-based Deep Architectures

In this section, we will review the Layer-wise Rel-
evance Propagation technique (LRP, as in (Bach
et al., 2015)), usually applied in image process-
ing, and show how it can be naturally extended
to Kernel-based Deep Architectures (KDA, as in
(Croce et al., 2017)) in order to select real exam-
ples useful to support the network decisions.

LRP is mainly a framework which allows to de-
compose the prediction of a deep neural network
computed over a sample, usually an image, down
to relevance scores for the single input dimensions
of the sample, such as sub-pixels of the image it-
self. More formally, let f : Rd → R+ be a func-
tion that quantifies, for example, the probability of
x ∈ Rd being in a certain class. The Layer-wise
Relevance Propagation assigns to each dimension,
or feature, xd a relevance score R

(1)
d such that

f(x) ≈
∑

dR
(1)
d . Features whose score is R

(1)
d >

0 or R(1)d < 0 correspond to evidence in favor
or against, respectively, the output classification.
In other words, LRP allows to identify fragments
of the input playing key roles in the decision, by
propagating relevance backwards. Let us suppose
to know the relevance score R(l+1)j of a neuron j
at network layer l + 1. This can be decomposed

into messages R(l,l+1)i←j sent from j to neurons i

in layer l according to R(l+1)j =
∑

i∈(l)R
(l,l+1)
i←j .

Then ti directly follows that the relevance of a neu-
ron i at layer l, that is the quantity of informa-
tion travelling through i, can be defined as R(l)i =∑

j∈(l+1)R
(l,l+1)
i←j . In this work, we adopted the �-

rule defined in (Bach et al., 2015) to compute the
messages R(l,l+1)i←j =

zij
zj+�·sign(zj)R

(l+1)
j , where

zij = xiwij and � > 0 is a small numerical sta-
bilizing term. The informative value is justified by
the fact that the weights zij are linked to the acti-
vation weights wij of the input neurons.

Given the capability of computing relevance
scores for input dimensions, we now summa-
rize the KDA to motivate how LRP can be ap-
plied also to tasks other than image classifica-
tion. In a nutshell, the KDA is a neural network
trained in low-dimensional spaces which approxi-
mate a generic Reproducing Kernel Hilbert Space
(RKHS) (Shawe-Taylor and Cristianini, 2004).
These low-dimensional approximations are de-
rived as a reconstruction from a set of real refer-
ence training examples, called landmarks, which
can be used to compile the representation of any
unseen test instance. As a consequence, the abil-
ity of making connections between the KDA de-
cisions and the landmarks corresponds to locating
the candidate training examples that justify (in the
LRP sense) decisions and trigger meaningful lin-
guistic explanations.

More formally, given an input dataset D, a ker-
nel K(oi, oj) is a function over D2 operating dot-
products, i.e., similarity scores, in an projection
space, given by the mapping Φ over the input in-
stances oi, which is implicit in the sense that the
kernel never explicitly accesses the representation
of projections Φ(oi). Here a RKHS corresponds
to the Gram Matrix G = XX>, whose element
Gi,j is K(oi, oj) = Φ(oi) · Φ(oj). The Nyström
method (Williams and Seeger, 2001) can be ap-
plied to derive the approximating matrix G̃ =
(CUS−

1
2 )(CUS−

1
2 )> ≈ G, where U , S are ob-

tained by applying the Singular Value Decompo-
sition to the matrix W ∈ Rl×l, a submatrix of G
containing the kernel evaluations of l sampled in-
stances (namely, the landmarks) and C ∈ R|D|×l,
whose row ~ci corresponds to the similarity scores
between oi ∈ D and the landmarks. Hence, a map-
ping from D to a l-dimensional embedding, with
l � n, is naturally provided by the projection



4040

N
ys

trö
m

Pr
oj

ec
tio

n

…… …

K(x, l1)

K(x, l2)

K(x, ld)

hidden
layers

classification
layer

landmarks

…

Nyström
layer

input
layer

x

Figure 1: Kernel-based Deep Architecture.

function ~̃x = ~c US−
1
2 . Therefore, the method

produces l-dimensional vectors1.
In (Croce et al., 2017), the Nyström represen-

tation ~̃x has been used to map semantically an-
notated grammatical trees to the linear input of
a Multi-Layer Perceptron (MLP). In fact, given
a dataset L, with o ∈ L denoting a generic in-
stance, the MLP architecture is defined with a spe-
cific Nyström input layer based on the Nyström
embeddings. The resulting Kernel-based Deep
Architecture (KDA) includes an input layer, the
Nyström layer, a sequence of hidden layers and
the final classification layer, which produces the
output. The input layer corresponds to the input
vector ~ci, i.e., the row of the C matrix associ-
ated to an example oi. The input layer is mapped
to the Nyström layer, through the Nyström pro-
jection. Notice that the embedding provides also
the proper weights, defined by US−

1
2 , so that the

mapping can be expressed through the Nyström
matrix HNy = US−

1
2 . The resulting ~̃x is the in-

put to one or more hidden layers. Clearly, the first
hidden layer receives in input ~̃x = ~cHNy. Finally,
the classification layer computes a linear classifi-
cation function with a softmax operator, as shown
in Figure 1. A KDA optimizes the standard cross-
entropy function with L2 regularization.

It is worth recalling that the network is trig-
gered by an input vector ~c expressing the kernel
evaluations K(x, li) between the example and the
landmarks. When using linguistic kernels (such as
Semantic Tree Kernels, (Croce et al., 2011)), this
measure corresponds to the grammatical and lexi-
cal semantic similarity between x and the subset of
landmarks. The expected explanation is obtained
from the network output by applying LRP to re-

1Note that, while any sampling policy for landmarks is
admissible, in (Kumar et al., 2012) it is demonstrated that
uniform sampling without replacement achieves results com-
parable with alternative, more resource-consuming policies.

vert the propagation process, thus linking the out-
put back to the input. In a KDA that models lin-
guistic instances, LRP implicitly traces back the
syntactic, semantic and lexical relations between
the example and the landmarks across the Nyström
layer: the side effect is to select those real exam-
ples that mostly influenced the identification of the
predicted structure in the sentence.

4 Generating explanations in
Kernel-based Deep Architectures

Justifications for the KDA emissions can be ob-
tained by exploiting landmarks {`} as the evidence
in favour or against a class. The idea is to select
those {`} that the LRP highlights as the most ac-
tive elements in layer 0. Once such active land-
marks are detected, an Explanatory Model is a
function in charge to compile a linguistically flu-
ent explanation by comparing the input case with
such selection.

The semantic expressiveness of such analo-
gies makes the resulting explanation clear and in-
creases the user confidence on the system reliabil-
ity. When a sentence s is classified, LRP assigns
activation scores rs` to each individual landmark
`: let L(+) (or L(−)) denote the set of landmarks
with positive (or negative) activation scores. For-
mally, each explanation is characterized by a triple
e = 〈s, C, τ〉 where s is the input sentence, C is
a target label and τ is the modality of the explana-
tion: τ = +1 for positive (i.e., acceptance) state-
ments while τ = −1 correspond to rejections of
C. A landmark ` is positively activated for a given
sentence s if there are at most k − 1 other active
landmarks `′ with activation value higher than the
one for `, i.e.,

|{`′ ∈ L(+) : `′ 6= ` ∧ rs`′ ≥ rs` > 0}| < k

Similarly, a landmark is negatively activated
when:

|{`′ ∈ L(−) : `′ 6= ` ∧ rs`′ ≤ rs` < 0}| < k

where k is a fixed parameter used to make the
explanation depending on not more than k land-
marks, denoted as a set by Lk. Positively
(or negative) active landmarks in Lk are as-
signed an activation value a(`, s) = +1 (−1),
while a(`, s) = 0 for not active landmarks.
Given the explanation e = 〈s, C, τ〉, a land-
mark `, whose known class is C`, is called con-
sistent (or inconsistent) with e if the function



4041

δ(C`, C) · a(`, q) · τ is positive (or negative, re-
spectively), where δ(C ′, C) = 2δkron(C ′, C)− 1
and δkron is the Kronecker delta.

We can thus partition such landmarks into
the set of positively consistent landmarks
Lc,+k ⊆ L

c
k ⊆ Lk and negatively consistent ones

Lc,−k ⊆ L
c
k ⊆ Lk, with L

c,+
k ∪ L

c,−
k = L

c
k, that

aggregates all the consistent landmarks.
An explanatory model is then a function

M(e,Lck) which maps an explanation e and the set
Lck for e into a sentence f in natural language. Of
course several definitions of M(e,Lck) are possi-
ble, e.g.,

M(e,Lck) =



“s is C since it recalls me of `”
∀` ∈ Lc,+k if τ > 0

“s is not C since it does not
recall me of ` which is C”
∀` ∈ Lc,−k if τ < 0

“s is C but I don’t know why”
if Lc ≡ ∅

Here we introduce three explanatory models
used during experimental evaluation:

• Singleton Model. The first model is the
simplest as it returns a single analogy with
the consistent landmark with the high-
est positive score, if τ = 1, or lowest
negative score, when τ = −1. As an
example, the explanation of an accepted
decision in a semantic argument classifi-
cation task, described by the triple e1 =
〈’Put this plate in the center of the table’,
THEMEPLACING, 1〉, would be mapped by the
model into: I think “this plate” is THEME
of PLACING in “Robot PUT this plate in the
center of the table” since it recalls me of
“the soap” in “Can you PUT the soap in the
washing machine?”.

• Conjunctive Model. In a second model, de-
noted as Conjunctive, the system makes ref-
erence to up to k1 ≤ k analogies with pos-
itively (or negatively) active and consistent
landmarks. Given the above explanation e1,
and k1 = 2, it would return: I think “this
plate” is THEME of PLACING in “Robot PUT
this plate in the center of the table” since it
recalls me of “the soap” in “Can you PUT
“the soap” in the washing machine?” and

also of “my coat” in “HANG my coat in the
closet in the bedroom”.

• Contrastive Model. The last proposed
model is more complex since it returns both a
positive and a negative analogy by selecting,
respectively, the most positively relevant and
the most negatively relevant consistent land-
mark. Given e1, it would return: I think “this
plate” is THEME of PLACING in “Robot PUT
this plate in the center of the table” since it
recalls me of “the soap” in “Can you PUT the
soap in the washing machine” and it is not
the GOAL of PLACING since it does not re-
call me of “on the counter” in “PUT the plate
on the counter”.

In case no active and consistent landmark can
be found, the models return a phrase stating only
the predicted class, with no explanation.

5 Experimental Evaluation

Evaluating the explanatory quality of an inductive
model is still an open problem and universally rec-
ognized gold standards are not available for com-
parative analysis. In order to rely on a quantita-
tive analysis, we assume that an explanation to be
effective should assist a human user to ascertain
whether the proposed classification is correct or
not. Plausible and coherent explanations should
thus be generated from correct system’s decisions,
while bad decisions should correspond to ambigu-
ous or plainly fallacious arguments.

Hence, the evaluation of an explanatory model
should reflect the model’s adherence to three de-
sired properties: semantic transparency, i.e., ar-
gument’s linguistic grounding should be clear and
straightforward, requiring as less knowledge on
the system’s functioning and on the specific task
as possible; informativeness with respect to the
system’s decision, i.e., the explanation’s generat-
ing process should be highly dependent on how the
system processes input information; effectiveness
w.r.t an audit against the system, i.e., the expla-
nation should convey enough meaningful informa-
tion so that a human can correctly decide whether
to trust the system prediction or not.

Consequently, we define a auditing task in
which annotators are required to judge if a pro-
posed explanation would commit them to trust
the system decision. This judgment is discretized



4042

within five possible labels: Very Good if the anal-
ogy is strongly convincing and linguistically clear;
Good if the explanation is still accepted but the
pertinence is slacker; Uncertain if the annotator
gains no meaningful information from the expla-
nation or no explanation is provided at all; Bad if
some connections can be detected between the in-
put sentence and the one used as a comparison but
they are so ambiguous that the explanation is re-
jected; Incoherent if the argument appears totally
inconsistent and meaningless. Given the nature of
the argument by analogy schema (Walton et al.,
2008), it follows that annotators assigning a Very
Good or Good label to an explanation are also im-
plicitly accepting the system decision as correct,
whereas they are rejecting it as wrong in the other
cases.

Given an explanation dataset E = {(e, c, xC)}
where e is an explanation, c ∈ {1,−1} expresses
if the explanation was generated from a correct
(c = 1) or incorrect (c = −1) classification, and
xC is the numerical value corresponding to one of
the five labels categories C above2, we can define
the setAc of accepted explanations generated from
correct predictions and the set Rnc of rejected ex-
planations generated from not correct predictions
as follows:

Ac = {(e, c, l) ∈ E|c = 1 ∧ xC > 0}

Rnc = {(e, c, l) ∈ E|c = −1 ∧ xC ≤ 0}

Accordingly, the Audit Accuracy (AuAcc)

AuAcc =
|Ac|+ |Rnc|
|E|

measures the ratio between correct accep-
tance/rejection decisions and the total number of
decisions made by the human auditor.

Additionally, the Pearson Correlation between
the system classification accuracy and the human
judgment of an explanation can be interpreted as a
concrete measure of the consistency of an explana-
tory model: an ideal model should map correct
classifications to convincing explanations and in-
correct classification to implausible explanations.
It will be thus exploited to compare alternative
explanatory models. To test an explanation ap-
proach as well as of the proposed evaluation met-
rics, we will address two different semantic pro-
cessing tasks, i.e., question classification (QC) and

2We assigned values xC ∈ [1,−1]: xVery Good = 1,
xGood = 0.8, xUncertain = 0, xBad = −0.8, xIncoherent = −1

argument classification (AC) in semantic role la-
beling.

Experimental Setup. The Nyström projection
has been implemented in the KeLP framework3,
while the LRP-integrated KDA in Tensorflow,
with 1 and 2 hidden layers, respectively, whose
layer-size is equal to the number of randomly se-
lected Nyström landmarks (500 and 200, in QC
and AC respectively). For both tasks, training have
been executed in 500 epochs, using the Adam op-
timizer and adopting early-stop and dropout strat-
egy while selecting the best model according to
performances over the development set. We con-
ducted preliminary evaluations on small samples
of the dataset and set the parameter k = 5, which
defines the cardinality of the active landmarks
Lk4. The remaining hyper-parameters were tuned
via grid-search.

A group of human annotators was asked to rate
each explanation with one out of the five labels
described early in this section, basing their judg-
ment only on the perceived level of trust w.r.t. the
explanations. Each annotator was exposed to ex-
planations derived from a perfectly balanced set of
correct and incorrect classifications, so that anno-
tators are not biased by the (possibly high) quality
of the classifier when judging the explanations.

5.1 Question Classification

We replicated the experiments reported in (Croce
et al., 2017) with respect to the question classifi-
cation task, using the UIUC dataset (Li and Roth,
2006), including a training and test set of 5452
and 500 questions, respectively, organized in 6
coarse-grained classes (as ENTITY or HUMAN).
We generated Nyström representation of the Com-
positionally Smoothed Partial Tree Kernel func-
tion (Annesi et al., 2014) consistently with (Croce
et al., 2017). Using 500 landmarks, the KDA accu-
racy was 93.6%, which is comparable with state-
of-the-art neural models, as discussed in (Croce
et al., 2017). The audit manual task was indepen-
dently performed by 3 annotators5: each annotator

3http://www.kelp-ml.org, presented in (Filice et al.,
2018).

4No particular differences have emerged in the generation
of explanations when slightly different values of k where con-
sidered.

5Annotators have different levels of fluency in English
(even though no one is a native speaker) and different levels
of expertise: one has no specific technical knowledge, one is
a graduate student with advanced knowledge in NLP, while

http://www.kelp-ml.org


4043

0% 

25% 

50% 

75% 

100% 

Correct  Incorrect  Correct  Incorrect  Correct  Incorrect  

Singleton Conjunctive Contrastive 

Supporting the acceptance of a class 

0% 

25% 

50% 

75% 

100% 

Correct  Incorrect  Correct  Incorrect  Correct  Incorrect  

Singleton Conjunctive Contrastive 

Supporting the rejection of a class 

Very Good Good Uncertain Bad Incoherent 

Figure 2: Results of the audit process in the QC
task for the three explanatory models separating sys-
tem acceptance (top) from rejection (bottom). Indi-
vidual columns represent the percentage of quality la-
bel across explanations. Left columns describe correct
classifications, while right ones are derived from incor-
rect classifications.

evaluated 300 explanations (100 for each model),
reaching an inter-annotation agreement if 0.82 on
these data.

Results in Figure 2 suggest that the annotators
were able to properly discriminate correct from
incorrect decisions, just through the exposure to
the explanations: in both acceptance or rejection
cases, all models tend to assign positive labels
(Very Good and Good) to explanations of correct
decisions and negative ones (Uncertain, Bad and
Incoherent) to explanations of incorrect decisions
instead. Note that an explanation rejecting a class
should be labeled as positive, if the landmark used
for negative analogy is actually not recalling the
input sentence. The graphical intuition in Fig-
ure 2 is confirmed by the metrics: the Singleton,
Conjunctive and Contrastive models reach an Au-
dit Accuracy of 89.3%, 84.7% and 86.3%, respec-
tively. The Pearson Correlation between accep-
tance and correctness is 78.9%, 69.4% and 72.8%,
while if we measure the correlation between the
explanation quality score and the decision cor-
rectness, the Pearson coefficients become 76.1%,
71.2% and 77.2%: these are slightly lower basi-
cally for the lower reward assigned to xGood w.r.t.
xVery Good. Small numerical differences among
models emerge: it seems that the Conjunctive and
Contrastive models are not always able to retrieve
meaningful additional information, while the Sin-

the last one is an expert in the field.

gleton model is simpler and more direct. An ex-
ample of output analogies is given by I think “How
many Admirals are there in the U.S. Navy?” refers
to a NUMBER since it recalls me of both “How
many words are there in the Spanish language?”
and ”How many sides does an obelisk have?”,
generated by the Conjunctive model. Here the se-
mantic hint corresponds to the discriminative frag-
ment “How many”. However meaningful connec-
tions between the input and landmarks are also
traced against poor overlaps in syntactic and lexi-
cal information as in: I think “Where is the Mall
of the America?” refers to a LOCATION since it
recalls me of “What town was the setting for The
Music Man?”.

Table 1 reports question-explanation pairs with
similarity estimates based on the adopted CSPTK
kernel function. It is clear from the examples that
similarity alone is not able to correlate with clas-
sification decisions: questions in different classes
(e.g. first two rows in the table) may have very
high similarity scores. Second, landmarks cor-
relate with decisions in interesting ways that do
not depend on strict lexical and grammatical simi-
larity. Conceptually more grounded associations
seem to emerge: e.g., explaining “What was
J.F.K.’s wife’s name” by the analogy with “What
was Darth Vader’s son named?” is abstracted
across a conceptual relation (e.g. has name) and
the derived analogy is quite clear. Notice that ac-
tive landmarks are independent from similar ques-
tions, as landmarks triggered by similar questions
are not similar to each other.

Interestingly, explanations of ambiguous in-
stances are harmonic with human uncertainty. The
explanation I think “What is the sales tax in Min-
nesota?” refers to a NUMBER since it recalls me
of “What is the population of Mozambique?” and
does not refer to a ENTITY since it does not re-
call me of “What is a fear of slime?” is convinc-
ing, but incorrect. Here, the lack of context im-
pacts on the disambiguation of two plausible inter-
pretations that are (1) the definition of the notion
of “sales tax” (ENTITY), w.r.t (2) its current value
(NUMBER): the gold standard suggests ENTITY as
the correct category.

5.2 Argument Classification

Semantic role labeling (SRL (Palmer et al., 2010))
consists in detecting the semantic arguments asso-
ciated with the predicate of a sentence and their



4044

Class Questions (qi) k(q1, q2) Activated Landmarks (li) k(l1, l2)

LOC “What is the capital of Ethiopia?” 0.98
NUM “What is the population of Nigeria?”
ENTY“What was FDR ’s dog ’s name?” 0.97 “What is the name of David Letterman’s dog?” 0.49
HUM “What was J.F.K.’s wife ’s name?” “What was Darth Vader ’s son named?”
ENTY“What is the Ohio state bird?” 0.90 “What is the name of David Letterman ’s dog?” 0.61
ENTY“What is the pH scale?” “What is viscosity?”
ENTY“What was the first satellite to go into space?” “What was the first TV set to include a remote control?”

HUM “Who was the first American to walk in space? 0.83 “What ’s the name of the actress who starred in the 0.61movie, Silence of the Lambs?”

NUM
“What was the last year that the Chicago

0.73
“The film Jaws was made in what year?”

0.31Cubs won the World Series?”

NUM
“What is the average speed of the horses at “What is average salary of restaurant manager in
the Kentucky Derby?” United States?”

Table 1: Examples of semantically similar questions in the same or different classes, with the corresponding
landmarks activated during the classification.

0% 

25% 

50% 

75% 

100% 

Correct  Incorrect  Correct  Incorrect  Correct  Incorrect  

Singleton Conjunctive Contrastive 

Supporting the acceptance of a class 

0% 

25% 

50% 

75% 

100% 

Correct  Incorrect  Correct  Incorrect  Correct  Incorrect  

Singleton Conjunctive Contrastive 

Supporting the rejection of a class 

Very Good Good Weak Bad Incoherent 

Figure 3: Results of the audit process in the SRL task
for the three explanatory models: system acceptance
(top), rejection (bottom).

classification into their specific roles ((Fillmore,
1985)). For example, given the sentence “Bring
the fruit onto the dining table”, the task would
be to recognize the verb “bring” as evoking the
BRINGING frame, with its roles, THEME for “the
fruit” and GOAL for “onto the dining table”. Ar-
gument classification corresponds to the subtask
of assigning labels to the sentence fragments span-
ning individual roles.

As proposed in (Moschitti et al., 2008), SRL
can be modeled as a multi-classification task over
each parse tree node n, where argument spans re-
flect sub-sentences covered by the tree rooted at
n. Consistently with (Croce et al., 2011), in our
experiments the KDA has been empowered with
a Smoothed Partial Tree Kernel, operating over
Grammatical Relation Centered Trees (GRCT) de-
rived from dependency grammar. The reference
benchmark, i.e., the HuRIC dataset related to an

Interactive Robotics task (Bastianelli et al., 2016),
includes about 650 annotated transcriptions of
spoken robotic commands, organized in 18 frames
and about 60 arguments. Individual arguments ex-
tracted amount to 1, 300 examples. Experimental
setup was similar to that of Section 5.1, but due
to the limited data size we applied 10-fold cross-
validation, optimizing network hyper-parameters
via grid-search for each fold. We generated the
Nyström representations of a SPTK function with
default parameters µ = λ = 0.4 as in (Croce et al.,
2011). With these settings, the KDA accuracy was
96.1%. Due to the slightly higher complexity of
the task, w.r.t. QC, in the case the two independent
auditors had at least graduate-level knowledge in
NLP. They were requested to judge about 700
generated explanations with an inter-annotation
agreement of 0.89. For the Singleton, Conjunctive
and Contrastive model, respectively, the Audit Ac-
curacy is 91.6%, 93.4%, 88.4% while Pearson Co-
efficients between acceptance/rejection and cor-
rectness are 83.3% (80.1% for quality-correctness
correlation), 86.9% (81.9%), 77.3% (78.7%): this
suggests an higher annotator’s sensitivity to the
explanations’ plausibility, as reflected also by the
charts in Figure 3, probably due to the task itself
being more challenging for humans.

As in QC, the system can convey semantically
transparent and useful information without relying
on lexical similarity alone; e.g., consider I think
“is hot” is DESIRED STATE of INSPECTING in
“Robot CHECK whether the oven is hot?” since
it recalls me of “is empty” in “SEE if the washing
machine is empty”. In this task, the Contrastive
model could also to produce explanations exem-
plifying differences between separate roles in the
same frame, for example: I think “to me” is not



4045

GOAL of BRINGING in “Can you go to the kitchen
find a glass and BRING it to me?” since it does
not recall me of “to the bedroom” in “BRING the
phone to the bedroom” and it’s the BENEFICIARY
of BRINGING since it recalls me of “to me” in
“can you please search the book and BRING it to
me”.

6 Conclusions

This paper proposes a quantitative evaluation
of the automatic generation of epistemologically
transparent and linguistically fluid explanations
for neural inferences. The proposed approach ap-
plies LRP to a Kernel-based Deep Architecture
(KDA) that redistributes the prediction value to
training entries (i.e., annotated landmarks). The
resulting sentence exploits analogies with train-
ing instances, according to different explanatory
strategies. Given that KDAs (based on Nyström
embeddings) can be flexibly adopted in neural
learning for NLP, we show how the auditing mech-
anism outlined in the paper is epistemologically
very effective and emphasizes the neural embed-
dings with a strong impact on explainability. First,
language semantics is promoted by design and as-
sociations generated between input instances and
decisions are obtained without ever leaving the
language level. Second, different and mathemat-
ically solid models for different levels of lan-
guage semantics can be obtained by modifying
the adopted kernel formulations. In this way, a
unique general auditing mechanism is able to sup-
port fine tuning towards very different tasks, with-
out changes in the learning architecture. Finally,
as Table 1 shows, explanations are strictly depen-
dent on the induced neural model and are not just
triggered by text similarity metrics: they are epis-
temologically principled evidences about the neu-
ral learning stages, based on the observed exam-
ples and the selected landmarks.

Empirical investigations carried out against the
QC and AC tasks also confirm that the good ex-
planatory models strongly correlate with consis-
tent decisions and effectively contribute to in-
crease the user confidence in the neural inference
consistency. This make an auditing activity for
human users viable. On one side, it allows to
limit the impact of machine mistakes, in a natural
and portable manner. Moreover, it can also serve
as a novel comparative evaluation paradigm. The
reachable auditing accuracy thus measures the ex-

planatory power of different models and can be
employed as a comparative benchmark. While
the methods proposed in this paper stem just from
early explorations, the ways activated landmarks
can be made useful to meaningful explanations
stimulate further research, involving feature based
analysis such as suggested in (Ribeiro et al., 2016)
or the application of LRP to architectures more
complex than a MLP. Argumentation theory, ap-
plied to the active landmark semantics and the
source input example as captured by the kernel,
provides a very rich framework to design future
and more complex justification mechanisms.

References
Paolo Annesi, Danilo Croce, and Roberto Basili. 2014.

Semantic compositionality in tree kernels. In Pro-
ceedings of the 23rd ACM International Conference
on Conference on Information and Knowledge Man-
agement, CIKM 2014, Shanghai, China, November
3-7, 2014, pages 1029–1038.

Sebastian Bach, Alexander Binder, Gregoire Mon-
tavon, Frederick Klauschen, Klaus-Robert Müller,
and Wojciech Samek. 2015. On pixel-wise explana-
tions for non-linear classifier decisions by layer-wise
relevance propagation. PLOS ONE, 10(7).

David Baehrens, Timon Schroeter, Stefan Harmel-
ing, Motoaki Kawanabe, Katja Hansen, and Klaus-
Robert Müller. 2010. How to explain individ-
ual classification decisions. J. Mach. Learn. Res.,
11:1803–1831.

Emanuele Bastianelli, Danilo Croce, Andrea Vanzo,
Roberto Basili, and Daniele Nardi. 2016. A discrim-
inative approach to grounded spoken language un-
derstanding in interactive robotics. In Proceedings
of IJCAI 2016, New York, NY, USA, pages 2747–
2753.

Supriyo Chakraborty, Richard Tomsett,
Ramya Raghavendra, Daniel Harborne,
and Moustafa Alzantot et al. 2017. In-
terpretability of deep learning mod-
els: A survey of results. 2017 Smart-
World/SCALCOM/UIC/ATC/CBDCom/IOP/SCI,
pages 1–6.

Michael Collins and Nigel Duffy. 2001. New rank-
ing algorithms for parsing and tagging: Kernels over
discrete structures, and the voted perceptron. In Pro-
ceedings of ACL ’02, July 7-12, 2002, Philadelphia,
PA, USA, pages 263–270.

Danilo Croce, Simone Filice, Giuseppe Castellucci,
and Roberto Basili. 2017. Deep learning in semantic
kernel spaces. In Proceedings of ACL 2017, pages
345–354, Vancouver, Canada.

https://doi.org/10.1145/2661829.2661955
http://dl.acm.org/citation.cfm?id=1756006.1859912
http://dl.acm.org/citation.cfm?id=1756006.1859912
http://aclweb.org/anthology/P17-1032
http://aclweb.org/anthology/P17-1032


4046

Danilo Croce, Alessandro Moschitti, and Roberto
Basili. 2011. Structured lexical similarity via convo-
lution kernels on dependency trees. In Proceedings
of EMNLP ’11, pages 1034–1046.

Danilo Croce, Daniele Rossini, and Roberto Basili.
2018. Explaining non-linear classifier decisions
within kernel-based deep architectures. In Proceed-
ings of the Workshop: Analyzing and Interpreting
Neural Networks for NLP, BlackboxNLP@EMNLP
2018, Brussels, Belgium, November 1, 2018, pages
16–24.

Virginia Dignum. 2017. Responsible autonomy. In
Proceedings of the Twenty-Sixth International Joint
Conference on Artificial Intelligence, IJCAI-17,
pages 4698–4704.

Finale Doshi-Velez, Mason Kortz, Ryan Budish, Chris
Bavitz, Sam Gershman, David O’Brien, Stuart
Schieber, James Waldo, David Weinberger, and
Alexandra Wood. 2017. Accountability of ai un-
der the law: The role of explanation. CoRR,
abs/1711.01134.

Dumitru Erhan, Aaron Courville, and Yoshua Ben-
gio. 2010. Understanding representations learned in
deep architectures. Technical Report 1355, Univer-
sité de Montréal/DIRO.

Simone Filice, Giuseppe Castellucci, Giovanni Da San
Martino, Alessandro Moschitti, Danilo Croce, and
Roberto Basili. 2018. Kelp: a kernel-based learning
platform. Journal of Machine Learning Research,
18(191):1–5.

Charles J. Fillmore. 1985. Frames and the semantics of
understanding. Quaderni di Semantica, 6(2):222–
254.

Nicholas Frosst and Geoffrey Hinton. 2017. Distilling
a neural network into a soft decision. CEUR Work-
shop Proceedings, 2071.

Simson Garfinkel, Jeanna Matthews, Stuart S. Shapiro,
and Jonathan M. Smith. 2017. Toward algorithmic
transparency and accountability. Commun. ACM,
60(9):5–5.

S. Krening, B. Harrison, K. M. Feigh, C. L. Isbell,
M. Riedl, and A. Thomaz. 2017. Learning from ex-
planations using sentiment and advice in rl. IEEE
Transactions on Cognitive and Developmental Sys-
tems, 9(1):44–55.

Kroll, Huey, BAROCAS, W. Isaac Edward, and
R. REIDENBERG. 2016. Accountable algorithms.
University of Pennsylvania Law Review, 16.

Sanjiv Kumar, Mehryar Mohri, and Ameet Talwalkar.
2012. Sampling methods for the nyström method.
J. Mach. Learn. Res., 13:981–1006.

Xin Li and Dan Roth. 2006. Learning question clas-
sifiers: the role of semantic information. Natural
Language Engineering, 12(3):229–249.

Zachary C. Lipton. 2018. The mythos of model inter-
pretability. Queue, 16(3):30:31–30:57.

Alessandro Moschitti, Daniele Pighin, and Robert
Basili. 2008. Tree kernels for semantic role label-
ing. Computational Linguistics, 34.

M.S. Palmer, D. Gildea, and N. Xue. 2010. Seman-
tic Role Labeling. Online access: IEEE (Institute
of Electrical and Electronics Engineers) IEEE Mor-
gan & Claypool Synthesis eBooks Library. Morgan
& Claypool Publishers.

Marco Túlio Ribeiro, Sameer Singh, and Carlos
Guestrin. 2016. ”why should I trust you?”: Ex-
plaining the predictions of any classifier. CoRR,
abs/1602.04938.

John Shawe-Taylor and Nello Cristianini. 2004. Ker-
nel Methods for Pattern Analysis. Cambridge Uni-
versity Press, Cambridge, UK.

Karen Simonyan, Andrea Vedaldi, and Andrew Zisser-
man. 2013. Deep inside convolutional networks: Vi-
sualising image classification models and saliency
maps. CoRR, abs/1312.6034.

Douglas Walton, Christopher Reed, and Fabrizio
Macagno. 2008. Argumentation Schemes. Cam-
bridge University Press.

Christopher K. I. Williams and Matthias Seeger. 2001.
Using the nyström method to speed up kernel ma-
chines. In T. K. Leen, T. G. Dietterich, and V. Tresp,
editors, Advances in Neural Information Processing
Systems 13, pages 682–688. MIT Press.

Matthew D. Zeiler and Rob Fergus. 2013. Visualizing
and understanding convolutional networks. CoRR,
abs/1311.2901.

https://aclanthology.info/papers/W18-5403/w18-5403
https://aclanthology.info/papers/W18-5403/w18-5403
https://doi.org/10.24963/ijcai.2017/655
http://jmlr.org/papers/v18/16-087.html
http://jmlr.org/papers/v18/16-087.html
https://doi.org/10.1145/3125780
https://doi.org/10.1145/3125780
https://doi.org/10.1145/3236386.3241340
https://doi.org/10.1145/3236386.3241340
https://books.google.it/books?id=6C1Ag3NUqNEC
https://books.google.it/books?id=6C1Ag3NUqNEC
http://arxiv.org/abs/1602.04938
http://arxiv.org/abs/1602.04938
http://arxiv.org/abs/1312.6034
http://arxiv.org/abs/1312.6034
http://arxiv.org/abs/1312.6034
http://arxiv.org/abs/1311.2901
http://arxiv.org/abs/1311.2901

