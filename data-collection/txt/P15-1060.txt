



















































Sentiment-Aspect Extraction based on Restricted Boltzmann Machines


Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics
and the 7th International Joint Conference on Natural Language Processing, pages 616–625,

Beijing, China, July 26-31, 2015. c©2015 Association for Computational Linguistics

Sentiment-Aspect Extraction based on Restricted Boltzmann Machines

Linlin Wang1, Kang Liu2∗, Zhu Cao1, Jun Zhao2 and Gerard de Melo1
1Institute for Interdisciplinary Information Sciences, Tsinghua University, Beijing, China

2National Laboratory of Pattern Recognition, Institute of Automation,
Chinese Academy of Sciences, Beijing, China

{ll-wang13, cao-z13}@mails.tsinghua.edu.cn,
{kliu, jzhao}@nlpr.ia.ac.cn, gdm@demelo.org

Abstract

Aspect extraction and sentiment analysis
of reviews are both important tasks in
opinion mining. We propose a novel senti-
ment and aspect extraction model based on
Restricted Boltzmann Machines to jointly
address these two tasks in an unsupervised
setting. This model reflects the gener-
ation process of reviews by introducing
a heterogeneous structure into the hidden
layer and incorporating informative priors.
Experiments show that our model outper-
forms previous state-of-the-art methods.

1 Introduction

Nowadays, it is commonplace for people to ex-
press their opinion about various sorts of entities,
e.g., products or services, on the Internet, espe-
cially in the course of e-commerce activities. Ana-
lyzing online reviews not only helps customers ob-
tain useful product information, but also provide
companies with feedback to enhance their prod-
ucts or service quality. Aspect-based opinion min-
ing enables people to consider much more fine-
grained analyses of vast quantities of online re-
views, perhaps from numerous different merchant
sites. Thus, automatic identification of aspects of
entities and relevant sentiment polarities in Big
Data is a significant and urgent task (Liu, 2012;
Pang and Lee, 2008; Popescu and Etzioni, 2005).

Identifying aspect and analyzing sentiment
words from reviews has the ultimate goal of dis-
cerning people’s opinions, attitudes, emotions, etc.
towards entities such as products, services, orga-
nizations, individuals, events, etc. In this con-
text, aspect-based opinion mining, also known as
feature-based opinion mining, aims at extracting
and summarizing particular salient aspects of enti-
ties and determining relevant sentiment polarities

∗Corresponding Author: Kang Liu (kliu@nlpr.ia.ac.cn)

from reviews (Hu and Liu, 2004). Consider re-
views of computers, for example. A given com-
puter’s components (e.g., hard disk, screen) and
attributes (e.g., volume, size) are viewed as aspects
to be extracted from the reviews, while sentiment
polarity classification consists in judging whether
an opinionated review expresses an overall posi-
tive or negative opinion.

Regarding aspect identification, previous meth-
ods can be divided into three main categories:
rule-based, supervised, and topic model-based
methods. For instance, association rule-based
methods (Hu and Liu, 2004; Liu et al., 1998)
tend to focus on extracting product feature words
and opinion words but neglect connecting product
features at the aspect level. Existing rule-based
methods typically are not able to group the ex-
tracted aspect terms into categories. Supervised
(Jin et al., 2009; Choi and Cardie, 2010) and semi-
supervised learning methods (Zagibalov and Car-
roll, 2008; Mukherjee and Liu, 2012) were intro-
duced to resolve certain aspect identification prob-
lems. However, supervised training requires hand-
labeled training data and has trouble coping with
domain adaptation scenarios.

Hence, unsupervised methods are often adopted
to avoid this sort of dependency on labeled data.
Latent Dirichlet Allocation, or LDA for short,
(Blei et al., 2003) performs well in automatically
extracting aspects and grouping corresponding
representative words into categories. Thus, a num-
ber of LDA-based aspect identification approaches
have been proposed in recent years (Brody and El-
hadad, 2010; Titov and McDonald, 2008; Zhao et
al., 2010). Still, these methods have several im-
portant drawbacks. First, inaccurate approxima-
tions of the distribution over topics may reduce the
computational accuracy. Second, mixture models
are unable to exploit the co-occurrence of topics
to yield high probability predictions for words that
are sharper than the distributions predicted by in-

616



dividual topics (Hinton and Salakhutdinov, 2009).
To overcome the weaknesses of existing meth-

ods and pursue the promising direction of
jointly learning aspect and sentiment, we present
the novel Sentiment-Aspect Extraction RBM
(SERBM) model to simultaneously extract as-
pects of entities and relevant sentiment-bearing
words. This two-layer structure model is inspired
by conventional Restricted Boltzmann machines
(RBMs). In previous work, RBMs with shared
parameters (RSMs) have achieved great success
in capturing distributed semantic representations
from text (Hinton and Salakhutdinov, 2009).

Aiming to make the most of their ability to
model latent topics while also accounting for
the structured nature of aspect opinion mining,
we propose replacing the standard hidden lay-
ers of RBMs with a novel heterogeneous struc-
ture. Three different types of hidden units are
used to represent aspects, sentiments, and back-
ground words, respectively. This modification bet-
ter reflects the generative process for reviews, in
which review words are generated not only from
the aspect distribution but also affected by senti-
ment information. Furthermore, we blend back-
ground knowledge into this model using priors and
regularization to help it acquire more accurate fea-
ture representations. After m-step Contrastive Di-
vergence for parameter estimation, we can capture
the required data distribution and easily compute
the posterior distribution over latent aspects and
sentiments from reviews. In this way, aspects and
sentiments are jointly extracted from reviews, with
limited computational effort. This model is hence
a promising alternative to more complex LDA-
based models presented previously. Overall, our
main contributions are as follows:

1. Compared with previous LDA-based meth-
ods, our model avoids inaccurate approxima-
tions and captures latent aspects and senti-
ment both adequately and efficiently.

2. Our model exploits RBMs’ advantage in
properly modeling distributed semantic rep-
resentations from text, but also introduces
heterogeneous structure into the hidden layer
to reflect the generative process for online re-
views. It also uses a form of regularization to
incorporate prior knowledge into the model.
Due these modifications, our model is very
well-suited for solving aspect-based opinion
mining tasks.

3. The optimal weight matrix of this RBM
model can exactly reflect individual word
features toward aspects and sentiment, which
is hard to achieve with LDA-based models
due to the mixture model sharing mechanism.

4. Last but not the least, this RBM model is ca-
pable of jointly modeling aspect and senti-
ment information together.

2 Related Work

We summarize prior state-of-the-art models for as-
pect extraction. In their seminal work, Hu and
Liu (2004) propose the idea of applying classical
information extraction to distinguish different as-
pects in online reviews. Methods following their
approach exploit frequent noun words and depen-
dency relations to extract product features without
supervision (Zhuang et al., 2006; Liu et al., 2005;
Somasundaran and Wiebe, 2009). These methods
work well when the aspect is strongly associated
with a single noun, but obtain less satisfactory re-
sults when the aspect emerges from a combination
of low frequency items. Additionally, rule-based
methods have a common shortcoming in failing to
group extracted aspect terms into categories.

Supervised learning methods (Jin et al., 2009;
Choi and Cardie, 2010; Jakob and Gurevych,
2010; Kobayashi et al., 2007) such as Hidden
Markov Models, one-class SVMs, and Condi-
tional Random Fields have been widely used in
aspect information extraction. These supervised
approaches for aspect identification are generally
based on standard sequence labeling techniques.
The downside of supervised learning is its require-
ment of large amounts of hand-labeled training
data to provide enough information for aspect and
opinion identification.

Subsequent studies have proposed unsuper-
vised learning methods, especially LDA-based
topic modeling, to classify aspects of comments.
Specific variants include the Multi-Grain LDA
model (Titov and McDonald, 2008) to capture
local rateable aspects, the two-step approach to
detect aspect-specific opinion words (Brody and
Elhadad, 2010), the joint sentiment/topic model
(JST) by Lin and He (2009), the topic-sentiment
mixture model with domain adaption (Mei et al.,
2007), which treats sentiment as different topics,
and MaxEnt-LDA (Zhao et al., 2010), which inte-
grates a maximum entropy approach into LDA.

617



h1

v1

hF

vDviv1 vi vD

W1,1

W1,F

Wi,F

WD,F

WD,1

Wi,1

!

!

!

!
hj

v

hi

Latent Topics

W1 W2

Figure 1: RBM Schema

However, these LDA-based methods can only
adopt inaccurate approximations for the posterior
distribution over topics rather than exact inference.
Additionally, as a mixture model, LDA suffers
from the drawbacks mentioned in Section 1 that
are common to all mixture models.

3 Model

In order to improve over previous work, we first
introduce a basic RBM-based model and then de-
scribe our modified full model.

3.1 Basic RBM-based Model
Restricted Boltzmann Machines can be used for
topic modeling by relying on the structure shown
in Figure 1. As shown on the left side of the fig-
ure, this model is a two-layer neural network com-
posed of one visible layer and one hidden layer.
The visible layer consists of a softmax over dis-
crete visible units for words in the text, while the
hidden layer captures its topics. More precisely,
the visible layer is represented as a K × D ma-
trix v, where K is the dictionary size, and D is the
document length. Here, if visible unit i in v takes
the k-th value, we set vki = 1. The hidden layer
can be expressed as h ∈ {0, 1}F , where F is the
number of hidden layer nodes, corresponding to
topics. The right side of Figure 1 is another way
of viewing the network, with a single multinomial
visible unit (Hinton and Salakhutdinov, 2009).

The energy function of the model can be defined
as

E(v, h) = −
D∑
i=1

F∑
j=1

K∑
k=1

W kijhjv
k
i

−
D∑
i=1

K∑
k=1

vki b
k
i −

F∑
j=1

hjaj ,

(1)

where W kij specifies the connection weight from
the i-th visible node of value k to the j-th hidden

node, bki corresponds to a bias of v
k
i , and aj corre-

sponds to a bias of hj .
The probability of the input layer v is defined as

P (v) =
1
Z

∑
h

exp(−E(v, h)), (2)

where Z is the partition function to normalize the
probability.

The conditional probabilities from the hidden to
the visible layer and from the visible to the hidden
one are given in terms of a softmax and logistic
function, respectively, i.e.

P ( vki = 1 | h) =
exp

(
bki +

F∑
j=1

hjW
k
ij

)
K∑
q=1

exp

(
bqi +

F∑
j=1

hjW
q
ij

) ,

P ( hj = 1 | v) = σ
(
aj +

D∑
i=1

K∑
k=1

vkiW
k
ij

)
,

(3)
where σ(x) = 1/(1 + exp(−x)) is the logistic
function.

3.2 Our Sentiment-Aspect Extraction model
While the basic RBM-based method provides a
simple model of latent topics, real online reviews
require a more fine-grained model, as they con-
sist of opinion aspects and sentiment information.
Therefore, aspect identification is a different task
from regular topic modeling and the basic RBM-
based model may not perform well in aspect ex-
traction for reviews.

To make the most of the ability of the basic
RBM-based model in extracting latent topics, and
obtain an effective method that is well-suited to
solve aspect identification tasks, we present our
novel Sentiment-Aspect Extraction RBM model.

3.2.1 Generative Perspective
From a generative perspective, product reviews
can be regarded as follows. Every word in a
review text may describe a specific aspect (e.g.
“expensive” for the price aspect), or an opinion
(e.g. “amazing” for a positive sentiment and “ter-
rible” for a negative one), or some irrelevant back-
ground information (e.g. “Sunday”). In a genera-
tive model, a word may be generated from a latent
aspect variable, a sentiment variable, or a back-
ground variable. Also, there may exist certain re-
lations between such latent variables.

618



v1

h1

vD

hFhj

W1,1
W1,F

Wx,y

WD,F
WD,1

!

id1_DT  id2_NN  id3_CC  id4_NNS  id5_NN  id6_JJ id7_VBZ   id8_JJ

Sentence

 

 
 

POS

!v1 vD

hi hk!!

!

h1 hi!! hj hk hFhF

Aspect Sentiment Background

  φ2
φ4

φ1

 φ

id_i"#"word count_i
D

v1 vD! vD! φ v1

W1,F WD,1

Joint Learning with φ

Prior Knowledge

Aspect_i Sentiment_i

 φ3

Figure 2: Sentiment-Aspect Extraction Model

3.2.2 Structure
To simulate this generative process for reviews,
we adapt the standard RBM structure to reflect the
aspect-sentiment identification task.

Undirected Model. Our Sentiment-Aspect Ex-
traction model structure is illustrated in Figure 2.

Compared to standard RBMs, a crucial differ-
ence is that hidden units now have a heterogeneous
structure instead of being homogeneous as in the
standard basic RBM model. In particular, we rely
on three types of hidden units, representing aspect,
sentiment, and background, respectively. The first
two types are self-explanatory, while the back-
ground units are intended to reflect the kind of
words that do not contribute much to the aspect or
sentiment information of review documents. Since
the output of the hidden units is a re-encoding of
the information in the visible layer, we obtain a
deeper representation and a more precise expres-
sion of information in the input reviews. Thus, this
approach enables the model to learn multi-faceted
information with a simple yet expressive structure.

To formalize this, we denote v̂k =
∑D

i=1 v
k
i as

the count for the k-th word, where D is the doc-
ument length. The energy function can then be
defined as follows:

E(v, h) = −
F∑
j=1

K∑
k=1

W kj hj v̂
k

−
K∑
k=1

v̂kbk −
F∑
j=1

hjaj ,

(4)

where W kj denotes the weight between the k-th

visible unit and the j-th hidden unit.
The conditional probability from visible to hid-

den unit can be expressed as:

P (hj = 1|v) = σ(aj +
K∑
k=1

v̂kW kj ). (5)

In an RBM, every hidden unit can be activated
or restrained by visible units. Thus, every visible
unit has a potential contribution towards the acti-
vation of a given hidden unit. The probability of
whether a given visible unit affects a specific hid-
den unit is described as follows (cf. appendix for
details):

P (hj = 1 | v̂k) =P (hj = 1 | h−j , v̂k)
=σ(aj +W kj v̂

k).
(6)

Under this architecture, this equation can be ex-
plained as the conditional probability from visible
unit k to hidden unit j (softmax of words to as-
pect or sentiment). According to Eq. 6, the con-
ditional probability for the k-th word feature to-
wards the j-th aspect or sentiment p(hj = 1 | vk)
is a monotone function of W kj , the (k, j)-th entry
of the optimal weight matrix. Thus, the optimal
weight matrix of this RBM model can directly re-
flect individual word features toward aspects and
sentiment.

Informative Priors. To improve the ability of
the model to extract aspects and identify senti-
ments, we capture priors for words in reviews and
incorporate this information into the learning pro-
cess of our Sentiment-Aspect Extraction model.
We regularize our model based on these priors to
constrain the aspect modeling and improve its ac-
curacy. Figure 3 provides an example of how such
priors can be applied to a sentence, with φi repre-
senting the prior knowledge.

Research has found that most aspect words are
nouns (or noun phrases), and sentiment is often
expressed with adjectives. This additional infor-
mation has been utilized in previous work on as-
pect extraction (Hu and Liu, 2004; Benamara et
al., 2007; Pang et al., 2002). Inspired by this, we
first rely on Part of Speech (POS) Tagging to iden-
tify nouns and adjectives. For all noun words, we
first calculate their term frequency (TF) in the re-
view corpus, and then compute their inverse doc-
ument frequency (IDF) from an external Google
n-gram corpus1. Finally, we rank their TF∗IDF

1http://books.google.com/ngrams/datasets

619



The_DT  delicious_JJ  dishes_NN  in_IN the_DT restaurant_NN  taste_VBZ  great_JJ

Sentence

 

Part of Speech Tagging

  

φ2 Aspect  

  

φ3 

  

φ1

Aspect_i Sentiment_i
φ4 Sentiment

Figure 3: Prior Feature Extraction

values and assign them an aspect prior probability
pA,vk , indicating their general probability of be-
ing an aspect word. This TF-IDF approach is mo-
tivated by the following intuitions: the most fre-
quently mentioned candidates in reviews have the
highest probability of being an opinion target and
false target words are non-domain specific and fre-
quently appear in a general text corpus (Liu et al.,
2012; Liu et al., 2013). For all adjective words, if
the words are also included in the online sentiment
resource SentiWordNet2, we assign prior probabil-
ity ps,vk to suggest that these words are generally
recognized as sentiment words.

Apart from these general priors, we obtain a
small amount of fine-grained information as an-
other type of prior knowledge. This fine-grained
prior knowledge serves to indicate the probabil-
ity of a known aspect word belonging to a specific
aspect, denoted as pAj ,vk and an identified senti-
ment word bearing positive or negative sentiment,
denoted as pSj ,vk . For instance, “salad” is always
considered as a general word that belongs to the
specific aspect food, and “great” is generally con-
sidered a positive sentiment word.

To extract pAj ,vk , we apply regular LDA on the
review dataset. Since the resulting topic clusters
are unlabeled, we manually assign top k words
from the topics to the target aspects. We thus
obtain fine-grained prior probabilities to suggest
these words as belonging to specific aspects. To
obtain pSj ,vk , we rely on SentiWordNet and sum
up the probabilities of an identified sentiment
word being positive or negative sentiment-bearing,
respectively. Then we adopt the corresponding
percentage value as a fine-grained specific senti-
ment prior.

It is worthwhile to mention that the priors are
not a compulsory component. However, the pro-
cedure for obtaining priors is generic and can eas-

2http://sentiwordnet.isti.cnr.it

ily be applied to any given dataset. Furthermore,
we only obtain such fine-grained prior knowledge
for a small amount of words in review sentences
and rely on the capability of model itself to deal
with the remaining words.

3.2.3 Objective Function

We now construct an objective function for our
SERBM model that includes regularization based
on the priors defined above in Section 3.2.2. Sup-
pose that the training set is S = v1, v2, . . . , vns ,
where ns is the number of training objects. Each
element has the form vi = (vi1, vi2, . . . , viK)

D,
where i = 1, 2, . . . , ns, and these data points are
assumed to be independent and identically dis-
tributed.

We define the following novel log-likelihood
function lnLS , with four forms of regularization
corresponding to the four kinds of priors:

lnLS = ln
ns∏
i=1

P (vi)−
ns∑
i=1

[

λ1 ln
F1−1∏
j=1

∏
k∈R1

[
P (hj = 1 | v̂k)− pAj ,vk

]2

+ λ2 ln
∏
k∈R2

[ F1∑
j=1

P (hj = 1 | v̂k)− pA,vk
]2

+ λ3 ln
F2+1∏
j=F2

∏
k∈R3

[
P (hj = 1 | v̂k)− pSj ,vk

]2

+ λ4 ln
∏
k∈R4

[F2+1∑
j=F2

P (hj = 1 | v̂k)− pS,vk
]2]

(7)
Here, P (hj = 1 | v̂k) stands for the probability of
a given input word belonging to a specific hidden
unit. We assume all λi > 0 for i = 1 . . . 4, while
F1 and F2 are integers for the offsets within the
hidden layer. Units up to index F1 capture aspects,
with the last one reserved for miscellaneous Other
Aspects, while units from F2 capture the sentiment
(with F1 = F2 + 1 < F for convenience).

Our goal will be to maximize the log-likelihood
lnLS in order to adequately model the data, in ac-
cordance with the regularization.

3.2.4 Training

We use Stochastic Gradient Descent (SGD) to find
suitable parameters that maximize the objective
function. Given a single training instance v from

620



the training set S, we obtain

∂ lnL
∂θ

=
∂ lnP (v)

∂θ

− λ1
F1−1∑
j=1

∑
k∈R1

∂ ln
[
P (hj = 1 | v̂k)− pAj ,vk

]2
∂θ

− λ2
∑
k∈R2

∂ ln
[∑F1

j=1 P (hj = 1 | v̂k)− pA,vk
]2

∂θ

− λ3
F2+1∑
j=F2

∑
k∈R3

∂ ln
[
P (hj = 1 | v̂k)− pSj ,vk

]2
∂θ

− λ4
∑
k∈R4

∂ ln
[∑F2+1

j=F2
P (hj = 1 | v̂k)− pS,vk

]2
∂θ

(8)
where θ = {W,aj , bi} stands for the parameters.
Given N documents {vn}Nn=1, the first term in the
log-likelihood function with respect to W is:

1
N

N∑
n=1

∂ lnP (vn)
∂W kj

= ED1 [v̂
khj ]− ED2 [v̂khj ].

(9)
Here, D1[·] and D2[·] represent the expectation
with respect to the data distribution and the dis-
tribution obtained by this model, respectively. We
use Contrastive Divergence (CD) to approximate
ED2 [v̂

khj ] (Hinton and Salakhutdinov, 2009).
Due to the m steps of transfer between input and
hidden layers in a CD-m run of the algorithm, the
two types of hidden units, aspect and sentiment,
will jointly affect input reviews together with the
connection matrix between the two layers.

Finally, we consider the partial derivative of the
entire log-likelihood function with respect to the
parameter W . Denoting ln ∂L∂W as ∇W , in each
step we update∇W kj by adding

λ
[
P (hj = 1|v(0))v(0)k − P (hj = 1|v(cdm))v(cdm)k

]
− λ1

F1−1∑
j=1

∑
k∈R1

2Gj v̂k

(1 +Gj)2( 11+Gj − pAj ,vk)

− λ2
∑
k∈R2

2v̂k∑F1
j=1

1
(1+Gj)

− pA,vk

F1∑
j=1

Gj
(1 +Gj)2

− λ3
F2+1∑
j=F2

∑
k∈R3

2Gj v̂k

(1 +Gj)2( 11+Gj − pSj ,vk)

− λ4
∑
k∈R4

2v̂k∑F2+1
j=F2

1
(1+Gj)

− pS,vk

F2+1∑
j=F2

Gj
(1 +Gj)2

,

where Gj=e−(aj+W
k
j v̂

k) for convenience, and
v(cdm) is the result from the CD-m steps.

4 Experiments

We present a series of experiments to evaluate our
model’s performance on the aspect identification
and sentiment classification tasks.

4.1 Data
For this evaluation, we rely on a restaurant review
dataset widely adopted by previous work (Ganu
et al., 2009; Brody and Elhadad, 2010; Zhao et
al., 2010), which contains 1,644,923 tokens and
52,574 documents in total. Documents in this
dataset are annotated with one or more labels from
a gold standard label set S = {Food, Staff, Ambi-
ence, Price, Anecdote, Miscellaneous}. Following
the previous studies, we select reviews with less
than 50 sentences and remove stop words. The
Stanford POS Tagger3 is used to distinguish noun
and adjective words from each other.

We later also rely on the Polarity dataset v2.04

to conduct an additional experiment on senti-
ment classification in order to better assess the
model’s overall performance. This dataset focuses
on movie reviews and consists of 1000 positive
review documents and 1000 negative ones. It
has also been used in the experiments by Lin &
He (2009), among others.

4.2 Aspect Identification
We first apply our novel model to identify aspects
from documents in the restaurant review dataset.

4.2.1 Experimental Setup
For the experimental setup, we use ten hidden
units in our Sentiment-Aspect Extraction RBM
(SERBM), where units 0–6 capture aspects, units
7–8 capture sentiment information, and unit 9
stores background information. In particular, we
fix hidden units 0–6 to represent the target aspects
Food, Staff, Ambience, Price, Ambience, Miscella-
neous, and Other Aspects, respectively. Units 7–8
represent positive and negative sentiment, respec-
tively. The remaining hidden unit is intended to
capture irrelevant background information.

Note that the structure of our model needs no
modifications for new reviews. There are two
cases for datasets from a new domain. If the new

3http://nlp.stanford.edu/software/tagger.shtml
4http://www.cs.cornell.edu/people/pabo/

movie-review-data/

621



Method RBM RSM SERBM
PPL 49.73 39.19 21.18

Table 1: Results in terms of perplexity

dataset has a gold standard label set, then we as-
sign one hidden unit to represent each label in the
gold standard set. If not, our model only obtains
the priors pA,vk and pS,vk , and the aspect set can
be inferred as in the work of Zhao et al. (2010).

For evaluation, following previous work, the an-
notated data is fed into our unsupervised model,
without any of the corresponding labels. The
model is then evaluated in terms of how well its
prediction matches the true labels. As for hyperpa-
rameter optimization, we use the perplexity scores
as defined in Eq. 10 to find the optimal hyper-
parameters.

As a baseline, we also re-implement standard
RBMs and the RSM model (Hinton and Salakhut-
dinov, 2009) to process this same restaurant re-
view dataset and identify aspects for every doc-
ument in this dataset under the same experimental
conditions. We recall that RSM is a similar undi-
rected graphical model that models topics from
raw text.

Last but not the least, we conduct addi-
tional comparative experiments, including
with LocLDA (Brody and Elhadad, 2010),
MaxEnt-LDA (Zhao et al., 2010) and the SAS
model (Mukherjee and Liu, 2012) to extract
aspects for this restaurant review dataset under the
same experimental conditions. In the following,
we use the abbreviated name MELDA to stand for
the MaxEnt LDA method.

4.2.2 Evaluation
Brody and Elhadad (2010) and Zhao et al. (2010)
utilize three aspects to perform a quantitative eval-
uation and only use sentences with a single label
for evaluation to avoid ambiguity. The three major
aspects chosen from the gold standard labels are
S = {Food, Staff, Ambience}. The evaluation cri-
terion essentially is to judge how well the predic-
tion matches the true label, resulting in Precision,
Recall, and F1 scores. Besides these, we consider
perplexity (PPL) as another evaluation metric to
analyze the aspect identification quality. The aver-
age test perplexity PPL over words is defined as:

exp

(
− 1
N

N∑
n=1

1
Dn

logP (vn)

)
, (10)

Aspect Method Precision Recall F1
RBM 0.753 0.680 0.715
RSM 0.718 0.736 0.727

food LocLDA 0.898 0.648 0.753
MELDA 0.874 0.787 0.828

SAS 0.867 0.772 0.817
SERBM 0.891 0.854 0.872

RBM 0.436 0.567 0.493
RSM 0.430 0.310 0.360

staff LocLDA 0.804 0.585 0.677
MELDA 0.779 0.540 0.638

SAS 0.774 0.556 0.647
SERBM 0.819 0.582 0.680

RBM 0.489 0.439 0.463
RSM 0.498 0.441 0.468

ambi LocLDA 0.603 0.677 0.638
-ence MELDA 0.773 0.588 0.668

SAS 0.780 0.542 0.640
SERBM 0.805 0.592 0.682

Table 2: Aspect identification results in terms of
precision, recall, and F1 scores on the restaurant
reviews dataset

where N is the number of documents, Dn repre-
sents the word number, and vn stands for the word-
count of document n.

Average perplexity results are reported in Ta-
ble 1, while Precision, Recall, and F1 evaluation
results for aspect identification are given in Ta-
ble 2. Some LDA-based methods require manual
mappings for evaluation, which causes difficulties
in obtaining a fair PPL result, so a few methods
are only considered in Table 2.

To illustrate the differences, in Table 3, we list
representative words for aspects identified by var-
ious models and highlight words without an obvi-
ous association or words that are rather unspecific
in bold.

4.2.3 Discussion
Considering the results from Table 1 and the
RBM, RSM, and SERBM-related results from Ta-
ble 2, we find that the RSM performs better than
the regular RBM model on this aspect identifi-
cation task. However, the average test perplex-
ity is greatly reduced even further by the SERBM
method, resulting in a relative improvement by
45.96% over the RSM model. Thus, despite
the elaborate modification, our SERBM inherits
RBMs’ ability in modeling latent topics, but sig-
nificantly outperforms other RBM family models

622



Aspect RSM RBM Loc-LDA ME-LDA SAS SERBM
great menu,drink chicken chocolate food,menu salad,cheese

dessert food,pizza menu,salad dessert dessert dessert
beef chicken good cream drinks chicken

Food drink,BBQ seafood fish ice,cake chicken sauce
menu good drinks desserts cheeses rice,pizza

delicious sandwich wine,sauce good beers,salad food
good soup rice bread delicious dish
fish flavor cheese cheese rice sushi,menu

service staff service service staff,slow service
room helpful staff,waiter staff,food waitress staff,friendly
slow waiter attentive wait,waiters attentive waitress

Staff table friendly busy waiter helpful waitstaff
quick good,attentive slow,friendly place service attentive

waitress slow,service table restaurant minutes waitresses
friendly restaurant wait waitress wait,friendly servers
waiter minutes minutes waitstaff waiter minutes

atmosphere place great room place atmosphere
music atmosphere atmosphere dining decor atmosphere
place cozy wonderful tables great scene

dinner door music bar good place
Ambience romantic cute seating place romantic tables

room bar experience decor tables outside
comfortable great relaxed scene bar area

tables seating bar space decor ambiance
good experience room area great outdoor

ambiance romantic outside table music romantic,cozy

Table 3: Aspects and representative words

on the aspect identification task.

In Table 2, we also observe that SERBM
achieves a higher accuracy compared with
other state-of-the-art aspect identification meth-
ods. More specifically, it is evident that our
SERBM model outperforms previous methods’ F1
scores. Compared with MELDA, the F1 scores
for the SERBM lead to relative improvements of
5.31%, 6.58%, and 2.10%, respectively, for the
Food, Staff, and Ambience aspects. Compared
with SAS, the F1 scores yield relative improve-
ments by 6.73%, 5.10%, and 6.56%, respectively,
on those same aspects. As for Precision and Re-
call, the SERBM also achieves a competitive per-
formance compared with other methods in aspect
identification.

Finally, we conclude from Table 3 that the
SERBM method has the capability of extracting
word with obvious aspect-specific features and
makes less errors compared with other models.

4.3 Sentiment Classification

We additionally conduct two experiments to eval-
uate the model’s performance on sentiment classi-
fication.

4.3.1 Comparison with SentiWordNet
We assign a sentiment score to every document in
the restaurant review dataset based on the output
of SERBM’s sentiment-type hidden units. To ana-
lyze SERBM’s performance in sentiment classifi-
cation, we compare these results with SentiWord-
Net5, a well-known sentiment lexicon. For this
SentiWordNet baseline, we consult the resource to
obtain a sentiment label for every word and ag-
gregate these to judge the sentiment information
of an entire review document in terms of the sum
of word-specific scores. Table 4 provides a com-
parison between SERBM and SentiWordNet, with
Accuracy as the evaluation metric.

We observe in Table 4 that the sentiment
5http://sentiwordnet.isti.cnr.it

623



Method SentiWordNet SERBM
Accuracy 0.703 0.788

Table 4: Accuracy for SERBM and SentiWordNet

classification accuracy on the restaurant review
dataset sees a relative improvement by 12.1% with
SERBM over the SentiWordNet baseline.

4.3.2 Comparison with JST

We additionally utilize the Polarity dataset v2.0 to
conduct an additional sentiment classification ex-
periment in order to assess SERBM’s performance
more thoroughly. We compare SERBM with the
advanced joint sentiment/topic model (JST) by
Lin & He (2009). For the JST and the Trying-
JST methods only, we use the filtered subjectiv-
ity lexicon (subjective MR) as prior information,
containing 374 positive and 675 negative entries,
which is the same experimental setting as in Lin
& He (2009). For SERBM, we use the same gen-
eral setup as before except for the fact that aspect-
specific priors are not used here.

Table 5 provides the sentiment classification ac-
curacies on both the overall dataset and on the sub-
sets for each polarity, where pos. and neg. refer to
the positive and negative reviews in the dataset, re-
spectively.

Method overall pos. neg.
JST(%) 84.6 96.2 73

Trying-JST(%) 82 89.2 74.8
SERBM(%) 89.1 92.0 86.2

Table 5: Accuracy for SERBM and JST

In Table 5, we observe that SERBM outper-
forms JST both in terms of the overall accu-
racy and for the positive/negative-specific subsets.
SERBM yields a relative improvement in the over-
all accuracy by 5.31% over JST and by 8.66% over
Trying-JST.

5 Conclusion

In this paper, we have proposed the novel
Sentiment-Aspect Extraction RBM (SERBM)
model to jointly extract review aspects and sen-
timent polarities in an unsupervised setting. Our
approach modifies the standard RBM model by
introducing a heterogeneous structure into the hid-
den layer and incorporating informative priors into

the model. Our experimental results show that this
model can outperform LDA-based methods.

Hence, our work opens up the avenue of uti-
lizing RBM-based undirected graphical models to
solve aspect extraction and sentiment classifica-
tion tasks as well as other unsupervised tasks with
similar structure.

Appendix

The joint probability distribution is defined as

pθ(v, h) =
1
Zθ
eEθ(v,h), (11)

where Zθ is the partition function. In conjunction
with Eq. 1, we obtain

Eθ(v̂k, h) = −biv̂k −
F∑
j=1

ajhj −
F∑
j=1

hjW
k
j v̂

k

(12)

Then, we can obtain the derivation in Eq. 6.

P (hj = 1 | v̂k)
=P (hj = 1 | h−j , v̂k)

=
P (hj = 1, h−j , v̂k)

P (h−j , v̂k)

=
P (hj = 1, h−j , v̂k)

P (hj = 1, h−j , v̂k) + P (hj = 0, h−j , v̂k)

=
1
Z e
−E(hj=1,h−j ,v̂k)

1
Z e
−E(hj=1,h−j ,v̂k) + 1Z e

−E(hj=0,h−j ,v̂k)

=
e−E(hj=1,h−j ,v̂k)

e−E(hj=1,h−j ,v̂k) + e−E(hj=0,h−j ,v̂k)

=
1

1 + e−E(hj=0,h−j ,v̂k)+E(hj=1,h−j ,v̂k)

=σ(aj +W kj v̂
k)

(13)

Acknowledgments

The research at IIIS was supported by China 973
Program Grants 2011CBA00300, 2011CBA00301
and NSFC Grants 61033001, 61361136003,
61450110088. The research at CASIA was sup-
ported by the National Basic Research Program
of China Grant No. 2012CB316300 and NSFC
Grants 61272332 and 61202329.

624



References
Farah Benamara, Carmine Cesarano, Antonio Pi-

cariello, Diego Reforgiato Recupero, and Venkatra-
mana Subrahmanian. 2007. Sentiment analysis:
Adjectives and adverbs are better than adjectives
alone. In Proceedings of ICWSM 2007.

David Blei, Andrew Ng, and Michael Jordan. 2003.
Latent dirichlet allocation. Journal of Machine
Learning Research, 3:993–1022.

Samuel Brody and Noemie Elhadad. 2010. An unsu-
pervised aspect-sentiment model for online reviews.
In Proceedings of NAACL-HLT 2010, pages 804–
812. Association for Computational Linguistics.

Yejin Choi and Claire Cardie. 2010. Hierarchical se-
quential learning for extracting opinions and their
attributes. In Proceedings of ACL 2010, pages 269–
274. Association for Computational Linguistics.

Gayatree Ganu, Noemie Elhadad, and Amélie Marian.
2009. Beyond the stars: Improving rating predic-
tions using review text content. In Proceedings of
WebDB 2009, pages 1–6.

Geoffrey Hinton and Ruslan Salakhutdinov. 2009.
Replicated softmax: an undirected topic model. In
Advances in Neural Information Processing Systems
(NIPS 2009), pages 1607–1614.

Minqing Hu and Bing Liu. 2004. Mining and sum-
marizing customer reviews. In Proceedings of KDD
2004, pages 168–177, New York, NY, USA. ACM.

Niklas Jakob and Iryna Gurevych. 2010. Extracting
opinion targets in a single-and cross-domain setting
with Conditional Random Fields. In Proceedings
of EMNLP 2010, pages 1035–1045. Association for
Computational Linguistics.

Wei Jin, Hung Hay Ho, and Rohini K Srihari. 2009. A
novel lexicalized HMM-based learning framework
for Web opinion mining. In Proceedings of ICML
2009, pages 465–472.

Nozomi Kobayashi, Kentaro Inui, and Yuji Matsumoto.
2007. Extracting aspect-evaluation and aspect-of
relations in opinion mining. In Proceedings of
EMNLP-CoNLL, pages 1065–1074.

Chenghua Lin and Yulan He. 2009. Joint senti-
ment/topic model for sentiment analysis. In Pro-
ceedings of the 18th ACM Conference on Infor-
mation and Knowledge Management (CIKM 2009),
pages 375–384. ACM.

Bing Liu, Wynne Hsu, and Yiming Ma. 1998. In-
tegrating classification and association rule mining.
In Proceedings of KDD 1998, pages 80–86. AAAI
Press.

Bing Liu, Minqing Hu, and Junsheng Cheng. 2005.
Opinion observer: analyzing and comparing opin-
ions on the Web. In Proceedings of the 14th inter-
national conference on World Wide Web, pages 342–
351. ACM.

Kang Liu, Liheng Xu, and Jun Zhao. 2012. Opin-
ion target extraction using word-based translation
model. In Proceedings of EMNLP-CoNLL 2012,
pages 1346–1356.

Kang Liu, Liheng Xu, Yang Liu, and Jun Zhao. 2013.
Opinion target extraction using partially-supervised
word alignment model. In Proceedings of the 23rd
International Joint Conference on Artificial Intelli-
gence (IJCAI 2013), pages 2134–2140. AAAI Press.

Bing Liu. 2012. Sentiment analysis and opinion min-
ing. Synthesis Lectures on Human Language Tech-
nologies, 5(1):1–167.

Qiaozhu Mei, Xu Ling, Matthew Wondra, Hang Su,
and ChengXiang Zhai. 2007. Topic sentiment mix-
ture: modeling facets and opinions in weblogs. In
Proceedings of the 16th international conference on
the World Wide Web (WWW 2007), pages 171–180.
ACM.

Arjun Mukherjee and Bing Liu. 2012. Aspect extrac-
tion through semi-supervised modeling. In Proceed-
ings of ACL 2012, pages 339–348.

Bo Pang and Lillian Lee. 2008. Opinion mining and
sentiment analysis. Foundations and trends in infor-
mation retrieval, 2(1-2):1–135.

Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan.
2002. Thumbs up?: Sentiment classification us-
ing machine learning techniques. In Proceedings of
EMNLP 2002, pages 79–86. Association for Com-
putational Linguistics.

Ana-Maria Popescu and Orena Etzioni. 2005. Extract-
ing product features and opinions from reviews. In
Proceedings of HLT/EMNLP 2005. Springer.

Swapna Somasundaran and Janyce Wiebe. 2009. Rec-
ognizing stances in online debates. In Proceedings
of ACL-IJCNLP 2009, pages 226–234. Association
for Computational Linguistics.

Ivan Titov and Ryan McDonald. 2008. Modeling
online reviews with multi-grain topic models. In
Proceedings of the 17th international conference on
the World Wide Web (WWW 2008), pages 111–120.
ACM.

Taras Zagibalov and John Carroll. 2008. Automatic
seed word selection for unsupervised sentiment clas-
sification of Chinese text. In Proceedings of COL-
ING 2008, pages 1073–1080.

Wayne Xin Zhao, Jing Jiang, Hongfei Yan, and Xiaom-
ing Li. 2010. Jointly modeling aspects and opin-
ions with a MaxEnt-LDA hybrid. In Proceedings of
EMNLP 2010, pages 56–65. Association for Com-
putational Linguistics.

Li Zhuang, Feng Jing, and Xiao-Yan Zhu. 2006.
Movie review mining and summarization. In Pro-
ceedings of the 15th ACM international Conference
on Information and Knowledge Management (CIKM
2006), pages 43–50. ACM.

625


