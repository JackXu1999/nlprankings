








































Entity Linking on Microblogs with Spatial and Temporal Signals

Yuan Fang ∗ †

University of Illinois at Urbana-Champaign
201 N. Goodwin Avenue
Urbana, IL 61801, USA

fang2@illinois.edu

Ming-Wei Chang

Microsoft Research
1 Microsoft Way

Redmond, WA 98052, USA

minchang@microsoft.com

Abstract

Microblogs present an excellent opportunity
for monitoring and analyzing world happen-
ings. Given that words are often ambiguous,
entity linking becomes a crucial step towards
understanding microblogs. In this paper, we
re-examine the problem of entity linking on
microblogs. We first observe that spatiotem-
poral (i.e., spatial and temporal) signals play
a key role, but they are not utilized in exist-
ing approaches. Thus, we propose a novel en-
tity linking framework that incorporates spa-
tiotemporal signals through a weakly super-
vised process. Using entity annotations1 on
real-world data, our experiments show that the
spatiotemporal model improves F1 by more
than 10 points over existing systems. Finally,
we present a qualitative study to visualize the
effectiveness of our approach.

1 Introduction

Microblogging services provide an immense plat-
form for intelligence gathering, such as market re-
search (Asur and Huberman, 2010), disaster mon-
itoring (Sadilek et al., 2012) and political analy-
sis (Tumasjan et al., 2010). Extracting entities from
microblogs is an essential step for many such ap-
plications. Suppose that a marketing firm is inter-
ested in the sentiment about some product on Twit-
ter. However, any sentiment analysis is potentially

∗Work done during an internship at Microsoft Research.
†Also affiliated with Agency for Science, Technology and

Research, 1 Fusionopolis Way, Singapore 138632.
1Can be downloaded at http://research.microsoft.com/en-

us/downloads/84ac9d88-c353-4059-97a4-87d129db0464/.

misleading if we cannot correctly retrieve the tweets
mentioning the target product.

To retrieve tweets for a given entity (e.g., a prod-
uct or an organization), a straightforward approach
is to formulate a keyword query. However, simple
keyword matching is largely ineffective, since key-
words are often ambiguous. For instance, “spurs”
can refer to two distinct sports teams (San Antonio
Spurs, which is a basketball team in the US, and Tot-
tenham Hotspur F.C., which is a soccer team in the
UK), besides being a non-entity verb or noun. Thus,
retrieving tweets via keyword queries inevitably
mixes different entities.

Given the ambiguity of keywords, in this paper,
we study the task of entity linking (Bunescu and
Pasca, 2006) on microblogs. As input, we are given
a short message (e.g., a tweet) and an entity database
(e.g., Wikipedia where each article is an entity). As
output, we map every surface form (e.g., “spurs”)
in the message to an entity (e.g., San Antonio Spurs)
or to ∅ (i.e., a non-entity). This task is particularly
challenging for microblogs due to their short, noisy
and colloquial nature. Fortunately, we also observe
two new opportunities, which are often missing in
traditional data.

First, microblogs usually embed rich meta-data,
most notably spatiotemporal (i.e., spatial and tem-
poral) signals. Specifically, all tweets are associ-
ated with a timestamp, and many can be mapped
to a location. We observe that entity priors often
change across time and location. Consider the ex-
ample tweets in Fig. 1. To understand the meaning
of the word “spurs,” it is challenging to only rely on
the textual features. However, with the help of lo-

259

Transactions of the Association for Computational Linguistics, 2 (2014) 259–272. Action Editor: Noah Smith.
Submitted 11/2013; Revised 3/2014; Published 10/2014. c©2014 Association for Computational Linguistics.



Tweets mentioning Tottenham Hotspur F.C., a soccer team in the UK
UK: Who scored the 2 goals for spurs I had to go out or 5 mins and missed it all
UK: Defoe again 3 nil to spurs how you doing @USER at Arsenal

Tweets mentioning San Antonio Spurs, a basketball team in the US
US: @USER who cares, nobody wanna see the spurs play. Remember they’re boring. He’s a great coach . . .
US: The fine on the spurs for 250k is ridiculous! I’m a laker fan; still think this is too much

Figure 1: Examples illustrating the importance of spatiotemporal signals to entity linking. The “UK” or “US”
tag indicates the location of the posting user. Based on deep semantic understanding of lexical items and additional
resources (such as the entire conversation and attached URL), the annotators can label “spurs” in the first two tweets as
the soccer team, and in the other two as the basketball team. While most entity linking systems handle merely textual
features, the task obviously becomes easier with location information.

cation information, the two teams can be easily dis-
tinguished. In particular, based on our labeled data,
San Antonio Spurs accounts for 91% of the “spurs”
tweets in the US, whereas it only accounts for 8%
in the UK. Similar trends also exist across different
time periods. Therefore, exploiting spatiotemporal
signals is crucial to entity linking.

Second, we can leverage the predictions from the
tweets carrying an easier surface form to help link
the tweets carrying an ambiguous surface form at a
similar time or location. The intuition is that cer-
tain surface forms are easier to disambiguate than
others. For instance, while only saying “spurs” in
a tweet can be quite ambiguous, many other tweets
around the same time or location might carry an eas-
ier surface form, such as “SA spurs” or “san antonio
spurs,” which are not ambiguous.

In this work, we focus on the offline mining set-
ting, where a corpus has been collected for analy-
sis offline. Developing algorithms for the streaming
setting is an important direction of future work. Our
contributions are summarized as follows:

• We propose a spatiotemporal framework for en-
tity linking on microblogs. To our best knowl-
edge, this is the first work to model spatiotempo-
ral signals for entity linking.
• We demonstrate the effectiveness of our frame-

work through extensive quantitative experiments.
In particular, we improve F1 by more than 10
points over the existing state of the art.
• We point out that entity linking should be evalu-

ated for both information extraction and retrieval
needs. In the former, we evaluate the extraction of
entities from tweets, while in the latter we evalu-
ate the retrieval of tweets for a query entity. A
qualitative study is also presented for the latter.

2 Related Work

Earlier research on entity linking (Bunescu and
Pasca, 2006; Cucerzan, 2007; Milne and Witten,
2008) has been focused on well-written documents
such as news and encyclopedia articles. The TAC
KBP track (Ji et al., 2010; Ji and Grishman, 2011)
also includes an entity linking task with a slightly
different setting—to link a given mention based on a
background document. These efforts exploit the sta-
tistical power aggregated by a semantic knowledge
bases, most notably Wikipedia. Various features in
the target document are also leveraged (Han et al.,
2011; Kulkarni et al., 2009; Hoffart et al., 2011;
Shen et al., 2012) to assess both local compatibility
and global coherence.

These techniques have also been adapted and tai-
lored to short texts including tweets, for the prob-
lem of entity linking (Ferragina and Scaiella, 2010;
Meij et al., 2012; Guo et al., 2013) as well as the
related problem of named entity recognition (NER)
(Ritter et al., 2011; Li et al., 2012a). However,
given the short and noisy nature of microblogs, these
approaches, which largely depend on textual fea-
tures, often result in unsatisfactory performance.
Fortunately, additional non-textual meta-data in mi-
croblogs can often help. A recent study (Shen et
al., 2013) improves entity linking by utilizing user
account information, based on the intuition that all
tweets posted by the same user share an underlying
topic distribution.

Inspired by the use of non-textual features, we
explore the spatiotemporal signals associated with
tweets. Although the spatiotemporal aspect of so-
cial media has been studied in many papers, includ-
ing temporal cycle tracking (Leskovec et al., 2009),
spatial object matching (Dalvi et al., 2012), min-

260



ing emerging topics (Mei et al., 2006; Cataldi et al.,
2010; Yin et al., 2011), event monitoring (Sakaki et
al., 2010; Xu et al., 2012), and identifying geograph-
ical linguistic variations (Eisenstein et al., 2010;
Wing and Baldridge, 2011), none of them addresses
the problem of entity linking. In this paper, we pro-
pose a novel spatiotemporal framework for entity
linking, building upon some of the previously ob-
served patterns in social media.

While we believe that entity linking is the first
step towards intelligence gathering, many existing
studies filter or cluster tweets based on merely key-
words. On the one hand, manual selection of key-
words (Sakaki et al., 2010; Tumasjan et al., 2010)
requires significant labor, and thus is not scalable
to the vast number of entities. On the other hand,
automatic approaches (Li et al., 2013) only identify
coarse-grained topics (e.g., crime or sports), falling
short of recognizing specific entities.

Lastly, there is a line of research on record ex-
traction from social media (Benson et al., 2011; Rit-
ter et al., 2012). Although the problem is different
from entity linking, they present an interesting in-
sight into social media. They observe that the same
record is often referenced by multiple messages, and
exploit this redundancy to help with extraction. The
redundant nature of social media can be potentially
leveraged to improve entity linking as well.

3 Spatiotemporal Entity Linking

Our spatiotemporal framework for entity linking re-
quires an input tweet m, as well as its associated
timestamp t and location l. For each tweet m, the
goal is to predict an output set {e1, e2, . . .} of enti-
ties that are mentioned in m.

3.1 Background

To build an entity linking system, we need both a
database and a lexicon.

A database is a set of entities that a tweet can link
to. Following earlier work (Sil and Yates, 2013), our
database consists of the intersection of Freebase and
Wikipedia. We then select the entities belonging to
the core types (Guo et al., 2013)2 based on the Free-

2Their core types include person, organization (org), loca-
tion (loc), book, tvshow and movie. We deal with two additional
types, namely, event and product.

base type information. In total, there are 2.7 million
entities in our database.

A lexicon is a dictionary that maps a candidate an-
chor a (i.e., a surface form) to its possible entity set
E(a). Similar to existing studies (Cucerzan, 2007;
Guo et al., 2013), our lexicon comprises information
from disambiguation pages, redirect pages and an-
chor texts in Wikipedia3. To handle the case where
a is not an entity, we add a special symbol ∅ to every
E(a). We have 6 million anchors in total.

Given a tweet, the system generates a set of can-
didate anchors based on the lexicon. Specifically,
each tweet is tokenized into individual words, keep-
ing each punctuation as a separate token. To iden-
tify if any sequence of tokens matches an anchor in
the lexicon, we use exact matching, but allow for
case-insensitivity. Furthermore, we employ an NER
system trained using structural perceptron (Collins,
2002) to filter the anchors of type person, loc or org,
such that only the anchors recognized by the NER
are retained.

3.2 Incorporating Spatiotemporal Signals
Consider a tweet m with timestamp t and location l.
Given anchor a in tweetm, our system links a to the
candidate entity e∗ as follows:

e∗ = arg maxe∈E(a) P (e|m, a, t, l)
= arg maxe∈E(a) P (e,m, a, t, l) (1)

We further adopt the following conditional inde-
pendence assumption in our model.

ASSUMPTION: Given an entity e, how e is expressed
(m, a), and when or where e is published (t, l), are
conditionally independent. In other words, we have
P (m, a|t, l, e) = P (m, a|e).

Intuitively, the expression (m, a) of a given en-
tity e is stable across most times and locations (t, l),
which could be attributed to the imitation nature
(Leskovec et al., 2009) of social media. That is, as
stories of e propagate on the social media, users over
different t and l often imitate each other (disregard-
ing cross-lingual scenarios). While there could be
a “burn-in” period for recent events of e, the distri-
bution of m and a would eventually “stabilize” over
most t and l.

3We use a snapshot of Wikipedia taken on 04/03/2013.

261



Our Contribution

fix 
re-estimate { }

Spatiotemporal Learning (Sec 5.1)

fix { }
re-estimate

Sec 5.2
binning ,
and MLE

graph-based
smoothing

(optionally)+

initialize

Non-spatiotemporal (Sec 4)
Example: (Guo et al., 2013)

Existing Work

Example: estimate using 
Wikipedia pageviews

Popularity of Entity

Our Contribution

( | , ) ,
Spatiotemporal Framework (Sec 3)argmax “weakly”

supervised

based on

Figure 2: Overall framework of spatiotemporal entity linking.

Subsequently, we decompose the model below,
to enable the reuse of existing non-spatiotemporal
models for P (e|m, a), as we shall see later.

P (e,m, a, t, l)

= P (m, a|t, l, e)P (t, l, e)
= P (m, a|e)P (t, l, e)
= P (e|m, a)P (m, a)P (e|t, l)P (t, l)/P (e) (2)

Note that in (2), we choose to rewrite P (t, l, e) as
P (e|t, l)P (t, l) instead of P (t, l|e)P (e). The choice
is due to computational issues. In the latter, we need
to estimate one distribution for every e; in the for-
mer, we need to estimate one distribution for every
t, l. There are 2.7 million entities in our database,
but with appropriate quantization the number of t, l
“bins” are only on the order of thousands.

Thus, the prediction model (1) is equivalent to

e∗ = arg maxe∈E(a) P (e|m, a)P (e|t, l)/P (e). (3)

Intuitively, if e at some t, l is more popular than
usual, i.e., P (e|t, l) > P (e), we should promote e
for a tweet at t, l; otherwise we should demote e.

The overall framework is summarized in Fig. 2,
which boils down to the three factors in (3), namely,
P (e|m, a), P (e|t, l) and P (e). In Sect. 4, we will
discuss briefly how P (e|m, a) is modeled based on
previous work. In Sect. 5, we will estimate spa-
tiotemporal signals in the form of P (e|t, l), which
is jointly optimized with entity assignments. Lastly,
we discuss P (e) here.
P (e) measures the popularity of e, and is esti-

mated by making it proportional to the Wikipedia
pageviews of e. While this is a fair proxy for a gen-
eral social media corpus, we acknowledge that more
advanced method is required for a specialized cor-
pus (e.g., for users of a sub-community). Note that
P (e) is not estimated based on the joint optimization
technique for P (e|t, l) in Sect. 5. The reason is that

our tweets used in the experiments only cover a one-
month period, which does not necessarily reflect the
general popularity of the entities.

4 End-to-End Entity Linking

The goal of this section is to describe our base sys-
tem that does not consider spatiotemporal signals,
i.e., to model P (e|m, a). Specifically, we adopt an
end-to-end entity linking system (E2E), which is de-
signed to jointly detect mentions and disambiguate
entities. E2E is a supervised method largely based
on a previous study (Guo et al., 2013).

For efficiency, we only adopt the first order
model. Therefore, the prediction function can be de-
composed for each anchor a independently,

e∗ = arg maxe∈E(a)w
TΦ(m, a, e). (4)

where w is a linear model trained using structural
SVM, and Φ is a feature function over message m,
anchor a, and candidate output e.

We use all the basic features and the cohesiveness
score feature (Guo et al., 2013)4. Additional features
are also included. First, for each mention and candi-
date entity pair, we add a feature to capture the num-
ber of highly correlated candidates carried by other
mentions in the same tweet with respect to the cur-
rent candidate. Second, we include a binary feature
that will be active if the type of the mention (when
recognized by our NER system) and the type of the
candidate entity (according to the Freebase type in-
formation) agree with each other.

Probability conversion. It is crucial to have a
well calibrated probability distribution for the pre-
dictions. In order to convert the output of the struc-
tural SVM model, we adapt an existing approach

4Described in Table 4 and Sect. 4.3 of the reference.

262



(Platt, 2000) to our case. We define

P (e|m, a)= exp(b1 + b2w
TΦ(m, a, e))∑

e′∈E(a) exp(b1+b2w
TΦ(m, a, e′))

,

where b1 and b2 are the calibration parameters that
will be tuned using labeled data.

Given a labeled development set, letG(e|m, a) =
1 if and only if anchor a in tweet m is labeled to
link to entity e, and let G(∅|m, a) = 1 if and only
if a in m is not labeled to link to any entity. Note
that

∑
e∈E(a)G(e|m, a) = 1. Thus,G represents the

ground-truth distribution, and we want P (e|m, a) to
be as close toG(e|m, a) as possible. To this end, we
optimize b1 and b2 by minimizing the cross entropy
between G and P :

minb1,b2 −
∑

m,aG(e|m, a) logP (e|m, a).

Alternative base system. We also consider Link-
Probability (LP) as an additional base system. As
pointed out earlier (Guo et al., 2013), mention detec-
tion is an important step for end-to-end entity link-
ing, and its design is crucial to the ultimate perfor-
mance. Hence, to detect candidate anchors, LP uses
the same design of database and lexicon discussed
in this paper and elsewhere (Cucerzan, 2007; Guo et
al., 2013), which is believed to be effective. Given a
potential anchor a in message m, P (e|m, a) is sim-
ply modeled as P (e|a), which can be estimated from
Wikipedia anchor statistics. In fact, anchor statistics
constitute one of the most useful features in more
sophisticated systems (Shen et al., 2012; Guo et al.,
2013). Given its robust mention detection mecha-
nism and the utility of anchor statistics, the simple
LP turns out surprisingly well.

5 Estimating Spatiotemporal Signals

There are two critical challenges for successfully es-
timating the spatiotemporal signals in the form of
P (e|t, l). First, it is impractical to collect sufficient
labeled data to directly estimate it. Second, we need
to properly handle the continuous space of the spa-
tiotemporal signals.

In the following, we detail the overall model for
learning spatiotemporal signals in a weakly super-
vised fashion (Sect. 5.1), and then discuss two ways
of handling continuous signals (Sect. 5.2).

5.1 Spatiotemporal Learning Model
We model the spatiotemporal signals by a genera-
tive model P (e|t, l) ∼ Multi(θtl), where θtl is the
parameter for the multinomial distribution over all
entities at t, l. Since there is no ground truth of the
entity assignment, our model will jointly optimize
the entity assignment and θtl. Based on (3), we use
the following objective function Ω({e}, θtl):
∑

m,a(logP (e|m, a) + logP (e|θtl)− logP (e)),

where m is an unlabeled message at time t and loca-
tion l, a is an anchor in m, and {e} is a set of entity
assignments for the set of m.

We use a block-coordinate ascent method to find
the best θtl and {e} iteratively. For each iteration,
the following two steps will be executed.

• Fix θtl. Find the entity assignments {e} that max-
imizes Ω. Note that if θtl is fixed, the most likely
assignment can be found using

arg maxe(logP (e|m, a)+logP (e|θtl)− logP (e)).

In fact, this equation is the same as (3), where
P (e|m, a) can be estimated by a supervised base
system (e.g., E2E or LP, see Sect. 4).

• Fix {e}. Re-estimate θtl by maximizing Ω. Once
the assignment of entities has been generated by
the previous step, we can re-estimate it by maxi-
mizing the objective function with

arg maxθtl
∑

m,a logP (e|θtl).

In other words, we are looking for the maximum
likelihood estimate (MLE) of θtl, given the (previ-
ously inferred) entity assignments {e}. Since θtl
is multinomial, its MLE can be computed as the
relative frequency of each e, that is,

θtl(e) =
# tweets containing e at t, l∑
e′ # tweets containing e

′ at t, l
.

Of course, t, l are continuous, so direct counting
is infeasible. Instead, we resort to discrete bins,
which will be treated in Sect. 5.2.

This process will be executed repeatedly, and can
be considered as a variant of the Hard EM algorithm.
In practice, we run it for up to five iterations, as Hard
EM often converges fast.

263



We call this learning process a “weakly super-
vised” model. For some time t and location l, the ob-
jective function Ω({e}, θtl) sums over all messages
at t, l. These messages themselves are unlabeled,
but some supervision is provided by the base sys-
tem indirectly. In other words, even though we do
not know the ground truth entity assignments in the
messages, we can leverage the predictions from the
base system to update the entity assignments.

5.2 Handling Continuous Time and Location
Given the continuous space of time t and location
l, we propose two methods to estimate θtl. While
it could be cast as an instance of the well-studied
density estimation problem (Vapnik and Mukherjee,
1999; Scott, 2009), we resort to a simple binning
method, which has also been applied to other spa-
tiotemporal contexts (Wing and Baldridge, 2011; Xu
et al., 2012). The binning approach can already
demonstrate the significance of spatiotemporal sig-
nals in entity linking.

Binning. Our first approach segments the continu-
ous space into discrete bins. Time is divided into a
set of equal intervals ∆T (e.g., every one hour), and
location is divided into a set of equal squares ∆L
(e.g., each 100×100 sqkm area5). Let ∆ = ∆T×∆L
denote the set of bins over time and location. We
further index the bins by I = {1, . . . , n} where
n = |∆|, and refer to each bin δi through its index
i ∈ I. Correspondingly, we denote the multinomial
parameter at bin δi by θi, which can be estimated us-
ing maximum likelihood (i.e., relative frequency of
entities in δi).

It can be shown that with sufficiently small bins,
θtl can be approximated accurately by θi when t, l ∈
δi. In practice, if the bins are too small, there are
often inadequate tweets in a bin, and thus θi cannot
be well estimated. On the contrary, if the bins are
too large, θi cannot reliably model θtl. In this paper,
we tune the bin size using development data.

Graph-based smoothing. Our algorithm contains
two steps: first, we estimate θ̂i for each bin δi as
in the above binning approach; next, we smooth the
initial estimate to obtain the final estimate θi using
graph-based regularization.

5The location grid is actually defined in longitude and lati-
tude. Here we show the equivalent distance on Earth’s surface.

As a common insight in graph-based learning
(Zhu et al., 2003; Fang et al., 2012; Fang et al.,
2014), if δi and δj are close to each other, θi and θj
should be similar. Moreover, θi should not deviate
too much from the initial estimate θ̂i. The intuition
can be captured by the following optimization:

min
(θ1,...,θn)

1
2(1− �)

∑
i,j∈IWij‖θi − θj‖2

+ �
∑

i∈I Dii‖θi − θ̂i‖2, (5)

where � ∈ (0, 1) is a regularization parameter, W is
an affinity matrix such thatWij measures the “close-
ness” of δi and δj , and D is a diagonal matrix such
that Dii =

∑
j∈IWij . In particular, we design the

affinity matrix as follows:

Wij =

{
(d0 + d(δi, δj))

γ i 6= j
0 i = j,

(6)

where d0 > 0 and γ < 0 are parameters, and d(·) is
a symmetric distance function for bins. Given that
γ < 0, Wij follows a polynomial decay when δi
and δj become farther apart, as previously suggested
(Dalvi et al., 2012).

It can be shown that the optimization problem (5)
is equivalent to finding θi such that ∀i ∈ I,

θi = (1− �)
∑

j∈I
Wij
Dii

θj + �θ̂i. (7)

Interestingly, we can consider (7) as a generaliza-
tion of previous observations on social media. It is
proposed that the temporal model on social media
needs to account for two factors: imitation and re-
cency (Leskovec et al., 2009). For imitation, users
often imitate one another, so that a past story can
be picked up and propagated by other users by writ-
ing new articles about the same story. For recency,
more recent stories are more likely to be imitated.
We generalize their idea to both temporal and spatial
signals, and extend it to entity linking. Specifically,
(7) can be interpreted as a result of imitation: tweets
mentioning entity e in δi are imitated from those in
δj . In addition, a closer δj has a higher chance to be
imitated, which captures recency.

To solve (7), let Q = [θ1, . . . , θn]T and Q̂ =
[θ̂1, . . . , θ̂n]

T , treating θi and θ̂i as column vectors.
Through the following iterative updates (Fang et al.,
2013), Q(t) converges to Q as t→∞, starting from

264



an arbitrary Q(0):

Q(t+1) = (1− �)(D−1W )Q(t) + �Q̂. (8)
The time complexity is O(tnk), where k is the

number of neighboring bins on the graph. Such cost
is reasonable, given that the update generally con-
verges for t < 50, and k is a constant if we only
consider k nearest neighbors on the graph.

Joint vs. separate modeling. Finally, while model-
ing time and location jointly is more expressive than
modeling each signal separately, the joint approach
also creates much more bins (and hence more multi-
nomial parameters), making data scarcity a severe
issue. In particular, jointly we have |∆T | × |∆L|
bins, but separately we only have |∆T |+ |∆L| bins.
By assuming the conditional independence of t and
l given e, we can rewrite (3) to model time and loca-
tion separately:

e∗ = arg max
e∈E(a)

P (e|m, a)P (e|t)P (e|l)/P (e)2 (9)

We refer to the joint model (3) as “T×L”, and the
separate model as “T+L” (9). For “T+L”, graph-
based smoothing can be carried out separately for
the time bins and the location bins. The two models
will be compared in our experiments.

6 Experiments

In this section, we assess the effectiveness of our ap-
proach through quantitative experiments.

6.1 Settings

Dataset. Our experiments are conducted on Twit-
ter data. We collect all English tweets from verified
accounts in December 2012, excluding retweets.6

These tweets amount to 7 million. As we assume an
offline analysis scenario instead of an online stream-
ing scenario, these tweets are fetched and stored lo-
cally in advance.

We further select tweets with both spatial and
temporal information. For time, we take the post-
ing time of each tweet. Locations are harder to ob-
tain, given that fewer than 5% of our tweets are geo-
tagged. For tweets without geo-coordinates, we use

6The identities of verified accounts are validated by Twit-
ter, and thus their tweets contain little spam. Moreover, we ig-
nore retweets here as their spatiotemporal behavior might sig-
nificantly differ from that of the original tweets.

the location in the user profile and map it to coor-
dinates based on a lookup table containing major
cities in the US7. Such mapping exists for about 25%
of the tweets. For the remaining tweets, their user
profiles are either uninformative (e.g., “home”) or
report a non-US location. In the end, 1.8 million
tweets remain in our dataset. Note that some stud-
ies (Dalvi et al., 2012; Li et al., 2012b) enable the
inference of missing locations based on user gener-
ated content or user network. We do not apply these
methods, which are beyond our focus.

Lastly, we collect the Wikipedia pageviews in the
year 2012 in order to estimate P (e) in (3).

Development set. We randomly sample 250 tweets
as the development set and label their core entities
(see Sect. 3.1). There are two human annotators.
Each annotator labels half of the tweets, which are
then counter-checked by the other to reach an agree-
ment. To label a tweet, the annotators are given all
available information to understand its content, e.g.,
the attached URL and the conversation.

On the other hand, the base system E2E is trained
on an independent set of 1000 tweets randomly sam-
pled from the year 2010.

Algorithms. For the binning approach, we tune the
bin size on the development set, ranging from 10
minutes to 1 day for time, and from 10×10 sqkm to
1500 × 1500 sqkm for location. Although the opti-
mal bin size may be entity specific, we use the same
size optimized over all entities for scalability con-
siderations. That said, if we are only interested in a
small set of target entities, it is possible to optimize
the bin size for each entity, in order to attain better
performance. Unless otherwise stated, in the rest of
the paper, binning is the default method to estimate
spatiotemporal signals.

For graph-based smoothing, we use the finest bin
for both time and location, and tune its parame-
ters � ∈ {.01, .05, .1, .5}, d0 = {1, 10} and γ ∈
{-.5, -1, -2, -4, -8} on the development set. To con-
struct the graph, we also need to define the dis-
tance function d(·) in (6). For time bins, we use the

7The lookup table is compiled from geonames.org. We only
consider city and state names in the US, discarding all oth-
ers. As city names may be ambiguous, we apply conservative
matching using city-state pair. We also account for popular ab-
breviations (e.g., IL for Illinois and NYC for New York City).

265



elapsed time between the middle timestamps of two
bins. For location bins, we use the distance between
the geographical centers of two bins on Earth’s sur-
face. For joint time-location bins, we take the aver-
age of time (min) and location (km) distances.

Finally, in both of the above, we use the inferred
entity assignments {e} to estimate θtl (see Sect 5.1).
Since the inferred assignments can be quite noisy,
we only count confident ones such that their inferred
probability is at least 0.5.

6.2 Evaluation Methodology

We adopt two different evaluation policies, which
are driven by information extraction (IE) and re-
trieval (IR) needs, respectively.

IE-driven evaluation. The IE-driven evaluation is
similar to the standard evaluation for an end-to-end
entity linking system, which evaluates the entities
“extracted” by linking. We randomly sample 250
tweets as the test set, which are labeled in the same
manner as the development set. For evaluation, we
compute the F1 score over the test set, as defined
earlier (Guo et al., 2013).

According to the labels, 40% of the tweets in the
test set contain at least one entity. A total of 179
entity instances are identified, or an average of 0.72
per tweet. These entities belong to different types,
including person (24%), org (36%), loc (16%), event
(9%), and others (15%).

IR-driven evaluation. One key application of en-
tity linking is to enable intelligence gathering for a
query entity, where the first step is to “retrieve” the
tweets mentioning the query entity. As listed in Ta-
ble 1, we sample ten query entities of different types,
where each entity is known to be influenced by spa-
tiotemporal signals, and has one or more ambiguous
anchors. For instance, Hillary Rodham Clinton may be
mentioned by “clinton” only, which could be mis-
takenly linked to Bill Clinton.

Type Query entity
person Hillary Rodham Clinton? Catherine, Duchess of Cambridge?

org San Antonia Spurs? Big Bang (South Korean band)?

loc Washington (state)? Newtown, Connecticut
event Hanukkah? Winter solstice
movie Les Misérables (2012 film) Django unchained (2012 film)

Table 1: Query entities for IR-driven evaluation.

For each query entity, we randomly sample 100
tweets as the test set (totaling 1000 tweets), such that
each tweet contains an ambiguous anchor of the en-
tity (through a lookup from our lexicon). Each tweet
is labeled to indicate whether it mentions the query
entity or not. About 37% of the sampled tweets
did not actually mention the query entity since the
anchors are ambiguous. For evaluation, we test if
the system can correctly identify the presence or ab-
sence of the query entity in every tweet. In particu-
lar, we compute the F1 score over the test set.

Note that this task is harder than it seems. Besides
ambiguous anchors, many entities are not dominant
for their anchor. For instance, for the anchor “big
bang”, Big Bang (South Korean band) is less popular
than The Big Bang Theory (a TV show). In fact, six of
the entities are not dominant, marked by ? in Table 1.
Thus, merely linking to the most popular one (i.e.,
the base system LP) is not a good strategy.

Significance test. To establish the statistical signifi-
cance of our results, we randomly divide the test set
into 10 splits of equal number of tweets, and com-
pute the F1 score on each split for each algorithm.
Two-tail paired t-test is then applied to determine if
the F1 scores of two algorithms over the 10 splits are
significantly different.

6.3 Results and Discussion

We present the empirical findings for the following
research questions.

Q1: How do our base systems perform?
Q2: Are spatiotemporal signals indeed useful?
Q3: Does the graph-based smoothing help?
Q4: What causes the errors? How to recover them?

Base system comparison (Q1). To show that our
base systems, in particular E2E, already outperform
other systems, we compare with Wikiminer (Milne
and Witten, 2008) and Illinois (Ratinov et al., 2011)
systems.8 As existing systems are more geared for
the IE scenario, we report in Table 2 the IE-drive F1
on the test set.

8We use the authors’ implementations. AIDA (Hoffart et
al., 2011) is not compared, as it mostly links to person, org and
loc only. TAGME (Ferragina and Scaiella, 2010) and Cucerzan
(Cucerzan, 2007) were already compared in an earlier paper
(Guo et al., 2013) which E2E is largely based on. To be fair,
we discard non-core entities linked by Wikiminer or Illinois.

266



E2E LP
IE IR IE IR

base 57.0 – 58.4 – 48.3 – 48.5 –
+T 64.9 *** 71.4 *** 52.4 * 59.7 ***
+L 65.0 ** 76.1 *** 50.3 * 61.8 ***
+T+L 68.6 *** 79.0 *** 49.0 53.3 ***
+T×L 66.2 *** 74.1 *** 50.6 61.2 ***

(a) F1 scores
E2E +T +L +T+L

60

70

80

90
85.5 86.9

82.4
85.6

Pr
ec

is
io

n

(b) Precision
E2E +T +L +T+L

30

40

50

60

42.8

51.8
53.6

57.2

R
ec

al
l

(c) Recall

Figure 3: Effect of using spatiotemporal signals. (a) F1 scores. ***, **, *: Significantly different from the base
system at .01, .05, .1 levels. (b, c) IE-driven evaluation of precision and recall, using E2E as the base system.

The results clearly show that E2E performs better
than other base systems. Also note that LP obtains
similar or better F1 than Illinois or Wikiminer, al-
though its precision is lower.

Precision Recall F1 Significance
Wikiminer 78.9 24.7 37.6 ***
Illinois 77.3 34.9 48.1 **
LP 49.7 47.0 48.3 **
E2E 85.5 42.8 57.0 –

Table 2: IE-driven comparison of base systems. ***, **,
*: Significantly different from E2E at .01, .05, .1 levels.

Spatiotemporal signals (Q2). To showcase our key
insight that spatiotemporal signals are crucial for en-
tity linking, we compare the base systems with their
time or location-aware counterparts.

As reported in Fig. 3a, using either temporal (+T)
or spatial (+L) signal can indeed improve entity link-
ing over the base systems. In particular, the im-
provements in IR-driven evaluation are more signif-
icant since the chosen entities are known to be influ-
enced by time or location.

Next, we combine both temporal and spatial sig-
nals. As Sect. 5 discussed, while the joint model
(+T×L) is ideal in theory, it is not necessarily better
than the separate model (+T+L) due to data scarcity,
as shown in the last two rows of Fig. 3a. In the fol-
lowing, we will use “+T+L” as the default setting.
Also note that using both time and location on LP
may be worse than using either alone, given that LP
has low precision and thus employing both signals
may aggregate more errors.

Let us also examine the precision and recall in
Fig. 3b and 3c. Interestingly, while precision is not
compromised, recall is greatly increased with spa-
tiotemporal signals. The reason is that E2E has high
precision, but misses a lot of mentions (i.e., linking

them to ∅). However, if an entity e is “trending”
at some time t or location l, that is, e is mentioned
by more tweets than usual at t or l, we shall expect
P (e|t, l) > P (e). Thus, the system is more likely
to link to e instead of ∅ based on our spatiotempo-
ral model (3), resulting in higher recall. A more in-
depth error analysis will be presented in Q4.

Graph-based smoothing (Q3). Next, we evalu-
ate the utility of smoothing in Fig. 4. The model
called “Fine” discretizes time and location using the
finest bins. The one called “Fine+Smoothing” is
our graph-based smoothing algorithm applied to the
“Fine” model. We also compare with “Optimal”,
which discretizes time and location using the opti-
mal bins tuned on the development set.

As we can see, smoothing is crucial when a
small bin size is used. Compared to “Optimal”, our
smoothing is better on LP but not on E2E. However,
smoothing is still useful, as “Optimal” depends a lot
on choosing the right bin size, while smoothing is
less sensitive to its parameters.

IE-driven IR-driven
50

60

70

80

90

57.0
58.5

68.6

79.0

59.6

67.6
66.0

76.6

F1
sc

or
e

Base
Optimal

Fine
Fine+Smoothing

(a) Base system: E2E

IE-driven IR-driven
45

50

55

60

65

48.3 48.6
49.2

53.4

49.2

52.9

49.7

59.8

F1
sc

or
e

Base
Optimal

Fine
Fine+Smoothing

(b) Base system: LP

Figure 4: F1 scores by different ways of estimating spa-
tiotemporal signals in the +T+L setting.

Error analysis (Q4). Let us investigate what causes
the errors. In Table 3, we first break down the errors
into false positives and false negatives. Each type is
further categorized as “mention” (the mention itself

267



is wrong) or “linking” (the mention is correct but
linking is wrong). Note that if the mention is wrong,
the linking would also be wrong.

False Positives False Negatives
mention linking total mention linking total

E2E 7 5 12 90 5 95
+T+L 9 7 16 64 7 71

Table 3: Sources of error in IE-driven evaluation.

On the one hand, both systems make few false
positives, suggesting high precision (see Fig. 3b).
On the other hand, while the base system (E2E)
makes many false negatives, the spatiotemporal
model (+T+L) substantially reduces these errors,
resulting in a significant increase in recall (see
Fig. 3c). +T+L is particularly effective in recovering
false negatives due to incorrect mention, i.e., linking
to ∅ when it should not, which account for the vast
majority (84%) of all the errors in E2E.

Next, we examine how these errors can be recov-
ered in +T+L. We zoom into entity types of person,
org, loc and event, which collectively cover 85% of
the entities in the test set. Fig. 5a reveals that +T+L
is consistently helpful to different types of entities.
In particular, +T+L improves the event entities the
most, since events are generally more correlated to
time and location, which are useful signals for re-
covering the errors.

We illustrate in Fig. 5b some entities that are
missed by E2E, but recovered by useful spatial or
temporal signals. Taking tweet #2 for illustration,
E2E is not confident linking to the entity Califor-
nia State University, Fullerton based on textual content
alone, and thus incorrectly links to ∅. This is a false
negative due to incorrect mention, the most frequent
kind in E2E (see Table 3). Fortunately, +T+L is con-
fident linking to the entity correctly, given that 1)
the tweet was posted during a campus emergency,
when many other tweets were also discussing this
entity; and 2) the posting user was in California,
where the users of many other tweets discussing this
entity were also located.

7 Qualitative Study: Entity vs. Keyword

The goal of this section is twofold. First, we provide
a qualitative analysis to visualize the performance
of our entity linking system. The results show that

person org loc event
0

20

40

60

80

100

66.7
60.9 59.3

11.1

78.1
73.1

68.0
58.3

F1
sc

or
e

E2E +T+L

(a) F1 scores

Tweet Example entity Posting time User location
#1 person: Colin Kaepernick during his game (not useful)
#2 org: Cali. State U. Fullerton in campus emergency California
#3 loc: Los Angeles (not useful) California
#4 event: New Year’s Eve on 31 December (not useful)

(b) Example entities recovered by spatiotemporal signals

Figure 5: IE-driven evaluation by entity type.

our system can consistently recover the real-world
happenings or physical locations of the query enti-
ties. Second, we show that entity linking is more ef-
fective than keyword matching in the retrieving sce-
nario (see Sect. 6.2).

Approach. We can retrieve a tweet by entity link-
ing or keywords. For entity linking, we classify all
the tweets using E2E+T+L, and a tweet is positive
(i.e., it will be retrieved) if its predictions contain the
query entity. For keyword matching, a tweet is posi-
tive if it contains the given keywords which describe
the query entity.

To visualize the retrieved tweets, we discretize
time into 24-hour bins, and location into 100 × 100
sqkm bins. In a bin δi, define the intensity of an en-
tity e as #(e, δi)/#(δi), where #(e, δi) is the number
of positive tweets for e in δi, and #(δi) is the total
number of tweets in δi. To visualize the intensity of
multiple entities simultaneously, we further compute
the normalized intensity of e, which normalizes e’s
intensity by its maximum over all the bins.

Case study 1: “washington.” We retrieve tweets
with keyword “washington”, a common anchor for
three entities: Washington (state), Wasghinton D.C. and
Washington Redskins. We also retrieve tweets for
each of them with entity linking.

We first examine the intensity of the entities over
December 2012. In Fig. 6a, the intensity based on
keyword “washington” inevitably represents a mix-
ture of different entities—it is unclear which “wash-
ington” entity corresponds to each intensity peak.
Fortunately, entity linking separates the three enti-

268



0

0.2

0.4

0.6

0.8

1

1 6 11 16 21 26 31

N
or

m
al

iz
ed

 in
te

ns
ity

Day

(a) with keywords (“washington”) (b) with entity linking

0

0.2

0.4

0.6

0.8

1

1 6 11 16 21 26 31

N
or

m
al

iz
ed

 in
te

ns
ity

Day

Washington, D.C.
Washington Redskins
Washington (state)

[1.1]

[3]

[3]
[3] [3]

[3][2]

[1.2]

0

0.2

0.4

0.6

0.8

1

1 6 11 16 21 26 31

N
or

m
al

iz
ed

 in
te

ns
ity

Day

keywords ("wa")
keywords ("washington state")

(c) with other keywords for Washington (state)

Figure 6: Intensity of “washington” over December 2012. In particular, entity linking (b) reveals the correspondence
to several real-world happenings: legalization of marijuana in Washington (state) [1.1] and Obama’s response [1.2], fiscal
cliff and weather alert in Washington D.C. [2], games of Washington Redskins [3].

ties in Fig. 6b, where major intensity peaks corre-
spond to real-world happenings for each entity. We
also use alternative keywords specifically targeted
for Washington (state), namely, “washington state”
and “wa”. While they are more precise than “wash-
ington” for the given entity, they have lower recall,
resulting in different intensity profiles in Fig. 6c.

Lastly, we examine their intensity over the US.
Using keywords, Fig. 6a does not isolate different
entities. In contrast, using entity linking, Fig. 6b
reveals more information about the three entities.
For instance, tweets for Washington (state) are mostly
concentrated in that state, and those for Washington
Redskins are mostly in their home location but also
have occurrences all over the US.

(a) with keywords (“washington”)

Washington Redskins
Washington, D.C.
Washington (state)

(b) with entity linking

Figure 7: Intensity of “washington” over the US. The size
of dots indicates the normalized intensity.

Case study 2: “spurs.” We retrieve tweets with
keyword “spurs”, a popular anchor for both San An-
tonio Spurs and Tottenham Hotspur F.C. We also re-
trieve tweets for each of them with entity linking.

As illustrated in Fig. 8, the intensity based on
keyword “spurs” roughly matches that of San Anto-
nio Spurs based on entity linking, but significantly
differs from Tottenham Hotspur F.C. In other words,
“spurs” only accounts for the former entity, even
though it is also a standard anchor for the latter. The
reason is that an overwhelming majority of the men-
tions of “spurs” in the US refer to San Antonio Spurs.

That means, while keywords may work for the domi-
nating entity, they are particularly weak for the other
entity. In contrast, with entity linking, both entities
can be identified, where the major peaks correspond
to the game days of each team.

0

0.2

0.4

0.6

0.8

1

1 6 11 16 21 26 31

N
or

m
al

iz
ed

 in
te

ns
ity

Day

entity  linking     (San Antonio Spurs)
entity  linking     (Tottenham Hotspur F.C.)

keywords ("spurs")
entity linking
entity linking

Figure 8: Intensity of “spurs” over December 2012.

8 Conclusion

As microblogging and other social media services
become increasingly popular, the number of short
and noisy messages are growing at an unprecedented
rate. We demonstrate, for the first time, that spa-
tiotemporal signals are critical in advancing entity
linking. After all, it might not be a mere text-based
language processing problem.

There are some important future directions for
this work. First, updating the spatiotemporal model
on the fly is a useful extension to cater to the online
streaming setting. Second, we foresee a more gen-
eral framework to integrate various meta-data such
as authorship, surrounding conversation, attached
URL and hashtags, in addition to the spatiotempo-
ral signals. Finally, it would also pay off to analyze
the interactions between the meta-data and the use of
language. We believe that exploring the meta-data
for entity linking on microblogs will be an interest-
ing and active line of research.

269



References

S. Asur and B.A. Huberman. 2010. Predicting the future
with social media. In Web Intelligence, pages 492–
499.

E. Benson, A. Haghighi, and R. Barzilay. 2011. Event
discovery in social media feeds. In Proceedings of the
Annual Meeting of the Association for Computational
Linguistics (ACL), pages 389–398.

R. C Bunescu and M. Pasca. 2006. Using encyclopedic
knowledge for named entity disambiguation. In Pro-
ceedings of the European Chapter of the ACL (EACL),
pages 9–16.

M. Cataldi, L. Di Caro, and C. Schifanella. 2010.
Emerging topic detection on twitter based on tempo-
ral and social terms evaluation. In Proceedings of the
International Workshop on Multimedia Data Mining
(MDMKDD), pages 4:1–10.

M. Collins. 2002. Discriminative training methods for
hidden Markov models: Theory and experiments with
perceptron algorithms. In Proceedings of the Confer-
ence on Empirical Methods for Natural Language Pro-
cessing (EMNLP).

S. Cucerzan. 2007. Large-scale named entity disam-
biguation based on Wikipedia data. In Proceedings
of the Joint Conference on Empirical Methods in Nat-
ural Language Processing and Computational Natu-
ral Language Learning (EMNLP-CoNLL), pages 708–
716.

N. Dalvi, R. Kumar, and B. Pang. 2012. Object matching
in tweets with spatial models. In Proceedings of In-
ternational Conference on Web Search and Web Data
Mining (WSDM), pages 43–52.

J. Eisenstein, B. O’Connor, N.A. Smith, and E.P. Xing.
2010. A latent variable model for geographic lexi-
cal variation. In Proceedings of the Conference on
Empirical Methods for Natural Language Processing
(EMNLP), pages 1277–1287.

Y. Fang, B.-J. Hsu, and K. C.-C. Chang. 2012.
Confidence-aware graph regularization with heteroge-
neous pairwise features. In Proceedings of Interna-
tional Conference on Research and Development in In-
formation Retrieval (SIGIR), pages 951–960.

Y. Fang, K. C.-C. Chang, and H.W. Lauw. 2013.
Roundtriprank: Graph-based proximity with impor-
tance and specificity. In Proceedings of International
Conference on Data Engineering (ICDE), pages 613–
624.

Y. Fang, K. C.-C. Chang, and H.W. Lauw. 2014. Graph-
based semi-supervised learning: Realizing pointwise
smoothness probabilistically. In Proceedings of the In-
ternational Conference on Machine Learning (ICML).

P. Ferragina and U. Scaiella. 2010. TAGME: on-the-fly
annotation of short text fragments (by Wikipedia en-
tities). In Proceedings of ACM Conference on Infor-
mation and Knowledge Management (CIKM), pages
1625–1628.

S. Guo, M.-W. Chang, and E. Kıcıman. 2013. To link
or not to link? A study on end-to-end tweet entity
linking. In Proceedings of the Annual Meeting of the
North American Association of Computational Lin-
guistics (NAACL), pages 1020–1030.

X. Han, L. Sun, and J. Zhao. 2011. Collective entity
linking in web text: a graph-based method. In Pro-
ceedings of International Conference on Research and
Development in Information Retrieval (SIGIR), pages
765–774.

J. Hoffart, M. Amir Yosef, I. Bordino, H. Fürstenau,
M. Pinkal, M. Spaniol, B. Taneva, S. Thater, and
G. Weikum. 2011. Robust disambiguation of named
entities in text. In Proceedings of the Conference on
Empirical Methods for Natural Language Processing
(EMNLP), pages 782–792.

H. Ji and R. Grishman. 2011. Knowledge base popula-
tion: Successful approaches and challenges. In Pro-
ceedings of the Annual Meeting of the Association for
Computational Linguistics (ACL), pages 1148–1158.

H. Ji, R. Grishman, H.T. Dang, K. Griffitt, and J. Ellis.
2010. Overview of the TAC 2010 knowledge base
population track. In Text Analysis Conference (TAC).

S. Kulkarni, A. Singh, G. Ramakrishnan, and
S. Chakrabarti. 2009. Collective annotation of
Wikipedia entities in web text. In Proceedings of
International Conference on Knowledge Discovery
and Data Mining (KDD), pages 457–466.

J. Leskovec, L. Backstrom, and J. Kleinberg. 2009.
Meme-tracking and the dynamics of the news cycle.
In Proceedings of International Conference on Knowl-
edge Discovery and Data Mining (KDD), pages 497–
506.

C. Li, J. Weng, Q. He, Y Yao, A. Datta, A. Sun, and B.-S.
Lee. 2012a. TwiNER: named entity recognition in tar-
geted twitter stream. In Proceedings of International
Conference on Research and Development in Informa-
tion Retrieval (SIGIR), pages 721–730.

R. Li, S. Wang, H. Deng, R. Wang, and K. Chen-Chuan
Chang. 2012b. Towards social user profiling: unified
and discriminative influence model for inferring home
locations. In Proceedings of International Conference
on Knowledge Discovery and Data Mining (KDD),
pages 1023–1031.

R. Li, S. Wang, and K. Chen-Chuan Chang. 2013. To-
wards social data platform: Automatic topic-focused
monitor for twitter stream. Proceedings of the VLDB
Endowment (PVLDB), 6(14).

270



Q. Mei, C. Liu, H. Su, and C.-X. Zhai. 2006. A proba-
bilistic approach to spatiotemporal theme pattern min-
ing on weblogs. In Proceedings of the International
World Wide Web Conference (WWW), pages 533–542.

E. Meij, W. Weerkamp, and M. de Rijke. 2012. Adding
semantics to microblog posts. In Proceedings of In-
ternational Conference on Web Search and Web Data
Mining (WSDM), pages 563–572.

D. Milne and I. H. Witten. 2008. Learning to link
with Wikipedia. In Proceedings of ACM Conference
on Information and Knowledge Management (CIKM),
pages 509–518.

J. Platt. 2000. Probabilistic outputs for support vec-
tor machines and comparison to regularize likelihood
methods. In Advances in Large Margin Classifiers,
pages 61–74.

L. Ratinov, D. Roth, D. Downey, and M. Anderson.
2011. Local and global algorithms for disambiguation
to Wikipedia. In Proceedings of the Annual Meeting of
the Association for Computational Linguistics (ACL),
pages 1375–1384.

A. Ritter, S. Clark, Mausam, and O. Etzioni. 2011.
Named entity recognition in tweets: an experimental
study. In Proceedings of the Conference on Empirical
Methods for Natural Language Processing (EMNLP),
pages 1524–1534.

A. Ritter, Mausam, O. Etzioni, and S. Clark. 2012. Open
domain event extraction from Twitter. In Proceedings
of International Conference on Knowledge Discovery
and Data Mining (KDD), pages 1104–1112.

A. Sadilek, H. Kautz, and V. Silenzio. 2012. Modeling
spread of disease from social interactions. In Proceed-
ings of the International Conference on Weblogs and
Social Media (ICWSM).

T. Sakaki, M. Okazaki, and Y. Matsuo. 2010. Earth-
quake shakes Twitter users: real-time event detection
by social sensors. In Proceedings of the International
World Wide Web Conference (WWW), pages 851–860.

D.W. Scott. 2009. Multivariate density estimation: the-
ory, practice, and visualization. John Wiley & Sons.

W. Shen, J. Wang, P. Luo, and M. Wang. 2012. LIN-
DEN: linking named entities with knowledge base via
semantic knowledge. In Proceedings of the Inter-
national World Wide Web Conference (WWW), pages
449–458.

W. Shen, J. Wang, P. Luo, and M. Wang. 2013. Link-
ing named entities in tweets with knowledge base via
user interest modeling. In Proceedings of Interna-
tional Conference on Knowledge Discovery and Data
Mining (KDD), pages 68–76.

A. Sil and A. Yates. 2013. Re-ranking for joint named-
entity recognition and linking. In Proceedings of ACM
Conference on Information and Knowledge Manage-
ment (CIKM).

A. Tumasjan, T. O. Sprenger, P. G Sandner, and I. M
Welpe. 2010. Predicting elections with Twitter: What
140 characters reveal about political sentiment. In
Proceedings of the International Conference on We-
blogs and Social Media (ICWSM), pages 178–185.

V. Vapnik and S. Mukherjee. 1999. Support vector
method for multivariate density estimation. In Pro-
ceedings of the Conference on Advances in Neural In-
formation Processing Systems (NIPS), pages 659–665.

B.P. Wing and J. Baldridge. 2011. Simple supervised
document geolocation with geodesic grids. In Pro-
ceedings of the Annual Meeting of the Association for
Computational Linguistics (ACL), pages 955–964.

J.-M. Xu, A. Bhargava, R. Nowak, and X. Zhu. 2012.
Socioscope: Spatio-temporal signal recovery from so-
cial media. In Proceedings of the European Confer-
ence on Machine Learning (ECML), pages 644–659.

Z. Yin, L. Cao, J. Han, C. Zhai, and T. Huang. 2011.
Geographical topic discovery and comparison. In Pro-
ceedings of the International World Wide Web Confer-
ence (WWW), pages 247–256.

X. Zhu, Z. Ghahramani, J. Lafferty, et al. 2003. Semi-
supervised learning using Gaussian fields and har-
monic functions. In Proceedings of the International
Conference on Machine Learning (ICML), pages 912–
919.

271



272


