


































Towards Compositional Tree Kernels

Paolo Annesi, Danilo Croce, Roberto Basili
Department of Enterprise Engineering

University of Roma, Tor Vergata
00133 Roma, Italy

{annesi,croce,basili}@info.uniroma2.it

Abstract

Distributional Compositional Semantics
(DCS) methods combine lexical vectors
according to algebraic operators or func-
tions to model the meaning of complex lin-
guistic phrases. On the other hand, several
textual inference tasks rely on supervised
kernel-based learning, whereas Tree Ker-
nels (TK) have been shown suitable to the
modeling of syntactic and semantic simi-
larity between linguistic instances. While
the modeling of DCS for complex phrases
is still an open research issue, TKs do not
account for compositionality. In this pa-
per, a novel kernel called Compositionally
Smoothed Partial Tree Kernel is proposed
integrating DCS operators into the TK es-
timation. Empirical results over Seman-
tic Text Similarity and Question Classifi-
cation tasks show the contribution of se-
mantic compositions with respect to tradi-
tional TKs.

1 Introduction

Since the introduction of Landauer and Dumais
in (Landauer and Dumais, 1997) and Schutze in
(Schütze, 1998), Distributional Semantic Models
(DMSs) have been an active area of research in
computational linguistics and a promising tech-
nique for solving the lexical acquisition bottle-
neck by unsupervised learning. However, it is very
difficult to reconcile these techniques with exist-
ing theories of meaning in language, which re-
volve around logical and ontological representa-
tions. According to logical theories (Kamp and
Reyle, 1993; Blackburn and Bos, 2005), sentences
should be translated to a logical form that can
be interpreted as a description of the state of the
world. On the contrary, vector-based techniques
are closer to the philosophy of “meaning as con-

text”, relying on the Wittgenstein’s (1953) “mean-
ing just is use” and Firth’s “you shall know a word
by the company it keeps” and the distributional hy-
pothesis of Harris (1968), that words will occur
in similar contexts if and only if they have simi-
lar meanings. In these years attention has been
focused on the question of how to combine word
representations in order to characterize a model
for sentence semantics. Since these models are
typically directed at the representation of isolated
words, a well formed theory on how to combine
vectors and to represent complex phrases still rep-
resents a research topic. Distributional Composi-
tional Semantic (DCS) models capture bi-gram se-
mantics, but they are not sensitive to the syntactic
structure yet. On the other hand, Convolution Ker-
nels (Haussler, 1999) are well-known similarity
functions among such complex structures. In par-
ticular, Tree Kernels (TKs) introduced in (Collins
and Duffy, 2001), are largely used in NLP for their
ability in capturing text grammatical information,
directly from syntactic parse trees.

In this paper, we investigate the combination
of DCS and Convolution Kernels. We extend
a kernel function recently proposed in (Croce et
al., 2011), called Smoothed Partial Tree Kernel
(SPTK), that enriches the similarity between tree
structures with a function of node similarity. As
words are leaves in constituency syntactic trees,
the lexical semantic similarity can be easily eval-
uated in term of similarity between their vector
counterparts. In our DCS perspective, this lexi-
cal semantic information will be distributed across
all the parse tree, as a carrier of the lexical com-
position, e.g. head/modifier relations, already ex-
plicit in dependency formalisms. The idea here
is to propagate lexical semantic information over
the entire parse tree, by building a Composition-
ally enriched Constituency Tree (CCT). By mak-
ing non-terminal nodes dependent on both syntac-
tic (e.g. the VP grammatical category) and lexi-

15



cal semantic information, it is possible to formu-
late a new kernel function based on this tree rep-
resentation, that takes into account for each node
a distributional compositional metrics. Thus, the
idea is to i) use the SPTK formulation in order
to exploit the lexical information of the leaves,
ii) define a procedure to mark nodes of a con-
stituency parse tree that allow to spread lexical
bigrams across the non-terminal nodes, iii) apply
smoothing metrics sensible to the compositional-
ity between the non-terminal labels. The resulting
model has been called Compositionally Smoothed
Partial Tree Kernel (CSPTK).

In Section 2, a summary of approaches for DCS
and TKs is presented. The entire process of mark-
ing parse trees is described in Section 3. Therefore
in Section 4 the CSPTK similarity function is pre-
sented. Finally, in Section 5, the CSPTK model
is investigated in Semantic Text Similarity (STS)
and Question Classification tasks.

2 Related Work

Distributional Compositional Semantics.
Vector-based models typically represent isolated
words and ignore grammatical structure (Turney
and Pantel, 2010). They have thus a limited
capability to model compositional operations
over phrases and sentences. In order to overcome
these limitations, Distributional Compositional
Semantics (DCS) models have been investigated.
In (Smolensky, 1990) compositionality of two
vector !u and !v is accounted by the tensor product
!u ⊗ !v, while in (Foltz et al., 1998) lexical vectors
are summed, keeping the resulting vector with the
same dimension of the input ones. In (Mitchell
and Lapata, 2008) two general classes of composi-
tional models have been defined: a linear additive
model !p = A!u + B!v and a multiplicative model
!p = C!u!v. A and B are weight matrices and C
is a weight tensor that project lexical vectors !u
and !v onto the space of !p, i.e. the vector resulting
from the composition.

These models usually assume that composition
is a symmetric function of the constituents. While
this might be reasonable for certain structures,
such as lists, a model of composition based on syn-
tactic structure requires some way of differentiat-
ing the contributions of each constituent. In (Erk
and Pado, 2008), the concept of a structured vec-
tor space is introduced, where each word is asso-
ciated to a set of vectors corresponding to differ-

ent syntactic dependencies. Noun component of a
composition between verb and noun is here given
by an average of verbs that the noun is typically
object of. In (Guevara, 2010) a regressor is trained
for adjective-noun (AN) compositionality: pairs of
adjective-noun vector concatenations are used as
input in training data, whilst corpus-derived AN
vectors as output. A similar approach was previ-
ously undertaken by (Zanzotto et al., 2010).

A specific linear model of semantic composi-
tion based on the idea of space projection is pro-
posed in (Annesi et al., 2012) for simple grammat-
ical structures, i.e. syntactically typed bi-grams.
Given a phrase such as “buy car” they project the
source vectors !buy and !car, into a so-called Sup-
port Subspace, that is a subset of the original fea-
ture space. Space Projection depends on both the
two involved lexicals and selects only their ”com-
mon” features: these concurrently constraint the
suitable lexical interpretation local to the phrase.

Given two phases p1 and p2, semantic similar-
ity can be computed by first projecting the two
pairs in the suitable Support Subspace and then
applying the traditional cosine metrics. Projec-
tion is expressed by a (filter) diagonal matrix M
that projects each word into a subset of the origi-
nal features. Different projections are discussed in
(Annesi et al., 2012) aimed at identifying suitable
semantic aspects of the underlying head/modifier
relationships. The compositional similarity judg-
ment between phrases p1 = (u, v) and p2 =
(u

′
, v

′
) over the support subspace of p1 is thus ex-

pressed as:

Φ(◦)(p1, p2) = (M!u ·M!u
′
) ◦ (M!v ·M!v′) (1)

where first cosine similarity (·) between the vec-
tors projected in the selected support subspaces is
computed and then a composition function ◦, such
as the sum or the product, is applied. Notice how
projection M may depend on the involved pairs
in complex ways. A Support Subspace can be de-
rived from just one pair pi and then being imposed
to the other with a corresponding asymmetric be-
havior of the Φ metrics, denoted by Φi. Alterna-
tively, M can be derived from projecting in two
Support Subspaces, as derived for the two pairs,
and then combining them by making again Φ sym-
metric. The symmetric composition function is
thus obtained as the combination:

Φ(")12 (p1, p2) = Φ
(◦)
1 (p1, p2) # Φ

(◦)
2 (p1, p2) (2)

where Φ1, as well as Φ2, projects both p1 and p2

16



into the Support Subspace of p1 (and p2, respec-
tively), and Φi are then combined via the ! opera-
tor (e.g. sum). Although Support Subspaces can-
not be applied to estimate similarity between com-
plex linguistic structures, they seem very effective
for simple syntactic structures. In (Annesi et al.,
2012) experiments over different variants of Eq. 1
and 2, i.e. different choices for projections M and
compositions ◦ and !, are there discussed. Best re-
sults are obtained within the dataset introduced in
(Mitchell and Lapata, 2010) when a multiplicative
operator ◦ is used in Eq. 1.

Recently, Compositional Semantics has been
used in syntactic parsing, as shown in (Socher et
al., 2013) where Compositional Vector Grammars
(CVGs) have been defined to extend small-state
Probabilistic Context-Free Grammars, introducing
distributional semantic constraints in constituency
parsing: interestingly, CVGs allows to estimate
the plausibility of the corresponding syntactic con-
stituent within a Recursive Neural Network, by as-
signing scores to nodes in the parse tree. A sim-
ilar integrated contribution of lexical information
(i.e. word vectors) and syntactic constituency is
proposed in semantic extensions of TKs, as in-
troduced in (Croce et al., 2011). As they offer
a framework to define similarity metrics strongly
tied to the syntactic structure of entire sentences,
they will be hereafter discussed.

Tree Kernels. Kernels are representationally
efficient ways to encode similarity metrics able
to support complex textual inferences (e.g. se-
mantic role classification) in supervised learning
models. Tree Kernels (TK) as they have been
early introduced by (Collins and Duffy, 2001)
correspond to Convolution Kernel (Haussler,
1999) over syntactic parse trees of sentence pairs.
A TK computes the number of substructures (as
well as their partial fragments) shared by two
parse trees T1 and T2. For this purpose, let the
set F = {f1, f2, . . . , f|F|} be a space of tree
fragments and χi(n) be an indicator function:
it is 1 if the target fi is rooted at node n and 0
otherwise. A tree-kernel function is a function
TK(T1, T2) =

∑
n1∈NT1

∑
n2∈NT2

∆(n1, n2),

where NT1 and NT2 are the sets of the
T1’s and T2’s nodes, respectively and
∆(n1, n2) =

∑|F|
i=1 χi(n1)χi(n2). The ∆

function recursively compute the amount of sim-
ilarity due to the similarity among substructures.
The type of fragments allowed determine the

expressiveness of the kernel space and different
tree kernels are characterized by different choices.
Lexical information has been early neglected in
recursive matching, so that only exact match-
ing between node labels were given a weight
higher than 0, (Collins and Duffy, 2001): even
when leaves are involved they must be equal, so
that no lexical generalization was considered.
An effective modeling of lexical information
is proposed by (Croce et al., 2011), in the so
called Smoothed Partial Tree Kernel (SPTK). In
SPTK, the TK extends the similarity between tree
structures allowing a smoothed function of node
similarity. The aim of SPTK is to measure the
similarity between syntactic tree structures, which
are semantically related, i.e. partially similar,
even when nodes, e.g. words at the leaves, differ.
This is achieved with the following formulation
of the function ∆ over nodes ni ∈ Ti:

∆σ(n1, n2)=µλσ(n1, n2),where n1 and n2
are leaves, else

∆σ(n1, n2)=µσ(n1, n2)
(
λ2 +

∑

"I1,"I2,l("I1)=l("I2)

(3)

λd(
"I1)+d("I2)

l("I1)∏

j=1

∆σ(cn1(#I1j), cn2(#I2j))
)

In Eq. 4, "I1j represents the sequence of sub-
trees, dominated by node n1, that are shared with
the children of n2 (i.e. "I2j ): as all other non-
matching substructures are neglected. Parameter λ
accounts for the decay factor penalizing embedded
trees, whose contribution affects too many domi-
nating structures towards the root. Moreover, σ
is a similarity between two nodes: for non termi-
nals it can be strict, such as the dot-product im-
posed to word vectors at the leaves. More de-
tails about SPTK as well as its efficient compu-
tation are discussed in (Croce et al., 2011). In
constituency parse trees, the lexical similarity is
only applied between leaves, which reflect words.
One main limitation of SPTK is that lexical simi-
larity does not consider compositional interaction
between words. Given the following phrase pairs
(np (nn river)(nn bank)) and (np (nn savings) (nn
bank)), the SPTK estimates the similarity between
bank without considering that they are composi-
tionally modified with respect different meanings.
Hereafter, the DCS operator of Eq. 1 and 2 will be
adopted to model semantic similarity at the nodes
in a parse tree, in general seen as head/modifier
syntactic pairs. Notice that this is the role of the

17



Figure 1: Constituency tree of the sentence “Polar
bears run towards the sea”

function σ in Eq. 4.

3 Explicit compositions in Parse Tree

In order to consider compositional semantic con-
straints during a Tree Kernel computation, parse
trees are here enriched to enable the definition
of a Compositionally Smoothed Partial Tree Ker-
nel, i.e. CSPTK. The input structures are thus
tree pairs whose nodes are enriched with lexi-
cal information needed for the recursive composi-
tional matching foreseen by the adopted convolu-
tion model. The syntactic structure of an individ-
ual sentence s is the constituency-based parse tree,
as shown in Figure 1. Nodes can be partitioned
into: terminal nodes n (T ), i.e. leaves repre-
senting lexical information, in terms of 〈ln::posn〉,
such as polar::j or bear::n, where l is the lemma
of the token and pos its part-of-speech1; Pre-
terminal nodes (PT ) are the direct ancestors of
terminals and they are marked through the pos of
their unique corresponding leaf; Non Pre-terminal
nodes (NPT ), i.e. nodes that are neither terminal
nor pre-terminal and reflect the phrase type, e.g.
nominal phrase (np) or verb phrase (vp). Notice
that all nodes in a tree express either lexical in-
formation (e.g. terminal n ∈ T ) or the composi-
tional information between one head and a mod-
ifier corresponding to subtrees, such as the non
pre-terminal in NPT . In order to model this in-
formation, we need to associate each node with
different types of information able to express ev-
ery aspect of compositionality, i.e. lexical as well
as grammatical properties. We model this infor-
mation in a form of a complex mark-up of generic
non pre-terminal nodes n ∈ NPT , in order to ex-
ploit them in a compositional extension of a tree
kernel (such as in Eq. 4). Compositionality oper-
ators acting on the subtrees depend on at least the
following types of syntactic as well as lexical in-

1General POS tags are obtained from the PennTreebank
standard by truncating at their first char (as in bear :: n).

formation:
Grammatical Types, denoted by GT , that express
the grammatical category of the constituent cor-
responding to the root of a subtree. Example of
these types are the np or vp traditional categories
of context-free grammars.
Lexical Information. Non pre-terminal nodes in
general express binary grammatical relations be-
tween a varying number of dominated subtrees
(i.e. direct descendants). Each node can be ex-
pressed in terms of an head/modifier pair, denoted
by (h,m). In order to emphasize the semantic
contribution that a subtree (compositionally) ex-
presses, the lexical information about the involved
head (lh) and modifier (lm) lexicals must be ex-
pressed: we denote this information through the
4-tuple 〈lh::posh,lm::posm〉. Notice that this in-
formation can be used as an index to a distribu-
tional semantic model where lexical entries are ex-
pressed by unique vectors for individual lemma
and POS tag pairs.
Syntactic Relations. Usually each node expresses
a specific syntactical relation between the head
and its modifier. Depending on linguistic theories
several system of types have been proposed. As in
this work, syntactic relations are only used to con-
strain structural analogies between two trees, the
reference relationship system adopted is not here
discussed. We denote the set of syntactic relations
SR, and they are usually derived by simply jux-
taposing grammatical labels of the involved head
and modifier, h and m, subtrees. Every relation
is denoted by relh,m ∈ SR. Examples of the
adopted syntactic relations are: vp/np for verb ob-
ject relation or nn/nn for noun compounds.

Therefore, according to the definitions above,
every non pre-terminal n ∈ NPT is marked with
the following triple

〈gT, relh,m, 〈lh :: posh, lm :: posm〉〉

where gT ∈ GT , relh,m ∈ SR, and li and posi
are lexical entries, and POS tags. This triple en-
ables the definition of a similarity function be-
tween sentence pairs through the recursive com-
parison of the marked subtrees. Given the recur-
sive nature of a Convolution Kernel, we will show
how the similarity estimation of two (sub)trees is
made dependent on the semantic equivalence be-
tween the triples assigned to their roots.

A shallow compositional function, that ignores
any syntactic information of gT and relh,m for the
head/modifier structure (h,m), can be straightfor-

18



wardly defined by adopting the DCS model dis-
cussed in Section 2, and in particular Eq. 2. Given
two subtrees in T1, T2, rooted at n1, n2, the corre-
sponding head-modifier pairs (h1,m1), (h2,m2)
are defined. This similarity metrics, based on the
geometric projection into Support Subspace (Eq.
2), can be applied as follows:

σComp
(
(h1,m1), (h2,m2)

)
= Φ(!)12 ((h1,m1), (h2,m2))

(4)

In particular, σComp is evaluated through the
Symmetric model introduced in (Annesi et al.,
2012). This model is characterized by a projec-
tion M that selects the 50 dimensions of the space
that maximize the component-wise product be-
tween compounds, and by the operator diamond
that combines the similarity scores with the prod-
uct function (Eq. 2). Moreover, similarity scores
in each subspace are obtained by defining ◦ as the
sum of cosine similarities in Eq. 1.

3.1 Mark-Up Rules for Constituency Trees
While marking terminal T nodes and pre-terminal
PT nodes is quite simple, the labeling of non pre-
terminal NPT nodes is complex. Grammar spe-
cific notations and rules are needed and mainly
differ with respect to (1) the type of the children
nodes, i.e. if they are all pre-terminal or not, and
(2) the arity of the branching at the root of a sub-
tree: n-ary , with n > 2, can be found indeed.

NPT nodes with binary branches correspond
to simple labeling, since exactly two subtrees are
always involved. On the basis of the underlying
CFG rule, the head and the modifier are deter-
mined and labeled. In particular, the treatment of
binary trees whose binary branches only involve
pre-terminal nodes depends exclusively on lexical
nodes n ∈ T . Given two terminal nodes (n1, n2),
described by 〈l1::pos1,l2::pos2〉, the mark-up rule
for their direct (non pre-terminal) ancestor is

pos2/pos1[h = n2,m = n1] ← (n1, n2) (5)

where [h = p2,m = p1] denotes that the sec-
ond leaf node has been selected as the head,
and the first as the modifier, while the rela-
tion is relpos2,pos1 . Figure 2 shows a fully la-
beled tree for the sentence whose unlabeled ver-
sion has been shown in Figure 1. The np
node spanning the polar bear phrase is labeled s
〈np, nns/jj, (bear :: n, polar :: j)〉.

Notice how usually the grammatical type as-
signed to a node does not change the assignment

Figure 2: Marking of a Compositional con-
stituency tree of the example in Figure 1

already provided by the tree. In some particular
cases, the pair head h and modifier m and the
syntactic relation relh,m implied by a node in the
tree are not fully defined either because some null
semantic content is encountered (e.g. a missing
modifier) or because it is not possible to model
some lexical as it is not present in distributional se-
mantic representation. Some relations exist where
the modifier seems not to carry relevant lexical in-
formation, as the case of the relation between a de-
terminer and a noun. In these cases, the modifier
of a non pre-terminal node as well as the syntac-
tic relation are neglected and null slots, i.e. ∗, are
used. An example in Figure 2 is the labeling of the
bi-gram the sea.

The labeling of prepositional phrases consti-
tute a somehow special case of the marking
of non pre-terminal nodes NPT . The lexi-
cal information carried out by prepositions is
not directly expressed lexically but it is inte-
grated both in the grammatical type gT and
in the relh,m. This is the case of the triple
〈towards|pp, np/towards, (sea :: n, ∗)〉 in Fig-
ure 2. The treatment of binary trees whose bi-
nary branches involves pre-terminal nodes or non
pre-terminal nodes is also straightforward. Where
PT nodes depend strictly from the lexical leaves,
NPT node dominate complex subtrees. The main
difference with respect the Equation 5 is that n1,
n2 or both subtrees may correspond to NPT
node ni. In a bottom up fashion, the head mod-
ifier pair (hi,mi) already assigned to ni is prop-
agated upward. The dominating node is marked-
up according to the head hi of its corresponding
dominated branches. For NPT nodes, the head
and the modifier are still assigned by Equation 5,
whereas only the heads of the involved subtrees
are used. For example, in Figure 2, the root is
marked as 〈s, np/vp, (run :: v, bear :: n)〉 ac-
cording to the heads, i.e. run::v and bear::n, of

19



the corresponding branches (i.e. the right vp and
the left np). When NPT nodes have more than
2 branches (e.g. all pre-terminal nodes or other
NPT nodes), criteria depending on the specific
context free rules of the underlying grammar are
adopted to select the proper head and modifier.

4 The Compositionally Smoothed Partial
Tree Kernel

When the compositionally enriched parse tree is
available, it is possible to measure the similar-
ity between this constituency structures through
a Tree Kernel. We define here the Composition-
ally Smoothed Partial Tree Kernel (CSPTK) as a
similarity function for such that structures, by ex-
tending the SPTK formulation. Let us consider
the application of the SPTK on the tree shown in
Figure 2. When estimating the similarity with a
tree derived from sentences such as “Bear market
runs towards the end” or “The commander runs
the offense”, the kernel will estimate the similarity
among all nodes. Then, the σ function in Equa-
tion 4 would not be able to exploit the different
senses of the verb run, as a traditional distribu-
tional model would provide a unique representa-
tion. The aim of the CSPTK is to exploit the
observable compositional relationships in order
to emphasize the contributions of the compounds
to the overall meaning, even where the syntactic
structure does not change, such as in “run the of-
fense”, i.e. attacking someone, vs. “run towards
the end”.

The core novelty of the CSPTK is the new es-
timation of σ as described in Algorithm 1. For
the terminal nodes (i.e. LEX type) a lexical kernel
σLEX , i.e. the cosine similarity between words
sharing the same POS-Tag, is applied. Other-
wise between pre-terminal nodes, a strong match-
ing is required, assigning 0/1 similarity only if pre-
terminal nodes share the same POS. The novel part
of Algorithm 1 is introduced with the similarity
computation over non pre-terminal nodes. In order
to activate the similarity function between NPT
nodes, they must have the same gT and relh,m. In
this case, the Subspace operator in Equation 2 is
applied between the involved (h,m) compounds:
lexical information pairs are checked and if their
respective heads and modifiers share the corre-
sponding POS, the compositional similarity func-
tion is applied.

As discussed in Section 3.1, modifier could be

missing in lexical information pair. The DCS
model is applied according to three strategies:
General case. If nodes have both heads and mod-
ifiers, the similarity function of Equation 4 is ap-
plied as usual. Notice that the pos-tags of heads
and modifiers must be the same.
A modifier is missing. An “optimistic” similar-
ity estimator can be defined in this case. Let be
(hx, ∗) and (hy,my) the lexical information of
two nodes x (that lacks of the modifier) and y.
The forced pair (hx,my) and the pair (hy,my)
projected and compared into their own subspaces,
provide a measure of how the head hx is similar
to hy, with respect to the meaning that they evoke
together with my. The more hx and hy could be
both modified by my to specify the same meaning,
the higher is the received score.
Both modifiers are missing. This case is reduced
to the treatment of lexical nodes (i.e. LEX type),
and no composition is observed: the lexical ker-
nel σLEX , i.e. the cosine similarity between word
vectors, is adopted as no subspace is needed.

Algorithm 1 στ (nx, ny, lw) Compositional estimation of
the lexical contribution to semantic tree kernel
στ ← 0,
if nx = 〈lexx::pos〉 and ny = 〈lexy ::pos〉 then

στ ← lw · σLEX(n1, n2)
end if
if nx = pos and nx = ny then

στ ← 1
end if
if nx =

〈
gT, syntRel, 〈lix〉

〉
and ny =

〈
gT, syntRel, 〈liy〉

〉

then
/*Both modifiers are missing*/
if lix = 〈hx::pos〉 and liy = 〈hy ::pos〉 then

στ ← σCOMP
(
(hx), (hy)

)
= σLEX(nx, ny)

end if
/*One modifier is missing*/
if lix = 〈hx::posh〉 and liy = 〈hy ::posh,my ::posm〉 then

στ ← σCOMP
(
(hx,my), (hy,my)

)

end if
/*General Case*/
if lix = 〈hx::posh,mx::posm〉 and
liy = 〈hy ::posh,my ::posm〉 then

στ ← σCOMP
(
(hx,mx), (hy,my)

)

end if
end if
return στ

Notice that Algorithm 1 could be be still mod-
ified further depending on how the non terminal
similarity has to be strict on gT and relh,m and
on how much is the weight of terminal and pre-
terminal nodes.

5 Experimental Evaluations

In this section the CSPTK model is used in Seman-
tic Textual Similarity (STS) and Question Classi-
fication (QC) tasks. The aim of this section is to
measure the CSPTK capability to account for the

20



similarity between sentences and as a feature to
train machine learning classifiers.

5.1 Experimental Setup
In all experiments, sentences are processed with
the Stanford CoreNLP2, for Part-of-speech tag-
ging, lemmatization, and dependency and com-
positionally enriched parsing. In order to reduce
data sparseness introduce by fined grained Part-of-
Speech classes, node are marked by coarse grained
classes, e.g. looking:VBG or looked:VBD are sim-
plified in look:V. In order to estimate the basic lex-
ical similarity function employed in the Tree Ker-
nels operators, a co-occurrence Word Space is ac-
quired through the distributional analysis of the
UkWaC corpus (Baroni et al., 2009). First, all
words occurring more than 100 times (i.e. the tar-
gets) are represented through vectors. The orig-
inal space dimensions are generated from the set
of the 20,000 most frequent words (i.e. features)
in the UkWaC corpus. A co-occurrence Word-
Space with a window of size 3 is acquired. Co-
occurrencies are weighted by estimating the Point-
wise Mutual Information between the 20k most
frequent words. The SVD reduction is then ap-
plied with a dimensionality cut of d = 250. Left
contexts are treated differently from the right ones,
in order to capture asymmetric syntactic behaviors
(e.g., useful for verbs): 40,000 dimensional vec-
tors are thus derived for each target. Similarity
between lexical nodes is estimated as the cosine
similarity in the co-occurrence Word Space, as in
(Croce et al., 2011).

5.2 The Semantic Text Similarity task
The first experiment aims to evaluate the contribu-
tion of the Kernel-based operators in a STS task.
In the Core STS task given two sentences, s1 and
s2, participants are asked to provide a score re-
flecting the corresponding text similarity (Agirre
et al., 2013). PTK, SPTK and CSPTK similar-
ity functions are employed over the dataset of the
*SEM 2013 shared task. In Table 1 results of
Pearson Correlations between the Kernels opera-
tors and the human scores are shown. We con-
sidered all datasets composing the challenge train-
ing set, i.e. MSRvid, MSRpar, SMTeuroparl, sur-
prise.OnWn and surprise.SMTnews as well as the
test set Headlines, FNWN and SMTnews. We did
not report any comparison with the best results of

2
http://nlp.stanford.edu/software/corenlp.shtml

the SemEval STS competition as those approaches
are mostly supervised. On the contrary the pre-
sented approach for the STS estimation is fully
unsupervised. The purpose of the experiments
is to i) investigate the differences between Ker-
nels operators when lexical semantics (i.e. SPTK
and CSPTK) is added to the syntactic information
(i.e. PTK), ii) analyze the role of the the composi-
tional compounds made explicit in parse trees and
iii) measure the contribution of the DCS model
adopted in the recursive CSPTK computation.

PTK and SPTK functions are both applied to the
Constituency Tree representations, labeled with
ct, while the CSPTK model consists in: i) lex-
ical mark-up as a form of lexical compositional
caching that generates the input Compositionally
labeled Constituency Tree representation (denoted
by cct) as introduced and discussed in Section 3
and ii) the matching function among the subtrees
3

Dataset PTKct SPTKct CSPTKcct
MSRVid .12 .18 .65
MSRPar .26 .28 .32

SMTEuroparl .45 .45 .50
surprise.OnWN .49 .55 .59

surprise.SMTNews .46 .46 .46
FNWN .15 .19 .21

Headlines .40 .49 .52
OnWN .04 .24 .37

SMTNews .28 .31 .33

Table 1: Unsupervised results of Pearson correlation for
Kernel-based features adopted in *SEM - Task 6 datasets

First and second columns in Table 1 show Pear-
son results of PTKct and SPTKct functions applied
over a constituency tree, while the last column
shows the CSPTKcct results over the composition-
ally labeled tree. Notice how the introduction of
the compositionality enrichment in a constituency
tree structure, together with the CSPTK function
led to a performance boost over all the training and
test datasets. In some cases, the boost between
SPTKct and CSPTKcct is remarkable, switching
from .18 to .65 in MSRvid and from .24 to .37
in OnWN.

The above difference is mainly due to the in-
creasing sensitivity of PTK, SPTK and CSPTK to
the incrementally rich lexical information. This
is especially evident in sentence pairs with very

3For the SPTK, we selected the parameters λ = 0.1, µ =
0.1 and lexical weight = 100 that provided best results
in (Croce et al., 2012). Otherwise for CSPTK we selected
λ = 0.4, µ = 0.4 and lexical weight = 10.

21



similar syntactic structure. For example in the
MSRvid dataset, a sentence pair is given by The
man are playing soccer and A man is riding a mo-
torcycle, that are strictly syntactically correlated.
As a side effect, PTK provides a similarity score
of .647 between the two sentences. It is a higher
score with respect to the SPTK and CSPTK: dif-
ferences between tree structures are confined only
to the leaves. By scoring .461, SPTK introduces
an improvement as the distributional similarity
(function σ in Eq. 4) that acts as a smoothing
factor between leaves better discriminates uncor-
related words, like motorcycle and soccer. How-
ever, ambiguous words such verbs ride and play
are still promoting a similarity that is locally mis-
leading. Notice that both PTK and SPTK receive
a strong contribution in the recursive computation
of the kernels by the left branching of the tree, as
the subject is the same, i.e. man. Compositional
information about direct objects (soccer vs. mo-
torcycle) is better propagated by the CSPTK op-
erator. Its final scores for the pair is .36, as se-
mantic differences between the sentences are em-
phasized. Even if grammatical types strongly con-
tribute to the final score (as in PTK or SPTK), now
the DCS computation over these nodes (the com-
pounds traced from the leaves, i.e. (ride::v, mo-
torcycle::n) and (play::v, soccer::n) is faced with
less ambiguous verb phrases, that contribute with
lower scores.

5.3 CSPTK for Question Classification

Thanks to structural kernel similarity, a question
classification (QC) task can be easily modeled by
representing questions, i.e., the classification tar-
gets, with their parse trees. The aim of the ex-
periments is to analyze the role of lexical simi-
larity embedded in the compositionally enriched
constituency trees by the CSPTK operator. Thus,
questions have been represented by the classic
constituency tree, i.e. ct, and by the composition-
ally enriched variant, i.e. cct. The first representa-
tion is used over PTK and SPTK functions, while
the latter over the CSPTK function. Our referring
corpus is the UIUC dataset (Li and Roth, 2002). It
is composed by a training set of 5,452 questions
and a test set of 500 questions4. The latter are
organized in six coarse-grained classes, i.e. AB-
BREVIATION, ENTITY, DESCRIPTION, HU-
MAN, LOCATION and NUMBER. For learning

4http://cogcomp.cs.illinois.edu/Data/QA/QC/

our models we employed LIBSVM5 after comput-
ing the entire Gram Matrix. The F1 of SVMs
using (i) PTK and SPTK applied to ct and (ii)
CSPTK applied to cct for QC, is reported in Ta-
ble 2. Notice that we want to carry out a compar-
ative evaluation of different syntactic kernels and
not to optimize the QC accuracy as this requires
a combination of lexical and syntagmatic kernels
as discussed in (Croce et al., 2011). The results
are in general outperforming the alternative ker-
nel formulations, even if the improvement is mod-
est. First, we outline that a more stable behavior
wrt parameters is observed, with a corresponding
lower risk of over-fitting the training data. Second,
it is to be noticed that a large number of questions
have a really simple syntactic structure: as a con-
sequence the interrogative form of the sentence is
very simple and very few compositional phenom-
ena are observed that are captured by the distribu-
tional information about word vectors.

SVM par PTKct SPTKct CSPTKcct
c=1 .78 .89 .91
c=2 .83 .90 .90
c=5 .88 .92 .92

Table 2: Results in the Question Classification task

6 Conclusions

In this paper, a novel kernel function has been pro-
posed in order to exploit Distributional Compo-
sitional operators within Tree Kernels. The pro-
posed approach propagates lexical semantic infor-
mation over an entire constituency parse tree, by
building a Compositionally labeled Constituency
Tree. It enables the definition of the Composi-
tional Smoothed Partial Tree Kernel as a Con-
volution Kernel that measures the semantic sim-
ilarity between complex linguistic structures by
applying metrics sensible to the compositional-
ity. First empirical results of the CSPTK in STS
and QC tasks demonstrate the robustness and the
generalization capability of the proposed kernel.
Future investigation is needed in the adoption of
the same compositionality perspective on depen-
dency graphs, were the compositional represen-
tations of the head/modifier compound are even
more explicit. Further experiments for assessing
the methodology are also foreseen against other
NLP-tasks, e.g. verb classification.

5http://www.csie.ntu.edu.tw/cjlin/libsvm/

22



References
Eneko Agirre, Daniel Cer, Mona Diab, Aitor Gonzalez-

Agirre, and Weiwei Guo. 2013. *sem 2013 shared
task: Semantic textual similarity. In Second Joint
Conference on Lexical and Computational Seman-
tics (*SEM), Volume 1: Proceedings of the Main
Conference and the Shared Task: Semantic Textual
Similarity, pages 32–43, Atlanta, Georgia, USA,
June. Association for Computational Linguistics.

P. Annesi, V. Storch, and R. Basili. 2012. Space pro-
jections as distributional models for semantic com-
position. In In Proceedings of CICLing 2012, vol-
ume 7181 of Lecture Notes in Computer Science,
pages 323–335. Springer.

M. Baroni, S. Bernardini, A. Ferraresi, and
E. Zanchetta. 2009. The wacky wide web: a
collection of very large linguistically processed
web-crawled corpora. Language Resources and
Evaluation, 43(3):209–226.

P. Blackburn and J. Bos. 2005. Representation and
Inference for Natural Language. A First Course in
Computational Semantics. CSLI Publications.

M. Collins and N. Duffy. 2001. Convolution kernels
for natural language. In Proceedings of Neural In-
formation Processing Systems (NIPS), pages 625–
632.

D. Croce, A. Moschitti, and R. Basili. 2011. Struc-
tured lexical similarity via convolution kernels on
dependency trees. In Proceedings of EMNLP, Ed-
inburgh, Scotland, UK.

D. Croce, P. Annesi, V. Storch, and R. Basili. 2012.
Unitor: Combining semantic text similarity func-
tions through sv regression. In *SEM 2012, pages
597–602, Montréal, Canada, 7-8 June.

K. Erk and S. Pado. 2008. A structured vector space
model for word meaning in context. In EMNLP ’08:
Proceedings of the Conference on Empirical Meth-
ods in Natural Language Processing, pages 897–
906. ACL.

P. Foltz, W. Kintsch, and T. Landauer. 1998. The mea-
surement of textual coherence with latent semantic
analysis. Discourse Processes, 25:285–307.

E. Guevara. 2010. A regression model of adjective-
noun compositionality in distributional semantics.
In Proceedings of the GEMS ’10, pages 33–37,
Stroudsburg, PA, USA. Association for Computa-
tional Linguistics.

D. Haussler. 1999. Convolution kernels on discrete
structures. Technical report, Dept. of Computer Sci-
ence, University of California at Santa Cruz.

H. Kamp and U. Reyle. 1993. From discourse to logic:
introduction to modeltheoretic semantics of natural
language, formal logic and discourse representation
theory. Part 1. Kluwer Academic.

T. Landauer and S. Dumais. 1997. A solution to platos
problem: The latent semantic analysis theory of ac-
quisition, induction, and representation of knowl-
edge. Psychological review, pages 211–240.

X. Li and D. Roth. 2002. Learning question classifiers.
In Proceedings of ACL ’02, COLING ’02, pages 1–
7, Stroudsburg, PA, USA. Association for Computa-
tional Linguistics.

J. Mitchell and M. Lapata. 2008. Vector-based mod-
els of semantic composition. In In Proceedings of
ACL/HLT 2008, pages 236–244.

J. Mitchell and M Lapata. 2010. Composition in dis-
tributional models of semantics. Cognitive Science,
34(8):1388–1429.

H. Schütze. 1998. Automatic word sense discrimina-
tion. Comput. Linguist., 24(1):97–123, March.

P. Smolensky. 1990. Tensor product variable bind-
ing and the representation of symbolic structures in
connectionist systems. Artif. Intell., 46:159–216,
November.

Richard Socher, John Bauer, Christopher D. Manning,
and Ng Andrew Y. 2013. Parsing with composi-
tional vector grammars. In Proceedings of the 51st
Annual Meeting of the Association for Computa-
tional Linguistics (Volume 1: Long Papers), pages
455–465, Sofia, Bulgaria, August. Association for
Computational Linguistics.

Peter D. Turney and Patrick Pantel. 2010. From
frequency to meaning: Vector space models of se-
mantics. Journal of artificial intelligence research,
37:141.

Fabio Massimo Zanzotto, Ioannis Korkontzelos,
Francesca Fallucchi, and Suresh Manandhar. 2010.
Estimating linear models for compositional distribu-
tional semantics. In Proceedings of the 23rd Inter-
national Conference on Computational Linguistics
(COLING).

23


