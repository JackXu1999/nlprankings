



















































Developing and Orchestrating a Portfolio of Natural Legal Language Processing and Document Curation Services


Proceedings of the Natural Legal Language Processing Workshop 2019, pages 55–66
Minneapolis, Minesotta, June 7, 2019. c©2019 Association for Computational Linguistics

55

Developing and Orchestrating a Portfolio of Natural Legal
Language Processing and Document Curation Services

Georg Rehm1, Julián Moreno-Schneider1, Jorge Gracia2, Artem Revenko3,
Victor Mireles3, Maria Khvalchik3, Ilan Kernerman4, Andis Lagzdins5, Marcis Pinnis5,

Arturs Vasilevskis5, Elena Leitner1, Jan Milde1, Pia Weißenhorn1
1DFKI GmbH, Germany; 2University of Zaragoza, Spain;

3Semantic Web Company, Austria; 4K Dictionaries, Israel; 5Tilde, Latvia

Corresponding author: georg.rehm@dfki.de

Abstract

We present a portfolio of natural legal lan-
guage processing and document curation ser-
vices currently under development in a collab-
orative European project. First, we give an
overview of the project and the different use
cases, while, in the main part of the article, we
focus upon the 13 different processing services
that are being deployed in different prototype
applications using a flexible and scalable mi-
croservices architecture. Their orchestration is
operationalised using a content and document
curation workflow manager.

1 Introduction

We present a portfolio of various Natural Legal
Language Processing and Document Curation ser-
vices currently under development in the collab-
orative EU project LYNX, in which a consortium
of partners from academia and industry develops a
platform for the easier and more efficient process-
ing of documents from the legal domain. First, our
platform is acquiring data and documents related
to compliance from multiple jurisdictions in dif-
ferent languages with a focus on Spanish, German,
Dutch, Italian and English, along with terminolo-
gies, dictionaries and other language resources.
Based on this collection of structured data and un-
structured documents we create the multilingual
Legal Knowledge Graph (LKG). Second, a set of
flexible language processing services is developed
to analyse and process the data and documents to
integrate them into the LKG. Semantic process-
ing components annotate, structure, and interlink
the LKG contents. The LKG is incrementally aug-
mented by linking to external data sets, by discov-
ering topics and entities linked implicitly, as well

as by using machine translation services to pro-
vide access to documents, previously unavailable,
in certain languages. Finally, three pilots are de-
veloped that exploit the LKG in industry use cases.

The remainder of this article is structured as fol-
lows. Section 2 describes the different use cases,
while, in the main part of the article (Section 3),
we focus upon the 13 different processing services
used in the prototype applications . The orches-
tration of the services is operationalised using a
content and document curation workflow manager
(Section 4). After a brief review of related work
(Section 5) we summarise the article and take a
look at future work (Section 6).

2 Use Cases

Within LYNX we work with three different use
cases embedded in use case scenarios. In the fol-
lowing, we briefly sketch the three use cases.

The objective of the contract analysis use case
is to enhance compliance with data protection
obligations through automation, reducing costs,
corporate risks and personal risks. The prototype
analyses data protection legislation and case law
from the EU and Member States and contracts be-
tween controllers, data subjects, processors, data
processing policies and general contracts.

The labour law use case provides access to ag-
gregated and interlinked legal information regard-
ing labour law across multiple legal orders, juris-
dictions, and languages. The prototype analyses
labour legislation from the EU and Member States,
and jurisprudence related to labour law issues.

The oil and gas use case is focused on compli-
ance management support for geothermal energy
projects and aims to obtain standards and regula-
tions associated with certain terms in the field of



56

geothermal energy. A user can submit a RFP or
feasibility study to the system and is then informed
which standards or regulations must be taken into
consideration to carry out the considered project in
a compliant manner. This scenario will innovate
and speed up existing compliance related services.

3 NLLP Services

In the following main part of this article we
describe many of the Natural Legal Language
Processing services currently under development
in our project: Term Extraction (Section 3.1),
Lexical Resources (Section 3.2), Named En-
tity Recognition (Section 3.3), Concept Extrac-
tion (Section 3.4), Word Sense Disambigua-
tion (Section 3.5), Temporal Expression Anal-
ysis (Section 3.6), Legal Reference Resolution
(Section 3.7), Text Structure Recognition (Sec-
tion 3.8), Text Summarisation (Section 3.9), Ma-
chine Translation (Section 3.10), Legal Knowl-
edge Graph Population (Section 3.11), Semantic
Similarity (Section 3.12) and Question Answering
(Section 3.13). This set of services is heteroge-
neous: some of the services make use of other ser-
vices, some services extract or annotate informa-
tion (e. g., NER or Temporal Expression Analy-
sis), while others operate on full documents (e. g.,
summarisation or machine translation), yet others
provide a user interface (e. g., QA).

3.1 Term Extraction

To enable the creation of a taxonomy for a certain
use case, domain or company, we use the cloud-
based Tilde Terminology term extraction service1.
It extracts terms from different corpora follow-
ing the methodology by Pinnis et al. (2012). As
a result, the platform creates a SKOS vocabulary
containing terms, contexts and references to their
source documents. Each term comes with a rank-
ing score to describe the terms specificity in the
source corpora compared to a general language
corpus. The score is calculated based on TF-IDF
(Spärck Jones, 1972) and co-occurrence statistics
for multi-word terms (Pinnis et al., 2012). Once
the term extraction workflow has been triggered,
a corresponding online platform takes over. The
workflow starts with plain text extraction from dif-
ferent file formats, then all plain-text documents
are annotated, and a single collection of terms is
created. As multiple surface forms of the same

1https://term.tilde.com

term may appear in the text, term normalisation is
performed. This term collection is the first step
towards, initially, creating or, later on, enriching
the Legal Knowledge Graph. The collection can
be used for creating hierarchical taxonomies aug-
mented with multilingual information and linked
to other knowledge bases.

3.2 Lexical Resources for the Legal Domain

An essential aspect of the LKG is its capability to
be easily adaptable across domains and sectors. It
is based on both domain-dependent and domain-
independent vocabularies, which will be accessi-
ble through a common RDF graph. The domain-
dependent vocabularies account for particular ter-
minologies coming from the legal sector and our
use case domains (e. g., EuroTermBank2). The
domain-independent vocabularies are taken from
families of monolingual, bilingual and multilin-
gual dictionaries published by one of our project
partners, such as Global, Password, and Ran-
dom House.3 They contain various cross-lingual
links for the five languages served by our plat-
form (Dutch, English, German, Italian, Spanish).
Besides their overall coverage of solely domain-
independent vocabularies, they contain informa-
tion on words and phrases that include also or
only domain-dependent meanings (e. g., court for
the former, lawyer for the latter). The motivation
of relying on domain-independent dictionary data
for the LKG is thus twofold: first, they provide
a common substrate across domains that facili-
tates traversing semantically annotated documents
coming from different specialised domains (e. g.,
Legal or Oil & Gas); second, they support certain
NLP functionalities such as Word Sense Disam-
biguation and Word Sense Induction by providing
a common catalogue of word senses. The data is
being remodeled in RDF according to the Ontolex
Lemon Lexicography Module Specification4 and
is accessed by the platform via a RESTful API.
The LKG has a common core part (terminologies,
sets of annotated legal corpora), but can be ex-
panded to accommodate the necessities of partic-
ular use cases (e. g., to store private contracts).

2http://www.eurotermbank.com
3https://www.lexicala.com
4https://jogracia.github.io/

ontolex-lexicog/

https://term.tilde.com
http://www.eurotermbank.com
https://www.lexicala.com
https://jogracia.github.io/ontolex-lexicog/
https://jogracia.github.io/ontolex-lexicog/


57

3.3 Named Entity Recognition

The service for named entity recognition (NER)
includes the elaboration of corresponding seman-
tic classes and the preparation of a German lan-
guage data set. Several state of the art mod-
els were trained, i. e., Conditional Random Fields
(CRFs) and bidirectional Long-Short Term Mem-
ory Networks (BiLSTMs), and evaluated (Finkel
et al., 2005; Faruqui and Padó, 2010; Benikova
et al., 2014, 2015; Huang et al., 2015; Lample
et al., 2016; Riedl and Padó, 2018, etc.). For
training and evaluating the system we used a data
set of German court decisions that was manually
annotated with seven coarse-grained and 19 fine-
grained classes: names and citations of people
(person, judge, lawyer), location (country, city,
street, area), organisation (organisation, company,
institution, court, brand), legal norm (law, legal
regulation, European legal norm), case-by-case
regulation (regulation, contract), case law, and le-
gal literature. The data set consists of approxi-
mately 67,000 sentences and around 54,000 anno-
tated entities. For the experiment, two tools for
sequence labeling were chosen. These are sklearn-
crfsuite (CRFs)5 and UKPLab-BiLSTM (BiL-
STMs)6 (Reimers and Gurevych, 2017). Three
different models and two classifications each are
developed for each of these model families (19
and seven classes, respectively). For CRFs these
are (1) CRF-F with features, (2) CRF-FG with fea-
tures and gazetteers, (3) CRF-FGL with features,
gazetteers, and the lookup table. For BiLSTMs
we used (1) BiLSTM-CRF without character em-
beddings, (2) BiLSTM-CRF+, and (3) BiLSTM-
CNN-CRF with character embeddings generated
by BiLSTM and by a CNN. In order to reliably
estimate the performance of the models, we use
stratified 10-fold cross-validation, which prevents
overfitting during training. The stratification guar-
antees that the semantic classes are equally fre-
quent in the test set relative to the size of the train-
ing set, which avoids measurement errors in the
case of unbalanced data. The results were mea-
sured with precision, recall, and F1-measure. The
BiLSTM models performed better compared to
CRF (see Table 1), the F1 values were between
93.75–95.46 % for the fine-grained classes and

5https://sklearn-crfsuite.readthedocs.
io/en/latest/

6https://github.com/UKPLab/
emnlp2017-bilstm-cnn-crf

between 94.68–95.95 % for the coarse-grained
classes. By contrast, the CRF models reached
93.05–93.23 % and 93.11–93.22 %. Overall, the
CRF models achieved about 1–10 % lower scores
per class than the BiLSTMs. The models pro-
vide the best results in the fine-grained classes
of judges, courts and laws; their F1 values were
95 %. Performance was an F1 value over 90 % in
the classes countries, institutions, case laws and
legal literature. The recognition of the classes per-
sons, lawyers, cities, companies, legal regulations,
European legal norms and contracts varied from
84 % to 93 %. In contrast, the values in the classes
streets, landscapes, organisations and regulations
were the lowest and amounted to 69–80 % with
the CRF models and to 72–83 % with BiLSTM.
The worst result was observed in the class brands.
With CRF, a maximum F1 value of 69.61 % was
reached and with BiLSTM a maximum F1 value of
79.17 %. The current NER tool is a working pro-
totype. It already provides named entities locally,
but is still being evaluated further. As of now, the
service is available for German texts, but it can be
easily adapted to other languages.

3.4 Concept Extraction

The LKG contains among its nodes entities from
controlled vocabularies. These are typically ex-
pressed as SKOS concepts, which permits assign-
ing to them multiple labels, i. e., various surface
forms, in multiple languages. Furthermore, one
can define relations between instances of concepts,
such as hypernymy, to create a taxonomy. Tax-
onomies become useful when their concepts can
be identified in documents, a process called Con-
cept Extraction. A simple example would be tak-
ing the sentence “The tenants must pay the heat-
ing costs by themselves”, and identifying the pres-
ence of the concepts “tenant” and “heating costs”.
If these are known to be instances of Contractual
parties and Energy costs, respectively, a search
for “energy costs” would point the user to this
sentence. Thus, once concept extraction is per-
formed, links between documents and elements of
controlled vocabularies in the LKG can be estab-
lished. While these relations are rather simple,
they are the first step for enriching text fragments
with knowledge from the LKG, as well as to en-
able further algorithms for the (semi-)automatic
extension of the LKG. Importantly, the inclusion
of labels in many languages allows linking of

https://sklearn-crfsuite.readthedocs.io/en/latest/
https://sklearn-crfsuite.readthedocs.io/en/latest/
https://github.com/UKPLab/emnlp2017-bilstm-cnn-crf
https://github.com/UKPLab/emnlp2017-bilstm-cnn-crf


58

Table 1: F1 values of the CRF and BiLSTM models for the coarse-grained classes.

Class CRFs BiLSTMs
F FG FGL CRF CRF+ CNN-CRF

Person 91.74 92.20 92.16 94.74 95.41 95.12
Location 89.26 89.45 90.18 91.68 93.31 92.57
Organisation 90.87 90.99 91.11 91.37 92.87 93.21
Legal norm 95.67 95.77 95.86 96.77 97.98 97.79
Case-by-case regulation 86.94 86.96 86.39 85.43 90.61 90.43
Case law 93.23 93.25 93.08 96.56 96.99 96.78
Legal literature 91.92 92.06 92.11 93.84 94.42 94.02
Total 93.11 93.22 93.22 94.68 95.95 95.79

documents in different languages, combining the
knowledge derived from them, as well as multi-
lingual search and recommendation. The Concept
Extraction service works in as many languages as
the taxonomies have labels in, and thus we can
leverage multinational efforts for creating multi-
lingual taxonomies such as EUROVOC7 or UN-
BIS8. Furthermore, in the case where documents
are in English, Spanish, Dutch, German, French,
Italian, Czech or Slovak languages, additional lin-
guistic processing increases the recall. The service
can be used for production. It is available in most
European languages.

3.5 Word Sense Disambiguation
To enable the use of incomplete KGs for automatic
text annotations, we introduce a robust method for
discriminating word senses using thesaurus infor-
mation like hypernyms, synonyms, types/classes,
which is contained in the KG. The method uses
collocations to induce word senses and to discrim-
inate the thesaurus sense from others. Its main
novelty is using thesaurus information already at
the stage of sense induction. The given KG en-
ables us to cast the task to a binary scenario,
namely telling apart the KG sense from all the
others. This method does not require all possi-
ble senses of a word to be contained in the KG,
which makes it especially useful in a production
environment, where usually only incomplete KGs
are available. We take as input a corpus, thesaurus
information, and a concept from the KG, one of
whose labels is found throughout the corpus (the
target label). We want to distinguish, for each
document in the data set, whether the target label
is used in the thesaurus sense or not.9 Thus, the

7https://publications.europa.eu/en/
web/eu-vocabularies/

8http://metadata.un.org/?lang=en
9Without loss of completeness we consider only the case

when the target label is used in the same sense in all occur-
rences in a document. One can, of course, consider the con-

Table 2: Cocktails WSID accuracy scores

Macro average Micro average
Our Method 0.841 0.896
Baseline 0.725 0.737

Table 3: MeSH WSID accuracy scores

Macro average Micro average
Our Method 0.723 0.739
Baseline 0.680 0.735

end result is a partition of the corpus into two dis-
joint collections: “this” and “other”. The collec-
tion “this” contains the documents that feature the
target label in the thesaurus sense, the collection
“other” contains any other sense which does not
match the domain captured in the thesaurus, which
can be more than one. The experiments were
conducted on two data sets10 created specifically
for this task: Cocktails and MeSH (Revenko and
Mireles, 2017) (Table 3). This service is used for
any kind of entity linking, especially after NER.
This is done to correctly identify which named en-
tities are indeed within the vocabulary scope of the
LKG. The service is a working prototype. It is
language agnostic, i. e., works for any language as
long as the text can be tokenised correctly.

3.6 Temporal Expression Analysis

Documents from the legal domain contain a multi-
tude of temporal expressions that can be analysed,
normalised (i. e., semantically interpreted) and
further exploited for document and information
mining purposes. We implemented a prototype
for the analysis of time expressions in German-
language legal documents, especially court deci-
sions and legislative texts. Temporal expressions

text of every occurrence of the target label as a separate doc-
ument, therefore extending the method to disambiguate every
single occurrence of the target label.

10https://github.com/artreven/
thesaural_wsi

https://publications.europa.eu/en/web/eu-vocabularies/
https://publications.europa.eu/en/web/eu-vocabularies/
http://metadata.un.org/?lang=en
https://github.com/artreven/thesaural_wsi
https://github.com/artreven/thesaural_wsi


59

Table 4: Comparison of the results of the original version of HeidelTime (HT) with the modified (HT nV) on the
evaluation corpus. The last line indicates the improvement.

strict partial strict+value partial+value
P R F1 P R F1 P R F1 Acc P R F1 Acc

HT 86.8 86.0 86.4 89.5 88.1 88.8 86.1 85.3 85.7 99.2 88.4 87.1 87.8 98.9
HTnV 94.9 92.0 93.5 96.6 93.5 95.0 94.0 91.1 92.5 99.0 95.3 92.3 93.8 98.7
+ 8.2 6.1 7.1 7.1 5.4 6.2 7.9 5.8 6.9 0.2 6.9 5.1 6.0 0.2

include dates, e. g., “1. Januar 2000” (1st January
2000), durations, e. g., “fünf Kalenderjahre” (five
calendar years) and repeating time intervals, e. g.,
“jeden Monat” (every month). Such expressions
should not only be identified, but also normalised
by translating them into a standardised ISO for-
mat. Since no suitable data set existed, a text col-
lection was prepared and annotated with tempo-
ral expressions using the TimeML standard. Pre-
viously, the automatic identification of temporal
expressions (temporal tagging) has been mainly
focused on English and domains such as news
and narrative texts. Research showed that this
task is domain- and language-sensitive, i. e., sys-
tems have to be adapted to the specific domain
or language to ensure consistent performance.
We can confirm this observation: during the an-
notation of the corpus deficits had become ap-
parent, which concerned not only the annota-
tion guidelines, but also the performance of the
rule-based temporal tagger HeidelTime (Strötgen
and Gertz, 2010), which was subsequently ex-
tended. One of the specifics of the domain are
references to other legal texts which contain (al-
leged) dates (“Richtlinie 2008 / 96 /EG”, “Direc-
tive 2008 / 96 /EG”). Other peculiarities of the do-
main and/or language are the frequent use of com-
pounds such as “Kalenderjahr” (calendar year),
“Fälligkeitsmonat” (due month) or “Bankarbeit-
stag” (banking day), generic use of temporal ex-
pressions such as “jeweils zum 1. Januar” (1st Jan-
uary of each year) and event-anchored temporal
expressions “Tag der Verkündigung” (proclama-
tion day). Based on our new annotated corpus,
HeidelTime was adapted to the domain. The eval-
uation showed that the adjustments made to Hei-
delTime significantly improved its performance
(Table 4. Particularly noteworthy is the recall with
an increase of approx. 10 percentage points. Nor-
malisation remains problematic, which is also due
to generic or event-based uses of temporal expres-
sions as well as legal references.

3.7 Legal Reference Resolution

References to other documents are another class of
expressions used in abundance in documents from
the legal domain. The considered problem con-
sists in recognizing and, ideally resolving, such
references. Usually, editors attempt to be consis-
tent and follow patterns to reference other docu-
ments. The developed methodology, currently im-
plemented as a language-agnostic prototype, fol-
lows this assumption and attempts to discover
patterns used in a semi-automatic manner. The
discovered patterns are constructed from features
that are either individual tokens (e. g., “Decision”,
“EU”, etc.) or processed features (e. g., “DIGITS”
as a placeholder for numbers). We use a seed col-
lection of documents, where references have been
manually annotated and resolved. For each refer-
ence we collect the tokens preceding the reference
and analyse the features present in these tokens.
Next we aggregate the most common combina-
tions of features – these form “patterns”. Example
of a pattern could be {“EU”, “Decision”, “DIG-
ITS/DIGITS”} or {“the”, “data”, “subject”}. The
second pattern is an example of a common combi-
nations of tokens in text and does not necessarily
indicate a reference. To filter out such irrelevant
patterns from the seed documents we extract the
strings containing the candidate pattern, but not
containing a reference. If several such strings are
found the pattern is discarded. In the next step the
most common undiscarded patterns are presented
to the user who can accept several patterns that are
later used to discover new references, enabling the
recursive improvement of patterns.

3.8 Text Structure Recognition

Knowing the structure of a document can dras-
tically improve the performance of the analysis
services applied to the text, as specialised fine-
grained models and focused approaches can be in-
tegrated. In the legal domain, it is important to
determine the structure of a document to iden-
tify sections, subsections, paragraphs, etc. cor-



60

rectly because many legal references also contain
this type of information, ideally enabling automat-
ically linking to the correct part of the text, in-
stead of the whole document. Robust text struc-
ture recognition is still an open research question.
Many approaches have been suggested in different
fields, such as Optical Layout Recognition (OLR)
for unstructured documents or markup-based ap-
proaches for structured documents. We try to
cover both in our prototype.

Unstructured documents do not contain any
structure information whatsoever, they are often
provided in plain text. To process (plain) text, we
start by applying a pattern based approach (regular
expressions) that allows the identification of the ti-
tle, headings and running text or paragraphs. Af-
ter that, we apply topic detection to all those parts
(both titles and running texts) in order to cluster
sections with related topics.

Structured documents include structural infor-
mation (e. g., markup). We consider two ways of
analysing them: (1) defining a mapping between
the elements important and relevant for the use
cases addressed by our platform and the structural
elements of the documents; and (2) extracting the
plain text from the document and then applying
the techniques for unstructured documents.

3.9 Text Summarisation

To enable our users to work with legal documents
more efficiently, we experiment with summarisa-
tion services (Allahyari et al., 2017). While ex-
tractive summarisation has been popular in the
past, the progress in neural technologies has re-
newed the interest in abstractive summarisation,
i. e., generating new sentences that capture a doc-
ument’s meaning. This approach requires highly
complex models and a lot of training data. In
the absence of labeled training data, extractive
methods are often used as the basis for abstrac-
tive methods, by assigning relevance scores to sen-
tences in an unsupervised way. Abstractive sum-
marisation is often augmented using word embed-
dings (Mikolov et al., 2013; Pennington et al.,
2014) that provide a shared semantic space for
those strongly related sentences that do no share
the same but similar or related words. We de-
velop two methods. The first tool is based on
TF-IDF (Neto et al., 2000). This is a popular
baseline as it is easy to implement, unsupervised,
and language independent. Instead of using bag-

of-words sentence representations, our approach
tries to improve on this, by analysing the texts
first. Searching the embedding space of all words
used in the text, we cluster similar words so that
morphological variants of a word like “tree” and
“trees” or “eat” and “eating”, but also synonyms
like “fast” and “rapid” are considered as belong-
ing to the same cluster. Based on these group-
ings we encode all documents and then calcu-
late the weights for the sentences using TF-IDF.
The second tool is based on the concept of cen-
troids (Rossiello et al., 2017; Ghalandari, 2017)
and benefits from the composability of word em-
beddings. Initially, keywords and concepts are ex-
tracted from the document. By composing their
embeddings, the centroid is created, which repre-
sents the document’s condensed meaningful infor-
mation. It is then projected into the embedding
space together with all sentence embeddings. Sen-
tences receive relevance scores depending on their
distance to the centroid in the embedding space.
To avoid redundancy in the summary, sentences
that are too similar to the ones already added to
the summary are not used. Both tools can be used
for multiple languages, single and multi-document
summarisation. The current version of the cen-
troid text summarisation is a working prototype. It
already provides extractive summaries for single
and multiple documents, but is still being tested
and optimised. The service is only available for
English but can be adapted to other languages by
training new embeddings.

3.10 Machine Translation

To enable multilingualism and cross-lingual ex-
traction, linking and search, we use the Machine
Translation (MT) service Tilde MT11. In order to
populate and process the Legal Knowledge Graph
in a multilingual way, custom Neural Machine
Translation (NMT) systems were trained for se-
lected language pairs – English ↔ Spanish, En-
glish ↔ German, and English ↔ Dutch. In-
domain business case specific legal data was gath-
ered and processed prior to training the NMT sys-
tems on a mix of broad-domain and in-domain
data to be able to translate both in-domain and
out-of-domain texts. Marian was used for training
(Junczys-Dowmunt et al., 2018). The translation
service provides support for a runtime scenario as
well as for asynchronous processes, i. e., support-

11https://tilde.com/mt

https://tilde.com/mt


61

Table 5: Evaluation results of NMT systems

Language pair Sentence pairs BLEUto EN from EN
English ↔ Dutch 41,639,299 43.54 34.12
English ↔ Spanish 81,176,632 32.52 38.36
English ↔ German 24,768,821 38.73 44.73

ing background data curation processes. The syn-
chronous translation service endpoint serves trans-
lation functionality for texts and documents anno-
tated with the Natural Language Processing Inter-
change Format ontology (NIF) (Hellmann et al.,
2013). The systems were automatically evaluated
using BLEU (Papineni et al., 2002) on held-out
evaluation sets. The sets were created from the in-
domain parts of the parallel corpora used for train-
ing of the NMT systems. Table 5 contains statis-
tics of the training data and the automatic evalua-
tion results of the NMT systems.

3.11 Legal Knowledge Graph Population
For the definition of our Knowledge Graph, we
benefit from predefined vocabularies such as EU-
ROVOC. However, their knowledge is limited to
that intended by their creators, and their level of
specificity and focus will, in general, not match
the ones required for an application. One possible
option of extending existing knowledge resources
is the large-scale analysis of documents in which
entities contained in the knowledge resources have
been identified, and to identify as well as to extract
new relations, claims, or facts, explicitly men-
tioned in the documents. This approach mimics
the process in which a human reads and under-
stands documents. In our project we follow distant
supervision (Ren et al., 2017). It takes a text cor-
pus as input and 1) identifies sentences containing
entity pairs for which relations are known, 2) uses
machine learning to derive a statistical classifier
to recognize these examples, and 3) applies this
classifier to sentences that, by virtue of the classes
of entities they contain, could also include an in-
stance of a relation. The result is a list of sentences
which have been annotated as containing a given
relation. The relations included so far in this first
experimental deployment, are of the type person
is located in location and location is contained in
location. They will be expanded with domain spe-
cific relations such as activity requires permit or
permit was issued on date. So far, such relations
can be recognized in English language texts, but
training for German, Spanish and Dutch, using the

same distant supervision approach is possible due
to the multilingual nature of general purpose cor-
pora and knowledge graphs (e. g., DBPedia).

3.12 Semantic Similarity
Using the services mentioned above, documents
in the LKG are annotated to semantically describe
their content and provenance. This added extra
knowledge is useful for several applications, such
as search, question answering, classification and
recommendations, all of which rely on a notion
of document similarity. Many such notions exist,
and they are usually encoded in a function s that
assigns, to every pair of documents, a number be-
tween 0 and 1, with 1 denoting the documents be-
ing identical (Gomaa and Fahmy, 2013). We use
a hybrid type of similarity measure. First, the text
entailed by the document, such as the resolution of
temporal or geographical references, is performed.
Second, similarity itself is computed using a linear
combination of text-based and knowledge-based
similarities. The former are encoded by cosine-
similarity of TF-IDF vectors (of the documents
or their translations), and the latter by the over-
lap (as measured by Jaccard coefficient of entities
that the two documents either mention directly, or
are linked in the LKG to mentioned ones. The
overlaps are weighted depending on how far away
in the LKG the entities mentioned in the docu-
ment are, and the weight coefficients are deter-
mined, along with the coefficients of the linear
combination, by training a linear regression classi-
fier (Bär et al., 2012). This approach allows us to
detect similarity between documents even if they
have only few entities in common, by considering
the knowledge about these entities. Additionally,
by comparing mentions of entities instead of their
surface forms, the multilingual nature of the LKG
is exploited. The knowledge-based component of
the similarity computation is language agnostic,
while the text-based depends only on basic NLP
tools (e. g., stemming, stop-word removal) which
are available for English, German, Spanish and
Dutch, among others. In order to compare doc-
uments in two different languages, machine trans-
lation between them, or to a third language must
be available. The semantic similarity service is a
prototype, requiring further testing and refining.

3.13 Question Answering
The Question Answering (QA) service accepts a
natural language question and responds with an



62

answer, extracted from a document in a given cor-
pus. The end-to-end system consists of three com-
ponents: 1) The Query Formulation module trans-
forms a question into a query, which can be ex-
panded using a domain specific vocabulary from
the LKG. The query is then processed through an
indexer to obtain matching documents from the
corresponding corpora. 2) The Answer Genera-
tion module extracts potential answers from the
retrieved documents from the LKG. 3) The An-
swer Selection module identifies the best answer
based on various criteria such as local structure of
the text and global interaction between each pair of
words based on specific layers of the model. The
QA service is the central component of two of our
use cases. In these, a user asks a natural language
question along with additional information such as
an appropriate jurisdiction. The question is meant
to trigger a query on a set of documents related to
the specific jurisdiction. These additional param-
eters influence the search in the Answer Selection
module by determining which subset of the docu-
ments should be used. With the help of the Ma-
chine Translation service, the QA service is able
to return answers in languages different from the
documents’ language. The benefit from the ser-
vice would be the time reduction in search for a
relevant article in the legislation. For a question
such as “How long is paternity leave?”, the sys-
tem returns relevant paragraphs and articles from
the Labor Law that can be further processed by the
lawyer. The QA service works for English and can
be retrained and adopted to other languages.

4 Orchestration of Individual Services

Our technology platform is based on microser-
vices, which provide multiple advantages, espe-
cially in a collaborative project with a distributed
set of partners from academia and industry, includ-
ing use case partners. Microservices are small and
autonomous and can be developed more efficiently
than a monolithic, integrated system. In addition,
the development and deployment of microservices
can, to a very large extent, be automated, also fa-
cilitating the monitoring of individual services. A
crucial advantage is concerned with the scalability
of systems based on microservices, which is a lot
easier than scaling monolithic systems. The com-
munication between different services is executed
over HTTP, the interfaces are documented using
the OpenAPI specification. For the deployment

of the containerised microservices we use Open-
Shift; alternative technologies such as, among oth-
ers, Kubernetes, could also be used.

In our project we conceptualise the specific re-
quirements of the different use cases as content
curation workflows (Schneider and Rehm, 2018b;
Bourgonje et al., 2016a,b; Rehm et al., 2018).
Workflows are defined as the execution of spe-
cific services to perform the processing of one or
more documents under the umbrella of a certain
task or use case. The specification of a workflow
includes its input and output as well as the func-
tionality it is supposed to perform: annotate or en-
rich a document, add a document to the knowl-
edge base, search for information, etc. The project
offers compliance-related features and functional-
ities through common services and data sets in-
cluded in the LKG. Workflows make use of these
services to implement the required functionality.
The content curation workflows for the different
use cases that we prototypically implement in the
project have been defined as follows. We per-
formed a systematic analysis of the microservices,
developed in parallel, and matched them with the
required functionalities for each use case. First,
we determine the principal elements involved in
each use case, i. e., the services, input and output.
Second, we define the order in which the services
have to be executed. Third, we identify the shared
components in the different workflows. Currently
we have defined five different workflows. Two are
defined for the acquisition and population of the
LKG with new information, the other three are de-
fined to address the requirements of each use case
(see Figures 1 to 3 in the appendix). We currently
work with two alternative workflow management
implementations that take care of the orchestra-
tion of services. The first one is based on Ca-
munda BPM,12 including a logic layer that man-
ages the various processes (including NIF anno-
tations). The second approach is based on Rab-
bitMQ,13 an open-source message broker, on top
of which we developed our own solution for paral-
lelising processing steps and improving the perfor-
mance of the overall orchestration. In both cases,
the main concept of the service orchestration is fo-
cused on the use of queuing systems, so that most
of the processes could be executed in parallel and
either synchronous or asynchronously.

12https://camunda.com
13https://www.rabbitmq.com

https://camunda.com
https://www.rabbitmq.com


63

5 Related Work

There are several systems, platforms and ap-
proaches that are related to the technology plat-
form, which is under development in the project
LYNX. In the wider area of legal document pro-
cessing, technologies from several fields are rele-
vant, among others, knowledge technologies, cita-
tion analysis, argument mining, reasoning and in-
formation retrieval. A literature overview can be
found in (Schneider and Rehm, 2018a) and (Ag-
noloni and Venturi, 2018).

Commercial Systems and Services – The Lex-
isNexis14 system is the market leader in the legal
domain; it offers services, such as legal research,
practical guidance, company research and media-
monitoring as well as compliance and due dili-
gence. WestLaw is an online service that allows
legal professionals to find and consult relevant le-
gal information.15 One of its goals is to enable
professionals to put together a strong argument.
There are also smaller companies that offer legal
research solutions and analytic environments, such
as RavelLax,16, or Lereto17. A commercial search
engine for legal documents, iSearch, is a service
offered by LegitQuest.18 The Casetext CARA Re-
search Suite allows uploading a brief and then re-
trieving, based on its contents, useful case law.19

There is also a growing number of startup compa-
nies active in the legal domain.

Research Prototypes – Most of the documented
research prototypes were developed in the 1990s
under the umbrella of Computer Assisted Legal
Research (CALR) (Span, 1994). In the follow-
ing we briefly review several of these systems,
which usually focus on one very specific feature
or functionality. One example is the open source
software for the analysis and visualisation of net-
works of Dutch case law (van Kuppevelt and van
Dijck, 2017). This technology determines rele-
vant precedents (analysing the citation network of
case law), compares them with those identified in
the literature, and determines clusters of related
cases. A similar prototype is described by (Ag-
noloni et al., 2017). (Gifford, 2017) propose a
search engine for legal documents where argu-

14https://www.lexisnexis.com
15http://legalsolutions.thomsonreuters.

com/law-products/westlaw-legal-research/
16http://ravellaw.com
17https://www.lereto.at
18https://www.legitquest.com
19https://casetext.com

ments are extracted from appellate cases and are
accessible either through selecting nodes in a lit-
igation issue ontology or through relational key-
word search. Lucem (Bhullar et al., 2016) is a sys-
tem that tries to mirror the way lawyers approach
legal research, developing visualisations that pro-
vide lawyers with an additional tool to approach
their research results. Eunomos is a prototype that
semi-automates the construction and analysis of
knowledge (Boella et al., 2012).

6 Summary and Future Work

This article presents the technology platform cur-
rently under development in the project LYNX, fo-
cusing upon processing services. These serve two
main purposes: 1) to extract semantic information
from a large and heterogeneous set of documents
to ingest the extracted information into the Legal
Knowledge Graph; 2) to extract semantic infor-
mation from documents that users of the platform
work with. In addition to the semantic extrac-
tion of information and knowledge, we provide
services for the processing and curation of whole
documents (summarisation, translation) with the
goal of mapping extracted terms and concepts to
the LKG, and services that aim at accessing the
LKG (question answering). We currently exper-
iment with two different curation workflow man-
agers to make specific sets of services available
for specific use cases. Future work includes com-
pleting development work on the services, adapt-
ing the services to all languages required in the
project’s use cases, implementing the prototype
applications and developing the freely accessible
web interface of the platform.

Acknowledgments

This work has been partially funded by the
project LYNX, which has received funding from
the European Union’s Horizon 2020 research
and innovation programme under grant agreement
no. 780602. For more information please see
http://www.lynx-project.eu.

References
Tommaso Agnoloni, Lorenzo Bacci, Ginevra Perug-

inelli, Marc van Opijnen, Jos van den Oever,
Monica Palmirani, Luca Cervone, Octavian Bujor,
Arantxa Arsuaga Lecuona, Alberto Boada Garcı́a,
Luigi Di Caro, and Giovanni Siragusa. 2017. Link-
ing european case law: BO-ECLI parser, an open

https://www.lexisnexis.com
http://legalsolutions.thomsonreuters.com/law-products/westlaw-legal-research/
http://legalsolutions.thomsonreuters.com/law-products/westlaw-legal-research/
http://ravellaw.com
https://www.lereto.at
https://www.legitquest.com
https://casetext.com
http://www.lynx-project.eu
https://doi.org/10.3233/978-1-61499-838-9-113
https://doi.org/10.3233/978-1-61499-838-9-113


64

framework for the automatic extraction of legal
links. In (Wyner and Casini, 2017), pages 113–118.

Tommaso Agnoloni and Giulia Venturi. 2018. Seman-
tic processing of legal texts. In Jacqueline Visconti,
editor, Handbook of Communication in the Legal
Sphere, pages 109–138. De Gruyter, Berlin, Boston.

Mehdi Allahyari, Seyedamin Pouriyeh, Mehdi Assefi,
Saeid Safaei, Elizabeth D Trippe, Juan B Gutier-
rez, and Krys Kochut. 2017. Text Summariza-
tion Techniques: A brief Survey. arXiv preprint
arXiv:1707.02268.

Daniel Bär, Chris Biemann, Iryna Gurevych, and
Torsten Zesch. 2012. Ukp: Computing seman-
tic textual similarity by combining multiple con-
tent similarity measures. In Proceedings of the
First Joint Conference on Lexical and Computa-
tional Semantics-Volume 1: Proceedings of the main
conference and the shared task, and Volume 2: Pro-
ceedings of the Sixth International Workshop on Se-
mantic Evaluation, pages 435–440. Association for
Computational Linguistics.

Darina Benikova, Chris Biemann, Max Kisselew, and
Sebastian Pad. 2014. GermEval 2014 Named En-
tity Recognition Shared Task: Companion Paper. In
Proceedings of the KONVENS GermEval workshop,
pages 104–112, Hildesheim, Germany.

Darina Benikova, Seid Muhie Yimam, Prabhakaran
Santhanam, and Chris Biemann. 2015. Germaner:
Free open german named entity recognition tool.
In Proceedings of the International Conference of
the German Society for Computational Linguistics
and Language Technology, GSCL 2015, University
of Duisburg-Essen, Germany, 30th September - 2nd
October 2015, pages 31–38.

Jagjoth Bhullar, Nathan Lam, Kenneth Pham, Adithya
Prabhakaran, and Albert Joseph Santillano. 2016.
Lucem: A Legal Research Tool, 63. Computer En-
gineering Senior Theses.

G. Boella, L. di Caro, L. Humphreys, L. Robaldo, and
L. van der Torre. 2012. Nlp challenges for eunomos,
a tool to build and manage legal knowledge.

Peter Bourgonje, Julian Moreno-Schneider, Jan
Nehring, Georg Rehm, Felix Sasaki, and Ankit
Srivastava. 2016a. Towards a Platform for Curation
Technologies: Enriching Text Collections with
a Semantic-Web Layer. In The Semantic Web,
number 9989 in Lecture Notes in Computer Sci-
ence, pages 65–68. Springer. ESWC 2016 Satellite
Events. Heraklion, Crete, Greece, May 29 – June 2,
2016 Revised Selected Papers.

Peter Bourgonje, Julián Moreno Schneider, Georg
Rehm, and Felix Sasaki. 2016b. Processing Doc-
ument Collections to Automatically Extract Linked
Data: Semantic Storytelling Technologies for Smart
Curation Workflows. In Proceedings of the 2nd In-
ternational Workshop on Natural Language Gener-
ation and the Semantic Web (WebNLG 2016), pages

13–16, Edinburgh, UK. The Association for Com-
putational Linguistics.

Manaal Faruqui and Sebastian Padó. 2010. Train-
ing and evaluating a german named entity recog-
nizer with semantic generalization. In Semantic
Approaches in Natural Language Processing: Pro-
ceedings of the 10th Conference on Natural Lan-
guage Processing, KONVENS 2010, September 6-8,
2010, Saarland University, Saarbrücken, Germany,
pages 129–133. universaar, Universitätsverlag des
Saarlandes / Saarland University Press / Presses uni-
versitaires de la Sarre.

Jenny Rose Finkel, Trond Grenager, and Christo-
pher D. Manning. 2005. Incorporating non-local
information into information extraction systems by
gibbs sampling. In ACL 2005, 43rd Annual Meeting
of the Association for Computational Linguistics,
Proceedings of the Conference, 25-30 June 2005,
University of Michigan, USA, pages 363–370. The
Association for Computer Linguistics.

Demian Gholipour Ghalandari. 2017. Revisiting the
Centroid-based Method: A Strong Baseline for
Multi-Document Summarization. arXiv preprint
arXiv:1708.07690.

Matthew Gifford. 2017. Lexridelaw: an argument
based legal search engine. In ICAIL ’17.

Wael H Gomaa and Aly A Fahmy. 2013. A survey of
text similarity approaches. International Journal of
Computer Applications, 68(13):13–18.

Sebastian Hellmann, Jens Lehmann, Sören Auer, and
Martin Brümmer. 2013. Integrating NLP using
Linked Data. In 12th International Semantic Web
Conference. 21-25 October.

Zhiheng Huang, Wei Xu, and Kai Yu. 2015. Bidi-
rectional LSTM-CRF models for sequence tagging.
CoRR, abs/1508.01991.

Marcin Junczys-Dowmunt, Roman Grundkiewicz,
Tomasz Dwojak, Hieu Hoang, Kenneth Heafield,
Tom Neckermann, Frank Seide, Ulrich Germann,
Alham Fikri Aji, Nikolay Bogoychev, André F T
Martins, and Alexandra Birch. 2018. Marian: Fast
Neural Machine Translation in C++. arXiv preprint
arXiv:1804.00344.

Dafne van Kuppevelt and Gijs van Dijck. 2017. An-
swering legal research questions about dutch case
law with network analysis and visualization. In
(Wyner and Casini, 2017), pages 95–100.

Guillaume Lample, Miguel Ballesteros, Sandeep Sub-
ramanian, Kazuya Kawakami, and Chris Dyer. 2016.
Neural architectures for named entity recognition.
In NAACL HLT 2016, The 2016 Conference of
the North American Chapter of the Association for
Computational Linguistics: Human Language Tech-
nologies, San Diego California, USA, June 12-17,
2016, pages 260–270.

https://doi.org/10.3233/978-1-61499-838-9-113
https://doi.org/10.3233/978-1-61499-838-9-113
https://docs.google.com/viewer?a=v&pid=sites&srcid=ZGVmYXVsdGRvbWFpbnxnZXJtZXZhbDIwMTRuZXJ8Z3g6MTA0MDkzMjU5MWQ1NGJlYg
https://docs.google.com/viewer?a=v&pid=sites&srcid=ZGVmYXVsdGRvbWFpbnxnZXJtZXZhbDIwMTRuZXJ8Z3g6MTA0MDkzMjU5MWQ1NGJlYg
http://citeseerx.ist.psu.edu/viewdoc/download;jsessionid=82C2DE93DF07239D5BAC8ED8F180539D?doi=10.1.1.720.5611&rep=rep1&type=pdf
http://citeseerx.ist.psu.edu/viewdoc/download;jsessionid=82C2DE93DF07239D5BAC8ED8F180539D?doi=10.1.1.720.5611&rep=rep1&type=pdf
https://scholarcommons.scu.edu/cseng_senior/63
http://www.lrec-conf.org/proceedings/lrec2012/pdf/1035_Paper.pdf
http://www.lrec-conf.org/proceedings/lrec2012/pdf/1035_Paper.pdf
https://s3.amazonaws.com/academia.edu.documents/25675452/konvens_2010.pdf?AWSAccessKeyId=AKIAIWOWYYGZ2Y53UL3A&Expires=1550673493&Signature=a%2B3MSRh8sB99hfpY9FZSQXW1MIM%3D&response-content-disposition=inline%3B%20filename%3DTransliteration_among_Indian_Languages_u.pdf#page=130
https://s3.amazonaws.com/academia.edu.documents/25675452/konvens_2010.pdf?AWSAccessKeyId=AKIAIWOWYYGZ2Y53UL3A&Expires=1550673493&Signature=a%2B3MSRh8sB99hfpY9FZSQXW1MIM%3D&response-content-disposition=inline%3B%20filename%3DTransliteration_among_Indian_Languages_u.pdf#page=130
https://s3.amazonaws.com/academia.edu.documents/25675452/konvens_2010.pdf?AWSAccessKeyId=AKIAIWOWYYGZ2Y53UL3A&Expires=1550673493&Signature=a%2B3MSRh8sB99hfpY9FZSQXW1MIM%3D&response-content-disposition=inline%3B%20filename%3DTransliteration_among_Indian_Languages_u.pdf#page=130
http://aclweb.org/anthology/P/P05/P05-1045.pdf
http://aclweb.org/anthology/P/P05/P05-1045.pdf
http://aclweb.org/anthology/P/P05/P05-1045.pdf
http://arxiv.org/abs/1508.01991
http://arxiv.org/abs/1508.01991
https://arxiv.org/abs/1804.00344
https://arxiv.org/abs/1804.00344
https://doi.org/10.3233/978-1-61499-838-9-95
https://doi.org/10.3233/978-1-61499-838-9-95
https://doi.org/10.3233/978-1-61499-838-9-95
https://arxiv.org/pdf/1603.01360.pdf


65

Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Cor-
rado, and Jeff Dean. 2013. Distributed Representa-
tions of Words and Phrases and their Composition-
ality. In Advances in neural information processing
systems, pages 3111–3119.

Joel Larocca Neto, Alexandre D Santos, Celso AA
Kaestner, Neto Alexandre, D Santos, et al. 2000.
Document Clustering and Text Summarization.

Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: a Method for Automatic
Evaluation of Machine Translation. In Proceedings
of the 40th annual meeting on association for com-
putational linguistics, pages 311–318. Association
for Computational Linguistics.

Jeffrey Pennington, Richard Socher, and Christopher
Manning. 2014. Glove: Global Vectors for Word
Representation. In Proceedings of the 2014 confer-
ence on empirical methods in natural language pro-
cessing (EMNLP), pages 1532–1543.

Mārcis Pinnis, Nikola Ljubešić, Dan Ştefănescu, In-
guna Skadiņa, Marko Tadić, and Tatiana Gornos-
tay. 2012. Term Extraction, Tagging, and Mapping
Tools for Under-Resourced Languages. In Proceed-
ings of the 10th Conference on Terminology and
Knowledge Engineering (TKE 2012), June 2012,
pages 193–208, Madrid, Spain.

Georg Rehm, Julián Moreno Schneider, Peter Bour-
gonje, Ankit Srivastava, Rolf Fricke, Jan Thom-
sen, Jing He, Joachim Quantz, Armin Berger, Luca
König, Sören Räuchle, Jens Gerth, and David Wab-
nitz. 2018. Different Types of Automated and Semi-
Automated Semantic Storytelling: Curation Tech-
nologies for Different Sectors. In Language Tech-
nologies for the Challenges of the Digital Age: 27th
International Conference, GSCL 2017, Berlin, Ger-
many, September 13-14, 2017, Proceedings, num-
ber 10713 in Lecture Notes in Artificial Intelli-
gence (LNAI), pages 232–247, Cham, Switzerland.
Gesellschaft für Sprachtechnologie und Computer-
linguistik e.V., Springer. 13/14 September 2017.

Nils Reimers and Iryna Gurevych. 2017. Reporting
score distributions makes a difference: Performance
study of lstm-networks for sequence tagging. In
Proceedings of the 2017 Conference on Empirical
Methods in Natural Language Processing, pages
338–348. Association for Computational Linguis-
tics.

Xiang Ren, Zeqiu Wu, Wenqi He, Meng Qu, Clare R
Voss, Heng Ji, Tarek F Abdelzaher, and Jiawei Han.
2017. Cotype: Joint extraction of typed entities and
relations with knowledge bases. In Proceedings of
the 26th International Conference on World Wide
Web, pages 1015–1024. International World Wide
Web Conferences Steering Committee.

Artem Revenko and Vı́ctor Mireles. 2017. Discrimina-
tion of word senses with hypernyms. In Proceed-
ings of the 5th International Workshop on Linked

Data for Information Extraction co-located with the
16th International Semantic Web Conference (ISWC
2017), Vienna, Austria, October 22, 2017., volume
1946 of CEUR Workshop Proceedings, pages 50–61.
CEUR-WS.org.

Martin Riedl and Sebastian Padó. 2018. A named en-
tity recognition shootout for german. In Proceed-
ings of the 56th Annual Meeting of the Association
for Computational Linguistics (Volume 2: Short Pa-
pers), volume 2, pages 120–125. Association for
Computational Linguistics.

Gaetano Rossiello, Pierpaolo Basile, and Giovanni Se-
meraro. 2017. Centroid-based Text Summarization
through Compositionality of Word Embeddings. In
Proceedings of the MultiLing 2017 Workshop on
Summarization and Summary Evaluation Across
Source Types and Genres, pages 12–21.

Julian Moreno Schneider and Georg Rehm. 2018a. Cu-
ration Technologies for the Construction and Utilisa-
tion of Legal Knowledge Graphs. In Proceedings of
the LREC 2018 Workshop on Language Resources
and Technologies for the Legal Knowledge Graph,
pages 23–29, Miyazaki, Japan. 12 May 2018.

Julian Moreno Schneider and Georg Rehm. 2018b. To-
wards a Workflow Manager for Curation Technolo-
gies in the Legal Domain. In Proceedings of the
LREC 2018 Workshop on Language Resources and
Technologies for the Legal Knowledge Graph, pages
30–35, Miyazaki, Japan. 12 May 2018.

Georges Span. 1994. Lites: An intelligent tutoring sys-
tem shell for legal education. International Review
of Law, Computers & Technology, 8(1):103–113.

Karen Spärck Jones. 1972. A Statistical Interpretation
of Term Specificity and Its Application in Retrieval.
Journal of Documentation, 28:11—-21.

Jannik Strötgen and Michael Gertz. 2010. HeidelTime:
High Quality Rule-based Extraction and Normaliza-
tion of Temporal Expressions. In Proceedings of
the 5th International Workshop on Semantic Evalua-
tion, SemEval ’10, pages 321–324, Stroudsburg, PA,
USA. Association for Computational Linguistics.

Adam Z. Wyner and Giovanni Casini, editors. 2017.
Legal Knowledge and Information Systems - JURIX
2017: The Thirtieth Annual Conference, Luxem-
bourg, 13-15 December 2017, volume 302 of Fron-
tiers in Artificial Intelligence and Applications. IOS
Press.

https://doi.org/10.18653/v1/D17-1035
https://doi.org/10.18653/v1/D17-1035
https://doi.org/10.18653/v1/D17-1035
http://ceur-ws.org/Vol-1946/paper-06.pdf
http://ceur-ws.org/Vol-1946/paper-06.pdf
http://www.aclweb.org/anthology/P18-2020
http://www.aclweb.org/anthology/P18-2020
https://doi.org/10.1080/13600869.1994.9966381
https://doi.org/10.1080/13600869.1994.9966381
http://dl.acm.org/citation.cfm?id=1859664.1859735
http://dl.acm.org/citation.cfm?id=1859664.1859735
http://dl.acm.org/citation.cfm?id=1859664.1859735


66

A Appendix: Examples of Use-Case-specific Curation Workflows

The following three figures are additional material with regard to Section 4. They provide illustrative
examples of use-case-specific processing service and curation workflows.

COMMON WORKFLOWS

Task Orchestration

Authentication

Conversion
Document 

Management

NIF PDF / HTML

NER

TIMEX

GEO

EntEx

ToClass

WSD

RelEx

TransX +

Input

LKG

IX

Split

Individual
Paragraph

Table of contents of document (list of paragraphs)

Figure 1: Legal Knowledge Graph population workflowSCENARIO 1:+CONTRACT ANALYSIS
Task Orchestration

Authentication

Conversion
Conversion

NIF PDF

NER

TIMEX

GEO

EntEx

ToClass

WSD

RelEx

Search

Summ

Trans

IX

X +

Output

Input

Split

X

SemSim

+

JSON HTML/TXT/JSON
LKG

Figure 2: Workflow of the Contract Analysis use case

SCENARIO 2:+LABOUR LAW

Task OrchestrationAuthentication

Conversion
Document 

Management
NIF Questions

SummQaDocInput

Output

JSON

Figure 3: Workflow of the Labour Law use case


