



















































Phone Merging For Code-Switched Speech Recognition


Proceedings of The Third Workshop on Computational Approaches to Code-Switching, pages 11–19
Melbourne, Australia, July 19, 2018. c©2018 Association for Computational Linguistics

11

Phone Merging for Code-switched Speech Recognition

Sunit Sivasankaran ∗
Université de Lorraine, CNRS,

Inria, LORIA,
F-54000 Nancy, France

sunit.sivasankaran@inria.fr

Brij Mohan Lal Srivastava, Sunayana Sitaram,
Kalika Bali, Monojit Choudhury

Microsoft Research - Bangalore, India
{ v-bmlals,susitara,kalikab,

monojitc}@microsoft.com

Abstract

Speakers in multilingual communities of-
ten switch between or mix multiple lan-
guages in the same conversation. Auto-
matic Speech Recognition (ASR) of code-
switched speech faces many challenges in-
cluding the influence of phones of differ-
ent languages on each other. This pa-
per shows evidence that phone sharing
between languages improves the Acous-
tic Model performance for Hindi-English
code-switched speech. We compare base-
line system built with separate phones for
Hindi and English with systems where the
phones were manually merged based on
linguistic knowledge. Encouraged by the
improved ASR performance after manu-
ally merging the phones, we further in-
vestigate multiple data-driven methods to
identify phones to be merged across the
languages. We show detailed analysis of
automatic phone merging in this language
pair and the impact it has on individual
phone accuracies and WER. Though the
best performance gain of 1.2% WER was
observed with manually merged phones,
we show experimentally that the manual
phone merge is not optimal.

1 Introduction

Multilingual speakers tend to alternate between
several languages within a conversation. This phe-
nomenon is referred to as code-switching (CS).
Automatic Speech Recognition for CS speech is
challenging. Code-switched speech recognition
present challenges in acoustic, language and pro-
nunciation modeling of speech. Acoustic Mod-

∗This work was done while interning at Microsoft
Research-India

els (AMs) need to model phones in a mixed lan-
guage setting, where co-articulation effects from
one language may influence the other. Moreover,
language models needs to be capable of predict-
ing code-switch points between the two languages.
The vocabulary size may be double of what is
present in monolingual systems. Accents and na-
tive language influence may pose challenges to
pronunciation models. Another major challenge in
building code-switched ASR is the lack of data for
different language-pairs. To curb the issue of con-
textual data availability per phone, we study the
effect of manual merging and two automatic merg-
ing over the performance of Hindi-English code-
switched speech recognition system.

In systems with a small amount of data for train-
ing the AMs, phones that are similar to each other
in the two languages being mixed can be merged,
leading to more data for each phone. This may
be especially useful in the case of related lan-
guages, or when a strong native language accent is
expected to influence pronunciations in the other
language. We experiment with phone merging
in AMs of Hindi-English code-switched conversa-
tional speech, and show that we can get improve-
ments on Word Error Rate (WER) by merging cer-
tain phones.

One technique to merge phones in the two lan-
guages being mixed is to use a common phone-
set such as the International Phonetic Alphabet
(IPA), or knowledge from a bilingual speaker to
decide which phones can be merged (manual-
merge). However, this may not always find the
optimal merges, particularly if the phoneset we
are starting with is not the appropriate represen-
tation for the dialect present in the speech. An-
other technique is to automatically find candidate
merges by taking into account phone errors made
by the ASR system in presence of a monolingual
context (data-driven). Thirdly, we can create lex-



12

icons with all possible pronunciation variants cov-
ering all candidate phones and allow the decoder
to choose the correct pronunciation variant during
decoding (probabilistic). We implement and dis-
cuss these techniques for phone merging in Hindi-
English code-switched speech recognition.

The paper is organized as follows. Section 2
relates this work to prior work in code-switched
speech recognition. Section 3 describes the Hindi-
English speech data that we used. We describe our
proposed techniques and experiments with phone
merging in Section 4 and conclude the paper in
Section 5.

2 Relation to Prior Work

Code-switched speech recognition has been stud-
ied in the context of acoustic, language and pro-
nunciation modeling. The Language Identifica-
tion (LID) based approach is to identify the lan-
guage boundaries and subsequently use an ap-
propriate monolingual ASR system to recognize
monolingual fragments (Chan et al., 2004) or run
multiple recognizers in parallel with an LID sys-
tem (Ahmed and Tan, 2012; Weiner, 2012). An-
other approach is to train an AM on bilingual data
(Lyu et al., 2006; Vu et al., 2012) or to use one
of the monolingual AMs (Bhuvanagirir and Kop-
parapu, 2012) or to pool the existing monolin-
gual AMs by sharing phones belonging to both
languages. Yeh et al. (Yeh and Lee, 2015)
tackle the problem of code-switching in which a
speaker speaks mainly in one language, leading to
an imbalance in the amount of data available in
the two languages, with cross-lingual data shar-
ing approaches. (Pandey et al., 2017) also pro-
pose studies to adapt matrix language (monolin-
gual Hindi) resource to build better code-mixed
acoustic model in case of read speech.

Yilmaz et al. (Yılmaz et al., 2016) describe
two DNN architectures for recognizing Frisian-
Dutch code-switched speech. They use language
dependent phones in which each phone is tagged
with the language and modeled separately. They
also use language independent phones by model-
ing them jointly and merging them on the basis of
the associated IPA symbol. In their experiments,
they find that the language dependent approach
performs better. (Lyudovyk and Pylypenko, 2014)
describe an approach for code-switched speech
recognition of closely related languages, namely,
Ukrainian and Russian by creating a bilingual pro-

nunciation lexicon.
(Chan et al., 2009) describe a two pass approach

for Cantonese-English mixed speech recognition,
in which they develop a cross-lingual phonetic
AM, with the phone set designed based on lin-
guistic knowledge. (Yu et al., 2004) present
three approaches for bilingual phone modeling
for Mandarin-English speech, namely combining
phone inventories, use IPA mappings to construct
a bilingual phone set and clustering phones with
hierarchical clustering by using the Bhattacharyya
distance and the acoustic likelihood. The third ap-
proach outperforms the IPA-based mapping and is
comparable to the combination of the phone in-
ventories.

A closely related area of research is the Multi-
Lingual speech recognition (Toshniwal et al.,
2018; Schultz and Waibel, 1997, 2001; Lin et al.,
2009; Vu et al., 2014). Though the problems
in multi-lingual ASR and ASR for code-switched
data seem similar such as; large phonetic space
due to the incorporation of the phones of both
languages, code-switched ASR has its own set of
challenges. For example, even with a large corpus,
getting enough data at code-switched points for
both the Acoustic Model and the Language Mod-
els is very challenging.

3 Data

The dataset used in this work contains conversa-
tional speech recordings spoken in code-switched
Hindi and English. Hindi-English bilinguals were
given a topic and asked to have a conversation
about the topic with another bilingual. They were
not explicitly asked to code-switch during record-
ing, but around 40% of the data contains at least
one English word in an utterance. The conver-
sations were transcribed by bilingual transcribers.
Hindi words were transcribed in the native Hindi
script Devanagari, and English words in Roman
script. There was no distinction made between
borrowed words and code-switching, which led to
some inconsistencies in transcription. Each con-
versation was broken down into utterances rang-
ing from very short one word utterances to long
sentences.

A summary of the Hindi-English code switched
dataset used in our experiments is shown in Ta-
ble 1. The code-switching statistics mentioned are
particular to this dataset and is subject to change
depending on the speaker. However, the phone



13

Data Utts # ofSpkrs Hrs
Total
Words En (%)

Unique
Words

En
(%)

Train 41276 429 46 560893 16.6 18900 40.23
Test 5193 53 5.6 69177 16.5 6000 41.01
Dev 4689 53 5.7 68995 16.05 6051 40.04

Table 1: Hindi-English code switched data

merging experiments described in this paper is still
relevant due to the performance gains of the acous-
tic model.

4 Phone merging

In this section, we first describe the baseline ASR
system built by combining the Hindi and English
phonesets, followed by experiments conducted on
phone merging and the resulting impact on Word
Error Rate (WER). All experiments were carried
out using Kaldi (Povey et al., 2011) and the phone
merging techniques are implemented in Python.

4.1 Baseline ASR System

The CMU Pronunciation dictionary (Weide, 1998)
containing 39 phones was used as the lexicon for
all the English words in the data. This is not the
most appropriate choice given that all the speak-
ers in the data speak Indian English, however, due
to the lack of a publicly available large Indian En-
glish lexicon, we used CMUdict. To generate the
lexicon for the Hindi words, we used the Festvox
Indic front end (Parlikar et al.), which does ba-
sic grapheme to phone conversion for Hindi, in-
cluding schwa deletion and contextual nasaliza-
tion. The Hindi phoneset consisted of 50 phones
after removing some very low frequency phones.

We used Feature-space maximum likelihood
linear regression (fMLLR) (Gales, 1998) to train
a context-dependent GMM-HMM acoustic model
and a trigram Language Model (LM) built on the
transcripts during decoding. With this system, we
obtained a baseline Word Error Rate (WER) of
40.36%.

We evaluated the accuracies at phone level by
comparing the transition arcs that correspond to
phones in the decoded lattices with the alignments
obtained from the GMM-HMM model as ground
truth. Figure 1 shows a scatter plot of the phone
accuracy with respect to the log of the data avail-
able per phone in the test dataset. Evidently, we
observe higher accuracies for phones with larger

0.2 0.4 0.6 0.8

2

3

4

5

Accuracy

lo
g

(c
ou

nt
)

Figure 1: Scatter plot of log of data count per
phone with respect to the phone accuracy on the
test set.

count with a few exceptions such as, /nN HP/, /zh/
and /dR HP/.

4.2 Manually merging similar sounding
phones

To increase the data availability per phone, we
merged similar sounds in both languages even if
they are not exactly the same linguistically (in
terms of their articulation). The mapping between
Hindi phones and CMUdict phones in the Festvox
Indic frontend, built for enabling a Hindi TTS
system to pronounce English words (Sitaram and
Black, 2016), was used for this purpose. All the
merged phones (a total of 31) were prefixed with
“-HP-M”.

To analyze the impact of merging, we started by
merging a pair of phones - the English phone eh
(example “academic ae k ah d eh m ih k”) with
a similar sounding Hindi phone E-HP (example
in Roman script: “kehana k-HP E-HP hv-HP nB-
HP Aa-HP”). This resulted in 38 English phones,
49 Hindi phones and 1 merged phone resulting
in 88 unique phones. We obtained a WER of
40.21 using a GMM-HMM acoustic model, which



14

is a negligible improvement over the system with
no merging. The bar plot in Figure 2 shows the
change in the accuracies for each phones with re-
spect to the baseline. We notice an improvement
of 4.18% and 4.5% in the accuracy of the ‘eh’ and
‘E-HP’ phones respectively. Similar performance
was obtained while merging the English phone n
with the Hindi phone nB-HP. In both cases, a de-
crease in accuracies for a few phones were ob-
served.

Then, we merged all the 31 pairs of similar
sounding English and Hindi phones. We refer to
this system as the “all merged” system, for which
we obtained a WER of 39.7%. There was a no-
ticeable improvement in the accuracies of about
75% of the phones as shown in Figure 3 . We ob-
served a decrease in accuracies for phones which
do not have similar sounding equivalents to merge,
such as the Hindi phones ‘sr-HP’ and ‘nN-HP’ and
the English phone ‘ng’. Large improvements in
the phone accuracies, amounting to around 50%,
were observed in merged phones such as ‘ow-HP’
(merged with the English phone ‘ao’) and ‘tr-HP’
(merged with the English phone ‘t’). Conspicu-
ously, the highest improvements were for phones
with low count prior to merging.

4.3 Measuring phone accuracy changes
To evaluate the performance of these systems
in terms of phone accuracy, we computed the
weighted average of change in accuracies. The
weights correspond to the data available per
phone. We measure Weighted Average of
Phoneme Improvement (henceforth referred to as
WAPI) as:

WAPI =

∑
iwi ×∆acci∑

iwi
,

where wi and ∆acci represent the data available
per phone and change in phone accuracy with re-
spect to the baseline respectively. A summary of
the WAPI score for different acoustic models is
shown in Table 2. The “all merged” system gives
the highest WAPI score of 2.45%, whereas merg-
ing ‘eh’ with ‘E-HP’ and ‘n’ with ‘nB-HP’ results
in a WAPI score of 0.31 and 0.47 respectively. In-
terestingly merging both ‘eh’ with ‘E-HP’ and ‘n’
with ‘nB-HP’ results in WAPI score of 0.49 which
is higher compared to individually merging these
phones.

From the above experiments we infer that merg-
ing phones result in more data for data-starved

Phones
Merged WAPI

All 2.45
eh with E-HP 0.31
n with nB-HP 0.47
eh with E-HP & n with nB-HP 0.49
P30 -0.59
P70 1.63

Table 2: Weighted average of phone improvement
(WAPI) scores for different phone merging.

phones which in turn improves the phone accura-
cies.

4.4 Improved Acoustic Models
We also performed the same experiments using
two Deep Neural Network (DNN) based acous-
tic models. The first model was built using 5 hid-
den layers with p-norm (Zhang et al., 2014) as the
non-linearity. The input dimension of each hidden
layer was 5000 and output dimension was 500. We
used 40 dimensional MFCC along with 4 splicing
frames on each side, appended with 100 dimen-
sional i-vectors (Dehak et al., 2011) as input fea-
tures. We also built a time-delay neural network
(TDNN) (Peddinti et al., 2015) with 6 hidden lay-
ers and ReLU as the non linearity.

Merging GMM p-norm DNN TDNN

No merge 40.36 32.81 29.15
All merge 39.70 31.89 28.78
DDPM 52.95 45.99 42.16
DDPM (αc(p)) 41.07 34.69 31.52
DDPM (αc) 40.75 34.48 31.28
P30 41.21 33.40 29.84
P70 40.92 34.30 28.94

Table 3: Word Error Rates of all systems. DDPM
stands for Data Driven Phone Merge. It has 3 vari-
ants as mentioned in section 4.5

Table 3 summarizes the WER for the different
systems. The all-merge model with p-norm DNN
had a WER of 31.89% (with confidence interval of
0.34%). This is 2.8% lower than the baseline DNN
model with no merging. The TDNN network out-
performed both the GMM and p-norm based DNN
AMs. The relative WER improvement of 1.29%
using TDNN with merging is statistically signifi-
cant compared to TDNN model without merging.



15

uw o
y

dr
-H

P
E

-H
P v jh eh

N
-H

P w
ph

-H
P ao g
f r

tB
h-

H
P l y

aU
-H

P b
e-

H
P ay aa

tr-
H

P
ow

g-
H

P m
Jh

-H
P ih

nB
-H

P
dB

-H
P ae

o-
H

P d
hv

-H
P ah

kh
-H

P
uu

-H
P

ss
il

p-
H

P
s-

H
P

l-
H

P
ch

-H
P iy

A
-H

P t
J-

H
P

ii-
H

P aw
u-

H
P

k-
H

P
gh

-H
P

nr
-H

P
A

a-
H

P
bh

-H
P

m
-H

P s
tB

-H
P

j-
H

P
b-

H
P

R
r-

H
P dh p e
r

ow
-H

P
i-

H
P

cC
-H

P n
c-

H
P

aI
-H

P
v-

H
P

dR
-H

P z
tR

-H
P ey k th sh zh

nN
-H

P uh
dB

h-
H

P
sr

-H
P ch ng hh

−5

0

5

C
ha

ng
e

in
ac

cu
ra

cy
(%

)

Figure 2: Accuracy changes (%) in the phone accuracies when eh (En) phone is merged with E-HP (Hi)
phone

ow
-H

P
tr-

H
P jh ow d
h m iy y k uw

ph
-H

P b v s l w n ch g
r

nr
-H

P hh ao
dr

-H
P

dR
-H

P aa p ih eh
f

uu
-H

P zh d
t

oy
i-

H
P

l-
H

P th
s-

H
P

p-
H

P uh
m

-H
P aw e
y

nB
-H

P
tB

h-
H

P
ii-

H
P

R
r-

H
P

k-
H

P ae ay
aU

-H
P er

o-
H

P
g-

H
P

j-
H

P
v-

H
P

b-
H

P
N

-H
P

c-
H

P
cC

-H
P

e-
H

P
u-

H
P

ch
-H

P sh
A

a-
H

P ng
J-

H
P

tB
-H

P
gh

-H
P

hv
-H

P
A

-H
P

Jh
-H

P
kh

-H
P ah

dB
-H

P
ss

il
R

rr
-H

P
bh

-H
P

E
-H

P
tR

-H
P

aI
-H

P z
dB

h-
H

P
sr

-H
P

nN
-H

P

0

20

40

C
ha

ng
e

in
ac

cu
ra

cy
(%

)

Figure 3: Change in phone accuracies after merging all similar sounding Hindi and English phones.

Motivated by this improved ASR performance, we
investigate approaches to merge phones using data
driven methods. We hypothesize that the data
driven methods will provide us with clues on the
acoustic similarity between phones to be merged.

4.5 Data-driven phone merging (DDPM)

The potential phone pairs to merge can be iden-
tified based on the errors made by the decoder,
with respect to the alignments. A TDNN acous-
tic model trained using the unmerged (89 phones)
phoneset was used to decode the utterances from
the dev dataset, with a low LM weight of 1 so
as to minimize the influence of LM. Phoneme se-
quences are then derived by parsing the best path
through the decoded lattice. The same acoustic
model is also used to align the dev data and ob-

tain the corresponding true phone sequences and
their durations. We choose pairs of aligned and
decoded utterances with a strict threshold of 80%
or more overlap in duration. Using the alignments,
we identify English phones which were wrongly
decoded as Hindi phones. We call them cross-
language swaps. Frequent swapping between
English-Hindi phone pair, indicates the need for
the pair to be merged.

Using this approach we observe several merges
that were present in the manual merge. For in-
stance, (e HP, ey), (A HP, ah), (ii HP, iy), (m HP,
m), (b HP, b), (ph HP, f). Errors such as, (Aa HP,
l), (u HP, ah), (j HP, ey), (g HP, l), were also no-
ticed. WER of 52.95, 45.99 and 42.16 were ob-
tained using GMM, DNN and TDNN models re-
spectively, after merging the phones identified by



16

the data-driven method. The decrease in perfor-
mance can be attributed to the wrong phone swaps.

4.5.1 Inducing context sensitivity through
reliability coefficients

Spurious phone swaps degrade the performance of
the ASR substantially. This can be reduced by
taking into account the phone context. We asso-
ciate a notion of context reliability with each con-
text c, which is defined as the proportion of correct
within-language phone predictions by the decoder
out of all the instances of a context c. We compute
this reliability as a coefficient (αc) for left, right
and both contexts in two different ways. αc can be
computed with respect to a specific center phone
(αc(p)) as:

αc(p) =
correct instances of p with c
total instances of p with c

= P (x = p|c)
(1)

An alternate method referred to as global con-
text reliability coefficient, is to compute the con-
text reliability coefficient for every context irre-
spective of the center phone p. This is obtained
by computing the ratio of the counts of the cor-
rect instances for any arbitrary phone in presence
of context c to the total instances of context c:

αc =
correct instances of context c
total instances of context c

(2)

The computed αc are applied as weights while
combining the probability of cross-language
swaps conditioned on the context c.

Our goal is to compute the conditional probabil-
ity of the decoded phone(xd) given the alignment
phone (xa), which is P (xd|xa). The phone (x̂d)
with the highest probability will be chosen as the
potential merge for xa (eq. 3).

x̂d = arg max
xd

P (xd|xa) (3)

The context information is incorporated by
computing the conditional probability specific to
a context as P (xd|xa, c) and then marginalizing
over all possible c to obtain the swap probability
P (xd|xa) (eq. 4).

P (xd|xa) =
∑
c

P (xd|xa, c)P (c|xa)

≈
∑
c

P (xd|xa, c)P (xa|c)P (c)

≈
∑
c

P (xd|xa, c)αcP (c)

(4)

P (c) is the prior probability for each context
c which is computed using the dev dataset. We
assign a neutral reliability (α0) score of 0.01 and
prior P (c) of 0.01 to all the unseen contexts. The
min and max values of αc are 0.0 and 1.0 respec-
tively.

Figure 4 shows cross-language phone confu-
sion matrices for the two context-sensitive data
driven phone merging approaches. We observe
that phone-specific coefficients are able to cap-
ture only the most prominent merges while global
coefficients produce merges that highly correlate
with the manual merge. This might be due to
the division of context information across phones
which reduces the context sensitivity. This clearly
suggests that some contexts help in producing bet-
ter predictions than others, regardless of the ref-
erence phone. Although many of the swaps pre-
dicted using DDPM closely resemble the manual
merges, the manual merge method outperformed
global-DDPM by approx. 3% absolute as seen in
Table 3. The distribution of αc values show that
the left and the right context exhibit high confi-
dence scores, whereas low confidence scores were
observed when both the context were considered.
Hence, we will benefit by removing the spurious
low-confidence contexts while merging.

Figure 5 presents the swap likelihood of the pre-
dicted phone-pairs in decreasing order. We ob-
serve that phone pairs that have the highest swap
likelihoods include nasals, close vowels and stops.
It is interesting that the data driven method iden-
tified new phone merges such as (ae, Aa HP) and
(aa, ow HP) compared to manual merged phones.
We believe that incorporating these new phone
merges into the manually merged phone set will
improve the ASR performance. Further experi-
ments need to be conducted to verify this claim.

4.6 Probabilistic merging

Next, we propose a method to allow the acous-
tic model to select appropriate phones during de-
coding. We trained an acoustic model using the
merged phones while also retaining the Hindi and
English phones. For example, the phone set for
the new AM contained the English phone ‘eh’, the
Hindi phone ‘E-HP’ as well as the merged phone
‘E-HP-M’. The intuition behind this approach is to
let the decoder choose between multiple pronun-
ciation variants in the lexicon so as to determine
the pronunciation used by a speaker who code-



17

A_
HP

Aa
_H

P
E_

HP J_H
P

Jh
_H

P
N_

HP
Rr

_H
P

Rr
r_

HP
aI

_H
P

aU
_H

P
ay

_H
P

b_
HP

bh
_H

P
cC

_H
P

c_
HP

ch
_H

P
dB

_H
P

dB
h_

HP
dR

_H
P

dr
_H

P
e_

HP
g_

HP
gh

_H
P

h_
HP

hv
_H

P
i_H

P
ii_

HP j_H
P

k_
HP

kh
_H

P
l_H

P
lr_

HP
m

_H
P

nB
_H

P
nN

_H
P

nr
_H

P
o_

HP
ow

_H
P

p_
HP

ph
_H

P
s_

HP
sr

_H
P

ss
il_

HP
tB

_H
P

tB
h_

HP
tR

_H
P

tr_
HP

u_
HP

uu
_H

P
v_

HP

Hindi phones

aa
ae
ah
ao
aw
ay
b

ch
d

dh
eh
er
ey

f
g

hh
ih
iy
jh
k
l

m
n

ng
ow
oy
p
r
s
t

th
uh
uw

v
w
y
z

zh
sh

En
gl

ish
 p

ho
ne

s

(a) Phone-specific

A_
HP

Aa
_H

P
E_

HP J_H
P

Jh
_H

P
N_

HP
Rr

_H
P

Rr
r_

HP
aI

_H
P

aU
_H

P
ay

_H
P

b_
HP

bh
_H

P
cC

_H
P

c_
HP

ch
_H

P
dB

_H
P

dB
h_

HP
dR

_H
P

dr
_H

P
e_

HP
g_

HP
gh

_H
P

h_
HP

hv
_H

P
i_H

P
ii_

HP j_H
P

k_
HP

kh
_H

P
l_H

P
lr_

HP
m

_H
P

nB
_H

P
nN

_H
P

nr
_H

P
o_

HP
ow

_H
P

p_
HP

ph
_H

P
s_

HP
sr

_H
P

ss
il_

HP
tB

_H
P

tB
h_

HP
tR

_H
P

tr_
HP

u_
HP

uu
_H

P
v_

HP

Hindi phones

aa
ae
ah
ao
aw
ay
b

ch
d

dh
eh
er
ey

f
g

hh
ih
iy
jh
k
l

m
n

ng
ow
oy
p
r
s
t

th
uh
uw

v
w
y
z

zh
sh

En
gl

ish
 p

ho
ne

s

(b) Global

Figure 4: Cross-language phone confusion matrices using phone-specific (αc(p)) and global (αc) context
reliability coefficients

m
→

m
H

P
s

H
P→

s
tr

H
P→

t
k

H
P→

k
nB

H
P→

n
l

H
P→

l
iy
→

ii
H

P
A

H
P→

ah
ih
→

i
H

P
r→

R
r

H
P

p
H

P→
p

d→
dr

H
P

b→
b

H
P

ow
→

o
H

P
ae
→

A
a

H
P

eh
→

e
H

P
hv

H
P→

hh
uw
→

uu
H

P
f→

ph
H

P
ow

H
P→

aa
J

H
P→

jh
ch
→

c
H

P
v

H
P→

v
g

H
P→

g
cC

H
P→

sh
j

H
P→

y
N

H
P→

ng
th
→

tB
h

H
P

uh
→

u
H

P
ao
→

aU
H

P
ay
→

aI
H

P
dB

H
P→

dh
w
→

bh
H

P
tB

H
P→

z
er
→

ch
H

P
aw
→

tR
H

P
E

H
P→

ey

0

0.5

1

1.5

·10−2

L
ik

el
ih

oo
d,

P
(x

d
|x

e
)

(a) Phone-specific

m
→

m
H

P
s

H
P→

s
k

H
P→

k
tr

H
P→

t
iy
→

ii
H

P
nB

H
P→

n
l

H
P→

l
A

H
P→

ah
i

H
P→

ih
r→

R
r

H
P

p
H

P→
p

d→
dr

H
P

ow
→

o
H

P
ay
→

A
a

H
P

b→
b

H
P

eh
→

e
H

P
ph

H
P→

f
uu

H
P→

uw
hv

H
P→

hh
c

H
P→

ch
J

H
P→

jh
ow

H
P→

aa
v

H
P→

v
y→

j
H

P
g→

g
H

P
sh
→

cC
H

P
ae
→

aI
H

P
N

H
P→

ng
th
→

tB
h

H
P

u
H

P→
uh

ao
→

aU
H

P
dh
→

dB
H

P
w
→

bh
H

P
z→

tB
H

P
aw
→

tR
H

P
er
→

ch
H

P
E

H
P→

ey

0

0.5

1

1.5

·10−2

(b) Global

Figure 5: Swap-pair likelihood for phone-specific (αc(p)) and global (αc) context reliability coefficients.

switched. This approach has been shown to work
well in speech synthesis for pronunciation model-
ing of homographs (Sitaram et al., 2015). Table
4 compares the number of unique Hindi, English
and merged phones for the different systems men-
tioned.

During training, we modified the lexicon so as
to retain part of the data for the unmerged phones
and assigned the rest to the merged phones. We as-
signed 30% and 70% of the data to merged phones
which we refer to as P30 and P70. During de-
coding, we created a different lexicon allowing
all possible pronunciation variants. We obtained
WERs of 41.21% and 40.92% using the HMM-
GMM based AM for P30 and P70 models respec-
tively which are lower than the baseline. WAPI

En Hi Merged Total

Baseline 39 50 0 89
All merge 8 19 31 58
DDPM 9 20 30 59
DDPM (αc(p)) 20 25 14 59
DDPM (αc) 21 24 14 59
Probabilistic
Merging 39 50 31 120

Table 4: Number of phones before and after merg-
ing



18

score of 1.63 for P70 model was higher compared
to -0.59 of P30 model but lower compared to the
all merge model. Table 4 shows the number of
English, Hindi and merged phones for each tech-
nique.

The WER is a function of number of insertion,
deletion and substitution errors as well as the cor-
rect token numbers. Figure 6 shows the relative
percentage change in the insertion, deletion and
substitution values of the “ all merge”, P30 and
P70 model compared to the baseline system us-
ing TDNN as AM. The best system should have
the lowest insertion, deletion and substitution er-
rors and highest correct tokens. The “all merge”
model, which has the best WER scores, has higher
insertion and substitution errors but performs bet-
ter on deletion errors and recognition of correct
tokens. We can infer that certain phone merges
are causing higher insertion and substitution er-
rors and should be avoided, thus concluding that
the manually merged phones are sub-optimal.

Insertion Deletion Substitutes Correct

−10

−5

0

5

10

R
el

at
iv

e
pe

rc
en

ta
ge

ch
an

ge

All phone merge
P30
P70

Figure 6: Percentage change in insertion, deletion,
substitution and correct token recognition num-
bers compared to the baseline system with TDNN
as AM. The best system should have the lowest of
insertion, substitution and deletion numbers and
the highest of correct token numbers. The number
of word tokens in baseline system for insertion,
deletion, substitution and correct tokens are 3342,
5735, 11089 and 49011 respectively.

5 Conclusion

In this work, we compare phone merging tech-
niques in context of code-mixed Hindi-English
speech with a baseline system built using a union
of both phone sets. We first merge similar sound-
ing phones across both languages manually in or-
der to reduce the phone set size and increase the
data availability per phone. We observe a 3% rel-
ative improvement in the WER values compared

to the baseline using a p-norm based DNN model
along with a significant improvement in phone ac-
curacies. We then propose data-driven approaches
to merge phones automatically. To correct the
errors made by data-driven method, we weight
the cross-language swaps using reliable within-
language contexts. These methods gave newer
phone merge recommendations which can be use-
ful to improve the ASR performance. We fur-
ther propose probabilistic methods where in the
decoder is provided with both the merged as well
as the unmerged phones which reduced the inser-
tion errors compared to the manually merged sys-
tem. these techniques came close to, but did not
improve upon the manually merged ASR system.
Error analysis of manual merging indicates that it
is not optimal and there is a need for better data-
driven techniques to automatically merge phones
for code-switched ASR.

6 Acknowledgments

We thank the anonymous reviewers for carefully
reading our manuscript and offering valuable sug-
gestions.

References
Basem HA Ahmed and Tien-Ping Tan. 2012. Auto-

matic speech recognition of code switching speech
using 1-best rescoring. In Asian Language Pro-
cessing (IALP), 2012 International Conference on,
pages 137–140. IEEE.

Kiran Bhuvanagirir and Sunil Kumar Kopparapu. 2012.
Mixed language speech recognition without explicit
identification of language. American Journal of Sig-
nal Processing, 2(5):92–97.

Joyce YC Chan, Houwei Cao, PC Ching, and Tan Lee.
2009. Automatic recognition of cantonese-english
code-mixing speech. Computational Linguistics
and Chinese Language Processing, 14(3):281–304.

Joyce YC Chan, PC Ching, Tan Lee, and Helen M
Meng. 2004. Detection of language boundary in
code-switching utterances by bi-phone probabilities.
In Chinese Spoken Language Processing, 2004 In-
ternational Symposium on, pages 293–296. IEEE.

Najim Dehak, Patrick J Kenny, Réda Dehak, Pierre Du-
mouchel, and Pierre Ouellet. 2011. Front-end fac-
tor analysis for speaker verification. IEEE Transac-
tions on Audio, Speech, and Language Processing,
19(4):788–798.

Mark JF Gales. 1998. Maximum likelihood linear
transformations for hmm-based speech recognition.
Computer speech & language, 12(2):75–98.



19

Hui Lin, Li Deng, Dong Yu, Yi-fan Gong, Alex Acero,
and Chin-Hui Lee. 2009. A study on multilin-
gual acoustic modeling for large vocabulary asr.
In Acoustics, Speech and Signal Processing, 2009.
ICASSP 2009. IEEE International Conference on,
pages 4333–4336. IEEE.

Dau-Cheng Lyu, Ren-Yuan Lyu, Yuang-chin Chiang,
and Chun-Nan Hsu. 2006. Speech recognition on
code-switching among the chinese dialects. In
2006 IEEE International Conference on Acoustics
Speech and Signal Processing Proceedings, vol-
ume 1, pages I–I. IEEE.

Tetyana Lyudovyk and Valeriy Pylypenko. 2014.
Code-switching speech recognition for closely re-
lated languages. Proc. SLTU, pages 188–193.

Ayushi Pandey, Brij Mohan Lai Srivastava, and
Suryakanth V Gangashetty. 2017. Adapting mono-
lingual resources for code-mixed hindi-english
speech recognition. In Asian Language Processing
(IALP), 2017 International Conference on, pages
218–221. IEEE.

Alok Parlikar, Sunayana Sitaram, Andrew Wilkinson,
and Alan W Black. The festvox indic frontend for
grapheme to phoneme conversion. In WILDRE:
Workshop on Indian Langauge Data, Resources and
Evaluation, 2016.

Vijayaditya Peddinti, Daniel Povey, and Sanjeev Khu-
danpur. 2015. A time delay neural network archi-
tecture for efficient modeling of long temporal con-
texts. In Sixteenth Annual Conference of the Inter-
national Speech Communication Association.

Daniel Povey, Arnab Ghoshal, Gilles Boulianne, Lukas
Burget, Ondrej Glembek, Nagendra Goel, Mirko
Hannemann, Petr Motlicek, Yanmin Qian, Petr
Schwarz, et al. 2011. The kaldi speech recog-
nition toolkit. In IEEE 2011 workshop on auto-
matic speech recognition and understanding, EPFL-
CONF-192584. IEEE Signal Processing Society.

Tanja Schultz and Alex Waibel. 1997. Fast bootstrap-
ping of lvcsr systems with multilingual phoneme
sets. In Fifth European Conference on Speech Com-
munication and Technology.

Tanja Schultz and Alex Waibel. 2001. Language-
independent and language-adaptive acoustic model-
ing for speech recognition. Speech Communication,
35(1-2):31–51.

Sunayana Sitaram and Alan W Black. 2016. Speech
synthesis of code-mixed text. In LREC.

Sunayana Sitaram, Serena Jeblee, and Alan W Black.
2015. Using acoustics to improve pronuncia-
tion for synthesis of low resource languages. In
Sixteenth Annual Conference of the International
Speech Communication Association.

Shubham Toshniwal, Tara N Sainath, Ron J Weiss,
Bo Li, Pedro Moreno, Eugene Weinstein, and Kan-
ishka Rao. 2018. Multilingual speech recognition
with a single end-to-end model. In ICASSP. IEEE.

Ngoc Thang Vu, David Imseng, Daniel Povey, Petr
Motlicek, Tanja Schultz, and Hervé Bourlard. 2014.
Multilingual deep neural network based acoustic
modeling for rapid language adaptation. In Acous-
tics, Speech and Signal Processing (ICASSP), 2014
IEEE International Conference on, pages 7639–
7643. IEEE.

Ngoc Thang Vu, Dau-Cheng Lyu, Jochen Weiner, Do-
minic Telaar, Tim Schlippe, Fabian Blaicher, Eng-
Siong Chng, Tanja Schultz, and Haizhou Li. 2012.
A first speech recognition system for mandarin-
english code-switch conversational speech. In
2012 IEEE International Conference on Acous-
tics, Speech and Signal Processing (ICASSP), pages
4889–4892. IEEE.

Robert L Weide. 1998. The cmu pronouncing
dictionary. URL: http://www. speech. cs. cmu.
edu/cgibin/cmudict.

Jochen Weiner. 2012. Integration of language identi-
fication into a recognition system for spoken con-
versations containing code-switches. Ph.D. thesis,
Language Technologies Institute.

Ching-Feng Yeh and Lin-Shan Lee. 2015. An im-
proved framework for recognizing highly imbal-
anced bilingual code-switched lectures with cross-
language acoustic modeling and frame-level lan-
guage identification. IEEE/ACM Transactions
on Audio, Speech, and Language Processing,
23(7):1144–1159.

Emre Yılmaz, Henk van den Heuvel, and David van
Leeuwen. 2016. Investigating bilingual deep neu-
ral networks for automatic recognition of code-
switching frisian speech. Procedia Computer Sci-
ence, 81:159–166.

Shengmin Yu, Shitwu Zhang, and Bo Xu. 2004.
Chinese-english bilingual phone modeling for cross-
language speech recognition. In Acoustics,
Speech, and Signal Processing, 2004. Proceed-
ings.(ICASSP’04). IEEE International Conference
on, volume 1, pages I–917. IEEE.

Xiaohui Zhang, Jan Trmal, Daniel Povey, and San-
jeev Khudanpur. 2014. Improving deep neural net-
work acoustic models using generalized maxout net-
works. In Acoustics, Speech and Signal Processing
(ICASSP), 2014 IEEE International Conference on,
pages 215–219. IEEE.


