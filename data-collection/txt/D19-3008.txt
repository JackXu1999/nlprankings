




































Controlling Sequence-to-Sequence Models - A Demonstration on Neural-based Acrostic Generator


Proceedings of the 2019 EMNLP and the 9th IJCNLP (System Demonstrations), pages 43–48
Hong Kong, China, November 3 – 7, 2019. c©2019 Association for Computational Linguistics

43

Controlling Sequence-to-Sequence Models - A Demonstration on
Neural-based Acrostic Generator

Liang-Hsin Shen, Pei-Lun Tai, Chao-Chung Wu, Shou-De Lin
Department of Computer Science and Information Engineering,

National Taiwan University
{laurice, b04902105, r05922042, sdlin}@csie.ntu.edu.tw

Abstract

An acrostic is a form of writing for which
the first token of each line (or other recurring
features in the text) forms a meaningful se-
quence. In this paper we present a generalized
acrostic generation system that can hide cer-
tain message in a flexible pattern specified by
the users. Different from previous works that
focus on rule-based solutions, here we adopt
a neural-based sequence-to-sequence model to
achieve this goal. Besides acrostic, users can
also specify the rhyme and length of the output
sequences. To the best of our knowledge, this
is the first neural-based natural language gen-
eration system that demonstrates the capability
of performing micro-level control over output
sentences.

1 Introduction

Acrostic is a form of writing aiming at hiding
messages in text, often used in sarcasm or to de-
liver private information. In previous works, En-
glish acrostic have been generated by searching
for paraphrases in WordNet’s synsets (Stein et al.,
2014). Synonyms that contain needed characters
replace the corresponding words in the context to
generate the acrostic. Nowadays Seq2Seq mod-
els have become a popular choice for text gen-
eration, including generating text from table (Liu
et al., 2018), summaries (Nallapati et al., 2016),
short-text conversations (Shang et al., 2015), ma-
chine translation (Bahdanau et al., 2015; Sutskever
et al., 2014) and so on. In contrast to a rule-based
or template-based generator, such Seq2Seq solu-
tions are often considered more general and cre-
ative, as they do not rely heavily on pre-requisite
knowledge or patterns to produce meaningful con-
tent. Although several works have presented au-
tomatic generation on rhymed text (Zhang and
Lapata, 2014; Ghazvininejad et al., 2016), the
works do not focus on controlling the rhyme

of the generated content. One drawback of a
neural-based Seq2Seq model is that the outputs
are hard to control since the generation follows
certain non-deterministic probabilistic model (or
language model), which makes it non-trivial to
impose a hard-constraint such as acrostic (i.e.
micro-controlling the position of a specific to-
ken) and rhyme. In this work, we present an
NLG system that allows the users to micro-control
the generation of a Seq2Seq model without any
post-processing. Besides specifying the tokens
and their corresponding locations for acrostic, our
model allows the users to choose the rhyme and
length of the generated lines. We show that
with simple adjustment, a Seq2Seq model such
as the Transformer (Vaswani et al., 2017) can
be trained to control the generation of the text.
Our demo system focuses on Chinese and English
lyrics, which can be regarded as a writing style
in between articles and poetry. We consider a
general version of acrostic writing, which means
the users can arbitrarily choose the position to
place acrostic tokens. The 2-minute demonstra-
tion video can be found at https://youtu.
be/9tX6ELCNMCE.

2 Model Description

Normally a neural-based Seq2Seq model is
learned using input/output sequences as training
pairs (Nallapati et al., 2016; Cho et al., 2014a). By
providing sufficient amount of such training pairs,
it is expected that the model learns how to produce
the output sequences based on the inputs. Here we
would like to first report a finding that a Seq2Seq
model is capable of discovering the hidden associ-
ations between inputting control signals and out-
putting sequences. Based on the finding we have
created a demo system to show that the users can
indeed guide the outputs of a Seq2Seq model in a

https://youtu.be/9tX6ELCNMCE
https://youtu.be/9tX6ELCNMCE


44

Figure 1: The structure of Transformer model.

fine-grained manner. In our demo, the users are al-
lowed to control three aspects of the generated se-
quences: rhyme, sentence length and the positions
of designated tokens. In other words, our Seq2Seq
model not only is capable of generate next line sat-
isfying the length and rhyme constraints provided
by the user, it can also produce the exact word at
a position specified by the user. The rhyme of a
sentence is the last syllable of the last word in that
sentence. The length of a sentence is the number
of tokens in that sentence. To elaborate how our
model is trained, we use three consecutive lines
(denoted as S1, S2, S3) of lyrics from the song
“Rhythm of the Rain” as an example. Normally a
Seq2Seq model is trained based on the following
input/output pairs.

S1: Listen to the rhythm of the falling
rain → S2: Telling me just what a fool
I’ve been

S2: Telling me just what a fool I’ve been
→ S3: I wish that it would go and let me
cry in vain

With some experiments on training Seq2Seq mod-
els, we have discovered an interesting fact: By
appending the control signals in the end of the
input sequences, after seeing sufficient amount of
such data, the Seq2Seq model can automatically
discover the association between input signals and
outputs. Once the associations are identified, then
we can use the control signals to guide the output
of the model. For instance, here we append addi-
tional control information to the end of the training
sequence as below

S1: Listen to the rhythm of the falling

rain || 1 Telling || IY N || 8 → S2:
Telling me just what a fool I’ve been

S2: Telling me just what a fool I’ve been
|| 2 wish 6 go || EY N || 12→ S3: I wish
that it would go and let me cry in vain

The three types of control signals are separated
by “||”. The first control signal indicates the po-
sition of the designated words. 1 Telling tells the
system the token Telling should be produced at
the first position of the output sequence s2. Sim-
ilarly, 2 wish 6 go means that the second/sixth
token in the output sequence shall be wish/go.
The second control signal is the rhyme of the
target sentence. For instance, IHN corresponds
to a specific rhyme (/In/) and EY N corresponds
to another (/en/). Note that here we use the formal
name of the rhyme (e.g. EY N ) to improve read-
ability. To train our system, any arbitrary symbol
would work. The third part contains a digit (e.g.
8) to control the length of the output line.

By adding such additional information,
Seq2Seq models can eventually learn the
meaning of the control signal as they can
produce outputs according to those signals with
very high accuracy. Note that in our demo,
all results are produced by our Seq2Seq model
without any post-processing, nor do we provide
any prerequisite knowledge about what length,
rhyme or position really stands for to the model.

We train our system based on the Transformer
model (Vaswani et al., 2017), though additional
experiments show that other RNN-based Seq2Seq
models such as the one based on GRU (Cho et al.,
2014b) or LSTM would also work. The model
consists of an encoder and a decoder. Our encoder



45

Figure 2: The structure of our acrostic generating system.

consists of two identical layers when training
on Chinese lyrics and four identical layers when
training on English lyrics. Each layer includes
two sub-layers. The first is a multi-head attention
layer and the second one is a fully connected feed-
forward layer. Residual connections (He et al.,
2016) are implemented between the sub-layers.
The decoder also consists of two identical layers
when training on Chinese lyrics and four identical
layers when training on English lyrics.. Each layer
includes three sub-layers: a masked multi-head at-
tention layer, a multi-head attention layer that per-
forms attention over the output of encoder and a
fully-connected feed-forward layer. The model
structure is shown in Figure 1. Note that in the
original paper (Vaswani et al., 2017), Transformer
consists of six identical layers for both encoder
and decoder. To save resource, we start training
with fewer layers than the original paper and dis-
cover that the model still performs well. Thus, we
use fewer layers than the proposed Transformer
model.

3 User Interface

Figure 2 illustrates the interface and data flow of
our acrostic lyric generating system. First, there
are several conditions (or control signals) that
can be specified by the users:

• Rhyme: For Chinese lyrics, there are 33

different rhymes for users to choose from.
As for English lyrics, there are 30 different
rhymes for users to choose from.

• Theme of topic: The theme given by user
is used to generate the zeroth sentence. In
Chinese Acrostic demonstration, our system
would pick a sentence from training set that
is most similar to the user input, measured
by the number of n-grams. As for English
Acrostic demo, the user input of theme is di-
rectly used as the zeroth sentence.

• Length of each line: User can specify the
length of every single line (separated by ;).
For example, “5;6;7” means that the user
wants to generate acrostic that contains 3
lines, with length equals to 5, 6, 7, respec-
tively.

• The sequence of tokens to be hidden in the
output sequences.

• Hidden Pattern: The exact positions for each
token to be hidden. Apart from the common
options, such as hiding in the first/last posi-
tions of each sentence or hiding in the diag-
onal positions, our system offers a more gen-
eral and flexible way to define the pattern,
realized through the Draw It Myself op-
tion. As shown in the bottom right corner of



46

Figure 2, a table based on the length of each
line specified by the users is created for the
users to select the positions to place acrostic
tokens.

The generation is done on the server side. After
receiving the control signals provided by users,
the server first uses the given theme to search
for a related line (denoted as zeroth sequence)
from the lyric corpus, based on both sentence-level
matching and character-level matching. Then the
given condition of first sentence is appended to
this zeroth sequence to serve as initial input to
the Seq2Seq model for generating first line of out-
puts. Next, the given condition of second sentence
is appended to the generated first line as input to
generate the second line. The same process is re-
peated until all lines are generated.

4 Experiment and Results

4.1 Data set
We have two versions: one training on Chinese
lyrics and one on English lyrics.

The Chinese lyrics are crawled from Mojim
lyrics site and NetEase Cloud. To avoid rare char-
acters, the vocabulary size is set to the most fre-
quent 50,000 characters. The English lyrics are
crawled from Lyrics Freak. The vocabulary size is
set to the most frequent 50,000 words. For each
line of lyrics, we first calculate its length and then
retrieve the rhyme of the last token. To generate
the training pairs, we randomly append to the input
sequence some tokens and their positions of
the targeting sequence as the first control signal,
followed by the rhyme and then length. Below
are two example training pairs:

S1: Listen to the rhythm of the falling
rain || 2 me 3 just || IY N || 8 → S2:
Telling me just what a fool I’ve been

S2: Telling me just what a fool I’ve been
|| 2 wish 6 go 7 and || EY N || 12→ S3:
I wish that it would go and let me cry in
vain

In total there are about 651,339/1,000,000 training
pairs we use to train our Chinese/English acrostic
systems.

4.2 Evaluation
Our system has three controllable conditions on
generating acrostic: the positions of designated

tokens, the rhyme of each line and the length
of each line. The evaluation set consists of
30,000 lines that are not included in training data.
We first evaluate how accurate the control con-
ditions can be satisfied. As shown in Table 1,
the model can almost perfectly satisfy the request
from users. We also evaluate the quality of learned
language model for Chinese/English lyrics. The
bi-gram perplexity of original training corpus is
54.56/53.2. The bi-gram perplexity of generated
lyrics becomes lower (42.33/42.34), which indi-
cates the language model does learn a better way
to represent the lyrics data. In this experiment
we find that training on English lyrics is harder
than training on Chinese lyrics. English has strict
grammatical rules while Chinese lyrics have more
freedom in forming a sentence. We also observe
that the model tends to generate sentences that use
the same words that appear in their previous sen-
tences. This behavior might be learned from the
repetition of lyrics lines.

4.3 Demonstration of Results

We provide our system outputs from different as-
pects.

The first example in Figure 3 shows that we
can control the length of each line to produce a
triangle-shaped lyrics.

Figure 3: Length control of each sentence.

Second, we would like to demonstrate the re-
sults in generating acrostic. Some people use
acrostic to hide message that has no resemblance
with the content of the full text. We would show
both English and Chinese examples generated by
our system.

Figure 4 shows hiding a sentence in the first

https://mojim.com/
https://mojim.com/
http://music.163.com/
https://www.lyricsfreak.com/


47

Condition Accuracy(Chinese) Accuracy(English)
Character (CH) / Word (EN) Position 99.38% 98.21%
Rhyme 99.31% 97.67%
Sentence Length 99.90% 99.85%
Source Perplexity(Chinese) Perplexity(English)
Training data 54.56 53.2
Model generated data 42.33 42.34

Table 1: The accuracy of each condition tested on 30,000 lines and the perplexity of the original text and the text
generated by our model.

word of each sentences. The sentence that be-
ing concealed in the lyrics is I don′t like you,
which is very different from the meaning of the
full lyrics.

Figure 4: Message in English lyrics: I don′t like you.

Figure 5: Hidden message in Chinese lyrics:甚麼都可
以藏 with rhyme eng.

Figure 5 shows a Chinese acrostic generated by
our system. We hide a message 甚麼都可以藏
(Anything can be hidden) in the diagonal line of
a piece of lyrics that talks about relationship and
dream.

Third, we can also play with the visual shape of
the designated words. Figure 6 shows an example

Figure 6: Message in English lyrics: be the change
you wish to see in the world. To make the diamond
shape clearer, the words are aligned.

of hiding a sentence in the shape of diamond in the
generated lyrics. The message being concealed is
be the change you wish to see in the world.
Figure 7 shows that we can hide the message using
the shape of a heart.

Figure 7: The designated characters form a heart. The
sentence hidden in the lyrics is疏影橫斜水清淺暗香
浮動月黃昏 (The shadow reflects on the water and the
fragrance drifts under the moon with the color of dusk)
with rhyme i.



48

5 Conclusion

We show that by appending additional informa-
tion in the training input sequences, it is possi-
ble to train a Seq2Seq model whose outputs can
be controlled in a fine-grained level. This find-
ing enables us to design and demonstrate a gen-
eral acrostic generating system with various fea-
tures controlled, including the length of each line,
the rhyme of each line and the target tokens to be
produced and their corresponding positions. Our
results have shown that the proposed model not
only is capable of generating meaningful content,
it also follows the constraints with very high ac-
curacy. We believe that this finding can further
lead to other useful applications in natural lan-
guage generation.

References
Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben-

gio. 2015. Neural machine translation by jointly
learning to align and translate. In 3rd Inter-
national Conference on Learning Representations,
ICLR 2015, San Diego, CA, USA, May 7-9, 2015,
Conference Track Proceedings.

Kyunghyun Cho, Bart van Merrienboer, Çaglar
Gülçehre, Dzmitry Bahdanau, Fethi Bougares, Hol-
ger Schwenk, and Yoshua Bengio. 2014a. Learning
phrase representations using RNN encoder-decoder
for statistical machine translation. In Proceedings of
the 2014 Conference on Empirical Methods in Nat-
ural Language Processing, EMNLP 2014, October
25-29, 2014, Doha, Qatar, A meeting of SIGDAT,
a Special Interest Group of the ACL, pages 1724–
1734.

Kyunghyun Cho, Bart van Merriënboer, Çalar
Gülçehre, Dzmitry Bahdanau, Fethi Bougares, Hol-
ger Schwenk, and Yoshua Bengio. 2014b. Learning
phrase representations using rnn encoder–decoder
for statistical machine translation. In Proceedings of
the 2014 Conference on Empirical Methods in Nat-
ural Language Processing (EMNLP), pages 1724–
1734, Doha, Qatar. Association for Computational
Linguistics.

Marjan Ghazvininejad, Xing Shi, Yejin Choi, and
Kevin Knight. 2016. Generating topical poetry. In
Proceedings of the 2016 Conference on Empirical
Methods in Natural Language Processing, EMNLP
2016, Austin, Texas, USA, November 1-4, 2016,
pages 1183–1191.

Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian
Sun. 2016. Deep residual learning for image recog-
nition. In 2016 IEEE Conference on Computer Vi-
sion and Pattern Recognition, CVPR 2016, Las Ve-
gas, NV, USA, June 27-30, 2016, pages 770–778.

Tianyu Liu, Kexiang Wang, Lei Sha, Baobao Chang,
and Zhifang Sui. 2018. Table-to-text generation
by structure-aware seq2seq learning. In Proceed-
ings of the Thirty-Second AAAI Conference on Ar-
tificial Intelligence, (AAAI-18), the 30th innovative
Applications of Artificial Intelligence (IAAI-18), and
the 8th AAAI Symposium on Educational Advances
in Artificial Intelligence (EAAI-18), New Orleans,
Louisiana, USA, February 2-7, 2018, pages 4881–
4888.

Ramesh Nallapati, Bowen Zhou, Cícero Nogueira dos
Santos, Çaglar Gülçehre, and Bing Xiang. 2016.
Abstractive text summarization using sequence-to-
sequence rnns and beyond. In Proceedings of the
20th SIGNLL Conference on Computational Natural
Language Learning, CoNLL 2016, Berlin, Germany,
August 11-12, 2016, pages 280–290.

Lifeng Shang, Zhengdong Lu, and Hang Li. 2015.
Neural responding machine for short-text conver-
sation. In Proceedings of the 53rd Annual Meet-
ing of the Association for Computational Linguistics
and the 7th International Joint Conference on Natu-
ral Language Processing of the Asian Federation of
Natural Language Processing, ACL 2015, July 26-
31, 2015, Beijing, China, Volume 1: Long Papers,
pages 1577–1586.

Benno Stein, Matthias Hagen, and Christof Bräutigam.
2014. Generating acrostics via paraphrasing and
heuristic search. In COLING 2014, 25th Inter-
national Conference on Computational Linguistics,
Proceedings of the Conference: Technical Papers,
August 23-29, 2014, Dublin, Ireland, pages 2018–
2029.

Ilya Sutskever, Oriol Vinyals, and Quoc V. Le. 2014.
Sequence to sequence learning with neural net-
works. In Advances in Neural Information Process-
ing Systems 27: Annual Conference on Neural In-
formation Processing Systems 2014, December 8-
13 2014, Montreal, Quebec, Canada, pages 3104–
3112.

Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob
Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz
Kaiser, and Illia Polosukhin. 2017. Attention is all
you need. In Advances in Neural Information Pro-
cessing Systems 30: Annual Conference on Neural
Information Processing Systems 2017, 4-9 Decem-
ber 2017, Long Beach, CA, USA, pages 6000–6010.

Xingxing Zhang and Mirella Lapata. 2014. Chinese
poetry generation with recurrent neural networks. In
Proceedings of the 2014 Conference on Empirical
Methods in Natural Language Processing, EMNLP
2014, October 25-29, 2014, Doha, Qatar, A meet-
ing of SIGDAT, a Special Interest Group of the ACL,
pages 670–680.

http://arxiv.org/abs/1409.0473
http://arxiv.org/abs/1409.0473
http://aclweb.org/anthology/D/D14/D14-1179.pdf
http://aclweb.org/anthology/D/D14/D14-1179.pdf
http://aclweb.org/anthology/D/D14/D14-1179.pdf
http://www.aclweb.org/anthology/D14-1179
http://www.aclweb.org/anthology/D14-1179
http://www.aclweb.org/anthology/D14-1179
http://aclweb.org/anthology/D/D16/D16-1126.pdf
https://doi.org/10.1109/CVPR.2016.90
https://doi.org/10.1109/CVPR.2016.90
https://www.aaai.org/ocs/index.php/AAAI/AAAI18/paper/view/16599
https://www.aaai.org/ocs/index.php/AAAI/AAAI18/paper/view/16599
http://aclweb.org/anthology/K/K16/K16-1028.pdf
http://aclweb.org/anthology/K/K16/K16-1028.pdf
http://aclweb.org/anthology/P/P15/P15-1152.pdf
http://aclweb.org/anthology/P/P15/P15-1152.pdf
http://aclweb.org/anthology/C/C14/C14-1190.pdf
http://aclweb.org/anthology/C/C14/C14-1190.pdf
http://papers.nips.cc/paper/5346-sequence-to-sequence-learning-with-neural-networks
http://papers.nips.cc/paper/5346-sequence-to-sequence-learning-with-neural-networks
http://papers.nips.cc/paper/7181-attention-is-all-you-need
http://papers.nips.cc/paper/7181-attention-is-all-you-need
http://aclweb.org/anthology/D/D14/D14-1074.pdf
http://aclweb.org/anthology/D/D14/D14-1074.pdf

