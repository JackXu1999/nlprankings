



















































Morphological Priors for Probabilistic Neural Word Embeddings


Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 490–500,
Austin, Texas, November 1-5, 2016. c©2016 Association for Computational Linguistics

Morphological Priors for Probabilistic Neural Word Embeddings

Parminder Bhatia∗
Yik Yak, Inc.

3525 Piedmont Rd NE, Building 6, Suite 500
Atlanta, GA

parminder@yikyakapp.com

Robert Guthrie∗ and Jacob Eisenstein
School of Interactive Computing
Georgia Institute of Technology

Atlanta, GA 30312 USA
{rguthrie3 + jacobe}@gatech.edu

Abstract

Word embeddings allow natural language pro-
cessing systems to share statistical information
across related words. These embeddings are
typically based on distributional statistics, mak-
ing it difficult for them to generalize to rare or
unseen words. We propose to improve word
embeddings by incorporating morphological
information, capturing shared sub-word fea-
tures. Unlike previous work that constructs
word embeddings directly from morphemes,
we combine morphological and distributional
information in a unified probabilistic frame-
work, in which the word embedding is a latent
variable. The morphological information pro-
vides a prior distribution on the latent word em-
beddings, which in turn condition a likelihood
function over an observed corpus. This ap-
proach yields improvements on intrinsic word
similarity evaluations, and also in the down-
stream task of part-of-speech tagging.

1 Introduction

Word embeddings have been shown to improve many
natural language processing applications, from lan-
guage models (Mikolov et al., 2010) to information
extraction (Collobert and Weston, 2008), and from
parsing (Chen and Manning, 2014) to machine trans-
lation (Cho et al., 2014). Word embeddings leverage
a classical idea in natural language processing: use
distributional statistics from large amounts of unla-
beled data to learn representations that allow sharing

∗The first two authors contributed equally. Code
is available at https://github.com/rguthrie3/
MorphologicalPriorsForWordEmbeddings.

across related words (Brown et al., 1992). While this
approach is undeniably effective, the long-tail nature
of linguistic data ensures that there will always be
words that are not observed in even the largest cor-
pus (Zipf, 1949). There will be many other words
which are observed only a handful of times, making
the distributional statistics too sparse to accurately
estimate the 100- or 1000-dimensional dense vectors
that are typically used for word embeddings. These
problems are particularly acute in morphologically
rich languages like German and Turkish, where each
word may have dozens of possible inflections.

Recent work has proposed to address this issue by
replacing word-level embeddings with embeddings
based on subword units: morphemes (Luong et al.,
2013; Botha and Blunsom, 2014) or individual char-
acters (Santos and Zadrozny, 2014; Ling et al., 2015;
Kim et al., 2016). Such models leverage the fact that
word meaning is often compositional, arising from
subword components. By learning representations of
subword units, it is possible to generalize to rare and
unseen words.

But while morphology and orthography are some-
times a signal of semantics, there are also many cases
similar spellings do not imply similar meanings: bet-
ter-batter, melon-felon, dessert-desert, etc. If each
word’s embedding is constrained to be a determinis-
tic function of its characters, as in prior work, then
it will be difficult to learn appropriately distinct em-
beddings for such pairs. Automated morphological
analysis may be incorrect: for example, really may
be segmented into re+ally, incorrectly suggesting a
similarity to revise and review. Even correct morpho-
logical segmentation may be misleading. Consider

490



that incredible and inflammable share a prefix in-,
which exerts the opposite effect in these two cases.1

Overall, a word’s observed internal structure gives
evidence about its meaning, but it must be possible to
override this evidence when the distributional facts
point in another direction.

We formalize this idea using the machinery of
probabilistic graphical models. We treat word em-
beddings as latent variables (Vilnis and McCallum,
2014), which are conditioned on a prior distribution
that is based on word morphology. We then maximize
a variational approximation to the expected likeli-
hood of an observed corpus of text, fitting variational
parameters over latent binary word embeddings. For
common words, the expected word embeddings are
largely determined by the expected corpus likelihood,
and thus, by the distributional statistics. For rare
words, the prior plays a larger role. Since the prior
distribution is a function of the morphology, it is pos-
sible to impute embeddings for unseen words after
training the model.

We model word embeddings as latent binary vec-
tors. This choice is based on linguistic theories of
lexical semantics and morphology. Morphemes are
viewed as adding morphosyntactic features to words:
for example, in English, un- adds a negation feature
(unbelievable), -s adds a plural feature, and -ed adds
a past tense feature (Halle and Marantz, 1993). Sim-
ilarly, the lexicon is often viewed as organized in
terms of features: for example, the word bachelor
carries the features HUMAN, MALE, and UNMAR-
RIED (Katz and Fodor, 1963). Each word’s semantic
role within a sentence can also be characterized in
terms of binary features (Dowty, 1991; Reisinger et
al., 2015). Our approach is more amenable to such
theoretical models than traditional distributed word
embeddings. However, we can also work with the ex-
pected word embeddings, which are vectors of prob-
abilities, and can therefore be expected to hold the
advantages of dense distributed representations (Ben-
gio et al., 2013).

1The confusion is resolved by morphologically analyzing the
second example as (in+flame)+able, but this requires hierarchi-
cal morphological parsing, not just segmentation.

2 Model

The modeling framework is illustrated in Figure 1,
focusing on the word sesquipedalianism. This word
is rare, but its morphology indicates several of its
properties: the -ism suffix suggests that the word is a
noun, likely describing some abstract property; the
sesqui- prefix refers to one and a half, and so on. If
the word is unknown, we must lean heavily on these
intuitions, but if the word is well attested then we can
rely instead on its examples in use.

It is this reasoning that our modeling framework
aims to formalize. We treat word embeddings as la-
tent variables in a joint probabilistic model. The prior
distribution over a word’s embedding is conditioned
on its morphological structure. The embedding it-
self then participates, as a latent variable, in a neural
sequence model over a corpus, contributing to the
overall corpus likelihood. If the word appears fre-
quently, then the corpus likelihood dominates the
prior — which is equivalent to relying on the word’s
distributional properties. If the word appears rarely,
then the prior distribution steps in, and gives a best
guess as to the word’s meaning.

Before describing these component pieces in detail,
we first introduce some notation. The representation
of word w is a latent binary vector bw ∈ {0, 1}k,
where k is the size of each word embedding. As
noted in the introduction, this binary representation
is motivated by feature-based theories of lexical se-
mantics (Katz and Fodor, 1963). Each word w is
constructed from a set of Mw observed morphemes,
Mw = (mw,1,mw,2, . . . ,mw,Mw). Each morpheme
is in turn drawn from a finite vocabulary of size
vm, so that mw,i ∈ {1, 2, . . . , vm}. Morphemes
are obtained from an unsupervised morphological
segmenter, which is treated as a black box. Fi-
nally, we are given a corpus, which is a sequence
of words, x = (x1, x2, . . . , xN ), where each word
xt ∈ {1, 2, . . . , vw}, with vw equal to the size of the
vocabulary, including the token 〈UNK〉 for unknown
words.

2.1 Prior distribution

The key differentiating property of this model is that
rather than estimating word embeddings directly, we
treat them as a latent variable, with a prior distri-
bution reflecting the word’s morphological proper-

491



plagued by sesquipedalianism . . .

h1 h2 h3

bplagued bby bsesquipedalianism

uplague uby usesqui

ued upedal
uian
uism

Figure 1: Model architecture, applied to the example sequence . . . plagued by sesquipedalianism . . . . Blue solid arrows indicate
direct computation, red dashed arrows indicate probabilistic dependency. For simplicity, we present our models as recurrent neural

networks rather than long short-term memories (LSTMs).

ties. To characterize this prior distribution, each mor-
pheme m is associated with an embedding of its own,
um ∈ Rk, where k is again the embedding size. Then
for position i of the word embedding bw, we have
the following prior,

bw,i ∼ Bernoulli
(
σ(

∑

m∈Mw
um,i)

)
, (1)

where σ(·) indicates the sigmoid function. The prior
log-likelihood for a set of word embeddings is,

logP (b;M,u) (2)

=

Vw∑

w

logP (bw;Mw,u) (3)

=

Vw∑

w

k∑

i

logP (bw,i;Mw,u) (4)

=

Vw∑

w

k∑

i

bw,i log σ

( ∑

m∈Mw
um,i

)
(5)

+ (1− bw,i) log
(

1− σ
( ∑

m∈Mw
um,i

))
.

2.2 Expected likelihood
The corpus likelihood is computed via a recurrent
neural network language model (Mikolov et al., 2010,
RNNLM), which is a generative model of sequences
of tokens. In the RNNLM, the probability of each
word is conditioned on all preceding words through

a recurrently updated state vector. This state vector
in turn depends on the embeddings of the previous
words, through the following update equations:

ht =f(bxt ,ht−1) (6)

xt+1 ∼Multinomial (Softmax [Vht]) . (7)

The function f(·) is a recurrent update equation; in
the RNN, it corresponds to σ(Θht−1 + bxt), where
σ(·) is the elementwise sigmoid function. The matrix
V ∈ Rv×k contains the “output embeddings” of each
word in the vocabulary. We can then define the condi-
tional log-likelihood of a corpusx = (x1, x2, . . . xN )
as,

logP (x | b) =
N∑

t

logP (xt | ht−1, b). (8)

Since ht−1 is deterministically computed from
x1:t−1 (conditioned on b), we can equivalently write
the log-likelihood as,

logP (x | b) =
∑

t

logP (xt | x1:t−1, b). (9)

This same notation can be applied to compute the
likelihood under a long-short term memory (LSTM)
language model (Sundermeyer et al., 2012). The only
difference is that the recurrence function f(·) from
Equation 6 now becomes more complex, including
the input, output, and forget gates, and the recurrent
state ht now includes the memory cell. As the LSTM

492



update equations are well known, we focus on the
more concise RNN notation, but we employ LSTMs
in all experiments due to their better ability to capture
long-range dependencies.

2.3 Variational approximation

Inference on the marginal likelihood P (x1:N ) =∫
P (x1:N , b)db is intractable. We address this is-

sue by making a variational approximation,

logP (x) = log
∑

b

P (x | b)P (b) (10)

= log
∑

b

Q(b)

Q(b)
P (x | b)P (b) (11)

= logEq

[
P (x | b)P (b)

Q(b)

]
(12)

≥Eq[logP (x | b) + logP (b)− logQ(b)]
(13)

The variational distribution Q(b) is defined using a
fully factorized mean field approximation,

Q(b;γ) =

vw∏

w

k∏

i

q(bw,i; γw,i). (14)

The variational distribution is a product of Bernoullis,
with parameters γw,j ∈ [0, 1]. In the evaluations
that follow, we use the expected word embeddings
q(bw), which are dense vectors in [0, 1]k. We can
then use Q(·) to place a variational lower bound on
the expected conditional likelihood,

Even with this variational approximation, the ex-
pected log-likelihood is still intractable to compute.
In recurrent neural network language models, each
word xt is conditioned on the entire prior history,
x1:t−1 — indeed, this is one of the key advantages
over fixed-length n-gram models. However, this
means that the individual expected log probabilities
involve not just the word embedding of xt and its
immediate predecessor, but rather, the embeddings

of all words in the sequence x1:t:

Eq [logP (x | b)] (15)

=
N∑

t

Eq [logP (xt | x1:t−1, b)] (16)

=
N∑

t

∑

{bw:w∈x1:t}
Q({bw : w ∈ x1:t})

× logP (xt | x1:t−1, b). (17)

We therefore make a further approximation by tak-
ing a local expectation over the recurrent state,

Eq [ht] ≈ f(Eq [bxt ] , Eq [ht−1])
(18)

Eq [logP (xt | x1:t−1, b)] ≈ log Softmax (VEq [ht]) .
(19)

This approximation means that we do not propa-
gate uncertainty about ht through the recurrent up-
date or through the likelihood function, but rather, we
use local point estimates. Alternative methods such
as variational autoencoders (Chung et al., 2015) or
sequential Monte Carlo (de Freitas et al., 2000) might
provide better and more principled approximations,
but this direction is left for future work.

Variational bounds in the form of Equation 13
can generally be expressed as a difference between
an expected log-likelihood term and a term for the
Kullback-Leibler (KL) divergence between the prior
distribution P (b) and the variational distribution
Q(b) (Wainwright and Jordan, 2008). Incorporat-
ing the approximation in Equation 19, the resulting
objective is,

L =
N∑

t

logP (xt | x1:t−1;Eq[b])

−DKL(Q(b) ‖ P (b)). (20)

493



The KL divergence is equal to,

DKL(Q(b) ‖ P (b)) (21)

=

vw∑

w

k∑

i

DKL(q(bw,i) ‖ P (bw,i)) (22)

=

vw∑

w

k∑

i

γw,i log σ(
∑

m∈Mw
um,i)

+ (1− γw,i) log(1− σ(
∑

m∈Mw
um,i))

− γw,i log γw,i − (1− γw,i) log(1− γw,i).
(23)

Each term in the variational bound can be easily
constructed in a computation graph, enabling auto-
matic differentiation and the application of standard
stochastic optimization techniques.

3 Implementation

The objective function is given by the variational
lower bound in Equation 20, using the approxima-
tion to the conditional likelihood described in Equa-
tion 19. This function is optimized in terms of several
parameters:

• the morpheme embeddings, {um}m∈1...vm ;

• the variational parameters on the word embed-
dings, {γ}w∈1...vw ;

• the output word embeddings V;

• the parameter of the recurrence function, Θ.

Each of these parameters is updated via the
RMSProp online learning algorithm (Tieleman and
Hinton, 2012). The model and baseline (described be-
low) are implemented in blocks (van Merriënboer
et al., 2015). In the remainder of the paper, we refer
to our model as VAREMBED.

3.1 Data and preprocessing
All embeddings are trained on 22 million tokens
from the the North American News Text (NANT)
corpus (Graff, 1995). We use an initial vocabu-
lary of 50,000 words, with a special 〈UNK〉 token
for words that are not among the 50,000 most com-
mon. We then perform downcasing and convert all
numeric tokens to a special 〈NUM〉 token. After these

steps, the vocabulary size decreases to 48,986. Note
that the method can impute word embeddings for
out-of-vocabulary words under the prior distribution
P (b;M,u); however, it is still necessary to decide
on a vocabulary size to determine the number of
variational parameters γ and output embeddings to
estimate.

Unsupervised morphological segmentation is per-
formed using Morfessor (Creutz and Lagus, 2002),
with a maximum of sixteen morphemes per word.
This results in a total of 14,000 morphemes, which
includes stems for monomorphemic words. We do
not rely on any labeled information about morpho-
logical structure, although the incorporation of gold
morphological analysis is a promising topic for future
work.

3.2 Learning details
The LSTM parameters are initialized uniformly in
the range [−0.08, 0.08]. The word embeddings are
initialized using pre-trained word2vec embeddings.
We train the model for 15 epochs, with an initial
learning rate of 0.01, a decay of 0.97 per epoch, and
minibatches of size 25. We clip the norm of the
gradients (normalized by minibatch size) at 1, using
the default settings in the RMSprop implementation
in blocks. These choices are motivated by prior
work (Zaremba et al., 2014). After each iteration, we
compute the objective function on the development
set; when the objective does not improve beyond a
small threshold, we halve the learning rate.

Training takes roughly one hour per iteration us-
ing an NVIDIA 670 GTX, which is a commodity
graphics processing unit (GPU) for gaming. This
is nearly identical to the training time required for
our reimplementation of the algorithm of Botha and
Blunsom (2014), described below.

3.3 Baseline
The most comparable approach is that of Botha and
Blunsom (2014). In their work, embeddings are es-
timated for each morpheme, as well as for each in-
vocabulary word. The final embedding for a word is
then the sum of these embeddings, e.g.,

greenhouse = greenhouse + green + house, (24)

where the italicized elements represent learned em-
beddings.

494



We build a baseline that is closely inspired by this
approach, which we call SUMEMBED. The key differ-
ence is that while Botha and Blunsom (2014) build on
the log-bilinear language model (Mnih and Hinton,
2007), we use the same LSTM-based architecture as
in our own model implementation. This enables our
evaluation to focus on the critical difference between
the two approaches: the use of latent variables rather
than summation to model the word embeddings. As
with our method, we used pre-trained word2vec
embeddings to initialize the model.

3.4 Number of parameters

The dominant terms in the overall number of parame-
ters are the (expected) word embeddings themselves.
The variational parameters of the input word em-
beddings, γ, are of size k × vw. The output word
embeddings are of size #|h| × vw. The morpheme
embeddings are of size k × vm, with vm � vw. In
our main experiments, we set vw = 48, 896 (see
above), k = 128, and #|h| = 128. After including
the character embeddings and the parameters of the
recurrent models, the total number of parameters is
roughly 12.8 million. This is identical to number of
parameters in the SUMEMBED baseline.

4 Evaluation

Our evaluation compares the following embeddings:

WORD2VEC We train the popular word2vec
CBOW (continuous bag of words)
model (Mikolov et al., 2013), using the
gensim implementation.

SUMEMBED We compare against the baseline de-
scribed in § 3.3, which can be viewed as a
reimplementation of the compositional model
of Botha and Blunsom (2014).

VAREMBED For our model, we take the expected
embeddings Eq[b], and then pass them through
an inverse sigmoid function to obtain values
over the entire real line.

4.1 Word similarity

Our first evaluation is based on two classical word
similarity datasets: Wordsim353 (Finkelstein et al.,

2001) and the Stanford “rare words” (rw) dataset (Lu-
ong et al., 2013). We report Spearmann’s ρ, a mea-
sure of rank correlation, evaluating on both the entire
vocabulary as well as the subset of in-vocabulary
words.

As shown in Table 1, VAREMBED consistently
outperforms SUMEMBED on both datasets. On the
subset of in-vocabulary words, WORD2VEC gives
slightly better results on the wordsim words that are
in the NANT vocabulary, but is not applicable to
the complete dataset. On the rare words dataset,
WORD2VEC performs considerably worse than both
morphology-based models, matching the findings of
Luong et al. (2013) and Botha and Blunsom (2014)
regarding the importance of morphology for doing
well on this dataset.

4.2 Alignment with lexical semantic features

Recent work questions whether these word similar-
ity metrics are predictive of performance on down-
stream tasks (Faruqui et al., 2016). The QVEC statis-
tic is another intrinsic evaluation method, which has
been shown to be better correlated with downstream
tasks (Tsvetkov et al., 2015). This metric measures
the alignment between word embeddings and a set
of lexical semantic features. Specifically, we use the
semcor noun verb supersenses oracle provided at the
qvec github repository.2

As shown in Table 2, VAREMBED outperforms
SUMEMBED on the full lexicon, and gives simi-
lar performance to WORD2VEC on the set of in-
vocabulary words. We also consider the morpheme
embeddings alone. For SUMEMBED, this means that
we construct the word embedding from the sum of
the embeddings for its morphemes, without the ad-
ditional embedding per word. For VAREMBED, we
use the expected embedding under the prior distribu-
tion E[b | c]. The results for these representations
are shown in the bottom half of Table 2, revealing
that VAREMBED learns much more meaningful em-
beddings at the morpheme level, while much of the
power of SUMEMBED seems to come from the word
embeddings.

2https://github.com/ytsvetko/qvec

495



WORD2VEC SUMEMBED VAREMBED

Wordsim353
all words (353) n/a 42.9 48.8
in-vocab (348) 51.4 45.9 51.3
rare words (rw)
all words (2034) n/a 23.0 24.0
in-vocab (715) 33.6 37.3 44.1

Table 1: Word similarity evaluation results, as measured by Spearmann’s ρ× 100. WORD2VEC cannot be evaluated on all words,
because embeddings are not available for out-of-vocabulary words. The total number of words in each dataset is indicated in

parentheses.

all words
(4199)

in vocab
(3997)

WORD2VEC n/a 34.8
SUMEMBED 32.8 33.5
VAREMBED 33.6 34.7
morphemes only
SUMEMBED 24.7 25.1
VAREMBED 30.2 31.0

Table 2: Alignment with lexical semantic features, as measured
by QVEC. Higher scores are better, with a maximum possible

score of 100.

4.3 Part-of-speech tagging

Our final evaluation is on the downstream task of
part-of-speech tagging, using the Penn Treebank.
We build a simple classification-based tagger, us-
ing a feedforward neural network. (This is not in-
tended as an alternative to state-of-the-art tagging
algorithms, but as a comparison of the syntactic
utility of the information encoded in the word em-
beddings.) The inputs to the network are the con-
catenated embeddings of the five word neighbor-
hood (xt−2, xt−1, xt, xt+1, xt+2); as in all evalua-
tions, 128-dimensional embeddings are used, so the
total size of the input is 640. This input is fed into
a network with two hidden layers of size 625, and
a softmax output layer over all tags. We train using
RMSProp (Tieleman and Hinton, 2012).

Results are shown in Table 3. Both
morphologically-informed embeddings are
significantly better to WORD2VEC (p < .01,
two-tailed binomial test), but the difference between
SUMEMBED and VAREMBED is not significant

dev test

WORD2VEC 92.42 92.40
SUMEMBED 93.26 93.26
VAREMBED 93.05 93.09

Table 3: Part-of-speech tagging accuracies

0-100 100-1000 1000-10000 10000-100000
word frequency in NANT

0.00
0.05
0.10
0.15
0.20
0.25
0.30
0.35

er
ro

r r
at

e

embedding
VarEmbed
SumEmbed
Word2Vec

Figure 2: Error rates by word frequency.

at p < .05. Figure 2 breaks down the errors by
word frequency. As shown in the figure, the tagger
based on WORD2VEC performs poorly for rare
words, which is expected because these embeddings
are estimated from sparse distributional statistics.
SUMEMBED is slightly better on the rarest words
(the 0 − 100 group accounts for roughly 10% of
all tokens). In this case, it appears that this simple
additive model is better, since the distributional
statistics are too sparse to offer much improvement.
The probabilistic VAREMBED embeddings are
best for all other frequency groups, showing that it
effectively combines morphology and distributional
statistics.

5 Related work

Adding side information to word embeddings
An alternative approach to incorporating additional

496



information into word embeddings is to constrain the
embeddings of semantically-related words to be sim-
ilar. Such work typically draws on existing lexical
semantic resources such as WordNet. For example,
Yu and Dredze (2014) define a joint training objec-
tive, in which the word embedding must predict not
only neighboring word tokens in a corpus, but also
related word types in a semantic resource; a similar
approach is taken by Bian et al. (2014). Alternatively,
Faruqui et al. (2015) propose to “retrofit” pre-trained
word embeddings over a semantic network. Both
retrofitting and our own approach treat the true word
embeddings as latent variables, from which the pre-
trained word embeddings are stochastically emitted.
However, a key difference from our approach is that
the underlying representation in these prior works is
relational, and not generative. These methods can
capture similarity between words in a relational lex-
icon such as WordNet, but they do not offer a gen-
erative account of how (approximate) meaning is
constructed from orthography or morphology.

Word embeddings and morphology The
SUMEMBED baseline is based on the work of Botha
and Blunsom (2014), in which words are segmented
into morphemes using MORFESSOR (Creutz and
Lagus, 2002), and then word representations are
computed through addition of morpheme represen-
tations. A key modeling difference from this prior
work is that rather than computing word embeddings
directly and deterministically from subcomponent
embeddings (morphemes or characters, as in (Ling
et al., 2015; Kim et al., 2016)), we use these
subcomponents to define a prior distribution, which
can be overridden by distributional statistics for
common words. Other work exploits morphology
by training word embeddings to optimize a joint
objective over distributional statistics and rich,
morphologically-augmented part of speech tags (Cot-
terell and Schütze, 2015). This can yield better word
embeddings, but does not provide a way to compute
embeddings for unseen words, as our approach does.

Recent work by Cotterell et al. (2016) extends the
idea of retrofitting, which was based on semantic
similarity, to a morphological framework. In this
model, embeddings are learned for morphemes as
well as for words, and each word embedding is con-
ditioned on the sum of the morpheme embeddings,

using a multivariate Gaussian. The covariance of this
Gaussian prior is set to the inverse of the number of
examples in the training corpus, which has the effect
of letting the morphology play a larger role for rare
or unseen words. Like all retrofitting approaches, this
method is applied in a pipeline fashion after training
word embeddings on a large corpus; in contrast, our
approach is a joint model over the morphology and
corpus. Another practical difference is that Cotterell
et al. (2016) use gold morphological features, while
we use an automated morphological segmentation.

Latent word embeddings Word embeddings are
typically treated as a parameter, and are optimized
through point estimation (Bengio et al., 2003; Col-
lobert and Weston, 2008; Mikolov et al., 2010). Cur-
rent models use word embeddings with hundreds or
even thousands of parameters per word, yet many
words are observed only a handful of times. It is
therefore natural to consider whether it might be
beneficial to model uncertainty over word embed-
dings. Vilnis and McCallum (2014) propose to model
Gaussian densities over dense vector word embed-
dings. They estimate the parameters of the Gaussian
directly, and, unlike our work, do not consider us-
ing orthographic information as a prior distribution.
This is easy to do in the latent binary framework
proposed here, which is also a better fit for some the-
oretical models of lexical semantics (Katz and Fodor,
1963; Reisinger et al., 2015). This view is shared by
Kruszewski et al. (2015), who induce binary word
representations using labeled data of lexical seman-
tic entailment relations, and by Henderson and Popa
(2016), who take a mean field approximation over
binary representations of lexical semantic features to
induce hyponymy relations.

More broadly, our work is inspired by recent ef-
forts to combine directed graphical models with dis-
criminatively trained “deep learning” architectures.
The variational autoencoder (Kingma and Welling,
2014), neural variational inference (Mnih and Gregor,
2014; Miao et al., 2016), and black box variational
inference (Ranganath et al., 2014) all propose to use
a neural network to compute the variational approx-
imation. These ideas are employed by Chung et al.
(2015) in the variational recurrent neural network,
which places a latent continuous variable at each time
step. In contrast, we have a dictionary of latent vari-

497



ables — the word embeddings — which introduce
uncertainty over the hidden state ht in a standard re-
current neural network or LSTM. We train this model
by employing a mean field approximation, but these
more recent techniques for neural variational infer-
ence may also be applicable. We plan to explore this
possibility in future work.

6 Conclusion and future work

We present a model that unifies compositional
and distributional perspectives on lexical semantics,
through the machinery of Bayesian latent variable
models. In this framework, our prior expectations
of word meaning are based on internal structure, but
these expectations can be overridden by distributional
statistics. The model is based on the very successful
long-short term memory (LSTM) for sequence mod-
eling, and while it employs a Bayesian justification,
its inference and estimation are little more compli-
cated than a standard LSTM. This demonstrates the
advantages of reasoning about uncertainty even when
working in a “neural” paradigm.

This work represents a first step, and we see many
possibilities for improving performance by extending
it. Clearly we would expect this model to be more ef-
fective in languages with richer morphological struc-
ture than English, and we plan to explore this possi-
bility in future work. From a modeling perspective,
our prior distribution merely sums the morpheme em-
beddings, but a more accurate model might account
for sequential or combinatorial structure, through
a recurrent (Ling et al., 2015), recursive (Luong et
al., 2013), or convolutional architecture (Kim et al.,
2016). There appears to be no technical obstacle
to imposing such structure in the prior distribution.
Furthermore, while we build the prior distribution
from morphemes, it is natural to ask whether char-
acters might be a better underlying representation:
character-based models may generalize well to non-
word tokens such as names and abbreviations, they
do not require morphological segmentation, and they
require a much smaller number of underlying em-
beddings. On the other hand, morphemes encode
rich regularities across words, which may make a
morphologically-informed prior easier to learn than
a prior which works directly at the character level.
It is possible that this tradeoff could be transcended

by combining characters and morphemes in a single
model.

Another advantage of latent variable models is that
they admit partial supervision. If we follow Tsvetkov
et al. (2015) in the argument that word embeddings
should correspond to lexical semantic features, then
an inventory of such features could be used as a
source of partial supervision, thus locking dimen-
sions of the word embeddings to specific semantic
properties. This would complement the graph-based
“retrofitting” supervision proposed by Faruqui et al.
(2015), by instead placing supervision at the level of
individual words.

Acknowledgments

Thanks to Erica Briscoe, Martin Hyatt, Yangfeng Ji,
Bryan Leslie Lee, and Yi Yang for helpful discussion
of this work. Thanks also the EMNLP reviewers for
constructive feedback. This research is supported by
the Defense Threat Reduction Agency under award
HDTRA1-15-1-0019.

References
Yoshua Bengio, Réjean Ducharme, Pascal Vincent, and

Christian Janvin. 2003. A neural probabilistic language
model. The Journal of Machine Learning Research,
3:1137–1155.

Yoshua Bengio, Aaron Courville, and Pascal Vincent.
2013. Representation learning: A review and new per-
spectives. IEEE Transactions on Pattern Analysis and
Machine Intelligence, 35(8):1798–1828.

Jiang Bian, Bin Gao, and Tie-Yan Liu. 2014. Knowledge-
powered deep learning for word embedding. In
Machine Learning and Knowledge Discovery in
Databases, pages 132–148. Springer.

Jan A Botha and Phil Blunsom. 2014. Compositional mor-
phology for word representations and language mod-
elling. In Proceedings of the International Conference
on Machine Learning (ICML).

Peter F Brown, Peter V Desouza, Robert L Mercer, Vin-
cent J Della Pietra, and Jenifer C Lai. 1992. Class-
based n-gram models of natural language. Computa-
tional linguistics, 18(4):467–479.

Danqi Chen and Christopher D Manning. 2014. A fast
and accurate dependency parser using neural networks.
In Proceedings of Empirical Methods for Natural Lan-
guage Processing (EMNLP), pages 740–750.

Kyunghyun Cho, Bart Van Merriënboer, Caglar Gulcehre,
Dzmitry Bahdanau, Fethi Bougares, Holger Schwenk,

498



and Yoshua Bengio. 2014. Learning phrase representa-
tions using rnn encoder-decoder for statistical machine
translation. In Proceedings of Empirical Methods for
Natural Language Processing (EMNLP).

Junyoung Chung, Kyle Kastner, Laurent Dinh, Kratarth
Goel, Aaron Courville, and Yoshua Bengio. 2015.
A recurrent latent variable model for sequential data.
In Neural Information Processing Systems (NIPS),
Montréal.

Ronan Collobert and Jason Weston. 2008. A unified
architecture for natural language processing: Deep neu-
ral networks with multitask learning. In Proceedings
of the International Conference on Machine Learning
(ICML), pages 160–167.

Ryan Cotterell and Hinrich Schütze. 2015. Morpho-
logical word-embeddings. In Proceedings of the North
American Chapter of the Association for Computational
Linguistics (NAACL), Denver, CO, May.

Ryan Cotterell, Hinrich Schütze, and Jason Eisner. 2016.
Morphological smoothing and extrapolation of word
embeddings. In Proceedings of the Association for
Computational Linguistics (ACL), Berlin, August.

Mathias Creutz and Krista Lagus. 2002. Unsuper-
vised discovery of morphemes. In Proceedings of the
ACL-02 workshop on Morphological and phonologi-
cal learning-Volume 6, pages 21–30. Association for
Computational Linguistics.

João FG de Freitas, Mahesan Niranjan, Andrew H. Gee,
and Arnaud Doucet. 2000. Sequential monte carlo
methods to train neural network models. Neural com-
putation, 12(4):955–993.

David Dowty. 1991. Thematic proto-roles and argument
selection. Language, pages 547–619.

Manaal Faruqui, Jesse Dodge, Sujay K Jauhar, Chris Dyer,
Eduard Hovy, and Noah A Smith. 2015. Retrofitting
word vectors to semantic lexicons. In Proceedings
of the North American Chapter of the Association for
Computational Linguistics (NAACL), Denver, CO, May.

Manaal Faruqui, Yulia Tsvetkov, Pushpendre Rastogi,
and Chris Dyer. 2016. Problems with evaluation of
word embeddings using word similarity tasks. arxiv,
1605.02276.

Lev Finkelstein, Evgeniy Gabrilovich, Yossi Matias, Ehud
Rivlin, Zach Solan, Gadi Wolfman, and Eytan Ruppin.
2001. Placing search in context: The concept revisited.
In WWW, pages 406–414. ACM.

David Graff. 1995. North american news text corpus.
Morris Halle and Alec Marantz. 1993. Distributed mor-

phology and the pieces of inflection. In Kenneth L.
Hale and Samuel J. Keyser, editors, The view from
building 20. MIT Press, Cambridge, MA.

James Henderson and Diana Nicoleta Popa. 2016. A
vector space for distributional semantics for entailment.

In Proceedings of the Association for Computational
Linguistics (ACL), Berlin, August.

Jerrold J Katz and Jerry A Fodor. 1963. The structure of
a semantic theory. Language, pages 170–210.

Yoon Kim, Yacine Jernite, David Sontag, and Alexan-
der M Rush. 2016. Character-aware neural language
models. In Proceedings of the National Conference on
Artificial Intelligence (AAAI).

Diederik P Kingma and Max Welling. 2014. Auto-
encoding variational bayes. In Proceedings of the In-
ternational Conference on Learning Representations
(ICLR).

German Kruszewski, Denis Paperno, and Marco Baroni.
2015. Deriving boolean structures from distributional
vectors. Transactions of the Association for Computa-
tional Linguistics, 3:375–388.

Wang Ling, Tiago Luı́s, Luı́s Marujo, Ramón Fernandez
Astudillo, Silvio Amir, Chris Dyer, Alan W Black, and
Isabel Trancoso. 2015. Finding function in form: Com-
positional character models for open vocabulary word
representation. In Proceedings of Empirical Methods
for Natural Language Processing (EMNLP), Lisbon,
September.

Minh-Thang Luong, Richard Socher, and Christopher D
Manning. 2013. Better word representations with
recursive neural networks for morphology. In Pro-
ceedings of the Conference on Computational Natural
Language Learning (CoNLL).

Yishu Miao, Lei Yu, and Phil Blunsom. 2016. Neural vari-
ational inference for text processing. In Proceedings
of the International Conference on Machine Learning
(ICML).

Tomas Mikolov, Martin Karafiát, Lukas Burget, Jan Cer-
nockỳ, and Sanjeev Khudanpur. 2010. Recurrent neu-
ral network based language model. In INTERSPEECH,
pages 1045–1048.

Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Cor-
rado, and Jeff Dean. 2013. Distributed representations
of words and phrases and their compositionality. In
Advances in Neural Information Processing Systems,
pages 3111–3119.

Andriy Mnih and Karol Gregor. 2014. Neural varia-
tional inference and learning in belief networks. In
Proceedings of the International Conference on Ma-
chine Learning (ICML), pages 1791–1799.

Andriy Mnih and Geoffrey Hinton. 2007. Three new
graphical models for statistical language modelling. In
Proceedings of the International Conference on Ma-
chine Learning (ICML).

Rajesh Ranganath, Sean Gerrish, and David Blei. 2014.
Black box variational inference. In Proceedings of
the Seventeenth International Conference on Artificial
Intelligence and Statistics, pages 814–822.

499



Drew Reisinger, Rachel Rudinger, Francis Ferraro, Craig
Harman, Kyle Rawlins, and Benjamin Van Durme.
2015. Semantic proto-roles. Transactions of the Asso-
ciation for Computational Linguistics, 3:475–488.

Cicero D. Santos and Bianca Zadrozny. 2014. Learning
character-level representations for part-of-speech tag-
ging. In Proceedings of the International Conference
on Machine Learning (ICML), pages 1818–1826.

Martin Sundermeyer, Ralf Schlüter, and Hermann Ney.
2012. LSTM neural networks for language modeling.
In Proceedings of INTERSPEECH.

Tijman Tieleman and Geoffrey Hinton. 2012. Lecture 6.5:
Rmsprop. Technical report, Coursera Neural Networks
for Machine Learning.

Yulia Tsvetkov, Manaal Faruqui, Wang Ling, Guillaume
Lample, and Chris Dyer. 2015. Evaluation of word
vector representations by subspace alignment. In Pro-
ceedings of Empirical Methods for Natural Language
Processing (EMNLP), Lisbon, September.

Bart van Merriënboer, Dzmitry Bahdanau, Vincent Du-
moulin, Dmitriy Serdyuk, David Warde-Farley, Jan
Chorowski, and Yoshua Bengio. 2015. Blocks and fuel:
Frameworks for deep learning. CoRR, abs/1506.00619.

Luke Vilnis and Andrew McCallum. 2014. Word
representations via gaussian embedding. CoRR,
abs/1412.6623.

Martin J. Wainwright and Michael I. Jordan. 2008. Graph-
ical models, exponential families, and variational infer-
ence. Foundations and Trends in Machine Learning,
1(1-2):1–305.

Mo Yu and Mark Dredze. 2014. Improving lexical em-
beddings with semantic knowledge. In Proceedings of
the Association for Computational Linguistics (ACL),
pages 545–550, Baltimore, MD.

Wojciech Zaremba, Ilya Sutskever, and Oriol Vinyals.
2014. Recurrent neural network regularization. arXiv
preprint arXiv:1409.2329.

George Kingsley Zipf. 1949. Human behavior and the
principle of least effort. Addison-Wesley.

500


