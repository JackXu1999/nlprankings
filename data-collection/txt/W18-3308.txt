



















































Seq2Seq2Sentiment: Multimodal Sequence to Sequence Models for Sentiment Analysis


Proceedings of the First Grand Challenge and Workshop on Human Multimodal Language (Challenge-HML), pages 53–63,
Melbourne, Australia July 20, 2018. c©2018 Association for Computational Linguistics

53

Seq2Seq2Sentiment:
Multimodal Sequence to Sequence Models for Sentiment Analysis

Hai Pham1*, Thomas Manzini1*, Paul Pu Liang2, Barnabás Poczós2
{1Language Technologies Institute, 2Machine Learning Department}, CMU, USA

{htpham,tmanzini,pliang,bapoczos}@cs.cmu.edu

Abstract

Multimodal machine learning is a core
research area spanning the language, vi-
sual and acoustic modalities. The cen-
tral challenge in multimodal learning in-
volves learning representations that can
process and relate information from multi-
ple modalities. In this paper, we propose
two methods for unsupervised learning of
joint multimodal representations using se-
quence to sequence (Seq2Seq) methods: a
Seq2Seq Modality Translation Model and
a Hierarchical Seq2Seq Modality Transla-
tion Model. We also explore multiple dif-
ferent variations on the multimodal inputs
and outputs of these seq2seq models. Our
experiments on multimodal sentiment anal-
ysis using the CMU-MOSI dataset indicate
that our methods learn informative mul-
timodal representations that outperform
the baselines and achieve improved per-
formance on multimodal sentiment analy-
sis, specifically in the Bimodal case where
our model is able to improve F1 Score by
twelve points. We also discuss future direc-
tions for multimodal Seq2Seq methods.

1 Introduction

Sentiment analysis, which involves identifying a
speaker’s sentiment, is an open research problem.
In this field, the majority of work done focused on
unimodal methodologies - primarily textual analy-
sis - where investigating was limited to identifying
usage of words in positive and negative scenar-
ios. However, unimodal textual sentiment anal-
ysis through usage of words, phrases, and their
interdependencies were found to be insufficient for
extracting affective content from textual opinions

(Rosas et al., 2013). 1 As a result, there has been
a recent push towards using statistical methods to
extract additional behavioral cues not present in
the language modality from the video and audio
modalities. This research field is known as multi-
modal sentiment analysis and it extends the con-
ventional text-based definition of sentiment anal-
ysis to a multimodal setup where different modal-
ities contribute to modeling the sentiment of the
speaker. For example, (Kaushik et al., 2013) ex-
plores modalities such as audio, while (Wöllmer
et al., 2013) explores a multimodal approach to
predicting sentiment. This push has been further
bolstered by the advent of multimodal social me-
dia platforms, such as YouTube, Facebook, and
VideoLectures which are used to express personal
opinions on a worldwide scale. As a result, several
multimodal datasets, such as CMU-MOSI (Zadeh
et al., 2016) and later CMU-MOSEI (Zadeh et al.,
2018c), ICT-MMMO (Wöllmer et al., 2013) and
YouTube (Morency et al., 2011), take advantage of
the abundance of multimodal data on the Internet.
At the same time, neural network based multimodal
models have been proposed that are highly effective
at learning multimodal representations for multi-
modal sentiment analysis (Chen et al., 2017; Poria
et al., 2017; Zadeh et al., 2018a,b).

Recent progress has been limited to supervised
learning using labeled data, and does not take ad-
vantage of the abundant unlabeled data on the In-
ternet. To address this gap, our work is primarily
one of unsupervised representation learning. We
attempt to learn a multimodal representation of our
data in a structured paradigm and explore whether
a joint multimodal representation trained via un-
supervised learning can improve the performance
for multimodal sentiment analysis. While represen-
tation learning has been an area of rapid research

1*These authors contributed equally.



54

in the past years, there has been limited work that
explores multimodal setting. To this end, we pro-
pose two methods: a Seq2Seq Modality Translation
Model and a Hierarchical Seq2Seq Modality Trans-
lation Model for unsupervised learning of multi-
modal representations. Our results show that us-
ing multimodal representations learned from our
Seq2Seq modality translation method outperforms
the baselines and achieves improved performance
on multimodal sentiment analysis.

2 Related Work

In the past, approaches to text-based emotion and
sentiment recognition rely mainly on rule-based
techniques, bag of words (BoW) modeling or
SNoW architecture (Chaumartin, 2007) using a
large sentiment or emotion lexicon (Mishne et al.,
2005), or statistical approaches that assume the
availability of a large dataset annotated with polar-
ity or emotion labels.

Multimodal sentiment analysis has gained a lot
of research interests over the last few years (Baltru-
saitis et al., 2017). Probably the most challenging
task in multimodal sentiment analysis is to find a
joint representation of multiple modalities. This
problem is has been approached in a number of
ways. Earlier works such as (Ngiam et al., 2011;
Lazaridou et al., 2015; Kiros et al., 2014) have
pushed some progress towards this direction.

Recently, more advanced neural network mod-
els were proposed to learn multimodal representa-
tions. The Multi-View LSTM (MV-LSTM) (Ra-
jagopalan et al., 2016) was suggested to exploit
fusion and temporal relationships. MV-LSTM par-
titions memory cells and gates into multiple regions
corresponding to different views. Tensor Fusion
Network (Zadeh et al., 2017) presented an efficient
method based on Cartesian-product to take into
consideration intramodal and intermodal relations
between video, audio and text of the reviews to
create a novel feature representation for each utter-
ance. The Gated Multimodal Embedding model
(Chen et al., 2017) created an algorithm using re-
inforcement learning to train an on-off switch that
decided what values the video and audio compo-
nents would have. Noisy modalities are turned off
and clean modalities are allowed to pass through.
(Zadeh et al., 2018a) utilizes external multimodal
memory mechanisms to store multimodal informa-
tion and create multimodal representations through
time. (Zadeh et al., 2018b) proposed using multi-

ple attention coefficient assignments to represent
multiple cross-modal interactions. However, all
these methods discussed so far are purely super-
vised approaches to multimodal sentiment analysis
and do not leverage the power of unsupervised data
and generative approaches towards learning multi-
modal representations.

Besides supervised approaches, generative meth-
ods based on generative adversarial networks
(GAN) (Goodfellow et al., 2014) have attracted
significant interest in learning joint distribution
between two or more modalities (Donahue et al.,
2016; Li et al., 2017; Gan et al., 2017). Another
method to deal with multimodal problems is to
view them as conditional problems which learn to
map a modality to the other (Mirza and Osindero,
2014; Kingma et al., 2014; Pandey and Dukkipati,
2017). Our work can be viewed as an extension
of the conditional approach, as both utilize unsu-
pervised learning. However, our work differs from
those in that it takes into account the sequential
dependency within each modality.

Finally, attention based layers have also proved
themselves to be effective tools to boost perfor-
mance of neural network models, such as in neural
machine translation (Klein et al.; Bahdanau et al.,
2014; Luong et al., 2015), speech recognition (Sri-
ram et al., 2017) and in image captioning (Xu et al.,
2015). Our work also employs this mechanism in
an attempt to better handle long-term dependencies
of variable-length sequences.

3 Problem Formulation

Given a dataset with data X =
(Xtext,Xaudio,Xvideo) where Xtext, Xaudio,
Xvideo stand for text, audio and video modality
inputs, respectively. Typically a dataset is
indexed by videos. This means that if we have
n videos, then X = (X1,X2, ...,Xn) where
Xi = (Xi

text,Xi
audio,Xi

video), 1 ≤ i ≤ n. The
corresponding labels for these n videos are
Y = (Y1, Y2, ..., Yn), Yi ∈ R.

To simplify the problem, we align the input
based on words. Typically, researchers often seg-
ment each video into a smaller set in which each
segmented video will last a couple of seconds, in-
stead of minutes as done in (Chen et al., 2017).
After such alignment and segmentation, we have
the equal-length inputs of each modality per video.
For example, at the ith video, we have Xitext =
(wi

(1),wi
(2), ...,wi

(Ti)) wherewi(t) stands for the



55

tth word and Ti is the length of the ith video’s
text input, a.k.a time steps. Note that different
videos will have different time steps. Similarly
for this video, we have a sequence of audio input
Xi

audio = (ai
(1), ai

(2), ..., ai
(Ti)) and video input

Xi
video = (vi

(1), vi
(2), ..., vi

(Ti)).
In this work we are tackling the input learning

problem where we want to learn the embedding
representation for all text, audio, and video modal-
ities: X̃i = f(Xi) = f((Xitext,Xiaudio,Xivideo)).
In our baseline model, the function f is sim-
ply the concatenation at time step level: x̃it =
[wi

t;ai
t; vi

t]

In our proposed method, we learn X̃i by us-
ing a Seq2Seq model. We do not calculate
each embedding representation for each time
step, but for the whole sequence. Formally,
X̃i = f(Xi) = Seq2Seq Encoder(Xi) where
Seq2Seq Encoder is the encoder part of our
Seq2Seq model.

Now, we have the transformed inputs X̃ =
(X̃1, X̃2, ..., X̃n) and outputs Y = (Y1, Y2, ..., Yn)
for n videos, where X̃i = (x̃i1, x̃i2, ..., x̃iTi). For
simplicity, in the next formula, we omit the in-
dex of video segment i, and so the input becomes
X̃ = (x̃1, x̃2, ..., x̃T ), and the labels become Y =
(y1, y2, ..., yT ).

We will be using a Recurrent Neural Network
(RNN) such as LSTM (Hochreiter and Schmidhu-
ber, 1997) or GRU (Chung et al., 2015) to model
this sequence. In detail, this RNN has a stack of
K hidden layers h = (h1, h2, ..., hK), each con-
tains D hidden neurons: hk = (hk1, h

k
2, ..., h

k
D), k ∈

[1,K]. We denote W and b to be weight and bias,
then for the first layer which contacts directly with
input:

h1t =H(Wxh1 x̃t +Wh1h1h
1
t−1 + bh1) (1)

where H is the RNN cell function. For example
of LSTM, it contains input, forget, output and cell
state. At hidden layer k ∈ [2,K]:

hkt =H(Wht−1hth
k−1
t +Whkhkh

k
t−1 + bhk) (2)

Optionally, we apply a soft attention mechanism
on top of the last hidden layer hK , with shared
weight Wα over T time steps, then we can obtain
the attention output α:

α = softmax

⎛
⎜
⎜
⎜
⎝

⎡
⎢
⎢
⎢
⎢
⎢
⎢
⎢
⎣

Wαh
K
1

Wαh
K
2

...

Wαh
K
T

⎤
⎥
⎥
⎥
⎥
⎥
⎥
⎥
⎦

⎞
⎟
⎟
⎟
⎠

(3)

Figure 1: Seq2Seq Modality Translation Model with input
(X1, ...,XN) and output is (Y1, ..., YT ). Seq2Seq makes use
of the whole input sequence in the decoding phase for every
token Yi. If attention model (yellow color) is used, for each
Yi, it learns a separate weight vector w.r.t each token of input
X to see which token should the decoder “attend” more.

The last hidden layer’s output now becomes:

A = [hK1 , h
K
2 , ..., h

K
T ]α =H

Kα (4)

And the last output layer with regression score is:

ỹt =WAyA + by (5)

Finally, we calculate the loss with respect to the
labels. As in (Chen et al., 2017), we choose Mean
Absolute Error (MAE) as our loss and later train
with stochastic gradient descent:

LMAE(Ỹ , Y ) = E[∣Ỹ − Y ∣] (6)

4 Proposed Approach

In this section we describe the different approaches
that we plan to take to improve affect recognition
through learning multimodal representations.

4.1 Seq2Seq Modality Translation Model
The Seq2Seq Modality Translation Model aims to
learn multimodal representations that can be used
for discriminative tasks. While Seq2Seq models
have been predominantly used for machine trans-
lation (Bahdanau et al., 2014; Luong et al., 2015),
we extend its usage to the realm of multimodal
machine learning where we use it to translate one
modality to another, or translate a joint represen-
tation to another single or joint representation. To
do so, we propose a Seq2Seq modality translation
model with attention mechanism, as shown in Fig-
ure 1. Modality X is translated into modality Y .
Our hypothesis is that the intermediate represen-
tation of this model, i.e. the output of Seq2Seq’s
encoder, or the input of its decoder, is close to the
joint representation (X,Y ) of the two modalities



56

Figure 2: Hierarchical Seq2Seq Modality Translation Model: first we train with 2 modalities, then we add one more on
the second phase, from which the results will be fed into RNN for sentiment prediction. The green boxes denote the joint
representation learned by Seq2Seq models: the joint representation of modalities A and B will be fed into another Seq2Seq
model which in turn learns the joint representation of AB and another modality C. Finally the joint representation of ABC will
be fed into a RNN to predict sentiment.

involved. As a result, this representation can be
used for tasks that involve learning joint represen-
tation across multiple modalities. The detail is in
Algorithm 1.

Algorithm 1 Seq2Seq Modality Translation
X,Y,S are 2 modalities and sentiment sequences
1: Phase 1: Train Seq2Seq
2: EXY ← Seq2Seq RNN Encode(X)
3: Ỹ ← Seq2Seq RNN Decode(EXY )
4: loss = cross entropy(Ỹ , Y )
5: Backprop to update params

6: Phase 2: Sentiment Regression
7: EXY ← Seq2Seq RNN Encode(X) ▷ trained

encoder in Seq2Seq model
8: R = RNN(EXY )
9: score← Regression(R)

10: loss←MAE(score, S)
11: Backprop to update params

Formally, the Seq2Seq Modality Translation
Model consists of 2 separate steps: encoding and
decoding, each phase typically consists of a sin-
gle RNN or a stack of them. This model accepts
variable-length inputs ofX and Y , and the network
should be trained to maximize the translational
condition probability p(Y ∣X). For encoding, it
encodes the whole input sequence X into an em-
bedded representation. The hidden state output of
each time step is based on the previous hidden state
along with the input sequence (refer to Figure 1):

hn = RNN(hn−1,Xn) (7)

The encoder’s output is the final hidden state’s out-
put of the encoding RNN:

E = hN = RNN(hN−1,XN) (8)

where N is the length of the input sequence X. The
decoder tries to decode each token Yi at a time
based on E and all previous decoded tokens, which
is formulated as:

p(Y ) =
T

∏
i=1
p(Yi∣E , Y1, ..., Yi−1) (9)

The Seq2Seq training target is to find the best trans-
lation sequence which is as close to the ground
truth Y as possible, or formally:

Ŷ = argmax
Y

p(Y ∣X) (10)

And while there are some other search algorithms
such as random sampling or greedy search to de-
code each token (Neubig, 2017), we use the tra-
ditional beam search approach (Sutskever et al.,
2014).

4.2 Hierarchical Seq2Seq Modality
Translation Model

The Seq2Seq Modality Translation Model only
learns joint representation between 2 modalities X
and Y . While this might be a strong starting point,
we believe an approach that captures the joint in-
teractions between all different modalities X,Y,Z
is more effective in modeling the full distribution
of the multimodal data and therefore more useful
for regression or classification. In response, we
propose the Hierarchical Seq2Seq Modality Trans-
lation Model that learns a joint multimodal rep-
resentation. Once the Seq2Seq Modality Trans-
lation Model is trained for 2 modalities X and
Y , we obtain the intermediate representation EXY
which is the joint representation of (X,Y ). EXY



57

is in turn treated as input sequence for the next
Seq2Seq Modality Translation Model to decode
the third modality Z. The final multimodal repre-
sentation EXY Z represents the joint representation
of (X,Y,Z). The Hierarchical Seq2Seq Modality
Translation Model is described as in Algorithm 2.

Algorithm 2 Hierarchical Seq2Seq Modality
Translation: X,Y,Z,S are 3 modalities and senti-
ment sequences
1: Phase 1: Train Seq2Seq for 2 modalities
2: EXY ← Seq2Seq RNN Encode(X)
3: Ỹ ← Seq2Seq RNN Decode(EXY )
4: loss = cross entropy(Ỹ , Y )
5: Backpropagate to update parameters

6: Phase 2: Train Seq2Seq for 3 modalities
7: EXY Z ← Seq2Seq RNN Encode(EXY )
8: Z̃ ← Seq2Seq RNN Decode(EXY Z)
9: loss = cross entropy(Z̃,Z)

10: Backpropagate to update parameters

11: Phase 3: Sentiment Regression
12: EXY Z ← Seq2Seq RNN Encode(EXY )
13: R = RNN(EXY Z)
14: score← Regression(R)
15: loss←MAE(score, S)
16: Backpropagate to update parameters

This strategy is also illustrated in Figure 2. The
output of the second Seq2Seq model is the input of
the last RNN model where we will train to predict
regression sentiment scores. This last Seq2Seq
model will be trained using MAE loss function and
it perform subsequent regression process.

5 Experimental Setup

We explored the applications of this model to the
CMU-MOSI dataset (Zadeh et al., 2016). We im-
plemented a baseline LSTM model based off the
work done in (Chen et al., 2017). Our implemen-
tation uses 66.67% of the data for training from
which we take a 15.15% held-out set for validation,
and the remaining 33.33% is used for testing. Fi-
nally, we evaluated our proposed model against the
baseline results generated by the implementation
of (Chen et al., 2017). Here we compared our re-
sults against the various multimodal configurations
evaluating our performance using precision, recall,
and F1 scores.

5.1 Dataset and Input Modalities
The dataset that we use to explore applications of
our model is the CMU Multimodal Opinion-level
Sentiment Intensity dataset (CMU-MOSI). The

dataset contains video, audio, and transcriptions
of 89 different speakers in 93 different videos di-
vided into 2199 separate opinion sentiments. Each
video has an associated sentiment label in the range
from -3 to 3. The low end of the spectrum (-3) in-
dicates strongly negative sentiment, where as the
high end of the spectrum indicates strongly posi-
tive sentiment (+3), and ratings of 0 indicate neutral
sentiment. The CMU-MOSI dataset is currently
subject to much research (Poria et al., 2017; Chen
et al., 2017; Zadeh et al., 2018a,b) and the current
state of the art is achieved by (Poria et al., 2017)
with an F1 score of 80.3 using a context aware
model across entire videos. The state of the art us-
ing only individual segments is achieved by (Zadeh
et al., 2018a) with an F1 score of 77.3.

With respect to raw features that are being given
as inputs to our model, we perform feature extrac-
tion in the same manner as described in (Chen
et al., 2017). In the text domain, pretrained 300 di-
mensional GLoVe embeddings (Pennington et al.,
2014) were used to represent the textual tokens.
In the audio domain, low level acoustic features
including 12 Mel-frequency cepstral coefficients
(MFCCs), pitch tracking and voiced/unvoiced seg-
menting features (Drugman and Alwan, 2011), glot-
tal source parameters (Childers and Lee, 1991;
Drugman et al., 2012; Alku, 1992; Alku et al.,
1997, 2002), peak slope parameters and maxima
dispersion quotients (Kane and Gobl, 2013) were
extracted automatically using COVAREP (Degot-
tex et al., 2014). Finally, in the video domain, Facet
(iMotions, 2017) is used to extract per-frame ba-
sic and advanced emotions and facial action units
as indicators of facial muscle movement (Ekman,
1992; Ekman et al., 1980).

In situations where the same time alignment be-
tween different modalities are required, we choose
the granularity of the input to be at the level of
words. The words are aligned with audio using
P2FA (Yuan and Liberman, 2008) to get their exact
utterance times. The visual and acoustic modalities
are aligned to words using these utterance times.

5.2 Baselines

We use a LSTM model implemented in 3 differ-
ent ways (one for each different grouping of the
modalities). First in the unimodal domain, we run
sentiment regression based solely on one modality,
second in the bimodal domain we change the input
to the concatenation of any pair of modality, and



58

Method Feature BINARY (−1, +1) 7-CLASS (−3, ..., +3)Prec Recall F1 Prec Recall F1

UniModal-Baseline
Text (T) 0.77 0.76 0.76 0.32 0.35 0.33
Audio (A) 0.56 0.56 0.56 0.12 0.19 0.14
Video (V) 0.57 0.47 0.48 0.12 0.19 0.12

Table 1: Unimodal baseline results with 3 metrics: Precision, Recall and F-Score (F1)

finally in the trimodal domain we concatenate all
three modalities. This baseline not only serves to
act as a benchmark for comparing our results but
also acts as a starting point for our code develop-
ment. As such, any improvements in our metrics
are strictly as a result of the representations that
we have learned and not structural changes in our
model.

5.3 Multimodal Model Variations

Throughout our experimentation, we apply the al-
gorithms in Section 4 with several intuitive varia-
tions of how to translate modalities. Below are all
approaches that we try to maximize our chances of
learning a strong representation.

For bimodal, we translate one modality into an-
other one. For example, A→ V stands for translat-
ing from Audio to Video, and take the embedding
state, which we refer to as embed(A+V), to predict
sentiment. Here we employ the Seq2Seq Modality
Translation Model mentioned in Algorithm 1.

For trimodal, there are a lot more variations as
follows. First, since we have 3 different modal-
ity and Seq2Seq is only capable of translating
one modality to another, we use the Hierarchi-
cal Seq2Seq Modality Translation Model which
is mentioned in Algorithm 2, e.g. we trans-
late from T to A to have the joint representation
embed(T+A), and then continue the translation
from embed(T+A) to the rest modality which is
V, which in turn yields the joint representation
embed(T+A+V) to make sentiment prediction.

Second, we reuse the previous Seq2Seq Modal-
ity Translation Model to translate a concatenation
of 2 modality to the rest, e.g. concat(T+V) to
A, and vice versa, e.g. translating from A back to
concat(T+V).

Finally, we still use the Seq2Seq Modality Trans-
lation Model to translate from a concatenation of 2
modality to another concatenation of other 2. With
this setting, at least one modality is repeated, and
base on many previous works and our experience,
we tend to favor text modality (T) over the other
two and make it repeated.

6 Results

6.1 Baseline Unimodal Results

We see that with the baseline model, as shown
Table 1, the text modality is by far the most dis-
criminative when it comes to detecting emotion.
This implies that users rely heavily on their word
choice and language to convey meaning and emo-
tion. While this may be true, we know that other
works such as (Zadeh et al., 2018a; Poria et al.,
2017) have achieved higher scores by combining
all these different modalities. This implies that
with some careful thinking and pointed model con-
struction, we should be able to improve upon our
baseline unimodal results through the integration
of additional modalities into our model.

6.2 Baseline Multimodal Results

The results of our different baseline multimodal
approaches is shown in Table 2 for bimodal and
Table 3 for trimodal. We see that of the multimodal
baselines the model which combines the 3 modali-
ties of text, speech, and video performed the best.
The baseline model which combined text and au-
dio arrived in second place followed closely by the
combined text and video model. The model which
combines video and audio arrived in last place by
a significant margin. This corroborates our results
from our unimodal baselines which implied that the
text modality is the most discriminative modality
in this dataset.

On the whole we can see that when all three
modalities are working in concert we get the best
result in a multimodal context, however, it is worth
noting that we were not able to match out unimodal
baseline with our multimodal models. This implies
that there is still more to be drawn from our data
when constructing our model and there is generally
more work to be done. We believe that incorpo-
rating a stronger more robust representation of our
data will be beneficial to our later attempts at clas-
sification. Though we view this to be out of scope
of this work as the focus of this work is on learning
informative representations.



59

Method Feature BINARY (−1, +1) 7-CLASS (−3, ..., +3)Prec Recall F1 Prec Recall F1

BiModal-Baseline
concat(T + V) 0.78 0.67 0.55 0.01 0.16 0.05
concat(T + A) 0.44 0.66 0.53 0.02 0.15 0.04
concat(A + V) 0.55 0.47 0.48 0.13 0.16 0.11

BiModal-Seq2Seq

T→ V 0.67 0.67 0.67 0.26 0.22 0.22
T→ A 0.66 0.64 0.65 0.28 0.24 0.18
A→ T 0.55 0.60 0.56 0.17 0.34 0.11
A→ V 0.55 0.55 0.54 0.16 0.18 0.16
V→ T 0.58 0.58 0.58 0.05 0.16 0.08
V→ A 0.58 0.62 0.58 0.12 0.17 0.01

Table 2: Bimodal results with 3 metrics: Precision, Recall and F-Score (F1)

Method Feature BINARY (−1, +1) 7-CLASS (−3, ..., +3)Prec Recall F1 Prec Recall F1

TriModal-Baseline concat(T + V + A) 0.75 0.75 0.75 0.24 0.27 0.24

TriModal-Seq2Seq

embed(T, V)→ A 0.56 0.60 0.57 0.10 0.16 0.09
embed(T, A)→ V 0.60 0.55 0.56 0.26 0.15 0.07
embed(A, V)→ T 0.66 0.53 0.44 0.16 0.04 0.09
embed(A, T)→ V 0.59 0.51 0.52 0.13 0.15 0.09
embed(V, T)→ A 0.59 0.60 0.59 0.11 0.17 0.10
embed(V, A)→ T 0.57 0.61 0.58 0.11 0.17 0.09
concat(T, V)→ A 0.67 0.66 0.65 0.22 0.17 0.18
concat(A, T)→ V 0.54 0.55 0.63 0.19 0.15 0.21
concat(V, A)→ T 0.59 0.59 0.58 0.16 0.12 0.12
T→ concat(A, V) 0.70 0.65 0.66 0.23 0.22 0.18
A→ concat(T, V) 0.55 0.53 0.54 0.18 0.20 0.18
concat(T, A)→ concat(T, V) 0.62 0.60 0.61 0.23 0.24 0.22
concat(T, V)→ concat(T, A) 0.68 0.70 0.67 0.31 0.24 0.19

Table 3: Trimodal results with 3 metrics: Precision, Recall and F-Score (F1)

6.3 Analysis of Baseline Failure Cases

The common trend that we see among all of those
baseline models is the consistent failure to identify
extreme cases of either positive or negative emo-
tions. We believe that this phenomenon is due to
two possibilities. First we see that there are very
few cases of highly positive (+3) and highly nega-
tive (−3) examples in the training data. As a result
the models that are trained are highly biased to-
wards not selecting +3 or −3 ratings. Secondly, our
baseline models are performing categorical classi-
fication as opposed to regression or ordinal classifi-
cation. We plan to solve by training the model to
perform this type of prediction as a regression task
as opposed to a categorical classification task.

6.4 Bimodal Seq2Seq Results

Our bimodal models require the exploration of two
modalities, one for the encoding step and another
for the decoding step. We explored several differ-
ent different encoder/decoder frameworks for these
models. The first model that we explored were
representations generated from encoding exactly
one modality and then decoding exactly one dif-

ferent modality. The results of this approach are
included below in Table 2. Here we can see that the
Seq2Seq Modality Translation Model outperforms
the baseline method in terms of F1 consistently
and outperforms in terms of precision and recall in
several cases, but not all.

6.5 Trimodal Seq2Seq Results

We try all variations mentioned in Section 5.3 and
the full breakdown of these results can be found
in the Table 3. According to that, while the Hier-
archical Seq2Seq Modality Translation Model is
a natural extension to the normal Seq2Seq Modal-
ity Translation model, it does not perform well
on the CMU-MOSI dataset. Otherwise, using the
normal non-hierarchical model with concatenation
variations does improve the performance, and par-
ticularly beats the baseline (for only F1 score) on
the model which translates from concat(T,V) to
concat(T+A) for the 7-class case. As mentioned
in Section 5.3, we favor the text (T) modality and
make it repeated in this setting because it typically
contributes more significantly to sentiment predic-
tion. Indeed, we have tried to repeat video or audio



60

modality but the result shrinks dramatically.
One possible reason for this behavior is the

scarcity of training data. Given that at every
phase of Seq2Seq translation, we only have 1289
train samples, 230 validation and 269 test samples,
Seq2Seq, which typically requires more data for
training a good model, does not work efficiently.
This affects even more in the hierarchical Seq2Seq
cases where we train two phases of Seq2Seq. We
project the performance will improve if we work
on other dataset which is bigger, or if we pretrain
our model on other dataset first before applying it
to MOSI.

7 Discussion

The language modality is the most discriminative
as well as the most important towards learning mul-
timodal representations. While we outperform the
baseline multimodal approach we were unable to
outperform the baseline unimodal text approach.
Clearly from these results we know that that the
text modality is the most discriminative of all of
these modalities. However, it appears that these
models which we have described are not able to
truly separate the importance of the text modality.
The fact that we are merging these modalities into a
shared representation space is likely decreasing the
resolution of the text domain and thus decreasing
the modeling power of the domain. This is why we
believe that the top performing multimodal model
is one that incorporates the text domain so much
(see Tables 2 and 3).

It is worth noting that some of the learned rep-
resentations were quite poor when it came to their
use in prediction. For example, representations that
were learned using only audio and video generally
performed poorly. This is to be expected given the
already known information that these modalities
are not as discriminative as the language modality.
At the same time, some of the worse performing
representations were learned in the methodology
of learning a representation based on an existing
embedding. We believe this to be due to the repre-
sentation losing the resolution of the original two
domains from which the original source embedding
was learned and instead being focused on learning
the best representation to predict the final modality.

8 Future Directions

This research opens up a promising direction in
joint unsupervised learning of multimodal repre-

sentations and supervised learning of multimodal
temporal data. We propose the following exten-
sions that could improve performance:

Firstly, using an Variational Autoencoder (VAE)
(Kingma and Welling, 2013) in conjunction with
LSTM Encoder/Decoder model (as in the case
of VAE Seq2Seq model) would be an interesting
avenue to explore. This is because VAEs have
been shown to learn better representations as com-
pared to vanilla autoencoders (Kingma and Welling,
2013; Pu et al., 2016).

Secondly, since our method for multimodal rep-
resentation learning is unsupervised, we could take
advantage of larger external datasets to pre-train the
multimodal representations before fine-tuning fur-
ther with CMU-MOSI. We believe this will boost
performance because we have limited data in CMU-
MOSI for training (CMU-MOSI has 2199 training
segments). Some datasets that come to mind in-
clude the Persuasion Opinion Multimodal (POM)
dataset (Park et al., 2014) with 1000 total videos
(longer than segments) and the IEMOCAP dataset
with 10000 total segment. Since these datasets also
consist of monologue speaker videos, we expect
the learnt multimodal representations to generalize.

Thirdly, our method does not train our combined
model end to end: the representations that we use
to generated during on training run and the sen-
timent classification model are trained separately.
Exploring an end-to-end version of this model end
to end could possibly result in better performance
where we could additionally fine tune the learned
multimodal representation for sentiment analysis.

9 Conclusion

To conclude, this paper investigate the problem
of multimodal representation learning to leverage
the abundance of unlabeled multimedia data avail-
able on the internet. We presente two methods for
unsupervised learning of joint multimodal repre-
sentations using multimodal Seq2Seq models: the
Seq2Seq Modality Translation Model and the Hi-
erarchical Seq2Seq Modality Translation Model.
We found that these intermediate multimodal repre-
sentations can then be used for multimodal down-
stream tasks. Our experiments indicate that the mul-
timodal representations learned from our Seq2Seq
modality translation method are highly informative
and achieves improved performance on multimodal
sentiment analysis.



61

10 Acknowledgements

The authors are thankful to the many student peers
who commented on and critiqued this work. Spe-
cific thanks to Louis-Phillipe Morency and Amir
Zadeh for their helpful discussions and thoughtful
critiques. We are grateful to our peers who helped
us evaluate our methodology, in particular Stephen
Tsou and Kshitij Khode. Finally, we also thank the
anonymous reviewers for helpful and constructive
feedback.

References
Paavo Alku. 1992. Glottal wave analysis with

pitch synchronous iterative adaptive inverse filtering.
Speech communication 11(2-3):109–118.

Paavo Alku, Tom Bäckström, and Erkki Vilkman. 2002.
Normalized amplitude quotient for parametrization
of the glottal flow. the Journal of the Acoustical So-
ciety of America 112(2):701–710.

Paavo Alku, Helmer Strik, and Erkki Vilkman. 1997.
Parabolic spectral parameter - a new method for
quantification of the glottal flow. Speech Commu-
nication 22(1):67–79.

Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben-
gio. 2014. Neural machine translation by jointly
learning to align and translate. arXiv preprint
arXiv:1409.0473 .

Tadas Baltrusaitis, Chaitanya Ahuja, and Louis-
Philippe Morency. 2017. Multimodal machine
learning: A survey and taxonomy. CoRR
abs/1705.09406. http://arxiv.org/abs/1705.09406.

François-Régis Chaumartin. 2007. Upar7: A
knowledge-based system for headline sentiment
tagging. In Proceedings of the 4th International
Workshop on Semantic Evaluations. Associa-
tion for Computational Linguistics, Strouds-
burg, PA, USA, SemEval ’07, pages 422–425.
http://dl.acm.org/citation.cfm?id=1621474.1621568.

Minghai Chen, Sen Wang, Paul Pu Liang, Tadas Bal-
trusaitis, Amir Zadeh, and Morency Louis-Phillippe.
2017. Multimodal sentiment analysis with word-
level fusion and reinforcement learning. ICMI,
Glassgow, United Kingdom .

Donald G Childers and CK Lee. 1991. Vocal qual-
ity factors: Analysis, synthesis, and perception.
the Journal of the Acoustical Society of America
90(5):2394–2410.

Junyoung Chung, Caglar Gulcehre, Kyunghyun Cho,
and Yoshua Bengio. 2015. Gated feedback recur-
rent neural networks. In International Conference
on Machine Learning. pages 2067–2075.

Gilles Degottex, John Kane, Thomas Drugman, Tuomo
Raitio, and Stefan Scherer. 2014. Covarep - a col-
laborative voice analysis repository for speech tech-
nologies. In Acoustics, Speech and Signal Process-
ing (ICASSP), 2014 IEEE International Conference
on. IEEE, pages 960–964.

Jeff Donahue, Philipp Krähenbühl, and Trevor Darrell.
2016. Adversarial feature learning. arXiv preprint
arXiv:1605.09782 .

Thomas Drugman and Abeer Alwan. 2011. Joint ro-
bust voicing detection and pitch estimation based
on residual harmonics. In Interspeech. pages 1973–
1976.

Thomas Drugman, Mark Thomas, Jon Gudnason,
Patrick Naylor, and Thierry Dutoit. 2012. Detec-
tion of glottal closure instants from speech signals:
A quantitative review. IEEE Transactions on Audio,
Speech, and Language Processing 20(3):994–1006.

Paul Ekman. 1992. An argument for basic emotions.
Cognition & emotion 6(3-4):169–200.

Paul Ekman, Wallace V Freisen, and Sonia Ancoli.
1980. Facial signs of emotional experience. Journal
of personality and social psychology 39(6):1125.

Zhe Gan, Liqun Chen, Weiyao Wang, Yunchen Pu,
Yizhe Zhang, Hao Liu, Chunyuan Li, and Lawrence
Carin. 2017. Triangle generative adversarial net-
works. arXiv preprint arXiv:1709.06548 .

Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza,
Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron
Courville, and Yoshua Bengio. 2014. Generative ad-
versarial nets. In Advances in neural information
processing systems. pages 2672–2680.

Sepp Hochreiter and Jürgen Schmidhuber. 1997. Long
short-term memory. Neural computation 9(8):1735–
1780.

iMotions. 2017. Facial expression analysis.
goo.gl/1rh1JN.

John Kane and Christer Gobl. 2013. Wavelet maxima
dispersion for breathy to tense voice discrimination.
IEEE Transactions on Audio, Speech, and Language
Processing 21(6):1170–1179.

Lakshmish Kaushik, Abhijeet Sangwan, and John HL
Hansen. 2013. Sentiment extraction from natural au-
dio streams. In Acoustics, speech and signal pro-
cessing (icassp), 2013 ieee international conference
on. IEEE, pages 8485–8489.

Diederik P Kingma, Shakir Mohamed, Danilo Jimenez
Rezende, and Max Welling. 2014. Semi-supervised
learning with deep generative models. In Advances
in Neural Information Processing Systems. pages
3581–3589.

Diederik P Kingma and Max Welling. 2013. Auto-
encoding variational bayes. arXiv preprint
arXiv:1312.6114 .

http://arxiv.org/abs/1705.09406
http://arxiv.org/abs/1705.09406
http://arxiv.org/abs/1705.09406
http://dl.acm.org/citation.cfm?id=1621474.1621568
http://dl.acm.org/citation.cfm?id=1621474.1621568
http://dl.acm.org/citation.cfm?id=1621474.1621568
http://dl.acm.org/citation.cfm?id=1621474.1621568


62

Ryan Kiros, Ruslan Salakhutdinov, and Richard S
Zemel. 2014. Unifying visual-semantic embeddings
with multimodal neural language models. arXiv
preprint arXiv:1411.2539 .

G. Klein, Y. Kim, Y. Deng, J. Senellart, and A. M. Rush.
???? OpenNMT: Open-Source Toolkit for Neural
Machine Translation. ArXiv e-prints .

Angeliki Lazaridou, Nghia The Pham, and Marco Ba-
roni. 2015. Combining language and vision with
a multimodal skip-gram model. arXiv preprint
arXiv:1501.02598 .

Chongxuan Li, Kun Xu, Jun Zhu, and Bo Zhang. 2017.
Triple generative adversarial nets. arXiv preprint
arXiv:1703.02291 .

Minh-Thang Luong, Hieu Pham, and Christopher D
Manning. 2015. Effective approaches to attention-
based neural machine translation. arXiv preprint
arXiv:1508.04025 .

Mehdi Mirza and Simon Osindero. 2014. Condi-
tional generative adversarial nets. arXiv preprint
arXiv:1411.1784 .

Gilad Mishne et al. 2005. Experiments with mood clas-
sification in blog posts. In Proceedings of ACM SI-
GIR 2005 workshop on stylistic analysis of text for
information access. volume 19, pages 321–327.

Louis-Philippe Morency, Rada Mihalcea, and Payal
Doshi. 2011. Towards multimodal sentiment anal-
ysis: Harvesting opinions from the web. In Proceed-
ings of the 13th international conference on multi-
modal interfaces. ACM, pages 169–176.

Graham Neubig. 2017. Neural machine translation
and sequence-to-sequence models: A tutorial. arXiv
preprint arXiv:1703.01619 .

Jiquan Ngiam, Aditya Khosla, Mingyu Kim, Juhan
Nam, Honglak Lee, and Andrew Y Ng. 2011. Multi-
modal deep learning. In Proceedings of the 28th in-
ternational conference on machine learning (ICML-
11). pages 689–696.

Gaurav Pandey and Ambedkar Dukkipati. 2017. Vari-
ational methods for conditional multimodal deep
learning. In Neural Networks (IJCNN), 2017 Inter-
national Joint Conference on. IEEE, pages 308–315.

Sunghyun Park, Han Suk Shim, Moitreya Chatterjee,
Kenji Sagae, and Louis-Philippe Morency. 2014.
Computational analysis of persuasiveness in social
multimedia: A novel dataset and multimodal pre-
diction approach. In Proceedings of the 16th In-
ternational Conference on Multimodal Interaction.
ACM, New York, NY, USA, ICMI ’14, pages 50–57.
https://doi.org/10.1145/2663204.2663260.

Jeffrey Pennington, Richard Socher, and Christopher D
Manning. 2014. Glove: Global vectors for word
representation. In EMNLP. volume 14, pages 1532–
1543.

Soujanya Poria, Erik Cambria, Devamanyu Hazarika,
Navonil Majumder, Amir Zadeh, and Louis-Philippe
Morency. 2017. Context-dependent sentiment anal-
ysis in user-generated videos. In Proceedings of the
55th Annual Meeting of the Association for Compu-
tational Linguistics (Volume 1: Long Papers). vol-
ume 1, pages 873–883.

Yunchen Pu, Zhe Gan, Ricardo Henao, Xin Yuan,
Chunyuan Li, Andrew Stevens, and Lawrence Carin.
2016. Variational autoencoder for deep learning
of images, labels and captions. In D. D. Lee,
M. Sugiyama, U. V. Luxburg, I. Guyon, and R. Gar-
nett, editors, Advances in Neural Information Pro-
cessing Systems 29, Curran Associates, Inc., pages
2352–2360.

Shyam Sundar Rajagopalan, Louis-Philippe Morency,
Tadas Baltrusaitis, and Roland Goecke. 2016. Ex-
tending Long Short-Term Memory for Multi-View
Structured Learning, Springer International Publish-
ing, Cham, pages 338–353.

Verónica Pérez Rosas, Rada Mihalcea, and Louis-
Philippe Morency. 2013. Multimodal sentiment
analysis of spanish online videos. IEEE Intelligent
Systems 28(3):38–45.

Anuroop Sriram, Heewoo Jun, Sanjeev Satheesh, and
Adam Coates. 2017. Cold fusion: Training seq2seq
models together with language models. arXiv
preprint arXiv:1708.06426 .

Ilya Sutskever, Oriol Vinyals, and Quoc V Le. 2014.
Sequence to sequence learning with neural networks.
In Advances in neural information processing sys-
tems. pages 3104–3112.

Martin Wöllmer, Felix Weninger, Tobias Knaup, Björn
Schuller, Congkai Sun, Kenji Sagae, and Louis-
Philippe Morency. 2013. Youtube movie reviews:
Sentiment analysis in an audio-visual context. IEEE
Intelligent Systems 28(3):46–53.

Kelvin Xu, Jimmy Ba, Ryan Kiros, Kyunghyun Cho,
Aaron Courville, Ruslan Salakhudinov, Rich Zemel,
and Yoshua Bengio. 2015. Show, attend and tell:
Neural image caption generation with visual at-
tention. In International Conference on Machine
Learning. pages 2048–2057.

Jiahong Yuan and Mark Liberman. 2008. Speaker iden-
tification on the scotus corpus. Journal of the Acous-
tical Society of America 123(5):3878.

Amir Zadeh, Minghai Chen, Soujanya Poria, Erik Cam-
bria, and Louis-Philippe Morency. 2017. Tensor
fusion network for multimodal sentiment analysis.
In Proceedings of the 2017 Conference on Empiri-
cal Methods in Natural Language Processing. pages
1114–1125.

Amir Zadeh, Paul Pu Liang, Navonil Mazumder,
Soujanya Poria, Erik Cambria, and Louis-Philippe
Morency. 2018a. Memory fusion network for
multi-view sequential learning. arXiv preprint
arXiv:1802.00927 .

https://doi.org/10.1145/2663204.2663260
https://doi.org/10.1145/2663204.2663260
https://doi.org/10.1145/2663204.2663260
https://doi.org/10.1145/2663204.2663260


63

Amir Zadeh, Paul Pu Liang, Soujanya Poria, Pra-
teek Vij, Erik Cambria, and Louis-Philippe Morency.
2018b. Multi-attention recurrent network for hu-
man communication comprehension. arXiv preprint
arXiv:1802.00923 .

Amir Zadeh, Paul Pu Liang, Jon Vanbriesen, Soujanya
Poria, Erik Cambria, Minghai Chen, and Louis-
Philippe Morency. 2018c. Multimodal language
analysis in the wild: Cmu-mosei dataset and inter-
pretable dynamic fusion graph. In Association for
Computational Linguistics (ACL).

Amir Zadeh, Rowan Zellers, Eli Pincus, and Louis-
Philippe Morency. 2016. Multimodal sentiment in-
tensity analysis in videos: Facial gestures and verbal
messages. IEEE Intelligent Systems 31(6):82–88.


