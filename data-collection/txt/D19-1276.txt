











































Specializing Word Embeddings (for Parsing) by Information Bottleneck


Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing
and the 9th International Joint Conference on Natural Language Processing, pages 2744–2754,
Hong Kong, China, November 3–7, 2019. c©2019 Association for Computational Linguistics

2744

Specializing Word Embeddings (for Parsing) by Information Bottleneck

Xiang Lisa Li

Department of Computer Science
Johns Hopkins University
xli150@jhu.edu

Jason Eisner

Department of Computer Science
Johns Hopkins University
jason@cs.jhu.edu

Abstract

Pre-trained word embeddings like ELMo and
BERT contain rich syntactic and semantic in-
formation, resulting in state-of-the-art perfor-
mance on various tasks. We propose a very
fast variational information bottleneck (VIB)
method to nonlinearly compress these embed-
dings, keeping only the information that helps
a discriminative parser. We compress each
word embedding to either a discrete tag or a
continuous vector. In the discrete version, our
automatically compressed tags form an alter-
native tag set: we show experimentally that
our tags capture most of the information in tra-
ditional POS tag annotations, but our tag se-
quences can be parsed more accurately at the
same level of tag granularity. In the continu-
ous version, we show experimentally that mod-
erately compressing the word embeddings by
our method yields a more accurate parser in 8
of 9 languages, unlike simple dimensionality
reduction.

1 Introduction

Word embedding systems like BERT and ELMo
use spelling and context to obtain contextual em-
beddings of word tokens. These systems are trained
on large corpora in a task-independent way. The
resulting embeddings have proved to then be useful
for both syntactic and semantic tasks, with different
layers of ELMo or BERT being somewhat special-
ized to different kinds of tasks (Peters et al., 2018b;
Goldberg, 2019). State-of-the-art performance on
many NLP tasks can be obtained by fine-tuning,
i.e., back-propagating task loss all the way back
into the embedding function (Peters et al., 2018a;
Devlin et al., 2018).

In this paper, we explore what task-specific in-
formation appears in the embeddings before fine-
tuning takes place. We focus on the task of de-
pendency parsing, but our method can be easily

sentence  w

embeddings  x

taggings  t

tree  y

decoder

encoder

ELMo

Ollivander   sold        the      wand        .  

dobjsubj
det

punct

Parser

Figure 1: Our instantiation of the information bottle-
neck, with bottleneck variable T . A jagged arrow indi-
cates a stochastic mapping, i.e. the jagged arrow points
from the parameters of a distribution to a sample drawn
from that distribution.

extended to other syntactic or semantic tasks. Our
method compresses the embeddings by extracting
just their syntactic properties—specifically, the in-
formation needed to reconstruct parse trees (be-
cause that is our task). Our nonlinear, stochas-
tic compression function is explicitly trained by
variational information bottleneck (VIB) to forget
task-irrelevant information. This is reminiscent
of canonical correspondence analysis (Anderson,
2003), a method for reducing the dimensionality of
an input vector so that it remains predictive of an
output vector, although we are predicting an out-
put tree instead. However, VIB goes beyond mere
dimensionality reduction to a fixed lower dimen-
sionality, since it also avoids unnecessary use of
the dimensions that are available in the compressed
representation, blurring unneeded capacity via ran-
domness. The effective number of dimensions may
therefore vary from token to token. For example, a
parser may be content to know about an adjective
token only that it is adjectival, whereas to find the
dependents of a verb token, it may need to know



2745

the verb’s number and transitivity, and to attach a
preposition token, it may need to know the identity
of the preposition.

We try compressing to both discrete and contin-
uous task-specific representations. Discrete rep-
resentations yield an interpretable clustering of
words. We also extend information bottleneck to
allow us to control the contextual specificity of the
token embeddings, making them more like type
embeddings.

This specialization method is complementary to
the previous fine-tuning approach. Fine-tuning in-
troduces new information into word embeddings by
backpropagating the loss, whereas the VIB method
learns to exploit the existing information found by
the ELMo or BERT language model. VIB also has
less capacity and less danger of overfitting, since it
fits fewer parameters than fine-tuning (which in the
case of BERT has the freedom to adjust the embed-
dings of all words and word pieces, even those that
are rare in the supervised fine-tuning data). VIB is
also very fast to train on a single GPU.

We discover that our syntactically specialized
embeddings are predictive of the gold POS tags
in the setting of few-shot-learning, validating the
intuition that a POS tag summarizes a word token’s
syntactic properties. However, our representations
are tuned explicitly for discriminative parsing, so
they prove to be even more useful for this task than
POS tags, even at the same level of granularity.
They are also more useful than the uncompressed
ELMo representations, when it comes to generaliz-
ing to test data. (The first comparison uses discrete
tags, and the second uses continuous tags.)

2 Background: Information Bottleneck

The information bottleneck (IB) method originated
in information theory and has been adopted by the
machine learning community as a training objective
(Tishby et al., 2000) and a theoretical framework
for analyzing deep neural networks (Tishby and
Zaslavsky, 2015).

Let X represent an “input” random variable such
as a sentence, and Y represent a correlated “out-
put” random variable such as a parse. Suppose we
know the joint distribution p(X,Y ). (In practice, we
will use the empirical distribution over a sample of
(x, y) pairs.) Our goal is to learn a stochastic map
p✓(t | x) from X to some compressed representa-
tion T , which in our setting will be something like
a tag sequence. IB seeks to minimize

LIB = � I(Y ;T) + � · I(X;T) (1)
where I(·; ·) is the mutual information.1 A low
loss means that T does not retain very much in-
formation about X (the second term), while still
retaining enough information to predict Y .2 The
balance between the two MI terms is controlled
by a Lagrange multiplier �. By increasing �, we
increase the pressure to keep I(X;T) small, which
“narrows the bottleneck” by favoring compression
over predictive accuracy I(Y ;T). Regarding � as
a Lagrange multiplier, we see that the goal of IB
is to maximize the predictive power of T subject
to some constraint on the amount of information
about X that T carries. If the map from X to T were
deterministic, then it could lose information only
by being non-injective: the traditional example is
dimensionality reduction, as in the encoder of an
encoder-decoder neural net. But IB works even if
T can take values throughout a high-dimensional
space, because the randomness in p✓(t | x) means
that T is noisy in a way that wipes out information
about X . Using a high-dimensional space is de-
sirable because it permits the amount of effective
dimensionality reduction to vary, with T perhaps
retaining much more information about some x
values than others, as long as the average retained
information I(X;T) is small.

3 Formal Model

In this paper, we extend the original IB objective
(1) and add terms I(Ti; X | X̂i) to control the context-
sensitivity of the extracted tags. Here Ti is the tag
associated with the ith word, Xi is the ELMo token
embedding of the ith word, and X̂i is the same
word’s ELMo type embedding (before context is
incorporated).

LIB = � I(Y ;T)+� I(X;T)+�
nX
i=1

I(Ti; X | X̂i) (2)

In this section, we will explain the motivation for
the additional term and how to efficiently estimate
variational bounds on all terms (lower bound for
I(Y ;T) and upper bound for the rest).3

1In our IB notation, larger � means more compression.
Note that there is another version of IB that puts � as the
coefficient in front of I(Y ;T): LIB = �� · I(Y ;T) + I(X;T)
The two versions are equivalent.

2 Since T is a stochastic function of X with no access to Y ,
it obviously cannot convey more information about Y than the
uncompressed input X does. As a result, Y is independent of
T given X , as in the graphical model T ! X ! Y .

3Traditional Shannon entropy H(·) is defined on discrete
variables. In the case of continuous variables, we interpret H



2746

We instantiate the variational IB (VIB) estima-
tion method (Alemi et al., 2016) on our depen-
dency parsing task, as illustrated in Figure 1. We
compress a sentence’s word embeddings Xi into
continuous vector-valued tags or discrete tags Ti
(“encoding”) such that the tag sequence T retains
maximum ability to predict the dependency parse Y
(“decoding”). Our chosen architecture compresses
each Xi independently using the same stochastic,
information-losing transformation.

The IB method introduces the new random vari-
able T , the tag sequence that compresses X , by
defining the conditional distribution p✓(t | x). In
our setting, p✓ is a stochastic tagger, for which
we will adopt a parametric form (§3.1 below). Its
parameters ✓ are chosen to minimize the IB objec-
tive (2). By IB’s independence assumption,2 the
joint probability can be factored as p✓(x, y, t) =
p(x) · p(y | x) · p✓(t | x).

3.1 I(X;T) — the Token Encoder p✓(t | x)
Under this distribution, I(X;T) def=
Ex,t [log p✓ (t |x)p✓ (t) ] = Ex [Et⇠p✓ (t |x) [log

p✓ (t |x)
p✓ (t) ]].

Making this term small yields a representation
T that, on average, retains little information
about X . The outer expectation is over the true
distribution of sentences x; we use an empirical
estimate, averaging over the unparsed sentences
in a dependency treebank. To estimate the inner
expectation, we could sample, drawing taggings t
from p✓(t | x).

We must also compute the quantities within the
inner brackets. The p✓(t | x) term is defined by
our parametric form. The troublesome term is
p✓(t) = Ex0 [p✓(t | x 0)], since even estimating it
from a treebank requires an inner loop over tree-
bank sentences x 0. To avoid this, variational IB
replaces p✓(t) with some variational distribution
r (t). This can only increase our objective func-
tion, since the difference between the variational
and original versions of this term is a KL diver-
gence and hence non-negative:

upper boundz                      }|                      {
E
x
[ E
t⇠p✓ (t |x)

[log p✓(t |x)
r (t)

]]�

I(X;T )z                       }|                       {
E
x
[ E
t⇠p✓ (t |x)

[log p✓(t | x)
p✓(t)

]]

= E
x
[KL(p✓(t) | | r (t))] � 0

to instead denote differential entropy (which would be �1
for discrete variables). Scaling a continuous random variable
affects its differential entropy—but not its mutual information
with another random variable, which is what we use here.

Thus, the variational version (the first term above)
is indeed an upper bound for I(X;T) (the second
term above). We will minimize this upper bound
by adjusting not only ✓ but also  , thus making the
bound as tight as possible given ✓. Also we will no
longer need to sample t for the inner expectation
of the upper bound, Et⇠p✓ (t |x) [log

p✓ (t |x)
r (t) ], because

this expectation equals KL[p✓(t | x) | | r (t)], and
we will define the parametric p✓ and r so that this
KL divergence can be computed exactly: see §4.

3.2 Two Token Encoder Architectures

We choose to define p✓(t | x) =
Qn

i=1 p✓(ti | xi).
That is, our stochastic encoder will compress each
word xi individually (although xi is itself a rep-
resentation that depends on context): see Fig-
ure 1. We make this choice not for computational
reasons—our method would remain tractable even
without this—but because our goal in this paper is
to find the syntactic information in each individual
ELMo token embedding (a goal we will further
pursue in §3.3 below).

To obtain continuous tags, define p✓(ti | xi) such
that ti 2 Rd is Gaussian-distributed with mean vec-
tor and diagonal covariance matrix computed from
the ELMo word vector xi via a feedforward neural
network with 2d outputs and no transfer function
at the output layer. To ensure positive semidefinite-
ness of the diagonal covariance matrix, we squared
the latter d outputs to obtain the diagonal entries.4

Alternatively, to obtain discrete tags, define
p✓(ti | xi) such that ti 2 {1, . . . , k} follows a soft-
max distribution, where the k softmax parameters
are similarly computed by a feedforward network
with k outputs and no transfer function at the output
layer.

We similarly define r (t) =
Qn

i=1 r (ti), where  
directly specifies the 2d or k values corresponding
to the output layer above (since there is no input xi
to condition on).

3.3 I(Ti; X | X̂i) — the Type Encoder s⇠ (ti | x̂i)
While the IB objective (1) asks each tag ti to be
informative about the parse Y , we were concerned
that it might not be interpretable as a tag of word i
specifically. Given ELMo or any other black-box
conversion of a length-n sentence to a sequence of
contextual vectors x1, . . . , xn, it is possible that xi

4Our restriction to diagonal covariance matrices follows
Alemi et al. (2016). In pilot experiments that dropped this
restriction, we found learning to be numerically unstable, al-
though that generalization is reasonable in principle.



2747

contains not only information about word i but also
information describing word i + 1, say, or the syn-
tactic constructions in the vicinity of word i. Thus,
while p✓(ti | xi) might extract some information
from xi that is very useful for parsing, there is no
guarantee that this information came from word i
and not its neighbors. Although we do want tag ti
to consider context—e.g., to distinguish between
noun and verb uses of word i—we want “most” of
ti’s information to come from word i itself. Specif-
ically, it should come from ELMo’s level-0 em-
bedding of word i, denoted by x̂i—a word type
embedding that does not depend on context.

To penalize Ti for capturing “too much” contex-
tual information, our modified objective (2) adds
a penalty term � · I(Ti; X | X̂i), which measures the
amount of information about Ti given by the sen-
tence X as a whole, beyond what is given by X̂i:
I(Ti; X | X̂i)

def
= Ex [Eti⇠p✓ (ti |x) [log

p✓ (ti |x)
p✓ (ti | x̂i ) ]]. Set-

ting � > 0 will reduce this contextual information.
In practice, we found that I(Ti; X | X̂i) was small

even when � = 0, on the order of 3.5 nats whereas
I(Ti; X) was 50 nats. In other words, the tags ex-
tracted by the classical method were already fairly
local, so increasing � above 0 had little qualitative
effect. Still, � might be important when applying
our method to ELMo’s competitors such as BERT.

We can derive an upper bound on I(Ti; X | X̂i) by
approximating the conditional distribution p✓(ti |
x̂i) with a variational distribution s⇠ (ti | x̂i), similar
to §3.1.

upper boundz                        }|                        {
E
x
[ E
ti⇠p✓ (ti |x)

[log p✓(ti |x)
s⇠ (ti | x̂i)

]]�

I(Ti ;X |X̂i )z                        }|                        {
E
x
[ E
ti⇠p✓ (ti |x)

[log p✓(ti |x)
p✓(ti | x̂i)

]]

= E
x
[KL(p✓(ti | x̂i) | | s⇠ (ti | x̂i))] � 0

We replace it in (2) with this upper bound, which
is equal to Ex [

Pn
i=1 KL[p✓(ti |x) | | s⇠ (ti | x̂i)]].

The formal presentation above does not assume
the specific factored model that we adopted in §3.2.
When we adopt that model, p✓(ti | x) above re-
duces to p✓(ti | xi)—but our method in this section
still has an effect, because xi still reflects the con-
text of the full sentence whereas x̂i does not.

Type Encoder Architectures Notice that s⇠ (ti |
x̂i) may be regarded as a type encoder, with param-
eters ⇠ that are distinct from the parameters ✓ of our
token encoder p✓(ti | xi). Given a choice of neural
architecture for p✓(ti | xi) (see §3.2), we always
use the same architecture for s⇠ (ti | x̂i), except that

p✓ takes a token vector as input whereas s⇠ takes a
context-independent type vector. s⇠ is not used at
test time, but only as part of our training objective.

3.4 I(Y ;T) — the Decoder q�(y | t)

Finally, I(Y ;T) def= Ey,t⇠p✓ [log
p✓ (y |t)
p(y) ]. The p(y)

can be omitted during optimization as it does not
depend on ✓. Thus, making I(Y ;T) large tries to
obtain a high log-probability p✓(y | t) for the true
parse y when reconstructing it from t alone.

But how do we compute p✓(y | t)? This quantity
effectively marginalizes over possible sentences x
that could have explained t. Recall that p✓ is a joint
distribution over x, y, t: see just above §3.1. So
p✓(y | t)

def
=

P
x p✓ (x,y,t)P

x,y0 p✓ (x,y0,t) . To estimate these sums
accurately, we would have to identify the sentences
x that are most consistent with the tagging t (that is,
p(x) · p✓(t |x) is large): these contribute the largest
summands, but might not appear in any corpus.

To avoid this, we replace p✓(y | t) with a varia-
tional approximation q�(y | t) in our formula for
I(Y ;T). Here q�(· | ·) is a tractable conditional
distribution, and may be regarded as a stochastic
parser that runs on a compressed tag sequence t
instead of a word embedding sequence x. This
modified version of I(Y ;T) forms a lower bound
on I(Y ;T), for any value of the variational parame-
ters �, since the difference between them is a KL
divergence and hence positive:

I(Y ;T )z             }|             {
E

y,t⇠p✓
[log p✓ (y |t)p(y) ]�

lower boundz             }|             {
E

y,t⇠p✓
[log q� (y |t)p(y) ]

= E
t⇠p✓

[KL(p✓(y | t) | | q�(y | t))] � 0

We will maximize this lower bound of I(Y ;T) with
respect to both ✓ and �. For any given ✓, the op-
timal � minimizes the expected KL divergence,
meaning that q� approximates p✓ well.

More precisely, we again drop p(y) as constant
and then maximize a sampling-based estimate of
Ey,t⇠p✓ [log q�(y |t)]. To sample y, t from the joint
p✓(x, y, t) we must first sample x, so we rewrite
as Ex,y [Et⇠p✓ (t |x) [log q�(y |t)]]. The outer expec-
tation Ex,y is estimated as usual over a training tree-
bank. The expectation Et⇠p✓ (t |x) recognizes that t
is stochastic, and again we estimate it by sampling.
In short, when t is a stochastic compression of a
treebank sentence x, we would like our variational
parser on average to assign high log-probability
q�(y | t) to its treebank parse y.



2748

Decoder Architecture We use the deep biaffine
dependency parser (Dozat and Manning, 2016) as
our variational distribution q�(y | t), which func-
tions as the decoder. This parser uses a Bi-LSTM
to extract features from compressed tags or vec-
tors and assign scores to each tree edge, setting
q�(y | t) proportional to the exp of the total score
of all edges in y. During IB training, the code5

computes only an approximation to q�(y |t) for the
gold tree y (although in principle, it could have
computed the exact normalizing constant in poly-
time with Tutte’s matrix-tree theorem (Smith and
Smith, 2007; Koo et al., 2007; McDonald and Satta,
2007)). When we test the parser, the code does ex-
actly find argmaxy q�(y | t) via the directed span-
ning tree algorithm of Edmonds (1966).

4 Training and Inference

With the approximations in §3, our final minimiza-
tion objective is this upper bound on (2):

E
x,y

h
E

t⇠p✓ (t |x)
[� log q�(y |t)] + �KL(p✓(t |x)| |r (t))

+ �
nX
i=1

KL(p✓(ti | x) | | s⇠ (ti | x̂i))
i

(3)

We apply stochastic gradient descent to optimize
this objective. To get a stochastic estimate of the
objective, we first sample some (x, y) from the
treebank. We then have many expectations over
t ⇠ p✓(t | x), including the KL terms. We could es-
timate these by sampling t from the token encoder
p✓(t | x) and then evaluating all q�, p✓, r , and s⇠
probabilities. However, in fact we use the sampled t
only to estimate the first expectation (by computing
the decoder probability q�(y | t) of the gold tree y);
we can compute the KL terms exactly by exploit-
ing the structure of our distributions. The structure
of p✓ and r means that the first KL term decom-
poses into

Pn
i=1 KL(p✓(ti |xi)| |r (ti)). All KL terms

are now between either two Gaussian distributions
over a continuous tagset6 or two categorical distri-
butions over a small discrete tagset.7

To compute the stochastic gradient, we run back-
propagation on this computation. We must ap-
ply the reparametrization trick to backpropagate

5We use the implementation from AllenNLP library (Gard-
ner et al., 2017).

6
KL(N0 | | N1) = 12 (tr(⌃

�1
1
⌃0)+(µ1�µ0)T⌃�11 (µ1�µ0)�

d + log( det(⌃1)
det(⌃0) )

7
KL(p✓ (ti |xi)| |r (ti)) =

Pk
ti=1

p✓ (ti | xi) log p✓ (ti |xi )r (ti )

Language Treebank #Tokens H(A | X̂) H(A)
Arabic PADT 282k 0.059 2.059
Chinese GSD 123k 0.162 2.201
English EWT 254k 0.216 2.494
French GSD 400k 0.106 2.335
Hindi HDTB 351k 0.146 2.261
Portuguese Bosque 319k 0.179 2.305
Russian GSD 98k 0.049 2.132
Spanish AnCora 549k 0.108 2.347
Italian ISDT 298K 0.120 2.304

Table 1: Statistics of the datasets used in this paper.
“Treebank” is the treebank identifier in UD, “#Token”
is the number of tokens in the treebank, “H(A)” is the
entropy of a gold POS tag (in nats), and “H(A | X̂)” is
the conditional entropy of a gold POS tag conditioned
on a word type (in nats).

through the step that sampled t. This finds the
gradient of parameters that derive t from a ran-
dom variate z, while holding z itself fixed. For
continuous t, we use the reparametrization trick
for multivariate Gaussians (Rezende et al., 2014).
For discrete t, we use the Gumbel-softmax variant
(Jang et al., 2016; Maddison et al., 2017).

To evaluate our trained model’s ability to parse a
sentence x from compressed tags, we obtain a parse
as argmaxy q�(y | t), where t ⇠ p✓(· | x) is a sin-
gle sample. A better parser would instead estimate
argmaxy Et [q�(y | t)] where Et averages over
many samples t, but this is computationally hard.

5 Experimental Setup

Data Throughout §§6–7, we will examine our
compressed tags on a subset of Universal Depen-
dencies (Nivre et al., 2018), or UD, a collection of
dependency treebanks across 76 languages using
the same POS tags and dependency labels. We ex-
periment on Arabic, Hindi, English, French, Span-
ish, Portuguese, Russian, Italian, and Chinese (Ta-
ble 1)—languages with different syntactic proper-
ties like word order. We use only the sentences with
length  30. For each sentence, x is obtained by
running the standard pre-trained ELMo on the UD
token sequence (although UD’s tokenization may
not perfectly match that of ELMo’s training data),
and y is the labeled UD dependency parse without
any part-of-speech (POS) tags. Thus, our tags t are
tuned to predict only the dependency relations in
UD, and not the gold POS tags a also in UD.

Pretrained Word Embeddings For English, we
used the pre-trained English ELMo model from the
AllenNLP library (Gardner et al., 2017). For the



2749

other 8 languages, we used the pre-trained models
from Che et al. (2018). Recall that ELMo has two
layers of bidirectional LSTM (layer 1 and 2) built
upon a context-independent character CNN (layer
0). We use either layer 1 or 2 as the input (xi) to
our token encoder p✓ . Layer 0 is the input (x̂i) to
our type encoder s⇠ . Each encoder network (§§3.2–
3.3) has a single hidden layer with a tanh transfer
function, which has 2d hidden units (typically 128
or 512) for continuous encodings and 512 hidden
units for discrete encodings.

Optimization We optimize with Adam (Kingma
and Ba, 2014), a variant of stochastic gradient de-
scent. We alternate between improving the model
p✓(t |x) on even epochs and the variational distribu-
tions q�(y |t), r (t), s⇠ (ti | x̂i) on odd epochs.

We train for 50 epochs with minibatches of size
20 and L2 regularization. The learning rate and the
regularization coefficients are tuned on dev data
for each language separately. For each training
sentence, we average over 5 i.i.d. samples of T
to reduce the variance of the stochastic gradient.
The initial parameters ✓, �, , ⇠ are all drawn from
N(0, I). We experiment with different dimensional-
ities d 2 {5, 32, 256, 512} for the continuous tags,
and different cardinalities k 2 {32, 64, 128} for
the discrete tag set. We also tried different values
�, � 2 {10�6, 10�5, · · · , 101} of the compression
tradeoff parameter. We use temperature annealing
when sampling from the Gumbel-softmax distribu-
tion (§4). At training epoch i, we use temperature
⌧i, where ⌧1 = 5 and ⌧i+1 = max(0.5, e��⌧i). We
set the annealing rate � = 0.1. During testing, we
use ⌧ = 0, which gives exact softmax sampling.

6 Scientific Evaluation

In this section, we study what information about
words is retained by our automatically constructed
tagging schemes. First, we show the relationship
between I(Y ;T) and I(X;T) on English as we re-
duce � to capture more information in our tags.8

Second, across 9 languages, we study how our
automatic tags correlate with gold part-of-speech
tags (and in English, with other syntactic proper-
ties), while suppressing information about semantic
properties. We also show how decreasing � grad-
ually refines the automatic discrete tag set, giving
intuitive fine-grained clusters of English words.

8We always set � = � to simplify the experimental design.

(a) Discrete Version

(b) Continuous Version
Figure 2: Compression-prediction tradeoff curves of
VIB in our dependency parsing setting. The upper fig-
ures use discrete tags, while the lower figures use con-
tinuous tags. The dashed lines are for test data, and the
solid lines for training data. The “dim” in the legends
means the dimensionality of the continuous tag vector
or the cardinality of the discrete tag set. On the left, we
plot predictiveness I(Y ;T) versus I(X;T) as we lower
� multiplicatively from 101 to 10�6 on a log-scale. On
the right, we alter the y-axis to show the labeled attach-
ment score (LAS) of 1-best dependency parsing. All
mutual information and entropy values in this paper are
reported in nats per token. Furthermore, the mutual in-
formation values that we report are actually our varia-
tional upper bounds, as described in §3. The reason that
I(X;T) is so large for continuous tags is that it is differ-
ential mutual information (see footnote 3). Additional
tradeoff curves w.r.t. I(Ti; X | X̂i) are in Appendix B.

6.1 Tradeoff Curves

As we lower � to retain more information about X ,
both I(X;T) and I(Y ;T) rise, as shown in Figure 2.
There are diminishing returns: after some point, the
additional information retained in T does not con-
tribute much to predicting Y . Also noteworthy is
that at each level of I(X,T), very low-dimensional
tags (d = 5) perform on par with high-dimensional
ones (d = 256). (Note that the high-dimensional
stochastic tags will be noisier to keep the same
I(X,T).) The low-dimensional tags allow far faster
CPU parsing. This indicates that VIB can achieve
strong practical task-specific compression.

6.2 Learned Tags vs. Gold POS Tags

We investigate how our automatic tag Ti correlates
with the gold POS tag Ai provided by UD.



2750

(a) ELMo, I(X;T) = H(X) ⇡ 400.6 (b) I(X;T) ⇡ 24.3 (c) I(X;T) ⇡ 0.069

Figure 3: t-SNE visualization of VIB model (d = 256) on the projected space of the continuous tags. Each marker
in the figure represents a word token, colored by its gold POS tag. This series of figures (from left to right) shows
a progression from no compression to moderate compression and to too-much compression.

Continuous Version We use t-SNE (van der
Maaten and Hinton, 2008) to visualize our com-
pressed continuous tags on held-out test data, col-
oring each token in Figure 3 according to its gold
POS tag. (Similar plots for the discrete tags are in
Figure 6 in the appendix.)

In Figure 3, the first figure shows the original
uncompressed level-1 ELMo embeddings of the
tokens in test data. In the two-dimensional visu-
alization, the POS tags are vaguely clustered but
the boundaries merge together and some tags are
diffuse. The second figure is when � = 10�3 (mod-
erate compression): our compressed embeddings
show clear clusters that correspond well to gold
POS tags. Note that the gold POS tags were not
used in training either ELMo or our method. The
third figure is when � = 1 (too much compression),
when POS information is largely lost. An interest-
ing observation is that the purple NOUN and blue
PROPN distributions overlap in the middle distribu-
tion, meaning that it was unnecessary to distinguish
common nouns from proper nouns for purposes of
our parsing task.9

Discrete Version We also quantify how well our
specialized discrete tags capture the traditional
POS categories, by investigating I(A;T). This
can be written as H(A) � H(A | T). Similarly
to §3.4, our probability distribution has the form
p✓(x, a, t) = p(x, a) · p✓(t | x), leading us to write
H(A | T)  Ex,a [Et⇠p✓ (t |x) [� log q(a | t)]] where
q(a | t) = Qi q(ai | ti) is a variational distribu-
tion that we train to minimize this upper bound.
This is equivalent to training q(a | t) by maximum
conditional likelihood. In effect, we are doing trans-
fer learning, fixing our trained IB encoder (p✓ ) and
now using it to predict A instead of Y , but otherwise

9Both can serve as arguments of verbs and prepositions.
Both can be modified by determiners and adjectives, giving
rise to proper NPs like “The Daily Tribune.”

following §3.4. We similarly upper-bound H(A) by
assuming a model q0(a) = Qi q0(ai) and estimat-
ing q0 as the empirical distribution over training
tags. Having trained q and q0 on training data, we
estimate H(A | T) and H(A) using the same upper-
bound formulas on our test data.

We experiment on all 9 languages, taking Ti at
the moderate compression level � = 0.001, k = 64.
As Figure 4 shows, averaging over the 9 languages,
the reconstruction retains 71% of POS information
(and as high as 80% on Spanish and French). We
can conclude that the information encoded in the
specialized tags correlates with the gold POS tags,
but does not perfectly predict the POS.

The graph in Figure 4 shows a “U-shaped” curve,
with the best overall error rate at � = 0.01. That is,
moderate compression of ELMo embeddings helps
for predicting POS tags. Too much compression
squeezes out POS-related information, while too
little compression allows the tagger to overfit the
training data, harming generalization to test data.
We will see the same pattern for parsing in §7.

Syntactic Features As a quick check, we deter-
mine that our tags also make syntactic distinctions
beyond those that are recognized by the UD POS
tag set, such as tense, number, and transitivity. See
Appendix D for graphs. For example, even with
moderate compression, we achieve 0.87 classifica-
tion accuracy in distinguishing between transitive
and intransitive English verbs, given only tag ti.

Stem When we compress ELMo embeddings to
k discrete tags, the semantic information must be
squeezed out because k is small. But what about
the continuous case? In order to verify that seman-
tic information is excluded, we train a classifier
that predicts the stem of word token i from its
mean tag vector E [Ti]. We expect “player” and
“buyer” to have similar compressed vectors, be-
cause they share syntactic roles, but we should fail



2751

embeddings Arabic English Spanish French Hindi Italian Portuguese Russian Chinese
H(A) 2.016 2.486 2.345 2.206 2.247 2.291 2.306 2.131 2.195
ELMo0 67.2% 74.2% 75.7% 79.6% 70.1% 77.9% 76.5% 73.2% 57.3%
ELMo1 67.2% 76.1% 71.7% 78.0% 70.5% 78.1% 72.3% 73.8% 59.8%
ELMo2 63.8% 71.0% 79.7% 78.7% 67.2% 74.5% 75.3% 72.2% 59.4%

Figure 4: Graph at left: I(A;T) vs. I(X;T) in English (in units of nats per token). Table at right: how well the
discrete specialized tags predict gold POS tags for 9 languages. The H(A) row is the entropy (in nats per token)
of the gold POS tags in the test data corpus, which is an upper bound for I(A;T). The remaining rows report the
percentage I(A;T)/H(A).

to predict that they have different stems “play” and
“buy.” The classifier is a feedforward neural net-
work with tanh activation function, and the last
layer is a softmax over the stem vocabulary. In
the English treebank, we take the word lemma in
UD treebank and use the NLTK library (Bird et al.,
2009) to stem each lemma token. Our result (Ap-
pendix E in the appendix) suggests that more com-
pression destroys stem information, as hoped. With
light compression, the error rate on stem prediction
can be below 15%. With moderate compression
� = 0.01, the error rate is 89% for ELMo layer 2
and 66% for ELMo layer 1. Other languages show
the same pattern, as shown in Appendix E in the
appendix. Thus, moderate and heavy compression
indeed squeeze out semantic information.

6.3 Annealing of Discrete Tags

Deterministic annealing (Rose, 1998; Friedman
et al., 2001) is a method that gradually decreases �
during training of IB. Each token i has a stochastic
distribution over the possible tags {1, . . . , k}. This
can be regarded as a soft clustering where each
token is fractionally associated with each of the k
clusters. With high �, the optimal solution turns
out to assign to all tokens an identical distribution
over clusters, for a mutual information of 0. Since
all clusters then have the same membership, this is
equivalent to having a single cluster. As we gradu-
ally reduce �, the cluster eventually splits. Further
reduction of � leads to recursive splitting, yielding
a hierarchical clustering of tokens (Appendix A).

We apply deterministic annealing to the English
dataset, and the resulting hierarchical structure re-
flects properties of English syntax. At the top of the
hierarchy, the model places nouns, adjectives, ad-
verbs, and verbs in different clusters. At lower lev-
els, the anaphors (“yourself,” “herself” . . . ), posses-
sive pronouns (“his,” “my,” “their” . . . ), accusative-
case pronouns (“them,” “me,” “him,” “myself” . . . ),
and nominative-case pronouns (“I,” “they,” “we”

. . . ) each form a cluster, as do the wh-words (“why,”
“how,” “which,” “who,” “what,” . . . ).

7 Engineering Evaluation

As we noted in §1, learning how to compress
ELMo’s tags for a given task is a fast alternative
to fine-tuning all the ELMo parameters. We find
that indeed, training a compression method to keep
only the relevant information does improve our
generalization performance on the parsing task.

We compare 6 different token representations ac-
cording to the test accuracy of a dependency parser
trained to use them. The same training data is used
to jointly train the parser and the token encoder that
produces the parser’s input representations.
Continuous tags:

Iden is an baseline model that leaves the ELMo
embeddings uncompressed, so d = 1024.
PCA is a baseline that simply uses Principal Com-
ponents Analysis to reduce the dimensionality to
d = 256. Again, this is not task-specific.
MLP is another deterministic baseline that uses a
multi-layer perceptron (as in Dozat and Manning
(2016)) to reduce the dimensionality to d = 256 in
a task-specific and nonlinear way. This is identical
to our continuous VIB method except that the vari-
ance of the output Gaussians is fixed to 0, so that
the d dimensions are fully informative.
VIBc uses our stochastic encoder, still with d =
256. The average amount of stochastic noise is
controlled by �, which is tuned per-language on
dev data.
Discrete tags:

POS is a baseline that uses the k  17 gold POS
tags from the UD dataset.
VIBd is our stochastic method with k = 64 tags.
To compare fairly with POS, we pick a � value for
each language such that H(Ti | Xi) ⇡ H(Ai | Xi).

Runtime. Our VIB approach is quite fast. With
minibatching on a single GPU, it is able to train on
10,000 sentences in 100 seconds, per epoch.



2752

Models Arabic Hindi English French Spanish Portuguese Russian Chinese Italian
Iden 0.751 0.870 0.824 0.784 0.808 0.813 0.783 0.709 0.863
PCA 0.743 0.866 0.823 0.749 0.802 0.808 0.777 0.697 0.857
MLP 0.759 0.871 0.839 0.816 0.835 0.821 0.800 0.734 0.867
VIBc 0.779 0.866 0.851 0.828 0.837 0.836 0.814 0.754 0.867

POS 0.652 0.713 0.712 0.718 0.739 0.743 0.662 0.510 0.779
VIBd 0.672 0.736 0.742 0.723 0.725 0.710 0.651 0.591 0.781

Table 2: Parsing accuracy of 9 languages (LAS). Black rows use continuous tags; gray rows use discrete tags
(which does worse). In each column, the best score for each color is boldfaced, along with all results of that color
that are not significantly worse (paired permutation test, p < 0.05). These results use only ELMo layer 1; results
from all layers are shown in Table 3 in the appendix, for both LAS and UAS metrics.

Analysis. Table 2 shows the test accuracies
of these parsers, using the standard train-
ing/development/test split for each UD language.

In the continuous case, the VIB representation
outperforms all three baselines in 8 of 9 languages,
and is not significantly worse in the 9th language
(Hindi). In short, our VIB joint training generalizes
better to test data. This is because the training ob-
jective (2) includes terms that focus on the parsing
task and also regularize the representations.

In the discrete case, the VIB representation out-
performs gold POS tags (at the same level of gran-
ularity) in 6 of 9 languages, and of the other 3, it is
not significantly worse in 2. This suggests that our
learned discrete tag set could be an improved al-
ternative to gold POS tags (cf. Klein and Manning,
2003) when a discrete tag set is needed for speed.

8 Related Work

Much recent NLP literature examines syntactic in-
formation encoded by deep models (Linzen et al.,
2016) and more specifically, by powerful unsu-
pervised word embeddings. Hewitt and Manning
(2019) learn a linear projection from the embed-
ding space to predict the distance between two
words in a parse tree. Peters et al. (2018b) and
Goldberg (2019) assess the ability of BERT and
ELMo directly on syntactic NLP tasks. Tenney
et al. (2019) extract information from the contex-
tual embeddings by self-attention pooling within a
span of word embeddings.

The IB framework was first used in NLP to clus-
ter distributionally similar words (Pereira et al.,
1993). In cognitive science, it has been used to ar-
gue that color-naming systems across languages are
nearly optimal (Zaslavsky et al., 2018). In machine
learning, IB provides an information-theoretic per-
spective to explain the performance of deep neural
networks (Tishby and Zaslavsky, 2015).

The VIB method makes use of variational upper

and lower bounds on mutual information. An al-
ternative lower bound was proposed by Poole et al.
(2019), who found it to work better empirically.

9 Conclusion and Future Work

In this paper, we have proposed two ways to syn-
tactically compress ELMo word token embeddings,
using variational information bottleneck. We auto-
matically induce stochastic discrete tags that corre-
late with gold POS tags but are as good or better
for parsing. We also induce stochastic continuous
token embeddings (each is a Gaussian distribution
over Rd) that forget non-syntactic information cap-
tured by ELMo. These stochastic vectors yield
improved parsing results, in a way that simpler di-
mensionality reduction methods do not. They also
transfer to the problem of predicting gold POS tags,
which were not used in training.

One could apply the same training method to
compress the ELMo or BERT token sequence x
for other tasks. All that is required is a model-
specific decoder q�(y | t). For example, in the
case of sentiment analysis, the approach should
preserve only sentiment information, discarding
most of the syntax. One possibility that does not
require supervised data is to create artificial tasks,
such as reproducing the input sentence or predict-
ing missing parts of the input (such as affixes and
function words). In this case, the latent representa-
tions would be essentially generative, as in the vari-
ational autoencoder (Kingma and Welling, 2013).

Acknowledgments

This work was supported by the National Science
Foundation under Grant No. 1718846 and by a
Provost’s Undergraduate Research Award to the
first author. The Maryland Advanced Research
Computing Center provided computing facilities.
We thank the anonymous reviewers and Hongyuan
Mei for helpful comments.



2753

References

Alexander A. Alemi, Ian Fischer, Joshua V. Dillon,
and Kevin Murphy. 2016. Deep variational infor-
mation bottleneck. Proceedings of the International
Conference on Learning Representations (ICLR),
abs/1612.00410.

T.W. Anderson. 2003. An Introduction to Multivariate
Statistical Analysis. Wiley Series in Probability and
Statistics. Wiley.

Steven Bird, Ewan Klein, and Edward Loper. 2009.
Natural Language Processing with Python, 1st edi-
tion. O’Reilly Media, Inc.

Wanxiang Che, Yijia Liu, Yuxuan Wang, Bo Zheng,
and Ting Liu. 2018. Towards better UD parsing:
Deep contextualized word embeddings, ensemble,
and treebank concatenation. In Proceedings of the
CoNLL 2018 Shared Task: Multilingual Parsing

from Raw Text to Universal Dependencies, pages
55–64, Brussels, Belgium. Association for Compu-
tational Linguistics.

Jacob Devlin, Ming-Wei Chang, Kenton Lee, and
Kristina Toutanova. 2018. BERT: pre-training of
deep bidirectional transformers for language under-
standing. CoRR, abs/1810.04805.

Timothy Dozat and Christopher D. Manning. 2016.
Deep biaffine attention for neural dependency pars-
ing. CoRR, abs/1611.01734.

Jack Edmonds. 1966. Optimum Branchings. Journal
of Research of the National Bureau of Standards.

Nir Friedman, Ori Mosenzon, Noam Slonim, and Naf-
tali Tishby. 2001. Multivariate information bottle-
neck. In Proceedings of the 17th Conference in Un-
certainty in Artificial Intelligence, UAI ’01, pages
152–161, San Francisco, CA, USA. Morgan Kauf-
mann Publishers Inc.

Matt Gardner, Joel Grus, Mark Neumann, Oyvind
Tafjord, Pradeep Dasigi, Nelson F. Liu, Matthew
Peters, Michael Schmitz, and Luke S. Zettlemoyer.
2017. AllenNLP: A Deep Semantic Natural Lan-
guage Processing Platform.

Yoav Goldberg. 2019. Assessing BERT’s syntactic
abilities. CoRR, abs/1901.05287.

John Hewitt and Christopher D. Manning. 2019. A
structural probe for finding syntax in word represen-
tations. In North American Chapter of the Associ-
ation for Computational Linguistics: Human Lan-

guage Technologies. Association for Computational
Linguistics.

Eric Jang, Shixiang Gu, and Ben Poole. 2016. Categor-
ical reparameterization with Gumbel-softmax. Inter-
national Conference on Learning Representations.

Diederik Kingma and Jimmy Ba. 2014. Adam: A
method for stochastic optimization. In Proceedings
of the International Conference on Learning Repre-

sentations (ICLR).

Diederik P. Kingma and Max Welling. 2013. Auto-
encoding variational Bayes. Proceedings of the
International Conference on Learning Representa-

tions (ICLR).

D. Klein and C. D. Manning. 2003. Accurate unlexi-
calized parsing. In Proceedings of the 41st Annual
Meeting of the Association for Computational Lin-

guistics.

Terry Koo, Amir Globerson, Xavier Carreras Pérez,
and Michael Collins. 2007. Structured prediction
models via the matrix-tree theorem. In Proceed-
ings of the 2007 Joint Conference on Empirical

Methods in Natural Language Processing and Com-

putational Natural Language Learning (EMNLP-

CoNLL), pages 141–150.

Tal Linzen, Emmanuel Dupoux, and Yoav Goldberg.
2016. Assessing the ability of LSTMs to learn
syntax-sensitive dependencies. Transactions of the
Association for Computational Linguistics, 4:521–
535.

Chris J. Maddison, Andriy Mnih, and Yee Whye Teh.
2017. The concrete distribution: A continuous relax-
ation of discrete random variables. In Proceedings
of the International Conference on Learning Repre-

sentations (ICLR).

Ryan McDonald and Giorgio Satta. 2007. On the com-
plexity of non-projective data-driven dependency
parsing. In Proceedings of the 10th International
Conference on Parsing Technologies, pages 121–
132. Association for Computational Linguistics.

Joakim Nivre et al. 2018. Universal dependencies 2.3.
LINDAT/CLARIN digital library at the Institute of
Formal and Applied Linguistics (ÚFAL), Faculty of
Mathematics and Physics, Charles University.

Fernando Pereira, Naftali Tishby, and Lillian Lee. 1993.
Distributional clustering of English words. In Pro-
ceedings of the 31st Annual Meeting of Association

for Computational Linguistics, pages 183–190. As-
sociation for Computational Linguistics.

Matthew Peters, Mark Neumann, Mohit Iyyer, Matt
Gardner, Christopher Clark, Kenton Lee, and Luke
Zettlemoyer. 2018a. Deep contextualized word rep-
resentations. Proceedings of the 2018 Conference of
the North American Chapter of the Association for

Computational Linguistics: Human Language Tech-

nologies, Volume 1 (Long Papers).

Matthew E. Peters, Mark Neumann, Luke Zettlemoyer,
and Wen-tau Yih. 2018b. Dissecting contextual
word embeddings: Architecture and representation.
In Proceedings of the 2018 Conference on Empirical
Methods in Natural Language Processing (EMNLP),
pages 1499–1509.

Ben Poole, Sherjil Ozair, Aäron van den Oord, Alexan-
der A. Alemi, and George Tucker. 2019. On
variational bounds of mutual information. CoRR,
abs/1905.06922.

http://arxiv.org/abs/1612.00410
http://arxiv.org/abs/1612.00410
https://books.google.com/books?id=Cmm9QgAACAAJ
https://books.google.com/books?id=Cmm9QgAACAAJ
http://www.aclweb.org/anthology/K18-2005
http://www.aclweb.org/anthology/K18-2005
http://www.aclweb.org/anthology/K18-2005
http://arxiv.org/abs/1810.04805
http://arxiv.org/abs/1810.04805
http://arxiv.org/abs/1810.04805
http://arxiv.org/abs/1611.01734
http://arxiv.org/abs/1611.01734
https://nvlpubs.nist.gov/nistpubs/jres/71b/jresv71bn4p233_a1b.pdf
http://dl.acm.org/citation.cfm?id=647235.720080
http://dl.acm.org/citation.cfm?id=647235.720080
http://arxiv.org/abs/arXiv:1803.07640
http://arxiv.org/abs/arXiv:1803.07640
http://arxiv.org/abs/1901.05287
http://arxiv.org/abs/1901.05287
https://nlp.stanford.edu/pubs/hewitt2019structural.pdf
https://nlp.stanford.edu/pubs/hewitt2019structural.pdf
https://nlp.stanford.edu/pubs/hewitt2019structural.pdf
https://arxiv.org/abs/1611.01144
https://arxiv.org/abs/1611.01144
https://arxiv.org/abs/1412.6980
https://arxiv.org/abs/1412.6980
https://arxiv.org/pdf/1312.6114.pdf
https://arxiv.org/pdf/1312.6114.pdf
https://www.transacl.org/ojs/index.php/tacl/article/view/972
https://www.transacl.org/ojs/index.php/tacl/article/view/972
https://arxiv.org/abs/1611.00712
https://arxiv.org/abs/1611.00712
http://hdl.handle.net/11234/1-2895
https://doi.org/10.3115/981574.981598
https://doi.org/10.18653/v1/n18-1202
https://doi.org/10.18653/v1/n18-1202
https://www.aclweb.org/anthology/D18-1179
https://www.aclweb.org/anthology/D18-1179
http://arxiv.org/abs/1905.06922
http://arxiv.org/abs/1905.06922


2754

Danilo Jimenez Rezende, Shakir Mohamed, and Daan
Wierstra. 2014. Stochastic backpropagation and
approximate inference in deep generative models.
arXiv preprint arXiv:1401.4082.

Kenneth Rose. 1998. Deterministic annealing for clus-
tering, compression, classification, regression, and
related optimization problems. Proceedings of the
IEEE, 80:2210–2239.

David A Smith and Noah A Smith. 2007. Proba-
bilistic models of nonprojective dependency trees.
In Proceedings of the 2007 Joint Conference on
Empirical Methods in Natural Language Process-

ing and Computational Natural Language Learning

(EMNLP-CoNLL), pages 132–140.

Ian Tenney, Patrick Xia, Berlin Chen, Alex Wang,
Adam Poliak, R. Thomas McCoy, Najoung Kim,
Benjamin Van Durme, Samuel R. Bowman, Dipan-
jan Das, and Ellie Pavlick. 2019. What do you learn
from context? Probing for sentence structure in con-
textualized word representations.

Naftali Tishby, Fernando C. Pereira, and William
Bialek. 2000. The information bottleneck method.
arXiv preprint physics/0004057.

Naftali Tishby and Noga Zaslavsky. 2015. Deep learn-
ing and the information bottleneck principle. 2015
IEEE Information Theory Workshop (ITW), pages 1–
5.

L. J. P. van der Maaten and G. E. Hinton. 2008. Visu-
alizing high-dimensional data using t-SNE. Journal
of Machine Learning Research, 9:2579–2605.

Noga Zaslavsky, Charles Kemp, Terry Regier, and Naf-
tali Tishby. 2018. Efficient human-like semantic rep-
resentations via the information bottleneck principle.
CoRR, abs/1808.03353.

https://arxiv.org/abs/1401.4082
https://arxiv.org/abs/1401.4082
http://scl.ece.ucsb.edu/pubs/pubs_B/b98_2.pdf
http://scl.ece.ucsb.edu/pubs/pubs_B/b98_2.pdf
http://scl.ece.ucsb.edu/pubs/pubs_B/b98_2.pdf
http://arxiv.org/abs/1905.06316
http://arxiv.org/abs/1905.06316
http://arxiv.org/abs/1905.06316
https://arxiv.org/abs/physics/0004057
http://arxiv.org/abs/1808.03353
http://arxiv.org/abs/1808.03353

