



















































Reconstructing Capsule Networks for Zero-shot Intent Classification


Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing
and the 9th International Joint Conference on Natural Language Processing, pages 4799–4809,
Hong Kong, China, November 3–7, 2019. c©2019 Association for Computational Linguistics

4799

Reconstructing Capsule Networks for Zero-shot Intent Classification

Han Liu1∗ Xiaotong Zhang1∗ Lu Fan1∗ Xuandi Fu1
Qimai Li1 Xiao-Ming Wu1† Albert Y.S. Lam2

Department of Computing, The Hong Kong Polytechnic University, Hong Kong S.A.R.1

Fano Labs, Hong Kong S.A.R.2

{cshliu,csxtzhang,cslfan}@comp.polyu.edu.hk
{csxfu,csqmli,csxmwu}@comp.polyu.edu.hk, albert@fano.ai

Abstract

Intent classification is an important building
block of dialogue systems. With the bur-
geoning of conversational AI, existing sys-
tems are not capable of handling numerous
fast-emerging intents, which motivates zero-
shot intent classification. Nevertheless, re-
search on this problem is still in the incipi-
ent stage and few methods are available. A
recently proposed zero-shot intent classifica-
tion method, IntentCapsNet, has been shown
to achieve state-of-the-art performance. How-
ever, it has two unaddressed limitations: (1)
it cannot deal with polysemy when extract-
ing semantic capsules; (2) it hardly recog-
nizes the utterances of unseen intents in the
generalized zero-shot intent classification set-
ting. To overcome these limitations, we pro-
pose to reconstruct capsule networks for zero-
shot intent classification. First, we introduce
a dimensional attention mechanism to fight a-
gainst polysemy. Second, we reconstruct the
transformation matrices for unseen intents by
utilizing abundant latent information of the la-
beled utterances, which significantly improves
the model generalization ability. Experimental
results on two task-oriented dialogue datasets
in different languages show that our proposed
method outperforms IntentCapsNet and other
strong baselines.

1 Introduction

With the advent of conversational AI, task-
oriented spoken dialogue systems are becoming
ubiquitous, e.g., chatbots deployed on differen-
t applications, or modules integrated in the pop-
ular virtual personal assistants like Apple Siri or
Microsoft Cortana (Chen et al., 2017). To im-
prove business effectiveness and user satisfaction,
accurately identifying the intents behind user ut-

∗ Equal contribution.
† Corresponding author.

terances is indispensable. However, it is extreme-
ly challenging not only because user queries are
sometimes short and expressed diversely, but al-
so because it may continuously encounter new or
unacquainted intents popped up quickly from var-
ious domains. Conventional intent classification
methods (Hu et al., 2009; Tur et al., 2012; Xu and
Sarikaya, 2013; Ravuri and Stolcke, 2015; Liu and
Lane, 2016; Nam et al., 2016) typically train a su-
pervised learning model on large amounts of la-
beled data, and are not effective in recognizing e-
merging unseen intents.

Several zero-shot learning approaches attempt-
ed to address the challenges for classifying intents
whose instances are not present during training.
One common idea is to utilize some external re-
sources (Ferreira et al., 2015a,b; Yazdani and Hen-
derson, 2015; Kumar et al., 2017; Zhang et al.,
2019) such as label ontologies or manually defined
attributes. However, such external resources are
usually unavailable, as they require substantial ex-
tra time and expensive human labour to produce.
To implement zero-shot intent classification more
easily and intelligently, recent works rely more
on the word embeddings of intent labels, which
can be easily pretrained on text corpus. Methods
proposed by Chen et al. (2016) and Kumar et al.
(2017) utilize neural networks to project intent la-
bels and data samples to the same semantic space
and then measure their similarity. However, learn-
ing a good projection function is usually difficult
due to the diversity of user expressions, especially
in some complex domains such as medical queries
(Zhang et al., 2016).

Unlike previous models, IntentCapsNet (Xia
et al., 2018) employs capsule networks to extrac-
t high-level semantic features and then transfers
the prediction vectors for seen intents to unseen
intents. Although IntentCapsNet has achieved
impressive performance in some zero-shot intent



4800

Figure 1: Illustration of our framework ReCapsNet-ZS. In the training process, labeled utterances are first en-
coded by Bi-LSTM. Then, a set of semantic capsules are extracted via the dimensional attention module. Finally,
these semantic capsules are fed to a capsule network to train a model for predicting the seen intents. In the testing
process, to predict the unseen intents, a metric learning method is trained on labeled utterances and intent label
embeddings to learn the similarities between the unseen and seen intents. Then, the learned similarities and the
transformation matrices for the seen intents trained by capsule networks are used to construct the transformation
matrices for the unseen intents. When a test utterance arrives, it is first encoded into semantic capsules by the
trained Bi-LSTM and dimensional attention module. There are two settings for intent classification. (1) Zero-shot
intent classification: only utterances of the unseen intents participate in testing, so each utterance needs to be clas-
sified to one of the unseen intents. In this case, only the transformation matrices for the unseen intents are used for
prediction. (2) Generalized zero-shot intent classification: test utterances may come from both the seen and unseen
intents, so each utterance needs to be classified to either a seen or an unseen intent. In this case, the transformation
matrices for the seen and unseen intents are all used for prediction.

classification tasks, it has two unaddressed limi-
tations. (1) The self-attention module of Intent-
CapsNet cannot handle polysemy, which weakens
the representation capacity of semantic capsules.
(2) For the generalized zero-shot classification set-
ting where newly arrived utterances come from
both seen and unseen intents, the method of In-
tentCapsNet for constructing the prediction vec-
tors can easily cause the model to completely fail
in detecting unseen intents, which is clearly unde-
sirable and inadequate for real dialogue systems.

In this paper, we propose to reconstruct cap-
sule networks for zero-shot intent classification
(ReCapsNet-ZS), which effectively addresses the
limitations of IntentCapsNet and adapts well to the
generalized zero-shot intent classification tasks.
As illustrated in Figure 1, ReCapsNet-ZS consists
of two components. First, it introduces a dimen-
sional attention module to alleviate the polysemy
problem, which helps to extract semantic features
for capsule networks. Second, it computes the
similarities between unseen and seen intents by u-
tilizing the rich latent information of labeled utter-

ances, and then constructs the transformation ma-
trices for unseen intents with the computed simi-
larities and the trained transformation matrices for
seen intents, which greatly improves the general-
ization ability to unseen intents.

To verify the effectiveness of the proposed
ReCapsNet-ZS for zero-shot intent classification,
we conduct extensive experiments on two re-
al task-oriented dialogue datasets in English and
Chinese respectively. The empirical study vali-
dates our proposals and shows promising results of
ReCapsNet-ZS, which are significantly better than
state-of-the-art methods, especially on the gener-
alized zero-shot intent classification tasks.

2 Related Works

Zero-shot Intent Classification. Zero-shot
learning (Larochelle et al., 2008; Palatucci et al.,
2009) aims to use the knowledge learned from
seen classes, of which abundant labeled samples
are typically available for training, to recognize
unseen classes, of which no labeled samples are
provided. It has been widely studied in computer



4801

vision (Ba et al., 2015; Xian et al., 2016) and
natural language processing (Sappadla et al.,
2016; Zhang et al., 2019).

Zero-shot intent classification is an importan-
t and challenging task for many natural language
understanding applications (Hu et al., 2009; Li-
u and Lane, 2016; Nam et al., 2016; Xu and
Sarikaya, 2013), in which new intents emerge con-
stantly and they cannot be easily recognized. Sev-
eral methods have been proposed to tackle this
problem. Ferreira et al. (2015a,b) and Yazdani and
Henderson (2015) utilize external resources such
as label ontologies or manually defined attributes
to find the relationship between seen and unseen
intent labels. However, the external resources are
usually difficult to obtain, as collecting them is
labor intensive and time consuming. Chen et al.
(2016) and Kumar et al. (2017) project the utter-
ances and intent labels to a same semantic space
and then compute the similarities between utter-
ances and intent labels (Chen et al., 2016; Kumar
et al., 2017). However, diverse user expression-
s may make it difficult to learn a good projection
function and thus affect the classification perfor-
mance. Recently, Xia et al. (2018) extend cap-
sule networks for zero-shot intent classification by
transferring the prediction vectors from seen class-
es to unseen classes. However, there are some key
issues left to be resolved, including how to deal
with polysemy in word embeddings and how to
improve the model generalization ability to unseen
intents in the generalized zero-shot intent classifi-
cation setting.

Capsule Networks. Capsule Networks (Sabour
et al., 2017) were first proposed to address the
shortcomings of convolutional neural networks
(CNN) in the domain of computer vision. It al-
lows the networks to learn part-whole invariant
relationships consecutively. Recently, some stud-
ies have attempted to apply capsule networks in
the domain of natural language processing (Yang
et al., 2018; Geng et al., 2019; Xia et al., 2018)
and obtained promising results. Yang et al. (2018)
first extend capsule networks for text classifica-
tion. Geng et al. (2019) successfully combine
the dynamic routing algorithm with some meta-
learning framework for few-shot text classifica-
tion. However, their model still requires some la-
beled samples for each class. Xia et al. (2018) pro-
pose a model based on capsule networks for zero-
shot intent classification and has achieved state-of-

the-art performance, but as mentioned above, their
model has some intrinsic limitations remained to
be addressed.

3 Preliminaries

3.1 Problem Formulation

Given the set of all intent labels Y = Y s
⋃
Y u,

where Y s = {ys1, ys2, . . . , ysK} and Y u =
{yu1 , yu2 , . . . , yuL} are the sets of seen and unseen
intent labels respectively. There is no overlap be-
tween Y s and Y u, i.e., Y s

⋂
Y u = ∅, and K

and L are the numbers of seen and unseen in-
tent labels respectively. The embeddings of the
seen and unseen intent labels are denoted byEs =
{es1, es2, . . . , esK} and Eu = {eu1 , eu2 , . . . , euL} re-
spectively. Each embedding is a d-dimensional
vector. For all the seen and unseen intent label-
s, their associated embeddings are available. The
sample (utterance) sets for the seen and unseen in-
tent labels are denoted byXs = {xs1, xs2, . . . , xsns}
and Xu = {xu1 , xu2 , . . . , xunu} respectively, where
ns is the number of instances of the seen labels
and nu is the number of instances of the unseen
labels.

Zero-shot Intent Classification. For this set-
ting, the training set is Xtr = {Xs, Y s}, and Xu
is not available for training. In the test phase, the
goal is to assign an unseen intent label y ∈ Y u to
a given utterance.

Generalized Zero-shot Intent Classification.
For this setting, the training procedure is the same
as above, while the difference is in the test phase,
where the goal is to assign an intent label y ∈
Y s

⋃
Y u to a given utterance.

In this paper, we aim to reconstruct capsule net-
works for handling both of the two settings of
zero-shot intent classification.

3.2 Limitations of IntentCapsNet

IntentCapsNet (Xia et al., 2018) is the first work to
employ capsule networks for zero-shot intent clas-
sification. It exploits the self-attention mechanism
to extract semantic features (capsules) of an utter-
ance. For zero-shot intent classification, it utilizes
the vote vectors of seen intents and the similari-
ties between seen and unseen intents based on Eu-
clidean distance to make predictions for unseen in-
tents. Although IntentCapsNet has demonstrated
strong performance, it has two fundamental limi-
tations.



4802

Limitation 1. The self-attention module of In-
tentCapsNet cannot handle the polysemy problem,
which limits the representation capacity of seman-
tic capsules.

Typically, a word is represented by a multi-
dimensional embedding. Since a word can have
different meanings in different contexts, some in-
teresting recent studies (Shen et al., 2018; Şenel
et al., 2018) suggest that different dimensions of
a word embedding may tend to represent differ-
ent semantic meanings. For example, the word
“book” has different meanings in the two utter-
ances: “Book a restaurant in Michigan for 4 peo-
ple” and “Give 4 out of 6 points to this book”.
For the embedding of the word “book”, it is hy-
pothesized that some dimensions may be more in-
dicative for the first meaning – ”reserve”, while
some other dimensions may be more indicative for
the second meaning. Apparently, the self-attention
mechanism cannot pay more attentions to the di-
mensions that best describe the specific meaning
of a word in a given context, as it assigns the same
attention score for all the dimensions, which sig-
nificantly limits the representation capacity of se-
mantic capsules and undermines the performance
of capsule networks.

Limitation 2. For the generalized zero-shot
classification setting, the method of IntentCapsNet
for constructing the prediction vectors is highly
likely to cause the model to lose generalization a-
bility to unseen intents.

Here, we provide an analysis of IntentCapsNet
for predicting an unseen intent in the generalized
zero-shot classification setting. In IntentCapsNet,
the probability of a test utterance x belonging to a
seen intent label k is computed as:

Pk = ‖
R∑

r=1

ckrpk|r‖ = ‖
R∑

r=1

gk,r‖, (1)

where ‖ · ‖ is the L2-norm of a vector, R is the
number of semantic capsules, pk|r is the predic-
tion vector for the r-th semantic capsule with re-
spect to the seen intent k, and ckr is the weight of
the r-th semantic capsule with respect to the seen
intent k, which is computed by the dynamic rout-
ing algorithm of capsule networks. gk,r = ckrpk|r
is called the r-th vote vector for the seen intent k.
By Eq. (1), we have a tight upper bound for Pk:

Pk 6
R∑

r=1

‖gk,r‖. (2)

IntentCapsNet computes the probability of x
belonging to an unseen intent label l as:

Pl = ‖
R∑

r=1

clrul|r‖ = ‖
R∑

r=1

clr

K∑
k=1

qlkgk,r‖, (3)

where ul|r is the prediction vector for the r-th
semantic capsule with respect to the unseen in-
tent l, and clr is the weight of the r-th seman-
tic capsule with respect to the unseen intent l,
which is determined by the dynamic routing al-
gorithm. ul|r =

∑K
k=1 qlkgk,r, where K is the

number of seen intents, gk,r is the r-th vote vec-
tor for the seen intent k, and qlk is the similarity
between an unseen intent yul ∈ Y u and a seen in-
tent ysk ∈ Y s. qlk =

exp(−d(esk,e
u
l ))∑K

k=1 exp(−d(esk,e
u
l )

, where

esk and e
u
l are the embeddings of the seen and un-

seen intents respectively, and d(esk, e
u
l ) is the s-

caled squared Euclidean distance between esk and
eul . Since qlk ∈ (0, 1), clr ∈ (0, 1),

∑K
k=1 qlk = 1

and
∑R

r=1 clr = 1, we have a tight upper bound
for Pl:

Pl 6
R∑

r=1

clr

K∑
k=1

qlk‖gk,r‖ 6 ‖gk,r‖max, (4)

where ‖gk,r‖max is the maximum among ‖gk,r‖,
∀r ∈ {1, 2, . . . , R} and ∀k ∈ {1, 2, . . . ,K}.

By Eq. (2) & (4), it can be seen that the up-
per bound of Pk is much larger than Pl, indicat-
ing that for any utterance x, it is highly likely that
P (y ∈ Y s|x) is larger than P (y ∈ Y u|x). Hence,
for generalized zero-shot classification, with high
probability IntentCapsNet will classify a test utter-
ance to the seen intents, which is also verified by
our experiments in section 5.

4 The Proposed Approach

To overcome the limitations of IntentCapsNet, we
propose to reconstruct capsule networks for zero-
shot intent classification. In particular, we intro-
duce two modules to capsule networks: (1) a di-
mensional attention module that helps to extrac-
t more representative semantic capsules and (2)
a new method for constructing the transformation
matrices to improve the model generalization abil-
ity to unseen intents.

4.1 Dimensional Attention Capsule Networks
Pre-processing. An utterance with T words can
be represented as x = {w1,w2, . . . ,wT }, where



4803

wt ∈ Rdw is the word embedding of the t-th
word and can be pretrained by the skip-gram mod-
el (Mikolov et al., 2013). Each word can be further
encoded sequentially using a recurrent neural net-
work such as bidirectional LSTM (Hochreiter and
Schmidhuber, 1997), i.e.,

−→
h t = LSTMfw(wt,

−→
h t−1),

←−
h t = LSTMbw(wt,

←−
h t+1),

(5)

where LSTMfw and LSTMbw denote the forward
and backward LSTM respectively, and

−→
h t ∈ Rdh

and
←−
h t ∈ Rdh are the hidden states of the word

wt learned from LSTMfw and LSTMbw respec-
tively. The entire hidden state of wt is represented
by concatenating

−→
h t and

←−
h t, i.e., ht = [

−→
h t,
←−
h t],

and the hidden state matrix of the utterance is
H = [h1,h2, . . . ,hT ] ∈ R2dh×T .

4.1.1 Extracting Semantic Capsules with
Dimensional Attention

In general, an utterance is composed of multiple
semantic features, and these semantic features col-
lectively contribute to a more abstract intent la-
bel. For example, an utterance “I want to know the
temperature of Hong Kong” is composed by mul-
tiple semantic features such as get action (want
to know), weather (temperature), and city name
(Hong Kong), and these semantic features col-
lectively reflect the intent label “Get Weather”.
Capsule networks provide a hierarchical reasoning
structure for modeling semantic features for intent
classification. First, the primary capsules in cap-
sule networks can properly match multiple seman-
tic features of an utterance. Second, the dynam-
ic routing mechanism of capsule networks can be
used to automatically learn the importance weight
of each semantic feature and aggregate them into
a high-level intent label.

It is assumed that a high-level semantic feature
of an utterance is largely generated by some of it-
s words that have similar semantic meaning (Xia
et al., 2018). To extract the semantic features of
an utterance, the key problem is to learn the im-
portance weight of each word for a semantic fea-
ture. IntentCapsNet (Xia et al., 2018) utilizes the
self-attention mechanism to extract the semantic
features (capsules) of each utterance. However,
self-attention cannot effectively deal with polyse-
my. Inspired by the work of Shen et al. (2018), we
propose to use the dimensional attention mecha-
nism to alleviate the polysemy problem in extract-

ing semantic features. Dimensional attention can
automatically assign different attention scores to
different dimensions of a word embedding, which
not only helps to solve the polysemy problem to
some extent, but also expands the search space
of the attention parameters, thus improving model
flexibility and effectiveness.

Assume each utterance hasR semantic features.
We propose to learn a dimensional attention ma-
trix Ar ∈ R2dh×T that encodes the dimensional
attentions of the T words with respect to the r-th
semantic feature by:

Ar = softmax (F2 ReLU (F1H)) , (6)

where F1 ∈ Rda×2dh and F2 ∈ R2dh×da are the
trainable parameters, and Ar(i, j) (the element of
Ar in the i-th row and j-th column) means the
importance weight of the i-th dimension of the j-
th word embedding to the r-th semantic feature.
Compared with self-attention, dimensional atten-
tion can help to choose the appropriate dimensions
of a word embedding that can best express the spe-
cific meaning of the word in a given context.

After obtaining Ar, the r-th semantic feature
mr ∈ R2dh is computed by:

mr =
∑

row
(Ar �H) , (7)

where� is element-wise multiplication, and
∑

row
is an operator that sums up elements of each row.
The entire semantic features for each utterance is
M = [m1,m2, . . . ,mR] ∈ R2dh×R.

4.1.2 Improved Max-margin Loss
The semantic features of the utterance can then be
fed into a capsule network to learn the intent. First,
we transform each semantic feature mr of the ut-
terance to a prediction vector with respect to each
intent as:

pk|r = Wkrmr, (8)

where pk|r ∈ Rdp is the prediction vector of the r-
th semantic feature with respect to the k-th intent,
and Wkr ∈ Rdp×2dh is the associated transforma-
tion matrix.

In training, there are K output capsules, corre-
sponding to K seen intents. The k-th output cap-
sule ok is the weighted sum of all the prediction
vectors pk|r (r ∈ {1, . . . , R}),

ok =

R∑
r=1

ckrpk|r, (9)



4804

Algorithm 1 Dynamic Routing Algorithm
Procedure Dynamic Routing(pk|r, nroute)

for all semantic capsule r and intent capsule k: bkr ← 0.
for nroute iterations do

for all semantic capsule r: cr ← softmax(br).
for all intent capsule k: ok ←

∑R
r=1 ckrpk|r .

for all intent capsule k: vk ← squash(ok).
for all semantic capsule r and intent capsule k:

bkr ← bkr + pk|r · vk.
end for
return vk.

where ckr is the coupling coefficient representing
the contribution degree of the r-th semantic fea-
ture to the k-th intent, which can be computed by
the dynamic routing algorithm (Algorithm 1).

Then, a squashing function squash(·) is applied
on ok, and the final output capsule of the k-th in-
tent is:

vk = squash(ok) =
‖ok‖2

1 + ‖ok‖2
ok
‖ok‖

. (10)

Now, the probability of the existence of the k-th
intent can be represented as the length of the out-
put capsule vk. The computation procedure of vk
is shown in Algorithm 1, where pk|r · vk denotes
the inner product between pk|r and vk.

To train the dimensional attention capsule net-
work, we propose an improved max-margin loss
function consisting of two parts.

The first part is the max-margin loss on each la-
beled utterance, which is the original loss function
of capsule networks (Sabour et al., 2017):

L1 =

K∑
k=1

{yk max(0,m+ − ‖vk‖)2

+ λ(1− yk)max(0, ‖vk‖ −m−)2},

(11)

where yk = 1 if the utterance is of intent label
k and yk = 0 otherwise, λ is a down-weighting
parameter, and m+ and m− are the margins.

The second part is to ensure the diversity of the
semantic capsules, i.e., different semantic capsules
are likely to be generated by different words in an
utterance. The importance weight of each word
to the r-th semantic capsule can be represented by
the average value of each column of the dimen-
sional attention matrix Ar, i.e.,

sr =
1

2dh

∑
col

Ar, (12)

where sr ∈ R1×T and
∑

col is an operator that
sums up elements of each column. Denote by S =

[s>1 , s
>
2 , . . . , s

>
R] ∈ RT×R the importance weight

matrix of each word to all theR semantic capsules.
To ensure the diversity of the semantic capsules, a
natural idea is to constrain the columns of S to be
orthogonal with the following loss function:

L2 = ‖S>S − I‖2F , (13)

where ‖ · ‖F is the Frobenius norm of a matrix.
Combining Eq. (11) and Eq. (13), the overall

loss function of the proposed dimensional atten-
tion capsule network is:

Ltotal = L1 + βL2, (14)

where β is a trade-off parameter. By minimiz-
ing Ltotal with gradient descent methods, all mod-
el parameters including F1, F2 and Wkr can be
learned.

4.2 Zero-shot Intent Classification
To solve zero-shot intent classification with cap-
sule networks, two key problems need to be ad-
dressed. (1) How to find the relationship between
unseen and seen intents? (2) How to make predic-
tions for unseen intents?

Measuring Intent Relations. To tackle the first
problem, we propose to learn a Mahalanobis dis-
tance metric to measure the relationship between
unseen and seen intents. Specifically, given the
embeddings of an unseen intent l and a seen intent
k, their squared Mahalanobis distance is given by:

dM (e
u
l , e

s
k) = (e

u
l − esk)>Ω−1(eul − esk), (15)

where Ω is a learnable covariance matrix which
models the correlation between dimensions of the
embedding. Note that IntentCapsNet (Xia et al.,
2018) also tries to use Eq. (15) to model the rela-
tionship between unseen and seen intents, but it ig-
nores the correlation between dimensions and sim-
ply sets Ω = σ2I (σ is a scaling hyper-parameter),
which is actually a scaled squared Euclidean dis-
tance.

As the number of intents is limited, it is difficult
to learn a desirable covariance matrix Ω with the
intent embeddings only. Fortunately, we can lever-
age the word embeddings of the utterances, which
come from the same semantic space as the inten-
t embeddings (pre-trained by the same skip-gram
model). Hence, we propose to learn the covari-
ance matrix Ω with the labeled utterances in the



4805

training set. Inspired by the work of Ying and Li
(2012), we propose to learn the Mahalanobis dis-
tance metric by optimizing the objective:

max
Ω

min
(i,j)∈D

dM (z
s
i , z

s
j ),

s.t.
∑

(i,j)∈S

dM (z
s
i , z

s
j ) 6 1,

(16)

where D and S respectively denote the pair set-
s in which utterances belong to different class-
es and the same class. For an utterance i, zsi
denotes the average sum of all the word embed-
dings. As shown by Ying and Li (2012), opti-
mizing Eq. (16) with respect to Ω is equivalen-
t to solving an efficient eigenvalue optimization
problem. With the learned metric Ω, we can have
the relationship between any unseen and seen in-
tents by substituting it into Eq. (15). Furthermore,
we can compute the similarity between them by
qlk = exp (−α · dM (eul , esk)), where α is a scal-
ing parameter.

Constructing Transformation Matrices. Intu-
itively, if an unseen intent is similar to a seen in-
tent, their corresponding transformation matrices
should also be similar. Based on this, to solve the
second problem, we propose to derive the transfor-
mation matrices of unseen intents using the trans-
formation matrices of seen intents and the simi-
larities between unseen and seen intents, and then
make predictions for unseen intents with the trans-
formation matrices. Specifically, given a matrix
Q ∈ RL×K that encodes the similarities between
unseen and seen intents, for an unseen intent l,
we propose to construct the transformation matrix
Wlr for the r-th semantic capsule with respect to
the l-th unseen intent by:

Wlr =

K∑
k=1

qlkWkr, (17)

where qlk is the element in the l-th row and the k-
th column of Q, Wkr is the transformation matrix
for the r-th semantic capsule with respect to the k-
th seen intent. By Eq. (17), the transformation ma-
trices for all unseen intents can be obtained. When
a test utterance arrives, it can be directly fed into
the trained dimensional attention capsule network
for intent prediction.

Dataset SNIPS-NLU SMP-2018

Vocab Size 11641 2682
Number of Samples 13802 2460
Average Sentence Length 9.05 4.86
Number of Seen Intents 5 24
Number of Unseen Intents 2 6

Table 1: Dataset statistics.

Dataset dw dh da dp R nroute
SNIPS-NLU 300 16 10 10 3 3
SMP-2018 300 32 30 10 8 3

Table 2: Network structure hyperparameters.

5 Experiments

5.1 Datasets
We evaluate our model on two real task-oriented
dialogue datasets in different languages. Table 1
summarizes the dataset statistics.

SNIPS-NLU. Following (Xia et al., 2018), we
use the SNIPS-NLU (SNIPS Natural Language
Understanding) benchmark dataset (Coucke et al.,
2018). SNIPS-NLU is an open-source single-
turn English corpus, which contains crowdsourced
queries evenly distributed in 7 intents.

SMP-2018. It is a real Chinese dialogue corpus
released in SMP 2018 (The China National Con-
ference on Social Media Processing) for user in-
tent classification tasks in Chinese (Zhang et al.,
2017). The dataset is provided by the iFLYTEK
Corporation, and it can be divided into two part-
s: chit-chat dialogues and task-oriented dialogues.
Here, we only use the task-oriented dialogues.

Dataset Splitting. For zero-shot intent classifi-
cation, we take all the samples of seen intents as
the training set, and all the samples of unseen in-
tents as the test set. For generalized zero-shot in-
tent classification, we randomly take 70% samples
of each seen intent as the training set, and the re-
maining 30% samples of each seen intent and all
the samples of unseen intents as the test set.

5.2 Baselines
We compare ReCapsNet-ZS with the following
state-of-the-art zero-shot learning methods: De-
ViSE (Frome et al., 2013), CMT (Socher et al.,
2013), CDSSM (Chen et al., 2016), Zero-shot
DNN (Kumar et al., 2017) and IntentCapsNet (X-
ia et al., 2018). To make DeViSE and CMT suit-



4806

Method
SNIP-NLU SMP-2018

Seen Unseen Overall Seen Unseen Overall

Acc F1 Acc F1 Acc F1 Acc F1 Acc F1 Acc F1

DeViSE 0.9481 0.6536 0.0211 0.0398 0.4215 0.3049 0.8040 0.6740 0.0270 0.0310 0.5030 0.4250
CMT 0.9755 0.6648 0.0397 0.0704 0.4438 0.3271 0.8314 0.7221 0.0798 0.1069 0.5398 0.4834
CDSSM 0.9549 0.7033 0.0111 0.0218 0.4234 0.3194 0.6653 0.5540 0.1436 0.1200 0.4864 0.4052
Zero-shot DNN 0.9432 0.6679 0.0682 0.1041 0.4488 0.3493 0.7323 0.6116 0.0590 0.0869 0.5013 0.4316
IntentCapsNet 0.9741 0.6517 0.0000 0.0000 0.4200 0.2810 0.8850 0.7281 0.0000 0.0000 0.5375 0.4423

ReCapsNet-ZS-Dim 0.9743 0.6580 0.0000 0.0000 0.4201 0.2837 0.8952 0.7360 0.0000 0.0000 0.5437 0.4470
ReCapsNet-ZS-TM 0.9663 0.6845 0.0981 0.1534 0.4724 0.3824 0.7863 0.7390 0.1896 0.1537 0.5521 0.5092
ReCapsNet-ZS 0.9664 0.6743 0.1121 0.1764 0.4805 0.3911 0.8230 0.7450 0.1720 0.1526 0.5674 0.5124

Table 4: Results of generalized zero-shot intent classification. “Seen”, “Unseen” and “Overall” respectively denote
the performance on the utterances from seen intents, unseen intents, and both seen and unseen intents.

Method SNIPS-NLU SMP-2018

Acc F1 Acc F1

DeViSE 0.7447 0.7446 0.5456 0.3875
CMT 0.7396 0.7206 0.4452 0.4245
CDSSM 0.7588 0.7580 0.4308 0.3765
Zero-shot DNN 0.7165 0.7116 0.4615 0.3897
IntentCapsNet 0.7752 0.7750 0.4864 0.4227

ReCapsNet-ZS-Dim 0.7868 0.7859 0.5005 0.4501
ReCapsNet-ZS-TM 0.7860 0.7837 0.5315 0.4630
ReCapsNet-ZS 0.7996 0.7980 0.5418 0.4769

Table 3: Results of zero-shot intent classification.

able for intent classification, we use a multi-head
self-attention Bi-LSTM model to encode the utter-
ances and then feed the final hidden states to their
zero-shot learning models. In addition, we con-
duct ablation study to evaluate the contribution of
each module of our ReCapsNet-ZS. “ReCapsNet-
ZS-Dim” refers to the model that only uses the di-
mensional attention mechanism, and “ReCapsNet-
ZS-TM” refers to the one that only uses the pro-
posed transformation matrix construction method.

5.3 Implementation Details

Parameter Settings. For SNIPS-NLU, we use
300-dim embeddings pre-trained on English
Wikipedia (Bojanowski et al., 2017). For SMP-
2018, we use 300-dim Chinese word embeddings
pre-trained by Li et al. (2018). The main network
structure hyperparameters are shown in Table 2.
In addition, for the zero-shot classification setting,
we set α to 1 for SNIPS-NLU and 10 for SMP-
2018 respectively. For the generalized zero-shot
classification setting, we set α to 1 for SNIPS-
NLU and 5 for SMP-2018 respectively. To avoid
overfitting, we use dropout with 0.5 dropout rate
on the input of the attention layer. For the loss

function, we set λ = 0.5, m+ = 0.9, m− = 0.1,
β = 0.001, and use the Adam optimizer (Kingma
and Ba, 2015) with initial learning rate 0.01.

Evaluation Metrics. We adopt two widely used
metrics: accuracy (Acc) and micro-average F1-
measure (F1) to evaluate the classification perfor-
mance. Both metrics are computed with the aver-
age value weighted by the support of each class,
where the support means the sample ratio of the
corresponding class.

5.4 Result Analysis

Zero-shot Intent Classification. Table 3 sum-
marizes the average results over 10 runs, where the
top 2 results are highlighted in bold. The baseline
results on SNIPS-NLU are taken from Xia et al.
(2018). The results show that ReCapsNet-ZS out-
performs all the baselines, demonstrating its su-
periority in tackling zero-shot intent classification.
We can also see that ReCapsNet-ZS performs bet-
ter than either ReCapsNet-ZS-Dim or ReCapsNet-
ZS-TM, which shows the effectiveness of both
of the dimensional attention mechanism and the
transformation matrix construction method.

Generalized Zero-shot Intent Classification.
Table 4 shows the average results over 10 runs,
where the top 2 results are highlighted in bold.
We can make the following observations. 1) The
performances look much worse than the standard
zero-shot setting, but it is not surprising since the
seen intent labels are included in the test phase and
it makes the problem harder. 2) ReCapsNet-ZS
sometimes performs slightly worse than the base-
lines in detecting seen intents, which is because
some baselines tend to classify the test utterances
as seen intents, which on the other hand explain-



4807

Figure 2: Comparison of the dimensional attentions of
the word “book” in different contexts.

s why they perform much worse in detecting un-
seen intents. 3) ReCapsNet-ZS and ReCapsNet-
ZS-TM perform much better than others in de-
tecting unseen intents, whereas IntentCapsNet and
ReCapsNet-ZS-Dim both have 0% Acc and F1.
This verifies that the proposed transformation ma-
trix construction method has much better gener-
alization ability in detecting unseen intents. 4)
Overall, ReCapsNet-ZS consistently performs the
best, which further demonstrates the superiority of
ReCapsNet-ZS on the generalized zero-shot intent
classification tasks.

5.5 Visualization

Dimensional Attentions. Figure 2 visualizes
the attention score for each dimension of the same
word “book” in two different utterances (contexts)
by heatmaps. The utterances are “Book a restau-
rant in Michigan for 4 people” and “Give 4 out
of 6 points to this book”, which are taken from
SNIPS-NLU. It can be seen that for the two utter-
ances the attention values of the word “book” ex-
hibit completely different patterns, which makes
sense as it contains different meanings in differ-
ent contexts. Furthermore, for each utterance, the
attention scores of “book” on different dimensions
are also quite different. This shows that the dimen-
sional attention mechanism can effectively capture
the semantic differences of the same word in dif-
ferent contexts and encode more useful informa-
tion than the traditional self-attention method, and
thus helps to alleviate the polysemy problem.

Similarity Scores. Figure 3 visualizes the sim-
ilarity scores between the unseen intent “movie”
and the seen intents learned via metric learning by
IntentCapsNet and ReCapsNet-ZS respectively on
SMP-2018. It can be seen that IntentCapsNet can
only discover few connections between the unseen
and seen intents. In contrast, ReCapsNet-ZS can
detect a lot more connections between them. Fur-
ther, though the similarity scores of ReCapsNet-

Figure 3: Comparison of the similarity scores between
the unseen intent “movie” and the seen intents learned
by IntentCapsNet and ReCapsNet-ZS (ours).

ZS may introduce some noise (e.g., “contacts”),
they can capture many more positive relations be-
tween “movie” and the seen intents (e.g., “nov-
el”, “video”, “schedule”, and “datetime”), which
is beneficial for detecting unseen intents.

6 Conclusion

In this paper, we have proposed a novel frame-
work to reconstruct capsule networks for zero-shot
intent classification and demonstrated empirically
that it compares favourably with existing methods
on some real dialogue datasets. The performance
gains of our method come from two aspects: the
introduction of a new dimensional attention mod-
ule to capsule networks for feature extraction and
the proposal of a new transformation scheme for
detecting unseen intents.

There are several directions of the future works.
One is to customize our model for few-shot intent
classification. Another is to extend our framework
to deal with multiple-intent classification. We also
plan to apply our model in dialogue systems for
low-resource languages such as Cantonese.

Acknowledgments

We would like to thank the anonymous reviewers
for their helpful comments towards improving the
manuscript. This research was supported by the
grant HK ITF UIM/377.

References
Lei Jimmy Ba, Kevin Swersky, Sanja Fidler, and Rus-

lan Salakhutdinov. 2015. Predicting deep zero-shot
convolutional neural networks using textual descrip-
tions. In IEEE International Conference on Com-
puter Vision (ICCV), pages 4247–4255.

Piotr Bojanowski, Edouard Grave, Armand Joulin, and
Tomas Mikolov. 2017. Enriching word vectors with
subword information. Transactions of the Associa-



4808

tion for Computational Linguistics (TACL), 5:135–
146.

Yun-Nung Chen, Asli Çelikyilmaz, and Dilek Hakkani-
Tür. 2017. Deep learning for dialogue systems. In
Annual Meeting of the Association for Computation-
al Linguistics (ACL), pages 8–14.

Yun-Nung Chen, Dilek Z. Hakkani-Tür, and Xiaodong
He. 2016. Zero-shot learning of intent embeddings
for expansion by convolutional deep structured se-
mantic models. In IEEE International Conference
on Acoustics, Speech and Signal Processing (ICAS-
SP), pages 6045–6049.

Alice Coucke, Alaa Saade, Adrien Ball, Théodore
Bluche, Alexandre Caulier, David Leroy, Clément
Doumouro, Thibault Gisselbrecht, Francesco Calt-
agirone, Thibaut Lavril, Maël Primet, and Joseph
Dureau. 2018. Snips voice platform: an embedded
spoken language understanding system for private-
by-design voice interfaces. arXiv, abs/1805.10190.

Emmanuel Ferreira, Bassam Jabaian, and Fabrice
Lefèvre. 2015a. Online adaptative zero-shot learn-
ing spoken language understanding using word-
embedding. In IEEE International Conference on
Acoustics, Speech and Signal Processing (ICASSP),
pages 5321–5325.

Emmanuel Ferreira, Bassam Jabaian, and Fabrice
Lefèvre. 2015b. Zero-shot semantic parser for spo-
ken language understanding. In Annual Conference
of the International Speech Communication Associ-
ation (INTERSPEECH), pages 1403–1407.

Andrea Frome, Gregory S. Corrado, Jonathon Shlens,
Samy Bengio, Jeffrey Dean, Marc’Aurelio Ranzato,
and Tomas Mikolov. 2013. Devise: A deep visual-
semantic embedding model. In Advances in Neu-
ral Information Processing Systems (NIPS), pages
2121–2129.

Ruiying Geng, Binhua Li, Yongbin Li, Yuxiao Ye,
Ping Jian, and Jian Sun. 2019. Few-shot tex-
t classification with induction network. arXiv, ab-
s/1902.10482.

Sepp Hochreiter and Jürgen Schmidhuber. 1997.
Long short-term memory. Neural Computation,
9(8):1735–1780.

Jian Hu, Gang Wang, Frederick H. Lochovsky, Jian-
Tao Sun, and Zheng Chen. 2009. Understanding
user’s query intent with wikipedia. In Internation-
al Conference on World Wide Web (WWW), pages
471–480.

Diederik P. Kingma and Jimmy Ba. 2015. Adam: A
method for stochastic optimization. In International
Conference on Learning Representations (ICLR).

Anjishnu Kumar, Pavankumar Reddy Muddireddy,
Markus Dreyer, and Björn Hoffmeister. 2017. Zero-
shot learning across heterogeneous overlapping do-
mains. In Annual Conference of the Interna-

tional Speech Communication Association (INTER-
SPEECH), pages 2914–2918.

Hugo Larochelle, Dumitru Erhan, and Yoshua Bengio.
2008. Zero-data learning of new tasks. In AAAI
Conference on Artificial Intelligence (AAAI), pages
646–651.

Shen Li, Zhe Zhao, Renfen Hu, Wensi Li, Tao Liu, and
Xiaoyong Du. 2018. Analogical reasoning on chi-
nese morphological and semantic relations. In An-
nual Meeting of the Association for Computational
Linguistics (ACL), pages 138–143.

Bing Liu and Ian Lane. 2016. Attention-based recur-
rent neural network models for joint intent detection
and slot filling. In Annual Conference of the Interna-
tional Speech Communication Association (INTER-
SPEECH), pages 685–689.

Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey
Dean. 2013. Efficient estimation of word represen-
tations in vector space. In International Conference
on Learning Representations (ICLR).

Jinseok Nam, Eneldo Loza Menc’ia, and Johannes
Fürnkranz. 2016. All-in text: Learning documen-
t, label, and word representations jointly. In AAAI
Conference on Artificial Intelligence (AAAI), pages
1948–1954.

Mark Palatucci, Dean Pomerleau, Geoffrey E. Hinton,
and Tom M. Mitchell. 2009. Zero-shot learning with
semantic output codes. In Advances in Neural In-
formation Processing Systems (NIPS), pages 1410–
1418.

Suman Ravuri and Andreas Stolcke. 2015. Recurrent
neural network and lstm models for lexical utterance
classification. In Annual Conference of the Interna-
tional Speech Communication Association (INTER-
SPEECH), pages 135–139.

Sara Sabour, Nicholas Frosst, and Geoffrey E. Hin-
ton. 2017. Dynamic routing between capsules. In
Advances in Neural Information Processing System-
s (NIPS), pages 3859–3869.

Prateek Veeranna Sappadla, Jinseok Nam, Eneldo
Loza Menc’ia, and Johannes Fürnkranz. 2016. Us-
ing semantic similarity for multi-label zero-shot
classification of text documents. In European Sym-
posium on Artificial Neural Networks (ESANN).

Lütfi Kerem Şenel, Ihsan Utlu, Veysel Yücesoy, Aykut
Koc, and Tolga Cukur. 2018. Semantic structure and
interpretability of word embeddings. IEEE/ACM
Transactions on Audio, Speech, and Language Pro-
cessing, 26(10):1769–1779.

Tao Shen, Tianyi Zhou, Guodong Long, Jing Jiang,
Shirui Pan, and Chengqi Zhang. 2018. Disan: Di-
rectional self-attention network for rnn/cnn-free lan-
guage understanding. In AAAI Conference on Artifi-
cial Intelligence (AAAI), pages 5446–5455.



4809

Richard Socher, Milind Ganjoo, Christopher D. Man-
ning, and Andrew Y. Ng. 2013. Zero-shot learning
through cross-modal transfer. In Advances in Neural
Information Processing Systems (NIPS), pages 935–
943.

Gokhan Tur, Li Deng, Dilek Hakkani-Tr, and Xi-
aodong He. 2012. Towards deeper understanding:
Deep convex networks for semantic utterance clas-
sification. In IEEE International Conference on A-
coustics, Speech and Signal Processing (ICASSP),
pages 5045–5048.

Congying Xia, Chenwei Zhang, Xiaohui Yan, Y-
i Chang, and Philip S. Yu. 2018. Zero-shot user in-
tent detection via capsule neural networks. In Con-
ference on Empirical Methods in Natural Language
Processing (EMNLP), pages 3090–3099.

Yongqin Xian, Zeynep Akata, Gaurav Sharma, Quyn-
h N. Nguyen, Matthias Hein, and Bernt Schiele.
2016. Latent embeddings for zero-shot classifica-
tion. In IEEE Conference on Computer Vision and
Pattern Recognition (CVPR), pages 69–77.

Puyang Xu and Ruhi Sarikaya. 2013. Convolutional
neural network based triangular CRF for joint inten-
t detection and slot filling. In IEEE Workshop on
Automatic Speech Recognition and Understanding
(ASRU Workshop), pages 78–83.

Min Yang, Wei Zhao, Jianbo Ye, Zeyang Lei, Zhou
Zhao, and Soufei Zhang. 2018. Investigating cap-
sule networks with dynamic routing for text classifi-
cation. In Conference on Empirical Methods in Nat-
ural Language Processing (EMNLP), pages 3110–
3119.

Majid Yazdani and James Henderson. 2015. A mod-
el of zero-shot learning of spoken language un-
derstanding. In Conference on Empirical Method-
s in Natural Language Processing (EMNLP), pages
244–249.

Yiming Ying and Peng Li. 2012. Distance metric learn-
ing with eigenvalue optimization. Journal of Ma-
chine Learning Research, 13:1–26.

Chenwei Zhang, Wei Fan, Nan Du, and Philip S. Yu.
2016. Mining user intentions from medical queries:
A neural network based heterogeneous jointly mod-
eling approach. In International Conference on
World Wide Web (WWW), pages 1373–1384.

Jingqing Zhang, Piyawat Lertvittayakumjorn, and Yike
Guo. 2019. Integrating semantic knowledge to
tackle zero-shot text classification. arXiv, ab-
s/1903.12626.

Weinan Zhang, Zhigang Chen, Wanxiang Che, Guop-
ing Hu, and Ting Liu. 2017. The first evaluation of
chinese human-computer dialogue technology. arX-
iv, abs/1709.10217.


