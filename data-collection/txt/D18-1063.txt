















































Learning Unsupervised Word Translations Without Adversaries


Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 627–632
Brussels, Belgium, October 31 - November 4, 2018. c©2018 Association for Computational Linguistics

627

Learning Unsupervised Word Translations Without Adversaries

Tanmoy Mukherjee1 and Makoto Yamada2 and Timothy Hospedales1
1 The University of Edinburgh 2 Kyoto University/ RIKEN AIP/ JST PRESTO

{t.mukherjee@sms.,t.hospedales@}ed.ac.uk, myamada@i.kyoto-u.ac.jp

Abstract

Word translation, or bilingual dictionary in-
duction, is an important capability that im-
pacts many multilingual language processing
tasks. Recent research has shown that word
translation can be achieved in an unsuper-
vised manner, without parallel seed dictionar-
ies or aligned corpora. However, state of the
art methods for unsupervised bilingual dictio-
nary induction are based on generative adver-
sarial models, and as such suffer from their
well known problems of instability and hyper-
parameter sensitivity. We present a statistical
dependency-based approach to bilingual dic-
tionary induction that is unsupervised – no
seed dictionary or parallel corpora required;
and introduces no adversary – therefore being
much easier to train. Our method performs
comparably to adversarial alternatives and out-
performs prior non-adversarial methods.

1 Introduction

Translating words between languages, or more
generally inferring bilingual dictionaries, is a
long-studied research direction with applications
including machine translation (Lample et al.,
2017), multilingual word embeddings (Klemen-
tiev et al., 2012), and knowledge transfer to low
resource languages (Guo et al., 2016). Research
here has a long history under the guise of deci-
pherment (Knight et al., 2006). Current contempo-
rary methods have achieve effective word transla-
tion through theme-aligned corpora (Gouws et al.,
2015), or seed dictionaries (Mikolov et al., 2013).

Mikolov et al. (2013) showed that monolingual
word embeddings exhibit isomorphism across lan-
guages, and can be aligned with a simple lin-
ear transformation. Given two sets word vectors
learned independently from monolingual corpora,
and a dictionary of seed pairs to learn a linear
transformation for alignment; they were able to

estimate a complete bilingual lexicon. Many stud-
ies have since followed this approach, proposing
various improvements such as orthogonal map-
pings (Artetxe et al., 2016) and improved objec-
tives (Lazaridou et al., 2015).

Obtaining aligned corpora or bilingual seed dic-
tionaries is nevertheless not straightforward for all
language pairs. This has motivated a wave of
very recent research into unsupervised word trans-
lation: inducing bilingual dictionaries given only
monolingual word embeddings (Conneau et al.,
2018; Zhang et al., 2017b,a; Artetxe et al., 2017).
The most successful have leveraged ideas from
Generative Adversarial Networks (GANs) (Good-
fellow et al., 2014). In this approach the genera-
tor provides the cross-modal mapping, taking em-
beddings of dictionary words in one language and
‘generating’ their translation in another. The dis-
criminator tries to distinguish between this ‘fake’
set of translations and the true dictionary of em-
beddings in the target language. The two play a
competitive game, and if the generator learns to
fool the discriminator, then its cross-modal map-
ping should be capable of inducing a complete dic-
tionary, as per Mikolov et al. (2013).

Despite these successes, such adversarial meth-
ods have a number of well-known drawbacks (Ar-
jovsky et al., 2017): Due to the nature of their
min-max game, adversarial training is very un-
stable, and they are prone to divergence. It
is extremely hyper-parameter sensitive, requiring
problem-specific tuning. Convergence is also hard
to diagnose and does not correspond well to effi-
cacy of the generator in downstream tasks (Hoshen
and Wolf, 2018).

In this paper, we propose an alternative sta-
tistical dependency-based approach to unsuper-
vised word translation. Specifically, we propose to
search for the cross-lingual word pairing that max-
imizes statistical dependency in terms of squared



628

loss mutual information (SMI) (Yamada et al.,
2015; Suzuki and Sugiyama, 2010). Compared
to prior statistical dependency-based approaches
such as Kernelized Sorting (KS) (Quadrianto
et al., 2009) we advance: (i) through use of SMI
rather than their Hilbert Schmidt Independence
Criterion (HSIC) and (ii) through jointly optimis-
ing cross-modal pairing with representation learn-
ing within each view. In contrast to prior work that
uses a fixed representation, by non-linearly pro-
jecting monolingual world vectors before match-
ing, we learn a new embedding where statistical
dependency is easier to establish. Our method: (i)
achieves similar unsupervised translation perfor-
mance to recent adversarial methods, while being
significantly easier to train and (ii) clearly outper-
forms prior non-adversarial methods.

2 Proposed model

2.1 Deep Distribution Matching
Let dataset D contain two sets of unpaired mono-
lingual word embeddings from two languages
D = ({xi}ni=1, {yj}nj=1) where x,y ∈ Rd. Let π
be a permutation function over {1, 2, . . . , n}, and
Π the corresponding permutation indicator ma-
trix: Π ∈ {0, 1}n×n,Π1n = 1n, and Π>1n =
1n. Where 1n is the n-dimensional vector with
all ones. We aim to optimize for both the per-
mutation Π (bilingual dictionary), and non-linear
transformations gx(·) and gy(·) of the respective
wordvectors, that maximize statistical dependency
between the views. While regularising by requir-
ing the original word embedding information is
preserved through reconstruction using decoders
fx(·) and fy(·). Our overall loss function is:

min
Θx,Θy ,Π

Ω(D; Θx,Θy)︸ ︷︷ ︸
Regularizer

−λDΠ(D; Θx,Θy)︸ ︷︷ ︸
Dependency

,

DΠ(D; Θx,Θy) = DΠ({gx(xi), gy(yπ(i))}ni=1),

Ω(D; Θx,Θy) =
n∑
i=1

‖xi − fx(gx(xi))‖22

+ ‖yi − fy(gy(yi))‖22
+R(Θx) +R(Θy).

(1)

where Θs parameterize the encoding and recon-
struction transformations, R(·) is a regularizer
(e.g., `2-norm and `1-norm), and DΠ(·, ·) is a sta-
tistical dependency measure. Crucially compared
to prior methods such as matching CCA (Haghighi

et al., 2008), dependency measures such as SMI do
not need comparable representations to get started,
making the bootstrapping problem less severe.

2.2 Dependence Estimation

Squared-Loss Mutual Information (SMI)
The squared loss mutual information between two
random variables x and y is defined as (Suzuki
and Sugiyama, 2010):

SMI =
∫∫ (

p(x,y)

p(x)p(y)
− 1
)2

p(x)p(y)dxdy,

which is the Pearson divergence (Pearson, 1900)
from p(x,y) to p(x)p(y). The SMI is an f -
divergence (Ali and Silvey, 1966). That is, it is
a non-negative measure and is zero only if the ran-
dom variables are independent.

To measure SMI from a set of samples we take
a direct density ratio estimation approach (Suzuki
and Sugiyama, 2010), which leads (Yamada et al.,
2015) to the estimator:

ŜMI({(xi,yi)}ni=1) =
1

2n
tr (diag (α̂)KL)− 1

2
,

where K ∈ Rn×n and L ∈ Rn×n are the gram
matricies for x and y respectively, and

Ĥ =
1

n2
(KK>) ◦ (LL>),

ĥ =
1

n
(K ◦L)1n, α̂ =

(
Ĥ + λIn

)−1
ĥ,

λ > 0 is a regularizer and In ∈ Rn×n is the iden-
tity matrix.

SMI for Matching SMI computes the depen-
dency between two sets of variables, under an as-
sumption of known correspondence. In our ap-
plication this corresponds to a measure of depen-
dency between two aligned sets of monolingual
wordvectors. To exploit SMI for matching, we
introduce a permutation variable Π by replacing
L→ Π>LΠ in the estimator:

ŜMI({(xi,yπ(i))}n1 )=
1

2n
tr
(
diag (α̂Π)KΠ

>LΠ
)
− 1
2
,

that will enable optimizing Π to maximize SMI.

2.3 Optimization of parameters
To initialize Θx and Θy, we first independently
estimate them using autoencoders. Then we em-
ploy an alternative optimization on Eq. (1) for



629

(Θx,Θy) and Π until convergence. We use 3
layer MLP neural networks for both f and g. Al-
gorithm 1 summarises the steps.
Optimization for Θx and Θy With fixed per-
mutation matrix Π (or π), the objective function

min
Θx,Θy

Ω(D; Θx,Θy)− λDΠ(D; Θx,Θy) (2)

is an autoencoder optimization with regularizer
DΠ(·), and can be solved with backpropagation.
Optimization for Π To find the permuta-
tion (word matching) Π that maximizes SMI
given fixed encoding parameters Θx,Θy, we only
need to optimize the dependency term DΠ in
Eq. (1). We employ the LSOM algorithm (Yamada
et al., 2015). The estimator of SMI for samples
{gx(xi), gy(yπ(i))}ni=1 encoded with gx, gy is:

ŜMI =
1

2n
tr
(

diag (α̂Θ,Π)KΘxΠ
>LΘyΠ

)
− 1

2
.

Which leads to the optimization problem:

max
Π∈{0,1}n×n

tr
(

diag (α̂Θ,Π)KΘxΠ
>LΘyΠ

)
s.t. Π1n = 1n,Π>1n = 1n. (3)

Since the optimization problem is NP-hard, we
iteratively solve the relaxed problem (Yamada
et al., 2015):

Πnew = (1− η)Πold+

η argmax
Π

tr
(
diag

(
α̂Θ,Πold

)
KΘxΠ

>LΘyΠ
old
)
,

where 0 < η ≤ 1 is a step size. The optimization
problem is a linear assignment problem (LAP).
Thus, we can efficiently solve the algorithm by
using the Hungarian method (Kuhn, 1955). To get
discrete Π, we solve the last step by setting η = 1.

Intuitively, this can be seen as searching for
the permutation Π for which the data in the two
(initially unsorted views) have a matching within-
view affinity (gram) matrix, where matching is de-
fined by maximum SMI.

3 Experiments

In this section, we evaluate the efficacy of our pro-
posed method against various state of the art meth-
ods for word translation.
Implementation Details Our autoencoder con-
sists of two layers with dropout and a tanh non-
linearity. We use polynomial kernel to compute

Algorithm 1 SMI-based unsupervised word trans-
lation
Input: Unpaired word embeddings D =
({xi}ni=1, {yj}nj=1).

1: Init: weights Θx, Θy, permutation matrix Π.
2: while not converged do
3: Update Θx,Θy given Π: Backprop (2).
4: Update Π given Θx,Θy: LSOM (3).
5: end while

Output: Permutation Matrix Π. Params Θx, Θy.

the gram matrices K and L. For all pairs of lan-
guages, we fix the number of training epochs to
20. All the word vectors are `2 unit normalized.
For CSLS we set the number of neighbors to 10.
For optimizing Π at each epoch, we set the step
size η = 0.75 and use 20 iterations. For the regu-
larization R(Θ), we use the sum of the Frobenius
norms of weight matrices. We train Θ using full
batch gradient-descent, with learning rate 0.05.
Datasets We performed experiments on the
publicly available English-Italian, English-
Spanish and English-Chinese datasets released
by (Dinu and Baroni, 2015; Zhang et al., 2017b;
Vulic and Moens, 2013). We name this collective
set of benchmarks BLI. We also conduct further
experiments on a much larger recent public
benchmark, MUSE (Conneau et al., 2018)1.
Setting and Metrics We evaluate all methods
in terms of Precision@1, following standard prac-
tice. We note that while various methods in the
literature were initially presented as fully super-
vised (Mikolov et al., 2013), semi-supervised (us-
ing a seed dictionary) (Haghighi et al., 2008), or
unsupervised (Zhang et al., 2017b), most of them
can be straightforwardly adapted to run in any of
these settings. Therefore we evaluate all methods
both in the unsupervised setting in which we are
primarily interested, and also the commonly eval-
uated semi-supervised setting with 500 seed pairs.
Competitors: Non-Adversarial In terms of
competitors that, like us, do not make use
of GANs, we evaluate: Translation Matrix
(Mikolov et al., 2013), which alternates between
estimating a linear transformation by least squares
and matching by nearest neighbour (NN). Mul-
tilingual Correlation (Faruqui and Dyer, 2014),
and Matching CCA (Haghighi et al., 2008),
which alternates between matching and estimat-

1https://github.com/facebookresearch/MUSE/



630

MUSE Dataset BLI Datasets

Methods es-en en-es it-en en-it zh-en en-zh es-en en-es it-en en-it zh-en en-zh

TM (Mikolov et al., 2013) 5.6 4.8 5.2 4.8 2.6 1.8 3.2 2.9 4.6 4.2 3.2 2.0

CCA (Faruqui and Dyer, 2014) 6.1 5.6 5.8 5.2 3.1 2.3 5.3 5.0 4.6 4.1 3.2 2.9

MCCA (Haghighi et al., 2008) 5.7 5.1 5.4 4.8 3.0 2.2 2.9 2.5 4.2 4.1 2.8 1.9

KS (Quadrianto et al., 2009) 8.3 7.4 6.3 5.7 4.8 3.2 9.6 8.9 8.2 7.3 3.7 3.5

Self-Training (Artetxe et al., 2017) 12.4 12.2 10.7 10.2 5.8 5.6 15.8 14.5 13.7 12.7 14.8 13.4

EMDOT (Zhang et al., 2017b) 72.4 71.8 72.8 72.6 32.8 31.7 29.3 31.2 25.6 28.4 24.2 27.8

W-GAN (Zhang et al., 2017b) 78.2 77.4 75.3 74.8 38.6 37.5 23.4 26.7 24.0 25.3 21.2 22.8

GAN-NN (Conneau et al., 2018) 69.8 71.3 72.1 71.5 41.3 40.2 21.4 24.3 22.7 23.2 21.3 21.8

Deep-SMI (Ours) 75.9 80.6 75.7 75.2 38.5 38.1 27.3 28.2 25.7 26.4 22.5 22.3

Deep-SMI-CSLS 79.2 84.5 78.8 78.5 43.7 42.8 28.6 29.3 26.7 28.2 23.2 24.7

Table 1: Unsupervised word translation on MUSE and BLI datasets. Precision @ 1 metric. Top group: Con-
ventional methods. Middle group: Adversarial methods. Bottom group: Our methods. Language codes
zh=Chinese,en=English,es=Spanish,it=Italian

MUSE Dataset BLI Datasets

Methods es-en en-es it-en en-it zh-en en-zh es-en en-es it-en en-it zh-en en-zh

TM (Mikolov et al., 2013) 32.6 30.1 34.3 33.6 32.4 31.2 28.2 32.1 29.2 32.1 28.5 27.4

CCA (Faruqui and Dyer, 2014) 27.3 27.1 25.4 24.2 23.1 20.2 25.8 28.3 24.3 25.1 19.2 22.8

MCCA (Haghighi et al., 2008) 26.3 25.8 22.7 21.3 24.5 23.8 24.2 26.1 17.6 19.2 18.4 21.6

KS (Quadrianto et al., 2009) 34.5 32.6 35.2 33.8 34.3 33.2 27.5 29.1 34.3 32.1 20.0 23.2

Self-Training (Artetxe et al., 2017) 35.8 31.4 36.0 34.6 34.3 33.0 27.8 29.8 39.7 33.8 23.6 21.4

EMDOT (Zhang et al., 2017b) 78.2 76.3 75.0 74.6 33.2 32.0 30.2 28.4 31.7 30.3 29.3 28.7

W-GAN (Zhang et al., 2017b) 81.2 80.5 77.2 75.1 39.0 38.2 28.6 27.9 33.7 29.5 36.7 34.4

GAN-NN (Conneau et al., 2018) 74.8 72.3 74.3 72.5 43.2 42.7 22.8 26.1 27.9 27.1 24.2 23.6

Deep-SMI (Ours) 80.6 75.9 78.2 76.7 45.7 44.6 38.5 37.6 42.3 38.2 29.2 27.4

Deep-SMI-CSLS 84.5 79.2 79.7 78.7 42.3 44.4 28.6 29.3 26.7 28.2 23.2 24.7

Table 2: Semi-supervised word translation on MUSE and BLI using 500 seed pair initial dictionary. Precision @
1 metric. Top group: Conventional methods. Middle group: Adversarial methods. Bottom group: Our methods.

ing a joint linear subspace. Kernelized Sort-
ing (Quadrianto et al., 2009), which directly uses
HSIC-based statistical dependency to match het-
erogeneous data points. Self Training (Artetxe
et al., 2017) A recent state of the art method
that alternate between estimating an orthonormal
transformation, and NN matching.

Competitors: Adversarial In terms of com-
petitors that do make use of adversarial training,
we compare: W-GAN and EMDOT (Zhang et al.,
2017b) make use of adversarial learning using
Wasserstein GAN and Earth Movers Distance re-
spectively. GAN-NN (Conneau et al., 2018) uses
adversarial learning to train an orthogonal trans-
formation, along with some refinement steps and
an improvement to the conventional NN match-
ing procedure called ‘cross-domain similarity lo-

cal scaling’ (CSLS). Since this is a distinct step,
we also evaluate our method with CSLS.

We use the provided code for GAN-NN
and Self-Train, while re-implementing EDOT/W-
GAN to avoid dependency on theano.

3.1 Results

Fully Unsupervised Table 1 presents compar-
ative results for unsupervised word translation on
BLI and MUSE. From these we observe: (i) Our
method (bottom) is consistently and significantly
better than non-adversarial alternatives (top). (ii)
Compared to adversarial alternatives Deep-SMI
performs comparably.

All methods generally perform better on the
MUSE dataset than BLI. These differences are
due to a few factors: MUSE is a significantly



631

0 10 20 30 40

Epochs

0

0.2

0.4

0.6

0.8

T
ra

n
s
la

ti
o

n
 A

c
c
u

ra
c
y

-0.9

-0.8

-0.7

-0.6

-0.5

L
o

s
sAcc. Unsupervised

Acc. Semisupervised

Loss Unsupervised

Figure 1: Training process of Deep-SMI

larger dataset than BLI, benefitting methods that
can exploit a large amount of training data. In the
ground-truth annotation, BLI contains 1-1 transla-
tions while MUSE contains more realistic 1-many
translations (if any correct translation is picked,
a success is counted), making it easier to reach a
higher score.

Semi-supervised Results using a 500-word
bilingual seed dictionary are presented in Table 2.
From these we observe: (i) The conventional
methods’ performances (top) jump up, showing
that they are more competitive if at least some
sparse data is available. (ii) Deep-SMI perfor-
mance also improves, and still outperforms the
classic methods significantly overall. (iii) Again,
we perform comparably to the GAN methods.

3.2 Discussion

Figure 1 shows the convergence process of Deep-
SMI. From this we see that: (i) Unlike the adver-
sarial methods, our objective (Eq. (1)) improves
smoothly over time, making convergence much
easier to assess. (ii) Unlike the adversarial meth-
ods, our accuracy generally mirrors the model’s
loss. In contrast, the various losses of the adver-
sarial approaches do not well reflect translation ac-
curacy, making model selection or early stopping
a challenge in itself. Please compare our Figure 1
with Fig 3 in Zhang et al. (2017b), and Fig 2 in
Conneau et al. (2018).

There are two steps in our optimization: match-
ing permutation Π and representation weights Θ.
Although this is an alternating optimization, it is
analogous to an EM-type algorithm optimizing la-
tent variables (Π) and parameters (Θ). While
local minima are a risk, every optimisation step
for either variable reduces our objective Eq. (1).

There is no min-max game, so no risk of di-
vergence as in the case of adversarial GAN-type
methods.

Our method can also be understood as provid-
ing an unsupervised Deep-CCA type model for re-
lating heterogeneous data across two views. This
is in contrast to the recently proposed unsuper-
vised shallow CCA (Hoshen and Wolf, 2018), and
conventional supervised Deep-CCA (Chang et al.,
2018) that requires paired data for training; and us-
ing SMI rather than correlation as the optimisation
objective.

4 Conclusion

We have presented an effective approach to unsu-
pervised word translation that performs compara-
bly to adversarial approaches while being signifi-
cantly easier to train and diagnose; as well as out-
performing prior non-adversarial approaches.

References
Syed M. Ali and Samuel. D. Silvey. 1966. A general

class of coefficients of divergence of one distribu-
tion from another. Journal of the Royal Statistical
Society. Series B (Methodological), 28(1):131–142.

Martín Arjovsky, Soumith Chintala, and Léon Bottou.
2017. Wasserstein generative adversarial networks.
In ICML.

Mikel Artetxe, Gorka Labaka, and Eneko Agirre. 2016.
Learning principled bilingual mappings of word em-
beddings while preserving monolingual invariance.
In EMNLP.

Mikel Artetxe, Gorka Labaka, and Eneko Agirre. 2017.
Learning bilingual word embeddings with (almost)
no bilingual data. In ACL.

Xiaobin Chang, Tao Xiang, and Timothy M.
Hospedales. 2018. Scalable and effective deep CCA
via soft decorrelation. In CVPR.

Alexis Conneau, Guillaume Lample, Marc’Aurelio
Ranzato, Ludovic Denoyer, and Hervé Jégou. 2018.
Word translation without parallel data. In ICLR.

Georgiana Dinu and Marco Baroni. 2015. Improving
zero-shot learning by mitigating the hubness prob-
lem. ICLR Workshops.

Manaal Faruqui and Chris Dyer. 2014. Improving vec-
tor space word representations using multilingual
correlation. In EACL.

Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza,
Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron
Courville, and Yoshua Bengio. 2014. Generative ad-
versarial nets. In NIPS.



632

Stephan Gouws, Yoshua Bengio, and Greg Corrado.
2015. Bilbowa: Fast bilingual distributed represen-
tations without word alignments. In ICML.

Jiang Guo, Wanxiang Che, David Yarowsky, Haifeng
Wang, and Ting Liu. 2016. A distributed
representation-based framework for cross-lingual
transfer parsing. JAIR, 55(1):995–1023.

Aria Haghighi, Percy Liang, Taylor Berg-Kirkpatrick,
and Dan Klein. 2008. Learning bilingual lexicons
from monolingual corpora. In ACL.

Yedid Hoshen and Lior Wolf. 2018. Unsupervised cor-
relation analysis. In CVPR.

Alexandre Klementiev, Ivan Titov, and Binod Bhat-
tarai. 2012. Inducing crosslingual distributed rep-
resentations of words. In COLING.

K. Knight, A. Nair, N. Rathod, and K. Yamada. 2006.
Unsupervised analysis for decipherment problems.
In Proc. ACL-COLING.

Harold W Kuhn. 1955. The hungarian method for the
assignment problem. Naval research logistics quar-
terly, 2(1-2):83–97.

Guillaume Lample, Ludovic Denoyer, and
Marc’Aurelio Ranzato. 2017. Unsupervised
machine translation using monolingual corpora
only. arXiv preprint arXiv:1711.00043.

Angeliki Lazaridou, Georgiana Dinu, and Marco Ba-
roni. 2015. Hubness and pollution: Delving into
cross-space mapping for zero-shot learning. In ACL.

Tomas Mikolov, Google Inc, Mountain View, Quoc V.
Le, Google Inc, Ilya Sutskever, and Google Inc.
2013. Exploiting similarities among languages for
machine translation.

Karl Pearson. 1900. On the criterion that a given
system of deviations from the probable in the case
of a correlated system of variables is such that it
can be reasonably supposed to have arisen from
random sampling. The London, Edinburgh, and
Dublin Philosophical Magazine and Journal of Sci-
ence, 50(302):157–175.

Novi Quadrianto, Le Song, and Alex J Smola. 2009.
Kernelized sorting. In NIPS.

Taiji Suzuki and Masashi Sugiyama. 2010. Sufficient
dimension reduction via squared-loss mutual infor-
mation estimation. In AISTATS.

Ivan Vulic and Marie-Francine Moens. 2013. Cross-
lingual semantic similarity of words as the similarity
of their semantic word responses. In HLT-NAACL.

Makoto Yamada, Leonid Sigal, Michalis Raptis,
Machiko Toyoda, Yi Chang, and Masashi Sugiyama.
2015. Cross-domain matching with squared-loss
mutual information. IEEE TPAMI, 37(9):1764–
1776.

Meng Zhang, Yang Liu, Huanbo Luan, and Maosong
Sun. 2017a. Adversarial training for unsupervised
bilingual lexicon induction. In ACL.

Meng Zhang, Yang Liu, Huanbo Luan, and Maosong
Sun. 2017b. Earth mover’s distance minimization
for unsupervised bilingual lexicon induction. In
EMNLP.


