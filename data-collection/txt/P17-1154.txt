



















































Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics


Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, pages 1679–1689
Vancouver, Canada, July 30 - August 4, 2017. c©2017 Association for Computational Linguistics

https://doi.org/10.18653/v1/P17-1154

Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, pages 1679–1689
Vancouver, Canada, July 30 - August 4, 2017. c©2017 Association for Computational Linguistics

https://doi.org/10.18653/v1/P17-1154

Linguistically Regularized LSTM for Sentiment Classification

Qiao Qian1, Minlie Huang1∗, Jinhao Lei2, Xiaoyan Zhu1
1State Key Laboratory of Intelligent Technology and Systems

Tsinghua National Laboratory for Information Science and Technology
Dept. of Computer Science and Technology, Tsinghua University, Beijing 100084, PR China

2Dept. of Thermal Engineering, Tsinghua University, Beijing 100084, PR China
qianqiaodecember29@126.com, aihuang@tsinghua.edu.cn

leijh14@gmail.com , zxy-dcs@tsinghua.edu.cn

Abstract

This paper deals with sentence-level sen-
timent classification. Though a variety
of neural network models have been pro-
posed recently, however, previous models
either depend on expensive phrase-level
annotation, most of which has remark-
ably degraded performance when trained
with only sentence-level annotation; or do
not fully employ linguistic resources (e.g.,
sentiment lexicons, negation words, inten-
sity words). In this paper, we propose sim-
ple models trained with sentence-level an-
notation, but also attempt to model the lin-
guistic role of sentiment lexicons, nega-
tion words, and intensity words. Results
show that our models are able to cap-
ture the linguistic role of sentiment words,
negation words, and intensity words in
sentiment expression.

1 Introduction

Sentiment classification aims to classify text to
sentiment classes such as positive or negative, or
more fine-grained classes such as very positive,
positive, neutral, etc. There has been a variety of
approaches for this purpose such as lexicon-based
classification (Turney, 2002; Taboada et al., 2011),
and early machine learning based methods (Pang
et al., 2002; Pang and Lee, 2005), and recently
neural network models such as convolutional neu-
ral network (CNN) (Kim, 2014; Kalchbrenner
et al., 2014; Lei et al., 2015), recursive autoen-
coders (Socher et al., 2011, 2013), Long Short-
Term Memory (LSTM) (Mikolov, 2012; Chung
et al., 2014; Tai et al., 2015; Zhu et al., 2015), and
many more.

∗Corresponding Author: Minlie Huang

In spite of the great success of these neural mod-
els, there are some defects in previous studies.
First, tree-structured models such as recursive au-
toencoders and Tree-LSTM (Tai et al., 2015; Zhu
et al., 2015), depend on parsing tree structures
and expensive phrase-level annotation, whose per-
formance drops substantially when only trained
with sentence-level annotation. Second, linguis-
tic knowledge such as sentiment lexicon, negation
words or negators (e.g., not, never), and intensity
words or intensifiers (e.g., very, absolutely), has
not been fully employed in neural models.

The goal of this research is to developing sim-
ple sequence models but also attempts to fully em-
ploying linguistic resources to benefit sentiment
classification. Firstly, we attempts to develop sim-
ple models that do not depend on parsing trees and
do not require phrase-level annotation which is too
expensive in real-world applications. Secondly,
in order to obtain competitive performance, sim-
ple models can benefit from linguistic resources.
Three types of resources will be addressed in this
paper: sentiment lexicon, negation words, and in-
tensity words. Sentiment lexicon offers the prior
polarity of a word which can be useful in deter-
mining the sentiment polarity of longer texts such
as phrases and sentences. Negators are typical sen-
timent shifters (Zhu et al., 2014), which constantly
change the polarity of sentiment expression. In-
tensifiers change the valence degree of the modi-
fied text, which is important for fine-grained sen-
timent classification.

In order to model the linguistic role of senti-
ment, negation, and intensity words, our central
idea is to regularize the difference between the
predicted sentiment distribution of the current po-
sition 1, and that of the previous or next positions,
in a sequence model. For instance, if the cur-

1Note that in sequence models, the hidden state of the cur-
rent position also encodes forward or backward contexts.

1679

https://doi.org/10.18653/v1/P17-1154
https://doi.org/10.18653/v1/P17-1154


rent position is a negator not, the negator should
change the sentiment distribution of the next posi-
tion accordingly. To summarize, our contributions
lie in two folds:

• We discover that modeling the linguistic role
of sentiment, negation, and intensity words
can enhance sentence-level sentiment classi-
fication. We address the issue by imposing
linguistic-inspired regularizers on sequence
LSTM models.

• Unlike previous models that depend on pars-
ing structures and expensive phrase-level an-
notation, our models are simple and efficient,
but the performance is on a par with the state-
of-the-art.

The rest of the paper is organized as follows:
In the following section, we survey related work.
In Section 3, we briefly introduce the background
of LSTM and bidirectional LSTM, and then de-
scribe in detail the lingistic regularizers for senti-
ment/negation/intensity words in Section 4. Ex-
periments are presented in Section 5, and Conclu-
sion follows in Section 6.

2 Related Work

2.1 Neural Networks for Sentiment
Classification

There are many neural networks proposed for sen-
timent classification. The most noticeable models
may be the recursive autoencoder neural network
which builds the representation of a sentence from
subphrases recursively (Socher et al., 2011, 2013;
Dong et al., 2014; Qian et al., 2015). Such recur-
sive models usually depend on a tree structure of
input text, and in order to obtain competitive re-
sults, usually require annotation of all subphrases.
Sequence models, for instance, convolutional neu-
ral network (CNN), do not require tree-structured
data, which are widely adopted for sentiment clas-
sification (Kim, 2014; Kalchbrenner et al., 2014;
Lei et al., 2015). Long short-term memory models
are also common for learning sentence-level rep-
resentation due to its capability of modeling the
prefix or suffix context (Hochreiter and Schmid-
huber, 1997). LSTM can be commonly applied to
sequential data but also tree-structured data (Zhu
et al., 2015; Tai et al., 2015).

2.2 Applying Linguistic Knowledge for
Sentiment Classification

Linguistic knowledge and sentiment resources,
such as sentiment lexicons, negation words (not,
never, neither, etc.) or negators, and intensity
words (very, extremely, etc.) or intensifiers, are
useful for sentiment analysis in general.

Sentiment lexicon (Hu and Liu, 2004; Wilson
et al., 2005) usually defines prior polarity of a lex-
ical entry, and is valuable for lexicon-based mod-
els (Turney, 2002; Taboada et al., 2011), and ma-
chine learning approaches (Pang and Lee, 2008).
There are recent works for automatic construction
of sentiment lexicons from social data (Vo and
Zhang, 2016) and for multiple languages (Chen
and Skiena, 2014). A noticeable work that ultilizes
sentiment lexicons can be seen in (Teng et al.,
2016) which treats the sentiment score of a sen-
tence as a weighted sum of prior sentiment scores
of negation words and sentiment words, where the
weights are learned by a neural network.

Negation words play a critical role in modify-
ing sentiment of textual expressions. Some early
negation models adopt the reversing assumption
that a negator reverses the sign of the sentiment
value of the modified text (Polanyi and Zaenen,
2006; Kennedy and Inkpen, 2006). The shifting
hyothesis assumes that negators change the senti-
ment values by a constant amount (Taboada et al.,
2011; Liu and Seneff, 2009). Since each negator
can affect the modified text in different ways, the
constant amount can be extended to be negator-
specific (Zhu et al., 2014), and further, the ef-
fect of negators could also depend on the syntax
and semantics of the modified text (Zhu et al.,
2014). Other approaches to negation modeling can
be seen in (Jia et al., 2009; Wiegand et al., 2010;
Benamara et al., 2012; Lapponi et al., 2012).

Sentiment intensity of a phrase indicates the
strength of associated sentiment, which is quite
important for fine-grained sentiment classification
or rating. Intensity words can change the valence
degree (i.e., sentiment intensity) of the modified
text. In (Wei et al., 2011) the authors propose a lin-
ear regression model to predict the valence value
for content words. In (Malandrakis et al., 2013),
a kernel-based model is proposed to combine se-
mantic information for predicting sentiment score.
In the SemEval-2016 task 7 subtask A, a learning-
to-rank model with a pair-wise strategy is pro-
posed to predict sentiment intensity scores (Wang

1680



et al., 2016). Linguistic intensity is not limited to
sentiment or intensity words, and there are works
that assign low/medium/high intensity scales to
adjectives such as okay, good, great (Sharma et al.,
2015) or to gradable terms (e.g. large, huge, gi-
gantic) (Shivade et al., 2015).

In (Dong et al., 2015), a sentiment parser is
proposed, and the authors studied how sentiment
changes when a phrase is modified by negators or
intensifiers.

Applying linguistic regularization to text clas-
sification can be seen in (Yogatama and Smith,
2014) which introduces three linguistically moti-
vated structured regularizers based on parse trees,
topics, and hierarchical word clusters for text cat-
egorization. Our work differs in that (Yogatama
and Smith, 2014) applies group lasso regularizers
to logistic regression on model parameters while
our regularizers are applied on intermediate out-
puts with KL divergence.

3 Long Short-term Memory Network

3.1 Long Short-Term Memory (LSTM)
Long Short-Term Memory has been widely
adopted for text processing. Briefly speaking, in
LSTM, the hidden states ht and memory cell ct
is a function of their previous ct−1 and ht−1 and
input vector xt, or formally as follows:

ct, ht = g
(LSTM)(ct−1, ht−1, xt) (1)

The hidden state ht ∈ Rd denotes the represen-
tation of position t while also encoding the pre-
ceding contexts of the position. For more details
about LSTM, we refer readers to (Hochreiter and
Schmidhuber, 1997).

3.2 Bidirectional LSTM
In LSTM, the hidden state of each position (ht)
only encodes the prefix context in a forward di-
rection while the backward context is not consid-
ered. Bidirectional LSTM (Graves et al., 2013)
exploited two parallel passes (forward and back-
ward) and concatenated hidden states of the two
LSTMs as the representation of each position. The
forward and backward LSTMs are respectively
formulated as follows:

−→c t,
−→
h t = g

(LSTM)(−→c t−1,
−→
h t−1, xt) (2)

←−c t,
←−
h t = g

(LSTM)(←−c t+1,
←−
h t+1, xt) (3)

where g(LSTM) is the same as that in Eq (1).
Particularly, parameters in the two LSTMs are
shared. The representation of the entire sentence
is [
−→
h n,
←−
h 1], where n is the length of the sen-

tence. At each position t, the new representa-
tion is ht = [

−→
h t,
←−
h t], which is the concatenation

of hidden states of the forward LSTM and back-
ward LSTM. In this way, the forward and back-
ward contexts can be considered simultaneously.

4 Linguistically Regularized LSTM

Figure 1: The overview of Linguistically Regular-
ized LSTM. Note that we apply a backward LSTM
(from right to left) to encode sentence since most
negators and intensifiers are modifying their fol-
lowing words.

The central idea of the paper is to model the
linguistic role of sentiment, negation, and inten-
sity words in sentence-level sentiment classifica-
tion by regularizing the outputs at adjacent posi-
tions of a sentence. For example in Fig 1, in sen-
tence “It’s not an interesting movie”, the predicted
sentiment distributions at “*an interesting movie2”
and “*interesting movie” should be close to each
other, while the predicted sentiment distribution
at “*interesting movie” should be quite different
from the preceding positions (in the backward di-
rection) (“*movie”) since a sentiment word (“in-
teresting”) is seen.

We propose a generic regularizer and three spe-
cial regularizers based on the following linguistic
observations:

• Non-Sentiment Regularizer: if the two ad-
jacent positions are all non-opinion words,
the sentiment distributions of the two posi-
tions should be close to each other. Though

2The asterisk denotes the current position.

1681



this is not always true (e.g., soap movie), this
assumption holds at most cases.

• Sentiment Regularizer: if the word is a sen-
timent word found in a lexicon, the sentiment
distribution of the current position should be
significantly different from that of the next
or previous positions. We approach this phe-
nomenon with a sentiment class specific shift-
ing distribution.

• Negation Regularizer: Negation words such
as “not” and “never” are critical sentiment
shifter or converter: in general they shift sen-
timent polarity from the positive end to the
negative end, but sometimes depend on the
negation word and the words they modify.
The negation regularizer models this linguis-
tic phenomena with a negator-specific trans-
formation matrix.

• Intensity Regularizer: Intensity words such
as “very” and “extremely” change the va-
lence degree of a sentiment expression: for
instance, from positive to very positive. Mod-
eling this effect is quite important for fine-
grained sentiment classification, and the in-
tensity regularizer is designed to formulate
this effect by a word-specific transformation
matrix.

More formally, the predicted sentiment distri-
bution (pt, based on ht, see Eq. 5) at position t
should be linguistically regularized with respect to
that of the preceding (t − 1) or following (t + 1)
positions. In order to enforce the model to produce
coherent predictions, we plug a new loss term into
the original cross entropy loss:

L(θ) = −
∑

i

ŷi log yi + α
∑

i

∑

t

Lt,i + β||θ||2

(4)
where ŷi is the gold distribution for sentence i,
yi is the predicted distribution, Lt,i is one of the
above regularizers or combination of these regu-
larizers on sentence i, α is the weight for the reg-
ularization term, and t is the word position in a
sentence.

Note that we do not consider the modification
span of negation and intensity words to preserve
the simplicity of the proposed models. Nega-
tion scope resolution is another complex problem
which has been extensively studied (Zou et al.,
2013; Packard et al., 2014; Fancellu et al., 2016),

which is beyond the scope of this work. Instead,
we resort to sequence LSTMs for encoding sur-
rounding contexts at a given position.

4.1 Non-Sentiment Regularizer (NSR)
This regularizer constrains that the sentiment dis-
tributions of adjacent positions should not vary
much if the additional input word xt is not a senti-
ment word, formally as follows:

L
(NSR)
t = max(0, DKL(pt||pt−1)−M) (5)

where M is a hyperparameter for margin, pt is the
predicted distribution at state of position t, (i.e.,
ht), and DKL(p||q) is a symmetric KL divergence
defined as follows:

DKL(p||q) =
1

2

C∑

l=1

p(l) log q(l) + q(l) log p(l)

(6)
where p, q are distributions over sentiment labels l
and C is the number of labels.

4.2 Sentiment Regularizer (SR)
The sentiment regularizer constrains that the sen-
timent distributions of adjacent positions should
drift accordingly if the input word is a sentiment
word. Let’s revisit the example “It’s not an inter-
esting movie” again. At position t = 2 (in the
backward direction) we see a positive word “in-
teresting” so the predicted distribution would be
more positive than that at position t = 1 (movie).
This is the issue of sentiment drift.

In order to address the sentiment drift issue, we
propose a polarity shifting distribution sc ∈ RC
for each sentiment class defined in a lexicon. For
instance, a sentiment lexicon may have class labels
like strong positive, weakly positive, weakly nega-
tive, and strong negative, and for each class, there
is a shifting distribution which will be learned by
the model. The sentiment regularizer states that
if the current word is a sentiment word, the senti-
ment distribution drift should be observed in com-
parison to the previous position, in more details:

p
(SR)
t−1 = pt−1 + sc(xt) (7)

L
(SR)
t = max(0, DKL(pt||p

(SR)
t−1 )−M) (8)

where p(SR)t−1 is the drifted sentiment distribution
after considering the shifting sentiment distribu-
tion corresponding to the state at position t, c(xt)

1682



is the prior sentiment class of word xt, and sc ∈ θ
is a parameter to be optimized but could also be set
fixed with prior knowledge. Note that in this way
all words of the same sentiment class share the
same drifting distribution, but in a refined setting,
we can learn a shifting distribution for each senti-
ment word if large-scale datasets are available.

4.3 Negation Regularizer (NR)
The negation regularizer approaches how negation
words shift the sentiment distribution of the modi-
fied text. When the input xt is a negation word, the
sentiment distribution should be shifted/reversed
accordingly. However, the negation role is more
complex than that by sentiment words, for exam-
ple, the word “not” in “not good” and “not bad”
have different roles in polarity change. The former
changes the polarity to negative, while the latter
changes to neutral instead of positive.

To respect such complex negation effects, we
propose a transformation matrix Tm ∈ RC×C for
each negation word m, and the matrix will be
learned by the model. The regularizer assumes
that if the current position is a negation word,
the sentiment distribution of the current position
should be close to that of the next or previous po-
sition with the transformation.

p
(NR)
t−1 = softmax(Txj × pt−1) (9)

p
(NR)
t+1 = softmax(Txj × pt+1) (10)

L
(NR)
t = min

{
max(0, DKL(pt||p(NR)t−1 )−M)
max(0, DKL(pt||p(NR)t+1 )−M)

(11)
where p(NR)t−1 and p

(NR)
t+1 is the sentiment distuibu-

tion after transformation, Txj ∈ θ is the transfor-
mation matrix for a negation word xj , a parameter
to be learned during training. In total, we train
m transformation matrixs for m negation words.
Such negator-specific transformation is in accor-
dance with the finding that each negator has its in-
dividual negation effect (Zhu et al., 2014).

4.4 Intensity Regularizer (IR)
Sentiment intensity of a phrase indicates the
strength of associated sentiment, which is quite
important for fine-grained sentiment classification
or rating. Intensifier can change the valence de-
gree of the content word. The intensity regularizer

models how intensity words influence the senti-
ment valence of a phrase or a sentence.

The formulation of the intensity effect is quite
the same as that in the negation regularizer, but
with different parameters of course. For each in-
tensity word, there is a transform matrix to favor
the different roles of various intensifiers on sen-
timent drift. For brevity, we will not repeat the
formulas here.

4.5 Applying Linguistic Regularizers to
Bidirectional LSTM

To preserve the simplicity of our proposals, we
do not consider the modification span of negation
and intensity words, which is a quite challenging
problem in the NLP community (Zou et al., 2013;
Packard et al., 2014; Fancellu et al., 2016). How-
ever, we can alleviate the problem by leveraging
bidirectional LSTM.

For a single LSTM, we employ a backward
LSTM from the end to the beginning of a sentence.
This is because, at most times, the modified words
of negation and intensity words are usually at the
right side of the modified text. But sometimes, the
modified words are at the left side of negation and
intensity words. To better address this issue, we
employ bidirectional LSTM and let the model de-
termine which side should be chosen.

More formally, in Bi-LSTM, we compute a
transformed sentiment distribution on−→p t−1 of the
forward LSTM and also that on←−p t+1 of the back-
ward LSTM, and compute the minimum distance
of the distribution of the current position to the two
distributions. This could be formulated as follows:

−→p (R)t−1 = softmax(Txj ×−→p t−1) (12)

←−p (R)t+1 = softmax(Txj ×←−p t+1) (13)

L
(R)
t = min

{
max(0, DKL(

−→p t||−→p (R)t−1)−M)
max(0, DKL(

←−p t||←−p (R)t+1)−M)
(14)

where −→p (R)t−1 and←−p
(R)
t+1 are the sentiment distribu-

tions transformed from the previous distribution
−→p t−1 and next distribution ←−p t+1 respectively.
Note that R ∈ {NR, IR} indicating the formu-
lation works for both negation and intensity regu-
larizers.

1683



Due to the same consideration, we redefine
L

(NSR)
t and L

(SR)
t with bidirectional LSTM simi-

larly. The formulation is the same and omitted for
brevity.

4.6 Discussion

Our models address these linguistic factors
with mathematical operations, parameterized with
shifting distribution vectors or transformation ma-
trices. In the sentiment regularizer, the senti-
ment shifting effect is parameterized with a class-
specific distribution (but could also be word-
specific if with more data). In the negation and
intensity regularizers, the effect is parameterized
with word-specific transformation matrices. This
is to respect the fact that the mechanism of how
negation and intensity words shift sentiment ex-
pression is quite complex and highly dependent on
individual words. Negation/Intensity effect also
depends on the syntax and semantics of the mod-
ified text, however, for simplicity we resort to se-
quence LSTM for encoding surrounding contexts
in this paper. We partially address the modification
scope issue by applying the minimization operator
in Eq. 11 and Eq. 14, and the bidirectional LSTM.

5 Experiment

5.1 Dataset and Sentiment Lexicon

Two datasets are used for evaluating the proposed
models: Movie Review (MR) (Pang and Lee,
2005) where each sentence is annotated with two
classes as negative, positive and Stanford Senti-
ment Treebank (SST) (Socher et al., 2013) with
five classes { very negative, negative, neutral, pos-
itive, very positive}. Note that SST has provided
phrase-level annotation on all inner nodes, but we
only use the sentence-level annotation since one of
our goals is to avoid expensive phrase-level anno-
tation.

The sentiment lexicon contains two parts. The
first part comes from MPQA (Wilson et al., 2005),
which contains 5, 153 sentiment words, each with
polarity rating. The second part consists of the
leaf nodes of the SST dataset (i.e., all sentiment
words) and there are 6, 886 polar words except
neural ones. We combine the two parts and ignore
those words that have conflicting sentiment labels,
and produce a lexicon of 9, 750 words with 4 senti-
ment labels. For negation and intensity words, we
collect them manually since the number is small,
some of which can be seen in Table 2.

Dataset MR SST
# sentences in total 10,662 11,885

#sen containing sentiment word 10,446 11,211
#sen containing negation word 1,644 1,832
#sen containing intensity word 2,687 2,472

Table 1: The data statistics.

5.2 The Details of Experiment Setting
In order to let others reproduce our results, we
present all the details of our models. We adopt
Glove vectors (Pennington et al., 2014) as the ini-
tial setting of word embeddings V . The shifting
vector for each sentiment class (sc), and the trans-
formation matrices for negation and intensity (Tm)
are initialized with a prior value. The other pa-
rameters for hidden layers (W (∗), U (∗), S) are ini-
tialized with Uniform(0, 1/sqrt(d)), where d is
the dimension of hidden representation, and we
set d=300. We adopt adaGrad to train the models,
and the learning rate is 0.1. It’s worth noting that,
we adopt stochastic gradient descent to update the
word embeddings (V ), with a learning rate of 0.2
but without momentum.

The optimal setting for α and β is 0.5 and
0.0001 respectively. During training, we adopt the
dropout operation before the softmax layer, with a
probability of 0.5. Mini-batch is taken to train the
models, each batch containing 25 samples. After
training with 3,000 mini-batch (about 9 epochs on
MR and 10 epochs on SST), we choose the results
of the model that performs best on the validation
dataset as the final performance.

Negation word
no, nothing, never, neither,
not, seldom, scarcely, etc.

Intensity word
terribly, greatly, absolutely,
too, very, completely, etc.

Table 2: Examples of negation and intensity
words.

5.3 Overall Comparison
We include several baselines, as listed below:

RNN/RNTN: Recursive Neural Network over
parsing trees, proposed by (Socher et al., 2011)
and Recursive Tensor Neural Network (Socher
et al., 2013) employs tensors to model correlations
between different dimensions of child nodes’ vec-
tors.

LSTM/Bi-LSTM: Long Short-Term Memory

1684



(Cho et al., 2014) and the bidirectional variant as
introduced previously.

Tree-LSTM: Tree-Structured Long Short-Term
Memory (Tai et al., 2015) introduces memory cells
and gates into tree-structured neural network.

CNN: Convolutional Neural Network (Kalch-
brenner et al., 2014) generates sentence represen-
tation by convolution and pooling operations.

CNN-Tensor: In (Lei et al., 2015), the convo-
lution operation is replaced by tensor product and
a dynamic programming is applied to enumerate
all skippable trigrams in a sentence. Very strong
results are reported.

DAN: Deep Average Network (DAN) (Iyyer
et al., 2015) averages all word vectors in a sen-
tence and connects an MLP layer to the output
layer.

Neural Context-Sensitive Lexicon: NCSL
(Teng et al., 2016) treats the sentiment score of
a sentence as a weighted sum of prior scores
of words in the sentence where the weights are
learned by a neural network.

Method MR
SST

Phrase-level
SST

Sent.-level
RNN 77.7* 44.8# 43.2*
RNTN 75.9# 45.7* 43.4#
LSTM 77.4# 46.4* 45.6#
Bi-LSTM 79.3# 49.1* 46.5#
Tree-LSTM 80.7# 51.0* 48.1#
CNN 81.5* 48.0* 46.9#
CNN-Tensor - 51.2* 50.6*
DAN - - 47.7*
NCSL 82.9 51.1* 47.1#
LR-Bi-LSTM 82.1 50.6 48.6
LR-LSTM 81.5 50.2 48.2

Table 3: The accuracy on MR and SST. Phrase-
level means the models use phrase-level annota-
tion for training. And Sent.-level means the mod-
els only use sentence-level annotation. Results
marked with * are re-printed from the references,
while those with # are obtained either by our own
implementation or with the same codes shared by
the original authors.

Firstly, we evaluate our model on the MR
dataset and the results are shown in Table 3. We
have the following observations:

First, both LR-LSTM and LR-Bi-LSTM out-
performs their counterparts (81.5% vs. 77.4% and
82.1% vs. 79.3%, resp.), demonstrating the ef-

fectiveness of the linguistic regularizers. Second,
LR-LSTM and LR-Bi-LSTM perform slightly bet-
ter than Tree-LSTM but Tree-LSTM leverages a
constituency tree structure while our model is a
simple sequence model. As future work, we will
apply such regularizers to tree-structured models.

Last, on the MR dataset, our model is compa-
rable to or slightly better than CNN.

For fine-grained sentiment classification, we
evaluate our model on the SST dataset which has
five sentiment classes { very negative, negative,
neutral, positive, very positive} so that we can
evaluate the sentiment shifting effect of intensity
words. The results are shown in Table 3. We have
the following observations:

First, linguistically regularized LSTM and Bi-
LSTM are better than their counterparts. It’s
worth noting that LR-Bi-LSTM (trained with just
sentence-level annotation) is even comparable to
Bi-LSTM trained with phrase-level annotation.
That means, LR-Bi-LSTM can avoid the heavy
phrase-level annotation but still obtain compara-
ble results.

Second, our models are comparable to Tree-
LSTM but our models are not dependent on a
parsing tree and more simple, and hence more
efficient. Further, for Tree-LSTM, the model is
heavily dependent on phrase-level annotation, oth-
erwise the performance drops substantially (from
51% to 48.1%).

Last, on the SST dataset, our model is better
than CNN, DAN, and NCSL. We conjecture that
the strong performance of CNN-Tensor may be
due to the tensor product operation, the enumer-
ation of all skippable trigrams, and the concate-
nated representations of all pooling layers for final
classification.

5.4 The Effect of Different Regularizers

In order to reveal the effect of each individual reg-
ularizer, we conduct ablation experiments. Each
time, we remove a regularizer and observe how
the performance varies. First of all, we conduct
this experiment on the entire datasets, and then we
experiment on sub-datasets that only contain nega-
tion words or intensity words.

The experiment results are shown in Table 4
where we can see that the non-sentiment regular-
izer (NSR) and sentiment regularizer (SR) play
a key role3, and the negation regularizer and in-

3Kindly note that almost all sentences contain sentiment

1685



Method MR SST
LR-Bi-LSTM 82.1 48.6
LR-Bi-LSTM (-NSR) 80.8 46.9
LR-Bi-LSTM (-SR) 80.6 46.9
LR-Bi-LSTM (-NR) 81.2 47.6
LR-Bi-LSTM (-IR) 81.7 47.9
LR-LSTM 81.5 48.2
LR-LSTM (-NSR) 80.2 46.4
LR-LSTM (-SR) 80.2 46.6
LR-LSTM (-NR) 80.8 47.4
LR-LSTM (-IR) 81.2 47.4

Table 4: The accuracy for LR-Bi-LSTM and LR-
LSTM with regularizer ablation. NSR, SR, NR and
IR denotes Non-sentiment Regularizer, Sentiment
Regularizer, Negation Regularizer, and Intensity
Regularizer respectively.

tensity regularizer are effective but less important
than NSR and SR. This may be due to the fact that
only 14% of sentences contains negation words
in the test datasets, and 23% contains intensity
words, and thus we further evaluate the models on
two subsets, as shown in Table 5.

The experiments on the subsets show that: 1)
With linguistic regularizers, LR-Bi-LSTM outper-
forms Bi-LSTM remarkably on these subsets; 2)
When the negation regularizer is removed from
the model, the performance drops significantly on
both MR and SST subsets; 3) Similar observations
can be found regarding the intensity regularizer.

Method
Neg. Sub. Int. Sub.
MR SST MR SST

BiLSTM 72.0 39.8 83.2 48.8
LR-Bi-LSTM (-NR) 74.2 41.6 - -
LR-Bi-LSTM (-IR) - - 85.2 50.0
LR-Bi-LSTM 78.5 44.4 87.1 53.2

Table 5: The accuracy on the negation sub-dataset
(Neg. Sub.) that only contains negators, and in-
tensity sub-dataset (Int. Sub.) that only contains
intensifiers.

5.5 The Effect of the Negation Regularizer
To further reveal the linguistic role of negation
words, we compare the predicted sentiment distri-
butions of a phrase pair with and without a nega-
tion word. The experimental results performed on
MR are shown in Fig. 2. Each dot denotes a phrase

words, see Tab. 1.

pair (for example, <interesting, not interesting>),
where the x-axis denotes the positive score4 of a
phrase without negators (e.g., interesting), and the
y-axis indicates the positive score for the phrase
with negators (e.g., not interesting). The curves
in the figures show this function: [1 − y, y] =
softmax(Tnw ∗ [1 − x, x]) where [1 − x, x] is a
sentiment distribution on [negative, positive], x
is the positive score of the phrase without negators
(x-axis) and y that of the phrase with negators (y-
axis), and Tnw is the transformation matrix for the
negation word nw (see Eq. 9). By looking into the

Figure 2: The sentiment shifts with negators. Each
dot < x, y > indicates that x is the sentiment score
of a phrase without negator and y is that of the
phrase with a negator.

detailed results of our model, we have the follow-
ing statements:

First, there is no dot at the up-right and bottom-
left blocks, indicating that negators generally
shift/convert very positive or very negative phrases
to other polarities. Typical phrases include not
very good, not too bad.

Second, the dots at the up-left and bottom-right
respectively indicates the negation effects: chang-
ing negative to positive and positive to negative.
Typical phrases include never seems hopelessly
(up-left), no good scenes (bottom-right), not in-
teresting (bottom-right), etc. There are also some
positive/negative phrases shifting to neutral senti-
ment such as not so good, and not too bad.

Last, the dots located at the center indicate that
neutral phrases maintain neutral sentiment with
negators. Typical phrases include not at home,
not here, where negators typically modify non-
sentiment words.

5.6 The Effect of the Intensity Regularizer

To further reveal the linguistic role of inten-
sity words, we perform experiments on the SST
dataset, as illustrated in Figure 3. We show the

4 The score is obtained from the predicted distribution,
where 1 means positive and 0 means negative.

1686



matrix that indicates how the sentiment shifts af-
ter being modified by intensifiers. Each number
in a cell (mij) indicates how many phrases are
predicted with a sentiment label i but the predic-
tion of the phrases with intensifiers changes to la-
bel j. For instance, the number 20 (m21) in the
second matrix , means that there are 20 phrases
predicted with a class of negative (-) but the pre-
diction changes to very negative (- -) after being
modified by intensifier “very”. Results in the first

Figure 3: The sentiment shifting with intensi-
fiers. The number in cell(mij) indicates how many
phrases are predicted with sentiment label i but the
prediction of phrases with intensifiers changes to
label j.

matrix show that, for intensifier “most”, there are
21/21/13/12 phrases whose sentiment is shifted af-
ter being modified by intensifiers, from negative
to very negative (eg. most irresponsible picture),
positive to very positive (eg. most famous author),
neutral to negative (eg. most plain), and neutral to
positive (eg. most closely), respectively.

There are also many phrases retaining the senti-
ment after being modified with intensifiers. Not
surprisingly, for very positive/negative phrases,
phrases modified by intensifiers still maintain the
strong sentiment. For the left phrases, they fall
into three categories: first, words modified by in-
tensifiers are non-sentiment words, such as most
of us, most part; second, intensifiers are not strong
enough to shift sentiment, such as most complex
(from neg. to neg.), most traditional (from pos. to
pos.); third, our models fail to shift sentiment with
intensifiers such as most vital, most resonant film.

6 Conclusion and Future Work

We present linguistically regularized LSTMs for
sentence-level sentiment classification. The pro-
posed models address the sentient shifting effect
of sentiment, negation, and intensity words. Fur-
thermore, our models are sequence LSTMs which
do not depend on a parsing tree-structure and do

not require expensive phrase-level annotation. Re-
sults show that our models are able to address the
linguistic role of sentiment, negation, and intensity
words.

To preserve the simplicity of the proposed mod-
els, we do not consider the modification scope of
negation and intensity words, though we partially
address this issue by applying a minimization op-
erartor (see Eq. 11, Eq. 14) and bi-directional
LSTM. As future work, we plan to apply the lin-
guistic regularizers to tree-LSTM to address the
scope issue since the parsing tree is easier to indi-
cate the modification scope explicitly.

Acknowledgments

This work was partly supported by the Na-
tional Basic Research Program (973 Program)
under grant No. 2013CB329403, and the Na-
tional Science Foundation of China under grant
No.61272227/61332007.

References
Farah Benamara, Baptiste Chardon, Yannick Math-

ieu, Vladimir Popescu, and Nicholas Asher. 2012.
How do negation and modality impact on opin-
ions? In Proceedings of the Workshop on Extra-
Propositional Aspects of Meaning in Computational
Linguistics. pages 10–18.

Yanqing Chen and Steven Skiena. 2014. Building sen-
timent lexicons for all major languages. In ACL.
pages 383–389.

Kyunghyun Cho, Bart Van Merriënboer, Caglar Gul-
cehre, Dzmitry Bahdanau, Fethi Bougares, Holger
Schwenk, and Yoshua Bengio. 2014. Learning
phrase representations using rnn encoder-decoder
for statistical machine translation. arXiv preprint
arXiv:1406.1078 .

Junyoung Chung, Caglar Gulcehre, KyungHyun Cho,
and Yoshua Bengio. 2014. Empirical evaluation of
gated recurrent neural networks on sequence model-
ing. arXiv preprint arXiv:1412.3555 .

Li Dong, Furu Wei, Shujie Liu, Ming Zhou, and Ke Xu.
2015. A statistical parsing framework for sentiment
classification. Computational Linguistics .

Li Dong, Furu Wei, Ming Zhou, and Ke Xu. 2014.
Adaptive multi-compositionality for recursive neu-
ral models with applications to sentiment analysis.
In AAAI. AAAI.

Federico Fancellu, Adam Lopez, and Bonnie Webber.
2016. Neural networks for negation scope detection.
In Proceedings of the 54th Annual Meeting of the As-
sociation for Computational Linguistics. pages 495–
504.

1687



Alex Graves, Navdeep Jaitly, and Abdel-rahman Mo-
hamed. 2013. Hybrid speech recognition with deep
bidirectional lstm. In Automatic Speech Recognition
and Understanding (ASRU), 2013 IEEE Workshop
on. IEEE, pages 273–278.

Sepp Hochreiter and Jürgen Schmidhuber. 1997.
Long short-term memory. Neural Computation
9(8):1735–1780.

Minqing Hu and Bing Liu. 2004. Mining and summa-
rizing customer reviews. In Proceedings of the tenth
ACM SIGKDD international conference on Knowl-
edge discovery and data mining. ACM, pages 168–
177.

Mohit Iyyer, Varun Manjunatha, Jordan Boyd-Graber,
and Hal Daumé III. 2015. Deep unordered compo-
sition rivals syntactic methods for text classification.
In Proceedings of the Association for Computational
Linguistics.

Lifeng Jia, Clement Yu, and Weiyi Meng. 2009. The
effect of negation on sentiment analysis and retrieval
effectiveness. In Proceedings of the 18th ACM con-
ference on Information and knowledge management.
pages 1827–1830.

Nal Kalchbrenner, Edward Grefenstette, and Phil Blun-
som. 2014. A convolutional neural network for
modelling sentences. In ACL. pages 655–665.

Alistair Kennedy and Diana Inkpen. 2006. Senti-
ment classification of movie reviews using contex-
tual valence shifters. Computational intelligence
22(2):110–125.

Yoon Kim. 2014. Convolutional neural networks for
sentence classification. In EMNLP. pages 1746–
1751.

Emanuele Lapponi, Jonathon Read, and Lilja Øvrelid.
2012. Representing and resolving negation for sen-
timent analysis. In 2012 IEEE 12th International
Conference on Data Mining Workshops. pages 687–
692.

Tao Lei, Regina Barzilay, and Tommi Jaakkola. 2015.
Molding cnns for text: non-linear, non-consecutive
convolutions. ACL .

Jingjing Liu and Stephanie Seneff. 2009. Review senti-
ment scoring via a parse-and-paraphrase paradigm.
In Proceedings of the 2009 Conference on Empiri-
cal Methods in Natural Language Processing. pages
161–169.

Nikolaos Malandrakis, Alexandros Potamianos, Elias
Iosif, and Shrikanth Narayanan. 2013. Distribu-
tional semantic models for affective text analysis.
IEEE Transactions on Audio, Speech, and Language
Processing 21(11):2379–2392.

Tomáš Mikolov. 2012. Statistical language models
based on neural networks. Presentation at Google,
Mountain View, 2nd April .

Woodley Packard, M. Emily Bender, Jonathon Read,
Stephan Oepen, and Rebecca Dridan. 2014. Simple
negation scope resolution through deep parsing: A
semantic solution to a semantic problem. In Pro-
ceedings of the 52nd Annual Meeting of the Associ-
ation for Computational Linguistics. pages 69–78.

Bo Pang and Lillian Lee. 2005. Seeing stars: Exploit-
ing class relationships for sentiment categorization
with respect to rating scales. In ACL. pages 115–
124.

Bo Pang and Lillian Lee. 2008. Opinion mining and
sentiment analysis. Foundations and trends in infor-
mation retrieval 2(1-2):1–135.

Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan.
2002. Thumbs up?: sentiment classification using
machine learning techniques. In ACL. pages 79–86.

Jeffrey Pennington, Richard Socher, and Christopher D
Manning. 2014. Glove: Global vectors for word
representation. EMNLP 12:1532–1543.

Livia Polanyi and Annie Zaenen. 2006. Contextual va-
lence shifters. In Computing attitude and affect in
text: Theory and applications, Springer, pages 1–10.

Qiao Qian, Bo Tian, Minlie Huang, Yang Liu, Xuan
Zhu, and Xiaoyan Zhu. 2015. Learning tag embed-
dings and tag-specific composition functions in re-
cursive neural network. In ACL. volume 1, pages
1365–1374.

Raksha Sharma, Mohit Gupta, Astha Agarwal, and
Pushpak Bhattacharyya. 2015. Adjective intensity
and sentiment analysis. EMNLP2015 .

Chaitanya Shivade, Marie-Catherine de Marneffe, Eric
Folser-Lussier, and Albert Lai. 2015. Corpus-based
discovery of semantic intensity scales. In Proceed-
ings of NAACL-HTL .

Richard Socher, Jeffrey Pennington, Eric H Huang,
Andrew Y Ng, and Christopher D Manning. 2011.
Semi-supervised recursive autoencoders for predict-
ing sentiment distributions. In EMNLP. pages 151–
161.

Richard Socher, Alex Perelygin, Jean Y Wu, Jason
Chuang, Christopher D Manning, Andrew Y Ng,
and Christopher Potts. 2013. Recursive deep mod-
els for semantic compositionality over a sentiment
treebank. In EMNLP. pages 1631–1642.

Maite Taboada, Julian Brooke, Milan Tofiloski, Kim-
berly Voll, and Manfred Stede. 2011. Lexicon-based
methods for sentiment analysis. Computational lin-
guistics 37(2):267–307.

Kai Sheng Tai, Richard Socher, and Christopher D
Manning. 2015. Improved semantic representations
from tree-structured long short-term memory net-
works. arXiv preprint arXiv:1503.00075 .

1688



Zhiyang Teng, Duy-Tin Vo, and Yue Zhang. 2016.
Context-sensitive lexicon features for neural senti-
ment analysis. In Proceedings of the 2016 Con-
ference on Empirical Methods in Natural Language
Processing. pages 1629–1638.

Peter D Turney. 2002. Thumbs up or thumbs down?:
semantic orientation applied to unsupervised classi-
fication of reviews. In ACL. pages 417–424.

Duy Tin Vo and Yue Zhang. 2016. Dont count, predict!
an automatic approach to learning sentiment lexi-
cons for short text. In Proceedings of the 54th An-
nual Meeting of the Association for Computational
Linguistics. volume 2, pages 219–224.

Feixiang Wang, Zhihua Zhang, and Man Lan. 2016.
Ecnu at semeval-2016 task 7: An enhanced super-
vised learning method for lexicon sentiment inten-
sity ranking. Proceedings of SemEval pages 491–
496.

Wen-Li Wei, Chung-Hsien Wu, and Jen-Chun Lin.
2011. A regression approach to affective rating of
chinese words from anew. In Affective Comput-
ing and Intelligent Interaction, Springer, pages 121–
131.

Michael Wiegand, Alexandra Balahur, Benjamin Roth,
Dietrich Klakow, and Andrés Montoyo. 2010. A
survey on the role of negation in sentiment analy-
sis. In Proceedings of the workshop on negation and
speculation in natural language processing. Associ-
ation for Computational Linguistics, pages 60–68.

Theresa Wilson, Janyce Wiebe, and Paul Hoffmann.
2005. Recognizing contextual polarity in phrase-
level sentiment analysis. In EMNLP. pages 347–
354.

Dani Yogatama and Noah A. Smith. 2014. Linguis-
tic structured sparsity in text categorization. In Pro-
ceedings of the 52nd Annual Meeting of the Associa-
tion for Computational Linguistics. pages 786–796.

Xiaodan Zhu, Hongyu Guo, Saif Mohammad, and
Svetlana Kiritchenko. 2014. An empirical study on
the effect of negation words on sentiment. In ACL.
pages 304–313.

Xiaodan Zhu, Parinaz Sobhani, and Hongyu Guo.
2015. Long short-term memory over recursive
structures. In ICML. pages 1604–1612.

Bowei Zou, Guodong Zhou, and Qiaoming Zhu. 2013.
Tree kernel-based negation and speculation scope
detection with structured syntactic parse features.
In Proceedings of the 2013 Conference on Empiri-
cal Methods in Natural Language Processing. pages
968–976.

1689


	Linguistically Regularized LSTM for Sentiment Classification

