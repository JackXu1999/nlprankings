



















































SINAI: Syntactic Approach for Aspect-Based Sentiment Analysis


Proceedings of the 9th International Workshop on Semantic Evaluation (SemEval 2015), pages 730–735,
Denver, Colorado, June 4-5, 2015. c©2015 Association for Computational Linguistics

SINAI: Syntactic approach for Aspect Based Sentiment Analysis

Salud M. Jiménez-Zafra, Eugenio Martı́nez-Cámara,
M. Teresa Martı́n-Valdivia, L. Alfonso Ureña-López

SINAI Research Group
University of Jaén

E-23071, Jaén (Spain)
{sjzafra, emcamara, maite, laurena}@ujaen.es

Abstract

This paper describes the participation of the
SINAI research group in the task Aspect
Based Sentiment Analysis of SemEval Work-
shop 2015 Edition. We propose a syntactic
approach for identifying the words that mod-
ify each aspect, with the aim of classifying the
sentiment expressed towards each attribute of
an entity.

1 Introduction

Opinion Mining (OM), also known as Sentiment
Analysis (SA), is the discipline that focuses on the
computational treatment of opinion, sentiment and
subjectivity in texts (Pang and Lee, 2008). Cur-
rently, OM is a trendy task in the field of Natural
Language Processing, due mainly to the fact of the
proliferation of user-generated content and the in-
terest in the knowledge of the opinion of people by
consumers and businesses.

Most of the systems developed up to now carry
out opinion analysis at document level ((Pang et al.,
2002), (Turney, 2002)) or at sentence level ((Wilson
et al., 2005), (Yu and Hatzivassiloglou, 2003)), that
is, they determine the overall sentiment expressed
by the reviewer about the topic, product, person. . . of
study. However, the fact that the overall sentiment of
a product is positive does not mean that the author
thinks that all the aspects of the product are posi-
tives, or the fact that is negative does not involve that
everything about the product is bad. For this reason,
users and companies are not satisfied with knowing
the overall sentiment of a product or service, they

seek a more detailed knowledge. Consequently, to
achieve a higher level of detail, part of the scientific
community related to this area is working on SA at
aspect level ((Quan and Ren, 2014), (Marcheggiani
et al., 2014), (Lu et al., 2011), (Thet et al., 2010))
and even, there is a competition on this topic that
began to conduct last year (Pontiki et al., 2014) in
the International Workshop on Semantic Evaluation
2014 (SemEval 2014).

This year, the 2015 edition of SemEval has
also proposed a task for SA at aspect level. The
SemEval-2015 Aspect Based Sentiment Analysis
task is a continuation of SemEval-2014 Task 4 (Pon-
tiki et al., 2014). The aim of this task is to identify
the attributes of an entity that are being reviewed
and the sentiment expressed for each one. It is di-
vided into three slots. The first one is focused on
the identification of every entity E and attribute A
pair (E#A) towards which an opinion is expressed
in the given text. Slot 2 proposes to determine the
expression used in the text to refer to the reviewed
entity, that is, the Opinion Target Expression (OTE).
Finally, Slot 3 has as goal to classify the sentiment
expressed over each category (E#A pair) as positive,
negative or neutral. We have participated in the slot
related to sentiment polarity (Slot 3).

Due to the fact that OM is a domain-dependent
task, the organization proposes the three slots in dif-
ferent domains, two known (restaurants and laptops)
and one unknown until the evaluation (hotels). A
wider description of the task and the dataset used
can be found in the task description paper (Pontiki
et al., 2015).

The rest of the paper is organized as follows. Sec-

730



tion 2 describes the system developed and the re-
sources that we have used. To sum up, the results
reached and an analysis of the same are shown in
Section 3.

2 System description - Slot 3

As we have mentioned above, we have taken part
in the Slot 3. The aim of this slot is to identify the
polarity of each category or each <category, OTE>
pair on which an opinion is expressed in a given re-
view. This task has been carried out on two known
domains an one unknown domain. For each of the
known domains, restaurants and laptops, the organi-
zation has provided a dataset for training, whereas
for the unknown domain any information has been
given until the test set has been released. Therefore,
we have used a supervised method for restaurants
and laptops domains and we have developed an un-
supervised method for the unknown domain.

2.1 Slot 3 - Restaurant domain ABSA

The training data related to restaurants domain con-
tains 254 reviews. Each review is composed of dif-
ferent sentences annotated with opinion tuples. Each
opinion tuple has information about the Opinion Tar-
get Expression (OTE), the Entity and Attribute pair
(E#A category) towards the opinion is expressed, the
polarity (positive, negative or neutral) and the posi-
tion of the OTE in the text (from - to).

Using this information we have developed differ-
ent experiments for polarity prediction. In all of
them an SVM classifier of type C-SVC with lin-
ear kernel and the default configuration has been
trained, and a 10-fold-cross validation model has
been used for the assessment (Table 1).

The features that have provided the best results
in the training and that we have used for our par-
ticipation in this slot are the following. For each
<category, OTE, polarity> tuple of the training
data, we have used as label the polarity value and as
features the words that modify the OTE, their PoS
tag, their syntactic relation and their polarity using
three lexicons (taking into account negation): Senti-
WordNet (Baccianella et al., 2010), MPQA (Wilson
et al., 2005) and eBLR (enriched version of Bing Liu
Lexicon (Hu and Liu, 2004) adapted to restaurant
domain). Below, we describe briefly how this infor-

Exp. Type Accuracy Features
Exp 1 U 75.57% Modifying words,

PoS, syntactic
relation, polarity
(SentiWordNet,
MPQA, BinLiu)

Exp 2 U 75.88% Modifying words,
PoS, syntactic

relation, polarity
(SentiWordNet,
MPQA, BinLiu)

taking into
account negation

Exp 3 U 75.67% Modifying words,
PoS, syntactic

relation, polarity
(SentiWordNet,
MPQA, eBLR)

Exp 4 U 75.94% Modifying words,
PoS, syntactic

relation, polarity
(SentiWordNet,
MPQA, eBLR)

taking into
account negation

Table 1: Experiments restaurants training data (U =
Unconstrained, C = Constrained).

mation has been obtained. Thereby, each <category,
OTE> tuple of the test data is classified using its fea-
tures vector and the trained SVM model.

2.1.1 Features
Words that modify the OTE
We call words that modify an OTE those words

that specifically have been used in the review to
discuss about the OTE. In order to determine what
these words are, we use the Stanford Dependencies
Parser1. This parser was designed to provide a sim-
ple description of the grammatical relationships that
can appear in a sentence and it can be easily under-
stood and effectively used by people without linguis-
tic expertise who want to extract textual relations
(De Marneffe and Manning, 2008). It represents
all sentence relationships uniformly as typed depen-

1http://nlp.stanford.edu/software/lex-parser.shtml

731



dency relations. In this experiment, we have consid-
ered the main relationships for expressing opinion
about a noun or nominal expression: using an ad-
jectival modifier (“amod”), an active or passive verb
(“nsub”, “nsubjpass”), a noun compound modifier
(“nn”) or a dependency relation with another word
(“dep”). In this way, for each OTE of a review,
we use these relationships to extract all the words
that modify the aspect of the entity that has been re-
viewed and we use them as features. If there is no
word related to the aspect using these relationships,
the previous word to the OTE and the following four
words will be used.

Pos Tag
In addition, for each of the words that modify an

aspect we get their particular Part of Speech Tag
(noun, verb, adjective. . . ).

Syntactic relations
As it has been mentioned above, the syntactic re-

lation of each modifying word with the OTE has also
been used as feature.

Polarity
The last feature of our SVM classifier is the polar-

ity of each modifying word according to three lexi-
cons: SentiWordNet, MPQA and eBLR. In addition,
it has been used the fixed window method for the
treatment of negation. Then, if any of the preceding
or following 3 words is a negative particle (“not”,
“n’t”, “no”, “never”. . . ), the modifying word polar-
ity will be reversed (positive —> negative, negative
—> positive, neutral —> neutral).

SentiWordNet is a lexical resource that assigns to
each synset of WordNet2 (Miller, 1995) three senti-
ment scores (positivity, negativity and objectivity)
that describe how positive, negative and objective
the terms contained in the synset are.

MPQA is a subjectivity lexicon formed by over
8000 subjectivity clues. For each word, it has infor-
mation about its prior polarity, its part of speech tag
and its grade of subjectivity (strong or weak).

Finally, eBLR is an enriched version of Bing Liu
Lexicon that we explain below. As is well-known
in the SA research community, the semantic orienta-
tion of a word is domain-dependent. Therefore, we
decided to generate a list of opinion words for the

2Wordnet is an English lexical database which groups words
according to their meaning.

restaurant domain, taking as baseline the Bing Liu
Lexicon and using the training data for restaurant
domain supplied by the organization. For this, we
have employed a corpus-based approach following
the methodology of (Molina-González et al., 2013)
that consists of the use of a sentiment labeled corpus
in order to select the most frequent positive and neg-
ative words. A word is added to the list of opinion
positive words if it only appears in positive reviews
and its frequency exceeds a certain threshold. The
same process is followed for negative words. In the
case of words that appear in both positive and neg-
ative reviews, a word is considered as opinion posi-
tive/negative word if the frequency of occurrence in
positive/negative reviews exceeds the frequency of
occurrence in negative/positive reviews in a certain
threshold.

2.2 Slot 3 - Laptops domain ABSA
The training data for laptops domain contains 277
reviews. Each review has different sentences anno-
tated at aspect level with the Entity and Attribute
pair (E#A category) towards each opinion is ex-
pressed and the polarity (positive, negative or neu-
tral). In this case no information about the OTE is
provided and thus, we have followed a different ap-
proach to that used in the restaurant domain. We
have also developed different experiments with an
SVM classifier of type C-SVC with linear kernel
and the default configuration, and we have also used
a 10-fold-cross validation model for the assessment
but with different features (Table 2).

Exp. Type Accuracy Features
Exp 1 C 75.08% Unigrams, PoS
Exp 2 U 73.76% Unigrams, total

positive words
(Bin Liu), total
negative words

(Bin Liu)
Exp 3 U 79.64% Unigrams, total

positive words
(eBLL), total

negative words
(eBLL)

Table 2: Experiments laptops training data (U =
Unconstrained, C = Constrained).

732



For this domain we have submitted two runs,
one constrained (using only the provided training
data) and another unconstrained (using additional
resources for training). These experiments are those
that have provided better results with the training
data and we have used them for our participation in
this domain.

• SINAI B Lap 1 (Exp 1 - constrained). For
each <category(E#A pair), polarity> tuple of
the training data we have used as label the po-
larity and as features the entity and the specific
attribute of this entity about someone is review-
ing, and all the words of the sentence with their
pertinent Part of Speech Tag.

• SINAI B Lap 2 (Exp 3 - unconstrained). In
this case, the features that we have selected
for each <category (E#A pair), polarity> tu-
ple of the training data are the entity and the
attribute about someone is reviewing, all the
words of the sentence and the number of pos-
itive and negative opinion words according to
eBLL. eBLL is an enriched version of Bing Liu
Lexicon for laptops domain. It has been built
using the training data supplied by the organi-
zation for laptops domain, in the same way that
eBLR Lexicon.

Thus, given a category of the test data, it is clas-
sified using its features vector and the trained SVM
model.

2.3 Slot 3 - Out of domain ABSA
For the last domain, the organization has not pro-
vided any information until the test set has been re-
leased. We only knew that we had to assign a polar-
ity value for each <OTE, category> tuple present
in the test data. In this case we have followed an
unsupervised approach that we present below.

In order to classify the sentiment expressed about
each OTE is important to determine the words that
have been used in the review to discuss about the
aspect. For this, we have employed the Stanford De-
pendencies Parser and the main relationships for ex-
pressing opinion about a noun or nominal expres-
sion: “amod”, “nsubj”, “nsubjpass”, “nn”, “dep”
(they are explained in Subsection 2.1). In this way,
for each OTE of a review, we use these relationships

to extract all the words that modify it and we use
them to determine the sentiment expressed about the
OTE. If there is no word related to the aspect us-
ing these relationships, the previous word to the as-
pect and the following four words will be used. We
calculate the polarity of each OTE through a voting
system based on three classifiers: Bing Liu Lexicon,
SentiWordNet and MPQA. To do this we determine,
with each of the classifiers individually, the polarity
of an OTE using the words that modify it. Thus, ac-
cording to Bing Liu Lexicon, we count the number
of positive (pw) and negative words (nw) that mod-
ify the OTE and tag it following the equation 1. On
the other hand, we use MPQA as classifier following
the same approach but in this case we take into ac-
count the PoS of the modifying words in order to get
their polarity. At last, we employ SentiWordNet also
following the approach of comparing the number of
positive and negative words but as this lexicon as-
signs three sentiment scores to each synset, we cal-
culate the polarity of each modifying word using the
Denecke method (Denecke, 2008), that is, we cal-
culate the average of the positivity, negativity and
objectivity scores of all the synsets of the word with
the same PoS and assign the word the polarity of the
highest average.

pol(OTE) =


positive if (pw > nw)
negative if (pw < nw)
neutral if (pw = nw)

(1)

Therefore, an OTE is positive/negative if there are
at least two classifiers that tag it as positive/negative
and neutral in another case. It may happen that an
OTE is affected by negation, so if any of the pre-
ceding or following 3 words is a negative particle
(“not”, “n’t”, “no”, “never”. . . ), the OTE polarity
will be reversed (positive —> negative, negative —
> positive, neutral —> neutral).

3 Analysis of results

This section shows the results reached in the evalua-
tion of the task using the system described in Section
2. Table 3 presents the official results of our submis-
sions. We also include the results of the best team
and the average of all participants for comparision.

A clear difference between the results obtained by
our team and the average may be seen in Table 3.

733



Furthermore, the results in restaurants and laptops
domain are worse than those achieved in the train-
ing phase (Table 1 and Table 2). Therefore, we have
calculated the confusion matrix related to each ex-
periment for a deeper analysis (Table 4, Table 5, Ta-
ble 6 and Table 7).

Accuracy
SINAI Avg. Best

team
Restaurants 0.6071 (U) 0.7119 0.7870

(U)

Laptops 0.6586 (C) 0.7093 0.7935
0.5184 (U) (U)

Hotels 0.6372 (U) 0.7079 0.8053
(U)

Table 3: Results test data (U = Unconstrained, C =
Constrained).

Restaurants
Pred.
pos.

Pred.
neu.

Pred.
neg.

Recall

Real pos. 446 0 8 0.9824
Real neu. 43 0 2 0
Real neg. 276 3 67 0.1936
Precision 0.583 0 0.8701

Table 4: Confusion matrix restaurants submission.

Laptops (C)
Pred.
pos.

Pred.
neu.

Pred.
neg.

Recall

Real pos. 491 0 50 0.9076
Real neu. 51 0 28 0
Real neg. 195 0 134 0.4073
Precision 0.6662 0 0.6321

Table 5: Confusion matrix laptops constraint
submission.

Laptops (U)
Pred.
pos.

Pred.
neu.

Pred.
neg.

Recall

Real pos. 391 0 150 0.7227
Real neu. 63 0 16 0
Real neg. 228 0 101 0.3070
Precision 0.5733 0 0.3783

Table 6: Confusion matrix laptops unconstraint
submission.

Hotels
Pred.
pos.

Pred.
neu.

Pred.
neg.

Recall

Real pos. 181 56 6 0.7449
Real neu. 5 6 1 0.5
Real neg. 15 40 29 0.3452
Precision 0.9005 0.0588 0.8056

Table 7: Confusion matrix hotels submission.

Restaurants Laptops
Positive opinions 1198 1103
Neutral opinions 53 106
Negative opinions 403 765

Table 8: Opinions in training data per class.

If we observe Table 4, Table 5 and Table 6, we can
see that, in restaurants and laptops domains, the sys-
tem has failed mainly in the classification of negative
and neutral opinions. It has classified most of them
as positive. We think that one of the reasons may
be that the training data for restaurants and laptops
domains is unbalanced (Table 8). For restaurants,
the number of positive opinions is almost three times
the number of negative opinions. Another possible
reason, in restaurants domain, is that we have only
taken into account the scope (words that modify the
OTE) and not the whole context (all words present
in the review). In future works, we will do experi-
ments balancing the datasets in order to test how the
system works. Furthermore, we will take into ac-
count the whole context in restaurants domain to see
if that improves the system.

Regarding the unsupervised system, that has been

734



tested with hotels domain, there are also differences
with respect to the mean accuracy of all teams (Table
3). This is a first approach that can be improved with
the consideration of other relationships to determine
which words modify the OTE and with a treatment
of negation more exhaustive. In future works we will
consider these possible improvements.

Acknowledgments

This work has been partially supported by a grant
from the Fondo Europeo de Desarrollo Regional
(FEDER), ATTOS project (TIN2012-38536-C03-0)
from the Spanish Government, AORESCU project
(P11-TIC-7684 MO) from the regional government
of Junta de Andalucı́a and CEATIC-2013-01 project
from the University of Jaén.

References

Stefano Baccianella, Andrea Esuli, and Fabrizio Sebas-
tiani. 2010. Sentiwordnet 3.0: An enhanced lexical
resource for sentiment analysis and opinion mining. In
LREC, volume 10, pages 2200–2204.

Marie-Catherine De Marneffe and Christopher D Man-
ning. 2008. Stanford typed dependencies manual.

Kerstin Denecke. 2008. Using sentiwordnet for multilin-
gual sentiment analysis. In Data Engineering Work-
shop, 2008. ICDEW 2008. IEEE 24th International
Conference on, pages 507–512. IEEE.

Minqing Hu and Bing Liu. 2004. Mining and summa-
rizing customer reviews. In Proceedings of the tenth
ACM SIGKDD international conference on Knowl-
edge discovery and data mining, pages 168–177.
ACM.

Bin Lu, Myle Ott, Claire Cardie, and Benjamin K Tsou.
2011. Multi-aspect sentiment analysis with topic mod-
els. In Data Mining Workshops (ICDMW), 2011 IEEE
11th International Conference on, pages 81–88. IEEE.

Diego Marcheggiani, Oscar Täckström, Andrea Esuli,
and Fabrizio Sebastiani. 2014. Hierarchical multi-
label conditional random fields for aspect-oriented
opinion mining. In Advances in Information Retrieval,
pages 273–285. Springer.

George A Miller. 1995. Wordnet: a lexical database for
english. Communications of the ACM, 38(11):39–41.

M Dolores Molina-González, Eugenio Martı́nez-Cámara,
Marı́a-Teresa Martı́n-Valdivia, and José M Perea-
Ortega. 2013. Semantic orientation for polarity clas-
sification in spanish reviews. Expert Systems with Ap-
plications, 40(18):7250–7257.

Bo Pang and Lillian Lee. 2008. Opinion mining and
sentiment analysis. Found. Trends Inf. Retr., 2(1-2):1–
135, January.

Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan.
2002. Thumbs up?: Sentiment classification using
machine learning techniques. In Proceedings of the
ACL-02 Conference on Empirical Methods in Natural
Language Processing - Volume 10, EMNLP ’02, pages
79–86, Stroudsburg, PA, USA.

Maria Pontiki, Haris Papageorgiou, Dimitrios Galanis,
Ion Androutsopoulos, John Pavlopoulos, and Suresh
Manandhar. 2014. Semeval-2014 task 4: Aspect
based sentiment analysis. In Proceedings of the 8th
International Workshop on Semantic Evaluation (Se-
mEval 2014), pages 27–35.

Maria Pontiki, Dimitrios Galanis, Harris Papageorgiou,
Suresh Manandhar, and Ion Androutsopoulos. 2015.
Semeval-2015 task 12: Aspect based sentiment analy-
sis. In Proceedings of the 9th International Workshop
on Semantic Evaluation (SemEval 2015), Denver, Col-
orado.

Changqin Quan and Fuji Ren. 2014. Unsupervised prod-
uct feature extraction for feature-oriented opinion de-
termination. Information Sciences, 272:16–28.

Tun Thura Thet, Jin-Cheon Na, and Christopher SG
Khoo. 2010. Aspect-based sentiment analysis of
movie reviews on discussion boards. Journal of In-
formation Science, page 0165551510388123.

Peter D. Turney. 2002. Thumbs up or thumbs down?:
Semantic orientation applied to unsupervised classi-
fication of reviews. In Proceedings of the 40th An-
nual Meeting on Association for Computational Lin-
guistics, ACL ’02, pages 417–424, Stroudsburg, PA,
USA.

Theresa Wilson, Janyce Wiebe, and Paul Hoffmann.
2005. Recognizing contextual polarity in phrase-level
sentiment analysis. In Proceedings of the Conference
on Human Language Technology and Empirical Meth-
ods in Natural Language Processing, HLT ’05, pages
347–354, Stroudsburg, PA, USA.

Hong Yu and Vasileios Hatzivassiloglou. 2003. Towards
answering opinion questions: Separating facts from
opinions and identifying the polarity of opinion sen-
tences. In Proceedings of the 2003 Conference on
Empirical Methods in Natural Language Processing,
EMNLP ’03, pages 129–136, Stroudsburg, PA, USA.

735


