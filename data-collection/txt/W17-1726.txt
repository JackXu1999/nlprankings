



















































Semi-Automated Resolution of Inconsistency for a Harmonized Multiword Expression and Dependency Parse Annotation


Proceedings of the 13th Workshop on Multiword Expressions (MWE 2017), pages 187–193,
Valencia, Spain, April 4. c©2017 Association for Computational Linguistics

Semi-Automated Resolution of Inconsistency for a Harmonized
Multiword Expression and Dependency Parse Annotation

King Chan, Julian Brooke, and Timothy Baldwin

Department of Computing and Information Systems, The University of Melbourne

chanking@gmail.com, julian.brooke@unimelb.edu.au, tb@ldwin.net

Abstract

This paper presents a methodology for
identifying and resolving various kinds
of inconsistency in the context of merg-
ing dependency and multiword expression
(MWE) annotations, to generate a depen-
dency treebank with comprehensive MWE
annotations. Candidates for correction are
identified using a variety of heuristics, in-
cluding an entirely novel one which iden-
tifies violations of MWE constituency in
the dependency tree, and resolved by ar-
bitration with minimal human interven-
tion. Using this technique, we identi-
fied and corrected several hundred errors
across both parse and MWE annotations,
representing changes to a significant per-
centage (well over 10%) of the MWE in-
stances in the joint corpus.

1 Introduction

The availability of gold-standard annotations is
important for the training and evaluation of a wide
variety of NLP tasks, including the evaluation of
dependency parsers (Buchholz and Marsi, 2006).
In recent years, there has been a focus on multi-
annotation of a single corpus, such as joint syn-
tactic, semantic role, named entity, coreference
and word sense annotation in Ontonotes (Hovy
et al., 2006) or constituency, semantic role, dis-
course, opinion, temporal, event and coreference
(among others) annotation of the Manually Anno-
tated Sub-Corpus of the ANC (Ide et al., 2010).
As part of this, there has been an increased fo-
cus on harmonizing and merging existing anno-
tated data sets as a means of extending the scope of
reference corpora (Ide and Suderman, 2007; De-
clerck, 2008; Simi et al., 2015). This effort some-
times presents an opportunity to fix conflicting an-
notations, a worthwhile endeavour since even a

small number of errors in a gold-standard syntactic
annotation can, for example, result in significant
changes in downstream applications (Habash et
al., 2007). This paper presents the results of a har-
monization effort for the overlapping STREUSLE
annotation (Schneider et al., 2014) of multiword
expressions (“MWEs”: Baldwin and Kim (2010))
and dependency parse structure in the English Web
Treebank (“EWT”: Bies et al. (2012)), with the
long-term goal of building reliable resources for
joint MWE/syntactic parsing (Constant and Nivre,
2016).

As part of merging these two sets of anno-
tations, we use analysis of cross-annotation and
type-level consistency to identify instances of po-
tential annotation inconsistency, with an eye to im-
proving the quality of the component and com-
bined annotations. It is important to point out that
our approach to identifying and handling inconsis-
tencies does not involve re-annotating the corpus;
instead we act as arbitrators, resolving inconsis-
tency in only those cases where human interven-
tion is necessary. Our three methods for identify-
ing potentially problematic annotations are:

• a cross-annotation heuristic that identifies
MWE tokens whose parse structure is incom-
patible with the syntactic annotation of the
MWE;

• a cross-type heuristic that identifies n-grams
with inconsistent token-level MWE annota-
tions; and

• a cross-type, cross-annotation heuristic that
identifies MWE types whose parse structure
is inconsistent across its token occurrences.

The first of these is specific to this harmonization
process, and as far as we aware, entirely novel.
The other two are adaptions of an approach to im-
proving syntactic annotations proposed by Dick-
inson and Meurers (2003). After applying these
heuristics and reviewing the candidates, we iden-
tified hundreds of errors in MWE annotation and

187



about a hundred errors in the original syntactic an-
notations. We make available a tool that applies
these fixes in the process of joining the two an-
notations into a single harmonized, corrected an-
notation, and release the harmonized annotations
in the form of HAMSTER (the HArmonized Mul-
tiword and Syntactic TreE Resource): https:
//github.com/eltimster/HAMSTER.

2 Related Work

Our long-term goal is in building reliable re-
sources for joint MWE/syntactic parsing. Explicit
modelling of MWEs has been shown to improve
parser accuracy (Nivre and Nilsson, 2004; Finkel
and Manning, 2009; Korkontzelos and Manand-
har, 2010; Green et al., 2013; Vincze et al., 2013;
Candito and Constant, 2014; Constant and Nivre,
2016). Treatment of MWEs has typically involved
parsing MWEs as single lexical units (Nivre and
Nilsson, 2004; Eryiğit et al., 2011; Aggeliki Fo-
topoulou, 2014), however this flattened, “words
with spaces” (Sag et al., 2002) approach is inflex-
ible in its coverage of MWEs where components
have some level of flexibility.

The English Web Treebank (Bies et al., 2012)
represents a gold-standard annotation effort over
informal web text. The original syntactic con-
stituency annotation of the corpus was based on
hand-correcting the output of the Stanford Parser
(Manning et al., 2014); for our purposes we have
converted this into a dependency parse using the
Stanford Typed Dependency converter (de Marn-
effe et al., 2006). We considered the use of
the Universal Dependencies representation (Nivre
et al., 2016), however we noted that several as-
pects of that annotation (in particular the treat-
ment of all prepositions as case markers depen-
dent on their noun) make it inappropriate for joint
MWE/syntactic parsing since it results in large
numbers of MWEs that are non-contiguous in their
syntactic structure (despite being contiguous at the
token-level). As such, the Stanford Typed Depen-
dencies are the representation which has the great-
est currency for joint MWE/syntactic parsing work
(Constant and Nivre, 2016).

The STREUSLE corpus (Schneider et al., 2014)
is based entirely on the Reviews subset of the
EWT, and comprises of 3,812 sentences repre-
senting 55,579 tokens. The annotation was com-
pleted by six linguists who were native English
speakers. Every sentence was assessed by at least

two annotators, which resulted in an average inter-
annotator F1 agreement of 0.7. The idiosyncratic
nature of MWEs lends itself to challenges associ-
ated with their interpretation, and this was read-
ily acknowledged by those involved in the devel-
opment of the STREUSLE corpus (Hollenstein et
al., 2016). Two important aspects of the MWE an-
notation are that it includes both contiguous and
non-contiguous MWEs (e.g. check ⇤ out), and that
it supports both weak and strong annotation; both
of these are considered in scope for our inconsis-
tency analysis. A variety of cues are employed to
determine this associative strength. The primary
factor relates to the degree in which the expres-
sion is semantically opaque and/or morphosyn-
tactically idiosyncratic. An example of a strong
MWE would be top notch, as used in the sentence:
We stayed at a top notch hotel. The semantics
of this expression are not immediately predictable
from the meanings of top and notch. On the other
hand, the expression highly recommend is consid-
ered to be a weak expression as it is largely com-
positional — one can highly recommend a prod-
uct — as indicated by the presence of alternatives
such as greatly recommend which are also accept-
able though less idiomatic. A total of 3,626 MWE
instances were identified in STREUSLE, across
2,334 MWE types.

Other MWE-aware dependency treebanks in-
clude the various UD treebanks (Nivre et al.,
2016), the Prague Dependency Treebank (Bejček
et al., 2013), and others (Nivre and Nilsson, 2004;
Eryiğit et al., 2011; Candito and Constant, 2014).
The representation of MWEs, and the scope of
types covered by these treebanks, can vary signif-
icantly. For example, the internal syntactic struc-
ture may be flattened (Nivre and Nilsson, 2004), or
in the case of Candito and Constant (2014), allow
for distinctions in the granularity of syntactic rep-
resentation for regular vs. irregular MWE types.

The identification of inconsistencies in anno-
tation requires comparisons to be made between
similar instances that are labeled differently. Boyd
et al. (2007) employed an alignment-based ap-
proach to assess differences in the annotation of
n-gram word sequences in order to establish the
likelihood of error occurrence. Other work in
the syntactic inconsistency detection domain in-
cludes those related to POS tagging (Loftsson,
2009; Eskin, 2000; Ma et al., 2001) and parse
structure (Ule and Simov, 2004; Kato and Mat-

188



Deep tissue massage helps with pain in neck and shoulders
JJ NN NN VBZ IN NN IN NN CC NNS

amod

nn nsubj

root

prep pobj prep pobj cc

conj

Figure 1: An example where the arc count heuristic is breached. Deep tissue has been labeled in the
sentence here as an MWE in STREUSLE. Deep and tissue act as modifiers to massage, a term that has
not been included as part of the MWE.

subara, 2010). Dickinson and Meurers (2003) out-
line various approaches for detecting inconsisten-
cies in parse structure within treebanks.

In general, inconsistencies associated with
MWE annotation fall under two categories: (1)
annotator error (i.e. false positives and false neg-
atives); and (2) ambiguity associated with the as-
sessment of hard cases. While annotation errors
apply to situations where a correct label can be ap-
plied but is not done so, hard cases are those where
the correct label is inherently difficult to assign,
and can be particularly relevant to certain classes
of MWEs. For example, there may be consider-
able differences in inter-annotator agreement as-
sociated with assessing the relative transparency
and associative strength of a non-fixed MWE.

3 Error Candidate Identification

3.1 MWE Syntactic Constituency Conflicts
The hypothesis that drives our first analysis is that
for nearly all MWE types, the component words
of the MWE should be syntactically connected,
which is to say that every word is a dependent
of another word in the MWE, except one word
which connects the MWE to the rest of the sen-
tence (or the root of the sentence). We can realise
this intuition by using an arc count heuristic: for
each labeled MWE instance we count the number
of incoming dependency arcs that are headed by a
term outside the MWE, and if the count is greater
than one, we flag it for manual analysis. Figure 1
gives an example where the arc count heuristic is
breached since both terms of the MWE deep tissue
act as modifiers to the head noun that sits outside
the MWE.

3.2 MWE Type Inconsistency
Our second analysis involves first collecting a list
of all MWE types in the STREUSLE corpus, cor-
responding to lemmatized n-grams, possibly with
gaps. We then match these n-grams across the

same corpus, and flag any MWE type which has
at least one inconsistency with regards to the an-
notation. That is, we extract as candidates any
MWE types where there were at least two occur-
rences of the corresponding n-gram in the corpus
that were incompatible with respect to their an-
notation in STREUSLE, including discrepancies
in weak/strong designation. For non-contiguous
MWE types, matches containing up to 4 words of
intervening context between the two parts of the
MWE type were included as candidates for further
assessment.

3.3 MWE Type Parse Inconsistency

The hypothesis that drives our third analysis is
that we would generally expect the internal syn-
tax of an MWE type to be consistent across all
its instances.1 For each MWE type, we extracted
the internal dependency structure of all its labeled
instances, and flagged for further assessment any
type for which the parse structure varied between
at least two of those instances. Note that although
this analysis is aimed at fixing parse errors, it
makes direct use of the MWE annotation provided
by STREUSLE to greatly limit the scope of error
candidates to those which are most relevant to our
interest.

4 Error Arbitration

Error arbitration was carried out by the authors (all
native English speakers with experience in MWE
identification), with at least two authors looking
at each error candidate in most instances, and
for certain difficult cases, the final annotation be-
ing based on discussion among all three authors.
One advantage of our arbitration approach over
a traditional token-based annotation was that we
could enforce consistency across similar error can-

1Noting that we would not expect this to occur between
MWE instances of a given combination of words, and non-
MWE combinations of those same words.

189



didates (e.g. disappointed with and happy with)
and also investigate non-candidates to arrive at a
consensus; where at all possible, our changes re-
lied on precedents that already existed in the rele-
vant annotation.

Arbitration for the MWE syntax conflicts usu-
ally involved identifying an error in one of the two
annotations, and in most cases this was relatively
obvious. For instance, in the candidate . . . the
usual lady called in sick hours earlier, called in
sick was correctly labeled as an MWE, but the
parse incorrectly includes sick as a dependent of
hours, rather than called in. An example of the op-
posite case is . . . just to make the appointment . . . ,
where make the had been labeled as an MWE, an
obvious error which was caught by our arc count
heuristic. There were cases where our arc count
heuristic was breached due to what we would view
as a general inadequacy in the syntactic annota-
tion, but we decided not to effect a change be-
cause the impact would be too far reaching; ex-
amples of this were certain discourse markers (e.g.
as soon as), and infinitives (e.g. have to complete
where the to is considered a dependent of its verb
rather than of the other term in the MWE have to).
The most interesting cases were a handful of non-
contiguous MWEs where there was truly a discon-
tinuity in the syntax between the two parts of the
MWE, for instance no amount of ⇤ can. This sug-
gests a basic limitation in our heuristic, although
the vast majority of MWEs did satisfy it.

For the two type-level arbitrations, there were
cases of inconsistency upheld by real usage dif-
ferences (e.g. a little house vs. a little tired). We
identified clear differences in usage first, and di-
vided the MWE types into sets, excluding from
further analysis non-MWE usages of MWE type
n-grams. For each consistent usage of an MWE
type, the default position was to prefer the major-
ity annotation across the set of instances, except
when there were other candidates that were essen-
tially equivalent: for instance, if we had relied on
majority annotation for job ⇤ do (e.g. the job that
he did) it would have been a different annotation
than do ⇤ job (e.g. do a good job), so we consid-
ered these two together. We treated contiguous
and non-contiguous versions of the same MWE
type in the same manner.

In the MWE type consistency arbitration, for
cases where majority rules did not provide a clear
answer and there was no overwhelming evidence

for non-compositionality, we introduced a special
internal label called hard. These correspond to
cases where the usage is consistent and the incon-
sistency seems to be a result of the difficulty of
the annotation item (as discussed earlier in Sec-
tion 2), which extended also to our arbitration.
Rather than enforce a specific annotation without
strong evidence, or allow the inconsistency to re-
main when there is no usage justification for it, the
corpus merging and correction tool gives the user
the option to treat hard annotated MWEs in vary-
ing ways: the annotation may be kept unchanged,
removed, converted to weak, or covered to hard
for the purpose of excluding it from evaluation.
Examples of hard cases include go back, go in,
more than, talk to, speak to, thanks guys, not that
great, pleased with, have ⇤ option, get ⇤ answer,
fix ⇤ problem. On a per capita basis, inconsisten-
cies are more common for non-contiguous MWEs
relative to their contiguous counterparts, and we
suspect that this is partially due to their tendency
to be weaker, in addition to the challenges in-
volved in correctly discerning the two parts, which
are sometimes at a significant distance from each
other.

Table 1 provides a summary of changes to
MWE annotation at the MWE type and token lev-
els. Mixed refer to MWEs that are heterogeneous
in the associative strength between terms in the
MWE (between weak and strong). Most of
the changes in Table 1 (98% of the types) were
the result of our type consistency analysis. Al-
most half of the changes involved the use of the
hard label, but even excluding these (since only
some of these annotations required actual changes
in the final version of the corpus) our changes in-
volve over 10% of the MWE tokens in the cor-
pus, and thus represent a significant improvement
to the STREUSLE annotation.

Relative to the changes to the MWE annotation,
the changes to the parse annotation were more
modest, but still not insignificant: for 181 MWE
tokens across 157 types, we identified and cor-
rected a dependency and/or POS annotation error.
The majority of these (61%) were identified us-
ing the arc count heuristic. Note we applied the
parse relevant heuristics after we fixed the MWE
type consistency errors, ensuring that MWE anno-
tations that were added were duly considered for
parse errors.

190



No MWE Weak Strong Mixed Hard TOTAL

Token

No MWE — 56 134 6 148 344
Weak 33 — 22 5 46 106
Strong 41 43 — 9 70 163
Mixed 0 4 5 14 2 25

TOTAL 74 103 161 34 266 638

Type

No MWE — 31 72 5 63 171
Weak 29 — 13 4 35 81
Strong 32 28 — 7 43 110
Mixed 0 4 4 9 2 19

TOTAL 61 63 89 25 143 381

Table 1: Summary of changes to MWE annotation at the MWE type and token level

5 Discussion

Our three heuristics are useful because they iden-
tify potential errors with a high degree of preci-
sion. For the MWE type consistency analysis 77%
of candidate types were problematic, and for parse
type consistency, 79%. For the arc count heuristic,
45% of candidate types were ultimately changed:
as mentioned earlier, some of the breaches in-
volved systematic issues with annotation schema
that we felt uncomfortable changing in isolation.
By bringing these candidate instances to our at-
tention, we were able to better focus our manual
analysis effort, including in some cases looking
across multiple related types, or even searching for
specialist knowledge which could resolve ambigu-
ities: for instance, in the example shown in Fig-
ure 1, though a layperson without reference ma-
terial may be unsure whether it is tissue or mas-
sage which is considered to be deep, a quick on-
line search indicates that the original EWT syntax
is in error (deep modifies tissue).

However, it would be an overstatement to claim
to have fixed all (or even almost all) the errors
in the corpus. For instance, our type consistency
heuristics only work when there are multiple in-
stances of the same type, yet it is worth noting
that 82% of the MWE types in the corpus are rep-
resented by a singleton instance. Our arc count
heuristic can identify issues with singletons, but its
scope is fairly limited. We cannot possibly iden-
tify missing annotations for types that were not an-
notated at least once. We might also miss certain
kinds of systematic annotation errors, for instance
those mentioned in De Smedt et al. (2015), though

that work focused on the use of mwe dependency
labels which are barely used in the EWT, one of
the reasons a resource like STREUSLE is so use-
ful.

6 Conclusion

We have proposed a methodology for merging
multiword expression and dependency parse anno-
tations, to generate HAMSTER: a gold-standard
MWE-annotated dependency treebank with high
consistency. The heuristics used to enforce con-
sistency operate at the type- and cross-annotation
level, and affected well over 10% of the MWEs in
the new resource.

References
Voula Giouli Aggeliki Fotopoulou, Stella Markantona-

tou. 2014. Encoding MWEs in a conceptual lexi-
con. In Proceedings of the 10th Workshop on Multi-
word Expressions (MWE 2014).

Timothy Baldwin and Su Nam Kim. 2010. Multi-
word expressions. In Nitin Indurkhya and Fred J.
Damerau, editors, Handbook of Natural Language
Processing, Second Edition. CRC Press, Taylor and
Francis Group, Boca Raton, FL.

Eduard Bejček, Eva Hajičová, Jan Hajič, Pavlı́na
Jı́nová, Václava Kettnerová, Veronika Kolářová,
Marie Mikulová, Jiřı́ Mı́rovskỳ, Anna Nedoluzhko,
Jarmila Panevová, et al. 2013. Prague dependency
treebank 3.0.

Ann Bies, Justin Mott, Colin Warner, and Seth Kulick.
2012. English web treebank. technical report
ldc2012t13. Technical report, Linguistic Data Con-
sortium.

191



Adriane Boyd, Markus Dickinson, and Detmar Meur-
ers. 2007. Increasing the recall of corpus annotation
error detection. In Proceedings of the Sixth Work-
shop on Treebanks and Linguistic Theories (TLT
2007), Bergen, Norway.

Sabine Buchholz and Erwin Marsi. 2006. Conll-x
shared task on multilingual dependency parsing. In
Proceedings of the Tenth Conference on Computa-
tional Natural Language Learning, CoNLL-X ’06,
pages 149–164, Stroudsburg, PA, USA. Association
for Computational Linguistics.

Marie Candito and Matthieu Constant. 2014. Strate-
gies for contiguous multiword expression analysis
and dependency parsing. In The 52nd Annual Meet-
ing of the Association for Computational Linguistics
(ACl ’14).

Matthieu Constant and Joakim Nivre. 2016. A
transition-based system for joint lexical and syntac-
tic analysis. In the 54th Annual Meeting of the As-
sociation for Computational Linguistics (ACL ’16),
pages 161–171.

Marie-Catherine de Marneffe, Bill MacCartney, and
Christopher D. Manning. 2006. Generating typed
dependency parses from phrase structure parses. In
Proceedings of the 5th International Conference on
Language Resources and Evaluation (LREC ’06),
Genova, Italy.

Koenraad De Smedt, Victoria Rosén, and Paul Meurer.
2015. Studying consistency in ud treebanks with
iness-search. In Proceedings of the Fourteenth
Workshop on Treebanks and Linguistic Theories
(TLT14), pages 258–267.

Thierry Declerck. 2008. A framework for standard-
ized syntactic annotation. In Proceedings of the
2008 Language Resource and Evaluation Confer-
ence (LREC 08).

Markus Dickinson and W. Detmar Meurers. 2003. De-
tecting inconsistencies in treebanks. In Proceedings
of the Second Workshop on Treebanks and Linguistic
Theories (TLT 2003).

Gülşen Eryiğit, Tugay İlbay, and Ozan Arkan Can.
2011. Multiword expressions in statistical depen-
dency parsing. In Proceedings of the Second Work-
shop on Statistical Parsing of Morphologically Rich
Languages, SPMRL ’11, pages 45–55, Stroudsburg,
PA, USA. Association for Computational Linguis-
tics.

Eleazar Eskin. 2000. Detecting errors within a cor-
pus using anomaly detection. In Proceedings of
the 1st North American Chapter of the Association
for Computational Linguistics Conference, NAACL
2000, pages 148–153, Stroudsburg, PA, USA. Asso-
ciation for Computational Linguistics.

Jenny Rose Finkel and Christopher D. Manning. 2009.
Joint parsing and named entity recognition. In Pro-
ceedings of Human Language Technologies: The

2009 Annual Conference of the North American
Chapter of the Association for Computational Lin-
guistics, NAACL ’09, pages 326–334, Stroudsburg,
PA, USA. Association for Computational Linguis-
tics.

Spence Green, Marie-Catherine de Marneffe, and
Christopher D. Manning. 2013. Parsing models for
identifying multiword expressions. Computational
Linguistics, 39(1):195–227, March.

Nizar Habash, Ryan Gabbard, Owen Rambow, Seth
Kulick, and Mitchell P Marcus. 2007. Determining
case in arabic: Learning complex linguistic behav-
ior requires complex linguistic features. In EMNLP-
CoNLL, pages 1084–1092.

Nora Hollenstein, Nathan Schneider, and Bonnie Web-
ber. 2016. Inconsistency detection in semantic an-
notation. In Proceedings of the 2016 Langauage Re-
sources and Evaluation Conference (LREC ’16).

Eduard Hovy, Mitchell Marcus, Martha Palmer,
Lance Ramshaw, and Ralph Weischedel. 2006.
OntoNotes: The 90% solution. In Proceedings of
the Main Conference on Human Language Technol-
ogy Conference of the North American Chapter of
the Association of Computational Linguistics, pages
57–60, New York City, USA.

Nancy Ide and Keith Suderman. 2007. Graf: A graph-
based format for linguistic annotations. In Proceed-
ings of the Linguistic Annotation Workshop, LAW
’07, pages 1–8, Stroudsburg, PA, USA. Association
for Computational Linguistics.

Nancy Ide, Collin Baker, Christiane Fellbaum, and Re-
becca Passonneau. 2010. The manually annotated
sub-corpus: A community resource for and by the
people. pages 68–73.

Yoshihide Kato and Shigeki Matsubara. 2010. Cor-
recting errors in a treebank based on synchronous
tree substitution grammar. In Proceedings of the
ACL 2010 Conference Short Papers, pages 74–79.
Association for Computational Linguistics.

Ioannis Korkontzelos and Suresh Manandhar. 2010.
Can recognising multiword expressions improve
shallow parsing? In Human Language Technolo-
gies: The 2010 Annual Conference of the North
American Chapter of the Association for Com-
putational Linguistics, HLT ’10, pages 636–644,
Stroudsburg, PA, USA. Association for Computa-
tional Linguistics.

Hrafn Loftsson. 2009. Correcting a pos-tagged corpus
using three complementary methods. In Proceed-
ings of the 12th Conference of the European Chap-
ter of the Association for Computational Linguistics,
pages 523–531. Association for Computational Lin-
guistics.

Qing Ma, Bao-Liang Lu, Masaki Murata, Michnori
Ichikawa, and Hitoshi Isahara. 2001. On-line error
detection of annotated corpus using modular neural

192



networks. In International Conference on Artificial
Neural Networks, pages 1185–1192. Springer.

Christopher D. Manning, Mihai Surdeanu, John Bauer,
Jenny Finkel, Steven J. Bethard, and David Mc-
Closky. 2014. The Stanford CoreNLP natural lan-
guage processing toolkit. In Proceedings of 52nd
Annual Meeting of the Association for Computa-
tional Linguistics: System Demonstrations, pages
55–60.

Joakim Nivre and Jens Nilsson. 2004. Multiword units
in syntactic parsing. In Methodologies and Evalua-
tion of Multiword Units in Real-World Applications
(MEMURA).

Joakim Nivre, Marie-Catherine de Marneffe, Filip Gin-
ter, Yoav Goldberg, Jan Hajic, et al. 2016. Univer-
sal dependencies v1: A multilingual treebank col-
lection.

Ivan A. Sag, Timothy Baldwin, Francis Bond, Ann
Copestake, and Dan Flickinger. 2002. Multiword
expressions: A pain in the neck for NLP. In Pro-
ceedings of the 3rd International Conference on In-
telligent Text Processing and Computational Lin-
guistics (CICLing ’02).

Nathan Schneider, Spencer Onuffer, Nora Kazour,
Emily Danchik, Michael T. Mordowanec, Henrietta
Conrad, and Noah A. Smith. 2014. Comprehensive
annotation of multiword expressions in a social web
corpus. In Proceedings of the Ninth International
Conference on Language Resources and Evaluation,
pages 455–461, Reykjavı́k, Iceland.

Maria Simi, Simonetta Montemagni, and Cristina
Bosco. 2015. Harmonizing and merging italian
treebanks: Towards a merged italian dependency
treebank and beyond. In Harmonization and De-
velopment of Resources and Tools for Italian Natu-
ral Language Processing within the PARLI Project,
pages 3–23.

Tylman Ule and Kiril Simov. 2004. Unexpected pro-
ductions may well be errors. In Proceedings of the
2004 Language Resources and Evaluation Confer-
ence (LREC ’04).

Veronika Vincze, Janos Zsibrita, and István Nagy T.
2013. Dependency parsing for identifying hungar-
ian light verb constructions. In Proceedings of In-
ternational Joint Conference on Natural Language
Processing (IJCNLP ’13).

193


