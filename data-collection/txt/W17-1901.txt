



















































Compositional Semantics using Feature-Based Models from WordNet


Proceedings of the 1st Workshop on Sense, Concept and Entity Representations and their Applications, pages 1–11,
Valencia, Spain, April 4 2017. c©2017 Association for Computational Linguistics

Compositional Semantics using Feature-Based Models from WordNet

Pablo Gamallo
Centro Singular de Investigación en

Tecnoloxı́as da Información (CiTIUS)
Universidade de Santiago de Compostela

Galiza, Spain
pablo.gamallo@usc.es

Martı́n Pereira-Fariña
Centre for Argument

Technology (ARG-tech)
University of Dundee

Dundee, DD1 4HN, Scotland (UK)
m.z.pereirafarina@dundee.ac.uk

Departamento de Filosofı́a e Antropoloxı́a
Universidade de Santiago de Compostela
Pz. de Mazarelos, 15782, Galiza, Spain

martin.pereira@usc.es

Abstract
This article describes a method to build
semantic representations of composite ex-
pressions in a compositional way by using
WordNet relations to represent the mean-
ing of words. The meaning of a target
word is modelled as a vector in which
its semantically related words are assigned
weights according to both the type of the
relationship and the distance to the tar-
get word. Word vectors are composi-
tionally combined by syntactic dependen-
cies. Each syntactic dependency triggers
two complementary compositional func-
tions: the named head function and depen-
dent function. The experiments show that
the proposed compositional method per-
forms as the state-of-the-art for subject-
verb expressions, and clearly outperforms
the best system for transitive subject-verb-
object constructions.

1 Introduction

The principle of compositionality (Partee, 1984)
states that the meaning of a complex expression
is a function of the meaning of its constituent
parts and of the mode of their combination. In
the recent years, different distributional semantic
models endowed with a compositional component
have been proposed. Most of them define words as
high-dimensional vectors where dimensions rep-
resent co-occurring context words. This distribu-
tional semantic representation makes it possible
to combine vectors using simple arithmetic opera-
tions such as addition and multiplication, or more
advanced compositional methods such as learning
functional words as tensors and composing con-
stituents through inner product operations.

Notwithstanding, these models are usually qual-
ified as black box systems because they are usually
not interpretable by humans. Currently, the field of
interpretable computational models is gaining rel-
evance1 and, therefore, the development of more
explainable and understandable models in compo-
sitional semantics is also an open challenge. in this
field. On the other hand, distributional semantic
models, given the size of the vectors, needs signif-
icant resources and they are dependent on particu-
lar corpus, which can generate some biases in their
application to different languages.

Thus, in this paper, we will pay attention to
compositional approaches which employ other
kind of word semantic models, such as those based
on the WordNet relationships; i.e., synsets, hyper-
nyms, hyponyms, etc. Only in (Faruqui and Dyer,
2015) we can find a proposal for word vector rep-
resentation using hand-crafted linguistic resources
(WordNet, FrameNet, etc.), although a composi-
tional frame is not explicitly adopted. Therefore,
to the best of our knowledge, this is the first work
using WordNet to build compositional semantic
interpretations. Thus, in this article, we propose
a method to compositionally build the semantic
representation of composite expressions using a
feature-based approach (Hadj Taieb et al., 2014):
constituent elements are induced by WordNet re-
lationships.

However, this proposal raises a serious prob-
lem: the semantic representation of two syntacti-
cally related words (e.g. the verb run and the noun
computer in “the computer runs”) encodes incom-
patible information and there is no direct way of
combining the features used to represent the mean-
ing of the two words. On the one hand, the verb

1http://www.darpa.mil/program/explainable-artificial-
intelligence

1



run is related by synonymy, hypernym, hyponym
and entailment to other verbs and, on the other, the
noun computer is put in relation with other nouns
by synonymy, hypernym, hyponym, and so on.

In order to solve this drawback, on the basis
of previous work on dependency-based distribu-
tional compositionality (Thater et al., 2010; Erk
and Padó, 2008), we distinguish between direct
denotation and selectional preferences within a
dependency relation. More precisely, when two
words are syntactically related, for instance com-
puter and the verb run by the subject relation, we
build two contextualized senses: the contextual-
ized sense of computer given the requirements of
run and the contextualized sense of run given com-
puter.

The sense of computer is built by combining
the semantic features of the noun (its direct de-
notation) with the selectional preferences imposed
by the verb. The features of the noun are built
from the set of words linked to computer in Word-
Net, while the selectional preferences of run in
the subject position are obtained by combining the
features of all the nouns that can be the nominal
subject of the verb (i.e. the features of runners).
Then, the two sets of features are combined and
the resulting new set represents the specific sense
of the noun computer as nominal subject of run.
The sense of the verb given the noun is built in a
analogous way: the semantic features of the verb
are combined with the (inverse) selectional pref-
erences imposed by the noun, resulting in a new
compositional representation of the verb run when
it is combined with computer at the subject po-
sition. The two new compositional feature sets
represent the contextualized senses of the two re-
lated words. During the contextualization process,
ambiguous or polysemous words may be disam-
biguated in order to obtain the right representation.

For dealing with any sequence with N (lexi-
cal) words (e.g., “the coach runs the team”), the
semantic process can be applied in two different
ways: from left-to-right and from right-to-left. In
the first case, it is appliedN−1 times dependency-
by-dependency in order to obtain N contextual-
ized senses, one per lexical word. Thus, firstly,
the subject dependency builds two contextualized
senses: that of run given the noun coach and that
of the noun given the verb. Then, the direct object
dependency is applied on the already contextual-
ized sense of the verb in order to contextualize it

again given team at the direct object position. This
dependency also yields the contextualized sense of
the object given the verb and its nominal subject
(coach+run). At the end of the interpretation pro-
cess, we obtain three fully contextualized senses.
In the second case, from right-to-left, the semantic
process process is applied in a similar way, being
contextualized (and disambiguated) using the re-
strictions imposed by the verb and its nominal ob-
ject (run+team). As in the first case, three slightly
different word senses are also obtained.

Lastly, word sense disambiguation is out of the
aim of this paper. Here, we only use WordNet for
extracting semantic information from words, but
not to identify word senses.

The article is organized as follow: In the next
section (2), different approaches on ontological
feature-based representations and compositional
semantics are introduced and discussed. Then,
sections 3 and 4 respectively describe our feature-
based semantic representation and compositional
strategy. In Section 5, some experiments are per-
formed to evaluate the quality of the word models
and compositional word vectors. Finally, relevant
conclusions are reported in Section 6.

2 Related Work

Our approach relies on two tasks: to build feature-
based representations using WordNet relations,
and to build compositional vectors using the
WordNet representations. In this section, we will
examine work related to these two tasks.

2.1 Feature-Based Approaches

Tversky (1977), in order to define a similarity
measure, assumes that any object can be repre-
sented as a collection (set) of features or proper-
ties. Therefore, a similarity metric is a feature-
matching process between two objects. This con-
sists of a linear combination of the measures of
their common and distinctive features. It is worth
noting that this is a non-symmetric measure.

In the particular case of semantic similarity met-
rics, each word or concept is featured by means of
a set of words (Hadj Taieb et al., 2014). Framed
into an ontology such as WordNet, these sets
of words are obtained from taxonomic (hyper-
nym, hyponym, etc.) and non-taxonomic (synsets,
glosses, meronyms, etc.) properties (Meng et al.,
2013), although these last ones are classified as
secondary in many cases (Slimani, 2013). The

2



main objective of this approach is to capture the
semantic knowledge induced by ontological rela-
tionships.

Our model is partly inspired by that defined
in (Rodrı́guez and Egenhofer, 2003). It proposes
that the set of properties that characterizes a word
may be stratified into three groups: i) synsets;
ii) features (e.g., meronyms, attributes, hyponym,
etc.), and, iii) neighbor concepts (those linked via
semantic pointers). Each one of these strata is
weighted according to its contribution to the rep-
resentation of the concept. The measure analyzes
the overlapping among the three strata between the
two terms under comparison.

2.2 Compositional Strategies

Several models for compositionality in vector
spaces have been proposed in recent years, and
most of them use bag-of-words as basic distribu-
tional representations of word contexts. The ba-
sic approach to composition, explored by Mitchell
and Lapata (2008; 2009; 2010), is to combine vec-
tors of two syntactically related words with arith-
metic operations: addition and component-wise
multiplication. The additive model produces a sort
of union of word contexts, whereas multiplication
has an intersective effect. According to Mitchell
and Lapata (2008), component-wise multiplica-
tion performs better than the additive model. How-
ever, in (Mitchell and Lapata, 2009; Mitchell and
Lapata, 2010), these authors explore weighted ad-
ditive models giving more weight to some con-
stituents in specific word combinations. For in-
stance, in a noun-subject-verb combination, the
verb is provided with higher weight because the
whole construction is closer to the verb than to
the noun. Other weighted additive models are de-
scribed in (Guevara, 2010) and (Zanzotto et al.,
2010). All these models have in common the fact
of defining composition operations for just word
pairs. Their main drawback is that they do not pro-
pose a more systematic model accounting for all
types of semantic composition. They do not focus
on the logical aspects of the functional approach
underlying compositionality.

Other distributional approaches develop sound
compositional models of meaning inspired by
Montagovian semantics, which induce the compo-
sitional meaning of the functional words from ex-
amples adopting regression techniques commonly
used in machine learning (Krishnamurthy and

Mitchell, 2013; Baroni and Zamparelli, 2010; Ba-
roni, 2013; Baroni et al., 2014). In our approach,
by contrast, compositional functions, which are
driven by dependencies and not by functional
words, are just basic arithmetic operations on vec-
tors as in (Mitchell and Lapata, 2008). Arithmetic
approaches are easy to implement and produce
high-quality compositional vectors, which makes
them a good choice for practical applications (Ba-
roni et al., 2014).

Other compositional approaches based on Cat-
egorial Grammar use tensor products for com-
position (Grefenstette et al., 2011; Coecke et
al., 2010). A neural network-based method
with tensor factorization for learning the embed-
dings of transitive clauses has been introduced
in (Hashimoto and Tsuruoka, 2015). Two prob-
lems arise with tensor products. First, they re-
sult in an information scalability problem, since
tensor representations grow exponentially as the
phrases grow longer (Turney, 2013). And sec-
ond, tensor products did not perform as well as
component-wise multiplication in Mitchell and
Lapata’s (2010) experiments.

There are also works focused on the notion
of sense contextualization, e.g., Dinu and Lapata
(2010) work on context-sensitive representations
for lexical substitution. Reddy et al. (2011) work
on dynamic prototypes for composing the seman-
tics of noun-noun compounds and evaluate their
approach on a compositionality-based similarity
task.

So far, all the cited works are based on bag-
of-words to represent vector contexts and, then,
word senses. However, there are a few works us-
ing vector spaces structured with syntactic infor-
mation. Thater et al. (2010) distinguish between
first-order and second-order vectors in order to al-
low two syntactically incompatible vectors to be
combined. This work is inspired by that described
in (Erk and Padó, 2008). Erk and Padó (2008) pro-
pose a method in which the combination of two
words, a and b, returns two vectors: a vector a’
representing the sense of a given the selectional
preferences imposed by b, and a vector b’ standing
for the sense of b given the (inverse) selectional
preferences imposed by a. A similar strategy is
reported in Gamallo (2017). Our approach is an at-
tempt to join the main ideas of these syntax-based
models (namely, second-order vectors, selectional
preferences and two returning words per combi-

3



nation) in order to apply them to WordNet-based
word representations.

3 Semantic Features from WordNet

A word meaning is described as a feature-value
structure. The features are the words with which
the target word is related to in the ontology (e.g., in
WordNet, hypernym, hyponym, etc.) and the val-
ues correspond to weights computed taking into
account two parameters: the relation type and the
edge-counting distance between the target word
and each word feature (i.e. the number of rela-
tions required to achieve the feature from the tar-
get word) (Rada et al., 1989).

The algorithm to set the feature values is the fol-
lowing. Given a target word w1 and the feature set
F , where wi ∈ F if wi is a word semantically re-
lated to w1 in WordNet, the weight for the relation
between w1 and wi is computed by equation 1:

weight(w1, wi) =
R∑

j=1

1
length(w1, wi, rj)

(1)

where R is the number of different semantic re-
lations (e.g. synonymy/synset, hyperonymy, hy-
ponymy, etc) that WordNet defines for the part-
of-speech of the target word. For instance, nouns
have five different relations, verbs four and ad-
jectives just two. length(w1, wi, rj is the length
of the path from the target word w1 to its fea-
ture wi in relation rj . length(w1, wi, rj) = 1
when rj stands for the synonymy relationship,
i.e. when w1 and wi belong to the same synset;
length(w1, wi, rj) = 2 if wi is at the first level
within the hierarchy associated to relation rj .

For instance, the length value of a direct hyper-
nym is 2 because there is a distance of two arcs
with regard to the target word: the first arc goes
from the target word to a synset and the second one
is the hyperonymy relation between the direct hy-
pernym and the synset. The length value increases
in one unit as the hierarchy level goes up, so at
level 4, the length score is 5 and then the partial
weight is 1/5 = 0.2. For some non-taxonomic re-
lations, namely meronymy, holonymy and coordi-
nates, there is only one level in WordNet, but the
distance is 3 since the target word and the word
feature (part, whole or coordinate term) are sepa-
rated by a synset and a hypernym.

As a feature word wi may be related to the
target w1 via different semantic relations (with-

out distinguishing between different word senses),
the final weight is the addition of all partial
weights. For instance, take the noun car. It is
related to automobile through two different re-
lationships: they belong to the same synset and
the latter is a direct hypernym of the former, so
weight(car, automobile) = 1/1 + 1/2 = 1.5.

To compute compositional operations on words,
the feature-value structure associated to each word
is modeled as a vector, where features are dimen-
sions, words are objects, and weights the values
for each object/dimension position.

4 Compositional Semantics

4.1 Syntactic Dependencies As
Compositional Functions

Our approach is also inspired in (Erk and Padó,
2008). Here, semantic composition is modeled in
terms of function application driven by binary de-
pendencies. A dependency is associated in the se-
mantic space with two compositional functions on
word vectors: the head and the dependent func-
tions. To explain how they work, let us take the
direct object relation (dobj) between the verb run
and the noun team in the expression “run a team”.
The head function, dobj↑, combines the vector of
the head verb ~run with the selectional preferences
imposed by the noun, which is also a vector of
WordNet features, and noted ~team◦. This combi-
nation is performed by component-wise multipli-
cation and results in a new vector ~rundobj↑, which
represents the contextualized sense of run given
team in the dobj relation:

dobj↑( ~run, ~team◦) = ~run� ~team◦ = ~rundobj↑
To build the (inverse) selectional preferences im-
posed by the dependent word team as direct ob-
ject on the verb, we require a reference corpus to
extract all those verbs of which team is the di-
rect object. The selectional preferences of team
as direct object of a verb, and noted ~team◦, is a
new vector obtained by component-wise addition
of the vectors of all those verbs (e.g. create, sup-
port, help, etc) that are in dobj relation with the
noun team:

~team◦ =
∑

~v∈ T
~v

where T is the vector set of verbs having team as
direct object (except run). T is thus included in the

4



subspace of verb vectors. Component-wise addi-
tion has an union effect.

Similarly, the dependent function, dobj↓, com-
bines the noun vector ~team with the selectional
preferences imposed by the verb, noted ~run◦, by
component-wise multiplication. Such a combina-
tions builds the new vector of ~teamdobj↓, which
stands for the contextualized sense of team given
run in the dobj relation:

dobj↓( ~run◦, ~team) = ~team� ~run◦ = ~teamdobj↓

The selectional preferences imposed by the
head word run to its direct object are represented
by the vector ~run◦, which is obtained by adding
the vectors of all those nouns (e.g. company,
project, marathon, etc) which are in relation dobj
with the verb run:

~run◦ =
∑

~v∈ R
~v

where R is the vector set of nouns playing the di-
rect object role of run (except team). R is included
in the subspace of nominal vectors.

Each multiplicative operation results in a
compositional vector of a contextualized word.
Component-wise multiplication has an intersec-
tive effect. The vector standing for the selectional
preferences restricts the vector of the target word
by assigning weight 0 to those WordNet features
that are not shared by both vectors. The new com-
positional vector as well as the two constituents all
belong to the same vector subspace (the subspace
of nouns, verbs, or adjectives).

Notice that, in approaches to computational
semantics inspired by Combinatory Categorial
Grammar (Steedman, 1996) and Montagovian se-
mantics (Montague, 1970), the interpretation pro-
cess for composite expressions such as “run a
team” or “electric coach” relies on rigid function-
argument structures: relational expressions, like
verbs and adjectives, are used as predicates while
nouns and nominals are their arguments. In the
composition process, each word is supposed to
play a rigid and fixed role: the relational word
is semantically represented as a selective func-
tion imposing constraints on the denotations of
the words it combines with, while non-relational
words are in turn seen as arguments filling the con-
straints imposed by the function. For instance, run

and electric would denote functions while team
and coach would be their arguments.

By contrast, we deny the rigid “predicate-
argument” structure. In our compositional ap-
proach, dependencies are the active functions that
control and rule the selectional requirements im-
posed by the two related words. Thus, each con-
stituent word imposes its selectional preferences
on the other within a dependency-based construc-
tion. This is in accordance with non-standard lin-
guistic research which assumes that the words in-
volved in a composite expression impose seman-
tic restrictions on each other (Pustejovsky, 1995;
Gamallo et al., 2005; Gamallo, 2008).

4.2 Recursive Compositional Application
In our approach, the consecutive application of the
syntactic dependencies found in a sentence is ac-
tually the process of building the contextualized
sense of all the lexical words which constitute it.
Thus, the whole sentence is not assigned to an
unique meaning (which could be the contextual-
ized sense of the root word), but one sense per
lemma, being the sense of the root just one of
them.

This incremental process may have two di-
rections: from left-to-right and vice versa (i.e.,
from right-to-left). Figure 1 illustrates the
incremental process of building the sense of
words dependency-by-dependency from left-to-
right. Thus, given the composite expression “the
coach runs the team” and its dependency analysis
depicted in the first row of the figure, two com-
positional processes are driven by the two depen-
dencies involved in the analysis (nsubj and dobj).
Each dependency is decomposed into two func-
tions: head (nsubj↑ and dobj↑) and dependent
(nsubj↓ and dobj↓) functions.2 The first compo-
sitional process applies, on the one hand, the head
function nsubj↑ on the denotation of the head verb
( ~run) and on the selectional preferences required
by coach ( ~coach◦), in order to build a contex-
tualized sense of the verb: ~runnsubj↑ . On the
other hand, the dependent function nsubj↓ builds
the sense of coach as nominal subject of run:
~coachnsubj↓. Then, the contextualized head vec-

tor is involved in the compositional process driven
by dobj. At this level of semantic composition, the
selectional preferences imposed on the noun team

2We do not consider the meaning of determiners, auxiliary
verbs, or tense affixes. Quantificational issues associated to
them are also beyond the scope of this work.

5



stand for the semantic features of all those nouns
which may be the direct object of coach+run. At
the end of the process, we have not obtained one
single sense for the whole expression, but one con-
textualized sense per lexical word: ~coachnsubj↓,
~runnsubj↑+dobj↑ and ~teamdobj↓.

In other case, from right-to-left, the verb run is
first restricted by team at the direct object position,
and then by its subject coach. In addition, this
noun is now restricted by the selectional prefer-
ences imposed by run and team, that is, it is com-
bined with the semantic features of all those nouns
that may be the nominal subject of run+team.

5 Experiments

We have performed several similarity-based ex-
periments using the semantic word model defined
in Section 3 and the compositional algorithm de-
scribed in 4.3 First, in Subsection 5.1, we evaluate
just word similarity without composition. Then,
in Subsection 5.2, we evaluate the simple compo-
sitional approach by making use of a dataset with
similar noun-verb pairs (NV constructions). Fi-
nally, the recursive application of compositional
functions is evaluated in Subsection 5.3, by mak-
ing use of a dataset with similar noun-verb-noun
pairs (NVN constructions).

In all experiments, we made use of datasets
suited for the task at hand, and compare our results
with those obtained by the best systems for the
corresponding dataset. Moreover, in order to build
the selectional preferences of the syntactically re-
lated words, we used the British National Corpus
(BNC). Syntactic analysis on BNC was performed
with the dependency parser DepPattern (Gamallo
and González, 2011; Gamallo, 2015), previously
PoS tagged with Tree-Tagger (Schmid, 1994).

5.1 Word Similarity

Recently, the use of word similarity methods has
been criticised as a reliable technique for evalu-
ating distributional semantic models (Batchkarov
et al., 2016), given the small size of the datasets
and the limitation of context information as well.
However, given this procedure still is widely ac-
cepted, we have performed two different kinds of
experiments: rating by similarity and synonym de-
tection with multiple-choice questions.

3Both the software and the semantic word model
are freely available at http://fegalaz.usc.es/
˜gamallo/resources/CompWordNet.tar.gz.

5.1.1 Rating by Similarity
In the first experiment, we use the WordSim353
dataset (Finkelstein et al., 2002), which was con-
structed by asking humans to rate the degree of se-
mantic similarity between two words on a numer-
ical scale. This is a small dataset with 353 word
pairs. The performance of a computational system
is measured in terms of correlation (Spearman) be-
tween the scores assigned by humans to the word
pairs and the similarity Dice coefficient assigned
by our system (WN) built with the WordNet-based
model space.

Table 1 compares the Spearman correlation ob-
tained by our model, WN, with that obtained by
the corpus-based system described in (Halawi et
al., 2012), which is the highest score reached so
far on that dataset. Even if our results were clearly
outperformed by that corpus-based method, WN
seems to behave well if compared with the state-
of-the-art knowledge-based (unsupervised) strat-
egy reported in (Agirre et al., 2009).

Systems ρ Method
WN 0.69 knowledge
(Hassan and Mihalcea, 2011) 0.62 knowledge
(Agirre et al., 2009) 0.66 knowledge
(Halawi et al., 2012) 0.81 corpus

Table 1: Spearman correlation between the Word-
Sim353 dataset and the rating obtained by our
knowledge-based system WN and the state-of-the-
art for both knowledge and corpus-based strate-
gies.

5.1.2 Synonym Detection with
Multiple-Choice Questions

In this evaluation task, a target word is presented
with four synonym candidates, one of them being
the correct synonym of the target. For instance,
for the target deserve, the system must choose be-
tween merit (the correct one), need, want, and
expect. Accuracy is the number of correct an-
swers divided by the total number of words in the

Systems Noun Adj Verb All
WN 0.85 0.85 0.75 0.80
(Freitag et al., 2005) 0.76 0.76 0.64 0.72
(Zhu, 2015) 0.71 0.71 0.63 0.69
(Kiela et al., 2015) - - - 0.88

Table 2: Accuracy of three systems on the WBST
test (synonym detection on nouns, adjectives, and
verbs)

6



coach run team

nsubj dobj

~coachnsubj↓ ~runnsubj↑ ~team

nsubj↑( ~run, ~coach◦), nsubj↓( ~run◦, ~coach)

~coachnsubj↓ ~runnsubj↑+dobj↑ ~teamdobj↓

dobj↑( ~runnsubj↑, ~team◦), dobj↓( ~coach + run◦, ~team)

Figure 1: Syntactic analysis of the expression “the coach runs the team” and left-to-right construction
of the word senses.

dataset.
The dataset is an extended TOEFL test, called

the WordNet-based Synonymy Test (WBST) pro-
posed in (Freitag et al., 2005). WBST was pro-
duced by generating automatically a large set
of TOEFL-like questions from the synonyms in
WordNet. In total, this procedure yields 9,887
noun, 7,398 verb, and 5,824 adjective questions,
a total of 23,509 questions, which is a very large
dataset. Table 2 shows the results. In this case, the
accuracy obtained by WN for the three syntactic
categories is close to state-of-the-art corpus-based
method for this task (Kiela et al., 2015), which is a
neural network trained with a huge corpus contain-
ing 8 billion words from English Wikipedia and
newswire texts.

5.2 Noun-Verb Composition
The first experiment aimed at evaluating our com-
positional strategy uses the test dataset by Mitchell
and Lapata (2008), which comprises a total of
3,600 human similarity judgments. Each item
consists of an intransitive verb and a subject noun,
which are compared to another noun-verb pair
(NV) combining the same noun with a synonym
of the verb that is chosen to be either similar o
dissimilar to the verb in the context of the given
subject. For instance, “child stray” is related to
“child roam”, being roam a synonym of stray.
The dataset was constructed by extracting NV
composite expressions from the British National
Corpus (BNC) and verb synonyms from WordNet.
In order to evaluate the results of the tested sys-
tems, Spearman correlation is computed between
individual human similarity scores and the sys-
tems’ predictions.

In this experiment, we compute the similarity
between the contextualized heads of two NV com-
posites and between their contextualized depen-
dent expressions. For instance, we compute the
similarity between “eye flare” vs “eye flame” by
comparing first the verbs flare and flame when
combined with eye in the subject position (head
function), and by comparing how (dis)similar is
the noun eye when combined with both the verbs
flare and flame (dependent function). In addition,
as we are provided with two similarities (head and
dep) for each pair of compared expressions, it is
possible to compute a new similarity score by av-
eraging the results of head and dependent func-
tions (head+dep).

Table 3 shows the Spearman’s correlation val-
ues (ρ) obtained by the three versions of WN:
only head function (head), only dependent func-
tion (dep) and average of both (head+dep). The
latter score value is comparable to the state-of-the-
art system for this dataset, reported in (Erk and
Padó, 2008). It is also very similar to the most re-
cent results described in (Dinu et al., 2013), where
the authors made use of the compositional strategy
defined in (Baroni and Zamparelli, 2010).

Systems ρ
WN (head+dep) 0.29
WN (head) 0.26
WN (dep) 0.14
(Erk and Padó, 2008) 0.27
(Dinu et al., 2013) 0.26

Table 3: Spearman correlation for intransitive ex-
pressions using the benchmark by Mitchell and
Lapata (2008)

7



5.3 Noun-Verb-Noun Composition

The last experiment consists in evaluating the
quality of compositional vectors built by means
of the consecutive application of head and de-
pendency functions associated with nominal sub-
ject and direct object. The experiment is per-
formed on the dataset developed in (Grefenstette
and Sadrzadeh, 2011a). The dataset was built us-
ing the same guidelines as Mitchell and Lapata
(2008), using transitive verbs paired with subjects
and direct objects: NVN composites.

Given our compositional strategy, we are able to
compositional build several vectors that somehow
represent the meaning of the whole NVN com-
posite expression. In order to known which is
the best compositional strategy and be exhaustive
and complete, we evaluate all of them; i.e., both
left-to-right and right-to-left strategies. Thus, take
again the expression “the coach runs the team”.
If we follow the left-to-right strategy (noted nv-n),
at the end of the compositional process, we obtain
two fully contextualized senses:

nv-n head The sense of the head run, as a result
of being contextualized first by the prefer-
ences imposed by the subject and then by the
preferences required by the direct object. We
note nv-n head) the final sense of the head in
a NVN composite expression following the
left-to-right strategy.

nv-n dep The sense of the object team, as a re-
sult of being contextualized by the prefer-
ences imposed by run previously combined
with the subject coach. We note nv-n dep the
final sense of the direct object in a NVN com-
posite expression following the left-to-right
strategy.

If we follow the right-to-left strategy (noted n-
vn), at the end of the compositional process, we
obtain two fully contextualized senses:

n-nv head The sense of the head run as a result of
being contextualized first by the preferences
imposed by the object and then by the sub-
ject.

n-nv dep The sense of the subject coach, as a
result of being contextualized by the prefer-
ences imposed by run previously combined
with the object team.

Systems ρ
WN (nv-n head+dep) 0.35
WN (nv-n head) 0.34
WN (nv-n dep) 0.13
WN (n-vn head+dep) 0.50
WN (n-vn head) 0.35
WN (n-vn dep) 0.44
WN (n-vn+nv-n) 0.47
(Grefenstette and Sadrzadeh, 2011b)* 0.28
(Van De Cruys et al., 2013)* 0.37
(Tsubaki et al., 2013)* 0.44
(Milajevs et al., 2014) 0.46
(Polajnar et al., 2015) 0.35
(Hashimoto et al., 2014) 0.48
(Hashimoto and Tsuruoka, 2015) 0.48
Human agreement 0.75

Table 4: Spearman correlation for transitive ex-
pressions using the benchmark by Grefenstette
and Sadrzadeh (2011)

Table 4 shows the Spearman’s correlation val-
ues (ρ) obtained by all the different versions built
from our model WN. The best score was achieved
by averaging the head and dependent similarity
values derived from the n-vn (right-to-left) strat-
egy. Let us note that, for NVN composite ex-
pressions, the left-to-right strategy seems to build
less reliable compositional vectors than the right-
to-left counterpart. Besides, the combination of
the two strategies (n-vn+nv-n) does not improve
the results of the best one (n-vn).4. The score val-
ues obtained by the different versions of the right-
to-left strategy outperform other systems for this
dataset (see results reported below in the table).
Our best strategy (ρ = 0.50) also outperforms the
neural network strategy described in (Hashimoto
and Tsuruoka, 2015), which achieved 0.48 with-
out considering extra linguistic information not in-
cluded in the dataset. The (ρ) scores for this task
are reported for averaged human ratings. This is
due to a disagreement in previous work regard-
ing which metric to use when reporting results.
We mark with asterisk those systems reporting (ρ)
scores based on non-averaged human ratings.

6 Conclusions

In this paper, we described a compositional model
based on WordNet features and dependency-based
functions on those features. It is a recursive pro-
posal since it can be repeated from left-to-right or
from right-to-left and the sense of each constituent
word is performed in a recursive way.

4n-vn+nv-n is computed by averaging the similarities of
both n-vn head+dep and nv-n head+dep

8



Our compositional model tackles the problem
of information scalability. This problem states
that the size of semantic representations should not
grow exponentially, but proportionally; and, infor-
mation must not be loss using fixed size of compo-
sitional vectors. In our approach, however, even if
the size of the compositional vectors is fixed, there
is no information loss since each word of the com-
posite expression is associated to a compositional
vector representing its context-sensitive sense. In
addition, the compositional vectors do not grow
exponentially since their size is fixed by the vector
space: they are all first-order (or direct) vectors.
Finally, the number of vectors increases in propor-
tion to the number of constituent words found in
the composite expression. So, both points are suc-
cessfully solved.

In future work, we will try to design a compo-
sitional model based on word semantic represen-
tations combining WordNet-based features with
syntactic-based distributional contexts as well as
extend our model to full sentences instead of the
simple ones described in this paper.

Acknowledgements

We would like to thank the anonymous review-
ers for helpful comments and suggestions. This
work was supported by a 2016 BBVA Founda-
tion Grant for Researchers and Cultural Creators,
and by project TELPARES (FFI2014-51978-C2-
1-R) and grant TIN2014-56633-C3-1-R, Ministry
of Economy and Competitiveness. It has received
financial support from the Consellerı́a de Cultura,
Educación e Ordenación Universitaria (Postdoc-
toral Training Grants 2016 and Centro Singular de
Investigación de Galicia accreditation 2016-2019,
ED431G/08) and the European Regional Develop-
ment Fund (ERDF).

References
Eneko Agirre, Enrique Alfonseca, Keith Hall, Jana

Kravalova, Marius Paşca, and Aitor Soroa. 2009.
A study on similarity and relatedness using distribu-
tional and wordnet-based approaches. In Proceed-
ings of Human Language Technologies: The 2009
Annual Conference of the North American Chap-
ter of the Association for Computational Linguistics,
NAACL ’09, pages 19–27, Stroudsburg, PA, USA.
Association for Computational Linguistics.

Marco Baroni and Roberto Zamparelli. 2010. Nouns
are vectors, adjectives are matrices: Represent-
ing adjective-noun constructions in semantic space.

In Proceedings of the 2010 Conference on Em-
pirical Methods in Natural Language Processing,
EMNLP’10, pages 1183–1193, Stroudsburg, PA,
USA.

Marco Baroni, Raffaella Bernardi, and Roberto Zam-
parelli. 2014. Frege in space: A program for com-
positional distributional semantics. LiLT, 9:241–
346.

Marco Baroni. 2013. Composition in distributional
semantics. Language and Linguistics Compass,
7:511–522.

Miroslav Batchkarov, Thomas Kober, Jeremy Reffin,
Julie Weeds, and David Weir. 2016. A critique of
word similarity as a method for evaluating distribu-
tional semantic models. In Proceedings of the ACL
Workshop on Evaluating Vector Space Representa-
tions for NLP, Berlin, Germany.

B. Coecke, M. Sadrzadeh, and S. Clark. 2010. Math-
ematical foundations for a compositional distribu-
tional model of meaning. Linguistic Analysis, 36(1-
4):345–384.

Georgiana Dinu and Mirella Lapata. 2010. Measur-
ing distributional similarity in context. In Proceed-
ings of the 2010 Conference on Empirical Methods
in Natural Language Processing, pages 1162–1172,
Cambridge, MA, October. Association for Compu-
tational Linguistics.

G. Dinu, N. Pham, and M. Baroni. 2013. General
estimation and evaluation of compositional distribu-
tional semantic models. In ACL 2013 Workshop on
Continuous Vector Space Models and their Compo-
sitionality (CVSC 2013), pages 50–58, East Strouds-
burg PA.

Katrin Erk and Sebastian Padó. 2008. A structured
vector space model for word meaning in context. In
Proceedings of EMNLP, Honolulu, HI.

Manaal Faruqui and Chris Dyer. 2015. Non-
distributional word vector representations. In Pro-
ceedings of ACL.

Lev Finkelstein, Evgeniy Gabrilovich, Yossi Matias,
Ehud Rivlin, Zach Solan, Gadi Wolfman, and Ey-
tan Ruppin. 2002. Placing search in context: the
concept revisited. ACM Trans. Inf. Syst., 20(1):116–
131.

Dayne Freitag, Matthias Blume, John Byrnes, Ed-
mond Chow, Sadik Kapadia, Richard Rohwer, and
Zhiqiang Wang. 2005. New experiments in distribu-
tional representations of synonymy. In Proceedings
of the Ninth Conference on Computational Natural
Language Learning, pages 25–32.

Pablo Gamallo and Isaac González. 2011. A grammat-
ical formalism based on patterns of part-of-speech
tags. International Journal of Corpus Linguistics,
16(1):45–71.

9



Pablo Gamallo, Alexandre Agustini, and Gabriel
Lopes. 2005. Clustering Syntactic Positions with
Similar Semantic Requirements. Computational
Linguistics, 31(1):107–146.

Pablo Gamallo. 2008. The meaning of syntactic de-
pendencies. Linguistik OnLine, 35(3):33–53.

Pablo Gamallo. 2015. Dependency parsing with com-
pression rules. In International Workshop on Pars-
ing Technology (IWPT 2015), Bilbao, Spain.

Pablo Gamallo. 2017. The role of syntactic dependen-
cies in compositional distributional semantics. Cor-
pus Linguistics and Linguistic Theory.

Edward Grefenstette and Mehrnoosh Sadrzadeh.
2011a. Experimental support for a categorical com-
positional distributional model of meaning. In Con-
ference on Empirical Methods in Natural Language
Processing.

Edward Grefenstette and Mehrnoosh Sadrzadeh.
2011b. Experimenting with transitive verbs in a dis-
cocat. In Workshop on Geometrical Models of Nat-
ural Language Semantics (EMNLP-2011).

Edward Grefenstette, Mehrnoosh Sadrzadeh, Stephen
Clark, Bob Coecke, and Stephen Pulman. 2011.
Concrete sentence spaces for compositional distri-
butional models of meaning. In Proceedings of the
Ninth International Conference on Computational
Semantics, IWCS ’11, pages 125–134.

Emiliano Guevara. 2010. A regression model of
adjective-noun compositionality in distributional se-
mantics. In Proceedings of the 2010 Workshop on
GEometrical Models of Natural Language Seman-
tics, GEMS ’10.

M. A. Hadj Taieb, M. Ben Aouicha, and
A. Ben Hamadou. 2014. Ontology-based approach
for measuring semantic similarity. Engineering
Applications of Artificial Intelligence, 36:238–261.

Guy Halawi, Gideon Dror, Evgeniy Gabrilovich, and
Yehuda Koren. 2012. Large-scale learning of word
relatedness with constraints. In Proceedings of the
18th ACM SIGKDD International Conference on
Knowledge Discovery and Data Mining, KDD ’12,
pages 1406–1414.

Kazuma Hashimoto and Yoshimasa Tsuruoka. 2015.
Learning embeddings for transitive verb disam-
biguation by implicit tensor factorization. In Pro-
ceedings of the 3rd Workshop on Continuous Vector
Space Models and their Compositionality, pages 1–
11, Beijing, China, July. Association for Computa-
tional Linguistics.

Kazuma Hashimoto, Pontus Stenetorp, Makoto Miwa,
and Yoshimasa Tsuruoka. 2014. Jointly learn-
ing word representations and composition functions
using predicate-argument structures. In Proceed-
ings of the 2014 Conference on Empirical Methods
in Natural Language Processing (EMNLP), pages

1544–1555, Doha, Qatar, October. Association for
Computational Linguistics.

S. Hassan and R. Mihalcea. 2011. Semantic related-
ness using salient semantic analysis. In Proceedings
of AAAI Conference on Artificial Intelligence.

Douwe Kiela, Felix Hill, and Stephen Clark. 2015.
Specializing word embeddings for similarity or re-
latedness. In Lluı́s Màrquez, Chris Callison-Burch,
Jian Su, Daniele Pighin, and Yuval Marton, edi-
tors, EMNLP, pages 2044–2048. The Association
for Computational Linguistics.

Jayant Krishnamurthy and Tom Mitchell, 2013. Pro-
ceedings of the Workshop on Continuous Vector
Space Models and their Compositionality, chapter
Vector Space Semantic Parsing: A Framework for
Compositional Vector Space Models, pages 1–10.
Association for Computational Linguistics.

Lingling Meng, Runquing Huang, and Junzhong Gu.
2013. A review of semantic similarity measures in
wordnet. International Journal of Hybrid Informa-
tion Technology, 6(1).

Dmitrijs Milajevs, Dimitri Kartsaklis, Mehrnoosh
Sadrzadeh, and Matthew Purver. 2014. Evaluating
neural word representations in tensor-based compo-
sitional settings. In Alessandro Moschitti, Bo Pang,
and Walter Daelemans, editors, EMNLP, pages 708–
719. ACL.

Jeff Mitchell and Mirella Lapata. 2008. Vector-based
models of semantic composition. In Proceedings of
ACL-08: HLT, pages 236–244.

Jeff Mitchell and Mirella Lapata. 2009. Language
models based on semantic composition. In Proceed-
ings of EMNLP, pages 430–439.

Jeff Mitchell and Mirella Lapata. 2010. Composition
in distributional models of semantics. Cognitive Sci-
ence, 34(8):1388–1439.

Richard Montague. 1970. Universal grammar. theoria.
Theoria, 36:373–398.

Barbara Partee. 1984. Compositionality. In Frank
Landman and Frank Veltman, editors, Varieties
of Formal Semantics, pages 281–312. Dordrecht:
Foris.

Tamara Polajnar, Laura Rimell, and Stephen Clark.
2015. An exploration of discourse-based sentence
spaces for compositional distributional semantics.
In Proceedings of the First Workshop on Linking
Computational Models of Lexical, Sentential and
Discourse-level Semantics, pages 1–11, Lisbon, Por-
tugal, September. Association for Computational
Linguistics.

James Pustejovsky. 1995. The Generative Lexicon.
MIT Press, Cambridge.

10



R. Rada, H. Mili, E. Bicknell, and M. Blettner. 1989.
Development and application of a metric on se-
mantic nets. Systems, Man and Cybernetics, IEEE
Transactions on, 19(1):17–30.

Siva Reddy, Ioannis P. Klapaftis, Diana McCarthy, and
Suresh Manandhar. 2011. Dynamic and static pro-
totype vectors for semantic composition. In Fifth In-
ternational Joint Conference on Natural Language
Processing, IJCNLP 2011, Chiang Mai, Thailand,
November 8-13, 2011, pages 705–713.

M. Andrea Rodrı́guez and Max J. Egenhofer. 2003.
Determining semantic similarity among entity
classes from different ontologies. IEEE Trans.
Knowl. Data Eng., 15(2):442–456.

H. Schmid. 1994. Probabilistic part-of-speech tagging
using decision trees. In International Conference on
New Methods in Language Processing.

Thabet Slimani. 2013. Description and evaluation of
semantic similarity measures approaches. Interna-
tional Journal of Computer Applications, 80(1):25–
33.

Mark Steedman. 1996. Surface Structure and Inter-
pretation. The MIT Press.

Stefan Thater, Hagen Fürstenau, and Manfred Pinkal.
2010. Contextualizing semantic representations us-
ing syntactically enriched vector models. In Pro-
ceedings of the 48th Annual Meeting of the Associa-
tion for Computational Linguistics, pages 948–957,
Stroudsburg, PA, USA.

Masashi Tsubaki, Kevin Duh, Masashi Shimbo, and
Yuji Matsumoto. 2013. Modeling and learning se-
mantic co-compositionality through prototype pro-
jections and neural networks. In EMNLP, pages
130–140. ACL.

Peter D. Turney. 2013. Domain and function: A dual-
space model of semantic relations and compositions.
Journal of Artificial Intelligence Research (JAIR),
44:533–585.

A. Tversky. 1977. Features of similarity. Psichologi-
cal Review, 84(4).

Tim Van De Cruys, Thierry Poibeau, and Anna Ko-
rhonen. 2013. A Tensor-based Factorization Model
of Semantic Compositionality. In Conference of the
North American Chapter of the Association of Com-
putational Linguistics (HTL-NAACL), pages 1142–
1151, Atlanta, United States, June.

Fabio Massimo Zanzotto, Ioannis Korkontzelos,
Francesca Fallucchi, and Suresh Manandhar. 2010.
Estimating linear models for compositional distribu-
tional semantics. In Proceedings of the 23rd Inter-
national Conference on Computational Linguistics,
COLING ’10, pages 1263–1271.

Peng Zhu. 2015. N-grams based linguistic search en-
gine. International Journal of Computational Lin-
guistics Research, 6(1):1–7.

11


