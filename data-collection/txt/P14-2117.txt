



















































Semantic Consistency: A Local Subspace Based Method for Distant Supervised Relation Extraction


Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 718–724,
Baltimore, Maryland, USA, June 23-25 2014. c©2014 Association for Computational Linguistics

Semantic Consistency: A Local Subspace Based Method for Distant 

Supervised Relation Extraction 
 

 Xianpei Han    and       Le Sun 

State Key Laboratory of Computer Science 

Institute of Software, Chinese Academy of Sciences 

HaiDian District, Beijing, China. 

{xianpei, sunle}@nfs.iscas.ac.cn 

  

Abstract 

One fundamental problem of distant supervi-

sion is the noisy training corpus problem. In 

this paper, we propose a new distant supervi-

sion method, called Semantic Consistency, 

which can identify reliable instances from 

noisy instances by inspecting whether an in-

stance is located in a semantically consistent 

region. Specifically, we propose a semantic 

consistency model, which first models the lo-

cal subspace around an instance as a sparse 

linear combination of training instances, then 

estimate the semantic consistency by exploit-

ing the characteristics of the local subspace. 

Experimental results verified the effectiveness 

of our method. 

1 Introduction 

Relation extraction aims to identify and categorize 

relations between pairs of entities in text. Due to 

the time-consuming annotation process, one criti-

cal challenge of relation extraction is the lack of 

training data. To address this limitation, a promis-

ing approach is distant supervision (DS), which 

can automatically gather labeled data by heuristi-

cally aligning entities in text with those in a 

knowledge base (Mintz et al., 2009). The under-

lying assumption of distant supervision is that 

every sentence that mentions two entities is likely 

to express their relation in a knowledge base. 

Relation Instance Label 

S1: Jobs was the founder of Apple Founder-of, CEO-of 

S2: Jobs joins Apple Founder-of, CEO-of 

Figure 1. Labeled instances by distant supervi-

sion, using relations CEO-of(Steve Jobs, Apple 

Inc.) and Founder-of(Steve Jobs, Apple Inc.) 

The distant supervision assumption, unfortu-

nately, can often fail and result in a noisy training 

corpus. For example, in Figure 1 DS assumption 

will wrongly label S1 as a CEO-of instance and S2 

as instance of Founder-of and CEO-of. The noisy 

training corpus in turn will lead to noisy extrac-

tions that hurt extraction accuracy (Riedel et al., 

2010). 

 

Figure 2. The regions the two instances in Figure 

1 located, where: 1) S1 locates in a semantically 

consistent region; and 2) S2 locates in a semanti-

cally inconsistent region 

To resolve the noisy training corpus problem, 

this paper proposes a new distant supervision 

method, called Semantic Consistency, which can 

effectively identify reliable instances from noisy 

instances by inspecting whether an instance is lo-

cated in a semantically consistent region. Figure 2 

shows two intuitive examples. We can see that, 

semantic consistency is an effective way to iden-

tify reliable instances. For example, in Figure 2 S1 

is highly likely a reliable Founder-of instance be-

cause its neighbors are highly semantically con-

sistent, i.e., most of them express the same rela-

tion type – Founder-of. On contrast S2 is highly 

likely a noisy instance because its neighbors are 

semantically inconsistent, i.e., they have a diverse 

relation types. The problem now is how to model 

the semantic consistency around an instance. 

To model the semantic consistency, this paper 

proposes a local subspace based method. Specifi-

cally, given sufficient training instances, our 

method first models each relation type as a linear 

subspace spanned by its training instances. Then, 

the local subspace around an instance is modeled 

and characterized by seeking the sparsest linear 

combination of training instances which can re-

construct the instance. Finally, we estimate the se-

mantic consistency of an instance by exploiting 

the characteristics of its local subspace. 

+

+

+

+

+
×

××

×
×

S2
× S1 ×

+:  CEO-of

×:  Founder-of

+

×

+

×
× ×

+    :  Manager-of

   :  CTO-of

718



This paper is organized as follows. Section 2 

reviews related work. Section 3 describes the pro-

posed method. Section 4 presents the experiments. 

Finally Section 5 concludes this paper. 

2 Related Work 

This section briefly reviews the related work. Cra-

ven and Kumlien (1999), Wu et al. (2007) and 

Mintz et al.(2009) were several pioneer work of 

distant supervision. One main problem of DS as-

sumption is that it often will lead to false positives 

in training data. To resolve this problem, Bunescu 

and Mooney (2007), Riedel et al. (2010) and Yao 

et al. (2010) relaxed the DS assumption to the at-

least-one assumption and employed multi-in-

stance learning techniques to identify wrongly la-

beled instances. Takamatsu et al. (2012) proposed 

a generative model to eliminate noisy instances. 

Another research issue of distant supervision is 

that a pair of entities may participate in more than 

one relation. To resolve this problem, Hoffmann 

et al. (2010) proposed a method which can com-

bine a sentence-level model with a corpus-level 

model to resolve the multi-label problem. 

Surdeanu et al. (2012) proposed a multi-instance 

multi-label learning approach which can jointly 

model all instances of an entity pair and all their 

labels. Several other research issues also have 

been addressed. Xu et al. (2013), Min et al. (2013) 

and Zhang et al. (2013) try to resolve the false 

negative problem raised by the incomplete 

knowledge base problem. Hoffmann et al. (2010) 

and Zhang et al. (2010) try to improve the extrac-

tion precision by learning a dynamic lexicon. 

3 The Semantic Consistency Model for 
Relation Extraction 

In this section, we describe our semantic con-

sistency model for relation extraction. We first 

model the subspaces of all relation types in the 

original feature space, then model and character-

ize the local subspace around an instance, finally 

estimate the semantic consistency of an instance 

and exploit it for relation extraction. 

3.1 Testing Instance as a Linear Combina-
tion of Training Instances 

In this paper, we assume that there exist k distinct 

relation types of interest and each relation type is 

represented with an integer index from 1 to k. For 

ith relation type, we assume that totally ni training 

instances Vi = fvi;1;vi;2; :::;vi;nigVi = fvi;1;vi;2; :::;vi;nig  have been 

collected using DS assumption. And each instance 

is represented as a weighted feature vector, such 

as the features used in (Mintz, 2009) or (Surdeanu 

et al., 2012), with each feature is TFIDF weighted 

by taking each instance as an individual document. 

To model the subspace of ith relation type in 

the original feature space, a variety of models 

have been proposed to discover the underlying 

patterns of Vi. In this paper, we make a simple and 

effective assumption that the instances of a single 

relation type can be represented as the linear 

combination of other instances of the same rela-

tion type. This assumption is well motived in rela-

tion extraction, because although there is nearly 

unlimited ways to express a specific relation, in 

many cases basic principles of economy of ex-

pression and/or conventions of genre will ensure 

that certain systematic ways will be used to ex-

press a specific relation (Wang et al., 2012). For 

example, as shown in (Hearst, 1992), the IS-A re-

lation is usually expressed using several regular 

patterns, such as “such NP as {NP ,}* {(or | and)} 

NP” and “NP {, NP}* {,} or other NP”. 

Based on the above assumption, we hold many 

instances for each relation type and directly use 

these instances to model the subspace of a relation 

type. Specifically, we represent an instance y of 

ith type as the linear combination of training in-

stances associated with ith type: 

y = ®i;1vi;1 + ®i;2vi;2 + ::: ++®i;nivi;niy = ®i;1vi;1 + ®i;2vi;2 + ::: ++®i;nivi;ni   (1) 

for some scalars , with j = 1, 2, …,ni. For ex-

ample, we can represent the CEO-of instance 

“Jobs was the CEO of Apple” as the following lin-

ear combination of CEO-of instances: 

 0.8: Steve Ballmer is the CEO of Microsoft 
 0.2: Rometty was served as the CEO of IBM 
For simplicity, we arrange the given ni training in-

stances of ith relation type as columns of a matrix 

Ai = [vi;1;vi;2; :::;vi;ni]Ai = [vi;1;vi;2; :::;vi;ni], then we can write the 

matrix form of Formula 1 as: 

y = Aixiy = Aixi                          (2) 

where xi = [®i;1; :::; ®i;ni]xi = [®i;1; :::; ®i;ni] is the coefficient vec-

tor. In this way, the subspace of a relation type is 

the linear subspace spanned by its training in-

stances, and if we can find a valid xi, we can ex-

plain y as a valid instance of ith relation type. 

3.2 Local Subspace Modeling 
via Sparse Representation 

Based on the above model, the local subspace of 

an instance is modeled as the linear combination 

of training instances which can reconstruct the in-

stance. Specifically, to model the local subspace, 

we first concatenate the n training instances of all 

k relation types: 

A = [A1;A2; :::; Ak]A = [A1;A2; :::; Ak] 

719



Then the local subspace around y is modeled by 

seeking the solution of the following formula: 

y = Axy = Ax                          (3) 

However, because of the redundancy of train-

ing instances, Formula 3 usually has more than 

one solution. In this paper, following the idea in 

(Wright et al., 2009) for robust face recognition, 

we use the sparsest solution (i.e., how to recon-

struct an instance using minimal training in-

stances), which have been shown is both discrimi-

nant and robust to noisiness. Concretely, we seek 

the sparse linear combination of training instances 

to reconstruct y by solving: 

(l1) : x¤ = arg min kxk1 s.t. kAx¡yk2 · "(l
1) : x¤ = arg min kxk1 s.t. kAx¡yk2 · "  (4) 

where x= [®1;1; :::;®1;n1; :::;®i;1;®i;2; :::;®i;ni; :::]x= [®1;1; :::;®1;n1; :::;®i;1;®i;2; :::;®i;ni; :::] 

is a coefficient vector which identifies the span-

ning instances of y’s local subspace, i.e., the in-

stances whose 𝛼𝑖,𝑗 ≠ 0 . In practice, the training 

corpus may be too large to direct solve Formula 4. 

Therefore, this paper uses the K-Nearest-Neigh-

bors (KNN) of y (1000 nearest neighbors in this 

paper) to construct the training instance matrix A 

for each y, and KNN can be searched very effi-

ciently using specialized algorithms such as the 

LSH functions in (Andoni & Indyk, 2006). 

Through the above semantic decomposition, 

we can see that, the entries of x can encode the 

underlying semantic information of instance y. 

For ith relation type, let  be a new vector 

whose only nonzero entries are the entries in x that 

are associated with ith relation type, then we can 

compute the semantic component corresponding 

to ith relation type as . In this way a 

testing instance y will be decomposed into k se-

mantic components, with each component corre-

sponds to one relation type (with an additional 

noise component ): 

y = y1 + :::+yi + :::+yk + ²y = y1 + :::+yi + :::+yk + ²        (5) 

S1 = 0:8£

2

6
6
4

was

co-founder

of

...

3

7
7
5 + 0:2£

2

6
6
4

Jobs

Apple

the

...

3

7
7
5S1 = 0:8£

2

6
6
4

was

co-founder

of

...

3

7
7
5 + 0:2£

2

6
6
4

Jobs

Apple

the

...

3

7
7
5 

 

S2 = 0:1

·
join

...

¸

+ 0:1

·
join

...

¸

+ 0:1

·
join

...

¸

+ ...S2 = 0:1

·
join

...

¸

+ 0:1

·
join

...

¸

+ 0:1

·
join

...

¸

+ ... 

Figure 3. The semantic decomposition of the two 

instances in Figure 1 

Figure 3 shows an example of semantic decom-

position. We can see that, the semantic decompo-

sition can effectively summarize the semantic 

consistency information of y’s local subspace: if 

the instances around an instance have diverse re-

lation types (S2 for example), its information will 

be scattered on many different semantic compo-

nents. On contrast if the instances around an in-

stance have consistent relation types (S1 for ex-

ample), most of its information will concentrate 

on the corresponding relation type. 

3.3 Semantic Consistency based 
Relation Extraction 

This section describes how to estimate and exploit 

the semantic consistency for relation extraction. 

Specifically, given y’s semantic decomposition: 
y = y1 + :::+yi + :::+yk + ²y = y1 + :::+yi + :::+yk + ² 

we observe that if instance y locates at a semantic 

consistent region, then all its information will con-

centrate on a specific component yi, with all other 

components equal to zero vector 0. However, 

modeling errors, expression ambiguity and noisy 

features will lead to small nonzero components. 

Based on the above discussion, we define the se-

mantic consistency of an instance as the semantic 

concentration degree of its decomposition: 

Definition 1(Semantic Consistency). For an in-

stance y, its semantic consistency with ith relation 

type is: 

Consistency(y; i) =
kyik2

P
i kyik2 + k²k2

Consistency(y; i) =
kyik2

P
i kyik2 + k²k2

 

where Consistency(y, i)  and will be 1.0 if 

all information of y is consistent with ith relation 

type; on contrast it will be 0 if no information in y 

is consistent with ith relation type. 

Semantic Consistency based Relation Ex-

traction. To get accurate extractions, we deter-

mine the relation type of y based on both: 1) How 

much information in y is related to ith type; and 2) 

its semantic consistency score with ith type, i.e., 

whether y is a reliable instance of ith type. 

To measure how much information in y is re-

lated to ith relation type, we compute the propor-

tion of common information between y and yi: 

sim(y;yi) =
y ¢ yi

y ¢ y
sim(y;yi) =

y ¢ yi

y ¢ y
                    (6) 

Then the likelihood for a testing instance y ex-

pressing ith relation type is scored by summariz-

ing both its information and semantic consistency: 
rel(y; i) = sim(y;yi)£Consistency(y; i)rel(y; i) = sim(y;yi)£Consistency(y; i) 

and y will be classified into ith relation type if its 

likelihood is larger than a threshold: 

rel(y; i) ¸ ¿irel(y; i) ¸ ¿i                      (7) 

where  is a relation type specific threshold 

learned from training dataset. 

Founder-of CEO-of 

Founder-of noise 

CTO-of 

720



Multi-Instance Evidence Combination. It is 

often that an entity pair will match more than one 

sentence. To exploit such redundancy for more 

confident extraction, this paper first combines the 

evidence from different instances by combing 

their underlying components. That is, given the 

matched m instances Y={y1, y2, …, ym} for an en-

tity pair (e1, e2), we first decompose each instance 

as yj = y
j
1 + ::: + y

j
k + ²y

j = y
j
1 + ::: + y

j
k + ² , then the entity-pair 

level decomposition y = y1 + :::+yk + ²y = y1 + :::+yk + ² is ob-

tained by summarizing semantic components of 

different instances: yi =
P

1·j·m y
j
iyi =

P
1·j·m y

j
i . Finally, the 

likelihood of an entity pair expressing ith relation 

type is scored as: 

rel(Y; i) = sim(y;yi)Consistency(y; i)log(m+1)rel(Y; i) = sim(y;yi)Consistency(y; i)log(m+1) 

where  is a score used to encourage 

extractions with more matching instances. 

3.4 One further Issue for Distant Supervi-
sion: Training Instance Selection 

The above model further provides new insights 

into one issue for distant supervision: training in-

stance selection. In this paper, we select informa-

tive training instances by seeking a most compact 

subset of instances which can span the whole sub-

space of a relation type. That is, all instances of 

ith type can be represented as a linear combination 

of these selected instances. 

However, finding the optimal subset of training 

instances is difficult, as there exist 2N possible so-

lutions for a relation type with N training instances. 

Therefore, this paper proposes an approximate 

training instance selection algorithm as follows: 

1) Computing the centroid of ith relation type as                
vi =

P
1·j·ni

vi;jvi =
P

1·j·ni
vi;j 

2) Finding the set of training instances which 
can most compactly span the centroid by 

solving: 

(l1) : xi = arg min kxk1 s.t. kAix¡ vik2 · "(l
1) : xi = arg min kxk1 s.t. kAix¡ vik2 · " 

3) Ranking all training instances according to 

their absolute coefficient weight value ; 

4) Selecting top p percent ranked instances as 
final training instances. 

The above training instance selection has two 

benefits. First, it will select informative instances 

and remove redundant instances: an informative 

instance will receive a high  value because 

many other instances can be represented using it; 

and if two instances are redundant, the sparse so-

lution will only retain one of them. Second, most 

of the wrongly labeled training instances will be 

filtered, because these instances are usually not 

regular expressions of ith type, so they appear 

only a few times and will receive a small . 

4 Experiments 

In this section, we assess the performance of our 

method and compare it with other methods. 

Dataset. We assess our method using the KBP 

dataset developed by Surdeanu et al. (2012). The 

KBP is constructed by aligning the relations from 

a subset of English Wikipedia infoboxes against a 

document collection that merges two distinct 

sources: (1) a 1.5 million documents collection 

provided by the KBP shared task(Ji et al., 2010; Ji 

et al., 2011); and (2) a complete snapshot of the 

June 2010 version of Wikipedia. Totally 183,062 

training relations and 3,334 testing relations are 

collected. For tuning and testing, we used the 

same partition as Surdeanu et al. (2012): 40 que-

ries for development and 160 queries for formal 

evaluation. In this paper, each instance in KBP is 

represented as a feature vector using the features 

as the same as in (Surdeanu et al., 2012). 

Baselines. We compare our method with four 

baselines as follows: 

 Mintz++. This is a traditional DS assump-
tion based model proposed by Mintz et al.(2009).  

 Hoffmann. This is an at-least-one as-
sumption based multi-instance learning method 

proposed by Hoffmann et al. (2011). 

 MIML. This is a multi-instance multi-la-
bel model proposed by Surdeanu et al. (2012).  

 KNN. This is a classical K-Nearest-
Neighbor classifier baseline. Specifically, given 

an entity pair, we first classify each matching in-

stance using the labels of its 5 (tuned on training 

corpus) nearest neighbors with cosine similarity, 

then all matching instances’ classification results 

are added together. 

Evaluation. We use the same evaluation set-

tings as Surdeanu et al. (2012). That is, we use the 

official KBP scorer with two changes: (a) relation 

mentions are evaluated regardless of their support 

document; and (b) we score only on the subset of 

gold relations that have at least one mention in 

matched sentences. For evaluation, we use 

Mintz++, Hoffmann, and MIML implementation 

from Stanford’s MIMLRE package (Surdeanu et 

al., 2012) and implement KNN by ourselves. 

4.1 Experimental Results 

4.1.1 Overall Results 
We conduct experiments using all baselines and 

our semantic consistency based method. For our 

721



method, we use top 10% weighted training in-

stances. All features occur less than 5 times are 

filtered. All l1-minimization problems in this pa-

per are solved using the augmented Lagrange 

multiplier algorithm (Yang et al., 2010), which 

has been proven is accurate, efficient, and robust. 

To select the classification threshold  for ith re-

lation type, we use the value which can achieve 

the best F-measure on training dataset (with an ad-

ditional restriction that precision should > 10%). 

 
Figure 4. Precision/recall curves in KBP dataset 

System Precision Recall F1 

Mintz++ 0.260 0.250 0.255 

Hoffmann 0.306 0.198 0.241 

MIML 0.249 0.314 0.278 

KNN 0.261 0.295 0.277 

Our method 0.286 0.342 0.311 

Table 1.  The best F1-measures in KBP dataset 

Figure 4 shows the precision/recall curves of 

different systems, and Table 1 shows their best 

F1-measures. From these results, we can see that: 

1) The semantic consistency based method 
can achieve robust and competitive performance: 

in KBP dataset, our method correspondingly 

achieves 5.6%, 7%, 3.3% and 3.4% F1 improve-

ments over the Mintz++, Hoffmann, MIML and 

KNN baselines. We believe this verifies that the 

semantic consistency around an instance is an ef-

fective way to identify reliable instances. 

2) From Figure 4 we can see that our method 
achieves a consistent improvement on the high-re-

call region of the KBP curves (when recall > 0.1). 

We believe this is because by modeling the se-

mantic consistency using the local subspace 

around each testing instance, our method can bet-

ter solve the classification of long tail instances 

which are not expressed using salient patterns. 

3) The local subspace around an instance 
can be effectively modeled as a linear subspace 

spanned by training instances. From Table 1 we 

can see that both our method and KNN baseline 

(where the local subspace is spanned using its k 

nearest neighbors) achieve competitive perfor-

mance: even the simple KNN baseline can achieve 

a competitive performance (0.277 in F1). This re-

sult shows: a) the effectiveness of instance-based 

subspace modeling; and b) by partitioning sub-

space into many local subspaces, the subspace 

model is more adaptive and robust to model prior. 

4) The sparse representation is an effective 
way to model the local subspace using training in-

stances. Compared with KNN baseline, our 

method can achieve a 3.4% F1 improvement. We 

believe this is because: (1) the discriminative na-

ture of sparse representation as shown in (Wright 

et al., 2009); and (2) the sparse representation 

globally seeks the combination of training in-

stances to characterize the local subspace, on con-

trast KNN uses only its nearest neighbor in the 

training data, which is more easily affected by 

noisy training instances(e.g., false positives). 

4.1.2 Training Instance Selection Results 

To demonstrate the effect of training instance se-

lection, Table 2 reports our method’s performance 

using different proportions of training instances. 

Proportion 5% 10% 20% 100% 

Best F1 0.282 0.311 0.305 0.280 

Table 2. The best F1-measures using different 

proportions of top weighted training instances 

From Table 2, we can see that: ① Our training in-
stance selection algorithm is effective: our method 

can achieve performance improvement using only 

top weighted instances. ② The training instances 
are highly redundant: using only 10% weighted 

instances can achieve a competitive performance. 

5 Conclusion and Future Work 

This paper proposes a semantic consistency 

method, which can identify reliable instances 

from noisy instances for distant supervised rela-

tion extraction. For future work, we want to de-

sign a more effective instance selection algorithm 

and embed it into our extraction framework. 

Acknowledgments 

This work is supported by the National Natural 

Science Foundation of China under Grants no. 

61100152 and 61272324, and the National High 

Technology Development 863 Program of China 

under Grants no. 2013AA01A603. 

722



Reference 

Andoni, Alexandr, and Piotr Indyk . 2006. Near-opti-

mal hashing algorithms for approximate nearest 

neighbor in high dimensions. In: Foundations of 

Computer Science, 2006,  pp. 459-468. 

Bunescu, Razvan, and Raymond Mooney. 2007. 

Learning to extract relations from the web using 

minimal supervision. In: ACL 2007, pp. 576. 

Craven, Mark, and Johan Kumlien. 1999. Constructing 

biological knowledge bases by extracting infor-

mation from text sources. In : Proceedings of AAAI 

1999. 

Downey, Doug, Oren Etzioni, and Stephen Soderland. 

2005. A probabilistic model of redundancy in infor-

mation extraction, In: Proceeding of IJCAI 2005. 

Gupta, Rahul, and Sunita Sarawagi. 2011. Joint train-

ing for open-domain extraction on the web: exploit-

ing overlap when supervision is limited. In: Pro-

ceedings of WSDM 2011, pp. 217-226. 

Hearst, Marti A. 1992. Automatic acquisition of hypo-

nyms from large text corpora. In: Proceedings of 

COLING 1992, pp. 539-545. 

Hoffmann, Raphael, Congle Zhang, and Daniel S. 

Weld. 2010. Learning 5000 relational extractors. In: 

Proceedings of the 48th Annual Meeting of the As-

sociation for Computational Linguistics, 2010, pp. 

286-295. 

Hoffmann, Raphael, Congle Zhang, Xiao Ling, Luke 

Zettlemoyer, and Daniel S. Weld. 2011. Knowledge-

based weak supervision for information extraction 

of overlapping relations. In: Proceedings of ACL 

2011, pp. 541-550. 

Ji, Heng, Ralph Grishman, Hoa Trang Dang, Kira Grif-

fitt, and Joe Ellis. 2010. Overview of the TAC 2010 

knowledge base population track. In: Proceedings 

of the Text Analytics Conference. 

Ji, Heng, Ralph Grishman, Hoa Trang Dang, Kira Grif-

fitt, and Joe Ellis. 2011. Overview of the TAC 2011 

knowledge base population track.  In Proceedings of 

the Text Analytics Conference. 

Krause, Sebastian, Hong Li, Hans Uszkoreit, and Feiyu 

Xu. 2012. Large-Scale learning of relation-extrac-

tion rules with distant supervision from the web. In: 

ISWC 2012, pp. 263-278. 

Mintz, Mike, Steven Bills, Rion Snow, and Dan Juraf-

sky. 2009. Distant supervision for relation extrac-

tion without labeled data. In: Proceedings ACL- 

AFNLP 2009, pp. 1003-1011. 

Min, Bonan, Ralph Grishman, Li Wan, Chang Wang, 

and David Gondek. 2013. Distant Supervision for 

Relation Extraction with an Incomplete Knowledge 

Base. In: Proceedings of NAACL-HLT 2013,pp. 

777-782. 

Min, Bonan, Xiang Li, Ralph Grishman, and Ang Sun. 

2012. New york university 2012 system for kbp slot 

filling. In: Proceedings of TAC 2012. 

Nguyen, Truc-Vien T., and Alessandro Moschitti. 

2011. Joint distant and direct supervision for rela-

tion extraction. In: Proceedings of IJCNLP 2011, pp. 

732-740. 

Riedel, Sebastian, Limin Yao, and Andrew McCallum. 

2010. Modeling relations and their mentions with-

out labeled text. In: Machine Learning and 

Knowledge Discovery in Databases, 2010, pp. 148-

163. 

Riedel, Sebastian, Limin Yao, Andrew McCallum, and 

Benjamin M. Marlin. 2013. Relation Extraction 

with Matrix Factorization and Universal Schemas. 

In: Proceedings of NAACL-HLT 2013, pp. 74-84. 

Roth, Benjamin, and Dietrich Klakow. 2013. Combin-

ing Generative and Discriminative Model Scores for 

Distant Supervision. In: Proceedings of ACL 2013, 

pp. 24-29. 

Surdeanu, Mihai, Julie Tibshirani, Ramesh Nallapati, 

and Christopher D. Manning. 2012. Multi-instance 

multi-label learning for relation extraction. In: Pro-

ceedings of EMNLP-CoNLL 2012, pp. 455-465. 

Takamatsu, Shingo, Issei Sato, and Hiroshi Nakagawa. 

2012. Reducing wrong labels in distant supervision 

for relation extraction. In: ACL 2012,pp. 721-729. 

Wang, Chang, Aditya Kalyanpur, James Fan, Branimir 

K. Boguraev, and D. C. Gondek. 2012. Relation ex-

traction and scoring in DeepQA. In: IBM Journal of 

Research and Development, 56(3.4), pp. 9-1. 

Wang, Chang, James Fan, Aditya Kalyanpur, and Da-

vid Gondek. 2011. Relation extraction with relation 

topics. In: Proceedings of EMNLP 2011, pp. 1426-

1436. 

Wright, John, Allen Y. Yang, Arvind Ganesh, Shankar 

S. Sastry, and Yi Ma. 2009. Robust face recognition 

via sparse representation. In: Pattern Analysis and 

Machine Intelligence, IEEE Transactions on, 31(2), 

210-227 

Wu, Fei, and Daniel S. Weld. 2007. Autonomously se-

mantifying wikipedia. In: Proceedings of CIKM 

2007,pp. 41-50. 

Xu, Wei, Raphael Hoffmann Le Zhao, and Ralph 

Grishman. 2013. Filling Knowledge Base Gaps for 

Distant Supervision of Relation Extraction. In: Pro-

ceedings of Proceedings of 2013, pp. 665-670. 

Yang, Allen Y., Shankar S. Sastry, Arvind Ganesh, and 

Yi Ma. 2010. Fast l1-Minimization Algorithms and 

An Application in Robust Face Recognition: A Re-

view. In: Proceedings of ICIP 2010. 

Yao, Limin, Sebastian Riedel, and Andrew McCallum. 

2010. Collective cross-document relation extraction 

723



without labelled data. In: Proceedings of EMNLP 

2010, pp. 1013-1023. 

Zhang, Congle, Raphael Hoffmann, and Daniel S. 

Weld. 2012. Ontological smoothing for relation ex-

traction with minimal supervision. In: Proceedings 

of AAAI 2012, pp. 157-163. 

Zhang, Xingxing, Zhang, Jianwen, Zeng, Junyu, Yan, 

Jun, Chen, Zheng and Sui, Zhifang. 2013. Towards 

Accurate Distant Supervision for Relational Facts 

Extraction. In: Proceedings of ACL 2013, pp. 810-

815. 

724


