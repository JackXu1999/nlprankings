



















































Off-topic Response Detection for Spontaneous Spoken English Assessment


Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 1075–1084,
Berlin, Germany, August 7-12, 2016. c©2016 Association for Computational Linguistics

Off-topic Response Detection for Spontaneous Spoken English
Assessment

Andrey Malinin, Rogier C. Van Dalen, Yu Wang, Kate M. Knill, Mark J. F. Gales
University of Cambridge, Department of Engineering

Trumpington St, Cambridge CB2 1PZ, UK
{am969, yw396, kate.knill, mjfg}@eng.cam.ac.uk

Abstract

Automatic spoken language assessment
systems are becoming increasingly impor-
tant to meet the demand for English sec-
ond language learning. This is a challeng-
ing task due to the high error rates of, even
state-of-the-art, non-native speech recog-
nition. Consequently current systems pri-
marily assess fluency and pronunciation.
However, content assessment is essential
for full automation. As a first stage it is
important to judge whether the speaker re-
sponds on topic to test questions designed
to elicit spontaneous speech. Standard ap-
proaches to off-topic response detection
assess similarity between the response and
question based on bag-of-words represen-
tations. An alternative framework based
on Recurrent Neural Network Language
Models (RNNLM) is proposed in this pa-
per. The RNNLM is adapted to the topic
of each test question. It learns to asso-
ciate example responses to questions with
points in a topic space constructed using
these example responses. Classification
is done by ranking the topic-conditional
posterior probabilities of a response. The
RNNLMs associate a broad range of re-
sponses with each topic, incorporate se-
quence information and scale better with
additional training data, unlike standard
methods. On experiments conducted on
data from the Business Language Testing
Service (BULATS) this approach outper-
forms standard approaches.

1 Introduction

As English has become the global lingua franca,
there is growing demand worldwide for assess-

ment of English as a second language (Seidlhofer,
2005). To assess spoken communication, sponta-
neous speech is typically elicited through a series
of questions such as ’describe the photo’ or ’plan
a meeting’. Grades are awarded based on a candi-
date’s responses.

Automatic assessment systems are becoming at-
tractive as they allow second language assessment
programmes to economically scale their opera-
tions while decreasing throughput time and pro-
vide testing on demand. Features for automatic
graders are derived from the audio and from hy-
potheses produced by automatic speech recogni-
tion (ASR) systems. The latter is highly errorful
due to the large variability in the input speech;
disfluencies common to spontaneous speech, non-
native accents and pronunciations. Current sys-
tems, such as ETS’ SpeechRater (Zechner et al.,
2009) and Pearson’s AZELLA (Metallinou and
Cheng, 2014), primarily assess pronunciation and
fluency. Although these are clearly indicative of
spoken language ability, full assessment of spo-
ken communication requires judgement of high-
level content and communication skills, such as re-
sponse construction and relevance. The first stage
of this is to assess whether the responses are off-
topic, that is, has the candidate misunderstood the
question and/or memorised a response.

While there has been little work done on de-
tecting off-topic responses for spoken language
assessment, detection of off-topic responses and
content assessment has been studied for essay as-
sessment. One approach for essay content as-
sessment uses features based on semantic similar-
ity metrics between vector space representations
of responses. Common vector representations in-
clude lexical Vector Space Models and Latent Se-
mantic Analysis (LSA) (Yannakoudakis, 2013).
This approach was first applied to spoken assess-

1075



ment in (Xie et al., 2012) and then in (Evanini et
al., 2013). Following this, (Yoon and Xie, 2014)
investigated the detection of responses for which
an automatic assessment system will have diffi-
culty in assigning a valid score, of which off-
topic responses are a specific type. A decision
tree classifier is used with features based on co-
sine similarity between a test response and tf-idf
vectors of both aggregate example responses and
questions, as well as pronunciation and fluency.
In (Evanini and Wang, 2014) text reuse and pla-
giarism in spoken responses are detected using a
decision tree classifier based on vector similarity
and lexical matching features which compare a re-
sponse to a set of example ’source texts’ . This
task is similar to off-topic response detection in
that it is based on comparing a test response to
example responses. Thus, a standard approach
to off-topic response detection would be based on
measuring the similarity between vector represen-
tations of a spoken response and the test ques-
tion. A major deficiency of this approach is that
it is based on bag-of-words vector representations,
which loses information about the sequential na-
ture of speech, which is important to evaluating
response construction and relevance. Addition-
ally, adapting the approach to model a range of
responses for each topic causes classification time
to scale poorly with training data size and the num-
ber of questions.

To address these issues a general off-topic
content detection framework based on topic
adapted Recurrent Neural Network language mod-
els (RNNLM) has been developed and applied to
off-topic response detection for spoken language
assessment. This framework uses example re-
sponses to test questions in training of the lan-
guage model and construction of the topic-space.
The RNNLM learns to associate the example re-
sponses with points in the topic-space. Classi-
fication is done by ranking the topic-conditional
posterior probabilities of a response. The advan-
tage of this approach is that sequence information
can be taken into account and broad ranges of re-
sponses can be associated with each topic with-
out affecting classifcation speed. Two topic vec-
tor representations are investigated: Latent Dirich-
let Allocation (LDA) (Blei et al., 2003; Griffiths
and Steyvers, 2004) and Latent Semantic Analysis
(LSA) (Landauer et al., 1998). They are compared
to standard approaches on data from the Cam-

bridge Business English (BULATS) exam.
The rest of this paper is structured as follows:

Section 2 discusses the RNNLM adaptation and
topic spaces; Section 3 discusses approaches to
topic detection; Section 4 presents data sets and
experimental infrastructure; Section 5 analyzes
experimental results; Section 6 concludes the pa-
per.

2 Topic Adapted RNNLMs

2.1 RNNLM Architecture

A statistical language model is used to model the
semantic and syntactic information in text in the
form of a probability distribution over word se-
quences. It assigns a probability to a word se-
quence w = {w0, w1, · · · , wL} as follows:

P(wi|wi−1, · · · , w0) = P(wi|hi−10 )

P(w) =
L∏

i=1

P(wi|hi−10 )

(1)

(2)

where w0 is the start of sentence symbol <s>. In
this work a language model is trained to model
example responses to questions on a spoken lan-
guage assessment test. P(wi|hi−10 ) can be es-
timated by a number of approaches, most no-
tably N-grams and Recurrent Neural Networks
(Mikolov et al., 2010).

Recurrent Neural Network language models
(RNNLMs) (Figure 1) (Mikolov, 2012) are a
variable context length language model, capable
of representing the entire context efficiently, un-
like N-grams. RNNLMs represent the full un-
truncated history hi−10 = {wi−1, · · · , w0} for
word wi as the hidden layer si−1, a form of short-
term memory, whose representation is learned
from the data. Words and phrases are represented
in a continuous space, which gives RNNLMs
greater generalization abilities than other language
models, such as N-grams.

RNNLMs can be adapted by adding a feature
vector f which represents information absent from
the RNN (Mikolov and Zweig, 2012). In this
work, the vector representation of a spoken lan-
guage test question topic fq is used for the con-
text vector f . Architecturally, a context adapted
RNNLM is described by equations 3-5. e(x) and
g(x) are element-wise sigmoid and softmax acti-

1076



Figure 1: Context adapted RNN language model

vation functions.

P(wi|hi−10 , f) = PRNN(wi|wi−1, si−2, f)
PRNN(wi|wi−1, si−2, f) = g(Vsi−1 + Hf)
si−1 = e(Uwi−1 + Wsi−2 + Gf)

(3)

(4)

(5)

Through the process of adaptation the RNNLM
learns to associate particular types of responses
with particular topics, thereby becoming more dis-
criminative. Thus, a sentence’s topic-conditional
probability PRNN(w|fq) will be higher if it corre-
sponds to the topic q than if it does not.

2.2 Example Response Based Topic Space
In order for the topic vectors fq to be informa-
tive they must span the space of all question top-
ics in the test. Thus a topic space needs to be de-
fined. Example responses, which are necessary to
train the RNNLM, are used to define a topic space
because typical responses to a question will be
definitive of the question’s topic. Multiple exam-
ple responses to a particular question are merged
into one aggregate response to capture a broad
range of response variations and increase the ro-
bustness of the vector representation estimation.

By default a topic t is defined for each ques-
tion q. However, multi-part questions are com-
mon, where candidates are given a scenario such
as providing tourist information in which indi-
vidual questions ask about food, hotels or sights.
Since the underlying topic is related this can con-
fuse a classifier. The responses for all these related
questions could be merged to form a single aggre-
gate vector, but the statistics of the responses to

each question can be sufficiently different that less
distinct topics are formed. Instead the aggregrate
example responses for each question are assigned
the same topic label. Thus, a mapping between
questions and topics and its inverse is introduced:

M : q → t
M−1t : {q ∈ Q|M(q) = t}

(6)

(7)

A vector representation of a question topic is
computed using the aggregate example responses.
As mentioned in Section 1, two common represen-
tations are LDA and LSA; both are investigated in
this work.

LDA is a generative model which allows docu-
ments to be modelled as distributions over latent
topics z ∈ Z. Each latent topic z is described by a
multinomial distribution over words P(wi|z), and
each word in a document is attributed to a particu-
lar latent topic (Blei et al., 2003). Thus, the adap-
tation vector fw represents a vector of posterior
probabilities over latent topics for word sequence
w:

fw = [P(z = 1|w), · · · , P(z = K|w)]T

P(z = k|w) =
∑N

i=1 δ(zwi = k)
N

(8)

(9)

LDA was found to perform better for RNNLM
adaptation than other representations in (Mikolov
and Zweig, 2012; Chen et al., 2015).

LSA (Landauer et al., 1998) is a popular repre-
sentation for information retrieval tasks. A word-
document matrix F is constructed using example
responses and each word is weighted by its term
frequency-inverse document frequency (TF-IDF).
Then a low-rank approximation Fk is computed
using Singular Value Decomposition (SVD):

Fk = UkΣkVTk
Fk = [f1, · · · , fQ]T
fw = Σ−1k U

T
k ftfidf

(10)

(11)

(12)

Only the k largest singular values of the singular
value matrix Σ are kept. Fk is a representation of
the data which retains only the k most significant
factors of variation. Each row is a topic vector fq.
New vectors fw for test responses can be projected
into the LSA space via equation 12, where ftfidf is
the TF-IDF weighted bag-of-word representation
of a test response.

1077



3 Topic Detection and Features

This section discusses the standard, vector similar-
ity feature-based, and the proposed topic adapted
RNNLM approaches for topic detection.

3.1 Vector Similarity Features

The essence of the standard approach to topic de-
tection is to assess the semantic distance Dsem be-
tween the test response w and the aggregate exam-
ple response wq by approximating it using a vec-
tor distance metric Dvec between vector represen-
tations of the response fw and the topic fq. Clas-
sification is done by selecting the topic closest to
the test response:

t̂w =M
(

arg min
q
{Dsem(w,wq)}

)
Dsem(w,wq) ≈ Dvec(fw, fq)

(13)

(14)

The selection of an appropriate distance metric
Dvec(fw, fq) can have a large effect on the classifi-
cation outcome. A common metric used in topic
classification and information retrieval is cosine
similarity, which measures the cosine of the an-
gle between two vectors. A distance metric based
on this, cosine distance, can be defined as:

Dcos(fw, fq) = 1− fw
Tfq

|fw||fq| (15)

While topics are robustly defined, this approach
fails to capture the range of responses which can
be given to a question. A different approach would
be to maintain a separate point in this topic space
for every example response. This retains the ro-
bust topic definition while allowing each topic to
be represented by a cloud of points in topic space,
thereby capturing a range of responses which can
be given. A K-nearest neighbour (KNN) classifier
can be used to detect the response topic by com-
puting distances of the test response to each of the
training points in topic space. However, classifi-
cation may become impractical for large data sets,
as the number of response points scales with the
size of the training data. Low-cost distance mea-
sures, such as cosine distance, allow this approach
to be used on large data sets before it becomes
computationally infeasible. This approach is used
as the baseline for comparison with the proposed
RNNLM based method. For multi-part questions,
topic vectors relating to the same overall topic are
simply given the same topic label.

The classification rate can be improved by tak-
ing the top N t̂N = {t̂1, · · · , t̂N} results into
account. The KNN classifier can be modified
to yield the N-best classification by removing all
training points from the 1-best class from the KNN
classifier and re-running the classification to get
the 2-best results, and so on.

One of the main deficiencies of methods based
on computing distances between vector represen-
tations is that commonly used representations,
such as LSA and LDA, ignore word-order in docu-
ments, thereby throwing away information which
is potentially useful for topic classification. Ad-
ditionally, if any of the test or example response
utterances are short, then their topic vector repre-
sentations may not be robustly estimated.

3.2 Topic Adapted RNNLM Framework

The RNNLM based approach to topic detection
is based on different principles. By combining
equations 2 and 3 the log-probability L(q) of a
response sentence given a particular topic vector
PRNN(w|fq) is computed. For each response w in
the test set L(q) is computed (equation 16) for all
topic vectors fq. L(q) is calculated using equa-
tion 17 for multi-part questions with responses wp
where p ∈ t. Classification is done by ranking
log-probability L(q) for an utterance w and L(q)
for all q ∈M−1t are averaged (equation 18).

L(q) =


log[PRNN(w|fq)]∑

p

1
Np

log[PRNN(wp|fq)]
(16)

(17)

t̂w = arg max
t
{ 1|M−1t |

∑
q∈M−1t

L(q)} (18)

It is trivial to extend this approach to yield the N-
best solutions by simply taking the top N outputs
of equation 18.

The RNNLM approach has several benefits over
standard approaches. Firstly, this approach explic-
itly takes account of word-order in determining
the topical similarity of the response. Secondly,
there is no need to explicitly select a distance met-
ric. Thirdly, the problems of robustly estimating
a vector representation fw of the test response are
sidestepped. Furthermore, the RNNLM accounts
for a broad range of responses because it is trained
on individual response utterances which it asso-
ciates with a question topic vector. This makes

1078



it more scalable than the KNN approach because
the number of comparisons which need to be made
scales only with the number of questions, not the
size of the training data. Thus, arbitrarily large
data sets can be used to train the model without
affecting classification time.

The RNNLM could be used in a KNN-style ap-
proach, where it associates each example response
with its individual topic vector, using L(q) as a
distance metric. However, this is computationally
infeasible since computing L(q) is significantly
more expensive than cosine distance and the pre-
viously mentioned scalability would be lost.

4 Data and Experimental Set-up

Data from the Business Language Testing Service
(BULATS) English tests is used for training and
testing. At test time, each response is recognised
using an ASR system and the 1-best hypothesis is
passed to the topic classifier. The topic detection
system decides whether the candidate has spoken
off topic by comparing the classifier output to the
topic of the question being answered.

4.1 BULATS Test Format and Data
The BULATS Online Speaking Test has five sec-
tions (Chambers and Ingham, 2011):

A Candidates respond to eight simple questions
about themselves and their work (e.g. what is
your name, where do you come from?).

B Candidates read aloud six short texts appro-
priate to a work environment.

C Candidates talk about a work-related topic
(e.g. the perfect office) with the help of
prompts which appear on the screen.

D Candidates must describe a graph or chart
such as a pie or a bar chart related to a busi-
ness situation (e.g. company exports).

E Candidates are asked to respond to five open-
ended questions related to a single context
prompt. For example a set of five questions
about organizing a stall at a trade fair.

Candidates are given a test consisting of 21 ques-
tions, however, only the last three sections, con-
sisting of 7 questions, are spontaneously con-
structed responses to open ended question, and
therefore of relevance to this work. Each unique
set of 7 questions is a question script.

Training, development and evaluation data sets
composed of predominantly Gujarati L1 candi-
dates are used in these experiments. The data sets
are designed to have an (approximately) even dis-
tribution over grades as well as over the different
question scripts.

During operation the system will detect off-
topic responses based on ASR transcriptions, so
for the system to be matched it needs to be trained
on ASR transcriptions as well. Thus, two train-
ing sets are made by using the ASR architec-
ture described in section 4.2 to transcribe candi-
date responses. Each training set covers the same
set of 282 unique topics. The first training set
consists of data from 490 candidates, containing
9.9K responses, with an average of 35.1 responses
per topic. The second, much larger, training set
consists of data from 10004 candidates, contain-
ing 202K responses, with an average of 715.5 re-
sponses per topic.

Characteristic
Section

A B C D E
# Unique Topics 18 144 17 18 85
# Questions/Section 6 8 1 1 5
Av. # Words/Resp. 10 10 61 77 20

Table 1: Data Characteristics.

As Table 1 shows, the average response length
varies across sections due to the nature of the sec-
tions. Shorter responses to questions are observed
for sections A, B and E, with longer responses to
C and D. Estimating topic representations for sec-
tions A, B and E questions based on individual re-
sponses would be problematic due to the short re-
sponse lengths. However, by aggregating example
responses across candidates, as described in sec-
tion 2.2, the average length of responses in all sec-
tions is significantly longer, allowing the example-
response topic space to be robustly defined.

Section E topics correspond to topics of sub-
questions relating to an overall question, thus there
are only 15 unique questions in section E. How-
ever, the sub-questions are sufficiently distinct to
merit their own topic vectors. At classification
time confusions between sub-questions of an over-
all section E question are not considered mistakes.

Held-out test sets are used for development,
DEV, and evaluation, EVAL, composed of 84 and
223 candidates, respectively. ASR transcriptions
are used for these test sets, as per the operating

1079



scenario. A version of the DEV set with pro-
fessionally produced transcriptions, DEV REF, is
also used in training and development.

The publicly available Gibbs-LDA
toolkit (Phan and Nguyen, 2007) is used to
estimate LDA posterior topic vectors and the
scikit-learn 17.0 toolkit (Pedregosa et al., 2011)
to estimate LSA topic representations. The topic
adapted RNNLM uses a 100-dimensional hidden
layer. DEV REF is used as a validation set for
early stopping to prevent over-fitting. The CUED
RNNLM toolkit v0.1 (Chen et al., 2016) is used
for RNNLM training, details of which can be
found in (Chen et al., 2014; Mikolov et al., 2010)

4.2 ASR System
A hybrid deep neural network DNN-HMM system
is used for ASR (Wang et al., 2015). The acoustic
models are trained on 108.6 hours of BULATS test
data (Gujarati L1 candidates) using an extended
version of the HTK v3.4.1 toolkit (Young et al.,
2009; Zhang and Woodland, 2015). A Kneser-Ney
trigram LM is trained on 186K words of BULATs
test data and interpolated with a general English
LM trained on a large broadcast news corpus, us-
ing the SRILM toolkit (Stolcke, 2002). Lattices
are re-scored with an interpolated trigram+RNN
LM (Mikolov et al., 2010) by applying the 4-
gram history approximation described in (Liu et
al., 2014), where the RNNLM is trained using the
CUED RNNLM toolkit (Chen et al., 2016). Inter-
polation weights are optimized on the DEV REF
data set. Table 2 shows the word error rate (WER)
on the DEV test set relative to the DEV REF ref-
erences for each section and the combined sponta-
neous speech sections (C-E).

% WER
A B C D E C-E

30.6 23.2 32.0 29.9 32.3 31.5

Table 2: ASR performance on DEV.

5 Experiments

Two forms of experiment are conducted in or-
der to assess the performance of the topic-adapted
RNNLM. First, a topic classification experiment
is run where the ability of the system to accurately
recognize the topic of a response is evaluated. Sec-
ond, a closed-set off-topic response detection ex-
periment is done.

In the experimental configuration used here a
response is classified into a topic and the accuracy
is measured. The topic of the question being an-
swered is known and all responses are actually on-
topic. A label (on-topic/off-topic) is given for each
response based on the output of the classifier rela-
tive to the question topic. Thus, results presented
are in terms of false rejection (FR) and false accep-
tance (FA) rates rather than precision and recall.

Initial topic detection experiments were run us-
ing the DEV test set with both the reference
transcriptions (REF) and recognition hypotheses
(ASR) to compare different KNN and RNN sys-
tems. After this, performance is evaluated on the
EVAL test set. The systems were trained using
data sets of 490 and 10004 candidates, as de-
scribed in section 4.1.

5.1 Topic Classification

Performance of the topic-adapted RNNLM is
compared to the KNN classifier in Table 3. The
RNN1 system outperforms the KNN system by
20-35 % using the LDA topic representation. Fur-
thermore, the KNN system performs worse on sec-
tion E than it does on section C, while RNN1 per-
formance is better on section E by 7-10% than on
section C. The LSA topic representation consis-
tently yields much better performance than LDA
by 25-50% for both systems. Thus, the LDA rep-
resentation is not further investigated in any exper-
iments.

When using the LSA representation the RNN1
system outperforms the KNN system only
marginally, due to better performance on section
E. Additionally, unlike the KNN-LDA system, the
KNN-LSA system does not have a performance
degradation on section E relative to section C. No-
tably, the RNN1 system performs better on sec-
tion E by 5-13% than on section C. Clearly, sec-
tion C questions are hardest to assess. Combining
both representations through concatenation does
not effect performance of the KNN system and
slightly degrades RNN1 performance on section
C. KNN and RNN1 systems with either topic rep-
resentation perform comparably on REF and ASR.
This suggests that the systems are quite robust
to WER rates of 31.5% and the differences are
mostly noise.

Training the RNN2 system on 20 times as much
data leads to greatly improved performance over
KNN and RNN1 systems, almost halving the over-

1080



Topic
System # Cands.

C D E ALL (C-E)
Repn. REF ASR REF ASR REF ASR REF ASR

LDA
KNN

490
75.0 81.0 37.0 42.0 91.8 91.1 68.0 71.4

RNN1 61.9 58.3 28.4 25.9 48.8 51.2 46.6 45.4

LSA
KNN

490
32.1 28.6 2.5 3.7 31.3 33.3 22.0 21.9

RNN1 29.8 31.0 4.9 6.2 23.8 23.8 19.7 20.5
RNN2 10004 19.0 19.0 3.7 3.7 9.5 10.7 10.8 11.2

LDA
KNN

490
30.9 29.8 2.5 3.7 31.5 33.3 21.7 22.3

+LSA
RNN1 32.1 35.7 4.9 4.9 23.8 22.6 20.5 21.3
RNN2 10004 25.0 22.6 4.9 4.9 10.7 10.7 13.7 12.9

Table 3: % False rejection in topic detection using KNN classifier with 6 nearest neighbour and distance
weights and RNNLM classifier on the DEV test set. 280 dim. topic spaces for LDA and LSA, and 560
dim. for LDA+LSA.

all error rate. The KNN system could not be eval-
uated effectively in reasonable time using 20 times
as many example responses and results are not
shown, while RNN2 evaluation times are unaf-
fected. Notably, RNN performance using the LSA
representation scales with training data size better
than with the LDA+LSA representation. Thus, we
further investigate systems only with the LSA rep-
resentation. Interestingly, section D performance
is improved only marginally.

Performance on section D is always best, as sec-
tion D questions relate to discussion of charts and
graphs for different conditions for which the vo-
cabulary is very specific. Section C and E ques-
tions are the less distinct because they have free-
form answers to broad questions, leading to higher
response variability. This makes the linking of
topic from the training data to the test data more
challenging, particularly for 1-best classification,
leading to higher error rates.

Figure 2 shows the topic classification confu-
sion matrix for the RNN1 LSA system. A similar
matrix is observed for the KNN LSA system. Most
confusions are with topics from the same section.
This is because each section has a distinct style
of questions and some questions within a section
are similar. An example is shown below. Ques-
tion SC-EX1 relates to personal local events in the
workplace. SC-EX2, which relates to similar is-
sues, is often confused with it. On the other hand,
SC-EX3 is rarely confused with SC-EX1 as it is
about non-personal events on a larger scale.

• SC-EX1: Talk about some advice from a colleague.
You should say: what the advice was, how it helped
you and whether you would give the same advice to
another colleague.

• SC-EX2: Talk about a socially challenging day you had
at work. You should say: what was the challenging
situation, how you resolved it and why you found it
challenging.

• SC-EX3: Talk about a company in your local town
which you admire. You should say: what company it
is, what they do, why you admire them, and how the
company impacts life in your town.

Figure 2: RNN1 LSA confusion matrix on DEV
ASR.

System performance can be increased by con-
sidering N -best results, as described in Section 3.
Results for systems trained on 490 and 10004 can-
didates are presented in Table 4. The error rate
decreases as the value of N increases for all sys-
tems. However, performance scales better with N
for the RNN systems than for KNN. Notably, for
values of N > 1 performance of all systems on
REF is better, which suggests that ASR errors do
have a minor impact on system performance.

5.2 Off-topic response detection

In the second experiment off-topic response de-
tection is investigated. Performance is measured

1081



N System # Cands. REF ASR

1
KNN

490
22.1 21.9

RNN1 19.7 20.5
RNN2 10004 10.8 11.2

2
KNN

490
15.9 16.0

RNN1 13.7 16.1
RNN2 10004 6.8 7.6

3
KNN

490
13.5 14.3

RNN1 10.4 11.2
RNN2 10004 6.4 7.2

4
KNN

490
11.1 12.5

RNN1 8.8 10.0
RNN2 10004 5.2 6.4

Table 4: N -Best % false rejection performance of
KNN and RNNLM classifiers with the LSA topic
space on the DEV test set

in terms of the false acceptance (FA) probability
of an off-topic response and false rejection (FR)
probability of an on-topic response. The experi-
ment is run on DEV and EVAL test sets. Since
neither DEV nor EVAL contain real off-topic re-
sponses, a pool Wq of such responses is synthet-
ically generated for each question by using valid
responses to other questions in the data set. Off-
topic responses are then selected from this pool.
A selection strategy defines which responses are
present in Wq. Rather than using a single se-
lection of off-topic responses, an expected per-
formance over all possible off-topic response se-
lections is estimated. The overall probability of
falsely accepting an off-topic response can be ex-
pressed using equation 19.

P(FA) =
Q∑

q=1

∑
w∈Wq

P(FA|w, q)P(w|q)P(q) (19)

In equation 19, the question q is selected with uni-
form probability from the set Q of possible ques-
tions. The candidate randomly selects with uni-
form probability P(w|q) a response w from the
pool Wq. The correct response to the question is
not present in the pool. The conditional probabil-
ity of false accept P(FA|w, q) = 1 ifM(q) ∈ t̂N ,
andM(q) is not the real topic of the response w,
otherwise P(FA|w, q) = 0.

As shown in Figure 2, the main confusions will
occur if the response is from the same section as
the question. Two strategies for selecting off-topic
responses are considered based on this: naive,

where an incorrect response can be selected from
any section; and directed, where an incorrect
response can only be selected from the same sec-
tion as the question. The naive strategy rep-
resents candidates who have little knowledge of
the system and memorise responses unrelated to
the test, while the directed strategy represents
those who are familiar with the test system and
have access to real responses from previous tests.

Test Set System
% Equal Error Rate
Directed Naive

DEV
KNN 13.5 10.0
RNN1 10.0 7.5
RNN2 7.5 6.0

EVAL
KNN 12.5 9.0
RNN1 8.0 6.0
RNN2 5.0 4.5

Table 5: % Equal Error Rate for LSA topic space
systems on the DEV and EVAL test sets.

Figure 3: ROC curves of LSA topic space systems
on the EVAL test set.

A Receiver Operating Characteristic (ROC)
curve (Figure 3) can be constructed by plotting the
FA and FR rates for a range of N . The RNN1 sys-
tem performs better at all operating points than the
KNN system for both selection strategies and eval-
uation test sets. Equal Error Rates (EER), where
FA = FR, are given in Table 5. Results on EVAL
are more representative of the difference between
the KNN and RNN performance, as they are eval-
uated on nearly 3 times as many candidates. The

1082



RNN2 system achieves the lowest EER. It is in-
teresting that for better systems the difference in
performance against the naive and directed
strategies decreases. This indicates that the sys-
tems become increasingly better at discriminating
between similar questions.

As expected, the equal error rate for the
directed strategy is higher than for the naive
strategy. In relation to the stated task of detect-
ing when a test candidate is giving a memorized
response, the naive strategy represents a lower-
bound on realistic system performance, as students
are not likely to respond with a valid response to
a different question. Most likely they will fail
to construct a valid response or will add com-
pletely unrelated phrases memorised beforehand,
which, unlike responses from other sections, may
not come from the same domain as the test (eg:
Business for BULATs).

6 Conclusion and Future Work

In this work a novel off-topic content detection
framework based on topic-adapted RNNLMs was
developed. The system was evaluated on the task
of detecting off-topic spoken responses on the BU-
LATS test. The proposed approach achieves better
topic classification and off-topic detection perfor-
mance than the standard approaches.

A limitation of both the standard and proposed
approach is that if a new question is created by
the test-makers, then it will be necessary to col-
lect example responses before it can be widely de-
ployed. However, since the system can be trained
on ASR transcriptions, the example responses do
not need to be hand-transcribed. This is an at-
tractive deployment scenario, as only a smaller
hand-transcribed data set is needed to train an ASR
system with which to cost-effectively transcribe a
large number of candidate recordings.

Further exploration of different topic vector rep-
resentations and their combinations is necessary in
future work.

Acknowledgements

This research was funded under the ALTA Insti-
tute, University of Cambridge. Thanks to Cam-
bridge English, University of Cambridge, for sup-
porting this research and providing access to the
BULATS data.

References
David M. Blei, Andrew Y. Ng, and Michael I. Jordan.

2003. Latent Dirichlet Allocation. Journal of Ma-
chine Learning Research, 3:993–1022, March.

Lucy Chambers and Kate Ingham. 2011. The BU-
LATS online speaking test. Research Notes, 43:21–
25.

Xie Chen, Yongqiang Wang, Xunying Liu, Mark J.F.
Gales, and P.C. Woodland. 2014. Efficient GPU-
based Training of Recurrent Neural Network Lan-
guage Models Using Spliced Sentence Bunch. In
Proc. INTERSPEECH.

Xie Chen, Tian Tan, Xunying Liu, Pierre Lanchantin,
Moquan Wan, Mark J.F. Gales, and Philip C. Wood-
land. 2015. Recurrent Neural Network Lan-
guage Model Adaptation for Multi-Genre Broadcast
Speech Recognition. In Proc. INTERSPEECH.

X. Chen, X. Liu, Y. Qian, M.J.F. Gales, and P.C. Wood-
land. 2016. CUED-RNNLM – an open-source
toolkit for efficient training and evaluation of re-
current neural network language models. In Proc.
ICASSP.

Keelan Evanini and Xinhao Wang. 2014. Automatic
detection of plagiarized spoken responses. In Pro-
ceedings of the Ninth Workshop on Innovative Use
of NLP for Building Educational Applications.

Keelan Evanini, Shasha Xie, and Klaus Zechner. 2013.
Prompt-based Content Scoring for Automated Spo-
ken Language Assessment. In Proc. NAACL-HLT.

Thomas L. Griffiths and Mark Steyvers. 2004. Find-
ing Scientific Topics. Proceedings of the National
Academy of Sciences, 101:5228–5235.

Thomas K Landauer, Peter W. Foltz, and Darrell La-
ham. 1998. Introduction to Latent Semantic Analy-
sis. Discourse Processes, 25:259–284.

Xunying Liu, Y. Wang, Xie Chen, Mark J.F. Gales,
and Philip C. Woodland. 2014. Efficient Lattice
Rescoring using Recurrent Neural Network Lan-
guage Models. In Proc. INTERSPEECH.

Angeliki Metallinou and Jian Cheng. 2014. Using
Deep Neural Networks to Improve Proficiency As-
sessment for Children English Language Learners.
In Proc. INTERSPEECH.

Tomas Mikolov and Geoffrey Zweig. 2012. Context
Dependent Recurrent Neural Network Language
Model. In Proc. IEEE Spoken Language Technol-
ogy Workshop (SLT).

Tomas Mikolov, Martin Karafiát, Lukás Burget, Jan
Cernocký, and Sanjeev Khudanpur. 2010. Recur-
rent Neural Network Based Language Model. In
Proc. INTERSPEECH.

1083



Tomas Mikolov. 2012. Statistical Language Models
Based on Neural Networks. Ph.D. thesis, Brno Uni-
versity of Technology.

F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel,
B. Thirion, O. Grisel, M. Blondel, P. Prettenhofer,
R. Weiss, V. Dubourg, J. Vanderplas, A. Passos,
D. Cournapeau, M. Brucher, M. Perrot, and E. Duch-
esnay. 2011. Scikit-learn: Machine Learning in
Python. Journal of Machine Learning Research,
12:2825–2830.

Xuan-Hieu Phan and Cam-Tu Nguyen. 2007. Gibb-
sLDA++: A C/C++ implementation of latent Dirich-
let allocation (LDA). http://gibbslda.
sourceforge.net/.

Barbara Seidlhofer. 2005. English as a lingua franca.
ELT journal, 59(4):339.

A Stolcke. 2002. SRILM – an extensible language
modelling toolkit. In Proc. ICSLP.

Haipeng Wang, Anton Ragni, Mark J. F. Gales, Kate M.
Knill, Philip C. Woodland, and Chao Zhang. 2015.
Joint Decoding of Tandem and Hybrid Systems for
Improved Keyword Spotting on Low Resource Lan-
guages. In Proc. INTERSPEECH.

Shasha Xie, Keelan Evanini, and Klaus Zechner. 2012.
Exploring Content Features for Automated Speech
Scoring. In Proc. NAACL-HLT.

Helen Yannakoudakis. 2013. Automated assess-
ment of English-learner writing. Technical Report
UCAM-CL-TR-842, University of Cambridge Com-
puter Laboratory.

Su-Youn Yoon and Shasha Xie. 2014. Similarity-
Based Non-Scorable Response Detection for Auto-
mated Speech Scoring. In Proceedings of the Ninth
Workshop on Innovative Use of NLP for Building
Educational Applications.

Steve Young, Gunnar Evermann, Mark J. F. Gales,
Thomas Hain, Dan Kershaw, Xunying (Andrew)
Liu, Gareth Moore, Julian Odell, Dave Ollason, Dan
Povey, Valtcho Valtchev, and Phil Woodland. 2009.
The HTK book (for HTK Version 3.4.1). University
of Cambridge.

Klaus Zechner, Derrick Higgins, Xiaoming Xi, and
David M. Williamson. 2009. Automatic scoring
of non-native spontaneous speech in tests of spoken
English. Speech Communication, 51(10):883–895.
Spoken Language Technology for Education Spoken
Language.

Chau Zhang and Philip C. Woodland. 2015. A General
Artificial Neural Network Extension for HTK. In
Proc. INTERSPEECH.

1084


