



















































Hierarchical Relation Extraction with Coarse-to-Fine Grained Attention


Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 2236–2245
Brussels, Belgium, October 31 - November 4, 2018. c©2018 Association for Computational Linguistics

2236

Hierarchical Relation Extraction with Coarse-to-Fine Grained Attention

Xu Han1,2,3∗ , Pengfei Yu2,3,4∗, Zhiyuan Liu1,2,3† , Maosong Sun1,2,3, Peng Li5
1Department of Computer Science and Technology, Tsinghua University, Beijing, China

2Institute for Artificial Intelligence, Tsinghua University, Beijing, China
3State Key Lab on Intelligent Technology and Systems, Tsinghua University, Beijing, China

4Department of Electronic Engineering, Tsinghua University, Beijing, China
5Pattern Recognition Center, WeChat, Tencent, China

Abstract

Distantly supervised relation extraction em-
ploys existing knowledge graphs to automati-
cally collect training data. While distant super-
vision is effective to scale relation extraction
up to large-scale corpora, it inevitably suffers
from the wrong labeling problem. Many ef-
forts have been devoted to identifying valid in-
stances from noisy data. However, most exist-
ing methods handle each relation in isolation,
regardless of rich semantic correlations lo-
cated in relation hierarchies. In this paper, we
aim to incorporate the hierarchical information
of relations for distantly supervised relation
extraction and propose a novel hierarchical at-
tention scheme. The multiple layers of our
hierarchical attention scheme provide coarse-
to-fine granularity to better identify valid in-
stances, which is especially effective for ex-
tracting those long-tail relations. The exper-
imental results on a large-scale benchmark
dataset demonstrate that our models are capa-
ble of modeling the hierarchical information
of relations and significantly outperform other
baselines. The source code of this paper can
be obtained from https://github.com/
thunlp/HNRE.

1 Introduction

Relation extraction (RE) aims to predict relational
facts from plain text. Conventional supervised
RE models (Zelenko et al., 2003; Mooney and
Bunescu, 2006) usually suffer from the lack of
high-quality training data, because manual label-
ing of training data is time-consuming and human-
intensive. Mintz et al. (2009) propose distant su-
pervision to automatically label training instances
by aligning existing knowledge graphs (KGs) and
text: For an entity pair in KGs, those sentences
containing both the entities will be labeled with

∗ indicates equal contribution
† Corresponding author: Z.Liu(liuzy@tsinghua.edu.cn)

Tail Entity:
Davos

Head Entity:
Ernst Haefliger

/people/deceased_person/place_of_death

/people/deceased_person

/people

     Parent Relation

Parent Relation Parent Relation

Parent RelationParent Relation

Attention
Scores

Ernst Haefliger was born in Davos, Switzerland. 

Ernst Haefliger was born in Davos on July 6, 1919

Ernst Haefliger died on Saturday in Davos. 0.6

0.2

0.2

0.3

0.4

0.3

0.8

0.1

0.1

Figure 1: An example of hierarchical relation ex-
traction.

the corresponding relation of the entity pair in
KGs. RE relies on distant supervision to scale up
to large-scale training corpora. However, this au-
tomatic mechanism is inevitably accompanied by
the wrong labeling problem, because not all sen-
tences containing two entities can exactly express
their relations in KGs, e.g., we may mistakenly la-
bel “Bill Gates retired from Microsoft” with the
relation business/company/founders.

To alleviate the wrong labeling problem, many
efforts (Riedel et al., 2010; Hoffmann et al., 2011;
Surdeanu et al., 2012; Zeng et al., 2015) have
been devoted to identifying valid instances from
noisy data, especially the recent state-of-the-art
attention-based methods (Lin et al., 2016; Ji et al.,
2017; Liu et al., 2017; Wu et al., 2017). Neverthe-
less, each relation is handled in isolation in most
existing methods. For each relation, there is often
a separate model (e.g. neural attention scheme) to
select relation-related informative instances from
noisy data, regardless of rich semantic correlations
among relations, typically located in the form of
relation hierarchies.

We take the KG Freebase (Bollacker et al.,
2008) as an example, in which relations are la-
beled as hierarchical structures. For example, the

https://github.com/thunlp/HNRE
https://github.com/thunlp/HNRE


2237

relation /location/province/capital in
Freebase indicates the relation between a province
and its capital. It is labeled under the location
branch. Under this branch, there are some other
relations /location/location/contains
and /location/country/capital, which
are closely correlated to each other. The rich
correlations among relations are well revealed by
these relation hierarchies. In fact, McCallum et al.
(1998) take advantage of hierarchies of classes to
improve classification models and inspire many
later models (Rousu et al., 2005; Weinberger and
Chapelle, 2009). Furthermore, the hierarchical in-
formation of entities in KGs has also been uti-
lized and demonstrated to be effective for model
enhancement (Hu et al., 2015; Xie et al., 2016).

To take advantage of the rich correlated infor-
mation among relations, we propose a novel hier-
archical attention scheme via utilizing the relation
hierarchies, rather than directly utilizing hierarchi-
cal information as features for models. Similar
to the conventional attention-based method, our
method also computes an attention score for each
instance according to its significance of express-
ing the corresponding relation. The key difference
is that, as illustrated in Figure 1, our hierarchical
attention scheme follows the relation hierarchies
to compute scores for those instances containing
the same entity pair on the each layer of the hier-
archies.

The hierarchical attention scheme provides
coarse-to-fine granularity for identifying valid in-
stances. The attention on the bottom layer can cap-
ture more specific features of the relation, which
has a comparable ability of fine-grained instance
selection like conventional attention-based meth-
ods. The attention on the top-layer can capture the
common features shared by several related sub-
relations, which provides coarse-grained instance
selection. Since there are more sufficient data for
training the top-layer attention, the whole hierar-
chical attention scheme can enhance RE models
for solving those long-tail relations.

We conduct experiments on a large-scale bench-
mark dataset for RE in this paper. The experi-
mental results show that the proposed coarse-to-
fine grained attention scheme based on relation hi-
erarchies significantly outperforms other baseline
methods, even as compared to the recent state-
of-the-art attention-based models, especially for
those long-tail relations.

2 Related Works

Supervised models (Zelenko et al., 2003; Zhou
et al., 2005; Mooney and Bunescu, 2006) for RE
require adequate amounts of annotated data for
their training. It is time-consuming to manu-
ally label large-scale training data. Hence, Mintz
et al. (2009) propose distant supervision to au-
tomatically label data. Distant supervision in-
evitably accompanies with the wrong labeling
problem. To alleviate the noise issue caused by
distant supervision, Riedel et al. (2010) and Hoff-
mann et al. (2011) propose multi-instance learning
(MIL) mechanisms. Riedel et al. (2013) propose
universal schema to transmit information between
relations of KGs and textual patterns to enhance
extraction performance.

These early RE methods mainly extract seman-
tic features using NLP tools to build relation clas-
sifiers. Recently, neural models have been widely
used for RE. These neural models can accurately
capture textual relations without explicit linguis-
tic analysis (Zeng et al., 2014; Xu et al., 2015;
Santos et al., 2015; Zhang and Wang, 2015; Verga
et al., 2016; Verga and McCallum, 2016). Zeng
et al. (2015) employ the MIL scheme by selecting
one most valid instance for distantly supervised
neural relation extraction (NRE), whose denois-
ing capability is far from satisfactory because most
informative instances are neglected. Lin et al.
(2016) and Zhang et al. (2017) propose neural
attention schemes to select those informative in-
stances. To further improve the attention perfor-
mance, some works incorporate knowledge infor-
mation (Zeng et al., 2017; Ji et al., 2017; Han et al.,
2018) and advanced training strategies (Liu et al.,
2017; Huang and Wang, 2017). More sophisti-
cated mechanisms, such as reinforcement learning
(Feng et al., 2018; Zeng et al., 2018) and adver-
sarial training (Wu et al., 2017), have also been
adapted for RE recently.

However, most existing works model each rela-
tion in isolation to identify informative instances,
neglecting rich correlations among relations, es-
pecially the hierarchical information of those re-
lations. Hierarchical information is widely ap-
plied for model enhancement, especially for clas-
sification models (McCallum et al., 1998; Rousu
et al., 2005; Weinberger and Chapelle, 2009; Zhao
et al., 2011; Bi and Kwok, 2011; Zhou et al., 2011;
Verma et al., 2012). Many efforts are also de-
voted to utilizing hierarchical information in KGs.



2238

Leacock and Chodorow (1998) and Ponzetto and
Strube (2007) adopt hierarchical information de-
rived from KGs to construct concept relatedness.
Morin and Bengio (2005) propose a neural lan-
guage model by utilizing hierarchical information
in WordNet. Further, Hu et al. (2015) learn entity
representations by considering the whole entity hi-
erarchies of Wikipedia and inspire many works
(Krompaß et al., 2015; Xie et al., 2016) to utilize
hierarchical type structures to help the representa-
tion learning of KGs.

Different from the recent hierarchical models
that mainly focus on entity hierarchies and directly
utilize hierarchical information as simple features,
we incorporate relation hierarchies to build a hier-
archical attention scheme with coarse-to-fine gran-
ularity to enhance RE performance. As compared
with the existing models for RE, our models could
take advantage of relation correlations to better
identify informative instances, especially for those
long-tail relations, by transferring the knowledge
from their related relations of high-frequency.

3 Methodology

In this section, we introduce the overall framework
of our hierarchical attention for RE, starting with
notations and definitions.

3.1 Notations

We denote a KG as G = {E ,R,F}, where E ,
R and F indicate the sets of entities, relations
and facts respectively. (h, r, t) ∈ F indicates
that there is a relation r ∈ R between h ∈ E
and t ∈ E . We follow the MIL setting and split
the entire instances into multiple entity-pair bags
{Sh1,t1 ,Sh2,t2 , . . .}. Each bag Shi,ti contains mul-
tiple instances {s1, s2, . . .} mentioning both the
entities hi and ti. The distant supervision mech-
anism will label the bag with the corresponding
relation of the mentioned entity pair. Each in-
stance s in these bags is denoted as a word se-
quence s = {w1, w2, . . .}.

3.2 Framework

Given an entity pair (h, t) and its entity-pair bag
Sh,t, we adopt our models to measure the prob-
ability of each relation r ∈ R holding between
the pair. As shown in Figure 2, the overall frame-
work of our models includes a sentence encoder
and a coarse-to-fine grained hierarchical attention.
The sentence encoder adopts several convolutional

neural networks to represent sentence semantics
with embeddings, and the hierarchical attention is
used to select the most informative instances to ex-
actly express their relations.

For each instance si ∈ Sh,t, we use the sentence
encoder to represent its semantic information as an
embedding si. The details of the sentence encoder
will be introduced in Section 3.3. Since not all in-
stances in the bag Sh,t are positive to express the
relation between h and t, we apply the hierarchi-
cal attention to compute an instance weight αi for
each instance si. The details of the hierarchical at-
tention will be introduced in Section 3.4. We build
the global textual relation representation rh,t with
the weighted sum of instance output embeddings,

rh,t =
m∑
i=1

αisi, s1, . . . , sm ∈ Sh,t. (1)

Here αi is the instance weight for the ith instance
output embedding si. By taking rh,t as the textual
relation representation of the entity pair (h, t), we
estimate its probability over each relation r ∈ R,
i.e., whether there is a specific relation r between
h and t. We define the conditional probability
P (r|h, t,Sh,t),

P (r|h, t,Sh,t) =
exp(or)∑
r̃∈R exp(or̃)

, (2)

where o is the scores of all relations, which is de-
fined as follows,

o = Mrh,t, (3)

where M is the representation matrix to calculate
the relation scores.

3.3 Sentence Encoder
Given an instance s containing two entities, we ap-
ply several neural architectures to encode the in-
stance into its corresponding embeddings s.

Input Layer
The input layer of the sentence encoder aims to
embed both semantic information and positional
information of words into their input embeddings.

Word Embedding is proposed by Hinton
(1986), which aims to transform words into dis-
tributed representations to capture syntactic and
semantic meanings of words. Given a sentence s
consisting of multiple words s = {w1, . . . , wn},
we adopt Skip-Gram (Mikolov et al., 2013) to



2239

Text

s1

encoder

y1

s1

encoder

y1

s1

encoder

y1

...

...

Relation Hierarchy

...

HATT

HATT

HATT

rs

Figure 2: The architecture of hierarchical attention model.

compute all kw-dimensional word embeddings
{w1, . . . ,wn}.

Position Embedding is proposed by Zeng et al.
(2014). Position embedding is used to embed the
relative distances of each word to the two entities
into two kp-dimensional vectors. By concatenat-
ing the distance embeddings for the current word
wi to the both head and tail entities, we get a uni-
fied position embedding pi ∈ Rkp×2.

For each word wi, we concatenate its word em-
bedding wi and position embedding pi to build its
input embedding xi ∈ Rki(ki = kw + kp × 2).

Encoding Layer

The encoding layer aims to compose the input
embeddings of the given instance into its cor-
responding instance embedding. In this paper,
we choose two convolutional neural architectures,
CNN (Zeng et al., 2014) and PCNN (Zeng et al.,
2015), to encode input embeddings into instance
embeddings.

Other neural architectures such as recurrent
neural architectures (Zhang and Wang, 2015) can
also be used as sentence encoders. Since previ-
ous works show that both convolutional and recur-
rent architectures can achieve comparable state-
of-the-art performance, we simply select convo-
lutional architectures in this paper. Note that, our
hierarchical attention scheme is designed indepen-
dently to the encoder choices, hence it can be eas-
ily adapted to fit other encoder architectures.

CNN slides a convolution kernel with the win-
dow size m over the input sequence {x1, . . . ,xn}

to get the kh-dimensional hidden embeddings.

hi = CNN
(
xi−m−1

2
, . . . ,xi+m−1

2

)
. (4)

A max-pooling is then applied over these hidden
embeddings to output the final instance embed-
ding s as follows,

[s]j = max
1≤i≤n

{[hi]j}, (5)

where [·]j is the j-th value of a vector.
PCNN is an extension to CNN, which also

adopts a convolution kernel to obtain hidden em-
beddings. Then, a piecewise max-pooling is ap-
plied over the hidden embeddings,

[s(1)]j = max
1≤i≤i1

{[hi]j},

[s(2)]j = max
i1+1≤i≤i2

{[hi]j},

[s(3)]j = max
i2+1≤i≤n

{[hi]j},

(6)

where [·]j is the j-th value of a vector, i1 and i2
are entity positions. The final instance embedding
s is achieved by concatenating these three pooling
results as follows,

s = [s(1); s(2); s(3)]. (7)

3.4 Hierarchical Selective Attention
Given the entity pair (h, t) and its bag of instances
Sh,t = {s1, s2, . . . , sm}, we achieve the instance
embeddings {s1, s2, . . . , sm} using the sentence
encoder. Afterwards, we apply a hierarchical se-
lective attention over them to get the textual rela-
tion representation rh,t for extracting relations. In



2240

this part, we will first introduce a plain selective
attention, and then introduce our hierarchical at-
tention.

Plain Selective Attention
The plain selective attention scheme computes the
attention score αi for each instance si to indicate
how well the instance can express the relation be-
tween the two entities. We assign a query vector
qr to each relation r ∈ R and the attention for
each sentence in Sh,t = {s1, s2, . . . , sm} is de-
fined as follows,

ei = q
>
r Wssi,

αi =
exp(ei)∑m
j=1 exp(ej)

,
(8)

where Ws is the weight matrix. The attention
scores can be used in Eq. 1 to compute textual re-
lation representations. For simplicity, we denote
such a plain selective attention operation as the
following equation,

rh,t = ATT(qr, {s1, s2, . . . , sm}). (9)

Hierarchical Selective Attention
The inherent hierarchical structure of relations
lead us to modeling hierarchical attention. Gen-
erally, given a relation set R of a KG G (e.g.
Freebase), which consists of base-level relations
(e.g. /location/province/capital), we
can generate the corresponding higher-level rela-
tion set RH . The relations in the high-level set
(e.g. location) are more general and common,
which usually contain several sub-relations in the
base-level set. We assume that the sub-relations of
different relations are disjoint, in other words, the
relation hierarchies are tree-structured. The gener-
ation process can be done recursively. In practice,
we start from R0 = R which is the set of all re-
lations we focus for RE, and generate k − 1 times
to get a total of k-level hierarchical relation sets
{R0,R1, . . . ,Rk−1}.

As shown in Figure 2, for a relation r = r0 ∈
R = R0, which is the focus for RE, we construct
its hierarchical chain of parent relations by back-
tracking the relation hierarchy as follows,

(r0, . . . , rk−1) ∈ R0 × . . .×Rk−1, (10)

where ri−1 is the sub-relation of ri.
As with the plain attention, we assign a query

vector qr to each relation r ∈
⋃k−1
i=0 Ri. With

the hierarchical chain, we compute attention oper-
ations on the each layer of the relation hierarchies
to obtain corresponding textual relation represen-
tations,

rih,t = ATT(qri , {s1, s2, . . . , sm}). (11)

During the training process, those relation query
vectors of high-level relations (i.e., qri with larger
i) have more instances for training than those
query vectors of base-level relations. Hence, the
high-level query vectors are more robust for in-
stance selection but with coarse-grained capabil-
ity. In contrast, the base-level query vectors
(i.e., qri with smaller i) always suffer from data
sparsity, especially for those long-tail base rela-
tions. Hence, the base-level query vectors can per-
form fine-grained instance selection but the per-
formance is not stable.

Based on the hierarchical selective attention, we
can simply concatenate the textual relation repre-
sentations on different layers as the final represen-
tation,

rh,t = [r
0
h,t; . . . ; r

k−1
h,t ]. (12)

The representation rh,t will be finally fed to com-
pute the conditional probability P (r|h, t,Sh,t) in
Eq. 2. Note that, those high-level representa-
tions (i.e., rih,t with larger i) are coarse-grained,
and those base-level representations (i.e., rih,t with
smaller i) are fine-grained. These hierarchical rep-
resentations can provide more informative infor-
mation than single-layered attention for relation
prediction, especially for those long-tail relations.

3.5 Initialization and Implementation Details

Here we introduce the learning and optimization
details for our hierarchical attention model. Dur-
ing the training process, we minimize the cross
entropy loss function. Given the collection of
entity-pair bags π = {Sh1,t1 ,Sh2,t2 , . . .} and cor-
responding labeled relations {r1, r2, . . .}, we de-
fine the loss function as follows,

J(θ) = − 1
|π|

|π|∑
i=1

logP (ri|hi, ti,Shi,ti) + λ‖θ‖
2
2,

(13)
where λ is a harmonic factor, and ‖θ‖22 is the reg-
ularizer defined as L2 normalization. All mod-
els are optimized using stochastic gradient descent
(SGD).



2241

4 Experiments

4.1 Datasets and Evaluation

We evaluate our models on the New York Times
(NYT) dataset developed by Riedel et al. (2010),
which is widely used in recent works (Lin et al.,
2016; Zeng et al., 2017; Ji et al., 2017; Han et al.,
2018; Liu et al., 2017; Wu et al., 2017; Huang and
Wang, 2017; Feng et al., 2018; Zeng et al., 2018).
The dataset has 53 relations including the NA re-
lation which indicates relations of instances are
not available. The training set has 522, 611 sen-
tences, 281, 270 entity pairs and 18, 252 relational
facts. In the test set, there are 172, 448 sentences,
96, 678 entity pairs and 1, 950 relational facts. In
both the training and test set, we truncate the sen-
tences which have more than 120 words into 120
words.

We evaluate all models in the held-out evalua-
tion. It evaluates models by comparing the rela-
tional facts discovered from the test articles with
those in Freebase and provides an approximate
measure of precision without human evaluation.
For evaluation, we draw precision-recall curves
for all models. Besides precision-recall curves, we
also show the precision values at the specific re-
call rate to conduct a more direct comparison, and
calculate the micro and macro average precision
scores to show the overall effect of different mod-
els. To further verify the effect of our hierarchical
attention for few-shot entity pairs, we follow the
previous works to report the Precision@N results.
The dataset and baseline code can be found from
Github 1 (Lin et al., 2016; Wu et al., 2017; Liu
et al., 2017).

4.2 Parameter Settings

To fairly compare the results of our hierarchical
attention models with those baselines, we also
set most of the experimental parameters following
Lin et al. (2016). Table 1 shows all experimen-
tal parameters used in the experiments. We ap-
ply dropout on the output layers of our models to
prevent overfitting. For CNN, we set the dropout
rate to 0.5. For PCNN, we observe that this model
tends to overfit on the training set very quickly,
and hence we set the dropout rate to 0.9 to allevi-
ate the overfitting problem. We also pre-train the
sentence encoder of PCNN before training our hi-
erarchical attention.

1NRE, AtNRE and soft-label-RE

Batch Size B 160
Learning Rate α 0.2
Hidden Layer Dimension kc for CNNs 230
Word Dimension kw 50
Position Dimension kp 5
Convolution Kernel Size m 3

Table 1: Parameter settings.

0.00 0.05 0.10 0.15 0.20 0.25 0.30 0.35 0.40
Recall

0.3

0.4

0.5

0.6

0.7

0.8

0.9

1.0

Pr
ec

isi
on

PCNN+HATT
CNN+HATT
PCNN+ATT
CNN+ATT
PCNN+ONE
CNN+ONE
MIML
MultiR
Mintz

Figure 3: Precision-recall curves for the proposed
model and various baseline models.

0.00 0.05 0.10 0.15 0.20 0.25 0.30 0.35 0.40
Recall

0.3

0.4

0.5

0.6

0.7

0.8

0.9

1.0

Pr
ec

isi
on

PCNN+HATT
PCNN+ATT
PCNN+ATT+ADV
PCNN+ATT+SL
PCNN+ONE

Figure 4: Precision-recall curves for the proposed
model and various attention-based neural models.

4.3 Overall Evaluation Results

To evaluate the performance of our proposed hier-
archical models, we compare the precision-recall
curves of our models with various previous re-
lation extraction models. The evaluation results
are shown in Figure 3 and Figure 4. We re-
port the results of the neural architectures in-
cluding CNN and PCNN with various attention-
based methods: +HATT is our hierarchical at-
tention method; +ATT is the plain selective at-
tention method over instances (Lin et al., 2016);
+ATT+ADV is the denoising attention method by
adding a small adversarial perturbation to instance
embeddings (Wu et al., 2017); +ATT+SL is the
attention-based model using soft-labeling method
to mitigate the side effect of the wrong label-



2242

ing problem at entity-pair level (Liu et al., 2017);
+ONE is a vanilla MIL neural model without at-
tention schemes (Zeng et al., 2015). We also com-
pare our method with feature-based models, in-
cluding Mintz (Mintz et al., 2009), MultiR (Hoff-
mann et al., 2011) and MIML (Surdeanu et al.,
2012).

From the results, we observe that:
(1) All methods have reasonable precision when

recall is smaller than 0.05. When the recall grad-
ually grows, the performance of the feature-based
methods drops much more faster than those neu-
ral models. It shows that human-designed features
are very limited as compared to neural models, es-
pecially in a noisy data environment. Hence, for
simplicity, we mainly show the results of our mod-
els and other attention-based neural models in the
following experiments.

(2) Both for CNNs and PCNNs, the models with
attention schemes outperform the vanilla models
without attention schemes. Though vanilla neural
models are powerful for relation classification, it is
still difficult to address data noise. The attention-
based methods apply attentions over multiple in-
stances and dynamically reduce the influence of
noisy instances, which can effectively improve the
performance of RE and achieve the state-of-the-art
results.

(3) As shown in both of the figures, the models
using hierarchical attention (HATT) achieve the
best results among all the attention-based models.
Even when compared with PCNN+ATT+ADV
and PCNN+ATT+SL which adopt sophisticated
denoising schemes and extra information, our
models still keep significant advantages. This in-
dicates that, as compared to the conventional plain
attention schemes which handle each relation in
isolation, our method can better take advantage
of the rich correlations among relations. We be-
lieve the performance of our hierarchical atten-
tion scheme can be further improved by adopting
extra mechanisms like adversarial training, rein-
forcement learning and soft-labeling at entity-pair
level, which will be left as our future work.

4.4 Effect of Hierarchical Attention for
Different Relations

To further verify the effectiveness of our hierar-
chical attention method for different relations, we
evaluate the RE performance of our method and
conventional attention methods. Since we focus

Method 0.1 0.2 0.3

CNN +ATT 67.5 52.8 58.5+HATT 78.9 69.9 58.5

PCNN +ATT 69.4 60.6 51.6+HATT 80.6 69.5 60.7

Method Mean Micro Macro

CNN +ATT 55.4 31.8 8.2+HATT 69.1 41.7 16.5

PCNN +ATT 60.5 38.0 15.1+HATT 70.3 42.3 17.0

Table 2: Precision (%) of attention-based models
for different recalls.

Training Instances <100 <200
Hits@K (Micro) 10 15 20 10 15 20

CNN +ATT <5.0 <5.0 21.1 <5.0 30.0 50.0+HATT 5.3 36.8 52.6 40.0 60.0 70.0

PCNN +ATT <5.0 10.5 47.4 33.3 43.3 66.7+HATT 31.6 52.6 63.2 53.3 70.0 76.7

Training Instances <100 <200
Hits@K (Macro) 10 15 20 10 15 20

CNN +ATT <5.0 <5.0 18.5 <5.0 16.2 33.3+HATT 5.6 31.5 57.4 22.7 43.9 65.1

PCNN +ATT <5.0 7.4 40.7 17.2 24.2 51.5+HATT 29.6 51.9 61.1 41.4 60.6 68.2

Table 3: Accuracy (%) of Hits@K on relations
with training instances fewer than 100/200.

more on the performance of those top-ranked re-
sults, we report the precision scores when the re-
call is 0.1, 0.2, 0.3 and their mean. We also report
micro average scores and macro average scores in
this experiment. As an approximation of the area
under the precision-recall curve, the micro aver-
age score gives a more complete view of the model
performance. Since the micro average score gen-
erally overlooks the influences of those long-tail
relations, we use the macro average score to give
more emphasis on long-tail relations in test sets,
which is often neglected by the previous works.

The evaluation results are shown in Table 2,
and from the results we observe that: Our HATT
method achieves consistent and significant im-
provements as compared to the plain ATT method.
From the micro and macro average precision
scores, we find that our HATT method effectively
improves RE performance especially for those
long-tail relations. As compared to the plain ATT
method, our method can take advantage of correla-
tions among relations to achieve the improvement,
especially on the long-tail relations.

To further demonstrate the improvements in
performance on long-tail relations after introduc-



2243

Test Mode ONE TWO ALL

P@N 100 200 300 Mean 100 200 300 Mean 100 200 300 Mean

CNN+ONE 68.3 60.7 53.8 60.9 70.3 62.7 55.8 62.9 67.3 64.7 58.1 63.4
CNN+ATT 76.2 65.2 60.8 67.4 76.2 65.7 62.1 68.0 76.2 68.6 59.8 68.2
CNN+HATT 88.0 74.5 68.0 76.8 85.0 76.0 73.0 78.0 88.0 79.0 77.7 81.6

PCNN+ONE 73.3 64.8 56.8 65.0 70.3 67.2 63.1 66.9 72.3 69.7 64.1 68.7
PCNN+ATT 73.3 69.2 60.8 67.8 77.2 71.6 66.1 71.6 76.2 73.1 67.4 72.2
PCNN+HATT 84.0 76.0 69.7 76.6 85.0 76.0 72.7 77.9 88.0 79.5 75.3 80.9

Table 4: Top-N precision (P@N) for RE on the entity pairs with different number of instances (%).

Relation: /people/person/children

High
Good David and Jody Smith and their son Nathan ofAnkeny , Iowa , stayed at the hotel, . . .

Bad . . . doting grandfather of Amanda, Lindsay,David, Alexa, Reese, Paige and Nathan.

Base
Good . . . cherished grandfather of David, Michael, Ja-son, Vicky, Andrew, Sam and Nathan

Bad David and Jody Smith and their son Nathan ofAnkeny, Iowa, stayed at the hotel . . .

Table 5: Example sentences for case study.

ing relation hierarchies, we extract a subset of the
test dataset in which all the relations has fewer
than 100/200 training instances. We employ
the Hits@K metric for evaluation. For each en-
tity pair, the evaluation requires its corresponding
golden relation in the first K candidate relations
recommanded by the models. Because it is diffi-
cult for the existing models to extract long-tail re-
lations, we select K from {10, 15, 20}. We report
the both micro and macro average Hits@K accura-
cies for these subsets. From the evaluation results
in Table 3, we observe that:

(1) For both CNN and PCNN models, our hier-
archical attention outperforms the plain attention
model. By taking advantage of the relation hier-
archy, our models can learn better about long-tail
relations via correlation information among rela-
tions. We also observe that even our hierarchical
CNN model presents a better performance than the
plain PCNN model. This shows the power of rela-
tion hierarchies, which makes our simpler CNN
model outperforms the PCNN model on those
long-tail relations.

(2) Although our HATT method has achieved
obvious progress on the long-tail relations as com-
pared with the plain ATT method, the results of all
these methods are still far from satisfactory. This
indicates that distantly supervised RE models suf-
fer from not only the wrong labeling problem, but
also the long-tail relation problem. We will in-
corporate more schemes and extra information to
solve this problem in the future.

4.5 Effect of Hierarchical Attention with
Different Instances

Since our method mainly focuses on modifications
over selective attention, we also conduct Preci-
sion@N tests on those entity pairs with few in-
stances following (Lin et al., 2016). We use the
three test settings for this experiment: the ONE
test set where we randomly select one instance for
each entity pair for evaluation; the TWO test set
where we randomly select two instances for each
entity pair; the ALL test set where we use all in-
stances for each remaining entity pair for evalua-
tion. For the ONE and TWO test set, we intend
to show that taking correlation information among
relations into consideration can lead to a better re-
lation classifier. The ALL test set is designed to
show the effect of our attention over multiple in-
stances. We report the precision values of top N
triples extracted, where N ∈ {100, 200, 300}.

The evaluation results are shown in Table 4, and
from the results we observe that:

(1) The performance of all methods is generally
improved as the instance number increases. This
shows that the selective attention model can effec-
tively take advantage of information from multi-
ple noisy instances by combining useful instances
while discarding useless ones.

(2) Our HATT method has higher precision val-
ues in the ONE test set. This indicates that even in
an insufficient information scenario, correlations
among relations can be caught by our hierarchical
attention.

4.6 Case Study

We give some examples of how our hierarchical
selective attention takes effect in selecting the sen-
tences. In Table 5, we display the sentences that
are scored highest (“Good”) or lowest (“Bad”)
by the attention of different hierarchical levels
(“High” and “Base”).

The relation /people/person/children



2244

has fewer than 1000 training instances and it is
a long-tail relation. For this relation, the in-
stance recommended by the higher-level attention
straightforwardly expresses the relational fact that
Nathan is the child of David by telling that Nathan
is David’s son, while the sentence with the low at-
tention score actually gives the relationship of be-
ing at the same generation. On the contrary, the
lower-level attention mistakenly assigns high at-
tention to the incorrect sentence. This example
shows that our hierarchical attention is beneficial
for these long-tail relations.

5 Conclusion and Future Work

In this paper, we take advantage of relation hierar-
chies and propose a novel hierarchical instance-
level attention for relation extraction. As com-
pared with previous attention-based methods,
our hierarchical attention provides coarse-to-fine
granularity in instance selection and performs bet-
ter extraction for long-tail relations. We con-
duct various experiments and the evaluation re-
sults show that incorporating the inherent hierar-
chical structure of relations into attention schemes
can take advantage of correlations among relations
and improve the performance significantly.

In the future, we plan to explore the following
directions: (1) It will be promising to adopt ex-
tra information to help train more efficient models
for solving the long-tail relation problem. (2) We
may also combine our attention method with re-
cent denoising methods to further improve model
performance.

Acknowledgments

This work is supported by the National Nat-
ural Science Foundation of China (NSFC No.
61572273, 61532010). This work is also funded
by the Natural Science Foundation of China
(NSFC) and the German Research Foundation
(DFG) in Project Crossmodal Learning, NSFC
61621136008 / DFC TRR-169.

References
Wei Bi and James T Kwok. 2011. Multi-label classi-

fication on tree-and dag-structured hierarchies. In
Proceedings of ICML, pages 17–24.

Kurt Bollacker, Colin Evans, Praveen Paritosh, Tim
Sturge, and Jamie Taylor. 2008. Freebase: a col-
laboratively created graph database for structuring

human knowledge. In Proceedings of KDD, pages
1247–1250.

Jun Feng, Minlie Huang, Li Zhao, Yang Yang, and Xi-
aoyan Zhu. 2018. Reinforcement learning for rela-
tion classification from noisy data. In Proceedings
of AAAI.

Xu Han, Zhiyuan Liu, and Maosong Sun. 2018. Neural
knowledge acquisition via mutual attention between
knowledge graph and text. In Proceedings of AAAI.

Geoffrey E Hinton. 1986. Learning distributed repre-
sentations of concepts. In Proceedings of COGSCI,
volume 1, page 12.

Raphael Hoffmann, Congle Zhang, Xiao Ling, Luke
Zettlemoyer, and Daniel S Weld. 2011. Knowledge-
based weak supervision for information extraction
of overlapping relations. In Proceedings of ACL,
pages 541–550.

Zhiting Hu, Poyao Huang, Yuntian Deng, Yingkai Gao,
and Eric P Xing. 2015. Entity hierarchy embedding.
In Proceedings of ACL, volume 1, pages 1292–1300.

Yi Yao Huang and William Yang Wang. 2017. Deep
residual learning for weakly-supervised relation ex-
traction. In Proceedings of EMNLP, pages 1803–
1807.

Guoliang Ji, Kang Liu, Shizhu He, Jun Zhao, et al.
2017. Distant supervision for relation extraction
with sentence-level attention and entity descriptions.
In AAAI, pages 3060–3066.

Denis Krompaß, Stephan Baier, and Volker Tresp.
2015. Type-constrained representation learning in
knowledge graphs. In Proceedings of ISWC.

Claudia Leacock and Martin Chodorow. 1998. Com-
bining local context and wordnet similarity for word
sense identification. Proceedings of WordNet: An
electronic lexical database, 49(2):265–283.

Yankai Lin, Shiqi Shen, Zhiyuan Liu, Huanbo Luan,
and Maosong Sun. 2016. Neural relation extraction
with selective attention over instances. In Proceed-
ings of ACL, pages 2124–2133.

Tianyu Liu, Kexiang Wang, Baobao Chang, and Zhi-
fang Sui. 2017. A soft-label method for noise-
tolerant distantly supervised relation extraction. In
Proceedings of EMNLP, pages 1790–1795.

Andrew McCallum, Ronald Rosenfeld, Tom M
Mitchell, and Andrew Y Ng. 1998. Improving text
classification by shrinkage in a hierarchy of classes.
In Proceedings of ICML, volume 98, pages 359–367.

Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey
Dean. 2013. Efficient estimation of word represen-
tations in vector space. In Proceedings of ICLR.

Mike Mintz, Steven Bills, Rion Snow, and Dan Juraf-
sky. 2009. Distant supervision for relation extrac-
tion without labeled data. In Proceedings of ACL-
IJCNLP, pages 1003–1011.



2245

Raymond J Mooney and Razvan C Bunescu. 2006.
Subsequence kernels for relation extraction. In Pro-
ceedings of NIPS, pages 171–178.

Frederic Morin and Yoshua Bengio. 2005. Hierarchi-
cal probabilistic neural network language model. In
Proceedings of AISTATS, pages 246–252.

Simone Paolo Ponzetto and Michael Strube. 2007.
Knowledge derived from wikipedia for computing
semantic relatedness. Proceedings of JAIR, 30:181–
212.

Sebastian Riedel, Limin Yao, and Andrew McCallum.
2010. Modeling relations and their mentions with-
out labeled text. In Proceedings of ECML-PKDD,
pages 148–163.

Sebastian Riedel, Limin Yao, Andrew McCallum, and
Benjamin M Marlin. 2013. Relation extraction with
matrix factorization and universal schemas. In Pro-
ceedings of NAACL, pages 74–84.

Juho Rousu, Craig Saunders, Sandor Szedmak, and
John Shawe-Taylor. 2005. Learning hierarchical
multi-category text classification models. In Pro-
ceedings of ICML, pages 744–751.

Cicero Nogueira dos Santos, Bing Xiang, and Bowen
Zhou. 2015. Classifying relations by ranking with
convolutional neural networks. In Proceedings of
ACL-IJCNLP, pages 626–634.

Mihai Surdeanu, Julie Tibshirani, Ramesh Nallapati,
and Christopher D Manning. 2012. Multi-instance
multi-label learning for relation extraction. In Pro-
ceedings of EMNLP, pages 455–465.

Patrick Verga, David Belanger, Emma Strubell, Ben-
jamin Roth, and Andrew McCallum. 2016. Multi-
lingual relation extraction using compositional uni-
versal schema. In Proceedings of NAACL, pages
886–896.

Patrick Verga and Andrew McCallum. 2016. Row-less
universal schema. In Proceedings of ACL, pages 63–
68.

Nakul Verma, Dhruv Mahajan, Sundararajan Sella-
manickam, and Vinod Nair. 2012. Learning hierar-
chical similarity metrics. In Proceedings of CVPR,
pages 2280–2287.

Kilian Q Weinberger and Olivier Chapelle. 2009.
Large margin taxonomy embedding for document
categorization. In Proceedings of NIPS, pages
1737–1744.

Yi Wu, David Bamman, and Stuart Russell. 2017. Ad-
versarial training for relation extraction. In Proceed-
ings of EMNLP, pages 1778–1783.

Ruobing Xie, Zhiyuan Liu, and Maosong Sun. 2016.
Representation learning of knowledge graphs with
hierarchical types. In Proceedings of IJCAI, pages
2965–2971.

Yan Xu, Lili Mou, Ge Li, Yunchuan Chen, Hao Peng,
and Zhi Jin. 2015. Classifying relations via long
short term memory networks along shortest depen-
dency paths. In Proceedings of EMNLP, pages
1785–1794.

Dmitry Zelenko, Chinatsu Aone, and Anthony
Richardella. 2003. Kernel methods for relation ex-
traction. Proceedings of JMLR, 3(Feb):1083–1106.

Daojian Zeng, Kang Liu, Yubo Chen, and Jun Zhao.
2015. Distant supervision for relation extraction via
piecewise convolutional neural networks. In Pro-
ceedings of EMNLP, pages 1753–1762.

Daojian Zeng, Kang Liu, Siwei Lai, Guangyou Zhou,
and Jun Zhao. 2014. Relation classification via con-
volutional deep neural network. In Proceedings of
COLING, pages 2335–2344.

Wenyuan Zeng, Yankai Lin, Zhiyuan Liu, and
Maosong Sun. 2017. Incorporating relation paths
in neural relation extraction. In Proceedings of
EMNLP, pages 1768–1777.

Xiangrong Zeng, Shizhu He, Kang Liu, and Jun Zhao.
2018. Large scaled relation extraction with rein-
forcement learning. In Proceedings of AAAI.

Dongxu Zhang and Dong Wang. 2015. Relation classi-
fication via recurrent neural network. arXiv preprint
arXiv:1508.01006.

Yuhao Zhang, Victor Zhong, Danqi Chen, Gabor An-
geli, and Christopher D Manning. 2017. Position-
aware attention and supervised data improve slot fill-
ing. In Proceedings of EMNLP, pages 35–45.

Bin Zhao, Fei Li, and Eric P Xing. 2011. Large-scale
category structure aware image categorization. In
Proceedings of NIPS, pages 87–96.

Denny Zhou, Lin Xiao, and Mingrui Wu. 2011. Hi-
erarchical classification via orthogonal transfer. In
Proceedings of ICML.

Guodong Zhou, Jian Su, Jie Zhang, and Min Zhang.
2005. Exploring various knowledge in relation ex-
traction. In Proceedings of ACL, pages 427–434.


